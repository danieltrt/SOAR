file_path,api_count,code
compute_dists.py,0,"b""import argparse\nimport models\nfrom util import util\n\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('-p0','--path0', type=str, default='./imgs/ex_ref.png')\nparser.add_argument('-p1','--path1', type=str, default='./imgs/ex_p0.png')\nparser.add_argument('--use_gpu', action='store_true', help='turn on flag to use GPU')\n\nopt = parser.parse_args()\n\n## Initializing the model\nmodel = models.PerceptualLoss(model='net-lin',net='alex',use_gpu=opt.use_gpu)\n\n# Load images\nimg0 = util.im2tensor(util.load_image(opt.path0)) # RGB image from [-1,1]\nimg1 = util.im2tensor(util.load_image(opt.path1))\n\nif(opt.use_gpu):\n\timg0 = img0.cuda()\n\timg1 = img1.cuda()\n\n\n# Compute distance\ndist01 = model.forward(img0,img1)\nprint('Distance: %.3f'%dist01)\n"""
compute_dists_dirs.py,0,"b""import argparse\nimport os\nimport models\nfrom util import util\n\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('-d0','--dir0', type=str, default='./imgs/ex_dir0')\nparser.add_argument('-d1','--dir1', type=str, default='./imgs/ex_dir1')\nparser.add_argument('-o','--out', type=str, default='./imgs/example_dists.txt')\nparser.add_argument('-v','--version', type=str, default='0.1')\nparser.add_argument('--use_gpu', action='store_true', help='turn on flag to use GPU')\n\nopt = parser.parse_args()\n\n## Initializing the model\nmodel = models.PerceptualLoss(model='net-lin',net='alex',use_gpu=opt.use_gpu,version=opt.version)\n\n# crawl directories\nf = open(opt.out,'w')\nfiles = os.listdir(opt.dir0)\n\nfor file in files:\n\tif(os.path.exists(os.path.join(opt.dir1,file))):\n\t\t# Load images\n\t\timg0 = util.im2tensor(util.load_image(os.path.join(opt.dir0,file))) # RGB image from [-1,1]\n\t\timg1 = util.im2tensor(util.load_image(os.path.join(opt.dir1,file)))\n\n\t\tif(opt.use_gpu):\n\t\t\timg0 = img0.cuda()\n\t\t\timg1 = img1.cuda()\n\n\t\t# Compute distance\n\t\tdist01 = model.forward(img0,img1)\n\t\tprint('%s: %.3f'%(file,dist01))\n\t\tf.writelines('%s: %.6f\\n'%(file,dist01))\n\nf.close()\n"""
compute_dists_pair.py,0,"b""import argparse\nimport os\nimport models\nfrom util import util\nimport numpy as np\nfrom IPython import embed\n\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('-d','--dir', type=str, default='./imgs/ex_dir0')\nparser.add_argument('-o','--out', type=str, default='./imgs/example_dists.txt')\nparser.add_argument('-v','--version', type=str, default='0.1')\nparser.add_argument('--all-pairs', action='store_true', help='turn on to test all N(N-1)/2 pairs, leave off to just do consecutive pairs (N-1)')\nparser.add_argument('-N', type=int, default=None)\nparser.add_argument('--use_gpu', action='store_true', help='turn on flag to use GPU')\n\nopt = parser.parse_args()\n\n## Initializing the model\nmodel = models.PerceptualLoss(model='net-lin',net='alex',use_gpu=opt.use_gpu,version=opt.version)\n\n# crawl directories\nf = open(opt.out,'w')\nfiles = os.listdir(opt.dir)\nif(opt.N is not None):\n\tfiles = files[:opt.N]\nF = len(files)\n\ndists = []\nfor (ff,file) in enumerate(files[:-1]):\n\timg0 = util.im2tensor(util.load_image(os.path.join(opt.dir,file))) # RGB image from [-1,1]\n\tif(opt.use_gpu):\n\t\timg0 = img0.cuda()\n\n\tif(opt.all_pairs):\n\t\tfiles1 = files[ff+1:]\n\telse:\n\t\tfiles1 = [files[ff+1],]\n\n\tfor file1 in files1:\n\t\timg1 = util.im2tensor(util.load_image(os.path.join(opt.dir,file1)))\n\n\t\tif(opt.use_gpu):\n\t\t\timg1 = img1.cuda()\n\n\t\t# Compute distance\n\t\tdist01 = model.forward(img0,img1)\n\t\tprint('(%s,%s): %.3f'%(file,file1,dist01))\n\t\tf.writelines('(%s,%s): %.6f\\n'%(file,file1,dist01))\n\n\t\tdists.append(dist01.item())\n\navg_dist = np.mean(np.array(dists))\nstderr_dist = np.std(np.array(dists))/np.sqrt(len(dists))\n\nprint('Avg: %.5f +/- %.5f'%(avg_dist,stderr_dist))\nf.writelines('Avg: %.6f +/- %.6f'%(avg_dist,stderr_dist))\n\nf.close()\n"""
perceptual_loss.py,5,"b""\nfrom __future__ import absolute_import\n\nimport sys\nimport scipy\nimport scipy.misc\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nimport models\n\nuse_gpu = True\n\nref_path  = './imgs/ex_ref.png'\npred_path = './imgs/ex_p1.png'\n\nref_img = scipy.misc.imread(ref_path).transpose(2, 0, 1) / 255.\npred_img = scipy.misc.imread(pred_path).transpose(2, 0, 1) / 255.\n\n# Torchify\nref = Variable(torch.FloatTensor(ref_img)[None,:,:,:])\npred = Variable(torch.FloatTensor(pred_img)[None,:,:,:], requires_grad=True)\n\nloss_fn = models.PerceptualLoss(model='net-lin', net='vgg', use_gpu=use_gpu)\noptimizer = torch.optim.Adam([pred,], lr=1e-3, betas=(0.9, 0.999))\n\nimport matplotlib.pyplot as plt\nplt.ion()\nfig = plt.figure(1)\nax = fig.add_subplot(131)\nax.imshow(ref_img.transpose(1, 2, 0))\nax.set_title('target')\nax = fig.add_subplot(133)\nax.imshow(pred_img.transpose(1, 2, 0))\nax.set_title('initialization')\n\nfor i in range(1000):\n    dist = loss_fn.forward(pred, ref, normalize=True)\n    optimizer.zero_grad()\n    dist.backward()\n    optimizer.step()\n    pred.data = torch.clamp(pred.data, 0, 1)\n    \n    if i % 10 == 0:\n        print('iter %d, dist %.3g' % (i, dist.view(-1).data.cpu().numpy()[0]))\n        pred_img = pred[0].data.cpu().numpy().transpose(1, 2, 0)\n        pred_img = np.clip(pred_img, 0, 1)\n        ax = fig.add_subplot(132)            \n        ax.imshow(pred_img)\n        ax.set_title('iter %d, dist %.3f' % (i, dist.view(-1).data.cpu().numpy()[0]))\n        plt.pause(5e-2)\n        # plt.imsave('imgs_saved/%04d.jpg'%i,pred_img)\n\n\n"""
test_dataset_model.py,0,"b""import numpy as np\nfrom models import dist_model as dm\nfrom data import data_loader as dl\nimport argparse\nfrom IPython import embed\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--dataset_mode', type=str, default='2afc', help='[2afc,jnd]')\nparser.add_argument('--datasets', type=str, nargs='+', default=['val/traditional','val/cnn','val/superres','val/deblur','val/color','val/frameinterp'], help='datasets to test - for jnd mode: [val/traditional],[val/cnn]; for 2afc mode: [train/traditional],[train/cnn],[train/mix],[val/traditional],[val/cnn],[val/color],[val/deblur],[val/frameinterp],[val/superres]')\nparser.add_argument('--model', type=str, default='net-lin', help='distance model type [net-lin] for linearly calibrated net, [net] for off-the-shelf network, [l2] for euclidean distance, [ssim] for Structured Similarity Image Metric')\nparser.add_argument('--net', type=str, default='alex', help='[squeeze], [alex], or [vgg] for network architectures')\nparser.add_argument('--colorspace', type=str, default='Lab', help='[Lab] or [RGB] for colorspace to use for l2, ssim model types')\nparser.add_argument('--batch_size', type=int, default=50, help='batch size to test image patches in')\nparser.add_argument('--use_gpu', action='store_true', help='turn on flag to use GPU')\nparser.add_argument('--gpu_ids', type=int, nargs='+', default=[0], help='gpus to use')\nparser.add_argument('--nThreads', type=int, default=4, help='number of threads to use in data loader')\n\nparser.add_argument('--model_path', type=str, default=None, help='location of model, will default to ./weights/v[version]/[net_name].pth')\n\nparser.add_argument('--from_scratch', action='store_true', help='model was initialized from scratch')\nparser.add_argument('--train_trunk', action='store_true', help='model trunk was trained/tuned')\nparser.add_argument('--version', type=str, default='0.1', help='v0.1 is latest, v0.0 was original release')\n\nopt = parser.parse_args()\nif(opt.model in ['l2','ssim']):\n\topt.batch_size = 1\n\n# initialize model\nmodel = dm.DistModel()\n# model.initialize(model=opt.model,net=opt.net,colorspace=opt.colorspace,model_path=opt.model_path,use_gpu=opt.use_gpu)\nmodel.initialize(model=opt.model, net=opt.net, colorspace=opt.colorspace, \n\tmodel_path=opt.model_path, use_gpu=opt.use_gpu, pnet_rand=opt.from_scratch, pnet_tune=opt.train_trunk,\n\tversion=opt.version, gpu_ids=opt.gpu_ids)\n\nif(opt.model in ['net-lin','net']):\n\tprint('Testing model [%s]-[%s]'%(opt.model,opt.net))\nelif(opt.model in ['l2','ssim']):\n\tprint('Testing model [%s]-[%s]'%(opt.model,opt.colorspace))\n\n# initialize data loader\nfor dataset in opt.datasets:\n\tdata_loader = dl.CreateDataLoader(dataset,dataset_mode=opt.dataset_mode, batch_size=opt.batch_size, nThreads=opt.nThreads)\n\n\t# evaluate model on data\n\tif(opt.dataset_mode=='2afc'):\n\t\t(score, results_verbose) = dm.score_2afc_dataset(data_loader, model.forward, name=dataset)\n\telif(opt.dataset_mode=='jnd'):\n\t\t(score, results_verbose) = dm.score_jnd_dataset(data_loader, model.forward, name=dataset)\n\n\t# print results\n\tprint('  Dataset [%s]: %.2f'%(dataset,100.*score))\n\n"""
test_network.py,2,"b""import torch\nfrom util import util\nimport models\nfrom models import dist_model as dm\nfrom IPython import embed\n\nuse_gpu = False         # Whether to use GPU\nspatial = True         # Return a spatial map of perceptual distance.\n\n# Linearly calibrated models (LPIPS)\nmodel = models.PerceptualLoss(model='net-lin', net='alex', use_gpu=use_gpu, spatial=spatial)\n\t# Can also set net = 'squeeze' or 'vgg'\n\n# Off-the-shelf uncalibrated networks\n# model = models.PerceptualLoss(model='net', net='alex', use_gpu=use_gpu, spatial=spatial)\n\t# Can also set net = 'squeeze' or 'vgg'\n\n# Low-level metrics\n# model = models.PerceptualLoss(model='L2', colorspace='Lab', use_gpu=use_gpu)\n# model = models.PerceptualLoss(model='ssim', colorspace='RGB', use_gpu=use_gpu)\n\n## Example usage with dummy tensors\ndummy_im0 = torch.zeros(1,3,64,64) # image should be RGB, normalized to [-1,1]\ndummy_im1 = torch.zeros(1,3,64,64)\nif(use_gpu):\n\tdummy_im0 = dummy_im0.cuda()\n\tdummy_im1 = dummy_im1.cuda()\ndist = model.forward(dummy_im0,dummy_im1)\n\n## Example usage with images\nex_ref = util.im2tensor(util.load_image('./imgs/ex_ref.png'))\nex_p0 = util.im2tensor(util.load_image('./imgs/ex_p0.png'))\nex_p1 = util.im2tensor(util.load_image('./imgs/ex_p1.png'))\nif(use_gpu):\n\tex_ref = ex_ref.cuda()\n\tex_p0 = ex_p0.cuda()\n\tex_p1 = ex_p1.cuda()\n\nex_d0 = model.forward(ex_ref,ex_p0)\nex_d1 = model.forward(ex_ref,ex_p1)\n\nif not spatial:\n    print('Distances: (%.3f, %.3f)'%(ex_d0, ex_d1))\nelse:\n    print('Distances: (%.3f, %.3f)'%(ex_d0.mean(), ex_d1.mean()))            # The mean distance is approximately the same as the non-spatial distance\n    \n    # Visualize a spatially-varying distance map between ex_p0 and ex_ref\n    import pylab\n    pylab.imshow(ex_d0[0,0,...].data.cpu().numpy())\n    pylab.show()\n"""
train.py,1,"b""import torch.backends.cudnn as cudnn\ncudnn.benchmark=False\n\nimport numpy as np\nimport time\nimport os\nfrom models import dist_model as dm\nfrom data import data_loader as dl\nimport argparse\nfrom util.visualizer import Visualizer\nfrom IPython import embed\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--datasets', type=str, nargs='+', default=['train/traditional','train/cnn','train/mix'], help='datasets to train on: [train/traditional],[train/cnn],[train/mix],[val/traditional],[val/cnn],[val/color],[val/deblur],[val/frameinterp],[val/superres]')\nparser.add_argument('--model', type=str, default='net-lin', help='distance model type [net-lin] for linearly calibrated net, [net] for off-the-shelf network, [l2] for euclidean distance, [ssim] for Structured Similarity Image Metric')\nparser.add_argument('--net', type=str, default='alex', help='[squeeze], [alex], or [vgg] for network architectures')\nparser.add_argument('--batch_size', type=int, default=50, help='batch size to test image patches in')\nparser.add_argument('--use_gpu', action='store_true', help='turn on flag to use GPU')\nparser.add_argument('--gpu_ids', type=int, nargs='+', default=[0], help='gpus to use')\n\nparser.add_argument('--nThreads', type=int, default=4, help='number of threads to use in data loader')\nparser.add_argument('--nepoch', type=int, default=5, help='# epochs at base learning rate')\nparser.add_argument('--nepoch_decay', type=int, default=5, help='# additional epochs at linearly learning rate')\nparser.add_argument('--display_freq', type=int, default=5000, help='frequency (in instances) of showing training results on screen')\nparser.add_argument('--print_freq', type=int, default=5000, help='frequency (in instances) of showing training results on console')\nparser.add_argument('--save_latest_freq', type=int, default=20000, help='frequency (in instances) of saving the latest results')\nparser.add_argument('--save_epoch_freq', type=int, default=1, help='frequency of saving checkpoints at the end of epochs')\nparser.add_argument('--display_id', type=int, default=0, help='window id of the visdom display, [0] for no displaying')\nparser.add_argument('--display_winsize', type=int, default=256,  help='display window size')\nparser.add_argument('--display_port', type=int, default=8001,  help='visdom display port')\nparser.add_argument('--use_html', action='store_true', help='save off html pages')\nparser.add_argument('--checkpoints_dir', type=str, default='checkpoints', help='checkpoints directory')\nparser.add_argument('--name', type=str, default='tmp', help='directory name for training')\n\nparser.add_argument('--from_scratch', action='store_true', help='model was initialized from scratch')\nparser.add_argument('--train_trunk', action='store_true', help='model trunk was trained/tuned')\nparser.add_argument('--train_plot', action='store_true', help='plot saving')\n\nopt = parser.parse_args()\nopt.save_dir = os.path.join(opt.checkpoints_dir,opt.name)\nif(not os.path.exists(opt.save_dir)):\n    os.mkdir(opt.save_dir)\n\n# initialize model\nmodel = dm.DistModel()\nmodel.initialize(model=opt.model, net=opt.net, use_gpu=opt.use_gpu, is_train=True, \n    pnet_rand=opt.from_scratch, pnet_tune=opt.train_trunk, gpu_ids=opt.gpu_ids)\n\n# load data from all training sets\ndata_loader = dl.CreateDataLoader(opt.datasets,dataset_mode='2afc', batch_size=opt.batch_size, serial_batches=False, nThreads=opt.nThreads)\ndataset = data_loader.load_data()\ndataset_size = len(data_loader)\nD = len(dataset)\nprint('Loading %i instances from'%dataset_size,opt.datasets)\nvisualizer = Visualizer(opt)\n\ntotal_steps = 0\nfid = open(os.path.join(opt.checkpoints_dir,opt.name,'train_log.txt'),'w+')\nfor epoch in range(1, opt.nepoch + opt.nepoch_decay + 1):\n    epoch_start_time = time.time()\n    for i, data in enumerate(dataset):\n        iter_start_time = time.time()\n        total_steps += opt.batch_size\n        epoch_iter = total_steps - dataset_size * (epoch - 1)\n\n        model.set_input(data)\n        model.optimize_parameters()\n\n        if total_steps % opt.display_freq == 0:\n            visualizer.display_current_results(model.get_current_visuals(), epoch)\n\n        if total_steps % opt.print_freq == 0:\n            errors = model.get_current_errors()\n            t = (time.time()-iter_start_time)/opt.batch_size\n            t2o = (time.time()-epoch_start_time)/3600.\n            t2 = t2o*D/(i+.0001)\n            visualizer.print_current_errors(epoch, epoch_iter, errors, t, t2=t2, t2o=t2o, fid=fid)\n\n            for key in errors.keys():\n                visualizer.plot_current_errors_save(epoch, float(epoch_iter)/dataset_size, opt, errors, keys=[key,], name=key, to_plot=opt.train_plot)\n\n            if opt.display_id > 0:\n                visualizer.plot_current_errors(epoch, float(epoch_iter)/dataset_size, opt, errors)\n\n        if total_steps % opt.save_latest_freq == 0:\n            print('saving the latest model (epoch %d, total_steps %d)' %\n                  (epoch, total_steps))\n            model.save(opt.save_dir, 'latest')\n\n    if epoch % opt.save_epoch_freq == 0:\n        print('saving the model at the end of epoch %d, iters %d' %\n              (epoch, total_steps))\n        model.save(opt.save_dir, 'latest')\n        model.save(opt.save_dir, epoch)\n\n    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n          (epoch, opt.nepoch + opt.nepoch_decay, time.time() - epoch_start_time))\n\n    if epoch > opt.nepoch:\n        model.update_learning_rate(opt.nepoch_decay)\n\n# model.save_done(True)\nfid.close()\n"""
data/__init__.py,0,b''
data/base_data_loader.py,0,b'\nclass BaseDataLoader():\n    def __init__(self):\n        pass\n    \n    def initialize(self):\n        pass\n\n    def load_data():\n        return None\n\n        \n        \n'
data/custom_dataset_data_loader.py,2,"b'import torch.utils.data\nfrom data.base_data_loader import BaseDataLoader\nimport os\n\ndef CreateDataset(dataroots,dataset_mode=\'2afc\',load_size=64,):\n    dataset = None\n    if dataset_mode==\'2afc\': # human judgements\n        from dataset.twoafc_dataset import TwoAFCDataset\n        dataset = TwoAFCDataset()\n    elif dataset_mode==\'jnd\': # human judgements\n        from dataset.jnd_dataset import JNDDataset\n        dataset = JNDDataset()\n    else:\n        raise ValueError(""Dataset Mode [%s] not recognized.""%self.dataset_mode)\n\n    dataset.initialize(dataroots,load_size=load_size)\n    return dataset\n\nclass CustomDatasetDataLoader(BaseDataLoader):\n    def name(self):\n        return \'CustomDatasetDataLoader\'\n\n    def initialize(self, datafolders, dataroot=\'./dataset\',dataset_mode=\'2afc\',load_size=64,batch_size=1,serial_batches=True, nThreads=1):\n        BaseDataLoader.initialize(self)\n        if(not isinstance(datafolders,list)):\n            datafolders = [datafolders,]\n        data_root_folders = [os.path.join(dataroot,datafolder) for datafolder in datafolders]\n        self.dataset = CreateDataset(data_root_folders,dataset_mode=dataset_mode,load_size=load_size)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=batch_size,\n            shuffle=not serial_batches,\n            num_workers=int(nThreads))\n\n    def load_data(self):\n        return self.dataloader\n\n    def __len__(self):\n        return len(self.dataset)\n'"
data/data_loader.py,0,"b""def CreateDataLoader(datafolder,dataroot='./dataset',dataset_mode='2afc',load_size=64,batch_size=1,serial_batches=True,nThreads=4):\n    from data.custom_dataset_data_loader import CustomDatasetDataLoader\n    data_loader = CustomDatasetDataLoader()\n    # print(data_loader.name())\n    data_loader.initialize(datafolder,dataroot=dataroot+'/'+dataset_mode,dataset_mode=dataset_mode,load_size=load_size,batch_size=batch_size,serial_batches=serial_batches, nThreads=nThreads)\n    return data_loader\n"""
data/image_folder.py,1,"b'################################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n################################################################################\n\nimport torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\nNP_EXTENSIONS = [\'.npy\',]\n\ndef is_image_file(filename, mode=\'img\'):\n    if(mode==\'img\'):\n        return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n    elif(mode==\'np\'):\n        return any(filename.endswith(extension) for extension in NP_EXTENSIONS)\n\ndef make_dataset(dirs, mode=\'img\'):\n    if(not isinstance(dirs,list)):\n        dirs = [dirs,]\n\n    images = []\n    for dir in dirs:\n        assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n        for root, _, fnames in sorted(os.walk(dir)):\n            for fname in fnames:\n                if is_image_file(fname, mode=mode):\n                    path = os.path.join(root, fname)\n                    images.append(path)\n\n    # print(""Found %i images in %s""%(len(images),root))\n    return images\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\nclass ImageFolder(data.Dataset):\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" + "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
models/__init__.py,6,"b'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom skimage.measure import compare_ssim\nimport torch\nfrom torch.autograd import Variable\n\nfrom models import dist_model\n\nclass PerceptualLoss(torch.nn.Module):\n    def __init__(self, model=\'net-lin\', net=\'alex\', colorspace=\'rgb\', spatial=False, use_gpu=True, gpu_ids=[0], version=\'0.1\'): # VGG using our perceptually-learned weights (LPIPS metric)\n    # def __init__(self, model=\'net\', net=\'vgg\', use_gpu=True): # ""default"" way of using VGG as a perceptual loss\n        super(PerceptualLoss, self).__init__()\n        print(\'Setting up Perceptual loss...\')\n        self.use_gpu = use_gpu\n        self.spatial = spatial\n        self.gpu_ids = gpu_ids\n        self.model = dist_model.DistModel()\n        self.model.initialize(model=model, net=net, use_gpu=use_gpu, colorspace=colorspace, spatial=self.spatial, gpu_ids=gpu_ids, version=version)\n        print(\'...[%s] initialized\'%self.model.name())\n        print(\'...Done\')\n\n    def forward(self, pred, target, normalize=False):\n        """"""\n        Pred and target are Variables.\n        If normalize is True, assumes the images are between [0,1] and then scales them between [-1,+1]\n        If normalize is False, assumes the images are already between [-1,+1]\n\n        Inputs pred and target are Nx3xHxW\n        Output pytorch Variable N long\n        """"""\n\n        if normalize:\n            target = 2 * target  - 1\n            pred = 2 * pred  - 1\n\n        return self.model.forward(target, pred)\n\ndef normalize_tensor(in_feat,eps=1e-10):\n    norm_factor = torch.sqrt(torch.sum(in_feat**2,dim=1,keepdim=True))\n    return in_feat/(norm_factor+eps)\n\ndef l2(p0, p1, range=255.):\n    return .5*np.mean((p0 / range - p1 / range)**2)\n\ndef psnr(p0, p1, peak=255.):\n    return 10*np.log10(peak**2/np.mean((1.*p0-1.*p1)**2))\n\ndef dssim(p0, p1, range=255.):\n    return (1 - compare_ssim(p0, p1, data_range=range, multichannel=True)) / 2.\n\ndef rgb2lab(in_img,mean_cent=False):\n    from skimage import color\n    img_lab = color.rgb2lab(in_img)\n    if(mean_cent):\n        img_lab[:,:,0] = img_lab[:,:,0]-50\n    return img_lab\n\ndef tensor2np(tensor_obj):\n    # change dimension of a tensor object into a numpy array\n    return tensor_obj[0].cpu().float().numpy().transpose((1,2,0))\n\ndef np2tensor(np_obj):\n     # change dimenion of np array into tensor array\n    return torch.Tensor(np_obj[:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n\ndef tensor2tensorlab(image_tensor,to_norm=True,mc_only=False):\n    # image tensor to lab tensor\n    from skimage import color\n\n    img = tensor2im(image_tensor)\n    img_lab = color.rgb2lab(img)\n    if(mc_only):\n        img_lab[:,:,0] = img_lab[:,:,0]-50\n    if(to_norm and not mc_only):\n        img_lab[:,:,0] = img_lab[:,:,0]-50\n        img_lab = img_lab/100.\n\n    return np2tensor(img_lab)\n\ndef tensorlab2tensor(lab_tensor,return_inbnd=False):\n    from skimage import color\n    import warnings\n    warnings.filterwarnings(""ignore"")\n\n    lab = tensor2np(lab_tensor)*100.\n    lab[:,:,0] = lab[:,:,0]+50\n\n    rgb_back = 255.*np.clip(color.lab2rgb(lab.astype(\'float\')),0,1)\n    if(return_inbnd):\n        # convert back to lab, see if we match\n        lab_back = color.rgb2lab(rgb_back.astype(\'uint8\'))\n        mask = 1.*np.isclose(lab_back,lab,atol=2.)\n        mask = np2tensor(np.prod(mask,axis=2)[:,:,np.newaxis])\n        return (im2tensor(rgb_back),mask)\n    else:\n        return im2tensor(rgb_back)\n\ndef rgb2lab(input):\n    from skimage import color\n    return color.rgb2lab(input / 255.)\n\ndef tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=255./2.):\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + cent) * factor\n    return image_numpy.astype(imtype)\n\ndef im2tensor(image, imtype=np.uint8, cent=1., factor=255./2.):\n    return torch.Tensor((image / factor - cent)\n                        [:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n\ndef tensor2vec(vector_tensor):\n    return vector_tensor.data.cpu().numpy()[:, :, 0, 0]\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\ndef tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=255./2.):\n# def tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=1.):\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + cent) * factor\n    return image_numpy.astype(imtype)\n\ndef im2tensor(image, imtype=np.uint8, cent=1., factor=255./2.):\n# def im2tensor(image, imtype=np.uint8, cent=1., factor=1.):\n    return torch.Tensor((image / factor - cent)\n                        [:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n'"
models/base_model.py,3,"b""import os\nimport torch\nfrom torch.autograd import Variable\nfrom pdb import set_trace as st\nfrom IPython import embed\n\nclass BaseModel():\n    def __init__(self):\n        pass;\n        \n    def name(self):\n        return 'BaseModel'\n\n    def initialize(self, use_gpu=True, gpu_ids=[0]):\n        self.use_gpu = use_gpu\n        self.gpu_ids = gpu_ids\n\n    def forward(self):\n        pass\n\n    def get_image_paths(self):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        return self.input\n\n    def get_current_errors(self):\n        return {}\n\n    def save(self, label):\n        pass\n\n    # helper saving function that can be used by subclasses\n    def save_network(self, network, path, network_label, epoch_label):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(path, save_filename)\n        torch.save(network.state_dict(), save_path)\n\n    # helper loading function that can be used by subclasses\n    def load_network(self, network, network_label, epoch_label):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        print('Loading network from %s'%save_path)\n        network.load_state_dict(torch.load(save_path))\n\n    def update_learning_rate():\n        pass\n\n    def get_image_paths(self):\n        return self.image_paths\n\n    def save_done(self, flag=False):\n        np.save(os.path.join(self.save_dir, 'done_flag'),flag)\n        np.savetxt(os.path.join(self.save_dir, 'done_flag'),[flag,],fmt='%i')\n\n"""
models/dist_model.py,8,"b'\nfrom __future__ import absolute_import\n\nimport sys\nimport numpy as np\nimport torch\nfrom torch import nn\nimport os\nfrom collections import OrderedDict\nfrom torch.autograd import Variable\nimport itertools\nfrom .base_model import BaseModel\nfrom scipy.ndimage import zoom\nimport fractions\nimport functools\nimport skimage.transform\nfrom tqdm import tqdm\n\nfrom IPython import embed\n\nfrom . import networks_basic as networks\nimport models as util\n\nclass DistModel(BaseModel):\n    def name(self):\n        return self.model_name\n\n    def initialize(self, model=\'net-lin\', net=\'alex\', colorspace=\'Lab\', pnet_rand=False, pnet_tune=False, model_path=None,\n            use_gpu=True, printNet=False, spatial=False, \n            is_train=False, lr=.0001, beta1=0.5, version=\'0.1\', gpu_ids=[0]):\n        \'\'\'\n        INPUTS\n            model - [\'net-lin\'] for linearly calibrated network\n                    [\'net\'] for off-the-shelf network\n                    [\'L2\'] for L2 distance in Lab colorspace\n                    [\'SSIM\'] for ssim in RGB colorspace\n            net - [\'squeeze\',\'alex\',\'vgg\']\n            model_path - if None, will look in weights/[NET_NAME].pth\n            colorspace - [\'Lab\',\'RGB\'] colorspace to use for L2 and SSIM\n            use_gpu - bool - whether or not to use a GPU\n            printNet - bool - whether or not to print network architecture out\n            spatial - bool - whether to output an array containing varying distances across spatial dimensions\n            spatial_shape - if given, output spatial shape. if None then spatial shape is determined automatically via spatial_factor (see below).\n            spatial_factor - if given, specifies upsampling factor relative to the largest spatial extent of a convolutional layer. if None then resized to size of input images.\n            spatial_order - spline order of filter for upsampling in spatial mode, by default 1 (bilinear).\n            is_train - bool - [True] for training mode\n            lr - float - initial learning rate\n            beta1 - float - initial momentum term for adam\n            version - 0.1 for latest, 0.0 was original (with a bug)\n            gpu_ids - int array - [0] by default, gpus to use\n        \'\'\'\n        BaseModel.initialize(self, use_gpu=use_gpu, gpu_ids=gpu_ids)\n\n        self.model = model\n        self.net = net\n        self.is_train = is_train\n        self.spatial = spatial\n        self.gpu_ids = gpu_ids\n        self.model_name = \'%s [%s]\'%(model,net)\n\n        if(self.model == \'net-lin\'): # pretrained net + linear layer\n            self.net = networks.PNetLin(pnet_rand=pnet_rand, pnet_tune=pnet_tune, pnet_type=net,\n                use_dropout=True, spatial=spatial, version=version, lpips=True)\n            kw = {}\n            if not use_gpu:\n                kw[\'map_location\'] = \'cpu\'\n            if(model_path is None):\n                import inspect\n                model_path = os.path.abspath(os.path.join(inspect.getfile(self.initialize), \'..\', \'weights/v%s/%s.pth\'%(version,net)))\n\n            if(not is_train):\n                print(\'Loading model from: %s\'%model_path)\n                self.net.load_state_dict(torch.load(model_path, **kw), strict=False)\n\n        elif(self.model==\'net\'): # pretrained network\n            self.net = networks.PNetLin(pnet_rand=pnet_rand, pnet_type=net, lpips=False)\n        elif(self.model in [\'L2\',\'l2\']):\n            self.net = networks.L2(use_gpu=use_gpu,colorspace=colorspace) # not really a network, only for testing\n            self.model_name = \'L2\'\n        elif(self.model in [\'DSSIM\',\'dssim\',\'SSIM\',\'ssim\']):\n            self.net = networks.DSSIM(use_gpu=use_gpu,colorspace=colorspace)\n            self.model_name = \'SSIM\'\n        else:\n            raise ValueError(""Model [%s] not recognized."" % self.model)\n\n        self.parameters = list(self.net.parameters())\n\n        if self.is_train: # training mode\n            # extra network on top to go from distances (d0,d1) => predicted human judgment (h*)\n            self.rankLoss = networks.BCERankingLoss()\n            self.parameters += list(self.rankLoss.net.parameters())\n            self.lr = lr\n            self.old_lr = lr\n            self.optimizer_net = torch.optim.Adam(self.parameters, lr=lr, betas=(beta1, 0.999))\n        else: # test mode\n            self.net.eval()\n\n        if(use_gpu):\n            self.net.to(gpu_ids[0])\n            self.net = torch.nn.DataParallel(self.net, device_ids=gpu_ids)\n            if(self.is_train):\n                self.rankLoss = self.rankLoss.to(device=gpu_ids[0]) # just put this on GPU0\n\n        if(printNet):\n            print(\'---------- Networks initialized -------------\')\n            networks.print_network(self.net)\n            print(\'-----------------------------------------------\')\n\n    def forward(self, in0, in1, retPerLayer=False):\n        \'\'\' Function computes the distance between image patches in0 and in1\n        INPUTS\n            in0, in1 - torch.Tensor object of shape Nx3xXxY - image patch scaled to [-1,1]\n        OUTPUT\n            computed distances between in0 and in1\n        \'\'\'\n\n        return self.net.forward(in0, in1, retPerLayer=retPerLayer)\n\n    # ***** TRAINING FUNCTIONS *****\n    def optimize_parameters(self):\n        self.forward_train()\n        self.optimizer_net.zero_grad()\n        self.backward_train()\n        self.optimizer_net.step()\n        self.clamp_weights()\n\n    def clamp_weights(self):\n        for module in self.net.modules():\n            if(hasattr(module, \'weight\') and module.kernel_size==(1,1)):\n                module.weight.data = torch.clamp(module.weight.data,min=0)\n\n    def set_input(self, data):\n        self.input_ref = data[\'ref\']\n        self.input_p0 = data[\'p0\']\n        self.input_p1 = data[\'p1\']\n        self.input_judge = data[\'judge\']\n\n        if(self.use_gpu):\n            self.input_ref = self.input_ref.to(device=self.gpu_ids[0])\n            self.input_p0 = self.input_p0.to(device=self.gpu_ids[0])\n            self.input_p1 = self.input_p1.to(device=self.gpu_ids[0])\n            self.input_judge = self.input_judge.to(device=self.gpu_ids[0])\n\n        self.var_ref = Variable(self.input_ref,requires_grad=True)\n        self.var_p0 = Variable(self.input_p0,requires_grad=True)\n        self.var_p1 = Variable(self.input_p1,requires_grad=True)\n\n    def forward_train(self): # run forward pass\n        # print(self.net.module.scaling_layer.shift)\n        # print(torch.norm(self.net.module.net.slice1[0].weight).item(), torch.norm(self.net.module.lin0.model[1].weight).item())\n\n        self.d0 = self.forward(self.var_ref, self.var_p0)\n        self.d1 = self.forward(self.var_ref, self.var_p1)\n        self.acc_r = self.compute_accuracy(self.d0,self.d1,self.input_judge)\n\n        self.var_judge = Variable(1.*self.input_judge).view(self.d0.size())\n\n        self.loss_total = self.rankLoss.forward(self.d0, self.d1, self.var_judge*2.-1.)\n\n        return self.loss_total\n\n    def backward_train(self):\n        torch.mean(self.loss_total).backward()\n\n    def compute_accuracy(self,d0,d1,judge):\n        \'\'\' d0, d1 are Variables, judge is a Tensor \'\'\'\n        d1_lt_d0 = (d1<d0).cpu().data.numpy().flatten()\n        judge_per = judge.cpu().numpy().flatten()\n        return d1_lt_d0*judge_per + (1-d1_lt_d0)*(1-judge_per)\n\n    def get_current_errors(self):\n        retDict = OrderedDict([(\'loss_total\', self.loss_total.data.cpu().numpy()),\n                            (\'acc_r\', self.acc_r)])\n\n        for key in retDict.keys():\n            retDict[key] = np.mean(retDict[key])\n\n        return retDict\n\n    def get_current_visuals(self):\n        zoom_factor = 256/self.var_ref.data.size()[2]\n\n        ref_img = util.tensor2im(self.var_ref.data)\n        p0_img = util.tensor2im(self.var_p0.data)\n        p1_img = util.tensor2im(self.var_p1.data)\n\n        ref_img_vis = zoom(ref_img,[zoom_factor, zoom_factor, 1],order=0)\n        p0_img_vis = zoom(p0_img,[zoom_factor, zoom_factor, 1],order=0)\n        p1_img_vis = zoom(p1_img,[zoom_factor, zoom_factor, 1],order=0)\n\n        return OrderedDict([(\'ref\', ref_img_vis),\n                            (\'p0\', p0_img_vis),\n                            (\'p1\', p1_img_vis)])\n\n    def save(self, path, label):\n        if(self.use_gpu):\n            self.save_network(self.net.module, path, \'\', label)\n        else:\n            self.save_network(self.net, path, \'\', label)\n        self.save_network(self.rankLoss.net, path, \'rank\', label)\n\n    def update_learning_rate(self,nepoch_decay):\n        lrd = self.lr / nepoch_decay\n        lr = self.old_lr - lrd\n\n        for param_group in self.optimizer_net.param_groups:\n            param_group[\'lr\'] = lr\n\n        print(\'update lr [%s] decay: %f -> %f\' % (type,self.old_lr, lr))\n        self.old_lr = lr\n\ndef score_2afc_dataset(data_loader, func, name=\'\'):\n    \'\'\' Function computes Two Alternative Forced Choice (2AFC) score using\n        distance function \'func\' in dataset \'data_loader\'\n    INPUTS\n        data_loader - CustomDatasetDataLoader object - contains a TwoAFCDataset inside\n        func - callable distance function - calling d=func(in0,in1) should take 2\n            pytorch tensors with shape Nx3xXxY, and return numpy array of length N\n    OUTPUTS\n        [0] - 2AFC score in [0,1], fraction of time func agrees with human evaluators\n        [1] - dictionary with following elements\n            d0s,d1s - N arrays containing distances between reference patch to perturbed patches \n            gts - N array in [0,1], preferred patch selected by human evaluators\n                (closer to ""0"" for left patch p0, ""1"" for right patch p1,\n                ""0.6"" means 60pct people preferred right patch, 40pct preferred left)\n            scores - N array in [0,1], corresponding to what percentage function agreed with humans\n    CONSTS\n        N - number of test triplets in data_loader\n    \'\'\'\n\n    d0s = []\n    d1s = []\n    gts = []\n\n    for data in tqdm(data_loader.load_data(), desc=name):\n        d0s+=func(data[\'ref\'],data[\'p0\']).data.cpu().numpy().flatten().tolist()\n        d1s+=func(data[\'ref\'],data[\'p1\']).data.cpu().numpy().flatten().tolist()\n        gts+=data[\'judge\'].cpu().numpy().flatten().tolist()\n\n    d0s = np.array(d0s)\n    d1s = np.array(d1s)\n    gts = np.array(gts)\n    scores = (d0s<d1s)*(1.-gts) + (d1s<d0s)*gts + (d1s==d0s)*.5\n\n    return(np.mean(scores), dict(d0s=d0s,d1s=d1s,gts=gts,scores=scores))\n\ndef score_jnd_dataset(data_loader, func, name=\'\'):\n    \'\'\' Function computes JND score using distance function \'func\' in dataset \'data_loader\'\n    INPUTS\n        data_loader - CustomDatasetDataLoader object - contains a JNDDataset inside\n        func - callable distance function - calling d=func(in0,in1) should take 2\n            pytorch tensors with shape Nx3xXxY, and return pytorch array of length N\n    OUTPUTS\n        [0] - JND score in [0,1], mAP score (area under precision-recall curve)\n        [1] - dictionary with following elements\n            ds - N array containing distances between two patches shown to human evaluator\n            sames - N array containing fraction of people who thought the two patches were identical\n    CONSTS\n        N - number of test triplets in data_loader\n    \'\'\'\n\n    ds = []\n    gts = []\n\n    for data in tqdm(data_loader.load_data(), desc=name):\n        ds+=func(data[\'p0\'],data[\'p1\']).data.cpu().numpy().tolist()\n        gts+=data[\'same\'].cpu().numpy().flatten().tolist()\n\n    sames = np.array(gts)\n    ds = np.array(ds)\n\n    sorted_inds = np.argsort(ds)\n    ds_sorted = ds[sorted_inds]\n    sames_sorted = sames[sorted_inds]\n\n    TPs = np.cumsum(sames_sorted)\n    FPs = np.cumsum(1-sames_sorted)\n    FNs = np.sum(sames_sorted)-TPs\n\n    precs = TPs/(TPs+FPs)\n    recs = TPs/(TPs+FNs)\n    score = util.voc_ap(recs,precs)\n\n    return(score, dict(ds=ds,sames=sames))\n'"
models/networks_basic.py,10,"b""\nfrom __future__ import absolute_import\n\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torch.autograd import Variable\nimport numpy as np\nfrom pdb import set_trace as st\nfrom skimage import color\nfrom IPython import embed\nfrom . import pretrained_networks as pn\n\nimport models as util\n\ndef spatial_average(in_tens, keepdim=True):\n    return in_tens.mean([2,3],keepdim=keepdim)\n\ndef upsample(in_tens, out_H=64): # assumes scale factor is same for H and W\n    in_H = in_tens.shape[2]\n    scale_factor = 1.*out_H/in_H\n\n    return nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=False)(in_tens)\n\n# Learned perceptual metric\nclass PNetLin(nn.Module):\n    def __init__(self, pnet_type='vgg', pnet_rand=False, pnet_tune=False, use_dropout=True, spatial=False, version='0.1', lpips=True):\n        super(PNetLin, self).__init__()\n\n        self.pnet_type = pnet_type\n        self.pnet_tune = pnet_tune\n        self.pnet_rand = pnet_rand\n        self.spatial = spatial\n        self.lpips = lpips\n        self.version = version\n        self.scaling_layer = ScalingLayer()\n\n        if(self.pnet_type in ['vgg','vgg16']):\n            net_type = pn.vgg16\n            self.chns = [64,128,256,512,512]\n        elif(self.pnet_type=='alex'):\n            net_type = pn.alexnet\n            self.chns = [64,192,384,256,256]\n        elif(self.pnet_type=='squeeze'):\n            net_type = pn.squeezenet\n            self.chns = [64,128,256,384,384,512,512]\n        self.L = len(self.chns)\n\n        self.net = net_type(pretrained=not self.pnet_rand, requires_grad=self.pnet_tune)\n\n        if(lpips):\n            self.lin0 = NetLinLayer(self.chns[0], use_dropout=use_dropout)\n            self.lin1 = NetLinLayer(self.chns[1], use_dropout=use_dropout)\n            self.lin2 = NetLinLayer(self.chns[2], use_dropout=use_dropout)\n            self.lin3 = NetLinLayer(self.chns[3], use_dropout=use_dropout)\n            self.lin4 = NetLinLayer(self.chns[4], use_dropout=use_dropout)\n            self.lins = [self.lin0,self.lin1,self.lin2,self.lin3,self.lin4]\n            if(self.pnet_type=='squeeze'): # 7 layers for squeezenet\n                self.lin5 = NetLinLayer(self.chns[5], use_dropout=use_dropout)\n                self.lin6 = NetLinLayer(self.chns[6], use_dropout=use_dropout)\n                self.lins+=[self.lin5,self.lin6]\n\n    def forward(self, in0, in1, retPerLayer=False):\n        # v0.0 - original release had a bug, where input was not scaled\n        in0_input, in1_input = (self.scaling_layer(in0), self.scaling_layer(in1)) if self.version=='0.1' else (in0, in1)\n        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)\n        feats0, feats1, diffs = {}, {}, {}\n\n        for kk in range(self.L):\n            feats0[kk], feats1[kk] = util.normalize_tensor(outs0[kk]), util.normalize_tensor(outs1[kk])\n            diffs[kk] = (feats0[kk]-feats1[kk])**2\n\n        if(self.lpips):\n            if(self.spatial):\n                res = [upsample(self.lins[kk].model(diffs[kk]), out_H=in0.shape[2]) for kk in range(self.L)]\n            else:\n                res = [spatial_average(self.lins[kk].model(diffs[kk]), keepdim=True) for kk in range(self.L)]\n        else:\n            if(self.spatial):\n                res = [upsample(diffs[kk].sum(dim=1,keepdim=True), out_H=in0.shape[2]) for kk in range(self.L)]\n            else:\n                res = [spatial_average(diffs[kk].sum(dim=1,keepdim=True), keepdim=True) for kk in range(self.L)]\n\n        val = res[0]\n        for l in range(1,self.L):\n            val += res[l]\n        \n        if(retPerLayer):\n            return (val, res)\n        else:\n            return val\n\nclass ScalingLayer(nn.Module):\n    def __init__(self):\n        super(ScalingLayer, self).__init__()\n        self.register_buffer('shift', torch.Tensor([-.030,-.088,-.188])[None,:,None,None])\n        self.register_buffer('scale', torch.Tensor([.458,.448,.450])[None,:,None,None])\n\n    def forward(self, inp):\n        return (inp - self.shift) / self.scale\n\n\nclass NetLinLayer(nn.Module):\n    ''' A single linear layer which does a 1x1 conv '''\n    def __init__(self, chn_in, chn_out=1, use_dropout=False):\n        super(NetLinLayer, self).__init__()\n\n        layers = [nn.Dropout(),] if(use_dropout) else []\n        layers += [nn.Conv2d(chn_in, chn_out, 1, stride=1, padding=0, bias=False),]\n        self.model = nn.Sequential(*layers)\n\n\nclass Dist2LogitLayer(nn.Module):\n    ''' takes 2 distances, puts through fc layers, spits out value between [0,1] (if use_sigmoid is True) '''\n    def __init__(self, chn_mid=32, use_sigmoid=True):\n        super(Dist2LogitLayer, self).__init__()\n\n        layers = [nn.Conv2d(5, chn_mid, 1, stride=1, padding=0, bias=True),]\n        layers += [nn.LeakyReLU(0.2,True),]\n        layers += [nn.Conv2d(chn_mid, chn_mid, 1, stride=1, padding=0, bias=True),]\n        layers += [nn.LeakyReLU(0.2,True),]\n        layers += [nn.Conv2d(chn_mid, 1, 1, stride=1, padding=0, bias=True),]\n        if(use_sigmoid):\n            layers += [nn.Sigmoid(),]\n        self.model = nn.Sequential(*layers)\n\n    def forward(self,d0,d1,eps=0.1):\n        return self.model.forward(torch.cat((d0,d1,d0-d1,d0/(d1+eps),d1/(d0+eps)),dim=1))\n\nclass BCERankingLoss(nn.Module):\n    def __init__(self, chn_mid=32):\n        super(BCERankingLoss, self).__init__()\n        self.net = Dist2LogitLayer(chn_mid=chn_mid)\n        # self.parameters = list(self.net.parameters())\n        self.loss = torch.nn.BCELoss()\n\n    def forward(self, d0, d1, judge):\n        per = (judge+1.)/2.\n        self.logit = self.net.forward(d0,d1)\n        return self.loss(self.logit, per)\n\n# L2, DSSIM metrics\nclass FakeNet(nn.Module):\n    def __init__(self, use_gpu=True, colorspace='Lab'):\n        super(FakeNet, self).__init__()\n        self.use_gpu = use_gpu\n        self.colorspace=colorspace\n\nclass L2(FakeNet):\n\n    def forward(self, in0, in1, retPerLayer=None):\n        assert(in0.size()[0]==1) # currently only supports batchSize 1\n\n        if(self.colorspace=='RGB'):\n            (N,C,X,Y) = in0.size()\n            value = torch.mean(torch.mean(torch.mean((in0-in1)**2,dim=1).view(N,1,X,Y),dim=2).view(N,1,1,Y),dim=3).view(N)\n            return value\n        elif(self.colorspace=='Lab'):\n            value = util.l2(util.tensor2np(util.tensor2tensorlab(in0.data,to_norm=False)), \n                util.tensor2np(util.tensor2tensorlab(in1.data,to_norm=False)), range=100.).astype('float')\n            ret_var = Variable( torch.Tensor((value,) ) )\n            if(self.use_gpu):\n                ret_var = ret_var.cuda()\n            return ret_var\n\nclass DSSIM(FakeNet):\n\n    def forward(self, in0, in1, retPerLayer=None):\n        assert(in0.size()[0]==1) # currently only supports batchSize 1\n\n        if(self.colorspace=='RGB'):\n            value = util.dssim(1.*util.tensor2im(in0.data), 1.*util.tensor2im(in1.data), range=255.).astype('float')\n        elif(self.colorspace=='Lab'):\n            value = util.dssim(util.tensor2np(util.tensor2tensorlab(in0.data,to_norm=False)), \n                util.tensor2np(util.tensor2tensorlab(in1.data,to_norm=False)), range=100.).astype('float')\n        ret_var = Variable( torch.Tensor((value,) ) )\n        if(self.use_gpu):\n            ret_var = ret_var.cuda()\n        return ret_var\n\ndef print_network(net):\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    print('Network',net)\n    print('Total number of parameters: %d' % num_params)\n"""
models/pretrained_networks.py,21,"b'from collections import namedtuple\nimport torch\nfrom torchvision import models as tv\nfrom IPython import embed\n\nclass squeezenet(torch.nn.Module):\n    def __init__(self, requires_grad=False, pretrained=True):\n        super(squeezenet, self).__init__()\n        pretrained_features = tv.squeezenet1_1(pretrained=pretrained).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        self.slice6 = torch.nn.Sequential()\n        self.slice7 = torch.nn.Sequential()\n        self.N_slices = 7\n        for x in range(2):\n            self.slice1.add_module(str(x), pretrained_features[x])\n        for x in range(2,5):\n            self.slice2.add_module(str(x), pretrained_features[x])\n        for x in range(5, 8):\n            self.slice3.add_module(str(x), pretrained_features[x])\n        for x in range(8, 10):\n            self.slice4.add_module(str(x), pretrained_features[x])\n        for x in range(10, 11):\n            self.slice5.add_module(str(x), pretrained_features[x])\n        for x in range(11, 12):\n            self.slice6.add_module(str(x), pretrained_features[x])\n        for x in range(12, 13):\n            self.slice7.add_module(str(x), pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu1 = h\n        h = self.slice2(h)\n        h_relu2 = h\n        h = self.slice3(h)\n        h_relu3 = h\n        h = self.slice4(h)\n        h_relu4 = h\n        h = self.slice5(h)\n        h_relu5 = h\n        h = self.slice6(h)\n        h_relu6 = h\n        h = self.slice7(h)\n        h_relu7 = h\n        vgg_outputs = namedtuple(""SqueezeOutputs"", [\'relu1\',\'relu2\',\'relu3\',\'relu4\',\'relu5\',\'relu6\',\'relu7\'])\n        out = vgg_outputs(h_relu1,h_relu2,h_relu3,h_relu4,h_relu5,h_relu6,h_relu7)\n\n        return out\n\n\nclass alexnet(torch.nn.Module):\n    def __init__(self, requires_grad=False, pretrained=True):\n        super(alexnet, self).__init__()\n        alexnet_pretrained_features = tv.alexnet(pretrained=pretrained).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        self.N_slices = 5\n        for x in range(2):\n            self.slice1.add_module(str(x), alexnet_pretrained_features[x])\n        for x in range(2, 5):\n            self.slice2.add_module(str(x), alexnet_pretrained_features[x])\n        for x in range(5, 8):\n            self.slice3.add_module(str(x), alexnet_pretrained_features[x])\n        for x in range(8, 10):\n            self.slice4.add_module(str(x), alexnet_pretrained_features[x])\n        for x in range(10, 12):\n            self.slice5.add_module(str(x), alexnet_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu1 = h\n        h = self.slice2(h)\n        h_relu2 = h\n        h = self.slice3(h)\n        h_relu3 = h\n        h = self.slice4(h)\n        h_relu4 = h\n        h = self.slice5(h)\n        h_relu5 = h\n        alexnet_outputs = namedtuple(""AlexnetOutputs"", [\'relu1\', \'relu2\', \'relu3\', \'relu4\', \'relu5\'])\n        out = alexnet_outputs(h_relu1, h_relu2, h_relu3, h_relu4, h_relu5)\n\n        return out\n\nclass vgg16(torch.nn.Module):\n    def __init__(self, requires_grad=False, pretrained=True):\n        super(vgg16, self).__init__()\n        vgg_pretrained_features = tv.vgg16(pretrained=pretrained).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        self.N_slices = 5\n        for x in range(4):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(4, 9):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(9, 16):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(16, 23):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(23, 30):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu1_2 = h\n        h = self.slice2(h)\n        h_relu2_2 = h\n        h = self.slice3(h)\n        h_relu3_3 = h\n        h = self.slice4(h)\n        h_relu4_3 = h\n        h = self.slice5(h)\n        h_relu5_3 = h\n        vgg_outputs = namedtuple(""VggOutputs"", [\'relu1_2\', \'relu2_2\', \'relu3_3\', \'relu4_3\', \'relu5_3\'])\n        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n\n        return out\n\n\n\nclass resnet(torch.nn.Module):\n    def __init__(self, requires_grad=False, pretrained=True, num=18):\n        super(resnet, self).__init__()\n        if(num==18):\n            self.net = tv.resnet18(pretrained=pretrained)\n        elif(num==34):\n            self.net = tv.resnet34(pretrained=pretrained)\n        elif(num==50):\n            self.net = tv.resnet50(pretrained=pretrained)\n        elif(num==101):\n            self.net = tv.resnet101(pretrained=pretrained)\n        elif(num==152):\n            self.net = tv.resnet152(pretrained=pretrained)\n        self.N_slices = 5\n\n        self.conv1 = self.net.conv1\n        self.bn1 = self.net.bn1\n        self.relu = self.net.relu\n        self.maxpool = self.net.maxpool\n        self.layer1 = self.net.layer1\n        self.layer2 = self.net.layer2\n        self.layer3 = self.net.layer3\n        self.layer4 = self.net.layer4\n\n    def forward(self, X):\n        h = self.conv1(X)\n        h = self.bn1(h)\n        h = self.relu(h)\n        h_relu1 = h\n        h = self.maxpool(h)\n        h = self.layer1(h)\n        h_conv2 = h\n        h = self.layer2(h)\n        h_conv3 = h\n        h = self.layer3(h)\n        h_conv4 = h\n        h = self.layer4(h)\n        h_conv5 = h\n\n        outputs = namedtuple(""Outputs"", [\'relu1\',\'conv2\',\'conv3\',\'conv4\',\'conv5\'])\n        out = outputs(h_relu1, h_conv2, h_conv3, h_conv4, h_conv5)\n\n        return out\n'"
util/__init__.py,0,b''
util/html.py,0,"b'import dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, image_subdir=\'\', reflesh=0):\n        self.title = title\n        self.web_dir = web_dir\n        # self.img_dir = os.path.join(self.web_dir, )\n        self.img_subdir = image_subdir\n        self.img_dir = os.path.join(self.web_dir, image_subdir)\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n        # print(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if reflesh > 0:\n            with self.doc.head:\n                meta(http_equiv=""reflesh"", content=str(reflesh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=400):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(link)):\n                                img(style=""width:%dpx"" % width, src=os.path.join(im))\n                            br()\n                            p(txt)\n\n    def save(self,file=\'index\'):\n        html_file = \'%s/%s.html\' % (self.web_dir,file)\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.png\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.png\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
util/util.py,1,"b""from __future__ import print_function\n\nimport numpy as np\nfrom PIL import Image\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport torch\n\ndef load_image(path):\n    if(path[-3:] == 'dng'):\n        import rawpy\n        with rawpy.imread(path) as raw:\n            img = raw.postprocess()\n    elif(path[-3:]=='bmp' or path[-3:]=='jpg' or path[-3:]=='png'):\n        import cv2\n        return cv2.imread(path)[:,:,::-1]\n    else:\n        img = (255*plt.imread(path)[:,:,:3]).astype('uint8')\n\n    return img\n\ndef save_image(image_numpy, image_path, ):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=255./2.):\n# def tensor2im(image_tensor, imtype=np.uint8, cent=1., factor=1.):\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + cent) * factor\n    return image_numpy.astype(imtype)\n\ndef im2tensor(image, imtype=np.uint8, cent=1., factor=255./2.):\n# def im2tensor(image, imtype=np.uint8, cent=1., factor=1.):\n    return torch.Tensor((image / factor - cent)\n                        [:, :, :, np.newaxis].transpose((3, 2, 0, 1)))\n"""
util/visualizer.py,0,"b'import numpy as np\nimport os\nimport time\nfrom . import util\nfrom . import html\n# from pdb import set_trace as st\nimport matplotlib.pyplot as plt\nimport math\n# from IPython import embed\n\ndef zoom_to_res(img,res=256,order=0,axis=0):\n    # img   3xXxX\n    from scipy.ndimage import zoom\n    zoom_factor = res/img.shape[1]\n    if(axis==0):\n        return zoom(img,[1,zoom_factor,zoom_factor],order=order)\n    elif(axis==2):\n        return zoom(img,[zoom_factor,zoom_factor,1],order=order)\n\nclass Visualizer():\n    def __init__(self, opt):\n        # self.opt = opt\n        self.display_id = opt.display_id\n        # self.use_html = opt.is_train and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        self.display_cnt = 0 # display_current_results counter\n        self.display_cnt_high = 0\n        self.use_html = opt.use_html\n\n        if self.display_id > 0:\n            import visdom\n            self.vis = visdom.Visdom(port = opt.display_port)\n\n        self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n        util.mkdirs([self.web_dir,])\n        if self.use_html:\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            print(\'create web directory %s...\' % self.web_dir)\n            util.mkdirs([self.img_dir,])\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch, nrows=None, res=256):\n        if self.display_id > 0: # show images in the browser\n            title = self.name\n            if(nrows is None):\n                nrows = int(math.ceil(len(visuals.items()) / 2.0))\n            images = []\n            idx = 0\n            for label, image_numpy in visuals.items():\n                title += "" | "" if idx % nrows == 0 else "", ""\n                title += label\n                img = image_numpy.transpose([2, 0, 1])\n                img = zoom_to_res(img,res=res,order=0)\n                images.append(img)\n                idx += 1\n            if len(visuals.items()) % 2 != 0:\n                white_image = np.ones_like(image_numpy.transpose([2, 0, 1]))*255\n                white_image = zoom_to_res(white_image,res=res,order=0)\n                images.append(white_image)\n            self.vis.images(images, nrow=nrows, win=self.display_id + 1,\n                            opts=dict(title=title))\n\n        if self.use_html: # save images to a html file\n            for label, image_numpy in visuals.items():\n                img_path = os.path.join(self.img_dir, \'epoch%.3d_cnt%.6d_%s.png\' % (epoch, self.display_cnt, label))\n                util.save_image(zoom_to_res(image_numpy, res=res, axis=2), img_path)\n\n            self.display_cnt += 1\n            self.display_cnt_high = np.maximum(self.display_cnt_high, self.display_cnt)\n\n            # update website\n            webpage = html.HTML(self.web_dir, \'Experiment name = %s\' % self.name, reflesh=1)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'epoch [%d]\' % n)\n                if(n==epoch):\n                    high = self.display_cnt\n                else:\n                    high = self.display_cnt_high\n                for c in range(high-1,-1,-1):\n                    ims = []\n                    txts = []\n                    links = []\n\n                    for label, image_numpy in visuals.items():\n                        img_path = \'epoch%.3d_cnt%.6d_%s.png\' % (n, c, label)\n                        ims.append(os.path.join(\'images\',img_path))\n                        txts.append(label)\n                        links.append(os.path.join(\'images\',img_path))\n                    webpage.add_images(ims, txts, links, width=self.win_size)\n            webpage.save()\n\n    # save errors into a directory\n    def plot_current_errors_save(self, epoch, counter_ratio, opt, errors,keys=\'+ALL\',name=\'loss\', to_plot=False):\n        if not hasattr(self, \'plot_data\'):\n            self.plot_data = {\'X\':[],\'Y\':[], \'legend\':list(errors.keys())}\n        self.plot_data[\'X\'].append(epoch + counter_ratio)\n        self.plot_data[\'Y\'].append([errors[k] for k in self.plot_data[\'legend\']])\n\n        # embed()\n        if(keys==\'+ALL\'):\n            plot_keys = self.plot_data[\'legend\']\n        else:\n            plot_keys = keys\n\n        if(to_plot):\n            (f,ax) = plt.subplots(1,1)\n        for (k,kname) in enumerate(plot_keys):\n            kk = np.where(np.array(self.plot_data[\'legend\'])==kname)[0][0]\n            x = self.plot_data[\'X\']\n            y = np.array(self.plot_data[\'Y\'])[:,kk]\n            if(to_plot):\n                ax.plot(x, y, \'o-\', label=kname)\n            np.save(os.path.join(self.web_dir,\'%s_x\')%kname,x)\n            np.save(os.path.join(self.web_dir,\'%s_y\')%kname,y)\n\n        if(to_plot):\n            plt.legend(loc=0,fontsize=\'small\')\n            plt.xlabel(\'epoch\')\n            plt.ylabel(\'Value\')\n            f.savefig(os.path.join(self.web_dir,\'%s.png\'%name))\n            f.clf()\n            plt.close()\n\n    # errors: dictionary of error labels and values\n    def plot_current_errors(self, epoch, counter_ratio, opt, errors):\n        if not hasattr(self, \'plot_data\'):\n            self.plot_data = {\'X\':[],\'Y\':[], \'legend\':list(errors.keys())}\n        self.plot_data[\'X\'].append(epoch + counter_ratio)\n        self.plot_data[\'Y\'].append([errors[k] for k in self.plot_data[\'legend\']])\n        self.vis.line(\n            X=np.stack([np.array(self.plot_data[\'X\'])]*len(self.plot_data[\'legend\']),1),\n            Y=np.array(self.plot_data[\'Y\']),\n            opts={\n                \'title\': self.name + \' loss over time\',\n                \'legend\': self.plot_data[\'legend\'],\n                \'xlabel\': \'epoch\',\n                \'ylabel\': \'loss\'},\n            win=self.display_id)\n\n    # errors: same format as |errors| of plotCurrentErrors\n    def print_current_errors(self, epoch, i, errors, t, t2=-1, t2o=-1, fid=None):\n        message = \'(ep: %d, it: %d, t: %.3f[s], ept: %.2f/%.2f[h]) \' % (epoch, i, t, t2o, t2)\n        message += (\', \').join([\'%s: %.3f\' % (k, v) for k, v in errors.items()])\n\n        print(message)\n        if(fid is not None):\n            fid.write(\'%s\\n\'%message)\n\n\n    # save image to the disk\n    def save_images_simple(self, webpage, images, names, in_txts, prefix=\'\', res=256):\n        image_dir = webpage.get_image_dir()\n        ims = []\n        txts = []\n        links = []\n\n        for name, image_numpy, txt in zip(names, images, in_txts):\n            image_name = \'%s_%s.png\' % (prefix, name)\n            save_path = os.path.join(image_dir, image_name)\n            if(res is not None):\n                util.save_image(zoom_to_res(image_numpy,res=res,axis=2), save_path)\n            else:\n                util.save_image(image_numpy, save_path)\n\n            ims.append(os.path.join(webpage.img_subdir,image_name))\n            # txts.append(name)\n            txts.append(txt)\n            links.append(os.path.join(webpage.img_subdir,image_name))\n        # embed()\n        webpage.add_images(ims, txts, links, width=self.win_size)\n\n    # save image to the disk\n    def save_images(self, webpage, images, names, image_path, title=\'\'):\n        image_dir = webpage.get_image_dir()\n        # short_path = ntpath.basename(image_path)\n        # name = os.path.splitext(short_path)[0]\n        # name = short_path\n        # webpage.add_header(\'%s, %s\' % (name, title))\n        ims = []\n        txts = []\n        links = []\n\n        for label, image_numpy in zip(names, images):\n            image_name = \'%s.jpg\' % (label,)\n            save_path = os.path.join(image_dir, image_name)\n            util.save_image(image_numpy, save_path)\n\n            ims.append(image_name)\n            txts.append(label)\n            links.append(image_name)\n        webpage.add_images(ims, txts, links, width=self.win_size)\n\n    # save image to the disk\n    # def save_images(self, webpage, visuals, image_path, short=False):\n    #     image_dir = webpage.get_image_dir()\n    #     if short:\n    #         short_path = ntpath.basename(image_path)\n    #         name = os.path.splitext(short_path)[0]\n    #     else:\n    #         name = image_path\n\n    #     webpage.add_header(name)\n    #     ims = []\n    #     txts = []\n    #     links = []\n\n    #     for label, image_numpy in visuals.items():\n    #         image_name = \'%s_%s.png\' % (name, label)\n    #         save_path = os.path.join(image_dir, image_name)\n    #         util.save_image(image_numpy, save_path)\n\n    #         ims.append(image_name)\n    #         txts.append(label)\n    #         links.append(image_name)\n    #     webpage.add_images(ims, txts, links, width=self.win_size)\n'"
data/dataset/__init__.py,0,b''
data/dataset/base_dataset.py,1,"b""import torch.utils.data as data\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n        \n    def name(self):\n        return 'BaseDataset'\n    \n    def initialize(self):\n        pass\n\n"""
data/dataset/jnd_dataset.py,1,"b""import os.path\nimport torchvision.transforms as transforms\nfrom data.dataset.base_dataset import BaseDataset\nfrom data.image_folder import make_dataset\nfrom PIL import Image\nimport numpy as np\nimport torch\nfrom IPython import embed\n\nclass JNDDataset(BaseDataset):\n    def initialize(self, dataroot, load_size=64):\n        self.root = dataroot\n        self.load_size = load_size\n\n        self.dir_p0 = os.path.join(self.root, 'p0')\n        self.p0_paths = make_dataset(self.dir_p0)\n        self.p0_paths = sorted(self.p0_paths)\n\n        self.dir_p1 = os.path.join(self.root, 'p1')\n        self.p1_paths = make_dataset(self.dir_p1)\n        self.p1_paths = sorted(self.p1_paths)\n\n        transform_list = []\n        transform_list.append(transforms.Scale(load_size))\n        transform_list += [transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))]\n\n        self.transform = transforms.Compose(transform_list)\n\n        # judgement directory\n        self.dir_S = os.path.join(self.root, 'same')\n        self.same_paths = make_dataset(self.dir_S,mode='np')\n        self.same_paths = sorted(self.same_paths)\n\n    def __getitem__(self, index):\n        p0_path = self.p0_paths[index]\n        p0_img_ = Image.open(p0_path).convert('RGB')\n        p0_img = self.transform(p0_img_)\n\n        p1_path = self.p1_paths[index]\n        p1_img_ = Image.open(p1_path).convert('RGB')\n        p1_img = self.transform(p1_img_)\n\n        same_path = self.same_paths[index]\n        same_img = np.load(same_path).reshape((1,1,1,)) # [0,1]\n\n        same_img = torch.FloatTensor(same_img)\n\n        return {'p0': p0_img, 'p1': p1_img, 'same': same_img,\n            'p0_path': p0_path, 'p1_path': p1_path, 'same_path': same_path}\n\n    def __len__(self):\n        return len(self.p0_paths)\n"""
data/dataset/twoafc_dataset.py,1,"b""import os.path\nimport torchvision.transforms as transforms\nfrom data.dataset.base_dataset import BaseDataset\nfrom data.image_folder import make_dataset\nfrom PIL import Image\nimport numpy as np\nimport torch\n# from IPython import embed\n\nclass TwoAFCDataset(BaseDataset):\n    def initialize(self, dataroots, load_size=64):\n        if(not isinstance(dataroots,list)):\n            dataroots = [dataroots,]\n        self.roots = dataroots\n        self.load_size = load_size\n\n        # image directory\n        self.dir_ref = [os.path.join(root, 'ref') for root in self.roots]\n        self.ref_paths = make_dataset(self.dir_ref)\n        self.ref_paths = sorted(self.ref_paths)\n\n        self.dir_p0 = [os.path.join(root, 'p0') for root in self.roots]\n        self.p0_paths = make_dataset(self.dir_p0)\n        self.p0_paths = sorted(self.p0_paths)\n\n        self.dir_p1 = [os.path.join(root, 'p1') for root in self.roots]\n        self.p1_paths = make_dataset(self.dir_p1)\n        self.p1_paths = sorted(self.p1_paths)\n\n        transform_list = []\n        transform_list.append(transforms.Scale(load_size))\n        transform_list += [transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))]\n\n        self.transform = transforms.Compose(transform_list)\n\n        # judgement directory\n        self.dir_J = [os.path.join(root, 'judge') for root in self.roots]\n        self.judge_paths = make_dataset(self.dir_J,mode='np')\n        self.judge_paths = sorted(self.judge_paths)\n\n    def __getitem__(self, index):\n        p0_path = self.p0_paths[index]\n        p0_img_ = Image.open(p0_path).convert('RGB')\n        p0_img = self.transform(p0_img_)\n\n        p1_path = self.p1_paths[index]\n        p1_img_ = Image.open(p1_path).convert('RGB')\n        p1_img = self.transform(p1_img_)\n\n        ref_path = self.ref_paths[index]\n        ref_img_ = Image.open(ref_path).convert('RGB')\n        ref_img = self.transform(ref_img_)\n\n        judge_path = self.judge_paths[index]\n        # judge_img = (np.load(judge_path)*2.-1.).reshape((1,1,1,)) # [-1,1]\n        judge_img = np.load(judge_path).reshape((1,1,1,)) # [0,1]\n\n        judge_img = torch.FloatTensor(judge_img)\n\n        return {'p0': p0_img, 'p1': p1_img, 'ref': ref_img, 'judge': judge_img,\n            'p0_path': p0_path, 'p1_path': p1_path, 'ref_path': ref_path, 'judge_path': judge_path}\n\n    def __len__(self):\n        return len(self.p0_paths)\n"""
