file_path,api_count,code
make_ilsvrc_dataset.py,0,"b""\nimport os\nfrom util import util\nimport numpy as np\nimport argparse\n\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\nparser.add_argument('--in_path', type=str, default='/data/big/dataset/ILSVRC2012')\nparser.add_argument('--out_path', type=str, default='./dataset/ilsvrc2012/')\n\nopt = parser.parse_args()\norig_path = opt.in_path\nprint('Copying ILSVRC from...[%s]' % orig_path)\n\n# Copy over part of training set (for initializer)\ntrn_small_path = os.path.join(opt.out_path, 'train_small')\nutil.mkdirs(opt.out_path)\nutil.mkdirs(trn_small_path)\ntrain_subdirs = os.listdir(os.path.join(opt.in_path, 'train'))\nfor train_subdir in train_subdirs[:10]:\n    os.symlink(os.path.join(opt.in_path, 'train', train_subdir), os.path.join(trn_small_path, train_subdir))\nprint('Making small training set in...[%s]' % trn_small_path)\n\n# Copy over whole training set\ntrn_path = os.path.join(opt.out_path, 'train')\nutil.mkdirs(opt.out_path)\nos.symlink(os.path.join(opt.in_path, 'train'), trn_path)\nprint('Making training set in...[%s]' % trn_path)\n\n# Copy over subset of ILSVRC12 val set for colorization val set\nval_path = os.path.join(opt.out_path, 'val/imgs')\nutil.mkdirs(val_path)\nprint('Making validation set in...[%s]' % val_path)\nfor val_ind in range(1000):\n    os.system('ln -s %s/val/ILSVRC2012_val_%08d.JPEG %s/ILSVRC2012_val_%08d.JPEG' % (orig_path, val_ind + 1, val_path, val_ind + 1))\n    # os.system('cp %s/val/ILSVRC2012_val_%08d.JPEG %s/ILSVRC2012_val_%08d.JPEG'%(orig_path,val_ind+1,val_path,val_ind+1))\n\n# Copy over subset of ILSVRC12 val set for colorization test set\ntest_path = os.path.join(opt.out_path, 'test/imgs')\nutil.mkdirs(test_path)\nval_inds = np.load('./resources/ilsvrclin12_val_inds.npy')\nprint('Making test set in...[%s]' % test_path)\nfor val_ind in val_inds:\n    os.system('ln -s %s/val/ILSVRC2012_val_%08d.JPEG %s/ILSVRC2012_val_%08d.JPEG' % (orig_path, val_ind + 1, test_path, val_ind + 1))\n    # os.system('cp %s/val/ILSVRC2012_val_%08d.JPEG %s/ILSVRC2012_val_%08d.JPEG'%(orig_path,val_ind+1,test_path,val_ind+1))\n"""
test.py,1,"b""\nimport os\nfrom options.train_options import TrainOptions\nfrom models import create_model\nfrom util.visualizer import save_images\nfrom util import html\n\nimport string\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom util import util\nimport numpy as np\n\n\nif __name__ == '__main__':\n    sample_ps = [1., .125, .03125]\n    to_visualize = ['gray', 'hint', 'hint_ab', 'fake_entr', 'real', 'fake_reg', 'real_ab', 'fake_ab_reg', ]\n    S = len(sample_ps)\n\n    opt = TrainOptions().parse()\n    opt.load_model = True\n    opt.num_threads = 1   # test code only supports num_threads = 1\n    opt.batch_size = 1  # test code only supports batch_size = 1\n    opt.display_id = -1  # no visdom display\n    opt.phase = 'val'\n    opt.dataroot = './dataset/ilsvrc2012/%s/' % opt.phase\n    opt.serial_batches = True\n    opt.aspect_ratio = 1.\n\n    dataset = torchvision.datasets.ImageFolder(opt.dataroot,\n                                               transform=transforms.Compose([\n                                                   transforms.Resize((opt.loadSize, opt.loadSize)),\n                                                   transforms.ToTensor()]))\n    dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=opt.batch_size, shuffle=not opt.serial_batches)\n\n    model = create_model(opt)\n    model.setup(opt)\n    model.eval()\n\n    # create website\n    web_dir = os.path.join(opt.results_dir, opt.name, '%s_%s' % (opt.phase, opt.which_epoch))\n    webpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.which_epoch))\n\n    # statistics\n    psnrs = np.zeros((opt.how_many, S))\n    entrs = np.zeros((opt.how_many, S))\n\n    for i, data_raw in enumerate(dataset_loader):\n        data_raw[0] = data_raw[0].cuda()\n        data_raw[0] = util.crop_mult(data_raw[0], mult=8)\n\n        # with no points\n        for (pp, sample_p) in enumerate(sample_ps):\n            img_path = [string.replace('%08d_%.3f' % (i, sample_p), '.', 'p')]\n            data = util.get_colorization_data(data_raw, opt, ab_thresh=0., p=sample_p)\n\n            model.set_input(data)\n            model.test(True)  # True means that losses will be computed\n            visuals = util.get_subset_dict(model.get_current_visuals(), to_visualize)\n\n            psnrs[i, pp] = util.calculate_psnr_np(util.tensor2im(visuals['real']), util.tensor2im(visuals['fake_reg']))\n            entrs[i, pp] = model.get_current_losses()['G_entr']\n\n            save_images(webpage, visuals, img_path, aspect_ratio=opt.aspect_ratio, width=opt.display_winsize)\n\n        if i % 5 == 0:\n            print('processing (%04d)-th image... %s' % (i, img_path))\n\n        if i == opt.how_many - 1:\n            break\n\n    webpage.save()\n\n    # Compute and print some summary statistics\n    psnrs_mean = np.mean(psnrs, axis=0)\n    psnrs_std = np.std(psnrs, axis=0) / np.sqrt(opt.how_many)\n\n    entrs_mean = np.mean(entrs, axis=0)\n    entrs_std = np.std(entrs, axis=0) / np.sqrt(opt.how_many)\n\n    for (pp, sample_p) in enumerate(sample_ps):\n        print('p=%.3f: %.2f+/-%.2f' % (sample_p, psnrs_mean[pp], psnrs_std[pp]))\n"""
test_sweep.py,1,"b""from options.train_options import TrainOptions\nfrom models import create_model\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom util import util\nimport numpy as np\nimport progressbar as pb\nimport shutil\n\nimport datetime as dt\nimport matplotlib.pyplot as plt\n\nif __name__ == '__main__':\n    opt = TrainOptions().parse()\n    opt.load_model = True\n    opt.num_threads = 1   # test code only supports num_threads = 1\n    opt.batch_size = 1  # test code only supports batch_size = 1\n    opt.display_id = -1  # no visdom display\n    opt.phase = 'test'\n    opt.dataroot = './dataset/ilsvrc2012/%s/' % opt.phase\n    opt.loadSize = 256\n    opt.how_many = 1000\n    opt.aspect_ratio = 1.0\n    opt.sample_Ps = [6, ]\n    opt.load_model = True\n\n    # number of random points to assign\n    num_points = np.round(10**np.arange(-.1, 2.8, .1))\n    num_points[0] = 0\n    num_points = np.unique(num_points.astype('int'))\n    N = len(num_points)\n\n    dataset = torchvision.datasets.ImageFolder(opt.dataroot,\n                                               transform=transforms.Compose([\n                                                   transforms.Resize((opt.loadSize, opt.loadSize)),\n                                                   transforms.ToTensor()]))\n    dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=opt.batch_size, shuffle=not opt.serial_batches)\n\n    model = create_model(opt)\n    model.setup(opt)\n    model.eval()\n\n    time = dt.datetime.now()\n    str_now = '%02d_%02d_%02d%02d' % (time.month, time.day, time.hour, time.minute)\n\n    shutil.copyfile('./checkpoints/%s/latest_net_G.pth' % opt.name, './checkpoints/%s/%s_net_G.pth' % (opt.name, str_now))\n\n    psnrs = np.zeros((opt.how_many, N))\n\n    bar = pb.ProgressBar(max_value=opt.how_many)\n    for i, data_raw in enumerate(dataset_loader):\n        data_raw[0] = data_raw[0].cuda()\n        data_raw[0] = util.crop_mult(data_raw[0], mult=8)\n\n        for nn in range(N):\n            # embed()\n            data = util.get_colorization_data(data_raw, opt, ab_thresh=0., num_points=num_points[nn])\n\n            model.set_input(data)\n            model.test()\n            visuals = model.get_current_visuals()\n\n            psnrs[i, nn] = util.calculate_psnr_np(util.tensor2im(visuals['real']), util.tensor2im(visuals['fake_reg']))\n\n        if i == opt.how_many - 1:\n            break\n\n        bar.update(i)\n\n    # Save results\n    psnrs_mean = np.mean(psnrs, axis=0)\n    psnrs_std = np.std(psnrs, axis=0) / np.sqrt(opt.how_many)\n\n    np.save('./checkpoints/%s/psnrs_mean_%s' % (opt.name, str_now), psnrs_mean)\n    np.save('./checkpoints/%s/psnrs_std_%s' % (opt.name, str_now), psnrs_std)\n    np.save('./checkpoints/%s/psnrs_%s' % (opt.name, str_now), psnrs)\n    print(', ').join(['%.2f' % psnr for psnr in psnrs_mean])\n\n    old_results = np.load('./resources/psnrs_siggraph.npy')\n    old_mean = np.mean(old_results, axis=0)\n    old_std = np.std(old_results, axis=0) / np.sqrt(old_results.shape[0])\n    print(', ').join(['%.2f' % psnr for psnr in old_mean])\n\n    num_points_hack = 1. * num_points\n    num_points_hack[0] = .4\n\n    plt.plot(num_points_hack, psnrs_mean, 'bo-', label=str_now)\n    plt.plot(num_points_hack, psnrs_mean + psnrs_std, 'b--')\n    plt.plot(num_points_hack, psnrs_mean - psnrs_std, 'b--')\n    plt.plot(num_points_hack, old_mean, 'ro-', label='siggraph17')\n    plt.plot(num_points_hack, old_mean + old_std, 'r--')\n    plt.plot(num_points_hack, old_mean - old_std, 'r--')\n\n    plt.xscale('log')\n    plt.xticks([.4, 1, 2, 5, 10, 20, 50, 100, 200, 500],\n               ['Auto', '1', '2', '5', '10', '20', '50', '100', '200', '500'])\n    plt.xlabel('Number of points')\n    plt.ylabel('PSNR [db]')\n    plt.legend(loc=0)\n    plt.xlim((num_points_hack[0], num_points_hack[-1]))\n    plt.savefig('./checkpoints/%s/sweep_%s.png' % (opt.name, str_now))\n"""
train.py,1,"b""import time\nfrom options.train_options import TrainOptions\nfrom models import create_model\nfrom util.visualizer import Visualizer\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom util import util\n\nif __name__ == '__main__':\n    opt = TrainOptions().parse()\n\n    opt.dataroot = './dataset/ilsvrc2012/%s/' % opt.phase\n    dataset = torchvision.datasets.ImageFolder(opt.dataroot,\n                                               transform=transforms.Compose([\n                                                   transforms.RandomChoice([transforms.Resize(opt.loadSize, interpolation=1),\n                                                                            transforms.Resize(opt.loadSize, interpolation=2),\n                                                                            transforms.Resize(opt.loadSize, interpolation=3),\n                                                                            transforms.Resize((opt.loadSize, opt.loadSize), interpolation=1),\n                                                                            transforms.Resize((opt.loadSize, opt.loadSize), interpolation=2),\n                                                                            transforms.Resize((opt.loadSize, opt.loadSize), interpolation=3)]),\n                                                   transforms.RandomChoice([transforms.RandomResizedCrop(opt.fineSize, interpolation=1),\n                                                                            transforms.RandomResizedCrop(opt.fineSize, interpolation=2),\n                                                                            transforms.RandomResizedCrop(opt.fineSize, interpolation=3)]),\n                                                   transforms.RandomHorizontalFlip(),\n                                                   transforms.ToTensor()]))\n    dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=opt.batch_size, shuffle=True, num_workers=int(opt.num_threads))\n\n    dataset_size = len(dataset)\n    print('#training images = %d' % dataset_size)\n\n    model = create_model(opt)\n    model.setup(opt)\n    model.print_networks(True)\n\n    visualizer = Visualizer(opt)\n    total_steps = 0\n\n    for epoch in range(opt.epoch_count, opt.niter + opt.niter_decay):\n        epoch_start_time = time.time()\n        iter_data_time = time.time()\n        epoch_iter = 0\n\n        # for i, data in enumerate(dataset):\n        for i, data_raw in enumerate(dataset_loader):\n            data_raw[0] = data_raw[0].cuda()\n            data = util.get_colorization_data(data_raw, opt, p=opt.sample_p)\n            if(data is None):\n                continue\n\n            iter_start_time = time.time()\n            if total_steps % opt.print_freq == 0:\n                # time to load data\n                t_data = iter_start_time - iter_data_time\n            visualizer.reset()\n            total_steps += opt.batch_size\n            epoch_iter += opt.batch_size\n            model.set_input(data)\n            model.optimize_parameters()\n\n            if total_steps % opt.display_freq == 0:\n                save_result = total_steps % opt.update_html_freq == 0\n                visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n\n            if total_steps % opt.print_freq == 0:\n                losses = model.get_current_losses()\n                # time to do forward&backward\n                t = time.time() - iter_start_time\n                visualizer.print_current_losses(epoch, epoch_iter, losses, t, t_data)\n                if opt.display_id > 0:\n                    visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, opt, losses)\n\n            if total_steps % opt.save_latest_freq == 0:\n                print('saving the latest model (epoch %d, total_steps %d)' %\n                      (epoch, total_steps))\n                model.save_networks('latest')\n\n            iter_data_time = time.time()\n\n        if epoch % opt.save_epoch_freq == 0:\n            print('saving the model at the end of epoch %d, iters %d' %\n                  (epoch, total_steps))\n            model.save_networks('latest')\n            model.save_networks(epoch)\n\n        print('End of epoch %d / %d \\t Time Taken: %d sec' %\n              (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n        model.update_learning_rate()\n"""
data/__init__.py,2,"b'import importlib\nimport torch.utils.data\nfrom data.base_data_loader import BaseDataLoader\nfrom data.base_dataset import BaseDataset\n\n\ndef find_dataset_using_name(dataset_name):\n    # Given the option --dataset_mode [datasetname],\n    # the file ""data/datasetname_dataset.py""\n    # will be imported.\n    dataset_filename = ""data."" + dataset_name + ""_dataset""\n    datasetlib = importlib.import_module(dataset_filename)\n\n    # In the file, the class called DatasetNameDataset() will\n    # be instantiated. It has to be a subclass of BaseDataset,\n    # and it is case-insensitive.\n    dataset = None\n    target_dataset_name = dataset_name.replace(\'_\', \'\') + \'dataset\'\n    for name, cls in datasetlib.__dict__.items():\n        if name.lower() == target_dataset_name.lower() \\\n           and issubclass(cls, BaseDataset):\n            dataset = cls\n\n    if dataset is None:\n        print(""In %s.py, there should be a subclass of BaseDataset with class name that matches %s in lowercase."" % (dataset_filename, target_dataset_name))\n        exit(0)\n\n    return dataset\n\n\ndef get_option_setter(dataset_name):\n    dataset_class = find_dataset_using_name(dataset_name)\n    return dataset_class.modify_commandline_options\n\n\ndef create_dataset(opt):\n    dataset = find_dataset_using_name(opt.dataset_mode)\n    instance = dataset()\n    instance.initialize(opt)\n    print(""dataset [%s] was created"" % (instance.name()))\n    return instance\n\n\ndef CreateDataLoader(opt):\n    data_loader = CustomDatasetDataLoader()\n    data_loader.initialize(opt)\n    return data_loader\n\n\n# Wrapper class of Dataset class that performs\n# multi-threaded data loading\nclass CustomDatasetDataLoader(BaseDataLoader):\n    def name(self):\n        return \'CustomDatasetDataLoader\'\n\n    def initialize(self, opt):\n        BaseDataLoader.initialize(self, opt)\n        self.dataset = create_dataset(opt)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batch_size,\n            shuffle=not opt.serial_batches,\n            num_workers=int(opt.num_threads))\n\n    def load_data(self):\n        return self\n\n    def __len__(self):\n        return min(len(self.dataset), self.opt.max_dataset_size)\n\n    def __iter__(self):\n        for i, data in enumerate(self.dataloader):\n            if i * self.opt.batch_size >= self.opt.max_dataset_size:\n                break\n            yield data\n'"
data/aligned_dataset.py,1,"b""import os.path\nimport random\nimport torchvision.transforms as transforms\nimport torch\nfrom data.base_dataset import BaseDataset\nfrom data.image_folder import make_dataset\nfrom PIL import Image\n\n\nclass AlignedDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_AB = os.path.join(opt.dataroot, opt.phase)\n        self.AB_paths = sorted(make_dataset(self.dir_AB))\n        assert(opt.resize_or_crop == 'resize_and_crop')\n\n    def __getitem__(self, index):\n        AB_path = self.AB_paths[index]\n        AB = Image.open(AB_path).convert('RGB')\n        w, h = AB.size\n        w2 = int(w / 2)\n        A = AB.crop((0, 0, w2, h)).resize((self.opt.loadSize, self.opt.loadSize), Image.BICUBIC)\n        B = AB.crop((w2, 0, w, h)).resize((self.opt.loadSize, self.opt.loadSize), Image.BICUBIC)\n        A = transforms.ToTensor()(A)\n        B = transforms.ToTensor()(B)\n        w_offset = random.randint(0, max(0, self.opt.loadSize - self.opt.fineSize - 1))\n        h_offset = random.randint(0, max(0, self.opt.loadSize - self.opt.fineSize - 1))\n\n        A = A[:, h_offset:h_offset + self.opt.fineSize, w_offset:w_offset + self.opt.fineSize]\n        B = B[:, h_offset:h_offset + self.opt.fineSize, w_offset:w_offset + self.opt.fineSize]\n\n        A = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(A)\n        B = transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))(B)\n\n        if self.opt.which_direction == 'BtoA':\n            input_nc = self.opt.output_nc\n            output_nc = self.opt.input_nc\n        else:\n            input_nc = self.opt.input_nc\n            output_nc = self.opt.output_nc\n\n        if (not self.opt.no_flip) and random.random() < 0.5:\n            idx = [i for i in range(A.size(2) - 1, -1, -1)]\n            idx = torch.LongTensor(idx)\n            A = A.index_select(2, idx)\n            B = B.index_select(2, idx)\n\n        if input_nc == 1:  # RGB to gray\n            tmp = A[0, ...] * 0.299 + A[1, ...] * 0.587 + A[2, ...] * 0.114\n            A = tmp.unsqueeze(0)\n\n        if output_nc == 1:  # RGB to gray\n            tmp = B[0, ...] * 0.299 + B[1, ...] * 0.587 + B[2, ...] * 0.114\n            B = tmp.unsqueeze(0)\n\n        return {'A': A, 'B': B,\n                'A_paths': AB_path, 'B_paths': AB_path}\n\n    def __len__(self):\n        return len(self.AB_paths)\n\n    def name(self):\n        return 'AlignedDataset'\n"""
data/base_data_loader.py,0,"b'class BaseDataLoader():\n    def __init__(self):\n        pass\n\n    def initialize(self, opt):\n        self.opt = opt\n        pass\n\n    def load_data():\n        return None\n'"
data/base_dataset.py,1,"b'import torch.utils.data as data\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\n\nclass BaseDataset(data.Dataset):\n    def __init__(self):\n        super(BaseDataset, self).__init__()\n\n    def name(self):\n        return \'BaseDataset\'\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def initialize(self, opt):\n        pass\n\n    def __len__(self):\n        return 0\n\n\ndef get_transform(opt):\n    transform_list = []\n    if opt.resize_or_crop == \'resize_and_crop\':\n        osize = [opt.loadSize, opt.loadSize]\n        transform_list.append(transforms.Resize(osize, Image.BICUBIC))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == \'crop\':\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == \'scale_width\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.fineSize)))\n    elif opt.resize_or_crop == \'scale_width_and_crop\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __scale_width(img, opt.loadSize)))\n        transform_list.append(transforms.RandomCrop(opt.fineSize))\n    elif opt.resize_or_crop == \'none\':\n        transform_list.append(transforms.Lambda(\n            lambda img: __adjust(img)))\n    else:\n        raise ValueError(\'--resize_or_crop %s is not a valid option.\' % opt.resize_or_crop)\n\n    if opt.isTrain and not opt.no_flip:\n        transform_list.append(transforms.RandomHorizontalFlip())\n\n    transform_list += [transforms.ToTensor(),\n                       transforms.Normalize((0.5, 0.5, 0.5),\n                                            (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\n# just modify the width and height to be multiple of 4\n\n\ndef __adjust(img):\n    ow, oh = img.size\n\n    # the size needs to be a multiple of this number,\n    # because going through generator network may change img size\n    # and eventually cause size mismatch error\n    mult = 4\n    if ow % mult == 0 and oh % mult == 0:\n        return img\n    w = (ow - 1) // mult\n    w = (w + 1) * mult\n    h = (oh - 1) // mult\n    h = (h + 1) * mult\n\n    if ow != w or oh != h:\n        __print_size_warning(ow, oh, w, h)\n\n    return img.resize((w, h), Image.BICUBIC)\n\n\ndef __scale_width(img, target_width):\n    ow, oh = img.size\n\n    # the size needs to be a multiple of this number,\n    # because going through generator network may change img size\n    # and eventually cause size mismatch error\n    mult = 4\n    assert target_width % mult == 0, ""the target width needs to be multiple of %d."" % mult\n    if (ow == target_width and oh % mult == 0):\n        return img\n    w = target_width\n    target_height = int(target_width * oh / ow)\n    m = (target_height - 1) // mult\n    h = (m + 1) * mult\n\n    if target_height != h:\n        __print_size_warning(target_width, target_height, w, h)\n\n    return img.resize((w, h), Image.BICUBIC)\n\n\ndef __print_size_warning(ow, oh, w, h):\n    if not hasattr(__print_size_warning, \'has_printed\'):\n        print(""The image size needs to be a multiple of 4. ""\n              ""The loaded image size was (%d, %d), so it was adjusted to ""\n              ""(%d, %d). This adjustment will be done to all images ""\n              ""whose sizes are not multiples of 4"" % (ow, oh, w, h))\n        __print_size_warning.has_printed = True\n'"
data/color_dataset.py,0,"b""import os.path\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset\nfrom PIL import Image\n\n\nclass ColorDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_A = os.path.join(opt.dataroot)\n\n        self.A_paths = make_dataset(self.dir_A)\n\n        self.A_paths = sorted(self.A_paths)\n\n        self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        A_path = self.A_paths[index]\n        A_img = Image.open(A_path).convert('RGB')\n        A = self.transform(A_img)\n        if self.opt.which_direction == 'BtoA':\n            input_nc = self.opt.output_nc\n        else:\n            input_nc = self.opt.input_nc\n\n        # convert to Lab\n        # rgb2lab(A_img)\n\n        if input_nc == 1:  # RGB to gray\n            tmp = A[0, ...] * 0.299 + A[1, ...] * 0.587 + A[2, ...] * 0.114\n            A = tmp.unsqueeze(0)\n\n        return {'A': A, 'A_paths': A_path}\n\n    def __len__(self):\n        return len(self.A_paths)\n\n    def name(self):\n        return 'ColorImageDataset'\n"""
data/image_folder.py,1,"b'###############################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n###############################################################################\n\nimport torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef make_dataset(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                images.append(path)\n\n    return images\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" +\n                               "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
data/single_dataset.py,0,"b""import os.path\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset\nfrom PIL import Image\n\n\nclass SingleDataset(BaseDataset):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.root = opt.dataroot\n        self.dir_A = os.path.join(opt.dataroot)\n\n        self.A_paths = make_dataset(self.dir_A)\n\n        self.A_paths = sorted(self.A_paths)\n\n        self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        A_path = self.A_paths[index]\n        A_img = Image.open(A_path).convert('RGB')\n        A = self.transform(A_img)\n        if self.opt.which_direction == 'BtoA':\n            input_nc = self.opt.output_nc\n        else:\n            input_nc = self.opt.input_nc\n\n        if input_nc == 1:  # RGB to gray\n            tmp = A[0, ...] * 0.299 + A[1, ...] * 0.587 + A[2, ...] * 0.114\n            A = tmp.unsqueeze(0)\n\n        return {'A': A, 'A_paths': A_path}\n\n    def __len__(self):\n        return len(self.A_paths)\n\n    def name(self):\n        return 'SingleImageDataset'\n"""
models/__init__.py,0,"b'import importlib\nfrom models.base_model import BaseModel\n\n\ndef find_model_using_name(model_name):\n    # Given the option --model [modelname],\n    # the file ""models/modelname_model.py""\n    # will be imported.\n    model_filename = ""models."" + model_name + ""_model""\n    modellib = importlib.import_module(model_filename)\n\n    # In the file, the class called ModelNameModel() will\n    # be instantiated. It has to be a subclass of BaseModel,\n    # and it is case-insensitive.\n    model = None\n    target_model_name = model_name.replace(\'_\', \'\') + \'model\'\n    for name, cls in modellib.__dict__.items():\n        if name.lower() == target_model_name.lower() \\\n           and issubclass(cls, BaseModel):\n            model = cls\n\n    if model is None:\n        print(""In %s.py, there should be a subclass of BaseModel with class name that matches %s in lowercase."" % (model_filename, target_model_name))\n        exit(0)\n\n    return model\n\n\ndef get_option_setter(model_name):\n    model_class = find_model_using_name(model_name)\n    return model_class.modify_commandline_options\n\n\ndef create_model(opt):\n    model = find_model_using_name(opt.model)\n    instance = model()\n    instance.initialize(opt)\n    print(""model [%s] was created"" % (instance.name()))\n    return instance\n'"
models/base_model.py,8,"b""import os\nimport torch\nfrom collections import OrderedDict\nfrom . import networks\n\n\nclass BaseModel():\n    # modify parser to add command line options,\n    # and also change the default values if needed\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def name(self):\n        return 'BaseModel'\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.isTrain = opt.isTrain\n        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n        if opt.resize_or_crop != 'scale_width':\n            torch.backends.cudnn.benchmark = True\n        self.loss_names = []\n        self.model_names = []\n        self.visual_names = []\n        self.image_paths = []\n\n    def set_input(self, input):\n        self.input = input\n\n    def forward(self):\n        pass\n\n    # load and print networks; create schedulers\n    def setup(self, opt, parser=None):\n        if self.isTrain:\n            self.schedulers = [networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n\n        if not self.isTrain or opt.load_model:\n            self.load_networks(opt.which_epoch)\n        self.print_networks(opt.verbose)\n\n    # make models eval mode during test time\n    def eval(self):\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                net.eval()\n\n    # used in test time, wrapping `forward` in no_grad() so we don't save\n    # intermediate steps for backprop\n    def test(self, compute_losses=False):\n        with torch.no_grad():\n            self.forward()\n            if(compute_losses):\n                self.compute_losses_G()\n\n    # get image paths\n    def get_image_paths(self):\n        return self.image_paths\n\n    def optimize_parameters(self):\n        pass\n\n    # update learning rate (called once every epoch)\n    def update_learning_rate(self):\n        for scheduler in self.schedulers:\n            scheduler.step()\n        lr = self.optimizers[0].param_groups[0]['lr']\n        print('learning rate = %.7f' % lr)\n\n    # return visualization images. train.py will display these images, and save the images to a html\n    def get_current_visuals(self):\n        visual_ret = OrderedDict()\n        for name in self.visual_names:\n            if isinstance(name, str):\n                visual_ret[name] = getattr(self, name)\n        return visual_ret\n\n    # return traning losses/errors. train.py will print out these errors as debugging information\n    def get_current_losses(self):\n        errors_ret = OrderedDict()\n        for name in self.loss_names:\n            if isinstance(name, str):\n                # float(...) works for both scalar tensor and float number\n                errors_ret[name] = float(getattr(self, 'loss_' + name))\n        return errors_ret\n\n    # save models to the disk\n    def save_networks(self, which_epoch):\n        for name in self.model_names:\n            if isinstance(name, str):\n                save_filename = '%s_net_%s.pth' % (which_epoch, name)\n                save_path = os.path.join(self.save_dir, save_filename)\n                net = getattr(self, 'net' + name)\n\n                if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n                    torch.save(net.module.cpu().state_dict(), save_path)\n                    net.cuda(self.gpu_ids[0])\n                else:\n                    torch.save(net.cpu().state_dict(), save_path)\n\n    def __patch_instance_norm_state_dict(self, state_dict, module, keys, i=0):\n        key = keys[i]\n        if i + 1 == len(keys):  # at the end, pointing to a parameter/buffer\n            if module.__class__.__name__.startswith('InstanceNorm') and \\\n                    (key == 'running_mean' or key == 'running_var'):\n                if getattr(module, key) is None:\n                    state_dict.pop('.'.join(keys))\n            if module.__class__.__name__.startswith('InstanceNorm') and \\\n               (key == 'num_batches_tracked'):\n                state_dict.pop('.'.join(keys))\n        else:\n            self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)\n\n    # load models from the disk\n    def load_networks(self, which_epoch):\n        for name in self.model_names:\n            if isinstance(name, str):\n                load_filename = '%s_net_%s.pth' % (which_epoch, name)\n                load_path = os.path.join(self.save_dir, load_filename)\n                net = getattr(self, 'net' + name)\n                if isinstance(net, torch.nn.DataParallel):\n                    net = net.module\n                print('loading the model from %s' % load_path)\n                # if you are using PyTorch newer than 0.4 (e.g., built from\n                # GitHub source), you can remove str() on self.device\n                state_dict = torch.load(load_path, map_location=str(self.device))\n                if hasattr(state_dict, '_metadata'):\n                    del state_dict._metadata\n\n                # patch InstanceNorm checkpoints prior to 0.4\n                for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop\n                    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))\n                net.load_state_dict(state_dict)\n\n    # print network information\n    def print_networks(self, verbose):\n        print('---------- Networks initialized -------------')\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                num_params = 0\n                for param in net.parameters():\n                    num_params += param.numel()\n                if verbose:\n                    print(net)\n                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))\n        print('-----------------------------------------------')\n\n    # set requies_grad=Fasle to avoid computation\n    def set_requires_grad(self, nets, requires_grad=False):\n        if not isinstance(nets, list):\n            nets = [nets]\n        for net in nets:\n            if net is not None:\n                for param in net.parameters():\n                    param.requires_grad = requires_grad\n"""
models/networks.py,15,"b""import torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport functools\nfrom torch.optim import lr_scheduler\n\n###############################################################################\n# Helper Functions\n###############################################################################\n\n\ndef get_norm_layer(norm_type='instance'):\n    if norm_type == 'batch':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n    elif norm_type == 'instance':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False)\n    elif norm_type == 'none':\n        norm_layer = None\n    else:\n        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n    return norm_layer\n\n\ndef get_scheduler(optimizer, opt):\n    if opt.lr_policy == 'lambda':\n        def lambda_rule(epoch):\n            lr_l = 1.0 - max(0, epoch + 1 + opt.epoch_count - opt.niter) / float(opt.niter_decay + 1)\n            return lr_l\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n    elif opt.lr_policy == 'step':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n    elif opt.lr_policy == 'plateau':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, threshold=0.01, patience=5)\n    else:\n        return NotImplementedError('learning rate policy [%s] is not implemented', opt.lr_policy)\n    return scheduler\n\n\ndef init_weights(net, init_type='xavier', gain=0.02):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n            if init_type == 'normal':\n                init.normal_(m.weight.data, 0.0, gain)\n            elif init_type == 'xavier':\n                init.xavier_normal_(m.weight.data, gain=gain)\n            elif init_type == 'kaiming':\n                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n            elif init_type == 'orthogonal':\n                init.orthogonal_(m.weight.data, gain=gain)\n            else:\n                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n            if hasattr(m, 'bias') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find('BatchNorm2d') != -1:\n            init.normal_(m.weight.data, 1.0, gain)\n            init.constant_(m.bias.data, 0.0)\n\n    print('initialize network with %s' % init_type)\n    net.apply(init_func)\n\n\ndef init_net(net, init_type='xavier', gpu_ids=[]):\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        net.to(gpu_ids[0])\n        net = torch.nn.DataParallel(net, gpu_ids)\n    init_weights(net, init_type)\n    return net\n\n\ndef define_G(input_nc, output_nc, ngf, which_model_netG, norm='batch', use_dropout=False, init_type='xavier', gpu_ids=[], use_tanh=True, classification=True):\n    netG = None\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if which_model_netG == 'resnet_9blocks':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\n    elif which_model_netG == 'resnet_6blocks':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6)\n    elif which_model_netG == 'unet_128':\n        netG = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    elif which_model_netG == 'unet_256':\n        netG = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n    elif which_model_netG == 'siggraph':\n        netG = SIGGRAPHGenerator(input_nc, output_nc, norm_layer=norm_layer, use_tanh=use_tanh, classification=classification)\n    else:\n        raise NotImplementedError('Generator model name [%s] is not recognized' % which_model_netG)\n    return init_net(netG, init_type, gpu_ids)\n\n\ndef define_D(input_nc, ndf, which_model_netD,\n             n_layers_D=3, norm='batch', use_sigmoid=False, init_type='xavier', gpu_ids=[]):\n    netD = None\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if which_model_netD == 'basic':\n        netD = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    elif which_model_netD == 'n_layers':\n        netD = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    elif which_model_netD == 'pixel':\n        netD = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer, use_sigmoid=use_sigmoid)\n    else:\n        raise NotImplementedError('Discriminator model name [%s] is not recognized' %\n                                  which_model_netD)\n    return init_net(netD, init_type, gpu_ids)\n\n\n##############################################################################\n# Classes\n##############################################################################\n\n\nclass HuberLoss(nn.Module):\n    def __init__(self, delta=.01):\n        super(HuberLoss, self).__init__()\n        self.delta = delta\n\n    def __call__(self, in0, in1):\n        mask = torch.zeros_like(in0)\n        mann = torch.abs(in0 - in1)\n        eucl = .5 * (mann**2)\n        mask[...] = mann < self.delta\n\n        # loss = eucl*mask + self.delta*(mann-.5*self.delta)*(1-mask)\n        loss = eucl * mask / self.delta + (mann - .5 * self.delta) * (1 - mask)\n        return torch.sum(loss, dim=1, keepdim=True)\n\n\nclass L1Loss(nn.Module):\n    def __init__(self):\n        super(L1Loss, self).__init__()\n\n    def __call__(self, in0, in1):\n        return torch.sum(torch.abs(in0 - in1), dim=1, keepdim=True)\n\n\nclass L2Loss(nn.Module):\n    def __init__(self):\n        super(L2Loss, self).__init__()\n\n    def __call__(self, in0, in1):\n        return torch.sum((in0 - in1)**2, dim=1, keepdim=True)\n\n\n# Defines the GAN loss which uses either LSGAN or the regular GAN.\n# When LSGAN is used, it is basically same as MSELoss,\n# but it abstracts away the need to create the target label tensor\n# that has the same size as the input\nclass GANLoss(nn.Module):\n    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0):\n        super(GANLoss, self).__init__()\n        self.register_buffer('real_label', torch.tensor(target_real_label))\n        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n        if use_lsgan:\n            self.loss = nn.MSELoss()\n        else:\n            self.loss = nn.BCELoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        if target_is_real:\n            target_tensor = self.real_label\n        else:\n            target_tensor = self.fake_label\n        return target_tensor.expand_as(input)\n\n    def __call__(self, input, target_is_real):\n        target_tensor = self.get_target_tensor(input, target_is_real)\n        return self.loss(input, target_tensor)\n\n\nclass SIGGRAPHGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, norm_layer=nn.BatchNorm2d, use_tanh=True, classification=True):\n        super(SIGGRAPHGenerator, self).__init__()\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.classification = classification\n        use_bias = True\n\n        # Conv1\n        # model1=[nn.ReflectionPad2d(1),]\n        model1 = [nn.Conv2d(input_nc, 64, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        # model1+=[norm_layer(64),]\n        model1 += [nn.ReLU(True), ]\n        # model1+=[nn.ReflectionPad2d(1),]\n        model1 += [nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        model1 += [nn.ReLU(True), ]\n        model1 += [norm_layer(64), ]\n        # add a subsampling operation\n\n        # Conv2\n        # model2=[nn.ReflectionPad2d(1),]\n        model2 = [nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        # model2+=[norm_layer(128),]\n        model2 += [nn.ReLU(True), ]\n        # model2+=[nn.ReflectionPad2d(1),]\n        model2 += [nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        model2 += [nn.ReLU(True), ]\n        model2 += [norm_layer(128), ]\n        # add a subsampling layer operation\n\n        # Conv3\n        # model3=[nn.ReflectionPad2d(1),]\n        model3 = [nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        # model3+=[norm_layer(256),]\n        model3 += [nn.ReLU(True), ]\n        # model3+=[nn.ReflectionPad2d(1),]\n        model3 += [nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        # model3+=[norm_layer(256),]\n        model3 += [nn.ReLU(True), ]\n        # model3+=[nn.ReflectionPad2d(1),]\n        model3 += [nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        model3 += [nn.ReLU(True), ]\n        model3 += [norm_layer(256), ]\n        # add a subsampling layer operation\n\n        # Conv4\n        # model47=[nn.ReflectionPad2d(1),]\n        model4 = [nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        # model4+=[norm_layer(512),]\n        model4 += [nn.ReLU(True), ]\n        # model4+=[nn.ReflectionPad2d(1),]\n        model4 += [nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        # model4+=[norm_layer(512),]\n        model4 += [nn.ReLU(True), ]\n        # model4+=[nn.ReflectionPad2d(1),]\n        model4 += [nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        model4 += [nn.ReLU(True), ]\n        model4 += [norm_layer(512), ]\n\n        # Conv5\n        # model47+=[nn.ReflectionPad2d(2),]\n        model5 = [nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias), ]\n        # model5+=[norm_layer(512),]\n        model5 += [nn.ReLU(True), ]\n        # model5+=[nn.ReflectionPad2d(2),]\n        model5 += [nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias), ]\n        # model5+=[norm_layer(512),]\n        model5 += [nn.ReLU(True), ]\n        # model5+=[nn.ReflectionPad2d(2),]\n        model5 += [nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias), ]\n        model5 += [nn.ReLU(True), ]\n        model5 += [norm_layer(512), ]\n\n        # Conv6\n        # model6+=[nn.ReflectionPad2d(2),]\n        model6 = [nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias), ]\n        # model6+=[norm_layer(512),]\n        model6 += [nn.ReLU(True), ]\n        # model6+=[nn.ReflectionPad2d(2),]\n        model6 += [nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias), ]\n        # model6+=[norm_layer(512),]\n        model6 += [nn.ReLU(True), ]\n        # model6+=[nn.ReflectionPad2d(2),]\n        model6 += [nn.Conv2d(512, 512, kernel_size=3, dilation=2, stride=1, padding=2, bias=use_bias), ]\n        model6 += [nn.ReLU(True), ]\n        model6 += [norm_layer(512), ]\n\n        # Conv7\n        # model47+=[nn.ReflectionPad2d(1),]\n        model7 = [nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        # model7+=[norm_layer(512),]\n        model7 += [nn.ReLU(True), ]\n        # model7+=[nn.ReflectionPad2d(1),]\n        model7 += [nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        # model7+=[norm_layer(512),]\n        model7 += [nn.ReLU(True), ]\n        # model7+=[nn.ReflectionPad2d(1),]\n        model7 += [nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        model7 += [nn.ReLU(True), ]\n        model7 += [norm_layer(512), ]\n\n        # Conv7\n        model8up = [nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=use_bias)]\n\n        # model3short8=[nn.ReflectionPad2d(1),]\n        model3short8 = [nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n\n        # model47+=[norm_layer(256),]\n        model8 = [nn.ReLU(True), ]\n        # model8+=[nn.ReflectionPad2d(1),]\n        model8 += [nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        # model8+=[norm_layer(256),]\n        model8 += [nn.ReLU(True), ]\n        # model8+=[nn.ReflectionPad2d(1),]\n        model8 += [nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        model8 += [nn.ReLU(True), ]\n        model8 += [norm_layer(256), ]\n\n        # Conv9\n        model9up = [nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=use_bias), ]\n\n        # model2short9=[nn.ReflectionPad2d(1),]\n        model2short9 = [nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        # add the two feature maps above\n\n        # model9=[norm_layer(128),]\n        model9 = [nn.ReLU(True), ]\n        # model9+=[nn.ReflectionPad2d(1),]\n        model9 += [nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        model9 += [nn.ReLU(True), ]\n        model9 += [norm_layer(128), ]\n\n        # Conv10\n        model10up = [nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1, bias=use_bias), ]\n\n        # model1short10=[nn.ReflectionPad2d(1),]\n        model1short10 = [nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=use_bias), ]\n        # add the two feature maps above\n\n        # model10=[norm_layer(128),]\n        model10 = [nn.ReLU(True), ]\n        # model10+=[nn.ReflectionPad2d(1),]\n        model10 += [nn.Conv2d(128, 128, kernel_size=3, dilation=1, stride=1, padding=1, bias=use_bias), ]\n        model10 += [nn.LeakyReLU(negative_slope=.2), ]\n\n        # classification output\n        model_class = [nn.Conv2d(256, 529, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias), ]\n\n        # regression output\n        model_out = [nn.Conv2d(128, 2, kernel_size=1, padding=0, dilation=1, stride=1, bias=use_bias), ]\n        if(use_tanh):\n            model_out += [nn.Tanh()]\n\n        self.model1 = nn.Sequential(*model1)\n        self.model2 = nn.Sequential(*model2)\n        self.model3 = nn.Sequential(*model3)\n        self.model4 = nn.Sequential(*model4)\n        self.model5 = nn.Sequential(*model5)\n        self.model6 = nn.Sequential(*model6)\n        self.model7 = nn.Sequential(*model7)\n        self.model8up = nn.Sequential(*model8up)\n        self.model8 = nn.Sequential(*model8)\n        self.model9up = nn.Sequential(*model9up)\n        self.model9 = nn.Sequential(*model9)\n        self.model10up = nn.Sequential(*model10up)\n        self.model10 = nn.Sequential(*model10)\n        self.model3short8 = nn.Sequential(*model3short8)\n        self.model2short9 = nn.Sequential(*model2short9)\n        self.model1short10 = nn.Sequential(*model1short10)\n\n        self.model_class = nn.Sequential(*model_class)\n        self.model_out = nn.Sequential(*model_out)\n\n        self.upsample4 = nn.Sequential(*[nn.Upsample(scale_factor=4, mode='nearest'), ])\n        self.softmax = nn.Sequential(*[nn.Softmax(dim=1), ])\n\n    def forward(self, input_A, input_B, mask_B):\n        conv1_2 = self.model1(torch.cat((input_A, input_B, mask_B), dim=1))\n        conv2_2 = self.model2(conv1_2[:, :, ::2, ::2])\n        conv3_3 = self.model3(conv2_2[:, :, ::2, ::2])\n        conv4_3 = self.model4(conv3_3[:, :, ::2, ::2])\n        conv5_3 = self.model5(conv4_3)\n        conv6_3 = self.model6(conv5_3)\n        conv7_3 = self.model7(conv6_3)\n        conv8_up = self.model8up(conv7_3) + self.model3short8(conv3_3)\n        conv8_3 = self.model8(conv8_up)\n\n        if(self.classification):\n            out_class = self.model_class(conv8_3)\n\n            conv9_up = self.model9up(conv8_3.detach()) + self.model2short9(conv2_2.detach())\n            conv9_3 = self.model9(conv9_up)\n            conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2.detach())\n            conv10_2 = self.model10(conv10_up)\n            out_reg = self.model_out(conv10_2)\n        else:\n            out_class = self.model_class(conv8_3.detach())\n\n            conv9_up = self.model9up(conv8_3) + self.model2short9(conv2_2)\n            conv9_3 = self.model9(conv9_up)\n            conv10_up = self.model10up(conv9_3) + self.model1short10(conv1_2)\n            conv10_2 = self.model10(conv10_up)\n            out_reg = self.model_out(conv10_2)\n\n        return (out_class, out_reg)\n\n# Defines the generator that consists of Resnet blocks between a few\n# downsampling/upsampling operations.\n# Code and idea originally from Justin Johnson's architecture.\n# https://github.com/jcjohnson/fast-neural-style/\n\n\nclass ResnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0,\n                           bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        return self.model(input)\n\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n        conv_block = []\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim),\n                       nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n                       norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\n\n# Defines the Unet generator.\n# |num_downs|: number of downsamplings in UNet. For example,\n# if |num_downs| == 7, image of size 128x128 will become of size 1x1\n# at the bottleneck\nclass UnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n                 norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetGenerator, self).__init__()\n\n        # construct unet structure\n        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n        for i in range(num_downs - 5):\n            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)\n\n        self.model = unet_block\n\n    def forward(self, input_A, input_B, mask_B):\n        # embed()\n        return self.model(torch.cat((input_A, input_B, mask_B), dim=1))\n\n\n# Defines the submodule with skip connection.\n# X -------------------identity---------------------- X\n#   |-- downsampling -- |submodule| -- upsampling --|\nclass UnetSkipConnectionBlock(nn.Module):\n    def __init__(self, outer_nc, inner_nc, input_nc=None,\n                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetSkipConnectionBlock, self).__init__()\n        self.outermost = outermost\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n        if input_nc is None:\n            input_nc = outer_nc\n        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n                             stride=2, padding=1, bias=use_bias)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = norm_layer(inner_nc)\n        uprelu = nn.ReLU(True)\n        upnorm = norm_layer(outer_nc)\n\n        if outermost:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1)\n            down = [downconv]\n            up = [uprelu, upconv, nn.Tanh()]\n            model = down + [submodule] + up\n        elif innermost:\n            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv]\n            up = [uprelu, upconv, upnorm]\n            model = down + up\n        else:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1, bias=use_bias)\n            down = [downrelu, downconv, downnorm]\n            up = [uprelu, upconv, upnorm]\n\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:\n            return torch.cat([x, self.model(x)], 1)\n\n\n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n        super(NLayerDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        return self.model(input)\n\n\nclass PixelDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d, use_sigmoid=False):\n        super(PixelDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        self.net = [\n            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n            norm_layer(ndf * 2),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n\n        if use_sigmoid:\n            self.net.append(nn.Sigmoid())\n\n        self.net = nn.Sequential(*self.net)\n\n    def forward(self, input):\n        return self.net(input)\n"""
models/pix2pix_model.py,43,"b""import torch\nfrom collections import OrderedDict\nfrom util.image_pool import ImagePool\nfrom util import util\nfrom .base_model import BaseModel\nfrom . import networks\nimport numpy as np\n\n\nclass Pix2PixModel(BaseModel):\n    def name(self):\n        return 'Pix2PixModel'\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        return parser\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n        self.isTrain = opt.isTrain\n        self.half = opt.half\n\n        self.use_D = self.opt.lambda_GAN > 0\n\n        # specify the training losses you want to print out. The program will call base_model.get_current_losses\n\n        if(self.use_D):\n            self.loss_names = ['G_GAN', ]\n        else:\n            self.loss_names = []\n\n        self.loss_names += ['G_CE', 'G_entr', 'G_entr_hint', ]\n        self.loss_names += ['G_L1_max', 'G_L1_mean', 'G_entr', 'G_L1_reg', ]\n        self.loss_names += ['G_fake_real', 'G_fake_hint', 'G_real_hint', ]\n        self.loss_names += ['0', ]\n\n        # specify the images you want to save/display. The program will call base_model.get_current_visuals\n        self.visual_names = ['real_A', 'fake_B', 'real_B']\n        # specify the models you want to save to the disk. The program will call base_model.save_networks and base_model.load_networks\n\n        if self.isTrain:\n            if(self.use_D):\n                self.model_names = ['G', 'D']\n            else:\n                self.model_names = ['G', ]\n        else:  # during test time, only load Gs\n            self.model_names = ['G']\n\n        # load/define networks\n        num_in = opt.input_nc + opt.output_nc + 1\n        self.netG = networks.define_G(num_in, opt.output_nc, opt.ngf,\n                                      opt.which_model_netG, opt.norm, not opt.no_dropout, opt.init_type, self.gpu_ids,\n                                      use_tanh=True, classification=opt.classification)\n\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            if self.use_D:\n                self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf,\n                                              opt.which_model_netD,\n                                              opt.n_layers_D, opt.norm, use_sigmoid, opt.init_type, self.gpu_ids)\n\n        if self.isTrain:\n            self.fake_AB_pool = ImagePool(opt.pool_size)\n            # define loss functions\n            self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan).to(self.device)\n            # self.criterionL1 = torch.nn.L1Loss()\n            self.criterionL1 = networks.L1Loss()\n            self.criterionHuber = networks.HuberLoss(delta=1. / opt.ab_norm)\n\n            # if(opt.classification):\n            self.criterionCE = torch.nn.CrossEntropyLoss()\n\n            # initialize optimizers\n            self.optimizers = []\n            self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizers.append(self.optimizer_G)\n\n            if self.use_D:\n                self.optimizer_D = torch.optim.Adam(self.netD.parameters(),\n                                                    lr=opt.lr, betas=(opt.beta1, 0.999))\n                self.optimizers.append(self.optimizer_D)\n\n        if self.half:\n            for model_name in self.model_names:\n                net = getattr(self, 'net' + model_name)\n                net.half()\n                for layer in net.modules():\n                    if(isinstance(layer, torch.nn.BatchNorm2d)):\n                        layer.float()\n                print('Net %s half precision' % model_name)\n\n        # initialize average loss values\n        self.avg_losses = OrderedDict()\n        self.avg_loss_alpha = opt.avg_loss_alpha\n        self.error_cnt = 0\n\n        # self.avg_loss_alpha = 0.9993 # half-life of 1000 iterations\n        # self.avg_loss_alpha = 0.9965 # half-life of 200 iterations\n        # self.avg_loss_alpha = 0.986 # half-life of 50 iterations\n        # self.avg_loss_alpha = 0. # no averaging\n        for loss_name in self.loss_names:\n            self.avg_losses[loss_name] = 0\n\n    def set_input(self, input):\n        if(self.half):\n            for key in input.keys():\n                input[key] = input[key].half()\n\n        AtoB = self.opt.which_direction == 'AtoB'\n        self.real_A = input['A' if AtoB else 'B'].to(self.device)\n        self.real_B = input['B' if AtoB else 'A'].to(self.device)\n        # self.image_paths = input['A_paths' if AtoB else 'B_paths']\n        self.hint_B = input['hint_B'].to(self.device)\n        self.mask_B = input['mask_B'].to(self.device)\n        self.mask_B_nc = self.mask_B + self.opt.mask_cent\n\n        self.real_B_enc = util.encode_ab_ind(self.real_B[:, :, ::4, ::4], self.opt)\n\n    def forward(self):\n        (self.fake_B_class, self.fake_B_reg) = self.netG(self.real_A, self.hint_B, self.mask_B)\n        # if(self.opt.classification):\n        self.fake_B_dec_max = self.netG.module.upsample4(util.decode_max_ab(self.fake_B_class, self.opt))\n        self.fake_B_distr = self.netG.module.softmax(self.fake_B_class)\n\n        self.fake_B_dec_mean = self.netG.module.upsample4(util.decode_mean(self.fake_B_distr, self.opt))\n\n        self.fake_B_entr = self.netG.module.upsample4(-torch.sum(self.fake_B_distr * torch.log(self.fake_B_distr + 1.e-10), dim=1, keepdim=True))\n        # embed()\n\n    def backward_D(self):\n        # Fake\n        # stop backprop to the generator by detaching fake_B\n        fake_AB = self.fake_AB_pool.query(torch.cat((self.real_A, self.fake_B), 1))\n        pred_fake = self.netD(fake_AB.detach())\n        self.loss_D_fake = self.criterionGAN(pred_fake, False)\n        # self.loss_D_fake = 0\n\n        # Real\n        real_AB = torch.cat((self.real_A, self.real_B), 1)\n        pred_real = self.netD(real_AB)\n        self.loss_D_real = self.criterionGAN(pred_real, True)\n        # self.loss_D_real = 0\n\n        # Combined loss\n        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n\n        self.loss_D.backward()\n\n    def compute_losses_G(self):\n        mask_avg = torch.mean(self.mask_B_nc.type(torch.cuda.FloatTensor)) + .000001\n\n        self.loss_0 = 0  # 0 for plot\n\n        # classification statistics\n        self.loss_G_CE = self.criterionCE(self.fake_B_class.type(torch.cuda.FloatTensor),\n                                          self.real_B_enc[:, 0, :, :].type(torch.cuda.LongTensor))  # cross-entropy loss\n        self.loss_G_entr = torch.mean(self.fake_B_entr.type(torch.cuda.FloatTensor))  # entropy of predicted distribution\n        self.loss_G_entr_hint = torch.mean(self.fake_B_entr.type(torch.cuda.FloatTensor) * self.mask_B_nc.type(torch.cuda.FloatTensor)) / mask_avg  # entropy of predicted distribution at hint points\n\n        # regression statistics\n        self.loss_G_L1_max = 10 * torch.mean(self.criterionL1(self.fake_B_dec_max.type(torch.cuda.FloatTensor),\n                                                              self.real_B.type(torch.cuda.FloatTensor)))\n        self.loss_G_L1_mean = 10 * torch.mean(self.criterionL1(self.fake_B_dec_mean.type(torch.cuda.FloatTensor),\n                                                               self.real_B.type(torch.cuda.FloatTensor)))\n        self.loss_G_L1_reg = 10 * torch.mean(self.criterionL1(self.fake_B_reg.type(torch.cuda.FloatTensor),\n                                                              self.real_B.type(torch.cuda.FloatTensor)))\n\n        # L1 loss at given points\n        self.loss_G_fake_real = 10 * torch.mean(self.criterionL1(self.fake_B_reg * self.mask_B_nc, self.real_B * self.mask_B_nc).type(torch.cuda.FloatTensor)) / mask_avg\n        self.loss_G_fake_hint = 10 * torch.mean(self.criterionL1(self.fake_B_reg * self.mask_B_nc, self.hint_B * self.mask_B_nc).type(torch.cuda.FloatTensor)) / mask_avg\n        self.loss_G_real_hint = 10 * torch.mean(self.criterionL1(self.real_B * self.mask_B_nc, self.hint_B * self.mask_B_nc).type(torch.cuda.FloatTensor)) / mask_avg\n\n        # self.loss_G_L1 = torch.mean(self.criterionL1(self.fake_B, self.real_B))\n        # self.loss_G_Huber = torch.mean(self.criterionHuber(self.fake_B, self.real_B))\n        # self.loss_G_fake_real = torch.mean(self.criterionHuber(self.fake_B*self.mask_B_nc, self.real_B*self.mask_B_nc)) / mask_avg\n        # self.loss_G_fake_hint = torch.mean(self.criterionHuber(self.fake_B*self.mask_B_nc, self.hint_B*self.mask_B_nc)) / mask_avg\n        # self.loss_G_real_hint = torch.mean(self.criterionHuber(self.real_B*self.mask_B_nc, self.hint_B*self.mask_B_nc)) / mask_avg\n\n        if self.use_D:\n            fake_AB = torch.cat((self.real_A, self.fake_B), 1)\n            pred_fake = self.netD(fake_AB)\n            self.loss_G_GAN = self.criterionGAN(pred_fake, True)\n        else:\n            self.loss_G = self.loss_G_CE * self.opt.lambda_A + self.loss_G_L1_reg\n            # self.loss_G = self.loss_G_Huber*self.opt.lambda_A\n\n    def backward_G(self):\n        self.compute_losses_G()\n        self.loss_G.backward()\n\n    def optimize_parameters(self):\n        self.forward()\n\n        if(self.use_D):\n            # update D\n            self.set_requires_grad(self.netD, True)\n            self.optimizer_D.zero_grad()\n            self.backward_D()\n            self.optimizer_D.step()\n\n            self.set_requires_grad(self.netD, False)\n\n        # update G\n        self.optimizer_G.zero_grad()\n        self.backward_G()\n        self.optimizer_G.step()\n\n    def get_current_visuals(self):\n        from collections import OrderedDict\n        visual_ret = OrderedDict()\n\n        visual_ret['gray'] = util.lab2rgb(torch.cat((self.real_A.type(torch.cuda.FloatTensor), torch.zeros_like(self.real_B).type(torch.cuda.FloatTensor)), dim=1), self.opt)\n        visual_ret['real'] = util.lab2rgb(torch.cat((self.real_A.type(torch.cuda.FloatTensor), self.real_B.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n\n        visual_ret['fake_max'] = util.lab2rgb(torch.cat((self.real_A.type(torch.cuda.FloatTensor), self.fake_B_dec_max.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n        visual_ret['fake_mean'] = util.lab2rgb(torch.cat((self.real_A.type(torch.cuda.FloatTensor), self.fake_B_dec_mean.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n        visual_ret['fake_reg'] = util.lab2rgb(torch.cat((self.real_A.type(torch.cuda.FloatTensor), self.fake_B_reg.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n\n        visual_ret['hint'] = util.lab2rgb(torch.cat((self.real_A.type(torch.cuda.FloatTensor), self.hint_B.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n\n        visual_ret['real_ab'] = util.lab2rgb(torch.cat((torch.zeros_like(self.real_A.type(torch.cuda.FloatTensor)), self.real_B.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n\n        visual_ret['fake_ab_max'] = util.lab2rgb(torch.cat((torch.zeros_like(self.real_A.type(torch.cuda.FloatTensor)), self.fake_B_dec_max.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n        visual_ret['fake_ab_mean'] = util.lab2rgb(torch.cat((torch.zeros_like(self.real_A.type(torch.cuda.FloatTensor)), self.fake_B_dec_mean.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n        visual_ret['fake_ab_reg'] = util.lab2rgb(torch.cat((torch.zeros_like(self.real_A.type(torch.cuda.FloatTensor)), self.fake_B_reg.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n\n        visual_ret['mask'] = self.mask_B_nc.expand(-1, 3, -1, -1).type(torch.cuda.FloatTensor)\n        visual_ret['hint_ab'] = visual_ret['mask'] * util.lab2rgb(torch.cat((torch.zeros_like(self.real_A.type(torch.cuda.FloatTensor)), self.hint_B.type(torch.cuda.FloatTensor)), dim=1), self.opt)\n\n        C = self.fake_B_distr.shape[1]\n        # scale to [-1, 2], then clamped to [-1, 1]\n        visual_ret['fake_entr'] = torch.clamp(3 * self.fake_B_entr.expand(-1, 3, -1, -1) / np.log(C) - 1, -1, 1)\n\n        return visual_ret\n\n    # return training losses/errors. train.py will print out these errors as debugging information\n    def get_current_losses(self):\n        self.error_cnt += 1\n        errors_ret = OrderedDict()\n        for name in self.loss_names:\n            if isinstance(name, str):\n                # float(...) works for both scalar tensor and float number\n                self.avg_losses[name] = float(getattr(self, 'loss_' + name)) + self.avg_loss_alpha * self.avg_losses[name]\n                errors_ret[name] = (1 - self.avg_loss_alpha) / (1 - self.avg_loss_alpha**self.error_cnt) * self.avg_losses[name]\n\n        # errors_ret['|ab|_gt'] = float(torch.mean(torch.abs(self.real_B[:,1:,:,:])).cpu())\n        # errors_ret['|ab|_pr'] = float(torch.mean(torch.abs(self.fake_B[:,1:,:,:])).cpu())\n\n        return errors_ret\n"""
options/__init__.py,0,b''
options/base_options.py,1,"b'import argparse\nimport os\nfrom util import util\nimport torch\nimport models\nimport data\n\n\nclass BaseOptions():\n    def __init__(self):\n        self.initialized = False\n\n    def initialize(self, parser):\n        parser.add_argument(\'--batch_size\', type=int, default=25, help=\'input batch size\')\n        parser.add_argument(\'--loadSize\', type=int, default=256, help=\'scale images to this size\')\n        parser.add_argument(\'--fineSize\', type=int, default=176, help=\'then crop to this size\')\n        parser.add_argument(\'--input_nc\', type=int, default=1, help=\'# of input image channels\')\n        parser.add_argument(\'--output_nc\', type=int, default=2, help=\'# of output image channels\')\n        parser.add_argument(\'--ngf\', type=int, default=64, help=\'# of gen filters in first conv layer\')\n        parser.add_argument(\'--ndf\', type=int, default=64, help=\'# of discrim filters in first conv layer\')\n        parser.add_argument(\'--which_model_netD\', type=str, default=\'basic\', help=\'selects model to use for netD\')\n        parser.add_argument(\'--which_model_netG\', type=str, default=\'siggraph\', help=\'selects model to use for netG\')\n        parser.add_argument(\'--n_layers_D\', type=int, default=3, help=\'only used if which_model_netD==n_layers\')\n        parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        parser.add_argument(\'--name\', type=str, default=\'experiment_name\', help=\'name of the experiment. It decides where to store samples and models\')\n        parser.add_argument(\'--dataset_mode\', type=str, default=\'aligned\', help=\'chooses how datasets are loaded. [unaligned | aligned | single]\')\n        parser.add_argument(\'--model\', type=str, default=\'pix2pix\',\n                            help=\'chooses which model to use. cycle_gan, pix2pix, test\')\n        parser.add_argument(\'--which_direction\', type=str, default=\'AtoB\', help=\'AtoB or BtoA\')\n        parser.add_argument(\'--num_threads\', default=4, type=int, help=\'# threads for loading data\')\n        parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n        parser.add_argument(\'--norm\', type=str, default=\'batch\', help=\'instance normalization or batch normalization\')\n        parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n        parser.add_argument(\'--display_winsize\', type=int, default=256, help=\'display window size\')\n        parser.add_argument(\'--display_id\', type=int, default=1, help=\'window id of the web display\')\n        parser.add_argument(\'--display_server\', type=str, default=""http://localhost"", help=\'visdom server of the web display\')\n        parser.add_argument(\'--display_port\', type=int, default=8097, help=\'visdom port of the web display\')\n        parser.add_argument(\'--no_dropout\', action=\'store_true\', help=\'no dropout for the generator\')\n        parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""),\n                            help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n        parser.add_argument(\'--resize_or_crop\', type=str, default=\'resize_and_crop\', help=\'scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop]\')\n        parser.add_argument(\'--no_flip\', action=\'store_true\', help=\'if specified, do not flip the images for data augmentation\')\n        parser.add_argument(\'--init_type\', type=str, default=\'normal\', help=\'network initialization [normal|xavier|kaiming|orthogonal]\')\n        parser.add_argument(\'--verbose\', action=\'store_true\', help=\'if specified, print more debugging information\')\n        parser.add_argument(\'--suffix\', default=\'\', type=str, help=\'customized suffix: opt.name = opt.name + suffix: e.g., {model}_{which_model_netG}_size{loadSize}\')\n        parser.add_argument(\'--ab_norm\', type=float, default=110., help=\'colorization normalization factor\')\n        parser.add_argument(\'--ab_max\', type=float, default=110., help=\'maximimum ab value\')\n        parser.add_argument(\'--ab_quant\', type=float, default=10., help=\'quantization factor\')\n        parser.add_argument(\'--l_norm\', type=float, default=100., help=\'colorization normalization factor\')\n        parser.add_argument(\'--l_cent\', type=float, default=50., help=\'colorization centering factor\')\n        parser.add_argument(\'--mask_cent\', type=float, default=.5, help=\'mask centering factor\')\n        parser.add_argument(\'--sample_p\', type=float, default=1.0, help=\'sampling geometric distribution, 1.0 means no hints\')\n        parser.add_argument(\'--sample_Ps\', type=int, nargs=\'+\', default=[1, 2, 3, 4, 5, 6, 7, 8, 9, ], help=\'patch sizes\')\n\n        parser.add_argument(\'--results_dir\', type=str, default=\'./results/\', help=\'saves results here.\')\n        parser.add_argument(\'--classification\', action=\'store_true\', help=\'backprop trunk using classification, otherwise use regression\')\n        parser.add_argument(\'--phase\', type=str, default=\'val\', help=\'train_small, train, val, test, etc\')\n        parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        parser.add_argument(\'--how_many\', type=int, default=200, help=\'how many test images to run\')\n        parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'aspect ratio of result images\')\n\n        parser.add_argument(\'--load_model\', action=\'store_true\', help=\'load the latest model\')\n        parser.add_argument(\'--half\', action=\'store_true\', help=\'half precision model\')\n\n        self.initialized = True\n        return parser\n\n    def gather_options(self):\n        # initialize parser with basic options\n        if not self.initialized:\n            parser = argparse.ArgumentParser(\n                formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n            parser = self.initialize(parser)\n\n        # get the basic options\n        opt, _ = parser.parse_known_args()\n\n        # modify model-related parser options\n        model_name = opt.model\n        model_option_setter = models.get_option_setter(model_name)\n        parser = model_option_setter(parser, self.isTrain)\n        opt, _ = parser.parse_known_args()  # parse again with the new defaults\n\n        # modify dataset-related parser options\n        dataset_name = opt.dataset_mode\n        dataset_option_setter = data.get_option_setter(dataset_name)\n        parser = dataset_option_setter(parser, self.isTrain)\n\n        self.parser = parser\n\n        return parser.parse_args()\n\n    def print_options(self, opt):\n        message = \'\'\n        message += \'----------------- Options ---------------\\n\'\n        for k, v in sorted(vars(opt).items()):\n            comment = \'\'\n            default = self.parser.get_default(k)\n            if v != default:\n                comment = \'\\t[default: %s]\' % str(default)\n            message += \'{:>25}: {:<30}{}\\n\'.format(str(k), str(v), comment)\n        message += \'----------------- End -------------------\'\n        print(message)\n\n        # save to the disk\n        expr_dir = os.path.join(opt.checkpoints_dir, opt.name)\n        util.mkdirs(expr_dir)\n        file_name = os.path.join(expr_dir, \'opt.txt\')\n        with open(file_name, \'wt\') as opt_file:\n            opt_file.write(message)\n            opt_file.write(\'\\n\')\n\n    def parse(self):\n\n        opt = self.gather_options()\n        opt.isTrain = self.isTrain   # train or test\n\n        # process opt.suffix\n        if opt.suffix:\n            suffix = (\'_\' + opt.suffix.format(**vars(opt))) if opt.suffix != \'\' else \'\'\n            opt.name = opt.name + suffix\n\n        self.print_options(opt)\n\n        # set gpu ids\n        str_ids = opt.gpu_ids.split(\',\')\n        opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                opt.gpu_ids.append(id)\n        if len(opt.gpu_ids) > 0:\n            torch.cuda.set_device(opt.gpu_ids[0])\n        opt.A = 2 * opt.ab_max / opt.ab_quant + 1\n        opt.B = opt.A\n\n        self.opt = opt\n        return self.opt\n'"
options/train_options.py,0,"b""from .base_options import BaseOptions\n\n\nclass TrainOptions(BaseOptions):\n    def initialize(self, parser):\n        BaseOptions.initialize(self, parser)\n        parser.add_argument('--display_freq', type=int, default=10000, help='frequency of showing training results on screen')\n        parser.add_argument('--display_ncols', type=int, default=5, help='if positive, display all images in a single visdom web panel with certain number of images per row.')\n        parser.add_argument('--update_html_freq', type=int, default=10000, help='frequency of saving training results to html')\n        parser.add_argument('--print_freq', type=int, default=200, help='frequency of showing training results on console')\n        parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n        parser.add_argument('--save_epoch_freq', type=int, default=1, help='frequency of saving checkpoints at the end of epochs')\n        parser.add_argument('--epoch_count', type=int, default=0, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n        parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n        parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n        parser.add_argument('--beta1', type=float, default=0.9, help='momentum term of adam')\n        parser.add_argument('--lr', type=float, default=0.0001, help='initial learning rate for adam')\n        parser.add_argument('--no_lsgan', action='store_true', help='do *not* use least square GAN, if false, use vanilla GAN')\n        parser.add_argument('--lambda_GAN', type=float, default=0., help='weight for GAN loss')\n        parser.add_argument('--lambda_A', type=float, default=1., help='weight for cycle loss (A -> B -> A)')\n        parser.add_argument('--lambda_B', type=float, default=1., help='weight for cycle loss (B -> A -> B)')\n        parser.add_argument('--lambda_identity', type=float, default=0.5,\n                            help='use identity mapping. Setting lambda_identity other than 0 has an effect of scaling the weight of the identity mapping loss.'\n                            'For example, if the weight of the identity loss should be 10 times smaller than the weight of the reconstruction loss, please set lambda_identity = 0.1')\n        parser.add_argument('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n        parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n        parser.add_argument('--lr_policy', type=str, default='lambda', help='learning rate policy: lambda|step|plateau')\n        parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n        parser.add_argument('--avg_loss_alpha', type=float, default=.986, help='exponential averaging weight for displaying loss')\n        self.isTrain = True\n        return parser\n"""
util/__init__.py,0,b''
util/get_data.py,0,"b'from __future__ import print_function\nimport os\nimport tarfile\nimport requests\nfrom warnings import warn\nfrom zipfile import ZipFile\nfrom bs4 import BeautifulSoup\nfrom os.path import abspath, isdir, join, basename\n\n\nclass GetData(object):\n    """"""\n\n    Download CycleGAN or Pix2Pix Data.\n\n    Args:\n        technique : str\n            One of: \'cyclegan\' or \'pix2pix\'.\n        verbose : bool\n            If True, print additional information.\n\n    Examples:\n        >>> from util.get_data import GetData\n        >>> gd = GetData(technique=\'cyclegan\')\n        >>> new_data_path = gd.get(save_path=\'./datasets\')  # options will be displayed.\n\n    """"""\n\n    def __init__(self, technique=\'cyclegan\', verbose=True):\n        url_dict = {\n            \'pix2pix\': \'https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets\',\n            \'cyclegan\': \'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets\'\n        }\n        self.url = url_dict.get(technique.lower())\n        self._verbose = verbose\n\n    def _print(self, text):\n        if self._verbose:\n            print(text)\n\n    @staticmethod\n    def _get_options(r):\n        soup = BeautifulSoup(r.text, \'lxml\')\n        options = [h.text for h in soup.find_all(\'a\', href=True)\n                   if h.text.endswith((\'.zip\', \'tar.gz\'))]\n        return options\n\n    def _present_options(self):\n        r = requests.get(self.url)\n        options = self._get_options(r)\n        print(\'Options:\\n\')\n        for i, o in enumerate(options):\n            print(""{0}: {1}"".format(i, o))\n        choice = input(""\\nPlease enter the number of the ""\n                       ""dataset above you wish to download:"")\n        return options[int(choice)]\n\n    def _download_data(self, dataset_url, save_path):\n        if not isdir(save_path):\n            os.makedirs(save_path)\n\n        base = basename(dataset_url)\n        temp_save_path = join(save_path, base)\n\n        with open(temp_save_path, ""wb"") as f:\n            r = requests.get(dataset_url)\n            f.write(r.content)\n\n        if base.endswith(\'.tar.gz\'):\n            obj = tarfile.open(temp_save_path)\n        elif base.endswith(\'.zip\'):\n            obj = ZipFile(temp_save_path, \'r\')\n        else:\n            raise ValueError(""Unknown File Type: {0}."".format(base))\n\n        self._print(""Unpacking Data..."")\n        obj.extractall(save_path)\n        obj.close()\n        os.remove(temp_save_path)\n\n    def get(self, save_path, dataset=None):\n        """"""\n\n        Download a dataset.\n\n        Args:\n            save_path : str\n                A directory to save the data to.\n            dataset : str, optional\n                A specific dataset to download.\n                Note: this must include the file extension.\n                If None, options will be presented for you\n                to choose from.\n\n        Returns:\n            save_path_full : str\n                The absolute path to the downloaded data.\n\n        """"""\n        if dataset is None:\n            selected_dataset = self._present_options()\n        else:\n            selected_dataset = dataset\n\n        save_path_full = join(save_path, selected_dataset.split(\'.\')[0])\n\n        if isdir(save_path_full):\n            warn(""\\n\'{0}\' already exists. Voiding Download."".format(\n                save_path_full))\n        else:\n            self._print(\'Downloading Data...\')\n            url = ""{0}/{1}"".format(self.url, selected_dataset)\n            self._download_data(url, save_path=save_path)\n\n        return abspath(save_path_full)\n'"
util/html.py,0,"b'import dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, reflesh=0):\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n        # print(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if reflesh > 0:\n            with self.doc.head:\n                meta(http_equiv=""reflesh"", content=str(reflesh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=400):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % width, src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.png\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.png\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
util/image_pool.py,2,"b'import random\nimport torch\n\n\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return images\n        return_images = []\n        for image in images:\n            image = torch.unsqueeze(image.data, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        return_images = torch.cat(return_images, 0)\n        return return_images\n'"
util/util.py,42,"b""from __future__ import print_function\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport os\nfrom collections import OrderedDict\nfrom IPython import embed\n\n# Converts a Tensor into an image array (numpy)\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(input_image, imtype=np.uint8):\n    if isinstance(input_image, torch.Tensor):\n        image_tensor = input_image.data\n    else:\n        return input_image\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    if image_numpy.shape[0] == 1:\n        image_numpy = np.tile(image_numpy, (3, 1, 1))\n    image_numpy = np.clip((np.transpose(image_numpy, (1, 2, 0)) ),0, 1) * 255.0\n    return image_numpy.astype(imtype)\n\n\ndef diagnose_network(net, name='network'):\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print(name)\n    print(mean)\n\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\n\ndef print_numpy(x, val=True, shp=False):\n    x = x.astype(np.float64)\n    if shp:\n        print('shape,', x.shape)\n    if val:\n        x = x.flatten()\n        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (\n            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef get_subset_dict(in_dict,keys):\n    if(len(keys)):\n        subset = OrderedDict()\n        for key in keys:\n            subset[key] = in_dict[key]\n    else:\n        subset = in_dict\n    return subset\n\n\n\n# Color conversion code\ndef rgb2xyz(rgb): # rgb from [0,1]\n    # xyz_from_rgb = np.array([[0.412453, 0.357580, 0.180423],\n        # [0.212671, 0.715160, 0.072169],\n        # [0.019334, 0.119193, 0.950227]])\n\n    mask = (rgb > .04045).type(torch.FloatTensor)\n    if(rgb.is_cuda):\n        mask = mask.cuda()\n\n    rgb = (((rgb+.055)/1.055)**2.4)*mask + rgb/12.92*(1-mask)\n\n    x = .412453*rgb[:,0,:,:]+.357580*rgb[:,1,:,:]+.180423*rgb[:,2,:,:]\n    y = .212671*rgb[:,0,:,:]+.715160*rgb[:,1,:,:]+.072169*rgb[:,2,:,:]\n    z = .019334*rgb[:,0,:,:]+.119193*rgb[:,1,:,:]+.950227*rgb[:,2,:,:]\n    out = torch.cat((x[:,None,:,:],y[:,None,:,:],z[:,None,:,:]),dim=1)\n\n    # if(torch.sum(torch.isnan(out))>0):\n        # print('rgb2xyz')\n        # embed()\n    return out\n\ndef xyz2rgb(xyz):\n    # array([[ 3.24048134, -1.53715152, -0.49853633],\n    #        [-0.96925495,  1.87599   ,  0.04155593],\n    #        [ 0.05564664, -0.20404134,  1.05731107]])\n\n    r = 3.24048134*xyz[:,0,:,:]-1.53715152*xyz[:,1,:,:]-0.49853633*xyz[:,2,:,:]\n    g = -0.96925495*xyz[:,0,:,:]+1.87599*xyz[:,1,:,:]+.04155593*xyz[:,2,:,:]\n    b = .05564664*xyz[:,0,:,:]-.20404134*xyz[:,1,:,:]+1.05731107*xyz[:,2,:,:]\n\n    rgb = torch.cat((r[:,None,:,:],g[:,None,:,:],b[:,None,:,:]),dim=1)\n    rgb = torch.max(rgb,torch.zeros_like(rgb)) # sometimes reaches a small negative number, which causes NaNs\n\n    mask = (rgb > .0031308).type(torch.FloatTensor)\n    if(rgb.is_cuda):\n        mask = mask.cuda()\n\n    rgb = (1.055*(rgb**(1./2.4)) - 0.055)*mask + 12.92*rgb*(1-mask)\n\n    # if(torch.sum(torch.isnan(rgb))>0):\n        # print('xyz2rgb')\n        # embed()\n    return rgb\n\ndef xyz2lab(xyz):\n    # 0.95047, 1., 1.08883 # white\n    sc = torch.Tensor((0.95047, 1., 1.08883))[None,:,None,None]\n    if(xyz.is_cuda):\n        sc = sc.cuda()\n\n    xyz_scale = xyz/sc\n\n    mask = (xyz_scale > .008856).type(torch.FloatTensor)\n    if(xyz_scale.is_cuda):\n        mask = mask.cuda()\n\n    xyz_int = xyz_scale**(1/3.)*mask + (7.787*xyz_scale + 16./116.)*(1-mask)\n\n    L = 116.*xyz_int[:,1,:,:]-16.\n    a = 500.*(xyz_int[:,0,:,:]-xyz_int[:,1,:,:])\n    b = 200.*(xyz_int[:,1,:,:]-xyz_int[:,2,:,:])\n    out = torch.cat((L[:,None,:,:],a[:,None,:,:],b[:,None,:,:]),dim=1)\n\n    # if(torch.sum(torch.isnan(out))>0):\n        # print('xyz2lab')\n        # embed()\n\n    return out\n\ndef lab2xyz(lab):\n    y_int = (lab[:,0,:,:]+16.)/116.\n    x_int = (lab[:,1,:,:]/500.) + y_int\n    z_int = y_int - (lab[:,2,:,:]/200.)\n    if(z_int.is_cuda):\n        z_int = torch.max(torch.Tensor((0,)).cuda(), z_int)\n    else:\n        z_int = torch.max(torch.Tensor((0,)), z_int)\n\n    out = torch.cat((x_int[:,None,:,:],y_int[:,None,:,:],z_int[:,None,:,:]),dim=1)\n    mask = (out > .2068966).type(torch.FloatTensor)\n    if(out.is_cuda):\n        mask = mask.cuda()\n\n    out = (out**3.)*mask + (out - 16./116.)/7.787*(1-mask)\n\n    sc = torch.Tensor((0.95047, 1., 1.08883))[None,:,None,None]\n    sc = sc.to(out.device)\n\n    out = out*sc\n\n    # if(torch.sum(torch.isnan(out))>0):\n        # print('lab2xyz')\n        # embed()\n\n    return out\n\ndef rgb2lab(rgb, opt):\n    lab = xyz2lab(rgb2xyz(rgb))\n    l_rs = (lab[:,[0],:,:]-opt.l_cent)/opt.l_norm\n    ab_rs = lab[:,1:,:,:]/opt.ab_norm\n    out = torch.cat((l_rs,ab_rs),dim=1)\n    # if(torch.sum(torch.isnan(out))>0):\n        # print('rgb2lab')\n        # embed()\n    return out\n\ndef lab2rgb(lab_rs, opt):\n    l = lab_rs[:,[0],:,:]*opt.l_norm + opt.l_cent\n    ab = lab_rs[:,1:,:,:]*opt.ab_norm\n    lab = torch.cat((l,ab),dim=1)\n    out = xyz2rgb(lab2xyz(lab))\n    # if(torch.sum(torch.isnan(out))>0):\n        # print('lab2rgb')\n        # embed()\n    return out\n\ndef get_colorization_data(data_raw, opt, ab_thresh=5., p=.125, num_points=None):\n    data = {}\n\n    data_lab = rgb2lab(data_raw[0], opt)\n    data['A'] = data_lab[:,[0,],:,:]\n    data['B'] = data_lab[:,1:,:,:]\n\n    if(ab_thresh > 0): # mask out grayscale images\n        thresh = 1.*ab_thresh/opt.ab_norm\n        mask = torch.sum(torch.abs(torch.max(torch.max(data['B'],dim=3)[0],dim=2)[0]-torch.min(torch.min(data['B'],dim=3)[0],dim=2)[0]),dim=1) >= thresh\n        data['A'] = data['A'][mask,:,:,:]\n        data['B'] = data['B'][mask,:,:,:]\n        # print('Removed %i points'%torch.sum(mask==0).numpy())\n        if(torch.sum(mask)==0):\n            return None\n\n    return add_color_patches_rand_gt(data, opt, p=p, num_points=num_points)\n\ndef add_color_patches_rand_gt(data,opt,p=.125,num_points=None,use_avg=True,samp='normal'):\n# Add random color points sampled from ground truth based on:\n#   Number of points\n#   - if num_points is 0, then sample from geometric distribution, drawn from probability p\n#   - if num_points > 0, then sample that number of points\n#   Location of points\n#   - if samp is 'normal', draw from N(0.5, 0.25) of image\n#   - otherwise, draw from U[0, 1] of image\n    N,C,H,W = data['B'].shape\n\n    data['hint_B'] = torch.zeros_like(data['B'])\n    data['mask_B'] = torch.zeros_like(data['A'])\n\n    for nn in range(N):\n        pp = 0\n        cont_cond = True\n        while(cont_cond):\n            if(num_points is None): # draw from geometric\n                # embed()\n                cont_cond = np.random.rand() < (1-p)\n            else: # add certain number of points\n                cont_cond = pp < num_points\n            if(not cont_cond): # skip out of loop if condition not met\n                continue\n\n            P = np.random.choice(opt.sample_Ps) # patch size\n\n            # sample location\n            if(samp=='normal'): # geometric distribution\n                h = int(np.clip(np.random.normal( (H-P+1)/2., (H-P+1)/4.), 0, H-P))\n                w = int(np.clip(np.random.normal( (W-P+1)/2., (W-P+1)/4.), 0, W-P))\n            else: # uniform distribution\n                h = np.random.randint(H-P+1)\n                w = np.random.randint(W-P+1)\n\n            # add color point\n            if(use_avg):\n                # embed()\n                data['hint_B'][nn,:,h:h+P,w:w+P] = torch.mean(torch.mean(data['B'][nn,:,h:h+P,w:w+P],dim=2,keepdim=True),dim=1,keepdim=True).view(1,C,1,1)\n            else:\n                data['hint_B'][nn,:,h:h+P,w:w+P] = data['B'][nn,:,h:h+P,w:w+P]\n\n            data['mask_B'][nn,:,h:h+P,w:w+P] = 1\n\n            # increment counter\n            pp+=1\n\n    data['mask_B']-=opt.mask_cent\n\n    return data\n\ndef add_color_patch(data,mask,opt,P=1,hw=[128,128],ab=[0,0]):\n    # Add a color patch at (h,w) with color (a,b)\n    data[:,0,hw[0]:hw[0]+P,hw[1]:hw[1]+P] = 1.*ab[0]/opt.ab_norm\n    data[:,1,hw[0]:hw[0]+P,hw[1]:hw[1]+P] = 1.*ab[1]/opt.ab_norm\n    mask[:,:,hw[0]:hw[0]+P,hw[1]:hw[1]+P] = 1-opt.mask_cent\n\n    return (data,mask)\n\ndef crop_mult(data,mult=16,HWmax=[800,1200]):\n    # crop image to a multiple\n    H,W = data.shape[2:]\n    Hnew = int(min(H/mult*mult,HWmax[0]))\n    Wnew = int(min(W/mult*mult,HWmax[1]))\n    h = (H-Hnew)/2\n    w = (W-Wnew)/2\n\n    return data[:,:,h:h+Hnew,w:w+Wnew]\n\ndef encode_ab_ind(data_ab, opt):\n    # Encode ab value into an index\n    # INPUTS\n    #   data_ab   Nx2xHxW \\in [-1,1]\n    # OUTPUTS\n    #   data_q    Nx1xHxW \\in [0,Q)\n\n    data_ab_rs = torch.round((data_ab*opt.ab_norm + opt.ab_max)/opt.ab_quant) # normalized bin number\n    data_q = data_ab_rs[:,[0],:,:]*opt.A + data_ab_rs[:,[1],:,:]\n    return data_q\n\ndef decode_ind_ab(data_q, opt):\n    # Decode index into ab value\n    # INPUTS\n    #   data_q      Nx1xHxW \\in [0,Q)\n    # OUTPUTS\n    #   data_ab     Nx2xHxW \\in [-1,1]\n\n    data_a = data_q/opt.A\n    data_b = data_q - data_a*opt.A\n    data_ab = torch.cat((data_a,data_b),dim=1)\n\n    if(data_q.is_cuda):\n        type_out = torch.cuda.FloatTensor\n    else:\n        type_out = torch.FloatTensor\n    data_ab = ((data_ab.type(type_out)*opt.ab_quant) - opt.ab_max)/opt.ab_norm\n\n    return data_ab\n\ndef decode_max_ab(data_ab_quant, opt):\n    # Decode probability distribution by using bin with highest probability\n    # INPUTS\n    #   data_ab_quant   NxQxHxW \\in [0,1]\n    # OUTPUTS\n    #   data_ab         Nx2xHxW \\in [-1,1]\n\n    data_q = torch.argmax(data_ab_quant,dim=1)[:,None,:,:]\n    return decode_ind_ab(data_q, opt)\n\ndef decode_mean(data_ab_quant, opt):\n    # Decode probability distribution by taking mean over all bins\n    # INPUTS\n    #   data_ab_quant   NxQxHxW \\in [0,1]\n    # OUTPUTS\n    #   data_ab_inf     Nx2xHxW \\in [-1,1]\n\n    (N,Q,H,W) = data_ab_quant.shape\n    a_range = torch.range(-opt.ab_max, opt.ab_max, step=opt.ab_quant).to(data_ab_quant.device)[None,:,None,None]\n    a_range = a_range.type(data_ab_quant.type())\n\n    # reshape to AB space\n    data_ab_quant = data_ab_quant.view((N,int(opt.A),int(opt.A),H,W))\n    data_a_total = torch.sum(data_ab_quant,dim=2)\n    data_b_total = torch.sum(data_ab_quant,dim=1)\n\n    # matrix multiply\n    data_a_inf = torch.sum(data_a_total * a_range,dim=1,keepdim=True)\n    data_b_inf = torch.sum(data_b_total * a_range,dim=1,keepdim=True)\n\n    data_ab_inf = torch.cat((data_a_inf,data_b_inf),dim=1)/opt.ab_norm\n\n    return data_ab_inf\n\ndef calculate_psnr_np(img1, img2):\n    import numpy as np\n    SE_map = (1.*img1-img2)**2\n    cur_MSE = np.mean(SE_map)\n    return 20*np.log10(255./np.sqrt(cur_MSE))\n\ndef calculate_psnr_torch(img1, img2):\n    SE_map = (1.*img1-img2)**2\n    cur_MSE = torch.mean(SE_map)\n    return 20*torch.log10(1./torch.sqrt(cur_MSE))\n"""
util/visualizer.py,0,"b'import numpy as np\nimport os\nimport ntpath\nimport time\nfrom . import util\nfrom . import html\nfrom scipy.misc import imresize\n\n\n# save image to the disk\ndef save_images(webpage, visuals, image_path, aspect_ratio=1.0, width=256):\n    image_dir = webpage.get_image_dir()\n    short_path = ntpath.basename(image_path[0])\n    name = os.path.splitext(short_path)[0]\n\n    webpage.add_header(name)\n    ims, txts, links = [], [], []\n\n    for label, im_data in visuals.items():\n        im = util.tensor2im(im_data)\n        image_name = \'%s_%s.png\' % (name, label)\n        save_path = os.path.join(image_dir, image_name)\n        h, w, _ = im.shape\n        if aspect_ratio > 1.0:\n            im = imresize(im, (h, int(w * aspect_ratio)), interp=\'bicubic\')\n        if aspect_ratio < 1.0:\n            im = imresize(im, (int(h / aspect_ratio), w), interp=\'bicubic\')\n        util.save_image(im, save_path)\n\n        ims.append(image_name)\n        txts.append(label)\n        links.append(image_name)\n    webpage.add_images(ims, txts, links, width=width)\n\n\nclass Visualizer():\n    def __init__(self, opt):\n        self.display_id = opt.display_id\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        self.opt = opt\n        self.saved = False\n        if self.display_id > 0:\n            import visdom\n            self.ncols = opt.display_ncols\n            self.vis = visdom.Visdom(server=opt.display_server, port=opt.display_port)\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, \'web\')\n            self.img_dir = os.path.join(self.web_dir, \'images\')\n            print(\'create web directory %s...\' % self.web_dir)\n            util.mkdirs([self.web_dir, self.img_dir])\n        self.log_name = os.path.join(opt.checkpoints_dir, opt.name, \'loss_log.txt\')\n        with open(self.log_name, ""a"") as log_file:\n            now = time.strftime(""%c"")\n            log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n\n    def reset(self):\n        self.saved = False\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch, save_result):\n        if self.display_id > 0:  # show images in the browser\n            ncols = self.ncols\n            if ncols > 0:\n                ncols = min(ncols, len(visuals))\n                h, w = next(iter(visuals.values())).shape[:2]\n                table_css = """"""<style>\n                        table {border-collapse: separate; border-spacing:4px; white-space:nowrap; text-align:center}\n                        table td {width: %dpx; height: %dpx; padding: 4px; outline: 4px solid black}\n                        </style>"""""" % (w, h)\n                title = self.name\n                label_html = \'\'\n                label_html_row = \'\'\n                images = []\n                idx = 0\n                for label, image in visuals.items():\n                    image_numpy = util.tensor2im(image)\n                    label_html_row += \'<td>%s</td>\' % label\n                    images.append(image_numpy.transpose([2, 0, 1]))\n                    idx += 1\n                    if idx % ncols == 0:\n                        label_html += \'<tr>%s</tr>\' % label_html_row\n                        label_html_row = \'\'\n                white_image = np.ones_like(image_numpy.transpose([2, 0, 1])) * 255\n                while idx % ncols != 0:\n                    images.append(white_image)\n                    label_html_row += \'<td></td>\'\n                    idx += 1\n                if label_html_row != \'\':\n                    label_html += \'<tr>%s</tr>\' % label_html_row\n                # pane col = image row\n                self.vis.images(images, nrow=ncols, win=self.display_id + 1,\n                                padding=2, opts=dict(title=title + \' images\'))\n                label_html = \'<table>%s</table>\' % label_html\n                self.vis.text(table_css + label_html, win=self.display_id + 2,\n                              opts=dict(title=title + \' labels\'))\n            else:\n                idx = 1\n                for label, image in visuals.items():\n                    image_numpy = util.tensor2im(image)\n                    self.vis.image(image_numpy.transpose([2, 0, 1]), opts=dict(title=label),\n                                   win=self.display_id + idx)\n                    idx += 1\n\n        if self.use_html and (save_result or not self.saved):  # save images to a html file\n            self.saved = True\n            for label, image in visuals.items():\n                image_numpy = util.tensor2im(image)\n                img_path = os.path.join(self.img_dir, \'epoch%.3d_%s.png\' % (epoch, label))\n                util.save_image(image_numpy, img_path)\n            # update website\n            webpage = html.HTML(self.web_dir, \'Experiment name = %s\' % self.name, reflesh=1)\n            for n in range(epoch, 0, -1):\n                webpage.add_header(\'epoch [%d]\' % n)\n                ims, txts, links = [], [], []\n\n                for label, image_numpy in visuals.items():\n                    image_numpy = util.tensor2im(image)\n                    img_path = \'epoch%.3d_%s.png\' % (n, label)\n                    ims.append(img_path)\n                    txts.append(label)\n                    links.append(img_path)\n                webpage.add_images(ims, txts, links, width=self.win_size)\n            webpage.save()\n\n    # losses: dictionary of error labels and values\n    def plot_current_losses(self, epoch, counter_ratio, opt, losses):\n        if not hasattr(self, \'plot_data\'):\n            self.plot_data = {\'X\': [], \'Y\': [], \'legend\': list(losses.keys())}\n        self.plot_data[\'X\'].append(epoch + counter_ratio)\n        self.plot_data[\'Y\'].append([losses[k] for k in self.plot_data[\'legend\']])\n        self.vis.line(\n            X=np.stack([np.array(self.plot_data[\'X\'])] * len(self.plot_data[\'legend\']), 1),\n            Y=np.array(self.plot_data[\'Y\']),\n            opts={\n                \'title\': self.name + \' loss over time\',\n                \'legend\': self.plot_data[\'legend\'],\n                \'xlabel\': \'epoch\',\n                \'ylabel\': \'loss\'},\n            win=self.display_id)\n\n    # losses: same format as |losses| of plot_current_losses\n    def print_current_losses(self, epoch, i, losses, t, t_data):\n        message = \'(epoch: %d, iters: %d, time: %.3f, data: %.3f) \' % (epoch, i, t, t_data)\n        for k, v in losses.items():\n            message += \'%s: %.3f, \' % (k, v)\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n'"
