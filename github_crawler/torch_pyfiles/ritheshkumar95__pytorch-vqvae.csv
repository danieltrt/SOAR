file_path,api_count,code
datasets.py,1,"b""import os\nimport csv\nimport torch.utils.data as data\nfrom PIL import Image\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    # Borrowed from https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n    with open(path, 'rb') as f:\n        img = Image.open(f)\n        return img.convert('RGB')\n\nclass MiniImagenet(data.Dataset):\n\n    base_folder = '/data/lisa/data/miniimagenet'\n    filename = 'miniimagenet.zip'\n    splits = {\n        'train': 'train.csv',\n        'valid': 'val.csv',\n        'test': 'test.csv'\n    }\n\n    def __init__(self, root, train=False, valid=False, test=False,\n                 transform=None, target_transform=None, download=False):\n        super(MiniImagenet, self).__init__()\n        self.root = root\n        self.train = train\n        self.valid = valid\n        self.test = test\n        self.transform = transform\n        self.target_transform = target_transform\n\n        if not (((train ^ valid ^ test) ^ (train & valid & test))):\n            raise ValueError('One and only one of `train`, `valid` or `test` '\n                'must be True (train={0}, valid={1}, test={2}).'.format(train,\n                valid, test))\n\n        self.image_folder = os.path.join(os.path.expanduser(root), 'images')\n        if train:\n            split = self.splits['train']\n        elif valid:\n            split = self.splits['valid']\n        elif test:\n            split = self.splits['test']\n        else:\n            raise ValueError('Unknown split.')\n        self.split_filename = os.path.join(os.path.expanduser(root), split)\n        if download:\n            self.download()\n        if not self._check_exists():\n            raise RuntimeError('Dataset not found. You can use `download=True` '\n                               'to download it')\n\n        # Extract filenames and labels\n        self._data = []\n        with open(self.split_filename, 'r') as f:\n            reader = csv.reader(f)\n            next(reader) # Skip the header\n            for line in reader:\n                self._data.append(tuple(line))\n        self._fit_label_encoding()\n\n    def __getitem__(self, index):\n        filename, label = self._data[index]\n        image = pil_loader(os.path.join(self.image_folder, filename))\n        label = self._label_encoder[label]\n        if self.transform is not None:\n            image = self.transform(image)\n        if self.target_transform is not None:\n            label = self.target_transform(label)\n\n        return image, label\n\n    def _fit_label_encoding(self):\n        _, labels = zip(*self._data)\n        unique_labels = set(labels)\n        self._label_encoder = dict((label, idx)\n            for (idx, label) in enumerate(unique_labels))\n\n    def _check_exists(self):\n        return (os.path.exists(self.image_folder) \n            and os.path.exists(self.split_filename))\n\n    def download(self):\n        from shutil import copyfile\n        from zipfile import ZipFile\n\n        # If the image folder already exists, break\n        if self._check_exists():\n            return True\n\n        # Create folder if it does not exist\n        root = os.path.expanduser(self.root)\n        if not os.path.exists(root):\n            os.makedirs(root)\n\n        # Copy the file to root\n        path_source = os.path.join(self.base_folder, self.filename)\n        path_dest = os.path.join(root, self.filename)\n        print('Copy file `{0}` to `{1}`...'.format(path_source, path_dest))\n        copyfile(path_source, path_dest)\n\n        # Extract the dataset\n        print('Extract files from `{0}`...'.format(path_dest))\n        with ZipFile(path_dest, 'r') as f:\n            f.extractall(root)\n\n        # Copy CSV files\n        for split in self.splits:\n            path_source = os.path.join(self.base_folder, self.splits[split])\n            path_dest = os.path.join(root, self.splits[split])\n            print('Copy file `{0}` to `{1}`...'.format(path_source, path_dest))\n            copyfile(path_source, path_dest)\n        print('Done!')\n\n    def __len__(self):\n        return len(self._data)\n"""
functions.py,8,"b""import torch\nfrom torch.autograd import Function\n\nclass VectorQuantization(Function):\n    @staticmethod\n    def forward(ctx, inputs, codebook):\n        with torch.no_grad():\n            embedding_size = codebook.size(1)\n            inputs_size = inputs.size()\n            inputs_flatten = inputs.view(-1, embedding_size)\n\n            codebook_sqr = torch.sum(codebook ** 2, dim=1)\n            inputs_sqr = torch.sum(inputs_flatten ** 2, dim=1, keepdim=True)\n\n            # Compute the distances to the codebook\n            distances = torch.addmm(codebook_sqr + inputs_sqr,\n                inputs_flatten, codebook.t(), alpha=-2.0, beta=1.0)\n\n            _, indices_flatten = torch.min(distances, dim=1)\n            indices = indices_flatten.view(*inputs_size[:-1])\n            ctx.mark_non_differentiable(indices)\n\n            return indices\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        raise RuntimeError('Trying to call `.grad()` on graph containing '\n            '`VectorQuantization`. The function `VectorQuantization` '\n            'is not differentiable. Use `VectorQuantizationStraightThrough` '\n            'if you want a straight-through estimator of the gradient.')\n\nclass VectorQuantizationStraightThrough(Function):\n    @staticmethod\n    def forward(ctx, inputs, codebook):\n        indices = vq(inputs, codebook)\n        indices_flatten = indices.view(-1)\n        ctx.save_for_backward(indices_flatten, codebook)\n        ctx.mark_non_differentiable(indices_flatten)\n\n        codes_flatten = torch.index_select(codebook, dim=0,\n            index=indices_flatten)\n        codes = codes_flatten.view_as(inputs)\n\n        return (codes, indices_flatten)\n\n    @staticmethod\n    def backward(ctx, grad_output, grad_indices):\n        grad_inputs, grad_codebook = None, None\n\n        if ctx.needs_input_grad[0]:\n            # Straight-through estimator\n            grad_inputs = grad_output.clone()\n        if ctx.needs_input_grad[1]:\n            # Gradient wrt. the codebook\n            indices, codebook = ctx.saved_tensors\n            embedding_size = codebook.size(1)\n\n            grad_output_flatten = (grad_output.contiguous()\n                                              .view(-1, embedding_size))\n            grad_codebook = torch.zeros_like(codebook)\n            grad_codebook.index_add_(0, indices, grad_output_flatten)\n\n        return (grad_inputs, grad_codebook)\n\nvq = VectorQuantization.apply\nvq_st = VectorQuantizationStraightThrough.apply\n__all__ = [vq, vq_st]\n"""
modules.py,8,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.distributions.normal import Normal\nfrom torch.distributions import kl_divergence\n\nfrom functions import vq, vq_st\n\ndef to_scalar(arr):\n    if type(arr) == list:\n        return [x.item() for x in arr]\n    else:\n        return arr.item()\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        try:\n            nn.init.xavier_uniform_(m.weight.data)\n            m.bias.data.fill_(0)\n        except AttributeError:\n            print(""Skipping initialization of "", classname)\n\n\nclass VAE(nn.Module):\n    def __init__(self, input_dim, dim, z_dim):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(input_dim, dim, 4, 2, 1),\n            nn.BatchNorm2d(dim),\n            nn.ReLU(True),\n            nn.Conv2d(dim, dim, 4, 2, 1),\n            nn.BatchNorm2d(dim),\n            nn.ReLU(True),\n            nn.Conv2d(dim, dim, 5, 1, 0),\n            nn.BatchNorm2d(dim),\n            nn.ReLU(True),\n            nn.Conv2d(dim, z_dim * 2, 3, 1, 0),\n            nn.BatchNorm2d(z_dim * 2)\n        )\n\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(z_dim, dim, 3, 1, 0),\n            nn.BatchNorm2d(dim),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(dim, dim, 5, 1, 0),\n            nn.BatchNorm2d(dim),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(dim, dim, 4, 2, 1),\n            nn.BatchNorm2d(dim),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(dim, input_dim, 4, 2, 1),\n            nn.Tanh()\n        )\n\n        self.apply(weights_init)\n\n    def forward(self, x):\n        mu, logvar = self.encoder(x).chunk(2, dim=1)\n\n        q_z_x = Normal(mu, logvar.mul(.5).exp())\n        p_z = Normal(torch.zeros_like(mu), torch.ones_like(logvar))\n        kl_div = kl_divergence(q_z_x, p_z).sum(1).mean()\n\n        x_tilde = self.decoder(q_z_x.rsample())\n        return x_tilde, kl_div\n\n\nclass VQEmbedding(nn.Module):\n    def __init__(self, K, D):\n        super().__init__()\n        self.embedding = nn.Embedding(K, D)\n        self.embedding.weight.data.uniform_(-1./K, 1./K)\n\n    def forward(self, z_e_x):\n        z_e_x_ = z_e_x.permute(0, 2, 3, 1).contiguous()\n        latents = vq(z_e_x_, self.embedding.weight)\n        return latents\n\n    def straight_through(self, z_e_x):\n        z_e_x_ = z_e_x.permute(0, 2, 3, 1).contiguous()\n        z_q_x_, indices = vq_st(z_e_x_, self.embedding.weight.detach())\n        z_q_x = z_q_x_.permute(0, 3, 1, 2).contiguous()\n\n        z_q_x_bar_flatten = torch.index_select(self.embedding.weight,\n            dim=0, index=indices)\n        z_q_x_bar_ = z_q_x_bar_flatten.view_as(z_e_x_)\n        z_q_x_bar = z_q_x_bar_.permute(0, 3, 1, 2).contiguous()\n\n        return z_q_x, z_q_x_bar\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.ReLU(True),\n            nn.Conv2d(dim, dim, 3, 1, 1),\n            nn.BatchNorm2d(dim),\n            nn.ReLU(True),\n            nn.Conv2d(dim, dim, 1),\n            nn.BatchNorm2d(dim)\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\n\nclass VectorQuantizedVAE(nn.Module):\n    def __init__(self, input_dim, dim, K=512):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(input_dim, dim, 4, 2, 1),\n            nn.BatchNorm2d(dim),\n            nn.ReLU(True),\n            nn.Conv2d(dim, dim, 4, 2, 1),\n            ResBlock(dim),\n            ResBlock(dim),\n        )\n\n        self.codebook = VQEmbedding(K, dim)\n\n        self.decoder = nn.Sequential(\n            ResBlock(dim),\n            ResBlock(dim),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(dim, dim, 4, 2, 1),\n            nn.BatchNorm2d(dim),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(dim, input_dim, 4, 2, 1),\n            nn.Tanh()\n        )\n\n        self.apply(weights_init)\n\n    def encode(self, x):\n        z_e_x = self.encoder(x)\n        latents = self.codebook(z_e_x)\n        return latents\n\n    def decode(self, latents):\n        z_q_x = self.codebook.embedding(latents).permute(0, 3, 1, 2)  # (B, D, H, W)\n        x_tilde = self.decoder(z_q_x)\n        return x_tilde\n\n    def forward(self, x):\n        z_e_x = self.encoder(x)\n        z_q_x_st, z_q_x = self.codebook.straight_through(z_e_x)\n        x_tilde = self.decoder(z_q_x_st)\n        return x_tilde, z_e_x, z_q_x\n\n\nclass GatedActivation(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        x, y = x.chunk(2, dim=1)\n        return F.tanh(x) * F.sigmoid(y)\n\n\nclass GatedMaskedConv2d(nn.Module):\n    def __init__(self, mask_type, dim, kernel, residual=True, n_classes=10):\n        super().__init__()\n        assert kernel % 2 == 1, print(""Kernel size must be odd"")\n        self.mask_type = mask_type\n        self.residual = residual\n\n        self.class_cond_embedding = nn.Embedding(\n            n_classes, 2 * dim\n        )\n\n        kernel_shp = (kernel // 2 + 1, kernel)  # (ceil(n/2), n)\n        padding_shp = (kernel // 2, kernel // 2)\n        self.vert_stack = nn.Conv2d(\n            dim, dim * 2,\n            kernel_shp, 1, padding_shp\n        )\n\n        self.vert_to_horiz = nn.Conv2d(2 * dim, 2 * dim, 1)\n\n        kernel_shp = (1, kernel // 2 + 1)\n        padding_shp = (0, kernel // 2)\n        self.horiz_stack = nn.Conv2d(\n            dim, dim * 2,\n            kernel_shp, 1, padding_shp\n        )\n\n        self.horiz_resid = nn.Conv2d(dim, dim, 1)\n\n        self.gate = GatedActivation()\n\n    def make_causal(self):\n        self.vert_stack.weight.data[:, :, -1].zero_()  # Mask final row\n        self.horiz_stack.weight.data[:, :, :, -1].zero_()  # Mask final column\n\n    def forward(self, x_v, x_h, h):\n        if self.mask_type == \'A\':\n            self.make_causal()\n\n        h = self.class_cond_embedding(h)\n        h_vert = self.vert_stack(x_v)\n        h_vert = h_vert[:, :, :x_v.size(-1), :]\n        out_v = self.gate(h_vert + h[:, :, None, None])\n\n        h_horiz = self.horiz_stack(x_h)\n        h_horiz = h_horiz[:, :, :, :x_h.size(-2)]\n        v2h = self.vert_to_horiz(h_vert)\n\n        out = self.gate(v2h + h_horiz + h[:, :, None, None])\n        if self.residual:\n            out_h = self.horiz_resid(out) + x_h\n        else:\n            out_h = self.horiz_resid(out)\n\n        return out_v, out_h\n\n\nclass GatedPixelCNN(nn.Module):\n    def __init__(self, input_dim=256, dim=64, n_layers=15, n_classes=10):\n        super().__init__()\n        self.dim = dim\n\n        # Create embedding layer to embed input\n        self.embedding = nn.Embedding(input_dim, dim)\n\n        # Building the PixelCNN layer by layer\n        self.layers = nn.ModuleList()\n\n        # Initial block with Mask-A convolution\n        # Rest with Mask-B convolutions\n        for i in range(n_layers):\n            mask_type = \'A\' if i == 0 else \'B\'\n            kernel = 7 if i == 0 else 3\n            residual = False if i == 0 else True\n\n            self.layers.append(\n                GatedMaskedConv2d(mask_type, dim, kernel, residual, n_classes)\n            )\n\n        # Add the output layer\n        self.output_conv = nn.Sequential(\n            nn.Conv2d(dim, 512, 1),\n            nn.ReLU(True),\n            nn.Conv2d(512, input_dim, 1)\n        )\n\n        self.apply(weights_init)\n\n    def forward(self, x, label):\n        shp = x.size() + (-1, )\n        x = self.embedding(x.view(-1)).view(shp)  # (B, H, W, C)\n        x = x.permute(0, 3, 1, 2)  # (B, C, W, W)\n\n        x_v, x_h = (x, x)\n        for i, layer in enumerate(self.layers):\n            x_v, x_h = layer(x_v, x_h, label)\n\n        return self.output_conv(x_h)\n\n    def generate(self, label, shape=(8, 8), batch_size=64):\n        param = next(self.parameters())\n        x = torch.zeros(\n            (batch_size, *shape),\n            dtype=torch.int64, device=param.device\n        )\n\n        for i in range(shape[0]):\n            for j in range(shape[1]):\n                logits = self.forward(x, label)\n                probs = F.softmax(logits[:, :, i, j], -1)\n                x.data[:, i, j].copy_(\n                    probs.multinomial(1).squeeze().data\n                )\n        return x\n'"
pixelcnn_baseline.py,7,"b'import torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\nfrom modules import GatedPixelCNN\nimport numpy as np\nfrom torchvision.utils import save_image\nimport time\n\n\nBATCH_SIZE = 32\nN_EPOCHS = 100\nPRINT_INTERVAL = 100\nALWAYS_SAVE = True\nDATASET = \'FashionMNIST\'  # CIFAR10 | MNIST | FashionMNIST\nNUM_WORKERS = 4\n\nIMAGE_SHAPE = (28, 28)  # (32, 32) | (28, 28)\nINPUT_DIM = 3  # 3 (RGB) | 1 (Grayscale)\nK = 256\nDIM = 64\nN_LAYERS = 15\nLR = 3e-4\n\n\ntrain_loader = torch.utils.data.DataLoader(\n    eval(\'datasets.\'+DATASET)(\n        \'../data/{}/\'.format(DATASET), train=True, download=True,\n        transform=transforms.ToTensor(),\n    ), batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=NUM_WORKERS, pin_memory=True\n)\ntest_loader = torch.utils.data.DataLoader(\n    eval(\'datasets.\'+DATASET)(\n        \'../data/{}/\'.format(DATASET), train=False,\n        transform=transforms.ToTensor(),\n    ), batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=NUM_WORKERS, pin_memory=True\n)\n\nmodel = GatedPixelCNN(K, DIM, N_LAYERS).cuda()\ncriterion = nn.CrossEntropyLoss().cuda()\nopt = torch.optim.Adam(model.parameters(), lr=LR)\n\n\ndef train():\n    train_loss = []\n    for batch_idx, (x, label) in enumerate(train_loader):\n        start_time = time.time()\n        x = (x[:, 0] * (K-1)).long().cuda()\n        label = label.cuda()\n\n        # Train PixelCNN with images\n        logits = model(x, label)\n        logits = logits.permute(0, 2, 3, 1).contiguous()\n\n        loss = criterion(\n            logits.view(-1, K),\n            x.view(-1)\n        )\n\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n        train_loss.append(loss.item())\n\n        if (batch_idx + 1) % PRINT_INTERVAL == 0:\n            print(\'\\tIter: [{}/{} ({:.0f}%)]\\tLoss: {} Time: {}\'.format(\n                batch_idx * len(x), len(train_loader.dataset),\n                PRINT_INTERVAL * batch_idx / len(train_loader),\n                np.asarray(train_loss)[-PRINT_INTERVAL:].mean(0),\n                time.time() - start_time\n            ))\n\n\ndef test():\n    start_time = time.time()\n    val_loss = []\n    with torch.no_grad():\n        for batch_idx, (x, label) in enumerate(test_loader):\n            x = (x[:, 0] * (K-1)).long().cuda()\n            label = label.cuda()\n\n            logits = model(x, label)\n            logits = logits.permute(0, 2, 3, 1).contiguous()\n            loss = criterion(\n                logits.view(-1, K),\n                x.view(-1)\n            )\n            val_loss.append(loss.item())\n\n    print(\'Validation Completed!\\tLoss: {} Time: {}\'.format(\n        np.asarray(val_loss).mean(0),\n        time.time() - start_time\n    ))\n    return np.asarray(val_loss).mean(0)\n\n\ndef generate_samples():\n    label = torch.arange(10).expand(10, 10).contiguous().view(-1)\n    label = label.long().cuda()\n\n    x_tilde = model.generate(label, shape=IMAGE_SHAPE, batch_size=100)\n    images = x_tilde.cpu().data.float() / (K - 1)\n\n    save_image(\n        images[:, None],\n        \'samples/pixelcnn_baseline_samples_{}.png\'.format(DATASET),\n        nrow=10\n    )\n\n\nBEST_LOSS = 999\nLAST_SAVED = -1\nfor epoch in range(1, N_EPOCHS):\n    print(""\\nEpoch {}:"".format(epoch))\n    train()\n    cur_loss = test()\n\n    if ALWAYS_SAVE or cur_loss <= BEST_LOSS:\n        BEST_LOSS = cur_loss\n        LAST_SAVED = epoch\n\n        print(""Saving model!"")\n        torch.save(model.state_dict(), \'models/{}_pixelcnn.pt\'.format(DATASET))\n    else:\n        print(""Not saving model! Last saved: {}"".format(LAST_SAVED))\n\n    generate_samples()\n'"
pixelcnn_prior.py,11,"b""import numpy as np\nimport torch\nimport torch.nn.functional as F\nimport json\nfrom torchvision import transforms\nfrom torchvision.utils import save_image, make_grid\n\nfrom modules import VectorQuantizedVAE, GatedPixelCNN\nfrom datasets import MiniImagenet\n\nfrom tensorboardX import SummaryWriter\n\ndef train(data_loader, model, prior, optimizer, args, writer):\n    for images, labels in data_loader:\n        with torch.no_grad():\n            images = images.to(args.device)\n            latents = model.encode(images)\n            latents = latents.detach()\n\n        labels = labels.to(args.device)\n        logits = prior(latents, labels)\n        logits = logits.permute(0, 2, 3, 1).contiguous()\n\n        optimizer.zero_grad()\n        loss = F.cross_entropy(logits.view(-1, args.k),\n                               latents.view(-1))\n        loss.backward()\n\n        # Logs\n        writer.add_scalar('loss/train', loss.item(), args.steps)\n\n        optimizer.step()\n        args.steps += 1\n\ndef test(data_loader, model, prior, args, writer):\n    with torch.no_grad():\n        loss = 0.\n        for images, labels in data_loader:\n            images = images.to(args.device)\n            labels = labels.to(args.device)\n\n            latents = model.encode(images)\n            latents = latents.detach()\n            logits = prior(latents, labels)\n            logits = logits.permute(0, 2, 3, 1).contiguous()\n            loss += F.cross_entropy(logits.view(-1, args.k),\n                                    latents.view(-1))\n\n        loss /= len(data_loader)\n\n    # Logs\n    writer.add_scalar('loss/valid', loss.item(), args.steps)\n\n    return loss.item()\n\ndef main(args):\n    writer = SummaryWriter('./logs/{0}'.format(args.output_folder))\n    save_filename = './models/{0}/prior.pt'.format(args.output_folder)\n\n    if args.dataset in ['mnist', 'fashion-mnist', 'cifar10']:\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        if args.dataset == 'mnist':\n            # Define the train & test datasets\n            train_dataset = datasets.MNIST(args.data_folder, train=True,\n                download=True, transform=transform)\n            test_dataset = datasets.MNIST(args.data_folder, train=False,\n                transform=transform)\n            num_channels = 1\n        elif args.dataset == 'fashion-mnist':\n            # Define the train & test datasets\n            train_dataset = datasets.FashionMNIST(args.data_folder,\n                train=True, download=True, transform=transform)\n            test_dataset = datasets.FashionMNIST(args.data_folder,\n                train=False, transform=transform)\n            num_channels = 1\n        elif args.dataset == 'cifar10':\n            # Define the train & test datasets\n            train_dataset = datasets.CIFAR10(args.data_folder,\n                train=True, download=True, transform=transform)\n            test_dataset = datasets.CIFAR10(args.data_folder,\n                train=False, transform=transform)\n            num_channels = 3\n        valid_dataset = test_dataset\n    elif args.dataset == 'miniimagenet':\n        transform = transforms.Compose([\n            transforms.RandomResizedCrop(128),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        # Define the train, valid & test datasets\n        train_dataset = MiniImagenet(args.data_folder, train=True,\n            download=True, transform=transform)\n        valid_dataset = MiniImagenet(args.data_folder, valid=True,\n            download=True, transform=transform)\n        test_dataset = MiniImagenet(args.data_folder, test=True,\n            download=True, transform=transform)\n        num_channels = 3\n\n    # Define the data loaders\n    train_loader = torch.utils.data.DataLoader(train_dataset,\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.num_workers, pin_memory=True)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n        batch_size=args.batch_size, shuffle=False, drop_last=True,\n        num_workers=args.num_workers, pin_memory=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset,\n        batch_size=16, shuffle=True)\n\n    # Save the label encoder\n    with open('./models/{0}/labels.json'.format(args.output_folder), 'w') as f:\n        json.dump(train_dataset._label_encoder, f)\n\n    # Fixed images for Tensorboard\n    fixed_images, _ = next(iter(test_loader))\n    fixed_grid = make_grid(fixed_images, nrow=8, range=(-1, 1), normalize=True)\n    writer.add_image('original', fixed_grid, 0)\n\n    model = VectorQuantizedVAE(num_channels, args.hidden_size_vae, args.k).to(args.device)\n    with open(args.model, 'rb') as f:\n        state_dict = torch.load(f)\n        model.load_state_dict(state_dict)\n    model.eval()\n\n    prior = GatedPixelCNN(args.k, args.hidden_size_prior,\n        args.num_layers, n_classes=len(train_dataset._label_encoder)).to(args.device)\n    optimizer = torch.optim.Adam(prior.parameters(), lr=args.lr)\n\n    best_loss = -1.\n    for epoch in range(args.num_epochs):\n        train(train_loader, model, prior, optimizer, args, writer)\n        # The validation loss is not properly computed since\n        # the classes in the train and valid splits of Mini-Imagenet\n        # do not overlap.\n        loss = test(valid_loader, model, prior, args, writer)\n\n        if (epoch == 0) or (loss < best_loss):\n            best_loss = loss\n            with open(save_filename, 'wb') as f:\n                torch.save(prior.state_dict(), f)\n\nif __name__ == '__main__':\n    import argparse\n    import os\n    import multiprocessing as mp\n\n    parser = argparse.ArgumentParser(description='PixelCNN Prior for VQ-VAE')\n\n    # General\n    parser.add_argument('--data-folder', type=str,\n        help='name of the data folder')\n    parser.add_argument('--dataset', type=str,\n        help='name of the dataset (mnist, fashion-mnist, cifar10, miniimagenet)')\n    parser.add_argument('--model', type=str,\n        help='filename containing the model')\n\n    # Latent space\n    parser.add_argument('--hidden-size-vae', type=int, default=256,\n        help='size of the latent vectors (default: 256)')\n    parser.add_argument('--hidden-size-prior', type=int, default=64,\n        help='hidden size for the PixelCNN prior (default: 64)')\n    parser.add_argument('--k', type=int, default=512,\n        help='number of latent vectors (default: 512)')\n    parser.add_argument('--num-layers', type=int, default=15,\n        help='number of layers for the PixelCNN prior (default: 15)')\n\n    # Optimization\n    parser.add_argument('--batch-size', type=int, default=128,\n        help='batch size (default: 128)')\n    parser.add_argument('--num-epochs', type=int, default=100,\n        help='number of epochs (default: 100)')\n    parser.add_argument('--lr', type=float, default=3e-4,\n        help='learning rate for Adam optimizer (default: 3e-4)')\n\n    # Miscellaneous\n    parser.add_argument('--output-folder', type=str, default='prior',\n        help='name of the output folder (default: prior)')\n    parser.add_argument('--num-workers', type=int, default=mp.cpu_count() - 1,\n        help='number of workers for trajectories sampling (default: {0})'.format(mp.cpu_count() - 1))\n    parser.add_argument('--device', type=str, default='cpu',\n        help='set the device (cpu or cuda, default: cpu)')\n\n    args = parser.parse_args()\n\n    # Create logs and models folder if they don't exist\n    if not os.path.exists('./logs'):\n        os.makedirs('./logs')\n    if not os.path.exists('./models'):\n        os.makedirs('./models')\n    # Device\n    args.device = torch.device(args.device\n        if torch.cuda.is_available() else 'cpu')\n    # Slurm\n    if 'SLURM_JOB_ID' in os.environ:\n        args.output_folder += '-{0}'.format(os.environ['SLURM_JOB_ID'])\n    if not os.path.exists('./models/{0}'.format(args.output_folder)):\n        os.makedirs('./models/{0}'.format(args.output_folder))\n    args.steps = 0\n\n    main(args)\n"""
test_functions.py,24,"b'import pytest\n\nimport numpy as np\nimport torch\n\nfrom functions import vq, vq_st\n\ndef test_vq_shape():\n    inputs = torch.rand((2, 3, 5, 7), dtype=torch.float32, requires_grad=True)\n    codebook = torch.rand((11, 7), dtype=torch.float32, requires_grad=True)\n    indices = vq(inputs, codebook)\n\n    assert indices.size() == (2, 3, 5)\n    assert not indices.requires_grad\n    assert indices.dtype == torch.int64\n\ndef test_vq():\n    inputs = torch.rand((2, 3, 5, 7), dtype=torch.float32, requires_grad=True)\n    codebook = torch.rand((11, 7), dtype=torch.float32, requires_grad=True)\n    indices = vq(inputs, codebook)\n\n    differences = inputs.unsqueeze(3) - codebook\n    distances = torch.norm(differences, p=2, dim=4)\n\n    _, indices_torch = torch.min(distances, dim=3)\n\n    assert np.allclose(indices.numpy(), indices_torch.numpy())\n\ndef test_vq_st_shape():\n    inputs = torch.rand((2, 3, 5, 7), dtype=torch.float32, requires_grad=True)\n    codebook = torch.rand((11, 7), dtype=torch.float32, requires_grad=True)\n    codes, indices = vq_st(inputs, codebook)\n\n    assert codes.size() == (2, 3, 5, 7)\n    assert codes.requires_grad\n    assert codes.dtype == torch.float32\n\n    assert indices.size() == (2 * 3 * 5,)\n    assert not indices.requires_grad\n    assert indices.dtype == torch.int64\n\ndef test_vq_st_gradient1():\n    inputs = torch.rand((2, 3, 5, 7), dtype=torch.float32, requires_grad=True)\n    codebook = torch.rand((11, 7), dtype=torch.float32, requires_grad=True)\n    codes, _ = vq_st(inputs, codebook)\n\n    grad_output = torch.rand((2, 3, 5, 7))\n    grad_inputs, = torch.autograd.grad(codes, inputs,\n        grad_outputs=[grad_output])\n\n    # Straight-through estimator\n    assert grad_inputs.size() == (2, 3, 5, 7)\n    assert np.allclose(grad_output.numpy(), grad_inputs.numpy())\n\ndef test_vq_st_gradient2():\n    inputs = torch.rand((2, 3, 5, 7), dtype=torch.float32, requires_grad=True)\n    codebook = torch.rand((11, 7), dtype=torch.float32, requires_grad=True)\n    codes, _ = vq_st(inputs, codebook)\n\n    indices = vq(inputs, codebook)\n    codes_torch = torch.embedding(codebook, indices, padding_idx=-1,\n        scale_grad_by_freq=False, sparse=False)\n\n    grad_output = torch.rand((2, 3, 5, 7), dtype=torch.float32)\n    grad_codebook, = torch.autograd.grad(codes, codebook,\n        grad_outputs=[grad_output])\n    grad_codebook_torch, = torch.autograd.grad(codes_torch, codebook,\n        grad_outputs=[grad_output])\n\n    # Gradient is the same as torch.embedding function\n    assert grad_codebook.size() == (11, 7)\n    assert np.allclose(grad_codebook.numpy(), grad_codebook_torch.numpy())\n'"
vae.py,10,"b'import numpy as np\nimport time\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.distributions.normal import Normal\n\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\n\nfrom modules import VAE\n\n\nBATCH_SIZE = 32\nN_EPOCHS = 100\nPRINT_INTERVAL = 500\nDATASET = \'FashionMNIST\'  # CIFAR10 | MNIST | FashionMNIST\nNUM_WORKERS = 4\n\nINPUT_DIM = 1\nDIM = 256\nZ_DIM = 128\nLR = 1e-3\n\n\npreproc_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrain_loader = torch.utils.data.DataLoader(\n    eval(\'datasets.\'+DATASET)(\n        \'../data/{}/\'.format(DATASET), train=True, download=True,\n        transform=preproc_transform,\n    ), batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=NUM_WORKERS, pin_memory=True\n)\ntest_loader = torch.utils.data.DataLoader(\n    eval(\'datasets.\'+DATASET)(\n        \'../data/{}/\'.format(DATASET), train=False,\n        transform=preproc_transform\n    ), batch_size=BATCH_SIZE, shuffle=False,\n    num_workers=NUM_WORKERS, pin_memory=True\n)\n\nmodel = VAE(INPUT_DIM, DIM, Z_DIM).cuda()\nprint(model)\nopt = torch.optim.Adam(model.parameters(), lr=LR, amsgrad=True)\n\n\ndef train():\n    train_loss = []\n    model.train()\n    for batch_idx, (x, _) in enumerate(train_loader):\n        start_time = time.time()\n        x = x.cuda()\n\n        x_tilde, kl_d = model(x)\n        loss_recons = F.mse_loss(x_tilde, x, size_average=False) / x.size(0)\n        loss = loss_recons + kl_d\n\n        nll = -Normal(x_tilde, torch.ones_like(x_tilde)).log_prob(x)\n        log_px = nll.mean().item() - np.log(128) + kl_d.item()\n        log_px /= np.log(2)\n\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n\n        train_loss.append([log_px, loss.item()])\n\n        if (batch_idx + 1) % PRINT_INTERVAL == 0:\n            print(\'\\tIter [{}/{} ({:.0f}%)]\\tLoss: {} Time: {:5.3f} ms/batch\'.format(\n                batch_idx * len(x), len(train_loader.dataset),\n                PRINT_INTERVAL * batch_idx / len(train_loader),\n                np.asarray(train_loss)[-PRINT_INTERVAL:].mean(0),\n                1000 * (time.time() - start_time)\n            ))\n\n\ndef test():\n    start_time = time.time()\n    val_loss = []\n    model.eval()\n    with torch.no_grad():\n        for batch_idx, (x, _) in enumerate(test_loader):\n            x = x.cuda()\n            x_tilde, kl_d = model(x)\n            loss_recons = F.mse_loss(x_tilde, x, size_average=False) / x.size(0)\n            loss = loss_recons + kl_d\n            val_loss.append(loss.item())\n\n    print(\'\\nValidation Completed!\\tLoss: {:5.4f} Time: {:5.3f} s\'.format(\n        np.asarray(val_loss).mean(0),\n        time.time() - start_time\n    ))\n    return np.asarray(val_loss).mean(0)\n\n\ndef generate_reconstructions():\n    model.eval()\n    x, _ = test_loader.__iter__().next()\n    x = x[:32].cuda()\n    x_tilde, kl_div = model(x)\n\n    x_cat = torch.cat([x, x_tilde], 0)\n    images = (x_cat.cpu().data + 1) / 2\n\n    save_image(\n        images,\n        \'samples/vae_reconstructions_{}.png\'.format(DATASET),\n        nrow=8\n    )\n\n\ndef generate_samples():\n    model.eval()\n    z_e_x = torch.randn(64, Z_DIM, 1, 1).cuda()\n    x_tilde = model.decoder(z_e_x)\n\n    images = (x_tilde.cpu().data + 1) / 2\n\n    save_image(\n        images,\n        \'samples/vae_samples_{}.png\'.format(DATASET),\n        nrow=8\n    )\n\n\nBEST_LOSS = 99999\nLAST_SAVED = -1\nfor epoch in range(1, N_EPOCHS):\n    print(""Epoch {}:"".format(epoch))\n    train()\n    cur_loss = test()\n\n    if cur_loss <= BEST_LOSS:\n        BEST_LOSS = cur_loss\n        LAST_SAVED = epoch\n        print(""Saving model!"")\n        torch.save(model.state_dict(), \'models/{}_vae.pt\'.format(DATASET))\n    else:\n        print(""Not saving model! Last saved: {}"".format(LAST_SAVED))\n\n    generate_reconstructions()\n    generate_samples()\n'"
vqvae.py,11,"b""import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torchvision import transforms, datasets\nfrom torchvision.utils import save_image, make_grid\n\nfrom modules import VectorQuantizedVAE, to_scalar\nfrom datasets import MiniImagenet\n\nfrom tensorboardX import SummaryWriter\n\ndef train(data_loader, model, optimizer, args, writer):\n    for images, _ in data_loader:\n        images = images.to(args.device)\n\n        optimizer.zero_grad()\n        x_tilde, z_e_x, z_q_x = model(images)\n\n        # Reconstruction loss\n        loss_recons = F.mse_loss(x_tilde, images)\n        # Vector quantization objective\n        loss_vq = F.mse_loss(z_q_x, z_e_x.detach())\n        # Commitment objective\n        loss_commit = F.mse_loss(z_e_x, z_q_x.detach())\n\n        loss = loss_recons + loss_vq + args.beta * loss_commit\n        loss.backward()\n\n        # Logs\n        writer.add_scalar('loss/train/reconstruction', loss_recons.item(), args.steps)\n        writer.add_scalar('loss/train/quantization', loss_vq.item(), args.steps)\n\n        optimizer.step()\n        args.steps += 1\n\ndef test(data_loader, model, args, writer):\n    with torch.no_grad():\n        loss_recons, loss_vq = 0., 0.\n        for images, _ in data_loader:\n            images = images.to(args.device)\n            x_tilde, z_e_x, z_q_x = model(images)\n            loss_recons += F.mse_loss(x_tilde, images)\n            loss_vq += F.mse_loss(z_q_x, z_e_x)\n\n        loss_recons /= len(data_loader)\n        loss_vq /= len(data_loader)\n\n    # Logs\n    writer.add_scalar('loss/test/reconstruction', loss_recons.item(), args.steps)\n    writer.add_scalar('loss/test/quantization', loss_vq.item(), args.steps)\n\n    return loss_recons.item(), loss_vq.item()\n\ndef generate_samples(images, model, args):\n    with torch.no_grad():\n        images = images.to(args.device)\n        x_tilde, _, _ = model(images)\n    return x_tilde\n\ndef main(args):\n    writer = SummaryWriter('./logs/{0}'.format(args.output_folder))\n    save_filename = './models/{0}'.format(args.output_folder)\n\n    if args.dataset in ['mnist', 'fashion-mnist', 'cifar10']:\n        transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        if args.dataset == 'mnist':\n            # Define the train & test datasets\n            train_dataset = datasets.MNIST(args.data_folder, train=True,\n                download=True, transform=transform)\n            test_dataset = datasets.MNIST(args.data_folder, train=False,\n                transform=transform)\n            num_channels = 1\n        elif args.dataset == 'fashion-mnist':\n            # Define the train & test datasets\n            train_dataset = datasets.FashionMNIST(args.data_folder,\n                train=True, download=True, transform=transform)\n            test_dataset = datasets.FashionMNIST(args.data_folder,\n                train=False, transform=transform)\n            num_channels = 1\n        elif args.dataset == 'cifar10':\n            # Define the train & test datasets\n            train_dataset = datasets.CIFAR10(args.data_folder,\n                train=True, download=True, transform=transform)\n            test_dataset = datasets.CIFAR10(args.data_folder,\n                train=False, transform=transform)\n            num_channels = 3\n        valid_dataset = test_dataset\n    elif args.dataset == 'miniimagenet':\n        transform = transforms.Compose([\n            transforms.RandomResizedCrop(128),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        # Define the train, valid & test datasets\n        train_dataset = MiniImagenet(args.data_folder, train=True,\n            download=True, transform=transform)\n        valid_dataset = MiniImagenet(args.data_folder, valid=True,\n            download=True, transform=transform)\n        test_dataset = MiniImagenet(args.data_folder, test=True,\n            download=True, transform=transform)\n        num_channels = 3\n\n    # Define the data loaders\n    train_loader = torch.utils.data.DataLoader(train_dataset,\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.num_workers, pin_memory=True)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n        batch_size=args.batch_size, shuffle=False, drop_last=True,\n        num_workers=args.num_workers, pin_memory=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset,\n        batch_size=16, shuffle=True)\n\n    # Fixed images for Tensorboard\n    fixed_images, _ = next(iter(test_loader))\n    fixed_grid = make_grid(fixed_images, nrow=8, range=(-1, 1), normalize=True)\n    writer.add_image('original', fixed_grid, 0)\n\n    model = VectorQuantizedVAE(num_channels, args.hidden_size, args.k).to(args.device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\n    # Generate the samples first once\n    reconstruction = generate_samples(fixed_images, model, args)\n    grid = make_grid(reconstruction.cpu(), nrow=8, range=(-1, 1), normalize=True)\n    writer.add_image('reconstruction', grid, 0)\n\n    best_loss = -1.\n    for epoch in range(args.num_epochs):\n        train(train_loader, model, optimizer, args, writer)\n        loss, _ = test(valid_loader, model, args, writer)\n\n        reconstruction = generate_samples(fixed_images, model, args)\n        grid = make_grid(reconstruction.cpu(), nrow=8, range=(-1, 1), normalize=True)\n        writer.add_image('reconstruction', grid, epoch + 1)\n\n        if (epoch == 0) or (loss < best_loss):\n            best_loss = loss\n            with open('{0}/best.pt'.format(save_filename), 'wb') as f:\n                torch.save(model.state_dict(), f)\n        with open('{0}/model_{1}.pt'.format(save_filename, epoch + 1), 'wb') as f:\n            torch.save(model.state_dict(), f)\n\nif __name__ == '__main__':\n    import argparse\n    import os\n    import multiprocessing as mp\n\n    parser = argparse.ArgumentParser(description='VQ-VAE')\n\n    # General\n    parser.add_argument('--data-folder', type=str,\n        help='name of the data folder')\n    parser.add_argument('--dataset', type=str,\n        help='name of the dataset (mnist, fashion-mnist, cifar10, miniimagenet)')\n\n    # Latent space\n    parser.add_argument('--hidden-size', type=int, default=256,\n        help='size of the latent vectors (default: 256)')\n    parser.add_argument('--k', type=int, default=512,\n        help='number of latent vectors (default: 512)')\n\n    # Optimization\n    parser.add_argument('--batch-size', type=int, default=128,\n        help='batch size (default: 128)')\n    parser.add_argument('--num-epochs', type=int, default=100,\n        help='number of epochs (default: 100)')\n    parser.add_argument('--lr', type=float, default=2e-4,\n        help='learning rate for Adam optimizer (default: 2e-4)')\n    parser.add_argument('--beta', type=float, default=1.0,\n        help='contribution of commitment loss, between 0.1 and 2.0 (default: 1.0)')\n\n    # Miscellaneous\n    parser.add_argument('--output-folder', type=str, default='vqvae',\n        help='name of the output folder (default: vqvae)')\n    parser.add_argument('--num-workers', type=int, default=mp.cpu_count() - 1,\n        help='number of workers for trajectories sampling (default: {0})'.format(mp.cpu_count() - 1))\n    parser.add_argument('--device', type=str, default='cpu',\n        help='set the device (cpu or cuda, default: cpu)')\n\n    args = parser.parse_args()\n\n    # Create logs and models folder if they don't exist\n    if not os.path.exists('./logs'):\n        os.makedirs('./logs')\n    if not os.path.exists('./models'):\n        os.makedirs('./models')\n    # Device\n    args.device = torch.device(args.device\n        if torch.cuda.is_available() else 'cpu')\n    # Slurm\n    if 'SLURM_JOB_ID' in os.environ:\n        args.output_folder += '-{0}'.format(os.environ['SLURM_JOB_ID'])\n    if not os.path.exists('./models/{0}'.format(args.output_folder)):\n        os.makedirs('./models/{0}'.format(args.output_folder))\n    args.steps = 0\n\n    main(args)\n"""
