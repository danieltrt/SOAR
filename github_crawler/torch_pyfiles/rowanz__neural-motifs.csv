file_path,api_count,code
config.py,0,"b'""""""\nConfiguration file!\n""""""\nimport os\nfrom argparse import ArgumentParser\nimport numpy as np\n\nROOT_PATH = os.path.dirname(os.path.realpath(__file__))\nDATA_PATH = os.path.join(ROOT_PATH, \'data\')\n\ndef path(fn):\n    return os.path.join(DATA_PATH, fn)\n\ndef stanford_path(fn):\n    return os.path.join(DATA_PATH, \'stanford_filtered\', fn)\n\n# =============================================================================\n# Update these with where your data is stored ~~~~~~~~~~~~~~~~~~~~~~~~~\n\nVG_IMAGES = \'/home/rowan/datasets2/VG_100K_2/VG_100K\'\nRCNN_CHECKPOINT_FN = path(\'faster_rcnn_500k.h5\')\n\nIM_DATA_FN = stanford_path(\'image_data.json\')\nVG_SGG_FN = stanford_path(\'VG-SGG.h5\')\nVG_SGG_DICT_FN = stanford_path(\'VG-SGG-dicts.json\')\nPROPOSAL_FN = stanford_path(\'proposals.h5\')\n\nCOCO_PATH = \'/home/rowan/datasets/mscoco\'\n# =============================================================================\n# =============================================================================\n\n\nMODES = (\'sgdet\', \'sgcls\', \'predcls\')\n\nBOX_SCALE = 1024  # Scale at which we have the boxes\nIM_SCALE = 592      # Our images will be resized to this res without padding\n\n# Proposal assignments\nBG_THRESH_HI = 0.5\nBG_THRESH_LO = 0.0\n\nRPN_POSITIVE_OVERLAP = 0.7\n# IOU < thresh: negative example\nRPN_NEGATIVE_OVERLAP = 0.3\n\n# Max number of foreground examples\nRPN_FG_FRACTION = 0.5\nFG_FRACTION = 0.25\n# Total number of examples\nRPN_BATCHSIZE = 256\nROIS_PER_IMG = 256\nREL_FG_FRACTION = 0.25\nRELS_PER_IMG = 256\n\nRELS_PER_IMG_REFINE = 64\n\nBATCHNORM_MOMENTUM = 0.01\nANCHOR_SIZE = 16\n\nANCHOR_RATIOS = (0.23232838, 0.63365731, 1.28478321, 3.15089189) #(0.5, 1, 2)\nANCHOR_SCALES = (2.22152954, 4.12315647, 7.21692515, 12.60263013, 22.7102731) #(4, 8, 16, 32)\n\nclass ModelConfig(object):\n    """"""Wrapper class for model hyperparameters.""""""\n    def __init__(self):\n        """"""\n        Defaults\n        """"""\n        self.coco = None\n        self.ckpt = None\n        self.save_dir = None\n        self.lr = None\n        self.batch_size = None\n        self.val_size = None\n        self.l2 = None\n        self.clip = None\n        self.num_gpus = None\n        self.num_workers = None\n        self.print_interval = None\n        self.gt_box = None\n        self.mode = None\n        self.refine = None\n        self.ad3 = False\n        self.test = False\n        self.adam = False\n        self.multi_pred=False\n        self.cache = None\n        self.model = None\n        self.use_proposals=False\n        self.use_resnet=False\n        self.use_tanh=False\n        self.use_bias = False\n        self.limit_vision=False\n        self.num_epochs=None\n        self.old_feats=False\n        self.order=None\n        self.det_ckpt=None\n        self.nl_edge=None\n        self.nl_obj=None\n        self.hidden_dim=None\n        self.pass_in_obj_feats_to_decoder = None\n        self.pass_in_obj_feats_to_edge = None\n        self.pooling_dim = None\n        self.rec_dropout = None\n        self.parser = self.setup_parser()\n        self.args = vars(self.parser.parse_args())\n\n        print(""~~~~~~~~ Hyperparameters used: ~~~~~~~"")\n        for x, y in self.args.items():\n            print(""{} : {}"".format(x, y))\n\n        self.__dict__.update(self.args)\n\n        if len(self.ckpt) != 0:\n            self.ckpt = os.path.join(ROOT_PATH, self.ckpt)\n        else:\n            self.ckpt = None\n\n        if len(self.cache) != 0:\n            self.cache = os.path.join(ROOT_PATH, self.cache)\n        else:\n            self.cache = None\n\n        if len(self.save_dir) == 0:\n            self.save_dir = None\n        else:\n            self.save_dir = os.path.join(ROOT_PATH, self.save_dir)\n            if not os.path.exists(self.save_dir):\n                os.mkdir(self.save_dir)\n\n        assert self.val_size >= 0\n\n        if self.mode not in MODES:\n            raise ValueError(""Invalid mode: mode must be in {}"".format(MODES))\n\n        if self.model not in (\'motifnet\', \'stanford\'):\n            raise ValueError(""Invalid model {}"".format(self.model))\n\n\n        if self.ckpt is not None and not os.path.exists(self.ckpt):\n            raise ValueError(""Ckpt file ({}) doesnt exist"".format(self.ckpt))\n\n    def setup_parser(self):\n        """"""\n        Sets up an argument parser\n        :return:\n        """"""\n        parser = ArgumentParser(description=\'training code\')\n\n\n        # Options to deprecate\n        parser.add_argument(\'-coco\', dest=\'coco\', help=\'Use COCO (default to VG)\', action=\'store_true\')\n        parser.add_argument(\'-ckpt\', dest=\'ckpt\', help=\'Filename to load from\', type=str, default=\'\')\n        parser.add_argument(\'-det_ckpt\', dest=\'det_ckpt\', help=\'Filename to load detection parameters from\', type=str, default=\'\')\n\n        parser.add_argument(\'-save_dir\', dest=\'save_dir\',\n                            help=\'Directory to save things to, such as checkpoints/save\', default=\'\', type=str)\n\n        parser.add_argument(\'-ngpu\', dest=\'num_gpus\', help=\'cuantos GPUs tienes\', type=int, default=3)\n        parser.add_argument(\'-nwork\', dest=\'num_workers\', help=\'num processes to use as workers\', type=int, default=1)\n\n        parser.add_argument(\'-lr\', dest=\'lr\', help=\'learning rate\', type=float, default=1e-3)\n\n        parser.add_argument(\'-b\', dest=\'batch_size\', help=\'batch size per GPU\',type=int, default=2)\n        parser.add_argument(\'-val_size\', dest=\'val_size\', help=\'val size to use (if 0 we wont use val)\', type=int, default=5000)\n\n        parser.add_argument(\'-l2\', dest=\'l2\', help=\'weight decay\', type=float, default=1e-4)\n        parser.add_argument(\'-clip\', dest=\'clip\', help=\'gradients will be clipped to have norm less than this\', type=float, default=5.0)\n        parser.add_argument(\'-p\', dest=\'print_interval\', help=\'print during training\', type=int,\n                            default=100)\n        parser.add_argument(\'-m\', dest=\'mode\', help=\'mode \\in {sgdet, sgcls, predcls}\', type=str,\n                            default=\'sgdet\')\n        parser.add_argument(\'-model\', dest=\'model\', help=\'which model to use? (motifnet, stanford). If you want to use the baseline (NoContext) model, then pass in motifnet here, and nl_obj, nl_edge=0\', type=str,\n                            default=\'motifnet\')\n        parser.add_argument(\'-old_feats\', dest=\'old_feats\', help=\'Use the original image features for the edges\', action=\'store_true\')\n        parser.add_argument(\'-order\', dest=\'order\', help=\'Linearization order for Rois (confidence -default, size, random)\',\n                            type=str, default=\'confidence\')\n        parser.add_argument(\'-cache\', dest=\'cache\', help=\'where should we cache predictions\', type=str,\n                            default=\'\')\n        parser.add_argument(\'-gt_box\', dest=\'gt_box\', help=\'use gt boxes during training\', action=\'store_true\')\n        parser.add_argument(\'-adam\', dest=\'adam\', help=\'use adam. Not recommended\', action=\'store_true\')\n        parser.add_argument(\'-test\', dest=\'test\', help=\'test set\', action=\'store_true\')\n        parser.add_argument(\'-multipred\', dest=\'multi_pred\', help=\'Allow multiple predicates per pair of box0, box1.\', action=\'store_true\')\n        parser.add_argument(\'-nepoch\', dest=\'num_epochs\', help=\'Number of epochs to train the model for\',type=int, default=25)\n        parser.add_argument(\'-resnet\', dest=\'use_resnet\', help=\'use resnet instead of VGG\', action=\'store_true\')\n        parser.add_argument(\'-proposals\', dest=\'use_proposals\', help=\'Use Xu et als proposals\', action=\'store_true\')\n        parser.add_argument(\'-nl_obj\', dest=\'nl_obj\', help=\'Num object layers\', type=int, default=1)\n        parser.add_argument(\'-nl_edge\', dest=\'nl_edge\', help=\'Num edge layers\', type=int, default=2)\n        parser.add_argument(\'-hidden_dim\', dest=\'hidden_dim\', help=\'Num edge layers\', type=int, default=256)\n        parser.add_argument(\'-pooling_dim\', dest=\'pooling_dim\', help=\'Dimension of pooling\', type=int, default=4096)\n        parser.add_argument(\'-pass_in_obj_feats_to_decoder\', dest=\'pass_in_obj_feats_to_decoder\', action=\'store_true\')\n        parser.add_argument(\'-pass_in_obj_feats_to_edge\', dest=\'pass_in_obj_feats_to_edge\', action=\'store_true\')\n        parser.add_argument(\'-rec_dropout\', dest=\'rec_dropout\', help=\'recurrent dropout to add\', type=float, default=0.1)\n        parser.add_argument(\'-use_bias\', dest=\'use_bias\',  action=\'store_true\')\n        parser.add_argument(\'-use_tanh\', dest=\'use_tanh\',  action=\'store_true\')\n        parser.add_argument(\'-limit_vision\', dest=\'limit_vision\',  action=\'store_true\')\n        return parser\n'"
dataloaders/__init__.py,0,b''
dataloaders/blob.py,7,"b'""""""\nData blob, hopefully to make collating less painful and MGPU training possible\n""""""\nfrom lib.fpn.anchor_targets import anchor_target_layer\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\n\nclass Blob(object):\n    def __init__(self, mode=\'det\', is_train=False, num_gpus=1, primary_gpu=0, batch_size_per_gpu=3):\n        """"""\n        Initializes an empty Blob object.\n        :param mode: \'det\' for detection and \'rel\' for det+relationship\n        :param is_train: True if it\'s training\n        """"""\n        assert mode in (\'det\', \'rel\')\n        assert num_gpus >= 1\n        self.mode = mode\n        self.is_train = is_train\n        self.num_gpus = num_gpus\n        self.batch_size_per_gpu = batch_size_per_gpu\n        self.primary_gpu = primary_gpu\n\n        self.imgs = []  # [num_images, 3, IM_SCALE, IM_SCALE] array\n        self.im_sizes = []  # [num_images, 4] array of (h, w, scale, num_valid_anchors)\n        self.all_anchor_inds = []  # [all_anchors, 2] array of (img_ind, anchor_idx). Only has valid\n        # boxes (meaning some are gonna get cut out)\n        self.all_anchors = []  # [num_im, IM_SCALE/4, IM_SCALE/4, num_anchors, 4] shapes. Anchors outside get squashed\n                               # to 0\n        self.gt_boxes = []  # [num_gt, 4] boxes\n        self.gt_classes = []  # [num_gt,2] array of img_ind, class\n        self.gt_rels = []  # [num_rels, 3]. Each row is (gtbox0, gtbox1, rel).\n\n        self.gt_sents = []\n        self.gt_nodes = []\n        self.sent_lengths = []\n\n        self.train_anchor_labels = []  # [train_anchors, 5] array of (img_ind, h, w, A, labels)\n        self.train_anchors = []  # [train_anchors, 8] shapes with anchor, target\n\n        self.train_anchor_inds = None  # This will be split into GPUs, just (img_ind, h, w, A).\n\n        self.batch_size = None\n        self.gt_box_chunks = None\n        self.anchor_chunks = None\n        self.train_chunks = None\n        self.proposal_chunks = None\n        self.proposals = []\n\n    @property\n    def is_flickr(self):\n        return self.mode == \'flickr\'\n\n    @property\n    def is_rel(self):\n        return self.mode == \'rel\'\n\n    @property\n    def volatile(self):\n        return not self.is_train\n\n    def append(self, d):\n        """"""\n        Adds a single image to the blob\n        :param datom:\n        :return:\n        """"""\n        i = len(self.imgs)\n        self.imgs.append(d[\'img\'])\n\n        h, w, scale = d[\'img_size\']\n\n        # all anchors\n        self.im_sizes.append((h, w, scale))\n\n        gt_boxes_ = d[\'gt_boxes\'].astype(np.float32) * d[\'scale\']\n        self.gt_boxes.append(gt_boxes_)\n\n        self.gt_classes.append(np.column_stack((\n            i * np.ones(d[\'gt_classes\'].shape[0], dtype=np.int64),\n            d[\'gt_classes\'],\n        )))\n\n        # Add relationship info\n        if self.is_rel:\n            self.gt_rels.append(np.column_stack((\n                i * np.ones(d[\'gt_relations\'].shape[0], dtype=np.int64),\n                d[\'gt_relations\'])))\n\n        # Augment with anchor targets\n        if self.is_train:\n            train_anchors_, train_anchor_inds_, train_anchor_targets_, train_anchor_labels_ = \\\n                anchor_target_layer(gt_boxes_, (h, w))\n\n            self.train_anchors.append(np.hstack((train_anchors_, train_anchor_targets_)))\n\n            self.train_anchor_labels.append(np.column_stack((\n                i * np.ones(train_anchor_inds_.shape[0], dtype=np.int64),\n                train_anchor_inds_,\n                train_anchor_labels_,\n            )))\n\n        if \'proposals\' in d:\n            self.proposals.append(np.column_stack((i * np.ones(d[\'proposals\'].shape[0], dtype=np.float32),\n                                                   d[\'scale\'] * d[\'proposals\'].astype(np.float32))))\n\n\n\n    def _chunkize(self, datom, tensor=torch.LongTensor):\n        """"""\n        Turn data list into chunks, one per GPU\n        :param datom: List of lists of numpy arrays that will be concatenated.\n        :return:\n        """"""\n        chunk_sizes = [0] * self.num_gpus\n        for i in range(self.num_gpus):\n            for j in range(self.batch_size_per_gpu):\n                chunk_sizes[i] += datom[i * self.batch_size_per_gpu + j].shape[0]\n        return Variable(tensor(np.concatenate(datom, 0)), volatile=self.volatile), chunk_sizes\n\n    def reduce(self):\n        """""" Merges all the detections into flat lists + numbers of how many are in each""""""\n        if len(self.imgs) != self.batch_size_per_gpu * self.num_gpus:\n            raise ValueError(""Wrong batch size? imgs len {} bsize/gpu {} numgpus {}"".format(\n                len(self.imgs), self.batch_size_per_gpu, self.num_gpus\n            ))\n\n        self.imgs = Variable(torch.stack(self.imgs, 0), volatile=self.volatile)\n        self.im_sizes = np.stack(self.im_sizes).reshape(\n            (self.num_gpus, self.batch_size_per_gpu, 3))\n\n        if self.is_rel:\n            self.gt_rels, self.gt_rel_chunks = self._chunkize(self.gt_rels)\n\n        self.gt_boxes, self.gt_box_chunks = self._chunkize(self.gt_boxes, tensor=torch.FloatTensor)\n        self.gt_classes, _ = self._chunkize(self.gt_classes)\n        if self.is_train:\n            self.train_anchor_labels, self.train_chunks = self._chunkize(self.train_anchor_labels)\n            self.train_anchors, _ = self._chunkize(self.train_anchors, tensor=torch.FloatTensor)\n            self.train_anchor_inds = self.train_anchor_labels[:, :-1].contiguous()\n\n        if len(self.proposals) != 0:\n            self.proposals, self.proposal_chunks = self._chunkize(self.proposals, tensor=torch.FloatTensor)\n\n\n\n    def _scatter(self, x, chunk_sizes, dim=0):\n        """""" Helper function""""""\n        if self.num_gpus == 1:\n            return x.cuda(self.primary_gpu, async=True)\n        return torch.nn.parallel.scatter_gather.Scatter.apply(\n            list(range(self.num_gpus)), chunk_sizes, dim, x)\n\n    def scatter(self):\n        """""" Assigns everything to the GPUs""""""\n        self.imgs = self._scatter(self.imgs, [self.batch_size_per_gpu] * self.num_gpus)\n\n        self.gt_classes_primary = self.gt_classes.cuda(self.primary_gpu, async=True)\n        self.gt_boxes_primary = self.gt_boxes.cuda(self.primary_gpu, async=True)\n\n        # Predcls might need these\n        self.gt_classes = self._scatter(self.gt_classes, self.gt_box_chunks)\n        self.gt_boxes = self._scatter(self.gt_boxes, self.gt_box_chunks)\n\n        if self.is_train:\n\n            self.train_anchor_inds = self._scatter(self.train_anchor_inds,\n                                                   self.train_chunks)\n            self.train_anchor_labels = self.train_anchor_labels.cuda(self.primary_gpu, async=True)\n            self.train_anchors = self.train_anchors.cuda(self.primary_gpu, async=True)\n\n            if self.is_rel:\n                self.gt_rels = self._scatter(self.gt_rels, self.gt_rel_chunks)\n        else:\n            if self.is_rel:\n                self.gt_rels = self.gt_rels.cuda(self.primary_gpu, async=True)\n\n        if self.proposal_chunks is not None:\n            self.proposals = self._scatter(self.proposals, self.proposal_chunks)\n\n    def __getitem__(self, index):\n        """"""\n        Returns a tuple containing data\n        :param index: Which GPU we\'re on, or 0 if no GPUs\n        :return: If training:\n        (image, im_size, img_start_ind, anchor_inds, anchors, gt_boxes, gt_classes, \n        train_anchor_inds)\n        test:\n        (image, im_size, img_start_ind, anchor_inds, anchors)\n        """"""\n        if index not in list(range(self.num_gpus)):\n            raise ValueError(""Out of bounds with index {} and {} gpus"".format(index, self.num_gpus))\n\n        if self.is_rel:\n            rels = self.gt_rels\n            if index > 0 or self.num_gpus != 1:\n                rels_i = rels[index] if self.is_rel else None\n        elif self.is_flickr:\n            rels = (self.gt_sents, self.gt_nodes)\n            if index > 0 or self.num_gpus != 1:\n                rels_i = (self.gt_sents[index], self.gt_nodes[index])\n        else:\n            rels = None\n            rels_i = None\n\n        if self.proposal_chunks is None:\n            proposals = None\n        else:\n            proposals = self.proposals\n\n        if index == 0 and self.num_gpus == 1:\n            image_offset = 0\n            if self.is_train:\n                return (self.imgs, self.im_sizes[0], image_offset,\n                        self.gt_boxes, self.gt_classes, rels, proposals, self.train_anchor_inds)\n            return self.imgs, self.im_sizes[0], image_offset, self.gt_boxes, self.gt_classes, rels, proposals\n\n        # Otherwise proposals is None\n        assert proposals is None\n\n        image_offset = self.batch_size_per_gpu * index\n        # TODO: Return a namedtuple\n        if self.is_train:\n            return (\n            self.imgs[index], self.im_sizes[index], image_offset,\n            self.gt_boxes[index], self.gt_classes[index], rels_i, None, self.train_anchor_inds[index])\n        return (self.imgs[index], self.im_sizes[index], image_offset,\n                self.gt_boxes[index], self.gt_classes[index], rels_i, None)\n\n'"
dataloaders/image_transforms.py,0,"b'# Some image transforms\n\nfrom PIL import Image, ImageOps, ImageFilter, ImageEnhance\nimport numpy as np\nfrom random import randint\n# All of these need to be called on PIL imagez\n\nclass SquarePad(object):\n    def __call__(self, img):\n        w, h = img.size\n        img_padded = ImageOps.expand(img, border=(0, 0, max(h - w, 0), max(w - h, 0)),\n                                     fill=(int(0.485 * 256), int(0.456 * 256), int(0.406 * 256)))\n        return img_padded\n\n\nclass Grayscale(object):\n    """"""\n    Converts to grayscale (not always, sometimes).\n    """"""\n    def __call__(self, img):\n        factor = np.sqrt(np.sqrt(np.random.rand(1)))\n        # print(""gray {}"".format(factor))\n        enhancer = ImageEnhance.Color(img)\n        return enhancer.enhance(factor)\n\n\nclass Brightness(object):\n    """"""\n    Converts to grayscale (not always, sometimes).\n    """"""\n    def __call__(self, img):\n        factor = np.random.randn(1)/6+1\n        factor = min(max(factor, 0.5), 1.5)\n        # print(""brightness {}"".format(factor))\n\n        enhancer = ImageEnhance.Brightness(img)\n        return enhancer.enhance(factor)\n\n\nclass Contrast(object):\n    """"""\n    Converts to grayscale (not always, sometimes).\n    """"""\n    def __call__(self, img):\n        factor = np.random.randn(1)/8+1.0\n        factor = min(max(factor, 0.5), 1.5)\n        # print(""contrast {}"".format(factor))\n\n        enhancer = ImageEnhance.Contrast(img)\n        return enhancer.enhance(factor)\n\n\nclass Hue(object):\n    """"""\n    Converts to grayscale\n    """"""\n    def __call__(self, img):\n        # 30 seems good\n        factor = int(np.random.randn(1)*8)\n        factor = min(max(factor, -30), 30)\n        factor = np.array(factor, dtype=np.uint8)\n\n        hsv = np.array(img.convert(\'HSV\'))\n        hsv[:,:,0] += factor\n        new_img = Image.fromarray(hsv, \'HSV\').convert(\'RGB\')\n\n        return new_img\n\n\nclass Sharpness(object):\n    """"""\n    Converts to grayscale\n    """"""\n    def __call__(self, img):\n        factor = 1.0 + np.random.randn(1)/5\n        # print(""sharpness {}"".format(factor))\n        enhancer = ImageEnhance.Sharpness(img)\n        return enhancer.enhance(factor)\n\n\ndef random_crop(img, boxes, box_scale, round_boxes=True, max_crop_fraction=0.1):\n    """"""\n    Randomly crops the image\n    :param img: PIL image\n    :param boxes: Ground truth boxes\n    :param box_scale: This is the scale that the boxes are at (e.g. 1024 wide). We\'ll preserve that ratio\n    :param round_boxes: Set this to true if we\'re going to round the boxes to ints\n    :return: Cropped image, new boxes\n    """"""\n\n    w, h = img.size\n\n    max_crop_w = int(w*max_crop_fraction)\n    max_crop_h = int(h*max_crop_fraction)\n    boxes_scaled = boxes * max(w,h) / box_scale\n    max_to_crop_top = min(int(boxes_scaled[:, 1].min()), max_crop_h)\n    max_to_crop_left = min(int(boxes_scaled[:, 0].min()), max_crop_w)\n    max_to_crop_right = min(int(w - boxes_scaled[:, 2].max()), max_crop_w)\n    max_to_crop_bottom = min(int(h - boxes_scaled[:, 3].max()), max_crop_h)\n\n    crop_top = randint(0, max(max_to_crop_top, 0))\n    crop_left = randint(0, max(max_to_crop_left, 0))\n    crop_right = randint(0, max(max_to_crop_right, 0))\n    crop_bottom = randint(0, max(max_to_crop_bottom, 0))\n    img_cropped = img.crop((crop_left, crop_top, w - crop_right, h - crop_bottom))\n\n    new_boxes = box_scale / max(img_cropped.size) * np.column_stack(\n        (boxes_scaled[:,0]-crop_left, boxes_scaled[:,1]-crop_top, boxes_scaled[:,2]-crop_left, boxes_scaled[:,3]-crop_top))\n\n    if round_boxes:\n        new_boxes = np.round(new_boxes).astype(np.int32)\n    return img_cropped, new_boxes\n\n\nclass RandomOrder(object):\n    """""" Composes several transforms together in random order - or not at all!\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        if self.transforms is None:\n            return img\n        num_to_pick = np.random.choice(len(self.transforms))\n        if num_to_pick == 0:\n            return img\n\n        order = np.random.choice(len(self.transforms), size=num_to_pick, replace=False)\n        for i in order:\n            img = self.transforms[i](img)\n        return img'"
dataloaders/mscoco.py,2,"b'from config import COCO_PATH, IM_SCALE, BOX_SCALE\nimport os\nfrom torch.utils.data import Dataset\nfrom pycocotools.coco import COCO\nfrom PIL import Image\nfrom lib.fpn.anchor_targets import anchor_target_layer\nfrom torchvision.transforms import Resize, Compose, ToTensor, Normalize\nfrom dataloaders.image_transforms import SquarePad, Grayscale, Brightness, Sharpness, Contrast, RandomOrder, Hue, random_crop\nimport numpy as np\nfrom dataloaders.blob import Blob\nimport torch\n\nclass CocoDetection(Dataset):\n    """"""\n    Adapted from the torchvision code\n    """"""\n\n    def __init__(self, mode):\n        """"""\n        :param mode: train2014 or val2014\n        """"""\n        self.mode = mode\n        self.root = os.path.join(COCO_PATH, mode)\n        self.ann_file = os.path.join(COCO_PATH, \'annotations\', \'instances_{}.json\'.format(mode))\n        self.coco = COCO(self.ann_file)\n        self.ids = [k for k in self.coco.imgs.keys() if len(self.coco.imgToAnns[k]) > 0]\n\n\n        tform = []\n        if self.is_train:\n             tform.append(RandomOrder([\n                 Grayscale(),\n                 Brightness(),\n                 Contrast(),\n                 Sharpness(),\n                 Hue(),\n             ]))\n\n        tform += [\n            SquarePad(),\n            Resize(IM_SCALE),\n            ToTensor(),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ]\n\n        self.transform_pipeline = Compose(tform)\n        self.ind_to_classes = [\'__background__\'] + [v[\'name\'] for k, v in self.coco.cats.items()]\n        # COCO inds are weird (84 inds in total but a bunch of numbers are skipped)\n        self.id_to_ind = {coco_id:(ind+1) for ind, coco_id in enumerate(self.coco.cats.keys())}\n        self.id_to_ind[0] = 0\n\n        self.ind_to_id = {x:y for y,x in self.id_to_ind.items()}\n\n    @property\n    def is_train(self):\n        return self.mode.startswith(\'train\')\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n\n        Returns: entry dict\n        """"""\n        img_id = self.ids[index]\n        path = self.coco.loadImgs(img_id)[0][\'file_name\']\n        image_unpadded = Image.open(os.path.join(self.root, path)).convert(\'RGB\')\n        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n        anns = self.coco.loadAnns(ann_ids)\n        gt_classes = np.array([self.id_to_ind[x[\'category_id\']] for x in anns], dtype=np.int64)\n\n        if np.any(gt_classes >= len(self.ind_to_classes)):\n            raise ValueError(""OH NO {}"".format(index))\n\n        if len(anns) == 0:\n            raise ValueError(""Annotations should not be empty"")\n        #     gt_boxes = np.array((0, 4), dtype=np.float32)\n        # else:\n        gt_boxes = np.array([x[\'bbox\'] for x in anns], dtype=np.float32)\n\n        if np.any(gt_boxes[:, [0,1]] < 0):\n            raise ValueError(""GT boxes empty columns"")\n        if np.any(gt_boxes[:, [2,3]] < 0):\n            raise ValueError(""GT boxes empty h/w"")\n        gt_boxes[:, [2, 3]] += gt_boxes[:, [0, 1]]\n\n        # Rescale so that the boxes are at BOX_SCALE\n        if self.is_train:\n            image_unpadded, gt_boxes = random_crop(image_unpadded,\n                                                   gt_boxes * BOX_SCALE / max(image_unpadded.size),\n                                                   BOX_SCALE,\n                                                   round_boxes=False,\n                                                   )\n        else:\n            # Seems a bit silly because we won\'t be using GT boxes then but whatever\n            gt_boxes = gt_boxes * BOX_SCALE / max(image_unpadded.size)\n        w, h = image_unpadded.size\n        box_scale_factor = BOX_SCALE / max(w, h)\n\n        # Optionally flip the image if we\'re doing training\n        flipped = self.is_train and np.random.random() > 0.5\n        if flipped:\n            scaled_w = int(box_scale_factor * float(w))\n            image_unpadded = image_unpadded.transpose(Image.FLIP_LEFT_RIGHT)\n            gt_boxes[:, [0, 2]] = scaled_w - gt_boxes[:, [2, 0]]\n\n        img_scale_factor = IM_SCALE / max(w, h)\n        if h > w:\n            im_size = (IM_SCALE, int(w*img_scale_factor), img_scale_factor)\n        elif h < w:\n            im_size = (int(h*img_scale_factor), IM_SCALE, img_scale_factor)\n        else:\n            im_size = (IM_SCALE, IM_SCALE, img_scale_factor)\n\n        entry = {\n            \'img\': self.transform_pipeline(image_unpadded),\n            \'img_size\': im_size,\n            \'gt_boxes\': gt_boxes,\n            \'gt_classes\': gt_classes,\n            \'scale\': IM_SCALE / BOX_SCALE,\n            \'index\': index,\n            \'image_id\': img_id,\n            \'flipped\': flipped,\n            \'fn\': path,\n        }\n\n        return entry\n\n    @classmethod\n    def splits(cls, *args, **kwargs):\n        """""" Helper method to generate splits of the dataset""""""\n        train = cls(\'train2014\', *args, **kwargs)\n        val = cls(\'val2014\', *args, **kwargs)\n        return train, val\n\n    def __len__(self):\n        return len(self.ids)\n\n\ndef coco_collate(data, num_gpus=3, is_train=False):\n    blob = Blob(mode=\'det\', is_train=is_train, num_gpus=num_gpus,\n                batch_size_per_gpu=len(data) // num_gpus)\n    for d in data:\n        blob.append(d)\n    blob.reduce()\n    return blob\n\n\nclass CocoDataLoader(torch.utils.data.DataLoader):\n    """"""\n    Iterates through the data, filtering out None,\n     but also loads everything as a (cuda) variable\n    """"""\n    # def __iter__(self):\n    #     for x in super(CocoDataLoader, self).__iter__():\n    #         if isinstance(x, tuple) or isinstance(x, list):\n    #             yield tuple(y.cuda(async=True) if hasattr(y, \'cuda\') else y for y in x)\n    #         else:\n    #             yield x.cuda(async=True)\n\n    @classmethod\n    def splits(cls, train_data, val_data, batch_size=3, num_workers=1, num_gpus=3, **kwargs):\n        train_load = cls(\n            dataset=train_data,\n            batch_size=batch_size*num_gpus,\n            shuffle=True,\n            num_workers=num_workers,\n            collate_fn=lambda x: coco_collate(x, num_gpus=num_gpus, is_train=True),\n            drop_last=True,\n            # pin_memory=True,\n            **kwargs,\n        )\n        val_load = cls(\n            dataset=val_data,\n            batch_size=batch_size*num_gpus,\n            shuffle=False,\n            num_workers=num_workers,\n            collate_fn=lambda x: coco_collate(x, num_gpus=num_gpus, is_train=False),\n            drop_last=True,\n            # pin_memory=True,\n            **kwargs,\n        )\n        return train_load, val_load\n\n\nif __name__ == \'__main__\':\n    train, val = CocoDetection.splits()\n    gtbox = train[0][\'gt_boxes\']\n    img_size = train[0][\'img_size\']\n    anchor_strides, labels, bbox_targets = anchor_target_layer(gtbox, img_size)\n'"
dataloaders/visual_genome.py,2,"b'""""""\nFile that involves dataloaders for the Visual Genome dataset.\n""""""\n\nimport json\nimport os\n\nimport h5py\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms import Resize, Compose, ToTensor, Normalize\nfrom dataloaders.blob import Blob\nfrom lib.fpn.box_intersections_cpu.bbox import bbox_overlaps\nfrom config import VG_IMAGES, IM_DATA_FN, VG_SGG_FN, VG_SGG_DICT_FN, BOX_SCALE, IM_SCALE, PROPOSAL_FN\nfrom dataloaders.image_transforms import SquarePad, Grayscale, Brightness, Sharpness, Contrast, \\\n    RandomOrder, Hue, random_crop\nfrom collections import defaultdict\nfrom pycocotools.coco import COCO\n\n\nclass VG(Dataset):\n    def __init__(self, mode, roidb_file=VG_SGG_FN, dict_file=VG_SGG_DICT_FN,\n                 image_file=IM_DATA_FN, filter_empty_rels=True, num_im=-1, num_val_im=5000,\n                 filter_duplicate_rels=True, filter_non_overlap=True,\n                 use_proposals=False):\n        """"""\n        Torch dataset for VisualGenome\n        :param mode: Must be train, test, or val\n        :param roidb_file:  HDF5 containing the GT boxes, classes, and relationships\n        :param dict_file: JSON Contains mapping of classes/relationships to words\n        :param image_file: HDF5 containing image filenames\n        :param filter_empty_rels: True if we filter out images without relationships between\n                             boxes. One might want to set this to false if training a detector.\n        :param filter_duplicate_rels: Whenever we see a duplicate relationship we\'ll sample instead\n        :param num_im: Number of images in the entire dataset. -1 for all images.\n        :param num_val_im: Number of images in the validation set (must be less than num_im\n               unless num_im is -1.)\n        :param proposal_file: If None, we don\'t provide proposals. Otherwise file for where we get RPN\n            proposals\n        """"""\n        if mode not in (\'test\', \'train\', \'val\'):\n            raise ValueError(""Mode must be in test, train, or val. Supplied {}"".format(mode))\n        self.mode = mode\n\n        # Initialize\n        self.roidb_file = roidb_file\n        self.dict_file = dict_file\n        self.image_file = image_file\n        self.filter_non_overlap = filter_non_overlap\n        self.filter_duplicate_rels = filter_duplicate_rels and self.mode == \'train\'\n\n        self.split_mask, self.gt_boxes, self.gt_classes, self.relationships = load_graphs(\n            self.roidb_file, self.mode, num_im, num_val_im=num_val_im,\n            filter_empty_rels=filter_empty_rels,\n            filter_non_overlap=self.filter_non_overlap and self.is_train,\n        )\n\n        self.filenames = load_image_filenames(image_file)\n        self.filenames = [self.filenames[i] for i in np.where(self.split_mask)[0]]\n\n        self.ind_to_classes, self.ind_to_predicates = load_info(dict_file)\n\n        if use_proposals:\n            print(""Loading proposals"", flush=True)\n            p_h5 = h5py.File(PROPOSAL_FN, \'r\')\n            rpn_rois = p_h5[\'rpn_rois\']\n            rpn_scores = p_h5[\'rpn_scores\']\n            rpn_im_to_roi_idx = np.array(p_h5[\'im_to_roi_idx\'][self.split_mask])\n            rpn_num_rois = np.array(p_h5[\'num_rois\'][self.split_mask])\n\n            self.rpn_rois = []\n            for i in range(len(self.filenames)):\n                rpn_i = np.column_stack((\n                    rpn_scores[rpn_im_to_roi_idx[i]:rpn_im_to_roi_idx[i] + rpn_num_rois[i]],\n                    rpn_rois[rpn_im_to_roi_idx[i]:rpn_im_to_roi_idx[i] + rpn_num_rois[i]],\n                ))\n                self.rpn_rois.append(rpn_i)\n        else:\n            self.rpn_rois = None\n\n        # You could add data augmentation here. But we didn\'t.\n        # tform = []\n        # if self.is_train:\n        #     tform.append(RandomOrder([\n        #         Grayscale(),\n        #         Brightness(),\n        #         Contrast(),\n        #         Sharpness(),\n        #         Hue(),\n        #     ]))\n\n        tform = [\n            SquarePad(),\n            Resize(IM_SCALE),\n            ToTensor(),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ]\n        self.transform_pipeline = Compose(tform)\n\n    @property\n    def coco(self):\n        """"""\n        :return: a Coco-like object that we can use to evaluate detection!\n        """"""\n        anns = []\n        for i, (cls_array, box_array) in enumerate(zip(self.gt_classes, self.gt_boxes)):\n            for cls, box in zip(cls_array.tolist(), box_array.tolist()):\n                anns.append({\n                    \'area\': (box[3] - box[1] + 1) * (box[2] - box[0] + 1),\n                    \'bbox\': [box[0], box[1], box[2] - box[0] + 1, box[3] - box[1] + 1],\n                    \'category_id\': cls,\n                    \'id\': len(anns),\n                    \'image_id\': i,\n                    \'iscrowd\': 0,\n                })\n        fauxcoco = COCO()\n        fauxcoco.dataset = {\n            \'info\': {\'description\': \'ayy lmao\'},\n            \'images\': [{\'id\': i} for i in range(self.__len__())],\n            \'categories\': [{\'supercategory\': \'person\',\n                               \'id\': i, \'name\': name} for i, name in enumerate(self.ind_to_classes) if name != \'__background__\'],\n            \'annotations\': anns,\n        }\n        fauxcoco.createIndex()\n        return fauxcoco\n\n    @property\n    def is_train(self):\n        return self.mode.startswith(\'train\')\n\n    @classmethod\n    def splits(cls, *args, **kwargs):\n        """""" Helper method to generate splits of the dataset""""""\n        train = cls(\'train\', *args, **kwargs)\n        val = cls(\'val\', *args, **kwargs)\n        test = cls(\'test\', *args, **kwargs)\n        return train, val, test\n\n    def __getitem__(self, index):\n        image_unpadded = Image.open(self.filenames[index]).convert(\'RGB\')\n\n        # Optionally flip the image if we\'re doing training\n        flipped = self.is_train and np.random.random() > 0.5\n        gt_boxes = self.gt_boxes[index].copy()\n\n        # Boxes are already at BOX_SCALE\n        if self.is_train:\n            # crop boxes that are too large. This seems to be only a problem for image heights, but whatevs\n            gt_boxes[:, [1, 3]] = gt_boxes[:, [1, 3]].clip(\n                None, BOX_SCALE / max(image_unpadded.size) * image_unpadded.size[1])\n            gt_boxes[:, [0, 2]] = gt_boxes[:, [0, 2]].clip(\n                None, BOX_SCALE / max(image_unpadded.size) * image_unpadded.size[0])\n\n            # # crop the image for data augmentation\n            # image_unpadded, gt_boxes = random_crop(image_unpadded, gt_boxes, BOX_SCALE, round_boxes=True)\n\n        w, h = image_unpadded.size\n        box_scale_factor = BOX_SCALE / max(w, h)\n\n        if flipped:\n            scaled_w = int(box_scale_factor * float(w))\n            # print(""Scaled w is {}"".format(scaled_w))\n            image_unpadded = image_unpadded.transpose(Image.FLIP_LEFT_RIGHT)\n            gt_boxes[:, [0, 2]] = scaled_w - gt_boxes[:, [2, 0]]\n\n        img_scale_factor = IM_SCALE / max(w, h)\n        if h > w:\n            im_size = (IM_SCALE, int(w * img_scale_factor), img_scale_factor)\n        elif h < w:\n            im_size = (int(h * img_scale_factor), IM_SCALE, img_scale_factor)\n        else:\n            im_size = (IM_SCALE, IM_SCALE, img_scale_factor)\n\n        gt_rels = self.relationships[index].copy()\n        if self.filter_duplicate_rels:\n            # Filter out dupes!\n            assert self.mode == \'train\'\n            old_size = gt_rels.shape[0]\n            all_rel_sets = defaultdict(list)\n            for (o0, o1, r) in gt_rels:\n                all_rel_sets[(o0, o1)].append(r)\n            gt_rels = [(k[0], k[1], np.random.choice(v)) for k,v in all_rel_sets.items()]\n            gt_rels = np.array(gt_rels)\n\n        entry = {\n            \'img\': self.transform_pipeline(image_unpadded),\n            \'img_size\': im_size,\n            \'gt_boxes\': gt_boxes,\n            \'gt_classes\': self.gt_classes[index].copy(),\n            \'gt_relations\': gt_rels,\n            \'scale\': IM_SCALE / BOX_SCALE,  # Multiply the boxes by this.\n            \'index\': index,\n            \'flipped\': flipped,\n            \'fn\': self.filenames[index],\n        }\n\n        if self.rpn_rois is not None:\n            entry[\'proposals\'] = self.rpn_rois[index]\n\n        assertion_checks(entry)\n        return entry\n\n    def __len__(self):\n        return len(self.filenames)\n\n    @property\n    def num_predicates(self):\n        return len(self.ind_to_predicates)\n\n    @property\n    def num_classes(self):\n        return len(self.ind_to_classes)\n\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# MISC. HELPER FUNCTIONS ~~~~~~~~~~~~~~~~~~~~~\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\ndef assertion_checks(entry):\n    im_size = tuple(entry[\'img\'].size())\n    if len(im_size) != 3:\n        raise ValueError(""Img must be dim-3"")\n\n    c, h, w = entry[\'img\'].size()\n    if c != 3:\n        raise ValueError(""Must have 3 color channels"")\n\n    num_gt = entry[\'gt_boxes\'].shape[0]\n    if entry[\'gt_classes\'].shape[0] != num_gt:\n        raise ValueError(""GT classes and GT boxes must have same number of examples"")\n\n    assert (entry[\'gt_boxes\'][:, 2] >= entry[\'gt_boxes\'][:, 0]).all()\n    assert (entry[\'gt_boxes\'] >= -1).all()\n\n\ndef load_image_filenames(image_file, image_dir=VG_IMAGES):\n    """"""\n    Loads the image filenames from visual genome from the JSON file that contains them.\n    This matches the preprocessing in scene-graph-TF-release/data_tools/vg_to_imdb.py.\n    :param image_file: JSON file. Elements contain the param ""image_id"".\n    :param image_dir: directory where the VisualGenome images are located\n    :return: List of filenames corresponding to the good images\n    """"""\n    with open(image_file, \'r\') as f:\n        im_data = json.load(f)\n\n    corrupted_ims = [\'1592.jpg\', \'1722.jpg\', \'4616.jpg\', \'4617.jpg\']\n    fns = []\n    for i, img in enumerate(im_data):\n        basename = \'{}.jpg\'.format(img[\'image_id\'])\n        if basename in corrupted_ims:\n            continue\n\n        filename = os.path.join(image_dir, basename)\n        if os.path.exists(filename):\n            fns.append(filename)\n    assert len(fns) == 108073\n    return fns\n\n\ndef load_graphs(graphs_file, mode=\'train\', num_im=-1, num_val_im=0, filter_empty_rels=True,\n                filter_non_overlap=False):\n    """"""\n    Load the file containing the GT boxes and relations, as well as the dataset split\n    :param graphs_file: HDF5\n    :param mode: (train, val, or test)\n    :param num_im: Number of images we want\n    :param num_val_im: Number of validation images\n    :param filter_empty_rels: (will be filtered otherwise.)\n    :param filter_non_overlap: If training, filter images that dont overlap.\n    :return: image_index: numpy array corresponding to the index of images we\'re using\n             boxes: List where each element is a [num_gt, 4] array of ground \n                    truth boxes (x1, y1, x2, y2)\n             gt_classes: List where each element is a [num_gt] array of classes\n             relationships: List where each element is a [num_r, 3] array of \n                    (box_ind_1, box_ind_2, predicate) relationships\n    """"""\n    if mode not in (\'train\', \'val\', \'test\'):\n        raise ValueError(\'{} invalid\'.format(mode))\n\n    roi_h5 = h5py.File(graphs_file, \'r\')\n    data_split = roi_h5[\'split\'][:]\n    split = 2 if mode == \'test\' else 0\n    split_mask = data_split == split\n\n    # Filter out images without bounding boxes\n    split_mask &= roi_h5[\'img_to_first_box\'][:] >= 0\n    if filter_empty_rels:\n        split_mask &= roi_h5[\'img_to_first_rel\'][:] >= 0\n\n    image_index = np.where(split_mask)[0]\n    if num_im > -1:\n        image_index = image_index[:num_im]\n    if num_val_im > 0:\n        if mode == \'val\':\n            image_index = image_index[:num_val_im]\n        elif mode == \'train\':\n            image_index = image_index[num_val_im:]\n\n\n    split_mask = np.zeros_like(data_split).astype(bool)\n    split_mask[image_index] = True\n\n    # Get box information\n    all_labels = roi_h5[\'labels\'][:, 0]\n    all_boxes = roi_h5[\'boxes_{}\'.format(BOX_SCALE)][:]  # will index later\n    assert np.all(all_boxes[:, :2] >= 0)  # sanity check\n    assert np.all(all_boxes[:, 2:] > 0)  # no empty box\n\n    # convert from xc, yc, w, h to x1, y1, x2, y2\n    all_boxes[:, :2] = all_boxes[:, :2] - all_boxes[:, 2:] / 2\n    all_boxes[:, 2:] = all_boxes[:, :2] + all_boxes[:, 2:]\n\n    im_to_first_box = roi_h5[\'img_to_first_box\'][split_mask]\n    im_to_last_box = roi_h5[\'img_to_last_box\'][split_mask]\n    im_to_first_rel = roi_h5[\'img_to_first_rel\'][split_mask]\n    im_to_last_rel = roi_h5[\'img_to_last_rel\'][split_mask]\n\n    # load relation labels\n    _relations = roi_h5[\'relationships\'][:]\n    _relation_predicates = roi_h5[\'predicates\'][:, 0]\n    assert (im_to_first_rel.shape[0] == im_to_last_rel.shape[0])\n    assert (_relations.shape[0] == _relation_predicates.shape[0])  # sanity check\n\n    # Get everything by image.\n    boxes = []\n    gt_classes = []\n    relationships = []\n    for i in range(len(image_index)):\n        boxes_i = all_boxes[im_to_first_box[i]:im_to_last_box[i] + 1, :]\n        gt_classes_i = all_labels[im_to_first_box[i]:im_to_last_box[i] + 1]\n\n        if im_to_first_rel[i] >= 0:\n            predicates = _relation_predicates[im_to_first_rel[i]:im_to_last_rel[i] + 1]\n            obj_idx = _relations[im_to_first_rel[i]:im_to_last_rel[i] + 1] - im_to_first_box[i]\n            assert np.all(obj_idx >= 0)\n            assert np.all(obj_idx < boxes_i.shape[0])\n            rels = np.column_stack((obj_idx, predicates))\n        else:\n            assert not filter_empty_rels\n            rels = np.zeros((0, 3), dtype=np.int32)\n\n        if filter_non_overlap:\n            assert mode == \'train\'\n            inters = bbox_overlaps(boxes_i, boxes_i)\n            rel_overs = inters[rels[:, 0], rels[:, 1]]\n            inc = np.where(rel_overs > 0.0)[0]\n\n            if inc.size > 0:\n                rels = rels[inc]\n            else:\n                split_mask[image_index[i]] = 0\n                continue\n\n        boxes.append(boxes_i)\n        gt_classes.append(gt_classes_i)\n        relationships.append(rels)\n\n    return split_mask, boxes, gt_classes, relationships\n\n\ndef load_info(info_file):\n    """"""\n    Loads the file containing the visual genome label meanings\n    :param info_file: JSON\n    :return: ind_to_classes: sorted list of classes\n             ind_to_predicates: sorted list of predicates\n    """"""\n    info = json.load(open(info_file, \'r\'))\n    info[\'label_to_idx\'][\'__background__\'] = 0\n    info[\'predicate_to_idx\'][\'__background__\'] = 0\n\n    class_to_ind = info[\'label_to_idx\']\n    predicate_to_ind = info[\'predicate_to_idx\']\n    ind_to_classes = sorted(class_to_ind, key=lambda k: class_to_ind[k])\n    ind_to_predicates = sorted(predicate_to_ind, key=lambda k: predicate_to_ind[k])\n\n    return ind_to_classes, ind_to_predicates\n\n\ndef vg_collate(data, num_gpus=3, is_train=False, mode=\'det\'):\n    assert mode in (\'det\', \'rel\')\n    blob = Blob(mode=mode, is_train=is_train, num_gpus=num_gpus,\n                batch_size_per_gpu=len(data) // num_gpus)\n    for d in data:\n        blob.append(d)\n    blob.reduce()\n    return blob\n\n\nclass VGDataLoader(torch.utils.data.DataLoader):\n    """"""\n    Iterates through the data, filtering out None,\n     but also loads everything as a (cuda) variable\n    """"""\n\n    @classmethod\n    def splits(cls, train_data, val_data, batch_size=3, num_workers=1, num_gpus=3, mode=\'det\',\n               **kwargs):\n        assert mode in (\'det\', \'rel\')\n        train_load = cls(\n            dataset=train_data,\n            batch_size=batch_size * num_gpus,\n            shuffle=True,\n            num_workers=num_workers,\n            collate_fn=lambda x: vg_collate(x, mode=mode, num_gpus=num_gpus, is_train=True),\n            drop_last=True,\n            # pin_memory=True,\n            **kwargs,\n        )\n        val_load = cls(\n            dataset=val_data,\n            batch_size=batch_size * num_gpus if mode==\'det\' else num_gpus,\n            shuffle=False,\n            num_workers=num_workers,\n            collate_fn=lambda x: vg_collate(x, mode=mode, num_gpus=num_gpus, is_train=False),\n            drop_last=True,\n            # pin_memory=True,\n            **kwargs,\n        )\n        return train_load, val_load\n'"
lib/__init__.py,0,b''
lib/get_dataset_counts.py,0,"b'""""""\nGet counts of all of the examples in the dataset. Used for creating the baseline\ndictionary model\n""""""\n\nimport numpy as np\nfrom dataloaders.visual_genome import VG\nfrom lib.fpn.box_intersections_cpu.bbox import bbox_overlaps\nfrom lib.pytorch_misc import nonintersecting_2d_inds\n\n\ndef get_counts(train_data=VG(mode=\'train\', filter_duplicate_rels=False, num_val_im=5000), must_overlap=True):\n    """"""\n    Get counts of all of the relations. Used for modeling directly P(rel | o1, o2)\n    :param train_data: \n    :param must_overlap: \n    :return: \n    """"""\n    fg_matrix = np.zeros((\n        train_data.num_classes,\n        train_data.num_classes,\n        train_data.num_predicates,\n    ), dtype=np.int64)\n\n    bg_matrix = np.zeros((\n        train_data.num_classes,\n        train_data.num_classes,\n    ), dtype=np.int64)\n\n    for ex_ind in range(len(train_data)):\n        gt_classes = train_data.gt_classes[ex_ind].copy()\n        gt_relations = train_data.relationships[ex_ind].copy()\n        gt_boxes = train_data.gt_boxes[ex_ind].copy()\n\n        # For the foreground, we\'ll just look at everything\n        o1o2 = gt_classes[gt_relations[:, :2]]\n        for (o1, o2), gtr in zip(o1o2, gt_relations[:,2]):\n            fg_matrix[o1, o2, gtr] += 1\n\n        # For the background, get all of the things that overlap.\n        o1o2_total = gt_classes[np.array(\n            box_filter(gt_boxes, must_overlap=must_overlap), dtype=int)]\n        for (o1, o2) in o1o2_total:\n            bg_matrix[o1, o2] += 1\n\n    return fg_matrix, bg_matrix\n\n\ndef box_filter(boxes, must_overlap=False):\n    """""" Only include boxes that overlap as possible relations. \n    If no overlapping boxes, use all of them.""""""\n    n_cands = boxes.shape[0]\n\n    overlaps = bbox_overlaps(boxes.astype(np.float), boxes.astype(np.float)) > 0\n    np.fill_diagonal(overlaps, 0)\n\n    all_possib = np.ones_like(overlaps, dtype=np.bool)\n    np.fill_diagonal(all_possib, 0)\n\n    if must_overlap:\n        possible_boxes = np.column_stack(np.where(overlaps))\n\n        if possible_boxes.size == 0:\n            possible_boxes = np.column_stack(np.where(all_possib))\n    else:\n        possible_boxes = np.column_stack(np.where(all_possib))\n    return possible_boxes\n\nif __name__ == \'__main__\':\n    fg, bg = get_counts(must_overlap=False)\n'"
lib/get_union_boxes.py,9,"b'""""""\ncredits to https://github.com/ruotianluo/pytorch-faster-rcnn/blob/master/lib/nets/network.py#L91\n""""""\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nfrom lib.fpn.roi_align.functions.roi_align import RoIAlignFunction\nfrom lib.draw_rectangles.draw_rectangles import draw_union_boxes\nimport numpy as np\nfrom torch.nn.modules.module import Module\nfrom torch import nn\nfrom config import BATCHNORM_MOMENTUM\n\nclass UnionBoxesAndFeats(Module):\n    def __init__(self, pooling_size=7, stride=16, dim=256, concat=False, use_feats=True):\n        """"""\n        :param pooling_size: Pool the union boxes to this dimension\n        :param stride: pixel spacing in the entire image\n        :param dim: Dimension of the feats\n        :param concat: Whether to concat (yes) or add (False) the representations\n        """"""\n        super(UnionBoxesAndFeats, self).__init__()\n        \n        self.pooling_size = pooling_size\n        self.stride = stride\n\n        self.dim = dim\n        self.use_feats = use_feats\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(2, dim //2, kernel_size=7, stride=2, padding=3, bias=True),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(dim//2, momentum=BATCHNORM_MOMENTUM),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(dim // 2, dim, kernel_size=3, stride=1, padding=1, bias=True),\n            nn.ReLU(inplace=True),\n            nn.BatchNorm2d(dim, momentum=BATCHNORM_MOMENTUM),\n        )\n        self.concat = concat\n\n    def forward(self, fmap, rois, union_inds):\n        union_pools = union_boxes(fmap, rois, union_inds, pooling_size=self.pooling_size, stride=self.stride)\n        if not self.use_feats:\n            return union_pools.detach()\n\n        pair_rois = torch.cat((rois[:, 1:][union_inds[:, 0]], rois[:, 1:][union_inds[:, 1]]),1).data.cpu().numpy()\n        # rects_np = get_rect_features(pair_rois, self.pooling_size*2-1) - 0.5\n        rects_np = draw_union_boxes(pair_rois, self.pooling_size*4-1) - 0.5\n        rects = Variable(torch.FloatTensor(rects_np).cuda(fmap.get_device()), volatile=fmap.volatile)\n        if self.concat:\n            return torch.cat((union_pools, self.conv(rects)), 1)\n        return union_pools + self.conv(rects)\n\n# def get_rect_features(roi_pairs, pooling_size):\n#     rects_np = draw_union_boxes(roi_pairs, pooling_size)\n#     # add union + intersection\n#     stuff_to_cat = [\n#         rects_np.max(1),\n#         rects_np.min(1),\n#         np.minimum(1-rects_np[:,0], rects_np[:,1]),\n#         np.maximum(1-rects_np[:,0], rects_np[:,1]),\n#         np.minimum(rects_np[:,0], 1-rects_np[:,1]),\n#         np.maximum(rects_np[:,0], 1-rects_np[:,1]),\n#         np.minimum(1-rects_np[:,0], 1-rects_np[:,1]),\n#         np.maximum(1-rects_np[:,0], 1-rects_np[:,1]),\n#     ]\n#     rects_np = np.concatenate([rects_np] + [x[:,None] for x in stuff_to_cat], 1)\n#     return rects_np\n\n\ndef union_boxes(fmap, rois, union_inds, pooling_size=14, stride=16):\n    """"""\n    :param fmap: (batch_size, d, IM_SIZE/stride, IM_SIZE/stride)\n    :param rois: (num_rois, 5) with [im_ind, x1, y1, x2, y2]\n    :param union_inds: (num_urois, 2) with [roi_ind1, roi_ind2]\n    :param pooling_size: we\'ll resize to this\n    :param stride:\n    :return:\n    """"""\n    assert union_inds.size(1) == 2\n    im_inds = rois[:,0][union_inds[:,0]]\n    assert (im_inds.data == rois.data[:,0][union_inds[:,1]]).sum() == union_inds.size(0)\n    union_rois = torch.cat((\n        im_inds[:,None],\n        torch.min(rois[:, 1:3][union_inds[:, 0]], rois[:, 1:3][union_inds[:, 1]]),\n        torch.max(rois[:, 3:5][union_inds[:, 0]], rois[:, 3:5][union_inds[:, 1]]),\n    ),1)\n\n    # (num_rois, d, pooling_size, pooling_size)\n    union_pools = RoIAlignFunction(pooling_size, pooling_size,\n                                   spatial_scale=1/stride)(fmap, union_rois)\n    return union_pools\n \n'"
lib/object_detector.py,16,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\n\nfrom config import ANCHOR_SIZE, ANCHOR_RATIOS, ANCHOR_SCALES\nfrom lib.fpn.generate_anchors import generate_anchors\nfrom lib.fpn.box_utils import bbox_preds, center_size, bbox_overlaps\nfrom lib.fpn.nms.functions.nms import apply_nms\nfrom lib.fpn.proposal_assignments.proposal_assignments_gtbox import proposal_assignments_gtbox\nfrom lib.fpn.proposal_assignments.proposal_assignments_det import proposal_assignments_det\n\nfrom lib.fpn.roi_align.functions.roi_align import RoIAlignFunction\nfrom lib.pytorch_misc import enumerate_by_image, gather_nd, diagonal_inds, Flattener\nfrom torchvision.models.vgg import vgg16\nfrom torchvision.models.resnet import resnet101\nfrom torch.nn.parallel._functions import Gather\n\n\nclass Result(object):\n    """""" little container class for holding the detection result\n        od: object detector, rm: rel model""""""\n\n    def __init__(self, od_obj_dists=None, rm_obj_dists=None,\n                 obj_scores=None, obj_preds=None, obj_fmap=None,\n                 od_box_deltas=None, rm_box_deltas=None,\n                 od_box_targets=None, rm_box_targets=None, od_box_priors=None, rm_box_priors=None,\n                 boxes_assigned=None, boxes_all=None, od_obj_labels=None, rm_obj_labels=None,\n                 rpn_scores=None, rpn_box_deltas=None, rel_labels=None,\n                 im_inds=None, fmap=None, rel_dists=None, rel_inds=None, rel_rep=None):\n        self.__dict__.update(locals())\n        del self.__dict__[\'self\']\n\n    def is_none(self):\n        return all([v is None for k, v in self.__dict__.items() if k != \'self\'])\n\n\ndef gather_res(outputs, target_device, dim=0):\n    """"""\n    Assuming the signatures are the same accross results!\n    """"""\n    out = outputs[0]\n    args = {field: Gather.apply(target_device, dim, *[getattr(o, field) for o in outputs])\n            for field, v in out.__dict__.items() if v is not None}\n    return type(out)(**args)\n\n\nclass ObjectDetector(nn.Module):\n    """"""\n    Core model for doing object detection + getting the visual features. This could be the first step in\n    a pipeline. We can provide GT rois or use the RPN (which would then be classification!)\n    """"""\n    MODES = (\'rpntrain\', \'gtbox\', \'refinerels\', \'proposals\')\n\n    def __init__(self, classes, mode=\'rpntrain\', num_gpus=1, nms_filter_duplicates=True,\n                 max_per_img=64, use_resnet=False, thresh=0.05):\n        """"""\n        :param classes: Object classes\n        :param rel_classes: Relationship classes. None if were not using rel mode\n        :param num_gpus: how many GPUS 2 use\n        """"""\n        super(ObjectDetector, self).__init__()\n\n        if mode not in self.MODES:\n            raise ValueError(""invalid mode"")\n        self.mode = mode\n\n        self.classes = classes\n        self.num_gpus = num_gpus\n        self.pooling_size = 7\n        self.nms_filter_duplicates = nms_filter_duplicates\n        self.max_per_img = max_per_img\n        self.use_resnet = use_resnet\n        self.thresh = thresh\n\n        if not self.use_resnet:\n            vgg_model = load_vgg()\n            self.features = vgg_model.features\n            self.roi_fmap = vgg_model.classifier\n            rpn_input_dim = 512\n            output_dim = 4096\n        else:  # Deprecated\n            self.features = load_resnet()\n            self.compress = nn.Sequential(\n                nn.Conv2d(1024, 256, kernel_size=1),\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(256),\n            )\n            self.roi_fmap = nn.Sequential(\n                nn.Linear(256 * 7 * 7, 2048),\n                nn.SELU(inplace=True),\n                nn.AlphaDropout(p=0.05),\n                nn.Linear(2048, 2048),\n                nn.SELU(inplace=True),\n                nn.AlphaDropout(p=0.05),\n            )\n            rpn_input_dim = 1024\n            output_dim = 2048\n\n        self.score_fc = nn.Linear(output_dim, self.num_classes)\n        self.bbox_fc = nn.Linear(output_dim, self.num_classes * 4)\n        self.rpn_head = RPNHead(dim=512, input_dim=rpn_input_dim)\n\n    @property\n    def num_classes(self):\n        return len(self.classes)\n\n    def feature_map(self, x):\n        """"""\n        Produces feature map from the input image\n        :param x: [batch_size, 3, size, size] float32 padded image\n        :return: Feature maps at 1/16 the original size.\n        Each one is [batch_size, dim, IM_SIZE/k, IM_SIZE/k].\n        """"""\n        if not self.use_resnet:\n            return self.features(x)  # Uncomment this for ""stanford"" setting in which it\'s frozen:      .detach()\n        x = self.features.conv1(x)\n        x = self.features.bn1(x)\n        x = self.features.relu(x)\n        x = self.features.maxpool(x)\n\n        c2 = self.features.layer1(x)\n        c3 = self.features.layer2(c2)\n        c4 = self.features.layer3(c3)\n        return c4\n\n    def obj_feature_map(self, features, rois):\n        """"""\n        Gets the ROI features\n        :param features: [batch_size, dim, IM_SIZE/4, IM_SIZE/4] (features at level p2)\n        :param rois: [num_rois, 5] array of [img_num, x0, y0, x1, y1].\n        :return: [num_rois, #dim] array\n        """"""\n        feature_pool = RoIAlignFunction(self.pooling_size, self.pooling_size, spatial_scale=1 / 16)(\n            self.compress(features) if self.use_resnet else features, rois)\n        return self.roi_fmap(feature_pool.view(rois.size(0), -1))\n\n    def rpn_boxes(self, fmap, im_sizes, image_offset, gt_boxes=None, gt_classes=None, gt_rels=None,\n                  train_anchor_inds=None, proposals=None):\n        """"""\n        Gets boxes from the RPN\n        :param fmap:\n        :param im_sizes:\n        :param image_offset:\n        :param gt_boxes:\n        :param gt_classes:\n        :param gt_rels:\n        :param train_anchor_inds:\n        :return:\n        """"""\n        rpn_feats = self.rpn_head(fmap)\n        rois = self.rpn_head.roi_proposals(\n            rpn_feats, im_sizes, nms_thresh=0.7,\n            pre_nms_topn=12000 if self.training and self.mode == \'rpntrain\' else 6000,\n            post_nms_topn=2000 if self.training and self.mode == \'rpntrain\' else 1000,\n        )\n        if self.training:\n            if gt_boxes is None or gt_classes is None or train_anchor_inds is None:\n                raise ValueError(\n                    ""Must supply GT boxes, GT classes, trainanchors when in train mode"")\n            rpn_scores, rpn_box_deltas = self.rpn_head.anchor_preds(rpn_feats, train_anchor_inds,\n                                                                    image_offset)\n\n            if gt_rels is not None and self.mode == \'rpntrain\':\n                raise ValueError(""Training the object detector and the relationship model with detection""\n                                 ""at the same time isn\'t supported"")\n\n            if self.mode == \'refinerels\':\n                all_rois = Variable(rois)\n                # Potentially you could add in GT rois if none match\n                # is_match = (bbox_overlaps(rois[:,1:].contiguous(), gt_boxes.data) > 0.5).long()\n                # gt_not_matched = (is_match.sum(0) == 0).nonzero()\n                #\n                # if gt_not_matched.dim() > 0:\n                #     gt_to_add = torch.cat((gt_classes[:,0,None][gt_not_matched.squeeze(1)].float(),\n                #                            gt_boxes[gt_not_matched.squeeze(1)]), 1)\n                #\n                #     all_rois = torch.cat((all_rois, gt_to_add),0)\n                #     num_gt = gt_to_add.size(0)\n                labels = None\n                bbox_targets = None\n                rel_labels = None\n            else:\n                all_rois, labels, bbox_targets = proposal_assignments_det(\n                    rois, gt_boxes.data, gt_classes.data, image_offset, fg_thresh=0.5)\n                rel_labels = None\n\n        else:\n            all_rois = Variable(rois, volatile=True)\n            labels = None\n            bbox_targets = None\n            rel_labels = None\n            rpn_box_deltas = None\n            rpn_scores = None\n\n        return all_rois, labels, bbox_targets, rpn_scores, rpn_box_deltas, rel_labels\n\n    def gt_boxes(self, fmap, im_sizes, image_offset, gt_boxes=None, gt_classes=None, gt_rels=None,\n                 train_anchor_inds=None, proposals=None):\n        """"""\n        Gets GT boxes!\n        :param fmap:\n        :param im_sizes:\n        :param image_offset:\n        :param gt_boxes:\n        :param gt_classes:\n        :param gt_rels:\n        :param train_anchor_inds:\n        :return:\n        """"""\n        assert gt_boxes is not None\n        im_inds = gt_classes[:, 0] - image_offset\n        rois = torch.cat((im_inds.float()[:, None], gt_boxes), 1)\n        if gt_rels is not None and self.training:\n            rois, labels, rel_labels = proposal_assignments_gtbox(\n                rois.data, gt_boxes.data, gt_classes.data, gt_rels.data, image_offset,\n                fg_thresh=0.5)\n        else:\n            labels = gt_classes[:, 1]\n            rel_labels = None\n\n        return rois, labels, None, None, None, rel_labels\n\n    def proposal_boxes(self, fmap, im_sizes, image_offset, gt_boxes=None, gt_classes=None, gt_rels=None,\n                       train_anchor_inds=None, proposals=None):\n        """"""\n        Gets boxes from the RPN\n        :param fmap:\n        :param im_sizes:\n        :param image_offset:\n        :param gt_boxes:\n        :param gt_classes:\n        :param gt_rels:\n        :param train_anchor_inds:\n        :return:\n        """"""\n        assert proposals is not None\n\n        rois = filter_roi_proposals(proposals[:, 2:].data.contiguous(), proposals[:, 1].data.contiguous(),\n                                    np.array([2000] * len(im_sizes)),\n                                    nms_thresh=0.7,\n                                    pre_nms_topn=12000 if self.training and self.mode == \'rpntrain\' else 6000,\n                                    post_nms_topn=2000 if self.training and self.mode == \'rpntrain\' else 1000,\n                                    )\n        if self.training:\n            all_rois, labels, bbox_targets = proposal_assignments_det(\n                rois, gt_boxes.data, gt_classes.data, image_offset, fg_thresh=0.5)\n\n            # RETRAINING FOR DETECTION HERE.\n            all_rois = torch.cat((all_rois, Variable(rois)), 0)\n        else:\n            all_rois = Variable(rois, volatile=True)\n            labels = None\n            bbox_targets = None\n\n        rpn_scores = None\n        rpn_box_deltas = None\n        rel_labels = None\n\n        return all_rois, labels, bbox_targets, rpn_scores, rpn_box_deltas, rel_labels\n\n    def get_boxes(self, *args, **kwargs):\n        if self.mode == \'gtbox\':\n            fn = self.gt_boxes\n        elif self.mode == \'proposals\':\n            assert kwargs[\'proposals\'] is not None\n            fn = self.proposal_boxes\n        else:\n            fn = self.rpn_boxes\n        return fn(*args, **kwargs)\n\n    def forward(self, x, im_sizes, image_offset,\n                gt_boxes=None, gt_classes=None, gt_rels=None, proposals=None, train_anchor_inds=None,\n                return_fmap=False):\n        """"""\n        Forward pass for detection\n        :param x: Images@[batch_size, 3, IM_SIZE, IM_SIZE]\n        :param im_sizes: A numpy array of (h, w, scale) for each image.\n        :param image_offset: Offset onto what image we\'re on for MGPU training (if single GPU this is 0)\n        :param gt_boxes:\n\n        Training parameters:\n        :param gt_boxes: [num_gt, 4] GT boxes over the batch.\n        :param gt_classes: [num_gt, 2] gt boxes where each one is (img_id, class)\n        :param proposals: things\n        :param train_anchor_inds: a [num_train, 2] array of indices for the anchors that will\n                                  be used to compute the training loss. Each (img_ind, fpn_idx)\n        :return: If train:\n        """"""\n        fmap = self.feature_map(x)\n\n        # Get boxes from RPN\n        rois, obj_labels, bbox_targets, rpn_scores, rpn_box_deltas, rel_labels = \\\n            self.get_boxes(fmap, im_sizes, image_offset, gt_boxes,\n                           gt_classes, gt_rels, train_anchor_inds, proposals=proposals)\n\n        # Now classify them\n        obj_fmap = self.obj_feature_map(fmap, rois)\n        od_obj_dists = self.score_fc(obj_fmap)\n        od_box_deltas = self.bbox_fc(obj_fmap).view(\n            -1, len(self.classes), 4) if self.mode != \'gtbox\' else None\n\n        od_box_priors = rois[:, 1:]\n\n        if (not self.training and not self.mode == \'gtbox\') or self.mode in (\'proposals\', \'refinerels\'):\n            nms_inds, nms_scores, nms_preds, nms_boxes_assign, nms_boxes, nms_imgs = self.nms_boxes(\n                od_obj_dists,\n                rois,\n                od_box_deltas, im_sizes,\n            )\n            im_inds = nms_imgs + image_offset\n            obj_dists = od_obj_dists[nms_inds]\n            obj_fmap = obj_fmap[nms_inds]\n            box_deltas = od_box_deltas[nms_inds]\n            box_priors = nms_boxes[:, 0]\n\n            if self.training and not self.mode == \'gtbox\':\n                # NOTE: If we\'re doing this during training, we need to assign labels here.\n                pred_to_gtbox = bbox_overlaps(box_priors, gt_boxes).data\n                pred_to_gtbox[im_inds.data[:, None] != gt_classes.data[None, :, 0]] = 0.0\n\n                max_overlaps, argmax_overlaps = pred_to_gtbox.max(1)\n                rm_obj_labels = gt_classes[:, 1][argmax_overlaps]\n                rm_obj_labels[max_overlaps < 0.5] = 0\n            else:\n                rm_obj_labels = None\n        else:\n            im_inds = rois[:, 0].long().contiguous() + image_offset\n            nms_scores = None\n            nms_preds = None\n            nms_boxes_assign = None\n            nms_boxes = None\n            box_priors = rois[:, 1:]\n            rm_obj_labels = obj_labels\n            box_deltas = od_box_deltas\n            obj_dists = od_obj_dists\n\n        return Result(\n            od_obj_dists=od_obj_dists,\n            rm_obj_dists=obj_dists,\n            obj_scores=nms_scores,\n            obj_preds=nms_preds,\n            obj_fmap=obj_fmap,\n            od_box_deltas=od_box_deltas,\n            rm_box_deltas=box_deltas,\n            od_box_targets=bbox_targets,\n            rm_box_targets=bbox_targets,\n            od_box_priors=od_box_priors,\n            rm_box_priors=box_priors,\n            boxes_assigned=nms_boxes_assign,\n            boxes_all=nms_boxes,\n            od_obj_labels=obj_labels,\n            rm_obj_labels=rm_obj_labels,\n            rpn_scores=rpn_scores,\n            rpn_box_deltas=rpn_box_deltas,\n            rel_labels=rel_labels,\n            im_inds=im_inds,\n            fmap=fmap if return_fmap else None,\n        )\n\n    def nms_boxes(self, obj_dists, rois, box_deltas, im_sizes):\n        """"""\n        Performs NMS on the boxes\n        :param obj_dists: [#rois, #classes]\n        :param rois: [#rois, 5]\n        :param box_deltas: [#rois, #classes, 4]\n        :param im_sizes: sizes of images\n        :return\n            nms_inds [#nms]\n            nms_scores [#nms]\n            nms_labels [#nms]\n            nms_boxes_assign [#nms, 4]\n            nms_boxes  [#nms, #classes, 4]. classid=0 is the box prior.\n        """"""\n        # Now produce the boxes\n        # box deltas is (num_rois, num_classes, 4) but rois is only #(num_rois, 4)\n        boxes = bbox_preds(rois[:, None, 1:].expand_as(box_deltas).contiguous().view(-1, 4),\n                           box_deltas.view(-1, 4)).view(*box_deltas.size())\n\n        # Clip the boxes and get the best N dets per image.\n        inds = rois[:, 0].long().contiguous()\n        dets = []\n        for i, s, e in enumerate_by_image(inds.data):\n            h, w = im_sizes[i, :2]\n            boxes[s:e, :, 0].data.clamp_(min=0, max=w - 1)\n            boxes[s:e, :, 1].data.clamp_(min=0, max=h - 1)\n            boxes[s:e, :, 2].data.clamp_(min=0, max=w - 1)\n            boxes[s:e, :, 3].data.clamp_(min=0, max=h - 1)\n            d_filtered = filter_det(\n                F.softmax(obj_dists[s:e], 1), boxes[s:e], start_ind=s,\n                nms_filter_duplicates=self.nms_filter_duplicates,\n                max_per_img=self.max_per_img,\n                thresh=self.thresh,\n            )\n            if d_filtered is not None:\n                dets.append(d_filtered)\n\n        if len(dets) == 0:\n            print(""nothing was detected"", flush=True)\n            return None\n        nms_inds, nms_scores, nms_labels = [torch.cat(x, 0) for x in zip(*dets)]\n        twod_inds = nms_inds * boxes.size(1) + nms_labels.data\n        nms_boxes_assign = boxes.view(-1, 4)[twod_inds]\n\n        nms_boxes = torch.cat((rois[:, 1:][nms_inds][:, None], boxes[nms_inds][:, 1:]), 1)\n        return nms_inds, nms_scores, nms_labels, nms_boxes_assign, nms_boxes, inds[nms_inds]\n\n    def __getitem__(self, batch):\n        """""" Hack to do multi-GPU training""""""\n        batch.scatter()\n        if self.num_gpus == 1:\n            return self(*batch[0])\n\n        replicas = nn.parallel.replicate(self, devices=list(range(self.num_gpus)))\n        outputs = nn.parallel.parallel_apply(replicas, [batch[i] for i in range(self.num_gpus)])\n\n        if any([x.is_none() for x in outputs]):\n            assert not self.training\n            return None\n        return gather_res(outputs, 0, dim=0)\n\n\ndef filter_det(scores, boxes, start_ind=0, max_per_img=100, thresh=0.001, pre_nms_topn=6000,\n               post_nms_topn=300, nms_thresh=0.3, nms_filter_duplicates=True):\n    """"""\n    Filters the detections for a single image\n    :param scores: [num_rois, num_classes]\n    :param boxes: [num_rois, num_classes, 4]. Assumes the boxes have been clamped\n    :param max_per_img: Max detections per image\n    :param thresh: Threshold for calling it a good box\n    :param nms_filter_duplicates: True if we shouldn\'t allow for mulitple detections of the\n           same box (with different labels)\n    :return: A numpy concatenated array with up to 100 detections/img [num_im, x1, y1, x2, y2, score, cls]\n    """"""\n\n    valid_cls = (scores[:, 1:].data.max(0)[0] > thresh).nonzero() + 1\n    if valid_cls.dim() == 0:\n        return None\n\n    nms_mask = scores.data.clone()\n    nms_mask.zero_()\n\n    for c_i in valid_cls.squeeze(1).cpu():\n        scores_ci = scores.data[:, c_i]\n        boxes_ci = boxes.data[:, c_i]\n\n        keep = apply_nms(scores_ci, boxes_ci,\n                         pre_nms_topn=pre_nms_topn, post_nms_topn=post_nms_topn,\n                         nms_thresh=nms_thresh)\n        nms_mask[:, c_i][keep] = 1\n\n    dists_all = Variable(nms_mask * scores.data, volatile=True)\n\n    if nms_filter_duplicates:\n        scores_pre, labels_pre = dists_all.data.max(1)\n        inds_all = scores_pre.nonzero()\n        assert inds_all.dim() != 0\n        inds_all = inds_all.squeeze(1)\n\n        labels_all = labels_pre[inds_all]\n        scores_all = scores_pre[inds_all]\n    else:\n        nz = nms_mask.nonzero()\n        assert nz.dim() != 0\n        inds_all = nz[:, 0]\n        labels_all = nz[:, 1]\n        scores_all = scores.data.view(-1)[inds_all * scores.data.size(1) + labels_all]\n\n    # dists_all = dists_all[inds_all]\n    # dists_all[:,0] = 1.0-dists_all.sum(1)\n\n    # # Limit to max per image detections\n    vs, idx = torch.sort(scores_all, dim=0, descending=True)\n    idx = idx[vs > thresh]\n    if max_per_img < idx.size(0):\n        idx = idx[:max_per_img]\n\n    inds_all = inds_all[idx] + start_ind\n    scores_all = Variable(scores_all[idx], volatile=True)\n    labels_all = Variable(labels_all[idx], volatile=True)\n    # dists_all = dists_all[idx]\n\n    return inds_all, scores_all, labels_all\n\n\nclass RPNHead(nn.Module):\n    """"""\n    Serves as the class + box outputs for each level in the FPN.\n    """"""\n\n    def __init__(self, dim=512, input_dim=1024):\n        """"""\n        :param aspect_ratios: Aspect ratios for the anchors. NOTE - this can\'t be changed now\n               as it depends on other things in the C code...\n        """"""\n        super(RPNHead, self).__init__()\n\n        self.anchor_target_dim = 6\n        self.stride = 16\n\n        self.conv = nn.Sequential(\n            nn.Conv2d(input_dim, dim, kernel_size=3, padding=1),\n            nn.ReLU6(inplace=True),  # Tensorflow docs use Relu6, so let\'s use it too....\n            nn.Conv2d(dim, self.anchor_target_dim * self._A,\n                      kernel_size=1)\n        )\n\n        ans_np = generate_anchors(base_size=ANCHOR_SIZE,\n                                  feat_stride=self.stride,\n                                  anchor_scales=ANCHOR_SCALES,\n                                  anchor_ratios=ANCHOR_RATIOS,\n                                  )\n        self.register_buffer(\'anchors\', torch.FloatTensor(ans_np))\n\n    @property\n    def _A(self):\n        return len(ANCHOR_RATIOS) * len(ANCHOR_SCALES)\n\n    def forward(self, fmap):\n        """"""\n        Gets the class / noclass predictions over all the scales\n\n        :param fmap: [batch_size, dim, IM_SIZE/16, IM_SIZE/16] featuremap\n        :return: [batch_size, IM_SIZE/16, IM_SIZE/16, A, 6]\n        """"""\n        rez = self._reshape_channels(self.conv(fmap))\n        rez = rez.view(rez.size(0), rez.size(1), rez.size(2),\n                       self._A, self.anchor_target_dim)\n        return rez\n\n    def anchor_preds(self, preds, train_anchor_inds, image_offset):\n        """"""\n        Get predictions for the training indices\n        :param preds: [batch_size, IM_SIZE/16, IM_SIZE/16, A, 6]\n        :param train_anchor_inds: [num_train, 4] indices into the predictions\n        :return: class_preds: [num_train, 2] array of yes/no\n                 box_preds:   [num_train, 4] array of predicted boxes\n        """"""\n        assert train_anchor_inds.size(1) == 4\n        tai = train_anchor_inds.data.clone()\n        tai[:, 0] -= image_offset\n        train_regions = gather_nd(preds, tai)\n\n        class_preds = train_regions[:, :2]\n        box_preds = train_regions[:, 2:]\n        return class_preds, box_preds\n\n    @staticmethod\n    def _reshape_channels(x):\n        """""" [batch_size, channels, h, w] -> [batch_size, h, w, channels] """"""\n        assert x.dim() == 4\n        batch_size, nc, h, w = x.size()\n\n        x_t = x.view(batch_size, nc, -1).transpose(1, 2).contiguous()\n        x_t = x_t.view(batch_size, h, w, nc)\n        return x_t\n\n    def roi_proposals(self, fmap, im_sizes, nms_thresh=0.7, pre_nms_topn=12000, post_nms_topn=2000):\n        """"""\n        :param fmap: [batch_size, IM_SIZE/16, IM_SIZE/16, A, 6]\n        :param im_sizes:        [batch_size, 3] numpy array of (h, w, scale)\n        :return: ROIS: shape [a <=post_nms_topn, 5] array of ROIS.\n        """"""\n        class_fmap = fmap[:, :, :, :, :2].contiguous()\n\n        # GET THE GOOD BOXES AYY LMAO :\')\n        class_preds = F.softmax(class_fmap, 4)[..., 1].data.contiguous()\n\n        box_fmap = fmap[:, :, :, :, 2:].data.contiguous()\n\n        anchor_stacked = torch.cat([self.anchors[None]] * fmap.size(0), 0)\n        box_preds = bbox_preds(anchor_stacked.view(-1, 4), box_fmap.view(-1, 4)).view(\n            *box_fmap.size())\n\n        for i, (h, w, scale) in enumerate(im_sizes):\n            # Zero out all the bad boxes h, w, A, 4\n            h_end = int(h) // self.stride\n            w_end = int(w) // self.stride\n            if h_end < class_preds.size(1):\n                class_preds[i, h_end:] = -0.01\n            if w_end < class_preds.size(2):\n                class_preds[i, :, w_end:] = -0.01\n\n            # and clamp the others\n            box_preds[i, :, :, :, 0].clamp_(min=0, max=w - 1)\n            box_preds[i, :, :, :, 1].clamp_(min=0, max=h - 1)\n            box_preds[i, :, :, :, 2].clamp_(min=0, max=w - 1)\n            box_preds[i, :, :, :, 3].clamp_(min=0, max=h - 1)\n\n        sizes = center_size(box_preds.view(-1, 4))\n        class_preds.view(-1)[(sizes[:, 2] < 4) | (sizes[:, 3] < 4)] = -0.01\n        return filter_roi_proposals(box_preds.view(-1, 4), class_preds.view(-1),\n                                    boxes_per_im=np.array([np.prod(box_preds.size()[1:-1])] * fmap.size(0)),\n                                    nms_thresh=nms_thresh,\n                                    pre_nms_topn=pre_nms_topn, post_nms_topn=post_nms_topn)\n\n\ndef filter_roi_proposals(box_preds, class_preds, boxes_per_im, nms_thresh=0.7, pre_nms_topn=12000, post_nms_topn=2000):\n    inds, im_per = apply_nms(\n        class_preds,\n        box_preds,\n        pre_nms_topn=pre_nms_topn,\n        post_nms_topn=post_nms_topn,\n        boxes_per_im=boxes_per_im,\n        nms_thresh=nms_thresh,\n    )\n    img_inds = torch.cat([val * torch.ones(i) for val, i in enumerate(im_per)], 0).cuda(\n        box_preds.get_device())\n    rois = torch.cat((img_inds[:, None], box_preds[inds]), 1)\n    return rois\n\n\ndef load_resnet():\n    model = resnet101(pretrained=True)\n    del model.layer4\n    del model.avgpool\n    del model.fc\n    return model\n\n\ndef load_vgg(use_dropout=True, use_relu=True, use_linear=True, pretrained=True):\n    model = vgg16(pretrained=pretrained)\n    del model.features._modules[\'30\']  # Get rid of the maxpool\n    del model.classifier._modules[\'6\']  # Get rid of class layer\n    if not use_dropout:\n        del model.classifier._modules[\'5\']  # Get rid of dropout\n        if not use_relu:\n            del model.classifier._modules[\'4\']  # Get rid of relu activation\n            if not use_linear:\n                del model.classifier._modules[\'3\']  # Get rid of linear layer\n    return model\n'"
lib/pytorch_misc.py,14,"b'""""""\nMiscellaneous functions that might be useful for pytorch\n""""""\n\nimport h5py\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nimport os\nimport dill as pkl\nfrom itertools import tee\nfrom torch import nn\n\ndef optimistic_restore(network, state_dict):\n    mismatch = False\n    own_state = network.state_dict()\n    for name, param in state_dict.items():\n        if name not in own_state:\n            print(""Unexpected key {} in state_dict with size {}"".format(name, param.size()))\n            mismatch = True\n        elif param.size() == own_state[name].size():\n            own_state[name].copy_(param)\n        else:\n            print(""Network has {} with size {}, ckpt has {}"".format(name,\n                                                                    own_state[name].size(),\n                                                                    param.size()))\n            mismatch = True\n\n    missing = set(own_state.keys()) - set(state_dict.keys())\n    if len(missing) > 0:\n        print(""We couldn\'t find {}"".format(\',\'.join(missing)))\n        mismatch = True\n    return not mismatch\n\n\ndef pairwise(iterable):\n    ""s -> (s0,s1), (s1,s2), (s2, s3), ...""\n    a, b = tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\n\ndef get_ranking(predictions, labels, num_guesses=5):\n    """"""\n    Given a matrix of predictions and labels for the correct ones, get the number of guesses\n    required to get the prediction right per example.\n    :param predictions: [batch_size, range_size] predictions\n    :param labels: [batch_size] array of labels\n    :param num_guesses: Number of guesses to return\n    :return:\n    """"""\n    assert labels.size(0) == predictions.size(0)\n    assert labels.dim() == 1\n    assert predictions.dim() == 2\n\n    values, full_guesses = predictions.topk(predictions.size(1), dim=1)\n    _, ranking = full_guesses.topk(full_guesses.size(1), dim=1, largest=False)\n    gt_ranks = torch.gather(ranking.data, 1, labels.data[:, None]).squeeze()\n\n    guesses = full_guesses[:, :num_guesses]\n    return gt_ranks, guesses\n\ndef cache(f):\n    """"""\n    Caches a computation\n    """"""\n    def cache_wrapper(fn, *args, **kwargs):\n        if os.path.exists(fn):\n            with open(fn, \'rb\') as file:\n                data = pkl.load(file)\n        else:\n            print(""file {} not found, so rebuilding"".format(fn))\n            data = f(*args, **kwargs)\n            with open(fn, \'wb\') as file:\n                pkl.dump(data, file)\n        return data\n    return cache_wrapper\n\n\nclass Flattener(nn.Module):\n    def __init__(self):\n        """"""\n        Flattens last 3 dimensions to make it only batch size, -1\n        """"""\n        super(Flattener, self).__init__()\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n\ndef to_variable(f):\n    """"""\n    Decorator that pushes all the outputs to a variable\n    :param f: \n    :return: \n    """"""\n    def variable_wrapper(*args, **kwargs):\n        rez = f(*args, **kwargs)\n        if isinstance(rez, tuple):\n            return tuple([Variable(x) for x in rez])\n        return Variable(rez)\n    return variable_wrapper\n\ndef arange(base_tensor, n=None):\n    new_size = base_tensor.size(0) if n is None else n\n    new_vec = base_tensor.new(new_size).long()\n    torch.arange(0, new_size, out=new_vec)\n    return new_vec\n\n\ndef to_onehot(vec, num_classes, fill=1000):\n    """"""\n    Creates a [size, num_classes] torch FloatTensor where\n    one_hot[i, vec[i]] = fill\n    \n    :param vec: 1d torch tensor\n    :param num_classes: int\n    :param fill: value that we want + and - things to be.\n    :return: \n    """"""\n    onehot_result = vec.new(vec.size(0), num_classes).float().fill_(-fill)\n    arange_inds = vec.new(vec.size(0)).long()\n    torch.arange(0, vec.size(0), out=arange_inds)\n\n    onehot_result.view(-1)[vec + num_classes*arange_inds] = fill\n    return onehot_result\n\ndef save_net(fname, net):\n    h5f = h5py.File(fname, mode=\'w\')\n    for k, v in list(net.state_dict().items()):\n        h5f.create_dataset(k, data=v.cpu().numpy())\n\n\ndef load_net(fname, net):\n    h5f = h5py.File(fname, mode=\'r\')\n    for k, v in list(net.state_dict().items()):\n        param = torch.from_numpy(np.asarray(h5f[k]))\n\n        if v.size() != param.size():\n            print(""On k={} desired size is {} but supplied {}"".format(k, v.size(), param.size()))\n        else:\n            v.copy_(param)\n\n\ndef batch_index_iterator(len_l, batch_size, skip_end=True):\n    """"""\n    Provides indices that iterate over a list\n    :param len_l: int representing size of thing that we will\n        iterate over\n    :param batch_size: size of each batch\n    :param skip_end: if true, don\'t iterate over the last batch\n    :return: A generator that returns (start, end) tuples\n        as it goes through all batches\n    """"""\n    iterate_until = len_l\n    if skip_end:\n        iterate_until = (len_l // batch_size) * batch_size\n\n    for b_start in range(0, iterate_until, batch_size):\n        yield (b_start, min(b_start+batch_size, len_l))\n\ndef batch_map(f, a, batch_size):\n    """"""\n    Maps f over the array a in chunks of batch_size.\n    :param f: function to be applied. Must take in a block of\n            (batch_size, dim_a) and map it to (batch_size, something).\n    :param a: Array to be applied over of shape (num_rows, dim_a).\n    :param batch_size: size of each array\n    :return: Array of size (num_rows, something).\n    """"""\n    rez = []\n    for s, e in batch_index_iterator(a.size(0), batch_size, skip_end=False):\n        print(""Calling on {}"".format(a[s:e].size()))\n        rez.append(f(a[s:e]))\n\n    return torch.cat(rez)\n\n\ndef const_row(fill, l, volatile=False):\n    input_tok = Variable(torch.LongTensor([fill] * l),volatile=volatile)\n    if torch.cuda.is_available():\n        input_tok = input_tok.cuda()\n    return input_tok\n\n\ndef print_para(model):\n    """"""\n    Prints parameters of a model\n    :param opt:\n    :return:\n    """"""\n    st = {}\n    strings = []\n    total_params = 0\n    for p_name, p in model.named_parameters():\n\n        if not (\'bias\' in p_name.split(\'.\')[-1] or \'bn\' in p_name.split(\'.\')[-1]):\n            st[p_name] = ([str(x) for x in p.size()], np.prod(p.size()), p.requires_grad)\n        total_params += np.prod(p.size())\n    for p_name, (size, prod, p_req_grad) in sorted(st.items(), key=lambda x: -x[1][1]):\n        strings.append(""{:<50s}: {:<16s}({:8d}) ({})"".format(\n            p_name, \'[{}]\'.format(\',\'.join(size)), prod, \'grad\' if p_req_grad else \'    \'\n        ))\n    return \'\\n {:.1f}M total parameters \\n ----- \\n \\n{}\'.format(total_params / 1000000.0, \'\\n\'.join(strings))\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\ndef nonintersecting_2d_inds(x):\n    """"""\n    Returns np.array([(a,b) for a in range(x) for b in range(x) if a != b]) efficiently\n    :param x: Size\n    :return: a x*(x-1) array that is [(0,1), (0,2)... (0, x-1), (1,0), (1,2), ..., (x-1, x-2)]\n    """"""\n    rs = 1 - np.diag(np.ones(x, dtype=np.int32))\n    relations = np.column_stack(np.where(rs))\n    return relations\n\n\ndef intersect_2d(x1, x2):\n    """"""\n    Given two arrays [m1, n], [m2,n], returns a [m1, m2] array where each entry is True if those\n    rows match.\n    :param x1: [m1, n] numpy array\n    :param x2: [m2, n] numpy array\n    :return: [m1, m2] bool array of the intersections\n    """"""\n    if x1.shape[1] != x2.shape[1]:\n        raise ValueError(""Input arrays must have same #columns"")\n\n    # This performs a matrix multiplication-esque thing between the two arrays\n    # Instead of summing, we want the equality, so we reduce in that way\n    res = (x1[..., None] == x2.T[None, ...]).all(1)\n    return res\n\ndef np_to_variable(x, is_cuda=True, dtype=torch.FloatTensor):\n    v = Variable(torch.from_numpy(x).type(dtype))\n    if is_cuda:\n        v = v.cuda()\n    return v\n\ndef gather_nd(x, index):\n    """"""\n\n    :param x: n dimensional tensor [x0, x1, x2, ... x{n-1}, dim]\n    :param index: [num, n-1] where each row contains the indices we\'ll use\n    :return: [num, dim]\n    """"""\n    nd = x.dim() - 1\n    assert nd > 0\n    assert index.dim() == 2\n    assert index.size(1) == nd\n    dim = x.size(-1)\n\n    sel_inds = index[:,nd-1].clone()\n    mult_factor = x.size(nd-1)\n    for col in range(nd-2, -1, -1): # [n-2, n-3, ..., 1, 0]\n        sel_inds += index[:,col] * mult_factor\n        mult_factor *= x.size(col)\n\n    grouped = x.view(-1, dim)[sel_inds]\n    return grouped\n\n\ndef enumerate_by_image(im_inds):\n    im_inds_np = im_inds.cpu().numpy()\n    initial_ind = int(im_inds_np[0])\n    s = 0\n    for i, val in enumerate(im_inds_np):\n        if val != initial_ind:\n            yield initial_ind, s, i\n            initial_ind = int(val)\n            s = i\n    yield initial_ind, s, len(im_inds_np)\n    # num_im = im_inds[-1] + 1\n    # # print(""Num im is {}"".format(num_im))\n    # for i in range(num_im):\n    #     # print(""On i={}"".format(i))\n    #     inds_i = (im_inds == i).nonzero()\n    #     if inds_i.dim() == 0:\n    #         continue\n    #     inds_i = inds_i.squeeze(1)\n    #     s = inds_i[0]\n    #     e = inds_i[-1] + 1\n    #     # print(""On i={} we have s={} e={}"".format(i, s, e))\n    #     yield i, s, e\n\ndef diagonal_inds(tensor):\n    """"""\n    Returns the indices required to go along first 2 dims of tensor in diag fashion\n    :param tensor: thing\n    :return: \n    """"""\n    assert tensor.dim() >= 2\n    assert tensor.size(0) == tensor.size(1)\n    size = tensor.size(0)\n    arange_inds = tensor.new(size).long()\n    torch.arange(0, tensor.size(0), out=arange_inds)\n    return (size+1)*arange_inds\n\ndef enumerate_imsize(im_sizes):\n    s = 0\n    for i, (h, w, scale, num_anchors) in enumerate(im_sizes):\n        na = int(num_anchors)\n        e = s + na\n        yield i, s, e, h, w, scale, na\n\n        s = e\n\ndef argsort_desc(scores):\n    """"""\n    Returns the indices that sort scores descending in a smart way\n    :param scores: Numpy array of arbitrary size\n    :return: an array of size [numel(scores), dim(scores)] where each row is the index you\'d\n             need to get the score.\n    """"""\n    return np.column_stack(np.unravel_index(np.argsort(-scores.ravel()), scores.shape))\n\n\ndef unravel_index(index, dims):\n    unraveled = []\n    index_cp = index.clone()\n    for d in dims[::-1]:\n        unraveled.append(index_cp % d)\n        index_cp /= d\n    return torch.cat([x[:,None] for x in unraveled[::-1]], 1)\n\ndef de_chunkize(tensor, chunks):\n    s = 0\n    for c in chunks:\n        yield tensor[s:(s+c)]\n        s = s+c\n\ndef random_choose(tensor, num):\n    ""randomly choose indices""\n    num_choose = min(tensor.size(0), num)\n    if num_choose == tensor.size(0):\n        return tensor\n\n    # Gotta do this in numpy because of https://github.com/pytorch/pytorch/issues/1868\n    rand_idx = np.random.choice(tensor.size(0), size=num, replace=False)\n    rand_idx = torch.LongTensor(rand_idx).cuda(tensor.get_device())\n    chosen = tensor[rand_idx].contiguous()\n\n    # rand_values = tensor.new(tensor.size(0)).float().normal_()\n    # _, idx = torch.sort(rand_values)\n    #\n    # chosen = tensor[idx[:num]].contiguous()\n    return chosen\n\n\ndef transpose_packed_sequence_inds(lengths):\n    """"""\n    Goes from a TxB packed sequence to a BxT or vice versa. Assumes that nothing is a variable\n    :param ps: PackedSequence\n    :return:\n    """"""\n\n    new_inds = []\n    new_lens = []\n    cum_add = np.cumsum([0] + lengths)\n    max_len = lengths[0]\n    length_pointer = len(lengths) - 1\n    for i in range(max_len):\n        while length_pointer > 0 and lengths[length_pointer] <= i:\n            length_pointer -= 1\n        new_inds.append(cum_add[:(length_pointer+1)].copy())\n        cum_add[:(length_pointer+1)] += 1\n        new_lens.append(length_pointer+1)\n    new_inds = np.concatenate(new_inds, 0)\n    return new_inds, new_lens\n\n\ndef right_shift_packed_sequence_inds(lengths):\n    """"""\n    :param lengths: e.g. [2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1]\n    :return: perm indices for the old stuff (TxB) to shift it right 1 slot so as to accomodate\n             BOS toks\n             \n             visual example: of lengths = [4,3,1,1]\n    before:\n    \n        a (0)  b (4)  c (7) d (8)\n        a (1)  b (5)\n        a (2)  b (6)\n        a (3)\n        \n    after:\n    \n        bos a (0)  b (4)  c (7)\n        bos a (1)\n        bos a (2)\n        bos              \n    """"""\n    cur_ind = 0\n    inds = []\n    for (l1, l2) in zip(lengths[:-1], lengths[1:]):\n        for i in range(l2):\n            inds.append(cur_ind + i)\n        cur_ind += l1\n    return inds\n\ndef clip_grad_norm(named_parameters, max_norm, clip=False, verbose=False):\n    r""""""Clips gradient norm of an iterable of parameters.\n\n    The norm is computed over all gradients together, as if they were\n    concatenated into a single vector. Gradients are modified in-place.\n\n    Arguments:\n        parameters (Iterable[Variable]): an iterable of Variables that will have\n            gradients normalized\n        max_norm (float or int): max norm of the gradients\n\n    Returns:\n        Total norm of the parameters (viewed as a single vector).\n    """"""\n    max_norm = float(max_norm)\n\n    total_norm = 0\n    param_to_norm = {}\n    param_to_shape = {}\n    for n, p in named_parameters:\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2)\n            total_norm += param_norm ** 2\n            param_to_norm[n] = param_norm\n            param_to_shape[n] = p.size()\n\n    total_norm = total_norm ** (1. / 2)\n    clip_coef = max_norm / (total_norm + 1e-6)\n    if clip_coef < 1 and clip:\n        for _, p in named_parameters:\n            if p.grad is not None:\n                p.grad.data.mul_(clip_coef)\n\n    if verbose:\n        print(\'---Total norm {:.3f} clip coef {:.3f}-----------------\'.format(total_norm, clip_coef))\n        for name, norm in sorted(param_to_norm.items(), key=lambda x: -x[1]):\n            print(""{:<50s}: {:.3f}, ({})"".format(name, norm, param_to_shape[name]))\n        print(\'-------------------------------\', flush=True)\n\n    return total_norm\n\ndef update_lr(optimizer, lr=1e-4):\n    print(""------ Learning rate -> {}"".format(lr))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr'"
lib/rel_model.py,18,"b'""""""\nLet\'s get the relationships yo\n""""""\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nfrom torch.nn.utils.rnn import PackedSequence\nfrom lib.resnet import resnet_l4\nfrom config import BATCHNORM_MOMENTUM\nfrom lib.fpn.nms.functions.nms import apply_nms\n\n# from lib.decoder_rnn import DecoderRNN, lstm_factory, LockedDropout\nfrom lib.lstm.decoder_rnn import DecoderRNN\nfrom lib.lstm.highway_lstm_cuda.alternating_highway_lstm import AlternatingHighwayLSTM\nfrom lib.fpn.box_utils import bbox_overlaps, center_size\nfrom lib.get_union_boxes import UnionBoxesAndFeats\nfrom lib.fpn.proposal_assignments.rel_assignments import rel_assignments\nfrom lib.object_detector import ObjectDetector, gather_res, load_vgg\nfrom lib.pytorch_misc import transpose_packed_sequence_inds, to_onehot, arange, enumerate_by_image, diagonal_inds, Flattener\nfrom lib.sparse_targets import FrequencyBias\nfrom lib.surgery import filter_dets\nfrom lib.word_vectors import obj_edge_vectors\nfrom lib.fpn.roi_align.functions.roi_align import RoIAlignFunction\nimport math\n\n\ndef _sort_by_score(im_inds, scores):\n    """"""\n    We\'ll sort everything scorewise from Hi->low, BUT we need to keep images together\n    and sort LSTM from l\n    :param im_inds: Which im we\'re on\n    :param scores: Goodness ranging between [0, 1]. Higher numbers come FIRST\n    :return: Permutation to put everything in the right order for the LSTM\n             Inverse permutation\n             Lengths for the TxB packed sequence.\n    """"""\n    num_im = im_inds[-1] + 1\n    rois_per_image = scores.new(num_im)\n    lengths = []\n    for i, s, e in enumerate_by_image(im_inds):\n        rois_per_image[i] = 2 * (s - e) * num_im + i\n        lengths.append(e - s)\n    lengths = sorted(lengths, reverse=True)\n    inds, ls_transposed = transpose_packed_sequence_inds(lengths)  # move it to TxB form\n    inds = torch.LongTensor(inds).cuda(im_inds.get_device())\n\n    # ~~~~~~~~~~~~~~~~\n    # HACKY CODE ALERT!!!\n    # we\'re sorting by confidence which is in the range (0,1), but more importantly by longest\n    # img....\n    # ~~~~~~~~~~~~~~~~\n    roi_order = scores - 2 * rois_per_image[im_inds]\n    _, perm = torch.sort(roi_order, 0, descending=True)\n    perm = perm[inds]\n    _, inv_perm = torch.sort(perm)\n\n    return perm, inv_perm, ls_transposed\n\nMODES = (\'sgdet\', \'sgcls\', \'predcls\')\n\n\nclass LinearizedContext(nn.Module):\n    """"""\n    Module for computing the object contexts and edge contexts\n    """"""\n    def __init__(self, classes, rel_classes, mode=\'sgdet\',\n                 embed_dim=200, hidden_dim=256, obj_dim=2048,\n                 nl_obj=2, nl_edge=2, dropout_rate=0.2, order=\'confidence\',\n                 pass_in_obj_feats_to_decoder=True,\n                 pass_in_obj_feats_to_edge=True):\n        super(LinearizedContext, self).__init__()\n        self.classes = classes\n        self.rel_classes = rel_classes\n        assert mode in MODES\n        self.mode = mode\n\n        self.nl_obj = nl_obj\n        self.nl_edge = nl_edge\n\n        self.embed_dim = embed_dim\n        self.hidden_dim = hidden_dim\n        self.obj_dim = obj_dim\n        self.dropout_rate = dropout_rate\n        self.pass_in_obj_feats_to_decoder = pass_in_obj_feats_to_decoder\n        self.pass_in_obj_feats_to_edge = pass_in_obj_feats_to_edge\n\n        assert order in (\'size\', \'confidence\', \'random\', \'leftright\')\n        self.order = order\n\n        # EMBEDDINGS\n        embed_vecs = obj_edge_vectors(self.classes, wv_dim=self.embed_dim)\n        self.obj_embed = nn.Embedding(self.num_classes, self.embed_dim)\n        self.obj_embed.weight.data = embed_vecs.clone()\n\n        self.obj_embed2 = nn.Embedding(self.num_classes, self.embed_dim)\n        self.obj_embed2.weight.data = embed_vecs.clone()\n\n        # This probably doesn\'t help it much\n        self.pos_embed = nn.Sequential(*[\n            nn.BatchNorm1d(4, momentum=BATCHNORM_MOMENTUM / 10.0),\n            nn.Linear(4, 128),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.1),\n        ])\n\n        if self.nl_obj > 0:\n            self.obj_ctx_rnn = AlternatingHighwayLSTM(\n                input_size=self.obj_dim+self.embed_dim+128,\n                hidden_size=self.hidden_dim,\n                num_layers=self.nl_obj,\n                recurrent_dropout_probability=dropout_rate)\n\n            decoder_inputs_dim = self.hidden_dim\n            if self.pass_in_obj_feats_to_decoder:\n                decoder_inputs_dim += self.obj_dim + self.embed_dim\n\n            self.decoder_rnn = DecoderRNN(self.classes, embed_dim=self.embed_dim,\n                                          inputs_dim=decoder_inputs_dim,\n                                          hidden_dim=self.hidden_dim,\n                                          recurrent_dropout_probability=dropout_rate)\n        else:\n            self.decoder_lin = nn.Linear(self.obj_dim + self.embed_dim + 128, self.num_classes)\n\n        if self.nl_edge > 0:\n            input_dim = self.embed_dim\n            if self.nl_obj > 0:\n                input_dim += self.hidden_dim\n            if self.pass_in_obj_feats_to_edge:\n                input_dim += self.obj_dim\n            self.edge_ctx_rnn = AlternatingHighwayLSTM(input_size=input_dim,\n                                                       hidden_size=self.hidden_dim,\n                                                       num_layers=self.nl_edge,\n                                                       recurrent_dropout_probability=dropout_rate)\n\n    def sort_rois(self, batch_idx, confidence, box_priors):\n        """"""\n        :param batch_idx: tensor with what index we\'re on\n        :param confidence: tensor with confidences between [0,1)\n        :param boxes: tensor with (x1, y1, x2, y2)\n        :return: Permutation, inverse permutation, and the lengths transposed (same as _sort_by_score)\n        """"""\n        cxcywh = center_size(box_priors)\n        if self.order == \'size\':\n            sizes = cxcywh[:,2] * cxcywh[:, 3]\n            # sizes = (box_priors[:, 2] - box_priors[:, 0] + 1) * (box_priors[:, 3] - box_priors[:, 1] + 1)\n            assert sizes.min() > 0.0\n            scores = sizes / (sizes.max() + 1)\n        elif self.order == \'confidence\':\n            scores = confidence\n        elif self.order == \'random\':\n            scores = torch.FloatTensor(np.random.rand(batch_idx.size(0))).cuda(batch_idx.get_device())\n        elif self.order == \'leftright\':\n            centers = cxcywh[:,0]\n            scores = centers / (centers.max() + 1)\n        else:\n            raise ValueError(""invalid mode {}"".format(self.order))\n        return _sort_by_score(batch_idx, scores)\n\n    @property\n    def num_classes(self):\n        return len(self.classes)\n\n    @property\n    def num_rels(self):\n        return len(self.rel_classes)\n\n    def edge_ctx(self, obj_feats, obj_dists, im_inds, obj_preds, box_priors=None):\n        """"""\n        Object context and object classification.\n        :param obj_feats: [num_obj, img_dim + object embedding0 dim]\n        :param obj_dists: [num_obj, #classes]\n        :param im_inds: [num_obj] the indices of the images\n        :return: edge_ctx: [num_obj, #feats] For later!\n        """"""\n\n        # Only use hard embeddings\n        obj_embed2 = self.obj_embed2(obj_preds)\n        # obj_embed3 = F.softmax(obj_dists, dim=1) @ self.obj_embed3.weight\n        inp_feats = torch.cat((obj_embed2, obj_feats), 1)\n\n        # Sort by the confidence of the maximum detection.\n        confidence = F.softmax(obj_dists, dim=1).data.view(-1)[\n            obj_preds.data + arange(obj_preds.data) * self.num_classes]\n        perm, inv_perm, ls_transposed = self.sort_rois(im_inds.data, confidence, box_priors)\n\n        edge_input_packed = PackedSequence(inp_feats[perm], ls_transposed)\n        edge_reps = self.edge_ctx_rnn(edge_input_packed)[0][0]\n\n        # now we\'re good! unperm\n        edge_ctx = edge_reps[inv_perm]\n        return edge_ctx\n\n    def obj_ctx(self, obj_feats, obj_dists, im_inds, obj_labels=None, box_priors=None, boxes_per_cls=None):\n        """"""\n        Object context and object classification.\n        :param obj_feats: [num_obj, img_dim + object embedding0 dim]\n        :param obj_dists: [num_obj, #classes]\n        :param im_inds: [num_obj] the indices of the images\n        :param obj_labels: [num_obj] the GT labels of the image\n        :param boxes: [num_obj, 4] boxes. We\'ll use this for NMS\n        :return: obj_dists: [num_obj, #classes] new probability distribution.\n                 obj_preds: argmax of that distribution.\n                 obj_final_ctx: [num_obj, #feats] For later!\n        """"""\n        # Sort by the confidence of the maximum detection.\n        confidence = F.softmax(obj_dists, dim=1).data[:, 1:].max(1)[0]\n        perm, inv_perm, ls_transposed = self.sort_rois(im_inds.data, confidence, box_priors)\n        # Pass object features, sorted by score, into the encoder LSTM\n        obj_inp_rep = obj_feats[perm].contiguous()\n        input_packed = PackedSequence(obj_inp_rep, ls_transposed)\n\n        encoder_rep = self.obj_ctx_rnn(input_packed)[0][0]\n        # Decode in order\n        if self.mode != \'predcls\':\n            decoder_inp = PackedSequence(torch.cat((obj_inp_rep, encoder_rep), 1) if self.pass_in_obj_feats_to_decoder else encoder_rep,\n                                         ls_transposed)\n            obj_dists, obj_preds = self.decoder_rnn(\n                decoder_inp, #obj_dists[perm],\n                labels=obj_labels[perm] if obj_labels is not None else None,\n                boxes_for_nms=boxes_per_cls[perm] if boxes_per_cls is not None else None,\n                )\n            obj_preds = obj_preds[inv_perm]\n            obj_dists = obj_dists[inv_perm]\n        else:\n            assert obj_labels is not None\n            obj_preds = obj_labels\n            obj_dists = Variable(to_onehot(obj_preds.data, self.num_classes))\n        encoder_rep = encoder_rep[inv_perm]\n\n        return obj_dists, obj_preds, encoder_rep\n\n    def forward(self, obj_fmaps, obj_logits, im_inds, obj_labels=None, box_priors=None, boxes_per_cls=None):\n        """"""\n        Forward pass through the object and edge context\n        :param obj_priors:\n        :param obj_fmaps:\n        :param im_inds:\n        :param obj_labels:\n        :param boxes:\n        :return:\n        """"""\n        obj_embed = F.softmax(obj_logits, dim=1) @ self.obj_embed.weight\n        pos_embed = self.pos_embed(Variable(center_size(box_priors)))\n        obj_pre_rep = torch.cat((obj_fmaps, obj_embed, pos_embed), 1)\n\n        if self.nl_obj > 0:\n            obj_dists2, obj_preds, obj_ctx = self.obj_ctx(\n                obj_pre_rep,\n                obj_logits,\n                im_inds,\n                obj_labels,\n                box_priors,\n                boxes_per_cls,\n            )\n        else:\n            # UNSURE WHAT TO DO HERE\n            if self.mode == \'predcls\':\n                obj_dists2 = Variable(to_onehot(obj_labels.data, self.num_classes))\n            else:\n                obj_dists2 = self.decoder_lin(obj_pre_rep)\n\n            if self.mode == \'sgdet\' and not self.training:\n                # NMS here for baseline\n\n                probs = F.softmax(obj_dists2, 1)\n                nms_mask = obj_dists2.data.clone()\n                nms_mask.zero_()\n                for c_i in range(1, obj_dists2.size(1)):\n                    scores_ci = probs.data[:, c_i]\n                    boxes_ci = boxes_per_cls.data[:, c_i]\n\n                    keep = apply_nms(scores_ci, boxes_ci,\n                                     pre_nms_topn=scores_ci.size(0), post_nms_topn=scores_ci.size(0),\n                                     nms_thresh=0.3)\n                    nms_mask[:, c_i][keep] = 1\n\n                obj_preds = Variable(nms_mask * probs.data, volatile=True)[:,1:].max(1)[1] + 1\n            else:\n                obj_preds = obj_labels if obj_labels is not None else obj_dists2[:,1:].max(1)[1] + 1\n            obj_ctx = obj_pre_rep\n\n        edge_ctx = None\n        if self.nl_edge > 0:\n            edge_ctx = self.edge_ctx(\n                torch.cat((obj_fmaps, obj_ctx), 1) if self.pass_in_obj_feats_to_edge else obj_ctx,\n                obj_dists=obj_dists2.detach(),  # Was previously obj_logits.\n                im_inds=im_inds,\n                obj_preds=obj_preds,\n                box_priors=box_priors,\n            )\n\n        return obj_dists2, obj_preds, edge_ctx\n\n\nclass RelModel(nn.Module):\n    """"""\n    RELATIONSHIPS\n    """"""\n    def __init__(self, classes, rel_classes, mode=\'sgdet\', num_gpus=1, use_vision=True, require_overlap_det=True,\n                 embed_dim=200, hidden_dim=256, pooling_dim=2048,\n                 nl_obj=1, nl_edge=2, use_resnet=False, order=\'confidence\', thresh=0.01,\n                 use_proposals=False, pass_in_obj_feats_to_decoder=True,\n                 pass_in_obj_feats_to_edge=True, rec_dropout=0.0, use_bias=True, use_tanh=True,\n                 limit_vision=True):\n\n        """"""\n        :param classes: Object classes\n        :param rel_classes: Relationship classes. None if were not using rel mode\n        :param mode: (sgcls, predcls, or sgdet)\n        :param num_gpus: how many GPUS 2 use\n        :param use_vision: Whether to use vision in the final product\n        :param require_overlap_det: Whether two objects must intersect\n        :param embed_dim: Dimension for all embeddings\n        :param hidden_dim: LSTM hidden size\n        :param obj_dim:\n        """"""\n        super(RelModel, self).__init__()\n        self.classes = classes\n        self.rel_classes = rel_classes\n        self.num_gpus = num_gpus\n        assert mode in MODES\n        self.mode = mode\n\n        self.pooling_size = 7\n        self.embed_dim = embed_dim\n        self.hidden_dim = hidden_dim\n        self.obj_dim = 2048 if use_resnet else 4096\n        self.pooling_dim = pooling_dim\n\n        self.use_bias = use_bias\n        self.use_vision = use_vision\n        self.use_tanh = use_tanh\n        self.limit_vision=limit_vision\n        self.require_overlap = require_overlap_det and self.mode == \'sgdet\'\n\n        self.detector = ObjectDetector(\n            classes=classes,\n            mode=(\'proposals\' if use_proposals else \'refinerels\') if mode == \'sgdet\' else \'gtbox\',\n            use_resnet=use_resnet,\n            thresh=thresh,\n            max_per_img=64,\n        )\n\n        self.context = LinearizedContext(self.classes, self.rel_classes, mode=self.mode,\n                                         embed_dim=self.embed_dim, hidden_dim=self.hidden_dim,\n                                         obj_dim=self.obj_dim,\n                                         nl_obj=nl_obj, nl_edge=nl_edge, dropout_rate=rec_dropout,\n                                         order=order,\n                                         pass_in_obj_feats_to_decoder=pass_in_obj_feats_to_decoder,\n                                         pass_in_obj_feats_to_edge=pass_in_obj_feats_to_edge)\n\n        # Image Feats (You\'ll have to disable if you want to turn off the features from here)\n        self.union_boxes = UnionBoxesAndFeats(pooling_size=self.pooling_size, stride=16,\n                                              dim=1024 if use_resnet else 512)\n\n        if use_resnet:\n            self.roi_fmap = nn.Sequential(\n                resnet_l4(relu_end=False),\n                nn.AvgPool2d(self.pooling_size),\n                Flattener(),\n            )\n        else:\n            roi_fmap = [\n                Flattener(),\n                load_vgg(use_dropout=False, use_relu=False, use_linear=pooling_dim == 4096, pretrained=False).classifier,\n            ]\n            if pooling_dim != 4096:\n                roi_fmap.append(nn.Linear(4096, pooling_dim))\n            self.roi_fmap = nn.Sequential(*roi_fmap)\n            self.roi_fmap_obj = load_vgg(pretrained=False).classifier\n\n        ###################################\n        self.post_lstm = nn.Linear(self.hidden_dim, self.pooling_dim * 2)\n\n        # Initialize to sqrt(1/2n) so that the outputs all have mean 0 and variance 1.\n        # (Half contribution comes from LSTM, half from embedding.\n\n        # In practice the pre-lstm stuff tends to have stdev 0.1 so I multiplied this by 10.\n        self.post_lstm.weight.data.normal_(0, 10.0 * math.sqrt(1.0 / self.hidden_dim))\n        self.post_lstm.bias.data.zero_()\n\n        if nl_edge == 0:\n            self.post_emb = nn.Embedding(self.num_classes, self.pooling_dim*2)\n            self.post_emb.weight.data.normal_(0, math.sqrt(1.0))\n\n        self.rel_compress = nn.Linear(self.pooling_dim, self.num_rels, bias=True)\n        self.rel_compress.weight = torch.nn.init.xavier_normal(self.rel_compress.weight, gain=1.0)\n        if self.use_bias:\n            self.freq_bias = FrequencyBias()\n\n    @property\n    def num_classes(self):\n        return len(self.classes)\n\n    @property\n    def num_rels(self):\n        return len(self.rel_classes)\n\n    def visual_rep(self, features, rois, pair_inds):\n        """"""\n        Classify the features\n        :param features: [batch_size, dim, IM_SIZE/4, IM_SIZE/4]\n        :param rois: [num_rois, 5] array of [img_num, x0, y0, x1, y1].\n        :param pair_inds inds to use when predicting\n        :return: score_pred, a [num_rois, num_classes] array\n                 box_pred, a [num_rois, num_classes, 4] array\n        """"""\n        assert pair_inds.size(1) == 2\n        uboxes = self.union_boxes(features, rois, pair_inds)\n        return self.roi_fmap(uboxes)\n\n    def get_rel_inds(self, rel_labels, im_inds, box_priors):\n        # Get the relationship candidates\n        if self.training:\n            rel_inds = rel_labels[:, :3].data.clone()\n        else:\n            rel_cands = im_inds.data[:, None] == im_inds.data[None]\n            rel_cands.view(-1)[diagonal_inds(rel_cands)] = 0\n\n            # Require overlap for detection\n            if self.require_overlap:\n                rel_cands = rel_cands & (bbox_overlaps(box_priors.data,\n                                                       box_priors.data) > 0)\n\n                # if there are fewer then 100 things then we might as well add some?\n                amt_to_add = 100 - rel_cands.long().sum()\n\n            rel_cands = rel_cands.nonzero()\n            if rel_cands.dim() == 0:\n                rel_cands = im_inds.data.new(1, 2).fill_(0)\n\n            rel_inds = torch.cat((im_inds.data[rel_cands[:, 0]][:, None], rel_cands), 1)\n        return rel_inds\n\n    def obj_feature_map(self, features, rois):\n        """"""\n        Gets the ROI features\n        :param features: [batch_size, dim, IM_SIZE/4, IM_SIZE/4] (features at level p2)\n        :param rois: [num_rois, 5] array of [img_num, x0, y0, x1, y1].\n        :return: [num_rois, #dim] array\n        """"""\n        feature_pool = RoIAlignFunction(self.pooling_size, self.pooling_size, spatial_scale=1 / 16)(\n            features, rois)\n        return self.roi_fmap_obj(feature_pool.view(rois.size(0), -1))\n\n    def forward(self, x, im_sizes, image_offset,\n                gt_boxes=None, gt_classes=None, gt_rels=None, proposals=None, train_anchor_inds=None,\n                return_fmap=False):\n        """"""\n        Forward pass for detection\n        :param x: Images@[batch_size, 3, IM_SIZE, IM_SIZE]\n        :param im_sizes: A numpy array of (h, w, scale) for each image.\n        :param image_offset: Offset onto what image we\'re on for MGPU training (if single GPU this is 0)\n        :param gt_boxes:\n\n        Training parameters:\n        :param gt_boxes: [num_gt, 4] GT boxes over the batch.\n        :param gt_classes: [num_gt, 2] gt boxes where each one is (img_id, class)\n        :param train_anchor_inds: a [num_train, 2] array of indices for the anchors that will\n                                  be used to compute the training loss. Each (img_ind, fpn_idx)\n        :return: If train:\n            scores, boxdeltas, labels, boxes, boxtargets, rpnscores, rpnboxes, rellabels\n            \n            if test:\n            prob dists, boxes, img inds, maxscores, classes\n            \n        """"""\n        result = self.detector(x, im_sizes, image_offset, gt_boxes, gt_classes, gt_rels, proposals,\n                               train_anchor_inds, return_fmap=True)\n        if result.is_none():\n            return ValueError(""heck"")\n\n        im_inds = result.im_inds - image_offset\n        boxes = result.rm_box_priors\n\n        if self.training and result.rel_labels is None:\n            assert self.mode == \'sgdet\'\n            result.rel_labels = rel_assignments(im_inds.data, boxes.data, result.rm_obj_labels.data,\n                                                gt_boxes.data, gt_classes.data, gt_rels.data,\n                                                image_offset, filter_non_overlap=True,\n                                                num_sample_per_gt=1)\n\n        rel_inds = self.get_rel_inds(result.rel_labels, im_inds, boxes)\n\n        rois = torch.cat((im_inds[:, None].float(), boxes), 1)\n\n        result.obj_fmap = self.obj_feature_map(result.fmap.detach(), rois)\n\n        # Prevent gradients from flowing back into score_fc from elsewhere\n        result.rm_obj_dists, result.obj_preds, edge_ctx = self.context(\n            result.obj_fmap,\n            result.rm_obj_dists.detach(),\n            im_inds, result.rm_obj_labels if self.training or self.mode == \'predcls\' else None,\n            boxes.data, result.boxes_all)\n\n        if edge_ctx is None:\n            edge_rep = self.post_emb(result.obj_preds)\n        else:\n            edge_rep = self.post_lstm(edge_ctx)\n\n        # Split into subject and object representations\n        edge_rep = edge_rep.view(edge_rep.size(0), 2, self.pooling_dim)\n\n        subj_rep = edge_rep[:, 0]\n        obj_rep = edge_rep[:, 1]\n\n        prod_rep = subj_rep[rel_inds[:, 1]] * obj_rep[rel_inds[:, 2]]\n\n        if self.use_vision:\n            vr = self.visual_rep(result.fmap.detach(), rois, rel_inds[:, 1:])\n            if self.limit_vision:\n                # exact value TBD\n                prod_rep = torch.cat((prod_rep[:,:2048] * vr[:,:2048], prod_rep[:,2048:]), 1)\n            else:\n                prod_rep = prod_rep * vr\n\n        if self.use_tanh:\n            prod_rep = F.tanh(prod_rep)\n\n        result.rel_dists = self.rel_compress(prod_rep)\n\n        if self.use_bias:\n            result.rel_dists = result.rel_dists + self.freq_bias.index_with_labels(torch.stack((\n                result.obj_preds[rel_inds[:, 1]],\n                result.obj_preds[rel_inds[:, 2]],\n            ), 1))\n\n        if self.training:\n            return result\n\n        twod_inds = arange(result.obj_preds.data) * self.num_classes + result.obj_preds.data\n        result.obj_scores = F.softmax(result.rm_obj_dists, dim=1).view(-1)[twod_inds]\n\n        # Bbox regression\n        if self.mode == \'sgdet\':\n            bboxes = result.boxes_all.view(-1, 4)[twod_inds].view(result.boxes_all.size(0), 4)\n        else:\n            # Boxes will get fixed by filter_dets function.\n            bboxes = result.rm_box_priors\n\n        rel_rep = F.softmax(result.rel_dists, dim=1)\n        return filter_dets(bboxes, result.obj_scores,\n                           result.obj_preds, rel_inds[:, 1:], rel_rep)\n\n    def __getitem__(self, batch):\n        """""" Hack to do multi-GPU training""""""\n        batch.scatter()\n        if self.num_gpus == 1:\n            return self(*batch[0])\n\n        replicas = nn.parallel.replicate(self, devices=list(range(self.num_gpus)))\n        outputs = nn.parallel.parallel_apply(replicas, [batch[i] for i in range(self.num_gpus)])\n\n        if self.training:\n            return gather_res(outputs, 0, dim=0)\n        return outputs\n'"
lib/rel_model_stanford.py,11,"b'""""""\nLet\'s get the relationships yo\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\nfrom lib.surgery import filter_dets\nfrom lib.fpn.proposal_assignments.rel_assignments import rel_assignments\nfrom lib.pytorch_misc import arange\nfrom lib.object_detector import filter_det\nfrom lib.rel_model import RelModel\n\nMODES = (\'sgdet\', \'sgcls\', \'predcls\')\n\nSIZE=512\n\nclass RelModelStanford(RelModel):\n    """"""\n    RELATIONSHIPS\n    """"""\n\n    def __init__(self, classes, rel_classes, mode=\'sgdet\', num_gpus=1, require_overlap_det=True,\n                 use_resnet=False, use_proposals=False, **kwargs):\n        """"""\n        :param classes: Object classes\n        :param rel_classes: Relationship classes. None if were not using rel mode\n        :param num_gpus: how many GPUS 2 use\n        """"""\n        super(RelModelStanford, self).__init__(classes, rel_classes, mode=mode, num_gpus=num_gpus,\n                                               require_overlap_det=require_overlap_det,\n                                               use_resnet=use_resnet,\n                                               nl_obj=0, nl_edge=0, use_proposals=use_proposals, thresh=0.01,\n                                               pooling_dim=4096)\n\n        del self.context\n        del self.post_lstm\n        del self.post_emb\n\n        self.rel_fc = nn.Linear(SIZE, self.num_rels)\n        self.obj_fc = nn.Linear(SIZE, self.num_classes)\n\n        self.obj_unary = nn.Linear(self.obj_dim, SIZE)\n        self.edge_unary = nn.Linear(4096, SIZE)\n\n\n        self.edge_gru = nn.GRUCell(input_size=SIZE, hidden_size=SIZE)\n        self.node_gru = nn.GRUCell(input_size=SIZE, hidden_size=SIZE)\n\n        self.n_iter = 3\n\n        self.sub_vert_w_fc = nn.Sequential(nn.Linear(SIZE*2, 1), nn.Sigmoid())\n        self.obj_vert_w_fc = nn.Sequential(nn.Linear(SIZE*2, 1), nn.Sigmoid())\n        self.out_edge_w_fc = nn.Sequential(nn.Linear(SIZE*2, 1), nn.Sigmoid())\n\n        self.in_edge_w_fc = nn.Sequential(nn.Linear(SIZE*2, 1), nn.Sigmoid())\n\n    def message_pass(self, rel_rep, obj_rep, rel_inds):\n        """"""\n\n        :param rel_rep: [num_rel, fc]\n        :param obj_rep: [num_obj, fc]\n        :param rel_inds: [num_rel, 2] of the valid relationships\n        :return: object prediction [num_obj, 151], bbox_prediction [num_obj, 151*4] \n                and rel prediction [num_rel, 51]\n        """"""\n        # [num_obj, num_rel] with binary!\n        numer = torch.arange(0, rel_inds.size(0)).long().cuda(rel_inds.get_device())\n\n        objs_to_outrels = rel_rep.data.new(obj_rep.size(0), rel_rep.size(0)).zero_()\n        objs_to_outrels.view(-1)[rel_inds[:, 0] * rel_rep.size(0) + numer] = 1\n        objs_to_outrels = Variable(objs_to_outrels)\n\n        objs_to_inrels = rel_rep.data.new(obj_rep.size(0), rel_rep.size(0)).zero_()\n        objs_to_inrels.view(-1)[rel_inds[:, 1] * rel_rep.size(0) + numer] = 1\n        objs_to_inrels = Variable(objs_to_inrels)\n\n        hx_rel = Variable(rel_rep.data.new(rel_rep.size(0), SIZE).zero_(), requires_grad=False)\n        hx_obj = Variable(obj_rep.data.new(obj_rep.size(0), SIZE).zero_(), requires_grad=False)\n\n        vert_factor = [self.node_gru(obj_rep, hx_obj)]\n        edge_factor = [self.edge_gru(rel_rep, hx_rel)]\n\n        for i in range(3):\n            # compute edge context\n            sub_vert = vert_factor[i][rel_inds[:, 0]]\n            obj_vert = vert_factor[i][rel_inds[:, 1]]\n            weighted_sub = self.sub_vert_w_fc(\n                torch.cat((sub_vert, edge_factor[i]), 1)) * sub_vert\n            weighted_obj = self.obj_vert_w_fc(\n                torch.cat((obj_vert, edge_factor[i]), 1)) * obj_vert\n\n            edge_factor.append(self.edge_gru(weighted_sub + weighted_obj, edge_factor[i]))\n\n            # Compute vertex context\n            pre_out = self.out_edge_w_fc(torch.cat((sub_vert, edge_factor[i]), 1)) * \\\n                      edge_factor[i]\n            pre_in = self.in_edge_w_fc(torch.cat((obj_vert, edge_factor[i]), 1)) * edge_factor[\n                i]\n\n            vert_ctx = objs_to_outrels @ pre_out + objs_to_inrels @ pre_in\n            vert_factor.append(self.node_gru(vert_ctx, vert_factor[i]))\n\n        # woohoo! done\n        return self.obj_fc(vert_factor[-1]), self.rel_fc(edge_factor[-1])\n               # self.box_fc(vert_factor[-1]).view(-1, self.num_classes, 4), \\\n               # self.rel_fc(edge_factor[-1])\n\n    def forward(self, x, im_sizes, image_offset,\n                gt_boxes=None, gt_classes=None, gt_rels=None, proposals=None, train_anchor_inds=None,\n                return_fmap=False):\n        """"""\n        Forward pass for detection\n        :param x: Images@[batch_size, 3, IM_SIZE, IM_SIZE]\n        :param im_sizes: A numpy array of (h, w, scale) for each image.\n        :param image_offset: Offset onto what image we\'re on for MGPU training (if single GPU this is 0)\n        :param gt_boxes:\n\n        Training parameters:\n        :param gt_boxes: [num_gt, 4] GT boxes over the batch.\n        :param gt_classes: [num_gt, 2] gt boxes where each one is (img_id, class)\n        :param train_anchor_inds: a [num_train, 2] array of indices for the anchors that will\n                                  be used to compute the training loss. Each (img_ind, fpn_idx)\n        :return: If train:\n            scores, boxdeltas, labels, boxes, boxtargets, rpnscores, rpnboxes, rellabels\n            \n            if test:\n            prob dists, boxes, img inds, maxscores, classes\n            \n        """"""\n        result = self.detector(x, im_sizes, image_offset, gt_boxes, gt_classes, gt_rels, proposals,\n                               train_anchor_inds, return_fmap=True)\n\n        if result.is_none():\n            return ValueError(""heck"")\n\n        im_inds = result.im_inds - image_offset\n        boxes = result.rm_box_priors\n\n        if self.training and result.rel_labels is None:\n            assert self.mode == \'sgdet\'\n            result.rel_labels = rel_assignments(im_inds.data, boxes.data, result.rm_obj_labels.data,\n                                                gt_boxes.data, gt_classes.data, gt_rels.data,\n                                                image_offset, filter_non_overlap=True, num_sample_per_gt=1)\n        rel_inds = self.get_rel_inds(result.rel_labels, im_inds, boxes)\n        rois = torch.cat((im_inds[:, None].float(), boxes), 1)\n        visual_rep = self.visual_rep(result.fmap, rois, rel_inds[:, 1:])\n\n        result.obj_fmap = self.obj_feature_map(result.fmap.detach(), rois)\n\n        # Now do the approximation WHEREVER THERES A VALID RELATIONSHIP.\n        result.rm_obj_dists, result.rel_dists = self.message_pass(\n            F.relu(self.edge_unary(visual_rep)), self.obj_unary(result.obj_fmap), rel_inds[:, 1:])\n\n        # result.box_deltas_update = box_deltas\n\n        if self.training:\n            return result\n\n        # Decode here ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        if self.mode == \'predcls\':\n            # Hack to get the GT object labels\n            result.obj_scores = result.rm_obj_dists.data.new(gt_classes.size(0)).fill_(1)\n            result.obj_preds = gt_classes.data[:, 1]\n        elif self.mode == \'sgdet\':\n            order, obj_scores, obj_preds= filter_det(F.softmax(result.rm_obj_dists),\n                                                              result.boxes_all,\n                                                              start_ind=0,\n                                                              max_per_img=100,\n                                                              thresh=0.00,\n                                                              pre_nms_topn=6000,\n                                                              post_nms_topn=300,\n                                                              nms_thresh=0.3,\n                                                              nms_filter_duplicates=True)\n            idx, perm = torch.sort(order)\n            result.obj_preds = rel_inds.new(result.rm_obj_dists.size(0)).fill_(1)\n            result.obj_scores = result.rm_obj_dists.data.new(result.rm_obj_dists.size(0)).fill_(0)\n            result.obj_scores[idx] = obj_scores.data[perm]\n            result.obj_preds[idx] = obj_preds.data[perm]\n        else:\n            scores_nz = F.softmax(result.rm_obj_dists).data\n            scores_nz[:, 0] = 0.0\n            result.obj_scores, score_ord = scores_nz[:, 1:].sort(dim=1, descending=True)\n            result.obj_preds = score_ord[:,0] + 1\n            result.obj_scores = result.obj_scores[:,0]\n\n        result.obj_preds = Variable(result.obj_preds)\n        result.obj_scores = Variable(result.obj_scores)\n\n        # Set result\'s bounding boxes to be size\n        # [num_boxes, topk, 4] instead of considering every single object assignment.\n        twod_inds = arange(result.obj_preds.data) * self.num_classes + result.obj_preds.data\n\n        if self.mode == \'sgdet\':\n            bboxes = result.boxes_all.view(-1, 4)[twod_inds].view(result.boxes_all.size(0), 4)\n        else:\n            # Boxes will get fixed by filter_dets function.\n            bboxes = result.rm_box_priors\n        rel_rep = F.softmax(result.rel_dists)\n\n        return filter_dets(bboxes, result.obj_scores,\n                           result.obj_preds, rel_inds[:, 1:], rel_rep)\n\n'"
lib/resnet.py,2,"b'import torch.nn as nn\nimport math\nimport torch.utils.model_zoo as model_zoo\nfrom torchvision.models.resnet import model_urls, conv3x3, BasicBlock\nfrom torchvision.models.vgg import vgg16\nfrom config import BATCHNORM_MOMENTUM\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, relu_end=True):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BATCHNORM_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BATCHNORM_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4, momentum=BATCHNORM_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.relu_end = relu_end\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n\n        if self.relu_end:\n            out = self.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64, momentum=BATCHNORM_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1)  # HACK\n        self.avgpool = nn.AvgPool2d(7)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion, momentum=BATCHNORM_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\']))\n    return model\n\ndef resnet_l123():\n    model = resnet101(pretrained=True)\n    del model.layer4\n    del model.avgpool\n    del model.fc\n    return model\n\ndef resnet_l4(relu_end=True):\n    model = resnet101(pretrained=True)\n    l4 = model.layer4\n    if not relu_end:\n        l4[-1].relu_end = False\n    l4[0].conv2.stride = (1, 1)\n    l4[0].downsample[0].stride = (1, 1)\n    return l4\n\ndef vgg_fc(relu_end=True, linear_end=True):\n    model = vgg16(pretrained=True)\n    vfc = model.classifier\n    del vfc._modules[\'6\'] # Get rid of linear layer\n    del vfc._modules[\'5\'] # Get rid of linear layer\n    if not relu_end:\n        del vfc._modules[\'4\'] # Get rid of linear layer\n        if not linear_end:\n            del vfc._modules[\'3\']\n    return vfc\n\n\n'"
lib/sparse_targets.py,3,"b'from lib.word_vectors import obj_edge_vectors\nimport torch.nn as nn\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nfrom config import DATA_PATH\nimport os\nfrom lib.get_dataset_counts import get_counts\n\n\nclass FrequencyBias(nn.Module):\n    """"""\n    The goal of this is to provide a simplified way of computing\n    P(predicate | obj1, obj2, img).\n    """"""\n\n    def __init__(self, eps=1e-3):\n        super(FrequencyBias, self).__init__()\n\n        fg_matrix, bg_matrix = get_counts(must_overlap=True)\n        bg_matrix += 1\n        fg_matrix[:, :, 0] = bg_matrix\n\n        pred_dist = np.log(fg_matrix / fg_matrix.sum(2)[:, :, None] + eps)\n\n        self.num_objs = pred_dist.shape[0]\n        pred_dist = torch.FloatTensor(pred_dist).view(-1, pred_dist.shape[2])\n\n        self.obj_baseline = nn.Embedding(pred_dist.size(0), pred_dist.size(1))\n        self.obj_baseline.weight.data = pred_dist\n\n    def index_with_labels(self, labels):\n        """"""\n        :param labels: [batch_size, 2] \n        :return: \n        """"""\n        return self.obj_baseline(labels[:, 0] * self.num_objs + labels[:, 1])\n\n    def forward(self, obj_cands0, obj_cands1):\n        """"""\n        :param obj_cands0: [batch_size, 151] prob distibution over cands.\n        :param obj_cands1: [batch_size, 151] prob distibution over cands.\n        :return: [batch_size, #predicates] array, which contains potentials for\n        each possibility\n        """"""\n        # [batch_size, 151, 151] repr of the joint distribution\n        joint_cands = obj_cands0[:, :, None] * obj_cands1[:, None]\n\n        # [151, 151, 51] of targets per.\n        baseline = joint_cands.view(joint_cands.size(0), -1) @ self.obj_baseline.weight\n\n        return baseline\n'"
lib/surgery.py,1,"b'# create predictions from the other stuff\n""""""\nGo from proposals + scores to relationships.\n\npred-cls: No bbox regression, obj dist is exactly known\nsg-cls : No bbox regression\nsg-det : Bbox regression\n\nin all cases we\'ll return:\nboxes, objs, rels, pred_scores\n\n""""""\n\nimport numpy as np\nimport torch\nfrom lib.pytorch_misc import unravel_index\nfrom lib.fpn.box_utils import bbox_overlaps\n# from ad3 import factor_graph as fg\nfrom time import time\n\ndef filter_dets(boxes, obj_scores, obj_classes, rel_inds, pred_scores):\n    """"""\n    Filters detections....\n    :param boxes: [num_box, topk, 4] if bbox regression else [num_box, 4]\n    :param obj_scores: [num_box] probabilities for the scores\n    :param obj_classes: [num_box] class labels for the topk\n    :param rel_inds: [num_rel, 2] TENSOR consisting of (im_ind0, im_ind1)\n    :param pred_scores: [topk, topk, num_rel, num_predicates]\n    :param use_nms: True if use NMS to filter dets.\n    :return: boxes, objs, rels, pred_scores\n\n    """"""\n    if boxes.dim() != 2:\n        raise ValueError(""Boxes needs to be [num_box, 4] but its {}"".format(boxes.size()))\n\n    num_box = boxes.size(0)\n    assert obj_scores.size(0) == num_box\n\n    assert obj_classes.size() == obj_scores.size()\n    num_rel = rel_inds.size(0)\n    assert rel_inds.size(1) == 2\n    assert pred_scores.size(0) == num_rel\n\n    obj_scores0 = obj_scores.data[rel_inds[:,0]]\n    obj_scores1 = obj_scores.data[rel_inds[:,1]]\n\n    pred_scores_max, pred_classes_argmax = pred_scores.data[:,1:].max(1)\n    pred_classes_argmax = pred_classes_argmax + 1\n\n    rel_scores_argmaxed = pred_scores_max * obj_scores0 * obj_scores1\n    rel_scores_vs, rel_scores_idx = torch.sort(rel_scores_argmaxed.view(-1), dim=0, descending=True)\n\n    rels = rel_inds[rel_scores_idx].cpu().numpy()\n    pred_scores_sorted = pred_scores[rel_scores_idx].data.cpu().numpy()\n    obj_scores_np = obj_scores.data.cpu().numpy()\n    objs_np = obj_classes.data.cpu().numpy()\n    boxes_out = boxes.data.cpu().numpy()\n\n    return boxes_out, objs_np, obj_scores_np, rels, pred_scores_sorted\n\n# def _get_similar_boxes(boxes, obj_classes_topk, nms_thresh=0.3):\n#     """"""\n#     Assuming bg is NOT A LABEL.\n#     :param boxes: [num_box, topk, 4] if bbox regression else [num_box, 4]\n#     :param obj_classes: [num_box, topk] class labels\n#     :return: num_box, topk, num_box, topk array containing similarities.\n#     """"""\n#     topk = obj_classes_topk.size(1)\n#     num_box = boxes.size(0)\n#\n#     box_flat = boxes.view(-1, 4) if boxes.dim() == 3 else boxes[:, None].expand(\n#         num_box, topk, 4).contiguous().view(-1, 4)\n#     jax = bbox_overlaps(box_flat, box_flat).data > nms_thresh\n#     # Filter out things that are not gonna compete.\n#     classes_eq = obj_classes_topk.data.view(-1)[:, None] == obj_classes_topk.data.view(-1)[None, :]\n#     jax &= classes_eq\n#     boxes_are_similar = jax.view(num_box, topk, num_box, topk)\n#     return boxes_are_similar.cpu().numpy().astype(np.bool)\n'"
lib/word_vectors.py,4,"b'""""""\nAdapted from PyTorch\'s text library.\n""""""\n\nimport array\nimport os\nimport zipfile\n\nimport six\nimport torch\nfrom six.moves.urllib.request import urlretrieve\nfrom tqdm import tqdm\n\nfrom config import DATA_PATH\nimport sys\n\ndef obj_edge_vectors(names, wv_type=\'glove.6B\', wv_dir=DATA_PATH, wv_dim=300):\n    wv_dict, wv_arr, wv_size = load_word_vectors(wv_dir, wv_type, wv_dim)\n\n    vectors = torch.Tensor(len(names), wv_dim)\n    vectors.normal_(0,1)\n\n    for i, token in enumerate(names):\n        wv_index = wv_dict.get(token, None)\n        if wv_index is not None:\n            vectors[i] = wv_arr[wv_index]\n        else:\n            # Try the longest word (hopefully won\'t be a preposition\n            lw_token = sorted(token.split(\' \'), key=lambda x: len(x), reverse=True)[0]\n            print(""{} -> {} "".format(token, lw_token))\n            wv_index = wv_dict.get(lw_token, None)\n            if wv_index is not None:\n                vectors[i] = wv_arr[wv_index]\n            else:\n                print(""fail on {}"".format(token))\n\n    return vectors\n\nURL = {\n        \'glove.42B\': \'http://nlp.stanford.edu/data/glove.42B.300d.zip\',\n        \'glove.840B\': \'http://nlp.stanford.edu/data/glove.840B.300d.zip\',\n        \'glove.twitter.27B\': \'http://nlp.stanford.edu/data/glove.twitter.27B.zip\',\n        \'glove.6B\': \'http://nlp.stanford.edu/data/glove.6B.zip\',\n        }\n\n\ndef load_word_vectors(root, wv_type, dim):\n    """"""Load word vectors from a path, trying .pt, .txt, and .zip extensions.""""""\n    if isinstance(dim, int):\n        dim = str(dim) + \'d\'\n    fname = os.path.join(root, wv_type + \'.\' + dim)\n    if os.path.isfile(fname + \'.pt\'):\n        fname_pt = fname + \'.pt\'\n        print(\'loading word vectors from\', fname_pt)\n        try:\n            return torch.load(fname_pt)\n        except Exception as e:\n            print(""""""\n                Error loading the model from {}\n\n                This could be because this code was previously run with one\n                PyTorch version to generate cached data and is now being\n                run with another version.\n                You can try to delete the cached files on disk (this file\n                  and others) and re-running the code\n\n                Error message:\n                ---------\n                {}\n                """""".format(fname_pt, str(e)))\n            sys.exit(-1)\n    if os.path.isfile(fname + \'.txt\'):\n        fname_txt = fname + \'.txt\'\n        cm = open(fname_txt, \'rb\')\n        cm = [line for line in cm]\n    elif os.path.basename(wv_type) in URL:\n        url = URL[wv_type]\n        print(\'downloading word vectors from {}\'.format(url))\n        filename = os.path.basename(fname)\n        if not os.path.exists(root):\n            os.makedirs(root)\n        with tqdm(unit=\'B\', unit_scale=True, miniters=1, desc=filename) as t:\n            fname, _ = urlretrieve(url, fname, reporthook=reporthook(t))\n            with zipfile.ZipFile(fname, ""r"") as zf:\n                print(\'extracting word vectors into {}\'.format(root))\n                zf.extractall(root)\n        if not os.path.isfile(fname + \'.txt\'):\n            raise RuntimeError(\'no word vectors of requested dimension found\')\n        return load_word_vectors(root, wv_type, dim)\n    else:\n        raise RuntimeError(\'unable to load word vectors\')\n\n    wv_tokens, wv_arr, wv_size = [], array.array(\'d\'), None\n    if cm is not None:\n        for line in tqdm(range(len(cm)), desc=""loading word vectors from {}"".format(fname_txt)):\n            entries = cm[line].strip().split(b\' \')\n            word, entries = entries[0], entries[1:]\n            if wv_size is None:\n                wv_size = len(entries)\n            try:\n                if isinstance(word, six.binary_type):\n                    word = word.decode(\'utf-8\')\n            except:\n                print(\'non-UTF8 token\', repr(word), \'ignored\')\n                continue\n            wv_arr.extend(float(x) for x in entries)\n            wv_tokens.append(word)\n\n    wv_dict = {word: i for i, word in enumerate(wv_tokens)}\n    wv_arr = torch.Tensor(wv_arr).view(-1, wv_size)\n    ret = (wv_dict, wv_arr, wv_size)\n    torch.save(ret, fname + \'.pt\')\n    return ret\n\ndef reporthook(t):\n    """"""https://github.com/tqdm/tqdm""""""\n    last_b = [0]\n\n    def inner(b=1, bsize=1, tsize=None):\n        """"""\n        b: int, optionala\n        Number of blocks just transferred [default: 1].\n        bsize: int, optional\n        Size of each block (in tqdm units) [default: 1].\n        tsize: int, optional\n        Total size (in tqdm units). If [default: None] remains unchanged.\n        """"""\n        if tsize is not None:\n            t.total = tsize\n        t.update((b - last_b[0]) * bsize)\n        last_b[0] = b\n    return inner\n'"
misc/__init__.py,0,b''
misc/motifs.py,0,"b'""""""\nSCRIPT TO MAKE MEMES. this was from an old version of the code, so it might require some fixes to get working.\n\n""""""\nfrom dataloaders.visual_genome import VG\n# import matplotlib\n# # matplotlib.use(\'Agg\')\nfrom tqdm import tqdm\nimport seaborn as sns\nimport numpy as np\nfrom lib.fpn.box_intersections_cpu.bbox import bbox_overlaps\nfrom collections import defaultdict\ntrain, val, test = VG.splits(filter_non_overlap=False, num_val_im=2000)\n\ncount_threshold = 50\npmi_threshold = 10\n\no_type = []\nf = open(""object_types.txt"")\nfor line in f.readlines():\n  tabs = line.strip().split(""\\t"")\n  t = tabs[1].split(""_"")[0]\n  o_type.append(t)\n\nr_type = []\nf = open(""relation_types.txt"")\nfor line in f.readlines():\n  tabs = line.strip().split(""\\t"")\n  t = tabs[1].split(""_"")[0]\n  r_type.append(t) \n\nmax_id = 0\n\nmemes_id_id = {}\n\nmemes_id = {}\nid_memes = {}\n\nid_key = {}\nkey_id = {}\n#go through and assign keys\ndataset = []\nfor i in range(0, len(train)):\n  item = []\n  _r = train.relationships[i]\n  _o = train.gt_classes[i]\n  for j in range(0, len(_r)):\n    h = _o[_r[j][0]]\n    t = _o[_r[j][1]]\n    e = _r[j][2]\n    key1 = (h,e,t)\n    if key1 not in key_id: \n      id_key[max_id] = key1\n      key_id[key1] = max_id\n      max_id += 1\n    item.append(key_id[key1])\n  dataset.append(item)\n\ncids = train.ind_to_classes\nrids = train.ind_to_predicates\nall_memes = []\n\ndef id_to_str(_id):\n  key = id_key[_id]\n  if len(key) == 2:\n    pair = key\n    l1, s1 = id_to_str(pair[0])\n    l2, s2 = id_to_str(pair[1])\n    return (l1 + l2, s1 + "" & "" + s2)\n  else:\n    return (1,""{}--{}-->{}"".format(cids[key[0]], rids[key[1]], cids[key[2]]))\n\nnew_meme_score = {}\nfor p in range(0,25):\n  print(""iteration : {}"".format(p)) \n  unigrams = defaultdict(float)\n  bigrams = defaultdict(float) \n  unigrams_ori = defaultdict(float)\n  T = 0\n  T2 = 0\n  for i in range(0, len(dataset)):\n    item = dataset[i]\n    for j in range(0, len(item)):\n      key1 = item[j] \n      unigrams_ori[key1] += 1\n      #T += 1\n      for j2 in range(j+1 , len(item)):\n        key2 = item[j2]\n        if key1 > key2 : jkey = (key1, key2)\n        else: jkey = (key2, key1)\n        unigrams[key1] += 1\n        unigrams[key2] += 1\n        bigrams[jkey] += 1\n        T2 += 1\n  \n  pmi = []\n  for (jkey,val) in bigrams.items():\n    pval = (val / T2) / ( (unigrams[jkey[0]]/ T2) * (unigrams[jkey[0]] / T2 )) \n    #print(""{} {} {}"".format(jkey, val, pval))\n    if val > count_threshold and unigrams_ori[jkey[0]] > count_threshold and unigrams_ori[jkey[1]] > count_threshold and pval > pmi_threshold : \n      pmi.append( (pval , jkey, val) )\n  #    new_memes.add(jkey)\n\n  new_memes = set()\n  pmi = sorted(pmi, key = lambda x: -x[0])\n  new_meme_c = set()\n  for (v,k, f) in pmi:\n    #if k[0] in all_memes and k[1] in all_memes: continue \n    #if len( new_memes) > 1000: break\n    if k[0] in new_meme_c or k[1] in new_meme_c: continue\n    new_meme_c.add(k[0])\n    new_meme_c.add(k[1])\n    print(""{} & {} \\t {} \\t {} \\t {} \\t {}"".format(id_to_str(k[0]), id_to_str(k[1]), v, unigrams[k[0]], unigrams[k[1]], bigrams[k]))\n    new_memes.add(k)\n  #assign new ids to the memes\n    new_meme_score[k] = v \n    #break\n  for meme in new_memes:\n    if meme in key_id: continue\n    all_memes.append(max_id)\n    id_key[max_id] = meme\n    key_id[meme] = max_id\n    max_id+=1\n  print(""{} memes discovered "".format(len(new_memes)))\n  #go through and adjust the dataset\n  new_dataset = []\n  eliminated = 0\n  for i in range(0,len(dataset)):\n    item_save = dataset[i]\n    item = item_save\n    new_item = []\n    #merges = {}\n    while True:\n     best = None\n     best_score = 0\n     for j in range(0, len(item)):\n      key1 = item[j]\n      for j2 in range(j+1 , len(item)):\n        key2 = item[j2]\n        if key1 > key2 : jkey = (key1, key2)\n        else: jkey = (key2, key1)\n        if jkey in new_meme_score and new_meme_score[jkey] > best_score: \n          best = (j, j2) \n          best_score = new_meme_score[jkey]\n        #if jkey in key_id and j not in merges and j2 not in merges: \n        #  merges[j] = j2\n        #  merges[j2] = j\n     if best is not None:\n      for j in range(0, len(item)):\n        if j == best[0]: \n          key1 = item[j]\n          key2 = item[best[1]]\n          if key1 > key2 : jkey = (key1, key2)\n          else: jkey = (key2, key1)\n          new_item.append(key_id[jkey]) \n        elif j == best[1]: continue\n        else: new_item.append(item[j])\n      #break\n      item = new_item\n      new_item = [] \n     else:\n      #print(""done"")\n      new_item = item \n      break\n    #for j in range(0, len(item)):\n    #  if j not in merges: new_item.append(item[j])\n    #  elif j < merges[j]: \n    #    key1 = item[j]\n    #    key2 = item[merges[j]]\n    #   if key1 > key2 : jkey = (key1, key2)\n    #    else: jkey = (key2, key1)\n    #    new_item.append(key_id[jkey])  \n    eliminated += len(item_save) - len(new_item)\n    new_dataset.append(new_item)\n  print (""{} total eliminated"".format(eliminated))\n  dataset = new_dataset\n\nmeme_freq = defaultdict(float)\n\ndef increment_recursive(i):\n  #meme = id_key[i]\n  if i in all_memes:\n    meme_freq[i] += 1\n    key1 = id_key[i][0]\n    key2 = id_key[i][1]\n    increment_recursive(key1)\n    increment_recursive(key2)\n\ndef meme_length(i):\n  if i in all_memes:\n    return meme_length(id_key[i][0]) + meme_length(id_key[i][1])\n  else: \n    return 1\n\n#compute statistics of memes\nfor i in range(0,len(dataset)):\n  item = dataset[i]\n  for j in range(0, len(item)):\n    increment_recursive(item[j])\n  \nfor meme in all_memes:\n  print (""{} {}"".format( id_to_str(meme), meme_freq[meme]))\n\nT = 0 \nT2 = 0\nn_images = defaultdict(float)\nn_edges = defaultdict(float)\nfor item in dataset:\n  meme_lengths = []\n  for j in range(0, len(item)):\n    meme_lengths.append(meme_length(item[j]))\n  n_images[max(meme_lengths)] += 1\n  #for l in meme_lengths: n_images[l] +=1\n  T += 1\n\nfor item in dataset:\n  for j in range(0, len(item)):\n    l = meme_length(item[j])\n    n_edges[l] += l\n    T2 += l\n\nfor (k,v) in n_images.items():\n  print(""{} {}"".format(k, v/T))\nprint(""---"")\nfor (k,v) in n_edges.items():\n  print(""{} {}"".format(k, v/T2))\n\n\n\n\n  \n'"
models/_visualize.py,1,"b'""""""\nVisualization script. I used this to create the figures in the paper.\n\nWARNING: I haven\'t tested this in a while. It\'s possible that some later features I added break things here, but hopefully there should be easy fixes. I\'m uploading this in the off chance it might help someone. If you get it to work, let me know (and also send a PR with bugs/etc)\n""""""\n\nfrom dataloaders.visual_genome import VGDataLoader, VG\nfrom lib.rel_model import RelModel\nimport numpy as np\nimport torch\n\nfrom config import ModelConfig\nfrom lib.pytorch_misc import optimistic_restore\nfrom lib.evaluation.sg_eval import BasicSceneGraphEvaluator\nfrom tqdm import tqdm\nfrom config import BOX_SCALE, IM_SCALE\nfrom lib.fpn.box_utils import bbox_overlaps\nfrom collections import defaultdict\nfrom PIL import Image, ImageDraw, ImageFont\nimport os\nfrom functools import reduce\n\nconf = ModelConfig()\ntrain, val, test = VG.splits(num_val_im=conf.val_size)\nif conf.test:\n    val = test\n\ntrain_loader, val_loader = VGDataLoader.splits(train, val, mode=\'rel\',\n                                               batch_size=conf.batch_size,\n                                               num_workers=conf.num_workers,\n                                               num_gpus=conf.num_gpus)\n\ndetector = RelModel(classes=train.ind_to_classes, rel_classes=train.ind_to_predicates,\n                    num_gpus=conf.num_gpus, mode=conf.mode, require_overlap_det=True,\n                    use_resnet=conf.use_resnet, order=conf.order,\n                    nl_edge=conf.nl_edge, nl_obj=conf.nl_obj, hidden_dim=conf.hidden_dim,\n                    use_proposals=conf.use_proposals,\n                    pass_in_obj_feats_to_decoder=conf.pass_in_obj_feats_to_decoder,\n                    pass_in_obj_feats_to_edge=conf.pass_in_obj_feats_to_edge,\n                    pooling_dim=conf.pooling_dim,\n                    rec_dropout=conf.rec_dropout,\n                    use_bias=conf.use_bias,\n                    use_tanh=conf.use_tanh,\n                    limit_vision=conf.limit_vision\n                    )\ndetector.cuda()\nckpt = torch.load(conf.ckpt)\n\noptimistic_restore(detector, ckpt[\'state_dict\'])\n\n\n############################################ HELPER FUNCTIONS ###################################\n\ndef get_cmap(N):\n    import matplotlib.cm as cmx\n    import matplotlib.colors as colors\n    """"""Returns a function that maps each index in 0, 1, ... N-1 to a distinct RGB color.""""""\n    color_norm = colors.Normalize(vmin=0, vmax=N - 1)\n    scalar_map = cmx.ScalarMappable(norm=color_norm, cmap=\'hsv\')\n\n    def map_index_to_rgb_color(index):\n        pad = 40\n        return np.round(np.array(scalar_map.to_rgba(index)) * (255 - pad) + pad)\n\n    return map_index_to_rgb_color\n\n\ncmap = get_cmap(len(train.ind_to_classes) + 1)\n\n\ndef load_unscaled(fn):\n    """""" Loads and scales images so that it\'s 1024 max-dimension""""""\n    image_unpadded = Image.open(fn).convert(\'RGB\')\n    im_scale = 1024.0 / max(image_unpadded.size)\n\n    image = image_unpadded.resize((int(im_scale * image_unpadded.size[0]), int(im_scale * image_unpadded.size[1])),\n                                  resample=Image.BICUBIC)\n    return image\n\n\nfont = ImageFont.truetype(\'/usr/share/fonts/truetype/freefont/FreeMonoBold.ttf\', 32)\n\n\ndef draw_box(draw, boxx, cls_ind, text_str):\n    box = tuple([float(b) for b in boxx])\n    if \'-GT\' in text_str:\n        color = (255, 128, 0, 255)\n    else:\n        color = (0, 128, 0, 255)\n\n    # color = tuple([int(x) for x in cmap(cls_ind)])\n\n    # draw the fucking box\n    draw.line([(box[0], box[1]), (box[2], box[1])], fill=color, width=8)\n    draw.line([(box[2], box[1]), (box[2], box[3])], fill=color, width=8)\n    draw.line([(box[2], box[3]), (box[0], box[3])], fill=color, width=8)\n    draw.line([(box[0], box[3]), (box[0], box[1])], fill=color, width=8)\n\n    # draw.rectangle(box, outline=color)\n    w, h = draw.textsize(text_str, font=font)\n\n    x1text = box[0]\n    y1text = max(box[1] - h, 0)\n    x2text = min(x1text + w, draw.im.size[0])\n    y2text = y1text + h\n    print(""drawing {}x{} rectangle at {:.1f} {:.1f} {:.1f} {:.1f}"".format(\n        h, w, x1text, y1text, x2text, y2text))\n\n    draw.rectangle((x1text, y1text, x2text, y2text), fill=color)\n    draw.text((x1text, y1text), text_str, fill=\'black\', font=font)\n    return draw\n\n\ndef val_epoch():\n    detector.eval()\n    evaluator = BasicSceneGraphEvaluator.all_modes()\n    for val_b, batch in enumerate(tqdm(val_loader)):\n        val_batch(conf.num_gpus * val_b, batch, evaluator)\n\n    evaluator[conf.mode].print_stats()\n\n\ndef val_batch(batch_num, b, evaluator, thrs=(20, 50, 100)):\n    det_res = detector[b]\n    # if conf.num_gpus == 1:\n    #     det_res = [det_res]\n    assert conf.num_gpus == 1\n    boxes_i, objs_i, obj_scores_i, rels_i, pred_scores_i = det_res\n\n    gt_entry = {\n        \'gt_classes\': val.gt_classes[batch_num].copy(),\n        \'gt_relations\': val.relationships[batch_num].copy(),\n        \'gt_boxes\': val.gt_boxes[batch_num].copy(),\n    }\n    # gt_entry = {\'gt_classes\': gtc[i], \'gt_relations\': gtr[i], \'gt_boxes\': gtb[i]}\n    assert np.all(objs_i[rels_i[:, 0]] > 0) and np.all(objs_i[rels_i[:, 1]] > 0)\n    # assert np.all(rels_i[:, 2] > 0)\n\n    pred_entry = {\n        \'pred_boxes\': boxes_i * BOX_SCALE / IM_SCALE,\n        \'pred_classes\': objs_i,\n        \'pred_rel_inds\': rels_i,\n        \'obj_scores\': obj_scores_i,\n        \'rel_scores\': pred_scores_i,\n    }\n    pred_to_gt, pred_5ples, rel_scores = evaluator[conf.mode].evaluate_scene_graph_entry(\n        gt_entry,\n        pred_entry,\n    )\n\n    # SET RECALL THRESHOLD HERE\n    pred_to_gt = pred_to_gt[:20]\n    pred_5ples = pred_5ples[:20]\n\n    # Get a list of objects that match, and GT objects that dont\n    objs_match = (bbox_overlaps(pred_entry[\'pred_boxes\'], gt_entry[\'gt_boxes\']) >= 0.5) & (\n            objs_i[:, None] == gt_entry[\'gt_classes\'][None]\n    )\n    objs_matched = objs_match.any(1)\n\n    has_seen = defaultdict(int)\n    has_seen_gt = defaultdict(int)\n    pred_ind2name = {}\n    gt_ind2name = {}\n    edges = {}\n    missededges = {}\n    badedges = {}\n\n    if val.filenames[batch_num].startswith(\'2343676\'):\n        import ipdb\n        ipdb.set_trace()\n\n    def query_pred(pred_ind):\n        if pred_ind not in pred_ind2name:\n            has_seen[objs_i[pred_ind]] += 1\n            pred_ind2name[pred_ind] = \'{}-{}\'.format(train.ind_to_classes[objs_i[pred_ind]],\n                                                     has_seen[objs_i[pred_ind]])\n        return pred_ind2name[pred_ind]\n\n    def query_gt(gt_ind):\n        gt_cls = gt_entry[\'gt_classes\'][gt_ind]\n        if gt_ind not in gt_ind2name:\n            has_seen_gt[gt_cls] += 1\n            gt_ind2name[gt_ind] = \'{}-GT{}\'.format(train.ind_to_classes[gt_cls], has_seen_gt[gt_cls])\n        return gt_ind2name[gt_ind]\n\n    matching_pred5ples = pred_5ples[np.array([len(x) > 0 for x in pred_to_gt])]\n    for fiveple in matching_pred5ples:\n        head_name = query_pred(fiveple[0])\n        tail_name = query_pred(fiveple[1])\n\n        edges[(head_name, tail_name)] = train.ind_to_predicates[fiveple[4]]\n\n    gt_5ples = np.column_stack((gt_entry[\'gt_relations\'][:, :2],\n                                gt_entry[\'gt_classes\'][gt_entry[\'gt_relations\'][:, 0]],\n                                gt_entry[\'gt_classes\'][gt_entry[\'gt_relations\'][:, 1]],\n                                gt_entry[\'gt_relations\'][:, 2],\n                                ))\n    has_match = reduce(np.union1d, pred_to_gt)\n    for gt in gt_5ples[np.setdiff1d(np.arange(gt_5ples.shape[0]), has_match)]:\n        # Head and tail\n        namez = []\n        for i in range(2):\n            matching_obj = np.where(objs_match[:, gt[i]])[0]\n            if matching_obj.size > 0:\n                name = query_pred(matching_obj[0])\n            else:\n                name = query_gt(gt[i])\n            namez.append(name)\n\n        missededges[tuple(namez)] = train.ind_to_predicates[gt[4]]\n\n    for fiveple in pred_5ples[np.setdiff1d(np.arange(pred_5ples.shape[0]), matching_pred5ples)]:\n\n        if fiveple[0] in pred_ind2name:\n            if fiveple[1] in pred_ind2name:\n                badedges[(pred_ind2name[fiveple[0]], pred_ind2name[fiveple[1]])] = train.ind_to_predicates[fiveple[4]]\n\n    theimg = load_unscaled(val.filenames[batch_num])\n    theimg2 = theimg.copy()\n    draw2 = ImageDraw.Draw(theimg2)\n\n    # Fix the names\n\n    for pred_ind in pred_ind2name.keys():\n        draw2 = draw_box(draw2, pred_entry[\'pred_boxes\'][pred_ind],\n                         cls_ind=objs_i[pred_ind],\n                         text_str=pred_ind2name[pred_ind])\n    for gt_ind in gt_ind2name.keys():\n        draw2 = draw_box(draw2, gt_entry[\'gt_boxes\'][gt_ind],\n                         cls_ind=gt_entry[\'gt_classes\'][gt_ind],\n                         text_str=gt_ind2name[gt_ind])\n\n    recall = int(100 * len(reduce(np.union1d, pred_to_gt)) / gt_entry[\'gt_relations\'].shape[0])\n\n    id = \'{}-{}\'.format(val.filenames[batch_num].split(\'/\')[-1][:-4], recall)\n    pathname = os.path.join(\'qualitative\', id)\n    if not os.path.exists(pathname):\n        os.mkdir(pathname)\n    theimg.save(os.path.join(pathname, \'img.jpg\'), quality=100, subsampling=0)\n    theimg2.save(os.path.join(pathname, \'imgbox.jpg\'), quality=100, subsampling=0)\n\n    with open(os.path.join(pathname, \'shit.txt\'), \'w\') as f:\n        f.write(\'good:\\n\')\n        for (o1, o2), p in edges.items():\n            f.write(\'{} - {} - {}\\n\'.format(o1, p, o2))\n        f.write(\'fn:\\n\')\n        for (o1, o2), p in missededges.items():\n            f.write(\'{} - {} - {}\\n\'.format(o1, p, o2))\n        f.write(\'shit:\\n\')\n        for (o1, o2), p in badedges.items():\n            f.write(\'{} - {} - {}\\n\'.format(o1, p, o2))\n\n\nmAp = val_epoch()\n'"
models/eval_rel_count.py,2,"b'""""""\nBaseline model that works by simply iterating through the training set to make a dictionary.\n\nAlso, caches this (we can use this for training).\n\nThe model is quite simple, so we don\'t use the base train/test code\n\n""""""\nfrom dataloaders.visual_genome import VGDataLoader, VG\nfrom lib.object_detector import ObjectDetector\nimport numpy as np\nimport torch\nimport os\nfrom lib.get_dataset_counts import get_counts, box_filter\n\nfrom config import ModelConfig, FG_FRACTION, RPN_FG_FRACTION, DATA_PATH, BOX_SCALE, IM_SCALE, PROPOSAL_FN\nimport torch.backends.cudnn as cudnn\nfrom lib.pytorch_misc import optimistic_restore, nonintersecting_2d_inds\nfrom lib.evaluation.sg_eval import BasicSceneGraphEvaluator\nfrom tqdm import tqdm\nfrom copy import deepcopy\nimport dill as pkl\n\ncudnn.benchmark = True\nconf = ModelConfig()\n\nMUST_OVERLAP=False\ntrain, val, test = VG.splits(num_val_im=conf.val_size, filter_non_overlap=MUST_OVERLAP,\n                             filter_duplicate_rels=True,\n                             use_proposals=conf.use_proposals)\nif conf.test:\n    print(""test data!"")\n    val = test\ntrain_loader, val_loader = VGDataLoader.splits(train, val, mode=\'rel\',\n                                               batch_size=conf.batch_size,\n                                               num_workers=conf.num_workers,\n                                               num_gpus=conf.num_gpus)\n\nfg_matrix, bg_matrix = get_counts(train_data=train, must_overlap=MUST_OVERLAP)\n\ndetector = ObjectDetector(classes=train.ind_to_classes, num_gpus=conf.num_gpus,\n                          mode=\'rpntrain\' if not conf.use_proposals else \'proposals\', use_resnet=conf.use_resnet,\n                          nms_filter_duplicates=True, thresh=0.01)\ndetector.eval()\ndetector.cuda()\n\nclassifier = ObjectDetector(classes=train.ind_to_classes, num_gpus=conf.num_gpus,\n                            mode=\'gtbox\', use_resnet=conf.use_resnet,\n                            nms_filter_duplicates=True, thresh=0.01)\nclassifier.eval()\nclassifier.cuda()\n\nckpt = torch.load(conf.ckpt)\nmismatch = optimistic_restore(detector, ckpt[\'state_dict\'])\nmismatch = optimistic_restore(classifier, ckpt[\'state_dict\'])\n\nMOST_COMMON_MODE = True\n\nif MOST_COMMON_MODE:\n    prob_matrix = fg_matrix.astype(np.float32)\n    prob_matrix[:,:,0] = bg_matrix\n\n    # TRYING SOMETHING NEW.\n    prob_matrix[:,:,0] += 1\n    prob_matrix /= np.sum(prob_matrix, 2)[:,:,None]\n    # prob_matrix /= float(fg_matrix.max())\n\n    np.save(os.path.join(DATA_PATH, \'pred_stats.npy\'), prob_matrix)\n    prob_matrix[:,:,0] = 0 # Zero out BG\nelse:\n    prob_matrix = fg_matrix.astype(np.float64)\n    prob_matrix = prob_matrix / prob_matrix.max(2)[:,:,None]\n    np.save(os.path.join(DATA_PATH, \'pred_dist.npy\'), prob_matrix)\n\n# It\'s test time!\ndef predict(boxes, classes):\n    relation_possibilities_ = np.array(box_filter(boxes, must_overlap=MUST_OVERLAP), dtype=int)\n    full_preds = np.zeros((boxes.shape[0], boxes.shape[0], train.num_predicates))\n    for o1, o2 in relation_possibilities_:\n        c1, c2 = classes[[o1, o2]]\n        full_preds[o1, o2] = prob_matrix[c1, c2]\n\n    full_preds[:,:,0] = 0.0 # Zero out BG.\n    return full_preds\n\n# ##########################################################################################\n# ##########################################################################################\n\n# For visualizing / exploring\n\nc_to_ind = {c: i for i, c in enumerate(train.ind_to_classes)}\ndef gimme_the_dist(c1name, c2name):\n    c1 = c_to_ind[c1name]\n    c2 = c_to_ind[c2name]\n    dist = prob_matrix[c1, c2]\n    argz = np.argsort(-dist)\n    for i, a in enumerate(argz):\n        if dist[a] > 0.0:\n            print(""{:3d}: {:10s} ({:.4f})"".format(i, train.ind_to_predicates[a], dist[a]))\n\ncounts = np.zeros((train.num_classes, train.num_classes, train.num_predicates), dtype=np.int64)\nfor ex_ind in tqdm(range(len(val))):\n    gt_relations = val.relationships[ex_ind].copy()\n    gt_classes = val.gt_classes[ex_ind].copy()\n    o1o2 = gt_classes[gt_relations[:, :2]].tolist()\n    for (o1, o2), pred in zip(o1o2, gt_relations[:, 2]):\n        counts[o1, o2, pred] += 1\n\nzeroshot_case = counts[np.where(prob_matrix == 0)].sum() / float(counts.sum())\n\nmax_inds = prob_matrix.argmax(2).ravel()\nmax_counts = counts.reshape(-1, 51)[np.arange(max_inds.shape[0]), max_inds]\n\nmost_freq_port = max_counts.sum()/float(counts.sum())\n\n\nprint("" Rel acc={:.2f}%, {:.2f}% zsl"".format(\n    most_freq_port*100, zeroshot_case*100))\n\n# ##########################################################################################\n# ##########################################################################################\nT = len(val)\nevaluator = BasicSceneGraphEvaluator.all_modes(multiple_preds=conf.multi_pred)\n\n # First do detection results\nimg_offset = 0\nall_pred_entries = {\'sgdet\':[], \'sgcls\':[], \'predcls\':[]}\nfor val_b, b in enumerate(tqdm(val_loader)):\n\n    det_result = detector[b]\n\n    img_ids = b.gt_classes_primary.data.cpu().numpy()[:,0]\n    scores_np = det_result.obj_scores.data.cpu().numpy()\n    cls_preds_np = det_result.obj_preds.data.cpu().numpy()\n    boxes_np = det_result.boxes_assigned.data.cpu().numpy()* BOX_SCALE/IM_SCALE\n    # boxpriors_np = det_result.box_priors.data.cpu().numpy()\n    im_inds_np = det_result.im_inds.data.cpu().numpy() + img_offset\n\n    for img_i in np.unique(img_ids + img_offset):\n        gt_entry = {\n            \'gt_classes\': val.gt_classes[img_i].copy(),\n            \'gt_relations\': val.relationships[img_i].copy(),\n            \'gt_boxes\': val.gt_boxes[img_i].copy(),\n        }\n\n        pred_boxes = boxes_np[im_inds_np == img_i]\n        pred_classes = cls_preds_np[im_inds_np == img_i]\n        obj_scores = scores_np[im_inds_np == img_i]\n\n        all_rels = nonintersecting_2d_inds(pred_boxes.shape[0])\n        fp = predict(pred_boxes, pred_classes)\n        fp_pred = fp[all_rels[:,0], all_rels[:,1]]\n\n        scores = np.column_stack((\n            obj_scores[all_rels[:,0]],\n            obj_scores[all_rels[:,1]],\n            fp_pred.max(1)\n        )).prod(1)\n        sorted_inds = np.argsort(-scores)\n        sorted_inds = sorted_inds[scores[sorted_inds] > 0] #[:100]\n        pred_entry = {\n            \'pred_boxes\': pred_boxes,\n            \'pred_classes\': pred_classes,\n            \'obj_scores\': obj_scores,\n            \'pred_rel_inds\': all_rels[sorted_inds],\n            \'rel_scores\': fp_pred[sorted_inds],\n        }\n        all_pred_entries[\'sgdet\'].append(pred_entry)\n        evaluator[\'sgdet\'].evaluate_scene_graph_entry(\n            gt_entry,\n            pred_entry,\n        )\n    img_offset += img_ids.max() + 1\nevaluator[\'sgdet\'].print_stats()\n\n# -----------------------------------------------------------------------------------------\n# EVAL CLS AND SG\n\nimg_offset = 0\nfor val_b, b in enumerate(tqdm(val_loader)):\n\n    det_result = classifier[b]\n    scores, cls_preds = det_result.rm_obj_dists[:,1:].data.max(1)\n    scores_np = scores.cpu().numpy()\n    cls_preds_np = (cls_preds+1).cpu().numpy()\n\n    img_ids = b.gt_classes_primary.data.cpu().numpy()[:,0]\n    boxes_np = b.gt_boxes_primary.data.cpu().numpy()\n    im_inds_np = det_result.im_inds.data.cpu().numpy() + img_offset\n\n    for img_i in np.unique(img_ids + img_offset):\n        gt_entry = {\n            \'gt_classes\': val.gt_classes[img_i].copy(),\n            \'gt_relations\': val.relationships[img_i].copy(),\n            \'gt_boxes\': val.gt_boxes[img_i].copy(),\n        }\n\n        pred_boxes = boxes_np[im_inds_np == img_i]\n        pred_classes = cls_preds_np[im_inds_np == img_i]\n        obj_scores = scores_np[im_inds_np == img_i]\n\n        all_rels = nonintersecting_2d_inds(pred_boxes.shape[0])\n        fp = predict(pred_boxes, pred_classes)\n        fp_pred = fp[all_rels[:,0], all_rels[:,1]]\n\n        sg_cls_scores = np.column_stack((\n            obj_scores[all_rels[:,0]],\n            obj_scores[all_rels[:,1]],\n            fp_pred.max(1)\n        )).prod(1)\n        sg_cls_inds = np.argsort(-sg_cls_scores)\n        sg_cls_inds = sg_cls_inds[sg_cls_scores[sg_cls_inds] > 0] #[:100]\n\n        pred_entry = {\n            \'pred_boxes\': pred_boxes,\n            \'pred_classes\': pred_classes,\n            \'obj_scores\': obj_scores,\n            \'pred_rel_inds\': all_rels[sg_cls_inds],\n            \'rel_scores\': fp_pred[sg_cls_inds],\n        }\n        all_pred_entries[\'sgcls\'].append(deepcopy(pred_entry))\n        evaluator[\'sgcls\'].evaluate_scene_graph_entry(\n            gt_entry,\n            pred_entry,\n        )\n\n        ########################################################\n        fp = predict(gt_entry[\'gt_boxes\'], gt_entry[\'gt_classes\'])\n        fp_pred = fp[all_rels[:, 0], all_rels[:, 1]]\n\n        pred_cls_scores = fp_pred.max(1)\n        pred_cls_inds = np.argsort(-pred_cls_scores)\n        pred_cls_inds = pred_cls_inds[pred_cls_scores[pred_cls_inds] > 0][:100]\n\n        pred_entry[\'pred_rel_inds\'] = all_rels[pred_cls_inds]\n        pred_entry[\'rel_scores\'] = fp_pred[pred_cls_inds]\n        pred_entry[\'pred_classes\'] = gt_entry[\'gt_classes\']\n        pred_entry[\'obj_scores\'] = np.ones(pred_entry[\'pred_classes\'].shape[0])\n\n        all_pred_entries[\'predcls\'].append(pred_entry)\n\n        evaluator[\'predcls\'].evaluate_scene_graph_entry(\n            gt_entry,\n            pred_entry,\n        )\n    img_offset += img_ids.max() + 1\nevaluator[\'predcls\'].print_stats()\nevaluator[\'sgcls\'].print_stats()\n\nfor mode, entries in all_pred_entries.items():\n    with open(\'caches/freqbaseline-{}-{}.pkl\'.format(\'overlap\' if MUST_OVERLAP else \'nonoverlap\', mode), \'wb\') as f:\n        pkl.dump(entries, f)\n'"
models/eval_rels.py,2,"b'\nfrom dataloaders.visual_genome import VGDataLoader, VG\nimport numpy as np\nimport torch\n\nfrom config import ModelConfig\nfrom lib.pytorch_misc import optimistic_restore\nfrom lib.evaluation.sg_eval import BasicSceneGraphEvaluator\nfrom tqdm import tqdm\nfrom config import BOX_SCALE, IM_SCALE\nimport dill as pkl\nimport os\n\nconf = ModelConfig()\nif conf.model == \'motifnet\':\n    from lib.rel_model import RelModel\nelif conf.model == \'stanford\':\n    from lib.rel_model_stanford import RelModelStanford as RelModel\nelse:\n    raise ValueError()\n\ntrain, val, test = VG.splits(num_val_im=conf.val_size, filter_duplicate_rels=True,\n                          use_proposals=conf.use_proposals,\n                          filter_non_overlap=conf.mode == \'sgdet\')\nif conf.test:\n    val = test\ntrain_loader, val_loader = VGDataLoader.splits(train, val, mode=\'rel\',\n                                               batch_size=conf.batch_size,\n                                               num_workers=conf.num_workers,\n                                               num_gpus=conf.num_gpus)\n\ndetector = RelModel(classes=train.ind_to_classes, rel_classes=train.ind_to_predicates,\n                    num_gpus=conf.num_gpus, mode=conf.mode, require_overlap_det=True,\n                    use_resnet=conf.use_resnet, order=conf.order,\n                    nl_edge=conf.nl_edge, nl_obj=conf.nl_obj, hidden_dim=conf.hidden_dim,\n                    use_proposals=conf.use_proposals,\n                    pass_in_obj_feats_to_decoder=conf.pass_in_obj_feats_to_decoder,\n                    pass_in_obj_feats_to_edge=conf.pass_in_obj_feats_to_edge,\n                    pooling_dim=conf.pooling_dim,\n                    rec_dropout=conf.rec_dropout,\n                    use_bias=conf.use_bias,\n                    use_tanh=conf.use_tanh,\n                    limit_vision=conf.limit_vision\n                    )\n\n\ndetector.cuda()\nckpt = torch.load(conf.ckpt)\n\noptimistic_restore(detector, ckpt[\'state_dict\'])\n# if conf.mode == \'sgdet\':\n#     det_ckpt = torch.load(\'checkpoints/new_vgdet/vg-19.tar\')[\'state_dict\']\n#     detector.detector.bbox_fc.weight.data.copy_(det_ckpt[\'bbox_fc.weight\'])\n#     detector.detector.bbox_fc.bias.data.copy_(det_ckpt[\'bbox_fc.bias\'])\n#     detector.detector.score_fc.weight.data.copy_(det_ckpt[\'score_fc.weight\'])\n#     detector.detector.score_fc.bias.data.copy_(det_ckpt[\'score_fc.bias\'])\n\nall_pred_entries = []\ndef val_batch(batch_num, b, evaluator, thrs=(20, 50, 100)):\n    det_res = detector[b]\n    if conf.num_gpus == 1:\n        det_res = [det_res]\n\n    for i, (boxes_i, objs_i, obj_scores_i, rels_i, pred_scores_i) in enumerate(det_res):\n        gt_entry = {\n            \'gt_classes\': val.gt_classes[batch_num + i].copy(),\n            \'gt_relations\': val.relationships[batch_num + i].copy(),\n            \'gt_boxes\': val.gt_boxes[batch_num + i].copy(),\n        }\n        assert np.all(objs_i[rels_i[:,0]] > 0) and np.all(objs_i[rels_i[:,1]] > 0)\n        # assert np.all(rels_i[:,2] > 0)\n\n        pred_entry = {\n            \'pred_boxes\': boxes_i * BOX_SCALE/IM_SCALE,\n            \'pred_classes\': objs_i,\n            \'pred_rel_inds\': rels_i,\n            \'obj_scores\': obj_scores_i,\n            \'rel_scores\': pred_scores_i,\n        }\n        all_pred_entries.append(pred_entry)\n\n        evaluator[conf.mode].evaluate_scene_graph_entry(\n            gt_entry,\n            pred_entry,\n        )\n\nevaluator = BasicSceneGraphEvaluator.all_modes(multiple_preds=conf.multi_pred)\nif conf.cache is not None and os.path.exists(conf.cache):\n    print(""Found {}! Loading from it"".format(conf.cache))\n    with open(conf.cache,\'rb\') as f:\n        all_pred_entries = pkl.load(f)\n    for i, pred_entry in enumerate(tqdm(all_pred_entries)):\n        gt_entry = {\n            \'gt_classes\': val.gt_classes[i].copy(),\n            \'gt_relations\': val.relationships[i].copy(),\n            \'gt_boxes\': val.gt_boxes[i].copy(),\n        }\n        evaluator[conf.mode].evaluate_scene_graph_entry(\n            gt_entry,\n            pred_entry,\n        )\n    evaluator[conf.mode].print_stats()\nelse:\n    detector.eval()\n    for val_b, batch in enumerate(tqdm(val_loader)):\n        val_batch(conf.num_gpus*val_b, batch, evaluator)\n\n    evaluator[conf.mode].print_stats()\n\n    if conf.cache is not None:\n        with open(conf.cache,\'wb\') as f:\n            pkl.dump(all_pred_entries, f)\n'"
models/train_detector.py,5,"b'""""""\nTraining script 4 Detection\n""""""\nfrom dataloaders.mscoco import CocoDetection, CocoDataLoader\nfrom dataloaders.visual_genome import VGDataLoader, VG\nfrom lib.object_detector import ObjectDetector\nimport numpy as np\nfrom torch import optim\nimport torch\nimport pandas as pd\nimport time\nimport os\nfrom config import ModelConfig, FG_FRACTION, RPN_FG_FRACTION, IM_SCALE, BOX_SCALE\nfrom torch.nn import functional as F\nfrom lib.fpn.box_utils import bbox_loss\nimport torch.backends.cudnn as cudnn\nfrom pycocotools.cocoeval import COCOeval\nfrom lib.pytorch_misc import optimistic_restore, clip_grad_norm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\ncudnn.benchmark = True\nconf = ModelConfig()\n\nif conf.coco:\n    train, val = CocoDetection.splits()\n    val.ids = val.ids[:conf.val_size]\n    train.ids = train.ids\n    train_loader, val_loader = CocoDataLoader.splits(train, val, batch_size=conf.batch_size,\n                                                     num_workers=conf.num_workers,\n                                                     num_gpus=conf.num_gpus)\nelse:\n    train, val, _ = VG.splits(num_val_im=conf.val_size, filter_non_overlap=False,\n                              filter_empty_rels=False, use_proposals=conf.use_proposals)\n    train_loader, val_loader = VGDataLoader.splits(train, val, batch_size=conf.batch_size,\n                                                   num_workers=conf.num_workers,\n                                                   num_gpus=conf.num_gpus)\n\ndetector = ObjectDetector(classes=train.ind_to_classes, num_gpus=conf.num_gpus,\n                          mode=\'rpntrain\' if not conf.use_proposals else \'proposals\', use_resnet=conf.use_resnet)\ndetector.cuda()\n\n# Note: if you\'re doing the stanford setup, you\'ll need to change this to freeze the lower layers\nif conf.use_proposals:\n    for n, param in detector.named_parameters():\n        if n.startswith(\'features\'):\n            param.requires_grad = False\n\noptimizer = optim.SGD([p for p in detector.parameters() if p.requires_grad],\n                      weight_decay=conf.l2, lr=conf.lr * conf.num_gpus * conf.batch_size, momentum=0.9)\nscheduler = ReduceLROnPlateau(optimizer, \'max\', patience=3, factor=0.1,\n                              verbose=True, threshold=0.001, threshold_mode=\'abs\', cooldown=1)\n\nstart_epoch = -1\nif conf.ckpt is not None:\n    ckpt = torch.load(conf.ckpt)\n    if optimistic_restore(detector, ckpt[\'state_dict\']):\n        start_epoch = ckpt[\'epoch\']\n\n\ndef train_epoch(epoch_num):\n    detector.train()\n    tr = []\n    start = time.time()\n    for b, batch in enumerate(train_loader):\n        tr.append(train_batch(batch))\n\n        if b % conf.print_interval == 0 and b >= conf.print_interval:\n            mn = pd.concat(tr[-conf.print_interval:], axis=1).mean(1)\n            time_per_batch = (time.time() - start) / conf.print_interval\n            print(""\\ne{:2d}b{:5d}/{:5d} {:.3f}s/batch, {:.1f}m/epoch"".format(\n                epoch_num, b, len(train_loader), time_per_batch, len(train_loader) * time_per_batch / 60))\n            print(mn)\n            print(\'-----------\', flush=True)\n            start = time.time()\n    return pd.concat(tr, axis=1)\n\n\ndef train_batch(b):\n    """"""\n    :param b: contains:\n          :param imgs: the image, [batch_size, 3, IM_SIZE, IM_SIZE]\n          :param all_anchors: [num_anchors, 4] the boxes of all anchors that we\'ll be using\n          :param all_anchor_inds: [num_anchors, 2] array of the indices into the concatenated\n                                  RPN feature vector that give us all_anchors,\n                                  each one (img_ind, fpn_idx)\n          :param im_sizes: a [batch_size, 4] numpy array of (h, w, scale, num_good_anchors) for each image.\n\n          :param num_anchors_per_img: int, number of anchors in total over the feature pyramid per img\n\n          Training parameters:\n          :param train_anchor_inds: a [num_train, 5] array of indices for the anchors that will\n                                    be used to compute the training loss (img_ind, fpn_idx)\n          :param gt_boxes: [num_gt, 4] GT boxes over the batch.\n          :param gt_classes: [num_gt, 2] gt boxes where each one is (img_id, class)\n\n    :return:\n    """"""\n    result = detector[b]\n    scores = result.od_obj_dists\n    box_deltas = result.od_box_deltas\n    labels = result.od_obj_labels\n    roi_boxes = result.od_box_priors\n    bbox_targets = result.od_box_targets\n    rpn_scores = result.rpn_scores\n    rpn_box_deltas = result.rpn_box_deltas\n\n    # detector loss\n    valid_inds = (labels.data != 0).nonzero().squeeze(1)\n    fg_cnt = valid_inds.size(0)\n    bg_cnt = labels.size(0) - fg_cnt\n    class_loss = F.cross_entropy(scores, labels)\n\n    # No gather_nd in pytorch so instead convert first 2 dims of tensor to 1d\n    box_reg_mult = 2 * (1. / FG_FRACTION) * fg_cnt / (fg_cnt + bg_cnt + 1e-4)\n    twod_inds = valid_inds * box_deltas.size(1) + labels[valid_inds].data\n\n    box_loss = bbox_loss(roi_boxes[valid_inds], box_deltas.view(-1, 4)[twod_inds],\n                         bbox_targets[valid_inds]) * box_reg_mult\n\n    loss = class_loss + box_loss\n\n    # RPN loss\n    if not conf.use_proposals:\n        train_anchor_labels = b.train_anchor_labels[:, -1]\n        train_anchors = b.train_anchors[:, :4]\n        train_anchor_targets = b.train_anchors[:, 4:]\n\n        train_valid_inds = (train_anchor_labels.data == 1).nonzero().squeeze(1)\n        rpn_class_loss = F.cross_entropy(rpn_scores, train_anchor_labels)\n\n        # print(""{} fg {} bg, ratio of {:.3f} vs {:.3f}. RPN {}fg {}bg ratio of {:.3f} vs {:.3f}"".format(\n        #     fg_cnt, bg_cnt, fg_cnt / (fg_cnt + bg_cnt + 1e-4), FG_FRACTION,\n        #     train_valid_inds.size(0), train_anchor_labels.size(0)-train_valid_inds.size(0),\n        #     train_valid_inds.size(0) / (train_anchor_labels.size(0) + 1e-4), RPN_FG_FRACTION), flush=True)\n        rpn_box_mult = 2 * (1. / RPN_FG_FRACTION) * train_valid_inds.size(0) / (train_anchor_labels.size(0) + 1e-4)\n        rpn_box_loss = bbox_loss(train_anchors[train_valid_inds],\n                                 rpn_box_deltas[train_valid_inds],\n                                 train_anchor_targets[train_valid_inds]) * rpn_box_mult\n\n        loss += rpn_class_loss + rpn_box_loss\n        res = pd.Series([rpn_class_loss.data[0], rpn_box_loss.data[0],\n                         class_loss.data[0], box_loss.data[0], loss.data[0]],\n                        [\'rpn_class_loss\', \'rpn_box_loss\', \'class_loss\', \'box_loss\', \'total\'])\n    else:\n        res = pd.Series([class_loss.data[0], box_loss.data[0], loss.data[0]],\n                        [\'class_loss\', \'box_loss\', \'total\'])\n\n    optimizer.zero_grad()\n    loss.backward()\n    clip_grad_norm(\n        [(n, p) for n, p in detector.named_parameters() if p.grad is not None],\n        max_norm=conf.clip, clip=True)\n    optimizer.step()\n\n    return res\n\n\ndef val_epoch():\n    detector.eval()\n    # all_boxes is a list of length number-of-classes.\n    # Each list element is a list of length number-of-images.\n    # Each of those list elements is either an empty list []\n    # or a numpy array of detection.\n    vr = []\n    for val_b, batch in enumerate(val_loader):\n        vr.append(val_batch(val_b, batch))\n    vr = np.concatenate(vr, 0)\n    if vr.shape[0] == 0:\n        print(""No detections anywhere"")\n        return 0.0\n\n    val_coco = val.coco\n    coco_dt = val_coco.loadRes(vr)\n    coco_eval = COCOeval(val_coco, coco_dt, \'bbox\')\n    coco_eval.params.imgIds = val.ids if conf.coco else [x for x in range(len(val))]\n\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    coco_eval.summarize()\n    mAp = coco_eval.stats[1]\n    return mAp\n\n\ndef val_batch(batch_num, b):\n    result = detector[b]\n    if result is None:\n        return np.zeros((0, 7))\n    scores_np = result.obj_scores.data.cpu().numpy()\n    cls_preds_np = result.obj_preds.data.cpu().numpy()\n    boxes_np = result.boxes_assigned.data.cpu().numpy()\n    im_inds_np = result.im_inds.data.cpu().numpy()\n    im_scales = b.im_sizes.reshape((-1, 3))[:, 2]\n    if conf.coco:\n        boxes_np /= im_scales[im_inds_np][:, None]\n        boxes_np[:, 2:4] = boxes_np[:, 2:4] - boxes_np[:, 0:2] + 1\n        cls_preds_np[:] = [val.ind_to_id[c_ind] for c_ind in cls_preds_np]\n        im_inds_np[:] = [val.ids[im_ind + batch_num * conf.batch_size * conf.num_gpus]\n                         for im_ind in im_inds_np]\n    else:\n        boxes_np *= BOX_SCALE / IM_SCALE\n        boxes_np[:, 2:4] = boxes_np[:, 2:4] - boxes_np[:, 0:2] + 1\n        im_inds_np += batch_num * conf.batch_size * conf.num_gpus\n\n    return np.column_stack((im_inds_np, boxes_np, scores_np, cls_preds_np))\n\n\nprint(""Training starts now!"")\nfor epoch in range(start_epoch + 1, start_epoch + 1 + conf.num_epochs):\n    rez = train_epoch(epoch)\n    print(""overall{:2d}: ({:.3f})\\n{}"".format(epoch, rez.mean(1)[\'total\'], rez.mean(1)), flush=True)\n    mAp = val_epoch()\n    scheduler.step(mAp)\n\n    torch.save({\n        \'epoch\': epoch,\n        \'state_dict\': detector.state_dict(),\n        \'optimizer\': optimizer.state_dict(),\n    }, os.path.join(conf.save_dir, \'{}-{}.tar\'.format(\'coco\' if conf.coco else \'vg\', epoch)))\n'"
models/train_rels.py,5,"b'""""""\nTraining script for scene graph detection. Integrated with my faster rcnn setup\n""""""\n\nfrom dataloaders.visual_genome import VGDataLoader, VG\nimport numpy as np\nfrom torch import optim\nimport torch\nimport pandas as pd\nimport time\nimport os\n\nfrom config import ModelConfig, BOX_SCALE, IM_SCALE\nfrom torch.nn import functional as F\nfrom lib.pytorch_misc import optimistic_restore, de_chunkize, clip_grad_norm\nfrom lib.evaluation.sg_eval import BasicSceneGraphEvaluator\nfrom lib.pytorch_misc import print_para\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nconf = ModelConfig()\nif conf.model == \'motifnet\':\n    from lib.rel_model import RelModel\nelif conf.model == \'stanford\':\n    from lib.rel_model_stanford import RelModelStanford as RelModel\nelse:\n    raise ValueError()\n\ntrain, val, _ = VG.splits(num_val_im=conf.val_size, filter_duplicate_rels=True,\n                          use_proposals=conf.use_proposals,\n                          filter_non_overlap=conf.mode == \'sgdet\')\ntrain_loader, val_loader = VGDataLoader.splits(train, val, mode=\'rel\',\n                                               batch_size=conf.batch_size,\n                                               num_workers=conf.num_workers,\n                                               num_gpus=conf.num_gpus)\n\ndetector = RelModel(classes=train.ind_to_classes, rel_classes=train.ind_to_predicates,\n                    num_gpus=conf.num_gpus, mode=conf.mode, require_overlap_det=True,\n                    use_resnet=conf.use_resnet, order=conf.order,\n                    nl_edge=conf.nl_edge, nl_obj=conf.nl_obj, hidden_dim=conf.hidden_dim,\n                    use_proposals=conf.use_proposals,\n                    pass_in_obj_feats_to_decoder=conf.pass_in_obj_feats_to_decoder,\n                    pass_in_obj_feats_to_edge=conf.pass_in_obj_feats_to_edge,\n                    pooling_dim=conf.pooling_dim,\n                    rec_dropout=conf.rec_dropout,\n                    use_bias=conf.use_bias,\n                    use_tanh=conf.use_tanh,\n                    limit_vision=conf.limit_vision\n                    )\n\n# Freeze the detector\nfor n, param in detector.detector.named_parameters():\n    param.requires_grad = False\n\nprint(print_para(detector), flush=True)\n\n\ndef get_optim(lr):\n    # Lower the learning rate on the VGG fully connected layers by 1/10th. It\'s a hack, but it helps\n    # stabilize the models.\n    fc_params = [p for n,p in detector.named_parameters() if n.startswith(\'roi_fmap\') and p.requires_grad]\n    non_fc_params = [p for n,p in detector.named_parameters() if not n.startswith(\'roi_fmap\') and p.requires_grad]\n    params = [{\'params\': fc_params, \'lr\': lr / 10.0}, {\'params\': non_fc_params}]\n    # params = [p for n,p in detector.named_parameters() if p.requires_grad]\n\n    if conf.adam:\n        optimizer = optim.Adam(params, weight_decay=conf.l2, lr=lr, eps=1e-3)\n    else:\n        optimizer = optim.SGD(params, weight_decay=conf.l2, lr=lr, momentum=0.9)\n\n    scheduler = ReduceLROnPlateau(optimizer, \'max\', patience=3, factor=0.1,\n                                  verbose=True, threshold=0.0001, threshold_mode=\'abs\', cooldown=1)\n    return optimizer, scheduler\n\n\nckpt = torch.load(conf.ckpt)\nif conf.ckpt.split(\'-\')[-2].split(\'/\')[-1] == \'vgrel\':\n    print(""Loading EVERYTHING"")\n    start_epoch = ckpt[\'epoch\']\n\n    if not optimistic_restore(detector, ckpt[\'state_dict\']):\n        start_epoch = -1\n        # optimistic_restore(detector.detector, torch.load(\'checkpoints/vgdet/vg-28.tar\')[\'state_dict\'])\nelse:\n    start_epoch = -1\n    optimistic_restore(detector.detector, ckpt[\'state_dict\'])\n\n    detector.roi_fmap[1][0].weight.data.copy_(ckpt[\'state_dict\'][\'roi_fmap.0.weight\'])\n    detector.roi_fmap[1][3].weight.data.copy_(ckpt[\'state_dict\'][\'roi_fmap.3.weight\'])\n    detector.roi_fmap[1][0].bias.data.copy_(ckpt[\'state_dict\'][\'roi_fmap.0.bias\'])\n    detector.roi_fmap[1][3].bias.data.copy_(ckpt[\'state_dict\'][\'roi_fmap.3.bias\'])\n\n    detector.roi_fmap_obj[0].weight.data.copy_(ckpt[\'state_dict\'][\'roi_fmap.0.weight\'])\n    detector.roi_fmap_obj[3].weight.data.copy_(ckpt[\'state_dict\'][\'roi_fmap.3.weight\'])\n    detector.roi_fmap_obj[0].bias.data.copy_(ckpt[\'state_dict\'][\'roi_fmap.0.bias\'])\n    detector.roi_fmap_obj[3].bias.data.copy_(ckpt[\'state_dict\'][\'roi_fmap.3.bias\'])\n\ndetector.cuda()\n\n\ndef train_epoch(epoch_num):\n    detector.train()\n    tr = []\n    start = time.time()\n    for b, batch in enumerate(train_loader):\n        tr.append(train_batch(batch, verbose=b % (conf.print_interval*10) == 0)) #b == 0))\n\n        if b % conf.print_interval == 0 and b >= conf.print_interval:\n            mn = pd.concat(tr[-conf.print_interval:], axis=1).mean(1)\n            time_per_batch = (time.time() - start) / conf.print_interval\n            print(""\\ne{:2d}b{:5d}/{:5d} {:.3f}s/batch, {:.1f}m/epoch"".format(\n                epoch_num, b, len(train_loader), time_per_batch, len(train_loader) * time_per_batch / 60))\n            print(mn)\n            print(\'-----------\', flush=True)\n            start = time.time()\n    return pd.concat(tr, axis=1)\n\n\ndef train_batch(b, verbose=False):\n    """"""\n    :param b: contains:\n          :param imgs: the image, [batch_size, 3, IM_SIZE, IM_SIZE]\n          :param all_anchors: [num_anchors, 4] the boxes of all anchors that we\'ll be using\n          :param all_anchor_inds: [num_anchors, 2] array of the indices into the concatenated\n                                  RPN feature vector that give us all_anchors,\n                                  each one (img_ind, fpn_idx)\n          :param im_sizes: a [batch_size, 4] numpy array of (h, w, scale, num_good_anchors) for each image.\n\n          :param num_anchors_per_img: int, number of anchors in total over the feature pyramid per img\n\n          Training parameters:\n          :param train_anchor_inds: a [num_train, 5] array of indices for the anchors that will\n                                    be used to compute the training loss (img_ind, fpn_idx)\n          :param gt_boxes: [num_gt, 4] GT boxes over the batch.\n          :param gt_classes: [num_gt, 2] gt boxes where each one is (img_id, class)\n    :return:\n    """"""\n    result = detector[b]\n\n    losses = {}\n    losses[\'class_loss\'] = F.cross_entropy(result.rm_obj_dists, result.rm_obj_labels)\n    losses[\'rel_loss\'] = F.cross_entropy(result.rel_dists, result.rel_labels[:, -1])\n    loss = sum(losses.values())\n\n    optimizer.zero_grad()\n    loss.backward()\n    clip_grad_norm(\n        [(n, p) for n, p in detector.named_parameters() if p.grad is not None],\n        max_norm=conf.clip, verbose=verbose, clip=True)\n    losses[\'total\'] = loss\n    optimizer.step()\n    res = pd.Series({x: y.data[0] for x, y in losses.items()})\n    return res\n\n\ndef val_epoch():\n    detector.eval()\n    evaluator = BasicSceneGraphEvaluator.all_modes()\n    for val_b, batch in enumerate(val_loader):\n        val_batch(conf.num_gpus * val_b, batch, evaluator)\n    evaluator[conf.mode].print_stats()\n    return np.mean(evaluator[conf.mode].result_dict[conf.mode + \'_recall\'][100])\n\n\ndef val_batch(batch_num, b, evaluator):\n    det_res = detector[b]\n    if conf.num_gpus == 1:\n        det_res = [det_res]\n\n    for i, (boxes_i, objs_i, obj_scores_i, rels_i, pred_scores_i) in enumerate(det_res):\n        gt_entry = {\n            \'gt_classes\': val.gt_classes[batch_num + i].copy(),\n            \'gt_relations\': val.relationships[batch_num + i].copy(),\n            \'gt_boxes\': val.gt_boxes[batch_num + i].copy(),\n        }\n        assert np.all(objs_i[rels_i[:, 0]] > 0) and np.all(objs_i[rels_i[:, 1]] > 0)\n\n        pred_entry = {\n            \'pred_boxes\': boxes_i * BOX_SCALE/IM_SCALE,\n            \'pred_classes\': objs_i,\n            \'pred_rel_inds\': rels_i,\n            \'obj_scores\': obj_scores_i,\n            \'rel_scores\': pred_scores_i,  # hack for now.\n        }\n\n        evaluator[conf.mode].evaluate_scene_graph_entry(\n            gt_entry,\n            pred_entry,\n        )\n\n\nprint(""Training starts now!"")\noptimizer, scheduler = get_optim(conf.lr * conf.num_gpus * conf.batch_size)\nfor epoch in range(start_epoch + 1, start_epoch + 1 + conf.num_epochs):\n    rez = train_epoch(epoch)\n    print(""overall{:2d}: ({:.3f})\\n{}"".format(epoch, rez.mean(1)[\'total\'], rez.mean(1)), flush=True)\n    if conf.save_dir is not None:\n        torch.save({\n            \'epoch\': epoch,\n            \'state_dict\': detector.state_dict(), #{k:v for k,v in detector.state_dict().items() if not k.startswith(\'detector.\')},\n            # \'optimizer\': optimizer.state_dict(),\n        }, os.path.join(conf.save_dir, \'{}-{}.tar\'.format(\'vgrel\', epoch)))\n\n    mAp = val_epoch()\n    scheduler.step(mAp)\n    if any([pg[\'lr\'] <= (conf.lr * conf.num_gpus * conf.batch_size)/99.0 for pg in optimizer.param_groups]):\n        print(""exiting training early"", flush=True)\n        break\n'"
lib/draw_rectangles/setup.py,0,"b'from distutils.core import setup\nfrom Cython.Build import cythonize\nimport numpy\n\nsetup(name=""draw_rectangles_cython"", ext_modules=cythonize(\'draw_rectangles.pyx\'), include_dirs=[numpy.get_include()])'"
lib/evaluation/__init__.py,0,b''
lib/evaluation/sg_eval.py,0,"b'""""""\nAdapted from Danfei Xu. In particular, slow code was removed\n""""""\nimport numpy as np\nfrom functools import reduce\nfrom lib.pytorch_misc import intersect_2d, argsort_desc\nfrom lib.fpn.box_intersections_cpu.bbox import bbox_overlaps\nfrom config import MODES\nnp.set_printoptions(precision=3)\n\nclass BasicSceneGraphEvaluator:\n    def __init__(self, mode, multiple_preds=False):\n        self.result_dict = {}\n        self.mode = mode\n        self.result_dict[self.mode + \'_recall\'] = {20: [], 50: [], 100: []}\n        self.multiple_preds = multiple_preds\n\n    @classmethod\n    def all_modes(cls, **kwargs):\n        evaluators = {m: cls(mode=m, **kwargs) for m in MODES}\n        return evaluators\n\n    @classmethod\n    def vrd_modes(cls, **kwargs):\n        evaluators = {m: cls(mode=m, multiple_preds=True, **kwargs) for m in (\'preddet\', \'phrdet\')}\n        return evaluators\n\n    def evaluate_scene_graph_entry(self, gt_entry, pred_scores, viz_dict=None, iou_thresh=0.5):\n        res = evaluate_from_dict(gt_entry, pred_scores, self.mode, self.result_dict,\n                                  viz_dict=viz_dict, iou_thresh=iou_thresh, multiple_preds=self.multiple_preds)\n        # self.print_stats()\n        return res\n\n    def save(self, fn):\n        np.save(fn, self.result_dict)\n\n    def print_stats(self):\n        print(\'======================\' + self.mode + \'============================\')\n        for k, v in self.result_dict[self.mode + \'_recall\'].items():\n            print(\'R@%i: %f\' % (k, np.mean(v)))\n\n\ndef evaluate_from_dict(gt_entry, pred_entry, mode, result_dict, multiple_preds=False,\n                       viz_dict=None, **kwargs):\n    """"""\n    Shortcut to doing evaluate_recall from dict\n    :param gt_entry: Dictionary containing gt_relations, gt_boxes, gt_classes\n    :param pred_entry: Dictionary containing pred_rels, pred_boxes (if detection), pred_classes\n    :param mode: \'det\' or \'cls\'\n    :param result_dict: \n    :param viz_dict: \n    :param kwargs: \n    :return: \n    """"""\n    gt_rels = gt_entry[\'gt_relations\']\n    gt_boxes = gt_entry[\'gt_boxes\'].astype(float)\n    gt_classes = gt_entry[\'gt_classes\']\n\n    pred_rel_inds = pred_entry[\'pred_rel_inds\']\n    rel_scores = pred_entry[\'rel_scores\']\n\n    if mode == \'predcls\':\n        pred_boxes = gt_boxes\n        pred_classes = gt_classes\n        obj_scores = np.ones(gt_classes.shape[0])\n    elif mode == \'sgcls\':\n        pred_boxes = gt_boxes\n        pred_classes = pred_entry[\'pred_classes\']\n        obj_scores = pred_entry[\'obj_scores\']\n    elif mode == \'sgdet\' or mode == \'phrdet\':\n        pred_boxes = pred_entry[\'pred_boxes\'].astype(float)\n        pred_classes = pred_entry[\'pred_classes\']\n        obj_scores = pred_entry[\'obj_scores\']\n    elif mode == \'preddet\':\n        # Only extract the indices that appear in GT\n        prc = intersect_2d(pred_rel_inds, gt_rels[:, :2])\n        if prc.size == 0:\n            for k in result_dict[mode + \'_recall\']:\n                result_dict[mode + \'_recall\'][k].append(0.0)\n            return None, None, None\n        pred_inds_per_gt = prc.argmax(0)\n        pred_rel_inds = pred_rel_inds[pred_inds_per_gt]\n        rel_scores = rel_scores[pred_inds_per_gt]\n\n        # Now sort the matching ones\n        rel_scores_sorted = argsort_desc(rel_scores[:,1:])\n        rel_scores_sorted[:,1] += 1\n        rel_scores_sorted = np.column_stack((pred_rel_inds[rel_scores_sorted[:,0]], rel_scores_sorted[:,1]))\n\n        matches = intersect_2d(rel_scores_sorted, gt_rels)\n        for k in result_dict[mode + \'_recall\']:\n            rec_i = float(matches[:k].any(0).sum()) / float(gt_rels.shape[0])\n            result_dict[mode + \'_recall\'][k].append(rec_i)\n        return None, None, None\n    else:\n        raise ValueError(\'invalid mode\')\n\n    if multiple_preds:\n        obj_scores_per_rel = obj_scores[pred_rel_inds].prod(1)\n        overall_scores = obj_scores_per_rel[:,None] * rel_scores[:,1:]\n        score_inds = argsort_desc(overall_scores)[:100]\n        pred_rels = np.column_stack((pred_rel_inds[score_inds[:,0]], score_inds[:,1]+1))\n        predicate_scores = rel_scores[score_inds[:,0], score_inds[:,1]+1]\n    else:\n        pred_rels = np.column_stack((pred_rel_inds, 1+rel_scores[:,1:].argmax(1)))\n        predicate_scores = rel_scores[:,1:].max(1)\n\n    pred_to_gt, pred_5ples, rel_scores = evaluate_recall(\n                gt_rels, gt_boxes, gt_classes,\n                pred_rels, pred_boxes, pred_classes,\n                predicate_scores, obj_scores, phrdet= mode==\'phrdet\',\n                **kwargs)\n\n    for k in result_dict[mode + \'_recall\']:\n\n        match = reduce(np.union1d, pred_to_gt[:k])\n\n        rec_i = float(len(match)) / float(gt_rels.shape[0])\n        result_dict[mode + \'_recall\'][k].append(rec_i)\n    return pred_to_gt, pred_5ples, rel_scores\n\n    # print("" "".join([""R@{:2d}: {:.3f}"".format(k, v[-1]) for k, v in result_dict[mode + \'_recall\'].items()]))\n    # Deal with visualization later\n    # # Optionally, log things to a separate dictionary\n    # if viz_dict is not None:\n    #     # Caution: pred scores has changed (we took off the 0 class)\n    #     gt_rels_scores = pred_scores[\n    #         gt_rels[:, 0],\n    #         gt_rels[:, 1],\n    #         gt_rels[:, 2] - 1,\n    #     ]\n    #     # gt_rels_scores_cls = gt_rels_scores * pred_class_scores[\n    #     #         gt_rels[:, 0]] * pred_class_scores[gt_rels[:, 1]]\n    #\n    #     viz_dict[mode + \'_pred_rels\'] = pred_5ples.tolist()\n    #     viz_dict[mode + \'_pred_rels_scores\'] = max_pred_scores.tolist()\n    #     viz_dict[mode + \'_pred_rels_scores_cls\'] = max_rel_scores.tolist()\n    #     viz_dict[mode + \'_gt_rels_scores\'] = gt_rels_scores.tolist()\n    #     viz_dict[mode + \'_gt_rels_scores_cls\'] = gt_rels_scores_cls.tolist()\n    #\n    #     # Serialize pred2gt matching as a list of lists, where each sublist is of the form\n    #     # pred_ind, gt_ind1, gt_ind2, ....\n    #     viz_dict[mode + \'_pred2gt_rel\'] = pred_to_gt\n\n\n###########################\ndef evaluate_recall(gt_rels, gt_boxes, gt_classes,\n                    pred_rels, pred_boxes, pred_classes, rel_scores=None, cls_scores=None,\n                    iou_thresh=0.5, phrdet=False):\n    """"""\n    Evaluates the recall\n    :param gt_rels: [#gt_rel, 3] array of GT relations\n    :param gt_boxes: [#gt_box, 4] array of GT boxes\n    :param gt_classes: [#gt_box] array of GT classes\n    :param pred_rels: [#pred_rel, 3] array of pred rels. Assumed these are in sorted order\n                      and refer to IDs in pred classes / pred boxes\n                      (id0, id1, rel)\n    :param pred_boxes:  [#pred_box, 4] array of pred boxes\n    :param pred_classes: [#pred_box] array of predicted classes for these boxes\n    :return: pred_to_gt: Matching from predicate to GT\n             pred_5ples: the predicted (id0, id1, cls0, cls1, rel)\n             rel_scores: [cls_0score, cls1_score, relscore]\n                   """"""\n    if pred_rels.size == 0:\n        return [[]], np.zeros((0,5)), np.zeros(0)\n\n    num_gt_boxes = gt_boxes.shape[0]\n    num_gt_relations = gt_rels.shape[0]\n    assert num_gt_relations != 0\n\n    gt_triplets, gt_triplet_boxes, _ = _triplet(gt_rels[:, 2],\n                                                gt_rels[:, :2],\n                                                gt_classes,\n                                                gt_boxes)\n    num_boxes = pred_boxes.shape[0]\n    assert pred_rels[:,:2].max() < pred_classes.shape[0]\n\n    # Exclude self rels\n    # assert np.all(pred_rels[:,0] != pred_rels[:,1])\n    assert np.all(pred_rels[:,2] > 0)\n\n    pred_triplets, pred_triplet_boxes, relation_scores = \\\n        _triplet(pred_rels[:,2], pred_rels[:,:2], pred_classes, pred_boxes,\n                 rel_scores, cls_scores)\n\n    scores_overall = relation_scores.prod(1)\n    if not np.all(scores_overall[1:] <= scores_overall[:-1] + 1e-5):\n        print(""Somehow the relations weren\'t sorted properly: \\n{}"".format(scores_overall))\n        # raise ValueError(""Somehow the relations werent sorted properly"")\n\n    # Compute recall. It\'s most efficient to match once and then do recall after\n    pred_to_gt = _compute_pred_matches(\n        gt_triplets,\n        pred_triplets,\n        gt_triplet_boxes,\n        pred_triplet_boxes,\n        iou_thresh,\n        phrdet=phrdet,\n    )\n\n    # Contains some extra stuff for visualization. Not needed.\n    pred_5ples = np.column_stack((\n        pred_rels[:,:2],\n        pred_triplets[:, [0, 2, 1]],\n    ))\n\n    return pred_to_gt, pred_5ples, relation_scores\n\n\ndef _triplet(predicates, relations, classes, boxes,\n             predicate_scores=None, class_scores=None):\n    """"""\n    format predictions into triplets\n    :param predicates: A 1d numpy array of num_boxes*(num_boxes-1) predicates, corresponding to\n                       each pair of possibilities\n    :param relations: A (num_boxes*(num_boxes-1), 2) array, where each row represents the boxes\n                      in that relation\n    :param classes: A (num_boxes) array of the classes for each thing.\n    :param boxes: A (num_boxes,4) array of the bounding boxes for everything.\n    :param predicate_scores: A (num_boxes*(num_boxes-1)) array of the scores for each predicate\n    :param class_scores: A (num_boxes) array of the likelihood for each object.\n    :return: Triplets: (num_relations, 3) array of class, relation, class\n             Triplet boxes: (num_relation, 8) array of boxes for the parts\n             Triplet scores: num_relation array of the scores overall for the triplets\n    """"""\n    assert (predicates.shape[0] == relations.shape[0])\n\n    sub_ob_classes = classes[relations[:, :2]]\n    triplets = np.column_stack((sub_ob_classes[:, 0], predicates, sub_ob_classes[:, 1]))\n    triplet_boxes = np.column_stack((boxes[relations[:, 0]], boxes[relations[:, 1]]))\n\n    triplet_scores = None\n    if predicate_scores is not None and class_scores is not None:\n        triplet_scores = np.column_stack((\n            class_scores[relations[:, 0]],\n            class_scores[relations[:, 1]],\n            predicate_scores,\n        ))\n\n    return triplets, triplet_boxes, triplet_scores\n\n\ndef _compute_pred_matches(gt_triplets, pred_triplets,\n                 gt_boxes, pred_boxes, iou_thresh, phrdet=False):\n    """"""\n    Given a set of predicted triplets, return the list of matching GT\'s for each of the\n    given predictions\n    :param gt_triplets: \n    :param pred_triplets: \n    :param gt_boxes: \n    :param pred_boxes: \n    :param iou_thresh: \n    :return: \n    """"""\n    # This performs a matrix multiplication-esque thing between the two arrays\n    # Instead of summing, we want the equality, so we reduce in that way\n    # The rows correspond to GT triplets, columns to pred triplets\n    keeps = intersect_2d(gt_triplets, pred_triplets)\n    gt_has_match = keeps.any(1)\n    pred_to_gt = [[] for x in range(pred_boxes.shape[0])]\n    for gt_ind, gt_box, keep_inds in zip(np.where(gt_has_match)[0],\n                                         gt_boxes[gt_has_match],\n                                         keeps[gt_has_match],\n                                         ):\n        boxes = pred_boxes[keep_inds]\n        if phrdet:\n            # Evaluate where the union box > 0.5\n            gt_box_union = gt_box.reshape((2, 4))\n            gt_box_union = np.concatenate((gt_box_union.min(0)[:2], gt_box_union.max(0)[2:]), 0)\n\n            box_union = boxes.reshape((-1, 2, 4))\n            box_union = np.concatenate((box_union.min(1)[:,:2], box_union.max(1)[:,2:]), 1)\n\n            inds = bbox_overlaps(gt_box_union[None], box_union)[0] >= iou_thresh\n\n        else:\n            sub_iou = bbox_overlaps(gt_box[None,:4], boxes[:, :4])[0]\n            obj_iou = bbox_overlaps(gt_box[None,4:], boxes[:, 4:])[0]\n\n            inds = (sub_iou >= iou_thresh) & (obj_iou >= iou_thresh)\n\n        for i in np.where(keep_inds)[0][inds]:\n            pred_to_gt[i].append(int(gt_ind))\n    return pred_to_gt\n'"
lib/evaluation/sg_eval_all_rel_cates.py,0,"b'""""""\nAdapted from Danfei Xu. In particular, slow code was removed\n""""""\nimport numpy as np\nfrom functools import reduce\nfrom lib.pytorch_misc import intersect_2d, argsort_desc\nfrom lib.fpn.box_intersections_cpu.bbox import bbox_overlaps\nfrom config import MODES\nimport sys\nnp.set_printoptions(precision=3)\n\nclass BasicSceneGraphEvaluator:\n    def __init__(self, mode, multiple_preds=False):\n        self.result_dict = {}\n        self.mode = mode\n        rel_cats = {\n            0: \'all_rel_cates\',\n            1: ""above"",\n            2: ""across"",\n            3: ""against"",\n            4: ""along"",\n            5: ""and"",\n            6: ""at"",\n            7: ""attached to"",\n            8: ""behind"",\n            9: ""belonging to"",\n            10: ""between"",\n            11: ""carrying"",\n            12: ""covered in"",\n            13: ""covering"",\n            14: ""eating"",\n            15: ""flying in"",\n            16: ""for"",\n            17: ""from"",\n            18: ""growing on"",\n            19: ""hanging from"",\n            20: ""has"",\n            21: ""holding"",\n            22: ""in"",\n            23: ""in front of"",\n            24: ""laying on"",\n            25: ""looking at"",\n            26: ""lying on"",\n            27: ""made of"",\n            28: ""mounted on"",\n            29: ""near"",\n            30: ""of"",\n            31: ""on"",\n            32: ""on back of"",\n            33: ""over"",\n            34: ""painted on"",\n            35: ""parked on"",\n            36: ""part of"",\n            37: ""playing"",\n            38: ""riding"",\n            39: ""says"",\n            40: ""sitting on"",\n            41: ""standing on"",\n            42: ""to"",\n            43: ""under"",\n            44: ""using"",\n            45: ""walking in"",\n            46: ""walking on"",\n            47: ""watching"",\n            48: ""wearing"",\n            49: ""wears"",\n            50: ""with""\n        }\n        self.rel_cats = rel_cats\n        self.result_dict[self.mode + \'_recall\'] = {20: {}, 50: {}, 100: []}\n        for key, value in self.result_dict[self.mode + \'_recall\'].items():\n            self.result_dict[self.mode + \'_recall\'][key] = {}\n            for rel_cat_id, rel_cat_name in rel_cats.items():\n                self.result_dict[self.mode + \'_recall\'][key][rel_cat_name] = []\n        self.multiple_preds = multiple_preds\n\n    @classmethod\n    def all_modes(cls, **kwargs):\n        evaluators = {m: cls(mode=m, **kwargs) for m in MODES}\n        return evaluators\n\n    @classmethod\n    def vrd_modes(cls, **kwargs):\n        evaluators = {m: cls(mode=m, multiple_preds=True, **kwargs) for m in (\'preddet\', \'phrdet\')}\n        return evaluators\n\n    def evaluate_scene_graph_entry(self, gt_entry, pred_scores, viz_dict=None, iou_thresh=0.5):\n        res = evaluate_from_dict(gt_entry, pred_scores, self.mode, self.result_dict,\n                                  viz_dict=viz_dict, iou_thresh=iou_thresh, multiple_preds=self.multiple_preds, rel_cats=self.rel_cats)\n        # self.print_stats()\n        return res\n\n    def save(self, fn):\n        np.save(fn, self.result_dict)\n\n    def print_stats(self):\n        print(\'======================\' + self.mode + \'============================\')\n        for k, v in self.result_dict[self.mode + \'_recall\'].items():\n            for rel_cat_id, rel_cat_name in self.rel_cats.items():\n                print(\'R@%i: %f\' % (k, np.mean(v[rel_cat_name])), rel_cat_name)\n            print(\'~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\')\n\ndef evaluate_from_dict(gt_entry, pred_entry, mode, result_dict, multiple_preds=False,\n                       viz_dict=None, rel_cats=None, **kwargs):\n    """"""\n    Shortcut to doing evaluate_recall from dict\n    :param gt_entry: Dictionary containing gt_relations, gt_boxes, gt_classes\n    :param pred_entry: Dictionary containing pred_rels, pred_boxes (if detection), pred_classes\n    :param mode: \'det\' or \'cls\'\n    :param result_dict:\n    :param viz_dict:\n    :param kwargs:\n    :return:\n    """"""\n    gt_rels = gt_entry[\'gt_relations\']\n    gt_boxes = gt_entry[\'gt_boxes\'].astype(float)\n    gt_classes = gt_entry[\'gt_classes\']\n\n    gt_rels_nums = [0 for x in range(len(rel_cats))]\n    for rel in gt_rels:\n        gt_rels_nums[rel[2]] += 1\n        gt_rels_nums[0] += 1\n\n\n    pred_rel_inds = pred_entry[\'pred_rel_inds\']\n    rel_scores = pred_entry[\'rel_scores\']\n\n    if mode == \'predcls\':\n        pred_boxes = gt_boxes\n        pred_classes = gt_classes\n        obj_scores = np.ones(gt_classes.shape[0])\n    elif mode == \'sgcls\':\n        pred_boxes = gt_boxes\n        pred_classes = pred_entry[\'pred_classes\']\n        obj_scores = pred_entry[\'obj_scores\']\n    elif mode == \'sgdet\' or mode == \'phrdet\':\n        pred_boxes = pred_entry[\'pred_boxes\'].astype(float)\n        pred_classes = pred_entry[\'pred_classes\']\n        obj_scores = pred_entry[\'obj_scores\']\n    elif mode == \'preddet\':\n        # Only extract the indices that appear in GT\n        prc = intersect_2d(pred_rel_inds, gt_rels[:, :2])\n        if prc.size == 0:\n            for k in result_dict[mode + \'_recall\']:\n                result_dict[mode + \'_recall\'][k].append(0.0)\n            return None, None, None\n        pred_inds_per_gt = prc.argmax(0)\n        pred_rel_inds = pred_rel_inds[pred_inds_per_gt]\n        rel_scores = rel_scores[pred_inds_per_gt]\n\n        # Now sort the matching ones\n        rel_scores_sorted = argsort_desc(rel_scores[:,1:])\n        rel_scores_sorted[:,1] += 1\n        rel_scores_sorted = np.column_stack((pred_rel_inds[rel_scores_sorted[:,0]], rel_scores_sorted[:,1]))\n\n        matches = intersect_2d(rel_scores_sorted, gt_rels)\n        for k in result_dict[mode + \'_recall\']:\n            rec_i = float(matches[:k].any(0).sum()) / float(gt_rels.shape[0])\n            result_dict[mode + \'_recall\'][k].append(rec_i)\n        return None, None, None\n    else:\n        raise ValueError(\'invalid mode\')\n\n    if multiple_preds:\n        obj_scores_per_rel = obj_scores[pred_rel_inds].prod(1)\n        overall_scores = obj_scores_per_rel[:,None] * rel_scores[:,1:]\n        score_inds = argsort_desc(overall_scores)[:100]\n        pred_rels = np.column_stack((pred_rel_inds[score_inds[:,0]], score_inds[:,1]+1))\n        predicate_scores = rel_scores[score_inds[:,0], score_inds[:,1]+1]\n    else:\n        pred_rels = np.column_stack((pred_rel_inds, 1+rel_scores[:,1:].argmax(1)))\n        predicate_scores = rel_scores[:,1:].max(1)\n\n    pred_to_gt, pred_5ples, rel_scores = evaluate_recall(\n                gt_rels, gt_boxes, gt_classes,\n                pred_rels, pred_boxes, pred_classes,\n                predicate_scores, obj_scores, phrdet= mode==\'phrdet\',rel_cats=rel_cats,\n                **kwargs)\n\n    for k in result_dict[mode + \'_recall\']:\n        for rel_cat_id, rel_cat_name in rel_cats.items():\n            match = reduce(np.union1d, pred_to_gt[rel_cat_name][:k])\n            rec_i = float(len(match)) / (float(gt_rels_nums[rel_cat_id]) + sys.float_info.min) #float(gt_rels.shape[0])\n            result_dict[mode + \'_recall\'][k][rel_cat_name].append(rec_i)\n\n    return pred_to_gt, pred_5ples, rel_scores\n\n    # print("" "".join([""R@{:2d}: {:.3f}"".format(k, v[-1]) for k, v in result_dict[mode + \'_recall\'].items()]))\n    # Deal with visualization later\n    # # Optionally, log things to a separate dictionary\n    # if viz_dict is not None:\n    #     # Caution: pred scores has changed (we took off the 0 class)\n    #     gt_rels_scores = pred_scores[\n    #         gt_rels[:, 0],\n    #         gt_rels[:, 1],\n    #         gt_rels[:, 2] - 1,\n    #     ]\n    #     # gt_rels_scores_cls = gt_rels_scores * pred_class_scores[\n    #     #         gt_rels[:, 0]] * pred_class_scores[gt_rels[:, 1]]\n    #\n    #     viz_dict[mode + \'_pred_rels\'] = pred_5ples.tolist()\n    #     viz_dict[mode + \'_pred_rels_scores\'] = max_pred_scores.tolist()\n    #     viz_dict[mode + \'_pred_rels_scores_cls\'] = max_rel_scores.tolist()\n    #     viz_dict[mode + \'_gt_rels_scores\'] = gt_rels_scores.tolist()\n    #     viz_dict[mode + \'_gt_rels_scores_cls\'] = gt_rels_scores_cls.tolist()\n    #\n    #     # Serialize pred2gt matching as a list of lists, where each sublist is of the form\n    #     # pred_ind, gt_ind1, gt_ind2, ....\n    #     viz_dict[mode + \'_pred2gt_rel\'] = pred_to_gt\n\n\n###########################\ndef evaluate_recall(gt_rels, gt_boxes, gt_classes,\n                    pred_rels, pred_boxes, pred_classes, rel_scores=None, cls_scores=None,\n                    iou_thresh=0.5, phrdet=False, rel_cats=None):\n    """"""\n    Evaluates the recall\n    :param gt_rels: [#gt_rel, 3] array of GT relations\n    :param gt_boxes: [#gt_box, 4] array of GT boxes\n    :param gt_classes: [#gt_box] array of GT classes\n    :param pred_rels: [#pred_rel, 3] array of pred rels. Assumed these are in sorted order\n                      and refer to IDs in pred classes / pred boxes\n                      (id0, id1, rel)\n    :param pred_boxes:  [#pred_box, 4] array of pred boxes\n    :param pred_classes: [#pred_box] array of predicted classes for these boxes\n    :return: pred_to_gt: Matching from predicate to GT\n             pred_5ples: the predicted (id0, id1, cls0, cls1, rel)\n             rel_scores: [cls_0score, cls1_score, relscore]\n                   """"""\n    if pred_rels.size == 0:\n        return [[]], np.zeros((0,5)), np.zeros(0)\n\n    num_gt_boxes = gt_boxes.shape[0]\n    num_gt_relations = gt_rels.shape[0]\n    assert num_gt_relations != 0\n\n    gt_triplets, gt_triplet_boxes, _ = _triplet(gt_rels[:, 2],\n                                                gt_rels[:, :2],\n                                                gt_classes,\n                                                gt_boxes)\n    num_boxes = pred_boxes.shape[0]\n    assert pred_rels[:,:2].max() < pred_classes.shape[0]\n\n    # Exclude self rels\n    # assert np.all(pred_rels[:,0] != pred_rels[:,1])\n    assert np.all(pred_rels[:,2] > 0)\n\n    pred_triplets, pred_triplet_boxes, relation_scores = \\\n        _triplet(pred_rels[:,2], pred_rels[:,:2], pred_classes, pred_boxes,\n                 rel_scores, cls_scores)\n\n    scores_overall = relation_scores.prod(1)\n    if not np.all(scores_overall[1:] <= scores_overall[:-1] + 1e-5):\n        print(""Somehow the relations weren\'t sorted properly: \\n{}"".format(scores_overall))\n        # raise ValueError(""Somehow the relations werent sorted properly"")\n\n    # Compute recall. It\'s most efficient to match once and then do recall after\n    pred_to_gt = _compute_pred_matches(\n        gt_triplets,\n        pred_triplets,\n        gt_triplet_boxes,\n        pred_triplet_boxes,\n        iou_thresh,\n        phrdet=phrdet,\n        rel_cats=rel_cats,\n    )\n\n    # Contains some extra stuff for visualization. Not needed.\n    pred_5ples = np.column_stack((\n        pred_rels[:,:2],\n        pred_triplets[:, [0, 2, 1]],\n    ))\n\n    return pred_to_gt, pred_5ples, relation_scores\n\n\ndef _triplet(predicates, relations, classes, boxes,\n             predicate_scores=None, class_scores=None):\n    """"""\n    format predictions into triplets\n    :param predicates: A 1d numpy array of num_boxes*(num_boxes-1) predicates, corresponding to\n                       each pair of possibilities\n    :param relations: A (num_boxes*(num_boxes-1), 2) array, where each row represents the boxes\n                      in that relation\n    :param classes: A (num_boxes) array of the classes for each thing.\n    :param boxes: A (num_boxes,4) array of the bounding boxes for everything.\n    :param predicate_scores: A (num_boxes*(num_boxes-1)) array of the scores for each predicate\n    :param class_scores: A (num_boxes) array of the likelihood for each object.\n    :return: Triplets: (num_relations, 3) array of class, relation, class\n             Triplet boxes: (num_relation, 8) array of boxes for the parts\n             Triplet scores: num_relation array of the scores overall for the triplets\n    """"""\n    assert (predicates.shape[0] == relations.shape[0])\n\n    sub_ob_classes = classes[relations[:, :2]]\n    triplets = np.column_stack((sub_ob_classes[:, 0], predicates, sub_ob_classes[:, 1]))\n    triplet_boxes = np.column_stack((boxes[relations[:, 0]], boxes[relations[:, 1]]))\n\n    triplet_scores = None\n    if predicate_scores is not None and class_scores is not None:\n        triplet_scores = np.column_stack((\n            class_scores[relations[:, 0]],\n            class_scores[relations[:, 1]],\n            predicate_scores,\n        ))\n\n    return triplets, triplet_boxes, triplet_scores\n\n\ndef _compute_pred_matches(gt_triplets, pred_triplets,\n                 gt_boxes, pred_boxes, iou_thresh, phrdet=False, rel_cats=None):\n    """"""\n    Given a set of predicted triplets, return the list of matching GT\'s for each of the\n    given predictions\n    :param gt_triplets:\n    :param pred_triplets:\n    :param gt_boxes:\n    :param pred_boxes:\n    :param iou_thresh:\n    :return:\n    """"""\n    # This performs a matrix multiplication-esque thing between the two arrays\n    # Instead of summing, we want the equality, so we reduce in that way\n    # The rows correspond to GT triplets, columns to pred triplets\n    keeps = intersect_2d(gt_triplets, pred_triplets)\n    gt_has_match = keeps.any(1)\n    pred_to_gt = {}\n    for rel_cat_id, rel_cat_name in rel_cats.items():\n        pred_to_gt[rel_cat_name] = [[] for x in range(pred_boxes.shape[0])]\n    for gt_ind, gt_box, keep_inds in zip(np.where(gt_has_match)[0],\n                                         gt_boxes[gt_has_match],\n                                         keeps[gt_has_match],\n                                         ):\n        boxes = pred_boxes[keep_inds]\n        if phrdet:\n            # Evaluate where the union box > 0.5\n            gt_box_union = gt_box.reshape((2, 4))\n            gt_box_union = np.concatenate((gt_box_union.min(0)[:2], gt_box_union.max(0)[2:]), 0)\n\n            box_union = boxes.reshape((-1, 2, 4))\n            box_union = np.concatenate((box_union.min(1)[:,:2], box_union.max(1)[:,2:]), 1)\n\n            inds = bbox_overlaps(gt_box_union[None], box_union)[0] >= iou_thresh\n\n        else:\n            sub_iou = bbox_overlaps(gt_box[None,:4], boxes[:, :4])[0]\n            obj_iou = bbox_overlaps(gt_box[None,4:], boxes[:, 4:])[0]\n\n            inds = (sub_iou >= iou_thresh) & (obj_iou >= iou_thresh)\n\n        for i in np.where(keep_inds)[0][inds]:\n            pred_to_gt[\'all_rel_cates\'][i].append(int(gt_ind))\n            pred_to_gt[rel_cats[gt_triplets[int(gt_ind), 1]]][i].append(int(gt_ind))\n    return pred_to_gt\n'"
lib/evaluation/sg_eval_slow.py,0,"b""# JUST TO CHECK THAT IT IS EXACTLY THE SAME..................................\nimport numpy as np\nfrom config import MODES\n\nclass BasicSceneGraphEvaluator:\n\n    def __init__(self, mode):\n        self.result_dict = {}\n        self.mode = {'sgdet':'sg_det', 'sgcls':'sg_cls', 'predcls':'pred_cls'}[mode]\n\n        self.result_dict = {}\n        self.result_dict[self.mode + '_recall'] = {20:[], 50:[], 100:[]}\n\n\n    @classmethod\n    def all_modes(cls):\n        evaluators = {m: cls(mode=m) for m in MODES}\n        return evaluators\n    def evaluate_scene_graph_entry(self, gt_entry, pred_entry, iou_thresh=0.5):\n\n        roidb_entry = {\n            'max_overlaps': np.ones(gt_entry['gt_classes'].shape[0], dtype=np.int64),\n            'boxes': gt_entry['gt_boxes'],\n            'gt_relations': gt_entry['gt_relations'],\n            'gt_classes': gt_entry['gt_classes'],\n        }\n        sg_entry = {\n            'boxes': pred_entry['pred_boxes'],\n            'relations': pred_entry['pred_rels'],\n            'obj_scores': pred_entry['obj_scores'],\n            'rel_scores': pred_entry['rel_scores'],\n            'pred_classes': pred_entry['pred_classes'],\n        }\n\n        pred_triplets, triplet_boxes = \\\n            eval_relation_recall(sg_entry, roidb_entry,\n                                self.result_dict,\n                                self.mode,\n                                iou_thresh=iou_thresh)\n        return pred_triplets, triplet_boxes\n\n\n    def save(self, fn):\n        np.save(fn, self.result_dict)\n\n\n    def print_stats(self):\n        print('======================' + self.mode + '============================')\n        for k, v in self.result_dict[self.mode + '_recall'].items():\n            print('R@%i: %f' % (k, np.mean(v)))\n\n    def save(self, fn):\n        np.save(fn, self.result_dict)\n\n    def print_stats(self):\n        print('======================' + self.mode + '============================')\n        for k, v in self.result_dict[self.mode + '_recall'].items():\n            print('R@%i: %f' % (k, np.mean(v)))\n\n\ndef eval_relation_recall(sg_entry,\n                         roidb_entry,\n                         result_dict,\n                         mode,\n                         iou_thresh):\n\n    # gt\n    gt_inds = np.where(roidb_entry['max_overlaps'] == 1)[0]\n    gt_boxes = roidb_entry['boxes'][gt_inds].copy().astype(float)\n    num_gt_boxes = gt_boxes.shape[0]\n    gt_relations = roidb_entry['gt_relations'].copy()\n    gt_classes = roidb_entry['gt_classes'].copy()\n\n    num_gt_relations = gt_relations.shape[0]\n    if num_gt_relations == 0:\n        return (None, None)\n    gt_class_scores = np.ones(num_gt_boxes)\n    gt_predicate_scores = np.ones(num_gt_relations)\n    gt_triplets, gt_triplet_boxes, _ = _triplet(gt_relations[:,2],\n                                             gt_relations[:,:2],\n                                             gt_classes,\n                                             gt_boxes,\n                                             gt_predicate_scores,\n                                             gt_class_scores)\n\n    # pred\n    box_preds = sg_entry['boxes']\n    num_boxes = box_preds.shape[0]\n    relations = sg_entry['relations']\n    classes = sg_entry['pred_classes'].copy()\n    class_scores = sg_entry['obj_scores'].copy()\n\n    num_relations = relations.shape[0]\n\n    if mode =='pred_cls':\n        # if predicate classification task\n        # use ground truth bounding boxes\n        assert(num_boxes == num_gt_boxes)\n        classes = gt_classes\n        class_scores = gt_class_scores\n        boxes = gt_boxes\n    elif mode =='sg_cls':\n        assert(num_boxes == num_gt_boxes)\n        # if scene graph classification task\n        # use gt boxes, but predicted classes\n        # classes = np.argmax(class_preds, 1)\n        # class_scores = class_preds.max(axis=1)\n        boxes = gt_boxes\n    elif mode =='sg_det':\n        # if scene graph detection task\n        # use preicted boxes and predicted classes\n        # classes = np.argmax(class_preds, 1)\n        # class_scores = class_preds.max(axis=1)\n        boxes = box_preds\n    else:\n        raise NotImplementedError('Incorrect Mode! %s' % mode)\n\n    pred_triplets = np.column_stack((\n        classes[relations[:, 0]],\n        relations[:,2],\n        classes[relations[:, 1]],\n    ))\n    pred_triplet_boxes = np.column_stack((\n        boxes[relations[:, 0]],\n        boxes[relations[:, 1]],\n    ))\n    relation_scores = np.column_stack((\n        class_scores[relations[:, 0]],\n        sg_entry['rel_scores'],\n        class_scores[relations[:, 1]],\n    )).prod(1)\n\n    sorted_inds = np.argsort(relation_scores)[::-1]\n    # compue recall\n    for k in result_dict[mode + '_recall']:\n        this_k = min(k, num_relations)\n        keep_inds = sorted_inds[:this_k]\n        recall = _relation_recall(gt_triplets,\n                                  pred_triplets[keep_inds,:],\n                                  gt_triplet_boxes,\n                                  pred_triplet_boxes[keep_inds,:],\n                                  iou_thresh)\n        result_dict[mode + '_recall'][k].append(recall)\n\n    # for visualization\n    return pred_triplets[sorted_inds, :], pred_triplet_boxes[sorted_inds, :]\n\n\ndef _triplet(predicates, relations, classes, boxes,\n             predicate_scores, class_scores):\n\n    # format predictions into triplets\n    assert(predicates.shape[0] == relations.shape[0])\n    num_relations = relations.shape[0]\n    triplets = np.zeros([num_relations, 3]).astype(np.int32)\n    triplet_boxes = np.zeros([num_relations, 8]).astype(np.int32)\n    triplet_scores = np.zeros([num_relations]).astype(np.float32)\n    for i in range(num_relations):\n        triplets[i, 1] = predicates[i]\n        sub_i, obj_i = relations[i,:2]\n        triplets[i, 0] = classes[sub_i]\n        triplets[i, 2] = classes[obj_i]\n        triplet_boxes[i, :4] = boxes[sub_i, :]\n        triplet_boxes[i, 4:] = boxes[obj_i, :]\n        # compute triplet score\n        score =  class_scores[sub_i]\n        score *= class_scores[obj_i]\n        score *= predicate_scores[i]\n        triplet_scores[i] = score\n    return triplets, triplet_boxes, triplet_scores\n\n\ndef _relation_recall(gt_triplets, pred_triplets,\n                     gt_boxes, pred_boxes, iou_thresh):\n\n    # compute the R@K metric for a set of predicted triplets\n\n    num_gt = gt_triplets.shape[0]\n    num_correct_pred_gt = 0\n\n    for gt, gt_box in zip(gt_triplets, gt_boxes):\n        keep = np.zeros(pred_triplets.shape[0]).astype(bool)\n        for i, pred in enumerate(pred_triplets):\n            if gt[0] == pred[0] and gt[1] == pred[1] and gt[2] == pred[2]:\n                keep[i] = True\n        if not np.any(keep):\n            continue\n        boxes = pred_boxes[keep,:]\n        sub_iou = iou(gt_box[:4], boxes[:,:4])\n        obj_iou = iou(gt_box[4:], boxes[:,4:])\n        inds = np.intersect1d(np.where(sub_iou >= iou_thresh)[0],\n                              np.where(obj_iou >= iou_thresh)[0])\n        if inds.size > 0:\n            num_correct_pred_gt += 1\n    return float(num_correct_pred_gt) / float(num_gt)\n\n\ndef iou(gt_box, pred_boxes):\n    # computer Intersection-over-Union between two sets of boxes\n    ixmin = np.maximum(gt_box[0], pred_boxes[:,0])\n    iymin = np.maximum(gt_box[1], pred_boxes[:,1])\n    ixmax = np.minimum(gt_box[2], pred_boxes[:,2])\n    iymax = np.minimum(gt_box[3], pred_boxes[:,3])\n    iw = np.maximum(ixmax - ixmin + 1., 0.)\n    ih = np.maximum(iymax - iymin + 1., 0.)\n    inters = iw * ih\n\n    # union\n    uni = ((gt_box[2] - gt_box[0] + 1.) * (gt_box[3] - gt_box[1] + 1.) +\n            (pred_boxes[:, 2] - pred_boxes[:, 0] + 1.) *\n            (pred_boxes[:, 3] - pred_boxes[:, 1] + 1.) - inters)\n\n    overlaps = inters / uni\n    return overlaps\n"""
lib/evaluation/test_sg_eval.py,0,"b""# Just some tests so you can be assured that sg_eval.py works the same as the (original) stanford evaluation\n\nimport numpy as np\nfrom six.moves import xrange\nfrom dataloaders.visual_genome import VG\nfrom lib.evaluation.sg_eval import evaluate_from_dict\nfrom tqdm import trange\nfrom lib.fpn.box_utils import center_size, point_form\ndef eval_relation_recall(sg_entry,\n                         roidb_entry,\n                         result_dict,\n                         mode,\n                         iou_thresh):\n\n    # gt\n    gt_inds = np.where(roidb_entry['max_overlaps'] == 1)[0]\n    gt_boxes = roidb_entry['boxes'][gt_inds].copy().astype(float)\n    num_gt_boxes = gt_boxes.shape[0]\n    gt_relations = roidb_entry['gt_relations'].copy()\n    gt_classes = roidb_entry['gt_classes'].copy()\n\n    num_gt_relations = gt_relations.shape[0]\n    if num_gt_relations == 0:\n        return (None, None)\n    gt_class_scores = np.ones(num_gt_boxes)\n    gt_predicate_scores = np.ones(num_gt_relations)\n    gt_triplets, gt_triplet_boxes, _ = _triplet(gt_relations[:,2],\n                                             gt_relations[:,:2],\n                                             gt_classes,\n                                             gt_boxes,\n                                             gt_predicate_scores,\n                                             gt_class_scores)\n\n    # pred\n    box_preds = sg_entry['boxes']\n    num_boxes = box_preds.shape[0]\n    predicate_preds = sg_entry['relations']\n    class_preds = sg_entry['scores']\n    predicate_preds = predicate_preds.reshape(num_boxes, num_boxes, -1)\n\n    # no bg\n    predicate_preds = predicate_preds[:, :, 1:]\n    predicates = np.argmax(predicate_preds, 2).ravel() + 1\n    predicate_scores = predicate_preds.max(axis=2).ravel()\n    relations = []\n    keep = []\n    for i in xrange(num_boxes):\n        for j in xrange(num_boxes):\n            if i != j:\n                keep.append(num_boxes*i + j)\n                relations.append([i, j])\n    # take out self relations\n    predicates = predicates[keep]\n    predicate_scores = predicate_scores[keep]\n\n    relations = np.array(relations)\n    assert(relations.shape[0] == num_boxes * (num_boxes - 1))\n    assert(predicates.shape[0] == relations.shape[0])\n    num_relations = relations.shape[0]\n\n    if mode =='predcls':\n        # if predicate classification task\n        # use ground truth bounding boxes\n        assert(num_boxes == num_gt_boxes)\n        classes = gt_classes\n        class_scores = gt_class_scores\n        boxes = gt_boxes\n    elif mode =='sgcls':\n        assert(num_boxes == num_gt_boxes)\n        # if scene graph classification task\n        # use gt boxes, but predicted classes\n        classes = np.argmax(class_preds, 1)\n        class_scores = class_preds.max(axis=1)\n        boxes = gt_boxes\n    elif mode =='sgdet':\n        # if scene graph detection task\n        # use preicted boxes and predicted classes\n        classes = np.argmax(class_preds, 1)\n        class_scores = class_preds.max(axis=1)\n        boxes = []\n        for i, c in enumerate(classes):\n            boxes.append(box_preds[i]) # no bbox regression, c*4:(c+1)*4])\n        boxes = np.vstack(boxes)\n    else:\n        raise NotImplementedError('Incorrect Mode! %s' % mode)\n\n    pred_triplets, pred_triplet_boxes, relation_scores = \\\n        _triplet(predicates, relations, classes, boxes,\n                 predicate_scores, class_scores)\n\n\n    sorted_inds = np.argsort(relation_scores)[::-1]\n    # compue recall\n    for k in result_dict[mode + '_recall']:\n        this_k = min(k, num_relations)\n        keep_inds = sorted_inds[:this_k]\n        recall = _relation_recall(gt_triplets,\n                                  pred_triplets[keep_inds,:],\n                                  gt_triplet_boxes,\n                                  pred_triplet_boxes[keep_inds,:],\n                                  iou_thresh)\n        result_dict[mode + '_recall'][k].append(recall)\n\n    # for visualization\n    return pred_triplets[sorted_inds, :], pred_triplet_boxes[sorted_inds, :]\n\n\ndef _triplet(predicates, relations, classes, boxes,\n             predicate_scores, class_scores):\n\n    # format predictions into triplets\n    assert(predicates.shape[0] == relations.shape[0])\n    num_relations = relations.shape[0]\n    triplets = np.zeros([num_relations, 3]).astype(np.int32)\n    triplet_boxes = np.zeros([num_relations, 8]).astype(np.int32)\n    triplet_scores = np.zeros([num_relations]).astype(np.float32)\n    for i in xrange(num_relations):\n        triplets[i, 1] = predicates[i]\n        sub_i, obj_i = relations[i,:2]\n        triplets[i, 0] = classes[sub_i]\n        triplets[i, 2] = classes[obj_i]\n        triplet_boxes[i, :4] = boxes[sub_i, :]\n        triplet_boxes[i, 4:] = boxes[obj_i, :]\n        # compute triplet score\n        score =  class_scores[sub_i]\n        score *= class_scores[obj_i]\n        score *= predicate_scores[i]\n        triplet_scores[i] = score\n    return triplets, triplet_boxes, triplet_scores\n\n\ndef _relation_recall(gt_triplets, pred_triplets,\n                     gt_boxes, pred_boxes, iou_thresh):\n\n    # compute the R@K metric for a set of predicted triplets\n\n    num_gt = gt_triplets.shape[0]\n    num_correct_pred_gt = 0\n\n    for gt, gt_box in zip(gt_triplets, gt_boxes):\n        keep = np.zeros(pred_triplets.shape[0]).astype(bool)\n        for i, pred in enumerate(pred_triplets):\n            if gt[0] == pred[0] and gt[1] == pred[1] and gt[2] == pred[2]:\n                keep[i] = True\n        if not np.any(keep):\n            continue\n        boxes = pred_boxes[keep,:]\n        sub_iou = iou(gt_box[:4], boxes[:,:4])\n        obj_iou = iou(gt_box[4:], boxes[:,4:])\n        inds = np.intersect1d(np.where(sub_iou >= iou_thresh)[0],\n                              np.where(obj_iou >= iou_thresh)[0])\n        if inds.size > 0:\n            num_correct_pred_gt += 1\n    return float(num_correct_pred_gt) / float(num_gt)\n\n\ndef iou(gt_box, pred_boxes):\n    # computer Intersection-over-Union between two sets of boxes\n    ixmin = np.maximum(gt_box[0], pred_boxes[:,0])\n    iymin = np.maximum(gt_box[1], pred_boxes[:,1])\n    ixmax = np.minimum(gt_box[2], pred_boxes[:,2])\n    iymax = np.minimum(gt_box[3], pred_boxes[:,3])\n    iw = np.maximum(ixmax - ixmin + 1., 0.)\n    ih = np.maximum(iymax - iymin + 1., 0.)\n    inters = iw * ih\n\n    # union\n    uni = ((gt_box[2] - gt_box[0] + 1.) * (gt_box[3] - gt_box[1] + 1.) +\n            (pred_boxes[:, 2] - pred_boxes[:, 0] + 1.) *\n            (pred_boxes[:, 3] - pred_boxes[:, 1] + 1.) - inters)\n\n    overlaps = inters / uni\n    return overlaps\n\ntrain, val, test = VG.splits()\n\nresult_dict_mine = {'sgdet_recall': {20: [], 50: [], 100: []}}\nresult_dict_theirs = {'sgdet_recall': {20: [], 50: [], 100: []}}\n\nfor img_i in trange(len(val)):\n    gt_entry = {\n        'gt_classes': val.gt_classes[img_i].copy(),\n        'gt_relations': val.relationships[img_i].copy(),\n        'gt_boxes': val.gt_boxes[img_i].copy(),\n    }\n\n    # Use shuffled GT boxes\n    gt_indices = np.arange(gt_entry['gt_boxes'].shape[0]) #np.random.choice(gt_entry['gt_boxes'].shape[0], 20)\n    pred_boxes = gt_entry['gt_boxes'][gt_indices]\n\n    # Jitter the boxes a bit\n    pred_boxes = center_size(pred_boxes)\n    pred_boxes[:,:2] += np.random.rand(pred_boxes.shape[0], 2)*128\n    pred_boxes[:,2:] *= (1+np.random.randn(pred_boxes.shape[0], 2).clip(-0.1, 0.1))\n    pred_boxes = point_form(pred_boxes)\n\n    obj_scores = np.random.rand(pred_boxes.shape[0])\n\n    rels_to_use = np.column_stack(np.where(1 - np.diag(np.ones(pred_boxes.shape[0], dtype=np.int32))))\n    rel_scores = np.random.rand(min(100, rels_to_use.shape[0]), 51)\n    rel_scores = rel_scores / rel_scores.sum(1, keepdims=True)\n    pred_rel_inds = rels_to_use[np.random.choice(rels_to_use.shape[0], rel_scores.shape[0],\n                                                               replace=False)]\n\n    # We must sort by P(o, o, r)\n    rel_order = np.argsort(-rel_scores[:,1:].max(1) * obj_scores[pred_rel_inds[:,0]] * obj_scores[pred_rel_inds[:,1]])\n\n    pred_entry = {\n        'pred_boxes': pred_boxes,\n        'pred_classes': gt_entry['gt_classes'][gt_indices], #1+np.random.choice(150, pred_boxes.shape[0], replace=True),\n        'obj_scores': obj_scores,\n        'pred_rel_inds': pred_rel_inds[rel_order],\n        'rel_scores': rel_scores[rel_order],\n    }\n\n    # def check_whether_they_are_the_same(gt_entry, pred_entry):\n    evaluate_from_dict(gt_entry, pred_entry, 'sgdet', result_dict_mine, multiple_preds=False,\n                       viz_dict=None)\n\n    #########################\n    predicate_scores_theirs = np.zeros((pred_boxes.shape[0], pred_boxes.shape[0], 51), dtype=np.float64)\n    for (o1, o2), s in zip(pred_entry['pred_rel_inds'], pred_entry['rel_scores']):\n        predicate_scores_theirs[o1, o2] = s\n\n    obj_scores_theirs = np.zeros((obj_scores.shape[0], 151), dtype=np.float64)\n    obj_scores_theirs[np.arange(obj_scores.shape[0]), pred_entry['pred_classes']] = obj_scores\n\n    sg_entry_orig_format = {\n        'boxes': pred_entry['pred_boxes'],\n        # 'gt_classes': gt_entry['gt_classes'],\n        # 'gt_relations': gt_entry['gt_relations'],\n        'relations': predicate_scores_theirs,\n        'scores': obj_scores_theirs\n    }\n    roidb_entry = {\n        'max_overlaps': np.concatenate((np.ones(gt_entry['gt_boxes'].shape[0]), np.zeros(pred_entry['pred_boxes'].shape[0])), 0),\n        'boxes': np.concatenate((gt_entry['gt_boxes'], pred_entry['pred_boxes']), 0),\n        'gt_classes': gt_entry['gt_classes'],\n        'gt_relations': gt_entry['gt_relations'],\n    }\n    eval_relation_recall(sg_entry_orig_format, roidb_entry, result_dict_theirs, 'sgdet', iou_thresh=0.5)\n\nmy_results = np.array(result_dict_mine['sgdet_recall'][20])\ntheir_results = np.array(result_dict_theirs['sgdet_recall'][20])\n\nassert np.all(my_results == their_results)"""
lib/fpn/anchor_targets.py,0,"b'""""""\nGenerates anchor targets to train the detector. Does this during the collate step in training\nas it\'s much cheaper to do this on a separate thread.\n\nHeavily adapted from faster_rcnn/rpn_msr/anchor_target_layer.py.\n""""""\nimport numpy as np\nimport numpy.random as npr\n\nfrom config import IM_SCALE, RPN_NEGATIVE_OVERLAP, RPN_POSITIVE_OVERLAP, \\\n    RPN_BATCHSIZE, RPN_FG_FRACTION, ANCHOR_SIZE, ANCHOR_SCALES, ANCHOR_RATIOS\nfrom lib.fpn.box_intersections_cpu.bbox import bbox_overlaps\nfrom lib.fpn.generate_anchors import generate_anchors\n\n\ndef anchor_target_layer(gt_boxes, im_size, \n                        allowed_border=0):\n    """"""\n    Assign anchors to ground-truth targets. Produces anchor classification\n    labels and bounding-box regression targets.\n\n    for each (H, W) location i\n      generate 3 anchor boxes centered on cell i\n    filter out-of-image anchors\n    measure GT overlap\n\n    :param gt_boxes: [x1, y1, x2, y2] boxes. These are assumed to be at the same scale as\n                     the image (IM_SCALE)\n    :param im_size: Size of the image (h, w). This is assumed to be scaled to IM_SCALE\n    """"""\n    if max(im_size) != IM_SCALE:\n        raise ValueError(""im size is {}"".format(im_size))\n    h, w = im_size\n\n    # Get the indices of the anchors in the feature map.\n    # h, w, A, 4\n    ans_np = generate_anchors(base_size=ANCHOR_SIZE,\n                              feat_stride=16,\n                              anchor_scales=ANCHOR_SCALES,\n                              anchor_ratios=ANCHOR_RATIOS,\n                              )\n    ans_np_flat = ans_np.reshape((-1, 4))\n    inds_inside = np.where(\n        (ans_np_flat[:, 0] >= -allowed_border) &\n        (ans_np_flat[:, 1] >= -allowed_border) &\n        (ans_np_flat[:, 2] < w + allowed_border) &  # width\n        (ans_np_flat[:, 3] < h + allowed_border)  # height\n    )[0]\n    good_ans_flat = ans_np_flat[inds_inside]\n    if good_ans_flat.size == 0:\n        raise ValueError(""There were no good anchors for an image of size {} with boxes {}"".format(im_size, gt_boxes))\n\n    # overlaps between the anchors and the gt boxes [num_anchors, num_gtboxes]\n    overlaps = bbox_overlaps(good_ans_flat, gt_boxes)\n    anchor_to_gtbox = overlaps.argmax(axis=1)\n    max_overlaps = overlaps[np.arange(anchor_to_gtbox.shape[0]), anchor_to_gtbox]\n    gtbox_to_anchor = overlaps.argmax(axis=0)\n    gt_max_overlaps = overlaps[gtbox_to_anchor, np.arange(overlaps.shape[1])]\n    gt_argmax_overlaps = np.where(overlaps == gt_max_overlaps)[0]\n\n    # Good anchors are those that match SOMEWHERE within a decent tolerance\n    # label: 1 is positive, 0 is negative, -1 is dont care.\n    # assign bg labels first so that positive labels can clobber them\n    labels = (-1) * np.ones(overlaps.shape[0], dtype=np.int64)\n    labels[max_overlaps < RPN_NEGATIVE_OVERLAP] = 0\n    labels[gt_argmax_overlaps] = 1\n    labels[max_overlaps >= RPN_POSITIVE_OVERLAP] = 1\n\n    # subsample positive labels if we have too many\n    num_fg = int(RPN_FG_FRACTION * RPN_BATCHSIZE)\n    fg_inds = np.where(labels == 1)[0]\n    if len(fg_inds) > num_fg:\n        labels[npr.choice(fg_inds, size=(len(fg_inds) - num_fg), replace=False)] = -1\n\n    # subsample negative labels if we have too many\n    num_bg = RPN_BATCHSIZE - np.sum(labels == 1)\n    bg_inds = np.where(labels == 0)[0]\n    if len(bg_inds) > num_bg:\n        labels[npr.choice(bg_inds, size=(len(bg_inds) - num_bg), replace=False)] = -1\n    # print(""{} fg {} bg ratio{:.3f} inds inside {}"".format(RPN_BATCHSIZE-num_bg, num_bg, (RPN_BATCHSIZE-num_bg)/RPN_BATCHSIZE, inds_inside.shape[0]))\n\n\n    # Get the labels at the original size\n    labels_unmap = (-1) * np.ones(ans_np_flat.shape[0], dtype=np.int64)\n    labels_unmap[inds_inside] = labels\n\n    # h, w, A\n    labels_unmap_res = labels_unmap.reshape(ans_np.shape[:-1])\n    anchor_inds = np.column_stack(np.where(labels_unmap_res >= 0))\n\n    # These ought to be in the same order\n    anchor_inds_flat = np.where(labels >= 0)[0]\n    anchors = good_ans_flat[anchor_inds_flat]\n    bbox_targets = gt_boxes[anchor_to_gtbox[anchor_inds_flat]]\n    labels = labels[anchor_inds_flat]\n\n    assert np.all(labels >= 0)\n\n\n    # Anchors: [num_used, 4]\n    # Anchor_inds: [num_used, 3] (h, w, A)\n    # bbox_targets: [num_used, 4]\n    # labels: [num_used]\n\n    return anchors, anchor_inds, bbox_targets, labels\n'"
lib/fpn/box_utils.py,13,"b'import torch\nimport numpy as np\nfrom torch.nn import functional as F\nfrom lib.fpn.box_intersections_cpu.bbox import bbox_overlaps as bbox_overlaps_np\nfrom lib.fpn.box_intersections_cpu.bbox import bbox_intersections as bbox_intersections_np\n\n\ndef bbox_loss(prior_boxes, deltas, gt_boxes, eps=1e-4, scale_before=1):\n    """"""\n    Computes the loss for predicting the GT boxes from prior boxes\n    :param prior_boxes: [num_boxes, 4] (x1, y1, x2, y2)\n    :param deltas: [num_boxes, 4]    (tx, ty, th, tw)\n    :param gt_boxes: [num_boxes, 4] (x1, y1, x2, y2)\n    :return:\n    """"""\n    prior_centers = center_size(prior_boxes) #(cx, cy, w, h)\n    gt_centers = center_size(gt_boxes) #(cx, cy, w, h)\n\n    center_targets = (gt_centers[:, :2] - prior_centers[:, :2]) / prior_centers[:, 2:]\n    size_targets = torch.log(gt_centers[:, 2:]) - torch.log(prior_centers[:, 2:])\n    all_targets = torch.cat((center_targets, size_targets), 1)\n\n    loss = F.smooth_l1_loss(deltas, all_targets, size_average=False)/(eps + prior_centers.size(0))\n\n    return loss\n\n\ndef bbox_preds(boxes, deltas):\n    """"""\n    Converts ""deltas"" (predicted by the network) along with prior boxes\n    into (x1, y1, x2, y2) representation.\n    :param boxes: Prior boxes, represented as (x1, y1, x2, y2)\n    :param deltas: Offsets (tx, ty, tw, th)\n    :param box_strides [num_boxes,] distance apart between boxes. anchor box can\'t go more than\n       \\pm box_strides/2 from its current position. If None then we\'ll use the widths\n       and heights\n    :return: Transformed boxes\n    """"""\n\n    if boxes.size(0) == 0:\n        return boxes\n    prior_centers = center_size(boxes)\n\n    xys = prior_centers[:, :2] + prior_centers[:, 2:] * deltas[:, :2]\n\n    whs = torch.exp(deltas[:, 2:]) * prior_centers[:, 2:]\n\n    return point_form(torch.cat((xys, whs), 1))\n\n\ndef center_size(boxes):\n    """""" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    wh = boxes[:, 2:] - boxes[:, :2] + 1.0\n\n    if isinstance(boxes, np.ndarray):\n        return np.column_stack((boxes[:, :2] + 0.5 * wh, wh))\n    return torch.cat((boxes[:, :2] + 0.5 * wh, wh), 1)\n\n\ndef point_form(boxes):\n    """""" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    if isinstance(boxes, np.ndarray):\n        return np.column_stack((boxes[:, :2] - 0.5 * boxes[:, 2:],\n                                boxes[:, :2] + 0.5 * (boxes[:, 2:] - 2.0)))\n    return torch.cat((boxes[:, :2] - 0.5 * boxes[:, 2:],\n                      boxes[:, :2] + 0.5 * (boxes[:, 2:] - 2.0)), 1)  # xmax, ymax\n\n\n###########################################################################\n### Torch Utils, creds to Max de Groot\n###########################################################################\n\ndef bbox_intersections(box_a, box_b):\n    """""" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    """"""\n    if isinstance(box_a, np.ndarray):\n        assert isinstance(box_b, np.ndarray)\n        return bbox_intersections_np(box_a, box_b)\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy + 1.0), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef bbox_overlaps(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    """"""\n    if isinstance(box_a, np.ndarray):\n        assert isinstance(box_b, np.ndarray)\n        return bbox_overlaps_np(box_a, box_b)\n\n    inter = bbox_intersections(box_a, box_b)\n    area_a = ((box_a[:, 2] - box_a[:, 0] + 1.0) *\n              (box_a[:, 3] - box_a[:, 1] + 1.0)).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2] - box_b[:, 0] + 1.0) *\n              (box_b[:, 3] - box_b[:, 1] + 1.0)).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\ndef nms_overlaps(boxes):\n    """""" get overlaps for each channel""""""\n    assert boxes.dim() == 3\n    N = boxes.size(0)\n    nc = boxes.size(1)\n    max_xy = torch.min(boxes[:, None, :, 2:].expand(N, N, nc, 2),\n                       boxes[None, :, :, 2:].expand(N, N, nc, 2))\n\n    min_xy = torch.max(boxes[:, None, :, :2].expand(N, N, nc, 2),\n                       boxes[None, :, :, :2].expand(N, N, nc, 2))\n\n    inter = torch.clamp((max_xy - min_xy + 1.0), min=0)\n\n    # n, n, 151\n    inters = inter[:,:,:,0]*inter[:,:,:,1]\n    boxes_flat = boxes.view(-1, 4)\n    areas_flat = (boxes_flat[:,2]- boxes_flat[:,0]+1.0)*(\n        boxes_flat[:,3]- boxes_flat[:,1]+1.0)\n    areas = areas_flat.view(boxes.size(0), boxes.size(1))\n    union = -inters + areas[None] + areas[:, None]\n    return inters / union\n\n'"
lib/fpn/generate_anchors.py,0,"b'# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\nfrom config import IM_SCALE\n\nimport numpy as np\n\n\n# Verify that we compute the same anchors as Shaoqing\'s matlab implementation:\n#\n#    >> load output/rpn_cachedir/faster_rcnn_VOC2007_ZF_stage1_rpn/anchors.mat\n#    >> anchors\n#\n#    anchors =\n#\n#       -83   -39   100    56\n#      -175   -87   192   104\n#      -359  -183   376   200\n#       -55   -55    72    72\n#      -119  -119   136   136\n#      -247  -247   264   264\n#       -35   -79    52    96\n#       -79  -167    96   184\n#      -167  -343   184   360\n\n# array([[ -83.,  -39.,  100.,   56.],\n#       [-175.,  -87.,  192.,  104.],\n#       [-359., -183.,  376.,  200.],\n#       [ -55.,  -55.,   72.,   72.],\n#       [-119., -119.,  136.,  136.],\n#       [-247., -247.,  264.,  264.],\n#       [ -35.,  -79.,   52.,   96.],\n#       [ -79., -167.,   96.,  184.],\n#       [-167., -343.,  184.,  360.]])\n\ndef generate_anchors(base_size=16, feat_stride=16, anchor_scales=(8,16,32), anchor_ratios=(0.5,1,2)):\n  """""" A wrapper function to generate anchors given different scales\n    Also return the number of anchors in variable \'length\'\n  """"""\n  anchors = generate_base_anchors(base_size=base_size, \n                                  ratios=np.array(anchor_ratios),\n                                  scales=np.array(anchor_scales))\n  A = anchors.shape[0]\n  shift_x = np.arange(0, IM_SCALE // feat_stride) * feat_stride # Same as shift_x\n  shift_x, shift_y = np.meshgrid(shift_x, shift_x)\n\n  shifts = np.stack([shift_x, shift_y, shift_x, shift_y], -1)  # h, w, 4\n  all_anchors = shifts[:, :, None] + anchors[None, None]  #h, w, A, 4\n  return all_anchors\n\n  # shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel())).transpose()\n  # K = shifts.shape[0]\n  # # width changes faster, so here it is H, W, C\n  # anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n  # anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False)\n  # length = np.int32(anchors.shape[0])\n\n\ndef generate_base_anchors(base_size=16, ratios=[0.5, 1, 2], scales=2 ** np.arange(3, 6)):\n  """"""\n  Generate anchor (reference) windows by enumerating aspect ratios X\n  scales wrt a reference (0, 0, 15, 15) window.\n  """"""\n\n  base_anchor = np.array([1, 1, base_size, base_size]) - 1\n  ratio_anchors = _ratio_enum(base_anchor, ratios)\n  anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)\n                       for i in range(ratio_anchors.shape[0])])\n  return anchors\n\n\ndef _whctrs(anchor):\n  """"""\n  Return width, height, x center, and y center for an anchor (window).\n  """"""\n\n  w = anchor[2] - anchor[0] + 1\n  h = anchor[3] - anchor[1] + 1\n  x_ctr = anchor[0] + 0.5 * (w - 1)\n  y_ctr = anchor[1] + 0.5 * (h - 1)\n  return w, h, x_ctr, y_ctr\n\n\ndef _mkanchors(ws, hs, x_ctr, y_ctr):\n  """"""\n  Given a vector of widths (ws) and heights (hs) around a center\n  (x_ctr, y_ctr), output a set of anchors (windows).\n  """"""\n\n  ws = ws[:, np.newaxis]\n  hs = hs[:, np.newaxis]\n  anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n                       y_ctr - 0.5 * (hs - 1),\n                       x_ctr + 0.5 * (ws - 1),\n                       y_ctr + 0.5 * (hs - 1)))\n  return anchors\n\n\ndef _ratio_enum(anchor, ratios):\n  """"""\n  Enumerate a set of anchors for each aspect ratio wrt an anchor.\n  """"""\n\n  w, h, x_ctr, y_ctr = _whctrs(anchor)\n  size = w * h\n  size_ratios = size / ratios\n  # NOTE: CHANGED TO NOT HAVE ROUNDING\n  ws = np.sqrt(size_ratios)\n  hs = ws * ratios\n  anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n  return anchors\n\n\ndef _scale_enum(anchor, scales):\n  """"""\n  Enumerate a set of anchors for each scale wrt an anchor.\n  """"""\n\n  w, h, x_ctr, y_ctr = _whctrs(anchor)\n  ws = w * scales\n  hs = h * scales\n  anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n  return anchors\n'"
lib/lstm/__init__.py,0,b''
lib/lstm/decoder_rnn.py,26,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import PackedSequence\nfrom typing import Optional, Tuple\n\nfrom lib.fpn.box_utils import nms_overlaps\nfrom lib.word_vectors import obj_edge_vectors\nfrom .highway_lstm_cuda.alternating_highway_lstm import block_orthogonal\nimport numpy as np\n\ndef get_dropout_mask(dropout_probability: float, tensor_for_masking: torch.autograd.Variable):\n    """"""\n    Computes and returns an element-wise dropout mask for a given tensor, where\n    each element in the mask is dropped out with probability dropout_probability.\n    Note that the mask is NOT applied to the tensor - the tensor is passed to retain\n    the correct CUDA tensor type for the mask.\n\n    Parameters\n    ----------\n    dropout_probability : float, required.\n        Probability of dropping a dimension of the input.\n    tensor_for_masking : torch.Variable, required.\n\n\n    Returns\n    -------\n    A torch.FloatTensor consisting of the binary mask scaled by 1/ (1 - dropout_probability).\n    This scaling ensures expected values and variances of the output of applying this mask\n     and the original tensor are the same.\n    """"""\n    binary_mask = tensor_for_masking.clone()\n    binary_mask.data.copy_(torch.rand(tensor_for_masking.size()) > dropout_probability)\n    # Scale mask by 1/keep_prob to preserve output statistics.\n    dropout_mask = binary_mask.float().div(1.0 - dropout_probability)\n    return dropout_mask\n\n\nclass DecoderRNN(torch.nn.Module):\n    def __init__(self, classes, embed_dim, inputs_dim, hidden_dim, recurrent_dropout_probability=0.2,\n                 use_highway=True, use_input_projection_bias=True):\n        """"""\n        Initializes the RNN\n        :param embed_dim: Dimension of the embeddings\n        :param encoder_hidden_dim: Hidden dim of the encoder, for attention purposes\n        :param hidden_dim: Hidden dim of the decoder\n        :param vocab_size: Number of words in the vocab\n        :param bos_token: To use during decoding (non teacher forcing mode))\n        :param bos: beginning of sentence token\n        :param unk: unknown token (not used)\n        """"""\n        super(DecoderRNN, self).__init__()\n\n        self.classes = classes\n        embed_vecs = obj_edge_vectors([\'start\'] + self.classes, wv_dim=100)\n        self.obj_embed = nn.Embedding(len(self.classes), embed_dim)\n        self.obj_embed.weight.data = embed_vecs\n        self.hidden_size = hidden_dim\n        self.inputs_dim = inputs_dim\n        self.nms_thresh = 0.3\n\n        self.recurrent_dropout_probability=recurrent_dropout_probability\n        self.use_highway=use_highway\n        # We do the projections for all the gates all at once, so if we are\n        # using highway layers, we need some extra projections, which is\n        # why the sizes of the Linear layers change here depending on this flag.\n        if use_highway:\n            self.input_linearity = torch.nn.Linear(self.input_size, 6 * self.hidden_size,\n                                                   bias=use_input_projection_bias)\n            self.state_linearity = torch.nn.Linear(self.hidden_size, 5 * self.hidden_size,\n                                                   bias=True)\n        else:\n            self.input_linearity = torch.nn.Linear(self.input_size, 4 * self.hidden_size,\n                                                   bias=use_input_projection_bias)\n            self.state_linearity = torch.nn.Linear(self.hidden_size, 4 * self.hidden_size,\n                                                   bias=True)\n\n        self.out = nn.Linear(self.hidden_size, len(self.classes))\n        self.reset_parameters()\n\n    @property\n    def input_size(self):\n        return self.inputs_dim + self.obj_embed.weight.size(1)\n\n    def reset_parameters(self):\n        # Use sensible default initializations for parameters.\n        block_orthogonal(self.input_linearity.weight.data, [self.hidden_size, self.input_size])\n        block_orthogonal(self.state_linearity.weight.data, [self.hidden_size, self.hidden_size])\n\n        self.state_linearity.bias.data.fill_(0.0)\n        # Initialize forget gate biases to 1.0 as per An Empirical\n        # Exploration of Recurrent Network Architectures, (Jozefowicz, 2015).\n        self.state_linearity.bias.data[self.hidden_size:2 * self.hidden_size].fill_(1.0)\n\n    def lstm_equations(self, timestep_input, previous_state, previous_memory, dropout_mask=None):\n        """"""\n        Does the hairy LSTM math\n        :param timestep_input:\n        :param previous_state:\n        :param previous_memory:\n        :param dropout_mask:\n        :return:\n        """"""\n        # Do the projections for all the gates all at once.\n        projected_input = self.input_linearity(timestep_input)\n        projected_state = self.state_linearity(previous_state)\n\n        # Main LSTM equations using relevant chunks of the big linear\n        # projections of the hidden state and inputs.\n        input_gate = torch.sigmoid(projected_input[:, 0 * self.hidden_size:1 * self.hidden_size] +\n                                   projected_state[:, 0 * self.hidden_size:1 * self.hidden_size])\n        forget_gate = torch.sigmoid(projected_input[:, 1 * self.hidden_size:2 * self.hidden_size] +\n                                    projected_state[:, 1 * self.hidden_size:2 * self.hidden_size])\n        memory_init = torch.tanh(projected_input[:, 2 * self.hidden_size:3 * self.hidden_size] +\n                                 projected_state[:, 2 * self.hidden_size:3 * self.hidden_size])\n        output_gate = torch.sigmoid(projected_input[:, 3 * self.hidden_size:4 * self.hidden_size] +\n                                    projected_state[:, 3 * self.hidden_size:4 * self.hidden_size])\n        memory = input_gate * memory_init + forget_gate * previous_memory\n        timestep_output = output_gate * torch.tanh(memory)\n\n        if self.use_highway:\n            highway_gate = torch.sigmoid(projected_input[:, 4 * self.hidden_size:5 * self.hidden_size] +\n                                         projected_state[:, 4 * self.hidden_size:5 * self.hidden_size])\n            highway_input_projection = projected_input[:, 5 * self.hidden_size:6 * self.hidden_size]\n            timestep_output = highway_gate * timestep_output + (1 - highway_gate) * highway_input_projection\n\n        # Only do dropout if the dropout prob is > 0.0 and we are in training mode.\n        if dropout_mask is not None and self.training:\n            timestep_output = timestep_output * dropout_mask\n        return timestep_output, memory\n\n    def forward(self,  # pylint: disable=arguments-differ\n                inputs: PackedSequence,\n                initial_state: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n                labels=None, boxes_for_nms=None):\n        """"""\n        Parameters\n        ----------\n        inputs : PackedSequence, required.\n            A tensor of shape (batch_size, num_timesteps, input_size)\n            to apply the LSTM over.\n\n        initial_state : Tuple[torch.Tensor, torch.Tensor], optional, (default = None)\n            A tuple (state, memory) representing the initial hidden state and memory\n            of the LSTM. Each tensor has shape (1, batch_size, output_dimension).\n\n        Returns\n        -------\n        A PackedSequence containing a torch.FloatTensor of shape\n        (batch_size, num_timesteps, output_dimension) representing\n        the outputs of the LSTM per timestep and a tuple containing\n        the LSTM state, with shape (1, batch_size, hidden_size) to\n        match the Pytorch API.\n        """"""\n        if not isinstance(inputs, PackedSequence):\n            raise ValueError(\'inputs must be PackedSequence but got %s\' % (type(inputs)))\n\n        assert isinstance(inputs, PackedSequence)\n        sequence_tensor, batch_lengths = inputs\n        batch_size = batch_lengths[0]\n\n        # We\'re just doing an LSTM decoder here so ignore states, etc\n        if initial_state is None:\n            previous_memory = Variable(sequence_tensor.data.new()\n                                                  .resize_(batch_size, self.hidden_size).fill_(0))\n            previous_state = Variable(sequence_tensor.data.new()\n                                                 .resize_(batch_size, self.hidden_size).fill_(0))\n        else:\n            assert len(initial_state) == 2\n            previous_state = initial_state[0].squeeze(0)\n            previous_memory = initial_state[1].squeeze(0)\n\n        previous_embed = self.obj_embed.weight[0, None].expand(batch_size, 100)\n\n        if self.recurrent_dropout_probability > 0.0:\n            dropout_mask = get_dropout_mask(self.recurrent_dropout_probability, previous_memory)\n        else:\n            dropout_mask = None\n\n        # Only accumulating label predictions here, discarding everything else\n        out_dists = []\n        out_commitments = []\n\n        end_ind = 0\n        for i, l_batch in enumerate(batch_lengths):\n            start_ind = end_ind\n            end_ind = end_ind + l_batch\n\n            if previous_memory.size(0) != l_batch:\n                previous_memory = previous_memory[:l_batch]\n                previous_state = previous_state[:l_batch]\n                previous_embed = previous_embed[:l_batch]\n                if dropout_mask is not None:\n                    dropout_mask = dropout_mask[:l_batch]\n\n            timestep_input = torch.cat((sequence_tensor[start_ind:end_ind], previous_embed), 1)\n\n            previous_state, previous_memory = self.lstm_equations(timestep_input, previous_state,\n                                                                  previous_memory, dropout_mask=dropout_mask)\n\n            pred_dist = self.out(previous_state)\n            out_dists.append(pred_dist)\n\n            if self.training:\n                labels_to_embed = labels[start_ind:end_ind].clone()\n                # Whenever labels are 0 set input to be our max prediction\n                nonzero_pred = pred_dist[:, 1:].max(1)[1] + 1\n                is_bg = (labels_to_embed.data == 0).nonzero()\n                if is_bg.dim() > 0:\n                    labels_to_embed[is_bg.squeeze(1)] = nonzero_pred[is_bg.squeeze(1)]\n                out_commitments.append(labels_to_embed)\n                previous_embed = self.obj_embed(labels_to_embed+1)\n            else:\n                assert l_batch == 1\n                out_dist_sample = F.softmax(pred_dist, dim=1)\n                # if boxes_for_nms is not None:\n                #     out_dist_sample[domains_allowed[i] == 0] = 0.0\n\n                # Greedily take the max here amongst non-bgs\n                best_ind = out_dist_sample[:, 1:].max(1)[1] + 1\n\n                # if boxes_for_nms is not None and i < boxes_for_nms.size(0):\n                #     best_int = int(best_ind.data[0])\n                #     domains_allowed[i:, best_int] *= (1 - is_overlap[i, i:, best_int])\n                out_commitments.append(best_ind)\n                previous_embed = self.obj_embed(best_ind+1)\n\n        # Do NMS here as a post-processing step\n        if boxes_for_nms is not None and not self.training:\n            is_overlap = nms_overlaps(boxes_for_nms.data).view(\n                boxes_for_nms.size(0), boxes_for_nms.size(0), boxes_for_nms.size(1)\n            ).cpu().numpy() >= self.nms_thresh\n            # is_overlap[np.arange(boxes_for_nms.size(0)), np.arange(boxes_for_nms.size(0))] = False\n\n            out_dists_sampled = F.softmax(torch.cat(out_dists,0), 1).data.cpu().numpy()\n            out_dists_sampled[:,0] = 0\n\n            out_commitments = out_commitments[0].data.new(len(out_commitments)).fill_(0)\n\n            for i in range(out_commitments.size(0)):\n                box_ind, cls_ind = np.unravel_index(out_dists_sampled.argmax(), out_dists_sampled.shape)\n                out_commitments[int(box_ind)] = int(cls_ind)\n                out_dists_sampled[is_overlap[box_ind,:,cls_ind], cls_ind] = 0.0\n                out_dists_sampled[box_ind] = -1.0 # This way we won\'t re-sample\n\n            out_commitments = Variable(out_commitments)\n        else:\n            out_commitments = torch.cat(out_commitments, 0)\n\n        return torch.cat(out_dists, 0), out_commitments\n'"
lib/fpn/box_intersections_cpu/setup.py,0,"b'from distutils.core import setup\nfrom Cython.Build import cythonize\nimport numpy\n\nsetup(name=""bbox_cython"", ext_modules=cythonize(\'bbox.pyx\'), include_dirs=[numpy.get_include()])'"
lib/fpn/nms/build.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n# Might have to export PATH=/usr/local/cuda-8.0/bin${PATH:+:${PATH}}\n\nsources = []\nheaders = []\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/nms_cuda.c']\n    headers += ['src/nms_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/cuda/nms.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.nms',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n\n"""
lib/fpn/proposal_assignments/proposal_assignments_det.py,8,"b'\nimport numpy as np\nimport numpy.random as npr\nfrom config import BG_THRESH_HI, BG_THRESH_LO, FG_FRACTION, ROIS_PER_IMG\nfrom lib.fpn.box_utils import bbox_overlaps\nfrom lib.pytorch_misc import to_variable\nimport torch\n\n#############################################################\n# The following is only for object detection\n@to_variable\ndef proposal_assignments_det(rpn_rois, gt_boxes, gt_classes, image_offset, fg_thresh=0.5):\n    """"""\n    Assign object detection proposals to ground-truth targets. Produces proposal\n    classification labels and bounding-box regression targets.\n    :param rpn_rois: [img_ind, x1, y1, x2, y2]\n    :param gt_boxes:   [num_boxes, 4] array of x0, y0, x1, y1\n    :param gt_classes: [num_boxes, 2] array of [img_ind, class]\n    :param Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n    :return:\n        rois: [num_rois, 5]\n        labels: [num_rois] array of labels\n        bbox_targets [num_rois, 4] array of targets for the labels.\n    """"""\n    fg_rois_per_image = int(np.round(ROIS_PER_IMG * FG_FRACTION))\n\n    gt_img_inds = gt_classes[:, 0] - image_offset\n\n    all_boxes = torch.cat([rpn_rois[:, 1:], gt_boxes], 0)\n\n    ims_per_box = torch.cat([rpn_rois[:, 0].long(), gt_img_inds], 0)\n\n    im_sorted, idx = torch.sort(ims_per_box, 0)\n    all_boxes = all_boxes[idx]\n\n    # Assume that the GT boxes are already sorted in terms of image id\n    num_images = int(im_sorted[-1]) + 1\n\n    labels = []\n    rois = []\n    bbox_targets = []\n    for im_ind in range(num_images):\n        g_inds = (gt_img_inds == im_ind).nonzero()\n\n        if g_inds.dim() == 0:\n            continue\n        g_inds = g_inds.squeeze(1)\n        g_start = g_inds[0]\n        g_end = g_inds[-1] + 1\n\n        t_inds = (im_sorted == im_ind).nonzero().squeeze(1)\n        t_start = t_inds[0]\n        t_end = t_inds[-1] + 1\n\n        # Max overlaps: for each predicted box, get the max ROI\n        # Get the indices into the GT boxes too (must offset by the box start)\n        ious = bbox_overlaps(all_boxes[t_start:t_end], gt_boxes[g_start:g_end])\n        max_overlaps, gt_assignment = ious.max(1)\n        max_overlaps = max_overlaps.cpu().numpy()\n        # print(""Best overlap is {}"".format(max_overlaps.max()))\n        # print(""\\ngt assignment is {} while g_start is {} \\n ---"".format(gt_assignment, g_start))\n        gt_assignment += g_start\n\n        keep_inds_np, num_fg = _sel_inds(max_overlaps, fg_thresh, fg_rois_per_image,\n                                         ROIS_PER_IMG)\n\n        if keep_inds_np.size == 0:\n            continue\n\n        keep_inds = torch.LongTensor(keep_inds_np).cuda(rpn_rois.get_device())\n\n        labels_ = gt_classes[:, 1][gt_assignment[keep_inds]]\n        bbox_target_ = gt_boxes[gt_assignment[keep_inds]]\n\n        # Clamp labels_ for the background RoIs to 0\n        if num_fg < labels_.size(0):\n            labels_[num_fg:] = 0\n\n        rois_ = torch.cat((\n            im_sorted[t_start:t_end, None][keep_inds].float(),\n            all_boxes[t_start:t_end][keep_inds],\n        ), 1)\n\n        labels.append(labels_)\n        rois.append(rois_)\n        bbox_targets.append(bbox_target_)\n\n    rois = torch.cat(rois, 0)\n    labels = torch.cat(labels, 0)\n    bbox_targets = torch.cat(bbox_targets, 0)\n    return rois, labels, bbox_targets\n\n\ndef _sel_inds(max_overlaps, fg_thresh=0.5, fg_rois_per_image=128, rois_per_image=256):\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = np.where(max_overlaps >= fg_thresh)[0]\n\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = min(fg_rois_per_image, fg_inds.shape[0])\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(fg_inds, size=fg_rois_per_this_image, replace=False)\n\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((max_overlaps < BG_THRESH_HI) & (max_overlaps >= BG_THRESH_LO))[0]\n\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    bg_rois_per_this_image = min(bg_rois_per_this_image, bg_inds.size)\n    # Sample background regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(bg_inds, size=bg_rois_per_this_image, replace=False)\n\n    return np.append(fg_inds, bg_inds), fg_rois_per_this_image\n\n'"
lib/fpn/proposal_assignments/proposal_assignments_gtbox.py,5,"b'from lib.pytorch_misc import enumerate_by_image, gather_nd, random_choose\nfrom lib.fpn.box_utils import bbox_preds, center_size, bbox_overlaps\nimport torch\nfrom lib.pytorch_misc import diagonal_inds, to_variable\nfrom config import RELS_PER_IMG, REL_FG_FRACTION\n\n\n@to_variable\ndef proposal_assignments_gtbox(rois, gt_boxes, gt_classes, gt_rels, image_offset, fg_thresh=0.5):\n    """"""\n    Assign object detection proposals to ground-truth targets. Produces proposal\n    classification labels and bounding-box regression targets.\n    :param rpn_rois: [img_ind, x1, y1, x2, y2]\n    :param gt_boxes:   [num_boxes, 4] array of x0, y0, x1, y1]. Not needed it seems\n    :param gt_classes: [num_boxes, 2] array of [img_ind, class]\n        Note, the img_inds here start at image_offset\n    :param gt_rels     [num_boxes, 4] array of [img_ind, box_0, box_1, rel type].\n        Note, the img_inds here start at image_offset\n    :param Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n    :return:\n        rois: [num_rois, 5]\n        labels: [num_rois] array of labels\n        bbox_targets [num_rois, 4] array of targets for the labels.\n        rel_labels: [num_rels, 4] (img ind, box0 ind, box1ind, rel type)\n    """"""\n    im_inds = rois[:,0].long()\n\n    num_im = im_inds[-1] + 1\n\n    # Offset the image indices in fg_rels to refer to absolute indices (not just within img i)\n    fg_rels = gt_rels.clone()\n    fg_rels[:,0] -= image_offset\n    offset = {}\n    for i, s, e in enumerate_by_image(im_inds):\n        offset[i] = s\n    for i, s, e in enumerate_by_image(fg_rels[:, 0]):\n        fg_rels[s:e, 1:3] += offset[i]\n\n    # Try ALL things, not just intersections.\n    is_cand = (im_inds[:, None] == im_inds[None])\n    is_cand.view(-1)[diagonal_inds(is_cand)] = 0\n\n    # # Compute salience\n    # gt_inds = fg_rels[:, 1:3].contiguous().view(-1)\n    # labels_arange = labels.data.new(labels.size(0))\n    # torch.arange(0, labels.size(0), out=labels_arange)\n    # salience_labels = ((gt_inds[:, None] == labels_arange[None]).long().sum(0) > 0).long()\n    # labels = torch.stack((labels, salience_labels), 1)\n\n    # Add in some BG labels\n\n    # NOW WE HAVE TO EXCLUDE THE FGs.\n    # TODO: check if this causes an error if many duplicate GTs havent been filtered out\n\n    is_cand.view(-1)[fg_rels[:,1]*im_inds.size(0) + fg_rels[:,2]] = 0\n    is_bgcand = is_cand.nonzero()\n    # TODO: make this sample on a per image case\n    # If too many then sample\n    num_fg = min(fg_rels.size(0), int(RELS_PER_IMG * REL_FG_FRACTION * num_im))\n    if num_fg < fg_rels.size(0):\n        fg_rels = random_choose(fg_rels, num_fg)\n\n    # If too many then sample\n    num_bg = min(is_bgcand.size(0) if is_bgcand.dim() > 0 else 0,\n                 int(RELS_PER_IMG * num_im) - num_fg)\n    if num_bg > 0:\n        bg_rels = torch.cat((\n            im_inds[is_bgcand[:, 0]][:, None],\n            is_bgcand,\n            (is_bgcand[:, 0, None] < -10).long(),\n        ), 1)\n\n        if num_bg < is_bgcand.size(0):\n            bg_rels = random_choose(bg_rels, num_bg)\n        rel_labels = torch.cat((fg_rels, bg_rels), 0)\n    else:\n        rel_labels = fg_rels\n\n\n    # last sort by rel.\n    _, perm = torch.sort(rel_labels[:, 0]*(gt_boxes.size(0)**2) +\n                         rel_labels[:,1]*gt_boxes.size(0) + rel_labels[:,2])\n\n    rel_labels = rel_labels[perm].contiguous()\n\n    labels = gt_classes[:,1].contiguous()\n    return rois, labels, rel_labels\n'"
lib/fpn/proposal_assignments/proposal_assignments_postnms.py,3,"b'# --------------------------------------------------------\n# Goal: assign ROIs to targets\n# --------------------------------------------------------\n\n\nimport numpy as np\nimport numpy.random as npr\nfrom .proposal_assignments_rel import _sel_rels\nfrom lib.fpn.box_utils import bbox_overlaps\nfrom lib.pytorch_misc import to_variable\nimport torch\n\n\n@to_variable\ndef proposal_assignments_postnms(\n        rois, gt_boxes, gt_classes, gt_rels, nms_inds, image_offset, fg_thresh=0.5,\n        max_objs=100, max_rels=100, rand_val=0.01):\n    """"""\n    Assign object detection proposals to ground-truth targets. Produces proposal\n    classification labels and bounding-box regression targets.\n    :param rpn_rois: [img_ind, x1, y1, x2, y2]\n    :param gt_boxes:   [num_boxes, 4] array of x0, y0, x1, y1]\n    :param gt_classes: [num_boxes, 2] array of [img_ind, class]\n    :param gt_rels     [num_boxes, 4] array of [img_ind, box_0, box_1, rel type]\n    :param Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n    :return:\n        rois: [num_rois, 5]\n        labels: [num_rois] array of labels\n        rel_labels: [num_rels, 4] (img ind, box0 ind, box1ind, rel type)\n    """"""\n    pred_inds_np = rois[:, 0].cpu().numpy().astype(np.int64)\n    pred_boxes_np = rois[:, 1:].cpu().numpy()\n    nms_inds_np = nms_inds.cpu().numpy()\n    sup_inds_np = np.setdiff1d(np.arange(pred_boxes_np.shape[0]), nms_inds_np)\n\n    # split into chosen and suppressed\n    chosen_inds_np = pred_inds_np[nms_inds_np]\n    chosen_boxes_np = pred_boxes_np[nms_inds_np]\n\n    suppre_inds_np = pred_inds_np[sup_inds_np]\n    suppre_boxes_np = pred_boxes_np[sup_inds_np]\n\n    gt_boxes_np = gt_boxes.cpu().numpy()\n    gt_classes_np = gt_classes.cpu().numpy()\n    gt_rels_np = gt_rels.cpu().numpy()\n\n    gt_classes_np[:, 0] -= image_offset\n    gt_rels_np[:, 0] -= image_offset\n\n    num_im = gt_classes_np[:, 0].max()+1\n\n    rois = []\n    obj_labels = []\n    rel_labels = []\n    num_box_seen = 0\n\n    for im_ind in range(num_im):\n        chosen_ind = np.where(chosen_inds_np == im_ind)[0]\n        suppre_ind = np.where(suppre_inds_np == im_ind)[0]\n\n        gt_ind = np.where(gt_classes_np[:, 0] == im_ind)[0]\n        gt_boxes_i = gt_boxes_np[gt_ind]\n        gt_classes_i = gt_classes_np[gt_ind, 1]\n        gt_rels_i = gt_rels_np[gt_rels_np[:, 0] == im_ind, 1:]\n\n        # Get IOUs between chosen and GT boxes and if needed we\'ll add more in\n\n        chosen_boxes_i = chosen_boxes_np[chosen_ind]\n        suppre_boxes_i = suppre_boxes_np[suppre_ind]\n\n        n_chosen = chosen_boxes_i.shape[0]\n        n_suppre = suppre_boxes_i.shape[0]\n        n_gt_box = gt_boxes_i.shape[0]\n\n        # add a teensy bit of random noise because some GT boxes might be duplicated, etc.\n        pred_boxes_i = np.concatenate((chosen_boxes_i, suppre_boxes_i, gt_boxes_i), 0)\n        ious = bbox_overlaps(pred_boxes_i, gt_boxes_i) + rand_val*(\n            np.random.rand(pred_boxes_i.shape[0], gt_boxes_i.shape[0])-0.5)\n\n        # Let\'s say that a box can only be assigned ONCE for now because we\'ve already done\n        # the NMS and stuff.\n        is_hit = ious > fg_thresh\n\n        obj_assignments_i = is_hit.argmax(1)\n        obj_assignments_i[~is_hit.any(1)] = -1\n\n        vals, first_occurance_ind = np.unique(obj_assignments_i, return_index=True)\n        obj_assignments_i[np.setdiff1d(\n            np.arange(obj_assignments_i.shape[0]), first_occurance_ind)] = -1\n\n        extra_to_add = np.where(obj_assignments_i[n_chosen:] != -1)[0] + n_chosen\n\n        # Add them in somewhere at random\n        num_inds_to_have = min(max_objs, n_chosen + extra_to_add.shape[0])\n        boxes_i = np.zeros((num_inds_to_have, 4), dtype=np.float32)\n        labels_i = np.zeros(num_inds_to_have, dtype=np.int64)\n\n        inds_from_nms = np.sort(np.random.choice(num_inds_to_have, size=n_chosen, replace=False))\n        inds_from_elsewhere = np.setdiff1d(np.arange(num_inds_to_have), inds_from_nms)\n\n        boxes_i[inds_from_nms] = chosen_boxes_i\n        labels_i[inds_from_nms] = gt_classes_i[obj_assignments_i[:n_chosen]]\n\n        boxes_i[inds_from_elsewhere] = pred_boxes_i[extra_to_add]\n        labels_i[inds_from_elsewhere] = gt_classes_i[obj_assignments_i[extra_to_add]]\n\n        # Now, we do the relationships. same as for rle\n        all_rels_i = _sel_rels(bbox_overlaps(boxes_i, gt_boxes_i),\n                               boxes_i,\n                               labels_i,\n                               gt_classes_i,\n                               gt_rels_i,\n                               fg_thresh=fg_thresh,\n                               fg_rels_per_image=100)\n        all_rels_i[:,0:2] += num_box_seen\n\n        rois.append(np.column_stack((\n            im_ind * np.ones(boxes_i.shape[0], dtype=np.float32),\n            boxes_i,\n        )))\n        obj_labels.append(labels_i)\n        rel_labels.append(np.column_stack((\n            im_ind*np.ones(all_rels_i.shape[0], dtype=np.int64),\n            all_rels_i,\n        )))\n        num_box_seen += boxes_i.size\n\n    rois = torch.FloatTensor(np.concatenate(rois, 0)).cuda(gt_boxes.get_device(), async=True)\n    labels = torch.LongTensor(np.concatenate(obj_labels, 0)).cuda(gt_boxes.get_device(), async=True)\n    rel_labels = torch.LongTensor(np.concatenate(rel_labels, 0)).cuda(gt_boxes.get_device(),\n                                                                      async=True)\n\n    return rois, labels, rel_labels\n'"
lib/fpn/proposal_assignments/proposal_assignments_rel.py,4,"b'# --------------------------------------------------------\n# Goal: assign ROIs to targets\n# --------------------------------------------------------\n\n\nimport numpy as np\nimport numpy.random as npr\nfrom config import BG_THRESH_HI, BG_THRESH_LO, FG_FRACTION_REL, ROIS_PER_IMG_REL, REL_FG_FRACTION, \\\n    RELS_PER_IMG\nfrom lib.fpn.box_utils import bbox_overlaps\nfrom lib.pytorch_misc import to_variable, nonintersecting_2d_inds\nfrom collections import defaultdict\nimport torch\n\n\n@to_variable\ndef proposal_assignments_rel(rpn_rois, gt_boxes, gt_classes, gt_rels, image_offset, fg_thresh=0.5):\n    """"""\n    Assign object detection proposals to ground-truth targets. Produces proposal\n    classification labels and bounding-box regression targets.\n    :param rpn_rois: [img_ind, x1, y1, x2, y2]\n    :param gt_boxes:   [num_boxes, 4] array of x0, y0, x1, y1]\n    :param gt_classes: [num_boxes, 2] array of [img_ind, class]\n    :param gt_rels     [num_boxes, 4] array of [img_ind, box_0, box_1, rel type]\n    :param Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n    :return:\n        rois: [num_rois, 5]\n        labels: [num_rois] array of labels\n        bbox_targets [num_rois, 4] array of targets for the labels.\n        rel_labels: [num_rels, 4] (img ind, box0 ind, box1ind, rel type)\n    """"""\n    fg_rois_per_image = int(np.round(ROIS_PER_IMG_REL * FG_FRACTION_REL))\n    fg_rels_per_image = int(np.round(REL_FG_FRACTION * RELS_PER_IMG))\n\n    pred_inds_np = rpn_rois[:, 0].cpu().numpy().astype(np.int64)\n    pred_boxes_np = rpn_rois[:, 1:].cpu().numpy()\n    gt_boxes_np = gt_boxes.cpu().numpy()\n    gt_classes_np = gt_classes.cpu().numpy()\n    gt_rels_np = gt_rels.cpu().numpy()\n\n    gt_classes_np[:, 0] -= image_offset\n    gt_rels_np[:, 0] -= image_offset\n\n    num_im = gt_classes_np[:, 0].max()+1\n\n    rois = []\n    obj_labels = []\n    rel_labels = []\n    bbox_targets = []\n\n    num_box_seen = 0\n\n    for im_ind in range(num_im):\n        pred_ind = np.where(pred_inds_np == im_ind)[0]\n\n        gt_ind = np.where(gt_classes_np[:, 0] == im_ind)[0]\n        gt_boxes_i = gt_boxes_np[gt_ind]\n        gt_classes_i = gt_classes_np[gt_ind, 1]\n        gt_rels_i = gt_rels_np[gt_rels_np[:, 0] == im_ind, 1:]\n\n        pred_boxes_i = np.concatenate((pred_boxes_np[pred_ind], gt_boxes_i), 0)\n        ious = bbox_overlaps(pred_boxes_i, gt_boxes_i)\n \n        obj_inds_i, obj_labels_i, obj_assignments_i = _sel_inds(ious, gt_classes_i, \n            fg_thresh, fg_rois_per_image, ROIS_PER_IMG_REL)\n\n        all_rels_i = _sel_rels(ious[obj_inds_i], pred_boxes_i[obj_inds_i], obj_labels_i,\n                               gt_classes_i, gt_rels_i,\n                               fg_thresh=fg_thresh, fg_rels_per_image=fg_rels_per_image)\n        all_rels_i[:,0:2] += num_box_seen\n\n        rois.append(np.column_stack((\n            im_ind * np.ones(obj_inds_i.shape[0], dtype=np.float32),\n            pred_boxes_i[obj_inds_i],\n        )))\n        obj_labels.append(obj_labels_i)\n        rel_labels.append(np.column_stack((\n            im_ind*np.ones(all_rels_i.shape[0], dtype=np.int64),\n            all_rels_i,\n        )))\n\n        # print(""Gtboxes i {} obj assignments i {}"".format(gt_boxes_i, obj_assignments_i))\n        bbox_targets.append(gt_boxes_i[obj_assignments_i])\n\n        num_box_seen += obj_inds_i.size\n\n    rois = torch.FloatTensor(np.concatenate(rois, 0)).cuda(rpn_rois.get_device(), async=True)\n    labels = torch.LongTensor(np.concatenate(obj_labels, 0)).cuda(rpn_rois.get_device(), async=True)\n    bbox_targets = torch.FloatTensor(np.concatenate(bbox_targets, 0)).cuda(rpn_rois.get_device(),\n                                                                           async=True)\n    rel_labels = torch.LongTensor(np.concatenate(rel_labels, 0)).cuda(rpn_rois.get_device(),\n                                                                      async=True)\n\n    return rois, labels, bbox_targets, rel_labels\n\n\ndef _sel_rels(ious, pred_boxes, pred_labels, gt_classes, gt_rels, fg_thresh=0.5, fg_rels_per_image=128, num_sample_per_gt=1, filter_non_overlap=True):\n    """"""\n    Selects the relations needed\n    :param ious: [num_pred\', num_gt]\n    :param pred_boxes: [num_pred\', num_gt]\n    :param pred_labels: [num_pred\']\n    :param gt_classes: [num_gt]\n    :param gt_rels: [num_gtrel, 3]\n    :param fg_thresh: \n    :param fg_rels_per_image: \n    :return: new rels, [num_predrel, 3] where each is (pred_ind1, pred_ind2, predicate)\n    """"""\n    is_match = (ious >= fg_thresh) & (pred_labels[:, None] == gt_classes[None, :])\n\n    pbi_iou = bbox_overlaps(pred_boxes, pred_boxes)\n\n    # Limit ourselves to only IOUs that overlap, but are not the exact same box\n    # since we duplicated stuff earlier.\n    if filter_non_overlap:\n        rel_possibilities = (pbi_iou < 1) & (pbi_iou > 0)\n        rels_intersect = rel_possibilities\n    else:\n        rel_possibilities = np.ones((pred_labels.shape[0], pred_labels.shape[0]),\n                                    dtype=np.int64) - np.eye(pred_labels.shape[0], dtype=np.int64)\n        rels_intersect = (pbi_iou < 1) & (pbi_iou > 0)\n\n    # ONLY select relations between ground truth because otherwise we get useless data\n    rel_possibilities[pred_labels == 0] = 0\n    rel_possibilities[:,pred_labels == 0] = 0\n\n    # For each GT relationship, sample exactly 1 relationship.\n    fg_rels = []\n    p_size = []\n    for i, (from_gtind, to_gtind, rel_id) in enumerate(gt_rels):\n        fg_rels_i = []\n        fg_scores_i = []\n\n        for from_ind in np.where(is_match[:,from_gtind])[0]:\n            for to_ind in np.where(is_match[:,to_gtind])[0]:\n                if from_ind != to_ind:\n                    fg_rels_i.append((from_ind, to_ind, rel_id))\n                    fg_scores_i.append((ious[from_ind, from_gtind]*ious[to_ind, to_gtind]))\n                    rel_possibilities[from_ind, to_ind] = 0\n        if len(fg_rels_i) == 0:\n            continue\n        p = np.array(fg_scores_i)\n        p = p/p.sum()\n        p_size.append(p.shape[0])\n        num_to_add = min(p.shape[0], num_sample_per_gt)\n        for rel_to_add in npr.choice(p.shape[0], p=p, size=num_to_add, replace=False):\n            fg_rels.append(fg_rels_i[rel_to_add])\n\n    bg_rels = np.column_stack(np.where(rel_possibilities))\n    bg_rels = np.column_stack((bg_rels, np.zeros(bg_rels.shape[0], dtype=np.int64)))\n\n    fg_rels = np.array(fg_rels, dtype=np.int64)\n    if fg_rels.size > 0 and fg_rels.shape[0] > fg_rels_per_image:\n        fg_rels = fg_rels[npr.choice(fg_rels.shape[0], size=fg_rels_per_image, replace=False)]\n        # print(""{} scores for {} GT. max={} min={} BG rels {}"".format(\n        #     fg_rels_scores.shape[0], gt_rels.shape[0], fg_rels_scores.max(), fg_rels_scores.min(),\n        #     bg_rels.shape))\n    elif fg_rels.size == 0:\n        fg_rels = np.zeros((0,3), dtype=np.int64)\n\n    num_bg_rel = min(RELS_PER_IMG - fg_rels.shape[0], bg_rels.shape[0])\n    if bg_rels.size > 0:\n\n        # Sample 4x as many intersecting relationships as non-intersecting.\n        bg_rels_intersect = rels_intersect[bg_rels[:,0], bg_rels[:,1]]\n        p = bg_rels_intersect.astype(np.float32)\n        p[bg_rels_intersect == 0] = 0.2\n        p[bg_rels_intersect == 1] = 0.8\n        p /= p.sum()\n        bg_rels = bg_rels[np.random.choice(bg_rels.shape[0], p=p, size=num_bg_rel, replace=False)]\n    else:\n        bg_rels = np.zeros((0,3), dtype=np.int64)\n\n    #print(""GTR {} -> AR {} vs {}"".format(gt_rels.shape, fg_rels.shape, bg_rels.shape))\n\n    all_rels = np.concatenate((fg_rels, bg_rels), 0)\n\n    # Sort by 2nd ind and then 1st ind\n    all_rels = all_rels[np.lexsort((all_rels[:, 1], all_rels[:, 0]))]\n    return all_rels\n\ndef _sel_inds(ious, gt_classes_i, fg_thresh=0.5, fg_rois_per_image=128, rois_per_image=256, n_sample_per=1):\n\n    #gt_assignment = ious.argmax(1)\n    #max_overlaps = ious[np.arange(ious.shape[0]), gt_assignment]\n    #fg_inds = np.where(max_overlaps >= fg_thresh)[0]\n    \n    fg_ious = ious.T >= fg_thresh #[num_gt, num_pred]\n    #is_bg = ~fg_ious.any(0)\n\n    # Sample K inds per GT image.\n    fg_inds = []\n    for i, (ious_i, cls_i) in enumerate(zip(fg_ious, gt_classes_i)):\n        n_sample_this_roi = min(n_sample_per, ious_i.sum())\n        if n_sample_this_roi > 0:\n            p = ious_i.astype(np.float64) / ious_i.sum()\n            for ind in npr.choice(ious_i.shape[0], p=p, size=n_sample_this_roi, replace=False):\n                fg_inds.append((ind, i))\n    \n    fg_inds = np.array(fg_inds, dtype=np.int64)\n    if fg_inds.size == 0:\n        fg_inds = np.zeros((0, 2), dtype=np.int64)\n    elif fg_inds.shape[0] > fg_rois_per_image:\n        #print(""sample FG"")\n        fg_inds = fg_inds[npr.choice(fg_inds.shape[0], size=fg_rois_per_image, replace=False)]\n    \n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    max_overlaps = ious.max(1)\n    bg_inds = np.where((max_overlaps < BG_THRESH_HI) & (max_overlaps >= BG_THRESH_LO))[0]\n\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = min(rois_per_image-fg_inds.shape[0], bg_inds.size)\n    # Sample background regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(bg_inds, size=bg_rois_per_this_image, replace=False)\n\n\n    # FIx for format issues\n    obj_inds = np.concatenate((fg_inds[:,0], bg_inds), 0)\n    obj_assignments_i = np.concatenate((fg_inds[:,1], np.zeros(bg_inds.shape[0], dtype=np.int64)))\n    obj_labels_i = gt_classes_i[obj_assignments_i]\n    obj_labels_i[fg_inds.shape[0]:] = 0\n    #print(""{} FG and {} BG"".format(fg_inds.shape[0], bg_inds.shape[0]))\n    return obj_inds, obj_labels_i, obj_assignments_i\n\n\n'"
lib/fpn/proposal_assignments/rel_assignments.py,1,"b'# --------------------------------------------------------\n# Goal: assign ROIs to targets\n# --------------------------------------------------------\n\n\nimport numpy as np\nimport numpy.random as npr\nfrom config import BG_THRESH_HI, BG_THRESH_LO, REL_FG_FRACTION, RELS_PER_IMG_REFINE\nfrom lib.fpn.box_utils import bbox_overlaps\nfrom lib.pytorch_misc import to_variable, nonintersecting_2d_inds\nfrom collections import defaultdict\nimport torch\n\n@to_variable\ndef rel_assignments(im_inds, rpn_rois, roi_gtlabels, gt_boxes, gt_classes, gt_rels, image_offset,\n                    fg_thresh=0.5, num_sample_per_gt=4, filter_non_overlap=True):\n    """"""\n    Assign object detection proposals to ground-truth targets. Produces proposal\n    classification labels and bounding-box regression targets.\n    :param rpn_rois: [img_ind, x1, y1, x2, y2]\n    :param gt_boxes:   [num_boxes, 4] array of x0, y0, x1, y1]\n    :param gt_classes: [num_boxes, 2] array of [img_ind, class]\n    :param gt_rels     [num_boxes, 4] array of [img_ind, box_0, box_1, rel type]\n    :param Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n    :return:\n        rois: [num_rois, 5]\n        labels: [num_rois] array of labels\n        bbox_targets [num_rois, 4] array of targets for the labels.\n        rel_labels: [num_rels, 4] (img ind, box0 ind, box1ind, rel type)\n    """"""\n    fg_rels_per_image = int(np.round(REL_FG_FRACTION * 64))\n\n    pred_inds_np = im_inds.cpu().numpy()\n    pred_boxes_np = rpn_rois.cpu().numpy()\n    pred_boxlabels_np = roi_gtlabels.cpu().numpy()\n    gt_boxes_np = gt_boxes.cpu().numpy()\n    gt_classes_np = gt_classes.cpu().numpy()\n    gt_rels_np = gt_rels.cpu().numpy()\n\n    gt_classes_np[:, 0] -= image_offset\n    gt_rels_np[:, 0] -= image_offset\n\n    num_im = gt_classes_np[:, 0].max()+1\n\n    # print(""Pred inds {} pred boxes {} pred box labels {} gt classes {} gt rels {}"".format(\n    #     pred_inds_np, pred_boxes_np, pred_boxlabels_np, gt_classes_np, gt_rels_np\n    # ))\n\n    rel_labels = []\n    num_box_seen = 0\n    for im_ind in range(num_im):\n        pred_ind = np.where(pred_inds_np == im_ind)[0]\n\n        gt_ind = np.where(gt_classes_np[:, 0] == im_ind)[0]\n        gt_boxes_i = gt_boxes_np[gt_ind]\n        gt_classes_i = gt_classes_np[gt_ind, 1]\n        gt_rels_i = gt_rels_np[gt_rels_np[:, 0] == im_ind, 1:]\n\n        # [num_pred, num_gt]\n        pred_boxes_i = pred_boxes_np[pred_ind]\n        pred_boxlabels_i = pred_boxlabels_np[pred_ind]\n\n        ious = bbox_overlaps(pred_boxes_i, gt_boxes_i)\n        is_match = (pred_boxlabels_i[:,None] == gt_classes_i[None]) & (ious >= fg_thresh)\n\n        # FOR BG. Limit ourselves to only IOUs that overlap, but are not the exact same box\n        pbi_iou = bbox_overlaps(pred_boxes_i, pred_boxes_i)\n        if filter_non_overlap:\n            rel_possibilities = (pbi_iou < 1) & (pbi_iou > 0)\n            rels_intersect = rel_possibilities\n        else:\n            rel_possibilities = np.ones((pred_boxes_i.shape[0], pred_boxes_i.shape[0]),\n                                        dtype=np.int64) - np.eye(pred_boxes_i.shape[0],\n                                                                 dtype=np.int64)\n            rels_intersect = (pbi_iou < 1) & (pbi_iou > 0)\n\n        # ONLY select relations between ground truth because otherwise we get useless data\n        rel_possibilities[pred_boxlabels_i == 0] = 0\n        rel_possibilities[:, pred_boxlabels_i == 0] = 0\n\n        # Sample the GT relationships.\n        fg_rels = []\n        p_size = []\n        for i, (from_gtind, to_gtind, rel_id) in enumerate(gt_rels_i):\n            fg_rels_i = []\n            fg_scores_i = []\n\n            for from_ind in np.where(is_match[:, from_gtind])[0]:\n                for to_ind in np.where(is_match[:, to_gtind])[0]:\n                    if from_ind != to_ind:\n                        fg_rels_i.append((from_ind, to_ind, rel_id))\n                        fg_scores_i.append((ious[from_ind, from_gtind] * ious[to_ind, to_gtind]))\n                        rel_possibilities[from_ind, to_ind] = 0\n            if len(fg_rels_i) == 0:\n                continue\n            p = np.array(fg_scores_i)\n            p = p / p.sum()\n            p_size.append(p.shape[0])\n            num_to_add = min(p.shape[0], num_sample_per_gt)\n            for rel_to_add in npr.choice(p.shape[0], p=p, size=num_to_add, replace=False):\n                fg_rels.append(fg_rels_i[rel_to_add])\n\n        fg_rels = np.array(fg_rels, dtype=np.int64)\n        if fg_rels.size > 0 and fg_rels.shape[0] > fg_rels_per_image:\n            fg_rels = fg_rels[npr.choice(fg_rels.shape[0], size=fg_rels_per_image, replace=False)]\n        elif fg_rels.size == 0:\n            fg_rels = np.zeros((0, 3), dtype=np.int64)\n\n        bg_rels = np.column_stack(np.where(rel_possibilities))\n        bg_rels = np.column_stack((bg_rels, np.zeros(bg_rels.shape[0], dtype=np.int64)))\n\n        num_bg_rel = min(64 - fg_rels.shape[0], bg_rels.shape[0])\n        if bg_rels.size > 0:\n            # Sample 4x as many intersecting relationships as non-intersecting.\n            # bg_rels_intersect = rels_intersect[bg_rels[:, 0], bg_rels[:, 1]]\n            # p = bg_rels_intersect.astype(np.float32)\n            # p[bg_rels_intersect == 0] = 0.2\n            # p[bg_rels_intersect == 1] = 0.8\n            # p /= p.sum()\n            bg_rels = bg_rels[\n                np.random.choice(bg_rels.shape[0],\n                                 #p=p,\n                                 size=num_bg_rel, replace=False)]\n        else:\n            bg_rels = np.zeros((0, 3), dtype=np.int64)\n\n        if fg_rels.size == 0 and bg_rels.size == 0:\n            # Just put something here\n            bg_rels = np.array([[0, 0, 0]], dtype=np.int64)\n\n        # print(""GTR {} -> AR {} vs {}"".format(gt_rels.shape, fg_rels.shape, bg_rels.shape))\n        all_rels_i = np.concatenate((fg_rels, bg_rels), 0)\n        all_rels_i[:,0:2] += num_box_seen\n\n        all_rels_i = all_rels_i[np.lexsort((all_rels_i[:,1], all_rels_i[:,0]))]\n\n        rel_labels.append(np.column_stack((\n            im_ind*np.ones(all_rels_i.shape[0], dtype=np.int64),\n            all_rels_i,\n        )))\n\n        num_box_seen += pred_boxes_i.shape[0]\n    rel_labels = torch.LongTensor(np.concatenate(rel_labels, 0)).cuda(rpn_rois.get_device(),\n                                                                      async=True)\n    return rel_labels\n'"
lib/fpn/roi_align/__init__.py,0,b''
lib/fpn/roi_align/build.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n# Might have to export PATH=/usr/local/cuda-8.0/bin${PATH:+:${PATH}}\n\n# sources = ['src/roi_align.c']\n# headers = ['src/roi_align.h']\nsources = []\nheaders = []\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/roi_align_cuda.c']\n    headers += ['src/roi_align_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/cuda/roi_align.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.roi_align',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
lib/lstm/highway_lstm_cuda/__init__.py,0,b''
lib/lstm/highway_lstm_cuda/alternating_highway_lstm.py,20,"b'from typing import Tuple\n\nfrom overrides import overrides\nimport torch\nfrom torch.autograd import Function, Variable\nfrom torch.nn import Parameter\nfrom torch.nn.utils.rnn import PackedSequence, pad_packed_sequence, pack_padded_sequence\nimport itertools\nfrom ._ext import highway_lstm_layer\n\n\ndef block_orthogonal(tensor, split_sizes, gain=1.0):\n    """"""\n    An initializer which allows initializing model parameters in ""blocks"". This is helpful\n    in the case of recurrent models which use multiple gates applied to linear projections,\n    which can be computed efficiently if they are concatenated together. However, they are\n    separate parameters which should be initialized independently.\n    Parameters\n    ----------\n    tensor : ``torch.Tensor``, required.\n        A tensor to initialize.\n    split_sizes : List[int], required.\n        A list of length ``tensor.ndim()`` specifying the size of the\n        blocks along that particular dimension. E.g. ``[10, 20]`` would\n        result in the tensor being split into chunks of size 10 along the\n        first dimension and 20 along the second.\n    gain : float, optional (default = 1.0)\n        The gain (scaling) applied to the orthogonal initialization.\n    """"""\n\n    if isinstance(tensor, Variable):\n        block_orthogonal(tensor.data, split_sizes, gain)\n        return tensor\n\n    sizes = list(tensor.size())\n    if any([a % b != 0 for a, b in zip(sizes, split_sizes)]):\n        raise ValueError(""tensor dimensions must be divisible by their respective ""\n                         ""split_sizes. Found size: {} and split_sizes: {}"".format(sizes, split_sizes))\n    indexes = [list(range(0, max_size, split))\n               for max_size, split in zip(sizes, split_sizes)]\n    # Iterate over all possible blocks within the tensor.\n    for block_start_indices in itertools.product(*indexes):\n        # A list of tuples containing the index to start at for this block\n        # and the appropriate step size (i.e split_size[i] for dimension i).\n        index_and_step_tuples = zip(block_start_indices, split_sizes)\n        # This is a tuple of slices corresponding to:\n        # tensor[index: index + step_size, ...]. This is\n        # required because we could have an arbitrary number\n        # of dimensions. The actual slices we need are the\n        # start_index: start_index + step for each dimension in the tensor.\n        block_slice = tuple([slice(start_index, start_index + step)\n                             for start_index, step in index_and_step_tuples])\n\n        # let\'s not initialize empty things to 0s because THAT SOUNDS REALLY BAD\n        assert len(block_slice) == 2\n        sizes = [x.stop - x.start for x in block_slice]\n        tensor_copy = tensor.new(max(sizes), max(sizes))\n        torch.nn.init.orthogonal(tensor_copy, gain=gain)\n        tensor[block_slice] = tensor_copy[0:sizes[0], 0:sizes[1]]\n\n\nclass _AlternatingHighwayLSTMFunction(Function):\n    def __init__(self, input_size: int, hidden_size: int, num_layers: int, train: bool) -> None:\n        super(_AlternatingHighwayLSTMFunction, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.train = train\n\n    @overrides\n    def forward(self,  # pylint: disable=arguments-differ\n                inputs: torch.Tensor,\n                weight: torch.Tensor,\n                bias: torch.Tensor,\n                state_accumulator: torch.Tensor,\n                memory_accumulator: torch.Tensor,\n                dropout_mask: torch.Tensor,\n                lengths: torch.Tensor,\n                gates: torch.Tensor) -> Tuple[torch.Tensor, None]:\n        sequence_length, batch_size, input_size = inputs.size()\n        tmp_i = inputs.new(batch_size, 6 * self.hidden_size)\n        tmp_h = inputs.new(batch_size, 5 * self.hidden_size)\n        is_training = 1 if self.train else 0\n        highway_lstm_layer.highway_lstm_forward_cuda(input_size,  # type: ignore # pylint: disable=no-member\n                                                     self.hidden_size,\n                                                     batch_size,\n                                                     self.num_layers,\n                                                     sequence_length,\n                                                     inputs,\n                                                     lengths,\n                                                     state_accumulator,\n                                                     memory_accumulator,\n                                                     tmp_i,\n                                                     tmp_h,\n                                                     weight,\n                                                     bias,\n                                                     dropout_mask,\n                                                     gates,\n                                                     is_training)\n\n        self.save_for_backward(inputs, lengths, weight, bias, state_accumulator,\n                               memory_accumulator, dropout_mask, gates)\n\n        # The state_accumulator has shape: (num_layers, sequence_length + 1, batch_size, hidden_size)\n        # so for the output, we want the last layer and all but the first timestep, which was the\n        # initial state.\n        output = state_accumulator[-1, 1:, :, :]\n        return output, state_accumulator[:, 1:, :, :]\n\n    @overrides\n    def backward(self, grad_output, grad_hy):  # pylint: disable=arguments-differ\n\n        (inputs, lengths, weight, bias, state_accumulator,  # pylint: disable=unpacking-non-sequence\n         memory_accumulator, dropout_mask, gates) = self.saved_tensors\n\n        inputs = inputs.contiguous()\n        sequence_length, batch_size, input_size = inputs.size()\n        parameters_need_grad = 1 if self.needs_input_grad[1] else 0  # pylint: disable=unsubscriptable-object\n\n        grad_input = inputs.new().resize_as_(inputs).zero_()\n        grad_state_accumulator = inputs.new().resize_as_(state_accumulator).zero_()\n        grad_memory_accumulator = inputs.new().resize_as_(memory_accumulator).zero_()\n        grad_weight = inputs.new()\n        grad_bias = inputs.new()\n        grad_dropout = None\n        grad_lengths = None\n        grad_gates = None\n\n        if parameters_need_grad:\n            grad_weight.resize_as_(weight).zero_()\n            grad_bias.resize_as_(bias).zero_()\n\n        tmp_i_gates_grad = inputs.new().resize_(batch_size, 6 * self.hidden_size).zero_()\n        tmp_h_gates_grad = inputs.new().resize_(batch_size, 5 * self.hidden_size).zero_()\n\n        is_training = 1 if self.train else 0\n        highway_lstm_layer.highway_lstm_backward_cuda(input_size,  # pylint: disable=no-member\n                                                      self.hidden_size,\n                                                      batch_size,\n                                                      self.num_layers,\n                                                      sequence_length,\n                                                      grad_output,\n                                                      lengths,\n                                                      grad_state_accumulator,\n                                                      grad_memory_accumulator,\n                                                      inputs,\n                                                      state_accumulator,\n                                                      memory_accumulator,\n                                                      weight,\n                                                      gates,\n                                                      dropout_mask,\n                                                      tmp_h_gates_grad,\n                                                      tmp_i_gates_grad,\n                                                      grad_hy,\n                                                      grad_input,\n                                                      grad_weight,\n                                                      grad_bias,\n                                                      is_training,\n                                                      parameters_need_grad)\n\n        return (grad_input, grad_weight, grad_bias, grad_state_accumulator,\n                grad_memory_accumulator, grad_dropout, grad_lengths, grad_gates)\n\n\nclass AlternatingHighwayLSTM(torch.nn.Module):\n    """"""\n    A stacked LSTM with LSTM layers which alternate between going forwards over\n    the sequence and going backwards, with highway connections between each of\n    the alternating layers. This implementation is based on the description in\n    `Deep Semantic Role Labelling - What works and what\'s next\n    <https://homes.cs.washington.edu/~luheng/files/acl2017_hllz.pdf>`_ .\n\n    Parameters\n    ----------\n    input_size : int, required\n        The dimension of the inputs to the LSTM.\n    hidden_size : int, required\n        The dimension of the outputs of the LSTM.\n    num_layers : int, required\n        The number of stacked LSTMs to use.\n    recurrent_dropout_probability: float, optional (default = 0.0)\n        The dropout probability to be used in a dropout scheme as stated in\n        `A Theoretically Grounded Application of Dropout in Recurrent Neural Networks\n        <https://arxiv.org/abs/1512.05287>`_ .\n\n    Returns\n    -------\n    output : PackedSequence\n        The outputs of the interleaved LSTMs per timestep. A tensor of shape\n        (batch_size, max_timesteps, hidden_size) where for a given batch\n        element, all outputs past the sequence length for that batch are\n        zero tensors.\n    """"""\n\n    def __init__(self,\n                 input_size: int,\n                 hidden_size: int,\n                 num_layers: int = 1,\n                 recurrent_dropout_probability: float = 0) -> None:\n        super(AlternatingHighwayLSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.recurrent_dropout_probability = recurrent_dropout_probability\n        self.training = True\n\n        # Input dimensions consider the fact that we do\n        # all of the LSTM projections (and highway parts)\n        # in a single matrix multiplication.\n        input_projection_size = 6 * hidden_size\n        state_projection_size = 5 * hidden_size\n        bias_size = 5 * hidden_size\n\n        # Here we are creating a single weight and bias with the\n        # parameters for all layers unfolded into it. This is necessary\n        # because unpacking and re-packing the weights inside the\n        # kernel would be slow, as it would happen every time it is called.\n        total_weight_size = 0\n        total_bias_size = 0\n        for layer in range(num_layers):\n            layer_input_size = input_size if layer == 0 else hidden_size\n\n            input_weights = input_projection_size * layer_input_size\n            state_weights = state_projection_size * hidden_size\n            total_weight_size += input_weights + state_weights\n\n            total_bias_size += bias_size\n\n        self.weight = Parameter(torch.FloatTensor(total_weight_size))\n        self.bias = Parameter(torch.FloatTensor(total_bias_size))\n        self.reset_parameters()\n\n    def reset_parameters(self) -> None:\n        self.bias.data.zero_()\n        weight_index = 0\n        bias_index = 0\n        for i in range(self.num_layers):\n            input_size = self.input_size if i == 0 else self.hidden_size\n\n            # Create a tensor of the right size and initialize it.\n            init_tensor = self.weight.data.new(input_size, self.hidden_size * 6).zero_()\n            block_orthogonal(init_tensor, [input_size, self.hidden_size])\n            # Copy it into the flat weight.\n            self.weight.data[weight_index: weight_index + init_tensor.nelement()] \\\n                .view_as(init_tensor).copy_(init_tensor)\n            weight_index += init_tensor.nelement()\n\n            # Same for the recurrent connection weight.\n            init_tensor = self.weight.data.new(self.hidden_size, self.hidden_size * 5).zero_()\n            block_orthogonal(init_tensor, [self.hidden_size, self.hidden_size])\n            self.weight.data[weight_index: weight_index + init_tensor.nelement()] \\\n                .view_as(init_tensor).copy_(init_tensor)\n            weight_index += init_tensor.nelement()\n\n            # Set the forget bias to 1.\n            self.bias.data[bias_index + self.hidden_size:bias_index + 2 * self.hidden_size].fill_(1)\n            bias_index += 5 * self.hidden_size\n\n    def forward(self, inputs, initial_state=None) -> Tuple[PackedSequence, torch.Tensor]:\n        """"""\n        Parameters\n        ----------\n        inputs : ``PackedSequence``, required.\n            A batch first ``PackedSequence`` to run the stacked LSTM over.\n        initial_state : Tuple[torch.Tensor, torch.Tensor], optional, (default = None)\n            Currently, this is ignored.\n\n        Returns\n        -------\n        output_sequence : ``PackedSequence``\n            The encoded sequence of shape (batch_size, sequence_length, hidden_size)\n        final_states: ``torch.Tensor``\n            The per-layer final (state, memory) states of the LSTM, each with shape\n            (num_layers, batch_size, hidden_size).\n        """"""\n        inputs, lengths = pad_packed_sequence(inputs, batch_first=False)\n\n        sequence_length, batch_size, _ = inputs.size()\n        accumulator_shape = [self.num_layers, sequence_length + 1, batch_size, self.hidden_size]\n        state_accumulator = Variable(inputs.data.new(*accumulator_shape).zero_(), requires_grad=False)\n        memory_accumulator = Variable(inputs.data.new(*accumulator_shape).zero_(), requires_grad=False)\n\n        dropout_weights = inputs.data.new().resize_(self.num_layers, batch_size, self.hidden_size).fill_(1.0)\n        if self.training:\n            # Normalize by 1 - dropout_prob to preserve the output statistics of the layer.\n            dropout_weights.bernoulli_(1 - self.recurrent_dropout_probability) \\\n                .div_((1 - self.recurrent_dropout_probability))\n\n        dropout_weights = Variable(dropout_weights, requires_grad=False)\n        gates = Variable(inputs.data.new().resize_(self.num_layers,\n                                                   sequence_length,\n                                                   batch_size, 6 * self.hidden_size))\n\n        lengths_variable = Variable(torch.IntTensor(lengths))\n        implementation = _AlternatingHighwayLSTMFunction(self.input_size,\n                                                         self.hidden_size,\n                                                         num_layers=self.num_layers,\n                                                         train=self.training)\n        output, _ = implementation(inputs, self.weight, self.bias, state_accumulator,\n                                   memory_accumulator, dropout_weights, lengths_variable, gates)\n\n        output = pack_padded_sequence(output, lengths, batch_first=False)\n        return output, None\n'"
lib/lstm/highway_lstm_cuda/build.py,2,"b""# pylint: disable=invalid-name\nimport os\nimport torch\nfrom torch.utils.ffi import create_extension\n\nif not torch.cuda.is_available():\n    raise Exception('HighwayLSTM can only be compiled with CUDA')\n\nsources = ['src/highway_lstm_cuda.c']\nheaders = ['src/highway_lstm_cuda.h']\ndefines = [('WITH_CUDA', None)]\nwith_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nextra_objects = ['src/highway_lstm_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n        '_ext.highway_lstm_layer',\n        headers=headers,\n        sources=sources,\n        define_macros=defines,\n        relative_to=__file__,\n        with_cuda=with_cuda,\n        extra_objects=extra_objects\n        )\n\nif __name__ == '__main__':\n    ffi.build()\n"""
lib/fpn/nms/functions/nms.py,3,"b'# Le code for doing NMS\nimport torch\nimport numpy as np\nfrom .._ext import nms\n\n\ndef apply_nms(scores, boxes,  pre_nms_topn=12000, post_nms_topn=2000, boxes_per_im=None,\n              nms_thresh=0.7):\n    """"""\n    Note - this function is non-differentiable so everything is assumed to be a tensor, not\n    a variable.\n        """"""\n    just_inds = boxes_per_im is None\n    if boxes_per_im is None:\n        boxes_per_im = [boxes.size(0)]\n\n\n    s = 0\n    keep = []\n    im_per = []\n    for bpi in boxes_per_im:\n        e = s + int(bpi)\n        keep_im = _nms_single_im(scores[s:e], boxes[s:e], pre_nms_topn, post_nms_topn, nms_thresh)\n        keep.append(keep_im + s)\n        im_per.append(keep_im.size(0))\n\n        s = e\n\n    inds = torch.cat(keep, 0)\n    if just_inds:\n        return inds\n    return inds, im_per\n\n\ndef _nms_single_im(scores, boxes,  pre_nms_topn=12000, post_nms_topn=2000, nms_thresh=0.7):\n    keep = torch.IntTensor(scores.size(0))\n    vs, idx = torch.sort(scores, dim=0, descending=True)\n    if idx.size(0) > pre_nms_topn:\n        idx = idx[:pre_nms_topn]\n    boxes_sorted = boxes[idx].contiguous()\n    num_out = nms.nms_apply(keep, boxes_sorted, nms_thresh)\n    num_out = min(num_out, post_nms_topn)\n    keep = keep[:num_out].long()\n    keep = idx[keep.cuda(scores.get_device())]\n    return keep\n'"
lib/fpn/roi_align/_ext/__init__.py,0,b''
lib/fpn/roi_align/functions/__init__.py,0,b''
lib/fpn/roi_align/functions/roi_align.py,1,"b'""""""\nperforms ROI aligning\n""""""\n\nimport torch\nfrom torch.autograd import Function\nfrom .._ext import roi_align\n\nclass RoIAlignFunction(Function):\n    def __init__(self, aligned_height, aligned_width, spatial_scale):\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n\n        self.feature_size = None\n\n    def forward(self, features, rois):\n        self.save_for_backward(rois)\n\n        rois_normalized = rois.clone()\n\n        self.feature_size = features.size()\n        batch_size, num_channels, data_height, data_width = self.feature_size\n\n        height = (data_height -1) / self.spatial_scale\n        width = (data_width - 1) / self.spatial_scale\n\n        rois_normalized[:,1] /= width\n        rois_normalized[:,2] /= height\n        rois_normalized[:,3] /= width\n        rois_normalized[:,4] /= height\n\n\n        num_rois = rois.size(0)\n\n        output = features.new(num_rois, num_channels, self.aligned_height,\n            self.aligned_width).zero_()\n\n        if features.is_cuda:\n            res = roi_align.roi_align_forward_cuda(self.aligned_height,\n                                             self.aligned_width,\n                                             self.spatial_scale, features,\n                                             rois_normalized, output)\n            assert res == 1\n        else:\n            raise ValueError\n\n        return output\n\n    def backward(self, grad_output):\n        assert(self.feature_size is not None and grad_output.is_cuda)\n\n        rois = self.saved_tensors[0]\n\n        rois_normalized = rois.clone()\n\n        batch_size, num_channels, data_height, data_width = self.feature_size\n\n        height = (data_height -1) / self.spatial_scale\n        width = (data_width - 1) / self.spatial_scale\n\n        rois_normalized[:,1] /= width\n        rois_normalized[:,2] /= height\n        rois_normalized[:,3] /= width\n        rois_normalized[:,4] /= height\n\n        grad_input = rois_normalized.new(batch_size, num_channels, data_height,\n                                  data_width).zero_()\n        res = roi_align.roi_align_backward_cuda(self.aligned_height,\n                                          self.aligned_width,\n                                          self.spatial_scale, grad_output,\n                                          rois_normalized, grad_input)\n        assert res == 1\n        return grad_input, None\n'"
lib/fpn/roi_align/modules/__init__.py,0,b''
lib/fpn/roi_align/modules/roi_align.py,2,"b'from torch.nn.modules.module import Module\nfrom torch.nn.functional import avg_pool2d, max_pool2d\nfrom ..functions.roi_align import RoIAlignFunction\n\n\nclass RoIAlign(Module):\n    def __init__(self, aligned_height, aligned_width, spatial_scale):\n        super(RoIAlign, self).__init__()\n\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        return RoIAlignFunction(self.aligned_height, self.aligned_width,\n                                self.spatial_scale)(features, rois)\n\nclass RoIAlignAvg(Module):\n    def __init__(self, aligned_height, aligned_width, spatial_scale):\n        super(RoIAlignAvg, self).__init__()\n\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        x =  RoIAlignFunction(self.aligned_height+1, self.aligned_width+1,\n                                self.spatial_scale)(features, rois)\n        return avg_pool2d(x, kernel_size=2, stride=1)\n\nclass RoIAlignMax(Module):\n    def __init__(self, aligned_height, aligned_width, spatial_scale):\n        super(RoIAlignMax, self).__init__()\n\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        x =  RoIAlignFunction(self.aligned_height+1, self.aligned_width+1,\n                                self.spatial_scale)(features, rois)\n        return max_pool2d(x, kernel_size=2, stride=1)\n'"
lib/lstm/highway_lstm_cuda/_ext/__init__.py,0,b''
lib/fpn/roi_align/_ext/roi_align/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._roi_align import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        locals[symbol] = _wrap_function(fn, _ffi)\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
lib/lstm/highway_lstm_cuda/_ext/highway_lstm_layer/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._highway_lstm_layer import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        locals[symbol] = _wrap_function(fn, _ffi)\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
