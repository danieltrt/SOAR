file_path,api_count,code
lib/setup.py,0,"b""# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom __future__ import print_function\n\nfrom Cython.Build import cythonize\nfrom Cython.Distutils import build_ext\nfrom setuptools import Extension\nfrom setuptools import setup\n\nimport numpy as np\n\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\next_modules = [\n    Extension(\n        name='utils.cython_bbox',\n        sources=['utils/cython_bbox.pyx'],\n        extra_compile_args=['-Wno-cpp'],\n        include_dirs=[numpy_include]\n    ),\n    Extension(\n        name='utils.cython_nms',\n        sources=['utils/cython_nms.pyx'],\n        extra_compile_args=['-Wno-cpp'],\n        include_dirs=[numpy_include]\n    )\n]\n\nsetup(\n    name='mask_rcnn',\n    ext_modules=cythonize(ext_modules)\n)\n\n"""
tools/_init_paths.py,0,"b'""""""Add {PROJECT_ROOT}/lib. to PYTHONPATH\n\nUsage:\nimport this module before import any modules under lib/\ne.g \n    import _init_paths\n    from core.config import cfg\n"""""" \n\nimport os.path as osp\nimport sys\n\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\nthis_dir = osp.abspath(osp.dirname(osp.dirname(__file__)))\n\n# Add lib to PYTHONPATH\nlib_path = osp.join(this_dir, \'lib\')\nadd_path(lib_path)\n'"
tools/download_imagenet_weights.py,0,"b'""""""Script to downlaod ImageNet pretrained weights from Google Drive\n\nExtra packages required to run the script:\n    colorama, argparse_color_formatter\n""""""\n\nimport argparse\nimport os\nimport requests\nfrom argparse_color_formatter import ColorHelpFormatter\nfrom colorama import init, Fore\n\nimport _init_paths  # pylint: disable=unused-import\nfrom core.config import cfg\n\n\ndef parse_args():\n    """"""Parser command line argumnets""""""\n    parser = argparse.ArgumentParser(formatter_class=ColorHelpFormatter)\n    parser.add_argument(\'--output_dir\', help=\'Directory to save downloaded weight files\',\n                        default=os.path.join(cfg.DATA_DIR, \'pretrained_model\'))\n    parser.add_argument(\'-t\', \'--targets\', nargs=\'+\', metavar=\'file_name\',\n                        help=\'Files to download. Allowed values are: \' +\n                        \', \'.join(map(lambda s: Fore.YELLOW + s + Fore.RESET,\n                                      list(PRETRAINED_WEIGHTS.keys()))),\n                        choices=list(PRETRAINED_WEIGHTS.keys()),\n                        default=list(PRETRAINED_WEIGHTS.keys()))\n    return parser.parse_args()\n\n\n# ---------------------------------------------------------------------------- #\n# Mapping from filename to google drive file_id\n# ---------------------------------------------------------------------------- #\nPRETRAINED_WEIGHTS = {\n    \'resnet50_caffe.pth\': \'1wHSvusQ1CiEMc5Nx5R8adqoHQjIDWXl1\',\n    \'resnet101_caffe.pth\': \'1x2fTMqLrn63EMW0VuK4GEa2eQKzvJ_7l\',\n    \'resnet152_caffe.pth\': \'1NSCycOb7pU0KzluH326zmyMFUU55JslF\',\n    \'vgg16_caffe.pth\': \'19UphT53C0Ua9JAtICnw84PPTa3sZZ_9k\',\n}\n\n\n# ---------------------------------------------------------------------------- #\n# Helper fucntions for download file from google drive\n# ---------------------------------------------------------------------------- #\n\ndef download_file_from_google_drive(id, destination):\n    URL = ""https://docs.google.com/uc?export=download""\n\n    session = requests.Session()\n\n    response = session.get(URL, params={\'id\': id}, stream=True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = {\'id\': id, \'confirm\': token}\n        response = session.get(URL, params=params, stream=True)\n\n    save_response_content(response, destination)\n\n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith(\'download_warning\'):\n            return value\n\n    return None\n\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, ""wb"") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n\n\ndef main():\n    init()  # colorama init. Only has effect on Windows\n    args = parse_args()\n    for filename in args.targets:\n        file_id = PRETRAINED_WEIGHTS[filename]\n        if not os.path.exists(args.output_dir):\n            os.makedirs(args.output_dir)\n        destination = os.path.join(args.output_dir, filename)\n        download_file_from_google_drive(file_id, destination)\n        print(\'Download {} to {}\'.format(filename, destination))\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tools/infer_simple.py,4,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport distutils.util\nimport os\nimport sys\nimport pprint\nimport subprocess\nfrom collections import defaultdict\nfrom six.moves import xrange\n\n# Use a non-interactive backend\nimport matplotlib\nmatplotlib.use(\'Agg\')\n\nimport numpy as np\nimport cv2\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nimport _init_paths\nimport nn as mynn\nfrom core.config import cfg, cfg_from_file, cfg_from_list, assert_and_infer_cfg\nfrom core.test import im_detect_all\nfrom modeling.model_builder import Generalized_RCNN\nimport datasets.dummy_datasets as datasets\nimport utils.misc as misc_utils\nimport utils.net as net_utils\nimport utils.vis as vis_utils\nfrom utils.detectron_weight_helper import load_detectron_weight\nfrom utils.timer import Timer\n\n# OpenCL may be enabled by default in OpenCV3; disable it because it\'s not\n# thread safe and causes unwanted GPU memory allocations.\ncv2.ocl.setUseOpenCL(False)\n\n\ndef parse_args():\n    """"""Parse in command line arguments""""""\n    parser = argparse.ArgumentParser(description=\'Demonstrate mask-rcnn results\')\n    parser.add_argument(\n        \'--dataset\', required=True,\n        help=\'training dataset\')\n\n    parser.add_argument(\n        \'--cfg\', dest=\'cfg_file\', required=True,\n        help=\'optional config file\')\n    parser.add_argument(\n        \'--set\', dest=\'set_cfgs\',\n        help=\'set config keys, will overwrite config in the cfg_file\',\n        default=[], nargs=\'+\')\n\n    parser.add_argument(\n        \'--no_cuda\', dest=\'cuda\', help=\'whether use CUDA\', action=\'store_false\')\n\n    parser.add_argument(\'--load_ckpt\', help=\'path of checkpoint to load\')\n    parser.add_argument(\n        \'--load_detectron\', help=\'path to the detectron weight pickle file\')\n\n    parser.add_argument(\n        \'--image_dir\',\n        help=\'directory to load images for demo\')\n    parser.add_argument(\n        \'--images\', nargs=\'+\',\n        help=\'images to infer. Must not use with --image_dir\')\n    parser.add_argument(\n        \'--output_dir\',\n        help=\'directory to save demo results\',\n        default=""infer_outputs"")\n    parser.add_argument(\n        \'--merge_pdfs\', type=distutils.util.strtobool, default=True)\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef main():\n    """"""main function""""""\n\n    if not torch.cuda.is_available():\n        sys.exit(""Need a CUDA device to run the code."")\n\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n\n    assert args.image_dir or args.images\n    assert bool(args.image_dir) ^ bool(args.images)\n\n    if args.dataset.startswith(""coco""):\n        dataset = datasets.get_coco_dataset()\n        cfg.MODEL.NUM_CLASSES = len(dataset.classes)\n    elif args.dataset.startswith(""keypoints_coco""):\n        dataset = datasets.get_coco_dataset()\n        cfg.MODEL.NUM_CLASSES = 2\n    else:\n        raise ValueError(\'Unexpected dataset name: {}\'.format(args.dataset))\n\n    print(\'load cfg from file: {}\'.format(args.cfg_file))\n    cfg_from_file(args.cfg_file)\n\n    if args.set_cfgs is not None:\n        cfg_from_list(args.set_cfgs)\n\n    assert bool(args.load_ckpt) ^ bool(args.load_detectron), \\\n        \'Exactly one of --load_ckpt and --load_detectron should be specified.\'\n    cfg.MODEL.LOAD_IMAGENET_PRETRAINED_WEIGHTS = False  # Don\'t need to load imagenet pretrained weights\n    assert_and_infer_cfg()\n\n    maskRCNN = Generalized_RCNN()\n\n    if args.cuda:\n        maskRCNN.cuda()\n\n    if args.load_ckpt:\n        load_name = args.load_ckpt\n        print(""loading checkpoint %s"" % (load_name))\n        checkpoint = torch.load(load_name, map_location=lambda storage, loc: storage)\n        net_utils.load_ckpt(maskRCNN, checkpoint[\'model\'])\n\n    if args.load_detectron:\n        print(""loading detectron weights %s"" % args.load_detectron)\n        load_detectron_weight(maskRCNN, args.load_detectron)\n\n    maskRCNN = mynn.DataParallel(maskRCNN, cpu_keywords=[\'im_info\', \'roidb\'],\n                                 minibatch=True, device_ids=[0])  # only support single GPU\n\n    maskRCNN.eval()\n    if args.image_dir:\n        imglist = misc_utils.get_imagelist_from_dir(args.image_dir)\n    else:\n        imglist = args.images\n    num_images = len(imglist)\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n\n    for i in xrange(num_images):\n        print(\'img\', i)\n        im = cv2.imread(imglist[i])\n        assert im is not None\n\n        timers = defaultdict(Timer)\n\n        cls_boxes, cls_segms, cls_keyps = im_detect_all(maskRCNN, im, timers=timers)\n\n        im_name, _ = os.path.splitext(os.path.basename(imglist[i]))\n        vis_utils.vis_one_image(\n            im[:, :, ::-1],  # BGR -> RGB for visualization\n            im_name,\n            args.output_dir,\n            cls_boxes,\n            cls_segms,\n            cls_keyps,\n            dataset=dataset,\n            box_alpha=0.3,\n            show_class=True,\n            thresh=0.7,\n            kp_thresh=2\n        )\n\n    if args.merge_pdfs and num_images > 1:\n        merge_out_path = \'{}/results.pdf\'.format(args.output_dir)\n        if os.path.exists(merge_out_path):\n            os.remove(merge_out_path)\n        command = ""pdfunite {}/*.pdf {}"".format(args.output_dir,\n                                                merge_out_path)\n        subprocess.call(command, shell=True)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/test_net.py,2,"b'""""""Perform inference on one or more datasets.""""""\n\nimport argparse\nimport cv2\nimport os\nimport pprint\nimport sys\nimport time\n\nimport torch\n\nimport _init_paths  # pylint: disable=unused-import\nfrom core.config import cfg, merge_cfg_from_file, merge_cfg_from_list, assert_and_infer_cfg\nfrom core.test_engine import run_inference\nimport utils.logging\n\n# OpenCL may be enabled by default in OpenCV3; disable it because it\'s not\n# thread safe and causes unwanted GPU memory allocations.\ncv2.ocl.setUseOpenCL(False)\n\n\ndef parse_args():\n    """"""Parse in command line arguments""""""\n    parser = argparse.ArgumentParser(description=\'Test a Fast R-CNN network\')\n    parser.add_argument(\n        \'--dataset\',\n        help=\'training dataset\')\n    parser.add_argument(\n        \'--cfg\', dest=\'cfg_file\', required=True,\n        help=\'optional config file\')\n\n    parser.add_argument(\n        \'--load_ckpt\', help=\'path of checkpoint to load\')\n    parser.add_argument(\n        \'--load_detectron\', help=\'path to the detectron weight pickle file\')\n\n    parser.add_argument(\n        \'--output_dir\',\n        help=\'output directory to save the testing results. If not provided, \'\n             \'defaults to [args.load_ckpt|args.load_detectron]/../test.\')\n\n    parser.add_argument(\n        \'--set\', dest=\'set_cfgs\',\n        help=\'set config keys, will overwrite config in the cfg_file.\'\n             \' See lib/core/config.py for all options\',\n        default=[], nargs=\'*\')\n\n    parser.add_argument(\n        \'--range\',\n        help=\'start (inclusive) and end (exclusive) indices\',\n        type=int, nargs=2)\n    parser.add_argument(\n        \'--multi-gpu-testing\', help=\'using multiple gpus for inference\',\n        action=\'store_true\')\n    parser.add_argument(\n        \'--vis\', dest=\'vis\', help=\'visualize detections\', action=\'store_true\')\n\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n\n    if not torch.cuda.is_available():\n        sys.exit(""Need a CUDA device to run the code."")\n\n    logger = utils.logging.setup_logging(__name__)\n    args = parse_args()\n    logger.info(\'Called with args:\')\n    logger.info(args)\n\n    assert (torch.cuda.device_count() == 1) ^ bool(args.multi_gpu_testing)\n\n    assert bool(args.load_ckpt) ^ bool(args.load_detectron), \\\n        \'Exactly one of --load_ckpt and --load_detectron should be specified.\'\n    if args.output_dir is None:\n        ckpt_path = args.load_ckpt if args.load_ckpt else args.load_detectron\n        args.output_dir = os.path.join(\n            os.path.dirname(os.path.dirname(ckpt_path)), \'test\')\n        logger.info(\'Automatically set output directory to %s\', args.output_dir)\n    if not os.path.exists(args.output_dir):\n        os.makedirs(args.output_dir)\n\n    cfg.VIS = args.vis\n\n    if args.cfg_file is not None:\n        merge_cfg_from_file(args.cfg_file)\n    if args.set_cfgs is not None:\n        merge_cfg_from_list(args.set_cfgs)\n\n    if args.dataset == ""coco2017"":\n        cfg.TEST.DATASETS = (\'coco_2017_val\',)\n        cfg.MODEL.NUM_CLASSES = 81\n    elif args.dataset == ""keypoints_coco2017"":\n        cfg.TEST.DATASETS = (\'keypoints_coco_2017_val\',)\n        cfg.MODEL.NUM_CLASSES = 2\n    else:  # For subprocess call\n        assert cfg.TEST.DATASETS, \'cfg.TEST.DATASETS shouldn\\\'t be empty\'\n    assert_and_infer_cfg()\n\n    logger.info(\'Testing with config:\')\n    logger.info(pprint.pformat(cfg))\n\n    # For test_engine.multi_gpu_test_net_on_dataset\n    args.test_net_file, _ = os.path.splitext(__file__)\n    # manually set args.cuda\n    args.cuda = True\n\n    run_inference(\n        args,\n        ind_range=args.range,\n        multi_gpu_testing=args.multi_gpu_testing,\n        check_expected_results=True)\n'"
tools/train_net.py,9,"b'"""""" Training Script """"""\n\nimport argparse\nimport distutils.util\nimport os\nimport sys\nimport pickle\nimport resource\nimport traceback\nimport logging\nfrom collections import defaultdict\n\nimport numpy as np\nimport yaml\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport cv2\ncv2.setNumThreads(0)  # pytorch issue 1355: possible deadlock in dataloader\n\nimport _init_paths  # pylint: disable=unused-import\nimport nn as mynn\nimport utils.net as net_utils\nimport utils.misc as misc_utils\nfrom core.config import cfg, cfg_from_file, cfg_from_list, assert_and_infer_cfg\nfrom datasets.roidb import combined_roidb_for_training\nfrom modeling.model_builder import Generalized_RCNN\nfrom roi_data.loader import RoiDataLoader, MinibatchSampler, collate_minibatch\nfrom utils.detectron_weight_helper import load_detectron_weight\nfrom utils.logging import log_stats\nfrom utils.timer import Timer\nfrom utils.training_stats import TrainingStats\n\n# OpenCL may be enabled by default in OpenCV3; disable it because it\'s not\n# thread safe and causes unwanted GPU memory allocations.\ncv2.ocl.setUseOpenCL(False)\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# RuntimeError: received 0 items of ancdata. Issue: pytorch/pytorch#973\nrlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\nresource.setrlimit(resource.RLIMIT_NOFILE, (4096, rlimit[1]))\n\n\ndef parse_args():\n    """"""Parse input arguments""""""\n    parser = argparse.ArgumentParser(description=\'Train a X-RCNN network\')\n\n    parser.add_argument(\n        \'--dataset\', dest=\'dataset\', required=True,\n        help=\'Dataset to use\')\n    parser.add_argument(\n        \'--cfg\', dest=\'cfg_file\', required=True,\n        help=\'Config file for training (and optionally testing)\')\n    parser.add_argument(\n        \'--set\', dest=\'set_cfgs\',\n        help=\'Set config keys. Key value sequence seperate by whitespace.\'\n             \'e.g. [key] [value] [key] [value]\',\n        default=[], nargs=\'+\')\n\n    parser.add_argument(\n        \'--disp_interval\',\n        help=\'Display training info every N iterations\',\n        default=100, type=int)\n    parser.add_argument(\n        \'--no_cuda\', dest=\'cuda\', help=\'Do not use CUDA device\', action=\'store_false\')\n\n    # Optimization\n    # These options has the highest prioity and can overwrite the values in config file\n    # or values set by set_cfgs. `None` means do not overwrite.\n    parser.add_argument(\n        \'--bs\', dest=\'batch_size\',\n        help=\'Explicitly specify to overwrite the value comed from cfg_file.\',\n        type=int)\n    parser.add_argument(\n        \'--nw\', dest=\'num_workers\',\n        help=\'Explicitly specify to overwrite number of workers to load data. Defaults to 4\',\n        type=int)\n\n    parser.add_argument(\n        \'--o\', dest=\'optimizer\', help=\'Training optimizer.\',\n        default=None)\n    parser.add_argument(\n        \'--lr\', help=\'Base learning rate.\',\n        default=None, type=float)\n    parser.add_argument(\n        \'--lr_decay_gamma\',\n        help=\'Learning rate decay rate.\',\n        default=None, type=float)\n    parser.add_argument(\n        \'--lr_decay_epochs\',\n        help=\'Epochs to decay the learning rate on. \'\n             \'Decay happens on the beginning of a epoch. \'\n             \'Epoch is 0-indexed.\',\n        default=[4, 5], nargs=\'+\', type=int)\n\n    # Epoch\n    parser.add_argument(\n        \'--start_iter\',\n        help=\'Starting iteration for first training epoch. 0-indexed.\',\n        default=0, type=int)\n    parser.add_argument(\n        \'--start_epoch\',\n        help=\'Starting epoch count. Epoch is 0-indexed.\',\n        default=0, type=int)\n    parser.add_argument(\n        \'--epochs\', dest=\'num_epochs\',\n        help=\'Number of epochs to train\',\n        default=6, type=int)\n\n    # Resume training: requires same iterations per epoch\n    parser.add_argument(\n        \'--resume\',\n        help=\'resume to training on a checkpoint\',\n        action=\'store_true\')\n\n    parser.add_argument(\n        \'--no_save\', help=\'do not save anything\', action=\'store_true\')\n\n    parser.add_argument(\n        \'--ckpt_num_per_epoch\',\n        help=\'number of checkpoints to save in each epoch. \'\n             \'Not include the one at the end of an epoch.\',\n        default=3, type=int)\n\n    parser.add_argument(\n        \'--load_ckpt\', help=\'checkpoint path to load\')\n    parser.add_argument(\n        \'--load_detectron\', help=\'path to the detectron weight pickle file\')\n\n    parser.add_argument(\n        \'--use_tfboard\', help=\'Use tensorflow tensorboard to log training info\',\n        action=\'store_true\')\n\n    return parser.parse_args()\n\n\ndef main():\n    """"""Main function""""""\n\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n\n    if not torch.cuda.is_available():\n        sys.exit(""Need a CUDA device to run the code."")\n\n    if args.cuda or cfg.NUM_GPUS > 0:\n        cfg.CUDA = True\n    else:\n        raise ValueError(""Need Cuda device to run !"")\n\n    if args.dataset == ""coco2017"":\n        cfg.TRAIN.DATASETS = (\'coco_2017_train\',)\n        cfg.MODEL.NUM_CLASSES = 81\n    elif args.dataset == ""keypoints_coco2017"":\n        cfg.TRAIN.DATASETS = (\'keypoints_coco_2017_train\',)\n        cfg.MODEL.NUM_CLASSES = 2\n    else:\n        raise ValueError(""Unexpected args.dataset: {}"".format(args.dataset))\n\n    cfg_from_file(args.cfg_file)\n    if args.set_cfgs is not None:\n        cfg_from_list(args.set_cfgs)\n\n    ### Adaptively adjust some configs ###\n    original_batch_size = cfg.NUM_GPUS * cfg.TRAIN.IMS_PER_BATCH\n    if args.batch_size is None:\n        args.batch_size = original_batch_size\n    cfg.NUM_GPUS = torch.cuda.device_count()\n    assert (args.batch_size % cfg.NUM_GPUS) == 0, \\\n        \'batch_size: %d, NUM_GPUS: %d\' % (args.batch_size, cfg.NUM_GPUS)\n    cfg.TRAIN.IMS_PER_BATCH = args.batch_size // cfg.NUM_GPUS\n    print(\'Batch size change from {} (in config file) to {}\'.format(\n        original_batch_size, args.batch_size))\n    print(\'NUM_GPUs: %d, TRAIN.IMS_PER_BATCH: %d\' % (cfg.NUM_GPUS, cfg.TRAIN.IMS_PER_BATCH))\n\n    if args.num_workers is not None:\n        cfg.DATA_LOADER.NUM_THREADS = args.num_workers\n    print(\'Number of data loading threads: %d\' % cfg.DATA_LOADER.NUM_THREADS)\n\n    ### Adjust learning based on batch size change linearly\n    old_base_lr = cfg.SOLVER.BASE_LR\n    cfg.SOLVER.BASE_LR *= args.batch_size / original_batch_size\n    print(\'Adjust BASE_LR linearly according to batch size change: {} --> {}\'.format(\n        old_base_lr, cfg.SOLVER.BASE_LR))\n\n    ### Overwrite some solver settings from command line arguments\n    if args.optimizer is not None:\n        cfg.SOLVER.TYPE = args.optimizer\n    if args.lr is not None:\n        cfg.SOLVER.BASE_LR = args.lr\n    if args.lr_decay_gamma is not None:\n        cfg.SOLVER.GAMMA = args.lr_decay_gamma\n\n    timers = defaultdict(Timer)\n\n    ### Dataset ###\n    timers[\'roidb\'].tic()\n    roidb, ratio_list, ratio_index = combined_roidb_for_training(\n        cfg.TRAIN.DATASETS, cfg.TRAIN.PROPOSAL_FILES)\n    timers[\'roidb\'].toc()\n    train_size = len(roidb)\n    logger.info(\'{:d} roidb entries\'.format(train_size))\n    logger.info(\'Takes %.2f sec(s) to construct roidb\', timers[\'roidb\'].average_time)\n\n    sampler = MinibatchSampler(ratio_list, ratio_index)\n    dataset = RoiDataLoader(\n        roidb,\n        cfg.MODEL.NUM_CLASSES,\n        training=True)\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=args.batch_size,\n        sampler=sampler,\n        num_workers=cfg.DATA_LOADER.NUM_THREADS,\n        collate_fn=collate_minibatch)\n\n    assert_and_infer_cfg()\n\n    ### Model ###\n    maskRCNN = Generalized_RCNN()\n\n    if cfg.CUDA:\n        maskRCNN.cuda()\n\n    ### Optimizer ###\n    bias_params = []\n    nonbias_params = []\n    for key, value in dict(maskRCNN.named_parameters()).items():\n        if value.requires_grad:\n            if \'bias\' in key:\n                bias_params.append(value)\n            else:\n                nonbias_params.append(value)\n    params = [\n        {\'params\': nonbias_params,\n         \'lr\': cfg.SOLVER.BASE_LR,\n         \'weight_decay\': cfg.SOLVER.WEIGHT_DECAY},\n        {\'params\': bias_params,\n         \'lr\': cfg.SOLVER.BASE_LR * (cfg.SOLVER.BIAS_DOUBLE_LR + 1),\n         \'weight_decay\': cfg.SOLVER.WEIGHT_DECAY if cfg.SOLVER.BIAS_WEIGHT_DECAY else 0}\n    ]\n\n    if cfg.SOLVER.TYPE == ""SGD"":\n        optimizer = torch.optim.SGD(params, momentum=cfg.SOLVER.MOMENTUM)\n    elif cfg.SOLVER.TYPE == ""Adam"":\n        optimizer = torch.optim.Adam(params)\n\n    ### Load checkpoint\n    if args.load_ckpt:\n        load_name = args.load_ckpt\n        logging.info(""loading checkpoint %s"", load_name)\n        checkpoint = torch.load(load_name, map_location=lambda storage, loc: storage)\n        net_utils.load_ckpt(maskRCNN, checkpoint[\'model\'])\n        if args.resume:\n            assert checkpoint[\'iters_per_epoch\'] == train_size // args.batch_size, \\\n                ""iters_per_epoch should match for resume""\n            # There is a bug in optimizer.load_state_dict on Pytorch 0.3.1.\n            # However it\'s fixed on master.\n            # optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            misc_utils.load_optimizer_state_dict(optimizer, checkpoint[\'optimizer\'])\n            if checkpoint[\'step\'] == (checkpoint[\'iters_per_epoch\'] - 1):\n                # Resume from end of an epoch\n                args.start_epoch = checkpoint[\'epoch\'] + 1\n                args.start_iter = 0\n            else:\n                # Resume from the middle of an epoch.\n                # NOTE: dataloader is not synced with previous state\n                args.start_epoch = checkpoint[\'epoch\']\n                args.start_iter = checkpoint[\'step\'] + 1\n        del checkpoint\n        torch.cuda.empty_cache()\n\n    if args.load_detectron:  #TODO resume for detectron weights (load sgd momentum values)\n        logging.info(""loading Detectron weights %s"", args.load_detectron)\n        load_detectron_weight(maskRCNN, args.load_detectron)\n\n    lr = optimizer.param_groups[0][\'lr\']  # lr of non-bias parameters, for commmand line outputs.\n\n    maskRCNN = mynn.DataParallel(maskRCNN, cpu_keywords=[\'im_info\', \'roidb\'],\n                                 minibatch=True)\n\n    ### Training Setups ###\n    args.run_name = misc_utils.get_run_name()\n    output_dir = misc_utils.get_output_dir(args, args.run_name)\n    args.cfg_filename = os.path.basename(args.cfg_file)\n\n    if not args.no_save:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        blob = {\'cfg\': yaml.dump(cfg), \'args\': args}\n        with open(os.path.join(output_dir, \'config_and_args.pkl\'), \'wb\') as f:\n            pickle.dump(blob, f, pickle.HIGHEST_PROTOCOL)\n\n        if args.use_tfboard:\n            from tensorboardX import SummaryWriter\n            # Set the Tensorboard logger\n            tblogger = SummaryWriter(output_dir)\n\n    ### Training Loop ###\n    maskRCNN.train()\n\n    training_stats = TrainingStats(\n        args,\n        args.disp_interval,\n        tblogger if args.use_tfboard and not args.no_save else None)\n\n    iters_per_epoch = int(train_size / args.batch_size)  # drop last\n    args.iters_per_epoch = iters_per_epoch\n    ckpt_interval_per_epoch = iters_per_epoch // args.ckpt_num_per_epoch\n    try:\n        logger.info(\'Training starts !\')\n        args.step = args.start_iter\n        global_step = iters_per_epoch * args.start_epoch + args.step\n        for args.epoch in range(args.start_epoch, args.start_epoch + args.num_epochs):\n            # ---- Start of epoch ----\n\n            # adjust learning rate\n            if args.lr_decay_epochs and args.epoch == args.lr_decay_epochs[0] and args.start_iter == 0:\n                args.lr_decay_epochs.pop(0)\n                net_utils.decay_learning_rate(optimizer, lr, cfg.SOLVER.GAMMA)\n                lr *= cfg.SOLVER.GAMMA\n\n            for args.step, input_data in zip(range(args.start_iter, iters_per_epoch), dataloader):\n\n                for key in input_data:\n                    if key != \'roidb\': # roidb is a list of ndarrays with inconsistent length\n                        input_data[key] = list(map(Variable, input_data[key]))\n\n                training_stats.IterTic()\n                net_outputs = maskRCNN(**input_data)\n                training_stats.UpdateIterStats(net_outputs)\n                loss = net_outputs[\'total_loss\']\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                training_stats.IterToc()\n\n                if (args.step+1) % ckpt_interval_per_epoch == 0:\n                    net_utils.save_ckpt(output_dir, args, maskRCNN, optimizer)\n\n                if args.step % args.disp_interval == 0:\n                    log_training_stats(training_stats, global_step, lr)\n\n                global_step += 1\n\n            # ---- End of epoch ----\n            # save checkpoint\n            net_utils.save_ckpt(output_dir, args, maskRCNN, optimizer)\n            # reset starting iter number after first epoch\n            args.start_iter = 0\n\n        # ---- Training ends ----\n        if iters_per_epoch % args.disp_interval != 0:\n            # log last stats at the end\n            log_training_stats(training_stats, global_step, lr)\n\n    except (RuntimeError, KeyboardInterrupt):\n        logger.info(\'Save ckpt on exception ...\')\n        net_utils.save_ckpt(output_dir, args, maskRCNN, optimizer)\n        logger.info(\'Save ckpt done.\')\n        stack_trace = traceback.format_exc()\n        print(stack_trace)\n\n    finally:\n        if args.use_tfboard and not args.no_save:\n            tblogger.close()\n\n\ndef log_training_stats(training_stats, global_step, lr):\n    stats = training_stats.GetStats(global_step, lr)\n    log_stats(stats, training_stats.misc_args)\n    if training_stats.tblogger:\n        training_stats.tb_log_stats(stats, global_step)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/train_net_step.py,10,"b'"""""" Training script for steps_with_decay policy""""""\n\nimport argparse\nimport os\nimport sys\nimport pickle\nimport resource\nimport traceback\nimport logging\nfrom collections import defaultdict\n\nimport numpy as np\nimport yaml\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport cv2\ncv2.setNumThreads(0)  # pytorch issue 1355: possible deadlock in dataloader\n\nimport _init_paths  # pylint: disable=unused-import\nimport nn as mynn\nimport utils.net as net_utils\nimport utils.misc as misc_utils\nfrom core.config import cfg, cfg_from_file, cfg_from_list, assert_and_infer_cfg\nfrom datasets.roidb import combined_roidb_for_training\nfrom roi_data.loader import RoiDataLoader, MinibatchSampler, BatchSampler, collate_minibatch\nfrom modeling.model_builder import Generalized_RCNN\nfrom utils.detectron_weight_helper import load_detectron_weight\nfrom utils.logging import setup_logging\nfrom utils.timer import Timer\nfrom utils.training_stats import TrainingStats\n\n# Set up logging and load config options\nlogger = setup_logging(__name__)\nlogging.getLogger(\'roi_data.loader\').setLevel(logging.INFO)\n\n# RuntimeError: received 0 items of ancdata. Issue: pytorch/pytorch#973\nrlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\nresource.setrlimit(resource.RLIMIT_NOFILE, (4096, rlimit[1]))\n\ndef parse_args():\n    """"""Parse input arguments""""""\n    parser = argparse.ArgumentParser(description=\'Train a X-RCNN network\')\n\n    parser.add_argument(\n        \'--dataset\', dest=\'dataset\', required=True,\n        help=\'Dataset to use\')\n    parser.add_argument(\n        \'--cfg\', dest=\'cfg_file\', required=True,\n        help=\'Config file for training (and optionally testing)\')\n    parser.add_argument(\n        \'--set\', dest=\'set_cfgs\',\n        help=\'Set config keys. Key value sequence seperate by whitespace.\'\n             \'e.g. [key] [value] [key] [value]\',\n        default=[], nargs=\'+\')\n\n    parser.add_argument(\n        \'--disp_interval\',\n        help=\'Display training info every N iterations\',\n        default=20, type=int)\n    parser.add_argument(\n        \'--no_cuda\', dest=\'cuda\', help=\'Do not use CUDA device\', action=\'store_false\')\n\n    # Optimization\n    # These options has the highest prioity and can overwrite the values in config file\n    # or values set by set_cfgs. `None` means do not overwrite.\n    parser.add_argument(\n        \'--bs\', dest=\'batch_size\',\n        help=\'Explicitly specify to overwrite the value comed from cfg_file.\',\n        type=int)\n    parser.add_argument(\n        \'--nw\', dest=\'num_workers\',\n        help=\'Explicitly specify to overwrite number of workers to load data. Defaults to 4\',\n        type=int)\n    parser.add_argument(\n        \'--iter_size\',\n        help=\'Update once every iter_size steps, as in Caffe.\',\n        default=1, type=int)\n\n    parser.add_argument(\n        \'--o\', dest=\'optimizer\', help=\'Training optimizer.\',\n        default=None)\n    parser.add_argument(\n        \'--lr\', help=\'Base learning rate.\',\n        default=None, type=float)\n    parser.add_argument(\n        \'--lr_decay_gamma\',\n        help=\'Learning rate decay rate.\',\n        default=None, type=float)\n\n    # Epoch\n    parser.add_argument(\n        \'--start_step\',\n        help=\'Starting step count for training epoch. 0-indexed.\',\n        default=0, type=int)\n\n    # Resume training: requires same iterations per epoch\n    parser.add_argument(\n        \'--resume\',\n        help=\'resume to training on a checkpoint\',\n        action=\'store_true\')\n\n    parser.add_argument(\n        \'--no_save\', help=\'do not save anything\', action=\'store_true\')\n\n    parser.add_argument(\n        \'--load_ckpt\', help=\'checkpoint path to load\')\n    parser.add_argument(\n        \'--load_detectron\', help=\'path to the detectron weight pickle file\')\n\n    parser.add_argument(\n        \'--use_tfboard\', help=\'Use tensorflow tensorboard to log training info\',\n        action=\'store_true\')\n\n    return parser.parse_args()\n\n\ndef save_ckpt(output_dir, args, step, train_size, model, optimizer):\n    """"""Save checkpoint""""""\n    if args.no_save:\n        return\n    ckpt_dir = os.path.join(output_dir, \'ckpt\')\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    save_name = os.path.join(ckpt_dir, \'model_step{}.pth\'.format(step))\n    if isinstance(model, mynn.DataParallel):\n        model = model.module\n    model_state_dict = model.state_dict()\n    torch.save({\n        \'step\': step,\n        \'train_size\': train_size,\n        \'batch_size\': args.batch_size,\n        \'model\': model.state_dict(),\n        \'optimizer\': optimizer.state_dict()}, save_name)\n    logger.info(\'save model: %s\', save_name)\n\n\ndef main():\n    """"""Main function""""""\n\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n\n    if not torch.cuda.is_available():\n        sys.exit(""Need a CUDA device to run the code."")\n\n    if args.cuda or cfg.NUM_GPUS > 0:\n        cfg.CUDA = True\n    else:\n        raise ValueError(""Need Cuda device to run !"")\n\n    if args.dataset == ""coco2017"":\n        cfg.TRAIN.DATASETS = (\'coco_2017_train\',)\n        cfg.MODEL.NUM_CLASSES = 81\n    elif args.dataset == ""keypoints_coco2017"":\n        cfg.TRAIN.DATASETS = (\'keypoints_coco_2017_train\',)\n        cfg.MODEL.NUM_CLASSES = 2\n    else:\n        raise ValueError(""Unexpected args.dataset: {}"".format(args.dataset))\n\n    cfg_from_file(args.cfg_file)\n    if args.set_cfgs is not None:\n        cfg_from_list(args.set_cfgs)\n\n    ### Adaptively adjust some configs ###\n    original_batch_size = cfg.NUM_GPUS * cfg.TRAIN.IMS_PER_BATCH\n    original_ims_per_batch = cfg.TRAIN.IMS_PER_BATCH\n    original_num_gpus = cfg.NUM_GPUS\n    if args.batch_size is None:\n        args.batch_size = original_batch_size\n    cfg.NUM_GPUS = torch.cuda.device_count()\n    assert (args.batch_size % cfg.NUM_GPUS) == 0, \\\n        \'batch_size: %d, NUM_GPUS: %d\' % (args.batch_size, cfg.NUM_GPUS)\n    cfg.TRAIN.IMS_PER_BATCH = args.batch_size // cfg.NUM_GPUS\n    effective_batch_size = args.iter_size * args.batch_size\n    print(\'effective_batch_size = batch_size * iter_size = %d * %d\' % (args.batch_size, args.iter_size))\n\n    print(\'Adaptive config changes:\')\n    print(\'    effective_batch_size: %d --> %d\' % (original_batch_size, effective_batch_size))\n    print(\'    NUM_GPUS:             %d --> %d\' % (original_num_gpus, cfg.NUM_GPUS))\n    print(\'    IMS_PER_BATCH:        %d --> %d\' % (original_ims_per_batch, cfg.TRAIN.IMS_PER_BATCH))\n\n    ### Adjust learning based on batch size change linearly\n    # For iter_size > 1, gradients are `accumulated`, so lr is scaled based\n    # on batch_size instead of effective_batch_size\n    old_base_lr = cfg.SOLVER.BASE_LR\n    cfg.SOLVER.BASE_LR *= args.batch_size / original_batch_size\n    print(\'Adjust BASE_LR linearly according to batch_size change:\\n\'\n          \'    BASE_LR: {} --> {}\'.format(old_base_lr, cfg.SOLVER.BASE_LR))\n\n    ### Adjust solver steps\n    step_scale = original_batch_size / effective_batch_size\n    old_solver_steps = cfg.SOLVER.STEPS\n    old_max_iter = cfg.SOLVER.MAX_ITER\n    cfg.SOLVER.STEPS = list(map(lambda x: int(x * step_scale + 0.5), cfg.SOLVER.STEPS))\n    cfg.SOLVER.MAX_ITER = int(cfg.SOLVER.MAX_ITER * step_scale + 0.5)\n    print(\'Adjust SOLVER.STEPS and SOLVER.MAX_ITER linearly based on effective_batch_size change:\\n\'\n          \'    SOLVER.STEPS: {} --> {}\\n\'\n          \'    SOLVER.MAX_ITER: {} --> {}\'.format(old_solver_steps, cfg.SOLVER.STEPS,\n                                                  old_max_iter, cfg.SOLVER.MAX_ITER))\n\n    # Scale FPN rpn_proposals collect size (post_nms_topN) in `collect` function\n    # of `collect_and_distribute_fpn_rpn_proposals.py`\n    #\n    # post_nms_topN = int(cfg[cfg_key].RPN_POST_NMS_TOP_N * cfg.FPN.RPN_COLLECT_SCALE + 0.5)\n    if cfg.FPN.FPN_ON and cfg.MODEL.FASTER_RCNN:\n        cfg.FPN.RPN_COLLECT_SCALE = cfg.TRAIN.IMS_PER_BATCH / original_ims_per_batch\n        print(\'Scale FPN rpn_proposals collect size directly propotional to the change of IMS_PER_BATCH:\\n\'\n              \'    cfg.FPN.RPN_COLLECT_SCALE: {}\'.format(cfg.FPN.RPN_COLLECT_SCALE))\n\n    if args.num_workers is not None:\n        cfg.DATA_LOADER.NUM_THREADS = args.num_workers\n    print(\'Number of data loading threads: %d\' % cfg.DATA_LOADER.NUM_THREADS)\n\n    ### Overwrite some solver settings from command line arguments\n    if args.optimizer is not None:\n        cfg.SOLVER.TYPE = args.optimizer\n    if args.lr is not None:\n        cfg.SOLVER.BASE_LR = args.lr\n    if args.lr_decay_gamma is not None:\n        cfg.SOLVER.GAMMA = args.lr_decay_gamma\n    assert_and_infer_cfg()\n\n    timers = defaultdict(Timer)\n\n    ### Dataset ###\n    timers[\'roidb\'].tic()\n    roidb, ratio_list, ratio_index = combined_roidb_for_training(\n        cfg.TRAIN.DATASETS, cfg.TRAIN.PROPOSAL_FILES)\n    timers[\'roidb\'].toc()\n    roidb_size = len(roidb)\n    logger.info(\'{:d} roidb entries\'.format(roidb_size))\n    logger.info(\'Takes %.2f sec(s) to construct roidb\', timers[\'roidb\'].average_time)\n\n    # Effective training sample size for one epoch\n    train_size = roidb_size // args.batch_size * args.batch_size\n\n    batchSampler = BatchSampler(\n        sampler=MinibatchSampler(ratio_list, ratio_index),\n        batch_size=args.batch_size,\n        drop_last=True\n    )\n    dataset = RoiDataLoader(\n        roidb,\n        cfg.MODEL.NUM_CLASSES,\n        training=True)\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_sampler=batchSampler,\n        num_workers=cfg.DATA_LOADER.NUM_THREADS,\n        collate_fn=collate_minibatch)\n    dataiterator = iter(dataloader)\n\n    ### Model ###\n    maskRCNN = Generalized_RCNN()\n\n    if cfg.CUDA:\n        maskRCNN.cuda()\n\n    ### Optimizer ###\n    gn_param_nameset = set()\n    for name, module in maskRCNN.named_modules():\n        if isinstance(module, nn.GroupNorm):\n            gn_param_nameset.add(name+\'.weight\')\n            gn_param_nameset.add(name+\'.bias\')\n    gn_params = []\n    gn_param_names = []\n    bias_params = []\n    bias_param_names = []\n    nonbias_params = []\n    nonbias_param_names = []\n    nograd_param_names = []\n    for key, value in maskRCNN.named_parameters():\n        if value.requires_grad:\n            if \'bias\' in key:\n                bias_params.append(value)\n                bias_param_names.append(key)\n            elif key in gn_param_nameset:\n                gn_params.append(value)\n                gn_param_names.append(key)\n            else:\n                nonbias_params.append(value)\n                nonbias_param_names.append(key)\n        else:\n            nograd_param_names.append(key)\n    assert (gn_param_nameset - set(nograd_param_names) - set(bias_param_names)) == set(gn_param_names)\n\n    # Learning rate of 0 is a dummy value to be set properly at the start of training\n    params = [\n        {\'params\': nonbias_params,\n         \'lr\': 0,\n         \'weight_decay\': cfg.SOLVER.WEIGHT_DECAY},\n        {\'params\': bias_params,\n         \'lr\': 0 * (cfg.SOLVER.BIAS_DOUBLE_LR + 1),\n         \'weight_decay\': cfg.SOLVER.WEIGHT_DECAY if cfg.SOLVER.BIAS_WEIGHT_DECAY else 0},\n        {\'params\': gn_params,\n         \'lr\': 0,\n         \'weight_decay\': cfg.SOLVER.WEIGHT_DECAY_GN}\n    ]\n    # names of paramerters for each paramter\n    param_names = [nonbias_param_names, bias_param_names, gn_param_names]\n\n    if cfg.SOLVER.TYPE == ""SGD"":\n        optimizer = torch.optim.SGD(params, momentum=cfg.SOLVER.MOMENTUM)\n    elif cfg.SOLVER.TYPE == ""Adam"":\n        optimizer = torch.optim.Adam(params)\n\n    ### Load checkpoint\n    if args.load_ckpt:\n        load_name = args.load_ckpt\n        logging.info(""loading checkpoint %s"", load_name)\n        checkpoint = torch.load(load_name, map_location=lambda storage, loc: storage)\n        net_utils.load_ckpt(maskRCNN, checkpoint[\'model\'])\n        if args.resume:\n            args.start_step = checkpoint[\'step\'] + 1\n            if \'train_size\' in checkpoint:  # For backward compatibility\n                if checkpoint[\'train_size\'] != train_size:\n                    print(\'train_size value: %d different from the one in checkpoint: %d\'\n                          % (train_size, checkpoint[\'train_size\']))\n\n            # reorder the params in optimizer checkpoint\'s params_groups if needed\n            # misc_utils.ensure_optimizer_ckpt_params_order(param_names, checkpoint)\n\n            # There is a bug in optimizer.load_state_dict on Pytorch 0.3.1.\n            # However it\'s fixed on master.\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            # misc_utils.load_optimizer_state_dict(optimizer, checkpoint[\'optimizer\'])\n        del checkpoint\n        torch.cuda.empty_cache()\n\n    if args.load_detectron:  #TODO resume for detectron weights (load sgd momentum values)\n        logging.info(""loading Detectron weights %s"", args.load_detectron)\n        load_detectron_weight(maskRCNN, args.load_detectron)\n\n    lr = optimizer.param_groups[0][\'lr\']  # lr of non-bias parameters, for commmand line outputs.\n\n    maskRCNN = mynn.DataParallel(maskRCNN, cpu_keywords=[\'im_info\', \'roidb\'],\n                                 minibatch=True)\n\n    ### Training Setups ###\n    args.run_name = misc_utils.get_run_name() + \'_step\'\n    output_dir = misc_utils.get_output_dir(args, args.run_name)\n    args.cfg_filename = os.path.basename(args.cfg_file)\n\n    if not args.no_save:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n\n        blob = {\'cfg\': yaml.dump(cfg), \'args\': args}\n        with open(os.path.join(output_dir, \'config_and_args.pkl\'), \'wb\') as f:\n            pickle.dump(blob, f, pickle.HIGHEST_PROTOCOL)\n\n        if args.use_tfboard:\n            from tensorboardX import SummaryWriter\n            # Set the Tensorboard logger\n            tblogger = SummaryWriter(output_dir)\n\n    ### Training Loop ###\n    maskRCNN.train()\n\n    CHECKPOINT_PERIOD = int(cfg.TRAIN.SNAPSHOT_ITERS / cfg.NUM_GPUS)\n\n    # Set index for decay steps\n    decay_steps_ind = None\n    for i in range(1, len(cfg.SOLVER.STEPS)):\n        if cfg.SOLVER.STEPS[i] >= args.start_step:\n            decay_steps_ind = i\n            break\n    if decay_steps_ind is None:\n        decay_steps_ind = len(cfg.SOLVER.STEPS)\n\n    training_stats = TrainingStats(\n        args,\n        args.disp_interval,\n        tblogger if args.use_tfboard and not args.no_save else None)\n    try:\n        logger.info(\'Training starts !\')\n        step = args.start_step\n        for step in range(args.start_step, cfg.SOLVER.MAX_ITER):\n\n            # Warm up\n            if step < cfg.SOLVER.WARM_UP_ITERS:\n                method = cfg.SOLVER.WARM_UP_METHOD\n                if method == \'constant\':\n                    warmup_factor = cfg.SOLVER.WARM_UP_FACTOR\n                elif method == \'linear\':\n                    alpha = step / cfg.SOLVER.WARM_UP_ITERS\n                    warmup_factor = cfg.SOLVER.WARM_UP_FACTOR * (1 - alpha) + alpha\n                else:\n                    raise KeyError(\'Unknown SOLVER.WARM_UP_METHOD: {}\'.format(method))\n                lr_new = cfg.SOLVER.BASE_LR * warmup_factor\n                net_utils.update_learning_rate(optimizer, lr, lr_new)\n                lr = optimizer.param_groups[0][\'lr\']\n                assert lr == lr_new\n            elif step == cfg.SOLVER.WARM_UP_ITERS:\n                net_utils.update_learning_rate(optimizer, lr, cfg.SOLVER.BASE_LR)\n                lr = optimizer.param_groups[0][\'lr\']\n                assert lr == cfg.SOLVER.BASE_LR\n\n            # Learning rate decay\n            if decay_steps_ind < len(cfg.SOLVER.STEPS) and \\\n                    step == cfg.SOLVER.STEPS[decay_steps_ind]:\n                logger.info(\'Decay the learning on step %d\', step)\n                lr_new = lr * cfg.SOLVER.GAMMA\n                net_utils.update_learning_rate(optimizer, lr, lr_new)\n                lr = optimizer.param_groups[0][\'lr\']\n                assert lr == lr_new\n                decay_steps_ind += 1\n\n            training_stats.IterTic()\n            optimizer.zero_grad()\n            for inner_iter in range(args.iter_size):\n                try:\n                    input_data = next(dataiterator)\n                except StopIteration:\n                    dataiterator = iter(dataloader)\n                    input_data = next(dataiterator)\n\n                for key in input_data:\n                    if key != \'roidb\': # roidb is a list of ndarrays with inconsistent length\n                        input_data[key] = list(map(Variable, input_data[key]))\n\n                net_outputs = maskRCNN(**input_data)\n                training_stats.UpdateIterStats(net_outputs, inner_iter)\n                loss = net_outputs[\'total_loss\']\n                loss.backward()\n            optimizer.step()\n            training_stats.IterToc()\n\n            training_stats.LogIterStats(step, lr)\n\n            if (step+1) % CHECKPOINT_PERIOD == 0:\n                save_ckpt(output_dir, args, step, train_size, maskRCNN, optimizer)\n\n        # ---- Training ends ----\n        # Save last checkpoint\n        save_ckpt(output_dir, args, step, train_size, maskRCNN, optimizer)\n\n    except (RuntimeError, KeyboardInterrupt):\n        del dataiterator\n        logger.info(\'Save ckpt on exception ...\')\n        save_ckpt(output_dir, args, step, train_size, maskRCNN, optimizer)\n        logger.info(\'Save ckpt done.\')\n        stack_trace = traceback.format_exc()\n        print(stack_trace)\n\n    finally:\n        if args.use_tfboard and not args.no_save:\n            tblogger.close()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
lib/core/__init__.py,0,b''
lib/core/config.py,3,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport six\nimport os\nimport os.path as osp\nimport copy\nfrom ast import literal_eval\n\nimport numpy as np\nfrom packaging import version\nimport torch\nimport torch.nn as nn\nfrom torch.nn import init\nimport yaml\n\nimport nn as mynn\nfrom utils.collections import AttrDict\n\n__C = AttrDict()\n# Consumers can get config by:\n#   from fast_rcnn_config import cfg\ncfg = __C\n\n\n# Random note: avoid using \'.ON\' as a config key since yaml converts it to True;\n# prefer \'ENABLED\' instead\n\n# ---------------------------------------------------------------------------- #\n# Training options\n# ---------------------------------------------------------------------------- #\n__C.TRAIN = AttrDict()\n\n# Datasets to train on\n# Available dataset list: datasets.dataset_catalog.DATASETS.keys()\n# If multiple datasets are listed, the model is trained on their union\n__C.TRAIN.DATASETS = ()\n\n# Scales to use during training\n# Each scale is the pixel size of an image\'s shortest side\n# If multiple scales are listed, then one is selected uniformly at random for\n# each training image (i.e., scale jitter data augmentation)\n__C.TRAIN.SCALES = (600, )\n\n# Max pixel size of the longest side of a scaled input image\n__C.TRAIN.MAX_SIZE = 1000\n\n# Images *per GPU* in the training minibatch\n# Total images per minibatch = TRAIN.IMS_PER_BATCH * NUM_GPUS\n__C.TRAIN.IMS_PER_BATCH = 2\n\n# RoI minibatch size *per image* (number of regions of interest [ROIs])\n# Total number of RoIs per training minibatch =\n#   TRAIN.BATCH_SIZE_PER_IM * TRAIN.IMS_PER_BATCH * NUM_GPUS\n# E.g., a common configuration is: 512 * 2 * 8 = 8192\n__C.TRAIN.BATCH_SIZE_PER_IM = 64\n\n# Fraction of minibatch that is labeled foreground (i.e. class > 0)\n__C.TRAIN.FG_FRACTION = 0.25\n\n# Overlap threshold for a ROI to be considered foreground (if >= FG_THRESH)\n__C.TRAIN.FG_THRESH = 0.5\n\n# Overlap threshold for a ROI to be considered background (class = 0 if\n# overlap in [LO, HI))\n__C.TRAIN.BG_THRESH_HI = 0.5\n__C.TRAIN.BG_THRESH_LO = 0.0\n\n# Use horizontally-flipped images during training?\n__C.TRAIN.USE_FLIPPED = True\n\n# Overlap required between a ROI and ground-truth box in order for that ROI to\n# be used as a bounding-box regression training example\n__C.TRAIN.BBOX_THRESH = 0.5\n\n# Train using these proposals\n# During training, all proposals specified in the file are used (no limit is\n# applied)\n# Proposal files must be in correspondence with the datasets listed in\n# TRAIN.DATASETS\n__C.TRAIN.PROPOSAL_FILES = ()\n\n# Snapshot (model checkpoint) period\n# Divide by NUM_GPUS to determine actual period (e.g., 20000/8 => 2500 iters)\n# to allow for linear training schedule scaling\n__C.TRAIN.SNAPSHOT_ITERS = 20000\n\n# Normalize the targets (subtract empirical mean, divide by empirical stddev)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS = True\n# Deprecated (inside weights)\n__C.TRAIN.BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0)\n# Normalize the targets using ""precomputed"" (or made up) means and stdevs\n# (BBOX_NORMALIZE_TARGETS must also be True) (legacy)\n__C.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED = False\n__C.TRAIN.BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)\n__C.TRAIN.BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)\n\n# Make minibatches from images that have similar aspect ratios (i.e. both\n# tall and thin or both short and wide)\n# This feature is critical for saving memory (and makes training slightly\n# faster)\n__C.TRAIN.ASPECT_GROUPING = True\n\n# Crop images that have too small or too large aspect ratio\n__C.TRAIN.ASPECT_CROPPING = False\n__C.TRAIN.ASPECT_HI = 2\n__C.TRAIN.ASPECT_LO = 0.5\n\n# ---------------------------------------------------------------------------- #\n# RPN training options\n# ---------------------------------------------------------------------------- #\n\n# Minimum overlap required between an anchor and ground-truth box for the\n# (anchor, gt box) pair to be a positive example (IOU >= thresh ==> positive RPN\n# example)\n__C.TRAIN.RPN_POSITIVE_OVERLAP = 0.7\n\n# Maximum overlap allowed between an anchor and ground-truth box for the\n# (anchor, gt box) pair to be a negative examples (IOU < thresh ==> negative RPN\n# example)\n__C.TRAIN.RPN_NEGATIVE_OVERLAP = 0.3\n\n# Target fraction of foreground (positive) examples per RPN minibatch\n__C.TRAIN.RPN_FG_FRACTION = 0.5\n\n# Total number of RPN examples per image\n__C.TRAIN.RPN_BATCH_SIZE_PER_IM = 256\n\n# NMS threshold used on RPN proposals (used during end-to-end training with RPN)\n__C.TRAIN.RPN_NMS_THRESH = 0.7\n\n# Number of top scoring RPN proposals to keep before applying NMS (per image)\n# When FPN is used, this is *per FPN level* (not total)\n__C.TRAIN.RPN_PRE_NMS_TOP_N = 12000\n\n# Number of top scoring RPN proposals to keep after applying NMS (per image)\n# This is the total number of RPN proposals produced (for both FPN and non-FPN\n# cases)\n__C.TRAIN.RPN_POST_NMS_TOP_N = 2000\n\n# Remove RPN anchors that go outside the image by RPN_STRADDLE_THRESH pixels\n# Set to -1 or a large value, e.g. 100000, to disable pruning anchors\n__C.TRAIN.RPN_STRADDLE_THRESH = 0\n\n# Proposal height and width both need to be greater than RPN_MIN_SIZE\n# (at orig image scale; not scale used during training or inference)\n__C.TRAIN.RPN_MIN_SIZE = 0\n\n# Filter proposals that are inside of crowd regions by CROWD_FILTER_THRESH\n# ""Inside"" is measured as: proposal-with-crowd intersection area divided by\n# proposal area\n__C.TRAIN.CROWD_FILTER_THRESH = 0.7\n\n# Ignore ground-truth objects with area < this threshold\n__C.TRAIN.GT_MIN_AREA = -1\n\n# Freeze the backbone architecture during training if set to True\n__C.TRAIN.FREEZE_CONV_BODY = False\n\n\n# ---------------------------------------------------------------------------- #\n# Data loader options\n# ---------------------------------------------------------------------------- #\n__C.DATA_LOADER = AttrDict()\n\n# Number of Python threads to use for the data loader (warning: using too many\n# threads can cause GIL-based interference with Python Ops leading to *slower*\n# training; 4 seems to be the sweet spot in our experience)\n__C.DATA_LOADER.NUM_THREADS = 4\n\n\n# ---------------------------------------------------------------------------- #\n# Inference (\'test\') options\n# ---------------------------------------------------------------------------- #\n__C.TEST = AttrDict()\n\n# Datasets to test on\n# Available dataset list: datasets.dataset_catalog.DATASETS.keys()\n# If multiple datasets are listed, testing is performed on each one sequentially\n__C.TEST.DATASETS = ()\n\n# Scale to use during testing (can NOT list multiple scales)\n# The scale is the pixel size of an image\'s shortest side\n__C.TEST.SCALE = 600\n\n# Max pixel size of the longest side of a scaled input image\n__C.TEST.MAX_SIZE = 1000\n\n# Overlap threshold used for non-maximum suppression (suppress boxes with\n# IoU >= this threshold)\n__C.TEST.NMS = 0.3\n\n# Apply Fast R-CNN style bounding-box regression if True\n__C.TEST.BBOX_REG = True\n\n# Test using these proposal files (must correspond with TEST.DATASETS)\n__C.TEST.PROPOSAL_FILES = ()\n\n# Limit on the number of proposals per image used during inference\n__C.TEST.PROPOSAL_LIMIT = 2000\n\n## NMS threshold used on RPN proposals\n__C.TEST.RPN_NMS_THRESH = 0.7\n\n# Number of top scoring RPN proposals to keep before applying NMS\n# When FPN is used, this is *per FPN level* (not total)\n__C.TEST.RPN_PRE_NMS_TOP_N = 12000\n\n# Number of top scoring RPN proposals to keep after applying NMS\n# This is the total number of RPN proposals produced (for both FPN and non-FPN\n# cases)\n__C.TEST.RPN_POST_NMS_TOP_N = 2000\n\n# Proposal height and width both need to be greater than RPN_MIN_SIZE\n# (at orig image scale; not scale used during training or inference)\n__C.TEST.RPN_MIN_SIZE = 0\n\n# Maximum number of detections to return per image (100 is based on the limit\n# established for the COCO dataset)\n__C.TEST.DETECTIONS_PER_IM = 100\n\n# Minimum score threshold (assuming scores in a [0, 1] range); a value chosen to\n# balance obtaining high recall with not having too many low precision\n# detections that will slow down inference post processing steps (like NMS)\n__C.TEST.SCORE_THRESH = 0.05\n\n# Save detection results files if True\n# If false, results files are cleaned up (they can be large) after local\n# evaluation\n__C.TEST.COMPETITION_MODE = True\n\n# Evaluate detections with the COCO json dataset eval code even if it\'s not the\n# evaluation code for the dataset (e.g. evaluate PASCAL VOC results using the\n# COCO API to get COCO style AP on PASCAL VOC)\n__C.TEST.FORCE_JSON_DATASET_EVAL = False\n\n# [Inferred value; do not set directly in a config]\n# Indicates if precomputed proposals are used at test time\n# Not set for 1-stage models and 2-stage models with RPN subnetwork enabled\n__C.TEST.PRECOMPUTED_PROPOSALS = True\n\n\n# ---------------------------------------------------------------------------- #\n# Test-time augmentations for bounding box detection\n# See configs/test_time_aug/e2e_mask_rcnn_R-50-FPN_2x.yaml for an example\n# ---------------------------------------------------------------------------- #\n__C.TEST.BBOX_AUG = AttrDict()\n\n# Enable test-time augmentation for bounding box detection if True\n__C.TEST.BBOX_AUG.ENABLED = False\n\n# Heuristic used to combine predicted box scores\n#   Valid options: (\'ID\', \'AVG\', \'UNION\')\n__C.TEST.BBOX_AUG.SCORE_HEUR = \'UNION\'\n\n# Heuristic used to combine predicted box coordinates\n#   Valid options: (\'ID\', \'AVG\', \'UNION\')\n__C.TEST.BBOX_AUG.COORD_HEUR = \'UNION\'\n\n# Horizontal flip at the original scale (id transform)\n__C.TEST.BBOX_AUG.H_FLIP = False\n\n# Each scale is the pixel size of an image\'s shortest side\n__C.TEST.BBOX_AUG.SCALES = ()\n\n# Max pixel size of the longer side\n__C.TEST.BBOX_AUG.MAX_SIZE = 4000\n\n# Horizontal flip at each scale\n__C.TEST.BBOX_AUG.SCALE_H_FLIP = False\n\n# Apply scaling based on object size\n__C.TEST.BBOX_AUG.SCALE_SIZE_DEP = False\n__C.TEST.BBOX_AUG.AREA_TH_LO = 50**2\n__C.TEST.BBOX_AUG.AREA_TH_HI = 180**2\n\n# Each aspect ratio is relative to image width\n__C.TEST.BBOX_AUG.ASPECT_RATIOS = ()\n\n# Horizontal flip at each aspect ratio\n__C.TEST.BBOX_AUG.ASPECT_RATIO_H_FLIP = False\n\n# ---------------------------------------------------------------------------- #\n# Test-time augmentations for mask detection\n# See configs/test_time_aug/e2e_mask_rcnn_R-50-FPN_2x.yaml for an example\n# ---------------------------------------------------------------------------- #\n__C.TEST.MASK_AUG = AttrDict()\n\n# Enable test-time augmentation for instance mask detection if True\n__C.TEST.MASK_AUG.ENABLED = False\n\n# Heuristic used to combine mask predictions\n# SOFT prefix indicates that the computation is performed on soft masks\n#   Valid options: (\'SOFT_AVG\', \'SOFT_MAX\', \'LOGIT_AVG\')\n__C.TEST.MASK_AUG.HEUR = \'SOFT_AVG\'\n\n# Horizontal flip at the original scale (id transform)\n__C.TEST.MASK_AUG.H_FLIP = False\n\n# Each scale is the pixel size of an image\'s shortest side\n__C.TEST.MASK_AUG.SCALES = ()\n\n# Max pixel size of the longer side\n__C.TEST.MASK_AUG.MAX_SIZE = 4000\n\n# Horizontal flip at each scale\n__C.TEST.MASK_AUG.SCALE_H_FLIP = False\n\n# Apply scaling based on object size\n__C.TEST.MASK_AUG.SCALE_SIZE_DEP = False\n__C.TEST.MASK_AUG.AREA_TH = 180**2\n\n# Each aspect ratio is relative to image width\n__C.TEST.MASK_AUG.ASPECT_RATIOS = ()\n\n# Horizontal flip at each aspect ratio\n__C.TEST.MASK_AUG.ASPECT_RATIO_H_FLIP = False\n\n# ---------------------------------------------------------------------------- #\n# Test-augmentations for keypoints detection\n# configs/test_time_aug/keypoint_rcnn_R-50-FPN_1x.yaml\n# ---------------------------------------------------------------------------- #\n__C.TEST.KPS_AUG = AttrDict()\n\n# Enable test-time augmentation for keypoint detection if True\n__C.TEST.KPS_AUG.ENABLED = False\n\n# Heuristic used to combine keypoint predictions\n#   Valid options: (\'HM_AVG\', \'HM_MAX\')\n__C.TEST.KPS_AUG.HEUR = \'HM_AVG\'\n\n# Horizontal flip at the original scale (id transform)\n__C.TEST.KPS_AUG.H_FLIP = False\n\n# Each scale is the pixel size of an image\'s shortest side\n__C.TEST.KPS_AUG.SCALES = ()\n\n# Max pixel size of the longer side\n__C.TEST.KPS_AUG.MAX_SIZE = 4000\n\n# Horizontal flip at each scale\n__C.TEST.KPS_AUG.SCALE_H_FLIP = False\n\n# Apply scaling based on object size\n__C.TEST.KPS_AUG.SCALE_SIZE_DEP = False\n__C.TEST.KPS_AUG.AREA_TH = 180**2\n\n# Eeach aspect ratio is realtive to image width\n__C.TEST.KPS_AUG.ASPECT_RATIOS = ()\n\n# Horizontal flip at each aspect ratio\n__C.TEST.KPS_AUG.ASPECT_RATIO_H_FLIP = False\n\n# ---------------------------------------------------------------------------- #\n# Soft NMS\n# ---------------------------------------------------------------------------- #\n__C.TEST.SOFT_NMS = AttrDict()\n\n# Use soft NMS instead of standard NMS if set to True\n__C.TEST.SOFT_NMS.ENABLED = False\n# See soft NMS paper for definition of these options\n__C.TEST.SOFT_NMS.METHOD = \'linear\'\n__C.TEST.SOFT_NMS.SIGMA = 0.5\n# For the soft NMS overlap threshold, we simply use TEST.NMS\n\n# ---------------------------------------------------------------------------- #\n# Bounding box voting (from the Multi-Region CNN paper)\n# ---------------------------------------------------------------------------- #\n__C.TEST.BBOX_VOTE = AttrDict()\n\n# Use box voting if set to True\n__C.TEST.BBOX_VOTE.ENABLED = False\n\n# We use TEST.NMS threshold for the NMS step. VOTE_TH overlap threshold\n# is used to select voting boxes (IoU >= VOTE_TH) for each box that survives NMS\n__C.TEST.BBOX_VOTE.VOTE_TH = 0.8\n\n# The method used to combine scores when doing bounding box voting\n# Valid options include (\'ID\', \'AVG\', \'IOU_AVG\', \'GENERALIZED_AVG\', \'QUASI_SUM\')\n__C.TEST.BBOX_VOTE.SCORING_METHOD = \'ID\'\n\n# Hyperparameter used by the scoring method (it has different meanings for\n# different methods)\n__C.TEST.BBOX_VOTE.SCORING_METHOD_BETA = 1.0\n\n\n# ---------------------------------------------------------------------------- #\n# Model options\n# ---------------------------------------------------------------------------- #\n__C.MODEL = AttrDict()\n\n# The type of model to use\n# The string must match a function in the modeling.model_builder module\n# (e.g., \'generalized_rcnn\', \'mask_rcnn\', ...)\n__C.MODEL.TYPE = \'\'\n\n# The backbone conv body to use\n__C.MODEL.CONV_BODY = \'\'\n\n# Number of classes in the dataset; must be set\n# E.g., 81 for COCO (80 foreground + 1 background)\n__C.MODEL.NUM_CLASSES = -1\n\n# Use a class agnostic bounding box regressor instead of the default per-class\n# regressor\n__C.MODEL.CLS_AGNOSTIC_BBOX_REG = False\n\n# Default weights on (dx, dy, dw, dh) for normalizing bbox regression targets\n# These are empirically chosen to approximately lead to unit variance targets\n#\n# In older versions, the weights were set such that the regression deltas\n# would have unit standard deviation on the training dataset. Presently, rather\n# than computing these statistics exactly, we use a fixed set of weights\n# (10., 10., 5., 5.) by default. These are approximately the weights one would\n# get from COCO using the previous unit stdev heuristic.\n__C.MODEL.BBOX_REG_WEIGHTS = (10., 10., 5., 5.)\n\n# The meaning of FASTER_RCNN depends on the context (training vs. inference):\n# 1) During training, FASTER_RCNN = True means that end-to-end training will be\n#    used to jointly train the RPN subnetwork and the Fast R-CNN subnetwork\n#    (Faster R-CNN = RPN + Fast R-CNN).\n# 2) During inference, FASTER_RCNN = True means that the model\'s RPN subnetwork\n#    will be used to generate proposals rather than relying on precomputed\n#    proposals. Note that FASTER_RCNN = True can be used at inference time even\n#    if the Faster R-CNN model was trained with stagewise training (which\n#    consists of alternating between RPN and Fast R-CNN training in a way that\n#    finally leads to a single network).\n__C.MODEL.FASTER_RCNN = False\n\n# Indicates the model makes instance mask predictions (as in Mask R-CNN)\n__C.MODEL.MASK_ON = False\n\n# Indicates the model makes keypoint predictions (as in Mask R-CNN for\n# keypoints)\n__C.MODEL.KEYPOINTS_ON = False\n\n# Indicates the model\'s computation terminates with the production of RPN\n# proposals (i.e., it outputs proposals ONLY, no actual object detections)\n__C.MODEL.RPN_ONLY = False\n\n# [Inferred value; do not set directly in a config]\n# Indicate whether the res5 stage weights and training forward computation\n# are shared from box head or not.\n__C.MODEL.SHARE_RES5 = False\n\n# Whether to load imagenet pretrained weights\n# If True, path to the weight file must be specified.\n# See: __C.RESNETS.IMAGENET_PRETRAINED_WEIGHTS\n__C.MODEL.LOAD_IMAGENET_PRETRAINED_WEIGHTS = True\n\n# ---------------------------------------------------------------------------- #\n# Unsupervise Pose\n# ---------------------------------------------------------------------------- #\n\n__C.MODEL.UNSUPERVISED_POSE = False\n\n\n# ---------------------------------------------------------------------------- #\n# RetinaNet options\n# ---------------------------------------------------------------------------- #\n__C.RETINANET = AttrDict()\n\n# RetinaNet is used (instead of Fast/er/Mask R-CNN/R-FCN/RPN) if True\n__C.RETINANET.RETINANET_ON = False\n\n# Anchor aspect ratios to use\n__C.RETINANET.ASPECT_RATIOS = (0.5, 1.0, 2.0)\n\n# Anchor scales per octave\n__C.RETINANET.SCALES_PER_OCTAVE = 3\n\n# At each FPN level, we generate anchors based on their scale, aspect_ratio,\n# stride of the level, and we multiply the resulting anchor by ANCHOR_SCALE\n__C.RETINANET.ANCHOR_SCALE = 4\n\n# Convolutions to use in the cls and bbox tower\n# NOTE: this doesn\'t include the last conv for logits\n__C.RETINANET.NUM_CONVS = 4\n\n# Weight for bbox_regression loss\n__C.RETINANET.BBOX_REG_WEIGHT = 1.0\n\n# Smooth L1 loss beta for bbox regression\n__C.RETINANET.BBOX_REG_BETA = 0.11\n\n# During inference, #locs to select based on cls score before NMS is performed\n# per FPN level\n__C.RETINANET.PRE_NMS_TOP_N = 1000\n\n# IoU overlap ratio for labeling an anchor as positive\n# Anchors with >= iou overlap are labeled positive\n__C.RETINANET.POSITIVE_OVERLAP = 0.5\n\n# IoU overlap ratio for labeling an anchor as negative\n# Anchors with < iou overlap are labeled negative\n__C.RETINANET.NEGATIVE_OVERLAP = 0.4\n\n# Focal loss parameter: alpha\n__C.RETINANET.LOSS_ALPHA = 0.25\n\n# Focal loss parameter: gamma\n__C.RETINANET.LOSS_GAMMA = 2.0\n\n# Prior prob for the positives at the beginning of training. This is used to set\n# the bias init for the logits layer\n__C.RETINANET.PRIOR_PROB = 0.01\n\n# Whether classification and bbox branch tower should be shared or not\n__C.RETINANET.SHARE_CLS_BBOX_TOWER = False\n\n# Use class specific bounding box regression instead of the default class\n# agnostic regression\n__C.RETINANET.CLASS_SPECIFIC_BBOX = False\n\n# Whether softmax should be used in classification branch training\n__C.RETINANET.SOFTMAX = False\n\n# Inference cls score threshold, anchors with score > INFERENCE_TH are\n# considered for inference\n__C.RETINANET.INFERENCE_TH = 0.05\n\n\n# ---------------------------------------------------------------------------- #\n# Solver options\n# Note: all solver options are used exactly as specified; the implication is\n# that if you switch from training on 1 GPU to N GPUs, you MUST adjust the\n# solver configuration accordingly. We suggest using gradual warmup and the\n# linear learning rate scaling rule as described in\n# ""Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour"" Goyal et al.\n# https://arxiv.org/abs/1706.02677\n# ---------------------------------------------------------------------------- #\n__C.SOLVER = AttrDict()\n\n# e.g \'SGD\', \'Adam\'\n__C.SOLVER.TYPE = \'SGD\'\n\n# Base learning rate for the specified schedule\n__C.SOLVER.BASE_LR = 0.001\n\n# Schedule type (see functions in utils.lr_policy for options)\n# E.g., \'step\', \'steps_with_decay\', ...\n__C.SOLVER.LR_POLICY = \'step\'\n\n# Some LR Policies (by example):\n# \'step\'\n#   lr = SOLVER.BASE_LR * SOLVER.GAMMA ** (cur_iter // SOLVER.STEP_SIZE)\n# \'steps_with_decay\'\n#   SOLVER.STEPS = [0, 60000, 80000]\n#   SOLVER.GAMMA = 0.1\n#   lr = SOLVER.BASE_LR * SOLVER.GAMMA ** current_step\n#   iters [0, 59999] are in current_step = 0, iters [60000, 79999] are in\n#   current_step = 1, and so on\n# \'steps_with_lrs\'\n#   SOLVER.STEPS = [0, 60000, 80000]\n#   SOLVER.LRS = [0.02, 0.002, 0.0002]\n#   lr = LRS[current_step]\n\n# Hyperparameter used by the specified policy\n# For \'step\', the current LR is multiplied by SOLVER.GAMMA at each step\n__C.SOLVER.GAMMA = 0.1\n\n# Uniform step size for \'steps\' policy\n__C.SOLVER.STEP_SIZE = 30000\n\n# Non-uniform step iterations for \'steps_with_decay\' or \'steps_with_lrs\'\n# policies\n__C.SOLVER.STEPS = []\n\n# Learning rates to use with \'steps_with_lrs\' policy\n__C.SOLVER.LRS = []\n\n# Maximum number of SGD iterations\n__C.SOLVER.MAX_ITER = 40000\n\n# Momentum to use with SGD\n__C.SOLVER.MOMENTUM = 0.9\n\n# L2 regularization hyperparameter\n__C.SOLVER.WEIGHT_DECAY = 0.0005\n# L2 regularization hyperparameter for GroupNorm\'s parameters\n__C.SOLVER.WEIGHT_DECAY_GN = 0.0\n\n# Whether to double the learning rate for bias\n__C.SOLVER.BIAS_DOUBLE_LR = True\n\n# Whether to have weight decay on bias as well\n__C.SOLVER.BIAS_WEIGHT_DECAY = False\n\n# Warm up to SOLVER.BASE_LR over this number of SGD iterations\n__C.SOLVER.WARM_UP_ITERS = 500\n\n# Start the warm up from SOLVER.BASE_LR * SOLVER.WARM_UP_FACTOR\n__C.SOLVER.WARM_UP_FACTOR = 1.0 / 3.0\n\n# WARM_UP_METHOD can be either \'constant\' or \'linear\' (i.e., gradual)\n__C.SOLVER.WARM_UP_METHOD = \'linear\'\n\n# Scale the momentum update history by new_lr / old_lr when updating the\n# learning rate (this is correct given MomentumSGDUpdateOp)\n__C.SOLVER.SCALE_MOMENTUM = True\n# Only apply the correction if the relative LR change exceeds this threshold\n# (prevents ever change in linear warm up from scaling the momentum by a tiny\n# amount; momentum scaling is only important if the LR change is large)\n__C.SOLVER.SCALE_MOMENTUM_THRESHOLD = 1.1\n\n# Suppress logging of changes to LR unless the relative change exceeds this\n# threshold (prevents linear warm up from spamming the training log)\n__C.SOLVER.LOG_LR_CHANGE_THRESHOLD = 1.1\n\n\n# ---------------------------------------------------------------------------- #\n# Fast R-CNN options\n# ---------------------------------------------------------------------------- #\n__C.FAST_RCNN = AttrDict()\n\n# The type of RoI head to use for bounding box classification and regression\n# The string must match a function this is imported in modeling.model_builder\n# (e.g., \'head_builder.add_roi_2mlp_head\' to specify a two hidden layer MLP)\n__C.FAST_RCNN.ROI_BOX_HEAD = \'\'\n\n# Hidden layer dimension when using an MLP for the RoI box head\n__C.FAST_RCNN.MLP_HEAD_DIM = 1024\n\n# Hidden Conv layer dimension when using Convs for the RoI box head\n__C.FAST_RCNN.CONV_HEAD_DIM = 256\n# Number of stacked Conv layers in the RoI box head\n__C.FAST_RCNN.NUM_STACKED_CONVS = 4\n\n# RoI transformation function (e.g., RoIPool or RoIAlign)\n# (RoIPoolF is the same as RoIPool; ignore the trailing \'F\')\n__C.FAST_RCNN.ROI_XFORM_METHOD = \'RoIPoolF\'\n\n# Number of grid sampling points in RoIAlign (usually use 2)\n# Only applies to RoIAlign\n__C.FAST_RCNN.ROI_XFORM_SAMPLING_RATIO = 0\n\n# RoI transform output resolution\n# Note: some models may have constraints on what they can use, e.g. they use\n# pretrained FC layers like in VGG16, and will ignore this option\n__C.FAST_RCNN.ROI_XFORM_RESOLUTION = 14\n\n\n# ---------------------------------------------------------------------------- #\n# RPN options\n# ---------------------------------------------------------------------------- #\n__C.RPN = AttrDict()\n\n# [Infered value; do not set directly in a config]\n# Indicates that the model contains an RPN subnetwork\n__C.RPN.RPN_ON = False\n\n# `True` for Detectron implementation. `False` for jwyang\'s implementation.\n__C.RPN.OUT_DIM_AS_IN_DIM = True\n\n# Output dim of conv2d. Ignored if `__C.RPN.OUT_DIM_AS_IN_DIM` is True.\n# 512 is the fixed value in jwyang\'s implementation.\n__C.RPN.OUT_DIM = 512\n\n# \'sigmoid\' or \'softmax\'. Detectron use \'sigmoid\'. jwyang use \'softmax\'\n# This will affect the conv2d output dim for classifying the bg/fg rois\n__C.RPN.CLS_ACTIVATION = \'sigmoid\'\n\n# RPN anchor sizes given in absolute pixels w.r.t. the scaled network input\n# Note: these options are *not* used by FPN RPN; see FPN.RPN* options\n__C.RPN.SIZES = (64, 128, 256, 512)\n\n# Stride of the feature map that RPN is attached\n__C.RPN.STRIDE = 16\n\n# RPN anchor aspect ratios\n__C.RPN.ASPECT_RATIOS = (0.5, 1, 2)\n\n\n# ---------------------------------------------------------------------------- #\n# FPN options\n# ---------------------------------------------------------------------------- #\n\n__C.FPN = AttrDict()\n\n# FPN is enabled if True\n__C.FPN.FPN_ON = False\n\n# Channel dimension of the FPN feature levels\n__C.FPN.DIM = 256\n\n# Initialize the lateral connections to output zero if True\n__C.FPN.ZERO_INIT_LATERAL = False\n\n# Stride of the coarsest FPN level\n# This is needed so the input can be padded properly\n__C.FPN.COARSEST_STRIDE = 32\n\n#\n# FPN may be used for just RPN, just object detection, or both\n#\n\n# Use FPN for RoI transform for object detection if True\n__C.FPN.MULTILEVEL_ROIS = False\n# Hyperparameters for the RoI-to-FPN level mapping heuristic\n__C.FPN.ROI_CANONICAL_SCALE = 224  # s0\n__C.FPN.ROI_CANONICAL_LEVEL = 4  # k0: where s0 maps to\n# Coarsest level of the FPN pyramid\n__C.FPN.ROI_MAX_LEVEL = 5\n# Finest level of the FPN pyramid\n__C.FPN.ROI_MIN_LEVEL = 2\n\n# Use FPN for RPN if True\n__C.FPN.MULTILEVEL_RPN = False\n# Coarsest level of the FPN pyramid\n__C.FPN.RPN_MAX_LEVEL = 6\n# Finest level of the FPN pyramid\n__C.FPN.RPN_MIN_LEVEL = 2\n# FPN RPN anchor aspect ratios\n__C.FPN.RPN_ASPECT_RATIOS = (0.5, 1, 2)\n# RPN anchors start at this size on RPN_MIN_LEVEL\n# The anchor size doubled each level after that\n# With a default of 32 and levels 2 to 6, we get anchor sizes of 32 to 512\n__C.FPN.RPN_ANCHOR_START_SIZE = 32\n# [Infered Value] Scale for RPN_POST_NMS_TOP_N.\n# Automatically infered in training, fixed to 1 in testing.\n__C.FPN.RPN_COLLECT_SCALE = 1\n# Use extra FPN levels, as done in the RetinaNet paper\n__C.FPN.EXTRA_CONV_LEVELS = False\n# Use GroupNorm in the FPN-specific layers (lateral, etc.)\n__C.FPN.USE_GN = False\n\n\n# ---------------------------------------------------------------------------- #\n# Mask R-CNN options (""MRCNN"" means Mask R-CNN)\n# ---------------------------------------------------------------------------- #\n__C.MRCNN = AttrDict()\n\n# The type of RoI head to use for instance mask prediction\n# The string must match a function this is imported in modeling.model_builder\n# (e.g., \'mask_rcnn_heads.ResNet_mask_rcnn_fcn_head_v1up4convs\')\n__C.MRCNN.ROI_MASK_HEAD = \'\'\n\n# Resolution of mask predictions\n__C.MRCNN.RESOLUTION = 14\n\n# RoI transformation function and associated options\n__C.MRCNN.ROI_XFORM_METHOD = \'RoIAlign\'\n\n# RoI transformation function (e.g., RoIPool or RoIAlign)\n__C.MRCNN.ROI_XFORM_RESOLUTION = 7\n\n# Number of grid sampling points in RoIAlign (usually use 2)\n# Only applies to RoIAlign\n__C.MRCNN.ROI_XFORM_SAMPLING_RATIO = 0\n\n# Number of channels in the mask head\n__C.MRCNN.DIM_REDUCED = 256\n\n# Use dilated convolution in the mask head\n__C.MRCNN.DILATION = 2\n\n# Upsample the predicted masks by this factor\n__C.MRCNN.UPSAMPLE_RATIO = 1\n\n# Use a fully-connected layer to predict the final masks instead of a conv layer\n__C.MRCNN.USE_FC_OUTPUT = False\n\n# Weight initialization method for the mask head and mask output layers. [\'GaussianFill\', \'MSRAFill\']\n__C.MRCNN.CONV_INIT = \'GaussianFill\'\n\n# Use class specific mask predictions if True (otherwise use class agnostic mask\n# predictions)\n__C.MRCNN.CLS_SPECIFIC_MASK = True\n\n# Multi-task loss weight for masks\n__C.MRCNN.WEIGHT_LOSS_MASK = 1.0\n\n# Binarization threshold for converting soft masks to hard masks\n__C.MRCNN.THRESH_BINARIZE = 0.5\n\n__C.MRCNN.MEMORY_EFFICIENT_LOSS = True  # TODO\n\n\n# ---------------------------------------------------------------------------- #\n# Keyoint Mask R-CNN options (""KRCNN"" = Mask R-CNN with Keypoint support)\n# ---------------------------------------------------------------------------- #\n__C.KRCNN = AttrDict()\n\n# The type of RoI head to use for instance keypoint prediction\n# The string must match a function this is imported in modeling.model_builder\n# (e.g., \'keypoint_rcnn_heads.add_roi_pose_head_v1convX\')\n__C.KRCNN.ROI_KEYPOINTS_HEAD = \'\'\n\n# Output size (and size loss is computed on), e.g., 56x56\n__C.KRCNN.HEATMAP_SIZE = -1\n\n# Use bilinear interpolation to upsample the final heatmap by this factor\n__C.KRCNN.UP_SCALE = -1\n\n# Apply a ConvTranspose layer to the hidden representation computed by the\n# keypoint head prior to predicting the per-keypoint heatmaps\n__C.KRCNN.USE_DECONV = False\n# Channel dimension of the hidden representation produced by the ConvTranspose\n__C.KRCNN.DECONV_DIM = 256\n\n# Use a ConvTranspose layer to predict the per-keypoint heatmaps\n__C.KRCNN.USE_DECONV_OUTPUT = False\n\n# Use dilation in the keypoint head\n__C.KRCNN.DILATION = 1\n\n# Size of the kernels to use in all ConvTranspose operations\n__C.KRCNN.DECONV_KERNEL = 4\n\n# Number of keypoints in the dataset (e.g., 17 for COCO)\n__C.KRCNN.NUM_KEYPOINTS = -1\n\n# Number of stacked Conv layers in keypoint head\n__C.KRCNN.NUM_STACKED_CONVS = 8\n\n# Dimension of the hidden representation output by the keypoint head\n__C.KRCNN.CONV_HEAD_DIM = 256\n\n# Conv kernel size used in the keypoint head\n__C.KRCNN.CONV_HEAD_KERNEL = 3\n# Conv kernel weight filling function\n__C.KRCNN.CONV_INIT = \'GaussianFill\'\n\n# Use NMS based on OKS if True\n__C.KRCNN.NMS_OKS = False\n\n# Source of keypoint confidence\n#   Valid options: (\'bbox\', \'logit\', \'prob\')\n__C.KRCNN.KEYPOINT_CONFIDENCE = \'bbox\'\n\n# Standard ROI XFORM options (see FAST_RCNN or MRCNN options)\n__C.KRCNN.ROI_XFORM_METHOD = \'RoIAlign\'\n__C.KRCNN.ROI_XFORM_RESOLUTION = 7\n__C.KRCNN.ROI_XFORM_SAMPLING_RATIO = 0\n\n# Minimum number of labeled keypoints that must exist in a minibatch (otherwise\n# the minibatch is discarded)\n__C.KRCNN.MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH = 20\n\n# When infering the keypoint locations from the heatmap, don\'t scale the heatmap\n# below this minimum size\n__C.KRCNN.INFERENCE_MIN_SIZE = 0\n\n# Multi-task loss weight to use for keypoints\n# Recommended values:\n#   - use 1.0 if KRCNN.NORMALIZE_BY_VISIBLE_KEYPOINTS is True\n#   - use 4.0 if KRCNN.NORMALIZE_BY_VISIBLE_KEYPOINTS is False\n__C.KRCNN.LOSS_WEIGHT = 1.0\n\n# Normalize by the total number of visible keypoints in the minibatch if True.\n# Otherwise, normalize by the total number of keypoints that could ever exist\n# in the minibatch. See comments in modeling.model_builder.add_keypoint_losses\n# for detailed discussion.\n__C.KRCNN.NORMALIZE_BY_VISIBLE_KEYPOINTS = True\n\n\n# ---------------------------------------------------------------------------- #\n# R-FCN options\n# ---------------------------------------------------------------------------- #\n__C.RFCN = AttrDict()\n\n# Position-sensitive RoI pooling output grid size (height and width)\n__C.RFCN.PS_GRID_SIZE = 3\n\n\n# ---------------------------------------------------------------------------- #\n# ResNets options (""ResNets"" = ResNet and ResNeXt)\n# ---------------------------------------------------------------------------- #\n__C.RESNETS = AttrDict()\n\n# Number of groups to use; 1 ==> ResNet; > 1 ==> ResNeXt\n__C.RESNETS.NUM_GROUPS = 1\n\n# Baseline width of each group\n__C.RESNETS.WIDTH_PER_GROUP = 64\n\n# Place the stride 2 conv on the 1x1 filter\n# Use True only for the original MSRA ResNet; use False for C2 and Torch models\n__C.RESNETS.STRIDE_1X1 = True\n\n# Residual transformation function\n__C.RESNETS.TRANS_FUNC = \'bottleneck_transformation\'\n# ResNet\'s stem function (conv1 and pool1)\n__C.RESNETS.STEM_FUNC = \'basic_bn_stem\'\n# ResNet\'s shortcut function\n__C.RESNETS.SHORTCUT_FUNC = \'basic_bn_shortcut\'\n\n# Apply dilation in stage ""res5""\n__C.RESNETS.RES5_DILATION = 1\n\n# Freeze model weights before and including which block.\n# Choices: [0, 2, 3, 4, 5]. O means not fixed. First conv and bn are defaults to\n# be fixed.\n__C.RESNETS.FREEZE_AT = 2\n\n# Path to pretrained resnet weights on ImageNet.\n# If start with \'/\', then it is treated as a absolute path.\n# Otherwise, treat as a relative path to __C.ROOT_DIR\n__C.RESNETS.IMAGENET_PRETRAINED_WEIGHTS = \'\'\n\n# Use GroupNorm instead of BatchNorm\n__C.RESNETS.USE_GN = False\n\n\n# ---------------------------------------------------------------------------- #\n# GroupNorm options\n# ---------------------------------------------------------------------------- #\n__C.GROUP_NORM = AttrDict()\n# Number of dimensions per group in GroupNorm (-1 if using NUM_GROUPS)\n__C.GROUP_NORM.DIM_PER_GP = -1\n# Number of groups in GroupNorm (-1 if using DIM_PER_GP)\n__C.GROUP_NORM.NUM_GROUPS = 32\n# GroupNorm\'s small constant in the denominator\n__C.GROUP_NORM.EPSILON = 1e-5\n\n\n# ---------------------------------------------------------------------------- #\n# MISC options\n# ---------------------------------------------------------------------------- #\n\n# Number of GPUs to use (applies to both training and testing)\n__C.NUM_GPUS = 1\n\n# The mapping from image coordinates to feature map coordinates might cause\n# some boxes that are distinct in image space to become identical in feature\n# coordinates. If DEDUP_BOXES > 0, then DEDUP_BOXES is used as the scale factor\n# for identifying duplicate boxes.\n# 1/16 is correct for {Alex,Caffe}Net, VGG_CNN_M_1024, and VGG16\n__C.DEDUP_BOXES = 1. / 16.\n\n# Clip bounding box transformation predictions to prevent np.exp from\n# overflowing\n# Heuristic choice based on that would scale a 16 pixel anchor up to 1000 pixels\n__C.BBOX_XFORM_CLIP = np.log(1000. / 16.)\n\n# Pixel mean values (BGR order) as a (1, 1, 3) array\n# We use the same pixel mean for all networks even though it\'s not exactly what\n# they were trained with\n# ""Fun"" fact: the history of where these values comes from is lost (From Detectron lol)\n__C.PIXEL_MEANS = np.array([[[102.9801, 115.9465, 122.7717]]])\n\n# For reproducibility\n__C.RNG_SEED = 3\n\n# A small number that\'s used many times\n__C.EPS = 1e-14\n\n# Root directory of project\n__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__), \'..\', \'..\'))\n\n# Output basedir\n__C.OUTPUT_DIR = \'Outputs\'\n\n# Name (or path to) the matlab executable\n__C.MATLAB = \'matlab\'\n\n# Dump detection visualizations\n__C.VIS = False\n\n# Score threshold for visualization\n__C.VIS_TH = 0.9\n\n# Expected results should take the form of a list of expectations, each\n# specified by four elements (dataset, task, metric, expected value). For\n# example: [[\'coco_2014_minival\', \'box_proposal\', \'AR@1000\', 0.387]]\n__C.EXPECTED_RESULTS = []\n# Absolute and relative tolerance to use when comparing to EXPECTED_RESULTS\n__C.EXPECTED_RESULTS_RTOL = 0.1\n__C.EXPECTED_RESULTS_ATOL = 0.005\n# Set to send email in case of an EXPECTED_RESULTS failure\n__C.EXPECTED_RESULTS_EMAIL = \'\'\n\n# ------------------------------\n# Data directory\n__C.DATA_DIR = osp.abspath(osp.join(__C.ROOT_DIR, \'data\'))\n\n# [Deprecate]\n__C.POOLING_MODE = \'crop\'\n\n# [Deprecate] Size of the pooled region after RoI pooling\n__C.POOLING_SIZE = 7\n\n__C.CROP_RESIZE_WITH_MAX_POOL = True\n\n# [Infered value]\n__C.CUDA = False\n\n__C.DEBUG = False\n\n# [Infered value]\n__C.PYTORCH_VERSION_LESS_THAN_040 = False\n\n# ---------------------------------------------------------------------------- #\n# mask heads or keypoint heads that share res5 stage weights and\n# training forward computation with box head.\n# ---------------------------------------------------------------------------- #\n_SHARE_RES5_HEADS = set(\n    [\n        \'mask_rcnn_heads.mask_rcnn_fcn_head_v0upshare\',\n    ]\n)\n\n\ndef assert_and_infer_cfg(make_immutable=True):\n    """"""Call this function in your script after you have finished setting all cfg\n    values that are necessary (e.g., merging a config from a file, merging\n    command line config options, etc.). By default, this function will also\n    mark the global cfg as immutable to prevent changing the global cfg settings\n    during script execution (which can lead to hard to debug errors or code\n    that\'s harder to understand than is necessary).\n    """"""\n    if __C.MODEL.RPN_ONLY or __C.MODEL.FASTER_RCNN:\n        __C.RPN.RPN_ON = True\n    if __C.RPN.RPN_ON or __C.RETINANET.RETINANET_ON:\n        __C.TEST.PRECOMPUTED_PROPOSALS = False\n    if __C.MODEL.LOAD_IMAGENET_PRETRAINED_WEIGHTS:\n        assert __C.RESNETS.IMAGENET_PRETRAINED_WEIGHTS, \\\n            ""Path to the weight file must not be empty to load imagenet pertrained resnets.""\n    if set([__C.MRCNN.ROI_MASK_HEAD, __C.KRCNN.ROI_KEYPOINTS_HEAD]) & _SHARE_RES5_HEADS:\n        __C.MODEL.SHARE_RES5 = True\n    if version.parse(torch.__version__) < version.parse(\'0.4.0\'):\n        __C.PYTORCH_VERSION_LESS_THAN_040 = True\n        # create alias for PyTorch version less than 0.4.0\n        init.uniform_ = init.uniform\n        init.normal_ = init.normal\n        init.constant_ = init.constant\n        nn.GroupNorm = mynn.GroupNorm\n    if make_immutable:\n        cfg.immutable(True)\n\n\ndef merge_cfg_from_file(cfg_filename):\n    """"""Load a yaml config file and merge it into the global config.""""""\n    with open(cfg_filename, \'r\') as f:\n        yaml_cfg = AttrDict(yaml.load(f))\n    _merge_a_into_b(yaml_cfg, __C)\n\ncfg_from_file = merge_cfg_from_file\n\n\ndef merge_cfg_from_cfg(cfg_other):\n    """"""Merge `cfg_other` into the global config.""""""\n    _merge_a_into_b(cfg_other, __C)\n\n\ndef merge_cfg_from_list(cfg_list):\n    """"""Merge config keys, values in a list (e.g., from command line) into the\n    global config. For example, `cfg_list = [\'TEST.NMS\', 0.5]`.\n    """"""\n    assert len(cfg_list) % 2 == 0\n    for full_key, v in zip(cfg_list[0::2], cfg_list[1::2]):\n        # if _key_is_deprecated(full_key):\n        #     continue\n        # if _key_is_renamed(full_key):\n        #     _raise_key_rename_error(full_key)\n        key_list = full_key.split(\'.\')\n        d = __C\n        for subkey in key_list[:-1]:\n            assert subkey in d, \'Non-existent key: {}\'.format(full_key)\n            d = d[subkey]\n        subkey = key_list[-1]\n        assert subkey in d, \'Non-existent key: {}\'.format(full_key)\n        value = _decode_cfg_value(v)\n        value = _check_and_coerce_cfg_value_type(\n            value, d[subkey], subkey, full_key\n        )\n        d[subkey] = value\n\ncfg_from_list = merge_cfg_from_list\n\n\ndef _merge_a_into_b(a, b, stack=None):\n    """"""Merge config dictionary a into config dictionary b, clobbering the\n    options in b whenever they are also specified in a.\n    """"""\n    assert isinstance(a, AttrDict), \'Argument `a` must be an AttrDict\'\n    assert isinstance(b, AttrDict), \'Argument `b` must be an AttrDict\'\n\n    for k, v_ in a.items():\n        full_key = \'.\'.join(stack) + \'.\' + k if stack is not None else k\n        # a must specify keys that are in b\n        if k not in b:\n            # if _key_is_deprecated(full_key):\n            #     continue\n            # elif _key_is_renamed(full_key):\n            #     _raise_key_rename_error(full_key)\n            # else:\n            raise KeyError(\'Non-existent config key: {}\'.format(full_key))\n\n        v = copy.deepcopy(v_)\n        v = _decode_cfg_value(v)\n        v = _check_and_coerce_cfg_value_type(v, b[k], k, full_key)\n\n        # Recursively merge dicts\n        if isinstance(v, AttrDict):\n            try:\n                stack_push = [k] if stack is None else stack + [k]\n                _merge_a_into_b(v, b[k], stack=stack_push)\n            except BaseException:\n                raise\n        else:\n            b[k] = v\n\n\ndef _decode_cfg_value(v):\n    """"""Decodes a raw config value (e.g., from a yaml config files or command\n    line argument) into a Python object.\n    """"""\n    # Configs parsed from raw yaml will contain dictionary keys that need to be\n    # converted to AttrDict objects\n    if isinstance(v, dict):\n        return AttrDict(v)\n    # All remaining processing is only applied to strings\n    if not isinstance(v, six.string_types):\n        return v\n    # Try to interpret `v` as a:\n    #   string, number, tuple, list, dict, boolean, or None\n    try:\n        v = literal_eval(v)\n    # The following two excepts allow v to pass through when it represents a\n    # string.\n    #\n    # Longer explanation:\n    # The type of v is always a string (before calling literal_eval), but\n    # sometimes it *represents* a string and other times a data structure, like\n    # a list. In the case that v represents a string, what we got back from the\n    # yaml parser is \'foo\' *without quotes* (so, not \'""foo""\'). literal_eval is\n    # ok with \'""foo""\', but will raise a ValueError if given \'foo\'. In other\n    # cases, like paths (v = \'foo/bar\' and not v = \'""foo/bar""\'), literal_eval\n    # will raise a SyntaxError.\n    except ValueError:\n        pass\n    except SyntaxError:\n        pass\n    return v\n\n\ndef _check_and_coerce_cfg_value_type(value_a, value_b, key, full_key):\n    """"""Checks that `value_a`, which is intended to replace `value_b` is of the\n    right type. The type is correct if it matches exactly or is one of a few\n    cases in which the type can be easily coerced.\n    """"""\n    # The types must match (with some exceptions)\n    type_b = type(value_b)\n    type_a = type(value_a)\n    if type_a is type_b:\n        return value_a\n\n    # Exceptions: numpy arrays, strings, tuple<->list\n    if isinstance(value_b, np.ndarray):\n        value_a = np.array(value_a, dtype=value_b.dtype)\n    elif isinstance(value_b, six.string_types):\n        value_a = str(value_a)\n    elif isinstance(value_a, tuple) and isinstance(value_b, list):\n        value_a = list(value_a)\n    elif isinstance(value_a, list) and isinstance(value_b, tuple):\n        value_a = tuple(value_a)\n    else:\n        raise ValueError(\n            \'Type mismatch ({} vs. {}) with values ({} vs. {}) for config \'\n            \'key: {}\'.format(type_b, type_a, value_b, value_a, full_key)\n        )\n    return value_a\n'"
lib/core/test.py,7,"b'# Written by Roy Tseng\n#\n# Based on:\n# --------------------------------------------------------\n# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import defaultdict\nimport cv2\nimport numpy as np\nimport pycocotools.mask as mask_util\n\nfrom torch.autograd import Variable\nimport torch\n\nfrom core.config import cfg\nfrom utils.timer import Timer\nimport utils.boxes as box_utils\nimport utils.blob as blob_utils\nimport utils.fpn as fpn_utils\nimport utils.image as image_utils\nimport utils.keypoints as keypoint_utils\n\n\ndef im_detect_all(model, im, box_proposals=None, timers=None):\n    """"""Process the outputs of model for testing\n    Args:\n      model: the network module\n      im_data: Pytorch variable. Input batch to the model.\n      im_info: Pytorch variable. Input batch to the model.\n      gt_boxes: Pytorch variable. Input batch to the model.\n      num_boxes: Pytorch variable. Input batch to the model.\n      args: arguments from command line.\n      timer: record the cost of time for different steps\n    The rest of inputs are of type pytorch Variables and either input to or output from the model.\n    """"""\n    if timers is None:\n        timers = defaultdict(Timer)\n\n    timers[\'im_detect_bbox\'].tic()\n    if cfg.TEST.BBOX_AUG.ENABLED:\n        scores, boxes, im_scale, blob_conv = im_detect_bbox_aug(\n            model, im, box_proposals)\n    else:\n        scores, boxes, im_scale, blob_conv = im_detect_bbox(\n            model, im, cfg.TEST.SCALE, cfg.TEST.MAX_SIZE, box_proposals)\n    timers[\'im_detect_bbox\'].toc()\n\n    # score and boxes are from the whole image after score thresholding and nms\n    # (they are not separated by class) (numpy.ndarray)\n    # cls_boxes boxes and scores are separated by class and in the format used\n    # for evaluating results\n    timers[\'misc_bbox\'].tic()\n    scores, boxes, cls_boxes = box_results_with_nms_and_limit(scores, boxes)\n    timers[\'misc_bbox\'].toc()\n\n    if cfg.MODEL.MASK_ON and boxes.shape[0] > 0:\n        timers[\'im_detect_mask\'].tic()\n        if cfg.TEST.MASK_AUG.ENABLED:\n            masks = im_detect_mask_aug(model, im, boxes, im_scale, blob_conv)\n        else:\n            masks = im_detect_mask(model, im_scale, boxes, blob_conv)\n        timers[\'im_detect_mask\'].toc()\n\n        timers[\'misc_mask\'].tic()\n        cls_segms = segm_results(cls_boxes, masks, boxes, im.shape[0], im.shape[1])\n        timers[\'misc_mask\'].toc()\n    else:\n        cls_segms = None\n\n    if cfg.MODEL.KEYPOINTS_ON and boxes.shape[0] > 0:\n        timers[\'im_detect_keypoints\'].tic()\n        if cfg.TEST.KPS_AUG.ENABLED:\n            heatmaps = im_detect_keypoints_aug(model, im, boxes, im_scale, blob_conv)\n        else:\n            heatmaps = im_detect_keypoints(model, im_scale, boxes, blob_conv)\n        timers[\'im_detect_keypoints\'].toc()\n\n        timers[\'misc_keypoints\'].tic()\n        cls_keyps = keypoint_results(cls_boxes, heatmaps, boxes)\n        timers[\'misc_keypoints\'].toc()\n    else:\n        cls_keyps = None\n\n    return cls_boxes, cls_segms, cls_keyps\n\n\ndef im_conv_body_only(model, im, target_scale, target_max_size):\n    inputs, im_scale = _get_blobs(im, None, target_scale, target_max_size)\n\n    if cfg.PYTORCH_VERSION_LESS_THAN_040:\n        inputs[\'data\'] = Variable(torch.from_numpy(inputs[\'data\']), volatile=True).cuda()\n    else:\n        inputs[\'data\'] = torch.from_numpy(inputs[\'data\']).cuda()\n    inputs.pop(\'im_info\')\n\n    blob_conv = model.module.convbody_net(**inputs)\n\n    return blob_conv, im_scale\n\n\ndef im_detect_bbox(model, im, target_scale, target_max_size, boxes=None):\n    """"""Prepare the bbox for testing""""""\n\n    inputs, im_scale = _get_blobs(im, boxes, target_scale, target_max_size)\n\n    if cfg.DEDUP_BOXES > 0 and not cfg.MODEL.FASTER_RCNN:\n        v = np.array([1, 1e3, 1e6, 1e9, 1e12])\n        hashes = np.round(inputs[\'rois\'] * cfg.DEDUP_BOXES).dot(v)\n        _, index, inv_index = np.unique(\n            hashes, return_index=True, return_inverse=True\n        )\n        inputs[\'rois\'] = inputs[\'rois\'][index, :]\n        boxes = boxes[index, :]\n\n    # Add multi-level rois for FPN\n    if cfg.FPN.MULTILEVEL_ROIS and not cfg.MODEL.FASTER_RCNN:\n        _add_multilevel_rois_for_test(inputs, \'rois\')\n\n    if cfg.PYTORCH_VERSION_LESS_THAN_040:\n        inputs[\'data\'] = [Variable(torch.from_numpy(inputs[\'data\']), volatile=True)]\n        inputs[\'im_info\'] = [Variable(torch.from_numpy(inputs[\'im_info\']), volatile=True)]\n    else:\n        inputs[\'data\'] = [torch.from_numpy(inputs[\'data\'])]\n        inputs[\'im_info\'] = [torch.from_numpy(inputs[\'im_info\'])]\n\n    return_dict = model(**inputs)\n\n    if cfg.MODEL.FASTER_RCNN:\n        rois = return_dict[\'rois\'].data.cpu().numpy()\n        # unscale back to raw image space\n        boxes = rois[:, 1:5] / im_scale\n\n    # cls prob (activations after softmax)\n    scores = return_dict[\'cls_score\'].data.cpu().numpy().squeeze()\n    # In case there is 1 proposal\n    scores = scores.reshape([-1, scores.shape[-1]])\n\n    if cfg.TEST.BBOX_REG:\n        # Apply bounding-box regression deltas\n        box_deltas = return_dict[\'bbox_pred\'].data.cpu().numpy().squeeze()\n        # In case there is 1 proposal\n        box_deltas = box_deltas.reshape([-1, box_deltas.shape[-1]])\n        if cfg.MODEL.CLS_AGNOSTIC_BBOX_REG:\n            # Remove predictions for bg class (compat with MSRA code)\n            box_deltas = box_deltas[:, -4:]\n        if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n            # (legacy) Optionally normalize targets by a precomputed mean and stdev\n            box_deltas = box_deltas.view(-1, 4) * cfg.TRAIN.BBOX_NORMALIZE_STDS \\\n                         + cfg.TRAIN.BBOX_NORMALIZE_MEANS\n        pred_boxes = box_utils.bbox_transform(boxes, box_deltas, cfg.MODEL.BBOX_REG_WEIGHTS)\n        pred_boxes = box_utils.clip_tiled_boxes(pred_boxes, im.shape)\n        if cfg.MODEL.CLS_AGNOSTIC_BBOX_REG:\n            pred_boxes = np.tile(pred_boxes, (1, scores.shape[1]))\n    else:\n        # Simply repeat the boxes, once for each class\n        pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n\n    if cfg.DEDUP_BOXES > 0 and not cfg.MODEL.FASTER_RCNN:\n        # Map scores and predictions back to the original set of boxes\n        scores = scores[inv_index, :]\n        pred_boxes = pred_boxes[inv_index, :]\n\n    return scores, pred_boxes, im_scale, return_dict[\'blob_conv\']\n\n\ndef im_detect_bbox_aug(model, im, box_proposals=None):\n    """"""Performs bbox detection with test-time augmentations.\n    Function signature is the same as for im_detect_bbox.\n    """"""\n    assert not cfg.TEST.BBOX_AUG.SCALE_SIZE_DEP, \\\n        \'Size dependent scaling not implemented\'\n    assert not cfg.TEST.BBOX_AUG.SCORE_HEUR == \'UNION\' or \\\n        cfg.TEST.BBOX_AUG.COORD_HEUR == \'UNION\', \\\n        \'Coord heuristic must be union whenever score heuristic is union\'\n    assert not cfg.TEST.BBOX_AUG.COORD_HEUR == \'UNION\' or \\\n        cfg.TEST.BBOX_AUG.SCORE_HEUR == \'UNION\', \\\n        \'Score heuristic must be union whenever coord heuristic is union\'\n    assert not cfg.MODEL.FASTER_RCNN or \\\n        cfg.TEST.BBOX_AUG.SCORE_HEUR == \'UNION\', \\\n        \'Union heuristic must be used to combine Faster RCNN predictions\'\n\n    # Collect detections computed under different transformations\n    scores_ts = []\n    boxes_ts = []\n\n    def add_preds_t(scores_t, boxes_t):\n        scores_ts.append(scores_t)\n        boxes_ts.append(boxes_t)\n\n    # Perform detection on the horizontally flipped image\n    if cfg.TEST.BBOX_AUG.H_FLIP:\n        scores_hf, boxes_hf, _ = im_detect_bbox_hflip(\n            model,\n            im,\n            cfg.TEST.SCALE,\n            cfg.TEST.MAX_SIZE,\n            box_proposals=box_proposals\n        )\n        add_preds_t(scores_hf, boxes_hf)\n\n    # Compute detections at different scales\n    for scale in cfg.TEST.BBOX_AUG.SCALES:\n        max_size = cfg.TEST.BBOX_AUG.MAX_SIZE\n        scores_scl, boxes_scl = im_detect_bbox_scale(\n            model, im, scale, max_size, box_proposals\n        )\n        add_preds_t(scores_scl, boxes_scl)\n\n        if cfg.TEST.BBOX_AUG.SCALE_H_FLIP:\n            scores_scl_hf, boxes_scl_hf = im_detect_bbox_scale(\n                model, im, scale, max_size, box_proposals, hflip=True\n            )\n            add_preds_t(scores_scl_hf, boxes_scl_hf)\n\n    # Perform detection at different aspect ratios\n    for aspect_ratio in cfg.TEST.BBOX_AUG.ASPECT_RATIOS:\n        scores_ar, boxes_ar = im_detect_bbox_aspect_ratio(\n            model, im, aspect_ratio, box_proposals\n        )\n        add_preds_t(scores_ar, boxes_ar)\n\n        if cfg.TEST.BBOX_AUG.ASPECT_RATIO_H_FLIP:\n            scores_ar_hf, boxes_ar_hf = im_detect_bbox_aspect_ratio(\n                model, im, aspect_ratio, box_proposals, hflip=True\n            )\n            add_preds_t(scores_ar_hf, boxes_ar_hf)\n\n    # Compute detections for the original image (identity transform) last to\n    # ensure that the Caffe2 workspace is populated with blobs corresponding\n    # to the original image on return (postcondition of im_detect_bbox)\n    scores_i, boxes_i, im_scale_i, blob_conv_i = im_detect_bbox(\n        model, im, cfg.TEST.SCALE, cfg.TEST.MAX_SIZE, boxes=box_proposals\n    )\n    add_preds_t(scores_i, boxes_i)\n\n    # Combine the predicted scores\n    if cfg.TEST.BBOX_AUG.SCORE_HEUR == \'ID\':\n        scores_c = scores_i\n    elif cfg.TEST.BBOX_AUG.SCORE_HEUR == \'AVG\':\n        scores_c = np.mean(scores_ts, axis=0)\n    elif cfg.TEST.BBOX_AUG.SCORE_HEUR == \'UNION\':\n        scores_c = np.vstack(scores_ts)\n    else:\n        raise NotImplementedError(\n            \'Score heur {} not supported\'.format(cfg.TEST.BBOX_AUG.SCORE_HEUR)\n        )\n\n    # Combine the predicted boxes\n    if cfg.TEST.BBOX_AUG.COORD_HEUR == \'ID\':\n        boxes_c = boxes_i\n    elif cfg.TEST.BBOX_AUG.COORD_HEUR == \'AVG\':\n        boxes_c = np.mean(boxes_ts, axis=0)\n    elif cfg.TEST.BBOX_AUG.COORD_HEUR == \'UNION\':\n        boxes_c = np.vstack(boxes_ts)\n    else:\n        raise NotImplementedError(\n            \'Coord heur {} not supported\'.format(cfg.TEST.BBOX_AUG.COORD_HEUR)\n        )\n\n    return scores_c, boxes_c, im_scale_i, blob_conv_i\n\n\ndef im_detect_bbox_hflip(\n        model, im, target_scale, target_max_size, box_proposals=None):\n    """"""Performs bbox detection on the horizontally flipped image.\n    Function signature is the same as for im_detect_bbox.\n    """"""\n    # Compute predictions on the flipped image\n    im_hf = im[:, ::-1, :]\n    im_width = im.shape[1]\n\n    if not cfg.MODEL.FASTER_RCNN:\n        box_proposals_hf = box_utils.flip_boxes(box_proposals, im_width)\n    else:\n        box_proposals_hf = None\n\n    scores_hf, boxes_hf, im_scale, _ = im_detect_bbox(\n        model, im_hf, target_scale, target_max_size, boxes=box_proposals_hf\n    )\n\n    # Invert the detections computed on the flipped image\n    boxes_inv = box_utils.flip_boxes(boxes_hf, im_width)\n\n    return scores_hf, boxes_inv, im_scale\n\n\ndef im_detect_bbox_scale(\n        model, im, target_scale, target_max_size, box_proposals=None, hflip=False):\n    """"""Computes bbox detections at the given scale.\n    Returns predictions in the original image space.\n    """"""\n    if hflip:\n        scores_scl, boxes_scl, _ = im_detect_bbox_hflip(\n            model, im, target_scale, target_max_size, box_proposals=box_proposals\n        )\n    else:\n        scores_scl, boxes_scl, _, _ = im_detect_bbox(\n            model, im, target_scale, target_max_size, boxes=box_proposals\n        )\n    return scores_scl, boxes_scl\n\n\ndef im_detect_bbox_aspect_ratio(\n        model, im, aspect_ratio, box_proposals=None, hflip=False):\n    """"""Computes bbox detections at the given width-relative aspect ratio.\n    Returns predictions in the original image space.\n    """"""\n    # Compute predictions on the transformed image\n    im_ar = image_utils.aspect_ratio_rel(im, aspect_ratio)\n\n    if not cfg.MODEL.FASTER_RCNN:\n        box_proposals_ar = box_utils.aspect_ratio(box_proposals, aspect_ratio)\n    else:\n        box_proposals_ar = None\n\n    if hflip:\n        scores_ar, boxes_ar, _ = im_detect_bbox_hflip(\n            model,\n            im_ar,\n            cfg.TEST.SCALE,\n            cfg.TEST.MAX_SIZE,\n            box_proposals=box_proposals_ar\n        )\n    else:\n        scores_ar, boxes_ar, _, _ = im_detect_bbox(\n            model,\n            im_ar,\n            cfg.TEST.SCALE,\n            cfg.TEST.MAX_SIZE,\n            boxes=box_proposals_ar\n        )\n\n    # Invert the detected boxes\n    boxes_inv = box_utils.aspect_ratio(boxes_ar, 1.0 / aspect_ratio)\n\n    return scores_ar, boxes_inv\n\n\ndef im_detect_mask(model, im_scale, boxes, blob_conv):\n    """"""Infer instance segmentation masks. This function must be called after\n    im_detect_bbox as it assumes that the Caffe2 workspace is already populated\n    with the necessary blobs.\n\n    Arguments:\n        model (DetectionModelHelper): the detection model to use\n        im_scale (list): image blob scales as returned by im_detect_bbox\n        boxes (ndarray): R x 4 array of bounding box detections (e.g., as\n            returned by im_detect_bbox)\n        blob_conv (Variable): base features from the backbone network.\n\n    Returns:\n        pred_masks (ndarray): R x K x M x M array of class specific soft masks\n            output by the network (must be processed by segm_results to convert\n            into hard masks in the original image coordinate space)\n    """"""\n    M = cfg.MRCNN.RESOLUTION\n    if boxes.shape[0] == 0:\n        pred_masks = np.zeros((0, M, M), np.float32)\n        return pred_masks\n\n    inputs = {\'mask_rois\': _get_rois_blob(boxes, im_scale)}\n\n    # Add multi-level rois for FPN\n    if cfg.FPN.MULTILEVEL_ROIS:\n        _add_multilevel_rois_for_test(inputs, \'mask_rois\')\n\n    pred_masks = model.module.mask_net(blob_conv, inputs)\n    pred_masks = pred_masks.data.cpu().numpy().squeeze()\n\n    if cfg.MRCNN.CLS_SPECIFIC_MASK:\n        pred_masks = pred_masks.reshape([-1, cfg.MODEL.NUM_CLASSES, M, M])\n    else:\n        pred_masks = pred_masks.reshape([-1, 1, M, M])\n\n    return pred_masks\n\n\ndef im_detect_mask_aug(model, im, boxes, im_scale, blob_conv):\n    """"""Performs mask detection with test-time augmentations.\n\n    Arguments:\n        model (DetectionModelHelper): the detection model to use\n        im (ndarray): BGR image to test\n        boxes (ndarray): R x 4 array of bounding boxes\n        im_scale (list): image blob scales as returned by im_detect_bbox\n        blob_conv (Tensor): base features from the backbone network.\n\n    Returns:\n        masks (ndarray): R x K x M x M array of class specific soft masks\n    """"""\n    assert not cfg.TEST.MASK_AUG.SCALE_SIZE_DEP, \\\n        \'Size dependent scaling not implemented\'\n\n    # Collect masks computed under different transformations\n    masks_ts = []\n\n    # Compute masks for the original image (identity transform)\n    masks_i = im_detect_mask(model, im_scale, boxes, blob_conv)\n    masks_ts.append(masks_i)\n\n    # Perform mask detection on the horizontally flipped image\n    if cfg.TEST.MASK_AUG.H_FLIP:\n        masks_hf = im_detect_mask_hflip(\n            model, im, cfg.TEST.SCALE, cfg.TEST.MAX_SIZE, boxes\n        )\n        masks_ts.append(masks_hf)\n\n    # Compute detections at different scales\n    for scale in cfg.TEST.MASK_AUG.SCALES:\n        max_size = cfg.TEST.MASK_AUG.MAX_SIZE\n        masks_scl = im_detect_mask_scale(model, im, scale, max_size, boxes)\n        masks_ts.append(masks_scl)\n\n        if cfg.TEST.MASK_AUG.SCALE_H_FLIP:\n            masks_scl_hf = im_detect_mask_scale(\n                model, im, scale, max_size, boxes, hflip=True\n            )\n            masks_ts.append(masks_scl_hf)\n\n    # Compute masks at different aspect ratios\n    for aspect_ratio in cfg.TEST.MASK_AUG.ASPECT_RATIOS:\n        masks_ar = im_detect_mask_aspect_ratio(model, im, aspect_ratio, boxes)\n        masks_ts.append(masks_ar)\n\n        if cfg.TEST.MASK_AUG.ASPECT_RATIO_H_FLIP:\n            masks_ar_hf = im_detect_mask_aspect_ratio(\n                model, im, aspect_ratio, boxes, hflip=True\n            )\n            masks_ts.append(masks_ar_hf)\n\n    # Combine the predicted soft masks\n    if cfg.TEST.MASK_AUG.HEUR == \'SOFT_AVG\':\n        masks_c = np.mean(masks_ts, axis=0)\n    elif cfg.TEST.MASK_AUG.HEUR == \'SOFT_MAX\':\n        masks_c = np.amax(masks_ts, axis=0)\n    elif cfg.TEST.MASK_AUG.HEUR == \'LOGIT_AVG\':\n\n        def logit(y):\n            return -1.0 * np.log((1.0 - y) / np.maximum(y, 1e-20))\n\n        logit_masks = [logit(y) for y in masks_ts]\n        logit_masks = np.mean(logit_masks, axis=0)\n        masks_c = 1.0 / (1.0 + np.exp(-logit_masks))\n    else:\n        raise NotImplementedError(\n            \'Heuristic {} not supported\'.format(cfg.TEST.MASK_AUG.HEUR)\n        )\n\n    return masks_c\n\n\ndef im_detect_mask_hflip(model, im, target_scale, target_max_size, boxes):\n    """"""Performs mask detection on the horizontally flipped image.\n    Function signature is the same as for im_detect_mask_aug.\n    """"""\n    # Compute the masks for the flipped image\n    im_hf = im[:, ::-1, :]\n    boxes_hf = box_utils.flip_boxes(boxes, im.shape[1])\n\n    blob_conv, im_scale = im_conv_body_only(model, im_hf, target_scale, target_max_size)\n    masks_hf = im_detect_mask(model, im_scale, boxes_hf, blob_conv)\n\n    # Invert the predicted soft masks\n    masks_inv = masks_hf[:, :, :, ::-1]\n\n    return masks_inv\n\n\ndef im_detect_mask_scale(\n        model, im, target_scale, target_max_size, boxes, hflip=False):\n    """"""Computes masks at the given scale.""""""\n    if hflip:\n        masks_scl = im_detect_mask_hflip(\n            model, im, target_scale, target_max_size, boxes\n        )\n    else:\n        blob_conv, im_scale = im_conv_body_only(model, im, target_scale, target_max_size)\n        masks_scl = im_detect_mask(model, im_scale, boxes, blob_conv)\n    return masks_scl\n\n\ndef im_detect_mask_aspect_ratio(model, im, aspect_ratio, boxes, hflip=False):\n    """"""Computes mask detections at the given width-relative aspect ratio.""""""\n\n    # Perform mask detection on the transformed image\n    im_ar = image_utils.aspect_ratio_rel(im, aspect_ratio)\n    boxes_ar = box_utils.aspect_ratio(boxes, aspect_ratio)\n\n    if hflip:\n        masks_ar = im_detect_mask_hflip(\n            model, im_ar, cfg.TEST.SCALE, cfg.TEST.MAX_SIZE, boxes_ar\n        )\n    else:\n        blob_conv, im_scale = im_conv_body_only(\n            model, im_ar, cfg.TEST.SCALE, cfg.TEST.MAX_SIZE\n        )\n        masks_ar = im_detect_mask(model, im_scale, boxes_ar, blob_conv)\n\n    return masks_ar\n\n\ndef im_detect_keypoints(model, im_scale, boxes, blob_conv):\n    """"""Infer instance keypoint poses. This function must be called after\n    im_detect_bbox as it assumes that the Caffe2 workspace is already populated\n    with the necessary blobs.\n\n    Arguments:\n        model (DetectionModelHelper): the detection model to use\n        im_scale (list): image blob scales as returned by im_detect_bbox\n        boxes (ndarray): R x 4 array of bounding box detections (e.g., as\n            returned by im_detect_bbox)\n\n    Returns:\n        pred_heatmaps (ndarray): R x J x M x M array of keypoint location\n            logits (softmax inputs) for each of the J keypoint types output\n            by the network (must be processed by keypoint_results to convert\n            into point predictions in the original image coordinate space)\n    """"""\n    M = cfg.KRCNN.HEATMAP_SIZE\n    if boxes.shape[0] == 0:\n        pred_heatmaps = np.zeros((0, cfg.KRCNN.NUM_KEYPOINTS, M, M), np.float32)\n        return pred_heatmaps\n\n    inputs = {\'keypoint_rois\': _get_rois_blob(boxes, im_scale)}\n\n    # Add multi-level rois for FPN\n    if cfg.FPN.MULTILEVEL_ROIS:\n        _add_multilevel_rois_for_test(inputs, \'keypoint_rois\')\n\n    pred_heatmaps = model.module.keypoint_net(blob_conv, inputs)\n    pred_heatmaps = pred_heatmaps.data.cpu().numpy().squeeze()\n\n    # In case of 1\n    if pred_heatmaps.ndim == 3:\n        pred_heatmaps = np.expand_dims(pred_heatmaps, axis=0)\n\n    return pred_heatmaps\n\n\ndef im_detect_keypoints_aug(model, im, boxes, im_scale, blob_conv):\n    """"""Computes keypoint predictions with test-time augmentations.\n\n    Arguments:\n        model (DetectionModelHelper): the detection model to use\n        im (ndarray): BGR image to test\n        boxes (ndarray): R x 4 array of bounding boxes\n        im_scale (list): image blob scales as returned by im_detect_bbox\n        blob_conv (Tensor): base features from the backbone network.\n\n    Returns:\n        heatmaps (ndarray): R x J x M x M array of keypoint location logits\n    """"""\n    # Collect heatmaps predicted under different transformations\n    heatmaps_ts = []\n    # Tag predictions computed under downscaling and upscaling transformations\n    ds_ts = []\n    us_ts = []\n\n    def add_heatmaps_t(heatmaps_t, ds_t=False, us_t=False):\n        heatmaps_ts.append(heatmaps_t)\n        ds_ts.append(ds_t)\n        us_ts.append(us_t)\n\n    # Compute the heatmaps for the original image (identity transform)\n    heatmaps_i = im_detect_keypoints(model, im_scale, boxes, blob_conv)\n    add_heatmaps_t(heatmaps_i)\n\n    # Perform keypoints detection on the horizontally flipped image\n    if cfg.TEST.KPS_AUG.H_FLIP:\n        heatmaps_hf = im_detect_keypoints_hflip(\n            model, im, cfg.TEST.SCALE, cfg.TEST.MAX_SIZE, boxes\n        )\n        add_heatmaps_t(heatmaps_hf)\n\n    # Compute detections at different scales\n    for scale in cfg.TEST.KPS_AUG.SCALES:\n        ds_scl = scale < cfg.TEST.SCALE\n        us_scl = scale > cfg.TEST.SCALE\n        heatmaps_scl = im_detect_keypoints_scale(\n            model, im, scale, cfg.TEST.KPS_AUG.MAX_SIZE, boxes\n        )\n        add_heatmaps_t(heatmaps_scl, ds_scl, us_scl)\n\n        if cfg.TEST.KPS_AUG.SCALE_H_FLIP:\n            heatmaps_scl_hf = im_detect_keypoints_scale(\n                model, im, scale, cfg.TEST.KPS_AUG.MAX_SIZE, boxes, hflip=True\n            )\n            add_heatmaps_t(heatmaps_scl_hf, ds_scl, us_scl)\n\n    # Compute keypoints at different aspect ratios\n    for aspect_ratio in cfg.TEST.KPS_AUG.ASPECT_RATIOS:\n        heatmaps_ar = im_detect_keypoints_aspect_ratio(\n            model, im, aspect_ratio, boxes\n        )\n        add_heatmaps_t(heatmaps_ar)\n\n        if cfg.TEST.KPS_AUG.ASPECT_RATIO_H_FLIP:\n            heatmaps_ar_hf = im_detect_keypoints_aspect_ratio(\n                model, im, aspect_ratio, boxes, hflip=True\n            )\n            add_heatmaps_t(heatmaps_ar_hf)\n\n    # Select the heuristic function for combining the heatmaps\n    if cfg.TEST.KPS_AUG.HEUR == \'HM_AVG\':\n        np_f = np.mean\n    elif cfg.TEST.KPS_AUG.HEUR == \'HM_MAX\':\n        np_f = np.amax\n    else:\n        raise NotImplementedError(\n            \'Heuristic {} not supported\'.format(cfg.TEST.KPS_AUG.HEUR)\n        )\n\n    def heur_f(hms_ts):\n        return np_f(hms_ts, axis=0)\n\n    # Combine the heatmaps\n    if cfg.TEST.KPS_AUG.SCALE_SIZE_DEP:\n        heatmaps_c = combine_heatmaps_size_dep(\n            heatmaps_ts, ds_ts, us_ts, boxes, heur_f\n        )\n    else:\n        heatmaps_c = heur_f(heatmaps_ts)\n\n    return heatmaps_c\n\n\ndef im_detect_keypoints_hflip(model, im, target_scale, target_max_size, boxes):\n    """"""Computes keypoint predictions on the horizontally flipped image.\n    Function signature is the same as for im_detect_keypoints_aug.\n    """"""\n    # Compute keypoints for the flipped image\n    im_hf = im[:, ::-1, :]\n    boxes_hf = box_utils.flip_boxes(boxes, im.shape[1])\n\n    blob_conv, im_scale = im_conv_body_only(model, im_hf, target_scale, target_max_size)\n    heatmaps_hf = im_detect_keypoints(model, im_scale, boxes_hf, blob_conv)\n\n    # Invert the predicted keypoints\n    heatmaps_inv = keypoint_utils.flip_heatmaps(heatmaps_hf)\n\n    return heatmaps_inv\n\n\ndef im_detect_keypoints_scale(\n    model, im, target_scale, target_max_size, boxes, hflip=False):\n    """"""Computes keypoint predictions at the given scale.""""""\n    if hflip:\n        heatmaps_scl = im_detect_keypoints_hflip(\n            model, im, target_scale, target_max_size, boxes\n        )\n    else:\n        blob_conv, im_scale = im_conv_body_only(model, im, target_scale, target_max_size)\n        heatmaps_scl = im_detect_keypoints(model, im_scale, boxes, blob_conv)\n    return heatmaps_scl\n\n\ndef im_detect_keypoints_aspect_ratio(\n    model, im, aspect_ratio, boxes, hflip=False):\n    """"""Detects keypoints at the given width-relative aspect ratio.""""""\n\n    # Perform keypoint detectionon the transformed image\n    im_ar = image_utils.aspect_ratio_rel(im, aspect_ratio)\n    boxes_ar = box_utils.aspect_ratio(boxes, aspect_ratio)\n\n    if hflip:\n        heatmaps_ar = im_detect_keypoints_hflip(\n            model, im_ar, cfg.TEST.SCALE, cfg.TEST.MAX_SIZE, boxes_ar\n        )\n    else:\n        blob_conv, im_scale = im_conv_body_only(\n            model, im_ar, cfg.TEST.SCALE, cfg.TEST.MAX_SIZE\n        )\n        heatmaps_ar = im_detect_keypoints(model, im_scale, boxes_ar, blob_conv)\n\n    return heatmaps_ar\n\n\ndef combine_heatmaps_size_dep(hms_ts, ds_ts, us_ts, boxes, heur_f):\n    """"""Combines heatmaps while taking object sizes into account.""""""\n    assert len(hms_ts) == len(ds_ts) and len(ds_ts) == len(us_ts), \\\n        \'All sets of hms must be tagged with downscaling and upscaling flags\'\n\n    # Classify objects into small+medium and large based on their box areas\n    areas = box_utils.boxes_area(boxes)\n    sm_objs = areas < cfg.TEST.KPS_AUG.AREA_TH\n    l_objs = areas >= cfg.TEST.KPS_AUG.AREA_TH\n\n    # Combine heatmaps computed under different transformations for each object\n    hms_c = np.zeros_like(hms_ts[0])\n\n    for i in range(hms_c.shape[0]):\n        hms_to_combine = []\n        for hms_t, ds_t, us_t in zip(hms_ts, ds_ts, us_ts):\n            # Discard downscaling predictions for small and medium objects\n            if sm_objs[i] and ds_t:\n                continue\n            # Discard upscaling predictions for large objects\n            if l_objs[i] and us_t:\n                continue\n            hms_to_combine.append(hms_t[i])\n        hms_c[i] = heur_f(hms_to_combine)\n\n    return hms_c\n\n\ndef box_results_with_nms_and_limit(scores, boxes):  # NOTE: support single-batch\n    """"""Returns bounding-box detection results by thresholding on scores and\n    applying non-maximum suppression (NMS).\n\n    `boxes` has shape (#detections, 4 * #classes), where each row represents\n    a list of predicted bounding boxes for each of the object classes in the\n    dataset (including the background class). The detections in each row\n    originate from the same object proposal.\n\n    `scores` has shape (#detection, #classes), where each row represents a list\n    of object detection confidence scores for each of the object classes in the\n    dataset (including the background class). `scores[i, j]`` corresponds to the\n    box at `boxes[i, j * 4:(j + 1) * 4]`.\n    """"""\n    num_classes = cfg.MODEL.NUM_CLASSES\n    cls_boxes = [[] for _ in range(num_classes)]\n    # Apply threshold on detection probabilities and apply NMS\n    # Skip j = 0, because it\'s the background class\n    for j in range(1, num_classes):\n        inds = np.where(scores[:, j] > cfg.TEST.SCORE_THRESH)[0]\n        scores_j = scores[inds, j]\n        boxes_j = boxes[inds, j * 4:(j + 1) * 4]\n        dets_j = np.hstack((boxes_j, scores_j[:, np.newaxis])).astype(np.float32, copy=False)\n        if cfg.TEST.SOFT_NMS.ENABLED:\n            nms_dets, _ = box_utils.soft_nms(\n                dets_j,\n                sigma=cfg.TEST.SOFT_NMS.SIGMA,\n                overlap_thresh=cfg.TEST.NMS,\n                score_thresh=0.0001,\n                method=cfg.TEST.SOFT_NMS.METHOD\n            )\n        else:\n            keep = box_utils.nms(dets_j, cfg.TEST.NMS)\n            nms_dets = dets_j[keep, :]\n        # Refine the post-NMS boxes using bounding-box voting\n        if cfg.TEST.BBOX_VOTE.ENABLED:\n            nms_dets = box_utils.box_voting(\n                nms_dets,\n                dets_j,\n                cfg.TEST.BBOX_VOTE.VOTE_TH,\n                scoring_method=cfg.TEST.BBOX_VOTE.SCORING_METHOD\n            )\n        cls_boxes[j] = nms_dets\n\n    # Limit to max_per_image detections **over all classes**\n    if cfg.TEST.DETECTIONS_PER_IM > 0:\n        image_scores = np.hstack(\n            [cls_boxes[j][:, -1] for j in range(1, num_classes)]\n        )\n        if len(image_scores) > cfg.TEST.DETECTIONS_PER_IM:\n            image_thresh = np.sort(image_scores)[-cfg.TEST.DETECTIONS_PER_IM]\n            for j in range(1, num_classes):\n                keep = np.where(cls_boxes[j][:, -1] >= image_thresh)[0]\n                cls_boxes[j] = cls_boxes[j][keep, :]\n\n    im_results = np.vstack([cls_boxes[j] for j in range(1, num_classes)])\n    boxes = im_results[:, :-1]\n    scores = im_results[:, -1]\n    return scores, boxes, cls_boxes\n\n\ndef segm_results(cls_boxes, masks, ref_boxes, im_h, im_w):\n    num_classes = cfg.MODEL.NUM_CLASSES\n    cls_segms = [[] for _ in range(num_classes)]\n    mask_ind = 0\n    # To work around an issue with cv2.resize (it seems to automatically pad\n    # with repeated border values), we manually zero-pad the masks by 1 pixel\n    # prior to resizing back to the original image resolution. This prevents\n    # ""top hat"" artifacts. We therefore need to expand the reference boxes by an\n    # appropriate factor.\n    M = cfg.MRCNN.RESOLUTION\n    scale = (M + 2.0) / M\n    ref_boxes = box_utils.expand_boxes(ref_boxes, scale)\n    ref_boxes = ref_boxes.astype(np.int32)\n    padded_mask = np.zeros((M + 2, M + 2), dtype=np.float32)\n\n    # skip j = 0, because it\'s the background class\n    for j in range(1, num_classes):\n        segms = []\n        for _ in range(cls_boxes[j].shape[0]):\n            if cfg.MRCNN.CLS_SPECIFIC_MASK:\n                padded_mask[1:-1, 1:-1] = masks[mask_ind, j, :, :]\n            else:\n                padded_mask[1:-1, 1:-1] = masks[mask_ind, 0, :, :]\n\n            ref_box = ref_boxes[mask_ind, :]\n            w = (ref_box[2] - ref_box[0] + 1)\n            h = (ref_box[3] - ref_box[1] + 1)\n            w = np.maximum(w, 1)\n            h = np.maximum(h, 1)\n\n            mask = cv2.resize(padded_mask, (w, h))\n            mask = np.array(mask > cfg.MRCNN.THRESH_BINARIZE, dtype=np.uint8)\n            im_mask = np.zeros((im_h, im_w), dtype=np.uint8)\n\n            x_0 = max(ref_box[0], 0)\n            x_1 = min(ref_box[2] + 1, im_w)\n            y_0 = max(ref_box[1], 0)\n            y_1 = min(ref_box[3] + 1, im_h)\n\n            im_mask[y_0:y_1, x_0:x_1] = mask[\n                (y_0 - ref_box[1]):(y_1 - ref_box[1]), (x_0 - ref_box[0]):(x_1 - ref_box[0])]\n\n            # Get RLE encoding used by the COCO evaluation API\n            rle = mask_util.encode(np.array(im_mask[:, :, np.newaxis], order=\'F\'))[0]\n            # For dumping to json, need to decode the byte string.\n            # https://github.com/cocodataset/cocoapi/issues/70\n            rle[\'counts\'] = rle[\'counts\'].decode(\'ascii\')\n            segms.append(rle)\n\n            mask_ind += 1\n\n        cls_segms[j] = segms\n\n    assert mask_ind == masks.shape[0]\n    return cls_segms\n\n\ndef keypoint_results(cls_boxes, pred_heatmaps, ref_boxes):\n    num_classes = cfg.MODEL.NUM_CLASSES\n    cls_keyps = [[] for _ in range(num_classes)]\n    person_idx = keypoint_utils.get_person_class_index()\n    xy_preds = keypoint_utils.heatmaps_to_keypoints(pred_heatmaps, ref_boxes)\n\n    # NMS OKS\n    if cfg.KRCNN.NMS_OKS:\n        keep = keypoint_utils.nms_oks(xy_preds, ref_boxes, 0.3)\n        xy_preds = xy_preds[keep, :, :]\n        ref_boxes = ref_boxes[keep, :]\n        pred_heatmaps = pred_heatmaps[keep, :, :, :]\n        cls_boxes[person_idx] = cls_boxes[person_idx][keep, :]\n\n    kps = [xy_preds[i] for i in range(xy_preds.shape[0])]\n    cls_keyps[person_idx] = kps\n    return cls_keyps\n\n\ndef _get_rois_blob(im_rois, im_scale):\n    """"""Converts RoIs into network inputs.\n\n    Arguments:\n        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n        im_scale_factors (list): scale factors as returned by _get_image_blob\n\n    Returns:\n        blob (ndarray): R x 5 matrix of RoIs in the image pyramid with columns\n            [level, x1, y1, x2, y2]\n    """"""\n    rois, levels = _project_im_rois(im_rois, im_scale)\n    rois_blob = np.hstack((levels, rois))\n    return rois_blob.astype(np.float32, copy=False)\n\n\ndef _project_im_rois(im_rois, scales):\n    """"""Project image RoIs into the image pyramid built by _get_image_blob.\n\n    Arguments:\n        im_rois (ndarray): R x 4 matrix of RoIs in original image coordinates\n        scales (list): scale factors as returned by _get_image_blob\n\n    Returns:\n        rois (ndarray): R x 4 matrix of projected RoI coordinates\n        levels (ndarray): image pyramid levels used by each projected RoI\n    """"""\n    rois = im_rois.astype(np.float, copy=False) * scales\n    levels = np.zeros((im_rois.shape[0], 1), dtype=np.int)\n    return rois, levels\n\n\ndef _add_multilevel_rois_for_test(blobs, name):\n    """"""Distributes a set of RoIs across FPN pyramid levels by creating new level\n    specific RoI blobs.\n\n    Arguments:\n        blobs (dict): dictionary of blobs\n        name (str): a key in \'blobs\' identifying the source RoI blob\n\n    Returns:\n        [by ref] blobs (dict): new keys named by `name + \'fpn\' + level`\n            are added to dict each with a value that\'s an R_level x 5 ndarray of\n            RoIs (see _get_rois_blob for format)\n    """"""\n    lvl_min = cfg.FPN.ROI_MIN_LEVEL\n    lvl_max = cfg.FPN.ROI_MAX_LEVEL\n    lvls = fpn_utils.map_rois_to_fpn_levels(blobs[name][:, 1:5], lvl_min, lvl_max)\n    fpn_utils.add_multilevel_roi_blobs(\n        blobs, name, blobs[name], lvls, lvl_min, lvl_max\n    )\n\n\ndef _get_blobs(im, rois, target_scale, target_max_size):\n    """"""Convert an image and RoIs within that image into network inputs.""""""\n    blobs = {}\n    blobs[\'data\'], im_scale, blobs[\'im_info\'] = \\\n        blob_utils.get_image_blob(im, target_scale, target_max_size)\n    if rois is not None:\n        blobs[\'rois\'] = _get_rois_blob(rois, im_scale)\n    return blobs, im_scale\n'"
lib/core/test_engine.py,1,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Test a Detectron network on an imdb (image database).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import defaultdict\nimport cv2\nimport datetime\nimport logging\nimport numpy as np\nimport os\nimport yaml\n\nimport torch\n\nfrom core.config import cfg\n# from core.rpn_generator import generate_rpn_on_dataset  #TODO: for rpn only case\n# from core.rpn_generator import generate_rpn_on_range\nfrom core.test import im_detect_all\nfrom datasets import task_evaluation\nfrom datasets.json_dataset import JsonDataset\nfrom modeling import model_builder\nimport nn as mynn\nfrom utils.detectron_weight_helper import load_detectron_weight\nimport utils.env as envu\nimport utils.net as net_utils\nimport utils.subprocess as subprocess_utils\nimport utils.vis as vis_utils\nfrom utils.io import save_object\nfrom utils.timer import Timer\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_eval_functions():\n    # Determine which parent or child function should handle inference\n    if cfg.MODEL.RPN_ONLY:\n        raise NotImplementedError\n        # child_func = generate_rpn_on_range\n        # parent_func = generate_rpn_on_dataset\n    else:\n        # Generic case that handles all network types other than RPN-only nets\n        # and RetinaNet\n        child_func = test_net\n        parent_func = test_net_on_dataset\n\n    return parent_func, child_func\n\n\ndef get_inference_dataset(index, is_parent=True):\n    assert is_parent or len(cfg.TEST.DATASETS) == 1, \\\n        \'The child inference process can only work on a single dataset\'\n\n    dataset_name = cfg.TEST.DATASETS[index]\n\n    if cfg.TEST.PRECOMPUTED_PROPOSALS:\n        assert is_parent or len(cfg.TEST.PROPOSAL_FILES) == 1, \\\n            \'The child inference process can only work on a single proposal file\'\n        assert len(cfg.TEST.PROPOSAL_FILES) == len(cfg.TEST.DATASETS), \\\n            \'If proposals are used, one proposal file must be specified for \' \\\n            \'each dataset\'\n        proposal_file = cfg.TEST.PROPOSAL_FILES[index]\n    else:\n        proposal_file = None\n\n    return dataset_name, proposal_file\n\n\ndef run_inference(\n        args, ind_range=None,\n        multi_gpu_testing=False, gpu_id=0,\n        check_expected_results=False):\n    parent_func, child_func = get_eval_functions()\n    is_parent = ind_range is None\n\n    def result_getter():\n        if is_parent:\n            # Parent case:\n            # In this case we\'re either running inference on the entire dataset in a\n            # single process or (if multi_gpu_testing is True) using this process to\n            # launch subprocesses that each run inference on a range of the dataset\n            all_results = {}\n            for i in range(len(cfg.TEST.DATASETS)):\n                dataset_name, proposal_file = get_inference_dataset(i)\n                output_dir = args.output_dir\n                results = parent_func(\n                    args,\n                    dataset_name,\n                    proposal_file,\n                    output_dir,\n                    multi_gpu=multi_gpu_testing\n                )\n                all_results.update(results)\n\n            return all_results\n        else:\n            # Subprocess child case:\n            # In this case test_net was called via subprocess.Popen to execute on a\n            # range of inputs on a single dataset\n            dataset_name, proposal_file = get_inference_dataset(0, is_parent=False)\n            output_dir = args.output_dir\n            return child_func(\n                args,\n                dataset_name,\n                proposal_file,\n                output_dir,\n                ind_range=ind_range,\n                gpu_id=gpu_id\n            )\n\n    all_results = result_getter()\n    if check_expected_results and is_parent:\n        task_evaluation.check_expected_results(\n            all_results,\n            atol=cfg.EXPECTED_RESULTS_ATOL,\n            rtol=cfg.EXPECTED_RESULTS_RTOL\n        )\n        task_evaluation.log_copy_paste_friendly_results(all_results)\n\n    return all_results\n\n\ndef test_net_on_dataset(\n        args,\n        dataset_name,\n        proposal_file,\n        output_dir,\n        multi_gpu=False,\n        gpu_id=0):\n    """"""Run inference on a dataset.""""""\n    dataset = JsonDataset(dataset_name)\n    test_timer = Timer()\n    test_timer.tic()\n    if multi_gpu:\n        num_images = len(dataset.get_roidb())\n        all_boxes, all_segms, all_keyps = multi_gpu_test_net_on_dataset(\n            args, dataset_name, proposal_file, num_images, output_dir\n        )\n    else:\n        all_boxes, all_segms, all_keyps = test_net(\n            args, dataset_name, proposal_file, output_dir, gpu_id=gpu_id\n        )\n    test_timer.toc()\n    logger.info(\'Total inference time: {:.3f}s\'.format(test_timer.average_time))\n    results = task_evaluation.evaluate_all(\n        dataset, all_boxes, all_segms, all_keyps, output_dir\n    )\n    return results\n\n\ndef multi_gpu_test_net_on_dataset(\n        args, dataset_name, proposal_file, num_images, output_dir):\n    """"""Multi-gpu inference on a dataset.""""""\n    binary_dir = envu.get_runtime_dir()\n    binary_ext = envu.get_py_bin_ext()\n    binary = os.path.join(binary_dir, args.test_net_file + binary_ext)\n    assert os.path.exists(binary), \'Binary \\\'{}\\\' not found\'.format(binary)\n\n    # Pass the target dataset and proposal file (if any) via the command line\n    opts = [\'TEST.DATASETS\', \'(""{}"",)\'.format(dataset_name)]\n    if proposal_file:\n        opts += [\'TEST.PROPOSAL_FILES\', \'(""{}"",)\'.format(proposal_file)]\n\n    # Run inference in parallel in subprocesses\n    # Outputs will be a list of outputs from each subprocess, where the output\n    # of each subprocess is the dictionary saved by test_net().\n    outputs = subprocess_utils.process_in_parallel(\n        \'detection\', num_images, binary, output_dir,\n        args.load_ckpt, args.load_detectron, opts\n    )\n\n    # Collate the results from each subprocess\n    all_boxes = [[] for _ in range(cfg.MODEL.NUM_CLASSES)]\n    all_segms = [[] for _ in range(cfg.MODEL.NUM_CLASSES)]\n    all_keyps = [[] for _ in range(cfg.MODEL.NUM_CLASSES)]\n    for det_data in outputs:\n        all_boxes_batch = det_data[\'all_boxes\']\n        all_segms_batch = det_data[\'all_segms\']\n        all_keyps_batch = det_data[\'all_keyps\']\n        for cls_idx in range(1, cfg.MODEL.NUM_CLASSES):\n            all_boxes[cls_idx] += all_boxes_batch[cls_idx]\n            all_segms[cls_idx] += all_segms_batch[cls_idx]\n            all_keyps[cls_idx] += all_keyps_batch[cls_idx]\n    det_file = os.path.join(output_dir, \'detections.pkl\')\n    cfg_yaml = yaml.dump(cfg)\n    save_object(\n        dict(\n            all_boxes=all_boxes,\n            all_segms=all_segms,\n            all_keyps=all_keyps,\n            cfg=cfg_yaml\n        ), det_file\n    )\n    logger.info(\'Wrote detections to: {}\'.format(os.path.abspath(det_file)))\n\n    return all_boxes, all_segms, all_keyps\n\n\ndef test_net(\n        args,\n        dataset_name,\n        proposal_file,\n        output_dir,\n        ind_range=None,\n        gpu_id=0):\n    """"""Run inference on all images in a dataset or over an index range of images\n    in a dataset using a single GPU.\n    """"""\n    assert not cfg.MODEL.RPN_ONLY, \\\n        \'Use rpn_generate to generate proposals from RPN-only models\'\n\n    roidb, dataset, start_ind, end_ind, total_num_images = get_roidb_and_dataset(\n        dataset_name, proposal_file, ind_range\n    )\n    model = initialize_model_from_cfg(args, gpu_id=gpu_id)\n    num_images = len(roidb)\n    num_classes = cfg.MODEL.NUM_CLASSES\n    all_boxes, all_segms, all_keyps = empty_results(num_classes, num_images)\n    timers = defaultdict(Timer)\n    for i, entry in enumerate(roidb):\n        if cfg.TEST.PRECOMPUTED_PROPOSALS:\n            # The roidb may contain ground-truth rois (for example, if the roidb\n            # comes from the training or val split). We only want to evaluate\n            # detection on the *non*-ground-truth rois. We select only the rois\n            # that have the gt_classes field set to 0, which means there\'s no\n            # ground truth.\n            box_proposals = entry[\'boxes\'][entry[\'gt_classes\'] == 0]\n            if len(box_proposals) == 0:\n                continue\n        else:\n            # Faster R-CNN type models generate proposals on-the-fly with an\n            # in-network RPN; 1-stage models don\'t require proposals.\n            box_proposals = None\n\n        im = cv2.imread(entry[\'image\'])\n        cls_boxes_i, cls_segms_i, cls_keyps_i = im_detect_all(model, im, box_proposals, timers)\n\n        extend_results(i, all_boxes, cls_boxes_i)\n        if cls_segms_i is not None:\n            extend_results(i, all_segms, cls_segms_i)\n        if cls_keyps_i is not None:\n            extend_results(i, all_keyps, cls_keyps_i)\n\n        if i % 10 == 0:  # Reduce log file size\n            ave_total_time = np.sum([t.average_time for t in timers.values()])\n            eta_seconds = ave_total_time * (num_images - i - 1)\n            eta = str(datetime.timedelta(seconds=int(eta_seconds)))\n            det_time = (\n                timers[\'im_detect_bbox\'].average_time +\n                timers[\'im_detect_mask\'].average_time +\n                timers[\'im_detect_keypoints\'].average_time\n            )\n            misc_time = (\n                timers[\'misc_bbox\'].average_time +\n                timers[\'misc_mask\'].average_time +\n                timers[\'misc_keypoints\'].average_time\n            )\n            logger.info(\n                (\n                    \'im_detect: range [{:d}, {:d}] of {:d}: \'\n                    \'{:d}/{:d} {:.3f}s + {:.3f}s (eta: {})\'\n                ).format(\n                    start_ind + 1, end_ind, total_num_images, start_ind + i + 1,\n                    start_ind + num_images, det_time, misc_time, eta\n                )\n            )\n\n        if cfg.VIS:\n            im_name = os.path.splitext(os.path.basename(entry[\'image\']))[0]\n            vis_utils.vis_one_image(\n                im[:, :, ::-1],\n                \'{:d}_{:s}\'.format(i, im_name),\n                os.path.join(output_dir, \'vis\'),\n                cls_boxes_i,\n                segms=cls_segms_i,\n                keypoints=cls_keyps_i,\n                thresh=cfg.VIS_TH,\n                box_alpha=0.8,\n                dataset=dataset,\n                show_class=True\n            )\n\n    cfg_yaml = yaml.dump(cfg)\n    if ind_range is not None:\n        det_name = \'detection_range_%s_%s.pkl\' % tuple(ind_range)\n    else:\n        det_name = \'detections.pkl\'\n    det_file = os.path.join(output_dir, det_name)\n    save_object(\n        dict(\n            all_boxes=all_boxes,\n            all_segms=all_segms,\n            all_keyps=all_keyps,\n            cfg=cfg_yaml\n        ), det_file\n    )\n    logger.info(\'Wrote detections to: {}\'.format(os.path.abspath(det_file)))\n    return all_boxes, all_segms, all_keyps\n\n\ndef initialize_model_from_cfg(args, gpu_id=0):\n    """"""Initialize a model from the global cfg. Loads test-time weights and\n    set to evaluation mode.\n    """"""\n    model = model_builder.Generalized_RCNN()\n    model.eval()\n\n    if args.cuda:\n        model.cuda()\n\n    if args.load_ckpt:\n        load_name = args.load_ckpt\n        logger.info(""loading checkpoint %s"", load_name)\n        checkpoint = torch.load(load_name, map_location=lambda storage, loc: storage)\n        net_utils.load_ckpt(model, checkpoint[\'model\'])\n\n    if args.load_detectron:\n        logger.info(""loading detectron weights %s"", args.load_detectron)\n        load_detectron_weight(model, args.load_detectron)\n\n    model = mynn.DataParallel(model, cpu_keywords=[\'im_info\', \'roidb\'], minibatch=True)\n\n    return model\n\n\ndef get_roidb_and_dataset(dataset_name, proposal_file, ind_range):\n    """"""Get the roidb for the dataset specified in the global cfg. Optionally\n    restrict it to a range of indices if ind_range is a pair of integers.\n    """"""\n    dataset = JsonDataset(dataset_name)\n    if cfg.TEST.PRECOMPUTED_PROPOSALS:\n        assert proposal_file, \'No proposal file given\'\n        roidb = dataset.get_roidb(\n            proposal_file=proposal_file,\n            proposal_limit=cfg.TEST.PROPOSAL_LIMIT\n        )\n    else:\n        roidb = dataset.get_roidb()\n\n    if ind_range is not None:\n        total_num_images = len(roidb)\n        start, end = ind_range\n        roidb = roidb[start:end]\n    else:\n        start = 0\n        end = len(roidb)\n        total_num_images = end\n\n    return roidb, dataset, start, end, total_num_images\n\n\ndef empty_results(num_classes, num_images):\n    """"""Return empty results lists for boxes, masks, and keypoints.\n    Box detections are collected into:\n      all_boxes[cls][image] = N x 5 array with columns (x1, y1, x2, y2, score)\n    Instance mask predictions are collected into:\n      all_segms[cls][image] = [...] list of COCO RLE encoded masks that are in\n      1:1 correspondence with the boxes in all_boxes[cls][image]\n    Keypoint predictions are collected into:\n      all_keyps[cls][image] = [...] list of keypoints results, each encoded as\n      a 3D array (#rois, 4, #keypoints) with the 4 rows corresponding to\n      [x, y, logit, prob] (See: utils.keypoints.heatmaps_to_keypoints).\n      Keypoints are recorded for person (cls = 1); they are in 1:1\n      correspondence with the boxes in all_boxes[cls][image].\n    """"""\n    # Note: do not be tempted to use [[] * N], which gives N references to the\n    # *same* empty list.\n    all_boxes = [[[] for _ in range(num_images)] for _ in range(num_classes)]\n    all_segms = [[[] for _ in range(num_images)] for _ in range(num_classes)]\n    all_keyps = [[[] for _ in range(num_images)] for _ in range(num_classes)]\n    return all_boxes, all_segms, all_keyps\n\n\ndef extend_results(index, all_res, im_res):\n    """"""Add results for an image to the set of all results at the specified\n    index.\n    """"""\n    # Skip cls_idx 0 (__background__)\n    for cls_idx in range(1, len(im_res)):\n        all_res[cls_idx][index] = im_res[cls_idx]\n'"
lib/datasets/__init__.py,0,b''
lib/datasets/cityscapes_json_dataset_evaluator.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Functions for evaluating results on Cityscapes.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport cv2\nimport logging\nimport os\nimport uuid\n\nimport pycocotools.mask as mask_util\n\nfrom core.config import cfg\nfrom datasets.dataset_catalog import DATASETS\nfrom datasets.dataset_catalog import RAW_DIR\n\nlogger = logging.getLogger(__name__)\n\n\ndef evaluate_masks(\n    json_dataset,\n    all_boxes,\n    all_segms,\n    output_dir,\n    use_salt=True,\n    cleanup=False\n):\n    if cfg.CLUSTER.ON_CLUSTER:\n        # On the cluster avoid saving these files in the job directory\n        output_dir = \'/tmp\'\n    res_file = os.path.join(\n        output_dir, \'segmentations_\' + json_dataset.name + \'_results\')\n    if use_salt:\n        res_file += \'_{}\'.format(str(uuid.uuid4()))\n    res_file += \'.json\'\n\n    results_dir = os.path.join(output_dir, \'results\')\n    if not os.path.exists(results_dir):\n        os.mkdir(results_dir)\n\n    os.environ[\'CITYSCAPES_DATASET\'] = DATASETS[json_dataset.name][RAW_DIR]\n    os.environ[\'CITYSCAPES_RESULTS\'] = output_dir\n\n    # Load the Cityscapes eval script *after* setting the required env vars,\n    # since the script reads their values into global variables (at load time).\n    import cityscapesscripts.evaluation.evalInstanceLevelSemanticLabeling \\\n        as cityscapes_eval\n\n    roidb = json_dataset.get_roidb()\n    for i, entry in enumerate(roidb):\n        im_name = entry[\'image\']\n\n        basename = os.path.splitext(os.path.basename(im_name))[0]\n        txtname = os.path.join(output_dir, basename + \'pred.txt\')\n        with open(txtname, \'w\') as fid_txt:\n            if i % 10 == 0:\n                logger.info(\'i: {}: {}\'.format(i, basename))\n            for j in range(1, len(all_segms)):\n                clss = json_dataset.classes[j]\n                clss_id = cityscapes_eval.name2label[clss].id\n                segms = all_segms[j][i]\n                boxes = all_boxes[j][i]\n                if segms == []:\n                    continue\n                masks = mask_util.decode(segms)\n\n                for k in range(boxes.shape[0]):\n                    score = boxes[k, -1]\n                    mask = masks[:, :, k]\n                    pngname = os.path.join(\n                        \'results\',\n                        basename + \'_\' + clss + \'_{}.png\'.format(k))\n                    # write txt\n                    fid_txt.write(\'{} {} {}\\n\'.format(pngname, clss_id, score))\n                    # save mask\n                    cv2.imwrite(os.path.join(output_dir, pngname), mask * 255)\n    logger.info(\'Evaluating...\')\n    cityscapes_eval.main([])\n    return None\n'"
lib/datasets/dataset_catalog.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Collection of available datasets.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\n\nfrom core.config import cfg\n\n# Path to data dir\n_DATA_DIR = cfg.DATA_DIR\n\n# Required dataset entry keys\nIM_DIR = \'image_directory\'\nANN_FN = \'annotation_file\'\n\n# Optional dataset entry keys\nIM_PREFIX = \'image_prefix\'\nDEVKIT_DIR = \'devkit_directory\'\nRAW_DIR = \'raw_dir\'\n\n# Available datasets\nDATASETS = {\n    \'cityscapes_fine_instanceonly_seg_train\': {\n        IM_DIR:\n            _DATA_DIR + \'/cityscapes/images\',\n        ANN_FN:\n            _DATA_DIR + \'/cityscapes/annotations/instancesonly_gtFine_train.json\',\n        RAW_DIR:\n            _DATA_DIR + \'/cityscapes/raw\'\n    },\n    \'cityscapes_fine_instanceonly_seg_val\': {\n        IM_DIR:\n            _DATA_DIR + \'/cityscapes/images\',\n        # use filtered validation as there is an issue converting contours\n        ANN_FN:\n            _DATA_DIR + \'/cityscapes/annotations/instancesonly_filtered_gtFine_val.json\',\n        RAW_DIR:\n            _DATA_DIR + \'/cityscapes/raw\'\n    },\n    \'cityscapes_fine_instanceonly_seg_test\': {\n        IM_DIR:\n            _DATA_DIR + \'/cityscapes/images\',\n        ANN_FN:\n            _DATA_DIR + \'/cityscapes/annotations/instancesonly_gtFine_test.json\',\n        RAW_DIR:\n            _DATA_DIR + \'/cityscapes/raw\'\n    },\n    \'coco_2014_train\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/train2014\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/instances_train2014.json\'\n    },\n    \'coco_2014_val\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/val2014\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/instances_val2014.json\'\n    },\n    \'coco_2014_minival\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/val2014\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/instances_minival2014.json\'\n    },\n    \'coco_2014_valminusminival\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/val2014\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/instances_valminusminival2014.json\'\n    },\n    \'coco_2015_test\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/test2015\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/image_info_test2015.json\'\n    },\n    \'coco_2015_test-dev\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/test2015\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/image_info_test-dev2015.json\'\n    },\n    \'coco_2017_train\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/train2017\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/instances_train2017.json\',\n    },\n    \'coco_2017_val\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/val2017\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/instances_val2017.json\',\n    },\n    \'coco_2017_test\': {  # 2017 test uses 2015 test images\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/test2015\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/image_info_test2017.json\',\n        IM_PREFIX:\n            \'COCO_test2015_\'\n    },\n    \'coco_2017_test-dev\': {  # 2017 test-dev uses 2015 test images\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/test2015\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/image_info_test-dev2017.json\',\n        IM_PREFIX:\n            \'COCO_test2015_\'\n    },\n    \'coco_stuff_train\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/train2014\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/stuff_train.json\'\n    },\n    \'coco_stuff_val\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/val2014\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/stuff_val.json\'\n    },\n    \'keypoints_coco_2014_train\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/train2014\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/person_keypoints_train2014.json\'\n    },\n    \'keypoints_coco_2014_val\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/val2014\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/person_keypoints_val2014.json\'\n    },\n    \'keypoints_coco_2014_minival\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/val2014\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/person_keypoints_minival2014.json\'\n    },\n    \'keypoints_coco_2014_valminusminival\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/val2014\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/person_keypoints_valminusminival2014.json\'\n    },\n    \'keypoints_coco_2015_test\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/test2015\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/image_info_test2015.json\'\n    },\n    \'keypoints_coco_2015_test-dev\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/test2015\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/image_info_test-dev2015.json\'\n    },\n    \'keypoints_coco_2017_train\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/train2017\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/person_keypoints_train2017.json\'\n    },\n    \'keypoints_coco_2017_val\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/val2017\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/person_keypoints_val2017.json\'\n    },\n    \'keypoints_coco_2017_test\': {\n        IM_DIR:\n            _DATA_DIR + \'/coco/images/test2017\',\n        ANN_FN:\n            _DATA_DIR + \'/coco/annotations/image_info_test2017.json\'\n    },\n    \'voc_2007_trainval\': {\n        IM_DIR:\n            _DATA_DIR + \'/VOC2007/JPEGImages\',\n        ANN_FN:\n            _DATA_DIR + \'/VOC2007/annotations/voc_2007_trainval.json\',\n        DEVKIT_DIR:\n            _DATA_DIR + \'/VOC2007/VOCdevkit2007\'\n    },\n    \'voc_2007_test\': {\n        IM_DIR:\n            _DATA_DIR + \'/VOC2007/JPEGImages\',\n        ANN_FN:\n            _DATA_DIR + \'/VOC2007/annotations/voc_2007_test.json\',\n        DEVKIT_DIR:\n            _DATA_DIR + \'/VOC2007/VOCdevkit2007\'\n    },\n    \'voc_2012_trainval\': {\n        IM_DIR:\n            _DATA_DIR + \'/VOC2012/JPEGImages\',\n        ANN_FN:\n            _DATA_DIR + \'/VOC2012/annotations/voc_2012_trainval.json\',\n        DEVKIT_DIR:\n            _DATA_DIR + \'/VOC2012/VOCdevkit2012\'\n    }\n}\n'"
lib/datasets/dummy_datasets.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n""""""Provide stub objects that can act as stand-in ""dummy"" datasets for simple use\ncases, like getting all classes in a dataset. This exists so that demos can be\nrun without requiring users to download/install datasets first.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom utils.collections import AttrDict\n\n\ndef get_coco_dataset():\n    """"""A dummy COCO dataset that includes only the \'classes\' field.""""""\n    ds = AttrDict()\n    classes = [\n        \'__background__\', \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\',\n        \'bus\', \'train\', \'truck\', \'boat\', \'traffic light\', \'fire hydrant\',\n        \'stop sign\', \'parking meter\', \'bench\', \'bird\', \'cat\', \'dog\', \'horse\',\n        \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\', \'backpack\',\n        \'umbrella\', \'handbag\', \'tie\', \'suitcase\', \'frisbee\', \'skis\',\n        \'snowboard\', \'sports ball\', \'kite\', \'baseball bat\', \'baseball glove\',\n        \'skateboard\', \'surfboard\', \'tennis racket\', \'bottle\', \'wine glass\',\n        \'cup\', \'fork\', \'knife\', \'spoon\', \'bowl\', \'banana\', \'apple\', \'sandwich\',\n        \'orange\', \'broccoli\', \'carrot\', \'hot dog\', \'pizza\', \'donut\', \'cake\',\n        \'chair\', \'couch\', \'potted plant\', \'bed\', \'dining table\', \'toilet\', \'tv\',\n        \'laptop\', \'mouse\', \'remote\', \'keyboard\', \'cell phone\', \'microwave\',\n        \'oven\', \'toaster\', \'sink\', \'refrigerator\', \'book\', \'clock\', \'vase\',\n        \'scissors\', \'teddy bear\', \'hair drier\', \'toothbrush\'\n    ]\n    ds.classes = {i: name for i, name in enumerate(classes)}\n    return ds\n'"
lib/datasets/json_dataset.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Representation of the standard COCO json dataset format.\n\nWhen working with a new dataset, we strongly suggest to convert the dataset into\nthe COCO json format and use the existing code; it is not recommended to write\ncode to support new dataset formats.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport copy\nfrom six.moves import cPickle as pickle\nimport logging\nimport numpy as np\nimport os\nimport scipy.sparse\n\n# Must happen before importing COCO API (which imports matplotlib)\nimport utils.env as envu\nenvu.set_up_matplotlib()\n# COCO API\nfrom pycocotools import mask as COCOmask\nfrom pycocotools.coco import COCO\n\nimport utils.boxes as box_utils\nfrom core.config import cfg\nfrom utils.timer import Timer\nfrom .dataset_catalog import ANN_FN\nfrom .dataset_catalog import DATASETS\nfrom .dataset_catalog import IM_DIR\nfrom .dataset_catalog import IM_PREFIX\n\nlogger = logging.getLogger(__name__)\n\n\nclass JsonDataset(object):\n    """"""A class representing a COCO json dataset.""""""\n\n    def __init__(self, name):\n        assert name in DATASETS.keys(), \\\n            \'Unknown dataset name: {}\'.format(name)\n        assert os.path.exists(DATASETS[name][IM_DIR]), \\\n            \'Image directory \\\'{}\\\' not found\'.format(DATASETS[name][IM_DIR])\n        assert os.path.exists(DATASETS[name][ANN_FN]), \\\n            \'Annotation file \\\'{}\\\' not found\'.format(DATASETS[name][ANN_FN])\n        logger.debug(\'Creating: {}\'.format(name))\n        self.name = name\n        self.image_directory = DATASETS[name][IM_DIR]\n        self.image_prefix = (\n            \'\' if IM_PREFIX not in DATASETS[name] else DATASETS[name][IM_PREFIX]\n        )\n        self.COCO = COCO(DATASETS[name][ANN_FN])\n        self.debug_timer = Timer()\n        # Set up dataset classes\n        category_ids = self.COCO.getCatIds()\n        categories = [c[\'name\'] for c in self.COCO.loadCats(category_ids)]\n        self.category_to_id_map = dict(zip(categories, category_ids))\n        self.classes = [\'__background__\'] + categories\n        self.num_classes = len(self.classes)\n        self.json_category_id_to_contiguous_id = {\n            v: i + 1\n            for i, v in enumerate(self.COCO.getCatIds())\n        }\n        self.contiguous_category_id_to_json_id = {\n            v: k\n            for k, v in self.json_category_id_to_contiguous_id.items()\n        }\n        self._init_keypoints()\n\n        # # Set cfg.MODEL.NUM_CLASSES\n        # if cfg.MODEL.NUM_CLASSES != -1:\n        #     assert cfg.MODEL.NUM_CLASSES == 2 if cfg.MODEL.KEYPOINTS_ON else self.num_classes, \\\n        #         ""number of classes should equal when using multiple datasets""\n        # else:\n        #     cfg.MODEL.NUM_CLASSES = 2 if cfg.MODEL.KEYPOINTS_ON else self.num_classes\n\n    @property\n    def cache_path(self):\n        cache_path = os.path.abspath(os.path.join(cfg.DATA_DIR, \'cache\'))\n        if not os.path.exists(cache_path):\n            os.makedirs(cache_path)\n        return cache_path\n\n    @property\n    def valid_cached_keys(self):\n        """""" Can load following key-ed values from the cached roidb file\n\n        \'image\'(image path) and \'flipped\' values are already filled on _prep_roidb_entry,\n        so we don\'t need to overwrite it again.\n        """"""\n        keys = [\'boxes\', \'segms\', \'gt_classes\', \'seg_areas\', \'gt_overlaps\',\n                \'is_crowd\', \'box_to_gt_ind_map\']\n        if self.keypoints is not None:\n            keys += [\'gt_keypoints\', \'has_visible_keypoints\']\n        return keys\n\n    def get_roidb(\n            self,\n            gt=False,\n            proposal_file=None,\n            min_proposal_size=2,\n            proposal_limit=-1,\n            crowd_filter_thresh=0\n        ):\n        """"""Return an roidb corresponding to the json dataset. Optionally:\n           - include ground truth boxes in the roidb\n           - add proposals specified in a proposals file\n           - filter proposals based on a minimum side length\n           - filter proposals that intersect with crowd regions\n        """"""\n        assert gt is True or crowd_filter_thresh == 0, \\\n            \'Crowd filter threshold must be 0 if ground-truth annotations \' \\\n            \'are not included.\'\n        image_ids = self.COCO.getImgIds()\n        image_ids.sort()\n        if cfg.DEBUG:\n            roidb = copy.deepcopy(self.COCO.loadImgs(image_ids))[:100]\n        else:\n            roidb = copy.deepcopy(self.COCO.loadImgs(image_ids))\n        for entry in roidb:\n            self._prep_roidb_entry(entry)\n        if gt:\n            # Include ground-truth object annotations\n            cache_filepath = os.path.join(self.cache_path, self.name+\'_gt_roidb.pkl\')\n            if os.path.exists(cache_filepath) and not cfg.DEBUG:\n                self.debug_timer.tic()\n                self._add_gt_from_cache(roidb, cache_filepath)\n                logger.debug(\n                    \'_add_gt_from_cache took {:.3f}s\'.\n                    format(self.debug_timer.toc(average=False))\n                )\n            else:\n                self.debug_timer.tic()\n                for entry in roidb:\n                    self._add_gt_annotations(entry)\n                logger.debug(\n                    \'_add_gt_annotations took {:.3f}s\'.\n                    format(self.debug_timer.toc(average=False))\n                )\n                if not cfg.DEBUG:\n                    with open(cache_filepath, \'wb\') as fp:\n                        pickle.dump(roidb, fp, pickle.HIGHEST_PROTOCOL)\n                    logger.info(\'Cache ground truth roidb to %s\', cache_filepath)\n        if proposal_file is not None:\n            # Include proposals from a file\n            self.debug_timer.tic()\n            self._add_proposals_from_file(\n                roidb, proposal_file, min_proposal_size, proposal_limit,\n                crowd_filter_thresh\n            )\n            logger.debug(\n                \'_add_proposals_from_file took {:.3f}s\'.\n                format(self.debug_timer.toc(average=False))\n            )\n        _add_class_assignments(roidb)\n        return roidb\n\n    def _prep_roidb_entry(self, entry):\n        """"""Adds empty metadata fields to an roidb entry.""""""\n        # Reference back to the parent dataset\n        entry[\'dataset\'] = self\n        # Make file_name an abs path\n        im_path = os.path.join(\n            self.image_directory, self.image_prefix + entry[\'file_name\']\n        )\n        assert os.path.exists(im_path), \'Image \\\'{}\\\' not found\'.format(im_path)\n        entry[\'image\'] = im_path\n        entry[\'flipped\'] = False\n        entry[\'has_visible_keypoints\'] = False\n        # Empty placeholders\n        entry[\'boxes\'] = np.empty((0, 4), dtype=np.float32)\n        entry[\'segms\'] = []\n        entry[\'gt_classes\'] = np.empty((0), dtype=np.int32)\n        entry[\'seg_areas\'] = np.empty((0), dtype=np.float32)\n        entry[\'gt_overlaps\'] = scipy.sparse.csr_matrix(\n            np.empty((0, self.num_classes), dtype=np.float32)\n        )\n        entry[\'is_crowd\'] = np.empty((0), dtype=np.bool)\n        # \'box_to_gt_ind_map\': Shape is (#rois). Maps from each roi to the index\n        # in the list of rois that satisfy np.where(entry[\'gt_classes\'] > 0)\n        entry[\'box_to_gt_ind_map\'] = np.empty((0), dtype=np.int32)\n        if self.keypoints is not None:\n            entry[\'gt_keypoints\'] = np.empty(\n                (0, 3, self.num_keypoints), dtype=np.int32\n            )\n        # Remove unwanted fields that come from the json file (if they exist)\n        for k in [\'date_captured\', \'url\', \'license\', \'file_name\']:\n            if k in entry:\n                del entry[k]\n\n    def _add_gt_annotations(self, entry):\n        """"""Add ground truth annotation metadata to an roidb entry.""""""\n        ann_ids = self.COCO.getAnnIds(imgIds=entry[\'id\'], iscrowd=None)\n        objs = self.COCO.loadAnns(ann_ids)\n        # Sanitize bboxes -- some are invalid\n        valid_objs = []\n        valid_segms = []\n        width = entry[\'width\']\n        height = entry[\'height\']\n        for obj in objs:\n            # crowd regions are RLE encoded and stored as dicts\n            if isinstance(obj[\'segmentation\'], list):\n                # Valid polygons have >= 3 points, so require >= 6 coordinates\n                obj[\'segmentation\'] = [\n                    p for p in obj[\'segmentation\'] if len(p) >= 6\n                ]\n            if obj[\'area\'] < cfg.TRAIN.GT_MIN_AREA:\n                continue\n            if \'ignore\' in obj and obj[\'ignore\'] == 1:\n                continue\n            # Convert form (x1, y1, w, h) to (x1, y1, x2, y2)\n            x1, y1, x2, y2 = box_utils.xywh_to_xyxy(obj[\'bbox\'])\n            x1, y1, x2, y2 = box_utils.clip_xyxy_to_image(\n                x1, y1, x2, y2, height, width\n            )\n            # Require non-zero seg area and more than 1x1 box size\n            if obj[\'area\'] > 0 and x2 > x1 and y2 > y1:\n                obj[\'clean_bbox\'] = [x1, y1, x2, y2]\n                valid_objs.append(obj)\n                valid_segms.append(obj[\'segmentation\'])\n        num_valid_objs = len(valid_objs)\n\n        boxes = np.zeros((num_valid_objs, 4), dtype=entry[\'boxes\'].dtype)\n        gt_classes = np.zeros((num_valid_objs), dtype=entry[\'gt_classes\'].dtype)\n        gt_overlaps = np.zeros(\n            (num_valid_objs, self.num_classes),\n            dtype=entry[\'gt_overlaps\'].dtype\n        )\n        seg_areas = np.zeros((num_valid_objs), dtype=entry[\'seg_areas\'].dtype)\n        is_crowd = np.zeros((num_valid_objs), dtype=entry[\'is_crowd\'].dtype)\n        box_to_gt_ind_map = np.zeros(\n            (num_valid_objs), dtype=entry[\'box_to_gt_ind_map\'].dtype\n        )\n        if self.keypoints is not None:\n            gt_keypoints = np.zeros(\n                (num_valid_objs, 3, self.num_keypoints),\n                dtype=entry[\'gt_keypoints\'].dtype\n            )\n\n        im_has_visible_keypoints = False\n        for ix, obj in enumerate(valid_objs):\n            cls = self.json_category_id_to_contiguous_id[obj[\'category_id\']]\n            boxes[ix, :] = obj[\'clean_bbox\']\n            gt_classes[ix] = cls\n            seg_areas[ix] = obj[\'area\']\n            is_crowd[ix] = obj[\'iscrowd\']\n            box_to_gt_ind_map[ix] = ix\n            if self.keypoints is not None:\n                gt_keypoints[ix, :, :] = self._get_gt_keypoints(obj)\n                if np.sum(gt_keypoints[ix, 2, :]) > 0:\n                    im_has_visible_keypoints = True\n            if obj[\'iscrowd\']:\n                # Set overlap to -1 for all classes for crowd objects\n                # so they will be excluded during training\n                gt_overlaps[ix, :] = -1.0\n            else:\n                gt_overlaps[ix, cls] = 1.0\n        entry[\'boxes\'] = np.append(entry[\'boxes\'], boxes, axis=0)\n        entry[\'segms\'].extend(valid_segms)\n        # To match the original implementation:\n        # entry[\'boxes\'] = np.append(\n        #     entry[\'boxes\'], boxes.astype(np.int).astype(np.float), axis=0)\n        entry[\'gt_classes\'] = np.append(entry[\'gt_classes\'], gt_classes)\n        entry[\'seg_areas\'] = np.append(entry[\'seg_areas\'], seg_areas)\n        entry[\'gt_overlaps\'] = np.append(\n            entry[\'gt_overlaps\'].toarray(), gt_overlaps, axis=0\n        )\n        entry[\'gt_overlaps\'] = scipy.sparse.csr_matrix(entry[\'gt_overlaps\'])\n        entry[\'is_crowd\'] = np.append(entry[\'is_crowd\'], is_crowd)\n        entry[\'box_to_gt_ind_map\'] = np.append(\n            entry[\'box_to_gt_ind_map\'], box_to_gt_ind_map\n        )\n        if self.keypoints is not None:\n            entry[\'gt_keypoints\'] = np.append(\n                entry[\'gt_keypoints\'], gt_keypoints, axis=0\n            )\n            entry[\'has_visible_keypoints\'] = im_has_visible_keypoints\n\n    def _add_gt_from_cache(self, roidb, cache_filepath):\n        """"""Add ground truth annotation metadata from cached file.""""""\n        logger.info(\'Loading cached gt_roidb from %s\', cache_filepath)\n        with open(cache_filepath, \'rb\') as fp:\n            cached_roidb = pickle.load(fp)\n\n        assert len(roidb) == len(cached_roidb)\n\n        for entry, cached_entry in zip(roidb, cached_roidb):\n            values = [cached_entry[key] for key in self.valid_cached_keys]\n            boxes, segms, gt_classes, seg_areas, gt_overlaps, is_crowd, \\\n                box_to_gt_ind_map = values[:7]\n            if self.keypoints is not None:\n                gt_keypoints, has_visible_keypoints = values[7:]\n            entry[\'boxes\'] = np.append(entry[\'boxes\'], boxes, axis=0)\n            entry[\'segms\'].extend(segms)\n            # To match the original implementation:\n            # entry[\'boxes\'] = np.append(\n            #     entry[\'boxes\'], boxes.astype(np.int).astype(np.float), axis=0)\n            entry[\'gt_classes\'] = np.append(entry[\'gt_classes\'], gt_classes)\n            entry[\'seg_areas\'] = np.append(entry[\'seg_areas\'], seg_areas)\n            entry[\'gt_overlaps\'] = scipy.sparse.csr_matrix(gt_overlaps)\n            entry[\'is_crowd\'] = np.append(entry[\'is_crowd\'], is_crowd)\n            entry[\'box_to_gt_ind_map\'] = np.append(\n                entry[\'box_to_gt_ind_map\'], box_to_gt_ind_map\n            )\n            if self.keypoints is not None:\n                entry[\'gt_keypoints\'] = np.append(\n                    entry[\'gt_keypoints\'], gt_keypoints, axis=0\n                )\n                entry[\'has_visible_keypoints\'] = has_visible_keypoints\n\n    def _add_proposals_from_file(\n        self, roidb, proposal_file, min_proposal_size, top_k, crowd_thresh\n    ):\n        """"""Add proposals from a proposals file to an roidb.""""""\n        logger.info(\'Loading proposals from: {}\'.format(proposal_file))\n        with open(proposal_file, \'r\') as f:\n            proposals = pickle.load(f)\n        id_field = \'indexes\' if \'indexes\' in proposals else \'ids\'  # compat fix\n        _sort_proposals(proposals, id_field)\n        box_list = []\n        for i, entry in enumerate(roidb):\n            if i % 2500 == 0:\n                logger.info(\' {:d}/{:d}\'.format(i + 1, len(roidb)))\n            boxes = proposals[\'boxes\'][i]\n            # Sanity check that these boxes are for the correct image id\n            assert entry[\'id\'] == proposals[id_field][i]\n            # Remove duplicate boxes and very small boxes and then take top k\n            boxes = box_utils.clip_boxes_to_image(\n                boxes, entry[\'height\'], entry[\'width\']\n            )\n            keep = box_utils.unique_boxes(boxes)\n            boxes = boxes[keep, :]\n            keep = box_utils.filter_small_boxes(boxes, min_proposal_size)\n            boxes = boxes[keep, :]\n            if top_k > 0:\n                boxes = boxes[:top_k, :]\n            box_list.append(boxes)\n        _merge_proposal_boxes_into_roidb(roidb, box_list)\n        if crowd_thresh > 0:\n            _filter_crowd_proposals(roidb, crowd_thresh)\n\n    def _init_keypoints(self):\n        """"""Initialize COCO keypoint information.""""""\n        self.keypoints = None\n        self.keypoint_flip_map = None\n        self.keypoints_to_id_map = None\n        self.num_keypoints = 0\n        # Thus far only the \'person\' category has keypoints\n        if \'person\' in self.category_to_id_map:\n            cat_info = self.COCO.loadCats([self.category_to_id_map[\'person\']])\n        else:\n            return\n\n        # Check if the annotations contain keypoint data or not\n        if \'keypoints\' in cat_info[0]:\n            keypoints = cat_info[0][\'keypoints\']\n            self.keypoints_to_id_map = dict(\n                zip(keypoints, range(len(keypoints))))\n            self.keypoints = keypoints\n            self.num_keypoints = len(keypoints)\n            if cfg.KRCNN.NUM_KEYPOINTS != -1:\n                assert cfg.KRCNN.NUM_KEYPOINTS == self.num_keypoints, \\\n                    ""number of keypoints should equal when using multiple datasets""\n            else:\n                cfg.KRCNN.NUM_KEYPOINTS = self.num_keypoints\n            self.keypoint_flip_map = {\n                \'left_eye\': \'right_eye\',\n                \'left_ear\': \'right_ear\',\n                \'left_shoulder\': \'right_shoulder\',\n                \'left_elbow\': \'right_elbow\',\n                \'left_wrist\': \'right_wrist\',\n                \'left_hip\': \'right_hip\',\n                \'left_knee\': \'right_knee\',\n                \'left_ankle\': \'right_ankle\'}\n\n    def _get_gt_keypoints(self, obj):\n        """"""Return ground truth keypoints.""""""\n        if \'keypoints\' not in obj:\n            return None\n        kp = np.array(obj[\'keypoints\'])\n        x = kp[0::3]  # 0-indexed x coordinates\n        y = kp[1::3]  # 0-indexed y coordinates\n        # 0: not labeled; 1: labeled, not inside mask;\n        # 2: labeled and inside mask\n        v = kp[2::3]\n        num_keypoints = len(obj[\'keypoints\']) / 3\n        assert num_keypoints == self.num_keypoints\n        gt_kps = np.ones((3, self.num_keypoints), dtype=np.int32)\n        for i in range(self.num_keypoints):\n            gt_kps[0, i] = x[i]\n            gt_kps[1, i] = y[i]\n            gt_kps[2, i] = v[i]\n        return gt_kps\n\n\ndef add_proposals(roidb, rois, scales, crowd_thresh):\n    """"""Add proposal boxes (rois) to an roidb that has ground-truth annotations\n    but no proposals. If the proposals are not at the original image scale,\n    specify the scale factor that separate them in scales.\n    """"""\n    box_list = []\n    for i in range(len(roidb)):\n        inv_im_scale = 1. / scales[i]\n        idx = np.where(rois[:, 0] == i)[0]\n        box_list.append(rois[idx, 1:] * inv_im_scale)\n    _merge_proposal_boxes_into_roidb(roidb, box_list)\n    if crowd_thresh > 0:\n        _filter_crowd_proposals(roidb, crowd_thresh)\n    _add_class_assignments(roidb)\n\n\ndef _merge_proposal_boxes_into_roidb(roidb, box_list):\n    """"""Add proposal boxes to each roidb entry.""""""\n    assert len(box_list) == len(roidb)\n    for i, entry in enumerate(roidb):\n        boxes = box_list[i]\n        num_boxes = boxes.shape[0]\n        gt_overlaps = np.zeros(\n            (num_boxes, entry[\'gt_overlaps\'].shape[1]),\n            dtype=entry[\'gt_overlaps\'].dtype\n        )\n        box_to_gt_ind_map = -np.ones(\n            (num_boxes), dtype=entry[\'box_to_gt_ind_map\'].dtype\n        )\n\n        # Note: unlike in other places, here we intentionally include all gt\n        # rois, even ones marked as crowd. Boxes that overlap with crowds will\n        # be filtered out later (see: _filter_crowd_proposals).\n        gt_inds = np.where(entry[\'gt_classes\'] > 0)[0]\n        if len(gt_inds) > 0:\n            gt_boxes = entry[\'boxes\'][gt_inds, :]\n            gt_classes = entry[\'gt_classes\'][gt_inds]\n            proposal_to_gt_overlaps = box_utils.bbox_overlaps(\n                boxes.astype(dtype=np.float32, copy=False),\n                gt_boxes.astype(dtype=np.float32, copy=False)\n            )\n            # Gt box that overlaps each input box the most\n            # (ties are broken arbitrarily by class order)\n            argmaxes = proposal_to_gt_overlaps.argmax(axis=1)\n            # Amount of that overlap\n            maxes = proposal_to_gt_overlaps.max(axis=1)\n            # Those boxes with non-zero overlap with gt boxes\n            I = np.where(maxes > 0)[0]\n            # Record max overlaps with the class of the appropriate gt box\n            gt_overlaps[I, gt_classes[argmaxes[I]]] = maxes[I]\n            box_to_gt_ind_map[I] = gt_inds[argmaxes[I]]\n        entry[\'boxes\'] = np.append(\n            entry[\'boxes\'],\n            boxes.astype(entry[\'boxes\'].dtype, copy=False),\n            axis=0\n        )\n        entry[\'gt_classes\'] = np.append(\n            entry[\'gt_classes\'],\n            np.zeros((num_boxes), dtype=entry[\'gt_classes\'].dtype)\n        )\n        entry[\'seg_areas\'] = np.append(\n            entry[\'seg_areas\'],\n            np.zeros((num_boxes), dtype=entry[\'seg_areas\'].dtype)\n        )\n        entry[\'gt_overlaps\'] = np.append(\n            entry[\'gt_overlaps\'].toarray(), gt_overlaps, axis=0\n        )\n        entry[\'gt_overlaps\'] = scipy.sparse.csr_matrix(entry[\'gt_overlaps\'])\n        entry[\'is_crowd\'] = np.append(\n            entry[\'is_crowd\'],\n            np.zeros((num_boxes), dtype=entry[\'is_crowd\'].dtype)\n        )\n        entry[\'box_to_gt_ind_map\'] = np.append(\n            entry[\'box_to_gt_ind_map\'],\n            box_to_gt_ind_map.astype(\n                entry[\'box_to_gt_ind_map\'].dtype, copy=False\n            )\n        )\n\n\ndef _filter_crowd_proposals(roidb, crowd_thresh):\n    """"""Finds proposals that are inside crowd regions and marks them as\n    overlap = -1 with each ground-truth rois, which means they will be excluded\n    from training.\n    """"""\n    for entry in roidb:\n        gt_overlaps = entry[\'gt_overlaps\'].toarray()\n        crowd_inds = np.where(entry[\'is_crowd\'] == 1)[0]\n        non_gt_inds = np.where(entry[\'gt_classes\'] == 0)[0]\n        if len(crowd_inds) == 0 or len(non_gt_inds) == 0:\n            continue\n        crowd_boxes = box_utils.xyxy_to_xywh(entry[\'boxes\'][crowd_inds, :])\n        non_gt_boxes = box_utils.xyxy_to_xywh(entry[\'boxes\'][non_gt_inds, :])\n        iscrowd_flags = [int(True)] * len(crowd_inds)\n        ious = COCOmask.iou(non_gt_boxes, crowd_boxes, iscrowd_flags)\n        bad_inds = np.where(ious.max(axis=1) > crowd_thresh)[0]\n        gt_overlaps[non_gt_inds[bad_inds], :] = -1\n        entry[\'gt_overlaps\'] = scipy.sparse.csr_matrix(gt_overlaps)\n\n\ndef _add_class_assignments(roidb):\n    """"""Compute object category assignment for each box associated with each\n    roidb entry.\n    """"""\n    for entry in roidb:\n        gt_overlaps = entry[\'gt_overlaps\'].toarray()\n        # max overlap with gt over classes (columns)\n        max_overlaps = gt_overlaps.max(axis=1)\n        # gt class that had the max overlap\n        max_classes = gt_overlaps.argmax(axis=1)\n        entry[\'max_classes\'] = max_classes\n        entry[\'max_overlaps\'] = max_overlaps\n        # sanity checks\n        # if max overlap is 0, the class must be background (class 0)\n        zero_inds = np.where(max_overlaps == 0)[0]\n        assert all(max_classes[zero_inds] == 0)\n        # if max overlap > 0, the class must be a fg class (not class 0)\n        nonzero_inds = np.where(max_overlaps > 0)[0]\n        assert all(max_classes[nonzero_inds] != 0)\n\n\ndef _sort_proposals(proposals, id_field):\n    """"""Sort proposals by the specified id field.""""""\n    order = np.argsort(proposals[id_field])\n    fields_to_sort = [\'boxes\', id_field, \'scores\']\n    for k in fields_to_sort:\n        proposals[k] = [proposals[k][i] for i in order]\n'"
lib/datasets/json_dataset_evaluator.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Functions for evaluating results computed for a json dataset.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport json\nimport logging\nimport numpy as np\nimport os\nimport uuid\n\nfrom pycocotools.cocoeval import COCOeval\n\nfrom core.config import cfg\nfrom utils.io import save_object\nimport utils.boxes as box_utils\n\nlogger = logging.getLogger(__name__)\n\n\ndef evaluate_masks(\n    json_dataset,\n    all_boxes,\n    all_segms,\n    output_dir,\n    use_salt=True,\n    cleanup=False\n):\n    res_file = os.path.join(\n        output_dir, \'segmentations_\' + json_dataset.name + \'_results\'\n    )\n    if use_salt:\n        res_file += \'_{}\'.format(str(uuid.uuid4()))\n    res_file += \'.json\'\n    _write_coco_segms_results_file(\n        json_dataset, all_boxes, all_segms, res_file)\n    # Only do evaluation on non-test sets (annotations are undisclosed on test)\n    if json_dataset.name.find(\'test\') == -1:\n        coco_eval = _do_segmentation_eval(json_dataset, res_file, output_dir)\n    else:\n        coco_eval = None\n    # Optionally cleanup results json file\n    if cleanup:\n        os.remove(res_file)\n    return coco_eval\n\n\ndef _write_coco_segms_results_file(\n    json_dataset, all_boxes, all_segms, res_file\n):\n    # [{""image_id"": 42,\n    #   ""category_id"": 18,\n    #   ""segmentation"": [...],\n    #   ""score"": 0.236}, ...]\n    results = []\n    for cls_ind, cls in enumerate(json_dataset.classes):\n        if cls == \'__background__\':\n            continue\n        if cls_ind >= len(all_boxes):\n            break\n        cat_id = json_dataset.category_to_id_map[cls]\n        results.extend(_coco_segms_results_one_category(\n            json_dataset, all_boxes[cls_ind], all_segms[cls_ind], cat_id))\n    logger.info(\n        \'Writing segmentation results json to: {}\'.format(\n            os.path.abspath(res_file)))\n    with open(res_file, \'w\') as fid:\n        json.dump(results, fid)\n\n\ndef _coco_segms_results_one_category(json_dataset, boxes, segms, cat_id):\n    results = []\n    image_ids = json_dataset.COCO.getImgIds()\n    image_ids.sort()\n    assert len(boxes) == len(image_ids)\n    assert len(segms) == len(image_ids)\n    for i, image_id in enumerate(image_ids):\n        dets = boxes[i]\n        rles = segms[i]\n\n        if isinstance(dets, list) and len(dets) == 0:\n            continue\n\n        dets = dets.astype(np.float)\n        scores = dets[:, -1]\n\n        results.extend(\n            [{\'image_id\': image_id,\n              \'category_id\': cat_id,\n              \'segmentation\': rles[k],\n              \'score\': scores[k]}\n              for k in range(dets.shape[0])])\n\n    return results\n\n\ndef _do_segmentation_eval(json_dataset, res_file, output_dir):\n    coco_dt = json_dataset.COCO.loadRes(str(res_file))\n    coco_eval = COCOeval(json_dataset.COCO, coco_dt, \'segm\')\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    _log_detection_eval_metrics(json_dataset, coco_eval)\n    eval_file = os.path.join(output_dir, \'segmentation_results.pkl\')\n    save_object(coco_eval, eval_file)\n    logger.info(\'Wrote json eval results to: {}\'.format(eval_file))\n    return coco_eval\n\n\ndef evaluate_boxes(\n    json_dataset, all_boxes, output_dir, use_salt=True, cleanup=False\n):\n    res_file = os.path.join(\n        output_dir, \'bbox_\' + json_dataset.name + \'_results\'\n    )\n    if use_salt:\n        res_file += \'_{}\'.format(str(uuid.uuid4()))\n    res_file += \'.json\'\n    _write_coco_bbox_results_file(json_dataset, all_boxes, res_file)\n    # Only do evaluation on non-test sets (annotations are undisclosed on test)\n    if json_dataset.name.find(\'test\') == -1:\n        coco_eval = _do_detection_eval(json_dataset, res_file, output_dir)\n    else:\n        coco_eval = None\n    # Optionally cleanup results json file\n    if cleanup:\n        os.remove(res_file)\n    return coco_eval\n\n\ndef _write_coco_bbox_results_file(json_dataset, all_boxes, res_file):\n    # [{""image_id"": 42,\n    #   ""category_id"": 18,\n    #   ""bbox"": [258.15,41.29,348.26,243.78],\n    #   ""score"": 0.236}, ...]\n    results = []\n    for cls_ind, cls in enumerate(json_dataset.classes):\n        if cls == \'__background__\':\n            continue\n        if cls_ind >= len(all_boxes):\n            break\n        cat_id = json_dataset.category_to_id_map[cls]\n        results.extend(_coco_bbox_results_one_category(\n            json_dataset, all_boxes[cls_ind], cat_id))\n    logger.info(\n        \'Writing bbox results json to: {}\'.format(os.path.abspath(res_file)))\n    with open(res_file, \'w\') as fid:\n        json.dump(results, fid)\n\n\ndef _coco_bbox_results_one_category(json_dataset, boxes, cat_id):\n    results = []\n    image_ids = json_dataset.COCO.getImgIds()\n    image_ids.sort()\n    assert len(boxes) == len(image_ids)\n    for i, image_id in enumerate(image_ids):\n        dets = boxes[i]\n        if isinstance(dets, list) and len(dets) == 0:\n            continue\n        dets = dets.astype(np.float)\n        scores = dets[:, -1]\n        xywh_dets = box_utils.xyxy_to_xywh(dets[:, 0:4])\n        xs = xywh_dets[:, 0]\n        ys = xywh_dets[:, 1]\n        ws = xywh_dets[:, 2]\n        hs = xywh_dets[:, 3]\n        results.extend(\n            [{\'image_id\': image_id,\n              \'category_id\': cat_id,\n              \'bbox\': [xs[k], ys[k], ws[k], hs[k]],\n              \'score\': scores[k]} for k in range(dets.shape[0])])\n    return results\n\n\ndef _do_detection_eval(json_dataset, res_file, output_dir):\n    coco_dt = json_dataset.COCO.loadRes(str(res_file))\n    coco_eval = COCOeval(json_dataset.COCO, coco_dt, \'bbox\')\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    _log_detection_eval_metrics(json_dataset, coco_eval)\n    eval_file = os.path.join(output_dir, \'detection_results.pkl\')\n    save_object(coco_eval, eval_file)\n    logger.info(\'Wrote json eval results to: {}\'.format(eval_file))\n    return coco_eval\n\n\ndef _log_detection_eval_metrics(json_dataset, coco_eval):\n    def _get_thr_ind(coco_eval, thr):\n        ind = np.where((coco_eval.params.iouThrs > thr - 1e-5) &\n                       (coco_eval.params.iouThrs < thr + 1e-5))[0][0]\n        iou_thr = coco_eval.params.iouThrs[ind]\n        assert np.isclose(iou_thr, thr)\n        return ind\n\n    IoU_lo_thresh = 0.5\n    IoU_hi_thresh = 0.95\n    ind_lo = _get_thr_ind(coco_eval, IoU_lo_thresh)\n    ind_hi = _get_thr_ind(coco_eval, IoU_hi_thresh)\n    # precision has dims (iou, recall, cls, area range, max dets)\n    # area range index 0: all area ranges\n    # max dets index 2: 100 per image\n    precision = coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, :, 0, 2]\n    ap_default = np.mean(precision[precision > -1])\n    logger.info(\n        \'~~~~ Mean and per-category AP @ IoU=[{:.2f},{:.2f}] ~~~~\'.format(\n            IoU_lo_thresh, IoU_hi_thresh))\n    logger.info(\'{:.1f}\'.format(100 * ap_default))\n    for cls_ind, cls in enumerate(json_dataset.classes):\n        if cls == \'__background__\':\n            continue\n        # minus 1 because of __background__\n        precision = coco_eval.eval[\'precision\'][\n            ind_lo:(ind_hi + 1), :, cls_ind - 1, 0, 2]\n        ap = np.mean(precision[precision > -1])\n        logger.info(\'{:.1f}\'.format(100 * ap))\n    logger.info(\'~~~~ Summary metrics ~~~~\')\n    coco_eval.summarize()\n\n\ndef evaluate_box_proposals(\n    json_dataset, roidb, thresholds=None, area=\'all\', limit=None\n):\n    """"""Evaluate detection proposal recall metrics. This function is a much\n    faster alternative to the official COCO API recall evaluation code. However,\n    it produces slightly different results.\n    """"""\n    # Record max overlap value for each gt box\n    # Return vector of overlap values\n    areas = {\n        \'all\': 0,\n        \'small\': 1,\n        \'medium\': 2,\n        \'large\': 3,\n        \'96-128\': 4,\n        \'128-256\': 5,\n        \'256-512\': 6,\n        \'512-inf\': 7}\n    area_ranges = [\n        [0**2, 1e5**2],    # all\n        [0**2, 32**2],     # small\n        [32**2, 96**2],    # medium\n        [96**2, 1e5**2],   # large\n        [96**2, 128**2],   # 96-128\n        [128**2, 256**2],  # 128-256\n        [256**2, 512**2],  # 256-512\n        [512**2, 1e5**2]]  # 512-inf\n    assert area in areas, \'Unknown area range: {}\'.format(area)\n    area_range = area_ranges[areas[area]]\n    gt_overlaps = np.zeros(0)\n    num_pos = 0\n    for entry in roidb:\n        gt_inds = np.where(\n            (entry[\'gt_classes\'] > 0) & (entry[\'is_crowd\'] == 0))[0]\n        gt_boxes = entry[\'boxes\'][gt_inds, :]\n        gt_areas = entry[\'seg_areas\'][gt_inds]\n        valid_gt_inds = np.where(\n            (gt_areas >= area_range[0]) & (gt_areas <= area_range[1]))[0]\n        gt_boxes = gt_boxes[valid_gt_inds, :]\n        num_pos += len(valid_gt_inds)\n        non_gt_inds = np.where(entry[\'gt_classes\'] == 0)[0]\n        boxes = entry[\'boxes\'][non_gt_inds, :]\n        if boxes.shape[0] == 0:\n            continue\n        if limit is not None and boxes.shape[0] > limit:\n            boxes = boxes[:limit, :]\n        overlaps = box_utils.bbox_overlaps(\n            boxes.astype(dtype=np.float32, copy=False),\n            gt_boxes.astype(dtype=np.float32, copy=False))\n        _gt_overlaps = np.zeros((gt_boxes.shape[0]))\n        for j in range(min(boxes.shape[0], gt_boxes.shape[0])):\n            # find which proposal box maximally covers each gt box\n            argmax_overlaps = overlaps.argmax(axis=0)\n            # and get the iou amount of coverage for each gt box\n            max_overlaps = overlaps.max(axis=0)\n            # find which gt box is \'best\' covered (i.e. \'best\' = most iou)\n            gt_ind = max_overlaps.argmax()\n            gt_ovr = max_overlaps.max()\n            assert gt_ovr >= 0\n            # find the proposal box that covers the best covered gt box\n            box_ind = argmax_overlaps[gt_ind]\n            # record the iou coverage of this gt box\n            _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n            assert _gt_overlaps[j] == gt_ovr\n            # mark the proposal box and the gt box as used\n            overlaps[box_ind, :] = -1\n            overlaps[:, gt_ind] = -1\n        # append recorded iou coverage level\n        gt_overlaps = np.hstack((gt_overlaps, _gt_overlaps))\n\n    gt_overlaps = np.sort(gt_overlaps)\n    if thresholds is None:\n        step = 0.05\n        thresholds = np.arange(0.5, 0.95 + 1e-5, step)\n    recalls = np.zeros_like(thresholds)\n    # compute recall for each iou threshold\n    for i, t in enumerate(thresholds):\n        recalls[i] = (gt_overlaps >= t).sum() / float(num_pos)\n    # ar = 2 * np.trapz(recalls, thresholds)\n    ar = recalls.mean()\n    return {\'ar\': ar, \'recalls\': recalls, \'thresholds\': thresholds,\n            \'gt_overlaps\': gt_overlaps, \'num_pos\': num_pos}\n\n\ndef evaluate_keypoints(\n    json_dataset,\n    all_boxes,\n    all_keypoints,\n    output_dir,\n    use_salt=True,\n    cleanup=False\n):\n    res_file = os.path.join(\n        output_dir, \'keypoints_\' + json_dataset.name + \'_results\'\n    )\n    if use_salt:\n        res_file += \'_{}\'.format(str(uuid.uuid4()))\n    res_file += \'.json\'\n    _write_coco_keypoint_results_file(\n        json_dataset, all_boxes, all_keypoints, res_file)\n    # Only do evaluation on non-test sets (annotations are undisclosed on test)\n    if json_dataset.name.find(\'test\') == -1:\n        coco_eval = _do_keypoint_eval(json_dataset, res_file, output_dir)\n    else:\n        coco_eval = None\n    # Optionally cleanup results json file\n    if cleanup:\n        os.remove(res_file)\n    return coco_eval\n\n\ndef _write_coco_keypoint_results_file(\n    json_dataset, all_boxes, all_keypoints, res_file\n):\n    results = []\n    for cls_ind, cls in enumerate(json_dataset.classes):\n        if cls == \'__background__\':\n            continue\n        if cls_ind >= len(all_keypoints):\n            break\n        logger.info(\n            \'Collecting {} results ({:d}/{:d})\'.format(\n                cls, cls_ind, len(all_keypoints) - 1))\n        cat_id = json_dataset.category_to_id_map[cls]\n        results.extend(_coco_kp_results_one_category(\n            json_dataset, all_boxes[cls_ind], all_keypoints[cls_ind], cat_id))\n    logger.info(\n        \'Writing keypoint results json to: {}\'.format(\n            os.path.abspath(res_file)))\n    with open(res_file, \'w\') as fid:\n        json.dump(results, fid)\n\n\ndef _coco_kp_results_one_category(json_dataset, boxes, kps, cat_id):\n    results = []\n    image_ids = json_dataset.COCO.getImgIds()\n    image_ids.sort()\n    assert len(kps) == len(image_ids)\n    assert len(boxes) == len(image_ids)\n    use_box_score = False\n    if cfg.KRCNN.KEYPOINT_CONFIDENCE == \'logit\':\n        # This is ugly; see utils.keypoints.heatmap_to_keypoints for the magic\n        # indexes\n        score_index = 2\n    elif cfg.KRCNN.KEYPOINT_CONFIDENCE == \'prob\':\n        score_index = 3\n    elif cfg.KRCNN.KEYPOINT_CONFIDENCE == \'bbox\':\n        use_box_score = True\n    else:\n        raise ValueError(\n            \'KRCNN.KEYPOINT_CONFIDENCE must be ""logit"", ""prob"", or ""bbox""\')\n    for i, image_id in enumerate(image_ids):\n        if len(boxes[i]) == 0:\n            continue\n        kps_dets = kps[i]\n        scores = boxes[i][:, -1].astype(np.float)\n        if len(kps_dets) == 0:\n            continue\n        for j in range(len(kps_dets)):\n            xy = []\n\n            kps_score = 0\n            for k in range(kps_dets[j].shape[1]):\n                xy.append(float(kps_dets[j][0, k]))\n                xy.append(float(kps_dets[j][1, k]))\n                xy.append(1)\n                if not use_box_score:\n                    kps_score += kps_dets[j][score_index, k]\n\n            if use_box_score:\n                kps_score = scores[j]\n            else:\n                kps_score /= kps_dets[j].shape[1]\n\n            results.extend([{\'image_id\': image_id,\n                             \'category_id\': cat_id,\n                             \'keypoints\': xy,\n                             \'score\': kps_score}])\n    return results\n\n\ndef _do_keypoint_eval(json_dataset, res_file, output_dir):\n    ann_type = \'keypoints\'\n    imgIds = json_dataset.COCO.getImgIds()\n    imgIds.sort()\n    coco_dt = json_dataset.COCO.loadRes(res_file)\n    coco_eval = COCOeval(json_dataset.COCO, coco_dt, ann_type)\n    coco_eval.params.imgIds = imgIds\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    eval_file = os.path.join(output_dir, \'keypoint_results.pkl\')\n    save_object(coco_eval, eval_file)\n    logger.info(\'Wrote json eval results to: {}\'.format(eval_file))\n    coco_eval.summarize()\n    return coco_eval\n'"
lib/datasets/roidb.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Functions for common roidb manipulations.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport six\nimport logging\nimport numpy as np\n\nimport utils.boxes as box_utils\nimport utils.keypoints as keypoint_utils\nimport utils.segms as segm_utils\nimport utils.blob as blob_utils\nfrom core.config import cfg\nfrom .json_dataset import JsonDataset\n\nlogger = logging.getLogger(__name__)\n\n\ndef combined_roidb_for_training(dataset_names, proposal_files):\n    """"""Load and concatenate roidbs for one or more datasets, along with optional\n    object proposals. The roidb entries are then prepared for use in training,\n    which involves caching certain types of metadata for each roidb entry.\n    """"""\n    def get_roidb(dataset_name, proposal_file):\n        ds = JsonDataset(dataset_name)\n        roidb = ds.get_roidb(\n            gt=True,\n            proposal_file=proposal_file,\n            crowd_filter_thresh=cfg.TRAIN.CROWD_FILTER_THRESH\n        )\n        if cfg.TRAIN.USE_FLIPPED:\n            logger.info(\'Appending horizontally-flipped training examples...\')\n            extend_with_flipped_entries(roidb, ds)\n        logger.info(\'Loaded dataset: {:s}\'.format(ds.name))\n        return roidb\n\n    if isinstance(dataset_names, six.string_types):\n        dataset_names = (dataset_names, )\n    if isinstance(proposal_files, six.string_types):\n        proposal_files = (proposal_files, )\n    if len(proposal_files) == 0:\n        proposal_files = (None, ) * len(dataset_names)\n    assert len(dataset_names) == len(proposal_files)\n    roidbs = [get_roidb(*args) for args in zip(dataset_names, proposal_files)]\n    roidb = roidbs[0]\n    for r in roidbs[1:]:\n        roidb.extend(r)\n    roidb = filter_for_training(roidb)\n\n    if cfg.TRAIN.ASPECT_GROUPING or cfg.TRAIN.ASPECT_CROPPING:\n        logger.info(\'Computing image aspect ratios and ordering the ratios...\')\n        ratio_list, ratio_index = rank_for_training(roidb)\n        logger.info(\'done\')\n    else:\n        ratio_list, ratio_index = None, None\n\n    logger.info(\'Computing bounding-box regression targets...\')\n    add_bbox_regression_targets(roidb)\n    logger.info(\'done\')\n\n    _compute_and_log_stats(roidb)\n\n    return roidb, ratio_list, ratio_index\n\n\ndef extend_with_flipped_entries(roidb, dataset):\n    """"""Flip each entry in the given roidb and return a new roidb that is the\n    concatenation of the original roidb and the flipped entries.\n\n    ""Flipping"" an entry means that that image and associated metadata (e.g.,\n    ground truth boxes and object proposals) are horizontally flipped.\n    """"""\n    flipped_roidb = []\n    for entry in roidb:\n        width = entry[\'width\']\n        boxes = entry[\'boxes\'].copy()\n        oldx1 = boxes[:, 0].copy()\n        oldx2 = boxes[:, 2].copy()\n        boxes[:, 0] = width - oldx2 - 1\n        boxes[:, 2] = width - oldx1 - 1\n        assert (boxes[:, 2] >= boxes[:, 0]).all()\n        flipped_entry = {}\n        dont_copy = (\'boxes\', \'segms\', \'gt_keypoints\', \'flipped\')\n        for k, v in entry.items():\n            if k not in dont_copy:\n                flipped_entry[k] = v\n        flipped_entry[\'boxes\'] = boxes\n        flipped_entry[\'segms\'] = segm_utils.flip_segms(\n            entry[\'segms\'], entry[\'height\'], entry[\'width\']\n        )\n        if dataset.keypoints is not None:\n            flipped_entry[\'gt_keypoints\'] = keypoint_utils.flip_keypoints(\n                dataset.keypoints, dataset.keypoint_flip_map,\n                entry[\'gt_keypoints\'], entry[\'width\']\n            )\n        flipped_entry[\'flipped\'] = True\n        flipped_roidb.append(flipped_entry)\n    roidb.extend(flipped_roidb)\n\n\ndef filter_for_training(roidb):\n    """"""Remove roidb entries that have no usable RoIs based on config settings.\n    """"""\n    def is_valid(entry):\n        # Valid images have:\n        #   (1) At least one foreground RoI OR\n        #   (2) At least one background RoI\n        overlaps = entry[\'max_overlaps\']\n        # find boxes with sufficient overlap\n        fg_inds = np.where(overlaps >= cfg.TRAIN.FG_THRESH)[0]\n        # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n        bg_inds = np.where((overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                           (overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n        # image is only valid if such boxes exist\n        valid = len(fg_inds) > 0 or len(bg_inds) > 0\n        if cfg.MODEL.KEYPOINTS_ON:\n            # If we\'re training for keypoints, exclude images with no keypoints\n            valid = valid and entry[\'has_visible_keypoints\']\n        return valid\n\n    num = len(roidb)\n    filtered_roidb = [entry for entry in roidb if is_valid(entry)]\n    num_after = len(filtered_roidb)\n    logger.info(\'Filtered {} roidb entries: {} -> {}\'.\n                format(num - num_after, num, num_after))\n    return filtered_roidb\n\n\ndef rank_for_training(roidb):\n    """"""Rank the roidb entries according to image aspect ration and mark for cropping\n    for efficient batching if image is too long.\n\n    Returns:\n        ratio_list: ndarray, list of aspect ratios from small to large\n        ratio_index: ndarray, list of roidb entry indices correspond to the ratios\n    """"""\n    RATIO_HI = cfg.TRAIN.ASPECT_HI  # largest ratio to preserve.\n    RATIO_LO = cfg.TRAIN.ASPECT_LO  # smallest ratio to preserve.\n\n    need_crop_cnt = 0\n\n    ratio_list = []\n    for entry in roidb:\n        width = entry[\'width\']\n        height = entry[\'height\']\n        ratio = width / float(height)\n\n        if cfg.TRAIN.ASPECT_CROPPING:\n            if ratio > RATIO_HI:\n                entry[\'need_crop\'] = True\n                ratio = RATIO_HI\n                need_crop_cnt += 1\n            elif ratio < RATIO_LO:\n                entry[\'need_crop\'] = True\n                ratio = RATIO_LO\n                need_crop_cnt += 1\n            else:\n                entry[\'need_crop\'] = False\n        else:\n            entry[\'need_crop\'] = False\n\n        ratio_list.append(ratio)\n\n    if cfg.TRAIN.ASPECT_CROPPING:\n        logging.info(\'Number of entries that need to be cropped: %d. Ratio bound: [%.2f, %.2f]\',\n                     need_crop_cnt, RATIO_LO, RATIO_HI)\n    ratio_list = np.array(ratio_list)\n    ratio_index = np.argsort(ratio_list)\n    return ratio_list[ratio_index], ratio_index\n\ndef add_bbox_regression_targets(roidb):\n    """"""Add information needed to train bounding-box regressors.""""""\n    for entry in roidb:\n        entry[\'bbox_targets\'] = _compute_targets(entry)\n\n\ndef _compute_targets(entry):\n    """"""Compute bounding-box regression targets for an image.""""""\n    # Indices of ground-truth ROIs\n    rois = entry[\'boxes\']\n    overlaps = entry[\'max_overlaps\']\n    labels = entry[\'max_classes\']\n    gt_inds = np.where((entry[\'gt_classes\'] > 0) & (entry[\'is_crowd\'] == 0))[0]\n    # Targets has format (class, tx, ty, tw, th)\n    targets = np.zeros((rois.shape[0], 5), dtype=np.float32)\n    if len(gt_inds) == 0:\n        # Bail if the image has no ground-truth ROIs\n        return targets\n\n    # Indices of examples for which we try to make predictions\n    ex_inds = np.where(overlaps >= cfg.TRAIN.BBOX_THRESH)[0]\n\n    # Get IoU overlap between each ex ROI and gt ROI\n    ex_gt_overlaps = box_utils.bbox_overlaps(\n        rois[ex_inds, :].astype(dtype=np.float32, copy=False),\n        rois[gt_inds, :].astype(dtype=np.float32, copy=False))\n\n    # Find which gt ROI each ex ROI has max overlap with:\n    # this will be the ex ROI\'s gt target\n    gt_assignment = ex_gt_overlaps.argmax(axis=1)\n    gt_rois = rois[gt_inds[gt_assignment], :]\n    ex_rois = rois[ex_inds, :]\n    # Use class ""1"" for all boxes if using class_agnostic_bbox_reg\n    targets[ex_inds, 0] = (\n        1 if cfg.MODEL.CLS_AGNOSTIC_BBOX_REG else labels[ex_inds])\n    targets[ex_inds, 1:] = box_utils.bbox_transform_inv(\n        ex_rois, gt_rois, cfg.MODEL.BBOX_REG_WEIGHTS)\n    return targets\n\n\ndef _compute_and_log_stats(roidb):\n    classes = roidb[0][\'dataset\'].classes\n    char_len = np.max([len(c) for c in classes])\n    hist_bins = np.arange(len(classes) + 1)\n\n    # Histogram of ground-truth objects\n    gt_hist = np.zeros((len(classes)), dtype=np.int)\n    for entry in roidb:\n        gt_inds = np.where(\n            (entry[\'gt_classes\'] > 0) & (entry[\'is_crowd\'] == 0))[0]\n        gt_classes = entry[\'gt_classes\'][gt_inds]\n        gt_hist += np.histogram(gt_classes, bins=hist_bins)[0]\n    logger.debug(\'Ground-truth class histogram:\')\n    for i, v in enumerate(gt_hist):\n        logger.debug(\n            \'{:d}{:s}: {:d}\'.format(\n                i, classes[i].rjust(char_len), v))\n    logger.debug(\'-\' * char_len)\n    logger.debug(\n        \'{:s}: {:d}\'.format(\n            \'total\'.rjust(char_len), np.sum(gt_hist)))\n'"
lib/datasets/task_evaluation.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Evaluation interface for supported tasks (box detection, instance\nsegmentation, keypoint detection, ...).\n\n\nResults are stored in an OrderedDict with the following nested structure:\n\n<dataset>:\n  <task>:\n    <metric>: <val>\n\n<dataset> is any valid dataset (e.g., \'coco_2014_minival\')\n<task> is in [\'box\', \'mask\', \'keypoint\', \'box_proposal\']\n<metric> can be [\'AP\', \'AP50\', \'AP75\', \'APs\', \'APm\', \'APl\', \'AR@1000\',\n                 \'ARs@1000\', \'ARm@1000\', \'ARl@1000\', ...]\n<val> is a floating point number\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import OrderedDict\nimport logging\nimport os\nimport pprint\n\nfrom core.config import cfg\nfrom utils.logging import send_email\nimport datasets.cityscapes_json_dataset_evaluator as cs_json_dataset_evaluator\nimport datasets.json_dataset_evaluator as json_dataset_evaluator\nimport datasets.voc_dataset_evaluator as voc_dataset_evaluator\n\nlogger = logging.getLogger(__name__)\n\n\ndef evaluate_all(\n    dataset, all_boxes, all_segms, all_keyps, output_dir, use_matlab=False\n):\n    """"""Evaluate ""all"" tasks, where ""all"" includes box detection, instance\n    segmentation, and keypoint detection.\n    """"""\n    all_results = evaluate_boxes(\n        dataset, all_boxes, output_dir, use_matlab=use_matlab\n    )\n    logger.info(\'Evaluating bounding boxes is done!\')\n    if cfg.MODEL.MASK_ON:\n        results = evaluate_masks(dataset, all_boxes, all_segms, output_dir)\n        all_results[dataset.name].update(results[dataset.name])\n        logger.info(\'Evaluating segmentations is done!\')\n    if cfg.MODEL.KEYPOINTS_ON:\n        results = evaluate_keypoints(dataset, all_boxes, all_keyps, output_dir)\n        all_results[dataset.name].update(results[dataset.name])\n        logger.info(\'Evaluating keypoints is done!\')\n    return all_results\n\n\ndef evaluate_boxes(dataset, all_boxes, output_dir, use_matlab=False):\n    """"""Evaluate bounding box detection.""""""\n    logger.info(\'Evaluating detections\')\n    not_comp = not cfg.TEST.COMPETITION_MODE\n    if _use_json_dataset_evaluator(dataset):\n        coco_eval = json_dataset_evaluator.evaluate_boxes(\n            dataset, all_boxes, output_dir, use_salt=not_comp, cleanup=not_comp\n        )\n        box_results = _coco_eval_to_box_results(coco_eval)\n    elif _use_cityscapes_evaluator(dataset):\n        logger.warn(\'Cityscapes bbox evaluated using COCO metrics/conversions\')\n        coco_eval = json_dataset_evaluator.evaluate_boxes(\n            dataset, all_boxes, output_dir, use_salt=not_comp, cleanup=not_comp\n        )\n        box_results = _coco_eval_to_box_results(coco_eval)\n    elif _use_voc_evaluator(dataset):\n        # For VOC, always use salt and always cleanup because results are\n        # written to the shared VOCdevkit results directory\n        voc_eval = voc_dataset_evaluator.evaluate_boxes(\n            dataset, all_boxes, output_dir, use_matlab=use_matlab\n        )\n        box_results = _voc_eval_to_box_results(voc_eval)\n    else:\n        raise NotImplementedError(\n            \'No evaluator for dataset: {}\'.format(dataset.name)\n        )\n    return OrderedDict([(dataset.name, box_results)])\n\n\ndef evaluate_masks(dataset, all_boxes, all_segms, output_dir):\n    """"""Evaluate instance segmentation.""""""\n    logger.info(\'Evaluating segmentations\')\n    not_comp = not cfg.TEST.COMPETITION_MODE\n    if _use_json_dataset_evaluator(dataset):\n        coco_eval = json_dataset_evaluator.evaluate_masks(\n            dataset,\n            all_boxes,\n            all_segms,\n            output_dir,\n            use_salt=not_comp,\n            cleanup=not_comp\n        )\n        mask_results = _coco_eval_to_mask_results(coco_eval)\n    elif _use_cityscapes_evaluator(dataset):\n        cs_eval = cs_json_dataset_evaluator.evaluate_masks(\n            dataset,\n            all_boxes,\n            all_segms,\n            output_dir,\n            use_salt=not_comp,\n            cleanup=not_comp\n        )\n        mask_results = _cs_eval_to_mask_results(cs_eval)\n    else:\n        raise NotImplementedError(\n            \'No evaluator for dataset: {}\'.format(dataset.name)\n        )\n    return OrderedDict([(dataset.name, mask_results)])\n\n\ndef evaluate_keypoints(dataset, all_boxes, all_keyps, output_dir):\n    """"""Evaluate human keypoint detection (i.e., 2D pose estimation).""""""\n    logger.info(\'Evaluating detections\')\n    not_comp = not cfg.TEST.COMPETITION_MODE\n    assert dataset.name.startswith(\'keypoints_coco_\'), \\\n        \'Only COCO keypoints are currently supported\'\n    coco_eval = json_dataset_evaluator.evaluate_keypoints(\n        dataset,\n        all_boxes,\n        all_keyps,\n        output_dir,\n        use_salt=not_comp,\n        cleanup=not_comp\n    )\n    keypoint_results = _coco_eval_to_keypoint_results(coco_eval)\n    return OrderedDict([(dataset.name, keypoint_results)])\n\n\ndef evaluate_box_proposals(dataset, roidb):\n    """"""Evaluate bounding box object proposals.""""""\n    res = _empty_box_proposal_results()\n    areas = {\'all\': \'\', \'small\': \'s\', \'medium\': \'m\', \'large\': \'l\'}\n    for limit in [100, 1000]:\n        for area, suffix in areas.items():\n            stats = json_dataset_evaluator.evaluate_box_proposals(\n                dataset, roidb, area=area, limit=limit\n            )\n            key = \'AR{}@{:d}\'.format(suffix, limit)\n            res[\'box_proposal\'][key] = stats[\'ar\']\n    return OrderedDict([(dataset.name, res)])\n\n\ndef log_box_proposal_results(results):\n    """"""Log bounding box proposal results.""""""\n    for dataset in results.keys():\n        keys = results[dataset][\'box_proposal\'].keys()\n        pad = max([len(k) for k in keys])\n        logger.info(dataset)\n        for k, v in results[dataset][\'box_proposal\'].items():\n            logger.info(\'{}: {:.3f}\'.format(k.ljust(pad), v))\n\n\ndef log_copy_paste_friendly_results(results):\n    """"""Log results in a format that makes it easy to copy-and-paste in a\n    spreadsheet. Lines are prefixed with \'copypaste: \' to make grepping easy.\n    """"""\n    for dataset in results.keys():\n        logger.info(\'copypaste: Dataset: {}\'.format(dataset))\n        for task, metrics in results[dataset].items():\n            logger.info(\'copypaste: Task: {}\'.format(task))\n            metric_names = metrics.keys()\n            metric_vals = [\'{:.4f}\'.format(v) for v in metrics.values()]\n            logger.info(\'copypaste: \' + \',\'.join(metric_names))\n            logger.info(\'copypaste: \' + \',\'.join(metric_vals))\n\n\ndef check_expected_results(results, atol=0.005, rtol=0.1):\n    """"""Check actual results against expected results stored in\n    cfg.EXPECTED_RESULTS. Optionally email if the match exceeds the specified\n    tolerance.\n\n    Expected results should take the form of a list of expectations, each\n    specified by four elements: [dataset, task, metric, expected value]. For\n    example: [[\'coco_2014_minival\', \'box_proposal\', \'AR@1000\', 0.387], ...].\n    """"""\n    # cfg contains a reference set of results that we want to check against\n    if len(cfg.EXPECTED_RESULTS) == 0:\n        return\n\n    for dataset, task, metric, expected_val in cfg.EXPECTED_RESULTS:\n        assert dataset in results, \'Dataset {} not in results\'.format(dataset)\n        assert task in results[dataset], \'Task {} not in results\'.format(task)\n        assert metric in results[dataset][task], \\\n            \'Metric {} not in results\'.format(metric)\n        actual_val = results[dataset][task][metric]\n        err = abs(actual_val - expected_val)\n        tol = atol + rtol * abs(expected_val)\n        msg = (\n            \'{} > {} > {} sanity check (actual vs. expected): \'\n            \'{:.3f} vs. {:.3f}, err={:.3f}, tol={:.3f}\'\n        ).format(dataset, task, metric, actual_val, expected_val, err, tol)\n        if err > tol:\n            msg = \'FAIL: \' + msg\n            logger.error(msg)\n            if cfg.EXPECTED_RESULTS_EMAIL != \'\':\n                subject = \'Detectron end-to-end test failure\'\n                job_name = os.environ[\n                    \'DETECTRON_JOB_NAME\'\n                ] if \'DETECTRON_JOB_NAME\' in os.environ else \'<unknown>\'\n                job_id = os.environ[\n                    \'WORKFLOW_RUN_ID\'\n                ] if \'WORKFLOW_RUN_ID\' in os.environ else \'<unknown>\'\n                body = [\n                    \'Name:\',\n                    job_name,\n                    \'Run ID:\',\n                    job_id,\n                    \'Failure:\',\n                    msg,\n                    \'Config:\',\n                    pprint.pformat(cfg),\n                    \'Env:\',\n                    pprint.pformat(dict(os.environ)),\n                ]\n                send_email(\n                    subject, \'\\n\\n\'.join(body), cfg.EXPECTED_RESULTS_EMAIL\n                )\n        else:\n            msg = \'PASS: \' + msg\n            logger.info(msg)\n\n\ndef _use_json_dataset_evaluator(dataset):\n    """"""Check if the dataset uses the general json dataset evaluator.""""""\n    return dataset.name.find(\'coco_\') > -1 or cfg.TEST.FORCE_JSON_DATASET_EVAL\n\n\ndef _use_cityscapes_evaluator(dataset):\n    """"""Check if the dataset uses the Cityscapes dataset evaluator.""""""\n    return dataset.name.find(\'cityscapes_\') > -1\n\n\ndef _use_voc_evaluator(dataset):\n    """"""Check if the dataset uses the PASCAL VOC dataset evaluator.""""""\n    return dataset.name[:4] == \'voc_\'\n\n\n# Indices in the stats array for COCO boxes and masks\nCOCO_AP = 0\nCOCO_AP50 = 1\nCOCO_AP75 = 2\nCOCO_APS = 3\nCOCO_APM = 4\nCOCO_APL = 5\n# Slight difference for keypoints\nCOCO_KPS_APM = 3\nCOCO_KPS_APL = 4\n\n\n# ---------------------------------------------------------------------------- #\n# Helper functions for producing properly formatted results.\n# ---------------------------------------------------------------------------- #\n\ndef _coco_eval_to_box_results(coco_eval):\n    res = _empty_box_results()\n    if coco_eval is not None:\n        s = coco_eval.stats\n        res[\'box\'][\'AP\'] = s[COCO_AP]\n        res[\'box\'][\'AP50\'] = s[COCO_AP50]\n        res[\'box\'][\'AP75\'] = s[COCO_AP75]\n        res[\'box\'][\'APs\'] = s[COCO_APS]\n        res[\'box\'][\'APm\'] = s[COCO_APM]\n        res[\'box\'][\'APl\'] = s[COCO_APL]\n    return res\n\n\ndef _coco_eval_to_mask_results(coco_eval):\n    res = _empty_mask_results()\n    if coco_eval is not None:\n        s = coco_eval.stats\n        res[\'mask\'][\'AP\'] = s[COCO_AP]\n        res[\'mask\'][\'AP50\'] = s[COCO_AP50]\n        res[\'mask\'][\'AP75\'] = s[COCO_AP75]\n        res[\'mask\'][\'APs\'] = s[COCO_APS]\n        res[\'mask\'][\'APm\'] = s[COCO_APM]\n        res[\'mask\'][\'APl\'] = s[COCO_APL]\n    return res\n\n\ndef _coco_eval_to_keypoint_results(coco_eval):\n    res = _empty_keypoint_results()\n    if coco_eval is not None:\n        s = coco_eval.stats\n        res[\'keypoint\'][\'AP\'] = s[COCO_AP]\n        res[\'keypoint\'][\'AP50\'] = s[COCO_AP50]\n        res[\'keypoint\'][\'AP75\'] = s[COCO_AP75]\n        res[\'keypoint\'][\'APm\'] = s[COCO_KPS_APM]\n        res[\'keypoint\'][\'APl\'] = s[COCO_KPS_APL]\n    return res\n\n\ndef _voc_eval_to_box_results(voc_eval):\n    # Not supported (return empty results)\n    return _empty_box_results()\n\n\ndef _cs_eval_to_mask_results(cs_eval):\n    # Not supported (return empty results)\n    return _empty_mask_results()\n\n\ndef _empty_box_results():\n    return OrderedDict({\n        \'box\':\n        OrderedDict(\n            [\n                (\'AP\', -1),\n                (\'AP50\', -1),\n                (\'AP75\', -1),\n                (\'APs\', -1),\n                (\'APm\', -1),\n                (\'APl\', -1),\n            ]\n        )\n    })\n\n\ndef _empty_mask_results():\n    return OrderedDict({\n        \'mask\':\n        OrderedDict(\n            [\n                (\'AP\', -1),\n                (\'AP50\', -1),\n                (\'AP75\', -1),\n                (\'APs\', -1),\n                (\'APm\', -1),\n                (\'APl\', -1),\n            ]\n        )\n    })\n\n\ndef _empty_keypoint_results():\n    return OrderedDict({\n        \'keypoint\':\n        OrderedDict(\n            [\n                (\'AP\', -1),\n                (\'AP50\', -1),\n                (\'AP75\', -1),\n                (\'APm\', -1),\n                (\'APl\', -1),\n            ]\n        )\n    })\n\n\ndef _empty_box_proposal_results():\n    return OrderedDict({\n        \'box_proposal\':\n        OrderedDict(\n            [\n                (\'AR@100\', -1),\n                (\'ARs@100\', -1),\n                (\'ARm@100\', -1),\n                (\'ARl@100\', -1),\n                (\'AR@1000\', -1),\n                (\'ARs@1000\', -1),\n                (\'ARm@1000\', -1),\n                (\'ARl@1000\', -1),\n            ]\n        )\n    })\n'"
lib/datasets/voc_dataset_evaluator.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""PASCAL VOC dataset evaluation interface.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\nimport numpy as np\nimport os\nimport shutil\nimport uuid\n\nfrom core.config import cfg\nfrom datasets.dataset_catalog import DATASETS\nfrom datasets.dataset_catalog import DEVKIT_DIR\nfrom datasets.voc_eval import voc_eval\nfrom utils.io import save_object\n\nlogger = logging.getLogger(__name__)\n\n\ndef evaluate_boxes(\n    json_dataset,\n    all_boxes,\n    output_dir,\n    use_salt=True,\n    cleanup=True,\n    use_matlab=False\n):\n    salt = \'_{}\'.format(str(uuid.uuid4())) if use_salt else \'\'\n    filenames = _write_voc_results_files(json_dataset, all_boxes, salt)\n    _do_python_eval(json_dataset, salt, output_dir)\n    if use_matlab:\n        _do_matlab_eval(json_dataset, salt, output_dir)\n    if cleanup:\n        for filename in filenames:\n            shutil.copy(filename, output_dir)\n            os.remove(filename)\n    return None\n\n\ndef _write_voc_results_files(json_dataset, all_boxes, salt):\n    filenames = []\n    image_set_path = voc_info(json_dataset)[\'image_set_path\']\n    assert os.path.exists(image_set_path), \\\n        \'Image set path does not exist: {}\'.format(image_set_path)\n    with open(image_set_path, \'r\') as f:\n        image_index = [x.strip() for x in f.readlines()]\n    # Sanity check that order of images in json dataset matches order in the\n    # image set\n    roidb = json_dataset.get_roidb()\n    for i, entry in enumerate(roidb):\n        index = os.path.splitext(os.path.split(entry[\'image\'])[1])[0]\n        assert index == image_index[i]\n    for cls_ind, cls in enumerate(json_dataset.classes):\n        if cls == \'__background__\':\n            continue\n        logger.info(\'Writing VOC results for: {}\'.format(cls))\n        filename = _get_voc_results_file_template(json_dataset,\n                                                  salt).format(cls)\n        filenames.append(filename)\n        assert len(all_boxes[cls_ind]) == len(image_index)\n        with open(filename, \'wt\') as f:\n            for im_ind, index in enumerate(image_index):\n                dets = all_boxes[cls_ind][im_ind]\n                if type(dets) == list:\n                    assert len(dets) == 0, \\\n                        \'dets should be numpy.ndarray or empty list\'\n                    continue\n                # the VOCdevkit expects 1-based indices\n                for k in range(dets.shape[0]):\n                    f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                            format(index, dets[k, -1],\n                                   dets[k, 0] + 1, dets[k, 1] + 1,\n                                   dets[k, 2] + 1, dets[k, 3] + 1))\n    return filenames\n\n\ndef _get_voc_results_file_template(json_dataset, salt):\n    info = voc_info(json_dataset)\n    year = info[\'year\']\n    image_set = info[\'image_set\']\n    devkit_path = info[\'devkit_path\']\n    # VOCdevkit/results/VOC2007/Main/<comp_id>_det_test_aeroplane.txt\n    filename = \'comp4\' + salt + \'_det_\' + image_set + \'_{:s}.txt\'\n    return os.path.join(devkit_path, \'results\', \'VOC\' + year, \'Main\', filename)\n\n\ndef _do_python_eval(json_dataset, salt, output_dir=\'output\'):\n    info = voc_info(json_dataset)\n    year = info[\'year\']\n    anno_path = info[\'anno_path\']\n    image_set_path = info[\'image_set_path\']\n    devkit_path = info[\'devkit_path\']\n    cachedir = os.path.join(devkit_path, \'annotations_cache\')\n    aps = []\n    # The PASCAL VOC metric changed in 2010\n    use_07_metric = True if int(year) < 2010 else False\n    logger.info(\'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\'))\n    if not os.path.isdir(output_dir):\n        os.mkdir(output_dir)\n    for _, cls in enumerate(json_dataset.classes):\n        if cls == \'__background__\':\n            continue\n        filename = _get_voc_results_file_template(\n            json_dataset, salt).format(cls)\n        rec, prec, ap = voc_eval(\n            filename, anno_path, image_set_path, cls, cachedir, ovthresh=0.5,\n            use_07_metric=use_07_metric)\n        aps += [ap]\n        logger.info(\'AP for {} = {:.4f}\'.format(cls, ap))\n        res_file = os.path.join(output_dir, cls + \'_pr.pkl\')\n        save_object({\'rec\': rec, \'prec\': prec, \'ap\': ap}, res_file)\n    logger.info(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n    logger.info(\'~~~~~~~~\')\n    logger.info(\'Results:\')\n    for ap in aps:\n        logger.info(\'{:.3f}\'.format(ap))\n    logger.info(\'{:.3f}\'.format(np.mean(aps)))\n    logger.info(\'~~~~~~~~\')\n    logger.info(\'\')\n    logger.info(\'----------------------------------------------------------\')\n    logger.info(\'Results computed with the **unofficial** Python eval code.\')\n    logger.info(\'Results should be very close to the official MATLAB code.\')\n    logger.info(\'Use `./tools/reval.py --matlab ...` for your paper.\')\n    logger.info(\'-- Thanks, The Management\')\n    logger.info(\'----------------------------------------------------------\')\n\n\ndef _do_matlab_eval(json_dataset, salt, output_dir=\'output\'):\n    import subprocess\n    logger.info(\'-----------------------------------------------------\')\n    logger.info(\'Computing results with the official MATLAB eval code.\')\n    logger.info(\'-----------------------------------------------------\')\n    info = voc_info(json_dataset)\n    path = os.path.join(\n        cfg.ROOT_DIR, \'lib\', \'datasets\', \'VOCdevkit-matlab-wrapper\')\n    cmd = \'cd {} && \'.format(path)\n    cmd += \'{:s} -nodisplay -nodesktop \'.format(cfg.MATLAB)\n    cmd += \'-r ""dbstop if error; \'\n    cmd += \'voc_eval(\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\',\\\'{:s}\\\'); quit;""\' \\\n       .format(info[\'devkit_path\'], \'comp4\' + salt, info[\'image_set\'],\n               output_dir)\n    logger.info(\'Running:\\n{}\'.format(cmd))\n    subprocess.call(cmd, shell=True)\n\n\ndef voc_info(json_dataset):\n    year = json_dataset.name[4:8]\n    image_set = json_dataset.name[9:]\n    devkit_path = DATASETS[json_dataset.name][DEVKIT_DIR]\n    assert os.path.exists(devkit_path), \\\n        \'Devkit directory {} not found\'.format(devkit_path)\n    anno_path = os.path.join(\n        devkit_path, \'VOC\' + year, \'Annotations\', \'{:s}.xml\')\n    image_set_path = os.path.join(\n        devkit_path, \'VOC\' + year, \'ImageSets\', \'Main\', image_set + \'.txt\')\n    return dict(\n        year=year,\n        image_set=image_set,\n        devkit_path=devkit_path,\n        anno_path=anno_path,\n        image_set_path=image_set_path)\n'"
lib/datasets/voc_eval.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\n\n""""""Python implementation of the PASCAL VOC devkit\'s AP evaluation code.""""""\n\nfrom six.moves import cPickle\nimport logging\nimport numpy as np\nimport os\nimport xml.etree.ElementTree as ET\n\nlogger = logging.getLogger(__name__)\n\n\ndef parse_rec(filename):\n    """"""Parse a PASCAL VOC xml file.""""""\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall(\'object\'):\n        obj_struct = {}\n        obj_struct[\'name\'] = obj.find(\'name\').text\n        obj_struct[\'pose\'] = obj.find(\'pose\').text\n        obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n        obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n        bbox = obj.find(\'bndbox\')\n        obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                              int(bbox.find(\'ymin\').text),\n                              int(bbox.find(\'xmax\').text),\n                              int(bbox.find(\'ymax\').text)]\n        objects.append(obj_struct)\n\n    return objects\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """"""Compute VOC AP given precision and recall. If use_07_metric is true, uses\n    the VOC 07 11-point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=False):\n    """"""rec, prec, ap = voc_eval(detpath,\n                                annopath,\n                                imagesetfile,\n                                classname,\n                                [ovthresh],\n                                [use_07_metric])\n\n    Top level function that does the PASCAL VOC evaluation.\n\n    detpath: Path to detections\n        detpath.format(classname) should produce the detection results file.\n    annopath: Path to annotations\n        annopath.format(imagename) should be the xml annotations file.\n    imagesetfile: Text file containing the list of images, one image per line.\n    classname: Category name (duh)\n    cachedir: Directory for caching the annotations\n    [ovthresh]: Overlap threshold (default = 0.5)\n    [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n        (default False)\n    """"""\n    # assumes detections are in detpath.format(classname)\n    # assumes annotations are in annopath.format(imagename)\n    # assumes imagesetfile is a text file with each line an image name\n    # cachedir caches the annotations in a pickle file\n\n    # first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    imageset = os.path.splitext(os.path.basename(imagesetfile))[0]\n    cachefile = os.path.join(cachedir, imageset + \'_annots.pkl\')\n    # read list of images\n    with open(imagesetfile, \'r\') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            recs[imagename] = parse_rec(annopath.format(imagename))\n            if i % 100 == 0:\n                logger.info(\n                    \'Reading annotation for {:d}/{:d}\'.format(\n                        i + 1, len(imagenames)))\n        # save\n        logger.info(\'Saving cached annotations to {:s}\'.format(cachefile))\n        with open(cachefile, \'w\') as f:\n            cPickle.dump(recs, f)\n    else:\n        # load\n        with open(cachefile, \'r\') as f:\n            recs = cPickle.load(f)\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for imagename in imagenames:\n        R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n        bbox = np.array([x[\'bbox\'] for x in R])\n        difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagename] = {\'bbox\': bbox,\n                                 \'difficult\': difficult,\n                                 \'det\': det}\n\n    # read dets\n    detfile = detpath.format(classname)\n    with open(detfile, \'r\') as f:\n        lines = f.readlines()\n\n    splitlines = [x.strip().split(\' \') for x in lines]\n    image_ids = [x[0] for x in splitlines]\n    confidence = np.array([float(x[1]) for x in splitlines])\n    BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n    # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]\n\n    # go down dets and mark TPs and FPs\n    nd = len(image_ids)\n    tp = np.zeros(nd)\n    fp = np.zeros(nd)\n    for d in range(nd):\n        R = class_recs[image_ids[d]]\n        bb = BB[d, :].astype(float)\n        ovmax = -np.inf\n        BBGT = R[\'bbox\'].astype(float)\n\n        if BBGT.size > 0:\n            # compute overlaps\n            # intersection\n            ixmin = np.maximum(BBGT[:, 0], bb[0])\n            iymin = np.maximum(BBGT[:, 1], bb[1])\n            ixmax = np.minimum(BBGT[:, 2], bb[2])\n            iymax = np.minimum(BBGT[:, 3], bb[3])\n            iw = np.maximum(ixmax - ixmin + 1., 0.)\n            ih = np.maximum(iymax - iymin + 1., 0.)\n            inters = iw * ih\n\n            # union\n            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n            overlaps = inters / uni\n            ovmax = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n\n        if ovmax > ovthresh:\n            if not R[\'difficult\'][jmax]:\n                if not R[\'det\'][jmax]:\n                    tp[d] = 1.\n                    R[\'det\'][jmax] = 1\n                else:\n                    fp[d] = 1.\n        else:\n            fp[d] = 1.\n\n    # compute precision recall\n    fp = np.cumsum(fp)\n    tp = np.cumsum(tp)\n    rec = tp / float(npos)\n    # avoid divide by zero in case the first detection matches a difficult\n    # ground truth\n    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n    ap = voc_ap(rec, prec, use_07_metric)\n\n    return rec, prec, ap\n'"
lib/model/__init__.py,0,b''
lib/modeling/FPN.py,3,"b'import collections\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\n\nfrom core.config import cfg\nimport utils.net as net_utils\nimport modeling.ResNet as ResNet\nfrom modeling.generate_anchors import generate_anchors\nfrom modeling.generate_proposals import GenerateProposalsOp\nfrom modeling.collect_and_distribute_fpn_rpn_proposals import CollectAndDistributeFpnRpnProposalsOp\nimport nn as mynn\n\n# Lowest and highest pyramid levels in the backbone network. For FPN, we assume\n# that all networks have 5 spatial reductions, each by a factor of 2. Level 1\n# would correspond to the input image, hence it does not make sense to use it.\nLOWEST_BACKBONE_LVL = 2   # E.g., ""conv2""-like level\nHIGHEST_BACKBONE_LVL = 5  # E.g., ""conv5""-like level\n\n\n# ---------------------------------------------------------------------------- #\n# FPN with ResNet\n# ---------------------------------------------------------------------------- #\n\ndef fpn_ResNet50_conv5_body():\n    return fpn(\n        ResNet.ResNet50_conv5_body, fpn_level_info_ResNet50_conv5()\n    )\n\n\ndef fpn_ResNet50_conv5_P2only_body():\n    return fpn(\n        ResNet.ResNet50_conv5_body,\n        fpn_level_info_ResNet50_conv5(),\n        P2only=True\n    )\n\n\ndef fpn_ResNet101_conv5_body():\n    return fpn(\n        ResNet.ResNet101_conv5_body, fpn_level_info_ResNet101_conv5()\n    )\n\n\ndef fpn_ResNet101_conv5_P2only_body():\n    return fpn(\n        ResNet.ResNet101_conv5_body,\n        fpn_level_info_ResNet101_conv5(),\n        P2only=True\n    )\n\n\ndef fpn_ResNet152_conv5_body():\n    return fpn(\n        ResNet.ResNet152_conv5_body, fpn_level_info_ResNet152_conv5()\n    )\n\n\ndef fpn_ResNet152_conv5_P2only_body():\n    return fpn(\n        ResNet.ResNet152_conv5_body,\n        fpn_level_info_ResNet152_conv5(),\n        P2only=True\n    )\n\n\n# ---------------------------------------------------------------------------- #\n# Functions for bolting FPN onto a backbone architectures\n# ---------------------------------------------------------------------------- #\nclass fpn(nn.Module):\n    """"""Add FPN connections based on the model described in the FPN paper.\n\n    fpn_output_blobs is in reversed order: e.g [fpn5, fpn4, fpn3, fpn2]\n    similarly for fpn_level_info.dims: e.g [2048, 1024, 512, 256]\n    similarly for spatial_scale: e.g [1/32, 1/16, 1/8, 1/4]\n    """"""\n    def __init__(self, conv_body_func, fpn_level_info, P2only=False):\n        super().__init__()\n        self.fpn_level_info = fpn_level_info\n        self.P2only = P2only\n\n        self.dim_out = fpn_dim = cfg.FPN.DIM\n        min_level, max_level = get_min_max_levels()\n        self.num_backbone_stages = len(fpn_level_info.blobs) - (min_level - LOWEST_BACKBONE_LVL)\n        fpn_dim_lateral = fpn_level_info.dims\n        self.spatial_scale = []  # a list of scales for FPN outputs\n\n        #\n        # Step 1: recursively build down starting from the coarsest backbone level\n        #\n        # For the coarest backbone level: 1x1 conv only seeds recursion\n        self.conv_top = nn.Conv2d(fpn_dim_lateral[0], fpn_dim, 1, 1, 0)\n        if cfg.FPN.USE_GN:\n            self.conv_top = nn.Sequential(\n                nn.Conv2d(fpn_dim_lateral[0], fpn_dim, 1, 1, 0, bias=False),\n                nn.GroupNorm(net_utils.get_group_gn(fpn_dim), fpn_dim,\n                             eps=cfg.GROUP_NORM.EPSILON)\n            )\n        else:\n            self.conv_top = nn.Conv2d(fpn_dim_lateral[0], fpn_dim, 1, 1, 0)\n        self.topdown_lateral_modules = nn.ModuleList()\n        self.posthoc_modules = nn.ModuleList()\n\n        # For other levels add top-down and lateral connections\n        for i in range(self.num_backbone_stages - 1):\n            self.topdown_lateral_modules.append(\n                topdown_lateral_module(fpn_dim, fpn_dim_lateral[i+1])\n            )\n\n        # Post-hoc scale-specific 3x3 convs\n        for i in range(self.num_backbone_stages):\n            if cfg.FPN.USE_GN:\n                self.posthoc_modules.append(nn.Sequential(\n                    nn.Conv2d(fpn_dim, fpn_dim, 3, 1, 1, bias=False),\n                    nn.GroupNorm(net_utils.get_group_gn(fpn_dim), fpn_dim,\n                                 eps=cfg.GROUP_NORM.EPSILON)\n                ))\n            else:\n                self.posthoc_modules.append(\n                    nn.Conv2d(fpn_dim, fpn_dim, 3, 1, 1)\n                )\n\n            self.spatial_scale.append(fpn_level_info.spatial_scales[i])\n\n        #\n        # Step 2: build up starting from the coarsest backbone level\n        #\n        # Check if we need the P6 feature map\n        if not cfg.FPN.EXTRA_CONV_LEVELS and max_level == HIGHEST_BACKBONE_LVL + 1:\n            # Original FPN P6 level implementation from our CVPR\'17 FPN paper\n            # Use max pooling to simulate stride 2 subsampling\n            self.maxpool_p6 = nn.MaxPool2d(kernel_size=1, stride=2, padding=0)\n            self.spatial_scale.insert(0, self.spatial_scale[0] * 0.5)\n\n        # Coarser FPN levels introduced for RetinaNet\n        if cfg.FPN.EXTRA_CONV_LEVELS and max_level > HIGHEST_BACKBONE_LVL:\n            self.extra_pyramid_modules = nn.ModuleList()\n            dim_in = fpn_level_info.dims[0]\n            for i in range(HIGHEST_BACKBONE_LVL + 1, max_level + 1):\n                self.extra_pyramid_modules(\n                    nn.Conv2d(dim_in, fpn_dim, 3, 2, 1)\n                )\n                dim_in = fpn_dim\n                self.spatial_scale.insert(0, self.spatial_scale[0] * 0.5)\n\n        if self.P2only:\n            # use only the finest level\n            self.spatial_scale = self.spatial_scale[-1]\n\n        self._init_weights()\n\n        # Deliberately add conv_body after _init_weights.\n        # conv_body has its own _init_weights function\n        self.conv_body = conv_body_func()  # e.g resnet\n\n    def _init_weights(self):\n        def init_func(m):\n            if isinstance(m, nn.Conv2d):\n                mynn.init.XavierFill(m.weight)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n        for child_m in self.children():\n            if (not isinstance(child_m, nn.ModuleList) or\n                not isinstance(child_m[0], topdown_lateral_module)):\n                # topdown_lateral_module has its own init method\n                child_m.apply(init_func)\n\n    def detectron_weight_mapping(self):\n        conv_body_mapping, orphan_in_detectron = self.conv_body.detectron_weight_mapping()\n        mapping_to_detectron = {}\n        for key, value in conv_body_mapping.items():\n            mapping_to_detectron[\'conv_body.\'+key] = value\n\n        d_prefix = \'fpn_inner_\' + self.fpn_level_info.blobs[0]\n        if cfg.FPN.USE_GN:\n            mapping_to_detectron[\'conv_top.0.weight\'] = d_prefix + \'_w\'\n            mapping_to_detectron[\'conv_top.1.weight\'] = d_prefix + \'_gn_s\'\n            mapping_to_detectron[\'conv_top.1.bias\'] = d_prefix + \'_gn_b\'\n        else:\n            mapping_to_detectron[\'conv_top.weight\'] = d_prefix + \'_w\'\n            mapping_to_detectron[\'conv_top.bias\'] = d_prefix + \'_b\'\n        for i in range(self.num_backbone_stages - 1):\n            p_prefix = \'topdown_lateral_modules.%d.conv_lateral\' % i\n            d_prefix = \'fpn_inner_\' + self.fpn_level_info.blobs[i+1] + \'_lateral\'\n            if cfg.FPN.USE_GN:\n                mapping_to_detectron.update({\n                    p_prefix + \'.0.weight\' : d_prefix + \'_w\',\n                    p_prefix + \'.1.weight\' : d_prefix + \'_gn_s\',\n                    p_prefix + \'.1.bias\': d_prefix + \'_gn_b\'\n                })\n            else:\n                mapping_to_detectron.update({\n                    p_prefix + \'.weight\' : d_prefix + \'_w\',\n                    p_prefix + \'.bias\': d_prefix + \'_b\'\n                })\n\n        for i in range(self.num_backbone_stages):\n            p_prefix = \'posthoc_modules.%d\' % i\n            d_prefix = \'fpn_\' + self.fpn_level_info.blobs[i]\n            if cfg.FPN.USE_GN:\n                mapping_to_detectron.update({\n                    p_prefix + \'.0.weight\': d_prefix + \'_w\',\n                    p_prefix + \'.1.weight\': d_prefix + \'_gn_s\',\n                    p_prefix + \'.1.bias\': d_prefix + \'_gn_b\'\n                })\n            else:\n                mapping_to_detectron.update({\n                    p_prefix + \'.weight\': d_prefix + \'_w\',\n                    p_prefix + \'.bias\': d_prefix + \'_b\'\n                })\n\n        if hasattr(self, \'extra_pyramid_modules\'):\n            for i in len(self.extra_pyramid_modules):\n                p_prefix = \'extra_pyramid_modules.%d\' % i\n                d_prefix = \'fpn_%d\' % (HIGHEST_BACKBONE_LVL + 1 + i)\n                mapping_to_detectron.update({\n                    p_prefix + \'.weight\': d_prefix + \'_w\',\n                    p_prefix + \'.bias\': d_prefix + \'_b\'\n                })\n\n        return mapping_to_detectron, orphan_in_detectron\n\n    def forward(self, x):\n        conv_body_blobs = [self.conv_body.res1(x)]\n        for i in range(1, self.conv_body.convX):\n            conv_body_blobs.append(\n                getattr(self.conv_body, \'res%d\' % (i+1))(conv_body_blobs[-1])\n            )\n        fpn_inner_blobs = [self.conv_top(conv_body_blobs[-1])]\n        for i in range(self.num_backbone_stages - 1):\n            fpn_inner_blobs.append(\n                self.topdown_lateral_modules[i](fpn_inner_blobs[-1], conv_body_blobs[-(i+2)])\n            )\n        fpn_output_blobs = []\n        for i in range(self.num_backbone_stages):\n            fpn_output_blobs.append(\n                self.posthoc_modules[i](fpn_inner_blobs[i])\n            )\n\n        if hasattr(self, \'maxpool_p6\'):\n            fpn_output_blobs.insert(0, self.maxpool_p6(fpn_output_blobs[0]))\n\n        if hasattr(self, \'extra_pyramid_modules\'):\n            blob_in = conv_body_blobs[-1]\n            fpn_output_blobs.insert(0, self.extra_pyramid_modules(blob_in))\n            for module in self.extra_pyramid_modules[1:]:\n                fpn_output_blobs.insert(0, module(F.relu(fpn_output_blobs[0], inplace=True)))\n\n        if self.P2only:\n            # use only the finest level\n            return fpn_output_blobs[-1]\n        else:\n            # use all levels\n            return fpn_output_blobs\n\n\nclass topdown_lateral_module(nn.Module):\n    """"""Add a top-down lateral module.""""""\n    def __init__(self, dim_in_top, dim_in_lateral):\n        super().__init__()\n        self.dim_in_top = dim_in_top\n        self.dim_in_lateral = dim_in_lateral\n        self.dim_out = dim_in_top\n        if cfg.FPN.USE_GN:\n            self.conv_lateral = nn.Sequential(\n                nn.Conv2d(dim_in_lateral, self.dim_out, 1, 1, 0, bias=False),\n                nn.GroupNorm(net_utils.get_group_gn(self.dim_out), self.dim_out,\n                             eps=cfg.GROUP_NORM.EPSILON)\n            )\n        else:\n            self.conv_lateral = nn.Conv2d(dim_in_lateral, self.dim_out, 1, 1, 0)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        if cfg.FPN.USE_GN:\n            conv = self.conv_lateral[0]\n        else:\n            conv = self.conv_lateral\n\n        if cfg.FPN.ZERO_INIT_LATERAL:\n            init.constant_(conv.weight, 0)\n        else:\n            mynn.init.XavierFill(conv.weight)\n        if conv.bias is not None:\n            init.constant_(conv.bias, 0)\n\n    def forward(self, top_blob, lateral_blob):\n        # Lateral 1x1 conv\n        lat = self.conv_lateral(lateral_blob)\n        # Top-down 2x upsampling\n        # td = F.upsample(top_blob, size=lat.size()[2:], mode=\'bilinear\')\n        td = F.upsample(top_blob, scale_factor=2, mode=\'nearest\')\n        # Sum lateral and top-down\n        return lat + td\n\n\ndef get_min_max_levels():\n    """"""The min and max FPN levels required for supporting RPN and/or RoI\n    transform operations on multiple FPN levels.\n    """"""\n    min_level = LOWEST_BACKBONE_LVL\n    max_level = HIGHEST_BACKBONE_LVL\n    if cfg.FPN.MULTILEVEL_RPN and not cfg.FPN.MULTILEVEL_ROIS:\n        max_level = cfg.FPN.RPN_MAX_LEVEL\n        min_level = cfg.FPN.RPN_MIN_LEVEL\n    if not cfg.FPN.MULTILEVEL_RPN and cfg.FPN.MULTILEVEL_ROIS:\n        max_level = cfg.FPN.ROI_MAX_LEVEL\n        min_level = cfg.FPN.ROI_MIN_LEVEL\n    if cfg.FPN.MULTILEVEL_RPN and cfg.FPN.MULTILEVEL_ROIS:\n        max_level = max(cfg.FPN.RPN_MAX_LEVEL, cfg.FPN.ROI_MAX_LEVEL)\n        min_level = min(cfg.FPN.RPN_MIN_LEVEL, cfg.FPN.ROI_MIN_LEVEL)\n    return min_level, max_level\n\n\n# ---------------------------------------------------------------------------- #\n# RPN with an FPN backbone\n# ---------------------------------------------------------------------------- #\n\nclass fpn_rpn_outputs(nn.Module):\n    """"""Add RPN on FPN specific outputs.""""""\n    def __init__(self, dim_in, spatial_scales):\n        super().__init__()\n        self.dim_in = dim_in\n        self.spatial_scales = spatial_scales\n        self.dim_out = self.dim_in\n        num_anchors = len(cfg.FPN.RPN_ASPECT_RATIOS)\n\n        # Create conv ops shared by all FPN levels\n        self.FPN_RPN_conv = nn.Conv2d(dim_in, self.dim_out, 3, 1, 1)\n        dim_score = num_anchors * 2 if cfg.RPN.CLS_ACTIVATION == \'softmax\' \\\n            else num_anchors\n        self.FPN_RPN_cls_score = nn.Conv2d(self.dim_out, dim_score, 1, 1, 0)\n        self.FPN_RPN_bbox_pred = nn.Conv2d(self.dim_out, 4 * num_anchors, 1, 1, 0)\n\n        self.GenerateProposals_modules = nn.ModuleList()\n        k_max = cfg.FPN.RPN_MAX_LEVEL  # coarsest level of pyramid\n        k_min = cfg.FPN.RPN_MIN_LEVEL  # finest level of pyramid\n        for lvl in range(k_min, k_max + 1):\n            sc = self.spatial_scales[k_max - lvl]  # in reversed order\n            lvl_anchors = generate_anchors(\n                stride=2.**lvl,\n                sizes=(cfg.FPN.RPN_ANCHOR_START_SIZE * 2.**(lvl - k_min), ),\n                aspect_ratios=cfg.FPN.RPN_ASPECT_RATIOS\n            )\n            self.GenerateProposals_modules.append(GenerateProposalsOp(lvl_anchors, sc))\n\n        self.CollectAndDistributeFpnRpnProposals = CollectAndDistributeFpnRpnProposalsOp()\n\n        self._init_weights()\n\n    def _init_weights(self):\n        init.normal_(self.FPN_RPN_conv.weight, std=0.01)\n        init.constant_(self.FPN_RPN_conv.bias, 0)\n        init.normal_(self.FPN_RPN_cls_score.weight, std=0.01)\n        init.constant_(self.FPN_RPN_cls_score.bias, 0)\n        init.normal_(self.FPN_RPN_bbox_pred.weight, std=0.01)\n        init.constant_(self.FPN_RPN_bbox_pred.bias, 0)\n\n    def detectron_weight_mapping(self):\n        k_min = cfg.FPN.RPN_MIN_LEVEL\n        mapping_to_detectron = {\n            \'FPN_RPN_conv.weight\': \'conv_rpn_fpn%d_w\' % k_min,\n            \'FPN_RPN_conv.bias\': \'conv_rpn_fpn%d_b\' % k_min,\n            \'FPN_RPN_cls_score.weight\': \'rpn_cls_logits_fpn%d_w\' % k_min,\n            \'FPN_RPN_cls_score.bias\': \'rpn_cls_logits_fpn%d_b\' % k_min,\n            \'FPN_RPN_bbox_pred.weight\': \'rpn_bbox_pred_fpn%d_w\' % k_min,\n            \'FPN_RPN_bbox_pred.bias\': \'rpn_bbox_pred_fpn%d_b\' % k_min\n        }\n        return mapping_to_detectron, []\n\n    def forward(self, blobs_in, im_info, roidb=None):\n        k_max = cfg.FPN.RPN_MAX_LEVEL  # coarsest level of pyramid\n        k_min = cfg.FPN.RPN_MIN_LEVEL  # finest level of pyramid\n        assert len(blobs_in) == k_max - k_min + 1\n        return_dict = {}\n        rois_blobs = []\n        score_blobs = []\n        for lvl in range(k_min, k_max + 1):\n            slvl = str(lvl)\n            bl_in = blobs_in[k_max - lvl]  # blobs_in is in reversed order\n\n            fpn_rpn_conv = F.relu(self.FPN_RPN_conv(bl_in), inplace=True)\n            fpn_rpn_cls_score = self.FPN_RPN_cls_score(fpn_rpn_conv)\n            fpn_rpn_bbox_pred = self.FPN_RPN_bbox_pred(fpn_rpn_conv)\n            return_dict[\'rpn_cls_logits_fpn\' + slvl] = fpn_rpn_cls_score\n            return_dict[\'rpn_bbox_pred_fpn\' + slvl] = fpn_rpn_bbox_pred\n\n            if not self.training or cfg.MODEL.FASTER_RCNN:\n                # Proposals are needed during:\n                #  1) inference (== not model.train) for RPN only and Faster R-CNN\n                #  OR\n                #  2) training for Faster R-CNN\n                # Otherwise (== training for RPN only), proposals are not needed\n                if cfg.RPN.CLS_ACTIVATION == \'softmax\':\n                    B, C, H, W = fpn_rpn_cls_score.size()\n                    fpn_rpn_cls_probs = F.softmax(\n                        fpn_rpn_cls_score.view(B, 2, C // 2, H, W), dim=1)\n                    fpn_rpn_cls_probs = fpn_rpn_cls_probs[:, 1].squeeze(dim=1)\n                else:  # sigmoid\n                    fpn_rpn_cls_probs = F.sigmoid(fpn_rpn_cls_score)\n\n                fpn_rpn_rois, fpn_rpn_roi_probs = self.GenerateProposals_modules[lvl - k_min](\n                    fpn_rpn_cls_probs, fpn_rpn_bbox_pred, im_info)\n                rois_blobs.append(fpn_rpn_rois)\n                score_blobs.append(fpn_rpn_roi_probs)\n                return_dict[\'rpn_rois_fpn\' + slvl] = fpn_rpn_rois\n                return_dict[\'rpn_rois_prob_fpn\' + slvl] = fpn_rpn_roi_probs\n\n        if cfg.MODEL.FASTER_RCNN:\n            # CollectAndDistributeFpnRpnProposals also labels proposals when in training mode\n            blobs_out = self.CollectAndDistributeFpnRpnProposals(rois_blobs + score_blobs, roidb, im_info)\n            return_dict.update(blobs_out)\n\n        return return_dict\n\n\ndef fpn_rpn_losses(**kwargs):\n    """"""Add RPN on FPN specific losses.""""""\n    losses_cls = []\n    losses_bbox = []\n    for lvl in range(cfg.FPN.RPN_MIN_LEVEL, cfg.FPN.RPN_MAX_LEVEL + 1):\n        slvl = str(lvl)\n        # Spatially narrow the full-sized RPN label arrays to match the feature map shape\n        b, c, h, w = kwargs[\'rpn_cls_logits_fpn\' + slvl].shape\n        rpn_labels_int32_fpn = kwargs[\'rpn_labels_int32_wide_fpn\' + slvl][:, :, :h, :w]\n        h, w = kwargs[\'rpn_bbox_pred_fpn\' + slvl].shape[2:]\n        rpn_bbox_targets_fpn = kwargs[\'rpn_bbox_targets_wide_fpn\' + slvl][:, :, :h, :w]\n        rpn_bbox_inside_weights_fpn = kwargs[\n            \'rpn_bbox_inside_weights_wide_fpn\' + slvl][:, :, :h, :w]\n        rpn_bbox_outside_weights_fpn = kwargs[\n            \'rpn_bbox_outside_weights_wide_fpn\' + slvl][:, :, :h, :w]\n\n        if cfg.RPN.CLS_ACTIVATION == \'softmax\':\n            rpn_cls_logits_fpn = kwargs[\'rpn_cls_logits_fpn\' + slvl].view(\n                b, 2, c // 2, h, w).permute(0, 2, 3, 4, 1).contiguous().view(-1, 2)\n            rpn_labels_int32_fpn = rpn_labels_int32_fpn.contiguous().view(-1).long()\n            # the loss is averaged over non-ignored targets\n            loss_rpn_cls_fpn = F.cross_entropy(\n                rpn_cls_logits_fpn, rpn_labels_int32_fpn, ignore_index=-1)\n        else:  # sigmoid\n            weight = (rpn_labels_int32_fpn >= 0).float()\n            loss_rpn_cls_fpn = F.binary_cross_entropy_with_logits(\n                kwargs[\'rpn_cls_logits_fpn\' + slvl], rpn_labels_int32_fpn.float(), weight,\n                size_average=False)\n            loss_rpn_cls_fpn /= cfg.TRAIN.RPN_BATCH_SIZE_PER_IM * cfg.TRAIN.IMS_PER_BATCH\n\n        # Normalization by (1) RPN_BATCH_SIZE_PER_IM and (2) IMS_PER_BATCH is\n        # handled by (1) setting bbox outside weights and (2) SmoothL1Loss\n        # normalizes by IMS_PER_BATCH\n        loss_rpn_bbox_fpn = net_utils.smooth_l1_loss(\n            kwargs[\'rpn_bbox_pred_fpn\' + slvl], rpn_bbox_targets_fpn,\n            rpn_bbox_inside_weights_fpn, rpn_bbox_outside_weights_fpn,\n            beta=1/9)\n\n        losses_cls.append(loss_rpn_cls_fpn)\n        losses_bbox.append(loss_rpn_bbox_fpn)\n\n    return losses_cls, losses_bbox\n\n\n# ---------------------------------------------------------------------------- #\n# FPN level info for stages 5, 4, 3, 2 for select models (more can be added)\n# ---------------------------------------------------------------------------- #\n\nFpnLevelInfo = collections.namedtuple(\n    \'FpnLevelInfo\',\n    [\'blobs\', \'dims\', \'spatial_scales\']\n)\n\n\ndef fpn_level_info_ResNet50_conv5():\n    return FpnLevelInfo(\n        blobs=(\'res5_2_sum\', \'res4_5_sum\', \'res3_3_sum\', \'res2_2_sum\'),\n        dims=(2048, 1024, 512, 256),\n        spatial_scales=(1. / 32., 1. / 16., 1. / 8., 1. / 4.)\n    )\n\n\ndef fpn_level_info_ResNet101_conv5():\n    return FpnLevelInfo(\n        blobs=(\'res5_2_sum\', \'res4_22_sum\', \'res3_3_sum\', \'res2_2_sum\'),\n        dims=(2048, 1024, 512, 256),\n        spatial_scales=(1. / 32., 1. / 16., 1. / 8., 1. / 4.)\n    )\n\n\ndef fpn_level_info_ResNet152_conv5():\n    return FpnLevelInfo(\n        blobs=(\'res5_2_sum\', \'res4_35_sum\', \'res3_7_sum\', \'res2_2_sum\'),\n        dims=(2048, 1024, 512, 256),\n        spatial_scales=(1. / 32., 1. / 16., 1. / 8., 1. / 4.)\n    )\n'"
lib/modeling/ResNet.py,2,"b'import os\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom core.config import cfg\nimport nn as mynn\nimport utils.net as net_utils\nfrom utils.resnet_weights_helper import convert_state_dict\n\n# ---------------------------------------------------------------------------- #\n# Bits for specific architectures (ResNet50, ResNet101, ...)\n# ---------------------------------------------------------------------------- #\n\ndef ResNet50_conv4_body():\n    return ResNet_convX_body((3, 4, 6))\n\n\ndef ResNet50_conv5_body():\n    return ResNet_convX_body((3, 4, 6, 3))\n\n\ndef ResNet101_conv4_body():\n    return ResNet_convX_body((3, 4, 23))\n\n\ndef ResNet101_conv5_body():\n    return ResNet_convX_body((3, 4, 23, 3))\n\n\ndef ResNet152_conv5_body():\n    return ResNet_convX_body((3, 8, 36, 3))\n\n\n# ---------------------------------------------------------------------------- #\n# Generic ResNet components\n# ---------------------------------------------------------------------------- #\n\n\nclass ResNet_convX_body(nn.Module):\n    def __init__(self, block_counts):\n        super().__init__()\n        self.block_counts = block_counts\n        self.convX = len(block_counts) + 1\n        self.num_layers = (sum(block_counts) + 3 * (self.convX == 4)) * 3 + 2\n\n        self.res1 = globals()[cfg.RESNETS.STEM_FUNC]()\n        dim_in = 64\n        dim_bottleneck = cfg.RESNETS.NUM_GROUPS * cfg.RESNETS.WIDTH_PER_GROUP\n        self.res2, dim_in = add_stage(dim_in, 256, dim_bottleneck, block_counts[0],\n                                      dilation=1, stride_init=1)\n        self.res3, dim_in = add_stage(dim_in, 512, dim_bottleneck * 2, block_counts[1],\n                                      dilation=1, stride_init=2)\n        self.res4, dim_in = add_stage(dim_in, 1024, dim_bottleneck * 4, block_counts[2],\n                                      dilation=1, stride_init=2)\n        if len(block_counts) == 4:\n            stride_init = 2 if cfg.RESNETS.RES5_DILATION == 1 else 1\n            self.res5, dim_in = add_stage(dim_in, 2048, dim_bottleneck * 8, block_counts[3],\n                                          cfg.RESNETS.RES5_DILATION, stride_init)\n            self.spatial_scale = 1 / 32 * cfg.RESNETS.RES5_DILATION\n        else:\n            self.spatial_scale = 1 / 16  # final feature scale wrt. original image scale\n\n        self.dim_out = dim_in\n\n        self._init_modules()\n\n    def _init_modules(self):\n        assert cfg.RESNETS.FREEZE_AT in [0, 2, 3, 4, 5]\n        assert cfg.RESNETS.FREEZE_AT <= self.convX\n        for i in range(1, cfg.RESNETS.FREEZE_AT + 1):\n            freeze_params(getattr(self, \'res%d\' % i))\n\n        # Freeze all bn (affine) layers !!!\n        self.apply(lambda m: freeze_params(m) if isinstance(m, mynn.AffineChannel2d) else None)\n\n    def detectron_weight_mapping(self):\n        if cfg.RESNETS.USE_GN:\n            mapping_to_detectron = {\n                \'res1.conv1.weight\': \'conv1_w\',\n                \'res1.gn1.weight\': \'conv1_gn_s\',\n                \'res1.gn1.bias\': \'conv1_gn_b\',\n            }\n            orphan_in_detectron = [\'pred_w\', \'pred_b\']\n        else:\n            mapping_to_detectron = {\n                \'res1.conv1.weight\': \'conv1_w\',\n                \'res1.bn1.weight\': \'res_conv1_bn_s\',\n                \'res1.bn1.bias\': \'res_conv1_bn_b\',\n            }\n            orphan_in_detectron = [\'conv1_b\', \'fc1000_w\', \'fc1000_b\']\n\n        for res_id in range(2, self.convX + 1):\n            stage_name = \'res%d\' % res_id\n            mapping, orphans = residual_stage_detectron_mapping(\n                getattr(self, stage_name), stage_name,\n                self.block_counts[res_id - 2], res_id)\n            mapping_to_detectron.update(mapping)\n            orphan_in_detectron.extend(orphans)\n\n        return mapping_to_detectron, orphan_in_detectron\n\n    def train(self, mode=True):\n        # Override\n        self.training = mode\n\n        for i in range(cfg.RESNETS.FREEZE_AT + 1, self.convX + 1):\n            getattr(self, \'res%d\' % i).train(mode)\n\n    def forward(self, x):\n        for i in range(self.convX):\n            x = getattr(self, \'res%d\' % (i + 1))(x)\n        return x\n\n\nclass ResNet_roi_conv5_head(nn.Module):\n    def __init__(self, dim_in, roi_xform_func, spatial_scale):\n        super().__init__()\n        self.roi_xform = roi_xform_func\n        self.spatial_scale = spatial_scale\n\n        dim_bottleneck = cfg.RESNETS.NUM_GROUPS * cfg.RESNETS.WIDTH_PER_GROUP\n        stride_init = cfg.FAST_RCNN.ROI_XFORM_RESOLUTION // 7\n        self.res5, self.dim_out = add_stage(dim_in, 2048, dim_bottleneck * 8, 3,\n                                            dilation=1, stride_init=stride_init)\n        self.avgpool = nn.AvgPool2d(7)\n\n        self._init_modules()\n\n    def _init_modules(self):\n        # Freeze all bn (affine) layers !!!\n        self.apply(lambda m: freeze_params(m) if isinstance(m, mynn.AffineChannel2d) else None)\n\n    def detectron_weight_mapping(self):\n        mapping_to_detectron, orphan_in_detectron = \\\n          residual_stage_detectron_mapping(self.res5, \'res5\', 3, 5)\n        return mapping_to_detectron, orphan_in_detectron\n\n    def forward(self, x, rpn_ret):\n        x = self.roi_xform(\n            x, rpn_ret,\n            blob_rois=\'rois\',\n            method=cfg.FAST_RCNN.ROI_XFORM_METHOD,\n            resolution=cfg.FAST_RCNN.ROI_XFORM_RESOLUTION,\n            spatial_scale=self.spatial_scale,\n            sampling_ratio=cfg.FAST_RCNN.ROI_XFORM_SAMPLING_RATIO\n        )\n        res5_feat = self.res5(x)\n        x = self.avgpool(res5_feat)\n        if cfg.MODEL.SHARE_RES5 and self.training:\n            return x, res5_feat\n        else:\n            return x\n\n\ndef add_stage(inplanes, outplanes, innerplanes, nblocks, dilation=1, stride_init=2):\n    """"""Make a stage consist of `nblocks` residual blocks.\n    Returns:\n        - stage module: an nn.Sequentail module of residual blocks\n        - final output dimension\n    """"""\n    res_blocks = []\n    stride = stride_init\n    for _ in range(nblocks):\n        res_blocks.append(add_residual_block(\n            inplanes, outplanes, innerplanes, dilation, stride\n        ))\n        inplanes = outplanes\n        stride = 1\n\n    return nn.Sequential(*res_blocks), outplanes\n\n\ndef add_residual_block(inplanes, outplanes, innerplanes, dilation, stride):\n    """"""Return a residual block module, including residual connection, """"""\n    if stride != 1 or inplanes != outplanes:\n        shortcut_func = globals()[cfg.RESNETS.SHORTCUT_FUNC]\n        downsample = shortcut_func(inplanes, outplanes, stride)\n    else:\n        downsample = None\n\n    trans_func = globals()[cfg.RESNETS.TRANS_FUNC]\n    res_block = trans_func(\n        inplanes, outplanes, innerplanes, stride,\n        dilation=dilation, group=cfg.RESNETS.NUM_GROUPS,\n        downsample=downsample)\n\n    return res_block\n\n\n# ------------------------------------------------------------------------------\n# various downsample shortcuts (may expand and may consider a new helper)\n# ------------------------------------------------------------------------------\n\ndef basic_bn_shortcut(inplanes, outplanes, stride):\n    return nn.Sequential(\n        nn.Conv2d(inplanes,\n                  outplanes,\n                  kernel_size=1,\n                  stride=stride,\n                  bias=False),\n        mynn.AffineChannel2d(outplanes),\n    )\n\n\ndef basic_gn_shortcut(inplanes, outplanes, stride):\n    return nn.Sequential(\n        nn.Conv2d(inplanes,\n                  outplanes,\n                  kernel_size=1,\n                  stride=stride,\n                  bias=False),\n        nn.GroupNorm(net_utils.get_group_gn(outplanes), outplanes,\n                     eps=cfg.GROUP_NORM.EPSILON)\n    )\n\n\n# ------------------------------------------------------------------------------\n# various stems (may expand and may consider a new helper)\n# ------------------------------------------------------------------------------\n\ndef basic_bn_stem():\n    return nn.Sequential(OrderedDict([\n        (\'conv1\', nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False)),\n        (\'bn1\', mynn.AffineChannel2d(64)),\n        (\'relu\', nn.ReLU(inplace=True)),\n        # (\'maxpool\', nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True))]))\n        (\'maxpool\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))]))\n\n\ndef basic_gn_stem():\n    return nn.Sequential(OrderedDict([\n        (\'conv1\', nn.Conv2d(3, 64, 7, stride=2, padding=3, bias=False)),\n        (\'gn1\', nn.GroupNorm(net_utils.get_group_gn(64), 64,\n                             eps=cfg.GROUP_NORM.EPSILON)),\n        (\'relu\', nn.ReLU(inplace=True)),\n        (\'maxpool\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1))]))\n\n\n# ------------------------------------------------------------------------------\n# various transformations (may expand and may consider a new helper)\n# ------------------------------------------------------------------------------\n\nclass bottleneck_transformation(nn.Module):\n    """""" Bottleneck Residual Block """"""\n\n    def __init__(self, inplanes, outplanes, innerplanes, stride=1, dilation=1, group=1,\n                 downsample=None):\n        super().__init__()\n        # In original resnet, stride=2 is on 1x1.\n        # In fb.torch resnet, stride=2 is on 3x3.\n        (str1x1, str3x3) = (stride, 1) if cfg.RESNETS.STRIDE_1X1 else (1, stride)\n        self.stride = stride\n\n        self.conv1 = nn.Conv2d(\n            inplanes, innerplanes, kernel_size=1, stride=str1x1, bias=False)\n        self.bn1 = mynn.AffineChannel2d(innerplanes)\n\n        self.conv2 = nn.Conv2d(\n            innerplanes, innerplanes, kernel_size=3, stride=str3x3, bias=False,\n            padding=1 * dilation, dilation=dilation, groups=group)\n        self.bn2 = mynn.AffineChannel2d(innerplanes)\n\n        self.conv3 = nn.Conv2d(\n            innerplanes, outplanes, kernel_size=1, stride=1, bias=False)\n        self.bn3 = mynn.AffineChannel2d(outplanes)\n\n        self.downsample = downsample\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass bottleneck_gn_transformation(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, outplanes, innerplanes, stride=1, dilation=1, group=1,\n                 downsample=None):\n        super().__init__()\n        # In original resnet, stride=2 is on 1x1.\n        # In fb.torch resnet, stride=2 is on 3x3.\n        (str1x1, str3x3) = (stride, 1) if cfg.RESNETS.STRIDE_1X1 else (1, stride)\n        self.stride = stride\n\n        self.conv1 = nn.Conv2d(\n            inplanes, innerplanes, kernel_size=1, stride=str1x1, bias=False)\n        self.gn1 = nn.GroupNorm(net_utils.get_group_gn(innerplanes), innerplanes,\n                                eps=cfg.GROUP_NORM.EPSILON)\n\n        self.conv2 = nn.Conv2d(\n            innerplanes, innerplanes, kernel_size=3, stride=str3x3, bias=False,\n            padding=1 * dilation, dilation=dilation, groups=group)\n        self.gn2 = nn.GroupNorm(net_utils.get_group_gn(innerplanes), innerplanes,\n                                eps=cfg.GROUP_NORM.EPSILON)\n\n        self.conv3 = nn.Conv2d(\n            innerplanes, outplanes, kernel_size=1, stride=1, bias=False)\n        self.gn3 = nn.GroupNorm(net_utils.get_group_gn(outplanes), outplanes,\n                                eps=cfg.GROUP_NORM.EPSILON)\n\n        self.downsample = downsample\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.gn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.gn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.gn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\n# ---------------------------------------------------------------------------- #\n# Helper functions\n# ---------------------------------------------------------------------------- #\n\ndef residual_stage_detectron_mapping(module_ref, module_name, num_blocks, res_id):\n    """"""Construct weight mapping relation for a residual stage with `num_blocks` of\n    residual blocks given the stage id: `res_id`\n    """"""\n    if cfg.RESNETS.USE_GN:\n        norm_suffix = \'_gn\'\n    else:\n        norm_suffix = \'_bn\'\n    mapping_to_detectron = {}\n    orphan_in_detectron = []\n    for blk_id in range(num_blocks):\n        detectron_prefix = \'res%d_%d\' % (res_id, blk_id)\n        my_prefix = \'%s.%d\' % (module_name, blk_id)\n\n        # residual branch (if downsample is not None)\n        if getattr(module_ref[blk_id], \'downsample\'):\n            dtt_bp = detectron_prefix + \'_branch1\'  # short for ""detectron_branch_prefix""\n            mapping_to_detectron[my_prefix\n                                 + \'.downsample.0.weight\'] = dtt_bp + \'_w\'\n            orphan_in_detectron.append(dtt_bp + \'_b\')\n            mapping_to_detectron[my_prefix\n                                 + \'.downsample.1.weight\'] = dtt_bp + norm_suffix + \'_s\'\n            mapping_to_detectron[my_prefix\n                                 + \'.downsample.1.bias\'] = dtt_bp + norm_suffix + \'_b\'\n\n        # conv branch\n        for i, c in zip([1, 2, 3], [\'a\', \'b\', \'c\']):\n            dtt_bp = detectron_prefix + \'_branch2\' + c\n            mapping_to_detectron[my_prefix\n                                 + \'.conv%d.weight\' % i] = dtt_bp + \'_w\'\n            orphan_in_detectron.append(dtt_bp + \'_b\')\n            mapping_to_detectron[my_prefix\n                                 + \'.\' + norm_suffix[1:] + \'%d.weight\' % i] = dtt_bp + norm_suffix + \'_s\'\n            mapping_to_detectron[my_prefix\n                                 + \'.\' + norm_suffix[1:] + \'%d.bias\' % i] = dtt_bp + norm_suffix + \'_b\'\n\n    return mapping_to_detectron, orphan_in_detectron\n\n\ndef freeze_params(m):\n    """"""Freeze all the weights by setting requires_grad to False\n    """"""\n    for p in m.parameters():\n        p.requires_grad = False\n'"
lib/modeling/__init__.py,0,b''
lib/modeling/collect_and_distribute_fpn_rpn_proposals.py,0,"b'import numpy as np\nfrom torch import nn\n\nfrom core.config import cfg\nfrom datasets import json_dataset\nimport roi_data.fast_rcnn\nimport utils.blob as blob_utils\nimport utils.fpn as fpn_utils\n\n\nclass CollectAndDistributeFpnRpnProposalsOp(nn.Module):\n    """"""Merge RPN proposals generated at multiple FPN levels and then\n    distribute those proposals to their appropriate FPN levels. An anchor\n    at one FPN level may predict an RoI that will map to another level,\n    hence the need to redistribute the proposals.\n\n    This function assumes standard blob names for input and output blobs.\n\n    Input blobs: [rpn_rois_fpn<min>, ..., rpn_rois_fpn<max>,\n                  rpn_roi_probs_fpn<min>, ..., rpn_roi_probs_fpn<max>]\n        - rpn_rois_fpn<i> are the RPN proposals for FPN level i; see rpn_rois\n        documentation from GenerateProposals.\n        - rpn_roi_probs_fpn<i> are the RPN objectness probabilities for FPN\n        level i; see rpn_roi_probs documentation from GenerateProposals.\n\n    If used during training, then the input blobs will also include:\n        [roidb, im_info] (see GenerateProposalLabels).\n\n    Output blobs: [rois_fpn<min>, ..., rois_rpn<max>, rois,\n                   rois_idx_restore]\n        - rois_fpn<i> are the RPN proposals for FPN level i\n        - rois_idx_restore is a permutation on the concatenation of all\n        rois_fpn<i>, i=min...max, such that when applied the RPN RoIs are\n        restored to their original order in the input blobs.\n\n    If used during training, then the output blobs will also include:\n        [labels, bbox_targets, bbox_inside_weights, bbox_outside_weights].\n    """"""\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, inputs, roidb, im_info):\n        """"""\n        Args:\n            inputs: a list of [rpn_rois_fpn2, ..., rpn_rois_fpn6,\n                               rpn_roi_probs_fpn2, ..., rpn_roi_probs_fpn6]\n            im_info: [[im_height, im_width, im_scale], ...]\n        """"""\n        rois = collect(inputs, self.training)\n        if self.training:\n            # During training we reuse the data loader code. We populate roidb\n            # entries on the fly using the rois generated by RPN.\n            im_scales = im_info.data.numpy()[:, 2]\n            # For historical consistency with the original Faster R-CNN\n            # implementation we are *not* filtering crowd proposals.\n            # This choice should be investigated in the future (it likely does\n            # not matter).\n            json_dataset.add_proposals(roidb, rois, im_scales, crowd_thresh=0)\n            # Compute training labels for the RPN proposals; also handles\n            # distributing the proposals over FPN levels\n            output_blob_names = roi_data.fast_rcnn.get_fast_rcnn_blob_names()\n            blobs = {k: [] for k in output_blob_names}\n            roi_data.fast_rcnn.add_fast_rcnn_blobs(blobs, im_scales, roidb)\n        else:\n            # For inference we have a special code path that avoids some data\n            # loader overhead\n            blobs = distribute(rois, None)\n\n        return blobs\n\n\ndef collect(inputs, is_training):\n    cfg_key = \'TRAIN\' if is_training else \'TEST\'\n    post_nms_topN = int(cfg[cfg_key].RPN_POST_NMS_TOP_N * cfg.FPN.RPN_COLLECT_SCALE + 0.5)\n    k_max = cfg.FPN.RPN_MAX_LEVEL\n    k_min = cfg.FPN.RPN_MIN_LEVEL\n    num_lvls = k_max - k_min + 1\n    roi_inputs = inputs[:num_lvls]\n    score_inputs = inputs[num_lvls:]\n\n    # rois are in [[batch_idx, x0, y0, x1, y2], ...] format\n    # Combine predictions across all levels and retain the top scoring\n    rois = np.concatenate(roi_inputs)\n    scores = np.concatenate(score_inputs).squeeze()\n    inds = np.argsort(-scores)[:post_nms_topN]\n    rois = rois[inds, :]\n    return rois\n\n\ndef distribute(rois, label_blobs):\n    """"""To understand the output blob order see return value of\n    roi_data.fast_rcnn.get_fast_rcnn_blob_names(is_training=False)\n    """"""\n    lvl_min = cfg.FPN.ROI_MIN_LEVEL\n    lvl_max = cfg.FPN.ROI_MAX_LEVEL\n    lvls = fpn_utils.map_rois_to_fpn_levels(rois[:, 1:5], lvl_min, lvl_max)\n\n    # Delete roi entries that have negative area\n    # idx_neg = np.where(lvls == -1)[0]\n    # rois = np.delete(rois, idx_neg, axis=0)\n    # lvls = np.delete(lvls, idx_neg, axis=0)\n\n    output_blob_names = roi_data.fast_rcnn.get_fast_rcnn_blob_names(is_training=False)\n    outputs = [None] * len(output_blob_names)\n    outputs[0] = rois\n\n    # Create new roi blobs for each FPN level\n    # (See: utils.fpn.add_multilevel_roi_blobs which is similar but annoying\n    # to generalize to support this particular case.)\n    rois_idx_order = np.empty((0, ))\n    for output_idx, lvl in enumerate(range(lvl_min, lvl_max + 1)):\n        idx_lvl = np.where(lvls == lvl)[0]\n        blob_roi_level = rois[idx_lvl, :]\n        outputs[output_idx + 1] = blob_roi_level\n        rois_idx_order = np.concatenate((rois_idx_order, idx_lvl))\n    rois_idx_restore = np.argsort(rois_idx_order)\n    outputs[-1] = rois_idx_restore.astype(np.int32)\n\n    return dict(zip(output_blob_names, outputs))\n'"
lib/modeling/fast_rcnn_heads.py,8,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.autograd import Variable\n\nfrom core.config import cfg\nimport nn as mynn\nimport utils.net as net_utils\n\n\nclass fast_rcnn_outputs(nn.Module):\n    def __init__(self, dim_in):\n        super().__init__()\n        self.cls_score = nn.Linear(dim_in, cfg.MODEL.NUM_CLASSES)\n        if cfg.MODEL.CLS_AGNOSTIC_BBOX_REG:  # bg and fg\n            self.bbox_pred = nn.Linear(dim_in, 4 * 2)\n        else:\n            self.bbox_pred = nn.Linear(dim_in, 4 * cfg.MODEL.NUM_CLASSES)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        init.normal_(self.cls_score.weight, std=0.01)\n        init.constant_(self.cls_score.bias, 0)\n        init.normal_(self.bbox_pred.weight, std=0.001)\n        init.constant_(self.bbox_pred.bias, 0)\n\n    def detectron_weight_mapping(self):\n        detectron_weight_mapping = {\n            \'cls_score.weight\': \'cls_score_w\',\n            \'cls_score.bias\': \'cls_score_b\',\n            \'bbox_pred.weight\': \'bbox_pred_w\',\n            \'bbox_pred.bias\': \'bbox_pred_b\'\n        }\n        orphan_in_detectron = []\n        return detectron_weight_mapping, orphan_in_detectron\n\n    def forward(self, x):\n        if x.dim() == 4:\n            x = x.squeeze(3).squeeze(2)\n        cls_score = self.cls_score(x)\n        if not self.training:\n            cls_score = F.softmax(cls_score, dim=1)\n        bbox_pred = self.bbox_pred(x)\n\n        return cls_score, bbox_pred\n\n\ndef fast_rcnn_losses(cls_score, bbox_pred, label_int32, bbox_targets,\n                     bbox_inside_weights, bbox_outside_weights):\n    device_id = cls_score.get_device()\n    rois_label = Variable(torch.from_numpy(label_int32.astype(\'int64\'))).cuda(device_id)\n    loss_cls = F.cross_entropy(cls_score, rois_label)\n\n    bbox_targets = Variable(torch.from_numpy(bbox_targets)).cuda(device_id)\n    bbox_inside_weights = Variable(torch.from_numpy(bbox_inside_weights)).cuda(device_id)\n    bbox_outside_weights = Variable(torch.from_numpy(bbox_outside_weights)).cuda(device_id)\n    loss_bbox = net_utils.smooth_l1_loss(\n        bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights)\n\n    # class accuracy\n    cls_preds = cls_score.max(dim=1)[1].type_as(rois_label)\n    accuracy_cls = cls_preds.eq(rois_label).float().mean(dim=0)\n\n    return loss_cls, loss_bbox, accuracy_cls\n\n\n# ---------------------------------------------------------------------------- #\n# Box heads\n# ---------------------------------------------------------------------------- #\n\nclass roi_2mlp_head(nn.Module):\n    """"""Add a ReLU MLP with two hidden layers.""""""\n    def __init__(self, dim_in, roi_xform_func, spatial_scale):\n        super().__init__()\n        self.dim_in = dim_in\n        self.roi_xform = roi_xform_func\n        self.spatial_scale = spatial_scale\n        self.dim_out = hidden_dim = cfg.FAST_RCNN.MLP_HEAD_DIM\n\n        roi_size = cfg.FAST_RCNN.ROI_XFORM_RESOLUTION\n        self.fc1 = nn.Linear(dim_in * roi_size**2, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        mynn.init.XavierFill(self.fc1.weight)\n        init.constant_(self.fc1.bias, 0)\n        mynn.init.XavierFill(self.fc2.weight)\n        init.constant_(self.fc2.bias, 0)\n\n    def detectron_weight_mapping(self):\n        detectron_weight_mapping = {\n            \'fc1.weight\': \'fc6_w\',\n            \'fc1.bias\': \'fc6_b\',\n            \'fc2.weight\': \'fc7_w\',\n            \'fc2.bias\': \'fc7_b\'\n        }\n        return detectron_weight_mapping, []\n\n    def forward(self, x, rpn_ret):\n        x = self.roi_xform(\n            x, rpn_ret,\n            blob_rois=\'rois\',\n            method=cfg.FAST_RCNN.ROI_XFORM_METHOD,\n            resolution=cfg.FAST_RCNN.ROI_XFORM_RESOLUTION,\n            spatial_scale=self.spatial_scale,\n            sampling_ratio=cfg.FAST_RCNN.ROI_XFORM_SAMPLING_RATIO\n        )\n        batch_size = x.size(0)\n        x = F.relu(self.fc1(x.view(batch_size, -1)), inplace=True)\n        x = F.relu(self.fc2(x), inplace=True)\n\n        return x\n\n\nclass roi_Xconv1fc_head(nn.Module):\n    """"""Add a X conv + 1fc head, as a reference if not using GroupNorm""""""\n    def __init__(self, dim_in, roi_xform_func, spatial_scale):\n        super().__init__()\n        self.dim_in = dim_in\n        self.roi_xform = roi_xform_func\n        self.spatial_scale = spatial_scale\n\n        hidden_dim = cfg.FAST_RCNN.CONV_HEAD_DIM\n        module_list = []\n        for i in range(cfg.FAST_RCNN.NUM_STACKED_CONVS):\n            module_list.extend([\n                nn.Conv2d(dim_in, hidden_dim, 3, 1, 1),\n                nn.ReLU(inplace=True)\n            ])\n            dim_in = hidden_dim\n        self.convs = nn.Sequential(*module_list)\n\n        self.dim_out = fc_dim = cfg.FAST_RCNN.MLP_HEAD_DIM\n        roi_size = cfg.FAST_RCNN.ROI_XFORM_RESOLUTION\n        self.fc = nn.Linear(dim_in * roi_size * roi_size, fc_dim)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        def _init(m):\n            if isinstance(m, nn.Conv2d):\n                mynn.init.MSRAFill(m.weight)\n                init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                mynn.init.XavierFill(m.weight)\n                init.constant_(m.bias, 0)\n        self.apply(_init)\n\n    def detectron_weight_mapping(self):\n        mapping = {}\n        for i in range(cfg.FAST_RCNN.NUM_STACKED_CONVS):\n            mapping.update({\n                \'convs.%d.weight\' % (i*2): \'head_conv%d_w\' % (i+1),\n                \'convs.%d.bias\' % (i*2): \'head_conv%d_b\' % (i+1)\n            })\n        mapping.update({\n            \'fc.weight\': \'fc6_w\',\n            \'fc.bias\': \'fc6_b\'\n        })\n        return mapping, []\n\n    def forward(self, x, rpn_ret):\n        x = self.roi_xform(\n            x, rpn_ret,\n            blob_rois=\'rois\',\n            method=cfg.FAST_RCNN.ROI_XFORM_METHOD,\n            resolution=cfg.FAST_RCNN.ROI_XFORM_RESOLUTION,\n            spatial_scale=self.spatial_scale,\n            sampling_ratio=cfg.FAST_RCNN.ROI_XFORM_SAMPLING_RATIO\n        )\n        batch_size = x.size(0)\n        x = self.convs(x)\n        x = F.relu(self.fc(x.view(batch_size, -1)), inplace=True)\n        return x\n\n\nclass roi_Xconv1fc_gn_head(nn.Module):\n    """"""Add a X conv + 1fc head, with GroupNorm""""""\n    def __init__(self, dim_in, roi_xform_func, spatial_scale):\n        super().__init__()\n        self.dim_in = dim_in\n        self.roi_xform = roi_xform_func\n        self.spatial_scale = spatial_scale\n\n        hidden_dim = cfg.FAST_RCNN.CONV_HEAD_DIM\n        module_list = []\n        for i in range(cfg.FAST_RCNN.NUM_STACKED_CONVS):\n            module_list.extend([\n                nn.Conv2d(dim_in, hidden_dim, 3, 1, 1, bias=False),\n                nn.GroupNorm(net_utils.get_group_gn(hidden_dim), hidden_dim,\n                             eps=cfg.GROUP_NORM.EPSILON),\n                nn.ReLU(inplace=True)\n            ])\n            dim_in = hidden_dim\n        self.convs = nn.Sequential(*module_list)\n\n        self.dim_out = fc_dim = cfg.FAST_RCNN.MLP_HEAD_DIM\n        roi_size = cfg.FAST_RCNN.ROI_XFORM_RESOLUTION\n        self.fc = nn.Linear(dim_in * roi_size * roi_size, fc_dim)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        def _init(m):\n            if isinstance(m, nn.Conv2d):\n                mynn.init.MSRAFill(m.weight)\n            elif isinstance(m, nn.Linear):\n                mynn.init.XavierFill(m.weight)\n                init.constant_(m.bias, 0)\n        self.apply(_init)\n\n    def detectron_weight_mapping(self):\n        mapping = {}\n        for i in range(cfg.FAST_RCNN.NUM_STACKED_CONVS):\n            mapping.update({\n                \'convs.%d.weight\' % (i*3): \'head_conv%d_w\' % (i+1),\n                \'convs.%d.weight\' % (i*3+1): \'head_conv%d_gn_s\' % (i+1),\n                \'convs.%d.bias\' % (i*3+1): \'head_conv%d_gn_b\' % (i+1)\n            })\n        mapping.update({\n            \'fc.weight\': \'fc6_w\',\n            \'fc.bias\': \'fc6_b\'\n        })\n        return mapping, []\n\n    def forward(self, x, rpn_ret):\n        x = self.roi_xform(\n            x, rpn_ret,\n            blob_rois=\'rois\',\n            method=cfg.FAST_RCNN.ROI_XFORM_METHOD,\n            resolution=cfg.FAST_RCNN.ROI_XFORM_RESOLUTION,\n            spatial_scale=self.spatial_scale,\n            sampling_ratio=cfg.FAST_RCNN.ROI_XFORM_SAMPLING_RATIO\n        )\n        batch_size = x.size(0)\n        x = self.convs(x)\n        x = F.relu(self.fc(x.view(batch_size, -1)), inplace=True)\n        return x\n'"
lib/modeling/generate_anchors.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Faster R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick and Sean Bell\n# --------------------------------------------------------\n\nimport numpy as np\n\n# Verify that we compute the same anchors as Shaoqing\'s matlab implementation:\n#\n#    >> load output/rpn_cachedir/faster_rcnn_VOC2007_ZF_stage1_rpn/anchors.mat\n#    >> anchors\n#\n#    anchors =\n#\n#       -83   -39   100    56\n#      -175   -87   192   104\n#      -359  -183   376   200\n#       -55   -55    72    72\n#      -119  -119   136   136\n#      -247  -247   264   264\n#       -35   -79    52    96\n#       -79  -167    96   184\n#      -167  -343   184   360\n\n# array([[ -83.,  -39.,  100.,   56.],\n#        [-175.,  -87.,  192.,  104.],\n#        [-359., -183.,  376.,  200.],\n#        [ -55.,  -55.,   72.,   72.],\n#        [-119., -119.,  136.,  136.],\n#        [-247., -247.,  264.,  264.],\n#        [ -35.,  -79.,   52.,   96.],\n#        [ -79., -167.,   96.,  184.],\n#        [-167., -343.,  184.,  360.]])\n\n\ndef generate_anchors(\n    stride=16, sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.5, 1, 2)\n):\n    """"""Generates a matrix of anchor boxes in (x1, y1, x2, y2) format. Anchors\n    are centered on stride / 2, have (approximate) sqrt areas of the specified\n    sizes, and aspect ratios as given.\n    """"""\n    return _generate_anchors(\n        stride,\n        np.array(sizes, dtype=np.float) / stride,\n        np.array(aspect_ratios, dtype=np.float)\n    )\n\n\ndef _generate_anchors(base_size, scales, aspect_ratios):\n    """"""Generate anchor (reference) windows by enumerating aspect ratios X\n    scales wrt a reference (0, 0, base_size - 1, base_size - 1) window.\n    """"""\n    anchor = np.array([1, 1, base_size, base_size], dtype=np.float) - 1\n    anchors = _ratio_enum(anchor, aspect_ratios)\n    anchors = np.vstack(\n        [_scale_enum(anchors[i, :], scales) for i in range(anchors.shape[0])]\n    )\n    return anchors\n\n\ndef _whctrs(anchor):\n    """"""Return width, height, x center, and y center for an anchor (window).""""""\n    w = anchor[2] - anchor[0] + 1\n    h = anchor[3] - anchor[1] + 1\n    x_ctr = anchor[0] + 0.5 * (w - 1)\n    y_ctr = anchor[1] + 0.5 * (h - 1)\n    return w, h, x_ctr, y_ctr\n\n\ndef _mkanchors(ws, hs, x_ctr, y_ctr):\n    """"""Given a vector of widths (ws) and heights (hs) around a center\n    (x_ctr, y_ctr), output a set of anchors (windows).\n    """"""\n    ws = ws[:, np.newaxis]\n    hs = hs[:, np.newaxis]\n    anchors = np.hstack(\n        (\n            x_ctr - 0.5 * (ws - 1),\n            y_ctr - 0.5 * (hs - 1),\n            x_ctr + 0.5 * (ws - 1),\n            y_ctr + 0.5 * (hs - 1)\n        )\n    )\n    return anchors\n\n\ndef _ratio_enum(anchor, ratios):\n    """"""Enumerate a set of anchors for each aspect ratio wrt an anchor.""""""\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    size = w * h\n    size_ratios = size / ratios\n    ws = np.round(np.sqrt(size_ratios))\n    hs = np.round(ws * ratios)\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n\n\ndef _scale_enum(anchor, scales):\n    """"""Enumerate a set of anchors for each scale wrt an anchor.""""""\n    w, h, x_ctr, y_ctr = _whctrs(anchor)\n    ws = w * scales\n    hs = h * scales\n    anchors = _mkanchors(ws, hs, x_ctr, y_ctr)\n    return anchors\n'"
lib/modeling/generate_proposal_labels.py,0,"b'from torch import nn\n\nfrom core.config import cfg\nfrom datasets import json_dataset\nimport roi_data.fast_rcnn\n\n\nclass GenerateProposalLabelsOp(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, rpn_rois, roidb, im_info):\n        """"""Op for generating training labels for RPN proposals. This is used\n        when training RPN jointly with Fast/Mask R-CNN (as in end-to-end\n        Faster R-CNN training).\n\n        blobs_in:\n          - \'rpn_rois\': 2D tensor of RPN proposals output by GenerateProposals\n          - \'roidb\': roidb entries that will be labeled\n          - \'im_info\': See GenerateProposals doc.\n\n        blobs_out:\n          - (variable set of blobs): returns whatever blobs are required for\n            training the model. It does this by querying the data loader for\n            the list of blobs that are needed.\n        """"""\n        im_scales = im_info.data.numpy()[:, 2]\n\n        output_blob_names = roi_data.fast_rcnn.get_fast_rcnn_blob_names()\n        # For historical consistency with the original Faster R-CNN\n        # implementation we are *not* filtering crowd proposals.\n        # This choice should be investigated in the future (it likely does\n        # not matter).\n        # Note: crowd_thresh=0 will ignore _filter_crowd_proposals\n        json_dataset.add_proposals(roidb, rpn_rois, im_scales, crowd_thresh=0)\n        blobs = {k: [] for k in output_blob_names}\n        roi_data.fast_rcnn.add_fast_rcnn_blobs(blobs, im_scales, roidb)\n\n        return blobs\n'"
lib/modeling/generate_proposals.py,1,"b'import logging\nimport numpy as np\n\nfrom torch import nn\n\nfrom core.config import cfg\nimport utils.boxes as box_utils\n\nlogger = logging.getLogger(__name__)\n\n\nclass GenerateProposalsOp(nn.Module):\n    def __init__(self, anchors, spatial_scale):\n        super().__init__()\n        self._anchors = anchors\n        self._num_anchors = self._anchors.shape[0]\n        self._feat_stride = 1. / spatial_scale\n\n    def forward(self, rpn_cls_prob, rpn_bbox_pred, im_info):\n        """"""Op for generating RPN porposals.\n\n        blobs_in:\n          - \'rpn_cls_probs\': 4D tensor of shape (N, A, H, W), where N is the\n            number of minibatch images, A is the number of anchors per\n            locations, and (H, W) is the spatial size of the prediction grid.\n            Each value represents a ""probability of object"" rating in [0, 1].\n          - \'rpn_bbox_pred\': 4D tensor of shape (N, 4 * A, H, W) of predicted\n            deltas for transformation anchor boxes into RPN proposals.\n          - \'im_info\': 2D tensor of shape (N, 3) where the three columns encode\n            the input image\'s [height, width, scale]. Height and width are\n            for the input to the network, not the original image; scale is the\n            scale factor used to scale the original image to the network input\n            size.\n\n        blobs_out:\n          - \'rpn_rois\': 2D tensor of shape (R, 5), for R RPN proposals where the\n            five columns encode [batch ind, x1, y1, x2, y2]. The boxes are\n            w.r.t. the network input, which is a *scaled* version of the\n            original image; these proposals must be scaled by 1 / scale (where\n            scale comes from im_info; see above) to transform it back to the\n            original input image coordinate system.\n          - \'rpn_roi_probs\': 1D tensor of objectness probability scores\n            (extracted from rpn_cls_probs; see above).\n        """"""\n        # 1. for each location i in a (H, W) grid:\n        #      generate A anchor boxes centered on cell i\n        #      apply predicted bbox deltas to each of the A anchors at cell i\n        # 2. clip predicted boxes to image\n        # 3. remove predicted boxes with either height or width < threshold\n        # 4. sort all (proposal, score) pairs by score from highest to lowest\n        # 5. take the top pre_nms_topN proposals before NMS\n        # 6. apply NMS with a loose threshold (0.7) to the remaining proposals\n        # 7. take after_nms_topN proposals after NMS\n        # 8. return the top proposals\n        \n        """"""Type conversion""""""\n        # predicted probability of fg object for each RPN anchor\n        scores = rpn_cls_prob.data.cpu().numpy()\n        # predicted achors transformations\n        bbox_deltas = rpn_bbox_pred.data.cpu().numpy()\n        # input image (height, width, scale), in which scale is the scale factor\n        # applied to the original dataset image to get the network input image\n        im_info = im_info.data.cpu().numpy()\n\n        # 1. Generate proposals from bbox deltas and shifted anchors\n        height, width = scores.shape[-2:]\n        # Enumerate all shifted positions on the (H, W) grid\n        shift_x = np.arange(0, width) * self._feat_stride\n        shift_y = np.arange(0, height) * self._feat_stride\n        shift_x, shift_y = np.meshgrid(shift_x, shift_y, copy=False)\n        # Convert to (K, 4), K=H*W, where the columns are (dx, dy, dx, dy)\n        # shift pointing to each grid location\n        shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(),\n                            shift_y.ravel())).transpose()\n\n        # Broacast anchors over shifts to enumerate all anchors at all positions\n        # in the (H, W) grid:\n        #   - add A anchors of shape (1, A, 4) to\n        #   - K shifts of shape (K, 1, 4) to get\n        #   - all shifted anchors of shape (K, A, 4)\n        #   - reshape to (K*A, 4) shifted anchors\n        num_images = scores.shape[0]\n        A = self._num_anchors\n        K = shifts.shape[0]\n        all_anchors = self._anchors[np.newaxis, :, :] + shifts[:, np.newaxis, :]\n        all_anchors = all_anchors.reshape((K * A, 4))\n        # all_anchors = torch.from_numpy(all_anchors).type_as(scores)\n\n        rois = np.empty((0, 5), dtype=np.float32)\n        roi_probs = np.empty((0, 1), dtype=np.float32)\n        for im_i in range(num_images):\n            im_i_boxes, im_i_probs = self.proposals_for_one_image(\n                im_info[im_i, :], all_anchors, bbox_deltas[im_i, :, :, :],\n                scores[im_i, :, :, :])\n            batch_inds = im_i * np.ones(\n                (im_i_boxes.shape[0], 1), dtype=np.float32)\n            im_i_rois = np.hstack((batch_inds, im_i_boxes))\n            rois = np.append(rois, im_i_rois, axis=0)\n            roi_probs = np.append(roi_probs, im_i_probs, axis=0)\n\n        return rois, roi_probs  # Note: ndarrays\n\n    def proposals_for_one_image(self, im_info, all_anchors, bbox_deltas, scores):\n        # Get mode-dependent configuration\n        cfg_key = \'TRAIN\' if self.training else \'TEST\'\n        pre_nms_topN = cfg[cfg_key].RPN_PRE_NMS_TOP_N\n        post_nms_topN = cfg[cfg_key].RPN_POST_NMS_TOP_N\n        nms_thresh = cfg[cfg_key].RPN_NMS_THRESH\n        min_size = cfg[cfg_key].RPN_MIN_SIZE\n        # print(\'generate_proposals:\', pre_nms_topN, post_nms_topN, nms_thresh, min_size)\n\n        # Transpose and reshape predicted bbox transformations to get them\n        # into the same order as the anchors:\n        #   - bbox deltas will be (4 * A, H, W) format from conv output\n        #   - transpose to (H, W, 4 * A)\n        #   - reshape to (H * W * A, 4) where rows are ordered by (H, W, A)\n        #     in slowest to fastest order to match the enumerated anchors\n        bbox_deltas = bbox_deltas.transpose((1, 2, 0)).reshape((-1, 4))\n\n        # Same story for the scores:\n        #   - scores are (A, H, W) format from conv output\n        #   - transpose to (H, W, A)\n        #   - reshape to (H * W * A, 1) where rows are ordered by (H, W, A)\n        #     to match the order of anchors and bbox_deltas\n        scores = scores.transpose((1, 2, 0)).reshape((-1, 1))\n        # print(\'pre_nms:\', bbox_deltas.shape, scores.shape)\n\n        # 4. sort all (proposal, score) pairs by score from highest to lowest\n        # 5. take top pre_nms_topN (e.g. 6000)\n        if pre_nms_topN <= 0 or pre_nms_topN >= len(scores):\n            order = np.argsort(-scores.squeeze())\n        else:\n            # Avoid sorting possibly large arrays; First partition to get top K\n            # unsorted and then sort just those (~20x faster for 200k scores)\n            inds = np.argpartition(-scores.squeeze(),\n                                   pre_nms_topN)[:pre_nms_topN]\n            order = np.argsort(-scores[inds].squeeze())\n            order = inds[order]\n        bbox_deltas = bbox_deltas[order, :]\n        all_anchors = all_anchors[order, :]\n        scores = scores[order]\n\n        # Transform anchors into proposals via bbox transformations\n        proposals = box_utils.bbox_transform(all_anchors, bbox_deltas,\n                                             (1.0, 1.0, 1.0, 1.0))\n\n        # 2. clip proposals to image (may result in proposals with zero area\n        # that will be removed in the next step)\n        proposals = box_utils.clip_tiled_boxes(proposals, im_info[:2])\n\n        # 3. remove predicted boxes with either height or width < min_size\n        keep = _filter_boxes(proposals, min_size, im_info)\n        proposals = proposals[keep, :]\n        scores = scores[keep]\n        # print(\'pre_nms:\', proposals.shape, scores.shape)\n\n        # 6. apply loose nms (e.g. threshold = 0.7)\n        # 7. take after_nms_topN (e.g. 300)\n        # 8. return the top proposals (-> RoIs top)\n        if nms_thresh > 0:\n            keep = box_utils.nms(np.hstack((proposals, scores)), nms_thresh)\n            # print(\'nms keep:\', keep.shape)\n            if post_nms_topN > 0:\n                keep = keep[:post_nms_topN]\n            proposals = proposals[keep, :]\n            scores = scores[keep]\n        # print(\'final proposals:\', proposals.shape, scores.shape)\n        return proposals, scores\n\n\ndef _filter_boxes(boxes, min_size, im_info):\n    """"""Only keep boxes with both sides >= min_size and center within the image.\n  """"""\n    # Scale min_size to match image scale\n    min_size *= im_info[2]\n    ws = boxes[:, 2] - boxes[:, 0] + 1\n    hs = boxes[:, 3] - boxes[:, 1] + 1\n    x_ctr = boxes[:, 0] + ws / 2.\n    y_ctr = boxes[:, 1] + hs / 2.\n    keep = np.where((ws >= min_size) & (hs >= min_size) &\n                    (x_ctr < im_info[1]) & (y_ctr < im_info[0]))[0]\n    return keep\n'"
lib/modeling/keypoint_rcnn_heads.py,7,"b'import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.autograd import Variable\n\nfrom core.config import cfg\nimport nn as mynn\n\n\n# ---------------------------------------------------------------------------- #\n# Keypoint R-CNN outputs and losses\n# ---------------------------------------------------------------------------- #\n\nclass keypoint_outputs(nn.Module):\n    """"""Mask R-CNN keypoint specific outputs: keypoint heatmaps.""""""\n    def __init__(self, dim_in):\n        super().__init__()\n        self.upsample_heatmap = (cfg.KRCNN.UP_SCALE > 1)\n\n        if cfg.KRCNN.USE_DECONV:\n            # Apply ConvTranspose to the feature representation; results in 2x # upsampling\n            self.deconv = nn.ConvTranspose2d(\n                dim_in, cfg.KRCNN.DECONV_DIM, cfg.KRCNN.DECONV_KERNEL,\n                2, padding=int(cfg.KRCNN.DECONV_KERNEL / 2) - 1)\n            dim_in = cfg.KRCNN.DECONV_DIM\n\n        if cfg.KRCNN.USE_DECONV_OUTPUT:\n            # Use ConvTranspose to predict heatmaps; results in 2x upsampling\n            self.classify = nn.ConvTranspose2d(\n                dim_in, cfg.KRCNN.NUM_KEYPOINTS, cfg.KRCNN.DECONV_KERNEL,\n                2, padding=int(cfg.KRCNN.DECONV_KERNEL / 2 - 1))\n        else:\n            # Use Conv to predict heatmaps; does no upsampling\n            self.classify = nn.Conv2d(dim_in, cfg.KRCNN.NUM_KEYPOINTS, 1, 1, padding=0)\n\n        if self.upsample_heatmap:\n            # self.upsample = nn.UpsamplingBilinear2d(scale_factor=cfg.KRCNN.UP_SCALE)\n            self.upsample = mynn.BilinearInterpolation2d(\n                cfg.KRCNN.NUM_KEYPOINTS, cfg.KRCNN.NUM_KEYPOINTS, cfg.KRCNN.UP_SCALE)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        if cfg.KRCNN.USE_DECONV:\n            init.normal_(self.deconv.weight, std=0.01)\n            init.constant_(self.deconv.bias, 0)\n\n        if cfg.KRCNN.CONV_INIT == \'GaussianFill\':\n            init.normal_(self.classify.weight, std=0.001)\n        elif cfg.KRCNN.CONV_INIT == \'MSRAFill\':\n            mynn.init.MSRAFill(self.classify.weight)\n        else:\n            raise ValueError(cfg.KRCNN.CONV_INIT)\n        init.constant_(self.classify.bias, 0)\n\n    def detectron_weight_mapping(self):\n        detectron_weight_mapping = {}\n        if cfg.KRCNN.USE_DECONV:\n            detectron_weight_mapping.update({\n                \'deconv.weight\': \'kps_deconv_w\',\n                \'deconv.bias\': \'kps_deconv_b\'\n            })\n\n        if self.upsample_heatmap:\n            blob_name = \'kps_score_lowres\'\n            detectron_weight_mapping.update({\n                \'upsample.upconv.weight\': None,  # 0: don\'t load from or save to checkpoint\n                \'upsample.upconv.bias\': None\n            })\n        else:\n            blob_name = \'kps_score\'\n        detectron_weight_mapping.update({\n            \'classify.weight\': blob_name + \'_w\',\n            \'classify.bias\': blob_name + \'_b\'\n        })\n\n        return detectron_weight_mapping, []\n\n    def forward(self, x):\n        if cfg.KRCNN.USE_DECONV:\n            x = F.relu(self.deconv(x), inplace=True)\n        x = self.classify(x)\n        if self.upsample_heatmap:\n            x = self.upsample(x)\n        return x\n\n\ndef keypoint_losses(kps_pred, keypoint_locations_int32, keypoint_weights,\n                    keypoint_loss_normalizer=None):\n    """"""Mask R-CNN keypoint specific losses.""""""\n    device_id = kps_pred.get_device()\n    kps_target = Variable(torch.from_numpy(\n        keypoint_locations_int32.astype(\'int64\'))).cuda(device_id)\n    keypoint_weights = Variable(torch.from_numpy(keypoint_weights)).cuda(device_id)\n    # Softmax across **space** (woahh....space!)\n    # Note: this is not what is commonly called ""spatial softmax""\n    # (i.e., softmax applied along the channel dimension at each spatial\n    # location); This is softmax applied over a set of spatial locations (i.e.,\n    # each spatial location is a ""class"").\n    loss = F.cross_entropy(\n        kps_pred.view(-1, cfg.KRCNN.HEATMAP_SIZE**2), kps_target, reduce=False)\n    loss = torch.sum(loss * keypoint_weights) / torch.sum(keypoint_weights)\n    loss *= cfg.KRCNN.LOSS_WEIGHT\n\n    if not cfg.KRCNN.NORMALIZE_BY_VISIBLE_KEYPOINTS:\n        # Discussion: the softmax loss above will average the loss by the sum of\n        # keypoint_weights, i.e. the total number of visible keypoints. Since\n        # the number of visible keypoints can vary significantly between\n        # minibatches, this has the effect of up-weighting the importance of\n        # minibatches with few visible keypoints. (Imagine the extreme case of\n        # only one visible keypoint versus N: in the case of N, each one\n        # contributes 1/N to the gradient compared to the single keypoint\n        # determining the gradient direction). Instead, we can normalize the\n        # loss by the total number of keypoints, if it were the case that all\n        # keypoints were visible in a full minibatch. (Returning to the example,\n        # this means that the one visible keypoint contributes as much as each\n        # of the N keypoints.)\n        loss *= keypoint_loss_normalizer.item() # np.float32 to float\n    return loss\n\n\n# ---------------------------------------------------------------------------- #\n# Keypoint heads\n# ---------------------------------------------------------------------------- #\n\nclass roi_pose_head_v1convX(nn.Module):\n    """"""Mask R-CNN keypoint head. v1convX design: X * (conv).""""""\n    def __init__(self, dim_in, roi_xform_func, spatial_scale):\n        super().__init__()\n        self.dim_in = dim_in\n        self.roi_xform = roi_xform_func\n        self.spatial_scale = spatial_scale\n\n        hidden_dim = cfg.KRCNN.CONV_HEAD_DIM\n        kernel_size = cfg.KRCNN.CONV_HEAD_KERNEL\n        pad_size = kernel_size // 2\n        module_list = []\n        for _ in range(cfg.KRCNN.NUM_STACKED_CONVS):\n            module_list.append(nn.Conv2d(dim_in, hidden_dim, kernel_size, 1, pad_size))\n            module_list.append(nn.ReLU(inplace=True))\n            dim_in = hidden_dim\n        self.conv_fcn = nn.Sequential(*module_list)\n        self.dim_out = hidden_dim\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, nn.Conv2d):\n            if cfg.KRCNN.CONV_INIT == \'GaussianFill\':\n                init.normal_(m.weight, std=0.01)\n            elif cfg.KRCNN.CONV_INIT == \'MSRAFill\':\n                mynn.init.MSRAFill(m.weight)\n            else:\n                ValueError(\'Unexpected cfg.KRCNN.CONV_INIT: {}\'.format(cfg.KRCNN.CONV_INIT))\n            init.constant_(m.bias, 0)\n\n    def detectron_weight_mapping(self):\n        detectron_weight_mapping = {}\n        orphan_in_detectron = []\n        for i in range(cfg.KRCNN.NUM_STACKED_CONVS):\n            detectron_weight_mapping[\'conv_fcn.%d.weight\' % (2*i)] = \'conv_fcn%d_w\' % (i+1)\n            detectron_weight_mapping[\'conv_fcn.%d.bias\' % (2*i)] = \'conv_fcn%d_b\' % (i+1)\n\n        return detectron_weight_mapping, orphan_in_detectron\n\n    def forward(self, x, rpn_ret):\n        x = self.roi_xform(\n            x, rpn_ret,\n            blob_rois=\'keypoint_rois\',\n            method=cfg.KRCNN.ROI_XFORM_METHOD,\n            resolution=cfg.KRCNN.ROI_XFORM_RESOLUTION,\n            spatial_scale=self.spatial_scale,\n            sampling_ratio=cfg.KRCNN.ROI_XFORM_SAMPLING_RATIO\n        )\n        x = self.conv_fcn(x)\n        return x\n'"
lib/modeling/mask_rcnn_heads.py,7,"b'from functools import partial\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.autograd import Variable\n\nfrom core.config import cfg\nfrom modeling import ResNet\nimport nn as mynn\nimport utils.net as net_utils\n\n\n# ---------------------------------------------------------------------------- #\n# Mask R-CNN outputs and losses\n# ---------------------------------------------------------------------------- #\n\nclass mask_rcnn_outputs(nn.Module):\n    """"""Mask R-CNN specific outputs: either mask logits or probs.""""""\n    def __init__(self, dim_in):\n        super().__init__()\n        self.dim_in = dim_in\n\n        n_classes = cfg.MODEL.NUM_CLASSES if cfg.MRCNN.CLS_SPECIFIC_MASK else 1\n        if cfg.MRCNN.USE_FC_OUTPUT:\n            # Predict masks with a fully connected layer\n            self.classify = nn.Linear(dim_in, n_classes * cfg.MRCNN.RESOLUTION**2)\n        else:\n            # Predict mask using Conv\n            self.classify = nn.Conv2d(dim_in, n_classes, 1, 1, 0)\n            if cfg.MRCNN.UPSAMPLE_RATIO > 1:\n                self.upsample = mynn.BilinearInterpolation2d(\n                    n_classes, n_classes, cfg.MRCNN.UPSAMPLE_RATIO)\n        self._init_weights()\n\n    def _init_weights(self):\n        if not cfg.MRCNN.USE_FC_OUTPUT and cfg.MRCNN.CLS_SPECIFIC_MASK and \\\n                cfg.MRCNN.CONV_INIT==\'MSRAFill\':\n            # Use GaussianFill for class-agnostic mask prediction; fills based on\n            # fan-in can be too large in this case and cause divergence\n            weight_init_func = mynn.init.MSRAFill\n        else:\n            weight_init_func = partial(init.normal_, std=0.001)\n        weight_init_func(self.classify.weight)\n        init.constant_(self.classify.bias, 0)\n\n    def detectron_weight_mapping(self):\n        mapping = {\n            \'classify.weight\': \'mask_fcn_logits_w\',\n            \'classify.bias\': \'mask_fcn_logits_b\'\n        }\n        if hasattr(self, \'upsample\'):\n            mapping.update({\n                \'upsample.upconv.weight\': None,  # don\'t load from or save to checkpoint\n                \'upsample.upconv.bias\': None\n            })\n        orphan_in_detectron = []\n        return mapping, orphan_in_detectron\n\n    def forward(self, x):\n        x = self.classify(x)\n        if cfg.MRCNN.UPSAMPLE_RATIO > 1:\n            x = self.upsample(x)\n        if not self.training:\n            x = F.sigmoid(x)\n        return x\n\n\n# def mask_rcnn_losses(mask_pred, rois_mask, rois_label, weight):\n#     n_rois, n_classes, _, _ = mask_pred.size()\n#     rois_mask_label = rois_label[weight.data.nonzero().view(-1)]\n#     # select pred mask corresponding to gt label\n#     if cfg.MRCNN.MEMORY_EFFICIENT_LOSS:  # About 200~300 MB less. Not really sure how.\n#         mask_pred_select = Variable(\n#             mask_pred.data.new(n_rois, cfg.MRCNN.RESOLUTION,\n#                                cfg.MRCNN.RESOLUTION))\n#         for n, l in enumerate(rois_mask_label.data):\n#             mask_pred_select[n] = mask_pred[n, l]\n#     else:\n#         inds = rois_mask_label.data + \\\n#           torch.arange(0, n_rois * n_classes, n_classes).long().cuda(rois_mask_label.data.get_device())\n#         mask_pred_select = mask_pred.view(-1, cfg.MRCNN.RESOLUTION,\n#                                           cfg.MRCNN.RESOLUTION)[inds]\n#     loss = F.binary_cross_entropy_with_logits(mask_pred_select, rois_mask)\n#     return loss\n\n\ndef mask_rcnn_losses(masks_pred, masks_int32):\n    """"""Mask R-CNN specific losses.""""""\n    n_rois, n_classes, _, _ = masks_pred.size()\n    device_id = masks_pred.get_device()\n    masks_gt = Variable(torch.from_numpy(masks_int32.astype(\'float32\'))).cuda(device_id)\n    weight = (masks_gt > -1).float()  # masks_int32 {1, 0, -1}, -1 means ignore\n    loss = F.binary_cross_entropy_with_logits(\n        masks_pred.view(n_rois, -1), masks_gt, weight, size_average=False)\n    loss /= weight.sum()\n    return loss * cfg.MRCNN.WEIGHT_LOSS_MASK\n\n\n# ---------------------------------------------------------------------------- #\n# Mask heads\n# ---------------------------------------------------------------------------- #\n\ndef mask_rcnn_fcn_head_v1up4convs(dim_in, roi_xform_func, spatial_scale):\n    """"""v1up design: 4 * (conv 3x3), convT 2x2.""""""\n    return mask_rcnn_fcn_head_v1upXconvs(\n        dim_in, roi_xform_func, spatial_scale, 4\n    )\n\n\ndef mask_rcnn_fcn_head_v1up4convs_gn(dim_in, roi_xform_func, spatial_scale):\n    """"""v1up design: 4 * (conv 3x3), convT 2x2, with GroupNorm""""""\n    return mask_rcnn_fcn_head_v1upXconvs_gn(\n        dim_in, roi_xform_func, spatial_scale, 4\n    )\n\n\ndef mask_rcnn_fcn_head_v1up(dim_in, roi_xform_func, spatial_scale):\n    """"""v1up design: 2 * (conv 3x3), convT 2x2.""""""\n    return mask_rcnn_fcn_head_v1upXconvs(\n        dim_in, roi_xform_func, spatial_scale, 2\n    )\n\n\nclass mask_rcnn_fcn_head_v1upXconvs(nn.Module):\n    """"""v1upXconvs design: X * (conv 3x3), convT 2x2.""""""\n    def __init__(self, dim_in, roi_xform_func, spatial_scale, num_convs):\n        super().__init__()\n        self.dim_in = dim_in\n        self.roi_xform = roi_xform_func\n        self.spatial_scale = spatial_scale\n        self.num_convs = num_convs\n\n        dilation = cfg.MRCNN.DILATION\n        dim_inner = cfg.MRCNN.DIM_REDUCED\n        self.dim_out = dim_inner\n\n        module_list = []\n        for i in range(num_convs):\n            module_list.extend([\n                nn.Conv2d(dim_in, dim_inner, 3, 1, padding=1*dilation, dilation=dilation),\n                nn.ReLU(inplace=True)\n            ])\n            dim_in = dim_inner\n        self.conv_fcn = nn.Sequential(*module_list)\n\n        # upsample layer\n        self.upconv = nn.ConvTranspose2d(dim_inner, dim_inner, 2, 2, 0)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n            if cfg.MRCNN.CONV_INIT == \'GaussianFill\':\n                init.normal_(m.weight, std=0.001)\n            elif cfg.MRCNN.CONV_INIT == \'MSRAFill\':\n                mynn.init.MSRAFill(m.weight)\n            else:\n                raise ValueError\n            init.constant_(m.bias, 0)\n\n    def detectron_weight_mapping(self):\n        mapping_to_detectron = {}\n        for i in range(self.num_convs):\n            mapping_to_detectron.update({\n                \'conv_fcn.%d.weight\' % (2*i): \'_[mask]_fcn%d_w\' % (i+1),\n                \'conv_fcn.%d.bias\' % (2*i): \'_[mask]_fcn%d_b\' % (i+1)\n            })\n        mapping_to_detectron.update({\n            \'upconv.weight\': \'conv5_mask_w\',\n            \'upconv.bias\': \'conv5_mask_b\'\n        })\n\n        return mapping_to_detectron, []\n\n    def forward(self, x, rpn_ret):\n        x = self.roi_xform(\n            x, rpn_ret,\n            blob_rois=\'mask_rois\',\n            method=cfg.MRCNN.ROI_XFORM_METHOD,\n            resolution=cfg.MRCNN.ROI_XFORM_RESOLUTION,\n            spatial_scale=self.spatial_scale,\n            sampling_ratio=cfg.MRCNN.ROI_XFORM_SAMPLING_RATIO\n        )\n        x = self.conv_fcn(x)\n        return F.relu(self.upconv(x), inplace=True)\n\n\nclass mask_rcnn_fcn_head_v1upXconvs_gn(nn.Module):\n    """"""v1upXconvs design: X * (conv 3x3), convT 2x2, with GroupNorm""""""\n    def __init__(self, dim_in, roi_xform_func, spatial_scale, num_convs):\n        super().__init__()\n        self.dim_in = dim_in\n        self.roi_xform = roi_xform_func\n        self.spatial_scale = spatial_scale\n        self.num_convs = num_convs\n\n        dilation = cfg.MRCNN.DILATION\n        dim_inner = cfg.MRCNN.DIM_REDUCED\n        self.dim_out = dim_inner\n\n        module_list = []\n        for i in range(num_convs):\n            module_list.extend([\n                nn.Conv2d(dim_in, dim_inner, 3, 1, padding=1*dilation, dilation=dilation, bias=False),\n                nn.GroupNorm(net_utils.get_group_gn(dim_inner), dim_inner, eps=cfg.GROUP_NORM.EPSILON),\n                nn.ReLU(inplace=True)\n            ])\n            dim_in = dim_inner\n        self.conv_fcn = nn.Sequential(*module_list)\n\n        # upsample layer\n        self.upconv = nn.ConvTranspose2d(dim_inner, dim_inner, 2, 2, 0)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, m):\n        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n            if cfg.MRCNN.CONV_INIT == \'GaussianFill\':\n                init.normal_(m.weight, std=0.001)\n            elif cfg.MRCNN.CONV_INIT == \'MSRAFill\':\n                mynn.init.MSRAFill(m.weight)\n            else:\n                raise ValueError\n            if m.bias is not None:\n                init.constant_(m.bias, 0)\n\n    def detectron_weight_mapping(self):\n        mapping_to_detectron = {}\n        for i in range(self.num_convs):\n            mapping_to_detectron.update({\n                \'conv_fcn.%d.weight\' % (3*i): \'_mask_fcn%d_w\' % (i+1),\n                \'conv_fcn.%d.weight\' % (3*i+1): \'_mask_fcn%d_gn_s\' % (i+1),\n                \'conv_fcn.%d.bias\' % (3*i+1): \'_mask_fcn%d_gn_b\' % (i+1)\n            })\n        mapping_to_detectron.update({\n            \'upconv.weight\': \'conv5_mask_w\',\n            \'upconv.bias\': \'conv5_mask_b\'\n        })\n\n        return mapping_to_detectron, []\n\n    def forward(self, x, rpn_ret):\n        x = self.roi_xform(\n            x, rpn_ret,\n            blob_rois=\'mask_rois\',\n            method=cfg.MRCNN.ROI_XFORM_METHOD,\n            resolution=cfg.MRCNN.ROI_XFORM_RESOLUTION,\n            spatial_scale=self.spatial_scale,\n            sampling_ratio=cfg.MRCNN.ROI_XFORM_SAMPLING_RATIO\n        )\n        x = self.conv_fcn(x)\n        return F.relu(self.upconv(x), inplace=True)\n\n\nclass mask_rcnn_fcn_head_v0upshare(nn.Module):\n    """"""Use a ResNet ""conv5"" / ""stage5"" head for mask prediction. Weights and\n    computation are shared with the conv5 box head. Computation can only be\n    shared during training, since inference is cascaded.\n\n    v0upshare design: conv5, convT 2x2.\n    """"""\n    def __init__(self, dim_in, roi_xform_func, spatial_scale):\n        super().__init__()\n        self.dim_in = dim_in\n        self.roi_xform = roi_xform_func\n        self.spatial_scale = spatial_scale\n        self.dim_out = cfg.MRCNN.DIM_REDUCED\n        self.SHARE_RES5 = True\n        assert cfg.MODEL.SHARE_RES5\n\n        self.res5 = None  # will be assigned later\n        dim_conv5 = 2048\n        self.upconv5 = nn.ConvTranspose2d(dim_conv5, self.dim_out, 2, 2, 0)\n\n        self._init_weights()\n\n    def _init_weights(self):\n        if cfg.MRCNN.CONV_INIT == \'GaussianFill\':\n            init.normal_(self.upconv5.weight, std=0.001)\n        elif cfg.MRCNN.CONV_INIT == \'MSRAFill\':\n            mynn.init.MSRAFill(self.upconv5.weight)\n        init.constant_(self.upconv5.bias, 0)\n\n    def share_res5_module(self, res5_target):\n        """""" Share res5 block with box head on training """"""\n        self.res5 = res5_target\n\n    def detectron_weight_mapping(self):\n        detectron_weight_mapping, orphan_in_detectron = \\\n          ResNet.residual_stage_detectron_mapping(self.res5, \'res5\', 3, 5)\n        # Assign None for res5 modules, do not load from or save to checkpoint\n        for k in detectron_weight_mapping:\n            detectron_weight_mapping[k] = None\n\n        detectron_weight_mapping.update({\n            \'upconv5.weight\': \'conv5_mask_w\',\n            \'upconv5.bias\': \'conv5_mask_b\'\n        })\n        return detectron_weight_mapping, orphan_in_detectron\n\n    def forward(self, x, rpn_ret, roi_has_mask_int32=None):\n        if self.training:\n            # On training, we share the res5 computation with bbox head, so it\'s necessary to\n            # sample \'useful\' batches from the input x (res5_2_sum). \'Useful\' means that the\n            # batch (roi) has corresponding mask groundtruth, namely having positive values in\n            # roi_has_mask_int32.\n            inds = np.nonzero(roi_has_mask_int32 > 0)[0]\n            inds = Variable(torch.from_numpy(inds)).cuda(x.get_device())\n            x = x[inds]\n        else:\n            # On testing, the computation is not shared with bbox head. This time input `x`\n            # is the output features from the backbone network\n            x = self.roi_xform(\n                x, rpn_ret,\n                blob_rois=\'mask_rois\',\n                method=cfg.MRCNN.ROI_XFORM_METHOD,\n                resolution=cfg.MRCNN.ROI_XFORM_RESOLUTION,\n                spatial_scale=self.spatial_scale,\n                sampling_ratio=cfg.MRCNN.ROI_XFORM_SAMPLING_RATIO\n            )\n            x = self.res5(x)\n        x = self.upconv5(x)\n        x = F.relu(x, inplace=True)\n        return x\n\n\nclass mask_rcnn_fcn_head_v0up(nn.Module):\n    """"""v0up design: conv5, deconv 2x2 (no weight sharing with the box head).""""""\n    def __init__(self, dim_in, roi_xform_func, spatial_scale):\n        super().__init__()\n        self.dim_in = dim_in\n        self.roi_xform = roi_xform_func\n        self.spatial_scale = spatial_scale\n        self.dim_out = cfg.MRCNN.DIM_REDUCED\n\n        self.res5, dim_out = ResNet_roi_conv5_head_for_masks(dim_in)\n        self.upconv5 = nn.ConvTranspose2d(dim_out, self.dim_out, 2, 2, 0)\n\n        # Freeze all bn (affine) layers in resnet!!!\n        self.res5.apply(\n            lambda m: ResNet.freeze_params(m)\n            if isinstance(m, mynn.AffineChannel2d) else None)\n        self._init_weights()\n\n    def _init_weights(self):\n        if cfg.MRCNN.CONV_INIT == \'GaussianFill\':\n            init.normal_(self.upconv5.weight, std=0.001)\n        elif cfg.MRCNN.CONV_INIT == \'MSRAFill\':\n            mynn.init.MSRAFill(self.upconv5.weight)\n        init.constant_(self.upconv5.bias, 0)\n\n    def detectron_weight_mapping(self):\n        detectron_weight_mapping, orphan_in_detectron = \\\n          ResNet.residual_stage_detectron_mapping(self.res5, \'res5\', 3, 5)\n        detectron_weight_mapping.update({\n            \'upconv5.weight\': \'conv5_mask_w\',\n            \'upconv5.bias\': \'conv5_mask_b\'\n        })\n        return detectron_weight_mapping, orphan_in_detectron\n\n    def forward(self, x, rpn_ret):\n        x = self.roi_xform(\n            x, rpn_ret,\n            blob_rois=\'mask_rois\',\n            method=cfg.MRCNN.ROI_XFORM_METHOD,\n            resolution=cfg.MRCNN.ROI_XFORM_RESOLUTION,\n            spatial_scale=self.spatial_scale,\n            sampling_ratio=cfg.MRCNN.ROI_XFORM_SAMPLING_RATIO\n        )\n        x = self.res5(x)\n        # print(x.size()) e.g. (128, 2048, 7, 7)\n        x = self.upconv5(x)\n        x = F.relu(x, inplace=True)\n        return x\n\n\ndef ResNet_roi_conv5_head_for_masks(dim_in):\n    """"""ResNet ""conv5"" / ""stage5"" head for predicting masks.""""""\n    dilation = cfg.MRCNN.DILATION\n    stride_init = cfg.MRCNN.ROI_XFORM_RESOLUTION // 7  # by default: 2\n    module, dim_out = ResNet.add_stage(dim_in, 2048, 512, 3, dilation, stride_init)\n    return module, dim_out\n'"
lib/modeling/model_builder.py,12,"b'from functools import wraps\nimport importlib\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom core.config import cfg\nfrom model.roi_pooling.functions.roi_pool import RoIPoolFunction\nfrom model.roi_crop.functions.roi_crop import RoICropFunction\nfrom modeling.roi_xfrom.roi_align.functions.roi_align import RoIAlignFunction\nimport modeling.rpn_heads as rpn_heads\nimport modeling.fast_rcnn_heads as fast_rcnn_heads\nimport modeling.mask_rcnn_heads as mask_rcnn_heads\nimport modeling.keypoint_rcnn_heads as keypoint_rcnn_heads\nimport utils.blob as blob_utils\nimport utils.net as net_utils\nimport utils.resnet_weights_helper as resnet_utils\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_func(func_name):\n    """"""Helper to return a function object by name. func_name must identify a\n    function in this module or the path to a function relative to the base\n    \'modeling\' module.\n    """"""\n    if func_name == \'\':\n        return None\n    try:\n        parts = func_name.split(\'.\')\n        # Refers to a function in this module\n        if len(parts) == 1:\n            return globals()[parts[0]]\n        # Otherwise, assume we\'re referencing a module under modeling\n        module_name = \'modeling.\' + \'.\'.join(parts[:-1])\n        module = importlib.import_module(module_name)\n        return getattr(module, parts[-1])\n    except Exception:\n        logger.error(\'Failed to find function: %s\', func_name)\n        raise\n\n\ndef compare_state_dict(sa, sb):\n    if sa.keys() != sb.keys():\n        return False\n    for k, va in sa.items():\n        if not torch.equal(va, sb[k]):\n            return False\n    return True\n\n\ndef check_inference(net_func):\n    @wraps(net_func)\n    def wrapper(self, *args, **kwargs):\n        if not self.training:\n            if cfg.PYTORCH_VERSION_LESS_THAN_040:\n                return net_func(self, *args, **kwargs)\n            else:\n                with torch.no_grad():\n                    return net_func(self, *args, **kwargs)\n        else:\n            raise ValueError(\'You should call this function only on inference.\'\n                              \'Set the network in inference mode by net.eval().\')\n\n    return wrapper\n\n\nclass Generalized_RCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # For cache\n        self.mapping_to_detectron = None\n        self.orphans_in_detectron = None\n\n        # Backbone for feature extraction\n        self.Conv_Body = get_func(cfg.MODEL.CONV_BODY)()\n\n        # Region Proposal Network\n        if cfg.RPN.RPN_ON:\n            self.RPN = rpn_heads.generic_rpn_outputs(\n                self.Conv_Body.dim_out, self.Conv_Body.spatial_scale)\n\n        if cfg.FPN.FPN_ON:\n            # Only supports case when RPN and ROI min levels are the same\n            assert cfg.FPN.RPN_MIN_LEVEL == cfg.FPN.ROI_MIN_LEVEL\n            # RPN max level can be >= to ROI max level\n            assert cfg.FPN.RPN_MAX_LEVEL >= cfg.FPN.ROI_MAX_LEVEL\n            # FPN RPN max level might be > FPN ROI max level in which case we\n            # need to discard some leading conv blobs (blobs are ordered from\n            # max/coarsest level to min/finest level)\n            self.num_roi_levels = cfg.FPN.ROI_MAX_LEVEL - cfg.FPN.ROI_MIN_LEVEL + 1\n\n            # Retain only the spatial scales that will be used for RoI heads. `Conv_Body.spatial_scale`\n            # may include extra scales that are used for RPN proposals, but not for RoI heads.\n            self.Conv_Body.spatial_scale = self.Conv_Body.spatial_scale[-self.num_roi_levels:]\n\n        # BBOX Branch\n        if not cfg.MODEL.RPN_ONLY:\n            self.Box_Head = get_func(cfg.FAST_RCNN.ROI_BOX_HEAD)(\n                self.RPN.dim_out, self.roi_feature_transform, self.Conv_Body.spatial_scale)\n            self.Box_Outs = fast_rcnn_heads.fast_rcnn_outputs(\n                self.Box_Head.dim_out)\n\n        # Mask Branch\n        if cfg.MODEL.MASK_ON:\n            self.Mask_Head = get_func(cfg.MRCNN.ROI_MASK_HEAD)(\n                self.RPN.dim_out, self.roi_feature_transform, self.Conv_Body.spatial_scale)\n            if getattr(self.Mask_Head, \'SHARE_RES5\', False):\n                self.Mask_Head.share_res5_module(self.Box_Head.res5)\n            self.Mask_Outs = mask_rcnn_heads.mask_rcnn_outputs(self.Mask_Head.dim_out)\n\n        # Keypoints Branch\n        if cfg.MODEL.KEYPOINTS_ON:\n            self.Keypoint_Head = get_func(cfg.KRCNN.ROI_KEYPOINTS_HEAD)(\n                self.RPN.dim_out, self.roi_feature_transform, self.Conv_Body.spatial_scale)\n            if getattr(self.Keypoint_Head, \'SHARE_RES5\', False):\n                self.Keypoint_Head.share_res5_module(self.Box_Head.res5)\n            self.Keypoint_Outs = keypoint_rcnn_heads.keypoint_outputs(self.Keypoint_Head.dim_out)\n\n        self._init_modules()\n\n    def _init_modules(self):\n        if cfg.MODEL.LOAD_IMAGENET_PRETRAINED_WEIGHTS:\n            resnet_utils.load_pretrained_imagenet_weights(self)\n            # Check if shared weights are equaled\n            if cfg.MODEL.MASK_ON and getattr(self.Mask_Head, \'SHARE_RES5\', False):\n                assert compare_state_dict(self.Mask_Head.res5.state_dict(), self.Box_Head.res5.state_dict())\n            if cfg.MODEL.KEYPOINTS_ON and getattr(self.Keypoint_Head, \'SHARE_RES5\', False):\n                assert compare_state_dict(self.Keypoint_Head.res5.state_dict(), self.Box_Head.res5.state_dict())\n\n        if cfg.TRAIN.FREEZE_CONV_BODY:\n            for p in self.Conv_Body.parameters():\n                p.requires_grad = False\n\n    def forward(self, data, im_info, roidb=None, **rpn_kwargs):\n        if cfg.PYTORCH_VERSION_LESS_THAN_040:\n            return self._forward(data, im_info, roidb, **rpn_kwargs)\n        else:\n            with torch.set_grad_enabled(self.training):\n                return self._forward(data, im_info, roidb, **rpn_kwargs)\n\n    def _forward(self, data, im_info, roidb=None, **rpn_kwargs):\n        im_data = data\n        if self.training:\n            roidb = list(map(lambda x: blob_utils.deserialize(x)[0], roidb))\n\n        device_id = im_data.get_device()\n\n        return_dict = {}  # A dict to collect return variables\n\n        blob_conv = self.Conv_Body(im_data)\n\n        rpn_ret = self.RPN(blob_conv, im_info, roidb)\n\n        # if self.training:\n        #     # can be used to infer fg/bg ratio\n        #     return_dict[\'rois_label\'] = rpn_ret[\'labels_int32\']\n\n        if cfg.FPN.FPN_ON:\n            # Retain only the blobs that will be used for RoI heads. `blob_conv` may include\n            # extra blobs that are used for RPN proposals, but not for RoI heads.\n            blob_conv = blob_conv[-self.num_roi_levels:]\n\n        if not self.training:\n            return_dict[\'blob_conv\'] = blob_conv\n\n        if not cfg.MODEL.RPN_ONLY:\n            if cfg.MODEL.SHARE_RES5 and self.training:\n                box_feat, res5_feat = self.Box_Head(blob_conv, rpn_ret)\n            else:\n                box_feat = self.Box_Head(blob_conv, rpn_ret)\n            cls_score, bbox_pred = self.Box_Outs(box_feat)\n        else:\n            # TODO: complete the returns for RPN only situation\n            pass\n\n        if self.training:\n            return_dict[\'losses\'] = {}\n            return_dict[\'metrics\'] = {}\n            # rpn loss\n            rpn_kwargs.update(dict(\n                (k, rpn_ret[k]) for k in rpn_ret.keys()\n                if (k.startswith(\'rpn_cls_logits\') or k.startswith(\'rpn_bbox_pred\'))\n            ))\n            loss_rpn_cls, loss_rpn_bbox = rpn_heads.generic_rpn_losses(**rpn_kwargs)\n            if cfg.FPN.FPN_ON:\n                for i, lvl in enumerate(range(cfg.FPN.RPN_MIN_LEVEL, cfg.FPN.RPN_MAX_LEVEL + 1)):\n                    return_dict[\'losses\'][\'loss_rpn_cls_fpn%d\' % lvl] = loss_rpn_cls[i]\n                    return_dict[\'losses\'][\'loss_rpn_bbox_fpn%d\' % lvl] = loss_rpn_bbox[i]\n            else:\n                return_dict[\'losses\'][\'loss_rpn_cls\'] = loss_rpn_cls\n                return_dict[\'losses\'][\'loss_rpn_bbox\'] = loss_rpn_bbox\n\n            # bbox loss\n            loss_cls, loss_bbox, accuracy_cls = fast_rcnn_heads.fast_rcnn_losses(\n                cls_score, bbox_pred, rpn_ret[\'labels_int32\'], rpn_ret[\'bbox_targets\'],\n                rpn_ret[\'bbox_inside_weights\'], rpn_ret[\'bbox_outside_weights\'])\n            return_dict[\'losses\'][\'loss_cls\'] = loss_cls\n            return_dict[\'losses\'][\'loss_bbox\'] = loss_bbox\n            return_dict[\'metrics\'][\'accuracy_cls\'] = accuracy_cls\n\n            if cfg.MODEL.MASK_ON:\n                if getattr(self.Mask_Head, \'SHARE_RES5\', False):\n                    mask_feat = self.Mask_Head(res5_feat, rpn_ret,\n                                               roi_has_mask_int32=rpn_ret[\'roi_has_mask_int32\'])\n                else:\n                    mask_feat = self.Mask_Head(blob_conv, rpn_ret)\n                mask_pred = self.Mask_Outs(mask_feat)\n                # return_dict[\'mask_pred\'] = mask_pred\n                # mask loss\n                loss_mask = mask_rcnn_heads.mask_rcnn_losses(mask_pred, rpn_ret[\'masks_int32\'])\n                return_dict[\'losses\'][\'loss_mask\'] = loss_mask\n\n            if cfg.MODEL.KEYPOINTS_ON:\n                if getattr(self.Keypoint_Head, \'SHARE_RES5\', False):\n                    # No corresponding keypoint head implemented yet (Neither in Detectron)\n                    # Also, rpn need to generate the label \'roi_has_keypoints_int32\'\n                    kps_feat = self.Keypoint_Head(res5_feat, rpn_ret,\n                                                  roi_has_keypoints_int32=rpn_ret[\'roi_has_keypoint_int32\'])\n                else:\n                    kps_feat = self.Keypoint_Head(blob_conv, rpn_ret)\n                kps_pred = self.Keypoint_Outs(kps_feat)\n                # return_dict[\'keypoints_pred\'] = kps_pred\n                # keypoints loss\n                if cfg.KRCNN.NORMALIZE_BY_VISIBLE_KEYPOINTS:\n                    loss_keypoints = keypoint_rcnn_heads.keypoint_losses(\n                        kps_pred, rpn_ret[\'keypoint_locations_int32\'], rpn_ret[\'keypoint_weights\'])\n                else:\n                    loss_keypoints = keypoint_rcnn_heads.keypoint_losses(\n                        kps_pred, rpn_ret[\'keypoint_locations_int32\'], rpn_ret[\'keypoint_weights\'],\n                        rpn_ret[\'keypoint_loss_normalizer\'])\n                return_dict[\'losses\'][\'loss_kps\'] = loss_keypoints\n\n            # pytorch0.4 bug on gathering scalar(0-dim) tensors\n            for k, v in return_dict[\'losses\'].items():\n                return_dict[\'losses\'][k] = v.unsqueeze(0)\n            for k, v in return_dict[\'metrics\'].items():\n                return_dict[\'metrics\'][k] = v.unsqueeze(0)\n\n        else:\n            # Testing\n            return_dict[\'rois\'] = rpn_ret[\'rois\']\n            return_dict[\'cls_score\'] = cls_score\n            return_dict[\'bbox_pred\'] = bbox_pred\n\n        return return_dict\n\n    def roi_feature_transform(self, blobs_in, rpn_ret, blob_rois=\'rois\', method=\'RoIPoolF\',\n                              resolution=7, spatial_scale=1. / 16., sampling_ratio=0):\n        """"""Add the specified RoI pooling method. The sampling_ratio argument\n        is supported for some, but not all, RoI transform methods.\n\n        RoIFeatureTransform abstracts away:\n          - Use of FPN or not\n          - Specifics of the transform method\n        """"""\n        assert method in {\'RoIPoolF\', \'RoICrop\', \'RoIAlign\'}, \\\n            \'Unknown pooling method: {}\'.format(method)\n\n        if isinstance(blobs_in, list):\n            # FPN case: add RoIFeatureTransform to each FPN level\n            device_id = blobs_in[0].get_device()\n            k_max = cfg.FPN.ROI_MAX_LEVEL  # coarsest level of pyramid\n            k_min = cfg.FPN.ROI_MIN_LEVEL  # finest level of pyramid\n            assert len(blobs_in) == k_max - k_min + 1\n            bl_out_list = []\n            for lvl in range(k_min, k_max + 1):\n                bl_in = blobs_in[k_max - lvl]  # blobs_in is in reversed order\n                sc = spatial_scale[k_max - lvl]  # in reversed order\n                bl_rois = blob_rois + \'_fpn\' + str(lvl)\n                if len(rpn_ret[bl_rois]):\n                    rois = Variable(torch.from_numpy(rpn_ret[bl_rois])).cuda(device_id)\n                    if method == \'RoIPoolF\':\n                        # Warning!: Not check if implementation matches Detectron\n                        xform_out = RoIPoolFunction(resolution, resolution, sc)(bl_in, rois)\n                    elif method == \'RoICrop\':\n                        # Warning!: Not check if implementation matches Detectron\n                        grid_xy = net_utils.affine_grid_gen(\n                            rois, bl_in.size()[2:], self.grid_size)\n                        grid_yx = torch.stack(\n                            [grid_xy.data[:, :, :, 1], grid_xy.data[:, :, :, 0]], 3).contiguous()\n                        xform_out = RoICropFunction()(bl_in, Variable(grid_yx).detach())\n                        if cfg.CROP_RESIZE_WITH_MAX_POOL:\n                            xform_out = F.max_pool2d(xform_out, 2, 2)\n                    elif method == \'RoIAlign\':\n                        xform_out = RoIAlignFunction(\n                            resolution, resolution, sc, sampling_ratio)(bl_in, rois)\n                    bl_out_list.append(xform_out)\n\n            # The pooled features from all levels are concatenated along the\n            # batch dimension into a single 4D tensor.\n            xform_shuffled = torch.cat(bl_out_list, dim=0)\n\n            # Unshuffle to match rois from dataloader\n            device_id = xform_shuffled.get_device()\n            restore_bl = rpn_ret[blob_rois + \'_idx_restore_int32\']\n            restore_bl = Variable(\n                torch.from_numpy(restore_bl.astype(\'int64\', copy=False))).cuda(device_id)\n            xform_out = xform_shuffled[restore_bl]\n        else:\n            # Single feature level\n            # rois: holds R regions of interest, each is a 5-tuple\n            # (batch_idx, x1, y1, x2, y2) specifying an image batch index and a\n            # rectangle (x1, y1, x2, y2)\n            device_id = blobs_in.get_device()\n            rois = Variable(torch.from_numpy(rpn_ret[blob_rois])).cuda(device_id)\n            if method == \'RoIPoolF\':\n                xform_out = RoIPoolFunction(resolution, resolution, spatial_scale)(blobs_in, rois)\n            elif method == \'RoICrop\':\n                grid_xy = net_utils.affine_grid_gen(rois, blobs_in.size()[2:], self.grid_size)\n                grid_yx = torch.stack(\n                    [grid_xy.data[:, :, :, 1], grid_xy.data[:, :, :, 0]], 3).contiguous()\n                xform_out = RoICropFunction()(blobs_in, Variable(grid_yx).detach())\n                if cfg.CROP_RESIZE_WITH_MAX_POOL:\n                    xform_out = F.max_pool2d(xform_out, 2, 2)\n            elif method == \'RoIAlign\':\n                xform_out = RoIAlignFunction(\n                    resolution, resolution, spatial_scale, sampling_ratio)(blobs_in, rois)\n\n        return xform_out\n\n    @check_inference\n    def convbody_net(self, data):\n        """"""For inference. Run Conv Body only""""""\n        blob_conv = self.Conv_Body(data)\n        if cfg.FPN.FPN_ON:\n            # Retain only the blobs that will be used for RoI heads. `blob_conv` may include\n            # extra blobs that are used for RPN proposals, but not for RoI heads.\n            blob_conv = blob_conv[-self.num_roi_levels:]\n        return blob_conv\n\n    @check_inference\n    def mask_net(self, blob_conv, rpn_blob):\n        """"""For inference""""""\n        mask_feat = self.Mask_Head(blob_conv, rpn_blob)\n        mask_pred = self.Mask_Outs(mask_feat)\n        return mask_pred\n\n    @check_inference\n    def keypoint_net(self, blob_conv, rpn_blob):\n        """"""For inference""""""\n        kps_feat = self.Keypoint_Head(blob_conv, rpn_blob)\n        kps_pred = self.Keypoint_Outs(kps_feat)\n        return kps_pred\n\n    @property\n    def detectron_weight_mapping(self):\n        if self.mapping_to_detectron is None:\n            d_wmap = {}  # detectron_weight_mapping\n            d_orphan = []  # detectron orphan weight list\n            for name, m_child in self.named_children():\n                if list(m_child.parameters()):  # if module has any parameter\n                    child_map, child_orphan = m_child.detectron_weight_mapping()\n                    d_orphan.extend(child_orphan)\n                    for key, value in child_map.items():\n                        new_key = name + \'.\' + key\n                        d_wmap[new_key] = value\n            self.mapping_to_detectron = d_wmap\n            self.orphans_in_detectron = d_orphan\n\n        return self.mapping_to_detectron, self.orphans_in_detectron\n\n    def _add_loss(self, return_dict, key, value):\n        """"""Add loss tensor to returned dictionary""""""\n        return_dict[\'losses\'][key] = value\n'"
lib/modeling/rpn_heads.py,2,"b'from torch import nn\nfrom torch.nn import init\nimport torch.nn.functional as F\n\nfrom core.config import cfg\nfrom modeling.generate_anchors import generate_anchors\nfrom modeling.generate_proposals import GenerateProposalsOp\nfrom modeling.generate_proposal_labels import GenerateProposalLabelsOp\nimport modeling.FPN as FPN\nimport utils.net as net_utils\n\n\n# ---------------------------------------------------------------------------- #\n# RPN and Faster R-CNN outputs and losses\n# ---------------------------------------------------------------------------- #\n\ndef generic_rpn_outputs(dim_in, spatial_scale_in):\n    """"""Add RPN outputs (objectness classification and bounding box regression)\n    to an RPN model. Abstracts away the use of FPN.\n    """"""\n    if cfg.FPN.FPN_ON:\n        # Delegate to the FPN module\n        return FPN.fpn_rpn_outputs(dim_in, spatial_scale_in)\n    else:\n        # Not using FPN, add RPN to a single scale\n        return single_scale_rpn_outputs(dim_in, spatial_scale_in)\n\n\ndef generic_rpn_losses(*inputs, **kwargs):\n    """"""Add RPN losses. Abstracts away the use of FPN.""""""\n    if cfg.FPN.FPN_ON:\n        return FPN.fpn_rpn_losses(*inputs, **kwargs)\n    else:\n        return single_scale_rpn_losses(*inputs, **kwargs)\n\n\nclass single_scale_rpn_outputs(nn.Module):\n    """"""Add RPN outputs to a single scale model (i.e., no FPN).""""""\n    def __init__(self, dim_in, spatial_scale):\n        super().__init__()\n        self.dim_in = dim_in\n        self.dim_out = dim_in if cfg.RPN.OUT_DIM_AS_IN_DIM else cfg.RPN.OUT_DIM\n        anchors = generate_anchors(\n            stride=1. / spatial_scale,\n            sizes=cfg.RPN.SIZES,\n            aspect_ratios=cfg.RPN.ASPECT_RATIOS)\n        num_anchors = anchors.shape[0]\n\n        # RPN hidden representation\n        self.RPN_conv = nn.Conv2d(self.dim_in, self.dim_out, 3, 1, 1)\n        # Proposal classification scores\n        self.n_score_out = num_anchors * 2 if cfg.RPN.CLS_ACTIVATION == \'softmax\' \\\n            else num_anchors\n        self.RPN_cls_score = nn.Conv2d(self.dim_out, self.n_score_out, 1, 1, 0)\n        # Proposal bbox regression deltas\n        self.RPN_bbox_pred = nn.Conv2d(self.dim_out, num_anchors * 4, 1, 1, 0)\n\n        self.RPN_GenerateProposals = GenerateProposalsOp(anchors, spatial_scale)\n        self.RPN_GenerateProposalLabels = GenerateProposalLabelsOp()\n\n        self._init_weights()\n\n    def _init_weights(self):\n        init.normal_(self.RPN_conv.weight, std=0.01)\n        init.constant_(self.RPN_conv.bias, 0)\n        init.normal_(self.RPN_cls_score.weight, std=0.01)\n        init.constant_(self.RPN_cls_score.bias, 0)\n        init.normal_(self.RPN_bbox_pred.weight, std=0.01)\n        init.constant_(self.RPN_bbox_pred.bias, 0)\n\n    def detectron_weight_mapping(self):\n        detectron_weight_mapping = {\n            \'RPN_conv.weight\': \'conv_rpn_w\',\n            \'RPN_conv.bias\': \'conv_rpn_b\',\n            \'RPN_cls_score.weight\': \'rpn_cls_logits_w\',\n            \'RPN_cls_score.bias\': \'rpn_cls_logits_b\',\n            \'RPN_bbox_pred.weight\': \'rpn_bbox_pred_w\',\n            \'RPN_bbox_pred.bias\': \'rpn_bbox_pred_b\'\n        }\n        orphan_in_detectron = []\n        return detectron_weight_mapping, orphan_in_detectron\n\n    def forward(self, x, im_info, roidb=None):\n        """"""\n        x: feature maps from the backbone network. (Variable)\n        im_info: (CPU Variable)\n        roidb: (list of ndarray)\n        """"""\n        rpn_conv = F.relu(self.RPN_conv(x), inplace=True)\n\n        rpn_cls_logits = self.RPN_cls_score(rpn_conv)\n\n        rpn_bbox_pred = self.RPN_bbox_pred(rpn_conv)\n\n        return_dict = {\n            \'rpn_cls_logits\': rpn_cls_logits, \'rpn_bbox_pred\': rpn_bbox_pred}\n\n        if not self.training or cfg.MODEL.FASTER_RCNN:\n            # Proposals are needed during:\n            #  1) inference (== not model.train) for RPN only and Faster R-CNN\n            #  OR\n            #  2) training for Faster R-CNN\n            # Otherwise (== training for RPN only), proposals are not needed\n            if cfg.RPN.CLS_ACTIVATION == \'softmax\':\n                B, C, H, W = rpn_cls_logits.size()\n                rpn_cls_prob = F.softmax(\n                    rpn_cls_logits.view(B, 2, C // 2, H, W), dim=1)\n                rpn_cls_prob = rpn_cls_prob[:, 1].squeeze(dim=1)\n            else:\n                rpn_cls_prob = F.sigmoid(rpn_cls_logits)\n\n            rpn_rois, rpn_rois_prob = self.RPN_GenerateProposals(\n                rpn_cls_prob, rpn_bbox_pred, im_info)\n\n            return_dict[\'rpn_rois\'] = rpn_rois\n            return_dict[\'rpn_roi_probs\'] = rpn_rois_prob\n\n        if cfg.MODEL.FASTER_RCNN :\n            if self.training:\n                # Add op that generates training labels for in-network RPN proposals\n                blobs_out = self.RPN_GenerateProposalLabels(rpn_rois, roidb, im_info)\n                return_dict.update(blobs_out)\n            else:\n                # Alias rois to rpn_rois for inference\n                return_dict[\'rois\'] = return_dict[\'rpn_rois\']\n\n        return return_dict\n\n\ndef single_scale_rpn_losses(\n        rpn_cls_logits, rpn_bbox_pred,\n        rpn_labels_int32_wide, rpn_bbox_targets_wide,\n        rpn_bbox_inside_weights_wide, rpn_bbox_outside_weights_wide):\n    """"""Add losses for a single scale RPN model (i.e., no FPN).""""""\n    h, w = rpn_cls_logits.shape[2:]\n    rpn_labels_int32 = rpn_labels_int32_wide[:, :, :h, :w]   # -1 means ignore\n    h, w = rpn_bbox_pred.shape[2:]\n    rpn_bbox_targets = rpn_bbox_targets_wide[:, :, :h, :w]\n    rpn_bbox_inside_weights = rpn_bbox_inside_weights_wide[:, :, :h, :w]\n    rpn_bbox_outside_weights = rpn_bbox_outside_weights_wide[:, :, :h, :w]\n\n    if cfg.RPN.CLS_ACTIVATION == \'softmax\':\n        B, C, H, W = rpn_cls_logits.size()\n        rpn_cls_logits = rpn_cls_logits.view(\n            B, 2, C // 2, H, W).permute(0, 2, 3, 4, 1).contiguous().view(-1, 2)\n        rpn_labels_int32 = rpn_labels_int32.contiguous().view(-1).long()\n        # the loss is averaged over non-ignored targets\n        loss_rpn_cls = F.cross_entropy(\n            rpn_cls_logits, rpn_labels_int32, ignore_index=-1)\n    else:\n        weight = (rpn_labels_int32 >= 0).float()\n        loss_rpn_cls = F.binary_cross_entropy_with_logits(\n            rpn_cls_logits, rpn_labels_int32.float(), weight, size_average=False)\n        loss_rpn_cls /= weight.sum()\n\n    loss_rpn_bbox = net_utils.smooth_l1_loss(\n        rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights,\n        beta=1/9)\n\n    return loss_rpn_cls, loss_rpn_bbox\n'"
lib/nn/__init__.py,0,b'from .modules import *\nfrom .parallel import DataParallel\nfrom . import init'
lib/nn/functional.py,0,"b'""""""Functional interface""""""\n\n\ndef group_norm(x, num_groups, weight=None, bias=None, eps=1e-5):\n    input_shape = x.shape\n    ndim = len(input_shape)\n    N, C = input_shape[:2]\n    G = num_groups\n    assert C % G == 0, ""input channel dimension must divisible by number of groups""\n    x = x.view(N, G, -1)\n    mean = x.mean(-1, keepdim=True)\n    var = x.var(-1, keepdim=True)\n    x = (x - mean) / (var + eps).sqrt()\n    x = x.view(input_shape)\n    view_shape = (1, -1) + (1,) * (ndim - 2)\n    if weight is not None:\n        return x * weight.view(view_shape) + bias.view(view_shape)\n    return x\n'"
lib/nn/init.py,1,"b'""""""Parameter initialization functions\n""""""\n\nimport math\nimport operator\nfrom functools import reduce\n\nimport torch.nn.init as init\n\n\ndef XavierFill(tensor):\n    """"""Caffe2 XavierFill Implementation""""""\n    size = reduce(operator.mul, tensor.shape, 1)\n    fan_in = size / tensor.shape[0]\n    scale = math.sqrt(3 / fan_in)\n    return init.uniform_(tensor, -scale, scale)\n\n\ndef MSRAFill(tensor):\n    """"""Caffe2 MSRAFill Implementation""""""\n    size = reduce(operator.mul, tensor.shape, 1)\n    fan_out = size / tensor.shape[1]\n    scale = math.sqrt(2 / fan_out)\n    return init.normal_(tensor, 0, scale)\n'"
lib/roi_data/__init__.py,0,b''
lib/roi_data/data_utils.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Common utility functions for RPN and RetinaNet minibtach blobs preparation.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import namedtuple\nimport logging\nimport numpy as np\nimport threading\n\nfrom core.config import cfg\nfrom modeling.generate_anchors import generate_anchors\nimport utils.boxes as box_utils\n\nlogger = logging.getLogger(__name__)\n\n\n# octave and aspect fields are only used on RetinaNet. Octave corresponds to the\n# scale of the anchor and aspect denotes which aspect ratio is used in the range\n# of aspect ratios\nFieldOfAnchors = namedtuple(\n    \'FieldOfAnchors\', [\n        \'field_of_anchors\', \'num_cell_anchors\', \'stride\', \'field_size\',\n        \'octave\', \'aspect\'\n    ]\n)\n\n# Cache for memoizing _get_field_of_anchors\n_threadlocal_foa = threading.local()\n\n\ndef get_field_of_anchors(\n    stride, anchor_sizes, anchor_aspect_ratios, octave=None, aspect=None\n):\n    global _threadlocal_foa\n    if not hasattr(_threadlocal_foa, \'cache\'):\n        _threadlocal_foa.cache = {}\n\n    cache_key = str(stride) + str(anchor_sizes) + str(anchor_aspect_ratios)\n    if cache_key in _threadlocal_foa.cache:\n        return _threadlocal_foa.cache[cache_key]\n\n    # Anchors at a single feature cell\n    cell_anchors = generate_anchors(\n        stride=stride, sizes=anchor_sizes, aspect_ratios=anchor_aspect_ratios\n    )\n    num_cell_anchors = cell_anchors.shape[0]\n\n    # Generate canonical proposals from shifted anchors\n    # Enumerate all shifted positions on the (H, W) grid\n    fpn_max_size = cfg.FPN.COARSEST_STRIDE * np.ceil(\n        cfg.TRAIN.MAX_SIZE / float(cfg.FPN.COARSEST_STRIDE)\n    )\n    field_size = int(np.ceil(fpn_max_size / float(stride)))\n    shifts = np.arange(0, field_size) * stride\n    shift_x, shift_y = np.meshgrid(shifts, shifts)\n    shift_x = shift_x.ravel()\n    shift_y = shift_y.ravel()\n    shifts = np.vstack((shift_x, shift_y, shift_x, shift_y)).transpose()\n\n    # Broacast anchors over shifts to enumerate all anchors at all positions\n    # in the (H, W) grid:\n    #   - add A cell anchors of shape (1, A, 4) to\n    #   - K shifts of shape (K, 1, 4) to get\n    #   - all shifted anchors of shape (K, A, 4)\n    #   - reshape to (K*A, 4) shifted anchors\n    A = num_cell_anchors\n    K = shifts.shape[0]\n    field_of_anchors = (\n        cell_anchors.reshape((1, A, 4)) +\n        shifts.reshape((1, K, 4)).transpose((1, 0, 2))\n    )\n    field_of_anchors = field_of_anchors.reshape((K * A, 4))\n    foa = FieldOfAnchors(\n        field_of_anchors=field_of_anchors.astype(np.float32),\n        num_cell_anchors=num_cell_anchors,\n        stride=stride,\n        field_size=field_size,\n        octave=octave,\n        aspect=aspect\n    )\n    _threadlocal_foa.cache[cache_key] = foa\n    return foa\n\n\ndef unmap(data, count, inds, fill=0):\n    """"""Unmap a subset of item (data) back to the original set of items (of\n    size count)""""""\n    if count == len(inds):\n        return data\n\n    if len(data.shape) == 1:\n        ret = np.empty((count, ), dtype=data.dtype)\n        ret.fill(fill)\n        ret[inds] = data\n    else:\n        ret = np.empty((count, ) + data.shape[1:], dtype=data.dtype)\n        ret.fill(fill)\n        ret[inds, :] = data\n    return ret\n\n\ndef compute_targets(ex_rois, gt_rois, weights=(1.0, 1.0, 1.0, 1.0)):\n    """"""Compute bounding-box regression targets for an image.""""""\n    return box_utils.bbox_transform_inv(ex_rois, gt_rois, weights).astype(\n        np.float32, copy=False\n    )\n'"
lib/roi_data/fast_rcnn.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n""""""Construct minibatches for Fast R-CNN training. Handles the minibatch blobs\nthat are specific to Fast R-CNN. Other blobs that are generic to RPN, etc.\nare handled by their respecitive roi_data modules.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\nimport numpy.random as npr\n\nfrom core.config import cfg\nimport roi_data.keypoint_rcnn\nimport roi_data.mask_rcnn\nimport utils.boxes as box_utils\nimport utils.blob as blob_utils\nimport utils.fpn as fpn_utils\n\n\ndef get_fast_rcnn_blob_names(is_training=True):\n    """"""Fast R-CNN blob names.""""""\n    # rois blob: holds R regions of interest, each is a 5-tuple\n    # (batch_idx, x1, y1, x2, y2) specifying an image batch index and a\n    # rectangle (x1, y1, x2, y2)\n    blob_names = [\'rois\']\n    if is_training:\n        # labels_int32 blob: R categorical labels in [0, ..., K] for K\n        # foreground classes plus background\n        blob_names += [\'labels_int32\']\n    if is_training:\n        # bbox_targets blob: R bounding-box regression targets with 4\n        # targets per class\n        blob_names += [\'bbox_targets\']\n        # bbox_inside_weights blob: At most 4 targets per roi are active\n        # this binary vector sepcifies the subset of active targets\n        blob_names += [\'bbox_inside_weights\']\n        blob_names += [\'bbox_outside_weights\']\n    if is_training and cfg.MODEL.MASK_ON:\n        # \'mask_rois\': RoIs sampled for training the mask prediction branch.\n        # Shape is (#masks, 5) in format (batch_idx, x1, y1, x2, y2).\n        blob_names += [\'mask_rois\']\n        # \'roi_has_mask\': binary labels for the RoIs specified in \'rois\'\n        # indicating if each RoI has a mask or not. Note that in some cases\n        # a *bg* RoI will have an all -1 (ignore) mask associated with it in\n        # the case that no fg RoIs can be sampled. Shape is (batchsize).\n        blob_names += [\'roi_has_mask_int32\']\n        # \'masks_int32\' holds binary masks for the RoIs specified in\n        # \'mask_rois\'. Shape is (#fg, M * M) where M is the ground truth\n        # mask size.\n        # if cfg.MRCNN.CLS_SPECIFIC_MASK: Shape is (#masks, #classes * M ** 2)\n        blob_names += [\'masks_int32\']\n    if is_training and cfg.MODEL.KEYPOINTS_ON:\n        # \'keypoint_rois\': RoIs sampled for training the keypoint prediction\n        # branch. Shape is (#instances, 5) in format (batch_idx, x1, y1, x2,\n        # y2).\n        blob_names += [\'keypoint_rois\']\n        # \'keypoint_locations_int32\': index of keypoint in\n        # KRCNN.HEATMAP_SIZE**2 sized array. Shape is (#instances * #keypoints). Used in\n        # SoftmaxWithLoss.\n        blob_names += [\'keypoint_locations_int32\']\n        # \'keypoint_weights\': weight assigned to each target in\n        # \'keypoint_locations_int32\'. Shape is (#instances * #keypoints). Used in\n        # SoftmaxWithLoss.\n        blob_names += [\'keypoint_weights\']\n        # \'keypoint_loss_normalizer\': optional normalization factor to use if\n        # cfg.KRCNN.NORMALIZE_BY_VISIBLE_KEYPOINTS is False.\n        blob_names += [\'keypoint_loss_normalizer\']\n    if cfg.FPN.FPN_ON and cfg.FPN.MULTILEVEL_ROIS:\n        # Support for FPN multi-level rois without bbox reg isn\'t\n        # implemented (... and may never be implemented)\n        k_max = cfg.FPN.ROI_MAX_LEVEL\n        k_min = cfg.FPN.ROI_MIN_LEVEL\n        # Same format as rois blob, but one per FPN level\n        for lvl in range(k_min, k_max + 1):\n            blob_names += [\'rois_fpn\' + str(lvl)]\n        blob_names += [\'rois_idx_restore_int32\']\n        if is_training:\n            if cfg.MODEL.MASK_ON:\n                for lvl in range(k_min, k_max + 1):\n                    blob_names += [\'mask_rois_fpn\' + str(lvl)]\n                blob_names += [\'mask_rois_idx_restore_int32\']\n            if cfg.MODEL.KEYPOINTS_ON:\n                for lvl in range(k_min, k_max + 1):\n                    blob_names += [\'keypoint_rois_fpn\' + str(lvl)]\n                blob_names += [\'keypoint_rois_idx_restore_int32\']\n    return blob_names\n\n\ndef add_fast_rcnn_blobs(blobs, im_scales, roidb):\n    """"""Add blobs needed for training Fast R-CNN style models.""""""\n    # Sample training RoIs from each image and append them to the blob lists\n    for im_i, entry in enumerate(roidb):\n        frcn_blobs = _sample_rois(entry, im_scales[im_i], im_i)\n        for k, v in frcn_blobs.items():\n            blobs[k].append(v)\n    # Concat the training blob lists into tensors\n    for k, v in blobs.items():\n        if isinstance(v, list) and len(v) > 0:\n            blobs[k] = np.concatenate(v)\n    # Add FPN multilevel training RoIs, if configured\n    if cfg.FPN.FPN_ON and cfg.FPN.MULTILEVEL_ROIS:\n        _add_multilevel_rois(blobs)\n\n    # Perform any final work and validity checks after the collating blobs for\n    # all minibatch images\n    valid = True\n    if cfg.MODEL.KEYPOINTS_ON:\n        valid = roi_data.keypoint_rcnn.finalize_keypoint_minibatch(blobs, valid)\n\n    return valid\n\n\ndef _sample_rois(roidb, im_scale, batch_idx):\n    """"""Generate a random sample of RoIs comprising foreground and background\n    examples.\n    """"""\n    rois_per_image = int(cfg.TRAIN.BATCH_SIZE_PER_IM)\n    fg_rois_per_image = int(np.round(cfg.TRAIN.FG_FRACTION * rois_per_image))\n    max_overlaps = roidb[\'max_overlaps\']\n\n    # Select foreground RoIs as those with >= FG_THRESH overlap\n    fg_inds = np.where(max_overlaps >= cfg.TRAIN.FG_THRESH)[0]\n    # Guard against the case when an image has fewer than fg_rois_per_image\n    # foreground RoIs\n    fg_rois_per_this_image = np.minimum(fg_rois_per_image, fg_inds.size)\n    # Sample foreground regions without replacement\n    if fg_inds.size > 0:\n        fg_inds = npr.choice(\n            fg_inds, size=fg_rois_per_this_image, replace=False)\n\n    # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n    bg_inds = np.where((max_overlaps < cfg.TRAIN.BG_THRESH_HI) &\n                       (max_overlaps >= cfg.TRAIN.BG_THRESH_LO))[0]\n    # Compute number of background RoIs to take from this image (guarding\n    # against there being fewer than desired)\n    bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n    bg_rois_per_this_image = np.minimum(bg_rois_per_this_image, bg_inds.size)\n    # Sample foreground regions without replacement\n    if bg_inds.size > 0:\n        bg_inds = npr.choice(\n            bg_inds, size=bg_rois_per_this_image, replace=False)\n\n    # The indices that we\'re selecting (both fg and bg)\n    keep_inds = np.append(fg_inds, bg_inds)\n    # Label is the class each RoI has max overlap with\n    sampled_labels = roidb[\'max_classes\'][keep_inds]\n    sampled_labels[fg_rois_per_this_image:] = 0  # Label bg RoIs with class 0\n    sampled_boxes = roidb[\'boxes\'][keep_inds]\n\n    if \'bbox_targets\' not in roidb:\n        gt_inds = np.where(roidb[\'gt_classes\'] > 0)[0]\n        gt_boxes = roidb[\'boxes\'][gt_inds, :]\n        gt_assignments = gt_inds[roidb[\'box_to_gt_ind_map\'][keep_inds]]\n        bbox_targets = _compute_targets(\n            sampled_boxes, gt_boxes[gt_assignments, :], sampled_labels)\n        bbox_targets, bbox_inside_weights = _expand_bbox_targets(bbox_targets)\n    else:\n        bbox_targets, bbox_inside_weights = _expand_bbox_targets(\n            roidb[\'bbox_targets\'][keep_inds, :])\n\n    bbox_outside_weights = np.array(\n        bbox_inside_weights > 0, dtype=bbox_inside_weights.dtype)\n\n    # Scale rois and format as (batch_idx, x1, y1, x2, y2)\n    sampled_rois = sampled_boxes * im_scale\n    repeated_batch_idx = batch_idx * blob_utils.ones((sampled_rois.shape[0], 1))\n    sampled_rois = np.hstack((repeated_batch_idx, sampled_rois))\n\n    # Base Fast R-CNN blobs\n    blob_dict = dict(\n        labels_int32=sampled_labels.astype(np.int32, copy=False),\n        rois=sampled_rois,\n        bbox_targets=bbox_targets,\n        bbox_inside_weights=bbox_inside_weights,\n        bbox_outside_weights=bbox_outside_weights)\n\n    # Optionally add Mask R-CNN blobs\n    if cfg.MODEL.MASK_ON:\n        roi_data.mask_rcnn.add_mask_rcnn_blobs(blob_dict, sampled_boxes, roidb,\n                                               im_scale, batch_idx)\n\n    # Optionally add Keypoint R-CNN blobs\n    if cfg.MODEL.KEYPOINTS_ON:\n        roi_data.keypoint_rcnn.add_keypoint_rcnn_blobs(\n            blob_dict, roidb, fg_rois_per_image, fg_inds, im_scale, batch_idx)\n\n    return blob_dict\n\n\ndef _compute_targets(ex_rois, gt_rois, labels):\n    """"""Compute bounding-box regression targets for an image.""""""\n\n    assert ex_rois.shape[0] == gt_rois.shape[0]\n    assert ex_rois.shape[1] == 4\n    assert gt_rois.shape[1] == 4\n\n    targets = box_utils.bbox_transform_inv(ex_rois, gt_rois,\n                                           cfg.MODEL.BBOX_REG_WEIGHTS)\n    # Use class ""1"" for all fg boxes if using class_agnostic_bbox_reg\n    if cfg.MODEL.CLS_AGNOSTIC_BBOX_REG:\n        labels.clip(max=1, out=labels)\n    return np.hstack((labels[:, np.newaxis], targets)).astype(\n        np.float32, copy=False)\n\n\ndef _expand_bbox_targets(bbox_target_data):\n    """"""Bounding-box regression targets are stored in a compact form in the\n    roidb.\n\n    This function expands those targets into the 4-of-4*K representation used\n    by the network (i.e. only one class has non-zero targets). The loss weights\n    are similarly expanded.\n\n    Returns:\n        bbox_target_data (ndarray): N x 4K blob of regression targets\n        bbox_inside_weights (ndarray): N x 4K blob of loss weights\n    """"""\n    num_bbox_reg_classes = cfg.MODEL.NUM_CLASSES\n    if cfg.MODEL.CLS_AGNOSTIC_BBOX_REG:\n        num_bbox_reg_classes = 2  # bg and fg\n\n    clss = bbox_target_data[:, 0]\n    bbox_targets = blob_utils.zeros((clss.size, 4 * num_bbox_reg_classes))\n    bbox_inside_weights = blob_utils.zeros(bbox_targets.shape)\n    inds = np.where(clss > 0)[0]\n    for ind in inds:\n        cls = int(clss[ind])\n        start = 4 * cls\n        end = start + 4\n        bbox_targets[ind, start:end] = bbox_target_data[ind, 1:]\n        bbox_inside_weights[ind, start:end] = (1.0, 1.0, 1.0, 1.0)\n    return bbox_targets, bbox_inside_weights\n\n\ndef _add_multilevel_rois(blobs):\n    """"""By default training RoIs are added for a single feature map level only.\n    When using FPN, the RoIs must be distributed over different FPN levels\n    according the level assignment heuristic (see: modeling.FPN.\n    map_rois_to_fpn_levels).\n    """"""\n    lvl_min = cfg.FPN.ROI_MIN_LEVEL\n    lvl_max = cfg.FPN.ROI_MAX_LEVEL\n\n    def _distribute_rois_over_fpn_levels(rois_blob_name):\n        """"""Distribute rois over the different FPN levels.""""""\n        # Get target level for each roi\n        # Recall blob rois are in (batch_idx, x1, y1, x2, y2) format, hence take\n        # the box coordinates from columns 1:5\n        target_lvls = fpn_utils.map_rois_to_fpn_levels(\n            blobs[rois_blob_name][:, 1:5], lvl_min, lvl_max\n        )\n        # Add per FPN level roi blobs named like: <rois_blob_name>_fpn<lvl>\n        fpn_utils.add_multilevel_roi_blobs(\n            blobs, rois_blob_name, blobs[rois_blob_name], target_lvls, lvl_min,\n            lvl_max\n        )\n\n    _distribute_rois_over_fpn_levels(\'rois\')\n    if cfg.MODEL.MASK_ON:\n        _distribute_rois_over_fpn_levels(\'mask_rois\')\n    if cfg.MODEL.KEYPOINTS_ON:\n        _distribute_rois_over_fpn_levels(\'keypoint_rois\')\n'"
lib/roi_data/keypoint_rcnn.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n""""""Construct minibatches for Mask R-CNN training when keypoints are enabled.\nHandles the minibatch blobs that are specific to training Mask R-CNN for\nkeypoint detection. Other blobs that are generic to RPN or Fast/er R-CNN are\nhandled by their respecitive roi_data modules.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\n\nfrom core.config import cfg\nimport utils.blob as blob_utils\nimport utils.keypoints as keypoint_utils\n\n\ndef add_keypoint_rcnn_blobs(blobs, roidb, fg_rois_per_image, fg_inds, im_scale,\n                            batch_idx):\n    """"""Add Mask R-CNN keypoint specific blobs to the given blobs dictionary.""""""\n    # Note: gt_inds must match how they\'re computed in\n    # datasets.json_dataset._merge_proposal_boxes_into_roidb\n    gt_inds = np.where(roidb[\'gt_classes\'] > 0)[0]\n    max_overlaps = roidb[\'max_overlaps\']\n    gt_keypoints = roidb[\'gt_keypoints\']\n\n    ind_kp = gt_inds[roidb[\'box_to_gt_ind_map\']]\n    within_box = _within_box(gt_keypoints[ind_kp, :, :], roidb[\'boxes\'])\n    vis_kp = gt_keypoints[ind_kp, 2, :] > 0\n    is_visible = np.sum(np.logical_and(vis_kp, within_box), axis=1) > 0\n    kp_fg_inds = np.where(\n        np.logical_and(max_overlaps >= cfg.TRAIN.FG_THRESH, is_visible))[0]\n\n    kp_fg_rois_per_this_image = np.minimum(fg_rois_per_image, kp_fg_inds.size)\n    if kp_fg_inds.size > kp_fg_rois_per_this_image:\n        kp_fg_inds = np.random.choice(\n            kp_fg_inds, size=kp_fg_rois_per_this_image, replace=False)\n\n    sampled_fg_rois = roidb[\'boxes\'][kp_fg_inds]\n    box_to_gt_ind_map = roidb[\'box_to_gt_ind_map\'][kp_fg_inds]\n\n    num_keypoints = gt_keypoints.shape[2]\n    sampled_keypoints = -np.ones(\n        (len(sampled_fg_rois), gt_keypoints.shape[1], num_keypoints),\n        dtype=gt_keypoints.dtype)\n    for ii in range(len(sampled_fg_rois)):\n        ind = box_to_gt_ind_map[ii]\n        if ind >= 0:\n            sampled_keypoints[ii, :, :] = gt_keypoints[gt_inds[ind], :, :]\n            assert np.sum(sampled_keypoints[ii, 2, :]) > 0\n\n    heats, weights = keypoint_utils.keypoints_to_heatmap_labels(\n        sampled_keypoints, sampled_fg_rois)\n\n    shape = (sampled_fg_rois.shape[0] * cfg.KRCNN.NUM_KEYPOINTS,)\n    heats = heats.reshape(shape)\n    weights = weights.reshape(shape)\n\n    sampled_fg_rois *= im_scale\n    repeated_batch_idx = batch_idx * blob_utils.ones((sampled_fg_rois.shape[0],\n                                                      1))\n    sampled_fg_rois = np.hstack((repeated_batch_idx, sampled_fg_rois))\n\n    blobs[\'keypoint_rois\'] = sampled_fg_rois\n    blobs[\'keypoint_locations_int32\'] = heats.astype(np.int32, copy=False)\n    blobs[\'keypoint_weights\'] = weights\n\n\ndef finalize_keypoint_minibatch(blobs, valid):\n    """"""Finalize the minibatch after blobs for all minibatch images have been\n    collated.\n    """"""\n    min_count = cfg.KRCNN.MIN_KEYPOINT_COUNT_FOR_VALID_MINIBATCH\n    num_visible_keypoints = np.sum(blobs[\'keypoint_weights\'])\n    valid = (valid and len(blobs[\'keypoint_weights\']) > 0\n             and num_visible_keypoints > min_count)\n    # Normalizer to use if cfg.KRCNN.NORMALIZE_BY_VISIBLE_KEYPOINTS is False.\n    # See modeling.model_builder.add_keypoint_losses\n    norm = num_visible_keypoints / (\n        cfg.TRAIN.IMS_PER_BATCH * cfg.TRAIN.BATCH_SIZE_PER_IM * cfg.TRAIN.\n        FG_FRACTION * cfg.KRCNN.NUM_KEYPOINTS)\n    blobs[\'keypoint_loss_normalizer\'] = np.array(norm, dtype=np.float32)\n    return valid\n\n\ndef _within_box(points, boxes):\n    """"""Validate which keypoints are contained inside a given box.\n\n    points: Nx2xK\n    boxes: Nx4\n    output: NxK\n    """"""\n    x_within = np.logical_and(\n        points[:, 0, :] >= np.expand_dims(boxes[:, 0], axis=1),\n        points[:, 0, :] <= np.expand_dims(boxes[:, 2], axis=1))\n    y_within = np.logical_and(\n        points[:, 1, :] >= np.expand_dims(boxes[:, 1], axis=1),\n        points[:, 1, :] <= np.expand_dims(boxes[:, 3], axis=1))\n    return np.logical_and(x_within, y_within)\n'"
lib/roi_data/loader.py,6,"b'import math\nimport numpy as np\nimport numpy.random as npr\n\nimport torch\nimport torch.utils.data as data\nimport torch.utils.data.sampler as torch_sampler\nfrom torch.utils.data.dataloader import default_collate\nfrom torch._six import int_classes as _int_classes\n\nfrom core.config import cfg\nfrom roi_data.minibatch import get_minibatch\nimport utils.blob as blob_utils\n# from model.rpn.bbox_transform import bbox_transform_inv, clip_boxes\n\n\nclass RoiDataLoader(data.Dataset):\n    def __init__(self, roidb, num_classes, training=True):\n        self._roidb = roidb\n        self._num_classes = num_classes\n        self.training = training\n        self.DATA_SIZE = len(self._roidb)\n\n    def __getitem__(self, index_tuple):\n        index, ratio = index_tuple\n        single_db = [self._roidb[index]]\n        blobs, valid = get_minibatch(single_db)\n        #TODO: Check if minibatch is valid ? If not, abandon it.\n        # Need to change _worker_loop in torch.utils.data.dataloader.py.\n\n        # Squeeze batch dim\n        for key in blobs:\n            if key != \'roidb\':\n                blobs[key] = blobs[key].squeeze(axis=0)\n\n        if self._roidb[index][\'need_crop\']:\n            self.crop_data(blobs, ratio)\n            # Check bounding box\n            entry = blobs[\'roidb\'][0]\n            boxes = entry[\'boxes\']\n            invalid = (boxes[:, 0] == boxes[:, 2]) | (boxes[:, 1] == boxes[:, 3])\n            valid_inds = np.nonzero(~ invalid)[0]\n            if len(valid_inds) < len(boxes):\n                for key in [\'boxes\', \'gt_classes\', \'seg_areas\', \'gt_overlaps\', \'is_crowd\',\n                            \'box_to_gt_ind_map\', \'gt_keypoints\']:\n                    if key in entry:\n                        entry[key] = entry[key][valid_inds]\n                entry[\'segms\'] = [entry[\'segms\'][ind] for ind in valid_inds]\n\n        blobs[\'roidb\'] = blob_utils.serialize(blobs[\'roidb\'])  # CHECK: maybe we can serialize in collate_fn\n\n        return blobs\n\n    def crop_data(self, blobs, ratio):\n        data_height, data_width = map(int, blobs[\'im_info\'][:2])\n        boxes = blobs[\'roidb\'][0][\'boxes\']\n        if ratio < 1:  # width << height, crop height\n            size_crop = math.ceil(data_width / ratio)  # size after crop\n            min_y = math.floor(np.min(boxes[:, 1]))\n            max_y = math.floor(np.max(boxes[:, 3]))\n            box_region = max_y - min_y + 1\n            if min_y == 0:\n                y_s = 0\n            else:\n                if (box_region - size_crop) < 0:\n                    y_s_min = max(max_y - size_crop, 0)\n                    y_s_max = min(min_y, data_height - size_crop)\n                    y_s = y_s_min if y_s_min == y_s_max else \\\n                        npr.choice(range(y_s_min, y_s_max + 1))\n                else:\n                    # CHECK: rethinking the mechnism for the case box_region > size_crop\n                    # Now, the crop is biased on the lower part of box_region caused by\n                    # // 2 for y_s_add\n                    y_s_add = (box_region - size_crop) // 2\n                    y_s = min_y if y_s_add == 0 else \\\n                        npr.choice(range(min_y, min_y + y_s_add + 1))\n            # Crop the image\n            blobs[\'data\'] = blobs[\'data\'][:, y_s:(y_s + size_crop), :,]\n            # Update im_info\n            blobs[\'im_info\'][0] = size_crop\n            # Shift and clamp boxes ground truth\n            boxes[:, 1] -= y_s\n            boxes[:, 3] -= y_s\n            np.clip(boxes[:, 1], 0, size_crop - 1, out=boxes[:, 1])\n            np.clip(boxes[:, 3], 0, size_crop - 1, out=boxes[:, 3])\n            blobs[\'roidb\'][0][\'boxes\'] = boxes\n        else:  # width >> height, crop width\n            size_crop = math.ceil(data_height * ratio)\n            min_x = math.floor(np.min(boxes[:, 0]))\n            max_x = math.floor(np.max(boxes[:, 2]))\n            box_region = max_x - min_x + 1\n            if min_x == 0:\n                x_s = 0\n            else:\n                if (box_region - size_crop) < 0:\n                    x_s_min = max(max_x - size_crop, 0)\n                    x_s_max = min(min_x, data_width - size_crop)\n                    x_s = x_s_min if x_s_min == x_s_max else \\\n                        npr.choice(range(x_s_min, x_s_max + 1))\n                else:\n                    x_s_add = (box_region - size_crop) // 2\n                    x_s = min_x if x_s_add == 0 else \\\n                        npr.choice(range(min_x, min_x + x_s_add + 1))\n            # Crop the image\n            blobs[\'data\'] = blobs[\'data\'][:, :, x_s:(x_s + size_crop)]\n            # Update im_info\n            blobs[\'im_info\'][1] = size_crop\n            # Shift and clamp boxes ground truth\n            boxes[:, 0] -= x_s\n            boxes[:, 2] -= x_s\n            np.clip(boxes[:, 0], 0, size_crop - 1, out=boxes[:, 0])\n            np.clip(boxes[:, 2], 0, size_crop - 1, out=boxes[:, 2])\n            blobs[\'roidb\'][0][\'boxes\'] = boxes\n\n    def __len__(self):\n        return self.DATA_SIZE\n\n\ndef cal_minibatch_ratio(ratio_list):\n    """"""Given the ratio_list, we want to make the RATIO same for each minibatch on each GPU.\n    Note: this only work for 1) cfg.TRAIN.MAX_SIZE is ignored during `prep_im_for_blob` \n    and 2) cfg.TRAIN.SCALES containing SINGLE scale.\n    Since all prepared images will have same min side length of cfg.TRAIN.SCALES[0], we can\n     pad and batch images base on that.\n    """"""\n    DATA_SIZE = len(ratio_list)\n    ratio_list_minibatch = np.empty((DATA_SIZE,))\n    num_minibatch = int(np.ceil(DATA_SIZE / cfg.TRAIN.IMS_PER_BATCH))  # Include leftovers\n    for i in range(num_minibatch):\n        left_idx = i * cfg.TRAIN.IMS_PER_BATCH\n        right_idx = min((i+1) * cfg.TRAIN.IMS_PER_BATCH - 1, DATA_SIZE - 1)\n\n        if ratio_list[right_idx] < 1:\n            # for ratio < 1, we preserve the leftmost in each batch.\n            target_ratio = ratio_list[left_idx]\n        elif ratio_list[left_idx] > 1:\n            # for ratio > 1, we preserve the rightmost in each batch.\n            target_ratio = ratio_list[right_idx]\n        else:\n            # for ratio cross 1, we make it to be 1.\n            target_ratio = 1\n\n        ratio_list_minibatch[left_idx:(right_idx+1)] = target_ratio\n    return ratio_list_minibatch\n\n\nclass MinibatchSampler(torch_sampler.Sampler):\n    def __init__(self, ratio_list, ratio_index):\n        self.ratio_list = ratio_list\n        self.ratio_index = ratio_index\n        self.num_data = len(ratio_list)\n\n        if cfg.TRAIN.ASPECT_GROUPING:\n            # Given the ratio_list, we want to make the ratio same\n            # for each minibatch on each GPU.\n            self.ratio_list_minibatch = cal_minibatch_ratio(ratio_list)\n\n    def __iter__(self):\n        if cfg.TRAIN.ASPECT_GROUPING:\n            # indices for aspect grouping awared permutation\n            n, rem = divmod(self.num_data, cfg.TRAIN.IMS_PER_BATCH)\n            round_num_data = n * cfg.TRAIN.IMS_PER_BATCH\n            indices = np.arange(round_num_data)\n            npr.shuffle(indices.reshape(-1, cfg.TRAIN.IMS_PER_BATCH))  # inplace shuffle\n            if rem != 0:\n                indices = np.append(indices, np.arange(round_num_data, round_num_data + rem))\n            ratio_index = self.ratio_index[indices]\n            ratio_list_minibatch = self.ratio_list_minibatch[indices]\n        else:\n            rand_perm = npr.permutation(self.num_data)\n            ratio_list = self.ratio_list[rand_perm]\n            ratio_index = self.ratio_index[rand_perm]\n            # re-calculate minibatch ratio list\n            ratio_list_minibatch = cal_minibatch_ratio(ratio_list)\n\n        return iter(zip(ratio_index.tolist(), ratio_list_minibatch.tolist()))\n\n    def __len__(self):\n        return self.num_data\n\n\nclass BatchSampler(torch_sampler.BatchSampler):\n    r""""""Wraps another sampler to yield a mini-batch of indices.\n    Args:\n        sampler (Sampler): Base sampler.\n        batch_size (int): Size of mini-batch.\n        drop_last (bool): If ``True``, the sampler will drop the last batch if\n            its size would be less than ``batch_size``\n    Example:\n        >>> list(BatchSampler(range(10), batch_size=3, drop_last=False))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n        >>> list(BatchSampler(range(10), batch_size=3, drop_last=True))\n        [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n    """"""\n\n    def __init__(self, sampler, batch_size, drop_last):\n        if not isinstance(sampler, torch_sampler.Sampler):\n            raise ValueError(""sampler should be an instance of ""\n                             ""torch.utils.data.Sampler, but got sampler={}""\n                             .format(sampler))\n        if not isinstance(batch_size, _int_classes) or isinstance(batch_size, bool) or \\\n                batch_size <= 0:\n            raise ValueError(""batch_size should be a positive integeral value, ""\n                             ""but got batch_size={}"".format(batch_size))\n        if not isinstance(drop_last, bool):\n            raise ValueError(""drop_last should be a boolean value, but got ""\n                             ""drop_last={}"".format(drop_last))\n        self.sampler = sampler\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n\n    def __iter__(self):\n        batch = []\n        for idx in self.sampler:\n            batch.append(idx)  # Difference: batch.append(int(idx))\n            if len(batch) == self.batch_size:\n                yield batch\n                batch = []\n        if len(batch) > 0 and not self.drop_last:\n            yield batch\n\n    def __len__(self):\n        if self.drop_last:\n            return len(self.sampler) // self.batch_size\n        else:\n            return (len(self.sampler) + self.batch_size - 1) // self.batch_size\n\n\n\ndef collate_minibatch(list_of_blobs):\n    """"""Stack samples seperately and return a list of minibatches\n    A batch contains NUM_GPUS minibatches and image size in different minibatch may be different.\n    Hence, we need to stack smaples from each minibatch seperately.\n    """"""\n    Batch = {key: [] for key in list_of_blobs[0]}\n    # Because roidb consists of entries of variable length, it can\'t be batch into a tensor.\n    # So we keep roidb in the type of ""list of ndarray"".\n    list_of_roidb = [blobs.pop(\'roidb\') for blobs in list_of_blobs]\n    for i in range(0, len(list_of_blobs), cfg.TRAIN.IMS_PER_BATCH):\n        mini_list = list_of_blobs[i:(i + cfg.TRAIN.IMS_PER_BATCH)]\n        # Pad image data\n        mini_list = pad_image_data(mini_list)\n        minibatch = default_collate(mini_list)\n        minibatch[\'roidb\'] = list_of_roidb[i:(i + cfg.TRAIN.IMS_PER_BATCH)]\n        for key in minibatch:\n            Batch[key].append(minibatch[key])\n\n    return Batch\n\n\ndef pad_image_data(list_of_blobs):\n    max_shape = blob_utils.get_max_shape([blobs[\'data\'].shape[1:] for blobs in list_of_blobs])\n    output_list = []\n    for blobs in list_of_blobs:\n        data_padded = np.zeros((3, max_shape[0], max_shape[1]), dtype=np.float32)\n        _, h, w = blobs[\'data\'].shape\n        data_padded[:, :h, :w] = blobs[\'data\']\n        blobs[\'data\'] = data_padded\n        output_list.append(blobs)\n    return output_list\n'"
lib/roi_data/mask_rcnn.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n""""""Construct minibatches for Mask R-CNN training. Handles the minibatch blobs\nthat are specific to Mask R-CNN. Other blobs that are generic to RPN or\nFast/er R-CNN are handled by their respecitive roi_data modules.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport logging\nimport numpy as np\n\nfrom core.config import cfg\nimport utils.blob as blob_utils\nimport utils.boxes as box_utils\nimport utils.segms as segm_utils\n\n\ndef add_mask_rcnn_blobs(blobs, sampled_boxes, roidb, im_scale, batch_idx):\n    """"""Add Mask R-CNN specific blobs to the input blob dictionary.""""""\n    # Prepare the mask targets by associating one gt mask to each training roi\n    # that has a fg (non-bg) class label.\n    M = cfg.MRCNN.RESOLUTION\n    polys_gt_inds = np.where((roidb[\'gt_classes\'] > 0) &\n                             (roidb[\'is_crowd\'] == 0))[0]\n    polys_gt = [roidb[\'segms\'][i] for i in polys_gt_inds]\n    boxes_from_polys = segm_utils.polys_to_boxes(polys_gt)\n    # boxes_from_polys = [roidb[\'boxes\'][i] for i in polys_gt_inds]\n    fg_inds = np.where(blobs[\'labels_int32\'] > 0)[0]\n    roi_has_mask = blobs[\'labels_int32\'].copy()\n    roi_has_mask[roi_has_mask > 0] = 1\n\n    if fg_inds.shape[0] > 0:\n        # Class labels for the foreground rois\n        mask_class_labels = blobs[\'labels_int32\'][fg_inds]\n        masks = blob_utils.zeros((fg_inds.shape[0], M**2), int32=True)\n\n        # Find overlap between all foreground rois and the bounding boxes\n        # enclosing each segmentation\n        rois_fg = sampled_boxes[fg_inds]\n        overlaps_bbfg_bbpolys = box_utils.bbox_overlaps(\n            rois_fg.astype(np.float32, copy=False),\n            boxes_from_polys.astype(np.float32, copy=False))\n        # Map from each fg rois to the index of the mask with highest overlap\n        # (measured by bbox overlap)\n        fg_polys_inds = np.argmax(overlaps_bbfg_bbpolys, axis=1)\n\n        # add fg targets\n        for i in range(rois_fg.shape[0]):\n            fg_polys_ind = fg_polys_inds[i]\n            poly_gt = polys_gt[fg_polys_ind]\n            roi_fg = rois_fg[i]\n            # Rasterize the portion of the polygon mask within the given fg roi\n            # to an M x M binary image\n            mask = segm_utils.polys_to_mask_wrt_box(poly_gt, roi_fg, M)\n            mask = np.array(mask > 0, dtype=np.int32)  # Ensure it\'s binary\n            masks[i, :] = np.reshape(mask, M**2)\n    else:  # If there are no fg masks (it does happen)\n        # The network cannot handle empty blobs, so we must provide a mask\n        # We simply take the first bg roi, given it an all -1\'s mask (ignore\n        # label), and label it with class zero (bg).\n        bg_inds = np.where(blobs[\'labels_int32\'] == 0)[0]\n        # rois_fg is actually one background roi, but that\'s ok because ...\n        rois_fg = sampled_boxes[bg_inds[0]].reshape((1, -1))\n        # We give it an -1\'s blob (ignore label)\n        masks = -blob_utils.ones((1, M**2), int32=True)\n        # We label it with class = 0 (background)\n        mask_class_labels = blob_utils.zeros((1, ))\n        # Mark that the first roi has a mask\n        roi_has_mask[0] = 1\n\n    if cfg.MRCNN.CLS_SPECIFIC_MASK:\n        masks = _expand_to_class_specific_mask_targets(masks,\n                                                       mask_class_labels)\n\n    # Scale rois_fg and format as (batch_idx, x1, y1, x2, y2)\n    rois_fg *= im_scale\n    repeated_batch_idx = batch_idx * blob_utils.ones((rois_fg.shape[0], 1))\n    rois_fg = np.hstack((repeated_batch_idx, rois_fg))\n\n    # Update blobs dict with Mask R-CNN blobs\n    blobs[\'mask_rois\'] = rois_fg\n    blobs[\'roi_has_mask_int32\'] = roi_has_mask\n    blobs[\'masks_int32\'] = masks\n\n\ndef _expand_to_class_specific_mask_targets(masks, mask_class_labels):\n    """"""Expand masks from shape (#masks, M ** 2) to (#masks, #classes * M ** 2)\n    to encode class specific mask targets.\n    """"""\n    assert masks.shape[0] == mask_class_labels.shape[0]\n    M = cfg.MRCNN.RESOLUTION\n\n    # Target values of -1 are ""don\'t care"" / ignore labels\n    mask_targets = -blob_utils.ones(\n        (masks.shape[0], cfg.MODEL.NUM_CLASSES * M**2), int32=True)\n\n    for i in range(masks.shape[0]):\n        cls = int(mask_class_labels[i])\n        start = M**2 * cls\n        end = start + M**2\n        # Ignore background instance\n        # (only happens when there is no fg samples in an image)\n        if cls > 0:\n            mask_targets[i, start:end] = masks[i, :]\n\n    return mask_targets\n'"
lib/roi_data/minibatch.py,0,"b'import numpy as np\nimport cv2\n\nfrom core.config import cfg\nimport utils.blob as blob_utils\nimport roi_data.rpn\n\n\ndef get_minibatch_blob_names(is_training=True):\n    """"""Return blob names in the order in which they are read by the data loader.\n    """"""\n    # data blob: holds a batch of N images, each with 3 channels\n    blob_names = [\'data\']\n    if cfg.RPN.RPN_ON:\n        # RPN-only or end-to-end Faster R-CNN\n        blob_names += roi_data.rpn.get_rpn_blob_names(is_training=is_training)\n    elif cfg.RETINANET.RETINANET_ON:\n        raise NotImplementedError\n    else:\n        # Fast R-CNN like models trained on precomputed proposals\n        blob_names += roi_data.fast_rcnn.get_fast_rcnn_blob_names(\n            is_training=is_training\n        )\n    return blob_names\n\n\ndef get_minibatch(roidb):\n    """"""Given a roidb, construct a minibatch sampled from it.""""""\n    # We collect blobs from each image onto a list and then concat them into a\n    # single tensor, hence we initialize each blob to an empty list\n    blobs = {k: [] for k in get_minibatch_blob_names()}\n\n    # Get the input image blob\n    im_blob, im_scales = _get_image_blob(roidb)\n    blobs[\'data\'] = im_blob\n    if cfg.RPN.RPN_ON:\n        # RPN-only or end-to-end Faster/Mask R-CNN\n        valid = roi_data.rpn.add_rpn_blobs(blobs, im_scales, roidb)\n    elif cfg.RETINANET.RETINANET_ON:\n        raise NotImplementedError\n    else:\n        # Fast R-CNN like models trained on precomputed proposals\n        valid = roi_data.fast_rcnn.add_fast_rcnn_blobs(blobs, im_scales, roidb)\n    return blobs, valid\n\n\ndef _get_image_blob(roidb):\n    """"""Builds an input blob from the images in the roidb at the specified\n    scales.\n    """"""\n    num_images = len(roidb)\n    # Sample random scales to use for each image in this batch\n    scale_inds = np.random.randint(\n        0, high=len(cfg.TRAIN.SCALES), size=num_images)\n    processed_ims = []\n    im_scales = []\n    for i in range(num_images):\n        im = cv2.imread(roidb[i][\'image\'])\n        assert im is not None, \\\n            \'Failed to read image \\\'{}\\\'\'.format(roidb[i][\'image\'])\n        # If NOT using opencv to read in images, uncomment following lines\n        # if len(im.shape) == 2:\n        #     im = im[:, :, np.newaxis]\n        #     im = np.concatenate((im, im, im), axis=2)\n        # # flip the channel, since the original one using cv2\n        # # rgb -> bgr\n        # im = im[:, :, ::-1]\n        if roidb[i][\'flipped\']:\n            im = im[:, ::-1, :]\n        target_size = cfg.TRAIN.SCALES[scale_inds[i]]\n        im, im_scale = blob_utils.prep_im_for_blob(\n            im, cfg.PIXEL_MEANS, [target_size], cfg.TRAIN.MAX_SIZE)\n        im_scales.append(im_scale[0])\n        processed_ims.append(im[0])\n\n    # Create a blob to hold the input images [n, c, h, w]\n    blob = blob_utils.im_list_to_blob(processed_ims)\n\n    return blob, im_scales\n'"
lib/roi_data/rpn.py,0,"b'import logging\nimport numpy as np\nimport numpy.random as npr\n\nfrom core.config import cfg\nimport roi_data.data_utils as data_utils\nimport utils.blob as blob_utils\nimport utils.boxes as box_utils\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_rpn_blob_names(is_training=True):\n    """"""Blob names used by RPN.""""""\n    # im_info: (height, width, image scale)\n    blob_names = [\'im_info\']\n    if is_training:\n        # gt boxes: (batch_idx, x1, y1, x2, y2, cls)\n        blob_names += [\'roidb\']\n        if cfg.FPN.FPN_ON and cfg.FPN.MULTILEVEL_RPN:\n            # Same format as RPN blobs, but one per FPN level\n            for lvl in range(cfg.FPN.RPN_MIN_LEVEL, cfg.FPN.RPN_MAX_LEVEL + 1):\n                blob_names += [\n                    \'rpn_labels_int32_wide_fpn\' + str(lvl),\n                    \'rpn_bbox_targets_wide_fpn\' + str(lvl),\n                    \'rpn_bbox_inside_weights_wide_fpn\' + str(lvl),\n                    \'rpn_bbox_outside_weights_wide_fpn\' + str(lvl)\n                ]\n        else:\n            # Single level RPN blobs\n            blob_names += [\n                \'rpn_labels_int32_wide\',\n                \'rpn_bbox_targets_wide\',\n                \'rpn_bbox_inside_weights_wide\',\n                \'rpn_bbox_outside_weights_wide\'\n            ]\n    return blob_names\n\n\ndef add_rpn_blobs(blobs, im_scales, roidb):\n    """"""Add blobs needed training RPN-only and end-to-end Faster R-CNN models.""""""\n    if cfg.FPN.FPN_ON and cfg.FPN.MULTILEVEL_RPN:\n        # RPN applied to many feature levels, as in the FPN paper\n        k_max = cfg.FPN.RPN_MAX_LEVEL\n        k_min = cfg.FPN.RPN_MIN_LEVEL\n        foas = []\n        for lvl in range(k_min, k_max + 1):\n            field_stride = 2.**lvl\n            anchor_sizes = (cfg.FPN.RPN_ANCHOR_START_SIZE * 2.**(lvl - k_min), )\n            anchor_aspect_ratios = cfg.FPN.RPN_ASPECT_RATIOS\n            foa = data_utils.get_field_of_anchors(\n                field_stride, anchor_sizes, anchor_aspect_ratios\n            )\n            foas.append(foa)\n        all_anchors = np.concatenate([f.field_of_anchors for f in foas])\n    else:\n        foa = data_utils.get_field_of_anchors(cfg.RPN.STRIDE, cfg.RPN.SIZES,\n                                              cfg.RPN.ASPECT_RATIOS)\n        all_anchors = foa.field_of_anchors\n\n    for im_i, entry in enumerate(roidb):\n        scale = im_scales[im_i]\n        im_height = np.round(entry[\'height\'] * scale)\n        im_width = np.round(entry[\'width\'] * scale)\n        gt_inds = np.where(\n            (entry[\'gt_classes\'] > 0) & (entry[\'is_crowd\'] == 0)\n        )[0]\n        gt_rois = entry[\'boxes\'][gt_inds, :] * scale\n        # TODO(rbg): gt_boxes is poorly named;\n        # should be something like \'gt_rois_info\'\n        gt_boxes = blob_utils.zeros((len(gt_inds), 6))\n        gt_boxes[:, 0] = im_i  # batch inds\n        gt_boxes[:, 1:5] = gt_rois\n        gt_boxes[:, 5] = entry[\'gt_classes\'][gt_inds]\n        im_info = np.array([[im_height, im_width, scale]], dtype=np.float32)\n        blobs[\'im_info\'].append(im_info)\n\n        # Add RPN targets\n        if cfg.FPN.FPN_ON and cfg.FPN.MULTILEVEL_RPN:\n            # RPN applied to many feature levels, as in the FPN paper\n            rpn_blobs = _get_rpn_blobs(\n                im_height, im_width, foas, all_anchors, gt_rois\n            )\n            for i, lvl in enumerate(range(k_min, k_max + 1)):\n                for k, v in rpn_blobs[i].items():\n                    blobs[k + \'_fpn\' + str(lvl)].append(v)\n        else:\n            # Classical RPN, applied to a single feature level\n            rpn_blobs = _get_rpn_blobs(\n                im_height, im_width, [foa], all_anchors, gt_rois\n            )\n            for k, v in rpn_blobs.items():\n                blobs[k].append(v)\n\n    for k, v in blobs.items():\n        if isinstance(v, list) and len(v) > 0:\n            blobs[k] = np.concatenate(v)\n\n    valid_keys = [\n        \'has_visible_keypoints\', \'boxes\', \'segms\', \'seg_areas\', \'gt_classes\',\n        \'gt_overlaps\', \'is_crowd\', \'box_to_gt_ind_map\', \'gt_keypoints\'\n    ]\n    minimal_roidb = [{} for _ in range(len(roidb))]\n    for i, e in enumerate(roidb):\n        for k in valid_keys:\n            if k in e:\n                minimal_roidb[i][k] = e[k]\n    # blobs[\'roidb\'] = blob_utils.serialize(minimal_roidb)\n    blobs[\'roidb\'] = minimal_roidb\n\n    # Always return valid=True, since RPN minibatches are valid by design\n    return True\n\n\ndef _get_rpn_blobs(im_height, im_width, foas, all_anchors, gt_boxes):\n    total_anchors = all_anchors.shape[0]\n    straddle_thresh = cfg.TRAIN.RPN_STRADDLE_THRESH\n\n    if straddle_thresh >= 0:\n        # Only keep anchors inside the image by a margin of straddle_thresh\n        # Set TRAIN.RPN_STRADDLE_THRESH to -1 (or a large value) to keep all\n        # anchors\n        inds_inside = np.where(\n            (all_anchors[:, 0] >= -straddle_thresh) &\n            (all_anchors[:, 1] >= -straddle_thresh) &\n            (all_anchors[:, 2] < im_width + straddle_thresh) &\n            (all_anchors[:, 3] < im_height + straddle_thresh)\n        )[0]\n        # keep only inside anchors\n        anchors = all_anchors[inds_inside, :]\n    else:\n        inds_inside = np.arange(all_anchors.shape[0])\n        anchors = all_anchors\n    num_inside = len(inds_inside)\n\n    logger.debug(\'total_anchors: %d\', total_anchors)\n    logger.debug(\'inds_inside: %d\', num_inside)\n    logger.debug(\'anchors.shape: %s\', str(anchors.shape))\n\n    # Compute anchor labels:\n    # label=1 is positive, 0 is negative, -1 is don\'t care (ignore)\n    labels = np.empty((num_inside, ), dtype=np.int32)\n    labels.fill(-1)\n    if len(gt_boxes) > 0:\n        # Compute overlaps between the anchors and the gt boxes overlaps\n        anchor_by_gt_overlap = box_utils.bbox_overlaps(anchors, gt_boxes)\n        # Map from anchor to gt box that has highest overlap\n        anchor_to_gt_argmax = anchor_by_gt_overlap.argmax(axis=1)\n        # For each anchor, amount of overlap with most overlapping gt box\n        anchor_to_gt_max = anchor_by_gt_overlap[np.arange(num_inside),\n                                                anchor_to_gt_argmax]\n\n        # Map from gt box to an anchor that has highest overlap\n        gt_to_anchor_argmax = anchor_by_gt_overlap.argmax(axis=0)\n        # For each gt box, amount of overlap with most overlapping anchor\n        gt_to_anchor_max = anchor_by_gt_overlap[\n            gt_to_anchor_argmax,\n            np.arange(anchor_by_gt_overlap.shape[1])\n        ]\n        # Find all anchors that share the max overlap amount\n        # (this includes many ties)\n        anchors_with_max_overlap = np.where(\n            anchor_by_gt_overlap == gt_to_anchor_max\n        )[0]\n\n        # Fg label: for each gt use anchors with highest overlap\n        # (including ties)\n        labels[anchors_with_max_overlap] = 1\n        # Fg label: above threshold IOU\n        labels[anchor_to_gt_max >= cfg.TRAIN.RPN_POSITIVE_OVERLAP] = 1\n\n    # subsample positive labels if we have too many\n    num_fg = int(cfg.TRAIN.RPN_FG_FRACTION * cfg.TRAIN.RPN_BATCH_SIZE_PER_IM)\n    fg_inds = np.where(labels == 1)[0]\n    if len(fg_inds) > num_fg:\n        disable_inds = npr.choice(\n            fg_inds, size=(len(fg_inds) - num_fg), replace=False\n        )\n        labels[disable_inds] = -1\n    fg_inds = np.where(labels == 1)[0]\n\n    # subsample negative labels if we have too many\n    # (samples with replacement, but since the set of bg inds is large most\n    # samples will not have repeats)\n    num_bg = cfg.TRAIN.RPN_BATCH_SIZE_PER_IM - np.sum(labels == 1)\n    bg_inds = np.where(anchor_to_gt_max < cfg.TRAIN.RPN_NEGATIVE_OVERLAP)[0]\n    if len(bg_inds) > num_bg:\n        enable_inds = bg_inds[npr.randint(len(bg_inds), size=num_bg)]\n        labels[enable_inds] = 0\n    bg_inds = np.where(labels == 0)[0]\n\n    bbox_targets = np.zeros((num_inside, 4), dtype=np.float32)\n    bbox_targets[fg_inds, :] = data_utils.compute_targets(\n        anchors[fg_inds, :], gt_boxes[anchor_to_gt_argmax[fg_inds], :]\n    )\n\n    # Bbox regression loss has the form:\n    #   loss(x) = weight_outside * L(weight_inside * x)\n    # Inside weights allow us to set zero loss on an element-wise basis\n    # Bbox regression is only trained on positive examples so we set their\n    # weights to 1.0 (or otherwise if config is different) and 0 otherwise\n    bbox_inside_weights = np.zeros((num_inside, 4), dtype=np.float32)\n    bbox_inside_weights[labels == 1, :] = (1.0, 1.0, 1.0, 1.0)\n\n    # The bbox regression loss only averages by the number of images in the\n    # mini-batch, whereas we need to average by the total number of example\n    # anchors selected\n    # Outside weights are used to scale each element-wise loss so the final\n    # average over the mini-batch is correct\n    bbox_outside_weights = np.zeros((num_inside, 4), dtype=np.float32)\n    # uniform weighting of examples (given non-uniform sampling)\n    num_examples = np.sum(labels >= 0)\n    bbox_outside_weights[labels == 1, :] = 1.0 / num_examples\n    bbox_outside_weights[labels == 0, :] = 1.0 / num_examples\n\n    # Map up to original set of anchors\n    labels = data_utils.unmap(labels, total_anchors, inds_inside, fill=-1)\n    bbox_targets = data_utils.unmap(\n        bbox_targets, total_anchors, inds_inside, fill=0\n    )\n    bbox_inside_weights = data_utils.unmap(\n        bbox_inside_weights, total_anchors, inds_inside, fill=0\n    )\n    bbox_outside_weights = data_utils.unmap(\n        bbox_outside_weights, total_anchors, inds_inside, fill=0\n    )\n\n    # Split the generated labels, etc. into labels per each field of anchors\n    blobs_out = []\n    start_idx = 0\n    for foa in foas:\n        H = foa.field_size\n        W = foa.field_size\n        A = foa.num_cell_anchors\n        end_idx = start_idx + H * W * A\n        _labels = labels[start_idx:end_idx]\n        _bbox_targets = bbox_targets[start_idx:end_idx, :]\n        _bbox_inside_weights = bbox_inside_weights[start_idx:end_idx, :]\n        _bbox_outside_weights = bbox_outside_weights[start_idx:end_idx, :]\n        start_idx = end_idx\n\n        # labels output with shape (1, A, height, width)\n        _labels = _labels.reshape((1, H, W, A)).transpose(0, 3, 1, 2)\n        # bbox_targets output with shape (1, 4 * A, height, width)\n        _bbox_targets = _bbox_targets.reshape(\n            (1, H, W, A * 4)).transpose(0, 3, 1, 2)\n        # bbox_inside_weights output with shape (1, 4 * A, height, width)\n        _bbox_inside_weights = _bbox_inside_weights.reshape(\n            (1, H, W, A * 4)).transpose(0, 3, 1, 2)\n        # bbox_outside_weights output with shape (1, 4 * A, height, width)\n        _bbox_outside_weights = _bbox_outside_weights.reshape(\n            (1, H, W, A * 4)).transpose(0, 3, 1, 2)\n        blobs_out.append(\n            dict(\n                rpn_labels_int32_wide=_labels,\n                rpn_bbox_targets_wide=_bbox_targets,\n                rpn_bbox_inside_weights_wide=_bbox_inside_weights,\n                rpn_bbox_outside_weights_wide=_bbox_outside_weights\n            )\n        )\n    return blobs_out[0] if len(blobs_out) == 1 else blobs_out\n'"
lib/utils/__init__.py,0,b''
lib/utils/blob.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n""""""blob helper functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom six.moves import cPickle as pickle\nimport numpy as np\nimport cv2\n\nfrom core.config import cfg\n\n\ndef get_image_blob(im, target_scale, target_max_size):\n    """"""Convert an image into a network input.\n\n    Arguments:\n        im (ndarray): a color image in BGR order\n\n    Returns:\n        blob (ndarray): a data blob holding an image pyramid\n        im_scale (float): image scale (target size) / (original size)\n        im_info (ndarray)\n    """"""\n    processed_im, im_scale = prep_im_for_blob(\n        im, cfg.PIXEL_MEANS, [target_scale], target_max_size\n    )\n    blob = im_list_to_blob(processed_im)\n    # NOTE: this height and width may be larger than actual scaled input image\n    # due to the FPN.COARSEST_STRIDE related padding in im_list_to_blob. We are\n    # maintaining this behavior for now to make existing results exactly\n    # reproducible (in practice using the true input image height and width\n    # yields nearly the same results, but they are sometimes slightly different\n    # because predictions near the edge of the image will be pruned more\n    # aggressively).\n    height, width = blob.shape[2], blob.shape[3]\n    im_info = np.hstack((height, width, im_scale))[np.newaxis, :]\n    return blob, im_scale, im_info.astype(np.float32)\n\n\ndef im_list_to_blob(ims):\n    """"""Convert a list of images into a network input. Assumes images were\n    prepared using prep_im_for_blob or equivalent: i.e.\n      - BGR channel order\n      - pixel means subtracted\n      - resized to the desired input size\n      - float32 numpy ndarray format\n    Output is a 4D HCHW tensor of the images concatenated along axis 0 with\n    shape.\n    """"""\n    if not isinstance(ims, list):\n        ims = [ims]\n    max_shape = get_max_shape([im.shape[:2] for im in ims])\n\n    num_images = len(ims)\n    blob = np.zeros(\n        (num_images, max_shape[0], max_shape[1], 3), dtype=np.float32)\n    for i in range(num_images):\n        im = ims[i]\n        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im\n    # Move channels (axis 3) to axis 1\n    # Axis order will become: (batch elem, channel, height, width)\n    channel_swap = (0, 3, 1, 2)\n    blob = blob.transpose(channel_swap)\n    return blob\n\n\ndef get_max_shape(im_shapes):\n    """"""Calculate max spatial size (h, w) for batching given a list of image shapes\n    """"""\n    max_shape = np.array(im_shapes).max(axis=0)\n    assert max_shape.size == 2\n    # Pad the image so they can be divisible by a stride\n    if cfg.FPN.FPN_ON:\n        stride = float(cfg.FPN.COARSEST_STRIDE)\n        max_shape[0] = int(np.ceil(max_shape[0] / stride) * stride)\n        max_shape[1] = int(np.ceil(max_shape[1] / stride) * stride)\n    return max_shape\n\n\ndef prep_im_for_blob(im, pixel_means, target_sizes, max_size):\n    """"""Prepare an image for use as a network input blob. Specially:\n      - Subtract per-channel pixel mean\n      - Convert to float32\n      - Rescale to each of the specified target size (capped at max_size)\n    Returns a list of transformed images, one for each target size. Also returns\n    the scale factors that were used to compute each returned image.\n    """"""\n    im = im.astype(np.float32, copy=False)\n    im -= pixel_means\n    im_shape = im.shape\n    im_size_min = np.min(im_shape[0:2])\n    im_size_max = np.max(im_shape[0:2])\n\n    ims = []\n    im_scales = []\n    for target_size in target_sizes:\n        im_scale = get_target_scale(im_size_min, im_size_max, target_size, max_size)\n        im_resized = cv2.resize(im, None, None, fx=im_scale, fy=im_scale,\n                                interpolation=cv2.INTER_LINEAR)\n        ims.append(im_resized)\n        im_scales.append(im_scale)\n    return ims, im_scales\n\n\ndef get_im_blob_sizes(im_shape, target_sizes, max_size):\n    """"""Calculate im blob size for multiple target_sizes given original im shape\n    """"""\n    im_size_min = np.min(im_shape)\n    im_size_max = np.max(im_shape)\n    im_sizes = []\n    for target_size in target_sizes:\n        im_scale = get_target_scale(im_size_min, im_size_max, target_size, max_size)\n        im_sizes.append(np.round(im_shape * im_scale))\n    return np.array(im_sizes)\n\n\ndef get_target_scale(im_size_min, im_size_max, target_size, max_size):\n    """"""Calculate target resize scale\n    """"""\n    im_scale = float(target_size) / float(im_size_min)\n    # Prevent the biggest axis from being more than max_size\n    if np.round(im_scale * im_size_max) > max_size:\n        im_scale = float(max_size) / float(im_size_max)\n    return im_scale\n\n\ndef zeros(shape, int32=False):\n    """"""Return a blob of all zeros of the given shape with the correct float or\n    int data type.\n    """"""\n    return np.zeros(shape, dtype=np.int32 if int32 else np.float32)\n\n\ndef ones(shape, int32=False):\n    """"""Return a blob of all ones of the given shape with the correct float or\n    int data type.\n    """"""\n    return np.ones(shape, dtype=np.int32 if int32 else np.float32)\n\n\ndef serialize(obj):\n    """"""Serialize a Python object using pickle and encode it as an array of\n    float32 values so that it can be feed into the workspace. See deserialize().\n    """"""\n    return np.fromstring(pickle.dumps(obj), dtype=np.uint8).astype(np.float32)\n\n\ndef deserialize(arr):\n    """"""Unserialize a Python object from an array of float32 values fetched from\n    a workspace. See serialize().\n    """"""\n    return pickle.loads(arr.astype(np.uint8).tobytes())\n'"
lib/utils/boxes.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n#\n# Based on:\n# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\n""""""Box manipulation functions. The internal Detectron box format is\n[x1, y1, x2, y2] where (x1, y1) specify the top-left box corner and (x2, y2)\nspecify the bottom-right box corner. Boxes from external sources, e.g.,\ndatasets, may be in other formats (such as [x, y, w, h]) and require conversion.\n\nThis module uses a convention that may seem strange at first: the width of a box\nis computed as x2 - x1 + 1 (likewise for height). The ""+ 1"" dates back to old\nobject detection days when the coordinates were integer pixel indices, rather\nthan floating point coordinates in a subpixel coordinate frame. A box with x2 =\nx1 and y2 = y1 was taken to include a single pixel, having a width of 1, and\nhence requiring the ""+ 1"". Now, most datasets will likely provide boxes with\nfloating point coordinates and the width should be more reasonably computed as\nx2 - x1.\n\nIn practice, as long as a model is trained and tested with a consistent\nconvention either decision seems to be ok (at least in our experience on COCO).\nSince we have a long history of training models with the ""+ 1"" convention, we\nare reluctant to change it even if our modern tastes prefer not to use it.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport warnings\nimport numpy as np\n\nfrom core.config import cfg\nimport utils.cython_bbox as cython_bbox\nimport utils.cython_nms as cython_nms\n\nbbox_overlaps = cython_bbox.bbox_overlaps\n\n\ndef boxes_area(boxes):\n    """"""Compute the area of an array of boxes.""""""\n    w = (boxes[:, 2] - boxes[:, 0] + 1)\n    h = (boxes[:, 3] - boxes[:, 1] + 1)\n    areas = w * h\n\n    neg_area_idx = np.where(areas < 0)[0]\n    if neg_area_idx.size:\n        warnings.warn(""Negative areas founds: %d"" % neg_area_idx.size, RuntimeWarning)\n    #TODO proper warm up and learning rate may reduce the prob of assertion fail\n    # assert np.all(areas >= 0), \'Negative areas founds\'\n    return areas, neg_area_idx\n\n\ndef unique_boxes(boxes, scale=1.0):\n    """"""Return indices of unique boxes.""""""\n    v = np.array([1, 1e3, 1e6, 1e9])\n    hashes = np.round(boxes * scale).dot(v)\n    _, index = np.unique(hashes, return_index=True)\n    return np.sort(index)\n\n\ndef xywh_to_xyxy(xywh):\n    """"""Convert [x1 y1 w h] box format to [x1 y1 x2 y2] format.""""""\n    if isinstance(xywh, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert len(xywh) == 4\n        x1, y1 = xywh[0], xywh[1]\n        x2 = x1 + np.maximum(0., xywh[2] - 1.)\n        y2 = y1 + np.maximum(0., xywh[3] - 1.)\n        return (x1, y1, x2, y2)\n    elif isinstance(xywh, np.ndarray):\n        # Multiple boxes given as a 2D ndarray\n        return np.hstack(\n            (xywh[:, 0:2], xywh[:, 0:2] + np.maximum(0, xywh[:, 2:4] - 1))\n        )\n    else:\n        raise TypeError(\'Argument xywh must be a list, tuple, or numpy array.\')\n\n\ndef xyxy_to_xywh(xyxy):\n    """"""Convert [x1 y1 x2 y2] box format to [x1 y1 w h] format.""""""\n    if isinstance(xyxy, (list, tuple)):\n        # Single box given as a list of coordinates\n        assert len(xyxy) == 4\n        x1, y1 = xyxy[0], xyxy[1]\n        w = xyxy[2] - x1 + 1\n        h = xyxy[3] - y1 + 1\n        return (x1, y1, w, h)\n    elif isinstance(xyxy, np.ndarray):\n        # Multiple boxes given as a 2D ndarray\n        return np.hstack((xyxy[:, 0:2], xyxy[:, 2:4] - xyxy[:, 0:2] + 1))\n    else:\n        raise TypeError(\'Argument xyxy must be a list, tuple, or numpy array.\')\n\n\ndef filter_small_boxes(boxes, min_size):\n    """"""Keep boxes with width and height both greater than min_size.""""""\n    w = boxes[:, 2] - boxes[:, 0] + 1\n    h = boxes[:, 3] - boxes[:, 1] + 1\n    keep = np.where((w > min_size) & (h > min_size))[0]\n    return keep\n\n\ndef clip_boxes_to_image(boxes, height, width):\n    """"""Clip an array of boxes to an image with the given height and width.""""""\n    boxes[:, [0, 2]] = np.minimum(width - 1., np.maximum(0., boxes[:, [0, 2]]))\n    boxes[:, [1, 3]] = np.minimum(height - 1., np.maximum(0., boxes[:, [1, 3]]))\n    return boxes\n\n\ndef clip_xyxy_to_image(x1, y1, x2, y2, height, width):\n    """"""Clip coordinates to an image with the given height and width.""""""\n    x1 = np.minimum(width - 1., np.maximum(0., x1))\n    y1 = np.minimum(height - 1., np.maximum(0., y1))\n    x2 = np.minimum(width - 1., np.maximum(0., x2))\n    y2 = np.minimum(height - 1., np.maximum(0., y2))\n    return x1, y1, x2, y2\n\n\ndef clip_tiled_boxes(boxes, im_shape):\n    """"""Clip boxes to image boundaries. im_shape is [height, width] and boxes\n    has shape (N, 4 * num_tiled_boxes).""""""\n    assert boxes.shape[1] % 4 == 0, \\\n        \'boxes.shape[1] is {:d}, but must be divisible by 4.\'.format(\n        boxes.shape[1]\n    )\n    # x1 >= 0\n    boxes[:, 0::4] = np.maximum(np.minimum(boxes[:, 0::4], im_shape[1] - 1), 0)\n    # y1 >= 0\n    boxes[:, 1::4] = np.maximum(np.minimum(boxes[:, 1::4], im_shape[0] - 1), 0)\n    # x2 < im_shape[1]\n    boxes[:, 2::4] = np.maximum(np.minimum(boxes[:, 2::4], im_shape[1] - 1), 0)\n    # y2 < im_shape[0]\n    boxes[:, 3::4] = np.maximum(np.minimum(boxes[:, 3::4], im_shape[0] - 1), 0)\n    return boxes\n\n\ndef bbox_transform(boxes, deltas, weights=(1.0, 1.0, 1.0, 1.0)):\n    """"""Forward transform that maps proposal boxes to predicted ground-truth\n    boxes using bounding-box regression deltas. See bbox_transform_inv for a\n    description of the weights argument.\n    """"""\n    if boxes.shape[0] == 0:\n        return np.zeros((0, deltas.shape[1]), dtype=deltas.dtype)\n\n    boxes = boxes.astype(deltas.dtype, copy=False)\n\n    widths = boxes[:, 2] - boxes[:, 0] + 1.0\n    heights = boxes[:, 3] - boxes[:, 1] + 1.0\n    ctr_x = boxes[:, 0] + 0.5 * widths\n    ctr_y = boxes[:, 1] + 0.5 * heights\n\n    wx, wy, ww, wh = weights\n    dx = deltas[:, 0::4] / wx\n    dy = deltas[:, 1::4] / wy\n    dw = deltas[:, 2::4] / ww\n    dh = deltas[:, 3::4] / wh\n\n    # Prevent sending too large values into np.exp()\n    dw = np.minimum(dw, cfg.BBOX_XFORM_CLIP)\n    dh = np.minimum(dh, cfg.BBOX_XFORM_CLIP)\n\n    pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]\n    pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]\n    pred_w = np.exp(dw) * widths[:, np.newaxis]\n    pred_h = np.exp(dh) * heights[:, np.newaxis]\n\n    pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)\n    # x1\n    pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w\n    # y1\n    pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h\n    # x2 (note: ""- 1"" is correct; don\'t be fooled by the asymmetry)\n    pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w - 1\n    # y2 (note: ""- 1"" is correct; don\'t be fooled by the asymmetry)\n    pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h - 1\n\n    return pred_boxes\n\n\ndef bbox_transform_inv(boxes, gt_boxes, weights=(1.0, 1.0, 1.0, 1.0)):\n    """"""Inverse transform that computes target bounding-box regression deltas\n    given proposal boxes and ground-truth boxes. The weights argument should be\n    a 4-tuple of multiplicative weights that are applied to the regression\n    target.\n\n    In older versions of this code (and in py-faster-rcnn), the weights were set\n    such that the regression deltas would have unit standard deviation on the\n    training dataset. Presently, rather than computing these statistics exactly,\n    we use a fixed set of weights (10., 10., 5., 5.) by default. These are\n    approximately the weights one would get from COCO using the previous unit\n    stdev heuristic.\n    """"""\n    ex_widths = boxes[:, 2] - boxes[:, 0] + 1.0\n    ex_heights = boxes[:, 3] - boxes[:, 1] + 1.0\n    ex_ctr_x = boxes[:, 0] + 0.5 * ex_widths\n    ex_ctr_y = boxes[:, 1] + 0.5 * ex_heights\n\n    gt_widths = gt_boxes[:, 2] - gt_boxes[:, 0] + 1.0\n    gt_heights = gt_boxes[:, 3] - gt_boxes[:, 1] + 1.0\n    gt_ctr_x = gt_boxes[:, 0] + 0.5 * gt_widths\n    gt_ctr_y = gt_boxes[:, 1] + 0.5 * gt_heights\n\n    wx, wy, ww, wh = weights\n    targets_dx = wx * (gt_ctr_x - ex_ctr_x) / ex_widths\n    targets_dy = wy * (gt_ctr_y - ex_ctr_y) / ex_heights\n    targets_dw = ww * np.log(gt_widths / ex_widths)\n    targets_dh = wh * np.log(gt_heights / ex_heights)\n\n    targets = np.vstack((targets_dx, targets_dy, targets_dw,\n                         targets_dh)).transpose()\n    return targets\n\n\ndef expand_boxes(boxes, scale):\n    """"""Expand an array of boxes by a given scale.""""""\n    w_half = (boxes[:, 2] - boxes[:, 0]) * .5\n    h_half = (boxes[:, 3] - boxes[:, 1]) * .5\n    x_c = (boxes[:, 2] + boxes[:, 0]) * .5\n    y_c = (boxes[:, 3] + boxes[:, 1]) * .5\n\n    w_half *= scale\n    h_half *= scale\n\n    boxes_exp = np.zeros(boxes.shape)\n    boxes_exp[:, 0] = x_c - w_half\n    boxes_exp[:, 2] = x_c + w_half\n    boxes_exp[:, 1] = y_c - h_half\n    boxes_exp[:, 3] = y_c + h_half\n\n    return boxes_exp\n\n\ndef flip_boxes(boxes, im_width):\n    """"""Flip boxes horizontally.""""""\n    boxes_flipped = boxes.copy()\n    boxes_flipped[:, 0::4] = im_width - boxes[:, 2::4] - 1\n    boxes_flipped[:, 2::4] = im_width - boxes[:, 0::4] - 1\n    return boxes_flipped\n\n\ndef aspect_ratio(boxes, aspect_ratio):\n    """"""Perform width-relative aspect ratio transformation.""""""\n    boxes_ar = boxes.copy()\n    boxes_ar[:, 0::4] = aspect_ratio * boxes[:, 0::4]\n    boxes_ar[:, 2::4] = aspect_ratio * boxes[:, 2::4]\n    return boxes_ar\n\n\ndef box_voting(top_dets, all_dets, thresh, scoring_method=\'ID\', beta=1.0):\n    """"""Apply bounding-box voting to refine `top_dets` by voting with `all_dets`.\n    See: https://arxiv.org/abs/1505.01749. Optional score averaging (not in the\n    referenced  paper) can be applied by setting `scoring_method` appropriately.\n    """"""\n    # top_dets is [N, 5] each row is [x1 y1 x2 y2, sore]\n    # all_dets is [N, 5] each row is [x1 y1 x2 y2, sore]\n    top_dets_out = top_dets.copy()\n    top_boxes = top_dets[:, :4]\n    all_boxes = all_dets[:, :4]\n    all_scores = all_dets[:, 4]\n    top_to_all_overlaps = bbox_overlaps(top_boxes, all_boxes)\n    for k in range(top_dets_out.shape[0]):\n        inds_to_vote = np.where(top_to_all_overlaps[k] >= thresh)[0]\n        boxes_to_vote = all_boxes[inds_to_vote, :]\n        ws = all_scores[inds_to_vote]\n        top_dets_out[k, :4] = np.average(boxes_to_vote, axis=0, weights=ws)\n        if scoring_method == \'ID\':\n            # Identity, nothing to do\n            pass\n        elif scoring_method == \'TEMP_AVG\':\n            # Average probabilities (considered as P(detected class) vs.\n            # P(not the detected class)) after smoothing with a temperature\n            # hyperparameter.\n            P = np.vstack((ws, 1.0 - ws))\n            P_max = np.max(P, axis=0)\n            X = np.log(P / P_max)\n            X_exp = np.exp(X / beta)\n            P_temp = X_exp / np.sum(X_exp, axis=0)\n            P_avg = P_temp[0].mean()\n            top_dets_out[k, 4] = P_avg\n        elif scoring_method == \'AVG\':\n            # Combine new probs from overlapping boxes\n            top_dets_out[k, 4] = ws.mean()\n        elif scoring_method == \'IOU_AVG\':\n            P = ws\n            ws = top_to_all_overlaps[k, inds_to_vote]\n            P_avg = np.average(P, weights=ws)\n            top_dets_out[k, 4] = P_avg\n        elif scoring_method == \'GENERALIZED_AVG\':\n            P_avg = np.mean(ws**beta)**(1.0 / beta)\n            top_dets_out[k, 4] = P_avg\n        elif scoring_method == \'QUASI_SUM\':\n            top_dets_out[k, 4] = ws.sum() / float(len(ws))**beta\n        else:\n            raise NotImplementedError(\n                \'Unknown scoring method {}\'.format(scoring_method)\n            )\n\n    return top_dets_out\n\n\ndef nms(dets, thresh):\n    """"""Apply classic DPM-style greedy NMS.""""""\n    if dets.shape[0] == 0:\n        return []\n    return cython_nms.nms(dets, thresh)\n\n\ndef soft_nms(\n    dets, sigma=0.5, overlap_thresh=0.3, score_thresh=0.001, method=\'linear\'\n):\n    """"""Apply the soft NMS algorithm from https://arxiv.org/abs/1704.04503.""""""\n    if dets.shape[0] == 0:\n        return dets, []\n\n    methods = {\'hard\': 0, \'linear\': 1, \'gaussian\': 2}\n    assert method in methods, \'Unknown soft_nms method: {}\'.format(method)\n\n    dets, keep = cython_nms.soft_nms(\n        np.ascontiguousarray(dets, dtype=np.float32),\n        np.float32(sigma),\n        np.float32(overlap_thresh),\n        np.float32(score_thresh),\n        np.uint8(methods[method])\n    )\n    return dets, keep\n'"
lib/utils/collections.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""A simple attribute dictionary used for representing configuration options.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\n\nclass AttrDict(dict):\n\n    IMMUTABLE = \'__immutable__\'\n\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__[AttrDict.IMMUTABLE] = False\n\n    def __getattr__(self, name):\n        if name in self.__dict__:\n            return self.__dict__[name]\n        elif name in self:\n            return self[name]\n        else:\n            raise AttributeError(name)\n\n    def __setattr__(self, name, value):\n        if not self.__dict__[AttrDict.IMMUTABLE]:\n            if name in self.__dict__:\n                self.__dict__[name] = value\n            else:\n                self[name] = value\n        else:\n            raise AttributeError(\n                \'Attempted to set ""{}"" to ""{}"", but AttrDict is immutable\'.\n                format(name, value)\n            )\n\n    def immutable(self, is_immutable):\n        """"""Set immutability to is_immutable and recursively apply the setting\n        to all nested AttrDicts.\n        """"""\n        self.__dict__[AttrDict.IMMUTABLE] = is_immutable\n        # Recursively set immutable state\n        for v in self.__dict__.values():\n            if isinstance(v, AttrDict):\n                v.immutable(is_immutable)\n        for v in self.values():\n            if isinstance(v, AttrDict):\n                v.immutable(is_immutable)\n\n    def is_immutable(self):\n        return self.__dict__[AttrDict.IMMUTABLE]\n'"
lib/utils/colormap.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""An awesome colormap for really neat visualizations.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\n\n\ndef colormap(rgb=False):\n    color_list = np.array(\n        [\n            0.000, 0.447, 0.741,\n            0.850, 0.325, 0.098,\n            0.929, 0.694, 0.125,\n            0.494, 0.184, 0.556,\n            0.466, 0.674, 0.188,\n            0.301, 0.745, 0.933,\n            0.635, 0.078, 0.184,\n            0.300, 0.300, 0.300,\n            0.600, 0.600, 0.600,\n            1.000, 0.000, 0.000,\n            1.000, 0.500, 0.000,\n            0.749, 0.749, 0.000,\n            0.000, 1.000, 0.000,\n            0.000, 0.000, 1.000,\n            0.667, 0.000, 1.000,\n            0.333, 0.333, 0.000,\n            0.333, 0.667, 0.000,\n            0.333, 1.000, 0.000,\n            0.667, 0.333, 0.000,\n            0.667, 0.667, 0.000,\n            0.667, 1.000, 0.000,\n            1.000, 0.333, 0.000,\n            1.000, 0.667, 0.000,\n            1.000, 1.000, 0.000,\n            0.000, 0.333, 0.500,\n            0.000, 0.667, 0.500,\n            0.000, 1.000, 0.500,\n            0.333, 0.000, 0.500,\n            0.333, 0.333, 0.500,\n            0.333, 0.667, 0.500,\n            0.333, 1.000, 0.500,\n            0.667, 0.000, 0.500,\n            0.667, 0.333, 0.500,\n            0.667, 0.667, 0.500,\n            0.667, 1.000, 0.500,\n            1.000, 0.000, 0.500,\n            1.000, 0.333, 0.500,\n            1.000, 0.667, 0.500,\n            1.000, 1.000, 0.500,\n            0.000, 0.333, 1.000,\n            0.000, 0.667, 1.000,\n            0.000, 1.000, 1.000,\n            0.333, 0.000, 1.000,\n            0.333, 0.333, 1.000,\n            0.333, 0.667, 1.000,\n            0.333, 1.000, 1.000,\n            0.667, 0.000, 1.000,\n            0.667, 0.333, 1.000,\n            0.667, 0.667, 1.000,\n            0.667, 1.000, 1.000,\n            1.000, 0.000, 1.000,\n            1.000, 0.333, 1.000,\n            1.000, 0.667, 1.000,\n            0.167, 0.000, 0.000,\n            0.333, 0.000, 0.000,\n            0.500, 0.000, 0.000,\n            0.667, 0.000, 0.000,\n            0.833, 0.000, 0.000,\n            1.000, 0.000, 0.000,\n            0.000, 0.167, 0.000,\n            0.000, 0.333, 0.000,\n            0.000, 0.500, 0.000,\n            0.000, 0.667, 0.000,\n            0.000, 0.833, 0.000,\n            0.000, 1.000, 0.000,\n            0.000, 0.000, 0.167,\n            0.000, 0.000, 0.333,\n            0.000, 0.000, 0.500,\n            0.000, 0.000, 0.667,\n            0.000, 0.000, 0.833,\n            0.000, 0.000, 1.000,\n            0.000, 0.000, 0.000,\n            0.143, 0.143, 0.143,\n            0.286, 0.286, 0.286,\n            0.429, 0.429, 0.429,\n            0.571, 0.571, 0.571,\n            0.714, 0.714, 0.714,\n            0.857, 0.857, 0.857,\n            1.000, 1.000, 1.000\n        ]\n    ).astype(np.float32)\n    color_list = color_list.reshape((-1, 3)) * 255\n    if not rgb:\n        color_list = color_list[:, ::-1]\n    return color_list\n'"
lib/utils/detectron_weight_helper.py,1,"b'""""""Helper functions for loading pretrained weights from Detectron pickle files\n""""""\n\nimport pickle\nimport re\nimport torch\n\n\ndef load_detectron_weight(net, detectron_weight_file):\n    name_mapping, orphan_in_detectron = net.detectron_weight_mapping\n\n    with open(detectron_weight_file, \'rb\') as fp:\n        src_blobs = pickle.load(fp, encoding=\'latin1\')\n    if \'blobs\' in src_blobs:\n        src_blobs = src_blobs[\'blobs\']\n\n    params = net.state_dict()\n    for p_name, p_tensor in params.items():\n        d_name = name_mapping[p_name]\n        if isinstance(d_name, str):  # maybe str, None or True\n            p_tensor.copy_(torch.Tensor(src_blobs[d_name]))\n\n\ndef resnet_weights_name_pattern():\n    pattern = re.compile(r""conv1_w|conv1_gn_[sb]|res_conv1_.+|res\\d+_\\d+_.+"")\n    return pattern\n\n\nif __name__ == \'__main__\':\n    """"""Testing""""""\n    from pprint import pprint\n    import sys\n    sys.path.insert(0, \'..\')\n    from modeling.model_builder import Generalized_RCNN\n    from core.config import cfg, cfg_from_file\n\n    cfg.MODEL.NUM_CLASSES = 81\n    cfg_from_file(\'../../cfgs/res50_mask.yml\')\n    net = Generalized_RCNN()\n\n    # pprint(list(net.state_dict().keys()), width=1)\n\n    mapping, orphans = net.detectron_weight_mapping\n    state_dict = net.state_dict()\n\n    for k in mapping.keys():\n        assert k in state_dict, \'%s\' % k\n\n    rest = set(state_dict.keys()) - set(mapping.keys())\n    assert len(rest) == 0\n'"
lib/utils/env.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Environment helper functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport os\nimport sys\n\n# Default value of the CMake install prefix\n_CMAKE_INSTALL_PREFIX = \'/usr/local\'\n\n\ndef get_runtime_dir():\n    """"""Retrieve the path to the runtime directory.""""""\n    return os.getcwd()\n\n\ndef get_py_bin_ext():\n    """"""Retrieve python binary extension.""""""\n    return \'.py\'\n\n\ndef set_up_matplotlib():\n    """"""Set matplotlib up.""""""\n    import matplotlib\n    # Use a non-interactive backend\n    matplotlib.use(\'Agg\')\n\n\ndef exit_on_error():\n    """"""Exit from a detectron tool when there\'s an error.""""""\n    sys.exit(1)\n'"
lib/utils/fpn.py,0,"b'import numpy as np\n\nimport utils.boxes as box_utils\nfrom core.config import cfg\n\n\n# ---------------------------------------------------------------------------- #\n# Helper functions for working with multilevel FPN RoIs\n# ---------------------------------------------------------------------------- #\n\ndef map_rois_to_fpn_levels(rois, k_min, k_max):\n    """"""Determine which FPN level each RoI in a set of RoIs should map to based\n    on the heuristic in the FPN paper.\n    """"""\n    # Compute level ids\n    areas, neg_idx = box_utils.boxes_area(rois)\n    areas[neg_idx] = 0  # np.sqrt will remove the entries with negative value\n    s = np.sqrt(areas)\n    s0 = cfg.FPN.ROI_CANONICAL_SCALE  # default: 224\n    lvl0 = cfg.FPN.ROI_CANONICAL_LEVEL  # default: 4\n\n    # Eqn.(1) in FPN paper\n    target_lvls = np.floor(lvl0 + np.log2(s / s0 + 1e-6))\n    target_lvls = np.clip(target_lvls, k_min, k_max)\n\n    # Mark to discard negative area roi. See utils.fpn.add_multilevel_roi_blobs\n    # target_lvls[neg_idx] = -1\n    return target_lvls\n\n\ndef add_multilevel_roi_blobs(\n        blobs, blob_prefix, rois, target_lvls, lvl_min, lvl_max\n    ):\n    """"""Add RoI blobs for multiple FPN levels to the blobs dict.\n\n    blobs: a dict mapping from blob name to numpy ndarray\n    blob_prefix: name prefix to use for the FPN blobs\n    rois: the source rois as a 2D numpy array of shape (N, 5) where each row is\n      an roi and the columns encode (batch_idx, x1, y1, x2, y2)\n    target_lvls: numpy array of shape (N, ) indicating which FPN level each roi\n      in rois should be assigned to. -1 means correspoind roi should be discarded.\n    lvl_min: the finest (highest resolution) FPN level (e.g., 2)\n    lvl_max: the coarest (lowest resolution) FPN level (e.g., 6)\n    """"""\n    rois_idx_order = np.empty((0, ))\n    rois_stacked = np.zeros((0, 5), dtype=np.float32)  # for assert\n    # target_lvls = remove_negative_area_roi_blobs(blobs, blob_prefix, rois, target_lvls)\n    for lvl in range(lvl_min, lvl_max + 1):\n        idx_lvl = np.where(target_lvls == lvl)[0]\n        blobs[blob_prefix + \'_fpn\' + str(lvl)] = rois[idx_lvl, :]\n        rois_idx_order = np.concatenate((rois_idx_order, idx_lvl))\n        rois_stacked = np.vstack(\n            [rois_stacked, blobs[blob_prefix + \'_fpn\' + str(lvl)]]\n        )\n    rois_idx_restore = np.argsort(rois_idx_order).astype(np.int32, copy=False)\n    blobs[blob_prefix + \'_idx_restore_int32\'] = rois_idx_restore\n    # Sanity check that restore order is correct\n    assert (rois_stacked[rois_idx_restore] == rois).all()\n\n\ndef remove_negative_area_roi_blobs(blobs, blob_prefix, rois, target_lvls):\n    """""" Delete roi entries that have negative area (Uncompleted) """"""\n    idx_neg = np.where(target_lvls == -1)[0]\n    rois = np.delete(rois, idx_neg, axis=0)\n    blobs[blob_prefix] = rois\n    target_lvls = np.delete(target_lvls, idx_neg, axis=0)\n    #TODO: other blobs in faster_rcnn.get_fast_rcnn_blob_names should also be modified\n    return target_lvls\n'"
lib/utils/image.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Image helper functions.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport cv2\nimport numpy as np\n\n\ndef aspect_ratio_rel(im, aspect_ratio):\n    """"""Performs width-relative aspect ratio transformation.""""""\n    im_h, im_w = im.shape[:2]\n    im_ar_w = int(round(aspect_ratio * im_w))\n    im_ar = cv2.resize(im, dsize=(im_ar_w, im_h))\n    return im_ar\n\n\ndef aspect_ratio_abs(im, aspect_ratio):\n    """"""Performs absolute aspect ratio transformation.""""""\n    im_h, im_w = im.shape[:2]\n    im_area = im_h * im_w\n\n    im_ar_w = np.sqrt(im_area * aspect_ratio)\n    im_ar_h = np.sqrt(im_area / aspect_ratio)\n    assert np.isclose(im_ar_w / im_ar_h, aspect_ratio)\n\n    im_ar = cv2.resize(im, dsize=(int(im_ar_w), int(im_ar_h)))\n    return im_ar\n'"
lib/utils/io.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""IO utilities.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom six.moves import cPickle as pickle\nimport hashlib\nimport logging\nimport os\nimport re\nimport sys\ntry:\n    from urllib.request import urlopen\nexcept ImportError:  #python2\n    from urllib2 import urlopen\n\nlogger = logging.getLogger(__name__)\n\n_DETECTRON_S3_BASE_URL = \'https://s3-us-west-2.amazonaws.com/detectron\'\n\n\ndef save_object(obj, file_name):\n    """"""Save a Python object by pickling it.""""""\n    file_name = os.path.abspath(file_name)\n    with open(file_name, \'wb\') as f:\n        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n\n\ndef cache_url(url_or_file, cache_dir):\n    """"""Download the file specified by the URL to the cache_dir and return the\n    path to the cached file. If the argument is not a URL, simply return it as\n    is.\n    """"""\n    is_url = re.match(r\'^(?:http)s?://\', url_or_file, re.IGNORECASE) is not None\n\n    if not is_url:\n        return url_or_file\n\n    url = url_or_file\n    assert url.startswith(_DETECTRON_S3_BASE_URL), \\\n        (\'Detectron only automatically caches URLs in the Detectron S3 \'\n         \'bucket: {}\').format(_DETECTRON_S3_BASE_URL)\n\n    cache_file_path = url.replace(_DETECTRON_S3_BASE_URL, cache_dir)\n    if os.path.exists(cache_file_path):\n        assert_cache_file_is_ok(url, cache_file_path)\n        return cache_file_path\n\n    cache_file_dir = os.path.dirname(cache_file_path)\n    if not os.path.exists(cache_file_dir):\n        os.makedirs(cache_file_dir)\n\n    logger.info(\'Downloading remote file {} to {}\'.format(url, cache_file_path))\n    download_url(url, cache_file_path)\n    assert_cache_file_is_ok(url, cache_file_path)\n    return cache_file_path\n\n\ndef assert_cache_file_is_ok(url, file_path):\n    """"""Check that cache file has the correct hash.""""""\n    # File is already in the cache, verify that the md5sum matches and\n    # return local path\n    cache_file_md5sum = _get_file_md5sum(file_path)\n    ref_md5sum = _get_reference_md5sum(url)\n    assert cache_file_md5sum == ref_md5sum, \\\n        (\'Target URL {} appears to be downloaded to the local cache file \'\n         \'{}, but the md5 hash of the local file does not match the \'\n         \'reference (actual: {} vs. expected: {}). You may wish to delete \'\n         \'the cached file and try again to trigger automatic \'\n         \'download.\').format(url, file_path, cache_file_md5sum, ref_md5sum)\n\n\ndef _progress_bar(count, total):\n    """"""Report download progress.\n    Credit:\n    https://stackoverflow.com/questions/3173320/text-progress-bar-in-the-console/27871113\n    """"""\n    bar_len = 60\n    filled_len = int(round(bar_len * count / float(total)))\n\n    percents = round(100.0 * count / float(total), 1)\n    bar = \'=\' * filled_len + \'-\' * (bar_len - filled_len)\n\n    sys.stdout.write(\n        \'  [{}] {}% of {:.1f}MB file  \\r\'.\n        format(bar, percents, total / 1024 / 1024)\n    )\n    sys.stdout.flush()\n    if count >= total:\n        sys.stdout.write(\'\\n\')\n\n\ndef download_url(\n    url, dst_file_path, chunk_size=8192, progress_hook=_progress_bar\n):\n    """"""Download url and write it to dst_file_path.\n    Credit:\n    https://stackoverflow.com/questions/2028517/python-urllib2-progress-hook\n    """"""\n    response = urlopen(url)\n    total_size = response.info().getheader(\'Content-Length\').strip()\n    total_size = int(total_size)\n    bytes_so_far = 0\n\n    with open(dst_file_path, \'wb\') as f:\n        while 1:\n            chunk = response.read(chunk_size)\n            bytes_so_far += len(chunk)\n            if not chunk:\n                break\n            if progress_hook:\n                progress_hook(bytes_so_far, total_size)\n            f.write(chunk)\n\n    return bytes_so_far\n\n\ndef _get_file_md5sum(file_name):\n    """"""Compute the md5 hash of a file.""""""\n    hash_obj = hashlib.md5()\n    with open(file_name, \'r\') as f:\n        hash_obj.update(f.read())\n    return hash_obj.hexdigest()\n\n\ndef _get_reference_md5sum(url):\n    """"""By convention the md5 hash for url is stored in url + \'.md5sum\'.""""""\n    url_md5sum = url + \'.md5sum\'\n    md5sum = urlopen(url_md5sum).read().strip()\n    return md5sum\n'"
lib/utils/keypoints.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Keypoint utilities (somewhat specific to COCO keypoints).""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport cv2\nimport numpy as np\n\nfrom core.config import cfg\nimport utils.blob as blob_utils\n\n\ndef get_keypoints():\n    """"""Get the COCO keypoints and their left/right flip coorespondence map.""""""\n    # Keypoints are not available in the COCO json for the test split, so we\n    # provide them here.\n    keypoints = [\n        \'nose\',\n        \'left_eye\',\n        \'right_eye\',\n        \'left_ear\',\n        \'right_ear\',\n        \'left_shoulder\',\n        \'right_shoulder\',\n        \'left_elbow\',\n        \'right_elbow\',\n        \'left_wrist\',\n        \'right_wrist\',\n        \'left_hip\',\n        \'right_hip\',\n        \'left_knee\',\n        \'right_knee\',\n        \'left_ankle\',\n        \'right_ankle\'\n    ]\n    keypoint_flip_map = {\n        \'left_eye\': \'right_eye\',\n        \'left_ear\': \'right_ear\',\n        \'left_shoulder\': \'right_shoulder\',\n        \'left_elbow\': \'right_elbow\',\n        \'left_wrist\': \'right_wrist\',\n        \'left_hip\': \'right_hip\',\n        \'left_knee\': \'right_knee\',\n        \'left_ankle\': \'right_ankle\'\n    }\n    return keypoints, keypoint_flip_map\n\n\ndef get_person_class_index():\n    """"""Index of the person class in COCO.""""""\n    return 1\n\n\ndef flip_keypoints(keypoints, keypoint_flip_map, keypoint_coords, width):\n    """"""Left/right flip keypoint_coords. keypoints and keypoint_flip_map are\n    accessible from get_keypoints().\n    """"""\n    flipped_kps = keypoint_coords.copy()\n    for lkp, rkp in keypoint_flip_map.items():\n        lid = keypoints.index(lkp)\n        rid = keypoints.index(rkp)\n        flipped_kps[:, :, lid] = keypoint_coords[:, :, rid]\n        flipped_kps[:, :, rid] = keypoint_coords[:, :, lid]\n\n    # Flip x coordinates\n    flipped_kps[:, 0, :] = width - flipped_kps[:, 0, :] - 1\n    # Maintain COCO convention that if visibility == 0, then x, y = 0\n    inds = np.where(flipped_kps[:, 2, :] == 0)\n    flipped_kps[inds[0], 0, inds[1]] = 0\n    return flipped_kps\n\n\ndef flip_heatmaps(heatmaps):\n    """"""Flip heatmaps horizontally.""""""\n    keypoints, flip_map = get_keypoints()\n    heatmaps_flipped = heatmaps.copy()\n    for lkp, rkp in flip_map.items():\n        lid = keypoints.index(lkp)\n        rid = keypoints.index(rkp)\n        heatmaps_flipped[:, rid, :, :] = heatmaps[:, lid, :, :]\n        heatmaps_flipped[:, lid, :, :] = heatmaps[:, rid, :, :]\n    heatmaps_flipped = heatmaps_flipped[:, :, :, ::-1]\n    return heatmaps_flipped\n\n\ndef heatmaps_to_keypoints(maps, rois):\n    """"""Extract predicted keypoint locations from heatmaps. Output has shape\n    (#rois, 4, #keypoints) with the 4 rows corresponding to (x, y, logit, prob)\n    for each keypoint.\n    """"""\n    # This function converts a discrete image coordinate in a HEATMAP_SIZE x\n    # HEATMAP_SIZE image to a continuous keypoint coordinate. We maintain\n    # consistency with keypoints_to_heatmap_labels by using the conversion from\n    # Heckbert 1990: c = d + 0.5, where d is a discrete coordinate and c is a\n    # continuous coordinate.\n    offset_x = rois[:, 0]\n    offset_y = rois[:, 1]\n\n    widths = rois[:, 2] - rois[:, 0]\n    heights = rois[:, 3] - rois[:, 1]\n    widths = np.maximum(widths, 1)\n    heights = np.maximum(heights, 1)\n    widths_ceil = np.ceil(widths)\n    heights_ceil = np.ceil(heights)\n\n    # NCHW to NHWC for use with OpenCV\n    maps = np.transpose(maps, [0, 2, 3, 1])\n    min_size = cfg.KRCNN.INFERENCE_MIN_SIZE\n    xy_preds = np.zeros(\n        (len(rois), 4, cfg.KRCNN.NUM_KEYPOINTS), dtype=np.float32)\n    for i in range(len(rois)):\n        if min_size > 0:\n            roi_map_width = int(np.maximum(widths_ceil[i], min_size))\n            roi_map_height = int(np.maximum(heights_ceil[i], min_size))\n        else:\n            roi_map_width = widths_ceil[i]\n            roi_map_height = heights_ceil[i]\n        width_correction = widths[i] / roi_map_width\n        height_correction = heights[i] / roi_map_height\n        roi_map = cv2.resize(\n            maps[i], (roi_map_width, roi_map_height),\n            interpolation=cv2.INTER_CUBIC)\n        # Bring back to CHW\n        roi_map = np.transpose(roi_map, [2, 0, 1])\n        roi_map_probs = scores_to_probs(roi_map.copy())\n        w = roi_map.shape[2]\n        for k in range(cfg.KRCNN.NUM_KEYPOINTS):\n            pos = roi_map[k, :, :].argmax()\n            x_int = pos % w\n            y_int = (pos - x_int) // w\n            assert (roi_map_probs[k, y_int, x_int] ==\n                    roi_map_probs[k, :, :].max())\n            x = (x_int + 0.5) * width_correction\n            y = (y_int + 0.5) * height_correction\n            xy_preds[i, 0, k] = x + offset_x[i]\n            xy_preds[i, 1, k] = y + offset_y[i]\n            xy_preds[i, 2, k] = roi_map[k, y_int, x_int]\n            xy_preds[i, 3, k] = roi_map_probs[k, y_int, x_int]\n\n    return xy_preds\n\n\ndef keypoints_to_heatmap_labels(keypoints, rois):\n    """"""Encode keypoint location in the target heatmap for use in\n    SoftmaxWithLoss.\n    """"""\n    # Maps keypoints from the half-open interval [x1, x2) on continuous image\n    # coordinates to the closed interval [0, HEATMAP_SIZE - 1] on discrete image\n    # coordinates. We use the continuous <-> discrete conversion from Heckbert\n    # 1990 (""What is the coordinate of a pixel?""): d = floor(c) and c = d + 0.5,\n    # where d is a discrete coordinate and c is a continuous coordinate.\n    assert keypoints.shape[2] == cfg.KRCNN.NUM_KEYPOINTS\n\n    shape = (len(rois), cfg.KRCNN.NUM_KEYPOINTS)\n    heatmaps = blob_utils.zeros(shape)\n    weights = blob_utils.zeros(shape)\n\n    offset_x = rois[:, 0]\n    offset_y = rois[:, 1]\n    scale_x = cfg.KRCNN.HEATMAP_SIZE / (rois[:, 2] - rois[:, 0])\n    scale_y = cfg.KRCNN.HEATMAP_SIZE / (rois[:, 3] - rois[:, 1])\n\n    for kp in range(keypoints.shape[2]):\n        vis = keypoints[:, 2, kp] > 0\n        x = keypoints[:, 0, kp].astype(np.float32)\n        y = keypoints[:, 1, kp].astype(np.float32)\n        # Since we use floor below, if a keypoint is exactly on the roi\'s right\n        # or bottom boundary, we shift it in by eps (conceptually) to keep it in\n        # the ground truth heatmap.\n        x_boundary_inds = np.where(x == rois[:, 2])[0]\n        y_boundary_inds = np.where(y == rois[:, 3])[0]\n        x = (x - offset_x) * scale_x\n        x = np.floor(x)\n        if len(x_boundary_inds) > 0:\n            x[x_boundary_inds] = cfg.KRCNN.HEATMAP_SIZE - 1\n\n        y = (y - offset_y) * scale_y\n        y = np.floor(y)\n        if len(y_boundary_inds) > 0:\n            y[y_boundary_inds] = cfg.KRCNN.HEATMAP_SIZE - 1\n\n        valid_loc = np.logical_and(\n            np.logical_and(x >= 0, y >= 0),\n            np.logical_and(\n                x < cfg.KRCNN.HEATMAP_SIZE, y < cfg.KRCNN.HEATMAP_SIZE))\n\n        valid = np.logical_and(valid_loc, vis)\n        valid = valid.astype(np.int32)\n\n        lin_ind = y * cfg.KRCNN.HEATMAP_SIZE + x\n        heatmaps[:, kp] = lin_ind * valid\n        weights[:, kp] = valid\n\n    return heatmaps, weights\n\n\ndef scores_to_probs(scores):\n    """"""Transforms CxHxW of scores to probabilities spatially.""""""\n    channels = scores.shape[0]\n    for c in range(channels):\n        temp = scores[c, :, :]\n        max_score = temp.max()\n        temp = np.exp(temp - max_score) / np.sum(np.exp(temp - max_score))\n        scores[c, :, :] = temp\n    return scores\n\n\ndef nms_oks(kp_predictions, rois, thresh):\n    """"""Nms based on kp predictions.""""""\n    scores = np.mean(kp_predictions[:, 2, :], axis=1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        ovr = compute_oks(\n            kp_predictions[i], rois[i], kp_predictions[order[1:]],\n            rois[order[1:]])\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n\n\ndef compute_oks(src_keypoints, src_roi, dst_keypoints, dst_roi):\n    """"""Compute OKS for predicted keypoints wrt gt_keypoints.\n    src_keypoints: 4xK\n    src_roi: 4x1\n    dst_keypoints: Nx4xK\n    dst_roi: Nx4\n    """"""\n\n    sigmas = np.array([\n        .26, .25, .25, .35, .35, .79, .79, .72, .72, .62, .62, 1.07, 1.07, .87,\n        .87, .89, .89]) / 10.0\n    vars = (sigmas * 2)**2\n\n    # area\n    src_area = (src_roi[2] - src_roi[0] + 1) * (src_roi[3] - src_roi[1] + 1)\n\n    # measure the per-keypoint distance if keypoints visible\n    dx = dst_keypoints[:, 0, :] - src_keypoints[0, :]\n    dy = dst_keypoints[:, 1, :] - src_keypoints[1, :]\n\n    e = (dx**2 + dy**2) / vars / (src_area + np.spacing(1)) / 2\n    e = np.sum(np.exp(-e), axis=1) / e.shape[1]\n\n    return e\n'"
lib/utils/logging.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Utilities for logging.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import deque\nfrom email.mime.text import MIMEText\nimport json\nimport logging\nimport numpy as np\nimport smtplib\nimport sys\n\nfrom core.config import cfg\n\n# Print lower precision floating point values than default FLOAT_REPR\n# Note! Has no use for json encode with C speedups\njson.encoder.FLOAT_REPR = lambda o: format(o, \'.6f\')\n\n\ndef log_json_stats(stats, sort_keys=True):\n    print(\'json_stats: {:s}\'.format(json.dumps(stats, sort_keys=sort_keys)))\n\n\ndef log_stats(stats, misc_args):\n    """"""Log training statistics to terminal""""""\n    if hasattr(misc_args, \'epoch\'):\n        lines = ""[%s][%s][Epoch %d][Iter %d / %d]\\n"" % (\n            misc_args.run_name, misc_args.cfg_filename,\n            misc_args.epoch, misc_args.step, misc_args.iters_per_epoch)\n    else:\n        lines = ""[%s][%s][Step %d / %d]\\n"" % (\n            misc_args.run_name, misc_args.cfg_filename, stats[\'iter\'], cfg.SOLVER.MAX_ITER)\n\n    lines += ""\\t\\tloss: %.6f, lr: %.6f time: %.6f, eta: %s\\n"" % (\n        stats[\'loss\'], stats[\'lr\'], stats[\'time\'], stats[\'eta\']\n    )\n    if stats[\'metrics\']:\n        lines += ""\\t\\t"" + "", "".join(""%s: %.6f"" % (k, v) for k, v in stats[\'metrics\'].items()) + ""\\n""\n    if stats[\'head_losses\']:\n        lines += ""\\t\\t"" + "", "".join(""%s: %.6f"" % (k, v) for k, v in stats[\'head_losses\'].items()) + ""\\n""\n    if cfg.RPN.RPN_ON:\n        lines += ""\\t\\t"" + "", "".join(""%s: %.6f"" % (k, v) for k, v in stats[\'rpn_losses\'].items()) + ""\\n""\n    if cfg.FPN.FPN_ON:\n        lines += ""\\t\\t"" + "", "".join(""%s: %.6f"" % (k, v) for k, v in stats[\'rpn_fpn_cls_losses\'].items()) + ""\\n""\n        lines += ""\\t\\t"" + "", "".join(""%s: %.6f"" % (k, v) for k, v in stats[\'rpn_fpn_bbox_losses\'].items()) + ""\\n""\n    print(lines[:-1])  # remove last new line\n\n\nclass SmoothedValue(object):\n    """"""Track a series of values and provide access to smoothed values over a\n    window or the global series average.\n    """"""\n\n    def __init__(self, window_size):\n        self.deque = deque(maxlen=window_size)\n        self.series = []\n        self.total = 0.0\n        self.count = 0\n\n    def AddValue(self, value):\n        self.deque.append(value)\n        self.series.append(value)\n        self.count += 1\n        self.total += value\n\n    def GetMedianValue(self):\n        return np.median(self.deque)\n\n    def GetAverageValue(self):\n        return np.mean(self.deque)\n\n    def GetGlobalAverageValue(self):\n        return self.total / self.count\n\n\ndef send_email(subject, body, to):\n    s = smtplib.SMTP(\'localhost\')\n    mime = MIMEText(body)\n    mime[\'Subject\'] = subject\n    mime[\'To\'] = to\n    s.sendmail(\'detectron\', to, mime.as_string())\n\n\ndef setup_logging(name):\n    FORMAT = \'%(levelname)s %(filename)s:%(lineno)4d: %(message)s\'\n    # Manually clear root loggers to prevent any module that may have called\n    # logging.basicConfig() from blocking our logging setup\n    logging.root.handlers = []\n    logging.basicConfig(level=logging.INFO, format=FORMAT, stream=sys.stdout)\n    logger = logging.getLogger(name)\n    return logger\n'"
lib/utils/misc.py,4,"b'import os\nimport socket\nfrom collections import defaultdict, Iterable\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom itertools import chain\n\nimport torch\n\nfrom core.config import cfg\n\n\ndef get_run_name():\n    """""" A unique name for each run """"""\n    return datetime.now().strftime(\n        \'%b%d-%H-%M-%S\') + \'_\' + socket.gethostname()\n\n\ndef get_output_dir(args, run_name):\n    """""" Get root output directory for each run """"""\n    cfg_filename, _ = os.path.splitext(os.path.split(args.cfg_file)[1])\n    return os.path.join(cfg.OUTPUT_DIR, cfg_filename, run_name)\n\n\nIMG_EXTENSIONS = [\'.jpg\', \'.jpeg\', \'.png\', \'.ppm\', \'.bmp\', \'.pgm\']\n\n\ndef is_image_file(filename):\n    """"""Checks if a file is an image.\n      Args:\n          filename (string): path to a file\n      Returns:\n          bool: True if the filename ends with a known image extension\n    """"""\n    filename_lower = filename.lower()\n    return any(filename_lower.endswith(ext) for ext in IMG_EXTENSIONS)\n\n\ndef get_imagelist_from_dir(dirpath):\n    images = []\n    for f in os.listdir(dirpath):\n        if is_image_file(f):\n            images.append(os.path.join(dirpath, f))\n    return images\n\n\ndef ensure_optimizer_ckpt_params_order(param_groups_names, checkpoint):\n    """"""Reorder the parameter ids in the SGD optimizer checkpoint to match\n    the current order in the program, in case parameter insertion order is changed.\n    """"""\n    assert len(param_groups_names) == len(checkpoint[\'optimizer\'][\'param_groups\'])\n    param_lens = (len(g) for g in param_groups_names)\n    saved_lens = (len(g[\'params\']) for g in checkpoint[\'optimizer\'][\'param_groups\'])\n    if any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):\n        raise ValueError(""loaded state dict contains a parameter group ""\n                         ""that doesn\'t match the size of optimizer\'s group"")\n\n    name_to_curpos = {}\n    for i, p_names in enumerate(param_groups_names):\n        for j, name in enumerate(p_names):\n            name_to_curpos[name] = (i, j)\n\n    param_groups_inds = [[] for _ in range(len(param_groups_names))]\n    cnts = [0] * len(param_groups_names)\n    for key in checkpoint[\'model\']:\n        pos = name_to_curpos.get(key)\n        if pos:\n            # print(key, pos, cnts[pos[0]])\n            saved_p_id = checkpoint[\'optimizer\'][\'param_groups\'][pos[0]][\'params\'][cnts[pos[0]]]\n            assert (checkpoint[\'model\'][key].shape ==\n                    checkpoint[\'optimizer\'][\'state\'][saved_p_id][\'momentum_buffer\'].shape), \\\n                   (\'param and momentum_buffer shape mismatch in checkpoint.\'\n                    \' param_name: {}, param_id: {}\'.format(key, saved_p_id))\n            param_groups_inds[pos[0]].append(pos[1])\n            cnts[pos[0]] += 1\n\n    for cnt, param_inds in enumerate(param_groups_inds):\n        ckpt_params = checkpoint[\'optimizer\'][\'param_groups\'][cnt][\'params\']\n        assert len(ckpt_params) == len(param_inds)\n        ckpt_params = [x for x, _ in sorted(zip(ckpt_params, param_inds), key=lambda x: x[1])]\n        checkpoint[\'optimizer\'][\'param_groups\'][cnt][\'params\'] = ckpt_params\n\n\ndef load_optimizer_state_dict(optimizer, state_dict):\n    # deepcopy, to be consistent with module API\n    state_dict = deepcopy(state_dict)\n    # Validate the state_dict\n    groups = optimizer.param_groups\n    saved_groups = state_dict[\'param_groups\']\n\n    if len(groups) != len(saved_groups):\n        raise ValueError(""loaded state dict has a different number of ""\n                         ""parameter groups"")\n    param_lens = (len(g[\'params\']) for g in groups)\n    saved_lens = (len(g[\'params\']) for g in saved_groups)\n    if any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):\n        raise ValueError(""loaded state dict contains a parameter group ""\n                         ""that doesn\'t match the size of optimizer\'s group"")\n\n    # Update the state\n    id_map = {old_id: p for old_id, p in\n                zip(chain(*(g[\'params\'] for g in saved_groups)),\n                    chain(*(g[\'params\'] for g in groups)))}\n\n    def cast(param, value):\n        """"""Make a deep copy of value, casting all tensors to device of param.""""""\n        if torch.is_tensor(value):\n            # Floating-point types are a bit special here. They are the only ones\n            # that are assumed to always match the type of params.\n            if isinstance(param.data, (torch.FloatTensor, torch.cuda.FloatTensor,\n                                       torch.DoubleTensor, torch.cuda.DoubleTensor,\n                                       torch.HalfTensor, torch.cuda.HalfTensor)):  # param.is_floating_point():\n                value = value.type_as(param.data)\n            value = value.cuda(param.get_device()) if param.is_cuda else value.cpu()\n            return value\n        elif isinstance(value, dict):\n            return {k: cast(param, v) for k, v in value.items()}\n        elif isinstance(value, Iterable):\n            return type(value)(cast(param, v) for v in value)\n        else:\n            return value\n\n    # Copy state assigned to params (and cast tensors to appropriate types).\n    # State that is not assigned to params is copied as is (needed for\n    # backward compatibility).\n    state = defaultdict(dict)\n    for k, v in state_dict[\'state\'].items():\n        if k in id_map:\n            param = id_map[k]\n            state[param] = cast(param, v)\n        else:\n            state[k] = v\n\n    # Update parameter groups, setting their \'params\' value\n    def update_group(group, new_group):\n        new_group[\'params\'] = group[\'params\']\n        return new_group\n    param_groups = [\n        update_group(g, ng) for g, ng in zip(groups, saved_groups)]\n    optimizer.__setstate__({\'state\': state, \'param_groups\': param_groups})\n'"
lib/utils/net.py,7,"b'import logging\nimport os\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom core.config import cfg\nimport nn as mynn\n\nlogger = logging.getLogger(__name__)\n\n\ndef smooth_l1_loss(bbox_pred, bbox_targets, bbox_inside_weights, bbox_outside_weights, beta=1.0):\n    """"""\n    SmoothL1(x) = 0.5 * x^2 / beta      if |x| < beta\n                  |x| - 0.5 * beta      otherwise.\n    1 / N * sum_i alpha_out[i] * SmoothL1(alpha_in[i] * (y_hat[i] - y[i])).\n    N is the number of batch elements in the input predictions\n    """"""\n    box_diff = bbox_pred - bbox_targets\n    in_box_diff = bbox_inside_weights * box_diff\n    abs_in_box_diff = torch.abs(in_box_diff)\n    smoothL1_sign = (abs_in_box_diff < beta).detach().float()\n    in_loss_box = smoothL1_sign * 0.5 * torch.pow(in_box_diff, 2) / beta + \\\n                  (1 - smoothL1_sign) * (abs_in_box_diff - (0.5 * beta))\n    out_loss_box = bbox_outside_weights * in_loss_box\n    loss_box = out_loss_box\n    N = loss_box.size(0)  # batch size\n    loss_box = loss_box.view(-1).sum(0) / N\n    return loss_box\n\n\ndef clip_gradient(model, clip_norm):\n    """"""Computes a gradient clipping coefficient based on gradient norm.""""""\n    totalnorm = 0\n    for p in model.parameters():\n        if p.requires_grad:\n            modulenorm = p.grad.data.norm()\n            totalnorm += modulenorm ** 2\n    totalnorm = np.sqrt(totalnorm)\n\n    norm = clip_norm / max(totalnorm, clip_norm)\n    for p in model.parameters():\n        if p.requires_grad:\n            p.grad.mul_(norm)\n\n\ndef decay_learning_rate(optimizer, cur_lr, decay_rate):\n    """"""Decay learning rate""""""\n    new_lr = cur_lr * decay_rate\n    # ratio = _get_lr_change_ratio(cur_lr, new_lr)\n    ratio = 1 / decay_rate\n    if ratio > cfg.SOLVER.LOG_LR_CHANGE_THRESHOLD:\n        logger.info(\'Changing learning rate %.6f -> %.6f\', cur_lr, new_lr)\n    # Update learning rate, note that different parameter may have different learning rate\n    for param_group in optimizer.param_groups:\n        cur_lr = param_group[\'lr\']\n        new_lr = decay_rate * param_group[\'lr\']\n        param_group[\'lr\'] = new_lr\n        if cfg.SOLVER.TYPE in [\'SGD\']:\n            if cfg.SOLVER.SCALE_MOMENTUM and cur_lr > 1e-7 and \\\n                    ratio > cfg.SOLVER.SCALE_MOMENTUM_THRESHOLD:\n                _CorrectMomentum(optimizer, param_group[\'params\'], new_lr / cur_lr)\n\ndef update_learning_rate(optimizer, cur_lr, new_lr):\n    """"""Update learning rate""""""\n    if cur_lr != new_lr:\n        ratio = _get_lr_change_ratio(cur_lr, new_lr)\n        if ratio > cfg.SOLVER.LOG_LR_CHANGE_THRESHOLD:\n            logger.info(\'Changing learning rate %.6f -> %.6f\', cur_lr, new_lr)\n        # Update learning rate, note that different parameter may have different learning rate\n        param_keys = []\n        for ind, param_group in enumerate(optimizer.param_groups):\n            if ind == 1 and cfg.SOLVER.BIAS_DOUBLE_LR:  # bias params\n                param_group[\'lr\'] = new_lr * 2\n            else:\n                param_group[\'lr\'] = new_lr\n            param_keys += param_group[\'params\']\n        if cfg.SOLVER.TYPE in [\'SGD\'] and cfg.SOLVER.SCALE_MOMENTUM and cur_lr > 1e-7 and \\\n                ratio > cfg.SOLVER.SCALE_MOMENTUM_THRESHOLD:\n            _CorrectMomentum(optimizer, param_keys, new_lr / cur_lr)\n\n\ndef _CorrectMomentum(optimizer, param_keys, correction):\n    """"""The MomentumSGDUpdate op implements the update V as\n\n        V := mu * V + lr * grad,\n\n    where mu is the momentum factor, lr is the learning rate, and grad is\n    the stochastic gradient. Since V is not defined independently of the\n    learning rate (as it should ideally be), when the learning rate is\n    changed we should scale the update history V in order to make it\n    compatible in scale with lr * grad.\n    """"""\n    logger.info(\'Scaling update history by %.6f (new lr / old lr)\', correction)\n    for p_key in param_keys:\n        optimizer.state[p_key][\'momentum_buffer\'] *= correction\n\n\ndef _get_lr_change_ratio(cur_lr, new_lr):\n    eps = 1e-10\n    ratio = np.max(\n        (new_lr / np.max((cur_lr, eps)), cur_lr / np.max((new_lr, eps)))\n    )\n    return ratio\n\n\ndef affine_grid_gen(rois, input_size, grid_size):\n\n    rois = rois.detach()\n    x1 = rois[:, 1::4] / 16.0\n    y1 = rois[:, 2::4] / 16.0\n    x2 = rois[:, 3::4] / 16.0\n    y2 = rois[:, 4::4] / 16.0\n\n    height = input_size[0]\n    width = input_size[1]\n\n    zero = Variable(rois.data.new(rois.size(0), 1).zero_())\n    theta = torch.cat([\\\n      (x2 - x1) / (width - 1),\n      zero,\n      (x1 + x2 - width + 1) / (width - 1),\n      zero,\n      (y2 - y1) / (height - 1),\n      (y1 + y2 - height + 1) / (height - 1)], 1).view(-1, 2, 3)\n\n    grid = F.affine_grid(theta, torch.Size((rois.size(0), 1, grid_size, grid_size)))\n\n    return grid\n\n\ndef save_ckpt(output_dir, args, model, optimizer):\n    """"""Save checkpoint""""""\n    if args.no_save:\n        return\n    ckpt_dir = os.path.join(output_dir, \'ckpt\')\n    if not os.path.exists(ckpt_dir):\n        os.makedirs(ckpt_dir)\n    save_name = os.path.join(ckpt_dir, \'model_{}_{}.pth\'.format(args.epoch, args.step))\n    if isinstance(model, mynn.DataParallel):\n        model = model.module\n    # TODO: (maybe) Do not save redundant shared params\n    # model_state_dict = model.state_dict()\n    torch.save({\n        \'epoch\': args.epoch,\n        \'step\': args.step,\n        \'iters_per_epoch\': args.iters_per_epoch,\n        \'model\': model.state_dict(),\n        \'optimizer\': optimizer.state_dict()}, save_name)\n    logger.info(\'save model: %s\', save_name)\n\n\ndef load_ckpt(model, ckpt):\n    """"""Load checkpoint""""""\n    mapping, _ = model.detectron_weight_mapping\n    state_dict = {}\n    for name in ckpt:\n        if mapping[name]:\n            state_dict[name] = ckpt[name]\n    model.load_state_dict(state_dict, strict=False)\n\n\ndef get_group_gn(dim):\n    """"""\n    get number of groups used by GroupNorm, based on number of channels\n    """"""\n    dim_per_gp = cfg.GROUP_NORM.DIM_PER_GP\n    num_groups = cfg.GROUP_NORM.NUM_GROUPS\n\n    assert dim_per_gp == -1 or num_groups == -1, \\\n        ""GroupNorm: can only specify G or C/G.""\n\n    if dim_per_gp > 0:\n        assert dim % dim_per_gp == 0\n        group_gn = dim // dim_per_gp\n    else:\n        assert dim % num_groups == 0\n        group_gn = num_groups\n    return group_gn\n'"
lib/utils/resnet_weights_helper.py,3,"b'""""""\nHelper functions for converting resnet pretrained weights from other formats\n""""""\nimport os\nimport pickle\n\nimport torch\n\nimport nn as mynn\nimport utils.detectron_weight_helper as dwh\nfrom core.config import cfg\n\n\ndef load_pretrained_imagenet_weights(model):\n    """"""Load pretrained weights\n    Args:\n        num_layers: 50 for res50 and so on.\n        model: the generalized rcnnn module\n    """"""\n    _, ext = os.path.splitext(cfg.RESNETS.IMAGENET_PRETRAINED_WEIGHTS)\n    if ext == \'.pkl\':\n        with open(cfg.RESNETS.IMAGENET_PRETRAINED_WEIGHTS, \'rb\') as fp:\n            src_blobs = pickle.load(fp, encoding=\'latin1\')\n        if \'blobs\' in src_blobs:\n            src_blobs = src_blobs[\'blobs\']\n        pretrianed_state_dict = src_blobs\n    else:\n        weights_file = os.path.join(cfg.ROOT_DIR, cfg.RESNETS.IMAGENET_PRETRAINED_WEIGHTS)\n        pretrianed_state_dict = convert_state_dict(torch.load(weights_file))\n\n        # Convert batchnorm weights\n        for name, mod in model.named_modules():\n            if isinstance(mod, mynn.AffineChannel2d):\n                if cfg.FPN.FPN_ON:\n                    pretrianed_name = name.split(\'.\', 2)[-1]\n                else:\n                    pretrianed_name = name.split(\'.\', 1)[-1]\n                bn_mean = pretrianed_state_dict[pretrianed_name + \'.running_mean\']\n                bn_var = pretrianed_state_dict[pretrianed_name + \'.running_var\']\n                scale = pretrianed_state_dict[pretrianed_name + \'.weight\']\n                bias = pretrianed_state_dict[pretrianed_name + \'.bias\']\n                std = torch.sqrt(bn_var + 1e-5)\n                new_scale = scale / std\n                new_bias = bias - bn_mean * scale / std\n                pretrianed_state_dict[pretrianed_name + \'.weight\'] = new_scale\n                pretrianed_state_dict[pretrianed_name + \'.bias\'] = new_bias\n\n    model_state_dict = model.state_dict()\n\n    pattern = dwh.resnet_weights_name_pattern()\n\n    name_mapping, _ = model.detectron_weight_mapping\n\n    for k, v in name_mapping.items():\n        if isinstance(v, str):  # maybe a str, None or True\n            if pattern.match(v):\n                if cfg.FPN.FPN_ON:\n                    pretrianed_key = k.split(\'.\', 2)[-1]\n                else:\n                    pretrianed_key = k.split(\'.\', 1)[-1]\n                if ext == \'.pkl\':\n                    model_state_dict[k].copy_(torch.Tensor(pretrianed_state_dict[v]))\n                else:\n                    model_state_dict[k].copy_(pretrianed_state_dict[pretrianed_key])\n\n\ndef convert_state_dict(src_dict):\n    """"""Return the correct mapping of tensor name and value\n\n    Mapping from the names of torchvision model to our resnet conv_body and box_head.\n    """"""\n    dst_dict = {}\n    for k, v in src_dict.items():\n        toks = k.split(\'.\')\n        if k.startswith(\'layer\'):\n            assert len(toks[0]) == 6\n            res_id = int(toks[0][5]) + 1\n            name = \'.\'.join([\'res%d\' % res_id] + toks[1:])\n            dst_dict[name] = v\n        elif k.startswith(\'fc\'):\n            continue\n        else:\n            name = \'.\'.join([\'res1\'] + toks)\n            dst_dict[name] = v\n    return dst_dict\n'"
lib/utils/segms.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n""""""Functions for interacting with segmentation masks in the COCO format.\n\nThe following terms are used in this module\n    mask: a binary mask encoded as a 2D numpy array\n    segm: a segmentation mask in one of the two COCO formats (polygon or RLE)\n    polygon: COCO\'s polygon format\n    RLE: COCO\'s run length encoding format\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport numpy as np\n\nimport pycocotools.mask as mask_util\n\n\ndef flip_segms(segms, height, width):\n  """"""Left/right flip each mask in a list of masks.""""""\n\n  def _flip_poly(poly, width):\n    flipped_poly = np.array(poly)\n    flipped_poly[0::2] = width - np.array(poly[0::2]) - 1\n    return flipped_poly.tolist()\n\n  def _flip_rle(rle, height, width):\n    if \'counts\' in rle and type(rle[\'counts\']) == list:\n      # Magic RLE format handling painfully discovered by looking at the\n      # COCO API showAnns function.\n      rle = mask_util.frPyObjects([rle], height, width)\n    mask = mask_util.decode(rle)\n    mask = mask[:, ::-1, :]\n    rle = mask_util.encode(np.array(mask, order=\'F\', dtype=np.uint8))\n    return rle\n\n  flipped_segms = []\n  for segm in segms:\n    if type(segm) == list:\n      # Polygon format\n      flipped_segms.append([_flip_poly(poly, width) for poly in segm])\n    else:\n      # RLE format\n      assert type(segm) == dict\n      flipped_segms.append(_flip_rle(segm, height, width))\n  return flipped_segms\n\n\ndef polys_to_mask(polygons, height, width):\n  """"""Convert from the COCO polygon segmentation format to a binary mask\n    encoded as a 2D array of data type numpy.float32. The polygon segmentation\n    is understood to be enclosed inside a height x width image. The resulting\n    mask is therefore of shape (height, width).\n    """"""\n  rle = mask_util.frPyObjects(polygons, height, width)\n  mask = np.array(mask_util.decode(rle), dtype=np.float32)\n  # Flatten in case polygons was a list\n  mask = np.sum(mask, axis=2)\n  mask = np.array(mask > 0, dtype=np.float32)\n  return mask\n\n\ndef mask_to_bbox(mask):\n  """"""Compute the tight bounding box of a binary mask.""""""\n  xs = np.where(np.sum(mask, axis=0) > 0)[0]\n  ys = np.where(np.sum(mask, axis=1) > 0)[0]\n\n  if len(xs) == 0 or len(ys) == 0:\n    return None\n\n  x0 = xs[0]\n  x1 = xs[-1]\n  y0 = ys[0]\n  y1 = ys[-1]\n  return np.array((x0, y0, x1, y1), dtype=np.float32)\n\n\ndef polys_to_mask_wrt_box(polygons, box, M):\n  """"""Convert from the COCO polygon segmentation format to a binary mask\n    encoded as a 2D array of data type numpy.float32. The polygon segmentation\n    is understood to be enclosed in the given box and rasterized to an M x M\n    mask. The resulting mask is therefore of shape (M, M).\n    """"""\n  w = box[2] - box[0]\n  h = box[3] - box[1]\n\n  w = np.maximum(w, 1)\n  h = np.maximum(h, 1)\n\n  polygons_norm = []\n  for poly in polygons:\n    p = np.array(poly, dtype=np.float32)\n    p[0::2] = (p[0::2] - box[0]) * M / w\n    p[1::2] = (p[1::2] - box[1]) * M / h\n    polygons_norm.append(p)\n\n  rle = mask_util.frPyObjects(polygons_norm, M, M)\n  mask = np.array(mask_util.decode(rle), dtype=np.float32)\n  # Flatten in case polygons was a list\n  mask = np.sum(mask, axis=2)\n  mask = np.array(mask > 0, dtype=np.float32)\n  return mask\n\n\ndef polys_to_boxes(polys):\n  """"""Convert a list of polygons into an array of tight bounding boxes.""""""\n  boxes_from_polys = np.zeros((len(polys), 4), dtype=np.float32)\n  for i in range(len(polys)):\n    poly = polys[i]\n    x0 = min(min(p[::2]) for p in poly)\n    x1 = max(max(p[::2]) for p in poly)\n    y0 = min(min(p[1::2]) for p in poly)\n    y1 = max(max(p[1::2]) for p in poly)\n    boxes_from_polys[i, :] = [x0, y0, x1, y1]\n\n  return boxes_from_polys\n\n\ndef rle_mask_voting(top_masks,\n                    all_masks,\n                    all_dets,\n                    iou_thresh,\n                    binarize_thresh,\n                    method=\'AVG\'):\n  """"""Returns new masks (in correspondence with `top_masks`) by combining\n    multiple overlapping masks coming from the pool of `all_masks`. Two methods\n    for combining masks are supported: \'AVG\' uses a weighted average of\n    overlapping mask pixels; \'UNION\' takes the union of all mask pixels.\n    """"""\n  if len(top_masks) == 0:\n    return\n\n  all_not_crowd = [False] * len(all_masks)\n  top_to_all_overlaps = mask_util.iou(top_masks, all_masks, all_not_crowd)\n  decoded_all_masks = [\n      np.array(mask_util.decode(rle), dtype=np.float32) for rle in all_masks\n  ]\n  decoded_top_masks = [\n      np.array(mask_util.decode(rle), dtype=np.float32) for rle in top_masks\n  ]\n  all_boxes = all_dets[:, :4].astype(np.int32)\n  all_scores = all_dets[:, 4]\n\n  # Fill box support with weights\n  mask_shape = decoded_all_masks[0].shape\n  mask_weights = np.zeros((len(all_masks), mask_shape[0], mask_shape[1]))\n  for k in range(len(all_masks)):\n    ref_box = all_boxes[k]\n    x_0 = max(ref_box[0], 0)\n    x_1 = min(ref_box[2] + 1, mask_shape[1])\n    y_0 = max(ref_box[1], 0)\n    y_1 = min(ref_box[3] + 1, mask_shape[0])\n    mask_weights[k, y_0:y_1, x_0:x_1] = all_scores[k]\n  mask_weights = np.maximum(mask_weights, 1e-5)\n\n  top_segms_out = []\n  for k in range(len(top_masks)):\n    # Corner case of empty mask\n    if decoded_top_masks[k].sum() == 0:\n      top_segms_out.append(top_masks[k])\n      continue\n\n    inds_to_vote = np.where(top_to_all_overlaps[k] >= iou_thresh)[0]\n    # Only matches itself\n    if len(inds_to_vote) == 1:\n      top_segms_out.append(top_masks[k])\n      continue\n\n    masks_to_vote = [decoded_all_masks[i] for i in inds_to_vote]\n    if method == \'AVG\':\n      ws = mask_weights[inds_to_vote]\n      soft_mask = np.average(masks_to_vote, axis=0, weights=ws)\n      mask = np.array(soft_mask > binarize_thresh, dtype=np.uint8)\n    elif method == \'UNION\':\n      # Any pixel that\'s on joins the mask\n      soft_mask = np.sum(masks_to_vote, axis=0)\n      mask = np.array(soft_mask > 1e-5, dtype=np.uint8)\n    else:\n      raise NotImplementedError(\'Method {} is unknown\'.format(method))\n    rle = mask_util.encode(np.array(mask[:, :, np.newaxis], order=\'F\'))[0]\n    top_segms_out.append(rle)\n\n  return top_segms_out\n\n\ndef rle_mask_nms(masks, dets, thresh, mode=\'IOU\'):\n  """"""Performs greedy non-maximum suppression based on an overlap measurement\n    between masks. The type of measurement is determined by `mode` and can be\n    either \'IOU\' (standard intersection over union) or \'IOMA\' (intersection over\n    mininum area).\n    """"""\n  if len(masks) == 0:\n    return []\n  if len(masks) == 1:\n    return [0]\n\n  if mode == \'IOU\':\n    # Computes ious[m1, m2] = area(intersect(m1, m2)) / area(union(m1, m2))\n    all_not_crowds = [False] * len(masks)\n    ious = mask_util.iou(masks, masks, all_not_crowds)\n  elif mode == \'IOMA\':\n    # Computes ious[m1, m2] = area(intersect(m1, m2)) / min(area(m1), area(m2))\n    all_crowds = [True] * len(masks)\n    # ious[m1, m2] = area(intersect(m1, m2)) / area(m2)\n    ious = mask_util.iou(masks, masks, all_crowds)\n    # ... = max(area(intersect(m1, m2)) / area(m2),\n    #           area(intersect(m2, m1)) / area(m1))\n    ious = np.maximum(ious, ious.transpose())\n  elif mode == \'CONTAINMENT\':\n    # Computes ious[m1, m2] = area(intersect(m1, m2)) / area(m2)\n    # Which measures how much m2 is contained inside m1\n    all_crowds = [True] * len(masks)\n    ious = mask_util.iou(masks, masks, all_crowds)\n  else:\n    raise NotImplementedError(\'Mode {} is unknown\'.format(mode))\n\n  scores = dets[:, 4]\n  order = np.argsort(-scores)\n\n  keep = []\n  while order.size > 0:\n    i = order[0]\n    keep.append(i)\n    ovr = ious[i, order[1:]]\n    inds_to_keep = np.where(ovr <= thresh)[0]\n    order = order[inds_to_keep + 1]\n\n  return keep\n\n\ndef rle_masks_to_boxes(masks):\n  """"""Computes the bounding box of each mask in a list of RLE encoded masks.""""""\n  if len(masks) == 0:\n    return []\n\n  decoded_masks = [\n      np.array(mask_util.decode(rle), dtype=np.float32) for rle in masks\n  ]\n\n  def get_bounds(flat_mask):\n    inds = np.where(flat_mask > 0)[0]\n    return inds.min(), inds.max()\n\n  boxes = np.zeros((len(decoded_masks), 4))\n  keep = [True] * len(decoded_masks)\n  for i, mask in enumerate(decoded_masks):\n    if mask.sum() == 0:\n      keep[i] = False\n      continue\n    flat_mask = mask.sum(axis=0)\n    x0, x1 = get_bounds(flat_mask)\n    flat_mask = mask.sum(axis=1)\n    y0, y1 = get_bounds(flat_mask)\n    boxes[i, :] = (x0, y0, x1, y1)\n\n  return boxes, np.where(keep)[0]\n'"
lib/utils/subprocess.py,1,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Primitives for running multiple single-GPU jobs in parallel over subranges of\ndata. These are used for running multi-GPU inference. Subprocesses are used to\navoid the GIL since inference may involve non-trivial amounts of Python code.\n""""""\n\n# from __future__ import absolute_import\n# from __future__ import division\n# from __future__ import print_function\n# from __future__ import unicode_literals\n\nfrom io import IOBase\nimport logging\nimport os\nimport subprocess\nfrom six.moves import shlex_quote\nfrom six.moves import cPickle as pickle\nimport yaml\nimport numpy as np\nimport torch\n\nfrom core.config import cfg\n\nlogger = logging.getLogger(__name__)\n\n\ndef process_in_parallel(\n        tag, total_range_size, binary, output_dir,\n        load_ckpt, load_detectron, opts=\'\'):\n    """"""Run the specified binary NUM_GPUS times in parallel, each time as a\n    subprocess that uses one GPU. The binary must accept the command line\n    arguments `--range {start} {end}` that specify a data processing range.\n    """"""\n    # Snapshot the current cfg state in order to pass to the inference\n    # subprocesses\n    cfg_file = os.path.join(output_dir, \'{}_range_config.yaml\'.format(tag))\n    with open(cfg_file, \'w\') as f:\n        yaml.dump(cfg, stream=f)\n    subprocess_env = os.environ.copy()\n    processes = []\n    NUM_GPUS = torch.cuda.device_count()\n    subinds = np.array_split(range(total_range_size), NUM_GPUS)\n    # Determine GPUs to use\n    cuda_visible_devices = os.environ.get(\'CUDA_VISIBLE_DEVICES\')\n    if cuda_visible_devices:\n        gpu_inds = list(map(int, cuda_visible_devices.split(\',\')))\n        assert -1 not in gpu_inds, \\\n            \'Hiding GPU indices using the \\\'-1\\\' index is not supported\'\n    else:\n        gpu_inds = range(cfg.NUM_GPUS)\n    gpu_inds = list(gpu_inds)\n    # Run the binary in cfg.NUM_GPUS subprocesses\n    for i, gpu_ind in enumerate(gpu_inds):\n        start = subinds[i][0]\n        end = subinds[i][-1] + 1\n        subprocess_env[\'CUDA_VISIBLE_DEVICES\'] = str(gpu_ind)\n        cmd = (\'python {binary} --range {start} {end} --cfg {cfg_file} --set {opts} \'\n               \'--output_dir {output_dir}\')\n        if load_ckpt is not None:\n            cmd += \' --load_ckpt {load_ckpt}\'\n        elif load_detectron is not None:\n            cmd += \' --load_detectron {load_detectron}\'\n        cmd = cmd.format(\n            binary=shlex_quote(binary),\n            start=int(start),\n            end=int(end),\n            cfg_file=shlex_quote(cfg_file),\n            output_dir=output_dir,\n            load_ckpt=load_ckpt,\n            load_detectron=load_detectron,\n            opts=\' \'.join([shlex_quote(opt) for opt in opts])\n        )\n        logger.info(\'{} range command {}: {}\'.format(tag, i, cmd))\n        if i == 0:\n            subprocess_stdout = subprocess.PIPE\n        else:\n            filename = os.path.join(\n                output_dir, \'%s_range_%s_%s.stdout\' % (tag, start, end)\n            )\n            subprocess_stdout = open(filename, \'w\')\n        p = subprocess.Popen(\n            cmd,\n            shell=True,\n            env=subprocess_env,\n            stdout=subprocess_stdout,\n            stderr=subprocess.STDOUT,\n            bufsize=1\n        )\n        processes.append((i, p, start, end, subprocess_stdout))\n    # Log output from inference processes and collate their results\n    outputs = []\n    for i, p, start, end, subprocess_stdout in processes:\n        log_subprocess_output(i, p, output_dir, tag, start, end)\n        if isinstance(subprocess_stdout, IOBase):\n            subprocess_stdout.close()\n        range_file = os.path.join(\n            output_dir, \'%s_range_%s_%s.pkl\' % (tag, start, end)\n        )\n        range_data = pickle.load(open(range_file, \'rb\'))\n        outputs.append(range_data)\n    return outputs\n\n\ndef log_subprocess_output(i, p, output_dir, tag, start, end):\n    """"""Capture the output of each subprocess and log it in the parent process.\n    The first subprocess\'s output is logged in realtime. The output from the\n    other subprocesses is buffered and then printed all at once (in order) when\n    subprocesses finish.\n    """"""\n    outfile = os.path.join(\n        output_dir, \'%s_range_%s_%s.stdout\' % (tag, start, end)\n    )\n    logger.info(\'# \' + \'-\' * 76 + \' #\')\n    logger.info(\n        \'stdout of subprocess %s with range [%s, %s]\' % (i, start + 1, end)\n    )\n    logger.info(\'# \' + \'-\' * 76 + \' #\')\n    if i == 0:\n        # Stream the piped stdout from the first subprocess in realtime\n        with open(outfile, \'w\') as f:\n            for line in iter(p.stdout.readline, b\'\'):\n                print(line.rstrip().decode(\'ascii\'))\n                f.write(str(line, encoding=\'ascii\'))\n        p.stdout.close()\n        ret = p.wait()\n    else:\n        # For subprocesses >= 1, wait and dump their log file\n        ret = p.wait()\n        with open(outfile, \'r\') as f:\n            print(\'\'.join(f.readlines()))\n    assert ret == 0, \'Range subprocess failed (exit code: {})\'.format(ret)\n'"
lib/utils/timer.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport time\n\n\nclass Timer(object):\n  """"""A simple timer.""""""\n\n  def __init__(self):\n    self.reset()\n\n  def tic(self):\n    # using time.time instead of time.clock because time time.clock\n    # does not normalize for multithreading\n    self.start_time = time.time()\n\n  def toc(self, average=True):\n    self.diff = time.time() - self.start_time\n    self.total_time += self.diff\n    self.calls += 1\n    self.average_time = self.total_time / self.calls\n    if average:\n      return self.average_time\n    else:\n      return self.diff\n\n  def reset(self):\n    self.total_time = 0.\n    self.calls = 0\n    self.start_time = 0.\n    self.diff = 0.\n    self.average_time = 0.\n'"
lib/utils/training_stats.py,0,"b'#!/usr/bin/env python2\n\n# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n""""""Utilities for training.""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom collections import defaultdict, OrderedDict\nimport datetime\nimport numpy as np\n\nfrom core.config import cfg\nfrom utils.logging import log_stats\nfrom utils.logging import SmoothedValue\nfrom utils.timer import Timer\nimport utils.net as nu\n\n\nclass TrainingStats(object):\n    """"""Track vital training statistics.""""""\n\n    def __init__(self, misc_args, log_period=20, tensorboard_logger=None):\n        # Output logging period in SGD iterations\n        self.misc_args = misc_args\n        self.LOG_PERIOD = log_period\n        self.tblogger = tensorboard_logger\n        self.tb_ignored_keys = [\'iter\', \'eta\']\n        self.iter_timer = Timer()\n        # Window size for smoothing tracked values (with median filtering)\n        self.WIN_SZ = 20\n        def create_smoothed_value():\n            return SmoothedValue(self.WIN_SZ)\n        self.smoothed_losses = defaultdict(create_smoothed_value)\n        self.smoothed_metrics = defaultdict(create_smoothed_value)\n        self.smoothed_total_loss = SmoothedValue(self.WIN_SZ)\n        # For the support of args.iter_size\n        self.inner_total_loss = []\n        self.inner_losses = defaultdict(list)\n        if cfg.FPN.FPN_ON:\n            self.inner_loss_rpn_cls = []\n            self.inner_loss_rpn_bbox = []\n        self.inner_metrics = defaultdict(list)\n\n    def IterTic(self):\n        self.iter_timer.tic()\n\n    def IterToc(self):\n        return self.iter_timer.toc(average=False)\n\n    def ResetIterTimer(self):\n        self.iter_timer.reset()\n\n    def UpdateIterStats(self, model_out, inner_iter=None):\n        """"""Update tracked iteration statistics.""""""\n        if inner_iter is not None and self.misc_args.iter_size > 1:\n            # For the case of using args.iter_size > 1\n            return self._UpdateIterStats_inner(model_out, inner_iter)\n\n        # Following code is saved for compatability of train_net.py and iter_size==1\n        total_loss = 0\n        if cfg.FPN.FPN_ON:\n            loss_rpn_cls_data = 0\n            loss_rpn_bbox_data = 0\n\n        for k, loss in model_out[\'losses\'].items():\n            assert loss.shape[0] == cfg.NUM_GPUS\n            loss = loss.mean(dim=0, keepdim=True)\n            total_loss += loss\n            loss_data = loss.data[0]\n            model_out[\'losses\'][k] = loss\n            if cfg.FPN.FPN_ON:\n                if k.startswith(\'loss_rpn_cls_\'):\n                    loss_rpn_cls_data += loss_data\n                elif k.startswith(\'loss_rpn_bbox_\'):\n                    loss_rpn_bbox_data += loss_data\n            self.smoothed_losses[k].AddValue(loss_data)\n\n        model_out[\'total_loss\'] = total_loss  # Add the total loss for back propagation\n        self.smoothed_total_loss.AddValue(total_loss.data[0])\n        if cfg.FPN.FPN_ON:\n            self.smoothed_losses[\'loss_rpn_cls\'].AddValue(loss_rpn_cls_data)\n            self.smoothed_losses[\'loss_rpn_bbox\'].AddValue(loss_rpn_bbox_data)\n\n        for k, metric in model_out[\'metrics\'].items():\n            metric = metric.mean(dim=0, keepdim=True)\n            self.smoothed_metrics[k].AddValue(metric.data[0])\n\n    def _UpdateIterStats_inner(self, model_out, inner_iter):\n        """"""Update tracked iteration statistics for the case of iter_size > 1""""""\n        assert inner_iter < self.misc_args.iter_size\n\n        total_loss = 0\n        if cfg.FPN.FPN_ON:\n            loss_rpn_cls_data = 0\n            loss_rpn_bbox_data = 0\n\n        if inner_iter == 0:\n            self.inner_total_loss = []\n            for k in model_out[\'losses\']:\n                self.inner_losses[k] = []\n            if cfg.FPN.FPN_ON:\n                self.inner_loss_rpn_cls = []\n                self.inner_loss_rpn_bbox = []\n            for k in model_out[\'metrics\']:\n                self.inner_metrics[k] = []\n\n        for k, loss in model_out[\'losses\'].items():\n            assert loss.shape[0] == cfg.NUM_GPUS\n            loss = loss.mean(dim=0, keepdim=True)\n            total_loss += loss\n            loss_data = loss.data[0]\n\n            model_out[\'losses\'][k] = loss\n            if cfg.FPN.FPN_ON:\n                if k.startswith(\'loss_rpn_cls_\'):\n                    loss_rpn_cls_data += loss_data\n                elif k.startswith(\'loss_rpn_bbox_\'):\n                    loss_rpn_bbox_data += loss_data\n\n            self.inner_losses[k].append(loss_data)\n            if inner_iter == (self.misc_args.iter_size - 1):\n                loss_data = self._mean_and_reset_inner_list(\'inner_losses\', k)\n                self.smoothed_losses[k].AddValue(loss_data)\n\n        model_out[\'total_loss\'] = total_loss  # Add the total loss for back propagation\n        total_loss_data = total_loss.data[0]\n        self.inner_total_loss.append(total_loss_data)\n        if cfg.FPN.FPN_ON:\n            self.inner_loss_rpn_cls.append(loss_rpn_cls_data)\n            self.inner_loss_rpn_bbox.append(loss_rpn_bbox_data)\n        if inner_iter == (self.misc_args.iter_size - 1):\n            total_loss_data = self._mean_and_reset_inner_list(\'inner_total_loss\')\n            self.smoothed_total_loss.AddValue(total_loss_data)\n            if cfg.FPN.FPN_ON:\n                loss_rpn_cls_data = self._mean_and_reset_inner_list(\'inner_loss_rpn_cls\')\n                loss_rpn_bbox_data = self._mean_and_reset_inner_list(\'inner_loss_rpn_bbox\')\n                self.smoothed_losses[\'loss_rpn_cls\'].AddValue(loss_rpn_cls_data)\n                self.smoothed_losses[\'loss_rpn_bbox\'].AddValue(loss_rpn_bbox_data)\n\n        for k, metric in model_out[\'metrics\'].items():\n            metric = metric.mean(dim=0, keepdim=True)\n            metric_data = metric.data[0]\n            self.inner_metrics[k].append(metric_data)\n            if inner_iter == (self.misc_args.iter_size - 1):\n                metric_data = self._mean_and_reset_inner_list(\'inner_metrics\', k)\n                self.smoothed_metrics[k].AddValue(metric_data)\n\n    def _mean_and_reset_inner_list(self, attr_name, key=None):\n        """"""Take the mean and reset list empty""""""\n        if key:\n            mean_val = sum(getattr(self, attr_name)[key]) / self.misc_args.iter_size\n            getattr(self, attr_name)[key] = []\n        else:\n            mean_val = sum(getattr(self, attr_name)) / self.misc_args.iter_size\n            setattr(self, attr_name, [])\n        return mean_val\n\n    def LogIterStats(self, cur_iter, lr):\n        """"""Log the tracked statistics.""""""\n        if (cur_iter % self.LOG_PERIOD == 0 or\n                cur_iter == cfg.SOLVER.MAX_ITER - 1):\n            stats = self.GetStats(cur_iter, lr)\n            log_stats(stats, self.misc_args)\n            if self.tblogger:\n                self.tb_log_stats(stats, cur_iter)\n\n    def tb_log_stats(self, stats, cur_iter):\n        """"""Log the tracked statistics to tensorboard""""""\n        for k in stats:\n            if k not in self.tb_ignored_keys:\n                v = stats[k]\n                if isinstance(v, dict):\n                    self.tb_log_stats(v, cur_iter)\n                else:\n                    self.tblogger.add_scalar(k, v, cur_iter)\n\n    def GetStats(self, cur_iter, lr):\n        eta_seconds = self.iter_timer.average_time * (\n            cfg.SOLVER.MAX_ITER - cur_iter\n        )\n        eta = str(datetime.timedelta(seconds=int(eta_seconds)))\n        stats = OrderedDict(\n            iter=cur_iter + 1,  # 1-indexed\n            time=self.iter_timer.average_time,\n            eta=eta,\n            loss=self.smoothed_total_loss.GetMedianValue(),\n            lr=lr,\n        )\n        stats[\'metrics\'] = OrderedDict()\n        for k in sorted(self.smoothed_metrics):\n            stats[\'metrics\'][k] = self.smoothed_metrics[k].GetMedianValue()\n\n        head_losses = []\n        rpn_losses = []\n        rpn_fpn_cls_losses = []\n        rpn_fpn_bbox_losses = []\n        for k, v in self.smoothed_losses.items():\n            toks = k.split(\'_\')\n            if len(toks) == 2:\n                head_losses.append((k, v.GetMedianValue()))\n            elif len(toks) == 3:\n                rpn_losses.append((k, v.GetMedianValue()))\n            elif len(toks) == 4 and toks[2] == \'cls\':\n                rpn_fpn_cls_losses.append((k, v.GetMedianValue()))\n            elif len(toks) == 4 and toks[2] == \'bbox\':\n                rpn_fpn_bbox_losses.append((k, v.GetMedianValue()))\n            else:\n                raise ValueError(""Unexpected loss key: %s"" % k)\n        stats[\'head_losses\'] = OrderedDict(head_losses)\n        stats[\'rpn_losses\'] = OrderedDict(rpn_losses)\n        stats[\'rpn_fpn_cls_losses\'] = OrderedDict(rpn_fpn_cls_losses)\n        stats[\'rpn_fpn_bbox_losses\'] = OrderedDict(rpn_fpn_bbox_losses)\n\n        return stats\n'"
lib/utils/vis.py,0,"b'# Written by Roy Tseng\n#\n# Based on:\n# --------------------------------------------------------\n# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport cv2\nimport numpy as np\nimport os\nimport pycocotools.mask as mask_util\n\nfrom utils.colormap import colormap\nimport utils.keypoints as keypoint_utils\n\n# Use a non-interactive backend\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\n\nplt.rcParams[\'pdf.fonttype\'] = 42  # For editing in Adobe Illustrator\n\n\n_GRAY = (218, 227, 218)\n_GREEN = (18, 127, 15)\n_WHITE = (255, 255, 255)\n\n\ndef kp_connections(keypoints):\n    kp_lines = [\n        [keypoints.index(\'left_eye\'), keypoints.index(\'right_eye\')],\n        [keypoints.index(\'left_eye\'), keypoints.index(\'nose\')],\n        [keypoints.index(\'right_eye\'), keypoints.index(\'nose\')],\n        [keypoints.index(\'right_eye\'), keypoints.index(\'right_ear\')],\n        [keypoints.index(\'left_eye\'), keypoints.index(\'left_ear\')],\n        [keypoints.index(\'right_shoulder\'), keypoints.index(\'right_elbow\')],\n        [keypoints.index(\'right_elbow\'), keypoints.index(\'right_wrist\')],\n        [keypoints.index(\'left_shoulder\'), keypoints.index(\'left_elbow\')],\n        [keypoints.index(\'left_elbow\'), keypoints.index(\'left_wrist\')],\n        [keypoints.index(\'right_hip\'), keypoints.index(\'right_knee\')],\n        [keypoints.index(\'right_knee\'), keypoints.index(\'right_ankle\')],\n        [keypoints.index(\'left_hip\'), keypoints.index(\'left_knee\')],\n        [keypoints.index(\'left_knee\'), keypoints.index(\'left_ankle\')],\n        [keypoints.index(\'right_shoulder\'), keypoints.index(\'left_shoulder\')],\n        [keypoints.index(\'right_hip\'), keypoints.index(\'left_hip\')],\n    ]\n    return kp_lines\n\n\ndef convert_from_cls_format(cls_boxes, cls_segms, cls_keyps):\n    """"""Convert from the class boxes/segms/keyps format generated by the testing\n    code.\n    """"""\n    box_list = [b for b in cls_boxes if len(b) > 0]\n    if len(box_list) > 0:\n        boxes = np.concatenate(box_list)\n    else:\n        boxes = None\n    if cls_segms is not None:\n        segms = [s for slist in cls_segms for s in slist]\n    else:\n        segms = None\n    if cls_keyps is not None:\n        keyps = [k for klist in cls_keyps for k in klist]\n    else:\n        keyps = None\n    classes = []\n    for j in range(len(cls_boxes)):\n        classes += [j] * len(cls_boxes[j])\n    return boxes, segms, keyps, classes\n\n\ndef vis_bbox_opencv(img, bbox, thick=1):\n    """"""Visualizes a bounding box.""""""\n    (x0, y0, w, h) = bbox\n    x1, y1 = int(x0 + w), int(y0 + h)\n    x0, y0 = int(x0), int(y0)\n    cv2.rectangle(img, (x0, y0), (x1, y1), _GREEN, thickness=thick)\n    return img\n\n\ndef get_class_string(class_index, score, dataset):\n    class_text = dataset.classes[class_index] if dataset is not None else \\\n        \'id{:d}\'.format(class_index)\n    return class_text + \' {:0.2f}\'.format(score).lstrip(\'0\')\n\n\ndef vis_one_image(\n        im, im_name, output_dir, boxes, segms=None, keypoints=None, thresh=0.9,\n        kp_thresh=2, dpi=200, box_alpha=0.0, dataset=None, show_class=False,\n        ext=\'pdf\'):\n    """"""Visual debugging of detections.""""""\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    if isinstance(boxes, list):\n        boxes, segms, keypoints, classes = convert_from_cls_format(\n            boxes, segms, keypoints)\n\n    if boxes is None or boxes.shape[0] == 0 or max(boxes[:, 4]) < thresh:\n        return\n\n    if segms is not None:\n        masks = mask_util.decode(segms)\n\n    color_list = colormap(rgb=True) / 255\n\n    dataset_keypoints, _ = keypoint_utils.get_keypoints()\n    kp_lines = kp_connections(dataset_keypoints)\n    cmap = plt.get_cmap(\'rainbow\')\n    colors = [cmap(i) for i in np.linspace(0, 1, len(kp_lines) + 2)]\n\n    fig = plt.figure(frameon=False)\n    fig.set_size_inches(im.shape[1] / dpi, im.shape[0] / dpi)\n    ax = plt.Axes(fig, [0., 0., 1., 1.])\n    ax.axis(\'off\')\n    fig.add_axes(ax)\n    ax.imshow(im)\n\n    # Display in largest to smallest order to reduce occlusion\n    areas = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n    sorted_inds = np.argsort(-areas)\n\n    mask_color_id = 0\n    for i in sorted_inds:\n        bbox = boxes[i, :4]\n        score = boxes[i, -1]\n        if score < thresh:\n            continue\n\n        print(dataset.classes[classes[i]], score)\n        # show box (off by default, box_alpha=0.0)\n        ax.add_patch(\n            plt.Rectangle((bbox[0], bbox[1]),\n                          bbox[2] - bbox[0],\n                          bbox[3] - bbox[1],\n                          fill=False, edgecolor=\'g\',\n                          linewidth=0.5, alpha=box_alpha))\n\n        if show_class:\n            ax.text(\n                bbox[0], bbox[1] - 2,\n                get_class_string(classes[i], score, dataset),\n                fontsize=3,\n                family=\'serif\',\n                bbox=dict(\n                    facecolor=\'g\', alpha=0.4, pad=0, edgecolor=\'none\'),\n                color=\'white\')\n\n        # show mask\n        if segms is not None and len(segms) > i:\n            img = np.ones(im.shape)\n            color_mask = color_list[mask_color_id % len(color_list), 0:3]\n            mask_color_id += 1\n\n            w_ratio = .4\n            for c in range(3):\n                color_mask[c] = color_mask[c] * (1 - w_ratio) + w_ratio\n            for c in range(3):\n                img[:, :, c] = color_mask[c]\n            e = masks[:, :, i]\n\n            _, contour, hier = cv2.findContours(\n                e.copy(), cv2.RETR_CCOMP, cv2.CHAIN_APPROX_NONE)\n\n            for c in contour:\n                polygon = Polygon(\n                    c.reshape((-1, 2)),\n                    fill=True, facecolor=color_mask,\n                    edgecolor=\'w\', linewidth=1.2,\n                    alpha=0.5)\n                ax.add_patch(polygon)\n\n        # show keypoints\n        if keypoints is not None and len(keypoints) > i:\n            kps = keypoints[i]\n            plt.autoscale(False)\n            for l in range(len(kp_lines)):\n                i1 = kp_lines[l][0]\n                i2 = kp_lines[l][1]\n                if kps[2, i1] > kp_thresh and kps[2, i2] > kp_thresh:\n                    x = [kps[0, i1], kps[0, i2]]\n                    y = [kps[1, i1], kps[1, i2]]\n                    line = ax.plot(x, y)\n                    plt.setp(line, color=colors[l], linewidth=1.0, alpha=0.7)\n                if kps[2, i1] > kp_thresh:\n                    ax.plot(\n                        kps[0, i1], kps[1, i1], \'.\', color=colors[l],\n                        markersize=3.0, alpha=0.7)\n                if kps[2, i2] > kp_thresh:\n                    ax.plot(\n                        kps[0, i2], kps[1, i2], \'.\', color=colors[l],\n                        markersize=3.0, alpha=0.7)\n\n            # add mid shoulder / mid hip for better visualization\n            mid_shoulder = (\n                kps[:2, dataset_keypoints.index(\'right_shoulder\')] +\n                kps[:2, dataset_keypoints.index(\'left_shoulder\')]) / 2.0\n            sc_mid_shoulder = np.minimum(\n                kps[2, dataset_keypoints.index(\'right_shoulder\')],\n                kps[2, dataset_keypoints.index(\'left_shoulder\')])\n            mid_hip = (\n                kps[:2, dataset_keypoints.index(\'right_hip\')] +\n                kps[:2, dataset_keypoints.index(\'left_hip\')]) / 2.0\n            sc_mid_hip = np.minimum(\n                kps[2, dataset_keypoints.index(\'right_hip\')],\n                kps[2, dataset_keypoints.index(\'left_hip\')])\n            if (sc_mid_shoulder > kp_thresh and\n                    kps[2, dataset_keypoints.index(\'nose\')] > kp_thresh):\n                x = [mid_shoulder[0], kps[0, dataset_keypoints.index(\'nose\')]]\n                y = [mid_shoulder[1], kps[1, dataset_keypoints.index(\'nose\')]]\n                line = ax.plot(x, y)\n                plt.setp(\n                    line, color=colors[len(kp_lines)], linewidth=1.0, alpha=0.7)\n            if sc_mid_shoulder > kp_thresh and sc_mid_hip > kp_thresh:\n                x = [mid_shoulder[0], mid_hip[0]]\n                y = [mid_shoulder[1], mid_hip[1]]\n                line = ax.plot(x, y)\n                plt.setp(\n                    line, color=colors[len(kp_lines) + 1], linewidth=1.0,\n                    alpha=0.7)\n\n        output_name = os.path.basename(im_name) + \'.\' + ext\n        fig.savefig(os.path.join(output_dir, \'{}\'.format(output_name)), dpi=dpi)\n        plt.close(\'all\')\n'"
lib/datasets/cityscapes/__init__.py,0,b''
lib/datasets/cityscapes/coco_to_cityscapes_id.py,0,"b'# Copyright (c) 2017-present, Facebook, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n##############################################################################\n\n# mapping coco categories to cityscapes (our converted json) id\n# cityscapes\n# INFO roidb.py: 220: 1       bicycle: 7286\n# INFO roidb.py: 220: 2           car: 53684\n# INFO roidb.py: 220: 3        person: 35704\n# INFO roidb.py: 220: 4         train: 336\n# INFO roidb.py: 220: 5         truck: 964\n# INFO roidb.py: 220: 6    motorcycle: 1468\n# INFO roidb.py: 220: 7           bus: 758\n# INFO roidb.py: 220: 8         rider: 3504\n\n# coco (val5k)\n# INFO roidb.py: 220: 1        person: 21296\n# INFO roidb.py: 220: 2       bicycle: 628\n# INFO roidb.py: 220: 3           car: 3818\n# INFO roidb.py: 220: 4    motorcycle: 732\n# INFO roidb.py: 220: 5      airplane: 286 <------ irrelevant\n# INFO roidb.py: 220: 6           bus: 564\n# INFO roidb.py: 220: 7         train: 380\n# INFO roidb.py: 220: 8         truck: 828\n\n\ndef cityscapes_to_coco(cityscapes_id):\n    lookup = {\n        0: 0,  # ... background\n        1: 2,  # bicycle\n        2: 3,  # car\n        3: 1,  # person\n        4: 7,  # train\n        5: 8,  # truck\n        6: 4,  # motorcycle\n        7: 6,  # bus\n        8: -1,  # rider (-1 means rand init)\n    }\n    return lookup[cityscapes_id]\n\n\ndef cityscapes_to_coco_with_rider(cityscapes_id):\n    lookup = {\n        0: 0,  # ... background\n        1: 2,  # bicycle\n        2: 3,  # car\n        3: 1,  # person\n        4: 7,  # train\n        5: 8,  # truck\n        6: 4,  # motorcycle\n        7: 6,  # bus\n        8: 1,  # rider (""person"", *rider has human right!*)\n    }\n    return lookup[cityscapes_id]\n\n\ndef cityscapes_to_coco_without_person_rider(cityscapes_id):\n    lookup = {\n        0: 0,  # ... background\n        1: 2,  # bicycle\n        2: 3,  # car\n        3: -1,  # person (ignore)\n        4: 7,  # train\n        5: 8,  # truck\n        6: 4,  # motorcycle\n        7: 6,  # bus\n        8: -1,  # rider (ignore)\n    }\n    return lookup[cityscapes_id]\n\n\ndef cityscapes_to_coco_all_random(cityscapes_id):\n    lookup = {\n        0: -1,  # ... background\n        1: -1,  # bicycle\n        2: -1,  # car\n        3: -1,  # person (ignore)\n        4: -1,  # train\n        5: -1,  # truck\n        6: -1,  # motorcycle\n        7: -1,  # bus\n        8: -1,  # rider (ignore)\n    }\n    return lookup[cityscapes_id]\n'"
lib/model/nms/__init__.py,0,b''
lib/model/nms/build.py,2,"b""from __future__ import print_function\nimport os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n#this_file = os.path.dirname(__file__)\n\nsources = []\nheaders = []\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/nms_cuda.c']\n    headers += ['src/nms_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/nms_cuda_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\nprint(extra_objects)\n\nffi = create_extension(\n    '_ext.nms',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
lib/model/nms/nms_gpu.py,0,"b'from __future__ import absolute_import\nimport torch\nimport numpy as np\nfrom ._ext import nms\nimport pdb\n\ndef nms_gpu(dets, thresh):\n\tkeep = dets.new(dets.size(0), 1).zero_().int()\n\tnum_out = dets.new(1).zero_().int()\n\tnms.nms_cuda(keep, dets, num_out, thresh)\n\tkeep = keep[:num_out[0]]\n\treturn keep\n'"
lib/model/nms/nms_wrapper.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\nimport torch\nfrom core.config import cfg\nfrom model.nms.nms_gpu import nms_gpu\n\ndef nms(dets, thresh, force_cpu=False):\n    """"""Dispatch to either CPU or GPU NMS implementations.""""""\n    if dets.shape[0] == 0:\n        return []\n    # ---numpy version---\n    # original: return gpu_nms(dets, thresh, device_id=cfg.GPU_ID)\n    # ---pytorch version---\n    return nms_gpu(dets, thresh)\n'"
lib/model/roi_align/__init__.py,0,b''
lib/model/roi_align/build.py,2,"b""from __future__ import print_function\nimport os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n# sources = ['src/roi_align.c']\n# headers = ['src/roi_align.h']\nsources = []\nheaders = []\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/roi_align_cuda.c']\n    headers += ['src/roi_align_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/roi_align_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.roi_align',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
lib/model/roi_crop/__init__.py,0,b''
lib/model/roi_crop/build.py,2,"b""from __future__ import print_function\nimport os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n#this_file = os.path.dirname(__file__)\n\nsources = ['src/roi_crop.c']\nheaders = ['src/roi_crop.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/roi_crop_cuda.c']\n    headers += ['src/roi_crop_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/roi_crop_cuda_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.roi_crop',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
lib/model/roi_pooling/__init__.py,0,b''
lib/model/roi_pooling/build.py,2,"b""from __future__ import print_function\nimport os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n\nsources = ['src/roi_pooling.c']\nheaders = ['src/roi_pooling.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/roi_pooling_cuda.c']\n    headers += ['src/roi_pooling_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/roi_pooling.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.roi_pooling',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
lib/model/utils/__init__.py,0,b''
lib/model/utils/net_utils.py,16,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport numpy as np\nimport torchvision.models as models\nfrom core.config import cfg\nfrom model.roi_crop.functions.roi_crop import RoICropFunction\nimport cv2\nimport pdb\nimport random\n\ndef save_net(fname, net):\n    import h5py\n    h5f = h5py.File(fname, mode=\'w\')\n    for k, v in net.state_dict().items():\n        h5f.create_dataset(k, data=v.cpu().numpy())\n\ndef load_net(fname, net):\n    import h5py\n    h5f = h5py.File(fname, mode=\'r\')\n    for k, v in net.state_dict().items():\n        param = torch.from_numpy(np.asarray(h5f[k]))\n        v.copy_(param)\n\ndef weights_normal_init(model, dev=0.01):\n    if isinstance(model, list):\n        for m in model:\n            weights_normal_init(m, dev)\n    else:\n        for m in model.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0.0, dev)\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0.0, dev)\n\n\ndef _crop_pool_layer(bottom, rois, max_pool=True):\n    # code modified from \n    # https://github.com/ruotianluo/pytorch-faster-rcnn\n    # implement it using stn\n    # box to affine\n    # input (x1,y1,x2,y2)\n    """"""\n    [  x2-x1             x1 + x2 - W + 1  ]\n    [  -----      0      ---------------  ]\n    [  W - 1                  W - 1       ]\n    [                                     ]\n    [           y2-y1    y1 + y2 - H + 1  ]\n    [    0      -----    ---------------  ]\n    [           H - 1         H - 1      ]\n    """"""\n    rois = rois.detach()\n    batch_size = bottom.size(0)\n    D = bottom.size(1)\n    H = bottom.size(2)\n    W = bottom.size(3)\n    roi_per_batch = rois.size(0) / batch_size\n    x1 = rois[:, 1::4] / 16.0\n    y1 = rois[:, 2::4] / 16.0\n    x2 = rois[:, 3::4] / 16.0\n    y2 = rois[:, 4::4] / 16.0\n\n    height = bottom.size(2)\n    width = bottom.size(3)\n\n    # affine theta\n    zero = Variable(rois.data.new(rois.size(0), 1).zero_())\n    theta = torch.cat([\\\n      (x2 - x1) / (width - 1),\n      zero,\n      (x1 + x2 - width + 1) / (width - 1),\n      zero,\n      (y2 - y1) / (height - 1),\n      (y1 + y2 - height + 1) / (height - 1)], 1).view(-1, 2, 3)\n\n    if max_pool:\n      pre_pool_size = cfg.POOLING_SIZE * 2\n      grid = F.affine_grid(theta, torch.Size((rois.size(0), 1, pre_pool_size, pre_pool_size)))\n      bottom = bottom.view(1, batch_size, D, H, W).contiguous().expand(roi_per_batch, batch_size, D, H, W)\\\n                                                                .contiguous().view(-1, D, H, W)\n      crops = F.grid_sample(bottom, grid)\n      crops = F.max_pool2d(crops, 2, 2)\n    else:\n      grid = F.affine_grid(theta, torch.Size((rois.size(0), 1, cfg.POOLING_SIZE, cfg.POOLING_SIZE)))\n      bottom = bottom.view(1, batch_size, D, H, W).contiguous().expand(roi_per_batch, batch_size, D, H, W)\\\n                                                                .contiguous().view(-1, D, H, W)\n      crops = F.grid_sample(bottom, grid)\n    \n    return crops, grid\n\ndef _affine_grid_gen(rois, input_size, grid_size):\n\n    rois = rois.detach()\n    x1 = rois[:, 1::4] / 16.0\n    y1 = rois[:, 2::4] / 16.0\n    x2 = rois[:, 3::4] / 16.0\n    y2 = rois[:, 4::4] / 16.0\n\n    height = input_size[0]\n    width = input_size[1]\n\n    zero = Variable(rois.data.new(rois.size(0), 1).zero_())\n    theta = torch.cat([\\\n      (x2 - x1) / (width - 1),\n      zero,\n      (x1 + x2 - width + 1) / (width - 1),\n      zero,\n      (y2 - y1) / (height - 1),\n      (y1 + y2 - height + 1) / (height - 1)], 1).view(-1, 2, 3)\n\n    grid = F.affine_grid(theta, torch.Size((rois.size(0), 1, grid_size, grid_size)))\n\n    return grid\n\ndef _affine_theta(rois, input_size):\n\n    rois = rois.detach()\n    x1 = rois[:, 1::4] / 16.0\n    y1 = rois[:, 2::4] / 16.0\n    x2 = rois[:, 3::4] / 16.0\n    y2 = rois[:, 4::4] / 16.0\n\n    height = input_size[0]\n    width = input_size[1]\n\n    zero = Variable(rois.data.new(rois.size(0), 1).zero_())\n\n    # theta = torch.cat([\\\n    #   (x2 - x1) / (width - 1),\n    #   zero,\n    #   (x1 + x2 - width + 1) / (width - 1),\n    #   zero,\n    #   (y2 - y1) / (height - 1),\n    #   (y1 + y2 - height + 1) / (height - 1)], 1).view(-1, 2, 3)\n\n    theta = torch.cat([\\\n      (y2 - y1) / (height - 1),\n      zero,\n      (y1 + y2 - height + 1) / (height - 1),\n      zero,\n      (x2 - x1) / (width - 1),\n      (x1 + x2 - width + 1) / (width - 1)], 1).view(-1, 2, 3)\n\n    return theta\n\ndef compare_grid_sample():\n    # do gradcheck\n    N = random.randint(1, 8)\n    C = 2 # random.randint(1, 8)\n    H = 5 # random.randint(1, 8)\n    W = 4 # random.randint(1, 8)\n    input = Variable(torch.randn(N, C, H, W).cuda(), requires_grad=True)\n    input_p = input.clone().data.contiguous()\n   \n    grid = Variable(torch.randn(N, H, W, 2).cuda(), requires_grad=True)\n    grid_clone = grid.clone().contiguous()\n\n    out_offcial = F.grid_sample(input, grid)    \n    grad_outputs = Variable(torch.rand(out_offcial.size()).cuda())\n    grad_outputs_clone = grad_outputs.clone().contiguous()\n    grad_inputs = torch.autograd.grad(out_offcial, (input, grid), grad_outputs.contiguous())\n    grad_input_off = grad_inputs[0]\n\n\n    crf = RoICropFunction()\n    grid_yx = torch.stack([grid_clone.data[:,:,:,1], grid_clone.data[:,:,:,0]], 3).contiguous().cuda()\n    out_stn = crf.forward(input_p, grid_yx)\n    grad_inputs = crf.backward(grad_outputs_clone.data)\n    grad_input_stn = grad_inputs[0]\n    pdb.set_trace()\n\n    delta = (grad_input_off.data - grad_input_stn).sum()\n'"
lib/modeling/roi_xfrom/__init__.py,0,b''
lib/nn/modules/__init__.py,0,b'from .affine import AffineChannel2d\nfrom .normalization import GroupNorm\nfrom .upsample import BilinearInterpolation2d\n'
lib/nn/modules/affine.py,3,"b'import torch\nimport torch.nn as nn\n\n\nclass AffineChannel2d(nn.Module):\n    """""" A simple channel-wise affine transformation operation """"""\n    def __init__(self, num_features):\n        super().__init__()\n        self.num_features = num_features\n        self.weight = nn.Parameter(torch.Tensor(num_features))\n        self.bias = nn.Parameter(torch.Tensor(num_features))\n        self.weight.data.uniform_()\n        self.bias.data.zero_()\n\n    def forward(self, x):\n        return x * self.weight.view(1, self.num_features, 1, 1) + \\\n            self.bias.view(1, self.num_features, 1, 1)\n'"
lib/nn/modules/normalization.py,3,"b'""""""Normalization Layers""""""\n\nimport torch\nimport torch.nn as nn\n\nimport nn.functional as myF\n\n\nclass GroupNorm(nn.Module):\n    def __init__(self, num_groups, num_channels, eps=1e-5, affine=True):\n        super().__init__()\n        self.num_groups = num_groups\n        self.num_channels = num_channels\n        self.eps = eps\n        self.affine = affine\n        if self.affine:\n            self.weight = nn.Parameter(torch.Tensor(num_channels))\n            self.bias = nn.Parameter(torch.Tensor(num_channels))\n        else:\n            self.register_parameter(\'weight\', None)\n            self.register_parameter(\'bias\', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        if self.affine:\n            self.weight.data.fill_(1)\n            self.bias.data.zero_()\n\n    def forward(self, x):\n        return myF.group_norm(\n            x, self.num_groups, self.weight, self.bias, self.eps\n        )\n\n    def extra_repr(self):\n        return \'{num_groups}, {num_channels}, eps={eps}, \' \\\n            \'affine={affine}\'.format(**self.__dict__)\n'"
lib/nn/modules/upsample.py,4,"b'import numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\nclass BilinearInterpolation2d(nn.Module):\n    """"""Bilinear interpolation in space of scale.\n\n    Takes input of NxKxHxW and outputs NxKx(sH)x(sW), where s:= up_scale\n\n    Adapted from the CVPR\'15 FCN code.\n    See: https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/surgery.py\n    """"""\n    def __init__(self, in_channels, out_channels, up_scale):\n        super().__init__()\n        assert in_channels == out_channels\n        assert up_scale % 2 == 0, \'Scale should be even\'\n        self.in_channes = in_channels\n        self.out_channels = out_channels\n        self.up_scale = int(up_scale)\n        self.padding = up_scale // 2\n\n        def upsample_filt(size):\n            factor = (size + 1) // 2\n            if size % 2 == 1:\n                center = factor - 1\n            else:\n                center = factor - 0.5\n            og = np.ogrid[:size, :size]\n            return ((1 - abs(og[0] - center) / factor) *\n                    (1 - abs(og[1] - center) / factor))\n\n        kernel_size = up_scale * 2\n        bil_filt = upsample_filt(kernel_size)\n\n        kernel = np.zeros(\n            (in_channels, out_channels, kernel_size, kernel_size), dtype=np.float32\n        )\n        kernel[range(in_channels), range(out_channels), :, :] = bil_filt\n\n        self.upconv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size,\n                                         stride=self.up_scale, padding=self.padding)\n\n        self.upconv.weight.data.copy_(torch.from_numpy(kernel))\n        self.upconv.bias.data.fill_(0)\n        self.upconv.weight.requires_grad = False\n        self.upconv.bias.requires_grad = False\n\n    def forward(self, x):\n        return self.upconv(x)\n'"
lib/nn/parallel/__init__.py,0,"b""from .parallel_apply import parallel_apply\nfrom .replicate import replicate\nfrom .data_parallel import DataParallel, data_parallel\nfrom .scatter_gather import scatter, gather\n\n__all__ = ['replicate', 'scatter', 'parallel_apply', 'gather', 'data_parallel',\n           'DataParallel']\n"""
lib/nn/parallel/_functions.py,6,"b'import torch\nimport torch.cuda.comm as comm\nfrom torch.autograd import Function\n\n\nclass Broadcast(Function):\n\n    @staticmethod\n    def forward(ctx, target_gpus, *inputs):\n        if not all(input.is_cuda for input in inputs):\n            raise TypeError(\'Broadcast function not implemented for CPU tensors\')\n        ctx.target_gpus = target_gpus\n        if len(inputs) == 0:\n            return tuple()\n        ctx.num_inputs = len(inputs)\n        ctx.input_device = inputs[0].get_device()\n        outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)\n        non_differentiables = []\n        for idx, input_requires_grad in enumerate(ctx.needs_input_grad[1:]):\n            if not input_requires_grad:\n                for output in outputs:\n                    non_differentiables.append(output[idx])\n        ctx.mark_non_differentiable(*non_differentiables)\n        return tuple([t for tensors in outputs for t in tensors])\n\n    @staticmethod\n    def backward(ctx, *grad_outputs):\n        return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)\n\n\nclass ReduceAddCoalesced(Function):\n\n    @staticmethod\n    def forward(ctx, destination, num_inputs, *grads):\n        ctx.target_gpus = [grads[i].get_device() for i in range(0, len(grads), num_inputs)]\n\n        grads = [grads[i:i + num_inputs]\n                 for i in range(0, len(grads), num_inputs)]\n        return comm.reduce_add_coalesced(grads, destination)\n\n    @staticmethod\n    def backward(ctx, *grad_outputs):\n        return (None, None,) + Broadcast.apply(ctx.target_gpus, *grad_outputs)\n\n\nclass Gather(Function):\n\n    @staticmethod\n    def forward(ctx, target_device, dim, *inputs):\n        assert all(map(lambda i: i.is_cuda, inputs))\n        ctx.target_device = target_device\n        ctx.dim = dim\n        ctx.input_gpus = tuple(map(lambda i: i.get_device(), inputs))\n        ctx.input_sizes = tuple(map(lambda i: i.size(ctx.dim), inputs))\n        return comm.gather(inputs, ctx.dim, ctx.target_device)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        return (None, None) + Scatter.apply(ctx.input_gpus, ctx.input_sizes, ctx.dim, grad_output)\n\n\nclass Scatter(Function):\n\n    @staticmethod\n    def forward(ctx, target_gpus, chunk_sizes, dim, input):\n        ctx.target_gpus = target_gpus\n        ctx.chunk_sizes = chunk_sizes\n        ctx.dim = dim\n        ctx.input_device = input.get_device() if input.is_cuda else -1\n        streams = None\n        if ctx.input_device == -1:\n            # Perform CPU to GPU copies in a background stream\n            streams = [_get_stream(device) for device in ctx.target_gpus]\n        outputs = comm.scatter(input, ctx.target_gpus, ctx.chunk_sizes, ctx.dim, streams)\n        # Synchronize with the copy stream\n        if streams is not None:\n            for i, output in enumerate(outputs):\n                with torch.cuda.device(ctx.target_gpus[i]):\n                    main_stream = torch.cuda.current_stream()\n                    main_stream.wait_stream(streams[i])\n                    output.record_stream(main_stream)\n        return outputs\n\n    @staticmethod\n    def backward(ctx, *grad_output):\n        return None, None, None, Gather.apply(ctx.input_device, ctx.dim, *grad_output)\n\n\n# background streams used for copying\n_streams = None\n\n\ndef _get_stream(device):\n    """"""Gets a background stream for copying between CPU and GPU""""""\n    global _streams\n    if device == -1:\n        return None\n    if _streams is None:\n        _streams = [None] * torch.cuda.device_count()\n    if _streams[device] is None:\n        _streams[device] = torch.cuda.Stream(device)\n    return _streams[device]\n'"
lib/nn/parallel/data_parallel.py,7,"b'import torch\nfrom torch.nn import Module\nfrom torch.autograd import Variable\nfrom .scatter_gather import scatter_kwargs, gather\nfrom .replicate import replicate\nfrom .parallel_apply import parallel_apply\n\n\nclass DataParallel(Module):\n    r""""""Implements data parallelism at the module level.\n\n    This container parallelizes the application of the given module by\n    splitting the input across the specified devices by chunking in the batch\n    dimension. In the forward pass, the module is replicated on each device,\n    and each replica handles a portion of the input. During the backwards\n    pass, gradients from each replica are summed into the original module.\n\n    The batch size should be larger than the number of GPUs used. It should\n    also be an integer multiple of the number of GPUs so that each chunk is the\n    same size (so that each GPU processes the same number of samples).\n\n    See also: :ref:`cuda-nn-dataparallel-instead`\n\n    Arbitrary positional and keyword inputs are allowed to be passed into\n    DataParallel EXCEPT Tensors. All variables will be scattered on dim\n    specified (default 0). Primitive types will be broadcasted, but all\n    other types will be a shallow copy and can be corrupted if written to in\n    the model\'s forward pass.\n\n    .. warning::\n        Forward and backwrad hooks defined on :attr:`module` and its submodules\n        won\'t be invoked anymore, unless the hooks are initialized in the\n        :meth:`forward` method.\n\n    Args:\n        module: module to be parallelized\n        device_ids: CUDA devices (default: all devices)\n        output_device: device location of output (default: device_ids[0])\n        cpu_keywords: list of argument keywords that could be used in `forward` to\n            indicating not moving the argument to gpu. Currently, only support\n            argument of type: Variable\n\n    Example::\n\n        >>> net = torch.nn.DataParallel(model, device_ids=[0, 1, 2])\n        >>> output = net(input_var)\n    """"""\n\n    # TODO: update notes/cuda.rst when this class handles 8+ GPUs well\n\n    def __init__(self, module, device_ids=None, output_device=None, dim=0, \n                 cpu_keywords=[], minibatch=False, batch_outputs=True):\n        super(DataParallel, self).__init__()\n\n        if not torch.cuda.is_available():\n            self.module = module\n            self.device_ids = []\n            return\n\n        if device_ids is None:\n            device_ids = list(range(torch.cuda.device_count()))\n        if output_device is None:\n            output_device = device_ids[0]\n        self.dim = dim\n        self.module = module\n        self.device_ids = device_ids\n        self.output_device = output_device\n        if len(self.device_ids) == 1:\n            self.module.cuda(device_ids[0])\n        self.cpu_keywords = cpu_keywords\n        self.minibatch = minibatch\n        self.batch_outputs = batch_outputs\n\n    def forward(self, *inputs, **kwargs):\n        if not self.device_ids:\n            return self.module(*inputs, **kwargs)\n\n        if self.minibatch:\n            inputs_list, kwargs_list = [], []\n            for i, device_id in enumerate(self.device_ids):\n                mini_inputs = [x[i] for x in inputs]\n                mini_kwargs = dict([(k, v[i]) for k, v in kwargs.items()])\n                a, b = self._minibatch_scatter(device_id, *mini_inputs, **mini_kwargs)\n                inputs_list.append(a)\n                kwargs_list.append(b)\n            inputs = inputs_list\n            kwargs = kwargs_list\n        else:\n            kwargs_cpu = {}\n            for k in kwargs:\n                if k in self.cpu_keywords:\n                    v = kwargs[k]\n                    kwargs_cpu[k] = v\n            for k in self.cpu_keywords:\n                kwargs.pop(k, None)\n            inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n            # Split cpu Variables\n            for k, v in kwargs_cpu.items():\n                split_size = v.size(self.dim) / len(self.device_ids)\n                assert split_size.is_integer()\n                kwargs_cpu[k] = list(map(Variable, torch.split(v.data, int(split_size), self.dim)))\n            kwargs_cpu = list(map(dict, zip(*[[(k, v) for v in vs] for k, vs in kwargs_cpu.items()]))) # a list of dict\n            # Merge cpu kwargs with gpu kwargs\n            for d_gpu, d_cpu in zip(kwargs, kwargs_cpu):\n                d_gpu.update(d_cpu)\n\n        if len(self.device_ids) == 1:\n            outputs = [self.module(*inputs[0], **kwargs[0])]\n        else:\n            replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n            outputs = self.parallel_apply(replicas, inputs, kwargs)\n\n        if self.batch_outputs:\n            return self.gather(outputs, self.output_device)\n        else:\n            return [self.gather([x], self.output_device) for x in outputs]\n\n    def _minibatch_scatter(self, device_id, *inputs, **kwargs):\n        kwargs_cpu = {}\n        for k in kwargs:\n            if k in self.cpu_keywords:\n                kwargs_cpu[k] = kwargs[k]\n        for k in self.cpu_keywords:\n            kwargs.pop(k, None)\n        inputs, kwargs = self.scatter(inputs, kwargs, [device_id])\n        kwargs_cpu = [kwargs_cpu] # a list of dict\n        # Merge cpu kwargs with gpu kwargs\n        for d_gpu, d_cpu in zip(kwargs, kwargs_cpu):\n            d_gpu.update(d_cpu)\n        return inputs[0], kwargs[0]\n\n    def replicate(self, module, device_ids):\n        return replicate(module, device_ids)\n\n    def scatter(self, inputs, kwargs, device_ids):\n        return scatter_kwargs(inputs, kwargs, device_ids, dim=self.dim)\n\n    def parallel_apply(self, replicas, inputs, kwargs):\n        return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n\n    def gather(self, outputs, output_device):\n        return gather(outputs, output_device, dim=self.dim)\n\n\ndef data_parallel(module, inputs, device_ids=None, output_device=None, dim=0, module_kwargs=None):\n    r""""""Evaluates module(input) in parallel across the GPUs given in device_ids.\n\n    This is the functional version of the DataParallel module.\n\n    Args:\n        module: the module to evaluate in parallel\n        inputs: inputs to the module\n        device_ids: GPU ids on which to replicate module\n        output_device: GPU location of the output  Use -1 to indicate the CPU.\n            (default: device_ids[0])\n    Returns:\n        a Variable containing the result of module(input) located on\n        output_device\n    """"""\n    if not isinstance(inputs, tuple):\n        inputs = (inputs,)\n\n    if device_ids is None:\n        device_ids = list(range(torch.cuda.device_count()))\n\n    if output_device is None:\n        output_device = device_ids[0]\n\n    inputs, module_kwargs = scatter_kwargs(inputs, module_kwargs, device_ids, dim)\n    if len(device_ids) == 1:\n        return module(*inputs[0], **module_kwargs[0])\n    used_device_ids = device_ids[:len(inputs)]\n    replicas = replicate(module, used_device_ids)\n    outputs = parallel_apply(replicas, inputs, module_kwargs, used_device_ids)\n    return gather(outputs, output_device, dim)\n'"
lib/nn/parallel/parallel_apply.py,2,"b'import threading\nimport torch\nfrom torch.autograd import Variable\n\n\ndef get_a_var(obj):\n    if isinstance(obj, Variable):\n        return obj\n\n    if isinstance(obj, list) or isinstance(obj, tuple):\n        results = map(get_a_var, obj)\n        for result in results:\n            if isinstance(result, Variable):\n                return result\n    if isinstance(obj, dict):\n        results = map(get_a_var, obj.items())\n        for result in results:\n            if isinstance(result, Variable):\n                return result\n    return None\n\n\ndef parallel_apply(modules, inputs, kwargs_tup=None, devices=None):\n    assert len(modules) == len(inputs)\n    if kwargs_tup is not None:\n        assert len(modules) == len(kwargs_tup)\n    else:\n        kwargs_tup = ({},) * len(modules)\n    if devices is not None:\n        assert len(modules) == len(devices)\n    else:\n        devices = [None] * len(modules)\n\n    lock = threading.Lock()\n    results = {}\n\n    def _worker(i, module, input, kwargs, results, lock, device=None):\n        if device is None:\n            device = get_a_var(input).get_device()\n        try:\n            with torch.cuda.device(device):\n                output = module(*input, **kwargs)\n            with lock:\n                results[i] = output\n        except Exception as e:\n            with lock:\n                results[i] = e\n\n    if len(modules) > 1:\n        threads = [threading.Thread(target=_worker,\n                                    args=(i, module, input, kwargs, results, lock, device),\n                                    )\n                   for i, (module, input, kwargs, device) in\n                   enumerate(zip(modules, inputs, kwargs_tup, devices))]\n\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n    else:\n        _worker(0, modules[0], inputs[0], kwargs_tup[0], results, lock, devices[0])\n\n    outputs = []\n    for i in range(len(inputs)):\n        output = results[i]\n        if isinstance(output, Exception):\n            raise output\n        outputs.append(output)\n    return outputs\n'"
lib/nn/parallel/replicate.py,1,"b'import torch.cuda.comm as comm\n\n\ndef replicate(network, devices):\n    from ._functions import Broadcast\n\n    devices = tuple(devices)\n    num_replicas = len(devices)\n\n    params = list(network.parameters())\n    param_indices = {param: idx for idx, param in enumerate(params)}\n    param_copies = Broadcast.apply(devices, *params)\n    if len(params) > 0:\n        param_copies = [param_copies[i:i + len(params)]\n                        for i in range(0, len(param_copies), len(params))]\n\n    buffers = list(network._all_buffers())\n    buffer_indices = {buf: idx for idx, buf in enumerate(buffers)}\n    buffer_copies = comm.broadcast_coalesced(buffers, devices)\n\n    modules = list(network.modules())\n    module_copies = [[] for device in devices]\n    module_indices = {}\n\n    for i, module in enumerate(modules):\n        module_indices[module] = i\n        for j in range(num_replicas):\n            replica = module.__new__(type(module))\n            replica.__dict__ = module.__dict__.copy()\n            replica._parameters = replica._parameters.copy()\n            replica._buffers = replica._buffers.copy()\n            replica._modules = replica._modules.copy()\n            module_copies[j].append(replica)\n\n    for i, module in enumerate(modules):\n        for key, child in module._modules.items():\n            if child is None:\n                for j in range(num_replicas):\n                    replica = module_copies[j][i]\n                    replica._modules[key] = None\n            else:\n                module_idx = module_indices[child]\n                for j in range(num_replicas):\n                    replica = module_copies[j][i]\n                    replica._modules[key] = module_copies[j][module_idx]\n        for key, param in module._parameters.items():\n            if param is None:\n                for j in range(num_replicas):\n                    replica = module_copies[j][i]\n                    replica._parameters[key] = None\n            else:\n                param_idx = param_indices[param]\n                for j in range(num_replicas):\n                    replica = module_copies[j][i]\n                    replica._parameters[key] = param_copies[j][param_idx]\n        for key, buf in module._buffers.items():\n            if buf is None:\n                for j in range(num_replicas):\n                    replica = module_copies[j][i]\n                    replica._buffers[key] = None\n            else:\n                buffer_idx = buffer_indices[buf]\n                for j in range(num_replicas):\n                    replica = module_copies[j][i]\n                    replica._buffers[key] = buffer_copies[j][buffer_idx]\n\n    return [module_copies[j][0] for j in range(num_replicas)]\n'"
lib/nn/parallel/scatter_gather.py,7,"b'import collections\nimport re\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom ._functions import Scatter, Gather\nfrom torch._six import string_classes, int_classes\nfrom torch.utils.data.dataloader import numpy_type_map\n\n\ndef scatter(inputs, target_gpus, dim=0):\n    r""""""\n    Slices variables into approximately equal chunks and\n    distributes them across given GPUs. Duplicates\n    references to objects that are not variables. Does not\n    support Tensors.\n    """"""\n    def scatter_map(obj):\n        if isinstance(obj, Variable):\n            return Scatter.apply(target_gpus, None, dim, obj)\n        assert not torch.is_tensor(obj), ""Tensors not supported in scatter.""\n        if isinstance(obj, tuple) and len(obj) > 0:\n            return list(zip(*map(scatter_map, obj)))\n        if isinstance(obj, list) and len(obj) > 0:\n            return list(map(list, zip(*map(scatter_map, obj))))\n        if isinstance(obj, dict) and len(obj) > 0:\n            return list(map(type(obj), zip(*map(scatter_map, obj.items()))))\n        return [obj for targets in target_gpus]\n\n    # After scatter_map is called, a scatter_map cell will exist. This cell\n    # has a reference to the actual function scatter_map, which has references\n    # to a closure that has a reference to the scatter_map cell (because the\n    # fn is recursive). To avoid this reference cycle, we set the function to\n    # None, clearing the cell\n    try:\n        return scatter_map(inputs)\n    finally:\n        scatter_map = None\n\n\ndef scatter_kwargs(inputs, kwargs, target_gpus, dim=0):\n    r""""""Scatter with support for kwargs dictionary""""""\n    inputs = scatter(inputs, target_gpus, dim) if inputs else []\n    kwargs = scatter(kwargs, target_gpus, dim) if kwargs else []\n    if len(inputs) < len(kwargs):\n        inputs.extend([() for _ in range(len(kwargs) - len(inputs))])\n    elif len(kwargs) < len(inputs):\n        kwargs.extend([{} for _ in range(len(inputs) - len(kwargs))])\n    inputs = tuple(inputs)\n    kwargs = tuple(kwargs)\n    return inputs, kwargs\n\n\ndef gather(outputs, target_device, dim=0):\n    r""""""\n    Gathers variables from different GPUs on a specified device\n      (-1 means the CPU).\n    """"""\n    error_msg = ""outputs must contain tensors, numbers, dicts or lists; found {}""\n\n    def gather_map(outputs):\n        out = outputs[0]\n        elem_type = type(out)\n        if isinstance(out, Variable):\n            return Gather.apply(target_device, dim, *outputs)\n        if out is None:\n            return None\n        if isinstance(out, collections.Sequence):\n            return type(out)(map(gather_map, zip(*outputs)))\n        elif isinstance(out, collections.Mapping):\n            return {key: gather_map([d[key] for d in outputs]) for key in out}\n        elif elem_type.__module__ == \'numpy\' and elem_type.__name__ != \'str_\' \\\n                and elem_type.__name__ != \'string_\':\n            elem = out\n            if elem_type.__name__ == \'ndarray\':\n                # array of string classes and object\n                if re.search(\'[SaUO]\', elem.dtype.str) is not None:\n                    raise TypeError(error_msg.format(elem.dtype))\n\n                return Variable(torch.from_numpy(np.concatenate(outputs, dim)))\n            if elem.shape == ():  # scalars\n                py_type = float if elem.dtype.name.startswith(\'float\') else int\n                return Variable(numpy_type_map[elem.dtype.name](list(map(py_type, outputs))))\n        elif isinstance(out, int_classes):\n            return Variable(torch.LongTensor(outputs))\n        elif isinstance(out, float):\n            return Variable(torch.DoubleTensor(outputs))\n        elif isinstance(out, string_classes):\n            return outputs\n\n        raise TypeError((error_msg.format(elem_type)))\n\n    # Recursive function calls like this create reference cycles.\n    # Setting the function to None clears the refcycle.\n    try:\n        return gather_map(outputs)\n    finally:\n        gather_map = None\n'"
lib/datasets/cityscapes/tools/convert_cityscapes_to_coco.py,0,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nimport argparse\nimport h5py\nimport json\nimport os\nimport scipy.misc\nimport sys\n\nimport cityscapesscripts.evaluation.instances2dict_with_polygons as cs\n\nimport utils.segms as segms_util\nimport utils.boxes as bboxs_util\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\'Convert dataset\')\n    parser.add_argument(\n        \'--dataset\', help=""cocostuff, cityscapes"", default=None, type=str)\n    parser.add_argument(\n        \'--outdir\', help=""output dir for json files"", default=None, type=str)\n    parser.add_argument(\n        \'--datadir\', help=""data dir for annotations to be converted"",\n        default=None, type=str)\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n    return parser.parse_args()\n\n\ndef convert_coco_stuff_mat(data_dir, out_dir):\n    """"""Convert to png and save json with path. This currently only contains\n    the segmentation labels for objects+stuff in cocostuff - if we need to\n    combine with other labels from original COCO that will be a TODO.""""""\n    sets = [\'train\', \'val\']\n    categories = []\n    json_name = \'coco_stuff_%s.json\'\n    ann_dict = {}\n    for data_set in sets:\n        file_list = os.path.join(data_dir, \'%s.txt\')\n        images = []\n        with open(file_list % data_set) as f:\n            for img_id, img_name in enumerate(f):\n                img_name = img_name.replace(\'coco\', \'COCO\').strip(\'\\n\')\n                image = {}\n                mat_file = os.path.join(\n                    data_dir, \'annotations/%s.mat\' % img_name)\n                data = h5py.File(mat_file, \'r\')\n                labelMap = data.get(\'S\')\n                if len(categories) == 0:\n                    labelNames = data.get(\'names\')\n                    for idx, n in enumerate(labelNames):\n                        categories.append(\n                            {""id"": idx, ""name"": \'\'.join(chr(i) for i in data[\n                                n[0]])})\n                    ann_dict[\'categories\'] = categories\n                scipy.misc.imsave(\n                    os.path.join(data_dir, img_name + \'.png\'), labelMap)\n                image[\'width\'] = labelMap.shape[0]\n                image[\'height\'] = labelMap.shape[1]\n                image[\'file_name\'] = img_name\n                image[\'seg_file_name\'] = img_name\n                image[\'id\'] = img_id\n                images.append(image)\n        ann_dict[\'images\'] = images\n        print(""Num images: %s"" % len(images))\n        with open(os.path.join(out_dir, json_name % data_set), \'wb\') as outfile:\n            outfile.write(json.dumps(ann_dict))\n\n\n# for Cityscapes\ndef getLabelID(self, instID):\n    if (instID < 1000):\n        return instID\n    else:\n        return int(instID / 1000)\n\n\ndef convert_cityscapes_instance_only(\n        data_dir, out_dir):\n    """"""Convert from cityscapes format to COCO instance seg format - polygons""""""\n    sets = [\n        \'gtFine_val\',\n        # \'gtFine_train\',\n        # \'gtFine_test\',\n\n        # \'gtCoarse_train\',\n        # \'gtCoarse_val\',\n        # \'gtCoarse_train_extra\'\n    ]\n    ann_dirs = [\n        \'gtFine_trainvaltest/gtFine/val\',\n        # \'gtFine_trainvaltest/gtFine/train\',\n        # \'gtFine_trainvaltest/gtFine/test\',\n\n        # \'gtCoarse/train\',\n        # \'gtCoarse/train_extra\',\n        # \'gtCoarse/val\'\n    ]\n    json_name = \'instancesonly_filtered_%s.json\'\n    ends_in = \'%s_polygons.json\'\n    img_id = 0\n    ann_id = 0\n    cat_id = 1\n    category_dict = {}\n\n    category_instancesonly = [\n        \'person\',\n        \'rider\',\n        \'car\',\n        \'truck\',\n        \'bus\',\n        \'train\',\n        \'motorcycle\',\n        \'bicycle\',\n    ]\n\n    for data_set, ann_dir in zip(sets, ann_dirs):\n        print(\'Starting %s\' % data_set)\n        ann_dict = {}\n        images = []\n        annotations = []\n        ann_dir = os.path.join(data_dir, ann_dir)\n        for root, _, files in os.walk(ann_dir):\n            for filename in files:\n                if filename.endswith(ends_in % data_set.split(\'_\')[0]):\n                    if len(images) % 50 == 0:\n                        print(""Processed %s images, %s annotations"" % (\n                            len(images), len(annotations)))\n                    json_ann = json.load(open(os.path.join(root, filename)))\n                    image = {}\n                    image[\'id\'] = img_id\n                    img_id += 1\n\n                    image[\'width\'] = json_ann[\'imgWidth\']\n                    image[\'height\'] = json_ann[\'imgHeight\']\n                    image[\'file_name\'] = filename[:-len(\n                        ends_in % data_set.split(\'_\')[0])] + \'leftImg8bit.png\'\n                    image[\'seg_file_name\'] = filename[:-len(\n                        ends_in % data_set.split(\'_\')[0])] + \\\n                        \'%s_instanceIds.png\' % data_set.split(\'_\')[0]\n                    images.append(image)\n\n                    fullname = os.path.join(root, image[\'seg_file_name\'])\n                    objects = cs.instances2dict_with_polygons(\n                        [fullname], verbose=False)[fullname]\n\n                    for object_cls in objects:\n                        if object_cls not in category_instancesonly:\n                            continue  # skip non-instance categories\n\n                        for obj in objects[object_cls]:\n                            if obj[\'contours\'] == []:\n                                print(\'Warning: empty contours.\')\n                                continue  # skip non-instance categories\n\n                            len_p = [len(p) for p in obj[\'contours\']]\n                            if min(len_p) <= 4:\n                                print(\'Warning: invalid contours.\')\n                                continue  # skip non-instance categories\n\n                            ann = {}\n                            ann[\'id\'] = ann_id\n                            ann_id += 1\n                            ann[\'image_id\'] = image[\'id\']\n                            ann[\'segmentation\'] = obj[\'contours\']\n\n                            if object_cls not in category_dict:\n                                category_dict[object_cls] = cat_id\n                                cat_id += 1\n                            ann[\'category_id\'] = category_dict[object_cls]\n                            ann[\'iscrowd\'] = 0\n                            ann[\'area\'] = obj[\'pixelCount\']\n                            ann[\'bbox\'] = bboxs_util.xyxy_to_xywh(\n                                segms_util.polys_to_boxes(\n                                    [ann[\'segmentation\']])).tolist()[0]\n\n                            annotations.append(ann)\n\n        ann_dict[\'images\'] = images\n        categories = [{""id"": category_dict[name], ""name"": name} for name in\n                      category_dict]\n        ann_dict[\'categories\'] = categories\n        ann_dict[\'annotations\'] = annotations\n        print(""Num categories: %s"" % len(categories))\n        print(""Num images: %s"" % len(images))\n        print(""Num annotations: %s"" % len(annotations))\n        with open(os.path.join(out_dir, json_name % data_set), \'wb\') as outfile:\n            outfile.write(json.dumps(ann_dict))\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    if args.dataset == ""cityscapes_instance_only"":\n        convert_cityscapes_instance_only(args.datadir, args.outdir)\n    elif args.dataset == ""cocostuff"":\n        convert_coco_stuff_mat(args.datadir, args.outdir)\n    else:\n        print(""Dataset not supported: %s"" % args.dataset)\n'"
lib/datasets/cityscapes/tools/convert_coco_model_to_cityscapes.py,0,"b""# Convert a detection model trained for COCO into a model that can be fine-tuned\n# on cityscapes\n#\n# cityscapes_to_coco\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nfrom __future__ import unicode_literals\n\nfrom six.moves import cPickle as pickle\nimport argparse\nimport os\nimport sys\nimport numpy as np\n\nimport datasets.cityscapes.coco_to_cityscapes_id as cs\n\nNUM_CS_CLS = 9\nNUM_COCO_CLS = 81\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Convert a COCO pre-trained model for use with Cityscapes')\n    parser.add_argument(\n        '--coco_model', dest='coco_model_file_name',\n        help='Pretrained network weights file path',\n        default=None, type=str)\n    parser.add_argument(\n        '--convert_func', dest='convert_func',\n        help='Blob conversion function',\n        default='cityscapes_to_coco', type=str)\n    parser.add_argument(\n        '--output', dest='out_file_name',\n        help='Output file path',\n        default=None, type=str)\n\n    if len(sys.argv) == 1:\n        parser.print_help()\n        sys.exit(1)\n\n    args = parser.parse_args()\n    return args\n\n\ndef convert_coco_blobs_to_cityscape_blobs(model_dict):\n    for k, v in model_dict['blobs'].items():\n        if v.shape[0] == NUM_COCO_CLS or v.shape[0] == 4 * NUM_COCO_CLS:\n            coco_blob = model_dict['blobs'][k]\n            print(\n                'Converting COCO blob {} with shape {}'.\n                format(k, coco_blob.shape)\n            )\n            cs_blob = convert_coco_blob_to_cityscapes_blob(\n                coco_blob, args.convert_func\n            )\n            print(' -> converted shape {}'.format(cs_blob.shape))\n            model_dict['blobs'][k] = cs_blob\n\n\ndef convert_coco_blob_to_cityscapes_blob(coco_blob, convert_func):\n    # coco blob (81, ...) or (81*4, ...)\n    coco_shape = coco_blob.shape\n    leading_factor = int(coco_shape[0] / NUM_COCO_CLS)\n    tail_shape = list(coco_shape[1:])\n    assert leading_factor == 1 or leading_factor == 4\n\n    # Reshape in [num_classes, ...] form for easier manipulations\n    coco_blob = coco_blob.reshape([NUM_COCO_CLS, -1] + tail_shape)\n    # Default initialization uses Gaussian with mean and std to match the\n    # existing parameters\n    std = coco_blob.std()\n    mean = coco_blob.mean()\n    cs_shape = [NUM_CS_CLS] + list(coco_blob.shape[1:])\n    cs_blob = (np.random.randn(*cs_shape) * std + mean).astype(np.float32)\n\n    # Replace random parameters with COCO parameters if class mapping exists\n    for i in range(NUM_CS_CLS):\n        coco_cls_id = getattr(cs, convert_func)(i)\n        if coco_cls_id >= 0:  # otherwise ignore (rand init)\n            cs_blob[i] = coco_blob[coco_cls_id]\n\n    cs_shape = [NUM_CS_CLS * leading_factor] + tail_shape\n    return cs_blob.reshape(cs_shape)\n\n\ndef remove_momentum(model_dict):\n    for k in model_dict['blobs'].keys():\n        if k.endswith('_momentum'):\n            del model_dict['blobs'][k]\n\n\ndef load_and_convert_coco_model(args):\n    with open(args.coco_model_file_name, 'r') as f:\n        model_dict = pickle.load(f)\n    remove_momentum(model_dict)\n    convert_coco_blobs_to_cityscape_blobs(model_dict)\n    return model_dict\n\n\nif __name__ == '__main__':\n    args = parse_args()\n    print(args)\n    assert os.path.exists(args.coco_model_file_name), \\\n        'Weights file does not exist'\n    weights = load_and_convert_coco_model(args)\n\n    with open(args.out_file_name, 'w') as f:\n        pickle.dump(weights, f, protocol=pickle.HIGHEST_PROTOCOL)\n    print('Wrote blobs to {}:'.format(args.out_file_name))\n    print(sorted(weights['blobs'].keys()))\n"""
lib/model/nms/_ext/__init__.py,0,b''
lib/model/roi_align/_ext/__init__.py,0,b''
lib/model/roi_align/functions/__init__.py,0,b''
lib/model/roi_align/functions/roi_align.py,1,"b'import torch\nfrom torch.autograd import Function\nfrom .._ext import roi_align\n\n\n# TODO use save_for_backward instead\nclass RoIAlignFunction(Function):\n    def __init__(self, aligned_height, aligned_width, spatial_scale):\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n        self.rois = None\n        self.feature_size = None\n\n    def forward(self, features, rois):\n        self.rois = rois\n        self.feature_size = features.size()\n\n        batch_size, num_channels, data_height, data_width = features.size()\n        num_rois = rois.size(0)\n\n        output = features.new(num_rois, num_channels, self.aligned_height, self.aligned_width).zero_()\n        if features.is_cuda:\n            roi_align.roi_align_forward_cuda(self.aligned_height,\n                                             self.aligned_width,\n                                             self.spatial_scale, features,\n                                             rois, output)\n        else:\n            raise NotImplementedError\n\n        return output\n\n    def backward(self, grad_output):\n        assert(self.feature_size is not None and grad_output.is_cuda)\n\n        batch_size, num_channels, data_height, data_width = self.feature_size\n\n        grad_input = self.rois.new(batch_size, num_channels, data_height,\n                                  data_width).zero_()\n        roi_align.roi_align_backward_cuda(self.aligned_height,\n                                          self.aligned_width,\n                                          self.spatial_scale, grad_output,\n                                          self.rois, grad_input)\n\n        # print grad_input\n\n        return grad_input, None\n'"
lib/model/roi_align/modules/__init__.py,0,b''
lib/model/roi_align/modules/roi_align.py,2,"b'from torch.nn.modules.module import Module\nfrom torch.nn.functional import avg_pool2d, max_pool2d\nfrom ..functions.roi_align import RoIAlignFunction\n\n\nclass RoIAlign(Module):\n    def __init__(self, aligned_height, aligned_width, spatial_scale):\n        super(RoIAlign, self).__init__()\n\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        return RoIAlignFunction(self.aligned_height, self.aligned_width,\n                                self.spatial_scale)(features, rois)\n\nclass RoIAlignAvg(Module):\n    def __init__(self, aligned_height, aligned_width, spatial_scale):\n        super(RoIAlignAvg, self).__init__()\n\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        x =  RoIAlignFunction(self.aligned_height+1, self.aligned_width+1,\n                                self.spatial_scale)(features, rois)\n        return avg_pool2d(x, kernel_size=2, stride=1)\n\nclass RoIAlignMax(Module):\n    def __init__(self, aligned_height, aligned_width, spatial_scale):\n        super(RoIAlignMax, self).__init__()\n\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        x =  RoIAlignFunction(self.aligned_height+1, self.aligned_width+1,\n                                self.spatial_scale)(features, rois)\n        return max_pool2d(x, kernel_size=2, stride=1)\n'"
lib/model/roi_crop/_ext/__init__.py,0,b''
lib/model/roi_crop/functions/__init__.py,0,b''
lib/model/roi_crop/functions/crop_resize.py,6,"b'# functions/add.py\nimport torch\nfrom torch.autograd import Function\nfrom .._ext import roi_crop\nfrom cffi import FFI\nffi = FFI()\n\nclass RoICropFunction(Function):\n    def forward(self, input1, input2):\n        self.input1 = input1\n        self.input2 = input2\n        self.device_c = ffi.new(""int *"")\n        output = torch.zeros(input2.size()[0], input1.size()[1], input2.size()[1], input2.size()[2])\n        #print(\'decice %d\' % torch.cuda.current_device())\n        if input1.is_cuda:\n            self.device = torch.cuda.current_device()\n        else:\n            self.device = -1\n        self.device_c[0] = self.device\n        if not input1.is_cuda:\n            roi_crop.BilinearSamplerBHWD_updateOutput(input1, input2, output)\n        else:\n            output = output.cuda(self.device)\n            roi_crop.BilinearSamplerBHWD_updateOutput_cuda(input1, input2, output)\n        return output\n\n    def backward(self, grad_output):\n        grad_input1 = torch.zeros(self.input1.size())\n        grad_input2 = torch.zeros(self.input2.size())\n        #print(\'backward decice %d\' % self.device)\n        if not grad_output.is_cuda:\n            roi_crop.BilinearSamplerBHWD_updateGradInput(self.input1, self.input2, grad_input1, grad_input2, grad_output)\n        else:\n            grad_input1 = grad_input1.cuda(self.device)\n            grad_input2 = grad_input2.cuda(self.device)\n            roi_crop.BilinearSamplerBHWD_updateGradInput_cuda(self.input1, self.input2, grad_input1, grad_input2, grad_output)\n        return grad_input1, grad_input2\n'"
lib/model/roi_crop/functions/gridgen.py,6,"b'# functions/add.py\nimport torch\nfrom torch.autograd import Function\nimport numpy as np\n\n\nclass AffineGridGenFunction(Function):\n    def __init__(self, height, width,lr=1):\n        super(AffineGridGenFunction, self).__init__()\n        self.lr = lr\n        self.height, self.width = height, width\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/(self.height)), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/(self.width)), 0), repeats = self.height, axis = 0), 0)\n        # self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/(self.height - 1)), 0), repeats = self.width, axis = 0).T, 0)\n        # self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/(self.width - 1)), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n        #print(self.grid)\n\n    def forward(self, input1):\n        self.input1 = input1\n        output = input1.new(torch.Size([input1.size(0)]) + self.grid.size()).zero_()\n        self.batchgrid = input1.new(torch.Size([input1.size(0)]) + self.grid.size()).zero_()\n        for i in range(input1.size(0)):\n            self.batchgrid[i] = self.grid.astype(self.batchgrid[i])\n\n        # if input1.is_cuda:\n        #    self.batchgrid = self.batchgrid.cuda()\n        #    output = output.cuda()\n\n        for i in range(input1.size(0)):\n            output = torch.bmm(self.batchgrid.view(-1, self.height*self.width, 3), torch.transpose(input1, 1, 2)).view(-1, self.height, self.width, 2)\n\n        return output\n\n    def backward(self, grad_output):\n\n        grad_input1 = self.input1.new(self.input1.size()).zero_()\n\n        # if grad_output.is_cuda:\n        #    self.batchgrid = self.batchgrid.cuda()\n        #    grad_input1 = grad_input1.cuda()\n\n        grad_input1 = torch.baddbmm(grad_input1, torch.transpose(grad_output.view(-1, self.height*self.width, 2), 1,2), self.batchgrid.view(-1, self.height*self.width, 3))\n        return grad_input1\n'"
lib/model/roi_crop/functions/roi_crop.py,1,"b'# functions/add.py\nimport torch\nfrom torch.autograd import Function\nfrom .._ext import roi_crop\nimport pdb\n\nclass RoICropFunction(Function):\n    def forward(self, input1, input2):\n        self.input1 = input1.clone()\n        self.input2 = input2.clone()\n        output = input2.new(input2.size()[0], input1.size()[1], input2.size()[1], input2.size()[2]).zero_()\n        assert output.get_device() == input1.get_device(), ""output and input1 must on the same device""\n        assert output.get_device() == input2.get_device(), ""output and input2 must on the same device""\n        roi_crop.BilinearSamplerBHWD_updateOutput_cuda(input1, input2, output)\n        return output\n\n    def backward(self, grad_output):\n        grad_input1 = self.input1.new(self.input1.size()).zero_()\n        grad_input2 = self.input2.new(self.input2.size()).zero_()\n        roi_crop.BilinearSamplerBHWD_updateGradInput_cuda(self.input1, self.input2, grad_input1, grad_input2, grad_output)\n        return grad_input1, grad_input2\n'"
lib/model/roi_crop/modules/__init__.py,0,b''
lib/model/roi_crop/modules/gridgen.py,80,"b'from torch.nn.modules.module import Module\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nfrom ..functions.gridgen import AffineGridGenFunction\n\nimport pyximport\npyximport.install(setup_args={""include_dirs"":np.get_include()},\n                  reload_support=True)\n\n\nclass _AffineGridGen(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(_AffineGridGen, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.f = AffineGridGenFunction(self.height, self.width, lr=lr)\n        self.lr = lr\n    def forward(self, input):\n        # if not self.aux_loss:\n        return self.f(input)\n        # else:\n        #     identity = torch.from_numpy(np.array([[1,0,0], [0,1,0]], dtype=np.float32))\n        #     batch_identity = torch.zeros([input.size(0), 2,3])\n        #     for i in range(input.size(0)):\n        #         batch_identity[i] = identity\n        #     batch_identity = Variable(batch_identity)\n        #     loss = torch.mul(input - batch_identity, input - batch_identity)\n        #     loss = torch.sum(loss,1)\n        #     loss = torch.sum(loss,2)\n\n        #       return self.f(input), loss.view(-1,1)\n\n\n# class CylinderGridGen(Module):\n#     def __init__(self, height, width, lr = 1, aux_loss = False):\n#         super(CylinderGridGen, self).__init__()\n#         self.height, self.width = height, width\n#         self.aux_loss = aux_loss\n#         self.f = CylinderGridGenFunction(self.height, self.width, lr=lr)\n#         self.lr = lr\n#     def forward(self, input):\n\n#         if not self.aux_loss:\n#             return self.f(input)\n#         else:\n#             return self.f(input), torch.mul(input, input).view(-1,1)\n\n\nclass AffineGridGenV2(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(AffineGridGenV2, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n\n    def forward(self, input1):\n        self.batchgrid = torch.zeros(torch.Size([input1.size(0)]) + self.grid.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid[i] = self.grid\n        self.batchgrid = Variable(self.batchgrid)\n\n        if input1.is_cuda:\n            self.batchgrid = self.batchgrid.cuda()\n\n        output = torch.bmm(self.batchgrid.view(-1, self.height*self.width, 3), torch.transpose(input1, 1, 2)).view(-1, self.height, self.width, 2)\n\n        return output\n\n\nclass CylinderGridGenV2(Module):\n    def __init__(self, height, width, lr = 1):\n        super(CylinderGridGenV2, self).__init__()\n        self.height, self.width = height, width\n        self.lr = lr\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n    def forward(self, input):\n        self.batchgrid = torch.zeros(torch.Size([input.size(0)]) + self.grid.size() )\n        #print(self.batchgrid.size())\n        for i in range(input.size(0)):\n            self.batchgrid[i,:,:,:] = self.grid\n        self.batchgrid = Variable(self.batchgrid)\n\n        #print(self.batchgrid.size())\n\n        input_u = input.view(-1,1,1,1).repeat(1,self.height, self.width,1)\n        #print(input_u.requires_grad, self.batchgrid)\n\n        output0 = self.batchgrid[:,:,:,0:1]\n        output1 = torch.atan(torch.tan(np.pi/2.0*(self.batchgrid[:,:,:,1:2] + self.batchgrid[:,:,:,2:] * input_u[:,:,:,:])))  /(np.pi/2)\n        #print(output0.size(), output1.size())\n\n        output = torch.cat([output0, output1], 3)\n        return output\n\n\nclass DenseAffineGridGen(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(DenseAffineGridGen, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n\n    def forward(self, input1):\n        self.batchgrid = torch.zeros(torch.Size([input1.size(0)]) + self.grid.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid[i] = self.grid\n\n        self.batchgrid = Variable(self.batchgrid)\n        #print self.batchgrid,  input1[:,:,:,0:3]\n        #print self.batchgrid,  input1[:,:,:,4:6]\n        x = torch.mul(self.batchgrid, input1[:,:,:,0:3])\n        y = torch.mul(self.batchgrid, input1[:,:,:,3:6])\n\n        output = torch.cat([torch.sum(x,3),torch.sum(y,3)], 3)\n        return output\n\n\n\n\nclass DenseAffine3DGridGen(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(DenseAffine3DGridGen, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n        self.theta = self.grid[:,:,0] * np.pi/2 + np.pi/2\n        self.phi = self.grid[:,:,1] * np.pi\n\n        self.x = torch.sin(self.theta) * torch.cos(self.phi)\n        self.y = torch.sin(self.theta) * torch.sin(self.phi)\n        self.z = torch.cos(self.theta)\n\n        self.grid3d = torch.from_numpy(np.zeros( [self.height, self.width, 4], dtype=np.float32))\n\n        self.grid3d[:,:,0] = self.x\n        self.grid3d[:,:,1] = self.y\n        self.grid3d[:,:,2] = self.z\n        self.grid3d[:,:,3] = self.grid[:,:,2]\n\n\n    def forward(self, input1):\n        self.batchgrid3d = torch.zeros(torch.Size([input1.size(0)]) + self.grid3d.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid3d[i] = self.grid3d\n\n        self.batchgrid3d = Variable(self.batchgrid3d)\n        #print(self.batchgrid3d)\n\n        x = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,0:4]), 3)\n        y = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,4:8]), 3)\n        z = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,8:]), 3)\n        #print(x)\n        r = torch.sqrt(x**2 + y**2 + z**2) + 1e-5\n\n        #print(r)\n        theta = torch.acos(z/r)/(np.pi/2)  - 1\n        #phi = torch.atan(y/x)\n        phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.FloatTensor) * (y.ge(0).type(torch.FloatTensor) - y.lt(0).type(torch.FloatTensor))\n        phi = phi/np.pi\n\n\n        output = torch.cat([theta,phi], 3)\n\n        return output\n\n\n\n\n\nclass DenseAffine3DGridGen_rotate(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(DenseAffine3DGridGen_rotate, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n        self.theta = self.grid[:,:,0] * np.pi/2 + np.pi/2\n        self.phi = self.grid[:,:,1] * np.pi\n\n        self.x = torch.sin(self.theta) * torch.cos(self.phi)\n        self.y = torch.sin(self.theta) * torch.sin(self.phi)\n        self.z = torch.cos(self.theta)\n\n        self.grid3d = torch.from_numpy(np.zeros( [self.height, self.width, 4], dtype=np.float32))\n\n        self.grid3d[:,:,0] = self.x\n        self.grid3d[:,:,1] = self.y\n        self.grid3d[:,:,2] = self.z\n        self.grid3d[:,:,3] = self.grid[:,:,2]\n\n\n    def forward(self, input1, input2):\n        self.batchgrid3d = torch.zeros(torch.Size([input1.size(0)]) + self.grid3d.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid3d[i] = self.grid3d\n\n        self.batchgrid3d = Variable(self.batchgrid3d)\n\n        self.batchgrid = torch.zeros(torch.Size([input1.size(0)]) + self.grid.size())\n\n        for i in range(input1.size(0)):\n            self.batchgrid[i] = self.grid\n\n        self.batchgrid = Variable(self.batchgrid)\n\n        #print(self.batchgrid3d)\n\n        x = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,0:4]), 3)\n        y = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,4:8]), 3)\n        z = torch.sum(torch.mul(self.batchgrid3d, input1[:,:,:,8:]), 3)\n        #print(x)\n        r = torch.sqrt(x**2 + y**2 + z**2) + 1e-5\n\n        #print(r)\n        theta = torch.acos(z/r)/(np.pi/2)  - 1\n        #phi = torch.atan(y/x)\n        phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.FloatTensor) * (y.ge(0).type(torch.FloatTensor) - y.lt(0).type(torch.FloatTensor))\n        phi = phi/np.pi\n\n        input_u = input2.view(-1,1,1,1).repeat(1,self.height, self.width,1)\n\n        output = torch.cat([theta,phi], 3)\n\n        output1 = torch.atan(torch.tan(np.pi/2.0*(output[:,:,:,1:2] + self.batchgrid[:,:,:,2:] * input_u[:,:,:,:])))  /(np.pi/2)\n        output2 = torch.cat([output[:,:,:,0:1], output1], 3)\n\n        return output2\n\n\nclass Depth3DGridGen(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False):\n        super(Depth3DGridGen, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n        self.theta = self.grid[:,:,0] * np.pi/2 + np.pi/2\n        self.phi = self.grid[:,:,1] * np.pi\n\n        self.x = torch.sin(self.theta) * torch.cos(self.phi)\n        self.y = torch.sin(self.theta) * torch.sin(self.phi)\n        self.z = torch.cos(self.theta)\n\n        self.grid3d = torch.from_numpy(np.zeros( [self.height, self.width, 4], dtype=np.float32))\n\n        self.grid3d[:,:,0] = self.x\n        self.grid3d[:,:,1] = self.y\n        self.grid3d[:,:,2] = self.z\n        self.grid3d[:,:,3] = self.grid[:,:,2]\n\n\n    def forward(self, depth, trans0, trans1, rotate):\n        self.batchgrid3d = torch.zeros(torch.Size([depth.size(0)]) + self.grid3d.size())\n\n        for i in range(depth.size(0)):\n            self.batchgrid3d[i] = self.grid3d\n\n        self.batchgrid3d = Variable(self.batchgrid3d)\n\n        self.batchgrid = torch.zeros(torch.Size([depth.size(0)]) + self.grid.size())\n\n        for i in range(depth.size(0)):\n            self.batchgrid[i] = self.grid\n\n        self.batchgrid = Variable(self.batchgrid)\n\n        x = self.batchgrid3d[:,:,:,0:1] * depth + trans0.view(-1,1,1,1).repeat(1, self.height, self.width, 1)\n\n        y = self.batchgrid3d[:,:,:,1:2] * depth + trans1.view(-1,1,1,1).repeat(1, self.height, self.width, 1)\n        z = self.batchgrid3d[:,:,:,2:3] * depth\n        #print(x.size(), y.size(), z.size())\n        r = torch.sqrt(x**2 + y**2 + z**2) + 1e-5\n\n        #print(r)\n        theta = torch.acos(z/r)/(np.pi/2)  - 1\n        #phi = torch.atan(y/x)\n        phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.FloatTensor) * (y.ge(0).type(torch.FloatTensor) - y.lt(0).type(torch.FloatTensor))\n        phi = phi/np.pi\n\n        #print(theta.size(), phi.size())\n\n\n        input_u = rotate.view(-1,1,1,1).repeat(1,self.height, self.width,1)\n\n        output = torch.cat([theta,phi], 3)\n        #print(output.size())\n\n        output1 = torch.atan(torch.tan(np.pi/2.0*(output[:,:,:,1:2] + self.batchgrid[:,:,:,2:] * input_u[:,:,:,:])))  /(np.pi/2)\n        output2 = torch.cat([output[:,:,:,0:1], output1], 3)\n\n        return output2\n\n\n\n\n\nclass Depth3DGridGen_with_mask(Module):\n    def __init__(self, height, width, lr = 1, aux_loss = False, ray_tracing = False):\n        super(Depth3DGridGen_with_mask, self).__init__()\n        self.height, self.width = height, width\n        self.aux_loss = aux_loss\n        self.lr = lr\n        self.ray_tracing = ray_tracing\n\n        self.grid = np.zeros( [self.height, self.width, 3], dtype=np.float32)\n        self.grid[:,:,0] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.height), 0), repeats = self.width, axis = 0).T, 0)\n        self.grid[:,:,1] = np.expand_dims(np.repeat(np.expand_dims(np.arange(-1, 1, 2.0/self.width), 0), repeats = self.height, axis = 0), 0)\n        self.grid[:,:,2] = np.ones([self.height, width])\n        self.grid = torch.from_numpy(self.grid.astype(np.float32))\n\n        self.theta = self.grid[:,:,0] * np.pi/2 + np.pi/2\n        self.phi = self.grid[:,:,1] * np.pi\n\n        self.x = torch.sin(self.theta) * torch.cos(self.phi)\n        self.y = torch.sin(self.theta) * torch.sin(self.phi)\n        self.z = torch.cos(self.theta)\n\n        self.grid3d = torch.from_numpy(np.zeros( [self.height, self.width, 4], dtype=np.float32))\n\n        self.grid3d[:,:,0] = self.x\n        self.grid3d[:,:,1] = self.y\n        self.grid3d[:,:,2] = self.z\n        self.grid3d[:,:,3] = self.grid[:,:,2]\n\n\n    def forward(self, depth, trans0, trans1, rotate):\n        self.batchgrid3d = torch.zeros(torch.Size([depth.size(0)]) + self.grid3d.size())\n\n        for i in range(depth.size(0)):\n            self.batchgrid3d[i] = self.grid3d\n\n        self.batchgrid3d = Variable(self.batchgrid3d)\n\n        self.batchgrid = torch.zeros(torch.Size([depth.size(0)]) + self.grid.size())\n\n        for i in range(depth.size(0)):\n            self.batchgrid[i] = self.grid\n\n        self.batchgrid = Variable(self.batchgrid)\n\n        if depth.is_cuda:\n            self.batchgrid = self.batchgrid.cuda()\n            self.batchgrid3d = self.batchgrid3d.cuda()\n\n\n        x_ = self.batchgrid3d[:,:,:,0:1] * depth + trans0.view(-1,1,1,1).repeat(1, self.height, self.width, 1)\n\n        y_ = self.batchgrid3d[:,:,:,1:2] * depth + trans1.view(-1,1,1,1).repeat(1, self.height, self.width, 1)\n        z = self.batchgrid3d[:,:,:,2:3] * depth\n        #print(x.size(), y.size(), z.size())\n\n        rotate_z = rotate.view(-1,1,1,1).repeat(1,self.height, self.width,1) * np.pi\n\n        x = x_ * torch.cos(rotate_z) - y_ * torch.sin(rotate_z)\n        y = x_ * torch.sin(rotate_z) + y_ * torch.cos(rotate_z)\n\n\n        r = torch.sqrt(x**2 + y**2 + z**2) + 1e-5\n\n        #print(r)\n        theta = torch.acos(z/r)/(np.pi/2)  - 1\n        #phi = torch.atan(y/x)\n\n        if depth.is_cuda:\n            phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.cuda.FloatTensor) * (y.ge(0).type(torch.cuda.FloatTensor) - y.lt(0).type(torch.cuda.FloatTensor))\n        else:\n            phi = torch.atan(y/(x + 1e-5))  + np.pi * x.lt(0).type(torch.FloatTensor) * (y.ge(0).type(torch.FloatTensor) - y.lt(0).type(torch.FloatTensor))\n\n\n        phi = phi/np.pi\n\n        output = torch.cat([theta,phi], 3)\n        return output\n'"
lib/model/roi_crop/modules/roi_crop.py,1,"b""from torch.nn.modules.module import Module\nfrom ..functions.roi_crop import RoICropFunction\n\nclass _RoICrop(Module):\n    def __init__(self, layout = 'BHWD'):\n        super(_RoICrop, self).__init__()\n    def forward(self, input1, input2):\n        return RoICropFunction()(input1, input2)\n"""
lib/model/roi_pooling/_ext/__init__.py,0,b''
lib/model/roi_pooling/functions/__init__.py,0,b''
lib/model/roi_pooling/functions/roi_pool.py,1,"b'import torch\nfrom torch.autograd import Function\nfrom .._ext import roi_pooling\nimport pdb\n\nclass RoIPoolFunction(Function):\n    def __init__(ctx, pooled_height, pooled_width, spatial_scale):\n        ctx.pooled_width = pooled_width\n        ctx.pooled_height = pooled_height\n        ctx.spatial_scale = spatial_scale\n        ctx.feature_size = None\n\n    def forward(ctx, features, rois): \n        ctx.feature_size = features.size()           \n        batch_size, num_channels, data_height, data_width = ctx.feature_size\n        num_rois = rois.size(0)\n        output = features.new(num_rois, num_channels, ctx.pooled_height, ctx.pooled_width).zero_()\n        ctx.argmax = features.new(num_rois, num_channels, ctx.pooled_height, ctx.pooled_width).zero_().int()\n        ctx.rois = rois\n        if not features.is_cuda:\n            _features = features.permute(0, 2, 3, 1)\n            roi_pooling.roi_pooling_forward(ctx.pooled_height, ctx.pooled_width, ctx.spatial_scale,\n                                            _features, rois, output)\n        else:\n            roi_pooling.roi_pooling_forward_cuda(ctx.pooled_height, ctx.pooled_width, ctx.spatial_scale,\n                                                 features, rois, output, ctx.argmax)\n\n        return output\n\n    def backward(ctx, grad_output):\n        assert(ctx.feature_size is not None and grad_output.is_cuda)\n        batch_size, num_channels, data_height, data_width = ctx.feature_size\n        grad_input = grad_output.new(batch_size, num_channels, data_height, data_width).zero_()\n\n        roi_pooling.roi_pooling_backward_cuda(ctx.pooled_height, ctx.pooled_width, ctx.spatial_scale,\n                                              grad_output, ctx.rois, grad_input, ctx.argmax)\n\n        return grad_input, None\n'"
lib/model/roi_pooling/modules/__init__.py,0,b''
lib/model/roi_pooling/modules/roi_pool.py,1,"b'from torch.nn.modules.module import Module\nfrom ..functions.roi_pool import RoIPoolFunction\n\n\nclass _RoIPooling(Module):\n    def __init__(self, pooled_height, pooled_width, spatial_scale):\n        super(_RoIPooling, self).__init__()\n\n        self.pooled_width = int(pooled_width)\n        self.pooled_height = int(pooled_height)\n        self.spatial_scale = float(spatial_scale)\n\n    def forward(self, features, rois):\n        return RoIPoolFunction(self.pooled_height, self.pooled_width, self.spatial_scale)(features, rois)\n'"
lib/modeling/roi_xfrom/roi_align/__init__.py,0,b''
lib/modeling/roi_xfrom/roi_align/build.py,2,"b""from __future__ import print_function\nimport os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n# sources = ['src/roi_align.c']\n# headers = ['src/roi_align.h']\nsources = []\nheaders = []\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/roi_align_cuda.c']\n    headers += ['src/roi_align_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/roi_align_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.roi_align',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
lib/model/nms/_ext/nms/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._nms import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
lib/model/roi_align/_ext/roi_align/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._roi_align import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
lib/model/roi_crop/_ext/crop_resize/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._crop_resize import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        locals[symbol] = _wrap_function(fn, _ffi)\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
lib/model/roi_crop/_ext/roi_crop/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._roi_crop import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
lib/model/roi_pooling/_ext/roi_pooling/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._roi_pooling import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
lib/modeling/roi_xfrom/roi_align/_ext/__init__.py,0,b''
lib/modeling/roi_xfrom/roi_align/functions/__init__.py,0,b''
lib/modeling/roi_xfrom/roi_align/functions/roi_align.py,1,"b'import torch\nfrom torch.autograd import Function\nfrom .._ext import roi_align\n\n\n# TODO use save_for_backward instead\nclass RoIAlignFunction(Function):\n    def __init__(self, aligned_height, aligned_width, spatial_scale, sampling_ratio):\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n        self.sampling_ratio = int(sampling_ratio)\n        self.rois = None\n        self.feature_size = None\n\n    def forward(self, features, rois):\n        self.rois = rois\n        self.feature_size = features.size()\n\n        batch_size, num_channels, data_height, data_width = features.size()\n        num_rois = rois.size(0)\n\n        output = features.new(num_rois, num_channels, self.aligned_height, self.aligned_width).zero_()\n        if features.is_cuda:\n            roi_align.roi_align_forward_cuda(self.aligned_height,\n                                             self.aligned_width,\n                                             self.spatial_scale, self.sampling_ratio, features,\n                                             rois, output)\n        else:\n            raise NotImplementedError\n\n        return output\n\n    def backward(self, grad_output):\n        assert(self.feature_size is not None and grad_output.is_cuda)\n\n        batch_size, num_channels, data_height, data_width = self.feature_size\n\n        grad_input = self.rois.new(batch_size, num_channels, data_height,\n                                  data_width).zero_()\n        roi_align.roi_align_backward_cuda(self.aligned_height,\n                                          self.aligned_width,\n                                          self.spatial_scale, self.sampling_ratio, grad_output,\n                                          self.rois, grad_input)\n\n        # print grad_input\n\n        return grad_input, None\n'"
lib/modeling/roi_xfrom/roi_align/modules/__init__.py,0,b''
lib/modeling/roi_xfrom/roi_align/modules/roi_align.py,2,"b'from torch.nn.modules.module import Module\nfrom torch.nn.functional import avg_pool2d, max_pool2d\nfrom ..functions.roi_align import RoIAlignFunction\n\n\nclass RoIAlign(Module):\n    def __init__(self, aligned_height, aligned_width, spatial_scale, sampling_ratio):\n        super(RoIAlign, self).__init__()\n\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n        self.sampling_ratio = int(sampling_ratio)\n\n    def forward(self, features, rois):\n        return RoIAlignFunction(self.aligned_height, self.aligned_width,\n                                self.spatial_scale, self.sampling_ratio)(features, rois)\n\nclass RoIAlignAvg(Module):\n    def __init__(self, aligned_height, aligned_width, spatial_scale, sampling_ratio):\n        super(RoIAlignAvg, self).__init__()\n\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n        self.sampling_ratio = int(sampling_ratio)\n\n    def forward(self, features, rois):\n        x =  RoIAlignFunction(self.aligned_height+1, self.aligned_width+1,\n                                self.spatial_scale, self.sampling_ratio)(features, rois)\n        return avg_pool2d(x, kernel_size=2, stride=1)\n\nclass RoIAlignMax(Module):\n    def __init__(self, aligned_height, aligned_width, spatial_scale, sampling_ratio):\n        super(RoIAlignMax, self).__init__()\n\n        self.aligned_width = int(aligned_width)\n        self.aligned_height = int(aligned_height)\n        self.spatial_scale = float(spatial_scale)\n        self.sampling_ratio = int(sampling_ratio)\n\n    def forward(self, features, rois):\n        x =  RoIAlignFunction(self.aligned_height+1, self.aligned_width+1,\n                                self.spatial_scale, self.sampling_ratio)(features, rois)\n        return max_pool2d(x, kernel_size=2, stride=1)\n'"
lib/modeling/roi_xfrom/roi_align/_ext/roi_align/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._roi_align import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
