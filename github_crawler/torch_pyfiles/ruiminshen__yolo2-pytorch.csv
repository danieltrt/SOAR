file_path,api_count,code
benchmark_caffe2.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport yaml\n\nimport onnx_caffe2.helper\n\nimport utils\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    model_dir = utils.get_model_dir(config)\n    init_net = onnx_caffe2.helper.load_caffe2_net(os.path.join(model_dir, \'init_net.pb\'))\n    predict_net = onnx_caffe2.helper.load_caffe2_net(os.path.join(model_dir, \'predict_net.pb\'))\n    benchmark = onnx_caffe2.helper.benchmark_caffe2_model(init_net, predict_net)\n    logging.info(\'benchmark=%f(milliseconds)\' % benchmark)\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'-b\', \'--benchmark\', action=\'store_true\')\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
cache.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport importlib\nimport pickle\nimport random\nimport shutil\nimport yaml\n\nimport utils\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    cache_dir = utils.get_cache_dir(config)\n    os.makedirs(cache_dir, exist_ok=True)\n    shutil.copyfile(os.path.expanduser(os.path.expandvars(config.get(\'cache\', \'category\'))), os.path.join(cache_dir, \'category\'))\n    category = utils.get_category(config)\n    category_index = dict([(name, i) for i, name in enumerate(category)])\n    datasets = config.get(\'cache\', \'datasets\').split()\n    for phase in args.phase:\n        path = os.path.join(cache_dir, phase) + \'.pkl\'\n        logging.info(\'save cache file: \' + path)\n        data = []\n        for dataset in datasets:\n            logging.info(\'load %s dataset\' % dataset)\n            module, func = dataset.rsplit(\'.\', 1)\n            module = importlib.import_module(module)\n            func = getattr(module, func)\n            data += func(config, path, category_index)\n        if config.getboolean(\'cache\', \'shuffle\'):\n            random.shuffle(data)\n        with open(path, \'wb\') as f:\n            pickle.dump(data, f)\n    logging.info(\'%s data are saved into %s\' % (str(args.phase), cache_dir))\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'-p\', \'--phase\', nargs=\'+\', default=[\'train\', \'val\', \'test\'])\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
checksum_caffe2.py,2,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport hashlib\nimport yaml\n\nimport torch\nfrom caffe2.proto import caffe2_pb2\nfrom caffe2.python import workspace\n\nimport utils\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    torch.manual_seed(args.seed)\n    model_dir = utils.get_model_dir(config)\n    init_net = caffe2_pb2.NetDef()\n    with open(os.path.join(model_dir, \'init_net.pb\'), \'rb\') as f:\n        init_net.ParseFromString(f.read())\n    predict_net = caffe2_pb2.NetDef()\n    with open(os.path.join(model_dir, \'predict_net.pb\'), \'rb\') as f:\n        predict_net.ParseFromString(f.read())\n    p = workspace.Predictor(init_net, predict_net)\n    height, width = tuple(map(int, config.get(\'image\', \'size\').split()))\n    tensor = torch.randn(1, 3, height, width)\n    # Checksum\n    output = p.run([tensor.numpy()])\n    for key, a in [\n        (\'tensor\', tensor.cpu().numpy()),\n        (\'output\', output[0]),\n    ]:\n        print(\'\\t\'.join(map(str, [key, a.shape, utils.abs_mean(a), hashlib.md5(a.tostring()).hexdigest()])))\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    parser.add_argument(\'-s\', \'--seed\', default=0, type=int, help=\'a seed to create a random image tensor\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
checksum_torch.py,6,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport hashlib\nimport yaml\n\nimport torch\nimport torch.autograd\nimport cv2\n\nimport utils\nimport utils.train\nimport model\nimport transform\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    torch.manual_seed(args.seed)\n    cache_dir = utils.get_cache_dir(config)\n    model_dir = utils.get_model_dir(config)\n    category = utils.get_category(config, cache_dir if os.path.exists(cache_dir) else None)\n    anchors = utils.get_anchors(config)\n    anchors = torch.from_numpy(anchors).contiguous()\n    path, step, epoch = utils.train.load_model(model_dir)\n    state_dict = torch.load(path, map_location=lambda storage, loc: storage)\n    dnn = utils.parse_attr(config.get(\'model\', \'dnn\'))(model.ConfigChannels(config, state_dict), anchors, len(category))\n    dnn.load_state_dict(state_dict)\n    height, width = tuple(map(int, config.get(\'image\', \'size\').split()))\n    tensor = torch.randn(1, 3, height, width)\n    # Checksum\n    for key, var in dnn.state_dict().items():\n        a = var.cpu().numpy()\n        print(\'\\t\'.join(map(str, [key, a.shape, utils.abs_mean(a), hashlib.md5(a.tostring()).hexdigest()])))\n    output = dnn(torch.autograd.Variable(tensor, volatile=True)).data\n    for key, a in [\n        (\'tensor\', tensor.cpu().numpy()),\n        (\'output\', output.cpu().numpy()),\n    ]:\n        print(\'\\t\'.join(map(str, [key, a.shape, utils.abs_mean(a), hashlib.md5(a.tostring()).hexdigest()])))\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    parser.add_argument(\'-s\', \'--seed\', default=0, type=int, help=\'a seed to create a random image tensor\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
convert_darknet_torch.py,4,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport struct\nimport collections\nimport shutil\nimport hashlib\nimport yaml\n\nimport numpy as np\nimport torch\nimport humanize\n\nimport model\nimport utils.train\n\n\ndef transpose_weight(weight, num_anchors):\n    _, channels_in, ksize1, ksize2 = weight.size()\n    weight = weight.view(num_anchors, -1, channels_in, ksize1, ksize2)\n    x = weight[:, 0:1, :, :, :]\n    y = weight[:, 1:2, :, :, :]\n    w = weight[:, 2:3, :, :, :]\n    h = weight[:, 3:4, :, :, :]\n    iou = weight[:, 4:5, :, :, :]\n    cls = weight[:, 5:, :, :, :]\n    return torch.cat([iou, y, x, h, w, cls], 1).view(-1, channels_in, ksize1, ksize2)\n\n\ndef transpose_bias(bias, num_anchors):\n    bias = bias.view([num_anchors, -1])\n    x = bias[:, 0:1]\n    y = bias[:, 1:2]\n    w = bias[:, 2:3]\n    h = bias[:, 3:4]\n    iou = bias[:, 4:5]\n    cls = bias[:, 5:]\n    return torch.cat([iou, y, x, h, w, cls], 1).view(-1)\n\n\ndef group_state(state_dict):\n    grouped_dict = collections.OrderedDict()\n    for key, var in state_dict.items():\n        layer, suffix1, suffix2 = key.rsplit(\'.\', 2)\n        suffix = suffix1 + \'.\' + suffix2\n        if layer in grouped_dict:\n            grouped_dict[layer][suffix] = var\n        else:\n            grouped_dict[layer] = {suffix: var}\n    return grouped_dict\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    cache_dir = utils.get_cache_dir(config)\n    model_dir = utils.get_model_dir(config)\n    category = utils.get_category(config, cache_dir if os.path.exists(cache_dir) else None)\n    anchors = utils.get_anchors(config)\n    anchors = torch.from_numpy(anchors).contiguous()\n    dnn = utils.parse_attr(config.get(\'model\', \'dnn\'))(model.ConfigChannels(config), anchors, len(category))\n    dnn.eval()\n    logging.info(humanize.naturalsize(sum(var.cpu().numpy().nbytes for var in dnn.state_dict().values())))\n    state_dict = dnn.state_dict()\n    grouped_dict = group_state(state_dict)\n    try:\n        layers = []\n        with open(os.path.expanduser(os.path.expandvars(args.file)), \'rb\') as f:\n            major, minor, revision, seen = struct.unpack(\'4i\', f.read(16))\n            logging.info(\'major=%d, minor=%d, revision=%d, seen=%d\' % (major, minor, revision, seen))\n            total = 0\n            filesize = os.fstat(f.fileno()).st_size\n            for layer in grouped_dict:\n                group = grouped_dict[layer]\n                for suffix in [\'conv.bias\', \'bn.bias\', \'bn.weight\', \'bn.running_mean\', \'bn.running_var\', \'conv.weight\']:\n                    if suffix in group:\n                        var = group[suffix]\n                        size = var.size()\n                        cnt = np.multiply.reduce(size)\n                        total += cnt\n                        key = layer + \'.\' + suffix\n                        val = np.array(struct.unpack(\'%df\' % cnt, f.read(cnt * 4)), np.float32)\n                        val = np.reshape(val, size)\n                        remaining = filesize - f.tell()\n                        logging.info(\'%s.%s: %s=%f (%s), remaining=%d\' % (layer, suffix, \'x\'.join(list(map(str, size))), utils.abs_mean(val), hashlib.md5(val.tostring()).hexdigest(), remaining))\n                        layers.append([key, torch.from_numpy(val)])\n                logging.info(\'%d parameters assigned\' % total)\n        layers[-1][1] = transpose_weight(layers[-1][1], len(anchors))\n        layers[-2][1] = transpose_bias(layers[-2][1], len(anchors))\n    finally:\n        if remaining > 0:\n            logging.warning(\'%d bytes remaining\' % remaining)\n        state_dict = collections.OrderedDict(layers)\n        if args.delete:\n            logging.warning(\'delete model directory: \' + model_dir)\n            shutil.rmtree(model_dir, ignore_errors=True)\n        saver = utils.train.Saver(model_dir, config.getint(\'save\', \'keep\'), logger=None)\n        path = saver(state_dict, 0, 0) + saver.ext\n        if args.copy is not None:\n            _path = os.path.expandvars(os.path.expanduser(args.copy))\n            logging.info(\'copy %s to %s\' % (path, _path))\n            shutil.copy(path, _path)\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'file\', help=\'Darknet .weights file\')\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'-d\', \'--delete\', action=\'store_true\', help=\'delete logdir\')\n    parser.add_argument(\'--copy\', help=\'copy model\')\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
convert_onnx_caffe2.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport yaml\n\nimport onnx\nimport onnx_caffe2.backend\nimport onnx_caffe2.helper\n\nimport utils\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    model_dir = utils.get_model_dir(config)\n    model = onnx.load(model_dir + \'.onnx\')\n    onnx.checker.check_model(model)\n    init_net, predict_net = onnx_caffe2.backend.Caffe2Backend.onnx_graph_to_caffe2_net(model.graph, device=\'CPU\')\n    onnx_caffe2.helper.save_caffe2_net(init_net, os.path.join(model_dir, \'init_net.pb\'))\n    onnx_caffe2.helper.save_caffe2_net(predict_net, os.path.join(model_dir, \'predict_net.pb\'), output_txt=True)\n    logging.info(model_dir)\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
convert_torch_onnx.py,9,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport yaml\n\nimport torch.autograd\nimport torch.cuda\nimport torch.optim\nimport torch.utils.data\nimport torch.onnx\nimport humanize\n\nimport utils.train\nimport model\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    height, width = tuple(map(int, config.get(\'image\', \'size\').split()))\n    cache_dir = utils.get_cache_dir(config)\n    model_dir = utils.get_model_dir(config)\n    category = utils.get_category(config, cache_dir if os.path.exists(cache_dir) else None)\n    anchors = utils.get_anchors(config)\n    anchors = torch.from_numpy(anchors).contiguous()\n    path, step, epoch = utils.train.load_model(model_dir)\n    state_dict = torch.load(path, map_location=lambda storage, loc: storage)\n    dnn = utils.parse_attr(config.get(\'model\', \'dnn\'))(model.ConfigChannels(config, state_dict), anchors, len(category))\n    inference = model.Inference(config, dnn, anchors)\n    inference.eval()\n    logging.info(humanize.naturalsize(sum(var.cpu().numpy().nbytes for var in inference.state_dict().values())))\n    dnn.load_state_dict(state_dict)\n    image = torch.autograd.Variable(torch.randn(args.batch_size, 3, height, width), volatile=True)\n    path = model_dir + \'.onnx\'\n    logging.info(\'save \' + path)\n    torch.onnx.export(dnn, image, path, export_params=True, verbose=args.verbose) # PyTorch\'s bug\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'-b\', \'--batch_size\', default=1, type=int, help=\'batch size\')\n    parser.add_argument(\'-v\', \'--verbose\', action=\'store_true\')\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
demo_data.py,2,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport multiprocessing\nimport yaml\n\nimport numpy as np\nimport torch.utils.data\nimport matplotlib.pyplot as plt\n\nimport utils.data\nimport utils.train\nimport utils.visualize\nimport transform.augmentation\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    cache_dir = utils.get_cache_dir(config)\n    category = utils.get_category(config, cache_dir)\n    draw_bbox = utils.visualize.DrawBBox(category)\n    batch_size = args.rows * args.cols\n    paths = [os.path.join(cache_dir, phase + \'.pkl\') for phase in args.phase]\n    dataset = utils.data.Dataset(\n        utils.data.load_pickles(paths),\n        transform=transform.augmentation.get_transform(config, config.get(\'transform\', \'augmentation\').split()),\n        shuffle=config.getboolean(\'data\', \'shuffle\'),\n    )\n    logging.info(\'num_examples=%d\' % len(dataset))\n    try:\n        workers = config.getint(\'data\', \'workers\')\n    except configparser.NoOptionError:\n        workers = multiprocessing.cpu_count()\n    collate_fn = utils.data.Collate(\n        transform.parse_transform(config, config.get(\'transform\', \'resize_train\')),\n        utils.train.load_sizes(config),\n        maintain=config.getint(\'data\', \'maintain\'),\n        transform_image=transform.get_transform(config, config.get(\'transform\', \'image_train\').split()),\n    )\n    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=workers, collate_fn=collate_fn)\n    for data in loader:\n        path, size, image, yx_min, yx_max, cls = (t.numpy() if hasattr(t, \'numpy\') else t for t in (data[key] for key in \'path, size, image, yx_min, yx_max, cls\'.split(\', \')))\n        fig, axes = plt.subplots(args.rows, args.cols)\n        axes = axes.flat if batch_size > 1 else [axes]\n        for ax, path, size, image, yx_min, yx_max, cls in zip(*[axes, path, size, image, yx_min, yx_max, cls]):\n            logging.info(path + \': \' + \'x\'.join(map(str, size)))\n            size = yx_max - yx_min\n            target = np.logical_and(*[np.squeeze(a, -1) > 0 for a in np.split(size, size.shape[-1], -1)])\n            yx_min, yx_max, cls = (a[target] for a in (yx_min, yx_max, cls))\n            image = draw_bbox(image, yx_min.astype(np.int), yx_max.astype(np.int), cls)\n            ax.imshow(image)\n            ax.set_title(\'%d objects\' % np.sum(target))\n            ax.set_xticks([])\n            ax.set_yticks([])\n        fig.tight_layout()\n        mng = plt.get_current_fig_manager()\n        mng.resize(*mng.window.maxsize())\n        plt.show()\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'-p\', \'--phase\', nargs=\'+\', default=[\'train\', \'val\', \'test\'])\n    parser.add_argument(\'--rows\', default=3, type=int)\n    parser.add_argument(\'--cols\', default=3, type=int)\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
demo_graph.py,7,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport yaml\n\nimport torch.autograd\nimport torch.cuda\nimport torch.optim\nimport torch.utils.data\nimport humanize\n\nimport model\nimport utils\nimport utils.train\nimport utils.visualize\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    model_dir = utils.get_model_dir(config)\n    category = utils.get_category(config)\n    anchors = torch.from_numpy(utils.get_anchors(config)).contiguous()\n    try:\n        path, step, epoch = utils.train.load_model(model_dir)\n        state_dict = torch.load(path, map_location=lambda storage, loc: storage)\n    except (FileNotFoundError, ValueError):\n        logging.warning(\'model cannot be loaded\')\n        state_dict = None\n    dnn = utils.parse_attr(config.get(\'model\', \'dnn\'))(model.ConfigChannels(config, state_dict), anchors, len(category))\n    logging.info(humanize.naturalsize(sum(var.cpu().numpy().nbytes for var in dnn.state_dict().values())))\n    if state_dict is not None:\n        dnn.load_state_dict(state_dict)\n    height, width = tuple(map(int, config.get(\'image\', \'size\').split()))\n    image = torch.autograd.Variable(torch.randn(args.batch_size, 3, height, width))\n    output = dnn(image)\n    state_dict = dnn.state_dict()\n    graph = utils.visualize.Graph(config, state_dict)\n    graph(output.grad_fn)\n    diff = [key for key in state_dict if key not in graph.drawn]\n    if diff:\n        logging.warning(\'variables not shown: \' + str(diff))\n    path = graph.dot.view(os.path.basename(model_dir) + \'.gv\', os.path.dirname(model_dir))\n    logging.info(path)\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'-b\', \'--batch_size\', default=1, type=int, help=\'batch size\')\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()'"
demo_lr.py,5,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport os\nimport yaml\n\nimport torch.autograd\nimport torch.cuda\nimport torch.optim\nimport torch.utils.data\n\nimport model\nimport utils.data\nimport utils.postprocess\nimport utils.train\nimport utils.visualize\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    category = utils.get_category(config)\n    anchors = torch.from_numpy(utils.get_anchors(config)).contiguous()\n    dnn = utils.parse_attr(config.get(\'model\', \'dnn\'))(model.ConfigChannels(config), anchors, len(category))\n    inference = model.Inference(config, dnn, anchors)\n    inference.train()\n    optimizer = eval(config.get(\'train\', \'optimizer\'))(filter(lambda p: p.requires_grad, inference.parameters()), args.learning_rate)\n    scheduler = eval(config.get(\'train\', \'scheduler\'))(optimizer)\n    for epoch in range(args.epoch):\n        scheduler.step(epoch)\n        lr = scheduler.get_lr()\n        print(\'\\t\'.join(map(str, [epoch] + lr)))\n\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'epoch\', type=int)\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'-o\', \'--optimizer\', default=\'adam\')\n    parser.add_argument(\'-lr\', \'--learning_rate\', default=1e-3, type=float, help=\'learning rate\')\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
detect.py,17,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport os\nimport time\nimport yaml\n\nimport numpy as np\nimport torch.autograd\nimport torch.cuda\nimport torch.optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport humanize\nimport pybenchmark\nimport cv2\n\nimport transform\nimport model\nimport utils.postprocess\nimport utils.train\nimport utils.visualize\n\n\ndef get_logits(pred):\n    if \'logits\' in pred:\n        return pred[\'logits\'].contiguous()\n    else:\n        size = pred[\'iou\'].size()\n        return torch.autograd.Variable(utils.ensure_device(torch.ones(*size, 1)))\n\n\ndef filter_visible(config, iou, yx_min, yx_max, prob):\n    prob_cls, cls = torch.max(prob, -1)\n    if config.getboolean(\'detect\', \'fix\'):\n        mask = (iou * prob_cls) > config.getfloat(\'detect\', \'threshold_cls\')\n    else:\n        mask = iou > config.getfloat(\'detect\', \'threshold\')\n    iou, prob_cls, cls = (t[mask].view(-1) for t in (iou, prob_cls, cls))\n    _mask = torch.unsqueeze(mask, -1).repeat(1, 2)  # PyTorch\'s bug\n    yx_min, yx_max = (t[_mask].view(-1, 2) for t in (yx_min, yx_max))\n    num = prob.size(-1)\n    _mask = torch.unsqueeze(mask, -1).repeat(1, num)  # PyTorch\'s bug\n    prob = prob[_mask].view(-1, num)\n    return iou, yx_min, yx_max, prob, prob_cls, cls\n\n\ndef postprocess(config, iou, yx_min, yx_max, prob):\n    iou, yx_min, yx_max, prob, prob_cls, cls = filter_visible(config, iou, yx_min, yx_max, prob)\n    keep = pybenchmark.profile(\'nms\')(utils.postprocess.nms)(iou, yx_min, yx_max, config.getfloat(\'detect\', \'overlap\'))\n    if keep:\n        keep = utils.ensure_device(torch.LongTensor(keep))\n        iou, yx_min, yx_max, prob, prob_cls, cls = (t[keep] for t in (iou, yx_min, yx_max, prob, prob_cls, cls))\n        if config.getboolean(\'detect\', \'fix\'):\n            score = torch.unsqueeze(iou, -1) * prob\n            mask = score > config.getfloat(\'detect\', \'threshold_cls\')\n            indices, cls = torch.unbind(mask.nonzero(), -1)\n            yx_min, yx_max = (t[indices] for t in (yx_min, yx_max))\n            score = score[mask]\n        else:\n            score = iou\n        return iou, yx_min, yx_max, cls, score\n\n\nclass Detect(object):\n    def __init__(self, args, config):\n        self.args = args\n        self.config = config\n        self.cache_dir = utils.get_cache_dir(config)\n        self.model_dir = utils.get_model_dir(config)\n        self.category = utils.get_category(config, self.cache_dir if os.path.exists(self.cache_dir) else None)\n        self.draw_bbox = utils.visualize.DrawBBox(self.category, colors=args.colors, thickness=args.thickness)\n        self.anchors = torch.from_numpy(utils.get_anchors(config)).contiguous()\n        self.height, self.width = tuple(map(int, config.get(\'image\', \'size\').split()))\n        self.path, self.step, self.epoch = utils.train.load_model(self.model_dir)\n        state_dict = torch.load(self.path, map_location=lambda storage, loc: storage)\n        self.dnn = utils.parse_attr(config.get(\'model\', \'dnn\'))(model.ConfigChannels(config, state_dict), self.anchors, len(self.category))\n        self.dnn.load_state_dict(state_dict)\n        self.inference = model.Inference(config, self.dnn, self.anchors)\n        self.inference.eval()\n        if torch.cuda.is_available():\n            self.inference.cuda()\n        logging.info(humanize.naturalsize(sum(var.cpu().numpy().nbytes for var in self.inference.state_dict().values())))\n        self.cap = self.create_cap()\n        self.keys = set(args.keys)\n        self.resize = transform.parse_transform(config, config.get(\'transform\', \'resize_test\'))\n        self.transform_image = transform.get_transform(config, config.get(\'transform\', \'image_test\').split())\n        self.transform_tensor = transform.get_transform(config, config.get(\'transform\', \'tensor\').split())\n\n    def __del__(self):\n        cv2.destroyAllWindows()\n        try:\n            self.writer.release()\n        except AttributeError:\n            pass\n        self.cap.release()\n\n    def create_cap(self):\n        try:\n            cap = int(self.args.input)\n        except ValueError:\n            cap = os.path.expanduser(os.path.expandvars(self.args.input))\n            assert os.path.exists(cap)\n        return cv2.VideoCapture(cap)\n\n    def create_writer(self, height, width):\n        fps = self.cap.get(cv2.CAP_PROP_FPS)\n        logging.info(\'cap fps=%f\' % fps)\n        path = os.path.expanduser(os.path.expandvars(self.args.output))\n        if self.args.fourcc:\n            fourcc = cv2.VideoWriter_fourcc(*self.args.fourcc.upper())\n        else:\n            fourcc = int(self.cap.get(cv2.CAP_PROP_FOURCC))\n        os.makedirs(os.path.dirname(path), exist_ok=True)\n        return cv2.VideoWriter(path, fourcc, fps, (width, height))\n\n    def get_image(self):\n        ret, image_bgr = self.cap.read()\n        if self.args.crop:\n            image_bgr = image_bgr[self.crop_ymin:self.crop_ymax, self.crop_xmin:self.crop_xmax]\n        return image_bgr\n\n    def __call__(self):\n        image_bgr = self.get_image()\n        image_resized = self.resize(image_bgr, self.height, self.width)\n        image = self.transform_image(image_resized)\n        tensor = self.transform_tensor(image)\n        tensor = utils.ensure_device(tensor.unsqueeze(0))\n        pred = pybenchmark.profile(\'inference\')(model._inference)(self.inference, torch.autograd.Variable(tensor, volatile=True))\n        rows, cols = pred[\'feature\'].size()[-2:]\n        iou = pred[\'iou\'].data.contiguous().view(-1)\n        yx_min, yx_max = (pred[key].data.view(-1, 2) for key in \'yx_min, yx_max\'.split(\', \'))\n        logits = get_logits(pred)\n        prob = F.softmax(logits, -1).data.view(-1, logits.size(-1))\n        ret = postprocess(self.config, iou, yx_min, yx_max, prob)\n        image_result = image_bgr.copy()\n        if ret is not None:\n            iou, yx_min, yx_max, cls, score = ret\n            try:\n                scale = self.scale\n            except AttributeError:\n                scale = utils.ensure_device(torch.from_numpy(np.array(image_result.shape[:2], np.float32) / np.array([rows, cols], np.float32)))\n                self.scale = scale\n            yx_min, yx_max = ((t * scale).cpu().numpy().astype(np.int) for t in (yx_min, yx_max))\n            image_result = self.draw_bbox(image_result, yx_min, yx_max, cls)\n        if self.args.output:\n            if not hasattr(self, \'writer\'):\n                self.writer = self.create_writer(*image_result.shape[:2])\n            self.writer.write(image_result)\n        else:\n            cv2.imshow(\'detection\', image_result)\n        if cv2.waitKey(0 if self.args.pause else 1) in self.keys:\n            root = os.path.join(self.model_dir, \'snapshot\')\n            os.makedirs(root, exist_ok=True)\n            path = os.path.join(root, time.strftime(self.args.format))\n            cv2.imwrite(path, image_bgr)\n            logging.warning(\'image dumped into \' + path)\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    detect = Detect(args, config)\n    try:\n        while detect.cap.isOpened():\n            detect()\n    except KeyboardInterrupt:\n        logging.warning(\'interrupted\')\n    finally:\n        logging.info(pybenchmark.stats)\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'-i\', \'--input\', default=-1)\n    parser.add_argument(\'-k\', \'--keys\', nargs=\'+\', type=int, default=[ord(\' \')], help=\'keys to dump images\')\n    parser.add_argument(\'-o\', \'--output\', help=\'output video file\')\n    parser.add_argument(\'-f\', \'--format\', default=\'%Y-%m-%d_%H-%M-%S.jpg\', help=\'dump file name format\')\n    parser.add_argument(\'--crop\', nargs=\'+\', type=float, default=[], help=\'ymin ymax xmin xmax\')\n    parser.add_argument(\'--pause\', action=\'store_true\')\n    parser.add_argument(\'--fourcc\', default=\'XVID\', help=\'4-character code of codec used to compress the frames, such as XVID, MJPG\')\n    parser.add_argument(\'--thickness\', default=3, type=int)\n    parser.add_argument(\'--colors\', nargs=\'+\', default=[])\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
dimension_cluster.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport yaml\n\nimport numpy as np\nimport nltk.cluster.kmeans\n\nimport utils.data\nimport utils.iou.numpy\n\n\ndef distance(a, b):\n    return 1 - utils.iou.numpy.iou(-a, a, -b, b)\n\n\ndef get_data(paths):\n    dataset = utils.data.Dataset(utils.data.load_pickles(paths))\n    return np.concatenate([(data[\'yx_max\'] - data[\'yx_min\']) / utils.image_size(data[\'path\']) for data in dataset.dataset])\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    cache_dir = utils.get_cache_dir(config)\n    paths = [os.path.join(cache_dir, phase + \'.pkl\') for phase in args.phase]\n    data = get_data(paths)\n    logging.info(\'num_examples=%d\' % len(data))\n    clusterer = nltk.cluster.kmeans.KMeansClusterer(args.num, distance, args.repeats)\n    try:\n        clusterer.cluster(data)\n    except KeyboardInterrupt:\n        logging.warning(\'interrupted\')\n    for m in clusterer.means():\n        print(\'\\t\'.join(map(str, m)))\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'num\', type=int)\n    parser.add_argument(\'-r\', \'--repeats\', type=int, default=np.iinfo(np.int).max)\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-p\', \'--phase\', nargs=\'+\', default=[\'train\', \'val\', \'test\'])\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
disable_bad_images.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport sys\nimport argparse\nimport shutil\nimport tqdm\n\nimport cv2\n\n\ndef main():\n    args = make_args()\n    root = os.path.expanduser(os.path.expandvars(args.root))\n    for dirpath, _, filenames in os.walk(root):\n        for filename in tqdm.tqdm(filenames, desc=dirpath):\n            if os.path.splitext(filename)[-1].lower() in args.exts and filename[0] != \'.\':\n                path = os.path.join(dirpath, filename)\n                image = cv2.imread(path)\n                if image is None:\n                    sys.stderr.write(\'disable bad image %s\\n\' % path)\n                    _path = os.path.join(os.path.dirname(path), \'.\' + os.path.basename(path))\n                    if os.path.exists(_path):\n                        os.remove(_path)\n                    shutil.move(path, _path)\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'root\')\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-e\', \'--exts\', nargs=\'+\', default=[\'.jpe\', \'.jpg\', \'.jpeg\', \'.png\'])\n    parser.add_argument(\'--level\', default=\'info\', help=\'logging level\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
download_url.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport sys\nimport argparse\nimport threading\n\nimport numpy as np\nimport tqdm\nimport wget\n\n\ndef _task(url, root, ext):\n    path = wget.download(url, bar=None)\n    with open(path + ext, \'w\') as f:\n        f.write(url)\n\n\ndef task(urls, root, ext, pbar, lock, f):\n    for url in urls:\n        url = url.rstrip()\n        try:\n            _task(url, root, ext)\n        except:\n            with lock:\n                f.write(url + \'\\n\')\n        pbar.update()\n\n\ndef main():\n    args = make_args()\n    root = os.path.expandvars(os.path.expanduser(args.root))\n    os.makedirs(root, exist_ok=True)\n    os.chdir(root)\n    workers = []\n    urls = list(set(sys.stdin.readlines()))\n    lock = threading.Lock()\n    with tqdm.tqdm(total=len(urls)) as pbar, open(root + args.ext, \'w\') as f:\n        for urls in np.array_split(urls, args.workers):\n            w = threading.Thread(target=task, args=(urls, root, args.ext, pbar, lock, f))\n            w.start()\n            workers.append(w)\n        for w in workers:\n            w.join()\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'root\')\n    parser.add_argument(\'-w\', \'--workers\', type=int, default=6)\n    parser.add_argument(\'-e\', \'--ext\', default=\'.url\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
eval.py,28,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport configparser\nimport datetime\nimport json\nimport logging\nimport logging.config\nimport multiprocessing\nimport importlib\nimport inspect\nimport inflection\nimport hashlib\nimport yaml\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torch.autograd\nimport torch.cuda\nimport torch.optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport tqdm\nimport humanize\nimport pybenchmark\nimport tinydb\nimport xlsxwriter\nimport cv2\n\nimport transform\nimport model\nimport utils.data\nimport utils.iou.torch\nimport utils.postprocess\nimport utils.train\nimport utils.visualize\nfrom detect import get_logits, postprocess\n\n\ndef _matching(positive, index):\n    detected = set()\n    tp = np.zeros([len(positive)], np.bool)\n    for i, (positive, index) in enumerate(zip(positive, index)):\n        if positive and index not in detected:\n            tp[i] = True\n            detected.add(index)\n    return tp\n\n\ndef matching(data_yx_min, data_yx_max, yx_min, yx_max, threshold):\n    if data_yx_min.numel() > 0:\n        matrix = utils.iou.torch.iou_matrix(yx_min, yx_max, data_yx_min, data_yx_max)\n        iou, index = torch.max(matrix, -1)\n        positive = iou > threshold\n        tp = _matching(positive.cpu().numpy(), index.cpu().numpy())\n    else:\n        tp = np.zeros([yx_min.size(0)], np.bool)\n    return tp\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef average_precision(config, tp, num, dtype=np.float):\n    fp = ~tp\n    tp = np.cumsum(tp)\n    fp = np.cumsum(fp)\n    if num > 0:\n        rec = tp / num\n    else:\n        rec = np.zeros(len(tp), dtype=dtype)\n    prec = tp / np.maximum(tp + fp, np.finfo(dtype).eps)\n    return voc_ap(rec, prec, config.getboolean(\'eval\', \'metric07\'))\n\n\ndef norm_bbox_data(data, keys=\'yx_min, yx_max\'.split(\', \')):\n    height, width = data[\'image\'].size()[1:3]\n    scale = utils.ensure_device(torch.from_numpy(np.reshape(np.array([height, width], dtype=np.float32), [1, 1, 2])))\n    for key in keys:\n        data[key] = data[key] / scale\n    return keys\n\n\ndef norm_bbox_pred(pred, keys=\'yx_min, yx_max\'.split(\', \')):\n    rows, cols = pred[\'feature\'].size()[-2:]\n    scale = utils.ensure_device(torch.from_numpy(np.reshape(np.array([rows, cols], dtype=np.float32), [1, 1, 1, 2])))\n    for key in keys:\n        pred[key] = pred[key] / scale\n    return keys\n\n\ndef filter_valid(yx_min, yx_max, cls, difficult):\n    mask = torch.prod(yx_min < yx_max, -1) & (difficult < 1)\n    _mask = torch.unsqueeze(mask, -1).repeat(1, 2) # PyTorch\'s bug\n    cls, = (t[mask] for t in (cls,))\n    yx_min, yx_max = (t[_mask].view(-1, 2) for t in (yx_min, yx_max))\n    return yx_min, yx_max, cls\n\n\ndef filter_cls_data(yx_min, yx_max, mask):\n    if mask.numel() > 0:\n        _mask = torch.unsqueeze(mask, -1).repeat(1, 2)  # PyTorch\'s bug\n        yx_min, yx_max = (t[_mask].view(-1, 2) for t in (yx_min, yx_max))\n    else:  # all bboxes are difficult\n        yx_min = utils.ensure_device(torch.zeros(0, 2))\n        yx_max = utils.ensure_device(torch.zeros(0, 2))\n    return yx_min, yx_max\n\n\ndef filter_cls_pred(yx_min, yx_max, score, mask):\n    _mask = torch.unsqueeze(mask, -1).repeat(1, 2)  # PyTorch\'s bug\n    yx_min, yx_max = (t[_mask].view(-1, 2) for t in (yx_min, yx_max))\n    score = score[mask]\n    return yx_min, yx_max, score\n\n\nclass Eval(object):\n    def __init__(self, args, config):\n        self.args = args\n        self.config = config\n        self.model_dir = utils.get_model_dir(config)\n        self.cache_dir = utils.get_cache_dir(config)\n        self.category = utils.get_category(config, self.cache_dir)\n        self.draw_bbox = utils.visualize.DrawBBox(self.category)\n        self.loader = self.get_loader()\n        self.anchors = torch.from_numpy(utils.get_anchors(config)).contiguous()\n        self.path, self.step, self.epoch = utils.train.load_model(self.model_dir)\n        state_dict = torch.load(self.path, map_location=lambda storage, loc: storage)\n        dnn = utils.parse_attr(config.get(\'model\', \'dnn\'))(model.ConfigChannels(config, state_dict), self.anchors, len(self.category))\n        dnn.load_state_dict(state_dict)\n        logging.info(humanize.naturalsize(sum(var.cpu().numpy().nbytes for var in dnn.state_dict().values())))\n        self.inference = model.Inference(config, dnn, self.anchors)\n        self.inference.eval()\n        if torch.cuda.is_available():\n            self.inference.cuda()\n        path = self.model_dir + \'.ini\'\n        if os.path.exists(path):\n            self._config = configparser.ConfigParser()\n            self._config.read(path)\n        else:\n            logging.warning(\'training config (%s) not found\' % path)\n        self.now = datetime.datetime.now()\n        self.mapper = dict([(inflection.underscore(name), member()) for name, member in inspect.getmembers(importlib.machinery.SourceFileLoader(\'\', self.config.get(\'eval\', \'mapper\')).load_module()) if inspect.isclass(member)])\n\n    def get_loader(self):\n        paths = [os.path.join(self.cache_dir, phase + \'.pkl\') for phase in self.config.get(\'eval\', \'phase\').split()]\n        dataset = utils.data.Dataset(utils.data.load_pickles(paths))\n        logging.info(\'num_examples=%d\' % len(dataset))\n        size = tuple(map(int, self.config.get(\'image\', \'size\').split()))\n        try:\n            workers = self.config.getint(\'data\', \'workers\')\n        except configparser.NoOptionError:\n            workers = multiprocessing.cpu_count()\n        collate_fn = utils.data.Collate(\n            transform.parse_transform(self.config, self.config.get(\'transform\', \'resize_eval\')),\n            [size],\n            transform_image=transform.get_transform(self.config, self.config.get(\'transform\', \'image_test\').split()),\n            transform_tensor=transform.get_transform(self.config, self.config.get(\'transform\', \'tensor\').split()),\n        )\n        return torch.utils.data.DataLoader(dataset, batch_size=self.args.batch_size, num_workers=workers, collate_fn=collate_fn)\n\n    def filter_cls(self, c, path, data_yx_min, data_yx_max, data_cls, yx_min, yx_max, cls, score):\n        data_yx_min, data_yx_max = filter_cls_data(data_yx_min, data_yx_max, data_cls == c)\n        yx_min, yx_max, score = filter_cls_pred(yx_min, yx_max, score, cls == c)\n        tp = pybenchmark.profile(\'matching\')(matching)(data_yx_min, data_yx_max, yx_min, yx_max, self.config.getfloat(\'eval\', \'iou\'))\n        if self.config.getboolean(\'eval\', \'debug\'):\n            self.debug_visualize(data_yx_min, data_yx_max, yx_min, yx_max, c, tp, path)\n        return score, tp\n\n    def debug_data(self, data):\n        for i, t in enumerate(torch.unbind(data[\'image\'])):\n            a = t.cpu().numpy()\n            logging.info(\'image%d: %f %s\' % (i, utils.abs_mean(a), hashlib.md5(a.tostring()).hexdigest()))\n        for i, t in enumerate(torch.unbind(data[\'tensor\'])):\n            a = t.cpu().numpy()\n            logging.info(\'tensor%d: %f %s\' % (i, utils.abs_mean(a), hashlib.md5(a.tostring()).hexdigest()))\n\n    def debug_pred(self, pred):\n        for i, t in enumerate(torch.unbind(pred[\'iou\'])):\n            a = t.cpu().numpy()\n            logging.info(\'iou%d: %f %s\' % (i, utils.abs_mean(a), hashlib.md5(a.tostring()).hexdigest()))\n        for i, t in enumerate(torch.unbind(pred[\'center_offset\'])):\n            a = t.cpu().numpy()\n            logging.info(\'center_offset%d: %f %s\' % (i, utils.abs_mean(a), hashlib.md5(a.tostring()).hexdigest()))\n        for i, t in enumerate(torch.unbind(pred[\'size_norm\'])):\n            a = t.cpu().numpy()\n            logging.info(\'size_norm%d: %f %s\' % (i, utils.abs_mean(a), hashlib.md5(a.tostring()).hexdigest()))\n        for i, t in enumerate(torch.unbind(pred[\'logits\'])):\n            a = t.cpu().numpy()\n            logging.info(\'logits%d: %f %s\' % (i, utils.abs_mean(a), hashlib.md5(a.tostring()).hexdigest()))\n\n    def debug_visualize(self, data_yx_min, data_yx_max, yx_min, yx_max, c, tp, path):\n        canvas = cv2.imread(path)\n        canvas = cv2.cvtColor(canvas, cv2.COLOR_BGR2RGB)\n        size = np.reshape(np.array(canvas.shape[:2], np.float32), [1, 2])\n        data_yx_min, data_yx_max, yx_min, yx_max = (np.reshape(t.cpu().numpy(), [-1, 2]) * size for t in (data_yx_min, data_yx_max, yx_min, yx_max))\n        canvas = self.draw_bbox(canvas, data_yx_min, data_yx_max, colors=[\'g\'])\n        canvas = self.draw_bbox(canvas, *(a[tp] for a in (yx_min, yx_max)), colors=[\'w\'])\n        fp = ~tp\n        canvas = self.draw_bbox(canvas, *(a[fp] for a in (yx_min, yx_max)), colors=[\'k\'])\n        fig = plt.figure()\n        ax = fig.gca()\n        ax.imshow(canvas)\n        ax.set_title(\'tp=%d, fp=%d\' % (np.sum(tp), np.sum(fp)))\n        fig.canvas.set_window_title(self.category[c] + \': \' + path)\n        plt.show()\n        plt.close(fig)\n\n    def stat_ap(self):\n        cls_num = [0 for _ in self.category]\n        cls_score = [np.array([], dtype=np.float32) for _ in self.category]\n        cls_tp = [np.array([], dtype=np.bool) for _ in self.category]\n        for data in tqdm.tqdm(self.loader):\n            for key in data:\n                t = data[key]\n                if torch.is_tensor(t):\n                    data[key] = utils.ensure_device(t)\n            tensor = torch.autograd.Variable(data[\'tensor\'], volatile=True)\n            pred = pybenchmark.profile(\'inference\')(model._inference)(self.inference, tensor)\n            pred[\'iou\'] = pred[\'iou\'].contiguous()\n            logits = get_logits(pred)\n            pred[\'prob\'] = F.softmax(logits, -1)\n            for key in pred:\n                pred[key] = pred[key].data\n            if self.config.getboolean(\'eval\', \'debug\'):\n                self.debug_data(data)\n                self.debug_pred(pred)\n            norm_bbox_data(data)\n            norm_bbox_pred(pred)\n            for path, difficult, image, data_yx_min, data_yx_max, data_cls, iou, yx_min, yx_max, prob in zip(*(data[key] for key in \'path, difficult\'.split(\', \')), *(torch.unbind(data[key]) for key in \'image, yx_min, yx_max, cls\'.split(\', \')), *(torch.unbind(pred[key]) for key in \'iou, yx_min, yx_max, prob\'.split(\', \'))):\n                data_yx_min, data_yx_max, data_cls = filter_valid(data_yx_min, data_yx_max, data_cls, difficult)\n                for c in data_cls.cpu().numpy():\n                    cls_num[c] += 1\n                iou = iou.view(-1)\n                yx_min, yx_max, prob = (t.view(-1, t.size(-1)) for t in (yx_min, yx_max, prob))\n                ret = postprocess(self.config, iou, yx_min, yx_max, prob)\n                if ret is not None:\n                    iou, yx_min, yx_max, cls, score = ret\n                    for c in set(cls.cpu().numpy()):\n                        c = int(c)  # PyTorch\'s bug\n                        _score, tp = self.filter_cls(c, path, data_yx_min, data_yx_max, data_cls, yx_min, yx_max, cls, score)\n                        cls_score[c] = np.append(cls_score[c], _score.cpu().numpy())\n                        cls_tp[c] = np.append(cls_tp[c], tp)\n        return cls_num, cls_score, cls_tp\n\n    def merge_ap(self, cls_num, cls_score, cls_tp):\n        cls_ap = {}\n        for c, (num, score, tp) in enumerate(zip(cls_num, cls_score, cls_tp)):\n            if num > 0:\n                indices = np.argsort(-score)\n                tp = tp[indices]\n                cls_ap[c] = average_precision(self.config, tp, num)\n        return cls_ap\n\n    def save_db(self, cls_ap, path):\n        with tinydb.TinyDB(path) as db:\n            row = dict([(key, fn(self, cls_ap=cls_ap)) for key, fn in self.mapper.items()])\n            db.insert(row)\n\n    def save_xlsx(self, df, path, worksheet=\'worksheet\'):\n        with xlsxwriter.Workbook(path, {\'strings_to_urls\': False, \'nan_inf_to_errors\': True}) as workbook:\n            worksheet = workbook.add_worksheet(worksheet)\n            for j, key in enumerate(df):\n                worksheet.write(0, j, key)\n                try:\n                    m = self.mapper[key]\n                except (KeyError, AttributeError):\n                    m = None\n                if hasattr(m, \'get_format\'):\n                    fmt = m.get_format(workbook, worksheet)\n                else:\n                    fmt = None\n                for i, value in enumerate(df[key]):\n                    worksheet.write(1 + i, j, value, fmt)\n                if hasattr(m, \'format\'):\n                    m.format(workbook, worksheet, i, j)\n            worksheet.autofilter(0, 0, i, len(self.mapper) - 1)\n            worksheet.freeze_panes(1, 0)\n\n    def logging(self, cls_ap):\n        for c in cls_ap:\n            logging.info(\'%s=%f\' % (self.category[c], cls_ap[c]))\n        logging.info(np.mean(list(cls_ap.values())))\n\n    def __call__(self):\n        cls_num, cls_score, cls_tp = self.stat_ap()\n        cls_ap = self.merge_ap(cls_num, cls_score, cls_tp)\n        path = utils.get_eval_db(self.config)\n        self.save_db(cls_ap, path)\n        with open(path, \'r\') as f:\n            df = pd.read_json(json.dumps(json.load(f)[\'_default\']), orient=\'index\', convert_dates=False)\n        df = df[sorted(df)]\n        try:\n            df = df.sort_values(self.config.get(\'eval\', \'sort\'))\n        except configparser.NoOptionError:\n            pass\n        self.save_xlsx(df, os.path.splitext(path)[0] + \'.xlsx\')\n        self.logging(cls_ap)\n        return cls_ap\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    eval = Eval(args, config)\n    eval()\n    logging.info(pybenchmark.stats)\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'-b\', \'--batch_size\', default=16, type=int, help=\'batch size\')\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
pruner.py,9,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport yaml\n\nimport numpy as np\nimport torch.autograd\nimport torch.cuda\nimport torch.optim\nimport torch.utils.data\nimport humanize\n\nimport model\nimport utils\nimport utils.train\nimport utils.channel\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    model_dir = utils.get_model_dir(config)\n    category = utils.get_category(config)\n    anchors = torch.from_numpy(utils.get_anchors(config)).contiguous()\n    path, step, epoch = utils.train.load_model(model_dir)\n    state_dict = torch.load(path, map_location=lambda storage, loc: storage)\n    _model = utils.parse_attr(config.get(\'model\', \'dnn\'))\n    dnn = _model(model.ConfigChannels(config, state_dict), anchors, len(category))\n    logging.info(humanize.naturalsize(sum(var.cpu().numpy().nbytes for var in dnn.state_dict().values())))\n    dnn.load_state_dict(state_dict)\n    height, width = tuple(map(int, config.get(\'image\', \'size\').split()))\n    image = torch.autograd.Variable(torch.randn(args.batch_size, 3, height, width))\n    output = dnn(image)\n    state_dict = dnn.state_dict()\n    d = utils.dense(state_dict[args.name])\n    keep = torch.LongTensor(np.argsort(d)[:int(len(d) * args.keep)])\n    modifier = utils.channel.Modifier(\n        args.name, state_dict, dnn,\n        lambda name, var: var[keep],\n        lambda name, var, mapper: var[mapper(keep, len(d))],\n        debug=args.debug,\n    )\n    modifier(output.grad_fn)\n    if args.debug:\n        path = modifier.dot.view(\'%s.%s.gv\' % (os.path.basename(model_dir), os.path.basename(os.path.splitext(__file__)[0])), os.path.dirname(model_dir))\n        logging.info(path)\n    assert len(keep) == len(state_dict[args.name])\n    dnn = _model(model.ConfigChannels(config, state_dict), anchors, len(category))\n    dnn.load_state_dict(state_dict)\n    dnn(image)\n    if not args.debug:\n        torch.save(state_dict, path)\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'name\')\n    parser.add_argument(\'keep\', type=float)\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'-b\', \'--batch_size\', default=1, type=int, help=\'batch size\')\n    parser.add_argument(\'-d\', \'--debug\', action=\'store_true\')\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()'"
receptive_field_analyzer.py,16,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport multiprocessing\nimport yaml\n\nimport numpy as np\nimport scipy.misc\nimport torch.autograd\nimport torch.cuda\nimport torch.optim\nimport torch.utils.data\nimport tqdm\nimport humanize\n\nimport model\nimport utils.data\nimport utils.iou.torch\nimport utils.postprocess\nimport utils.train\nimport utils.visualize\n\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, height, width):\n        self.points = np.array([(i, j) for i in range(height) for j in range(width)])\n\n    def __len__(self):\n        return len(self.points)\n\n    def __getitem__(self, index):\n        return self.points[index]\n\n\nclass Analyzer(object):\n    def __init__(self, args, config):\n        self.args = args\n        self.config = config\n        self.model_dir = utils.get_model_dir(config)\n        self.category = utils.get_category(config)\n        self.anchors = torch.from_numpy(utils.get_anchors(config)).contiguous()\n        self.dnn = utils.parse_attr(config.get(\'model\', \'dnn\'))(model.ConfigChannels(config), self.anchors, len(self.category))\n        self.dnn.eval()\n        logging.info(humanize.naturalsize(sum(var.cpu().numpy().nbytes for var in self.dnn.state_dict().values())))\n        if torch.cuda.is_available():\n            self.dnn.cuda()\n        self.height, self.width = tuple(map(int, config.get(\'image\', \'size\').split()))\n        output = self.dnn(torch.autograd.Variable(utils.ensure_device(torch.zeros(1, 3, self.height, self.width)), volatile=True))\n        _, _, self.rows, self.cols = output.size()\n        self.i, self.j = self.rows // 2, self.cols // 2\n        self.output = output[:, :, self.i, self.j]\n        dataset = Dataset(self.height, self.width)\n        try:\n            workers = self.config.getint(\'data\', \'workers\')\n        except configparser.NoOptionError:\n            workers = multiprocessing.cpu_count()\n        self.loader = torch.utils.data.DataLoader(dataset, batch_size=self.args.batch_size, num_workers=workers)\n\n    def __call__(self):\n        changed = np.zeros([self.height, self.width], np.bool)\n        for yx in tqdm.tqdm(self.loader):\n            batch_size = yx.size(0)\n            tensor = torch.zeros(batch_size, 3, self.height, self.width)\n            for i, _yx in enumerate(torch.unbind(yx)):\n                y, x = torch.unbind(_yx)\n                tensor[i, :, y, x] = 1\n            tensor = utils.ensure_device(tensor)\n            output = self.dnn(torch.autograd.Variable(tensor, volatile=True))\n            output = output[:, :, self.i, self.j]\n            cmp = output == self.output\n            cmp = torch.prod(cmp, -1).data\n            for _yx, c in zip(torch.unbind(yx), torch.unbind(cmp)):\n                y, x = torch.unbind(_yx)\n                changed[y, x] = c\n        return changed\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    analyzer = Analyzer(args, config)\n    changed = analyzer()\n    os.makedirs(analyzer.model_dir, exist_ok=True)\n    path = os.path.join(analyzer.model_dir, args.filename)\n    scipy.misc.imsave(path, (~changed).astype(np.uint8) * 255)\n    logging.info(path)\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'-b\', \'--batch_size\', default=16, type=int, help=\'batch size\')\n    parser.add_argument(\'-n\', \'--filename\', default=\'receptive_field.jpg\')\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
split_data.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport random\n\n\ndef main():\n    args = make_args()\n    root = os.path.expanduser(os.path.expandvars(args.root))\n    realpaths = []\n    for dirpath, _, filenames in os.walk(root):\n        for filename in filenames:\n            if os.path.splitext(filename)[-1].lower() in args.exts and filename[0] != \'.\':\n                path = os.path.join(dirpath, filename)\n                realpath = os.path.relpath(path, root)\n                realpaths.append(realpath)\n    random.shuffle(realpaths)\n    total = args.train + args.val + args.test\n    nval = int(len(realpaths) * args.val / total)\n    ntest = nval + int(len(realpaths) * args.test / total)\n    val = realpaths[:nval]\n    test = realpaths[nval:ntest]\n    train = realpaths[ntest:]\n    print(\'train=%d, val=%d, test=%d\' % (len(train), len(val), len(test)))\n    with open(os.path.join(root, \'train\' + args.ext), \'w\') as f:\n        for path in train:\n            f.write(path + \'\\n\')\n    with open(os.path.join(root, \'val\' + args.ext), \'w\') as f:\n        for path in val:\n            f.write(path + \'\\n\')\n    with open(os.path.join(root, \'test\' + args.ext), \'w\') as f:\n        for path in test:\n            f.write(path + \'\\n\')\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'root\')\n    parser.add_argument(\'-e\', \'--exts\', nargs=\'+\', default=[\'.jpe\', \'.jpg\', \'.jpeg\', \'.png\'])\n    parser.add_argument(\'--train\', type=float, default=7)\n    parser.add_argument(\'--val\', type=float, default=2)\n    parser.add_argument(\'--test\', type=float, default=1)\n    parser.add_argument(\'--ext\', default=\'.txt\')\n    return parser.parse_args()\n\nif __name__ == \'__main__\':\n    main()\n'"
train.py,30,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport sys\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport collections\nimport multiprocessing\nimport os\nimport shutil\nimport io\nimport hashlib\nimport subprocess\nimport pickle\nimport traceback\nimport yaml\n\nimport numpy as np\nimport torch.autograd\nimport torch.cuda\nimport torch.nn as nn\nimport torch.optim\nimport torch.utils.data\nimport torch.nn.functional as F\nimport torchvision.transforms\nimport tqdm\nimport humanize\nimport pybenchmark\nimport filelock\nfrom tensorboardX import SummaryWriter\n\nimport model\nimport transform.augmentation\nimport utils.data\nimport utils.postprocess\nimport utils.train\nimport utils.visualize\nimport eval as _eval\n\n\ndef norm_data(data, height, width, rows, cols, keys=\'yx_min, yx_max\'):\n    _data = {key: data[key] for key in data}\n    scale = utils.ensure_device(torch.from_numpy(np.reshape(np.array([rows / height, cols / width], dtype=np.float32), [1, 1, 2])))\n    for key in keys.split(\', \'):\n        _data[key] = _data[key] * scale\n    return _data\n\n\ndef ensure_model(model):\n    if torch.cuda.is_available():\n        model.cuda()\n        if torch.cuda.device_count() > 1:\n            logging.info(\'%d GPUs are used\' % torch.cuda.device_count())\n            model = nn.DataParallel(model).cuda()\n    return model\n\n\nclass SummaryWorker(multiprocessing.Process):\n    def __init__(self, env):\n        super(SummaryWorker, self).__init__()\n        self.env = env\n        self.config = env.config\n        self.queue = multiprocessing.Queue()\n        try:\n            self.timer_scalar = utils.train.Timer(env.config.getfloat(\'summary\', \'scalar\'))\n        except configparser.NoOptionError:\n            self.timer_scalar = lambda: False\n        try:\n            self.timer_image = utils.train.Timer(env.config.getfloat(\'summary\', \'image\'))\n        except configparser.NoOptionError:\n            self.timer_image = lambda: False\n        try:\n            self.timer_histogram = utils.train.Timer(env.config.getfloat(\'summary\', \'histogram\'))\n        except configparser.NoOptionError:\n            self.timer_histogram = lambda: False\n        with open(os.path.expanduser(os.path.expandvars(env.config.get(\'summary_histogram\', \'parameters\'))), \'r\') as f:\n            self.histogram_parameters = utils.RegexList([line.rstrip() for line in f])\n        self.draw_bbox = utils.visualize.DrawBBox(env.category)\n        self.draw_feature = utils.visualize.DrawFeature()\n\n    def __call__(self, name, **kwargs):\n        if getattr(self, \'timer_\' + name)():\n            kwargs = getattr(self, \'copy_\' + name)(**kwargs)\n            self.queue.put((name, kwargs))\n\n    def stop(self):\n        self.queue.put((None, {}))\n\n    def run(self):\n        self.writer = SummaryWriter(os.path.join(self.env.model_dir, self.env.args.run))\n        try:\n            height, width = tuple(map(int, self.config.get(\'image\', \'size\').split()))\n            tensor = torch.randn(1, 3, height, width)\n            step, epoch, dnn = self.env.load()\n            self.writer.add_graph(dnn, (torch.autograd.Variable(tensor),))\n        except:\n            traceback.print_exc()\n        while True:\n            name, kwargs = self.queue.get()\n            if name is None:\n                break\n            func = getattr(self, \'summary_\' + name)\n            try:\n                func(**kwargs)\n            except:\n                traceback.print_exc()\n\n    def copy_scalar(self, **kwargs):\n        step, loss_total, loss, loss_hparam = (kwargs[key] for key in \'step, loss_total, loss, loss_hparam\'.split(\', \'))\n        loss_total = loss_total.data.clone().cpu().numpy()\n        loss = {key: l.data.clone().cpu().numpy() for key, l in loss.items()}\n        loss_hparam = {key: l.data.clone().cpu().numpy() for key, l in loss_hparam.items()}\n        return dict(\n            step=step,\n            loss_total=loss_total,\n            loss=loss, loss_hparam=loss_hparam,\n        )\n\n    def summary_scalar(self, **kwargs):\n        step, loss_total, loss, loss_hparam = (kwargs[key] for key in \'step, loss_total, loss, loss_hparam\'.split(\', \'))\n        for key, l in loss.items():\n            self.writer.add_scalar(\'loss/\' + key, l[0], step)\n        if self.config.getboolean(\'summary_scalar\', \'loss_hparam\'):\n            self.writer.add_scalars(\'loss_hparam\', {key: l[0] for key, l in loss_hparam.items()}, step)\n        self.writer.add_scalar(\'loss_total\', loss_total[0], step)\n\n    def copy_image(self, **kwargs):\n        step, height, width, rows, cols, data, pred, debug = (kwargs[key] for key in \'step, height, width, rows, cols, data, pred, debug\'.split(\', \'))\n        data = {key: data[key].clone().cpu().numpy() for key in \'image, yx_min, yx_max, cls\'.split(\', \')}\n        pred = {key: pred[key].data.clone().cpu().numpy() for key in \'yx_min, yx_max, iou, logits\'.split(\', \') if key in pred}\n        matching = (debug[\'positive\'].float() - debug[\'negative\'].float() + 1) / 2\n        matching = matching.data.clone().cpu().numpy()\n        return dict(\n            step=step, height=height, width=width, rows=rows, cols=cols,\n            data=data, pred=pred,\n            matching=matching,\n        )\n\n    def summary_image(self, **kwargs):\n        step, height, width, rows, cols, data, pred, matching = (kwargs[key] for key in \'step, height, width, rows, cols, data, pred, matching\'.split(\', \'))\n        image = data[\'image\']\n        limit = min(self.config.getint(\'summary_image\', \'limit\'), image.shape[0])\n        image = image[:limit, :, :, :]\n        yx_min, yx_max, iou = (pred[key] for key in \'yx_min, yx_max, iou\'.split(\', \'))\n        scale = [height / rows, width / cols]\n        yx_min, yx_max = (a * scale for a in (yx_min, yx_max))\n        if \'logits\' in pred:\n            cls = np.argmax(F.softmax(torch.autograd.Variable(torch.from_numpy(pred[\'logits\'])), -1).data.cpu().numpy(), -1)\n        else:\n            cls = np.zeros(iou.shape, np.int)\n        if self.config.getboolean(\'summary_image\', \'bbox\'):\n            # data\n            canvas = np.copy(image)\n            canvas = pybenchmark.profile(\'bbox/data\')(self.draw_bbox_data)(canvas, *(data[key] for key in \'yx_min, yx_max, cls\'.split(\', \')))\n            self.writer.add_image(\'bbox/data\', torchvision.utils.make_grid(torch.from_numpy(np.stack(canvas)).permute(0, 3, 1, 2).float(), normalize=True, scale_each=True), step)\n            # pred\n            canvas = np.copy(image)\n            canvas = pybenchmark.profile(\'bbox/pred\')(self.draw_bbox_pred)(canvas, yx_min, yx_max, cls, iou, nms=True)\n            self.writer.add_image(\'bbox/pred\', torchvision.utils.make_grid(torch.from_numpy(np.stack(canvas)).permute(0, 3, 1, 2).float(), normalize=True, scale_each=True), step)\n        if self.config.getboolean(\'summary_image\', \'iou\'):\n            # bbox\n            canvas = np.copy(image)\n            canvas_data = self.draw_bbox_data(canvas, *(data[key] for key in \'yx_min, yx_max, cls\'.split(\', \')), colors=[\'g\'])\n            # data\n            for i, canvas in enumerate(pybenchmark.profile(\'iou/data\')(self.draw_bbox_iou)(list(map(np.copy, canvas_data)), yx_min, yx_max, cls, matching, rows, cols, colors=[\'w\'])):\n                canvas = np.stack(canvas)\n                canvas = torch.from_numpy(canvas).permute(0, 3, 1, 2)\n                canvas = torchvision.utils.make_grid(canvas.float(), normalize=True, scale_each=True)\n                self.writer.add_image(\'iou/data%d\' % i, canvas, step)\n            # pred\n            for i, canvas in enumerate(pybenchmark.profile(\'iou/pred\')(self.draw_bbox_iou)(list(map(np.copy, canvas_data)), yx_min, yx_max, cls, iou, rows, cols, colors=[\'w\'])):\n                canvas = np.stack(canvas)\n                canvas = torch.from_numpy(canvas).permute(0, 3, 1, 2)\n                canvas = torchvision.utils.make_grid(canvas.float(), normalize=True, scale_each=True)\n                self.writer.add_image(\'iou/pred%d\' % i, canvas, step)\n\n    def draw_bbox_data(self, canvas, yx_min, yx_max, cls, colors=None):\n        batch_size = len(canvas)\n        if len(cls.shape) == len(yx_min.shape):\n            cls = np.argmax(cls, -1)\n        yx_min, yx_max, cls = ([a[b] for b in range(batch_size)] for a in (yx_min, yx_max, cls))\n        return [self.draw_bbox(canvas, yx_min.astype(np.int), yx_max.astype(np.int), cls, colors=colors) for canvas, yx_min, yx_max, cls in zip(canvas, yx_min, yx_max, cls)]\n\n    def draw_bbox_pred(self, canvas, yx_min, yx_max, cls, iou, colors=None, nms=False):\n        batch_size = len(canvas)\n        mask = iou > self.config.getfloat(\'detect\', \'threshold\')\n        yx_min, yx_max = (np.reshape(a, [a.shape[0], -1, 2]) for a in (yx_min, yx_max))\n        cls, iou, mask = (np.reshape(a, [a.shape[0], -1]) for a in (cls, iou, mask))\n        yx_min, yx_max, cls, iou, mask = ([a[b] for b in range(batch_size)] for a in (yx_min, yx_max, cls, iou, mask))\n        yx_min, yx_max, cls, iou = ([a[m] for a, m in zip(l, mask)] for l in (yx_min, yx_max, cls, iou))\n        if nms:\n            overlap = self.config.getfloat(\'detect\', \'overlap\')\n            keep = [pybenchmark.profile(\'nms\')(utils.postprocess.nms)(torch.Tensor(iou), torch.Tensor(yx_min), torch.Tensor(yx_max), overlap) if iou.shape[0] > 0 else [] for yx_min, yx_max, iou in zip(yx_min, yx_max, iou)]\n            keep = [np.array(k, np.int) for k in keep]\n            yx_min, yx_max, cls = ([a[k] for a, k in zip(l, keep)] for l in (yx_min, yx_max, cls))\n        return [self.draw_bbox(canvas, yx_min.astype(np.int), yx_max.astype(np.int), cls, colors=colors) for canvas, yx_min, yx_max, cls in zip(canvas, yx_min, yx_max, cls)]\n\n    def draw_bbox_iou(self, canvas_share, yx_min, yx_max, cls, iou, rows, cols, colors=None):\n        batch_size = len(canvas_share)\n        yx_min, yx_max = ([np.squeeze(a, -2) for a in np.split(a, a.shape[-2], -2)] for a in (yx_min, yx_max))\n        cls, iou = ([np.squeeze(a, -1) for a in np.split(a, a.shape[-1], -1)] for a in (cls, iou))\n        results = []\n        for i, (yx_min, yx_max, cls, iou) in enumerate(zip(yx_min, yx_max, cls, iou)):\n            mask = iou > self.config.getfloat(\'detect\', \'threshold\')\n            yx_min, yx_max = (np.reshape(a, [a.shape[0], -1, 2]) for a in (yx_min, yx_max))\n            cls, iou, mask = (np.reshape(a, [a.shape[0], -1]) for a in (cls, iou, mask))\n            yx_min, yx_max, cls, iou, mask = ([a[b] for b in range(batch_size)] for a in (yx_min, yx_max, cls, iou, mask))\n            yx_min, yx_max, cls = ([a[m] for a, m in zip(l, mask)] for l in (yx_min, yx_max, cls))\n            canvas = [self.draw_bbox(canvas, yx_min.astype(np.int), yx_max.astype(np.int), cls, colors=colors) for canvas, yx_min, yx_max, cls in zip(np.copy(canvas_share), yx_min, yx_max, cls)]\n            iou = [np.reshape(a, [rows, cols]) for a in iou]\n            canvas = [self.draw_feature(_canvas, iou) for _canvas, iou in zip(canvas, iou)]\n            results.append(canvas)\n        return results\n\n    def copy_histogram(self, **kwargs):\n        return {\n            \'step\': kwargs[\'step\'],\n            \'state_dict\': self.env.dnn.state_dict(),\n        }\n\n    def summary_histogram(self, **kwargs):\n        step, state_dict = (kwargs[key] for key in \'step, state_dict\'.split(\', \'))\n        for name, var in state_dict.items():\n            if self.histogram_parameters(name):\n                self.writer.add_histogram(name, var, step)\n\n\nclass Train(object):\n    def __init__(self, args, config):\n        self.args = args\n        self.config = config\n        self.model_dir = utils.get_model_dir(config)\n        self.cache_dir = utils.get_cache_dir(config)\n        self.category = utils.get_category(config, self.cache_dir)\n        self.anchors = torch.from_numpy(utils.get_anchors(config)).contiguous()\n        logging.info(\'use cache directory \' + self.cache_dir)\n        logging.info(\'tensorboard --logdir \' + self.model_dir)\n        if args.delete:\n            logging.warning(\'delete model directory: \' + self.model_dir)\n            shutil.rmtree(self.model_dir, ignore_errors=True)\n        os.makedirs(self.model_dir, exist_ok=True)\n        with open(self.model_dir + \'.ini\', \'w\') as f:\n            config.write(f)\n\n        self.step, self.epoch, self.dnn = self.load()\n        self.inference = model.Inference(self.config, self.dnn, self.anchors)\n        logging.info(humanize.naturalsize(sum(var.cpu().numpy().nbytes for var in self.inference.state_dict().values())))\n        if self.args.finetune:\n            path = os.path.expanduser(os.path.expandvars(self.args.finetune))\n            logging.info(\'finetune from \' + path)\n            self.finetune(self.dnn, path)\n        self.inference = ensure_model(self.inference)\n        self.inference.train()\n        self.optimizer = eval(self.config.get(\'train\', \'optimizer\'))(filter(lambda p: p.requires_grad, self.inference.parameters()), self.args.learning_rate)\n\n        self.saver = utils.train.Saver(self.model_dir, config.getint(\'save\', \'keep\'))\n        self.timer_save = utils.train.Timer(config.getfloat(\'save\', \'secs\'), False)\n        try:\n            self.timer_eval = utils.train.Timer(eval(config.get(\'eval\', \'secs\')), config.getboolean(\'eval\', \'first\'))\n        except configparser.NoOptionError:\n            self.timer_eval = lambda: False\n        self.summary_worker = SummaryWorker(self)\n        self.summary_worker.start()\n\n    def stop(self):\n        self.summary_worker.stop()\n        self.summary_worker.join()\n\n    def get_loader(self):\n        paths = [os.path.join(self.cache_dir, phase + \'.pkl\') for phase in self.config.get(\'train\', \'phase\').split()]\n        dataset = utils.data.Dataset(\n            utils.data.load_pickles(paths),\n            transform=transform.augmentation.get_transform(self.config, self.config.get(\'transform\', \'augmentation\').split()),\n            one_hot=None if self.config.getboolean(\'train\', \'cross_entropy\') else len(self.category),\n            shuffle=self.config.getboolean(\'data\', \'shuffle\'),\n            dir=os.path.join(self.model_dir, \'exception\'),\n        )\n        logging.info(\'num_examples=%d\' % len(dataset))\n        try:\n            workers = self.config.getint(\'data\', \'workers\')\n            if torch.cuda.is_available():\n                workers = workers * torch.cuda.device_count()\n        except configparser.NoOptionError:\n            workers = multiprocessing.cpu_count()\n        collate_fn = utils.data.Collate(\n            transform.parse_transform(self.config, self.config.get(\'transform\', \'resize_train\')),\n            utils.train.load_sizes(self.config),\n            maintain=self.config.getint(\'data\', \'maintain\'),\n            transform_image=transform.get_transform(self.config, self.config.get(\'transform\', \'image_train\').split()),\n            transform_tensor=transform.get_transform(self.config, self.config.get(\'transform\', \'tensor\').split()),\n            dir=os.path.join(self.model_dir, \'exception\'),\n        )\n        return torch.utils.data.DataLoader(dataset, batch_size=self.args.batch_size * torch.cuda.device_count() if torch.cuda.is_available() else self.args.batch_size, shuffle=True, num_workers=workers, collate_fn=collate_fn, pin_memory=torch.cuda.is_available())\n\n    def load(self):\n        try:\n            path, step, epoch = utils.train.load_model(self.model_dir)\n            state_dict = torch.load(path, map_location=lambda storage, loc: storage)\n            config_channels = model.ConfigChannels(self.config, state_dict)\n        except (FileNotFoundError, ValueError):\n            step, epoch = 0, 0\n            config_channels = model.ConfigChannels(self.config)\n        dnn = utils.parse_attr(self.config.get(\'model\', \'dnn\'))(config_channels, self.anchors, len(self.category))\n        if config_channels.state_dict is not None:\n            dnn.load_state_dict(config_channels.state_dict)\n        return step, epoch, dnn\n\n    def finetune(self, model, path):\n        if os.path.isdir(path):\n            path, _step, _epoch = utils.train.load_model(path)\n        _state_dict = torch.load(path, map_location=lambda storage, loc: storage)\n        state_dict = model.state_dict()\n        ignore = utils.RegexList(self.args.ignore)\n        for key, value in state_dict.items():\n            try:\n                if not ignore(key):\n                    state_dict[key] = _state_dict[key]\n            except KeyError:\n                logging.warning(\'%s not in finetune file %s\' % (key, path))\n        model.load_state_dict(state_dict)\n\n    def iterate(self, data):\n        for key in data:\n            t = data[key]\n            if torch.is_tensor(t):\n                data[key] = utils.ensure_device(t)\n        tensor = torch.autograd.Variable(data[\'tensor\'])\n        pred = pybenchmark.profile(\'inference\')(model._inference)(self.inference, tensor)\n        height, width = data[\'image\'].size()[1:3]\n        rows, cols = pred[\'feature\'].size()[-2:]\n        loss, debug = pybenchmark.profile(\'loss\')(model.loss)(self.anchors, norm_data(data, height, width, rows, cols), pred, self.config.getfloat(\'model\', \'threshold\'))\n        loss_hparam = {key: loss[key] * self.config.getfloat(\'hparam\', key) for key in loss}\n        loss_total = sum(loss_hparam.values())\n        self.optimizer.zero_grad()\n        loss_total.backward()\n        try:\n            clip = self.config.getfloat(\'train\', \'clip\')\n            nn.utils.clip_grad_norm(self.inference.parameters(), clip)\n        except configparser.NoOptionError:\n            pass\n        self.optimizer.step()\n        return dict(\n            height=height, width=width, rows=rows, cols=cols,\n            data=data, pred=pred, debug=debug,\n            loss_total=loss_total, loss=loss, loss_hparam=loss_hparam,\n        )\n\n    def __call__(self):\n        with filelock.FileLock(os.path.join(self.model_dir, \'lock\'), 0):\n            try:\n                try:\n                    scheduler = eval(self.config.get(\'train\', \'scheduler\'))(self.optimizer)\n                except configparser.NoOptionError:\n                    scheduler = None\n                loader = self.get_loader()\n                logging.info(\'num_workers=%d\' % loader.num_workers)\n                step = self.step\n                for epoch in range(0 if self.epoch is None else self.epoch, self.args.epoch):\n                    if scheduler is not None:\n                        scheduler.step(epoch)\n                        logging.info(\'epoch=%d, lr=%s\' % (epoch, str(scheduler.get_lr())))\n                    for data in loader if self.args.quiet else tqdm.tqdm(loader, desc=\'epoch=%d/%d\' % (epoch, self.args.epoch)):\n                        kwargs = self.iterate(data)\n                        step += 1\n                        kwargs = {**kwargs, **dict(step=step, epoch=epoch)}\n                        self.summary_worker(\'scalar\', **kwargs)\n                        self.summary_worker(\'image\', **kwargs)\n                        self.summary_worker(\'histogram\', **kwargs)\n                        if self.timer_save():\n                            self.save(**kwargs)\n                        if self.timer_eval():\n                            self.eval(**kwargs)\n                self.save(**kwargs)\n                logging.info(\'finished\')\n            except KeyboardInterrupt:\n                logging.warning(\'interrupted\')\n                self.save(**kwargs)\n            except:\n                traceback.print_exc()\n                try:\n                    with open(os.path.join(self.model_dir, \'data.pkl\'), \'wb\') as f:\n                        pickle.dump(data, f)\n                except UnboundLocalError:\n                    pass\n                raise\n            finally:\n                self.stop()\n\n    def check_nan(self, **kwargs):\n        step, loss_total, loss, data = (kwargs[key] for key in \'step, loss_total, loss, data\'.split(\', \'))\n        if np.isnan(loss_total.data.cpu()[0]):\n            dump_dir = os.path.join(self.model_dir, str(step))\n            os.makedirs(dump_dir, exist_ok=True)\n            torch.save(collections.OrderedDict([(key, var.cpu()) for key, var in self.dnn.state_dict().items()]), os.path.join(dump_dir, \'model.pth\'))\n            torch.save(data, os.path.join(dump_dir, \'data.pth\'))\n            for key, l in loss.items():\n                logging.warning(\'%s=%f\' % (key, l.data.cpu()[0]))\n            raise OverflowError(\'NaN loss detected, dump runtime information into \' + dump_dir)\n\n    def save(self, **kwargs):\n        step, epoch = (kwargs[key] for key in \'step, epoch\'.split(\', \'))\n        self.check_nan(**kwargs)\n        self.saver(collections.OrderedDict([(key, var.cpu()) for key, var in self.dnn.state_dict().items()]), step, epoch)\n\n    def eval(self, **kwargs):\n        logging.info(\'evaluating\')\n        if torch.cuda.is_available():\n            self.inference.cpu()\n        try:\n            e = _eval.Eval(self.args, self.config)\n            cls_ap = e()\n            self.backup_best(cls_ap, e.path)\n        except:\n            traceback.print_exc()\n        if torch.cuda.is_available():\n            self.inference.cuda()\n\n    def backup_best(self, cls_ap, path):\n        try:\n            with open(self.model_dir + \'.pkl\', \'rb\') as f:\n                best = np.mean(list(pickle.load(f).values()))\n        except:\n            best = np.finfo(np.float32).min\n        metric = np.mean(list(cls_ap.values()))\n        if metric > best:\n            with open(self.model_dir + \'.pkl\', \'wb\') as f:\n                pickle.dump(cls_ap, f)\n            shutil.copy(path, self.model_dir + \'.pth\')\n            logging.info(\'best model (%f) saved into %s.*\' % (metric, self.model_dir))\n        else:\n            logging.info(\'best metric %f >= %f\' % (best, metric))\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    if args.run is None:\n        buffer = io.StringIO()\n        config.write(buffer)\n        args.run = hashlib.md5(buffer.getvalue().encode()).hexdigest()\n    logging.info(\'cd \' + os.getcwd() + \' && \' + subprocess.list2cmdline([sys.executable] + sys.argv))\n    train = Train(args, config)\n    train()\n    logging.info(pybenchmark.stats)\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'-b\', \'--batch_size\', default=16, type=int, help=\'batch size\')\n    parser.add_argument(\'-f\', \'--finetune\')\n    parser.add_argument(\'-i\', \'--ignore\', nargs=\'+\', default=[], help=\'regex to ignore weights while fintuning\')\n    parser.add_argument(\'-lr\', \'--learning_rate\', default=1e-3, type=float, help=\'learning rate\')\n    parser.add_argument(\'-e\', \'--epoch\', type=int, default=np.iinfo(np.int).max)\n    parser.add_argument(\'-d\', \'--delete\', action=\'store_true\', help=\'delete model\')\n    parser.add_argument(\'-q\', \'--quiet\', action=\'store_true\', help=\'quiet mode\')\n    parser.add_argument(\'-r\', \'--run\', help=\'the run name in TensorBoard\')\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
variable_stat.py,1,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport argparse\nimport configparser\nimport logging\nimport logging.config\nimport importlib\nimport inspect\nimport inflection\nimport yaml\n\nimport numpy as np\nimport torch\nimport humanize\nimport xlsxwriter\n\nimport utils\nimport utils.train\nimport utils.channel\n\n\nclass Name(object):\n    def __call__(self, name, variable):\n        return name\n\n\nclass Size(object):\n    def __call__(self, name, variable):\n        return \'x\'.join(map(str, variable.size()))\n\n\nclass Bytes(object):\n    def __call__(self, name, variable):\n        return variable.numpy().nbytes\n\n    def format(self, workbook, worksheet, num, col):\n        worksheet.conditional_format(1, col, num, col, {\'type\': \'data_bar\', \'bar_color\': \'#FFC7CE\'})\n\n\nclass BytesNatural(object):\n    def __call__(self, name, variable):\n        return humanize.naturalsize(variable.numpy().nbytes)\n\n\nclass MeanDense(object):\n    def __call__(self, name, variable):\n        return np.mean(utils.channel.dense(variable))\n\n    def format(self, workbook, worksheet, num, col):\n        worksheet.conditional_format(1, col, num, col, {\'type\': \'data_bar\', \'bar_color\': \'#FFC7CE\'})\n\n\nclass Rank(object):\n    def __call__(self, name, variable):\n        return len(variable.size())\n\n    def format(self, workbook, worksheet, num, col):\n        worksheet.conditional_format(1, col, num, col, {\'type\': \'data_bar\', \'bar_color\': \'#FFC7CE\'})\n\n\ndef main():\n    args = make_args()\n    config = configparser.ConfigParser()\n    utils.load_config(config, args.config)\n    for cmd in args.modify:\n        utils.modify_config(config, cmd)\n    with open(os.path.expanduser(os.path.expandvars(args.logging)), \'r\') as f:\n        logging.config.dictConfig(yaml.load(f))\n    model_dir = utils.get_model_dir(config)\n    path, step, epoch = utils.train.load_model(model_dir)\n    state_dict = torch.load(path, map_location=lambda storage, loc: storage)\n    mapper = [(inflection.underscore(name), member()) for name, member in inspect.getmembers(importlib.machinery.SourceFileLoader(\'\', __file__).load_module()) if inspect.isclass(member)]\n    path = os.path.join(model_dir, os.path.basename(os.path.splitext(__file__)[0])) + \'.xlsx\'\n    with xlsxwriter.Workbook(path, {\'strings_to_urls\': False, \'nan_inf_to_errors\': True}) as workbook:\n        worksheet = workbook.add_worksheet(args.worksheet)\n        for j, (key, m) in enumerate(mapper):\n            worksheet.write(0, j, key)\n            for i, (name, variable) in enumerate(state_dict.items()):\n                value = m(name, variable)\n                worksheet.write(1 + i, j, value)\n            if hasattr(m, \'format\'):\n                m.format(workbook, worksheet, i, j)\n        worksheet.autofilter(0, 0, i, len(mapper) - 1)\n        worksheet.freeze_panes(1, 0)\n    logging.info(path)\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', nargs=\'+\', default=[\'config.ini\'], help=\'config file\')\n    parser.add_argument(\'-m\', \'--modify\', nargs=\'+\', default=[], help=\'modify config\')\n    parser.add_argument(\'--logging\', default=\'logging.yml\', help=\'logging config\')\n    parser.add_argument(\'--worksheet\', default=\'sheet\')\n    parser.add_argument(\'--nohead\', action=\'store_true\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
video2image.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport sys\nimport argparse\n\nimport pymediainfo\nimport tqdm\nfrom contextlib import closing\nimport videosequence\n\n\ndef get_step(frames, video_track, **kwargs):\n    if \'frames\' in kwargs:\n        step = len(frames) // kwargs[\'frames\']\n    elif \'frames_per_sec\' in kwargs > 0:\n        frame_rate = float(video_track.frame_rate)\n        step = int(frame_rate / kwargs[\'frames_per_sec\'])\n    assert step > 0\n    return step\n\n\ndef convert(video_file, image_prefix, **kwargs):\n    media_info = pymediainfo.MediaInfo.parse(video_file)\n    video_tracks = [track for track in media_info.tracks if track.track_type == \'Video\']\n    if len(video_tracks) < 1:\n        raise videosequence.VideoError()\n    video_track = video_tracks[0]\n    _rotation = float(video_track.rotation)\n    rotation = int(_rotation)\n    assert rotation - _rotation == 0\n    with closing(videosequence.VideoSequence(video_file)) as frames:\n        step = get_step(frames, video_track, **kwargs)\n        _frames = frames[::step]\n        for idx, frame in enumerate(tqdm.tqdm(_frames)):\n            frame = frame.rotate(-rotation, expand=True)\n            frame.save(\'%s_%04d.jpg\' % (image_prefix, idx))\n\n\ndef main():\n    args = make_args()\n    src = os.path.expanduser(os.path.expandvars(args.src))\n    dst = os.path.expanduser(os.path.expandvars(args.dst))\n    os.makedirs(dst, exist_ok=True)\n    kwargs = {}\n    if args.frames > 0:\n        kwargs[\'frames\'] = args.frames\n    elif args.frames_per_sec > 0:\n        kwargs[\'frames_per_sec\'] = args.frames_per_sec\n    exts = set()\n    for dirpath, _, filenames in os.walk(src):\n        for filename in filenames:\n            ext = os.path.splitext(filename)[-1].lower()\n            if ext in args.ext:\n                path = os.path.join(dirpath, filename)\n                print(path)\n                name = os.path.relpath(path, src).replace(os.path.sep, args.replace)\n                _path = os.path.join(dst, name)\n                try:\n                    convert(path, _path, **kwargs)\n                except videosequence.VideoError as e:\n                    sys.stderr.write(str(e) + \'\\n\')\n            else:\n                exts.add(ext)\n    print(exts)\n\n\ndef make_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'src\')\n    parser.add_argument(\'dst\')\n    parser.add_argument(\'-e\', \'--ext\', nargs=\'+\', default=[\'.mp4\', \'.mov\', \'.m4v\'])\n    parser.add_argument(\'-r\', \'--replace\', default=\'_\', help=\'replace the path separator into the given character\')\n    parser.add_argument(\'-f\', \'--frames\', default=0, type=int, help=\'total output frames in a video\')\n    parser.add_argument(\'--frames_per_sec\', default=0, type=int, help=\'output frames in a second\')\n    return parser.parse_args()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
cache/__init__.py,0,b''
cache/coco.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport logging\nimport configparser\n\nimport numpy as np\nimport pandas as pd\nimport tqdm\nimport pycocotools.coco\nimport cv2\n\nimport utils.cache\n\n\ndef cache(config, path, category_index):\n    phase = os.path.splitext(os.path.basename(path))[0]\n    data = []\n    for i, row in pd.read_csv(os.path.splitext(__file__)[0] + \'.tsv\', sep=\'\\t\').iterrows():\n        logging.info(\'loading data %d (%s)\' % (i, \', \'.join([k + \'=\' + str(v) for k, v in row.items()])))\n        root = os.path.expanduser(os.path.expandvars(row[\'root\']))\n        year = str(row[\'year\'])\n        suffix = phase + year\n        path = os.path.join(root, \'annotations\', \'instances_%s.json\' % suffix)\n        if not os.path.exists(path):\n            logging.warning(path + \' not exists\')\n            continue\n        coco = pycocotools.coco.COCO(path)\n        catIds = coco.getCatIds(catNms=list(category_index.keys()))\n        cats = coco.loadCats(catIds)\n        id_index = dict((cat[\'id\'], category_index[cat[\'name\']]) for cat in cats)\n        imgIds = coco.getImgIds()\n        path = os.path.join(root, suffix)\n        imgs = coco.loadImgs(imgIds)\n        _imgs = list(filter(lambda img: os.path.exists(os.path.join(path, img[\'file_name\'])), imgs))\n        if len(imgs) > len(_imgs):\n            logging.warning(\'%d of %d images not exists\' % (len(imgs) - len(_imgs), len(imgs)))\n        for img in tqdm.tqdm(_imgs):\n            annIds = coco.getAnnIds(imgIds=img[\'id\'], catIds=catIds, iscrowd=None)\n            anns = coco.loadAnns(annIds)\n            if len(anns) <= 0:\n                continue\n            path = os.path.join(path, img[\'file_name\'])\n            width, height = img[\'width\'], img[\'height\']\n            bbox = np.array([ann[\'bbox\'] for ann in anns], dtype=np.float32)\n            yx_min = bbox[:, 1::-1]\n            hw = bbox[:, -1:1:-1]\n            yx_max = yx_min + hw\n            cls = np.array([id_index[ann[\'category_id\']] for ann in anns], dtype=np.int)\n            difficult = np.zeros(cls.shape, dtype=np.uint8)\n            try:\n                if config.getboolean(\'cache\', \'verify\'):\n                    size = (height, width)\n                    image = cv2.imread(path)\n                    assert image is not None\n                    assert image.shape[:2] == size[:2]\n                    utils.cache.verify_coords(yx_min, yx_max, size[:2])\n            except configparser.NoOptionError:\n                pass\n            assert len(yx_min) == len(cls)\n            assert yx_min.shape == yx_max.shape\n            assert len(yx_min.shape) == 2 and yx_min.shape[-1] == 2\n            data.append(dict(path=path, yx_min=yx_min, yx_max=yx_max, cls=cls, difficult=difficult))\n        logging.warning(\'%d of %d images are saved\' % (len(data), len(_imgs)))\n    return data\n'"
cache/voc.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport logging\nimport configparser\n\nimport numpy as np\nimport tqdm\nimport xml.etree.ElementTree\nimport cv2\n\nimport utils.cache\n\n\ndef load_annotation(path, category_index):\n    tree = xml.etree.ElementTree.parse(path)\n    yx_min = []\n    yx_max = []\n    cls = []\n    difficult = []\n    for obj in tree.findall(\'object\'):\n        try:\n            cls.append(category_index[obj.find(\'name\').text])\n        except KeyError:\n            continue\n        bbox = obj.find(\'bndbox\')\n        ymin = float(bbox.find(\'ymin\').text) - 1\n        xmin = float(bbox.find(\'xmin\').text) - 1\n        ymax = float(bbox.find(\'ymax\').text) - 1\n        xmax = float(bbox.find(\'xmax\').text) - 1\n        assert ymin < ymax\n        assert xmin < xmax\n        yx_min.append((ymin, xmin))\n        yx_max.append((ymax, xmax))\n        difficult.append(int(obj.find(\'difficult\').text))\n    size = tree.find(\'size\')\n    return tree.find(\'filename\').text, (int(size.find(\'height\').text), int(size.find(\'width\').text), int(size.find(\'depth\').text)), yx_min, yx_max, cls, difficult\n\n\ndef load_root():\n    with open(os.path.splitext(__file__)[0] + \'.txt\', \'r\') as f:\n        return [line.rstrip() for line in f]\n\n\ndef cache(config, path, category_index, root=load_root()):\n    phase = os.path.splitext(os.path.basename(path))[0]\n    data = []\n    for root in root:\n        logging.info(\'loading \' + root)\n        root = os.path.expanduser(os.path.expandvars(root))\n        path = os.path.join(root, \'ImageSets\', \'Main\', phase) + \'.txt\'\n        if not os.path.exists(path):\n            logging.warning(path + \' not exists\')\n            continue\n        with open(path, \'r\') as f:\n            filenames = [line.strip() for line in f]\n        for filename in tqdm.tqdm(filenames):\n            filename, size, yx_min, yx_max, cls, difficult = load_annotation(os.path.join(root, \'Annotations\', filename + \'.xml\'), category_index)\n            if len(cls) <= 0:\n                continue\n            path = os.path.join(root, \'JPEGImages\', filename)\n            yx_min = np.array(yx_min, dtype=np.float32)\n            yx_max = np.array(yx_max, dtype=np.float32)\n            cls = np.array(cls, dtype=np.int)\n            difficult = np.array(difficult, dtype=np.uint8)\n            assert len(yx_min) == len(cls)\n            assert yx_min.shape == yx_max.shape\n            assert len(yx_min.shape) == 2 and yx_min.shape[-1] == 2\n            try:\n                if config.getboolean(\'cache\', \'verify\'):\n                    try:\n                        image = cv2.imread(path)\n                        assert image is not None\n                        assert image.shape[:2] == size[:2]\n                        utils.cache.verify_coords(yx_min, yx_max, size[:2])\n                    except AssertionError as e:\n                        logging.error(path + \': \' + str(e))\n                        continue\n            except configparser.NoOptionError:\n                pass\n            data.append(dict(path=path, yx_min=yx_min, yx_max=yx_max, cls=cls, difficult=difficult))\n        logging.info(\'%d of %d images are saved\' % (len(data), len(filenames)))\n    return data\n'"
config/eval.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport configparser\n\nimport numpy as np\nimport humanize\nimport pybenchmark\n\n\nclass Timestamp(object):\n    def __call__(self, env, **kwargs):\n        return float(env.now.timestamp())\n\n\nclass Time(object):\n    def __call__(self, env, **kwargs):\n        return env.now.strftime(\'%Y-%m-%d %H:%M:%S\')\n\n    def get_format(self, workbook, worksheet):\n        return workbook.add_format({\'num_format\': \'yyyy-mm-dd hh:mm:ss\'})\n\n\nclass Step(object):\n    def __call__(self, env, **kwargs):\n        return env.step\n\n\nclass Epoch(object):\n    def __call__(self, env, **kwargs):\n        return env.epoch\n\n\nclass Model(object):\n    def __call__(self, env, **kwargs):\n        return env.config.get(\'model\', \'dnn\')\n\n\nclass SizeDnn(object):\n    def __call__(self, env, **kwargs):\n        return sum(var.cpu().numpy().nbytes for var in env.inference.state_dict().values())\n\n    def format(self, workbook, worksheet, num, col):\n        worksheet.conditional_format(1, col, num + 1, col, {\'type\': \'data_bar\', \'bar_color\': \'#FFC7CE\'})\n\n\nclass SizeDnnNature(object):\n    def __call__(self, env, **kwargs):\n        return humanize.naturalsize(sum(var.cpu().numpy().nbytes for var in env.inference.state_dict().values()))\n\n\nclass TimeInference(object):\n    def __call__(self, env, **kwargs):\n        return pybenchmark.stats[\'inference\'][\'time\']\n\n    def format(self, workbook, worksheet, num, col):\n        worksheet.conditional_format(1, col, num + 1, col, {\'type\': \'data_bar\', \'bar_color\': \'#FFC7CE\'})\n\n\nclass Root(object):\n    def __call__(self, env, **kwargs):\n        return os.path.basename(env.config.get(\'config\', \'root\'))\n\n\nclass CacheName(object):\n    def __call__(self, env, **kwargs):\n        return env.config.get(\'cache\', \'name\')\n\n\nclass ModelName(object):\n    def __call__(self, env, **kwargs):\n        return env.config.get(\'model\', \'name\')\n\n\nclass Category(object):\n    def __call__(self, env, **kwargs):\n        return env.config.get(\'cache\', \'category\')\n\n\nclass DatasetSize(object):\n    def __call__(self, env, **kwargs):\n        return len(env.loader.dataset)\n\n    def format(self, workbook, worksheet, num, col):\n        worksheet.conditional_format(1, col, num + 1, col, {\'type\': \'data_bar\', \'bar_color\': \'#FFC7CE\'})\n\n\nclass DetectThreshold(object):\n    def __call__(self, env, **kwargs):\n        return env.config.getfloat(\'detect\', \'threshold\')\n\n    def format(self, workbook, worksheet, num, col):\n        worksheet.conditional_format(1, col, num + 1, col, {\'type\': \'data_bar\', \'bar_color\': \'#FFC7CE\'})\n\n\nclass DetectThresholdCls(object):\n    def __call__(self, env, **kwargs):\n        return env.config.getfloat(\'detect\', \'threshold_cls\')\n\n    def format(self, workbook, worksheet, num, col):\n        worksheet.conditional_format(1, col, num + 1, col, {\'type\': \'data_bar\', \'bar_color\': \'#FFC7CE\'})\n\n\nclass DetectFix(object):\n    def __call__(self, env, **kwargs):\n        return env.config.getboolean(\'detect\', \'fix\')\n\n    def format(self, workbook, worksheet, num, col):\n        format_green = workbook.add_format({\'bg_color\': \'#C6EFCE\', \'font_color\': \'#006100\'})\n        format_red = workbook.add_format({\'bg_color\': \'#FFC7CE\', \'font_color\': \'#9C0006\'})\n        worksheet.conditional_format(1, col, num + 1, col, {\'type\': \'cell\', \'criteria\': \'==\', \'value\': \'1\', \'format\': format_green})\n        worksheet.conditional_format(1, col, num + 1, col, {\'type\': \'cell\', \'criteria\': \'<>\', \'value\': \'1\', \'format\': format_red})\n\n\nclass DetectOverlap(object):\n    def __call__(self, env, **kwargs):\n        return env.config.getfloat(\'detect\', \'overlap\')\n\n    def format(self, workbook, worksheet, num, col):\n        worksheet.conditional_format(1, col, num + 1, col, {\'type\': \'data_bar\', \'bar_color\': \'#FFC7CE\'})\n\n\nclass EvalIou(object):\n    def __call__(self, env, **kwargs):\n        return env.config.getfloat(\'eval\', \'iou\')\n\n    def format(self, workbook, worksheet, num, col):\n        worksheet.conditional_format(1, col, num + 1, col, {\'type\': \'data_bar\', \'bar_color\': \'#FFC7CE\'})\n\n\nclass EvalMeanAp(object):\n    def __call__(self, env, **kwargs):\n        return np.mean(list(kwargs[\'cls_ap\'].values()))\n\n    def format(self, workbook, worksheet, num, col):\n        worksheet.conditional_format(1, col, num + 1, col, {\'type\': \'data_bar\', \'bar_color\': \'#FFC7CE\'})\n\n\nclass EvalAp(object):\n    def __call__(self, env, **kwargs):\n        cls_ap = kwargs[\'cls_ap\']\n        return \', \'.join([\'%s=%f\' % (env.category[c], cls_ap[c]) for c in sorted(cls_ap.keys())])\n\n\nclass Hparam(object):\n    def __call__(self, env, **kwargs):\n        try:\n            return \', \'.join([option + \'=\' + value for option, value in env._config.items(\'hparam\')])\n        except AttributeError:\n            return None\n\n\nclass Optimizer(object):\n    def __call__(self, env, **kwargs):\n        try:\n            return env._config.get(\'train\', \'optimizer\')\n        except (AttributeError, configparser.NoOptionError):\n            return None\n\n\nclass Scheduler(object):\n    def __call__(self, env, **kwargs):\n        try:\n            return env._config.get(\'train\', \'scheduler\')\n        except (AttributeError, configparser.NoOptionError):\n            return None\n'"
model/__init__.py,28,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport logging\n\nimport numpy as np\nimport torch\nimport torch.autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport utils.iou.torch\n\n\nclass ConfigChannels(object):\n    def __init__(self, config, state_dict=None, channels=3):\n        self.config = config\n        self.state_dict = state_dict\n        self.channels = channels\n\n    def __call__(self, default, name, fn=lambda var: var.size(0)):\n        if self.state_dict is None:\n            self.channels = default\n        else:\n            var = self.state_dict[name]\n            self.channels = fn(var)\n            if self.channels != default:\n                logging.warning(\'%s: change number of output channels from %d to %d\' % (name, default, self.channels))\n        return self.channels\n\n\ndef output_channels(num_anchors, num_cls):\n    if num_cls > 1:\n        return num_anchors * (5 + num_cls)\n    else:\n        return num_anchors * 5\n\n\ndef meshgrid(rows, cols, swap=False):\n    i = torch.arange(0, rows).repeat(cols).view(-1, 1)\n    j = torch.arange(0, cols).view(-1, 1).repeat(1, rows).view(-1, 1)\n    return torch.cat([i, j], 1) if swap else torch.cat([j, i], 1)\n\n\ndef iou_match(yx_min, yx_max, data):\n    batch_size, cells, num_anchors, _ = yx_min.size()\n    iou_matrix = utils.iou.torch.batch_iou_matrix(yx_min.view(batch_size, -1, 2), yx_max.view(batch_size, -1, 2), data[\'yx_min\'], data[\'yx_max\'])\n    iou_matrix = iou_matrix.view(batch_size, cells, num_anchors, -1)\n    iou, index = iou_matrix.max(-1)\n    _index = torch.unbind(index.view(batch_size, -1))\n    _data = {}\n    for key in \'yx_min, yx_max, cls\'.split(\', \'):\n        t = data[key]\n        if len(t.size()) == 2:\n            t = torch.stack([d[i] for d, i in zip(torch.unbind(t, 0), _index)]).view(batch_size, cells, num_anchors)\n        elif len(t.size()) == 3:\n            t = torch.stack([d[i] for d, i in zip(torch.unbind(t, 0), _index)]).view(batch_size, cells, num_anchors, -1)\n        _data[key] = t\n    return iou_matrix, iou, index, _data\n\n\ndef fit_positive(rows, cols, yx_min, yx_max, anchors):\n    device_id = anchors.get_device() if torch.cuda.is_available() else None\n    batch_size, num, _ = yx_min.size()\n    num_anchors, _ = anchors.size()\n    valid = torch.prod(yx_min < yx_max, -1)\n    center = (yx_min + yx_max) / 2\n    ij = torch.floor(center)\n    i, j = torch.unbind(ij.long(), -1)\n    index = i * cols + j\n    anchors2 = anchors / 2\n    iou_matrix = utils.iou.torch.iou_matrix((yx_min - center).view(-1, 2), (yx_max - center).view(-1, 2), -anchors2, anchors2).view(batch_size, -1, num_anchors)\n    iou, index_anchor = iou_matrix.max(-1)\n    _positive = []\n    cells = rows * cols\n    for valid, index, index_anchor in zip(torch.unbind(valid), torch.unbind(index), torch.unbind(index_anchor)):\n        index, index_anchor = (t[valid] for t in (index, index_anchor))\n        t = utils.ensure_device(torch.ByteTensor(cells, num_anchors).zero_(), device_id)\n        t[index, index_anchor] = 1\n        _positive.append(t)\n    return torch.stack(_positive)\n\n\ndef fill_norm(yx_min, yx_max, anchors):\n    center = (yx_min + yx_max) / 2\n    ij = torch.floor(center)\n    center_offset = center - ij\n    size = yx_max - yx_min\n    return center_offset, torch.log(size / anchors.view(1, -1, 2))\n\n\ndef square(t):\n    return t * t\n\n\nclass Inference(nn.Module):\n    def __init__(self, config, dnn, anchors):\n        nn.Module.__init__(self)\n        self.config = config\n        self.dnn = dnn\n        self.anchors = anchors\n\n    def forward(self, x):\n        device_id = x.get_device() if torch.cuda.is_available() else None\n        feature = self.dnn(x)\n        rows, cols = feature.size()[-2:]\n        cells = rows * cols\n        _feature = feature.permute(0, 2, 3, 1).contiguous().view(feature.size(0), cells, self.anchors.size(0), -1)\n        sigmoid = F.sigmoid(_feature[:, :, :, :3])\n        iou = sigmoid[:, :, :, 0]\n        ij = torch.autograd.Variable(utils.ensure_device(meshgrid(rows, cols).view(1, -1, 1, 2), device_id))\n        center_offset = sigmoid[:, :, :, 1:3]\n        center = ij + center_offset\n        size_norm = _feature[:, :, :, 3:5]\n        anchors = torch.autograd.Variable(utils.ensure_device(self.anchors.view(1, 1, -1, 2), device_id))\n        size = torch.exp(size_norm) * anchors\n        size2 = size / 2\n        yx_min = center - size2\n        yx_max = center + size2\n        logits = _feature[:, :, :, 5:] if _feature.size(-1) > 5 else None\n        return feature, iou, center_offset, size_norm, yx_min, yx_max, logits\n\n\ndef loss(anchors, data, pred, threshold):\n    iou = pred[\'iou\']\n    device_id = iou.get_device() if torch.cuda.is_available() else None\n    rows, cols = pred[\'feature\'].size()[-2:]\n    iou_matrix, _iou, _, _data = iou_match(pred[\'yx_min\'].data, pred[\'yx_max\'].data, data)\n    anchors = utils.ensure_device(anchors, device_id)\n    positive = fit_positive(rows, cols, *(data[key] for key in \'yx_min, yx_max\'.split(\', \')), anchors)\n    negative = ~positive & (_iou < threshold)\n    _center_offset, _size_norm = fill_norm(*(_data[key] for key in \'yx_min, yx_max\'.split(\', \')), anchors)\n    positive, negative, _iou, _center_offset, _size_norm, _cls = (torch.autograd.Variable(t) for t in (positive, negative, _iou, _center_offset, _size_norm, _data[\'cls\']))\n    _positive = torch.unsqueeze(positive, -1)\n    loss = {}\n    # iou\n    loss[\'foreground\'] = F.mse_loss(iou[positive], _iou[positive], size_average=False)\n    loss[\'background\'] = torch.sum(square(iou[negative]))\n    # bbox\n    loss[\'center\'] = F.mse_loss(pred[\'center_offset\'][_positive], _center_offset[_positive], size_average=False)\n    loss[\'size\'] = F.mse_loss(pred[\'size_norm\'][_positive], _size_norm[_positive], size_average=False)\n    # cls\n    if \'logits\' in pred:\n        logits = pred[\'logits\']\n        if len(_cls.size()) > 3:\n            loss[\'cls\'] = F.mse_loss(F.softmax(logits, -1)[_positive], _cls[_positive], size_average=False)\n        else:\n            loss[\'cls\'] = F.cross_entropy(logits[_positive].view(-1, logits.size(-1)), _cls[positive].view(-1))\n    # normalize\n    cnt = float(np.multiply.reduce(positive.size()))\n    for key in loss:\n        loss[key] /= cnt\n    return loss, dict(iou=_iou, data=_data, positive=positive, negative=negative)\n\n\ndef _inference(inference, tensor):\n    feature, iou, center_offset, size_norm, yx_min, yx_max, logits = inference(tensor)\n    pred = dict(\n        feature=feature, iou=iou,\n        center_offset=center_offset, size_norm=size_norm,\n        yx_min=yx_min, yx_max=yx_max,\n    )\n    if logits is not None:\n        pred[\'logits\'] = logits.contiguous()\n    return pred\n'"
model/densenet.py,2,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport logging\nfrom collections import OrderedDict\n\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport torchvision.models.densenet as _model\nfrom torchvision.models.densenet import _DenseBlock, _Transition, model_urls\n\nimport model\n\n\nclass DenseNet(_model.DenseNet):\n    def __init__(self, config_channels, anchors, num_cls, growth_rate=32, block_config=(6, 12, 24, 16), num_init_features=64, bn_size=4, drop_rate=0):\n        nn.Module.__init__(self)\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            (\'norm0\', nn.BatchNorm2d(num_init_features)),\n            (\'relu0\', nn.ReLU(inplace=True)),\n            (\'pool0\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features, bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', nn.BatchNorm2d(num_features))\n        self.features.add_module(\'conv\', nn.Conv2d(num_features, model.output_channels(len(anchors), num_cls), 1))\n\n        # init\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight = nn.init.kaiming_normal(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        return self.features(x)\n\n\ndef densenet121(config_channels, anchors, num_cls, **kwargs):\n    model = DenseNet(config_channels, anchors, num_cls, num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16), **kwargs)\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = model_urls[\'densenet121\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef densenet169(config_channels, anchors, num_cls, **kwargs):\n    model = DenseNet(config_channels, anchors, num_cls, num_init_features=64, growth_rate=32, block_config=(6, 12, 32, 32), **kwargs)\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = model_urls[\'densenet169\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef densenet201(config_channels, anchors, num_cls, **kwargs):\n    model = DenseNet(config_channels, anchors, num_cls, num_init_features=64, growth_rate=32, block_config=(6, 12, 48, 32), **kwargs)\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = model_urls[\'densenet201\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef densenet161(config_channels, anchors, num_cls, **kwargs):\n    model = DenseNet(config_channels, anchors, num_cls, num_init_features=96, growth_rate=48, block_config=(6, 12, 36, 24), **kwargs)\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = model_urls[\'densenet161\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n'"
model/inception3.py,5,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport logging\n\nimport scipy.stats as stats\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.model_zoo\nimport torchvision.models.inception as _model\nfrom torchvision.models.inception import BasicConv2d, InceptionA, InceptionB, InceptionC, InceptionD, InceptionE\n\nimport model\n\n\nclass Inception3(_model.Inception3):\n    def __init__(self, config_channels, anchors, num_cls, transform_input=False):\n        nn.Module.__init__(self)\n        self.transform_input = transform_input\n        self.Conv2d_1a_3x3 = BasicConv2d(3, 32, kernel_size=3, stride=2)\n        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)\n        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n        self.Mixed_5b = InceptionA(192, pool_features=32)\n        self.Mixed_5c = InceptionA(256, pool_features=64)\n        self.Mixed_5d = InceptionA(288, pool_features=64)\n        self.Mixed_6a = InceptionB(288)\n        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n        # aux_logits\n        self.Mixed_7a = InceptionD(768)\n        self.Mixed_7b = InceptionE(1280)\n        self.Mixed_7c = InceptionE(2048)\n        self.conv = nn.Conv2d(2048, model.output_channels(len(anchors), num_cls), 1)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                stddev = m.stddev if hasattr(m, \'stddev\') else 0.1\n                X = stats.truncnorm(-2, 2, scale=stddev)\n                values = torch.Tensor(X.rvs(m.weight.data.numel()))\n                m.weight.data.copy_(values)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n        if config_channels.config.getboolean(\'model\', \'pretrained\'):\n            url = _model.model_urls[\'inception_v3_google\']\n            logging.info(\'use pretrained model: \' + url)\n            state_dict = self.state_dict()\n            for key, value in torch.utils.model_zoo.load_url(url).items():\n                if key in state_dict:\n                    state_dict[key] = value\n            self.load_state_dict(state_dict)\n\n    def forward(self, x):\n        if self.transform_input:\n            x = x.clone()\n            x[:, 0] = x[:, 0] * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x[:, 1] = x[:, 1] * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x[:, 2] = x[:, 2] * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n        # 299 x 299 x 3\n        x = self.Conv2d_1a_3x3(x)\n        # 149 x 149 x 32\n        x = self.Conv2d_2a_3x3(x)\n        # 147 x 147 x 32\n        x = self.Conv2d_2b_3x3(x)\n        # 147 x 147 x 64\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 73 x 73 x 64\n        x = self.Conv2d_3b_1x1(x)\n        # 73 x 73 x 80\n        x = self.Conv2d_4a_3x3(x)\n        # 71 x 71 x 192\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # 35 x 35 x 192\n        x = self.Mixed_5b(x)\n        # 35 x 35 x 256\n        x = self.Mixed_5c(x)\n        # 35 x 35 x 288\n        x = self.Mixed_5d(x)\n        # 35 x 35 x 288\n        x = self.Mixed_6a(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6b(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6c(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6d(x)\n        # 17 x 17 x 768\n        x = self.Mixed_6e(x)\n        # 17 x 17 x 768\n        # aux_logits\n        # 17 x 17 x 768\n        x = self.Mixed_7a(x)\n        # 8 x 8 x 1280\n        x = self.Mixed_7b(x)\n        # 8 x 8 x 2048\n        x = self.Mixed_7c(x)\n        # 8 x 8 x 2048\n        return self.conv(x)\n'"
model/inception4.py,12,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport logging\nimport configparser\nimport collections.abc\n\nimport torch\nimport torch.nn as nn\nfrom pretrainedmodels.models.inceptionv4 import pretrained_settings\n\nimport model\n\n\nclass Conv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0, stride=1, bn=True, act=True):\n        nn.Module.__init__(self)\n        if isinstance(padding, bool):\n            if isinstance(kernel_size, collections.abc.Iterable):\n                padding = tuple((kernel_size - 1) // 2 for kernel_size in kernel_size) if padding else 0\n            else:\n                padding = (kernel_size - 1) // 2 if padding else 0\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding, bias=not bn)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True) if bn else lambda x: x\n        self.act = nn.ReLU(inplace=True) if act else lambda x: x\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.act(x)\n        return x\n\n\nclass Mixed_3a(nn.Module):\n    def __init__(self, config_channels, prefix, bn=True, ratio=1):\n        nn.Module.__init__(self)\n        channels = config_channels.channels\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n        self.conv = Conv2d(config_channels.channels, config_channels(int(96 * ratio), \'%s.conv.conv.weight\' % prefix), kernel_size=3, stride=2, bn=bn)\n        config_channels.channels = channels + self.conv.conv.weight.size(0)\n\n    def forward(self, x):\n        x0 = self.maxpool(x)\n        x1 = self.conv(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed_4a(nn.Module):\n    def __init__(self, config_channels, prefix, bn=True, ratio=1):\n        nn.Module.__init__(self)\n        # branch0\n        channels = config_channels.channels\n        branch = []\n        branch.append(Conv2d(config_channels.channels, config_channels(int(64 * ratio), \'%s.branch0.%d.conv.weight\' % (prefix, len(branch))), kernel_size=1, stride=1, bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(96 * ratio), \'%s.branch0.%d.conv.weight\' % (prefix, len(branch))), kernel_size=3, stride=1, bn=bn))\n        self.branch0 = nn.Sequential(*branch)\n        # branch1\n        config_channels.channels = channels\n        branch = []\n        branch.append(Conv2d(config_channels.channels, config_channels(int(64 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=1, stride=1, bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(64 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=(1, 7), stride=1, padding=(0, 3), bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(64 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=(7, 1), stride=1, padding=(3, 0), bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(96 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=(3, 3), stride=1, bn=bn))\n        self.branch1 = nn.Sequential(*branch)\n        # output\n        config_channels.channels = self.branch0[-1].conv.weight.size(0) + self.branch1[-1].conv.weight.size(0)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed_5a(nn.Module):\n    def __init__(self, config_channels, prefix, bn=True, ratio=1):\n        nn.Module.__init__(self)\n        channels = config_channels.channels\n        self.conv = Conv2d(config_channels.channels, config_channels(int(192 * ratio), \'%s.conv.conv.weight\' % prefix), kernel_size=3, stride=2, bn=bn)\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n        config_channels.channels = self.conv.conv.weight.size(0) + channels\n\n    def forward(self, x):\n        x0 = self.conv(x)\n        x1 = self.maxpool(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Inception_A(nn.Module):\n    def __init__(self, config_channels, prefix, bn=True, ratio=1):\n        nn.Module.__init__(self)\n        channels = config_channels.channels\n        self.branch0 = Conv2d(config_channels.channels, config_channels(int(96 * ratio), \'%s.branch0.conv.weight\' % prefix), kernel_size=1, stride=1, bn=bn)\n        # branch1\n        config_channels.channels = channels\n        branch = []\n        branch.append(Conv2d(config_channels.channels, config_channels(int(64 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=1, stride=1, bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(96 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=3, stride=1, padding=1, bn=bn))\n        self.branch1 = nn.Sequential(*branch)\n        # branch2\n        config_channels.channels = channels\n        branch = []\n        branch.append(Conv2d(config_channels.channels, config_channels(int(64 * ratio), \'%s.branch2.%d.conv.weight\' % (prefix, len(branch))), kernel_size=1, stride=1, bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(96 * ratio), \'%s.branch2.%d.conv.weight\' % (prefix, len(branch))), kernel_size=3, stride=1, padding=1, bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(96 * ratio), \'%s.branch2.%d.conv.weight\' % (prefix, len(branch))), kernel_size=3, stride=1, padding=1, bn=bn))\n        self.branch2 = nn.Sequential(*branch)\n        #branch3\n        config_channels.channels = channels\n        branch = []\n        branch.append(nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(96 * ratio), \'%s.branch3.%d.conv.weight\' % (prefix, len(branch))), kernel_size=1, stride=1, bn=bn))\n        self.branch3 = nn.Sequential(*branch)\n        # output\n        config_channels.channels = self.branch0.conv.weight.size(0) + self.branch1[-1].conv.weight.size(0) + self.branch2[-1].conv.weight.size(0) + self.branch3[-1].conv.weight.size(0)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Reduction_A(nn.Module):\n    def __init__(self, config_channels, prefix, bn=True, ratio=1):\n        nn.Module.__init__(self)\n        channels = config_channels.channels\n        self.branch0 = Conv2d(config_channels.channels, config_channels(int(384 * ratio), \'%s.branch0.conv.weight\' % prefix), kernel_size=3, stride=2, bn=bn)\n        # branch1\n        config_channels.channels = channels\n        branch = []\n        branch.append(Conv2d(config_channels.channels, config_channels(int(192 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=1, stride=1, bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(224 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=3, stride=1, padding=1, bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(256 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=3, stride=2, bn=bn))\n        self.branch1 = nn.Sequential(*branch)\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n        # output\n        config_channels.channels = self.branch0.conv.weight.size(0) + self.branch1[-1].conv.weight.size(0) + channels\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Inception_B(nn.Module):\n    def __init__(self, config_channels, prefix, bn=True, ratio=1):\n        nn.Module.__init__(self)\n        channels = config_channels.channels\n        self.branch0 = Conv2d(config_channels.channels, config_channels(int(384 * ratio), \'%s.branch0.conv.weight\' % prefix), kernel_size=1, stride=1, bn=bn)\n        # branch1\n        config_channels.channels = channels\n        branch = []\n        branch.append(Conv2d(config_channels.channels, config_channels(int(192 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=1, stride=1, bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(224 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=(1, 7), stride=1, padding=(0, 3), bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(256 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=(7, 1), stride=1, padding=(3, 0), bn=bn))\n        self.branch1 = nn.Sequential(*branch)\n        # branch2\n        config_channels.channels = channels\n        branch = []\n        branch.append(Conv2d(config_channels.channels, config_channels(int(192 * ratio), \'%s.branch2.%d.conv.weight\' % (prefix, len(branch))), kernel_size=1, stride=1, bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(192 * ratio), \'%s.branch2.%d.conv.weight\' % (prefix, len(branch))), kernel_size=(7, 1), stride=1, padding=(3, 0), bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(224 * ratio), \'%s.branch2.%d.conv.weight\' % (prefix, len(branch))), kernel_size=(1, 7), stride=1, padding=(0, 3), bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(224 * ratio), \'%s.branch2.%d.conv.weight\' % (prefix, len(branch))), kernel_size=(7, 1), stride=1, padding=(3, 0), bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(256 * ratio), \'%s.branch2.%d.conv.weight\' % (prefix, len(branch))), kernel_size=(1, 7), stride=1, padding=(0, 3), bn=bn))\n        self.branch2 = nn.Sequential(*branch)\n        # branch3\n        config_channels.channels = channels\n        branch = []\n        branch.append(nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(128 * ratio), \'%s.branch3.%d.conv.weight\' % (prefix, len(branch))), kernel_size=1, stride=1, bn=bn))\n        self.branch3 = nn.Sequential(*branch)\n        # output\n        config_channels.channels = self.branch0.conv.weight.size(0) + self.branch1[-1].conv.weight.size(0) + self.branch2[-1].conv.weight.size(0) + self.branch3[-1].conv.weight.size(0)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Reduction_B(nn.Module):\n    def __init__(self, config_channels, prefix, bn=True, ratio=1):\n        nn.Module.__init__(self)\n        # branch0\n        channels = config_channels.channels\n        branch = []\n        branch.append(Conv2d(config_channels.channels, config_channels(int(192 * ratio), \'%s.branch0.%d.conv.weight\' % (prefix, len(branch))), kernel_size=1, stride=1, bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(192 * ratio), \'%s.branch0.%d.conv.weight\' % (prefix, len(branch))), kernel_size=3, stride=2, bn=bn))\n        self.branch0 = nn.Sequential(*branch)\n        # branch1\n        config_channels.channels = channels\n        branch = []\n        branch.append(Conv2d(config_channels.channels, config_channels(int(256 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=1, stride=1, bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(256 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=(1, 7), stride=1, padding=(0, 3), bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(320 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=(7, 1), stride=1, padding=(3, 0), bn=bn))\n        branch.append(Conv2d(config_channels.channels, config_channels(int(320 * ratio), \'%s.branch1.%d.conv.weight\' % (prefix, len(branch))), kernel_size=3, stride=2, bn=bn))\n        self.branch1 = nn.Sequential(*branch)\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n        # output\n        config_channels.channels = self.branch0[-1].conv.weight.size(0) + self.branch1[-1].conv.weight.size(0) + channels\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Inception_C(nn.Module):\n    def __init__(self, config_channels, prefix, bn=True, ratio=1):\n        nn.Module.__init__(self)\n        channels = config_channels.channels\n        self.branch0 = Conv2d(config_channels.channels, config_channels(int(256 * ratio), \'%s.branch0.conv.weight\' % prefix), kernel_size=1, stride=1, bn=bn)\n        # branch1\n        config_channels.channels = channels\n        self.branch1_0 = Conv2d(config_channels.channels, config_channels(int(384 * ratio), \'%s.branch1_0.conv.weight\' % prefix), kernel_size=1, stride=1, bn=bn)\n        _channels = config_channels.channels\n        self.branch1_1a = Conv2d(_channels, config_channels(int(256 * ratio), \'%s.branch1_1a.conv.weight\' % prefix), kernel_size=(1, 3), stride=1, padding=(0, 1), bn=bn)\n        self.branch1_1b = Conv2d(_channels, config_channels(int(256 * ratio), \'%s.branch1_1b.conv.weight\' % prefix), kernel_size=(3, 1), stride=1, padding=(1, 0), bn=bn)\n        # branch2\n        config_channels.channels = channels\n        self.branch2_0 = Conv2d(config_channels.channels, config_channels(int(384 * ratio), \'%s.branch2_0.conv.weight\' % prefix), kernel_size=1, stride=1, bn=bn)\n        self.branch2_1 = Conv2d(config_channels.channels, config_channels(int(448 * ratio), \'%s.branch2_1.conv.weight\' % prefix), kernel_size=(3, 1), stride=1, padding=(1, 0), bn=bn)\n        self.branch2_2 = Conv2d(config_channels.channels, config_channels(int(512 * ratio), \'%s.branch2_2.conv.weight\' % prefix), kernel_size=(1, 3), stride=1, padding=(0, 1), bn=bn)\n        _channels = config_channels.channels\n        self.branch2_3a = Conv2d(_channels, config_channels(int(256 * ratio), \'%s.branch2_3a.conv.weight\' % prefix), kernel_size=(1, 3), stride=1, padding=(0, 1), bn=bn)\n        self.branch2_3b = Conv2d(_channels, config_channels(int(256 * ratio), \'%s.branch2_3b.conv.weight\' % prefix), kernel_size=(3, 1), stride=1, padding=(1, 0), bn=bn)\n        # branch3\n        config_channels.channels = channels\n        branch = []\n        branch.append(nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False))\n        branch.append(Conv2d(config_channels.channels, int(256 * ratio), kernel_size=1, stride=1, bn=bn))\n        self.branch3 = nn.Sequential(*branch)\n        # output\n        config_channels.channels = self.branch0.conv.weight.size(0) + self.branch1_1a.conv.weight.size(0) + self.branch1_1b.conv.weight.size(0) + self.branch2_3a.conv.weight.size(0) + self.branch2_3b.conv.weight.size(0) + self.branch3[-1].conv.weight.size(0)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n\n        x1_0 = self.branch1_0(x)\n        x1_1a = self.branch1_1a(x1_0)\n        x1_1b = self.branch1_1b(x1_0)\n        x1 = torch.cat((x1_1a, x1_1b), 1)\n\n        x2_0 = self.branch2_0(x)\n        x2_1 = self.branch2_1(x2_0)\n        x2_2 = self.branch2_2(x2_1)\n        x2_3a = self.branch2_3a(x2_2)\n        x2_3b = self.branch2_3b(x2_2)\n        x2 = torch.cat((x2_3a, x2_3b), 1)\n\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Inception4(nn.Module):\n    def __init__(self, config_channels, anchors, num_cls, ratio=1):\n        nn.Module.__init__(self)\n        features = []\n        bn = config_channels.config.getboolean(\'batch_norm\', \'enable\')\n        features.append(Conv2d(config_channels.channels, config_channels(32, \'features.%d.conv.weight\' % len(features)), kernel_size=3, stride=2, bn=bn))\n        features.append(Conv2d(config_channels.channels, config_channels(32, \'features.%d.conv.weight\' % len(features)), kernel_size=3, stride=1, bn=bn))\n        features.append(Conv2d(config_channels.channels, config_channels(64, \'features.%d.conv.weight\' % len(features)), kernel_size=3, stride=1, padding=1, bn=bn))\n        features.append(Mixed_3a(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Mixed_4a(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Mixed_5a(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Inception_A(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Inception_A(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Inception_A(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Inception_A(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Reduction_A(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio)) # Mixed_6a\n        features.append(Inception_B(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Inception_B(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Inception_B(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Inception_B(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Inception_B(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Inception_B(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Inception_B(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Reduction_B(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio)) # Mixed_7a\n        features.append(Inception_C(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Inception_C(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(Inception_C(config_channels, \'features.%d\' % len(features), bn=bn, ratio=ratio))\n        features.append(nn.Conv2d(config_channels.channels, model.output_channels(len(anchors), num_cls), 1))\n        self.features = nn.Sequential(*features)\n        self.init(config_channels)\n\n    def init(self, config_channels):\n        try:\n            gamma = config_channels.config.getboolean(\'batch_norm\', \'gamma\')\n        except (configparser.NoSectionError, configparser.NoOptionError):\n            gamma = True\n        try:\n            beta = config_channels.config.getboolean(\'batch_norm\', \'beta\')\n        except (configparser.NoSectionError, configparser.NoOptionError):\n            beta = True\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight = nn.init.kaiming_normal(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n                m.weight.requires_grad = gamma\n                m.bias.requires_grad = beta\n        try:\n            if config_channels.config.getboolean(\'model\', \'pretrained\'):\n                settings = pretrained_settings[\'inceptionv4\'][config_channels.config.get(\'inception4\', \'pretrained\')]\n                logging.info(\'use pretrained model: \' + str(settings))\n                state_dict = self.state_dict()\n                for key, value in torch.utils.model_zoo.load_url(settings[\'url\']).items():\n                    if key in state_dict:\n                        state_dict[key] = value\n                self.load_state_dict(state_dict)\n        except (configparser.NoSectionError, configparser.NoOptionError):\n            pass\n\n    def forward(self, x):\n        return self.features(x)\n\n    def scope(self, name):\n        return \'.\'.join(name.split(\'.\')[:-2])\n'"
model/mobilenet.py,1,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport collections\n\nimport torch.nn as nn\n\nimport model\n\n\ndef conv_bn(in_channels, out_channels, stride):\n    return nn.Sequential(collections.OrderedDict([\n        (\'conv\', nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)),\n        (\'bn\', nn.BatchNorm2d(out_channels)),\n        (\'act\', nn.ReLU(inplace=True)),\n    ]))\n\n\ndef conv_dw(in_channels, stride):\n    return nn.Sequential(collections.OrderedDict([\n        (\'conv\', nn.Conv2d(in_channels, in_channels, 3, stride, 1, groups=in_channels, bias=False)),\n        (\'bn\', nn.BatchNorm2d(in_channels)),\n        (\'act\', nn.ReLU(inplace=True)),\n    ]))\n\n\ndef conv_pw(in_channels, out_channels):\n    return nn.Sequential(collections.OrderedDict([\n        (\'conv\', nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)),\n        (\'bn\', nn.BatchNorm2d(out_channels)),\n        (\'act\', nn.ReLU(inplace=True)),\n    ]))\n\n\ndef conv_unit(in_channels, out_channels, stride):\n    return nn.Sequential(collections.OrderedDict([\n        (\'dw\', conv_dw(in_channels, stride)),\n        (\'pw\', conv_pw(in_channels, out_channels)),\n    ]))\n\n\nclass MobileNet(nn.Module):\n    def __init__(self, config_channels, anchors, num_cls):\n        nn.Module.__init__(self)\n        layers = []\n        layers.append(conv_bn(config_channels.channels, config_channels(32, \'layers.%d.conv.weight\' % len(layers)), 2))\n        layers.append(conv_unit(config_channels.channels, config_channels(64, \'layers.%d.pw.conv.weight\' % len(layers)), 1))\n        layers.append(conv_unit(config_channels.channels, config_channels(128, \'layers.%d.pw.conv.weight\' % len(layers)), 2))\n        layers.append(conv_unit(config_channels.channels, config_channels(128, \'layers.%d.pw.conv.weight\' % len(layers)), 1))\n        layers.append(conv_unit(config_channels.channels, config_channels(256, \'layers.%d.pw.conv.weight\' % len(layers)), 2))\n        layers.append(conv_unit(config_channels.channels, config_channels(256, \'layers.%d.pw.conv.weight\' % len(layers)), 1))\n        layers.append(conv_unit(config_channels.channels, config_channels(512, \'layers.%d.pw.conv.weight\' % len(layers)), 2))\n        layers.append(conv_unit(config_channels.channels, config_channels(512, \'layers.%d.pw.conv.weight\' % len(layers)), 1))\n        layers.append(conv_unit(config_channels.channels, config_channels(512, \'layers.%d.pw.conv.weight\' % len(layers)), 1))\n        layers.append(conv_unit(config_channels.channels, config_channels(512, \'layers.%d.pw.conv.weight\' % len(layers)), 1))\n        layers.append(conv_unit(config_channels.channels, config_channels(512, \'layers.%d.pw.conv.weight\' % len(layers)), 1))\n        layers.append(conv_unit(config_channels.channels, config_channels(512, \'layers.%d.pw.conv.weight\' % len(layers)), 1))\n        layers.append(conv_unit(config_channels.channels, config_channels(1024, \'layers.%d.pw.conv.weight\' % len(layers)), 2))\n        layers.append(conv_unit(config_channels.channels, config_channels(1024, \'layers.%d.pw.conv.weight\' % len(layers)), 1))\n        layers.append(nn.Conv2d(config_channels.channels, model.output_channels(len(anchors), num_cls), 1))\n        self.layers = nn.Sequential(*layers)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight = nn.init.kaiming_normal(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        return self.layers(x)\n'"
model/resnet.py,2,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport logging\nimport re\n\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport torchvision.models.resnet as _model\nfrom torchvision.models.resnet import conv3x3\n\nimport model\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, config_channels, prefix, channels, stride=1):\n        nn.Module.__init__(self)\n        channels_in = config_channels.channels\n        self.conv1 = conv3x3(config_channels.channels, config_channels(channels, \'%s.conv1.weight\' % prefix), stride)\n        self.bn1 = nn.BatchNorm2d(config_channels.channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(config_channels.channels, config_channels(channels, \'%s.conv2.weight\' % prefix))\n        self.bn2 = nn.BatchNorm2d(config_channels.channels)\n        if stride > 1 or channels_in != config_channels.channels:\n            downsample = []\n            downsample.append(nn.Conv2d(channels_in, config_channels.channels, kernel_size=1, stride=stride, bias=False))\n            downsample.append(nn.BatchNorm2d(config_channels.channels))\n            self.downsample = nn.Sequential(*downsample)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, config_channels, prefix, channels, stride=1):\n        nn.Module.__init__(self)\n        channels_in = config_channels.channels\n        self.conv1 = nn.Conv2d(config_channels.channels, config_channels(channels, \'%s.conv1.weight\' % prefix), kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(config_channels.channels)\n        self.conv2 = nn.Conv2d(config_channels.channels, config_channels(channels, \'%s.conv2.weight\' % prefix), kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(config_channels.channels)\n        self.conv3 = nn.Conv2d(config_channels.channels, config_channels(channels * 4, \'%s.conv3.weight\' % prefix), kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(config_channels.channels)\n        self.relu = nn.ReLU(inplace=True)\n        if stride > 1 or channels_in != config_channels.channels:\n            downsample = []\n            downsample.append(nn.Conv2d(channels_in, config_channels.channels, kernel_size=1, stride=stride, bias=False))\n            downsample.append(nn.BatchNorm2d(config_channels.channels))\n            self.downsample = nn.Sequential(*downsample)\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(_model.ResNet):\n    def __init__(self, config_channels, anchors, num_cls, block, layers):\n        nn.Module.__init__(self)\n        self.conv1 = nn.Conv2d(config_channels.channels, config_channels(64, \'conv1.weight\'), kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(config_channels.channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(config_channels, \'layer1\', block, 64, layers[0])\n        self.layer2 = self._make_layer(config_channels, \'layer2\', block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(config_channels, \'layer3\', block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(config_channels, \'layer4\', block, 512, layers[3], stride=2)\n        self.conv = nn.Conv2d(config_channels.channels, model.output_channels(len(anchors), num_cls), 1)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight = nn.init.kaiming_normal(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, config_channels, prefix, block, channels, blocks, stride=1):\n        layers = []\n        layers.append(block(config_channels, \'%s.%d\' % (prefix, len(layers)), channels, stride))\n        for i in range(1, blocks):\n            layers.append(block(config_channels, \'%s.%d\' % (prefix, len(layers)), channels))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return self.conv(x)\n\n    def scope(self, name):\n        comp = name.split(\'.\')[:-1]\n        try:\n            comp[-1] = re.search(\'[(conv)|(bn)](\\d+)\', comp[-1]).group(1)\n        except AttributeError:\n            if len(comp) > 1:\n                if comp[-2] == \'downsample\':\n                    comp = comp[:-1]\n                else:\n                    assert False, name\n            else:\n                assert comp[-1] == \'conv\', name\n        return \'.\'.join(comp)\n\n\ndef resnet18(config_channels, anchors, num_cls, **kwargs):\n    model = ResNet(config_channels, anchors, num_cls, BasicBlock, [2, 2, 2, 2], **kwargs)\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = _model.model_urls[\'resnet18\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnet34(config_channels, anchors, num_cls, **kwargs):\n    model = ResNet(config_channels, anchors, num_cls, BasicBlock, [3, 4, 6, 3], **kwargs)\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = _model.model_urls[\'resnet34\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnet50(config_channels, anchors, num_cls, **kwargs):\n    model = ResNet(config_channels, anchors, num_cls, Bottleneck, [3, 4, 6, 3], **kwargs)\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = _model.model_urls[\'resnet50\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnet101(config_channels, anchors, num_cls, **kwargs):\n    model = ResNet(config_channels, anchors, num_cls, Bottleneck, [3, 4, 23, 3], **kwargs)\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = _model.model_urls[\'resnet101\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef resnet152(config_channels, anchors, num_cls, **kwargs):\n    model = ResNet(config_channels, anchors, num_cls, Bottleneck, [3, 8, 36, 3], **kwargs)\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = _model.model_urls[\'resnet152\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n'"
model/vgg.py,2,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport logging\n\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nimport torchvision.models.vgg as _model\nfrom torchvision.models.vgg import model_urls, cfg\n\nimport model\n\n\nclass VGG(_model.VGG):\n    def __init__(self, config_channels, anchors, num_cls, features):\n        nn.Module.__init__(self)\n        self.features = features\n        self.conv = nn.Conv2d(config_channels.channels, model.output_channels(len(anchors), num_cls), 1)\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        return self.conv(x)\n\n\ndef make_layers(config_channels, cfg, batch_norm=False):\n    features = []\n    for v in cfg:\n        if v == \'M\':\n            features += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(config_channels.channels, config_channels(v, \'features.%d.weight\' % len(features)), kernel_size=3, padding=1)\n            if batch_norm:\n                features += [conv2d, nn.BatchNorm2d(config_channels.channels), nn.ReLU(inplace=True)]\n            else:\n                features += [conv2d, nn.ReLU(inplace=True)]\n    return nn.Sequential(*features)\n\n\ndef vgg11(config_channels, anchors, num_cls):\n    model = VGG(config_channels, anchors, num_cls, make_layers(config_channels, cfg[\'A\']))\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = model_urls[\'vgg11\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef vgg11_bn(config_channels, anchors, num_cls):\n    model = VGG(config_channels, anchors, num_cls, make_layers(config_channels, cfg[\'A\'], batch_norm=True))\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = model_urls[\'vgg11_bn\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef vgg13(config_channels, anchors, num_cls):\n    model = VGG(config_channels, anchors, num_cls, make_layers(config_channels, cfg[\'B\']))\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = model_urls[\'vgg13\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef vgg13_bn(config_channels, anchors, num_cls):\n    model = VGG(config_channels, anchors, num_cls, make_layers(config_channels, cfg[\'B\'], batch_norm=True))\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = model_urls[\'vgg13_bn\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef vgg16(config_channels, anchors, num_cls):\n    model = VGG(config_channels, anchors, num_cls, make_layers(config_channels, cfg[\'D\']))\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = model_urls[\'vgg16\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef vgg16_bn(config_channels, anchors, num_cls):\n    model = VGG(config_channels, anchors, num_cls, make_layers(config_channels, cfg[\'D\'], batch_norm=True))\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = model_urls[\'vgg16_bn\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef vgg19(config_channels, anchors, num_cls):\n    model = VGG(config_channels, anchors, num_cls, make_layers(config_channels, cfg[\'E\']))\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = model_urls[\'vgg19\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n\n\ndef vgg19_bn(config_channels, anchors, num_cls):\n    model = VGG(config_channels, anchors, num_cls, make_layers(config_channels, cfg[\'E\'], batch_norm=True))\n    if config_channels.config.getboolean(\'model\', \'pretrained\'):\n        url = model_urls[\'vgg19_bn\']\n        logging.info(\'use pretrained model: \' + url)\n        state_dict = model.state_dict()\n        for key, value in model_zoo.load_url(url).items():\n            if key in state_dict:\n                state_dict[key] = value\n        model.load_state_dict(state_dict)\n    return model\n'"
model/yolo2.py,4,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport collections.abc\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.autograd\n\nimport model\n\n\nsettings = {\n    \'size\': (416, 416),\n}\n\n\ndef reorg(x, stride_h=2, stride_w=2):\n    batch_size, channels, height, width = x.size()\n    _height, _width = height // stride_h, width // stride_w\n    if 1:\n        x = x.view(batch_size, channels, _height, stride_h, _width, stride_w).transpose(3, 4).contiguous()\n        x = x.view(batch_size, channels, _height * _width, stride_h * stride_w).transpose(2, 3).contiguous()\n        x = x.view(batch_size, channels, stride_h * stride_w, _height, _width).transpose(1, 2).contiguous()\n        x = x.view(batch_size, -1, _height, _width)\n    else:\n        x = x.view(batch_size, channels, _height, stride_h, _width, stride_w)\n        x = x.permute(0, 1, 3, 5, 2, 4) # batch_size, channels, stride, stride, _height, _width\n        x = x.contiguous()\n        x = x.view(batch_size, -1, _height, _width)\n    return x\n\n\nclass Conv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0, stride=1, bn=True, act=True):\n        nn.Module.__init__(self)\n        if isinstance(padding, bool):\n            if isinstance(kernel_size, collections.abc.Iterable):\n                padding = tuple((kernel_size - 1) // 2 for kernel_size in kernel_size) if padding else 0\n            else:\n                padding = (kernel_size - 1) // 2 if padding else 0\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding, bias=not bn)\n        self.bn = nn.BatchNorm2d(out_channels, momentum=0.01) if bn else lambda x: x\n        self.act = nn.LeakyReLU(0.1, inplace=True) if act else lambda x: x\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.act(x)\n        return x\n\n\nclass Darknet(nn.Module):\n    def __init__(self, config_channels, anchors, num_cls, stride=2, ratio=1):\n        nn.Module.__init__(self)\n        self.stride = stride\n        channels = int(32 * ratio)\n        layers = []\n\n        bn = config_channels.config.getboolean(\'batch_norm\', \'enable\')\n        # layers1\n        for _ in range(2):\n            layers.append(Conv2d(config_channels.channels, config_channels(channels, \'layers1.%d.conv.weight\' % len(layers)), 3, bn=bn, padding=True))\n            layers.append(nn.MaxPool2d(kernel_size=2))\n            channels *= 2\n        # down 4\n        for _ in range(2):\n            layers.append(Conv2d(config_channels.channels, config_channels(channels, \'layers1.%d.conv.weight\' % len(layers)), 3, bn=bn, padding=True))\n            layers.append(Conv2d(config_channels.channels, config_channels(channels // 2, \'layers1.%d.conv.weight\' % len(layers)), 1, bn=bn))\n            layers.append(Conv2d(config_channels.channels, config_channels(channels, \'layers1.%d.conv.weight\' % len(layers)), 3, bn=bn, padding=True))\n            layers.append(nn.MaxPool2d(kernel_size=2))\n            channels *= 2\n        # down 16\n        for _ in range(2):\n            layers.append(Conv2d(config_channels.channels, config_channels(channels, \'layers1.%d.conv.weight\' % len(layers)), 3, bn=bn, padding=True))\n            layers.append(Conv2d(config_channels.channels, config_channels(channels // 2, \'layers1.%d.conv.weight\' % len(layers)), 1, bn=bn))\n        layers.append(Conv2d(config_channels.channels, config_channels(channels, \'layers1.%d.conv.weight\' % len(layers)), 3, bn=bn, padding=True))\n        self.layers1 = nn.Sequential(*layers)\n\n        # layers2\n        layers = []\n        layers.append(nn.MaxPool2d(kernel_size=2))\n        channels *= 2\n        # down 32\n        for _ in range(2):\n            layers.append(Conv2d(config_channels.channels, config_channels(channels, \'layers2.%d.conv.weight\' % len(layers)), 3, bn=bn, padding=True))\n            layers.append(Conv2d(config_channels.channels, config_channels(channels // 2, \'layers2.%d.conv.weight\' % len(layers)), 1, bn=bn))\n        for _ in range(3):\n            layers.append(Conv2d(config_channels.channels, config_channels(channels, \'layers2.%d.conv.weight\' % len(layers)), 3, bn=bn, padding=True))\n        self.layers2 = nn.Sequential(*layers)\n\n        self.passthrough = Conv2d(self.layers1[-1].conv.weight.size(0), config_channels(int(64 * ratio), \'passthrough.conv.weight\'), 1, bn=bn)\n\n        # layers3\n        layers = []\n        layers.append(Conv2d(self.passthrough.conv.weight.size(0) * self.stride * self.stride + self.layers2[-1].conv.weight.size(0), config_channels(int(1024 * ratio), \'layers3.%d.conv.weight\' % len(layers)), 3, bn=bn, padding=True))\n        layers.append(Conv2d(config_channels.channels, model.output_channels(len(anchors), num_cls), 1, bn=False, act=False))\n        self.layers3 = nn.Sequential(*layers)\n\n        self.init()\n\n    def init(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight = nn.init.kaiming_normal(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.layers1(x)\n        _x = reorg(self.passthrough(x), self.stride)\n        x = self.layers2(x)\n        x = torch.cat([_x, x], 1)\n        return self.layers3(x)\n\n    def scope(self, name):\n        return \'.\'.join(name.split(\'.\')[:-2])\n\n    def get_mapper(self, index):\n        if index == 94:\n            return lambda indices, channels: torch.cat([indices + i * channels for i in range(self.stride * self.stride)])\n\n\nclass Tiny(nn.Module):\n    def __init__(self, config_channels, anchors, num_cls, channels=16):\n        nn.Module.__init__(self)\n        layers = []\n\n        bn = config_channels.config.getboolean(\'batch_norm\', \'enable\')\n        for _ in range(5):\n            layers.append(Conv2d(config_channels.channels, config_channels(channels, \'layers.%d.conv.weight\' % len(layers)), 3, bn=bn, padding=True))\n            layers.append(nn.MaxPool2d(kernel_size=2))\n            channels *= 2\n        layers.append(Conv2d(config_channels.channels, config_channels(channels, \'layers.%d.conv.weight\' % len(layers)), 3, bn=bn, padding=True))\n        layers.append(nn.ConstantPad2d((0, 1, 0, 1), float(np.finfo(np.float32).min)))\n        layers.append(nn.MaxPool2d(kernel_size=2, stride=1))\n        channels *= 2\n        for _ in range(2):\n            layers.append(Conv2d(config_channels.channels, config_channels(channels, \'layers.%d.conv.weight\' % len(layers)), 3, bn=bn, padding=True))\n        layers.append(Conv2d(config_channels.channels, model.output_channels(len(anchors), num_cls), 1, bn=False, act=False))\n        self.layers = nn.Sequential(*layers)\n\n        self.init()\n\n    def init(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                m.weight = nn.init.xavier_normal(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        return self.layers(x)\n\n    def scope(self, name):\n        return \'.\'.join(name.split(\'.\')[:-2])\n'"
transform/__init__.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport inspect\n\nimport torchvision\n\nimport utils\n\n\ndef parse_transform(config, method):\n    if isinstance(method, str):\n        attr = utils.parse_attr(method)\n        sig = inspect.signature(attr)\n        if len(sig.parameters) == 1:\n            return attr(config)\n        else:\n            return attr()\n    else:\n        return method\n\n\ndef get_transform(config, sequence, compose=torchvision.transforms.Compose):\n    return compose([parse_transform(config, method) for method in sequence])\n'"
transform/augmentation.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport inspect\nimport random\n\nimport inflection\nimport numpy as np\nimport cv2\n\nimport transform\n\n\nclass Rotator(object):\n    def __init__(self, y, x, height, width, angle):\n        """"""\n        A efficient tool to rotate multiple images in the same size.\n        :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n        :param y: The y coordinate of rotation point.\n        :param x: The x coordinate of rotation point.\n        :param height: Image height.\n        :param width: Image width.\n        :param angle: Rotate angle.\n        """"""\n        self._mat = cv2.getRotationMatrix2D((x, y), angle, 1.0)\n        r = np.abs(self._mat[0, :2])\n        _height, _width = np.inner(r, [height, width]), np.inner(r, [width, height])\n        fix_y, fix_x = _height / 2 - y, _width / 2 - x\n        self._mat[:, 2] += [fix_x, fix_y]\n        self._size = int(_width), int(_height)\n\n    def __call__(self, image, flags=cv2.INTER_LINEAR, fill=None):\n        if fill is None:\n            fill = np.random.rand(3) * 256\n        return cv2.warpAffine(image, self._mat, self._size, flags=flags, borderMode=cv2.BORDER_CONSTANT, borderValue=fill)\n\n    def _rotate_points(self, points):\n        _points = np.pad(points, [(0, 0), (0, 1)], \'constant\')\n        _points[:, 2] = 1\n        _points = np.dot(self._mat, _points.T)\n        return _points.T.astype(points.dtype)\n\n    def rotate_points(self, points):\n        return self._rotate_points(points[:, ::-1])[:, ::-1]\n\n\ndef random_rotate(config, image, yx_min, yx_max):\n    name = inspect.stack()[0][3]\n    angle = random.uniform(*tuple(map(float, config.get(\'augmentation\', name).split())))\n    height, width = image.shape[:2]\n    p1, p2 = np.copy(yx_min), np.copy(yx_max)\n    p1[:, 0] = yx_max[:, 0]\n    p2[:, 0] = yx_min[:, 0]\n    points = np.concatenate([yx_min, yx_max, p1, p2], 0)\n    rotator = Rotator(height / 2, width / 2, height, width, angle)\n    image = rotator(image, fill=0)\n    points = rotator.rotate_points(points)\n    bbox_points = np.reshape(points, [4, -1, 2])\n    yx_min = np.apply_along_axis(lambda points: np.min(points, 0), 0, bbox_points)\n    yx_max = np.apply_along_axis(lambda points: np.max(points, 0), 0, bbox_points)\n    return image, yx_min, yx_max\n\n\nclass RandomRotate(object):\n    def __init__(self, config):\n        self.config = config\n        self.fn = eval(inflection.underscore(type(self).__name__))\n\n    def __call__(self, data):\n        data[\'image\'], data[\'yx_min\'], data[\'yx_max\'] = self.fn(self.config, data[\'image\'], data[\'yx_min\'], data[\'yx_max\'])\n        return data\n\n\ndef flip_horizontally(image, yx_min, yx_max):\n    assert len(image.shape) == 3\n    image = cv2.flip(image, 1)\n    width = image.shape[1]\n    temp = width - yx_min[:, 1]\n    yx_min[:, 1] = width - yx_max[:, 1]\n    yx_max[:, 1] = temp\n    return image, yx_min, yx_max\n\n\ndef random_flip_horizontally(config, image, yx_min, yx_max):\n    name = inspect.stack()[0][3]\n    if random.random() > config.getfloat(\'augmentation\', name):\n        return flip_horizontally(image, yx_min, yx_max)\n    else:\n        return image, yx_min, yx_max\n\n\nclass RandomFlipHorizontally(object):\n    def __init__(self, config):\n        self.config = config\n        self.fn = eval(inflection.underscore(type(self).__name__))\n\n    def __call__(self, data):\n        data[\'image\'], data[\'yx_min\'], data[\'yx_max\'] = self.fn(self.config, data[\'image\'], data[\'yx_min\'], data[\'yx_max\'])\n        return data\n\n\ndef get_transform(config, sequence):\n    return transform.get_transform(config, sequence)\n'"
transform/image.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport random\n\nimport numpy as np\nimport torchvision\nimport inflection\nimport skimage.exposure\nimport cv2\n\n\nclass BGR2RGB(object):\n    def __call__(self, image):\n        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n\nclass BGR2HSV(object):\n    def __call__(self, image):\n        return cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n\nclass HSV2RGB(object):\n    def __call__(self, image):\n        return cv2.cvtColor(image, cv2.COLOR_HSV2RGB)\n\n\nclass RandomBlur(object):\n    def __init__(self, config):\n        name = inflection.underscore(type(self).__name__)\n        self.adjust = tuple(map(int, config.get(\'augmentation\', name).split()))\n\n    def __call__(self, image):\n        adjust = tuple(random.randint(1, adjust) for adjust in self.adjust)\n        return cv2.blur(image, adjust)\n\n\nclass RandomHue(object):\n    def __init__(self, config):\n        name = inflection.underscore(type(self).__name__)\n        self.adjust = tuple(map(int, config.get(\'augmentation\', name).split()))\n\n    def __call__(self, hsv):\n        h, s, v = cv2.split(hsv)\n        adjust = random.randint(*self.adjust)\n        h = h.astype(np.int) + adjust\n        h = np.clip(h, 0, 179).astype(hsv.dtype)\n        return cv2.merge((h, s, v))\n\n\nclass RandomSaturation(object):\n    def __init__(self, config):\n        name = inflection.underscore(type(self).__name__)\n        self.adjust = tuple(map(float, config.get(\'augmentation\', name).split()))\n\n    def __call__(self, hsv):\n        h, s, v = cv2.split(hsv)\n        adjust = random.uniform(*self.adjust)\n        s = s * adjust\n        s = np.clip(s, 0, 255).astype(hsv.dtype)\n        return cv2.merge((h, s, v))\n\n\nclass RandomBrightness(object):\n    def __init__(self, config):\n        name = inflection.underscore(type(self).__name__)\n        self.adjust = tuple(map(float, config.get(\'augmentation\', name).split()))\n\n    def __call__(self, hsv):\n        h, s, v = cv2.split(hsv)\n        adjust = random.uniform(*self.adjust)\n        v = v * adjust\n        v = np.clip(v, 0, 255).astype(hsv.dtype)\n        return cv2.merge((h, s, v))\n\n\nclass RandomGamma(object):\n    def __init__(self, config):\n        name = inflection.underscore(type(self).__name__)\n        self.adjust = tuple(map(float, config.get(\'augmentation\', name).split()))\n\n    def __call__(self, image):\n        adjust = random.uniform(*self.adjust)\n        return skimage.exposure.adjust_gamma(image, adjust)\n\n\nclass Normalize(torchvision.transforms.Normalize):\n    def __init__(self, config):\n        name = inflection.underscore(type(self).__name__)\n        mean, std = tuple(map(float, config.get(\'transform\', name).split()))\n        torchvision.transforms.Normalize.__init__(self, (mean, mean, mean), (std, std, std))\n'"
utils/__init__.py,3,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport re\nimport configparser\nimport importlib\nimport inspect\n\nimport numpy as np\nimport pandas as pd\nimport torch.autograd\nfrom PIL import Image\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, yx_min, yx_max, cls):\n        for t in self.transforms:\n            img, yx_min, yx_max, cls = t(img, yx_min, yx_max, cls)\n        return img, yx_min, yx_max, cls\n\n\nclass RegexList(list):\n    def __init__(self, l):\n        for s in l:\n            prog = re.compile(s)\n            self.append(prog)\n\n    def __call__(self, s):\n        for prog in self:\n            if prog.match(s):\n                return True\n        return False\n\n\ndef get_cache_dir(config):\n    root = os.path.expanduser(os.path.expandvars(config.get(\'config\', \'root\')))\n    name = config.get(\'cache\', \'name\')\n    return os.path.join(root, name)\n\n\ndef get_model_dir(config):\n    root = os.path.expanduser(os.path.expandvars(config.get(\'config\', \'root\')))\n    name = config.get(\'model\', \'name\')\n    model = config.get(\'model\', \'dnn\')\n    return os.path.join(root, name, model)\n\n\ndef get_eval_db(config):\n    root = os.path.expanduser(os.path.expandvars(config.get(\'config\', \'root\')))\n    db = config.get(\'eval\', \'db\')\n    return os.path.join(root, db)\n\n\ndef get_category(config, cache_dir=None):\n    path = os.path.expanduser(os.path.expandvars(config.get(\'cache\', \'category\'))) if cache_dir is None else os.path.join(cache_dir, \'category\')\n    with open(path, \'r\') as f:\n        return [line.strip() for line in f]\n\n\ndef get_anchors(config, dtype=np.float32):\n    path = os.path.expanduser(os.path.expandvars(config.get(\'model\', \'anchors\')))\n    df = pd.read_csv(path, sep=\'\\t\', dtype=dtype)\n    return df[[\'height\', \'width\']].values\n\n\ndef parse_attr(s):\n    m, n = s.rsplit(\'.\', 1)\n    m = importlib.import_module(m)\n    return getattr(m, n)\n\n\ndef load_config(config, paths):\n    for path in paths:\n        path = os.path.expanduser(os.path.expandvars(path))\n        assert os.path.exists(path)\n        config.read(path)\n\n\ndef modify_config(config, cmd):\n    var, value = cmd.split(\'=\', 1)\n    section, option = var.split(\'/\')\n    if value:\n        config.set(section, option, value)\n    else:\n        try:\n            config.remove_option(section, option)\n        except (configparser.NoSectionError, configparser.NoOptionError):\n            pass\n\n\ndef ensure_device(t, device_id=None, async=False):\n    if torch.cuda.is_available():\n        t = t.cuda(device_id, async)\n    return t\n\n\ndef dense(var):\n    return [torch.mean(torch.abs(x)) if torch.is_tensor(x) else np.abs(x) for x in var]\n\n\ndef abs_mean(data, dtype=np.float32):\n    assert isinstance(data, np.ndarray), type(data)\n    return np.sum(np.abs(data)) / dtype(data.size)\n\n\ndef image_size(path):\n    with Image.open(path) as image:\n        return image.size\n'"
utils/cache.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport numpy as np\n\n\ndef verify_coords(yx_min, yx_max, size):\n    assert np.all(yx_min <= yx_max), \'yx_min <= yx_max\'\n    assert np.all(0 <= yx_min), \'0 <= yx_min\'\n    assert np.all(0 <= yx_max), \'0 <= yx_max\'\n    assert np.all(yx_min < size), \'yx_min < size\'\n    assert np.all(yx_max < size), \'yx_max < size\'\n\n\ndef fix_coords(yx_min, yx_max, size):\n    assert np.all(yx_min <= yx_max)\n    assert yx_min.dtype == yx_max.dtype\n    coord_min = np.zeros([2], dtype=yx_min.dtype)\n    coord_max = np.array(size, dtype=yx_min.dtype) - 1\n    yx_min = np.minimum(np.maximum(yx_min, coord_min), coord_max)\n    yx_max = np.minimum(np.maximum(yx_max, coord_min), coord_max)\n    return yx_min, yx_max\n'"
utils/channel.py,23,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport inspect\nimport collections\nimport copy\nimport unittest\nimport configparser\n\nimport numpy as np\nimport torch\nimport humanize\nimport graphviz\n\nimport model.inception4\nimport model.yolo2\nimport utils\n\n\nclass Modifier(object):\n    node_attr = dict(\n        style=\'filled\',\n        shape=\'box\',\n        align=\'left\',\n        fontsize=\'12\',\n        ranksep=\'0.1\',\n        height=\'0.2\',\n    )\n    graph_attr = dict(\n        size=\'12, 12\'\n    )\n    format = \'svg\'\n\n    def __init__(self, name, state_dict, model, modify_output, modify_input, debug=False):\n        self.name = name\n        self.state_dict = state_dict\n        self.model = model\n        self.modify_output = modify_output\n        self.modify_input = modify_input\n        if debug:\n            self.dot = graphviz.Digraph(node_attr=self.node_attr, graph_attr=self.graph_attr)\n            self.dot.format = self.format\n        self.var_name = {t._cdata: k for k, t in state_dict.items()}\n        self.seen = collections.OrderedDict()\n        self.index = 0\n\n    def __call__(self, node):\n        edge = dict(\n            scope=self.model.scope(self.name),\n        )\n        return self.traverse(node, **edge)\n\n    def traverse(self, node, **edge_init):\n        if node in self.seen:\n            return self.seen[node]\n        else:\n            inputs, edge = self.merge_edges(node, edge_init)\n            edge = self.traverse_vars(node, edge)\n            edge[\'index\'] = self.index\n            if hasattr(self, \'dot\'):\n                for input in inputs:\n                    self._draw_node_edge(node, input)\n                self._draw_node(node, edge)\n            edge = copy.deepcopy(edge)\n            self.seen[node] = edge\n            self.index += 1\n        return edge\n\n    def merge_edges(self, node, edge_init):\n        edge = copy.deepcopy(edge_init)\n        edge_init.pop(\'_modify\', None)\n        if hasattr(node, \'next_functions\'):\n            inputs = []\n            for _node, _ in node.next_functions:\n                if _node is not None:\n                    _edge = self.traverse(_node, **edge_init)\n                    if \'modify\' in _edge:\n                        edge_init[\'_modify\'] = copy.deepcopy(_edge[\'modify\'])\n                    inputs.append(dict(\n                        node=_node,\n                        edge=_edge,\n                    ))\n            if inputs:\n                for key in \'channels, _channels\'.split(\', \'):\n                    edge[key] = self.merge_channels(node, inputs, key)\n                self.merge_modify(node, inputs, edge)\n        return inputs, edge\n\n    def merge_channels(self, node, inputs, key):\n        name = type(node).__name__\n        if name == \'CatBackward\':\n            channels = sum(map(lambda input: input[\'edge\'][key], inputs))\n        else:\n            channels = inputs[-1][\'edge\'][key]\n        if hasattr(self.model, \'get_mapper\'):\n            mapper = self.model.get_mapper(self.index)\n            if mapper is not None:\n                indices = torch.LongTensor(np.arange(channels))\n                indices = mapper(indices, channels)\n                channels = len(indices)\n        return channels\n\n    def merge_modify(self, node, inputs, edge):\n        for index, input in enumerate(inputs):\n            _edge = input[\'edge\']\n            if \'modify\' in _edge:\n                modify = copy.deepcopy(_edge[\'modify\'])\n                if \'modify\' in edge:\n                    if \'scope\' in modify:\n                        edge.pop(\'modify\')\n                else:\n                    begin, end = modify[\'range\']\n                    name = type(node).__name__\n                    if name == \'CatBackward\':\n                        offset = sum(map(lambda input: input[\'edge\'][\'_channels\'], inputs[:index]))\n                        begin += offset\n                        end += offset\n                    if hasattr(self.model, \'get_mapper\'):\n                        mapper = self.model.get_mapper(self.index)\n                        if mapper is not None:\n                            channels = end - begin\n                            indices = torch.LongTensor(np.arange(channels))\n                            indices = mapper(indices, channels)\n                            channels = len(indices)\n                            end = begin + channels\n                            modify[\'mappers\'].append(mapper)\n                    modify[\'range\'] = (begin, end)\n                    edge[\'modify\'] = modify\n\n    def traverse_vars(self, node, edge):\n        if hasattr(node, \'variable\'):\n            name = self.var_name[node.variable.data._cdata]\n            edge = self.modify(name, edge)\n        tensors = [t for name, t in inspect.getmembers(node) if torch.is_tensor(t)]\n        if hasattr(node, \'saved_tensors\'):\n            tensors += node.saved_tensors\n        for tensor in tensors:\n            name = self.var_name[tensor._cdata]\n            edge = self.modify(name, edge)\n            if hasattr(self, \'dot\'):\n                self._draw_tensor(node, tensor, edge)\n        return edge\n\n    def modify(self, name, edge):\n        scope = self.model.scope(name)\n        var = self.state_dict[name]\n        edge[\'_channels\'] = var.size(0)\n        if scope == edge[\'scope\']:\n            edge[\'_size\'] = var.size()\n            edge[\'modify\'] = dict(\n                range=(0, var.size(0)),\n                mappers=[],\n            )\n            var = self.modify_output(name, var)\n        elif \'_modify\' in edge:\n            modify = edge[\'_modify\']\n            if \'scope\' not in modify:\n                modify[\'scope\'] = scope\n            if \'scope\' in modify and modify[\'scope\'] == scope:\n                if len(var.size()) > 1:\n                    edge[\'_size\'] = var.size()\n                    begin, end = modify[\'range\']\n                    def mapper(indices, channels):\n                        for m in modify[\'mappers\']:\n                            indices = m(indices, channels)\n                        return indices\n                    vars = []\n                    for v in torch.unbind(var):\n                        comp = []\n                        if begin > 0:\n                            comp.append(v[:begin])\n                        _v = v[begin:end]\n                        _v = self.modify_input(name, _v, mapper)\n                        comp.append(_v)\n                        if end < var.size(1):\n                            comp.append(v[end:])\n                        v = torch.cat(comp)\n                        vars.append(v)\n                    var = torch.stack(vars)\n                    edge[\'modify\'] = modify\n            else:\n                edge.pop(\'modify\', None)\n        edge[\'channels\'] = var.size(0)\n        self.state_dict[name] = var\n        return edge\n\n    def _draw_node(self, node, edge):\n        if hasattr(node, \'variable\'):\n            name = self.var_name[node.variable.data._cdata]\n            tensor = self.state_dict[name]\n            label = \'\\n\'.join(map(str, filter(lambda x: x is not None, [\n                \'%d: %s\' % (self.index, name),\n                type(self)._pretty_size(tensor.size(), edge),\n                humanize.naturalsize(tensor.numpy().nbytes),\n            ])))\n            self.dot.node(str(id(node)), label, shape=\'note\')\n        else:\n            name = type(node).__name__\n            label = \'%d: %s\' % (self.index, name)\n            self.dot.node(str(id(node)), label, fillcolor=\'white\')\n\n    def _draw_node_edge(self, node, input):\n        _node = input[\'node\']\n        edge = input[\'edge\']\n        channels, _channels = (edge[key] for key in \'channels, _channels\'.split(\', \'))\n        label = \'\\n\'.join(map(str, filter(lambda x: x is not None, [\n            channels if channels == _channels else \'%d->%d\' % (_channels, channels),\n            type(self)._pretty_modify(edge),\n        ])))\n        if hasattr(_node, \'variable\'):\n            self.dot.edge(str(id(_node)), str(id(node)), label, arrowhead=\'none\', arrowtail=\'none\')\n        else:\n            self.dot.edge(str(id(_node)), str(id(node)), label)\n\n    def _draw_tensor(self, node, tensor, edge):\n        name = self.var_name[tensor._cdata]\n        tensor = self.state_dict[name]\n        label = \'\\n\'.join(map(str, filter(lambda x: x is not None, [\n            name,\n            type(self)._pretty_size(tensor.size(), edge),\n            humanize.naturalsize(tensor.numpy().nbytes),\n        ])))\n        self.dot.node(name, label, style=\'filled, rounded\')\n        channels, _channels = (edge[key] for key in \'channels, _channels\'.split(\', \'))\n        label = \'\\n\'.join(map(str, filter(lambda x: x is not None, [\n            channels if channels == _channels else \'%d->%d\' % (_channels, channels),\n            type(self)._pretty_modify(edge),\n        ])))\n        self.dot.edge(name, str(id(node)), label, style=\'dashed\', arrowhead=\'none\', arrowtail=\'none\')\n\n    @staticmethod\n    def _pretty_size(size, edge):\n        if \'_size\' in edge:\n            comp = []\n            for _s, s in zip(edge[\'_size\'], size):\n                if s == _s:\n                    content = str(s)\n                elif \'range\' in edge:\n                    begin, end = edge[\'range\']\n                    content = \'%d[%d:%d]->%d\' % (_s, begin, end, s)\n                else:\n                    content = \'%d->%d\' % (_s, s)\n                comp.append(content)\n            return \', \'.join(comp)\n        else:\n            return \', \'.join(map(str, size))\n\n    @staticmethod\n    def _pretty_modify(edge):\n        if \'modify\' in edge:\n            modify = edge[\'modify\']\n            mode = \'input\' if \'scope\' in modify else \'output\'\n            begin, end = modify[\'range\']\n            return \'%s[%d:%d]\' % (mode, begin, end)\n\n\nclass TestInception4(unittest.TestCase):\n    def setUp(self):\n        config = configparser.ConfigParser()\n        self.config_channels = model.ConfigChannels(config)\n        self.category = [\'test%d\' % i for i in range(1)]\n        self.anchors = torch.from_numpy(np.array([\n            (1, 1),\n            (1, 2),\n        ], dtype=np.float32))\n        module = model.inception4\n        self.model = module.Inception4\n        size = module.pretrained_settings[\'inceptionv4\'][\'imagenet\'][\'input_size\'][1:]\n        self.image = torch.autograd.Variable(torch.randn(1, 3, *size))\n        self.keep = 0.3\n\n    def test_features_2_conv_weight(self):\n        dnn = self.model(self.config_channels, self.anchors, len(self.category))\n        output = dnn(self.image)\n        state_dict = dnn.state_dict()\n        name = \'.\'.join(self.id().split(\'.\')[-1].split(\'_\')[1:])\n        d = utils.dense(state_dict[name])\n        keep = torch.LongTensor(np.argsort(d)[int(len(d) * 0.5):])\n        modifier = Modifier(\n            name, state_dict, dnn,\n            lambda name, var: var[keep],\n            lambda name, var, mapper: var[mapper(keep, len(d))],\n        )\n        modifier(output.grad_fn)\n        # check channels\n        scope = dnn.scope(name)\n        self.assertEqual(state_dict[name].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.weight\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.bias\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_mean\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_var\'].size(0), len(keep))\n        # check if runnable\n        config_channels = model.ConfigChannels(self.config_channels.config, state_dict)\n        dnn = self.model(config_channels, self.anchors, len(self.category))\n        dnn.load_state_dict(state_dict)\n        dnn(self.image)\n\n    def test_features_3_conv_conv_weight(self):\n        dnn = self.model(self.config_channels, self.anchors, len(self.category))\n        output = dnn(self.image)\n        state_dict = dnn.state_dict()\n        name = \'.\'.join(self.id().split(\'.\')[-1].split(\'_\')[1:])\n        d = utils.dense(state_dict[name])\n        keep = torch.LongTensor(np.argsort(d)[int(len(d) * 0.5):])\n        modifier = Modifier(\n            name, state_dict, dnn,\n            lambda name, var: var[keep],\n            lambda name, var, mapper: var[mapper(keep, len(d))],\n        )\n        modifier(output.grad_fn)\n        # check channels\n        scope = dnn.scope(name)\n        self.assertEqual(state_dict[name].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.weight\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.bias\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_mean\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_var\'].size(0), len(keep))\n        # check if runnable\n        config_channels = model.ConfigChannels(self.config_channels.config, state_dict)\n        dnn = self.model(config_channels, self.anchors, len(self.category))\n        dnn.load_state_dict(state_dict)\n        dnn(self.image)\n\n    def test_features_4_branch1_3_conv_weight(self):\n        dnn = self.model(self.config_channels, self.anchors, len(self.category))\n        output = dnn(self.image)\n        state_dict = dnn.state_dict()\n        name = \'.\'.join(self.id().split(\'.\')[-1].split(\'_\')[1:])\n        d = utils.dense(state_dict[name])\n        keep = torch.LongTensor(np.argsort(d)[int(len(d) * 0.5):])\n        modifier = Modifier(\n            name, state_dict, dnn,\n            lambda name, var: var[keep],\n            lambda name, var, mapper: var[mapper(keep, len(d))],\n        )\n        modifier(output.grad_fn)\n        # check channels\n        scope = dnn.scope(name)\n        self.assertEqual(state_dict[name].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.weight\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.bias\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_mean\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_var\'].size(0), len(keep))\n        # check if runnable\n        config_channels = model.ConfigChannels(self.config_channels.config, state_dict)\n        dnn = self.model(config_channels, self.anchors, len(self.category))\n        dnn.load_state_dict(state_dict)\n        dnn(self.image)\n\n    def test_features_5_conv_conv_weight(self):\n        dnn = self.model(self.config_channels, self.anchors, len(self.category))\n        output = dnn(self.image)\n        state_dict = dnn.state_dict()\n        name = \'.\'.join(self.id().split(\'.\')[-1].split(\'_\')[1:])\n        d = utils.dense(state_dict[name])\n        keep = torch.LongTensor(np.argsort(d)[int(len(d) * 0.5):])\n        modifier = Modifier(\n            name, state_dict, dnn,\n            lambda name, var: var[keep],\n            lambda name, var, mapper: var[mapper(keep, len(d))],\n        )\n        modifier(output.grad_fn)\n        # check channels\n        scope = dnn.scope(name)\n        self.assertEqual(state_dict[name].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.weight\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.bias\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_mean\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_var\'].size(0), len(keep))\n        # check if runnable\n        config_channels = model.ConfigChannels(self.config_channels.config, state_dict)\n        dnn = self.model(config_channels, self.anchors, len(self.category))\n        dnn.load_state_dict(state_dict)\n        dnn(self.image)\n\n\nclass TestYolo2Tiny(unittest.TestCase):\n    def setUp(self):\n        config = configparser.ConfigParser()\n        self.config_channels = model.ConfigChannels(config)\n        self.category = [\'test%d\' % i for i in range(1)]\n        self.anchors = torch.from_numpy(np.array([\n            (1, 1),\n            (1, 2),\n        ], dtype=np.float32))\n        module = model.yolo2\n        self.model = module.Tiny\n        size = module.settings[\'size\']\n        self.image = torch.autograd.Variable(torch.randn(1, 3, *size))\n        self.keep = 0.3\n\n    def test_layers_14_conv_weight(self):\n        dnn = self.model(self.config_channels, self.anchors, len(self.category))\n        output = dnn(self.image)\n        state_dict = dnn.state_dict()\n        name = \'.\'.join(self.id().split(\'.\')[-1].split(\'_\')[1:])\n        d = utils.dense(state_dict[name])\n        keep = torch.LongTensor(np.argsort(d)[int(len(d) * 0.5):])\n        modifier = Modifier(\n            name, state_dict, dnn,\n            lambda name, var: var[keep],\n            lambda name, var, mapper: var[mapper(keep, len(d))],\n        )\n        modifier(output.grad_fn)\n        # check channels\n        scope = dnn.scope(name)\n        self.assertEqual(state_dict[name].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.weight\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.bias\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_mean\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_var\'].size(0), len(keep))\n        # check if runnable\n        config_channels = model.ConfigChannels(self.config_channels.config, state_dict)\n        dnn = self.model(config_channels, self.anchors, len(self.category))\n        dnn.load_state_dict(state_dict)\n        dnn(self.image)\n\n\nclass TestYolo2Darknet(unittest.TestCase):\n    def setUp(self):\n        config = configparser.ConfigParser()\n        self.config_channels = model.ConfigChannels(config)\n        self.category = [\'test%d\' % i for i in range(1)]\n        self.anchors = torch.from_numpy(np.array([\n            (1, 1),\n            (1, 2),\n        ], dtype=np.float32))\n        module = model.yolo2\n        self.model = module.Darknet\n        size = module.settings[\'size\']\n        self.image = torch.autograd.Variable(torch.randn(1, 3, *size))\n        self.keep = 0.3\n\n    def test_layers1_0_conv_weight(self):\n        dnn = self.model(self.config_channels, self.anchors, len(self.category))\n        output = dnn(self.image)\n        state_dict = dnn.state_dict()\n        name = \'.\'.join(self.id().split(\'.\')[-1].split(\'_\')[1:])\n        d = utils.dense(state_dict[name])\n        keep = torch.LongTensor(np.argsort(d)[:int(len(d) * self.keep)])\n        modifier = Modifier(\n            name, state_dict, dnn,\n            lambda name, var: var[keep],\n            lambda name, var, mapper: var[mapper(keep, len(d))],\n        )\n        modifier(output.grad_fn)\n        # check channels\n        scope = dnn.scope(name)\n        self.assertEqual(state_dict[name].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.weight\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.bias\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_mean\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_var\'].size(0), len(keep))\n        # check if runnable\n        config_channels = model.ConfigChannels(self.config_channels.config, state_dict)\n        dnn = self.model(config_channels, self.anchors, len(self.category))\n        dnn.load_state_dict(state_dict)\n        dnn(self.image)\n\n    def test_layers1_16_conv_weight(self):\n        dnn = self.model(self.config_channels, self.anchors, len(self.category))\n        output = dnn(self.image)\n        state_dict = dnn.state_dict()\n        name = \'.\'.join(self.id().split(\'.\')[-1].split(\'_\')[1:])\n        d = utils.dense(state_dict[name])\n        keep = torch.LongTensor(np.argsort(d)[:int(len(d) * self.keep)])\n        modifier = Modifier(\n            name, state_dict, dnn,\n            lambda name, var: var[keep],\n            lambda name, var, mapper: var[mapper(keep, len(d))],\n        )\n        modifier(output.grad_fn)\n        # check channels\n        scope = dnn.scope(name)\n        self.assertEqual(state_dict[name].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.weight\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.bias\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_mean\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_var\'].size(0), len(keep))\n        # check if runnable\n        config_channels = model.ConfigChannels(self.config_channels.config, state_dict)\n        dnn = self.model(config_channels, self.anchors, len(self.category))\n        dnn.load_state_dict(state_dict)\n        dnn(self.image)\n\n    def test_passthrough_conv_weight(self):\n        dnn = self.model(self.config_channels, self.anchors, len(self.category))\n        output = dnn(self.image)\n        state_dict = dnn.state_dict()\n        name = \'.\'.join(self.id().split(\'.\')[-1].split(\'_\')[1:])\n        d = utils.dense(state_dict[name])\n        keep = torch.LongTensor(np.argsort(d)[int(len(d) * 0.5):])\n        modifier = Modifier(\n            name, state_dict, dnn,\n            lambda name, var: var[keep],\n            lambda name, var, mapper: var[mapper(keep, len(d))],\n        )\n        modifier(output.grad_fn)\n        # check channels\n        scope = dnn.scope(name)\n        self.assertEqual(state_dict[name].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.weight\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.bias\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_mean\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_var\'].size(0), len(keep))\n        # check if runnable\n        config_channels = model.ConfigChannels(self.config_channels.config, state_dict)\n        dnn = self.model(config_channels, self.anchors, len(self.category))\n        dnn.load_state_dict(state_dict)\n        dnn(self.image)\n\n    def test_layers2_1_conv_weight(self):\n        dnn = self.model(self.config_channels, self.anchors, len(self.category))\n        output = dnn(self.image)\n        state_dict = dnn.state_dict()\n        name = \'.\'.join(self.id().split(\'.\')[-1].split(\'_\')[1:])\n        d = utils.dense(state_dict[name])\n        keep = torch.LongTensor(np.argsort(d)[int(len(d) * 0.5):])\n        modifier = Modifier(\n            name, state_dict, dnn,\n            lambda name, var: var[keep],\n            lambda name, var, mapper: var[mapper(keep, len(d))],\n        )\n        modifier(output.grad_fn)\n        # check channels\n        scope = dnn.scope(name)\n        self.assertEqual(state_dict[name].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.weight\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.bias\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_mean\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_var\'].size(0), len(keep))\n        # check if runnable\n        config_channels = model.ConfigChannels(self.config_channels.config, state_dict)\n        dnn = self.model(config_channels, self.anchors, len(self.category))\n        dnn.load_state_dict(state_dict)\n        dnn(self.image)\n\n    def test_layers2_7_conv_weight(self):\n        dnn = self.model(self.config_channels, self.anchors, len(self.category))\n        output = dnn(self.image)\n        state_dict = dnn.state_dict()\n        name = \'.\'.join(self.id().split(\'.\')[-1].split(\'_\')[1:])\n        d = utils.dense(state_dict[name])\n        keep = torch.LongTensor(np.argsort(d)[int(len(d) * 0.5):])\n        modifier = Modifier(\n            name, state_dict, dnn,\n            lambda name, var: var[keep],\n            lambda name, var, mapper: var[mapper(keep, len(d))],\n        )\n        modifier(output.grad_fn)\n        # check channels\n        scope = dnn.scope(name)\n        self.assertEqual(state_dict[name].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.weight\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.bias\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_mean\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_var\'].size(0), len(keep))\n        # check if runnable\n        config_channels = model.ConfigChannels(self.config_channels.config, state_dict)\n        dnn = self.model(config_channels, self.anchors, len(self.category))\n        dnn.load_state_dict(state_dict)\n        dnn(self.image)\n\n    def test_layers3_0_conv_weight(self):\n        dnn = self.model(self.config_channels, self.anchors, len(self.category))\n        output = dnn(self.image)\n        state_dict = dnn.state_dict()\n        name = \'.\'.join(self.id().split(\'.\')[-1].split(\'_\')[1:])\n        d = utils.dense(state_dict[name])\n        keep = torch.LongTensor(np.argsort(d)[int(len(d) * 0.5):])\n        modifier = Modifier(\n            name, state_dict, dnn,\n            lambda name, var: var[keep],\n            lambda name, var, mapper: var[mapper(keep, len(d))],\n        )\n        modifier(output.grad_fn)\n        # check channels\n        scope = dnn.scope(name)\n        self.assertEqual(state_dict[name].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.weight\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.bias\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_mean\'].size(0), len(keep))\n        self.assertEqual(state_dict[scope + \'.bn.running_var\'].size(0), len(keep))\n        # check if runnable\n        config_channels = model.ConfigChannels(self.config_channels.config, state_dict)\n        dnn = self.model(config_channels, self.anchors, len(self.category))\n        dnn.load_state_dict(state_dict)\n        dnn(self.image)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
utils/data.py,3,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport pickle\nimport random\nimport copy\n\nimport numpy as np\nimport torch.utils.data\nimport sklearn.preprocessing\nimport cv2\n\n\ndef padding_labels(data, dim, labels=\'yx_min, yx_max, cls, difficult\'.split(\', \')):\n    """"""\n    Padding labels into the same dimension (to form a batch).\n    :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n    :param data: A dict contains the labels to be padded.\n    :param dim: The target dimension.\n    :param labels: The list of label names.\n    :return: The padded label dict.\n    """"""\n    pad = dim - len(data[labels[0]])\n    for key in labels:\n        label = data[key]\n        data[key] = np.pad(label, [(0, pad)] + [(0, 0)] * (len(label.shape) - 1), \'constant\')\n    return data\n\n\ndef load_pickles(paths):\n    data = []\n    for path in paths:\n        with open(path, \'rb\') as f:\n            data += pickle.load(f)\n    return data\n\n\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, data, transform=lambda data: data, one_hot=None, shuffle=False, dir=None):\n        """"""\n        Load the cached data (.pkl) into memory.\n        :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n        :param data: A list contains the data samples (dict).\n        :param transform: A function transforms (usually performs a sequence of data augmentation operations) the labels in a dict.\n        :param one_hot: If a int value (total number of classes) is given, the class label (key ""cls"") will be generated in a one-hot format.\n        :param shuffle: Shuffle the loaded dataset.\n        :param dir: The directory to store the exception data.\n        """"""\n        self.data = data\n        if shuffle:\n            random.shuffle(self.data)\n        self.transform = transform\n        self.one_hot = None if one_hot is None else sklearn.preprocessing.OneHotEncoder(one_hot, dtype=np.float32)\n        self.dir = dir\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        data = copy.deepcopy(self.data[index])\n        try:\n            image = cv2.imread(data[\'path\'])\n            data[\'image\'] = image\n            data[\'size\'] = np.array(image.shape[:2])\n            data = self.transform(data)\n            if self.one_hot is not None:\n                data[\'cls\'] = self.one_hot.fit_transform(np.expand_dims(data[\'cls\'], -1)).todense()\n        except:\n            if self.dir is not None:\n                os.makedirs(self.dir, exist_ok=True)\n                name = self.__module__ + \'.\' + type(self).__name__\n                with open(os.path.join(self.dir, name + \'.pkl\'), \'wb\') as f:\n                    pickle.dump(data, f)\n            raise\n        return data\n\n\nclass Collate(object):\n    def __init__(self, resize, sizes, maintain=1, transform_image=lambda image: image, transform_tensor=None, dir=None):\n        """"""\n        Unify multiple data samples (e.g., resize images into the same size, and padding bounding box labels into the same number) to form a batch.\n        :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n        :param resize: A function to resize the image and labels.\n        :param sizes: The image sizes to be randomly choosed.\n        :param maintain: How many times a size to be maintained.\n        :param transform_image: A function to transform the resized image.\n        :param transform_tensor: A function to standardize a image into a tensor.\n        :param dir: The directory to store the exception data.\n        """"""\n        self.resize = resize\n        self.sizes = sizes\n        assert maintain > 0\n        self.maintain = maintain\n        self._maintain = maintain\n        self.transform_image = transform_image\n        self.transform_tensor = transform_tensor\n        self.dir = dir\n\n    def __call__(self, batch):\n        height, width = self.next_size()\n        dim = max(len(data[\'cls\']) for data in batch)\n        _batch = []\n        for data in batch:\n            try:\n                data = self.resize(data, height, width)\n                data[\'image\'] = self.transform_image(data[\'image\'])\n                data = padding_labels(data, dim)\n                if self.transform_tensor is not None:\n                    data[\'tensor\'] = self.transform_tensor(data[\'image\'])\n                _batch.append(data)\n            except:\n                if self.dir is not None:\n                    os.makedirs(self.dir, exist_ok=True)\n                    name = self.__module__ + \'.\' + type(self).__name__\n                    with open(os.path.join(self.dir, name + \'.pkl\'), \'wb\') as f:\n                        pickle.dump(data, f)\n                raise\n        return torch.utils.data.dataloader.default_collate(_batch)\n\n    def next_size(self):\n        if self._maintain < self.maintain:\n            self._maintain += 1\n        else:\n            self.size = random.choice(self.sizes)\n            self._maintain = 0\n        return self.size\n'"
utils/postprocess.py,3,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport torch\n\nimport utils.iou.torch\n\n\ndef nms(score, yx_min, yx_max, overlap=0.5, limit=200):\n    """"""\n    Filtering the overlapping (IoU > overlap threshold) bounding boxes according to the score (in descending order).\n    :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n    :param score: The scores of the list (size [N]) of bounding boxes.\n    :param yx_min: The top left coordinates (y, x) of the list (size [N, 2]) of bounding boxes.\n    :param yx_max: The bottom right coordinates (y, x) of the list (size [N, 2]) of bounding boxes.\n    :param overlap: The IoU threshold.\n    :param limit: Limits the number of results.\n    :return: The indices of the selected bounding boxes.\n    """"""\n    keep = []\n    if score.numel() == 0:\n        return keep\n    _, index = score.sort(descending=True)\n    index = index[:limit]\n    while index.numel() > 0:\n        i = index[0]\n        keep.append(i)\n        if index.size(0) == 1:\n            break\n        index = index[1:]\n        yx_min1, yx_max1 = (torch.unsqueeze(t[i], 0) for t in (yx_min, yx_max))\n        yx_min2, yx_max2 = (torch.index_select(t, 0, index) for t in (yx_min, yx_max))\n        iou = utils.iou.torch.iou_matrix(yx_min1, yx_max1, yx_min2, yx_max2)[0]\n        index = index[iou <= overlap]\n    return keep\n'"
utils/train.py,1,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport os\nimport time\nimport operator\nimport logging\n\nimport torch\n\n\nclass Timer(object):\n    def __init__(self, max, first=True):\n        """"""\n        A simple function object to determine time event.\n        :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n        :param max: Number of seconds to trigger a time event.\n        :param first: Should a time event to be triggered at the first time.\n        """"""\n        self.start = 0 if first else time.time()\n        self.max = max\n\n    def __call__(self):\n        """"""\n        Return a boolean value to indicate if the time event is occurred.\n        :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n        """"""\n        t = time.time()\n        elapsed = t - self.start\n        if elapsed > self.max:\n            self.start = t\n            return True\n        else:\n            return False\n\n\ndef load_model(model_dir, step=None, ext=\'.pth\', ext_epoch=\'.epoch\', logger=logging.info):\n    """"""\n    Load the latest checkpoint in a model directory.\n    :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n    :param model_dir: The directory to store the model checkpoint files.\n    :param step: If a integer value is given, the corresponding checkpoint will be loaded. Otherwise, the latest checkpoint (with the largest step value) will be loaded.\n    :param ext: The extension of the model file.\n    :param ext_epoch: The extension of the epoch file.\n    :return:\n    """"""\n    if step is None:\n        steps = [(int(n), n) for n, e in map(os.path.splitext, os.listdir(model_dir)) if n.isdigit() and e == ext]\n        step, name = max(steps, key=operator.itemgetter(0))\n    else:\n        name = str(step)\n    prefix = os.path.join(model_dir, name)\n    if logger is not None:\n        logger(\'load %s.*\' % prefix)\n    try:\n        with open(prefix + ext_epoch, \'r\') as f:\n            epoch = int(f.read())\n    except (FileNotFoundError, ValueError):\n        epoch = None\n    path = prefix + ext\n    assert os.path.exists(path), path\n    return path, step, epoch\n\n\nclass Saver(object):\n    def __init__(self, model_dir, keep, ext=\'.pth\', ext_epoch=\'.epoch\', logger=logging.info):\n        """"""\n        Manage several latest checkpoints (with the largest step values) in a model directory.\n        :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n        :param model_dir: The directory to store the model checkpoint files.\n        :param keep: How many latest checkpoints to be maintained.\n        :param ext: The extension of the model file.\n        :param ext_epoch: The extension of the epoch file.\n        """"""\n        self.model_dir = model_dir\n        self.keep = keep\n        self.ext = ext\n        self.ext_epoch = ext_epoch\n        self.logger = (lambda s: s) if logger is None else logger\n\n    def __call__(self, obj, step, epoch=None):\n        """"""\n        Save the PyTorch module.\n        :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n        :param obj: The PyTorch module to be saved.\n        :param step: Current step.\n        :param epoch: Current epoch.\n        """"""\n        os.makedirs(self.model_dir, exist_ok=True)\n        prefix = os.path.join(self.model_dir, str(step))\n        torch.save(obj, prefix + self.ext)\n        if epoch is not None:\n            with open(prefix + self.ext_epoch, \'w\') as f:\n                f.write(str(epoch))\n        self.logger(\'model saved into %s.*\' % prefix)\n        self.tidy()\n        return prefix\n\n    def tidy(self):\n        steps = [(int(n), n) for n, e in map(os.path.splitext, os.listdir(self.model_dir)) if n.isdigit() and e == self.ext]\n        if len(steps) > self.keep:\n            steps = sorted(steps, key=operator.itemgetter(0))\n            remove = steps[:len(steps) - self.keep]\n            for _, n in remove:\n                path = os.path.join(self.model_dir, n)\n                os.remove(path + self.ext)\n                path_epoch = path + self.ext_epoch\n                try:\n                    os.remove(path_epoch)\n                except FileNotFoundError:\n                    self.logger(path_epoch + \' not found\')\n                logging.debug(\'tidy \' + path)\n\n\ndef load_sizes(config):\n    sizes = [s.split(\',\') for s in config.get(\'data\', \'sizes\').split()]\n    return [(int(height), int(width)) for height, width in sizes]\n'"
utils/visualize.py,1,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport logging\nimport itertools\nimport inspect\n\nimport numpy as np\nimport torch\nimport matplotlib\nimport matplotlib.cm\nimport matplotlib.colors\nimport matplotlib.pyplot as plt\nimport humanize\nimport graphviz\nimport cv2\n\nimport utils\n\n\nclass DrawBBox(object):\n    def __init__(self, category, colors=[], thickness=3, line_type=cv2.LINE_8, shift=0, font_face=cv2.FONT_HERSHEY_SIMPLEX, font_scale=1):\n        self.category = category\n        if colors:\n            self.colors = [tuple(map(lambda c: c * 255, matplotlib.colors.colorConverter.to_rgb(c)[::-1])) for c in colors]\n        else:\n            self.colors = [tuple(map(lambda c: c * 255, matplotlib.colors.colorConverter.to_rgb(prop[\'color\'])[::-1])) for prop in plt.rcParams[\'axes.prop_cycle\']]\n        self.thickness = thickness\n        self.line_type = line_type\n        self.shift = shift\n        self.font_face = font_face\n        self.font_scale = font_scale\n\n    def __call__(self, image, yx_min, yx_max, cls=None, colors=None, debug=False):\n        colors = self.colors if colors is None else [tuple(map(lambda c: c * 255, matplotlib.colors.colorConverter.to_rgb(c)[::-1])) for c in colors]\n        if cls is None:\n            cls = [None] * len(yx_min)\n        for color, (ymin, xmin), (ymax, xmax), cls in zip(itertools.cycle(colors), yx_min, yx_max, cls):\n            try:\n                cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, thickness=self.thickness, lineType=self.line_type, shift=self.shift)\n                if cls is not None:\n                    cv2.putText(image, self.category[cls], (xmin, ymin), self.font_face, self.font_scale, color=color, thickness=self.thickness)\n            except OverflowError as e:\n                logging.warning(e, (xmin, ymin), (xmax, ymax))\n        if debug:\n            cv2.imshow(\'\', image)\n            cv2.waitKey(0)\n        return image\n\n\nclass DrawFeature(object):\n    def __init__(self, alpha=0.5, cmap=None):\n        self.alpha = alpha\n        self.cm = matplotlib.cm.get_cmap(cmap)\n\n    def __call__(self, image, feature, debug=False):\n        _feature = (feature * self.cm.N).astype(np.int)\n        heatmap = self.cm(_feature)[:, :, :3] * 255\n        heatmap = cv2.resize(heatmap, image.shape[1::-1], interpolation=cv2.INTER_NEAREST)\n        canvas = (image * (1 - self.alpha) + heatmap * self.alpha).astype(np.uint8)\n        if debug:\n            cv2.imshow(\'max=%f, sum=%f\' % (np.max(feature), np.sum(feature)), canvas)\n            cv2.waitKey(0)\n        return canvas\n\n\nclass Graph(object):\n    def __init__(self, config, state_dict, cmap=None):\n        self.dot = graphviz.Digraph(node_attr=dict(config.items(\'digraph_node_attr\')), graph_attr=dict(config.items(\'digraph_graph_attr\')))\n        self.dot.format = config.get(\'graph\', \'format\')\n        self.state_dict = state_dict\n        self.var_name = {t._cdata: k for k, t in state_dict.items()}\n        self.seen = set()\n        self.index = 0\n        self.drawn = set()\n        self.cm = matplotlib.cm.get_cmap(cmap)\n        self.metric = eval(config.get(\'graph\', \'metric\'))\n        metrics = [self.metric(t) for t in state_dict.values()]\n        self.minmax = [min(metrics), max(metrics)]\n\n    def __call__(self, node):\n        if node not in self.seen:\n            self.traverse_next(node)\n            self.traverse_tensor(node)\n            self.seen.add(node)\n            self.index += 1\n\n    def traverse_next(self, node):\n        if hasattr(node, \'next_functions\'):\n            for n, _ in node.next_functions:\n                if n is not None:\n                    self.__call__(n)\n                    self._draw_node_edge(node, n)\n        self._draw_node(node)\n\n    def traverse_tensor(self, node):\n        tensors = [t for name, t in inspect.getmembers(node) if torch.is_tensor(t)]\n        if hasattr(node, \'saved_tensors\'):\n            tensors += node.saved_tensors\n        for tensor in tensors:\n            name = self.var_name[tensor._cdata]\n            self.drawn.add(name)\n            self._draw_tensor(node, tensor)\n\n    def _draw_node(self, node):\n        if hasattr(node, \'variable\'):\n            tensor = node.variable.data\n            name = self.var_name[tensor._cdata]\n            label = \'\\n\'.join(map(str, [\n                \'%d: %s\' % (self.index, name),\n                list(tensor.size()),\n                humanize.naturalsize(tensor.numpy().nbytes),\n            ]))\n            fillcolor, fontcolor = self._tensor_color(tensor)\n            self.dot.node(str(id(node)), label, shape=\'note\', fillcolor=fillcolor, fontcolor=fontcolor)\n            self.drawn.add(name)\n        else:\n            self.dot.node(str(id(node)), \'%d: %s\' % (self.index, type(node).__name__), fillcolor=\'white\')\n\n    def _draw_node_edge(self, node, n):\n        if hasattr(n, \'variable\'):\n            self.dot.edge(str(id(n)), str(id(node)), arrowhead=\'none\', arrowtail=\'none\')\n        else:\n            self.dot.edge(str(id(n)), str(id(node)))\n\n    def _draw_tensor(self, node, tensor):\n        name = self.var_name[tensor._cdata]\n        label = \'\\n\'.join(map(str, [\n            name,\n            list(tensor.size()),\n            humanize.naturalsize(tensor.numpy().nbytes),\n        ]))\n        fillcolor, fontcolor = self._tensor_color(tensor)\n        self.dot.node(name, label, style=\'filled, rounded\', fillcolor=fillcolor, fontcolor=fontcolor)\n        self.dot.edge(name, str(id(node)), style=\'dashed\', arrowhead=\'none\', arrowtail=\'none\')\n\n    def _tensor_color(self, tensor):\n        level = self._norm(self.metric(tensor))\n        fillcolor = self.cm(np.int(level * self.cm.N))\n        fontcolor = self.cm(self.cm.N if level < 0.5 else 0)\n        return matplotlib.colors.to_hex(fillcolor), matplotlib.colors.to_hex(fontcolor)\n\n    def _norm(self, metric):\n        min, max = self.minmax\n        assert min <= metric <= max, (metric, self.minmax)\n        if min < max:\n            return (metric - min) / (max - min)\n        else:\n            return metric\n'"
transform/resize/__init__.py,0,b''
transform/resize/image.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport inflection\nimport numpy as np\nimport cv2\n\n\ndef rescale(image, height, width):\n    return cv2.resize(image, (width, height))\n\n\nclass Rescale(object):\n    def __init__(self):\n        name = inflection.underscore(type(self).__name__)\n        self.fn = eval(name)\n\n    def __call__(self, image, height, width):\n        return self.fn(image, height, width)\n\n\ndef fixed(image, height, width):\n    _height, _width, _ = image.shape\n    if _height / _width > height / width:\n        scale = height / _height\n    else:\n        scale = width / _width\n    m = np.eye(2, 3)\n    m[0, 0] = scale\n    m[1, 1] = scale\n    flags = cv2.INTER_AREA if scale < 1 else cv2.INTER_CUBIC\n    return cv2.warpAffine(image, m, (width, height), flags=flags)\n\n\nclass Fixed(object):\n    def __init__(self):\n        name = inflection.underscore(type(self).__name__)\n        self.fn = eval(name)\n\n    def __call__(self, image, height, width):\n        return self.fn(image, height, width)\n\n\nclass Resize(object):\n    def __init__(self, config):\n        name = config.get(\'data\', inflection.underscore(type(self).__name__))\n        self.fn = eval(name)\n\n    def __call__(self, image, height, width):\n        return self.fn(image, height, width)\n'"
transform/resize/label.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport inspect\n\nimport inflection\nimport numpy as np\nimport cv2\n\n\ndef rescale(image, yx_min, yx_max, height, width):\n    _height, _width = image.shape[:2]\n    scale = np.array([height / _height, width / _width], np.float32)\n    image = cv2.resize(image, (width, height))\n    yx_min *= scale\n    yx_max *= scale\n    return image, yx_min, yx_max\n\n\nclass Rescale(object):\n    def __init__(self):\n        self.fn = eval(inflection.underscore(type(self).__name__))\n\n    def __call__(self, data, height, width):\n        data[\'image\'], data[\'yx_min\'], data[\'yx_max\'] = self.fn(data[\'image\'], data[\'yx_min\'], data[\'yx_max\'], height, width)\n        return data\n\n\ndef resize(config, image, yx_min, yx_max, height, width):\n    fn = eval(config.get(\'data\', inspect.stack()[0][3]))\n    return fn(image, yx_min, yx_max, height, width)\n\n\nclass Resize(object):\n    def __init__(self, config):\n        self.config = config\n        self.fn = eval(config.get(\'data\', inflection.underscore(type(self).__name__)))\n\n    def __call__(self, data, height, width):\n        data[\'image\'], data[\'yx_min\'], data[\'yx_max\'] = self.fn(data[\'image\'], data[\'yx_min\'], data[\'yx_max\'], height, width)\n        return data\n\n\ndef random_crop(config, image, yx_min, yx_max, height, width):\n    name = inspect.stack()[0][3]\n    scale = config.getfloat(\'augmentation\', name)\n    assert 0 < scale <= 1\n    _yx_min = np.min(yx_min, 0)\n    _yx_max = np.max(yx_max, 0)\n    dtype = yx_min.dtype\n    size = np.array(image.shape[:2], dtype)\n    margin = scale * np.random.rand(4).astype(dtype) * np.concatenate([_yx_min, size - _yx_max], 0)\n    _yx_min = margin[:2]\n    _yx_max = size - margin[2:]\n    _ymin, _xmin = _yx_min\n    _ymax, _xmax = _yx_max\n    _ymin, _xmin, _ymax, _xmax = tuple(map(int, (_ymin, _xmin, _ymax, _xmax)))\n    image = image[_ymin:_ymax, _xmin:_xmax, :]\n    yx_min, yx_max = yx_min - _yx_min, yx_max - _yx_min\n    return resize(config, image, yx_min, yx_max, height, width)\n\n\nclass RandomCrop(object):\n    def __init__(self, config):\n        self.config = config\n        self.fn = eval(inflection.underscore(type(self).__name__))\n\n    def __call__(self, data, height, width):\n        data[\'image\'], data[\'yx_min\'], data[\'yx_max\'] = self.fn(self.config, data[\'image\'], data[\'yx_min\'], data[\'yx_max\'], height, width)\n        return data\n'"
utils/iou/__init__.py,0,b''
utils/iou/numpy.py,0,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport unittest\n\nimport numpy as np\n\n\ndef iou(yx_min1, yx_max1, yx_min2, yx_max2, min=None):\n    """"""\n    Calculates the IoU of two bounding boxes.\n    :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n    :param yx_min1: The top left coordinates (y, x) of the first bounding boxe.\n    :param yx_max1: The bottom right coordinates (y, x) of the first bounding boxe.\n    :param yx_min2: The top left coordinates (y, x) of the second bounding boxe.\n    :param yx_max2: The bottom right coordinates (y, x) of the second bounding boxe.\n    :return: The IoU.\n    """"""\n    assert np.all(yx_min1 <= yx_max1)\n    assert np.all(yx_min2 <= yx_max2)\n    if min is None:\n        min = np.finfo(yx_min1.dtype).eps\n    yx_min = np.maximum(yx_min1, yx_min2)\n    yx_max = np.minimum(yx_max1, yx_max2)\n    intersect_area = np.multiply.reduce(np.maximum(0.0, yx_max - yx_min))\n    area1 = np.multiply.reduce(yx_max1 - yx_min1)\n    area2 = np.multiply.reduce(yx_max2 - yx_min2)\n    assert np.all(intersect_area >= 0)\n    assert np.all(intersect_area <= area1)\n    assert np.all(intersect_area <= area2)\n    union_area = np.maximum(area1 + area2 - intersect_area, min)\n    return intersect_area / union_area\n\n\ndef intersection_area(yx_min1, yx_max1, yx_min2, yx_max2):\n    """"""\n    Calculates the intersection area of two lists of bounding boxes.\n    :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n    :param yx_min1: The top left coordinates (y, x) of the first list (size [N1, 2]) of bounding boxes.\n    :param yx_max1: The bottom right coordinates (y, x) of the first list (size [N1, 2]) of bounding boxes.\n    :param yx_min2: The top left coordinates (y, x) of the second list (size [N2, 2]) of bounding boxes.\n    :param yx_max2: The bottom right coordinates (y, x) of the second list (size [N2, 2]) of bounding boxes.\n    :return: The matrix (size [N1, N2]) of the intersection area.\n    """"""\n    ymin1, xmin1 = yx_min1.T\n    ymax1, xmax1 = yx_max1.T\n    ymin2, xmin2 = yx_min2.T\n    ymax2, xmax2 = yx_max2.T\n    ymin1, xmin1, ymax1, xmax1, ymin2, xmin2, ymax2, xmax2 = (np.expand_dims(a, -1) for a in (ymin1, xmin1, ymax1, xmax1, ymin2, xmin2, ymax2, xmax2))\n    max_ymin = np.maximum(ymin1, np.transpose(ymin2))\n    min_ymax = np.minimum(ymax1, np.transpose(ymax2))\n    height = np.maximum(0.0, min_ymax - max_ymin)\n    max_xmin = np.maximum(xmin1, np.transpose(xmin2))\n    min_xmax = np.minimum(xmax1, np.transpose(xmax2))\n    width = np.maximum(0.0, min_xmax - max_xmin)\n    return height * width\n\n\ndef iou_matrix(yx_min1, yx_max1, yx_min2, yx_max2, min=None):\n    """"""\n    Calculates the IoU of two lists of bounding boxes.\n    :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n    :param yx_min1: The top left coordinates (y, x) of the first list (size [N1, 2]) of bounding boxes.\n    :param yx_max1: The bottom right coordinates (y, x) of the first list (size [N1, 2]) of bounding boxes.\n    :param yx_min2: The top left coordinates (y, x) of the second list (size [N2, 2]) of bounding boxes.\n    :param yx_max2: The bottom right coordinates (y, x) of the second list (size [N2, 2]) of bounding boxes.\n    :return: The matrix (size [N1, N2]) of the IoU.\n    """"""\n    if min is None:\n        min = np.finfo(yx_min1.dtype).eps\n    assert np.all(yx_min1 <= yx_max1)\n    assert np.all(yx_min2 <= yx_max2)\n    intersect_area = intersection_area(yx_min1, yx_max1, yx_min2, yx_max2)\n    area1 = np.expand_dims(np.multiply.reduce(yx_max1 - yx_min1, -1), 1)\n    area2 = np.expand_dims(np.multiply.reduce(yx_max2 - yx_min2, -1), 0)\n    assert np.all(intersect_area >= 0)\n    assert np.all(intersect_area <= area1)\n    assert np.all(intersect_area <= area2)\n    union_area = np.maximum(area1 + area2 - intersect_area, min)\n    return intersect_area / union_area\n\n\nclass TestIouMatrix(unittest.TestCase):\n    def _test(self, bbox1, bbox2, ans, dtype=np.float32):\n        bbox1, bbox2, ans = (np.array(a, dtype) for a in (bbox1, bbox2, ans))\n        yx_min1, yx_max1 = np.split(bbox1, 2, -1)\n        yx_min2, yx_max2 = np.split(bbox2, 2, -1)\n        assert np.all(yx_min1 <= yx_max1)\n        assert np.all(yx_min2 <= yx_max2)\n        assert np.all(ans >= 0)\n        matrix = iou_matrix(yx_min1, yx_max1, yx_min2, yx_max2)\n        np.testing.assert_almost_equal(matrix, ans)\n\n    def test0(self):\n        bbox1 = [\n            (1, 1, 2, 2),\n        ]\n        bbox2 = [\n            (0, 0, 1, 1),\n            (0, 1, 1, 2),\n            (0, 2, 1, 3),\n            (1, 0, 2, 1),\n            (2, 0, 3, 1),\n            (1, 2, 2, 3),\n            (2, 1, 3, 2),\n            (2, 2, 3, 3),\n        ]\n        ans = [\n            [0] * len(bbox2),\n        ]\n        self._test(bbox1, bbox2, ans)\n\n    def test1(self):\n        bbox1 = [\n            (1, 1, 3, 3),\n            (0, 0, 4, 4),\n        ]\n        bbox2 = [\n            (0, 0, 2, 2),\n            (2, 0, 4, 2),\n            (0, 2, 2, 4),\n            (2, 2, 4, 4),\n        ]\n        ans = [\n            [1 / (4 + 4 - 1)] * len(bbox2),\n            [4 / 16] * len(bbox2),\n        ]\n        self._test(bbox1, bbox2, ans)\n'"
utils/iou/torch.py,42,"b'""""""\nCopyright (C) 2017, \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n""""""\n\nimport unittest\n\nimport numpy as np\nimport torch\n\n\ndef intersection_area(yx_min1, yx_max1, yx_min2, yx_max2):\n    """"""\n    Calculates the intersection area of two lists of bounding boxes.\n    :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n    :param yx_min1: The top left coordinates (y, x) of the first list (size [N1, 2]) of bounding boxes.\n    :param yx_max1: The bottom right coordinates (y, x) of the first list (size [N1, 2]) of bounding boxes.\n    :param yx_min2: The top left coordinates (y, x) of the second list (size [N2, 2]) of bounding boxes.\n    :param yx_max2: The bottom right coordinates (y, x) of the second list (size [N2, 2]) of bounding boxes.\n    :return: The matrix (size [N1, N2]) of the intersection area.\n    """"""\n    ymin1, xmin1 = torch.split(yx_min1, 1, -1)\n    ymax1, xmax1 = torch.split(yx_max1, 1, -1)\n    ymin2, xmin2 = torch.split(yx_min2, 1, -1)\n    ymax2, xmax2 = torch.split(yx_max2, 1, -1)\n    max_ymin = torch.max(ymin1.repeat(1, ymin2.size(0)), torch.transpose(ymin2, 0, 1).repeat(ymin1.size(0), 1)) # PyTorch\'s bug\n    min_ymax = torch.min(ymax1.repeat(1, ymax2.size(0)), torch.transpose(ymax2, 0, 1).repeat(ymax1.size(0), 1)) # PyTorch\'s bug\n    height = torch.clamp(min_ymax - max_ymin, min=0)\n    max_xmin = torch.max(xmin1.repeat(1, xmin2.size(0)), torch.transpose(xmin2, 0, 1).repeat(xmin1.size(0), 1)) # PyTorch\'s bug\n    min_xmax = torch.min(xmax1.repeat(1, xmax2.size(0)), torch.transpose(xmax2, 0, 1).repeat(xmax1.size(0), 1)) # PyTorch\'s bug\n    width = torch.clamp(min_xmax - max_xmin, min=0)\n    return height * width\n\n\ndef iou_matrix(yx_min1, yx_max1, yx_min2, yx_max2, min=float(np.finfo(np.float32).eps)):\n    """"""\n    Calculates the IoU of two lists of bounding boxes.\n    :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n    :param yx_min1: The top left coordinates (y, x) of the first list (size [N1, 2]) of bounding boxes.\n    :param yx_max1: The bottom right coordinates (y, x) of the first list (size [N1, 2]) of bounding boxes.\n    :param yx_min2: The top left coordinates (y, x) of the second list (size [N2, 2]) of bounding boxes.\n    :param yx_max2: The bottom right coordinates (y, x) of the second list (size [N2, 2]) of bounding boxes.\n    :return: The matrix (size [N1, N2]) of the IoU.\n    """"""\n    intersect_area = intersection_area(yx_min1, yx_max1, yx_min2, yx_max2)\n    area1 = torch.prod(yx_max1 - yx_min1, -1).unsqueeze(-1)\n    area2 = torch.prod(yx_max2 - yx_min2, -1).unsqueeze(-2)\n    union_area = torch.clamp(area1 + area2 - intersect_area, min=min)\n    return intersect_area / union_area\n\n\nclass TestIouMatrix(unittest.TestCase):\n    def _test(self, bbox1, bbox2, ans, dtype=np.float32):\n        bbox1, bbox2, ans = (np.array(a, dtype) for a in (bbox1, bbox2, ans))\n        yx_min1, yx_max1 = np.split(bbox1, 2, -1)\n        yx_min2, yx_max2 = np.split(bbox2, 2, -1)\n        assert np.all(yx_min1 <= yx_max1)\n        assert np.all(yx_min2 <= yx_max2)\n        assert np.all(ans >= 0)\n        yx_min1, yx_max1 = torch.autograd.Variable(torch.from_numpy(yx_min1)), torch.autograd.Variable(torch.from_numpy(yx_max1))\n        yx_min2, yx_max2 = torch.autograd.Variable(torch.from_numpy(yx_min2)), torch.autograd.Variable(torch.from_numpy(yx_max2))\n        if torch.cuda.is_available():\n            yx_min1, yx_max1, yx_min2, yx_max2 = (v.cuda() for v in (yx_min1, yx_max1, yx_min2, yx_max2))\n        matrix = iou_matrix(yx_min1, yx_max1, yx_min2, yx_max2).data.cpu().numpy()\n        np.testing.assert_almost_equal(matrix, ans)\n\n    def test0(self):\n        bbox1 = [\n            (1, 1, 2, 2),\n        ]\n        bbox2 = [\n            (0, 0, 1, 1),\n            (0, 1, 1, 2),\n            (0, 2, 1, 3),\n            (1, 0, 2, 1),\n            (2, 0, 3, 1),\n            (1, 2, 2, 3),\n            (2, 1, 3, 2),\n            (2, 2, 3, 3),\n        ]\n        ans = [\n            [0] * len(bbox2),\n        ]\n        self._test(bbox1, bbox2, ans)\n\n    def test1(self):\n        bbox1 = [\n            (1, 1, 3, 3),\n            (0, 0, 4, 4),\n        ]\n        bbox2 = [\n            (0, 0, 2, 2),\n            (2, 0, 4, 2),\n            (0, 2, 2, 4),\n            (2, 2, 4, 4),\n        ]\n        ans = [\n            [1 / (4 + 4 - 1)] * len(bbox2),\n            [4 / 16] * len(bbox2),\n        ]\n        self._test(bbox1, bbox2, ans)\n\n\ndef batch_intersection_area(yx_min1, yx_max1, yx_min2, yx_max2):\n    """"""\n    Calculates the intersection area of two lists of bounding boxes for N independent batches.\n    :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n    :param yx_min1: The top left coordinates (y, x) of the first lists (size [N, N1, 2]) of bounding boxes.\n    :param yx_max1: The bottom right coordinates (y, x) of the first lists (size [N, N1, 2]) of bounding boxes.\n    :param yx_min2: The top left coordinates (y, x) of the second lists (size [N, N2, 2]) of bounding boxes.\n    :param yx_max2: The bottom right coordinates (y, x) of the second lists (size [N, N2, 2]) of bounding boxes.\n    :return: The matrics (size [N, N1, N2]) of the intersection area.\n    """"""\n    ymin1, xmin1 = torch.split(yx_min1, 1, -1)\n    ymax1, xmax1 = torch.split(yx_max1, 1, -1)\n    ymin2, xmin2 = torch.split(yx_min2, 1, -1)\n    ymax2, xmax2 = torch.split(yx_max2, 1, -1)\n    max_ymin = torch.max(ymin1.repeat(1, 1, ymin2.size(1)), torch.transpose(ymin2, 1, 2).repeat(1, ymin1.size(1), 1)) # PyTorch\'s bug\n    min_ymax = torch.min(ymax1.repeat(1, 1, ymax2.size(1)), torch.transpose(ymax2, 1, 2).repeat(1, ymax1.size(1), 1)) # PyTorch\'s bug\n    height = torch.clamp(min_ymax - max_ymin, min=0)\n    max_xmin = torch.max(xmin1.repeat(1, 1, xmin2.size(1)), torch.transpose(xmin2, 1, 2).repeat(1, xmin1.size(1), 1)) # PyTorch\'s bug\n    min_xmax = torch.min(xmax1.repeat(1, 1, xmax2.size(1)), torch.transpose(xmax2, 1, 2).repeat(1, xmax1.size(1), 1)) # PyTorch\'s bug\n    width = torch.clamp(min_xmax - max_xmin, min=0)\n    return height * width\n\n\ndef batch_iou_matrix(yx_min1, yx_max1, yx_min2, yx_max2, min=float(np.finfo(np.float32).eps)):\n    """"""\n    Calculates the IoU of two lists of bounding boxes for N independent batches.\n    :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n    :param yx_min1: The top left coordinates (y, x) of the first lists (size [N, N1, 2]) of bounding boxes.\n    :param yx_max1: The bottom right coordinates (y, x) of the first lists (size [N, N1, 2]) of bounding boxes.\n    :param yx_min2: The top left coordinates (y, x) of the second lists (size [N, N2, 2]) of bounding boxes.\n    :param yx_max2: The bottom right coordinates (y, x) of the second lists (size [N, N2, 2]) of bounding boxes.\n    :return: The matrics (size [N, N1, N2]) of the IoU.\n    """"""\n    intersect_area = batch_intersection_area(yx_min1, yx_max1, yx_min2, yx_max2)\n    area1 = torch.prod(yx_max1 - yx_min1, -1).unsqueeze(-1)\n    area2 = torch.prod(yx_max2 - yx_min2, -1).unsqueeze(-2)\n    union_area = torch.clamp(area1 + area2 - intersect_area, min=min)\n    return intersect_area / union_area\n\n\nclass TestBatchIouMatrix(unittest.TestCase):\n    def _test(self, bbox1, bbox2, ans, batch_size=2, dtype=np.float32):\n        bbox1, bbox2, ans = (np.expand_dims(np.array(a, dtype), 0) for a in (bbox1, bbox2, ans))\n        if batch_size > 1:\n            bbox1, bbox2, ans = (np.tile(a, (batch_size, 1, 1)) for a in (bbox1, bbox2, ans))\n            for b in range(batch_size):\n                indices1 = np.random.permutation(bbox1.shape[1])\n                indices2 = np.random.permutation(bbox2.shape[1])\n                bbox1[b] = bbox1[b][indices1]\n                bbox2[b] = bbox2[b][indices2]\n                ans[b] = ans[b][indices1][:, indices2]\n        yx_min1, yx_max1 = np.split(bbox1, 2, -1)\n        yx_min2, yx_max2 = np.split(bbox2, 2, -1)\n        assert np.all(yx_min1 <= yx_max1)\n        assert np.all(yx_min2 <= yx_max2)\n        assert np.all(ans >= 0)\n        yx_min1, yx_max1 = torch.autograd.Variable(torch.from_numpy(yx_min1)), torch.autograd.Variable(torch.from_numpy(yx_max1))\n        yx_min2, yx_max2 = torch.autograd.Variable(torch.from_numpy(yx_min2)), torch.autograd.Variable(torch.from_numpy(yx_max2))\n        if torch.cuda.is_available():\n            yx_min1, yx_max1, yx_min2, yx_max2 = (v.cuda() for v in (yx_min1, yx_max1, yx_min2, yx_max2))\n        matrix = batch_iou_matrix(yx_min1, yx_max1, yx_min2, yx_max2).data.cpu().numpy()\n        np.testing.assert_almost_equal(matrix, ans)\n\n    def test0(self):\n        bbox1 = [\n            (1, 1, 2, 2),\n        ]\n        bbox2 = [\n            (0, 0, 1, 1),\n            (0, 1, 1, 2),\n            (0, 2, 1, 3),\n            (1, 0, 2, 1),\n            (2, 0, 3, 1),\n            (1, 2, 2, 3),\n            (2, 1, 3, 2),\n            (2, 2, 3, 3),\n        ]\n        ans = [\n            [0] * len(bbox2),\n        ]\n        self._test(bbox1, bbox2, ans)\n\n    def test1(self):\n        bbox1 = [\n            (1, 1, 3, 3),\n            (0, 0, 4, 4),\n        ]\n        bbox2 = [\n            (0, 0, 2, 2),\n            (2, 0, 4, 2),\n            (0, 2, 2, 4),\n            (2, 2, 4, 4),\n        ]\n        ans = [\n            [1 / (4 + 4 - 1)] * len(bbox2),\n            [4 / 16] * len(bbox2),\n        ]\n        self._test(bbox1, bbox2, ans)\n\n\ndef batch_iou_pair(yx_min1, yx_max1, yx_min2, yx_max2, min=float(np.finfo(np.float32).eps)):\n    """"""\n    Pairwisely calculates the IoU of two lists (at the same size M) of bounding boxes for N independent batches.\n    :author \xe7\x94\xb3\xe7\x91\x9e\xe7\x8f\x89 (Ruimin Shen)\n    :param yx_min1: The top left coordinates (y, x) of the first lists (size [N, M, 2]) of bounding boxes.\n    :param yx_max1: The bottom right coordinates (y, x) of the first lists (size [N, M, 2]) of bounding boxes.\n    :param yx_min2: The top left coordinates (y, x) of the second lists (size [N, M, 2]) of bounding boxes.\n    :param yx_max2: The bottom right coordinates (y, x) of the second lists (size [N, M, 2]) of bounding boxes.\n    :return: The lists (size [N, M]) of the IoU.\n    """"""\n    yx_min = torch.max(yx_min1, yx_min2)\n    yx_max = torch.min(yx_max1, yx_max2)\n    size = torch.clamp(yx_max - yx_min, min=0)\n    intersect_area = torch.prod(size, -1)\n    area1 = torch.prod(yx_max1 - yx_min1, -1)\n    area2 = torch.prod(yx_max2 - yx_min2, -1)\n    union_area = torch.clamp(area1 + area2 - intersect_area, min=min)\n    return intersect_area / union_area\n\n\nclass TestBatchIouPair(unittest.TestCase):\n    def _test(self, bbox1, bbox2, ans, dtype=np.float32):\n        bbox1, bbox2, ans = (np.array(a, dtype) for a in (bbox1, bbox2, ans))\n        batch_size = bbox1.shape[0]\n        cells = bbox2.shape[0]\n        bbox1 = np.tile(np.reshape(bbox1, [-1, 1, 4]), [1, cells, 1])\n        bbox2 = np.tile(np.reshape(bbox2, [1, -1, 4]), [batch_size, 1, 1])\n        yx_min1, yx_max1 = np.split(bbox1, 2, -1)\n        yx_min2, yx_max2 = np.split(bbox2, 2, -1)\n        assert np.all(yx_min1 <= yx_max1)\n        assert np.all(yx_min2 <= yx_max2)\n        assert np.all(ans >= 0)\n        yx_min1, yx_max1 = torch.autograd.Variable(torch.from_numpy(yx_min1)), torch.autograd.Variable(torch.from_numpy(yx_max1))\n        yx_min2, yx_max2 = torch.autograd.Variable(torch.from_numpy(yx_min2)), torch.autograd.Variable(torch.from_numpy(yx_max2))\n        if torch.cuda.is_available():\n            yx_min1, yx_max1, yx_min2, yx_max2 = (v.cuda() for v in (yx_min1, yx_max1, yx_min2, yx_max2))\n        iou = batch_iou_pair(yx_min1, yx_max1, yx_min2, yx_max2).data.cpu().numpy()\n        np.testing.assert_almost_equal(iou, ans)\n\n    def test0(self):\n        bbox1 = [\n            (1, 1, 2, 2),\n        ]\n        bbox2 = [\n            (0, 0, 1, 1),\n            (0, 1, 1, 2),\n            (0, 2, 1, 3),\n            (1, 0, 2, 1),\n            (2, 0, 3, 1),\n            (1, 2, 2, 3),\n            (2, 1, 3, 2),\n            (2, 2, 3, 3),\n        ]\n        ans = [\n            [0] * len(bbox2),\n        ]\n        self._test(bbox1, bbox2, ans)\n\n    def test1(self):\n        bbox1 = [\n            (1, 1, 3, 3),\n            (0, 0, 4, 4),\n        ]\n        bbox2 = [\n            (0, 0, 2, 2),\n            (2, 0, 4, 2),\n            (0, 2, 2, 4),\n            (2, 2, 4, 4),\n        ]\n        ans = [\n            [1 / (4 + 4 - 1)] * len(bbox2),\n            [4 / 16] * len(bbox2),\n        ]\n        self._test(bbox1, bbox2, ans)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
