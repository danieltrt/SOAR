file_path,api_count,code
test_RFB.py,8,"b""from __future__ import print_function\nimport sys\nimport os\nimport pickle\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as transforms\nimport numpy as np\nfrom torch.autograd import Variable\nfrom data import VOCroot,COCOroot \nfrom data import AnnotationTransform, COCODetection, VOCDetection, BaseTransform, VOC_300,VOC_512,COCO_300,COCO_512, COCO_mobile_300\n\nimport torch.utils.data as data\nfrom layers.functions import Detect,PriorBox\nfrom utils.nms_wrapper import nms\nfrom utils.timer import Timer\n\nparser = argparse.ArgumentParser(description='Receptive Field Block Net')\n\nparser.add_argument('-v', '--version', default='RFB_vgg',\n                    help='RFB_vgg ,RFB_E_vgg or RFB_mobile version.')\nparser.add_argument('-s', '--size', default='300',\n                    help='300 or 512 input size.')\nparser.add_argument('-d', '--dataset', default='VOC',\n                    help='VOC or COCO version')\nparser.add_argument('-m', '--trained_model', default='weights/RFB300_80_5.pth',\n                    type=str, help='Trained state_dict file path to open')\nparser.add_argument('--save_folder', default='eval/', type=str,\n                    help='Dir to save results')\nparser.add_argument('--cuda', default=True, type=bool,\n                    help='Use cuda to train model')\nparser.add_argument('--cpu', default=False, type=bool,\n                    help='Use cpu nms')\nparser.add_argument('--retest', default=False, type=bool,\n                    help='test cache results')\nargs = parser.parse_args()\n\nif not os.path.exists(args.save_folder):\n    os.mkdir(args.save_folder)\n\nif args.dataset == 'VOC':\n    cfg = (VOC_300, VOC_512)[args.size == '512']\nelse:\n    cfg = (COCO_300, COCO_512)[args.size == '512']\n\nif args.version == 'RFB_vgg':\n    from models.RFB_Net_vgg import build_net\nelif args.version == 'RFB_E_vgg':\n    from models.RFB_Net_E_vgg import build_net\nelif args.version == 'RFB_mobile':\n    from models.RFB_Net_mobile import build_net\n    cfg = COCO_mobile_300\nelse:\n    print('Unkown version!')\n\npriorbox = PriorBox(cfg)\nwith torch.no_grad():\n    priors = priorbox.forward()\n    if args.cuda:\n        priors = priors.cuda()\n\n\ndef test_net(save_folder, net, detector, cuda, testset, transform, max_per_image=300, thresh=0.005):\n\n    if not os.path.exists(save_folder):\n        os.mkdir(save_folder)\n    # dump predictions and assoc. ground truth to text file for now\n    num_images = len(testset)\n    num_classes = (21, 81)[args.dataset == 'COCO']\n    all_boxes = [[[] for _ in range(num_images)]\n                 for _ in range(num_classes)]\n\n    _t = {'im_detect': Timer(), 'misc': Timer()}\n    det_file = os.path.join(save_folder, 'detections.pkl')\n\n    if args.retest:\n        f = open(det_file,'rb')\n        all_boxes = pickle.load(f)\n        print('Evaluating detections')\n        testset.evaluate_detections(all_boxes, save_folder)\n        return\n\n\n    for i in range(num_images):\n        img = testset.pull_image(i)\n        scale = torch.Tensor([img.shape[1], img.shape[0],\n                             img.shape[1], img.shape[0]])\n        with torch.no_grad():\n            x = transform(img).unsqueeze(0)\n            if cuda:\n                x = x.cuda()\n                scale = scale.cuda()\n\n        _t['im_detect'].tic()\n        out = net(x)      # forward pass\n        boxes, scores = detector.forward(out,priors)\n        detect_time = _t['im_detect'].toc()\n        boxes = boxes[0]\n        scores=scores[0]\n\n        boxes *= scale\n        boxes = boxes.cpu().numpy()\n        scores = scores.cpu().numpy()\n        # scale each detection back up to the image\n\n        _t['misc'].tic()\n\n        for j in range(1, num_classes):\n            inds = np.where(scores[:, j] > thresh)[0]\n            if len(inds) == 0:\n                all_boxes[j][i] = np.empty([0, 5], dtype=np.float32)\n                continue\n            c_bboxes = boxes[inds]\n            c_scores = scores[inds, j]\n            c_dets = np.hstack((c_bboxes, c_scores[:, np.newaxis])).astype(\n                np.float32, copy=False)\n\n            keep = nms(c_dets, 0.45, force_cpu=args.cpu)\n            c_dets = c_dets[keep, :]\n            all_boxes[j][i] = c_dets\n        if max_per_image > 0:\n            image_scores = np.hstack([all_boxes[j][i][:, -1] for j in range(1,num_classes)])\n            if len(image_scores) > max_per_image:\n                image_thresh = np.sort(image_scores)[-max_per_image]\n                for j in range(1, num_classes):\n                    keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n                    all_boxes[j][i] = all_boxes[j][i][keep, :]\n\n        nms_time = _t['misc'].toc()\n\n        if i % 20 == 0:\n            print('im_detect: {:d}/{:d} {:.3f}s {:.3f}s'\n                .format(i + 1, num_images, detect_time, nms_time))\n            _t['im_detect'].clear()\n            _t['misc'].clear()\n\n    with open(det_file, 'wb') as f:\n        pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n\n    print('Evaluating detections')\n    testset.evaluate_detections(all_boxes, save_folder)\n\n\nif __name__ == '__main__':\n    # load net\n    img_dim = (300,512)[args.size=='512']\n    num_classes = (21, 81)[args.dataset == 'COCO']\n    net = build_net('test', img_dim, num_classes)    # initialize detector\n    state_dict = torch.load(args.trained_model)\n    # create new OrderedDict that does not contain `module.`\n\n    from collections import OrderedDict\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        head = k[:7]\n        if head == 'module.':\n            name = k[7:] # remove `module.`\n        else:\n            name = k\n        new_state_dict[name] = v\n    net.load_state_dict(new_state_dict)\n    net.eval()\n    print('Finished loading model!')\n    print(net)\n    # load data\n    if args.dataset == 'VOC':\n        testset = VOCDetection(\n            VOCroot, [('2007', 'test')], None, AnnotationTransform())\n    elif args.dataset == 'COCO':\n        testset = COCODetection(\n            COCOroot, [('2014', 'minival')], None)\n            #COCOroot, [('2015', 'test-dev')], None)\n    else:\n        print('Only VOC and COCO dataset are supported now!')\n    if args.cuda:\n        net = net.cuda()\n        cudnn.benchmark = True\n    else:\n        net = net.cpu()\n    # evaluation\n    #top_k = (300, 200)[args.dataset == 'COCO']\n    top_k = 200\n    detector = Detect(num_classes,0,cfg)\n    save_folder = os.path.join(args.save_folder,args.dataset)\n    rgb_means = ((104, 117, 123),(103.94,116.78,123.68))[args.version == 'RFB_mobile']\n    test_net(save_folder, net, detector, args.cuda, testset,\n             BaseTransform(net.size, rgb_means, (2, 0, 1)),\n             top_k, thresh=0.01)\n"""
train_RFB.py,13,"b'from __future__ import print_function\nimport sys\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as transforms\nimport torch.nn.init as init\nimport argparse\nimport numpy as np\nfrom torch.autograd import Variable\nimport torch.utils.data as data\nfrom data import VOCroot, COCOroot, VOC_300, VOC_512, COCO_300, COCO_512, COCO_mobile_300, AnnotationTransform, COCODetection, VOCDetection, detection_collate, BaseTransform, preproc\nfrom layers.modules import MultiBoxLoss\nfrom layers.functions import PriorBox\nimport time\n\n\nparser = argparse.ArgumentParser(\n    description=\'Receptive Field Block Net Training\')\nparser.add_argument(\'-v\', \'--version\', default=\'RFB_vgg\',\n                    help=\'RFB_vgg ,RFB_E_vgg or RFB_mobile version.\')\nparser.add_argument(\'-s\', \'--size\', default=\'300\',\n                    help=\'300 or 512 input size.\')\nparser.add_argument(\'-d\', \'--dataset\', default=\'VOC\',\n                    help=\'VOC or COCO dataset\')\nparser.add_argument(\n    \'--basenet\', default=\'./weights/vgg16_reducedfc.pth\', help=\'pretrained base model\')\nparser.add_argument(\'--jaccard_threshold\', default=0.5,\n                    type=float, help=\'Min Jaccard index for matching\')\nparser.add_argument(\'-b\', \'--batch_size\', default=32,\n                    type=int, help=\'Batch size for training\')\nparser.add_argument(\'--num_workers\', default=8,\n                    type=int, help=\'Number of workers used in dataloading\')\nparser.add_argument(\'--cuda\', default=True,\n                    type=bool, help=\'Use cuda to train model\')\nparser.add_argument(\'--ngpu\', default=1, type=int, help=\'gpus\')\nparser.add_argument(\'--lr\', \'--learning-rate\',\n                    default=4e-3, type=float, help=\'initial learning rate\')\nparser.add_argument(\'--momentum\', default=0.9, type=float, help=\'momentum\')\nparser.add_argument(\n    \'--resume_net\', default=None, help=\'resume net for retraining\')\nparser.add_argument(\'--resume_epoch\', default=0,\n                    type=int, help=\'resume iter for retraining\')\nparser.add_argument(\'-max\',\'--max_epoch\', default=300,\n                    type=int, help=\'max epoch for retraining\')\nparser.add_argument(\'--weight_decay\', default=5e-4,\n                    type=float, help=\'Weight decay for SGD\')\nparser.add_argument(\'--gamma\', default=0.1,\n                    type=float, help=\'Gamma update for SGD\')\nparser.add_argument(\'--log_iters\', default=True,\n                    type=bool, help=\'Print the loss at each iteration\')\nparser.add_argument(\'--save_folder\', default=\'./weights/\',\n                    help=\'Location to save checkpoint models\')\nargs = parser.parse_args()\n\n\nif not os.path.exists(args.save_folder):\n    os.mkdir(args.save_folder)\n\nif args.dataset == \'VOC\':\n    train_sets = [(\'2007\', \'trainval\'), (\'2012\', \'trainval\')]\n    cfg = (VOC_300, VOC_512)[args.size == \'512\']\nelse:\n    train_sets = [(\'2014\', \'train\'),(\'2014\', \'valminusminival\')]\n    cfg = (COCO_300, COCO_512)[args.size == \'512\']\n\nif args.version == \'RFB_vgg\':\n    from models.RFB_Net_vgg import build_net\nelif args.version == \'RFB_E_vgg\':\n    from models.RFB_Net_E_vgg import build_net\nelif args.version == \'RFB_mobile\':\n    from models.RFB_Net_mobile import build_net\n    cfg = COCO_mobile_300\nelse:\n    print(\'Unkown version!\')\n\nimg_dim = (300,512)[args.size==\'512\']\nrgb_means = ((104, 117, 123),(103.94,116.78,123.68))[args.version == \'RFB_mobile\']\np = (0.6,0.2)[args.version == \'RFB_mobile\']\nnum_classes = (21, 81)[args.dataset == \'COCO\']\nbatch_size = args.batch_size\nweight_decay = 0.0005\ngamma = 0.1\nmomentum = 0.9\n\nnet = build_net(\'train\', img_dim, num_classes)\nprint(net)\nif args.resume_net == None:\n    base_weights = torch.load(args.basenet)\n    print(\'Loading base network...\')\n    net.base.load_state_dict(base_weights)\n\n    def xavier(param):\n        init.xavier_uniform(param)\n\n    def weights_init(m):\n        for key in m.state_dict():\n            if key.split(\'.\')[-1] == \'weight\':\n                if \'conv\' in key:\n                    init.kaiming_normal_(m.state_dict()[key], mode=\'fan_out\')\n                if \'bn\' in key:\n                    m.state_dict()[key][...] = 1\n            elif key.split(\'.\')[-1] == \'bias\':\n                m.state_dict()[key][...] = 0\n\n    print(\'Initializing weights...\')\n# initialize newly added layers\' weights with kaiming_normal method\n    net.extras.apply(weights_init)\n    net.loc.apply(weights_init)\n    net.conf.apply(weights_init)\n    net.Norm.apply(weights_init)\n    if args.version == \'RFB_E_vgg\':\n        net.reduce.apply(weights_init)\n        net.up_reduce.apply(weights_init)\n\nelse:\n# load resume network\n    print(\'Loading resume network...\')\n    state_dict = torch.load(args.resume_net)\n    # create new OrderedDict that does not contain `module.`\n    from collections import OrderedDict\n    new_state_dict = OrderedDict()\n    for k, v in state_dict.items():\n        head = k[:7]\n        if head == \'module.\':\n            name = k[7:] # remove `module.`\n        else:\n            name = k\n        new_state_dict[name] = v\n    net.load_state_dict(new_state_dict)\n\nif args.ngpu > 1:\n    net = torch.nn.DataParallel(net, device_ids=list(range(args.ngpu)))\n\nif args.cuda:\n    net.cuda()\n    cudnn.benchmark = True\n\n\noptimizer = optim.SGD(net.parameters(), lr=args.lr,\n                      momentum=args.momentum, weight_decay=args.weight_decay)\n#optimizer = optim.RMSprop(net.parameters(), lr=args.lr,alpha = 0.9, eps=1e-08,\n#                      momentum=args.momentum, weight_decay=args.weight_decay)\n\ncriterion = MultiBoxLoss(num_classes, 0.5, True, 0, True, 3, 0.5, False)\npriorbox = PriorBox(cfg)\nwith torch.no_grad():\n    priors = priorbox.forward()\n    if args.cuda:\n        priors = priors.cuda()\n\n\n\ndef train():\n    net.train()\n    # loss counters\n    loc_loss = 0  # epoch\n    conf_loss = 0\n    epoch = 0 + args.resume_epoch\n    print(\'Loading Dataset...\')\n\n    if args.dataset == \'VOC\':\n        dataset = VOCDetection(VOCroot, train_sets, preproc(\n            img_dim, rgb_means, p), AnnotationTransform())\n    elif args.dataset == \'COCO\':\n        dataset = COCODetection(COCOroot, train_sets, preproc(\n            img_dim, rgb_means, p))\n    else:\n        print(\'Only VOC and COCO are supported now!\')\n        return\n\n    epoch_size = len(dataset) // args.batch_size\n    max_iter = args.max_epoch * epoch_size\n\n    stepvalues_VOC = (150 * epoch_size, 200 * epoch_size, 250 * epoch_size)\n    stepvalues_COCO = (90 * epoch_size, 120 * epoch_size, 140 * epoch_size)\n    stepvalues = (stepvalues_VOC,stepvalues_COCO)[args.dataset==\'COCO\']\n    print(\'Training\',args.version, \'on\', dataset.name)\n    step_index = 0\n\n    if args.resume_epoch > 0:\n        start_iter = args.resume_epoch * epoch_size\n    else:\n        start_iter = 0\n\n    lr = args.lr\n    for iteration in range(start_iter, max_iter):\n        if iteration % epoch_size == 0:\n            # create batch iterator\n            batch_iterator = iter(data.DataLoader(dataset, batch_size,\n                                                  shuffle=True, num_workers=args.num_workers, collate_fn=detection_collate))\n            loc_loss = 0\n            conf_loss = 0\n            if (epoch % 10 == 0 and epoch > 0) or (epoch % 5 ==0 and epoch > 200):\n                torch.save(net.state_dict(), args.save_folder+args.version+\'_\'+args.dataset + \'_epoches_\'+\n                           repr(epoch) + \'.pth\')\n            epoch += 1\n\n        load_t0 = time.time()\n        if iteration in stepvalues:\n            step_index += 1\n        lr = adjust_learning_rate(optimizer, args.gamma, epoch, step_index, iteration, epoch_size)\n\n\n        # load train data\n        images, targets = next(batch_iterator)\n        \n        #print(np.sum([torch.sum(anno[:,-1] == 2) for anno in targets]))\n\n        if args.cuda:\n            images = Variable(images.cuda())\n            targets = [Variable(anno.cuda()) for anno in targets]\n        else:\n            images = Variable(images)\n            targets = [Variable(anno) for anno in targets]\n        # forward\n        t0 = time.time()\n        out = net(images)\n        # backprop\n        optimizer.zero_grad()\n        loss_l, loss_c = criterion(out, priors, targets)\n        loss = loss_l + loss_c\n        loss.backward()\n        optimizer.step()\n        t1 = time.time()\n        loc_loss += loss_l.item()\n        conf_loss += loss_c.item()\n        load_t1 = time.time()\n        if iteration % 10 == 0:\n            print(\'Epoch:\' + repr(epoch) + \' || epochiter: \' + repr(iteration % epoch_size) + \'/\' + repr(epoch_size)\n                  + \'|| Totel iter \' +\n                  repr(iteration) + \' || L: %.4f C: %.4f||\' % (\n                loss_l.item(),loss_c.item()) + \n                \'Batch time: %.4f sec. ||\' % (load_t1 - load_t0) + \'LR: %.8f\' % (lr))\n\n    torch.save(net.state_dict(), args.save_folder +\n               \'Final_\' + args.version +\'_\' + args.dataset+ \'.pth\')\n\n\ndef adjust_learning_rate(optimizer, gamma, epoch, step_index, iteration, epoch_size):\n    """"""Sets the learning rate \n    # Adapted from PyTorch Imagenet example:\n    # https://github.com/pytorch/examples/blob/master/imagenet/main.py\n    """"""\n    if epoch < 6:\n        lr = 1e-6 + (args.lr-1e-6) * iteration / (epoch_size * 5) \n    else:\n        lr = args.lr * (gamma ** (step_index))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return lr\n\n\nif __name__ == \'__main__\':\n    train()\n'"
data/__init__.py,0,"b'# from .voc import VOCDetection, AnnotationTransform, detection_collate, VOC_CLASSES\nfrom .voc0712 import VOCDetection, AnnotationTransform, detection_collate, VOC_CLASSES\nfrom .coco import COCODetection\nfrom .data_augment import *\nfrom .config import *\n'"
data/coco.py,2,"b'""""""VOC Dataset Classes\n\nOriginal author: Francisco Massa\nhttps://github.com/fmassa/vision/blob/voc_dataset/torchvision/datasets/voc.py\n\nUpdated by: Ellis Brown, Max deGroot\n""""""\n\nimport os\nimport pickle\nimport os.path\nimport sys\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport cv2\nimport numpy as np\nimport json\nimport uuid\n\nfrom utils.pycocotools.coco import COCO\nfrom utils.pycocotools.cocoeval import COCOeval\nfrom utils.pycocotools import mask as COCOmask\n\n\nclass COCODetection(data.Dataset):\n\n    """"""VOC Detection Dataset Object\n\n    input is image, target is annotation\n\n    Arguments:\n        root (string): filepath to VOCdevkit folder.\n        image_set (string): imageset to use (eg. \'train\', \'val\', \'test\')\n        transform (callable, optional): transformation to perform on the\n            input image\n        target_transform (callable, optional): transformation to perform on the\n            target `annotation`\n            (eg: take in caption string, return tensor of word indices)\n        dataset_name (string, optional): which dataset to load\n            (default: \'VOC2007\')\n    """"""\n\n    def __init__(self, root, image_sets, preproc=None, target_transform=None,\n                 dataset_name=\'COCO\'):\n        self.root = root\n        self.cache_path = os.path.join(self.root, \'cache\')\n        self.image_set = image_sets\n        self.preproc = preproc\n        self.target_transform = target_transform\n        self.name = dataset_name\n        self.ids = list()\n        self.annotations = list()\n        self._view_map = {\n            \'minival2014\' : \'val2014\',          # 5k val2014 subset\n            \'valminusminival2014\' : \'val2014\',  # val2014 \\setminus minival2014\n            \'test-dev2015\' : \'test2015\',\n        }\n\n        for (year, image_set) in image_sets:\n            coco_name = image_set+year\n            data_name = (self._view_map[coco_name]\n                        if coco_name in self._view_map\n                        else coco_name)\n            annofile = self._get_ann_file(coco_name)\n            _COCO = COCO(annofile)\n            self._COCO = _COCO\n            self.coco_name = coco_name\n            cats = _COCO.loadCats(_COCO.getCatIds())\n            self._classes = tuple([\'__background__\'] + [c[\'name\'] for c in cats])\n            self.num_classes = len(self._classes)\n            self._class_to_ind = dict(zip(self._classes, range(self.num_classes)))\n            self._class_to_coco_cat_id = dict(zip([c[\'name\'] for c in cats],\n                                                  _COCO.getCatIds()))\n            indexes = _COCO.getImgIds()\n            self.image_indexes = indexes\n            self.ids.extend([self.image_path_from_index(data_name, index) for index in indexes ])\n            if image_set.find(\'test\') != -1:\n                print(\'test set will not load annotations!\')\n            else:\n                self.annotations.extend(self._load_coco_annotations(coco_name, indexes,_COCO))\n\n\n\n    def image_path_from_index(self, name, index):\n        """"""\n        Construct an image path from the image\'s ""index"" identifier.\n        """"""\n        # Example image path for index=119993:\n        #   images/train2014/COCO_train2014_000000119993.jpg\n        file_name = (\'COCO_\' + name + \'_\' +\n                     str(index).zfill(12) + \'.jpg\')\n        image_path = os.path.join(self.root, \'images\',\n                              name, file_name)\n        assert os.path.exists(image_path), \\\n                \'Path does not exist: {}\'.format(image_path)\n        return image_path\n\n\n    def _get_ann_file(self, name):\n        prefix = \'instances\' if name.find(\'test\') == -1 \\\n                else \'image_info\'\n        return os.path.join(self.root, \'annotations\',\n                        prefix + \'_\' + name + \'.json\')\n\n\n    def _load_coco_annotations(self, coco_name, indexes, _COCO):\n        cache_file=os.path.join(self.cache_path,coco_name+\'_gt_roidb.pkl\')\n        if os.path.exists(cache_file):\n            with open(cache_file, \'rb\') as fid:\n                roidb = pickle.load(fid)\n            print(\'{} gt roidb loaded from {}\'.format(coco_name,cache_file))\n            return roidb\n\n        gt_roidb = [self._annotation_from_index(index, _COCO)\n                    for index in indexes]\n        with open(cache_file, \'wb\') as fid:\n            pickle.dump(gt_roidb,fid,pickle.HIGHEST_PROTOCOL)\n        print(\'wrote gt roidb to {}\'.format(cache_file))\n        return gt_roidb\n\n\n    def _annotation_from_index(self, index, _COCO):\n        """"""\n        Loads COCO bounding-box instance annotations. Crowd instances are\n        handled by marking their overlaps (with all categories) to -1. This\n        overlap value means that crowd ""instances"" are excluded from training.\n        """"""\n        im_ann = _COCO.loadImgs(index)[0]\n        width = im_ann[\'width\']\n        height = im_ann[\'height\']\n\n        annIds = _COCO.getAnnIds(imgIds=index, iscrowd=None)\n        objs = _COCO.loadAnns(annIds)\n        # Sanitize bboxes -- some are invalid\n        valid_objs = []\n        for obj in objs:\n            x1 = np.max((0, obj[\'bbox\'][0]))\n            y1 = np.max((0, obj[\'bbox\'][1]))\n            x2 = np.min((width - 1, x1 + np.max((0, obj[\'bbox\'][2] - 1))))\n            y2 = np.min((height - 1, y1 + np.max((0, obj[\'bbox\'][3] - 1))))\n            if obj[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n                obj[\'clean_bbox\'] = [x1, y1, x2, y2]\n                valid_objs.append(obj)\n        objs = valid_objs\n        num_objs = len(objs)\n\n        res = np.zeros((num_objs, 5))\n\n        # Lookup table to map from COCO category ids to our internal class\n        # indices\n        coco_cat_id_to_class_ind = dict([(self._class_to_coco_cat_id[cls],\n                                          self._class_to_ind[cls])\n                                         for cls in self._classes[1:]])\n\n        for ix, obj in enumerate(objs):\n            cls = coco_cat_id_to_class_ind[obj[\'category_id\']]\n            res[ix, 0:4] = obj[\'clean_bbox\']\n            res[ix, 4] = cls\n\n        return res\n\n\n\n    def __getitem__(self, index):\n        img_id = self.ids[index]\n        target = self.annotations[index]\n        img = cv2.imread(img_id, cv2.IMREAD_COLOR)\n        height, width, _ = img.shape\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n\n        if self.preproc is not None:\n            img, target = self.preproc(img, target)\n\n                    # target = self.target_transform(target, width, height)\n        #print(target.shape)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ids)\n\n    def pull_image(self, index):\n        \'\'\'Returns the original image object at index in PIL form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            PIL img\n        \'\'\'\n        img_id = self.ids[index]\n        return cv2.imread(img_id, cv2.IMREAD_COLOR)\n\n\n    def pull_tensor(self, index):\n        \'\'\'Returns the original image at an index in tensor form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            tensorized version of img, squeezed\n        \'\'\'\n        to_tensor = transforms.ToTensor()\n        return torch.Tensor(self.pull_image(index)).unsqueeze_(0)\n\n    def _print_detection_eval_metrics(self, coco_eval):\n        IoU_lo_thresh = 0.5\n        IoU_hi_thresh = 0.95\n        def _get_thr_ind(coco_eval, thr):\n            ind = np.where((coco_eval.params.iouThrs > thr - 1e-5) &\n                           (coco_eval.params.iouThrs < thr + 1e-5))[0][0]\n            iou_thr = coco_eval.params.iouThrs[ind]\n            assert np.isclose(iou_thr, thr)\n            return ind\n\n        ind_lo = _get_thr_ind(coco_eval, IoU_lo_thresh)\n        ind_hi = _get_thr_ind(coco_eval, IoU_hi_thresh)\n        # precision has dims (iou, recall, cls, area range, max dets)\n        # area range index 0: all area ranges\n        # max dets index 2: 100 per image\n        precision = \\\n            coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, :, 0, 2]\n        ap_default = np.mean(precision[precision > -1])\n        print(\'~~~~ Mean and per-category AP @ IoU=[{:.2f},{:.2f}] \'\n               \'~~~~\'.format(IoU_lo_thresh, IoU_hi_thresh))\n        print(\'{:.1f}\'.format(100 * ap_default))\n        for cls_ind, cls in enumerate(self._classes):\n            if cls == \'__background__\':\n                continue\n            # minus 1 because of __background__\n            precision = coco_eval.eval[\'precision\'][ind_lo:(ind_hi + 1), :, cls_ind - 1, 0, 2]\n            ap = np.mean(precision[precision > -1])\n            print(\'{:.1f}\'.format(100 * ap))\n\n        print(\'~~~~ Summary metrics ~~~~\')\n        coco_eval.summarize()\n\n    def _do_detection_eval(self, res_file, output_dir):\n        ann_type = \'bbox\'\n        coco_dt = self._COCO.loadRes(res_file)\n        coco_eval = COCOeval(self._COCO, coco_dt)\n        coco_eval.params.useSegm = (ann_type == \'segm\')\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        self._print_detection_eval_metrics(coco_eval)\n        eval_file = os.path.join(output_dir, \'detection_results.pkl\')\n        with open(eval_file, \'wb\') as fid:\n            pickle.dump(coco_eval, fid, pickle.HIGHEST_PROTOCOL)\n        print(\'Wrote COCO eval results to: {}\'.format(eval_file))\n\n    def _coco_results_one_category(self, boxes, cat_id):\n        results = []\n        for im_ind, index in enumerate(self.image_indexes):\n            dets = boxes[im_ind].astype(np.float)\n            if dets == []:\n                continue\n            scores = dets[:, -1]\n            xs = dets[:, 0]\n            ys = dets[:, 1]\n            ws = dets[:, 2] - xs + 1\n            hs = dets[:, 3] - ys + 1\n            results.extend(\n              [{\'image_id\' : index,\n                \'category_id\' : cat_id,\n                \'bbox\' : [xs[k], ys[k], ws[k], hs[k]],\n                \'score\' : scores[k]} for k in range(dets.shape[0])])\n        return results\n\n    def _write_coco_results_file(self, all_boxes, res_file):\n        # [{""image_id"": 42,\n        #   ""category_id"": 18,\n        #   ""bbox"": [258.15,41.29,348.26,243.78],\n        #   ""score"": 0.236}, ...]\n        results = []\n        for cls_ind, cls in enumerate(self._classes):\n            if cls == \'__background__\':\n                continue\n            print(\'Collecting {} results ({:d}/{:d})\'.format(cls, cls_ind,\n                                                          self.num_classes ))\n            coco_cat_id = self._class_to_coco_cat_id[cls]\n            results.extend(self._coco_results_one_category(all_boxes[cls_ind],\n                                                           coco_cat_id))\n            \'\'\'\n            if cls_ind ==30:\n                res_f = res_file+ \'_1.json\'\n                print(\'Writing results json to {}\'.format(res_f))\n                with open(res_f, \'w\') as fid:\n                    json.dump(results, fid)\n                results = []\n            \'\'\'\n        #res_f2 = res_file+\'_2.json\'\n        print(\'Writing results json to {}\'.format(res_file))\n        with open(res_file, \'w\') as fid:\n            json.dump(results, fid)\n\n    def evaluate_detections(self, all_boxes, output_dir):\n        res_file = os.path.join(output_dir, (\'detections_\' +\n                                         self.coco_name +\n                                         \'_results\'))\n        res_file += \'.json\'\n        self._write_coco_results_file(all_boxes, res_file)\n        # Only do evaluation on non-test sets\n        if self.coco_name.find(\'test\') == -1:\n            self._do_detection_eval(res_file, output_dir)\n        # Optionally cleanup results json file\n\n'"
data/config.py,0,"b'# config.py\nimport os.path\n\n# gets home dir cross platform\nhome = os.path.expanduser(""~"")\nddir = os.path.join(home,""data/VOCdevkit/"")\n\n# note: if you used our download scripts, this should be right\nVOCroot = ddir # path to VOCdevkit root dir\nCOCOroot = os.path.join(home,""data/COCO/"")\n\n\n#RFB CONFIGS\nVOC_300 = {\n    \'feature_maps\' : [38, 19, 10, 5, 3, 1],\n\n    \'min_dim\' : 300,\n\n    \'steps\' : [8, 16, 32, 64, 100, 300],\n\n    \'min_sizes\' : [30, 60, 111, 162, 213, 264],\n\n    \'max_sizes\' : [60, 111, 162, 213, 264, 315],\n\n    \'aspect_ratios\' : [[2,3], [2, 3], [2, 3], [2, 3], [2], [2]],\n\n    \'variance\' : [0.1, 0.2],\n\n    \'clip\' : True,\n}\n\nVOC_512= {\n    \'feature_maps\' : [64, 32, 16, 8, 4, 2, 1],\n\n    \'min_dim\' : 512,\n\n    \'steps\' : [8, 16, 32, 64, 128, 256, 512],\n\n    \'min_sizes\'  : [35.84, 76.8, 153.6, 230.4, 307.2, 384.0, 460.8 ],\n\n    \'max_sizes\'  : [76.8, 153.6, 230.4, 307.2, 384.0, 460.8, 537.6],\n\n    \'aspect_ratios\' : [[2,3], [2, 3], [2, 3], [2, 3], [2,3], [2], [2]],\n\n    \'variance\' : [0.1, 0.2],\n\n    \'clip\' : True,\n}\n\n\nCOCO_300 = {\n    \'feature_maps\' : [38, 19, 10, 5, 3, 1],\n\n    \'min_dim\' : 300,\n\n    \'steps\' : [8, 16, 32, 64, 100, 300],\n\n    \'min_sizes\' : [21, 45, 99, 153, 207, 261],\n\n    \'max_sizes\' : [45, 99, 153, 207, 261, 315],\n\n    \'aspect_ratios\' : [[2,3], [2, 3], [2, 3], [2, 3], [2], [2]],\n\n    \'variance\' : [0.1, 0.2],\n\n    \'clip\' : True,\n}\n\nCOCO_512= {\n    \'feature_maps\' : [64, 32, 16, 8, 4, 2, 1],\n\n    \'min_dim\' : 512,\n\n    \'steps\' : [8, 16, 32, 64, 128, 256, 512],\n\n    \'min_sizes\' : [20.48, 51.2, 133.12, 215.04, 296.96, 378.88, 460.8],\n\n    \'max_sizes\' : [51.2, 133.12, 215.04, 296.96, 378.88, 460.8, 542.72],\n\n    \'aspect_ratios\' : [[2,3], [2, 3], [2, 3], [2, 3], [2,3], [2], [2]],\n\n    \'variance\' : [0.1, 0.2],\n\n    \'clip\' : True,\n}\n\nCOCO_mobile_300 = {\n    \'feature_maps\' : [19, 10, 5, 3, 2, 1],\n\n    \'min_dim\' : 300,\n\n    \'steps\' : [16, 32, 64, 100, 150, 300],\n\n    \'min_sizes\' : [45, 90, 135, 180, 225, 270],\n\n    \'max_sizes\' : [90, 135, 180, 225, 270, 315],\n\n    \'aspect_ratios\' : [[2,3], [2, 3], [2, 3], [2, 3], [2], [2]],\n\n    \'variance\' : [0.1, 0.2],\n\n    \'clip\' : True,\n}\n'"
data/data_augment.py,4,"b'""""""Data augmentation functionality. Passed as callable transformations to\nDataset classes.\n\nThe data augmentation procedures were interpreted from @weiliu89\'s SSD paper\nhttp://arxiv.org/abs/1512.02325\n""""""\n\nimport torch\nfrom torchvision import transforms\nimport cv2\nimport numpy as np\nimport random\nimport math\nfrom utils.box_utils import matrix_iou\n# import torch_transforms\n\ndef _crop(image, boxes, labels):\n    height, width, _ = image.shape\n\n    if len(boxes)== 0:\n        return image, boxes, labels\n\n    while True:\n        mode = random.choice((\n            None,\n            (0.1, None),\n            (0.3, None),\n            (0.5, None),\n            (0.7, None),\n            (0.9, None),\n            (None, None),\n        ))\n\n        if mode is None:\n            return image, boxes, labels\n\n        min_iou, max_iou = mode\n        if min_iou is None:\n            min_iou = float(\'-inf\')\n        if max_iou is None:\n            max_iou = float(\'inf\')\n\n        for _ in range(50):\n            scale = random.uniform(0.3,1.)\n            min_ratio = max(0.5, scale*scale)\n            max_ratio = min(2, 1. / scale / scale)\n            ratio = math.sqrt(random.uniform(min_ratio, max_ratio))\n            w = int(scale * ratio * width)\n            h = int((scale / ratio) * height)\n\n\n            l = random.randrange(width - w)\n            t = random.randrange(height - h)\n            roi = np.array((l, t, l + w, t + h))\n\n            iou = matrix_iou(boxes, roi[np.newaxis])\n            \n            if not (min_iou <= iou.min() and iou.max() <= max_iou):\n                continue\n\n            image_t = image[roi[1]:roi[3], roi[0]:roi[2]]\n\n            centers = (boxes[:, :2] + boxes[:, 2:]) / 2\n            mask = np.logical_and(roi[:2] < centers, centers < roi[2:]) \\\n                     .all(axis=1)\n            boxes_t = boxes[mask].copy()\n            labels_t = labels[mask].copy()\n            if len(boxes_t) == 0:\n                continue\n\n            boxes_t[:, :2] = np.maximum(boxes_t[:, :2], roi[:2])\n            boxes_t[:, :2] -= roi[:2]\n            boxes_t[:, 2:] = np.minimum(boxes_t[:, 2:], roi[2:])\n            boxes_t[:, 2:] -= roi[:2]\n\n            return image_t, boxes_t,labels_t\n\n\ndef _distort(image):\n    def _convert(image, alpha=1, beta=0):\n        tmp = image.astype(float) * alpha + beta\n        tmp[tmp < 0] = 0\n        tmp[tmp > 255] = 255\n        image[:] = tmp\n\n    image = image.copy()\n\n    if random.randrange(2):\n        _convert(image, beta=random.uniform(-32, 32))\n\n    if random.randrange(2):\n        _convert(image, alpha=random.uniform(0.5, 1.5))\n\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n    if random.randrange(2):\n        tmp = image[:, :, 0].astype(int) + random.randint(-18, 18)\n        tmp %= 180\n        image[:, :, 0] = tmp\n\n    if random.randrange(2):\n        _convert(image[:, :, 1], alpha=random.uniform(0.5, 1.5))\n\n    image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n\n    return image\n\n\ndef _expand(image, boxes,fill, p):\n    if random.random() > p:\n        return image, boxes\n\n    height, width, depth = image.shape\n    for _ in range(50):\n        scale = random.uniform(1,4)\n\n        min_ratio = max(0.5, 1./scale/scale)\n        max_ratio = min(2, scale*scale)\n        ratio = math.sqrt(random.uniform(min_ratio, max_ratio))\n        ws = scale*ratio\n        hs = scale/ratio\n        if ws < 1 or hs < 1:\n            continue\n        w = int(ws * width)\n        h = int(hs * height)\n\n        left = random.randint(0, w - width)\n        top = random.randint(0, h - height)\n\n        boxes_t = boxes.copy()\n        boxes_t[:, :2] += (left, top)\n        boxes_t[:, 2:] += (left, top)\n\n\n        expand_image = np.empty(\n            (h, w, depth),\n            dtype=image.dtype)\n        expand_image[:, :] = fill\n        expand_image[top:top + height, left:left + width] = image\n        image = expand_image\n\n        return image, boxes_t\n\n\ndef _mirror(image, boxes):\n    _, width, _ = image.shape\n    if random.randrange(2):\n        image = image[:, ::-1]\n        boxes = boxes.copy()\n        boxes[:, 0::2] = width - boxes[:, 2::-2]\n    return image, boxes\n\n\ndef preproc_for_test(image, insize, mean):\n    interp_methods = [cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_NEAREST, cv2.INTER_LANCZOS4]\n    interp_method = interp_methods[random.randrange(5)]\n    image = cv2.resize(image, (insize, insize),interpolation=interp_method)\n    image = image.astype(np.float32)\n    image -= mean\n    return image.transpose(2, 0, 1)\n\n\nclass preproc(object):\n\n    def __init__(self, resize, rgb_means, p):\n        self.means = rgb_means\n        self.resize = resize\n        self.p = p\n\n    def __call__(self, image, targets):\n        boxes = targets[:,:-1].copy()\n        labels = targets[:,-1].copy()\n        if len(boxes) == 0:\n            #boxes = np.empty((0, 4))\n            targets = np.zeros((1,5))\n            image = preproc_for_test(image, self.resize, self.means)\n            return torch.from_numpy(image), targets\n\n        image_o = image.copy()\n        targets_o = targets.copy()\n        height_o, width_o, _ = image_o.shape\n        boxes_o = targets_o[:,:-1]\n        labels_o = targets_o[:,-1]\n        boxes_o[:, 0::2] /= width_o\n        boxes_o[:, 1::2] /= height_o\n        labels_o = np.expand_dims(labels_o,1)\n        targets_o = np.hstack((boxes_o,labels_o))\n\n        image_t, boxes, labels = _crop(image, boxes, labels)\n        image_t = _distort(image_t)\n        image_t, boxes = _expand(image_t, boxes, self.means, self.p)\n        image_t, boxes = _mirror(image_t, boxes)\n        #image_t, boxes = _mirror(image, boxes)\n\n        height, width, _ = image_t.shape\n        image_t = preproc_for_test(image_t, self.resize, self.means)\n        boxes = boxes.copy()\n        boxes[:, 0::2] /= width\n        boxes[:, 1::2] /= height\n        b_w = (boxes[:, 2] - boxes[:, 0])*1.\n        b_h = (boxes[:, 3] - boxes[:, 1])*1.\n        mask_b= np.minimum(b_w, b_h) > 0.01\n        boxes_t = boxes[mask_b]\n        labels_t = labels[mask_b].copy()\n\n        if len(boxes_t)==0:\n            image = preproc_for_test(image_o, self.resize, self.means)\n            return torch.from_numpy(image),targets_o\n\n        labels_t = np.expand_dims(labels_t,1)\n        targets_t = np.hstack((boxes_t,labels_t))\n\n        return torch.from_numpy(image_t), targets_t\n\n\n\nclass BaseTransform(object):\n    """"""Defines the transformations that should be applied to test PIL image\n        for input into the network\n\n    dimension -> tensorize -> color adj\n\n    Arguments:\n        resize (int): input dimension to SSD\n        rgb_means ((int,int,int)): average RGB of the dataset\n            (104,117,123)\n        swap ((int,int,int)): final order of channels\n    Returns:\n        transform (transform) : callable transform to be applied to test/val\n        data\n    """"""\n    def __init__(self, resize, rgb_means, swap=(2, 0, 1)):\n        self.means = rgb_means\n        self.resize = resize\n        self.swap = swap\n\n    # assume input is cv2 img for now\n    def __call__(self, img):\n\n        interp_methods = [cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_NEAREST, cv2.INTER_LANCZOS4]\n        interp_method = interp_methods[0]\n        img = cv2.resize(np.array(img), (self.resize,\n                                         self.resize),interpolation = interp_method).astype(np.float32)\n        img -= self.means\n        img = img.transpose(self.swap)\n        return torch.from_numpy(img)\n'"
data/voc0712.py,5,"b'""""""VOC Dataset Classes\n\nOriginal author: Francisco Massa\nhttps://github.com/fmassa/vision/blob/voc_dataset/torchvision/datasets/voc.py\n\nUpdated by: Ellis Brown, Max deGroot\n""""""\n\nimport os\nimport pickle\nimport os.path\nimport sys\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nfrom PIL import Image, ImageDraw, ImageFont\nimport cv2\nimport numpy as np\nfrom .voc_eval import voc_eval\nif sys.version_info[0] == 2:\n    import xml.etree.cElementTree as ET\nelse:\n    import xml.etree.ElementTree as ET\n\n\nVOC_CLASSES = ( \'__background__\', # always index 0\n    \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n    \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n    \'cow\', \'diningtable\', \'dog\', \'horse\',\n    \'motorbike\', \'person\', \'pottedplant\',\n    \'sheep\', \'sofa\', \'train\', \'tvmonitor\')\n\n# for making bounding boxes pretty\nCOLORS = ((255, 0, 0, 128), (0, 255, 0, 128), (0, 0, 255, 128),\n          (0, 255, 255, 128), (255, 0, 255, 128), (255, 255, 0, 128))\n\n\nclass VOCSegmentation(data.Dataset):\n\n    """"""VOC Segmentation Dataset Object\n    input and target are both images\n\n    NOTE: need to address https://github.com/pytorch/vision/issues/9\n\n    Arguments:\n        root (string): filepath to VOCdevkit folder.\n        image_set (string): imageset to use (eg: \'train\', \'val\', \'test\').\n        transform (callable, optional): transformation to perform on the\n            input image\n        target_transform (callable, optional): transformation to perform on the\n            target image\n        dataset_name (string, optional): which dataset to load\n            (default: \'VOC2007\')\n    """"""\n\n    def __init__(self, root, image_set, transform=None, target_transform=None,\n                 dataset_name=\'VOC2007\'):\n        self.root = root\n        self.image_set = image_set\n        self.transform = transform\n        self.target_transform = target_transform\n\n        self._annopath = os.path.join(\n            self.root, dataset_name, \'SegmentationClass\', \'%s.png\')\n        self._imgpath = os.path.join(\n            self.root, dataset_name, \'JPEGImages\', \'%s.jpg\')\n        self._imgsetpath = os.path.join(\n            self.root, dataset_name, \'ImageSets\', \'Segmentation\', \'%s.txt\')\n\n        with open(self._imgsetpath % self.image_set) as f:\n            self.ids = f.readlines()\n        self.ids = [x.strip(\'\\n\') for x in self.ids]\n\n    def __getitem__(self, index):\n        img_id = self.ids[index]\n\n        target = Image.open(self._annopath % img_id).convert(\'RGB\')\n        img = Image.open(self._imgpath % img_id).convert(\'RGB\')\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ids)\n\n\nclass AnnotationTransform(object):\n\n    """"""Transforms a VOC annotation into a Tensor of bbox coords and label index\n    Initilized with a dictionary lookup of classnames to indexes\n\n    Arguments:\n        class_to_ind (dict, optional): dictionary lookup of classnames -> indexes\n            (default: alphabetic indexing of VOC\'s 20 classes)\n        keep_difficult (bool, optional): keep difficult instances or not\n            (default: False)\n        height (int): height\n        width (int): width\n    """"""\n\n    def __init__(self, class_to_ind=None, keep_difficult=True):\n        self.class_to_ind = class_to_ind or dict(\n            zip(VOC_CLASSES, range(len(VOC_CLASSES))))\n        self.keep_difficult = keep_difficult\n\n    def __call__(self, target):\n        """"""\n        Arguments:\n            target (annotation) : the target annotation to be made usable\n                will be an ET.Element\n        Returns:\n            a list containing lists of bounding boxes  [bbox coords, class name]\n        """"""\n        res = np.empty((0,5)) \n        for obj in target.iter(\'object\'):\n            difficult = int(obj.find(\'difficult\').text) == 1\n            if not self.keep_difficult and difficult:\n                continue\n            name = obj.find(\'name\').text.lower().strip()\n            bbox = obj.find(\'bndbox\')\n\n            pts = [\'xmin\', \'ymin\', \'xmax\', \'ymax\']\n            bndbox = []\n            for i, pt in enumerate(pts):\n                cur_pt = int(bbox.find(pt).text) - 1\n                # scale height or width\n                #cur_pt = cur_pt / width if i % 2 == 0 else cur_pt / height\n                bndbox.append(cur_pt)\n            label_idx = self.class_to_ind[name]\n            bndbox.append(label_idx)\n            res = np.vstack((res,bndbox))  # [xmin, ymin, xmax, ymax, label_ind]\n            # img_id = target.find(\'filename\').text[:-4]\n\n        return res  # [[xmin, ymin, xmax, ymax, label_ind], ... ]\n\n\nclass VOCDetection(data.Dataset):\n\n    """"""VOC Detection Dataset Object\n\n    input is image, target is annotation\n\n    Arguments:\n        root (string): filepath to VOCdevkit folder.\n        image_set (string): imageset to use (eg. \'train\', \'val\', \'test\')\n        transform (callable, optional): transformation to perform on the\n            input image\n        target_transform (callable, optional): transformation to perform on the\n            target `annotation`\n            (eg: take in caption string, return tensor of word indices)\n        dataset_name (string, optional): which dataset to load\n            (default: \'VOC2007\')\n    """"""\n\n    def __init__(self, root, image_sets, preproc=None, target_transform=None,\n                 dataset_name=\'VOC0712\'):\n        self.root = root\n        self.image_set = image_sets\n        self.preproc = preproc\n        self.target_transform = target_transform\n        self.name = dataset_name\n        self._annopath = os.path.join(\'%s\', \'Annotations\', \'%s.xml\')\n        self._imgpath = os.path.join(\'%s\', \'JPEGImages\', \'%s.jpg\')\n        self.ids = list()\n        for (year, name) in image_sets:\n            self._year = year\n            rootpath = os.path.join(self.root, \'VOC\' + year)\n            for line in open(os.path.join(rootpath, \'ImageSets\', \'Main\', name + \'.txt\')):\n                self.ids.append((rootpath, line.strip()))\n\n    def __getitem__(self, index):\n        img_id = self.ids[index]\n        target = ET.parse(self._annopath % img_id).getroot()\n        img = cv2.imread(self._imgpath % img_id, cv2.IMREAD_COLOR)\n        height, width, _ = img.shape\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n\n        if self.preproc is not None:\n            img, target = self.preproc(img, target)\n            #print(img.size())\n\n                    # target = self.target_transform(target, width, height)\n        #print(target.shape)\n\n        return img, target\n\n    def __len__(self):\n        return len(self.ids)\n\n    def pull_image(self, index):\n        \'\'\'Returns the original image object at index in PIL form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            PIL img\n        \'\'\'\n        img_id = self.ids[index]\n        return cv2.imread(self._imgpath % img_id, cv2.IMREAD_COLOR)\n\n    def pull_anno(self, index):\n        \'\'\'Returns the original annotation of image at index\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to get annotation of\n        Return:\n            list:  [img_id, [(label, bbox coords),...]]\n                eg: (\'001718\', [(\'dog\', (96, 13, 438, 332))])\n        \'\'\'\n        img_id = self.ids[index]\n        anno = ET.parse(self._annopath % img_id).getroot()\n        gt = self.target_transform(anno, 1, 1)\n        return img_id[1], gt\n\n    def pull_tensor(self, index):\n        \'\'\'Returns the original image at an index in tensor form\n\n        Note: not using self.__getitem__(), as any transformations passed in\n        could mess up this functionality.\n\n        Argument:\n            index (int): index of img to show\n        Return:\n            tensorized version of img, squeezed\n        \'\'\'\n        to_tensor = transforms.ToTensor()\n        return torch.Tensor(self.pull_image(index)).unsqueeze_(0)\n\n    def evaluate_detections(self, all_boxes, output_dir=None):\n        """"""\n        all_boxes is a list of length number-of-classes.\n        Each list element is a list of length number-of-images.\n        Each of those list elements is either an empty list []\n        or a numpy array of detection.\n\n        all_boxes[class][image] = [] or np.array of shape #dets x 5\n        """"""\n        self._write_voc_results_file(all_boxes)\n        self._do_python_eval(output_dir)\n\n    def _get_voc_results_file_template(self):\n        filename = \'comp4_det_test\' + \'_{:s}.txt\'\n        filedir = os.path.join(\n            self.root, \'results\', \'VOC\' + self._year, \'Main\')\n        if not os.path.exists(filedir):\n            os.makedirs(filedir)\n        path = os.path.join(filedir, filename)\n        return path\n\n    def _write_voc_results_file(self, all_boxes):\n        for cls_ind, cls in enumerate(VOC_CLASSES):\n            cls_ind = cls_ind \n            if cls == \'__background__\':\n                continue\n            print(\'Writing {} VOC results file\'.format(cls))\n            filename = self._get_voc_results_file_template().format(cls)\n            with open(filename, \'wt\') as f:\n                for im_ind, index in enumerate(self.ids):\n                    index = index[1]\n                    dets = all_boxes[cls_ind][im_ind]\n                    if dets == []:\n                        continue\n                    for k in range(dets.shape[0]):\n                        f.write(\'{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n\'.\n                                format(index, dets[k, -1],\n                                dets[k, 0] + 1, dets[k, 1] + 1,\n                                dets[k, 2] + 1, dets[k, 3] + 1))\n\n    def _do_python_eval(self, output_dir=\'output\'):\n        rootpath = os.path.join(self.root, \'VOC\' + self._year)\n        name = self.image_set[0][1]\n        annopath = os.path.join(\n                                rootpath,\n                                \'Annotations\',\n                                \'{:s}.xml\')\n        imagesetfile = os.path.join(\n                                rootpath,\n                                \'ImageSets\',\n                                \'Main\',\n                                name+\'.txt\')\n        cachedir = os.path.join(self.root, \'annotations_cache\')\n        aps = []\n        # The PASCAL VOC metric changed in 2010\n        use_07_metric = True if int(self._year) < 2010 else False\n        print(\'VOC07 metric? \' + (\'Yes\' if use_07_metric else \'No\'))\n        if output_dir is not None and not os.path.isdir(output_dir):\n            os.mkdir(output_dir)\n        for i, cls in enumerate(VOC_CLASSES):\n\n            if cls == \'__background__\':\n                continue\n\n            filename = self._get_voc_results_file_template().format(cls)\n            rec, prec, ap = voc_eval(\n                                    filename, annopath, imagesetfile, cls, cachedir, ovthresh=0.5,\n                                    use_07_metric=use_07_metric)\n            aps += [ap]\n            print(\'AP for {} = {:.4f}\'.format(cls, ap))\n            if output_dir is not None:\n                with open(os.path.join(output_dir, cls + \'_pr.pkl\'), \'wb\') as f:\n                    pickle.dump({\'rec\': rec, \'prec\': prec, \'ap\': ap}, f)\n        print(\'Mean AP = {:.4f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'Results:\')\n        for ap in aps:\n            print(\'{:.3f}\'.format(ap))\n        print(\'{:.3f}\'.format(np.mean(aps)))\n        print(\'~~~~~~~~\')\n        print(\'\')\n        print(\'--------------------------------------------------------------\')\n        print(\'Results computed with the **unofficial** Python eval code.\')\n        print(\'Results should be very close to the official MATLAB eval code.\')\n        print(\'Recompute with `./tools/reval.py --matlab ...` for your paper.\')\n        print(\'-- Thanks, The Management\')\n        print(\'--------------------------------------------------------------\')\n\ndef detection_collate(batch):\n    """"""Custom collate fn for dealing with batches of images that have a different\n    number of associated object annotations (bounding boxes).\n\n    Arguments:\n        batch: (tuple) A tuple of tensor images and lists of annotations\n\n    Return:\n        A tuple containing:\n            1) (tensor) batch of images stacked on their 0 dim\n            2) (list of tensors) annotations for a given image are stacked on 0 dim\n    """"""\n    targets = []\n    imgs = []\n    for _, sample in enumerate(batch):\n        for _, tup in enumerate(sample):\n            if torch.is_tensor(tup):\n                imgs.append(tup)\n            elif isinstance(tup, type(np.empty(0))):\n                annos = torch.from_numpy(tup).float()\n                targets.append(annos)\n\n    return (torch.stack(imgs, 0), targets)\n'"
data/voc_eval.py,0,"b'# --------------------------------------------------------\n# Fast/er R-CNN\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Bharath Hariharan\n# --------------------------------------------------------\n\nimport xml.etree.ElementTree as ET\nimport os\nimport pickle\nimport numpy as np\nimport pdb\n\n\ndef parse_rec(filename):\n    """""" Parse a PASCAL VOC xml file """"""\n    tree = ET.parse(filename)\n    objects = []\n    for obj in tree.findall(\'object\'):\n        obj_struct = {}\n        obj_struct[\'name\'] = obj.find(\'name\').text\n        obj_struct[\'pose\'] = obj.find(\'pose\').text\n        obj_struct[\'truncated\'] = int(obj.find(\'truncated\').text)\n        obj_struct[\'difficult\'] = int(obj.find(\'difficult\').text)\n        bbox = obj.find(\'bndbox\')\n        obj_struct[\'bbox\'] = [int(bbox.find(\'xmin\').text),\n                              int(bbox.find(\'ymin\').text),\n                              int(bbox.find(\'xmax\').text),\n                              int(bbox.find(\'ymax\').text)]\n        objects.append(obj_struct)\n\n    return objects\n\n\n\ndef voc_ap(rec, prec, use_07_metric=False):\n    """""" ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """"""\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(rec >= t) == 0:\n                p = 0\n            else:\n                p = np.max(prec[rec >= t])\n            ap = ap + p / 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], rec, [1.]))\n        mpre = np.concatenate(([0.], prec, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\ndef voc_eval(detpath,\n             annopath,\n             imagesetfile,\n             classname,\n             cachedir,\n             ovthresh=0.5,\n             use_07_metric=False):\n    """"""rec, prec, ap = voc_eval(detpath,\n                                annopath,\n                                imagesetfile,\n                                classname,\n                                [ovthresh],\n                                [use_07_metric])\n\n    Top level function that does the PASCAL VOC evaluation.\n\n    detpath: Path to detections\n        detpath.format(classname) should produce the detection results file.\n    annopath: Path to annotations\n        annopath.format(imagename) should be the xml annotations file.\n    imagesetfile: Text file containing the list of images, one image per line.\n    classname: Category name (duh)\n    cachedir: Directory for caching the annotations\n    [ovthresh]: Overlap threshold (default = 0.5)\n    [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n        (default False)\n    """"""\n    # assumes detections are in detpath.format(classname)\n    # assumes annotations are in annopath.format(imagename)\n    # assumes imagesetfile is a text file with each line an image name\n    # cachedir caches the annotations in a pickle file\n\n    # first load gt\n    if not os.path.isdir(cachedir):\n        os.mkdir(cachedir)\n    cachefile = os.path.join(cachedir, \'annots.pkl\')\n    # read list of images\n    with open(imagesetfile, \'r\') as f:\n        lines = f.readlines()\n    imagenames = [x.strip() for x in lines]\n\n    if not os.path.isfile(cachefile):\n        # load annots\n        recs = {}\n        for i, imagename in enumerate(imagenames):\n            recs[imagename] = parse_rec(annopath.format(imagename))\n            if i % 100 == 0:\n                print(\'Reading annotation for {:d}/{:d}\'.format(\n                    i + 1, len(imagenames)))\n        # save\n        print(\'Saving cached annotations to {:s}\'.format(cachefile))\n        with open(cachefile, \'wb\') as f:\n            pickle.dump(recs, f)\n    else:\n        # load\n        with open(cachefile, \'rb\') as f:\n            recs = pickle.load(f)\n\n    # extract gt objects for this class\n    class_recs = {}\n    npos = 0\n    for imagename in imagenames:\n        R = [obj for obj in recs[imagename] if obj[\'name\'] == classname]\n        bbox = np.array([x[\'bbox\'] for x in R])\n        difficult = np.array([x[\'difficult\'] for x in R]).astype(np.bool)\n        det = [False] * len(R)\n        npos = npos + sum(~difficult)\n        class_recs[imagename] = {\'bbox\': bbox,\n                                 \'difficult\': difficult,\n                                 \'det\': det}\n\n    # read dets\n    detfile = detpath.format(classname)\n    with open(detfile, \'r\') as f:\n        lines = f.readlines()\n\n    splitlines = [x.strip().split(\' \') for x in lines]\n    image_ids = [x[0] for x in splitlines]\n    confidence = np.array([float(x[1]) for x in splitlines])\n    BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n\n        # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    sorted_scores = np.sort(-confidence)\n    BB = BB[sorted_ind, :]\n    image_ids = [image_ids[x] for x in sorted_ind]\n\n        # go down dets and mark TPs and FPs\n    nd = len(image_ids)\n    tp = np.zeros(nd)\n    fp = np.zeros(nd)\n    for d in range(nd):\n        R = class_recs[image_ids[d]]\n        bb = BB[d, :].astype(float)\n        ovmax = -np.inf\n        BBGT = R[\'bbox\'].astype(float)\n\n        if BBGT.size > 0:\n            # compute overlaps\n            # intersection\n            ixmin = np.maximum(BBGT[:, 0], bb[0])\n            iymin = np.maximum(BBGT[:, 1], bb[1])\n            ixmax = np.minimum(BBGT[:, 2], bb[2])\n            iymax = np.minimum(BBGT[:, 3], bb[3])\n            iw = np.maximum(ixmax - ixmin + 1., 0.)\n            ih = np.maximum(iymax - iymin + 1., 0.)\n            inters = iw * ih\n\n                # union\n            uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n                   (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n                   (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n\n            overlaps = inters / uni\n            ovmax = np.max(overlaps)\n            jmax = np.argmax(overlaps)\n\n        if ovmax > ovthresh:\n            if not R[\'difficult\'][jmax]:\n                if not R[\'det\'][jmax]:\n                    tp[d] = 1.\n                    R[\'det\'][jmax] = 1\n                else:\n                    fp[d] = 1.\n        else:\n            fp[d] = 1.\n\n        # compute precision recall\n    fp = np.cumsum(fp)\n    tp = np.cumsum(tp)\n    rec = tp / float(npos)\n        # avoid divide by zero in case the first detection matches a difficult\n        # ground truth\n    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n    ap = voc_ap(rec, prec, use_07_metric)\n\n    return rec, prec, ap\n'"
layers/__init__.py,0,b'from .functions import *\nfrom .modules import *\n'
models/RFB_Net_E_vgg.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom layers import *\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.backends.cudnn as cudnn\nimport os\n\nclass BasicConv(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(BasicConv, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU(inplace=True) if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\n\nclass BasicRFB(nn.Module):\n\n    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1,map_reduce=8):\n        super(BasicRFB, self).__init__()\n        self.scale = scale\n        self.out_channels = out_planes\n        inter_planes = in_planes // map_reduce\n\n        self.branch0 = nn.Sequential(\n                BasicConv(in_planes, 2*inter_planes, kernel_size=1, stride=stride),\n                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=1,relu=False)\n                )\n        self.branch1 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, 2*inter_planes, kernel_size=(3,3), stride=stride, padding=(1,1)),\n                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch2 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=3, stride=1, padding=1),\n                BasicConv((inter_planes//2)*3, 2*inter_planes, kernel_size=3, stride=stride, padding=1),\n                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n                )\n        self.branch3 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=(1,7), stride=1, padding=(0,3)),\n                BasicConv((inter_planes//2)*3, 2*inter_planes, kernel_size=(7,1), stride=stride, padding=(3,0)),\n                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=7, dilation=7, relu=False)\n                )\n\n        self.ConvLinear = BasicConv(8*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n        self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self,x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0,x1,x2,x3),1)\n        out = self.ConvLinear(out)\n        short = self.shortcut(x)\n        out = out*self.scale + short\n        out = self.relu(out)\n\n        return out\n\nclass BasicRFB_c(nn.Module):\n\n    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1,map_reduce=8):\n        super(BasicRFB_c, self).__init__()\n        self.scale = scale\n        self.out_channels = out_planes\n        inter_planes = in_planes // map_reduce\n        self.branch0 = nn.Sequential(\n                BasicConv(in_planes, 2*inter_planes, kernel_size=1, stride=stride),\n                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=1,relu=False)\n                )\n        self.branch1 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, 2*inter_planes, kernel_size=(3,3), stride=stride, padding=(1,1)),\n                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch2 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=(1,7), stride=1, padding=(0,3)),\n                BasicConv((inter_planes//2)*3, 2*inter_planes, kernel_size=(7,1), stride=stride, padding=(3,0)),\n                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=7, dilation=7, relu=False)\n                )\n\n        self.ConvLinear = BasicConv(6*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n        self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self,x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n\n        out = torch.cat((x0,x1,x2),1)\n        out = self.ConvLinear(out)\n        short = self.shortcut(x)\n        out = out*self.scale + short\n        out = self.relu(out)\n\n        return out\n\nclass BasicRFB_a(nn.Module):\n\n    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1):\n        super(BasicRFB_a, self).__init__()\n        self.scale = scale\n        self.out_channels = out_planes\n        inter_planes = in_planes //8\n\n\n        self.branch0 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=1,relu=False)\n                )\n        self.branch1 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=(3,1), stride=1, padding=(1,0)),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch2 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=(1,3), stride=stride, padding=(0,1)),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch3 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=(3,1), stride=1, padding=(1,0)),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n                )\n        self.branch4 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=(1,3), stride=stride, padding=(0,1)),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n                )\n        self.branch5 = nn.Sequential(\n                BasicConv(in_planes, inter_planes//2, kernel_size=1, stride=1),\n                BasicConv(inter_planes//2, (inter_planes//4)*3, kernel_size=(1,3), stride=1, padding=(0,1)),\n                BasicConv((inter_planes//4)*3, inter_planes, kernel_size=(3,1), stride=stride, padding=(1,0)),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=7, dilation=7, relu=False)\n                )\n\n        self.branch6 = nn.Sequential(\n                BasicConv(in_planes, inter_planes//2, kernel_size=1, stride=1),\n                BasicConv(inter_planes//2, (inter_planes//4)*3, kernel_size=(3,1), stride=1, padding=(1,0)),\n                BasicConv((inter_planes//4)*3, inter_planes, kernel_size=(1,3), stride=stride, padding=(0,1)),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=7, dilation=7, relu=False)\n                )\n        self.ConvLinear = BasicConv(7*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n        self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self,x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        x4 = self.branch4(x)\n        x5 = self.branch5(x)\n        x6 = self.branch6(x)\n\n        out = torch.cat((x0,x1,x2,x3,x4,x5,x6),1)\n        out = self.ConvLinear(out)\n        short = self.shortcut(x)\n        out = out*self.scale + short\n        out = self.relu(out)\n\n        return out\n\nclass RFBNet(nn.Module):\n\n    def __init__(self, phase, size, base, extras, head, num_classes):\n        super(RFBNet, self).__init__()\n        self.phase = phase\n        self.num_classes = num_classes\n        self.size = size\n\n        if size == 300:\n            self.indicator = 3\n        elif size == 512:\n            self.indicator = 5\n        else:\n            print(""Error: Sorry only RFB300 and RFB512 are supported!"")\n            return\n        self.base = nn.ModuleList(base)\n        # Layer learns to scale the l2 normalized features from conv4_3\n\n        self.reduce= BasicConv(512,256,kernel_size=1,stride=1)\n        self.up_reduce= BasicConv(1024,256,kernel_size=1,stride=1)\n\n        self.Norm = BasicRFB_a(512,512,stride = 1,scale=1.0)\n        self.extras = nn.ModuleList(extras)\n\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n        if self.phase == \'test\':\n            self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3*batch,300,300].\n\n        Return:\n            Depending on phase:\n            test:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n                    3: priorbox layers, Shape: [2,num_priors*4]\n        """"""\n        sources = list()\n        loc = list()\n        conf = list()\n\n        # apply vgg up to conv4_3 relu\n        for k in range(23):\n            x = self.base[k](x)\n\n        s1 = self.reduce(x)\n\n        # apply vgg up to fc7\n        for k in range(23, len(self.base)):\n            x = self.base[k](x)\n        s2 = self.up_reduce(x)\n        s2 = F.upsample(s2, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        s = torch.cat((s1,s2),1)\n\n        ss = self.Norm(s)\n        sources.append(ss)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            x = v(x)\n            if k < self.indicator or k%2 ==0:\n                sources.append(x)\n\n        # apply multibox head to source layers\n        for (x, l, c) in zip(sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n\n        #print([o.size() for o in loc])\n\n\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n        if self.phase == ""test"":\n            output = (\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n            )\n        return output\n\n    def load_weights(self, base_file):\n        other, ext = os.path.splitext(base_file)\n        if ext == \'.pkl\' or \'.pth\':\n            print(\'Loading weights into state dict...\')\n            self.load_state_dict(torch.load(base_file))\n            print(\'Finished!\')\n        else:\n            print(\'Sorry only .pth and .pkl files supported.\')\n\n\n# This function is derived from torchvision VGG make_layers()\n# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\ndef vgg(cfg, i, batch_norm=False):\n    layers = []\n    in_channels = i\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        elif v == \'C\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n    layers += [pool5, conv6,\n               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n    return layers\n\nbase = {\n    \'300\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\',\n            512, 512, 512],\n    \'512\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\',\n            512, 512, 512],\n}\n\n\ndef add_extras(size, cfg, i, batch_norm=False):\n    # Extra layers added to VGG for feature scaling\n    layers = []\n    in_channels = i\n    flag = False\n    for k, v in enumerate(cfg):\n        if in_channels != \'S\':\n            if v == \'S\':\n                if in_channels == 256:\n                    layers += [BasicRFB_c(in_channels, cfg[k+1], stride=2, scale = 1.0)]\n                else:\n                    layers += [BasicRFB(in_channels, cfg[k+1], stride=2, scale = 1.0)]\n            else:\n                layers += [BasicRFB(in_channels, v, scale = 1.0)]\n        in_channels = v\n    if size == 512:\n        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n        layers += [BasicConv(128,256,kernel_size=4,stride=1,padding=1)]\n    elif size ==300:\n        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n        layers += [BasicConv(128,256,kernel_size=3,stride=1)]\n        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n        layers += [BasicConv(128,256,kernel_size=3,stride=1)]\n    else:\n        print(""Error: Sorry only RFB300 and RFB512 are supported!"")\n        return\n    return layers\n\nextras = {\n    \'300\': [1024, \'S\', 512, \'S\', 256],\n    \'512\': [1024, \'S\', 512, \'S\', 256, \'S\', 256,\'S\',256],\n}\n\n\ndef multibox(size, vgg, extra_layers, cfg, num_classes):\n    loc_layers = []\n    conf_layers = []\n    vgg_source = [-2]\n    for k, v in enumerate(vgg_source):\n        if k == 0:\n            loc_layers += [nn.Conv2d(512,\n                                 cfg[k] * 4, kernel_size=3, padding=1)]\n            conf_layers +=[nn.Conv2d(512,\n                                 cfg[k] * num_classes, kernel_size=3, padding=1)]\n        else:\n            loc_layers += [nn.Conv2d(vgg[v].out_channels,\n                                 cfg[k] * 4, kernel_size=3, padding=1)]\n            conf_layers += [nn.Conv2d(vgg[v].out_channels,\n                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n    i = 1\n    indicator = 0\n    if size == 300:\n        indicator = 3\n    elif size == 512:\n        indicator = 5\n    else:\n        print(""Error: Sorry only RFB300 and RFB512 are supported!"")\n        return\n\n    for k, v in enumerate(extra_layers):\n        if k < indicator or k%2== 0:\n            loc_layers += [nn.Conv2d(v.out_channels, cfg[i]\n                                 * 4, kernel_size=3, padding=1)]\n            conf_layers += [nn.Conv2d(v.out_channels, cfg[i]\n                                  * num_classes, kernel_size=3, padding=1)]\n            i +=1\n    return vgg, extra_layers, (loc_layers, conf_layers)\n\nmbox = {\n    \'300\': [6, 6, 6, 6, 4, 4],  # number of boxes per feature map location\n    \'512\': [6, 6, 6, 6, 6, 4, 4],\n}\n\n\ndef build_net(phase, size=300, num_classes=21):\n    if phase != ""test"" and phase != ""train"":\n        print(""Error: Phase not recognized"")\n        return\n    if size != 300 and size != 512:\n        print(""Error: Sorry only RFB300 and RFB512 are supported!"")\n        return\n\n    return RFBNet(phase, size, *multibox(size, vgg(base[str(size)], 3),\n                                add_extras(size, extras[str(size)], 1024),\n                                mbox[str(size)], num_classes), num_classes)\n'"
models/RFB_Net_mobile.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom layers import *\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.backends.cudnn as cudnn\nimport os\n\nclass BasicConv(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(BasicConv, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU(inplace=True) if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\nclass BasicSepConv(nn.Module):\n\n    def __init__(self, in_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(BasicSepConv, self).__init__()\n        self.out_channels = in_planes\n        self.conv = nn.Conv2d(in_planes, in_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups = in_planes, bias=bias)\n        self.bn = nn.BatchNorm2d(in_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU(inplace=True) if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\nclass BasicRFB(nn.Module):\n\n    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1):\n        super(BasicRFB, self).__init__()\n        self.scale = scale\n        self.out_channels = out_planes\n        inter_planes = in_planes // 8\n        self.branch1 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=(1,3), stride=1, padding=(0,1)),\n                BasicConv((inter_planes//2)*3, (inter_planes//2)*3, kernel_size=(3,1), stride=stride, padding=(1,0)),\n                BasicSepConv((inter_planes//2)*3, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch2 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=3, stride=1, padding=1),\n                BasicConv((inter_planes//2)*3, (inter_planes//2)*3, kernel_size=3, stride=stride, padding=1),\n                BasicSepConv((inter_planes//2)*3, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n                )\n\n        self.ConvLinear = BasicConv(3*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n        if in_planes == out_planes:\n            self.identity = True\n        else:\n            self.identity = False\n            self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self,x):\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n\n        out = torch.cat((x1,x2),1)\n        out = self.ConvLinear(out)\n        if self.identity:\n            out = out*self.scale + x\n        else:\n            short = self.shortcut(x)\n            out = out*self.scale + short\n        out = self.relu(out)\n\n        return out\n\n\nclass BasicRFB_a(nn.Module):\n\n    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1):\n        super(BasicRFB_a, self).__init__()\n        self.scale = scale\n        self.out_channels = out_planes\n        inter_planes = in_planes //4\n\n\n        self.branch0 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicSepConv(inter_planes, kernel_size=3, stride=1, padding=1, dilation=1, relu=False)\n                )\n        self.branch1 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=(3,1), stride=1, padding=(1,0)),\n                BasicSepConv(inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch2 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=(1,3), stride=stride, padding=(0,1)),\n                BasicSepConv(inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch3 = nn.Sequential(\n                BasicConv(in_planes, inter_planes//2, kernel_size=1, stride=1),\n                BasicConv(inter_planes//2, (inter_planes//4)*3, kernel_size=(1,3), stride=1, padding=(0,1)),\n                BasicConv((inter_planes//4)*3, inter_planes, kernel_size=(3,1), stride=stride, padding=(1,0)),\n                BasicSepConv(inter_planes, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n                )\n\n        self.ConvLinear = BasicConv(4*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self,x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0,x1,x2,x3),1)\n        out = self.ConvLinear(out)\n        out = out*self.scale + x\n        out = self.relu(out)\n\n        return out\n\nclass RFBNet(nn.Module):\n\n    def __init__(self, phase, size, base, extras, head, num_classes):\n        super(RFBNet, self).__init__()\n        self.phase = phase\n        self.num_classes = num_classes\n        self.size = size\n\n        if size == 300:\n            self.indicator = 1\n        else:\n            print(""Error: Sorry only RFB300_mobile is supported!"")\n            return\n\n        self.base = nn.ModuleList(base)\n        self.Norm = BasicRFB_a(512,512,stride = 1,scale=1.0)\n        self.extras = nn.ModuleList(extras)\n\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n        if self.phase == \'test\':\n            self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3*batch,300,300].\n\n        Return:\n            Depending on phase:\n            test:\n                Variable(tensor) of output class label predictions,\n                confidence score, and corresponding location predictions for\n                each object detected. Shape: [batch,topk,7]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n                    3: priorbox layers, Shape: [2,num_priors*4]\n        """"""\n        sources = list()\n        loc = list()\n        conf = list()\n\n        # apply vgg up to conv4_3 relu\n        for k in range(12):\n            x = self.base[k](x)\n\n\n        s = self.Norm(x)\n        sources.append(s)\n\n        for k in range(12, len(self.base)):\n            x = self.base[k](x)\n        sources.append(x)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            x = v(x)\n            if k < self.indicator or k%2 == 0:\n                sources.append(x)\n\n        # apply multibox head to source layers\n        for (x, l, c) in zip(sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n\n        #print([o.size() for o in loc])\n\n\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n        if self.phase == ""test"":\n            output = (\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n            )\n        return output\n\n    def load_weights(self, base_file):\n        other, ext = os.path.splitext(base_file)\n        if ext == \'.pkl\' or \'.pth\':\n            print(\'Loading weights into state dict...\')\n            self.load_state_dict(torch.load(base_file))\n            print(\'Finished!\')\n        else:\n            print(\'Sorry only .pth and .pkl files supported.\')\n\n\ndef conv_bn(inp,oup,stride):\n    return nn.Sequential(\n            nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n            nn.BatchNorm2d(oup),\n            nn.ReLU(inplace=True)\n    )\n\ndef conv_dw(inp, oup, stride):\n    return nn.Sequential(\n            nn.Conv2d(inp,inp, kernel_size=3, stride=stride, padding=1,groups = inp, bias=False),\n            nn.BatchNorm2d(inp),\n            nn.ReLU(inplace=True),\n\n            nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n            nn.ReLU(inplace=True),\n    )\n\ndef MobileNet():\n    layers = []\n    layers += [conv_bn(3, 32, 2)]\n    layers += [conv_dw(32, 64, 1)]\n    layers += [conv_dw(64, 128, 2)]\n    layers += [conv_dw(128, 128, 1)]\n    layers += [conv_dw(128, 256, 2)]\n    layers += [conv_dw(256, 256, 1)]\n    layers += [conv_dw(256, 512, 2)]\n    layers += [conv_dw(512, 512, 1)]\n    layers += [conv_dw(512, 512, 1)]\n    layers += [conv_dw(512, 512, 1)]\n    layers += [conv_dw(512, 512, 1)]\n    layers += [conv_dw(512, 512, 1)]\n    layers += [conv_dw(512, 1024, 2)]\n    layers += [conv_dw(1024, 1024, 1)]\n\n    return layers\n\n\n\ndef add_extras(size, cfg, i, batch_norm=False):\n    layers = []\n    in_channels = i\n    flag = False\n    for k, v in enumerate(cfg):\n        if in_channels != \'S\':\n            if v == \'S\':\n                layers += [BasicRFB(in_channels, cfg[k+1], stride=2, scale = 1.0)]\n            else:\n                layers += [BasicRFB(in_channels, v, scale = 1.0)]\n        in_channels = v\n    if size ==300:\n        layers += [BasicConv(512,128,kernel_size=1,stride=1)]\n        layers += [BasicConv(128,256,kernel_size=3,stride=2, padding=1)]\n        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n        layers += [BasicConv(128,256,kernel_size=3,stride=2, padding=1)]\n        layers += [BasicConv(256,64,kernel_size=1,stride=1)]\n        layers += [BasicConv(64,128,kernel_size=3,stride=2, padding=1)]\n    else:\n        print(""Error: Sorry only RFB300_mobile is supported!"")\n        return\n    return layers\n\nextras = {\n    \'300\': [\'S\', 512 ],\n}\n\n\ndef multibox(size, base, extra_layers, cfg, num_classes):\n    loc_layers = []\n    conf_layers = []\n    base_net= [-2,-1]\n    for k, v in enumerate(base_net):\n        if k == 0:\n            loc_layers += [nn.Conv2d(512,\n                                 cfg[k] * 4, kernel_size=1, padding=0)]\n            conf_layers +=[nn.Conv2d(512,\n                                 cfg[k] * num_classes, kernel_size=1, padding=0)]\n        else:\n            loc_layers += [nn.Conv2d(1024,\n                                 cfg[k] * 4, kernel_size=1, padding=0)]\n            conf_layers += [nn.Conv2d(1024,\n                        cfg[k] * num_classes, kernel_size=1, padding=0)]\n    i = 2\n    indicator = 0\n    if size == 300:\n        indicator = 1\n    else:\n        print(""Error: Sorry only RFB300_mobile is supported!"")\n        return\n\n    for k, v in enumerate(extra_layers):\n        if k < indicator or k%2== 0:\n            loc_layers += [nn.Conv2d(v.out_channels, cfg[i]\n                                 * 4, kernel_size=1, padding=0)]\n            conf_layers += [nn.Conv2d(v.out_channels, cfg[i]\n                                  * num_classes, kernel_size=1, padding=0)]\n            i +=1\n    return base, extra_layers, (loc_layers, conf_layers)\n\nmbox = {\n    \'300\': [6, 6, 6, 6, 4, 4],  # number of boxes per feature map location\n}\n\n\ndef build_net(phase, size=300, num_classes=21):\n    if phase != ""test"" and phase != ""train"":\n        print(""Error: Phase not recognized"")\n        return\n    if size != 300:\n        print(""Error: Sorry only RFB300_mobile is supported!"")\n        return\n\n    return RFBNet(phase, size, *multibox(size, MobileNet(),\n                                add_extras(size, extras[str(size)], 1024),\n                                mbox[str(size)], num_classes), num_classes)\n'"
models/RFB_Net_vgg.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom layers import *\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nimport torch.backends.cudnn as cudnn\nimport os\n\nclass BasicConv(nn.Module):\n\n    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n        super(BasicConv, self).__init__()\n        self.out_channels = out_planes\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n        self.relu = nn.ReLU(inplace=True) if relu else None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\n\nclass BasicRFB(nn.Module):\n\n    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1, visual = 1):\n        super(BasicRFB, self).__init__()\n        self.scale = scale\n        self.out_channels = out_planes\n        inter_planes = in_planes // 8\n        self.branch0 = nn.Sequential(\n                BasicConv(in_planes, 2*inter_planes, kernel_size=1, stride=stride),\n                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=visual, dilation=visual, relu=False)\n                )\n        self.branch1 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, 2*inter_planes, kernel_size=(3,3), stride=stride, padding=(1,1)),\n                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=visual+1, dilation=visual+1, relu=False)\n                )\n        self.branch2 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=3, stride=1, padding=1),\n                BasicConv((inter_planes//2)*3, 2*inter_planes, kernel_size=3, stride=stride, padding=1),\n                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=2*visual+1, dilation=2*visual+1, relu=False)\n                )\n\n        self.ConvLinear = BasicConv(6*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n        self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self,x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n\n        out = torch.cat((x0,x1,x2),1)\n        out = self.ConvLinear(out)\n        short = self.shortcut(x)\n        out = out*self.scale + short\n        out = self.relu(out)\n\n        return out\n\n\n\nclass BasicRFB_a(nn.Module):\n\n    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1):\n        super(BasicRFB_a, self).__init__()\n        self.scale = scale\n        self.out_channels = out_planes\n        inter_planes = in_planes //4\n\n\n        self.branch0 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=1,relu=False)\n                )\n        self.branch1 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=(3,1), stride=1, padding=(1,0)),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch2 = nn.Sequential(\n                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n                BasicConv(inter_planes, inter_planes, kernel_size=(1,3), stride=stride, padding=(0,1)),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n                )\n        self.branch3 = nn.Sequential(\n                BasicConv(in_planes, inter_planes//2, kernel_size=1, stride=1),\n                BasicConv(inter_planes//2, (inter_planes//4)*3, kernel_size=(1,3), stride=1, padding=(0,1)),\n                BasicConv((inter_planes//4)*3, inter_planes, kernel_size=(3,1), stride=stride, padding=(1,0)),\n                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n                )\n\n        self.ConvLinear = BasicConv(4*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n        self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self,x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0,x1,x2,x3),1)\n        out = self.ConvLinear(out)\n        short = self.shortcut(x)\n        out = out*self.scale + short\n        out = self.relu(out)\n\n        return out\n\nclass RFBNet(nn.Module):\n    """"""RFB Net for object detection\n    The network is based on the SSD architecture.\n    Each multibox layer branches into\n        1) conv2d for class conf scores\n        2) conv2d for localization predictions\n        3) associated priorbox layer to produce default bounding\n           boxes specific to the layer\'s feature map size.\n    See: https://arxiv.org/pdf/1711.07767.pdf for more details on RFB Net.\n\n    Args:\n        phase: (string) Can be ""test"" or ""train""\n        base: VGG16 layers for input, size of either 300 or 512\n        extras: extra layers that feed to multibox loc and conf layers\n        head: ""multibox head"" consists of loc and conf conv layers\n    """"""\n\n    def __init__(self, phase, size, base, extras, head, num_classes):\n        super(RFBNet, self).__init__()\n        self.phase = phase\n        self.num_classes = num_classes\n        self.size = size\n\n        if size == 300:\n            self.indicator = 3\n        elif size == 512:\n            self.indicator = 5\n        else:\n            print(""Error: Sorry only SSD300 and SSD512 are supported!"")\n            return\n        # vgg network\n        self.base = nn.ModuleList(base)\n        # conv_4\n        self.Norm = BasicRFB_a(512,512,stride = 1,scale=1.0)\n        self.extras = nn.ModuleList(extras)\n\n        self.loc = nn.ModuleList(head[0])\n        self.conf = nn.ModuleList(head[1])\n        if self.phase == \'test\':\n            self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, x):\n        """"""Applies network layers and ops on input image(s) x.\n\n        Args:\n            x: input image or batch of images. Shape: [batch,3*batch,300,300].\n\n        Return:\n            Depending on phase:\n            test:\n                list of concat outputs from:\n                    1: softmax layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n                    3: priorbox layers, Shape: [2,num_priors*4]\n\n            train:\n                list of concat outputs from:\n                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n                    2: localization layers, Shape: [batch,num_priors*4]\n                    3: priorbox layers, Shape: [2,num_priors*4]\n        """"""\n        sources = list()\n        loc = list()\n        conf = list()\n\n        # apply vgg up to conv4_3 relu\n        for k in range(23):\n            x = self.base[k](x)\n\n        s = self.Norm(x)\n        sources.append(s)\n\n        # apply vgg up to fc7\n        for k in range(23, len(self.base)):\n            x = self.base[k](x)\n\n        # apply extra layers and cache source layer outputs\n        for k, v in enumerate(self.extras):\n            x = v(x)\n            if k < self.indicator or k%2 ==0:\n                sources.append(x)\n\n        # apply multibox head to source layers\n        for (x, l, c) in zip(sources, self.loc, self.conf):\n            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n\n        #print([o.size() for o in loc])\n\n\n        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n\n        if self.phase == ""test"":\n            output = (\n                loc.view(loc.size(0), -1, 4),                   # loc preds\n                self.softmax(conf.view(-1, self.num_classes)),  # conf preds\n            )\n        else:\n            output = (\n                loc.view(loc.size(0), -1, 4),\n                conf.view(conf.size(0), -1, self.num_classes),\n            )\n        return output\n\n    def load_weights(self, base_file):\n        other, ext = os.path.splitext(base_file)\n        if ext == \'.pkl\' or \'.pth\':\n            print(\'Loading weights into state dict...\')\n            self.load_state_dict(torch.load(base_file))\n            print(\'Finished!\')\n        else:\n            print(\'Sorry only .pth and .pkl files supported.\')\n\n\n# This function is derived from torchvision VGG make_layers()\n# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\ndef vgg(cfg, i, batch_norm=False):\n    layers = []\n    in_channels = i\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        elif v == \'C\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n    layers += [pool5, conv6,\n               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n    return layers\n\nbase = {\n    \'300\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\',\n            512, 512, 512],\n    \'512\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'C\', 512, 512, 512, \'M\',\n            512, 512, 512],\n}\n\n\ndef add_extras(size, cfg, i, batch_norm=False):\n    # Extra layers added to VGG for feature scaling\n    layers = []\n    in_channels = i\n    flag = False\n    for k, v in enumerate(cfg):\n        if in_channels != \'S\':\n            if v == \'S\':\n                if in_channels == 256 and size == 512:\n                    layers += [BasicRFB(in_channels, cfg[k+1], stride=2, scale = 1.0, visual=1)]\n                else:\n                    layers += [BasicRFB(in_channels, cfg[k+1], stride=2, scale = 1.0, visual=2)]\n            else:\n                layers += [BasicRFB(in_channels, v, scale = 1.0, visual=2)]\n        in_channels = v\n    if size == 512:\n        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n        layers += [BasicConv(128,256,kernel_size=4,stride=1,padding=1)]\n    elif size ==300:\n        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n        layers += [BasicConv(128,256,kernel_size=3,stride=1)]\n        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n        layers += [BasicConv(128,256,kernel_size=3,stride=1)]\n    else:\n        print(""Error: Sorry only RFBNet300 and RFBNet512 are supported!"")\n        return\n    return layers\n\nextras = {\n    \'300\': [1024, \'S\', 512, \'S\', 256],\n    \'512\': [1024, \'S\', 512, \'S\', 256, \'S\', 256,\'S\',256],\n}\n\n\ndef multibox(size, vgg, extra_layers, cfg, num_classes):\n    loc_layers = []\n    conf_layers = []\n    vgg_source = [-2]\n    for k, v in enumerate(vgg_source):\n        if k == 0:\n            loc_layers += [nn.Conv2d(512,\n                                 cfg[k] * 4, kernel_size=3, padding=1)]\n            conf_layers +=[nn.Conv2d(512,\n                                 cfg[k] * num_classes, kernel_size=3, padding=1)]\n        else:\n            loc_layers += [nn.Conv2d(vgg[v].out_channels,\n                                 cfg[k] * 4, kernel_size=3, padding=1)]\n            conf_layers += [nn.Conv2d(vgg[v].out_channels,\n                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n    i = 1\n    indicator = 0\n    if size == 300:\n        indicator = 3\n    elif size == 512:\n        indicator = 5\n    else:\n        print(""Error: Sorry only RFBNet300 and RFBNet512 are supported!"")\n        return\n\n    for k, v in enumerate(extra_layers):\n        if k < indicator or k%2== 0:\n            loc_layers += [nn.Conv2d(v.out_channels, cfg[i]\n                                 * 4, kernel_size=3, padding=1)]\n            conf_layers += [nn.Conv2d(v.out_channels, cfg[i]\n                                  * num_classes, kernel_size=3, padding=1)]\n            i +=1\n    return vgg, extra_layers, (loc_layers, conf_layers)\n\nmbox = {\n    \'300\': [6, 6, 6, 6, 4, 4],  # number of boxes per feature map location\n    \'512\': [6, 6, 6, 6, 6, 4, 4],\n}\n\n\ndef build_net(phase, size=300, num_classes=21):\n    if phase != ""test"" and phase != ""train"":\n        print(""Error: Phase not recognized"")\n        return\n    if size != 300 and size != 512:\n        print(""Error: Sorry only RFBNet300 and RFBNet512 are supported!"")\n        return\n\n    return RFBNet(phase, size, *multibox(size, vgg(base[str(size)], 3),\n                                add_extras(size, extras[str(size)], 1024),\n                                mbox[str(size)], num_classes), num_classes)\n'"
models/__init__.py,0,b''
utils/__init__.py,0,b''
utils/box_utils.py,31,"b'import torch\nimport torch.nn as nn\nimport math\nimport numpy as np\nif torch.cuda.is_available():\n    import torch.backends.cudnn as cudnn\n\n\ndef point_form(boxes):\n    """""" Convert prior_boxes to (xmin, ymin, xmax, ymax)\n    representation for comparison to point form ground truth data.\n    Args:\n        boxes: (tensor) center-size default boxes from priorbox layers.\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, :2] - boxes[:, 2:]/2,     # xmin, ymin\n                     boxes[:, :2] + boxes[:, 2:]/2), 1)  # xmax, ymax\n\n\ndef center_size(boxes):\n    """""" Convert prior_boxes to (cx, cy, w, h)\n    representation for comparison to center-size form ground truth data.\n    Args:\n        boxes: (tensor) point_form boxes\n    Return:\n        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.\n    """"""\n    return torch.cat((boxes[:, 2:] + boxes[:, :2])/2,  # cx, cy\n                     boxes[:, 2:] - boxes[:, :2], 1)  # w, h\n\n\ndef intersect(box_a, box_b):\n    """""" We resize both tensors to [A,B,2] without new malloc:\n    [A,2] -> [A,1,2] -> [A,B,2]\n    [B,2] -> [1,B,2] -> [A,B,2]\n    Then we compute the area of intersect between box_a and box_b.\n    Args:\n      box_a: (tensor) bounding boxes, Shape: [A,4].\n      box_b: (tensor) bounding boxes, Shape: [B,4].\n    Return:\n      (tensor) intersection area, Shape: [A,B].\n    """"""\n    A = box_a.size(0)\n    B = box_b.size(0)\n    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))\n    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),\n                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))\n    inter = torch.clamp((max_xy - min_xy), min=0)\n    return inter[:, :, 0] * inter[:, :, 1]\n\n\ndef jaccard(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.  Here we operate on\n    ground truth boxes and default boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n    Return:\n        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2]-box_a[:, 0]) *\n              (box_a[:, 3]-box_a[:, 1])).unsqueeze(1).expand_as(inter)  # [A,B]\n    area_b = ((box_b[:, 2]-box_b[:, 0]) *\n              (box_b[:, 3]-box_b[:, 1])).unsqueeze(0).expand_as(inter)  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\ndef matrix_iou(a,b):\n    """"""\n    return iou of a and b, numpy version for data augenmentation\n    """"""\n    lt = np.maximum(a[:, np.newaxis, :2], b[:, :2])\n    rb = np.minimum(a[:, np.newaxis, 2:], b[:, 2:])\n\n    area_i = np.prod(rb - lt, axis=2) * (lt < rb).all(axis=2)\n    area_a = np.prod(a[:, 2:] - a[:, :2], axis=1)\n    area_b = np.prod(b[:, 2:] - b[:, :2], axis=1)\n    return area_i / (area_a[:, np.newaxis] + area_b - area_i)\n\n\ndef match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n    """"""Match each prior box with the ground truth box of the highest jaccard\n    overlap, encode the bounding boxes, then return the matched indices\n    corresponding to both confidence and location preds.\n    Args:\n        threshold: (float) The overlap threshold used when mathing boxes.\n        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n        variances: (tensor) Variances corresponding to each prior coord,\n            Shape: [num_priors, 4].\n        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n        idx: (int) current batch index\n    Return:\n        The matched indices corresponding to 1)location and 2)confidence preds.\n    """"""\n    # jaccard index\n    overlaps = jaccard(\n        truths,\n        point_form(priors)\n    )\n    # (Bipartite Matching)\n    # [1,num_objects] best prior for each ground truth\n    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n    # [1,num_priors] best ground truth for each prior\n    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n    best_truth_idx.squeeze_(0)\n    best_truth_overlap.squeeze_(0)\n    best_prior_idx.squeeze_(1)\n    best_prior_overlap.squeeze_(1)\n    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n    # TODO refactor: index  best_prior_idx with long tensor\n    # ensure every gt matches with its prior of max overlap\n    for j in range(best_prior_idx.size(0)):\n        best_truth_idx[best_prior_idx[j]] = j\n    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n    conf = labels[best_truth_idx]          # Shape: [num_priors]\n    conf[best_truth_overlap < threshold] = 0  # label as background\n    loc = encode(matches, priors, variances)\n    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n    conf_t[idx] = conf  # [num_priors] top class label for each prior\n\ndef encode(matched, priors, variances):\n    """"""Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    """"""\n\n    # dist b/t match center and prior\'s center\n    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n    # encode variance\n    g_cxcy /= (variances[0] * priors[:, 2:])\n    # match wh / prior wh\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n    # return target for smooth_l1_loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n\ndef encode_multi(matched, priors, offsets, variances):\n    """"""Encode the variances from the priorbox layers into the ground truth boxes\n    we have matched (based on jaccard overlap) with the prior boxes.\n    Args:\n        matched: (tensor) Coords of ground truth for each prior in point-form\n            Shape: [num_priors, 4].\n        priors: (tensor) Prior boxes in center-offset form\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        encoded boxes (tensor), Shape: [num_priors, 4]\n    """"""\n\n    # dist b/t match center and prior\'s center\n    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2] - offsets[:,:2]\n    # encode variance\n    #g_cxcy /= (variances[0] * priors[:, 2:])\n    g_cxcy.div_(variances[0] * offsets[:, 2:])\n    # match wh / prior wh\n    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n    g_wh = torch.log(g_wh) / variances[1]\n    # return target for smooth_l1_loss\n    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n\n# Adapted from https://github.com/Hakuyume/chainer-ssd\ndef decode(loc, priors, variances):\n    """"""Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    """"""\n\n    boxes = torch.cat((\n        priors[:, :2] + loc[:, :2] * variances[0] * priors[:, 2:],\n        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\ndef decode_multi(loc, priors, offsets, variances):\n    """"""Decode locations from predictions using priors to undo\n    the encoding we did for offset regression at train time.\n    Args:\n        loc (tensor): location predictions for loc layers,\n            Shape: [num_priors,4]\n        priors (tensor): Prior boxes in center-offset form.\n            Shape: [num_priors,4].\n        variances: (list[float]) Variances of priorboxes\n    Return:\n        decoded bounding box predictions\n    """"""\n\n    boxes = torch.cat((\n        priors[:, :2] + offsets[:,:2]+ loc[:, :2] * variances[0] * offsets[:, 2:],\n        priors[:, 2:] * torch.exp(loc[:, 2:] * variances[1])), 1)\n    boxes[:, :2] -= boxes[:, 2:] / 2\n    boxes[:, 2:] += boxes[:, :2]\n    return boxes\n\ndef log_sum_exp(x):\n    """"""Utility function for computing log_sum_exp while determining\n    This will be used to determine unaveraged confidence loss across\n    all examples in a batch.\n    Args:\n        x (Variable(tensor)): conf_preds from conf layers\n    """"""\n    x_max = x.data.max()\n    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n\n\n# Original author: Francisco Massa:\n# https://github.com/fmassa/object-detection.torch\n# Ported to PyTorch by Max deGroot (02/01/2017)\ndef nms(boxes, scores, overlap=0.5, top_k=200):\n    """"""Apply non-maximum suppression at test time to avoid detecting too many\n    overlapping bounding boxes for a given object.\n    Args:\n        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n        top_k: (int) The Maximum number of box preds to consider.\n    Return:\n        The indices of the kept boxes with respect to num_priors.\n    """"""\n\n    keep = torch.Tensor(scores.size(0)).fill_(0).long()\n    if boxes.numel() == 0:\n        return keep\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    area = torch.mul(x2 - x1, y2 - y1)\n    v, idx = scores.sort(0)  # sort in ascending order\n    # I = I[v >= 0.01]\n    idx = idx[-top_k:]  # indices of the top-k largest vals\n    xx1 = boxes.new()\n    yy1 = boxes.new()\n    xx2 = boxes.new()\n    yy2 = boxes.new()\n    w = boxes.new()\n    h = boxes.new()\n\n    # keep = torch.Tensor()\n    count = 0\n    while idx.numel() > 0:\n        i = idx[-1]  # index of current largest val\n        # keep.append(i)\n        keep[count] = i\n        count += 1\n        if idx.size(0) == 1:\n            break\n        idx = idx[:-1]  # remove kept element from view\n        # load bboxes of next highest vals\n        torch.index_select(x1, 0, idx, out=xx1)\n        torch.index_select(y1, 0, idx, out=yy1)\n        torch.index_select(x2, 0, idx, out=xx2)\n        torch.index_select(y2, 0, idx, out=yy2)\n        # store element-wise max with next highest score\n        xx1 = torch.clamp(xx1, min=x1[i])\n        yy1 = torch.clamp(yy1, min=y1[i])\n        xx2 = torch.clamp(xx2, max=x2[i])\n        yy2 = torch.clamp(yy2, max=y2[i])\n        w.resize_as_(xx2)\n        h.resize_as_(yy2)\n        w = xx2 - xx1\n        h = yy2 - yy1\n        # check sizes of xx1 and xx2.. after each iteration\n        w = torch.clamp(w, min=0.0)\n        h = torch.clamp(h, min=0.0)\n        inter = w*h\n        # IoU = i / (area(a) + area(b) - i)\n        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n        union = (rem_areas - inter) + area[i]\n        IoU = inter/union  # store result in iou\n        # keep only elements with an IoU <= overlap\n        idx = idx[IoU.le(overlap)]\n    return keep, count\n\n\n'"
utils/build.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nimport numpy as np\nfrom distutils.core import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # adapted fom http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                                   \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\': home, \'nvcc\': nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\n\n\nCUDA = locate_cuda()\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        print(extra_postargs)\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    Extension(\n        ""nms.cpu_nms"",\n        [""nms/cpu_nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs=[numpy_include]\n    ),\n    Extension(\'nms.gpu_nms\',\n              [\'nms/nms_kernel.cu\', \'nms/gpu_nms.pyx\'],\n              library_dirs=[CUDA[\'lib64\']],\n              libraries=[\'cudart\'],\n              language=\'c++\',\n              runtime_library_dirs=[CUDA[\'lib64\']],\n              # this syntax is specific to this build system\n              # we\'re only going to use certain compiler args with nvcc and not with gcc\n              # the implementation of this trick is in customize_compiler() below\n              extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                                  \'nvcc\': [\'-arch=sm_52\',\n                                           \'--ptxas-options=-v\',\n                                           \'-c\',\n                                           \'--compiler-options\',\n                                           ""\'-fPIC\'""]},\n              include_dirs=[numpy_include, CUDA[\'include\']]\n              ),\n    Extension(\n        \'pycocotools._mask\',\n        sources=[\'pycocotools/maskApi.c\', \'pycocotools/_mask.pyx\'],\n        include_dirs=[numpy_include, \'pycocotools\'],\n        extra_compile_args={\n            \'gcc\': [\'-Wno-cpp\', \'-Wno-unused-function\', \'-std=c99\']},\n    ),\n]\n\nsetup(\n    name=\'mot_utils\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
utils/nms_wrapper.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nfrom .nms.cpu_nms import cpu_nms, cpu_soft_nms\nfrom .nms.gpu_nms import gpu_nms\n\n\n# def nms(dets, thresh, force_cpu=False):\n#     """"""Dispatch to either CPU or GPU NMS implementations.""""""\n#\n#     if dets.shape[0] == 0:\n#         return []\n#     if cfg.USE_GPU_NMS and not force_cpu:\n#         return gpu_nms(dets, thresh, device_id=cfg.GPU_ID)\n#     else:\n#         return cpu_nms(dets, thresh)\n\n\ndef nms(dets, thresh, force_cpu=False):\n    """"""Dispatch to either CPU or GPU NMS implementations.""""""\n\n    if dets.shape[0] == 0:\n        return []\n    if force_cpu:\n        #return cpu_soft_nms(dets, thresh, method = 0)\n        return cpu_nms(dets, thresh)\n    return gpu_nms(dets, thresh)\n'"
utils/timer.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport time\n\n\nclass Timer(object):\n    """"""A simple timer.""""""\n    def __init__(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n\n    def tic(self):\n        # using time.time instead of time.clock because time time.clock\n        # does not normalize for multithreading\n        self.start_time = time.time()\n\n    def toc(self, average=True):\n        self.diff = time.time() - self.start_time\n        self.total_time += self.diff\n        self.calls += 1\n        self.average_time = self.total_time / self.calls\n        if average:\n            return self.average_time\n        else:\n            return self.diff\n\n    def clear(self):\n        self.total_time = 0.\n        self.calls = 0\n        self.start_time = 0.\n        self.diff = 0.\n        self.average_time = 0.\n'"
layers/functions/__init__.py,0,"b""from .detection import Detect\nfrom .prior_box import PriorBox\n\n\n__all__ = ['Detect', 'PriorBox']\n"""
layers/functions/detection.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Function\nfrom torch.autograd import Variable\nfrom utils.box_utils import decode\n\n\nclass Detect(Function):\n    """"""At test time, Detect is the final layer of SSD.  Decode location preds,\n    apply non-maximum suppression to location predictions based on conf\n    scores and threshold to a top_k number of output predictions for both\n    confidence score and locations.\n    """"""\n    def __init__(self, num_classes, bkg_label, cfg):\n        self.num_classes = num_classes\n        self.background_label = bkg_label\n\n        self.variance = cfg[\'variance\']\n\n    def forward(self, predictions, prior):\n        """"""\n        Args:\n            loc_data: (tensor) Loc preds from loc layers\n                Shape: [batch,num_priors*4]\n            conf_data: (tensor) Shape: Conf preds from conf layers\n                Shape: [batch*num_priors,num_classes]\n            prior_data: (tensor) Prior boxes and variances from priorbox layers\n                Shape: [1,num_priors,4]\n        """"""\n\n        loc, conf = predictions\n\n        loc_data = loc.data\n        conf_data = conf.data\n        prior_data = prior.data\n        num = loc_data.size(0)  # batch size\n        self.num_priors = prior_data.size(0)\n        self.boxes = torch.zeros(1, self.num_priors, 4)\n        self.scores = torch.zeros(1, self.num_priors, self.num_classes)\n        if loc_data.is_cuda:\n            self.boxes = self.boxes.cuda()\n            self.scores = self.scores.cuda()\n\n        if num == 1:\n            # size batch x num_classes x num_priors\n            conf_preds = conf_data.unsqueeze(0)\n\n        else:\n            conf_preds = conf_data.view(num, num_priors,\n                                        self.num_classes)\n            self.boxes.expand_(num, self.num_priors, 4)\n            self.scores.expand_(num, self.num_priors, self.num_classes)\n\n        # Decode predictions into bboxes.\n        for i in range(num):\n            decoded_boxes = decode(loc_data[i], prior_data, self.variance)\n            conf_scores = conf_preds[i].clone()\n\n            self.boxes[i] = decoded_boxes\n            self.scores[i] = conf_scores\n\n        return self.boxes, self.scores\n'"
layers/functions/prior_box.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nfrom math import sqrt as sqrt\nfrom itertools import product as product\n\n\nclass PriorBox(object):\n    """"""Compute priorbox coordinates in center-offset form for each source\n    feature map.\n    Note:\n    This \'layer\' has changed between versions of the original SSD\n    paper, so we include both versions, but note v2 is the most tested and most\n    recent version of the paper.\n\n    """"""\n    def __init__(self, cfg):\n        super(PriorBox, self).__init__()\n        self.image_size = cfg[\'min_dim\']\n        # number of priors for feature map location (either 4 or 6)\n        self.num_priors = len(cfg[\'aspect_ratios\'])\n        self.variance = cfg[\'variance\'] or [0.1]\n        self.feature_maps = cfg[\'feature_maps\']\n        self.min_sizes = cfg[\'min_sizes\']\n        self.max_sizes = cfg[\'max_sizes\']\n        self.steps = cfg[\'steps\']\n        self.aspect_ratios = cfg[\'aspect_ratios\']\n        self.clip = cfg[\'clip\']\n        for v in self.variance:\n            if v <= 0:\n                raise ValueError(\'Variances must be greater than 0\')\n\n    def forward(self):\n        mean = []\n        for k, f in enumerate(self.feature_maps):\n            for i, j in product(range(f), repeat=2):\n                f_k = self.image_size / self.steps[k]\n                cx = (j + 0.5) / f_k\n                cy = (i + 0.5) / f_k\n\n                s_k = self.min_sizes[k]/self.image_size\n                mean += [cx, cy, s_k, s_k]\n\n                # aspect_ratio: 1\n                # rel size: sqrt(s_k * s_(k+1))\n                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n                mean += [cx, cy, s_k_prime, s_k_prime]\n\n                # rest of aspect ratios\n                for ar in self.aspect_ratios[k]:\n                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n\n        # back to torch land\n        output = torch.Tensor(mean).view(-1, 4)\n        if self.clip:\n            output.clamp_(max=1, min=0)\n        return output\n'"
layers/modules/__init__.py,0,"b""from .multibox_loss import MultiBoxLoss\n\n__all__ = ['MultiBoxLoss']\n"""
layers/modules/multibox_loss.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom utils.box_utils import match, log_sum_exp\nGPU = False\nif torch.cuda.is_available():\n    GPU = True\n\n\nclass MultiBoxLoss(nn.Module):\n    """"""SSD Weighted Loss Function\n    Compute Targets:\n        1) Produce Confidence Target Indices by matching  ground truth boxes\n           with (default) \'priorboxes\' that have jaccard index > threshold parameter\n           (default threshold: 0.5).\n        2) Produce localization target by \'encoding\' variance into offsets of ground\n           truth boxes and their matched  \'priorboxes\'.\n        3) Hard negative mining to filter the excessive number of negative examples\n           that comes with using a large number of default bounding boxes.\n           (default negative:positive ratio 3:1)\n    Objective Loss:\n        L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n        weighted by \xce\xb1 which is set to 1 by cross val.\n        Args:\n            c: class confidences,\n            l: predicted boxes,\n            g: ground truth boxes\n            N: number of matched default boxes\n        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n    """"""\n\n\n    def __init__(self, num_classes,overlap_thresh,prior_for_matching,bkg_label,neg_mining,neg_pos,neg_overlap,encode_target):\n        super(MultiBoxLoss, self).__init__()\n        self.num_classes = num_classes\n        self.threshold = overlap_thresh\n        self.background_label = bkg_label\n        self.encode_target = encode_target\n        self.use_prior_for_matching  = prior_for_matching\n        self.do_neg_mining = neg_mining\n        self.negpos_ratio = neg_pos\n        self.neg_overlap = neg_overlap\n        self.variance = [0.1,0.2]\n\n    def forward(self, predictions, priors, targets):\n        """"""Multibox Loss\n        Args:\n            predictions (tuple): A tuple containing loc preds, conf preds,\n            and prior boxes from SSD net.\n                conf shape: torch.size(batch_size,num_priors,num_classes)\n                loc shape: torch.size(batch_size,num_priors,4)\n                priors shape: torch.size(num_priors,4)\n\n            ground_truth (tensor): Ground truth boxes and labels for a batch,\n                shape: [batch_size,num_objs,5] (last idx is the label).\n        """"""\n\n        loc_data, conf_data = predictions\n        priors = priors\n        num = loc_data.size(0)\n        num_priors = (priors.size(0))\n        num_classes = self.num_classes\n\n        # match priors (default boxes) and ground truth boxes\n        loc_t = torch.Tensor(num, num_priors, 4)\n        conf_t = torch.LongTensor(num, num_priors)\n        for idx in range(num):\n            truths = targets[idx][:,:-1].data\n            labels = targets[idx][:,-1].data\n            defaults = priors.data\n            match(self.threshold,truths,defaults,self.variance,labels,loc_t,conf_t,idx)\n        if GPU:\n            loc_t = loc_t.cuda()\n            conf_t = conf_t.cuda()\n        # wrap targets\n        loc_t = Variable(loc_t, requires_grad=False)\n        conf_t = Variable(conf_t,requires_grad=False)\n\n        pos = conf_t > 0\n\n        # Localization Loss (Smooth L1)\n        # Shape: [batch,num_priors,4]\n        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n        loc_p = loc_data[pos_idx].view(-1,4)\n        loc_t = loc_t[pos_idx].view(-1,4)\n        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n\n        # Compute max conf across batch for hard negative mining\n        batch_conf = conf_data.view(-1,self.num_classes)\n        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1,1))\n\n        # Hard Negative Mining\n        loss_c[pos.view(-1,1)] = 0 # filter out pos boxes for now\n        loss_c = loss_c.view(num, -1)\n        _,loss_idx = loss_c.sort(1, descending=True)\n        _,idx_rank = loss_idx.sort(1)\n        num_pos = pos.long().sum(1,keepdim=True)\n        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n        neg = idx_rank < num_neg.expand_as(idx_rank)\n\n        # Confidence Loss Including Positive and Negative Examples\n        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1,self.num_classes)\n        targets_weighted = conf_t[(pos+neg).gt(0)]\n        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)\n\n        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + \xce\xb1Lloc(x,l,g)) / N\n\n        N = max(num_pos.data.sum().float(), 1)\n        loss_l/=N\n        loss_c/=N\n        return loss_l,loss_c\n'"
utils/nms/__init__.py,0,b''
utils/nms/py_cpu_nms.py,0,"b'# --------------------------------------------------------\n# Fast R-CNN\n# Copyright (c) 2015 Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Written by Ross Girshick\n# --------------------------------------------------------\n\nimport numpy as np\n\ndef py_cpu_nms(dets, thresh):\n    """"""Pure Python NMS baseline.""""""\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n'"
utils/pycocotools/__init__.py,0,"b""__author__ = 'tylin'\n"""
utils/pycocotools/coco.py,0,"b'__author__ = \'tylin\'\n__version__ = \'2.0\'\n# Interface for accessing the Microsoft COCO dataset.\n\n# Microsoft COCO is a large image dataset designed for object detection,\n# segmentation, and caption generation. pycocotools is a Python API that\n# assists in loading, parsing and visualizing the annotations in COCO.\n# Please visit http://mscoco.org/ for more information on COCO, including\n# for the data, paper, and tutorials. The exact format of the annotations\n# is also described on the COCO website. For example usage of the pycocotools\n# please see pycocotools_demo.ipynb. In addition to this API, please download both\n# the COCO images and annotations in order to run the demo.\n\n# An alternative to using the API is to load the annotations directly\n# into Python dictionary\n# Using the API provides additional utility functions. Note that this API\n# supports both *instance* and *caption* annotations. In the case of\n# captions not all functions are defined (e.g. categories are undefined).\n\n# The following API functions are defined:\n#  COCO       - COCO api class that loads COCO annotation file and prepare data structures.\n#  decodeMask - Decode binary mask M encoded via run-length encoding.\n#  encodeMask - Encode binary mask M using run-length encoding.\n#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n#  getCatIds  - Get cat ids that satisfy given filter conditions.\n#  getImgIds  - Get img ids that satisfy given filter conditions.\n#  loadAnns   - Load anns with the specified ids.\n#  loadCats   - Load cats with the specified ids.\n#  loadImgs   - Load imgs with the specified ids.\n#  annToMask  - Convert segmentation in an annotation to binary mask.\n#  showAnns   - Display the specified annotations.\n#  loadRes    - Load algorithm results and create API for accessing them.\n#  download   - Download COCO images from mscoco.org server.\n# Throughout the API ""ann""=annotation, ""cat""=category, and ""img""=image.\n# Help on each functions can be accessed by: ""help COCO>function"".\n\n# See also COCO>decodeMask,\n# COCO>encodeMask, COCO>getAnnIds, COCO>getCatIds,\n# COCO>getImgIds, COCO>loadAnns, COCO>loadCats,\n# COCO>loadImgs, COCO>annToMask, COCO>showAnns\n\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2014.\n# Licensed under the Simplified BSD License [see bsd.txt]\n\nimport json\nimport time\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import PatchCollection\nfrom matplotlib.patches import Polygon\nimport numpy as np\nimport copy\nimport itertools\nfrom . import mask as maskUtils\nimport os\nfrom collections import defaultdict\nimport sys\nPYTHON_VERSION = sys.version_info[0]\nif PYTHON_VERSION == 2:\n    from urllib import urlretrieve\nelif PYTHON_VERSION == 3:\n    from urllib.request import urlretrieve\n\nclass COCO:\n    def __init__(self, annotation_file=None):\n        """"""\n        Constructor of Microsoft COCO helper class for reading and visualizing annotations.\n        :param annotation_file (str): location of annotation file\n        :param image_folder (str): location to the folder that hosts images.\n        :return:\n        """"""\n        # load dataset\n        self.dataset,self.anns,self.cats,self.imgs = dict(),dict(),dict(),dict()\n        self.imgToAnns, self.catToImgs = defaultdict(list), defaultdict(list)\n        if not annotation_file == None:\n            print(\'loading annotations into memory...\')\n            tic = time.time()\n            dataset = json.load(open(annotation_file, \'r\'))\n            assert type(dataset)==dict, \'annotation file format {} not supported\'.format(type(dataset))\n            print(\'Done (t={:0.2f}s)\'.format(time.time()- tic))\n            self.dataset = dataset\n            self.createIndex()\n\n    def createIndex(self):\n        # create index\n        print(\'creating index...\')\n        anns, cats, imgs = {}, {}, {}\n        imgToAnns,catToImgs = defaultdict(list),defaultdict(list)\n        if \'annotations\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                imgToAnns[ann[\'image_id\']].append(ann)\n                anns[ann[\'id\']] = ann\n\n        if \'images\' in self.dataset:\n            for img in self.dataset[\'images\']:\n                imgs[img[\'id\']] = img\n\n        if \'categories\' in self.dataset:\n            for cat in self.dataset[\'categories\']:\n                cats[cat[\'id\']] = cat\n\n        if \'annotations\' in self.dataset and \'categories\' in self.dataset:\n            for ann in self.dataset[\'annotations\']:\n                catToImgs[ann[\'category_id\']].append(ann[\'image_id\'])\n\n        print(\'index created!\')\n\n        # create class members\n        self.anns = anns\n        self.imgToAnns = imgToAnns\n        self.catToImgs = catToImgs\n        self.imgs = imgs\n        self.cats = cats\n\n    def info(self):\n        """"""\n        Print information about the annotation file.\n        :return:\n        """"""\n        for key, value in self.dataset[\'info\'].items():\n            print(\'{}: {}\'.format(key, value))\n\n    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[], iscrowd=None):\n        """"""\n        Get ann ids that satisfy given filter conditions. default skips that filter\n        :param imgIds  (int array)     : get anns for given imgs\n               catIds  (int array)     : get anns for given cats\n               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n               iscrowd (boolean)       : get anns for given crowd label (False or True)\n        :return: ids (int array)       : integer array of ann ids\n        """"""\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n            anns = self.dataset[\'annotations\']\n        else:\n            if not len(imgIds) == 0:\n                lists = [self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns]\n                anns = list(itertools.chain.from_iterable(lists))\n            else:\n                anns = self.dataset[\'annotations\']\n            anns = anns if len(catIds)  == 0 else [ann for ann in anns if ann[\'category_id\'] in catIds]\n            anns = anns if len(areaRng) == 0 else [ann for ann in anns if ann[\'area\'] > areaRng[0] and ann[\'area\'] < areaRng[1]]\n        if not iscrowd == None:\n            ids = [ann[\'id\'] for ann in anns if ann[\'iscrowd\'] == iscrowd]\n        else:\n            ids = [ann[\'id\'] for ann in anns]\n        return ids\n\n    def getCatIds(self, catNms=[], supNms=[], catIds=[]):\n        """"""\n        filtering parameters. default skips that filter.\n        :param catNms (str array)  : get cats for given cat names\n        :param supNms (str array)  : get cats for given supercategory names\n        :param catIds (int array)  : get cats for given cat ids\n        :return: ids (int array)   : integer array of cat ids\n        """"""\n        catNms = catNms if type(catNms) == list else [catNms]\n        supNms = supNms if type(supNms) == list else [supNms]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(catNms) == len(supNms) == len(catIds) == 0:\n            cats = self.dataset[\'categories\']\n        else:\n            cats = self.dataset[\'categories\']\n            cats = cats if len(catNms) == 0 else [cat for cat in cats if cat[\'name\']          in catNms]\n            cats = cats if len(supNms) == 0 else [cat for cat in cats if cat[\'supercategory\'] in supNms]\n            cats = cats if len(catIds) == 0 else [cat for cat in cats if cat[\'id\']            in catIds]\n        ids = [cat[\'id\'] for cat in cats]\n        return ids\n\n    def getImgIds(self, imgIds=[], catIds=[]):\n        \'\'\'\n        Get img ids that satisfy given filter conditions.\n        :param imgIds (int array) : get imgs for given ids\n        :param catIds (int array) : get imgs with all given cats\n        :return: ids (int array)  : integer array of img ids\n        \'\'\'\n        imgIds = imgIds if type(imgIds) == list else [imgIds]\n        catIds = catIds if type(catIds) == list else [catIds]\n\n        if len(imgIds) == len(catIds) == 0:\n            ids = self.imgs.keys()\n        else:\n            ids = set(imgIds)\n            for i, catId in enumerate(catIds):\n                if i == 0 and len(ids) == 0:\n                    ids = set(self.catToImgs[catId])\n                else:\n                    ids &= set(self.catToImgs[catId])\n        return list(ids)\n\n    def loadAnns(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying anns\n        :return: anns (object array) : loaded ann objects\n        """"""\n        if type(ids) == list:\n            return [self.anns[id] for id in ids]\n        elif type(ids) == int:\n            return [self.anns[ids]]\n\n    def loadCats(self, ids=[]):\n        """"""\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        """"""\n        if type(ids) == list:\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]\n\n    def loadImgs(self, ids=[]):\n        """"""\n        Load anns with the specified ids.\n        :param ids (int array)       : integer ids specifying img\n        :return: imgs (object array) : loaded img objects\n        """"""\n        if type(ids) == list:\n            return [self.imgs[id] for id in ids]\n        elif type(ids) == int:\n            return [self.imgs[ids]]\n\n    def showAnns(self, anns):\n        """"""\n        Display the specified annotations.\n        :param anns (array of object): annotations to display\n        :return: None\n        """"""\n        if len(anns) == 0:\n            return 0\n        if \'segmentation\' in anns[0] or \'keypoints\' in anns[0]:\n            datasetType = \'instances\'\n        elif \'caption\' in anns[0]:\n            datasetType = \'captions\'\n        else:\n            raise Exception(\'datasetType not supported\')\n        if datasetType == \'instances\':\n            ax = plt.gca()\n            ax.set_autoscale_on(False)\n            polygons = []\n            color = []\n            for ann in anns:\n                c = (np.random.random((1, 3))*0.6+0.4).tolist()[0]\n                if \'segmentation\' in ann:\n                    if type(ann[\'segmentation\']) == list:\n                        # polygon\n                        for seg in ann[\'segmentation\']:\n                            poly = np.array(seg).reshape((int(len(seg)/2), 2))\n                            polygons.append(Polygon(poly))\n                            color.append(c)\n                    else:\n                        # mask\n                        t = self.imgs[ann[\'image_id\']]\n                        if type(ann[\'segmentation\'][\'counts\']) == list:\n                            rle = maskUtils.frPyObjects([ann[\'segmentation\']], t[\'height\'], t[\'width\'])\n                        else:\n                            rle = [ann[\'segmentation\']]\n                        m = maskUtils.decode(rle)\n                        img = np.ones( (m.shape[0], m.shape[1], 3) )\n                        if ann[\'iscrowd\'] == 1:\n                            color_mask = np.array([2.0,166.0,101.0])/255\n                        if ann[\'iscrowd\'] == 0:\n                            color_mask = np.random.random((1, 3)).tolist()[0]\n                        for i in range(3):\n                            img[:,:,i] = color_mask[i]\n                        ax.imshow(np.dstack( (img, m*0.5) ))\n                if \'keypoints\' in ann and type(ann[\'keypoints\']) == list:\n                    # turn skeleton into zero-based index\n                    sks = np.array(self.loadCats(ann[\'category_id\'])[0][\'skeleton\'])-1\n                    kp = np.array(ann[\'keypoints\'])\n                    x = kp[0::3]\n                    y = kp[1::3]\n                    v = kp[2::3]\n                    for sk in sks:\n                        if np.all(v[sk]>0):\n                            plt.plot(x[sk],y[sk], linewidth=3, color=c)\n                    plt.plot(x[v>0], y[v>0],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=\'k\',markeredgewidth=2)\n                    plt.plot(x[v>1], y[v>1],\'o\',markersize=8, markerfacecolor=c, markeredgecolor=c, markeredgewidth=2)\n            p = PatchCollection(polygons, facecolor=color, linewidths=0, alpha=0.4)\n            ax.add_collection(p)\n            p = PatchCollection(polygons, facecolor=\'none\', edgecolors=color, linewidths=2)\n            ax.add_collection(p)\n        elif datasetType == \'captions\':\n            for ann in anns:\n                print(ann[\'caption\'])\n\n    def loadRes(self, resFile):\n        """"""\n        Load result file and return a result api object.\n        :param   resFile (str)     : file name of result file\n        :return: res (obj)         : result api object\n        """"""\n        res = COCO()\n        res.dataset[\'images\'] = [img for img in self.dataset[\'images\']]\n\n        print(\'Loading and preparing results...\')\n        tic = time.time()\n        if type(resFile) == str or type(resFile) == unicode:\n            anns = json.load(open(resFile))\n        elif type(resFile) == np.ndarray:\n            anns = self.loadNumpyAnnotations(resFile)\n        else:\n            anns = resFile\n        assert type(anns) == list, \'results in not an array of objects\'\n        annsImgIds = [ann[\'image_id\'] for ann in anns]\n        assert set(annsImgIds) == (set(annsImgIds) & set(self.getImgIds())), \\\n               \'Results do not correspond to current coco set\'\n        if \'caption\' in anns[0]:\n            imgIds = set([img[\'id\'] for img in res.dataset[\'images\']]) & set([ann[\'image_id\'] for ann in anns])\n            res.dataset[\'images\'] = [img for img in res.dataset[\'images\'] if img[\'id\'] in imgIds]\n            for id, ann in enumerate(anns):\n                ann[\'id\'] = id+1\n        elif \'bbox\' in anns[0] and not anns[0][\'bbox\'] == []:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                bb = ann[\'bbox\']\n                x1, x2, y1, y2 = [bb[0], bb[0]+bb[2], bb[1], bb[1]+bb[3]]\n                if not \'segmentation\' in ann:\n                    ann[\'segmentation\'] = [[x1, y1, x1, y2, x2, y2, x2, y1]]\n                ann[\'area\'] = bb[2]*bb[3]\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'segmentation\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                # now only support compressed RLE format as segmentation results\n                ann[\'area\'] = maskUtils.area(ann[\'segmentation\'])\n                if not \'bbox\' in ann:\n                    ann[\'bbox\'] = maskUtils.toBbox(ann[\'segmentation\'])\n                ann[\'id\'] = id+1\n                ann[\'iscrowd\'] = 0\n        elif \'keypoints\' in anns[0]:\n            res.dataset[\'categories\'] = copy.deepcopy(self.dataset[\'categories\'])\n            for id, ann in enumerate(anns):\n                s = ann[\'keypoints\']\n                x = s[0::3]\n                y = s[1::3]\n                x0,x1,y0,y1 = np.min(x), np.max(x), np.min(y), np.max(y)\n                ann[\'area\'] = (x1-x0)*(y1-y0)\n                ann[\'id\'] = id + 1\n                ann[\'bbox\'] = [x0,y0,x1-x0,y1-y0]\n        print(\'DONE (t={:0.2f}s)\'.format(time.time()- tic))\n\n        res.dataset[\'annotations\'] = anns\n        res.createIndex()\n        return res\n\n    def download(self, tarDir = None, imgIds = [] ):\n        \'\'\'\n        Download COCO images from mscoco.org server.\n        :param tarDir (str): COCO results directory name\n               imgIds (list): images to be downloaded\n        :return:\n        \'\'\'\n        if tarDir is None:\n            print(\'Please specify target directory\')\n            return -1\n        if len(imgIds) == 0:\n            imgs = self.imgs.values()\n        else:\n            imgs = self.loadImgs(imgIds)\n        N = len(imgs)\n        if not os.path.exists(tarDir):\n            os.makedirs(tarDir)\n        for i, img in enumerate(imgs):\n            tic = time.time()\n            fname = os.path.join(tarDir, img[\'file_name\'])\n            if not os.path.exists(fname):\n                urlretrieve(img[\'coco_url\'], fname)\n            print(\'downloaded {}/{} images (t={:0.1f}s)\'.format(i, N, time.time()- tic))\n\n    def loadNumpyAnnotations(self, data):\n        """"""\n        Convert result data from a numpy array [Nx7] where each row contains {imageID,x1,y1,w,h,score,class}\n        :param  data (numpy.ndarray)\n        :return: annotations (python nested list)\n        """"""\n        print(\'Converting ndarray to lists...\')\n        assert(type(data) == np.ndarray)\n        print(data.shape)\n        assert(data.shape[1] == 7)\n        N = data.shape[0]\n        ann = []\n        for i in range(N):\n            if i % 1000000 == 0:\n                print(\'{}/{}\'.format(i,N))\n            ann += [{\n                \'image_id\'  : int(data[i, 0]),\n                \'bbox\'  : [ data[i, 1], data[i, 2], data[i, 3], data[i, 4] ],\n                \'score\' : data[i, 5],\n                \'category_id\': int(data[i, 6]),\n                }]\n        return ann\n\n    def annToRLE(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE to RLE.\n        :return: binary mask (numpy 2D array)\n        """"""\n        t = self.imgs[ann[\'image_id\']]\n        h, w = t[\'height\'], t[\'width\']\n        segm = ann[\'segmentation\']\n        if type(segm) == list:\n            # polygon -- a single object might consist of multiple parts\n            # we merge all parts into one mask rle code\n            rles = maskUtils.frPyObjects(segm, h, w)\n            rle = maskUtils.merge(rles)\n        elif type(segm[\'counts\']) == list:\n            # uncompressed RLE\n            rle = maskUtils.frPyObjects(segm, h, w)\n        else:\n            # rle\n            rle = ann[\'segmentation\']\n        return rle\n\n    def annToMask(self, ann):\n        """"""\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        """"""\n        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m'"
utils/pycocotools/cocoeval.py,0,"b'__author__ = \'tsungyi\'\n\nimport numpy as np\nimport datetime\nimport time\nfrom collections import defaultdict\nfrom . import mask as maskUtils\nimport copy\n\nclass COCOeval:\n    # Interface for evaluating detection on the Microsoft COCO dataset.\n    #\n    # The usage for CocoEval is as follows:\n    #  cocoGt=..., cocoDt=...       # load dataset and results\n    #  E = CocoEval(cocoGt,cocoDt); # initialize CocoEval object\n    #  E.params.recThrs = ...;      # set parameters as desired\n    #  E.evaluate();                # run per image evaluation\n    #  E.accumulate();              # accumulate per image results\n    #  E.summarize();               # display summary metrics of results\n    # For example usage see evalDemo.m and http://mscoco.org/.\n    #\n    # The evaluation parameters are as follows (defaults in brackets):\n    #  imgIds     - [all] N img ids to use for evaluation\n    #  catIds     - [all] K cat ids to use for evaluation\n    #  iouThrs    - [.5:.05:.95] T=10 IoU thresholds for evaluation\n    #  recThrs    - [0:.01:1] R=101 recall thresholds for evaluation\n    #  areaRng    - [...] A=4 object area ranges for evaluation\n    #  maxDets    - [1 10 100] M=3 thresholds on max detections per image\n    #  iouType    - [\'segm\'] set iouType to \'segm\', \'bbox\' or \'keypoints\'\n    #  iouType replaced the now DEPRECATED useSegm parameter.\n    #  useCats    - [1] if true use category labels for evaluation\n    # Note: if useCats=0 category labels are ignored as in proposal scoring.\n    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.\n    #\n    # evaluate(): evaluates detections on every image and every category and\n    # concats the results into the ""evalImgs"" with fields:\n    #  dtIds      - [1xD] id for each of the D detections (dt)\n    #  gtIds      - [1xG] id for each of the G ground truths (gt)\n    #  dtMatches  - [TxD] matching gt id at each IoU or 0\n    #  gtMatches  - [TxG] matching dt id at each IoU or 0\n    #  dtScores   - [1xD] confidence of each dt\n    #  gtIgnore   - [1xG] ignore flag for each gt\n    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU\n    #\n    # accumulate(): accumulates the per-image, per-category evaluation\n    # results in ""evalImgs"" into the dictionary ""eval"" with fields:\n    #  params     - parameters used for evaluation\n    #  date       - date evaluation was performed\n    #  counts     - [T,R,K,A,M] parameter dimensions (see above)\n    #  precision  - [TxRxKxAxM] precision for every evaluation setting\n    #  recall     - [TxKxAxM] max recall for every evaluation setting\n    # Note: precision and recall==-1 for settings with no gt objects.\n    #\n    # See also coco, mask, pycocoDemo, pycocoEvalDemo\n    #\n    # Microsoft COCO Toolbox.      version 2.0\n    # Data, paper, and tutorials available at:  http://mscoco.org/\n    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n    # Licensed under the Simplified BSD License [see coco/license.txt]\n    def __init__(self, cocoGt=None, cocoDt=None, iouType=\'segm\'):\n        \'\'\'\n        Initialize CocoEval using coco APIs for gt and dt\n        :param cocoGt: coco object with ground truth annotations\n        :param cocoDt: coco object with detection results\n        :return: None\n        \'\'\'\n        if not iouType:\n            print(\'iouType not specified. use default iouType segm\')\n        self.cocoGt   = cocoGt              # ground truth COCO API\n        self.cocoDt   = cocoDt              # detections COCO API\n        self.params   = {}                  # evaluation parameters\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements\n        self.eval     = {}                  # accumulated evaluation results\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        self.params = Params(iouType=iouType) # parameters\n        self._paramsEval = {}               # parameters for evaluation\n        self.stats = []                     # result summarization\n        self.ious = {}                      # ious between all gts and dts\n        if not cocoGt is None:\n            self.params.imgIds = sorted(cocoGt.getImgIds())\n            self.params.catIds = sorted(cocoGt.getCatIds())\n\n\n    def _prepare(self):\n        \'\'\'\n        Prepare ._gts and ._dts for evaluation based on params\n        :return: None\n        \'\'\'\n        def _toMask(anns, coco):\n            # modify ann[\'segmentation\'] by reference\n            for ann in anns:\n                rle = coco.annToRLE(ann)\n                ann[\'segmentation\'] = rle\n        p = self.params\n        if p.useCats:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds, catIds=p.catIds))\n        else:\n            gts=self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds=p.imgIds))\n            dts=self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds=p.imgIds))\n\n        # convert ground truth to mask if iouType == \'segm\'\n        if p.iouType == \'segm\':\n            _toMask(gts, self.cocoGt)\n            _toMask(dts, self.cocoDt)\n        # set ignore flag\n        for gt in gts:\n            gt[\'ignore\'] = gt[\'ignore\'] if \'ignore\' in gt else 0\n            gt[\'ignore\'] = \'iscrowd\' in gt and gt[\'iscrowd\']\n            if p.iouType == \'keypoints\':\n                gt[\'ignore\'] = (gt[\'num_keypoints\'] == 0) or gt[\'ignore\']\n        self._gts = defaultdict(list)       # gt for evaluation\n        self._dts = defaultdict(list)       # dt for evaluation\n        for gt in gts:\n            self._gts[gt[\'image_id\'], gt[\'category_id\']].append(gt)\n        for dt in dts:\n            self._dts[dt[\'image_id\'], dt[\'category_id\']].append(dt)\n        self.evalImgs = defaultdict(list)   # per-image per-category evaluation results\n        self.eval     = {}                  # accumulated evaluation results\n\n    def evaluate(self):\n        \'\'\'\n        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs\n        :return: None\n        \'\'\'\n        tic = time.time()\n        print(\'Running per image evaluation...\')\n        p = self.params\n        # add backward compatibility if useSegm is specified in params\n        if not p.useSegm is None:\n            p.iouType = \'segm\' if p.useSegm == 1 else \'bbox\'\n            print(\'useSegm (deprecated) is not None. Running {} evaluation\'.format(p.iouType))\n        print(\'Evaluate annotation type *{}*\'.format(p.iouType))\n        p.imgIds = list(np.unique(p.imgIds))\n        if p.useCats:\n            p.catIds = list(np.unique(p.catIds))\n        p.maxDets = sorted(p.maxDets)\n        self.params=p\n\n        self._prepare()\n        # loop through images, area range, max detection number\n        catIds = p.catIds if p.useCats else [-1]\n\n        if p.iouType == \'segm\' or p.iouType == \'bbox\':\n            computeIoU = self.computeIoU\n        elif p.iouType == \'keypoints\':\n            computeIoU = self.computeOks\n        self.ious = {(imgId, catId): computeIoU(imgId, catId) \\\n                        for imgId in p.imgIds\n                        for catId in catIds}\n\n        evaluateImg = self.evaluateImg\n        maxDet = p.maxDets[-1]\n        self.evalImgs = [evaluateImg(imgId, catId, areaRng, maxDet)\n                 for catId in catIds\n                 for areaRng in p.areaRng\n                 for imgId in p.imgIds\n             ]\n        self._paramsEval = copy.deepcopy(self.params)\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format(toc-tic))\n\n    def computeIoU(self, imgId, catId):\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return []\n        inds = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in inds]\n        if len(dt) > p.maxDets[-1]:\n            dt=dt[0:p.maxDets[-1]]\n\n        if p.iouType == \'segm\':\n            g = [g[\'segmentation\'] for g in gt]\n            d = [d[\'segmentation\'] for d in dt]\n        elif p.iouType == \'bbox\':\n            g = [g[\'bbox\'] for g in gt]\n            d = [d[\'bbox\'] for d in dt]\n        else:\n            raise Exception(\'unknown iouType for iou computation\')\n\n        # compute iou between each dt and gt region\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        ious = maskUtils.iou(d,g,iscrowd)\n        return ious\n\n    def computeOks(self, imgId, catId):\n        p = self.params\n        # dimention here should be Nxm\n        gts = self._gts[imgId, catId]\n        dts = self._dts[imgId, catId]\n        inds = np.argsort([-d[\'score\'] for d in dts], kind=\'mergesort\')\n        dts = [dts[i] for i in inds]\n        if len(dts) > p.maxDets[-1]:\n            dts = dts[0:p.maxDets[-1]]\n        # if len(gts) == 0 and len(dts) == 0:\n        if len(gts) == 0 or len(dts) == 0:\n            return []\n        ious = np.zeros((len(dts), len(gts)))\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n        vars = (sigmas * 2)**2\n        k = len(sigmas)\n        # compute oks between each detection and ground truth object\n        for j, gt in enumerate(gts):\n            # create bounds for ignore regions(double the gt bbox)\n            g = np.array(gt[\'keypoints\'])\n            xg = g[0::3]; yg = g[1::3]; vg = g[2::3]\n            k1 = np.count_nonzero(vg > 0)\n            bb = gt[\'bbox\']\n            x0 = bb[0] - bb[2]; x1 = bb[0] + bb[2] * 2\n            y0 = bb[1] - bb[3]; y1 = bb[1] + bb[3] * 2\n            for i, dt in enumerate(dts):\n                d = np.array(dt[\'keypoints\'])\n                xd = d[0::3]; yd = d[1::3]\n                if k1>0:\n                    # measure the per-keypoint distance if keypoints visible\n                    dx = xd - xg\n                    dy = yd - yg\n                else:\n                    # measure minimum distance to keypoints in (x0,y0) & (x1,y1)\n                    z = np.zeros((k))\n                    dx = np.max((z, x0-xd),axis=0)+np.max((z, xd-x1),axis=0)\n                    dy = np.max((z, y0-yd),axis=0)+np.max((z, yd-y1),axis=0)\n                e = (dx**2 + dy**2) / vars / (gt[\'area\']+np.spacing(1)) / 2\n                if k1 > 0:\n                    e=e[vg > 0]\n                ious[i, j] = np.sum(np.exp(-e)) / e.shape[0]\n        return ious\n\n    def evaluateImg(self, imgId, catId, aRng, maxDet):\n        \'\'\'\n        perform evaluation for single category and image\n        :return: dict (single image results)\n        \'\'\'\n        p = self.params\n        if p.useCats:\n            gt = self._gts[imgId,catId]\n            dt = self._dts[imgId,catId]\n        else:\n            gt = [_ for cId in p.catIds for _ in self._gts[imgId,cId]]\n            dt = [_ for cId in p.catIds for _ in self._dts[imgId,cId]]\n        if len(gt) == 0 and len(dt) ==0:\n            return None\n\n        for g in gt:\n            if g[\'ignore\'] or (g[\'area\']<aRng[0] or g[\'area\']>aRng[1]):\n                g[\'_ignore\'] = 1\n            else:\n                g[\'_ignore\'] = 0\n\n        # sort dt highest score first, sort gt ignore last\n        gtind = np.argsort([g[\'_ignore\'] for g in gt], kind=\'mergesort\')\n        gt = [gt[i] for i in gtind]\n        dtind = np.argsort([-d[\'score\'] for d in dt], kind=\'mergesort\')\n        dt = [dt[i] for i in dtind[0:maxDet]]\n        iscrowd = [int(o[\'iscrowd\']) for o in gt]\n        # load computed ious\n        ious = self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) > 0 else self.ious[imgId, catId]\n\n        T = len(p.iouThrs)\n        G = len(gt)\n        D = len(dt)\n        gtm  = np.zeros((T,G))\n        dtm  = np.zeros((T,D))\n        gtIg = np.array([g[\'_ignore\'] for g in gt])\n        dtIg = np.zeros((T,D))\n        if not len(ious)==0:\n            for tind, t in enumerate(p.iouThrs):\n                for dind, d in enumerate(dt):\n                    # information about best match so far (m=-1 -> unmatched)\n                    iou = min([t,1-1e-10])\n                    m   = -1\n                    for gind, g in enumerate(gt):\n                        # if this gt already matched, and not a crowd, continue\n                        if gtm[tind,gind]>0 and not iscrowd[gind]:\n                            continue\n                        # if dt matched to reg gt, and on ignore gt, stop\n                        if m>-1 and gtIg[m]==0 and gtIg[gind]==1:\n                            break\n                        # continue to next gt unless better match made\n                        if ious[dind,gind] < iou:\n                            continue\n                        # if match successful and best so far, store appropriately\n                        iou=ious[dind,gind]\n                        m=gind\n                    # if match made store id of match for both dt and gt\n                    if m ==-1:\n                        continue\n                    dtIg[tind,dind] = gtIg[m]\n                    dtm[tind,dind]  = gt[m][\'id\']\n                    gtm[tind,m]     = d[\'id\']\n        # set unmatched detections outside of area range to ignore\n        a = np.array([d[\'area\']<aRng[0] or d[\'area\']>aRng[1] for d in dt]).reshape((1, len(dt)))\n        dtIg = np.logical_or(dtIg, np.logical_and(dtm==0, np.repeat(a,T,0)))\n        # store results for given image and category\n        return {\n                \'image_id\':     imgId,\n                \'category_id\':  catId,\n                \'aRng\':         aRng,\n                \'maxDet\':       maxDet,\n                \'dtIds\':        [d[\'id\'] for d in dt],\n                \'gtIds\':        [g[\'id\'] for g in gt],\n                \'dtMatches\':    dtm,\n                \'gtMatches\':    gtm,\n                \'dtScores\':     [d[\'score\'] for d in dt],\n                \'gtIgnore\':     gtIg,\n                \'dtIgnore\':     dtIg,\n            }\n\n    def accumulate(self, p = None):\n        \'\'\'\n        Accumulate per image evaluation results and store the result in self.eval\n        :param p: input params for evaluation\n        :return: None\n        \'\'\'\n        print(\'Accumulating evaluation results...\')\n        tic = time.time()\n        if not self.evalImgs:\n            print(\'Please run evaluate() first\')\n        # allows input customized parameters\n        if p is None:\n            p = self.params\n        p.catIds = p.catIds if p.useCats == 1 else [-1]\n        T           = len(p.iouThrs)\n        R           = len(p.recThrs)\n        K           = len(p.catIds) if p.useCats else 1\n        A           = len(p.areaRng)\n        M           = len(p.maxDets)\n        precision   = -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories\n        recall      = -np.ones((T,K,A,M))\n\n        # create dictionary for future indexing\n        _pe = self._paramsEval\n        catIds = _pe.catIds if _pe.useCats else [-1]\n        setK = set(catIds)\n        setA = set(map(tuple, _pe.areaRng))\n        setM = set(_pe.maxDets)\n        setI = set(_pe.imgIds)\n        # get inds to evaluate\n        k_list = [n for n, k in enumerate(p.catIds)  if k in setK]\n        m_list = [m for n, m in enumerate(p.maxDets) if m in setM]\n        a_list = [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]\n        i_list = [n for n, i in enumerate(p.imgIds)  if i in setI]\n        I0 = len(_pe.imgIds)\n        A0 = len(_pe.areaRng)\n        # retrieve E at each category, area range, and max number of detections\n        for k, k0 in enumerate(k_list):\n            Nk = k0*A0*I0\n            for a, a0 in enumerate(a_list):\n                Na = a0*I0\n                for m, maxDet in enumerate(m_list):\n                    E = [self.evalImgs[Nk + Na + i] for i in i_list]\n                    E = [e for e in E if not e is None]\n                    if len(E) == 0:\n                        continue\n                    dtScores = np.concatenate([e[\'dtScores\'][0:maxDet] for e in E])\n\n                    # different sorting method generates slightly different results.\n                    # mergesort is used to be consistent as Matlab implementation.\n                    inds = np.argsort(-dtScores, kind=\'mergesort\')\n\n                    dtm  = np.concatenate([e[\'dtMatches\'][:,0:maxDet] for e in E], axis=1)[:,inds]\n                    dtIg = np.concatenate([e[\'dtIgnore\'][:,0:maxDet]  for e in E], axis=1)[:,inds]\n                    gtIg = np.concatenate([e[\'gtIgnore\'] for e in E])\n                    npig = np.count_nonzero(gtIg==0 )\n                    if npig == 0:\n                        continue\n                    tps = np.logical_and(               dtm,  np.logical_not(dtIg) )\n                    fps = np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )\n\n                    tp_sum = np.cumsum(tps, axis=1).astype(dtype=np.float)\n                    fp_sum = np.cumsum(fps, axis=1).astype(dtype=np.float)\n                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):\n                        tp = np.array(tp)\n                        fp = np.array(fp)\n                        nd = len(tp)\n                        rc = tp / npig\n                        pr = tp / (fp+tp+np.spacing(1))\n                        q  = np.zeros((R,))\n\n                        if nd:\n                            recall[t,k,a,m] = rc[-1]\n                        else:\n                            recall[t,k,a,m] = 0\n\n                        # numpy is slow without cython optimization for accessing elements\n                        # use python array gets significant speed improvement\n                        pr = pr.tolist(); q = q.tolist()\n\n                        for i in range(nd-1, 0, -1):\n                            if pr[i] > pr[i-1]:\n                                pr[i-1] = pr[i]\n\n                        inds = np.searchsorted(rc, p.recThrs, side=\'left\')\n                        try:\n                            for ri, pi in enumerate(inds):\n                                q[ri] = pr[pi]\n                        except:\n                            pass\n                        precision[t,:,k,a,m] = np.array(q)\n        self.eval = {\n            \'params\': p,\n            \'counts\': [T, R, K, A, M],\n            \'date\': datetime.datetime.now().strftime(\'%Y-%m-%d %H:%M:%S\'),\n            \'precision\': precision,\n            \'recall\':   recall,\n        }\n        toc = time.time()\n        print(\'DONE (t={:0.2f}s).\'.format( toc-tic))\n\n    def summarize(self):\n        \'\'\'\n        Compute and display summary metrics for evaluation results.\n        Note this functin can *only* be applied on the default parameter setting\n        \'\'\'\n        def _summarize( ap=1, iouThr=None, areaRng=\'all\', maxDets=100 ):\n            p = self.params\n            iStr = \' {:<18} {} @[ IoU={:<9} | area={:>6s} | maxDets={:>3d} ] = {:0.3f}\'\n            titleStr = \'Average Precision\' if ap == 1 else \'Average Recall\'\n            typeStr = \'(AP)\' if ap==1 else \'(AR)\'\n            iouStr = \'{:0.2f}:{:0.2f}\'.format(p.iouThrs[0], p.iouThrs[-1]) \\\n                if iouThr is None else \'{:0.2f}\'.format(iouThr)\n\n            aind = [i for i, aRng in enumerate(p.areaRngLbl) if aRng == areaRng]\n            mind = [i for i, mDet in enumerate(p.maxDets) if mDet == maxDets]\n            if ap == 1:\n                # dimension of precision: [TxRxKxAxM]\n                s = self.eval[\'precision\']\n                # IoU\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,:,aind,mind]\n            else:\n                # dimension of recall: [TxKxAxM]\n                s = self.eval[\'recall\']\n                if iouThr is not None:\n                    t = np.where(iouThr == p.iouThrs)[0]\n                    s = s[t]\n                s = s[:,:,aind,mind]\n            if len(s[s>-1])==0:\n                mean_s = -1\n            else:\n                mean_s = np.mean(s[s>-1])\n            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))\n            return mean_s\n        def _summarizeDets():\n            stats = np.zeros((12,))\n            stats[0] = _summarize(1)\n            stats[1] = _summarize(1, iouThr=.5, maxDets=self.params.maxDets[2])\n            stats[2] = _summarize(1, iouThr=.75, maxDets=self.params.maxDets[2])\n            stats[3] = _summarize(1, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[4] = _summarize(1, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[5] = _summarize(1, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            stats[6] = _summarize(0, maxDets=self.params.maxDets[0])\n            stats[7] = _summarize(0, maxDets=self.params.maxDets[1])\n            stats[8] = _summarize(0, maxDets=self.params.maxDets[2])\n            stats[9] = _summarize(0, areaRng=\'small\', maxDets=self.params.maxDets[2])\n            stats[10] = _summarize(0, areaRng=\'medium\', maxDets=self.params.maxDets[2])\n            stats[11] = _summarize(0, areaRng=\'large\', maxDets=self.params.maxDets[2])\n            return stats\n        def _summarizeKps():\n            stats = np.zeros((10,))\n            stats[0] = _summarize(1, maxDets=20)\n            stats[1] = _summarize(1, maxDets=20, iouThr=.5)\n            stats[2] = _summarize(1, maxDets=20, iouThr=.75)\n            stats[3] = _summarize(1, maxDets=20, areaRng=\'medium\')\n            stats[4] = _summarize(1, maxDets=20, areaRng=\'large\')\n            stats[5] = _summarize(0, maxDets=20)\n            stats[6] = _summarize(0, maxDets=20, iouThr=.5)\n            stats[7] = _summarize(0, maxDets=20, iouThr=.75)\n            stats[8] = _summarize(0, maxDets=20, areaRng=\'medium\')\n            stats[9] = _summarize(0, maxDets=20, areaRng=\'large\')\n            return stats\n        if not self.eval:\n            raise Exception(\'Please run accumulate() first\')\n        iouType = self.params.iouType\n        if iouType == \'segm\' or iouType == \'bbox\':\n            summarize = _summarizeDets\n        elif iouType == \'keypoints\':\n            summarize = _summarizeKps\n        self.stats = summarize()\n\n    def __str__(self):\n        self.summarize()\n\nclass Params:\n    \'\'\'\n    Params for coco evaluation api\n    \'\'\'\n    def setDetParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [1, 10, 100]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'small\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def setKpParams(self):\n        self.imgIds = []\n        self.catIds = []\n        # np.arange causes trouble.  the data point on arange is slightly larger than the true value\n        self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n        self.recThrs = np.linspace(.0, 1.00, np.round((1.00 - .0) / .01) + 1, endpoint=True)\n        self.maxDets = [20]\n        self.areaRng = [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]\n        self.areaRngLbl = [\'all\', \'medium\', \'large\']\n        self.useCats = 1\n\n    def __init__(self, iouType=\'segm\'):\n        if iouType == \'segm\' or iouType == \'bbox\':\n            self.setDetParams()\n        elif iouType == \'keypoints\':\n            self.setKpParams()\n        else:\n            raise Exception(\'iouType not supported\')\n        self.iouType = iouType\n        # useSegm is deprecated\n        self.useSegm = None'"
utils/pycocotools/mask.py,0,"b'__author__ = \'tsungyi\'\n\n#import pycocotools._mask as _mask\nfrom . import _mask\n\n# Interface for manipulating masks stored in RLE format.\n#\n# RLE is a simple yet efficient format for storing binary masks. RLE\n# first divides a vector (or vectorized image) into a series of piecewise\n# constant regions and then for each piece simply stores the length of\n# that piece. For example, given M=[0 0 1 1 1 0 1] the RLE counts would\n# be [2 3 1 1], or for M=[1 1 1 1 1 1 0] the counts would be [0 6 1]\n# (note that the odd counts are always the numbers of zeros). Instead of\n# storing the counts directly, additional compression is achieved with a\n# variable bitrate representation based on a common scheme called LEB128.\n#\n# Compression is greatest given large piecewise constant regions.\n# Specifically, the size of the RLE is proportional to the number of\n# *boundaries* in M (or for an image the number of boundaries in the y\n# direction). Assuming fairly simple shapes, the RLE representation is\n# O(sqrt(n)) where n is number of pixels in the object. Hence space usage\n# is substantially lower, especially for large simple objects (large n).\n#\n# Many common operations on masks can be computed directly using the RLE\n# (without need for decoding). This includes computations such as area,\n# union, intersection, etc. All of these operations are linear in the\n# size of the RLE, in other words they are O(sqrt(n)) where n is the area\n# of the object. Computing these operations on the original mask is O(n).\n# Thus, using the RLE can result in substantial computational savings.\n#\n# The following API functions are defined:\n#  encode         - Encode binary masks using RLE.\n#  decode         - Decode binary masks encoded via RLE.\n#  merge          - Compute union or intersection of encoded masks.\n#  iou            - Compute intersection over union between masks.\n#  area           - Compute area of encoded masks.\n#  toBbox         - Get bounding boxes surrounding encoded masks.\n#  frPyObjects    - Convert polygon, bbox, and uncompressed RLE to encoded RLE mask.\n#\n# Usage:\n#  Rs     = encode( masks )\n#  masks  = decode( Rs )\n#  R      = merge( Rs, intersect=false )\n#  o      = iou( dt, gt, iscrowd )\n#  a      = area( Rs )\n#  bbs    = toBbox( Rs )\n#  Rs     = frPyObjects( [pyObjects], h, w )\n#\n# In the API the following formats are used:\n#  Rs      - [dict] Run-length encoding of binary masks\n#  R       - dict Run-length encoding of binary mask\n#  masks   - [hxwxn] Binary mask(s) (must have type np.ndarray(dtype=uint8) in column-major order)\n#  iscrowd - [nx1] list of np.ndarray. 1 indicates corresponding gt image has crowd region to ignore\n#  bbs     - [nx4] Bounding box(es) stored as [x y w h]\n#  poly    - Polygon stored as [[x1 y1 x2 y2...],[x1 y1 ...],...] (2D list)\n#  dt,gt   - May be either bounding boxes or encoded masks\n# Both poly and bbs are 0-indexed (bbox=[0 0 1 1] encloses first pixel).\n#\n# Finally, a note about the intersection over union (iou) computation.\n# The standard iou of a ground truth (gt) and detected (dt) object is\n#  iou(gt,dt) = area(intersect(gt,dt)) / area(union(gt,dt))\n# For ""crowd"" regions, we use a modified criteria. If a gt object is\n# marked as ""iscrowd"", we allow a dt to match any subregion of the gt.\n# Choosing gt\' in the crowd gt that best matches the dt can be done using\n# gt\'=intersect(dt,gt). Since by definition union(gt\',dt)=dt, computing\n#  iou(gt,dt,iscrowd) = iou(gt\',dt) = area(intersect(gt,dt)) / area(dt)\n# For crowd gt regions we use this modified criteria above for the iou.\n#\n# To compile run ""python setup.py build_ext --inplace""\n# Please do not contact us for help with compiling.\n#\n# Microsoft COCO Toolbox.      version 2.0\n# Data, paper, and tutorials available at:  http://mscoco.org/\n# Code written by Piotr Dollar and Tsung-Yi Lin, 2015.\n# Licensed under the Simplified BSD License [see coco/license.txt]\n\niou         = _mask.iou\nmerge       = _mask.merge\nfrPyObjects = _mask.frPyObjects\n\ndef encode(bimask):\n    if len(bimask.shape) == 3:\n        return _mask.encode(bimask)\n    elif len(bimask.shape) == 2:\n        h, w = bimask.shape\n        return _mask.encode(bimask.reshape((h, w, 1), order=\'F\'))[0]\n\ndef decode(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.decode(rleObjs)\n    else:\n        return _mask.decode([rleObjs])[:,:,0]\n\ndef area(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.area(rleObjs)\n    else:\n        return _mask.area([rleObjs])[0]\n\ndef toBbox(rleObjs):\n    if type(rleObjs) == list:\n        return _mask.toBbox(rleObjs)\n    else:\n        return _mask.toBbox([rleObjs])[0]\n'"
