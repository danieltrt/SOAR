file_path,api_count,code
setup.py,3,"b""import os\nimport os.path as osp\nimport glob\nfrom setuptools import setup, find_packages\n\nimport torch\nfrom torch.utils.cpp_extension import BuildExtension\nfrom torch.utils.cpp_extension import CppExtension, CUDAExtension, CUDA_HOME\n\nWITH_CUDA = torch.cuda.is_available() and CUDA_HOME is not None\nif os.getenv('FORCE_CUDA', '0') == '1':\n    WITH_CUDA = True\nif os.getenv('FORCE_CPU', '0') == '1':\n    WITH_CUDA = False\n\nBUILD_DOCS = os.getenv('BUILD_DOCS', '0') == '1'\n\n\ndef get_extensions():\n    Extension = CppExtension\n    define_macros = []\n    extra_compile_args = {'cxx': []}\n\n    if WITH_CUDA:\n        Extension = CUDAExtension\n        define_macros += [('WITH_CUDA', None)]\n        nvcc_flags = os.getenv('NVCC_FLAGS', '')\n        nvcc_flags = [] if nvcc_flags == '' else nvcc_flags.split(' ')\n        nvcc_flags += ['-arch=sm_35', '--expt-relaxed-constexpr']\n        extra_compile_args['nvcc'] = nvcc_flags\n\n    extensions_dir = osp.join(osp.dirname(osp.abspath(__file__)), 'csrc')\n    main_files = glob.glob(osp.join(extensions_dir, '*.cpp'))\n    extensions = []\n    for main in main_files:\n        name = main.split(os.sep)[-1][:-4]\n\n        sources = [main]\n\n        path = osp.join(extensions_dir, 'cpu', f'{name}_cpu.cpp')\n        if osp.exists(path):\n            sources += [path]\n\n        path = osp.join(extensions_dir, 'cuda', f'{name}_cuda.cu')\n        if WITH_CUDA and osp.exists(path):\n            sources += [path]\n\n        extension = Extension(\n            'torch_scatter._' + name,\n            sources,\n            include_dirs=[extensions_dir],\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n        )\n        extensions += [extension]\n\n    return extensions\n\n\ninstall_requires = []\nsetup_requires = ['pytest-runner']\ntests_require = ['pytest', 'pytest-cov']\n\nsetup(\n    name='torch_scatter',\n    version='2.0.4',\n    author='Matthias Fey',\n    author_email='matthias.fey@tu-dortmund.de',\n    url='https://github.com/rusty1s/pytorch_scatter',\n    description='PyTorch Extension Library of Optimized Scatter Operations',\n    keywords=['pytorch', 'scatter', 'segment', 'gather'],\n    license='MIT',\n    python_requires='>=3.6',\n    install_requires=install_requires,\n    setup_requires=setup_requires,\n    tests_require=tests_require,\n    ext_modules=get_extensions() if not BUILD_DOCS else [],\n    cmdclass={\n        'build_ext':\n        BuildExtension.with_options(no_python_abi_suffix=True, use_ninja=False)\n    },\n    packages=find_packages(),\n)\n"""
benchmark/gather.py,22,"b""import time\nimport itertools\n\nimport argparse\nimport torch\nfrom scipy.io import loadmat\n\nfrom torch_scatter import gather_coo, gather_csr\n\nfrom scatter_segment import short_rows, long_rows, download, bold\n\n\n@torch.no_grad()\ndef correctness(dataset):\n    group, name = dataset\n    mat = loadmat(f'{name}.mat')['Problem'][0][0][2].tocsr()\n    rowptr = torch.from_numpy(mat.indptr).to(args.device, torch.long)\n    row = torch.from_numpy(mat.tocoo().row).to(args.device, torch.long)\n    dim_size = rowptr.size(0) - 1\n\n    for size in sizes[1:]:\n        try:\n            x = torch.randn((dim_size, size), device=args.device)\n            x = x.squeeze(-1) if size == 1 else x\n\n            out1 = x.index_select(0, row)\n            out2 = gather_coo(x, row)\n            out3 = gather_csr(x, rowptr)\n\n            assert torch.allclose(out1, out2, atol=1e-4)\n            assert torch.allclose(out1, out3, atol=1e-4)\n        except RuntimeError as e:\n            if 'out of memory' not in str(e):\n                raise RuntimeError(e)\n            torch.cuda.empty_cache()\n\n\ndef time_func(func, x):\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        t = time.perf_counter()\n\n        if not args.with_backward:\n            with torch.no_grad():\n                for _ in range(iters):\n                    func(x)\n        else:\n            x = x.requires_grad_()\n            for _ in range(iters):\n                out = func(x)\n                torch.autograd.grad(out, x, out, only_inputs=True)\n\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        return time.perf_counter() - t\n    except RuntimeError as e:\n        if 'out of memory' not in str(e):\n            raise RuntimeError(e)\n        torch.cuda.empty_cache()\n        return float('inf')\n\n\ndef timing(dataset):\n    group, name = dataset\n    mat = loadmat(f'{name}.mat')['Problem'][0][0][2].tocsr()\n    rowptr = torch.from_numpy(mat.indptr).to(args.device, torch.long)\n    row = torch.from_numpy(mat.tocoo().row).to(args.device, torch.long)\n    dim_size = rowptr.size(0) - 1\n    avg_row_len = row.size(0) / dim_size\n\n    def select(x):\n        return x.index_select(0, row)\n\n    def gather(x):\n        return x.gather(0, row.view(-1, 1).expand(-1, x.size(1)))\n\n    def gat_coo(x):\n        return gather_coo(x, row)\n\n    def gat_csr(x):\n        return gather_csr(x, rowptr)\n\n    t1, t2, t3, t4 = [], [], [], []\n    for size in sizes:\n        try:\n            x = torch.randn((dim_size, size), device=args.device)\n\n            t1 += [time_func(select, x)]\n            t2 += [time_func(gather, x)]\n            t3 += [time_func(gat_coo, x)]\n            t4 += [time_func(gat_csr, x)]\n\n            del x\n\n        except RuntimeError as e:\n            if 'out of memory' not in str(e):\n                raise RuntimeError(e)\n            torch.cuda.empty_cache()\n            for t in (t1, t2, t3, t4):\n                t.append(float('inf'))\n\n    ts = torch.tensor([t1, t2, t3, t4])\n    winner = torch.zeros_like(ts, dtype=torch.bool)\n    winner[ts.argmin(dim=0), torch.arange(len(sizes))] = 1\n    winner = winner.tolist()\n\n    name = f'{group}/{name}'\n    print(f'{bold(name)} (avg row length: {avg_row_len:.2f}):')\n    print('\\t'.join(['       '] + [f'{size:>5}' for size in sizes]))\n    print('\\t'.join([bold('SELECT ')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t1, winner[0])]))\n    print('\\t'.join([bold('GAT    ')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t2, winner[1])]))\n    print('\\t'.join([bold('GAT_COO')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t3, winner[2])]))\n    print('\\t'.join([bold('GAT_CSR')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t4, winner[3])]))\n    print()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--with_backward', action='store_true')\n    parser.add_argument('--device', type=str, default='cuda')\n    args = parser.parse_args()\n    iters = 1 if args.device == 'cpu' else 20\n    sizes = [1, 16, 32, 64, 128, 256, 512]\n    sizes = sizes[:3] if args.device == 'cpu' else sizes\n\n    for _ in range(10):  # Warmup.\n        torch.randn(100, 100, device=args.device).sum()\n    for dataset in itertools.chain(short_rows, long_rows):\n        download(dataset)\n        correctness(dataset)\n        timing(dataset)\n"""
benchmark/scatter_segment.py,31,"b""import time\nimport os.path as osp\nimport itertools\n\nimport argparse\nimport wget\nimport torch\nfrom scipy.io import loadmat\n\nfrom torch_scatter import scatter, segment_coo, segment_csr\n\nshort_rows = [\n    ('DIMACS10', 'citationCiteseer'),\n    ('SNAP', 'web-Stanford'),\n]\nlong_rows = [\n    ('Janna', 'StocF-1465'),\n    ('GHS_psdef', 'ldoor'),\n]\n\n\ndef download(dataset):\n    url = 'https://sparse.tamu.edu/mat/{}/{}.mat'\n    for group, name in itertools.chain(long_rows, short_rows):\n        if not osp.exists(f'{name}.mat'):\n            print(f'Downloading {group}/{name}:')\n            wget.download(url.format(group, name))\n            print('')\n\n\ndef bold(text, flag=True):\n    return f'\\033[1m{text}\\033[0m' if flag else text\n\n\n@torch.no_grad()\ndef correctness(dataset):\n    group, name = dataset\n    mat = loadmat(f'{name}.mat')['Problem'][0][0][2].tocsr()\n    rowptr = torch.from_numpy(mat.indptr).to(args.device, torch.long)\n    row = torch.from_numpy(mat.tocoo().row).to(args.device, torch.long)\n    dim_size = rowptr.size(0) - 1\n\n    for size in sizes:\n        try:\n            x = torch.randn((row.size(0), size), device=args.device)\n            x = x.squeeze(-1) if size == 1 else x\n\n            out1 = scatter(x, row, dim=0, dim_size=dim_size, reduce='add')\n            out2 = segment_coo(x, row, dim_size=dim_size, reduce='add')\n            out3 = segment_csr(x, rowptr, reduce='add')\n\n            assert torch.allclose(out1, out2, atol=1e-4)\n            assert torch.allclose(out1, out3, atol=1e-4)\n\n            out1 = scatter(x, row, dim=0, dim_size=dim_size, reduce='mean')\n            out2 = segment_coo(x, row, dim_size=dim_size, reduce='mean')\n            out3 = segment_csr(x, rowptr, reduce='mean')\n\n            assert torch.allclose(out1, out2, atol=1e-4)\n            assert torch.allclose(out1, out3, atol=1e-4)\n\n            out1 = scatter(x, row, dim=0, dim_size=dim_size, reduce='min')\n            out2 = segment_coo(x, row, reduce='min')\n            out3 = segment_csr(x, rowptr, reduce='min')\n\n            assert torch.allclose(out1, out2, atol=1e-4)\n            assert torch.allclose(out1, out3, atol=1e-4)\n\n            out1 = scatter(x, row, dim=0, dim_size=dim_size, reduce='max')\n            out2 = segment_coo(x, row, reduce='max')\n            out3 = segment_csr(x, rowptr, reduce='max')\n\n            assert torch.allclose(out1, out2, atol=1e-4)\n            assert torch.allclose(out1, out3, atol=1e-4)\n\n        except RuntimeError as e:\n            if 'out of memory' not in str(e):\n                raise RuntimeError(e)\n            torch.cuda.empty_cache()\n\n\ndef time_func(func, x):\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        t = time.perf_counter()\n\n        if not args.with_backward:\n            with torch.no_grad():\n                for _ in range(iters):\n                    func(x)\n        else:\n            x = x.requires_grad_()\n            for _ in range(iters):\n                out = func(x)\n                out = out[0] if isinstance(out, tuple) else out\n                torch.autograd.grad(out, x, out, only_inputs=True)\n\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        return time.perf_counter() - t\n    except RuntimeError as e:\n        if 'out of memory' not in str(e):\n            raise RuntimeError(e)\n        torch.cuda.empty_cache()\n        return float('inf')\n\n\ndef timing(dataset):\n    group, name = dataset\n    mat = loadmat(f'{name}.mat')['Problem'][0][0][2].tocsr()\n    rowptr = torch.from_numpy(mat.indptr).to(args.device, torch.long)\n    row = torch.from_numpy(mat.tocoo().row).to(args.device, torch.long)\n    row2 = row[torch.randperm(row.size(0))]\n    dim_size = rowptr.size(0) - 1\n    avg_row_len = row.size(0) / dim_size\n\n    def sca1_row(x):\n        out = x.new_zeros(dim_size, *x.size()[1:])\n        row_tmp = row.view(-1, 1).expand_as(x) if x.dim() > 1 else row\n        return out.scatter_add_(0, row_tmp, x)\n\n    def sca1_col(x):\n        out = x.new_zeros(dim_size, *x.size()[1:])\n        row2_tmp = row2.view(-1, 1).expand_as(x) if x.dim() > 1 else row2\n        return out.scatter_add_(0, row2_tmp, x)\n\n    def sca2_row(x):\n        return scatter(x, row, dim=0, dim_size=dim_size, reduce=args.reduce)\n\n    def sca2_col(x):\n        return scatter(x, row2, dim=0, dim_size=dim_size, reduce=args.reduce)\n\n    def seg_coo(x):\n        return segment_coo(x, row, reduce=args.reduce)\n\n    def seg_csr(x):\n        return segment_csr(x, rowptr, reduce=args.reduce)\n\n    def dense1(x):\n        return getattr(torch, args.reduce)(x, dim=-2)\n\n    def dense2(x):\n        return getattr(torch, args.reduce)(x, dim=-1)\n\n    t1, t2, t3, t4, t5, t6, t7, t8 = [], [], [], [], [], [], [], []\n\n    for size in sizes:\n        try:\n            x = torch.randn((row.size(0), size), device=args.device)\n            x = x.squeeze(-1) if size == 1 else x\n\n            t1 += [time_func(sca1_row, x)]\n            t2 += [time_func(sca1_col, x)]\n            t3 += [time_func(sca2_row, x)]\n            t4 += [time_func(sca2_col, x)]\n            t5 += [time_func(seg_coo, x)]\n            t6 += [time_func(seg_csr, x)]\n\n            del x\n\n        except RuntimeError as e:\n            if 'out of memory' not in str(e):\n                raise RuntimeError(e)\n            torch.cuda.empty_cache()\n            for t in (t1, t2, t3, t4, t5, t6):\n                t.append(float('inf'))\n\n        try:\n            x = torch.randn((dim_size, int(avg_row_len + 1), size),\n                            device=args.device)\n\n            t7 += [time_func(dense1, x)]\n            x = x.view(dim_size, size, int(avg_row_len + 1))\n            t8 += [time_func(dense2, x)]\n\n            del x\n\n        except RuntimeError as e:\n            if 'out of memory' not in str(e):\n                raise RuntimeError(e)\n            torch.cuda.empty_cache()\n            for t in (t7, t8):\n                t.append(float('inf'))\n\n    ts = torch.tensor([t1, t2, t3, t4, t5, t6, t7, t8])\n    winner = torch.zeros_like(ts, dtype=torch.bool)\n    winner[ts.argmin(dim=0), torch.arange(len(sizes))] = 1\n    winner = winner.tolist()\n\n    name = f'{group}/{name}'\n    print(f'{bold(name)} (avg row length: {avg_row_len:.2f}):')\n    print('\\t'.join(['        '] + [f'{size:>5}' for size in sizes]))\n    print('\\t'.join([bold('SCA1_ROW')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t1, winner[0])]))\n    print('\\t'.join([bold('SCA1_COL')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t2, winner[1])]))\n    print('\\t'.join([bold('SCA2_ROW')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t3, winner[2])]))\n    print('\\t'.join([bold('SCA2_COL')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t4, winner[3])]))\n    print('\\t'.join([bold('SEG_COO ')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t5, winner[4])]))\n    print('\\t'.join([bold('SEG_CSR ')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t6, winner[5])]))\n    print('\\t'.join([bold('DENSE1  ')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t7, winner[6])]))\n    print('\\t'.join([bold('DENSE2  ')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t8, winner[7])]))\n    print()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--reduce', type=str, required=True,\n                        choices=['sum', 'mean', 'min', 'max'])\n    parser.add_argument('--with_backward', action='store_true')\n    parser.add_argument('--device', type=str, default='cuda')\n    args = parser.parse_args()\n    iters = 1 if args.device == 'cpu' else 20\n    sizes = [1, 16, 32, 64, 128, 256, 512]\n    sizes = sizes[:3] if args.device == 'cpu' else sizes\n\n    for _ in range(10):  # Warmup.\n        torch.randn(100, 100, device=args.device).sum()\n    for dataset in itertools.chain(short_rows, long_rows):\n        download(dataset)\n        correctness(dataset)\n        timing(dataset)\n"""
script/rename_wheel.py,0,"b""import sys\nimport os\nimport os.path as osp\nimport glob\nimport shutil\n\nidx = sys.argv[1]\nassert idx in ['cpu', 'cu92', 'cu101', 'cu102']\n\ndist_dir = osp.join(osp.dirname(osp.abspath(__file__)), '..', 'dist')\nwheels = glob.glob(osp.join('dist', '**', '*.whl'), recursive=True)\n\nfor wheel in wheels:\n    if idx in wheel:\n        continue\n\n    paths = wheel.split(osp.sep)\n    names = paths[-1].split('-')\n\n    name = '-'.join(names[:-4] + ['latest+' + idx] + names[-3:])\n    shutil.copyfile(wheel, osp.join(*paths[:-1], name))\n\n    name = '-'.join(names[:-4] + [names[-4] + '+' + idx] + names[-3:])\n    os.rename(wheel, osp.join(*paths[:-1], name))\n"""
test/__init__.py,0,b''
test/test_broadcasting.py,6,"b""from itertools import product\n\nimport pytest\nimport torch\nfrom torch_scatter import scatter\n\nfrom .utils import reductions, devices\n\n\n@pytest.mark.parametrize('reduce,device', product(reductions, devices))\ndef test_broadcasting(reduce, device):\n    B, C, H, W = (4, 3, 8, 8)\n\n    src = torch.randn((B, C, H, W), device=device)\n    index = torch.randint(0, H, (H, )).to(device, torch.long)\n    out = scatter(src, index, dim=2, dim_size=H, reduce=reduce)\n    assert out.size() == (B, C, H, W)\n\n    src = torch.randn((B, C, H, W), device=device)\n    index = torch.randint(0, H, (B, 1, H, W)).to(device, torch.long)\n    out = scatter(src, index, dim=2, dim_size=H, reduce=reduce)\n    assert out.size() == (B, C, H, W)\n\n    src = torch.randn((B, C, H, W), device=device)\n    index = torch.randint(0, H, (H, )).to(device, torch.long)\n    out = scatter(src, index, dim=2, dim_size=H, reduce=reduce)\n    assert out.size() == (B, C, H, W)\n"""
test/test_gather.py,16,"b""from itertools import product\n\nimport pytest\nimport torch\nfrom torch.autograd import gradcheck\nfrom torch_scatter import gather_csr, gather_coo\n\nfrom .utils import tensor, dtypes, devices\n\ntests = [\n    {\n        'src': [1, 2, 3, 4],\n        'index': [0, 0, 1, 1, 1, 3],\n        'indptr': [0, 2, 5, 5, 6],\n        'expected': [1, 1, 2, 2, 2, 4],\n    },\n    {\n        'src': [[1, 2], [3, 4], [5, 6], [7, 8]],\n        'index': [0, 0, 1, 1, 1, 3],\n        'indptr': [0, 2, 5, 5, 6],\n        'expected': [[1, 2], [1, 2], [3, 4], [3, 4], [3, 4], [7, 8]]\n    },\n    {\n        'src': [[1, 3, 5, 7], [2, 4, 6, 8]],\n        'index': [[0, 0, 1, 1, 1, 3], [0, 0, 0, 1, 1, 2]],\n        'indptr': [[0, 2, 5, 5, 6], [0, 3, 5, 6, 6]],\n        'expected': [[1, 1, 3, 3, 3, 7], [2, 2, 2, 4, 4, 6]],\n    },\n    {\n        'src': [[[1, 2], [3, 4], [5, 6]], [[7, 9], [10, 11], [12, 13]]],\n        'index': [[0, 0, 1], [0, 2, 2]],\n        'indptr': [[0, 2, 3, 3], [0, 1, 1, 3]],\n        'expected': [[[1, 2], [1, 2], [3, 4]], [[7, 9], [12, 13], [12, 13]]],\n    },\n    {\n        'src': [[1], [2]],\n        'index': [[0, 0], [0, 0]],\n        'indptr': [[0, 2], [0, 2]],\n        'expected': [[1, 1], [2, 2]],\n    },\n    {\n        'src': [[[1, 1]], [[2, 2]]],\n        'index': [[0, 0], [0, 0]],\n        'indptr': [[0, 2], [0, 2]],\n        'expected': [[[1, 1], [1, 1]], [[2, 2], [2, 2]]],\n    },\n]\n\n\n@pytest.mark.parametrize('test,dtype,device', product(tests, dtypes, devices))\ndef test_forward(test, dtype, device):\n    src = tensor(test['src'], dtype, device)\n    index = tensor(test['index'], torch.long, device)\n    indptr = tensor(test['indptr'], torch.long, device)\n    expected = tensor(test['expected'], dtype, device)\n\n    out = gather_csr(src, indptr)\n    assert torch.all(out == expected)\n\n    out = gather_coo(src, index)\n    assert torch.all(out == expected)\n\n\n@pytest.mark.parametrize('test,device', product(tests, devices))\ndef test_backward(test, device):\n    src = tensor(test['src'], torch.double, device)\n    src.requires_grad_()\n    index = tensor(test['index'], torch.long, device)\n    indptr = tensor(test['indptr'], torch.long, device)\n\n    assert gradcheck(gather_csr, (src, indptr, None)) is True\n    assert gradcheck(gather_coo, (src, index, None)) is True\n\n\n@pytest.mark.parametrize('test,dtype,device', product(tests, dtypes, devices))\ndef test_out(test, dtype, device):\n    src = tensor(test['src'], dtype, device)\n    index = tensor(test['index'], torch.long, device)\n    indptr = tensor(test['indptr'], torch.long, device)\n    expected = tensor(test['expected'], dtype, device)\n\n    size = list(src.size())\n    size[index.dim() - 1] = index.size(-1)\n    out = src.new_full(size, -2)\n\n    gather_csr(src, indptr, out)\n    assert torch.all(out == expected)\n\n    out.fill_(-2)\n\n    gather_coo(src, index, out)\n    assert torch.all(out == expected)\n\n\n@pytest.mark.parametrize('test,dtype,device', product(tests, dtypes, devices))\ndef test_non_contiguous(test, dtype, device):\n    src = tensor(test['src'], dtype, device)\n    index = tensor(test['index'], torch.long, device)\n    indptr = tensor(test['indptr'], torch.long, device)\n    expected = tensor(test['expected'], dtype, device)\n\n    if src.dim() > 1:\n        src = src.transpose(0, 1).contiguous().transpose(0, 1)\n    if index.dim() > 1:\n        index = index.transpose(0, 1).contiguous().transpose(0, 1)\n    if indptr.dim() > 1:\n        indptr = indptr.transpose(0, 1).contiguous().transpose(0, 1)\n\n    out = gather_csr(src, indptr)\n    assert torch.all(out == expected)\n\n    out = gather_coo(src, index)\n    assert torch.all(out == expected)\n"""
test/test_multi_gpu.py,8,"b""from itertools import product\n\nimport pytest\nimport torch\nimport torch_scatter\n\nfrom .utils import reductions, tensor, dtypes\n\ntests = [\n    {\n        'src': [1, 2, 3, 4, 5, 6],\n        'index': [0, 0, 1, 1, 1, 3],\n        'indptr': [0, 2, 5, 5, 6],\n        'dim': 0,\n        'sum': [3, 12, 0, 6],\n        'add': [3, 12, 0, 6],\n        'mean': [1.5, 4, 0, 6],\n        'min': [1, 3, 0, 6],\n        'max': [2, 5, 0, 6],\n    },\n]\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason='CUDA not available')\n@pytest.mark.skipif(torch.cuda.device_count() < 2, reason='No multiple GPUS')\n@pytest.mark.parametrize('test,reduce,dtype', product(tests, reductions,\n                                                      dtypes))\ndef test_forward(test, reduce, dtype):\n    device = torch.device('cuda:1')\n    src = tensor(test['src'], dtype, device)\n    index = tensor(test['index'], torch.long, device)\n    indptr = tensor(test['indptr'], torch.long, device)\n    dim = test['dim']\n    expected = tensor(test[reduce], dtype, device)\n\n    out = torch_scatter.scatter(src, index, dim, reduce=reduce)\n    assert torch.all(out == expected)\n\n    out = torch_scatter.segment_coo(src, index, reduce=reduce)\n    assert torch.all(out == expected)\n\n    out = torch_scatter.segment_csr(src, indptr, reduce=reduce)\n    assert torch.all(out == expected)\n"""
test/test_scatter.py,14,"b""from itertools import product\n\nimport pytest\nimport torch\nfrom torch.autograd import gradcheck\nimport torch_scatter\n\nfrom .utils import reductions, tensor, dtypes, devices\n\ntests = [\n    {\n        'src': [1, 3, 2, 4, 5, 6],\n        'index': [0, 1, 0, 1, 1, 3],\n        'dim': 0,\n        'sum': [3, 12, 0, 6],\n        'add': [3, 12, 0, 6],\n        'mean': [1.5, 4, 0, 6],\n        'min': [1, 3, 0, 6],\n        'arg_min': [0, 1, 6, 5],\n        'max': [2, 5, 0, 6],\n        'arg_max': [2, 4, 6, 5],\n    },\n    {\n        'src': [[1, 2], [5, 6], [3, 4], [7, 8], [9, 10], [11, 12]],\n        'index': [0, 1, 0, 1, 1, 3],\n        'dim': 0,\n        'sum': [[4, 6], [21, 24], [0, 0], [11, 12]],\n        'add': [[4, 6], [21, 24], [0, 0], [11, 12]],\n        'mean': [[2, 3], [7, 8], [0, 0], [11, 12]],\n        'min': [[1, 2], [5, 6], [0, 0], [11, 12]],\n        'arg_min': [[0, 0], [1, 1], [6, 6], [5, 5]],\n        'max': [[3, 4], [9, 10], [0, 0], [11, 12]],\n        'arg_max': [[2, 2], [4, 4], [6, 6], [5, 5]],\n    },\n    {\n        'src': [[1, 5, 3, 7, 9, 11], [2, 4, 8, 6, 10, 12]],\n        'index': [[0, 1, 0, 1, 1, 3], [0, 0, 1, 0, 1, 2]],\n        'dim': 1,\n        'sum': [[4, 21, 0, 11], [12, 18, 12, 0]],\n        'add': [[4, 21, 0, 11], [12, 18, 12, 0]],\n        'mean': [[2, 7, 0, 11], [4, 9, 12, 0]],\n        'min': [[1, 5, 0, 11], [2, 8, 12, 0]],\n        'arg_min': [[0, 1, 6, 5], [0, 2, 5, 6]],\n        'max': [[3, 9, 0, 11], [6, 10, 12, 0]],\n        'arg_max': [[2, 4, 6, 5], [3, 4, 5, 6]],\n    },\n    {\n        'src': [[[1, 2], [5, 6], [3, 4]], [[10, 11], [7, 9], [12, 13]]],\n        'index': [[0, 1, 0], [2, 0, 2]],\n        'dim': 1,\n        'sum': [[[4, 6], [5, 6], [0, 0]], [[7, 9], [0, 0], [22, 24]]],\n        'add': [[[4, 6], [5, 6], [0, 0]], [[7, 9], [0, 0], [22, 24]]],\n        'mean': [[[2, 3], [5, 6], [0, 0]], [[7, 9], [0, 0], [11, 12]]],\n        'min': [[[1, 2], [5, 6], [0, 0]], [[7, 9], [0, 0], [10, 11]]],\n        'arg_min': [[[0, 0], [1, 1], [3, 3]], [[1, 1], [3, 3], [0, 0]]],\n        'max': [[[3, 4], [5, 6], [0, 0]], [[7, 9], [0, 0], [12, 13]]],\n        'arg_max': [[[2, 2], [1, 1], [3, 3]], [[1, 1], [3, 3], [2, 2]]],\n    },\n    {\n        'src': [[1, 3], [2, 4]],\n        'index': [[0, 0], [0, 0]],\n        'dim': 1,\n        'sum': [[4], [6]],\n        'add': [[4], [6]],\n        'mean': [[2], [3]],\n        'min': [[1], [2]],\n        'arg_min': [[0], [0]],\n        'max': [[3], [4]],\n        'arg_max': [[1], [1]],\n    },\n    {\n        'src': [[[1, 1], [3, 3]], [[2, 2], [4, 4]]],\n        'index': [[0, 0], [0, 0]],\n        'dim': 1,\n        'sum': [[[4, 4]], [[6, 6]]],\n        'add': [[[4, 4]], [[6, 6]]],\n        'mean': [[[2, 2]], [[3, 3]]],\n        'min': [[[1, 1]], [[2, 2]]],\n        'arg_min': [[[0, 0]], [[0, 0]]],\n        'max': [[[3, 3]], [[4, 4]]],\n        'arg_max': [[[1, 1]], [[1, 1]]],\n    },\n]\n\n\n@pytest.mark.parametrize('test,reduce,dtype,device',\n                         product(tests, reductions, dtypes, devices))\ndef test_forward(test, reduce, dtype, device):\n    src = tensor(test['src'], dtype, device)\n    index = tensor(test['index'], torch.long, device)\n    dim = test['dim']\n    expected = tensor(test[reduce], dtype, device)\n\n    out = getattr(torch_scatter, 'scatter_' + reduce)(src, index, dim)\n    if isinstance(out, tuple):\n        out, arg_out = out\n        arg_expected = tensor(test['arg_' + reduce], torch.long, device)\n        assert torch.all(arg_out == arg_expected)\n    assert torch.all(out == expected)\n\n\n@pytest.mark.parametrize('test,reduce,device',\n                         product(tests, reductions, devices))\ndef test_backward(test, reduce, device):\n    src = tensor(test['src'], torch.double, device)\n    src.requires_grad_()\n    index = tensor(test['index'], torch.long, device)\n    dim = test['dim']\n\n    assert gradcheck(torch_scatter.scatter,\n                     (src, index, dim, None, None, reduce))\n\n\n@pytest.mark.parametrize('test,reduce,dtype,device',\n                         product(tests, reductions, dtypes, devices))\ndef test_out(test, reduce, dtype, device):\n    src = tensor(test['src'], dtype, device)\n    index = tensor(test['index'], torch.long, device)\n    dim = test['dim']\n    expected = tensor(test[reduce], dtype, device)\n\n    out = torch.full_like(expected, -2)\n\n    getattr(torch_scatter, 'scatter_' + reduce)(src, index, dim, out)\n\n    if reduce == 'sum' or reduce == 'add':\n        expected = expected - 2\n    elif reduce == 'mean':\n        expected = out  # We can not really test this here.\n    elif reduce == 'min':\n        expected = expected.fill_(-2)\n    elif reduce == 'max':\n        expected[expected == 0] = -2\n    else:\n        raise ValueError\n\n    assert torch.all(out == expected)\n\n\n@pytest.mark.parametrize('test,reduce,dtype,device',\n                         product(tests, reductions, dtypes, devices))\ndef test_non_contiguous(test, reduce, dtype, device):\n    src = tensor(test['src'], dtype, device)\n    index = tensor(test['index'], torch.long, device)\n    dim = test['dim']\n    expected = tensor(test[reduce], dtype, device)\n\n    if src.dim() > 1:\n        src = src.transpose(0, 1).contiguous().transpose(0, 1)\n    if index.dim() > 1:\n        index = index.transpose(0, 1).contiguous().transpose(0, 1)\n\n    out = getattr(torch_scatter, 'scatter_' + reduce)(src, index, dim)\n    if isinstance(out, tuple):\n        out, arg_out = out\n        arg_expected = tensor(test['arg_' + reduce], torch.long, device)\n        assert torch.all(arg_out == arg_expected)\n    assert torch.all(out == expected)\n"""
test/test_segment.py,25,"b""from itertools import product\n\nimport pytest\nimport torch\nfrom torch.autograd import gradcheck\nimport torch_scatter\n\nfrom .utils import reductions, tensor, dtypes, devices\n\ntests = [\n    {\n        'src': [1, 2, 3, 4, 5, 6],\n        'index': [0, 0, 1, 1, 1, 3],\n        'indptr': [0, 2, 5, 5, 6],\n        'sum': [3, 12, 0, 6],\n        'add': [3, 12, 0, 6],\n        'mean': [1.5, 4, 0, 6],\n        'min': [1, 3, 0, 6],\n        'arg_min': [0, 2, 6, 5],\n        'max': [2, 5, 0, 6],\n        'arg_max': [1, 4, 6, 5],\n    },\n    {\n        'src': [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12]],\n        'index': [0, 0, 1, 1, 1, 3],\n        'indptr': [0, 2, 5, 5, 6],\n        'sum': [[4, 6], [21, 24], [0, 0], [11, 12]],\n        'add': [[4, 6], [21, 24], [0, 0], [11, 12]],\n        'mean': [[2, 3], [7, 8], [0, 0], [11, 12]],\n        'min': [[1, 2], [5, 6], [0, 0], [11, 12]],\n        'arg_min': [[0, 0], [2, 2], [6, 6], [5, 5]],\n        'max': [[3, 4], [9, 10], [0, 0], [11, 12]],\n        'arg_max': [[1, 1], [4, 4], [6, 6], [5, 5]],\n    },\n    {\n        'src': [[1, 3, 5, 7, 9, 11], [2, 4, 6, 8, 10, 12]],\n        'index': [[0, 0, 1, 1, 1, 3], [0, 0, 0, 1, 1, 2]],\n        'indptr': [[0, 2, 5, 5, 6], [0, 3, 5, 6, 6]],\n        'sum': [[4, 21, 0, 11], [12, 18, 12, 0]],\n        'add': [[4, 21, 0, 11], [12, 18, 12, 0]],\n        'mean': [[2, 7, 0, 11], [4, 9, 12, 0]],\n        'min': [[1, 5, 0, 11], [2, 8, 12, 0]],\n        'arg_min': [[0, 2, 6, 5], [0, 3, 5, 6]],\n        'max': [[3, 9, 0, 11], [6, 10, 12, 0]],\n        'arg_max': [[1, 4, 6, 5], [2, 4, 5, 6]],\n    },\n    {\n        'src': [[[1, 2], [3, 4], [5, 6]], [[7, 9], [10, 11], [12, 13]]],\n        'index': [[0, 0, 1], [0, 2, 2]],\n        'indptr': [[0, 2, 3, 3], [0, 1, 1, 3]],\n        'sum': [[[4, 6], [5, 6], [0, 0]], [[7, 9], [0, 0], [22, 24]]],\n        'add': [[[4, 6], [5, 6], [0, 0]], [[7, 9], [0, 0], [22, 24]]],\n        'mean': [[[2, 3], [5, 6], [0, 0]], [[7, 9], [0, 0], [11, 12]]],\n        'min': [[[1, 2], [5, 6], [0, 0]], [[7, 9], [0, 0], [10, 11]]],\n        'arg_min': [[[0, 0], [2, 2], [3, 3]], [[0, 0], [3, 3], [1, 1]]],\n        'max': [[[3, 4], [5, 6], [0, 0]], [[7, 9], [0, 0], [12, 13]]],\n        'arg_max': [[[1, 1], [2, 2], [3, 3]], [[0, 0], [3, 3], [2, 2]]],\n    },\n    {\n        'src': [[1, 3], [2, 4]],\n        'index': [[0, 0], [0, 0]],\n        'indptr': [[0, 2], [0, 2]],\n        'sum': [[4], [6]],\n        'add': [[4], [6]],\n        'mean': [[2], [3]],\n        'min': [[1], [2]],\n        'arg_min': [[0], [0]],\n        'max': [[3], [4]],\n        'arg_max': [[1], [1]],\n    },\n    {\n        'src': [[[1, 1], [3, 3]], [[2, 2], [4, 4]]],\n        'index': [[0, 0], [0, 0]],\n        'indptr': [[0, 2], [0, 2]],\n        'sum': [[[4, 4]], [[6, 6]]],\n        'add': [[[4, 4]], [[6, 6]]],\n        'mean': [[[2, 2]], [[3, 3]]],\n        'min': [[[1, 1]], [[2, 2]]],\n        'arg_min': [[[0, 0]], [[0, 0]]],\n        'max': [[[3, 3]], [[4, 4]]],\n        'arg_max': [[[1, 1]], [[1, 1]]],\n    },\n]\n\n\n@pytest.mark.parametrize('test,reduce,dtype,device',\n                         product(tests, reductions, dtypes, devices))\ndef test_forward(test, reduce, dtype, device):\n    src = tensor(test['src'], dtype, device)\n    index = tensor(test['index'], torch.long, device)\n    indptr = tensor(test['indptr'], torch.long, device)\n    expected = tensor(test[reduce], dtype, device)\n\n    out = getattr(torch_scatter, 'segment_' + reduce + '_csr')(src, indptr)\n    if isinstance(out, tuple):\n        out, arg_out = out\n        arg_expected = tensor(test['arg_' + reduce], torch.long, device)\n        assert torch.all(arg_out == arg_expected)\n    assert torch.all(out == expected)\n\n    out = getattr(torch_scatter, 'segment_' + reduce + '_coo')(src, index)\n    if isinstance(out, tuple):\n        out, arg_out = out\n        arg_expected = tensor(test['arg_' + reduce], torch.long, device)\n        assert torch.all(arg_out == arg_expected)\n    assert torch.all(out == expected)\n\n\n@pytest.mark.parametrize('test,reduce,device',\n                         product(tests, reductions, devices))\ndef test_backward(test, reduce, device):\n    src = tensor(test['src'], torch.double, device)\n    src.requires_grad_()\n    index = tensor(test['index'], torch.long, device)\n    indptr = tensor(test['indptr'], torch.long, device)\n\n    assert gradcheck(torch_scatter.segment_csr, (src, indptr, None, reduce))\n    assert gradcheck(torch_scatter.segment_coo,\n                     (src, index, None, None, reduce))\n\n\n@pytest.mark.parametrize('test,reduce,dtype,device',\n                         product(tests, reductions, dtypes, devices))\ndef test_out(test, reduce, dtype, device):\n    src = tensor(test['src'], dtype, device)\n    index = tensor(test['index'], torch.long, device)\n    indptr = tensor(test['indptr'], torch.long, device)\n    expected = tensor(test[reduce], dtype, device)\n\n    out = torch.full_like(expected, -2)\n\n    getattr(torch_scatter, 'segment_' + reduce + '_csr')(src, indptr, out)\n    assert torch.all(out == expected)\n\n    out.fill_(-2)\n\n    getattr(torch_scatter, 'segment_' + reduce + '_coo')(src, index, out)\n\n    if reduce == 'sum' or reduce == 'add':\n        expected = expected - 2\n    elif reduce == 'mean':\n        expected = out  # We can not really test this here.\n    elif reduce == 'min':\n        expected = expected.fill_(-2)\n    elif reduce == 'max':\n        expected[expected == 0] = -2\n    else:\n        raise ValueError\n\n    assert torch.all(out == expected)\n\n\n@pytest.mark.parametrize('test,reduce,dtype,device',\n                         product(tests, reductions, dtypes, devices))\ndef test_non_contiguous(test, reduce, dtype, device):\n    src = tensor(test['src'], dtype, device)\n    index = tensor(test['index'], torch.long, device)\n    indptr = tensor(test['indptr'], torch.long, device)\n    expected = tensor(test[reduce], dtype, device)\n\n    if src.dim() > 1:\n        src = src.transpose(0, 1).contiguous().transpose(0, 1)\n    if index.dim() > 1:\n        index = index.transpose(0, 1).contiguous().transpose(0, 1)\n    if indptr.dim() > 1:\n        indptr = indptr.transpose(0, 1).contiguous().transpose(0, 1)\n\n    out = getattr(torch_scatter, 'segment_' + reduce + '_csr')(src, indptr)\n    if isinstance(out, tuple):\n        out, arg_out = out\n        arg_expected = tensor(test['arg_' + reduce], torch.long, device)\n        assert torch.all(arg_out == arg_expected)\n    assert torch.all(out == expected)\n\n    out = getattr(torch_scatter, 'segment_' + reduce + '_coo')(src, index)\n    if isinstance(out, tuple):\n        out, arg_out = out\n        arg_expected = tensor(test['arg_' + reduce], torch.long, device)\n        assert torch.all(arg_out == arg_expected)\n    assert torch.all(out == expected)\n"""
test/test_zero_tensors.py,8,"b""from itertools import product\n\nimport pytest\nimport torch\nfrom torch_scatter import scatter, segment_coo, gather_coo\nfrom torch_scatter import segment_csr, gather_csr\n\nfrom .utils import reductions, tensor, grad_dtypes, devices\n\n\n@pytest.mark.parametrize('reduce,dtype,device',\n                         product(reductions, grad_dtypes, devices))\ndef test_zero_elements(reduce, dtype, device):\n    x = torch.randn(0, 0, 0, 16, dtype=dtype, device=device,\n                    requires_grad=True)\n    index = tensor([], torch.long, device)\n    indptr = tensor([], torch.long, device)\n\n    out = scatter(x, index, dim=0, dim_size=0, reduce=reduce)\n    out.backward(torch.randn_like(out))\n    assert out.size() == (0, 0, 0, 16)\n\n    out = segment_coo(x, index, dim_size=0, reduce=reduce)\n    out.backward(torch.randn_like(out))\n    assert out.size() == (0, 0, 0, 16)\n\n    out = gather_coo(x, index)\n    out.backward(torch.randn_like(out))\n    assert out.size() == (0, 0, 0, 16)\n\n    out = segment_csr(x, indptr, reduce=reduce)\n    out.backward(torch.randn_like(out))\n    assert out.size() == (0, 0, 0, 16)\n\n    out = gather_csr(x, indptr)\n    out.backward(torch.randn_like(out))\n    assert out.size() == (0, 0, 0, 16)\n"""
test/utils.py,6,"b""import torch\n\nreductions = ['sum', 'add', 'mean', 'min', 'max']\n\ndtypes = [torch.float, torch.double, torch.int, torch.long]\ngrad_dtypes = [torch.float, torch.double]\n\ndevices = [torch.device('cpu')]\nif torch.cuda.is_available():\n    devices += [torch.device(f'cuda:{torch.cuda.current_device()}')]\n\n\ndef tensor(x, dtype, device):\n    return None if x is None else torch.tensor(x, device=device).to(dtype)\n"""
torch_scatter/__init__.py,17,"b""import os\nimport importlib\nimport os.path as osp\n\nimport torch\n\n__version__ = '2.0.4'\n\ntry:\n    for library in ['_version', '_scatter', '_segment_csr', '_segment_coo']:\n        torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n            library, [osp.dirname(__file__)]).origin)\nexcept AttributeError as e:\n    if os.getenv('BUILD_DOCS', '0') != '1':\n        raise AttributeError(e)\n\n    from .placeholder import cuda_version_placeholder\n    torch.ops.torch_scatter.cuda_version = cuda_version_placeholder\n\n    from .placeholder import scatter_arg_placeholder\n    torch.ops.torch_scatter.scatter_min = scatter_arg_placeholder\n    torch.ops.torch_scatter.scatter_max = scatter_arg_placeholder\n\n    from .placeholder import segment_csr_placeholder\n    from .placeholder import segment_csr_arg_placeholder\n    from .placeholder import gather_csr_placeholder\n    torch.ops.torch_scatter.segment_sum_csr = segment_csr_placeholder\n    torch.ops.torch_scatter.segment_mean_csr = segment_csr_placeholder\n    torch.ops.torch_scatter.segment_min_csr = segment_csr_arg_placeholder\n    torch.ops.torch_scatter.segment_max_csr = segment_csr_arg_placeholder\n    torch.ops.torch_scatter.gather_csr = gather_csr_placeholder\n\n    from .placeholder import segment_coo_placeholder\n    from .placeholder import segment_coo_arg_placeholder\n    from .placeholder import gather_coo_placeholder\n    torch.ops.torch_scatter.segment_sum_coo = segment_coo_placeholder\n    torch.ops.torch_scatter.segment_mean_coo = segment_coo_placeholder\n    torch.ops.torch_scatter.segment_min_coo = segment_coo_arg_placeholder\n    torch.ops.torch_scatter.segment_max_coo = segment_coo_arg_placeholder\n    torch.ops.torch_scatter.gather_coo = gather_coo_placeholder\n\nif torch.version.cuda is not None:  # pragma: no cover\n    cuda_version = torch.ops.torch_scatter.cuda_version()\n\n    if cuda_version == -1:\n        major = minor = 0\n    elif cuda_version < 10000:\n        major, minor = int(str(cuda_version)[0]), int(str(cuda_version)[2])\n    else:\n        major, minor = int(str(cuda_version)[0:2]), int(str(cuda_version)[3])\n    t_major, t_minor = [int(x) for x in torch.version.cuda.split('.')]\n\n    if t_major != major or t_minor != minor:\n        raise RuntimeError(\n            f'Detected that PyTorch and torch_scatter were compiled with '\n            f'different CUDA versions. PyTorch has CUDA version '\n            f'{t_major}.{t_minor} and torch_scatter has CUDA version '\n            f'{major}.{minor}. Please reinstall the torch_scatter that '\n            f'matches your PyTorch install.')\n\nfrom .scatter import (scatter_sum, scatter_add, scatter_mean, scatter_min,\n                      scatter_max, scatter)  # noqa\nfrom .segment_csr import (segment_sum_csr, segment_add_csr, segment_mean_csr,\n                          segment_min_csr, segment_max_csr, segment_csr,\n                          gather_csr)  # noqa\nfrom .segment_coo import (segment_sum_coo, segment_add_coo, segment_mean_coo,\n                          segment_min_coo, segment_max_coo, segment_coo,\n                          gather_coo)  # noqa\nfrom .composite import (scatter_std, scatter_logsumexp, scatter_softmax,\n                        scatter_log_softmax)  # noqa\n\n__all__ = [\n    'scatter_sum',\n    'scatter_add',\n    'scatter_mean',\n    'scatter_min',\n    'scatter_max',\n    'scatter',\n    'segment_sum_csr',\n    'segment_add_csr',\n    'segment_mean_csr',\n    'segment_min_csr',\n    'segment_max_csr',\n    'segment_csr',\n    'gather_csr',\n    'segment_sum_coo',\n    'segment_add_coo',\n    'segment_mean_coo',\n    'segment_min_coo',\n    'segment_max_coo',\n    'segment_coo',\n    'gather_coo',\n    'scatter_std',\n    'scatter_logsumexp',\n    'scatter_softmax',\n    'scatter_log_softmax',\n    'torch_scatter',\n    '__version__',\n]\n"""
torch_scatter/placeholder.py,21,"b'from typing import Optional, Tuple\n\nimport torch\n\n\ndef cuda_version_placeholder() -> int:\n    return -1\n\n\ndef scatter_placeholder(src: torch.Tensor, index: torch.Tensor, dim: int,\n                        out: Optional[torch.Tensor],\n                        dim_size: Optional[int]) -> torch.Tensor:\n    raise ImportError\n    return src\n\n\ndef scatter_arg_placeholder(src: torch.Tensor, index: torch.Tensor, dim: int,\n                            out: Optional[torch.Tensor],\n                            dim_size: Optional[int]\n                            ) -> Tuple[torch.Tensor, torch.Tensor]:\n    raise ImportError\n    return src, index\n\n\ndef segment_csr_placeholder(src: torch.Tensor, indptr: torch.Tensor,\n                            out: Optional[torch.Tensor]) -> torch.Tensor:\n    raise ImportError\n    return src\n\n\ndef segment_csr_arg_placeholder(src: torch.Tensor, indptr: torch.Tensor,\n                                out: Optional[torch.Tensor]\n                                ) -> Tuple[torch.Tensor, torch.Tensor]:\n    raise ImportError\n    return src, indptr\n\n\ndef gather_csr_placeholder(src: torch.Tensor, indptr: torch.Tensor,\n                           out: Optional[torch.Tensor]) -> torch.Tensor:\n    raise ImportError\n    return src\n\n\ndef segment_coo_placeholder(src: torch.Tensor, index: torch.Tensor,\n                            out: Optional[torch.Tensor],\n                            dim_size: Optional[int]) -> torch.Tensor:\n    raise ImportError\n    return src\n\n\ndef segment_coo_arg_placeholder(src: torch.Tensor, index: torch.Tensor,\n                                out: Optional[torch.Tensor],\n                                dim_size: Optional[int]\n                                ) -> Tuple[torch.Tensor, torch.Tensor]:\n    raise ImportError\n    return src, index\n\n\ndef gather_coo_placeholder(src: torch.Tensor, index: torch.Tensor,\n                           out: Optional[torch.Tensor]) -> torch.Tensor:\n    raise ImportError\n    return src\n'"
torch_scatter/scatter.py,31,"b'from typing import Optional, Tuple\n\nimport torch\n\nfrom .utils import broadcast\n\n\n@torch.jit.script\ndef scatter_sum(src: torch.Tensor, index: torch.Tensor, dim: int = -1,\n                out: Optional[torch.Tensor] = None,\n                dim_size: Optional[int] = None) -> torch.Tensor:\n    index = broadcast(index, src, dim)\n    if out is None:\n        size = src.size()\n        if dim_size is not None:\n            size[dim] = dim_size\n        elif index.numel() == 0:\n            size[dim] = 0\n        else:\n            size[dim] = int(index.max()) + 1\n        out = torch.zeros(size, dtype=src.dtype, device=src.device)\n        return out.scatter_add_(dim, index, src)\n    else:\n        return out.scatter_add_(dim, index, src)\n\n\n@torch.jit.script\ndef scatter_add(src: torch.Tensor, index: torch.Tensor, dim: int = -1,\n                out: Optional[torch.Tensor] = None,\n                dim_size: Optional[int] = None) -> torch.Tensor:\n    return scatter_sum(src, index, dim, out, dim_size)\n\n\n@torch.jit.script\ndef scatter_mean(src: torch.Tensor, index: torch.Tensor, dim: int = -1,\n                 out: Optional[torch.Tensor] = None,\n                 dim_size: Optional[int] = None) -> torch.Tensor:\n\n    out = scatter_sum(src, index, dim, out, dim_size)\n    dim_size = out.size(dim)\n\n    index_dim = dim\n    if index_dim < 0:\n        index_dim = index_dim + src.dim()\n    if index.dim() <= index_dim:\n        index_dim = index.dim() - 1\n\n    ones = torch.ones(index.size(), dtype=src.dtype, device=src.device)\n    count = scatter_sum(ones, index, index_dim, None, dim_size)\n    count.clamp_(1)\n    count = broadcast(count, out, dim)\n    if torch.is_floating_point(out):\n        out.true_divide_(count)\n    else:\n        out.floor_divide_(count)\n    return out\n\n\n@torch.jit.script\ndef scatter_min(src: torch.Tensor, index: torch.Tensor, dim: int = -1,\n                out: Optional[torch.Tensor] = None,\n                dim_size: Optional[int] = None\n                ) -> Tuple[torch.Tensor, torch.Tensor]:\n    return torch.ops.torch_scatter.scatter_min(src, index, dim, out, dim_size)\n\n\n@torch.jit.script\ndef scatter_max(src: torch.Tensor, index: torch.Tensor, dim: int = -1,\n                out: Optional[torch.Tensor] = None,\n                dim_size: Optional[int] = None\n                ) -> Tuple[torch.Tensor, torch.Tensor]:\n    return torch.ops.torch_scatter.scatter_max(src, index, dim, out, dim_size)\n\n\ndef scatter(src: torch.Tensor, index: torch.Tensor, dim: int = -1,\n            out: Optional[torch.Tensor] = None, dim_size: Optional[int] = None,\n            reduce: str = ""sum"") -> torch.Tensor:\n    r""""""\n    |\n\n    .. image:: https://raw.githubusercontent.com/rusty1s/pytorch_scatter/\n            master/docs/source/_figures/add.svg?sanitize=true\n        :align: center\n        :width: 400px\n\n    |\n\n    Reduces all values from the :attr:`src` tensor into :attr:`out` at the\n    indices specified in the :attr:`index` tensor along a given axis\n    :attr:`dim`.\n    For each value in :attr:`src`, its output index is specified by its index\n    in :attr:`src` for dimensions outside of :attr:`dim` and by the\n    corresponding value in :attr:`index` for dimension :attr:`dim`.\n    The applied reduction is defined via the :attr:`reduce` argument.\n\n    Formally, if :attr:`src` and :attr:`index` are :math:`n`-dimensional\n    tensors with size :math:`(x_0, ..., x_{i-1}, x_i, x_{i+1}, ..., x_{n-1})`\n    and :attr:`dim` = `i`, then :attr:`out` must be an :math:`n`-dimensional\n    tensor with size :math:`(x_0, ..., x_{i-1}, y, x_{i+1}, ..., x_{n-1})`.\n    Moreover, the values of :attr:`index` must be between :math:`0` and\n    :math:`y - 1` in ascending order.\n    The :attr:`index` tensor supports broadcasting in case its dimensions do\n    not match with :attr:`src`.\n\n    For one-dimensional tensors with :obj:`reduce=""sum""`, the operation\n    computes\n\n    .. math::\n        \\mathrm{out}_i = \\mathrm{out}_i + \\sum_j~\\mathrm{src}_j\n\n    where :math:`\\sum_j` is over :math:`j` such that\n    :math:`\\mathrm{index}_j = i`.\n\n    .. note::\n\n        This operation is implemented via atomic operations on the GPU and is\n        therefore **non-deterministic** since the order of parallel operations\n        to the same value is undetermined.\n        For floating-point variables, this results in a source of variance in\n        the result.\n\n    :param src: The source tensor.\n    :param index: The indices of elements to scatter.\n    :param dim: The axis along which to index. (default: :obj:`-1`)\n    :param out: The destination tensor.\n    :param dim_size: If :attr:`out` is not given, automatically create output\n        with size :attr:`dim_size` at dimension :attr:`dim`.\n        If :attr:`dim_size` is not given, a minimal sized output tensor\n        according to :obj:`index.max() + 1` is returned.\n    :param reduce: The reduce operation (:obj:`""sum""`, :obj:`""mean""`,\n        :obj:`""min""` or :obj:`""max""`). (default: :obj:`""sum""`)\n\n    :rtype: :class:`Tensor`\n\n    .. code-block:: python\n\n        from torch_scatter import scatter\n\n        src = torch.randn(10, 6, 64)\n        index = torch.tensor([0, 1, 0, 1, 2, 1])\n\n        # Broadcasting in the first and last dim.\n        out = scatter(src, index, dim=1, reduce=""sum"")\n\n        print(out.size())\n\n    .. code-block::\n\n        torch.Size([10, 3, 64])\n    """"""\n    if reduce == \'sum\' or reduce == \'add\':\n        return scatter_sum(src, index, dim, out, dim_size)\n    elif reduce == \'mean\':\n        return scatter_mean(src, index, dim, out, dim_size)\n    elif reduce == \'min\':\n        return scatter_min(src, index, dim, out, dim_size)[0]\n    elif reduce == \'max\':\n        return scatter_max(src, index, dim, out, dim_size)[0]\n    else:\n        raise ValueError\n'"
torch_scatter/segment_coo.py,35,"b'from typing import Optional, Tuple\n\nimport torch\n\n\n@torch.jit.script\ndef segment_sum_coo(src: torch.Tensor, index: torch.Tensor,\n                    out: Optional[torch.Tensor] = None,\n                    dim_size: Optional[int] = None) -> torch.Tensor:\n    return torch.ops.torch_scatter.segment_sum_coo(src, index, out, dim_size)\n\n\n@torch.jit.script\ndef segment_add_coo(src: torch.Tensor, index: torch.Tensor,\n                    out: Optional[torch.Tensor] = None,\n                    dim_size: Optional[int] = None) -> torch.Tensor:\n    return torch.ops.torch_scatter.segment_sum_coo(src, index, out, dim_size)\n\n\n@torch.jit.script\ndef segment_mean_coo(src: torch.Tensor, index: torch.Tensor,\n                     out: Optional[torch.Tensor] = None,\n                     dim_size: Optional[int] = None) -> torch.Tensor:\n    return torch.ops.torch_scatter.segment_mean_coo(src, index, out, dim_size)\n\n\n@torch.jit.script\ndef segment_min_coo(src: torch.Tensor, index: torch.Tensor,\n                    out: Optional[torch.Tensor] = None,\n                    dim_size: Optional[int] = None\n                    ) -> Tuple[torch.Tensor, torch.Tensor]:\n    return torch.ops.torch_scatter.segment_min_coo(src, index, out, dim_size)\n\n\n@torch.jit.script\ndef segment_max_coo(src: torch.Tensor, index: torch.Tensor,\n                    out: Optional[torch.Tensor] = None,\n                    dim_size: Optional[int] = None\n                    ) -> Tuple[torch.Tensor, torch.Tensor]:\n    return torch.ops.torch_scatter.segment_max_coo(src, index, out, dim_size)\n\n\ndef segment_coo(src: torch.Tensor, index: torch.Tensor,\n                out: Optional[torch.Tensor] = None,\n                dim_size: Optional[int] = None,\n                reduce: str = ""sum"") -> torch.Tensor:\n    r""""""\n    |\n\n    .. image:: https://raw.githubusercontent.com/rusty1s/pytorch_scatter/\n            master/docs/source/_figures/segment_coo.svg?sanitize=true\n        :align: center\n        :width: 400px\n\n    |\n\n    Reduces all values from the :attr:`src` tensor into :attr:`out` at the\n    indices specified in the :attr:`index` tensor along the last dimension of\n    :attr:`index`.\n    For each value in :attr:`src`, its output index is specified by its index\n    in :attr:`src` for dimensions outside of :obj:`index.dim() - 1` and by the\n    corresponding value in :attr:`index` for dimension :obj:`index.dim() - 1`.\n    The applied reduction is defined via the :attr:`reduce` argument.\n\n    Formally, if :attr:`src` and :attr:`index` are :math:`n`-dimensional and\n    :math:`m`-dimensional tensors with\n    size :math:`(x_0, ..., x_{m-1}, x_m, x_{m+1}, ..., x_{n-1})` and\n    :math:`(x_0, ..., x_{m-1}, x_m)`, respectively, then :attr:`out` must be an\n    :math:`n`-dimensional tensor with size\n    :math:`(x_0, ..., x_{m-1}, y, x_{m+1}, ..., x_{n-1})`.\n    Moreover, the values of :attr:`index` must be between :math:`0` and\n    :math:`y - 1` in ascending order.\n    The :attr:`index` tensor supports broadcasting in case its dimensions do\n    not match with :attr:`src`.\n\n    For one-dimensional tensors with :obj:`reduce=""sum""`, the operation\n    computes\n\n    .. math::\n        \\mathrm{out}_i = \\mathrm{out}_i + \\sum_j~\\mathrm{src}_j\n\n    where :math:`\\sum_j` is over :math:`j` such that\n    :math:`\\mathrm{index}_j = i`.\n\n    In contrast to :meth:`scatter`, this method expects values in :attr:`index`\n    **to be sorted** along dimension :obj:`index.dim() - 1`.\n    Due to the use of sorted indices, :meth:`segment_coo` is usually faster\n    than the more general :meth:`scatter` operation.\n\n    .. note::\n\n        This operation is implemented via atomic operations on the GPU and is\n        therefore **non-deterministic** since the order of parallel operations\n        to the same value is undetermined.\n        For floating-point variables, this results in a source of variance in\n        the result.\n\n    :param src: The source tensor.\n    :param index: The sorted indices of elements to segment.\n        The number of dimensions of :attr:`index` needs to be less than or\n        equal to :attr:`src`.\n    :param out: The destination tensor.\n    :param dim_size: If :attr:`out` is not given, automatically create output\n        with size :attr:`dim_size` at dimension :obj:`index.dim() - 1`.\n        If :attr:`dim_size` is not given, a minimal sized output tensor\n        according to :obj:`index.max() + 1` is returned.\n    :param reduce: The reduce operation (:obj:`""sum""`, :obj:`""mean""`,\n        :obj:`""min""` or :obj:`""max""`). (default: :obj:`""sum""`)\n\n    :rtype: :class:`Tensor`\n\n    .. code-block:: python\n\n        from torch_scatter import segment_coo\n\n        src = torch.randn(10, 6, 64)\n        index = torch.tensor([0, 0, 1, 1, 1, 2])\n        index = index.view(1, -1)  # Broadcasting in the first and last dim.\n\n        out = segment_coo(src, index, reduce=""sum"")\n\n        print(out.size())\n\n    .. code-block::\n\n        torch.Size([10, 3, 64])\n    """"""\n    if reduce == \'sum\' or reduce == \'add\':\n        return segment_sum_coo(src, index, out, dim_size)\n    elif reduce == \'mean\':\n        return segment_mean_coo(src, index, out, dim_size)\n    elif reduce == \'min\':\n        return segment_min_coo(src, index, out, dim_size)[0]\n    elif reduce == \'max\':\n        return segment_max_coo(src, index, out, dim_size)[0]\n    else:\n        raise ValueError\n\n\n@torch.jit.script\ndef gather_coo(src: torch.Tensor, index: torch.Tensor,\n               out: Optional[torch.Tensor] = None) -> torch.Tensor:\n    return torch.ops.torch_scatter.gather_coo(src, index, out)\n'"
torch_scatter/segment_csr.py,32,"b'from typing import Optional, Tuple\n\nimport torch\n\n\n@torch.jit.script\ndef segment_sum_csr(src: torch.Tensor, indptr: torch.Tensor,\n                    out: Optional[torch.Tensor] = None) -> torch.Tensor:\n    return torch.ops.torch_scatter.segment_sum_csr(src, indptr, out)\n\n\n@torch.jit.script\ndef segment_add_csr(src: torch.Tensor, indptr: torch.Tensor,\n                    out: Optional[torch.Tensor] = None) -> torch.Tensor:\n    return torch.ops.torch_scatter.segment_sum_csr(src, indptr, out)\n\n\n@torch.jit.script\ndef segment_mean_csr(src: torch.Tensor, indptr: torch.Tensor,\n                     out: Optional[torch.Tensor] = None) -> torch.Tensor:\n    return torch.ops.torch_scatter.segment_mean_csr(src, indptr, out)\n\n\n@torch.jit.script\ndef segment_min_csr(src: torch.Tensor, indptr: torch.Tensor,\n                    out: Optional[torch.Tensor] = None\n                    ) -> Tuple[torch.Tensor, torch.Tensor]:\n    return torch.ops.torch_scatter.segment_min_csr(src, indptr, out)\n\n\n@torch.jit.script\ndef segment_max_csr(src: torch.Tensor, indptr: torch.Tensor,\n                    out: Optional[torch.Tensor] = None\n                    ) -> Tuple[torch.Tensor, torch.Tensor]:\n    return torch.ops.torch_scatter.segment_max_csr(src, indptr, out)\n\n\ndef segment_csr(src: torch.Tensor, indptr: torch.Tensor,\n                out: Optional[torch.Tensor] = None,\n                reduce: str = ""sum"") -> torch.Tensor:\n    r""""""\n    Reduces all values from the :attr:`src` tensor into :attr:`out` within the\n    ranges specified in the :attr:`indptr` tensor along the last dimension of\n    :attr:`indptr`.\n    For each value in :attr:`src`, its output index is specified by its index\n    in :attr:`src` for dimensions outside of :obj:`indptr.dim() - 1` and by the\n    corresponding range index in :attr:`indptr` for dimension\n    :obj:`indptr.dim() - 1`.\n    The applied reduction is defined via the :attr:`reduce` argument.\n\n    Formally, if :attr:`src` and :attr:`indptr` are :math:`n`-dimensional and\n    :math:`m`-dimensional tensors with\n    size :math:`(x_0, ..., x_{m-1}, x_m, x_{m+1}, ..., x_{n-1})` and\n    :math:`(x_0, ..., x_{m-1}, y)`, respectively, then :attr:`out` must be an\n    :math:`n`-dimensional tensor with size\n    :math:`(x_0, ..., x_{m-1}, y - 1, x_{m+1}, ..., x_{n-1})`.\n    Moreover, the values of :attr:`indptr` must be between :math:`0` and\n    :math:`x_m` in ascending order.\n    The :attr:`indptr` tensor supports broadcasting in case its dimensions do\n    not match with :attr:`src`.\n\n    For one-dimensional tensors with :obj:`reduce=""sum""`, the operation\n    computes\n\n    .. math::\n        \\mathrm{out}_i =\n        \\sum_{j = \\mathrm{indptr}[i]}^{\\mathrm{indptr}[i+i]}~\\mathrm{src}_j.\n\n    Due to the use of index pointers, :meth:`segment_csr` is the fastest\n    method to apply for grouped reductions.\n\n    .. note::\n\n        In contrast to :meth:`scatter()` and :meth:`segment_coo`, this\n        operation is **fully-deterministic**.\n\n    :param src: The source tensor.\n    :param indptr: The index pointers between elements to segment.\n        The number of dimensions of :attr:`index` needs to be less than or\n        equal to :attr:`src`.\n    :param out: The destination tensor.\n    :param reduce: The reduce operation (:obj:`""sum""`, :obj:`""mean""`,\n        :obj:`""min""` or :obj:`""max""`). (default: :obj:`""sum""`)\n\n    :rtype: :class:`Tensor`\n\n    .. code-block:: python\n\n        from torch_scatter import segment_csr\n\n        src = torch.randn(10, 6, 64)\n        indptr = torch.tensor([0, 2, 5, 6])\n        indptr = indptr.view(1, -1)  # Broadcasting in the first and last dim.\n\n        out = segment_csr(src, indptr, reduce=""sum"")\n\n        print(out.size())\n\n    .. code-block::\n\n        torch.Size([10, 3, 64])\n    """"""\n    if reduce == \'sum\' or reduce == \'add\':\n        return segment_sum_csr(src, indptr, out)\n    elif reduce == \'mean\':\n        return segment_mean_csr(src, indptr, out)\n    elif reduce == \'min\':\n        return segment_min_csr(src, indptr, out)[0]\n    elif reduce == \'max\':\n        return segment_max_csr(src, indptr, out)[0]\n    else:\n        raise ValueError\n\n\n@torch.jit.script\ndef gather_csr(src: torch.Tensor, indptr: torch.Tensor,\n               out: Optional[torch.Tensor] = None) -> torch.Tensor:\n    return torch.ops.torch_scatter.gather_csr(src, indptr, out)\n'"
torch_scatter/utils.py,2,"b'import torch\n\n\n@torch.jit.script\ndef broadcast(src: torch.Tensor, other: torch.Tensor, dim: int):\n    if dim < 0:\n        dim = other.dim() + dim\n    if src.dim() == 1:\n        for _ in range(0, dim):\n            src = src.unsqueeze(0)\n    for _ in range(src.dim(), other.dim()):\n        src = src.unsqueeze(-1)\n    src = src.expand_as(other)\n    return src\n'"
docs/source/conf.py,0,"b""import datetime\nimport sphinx_rtd_theme\nimport doctest\nimport torch_scatter\n\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.doctest',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.mathjax',\n    'sphinx.ext.napoleon',\n    'sphinx.ext.viewcode',\n    'sphinx.ext.githubpages',\n    'sphinx_autodoc_typehints',\n]\n\nsource_suffix = '.rst'\nmaster_doc = 'index'\n\nauthor = 'Matthias Fey'\nproject = 'pytorch_scatter'\ncopyright = '{}, {}'.format(datetime.datetime.now().year, author)\n\nversion = torch_scatter.__version__\nrelease = torch_scatter.__version__\n\nhtml_theme = 'sphinx_rtd_theme'\nhtml_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\ndoctest_default_flags = doctest.NORMALIZE_WHITESPACE\nintersphinx_mapping = {'python': ('https://docs.python.org/', None)}\n"""
test/composite/test_logsumexp.py,4,"b""import torch\nfrom torch_scatter import scatter_logsumexp\n\n\ndef test_logsumexp():\n    inputs = torch.tensor([\n        0.5, 0.5, 0.0, -2.1, 3.2, 7.0, -1.0, -100.0,\n        float('-inf'),\n        float('-inf'), 0.0\n    ])\n    inputs.requires_grad_()\n    index = torch.tensor([0, 0, 1, 1, 1, 2, 4, 4, 5, 6, 6])\n    splits = [2, 3, 1, 0, 2, 1, 2]\n\n    outputs = scatter_logsumexp(inputs, index)\n\n    for src, out in zip(inputs.split(splits), outputs.unbind()):\n        assert out.tolist() == torch.logsumexp(src, dim=0).tolist()\n\n    outputs.backward(torch.randn_like(outputs))\n"""
test/composite/test_softmax.py,18,"b""import torch\nfrom torch_scatter import scatter_log_softmax, scatter_softmax\n\n\ndef test_softmax():\n    src = torch.tensor([0.2, 0, 0.2, -2.1, 3.2, 7, -1, float('-inf')])\n    src.requires_grad_()\n    index = torch.tensor([0, 1, 0, 1, 1, 2, 4, 4])\n\n    out = scatter_softmax(src, index)\n\n    out0 = torch.softmax(torch.tensor([0.2, 0.2]), dim=-1)\n    out1 = torch.softmax(torch.tensor([0, -2.1, 3.2]), dim=-1)\n    out2 = torch.softmax(torch.tensor([7], dtype=torch.float), dim=-1)\n    out4 = torch.softmax(torch.tensor([-1, float('-inf')]), dim=-1)\n\n    expected = torch.stack([\n        out0[0], out1[0], out0[1], out1[1], out1[2], out2[0], out4[0], out4[1]\n    ], dim=0)\n\n    assert torch.allclose(out, expected)\n\n    out.backward(torch.randn_like(out))\n\n\ndef test_log_softmax():\n    src = torch.tensor([0.2, 0, 0.2, -2.1, 3.2, 7, -1, float('-inf')])\n    src.requires_grad_()\n    index = torch.tensor([0, 1, 0, 1, 1, 2, 4, 4])\n\n    out = scatter_log_softmax(src, index)\n\n    out0 = torch.log_softmax(torch.tensor([0.2, 0.2]), dim=-1)\n    out1 = torch.log_softmax(torch.tensor([0, -2.1, 3.2]), dim=-1)\n    out2 = torch.log_softmax(torch.tensor([7], dtype=torch.float), dim=-1)\n    out4 = torch.log_softmax(torch.tensor([-1, float('-inf')]), dim=-1)\n\n    expected = torch.stack([\n        out0[0], out1[0], out0[1], out1[1], out1[2], out2[0], out4[0], out4[1]\n    ], dim=0)\n\n    assert torch.allclose(out, expected)\n\n    out.backward(torch.randn_like(out))\n"""
test/composite/test_std.py,5,"b'import torch\nfrom torch_scatter import scatter_std\n\n\ndef test_std():\n    src = torch.tensor([[2, 0, 1, 4, 3], [0, 2, 1, 3, 4]], dtype=torch.float)\n    src.requires_grad_()\n    index = torch.tensor([[0, 0, 0, 0, 0], [1, 1, 1, 1, 1]], dtype=torch.long)\n\n    out = scatter_std(src, index, dim=-1, unbiased=True)\n    std = src.std(dim=-1, unbiased=True)[0]\n    expected = torch.tensor([[std, 0], [0, std]])\n    assert torch.allclose(out, expected)\n\n    out.backward(torch.randn_like(out))\n'"
torch_scatter/composite/__init__.py,0,"b""from .std import scatter_std\nfrom .logsumexp import scatter_logsumexp\nfrom .softmax import scatter_log_softmax, scatter_softmax\n\n__all__ = [\n    'scatter_std',\n    'scatter_logsumexp',\n    'scatter_softmax',\n    'scatter_log_softmax',\n]\n"""
torch_scatter/composite/logsumexp.py,7,"b""from typing import Optional\n\nimport torch\nfrom torch_scatter import scatter_sum, scatter_max\n\nfrom torch_scatter.utils import broadcast\n\n\n@torch.jit.script\ndef scatter_logsumexp(src: torch.Tensor, index: torch.Tensor, dim: int = -1,\n                      out: Optional[torch.Tensor] = None,\n                      dim_size: Optional[int] = None,\n                      eps: float = 1e-12) -> torch.Tensor:\n    if not torch.is_floating_point(src):\n        raise ValueError('`scatter_logsumexp` can only be computed over '\n                         'tensors with floating point data types.')\n\n    index = broadcast(index, src, dim)\n\n    if out is not None:\n        dim_size = out.size(dim)\n    else:\n        if dim_size is None:\n            dim_size = int(index.max()) + 1\n\n    size = list(src.size())\n    size[dim] = dim_size\n    max_value_per_index = torch.full(size, float('-inf'), dtype=src.dtype,\n                                     device=src.device)\n    scatter_max(src, index, dim, max_value_per_index, dim_size=dim_size)[0]\n    max_per_src_element = max_value_per_index.gather(dim, index)\n    recentered_score = src - max_per_src_element\n    recentered_score.masked_fill_(torch.isnan(recentered_score), float('-inf'))\n\n    if out is not None:\n        out = out.sub(max_per_src_element).exp()\n\n    sum_per_index = scatter_sum(recentered_score.exp_(), index, dim, out,\n                                dim_size)\n\n    return sum_per_index.add_(eps).log_().add_(max_value_per_index)\n"""
torch_scatter/composite/softmax.py,8,"b""import torch\n\nfrom torch_scatter import scatter_sum, scatter_max\nfrom torch_scatter.utils import broadcast\n\n\n@torch.jit.script\ndef scatter_softmax(src: torch.Tensor, index: torch.Tensor, dim: int = -1,\n                    eps: float = 1e-12) -> torch.Tensor:\n    if not torch.is_floating_point(src):\n        raise ValueError('`scatter_softmax` can only be computed over tensors '\n                         'with floating point data types.')\n\n    index = broadcast(index, src, dim)\n\n    max_value_per_index = scatter_max(src, index, dim=dim)[0]\n    max_per_src_element = max_value_per_index.gather(dim, index)\n\n    recentered_scores = src - max_per_src_element\n    recentered_scores_exp = recentered_scores.exp()\n\n    sum_per_index = scatter_sum(recentered_scores_exp, index, dim)\n    normalizing_constants = sum_per_index.add_(eps).gather(dim, index)\n\n    return recentered_scores_exp.div(normalizing_constants)\n\n\n@torch.jit.script\ndef scatter_log_softmax(src: torch.Tensor, index: torch.Tensor, dim: int = -1,\n                        eps: float = 1e-12) -> torch.Tensor:\n    if not torch.is_floating_point(src):\n        raise ValueError('`scatter_log_softmax` can only be computed over '\n                         'tensors with floating point data types.')\n\n    index = broadcast(index, src, dim)\n\n    max_value_per_index = scatter_max(src, index, dim=dim)[0]\n    max_per_src_element = max_value_per_index.gather(dim, index)\n\n    recentered_scores = src - max_per_src_element\n\n    sum_per_index = scatter_sum(recentered_scores.exp(), index, dim)\n    normalizing_constants = sum_per_index.add_(eps).log_().gather(dim, index)\n\n    return recentered_scores.sub_(normalizing_constants)\n"""
torch_scatter/composite/std.py,5,"b'from typing import Optional\n\nimport torch\nfrom torch_scatter import scatter_sum\nfrom torch_scatter.utils import broadcast\n\n\n@torch.jit.script\ndef scatter_std(src: torch.Tensor, index: torch.Tensor, dim: int = -1,\n                out: Optional[torch.Tensor] = None,\n                dim_size: Optional[int] = None,\n                unbiased: bool = True) -> torch.Tensor:\n\n    if out is not None:\n        dim_size = out.size(dim)\n\n    if dim < 0:\n        dim = src.dim() + dim\n\n    count_dim = dim\n    if index.dim() <= dim:\n        count_dim = index.dim() - 1\n\n    ones = torch.ones(index.size(), dtype=src.dtype, device=src.device)\n    count = scatter_sum(ones, index, count_dim, dim_size=dim_size)\n\n    index = broadcast(index, src, dim)\n    tmp = scatter_sum(src, index, dim, dim_size=dim_size)\n    count = broadcast(count, tmp, dim).clamp_(1)\n    mean = tmp.div(count)\n\n    var = (src - mean.gather(dim, index))\n    var = var * var\n    out = scatter_sum(var, index, dim, out, dim_size)\n\n    if unbiased:\n        count = count.sub(1).clamp_(1)\n    out = out.div(count).sqrt()\n\n    return out\n'"
