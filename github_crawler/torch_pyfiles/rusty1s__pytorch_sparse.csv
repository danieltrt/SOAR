file_path,api_count,code
setup.py,3,"b""import os\nimport os.path as osp\nimport sys\nimport glob\nfrom setuptools import setup, find_packages\n\nimport torch\nfrom torch.utils.cpp_extension import BuildExtension\nfrom torch.utils.cpp_extension import CppExtension, CUDAExtension, CUDA_HOME\n\nWITH_CUDA = torch.cuda.is_available() and CUDA_HOME is not None\nif os.getenv('FORCE_CUDA', '0') == '1':\n    WITH_CUDA = True\nif os.getenv('FORCE_CPU', '0') == '1':\n    WITH_CUDA = False\n\nBUILD_DOCS = os.getenv('BUILD_DOCS', '0') == '1'\n\nWITH_METIS = False\nif os.getenv('WITH_METIS', '0') == '1':\n    WITH_METIS = True\n\n\ndef get_extensions():\n    Extension = CppExtension\n    define_macros = []\n    libraries = []\n    if WITH_METIS:\n        define_macros += [('WITH_METIS', None)]\n        libraries += ['metis']\n    extra_compile_args = {'cxx': []}\n    extra_link_args = []\n\n    if WITH_CUDA:\n        Extension = CUDAExtension\n        define_macros += [('WITH_CUDA', None)]\n        nvcc_flags = os.getenv('NVCC_FLAGS', '')\n        nvcc_flags = [] if nvcc_flags == '' else nvcc_flags.split(' ')\n        nvcc_flags += ['-arch=sm_35', '--expt-relaxed-constexpr']\n        extra_compile_args['nvcc'] = nvcc_flags\n\n        if sys.platform == 'win32':\n            extra_link_args += ['cusparse.lib']\n        else:\n            extra_link_args += ['-lcusparse', '-l', 'cusparse']\n\n    extensions_dir = osp.join(osp.dirname(osp.abspath(__file__)), 'csrc')\n    main_files = glob.glob(osp.join(extensions_dir, '*.cpp'))\n    extensions = []\n    for main in main_files:\n        name = main.split(os.sep)[-1][:-4]\n\n        sources = [main]\n\n        path = osp.join(extensions_dir, 'cpu', f'{name}_cpu.cpp')\n        if osp.exists(path):\n            sources += [path]\n\n        path = osp.join(extensions_dir, 'cuda', f'{name}_cuda.cu')\n        if WITH_CUDA and osp.exists(path):\n            sources += [path]\n\n        extension = Extension(\n            'torch_sparse._' + name,\n            sources,\n            include_dirs=[extensions_dir],\n            define_macros=define_macros,\n            extra_compile_args=extra_compile_args,\n            extra_link_args=extra_link_args,\n            libraries=libraries,\n        )\n        extensions += [extension]\n\n    return extensions\n\n\ninstall_requires = ['scipy']\nsetup_requires = ['pytest-runner']\ntests_require = ['pytest', 'pytest-cov']\n\nsetup(\n    name='torch_sparse',\n    version='0.6.4',\n    author='Matthias Fey',\n    author_email='matthias.fey@tu-dortmund.de',\n    url='https://github.com/rusty1s/pytorch_sparse',\n    description=('PyTorch Extension Library of Optimized Autograd Sparse '\n                 'Matrix Operations'),\n    keywords=['pytorch', 'sparse', 'sparse-matrices', 'autograd'],\n    license='MIT',\n    python_requires='>=3.6',\n    install_requires=install_requires,\n    setup_requires=setup_requires,\n    tests_require=tests_require,\n    ext_modules=get_extensions() if not BUILD_DOCS else [],\n    cmdclass={\n        'build_ext':\n        BuildExtension.with_options(no_python_abi_suffix=True, use_ninja=False)\n    },\n    packages=find_packages(),\n)\n"""
benchmark/main.py,21,"b""import time\nimport os.path as osp\nimport itertools\n\nimport argparse\nimport wget\nimport torch\nfrom scipy.io import loadmat\n\nfrom torch_scatter import scatter_add\nfrom torch_sparse.tensor import SparseTensor\n\nshort_rows = [\n    ('DIMACS10', 'citationCiteseer'),\n    ('SNAP', 'web-Stanford'),\n]\nlong_rows = [\n    ('Janna', 'StocF-1465'),\n    ('GHS_psdef', 'ldoor'),\n]\n\n\ndef download(dataset):\n    url = 'https://sparse.tamu.edu/mat/{}/{}.mat'\n    for group, name in itertools.chain(long_rows, short_rows):\n        if not osp.exists(f'{name}.mat'):\n            print(f'Downloading {group}/{name}:')\n            wget.download(url.format(group, name))\n            print('')\n\n\ndef bold(text, flag=True):\n    return f'\\033[1m{text}\\033[0m' if flag else text\n\n\n@torch.no_grad()\ndef correctness(dataset):\n    group, name = dataset\n    mat_scipy = loadmat(f'{name}.mat')['Problem'][0][0][2].tocsr()\n    row = torch.from_numpy(mat_scipy.tocoo().row).to(args.device, torch.long)\n    col = torch.from_numpy(mat_scipy.tocoo().col).to(args.device, torch.long)\n    mat = SparseTensor(row=row, col=col, sparse_sizes=mat_scipy.shape)\n    mat.fill_cache_()\n    mat_pytorch = mat.to_torch_sparse_coo_tensor().coalesce()\n\n    for size in sizes:\n        try:\n            x = torch.randn((mat.size(1), size), device=args.device)\n\n            out1 = mat @ x\n            out2 = mat_pytorch @ x\n\n            assert torch.allclose(out1, out2, atol=1e-4)\n\n        except RuntimeError as e:\n            if 'out of memory' not in str(e):\n                raise RuntimeError(e)\n            torch.cuda.empty_cache()\n\n\ndef time_func(func, x):\n    try:\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        t = time.perf_counter()\n\n        if not args.with_backward:\n            with torch.no_grad():\n                for _ in range(iters):\n                    func(x)\n        else:\n            x = x.requires_grad_()\n            for _ in range(iters):\n                out = func(x)\n                out = out[0] if isinstance(out, tuple) else out\n                torch.autograd.grad(out, x, out, only_inputs=True)\n\n        if torch.cuda.is_available():\n            torch.cuda.synchronize()\n        return time.perf_counter() - t\n    except RuntimeError as e:\n        if 'out of memory' not in str(e):\n            raise RuntimeError(e)\n        torch.cuda.empty_cache()\n        return float('inf')\n\n\ndef timing(dataset):\n    group, name = dataset\n    mat_scipy = loadmat(f'{name}.mat')['Problem'][0][0][2].tocsr()\n    row = torch.from_numpy(mat_scipy.tocoo().row).to(args.device, torch.long)\n    col = torch.from_numpy(mat_scipy.tocoo().col).to(args.device, torch.long)\n    mat = SparseTensor(row=row, col=col, sparse_sizes=mat_scipy.shape)\n    mat.fill_cache_()\n    mat_pytorch = mat.to_torch_sparse_coo_tensor().coalesce()\n    mat_scipy = mat.to_scipy(layout='csr')\n\n    def scatter(x):\n        return scatter_add(x[col], row, dim=0, dim_size=mat_scipy.shape[0])\n\n    def spmm_scipy(x):\n        if x.is_cuda:\n            raise RuntimeError('out of memory')\n        return mat_scipy @ x\n\n    def spmm_pytorch(x):\n        return mat_pytorch @ x\n\n    def spmm(x):\n        return mat @ x\n\n    t1, t2, t3, t4 = [], [], [], []\n\n    for size in sizes:\n        try:\n            x = torch.randn((mat.size(1), size), device=args.device)\n\n            t1 += [time_func(scatter, x)]\n            t2 += [time_func(spmm_scipy, x)]\n            t3 += [time_func(spmm_pytorch, x)]\n            t4 += [time_func(spmm, x)]\n\n            del x\n\n        except RuntimeError as e:\n            if 'out of memory' not in str(e):\n                raise RuntimeError(e)\n            torch.cuda.empty_cache()\n            for t in (t1, t2, t3, t4):\n                t.append(float('inf'))\n\n    ts = torch.tensor([t1, t2, t3, t4])\n    winner = torch.zeros_like(ts, dtype=torch.bool)\n    winner[ts.argmin(dim=0), torch.arange(len(sizes))] = 1\n    winner = winner.tolist()\n\n    name = f'{group}/{name}'\n    print(f'{bold(name)} (avg row length: {mat.avg_row_length():.2f}):')\n    print('\\t'.join(['            '] + [f'{size:>5}' for size in sizes]))\n    print('\\t'.join([bold('Scatter     ')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t1, winner[0])]))\n    print('\\t'.join([bold('SPMM SciPy  ')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t2, winner[1])]))\n    print('\\t'.join([bold('SPMM PyTorch')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t3, winner[2])]))\n    print('\\t'.join([bold('SPMM Own    ')] +\n                    [bold(f'{t:.5f}', f) for t, f in zip(t4, winner[3])]))\n    print()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--with_backward', action='store_true')\n    parser.add_argument('--device', type=str, default='cuda')\n    args = parser.parse_args()\n    iters = 1 if args.device == 'cpu' else 20\n    sizes = [1, 16, 32, 64, 128, 256, 512]\n    sizes = sizes[:4] if args.device == 'cpu' else sizes\n\n    for _ in range(10):  # Warmup.\n        torch.randn(100, 100, device=args.device).sum()\n    for dataset in itertools.chain(short_rows, long_rows):\n        download(dataset)\n        correctness(dataset)\n        timing(dataset)\n"""
script/rename_wheel.py,0,"b""import sys\nimport os\nimport os.path as osp\nimport glob\nimport shutil\n\nidx = sys.argv[1]\nassert idx in ['cpu', 'cu92', 'cu101', 'cu102']\n\ndist_dir = osp.join(osp.dirname(osp.abspath(__file__)), '..', 'dist')\nwheels = glob.glob(osp.join('dist', '**', '*.whl'), recursive=True)\n\nfor wheel in wheels:\n    if idx in wheel:\n        continue\n\n    paths = wheel.split(osp.sep)\n    names = paths[-1].split('-')\n\n    name = '-'.join(names[:-4] + ['latest+' + idx] + names[-3:])\n    shutil.copyfile(wheel, osp.join(*paths[:-1], name))\n\n    name = '-'.join(names[:-4] + [names[-4] + '+' + idx] + names[-3:])\n    os.rename(wheel, osp.join(*paths[:-1], name))\n"""
test/__init__.py,0,b''
test/test_cat.py,3,"b""import pytest\nimport torch\nfrom torch_sparse.tensor import SparseTensor\nfrom torch_sparse.cat import cat, cat_diag\n\nfrom .utils import devices, tensor\n\n\n@pytest.mark.parametrize('device', devices)\ndef test_cat(device):\n    row, col = tensor([[0, 0, 1], [0, 1, 2]], torch.long, device)\n    mat1 = SparseTensor(row=row, col=col)\n    mat1.fill_cache_()\n\n    row, col = tensor([[0, 0, 1, 2], [0, 1, 1, 0]], torch.long, device)\n    mat2 = SparseTensor(row=row, col=col)\n    mat2.fill_cache_()\n\n    out = cat([mat1, mat2], dim=0)\n    assert out.to_dense().tolist() == [[1, 1, 0], [0, 0, 1], [1, 1, 0],\n                                       [0, 1, 0], [1, 0, 0]]\n    assert out.storage.has_row()\n    assert out.storage.has_rowptr()\n    assert out.storage.has_rowcount()\n    assert out.storage.num_cached_keys() == 1\n\n    out = cat([mat1, mat2], dim=1)\n    assert out.to_dense().tolist() == [[1, 1, 0, 1, 1], [0, 0, 1, 0, 1],\n                                       [0, 0, 0, 1, 0]]\n    assert out.storage.has_row()\n    assert not out.storage.has_rowptr()\n    assert out.storage.num_cached_keys() == 2\n\n    out = cat_diag([mat1, mat2])\n    assert out.to_dense().tolist() == [[1, 1, 0, 0, 0], [0, 0, 1, 0, 0],\n                                       [0, 0, 0, 1, 1], [0, 0, 0, 0, 1],\n                                       [0, 0, 0, 1, 0]]\n    assert out.storage.has_row()\n    assert out.storage.has_rowptr()\n    assert out.storage.num_cached_keys() == 5\n\n    value = torch.randn((mat1.nnz(), 4), device=device)\n    mat1 = mat1.set_value_(value, layout='coo')\n    out = cat([mat1, mat1], dim=-1)\n    assert out.storage.value().size() == (mat1.nnz(), 8)\n    assert out.storage.has_row()\n    assert out.storage.has_rowptr()\n    assert out.storage.num_cached_keys() == 5\n"""
test/test_coalesce.py,11,"b""import torch\nfrom torch_sparse import coalesce\n\n\ndef test_coalesce():\n    row = torch.tensor([1, 0, 1, 0, 2, 1])\n    col = torch.tensor([0, 1, 1, 1, 0, 0])\n    index = torch.stack([row, col], dim=0)\n\n    index, _ = coalesce(index, None, m=3, n=2)\n    assert index.tolist() == [[0, 1, 1, 2], [1, 0, 1, 0]]\n\n\ndef test_coalesce_add():\n    row = torch.tensor([1, 0, 1, 0, 2, 1])\n    col = torch.tensor([0, 1, 1, 1, 0, 0])\n    index = torch.stack([row, col], dim=0)\n    value = torch.tensor([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\n\n    index, value = coalesce(index, value, m=3, n=2)\n    assert index.tolist() == [[0, 1, 1, 2], [1, 0, 1, 0]]\n    assert value.tolist() == [[6, 8], [7, 9], [3, 4], [5, 6]]\n\n\ndef test_coalesce_max():\n    row = torch.tensor([1, 0, 1, 0, 2, 1])\n    col = torch.tensor([0, 1, 1, 1, 0, 0])\n    index = torch.stack([row, col], dim=0)\n    value = torch.tensor([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\n\n    index, value = coalesce(index, value, m=3, n=2, op='max')\n    assert index.tolist() == [[0, 1, 1, 2], [1, 0, 1, 0]]\n    assert value.tolist() == [[4, 5], [6, 7], [3, 4], [5, 6]]\n"""
test/test_convert.py,4,"b'import torch\nfrom torch_sparse import to_scipy, from_scipy\nfrom torch_sparse import to_torch_sparse, from_torch_sparse\n\n\ndef test_convert_scipy():\n    index = torch.tensor([[0, 0, 1, 2, 2], [0, 2, 1, 0, 1]])\n    value = torch.Tensor([1, 2, 4, 1, 3])\n    N = 3\n\n    out = from_scipy(to_scipy(index, value, N, N))\n    assert out[0].tolist() == index.tolist()\n    assert out[1].tolist() == value.tolist()\n\n\ndef test_convert_torch_sparse():\n    index = torch.tensor([[0, 0, 1, 2, 2], [0, 2, 1, 0, 1]])\n    value = torch.Tensor([1, 2, 4, 1, 3])\n    N = 3\n\n    out = from_torch_sparse(to_torch_sparse(index, value, N, N).coalesce())\n    assert out[0].tolist() == index.tolist()\n    assert out[1].tolist() == value.tolist()\n'"
test/test_diag.py,3,"b""from itertools import product\n\nimport pytest\nimport torch\nfrom torch_sparse.tensor import SparseTensor\n\nfrom .utils import dtypes, devices, tensor\n\n\n@pytest.mark.parametrize('dtype,device', product(dtypes, devices))\ndef test_remove_diag(dtype, device):\n    row, col = tensor([[0, 0, 1, 2], [0, 1, 2, 2]], torch.long, device)\n    value = tensor([1, 2, 3, 4], dtype, device)\n    mat = SparseTensor(row=row, col=col, value=value)\n    mat.fill_cache_()\n\n    mat = mat.remove_diag()\n    assert mat.storage.row().tolist() == [0, 1]\n    assert mat.storage.col().tolist() == [1, 2]\n    assert mat.storage.value().tolist() == [2, 3]\n    assert mat.storage.num_cached_keys() == 2\n    assert mat.storage.rowcount().tolist() == [1, 1, 0]\n    assert mat.storage.colcount().tolist() == [0, 1, 1]\n\n    mat = SparseTensor(row=row, col=col, value=value)\n    mat.fill_cache_()\n\n    mat = mat.remove_diag(k=1)\n    assert mat.storage.row().tolist() == [0, 2]\n    assert mat.storage.col().tolist() == [0, 2]\n    assert mat.storage.value().tolist() == [1, 4]\n    assert mat.storage.num_cached_keys() == 2\n    assert mat.storage.rowcount().tolist() == [1, 0, 1]\n    assert mat.storage.colcount().tolist() == [1, 0, 1]\n\n\n@pytest.mark.parametrize('dtype,device', product(dtypes, devices))\ndef test_set_diag(dtype, device):\n    row, col = tensor([[0, 0, 9, 9], [0, 1, 0, 1]], torch.long, device)\n    value = tensor([1, 2, 3, 4], dtype, device)\n    mat = SparseTensor(row=row, col=col, value=value)\n\n    mat = mat.set_diag(tensor([-8, -8], dtype, device), k=-1)\n    mat = mat.set_diag(tensor([-8], dtype, device), k=1)\n\n\n@pytest.mark.parametrize('dtype,device', product(dtypes, devices))\ndef test_fill_diag(dtype, device):\n    row, col = tensor([[0, 0, 9, 9], [0, 1, 0, 1]], torch.long, device)\n    value = tensor([1, 2, 3, 4], dtype, device)\n    mat = SparseTensor(row=row, col=col, value=value)\n\n    mat = mat.fill_diag(-8, k=-1)\n    mat = mat.fill_diag(-8, k=1)\n"""
test/test_eye.py,1,"b""from itertools import product\n\nimport pytest\nimport torch\nfrom torch_sparse.tensor import SparseTensor\n\nfrom .utils import dtypes, devices\n\n\n@pytest.mark.parametrize('dtype,device', product(dtypes, devices))\ndef test_eye(dtype, device):\n    options = torch.tensor(0, dtype=dtype, device=device)\n\n    mat = SparseTensor.eye(3, options=options)\n    assert mat.storage.sparse_sizes() == (3, 3)\n    assert mat.storage.row().tolist() == [0, 1, 2]\n    assert mat.storage.rowptr().tolist() == [0, 1, 2, 3]\n    assert mat.storage.col().tolist() == [0, 1, 2]\n    assert mat.storage.value().tolist() == [1, 1, 1]\n    assert mat.storage.num_cached_keys() == 0\n\n    mat = SparseTensor.eye(3, options=options, has_value=False)\n    assert mat.storage.sparse_sizes() == (3, 3)\n    assert mat.storage.row().tolist() == [0, 1, 2]\n    assert mat.storage.rowptr().tolist() == [0, 1, 2, 3]\n    assert mat.storage.col().tolist() == [0, 1, 2]\n    assert mat.storage.value() is None\n    assert mat.storage.num_cached_keys() == 0\n\n    mat = SparseTensor.eye(3, 4, options=options, fill_cache=True)\n    assert mat.storage.sparse_sizes() == (3, 4)\n    assert mat.storage.row().tolist() == [0, 1, 2]\n    assert mat.storage.rowptr().tolist() == [0, 1, 2, 3]\n    assert mat.storage.col().tolist() == [0, 1, 2]\n    assert mat.storage.num_cached_keys() == 5\n    assert mat.storage.rowcount().tolist() == [1, 1, 1]\n    assert mat.storage.colptr().tolist() == [0, 1, 2, 3, 3]\n    assert mat.storage.colcount().tolist() == [1, 1, 1, 0]\n    assert mat.storage.csr2csc().tolist() == [0, 1, 2]\n    assert mat.storage.csc2csr().tolist() == [0, 1, 2]\n\n    mat = SparseTensor.eye(4, 3, options=options, fill_cache=True)\n    assert mat.storage.sparse_sizes() == (4, 3)\n    assert mat.storage.row().tolist() == [0, 1, 2]\n    assert mat.storage.rowptr().tolist() == [0, 1, 2, 3, 3]\n    assert mat.storage.col().tolist() == [0, 1, 2]\n    assert mat.storage.num_cached_keys() == 5\n    assert mat.storage.rowcount().tolist() == [1, 1, 1, 0]\n    assert mat.storage.colptr().tolist() == [0, 1, 2, 3]\n    assert mat.storage.colcount().tolist() == [1, 1, 1]\n    assert mat.storage.csr2csc().tolist() == [0, 1, 2]\n    assert mat.storage.csc2csr().tolist() == [0, 1, 2]\n"""
test/test_matmul.py,7,"b""from itertools import product\n\nimport pytest\nimport torch\n\nfrom torch_sparse.matmul import matmul\nfrom torch_sparse.tensor import SparseTensor\nimport torch_scatter\n\nfrom .utils import reductions, devices, grad_dtypes\n\n\n@pytest.mark.parametrize('dtype,device,reduce',\n                         product(grad_dtypes, devices, reductions))\ndef test_spmm(dtype, device, reduce):\n    src = torch.randn((10, 8), dtype=dtype, device=device)\n    src[2:4, :] = 0  # Remove multiple rows.\n    src[:, 2:4] = 0  # Remove multiple columns.\n    src = SparseTensor.from_dense(src).requires_grad_()\n    row, col, value = src.coo()\n\n    other = torch.randn((2, 8, 2), dtype=dtype, device=device,\n                        requires_grad=True)\n\n    src_col = other.index_select(-2, col) * value.unsqueeze(-1)\n    expected = torch_scatter.scatter(src_col, row, dim=-2, reduce=reduce)\n    if reduce == 'min':\n        expected[expected > 1000] = 0\n    if reduce == 'max':\n        expected[expected < -1000] = 0\n\n    grad_out = torch.randn_like(expected)\n\n    expected.backward(grad_out)\n    expected_grad_value = value.grad\n    value.grad = None\n    expected_grad_other = other.grad\n    other.grad = None\n\n    out = matmul(src, other, reduce)\n    out.backward(grad_out)\n\n    assert torch.allclose(expected, out, atol=1e-6)\n    assert torch.allclose(expected_grad_value, value.grad, atol=1e-6)\n    assert torch.allclose(expected_grad_other, other.grad, atol=1e-6)\n\n\n@pytest.mark.parametrize('dtype,device', product(grad_dtypes, devices))\ndef test_spspmm(dtype, device):\n    src = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=dtype,\n                       device=device)\n\n    src = SparseTensor.from_dense(src)\n    out = matmul(src, src)\n    assert out.sizes() == [3, 3]\n    assert out.has_value()\n    rowptr, col, value = out.csr()\n    assert rowptr.tolist() == [0, 1, 2, 3]\n    assert col.tolist() == [0, 1, 2]\n    assert value.tolist() == [1, 1, 1]\n\n    src.set_value_(None)\n    out = matmul(src, src)\n    assert out.sizes() == [3, 3]\n    assert not out.has_value()\n    rowptr, col, value = out.csr()\n    assert rowptr.tolist() == [0, 1, 2, 3]\n    assert col.tolist() == [0, 1, 2]\n"""
test/test_metis.py,4,"b""import pytest\nimport torch\nfrom torch_sparse.tensor import SparseTensor\n\nfrom .utils import devices\n\ntry:\n    torch.ops.torch_sparse.partition\n    with_metis = True\nexcept RuntimeError:\n    with_metis = False\n\n\n@pytest.mark.skipif(not with_metis, reason='Not compiled with METIS support')\n@pytest.mark.parametrize('device', devices)\ndef test_metis(device):\n    value1 = torch.randn(6 * 6, device=device).view(6, 6)\n    value2 = torch.arange(6 * 6, dtype=torch.long, device=device).view(6, 6)\n    value3 = torch.ones(6 * 6, device=device).view(6, 6)\n\n    for value in [value1, value2, value3]:\n        mat = SparseTensor.from_dense(value)\n\n        _, partptr, perm = mat.partition(num_parts=2, recursive=False,\n                                         weighted=True)\n        assert partptr.numel() == 3\n        assert perm.numel() == 6\n\n        _, partptr, perm = mat.partition(num_parts=2, recursive=False,\n                                         weighted=False)\n        assert partptr.numel() == 3\n        assert perm.numel() == 6\n"""
test/test_overload.py,4,"b'import torch\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef test_overload():\n    row = torch.tensor([0, 1, 1, 2, 2])\n    col = torch.tensor([1, 0, 2, 1, 2])\n    mat = SparseTensor(row=row, col=col)\n\n    other = torch.tensor([1, 2, 3]).view(3, 1)\n    other + mat\n    mat + other\n    other * mat\n    mat * other\n\n    other = torch.tensor([1, 2, 3]).view(1, 3)\n    other + mat\n    mat + other\n    other * mat\n    mat * other\n'"
test/test_padding.py,17,"b""from itertools import product\n\nimport pytest\nimport torch\nfrom torch_sparse import SparseTensor, padded_index_select\n\nfrom .utils import grad_dtypes, devices, tensor\n\n\n@pytest.mark.parametrize('dtype,device', product(grad_dtypes, devices))\ndef test_padded_index_select(dtype, device):\n    row = torch.tensor([0, 0, 0, 0, 1, 1, 1, 2, 2, 3])\n    col = torch.tensor([0, 1, 2, 3, 0, 2, 3, 1, 3, 2])\n    adj = SparseTensor(row=row, col=col).to(device)\n    binptr = torch.tensor([0, 3, 5], device=device)\n\n    data = adj.padded_index(binptr)\n    node_perm, row_perm, col_perm, mask, node_size, edge_size = data\n\n    assert node_perm.tolist() == [2, 3, 0, 1]\n    assert row_perm.tolist() == [2, 2, 3, -1, 0, 0, 0, 0, 1, 1, 1, -1]\n    assert col_perm.tolist() == [1, 3, 2, -1, 0, 1, 2, 3, 0, 2, 3, -1]\n    assert mask.long().tolist() == [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n    assert node_size == [2, 2]\n    assert edge_size == [4, 8]\n\n    x = tensor([0, 1, 2, 3], dtype, device).view(-1, 1).requires_grad_()\n    x_j = padded_index_select(x, col_perm)\n\n    assert x_j.flatten().tolist() == [1, 3, 2, 0, 0, 1, 2, 3, 0, 2, 3, 0]\n\n    grad_out = tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype, device)\n    x_j.backward(grad_out.view(-1, 1))\n\n    assert x.grad.flatten().tolist() == [12, 5, 17, 18]\n\n\ndef test_padded_index_select_runtime():\n    return\n    from torch_geometric.datasets import Planetoid\n\n    device = torch.device('cuda')\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n\n    dataset = Planetoid('/tmp/Planetoid', name='PubMed')\n    data = dataset[0]\n    row, col = data.edge_index.to(device)\n\n    adj = SparseTensor(row=row, col=col)\n    rowcount = adj.storage.rowcount().to(device)\n    rowptr = adj.storage.rowptr().to(device)\n    binptr = torch.tensor([0, 4, 11, 30, 50, 80, 120, 140, 2000]).to(device)\n\n    x = torch.randn(adj.size(0), 512).to(device)\n\n    data = torch.ops.torch_sparse.padded_index(rowptr, col, rowcount, binptr)\n    node_perm, row_perm, col_perm, mask, node_sizes, edge_sizes = data\n\n    out = torch.ops.torch_sparse.padded_index_select(x, col_perm,\n                                                     torch.tensor(0.))\n    outs = out.split(edge_sizes)\n    for out, size in zip(outs, node_sizes):\n        print(out.view(size, -1, x.size(-1)).shape)\n\n    for i in range(110):\n        if i == 10:\n            start.record()\n        torch.ops.torch_sparse.padded_index(rowptr, col, rowcount, binptr)\n    end.record()\n    torch.cuda.synchronize()\n    print('padded index', start.elapsed_time(end))\n\n    for i in range(110):\n        if i == 10:\n            start.record()\n        out = torch.ops.torch_sparse.padded_index_select(\n            x, col_perm, torch.tensor(0.))\n        out.split(edge_sizes)\n    end.record()\n    torch.cuda.synchronize()\n    print('padded index select', start.elapsed_time(end))\n\n    for i in range(110):\n        if i == 10:\n            start.record()\n        x.index_select(0, col)\n    end.record()\n    torch.cuda.synchronize()\n    print('index_select', start.elapsed_time(end))\n"""
test/test_permute.py,3,"b""import pytest\nimport torch\nfrom torch_sparse.tensor import SparseTensor\n\nfrom .utils import devices, tensor\n\n\n@pytest.mark.parametrize('device', devices)\ndef test_permute(device):\n    row, col = tensor([[0, 0, 1, 2, 2], [0, 1, 0, 1, 2]], torch.long, device)\n    value = tensor([1, 2, 3, 4, 5], torch.float, device)\n    adj = SparseTensor(row=row, col=col, value=value)\n\n    row, col, value = adj.permute(torch.tensor([1, 0, 2])).coo()\n    assert row.tolist() == [0, 1, 1, 2, 2]\n    assert col.tolist() == [1, 0, 1, 0, 2]\n    assert value.tolist() == [3, 2, 1, 4, 5]\n"""
test/test_saint.py,3,"b'import torch\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef test_saint_subgraph():\n    row = torch.tensor([0, 0, 1, 1, 2, 2, 2, 3, 3, 4])\n    col = torch.tensor([1, 2, 0, 2, 0, 1, 3, 2, 4, 3])\n    adj = SparseTensor(row=row, col=col)\n    node_idx = torch.tensor([0, 1, 2])\n\n    adj, edge_index = adj.saint_subgraph(node_idx)\n'"
test/test_spmm.py,3,"b""from itertools import product\n\nimport pytest\nimport torch\nfrom torch_sparse import spmm\n\nfrom .utils import dtypes, devices, tensor\n\n\n@pytest.mark.parametrize('dtype,device', product(dtypes, devices))\ndef test_spmm(dtype, device):\n    row = torch.tensor([0, 0, 1, 2, 2], device=device)\n    col = torch.tensor([0, 2, 1, 0, 1], device=device)\n    index = torch.stack([row, col], dim=0)\n    value = tensor([1, 2, 4, 1, 3], dtype, device)\n    x = tensor([[1, 4], [2, 5], [3, 6]], dtype, device)\n\n    out = spmm(index, value, 3, 3, x)\n    assert out.tolist() == [[7, 16], [8, 20], [7, 19]]\n"""
test/test_spspmm.py,8,"b""from itertools import product\n\nimport pytest\nimport torch\nfrom torch_sparse import spspmm, SparseTensor\n\nfrom .utils import grad_dtypes, devices, tensor\n\n\n@pytest.mark.parametrize('dtype,device', product(grad_dtypes, devices))\ndef test_spspmm(dtype, device):\n    indexA = torch.tensor([[0, 0, 1, 2, 2], [1, 2, 0, 0, 1]], device=device)\n    valueA = tensor([1, 2, 3, 4, 5], dtype, device)\n    indexB = torch.tensor([[0, 2], [1, 0]], device=device)\n    valueB = tensor([2, 4], dtype, device)\n\n    indexC, valueC = spspmm(indexA, valueA, indexB, valueB, 3, 3, 2)\n    assert indexC.tolist() == [[0, 1, 2], [0, 1, 1]]\n    assert valueC.tolist() == [8, 6, 8]\n\n\n@pytest.mark.parametrize('dtype,device', product(grad_dtypes, devices))\ndef test_sparse_tensor_spspmm(dtype, device):\n    x = SparseTensor(\n        row=torch.tensor(\n            [0, 1, 1, 1, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 8, 8, 9, 9],\n            device=device),\n        col=torch.tensor(\n            [0, 5, 10, 15, 1, 2, 3, 7, 13, 6, 9, 5, 10, 15, 11, 14, 5, 15],\n            device=device),\n        value=torch.tensor([\n            1, 3**-0.5, 3**-0.5, 3**-0.5, 1, 1, 1, -2**-0.5, -2**-0.5,\n            -2**-0.5, -2**-0.5, 6**-0.5, -6**0.5 / 3, 6**-0.5, -2**-0.5,\n            -2**-0.5, 2**-0.5, -2**-0.5\n        ], dtype=dtype, device=device),\n    )\n\n    expected = torch.eye(10, dtype=dtype, device=device)\n\n    out = x @ x.to_dense().t()\n    assert torch.allclose(out, expected, atol=1e-7)\n\n    out = x @ x.t()\n    out = out.to_dense()\n    assert torch.allclose(out, expected, atol=1e-7)\n"""
test/test_storage.py,6,"b""from itertools import product\n\nimport pytest\nimport torch\nfrom torch_sparse.storage import SparseStorage\n\nfrom .utils import dtypes, devices, tensor\n\n\n@pytest.mark.parametrize('dtype,device', product(dtypes, devices))\ndef test_storage(dtype, device):\n    row, col = tensor([[0, 0, 1, 1], [0, 1, 0, 1]], torch.long, device)\n\n    storage = SparseStorage(row=row, col=col)\n    assert storage.row().tolist() == [0, 0, 1, 1]\n    assert storage.col().tolist() == [0, 1, 0, 1]\n    assert storage.value() is None\n    assert storage.sparse_sizes() == (2, 2)\n\n    row, col = tensor([[0, 0, 1, 1], [1, 0, 1, 0]], torch.long, device)\n    value = tensor([2, 1, 4, 3], dtype, device)\n    storage = SparseStorage(row=row, col=col, value=value)\n    assert storage.row().tolist() == [0, 0, 1, 1]\n    assert storage.col().tolist() == [0, 1, 0, 1]\n    assert storage.value().tolist() == [1, 2, 3, 4]\n    assert storage.sparse_sizes() == (2, 2)\n\n\n@pytest.mark.parametrize('dtype,device', product(dtypes, devices))\ndef test_caching(dtype, device):\n    row, col = tensor([[0, 0, 1, 1], [0, 1, 0, 1]], torch.long, device)\n    storage = SparseStorage(row=row, col=col)\n\n    assert storage._row.tolist() == row.tolist()\n    assert storage._col.tolist() == col.tolist()\n    assert storage._value is None\n\n    assert storage._rowcount is None\n    assert storage._rowptr is None\n    assert storage._colcount is None\n    assert storage._colptr is None\n    assert storage._csr2csc is None\n    assert storage.num_cached_keys() == 0\n\n    storage.fill_cache_()\n    assert storage._rowcount.tolist() == [2, 2]\n    assert storage._rowptr.tolist() == [0, 2, 4]\n    assert storage._colcount.tolist() == [2, 2]\n    assert storage._colptr.tolist() == [0, 2, 4]\n    assert storage._csr2csc.tolist() == [0, 2, 1, 3]\n    assert storage._csc2csr.tolist() == [0, 2, 1, 3]\n    assert storage.num_cached_keys() == 5\n\n    storage = SparseStorage(row=row, rowptr=storage._rowptr, col=col,\n                            value=storage._value,\n                            sparse_sizes=storage._sparse_sizes,\n                            rowcount=storage._rowcount, colptr=storage._colptr,\n                            colcount=storage._colcount,\n                            csr2csc=storage._csr2csc, csc2csr=storage._csc2csr)\n\n    assert storage._rowcount.tolist() == [2, 2]\n    assert storage._rowptr.tolist() == [0, 2, 4]\n    assert storage._colcount.tolist() == [2, 2]\n    assert storage._colptr.tolist() == [0, 2, 4]\n    assert storage._csr2csc.tolist() == [0, 2, 1, 3]\n    assert storage._csc2csr.tolist() == [0, 2, 1, 3]\n    assert storage.num_cached_keys() == 5\n\n    storage.clear_cache_()\n    assert storage._rowcount is None\n    assert storage._rowptr is not None\n    assert storage._colcount is None\n    assert storage._colptr is None\n    assert storage._csr2csc is None\n    assert storage.num_cached_keys() == 0\n\n\n@pytest.mark.parametrize('dtype,device', product(dtypes, devices))\ndef test_utility(dtype, device):\n    row, col = tensor([[0, 0, 1, 1], [1, 0, 1, 0]], torch.long, device)\n    value = tensor([1, 2, 3, 4], dtype, device)\n    storage = SparseStorage(row=row, col=col, value=value)\n\n    assert storage.has_value()\n\n    storage.set_value_(value, layout='csc')\n    assert storage.value().tolist() == [1, 3, 2, 4]\n    storage.set_value_(value, layout='coo')\n    assert storage.value().tolist() == [1, 2, 3, 4]\n\n    storage = storage.set_value(value, layout='csc')\n    assert storage.value().tolist() == [1, 3, 2, 4]\n    storage = storage.set_value(value, layout='coo')\n    assert storage.value().tolist() == [1, 2, 3, 4]\n\n    storage = storage.sparse_resize((3, 3))\n    assert storage.sparse_sizes() == (3, 3)\n\n    new_storage = storage.copy()\n    assert new_storage != storage\n    assert new_storage.col().data_ptr() == storage.col().data_ptr()\n\n    new_storage = storage.clone()\n    assert new_storage != storage\n    assert new_storage.col().data_ptr() != storage.col().data_ptr()\n\n\n@pytest.mark.parametrize('dtype,device', product(dtypes, devices))\ndef test_coalesce(dtype, device):\n    row, col = tensor([[0, 0, 0, 1, 1], [0, 1, 1, 0, 1]], torch.long, device)\n    value = tensor([1, 1, 1, 3, 4], dtype, device)\n    storage = SparseStorage(row=row, col=col, value=value)\n\n    assert storage.row().tolist() == row.tolist()\n    assert storage.col().tolist() == col.tolist()\n    assert storage.value().tolist() == value.tolist()\n\n    assert not storage.is_coalesced()\n    storage = storage.coalesce()\n    assert storage.is_coalesced()\n\n    assert storage.row().tolist() == [0, 0, 1, 1]\n    assert storage.col().tolist() == [0, 1, 0, 1]\n    assert storage.value().tolist() == [1, 2, 3, 4]\n\n\n@pytest.mark.parametrize('dtype,device', product(dtypes, devices))\ndef test_sparse_reshape(dtype, device):\n    row, col = tensor([[0, 1, 2, 3], [0, 1, 2, 3]], torch.long, device)\n    storage = SparseStorage(row=row, col=col)\n\n    storage = storage.sparse_reshape(2, 8)\n    assert storage.sparse_sizes() == (2, 8)\n    assert storage.row().tolist() == [0, 0, 1, 1]\n    assert storage.col().tolist() == [0, 5, 2, 7]\n\n    storage = storage.sparse_reshape(-1, 4)\n    assert storage.sparse_sizes() == (4, 4)\n    assert storage.row().tolist() == [0, 1, 2, 3]\n    assert storage.col().tolist() == [0, 1, 2, 3]\n\n    storage = storage.sparse_reshape(2, -1)\n    assert storage.sparse_sizes() == (2, 8)\n    assert storage.row().tolist() == [0, 0, 1, 1]\n    assert storage.col().tolist() == [0, 5, 2, 7]\n"""
test/test_transpose.py,6,"b""from itertools import product\n\nimport pytest\nimport torch\nfrom torch_sparse import transpose\n\nfrom .utils import dtypes, devices, tensor\n\n\n@pytest.mark.parametrize('dtype,device', product(dtypes, devices))\ndef test_transpose_matrix(dtype, device):\n    row = torch.tensor([1, 0, 1, 2], device=device)\n    col = torch.tensor([0, 1, 1, 0], device=device)\n    index = torch.stack([row, col], dim=0)\n    value = tensor([1, 2, 3, 4], dtype, device)\n\n    index, value = transpose(index, value, m=3, n=2)\n    assert index.tolist() == [[0, 0, 1, 1], [1, 2, 0, 1]]\n    assert value.tolist() == [1, 4, 2, 3]\n\n\n@pytest.mark.parametrize('dtype,device', product(dtypes, devices))\ndef test_transpose(dtype, device):\n    row = torch.tensor([1, 0, 1, 0, 2, 1], device=device)\n    col = torch.tensor([0, 1, 1, 1, 0, 0], device=device)\n    index = torch.stack([row, col], dim=0)\n    value = tensor([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6], [6, 7]], dtype,\n                   device)\n\n    index, value = transpose(index, value, m=3, n=2)\n    assert index.tolist() == [[0, 0, 1, 1], [1, 2, 0, 1]]\n    assert value.tolist() == [[7, 9], [5, 6], [6, 8], [3, 4]]\n"""
test/utils.py,6,"b""import torch\n\nreductions = ['sum', 'add', 'mean', 'min', 'max']\n\ndtypes = [torch.float, torch.double, torch.int, torch.long]\ngrad_dtypes = [torch.float, torch.double]\n\ndevices = [torch.device('cpu')]\nif torch.cuda.is_available():\n    devices += [torch.device(f'cuda:{torch.cuda.current_device()}')]\n\n\ndef tensor(x, dtype, device):\n    return None if x is None else torch.tensor(x, dtype=dtype, device=device)\n"""
torch_sparse/__init__.py,4,"b""import importlib\nimport os.path as osp\n\nimport torch\n\n__version__ = '0.6.4'\n\nfor library in [\n        '_version', '_convert', '_diag', '_spmm', '_spspmm', '_metis', '_rw',\n        '_saint', '_padding', '_sample'\n]:\n    torch.ops.load_library(importlib.machinery.PathFinder().find_spec(\n        library, [osp.dirname(__file__)]).origin)\n\nif torch.version.cuda is not None:  # pragma: no cover\n    cuda_version = torch.ops.torch_sparse.cuda_version()\n\n    if cuda_version == -1:\n        major = minor = 0\n    elif cuda_version < 10000:\n        major, minor = int(str(cuda_version)[0]), int(str(cuda_version)[2])\n    else:\n        major, minor = int(str(cuda_version)[0:2]), int(str(cuda_version)[3])\n    t_major, t_minor = [int(x) for x in torch.version.cuda.split('.')]\n\n    if t_major != major or t_minor != minor:\n        raise RuntimeError(\n            f'Detected that PyTorch and torch_sparse were compiled with '\n            f'different CUDA versions. PyTorch has CUDA version '\n            f'{t_major}.{t_minor} and torch_sparse has CUDA version '\n            f'{major}.{minor}. Please reinstall the torch_sparse that '\n            f'matches your PyTorch install.')\n\nfrom .storage import SparseStorage  # noqa\nfrom .tensor import SparseTensor  # noqa\nfrom .transpose import t  # noqa\nfrom .narrow import narrow, __narrow_diag__  # noqa\nfrom .select import select  # noqa\nfrom .index_select import index_select, index_select_nnz  # noqa\nfrom .masked_select import masked_select, masked_select_nnz  # noqa\nfrom .permute import permute  # noqa\nfrom .diag import remove_diag, set_diag, fill_diag  # noqa\nfrom .add import add, add_, add_nnz, add_nnz_  # noqa\nfrom .mul import mul, mul_, mul_nnz, mul_nnz_  # noqa\nfrom .reduce import sum, mean, min, max  # noqa\nfrom .matmul import matmul  # noqa\nfrom .cat import cat, cat_diag  # noqa\nfrom .rw import random_walk  # noqa\nfrom .metis import partition  # noqa\nfrom .bandwidth import reverse_cuthill_mckee  # noqa\nfrom .saint import saint_subgraph  # noqa\nfrom .padding import padded_index, padded_index_select  # noqa\nfrom .sample import sample, sample_adj  # noqa\n\nfrom .convert import to_torch_sparse, from_torch_sparse  # noqa\nfrom .convert import to_scipy, from_scipy  # noqa\nfrom .coalesce import coalesce  # noqa\nfrom .transpose import transpose  # noqa\nfrom .eye import eye  # noqa\nfrom .spmm import spmm  # noqa\nfrom .spspmm import spspmm  # noqa\n\n__all__ = [\n    'SparseStorage',\n    'SparseTensor',\n    't',\n    'narrow',\n    '__narrow_diag__',\n    'select',\n    'index_select',\n    'index_select_nnz',\n    'masked_select',\n    'masked_select_nnz',\n    'permute',\n    'remove_diag',\n    'set_diag',\n    'fill_diag',\n    'add',\n    'add_',\n    'add_nnz',\n    'add_nnz_',\n    'mul',\n    'mul_',\n    'mul_nnz',\n    'mul_nnz_',\n    'sum',\n    'mean',\n    'min',\n    'max',\n    'matmul',\n    'cat',\n    'cat_diag',\n    'random_walk',\n    'partition',\n    'reverse_cuthill_mckee',\n    'saint_subgraph',\n    'padded_index',\n    'padded_index_select',\n    'to_torch_sparse',\n    'from_torch_sparse',\n    'to_scipy',\n    'from_scipy',\n    'coalesce',\n    'transpose',\n    'eye',\n    'spmm',\n    'spspmm',\n    '__version__',\n]\n"""
torch_sparse/add.py,4,"b""from typing import Optional\n\nimport torch\nfrom torch_scatter import gather_csr\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef add(src: SparseTensor, other: torch.Tensor) -> SparseTensor:\n    rowptr, col, value = src.csr()\n    if other.size(0) == src.size(0) and other.size(1) == 1:  # Row-wise...\n        other = gather_csr(other.squeeze(1), rowptr)\n        pass\n    elif other.size(0) == 1 and other.size(1) == src.size(1):  # Col-wise...\n        other = other.squeeze(0)[col]\n    else:\n        raise ValueError(\n            f'Size mismatch: Expected size ({src.size(0)}, 1, ...) or '\n            f'(1, {src.size(1)}, ...), but got size {other.size()}.')\n    if value is not None:\n        value = other.to(value.dtype).add_(value)\n    else:\n        value = other.add_(1)\n    return src.set_value(value, layout='coo')\n\n\ndef add_(src: SparseTensor, other: torch.Tensor) -> SparseTensor:\n    rowptr, col, value = src.csr()\n    if other.size(0) == src.size(0) and other.size(1) == 1:  # Row-wise...\n        other = gather_csr(other.squeeze(1), rowptr)\n        pass\n    elif other.size(0) == 1 and other.size(1) == src.size(1):  # Col-wise...\n        other = other.squeeze(0)[col]\n    else:\n        raise ValueError(\n            f'Size mismatch: Expected size ({src.size(0)}, 1, ...) or '\n            f'(1, {src.size(1)}, ...), but got size {other.size()}.')\n\n    if value is not None:\n        value = value.add_(other.to(value.dtype))\n    else:\n        value = other.add_(1)\n    return src.set_value_(value, layout='coo')\n\n\ndef add_nnz(src: SparseTensor, other: torch.Tensor,\n            layout: Optional[str] = None) -> SparseTensor:\n    value = src.storage.value()\n    if value is not None:\n        value = value.add(other.to(value.dtype))\n    else:\n        value = other.add(1)\n    return src.set_value(value, layout=layout)\n\n\ndef add_nnz_(src: SparseTensor, other: torch.Tensor,\n             layout: Optional[str] = None) -> SparseTensor:\n    value = src.storage.value()\n    if value is not None:\n        value = value.add_(other.to(value.dtype))\n    else:\n        value = other.add(1)\n    return src.set_value_(value, layout=layout)\n\n\nSparseTensor.add = lambda self, other: add(self, other)\nSparseTensor.add_ = lambda self, other: add_(self, other)\nSparseTensor.add_nnz = lambda self, other, layout=None: add_nnz(\n    self, other, layout)\nSparseTensor.add_nnz_ = lambda self, other, layout=None: add_nnz_(\n    self, other, layout)\nSparseTensor.__add__ = SparseTensor.add\nSparseTensor.__radd__ = SparseTensor.add\nSparseTensor.__iadd__ = SparseTensor.add_\n"""
torch_sparse/bandwidth.py,2,"b""import scipy.sparse as sp\nfrom typing import Tuple, Optional\n\nimport torch\nfrom torch_sparse.tensor import SparseTensor\nfrom torch_sparse.permute import permute\n\n\ndef reverse_cuthill_mckee(src: SparseTensor,\n                          is_symmetric: Optional[bool] = None\n                          ) -> Tuple[SparseTensor, torch.Tensor]:\n\n    if is_symmetric is None:\n        is_symmetric = src.is_symmetric()\n\n    if not is_symmetric:\n        src = src.to_symmetric()\n\n    sp_src = src.to_scipy(layout='csr')\n    perm = sp.csgraph.reverse_cuthill_mckee(sp_src, symmetric_mode=True).copy()\n    perm = torch.from_numpy(perm).to(torch.long).to(src.device())\n\n    out = permute(src, perm)\n\n    return out, perm\n\n\nSparseTensor.reverse_cuthill_mckee = reverse_cuthill_mckee\n"""
torch_sparse/cat.py,56,"b""from typing import Optional, List\n\nimport torch\nfrom torch_sparse.storage import SparseStorage\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef cat(tensors: List[SparseTensor], dim: int) -> SparseTensor:\n    assert len(tensors) > 0\n    if dim < 0:\n        dim = tensors[0].dim() + dim\n\n    if dim == 0:\n        rows: List[torch.Tensor] = []\n        rowptrs: List[torch.Tensor] = []\n        cols: List[torch.Tensor] = []\n        values: List[torch.Tensor] = []\n        sparse_sizes: List[int] = [0, 0]\n        rowcounts: List[torch.Tensor] = []\n\n        nnz: int = 0\n        for tensor in tensors:\n            row = tensor.storage._row\n            if row is not None:\n                rows.append(row + sparse_sizes[0])\n\n            rowptr = tensor.storage._rowptr\n            if rowptr is not None:\n                if len(rowptrs) > 0:\n                    rowptr = rowptr[1:]\n                rowptrs.append(rowptr + nnz)\n\n            cols.append(tensor.storage._col)\n\n            value = tensor.storage._value\n            if value is not None:\n                values.append(value)\n\n            rowcount = tensor.storage._rowcount\n            if rowcount is not None:\n                rowcounts.append(rowcount)\n\n            sparse_sizes[0] += tensor.sparse_size(0)\n            sparse_sizes[1] = max(sparse_sizes[1], tensor.sparse_size(1))\n            nnz += tensor.nnz()\n\n        row: Optional[torch.Tensor] = None\n        if len(rows) == len(tensors):\n            row = torch.cat(rows, dim=0)\n\n        rowptr: Optional[torch.Tensor] = None\n        if len(rowptrs) == len(tensors):\n            rowptr = torch.cat(rowptrs, dim=0)\n\n        col = torch.cat(cols, dim=0)\n\n        value: Optional[torch.Tensor] = None\n        if len(values) == len(tensors):\n            value = torch.cat(values, dim=0)\n\n        rowcount: Optional[torch.Tensor] = None\n        if len(rowcounts) == len(tensors):\n            rowcount = torch.cat(rowcounts, dim=0)\n\n        storage = SparseStorage(row=row, rowptr=rowptr, col=col, value=value,\n                                sparse_sizes=sparse_sizes, rowcount=rowcount,\n                                colptr=None, colcount=None, csr2csc=None,\n                                csc2csr=None, is_sorted=True)\n        return tensors[0].from_storage(storage)\n\n    elif dim == 1:\n        rows: List[torch.Tensor] = []\n        cols: List[torch.Tensor] = []\n        values: List[torch.Tensor] = []\n        sparse_sizes: List[int] = [0, 0]\n        colptrs: List[torch.Tensor] = []\n        colcounts: List[torch.Tensor] = []\n\n        nnz: int = 0\n        for tensor in tensors:\n            row, col, value = tensor.coo()\n\n            rows.append(row)\n\n            cols.append(tensor.storage._col + sparse_sizes[1])\n\n            if value is not None:\n                values.append(value)\n\n            colptr = tensor.storage._colptr\n            if colptr is not None:\n                if len(colptrs) > 0:\n                    colptr = colptr[1:]\n                colptrs.append(colptr + nnz)\n\n            colcount = tensor.storage._colcount\n            if colcount is not None:\n                colcounts.append(colcount)\n\n            sparse_sizes[0] = max(sparse_sizes[0], tensor.sparse_size(0))\n            sparse_sizes[1] += tensor.sparse_size(1)\n            nnz += tensor.nnz()\n\n        row = torch.cat(rows, dim=0)\n\n        col = torch.cat(cols, dim=0)\n\n        value: Optional[torch.Tensor] = None\n        if len(values) == len(tensors):\n            value = torch.cat(values, dim=0)\n\n        colptr: Optional[torch.Tensor] = None\n        if len(colptrs) == len(tensors):\n            colptr = torch.cat(colptrs, dim=0)\n\n        colcount: Optional[torch.Tensor] = None\n        if len(colcounts) == len(tensors):\n            colcount = torch.cat(colcounts, dim=0)\n\n        storage = SparseStorage(row=row, rowptr=None, col=col, value=value,\n                                sparse_sizes=sparse_sizes, rowcount=None,\n                                colptr=colptr, colcount=colcount, csr2csc=None,\n                                csc2csr=None, is_sorted=False)\n        return tensors[0].from_storage(storage)\n\n    elif dim > 1 and dim < tensors[0].dim():\n        values: List[torch.Tensor] = []\n        for tensor in tensors:\n            value = tensor.storage.value()\n            if value is not None:\n                values.append(value)\n\n        value: Optional[torch.Tensor] = None\n        if len(values) == len(tensors):\n            value = torch.cat(values, dim=dim - 1)\n\n        return tensors[0].set_value(value, layout='coo')\n    else:\n        raise IndexError(\n            (f'Dimension out of range: Expected to be in range of '\n             f'[{-tensors[0].dim()}, {tensors[0].dim() - 1}], but got {dim}.'))\n\n\ndef cat_diag(tensors: List[SparseTensor]) -> SparseTensor:\n    assert len(tensors) > 0\n\n    rows: List[torch.Tensor] = []\n    rowptrs: List[torch.Tensor] = []\n    cols: List[torch.Tensor] = []\n    values: List[torch.Tensor] = []\n    sparse_sizes: List[int] = [0, 0]\n    rowcounts: List[torch.Tensor] = []\n    colptrs: List[torch.Tensor] = []\n    colcounts: List[torch.Tensor] = []\n    csr2cscs: List[torch.Tensor] = []\n    csc2csrs: List[torch.Tensor] = []\n\n    nnz: int = 0\n    for tensor in tensors:\n        row = tensor.storage._row\n        if row is not None:\n            rows.append(row + sparse_sizes[0])\n\n        rowptr = tensor.storage._rowptr\n        if rowptr is not None:\n            if len(rowptrs) > 0:\n                rowptr = rowptr[1:]\n            rowptrs.append(rowptr + nnz)\n\n        cols.append(tensor.storage._col + sparse_sizes[1])\n\n        value = tensor.storage._value\n        if value is not None:\n            values.append(value)\n\n        rowcount = tensor.storage._rowcount\n        if rowcount is not None:\n            rowcounts.append(rowcount)\n\n        colptr = tensor.storage._colptr\n        if colptr is not None:\n            if len(colptrs) > 0:\n                colptr = colptr[1:]\n            colptrs.append(colptr + nnz)\n\n        colcount = tensor.storage._colcount\n        if colcount is not None:\n            colcounts.append(colcount)\n\n        csr2csc = tensor.storage._csr2csc\n        if csr2csc is not None:\n            csr2cscs.append(csr2csc + nnz)\n\n        csc2csr = tensor.storage._csc2csr\n        if csc2csr is not None:\n            csc2csrs.append(csc2csr + nnz)\n\n        sparse_sizes[0] += tensor.sparse_size(0)\n        sparse_sizes[1] += tensor.sparse_size(1)\n        nnz += tensor.nnz()\n\n    row: Optional[torch.Tensor] = None\n    if len(rows) == len(tensors):\n        row = torch.cat(rows, dim=0)\n\n    rowptr: Optional[torch.Tensor] = None\n    if len(rowptrs) == len(tensors):\n        rowptr = torch.cat(rowptrs, dim=0)\n\n    col = torch.cat(cols, dim=0)\n\n    value: Optional[torch.Tensor] = None\n    if len(values) == len(tensors):\n        value = torch.cat(values, dim=0)\n\n    rowcount: Optional[torch.Tensor] = None\n    if len(rowcounts) == len(tensors):\n        rowcount = torch.cat(rowcounts, dim=0)\n\n    colptr: Optional[torch.Tensor] = None\n    if len(colptrs) == len(tensors):\n        colptr = torch.cat(colptrs, dim=0)\n\n    colcount: Optional[torch.Tensor] = None\n    if len(colcounts) == len(tensors):\n        colcount = torch.cat(colcounts, dim=0)\n\n    csr2csc: Optional[torch.Tensor] = None\n    if len(csr2cscs) == len(tensors):\n        csr2csc = torch.cat(csr2cscs, dim=0)\n\n    csc2csr: Optional[torch.Tensor] = None\n    if len(csc2csrs) == len(tensors):\n        csc2csr = torch.cat(csc2csrs, dim=0)\n\n    storage = SparseStorage(row=row, rowptr=rowptr, col=col, value=value,\n                            sparse_sizes=sparse_sizes, rowcount=rowcount,\n                            colptr=colptr, colcount=colcount, csr2csc=csr2csc,\n                            csc2csr=csc2csr, is_sorted=True)\n    return tensors[0].from_storage(storage)\n"""
torch_sparse/coalesce.py,1,"b'import torch\nfrom torch_sparse.storage import SparseStorage\n\n\ndef coalesce(index, value, m, n, op=""add""):\n    """"""Row-wise sorts :obj:`value` and removes duplicate entries. Duplicate\n    entries are removed by scattering them together. For scattering, any\n    operation of `""torch_scatter""<https://github.com/rusty1s/pytorch_scatter>`_\n    can be used.\n\n    Args:\n        index (:class:`LongTensor`): The index tensor of sparse matrix.\n        value (:class:`Tensor`): The value tensor of sparse matrix.\n        m (int): The first dimension of corresponding dense matrix.\n        n (int): The second dimension of corresponding dense matrix.\n        op (string, optional): The scatter operation to use. (default:\n            :obj:`""add""`)\n\n    :rtype: (:class:`LongTensor`, :class:`Tensor`)\n    """"""\n\n    storage = SparseStorage(row=index[0], col=index[1], value=value,\n                            sparse_sizes=(m, n), is_sorted=False)\n    storage = storage.coalesce(reduce=op)\n    return torch.stack([storage.row(), storage.col()], dim=0), storage.value()\n'"
torch_sparse/convert.py,2,"b'import numpy as np\nimport scipy.sparse\nimport torch\nfrom torch import from_numpy\n\n\ndef to_torch_sparse(index, value, m, n):\n    return torch.sparse_coo_tensor(index.detach(), value, (m, n))\n\n\ndef from_torch_sparse(A):\n    return A.indices().detach(), A.values()\n\n\ndef to_scipy(index, value, m, n):\n    assert not index.is_cuda and not value.is_cuda\n    (row, col), data = index.detach(), value.detach()\n    return scipy.sparse.coo_matrix((data, (row, col)), (m, n))\n\n\ndef from_scipy(A):\n    A = A.tocoo()\n    row, col, value = A.row.astype(np.int64), A.col.astype(np.int64), A.data\n    row, col, value = from_numpy(row), from_numpy(col), from_numpy(value)\n    index = torch.stack([row, col], dim=0)\n    return index, value\n'"
torch_sparse/diag.py,5,"b'from typing import Optional\n\nimport torch\nfrom torch_sparse.storage import SparseStorage\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef remove_diag(src: SparseTensor, k: int = 0) -> SparseTensor:\n    row, col, value = src.coo()\n    inv_mask = row != col if k == 0 else row != (col - k)\n    new_row, new_col = row[inv_mask], col[inv_mask]\n\n    if value is not None:\n        value = value[inv_mask]\n\n    rowcount = src.storage._rowcount\n    colcount = src.storage._colcount\n    if rowcount is not None or colcount is not None:\n        mask = ~inv_mask\n        if rowcount is not None:\n            rowcount = rowcount.clone()\n            rowcount[row[mask]] -= 1\n        if colcount is not None:\n            colcount = colcount.clone()\n            colcount[col[mask]] -= 1\n\n    storage = SparseStorage(row=new_row, rowptr=None, col=new_col, value=value,\n                            sparse_sizes=src.sparse_sizes(), rowcount=rowcount,\n                            colptr=None, colcount=colcount, csr2csc=None,\n                            csc2csr=None, is_sorted=True)\n    return src.from_storage(storage)\n\n\ndef set_diag(src: SparseTensor, values: Optional[torch.Tensor] = None,\n             k: int = 0) -> SparseTensor:\n    src = remove_diag(src, k=k)\n    row, col, value = src.coo()\n\n    mask = torch.ops.torch_sparse.non_diag_mask(row, col, src.size(0),\n                                                src.size(1), k)\n    inv_mask = ~mask\n\n    start, num_diag = -k if k < 0 else 0, mask.numel() - row.numel()\n    diag = torch.arange(start, start + num_diag, device=row.device)\n\n    new_row = row.new_empty(mask.size(0))\n    new_row[mask] = row\n    new_row[inv_mask] = diag\n\n    new_col = col.new_empty(mask.size(0))\n    new_col[mask] = col\n    new_col[inv_mask] = diag.add_(k)\n\n    new_value: Optional[torch.Tensor] = None\n    if value is not None:\n        new_value = value.new_empty((mask.size(0), ) + value.size()[1:])\n        new_value[mask] = value\n        if values is not None:\n            new_value[inv_mask] = values\n        else:\n            new_value[inv_mask] = torch.ones((num_diag, ), dtype=value.dtype,\n                                             device=value.device)\n\n    rowcount = src.storage._rowcount\n    if rowcount is not None:\n        rowcount = rowcount.clone()\n        rowcount[start:start + num_diag] += 1\n\n    colcount = src.storage._colcount\n    if colcount is not None:\n        colcount = colcount.clone()\n        colcount[start + k:start + num_diag + k] += 1\n\n    storage = SparseStorage(row=new_row, rowptr=None, col=new_col,\n                            value=new_value, sparse_sizes=src.sparse_sizes(),\n                            rowcount=rowcount, colptr=None, colcount=colcount,\n                            csr2csc=None, csc2csr=None, is_sorted=True)\n    return src.from_storage(storage)\n\n\ndef fill_diag(src: SparseTensor, fill_value: int, k: int = 0) -> SparseTensor:\n    num_diag = min(src.sparse_size(0), src.sparse_size(1) - k)\n    if k < 0:\n        num_diag = min(src.sparse_size(0) + k, src.sparse_size(1))\n\n    value = src.storage.value()\n    if value is not None:\n        sizes = [num_diag] + src.sizes()[2:]\n        return set_diag(src, value.new_full(sizes, fill_value), k)\n    else:\n        return set_diag(src, None, k)\n\n\nSparseTensor.remove_diag = lambda self, k=0: remove_diag(self, k)\nSparseTensor.set_diag = lambda self, values=None, k=0: set_diag(\n    self, values, k)\nSparseTensor.fill_diag = lambda self, fill_value, k=0: fill_diag(\n    self, fill_value, k)\n'"
torch_sparse/eye.py,7,"b'import torch\n\n\ndef eye(m, dtype=None, device=None):\n    """"""Returns a sparse matrix with ones on the diagonal and zeros elsewhere.\n\n    Args:\n        m (int): The first dimension of corresponding dense matrix.\n        dtype (`torch.dtype`, optional): The desired data type of returned\n            value vector. (default is set by `torch.set_default_tensor_type()`)\n        device (`torch.device`, optional): The desired device of returned\n            tensors. (default is set by `torch.set_default_tensor_type()`)\n\n    :rtype: (:class:`LongTensor`, :class:`Tensor`)\n    """"""\n\n    row = torch.arange(m, dtype=torch.long, device=device)\n    index = torch.stack([row, row], dim=0)\n\n    value = torch.ones(m, dtype=dtype, device=device)\n\n    return index, value\n'"
torch_sparse/index_select.py,8,"b""from typing import Optional\n\nimport torch\nfrom torch_scatter import gather_csr\nfrom torch_sparse.storage import SparseStorage, get_layout\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef index_select(src: SparseTensor, dim: int,\n                 idx: torch.Tensor) -> SparseTensor:\n    dim = src.dim() + dim if dim < 0 else dim\n    assert idx.dim() == 1\n\n    if dim == 0:\n        old_rowptr, col, value = src.csr()\n        rowcount = src.storage.rowcount()\n\n        rowcount = rowcount[idx]\n\n        rowptr = col.new_zeros(idx.size(0) + 1)\n        torch.cumsum(rowcount, dim=0, out=rowptr[1:])\n\n        row = torch.arange(idx.size(0),\n                           device=col.device).repeat_interleave(rowcount)\n\n        perm = torch.arange(row.size(0), device=row.device)\n        perm += gather_csr(old_rowptr[idx] - rowptr[:-1], rowptr)\n\n        col = col[perm]\n\n        if value is not None:\n            value = value[perm]\n\n        sparse_sizes = (idx.size(0), src.sparse_size(1))\n\n        storage = SparseStorage(row=row, rowptr=rowptr, col=col, value=value,\n                                sparse_sizes=sparse_sizes, rowcount=rowcount,\n                                colptr=None, colcount=None, csr2csc=None,\n                                csc2csr=None, is_sorted=True)\n        return src.from_storage(storage)\n\n    elif dim == 1:\n        old_colptr, row, value = src.csc()\n        colcount = src.storage.colcount()\n\n        colcount = colcount[idx]\n\n        colptr = row.new_zeros(idx.size(0) + 1)\n        torch.cumsum(colcount, dim=0, out=colptr[1:])\n\n        col = torch.arange(idx.size(0),\n                           device=row.device).repeat_interleave(colcount)\n\n        perm = torch.arange(col.size(0), device=col.device)\n        perm += gather_csr(old_colptr[idx] - colptr[:-1], colptr)\n\n        row = row[perm]\n        csc2csr = (idx.size(0) * row + col).argsort()\n        row, col = row[csc2csr], col[csc2csr]\n\n        if value is not None:\n            value = value[perm][csc2csr]\n\n        sparse_sizes = (src.sparse_size(0), idx.size(0))\n\n        storage = SparseStorage(row=row, rowptr=None, col=col, value=value,\n                                sparse_sizes=sparse_sizes, rowcount=None,\n                                colptr=colptr, colcount=colcount, csr2csc=None,\n                                csc2csr=csc2csr, is_sorted=True)\n        return src.from_storage(storage)\n\n    else:\n        value = src.storage.value()\n        if value is not None:\n            return src.set_value(value.index_select(dim - 1, idx),\n                                 layout='coo')\n        else:\n            raise ValueError\n\n\ndef index_select_nnz(src: SparseTensor, idx: torch.Tensor,\n                     layout: Optional[str] = None) -> SparseTensor:\n    assert idx.dim() == 1\n\n    if get_layout(layout) == 'csc':\n        idx = src.storage.csc2csr()[idx]\n\n    row, col, value = src.coo()\n    row, col = row[idx], col[idx]\n\n    if value is not None:\n        value = value[idx]\n\n    return SparseTensor(row=row, rowptr=None, col=col, value=value,\n                        sparse_sizes=src.sparse_sizes(), is_sorted=True)\n\n\nSparseTensor.index_select = lambda self, dim, idx: index_select(self, dim, idx)\ntmp = lambda self, idx, layout=None: index_select_nnz(  # noqa\n    self, idx, layout)\nSparseTensor.index_select_nnz = tmp\n"""
torch_sparse/masked_select.py,4,"b""from typing import Optional\n\nimport torch\nfrom torch_sparse.storage import SparseStorage, get_layout\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef masked_select(src: SparseTensor, dim: int,\n                  mask: torch.Tensor) -> SparseTensor:\n    dim = src.dim() + dim if dim < 0 else dim\n\n    assert mask.dim() == 1\n    storage = src.storage\n\n    if dim == 0:\n        row, col, value = src.coo()\n        rowcount = src.storage.rowcount()\n\n        rowcount = rowcount[mask]\n\n        mask = mask[row]\n        row = torch.arange(rowcount.size(0),\n                           device=row.device).repeat_interleave(rowcount)\n\n        col = col[mask]\n\n        if value is not None:\n            value = value[mask]\n\n        sparse_sizes = (rowcount.size(0), src.sparse_size(1))\n\n        storage = SparseStorage(row=row, rowptr=None, col=col, value=value,\n                                sparse_sizes=sparse_sizes, rowcount=rowcount,\n                                colcount=None, colptr=None, csr2csc=None,\n                                csc2csr=None, is_sorted=True)\n        return src.from_storage(storage)\n\n    elif dim == 1:\n        row, col, value = src.coo()\n        csr2csc = src.storage.csr2csc()\n        row = row[csr2csc]\n        col = col[csr2csc]\n        colcount = src.storage.colcount()\n\n        colcount = colcount[mask]\n\n        mask = mask[col]\n        col = torch.arange(colcount.size(0),\n                           device=col.device).repeat_interleave(colcount)\n        row = row[mask]\n        csc2csr = (colcount.size(0) * row + col).argsort()\n        row, col = row[csc2csr], col[csc2csr]\n\n        if value is not None:\n            value = value[csr2csc][mask][csc2csr]\n\n        sparse_sizes = (src.sparse_size(0), colcount.size(0))\n\n        storage = SparseStorage(row=row, rowptr=None, col=col, value=value,\n                                sparse_sizes=sparse_sizes, rowcount=None,\n                                colcount=colcount, colptr=None, csr2csc=None,\n                                csc2csr=csc2csr, is_sorted=True)\n        return src.from_storage(storage)\n\n    else:\n        value = src.storage.value()\n        if value is not None:\n            idx = mask.nonzero().flatten()\n            return src.set_value(value.index_select(dim - 1, idx),\n                                 layout='coo')\n        else:\n            raise ValueError\n\n\ndef masked_select_nnz(src: SparseTensor, mask: torch.Tensor,\n                      layout: Optional[str] = None) -> SparseTensor:\n    assert mask.dim() == 1\n\n    if get_layout(layout) == 'csc':\n        mask = mask[src.storage.csc2csr()]\n\n    row, col, value = src.coo()\n    row, col = row[mask], col[mask]\n\n    if value is not None:\n        value = value[mask]\n\n    return SparseTensor(row=row, rowptr=None, col=col, value=value,\n                        sparse_sizes=src.sparse_sizes(), is_sorted=True)\n\n\nSparseTensor.masked_select = lambda self, dim, mask: masked_select(\n    self, dim, mask)\ntmp = lambda self, mask, layout=None: masked_select_nnz(  # noqa\n    self, mask, layout)\nSparseTensor.masked_select_nnz = tmp\n"""
torch_sparse/matmul.py,14,"b'from typing import Union, Tuple\n\nimport torch\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef spmm_sum(src: SparseTensor, other: torch.Tensor) -> torch.Tensor:\n    rowptr, col, value = src.csr()\n\n    row = src.storage._row\n    csr2csc = src.storage._csr2csc\n    colptr = src.storage._colptr\n\n    if value is not None and value.requires_grad:\n        row = src.storage.row()\n\n    if other.requires_grad:\n        row = src.storage.row()\n        csr2csc = src.storage.csr2csc()\n        colptr = src.storage.colptr()\n\n    return torch.ops.torch_sparse.spmm_sum(row, rowptr, col, value, colptr,\n                                           csr2csc, other)\n\n\ndef spmm_add(src: SparseTensor, other: torch.Tensor) -> torch.Tensor:\n    return spmm_sum(src, other)\n\n\ndef spmm_mean(src: SparseTensor, other: torch.Tensor) -> torch.Tensor:\n    rowptr, col, value = src.csr()\n\n    row = src.storage._row\n    rowcount = src.storage._rowcount\n    csr2csc = src.storage._csr2csc\n    colptr = src.storage._colptr\n\n    if value is not None and value.requires_grad:\n        row = src.storage.row()\n\n    if other.requires_grad:\n        row = src.storage.row()\n        rowcount = src.storage.rowcount()\n        csr2csc = src.storage.csr2csc()\n        colptr = src.storage.colptr()\n\n    return torch.ops.torch_sparse.spmm_mean(row, rowptr, col, value, rowcount,\n                                            colptr, csr2csc, other)\n\n\ndef spmm_min(src: SparseTensor,\n             other: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n    rowptr, col, value = src.csr()\n    return torch.ops.torch_sparse.spmm_min(rowptr, col, value, other)\n\n\ndef spmm_max(src: SparseTensor,\n             other: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n    rowptr, col, value = src.csr()\n    return torch.ops.torch_sparse.spmm_max(rowptr, col, value, other)\n\n\ndef spmm(src: SparseTensor, other: torch.Tensor,\n         reduce: str = ""sum"") -> torch.Tensor:\n    if reduce == \'sum\' or reduce == \'add\':\n        return spmm_sum(src, other)\n    elif reduce == \'mean\':\n        return spmm_mean(src, other)\n    elif reduce == \'min\':\n        return spmm_min(src, other)[0]\n    elif reduce == \'max\':\n        return spmm_max(src, other)[0]\n    else:\n        raise ValueError\n\n\ndef spspmm_sum(src: SparseTensor, other: SparseTensor) -> SparseTensor:\n    assert src.sparse_size(1) == other.sparse_size(0)\n    rowptrA, colA, valueA = src.csr()\n    rowptrB, colB, valueB = other.csr()\n    M, K = src.sparse_size(0), other.sparse_size(1)\n    rowptrC, colC, valueC = torch.ops.torch_sparse.spspmm_sum(\n        rowptrA, colA, valueA, rowptrB, colB, valueB, K)\n    return SparseTensor(row=None, rowptr=rowptrC, col=colC, value=valueC,\n                        sparse_sizes=(M, K), is_sorted=True)\n\n\ndef spspmm_add(src: SparseTensor, other: SparseTensor) -> SparseTensor:\n    return spspmm_sum(src, other)\n\n\ndef spspmm(src: SparseTensor, other: SparseTensor,\n           reduce: str = ""sum"") -> SparseTensor:\n    if reduce == \'sum\' or reduce == \'add\':\n        return spspmm_sum(src, other)\n    elif reduce == \'mean\' or reduce == \'min\' or reduce == \'max\':\n        raise NotImplementedError\n    else:\n        raise ValueError\n\n\ndef matmul(src: SparseTensor, other: Union[torch.Tensor, SparseTensor],\n           reduce: str = ""sum""):\n    if torch.is_tensor(other):\n        return spmm(src, other, reduce)\n    elif isinstance(other, SparseTensor):\n        return spspmm(src, other, reduce)\n    else:\n        raise ValueError\n\n\nSparseTensor.spmm = lambda self, other, reduce=""sum"": spmm(self, other, reduce)\nSparseTensor.spspmm = lambda self, other, reduce=""sum"": spspmm(\n    self, other, reduce)\nSparseTensor.matmul = lambda self, other, reduce=""sum"": matmul(\n    self, other, reduce)\nSparseTensor.__matmul__ = lambda self, other: matmul(self, other, \'sum\')\n'"
torch_sparse/metis.py,5,"b'from typing import Tuple, Optional\n\nimport torch\nfrom torch_sparse.tensor import SparseTensor\nfrom torch_sparse.permute import permute\n\n\ndef weight2metis(weight: torch.Tensor) -> Optional[torch.Tensor]:\n    sorted_weight = weight.sort()[0]\n    diff = sorted_weight[1:] - sorted_weight[:-1]\n    if diff.sum() == 0:\n        return None\n    weight_min, weight_max = sorted_weight[0], sorted_weight[-1]\n    srange = weight_max - weight_min\n    min_diff = diff.min()\n    scale = (min_diff / srange).item()\n    tick, arange = scale.as_integer_ratio()\n    weight_ratio = (weight - weight_min).div_(srange).mul_(arange).add_(tick)\n    return weight_ratio.to(torch.long)\n\n\ndef partition(src: SparseTensor, num_parts: int, recursive: bool = False,\n              weighted=False\n              ) -> Tuple[SparseTensor, torch.Tensor, torch.Tensor]:\n    rowptr, col, value = src.csr()\n    rowptr, col = rowptr.cpu(), col.cpu()\n\n    if value is not None and weighted:\n        assert value.numel() == col.numel()\n        value = value.view(-1).detach().cpu()\n        if value.is_floating_point():\n            value = weight2metis(value)\n    else:\n        value = None\n\n    cluster = torch.ops.torch_sparse.partition(rowptr, col, value, num_parts,\n                                               recursive)\n    cluster = cluster.to(src.device())\n\n    cluster, perm = cluster.sort()\n    out = permute(src, perm)\n    partptr = torch.ops.torch_sparse.ind2ptr(cluster, num_parts)\n\n    return out, partptr, perm\n\n\nSparseTensor.partition = partition\n'"
torch_sparse/mul.py,4,"b""from typing import Optional\n\nimport torch\nfrom torch_scatter import gather_csr\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef mul(src: SparseTensor, other: torch.Tensor) -> SparseTensor:\n    rowptr, col, value = src.csr()\n    if other.size(0) == src.size(0) and other.size(1) == 1:  # Row-wise...\n        other = gather_csr(other.squeeze(1), rowptr)\n        pass\n    elif other.size(0) == 1 and other.size(1) == src.size(1):  # Col-wise...\n        other = other.squeeze(0)[col]\n    else:\n        raise ValueError(\n            f'Size mismatch: Expected size ({src.size(0)}, 1, ...) or '\n            f'(1, {src.size(1)}, ...), but got size {other.size()}.')\n\n    if value is not None:\n        value = other.to(value.dtype).mul_(value)\n    else:\n        value = other\n    return src.set_value(value, layout='coo')\n\n\ndef mul_(src: SparseTensor, other: torch.Tensor) -> SparseTensor:\n    rowptr, col, value = src.csr()\n    if other.size(0) == src.size(0) and other.size(1) == 1:  # Row-wise...\n        other = gather_csr(other.squeeze(1), rowptr)\n        pass\n    elif other.size(0) == 1 and other.size(1) == src.size(1):  # Col-wise...\n        other = other.squeeze(0)[col]\n    else:\n        raise ValueError(\n            f'Size mismatch: Expected size ({src.size(0)}, 1, ...) or '\n            f'(1, {src.size(1)}, ...), but got size {other.size()}.')\n\n    if value is not None:\n        value = value.mul_(other.to(value.dtype))\n    else:\n        value = other\n    return src.set_value_(value, layout='coo')\n\n\ndef mul_nnz(src: SparseTensor, other: torch.Tensor,\n            layout: Optional[str] = None) -> SparseTensor:\n    value = src.storage.value()\n    if value is not None:\n        value = value.mul(other.to(value.dtype))\n    else:\n        value = other\n    return src.set_value(value, layout=layout)\n\n\ndef mul_nnz_(src: SparseTensor, other: torch.Tensor,\n             layout: Optional[str] = None) -> SparseTensor:\n    value = src.storage.value()\n    if value is not None:\n        value = value.mul_(other.to(value.dtype))\n    else:\n        value = other\n    return src.set_value_(value, layout=layout)\n\n\nSparseTensor.mul = lambda self, other: mul(self, other)\nSparseTensor.mul_ = lambda self, other: mul_(self, other)\nSparseTensor.mul_nnz = lambda self, other, layout=None: mul_nnz(\n    self, other, layout)\nSparseTensor.mul_nnz_ = lambda self, other, layout=None: mul_nnz_(\n    self, other, layout)\nSparseTensor.__mul__ = SparseTensor.mul\nSparseTensor.__rmul__ = SparseTensor.mul\nSparseTensor.__imul__ = SparseTensor.mul_\n"""
torch_sparse/narrow.py,0,"b""from typing import Tuple\n\nfrom torch_sparse.storage import SparseStorage\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef narrow(src: SparseTensor, dim: int, start: int,\n           length: int) -> SparseTensor:\n    if dim < 0:\n        dim = src.dim() + dim\n\n    if start < 0:\n        start = src.size(dim) + start\n\n    if dim == 0:\n        rowptr, col, value = src.csr()\n\n        rowptr = rowptr.narrow(0, start=start, length=length + 1)\n        row_start = rowptr[0]\n        rowptr = rowptr - row_start\n        row_length = rowptr[-1]\n\n        row = src.storage._row\n        if row is not None:\n            row = row.narrow(0, row_start, row_length) - start\n\n        col = col.narrow(0, row_start, row_length)\n\n        if value is not None:\n            value = value.narrow(0, row_start, row_length)\n\n        sparse_sizes = (length, src.sparse_size(1))\n\n        rowcount = src.storage._rowcount\n        if rowcount is not None:\n            rowcount = rowcount.narrow(0, start=start, length=length)\n\n        storage = SparseStorage(row=row, rowptr=rowptr, col=col, value=value,\n                                sparse_sizes=sparse_sizes, rowcount=rowcount,\n                                colptr=None, colcount=None, csr2csc=None,\n                                csc2csr=None, is_sorted=True)\n        return src.from_storage(storage)\n\n    elif dim == 1:\n        # This is faster than accessing `csc()` contrary to the `dim=0` case.\n        row, col, value = src.coo()\n        mask = (col >= start) & (col < start + length)\n\n        row = row[mask]\n        col = col[mask] - start\n\n        if value is not None:\n            value = value[mask]\n\n        sparse_sizes = (src.sparse_size(0), length)\n\n        colptr = src.storage._colptr\n        if colptr is not None:\n            colptr = colptr.narrow(0, start=start, length=length + 1)\n            colptr = colptr - colptr[0]\n\n        colcount = src.storage._colcount\n        if colcount is not None:\n            colcount = colcount.narrow(0, start=start, length=length)\n\n        storage = SparseStorage(row=row, rowptr=None, col=col, value=value,\n                                sparse_sizes=sparse_sizes, rowcount=None,\n                                colptr=colptr, colcount=colcount, csr2csc=None,\n                                csc2csr=None, is_sorted=True)\n        return src.from_storage(storage)\n\n    else:\n        value = src.storage.value()\n        if value is not None:\n            return src.set_value(value.narrow(dim - 1, start, length),\n                                 layout='coo')\n        else:\n            raise ValueError\n\n\ndef __narrow_diag__(src: SparseTensor, start: Tuple[int, int],\n                    length: Tuple[int, int]) -> SparseTensor:\n    # This function builds the inverse operation of `cat_diag` and should hence\n    # only be used on *diagonally stacked* sparse matrices.\n    # That's the reason why this method is marked as *private*.\n\n    rowptr, col, value = src.csr()\n\n    rowptr = rowptr.narrow(0, start=start[0], length=length[0] + 1)\n    row_start = int(rowptr[0])\n    rowptr = rowptr - row_start\n    row_length = int(rowptr[-1])\n\n    row = src.storage._row\n    if row is not None:\n        row = row.narrow(0, row_start, row_length) - start[0]\n\n    col = col.narrow(0, row_start, row_length) - start[1]\n\n    if value is not None:\n        value = value.narrow(0, row_start, row_length)\n\n    sparse_sizes = length\n\n    rowcount = src.storage._rowcount\n    if rowcount is not None:\n        rowcount = rowcount.narrow(0, start[0], length[0])\n\n    colptr = src.storage._colptr\n    if colptr is not None:\n        colptr = colptr.narrow(0, start[1], length[1] + 1)\n        colptr = colptr - int(colptr[0])  # i.e. `row_start`\n\n    colcount = src.storage._colcount\n    if colcount is not None:\n        colcount = colcount.narrow(0, start[1], length[1])\n\n    csr2csc = src.storage._csr2csc\n    if csr2csc is not None:\n        csr2csc = csr2csc.narrow(0, row_start, row_length) - row_start\n\n    csc2csr = src.storage._csc2csr\n    if csc2csr is not None:\n        csc2csr = csc2csr.narrow(0, row_start, row_length) - row_start\n\n    storage = SparseStorage(row=row, rowptr=rowptr, col=col, value=value,\n                            sparse_sizes=sparse_sizes, rowcount=rowcount,\n                            colptr=colptr, colcount=colcount, csr2csc=csr2csc,\n                            csc2csr=csc2csr, is_sorted=True)\n    return src.from_storage(storage)\n\n\nSparseTensor.narrow = lambda self, dim, start, length: narrow(\n    self, dim, start, length)\nSparseTensor.__narrow_diag__ = lambda self, start, length: __narrow_diag__(\n    self, start, length)\n"""
torch_sparse/padding.py,7,"b'from typing import Tuple, List\n\nimport torch\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef padded_index(src: SparseTensor, binptr: torch.Tensor\n                 ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.\n                            Tensor, List[int], List[int]]:\n    return torch.ops.torch_sparse.padded_index(src.storage.rowptr(),\n                                               src.storage.col(),\n                                               src.storage.rowcount(), binptr)\n\n\ndef padded_index_select(src: torch.Tensor, index: torch.Tensor,\n                        fill_value: float = 0.) -> torch.Tensor:\n    fill_value = torch.tensor(fill_value, dtype=src.dtype)\n    return torch.ops.torch_sparse.padded_index_select(src, index, fill_value)\n\n\nSparseTensor.padded_index = padded_index\n'"
torch_sparse/permute.py,1,"b'import torch\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef permute(src: SparseTensor, perm: torch.Tensor) -> SparseTensor:\n    assert src.is_quadratic()\n    return src.index_select(0, perm).index_select(1, perm)\n\n\nSparseTensor.permute = lambda self, perm: permute(self, perm)\n'"
torch_sparse/reduce.py,9,"b""from typing import Optional\n\nimport torch\nfrom torch_scatter import scatter, segment_csr\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef reduction(src: SparseTensor, dim: Optional[int] = None,\n              reduce: str = 'sum') -> torch.Tensor:\n    value = src.storage.value()\n\n    if dim is None:\n        if value is not None:\n            if reduce == 'sum' or reduce == 'add':\n                return value.sum()\n            elif reduce == 'mean':\n                return value.mean()\n            elif reduce == 'min':\n                return value.min()\n            elif reduce == 'max':\n                return value.max()\n            else:\n                raise ValueError\n        else:\n            if reduce == 'sum' or reduce == 'add':\n                return torch.tensor(src.nnz(), dtype=src.dtype(),\n                                    device=src.device())\n            elif reduce == 'mean' or reduce == 'min' or reduce == 'max':\n                return torch.tensor(1, dtype=src.dtype(), device=src.device())\n            else:\n                raise ValueError\n    else:\n        if dim < 0:\n            dim = src.dim() + dim\n\n        if dim == 0 and value is not None:\n            col = src.storage.col()\n            return scatter(value, col, dim=0, dim_size=src.size(0))\n        elif dim == 0 and value is None:\n            if reduce == 'sum' or reduce == 'add':\n                return src.storage.colcount().to(src.dtype())\n            elif reduce == 'mean' or reduce == 'min' or reduce == 'max':\n                return torch.ones(src.size(1), dtype=src.dtype())\n            else:\n                raise ValueError\n        elif dim == 1 and value is not None:\n            return segment_csr(value, src.storage.rowptr(), None, reduce)\n        elif dim == 1 and value is None:\n            if reduce == 'sum' or reduce == 'add':\n                return src.storage.rowcount().to(src.dtype())\n            elif reduce == 'mean' or reduce == 'min' or reduce == 'max':\n                return torch.ones(src.size(0), dtype=src.dtype())\n            else:\n                raise ValueError\n        elif dim > 1 and value is not None:\n            if reduce == 'sum' or reduce == 'add':\n                return value.sum(dim=dim - 1)\n            elif reduce == 'mean':\n                return value.mean(dim=dim - 1)\n            elif reduce == 'min':\n                return value.min(dim=dim - 1)[0]\n            elif reduce == 'max':\n                return value.max(dim=dim - 1)[0]\n            else:\n                raise ValueError\n        else:\n            raise ValueError\n\n\ndef sum(src: SparseTensor, dim: Optional[int] = None) -> torch.Tensor:\n    return reduction(src, dim, reduce='sum')\n\n\ndef mean(src: SparseTensor, dim: Optional[int] = None) -> torch.Tensor:\n    return reduction(src, dim, reduce='mean')\n\n\ndef min(src: SparseTensor, dim: Optional[int] = None) -> torch.Tensor:\n    return reduction(src, dim, reduce='min')\n\n\ndef max(src: SparseTensor, dim: Optional[int] = None) -> torch.Tensor:\n    return reduction(src, dim, reduce='max')\n\n\nSparseTensor.sum = lambda self, dim=None: sum(self, dim)\nSparseTensor.mean = lambda self, dim=None: mean(self, dim)\nSparseTensor.min = lambda self, dim=None: min(self, dim)\nSparseTensor.max = lambda self, dim=None: max(self, dim)\n"""
torch_sparse/rw.py,3,"b'import torch\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef random_walk(src: SparseTensor, start: torch.Tensor,\n                walk_length: int) -> torch.Tensor:\n    rowptr, col, _ = src.csr()\n    return torch.ops.torch_sparse.random_walk(rowptr, col, start, walk_length)\n\n\nSparseTensor.random_walk = random_walk\n'"
torch_sparse/saint.py,3,"b'from typing import Tuple\n\nimport torch\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef saint_subgraph(src: SparseTensor, node_idx: torch.Tensor\n                   ) -> Tuple[SparseTensor, torch.Tensor]:\n    row, col, value = src.coo()\n    rowptr = src.storage.rowptr()\n\n    data = torch.ops.torch_sparse.saint_subgraph(node_idx, rowptr, row, col)\n    row, col, edge_index = data\n\n    if value is not None:\n        value = value[edge_index]\n\n    out = SparseTensor(row=row, rowptr=None, col=col, value=value,\n                       sparse_sizes=(node_idx.size(0), node_idx.size(0)),\n                       is_sorted=True)\n\n    return out, edge_index\n\n\nSparseTensor.saint_subgraph = saint_subgraph\n'"
torch_sparse/sample.py,6,"b'from typing import Optional, Tuple\n\nimport torch\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef sample(src: SparseTensor, num_neighbors: int,\n           subset: Optional[torch.Tensor] = None) -> torch.Tensor:\n\n    rowptr, col, _ = src.csr()\n    rowcount = src.storage.rowcount()\n\n    if subset is not None:\n        rowcount = rowcount[subset]\n        rowptr = rowptr[subset]\n\n    rand = torch.rand((rowcount.size(0), num_neighbors), device=col.device)\n    rand.mul_(rowcount.to(rand.dtype).view(-1, 1))\n    rand = rand.to(torch.long)\n    rand.add_(rowptr.view(-1, 1))\n\n    return col[rand]\n\n\ndef sample_adj(src: SparseTensor, subset: torch.Tensor, num_neighbors: int,\n               replace: bool = False) -> Tuple[SparseTensor, torch.Tensor]:\n\n    rowptr, col, value = src.csr()\n    rowcount = src.storage.rowcount()\n\n    rowptr, col, n_id, e_id = torch.ops.torch_sparse.sample_adj(\n        rowptr, col, rowcount, subset, num_neighbors, replace)\n\n    if value is not None:\n        value = value[e_id]\n\n    out = SparseTensor(rowptr=rowptr, row=None, col=col, value=value,\n                       sparse_sizes=(subset.size(0), n_id.size(0)),\n                       is_sorted=True)\n\n    return out, n_id\n\n\nSparseTensor.sample = sample\nSparseTensor.sample_adj = sample_adj\n'"
torch_sparse/select.py,0,"b'from torch_sparse.tensor import SparseTensor\nfrom torch_sparse.narrow import narrow\n\n\ndef select(src: SparseTensor, dim: int, idx: int) -> SparseTensor:\n    return narrow(src, dim, start=idx, length=1)\n\n\nSparseTensor.select = lambda self, dim, idx: select(self, dim, idx)\n'"
torch_sparse/spmm.py,0,"b'# import torch\nfrom torch_scatter import scatter_add\n\n\ndef spmm(index, value, m, n, matrix):\n    """"""Matrix product of sparse matrix with dense matrix.\n\n    Args:\n        index (:class:`LongTensor`): The index tensor of sparse matrix.\n        value (:class:`Tensor`): The value tensor of sparse matrix.\n        m (int): The first dimension of corresponding dense matrix.\n        n (int): The second dimension of corresponding dense matrix.\n        matrix (:class:`Tensor`): The dense matrix.\n\n    :rtype: :class:`Tensor`\n    """"""\n\n    assert n == matrix.size(0)\n\n    row, col = index\n    matrix = matrix if matrix.dim() > 1 else matrix.unsqueeze(-1)\n\n    out = matrix[col]\n    out = out * value.unsqueeze(-1)\n    out = scatter_add(out, row, dim=0, dim_size=m)\n\n    return out\n'"
torch_sparse/spspmm.py,1,"b'import torch\nfrom torch_sparse.tensor import SparseTensor\nfrom torch_sparse.matmul import matmul\n\n\ndef spspmm(indexA, valueA, indexB, valueB, m, k, n, coalesced=False):\n    """"""Matrix product of two sparse tensors. Both input sparse matrices need to\n    be coalesced (use the :obj:`coalesced` attribute to force).\n\n    Args:\n        indexA (:class:`LongTensor`): The index tensor of first sparse matrix.\n        valueA (:class:`Tensor`): The value tensor of first sparse matrix.\n        indexB (:class:`LongTensor`): The index tensor of second sparse matrix.\n        valueB (:class:`Tensor`): The value tensor of second sparse matrix.\n        m (int): The first dimension of first corresponding dense matrix.\n        k (int): The second dimension of first corresponding dense matrix and\n            first dimension of second corresponding dense matrix.\n        n (int): The second dimension of second corresponding dense matrix.\n        coalesced (bool, optional): If set to :obj:`True`, will coalesce both\n            input sparse matrices. (default: :obj:`False`)\n\n    :rtype: (:class:`LongTensor`, :class:`Tensor`)\n    """"""\n\n    A = SparseTensor(row=indexA[0], col=indexA[1], value=valueA,\n                     sparse_sizes=(m, k), is_sorted=not coalesced)\n    B = SparseTensor(row=indexB[0], col=indexB[1], value=valueB,\n                     sparse_sizes=(k, n), is_sorted=not coalesced)\n\n    C = matmul(A, B)\n    row, col, value = C.coo()\n\n    return torch.stack([row, col], dim=0), value\n'"
torch_sparse/storage.py,49,"b'import warnings\nfrom typing import Optional, List, Tuple\n\nimport torch\nfrom torch_scatter import segment_csr, scatter_add\nfrom torch_sparse.utils import Final\n\nlayouts: Final[List[str]] = [\'coo\', \'csr\', \'csc\']\n\n\ndef get_layout(layout: Optional[str] = None) -> str:\n    if layout is None:\n        layout = \'coo\'\n        warnings.warn(\'`layout` argument unset, using default layout \'\n                      \'""coo"". This may lead to unexpected behaviour.\')\n    assert layout == \'coo\' or layout == \'csr\' or layout == \'csc\'\n    return layout\n\n\n@torch.jit.script\nclass SparseStorage(object):\n    _row: Optional[torch.Tensor]\n    _rowptr: Optional[torch.Tensor]\n    _col: torch.Tensor\n    _value: Optional[torch.Tensor]\n    _sparse_sizes: Tuple[int, int]\n    _rowcount: Optional[torch.Tensor]\n    _colptr: Optional[torch.Tensor]\n    _colcount: Optional[torch.Tensor]\n    _csr2csc: Optional[torch.Tensor]\n    _csc2csr: Optional[torch.Tensor]\n\n    def __init__(self, row: Optional[torch.Tensor] = None,\n                 rowptr: Optional[torch.Tensor] = None,\n                 col: Optional[torch.Tensor] = None,\n                 value: Optional[torch.Tensor] = None,\n                 sparse_sizes: Optional[Tuple[int, int]] = None,\n                 rowcount: Optional[torch.Tensor] = None,\n                 colptr: Optional[torch.Tensor] = None,\n                 colcount: Optional[torch.Tensor] = None,\n                 csr2csc: Optional[torch.Tensor] = None,\n                 csc2csr: Optional[torch.Tensor] = None,\n                 is_sorted: bool = False):\n\n        assert row is not None or rowptr is not None\n        assert col is not None\n        assert col.dtype == torch.long\n        assert col.dim() == 1\n        col = col.contiguous()\n\n        if sparse_sizes is None:\n            if rowptr is not None:\n                M = rowptr.numel() - 1\n            elif row is not None:\n                M = row.max().item() + 1\n            else:\n                raise ValueError\n            N = col.max().item() + 1\n            sparse_sizes = (int(M), int(N))\n        else:\n            assert len(sparse_sizes) == 2\n\n        if row is not None:\n            assert row.dtype == torch.long\n            assert row.device == col.device\n            assert row.dim() == 1\n            assert row.numel() == col.numel()\n            row = row.contiguous()\n\n        if rowptr is not None:\n            assert rowptr.dtype == torch.long\n            assert rowptr.device == col.device\n            assert rowptr.dim() == 1\n            assert rowptr.numel() - 1 == sparse_sizes[0]\n            rowptr = rowptr.contiguous()\n\n        if value is not None:\n            assert value.device == col.device\n            assert value.size(0) == col.size(0)\n            value = value.contiguous()\n\n        if rowcount is not None:\n            assert rowcount.dtype == torch.long\n            assert rowcount.device == col.device\n            assert rowcount.dim() == 1\n            assert rowcount.numel() == sparse_sizes[0]\n            rowcount = rowcount.contiguous()\n\n        if colptr is not None:\n            assert colptr.dtype == torch.long\n            assert colptr.device == col.device\n            assert colptr.dim() == 1\n            assert colptr.numel() - 1 == sparse_sizes[1]\n            colptr = colptr.contiguous()\n\n        if colcount is not None:\n            assert colcount.dtype == torch.long\n            assert colcount.device == col.device\n            assert colcount.dim() == 1\n            assert colcount.numel() == sparse_sizes[1]\n            colcount = colcount.contiguous()\n\n        if csr2csc is not None:\n            assert csr2csc.dtype == torch.long\n            assert csr2csc.device == col.device\n            assert csr2csc.dim() == 1\n            assert csr2csc.numel() == col.size(0)\n            csr2csc = csr2csc.contiguous()\n\n        if csc2csr is not None:\n            assert csc2csr.dtype == torch.long\n            assert csc2csr.device == col.device\n            assert csc2csr.dim() == 1\n            assert csc2csr.numel() == col.size(0)\n            csc2csr = csc2csr.contiguous()\n\n        self._row = row\n        self._rowptr = rowptr\n        self._col = col\n        self._value = value\n        self._sparse_sizes = tuple(sparse_sizes)\n        self._rowcount = rowcount\n        self._colptr = colptr\n        self._colcount = colcount\n        self._csr2csc = csr2csc\n        self._csc2csr = csc2csr\n\n        if not is_sorted:\n            idx = self._col.new_zeros(self._col.numel() + 1)\n            idx[1:] = self._sparse_sizes[1] * self.row() + self._col\n            if (idx[1:] < idx[:-1]).any():\n                perm = idx[1:].argsort()\n                self._row = self.row()[perm]\n                self._col = self._col[perm]\n                if value is not None:\n                    self._value = value[perm]\n                self._csr2csc = None\n                self._csc2csr = None\n\n    @classmethod\n    def empty(self):\n        self = SparseStorage.__new__(SparseStorage)\n        self._row = None\n        self._rowptr = None\n        self._value = None\n        self._rowcount = None\n        self._colptr = None\n        self._colcount = None\n        self._csr2csc = None\n        self._csc2csr = None\n        return self\n\n    def has_row(self) -> bool:\n        return self._row is not None\n\n    def row(self):\n        row = self._row\n        if row is not None:\n            return row\n\n        rowptr = self._rowptr\n        if rowptr is not None:\n            row = torch.ops.torch_sparse.ptr2ind(rowptr, self._col.numel())\n            self._row = row\n            return row\n\n        raise ValueError\n\n    def has_rowptr(self) -> bool:\n        return self._rowptr is not None\n\n    def rowptr(self) -> torch.Tensor:\n        rowptr = self._rowptr\n        if rowptr is not None:\n            return rowptr\n\n        row = self._row\n        if row is not None:\n            rowptr = torch.ops.torch_sparse.ind2ptr(row, self._sparse_sizes[0])\n            self._rowptr = rowptr\n            return rowptr\n\n        raise ValueError\n\n    def col(self) -> torch.Tensor:\n        return self._col\n\n    def has_value(self) -> bool:\n        return self._value is not None\n\n    def value(self) -> Optional[torch.Tensor]:\n        return self._value\n\n    def set_value_(self, value: Optional[torch.Tensor],\n                   layout: Optional[str] = None):\n        if value is not None:\n            if get_layout(layout) == \'csc\':\n                value = value[self.csc2csr()]\n            value = value.contiguous()\n            assert value.device == self._col.device\n            assert value.size(0) == self._col.numel()\n\n        self._value = value\n        return self\n\n    def set_value(self, value: Optional[torch.Tensor],\n                  layout: Optional[str] = None):\n        if value is not None:\n            if get_layout(layout) == \'csc\':\n                value = value[self.csc2csr()]\n            value = value.contiguous()\n            assert value.device == self._col.device\n            assert value.size(0) == self._col.numel()\n\n        return SparseStorage(row=self._row, rowptr=self._rowptr, col=self._col,\n                             value=value, sparse_sizes=self._sparse_sizes,\n                             rowcount=self._rowcount, colptr=self._colptr,\n                             colcount=self._colcount, csr2csc=self._csr2csc,\n                             csc2csr=self._csc2csr, is_sorted=True)\n\n    def sparse_sizes(self) -> Tuple[int, int]:\n        return self._sparse_sizes\n\n    def sparse_size(self, dim: int) -> int:\n        return self._sparse_sizes[dim]\n\n    def sparse_resize(self, sparse_sizes: Tuple[int, int]):\n        assert len(sparse_sizes) == 2\n        old_sparse_sizes, nnz = self._sparse_sizes, self._col.numel()\n\n        diff_0 = sparse_sizes[0] - old_sparse_sizes[0]\n        rowcount, rowptr = self._rowcount, self._rowptr\n        if diff_0 > 0:\n            if rowptr is not None:\n                rowptr = torch.cat([rowptr, rowptr.new_full((diff_0, ), nnz)])\n            if rowcount is not None:\n                rowcount = torch.cat([rowcount, rowcount.new_zeros(diff_0)])\n        else:\n            if rowptr is not None:\n                rowptr = rowptr[:-diff_0]\n            if rowcount is not None:\n                rowcount = rowcount[:-diff_0]\n\n        diff_1 = sparse_sizes[1] - old_sparse_sizes[1]\n        colcount, colptr = self._colcount, self._colptr\n        if diff_1 > 0:\n            if colptr is not None:\n                colptr = torch.cat([colptr, colptr.new_full((diff_1, ), nnz)])\n            if colcount is not None:\n                colcount = torch.cat([colcount, colcount.new_zeros(diff_1)])\n        else:\n            if colptr is not None:\n                colptr = colptr[:-diff_1]\n            if colcount is not None:\n                colcount = colcount[:-diff_1]\n\n        return SparseStorage(row=self._row, rowptr=rowptr, col=self._col,\n                             value=self._value, sparse_sizes=sparse_sizes,\n                             rowcount=rowcount, colptr=colptr,\n                             colcount=colcount, csr2csc=self._csr2csc,\n                             csc2csr=self._csc2csr, is_sorted=True)\n\n    def sparse_reshape(self, num_rows: int, num_cols: int):\n        assert num_rows > 0 or num_rows == -1\n        assert num_cols > 0 or num_cols == -1\n        assert num_rows > 0 or num_cols > 0\n\n        total = self.sparse_size(0) * self.sparse_size(1)\n\n        if num_rows == -1:\n            num_rows = total // num_cols\n\n        if num_cols == -1:\n            num_cols = total // num_rows\n\n        assert num_rows * num_cols == total\n\n        idx = self.sparse_size(1) * self.row() + self.col()\n\n        row = idx // num_cols\n        col = idx % num_cols\n\n        return SparseStorage(row=row, rowptr=None, col=col, value=self._value,\n                             sparse_sizes=(num_rows, num_cols), rowcount=None,\n                             colptr=None, colcount=None, csr2csc=None,\n                             csc2csr=None, is_sorted=True)\n\n    def has_rowcount(self) -> bool:\n        return self._rowcount is not None\n\n    def rowcount(self) -> torch.Tensor:\n        rowcount = self._rowcount\n        if rowcount is not None:\n            return rowcount\n\n        rowptr = self.rowptr()\n        rowcount = rowptr[1:] - rowptr[:-1]\n        self._rowcount = rowcount\n        return rowcount\n\n    def has_colptr(self) -> bool:\n        return self._colptr is not None\n\n    def colptr(self) -> torch.Tensor:\n        colptr = self._colptr\n        if colptr is not None:\n            return colptr\n\n        csr2csc = self._csr2csc\n        if csr2csc is not None:\n            colptr = torch.ops.torch_sparse.ind2ptr(self._col[csr2csc],\n                                                    self._sparse_sizes[1])\n        else:\n            colptr = self._col.new_zeros(self._sparse_sizes[1] + 1)\n            torch.cumsum(self.colcount(), dim=0, out=colptr[1:])\n        self._colptr = colptr\n        return colptr\n\n    def has_colcount(self) -> bool:\n        return self._colcount is not None\n\n    def colcount(self) -> torch.Tensor:\n        colcount = self._colcount\n        if colcount is not None:\n            return colcount\n\n        colptr = self._colptr\n        if colptr is not None:\n            colcount = colptr[1:] - colptr[:-1]\n        else:\n            colcount = scatter_add(torch.ones_like(self._col), self._col,\n                                   dim_size=self._sparse_sizes[1])\n        self._colcount = colcount\n        return colcount\n\n    def has_csr2csc(self) -> bool:\n        return self._csr2csc is not None\n\n    def csr2csc(self) -> torch.Tensor:\n        csr2csc = self._csr2csc\n        if csr2csc is not None:\n            return csr2csc\n\n        idx = self._sparse_sizes[0] * self._col + self.row()\n        csr2csc = idx.argsort()\n        self._csr2csc = csr2csc\n        return csr2csc\n\n    def has_csc2csr(self) -> bool:\n        return self._csc2csr is not None\n\n    def csc2csr(self) -> torch.Tensor:\n        csc2csr = self._csc2csr\n        if csc2csr is not None:\n            return csc2csr\n\n        csc2csr = self.csr2csc().argsort()\n        self._csc2csr = csc2csr\n        return csc2csr\n\n    def is_coalesced(self) -> bool:\n        idx = self._col.new_full((self._col.numel() + 1, ), -1)\n        idx[1:] = self._sparse_sizes[1] * self.row() + self._col\n        return bool((idx[1:] > idx[:-1]).all())\n\n    def coalesce(self, reduce: str = ""add""):\n        idx = self._col.new_full((self._col.numel() + 1, ), -1)\n        idx[1:] = self._sparse_sizes[1] * self.row() + self._col\n        mask = idx[1:] > idx[:-1]\n\n        if mask.all():  # Skip if indices are already coalesced.\n            return self\n\n        row = self.row()[mask]\n        col = self._col[mask]\n\n        value = self._value\n        if value is not None:\n            ptr = mask.nonzero().flatten()\n            ptr = torch.cat([ptr, ptr.new_full((1, ), value.size(0))])\n            value = segment_csr(value, ptr, reduce=reduce)\n            value = value[0] if isinstance(value, tuple) else value\n\n        return SparseStorage(row=row, rowptr=None, col=col, value=value,\n                             sparse_sizes=self._sparse_sizes, rowcount=None,\n                             colptr=None, colcount=None, csr2csc=None,\n                             csc2csr=None, is_sorted=True)\n\n    def fill_cache_(self):\n        self.row()\n        self.rowptr()\n        self.rowcount()\n        self.colptr()\n        self.colcount()\n        self.csr2csc()\n        self.csc2csr()\n        return self\n\n    def clear_cache_(self):\n        self._rowcount = None\n        self._colptr = None\n        self._colcount = None\n        self._csr2csc = None\n        self._csc2csr = None\n        return self\n\n    def cached_keys(self) -> List[str]:\n        keys: List[str] = []\n        if self.has_rowcount():\n            keys.append(\'rowcount\')\n        if self.has_colptr():\n            keys.append(\'colptr\')\n        if self.has_colcount():\n            keys.append(\'colcount\')\n        if self.has_csr2csc():\n            keys.append(\'csr2csc\')\n        if self.has_csc2csr():\n            keys.append(\'csc2csr\')\n        return keys\n\n    def num_cached_keys(self) -> int:\n        return len(self.cached_keys())\n\n    def copy(self):\n        return SparseStorage(row=self._row, rowptr=self._rowptr, col=self._col,\n                             value=self._value,\n                             sparse_sizes=self._sparse_sizes,\n                             rowcount=self._rowcount, colptr=self._colptr,\n                             colcount=self._colcount, csr2csc=self._csr2csc,\n                             csc2csr=self._csc2csr, is_sorted=True)\n\n    def clone(self):\n        row = self._row\n        if row is not None:\n            row = row.clone()\n        rowptr = self._rowptr\n        if rowptr is not None:\n            rowptr = rowptr.clone()\n        col = self._col.clone()\n        value = self._value\n        if value is not None:\n            value = value.clone()\n        rowcount = self._rowcount\n        if rowcount is not None:\n            rowcount = rowcount.clone()\n        colptr = self._colptr\n        if colptr is not None:\n            colptr = colptr.clone()\n        colcount = self._colcount\n        if colcount is not None:\n            colcount = colcount.clone()\n        csr2csc = self._csr2csc\n        if csr2csc is not None:\n            csr2csc = csr2csc.clone()\n        csc2csr = self._csc2csr\n        if csc2csr is not None:\n            csc2csr = csc2csr.clone()\n        return SparseStorage(row=row, rowptr=rowptr, col=col, value=value,\n                             sparse_sizes=self._sparse_sizes,\n                             rowcount=rowcount, colptr=colptr,\n                             colcount=colcount, csr2csc=csr2csc,\n                             csc2csr=csc2csr, is_sorted=True)\n\n    def type_as(self, tensor=torch.Tensor):\n        value = self._value\n        if value is not None:\n            if tensor.dtype == value.dtype:\n                return self\n            else:\n                return self.set_value(value.type_as(tensor), layout=\'coo\')\n        else:\n            return self\n\n    def device_as(self, tensor: torch.Tensor, non_blocking: bool = False):\n        if tensor.device == self._col.device:\n            return self\n\n        row = self._row\n        if row is not None:\n            row = row.to(tensor.device, non_blocking=non_blocking)\n        rowptr = self._rowptr\n        if rowptr is not None:\n            rowptr = rowptr.to(tensor.device, non_blocking=non_blocking)\n        col = self._col.to(tensor.device, non_blocking=non_blocking)\n        value = self._value\n        if value is not None:\n            value = value.to(tensor.device, non_blocking=non_blocking)\n        rowcount = self._rowcount\n        if rowcount is not None:\n            rowcount = rowcount.to(tensor.device, non_blocking=non_blocking)\n        colptr = self._colptr\n        if colptr is not None:\n            colptr = colptr.to(tensor.device, non_blocking=non_blocking)\n        colcount = self._colcount\n        if colcount is not None:\n            colcount = colcount.to(tensor.device, non_blocking=non_blocking)\n        csr2csc = self._csr2csc\n        if csr2csc is not None:\n            csr2csc = csr2csc.to(tensor.device, non_blocking=non_blocking)\n        csc2csr = self._csc2csr\n        if csc2csr is not None:\n            csc2csr = csc2csr.to(tensor.device, non_blocking=non_blocking)\n        return SparseStorage(row=row, rowptr=rowptr, col=col, value=value,\n                             sparse_sizes=self._sparse_sizes,\n                             rowcount=rowcount, colptr=colptr,\n                             colcount=colcount, csr2csc=csr2csc,\n                             csc2csr=csc2csr, is_sorted=True)\n\n    def pin_memory(self):\n        row = self._row\n        if row is not None:\n            row = row.pin_memory()\n        rowptr = self._rowptr\n        if rowptr is not None:\n            rowptr = rowptr.pin_memory()\n        col = self._col.pin_memory()\n        value = self._value\n        if value is not None:\n            value = value.pin_memory()\n        rowcount = self._rowcount\n        if rowcount is not None:\n            rowcount = rowcount.pin_memory()\n        colptr = self._colptr\n        if colptr is not None:\n            colptr = colptr.pin_memory()\n        colcount = self._colcount\n        if colcount is not None:\n            colcount = colcount.pin_memory()\n        csr2csc = self._csr2csc\n        if csr2csc is not None:\n            csr2csc = csr2csc.pin_memory()\n        csc2csr = self._csc2csr\n        if csc2csr is not None:\n            csc2csr = csc2csr.pin_memory()\n        return SparseStorage(row=row, rowptr=rowptr, col=col, value=value,\n                             sparse_sizes=self._sparse_sizes,\n                             rowcount=rowcount, colptr=colptr,\n                             colcount=colcount, csr2csc=csr2csc,\n                             csc2csr=csc2csr, is_sorted=True)\n\n    def is_pinned(self) -> bool:\n        is_pinned = True\n        row = self._row\n        if row is not None:\n            is_pinned = is_pinned and row.is_pinned()\n        rowptr = self._rowptr\n        if rowptr is not None:\n            is_pinned = is_pinned and rowptr.is_pinned()\n        is_pinned = self._col.is_pinned()\n        value = self._value\n        if value is not None:\n            is_pinned = is_pinned and value.is_pinned()\n        rowcount = self._rowcount\n        if rowcount is not None:\n            is_pinned = is_pinned and rowcount.is_pinned()\n        colptr = self._colptr\n        if colptr is not None:\n            is_pinned = is_pinned and colptr.is_pinned()\n        colcount = self._colcount\n        if colcount is not None:\n            is_pinned = is_pinned and colcount.is_pinned()\n        csr2csc = self._csr2csc\n        if csr2csc is not None:\n            is_pinned = is_pinned and csr2csc.is_pinned()\n        csc2csr = self._csc2csr\n        if csc2csr is not None:\n            is_pinned = is_pinned and csc2csr.is_pinned()\n        return is_pinned\n\n\ndef share_memory_(self) -> SparseStorage:\n    row = self._row\n    if row is not None:\n        row.share_memory_()\n    rowptr = self._rowptr\n    if rowptr is not None:\n        rowptr.share_memory_()\n    self._col.share_memory_()\n    value = self._value\n    if value is not None:\n        value.share_memory_()\n    rowcount = self._rowcount\n    if rowcount is not None:\n        rowcount.share_memory_()\n    colptr = self._colptr\n    if colptr is not None:\n        colptr.share_memory_()\n    colcount = self._colcount\n    if colcount is not None:\n        colcount.share_memory_()\n    csr2csc = self._csr2csc\n    if csr2csc is not None:\n        csr2csc.share_memory_()\n    csc2csr = self._csc2csr\n    if csc2csr is not None:\n        csc2csr.share_memory_()\n\n\ndef is_shared(self) -> bool:\n    is_shared = True\n    row = self._row\n    if row is not None:\n        is_shared = is_shared and row.is_shared()\n    rowptr = self._rowptr\n    if rowptr is not None:\n        is_shared = is_shared and rowptr.is_shared()\n    is_shared = is_shared and self._col.is_shared()\n    value = self._value\n    if value is not None:\n        is_shared = is_shared and value.is_shared()\n    rowcount = self._rowcount\n    if rowcount is not None:\n        is_shared = is_shared and rowcount.is_shared()\n    colptr = self._colptr\n    if colptr is not None:\n        is_shared = is_shared and colptr.is_shared()\n    colcount = self._colcount\n    if colcount is not None:\n        is_shared = is_shared and colcount.is_shared()\n    csr2csc = self._csr2csc\n    if csr2csc is not None:\n        is_shared = is_shared and csr2csc.is_shared()\n    csc2csr = self._csc2csr\n    if csc2csr is not None:\n        is_shared = is_shared and csc2csr.is_shared()\n    return is_shared\n\n\nSparseStorage.share_memory_ = share_memory_\nSparseStorage.is_shared = is_shared\n'"
torch_sparse/tensor.py,87,"b'from textwrap import indent\nfrom typing import Optional, List, Tuple, Dict, Union, Any\n\nimport torch\nimport scipy.sparse\n\nfrom torch_sparse.storage import SparseStorage, get_layout\n\n\n@torch.jit.script\nclass SparseTensor(object):\n    storage: SparseStorage\n\n    def __init__(self, row: Optional[torch.Tensor] = None,\n                 rowptr: Optional[torch.Tensor] = None,\n                 col: Optional[torch.Tensor] = None,\n                 value: Optional[torch.Tensor] = None,\n                 sparse_sizes: Optional[Tuple[int, int]] = None,\n                 is_sorted: bool = False):\n        self.storage = SparseStorage(row=row, rowptr=rowptr, col=col,\n                                     value=value, sparse_sizes=sparse_sizes,\n                                     rowcount=None, colptr=None, colcount=None,\n                                     csr2csc=None, csc2csr=None,\n                                     is_sorted=is_sorted)\n\n    @classmethod\n    def from_storage(self, storage: SparseStorage):\n        self = SparseTensor.__new__(SparseTensor)\n        self.storage = storage\n        return self\n\n    @classmethod\n    def from_edge_index(self, edge_index: torch.Tensor,\n                        edge_attr: Optional[torch.Tensor] = None,\n                        sparse_sizes: Optional[Tuple[int, int]] = None,\n                        is_sorted: bool = False):\n        return SparseTensor(row=edge_index[0], rowptr=None, col=edge_index[1],\n                            value=edge_attr, sparse_sizes=sparse_sizes,\n                            is_sorted=is_sorted)\n\n    @classmethod\n    def from_dense(self, mat: torch.Tensor, has_value: bool = True):\n        if mat.dim() > 2:\n            index = mat.abs().sum([i for i in range(2, mat.dim())]).nonzero()\n        else:\n            index = mat.nonzero()\n        index = index.t()\n\n        row = index[0]\n        col = index[1]\n\n        value: Optional[torch.Tensor] = None\n        if has_value:\n            value = mat[row, col]\n\n        return SparseTensor(row=row, rowptr=None, col=col, value=value,\n                            sparse_sizes=(mat.size(0), mat.size(1)),\n                            is_sorted=True)\n\n    @classmethod\n    def from_torch_sparse_coo_tensor(self, mat: torch.Tensor,\n                                     has_value: bool = True):\n        mat = mat.coalesce()\n        index = mat._indices()\n        row, col = index[0], index[1]\n\n        value: Optional[torch.Tensor] = None\n        if has_value:\n            value = mat._values()\n\n        return SparseTensor(row=row, rowptr=None, col=col, value=value,\n                            sparse_sizes=(mat.size(0), mat.size(1)),\n                            is_sorted=True)\n\n    @classmethod\n    def eye(self, M: int, N: Optional[int] = None,\n            options: Optional[torch.Tensor] = None, has_value: bool = True,\n            fill_cache: bool = False):\n\n        N = M if N is None else N\n\n        if options is not None:\n            row = torch.arange(min(M, N), device=options.device)\n        else:\n            row = torch.arange(min(M, N))\n        col = row\n\n        rowptr = torch.arange(M + 1, dtype=torch.long, device=row.device)\n        if M > N:\n            rowptr[N + 1:] = N\n\n        value: Optional[torch.Tensor] = None\n        if has_value:\n            if options is not None:\n                value = torch.ones(row.numel(), dtype=options.dtype,\n                                   device=row.device)\n            else:\n                value = torch.ones(row.numel(), device=row.device)\n\n        rowcount: Optional[torch.Tensor] = None\n        colptr: Optional[torch.Tensor] = None\n        colcount: Optional[torch.Tensor] = None\n        csr2csc: Optional[torch.Tensor] = None\n        csc2csr: Optional[torch.Tensor] = None\n\n        if fill_cache:\n            rowcount = torch.ones(M, dtype=torch.long, device=row.device)\n            if M > N:\n                rowcount[N:] = 0\n\n            colptr = torch.arange(N + 1, dtype=torch.long, device=row.device)\n            colcount = torch.ones(N, dtype=torch.long, device=row.device)\n            if N > M:\n                colptr[M + 1:] = M\n                colcount[M:] = 0\n            csr2csc = csc2csr = row\n\n        storage: SparseStorage = SparseStorage(\n            row=row, rowptr=rowptr, col=col, value=value, sparse_sizes=(M, N),\n            rowcount=rowcount, colptr=colptr, colcount=colcount,\n            csr2csc=csr2csc, csc2csr=csc2csr, is_sorted=True)\n\n        self = SparseTensor.__new__(SparseTensor)\n        self.storage = storage\n        return self\n\n    def copy(self):\n        return self.from_storage(self.storage)\n\n    def clone(self):\n        return self.from_storage(self.storage.clone())\n\n    def type_as(self, tensor=torch.Tensor):\n        value = self.storage._value\n        if value is None or tensor.dtype == value.dtype:\n            return self\n        return self.from_storage(self.storage.type_as(tensor))\n\n    def device_as(self, tensor: torch.Tensor, non_blocking: bool = False):\n        if tensor.device == self.device():\n            return self\n        return self.from_storage(self.storage.device_as(tensor, non_blocking))\n\n    # Formats #################################################################\n\n    def coo(self) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n        return self.storage.row(), self.storage.col(), self.storage.value()\n\n    def csr(self) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n        return self.storage.rowptr(), self.storage.col(), self.storage.value()\n\n    def csc(self) -> Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]:\n        perm = self.storage.csr2csc()\n        value = self.storage.value()\n        if value is not None:\n            value = value[perm]\n        return self.storage.colptr(), self.storage.row()[perm], value\n\n    # Storage inheritance #####################################################\n\n    def has_value(self) -> bool:\n        return self.storage.has_value()\n\n    def set_value_(self, value: Optional[torch.Tensor],\n                   layout: Optional[str] = None):\n        self.storage.set_value_(value, layout)\n        return self\n\n    def set_value(self, value: Optional[torch.Tensor],\n                  layout: Optional[str] = None):\n        return self.from_storage(self.storage.set_value(value, layout))\n\n    def sparse_sizes(self) -> Tuple[int, int]:\n        return self.storage.sparse_sizes()\n\n    def sparse_size(self, dim: int) -> int:\n        return self.storage.sparse_sizes()[dim]\n\n    def sparse_resize(self, sparse_sizes: Tuple[int, int]):\n        return self.from_storage(self.storage.sparse_resize(sparse_sizes))\n\n    def sparse_reshape(self, num_rows: int, num_cols: int):\n        return self.from_storage(\n            self.storage.sparse_reshape(num_rows, num_cols))\n\n    def is_coalesced(self) -> bool:\n        return self.storage.is_coalesced()\n\n    def coalesce(self, reduce: str = ""sum""):\n        return self.from_storage(self.storage.coalesce(reduce))\n\n    def fill_cache_(self):\n        self.storage.fill_cache_()\n        return self\n\n    def clear_cache_(self):\n        self.storage.clear_cache_()\n        return self\n\n    # Utility functions #######################################################\n\n    def fill_value_(self, fill_value: float,\n                    options: Optional[torch.Tensor] = None):\n        if options is not None:\n            value = torch.full((self.nnz(), ), fill_value, dtype=options.dtype,\n                               device=self.device())\n        else:\n            value = torch.full((self.nnz(), ), fill_value,\n                               device=self.device())\n        return self.set_value_(value, layout=\'coo\')\n\n    def fill_value(self, fill_value: float,\n                   options: Optional[torch.Tensor] = None):\n        if options is not None:\n            value = torch.full((self.nnz(), ), fill_value, dtype=options.dtype,\n                               device=self.device())\n        else:\n            value = torch.full((self.nnz(), ), fill_value,\n                               device=self.device())\n        return self.set_value(value, layout=\'coo\')\n\n    def sizes(self) -> List[int]:\n        sparse_sizes = self.sparse_sizes()\n        value = self.storage.value()\n        if value is not None:\n            return list(sparse_sizes) + list(value.size())[1:]\n        else:\n            return list(sparse_sizes)\n\n    def size(self, dim: int) -> int:\n        return self.sizes()[dim]\n\n    def dim(self) -> int:\n        return len(self.sizes())\n\n    def nnz(self) -> int:\n        return self.storage.col().numel()\n\n    def numel(self) -> int:\n        value = self.storage.value()\n        if value is not None:\n            return value.numel()\n        else:\n            return self.nnz()\n\n    def density(self) -> float:\n        return self.nnz() / (self.sparse_size(0) * self.sparse_size(1))\n\n    def sparsity(self) -> float:\n        return 1 - self.density()\n\n    def avg_row_length(self) -> float:\n        return self.nnz() / self.sparse_size(0)\n\n    def avg_col_length(self) -> float:\n        return self.nnz() / self.sparse_size(1)\n\n    def bandwidth(self) -> int:\n        row, col, _ = self.coo()\n        return int((row - col).abs_().max())\n\n    def avg_bandwidth(self) -> float:\n        row, col, _ = self.coo()\n        return float((row - col).abs_().to(torch.float).mean())\n\n    def bandwidth_proportion(self, bandwidth: int) -> float:\n        row, col, _ = self.coo()\n        tmp = (row - col).abs_()\n        return int((tmp <= bandwidth).sum()) / self.nnz()\n\n    def is_quadratic(self) -> bool:\n        return self.sparse_size(0) == self.sparse_size(1)\n\n    def is_symmetric(self) -> bool:\n        if not self.is_quadratic():\n            return False\n\n        rowptr, col, value1 = self.csr()\n        colptr, row, value2 = self.csc()\n\n        if (rowptr != colptr).any() or (col != row).any():\n            return False\n\n        if value1 is None or value2 is None:\n            return True\n        else:\n            return bool((value1 == value2).all())\n\n    def to_symmetric(self, reduce: str = ""sum""):\n        row, col, value = self.coo()\n\n        row, col = torch.cat([row, col], dim=0), torch.cat([col, row], dim=0)\n        if value is not None:\n            value = torch.cat([value, value], dim=0)\n\n        N = max(self.size(0), self.size(1))\n\n        out = SparseTensor(row=row, rowptr=None, col=col, value=value,\n                           sparse_sizes=(N, N), is_sorted=False)\n        out = out.coalesce(reduce)\n        return out\n\n    def detach_(self):\n        value = self.storage.value()\n        if value is not None:\n            value.detach_()\n        return self\n\n    def detach(self):\n        value = self.storage.value()\n        if value is not None:\n            value = value.detach()\n        return self.set_value(value, layout=\'coo\')\n\n    def requires_grad(self) -> bool:\n        value = self.storage.value()\n        if value is not None:\n            return value.requires_grad\n        else:\n            return False\n\n    def requires_grad_(self, requires_grad: bool = True,\n                       options: Optional[torch.Tensor] = None):\n        if requires_grad and not self.has_value():\n            self.fill_value_(1., options=options)\n\n        value = self.storage.value()\n        if value is not None:\n            value.requires_grad_(requires_grad)\n        return self\n\n    def pin_memory(self):\n        return self.from_storage(self.storage.pin_memory())\n\n    def is_pinned(self) -> bool:\n        return self.storage.is_pinned()\n\n    def options(self) -> torch.Tensor:\n        value = self.storage.value()\n        if value is not None:\n            return value\n        else:\n            return torch.tensor(0., dtype=torch.float,\n                                device=self.storage.col().device)\n\n    def device(self):\n        return self.storage.col().device\n\n    def cpu(self):\n        return self.device_as(torch.tensor(0.), non_blocking=False)\n\n    def cuda(self, options: Optional[torch.Tensor] = None,\n             non_blocking: bool = False):\n        if options is not None:\n            return self.device_as(options, non_blocking)\n        else:\n            options = torch.tensor(0.).cuda()\n            return self.device_as(options, non_blocking)\n\n    def is_cuda(self) -> bool:\n        return self.storage.col().is_cuda\n\n    def dtype(self):\n        return self.options().dtype\n\n    def is_floating_point(self) -> bool:\n        return torch.is_floating_point(self.options())\n\n    def bfloat16(self):\n        return self.type_as(\n            torch.tensor(0, dtype=torch.bfloat16, device=self.device()))\n\n    def bool(self):\n        return self.type_as(\n            torch.tensor(0, dtype=torch.bool, device=self.device()))\n\n    def byte(self):\n        return self.type_as(\n            torch.tensor(0, dtype=torch.uint8, device=self.device()))\n\n    def char(self):\n        return self.type_as(\n            torch.tensor(0, dtype=torch.int8, device=self.device()))\n\n    def half(self):\n        return self.type_as(\n            torch.tensor(0, dtype=torch.half, device=self.device()))\n\n    def float(self):\n        return self.type_as(\n            torch.tensor(0, dtype=torch.float, device=self.device()))\n\n    def double(self):\n        return self.type_as(\n            torch.tensor(0, dtype=torch.double, device=self.device()))\n\n    def short(self):\n        return self.type_as(\n            torch.tensor(0, dtype=torch.short, device=self.device()))\n\n    def int(self):\n        return self.type_as(\n            torch.tensor(0, dtype=torch.int, device=self.device()))\n\n    def long(self):\n        return self.type_as(\n            torch.tensor(0, dtype=torch.long, device=self.device()))\n\n    # Conversions #############################################################\n\n    def to_dense(self, options: Optional[torch.Tensor] = None) -> torch.Tensor:\n        row, col, value = self.coo()\n\n        if value is not None:\n            mat = torch.zeros(self.sizes(), dtype=value.dtype,\n                              device=self.device())\n        elif options is not None:\n            mat = torch.zeros(self.sizes(), dtype=options.dtype,\n                              device=self.device())\n        else:\n            mat = torch.zeros(self.sizes(), device=self.device())\n\n        if value is not None:\n            mat[row, col] = value\n        else:\n            mat[row, col] = torch.ones(self.nnz(), dtype=mat.dtype,\n                                       device=mat.device)\n\n        return mat\n\n    def to_torch_sparse_coo_tensor(self,\n                                   options: Optional[torch.Tensor] = None):\n        row, col, value = self.coo()\n        index = torch.stack([row, col], dim=0)\n        if value is None:\n            if options is not None:\n                value = torch.ones(self.nnz(), dtype=options.dtype,\n                                   device=self.device())\n            else:\n                value = torch.ones(self.nnz(), device=self.device())\n\n        return torch.sparse_coo_tensor(index, value, self.sizes())\n\n\n# Python Bindings #############################################################\n\nDtype = Optional[torch.dtype]\nDevice = Optional[Union[torch.device, str]]\n\n\ndef share_memory_(self: SparseTensor) -> SparseTensor:\n    self.storage.share_memory_()\n\n\ndef is_shared(self: SparseTensor) -> bool:\n    return self.storage.is_shared()\n\n\ndef to(self, *args: Optional[List[Any]],\n       **kwargs: Optional[Dict[str, Any]]) -> SparseTensor:\n\n    device, dtype, non_blocking = torch._C._nn._parse_to(*args, **kwargs)[:3]\n\n    if dtype is not None:\n        self = self.type_as(torch.tensor(0., dtype=dtype))\n    if device is not None:\n        self = self.device_as(torch.tensor(0., device=device), non_blocking)\n\n    return self\n\n\ndef __getitem__(self: SparseTensor, index: Any) -> SparseTensor:\n    index = list(index) if isinstance(index, tuple) else [index]\n    # More than one `Ellipsis` is not allowed...\n    if len([i for i in index if not torch.is_tensor(i) and i == ...]) > 1:\n        raise SyntaxError\n\n    dim = 0\n    out = self\n    while len(index) > 0:\n        item = index.pop(0)\n        if isinstance(item, int):\n            out = out.select(dim, item)\n            dim += 1\n        elif isinstance(item, slice):\n            if item.step is not None:\n                raise ValueError(\'Step parameter not yet supported.\')\n\n            start = 0 if item.start is None else item.start\n            start = self.size(dim) + start if start < 0 else start\n\n            stop = self.size(dim) if item.stop is None else item.stop\n            stop = self.size(dim) + stop if stop < 0 else stop\n\n            out = out.narrow(dim, start, max(stop - start, 0))\n            dim += 1\n        elif torch.is_tensor(item):\n            if item.dtype == torch.bool:\n                out = out.masked_select(dim, item)\n                dim += 1\n            elif item.dtype == torch.long:\n                out = out.index_select(dim, item)\n                dim += 1\n        elif item == Ellipsis:\n            if self.dim() - len(index) < dim:\n                raise SyntaxError\n            dim = self.dim() - len(index)\n        else:\n            raise SyntaxError\n\n    return out\n\n\ndef __repr__(self: SparseTensor) -> str:\n    i = \' \' * 6\n    row, col, value = self.coo()\n    infos = []\n    infos += [f\'row={indent(row.__repr__(), i)[len(i):]}\']\n    infos += [f\'col={indent(col.__repr__(), i)[len(i):]}\']\n\n    if value is not None:\n        infos += [f\'val={indent(value.__repr__(), i)[len(i):]}\']\n\n    infos += [\n        f\'size={tuple(self.sizes())}, nnz={self.nnz()}, \'\n        f\'density={100 * self.density():.02f}%\'\n    ]\n\n    infos = \',\\n\'.join(infos)\n\n    i = \' \' * (len(self.__class__.__name__) + 1)\n    return f\'{self.__class__.__name__}({indent(infos, i)[len(i):]})\'\n\n\nSparseTensor.share_memory_ = share_memory_\nSparseTensor.is_shared = is_shared\nSparseTensor.to = to\nSparseTensor.__getitem__ = __getitem__\nSparseTensor.__repr__ = __repr__\n\n# Scipy Conversions ###########################################################\n\nScipySparseMatrix = Union[scipy.sparse.coo_matrix, scipy.sparse.\n                          csr_matrix, scipy.sparse.csc_matrix]\n\n\n@torch.jit.ignore\ndef from_scipy(mat: ScipySparseMatrix, has_value: bool = True) -> SparseTensor:\n    colptr = None\n    if isinstance(mat, scipy.sparse.csc_matrix):\n        colptr = torch.from_numpy(mat.indptr).to(torch.long)\n\n    mat = mat.tocsr()\n    rowptr = torch.from_numpy(mat.indptr).to(torch.long)\n    mat = mat.tocoo()\n    row = torch.from_numpy(mat.row).to(torch.long)\n    col = torch.from_numpy(mat.col).to(torch.long)\n    value = None\n    if has_value:\n        value = torch.from_numpy(mat.data)\n    sparse_sizes = mat.shape[:2]\n\n    storage = SparseStorage(row=row, rowptr=rowptr, col=col, value=value,\n                            sparse_sizes=sparse_sizes, rowcount=None,\n                            colptr=colptr, colcount=None, csr2csc=None,\n                            csc2csr=None, is_sorted=True)\n\n    return SparseTensor.from_storage(storage)\n\n\n@torch.jit.ignore\ndef to_scipy(self: SparseTensor, layout: Optional[str] = None,\n             dtype: Optional[torch.dtype] = None) -> ScipySparseMatrix:\n    assert self.dim() == 2\n    layout = get_layout(layout)\n\n    if not self.has_value():\n        ones = torch.ones(self.nnz(), dtype=dtype).numpy()\n\n    if layout == \'coo\':\n        row, col, value = self.coo()\n        row = row.detach().cpu().numpy()\n        col = col.detach().cpu().numpy()\n        value = value.detach().cpu().numpy() if self.has_value() else ones\n        return scipy.sparse.coo_matrix((value, (row, col)), self.sizes())\n    elif layout == \'csr\':\n        rowptr, col, value = self.csr()\n        rowptr = rowptr.detach().cpu().numpy()\n        col = col.detach().cpu().numpy()\n        value = value.detach().cpu().numpy() if self.has_value() else ones\n        return scipy.sparse.csr_matrix((value, col, rowptr), self.sizes())\n    elif layout == \'csc\':\n        colptr, row, value = self.csc()\n        colptr = colptr.detach().cpu().numpy()\n        row = row.detach().cpu().numpy()\n        value = value.detach().cpu().numpy() if self.has_value() else ones\n        return scipy.sparse.csc_matrix((value, row, colptr), self.sizes())\n\n\nSparseTensor.from_scipy = from_scipy\nSparseTensor.to_scipy = to_scipy\n'"
torch_sparse/transpose.py,1,"b'import torch\n\nfrom torch_sparse.storage import SparseStorage\nfrom torch_sparse.tensor import SparseTensor\n\n\ndef t(src: SparseTensor) -> SparseTensor:\n    csr2csc = src.storage.csr2csc()\n\n    row, col, value = src.coo()\n\n    if value is not None:\n        value = value[csr2csc]\n\n    sparse_sizes = src.storage.sparse_sizes()\n\n    storage = SparseStorage(\n        row=col[csr2csc],\n        rowptr=src.storage._colptr,\n        col=row[csr2csc],\n        value=value,\n        sparse_sizes=(sparse_sizes[1], sparse_sizes[0]),\n        rowcount=src.storage._colcount,\n        colptr=src.storage._rowptr,\n        colcount=src.storage._rowcount,\n        csr2csc=src.storage._csc2csr,\n        csc2csr=csr2csc,\n        is_sorted=True,\n    )\n\n    return src.from_storage(storage)\n\n\nSparseTensor.t = lambda self: t(self)\n\n###############################################################################\n\n\ndef transpose(index, value, m, n, coalesced=True):\n    """"""Transposes dimensions 0 and 1 of a sparse tensor.\n\n    Args:\n        index (:class:`LongTensor`): The index tensor of sparse matrix.\n        value (:class:`Tensor`): The value tensor of sparse matrix.\n        m (int): The first dimension of corresponding dense matrix.\n        n (int): The second dimension of corresponding dense matrix.\n        coalesced (bool, optional): If set to :obj:`False`, will not coalesce\n            the output. (default: :obj:`True`)\n    :rtype: (:class:`LongTensor`, :class:`Tensor`)\n    """"""\n\n    row, col = index\n    row, col = col, row\n\n    if coalesced:\n        sparse_sizes = (n, m)\n        storage = SparseStorage(row=row, col=col, value=value,\n                                sparse_sizes=sparse_sizes, is_sorted=False)\n        storage = storage.coalesce()\n        row, col, value = storage.row(), storage.col(), storage.value()\n\n    return torch.stack([row, col], dim=0), value\n'"
torch_sparse/utils.py,1,"b'from typing import Any\n\ntry:\n    from typing_extensions import Final  # noqa\nexcept ImportError:\n    from torch.jit import Final  # noqa\n\n\ndef is_scalar(other: Any) -> bool:\n    return isinstance(other, int) or isinstance(other, float)\n'"
