file_path,api_count,code
avg_checkpoints.py,6,"b'#!/usr/bin/env python\n"""""" Checkpoint Averaging Script\n\nThis script averages all model weights for checkpoints in specified path that match\nthe specified filter wildcard. All checkpoints must be from the exact same model.\n\nFor any hope of decent results, the checkpoints should be from the same or child\n(via resumes) training session. This can be viewed as similar to maintaining running\nEMA (exponential moving average) of the model weights or performing SWA (stochastic\nweight averaging), but post-training.\n\nHacked together by Ross Wightman (https://github.com/rwightman)\n""""""\nimport torch\nimport argparse\nimport os\nimport glob\nimport hashlib\nfrom timm.models.helpers import load_state_dict\n\nparser = argparse.ArgumentParser(description=\'PyTorch Checkpoint Averager\')\nparser.add_argument(\'--input\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to base input folder containing checkpoints\')\nparser.add_argument(\'--filter\', default=\'*.pth.tar\', type=str, metavar=\'WILDCARD\',\n                    help=\'checkpoint filter (path wildcard)\')\nparser.add_argument(\'--output\', default=\'./averaged.pth\', type=str, metavar=\'PATH\',\n                    help=\'output filename\')\nparser.add_argument(\'--no-use-ema\', dest=\'no_use_ema\', action=\'store_true\',\n                    help=\'Force not using ema version of weights (if present)\')\nparser.add_argument(\'--descending\', dest=\'descending\', action=\'store_true\',\n                    help=\'Set if eval metric is descending (like loss)\')\nparser.add_argument(\'--no-sort\', dest=\'no_sort\', action=\'store_true\',\n                    help=\'Do not sort and select by checkpoint metric, also makes ""n"" argument irrelevant\')\nparser.add_argument(\'-n\', type=int, default=10, metavar=\'N\',\n                    help=\'Number of checkpoints to average\')\n\n\ndef checkpoint_metric(checkpoint_path):\n    if not checkpoint_path or not os.path.isfile(checkpoint_path):\n        return {}\n    print(""=> Extracting metric from checkpoint \'{}\'"".format(checkpoint_path))\n    checkpoint = torch.load(checkpoint_path, map_location=\'cpu\')\n    metric = None\n    if \'metric\' in checkpoint:\n        metric = checkpoint[\'metric\']\n    return metric\n\n\ndef main():\n    args = parser.parse_args()\n    # by default use the EMA weights (if present)\n    args.use_ema = not args.no_use_ema\n    # by default sort by checkpoint metric (if present) and avg top n checkpoints\n    args.sort = not args.no_sort\n\n    if os.path.exists(args.output):\n        print(""Error: Output filename ({}) already exists."".format(args.output))\n        exit(1)\n\n    pattern = args.input\n    if not args.input.endswith(os.path.sep) and not args.filter.startswith(os.path.sep):\n        pattern += os.path.sep\n    pattern += args.filter\n    checkpoints = glob.glob(pattern, recursive=True)\n    if not checkpoints:\n        print(""Error: No checkpoints to average."")\n        exit(1)\n    \n    if args.sort:\n        checkpoint_metrics = []\n        for c in checkpoints:\n            metric = checkpoint_metric(c)\n            if metric is not None:\n                checkpoint_metrics.append((metric, c))\n        checkpoint_metrics = list(sorted(checkpoint_metrics, reverse=not args.descending))\n        checkpoint_metrics = checkpoint_metrics[:args.n]\n        print(""Selected checkpoints:"")\n        [print(m, c) for m, c in checkpoint_metrics]\n        avg_checkpoints = [c for m, c in checkpoint_metrics]\n    else:\n        avg_checkpoints = checkpoints\n        print(""Selected checkpoints:"")\n        [print(c) for c in checkpoints]\n\n    avg_state_dict = {}\n    avg_counts = {}\n    for c in avg_checkpoints:\n        new_state_dict = load_state_dict(c, args.use_ema)\n        if not new_state_dict:\n            print(""Error: Checkpoint ({}) doesn\'t exist"".format(args.checkpoint))\n            continue\n\n        for k, v in new_state_dict.items():\n            if k not in avg_state_dict:\n                avg_state_dict[k] = v.clone().to(dtype=torch.float64)\n                avg_counts[k] = 1\n            else:\n                avg_state_dict[k] += v.to(dtype=torch.float64)\n                avg_counts[k] += 1\n\n    for k, v in avg_state_dict.items():\n        v.div_(avg_counts[k])\n\n    # float32 overflow seems unlikely based on weights seen to date, but who knows\n    float32_info = torch.finfo(torch.float32)\n    final_state_dict = {}\n    for k, v in avg_state_dict.items():\n        v = v.clamp(float32_info.min, float32_info.max)\n        final_state_dict[k] = v.to(dtype=torch.float32)\n\n    torch.save(final_state_dict, args.output)\n    with open(args.output, \'rb\') as f:\n        sha_hash = hashlib.sha256(f.read()).hexdigest()\n    print(""=> Saved state_dict to \'{}, SHA256: {}\'"".format(args.output, sha_hash))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
setup.py,0,"b'"""""" Setup\n""""""\nfrom setuptools import setup, find_packages\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nexec(open(\'effdet/version.py\').read())\nsetup(\n    name=\'effdet\',\n    version=__version__,\n    description=\'EfficientDet for PyTorch\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    url=\'https://github.com/rwightman/efficientdet-pytorch\',\n    author=\'Ross Wightman\',\n    author_email=\'hello@rwightman.com\',\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        \'Development Status :: 3 - Alpha\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Python :: 3.8\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ],\n\n    # Note that this is a string of words separated by whitespace, not a list.\n    keywords=\'pytorch pretrained efficientdet efficientnet bifpn object detection\',\n    packages=find_packages(exclude=[\'data\']),\n    install_requires=[\'torch >= 1.4\', \'torchvision\'],\n    python_requires=\'>=3.6\',\n)\n'"
sotabench.py,2,"b'import os\nimport tqdm\nimport torch\ntry:\n    from apex import amp\n    has_amp = True\nexcept ImportError:\n    has_amp = False\nfrom sotabencheval.object_detection import COCOEvaluator\nfrom sotabencheval.utils import is_server, extract_archive\nfrom effdet import create_model\nfrom data import CocoDetection, create_loader\n\nNUM_GPU = 1\nBATCH_SIZE = (128 if has_amp else 64) * NUM_GPU\nANNO_SET = \'val2017\'\n\nif is_server():\n    DATA_ROOT = \'./.data/vision/coco\'\n    image_dir_zip = os.path.join(\'./.data/vision/coco\', f\'{ANNO_SET}.zip\')\n    extract_archive(from_path=image_dir_zip, to_path=\'./.data/vision/coco\')\nelse:\n    # local settings\n    DATA_ROOT = \'\'\n\n\ndef _bs(b=64):\n    b *= NUM_GPU\n    if has_amp:\n        b *= 2\n    return b\n\n\ndef _entry(model_name, paper_model_name, paper_arxiv_id, batch_size=BATCH_SIZE, model_desc=None):\n    return dict(\n        model_name=model_name,\n        model_description=model_desc,\n        paper_model_name=paper_model_name,\n        paper_arxiv_id=paper_arxiv_id,\n        batch_size=batch_size)\n\n# NOTE For any original PyTorch models, I\'ll remove from this list when you add to sotabench to\n# avoid overlap and confusion. Please contact me.\nmodel_list = [\n    ## Weights ported by myself from other frameworks\n    _entry(\'tf_efficientdet_d0\', \'EfficientDet-D0\', \'1911.09070\', batch_size=_bs(112),\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientdet_d1\', \'EfficientDet-D1\', \'1911.09070\', batch_size=_bs(72),\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientdet_d2\', \'EfficientDet-D2\', \'1911.09070\', batch_size=_bs(48),\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientdet_d3\', \'EfficientDet-D3\', \'1911.09070\', batch_size=_bs(32),\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientdet_d4\', \'EfficientDet-D4\', \'1911.09070\', batch_size=_bs(16),\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientdet_d5\', \'EfficientDet-D5\', \'1911.09070\', batch_size=_bs(12),\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientdet_d6\', \'EfficientDet-D6\', \'1911.09070\', batch_size=_bs(8),\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientdet_d7\', \'EfficientDet-D7\', \'1911.09070\', batch_size=_bs(4),\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n\n    ## Weights trained by myself in PyTorch\n    ## TODO\n]\n\n\ndef eval_model(model_name, paper_model_name, paper_arxiv_id, batch_size=64, model_description=\'\'):\n\n    # create model\n    bench = create_model(\n        model_name,\n        bench_task=\'predict\',\n        pretrained=True,\n    )\n    bench.eval()\n    input_size = bench.config.image_size\n\n    param_count = sum([m.numel() for m in bench.parameters()])\n    print(\'Model %s created, param count: %d\' % (model_name, param_count))\n\n    bench = bench.cuda()\n    if has_amp:\n        print(\'Using AMP mixed precision.\')\n        bench = amp.initialize(bench, opt_level=\'O1\')\n    else:\n        print(\'AMP not installed, running network in FP32.\')\n\n    annotation_path = os.path.join(DATA_ROOT, \'annotations\', f\'instances_{ANNO_SET}.json\')\n    evaluator = COCOEvaluator(\n        root=DATA_ROOT,\n        model_name=paper_model_name,\n        model_description=model_description,\n        paper_arxiv_id=paper_arxiv_id)\n\n    dataset = CocoDetection(os.path.join(DATA_ROOT, ANNO_SET), annotation_path)\n\n    loader = create_loader(\n        dataset,\n        input_size=input_size,\n        batch_size=batch_size,\n        use_prefetcher=True,\n        fill_color=\'mean\',\n        num_workers=4,\n        pin_mem=True)\n\n    iterator = tqdm.tqdm(loader, desc=""Evaluation"", mininterval=5)\n    evaluator.reset_time()\n\n    with torch.no_grad():\n        for i, (input, target) in enumerate(iterator):\n            output = bench(input, target[\'img_scale\'], target[\'img_size\'])\n            output = output.cpu()\n            sample_ids = target[\'img_id\'].cpu()\n            results = []\n            for index, sample in enumerate(output):\n                image_id = int(sample_ids[index])\n                for det in sample:\n                    score = float(det[4])\n                    if score < .001:  # stop when below this threshold, scores in descending order\n                        break\n                    coco_det = dict(\n                        image_id=image_id,\n                        bbox=det[0:4].tolist(),\n                        score=score,\n                        category_id=int(det[5]))\n                    results.append(coco_det)\n            evaluator.add(results)\n\n            if evaluator.cache_exists:\n                break\n\n    evaluator.save()\n\n\nfor m in model_list:\n    eval_model(**m)\n    torch.cuda.empty_cache()\n'"
train.py,13,"b'#!/usr/bin/env python\n"""""" EfficientDet Training Script\n\nThis script was started from an early version of the PyTorch ImageNet example\n(https://github.com/pytorch/examples/tree/master/imagenet)\n\nNVIDIA CUDA specific speedups adopted from NVIDIA Apex examples\n(https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\n\nHacked together by Ross Wightman (https://github.com/rwightman)\n""""""\nimport argparse\nimport time\nimport yaml\nfrom datetime import datetime\n\nimport torch\nimport torchvision.utils\ntry:\n    from apex import amp\n    from apex.parallel import DistributedDataParallel as DDP\n    from apex.parallel import convert_syncbn_model\n    has_apex = True\nexcept ImportError:\n    from torch.nn.parallel import DistributedDataParallel as DDP\n    has_apex = False\n\nfrom effdet import create_model, COCOEvaluator, unwrap_bench\nfrom data import create_loader, CocoDetection\n\nfrom timm.models import resume_checkpoint, load_checkpoint\nfrom timm.utils import *\nfrom timm.optim import create_optimizer\nfrom timm.scheduler import create_scheduler\n\ntorch.backends.cudnn.benchmark = True\n\n\n# The first arg parser parses out only the --config argument, this argument is used to\n# load a yaml file containing key-values that override the defaults for the main parser below\nconfig_parser = parser = argparse.ArgumentParser(description=\'Training Config\', add_help=False)\nparser.add_argument(\'-c\', \'--config\', default=\'\', type=str, metavar=\'FILE\',\n                    help=\'YAML config file specifying default arguments\')\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\n# Dataset / Model parameters\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--model\', default=\'tf_efficientdet_d1\', type=str, metavar=\'MODEL\',\n                    help=\'Name of model to train (default: ""countception""\')\nparser.add_argument(\'--redundant-bias\', action=\'store_true\', default=False,\n                    help=\'include redundant bias layers if True, need True to match official TF weights\')\nparser.add_argument(\'--pretrained\', action=\'store_true\', default=False,\n                    help=\'Start with pretrained version of specified network (if avail)\')\nparser.add_argument(\'--no-pretrained-backbone\', action=\'store_true\', default=False,\n                    help=\'Do not start with pretrained backbone weights, fully random.\')\nparser.add_argument(\'--initial-checkpoint\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'Initialize model from this checkpoint (default: none)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'Resume full model and optimizer state from checkpoint (default: none)\')\nparser.add_argument(\'--no-resume-opt\', action=\'store_true\', default=False,\n                    help=\'prevent resume of optimizer state when resuming model\')\nparser.add_argument(\'--mean\', type=float, nargs=\'+\', default=None, metavar=\'MEAN\',\n                    help=\'Override mean pixel value of dataset\')\nparser.add_argument(\'--std\', type=float, nargs=\'+\', default=None, metavar=\'STD\',\n                    help=\'Override std deviation of of dataset\')\nparser.add_argument(\'--interpolation\', default=\'\', type=str, metavar=\'NAME\',\n                    help=\'Image resize interpolation type (overrides model)\')\nparser.add_argument(\'--fill-color\', default=\'0\', type=str, metavar=\'NAME\',\n                    help=\'Image augmentation fill (background) color (""mean"" or int)\')\nparser.add_argument(\'-b\', \'--batch-size\', type=int, default=32, metavar=\'N\',\n                    help=\'input batch size for training (default: 32)\')\nparser.add_argument(\'-vb\', \'--validation-batch-size-multiplier\', type=int, default=1, metavar=\'N\',\n                    help=\'ratio of validation batch size to training batch size (default: 1)\')\nparser.add_argument(\'--drop\', type=float, default=0.0, metavar=\'PCT\',\n                    help=\'Dropout rate (default: 0.)\')\nparser.add_argument(\'--drop-connect\', type=float, default=None, metavar=\'PCT\',\n                    help=\'Drop connect rate, DEPRECATED, use drop-path (default: None)\')\nparser.add_argument(\'--drop-path\', type=float, default=None, metavar=\'PCT\',\n                    help=\'Drop path rate (default: None)\')\nparser.add_argument(\'--drop-block\', type=float, default=None, metavar=\'PCT\',\n                    help=\'Drop block rate (default: None)\')\nparser.add_argument(\'--clip-grad\', type=float, default=10.0, metavar=\'NORM\',\n                    help=\'Clip gradient norm (default: 10.0)\')\n\n# Optimizer parameters\nparser.add_argument(\'--opt\', default=\'momentum\', type=str, metavar=\'OPTIMIZER\',\n                    help=\'Optimizer (default: ""momentum""\')\nparser.add_argument(\'--opt-eps\', default=1e-3, type=float, metavar=\'EPSILON\',\n                    help=\'Optimizer Epsilon (default: 1e-8)\')\nparser.add_argument(\'--momentum\', type=float, default=0.9, metavar=\'M\',\n                    help=\'SGD momentum (default: 0.9)\')\nparser.add_argument(\'--weight-decay\', type=float, default=4e-5,\n                    help=\'weight decay (default: 0.00004)\')\n\n# Learning rate schedule parameters\nparser.add_argument(\'--sched\', default=\'cosine\', type=str, metavar=\'SCHEDULER\',\n                    help=\'LR scheduler (default: ""step""\')\nparser.add_argument(\'--lr\', type=float, default=0.01, metavar=\'LR\',\n                    help=\'learning rate (default: 0.01)\')\nparser.add_argument(\'--lr-noise\', type=float, nargs=\'+\', default=None, metavar=\'pct, pct\',\n                    help=\'learning rate noise on/off epoch percentages\')\nparser.add_argument(\'--lr-noise-pct\', type=float, default=0.67, metavar=\'PERCENT\',\n                    help=\'learning rate noise limit percent (default: 0.67)\')\nparser.add_argument(\'--lr-noise-std\', type=float, default=1.0, metavar=\'STDDEV\',\n                    help=\'learning rate noise std-dev (default: 1.0)\')\nparser.add_argument(\'--warmup-lr\', type=float, default=0.0001, metavar=\'LR\',\n                    help=\'warmup learning rate (default: 0.0001)\')\nparser.add_argument(\'--min-lr\', type=float, default=1e-5, metavar=\'LR\',\n                    help=\'lower lr bound for cyclic schedulers that hit 0 (1e-5)\')\nparser.add_argument(\'--epochs\', type=int, default=300, metavar=\'N\',\n                    help=\'number of epochs to train (default: 2)\')\nparser.add_argument(\'--start-epoch\', default=None, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--decay-epochs\', type=float, default=30, metavar=\'N\',\n                    help=\'epoch interval to decay LR\')\nparser.add_argument(\'--warmup-epochs\', type=int, default=5, metavar=\'N\',\n                    help=\'epochs to warmup LR, if scheduler supports\')\nparser.add_argument(\'--cooldown-epochs\', type=int, default=10, metavar=\'N\',\n                    help=\'epochs to cooldown LR at min_lr, after cyclic schedule ends\')\nparser.add_argument(\'--patience-epochs\', type=int, default=10, metavar=\'N\',\n                    help=\'patience epochs for Plateau LR scheduler (default: 10\')\nparser.add_argument(\'--decay-rate\', \'--dr\', type=float, default=0.1, metavar=\'RATE\',\n                    help=\'LR decay rate (default: 0.1)\')\n\n# Augmentation parameters\nparser.add_argument(\'--color-jitter\', type=float, default=0.4, metavar=\'PCT\',\n                    help=\'Color jitter factor (default: 0.4)\')\nparser.add_argument(\'--aa\', type=str, default=None, metavar=\'NAME\',\n                    help=\'Use AutoAugment policy. ""v0"" or ""original"". (default: None)\'),\nparser.add_argument(\'--reprob\', type=float, default=0., metavar=\'PCT\',\n                    help=\'Random erase prob (default: 0.)\')\nparser.add_argument(\'--remode\', type=str, default=\'const\',\n                    help=\'Random erase mode (default: ""const"")\')\nparser.add_argument(\'--recount\', type=int, default=1,\n                    help=\'Random erase count (default: 1)\')\nparser.add_argument(\'--resplit\', action=\'store_true\', default=False,\n                    help=\'Do not random erase first (clean) augmentation split\')\nparser.add_argument(\'--mixup\', type=float, default=0.0,\n                    help=\'mixup alpha, mixup enabled if > 0. (default: 0.)\')\nparser.add_argument(\'--mixup-off-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'turn off mixup after this epoch, disabled if 0 (default: 0)\')\nparser.add_argument(\'--smoothing\', type=float, default=0.1,\n                    help=\'label smoothing (default: 0.1)\')\nparser.add_argument(\'--train-interpolation\', type=str, default=\'random\',\n                    help=\'Training interpolation (random, bilinear, bicubic default: ""random"")\')\nparser.add_argument(\'--sync-bn\', action=\'store_true\',\n                    help=\'Enable NVIDIA Apex or Torch synchronized BatchNorm.\')\nparser.add_argument(\'--dist-bn\', type=str, default=\'\',\n                    help=\'Distribute BatchNorm stats between nodes after each epoch (""broadcast"", ""reduce"", or """")\')\n\n# Model Exponential Moving Average\nparser.add_argument(\'--model-ema\', action=\'store_true\', default=False,\n                    help=\'Enable tracking moving average of model weights\')\nparser.add_argument(\'--model-ema-decay\', type=float, default=0.9998,\n                    help=\'decay factor for model weights moving average (default: 0.9998)\')\n\n# Misc\nparser.add_argument(\'--seed\', type=int, default=42, metavar=\'S\',\n                    help=\'random seed (default: 42)\')\nparser.add_argument(\'--log-interval\', type=int, default=50, metavar=\'N\',\n                    help=\'how many batches to wait before logging training status\')\nparser.add_argument(\'--recovery-interval\', type=int, default=0, metavar=\'N\',\n                    help=\'how many batches to wait before writing recovery checkpoint\')\nparser.add_argument(\'-j\', \'--workers\', type=int, default=4, metavar=\'N\',\n                    help=\'how many training processes to use (default: 1)\')\nparser.add_argument(\'--save-images\', action=\'store_true\', default=False,\n                    help=\'save images of input bathes every log interval for debugging\')\nparser.add_argument(\'--amp\', action=\'store_true\', default=False,\n                    help=\'use NVIDIA amp for mixed precision training\')\nparser.add_argument(\'--pin-mem\', action=\'store_true\', default=False,\n                    help=\'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\')\nparser.add_argument(\'--no-prefetcher\', action=\'store_true\', default=False,\n                    help=\'disable fast prefetcher\')\nparser.add_argument(\'--output\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to output folder (default: none, current dir)\')\nparser.add_argument(\'--eval-metric\', default=\'map\', type=str, metavar=\'EVAL_METRIC\',\n                    help=\'Best metric (default: ""map""\')\nparser.add_argument(\'--tta\', type=int, default=0, metavar=\'N\',\n                    help=\'Test/inference time augmentation (oversampling) factor. 0=None (default: 0)\')\nparser.add_argument(""--local_rank"", default=0, type=int)\n\n\ndef _parse_args():\n    # Do we have a config file to parse?\n    args_config, remaining = config_parser.parse_known_args()\n    if args_config.config:\n        with open(args_config.config, \'r\') as f:\n            cfg = yaml.safe_load(f)\n            parser.set_defaults(**cfg)\n\n    # The main arg parser parses the rest of the args, the usual\n    # defaults will have been overridden if config file specified.\n    args = parser.parse_args(remaining)\n\n    # Cache the args as a text string to save them in the output dir later\n    args_text = yaml.safe_dump(args.__dict__, default_flow_style=False)\n    return args, args_text\n\n\ndef main():\n    setup_default_logging()\n    args, args_text = _parse_args()\n\n    args.pretrained_backbone = not args.no_pretrained_backbone\n    args.prefetcher = not args.no_prefetcher\n    args.distributed = False\n    if \'WORLD_SIZE\' in os.environ:\n        args.distributed = int(os.environ[\'WORLD_SIZE\']) > 1\n    args.device = \'cuda:0\'\n    args.world_size = 1\n    args.rank = 0  # global rank\n    if args.distributed:\n        args.device = \'cuda:%d\' % args.local_rank\n        torch.cuda.set_device(args.local_rank)\n        torch.distributed.init_process_group(backend=\'nccl\', init_method=\'env://\')\n        args.world_size = torch.distributed.get_world_size()\n        args.rank = torch.distributed.get_rank()\n    assert args.rank >= 0\n\n    if args.distributed:\n        logging.info(\'Training in distributed mode with multiple processes, 1 GPU per process. Process %d, total %d.\'\n                     % (args.rank, args.world_size))\n    else:\n        logging.info(\'Training with a single process on 1 GPU.\')\n\n    torch.manual_seed(args.seed + args.rank)\n\n    model = create_model(\n        args.model,\n        bench_task=\'train\',\n        pretrained=args.pretrained,\n        pretrained_backbone=args.pretrained_backbone,\n        redundant_bias=args.redundant_bias,\n        checkpoint_path=args.initial_checkpoint,\n    )\n    # FIXME decide which args to keep and overlay on config / pass to backbone\n    #     num_classes=args.num_classes,\n    #     drop_rate=args.drop,\n    #     drop_path_rate=args.drop_path,\n    #     drop_block_rate=args.drop_block,\n    input_size = model.config.image_size\n\n    if args.local_rank == 0:\n        logging.info(\'Model %s created, param count: %d\' %\n                     (args.model, sum([m.numel() for m in model.parameters()])))\n\n    model.cuda()\n    optimizer = create_optimizer(args, model)\n    use_amp = False\n    if has_apex and args.amp:\n        model, optimizer = amp.initialize(model, optimizer, opt_level=\'O1\')\n        use_amp = True\n    if args.local_rank == 0:\n        logging.info(\'NVIDIA APEX {}. AMP {}.\'.format(\n            \'installed\' if has_apex else \'not installed\', \'on\' if use_amp else \'off\'))\n\n    # optionally resume from a checkpoint\n    resume_state = {}\n    resume_epoch = None\n    if args.resume:\n        resume_state, resume_epoch = resume_checkpoint(unwrap_bench(model), args.resume)\n    if resume_state and not args.no_resume_opt:\n        if \'optimizer\' in resume_state:\n            if args.local_rank == 0:\n                logging.info(\'Restoring Optimizer state from checkpoint\')\n            optimizer.load_state_dict(resume_state[\'optimizer\'])\n        if use_amp and \'amp\' in resume_state and \'load_state_dict\' in amp.__dict__:\n            if args.local_rank == 0:\n                logging.info(\'Restoring NVIDIA AMP state from checkpoint\')\n            amp.load_state_dict(resume_state[\'amp\'])\n    del resume_state\n\n    model_ema = None\n    if args.model_ema:\n        # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper\n        model_ema = ModelEma(\n            model,\n            decay=args.model_ema_decay)\n            #resume=args.resume)  # FIXME bit of a mess with bench\n        if args.resume:\n            load_checkpoint(unwrap_bench(model_ema), args.resume, use_ema=True)\n\n    if args.distributed:\n        if args.sync_bn:\n            try:\n                if has_apex:\n                    model = convert_syncbn_model(model)\n                else:\n                    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n                if args.local_rank == 0:\n                    logging.info(\n                        \'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using \'\n                        \'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.\')\n            except Exception as e:\n                logging.error(\'Failed to enable Synchronized BatchNorm. Install Apex or Torch >= 1.1\')\n        if has_apex:\n            model = DDP(model, delay_allreduce=True)\n        else:\n            if args.local_rank == 0:\n                logging.info(""Using torch DistributedDataParallel. Install NVIDIA Apex for Apex DDP."")\n            model = DDP(model, device_ids=[args.local_rank])  # can use device str in Torch >= 1.1\n        # NOTE: EMA model does not need to be wrapped by DDP\n\n    lr_scheduler, num_epochs = create_scheduler(args, optimizer)\n    start_epoch = 0\n    if args.start_epoch is not None:\n        # a specified start_epoch will always override the resume epoch\n        start_epoch = args.start_epoch\n    elif resume_epoch is not None:\n        start_epoch = resume_epoch\n    if lr_scheduler is not None and start_epoch > 0:\n        lr_scheduler.step(start_epoch)\n\n    if args.local_rank == 0:\n        logging.info(\'Scheduled epochs: {}\'.format(num_epochs))\n\n    train_anno_set = \'train2017\'\n    train_annotation_path = os.path.join(args.data, \'annotations\', f\'instances_{train_anno_set}.json\')\n    train_image_dir = train_anno_set\n    dataset_train = CocoDetection(os.path.join(args.data, train_image_dir), train_annotation_path)\n\n    # FIXME cutmix/mixup worth investigating?\n    # collate_fn = None\n    # if args.prefetcher and args.mixup > 0:\n    #     collate_fn = FastCollateMixup(args.mixup, args.smoothing, args.num_classes)\n\n    loader_train = create_loader(\n        dataset_train,\n        input_size=input_size,\n        batch_size=args.batch_size,\n        is_training=True,\n        use_prefetcher=args.prefetcher,\n        #re_prob=args.reprob,  # FIXME add back various augmentations\n        #re_mode=args.remode,\n        #re_count=args.recount,\n        #re_split=args.resplit,\n        #color_jitter=args.color_jitter,\n        #auto_augment=args.aa,\n        interpolation=args.train_interpolation,\n        #mean=data_config[\'mean\'],\n        #std=data_config[\'std\'],\n        num_workers=args.workers,\n        distributed=args.distributed,\n        #collate_fn=collate_fn,\n        pin_mem=args.pin_mem,\n    )\n\n    train_anno_set = \'val2017\'\n    train_annotation_path = os.path.join(args.data, \'annotations\', f\'instances_{train_anno_set}.json\')\n    train_image_dir = train_anno_set\n    dataset_eval = CocoDetection(os.path.join(args.data, train_image_dir), train_annotation_path)\n\n    loader_eval = create_loader(\n        dataset_eval,\n        input_size=input_size,\n        batch_size=args.validation_batch_size_multiplier * args.batch_size,\n        is_training=False,\n        use_prefetcher=args.prefetcher,\n        interpolation=args.interpolation,\n        #mean=data_config[\'mean\'],\n        #std=data_config[\'std\'],\n        num_workers=args.workers,\n        #distributed=args.distributed,\n        pin_mem=args.pin_mem,\n    )\n\n    evaluator = COCOEvaluator(dataset_eval.coco, distributed=args.distributed)\n\n    eval_metric = args.eval_metric\n    best_metric = None\n    best_epoch = None\n    saver = None\n    output_dir = \'\'\n    if args.local_rank == 0:\n        output_base = args.output if args.output else \'./output\'\n        exp_name = \'-\'.join([\n            datetime.now().strftime(""%Y%m%d-%H%M%S""),\n            args.model\n        ])\n        output_dir = get_outdir(output_base, \'train\', exp_name)\n        decreasing = True if eval_metric == \'loss\' else False\n        saver = CheckpointSaver(checkpoint_dir=output_dir, decreasing=decreasing)\n        with open(os.path.join(output_dir, \'args.yaml\'), \'w\') as f:\n            f.write(args_text)\n\n    try:\n        for epoch in range(start_epoch, num_epochs):\n            if args.distributed:\n                loader_train.sampler.set_epoch(epoch)\n\n            train_metrics = train_epoch(\n                epoch, model, loader_train, optimizer, args,\n                lr_scheduler=lr_scheduler, saver=saver, output_dir=output_dir, use_amp=use_amp, model_ema=model_ema)\n\n            if args.distributed and args.dist_bn in (\'broadcast\', \'reduce\'):\n                if args.local_rank == 0:\n                    logging.info(""Distributing BatchNorm running means and vars"")\n                distribute_bn(model, args.world_size, args.dist_bn == \'reduce\')\n\n            # the overhead of evaluating with coco style datasets is fairly high, so just ema or non, not both\n            if model_ema is not None:\n                if args.distributed and args.dist_bn in (\'broadcast\', \'reduce\'):\n                    distribute_bn(model_ema, args.world_size, args.dist_bn == \'reduce\')\n\n                eval_metrics = validate(model_ema.ema, loader_eval, args, evaluator, log_suffix=\' (EMA)\')\n            else:\n                eval_metrics = validate(model, loader_eval, args, evaluator)\n\n            if lr_scheduler is not None:\n                # step LR for next epoch\n                lr_scheduler.step(epoch + 1, eval_metrics[eval_metric])\n\n            if saver is not None:\n                update_summary(\n                    epoch, train_metrics, eval_metrics, os.path.join(output_dir, \'summary.csv\'),\n                    write_header=best_metric is None)\n\n                # save proper checkpoint with eval metric\n                save_metric = eval_metrics[eval_metric]\n                best_metric, best_epoch = saver.save_checkpoint(\n                    unwrap_bench(model), optimizer, args,\n                    epoch=epoch, model_ema=unwrap_bench(model_ema), metric=save_metric, use_amp=use_amp)\n\n    except KeyboardInterrupt:\n        pass\n    if best_metric is not None:\n        logging.info(\'*** Best metric: {0} (epoch {1})\'.format(best_metric, best_epoch))\n\n\ndef train_epoch(\n        epoch, model, loader, optimizer, args,\n        lr_scheduler=None, saver=None, output_dir=\'\', use_amp=False, model_ema=None):\n\n    if args.prefetcher and args.mixup > 0 and loader.mixup_enabled:\n        if args.mixup_off_epoch and epoch >= args.mixup_off_epoch:\n            loader.mixup_enabled = False\n\n    batch_time_m = AverageMeter()\n    data_time_m = AverageMeter()\n    losses_m = AverageMeter()\n\n    model.train()\n\n    end = time.time()\n    last_idx = len(loader) - 1\n    num_updates = epoch * len(loader)\n    for batch_idx, (input, target) in enumerate(loader):\n        last_batch = batch_idx == last_idx\n        data_time_m.update(time.time() - end)\n\n        output = model(input, target)\n        loss = output[\'loss\']\n\n        if not args.distributed:\n            losses_m.update(loss.item(), input.size(0))\n\n        optimizer.zero_grad()\n        if use_amp:\n            with amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n            if args.clip_grad:\n                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.clip_grad)\n        else:\n            loss.backward()\n            if args.clip_grad:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n        optimizer.step()\n\n        torch.cuda.synchronize()\n        if model_ema is not None:\n            model_ema.update(model)\n        num_updates += 1\n\n        batch_time_m.update(time.time() - end)\n        if last_batch or batch_idx % args.log_interval == 0:\n            lrl = [param_group[\'lr\'] for param_group in optimizer.param_groups]\n            lr = sum(lrl) / len(lrl)\n\n            if args.distributed:\n                reduced_loss = reduce_tensor(loss.data, args.world_size)\n                losses_m.update(reduced_loss.item(), input.size(0))\n\n            if args.local_rank == 0:\n                logging.info(\n                    \'Train: {} [{:>4d}/{} ({:>3.0f}%)]  \'\n                    \'Loss: {loss.val:>9.6f} ({loss.avg:>6.4f})  \'\n                    \'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  \'\n                    \'({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  \'\n                    \'LR: {lr:.3e}  \'\n                    \'Data: {data_time.val:.3f} ({data_time.avg:.3f})\'.format(\n                        epoch,\n                        batch_idx, len(loader),\n                        100. * batch_idx / last_idx,\n                        loss=losses_m,\n                        batch_time=batch_time_m,\n                        rate=input.size(0) * args.world_size / batch_time_m.val,\n                        rate_avg=input.size(0) * args.world_size / batch_time_m.avg,\n                        lr=lr,\n                        data_time=data_time_m))\n\n                if args.save_images and output_dir:\n                    torchvision.utils.save_image(\n                        input,\n                        os.path.join(output_dir, \'train-batch-%d.jpg\' % batch_idx),\n                        padding=0,\n                        normalize=True)\n\n        if saver is not None and args.recovery_interval and (\n                last_batch or (batch_idx + 1) % args.recovery_interval == 0):\n            saver.save_recovery(\n                unwrap_bench(model), optimizer, args, epoch, model_ema=unwrap_bench(model_ema),\n                use_amp=use_amp, batch_idx=batch_idx)\n\n        if lr_scheduler is not None:\n            lr_scheduler.step_update(num_updates=num_updates, metric=losses_m.avg)\n\n        end = time.time()\n        # end for\n\n    if hasattr(optimizer, \'sync_lookahead\'):\n        optimizer.sync_lookahead()\n\n    return OrderedDict([(\'loss\', losses_m.avg)])\n\n\ndef validate(model, loader, args, evaluator=None, log_suffix=\'\'):\n    batch_time_m = AverageMeter()\n    losses_m = AverageMeter()\n\n    model.eval()\n\n    end = time.time()\n    last_idx = len(loader) - 1\n    with torch.no_grad():\n        for batch_idx, (input, target) in enumerate(loader):\n            last_batch = batch_idx == last_idx\n\n            output = model(input, target)\n            loss = output[\'loss\']\n\n            if evaluator is not None:\n                evaluator.add_predictions(output[\'detections\'], target)\n\n            if args.distributed:\n                reduced_loss = reduce_tensor(loss.data, args.world_size)\n            else:\n                reduced_loss = loss.data\n\n            torch.cuda.synchronize()\n\n            losses_m.update(reduced_loss.item(), input.size(0))\n\n            batch_time_m.update(time.time() - end)\n            end = time.time()\n            if args.local_rank == 0 and (last_batch or batch_idx % args.log_interval == 0):\n                log_name = \'Test\' + log_suffix\n                logging.info(\n                    \'{0}: [{1:>4d}/{2}]  \'\n                    \'Time: {batch_time.val:.3f} ({batch_time.avg:.3f})  \'\n                    \'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  \'.format(\n                        log_name, batch_idx, last_idx, batch_time=batch_time_m, loss=losses_m))\n\n    metrics = OrderedDict([(\'loss\', losses_m.avg)])\n    if evaluator is not None:\n        metrics[\'map\'] = evaluator.evaluate()\n\n    return metrics\n\n\nif __name__ == \'__main__\':\n    main()\n'"
validate.py,4,"b'#!/usr/bin/env python\n"""""" COCO validation script\n\nHacked together by Ross Wightman (https://github.com/rwightman)\n""""""\nimport argparse\nimport os\nimport json\nimport time\nimport logging\nimport torch\nimport torch.nn.parallel\ntry:\n    from apex import amp\n    has_amp = True\nexcept ImportError:\n    has_amp = False\n\nfrom effdet import create_model\nfrom data import create_loader, CocoDetection\nfrom timm.utils import AverageMeter, setup_default_logging\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\n\ntorch.backends.cudnn.benchmark = True\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Validation\')\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--anno\', default=\'val2017\',\n                    help=\'mscoco annotation set (one of val2017, train2017, test-dev2017)\')\nparser.add_argument(\'--model\', \'-m\', metavar=\'MODEL\', default=\'tf_efficientdet_d1\',\n                    help=\'model architecture (default: tf_efficientdet_d1)\')\nparser.add_argument(\'--no-redundant-bias\', action=\'store_true\', default=None,\n                    help=\'remove redundant bias layers if True, need False for official TF weights\')\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 4)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=128, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 128)\')\nparser.add_argument(\'--img-size\', default=None, type=int,\n                    metavar=\'N\', help=\'Input image dimension, uses model default if empty\')\nparser.add_argument(\'--mean\', type=float, nargs=\'+\', default=None, metavar=\'MEAN\',\n                    help=\'Override mean pixel value of dataset\')\nparser.add_argument(\'--std\', type=float,  nargs=\'+\', default=None, metavar=\'STD\',\n                    help=\'Override std deviation of of dataset\')\nparser.add_argument(\'--interpolation\', default=\'bilinear\', type=str, metavar=\'NAME\',\n                    help=\'Image resize interpolation type (overrides model)\')\nparser.add_argument(\'--fill-color\', default=\'mean\', type=str, metavar=\'NAME\',\n                    help=\'Image augmentation fill (background) color (""mean"" or int)\')\nparser.add_argument(\'--log-freq\', default=10, type=int,\n                    metavar=\'N\', help=\'batch logging frequency (default: 10)\')\nparser.add_argument(\'--checkpoint\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                    help=\'use pre-trained model\')\nparser.add_argument(\'--num-gpu\', type=int, default=1,\n                    help=\'Number of GPUS to use\')\nparser.add_argument(\'--no-prefetcher\', action=\'store_true\', default=False,\n                    help=\'disable fast prefetcher\')\nparser.add_argument(\'--pin-mem\', action=\'store_true\', default=False,\n                    help=\'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\')\nparser.add_argument(\'--use-ema\', dest=\'use_ema\', action=\'store_true\',\n                    help=\'use ema version of weights if present\')\nparser.add_argument(\'--torchscript\', dest=\'torchscript\', action=\'store_true\',\n                    help=\'convert model torchscript for inference\')\nparser.add_argument(\'--results\', default=\'./results.json\', type=str, metavar=\'FILENAME\',\n                    help=\'JSON filename for evaluation results\')\n\n\ndef validate(args):\n    setup_default_logging()\n\n    # might as well try to validate something\n    args.pretrained = args.pretrained or not args.checkpoint\n    args.prefetcher = not args.no_prefetcher\n    if args.no_redundant_bias is None:\n        args.redundant_bias = None\n    else:\n        args.redundant_bias = not args.no_redundant_bias\n\n    # create model\n    bench = create_model(\n        args.model,\n        bench_task=\'predict\',\n        pretrained=args.pretrained,\n        redundant_bias=args.redundant_bias,\n        checkpoint_path=args.checkpoint,\n        checkpoint_ema=args.use_ema,\n    )\n    input_size = bench.config.image_size\n\n    param_count = sum([m.numel() for m in bench.parameters()])\n    print(\'Model %s created, param count: %d\' % (args.model, param_count))\n\n    bench = bench.cuda()\n    if has_amp:\n        print(\'Using AMP mixed precision.\')\n        bench = amp.initialize(bench, opt_level=\'O1\')\n    else:\n        print(\'AMP not installed, running network in FP32.\')\n\n    if args.num_gpu > 1:\n        bench = torch.nn.DataParallel(bench, device_ids=list(range(args.num_gpu)))\n\n    if \'test\' in args.anno:\n        annotation_path = os.path.join(args.data, \'annotations\', f\'image_info_{args.anno}.json\')\n        image_dir = \'test2017\'\n    else:\n        annotation_path = os.path.join(args.data, \'annotations\', f\'instances_{args.anno}.json\')\n        image_dir = args.anno\n    dataset = CocoDetection(os.path.join(args.data, image_dir), annotation_path)\n\n    loader = create_loader(\n        dataset,\n        input_size=input_size,\n        batch_size=args.batch_size,\n        use_prefetcher=args.prefetcher,\n        interpolation=args.interpolation,\n        fill_color=args.fill_color,\n        num_workers=args.workers,\n        pin_mem=args.pin_mem)\n\n    img_ids = []\n    results = []\n    bench.eval()\n    batch_time = AverageMeter()\n    end = time.time()\n    with torch.no_grad():\n        for i, (input, target) in enumerate(loader):\n            output = bench(input, target[\'img_scale\'], target[\'img_size\'])\n            output = output.cpu()\n            sample_ids = target[\'img_id\'].cpu()\n            for index, sample in enumerate(output):\n                image_id = int(sample_ids[index])\n                for det in sample:\n                    score = float(det[4])\n                    if score < .001:  # stop when below this threshold, scores in descending order\n                        break\n                    coco_det = dict(\n                        image_id=image_id,\n                        bbox=det[0:4].tolist(),\n                        score=score,\n                        category_id=int(det[5]))\n                    img_ids.append(image_id)\n                    results.append(coco_det)\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % args.log_freq == 0:\n                print(\n                    \'Test: [{0:>4d}/{1}]  \'\n                    \'Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  \'\n                    .format(\n                        i, len(loader), batch_time=batch_time,\n                        rate_avg=input.size(0) / batch_time.avg,\n                     )\n                )\n\n    json.dump(results, open(args.results, \'w\'), indent=4)\n    if \'test\' not in args.anno:\n        coco_results = dataset.coco.loadRes(args.results)\n        coco_eval = COCOeval(dataset.coco, coco_results, \'bbox\')\n        coco_eval.params.imgIds = img_ids  # score only ids we\'ve used\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        coco_eval.summarize()\n\n    return results\n\n\ndef main():\n    args = parser.parse_args()\n    validate(args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
data/__init__.py,0,b'from .dataset import CocoDetection\nfrom .transforms import *\nfrom .loader import create_loader\n'
data/dataset.py,2,"b'"""""" COCO dataset (quick and dirty)\n\nHacked together by Ross Wightman\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch.utils.data as data\n\nimport os\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom pycocotools.coco import COCO\n\n\nclass CocoDetection(data.Dataset):\n    """"""`MS Coco Detection <http://mscoco.org/dataset/#detections-challenge2016>`_ Dataset.\n    Args:\n        root (string): Root directory where images are downloaded to.\n        ann_file (string): Path to json annotation file.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.ToTensor``\n\n    """"""\n\n    def __init__(self, root, ann_file, transform=None):\n        super(CocoDetection, self).__init__()\n        if isinstance(root, torch._six.string_classes):\n            root = os.path.expanduser(root)\n        self.root = root\n        self.transform = transform\n        self.yxyx = True   # expected for TF model, most PT are xyxy\n        self.include_masks = False\n        self.include_bboxes_ignore = False\n        self.has_annotations = \'image_info\' not in ann_file\n        self.coco = None\n        self.cat_ids = []\n        self.cat_to_label = dict()\n        self.img_ids = []\n        self.img_ids_invalid = []\n        self.img_infos = []\n        self._load_annotations(ann_file)\n\n    def _load_annotations(self, ann_file):\n        assert self.coco is None\n        self.coco = COCO(ann_file)\n        self.cat_ids = self.coco.getCatIds()\n        img_ids_with_ann = set(_[\'image_id\'] for _ in self.coco.anns.values())\n        for img_id in sorted(self.coco.imgs.keys()):\n            info = self.coco.loadImgs([img_id])[0]\n            valid_annotation = not self.has_annotations or img_id in img_ids_with_ann\n            if valid_annotation and min(info[\'width\'], info[\'height\']) >= 32:\n                self.img_ids.append(img_id)\n                self.img_infos.append(info)\n            else:\n                self.img_ids_invalid.append(img_id)\n\n    def _parse_img_ann(self, img_id, img_info):\n        ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n        ann_info = self.coco.loadAnns(ann_ids)\n        bboxes = []\n        bboxes_ignore = []\n        cls = []\n\n        for i, ann in enumerate(ann_info):\n            if ann.get(\'ignore\', False):\n                continue\n            x1, y1, w, h = ann[\'bbox\']\n            if self.include_masks and ann[\'area\'] <= 0:\n                continue\n            if w < 1 or h < 1:\n                continue\n\n            # To subtract 1 or not, TF doesn\'t appear to do this so will keep it out for now.\n            if self.yxyx:\n                #bbox = [y1, x1, y1 + h - 1, x1 + w - 1]\n                bbox = [y1, x1, y1 + h, x1 + w]\n            else:\n                #bbox = [x1, y1, x1 + w - 1, y1 + h - 1]\n                bbox = [x1, y1, x1 + w, y1 + h]\n\n            if ann.get(\'iscrowd\', False):\n                if self.include_bboxes_ignore:\n                    bboxes_ignore.append(bbox)\n            else:\n                bboxes.append(bbox)\n                cls.append(self.cat_to_label[ann[\'category_id\']] if self.cat_to_label else ann[\'category_id\'])\n\n        if bboxes:\n            bboxes = np.array(bboxes, dtype=np.float32)\n            cls = np.array(cls, dtype=np.int64)\n        else:\n            bboxes = np.zeros((0, 4), dtype=np.float32)\n            cls = np.array([], dtype=np.int64)\n\n        if self.include_bboxes_ignore:\n            if bboxes_ignore:\n                bboxes_ignore = np.array(bboxes_ignore, dtype=np.float32)\n            else:\n                bboxes_ignore = np.zeros((0, 4), dtype=np.float32)\n\n        ann = dict(img_id=img_id, bbox=bboxes, cls=cls, img_size=(img_info[\'width\'], img_info[\'height\']))\n\n        if self.include_bboxes_ignore:\n            ann[\'bbox_ignore\'] = bboxes_ignore\n\n        return ann\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n        Returns:\n            tuple: Tuple (image, annotations (target)).\n        """"""\n        img_id = self.img_ids[index]\n        img_info = self.img_infos[index]\n        if self.has_annotations:\n            ann = self._parse_img_ann(img_id, img_info)\n        else:\n            ann = dict(img_id=img_id, img_size=(img_info[\'width\'], img_info[\'height\']))\n\n        path = img_info[\'file_name\']\n        img = Image.open(os.path.join(self.root, path)).convert(\'RGB\')\n        if self.transform is not None:\n            img, ann = self.transform(img, ann)\n\n        return img, ann\n\n    def __len__(self):\n        return len(self.img_ids)\n'"
data/loader.py,17,"b'"""""" Object detection loader/collate\n\nHacked together by Ross Wightman\n""""""\nimport torch.utils.data\nfrom .transforms import *\nfrom timm.data.distributed_sampler import OrderedDistributedSampler\n\n\nMAX_NUM_INSTANCES = 100\n\n\ndef fast_collate(batch):\n    batch_size = len(batch)\n\n    # FIXME this needs to be more robust\n    target = dict()\n    for k, v in batch[0][1].items():\n        if isinstance(v, np.ndarray):\n            # if a numpy array, assume it relates to object instances, pad to MAX_NUM_INSTANCES\n            target_shape = (batch_size, MAX_NUM_INSTANCES)\n            if len(v.shape) > 1:\n                target_shape = target_shape + v.shape[1:]\n            target_dtype = torch.float32\n        elif isinstance(v, (tuple, list)):\n            # if tuple or list, assume per batch\n            target_shape = (batch_size, len(v))\n            target_dtype = torch.float32 if isinstance(v[0], float) else torch.int32\n        else:\n            # scalar, assume per batch\n            target_shape = batch_size,\n            target_dtype = torch.float32 if isinstance(v, float) else torch.int64\n        target[k] = torch.zeros(target_shape, dtype=target_dtype)\n\n    tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n    for i in range(batch_size):\n        tensor[i] += torch.from_numpy(batch[i][0])\n        for tk, tv in batch[i][1].items():\n            if isinstance(tv, np.ndarray) and len(tv.shape):\n                target[tk][i, 0:tv.shape[0]] = torch.from_numpy(tv)\n            else:\n                target[tk][i] = torch.tensor(tv, dtype=target[tk].dtype)\n\n    return tensor, target\n\n\nclass PrefetchLoader:\n\n    def __init__(self,\n            loader,\n            mean=IMAGENET_DEFAULT_MEAN,\n            std=IMAGENET_DEFAULT_STD):\n        self.loader = loader\n        self.mean = torch.tensor([x * 255 for x in mean]).cuda().view(1, 3, 1, 1)\n        self.std = torch.tensor([x * 255 for x in std]).cuda().view(1, 3, 1, 1)\n\n    def __iter__(self):\n        stream = torch.cuda.Stream()\n        first = True\n\n        for next_input, next_target in self.loader:\n            with torch.cuda.stream(stream):\n                next_input = next_input.cuda(non_blocking=True)\n                next_input = next_input.float().sub_(self.mean).div_(self.std)\n                next_target = {k: v.cuda(non_blocking=True) for k, v in next_target.items()}\n\n            if not first:\n                yield input, target\n            else:\n                first = False\n\n            torch.cuda.current_stream().wait_stream(stream)\n            input = next_input\n            target = next_target\n\n        yield input, target\n\n    def __len__(self):\n        return len(self.loader)\n\n    @property\n    def sampler(self):\n        return self.loader.sampler\n\n\ndef create_loader(\n        dataset,\n        input_size,\n        batch_size,\n        is_training=False,\n        use_prefetcher=True,\n        interpolation=\'bilinear\',\n        fill_color=\'mean\',\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD,\n        num_workers=1,\n        distributed=False,\n        pin_mem=False,\n):\n    if isinstance(input_size, tuple):\n        img_size = input_size[-2:]\n    else:\n        img_size = input_size\n\n    if is_training:\n        transform = transforms_coco_train(\n            img_size,\n            interpolation=interpolation,\n            use_prefetcher=use_prefetcher,\n            fill_color=fill_color,\n            mean=mean,\n            std=std)\n    else:\n        transform = transforms_coco_eval(\n            img_size,\n            interpolation=interpolation,\n            use_prefetcher=use_prefetcher,\n            fill_color=fill_color,\n            mean=mean,\n            std=std)\n\n    dataset.transform = transform\n\n    sampler = None\n    if distributed:\n        if is_training:\n            sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n        else:\n            # This will add extra duplicate entries to result in equal num\n            # of samples per-process, will slightly alter validation results\n            sampler = OrderedDistributedSampler(dataset)\n\n    loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        sampler=sampler,\n        pin_memory=pin_mem,\n        collate_fn=fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate,\n    )\n    if use_prefetcher:\n        loader = PrefetchLoader(loader, mean=mean, std=std)\n\n    return loader\n'"
data/transforms.py,2,"b'"""""" COCO transforms (quick and dirty)\n\nHacked together by Ross Wightman\n""""""\nimport torch\nfrom PIL import Image\nimport numpy as np\nimport random\nimport math\n\nIMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\nIMAGENET_INCEPTION_MEAN = (0.5, 0.5, 0.5)\nIMAGENET_INCEPTION_STD = (0.5, 0.5, 0.5)\n\n\nclass ImageToNumpy:\n\n    def __call__(self, pil_img, annotations: dict):\n        np_img = np.array(pil_img, dtype=np.uint8)\n        if np_img.ndim < 3:\n            np_img = np.expand_dims(np_img, axis=-1)\n        np_img = np.moveaxis(np_img, 2, 0)  # HWC to CHW\n        return np_img, annotations\n\n\nclass ImageToTensor:\n\n    def __init__(self, dtype=torch.float32):\n        self.dtype = dtype\n\n    def __call__(self, pil_img, annotations: dict):\n        np_img = np.array(pil_img, dtype=np.uint8)\n        if np_img.ndim < 3:\n            np_img = np.expand_dims(np_img, axis=-1)\n        np_img = np.moveaxis(np_img, 2, 0)  # HWC to CHW\n        return torch.from_numpy(np_img).to(dtype=self.dtype), annotations\n\n\ndef _pil_interp(method):\n    if method == \'bicubic\':\n        return Image.BICUBIC\n    elif method == \'lanczos\':\n        return Image.LANCZOS\n    elif method == \'hamming\':\n        return Image.HAMMING\n    else:\n        # default bilinear, do we want to allow nearest?\n        return Image.BILINEAR\n\n\ndef clip_boxes_(boxes, img_size):\n    height, width = img_size\n    clip_upper = np.array([height, width] * 2, dtype=boxes.dtype)\n    np.clip(boxes, 0, clip_upper, out=boxes)\n\n\ndef clip_boxes(boxes, img_size):\n    clipped_boxes = boxes.copy()\n    clip_boxes_(clipped_boxes, img_size)\n    return clipped_boxes\n\n\ndef _size_tuple(size):\n    if isinstance(size, int):\n        return size, size\n    else:\n        assert len(size) == 2\n        return size\n\n\nclass ResizePad:\n\n    def __init__(self, target_size: int, interpolation: str = \'bilinear\', fill_color: tuple = (0, 0, 0)):\n        self.target_size = _size_tuple(target_size)\n        self.interpolation = interpolation\n        self.fill_color = fill_color\n\n    def __call__(self, img, anno: dict):\n        width, height = img.size\n\n        img_scale_y = self.target_size[0] / height\n        img_scale_x = self.target_size[1] / width\n        img_scale = min(img_scale_y, img_scale_x)\n        scaled_h = int(height * img_scale)\n        scaled_w = int(width * img_scale)\n\n        new_img = Image.new(""RGB"", (self.target_size[1], self.target_size[0]), color=self.fill_color)\n        interp_method = _pil_interp(self.interpolation)\n        img = img.resize((scaled_w, scaled_h), interp_method)\n        new_img.paste(img)\n\n        if \'bbox\' in anno:\n            # FIXME haven\'t tested this path since not currently using dataset annotations for train/eval\n            bbox = anno[\'bbox\']\n            bbox[:, :4] *= img_scale\n            clip_boxes_(bbox, (scaled_h, scaled_w))\n            valid_indices = (bbox[:, :2] < bbox[:, 2:4]).all(axis=1)\n            anno[\'bbox\'] = bbox[valid_indices, :]\n            anno[\'cls\'] = anno[\'cls\'][valid_indices]\n\n        anno[\'img_scale\'] = 1. / img_scale  # back to original\n\n        return new_img, anno\n\n\nclass RandomResizePad:\n\n    def __init__(self, target_size: int, scale: tuple = (0.1, 2.0), interpolation: str = \'bilinear\',\n                 fill_color: tuple = (0, 0, 0)):\n        self.target_size = _size_tuple(target_size)\n        self.scale = scale\n        self.interpolation = interpolation\n        self.fill_color = fill_color\n\n    def _get_params(self, img):\n        # Select a random scale factor.\n        scale_factor = random.uniform(*self.scale)\n        scaled_target_height = scale_factor * self.target_size[0]\n        scaled_target_width = scale_factor * self.target_size[1]\n\n        # Recompute the accurate scale_factor using rounded scaled image size.\n        width, height = img.size\n        img_scale_y = scaled_target_height / height\n        img_scale_x = scaled_target_width / width\n        img_scale = min(img_scale_y, img_scale_x)\n\n        # Select non-zero random offset (x, y) if scaled image is larger than target size\n        scaled_h = int(height * img_scale)\n        scaled_w = int(width * img_scale)\n        offset_y = scaled_h - self.target_size[0]\n        offset_x = scaled_w - self.target_size[1]\n        offset_y = int(max(0.0, float(offset_y)) * random.uniform(0, 1))\n        offset_x = int(max(0.0, float(offset_x)) * random.uniform(0, 1))\n        return scaled_h, scaled_w, offset_y, offset_x, img_scale\n\n    def __call__(self, img, anno: dict):\n        scaled_h, scaled_w, offset_y, offset_x, img_scale = self._get_params(img)\n\n        interp_method = _pil_interp(self.interpolation)\n        img = img.resize((scaled_w, scaled_h), interp_method)\n        right, lower = min(scaled_w, offset_x + self.target_size[1]), min(scaled_h, offset_y + self.target_size[0])\n        img = img.crop((offset_x, offset_y, right, lower))\n        new_img = Image.new(""RGB"", (self.target_size[1], self.target_size[0]), color=self.fill_color)\n        new_img.paste(img)\n\n        if \'bbox\' in anno:\n            # FIXME not fully tested\n            bbox = anno[\'bbox\'].copy()  # FIXME copy for debugger inspection, back to inplace\n            bbox[:, :4] *= img_scale\n            box_offset = np.stack([offset_y, offset_x] * 2)\n            bbox -= box_offset\n            clip_boxes_(bbox, (scaled_h, scaled_w))\n            valid_indices = (bbox[:, :2] < bbox[:, 2:4]).all(axis=1)\n            anno[\'bbox\'] = bbox[valid_indices, :]\n            anno[\'cls\'] = anno[\'cls\'][valid_indices]\n\n        anno[\'img_scale\'] = 1. / img_scale  # back to original\n\n        return new_img, anno\n\n\nclass RandomFlip:\n\n    def __init__(self, horizontal=True, vertical=False, prob=0.5):\n        self.horizontal = horizontal\n        self.vertical = vertical\n        self.prob = prob\n\n    def _get_params(self):\n        do_horizontal = random.random() < self.prob if self.horizontal else False\n        do_vertical = random.random() < self.prob if self.vertical else False\n        return do_horizontal, do_vertical\n\n    def __call__(self, img, annotations: dict):\n        do_horizontal, do_vertical = self._get_params()\n        width, height = img.size\n\n        def _fliph(bbox):\n            x_max = width - bbox[:, 1]\n            x_min = width - bbox[:, 3]\n            bbox[:, 1] = x_min\n            bbox[:, 3] = x_max\n\n        def _flipv(bbox):\n            y_max = height - bbox[:, 0]\n            y_min = height - bbox[:, 2]\n            bbox[:, 0] = y_min\n            bbox[:, 2] = y_max\n\n        if do_horizontal and do_vertical:\n            img = img.transpose(Image.ROTATE_180)\n            if \'bbox\' in annotations:\n                _fliph(annotations[\'bbox\'])\n                _flipv(annotations[\'bbox\'])\n        elif do_horizontal:\n            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n            if \'bbox\' in annotations:\n                _fliph(annotations[\'bbox\'])\n        elif do_vertical:\n            img = img.transpose(Image.FLIP_TOP_BOTTOM)\n            if \'bbox\' in annotations:\n                _flipv(annotations[\'bbox\'])\n\n        return img, annotations\n\n\ndef resolve_fill_color(fill_color, img_mean=IMAGENET_DEFAULT_MEAN):\n    if isinstance(fill_color, tuple):\n        assert len(fill_color) == 3\n        fill_color = fill_color\n    else:\n        try:\n            int_color = int(fill_color)\n            fill_color = (int_color,) * 3\n        except ValueError:\n            assert fill_color == \'mean\'\n            fill_color = tuple([int(round(255 * x)) for x in img_mean])\n    return fill_color\n\n\nclass Compose:\n\n    def __init__(self, transforms: list):\n        self.transforms = transforms\n\n    def __call__(self, img, annotations: dict):\n        for t in self.transforms:\n            img, annotations = t(img, annotations)\n        return img, annotations\n\n\ndef transforms_coco_eval(\n        img_size=224,\n        interpolation=\'bilinear\',\n        use_prefetcher=False,\n        fill_color=\'mean\',\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD):\n\n    fill_color = resolve_fill_color(fill_color, mean)\n\n    image_tfl = [\n        ResizePad(\n            target_size=img_size, interpolation=interpolation, fill_color=fill_color),\n        ImageToNumpy(),\n    ]\n\n    assert use_prefetcher, ""Only supporting prefetcher usage right now""\n\n    image_tf = Compose(image_tfl)\n    return image_tf\n\n\ndef transforms_coco_train(\n        img_size=224,\n        interpolation=\'random\',\n        use_prefetcher=False,\n        fill_color=\'mean\',\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD):\n\n    fill_color = resolve_fill_color(fill_color, mean)\n\n    image_tfl = [\n        RandomFlip(horizontal=True, prob=0.5),\n        RandomResizePad(\n            target_size=img_size, interpolation=interpolation, fill_color=fill_color),\n        ImageToNumpy(),\n    ]\n\n    assert use_prefetcher, ""Only supporting prefetcher usage right now""\n\n    image_tf = Compose(image_tfl)\n    return image_tf\n'"
effdet/__init__.py,0,"b'from .efficientdet import EfficientDet\nfrom .bench import DetBenchPredict, DetBenchTrain, unwrap_bench\nfrom .evaluator import COCOEvaluator, FastMapEvalluator\nfrom .config import get_efficientdet_config, default_detection_model_configs\nfrom .factory import create_model, create_model_from_config\nfrom .helpers import load_checkpoint, load_pretrained\n'"
effdet/anchors.py,21,"b'"""""" RetinaNet / EfficientDet Anchor Gen\n\nAdapted for PyTorch from Tensorflow impl at\n    https://github.com/google/automl/blob/6f6694cec1a48cdb33d5d1551a2d5db8ad227798/efficientdet/anchors.py\n\nHacked together by Ross Wightman, original copyright below\n""""""\n# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Anchor definition.\n\nThis module is borrowed from TPU RetinaNet implementation:\nhttps://github.com/tensorflow/tpu/blob/master/models/official/retinanet/anchors.py\n""""""\nimport collections\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torchvision.ops.boxes import batched_nms, remove_small_boxes\n\nfrom effdet.object_detection import ArgMaxMatcher, FasterRcnnBoxCoder, BoxList, IouSimilarity, TargetAssigner\n\n# The minimum score to consider a logit for identifying detections.\nMIN_CLASS_SCORE = -5.0\n\n# The score for a dummy detection\n_DUMMY_DETECTION_SCORE = -1e5\n\n# The maximum number of (anchor,class) pairs to keep for non-max suppression.\nMAX_DETECTION_POINTS = 5000\n\n# The maximum number of detections per image.\nMAX_DETECTIONS_PER_IMAGE = 100\n\n\ndef decode_box_outputs(rel_codes, anchors, output_xyxy: bool=False):\n    """"""Transforms relative regression coordinates to absolute positions.\n\n    Network predictions are normalized and relative to a given anchor; this\n    reverses the transformation and outputs absolute coordinates for the input image.\n\n    Args:\n        rel_codes: box regression targets.\n\n        anchors: anchors on all feature levels.\n\n    Returns:\n        outputs: bounding boxes.\n\n    """"""\n    ycenter_a = (anchors[:, 0] + anchors[:, 2]) / 2\n    xcenter_a = (anchors[:, 1] + anchors[:, 3]) / 2\n    ha = anchors[:, 2] - anchors[:, 0]\n    wa = anchors[:, 3] - anchors[:, 1]\n\n    ty, tx, th, tw = rel_codes.unbind(dim=1)\n\n    w = torch.exp(tw) * wa\n    h = torch.exp(th) * ha\n    ycenter = ty * ha + ycenter_a\n    xcenter = tx * wa + xcenter_a\n    ymin = ycenter - h / 2.\n    xmin = xcenter - w / 2.\n    ymax = ycenter + h / 2.\n    xmax = xcenter + w / 2.\n    if output_xyxy:\n        out = torch.stack([xmin, ymin, xmax, ymax], dim=1)\n    else:\n        out = torch.stack([ymin, xmin, ymax, xmax], dim=1)\n    return out\n\n\ndef _generate_anchor_configs(min_level, max_level, num_scales, aspect_ratios):\n    """"""Generates mapping from output level to a list of anchor configurations.\n\n    A configuration is a tuple of (num_anchors, scale, aspect_ratio).\n\n    Args:\n        min_level: integer number of minimum level of the output feature pyramid.\n\n        max_level: integer number of maximum level of the output feature pyramid.\n\n        num_scales: integer number representing intermediate scales added on each level.\n            For instances, num_scales=2 adds two additional anchor scales [2^0, 2^0.5] on each level.\n\n        aspect_ratios: list of tuples representing the aspect ratio anchors added on each level.\n            For instances, aspect_ratios = [(1, 1), (1.4, 0.7), (0.7, 1.4)] adds three anchors on each level.\n\n    Returns:\n        anchor_configs: a dictionary with keys as the levels of anchors and\n            values as a list of anchor configuration.\n    """"""\n    anchor_configs = {}\n    for level in range(min_level, max_level + 1):\n        anchor_configs[level] = []\n        for scale_octave in range(num_scales):\n            for aspect in aspect_ratios:\n                anchor_configs[level].append((2 ** level, scale_octave / float(num_scales), aspect))\n    return anchor_configs\n\n\ndef _generate_anchor_boxes(image_size, anchor_scale, anchor_configs):\n    """"""Generates multiscale anchor boxes.\n\n    Args:\n        image_size: integer number of input image size. The input image has the same dimension for\n            width and height. The image_size should be divided by the largest feature stride 2^max_level.\n\n        anchor_scale: float number representing the scale of size of the base\n            anchor to the feature stride 2^level.\n\n        anchor_configs: a dictionary with keys as the levels of anchors and\n            values as a list of anchor configuration.\n\n    Returns:\n        anchor_boxes: a numpy array with shape [N, 4], which stacks anchors on all feature levels.\n\n    Raises:\n        ValueError: input size must be the multiple of largest feature stride.\n    """"""\n    boxes_all = []\n    for _, configs in anchor_configs.items():\n        boxes_level = []\n        for config in configs:\n            stride, octave_scale, aspect = config\n            if image_size % stride != 0:\n                raise ValueError(""input size must be divided by the stride."")\n            base_anchor_size = anchor_scale * stride * 2 ** octave_scale\n            anchor_size_x_2 = base_anchor_size * aspect[0] / 2.0\n            anchor_size_y_2 = base_anchor_size * aspect[1] / 2.0\n\n            x = np.arange(stride / 2, image_size, stride)\n            y = np.arange(stride / 2, image_size, stride)\n            xv, yv = np.meshgrid(x, y)\n            xv = xv.reshape(-1)\n            yv = yv.reshape(-1)\n\n            boxes = np.vstack((yv - anchor_size_y_2, xv - anchor_size_x_2,\n                               yv + anchor_size_y_2, xv + anchor_size_x_2))\n            boxes = np.swapaxes(boxes, 0, 1)\n            boxes_level.append(np.expand_dims(boxes, axis=1))\n        # concat anchors on the same level to the reshape NxAx4\n        boxes_level = np.concatenate(boxes_level, axis=1)\n        boxes_all.append(boxes_level.reshape([-1, 4]))\n\n    anchor_boxes = np.vstack(boxes_all)\n    return anchor_boxes\n\n\ndef clip_boxes_xyxy(boxes: torch.Tensor, size: torch.Tensor):\n    boxes = boxes.clamp(min=0)\n    size = torch.cat([size, size], dim=0)\n    boxes = boxes.min(size)\n    return boxes\n\n\ndef generate_detections(\n        cls_outputs, box_outputs, anchor_boxes, indices, classes, img_scale, img_size,\n        max_det_per_image: int = MAX_DETECTIONS_PER_IMAGE):\n    """"""Generates detections with RetinaNet model outputs and anchors.\n\n    Args:\n        cls_outputs: a torch tensor with shape [N, 1], which has the highest class\n            scores on all feature levels. The N is the number of selected\n            top-K total anchors on all levels.  (k being MAX_DETECTION_POINTS)\n\n        box_outputs: a torch tensor with shape [N, 4], which stacks box regression\n            outputs on all feature levels. The N is the number of selected top-k\n            total anchors on all levels. (k being MAX_DETECTION_POINTS)\n\n        anchor_boxes: a torch tensor with shape [N, 4], which stacks anchors on all\n            feature levels. The N is the number of selected top-k total anchors on all levels.\n\n        indices: a torch tensor with shape [N], which is the indices from top-k selection.\n\n        classes: a torch tensor with shape [N], which represents the class\n            prediction on all selected anchors from top-k selection.\n\n        img_scale: a float tensor representing the scale between original image\n            and input image for the detector. It is used to rescale detections for\n            evaluating with the original groundtruth annotations.\n\n        max_det_per_image: an int constant, added as argument to make torchscript happy\n\n    Returns:\n        detections: detection results in a tensor with shape [MAX_DETECTION_POINTS, 6],\n            each row representing [x, y, width, height, score, class]\n    """"""\n    anchor_boxes = anchor_boxes[indices, :]\n\n    # apply bounding box regression to anchors\n    boxes = decode_box_outputs(box_outputs.float(), anchor_boxes, output_xyxy=True)\n    boxes = clip_boxes_xyxy(boxes, img_size / img_scale)  # clip before NMS better?\n\n    scores = cls_outputs.sigmoid().squeeze(1).float()\n    top_detection_idx = batched_nms(boxes, scores, classes, iou_threshold=0.5)\n\n    # keep only topk scoring predictions\n    top_detection_idx = top_detection_idx[:max_det_per_image]\n    boxes = boxes[top_detection_idx]\n    scores = scores[top_detection_idx, None]\n    classes = classes[top_detection_idx, None]\n\n    # xyxy to xywh & rescale to original image\n    boxes[:, 2] -= boxes[:, 0]\n    boxes[:, 3] -= boxes[:, 1]\n    boxes *= img_scale\n\n    classes += 1  # back to class idx with background class = 0\n\n    # stack em and pad out to MAX_DETECTIONS_PER_IMAGE if necessary\n    detections = torch.cat([boxes, scores, classes.float()], dim=1)\n    if len(top_detection_idx) < max_det_per_image:\n        detections = torch.cat([\n            detections,\n            torch.zeros(\n                (max_det_per_image - len(top_detection_idx), 6), device=detections.device, dtype=detections.dtype)\n        ], dim=0)\n    return detections\n\n\nclass Anchors(nn.Module):\n    """"""RetinaNet Anchors class.""""""\n\n    def __init__(self, min_level, max_level, num_scales, aspect_ratios, anchor_scale, image_size):\n        """"""Constructs multiscale RetinaNet anchors.\n\n        Args:\n            min_level: integer number of minimum level of the output feature pyramid.\n\n            max_level: integer number of maximum level of the output feature pyramid.\n\n            num_scales: integer number representing intermediate scales added\n                on each level. For instances, num_scales=2 adds two additional\n                anchor scales [2^0, 2^0.5] on each level.\n\n            aspect_ratios: list of tuples representing the aspect ratio anchors added\n                on each level. For instances, aspect_ratios =\n                [(1, 1), (1.4, 0.7), (0.7, 1.4)] adds three anchors on each level.\n\n            anchor_scale: float number representing the scale of size of the base\n                anchor to the feature stride 2^level.\n\n            image_size: integer number of input image size. The input image has the\n                same dimension for width and height. The image_size should be divided by\n                the largest feature stride 2^max_level.\n        """"""\n        super(Anchors, self).__init__()\n        self.min_level = min_level\n        self.max_level = max_level\n        self.num_scales = num_scales\n        self.aspect_ratios = aspect_ratios\n        self.anchor_scale = anchor_scale\n        self.image_size = image_size\n        self.config = self._generate_configs()\n        self.register_buffer(\'boxes\', self._generate_boxes())\n\n    def _generate_configs(self):\n        """"""Generate configurations of anchor boxes.""""""\n        return _generate_anchor_configs(self.min_level, self.max_level, self.num_scales, self.aspect_ratios)\n\n    def _generate_boxes(self):\n        """"""Generates multiscale anchor boxes.""""""\n        boxes = _generate_anchor_boxes(self.image_size, self.anchor_scale, self.config)\n        boxes = torch.from_numpy(boxes).float()\n        return boxes\n\n    def get_anchors_per_location(self):\n        return self.num_scales * len(self.aspect_ratios)\n\n\n#@torch.jit.script\nclass AnchorLabeler(object):\n    """"""Labeler for multiscale anchor boxes.\n    """"""\n\n    def __init__(self, anchors, num_classes: int, match_threshold: float = 0.5):\n        """"""Constructs anchor labeler to assign labels to anchors.\n\n        Args:\n            anchors: an instance of class Anchors.\n\n            num_classes: integer number representing number of classes in the dataset.\n\n            match_threshold: float number between 0 and 1 representing the threshold\n                to assign positive labels for anchors.\n        """"""\n        similarity_calc = IouSimilarity()\n        matcher = ArgMaxMatcher(\n            match_threshold,\n            unmatched_threshold=match_threshold,\n            negatives_lower_than_unmatched=True,\n            force_match_for_each_row=True)\n        box_coder = FasterRcnnBoxCoder()\n\n        self.target_assigner = TargetAssigner(similarity_calc, matcher, box_coder)\n        self.anchors = anchors\n        self.match_threshold = match_threshold\n        self.num_classes = num_classes\n        self.feat_size = {}\n        for level in range(self.anchors.min_level, self.anchors.max_level + 1):\n            self.feat_size[level] = int(self.anchors.image_size / 2 ** level)\n        self.indices_cache = {}\n\n    def label_anchors(self, gt_boxes, gt_labels):\n        """"""Labels anchors with ground truth inputs.\n\n        Args:\n            gt_boxes: A float tensor with shape [N, 4] representing groundtruth boxes.\n                For each row, it stores [y0, x0, y1, x1] for four corners of a box.\n\n            gt_labels: A integer tensor with shape [N, 1] representing groundtruth classes.\n\n        Returns:\n            cls_targets_dict: ordered dictionary with keys [min_level, min_level+1, ..., max_level].\n                The values are tensor with shape [height_l, width_l, num_anchors]. The height_l and width_l\n                represent the dimension of class logits at l-th level.\n\n            box_targets_dict: ordered dictionary with keys [min_level, min_level+1, ..., max_level].\n                The values are tensor with shape [height_l, width_l, num_anchors * 4]. The height_l and\n                width_l represent the dimension of bounding box regression output at l-th level.\n\n            num_positives: scalar tensor storing number of positives in an image.\n        """"""\n        cls_targets_out = []\n        box_targets_out = []\n\n        gt_box_list = BoxList(gt_boxes)\n        anchor_box_list = BoxList(self.anchors.boxes)\n\n        # cls_weights, box_weights are not used\n        cls_targets, _, box_targets, _, matches = self.target_assigner.assign(anchor_box_list, gt_box_list, gt_labels)\n\n        # class labels start from 1 and the background class = -1\n        cls_targets -= 1\n        cls_targets = cls_targets.long()\n\n        # Unpack labels.\n        """"""Unpacks an array of cls/box into multiple scales.""""""\n        count = 0\n        for level in range(self.anchors.min_level, self.anchors.max_level + 1):\n            feat_size = self.feat_size[level]\n            steps = feat_size ** 2 * self.anchors.get_anchors_per_location()\n            indices = torch.arange(count, count + steps, device=cls_targets.device)\n            count += steps\n            cls_targets_out.append(\n                torch.index_select(cls_targets, 0, indices).view([feat_size, feat_size, -1]))\n            box_targets_out.append(\n                torch.index_select(box_targets, 0, indices).view([feat_size, feat_size, -1]))\n\n        num_positives = (matches.match_results != -1).float().sum()\n\n        return cls_targets_out, box_targets_out, num_positives\n\n    def _build_indices(self, device):\n        anchors_per_loc = self.anchors.get_anchors_per_location()\n        indices_dict = {}\n        count = 0\n        for level in range(self.anchors.min_level, self.anchors.max_level + 1):\n            feat_size = self.feat_size[level]\n            steps = feat_size ** 2 * anchors_per_loc\n            indices = torch.arange(count, count + steps, device=device)\n            indices_dict[level] = indices\n            count += steps\n        return indices_dict\n\n    def _get_indices(self, device, level):\n        if device not in self.indices_cache:\n            self.indices_cache[device] = self._build_indices(device)\n        return self.indices_cache[device][level]\n\n    def batch_label_anchors(self, batch_size: int, gt_boxes, gt_classes):\n        num_levels = self.anchors.max_level - self.anchors.min_level + 1\n        cls_targets_out = [[] for _ in range(num_levels)]\n        box_targets_out = [[] for _ in range(num_levels)]\n        num_positives_out = []\n\n        # FIXME this may be a bottleneck, would be faster if batched, or should be done in loader/dataset?\n        anchor_box_list = BoxList(self.anchors.boxes)\n        for i in range(batch_size):\n            last_sample = i == batch_size - 1\n            # cls_weights, box_weights are not used\n            cls_targets, _, box_targets, _, matches = self.target_assigner.assign(\n                anchor_box_list, BoxList(gt_boxes[i]), gt_classes[i])\n\n            # class labels start from 1 and the background class = -1\n            cls_targets -= 1\n            cls_targets = cls_targets.long()\n\n            # Unpack labels.\n            """"""Unpacks an array of cls/box into multiple scales.""""""\n            for level in range(self.anchors.min_level, self.anchors.max_level + 1):\n                level_index = level - self.anchors.min_level\n                feat_size = self.feat_size[level]\n                indices = self._get_indices(cls_targets.device, level)\n                cls_targets_out[level_index].append(\n                    torch.index_select(cls_targets, 0, indices).view([feat_size, feat_size, -1]))\n                box_targets_out[level_index].append(\n                    torch.index_select(box_targets, 0, indices).view([feat_size, feat_size, -1]))\n                if last_sample:\n                    cls_targets_out[level_index] = torch.stack(cls_targets_out[level_index])\n                    box_targets_out[level_index] = torch.stack(box_targets_out[level_index])\n\n            num_positives_out.append((matches.match_results != -1).float().sum())\n            if last_sample:\n                num_positives_out = torch.stack(num_positives_out)\n\n        return cls_targets_out, box_targets_out, num_positives_out\n\n'"
effdet/bench.py,9,"b'"""""" PyTorch EfficientDet support benches\n\nHacked together by Ross Wightman\n""""""\nimport torch\nimport torch.nn as nn\nfrom timm.utils import ModelEma\nfrom .anchors import Anchors, AnchorLabeler, generate_detections, MAX_DETECTION_POINTS\nfrom .loss import DetectionLoss\n\n\ndef _post_process(config, cls_outputs, box_outputs):\n    """"""Selects top-k predictions.\n\n    Post-proc code adapted from Tensorflow version at: https://github.com/google/automl/tree/master/efficientdet\n    and optimized for PyTorch.\n\n    Args:\n        config: a parameter dictionary that includes `min_level`, `max_level`,  `batch_size`, and `num_classes`.\n\n        cls_outputs: an OrderDict with keys representing levels and values\n            representing logits in [batch_size, height, width, num_anchors].\n\n        box_outputs: an OrderDict with keys representing levels and values\n            representing box regression targets in [batch_size, height, width, num_anchors * 4].\n    """"""\n    batch_size = cls_outputs[0].shape[0]\n    cls_outputs_all = torch.cat([\n        cls_outputs[level].permute(0, 2, 3, 1).reshape([batch_size, -1, config.num_classes])\n        for level in range(config.num_levels)], 1)\n\n    box_outputs_all = torch.cat([\n        box_outputs[level].permute(0, 2, 3, 1).reshape([batch_size, -1, 4])\n        for level in range(config.num_levels)], 1)\n\n    _, cls_topk_indices_all = torch.topk(cls_outputs_all.reshape(batch_size, -1), dim=1, k=MAX_DETECTION_POINTS)\n    indices_all = cls_topk_indices_all / config.num_classes\n    classes_all = cls_topk_indices_all % config.num_classes\n\n    box_outputs_all_after_topk = torch.gather(\n        box_outputs_all, 1, indices_all.unsqueeze(2).expand(-1, -1, 4))\n\n    cls_outputs_all_after_topk = torch.gather(\n        cls_outputs_all, 1, indices_all.unsqueeze(2).expand(-1, -1, config.num_classes))\n    cls_outputs_all_after_topk = torch.gather(\n        cls_outputs_all_after_topk, 2, classes_all.unsqueeze(2))\n\n    return cls_outputs_all_after_topk, box_outputs_all_after_topk, indices_all, classes_all\n\n\n@torch.jit.script\ndef _batch_detection(batch_size: int, class_out, box_out, anchor_boxes, indices, classes, img_scale, img_size):\n    batch_detections = []\n    # FIXME we may be able to do this as a batch with some tensor reshaping/indexing, PR welcome\n    for i in range(batch_size):\n        detections = generate_detections(\n            class_out[i], box_out[i], anchor_boxes, indices[i], classes[i], img_scale[i], img_size[i])\n        batch_detections.append(detections)\n    return torch.stack(batch_detections, dim=0)\n\n\nclass DetBenchPredict(nn.Module):\n    def __init__(self, model, config):\n        super(DetBenchPredict, self).__init__()\n        self.config = config\n        self.model = model\n        self.anchors = Anchors(\n            config.min_level, config.max_level,\n            config.num_scales, config.aspect_ratios,\n            config.anchor_scale, config.image_size)\n\n    def forward(self, x, img_scales, img_size):\n        class_out, box_out = self.model(x)\n        class_out, box_out, indices, classes = _post_process(self.config, class_out, box_out)\n        return _batch_detection(\n            x.shape[0], class_out, box_out, self.anchors.boxes, indices, classes, img_scales, img_size)\n\n\nclass DetBenchTrain(nn.Module):\n    def __init__(self, model, config):\n        super(DetBenchTrain, self).__init__()\n        self.config = config\n        self.model = model\n        self.anchors = Anchors(\n            config.min_level, config.max_level,\n            config.num_scales, config.aspect_ratios,\n            config.anchor_scale, config.image_size)\n        self.anchor_labeler = AnchorLabeler(self.anchors, config.num_classes, match_threshold=0.5)\n        self.loss_fn = DetectionLoss(self.config)\n\n    def forward(self, x, target):\n        class_out, box_out = self.model(x)\n        cls_targets, box_targets, num_positives = self.anchor_labeler.batch_label_anchors(\n            x.shape[0], target[\'bbox\'], target[\'cls\'])\n        loss, class_loss, box_loss = self.loss_fn(class_out, box_out, cls_targets, box_targets, num_positives)\n        output = dict(loss=loss, class_loss=class_loss, box_loss=box_loss)\n        if not self.training:\n            # if eval mode, output detections for evaluation\n            class_out, box_out, indices, classes = _post_process(self.config, class_out, box_out)\n            output[\'detections\'] = _batch_detection(\n                x.shape[0], class_out, box_out, self.anchors.boxes, indices, classes,\n                target[\'img_scale\'], target[\'img_size\'])\n        return output\n\n\ndef unwrap_bench(model):\n    # Unwrap a model in support bench so that various other fns can access the weights and attribs of the\n    # underlying model directly\n    if isinstance(model, ModelEma):  # unwrap ModelEma\n        return unwrap_bench(model.ema)\n    elif hasattr(model, \'module\'):  # unwrap DDP\n        return unwrap_bench(model.module)\n    elif hasattr(model, \'model\'):  # unwrap Bench -> model\n        return unwrap_bench(model.model)\n    else:\n        return model\n'"
effdet/distributed.py,18,"b'"""""" PyTorch distributed helpers\n\nSome of this lifted from Detectron2 with other fns added by myself. Some of the Detectron2 fns\nwere intended for use with GLOO PG. I am using NCCL here with default PG so not everything will work\nas is -RW\n""""""\nimport functools\nimport logging\nimport numpy as np\nimport pickle\nimport torch\nimport torch.distributed as dist\n\n_LOCAL_PROCESS_GROUP = None\n""""""\nA torch process group which only includes processes that on the same machine as the current process.\nThis variable is set when processes are spawned by `launch()` in ""engine/launch.py"".\n""""""\n\n\ndef get_world_size() -> int:\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size()\n\n\ndef get_rank() -> int:\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    return dist.get_rank()\n\n\ndef get_local_rank() -> int:\n    """"""\n    Returns:\n        The rank of the current process within the local (per-machine) process group.\n    """"""\n    if not dist.is_available():\n        return 0\n    if not dist.is_initialized():\n        return 0\n    assert _LOCAL_PROCESS_GROUP is not None\n    return dist.get_rank(group=_LOCAL_PROCESS_GROUP)\n\n\ndef get_local_size() -> int:\n    """"""\n    Returns:\n        The size of the per-machine process group,\n        i.e. the number of processes per machine.\n    """"""\n    if not dist.is_available():\n        return 1\n    if not dist.is_initialized():\n        return 1\n    return dist.get_world_size(group=_LOCAL_PROCESS_GROUP)\n\n\ndef is_main_process() -> bool:\n    return get_rank() == 0\n\n\ndef synchronize():\n    """"""\n    Helper function to synchronize (barrier) among all processes when\n    using distributed training\n    """"""\n    if not dist.is_available():\n        return\n    if not dist.is_initialized():\n        return\n    world_size = dist.get_world_size()\n    if world_size == 1:\n        return\n    dist.barrier()\n\n\n@functools.lru_cache()\ndef _get_global_gloo_group():\n    """"""\n    Return a process group based on gloo backend, containing all the ranks\n    The result is cached.\n    """"""\n    if dist.get_backend() == ""nccl"":\n        return dist.new_group(backend=""gloo"")\n    else:\n        return dist.group.WORLD\n\n\ndef _serialize_to_tensor(data, group):\n    backend = dist.get_backend(group)\n    assert backend in [""gloo"", ""nccl""]\n    device = torch.device(""cpu"" if backend == ""gloo"" else ""cuda"")\n\n    buffer = pickle.dumps(data)\n    if len(buffer) > 1024 ** 3:\n        logger = logging.getLogger(__name__)\n        logger.warning(\n            ""Rank {} trying to all-gather {:.2f} GB of data on device {}"".format(\n                get_rank(), len(buffer) / (1024 ** 3), device\n            )\n        )\n    storage = torch.ByteStorage.from_buffer(buffer)\n    tensor = torch.ByteTensor(storage).to(device=device)\n    return tensor\n\n\ndef _pad_to_largest_tensor(tensor, group):\n    """"""\n    Returns:\n        list[int]: size of the tensor, on each rank\n        Tensor: padded tensor that has the max size\n    """"""\n    world_size = dist.get_world_size(group=group)\n    assert (\n        world_size >= 1\n    ), ""comm.gather/all_gather must be called from ranks within the given group!""\n    local_size = torch.tensor([tensor.numel()], dtype=torch.int64, device=tensor.device)\n    size_list = [\n        torch.zeros([1], dtype=torch.int64, device=tensor.device) for _ in range(world_size)\n    ]\n    dist.all_gather(size_list, local_size, group=group)\n    size_list = [int(size.item()) for size in size_list]\n\n    max_size = max(size_list)\n\n    # we pad the tensor because torch all_gather does not support\n    # gathering tensors of different shapes\n    if local_size != max_size:\n        padding = torch.zeros((max_size - local_size,), dtype=torch.uint8, device=tensor.device)\n        tensor = torch.cat((tensor, padding), dim=0)\n    return size_list, tensor\n\n\ndef all_gather(data, group=None):\n    """"""\n    Run all_gather on arbitrary picklable data (not necessarily tensors).\n    Args:\n        data: any picklable object\n        group: a torch process group. By default, will use a group which\n            contains all ranks on gloo backend.\n    Returns:\n        list[data]: list of data gathered from each rank\n    """"""\n    if get_world_size() == 1:\n        return [data]\n    if group is None:\n        group = _get_global_gloo_group()\n    if dist.get_world_size(group) == 1:\n        return [data]\n\n    tensor = _serialize_to_tensor(data, group)\n\n    size_list, tensor = _pad_to_largest_tensor(tensor, group)\n    max_size = max(size_list)\n\n    # receiving Tensor from all ranks\n    tensor_list = [torch.empty((max_size,), dtype=torch.uint8, device=tensor.device) for _ in size_list]\n    dist.all_gather(tensor_list, tensor, group=group)\n\n    data_list = []\n    for size, tensor in zip(size_list, tensor_list):\n        buffer = tensor.cpu().numpy().tobytes()[:size]\n        data_list.append(pickle.loads(buffer))\n\n    return data_list\n\n\ndef gather(data, dst=0, group=None):\n    """"""\n    Run gather on arbitrary picklable data (not necessarily tensors).\n    Args:\n        data: any picklable object\n        dst (int): destination rank\n        group: a torch process group. By default, will use a group which\n            contains all ranks on gloo backend.\n    Returns:\n        list[data]: on dst, a list of data gathered from each rank. Otherwise,\n            an empty list.\n    """"""\n    if get_world_size() == 1:\n        return [data]\n    if group is None:\n        group = _get_global_gloo_group()\n    if dist.get_world_size(group=group) == 1:\n        return [data]\n    rank = dist.get_rank(group=group)\n\n    tensor = _serialize_to_tensor(data, group)\n    size_list, tensor = _pad_to_largest_tensor(tensor, group)\n\n    # receiving Tensor from all ranks\n    if rank == dst:\n        max_size = max(size_list)\n        tensor_list = [torch.empty((max_size,), dtype=torch.uint8, device=tensor.device) for _ in size_list]\n        dist.gather(tensor, tensor_list, dst=dst, group=group)\n\n        data_list = []\n        for size, tensor in zip(size_list, tensor_list):\n            buffer = tensor.cpu().numpy().tobytes()[:size]\n            data_list.append(pickle.loads(buffer))\n        return data_list\n    else:\n        dist.gather(tensor, [], dst=dst, group=group)\n        return []\n\n\ndef shared_random_seed():\n    """"""\n    Returns:\n        int: a random number that is the same across all workers.\n            If workers need a shared RNG, they can use this shared seed to\n            create one.\n    All workers must call this function, otherwise it will deadlock.\n    """"""\n    ints = np.random.randint(2 ** 31)\n    all_ints = all_gather(ints)\n    return all_ints[0]\n\n\ndef reduce_dict(input_dict, average=True):\n    """"""\n    Reduce the values in the dictionary from all processes so that process with rank\n    0 has the reduced results.\n    Args:\n        input_dict (dict): inputs to be reduced. All the values must be scalar CUDA Tensor.\n        average (bool): whether to do average or sum\n    Returns:\n        a dict with the same keys as input_dict, after reduction.\n    """"""\n    world_size = get_world_size()\n    if world_size < 2:\n        return input_dict\n    with torch.no_grad():\n        names = []\n        values = []\n        # sort the keys so that they are consistent across processes\n        for k in sorted(input_dict.keys()):\n            names.append(k)\n            values.append(input_dict[k])\n        values = torch.stack(values, dim=0)\n        dist.reduce(values, dst=0)\n        if dist.get_rank() == 0 and average:\n            # only main process gets accumulated, so only divide by\n            # world_size in this case\n            values /= world_size\n        reduced_dict = {k: v for k, v in zip(names, values)}\n    return reduced_dict\n\n\ndef all_gather_container(container, group=dist.group.WORLD):\n    world_size = dist.get_world_size(group)\n\n    def _do_gather(tensor):\n        tensor_list = [torch.empty_like(tensor) for _ in range(world_size)]\n        dist.all_gather(tensor_list, tensor, group=group)\n        return torch.cat(tensor_list, dim=-1)\n\n    if isinstance(container, dict):\n        gathered = dict()\n        for k, v in container.items():\n            v = _do_gather(v)\n            gathered[k] = v\n        return gathered\n    elif isinstance(container, (list, tuple)):\n        gathered = [_do_gather(v) for v in container]\n        if isinstance(container, tuple):\n            gathered = tuple(gathered)\n        return gathered\n    else:\n        # if not a dict, list, tuple, expect a singular tensor\n        assert isinstance(container, torch.Tensor)\n        return _do_gather(container)\n\n\ndef gather_container(container, dst, group=dist.group.WORLD):\n    world_size = dist.get_world_size(group)\n    this_rank = dist.get_rank(group)\n\n    def _do_gather(tensor):\n        if this_rank == dst:\n            tensor_list = [torch.empty_like(tensor) for _ in range(world_size)]\n        else:\n            tensor_list = None\n        dist.gather(tensor, tensor_list, dst=dst, group=group)\n        return torch.cat(tensor_list, dim=-1)\n\n    if isinstance(container, dict):\n        gathered = dict()\n        for k, v in container.items():\n            v = _do_gather(v)\n            gathered[k] = v\n        return gathered\n    elif isinstance(container, (list, tuple)):\n        gathered = [_do_gather(v) for v in container]\n        if isinstance(container, tuple):\n            gathered = tuple(gathered)\n        return gathered\n    else:\n        # if not a dict, list, tuple, expect a singular tensor\n        assert isinstance(container, torch.Tensor)\n        return _do_gather(container)\n'"
effdet/efficientdet.py,10,"b'"""""" PyTorch EfficientDet model\n\nBased on official Tensorflow version at: https://github.com/google/automl/tree/master/efficientdet\nPaper: https://arxiv.org/abs/1911.09070\n\nHacked together by Ross Wightman\n""""""\nimport torch\nimport torch.nn as nn\nimport logging\nimport math\nfrom collections import OrderedDict\nfrom typing import List\n\nfrom timm import create_model\nfrom timm.models.layers import create_conv2d, drop_path, create_pool2d, Swish\nfrom .config import get_fpn_config\n\n_DEBUG = False\n\n_ACT_LAYER = Swish\n\n\nclass SequentialAppend(nn.Sequential):\n    def __init__(self, *args):\n        super(SequentialAppend, self).__init__(*args)\n\n    def forward(self, x: List[torch.Tensor]):\n        for module in self:\n            x.append(module(x))\n        return x\n\n\nclass SequentialAppendLast(nn.Sequential):\n    def __init__(self, *args):\n        super(SequentialAppendLast, self).__init__(*args)\n\n    def forward(self, x: List[torch.Tensor]):\n        for module in self:\n            x.append(module(x[-1]))\n        return x\n\n\nclass ConvBnAct2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, padding=\'\', bias=False,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None, act_layer=_ACT_LAYER):\n        super(ConvBnAct2d, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n        self.conv = create_conv2d(\n            in_channels, out_channels, kernel_size, stride=stride, dilation=dilation, padding=padding, bias=bias)\n        self.bn = None if norm_layer is None else norm_layer(out_channels, **norm_kwargs)\n        self.act = None if act_layer is None else act_layer(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.act is not None:\n            x = self.act(x)\n        return x\n\n\nclass SeparableConv2d(nn.Module):\n    """""" Separable Conv\n    """"""\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1, padding=\'\', bias=False,\n                 channel_multiplier=1.0, pw_kernel_size=1, act_layer=_ACT_LAYER,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(SeparableConv2d, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n\n        self.conv_dw = create_conv2d(\n            in_channels, int(in_channels * channel_multiplier), kernel_size,\n            stride=stride, dilation=dilation, padding=padding, depthwise=True)\n\n        self.conv_pw = create_conv2d(\n            int(in_channels * channel_multiplier), out_channels, pw_kernel_size, padding=padding, bias=bias)\n\n        self.bn = None if norm_layer is None else norm_layer(out_channels, **norm_kwargs)\n        self.act = None if act_layer is None else act_layer(inplace=True)\n\n    def forward(self, x):\n        x = self.conv_dw(x)\n        x = self.conv_pw(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.act is not None:\n            x = self.act(x)\n        return x\n\n\nclass ResampleFeatureMap(nn.Sequential):\n\n    def __init__(self, in_channels, out_channels, reduction_ratio=1., pad_type=\'\', pooling_type=\'max\',\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None, apply_bn=False, conv_after_downsample=False,\n                 redundant_bias=False):\n        super(ResampleFeatureMap, self).__init__()\n        pooling_type = pooling_type or \'max\'\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.reduction_ratio = reduction_ratio\n        self.conv_after_downsample = conv_after_downsample\n\n        conv = None\n        if in_channels != out_channels:\n            conv = ConvBnAct2d(\n                in_channels, out_channels, kernel_size=1, padding=pad_type,\n                norm_layer=norm_layer if apply_bn else None, norm_kwargs=norm_kwargs,\n                bias=not apply_bn or redundant_bias, act_layer=None)\n\n        if reduction_ratio > 1:\n            stride_size = int(reduction_ratio)\n            if conv is not None and not self.conv_after_downsample:\n                self.add_module(\'conv\', conv)\n            self.add_module(\n                \'downsample\',\n                create_pool2d(\n                    pooling_type, kernel_size=stride_size + 1, stride=stride_size, padding=pad_type))\n            if conv is not None and self.conv_after_downsample:\n                self.add_module(\'conv\', conv)\n        else:\n            if conv is not None:\n                self.add_module(\'conv\', conv)\n            if reduction_ratio < 1:\n                scale = int(1 // reduction_ratio)\n                self.add_module(\'upsample\', nn.UpsamplingNearest2d(scale_factor=scale))\n\n    # def forward(self, x):\n    #     #  here for debugging only\n    #     assert x.shape[1] == self.in_channels\n    #     if self.reduction_ratio > 1:\n    #         if hasattr(self, \'conv\') and not self.conv_after_downsample:\n    #             x = self.conv(x)\n    #         x = self.downsample(x)\n    #         if hasattr(self, \'conv\') and self.conv_after_downsample:\n    #             x = self.conv(x)\n    #     else:\n    #         if hasattr(self, \'conv\'):\n    #             x = self.conv(x)\n    #         if self.reduction_ratio < 1:\n    #             x = self.upsample(x)\n    #     return x\n\n\nclass FpnCombine(nn.Module):\n    def __init__(self, feature_info, fpn_config, fpn_channels, inputs_offsets, target_reduction, pad_type=\'\',\n                 pooling_type=\'max\', norm_layer=nn.BatchNorm2d, norm_kwargs=None, apply_bn_for_resampling=False,\n                 conv_after_downsample=False, redundant_bias=False, weight_method=\'attn\'):\n        super(FpnCombine, self).__init__()\n        self.inputs_offsets = inputs_offsets\n        self.weight_method = weight_method\n\n        self.resample = nn.ModuleDict()\n        for idx, offset in enumerate(inputs_offsets):\n            in_channels = fpn_channels\n            if offset < len(feature_info):\n                in_channels = feature_info[offset][\'num_chs\']\n                input_reduction = feature_info[offset][\'reduction\']\n            else:\n                node_idx = offset - len(feature_info)\n                input_reduction = fpn_config.nodes[node_idx][\'reduction\']\n            reduction_ratio = target_reduction / input_reduction\n            self.resample[str(offset)] = ResampleFeatureMap(\n                in_channels, fpn_channels, reduction_ratio=reduction_ratio, pad_type=pad_type,\n                pooling_type=pooling_type, norm_layer=norm_layer, norm_kwargs=norm_kwargs,\n                apply_bn=apply_bn_for_resampling, conv_after_downsample=conv_after_downsample,\n                redundant_bias=redundant_bias)\n\n        if weight_method == \'attn\' or weight_method == \'fastattn\':\n            # WSM\n            self.edge_weights = nn.Parameter(torch.ones(len(inputs_offsets)), requires_grad=True)\n        else:\n            self.edge_weights = None\n\n    def forward(self, x):\n        dtype = x[0].dtype\n        nodes = []\n        for offset in self.inputs_offsets:\n            input_node = x[offset]\n            input_node = self.resample[str(offset)](input_node)\n            nodes.append(input_node)\n\n        if self.weight_method == \'attn\':\n            normalized_weights = torch.softmax(self.edge_weights.type(dtype), dim=0)\n            x = torch.stack(nodes, dim=-1) * normalized_weights\n        elif self.weight_method == \'fastattn\':\n            edge_weights = nn.functional.relu(self.edge_weights.type(dtype))\n            weights_sum = torch.sum(edge_weights)\n            x = torch.stack(\n                [(nodes[i] * edge_weights[i]) / (weights_sum + 0.0001) for i in range(len(nodes))], dim=-1)\n        elif self.weight_method == \'sum\':\n            x = torch.stack(nodes, dim=-1)\n        else:\n            raise ValueError(\'unknown weight_method {}\'.format(self.weight_method))\n        x = torch.sum(x, dim=-1)\n        return x\n\n\nclass BiFpnLayer(nn.Module):\n    def __init__(self, feature_info, fpn_config, fpn_channels, num_levels=5, pad_type=\'\',\n                 pooling_type=\'max\', norm_layer=nn.BatchNorm2d, norm_kwargs=None, act_layer=_ACT_LAYER,\n                 apply_bn_for_resampling=False, conv_after_downsample=True, conv_bn_relu_pattern=False,\n                 separable_conv=True, redundant_bias=False):\n        super(BiFpnLayer, self).__init__()\n        self.fpn_config = fpn_config\n        self.num_levels = num_levels\n        self.conv_bn_relu_pattern = False\n\n        self.feature_info = []\n        self.fnode = SequentialAppend()\n        for i, fnode_cfg in enumerate(fpn_config.nodes):\n            logging.debug(\'fnode {} : {}\'.format(i, fnode_cfg))\n            fnode_layers = OrderedDict()\n\n            # combine features\n            reduction = fnode_cfg[\'reduction\']\n            fnode_layers[\'combine\'] = FpnCombine(\n                feature_info, fpn_config, fpn_channels, fnode_cfg[\'inputs_offsets\'], target_reduction=reduction,\n                pad_type=pad_type, pooling_type=pooling_type, norm_layer=norm_layer, norm_kwargs=norm_kwargs,\n                apply_bn_for_resampling=apply_bn_for_resampling, conv_after_downsample=conv_after_downsample,\n                redundant_bias=redundant_bias, weight_method=fpn_config.weight_method)\n            self.feature_info.append(dict(num_chs=fpn_channels, reduction=reduction))\n\n            # after combine ops\n            after_combine = OrderedDict()\n            if not conv_bn_relu_pattern:\n                after_combine[\'act\'] = act_layer(inplace=True)\n                conv_bias = redundant_bias\n                conv_act = None\n            else:\n                conv_bias = False\n                conv_act = act_layer\n            conv_kwargs = dict(\n                in_channels=fpn_channels, out_channels=fpn_channels, kernel_size=3, padding=pad_type,\n                bias=conv_bias, norm_layer=norm_layer, norm_kwargs=norm_kwargs, act_layer=conv_act)\n            after_combine[\'conv\'] = SeparableConv2d(**conv_kwargs) if separable_conv else ConvBnAct2d(**conv_kwargs)\n            fnode_layers[\'after_combine\'] = nn.Sequential(after_combine)\n\n            self.fnode.add_module(str(i), nn.Sequential(fnode_layers))\n\n        self.feature_info = self.feature_info[-num_levels::]\n\n    def forward(self, x):\n        x = self.fnode(x)\n        return x[-self.num_levels::]\n\n\nclass BiFpn(nn.Module):\n\n    def __init__(self, config, feature_info, norm_layer=nn.BatchNorm2d, norm_kwargs=None, act_layer=_ACT_LAYER):\n        super(BiFpn, self).__init__()\n        self.config = config\n        fpn_config = config.fpn_config or get_fpn_config(config.fpn_name)\n\n        self.resample = SequentialAppendLast()\n        for level in range(config.num_levels):\n            if level < len(feature_info):\n                in_chs = feature_info[level][\'num_chs\']\n                reduction = feature_info[level][\'reduction\']\n            else:\n                # Adds a coarser level by downsampling the last feature map\n                reduction_ratio = 2\n                self.resample.add_module(str(level), ResampleFeatureMap(\n                    in_channels=in_chs,\n                    out_channels=config.fpn_channels,\n                    pad_type=config.pad_type,\n                    pooling_type=config.pooling_type,\n                    norm_layer=norm_layer,\n                    norm_kwargs=norm_kwargs,\n                    reduction_ratio=reduction_ratio,\n                    apply_bn=config.apply_bn_for_resampling,\n                    conv_after_downsample=config.conv_after_downsample,\n                    redundant_bias=config.redundant_bias,\n                ))\n                in_chs = config.fpn_channels\n                reduction = int(reduction * reduction_ratio)\n                feature_info.append(dict(num_chs=in_chs, reduction=reduction))\n\n        self.cell = nn.Sequential()\n        for rep in range(config.fpn_cell_repeats):\n            logging.debug(\'building cell {}\'.format(rep))\n            fpn_layer = BiFpnLayer(\n                feature_info=feature_info,\n                fpn_config=fpn_config,\n                fpn_channels=config.fpn_channels,\n                num_levels=config.num_levels,\n                pad_type=config.pad_type,\n                pooling_type=config.pooling_type,\n                norm_layer=norm_layer,\n                norm_kwargs=norm_kwargs,\n                act_layer=act_layer,\n                separable_conv=config.separable_conv,\n                apply_bn_for_resampling=config.apply_bn_for_resampling,\n                conv_after_downsample=config.conv_after_downsample,\n                conv_bn_relu_pattern=config.conv_bn_relu_pattern,\n                redundant_bias=config.redundant_bias,\n            )\n            self.cell.add_module(str(rep), fpn_layer)\n            feature_info = fpn_layer.feature_info\n\n    def forward(self, x):\n        assert len(self.resample) == self.config.num_levels - len(x)\n        x = self.resample(x)\n        x = self.cell(x)\n        return x\n\n\nclass HeadNet(nn.Module):\n    def __init__(self, config, num_outputs, norm_layer=nn.BatchNorm2d, norm_kwargs=None, act_layer=_ACT_LAYER):\n        super(HeadNet, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n        self.config = config\n        num_anchors = len(config.aspect_ratios) * config.num_scales\n\n        self.conv_rep = nn.ModuleList()\n        self.bn_rep = nn.ModuleList()\n        conv_kwargs = dict(\n            in_channels=config.fpn_channels, out_channels=config.fpn_channels, kernel_size=3,\n            padding=self.config.pad_type, bias=config.redundant_bias, act_layer=None, norm_layer=None)\n        for i in range(config.box_class_repeats):\n            conv = SeparableConv2d(**conv_kwargs) if config.separable_conv else ConvBnAct2d(**conv_kwargs)\n            self.conv_rep.append(conv)\n\n            bn_levels = []\n            for _ in range(config.num_levels):\n                bn_seq = nn.Sequential()\n                bn_seq.add_module(\'bn\', norm_layer(config.fpn_channels, **norm_kwargs))\n                bn_levels.append(bn_seq)\n            self.bn_rep.append(nn.ModuleList(bn_levels))\n\n        self.act = act_layer(inplace=True)\n\n        predict_kwargs = dict(\n            in_channels=config.fpn_channels, out_channels=num_outputs * num_anchors, kernel_size=3,\n            padding=self.config.pad_type, bias=True, norm_layer=None, act_layer=None)\n        if config.separable_conv:\n            self.predict = SeparableConv2d(**predict_kwargs)\n        else:\n            self.predict = ConvBnAct2d(**predict_kwargs)\n\n    def forward(self, x):\n        outputs = []\n        for level in range(self.config.num_levels):\n            x_level = x[level]\n            for i in range(self.config.box_class_repeats):\n                x_level_ident = x_level\n                x_level = self.conv_rep[i](x_level)\n                x_level = self.bn_rep[i][level](x_level)\n                x_level = self.act(x_level)\n                if i > 0 and self.config.fpn_drop_path_rate:\n                    x_level = drop_path(x_level, self.config.fpn_drop_path_rate, self.training)\n                    x_level += x_level_ident\n            outputs.append(self.predict(x_level))\n        return outputs\n\n\ndef _init_weight(m, n=\'\', ):\n    """""" Weight initialization as per Tensorflow official implementations.\n    """"""\n\n    def _fan_in_out(w, groups=1):\n        dimensions = w.dim()\n        if dimensions < 2:\n            raise ValueError(""Fan in and fan out can not be computed for tensor with fewer than 2 dimensions"")\n        num_input_fmaps = w.size(1)\n        num_output_fmaps = w.size(0)\n        receptive_field_size = 1\n        if w.dim() > 2:\n            receptive_field_size = w[0][0].numel()\n        fan_in = num_input_fmaps * receptive_field_size\n        fan_out = num_output_fmaps * receptive_field_size\n        fan_out //= groups\n        return fan_in, fan_out\n\n    def _glorot_uniform(w, gain=1, groups=1):\n        fan_in, fan_out = _fan_in_out(w, groups)\n        gain /= max(1., (fan_in + fan_out) / 2.)  # fan avg\n        limit = math.sqrt(3.0 * gain)\n        w.data.uniform_(-limit, limit)\n\n    def _variance_scaling(w, gain=1, groups=1):\n        fan_in, fan_out = _fan_in_out(w, groups)\n        gain /= max(1., fan_in)  # fan in\n        # gain /= max(1., (fan_in + fan_out) / 2.)  # fan\n\n        # should it be normal or trunc normal? using normal for now since no good trunc in PT\n        # constant taken from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n        # std = math.sqrt(gain) / .87962566103423978\n        # w.data.trunc_normal(std=std)\n        std = math.sqrt(gain)\n        w.data.normal_(std=std)\n\n    if isinstance(m, SeparableConv2d):\n        if \'box_net\' in n or \'class_net\' in n:\n            _variance_scaling(m.conv_dw.weight, groups=m.conv_dw.groups)\n            _variance_scaling(m.conv_pw.weight)\n            if m.conv_pw.bias is not None:\n                if \'class_net.predict\' in n:\n                    m.conv_pw.bias.data.fill_(-math.log((1 - 0.01) / 0.01))\n                else:\n                    m.conv_pw.bias.data.zero_()\n        else:\n            _glorot_uniform(m.conv_dw.weight, groups=m.conv_dw.groups)\n            _glorot_uniform(m.conv_pw.weight)\n            if m.conv_pw.bias is not None:\n                m.conv_pw.bias.data.zero_()\n    elif isinstance(m, ConvBnAct2d):\n        if \'box_net\' in n or \'class_net\' in n:\n            m.conv.weight.data.normal_(std=.01)\n            if m.conv.bias is not None:\n                if \'class_net.predict\' in n:\n                    m.conv.bias.data.fill_(-math.log((1 - 0.01) / 0.01))\n                else:\n                    m.conv.bias.data.zero_()\n        else:\n            _glorot_uniform(m.conv.weight)\n            if m.conv.bias is not None:\n                m.conv.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        # looks like all bn init the same?\n        m.weight.data.fill_(1.0)\n        m.bias.data.zero_()\n\n\ndef _init_weight_alt(m, n=\'\', ):\n    """""" Weight initialization alternative, based on EfficientNet bacbkone init w/ class bias addition\n    NOTE: this will likely be removed after some experimentation\n    """"""\n    if isinstance(m, nn.Conv2d):\n        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        fan_out //= m.groups\n        m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n        if m.bias is not None:\n            if \'class_net.predict\' in n:\n                m.bias.data.fill_(-math.log((1 - 0.01) / 0.01))\n            else:\n                m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1.0)\n        m.bias.data.zero_()\n\n\nclass EfficientDet(nn.Module):\n\n    def __init__(self, config, norm_kwargs=None, pretrained_backbone=True, alternate_init=False):\n        super(EfficientDet, self).__init__()\n        norm_kwargs = norm_kwargs or dict(eps=.001, momentum=.01)\n        self.backbone = create_model(\n            config.backbone_name, features_only=True, out_indices=(2, 3, 4),\n            pretrained=pretrained_backbone, **config.backbone_args)\n        feature_info = [dict(num_chs=f[\'num_chs\'], reduction=f[\'reduction\'])\n                        for i, f in enumerate(self.backbone.feature_info())]\n        self.fpn = BiFpn(config, feature_info, norm_kwargs=norm_kwargs)\n        self.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=norm_kwargs)\n        self.box_net = HeadNet(config, num_outputs=4, norm_kwargs=norm_kwargs)\n\n        for n, m in self.named_modules():\n            if \'backbone\' not in n:\n                if alternate_init:\n                    _init_weight_alt(m, n)\n                else:\n                    _init_weight(m, n)\n\n    def forward(self, x):\n        x = self.backbone(x)\n        x = self.fpn(x)\n        x_class = self.class_net(x)\n        x_box = self.box_net(x)\n        return x_class, x_box\n'"
effdet/evaluator.py,3,"b""import torch\nimport torch.distributed as dist\nimport abc\nimport json\nfrom .distributed import synchronize, is_main_process, all_gather_container\nfrom pycocotools.cocoeval import COCOeval\n\n\nclass Evaluator:\n\n    def __init__(self):\n        pass\n\n    @abc.abstractmethod\n    def add_predictions(self, output, target):\n        pass\n\n    @abc.abstractmethod\n    def evaluate(self):\n        pass\n\n\nclass COCOEvaluator(Evaluator):\n\n    def __init__(self, coco_api, distributed=False):\n        super().__init__()\n        self.coco_api = coco_api\n        self.distributed = distributed\n        self.distributed_device = None\n        self.img_ids = []\n        self.predictions = []\n\n    def reset(self):\n        self.img_ids = []\n        self.predictions = []\n\n    def add_predictions(self, detections, target):\n        if self.distributed:\n            if self.distributed_device is None:\n                # cache for use later to broadcast end metric\n                self.distributed_device = detections.device\n            synchronize()\n            detections = all_gather_container(detections)\n            #target = all_gather_container(target)\n            sample_ids = all_gather_container(target['img_id'])\n            if not is_main_process():\n                return\n        else:\n            sample_ids = target['img_id']\n\n        detections = detections.cpu()\n        sample_ids = sample_ids.cpu()\n        for index, sample in enumerate(detections):\n            image_id = int(sample_ids[index])\n            for det in sample:\n                score = float(det[4])\n                if score < .001:  # stop when below this threshold, scores in descending order\n                    break\n                coco_det = dict(\n                    image_id=image_id,\n                    bbox=det[0:4].tolist(),\n                    score=score,\n                    category_id=int(det[5]))\n                self.img_ids.append(image_id)\n                self.predictions.append(coco_det)\n\n    def evaluate(self):\n        if not self.distributed or dist.get_rank() == 0:\n            assert len(self.predictions)\n            json.dump(self.predictions, open('./temp.json', 'w'), indent=4)\n            results = self.coco_api.loadRes('./temp.json')\n            coco_eval = COCOeval(self.coco_api, results, 'bbox')\n            coco_eval.params.imgIds = self.img_ids  # score only ids we've used\n            coco_eval.evaluate()\n            coco_eval.accumulate()\n            coco_eval.summarize()\n            metric = coco_eval.stats[0]  # mAP 0.5-0.95\n            if self.distributed:\n                dist.broadcast(torch.tensor(metric, device=self.distributed_device), 0)\n        else:\n            metric = torch.tensor(0, device=self.distributed_device)\n            dist.broadcast(metric, 0)\n            metric = metric.item()\n        self.reset()\n        return metric\n\n\nclass FastMapEvalluator(Evaluator):\n\n    def __init__(self, distributed=False):\n        super().__init__()\n        self.distributed = distributed\n        self.predictions = []\n\n    def add_predictions(self, output, target):\n        pass\n\n    def evaluate(self):\n        pass"""
effdet/factory.py,0,"b""from .efficientdet import EfficientDet\nfrom .bench import DetBenchTrain, DetBenchPredict\nfrom .config import get_efficientdet_config\nfrom .helpers import load_pretrained, load_checkpoint\n\n\ndef create_model(\n        model_name, bench_task='', pretrained=False, checkpoint_path='', checkpoint_ema=False, **kwargs):\n    config = get_efficientdet_config(model_name)\n\n    pretrained_backbone = kwargs.pop('pretrained_backbone', True)\n    if pretrained or checkpoint_path:\n        pretrained_backbone = False  # no point in loading backbone weights\n\n    redundant_bias = kwargs.pop('redundant_bias', None)\n    if redundant_bias is not None:\n        # override config if set to something\n        config.redundant_bias = redundant_bias\n\n    model = EfficientDet(config, pretrained_backbone=pretrained_backbone, **kwargs)\n\n    # FIXME handle different head classes / anchors and re-init of necessary layers w/ pretrained load\n\n    if checkpoint_path:\n        load_checkpoint(model, checkpoint_path, use_ema=checkpoint_ema)\n    elif pretrained:\n        load_pretrained(model, config.url)\n\n    # wrap model in task specific bench if set\n    if bench_task == 'train':\n        model = DetBenchTrain(model, config)\n    elif bench_task == 'predict':\n        model = DetBenchPredict(model, config)\n    return model\n\n\ndef create_model_from_config(config, bench_name='', pretrained=False, checkpoint_path='', **kwargs):\n    model = EfficientDet(config, **kwargs)\n\n    # FIXME handle different head classes / anchors and re-init of necessary layers w/ pretrained load\n\n    if checkpoint_path:\n        load_checkpoint(model, checkpoint_path)\n    elif pretrained:\n        load_pretrained(model, config.url)\n\n    # wrap model in task specific bench if set\n    if bench_name == 'train':\n        model = DetBenchTrain(model, config)\n    elif bench_name == 'predict':\n        model = DetBenchPredict(model, config)\n    return model\n"""
effdet/helpers.py,2,"b'import torch\nimport os\nimport logging\nfrom collections import OrderedDict\n\nfrom timm.models import load_checkpoint\n\ntry:\n    from torch.hub import load_state_dict_from_url\nexcept ImportError:\n    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n\n\ndef load_pretrained(model, url, filter_fn=None, strict=True):\n    if not url:\n        logging.warning(""Pretrained model URL is empty, using random initialization. ""\n                        ""Did you intend to use a `tf_` variant of the model?"")\n        return\n    state_dict = load_state_dict_from_url(url, progress=False, map_location=\'cpu\')\n    if filter_fn is not None:\n        state_dict = filter_fn(state_dict)\n    model.load_state_dict(state_dict, strict=strict)\n'"
effdet/loss.py,17,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom typing import Optional, List\n\n\ndef focal_loss(logits, targets, alpha: float, gamma: float, normalizer):\n    """"""Compute the focal loss between `logits` and the golden `target` values.\n\n    Focal loss = -(1-pt)^gamma * log(pt)\n    where pt is the probability of being classified to the true class.\n\n    Args:\n        logits: A float32 tensor of size [batch, height_in, width_in, num_predictions].\n\n        targets: A float32 tensor of size [batch, height_in, width_in, num_predictions].\n\n        alpha: A float32 scalar multiplying alpha to the loss from positive examples\n            and (1-alpha) to the loss from negative examples.\n\n        gamma: A float32 scalar modulating loss from hard and easy examples.\n\n         normalizer: A float32 scalar normalizes the total loss from all examples.\n\n    Returns:\n        loss: A float32 scalar representing normalized total loss.\n    """"""\n\n    positive_label_mask = targets == 1.0\n    cross_entropy = F.binary_cross_entropy_with_logits(logits, targets.to(logits.dtype), reduction=\'none\')\n    # Below are comments/derivations for computing modulator.\n    # For brevity, let x = logits,  z = targets, r = gamma, and p_t = sigmod(x)\n    # for positive samples and 1 - sigmoid(x) for negative examples.\n    #\n    # The modulator, defined as (1 - P_t)^r, is a critical part in focal loss\n    # computation. For r > 0, it puts more weights on hard examples, and less\n    # weights on easier ones. However if it is directly computed as (1 - P_t)^r,\n    # its back-propagation is not stable when r < 1. The implementation here\n    # resolves the issue.\n    #\n    # For positive samples (labels being 1),\n    #    (1 - p_t)^r\n    #  = (1 - sigmoid(x))^r\n    #  = (1 - (1 / (1 + exp(-x))))^r\n    #  = (exp(-x) / (1 + exp(-x)))^r\n    #  = exp(log((exp(-x) / (1 + exp(-x)))^r))\n    #  = exp(r * log(exp(-x)) - r * log(1 + exp(-x)))\n    #  = exp(- r * x - r * log(1 + exp(-x)))\n    #\n    # For negative samples (labels being 0),\n    #    (1 - p_t)^r\n    #  = (sigmoid(x))^r\n    #  = (1 / (1 + exp(-x)))^r\n    #  = exp(log((1 / (1 + exp(-x)))^r))\n    #  = exp(-r * log(1 + exp(-x)))\n    #\n    # Therefore one unified form for positive (z = 1) and negative (z = 0)\n    # samples is:\n    #      (1 - p_t)^r = exp(-r * z * x - r * log(1 + exp(-x))).\n    neg_logits = -1.0 * logits\n    modulator = torch.exp(gamma * targets * neg_logits - gamma * torch.log1p(torch.exp(neg_logits)))\n    loss = modulator * cross_entropy\n    weighted_loss = torch.where(positive_label_mask, alpha * loss, (1.0 - alpha) * loss)\n    weighted_loss /= normalizer\n    return weighted_loss\n\n\ndef huber_loss(\n        input, target, delta: float = 1., weights: Optional[torch.Tensor] = None, size_average: bool = True):\n    """"""\n    """"""\n    err = input - target\n    abs_err = err.abs()\n    quadratic = torch.clamp(abs_err, max=delta)\n    linear = abs_err - quadratic\n    loss = 0.5 * quadratic.pow(2) + delta * linear\n    if weights is not None:\n        loss *= weights\n    return loss.mean() if size_average else loss.sum()\n\n\ndef smooth_l1_loss(\n        input, target, beta: float = 1. / 9, weights: Optional[torch.Tensor] = None, size_average: bool = True):\n    """"""\n    very similar to the smooth_l1_loss from pytorch, but with the extra beta parameter\n    """"""\n    if beta < 1e-5:\n        # if beta == 0, then torch.where will result in nan gradients when\n        # the chain rule is applied due to pytorch implementation details\n        # (the False branch ""0.5 * n ** 2 / 0"" has an incoming gradient of\n        # zeros, rather than ""no gradient""). To avoid this issue, we define\n        # small values of beta to be exactly l1 loss.\n        loss = torch.abs(input - target)\n    else:\n        err = torch.abs(input - target)\n        loss = torch.where(err < beta, 0.5 * err.pow(2) / beta, err - 0.5 * beta)\n    if weights is not None:\n        loss *= weights\n    return loss.mean() if size_average else loss.sum()\n\n\ndef _classification_loss(cls_outputs, cls_targets, num_positives, alpha: float = 0.25, gamma: float = 2.0):\n    """"""Computes classification loss.""""""\n    normalizer = num_positives\n    classification_loss = focal_loss(cls_outputs, cls_targets, alpha, gamma, normalizer)\n    return classification_loss\n\n\ndef _box_loss(box_outputs, box_targets, num_positives, delta: float = 0.1):\n    """"""Computes box regression loss.""""""\n    # delta is typically around the mean value of regression target.\n    # for instances, the regression targets of 512x512 input with 6 anchors on\n    # P3-P7 pyramid is about [0.1, 0.1, 0.2, 0.2].\n    normalizer = num_positives * 4.0\n    mask = box_targets != 0.0\n    box_loss = huber_loss(box_targets, box_outputs, weights=mask, delta=delta, size_average=False)\n    box_loss /= normalizer\n    return box_loss\n\n\nclass DetectionLoss(nn.Module):\n    def __init__(self, config):\n        super(DetectionLoss, self).__init__()\n        self.config = config\n        self.num_classes = config.num_classes\n        self.alpha = config.alpha\n        self.gamma = config.gamma\n        self.delta = config.delta\n        self.box_loss_weight = config.box_loss_weight\n\n    def forward(\n            self, cls_outputs: List[torch.Tensor], box_outputs: List[torch.Tensor],\n            cls_targets: List[torch.Tensor], box_targets: List[torch.Tensor], num_positives: torch.Tensor):\n        """"""Computes total detection loss.\n        Computes total detection loss including box and class loss from all levels.\n        Args:\n            cls_outputs: a List with values representing logits in [batch_size, height, width, num_anchors].\n                at each feature level (index)\n\n            box_outputs: a List with values representing box regression targets in\n                [batch_size, height, width, num_anchors * 4] at each feature level (index)\n\n            cls_targets: groundtruth class targets.\n\n            box_targets: groundtrusth box targets.\n\n            num_positives: num positive grountruth anchors\n\n        Returns:\n            total_loss: an integer tensor representing total loss reducing from class and box losses from all levels.\n\n            cls_loss: an integer tensor representing total class loss.\n\n            box_loss: an integer tensor representing total box regression loss.\n        """"""\n        # Sum all positives in a batch for normalization and avoid zero\n        # num_positives_sum, which would lead to inf loss during training\n        num_positives_sum = num_positives.sum() + 1.0\n        levels = len(cls_outputs)\n\n        cls_losses = []\n        box_losses = []\n        for l in range(levels):\n            cls_targets_at_level = cls_targets[l]\n            box_targets_at_level = box_targets[l]\n\n            # Onehot encoding for classification labels.\n            # NOTE: PyTorch one-hot does not handle -ve entries (no hot) like Tensorflow, so mask them out\n            cls_targets_non_neg = cls_targets_at_level >= 0\n            cls_targets_at_level_oh = F.one_hot(cls_targets_at_level * cls_targets_non_neg, self.num_classes)\n            cls_targets_at_level_oh = torch.where(\n               cls_targets_non_neg.unsqueeze(-1), cls_targets_at_level_oh, torch.zeros_like(cls_targets_at_level_oh))\n\n            bs, height, width, _, _ = cls_targets_at_level_oh.shape\n            cls_targets_at_level_oh = cls_targets_at_level_oh.view(bs, height, width, -1)\n            cls_loss = _classification_loss(\n                cls_outputs[l].permute(0, 2, 3, 1),\n                cls_targets_at_level_oh,\n                num_positives_sum,\n                alpha=self.alpha, gamma=self.gamma)\n            cls_loss = cls_loss.view(bs, height, width, -1, self.num_classes)\n            cls_loss *= (cls_targets_at_level != -2).unsqueeze(-1).float()\n            cls_losses.append(cls_loss.sum())\n\n            box_losses.append(_box_loss(\n                box_outputs[l].permute(0, 2, 3, 1),\n                box_targets_at_level,\n                num_positives_sum,\n                delta=self.delta))\n\n        # Sum per level losses to total loss.\n        cls_loss = torch.sum(torch.stack(cls_losses, dim=-1), dim=-1)\n        box_loss = torch.sum(torch.stack(box_losses, dim=-1), dim=-1)\n        total_loss = cls_loss + self.box_loss_weight * box_loss\n        return total_loss, cls_loss, box_loss\n\n'"
effdet/version.py,0,"b""__version__ = '0.1.2'\n"""
effdet/config/__init__.py,0,"b'from .model_config import get_efficientdet_config, get_fpn_config, default_detection_model_configs\nfrom .train_config import default_detection_train_config\n'"
effdet/config/model_config.py,0,"b'""""""EfficientDet Configurations\n\nAdapted from official impl at https://github.com/google/automl/tree/master/efficientdet\n\nTODO use a different config system (OmegaConfig -> Hydra?), separate model from train specific hparams\n""""""\n\nfrom omegaconf import OmegaConf\n\n\ndef default_detection_model_configs():\n    """"""Returns a default detection configs.""""""\n    h = OmegaConf.create()\n\n    # model name.\n    h.name = \'tf_efficientdet_d1\'\n\n    h.backbone_name = \'tf_efficientnet_b1\'\n    h.backbone_args = None  # FIXME sort out kwargs vs config for backbone creation\n\n    # model specific, input preprocessing parameters\n    h.image_size = 640\n\n    # dataset specific head parameters\n    h.num_classes = 90\n\n    # model architecture\n    h.min_level = 3\n    h.max_level = 7\n    h.num_levels = h.max_level - h.min_level + 1\n    h.num_scales = 3\n    h.aspect_ratios = [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)]\n    h.anchor_scale = 4.0\n    h.pad_type = \'same\'  # original TF models require an equivalent of Tensorflow \'SAME\' padding\n\n    # For detection.\n    h.box_class_repeats = 3\n    h.fpn_cell_repeats = 3\n    h.fpn_channels = 88\n    h.separable_conv = True\n    h.apply_bn_for_resampling = True\n    h.conv_after_downsample = False\n    h.conv_bn_relu_pattern = False\n    h.use_native_resize_op = False\n    h.pooling_type = None\n    h.redundant_bias = True  # original TF models have back to back bias + BN layers, not necessary!\n\n    # version.\n    h.fpn_name = None\n    h.fpn_config = None\n    h.fpn_drop_path_rate = 0.  # No stochastic depth in default.\n\n    # classification loss (used by train bench)\n    h.alpha = 0.25\n    h.gamma = 1.5\n\n    # localization loss (used by train bench)\n    h.delta = 0.1\n    h.box_loss_weight = 50.0\n\n    return h\n\n\nefficientdet_model_param_dict = dict(\n    # Models with PyTorch friendly padding and PyTorch pretrained backbones, training TBD\n    efficientdet_d0=dict(\n        name=\'efficientdet_d0\',\n        backbone_name=\'efficientnet_b0\',\n        image_size=512,\n        fpn_channels=64,\n        fpn_cell_repeats=3,\n        box_class_repeats=3,\n        pad_type=\'\',\n        redundant_bias=False,\n        backbone_args=dict(drop_path_rate=0.1),\n        url=\'https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/efficientdet_d0-f3276ba8.pth\',\n    ),\n    efficientdet_d1=dict(\n        name=\'efficientdet_d1\',\n        backbone_name=\'efficientnet_b1\',\n        image_size=640,\n        fpn_channels=88,\n        fpn_cell_repeats=4,\n        box_class_repeats=3,\n        pad_type=\'\',\n        redundant_bias=False,\n        backbone_args=dict(drop_path_rate=0.2),\n        url=\'\',  # no pretrained weights yet\n    ),\n    efficientdet_d2=dict(\n        name=\'efficientdet_d2\',\n        backbone_name=\'efficientnet_b2\',\n        image_size=768,\n        fpn_channels=112,\n        fpn_cell_repeats=5,\n        box_class_repeats=3,\n        pad_type=\'\',\n        redundant_bias=False,\n        backbone_args=dict(drop_path_rate=0.2),\n        url=\'\',  # no pretrained weights yet\n    ),\n    efficientdet_d3=dict(\n        name=\'efficientdet_d3\',\n        backbone_name=\'efficientnet_b3\',\n        image_size=896,\n        fpn_channels=160,\n        fpn_cell_repeats=6,\n        box_class_repeats=4,\n        pad_type=\'\',\n        redundant_bias=False,\n        backbone_args=dict(drop_path_rate=0.2),\n        url=\'\',  # no pretrained weights yet\n    ),\n\n    # Experimental configs with alternate models, training TBD\n    # Note: any \'timm\' model in the EfficientDet family can be used as a backone here.\n    # TODO: add support in config for activation via string/factory so we can use ReLU/ReLU6 for EffNet-Lite & Mnv2\n    mixdet_m=dict(\n        name=\'mixdet_m\',\n        backbone_name=\'mixnet_m\',\n        image_size=512,\n        fpn_channels=64,\n        fpn_cell_repeats=3,\n        box_class_repeats=3,\n        pad_type=\'\',\n        redundant_bias=False,\n        backbone_args=dict(drop_path_rate=0.2),\n        url=\'\',  # no pretrained weights yet\n    ),\n    mixdet_l=dict(\n        name=\'mixdet_l\',\n        backbone_name=\'mixnet_l\',\n        image_size=640,\n        fpn_channels=88,\n        fpn_cell_repeats=4,\n        box_class_repeats=3,\n        pad_type=\'\',\n        redundant_bias=False,\n        backbone_args=dict(drop_path_rate=0.2),\n        url=\'\',  # no pretrained weights yet\n    ),\n\n    # Models ported from Tensorflow with pretrained backbones ported from Tensorflow\n    tf_efficientdet_d0=dict(\n        name=\'tf_efficientdet_d0\',\n        backbone_name=\'tf_efficientnet_b0\',\n        image_size=512,\n        fpn_channels=64,\n        fpn_cell_repeats=3,\n        box_class_repeats=3,\n        backbone_args=dict(drop_path_rate=0.2),\n        url=\'https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d0-d92fd44f.pth\',\n    ),\n    tf_efficientdet_d1=dict(\n        name=\'tf_efficientdet_d1\',\n        backbone_name=\'tf_efficientnet_b1\',\n        image_size=640,\n        fpn_channels=88,\n        fpn_cell_repeats=4,\n        box_class_repeats=3,\n        backbone_args=dict(drop_path_rate=0.2),\n        url=\'https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d1-4c7ebaf2.pth\'\n    ),\n    tf_efficientdet_d2=dict(\n        name=\'tf_efficientdet_d2\',\n        backbone_name=\'tf_efficientnet_b2\',\n        image_size=768,\n        fpn_channels=112,\n        fpn_cell_repeats=5,\n        box_class_repeats=3,\n        backbone_args=dict(drop_path_rate=0.2),\n        url=\'https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d2-cb4ce77d.pth\',\n    ),\n    tf_efficientdet_d3=dict(\n        name=\'tf_efficientdet_d3\',\n        backbone_name=\'tf_efficientnet_b3\',\n        image_size=896,\n        fpn_channels=160,\n        fpn_cell_repeats=6,\n        box_class_repeats=4,\n        backbone_args=dict(drop_path_rate=0.2),\n        url=\'https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d3-b0ea2cbc.pth\',\n    ),\n    tf_efficientdet_d4=dict(\n        name=\'tf_efficientdet_d4\',\n        backbone_name=\'tf_efficientnet_b4\',\n        image_size=1024,\n        fpn_channels=224,\n        fpn_cell_repeats=7,\n        box_class_repeats=4,\n        backbone_args=dict(drop_path_rate=0.2),\n        url=\'https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d4-5b370b7a.pth\',\n    ),\n    tf_efficientdet_d5=dict(\n        name=\'tf_efficientdet_d5\',\n        backbone_name=\'tf_efficientnet_b5\',\n        image_size=1280,\n        fpn_channels=288,\n        fpn_cell_repeats=7,\n        box_class_repeats=4,\n        backbone_args=dict(drop_path_rate=0.2),\n        url=\'https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d5-ef44aea8.pth\',\n    ),\n    tf_efficientdet_d6=dict(\n        name=\'tf_efficientdet_d6\',\n        backbone_name=\'tf_efficientnet_b6\',\n        image_size=1280,\n        fpn_channels=384,\n        fpn_cell_repeats=8,\n        box_class_repeats=5,\n        fpn_name=\'bifpn_sum\',  # Use unweighted sum for training stability.\n        backbone_args=dict(drop_path_rate=0.2),\n        url=\'https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d6-51cb0132.pth\'\n    ),\n    tf_efficientdet_d7=dict(\n        name=\'tf_efficientdet_d7\',\n        backbone_name=\'tf_efficientnet_b6\',\n        image_size=1536,\n        fpn_channels=384,\n        fpn_cell_repeats=8,\n        box_class_repeats=5,\n        anchor_scale=5.0,\n        fpn_name=\'bifpn_sum\',  # Use unweighted sum for training stability.\n        backbone_args=dict(drop_path_rate=0.2),\n        url=\'https://github.com/rwightman/efficientdet-pytorch/releases/download/v0.1/tf_efficientdet_d7-f05bf714.pth\'\n    ),\n)\n\n\ndef get_efficientdet_config(model_name=\'tf_efficientdet_d1\'):\n    """"""Get the default config for EfficientDet based on model name.""""""\n    h = default_detection_model_configs()\n    h.update(efficientdet_model_param_dict[model_name])\n    return h\n\n\ndef bifpn_sum_config(base_reduction=8):\n    """"""BiFPN config with sum.""""""\n    p = OmegaConf.create()\n    p.nodes = [\n        {\'reduction\': base_reduction << 3, \'inputs_offsets\': [3, 4]},\n        {\'reduction\': base_reduction << 2, \'inputs_offsets\': [2, 5]},\n        {\'reduction\': base_reduction << 1, \'inputs_offsets\': [1, 6]},\n        {\'reduction\': base_reduction, \'inputs_offsets\': [0, 7]},\n        {\'reduction\': base_reduction << 1, \'inputs_offsets\': [1, 7, 8]},\n        {\'reduction\': base_reduction << 2, \'inputs_offsets\': [2, 6, 9]},\n        {\'reduction\': base_reduction << 3, \'inputs_offsets\': [3, 5, 10]},\n        {\'reduction\': base_reduction << 4, \'inputs_offsets\': [4, 11]},\n    ]\n    p.weight_method = \'sum\'\n    return p\n\n\ndef bifpn_attn_config():\n    """"""BiFPN config with fast weighted sum.""""""\n    p = bifpn_sum_config()\n    p.weight_method = \'attn\'\n    return p\n\n\ndef bifpn_fa_config():\n    """"""BiFPN config with fast weighted sum.""""""\n    p = bifpn_sum_config()\n    p.weight_method = \'fastattn\'\n    return p\n\n\ndef get_fpn_config(fpn_name):\n    if not fpn_name:\n        fpn_name = \'bifpn_fa\'\n    name_to_config = {\n        \'bifpn_sum\': bifpn_sum_config(),\n        \'bifpn_attn\': bifpn_attn_config(),\n        \'bifpn_fa\': bifpn_fa_config(),\n    }\n    return name_to_config[fpn_name]\n'"
effdet/config/train_config.py,0,"b""from omegaconf import OmegaConf\n\n\ndef default_detection_train_config():\n    # FIXME currently using args for train config, will revisit, perhaps move to Hydra\n    h = OmegaConf.create()\n\n    # dataset\n    h.skip_crowd_during_training = True\n\n    # augmentation\n    h.input_rand_hflip = True\n    h.train_scale_min = 0.1\n    h.train_scale_max = 2.0\n    h.autoaugment_policy = None\n\n    # optimization\n    h.momentum = 0.9\n    h.learning_rate = 0.08\n    h.lr_warmup_init = 0.008\n    h.lr_warmup_epoch = 1.0\n    h.first_lr_drop_epoch = 200.0\n    h.second_lr_drop_epoch = 250.0\n    h.clip_gradients_norm = 10.0\n    h.num_epochs = 300\n\n    # regularization l2 loss.\n    h.weight_decay = 4e-5\n\n    h.lr_decay_method = 'cosine'\n    h.moving_average_decay = 0.9998\n    h.ckpt_var_scope = None\n\n    return h\n"""
effdet/object_detection/__init__.py,0,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n# Object detection data loaders and libraries are mostly based on RetinaNet:\n# https://github.com/tensorflow/tpu/tree/master/models/official/retinanet\nfrom .argmax_matcher import ArgMaxMatcher\nfrom .box_coder import FasterRcnnBoxCoder\nfrom .box_list import BoxList\nfrom .matcher import Match\nfrom .region_similarity_calculator import IouSimilarity\nfrom .target_assigner import TargetAssigner\n'"
effdet/object_detection/argmax_matcher.py,9,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Argmax matcher implementation.\n\nThis class takes a similarity matrix and matches columns to rows based on the\nmaximum value per column. One can specify matched_thresholds and\nto prevent columns from matching to rows (generally resulting in a negative\ntraining example) and unmatched_theshold to ignore the match (generally\nresulting in neither a positive or negative training example).\n\nThis matcher is used in Fast(er)-RCNN.\n\nNote: matchers are used in TargetAssigners. There is a create_target_assigner\nfactory function for popular implementations.\n""""""\nimport torch\nfrom torch.nn.functional import one_hot\nfrom .matcher import Match\nfrom typing import Optional\n\n\n@torch.jit.script\nclass ArgMaxMatcher(object):  # cannot inherit with torchscript\n    """"""Matcher based on highest value.\n\n    This class computes matches from a similarity matrix. Each column is matched\n    to a single row.\n\n    To support object detection target assignment this class enables setting both\n    matched_threshold (upper threshold) and unmatched_threshold (lower thresholds)\n    defining three categories of similarity which define whether examples are\n    positive, negative, or ignored:\n    (1) similarity >= matched_threshold: Highest similarity. Matched/Positive!\n    (2) matched_threshold > similarity >= unmatched_threshold: Medium similarity.\n            Depending on negatives_lower_than_unmatched, this is either\n            Unmatched/Negative OR Ignore.\n    (3) unmatched_threshold > similarity: Lowest similarity. Depending on flag\n            negatives_lower_than_unmatched, either Unmatched/Negative OR Ignore.\n    For ignored matches this class sets the values in the Match object to -2.\n    """"""\n\n    def __init__(self,\n                 matched_threshold: float,\n                 unmatched_threshold: Optional[float] = None,\n                 negatives_lower_than_unmatched: bool = True,\n                 force_match_for_each_row: bool = False):\n        """"""Construct ArgMaxMatcher.\n\n        Args:\n            matched_threshold: Threshold for positive matches. Positive if\n                sim >= matched_threshold, where sim is the maximum value of the\n                similarity matrix for a given column. Set to None for no threshold.\n            unmatched_threshold: Threshold for negative matches. Negative if\n                sim < unmatched_threshold. Defaults to matched_threshold\n                when set to None.\n            negatives_lower_than_unmatched: Boolean which defaults to True. If True\n                then negative matches are the ones below the unmatched_threshold,\n                whereas ignored matches are in between the matched and unmatched\n                threshold. If False, then negative matches are in between the matched\n                and unmatched threshold, and everything lower than unmatched is ignored.\n            force_match_for_each_row: If True, ensures that each row is matched to\n                at least one column (which is not guaranteed otherwise if the\n                matched_threshold is high). Defaults to False. See\n                argmax_matcher_test.testMatcherForceMatch() for an example.\n\n        Raises:\n            ValueError: if unmatched_threshold is set but matched_threshold is not set\n                or if unmatched_threshold > matched_threshold.\n        """"""\n        if (matched_threshold is None) and (unmatched_threshold is not None):\n            raise ValueError(\'Need to also define matched_threshold when unmatched_threshold is defined\')\n        self._matched_threshold = matched_threshold\n        self._unmatched_threshold: float = 0.\n        if unmatched_threshold is None:\n            self._unmatched_threshold = matched_threshold\n        else:\n            if unmatched_threshold > matched_threshold:\n                raise ValueError(\'unmatched_threshold needs to be smaller or equal to matched_threshold\')\n            self._unmatched_threshold = unmatched_threshold\n        if not negatives_lower_than_unmatched:\n            if self._unmatched_threshold == self._matched_threshold:\n                raise ValueError(\'When negatives are in between matched and unmatched thresholds, these \'\n                                 \'cannot be of equal value. matched: %s, unmatched: %s\',\n                                 self._matched_threshold, self._unmatched_threshold)\n        self._force_match_for_each_row = force_match_for_each_row\n        self._negatives_lower_than_unmatched = negatives_lower_than_unmatched\n\n    def _match_when_rows_are_empty(self, similarity_matrix):\n        """"""Performs matching when the rows of similarity matrix are empty.\n\n        When the rows are empty, all detections are false positives. So we return\n        a tensor of -1\'s to indicate that the columns do not match to any rows.\n\n        Returns:\n            matches:  int32 tensor indicating the row each column matches to.\n        """"""\n        return -1 * torch.ones(similarity_matrix.shape[1], dtype=torch.long)\n\n    def _match_when_rows_are_non_empty(self, similarity_matrix):\n        """"""Performs matching when the rows of similarity matrix are non empty.\n\n        Returns:\n            matches:  int32 tensor indicating the row each column matches to.\n        """"""\n        # Matches for each column\n        matches = torch.argmax(similarity_matrix, 0)\n\n        # Deal with matched and unmatched threshold\n        if self._matched_threshold is not None:\n            # Get logical indices of ignored and unmatched columns as tf.int64\n            matched_vals = torch.max(similarity_matrix, 0)[0]\n            below_unmatched_threshold = self._unmatched_threshold > matched_vals\n            between_thresholds = (matched_vals >= self._unmatched_threshold) & \\\n                                 (self._matched_threshold > matched_vals)\n\n            if self._negatives_lower_than_unmatched:\n                matches = self._set_values_using_indicator(matches, below_unmatched_threshold, -1)\n                matches = self._set_values_using_indicator(matches, between_thresholds, -2)\n            else:\n                matches = self._set_values_using_indicator(matches, below_unmatched_threshold, -2)\n                matches = self._set_values_using_indicator(matches, between_thresholds, -1)\n\n        if self._force_match_for_each_row:\n            force_match_column_ids = torch.argmax(similarity_matrix, 1)\n            force_match_column_indicators = one_hot(force_match_column_ids, similarity_matrix.shape[1])\n            force_match_row_ids = torch.argmax(force_match_column_indicators, 0)\n            force_match_column_mask = torch.max(force_match_column_indicators, 0)[0] != 0\n            final_matches = torch.where(force_match_column_mask, force_match_row_ids, matches)\n            return final_matches\n        else:\n            return matches\n\n    def match(self, similarity_matrix):\n        """"""Tries to match each column of the similarity matrix to a row.\n\n        Args:\n            similarity_matrix: tensor of shape [N, M] representing any similarity metric.\n\n        Returns:\n            Match object with corresponding matches for each of M columns.\n        """"""\n        if similarity_matrix.shape[0] == 0:\n            return Match(self._match_when_rows_are_empty(similarity_matrix))\n        else:\n            return Match(self._match_when_rows_are_non_empty(similarity_matrix))\n\n    def _set_values_using_indicator(self, x, indicator, val: int):\n        """"""Set the indicated fields of x to val.\n\n        Args:\n            x: tensor.\n            indicator: boolean with same shape as x.\n            val: scalar with value to set.\n\n        Returns:\n            modified tensor.\n        """"""\n        indicator = indicator.to(dtype=x.dtype)\n        return x * (1 - indicator) + val * indicator\n'"
effdet/object_detection/box_coder.py,8,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Base box coder.\n\nBox coders convert between coordinate frames, namely image-centric\n(with (0,0) on the top left of image) and anchor-centric (with (0,0) being\ndefined by a specific anchor).\n\nUsers of a BoxCoder can call two methods:\n encode: which encodes a box with respect to a given anchor\n  (or rather, a tensor of boxes wrt a corresponding tensor of anchors) and\n decode: which inverts this encoding with a decode operation.\nIn both cases, the arguments are assumed to be in 1-1 correspondence already;\nit is not the job of a BoxCoder to perform matching.\n""""""\nimport torch\nfrom typing import List, Optional\nfrom .box_list import BoxList\n\n# Box coder types.\nFASTER_RCNN = \'faster_rcnn\'\nKEYPOINT = \'keypoint\'\nMEAN_STDDEV = \'mean_stddev\'\nSQUARE = \'square\'\n\n\n""""""Faster RCNN box coder.\n\nFaster RCNN box coder follows the coding schema described below:\n  ty = (y - ya) / ha\n  tx = (x - xa) / wa\n  th = log(h / ha)\n  tw = log(w / wa)\n  where x, y, w, h denote the box\'s center coordinates, width and height\n  respectively. Similarly, xa, ya, wa, ha denote the anchor\'s center\n  coordinates, width and height. tx, ty, tw and th denote the anchor-encoded\n  center, width and height respectively.\n\n  See http://arxiv.org/abs/1506.01497 for details.\n""""""\n\n\nEPS = 1e-8\n\n\n#@torch.jit.script\nclass FasterRcnnBoxCoder(object):\n    """"""Faster RCNN box coder.""""""\n\n    def __init__(self, scale_factors: Optional[List[float]] = None, eps: float = EPS):\n        """"""Constructor for FasterRcnnBoxCoder.\n\n        Args:\n            scale_factors: List of 4 positive scalars to scale ty, tx, th and tw.\n                If set to None, does not perform scaling. For Faster RCNN,\n                the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0].\n        """"""\n        self._scale_factors = scale_factors\n        if scale_factors is not None:\n            assert len(scale_factors) == 4\n            for scalar in scale_factors:\n                assert scalar > 0\n        self.eps = eps\n\n    #@property\n    def code_size(self):\n        return 4\n\n    def encode(self, boxes: BoxList, anchors: BoxList):\n        """"""Encode a box collection with respect to anchor collection.\n\n        Args:\n            boxes: BoxList holding N boxes to be encoded.\n            anchors: BoxList of anchors.\n\n        Returns:\n            a tensor representing N anchor-encoded boxes of the format [ty, tx, th, tw].\n        """"""\n        # Convert anchors to the center coordinate representation.\n        ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()\n        ycenter, xcenter, h, w = boxes.get_center_coordinates_and_sizes()\n        # Avoid NaN in division and log below.\n        ha += self.eps\n        wa += self.eps\n        h += self.eps\n        w += self.eps\n\n        tx = (xcenter - xcenter_a) / wa\n        ty = (ycenter - ycenter_a) / ha\n        tw = torch.log(w / wa)\n        th = torch.log(h / ha)\n        # Scales location targets as used in paper for joint training.\n        if self._scale_factors is not None:\n            ty *= self._scale_factors[0]\n            tx *= self._scale_factors[1]\n            th *= self._scale_factors[2]\n            tw *= self._scale_factors[3]\n        return torch.stack([ty, tx, th, tw]).t()\n\n    def decode(self, rel_codes, anchors: BoxList):\n        """"""Decode relative codes to boxes.\n\n        Args:\n            rel_codes: a tensor representing N anchor-encoded boxes.\n            anchors: BoxList of anchors.\n\n        Returns:\n            boxes: BoxList holding N bounding boxes.\n        """"""\n        ycenter_a, xcenter_a, ha, wa = anchors.get_center_coordinates_and_sizes()\n\n        ty, tx, th, tw = rel_codes.t().unbind()\n        if self._scale_factors is not None:\n            ty /= self._scale_factors[0]\n            tx /= self._scale_factors[1]\n            th /= self._scale_factors[2]\n            tw /= self._scale_factors[3]\n        w = torch.exp(tw) * wa\n        h = torch.exp(th) * ha\n        ycenter = ty * ha + ycenter_a\n        xcenter = tx * wa + xcenter_a\n        ymin = ycenter - h / 2.\n        xmin = xcenter - w / 2.\n        ymax = ycenter + h / 2.\n        xmax = xcenter + w / 2.\n        return BoxList(torch.stack([ymin, xmin, ymax, xmax]).t())\n\n\ndef batch_decode(encoded_boxes, box_coder: FasterRcnnBoxCoder, anchors: BoxList):\n    """"""Decode a batch of encoded boxes.\n\n    This op takes a batch of encoded bounding boxes and transforms\n    them to a batch of bounding boxes specified by their corners in\n    the order of [y_min, x_min, y_max, x_max].\n\n    Args:\n        encoded_boxes: a float32 tensor of shape [batch_size, num_anchors,\n            code_size] representing the location of the objects.\n        box_coder: a BoxCoder object.\n        anchors: a BoxList of anchors used to encode `encoded_boxes`.\n\n    Returns:\n        decoded_boxes: a float32 tensor of shape [batch_size, num_anchors, coder_size]\n            representing the corners of the objects in the order of [y_min, x_min, y_max, x_max].\n\n    Raises:\n        ValueError: if batch sizes of the inputs are inconsistent, or if\n        the number of anchors inferred from encoded_boxes and anchors are inconsistent.\n    """"""\n    assert len(encoded_boxes.shape) == 3\n    if encoded_boxes.shape[1] != anchors.num_boxes():\n        raise ValueError(\'The number of anchors inferred from encoded_boxes\'\n                         \' and anchors are inconsistent: shape[1] of encoded_boxes\'\n                         \' %s should be equal to the number of anchors: %s.\' %\n                         (encoded_boxes.shape[1], anchors.num_boxes()))\n\n    decoded_boxes = torch.stack([\n        box_coder.decode(boxes, anchors).boxes for boxes in encoded_boxes.unbind()\n    ])\n    return decoded_boxes\n'"
effdet/object_detection/box_list.py,6,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Bounding Box List definition.\n\nBoxList represents a list of bounding boxes as tensorflow\ntensors, where each bounding box is represented as a row of 4 numbers,\n[y_min, x_min, y_max, x_max].  It is assumed that all bounding boxes\nwithin a given list correspond to a single image.  See also\nbox_list_ops.py for common box related operations (such as area, iou, etc).\n\nOptionally, users can add additional related fields (such as weights).\nWe assume the following things to be true about fields:\n* they correspond to boxes in the box_list along the 0th dimension\n* they have inferable rank at graph construction time\n* all dimensions except for possibly the 0th can be inferred\n  (i.e., not None) at graph construction time.\n\nSome other notes:\n    * Following tensorflow conventions, we use height, width ordering,\n        and correspondingly, y,x (or ymin, xmin, ymax, xmax) ordering\n    * Tensors are always provided as (flat) [N, 4] tensors.\n""""""\nimport torch\nfrom typing import Optional, List, Dict\n\n\n@torch.jit.script\nclass BoxList(object):\n    """"""Box collection.""""""\n    data: Dict[str, torch.Tensor]\n\n    def __init__(self, boxes):\n        """"""Constructs box collection.\n\n        Args:\n            boxes: a tensor of shape [N, 4] representing box corners\n\n        Raises:\n            ValueError: if invalid dimensions for bbox data or if bbox data is not in float32 format.\n        """"""\n        if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\n            raise ValueError(\'Invalid dimensions for box data.\')\n        if boxes.dtype != torch.float32:\n            raise ValueError(\'Invalid tensor type: should be tf.float32\')\n        self.data = {\'boxes\': boxes}\n\n    def num_boxes(self):\n        """"""Returns number of boxes held in collection.\n\n        Returns:\n          a tensor representing the number of boxes held in the collection.\n        """"""\n        return self.data[\'boxes\'].shape[0]\n\n    def get_all_fields(self):\n        """"""Returns all fields.""""""\n        return self.data.keys()\n\n    def get_extra_fields(self):\n        """"""Returns all non-box fields (i.e., everything not named \'boxes\').""""""\n        # return [k for k in self.data.keys() if k != \'boxes\']  # FIXME torscript doesn\'t support comprehensions yet\n        extra: List[str] = []\n        for k in self.data.keys():\n            if k != \'boxes\':\n                extra.append(k)\n        return extra\n\n    def add_field(self, field: str, field_data: torch.Tensor):\n        """"""Add field to box list.\n\n        This method can be used to add related box data such as weights/labels, etc.\n\n        Args:\n            field: a string key to access the data via `get`\n            field_data: a tensor containing the data to store in the BoxList\n        """"""\n        self.data[field] = field_data\n\n    def has_field(self, field: str):\n        return field in self.data\n\n    #@property  # FIXME for torchscript compat\n    def boxes(self):\n        """"""Convenience function for accessing box coordinates.\n\n        Returns:\n            a tensor with shape [N, 4] representing box coordinates.\n        """"""\n        return self.get_field(\'boxes\')\n\n    #@boxes.setter  # FIXME for torchscript compat\n    def set_boxes(self, boxes):\n        """"""Convenience function for setting box coordinates.\n\n        Args:\n            boxes: a tensor of shape [N, 4] representing box corners\n\n        Raises:\n            ValueError: if invalid dimensions for bbox data\n        """"""\n        if len(boxes.shape) != 2 or boxes.shape[-1] != 4:\n            raise ValueError(\'Invalid dimensions for box data.\')\n        self.data[\'boxes\'] = boxes\n\n    def get_field(self, field: str):\n        """"""Accesses a box collection and associated fields.\n\n        This function returns specified field with object; if no field is specified,\n        it returns the box coordinates.\n\n        Args:\n            field: this optional string parameter can be used to specify a related field to be accessed.\n\n        Returns:\n            a tensor representing the box collection or an associated field.\n\n        Raises:\n            ValueError: if invalid field\n        """"""\n        if not self.has_field(field):\n            raise ValueError(\'field \' + str(field) + \' does not exist\')\n        return self.data[field]\n\n    def set_field(self, field: str, value: torch.Tensor):\n        """"""Sets the value of a field.\n\n        Updates the field of a box_list with a given value.\n\n        Args:\n            field: (string) name of the field to set value.\n            value: the value to assign to the field.\n\n        Raises:\n            ValueError: if the box_list does not have specified field.\n        """"""\n        if not self.has_field(field):\n            raise ValueError(\'field %s does not exist\' % field)\n        self.data[field] = value\n\n    def get_center_coordinates_and_sizes(self):\n        """"""Computes the center coordinates, height and width of the boxes.\n\n        Returns:\n            a list of 4 1-D tensors [ycenter, xcenter, height, width].\n        """"""\n        box_corners = self.boxes()\n        ymin, xmin, ymax, xmax = box_corners.t().unbind()\n        width = xmax - xmin\n        height = ymax - ymin\n        ycenter = ymin + height / 2.\n        xcenter = xmin + width / 2.\n        return [ycenter, xcenter, height, width]\n\n    def transpose_coordinates(self):\n        """"""Transpose the coordinate representation in a boxlist.\n\n        """"""\n        y_min, x_min, y_max, x_max = self.boxes().chunk(4, dim=1)\n        self.set_boxes(torch.cat([x_min, y_min, x_max, y_max], 1))\n\n    def as_tensor_dict(self, fields: Optional[List[str]] = None):\n        """"""Retrieves specified fields as a dictionary of tensors.\n\n        Args:\n            fields: (optional) list of fields to return in the dictionary.\n                If None (default), all fields are returned.\n\n        Returns:\n            tensor_dict: A dictionary of tensors specified by fields.\n\n        Raises:\n            ValueError: if specified field is not contained in boxlist.\n        """"""\n        tensor_dict = {}\n        if fields is None:\n            fields = self.get_all_fields()\n        for field in fields:\n            if not self.has_field(field):\n                raise ValueError(\'boxlist must contain all specified fields\')\n            tensor_dict[field] = self.get_field(field)\n        return tensor_dict\n\n    #@property\n    def device(self):\n        return self.data[\'boxes\'].device\n'"
effdet/object_detection/matcher.py,12,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Matcher interface and Match class.\n\nThis module defines the Matcher interface and the Match object. The job of the\nmatcher is to match row and column indices based on the similarity matrix and\nother optional parameters. Each column is matched to at most one row. There\nare three possibilities for the matching:\n\n1) match: A column matches a row.\n2) no_match: A column does not match any row.\n3) ignore: A column that is neither \'match\' nor no_match.\n\nThe ignore case is regularly encountered in object detection: when an anchor has\na relatively small overlap with a ground-truth box, one neither wants to\nconsider this box a positive example (match) nor a negative example (no match).\n\nThe Match class is used to store the match results and it provides simple apis\nto query the results.\n""""""\nimport torch\n\n\n@torch.jit.script\nclass Match(object):\n    """"""Class to store results from the matcher.\n\n    This class is used to store the results from the matcher. It provides\n    convenient methods to query the matching results.\n    """"""\n\n    def __init__(self, match_results: torch.Tensor):\n        """"""Constructs a Match object.\n\n        Args:\n            match_results: Integer tensor of shape [N] with (1) match_results[i]>=0,\n                meaning that column i is matched with row match_results[i].\n                (2) match_results[i]=-1, meaning that column i is not matched.\n                (3) match_results[i]=-2, meaning that column i is ignored.\n\n        Raises:\n            ValueError: if match_results does not have rank 1 or is not an integer int32 scalar tensor\n        """"""\n        if len(match_results.shape) != 1:\n            raise ValueError(\'match_results should have rank 1\')\n        if match_results.dtype not in (torch.int32, torch.int64):\n            raise ValueError(\'match_results should be an int32 or int64 scalar tensor\')\n        self.match_results = match_results\n\n    def matched_column_indices(self):\n        """"""Returns column indices that match to some row.\n\n        The indices returned by this op are always sorted in increasing order.\n\n        Returns:\n            column_indices: int32 tensor of shape [K] with column indices.\n        """"""\n        return torch.nonzero(self.match_results > -1).flatten().long()\n\n    def matched_column_indicator(self):\n        """"""Returns column indices that are matched.\n\n        Returns:\n            column_indices: int32 tensor of shape [K] with column indices.\n        """"""\n        return self.match_results >= 0\n\n    def num_matched_columns(self):\n        """"""Returns number (int32 scalar tensor) of matched columns.""""""\n        return self.matched_column_indices().numel()\n\n    def unmatched_column_indices(self):\n        """"""Returns column indices that do not match any row.\n\n        The indices returned by this op are always sorted in increasing order.\n\n        Returns:\n          column_indices: int32 tensor of shape [K] with column indices.\n        """"""\n        return torch.nonzero(self.match_results == -1).flatten().long()\n\n    def unmatched_column_indicator(self):\n        """"""Returns column indices that are unmatched.\n\n        Returns:\n          column_indices: int32 tensor of shape [K] with column indices.\n        """"""\n        return self.match_results == -1\n\n    def num_unmatched_columns(self):\n        """"""Returns number (int32 scalar tensor) of unmatched columns.""""""\n        return self.unmatched_column_indices().numel()\n\n    def ignored_column_indices(self):\n        """"""Returns column indices that are ignored (neither Matched nor Unmatched).\n\n        The indices returned by this op are always sorted in increasing order.\n\n        Returns:\n          column_indices: int32 tensor of shape [K] with column indices.\n        """"""\n        return torch.nonzero(self.ignored_column_indicator()).flatten().long()\n\n    def ignored_column_indicator(self):\n        """"""Returns boolean column indicator where True means the column is ignored.\n\n        Returns:\n            column_indicator: boolean vector which is True for all ignored column indices.\n        """"""\n        return self.match_results == -2\n\n    def num_ignored_columns(self):\n        """"""Returns number (int32 scalar tensor) of matched columns.""""""\n        return self.ignored_column_indices().numel()\n\n    def unmatched_or_ignored_column_indices(self):\n        """"""Returns column indices that are unmatched or ignored.\n\n        The indices returned by this op are always sorted in increasing order.\n\n        Returns:\n            column_indices: int32 tensor of shape [K] with column indices.\n        """"""\n        return torch.nonzero(0 > self.match_results).flatten().long()\n\n    def matched_row_indices(self):\n        """"""Returns row indices that match some column.\n\n        The indices returned by this op are ordered so as to be in correspondence with the output of\n        matched_column_indicator().  For example if self.matched_column_indicator() is [0,2],\n        and self.matched_row_indices() is [7, 3], then we know that column 0 was matched to row 7 and\n        column 2 was matched to row 3.\n\n        Returns:\n            row_indices: int32 tensor of shape [K] with row indices.\n        """"""\n        return torch.gather(self.match_results, 0, self.matched_column_indices()).flatten().long()\n\n    def gather_based_on_match(self, input_tensor, unmatched_value, ignored_value):\n        """"""Gathers elements from `input_tensor` based on match results.\n\n        For columns that are matched to a row, gathered_tensor[col] is set to input_tensor[match_results[col]].\n        For columns that are unmatched, gathered_tensor[col] is set to unmatched_value. Finally, for columns that\n        are ignored gathered_tensor[col] is set to ignored_value.\n\n        Note that the input_tensor.shape[1:] must match with unmatched_value.shape\n        and ignored_value.shape\n\n        Args:\n            input_tensor: Tensor to gather values from.\n            unmatched_value: Constant tensor value for unmatched columns.\n            ignored_value: Constant tensor value for ignored columns.\n\n        Returns:\n            gathered_tensor: A tensor containing values gathered from input_tensor.\n                The shape of the gathered tensor is [match_results.shape[0]] + input_tensor.shape[1:].\n        """"""\n        ss = torch.stack([ignored_value, unmatched_value])\n        input_tensor = torch.cat([ss, input_tensor], dim=0)\n        gather_indices = torch.clamp(self.match_results + 2, min=0)\n        gathered_tensor = torch.index_select(input_tensor, 0, gather_indices)\n        return gathered_tensor\n'"
effdet/object_detection/region_similarity_calculator.py,8,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Region Similarity Calculators for BoxLists.\n\nRegion Similarity Calculators compare a pairwise measure of similarity\nbetween the boxes in two BoxLists.\n""""""\nimport torch\nfrom .box_list import BoxList\n\n\ndef area(boxlist: BoxList):\n    """"""Computes area of boxes.\n\n    Args:\n        boxlist: BoxList holding N boxes\n\n    Returns:\n        a tensor with shape [N] representing box areas.\n    """"""\n    y_min, x_min, y_max, x_max = boxlist.boxes().chunk(4, dim=1)\n    out = (y_max - y_min).squeeze(1) * (x_max - x_min).squeeze(1)\n    return out\n\n\ndef intersection(boxlist1: BoxList, boxlist2: BoxList):\n    """"""Compute pairwise intersection areas between boxes.\n\n    Args:\n        boxlist1: BoxList holding N boxes\n        boxlist2: BoxList holding M boxes\n\n    Returns:\n        a tensor with shape [N, M] representing pairwise intersections\n    """"""\n    y_min1, x_min1, y_max1, x_max1 = boxlist1.boxes().chunk(4, dim=1)\n    y_min2, x_min2, y_max2, x_max2 = boxlist2.boxes().chunk(4, dim=1)\n    all_pairs_min_ymax = torch.min(y_max1, y_max2.t())\n    all_pairs_max_ymin = torch.max(y_min1, y_min2.t())\n    intersect_heights = torch.clamp(all_pairs_min_ymax - all_pairs_max_ymin, min=0)\n    all_pairs_min_xmax = torch.min(x_max1, x_max2.t())\n    all_pairs_max_xmin = torch.max(x_min1, x_min2.t())\n    intersect_widths = torch.clamp(all_pairs_min_xmax - all_pairs_max_xmin, min=0)\n    return intersect_heights * intersect_widths\n\n\ndef iou(boxlist1: BoxList, boxlist2: BoxList):\n    """"""Computes pairwise intersection-over-union between box collections.\n\n    Args:\n        boxlist1: BoxList holding N boxes\n        boxlist2: BoxList holding M boxes\n\n    Returns:\n        a tensor with shape [N, M] representing pairwise iou scores.\n    """"""\n    intersections = intersection(boxlist1, boxlist2)\n    areas1 = area(boxlist1)\n    areas2 = area(boxlist2)\n    unions = areas1.unsqueeze(1) + areas2.unsqueeze(0) - intersections\n    return torch.where(intersections == 0.0, torch.zeros_like(intersections), intersections / unions)\n\n\n@torch.jit.script\nclass IouSimilarity(object):\n    """"""Class to compute similarity based on Intersection over Union (IOU) metric.\n\n    This class computes pairwise similarity between two BoxLists based on IOU.\n    """"""\n    def __init__(self):\n        pass\n\n    def compare(self, boxlist1: BoxList, boxlist2: BoxList):\n        """"""Computes matrix of pairwise similarity between BoxLists.\n\n        This op (to be overridden) computes a measure of pairwise similarity between\n        the boxes in the given BoxLists. Higher values indicate more similarity.\n\n        Note that this method simply measures similarity and does not explicitly\n        perform a matching.\n\n        Args:\n            boxlist1: BoxList holding N boxes.\n            boxlist2: BoxList holding M boxes.\n\n        Returns:\n            a (float32) tensor of shape [N, M] with pairwise similarity score.\n        """"""\n        return iou(boxlist1, boxlist2)\n'"
effdet/object_detection/target_assigner.py,12,"b'# Copyright 2020 Google Research. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""Base target assigner module.\n\nThe job of a TargetAssigner is, for a given set of anchors (bounding boxes) and\ngroundtruth detections (bounding boxes), to assign classification and regression\ntargets to each anchor as well as weights to each anchor (specifying, e.g.,\nwhich anchors should not contribute to training loss).\n\nIt assigns classification/regression targets by performing the following steps:\n1) Computing pairwise similarity between anchors and groundtruth boxes using a\n  provided RegionSimilarity Calculator\n2) Computing a matching based on the similarity matrix using a provided Matcher\n3) Assigning regression targets based on the matching and a provided BoxCoder\n4) Assigning classification targets based on the matching and groundtruth labels\n\nNote that TargetAssigners only operate on detections from a single\nimage at a time, so any logic for applying a TargetAssigner to multiple\nimages must be handled externally.\n""""""\nimport torch\n\nfrom . import box_list\nfrom .region_similarity_calculator import IouSimilarity\nfrom .argmax_matcher import ArgMaxMatcher\nfrom .matcher import Match\nfrom .box_list import BoxList\nfrom .box_coder import FasterRcnnBoxCoder\n\nKEYPOINTS_FIELD_NAME = \'keypoints\'\n\n\n#@torch.jit.script\nclass TargetAssigner(object):\n    """"""Target assigner to compute classification and regression targets.""""""\n\n    def __init__(self, similarity_calc: IouSimilarity, matcher: ArgMaxMatcher, box_coder: FasterRcnnBoxCoder,\n                 negative_class_weight: float = 1.0, unmatched_cls_target=None,\n                 keypoints_field_name: str = KEYPOINTS_FIELD_NAME):\n        """"""Construct Object Detection Target Assigner.\n\n        Args:\n            similarity_calc: a RegionSimilarityCalculator\n\n            matcher: Matcher used to match groundtruth to anchors.\n\n            box_coder: BoxCoder used to encode matching groundtruth boxes with respect to anchors.\n\n            negative_class_weight: classification weight to be associated to negative\n                anchors (default: 1.0). The weight must be in [0., 1.].\n\n            unmatched_cls_target: a float32 tensor with shape [d_1, d_2, ..., d_k]\n                which is consistent with the classification target for each\n                anchor (and can be empty for scalar targets).  This shape must thus be\n                compatible with the groundtruth labels that are passed to the ""assign""\n                function (which have shape [num_gt_boxes, d_1, d_2, ..., d_k]).\n                If set to None, unmatched_cls_target is set to be [0] for each anchor.\n\n        Raises:\n            ValueError: if similarity_calc is not a RegionSimilarityCalculator or\n                if matcher is not a Matcher or if box_coder is not a BoxCoder\n        """"""\n        self._similarity_calc = similarity_calc\n        self._matcher = matcher\n        self._box_coder = box_coder\n        self._negative_class_weight = negative_class_weight\n        self._unmatched_cls_target = unmatched_cls_target\n        self._keypoints_field_name = keypoints_field_name\n\n    def assign(self, anchors: BoxList, groundtruth_boxes: BoxList, groundtruth_labels=None, groundtruth_weights=None):\n        """"""Assign classification and regression targets to each anchor.\n\n        For a given set of anchors and groundtruth detections, match anchors\n        to groundtruth_boxes and assign classification and regression targets to\n        each anchor as well as weights based on the resulting match (specifying,\n        e.g., which anchors should not contribute to training loss).\n\n        Anchors that are not matched to anything are given a classification target\n        of self._unmatched_cls_target which can be specified via the constructor.\n\n        Args:\n            anchors: a BoxList representing N anchors\n\n            groundtruth_boxes: a BoxList representing M groundtruth boxes\n\n            groundtruth_labels:  a tensor of shape [M, d_1, ... d_k]\n                with labels for each of the ground_truth boxes. The subshape\n                [d_1, ... d_k] can be empty (corresponding to scalar inputs).  When set\n                to None, groundtruth_labels assumes a binary problem where all\n                ground_truth boxes get a positive label (of 1).\n\n            groundtruth_weights: a float tensor of shape [M] indicating the weight to\n                assign to all anchors match to a particular groundtruth box. The weights\n                must be in [0., 1.]. If None, all weights are set to 1.\n\n            **params: Additional keyword arguments for specific implementations of the Matcher.\n\n        Returns:\n            cls_targets: a float32 tensor with shape [num_anchors, d_1, d_2 ... d_k],\n                where the subshape [d_1, ..., d_k] is compatible with groundtruth_labels\n                which has shape [num_gt_boxes, d_1, d_2, ... d_k].\n\n            cls_weights: a float32 tensor with shape [num_anchors]\n\n            reg_targets: a float32 tensor with shape [num_anchors, box_code_dimension]\n\n            reg_weights: a float32 tensor with shape [num_anchors]\n\n            match: a matcher.Match object encoding the match between anchors and groundtruth boxes,\n                with rows corresponding to groundtruth boxes and columns corresponding to anchors.\n\n        Raises:\n            ValueError: if anchors or groundtruth_boxes are not of type box_list.BoxList\n        """"""\n        if not isinstance(anchors, box_list.BoxList):\n            raise ValueError(\'anchors must be an BoxList\')\n        if not isinstance(groundtruth_boxes, box_list.BoxList):\n            raise ValueError(\'groundtruth_boxes must be an BoxList\')\n\n        device = anchors.device()\n\n        if groundtruth_labels is None:\n            groundtruth_labels = torch.ones(groundtruth_boxes.num_boxes(), device=device).unsqueeze(0)\n            groundtruth_labels = groundtruth_labels.unsqueeze(-1)\n\n        if groundtruth_weights is None:\n            num_gt_boxes = groundtruth_boxes.num_boxes()\n            if not num_gt_boxes:\n                num_gt_boxes = groundtruth_boxes.num_boxes()\n            groundtruth_weights = torch.ones([num_gt_boxes], device=device)\n\n        match_quality_matrix = self._similarity_calc.compare(groundtruth_boxes, anchors)\n        match = self._matcher.match(match_quality_matrix)\n        reg_targets = self._create_regression_targets(anchors, groundtruth_boxes, match)\n        cls_targets = self._create_classification_targets(groundtruth_labels, match)\n        reg_weights = self._create_regression_weights(match, groundtruth_weights)\n        cls_weights = self._create_classification_weights(match, groundtruth_weights)\n\n        return cls_targets, cls_weights, reg_targets, reg_weights, match\n\n    def _create_regression_targets(self, anchors: BoxList, groundtruth_boxes: BoxList, match: Match):\n        """"""Returns a regression target for each anchor.\n\n        Args:\n            anchors: a BoxList representing N anchors\n\n            groundtruth_boxes: a BoxList representing M groundtruth_boxes\n\n            match: a matcher.Match object\n\n        Returns:\n            reg_targets: a float32 tensor with shape [N, box_code_dimension]\n        """"""\n        device = anchors.device()\n        zero_box = torch.zeros(4, device=device)\n        matched_gt_boxes = match.gather_based_on_match(\n            groundtruth_boxes.boxes(), unmatched_value=zero_box, ignored_value=zero_box)\n        matched_gt_boxlist = box_list.BoxList(matched_gt_boxes)\n        if groundtruth_boxes.has_field(self._keypoints_field_name):\n            groundtruth_keypoints = groundtruth_boxes.get_field(self._keypoints_field_name)\n            zero_kp = torch.zeros(groundtruth_keypoints.shape[1:], device=device)\n            matched_keypoints = match.gather_based_on_match(\n                groundtruth_keypoints, unmatched_value=zero_kp, ignored_value=zero_kp)\n            matched_gt_boxlist.add_field(self._keypoints_field_name, matched_keypoints)\n        matched_reg_targets = self._box_coder.encode(matched_gt_boxlist, anchors)\n\n        unmatched_ignored_reg_targets = self._default_regression_target(device).repeat(match.match_results.shape[0], 1)\n\n        matched_anchors_mask = match.matched_column_indicator()\n        reg_targets = torch.where(matched_anchors_mask.unsqueeze(1), matched_reg_targets, unmatched_ignored_reg_targets)\n        return reg_targets\n\n    def _default_regression_target(self, device: torch.device):\n        """"""Returns the default target for anchors to regress to.\n\n        Default regression targets are set to zero (though in this implementation what\n        these targets are set to should not matter as the regression weight of any box\n        set to regress to the default target is zero).\n\n        Returns:\n            default_target: a float32 tensor with shape [1, box_code_dimension]\n        """"""\n        return torch.zeros(1, self._box_coder.code_size(), device=device)\n\n    def _create_classification_targets(self, groundtruth_labels, match: Match):\n        """"""Create classification targets for each anchor.\n\n        Assign a classification target of for each anchor to the matching\n        groundtruth label that is provided by match.  Anchors that are not matched\n        to anything are given the target self._unmatched_cls_target\n\n        Args:\n            groundtruth_labels:  a tensor of shape [num_gt_boxes, d_1, ... d_k]\n                with labels for each of the ground_truth boxes. The subshape\n                [d_1, ... d_k] can be empty (corresponding to scalar labels).\n            match: a matcher.Match object that provides a matching between anchors\n                and groundtruth boxes.\n\n        Returns:\n            a float32 tensor with shape [num_anchors, d_1, d_2 ... d_k], where the\n            subshape [d_1, ..., d_k] is compatible with groundtruth_labels which has\n            shape [num_gt_boxes, d_1, d_2, ... d_k].\n        """"""\n        if self._unmatched_cls_target is not None:\n            uct = self._unmatched_cls_target\n        else:\n            uct = torch.scalar_tensor(0, device=groundtruth_labels.device)\n        return match.gather_based_on_match(groundtruth_labels, unmatched_value=uct, ignored_value=uct)\n\n    def _create_regression_weights(self, match: Match, groundtruth_weights):\n        """"""Set regression weight for each anchor.\n\n        Only positive anchors are set to contribute to the regression loss, so this\n        method returns a weight of 1 for every positive anchor and 0 for every\n        negative anchor.\n\n        Args:\n            match: a matcher.Match object that provides a matching between anchors and groundtruth boxes.\n            groundtruth_weights: a float tensor of shape [M] indicating the weight to\n                assign to all anchors match to a particular groundtruth box.\n\n        Returns:\n            a float32 tensor with shape [num_anchors] representing regression weights.\n        """"""\n        zs = torch.scalar_tensor(0, device=groundtruth_weights.device)\n        return match.gather_based_on_match(groundtruth_weights, ignored_value=zs, unmatched_value=zs)\n\n    def _create_classification_weights(self, match: Match, groundtruth_weights):\n        """"""Create classification weights for each anchor.\n\n        Positive (matched) anchors are associated with a weight of\n        positive_class_weight and negative (unmatched) anchors are associated with\n        a weight of negative_class_weight. When anchors are ignored, weights are set\n        to zero. By default, both positive/negative weights are set to 1.0,\n        but they can be adjusted to handle class imbalance (which is almost always\n        the case in object detection).\n\n        Args:\n            match: a matcher.Match object that provides a matching between anchors and groundtruth boxes.\n            groundtruth_weights: a float tensor of shape [M] indicating the weight to\n                assign to all anchors match to a particular groundtruth box.\n\n        Returns:\n            a float32 tensor with shape [num_anchors] representing classification weights.\n        """"""\n        ignored = torch.scalar_tensor(0, device=groundtruth_weights.device)\n        ncw = torch.scalar_tensor(self._negative_class_weight, device=groundtruth_weights.device)\n        return match.gather_based_on_match(groundtruth_weights, ignored_value=ignored, unmatched_value=ncw)\n\n    def box_coder(self):\n        """"""Get BoxCoder of this TargetAssigner.\n\n        Returns:\n            BoxCoder object.\n        """"""\n        return self._box_coder\n'"
