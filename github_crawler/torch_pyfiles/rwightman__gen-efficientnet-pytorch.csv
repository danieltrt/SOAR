file_path,api_count,code
caffe2_benchmark.py,0,"b'"""""" Caffe2 validation script\nThis script runs Caffe2 benchmark on exported model.\n""""""\nimport argparse\nfrom caffe2.python import core, workspace, model_helper\nfrom caffe2.proto import caffe2_pb2\n\n\nparser = argparse.ArgumentParser(description=\'Caffe2 Model Benchmark\')\nparser.add_argument(\'--c2-init\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--c2-predict\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=1, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 1)\')\nparser.add_argument(\'--img-size\', default=224, type=int,\n                    metavar=\'N\', help=\'Input image dimension, uses model default if empty\')\n\n\ndef main():\n    args = parser.parse_args()\n    args.gpu_id = 0\n\n    model = model_helper.ModelHelper(name=""le_net"", init_params=False)\n\n    # Bring in the init net from init_net.pb\n    init_net_proto = caffe2_pb2.NetDef()\n    with open(args.c2_init, ""rb"") as f:\n        init_net_proto.ParseFromString(f.read())\n    model.param_init_net = core.Net(init_net_proto)  # model.param_init_net.AppendNet(core.Net(init_net_proto)) #\n\n    # bring in the predict net from predict_net.pb\n    predict_net_proto = caffe2_pb2.NetDef()\n    with open(args.c2_predict, ""rb"") as f:\n        predict_net_proto.ParseFromString(f.read())\n    model.net = core.Net(predict_net_proto)  # model.net.AppendNet(core.Net(predict_net_proto))\n\n    # CUDA performance not impressive\n    #device_opts = core.DeviceOption(caffe2_pb2.PROTO_CUDA, args.gpu_id)\n    #model.net.RunAllOnGPU(gpu_id=args.gpu_id, use_cudnn=True)\n    #model.param_init_net.RunAllOnGPU(gpu_id=args.gpu_id, use_cudnn=True)\n\n    input_blob = model.net.external_inputs[0]\n    model.param_init_net.GaussianFill(\n        [],\n        input_blob.GetUnscopedName(),\n        shape=(args.batch_size, 3, args.img_size, args.img_size),\n        mean=0.0,\n        std=1.0)\n    workspace.RunNetOnce(model.param_init_net)\n    workspace.CreateNet(model.net, overwrite=True)\n    workspace.BenchmarkNet(model.net.Proto().name, 5, 20, True)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
caffe2_validate.py,0,"b'"""""" Caffe2 validation script\nThis script is intended to verify exported models running in Caffe2\nIt utilizes the same PyTorch dataloader/processing pipeline for comparison against\nthe originals, I also have no desire to write that code in Caffe2.\n""""""\nimport argparse\nimport numpy as np\nfrom caffe2.python import core, workspace, model_helper\nfrom caffe2.proto import caffe2_pb2\nfrom data import create_loader, resolve_data_config, Dataset\nfrom utils import AverageMeter\nimport time\n\nparser = argparse.ArgumentParser(description=\'Caffe2 ImageNet Validation\')\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--model\', \'-m\', metavar=\'MODEL\', default=\'spnasnet1_00\',\n                    help=\'model architecture (default: dpn92)\')\nparser.add_argument(\'--c2-init\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--c2-predict\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'-j\', \'--workers\', default=2, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 2)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=256, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 256)\')\nparser.add_argument(\'--img-size\', default=None, type=int,\n                    metavar=\'N\', help=\'Input image dimension, uses model default if empty\')\nparser.add_argument(\'--mean\', type=float, nargs=\'+\', default=None, metavar=\'MEAN\',\n                    help=\'Override mean pixel value of dataset\')\nparser.add_argument(\'--std\', type=float,  nargs=\'+\', default=None, metavar=\'STD\',\n                    help=\'Override std deviation of of dataset\')\nparser.add_argument(\'--crop-pct\', type=float, default=None, metavar=\'PCT\',\n                    help=\'Override default crop pct of 0.875\')\nparser.add_argument(\'--interpolation\', default=\'\', type=str, metavar=\'NAME\',\n                    help=\'Image resize interpolation type (overrides model)\')\nparser.add_argument(\'--tf-preprocessing\', dest=\'tf_preprocessing\', action=\'store_true\',\n                    help=\'use tensorflow mnasnet preporcessing\')\nparser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                    metavar=\'N\', help=\'print frequency (default: 10)\')\n\n\ndef main():\n    args = parser.parse_args()\n    args.gpu_id = 0\n\n    model = model_helper.ModelHelper(name=""validation_net"", init_params=False)\n\n    # Bring in the init net from init_net.pb\n    init_net_proto = caffe2_pb2.NetDef()\n    with open(args.c2_init, ""rb"") as f:\n        init_net_proto.ParseFromString(f.read())\n    model.param_init_net = core.Net(init_net_proto)  # model.param_init_net.AppendNet(core.Net(init_net_proto)) #\n\n    # bring in the predict net from predict_net.pb\n    predict_net_proto = caffe2_pb2.NetDef()\n    with open(args.c2_predict, ""rb"") as f:\n        predict_net_proto.ParseFromString(f.read())\n    model.net = core.Net(predict_net_proto)  # model.net.AppendNet(core.Net(predict_net_proto))\n\n    data_config = resolve_data_config(args.model, args)\n    loader = create_loader(\n        Dataset(args.data, load_bytes=args.tf_preprocessing),\n        input_size=data_config[\'input_size\'],\n        batch_size=args.batch_size,\n        use_prefetcher=False,\n        interpolation=data_config[\'interpolation\'],\n        mean=data_config[\'mean\'],\n        std=data_config[\'std\'],\n        num_workers=args.workers,\n        crop_pct=data_config[\'crop_pct\'],\n        tensorflow_preprocessing=args.tf_preprocessing)\n\n    # this is so obvious, wonderful interface </sarcasm>\n    input_blob = model.net.external_inputs[0]\n    output_blob = model.net.external_outputs[0]\n\n    if True:\n        device_opts = None\n    else:\n        # CUDA is crashing, no idea why, awesome error message, give it a try for kicks\n        device_opts = core.DeviceOption(caffe2_pb2.PROTO_CUDA, args.gpu_id)\n        model.net.RunAllOnGPU(gpu_id=args.gpu_id, use_cudnn=True)\n        model.param_init_net.RunAllOnGPU(gpu_id=args.gpu_id, use_cudnn=True)\n\n    model.param_init_net.GaussianFill(\n        [], input_blob.GetUnscopedName(),\n        shape=(1,) + data_config[\'input_size\'], mean=0.0, std=1.0)\n    workspace.RunNetOnce(model.param_init_net)\n    workspace.CreateNet(model.net, overwrite=True)\n\n    batch_time = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    end = time.time()\n    for i, (input, target) in enumerate(loader):\n        # run the net and return prediction\n        caffe2_in = input.data.numpy()\n        workspace.FeedBlob(input_blob, caffe2_in, device_opts)\n        workspace.RunNet(model.net, num_iter=1)\n        output = workspace.FetchBlob(output_blob)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy_np(output.data, target.numpy())\n        top1.update(prec1.item(), input.size(0))\n        top5.update(prec5.item(), input.size(0))\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print(\'Test: [{0}/{1}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f}, {rate_avg:.3f}/s, {ms_avg:.3f} ms/sample) \\t\'\n                  \'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t\'\n                  \'Prec@5 {top5.val:.3f} ({top5.avg:.3f})\'.format(\n                i, len(loader), batch_time=batch_time, rate_avg=input.size(0) / batch_time.avg,\n                ms_avg=100 * batch_time.avg / input.size(0), top1=top1, top5=top5))\n\n    print(\' * Prec@1 {top1.avg:.3f} ({top1a:.3f}) Prec@5 {top5.avg:.3f} ({top5a:.3f})\'.format(\n        top1=top1, top1a=100-top1.avg, top5=top5, top5a=100.-top5.avg))\n\n\ndef accuracy_np(output, target):\n    max_indices = np.argsort(output, axis=1)[:, ::-1]\n    top5 = 100 * np.equal(max_indices[:, :5], target[:, np.newaxis]).sum(axis=1).mean()\n    top1 = 100 * np.equal(max_indices[:, 0], target).mean()\n    return top1, top5\n\n\nif __name__ == \'__main__\':\n    main()\n'"
hubconf.py,0,"b""dependencies = ['torch', 'math']\n\nfrom geffnet import efficientnet_b0\nfrom geffnet import efficientnet_b1\nfrom geffnet import efficientnet_b2\nfrom geffnet import efficientnet_b3\n\nfrom geffnet import efficientnet_es\n\nfrom geffnet import mixnet_s\nfrom geffnet import mixnet_m\nfrom geffnet import mixnet_l\nfrom geffnet import mixnet_xl\n\nfrom geffnet import mobilenetv2_100\nfrom geffnet import mobilenetv2_110d\nfrom geffnet import mobilenetv2_120d\nfrom geffnet import mobilenetv2_140\n\nfrom geffnet import mobilenetv3_large_100\nfrom geffnet import mobilenetv3_rw\nfrom geffnet import mnasnet_a1\nfrom geffnet import mnasnet_b1\nfrom geffnet import fbnetc_100\nfrom geffnet import spnasnet_100\n\nfrom geffnet import tf_efficientnet_b0\nfrom geffnet import tf_efficientnet_b1\nfrom geffnet import tf_efficientnet_b2\nfrom geffnet import tf_efficientnet_b3\nfrom geffnet import tf_efficientnet_b4\nfrom geffnet import tf_efficientnet_b5\nfrom geffnet import tf_efficientnet_b6\nfrom geffnet import tf_efficientnet_b7\nfrom geffnet import tf_efficientnet_b8\n\nfrom geffnet import tf_efficientnet_b0_ap\nfrom geffnet import tf_efficientnet_b1_ap\nfrom geffnet import tf_efficientnet_b2_ap\nfrom geffnet import tf_efficientnet_b3_ap\nfrom geffnet import tf_efficientnet_b4_ap\nfrom geffnet import tf_efficientnet_b5_ap\nfrom geffnet import tf_efficientnet_b6_ap\nfrom geffnet import tf_efficientnet_b7_ap\nfrom geffnet import tf_efficientnet_b8_ap\n\nfrom geffnet import tf_efficientnet_b0_ns\nfrom geffnet import tf_efficientnet_b1_ns\nfrom geffnet import tf_efficientnet_b2_ns\nfrom geffnet import tf_efficientnet_b3_ns\nfrom geffnet import tf_efficientnet_b4_ns\nfrom geffnet import tf_efficientnet_b5_ns\nfrom geffnet import tf_efficientnet_b6_ns\nfrom geffnet import tf_efficientnet_b7_ns\nfrom geffnet import tf_efficientnet_l2_ns_475\nfrom geffnet import tf_efficientnet_l2_ns\n\nfrom geffnet import tf_efficientnet_es\nfrom geffnet import tf_efficientnet_em\nfrom geffnet import tf_efficientnet_el\n\nfrom geffnet import tf_efficientnet_cc_b0_4e\nfrom geffnet import tf_efficientnet_cc_b0_8e\nfrom geffnet import tf_efficientnet_cc_b1_8e\n\nfrom geffnet import tf_efficientnet_lite0\nfrom geffnet import tf_efficientnet_lite1\nfrom geffnet import tf_efficientnet_lite2\nfrom geffnet import tf_efficientnet_lite3\nfrom geffnet import tf_efficientnet_lite4\n\nfrom geffnet import tf_mixnet_s\nfrom geffnet import tf_mixnet_m\nfrom geffnet import tf_mixnet_l\n\nfrom geffnet import tf_mobilenetv3_large_075\nfrom geffnet import tf_mobilenetv3_large_100\nfrom geffnet import tf_mobilenetv3_large_minimal_100\nfrom geffnet import tf_mobilenetv3_small_075\nfrom geffnet import tf_mobilenetv3_small_100\nfrom geffnet import tf_mobilenetv3_small_minimal_100\n"""
onnx_export.py,5,"b'import argparse\nimport os\nimport time\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nimport onnx\nimport caffe2.python.onnx.backend as onnx_caffe2\n\nimport geffnet\n\ntorch.backends.cudnn.benchmark = True\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Validation\')\nparser.add_argument(\'output\', metavar=\'ONNX_FILE\',\n                    help=\'output model filename\')\nparser.add_argument(\'--model\', \'-m\', metavar=\'MODEL\', default=\'spnasnet1_00\',\n                    help=\'model architecture (default: dpn92)\')\nparser.add_argument(\'--img-size\', default=None, type=int,\n                    metavar=\'N\', help=\'Input image dimension, uses model default if empty\')\nparser.add_argument(\'--mean\', type=float, nargs=\'+\', default=None, metavar=\'MEAN\',\n                    help=\'Override mean pixel value of dataset\')\nparser.add_argument(\'--std\', type=float,  nargs=\'+\', default=None, metavar=\'STD\',\n                    help=\'Override std deviation of of dataset\')\nparser.add_argument(\'--num-classes\', type=int, default=1000,\n                    help=\'Number classes in dataset\')\nparser.add_argument(\'--checkpoint\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\n\n\ndef main():\n    args = parser.parse_args()\n\n    if not args.checkpoint:\n        args.pretrained = True\n    else:\n        args.pretrained = False\n\n    # create model\n    geffnet.config.set_exportable(True)\n    print(""==> Creating PyTorch {} model"".format(args.model))\n    model = geffnet.create_model(\n        args.model,\n        num_classes=args.num_classes,\n        in_chans=3,\n        pretrained=args.pretrained,\n        checkpoint_path=args.checkpoint)\n\n    model.eval()\n\n    x = torch.randn((1, 3, args.img_size or 224, args.img_size or 224), requires_grad=True)\n    model(x)  # run model once before export trace\n\n    print(""==> Exporting model to ONNX format at \'{}\'"".format(args.output))\n    input_names = [""input0""]\n    output_names = [""output0""]\n    optional_args = dict(keep_initializers_as_inputs=True)  # pytorch 1.3 needs this for export to succeed\n    try:\n        torch_out = torch.onnx._export(\n            model, x, args.output, export_params=True, verbose=False,\n            input_names=input_names, output_names=output_names, **optional_args)\n    except TypeError:\n        # fallback to no keep_initializers arg for pytorch < 1.3\n        torch_out = torch.onnx._export(\n            model, x, args.output, export_params=True, verbose=False,\n            input_names=input_names, output_names=output_names)\n\n    print(""==> Loading and checking exported model from \'{}\'"".format(args.output))\n    onnx_model = onnx.load(args.output)\n    onnx.checker.check_model(onnx_model)  # assuming throw on error\n    print(""==> Passed"")\n\n    print(""==> Loading model into Caffe2 backend and comparing forward pass."".format(args.output))\n    caffe2_backend = onnx_caffe2.prepare(onnx_model)\n    B = {onnx_model.graph.input[0].name: x.data.numpy()}\n    c2_out = caffe2_backend.run(B)[0]\n    np.testing.assert_almost_equal(torch_out.data.numpy(), c2_out, decimal=5)\n    print(""==> Passed"")\n\n\nif __name__ == \'__main__\':\n    main()\n'"
onnx_optimize.py,0,"b'import argparse\n\nimport onnx\nfrom onnx import optimizer\n\n\nparser = argparse.ArgumentParser(description=""Optimize ONNX model"")\n\nparser.add_argument(""model"", help=""The ONNX model"")\nparser.add_argument(""--output"", required=True, help=""The optimized model output filename"")\n\n\ndef traverse_graph(graph, prefix=\'\'):\n    content = []\n    indent = prefix + \'  \'\n    graphs = []\n    num_nodes = 0\n    for node in graph.node:\n        pn, gs = onnx.helper.printable_node(node, indent, subgraphs=True)\n        assert isinstance(gs, list)\n        content.append(pn)\n        graphs.extend(gs)\n        num_nodes += 1\n    for g in graphs:\n        g_count, g_str = traverse_graph(g)\n        content.append(\'\\n\' + g_str)\n        num_nodes += g_count\n    return num_nodes, \'\\n\'.join(content)\n\n\ndef main():\n    args = parser.parse_args()\n    onnx_model = onnx.load(args.model)\n    num_original_nodes, original_graph_str = traverse_graph(onnx_model.graph)\n\n    # Optimizer passes to perform\n    passes = [\n        #\'eliminate_deadend\',\n        \'eliminate_identity\',\n        \'eliminate_nop_dropout\',\n        \'eliminate_nop_pad\',\n        \'eliminate_nop_transpose\',\n        \'eliminate_unused_initializer\',\n        \'extract_constant_to_initializer\',\n        \'fuse_add_bias_into_conv\',\n        \'fuse_bn_into_conv\',\n        \'fuse_consecutive_concats\',\n        \'fuse_consecutive_reduce_unsqueeze\',\n        \'fuse_consecutive_squeezes\',\n        \'fuse_consecutive_transposes\',\n        #\'fuse_matmul_add_bias_into_gemm\',\n        \'fuse_pad_into_conv\',\n        #\'fuse_transpose_into_gemm\',\n        #\'lift_lexical_references\',\n    ]\n\n    # Apply the optimization on the original serialized model\n    optimized_model = optimizer.optimize(onnx_model, passes)\n\n    num_optimized_nodes, optimzied_graph_str = traverse_graph(optimized_model.graph)\n    print(\'==> The model after optimization:\\n{}\\n\'.format(optimzied_graph_str))\n    print(\'==> The optimized model has {} nodes, the original had {}.\'.format(num_optimized_nodes, num_original_nodes))\n\n    # Save the ONNX model\n    onnx.save(optimized_model, args.output)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
onnx_to_caffe.py,0,"b'import argparse\n\nimport onnx\nfrom caffe2.python.onnx.backend import Caffe2Backend\n\n\nparser = argparse.ArgumentParser(description=""Convert ONNX to Caffe2"")\n\nparser.add_argument(""model"", help=""The ONNX model"")\nparser.add_argument(""--c2-prefix"", required=True,\n    help=""The output file prefix for the caffe2 model init and predict file. "")\n\n\ndef main():\n    args = parser.parse_args()\n    onnx_model = onnx.load(args.model)\n    caffe2_init, caffe2_predict = Caffe2Backend.onnx_graph_to_caffe2_net(onnx_model)\n    caffe2_init_str = caffe2_init.SerializeToString()\n    with open(args.c2_prefix + \'.init.pb\', ""wb"") as f:\n        f.write(caffe2_init_str)\n    caffe2_predict_str = caffe2_predict.SerializeToString()\n    with open(args.c2_prefix + \'.predict.pb\', ""wb"") as f:\n        f.write(caffe2_predict_str)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
setup.py,0,"b'"""""" Setup\n""""""\nfrom setuptools import setup, find_packages\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nexec(open(\'geffnet/version.py\').read())\nsetup(\n    name=\'geffnet\',\n    version=__version__,\n    description=\'(Generic) EfficientNets for PyTorch\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    url=\'https://github.com/rwightman/gen-efficientnet-pytorch\',\n    author=\'Ross Wightman\',\n    author_email=\'hello@rwightman.com\',\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        \'Development Status :: 3 - Alpha\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ],\n\n    # Note that this is a string of words separated by whitespace, not a list.\n    keywords=\'pytorch pretrained models efficientnet mixnet mobilenetv3 mnasnet\',\n    packages=find_packages(exclude=[\'data\']),\n    install_requires=[\'torch >= 1.2\', \'torchvision\'],\n    python_requires=\'>=3.6\',\n)\n'"
utils.py,0,"b'import os\n\n\nclass AverageMeter:\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\ndef get_outdir(path, *paths, inc=False):\n    outdir = os.path.join(path, *paths)\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n    elif inc:\n        count = 1\n        outdir_inc = outdir + \'-\' + str(count)\n        while os.path.exists(outdir_inc):\n            count = count + 1\n            outdir_inc = outdir + \'-\' + str(count)\n            assert count < 100\n        outdir = outdir_inc\n        os.makedirs(outdir)\n    return outdir\n\n'"
validate.py,7,"b""from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport os\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\n\nimport geffnet\nfrom data import Dataset, create_loader, resolve_data_config\nfrom utils import accuracy, AverageMeter\n\ntorch.backends.cudnn.benchmark = True\n\nparser = argparse.ArgumentParser(description='PyTorch ImageNet Validation')\nparser.add_argument('data', metavar='DIR',\n                    help='path to dataset')\nparser.add_argument('--model', '-m', metavar='MODEL', default='spnasnet1_00',\n                    help='model architecture (default: dpn92)')\nparser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n                    help='number of data loading workers (default: 2)')\nparser.add_argument('-b', '--batch-size', default=256, type=int,\n                    metavar='N', help='mini-batch size (default: 256)')\nparser.add_argument('--img-size', default=None, type=int,\n                    metavar='N', help='Input image dimension, uses model default if empty')\nparser.add_argument('--mean', type=float, nargs='+', default=None, metavar='MEAN',\n                    help='Override mean pixel value of dataset')\nparser.add_argument('--std', type=float,  nargs='+', default=None, metavar='STD',\n                    help='Override std deviation of of dataset')\nparser.add_argument('--crop-pct', type=float, default=None, metavar='PCT',\n                    help='Override default crop pct of 0.875')\nparser.add_argument('--interpolation', default='', type=str, metavar='NAME',\n                    help='Image resize interpolation type (overrides model)')\nparser.add_argument('--num-classes', type=int, default=1000,\n                    help='Number classes in dataset')\nparser.add_argument('--print-freq', '-p', default=10, type=int,\n                    metavar='N', help='print frequency (default: 10)')\nparser.add_argument('--checkpoint', default='', type=str, metavar='PATH',\n                    help='path to latest checkpoint (default: none)')\nparser.add_argument('--pretrained', dest='pretrained', action='store_true',\n                    help='use pre-trained model')\nparser.add_argument('--torchscript', dest='torchscript', action='store_true',\n                    help='convert model torchscript for inference')\nparser.add_argument('--num-gpu', type=int, default=1,\n                    help='Number of GPUS to use')\nparser.add_argument('--tf-preprocessing', dest='tf_preprocessing', action='store_true',\n                    help='use tensorflow mnasnet preporcessing')\nparser.add_argument('--no-cuda', dest='no_cuda', action='store_true',\n                    help='')\n\n\ndef main():\n    args = parser.parse_args()\n\n    if not args.checkpoint and not args.pretrained:\n        args.pretrained = True\n\n    if args.torchscript:\n        geffnet.config.set_scriptable(True)\n\n    # create model\n    model = geffnet.create_model(\n        args.model,\n        num_classes=args.num_classes,\n        in_chans=3,\n        pretrained=args.pretrained,\n        checkpoint_path=args.checkpoint)\n\n    if args.torchscript:\n        torch.jit.optimized_execution(True)\n        model = torch.jit.script(model)\n\n    print('Model %s created, param count: %d' %\n          (args.model, sum([m.numel() for m in model.parameters()])))\n\n    data_config = resolve_data_config(model, args)\n\n    criterion = nn.CrossEntropyLoss()\n\n    if not args.no_cuda:\n        if args.num_gpu > 1:\n            model = torch.nn.DataParallel(model, device_ids=list(range(args.num_gpu))).cuda()\n        else:\n            model = model.cuda()\n        criterion = criterion.cuda()\n\n    loader = create_loader(\n        Dataset(args.data, load_bytes=args.tf_preprocessing),\n        input_size=data_config['input_size'],\n        batch_size=args.batch_size,\n        use_prefetcher=not args.no_cuda,\n        interpolation=data_config['interpolation'],\n        mean=data_config['mean'],\n        std=data_config['std'],\n        num_workers=args.workers,\n        crop_pct=data_config['crop_pct'],\n        tensorflow_preprocessing=args.tf_preprocessing)\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    model.eval()\n    end = time.time()\n    with torch.no_grad():\n        for i, (input, target) in enumerate(loader):\n            if not args.no_cuda:\n                target = target.cuda()\n                input = input.cuda()\n\n            # compute output\n            output = model(input)\n            loss = criterion(output, target)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(output.data, target, topk=(1, 5))\n            losses.update(loss.item(), input.size(0))\n            top1.update(prec1.item(), input.size(0))\n            top5.update(prec5.item(), input.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % args.print_freq == 0:\n                print('Test: [{0}/{1}]\\t'\n                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f}, {rate_avg:.3f}/s) \\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n                      'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n                    i, len(loader), batch_time=batch_time,\n                    rate_avg=input.size(0) / batch_time.avg,\n                    loss=losses, top1=top1, top5=top5))\n\n    print(' * Prec@1 {top1.avg:.3f} ({top1a:.3f}) Prec@5 {top5.avg:.3f} ({top5a:.3f})'.format(\n        top1=top1, top1a=100-top1.avg, top5=top5, top5a=100.-top5.avg))\n\n\nif __name__ == '__main__':\n    main()\n"""
data/__init__.py,0,b'from .dataset import Dataset\nfrom .transforms import *\nfrom .loader import create_loader\n'
data/dataset.py,2,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch.utils.data as data\n\nimport os\nimport re\nimport torch\nfrom PIL import Image\n\n\nIMG_EXTENSIONS = [\'.png\', \'.jpg\', \'.jpeg\']\n\n\ndef natural_key(string_):\n    """"""See http://www.codinghorror.com/blog/archives/001018.html""""""\n    return [int(s) if s.isdigit() else s for s in re.split(r\'(\\d+)\', string_.lower())]\n\n\ndef find_images_and_targets(folder, types=IMG_EXTENSIONS, class_to_idx=None, leaf_name_only=True, sort=True):\n    if class_to_idx is None:\n        class_to_idx = dict()\n        build_class_idx = True\n    else:\n        build_class_idx = False\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder, topdown=False):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else \'\'\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, \'_\')\n        if build_class_idx and not subdirs:\n            class_to_idx[label] = None\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n    if build_class_idx:\n        classes = sorted(class_to_idx.keys(), key=natural_key)\n        for idx, c in enumerate(classes):\n            class_to_idx[c] = idx\n    images_and_targets = zip(filenames, [class_to_idx[l] for l in labels])\n    if sort:\n        images_and_targets = sorted(images_and_targets, key=lambda k: natural_key(k[0]))\n    if build_class_idx:\n        return images_and_targets, classes, class_to_idx\n    else:\n        return images_and_targets\n\n\nclass Dataset(data.Dataset):\n\n    def __init__(\n            self,\n            root,\n            transform=None,\n            load_bytes=False):\n\n        imgs, _, _ = find_images_and_targets(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in subfolders of: "" + root + ""\\n""\n                               ""Supported image extensions are: "" + "","".join(IMG_EXTENSIONS)))\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.load_bytes = load_bytes\n\n    def __getitem__(self, index):\n        path, target = self.imgs[index]\n        img = open(path, \'rb\').read() if self.load_bytes else Image.open(path).convert(\'RGB\')\n        if self.transform is not None:\n            img = self.transform(img)\n        if target is None:\n            target = torch.zeros(1).long()\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def filenames(self, indices=[], basename=False):\n        if indices:\n            if basename:\n                return [os.path.basename(self.imgs[i][0]) for i in indices]\n            else:\n                return [self.imgs[i][0] for i in indices]\n        else:\n            if basename:\n                return [os.path.basename(x[0]) for x in self.imgs]\n            else:\n                return [x[0] for x in self.imgs]\n'"
data/loader.py,11,"b""import torch\nimport torch.utils.data\nfrom .transforms import *\n\n\ndef fast_collate(batch):\n    targets = torch.tensor([b[1] for b in batch], dtype=torch.int64)\n    batch_size = len(targets)\n    tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n    for i in range(batch_size):\n        tensor[i] += torch.from_numpy(batch[i][0])\n\n    return tensor, targets\n\n\nclass PrefetchLoader:\n\n    def __init__(self,\n            loader,\n            mean=IMAGENET_DEFAULT_MEAN,\n            std=IMAGENET_DEFAULT_STD):\n        self.loader = loader\n        self.mean = torch.tensor([x * 255 for x in mean]).cuda().view(1, 3, 1, 1)\n        self.std = torch.tensor([x * 255 for x in std]).cuda().view(1, 3, 1, 1)\n\n    def __iter__(self):\n        stream = torch.cuda.Stream()\n        first = True\n\n        for next_input, next_target in self.loader:\n            with torch.cuda.stream(stream):\n                next_input = next_input.cuda(non_blocking=True)\n                next_target = next_target.cuda(non_blocking=True)\n                next_input = next_input.float().sub_(self.mean).div_(self.std)\n\n            if not first:\n                yield input, target\n            else:\n                first = False\n\n            torch.cuda.current_stream().wait_stream(stream)\n            input = next_input\n            target = next_target\n\n        yield input, target\n\n    def __len__(self):\n        return len(self.loader)\n\n    @property\n    def sampler(self):\n        return self.loader.sampler\n\n\ndef create_loader(\n        dataset,\n        input_size,\n        batch_size,\n        is_training=False,\n        use_prefetcher=True,\n        interpolation='bilinear',\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD,\n        num_workers=1,\n        crop_pct=None,\n        tensorflow_preprocessing=False\n):\n    if isinstance(input_size, tuple):\n        img_size = input_size[-2:]\n    else:\n        img_size = input_size\n\n    if tensorflow_preprocessing and use_prefetcher:\n        from data.tf_preprocessing import TfPreprocessTransform\n        transform = TfPreprocessTransform(\n            is_training=is_training, size=img_size, interpolation=interpolation)\n    else:\n        transform = transforms_imagenet_eval(\n            img_size,\n            interpolation=interpolation,\n            use_prefetcher=use_prefetcher,\n            mean=mean,\n            std=std,\n            crop_pct=crop_pct)\n\n    dataset.transform = transform\n\n    loader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        collate_fn=fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate,\n    )\n    if use_prefetcher:\n        loader = PrefetchLoader(\n            loader,\n            mean=mean,\n            std=std)\n\n    return loader\n"""
data/tf_preprocessing.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""ImageNet preprocessing for MnasNet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\n\nIMAGE_SIZE = 224\nCROP_PADDING = 32\n\n\ndef distorted_bounding_box_crop(image_bytes,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n    """"""Generates cropped_image using one of the bboxes randomly distorted.\n\n    See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n    Args:\n      image_bytes: `Tensor` of binary image data.\n      bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n          where each coordinate is [0, 1) and the coordinates are arranged\n          as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n          image.\n      min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n          area of the image must contain at least this fraction of any bounding\n          box supplied.\n      aspect_ratio_range: An optional list of `float`s. The cropped area of the\n          image must have an aspect ratio = width / height within this range.\n      area_range: An optional list of `float`s. The cropped area of the image\n          must contain a fraction of the supplied image within in this range.\n      max_attempts: An optional `int`. Number of attempts at generating a cropped\n          region of the image of the specified constraints. After `max_attempts`\n          failures, return the entire image.\n      scope: Optional `str` for name scope.\n    Returns:\n      cropped image `Tensor`\n    """"""\n    with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image_bytes, bbox]):\n        shape = tf.image.extract_jpeg_shape(image_bytes)\n        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n            shape,\n            bounding_boxes=bbox,\n            min_object_covered=min_object_covered,\n            aspect_ratio_range=aspect_ratio_range,\n            area_range=area_range,\n            max_attempts=max_attempts,\n            use_image_if_no_bounding_boxes=True)\n        bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n        # Crop the image to the specified bounding box.\n        offset_y, offset_x, _ = tf.unstack(bbox_begin)\n        target_height, target_width, _ = tf.unstack(bbox_size)\n        crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n        image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n\n        return image\n\n\ndef _at_least_x_are_equal(a, b, x):\n    """"""At least `x` of `a` and `b` `Tensors` are equal.""""""\n    match = tf.equal(a, b)\n    match = tf.cast(match, tf.int32)\n    return tf.greater_equal(tf.reduce_sum(match), x)\n\n\ndef _decode_and_random_crop(image_bytes, image_size, resize_method):\n    """"""Make a random crop of image_size.""""""\n    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n    image = distorted_bounding_box_crop(\n        image_bytes,\n        bbox,\n        min_object_covered=0.1,\n        aspect_ratio_range=(3. / 4, 4. / 3.),\n        area_range=(0.08, 1.0),\n        max_attempts=10,\n        scope=None)\n    original_shape = tf.image.extract_jpeg_shape(image_bytes)\n    bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)\n\n    image = tf.cond(\n        bad,\n        lambda: _decode_and_center_crop(image_bytes, image_size),\n        lambda: tf.image.resize([image], [image_size, image_size], resize_method)[0])\n\n    return image\n\n\ndef _decode_and_center_crop(image_bytes, image_size, resize_method):\n    """"""Crops to center of image with padding then scales image_size.""""""\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n\n    return image\n\n\ndef _flip(image):\n    """"""Random horizontal image flip.""""""\n    image = tf.image.random_flip_left_right(image)\n    return image\n\n\ndef preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation=\'bicubic\'):\n    """"""Preprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor`.\n    """"""\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == \'bicubic\' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n\ndef preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation=\'bicubic\'):\n    """"""Preprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor`.\n    """"""\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == \'bicubic\' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n\ndef preprocess_image(image_bytes,\n                     is_training=False,\n                     use_bfloat16=False,\n                     image_size=IMAGE_SIZE,\n                     interpolation=\'bicubic\'):\n    """"""Preprocesses the given image.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      is_training: `bool` for whether the preprocessing is for training.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor` with value range of [0, 255].\n    """"""\n    if is_training:\n        return preprocess_for_train(image_bytes, use_bfloat16, image_size, interpolation)\n    else:\n        return preprocess_for_eval(image_bytes, use_bfloat16, image_size, interpolation)\n\n\nclass TfPreprocessTransform:\n\n    def __init__(self, is_training=False, size=224, interpolation=\'bicubic\'):\n        self.is_training = is_training\n        self.size = size[0] if isinstance(size, tuple) else size\n        self.interpolation = interpolation\n        self._image_bytes = None\n        self.process_image = self._build_tf_graph()\n        self.sess = None\n\n    def _build_tf_graph(self):\n        with tf.device(\'/cpu:0\'):\n            self._image_bytes = tf.placeholder(\n                shape=[],\n                dtype=tf.string,\n            )\n            img = preprocess_image(\n                self._image_bytes, self.is_training, False, self.size, self.interpolation)\n        return img\n\n    def __call__(self, image_bytes):\n        if self.sess is None:\n            self.sess = tf.Session()\n        img = self.sess.run(self.process_image, feed_dict={self._image_bytes: image_bytes})\n        img = img.round().clip(0, 255).astype(np.uint8)\n        if img.ndim < 3:\n            img = np.expand_dims(img, axis=-1)\n        img = np.rollaxis(img, 2)  # HWC to CHW\n        return img\n'"
data/transforms.py,4,"b""import torch\nfrom torchvision import transforms\nfrom PIL import Image\nimport math\nimport numpy as np\n\nDEFAULT_CROP_PCT = 0.875\n\nIMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\nIMAGENET_INCEPTION_MEAN = (0.5, 0.5, 0.5)\nIMAGENET_INCEPTION_STD = (0.5, 0.5, 0.5)\nIMAGENET_DPN_MEAN = (124 / 255, 117 / 255, 104 / 255)\nIMAGENET_DPN_STD = tuple([1 / (.0167 * 255)] * 3)\n\n\ndef resolve_data_config(model, args, default_cfg={}, verbose=True):\n    new_config = {}\n    default_cfg = default_cfg\n    if not default_cfg and hasattr(model, 'default_cfg'):\n        default_cfg = model.default_cfg\n\n    # Resolve input/image size\n    # FIXME grayscale/chans arg to use different # channels?\n    in_chans = 3\n    input_size = (in_chans, 224, 224)\n    if args.img_size is not None:\n        # FIXME support passing img_size as tuple, non-square\n        assert isinstance(args.img_size, int)\n        input_size = (in_chans, args.img_size, args.img_size)\n    elif 'input_size' in default_cfg:\n        input_size = default_cfg['input_size']\n    new_config['input_size'] = input_size\n\n    # resolve interpolation method\n    new_config['interpolation'] = 'bicubic'\n    if args.interpolation:\n        new_config['interpolation'] = args.interpolation\n    elif 'interpolation' in default_cfg:\n        new_config['interpolation'] = default_cfg['interpolation']\n\n    # resolve dataset + model mean for normalization\n    new_config['mean'] = get_mean_by_model(args.model)\n    if args.mean is not None:\n        mean = tuple(args.mean)\n        if len(mean) == 1:\n            mean = tuple(list(mean) * in_chans)\n        else:\n            assert len(mean) == in_chans\n        new_config['mean'] = mean\n    elif 'mean' in default_cfg:\n        new_config['mean'] = default_cfg['mean']\n\n    # resolve dataset + model std deviation for normalization\n    new_config['std'] = get_std_by_model(args.model)\n    if args.std is not None:\n        std = tuple(args.std)\n        if len(std) == 1:\n            std = tuple(list(std) * in_chans)\n        else:\n            assert len(std) == in_chans\n        new_config['std'] = std\n    elif 'std' in default_cfg:\n        new_config['std'] = default_cfg['std']\n\n    # resolve default crop percentage\n    new_config['crop_pct'] = DEFAULT_CROP_PCT\n    if args.crop_pct is not None:\n        new_config['crop_pct'] = args.crop_pct\n    elif 'crop_pct' in default_cfg:\n        new_config['crop_pct'] = default_cfg['crop_pct']\n\n    if verbose:\n        print('Data processing configuration for current model + dataset:')\n        for n, v in new_config.items():\n            print('\\t%s: %s' % (n, str(v)))\n\n    return new_config\n\n\ndef get_mean_by_model(model_name):\n    model_name = model_name.lower()\n    if 'dpn' in model_name:\n        return IMAGENET_DPN_STD\n    elif 'ception' in model_name:\n        return IMAGENET_INCEPTION_MEAN\n    else:\n        return IMAGENET_DEFAULT_MEAN\n\n\ndef get_std_by_model(model_name):\n    model_name = model_name.lower()\n    if 'dpn' in model_name:\n        return IMAGENET_DEFAULT_STD\n    elif 'ception' in model_name:\n        return IMAGENET_INCEPTION_STD\n    else:\n        return IMAGENET_DEFAULT_STD\n\n\nclass ToNumpy:\n\n    def __call__(self, pil_img):\n        np_img = np.array(pil_img, dtype=np.uint8)\n        if np_img.ndim < 3:\n            np_img = np.expand_dims(np_img, axis=-1)\n        np_img = np.rollaxis(np_img, 2)  # HWC to CHW\n        return np_img\n\n\nclass ToTensor:\n\n    def __init__(self, dtype=torch.float32):\n        self.dtype = dtype\n\n    def __call__(self, pil_img):\n        np_img = np.array(pil_img, dtype=np.uint8)\n        if np_img.ndim < 3:\n            np_img = np.expand_dims(np_img, axis=-1)\n        np_img = np.rollaxis(np_img, 2)  # HWC to CHW\n        return torch.from_numpy(np_img).to(dtype=self.dtype)\n\n\ndef _pil_interp(method):\n    if method == 'bicubic':\n        return Image.BICUBIC\n    elif method == 'lanczos':\n        return Image.LANCZOS\n    elif method == 'hamming':\n        return Image.HAMMING\n    else:\n        # default bilinear, do we want to allow nearest?\n        return Image.BILINEAR\n\n\ndef transforms_imagenet_eval(\n        img_size=224,\n        crop_pct=None,\n        interpolation='bilinear',\n        use_prefetcher=False,\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD):\n    crop_pct = crop_pct or DEFAULT_CROP_PCT\n\n    if isinstance(img_size, tuple):\n        assert len(img_size) == 2\n        if img_size[-1] == img_size[-2]:\n            # fall-back to older behaviour so Resize scales to shortest edge if target is square\n            scale_size = int(math.floor(img_size[0] / crop_pct))\n        else:\n            scale_size = tuple([int(x / crop_pct) for x in img_size])\n    else:\n        scale_size = int(math.floor(img_size / crop_pct))\n\n    tfl = [\n        transforms.Resize(scale_size, _pil_interp(interpolation)),\n        transforms.CenterCrop(img_size),\n    ]\n    if use_prefetcher:\n        # prefetcher and collate will handle tensor conversion and norm\n        tfl += [ToNumpy()]\n    else:\n        tfl += [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                     mean=torch.tensor(mean),\n                     std=torch.tensor(std))\n        ]\n\n    return transforms.Compose(tfl)\n"""
geffnet/__init__.py,0,"b'from .gen_efficientnet import *\nfrom .mobilenetv3 import *\nfrom .model_factory import create_model\nfrom .config import is_exportable, is_scriptable, set_exportable, set_scriptable\nfrom .activations import *'"
geffnet/config.py,1,"b'"""""" Global Config and Constants\n""""""\n\n__all__ = [\'is_exportable\', \'is_scriptable\', \'set_exportable\', \'set_scriptable\']\n\n# Set to True if exporting a model with Same padding via ONNX\n_EXPORTABLE = False\n\n# Set to True if wanting to use torch.jit.script on a model\n_SCRIPTABLE = False\n\n\ndef is_exportable():\n    return _EXPORTABLE\n\n\ndef set_exportable(value):\n    global _EXPORTABLE\n    _EXPORTABLE = value\n\n\ndef is_scriptable():\n    return _SCRIPTABLE\n\n\ndef set_scriptable(value):\n    global _SCRIPTABLE\n    _SCRIPTABLE = value\n\n'"
geffnet/conv2d_layers.py,16,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch._six import container_abcs\n\nfrom itertools import repeat\nfrom functools import partial\nfrom typing import Union, List, Tuple, Optional, Callable\nimport numpy as np\nimport math\n\nfrom .config import *\n\n\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, container_abcs.Iterable):\n            return x\n        return tuple(repeat(x, n))\n    return parse\n\n\n_single = _ntuple(1)\n_pair = _ntuple(2)\n_triple = _ntuple(3)\n_quadruple = _ntuple(4)\n\n\ndef _is_static_pad(kernel_size, stride=1, dilation=1, **_):\n    return stride == 1 and (dilation * (kernel_size - 1)) % 2 == 0\n\n\ndef _get_padding(kernel_size, stride=1, dilation=1, **_):\n    padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n    return padding\n\n\ndef _calc_same_pad(i: int, k: int, s: int, d: int):\n    return max((math.ceil(i / s) - 1) * s + (k - 1) * d + 1 - i, 0)\n\n\ndef _same_pad_arg(input_size, kernel_size, stride, dilation):\n    ih, iw = input_size\n    kh, kw = kernel_size\n    pad_h = _calc_same_pad(ih, kh, stride[0], dilation[0])\n    pad_w = _calc_same_pad(iw, kw, stride[1], dilation[1])\n    return [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2]\n\n\ndef _split_channels(num_chan, num_groups):\n    split = [num_chan // num_groups for _ in range(num_groups)]\n    split[0] += num_chan - sum(split)\n    return split\n\n\ndef conv2d_same(\n        x, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, stride: Tuple[int, int] = (1, 1),\n        padding: Tuple[int, int] = (0, 0), dilation: Tuple[int, int] = (1, 1), groups: int = 1):\n    ih, iw = x.size()[-2:]\n    kh, kw = weight.size()[-2:]\n    pad_h = _calc_same_pad(ih, kh, stride[0], dilation[0])\n    pad_w = _calc_same_pad(iw, kw, stride[1], dilation[1])\n    if pad_h > 0 or pad_w > 0:\n        x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2])\n    return F.conv2d(x, weight, bias, stride, (0, 0), dilation, groups)\n\n\nclass Conv2dSame(nn.Conv2d):\n    """""" Tensorflow like \'SAME\' convolution wrapper for 2D convolutions\n    """"""\n\n    # pylint: disable=unused-argument\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        super(Conv2dSame, self).__init__(\n            in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n\n    def forward(self, x):\n        return conv2d_same(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass Conv2dSameExport(nn.Conv2d):\n    """""" ONNX export friendly Tensorflow like \'SAME\' convolution wrapper for 2D convolutions\n\n    NOTE: This does not currently work with torch.jit.script\n    """"""\n\n    # pylint: disable=unused-argument\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        super(Conv2dSameExport, self).__init__(\n            in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n        self.pad = None\n        self.pad_input_size = (0, 0)\n\n    def forward(self, x):\n        input_size = x.size()[-2:]\n        if self.pad is None:\n            pad_arg = _same_pad_arg(input_size, self.weight.size()[-2:], self.stride, self.dilation)\n            self.pad = nn.ZeroPad2d(pad_arg)\n            self.pad_input_size = input_size\n        else:\n            assert self.pad_input_size == input_size\n\n        x = self.pad(x)\n        return F.conv2d(\n            x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\n\ndef get_padding_value(padding, kernel_size, **kwargs):\n    dynamic = False\n    if isinstance(padding, str):\n        # for any string padding, the padding will be calculated for you, one of three ways\n        padding = padding.lower()\n        if padding == \'same\':\n            # TF compatible \'SAME\' padding, has a performance and GPU memory allocation impact\n            if _is_static_pad(kernel_size, **kwargs):\n                # static case, no extra overhead\n                padding = _get_padding(kernel_size, **kwargs)\n            else:\n                # dynamic padding\n                padding = 0\n                dynamic = True\n        elif padding == \'valid\':\n            # \'VALID\' padding, same as padding=0\n            padding = 0\n        else:\n            # Default to PyTorch style \'same\'-ish symmetric padding\n            padding = _get_padding(kernel_size, **kwargs)\n    return padding, dynamic\n\n\ndef create_conv2d_pad(in_chs, out_chs, kernel_size, **kwargs):\n    padding = kwargs.pop(\'padding\', \'\')\n    kwargs.setdefault(\'bias\', False)\n    padding, is_dynamic = get_padding_value(padding, kernel_size, **kwargs)\n    if is_dynamic:\n        if is_exportable():\n            assert not is_scriptable()\n            return Conv2dSameExport(in_chs, out_chs, kernel_size, **kwargs)\n        else:\n            return Conv2dSame(in_chs, out_chs, kernel_size, **kwargs)\n    else:\n        return nn.Conv2d(in_chs, out_chs, kernel_size, padding=padding, **kwargs)\n\n\nclass MixedConv2d(nn.ModuleDict):\n    """""" Mixed Grouped Convolution\n    Based on MDConv and GroupedConv in MixNet impl:\n      https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mixnet/custom_layers.py\n    """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size=3,\n                 stride=1, padding=\'\', dilation=1, depthwise=False, **kwargs):\n        super(MixedConv2d, self).__init__()\n\n        kernel_size = kernel_size if isinstance(kernel_size, list) else [kernel_size]\n        num_groups = len(kernel_size)\n        in_splits = _split_channels(in_channels, num_groups)\n        out_splits = _split_channels(out_channels, num_groups)\n        self.in_channels = sum(in_splits)\n        self.out_channels = sum(out_splits)\n        for idx, (k, in_ch, out_ch) in enumerate(zip(kernel_size, in_splits, out_splits)):\n            conv_groups = out_ch if depthwise else 1\n            self.add_module(\n                str(idx),\n                create_conv2d_pad(\n                    in_ch, out_ch, k, stride=stride,\n                    padding=padding, dilation=dilation, groups=conv_groups, **kwargs)\n            )\n        self.splits = in_splits\n\n    def forward(self, x):\n        x_split = torch.split(x, self.splits, 1)\n        x_out = [conv(x_split[i]) for i, conv in enumerate(self.values())]\n        x = torch.cat(x_out, 1)\n        return x\n\n\ndef get_condconv_initializer(initializer, num_experts, expert_shape):\n    def condconv_initializer(weight):\n        """"""CondConv initializer function.""""""\n        num_params = np.prod(expert_shape)\n        if (len(weight.shape) != 2 or weight.shape[0] != num_experts or\n                weight.shape[1] != num_params):\n            raise (ValueError(\n                \'CondConv variables must have shape [num_experts, num_params]\'))\n        for i in range(num_experts):\n            initializer(weight[i].view(expert_shape))\n    return condconv_initializer\n\n\nclass CondConv2d(nn.Module):\n    """""" Conditional Convolution\n    Inspired by: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/condconv/condconv_layers.py\n\n    Grouped convolution hackery for parallel execution of the per-sample kernel filters inspired by this discussion:\n    https://github.com/pytorch/pytorch/issues/17983\n    """"""\n    __constants__ = [\'bias\', \'in_channels\', \'out_channels\', \'dynamic_padding\']\n\n    def __init__(self, in_channels, out_channels, kernel_size=3,\n                 stride=1, padding=\'\', dilation=1, groups=1, bias=False, num_experts=4):\n        super(CondConv2d, self).__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = _pair(kernel_size)\n        self.stride = _pair(stride)\n        padding_val, is_padding_dynamic = get_padding_value(\n            padding, kernel_size, stride=stride, dilation=dilation)\n        self.dynamic_padding = is_padding_dynamic  # if in forward to work with torchscript\n        self.padding = _pair(padding_val)\n        self.dilation = _pair(dilation)\n        self.groups = groups\n        self.num_experts = num_experts\n\n        self.weight_shape = (self.out_channels, self.in_channels // self.groups) + self.kernel_size\n        weight_num_param = 1\n        for wd in self.weight_shape:\n            weight_num_param *= wd\n        self.weight = torch.nn.Parameter(torch.Tensor(self.num_experts, weight_num_param))\n\n        if bias:\n            self.bias_shape = (self.out_channels,)\n            self.bias = torch.nn.Parameter(torch.Tensor(self.num_experts, self.out_channels))\n        else:\n            self.register_parameter(\'bias\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        init_weight = get_condconv_initializer(\n            partial(nn.init.kaiming_uniform_, a=math.sqrt(5)), self.num_experts, self.weight_shape)\n        init_weight(self.weight)\n        if self.bias is not None:\n            fan_in = np.prod(self.weight_shape[1:])\n            bound = 1 / math.sqrt(fan_in)\n            init_bias = get_condconv_initializer(\n                partial(nn.init.uniform_, a=-bound, b=bound), self.num_experts, self.bias_shape)\n            init_bias(self.bias)\n\n    def forward(self, x, routing_weights):\n        B, C, H, W = x.shape\n        weight = torch.matmul(routing_weights, self.weight)\n        new_weight_shape = (B * self.out_channels, self.in_channels // self.groups) + self.kernel_size\n        weight = weight.view(new_weight_shape)\n        bias = None\n        if self.bias is not None:\n            bias = torch.matmul(routing_weights, self.bias)\n            bias = bias.view(B * self.out_channels)\n        # move batch elements with channels so each batch element can be efficiently convolved with separate kernel\n        x = x.view(1, B * C, H, W)\n        if self.dynamic_padding:\n            out = conv2d_same(\n                x, weight, bias, stride=self.stride, padding=self.padding,\n                dilation=self.dilation, groups=self.groups * B)\n        else:\n            out = F.conv2d(\n                x, weight, bias, stride=self.stride, padding=self.padding,\n                dilation=self.dilation, groups=self.groups * B)\n        out = out.permute([1, 0, 2, 3]).view(B, self.out_channels, out.shape[-2], out.shape[-1])\n\n        # Literal port (from TF definition)\n        # x = torch.split(x, 1, 0)\n        # weight = torch.split(weight, 1, 0)\n        # if self.bias is not None:\n        #     bias = torch.matmul(routing_weights, self.bias)\n        #     bias = torch.split(bias, 1, 0)\n        # else:\n        #     bias = [None] * B\n        # out = []\n        # for xi, wi, bi in zip(x, weight, bias):\n        #     wi = wi.view(*self.weight_shape)\n        #     if bi is not None:\n        #         bi = bi.view(*self.bias_shape)\n        #     out.append(self.conv_fn(\n        #         xi, wi, bi, stride=self.stride, padding=self.padding,\n        #         dilation=self.dilation, groups=self.groups))\n        # out = torch.cat(out, 0)\n        return out\n\n\ndef select_conv2d(in_chs, out_chs, kernel_size, **kwargs):\n    assert \'groups\' not in kwargs  # only use \'depthwise\' bool arg\n    if isinstance(kernel_size, list):\n        assert \'num_experts\' not in kwargs  # MixNet + CondConv combo not supported currently\n        # We\'re going to use only lists for defining the MixedConv2d kernel groups,\n        # ints, tuples, other iterables will continue to pass to normal conv and specify h, w.\n        m = MixedConv2d(in_chs, out_chs, kernel_size, **kwargs)\n    else:\n        depthwise = kwargs.pop(\'depthwise\', False)\n        groups = out_chs if depthwise else 1\n        if \'num_experts\' in kwargs and kwargs[\'num_experts\'] > 0:\n            m = CondConv2d(in_chs, out_chs, kernel_size, groups=groups, **kwargs)\n        else:\n            m = create_conv2d_pad(in_chs, out_chs, kernel_size, groups=groups, **kwargs)\n    return m\n'"
geffnet/efficientnet_builder.py,2,"b'import re\nfrom copy import deepcopy\n\nfrom .conv2d_layers import *\nfrom geffnet.activations import *\n\n\n# Defaults used for Google/Tensorflow training of mobile networks /w RMSprop as per\n# papers and TF reference implementations. PT momentum equiv for TF decay is (1 - TF decay)\n# NOTE: momentum varies btw .99 and .9997 depending on source\n# .99 in official TF TPU impl\n# .9997 (/w .999 in search space) for paper\n#\n# PyTorch defaults are momentum = .1, eps = 1e-5\n#\nBN_MOMENTUM_TF_DEFAULT = 1 - 0.99\nBN_EPS_TF_DEFAULT = 1e-3\n_BN_ARGS_TF = dict(momentum=BN_MOMENTUM_TF_DEFAULT, eps=BN_EPS_TF_DEFAULT)\n\n\ndef get_bn_args_tf():\n    return _BN_ARGS_TF.copy()\n\n\ndef resolve_bn_args(kwargs):\n    bn_args = get_bn_args_tf() if kwargs.pop(\'bn_tf\', False) else {}\n    bn_momentum = kwargs.pop(\'bn_momentum\', None)\n    if bn_momentum is not None:\n        bn_args[\'momentum\'] = bn_momentum\n    bn_eps = kwargs.pop(\'bn_eps\', None)\n    if bn_eps is not None:\n        bn_args[\'eps\'] = bn_eps\n    return bn_args\n\n\n_SE_ARGS_DEFAULT = dict(\n    gate_fn=sigmoid,\n    act_layer=None,  # None == use containing block\'s activation layer\n    reduce_mid=False,\n    divisor=1)\n\n\ndef resolve_se_args(kwargs, in_chs, act_layer=None):\n    se_kwargs = kwargs.copy() if kwargs is not None else {}\n    # fill in args that aren\'t specified with the defaults\n    for k, v in _SE_ARGS_DEFAULT.items():\n        se_kwargs.setdefault(k, v)\n    # some models, like MobilNetV3, calculate SE reduction chs from the containing block\'s mid_ch instead of in_ch\n    if not se_kwargs.pop(\'reduce_mid\'):\n        se_kwargs[\'reduced_base_chs\'] = in_chs\n    # act_layer override, if it remains None, the containing block\'s act_layer will be used\n    if se_kwargs[\'act_layer\'] is None:\n        assert act_layer is not None\n        se_kwargs[\'act_layer\'] = act_layer\n    return se_kwargs\n\n\ndef resolve_act_layer(kwargs, default=\'relu\'):\n    act_layer = kwargs.pop(\'act_layer\', default)\n    if isinstance(act_layer, str):\n        act_layer = get_act_layer(act_layer)\n    return act_layer\n\n\ndef make_divisible(v: int, divisor: int = 8, min_value: int = None):\n    min_value = min_value or divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    if new_v < 0.9 * v:  # ensure round down does not go down by more than 10%.\n        new_v += divisor\n    return new_v\n\n\ndef round_channels(channels, multiplier=1.0, divisor=8, channel_min=None):\n    """"""Round number of filters based on depth multiplier.""""""\n    if not multiplier:\n        return channels\n    channels *= multiplier\n    return make_divisible(channels, divisor, channel_min)\n\n\ndef drop_connect(inputs, training: bool = False, drop_connect_rate: float = 0.):\n    """"""Apply drop connect.""""""\n    if not training:\n        return inputs\n\n    keep_prob = 1 - drop_connect_rate\n    random_tensor = keep_prob + torch.rand(\n        (inputs.size()[0], 1, 1, 1), dtype=inputs.dtype, device=inputs.device)\n    random_tensor.floor_()  # binarize\n    output = inputs.div(keep_prob) * random_tensor\n    return output\n\n\nclass SqueezeExcite(nn.Module):\n\n    def __init__(self, in_chs, se_ratio=0.25, reduced_base_chs=None, act_layer=nn.ReLU, gate_fn=sigmoid, divisor=1):\n        super(SqueezeExcite, self).__init__()\n        self.gate_fn = gate_fn\n        reduced_chs = make_divisible((reduced_base_chs or in_chs) * se_ratio, divisor)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv_reduce = nn.Conv2d(in_chs, reduced_chs, 1, bias=True)\n        self.act1 = act_layer(inplace=True)\n        self.conv_expand = nn.Conv2d(reduced_chs, in_chs, 1, bias=True)\n\n    def forward(self, x):\n        # tensor.view + mean bad for ONNX export (produces mess of gather ops that break TensorRT)\n        x_se = self.avg_pool(x)\n        x_se = self.conv_reduce(x_se)\n        x_se = self.act1(x_se)\n        x_se = self.conv_expand(x_se)\n        x = x * self.gate_fn(x_se)\n        return x\n\n\nclass ConvBnAct(nn.Module):\n    def __init__(self, in_chs, out_chs, kernel_size,\n                 stride=1, pad_type=\'\', act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(ConvBnAct, self).__init__()\n        assert stride in [1, 2]\n        norm_kwargs = norm_kwargs or {}\n        self.conv = select_conv2d(in_chs, out_chs, kernel_size, stride=stride, padding=pad_type)\n        self.bn1 = norm_layer(out_chs, **norm_kwargs)\n        self.act1 = act_layer(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        return x\n\n\nclass DepthwiseSeparableConv(nn.Module):\n    """""" DepthwiseSeparable block\n    Used for DS convs in MobileNet-V1 and in the place of IR blocks with an expansion\n    factor of 1.0. This is an alternative to having a IR with optional first pw conv.\n    """"""\n    def __init__(self, in_chs, out_chs, dw_kernel_size=3,\n                 stride=1, pad_type=\'\', act_layer=nn.ReLU, noskip=False,\n                 pw_kernel_size=1, pw_act=False, se_ratio=0., se_kwargs=None,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None, drop_connect_rate=0.):\n        super(DepthwiseSeparableConv, self).__init__()\n        assert stride in [1, 2]\n        norm_kwargs = norm_kwargs or {}\n        self.has_residual = (stride == 1 and in_chs == out_chs) and not noskip\n        self.drop_connect_rate = drop_connect_rate\n\n        self.conv_dw = select_conv2d(\n            in_chs, in_chs, dw_kernel_size, stride=stride, padding=pad_type, depthwise=True)\n        self.bn1 = norm_layer(in_chs, **norm_kwargs)\n        self.act1 = act_layer(inplace=True)\n\n        # Squeeze-and-excitation\n        if se_ratio is not None and se_ratio > 0.:\n            se_kwargs = resolve_se_args(se_kwargs, in_chs, act_layer)\n            self.se = SqueezeExcite(in_chs, se_ratio=se_ratio, **se_kwargs)\n        else:\n            self.se = nn.Identity()\n\n        self.conv_pw = select_conv2d(in_chs, out_chs, pw_kernel_size, padding=pad_type)\n        self.bn2 = norm_layer(out_chs, **norm_kwargs)\n        self.act2 = act_layer(inplace=True) if pw_act else nn.Identity()\n\n    def forward(self, x):\n        residual = x\n\n        x = self.conv_dw(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n\n        x = self.se(x)\n\n        x = self.conv_pw(x)\n        x = self.bn2(x)\n        x = self.act2(x)\n\n        if self.has_residual:\n            if self.drop_connect_rate > 0.:\n                x = drop_connect(x, self.training, self.drop_connect_rate)\n            x += residual\n        return x\n\n\nclass InvertedResidual(nn.Module):\n    """""" Inverted residual block w/ optional SE""""""\n\n    def __init__(self, in_chs, out_chs, dw_kernel_size=3,\n                 stride=1, pad_type=\'\', act_layer=nn.ReLU, noskip=False,\n                 exp_ratio=1.0, exp_kernel_size=1, pw_kernel_size=1,\n                 se_ratio=0., se_kwargs=None, norm_layer=nn.BatchNorm2d, norm_kwargs=None,\n                 conv_kwargs=None, drop_connect_rate=0.):\n        super(InvertedResidual, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n        conv_kwargs = conv_kwargs or {}\n        mid_chs: int = make_divisible(in_chs * exp_ratio)\n        self.has_residual = (in_chs == out_chs and stride == 1) and not noskip\n        self.drop_connect_rate = drop_connect_rate\n\n        # Point-wise expansion\n        self.conv_pw = select_conv2d(in_chs, mid_chs, exp_kernel_size, padding=pad_type, **conv_kwargs)\n        self.bn1 = norm_layer(mid_chs, **norm_kwargs)\n        self.act1 = act_layer(inplace=True)\n\n        # Depth-wise convolution\n        self.conv_dw = select_conv2d(\n            mid_chs, mid_chs, dw_kernel_size, stride=stride, padding=pad_type, depthwise=True, **conv_kwargs)\n        self.bn2 = norm_layer(mid_chs, **norm_kwargs)\n        self.act2 = act_layer(inplace=True)\n\n        # Squeeze-and-excitation\n        if se_ratio is not None and se_ratio > 0.:\n            se_kwargs = resolve_se_args(se_kwargs, in_chs, act_layer)\n            self.se = SqueezeExcite(mid_chs, se_ratio=se_ratio, **se_kwargs)\n        else:\n            self.se = nn.Identity()  # for jit.script compat\n\n        # Point-wise linear projection\n        self.conv_pwl = select_conv2d(mid_chs, out_chs, pw_kernel_size, padding=pad_type, **conv_kwargs)\n        self.bn3 = norm_layer(out_chs, **norm_kwargs)\n\n    def forward(self, x):\n        residual = x\n\n        # Point-wise expansion\n        x = self.conv_pw(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n\n        # Depth-wise convolution\n        x = self.conv_dw(x)\n        x = self.bn2(x)\n        x = self.act2(x)\n\n        # Squeeze-and-excitation\n        x = self.se(x)\n\n        # Point-wise linear projection\n        x = self.conv_pwl(x)\n        x = self.bn3(x)\n\n        if self.has_residual:\n            if self.drop_connect_rate > 0.:\n                x = drop_connect(x, self.training, self.drop_connect_rate)\n            x += residual\n        return x\n\n\nclass CondConvResidual(InvertedResidual):\n    """""" Inverted residual block w/ CondConv routing""""""\n\n    def __init__(self, in_chs, out_chs, dw_kernel_size=3,\n                 stride=1, pad_type=\'\', act_layer=nn.ReLU, noskip=False,\n                 exp_ratio=1.0, exp_kernel_size=1, pw_kernel_size=1,\n                 se_ratio=0., se_kwargs=None, norm_layer=nn.BatchNorm2d, norm_kwargs=None,\n                 num_experts=0, drop_connect_rate=0.):\n\n        self.num_experts = num_experts\n        conv_kwargs = dict(num_experts=self.num_experts)\n\n        super(CondConvResidual, self).__init__(\n            in_chs, out_chs, dw_kernel_size=dw_kernel_size, stride=stride, pad_type=pad_type,\n            act_layer=act_layer, noskip=noskip, exp_ratio=exp_ratio, exp_kernel_size=exp_kernel_size,\n            pw_kernel_size=pw_kernel_size, se_ratio=se_ratio, se_kwargs=se_kwargs,\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs, conv_kwargs=conv_kwargs,\n            drop_connect_rate=drop_connect_rate)\n\n        self.routing_fn = nn.Linear(in_chs, self.num_experts)\n\n    def forward(self, x):\n        residual = x\n\n        # CondConv routing\n        pooled_inputs = F.adaptive_avg_pool2d(x, 1).flatten(1)\n        routing_weights = torch.sigmoid(self.routing_fn(pooled_inputs))\n\n        # Point-wise expansion\n        x = self.conv_pw(x, routing_weights)\n        x = self.bn1(x)\n        x = self.act1(x)\n\n        # Depth-wise convolution\n        x = self.conv_dw(x, routing_weights)\n        x = self.bn2(x)\n        x = self.act2(x)\n\n        # Squeeze-and-excitation\n        x = self.se(x)\n\n        # Point-wise linear projection\n        x = self.conv_pwl(x, routing_weights)\n        x = self.bn3(x)\n\n        if self.has_residual:\n            if self.drop_connect_rate > 0.:\n                x = drop_connect(x, self.training, self.drop_connect_rate)\n            x += residual\n        return x\n\n\nclass EdgeResidual(nn.Module):\n    """""" EdgeTPU Residual block with expansion convolution followed by pointwise-linear w/ stride""""""\n\n    def __init__(self, in_chs, out_chs, exp_kernel_size=3, exp_ratio=1.0, fake_in_chs=0,\n                 stride=1, pad_type=\'\', act_layer=nn.ReLU, noskip=False, pw_kernel_size=1,\n                 se_ratio=0., se_kwargs=None, norm_layer=nn.BatchNorm2d, norm_kwargs=None, drop_connect_rate=0.):\n        super(EdgeResidual, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n        mid_chs = make_divisible(fake_in_chs * exp_ratio) if fake_in_chs > 0 else make_divisible(in_chs * exp_ratio)\n        self.has_residual = (in_chs == out_chs and stride == 1) and not noskip\n        self.drop_connect_rate = drop_connect_rate\n\n        # Expansion convolution\n        self.conv_exp = select_conv2d(in_chs, mid_chs, exp_kernel_size, padding=pad_type)\n        self.bn1 = norm_layer(mid_chs, **norm_kwargs)\n        self.act1 = act_layer(inplace=True)\n\n        # Squeeze-and-excitation\n        if se_ratio is not None and se_ratio > 0.:\n            se_kwargs = resolve_se_args(se_kwargs, in_chs, act_layer)\n            self.se = SqueezeExcite(mid_chs, se_ratio=se_ratio, **se_kwargs)\n        else:\n            self.se = nn.Identity()\n\n        # Point-wise linear projection\n        self.conv_pwl = select_conv2d(mid_chs, out_chs, pw_kernel_size, stride=stride, padding=pad_type)\n        self.bn2 = nn.BatchNorm2d(out_chs, **norm_kwargs)\n\n    def forward(self, x):\n        residual = x\n\n        # Expansion convolution\n        x = self.conv_exp(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n\n        # Squeeze-and-excitation\n        x = self.se(x)\n\n        # Point-wise linear projection\n        x = self.conv_pwl(x)\n        x = self.bn2(x)\n\n        if self.has_residual:\n            if self.drop_connect_rate > 0.:\n                x = drop_connect(x, self.training, self.drop_connect_rate)\n            x += residual\n\n        return x\n\n\nclass EfficientNetBuilder:\n    """""" Build Trunk Blocks for Efficient/Mobile Networks\n\n    This ended up being somewhat of a cross between\n    https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mnasnet_models.py\n    and\n    https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/modeling/backbone/fbnet_builder.py\n\n    """"""\n\n    def __init__(self, channel_multiplier=1.0, channel_divisor=8, channel_min=None,\n                 pad_type=\'\', act_layer=None, se_kwargs=None,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None, drop_connect_rate=0.):\n        self.channel_multiplier = channel_multiplier\n        self.channel_divisor = channel_divisor\n        self.channel_min = channel_min\n        self.pad_type = pad_type\n        self.act_layer = act_layer\n        self.se_kwargs = se_kwargs\n        self.norm_layer = norm_layer\n        self.norm_kwargs = norm_kwargs\n        self.drop_connect_rate = drop_connect_rate\n\n        # updated during build\n        self.in_chs = None\n        self.block_idx = 0\n        self.block_count = 0\n\n    def _round_channels(self, chs):\n        return round_channels(chs, self.channel_multiplier, self.channel_divisor, self.channel_min)\n\n    def _make_block(self, ba):\n        bt = ba.pop(\'block_type\')\n        ba[\'in_chs\'] = self.in_chs\n        ba[\'out_chs\'] = self._round_channels(ba[\'out_chs\'])\n        if \'fake_in_chs\' in ba and ba[\'fake_in_chs\']:\n            # FIXME this is a hack to work around mismatch in origin impl input filters for EdgeTPU\n            ba[\'fake_in_chs\'] = self._round_channels(ba[\'fake_in_chs\'])\n        ba[\'norm_layer\'] = self.norm_layer\n        ba[\'norm_kwargs\'] = self.norm_kwargs\n        ba[\'pad_type\'] = self.pad_type\n        # block act fn overrides the model default\n        ba[\'act_layer\'] = ba[\'act_layer\'] if ba[\'act_layer\'] is not None else self.act_layer\n        assert ba[\'act_layer\'] is not None\n        if bt == \'ir\':\n            ba[\'drop_connect_rate\'] = self.drop_connect_rate * self.block_idx / self.block_count\n            ba[\'se_kwargs\'] = self.se_kwargs\n            if ba.get(\'num_experts\', 0) > 0:\n                block = CondConvResidual(**ba)\n            else:\n                block = InvertedResidual(**ba)\n        elif bt == \'ds\' or bt == \'dsa\':\n            ba[\'drop_connect_rate\'] = self.drop_connect_rate * self.block_idx / self.block_count\n            ba[\'se_kwargs\'] = self.se_kwargs\n            block = DepthwiseSeparableConv(**ba)\n        elif bt == \'er\':\n            ba[\'drop_connect_rate\'] = self.drop_connect_rate * self.block_idx / self.block_count\n            ba[\'se_kwargs\'] = self.se_kwargs\n            block = EdgeResidual(**ba)\n        elif bt == \'cn\':\n            block = ConvBnAct(**ba)\n        else:\n            assert False, \'Uknkown block type (%s) while building model.\' % bt\n        self.in_chs = ba[\'out_chs\']  # update in_chs for arg of next block\n        return block\n\n    def _make_stack(self, stack_args):\n        blocks = []\n        # each stack (stage) contains a list of block arguments\n        for i, ba in enumerate(stack_args):\n            if i >= 1:\n                # only the first block in any stack can have a stride > 1\n                ba[\'stride\'] = 1\n            block = self._make_block(ba)\n            blocks.append(block)\n            self.block_idx += 1  # incr global idx (across all stacks)\n        return nn.Sequential(*blocks)\n\n    def __call__(self, in_chs, block_args):\n        """""" Build the blocks\n        Args:\n            in_chs: Number of input-channels passed to first block\n            block_args: A list of lists, outer list defines stages, inner\n                list contains strings defining block configuration(s)\n        Return:\n             List of block stacks (each stack wrapped in nn.Sequential)\n        """"""\n        self.in_chs = in_chs\n        self.block_count = sum([len(x) for x in block_args])\n        self.block_idx = 0\n        blocks = []\n        # outer list of block_args defines the stacks (\'stages\' by some conventions)\n        for stack_idx, stack in enumerate(block_args):\n            assert isinstance(stack, list)\n            stack = self._make_stack(stack)\n            blocks.append(stack)\n        return blocks\n\n\ndef _parse_ksize(ss):\n    if ss.isdigit():\n        return int(ss)\n    else:\n        return [int(k) for k in ss.split(\'.\')]\n\n\ndef _decode_block_str(block_str):\n    """""" Decode block definition string\n\n    Gets a list of block arg (dicts) through a string notation of arguments.\n    E.g. ir_r2_k3_s2_e1_i32_o16_se0.25_noskip\n\n    All args can exist in any order with the exception of the leading string which\n    is assumed to indicate the block type.\n\n    leading string - block type (\n      ir = InvertedResidual, ds = DepthwiseSep, dsa = DeptwhiseSep with pw act, cn = ConvBnAct)\n    r - number of repeat blocks,\n    k - kernel size,\n    s - strides (1-9),\n    e - expansion ratio,\n    c - output channels,\n    se - squeeze/excitation ratio\n    n - activation fn (\'re\', \'r6\', \'hs\', or \'sw\')\n    Args:\n        block_str: a string representation of block arguments.\n    Returns:\n        A list of block args (dicts)\n    Raises:\n        ValueError: if the string def not properly specified (TODO)\n    """"""\n    assert isinstance(block_str, str)\n    ops = block_str.split(\'_\')\n    block_type = ops[0]  # take the block type off the front\n    ops = ops[1:]\n    options = {}\n    noskip = False\n    for op in ops:\n        # string options being checked on individual basis, combine if they grow\n        if op == \'noskip\':\n            noskip = True\n        elif op.startswith(\'n\'):\n            # activation fn\n            key = op[0]\n            v = op[1:]\n            if v == \'re\':\n                value = get_act_layer(\'relu\')\n            elif v == \'r6\':\n                value = get_act_layer(\'relu6\')\n            elif v == \'hs\':\n                value = get_act_layer(\'hard_swish\')\n            elif v == \'sw\':\n                value = get_act_layer(\'swish\')\n            else:\n                continue\n            options[key] = value\n        else:\n            # all numeric options\n            splits = re.split(r\'(\\d.*)\', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n    # if act_layer is None, the model default (passed to model init) will be used\n    act_layer = options[\'n\'] if \'n\' in options else None\n    exp_kernel_size = _parse_ksize(options[\'a\']) if \'a\' in options else 1\n    pw_kernel_size = _parse_ksize(options[\'p\']) if \'p\' in options else 1\n    fake_in_chs = int(options[\'fc\']) if \'fc\' in options else 0  # FIXME hack to deal with in_chs issue in TPU def\n\n    num_repeat = int(options[\'r\'])\n    # each type of block has different valid arguments, fill accordingly\n    if block_type == \'ir\':\n        block_args = dict(\n            block_type=block_type,\n            dw_kernel_size=_parse_ksize(options[\'k\']),\n            exp_kernel_size=exp_kernel_size,\n            pw_kernel_size=pw_kernel_size,\n            out_chs=int(options[\'c\']),\n            exp_ratio=float(options[\'e\']),\n            se_ratio=float(options[\'se\']) if \'se\' in options else None,\n            stride=int(options[\'s\']),\n            act_layer=act_layer,\n            noskip=noskip,\n        )\n        if \'cc\' in options:\n            block_args[\'num_experts\'] = int(options[\'cc\'])\n    elif block_type == \'ds\' or block_type == \'dsa\':\n        block_args = dict(\n            block_type=block_type,\n            dw_kernel_size=_parse_ksize(options[\'k\']),\n            pw_kernel_size=pw_kernel_size,\n            out_chs=int(options[\'c\']),\n            se_ratio=float(options[\'se\']) if \'se\' in options else None,\n            stride=int(options[\'s\']),\n            act_layer=act_layer,\n            pw_act=block_type == \'dsa\',\n            noskip=block_type == \'dsa\' or noskip,\n        )\n    elif block_type == \'er\':\n        block_args = dict(\n            block_type=block_type,\n            exp_kernel_size=_parse_ksize(options[\'k\']),\n            pw_kernel_size=pw_kernel_size,\n            out_chs=int(options[\'c\']),\n            exp_ratio=float(options[\'e\']),\n            fake_in_chs=fake_in_chs,\n            se_ratio=float(options[\'se\']) if \'se\' in options else None,\n            stride=int(options[\'s\']),\n            act_layer=act_layer,\n            noskip=noskip,\n        )\n    elif block_type == \'cn\':\n        block_args = dict(\n            block_type=block_type,\n            kernel_size=int(options[\'k\']),\n            out_chs=int(options[\'c\']),\n            stride=int(options[\'s\']),\n            act_layer=act_layer,\n        )\n    else:\n        assert False, \'Unknown block type (%s)\' % block_type\n\n    return block_args, num_repeat\n\n\ndef _scale_stage_depth(stack_args, repeats, depth_multiplier=1.0, depth_trunc=\'ceil\'):\n    """""" Per-stage depth scaling\n    Scales the block repeats in each stage. This depth scaling impl maintains\n    compatibility with the EfficientNet scaling method, while allowing sensible\n    scaling for other models that may have multiple block arg definitions in each stage.\n    """"""\n\n    # We scale the total repeat count for each stage, there may be multiple\n    # block arg defs per stage so we need to sum.\n    num_repeat = sum(repeats)\n    if depth_trunc == \'round\':\n        # Truncating to int by rounding allows stages with few repeats to remain\n        # proportionally smaller for longer. This is a good choice when stage definitions\n        # include single repeat stages that we\'d prefer to keep that way as long as possible\n        num_repeat_scaled = max(1, round(num_repeat * depth_multiplier))\n    else:\n        # The default for EfficientNet truncates repeats to int via \'ceil\'.\n        # Any multiplier > 1.0 will result in an increased depth for every stage.\n        num_repeat_scaled = int(math.ceil(num_repeat * depth_multiplier))\n\n    # Proportionally distribute repeat count scaling to each block definition in the stage.\n    # Allocation is done in reverse as it results in the first block being less likely to be scaled.\n    # The first block makes less sense to repeat in most of the arch definitions.\n    repeats_scaled = []\n    for r in repeats[::-1]:\n        rs = max(1, round((r / num_repeat * num_repeat_scaled)))\n        repeats_scaled.append(rs)\n        num_repeat -= r\n        num_repeat_scaled -= rs\n    repeats_scaled = repeats_scaled[::-1]\n\n    # Apply the calculated scaling to each block arg in the stage\n    sa_scaled = []\n    for ba, rep in zip(stack_args, repeats_scaled):\n        sa_scaled.extend([deepcopy(ba) for _ in range(rep)])\n    return sa_scaled\n\n\ndef decode_arch_def(arch_def, depth_multiplier=1.0, depth_trunc=\'ceil\', experts_multiplier=1, fix_first_last=False):\n    arch_args = []\n    for stack_idx, block_strings in enumerate(arch_def):\n        assert isinstance(block_strings, list)\n        stack_args = []\n        repeats = []\n        for block_str in block_strings:\n            assert isinstance(block_str, str)\n            ba, rep = _decode_block_str(block_str)\n            if ba.get(\'num_experts\', 0) > 0 and experts_multiplier > 1:\n                ba[\'num_experts\'] *= experts_multiplier\n            stack_args.append(ba)\n            repeats.append(rep)\n        if fix_first_last and (stack_idx == 0 or stack_idx == len(arch_def) - 1):\n            arch_args.append(_scale_stage_depth(stack_args, repeats, 1.0, depth_trunc))\n        else:\n            arch_args.append(_scale_stage_depth(stack_args, repeats, depth_multiplier, depth_trunc))\n    return arch_args\n\n\ndef initialize_weight_goog(m, n=\'\', fix_group_fanout=True):\n    # weight init as per Tensorflow Official impl\n    # https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mnasnet_model.py\n    if isinstance(m, CondConv2d):\n        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        if fix_group_fanout:\n            fan_out //= m.groups\n        init_weight_fn = get_condconv_initializer(\n            lambda w: w.data.normal_(0, math.sqrt(2.0 / fan_out)), m.num_experts, m.weight_shape)\n        init_weight_fn(m.weight)\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.Conv2d):\n        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        if fix_group_fanout:\n            fan_out //= m.groups\n        m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1.0)\n        m.bias.data.zero_()\n    elif isinstance(m, nn.Linear):\n        fan_out = m.weight.size(0)  # fan-out\n        fan_in = 0\n        if \'routing_fn\' in n:\n            fan_in = m.weight.size(1)\n        init_range = 1.0 / math.sqrt(fan_in + fan_out)\n        m.weight.data.uniform_(-init_range, init_range)\n        m.bias.data.zero_()\n\n\ndef initialize_weight_default(m, n=\'\'):\n    if isinstance(m, CondConv2d):\n        init_fn = get_condconv_initializer(partial(\n            nn.init.kaiming_normal_, mode=\'fan_out\', nonlinearity=\'relu\'), m.num_experts, m.weight_shape)\n        init_fn(m.weight)\n    elif isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1.0)\n        m.bias.data.zero_()\n    elif isinstance(m, nn.Linear):\n        nn.init.kaiming_uniform_(m.weight, mode=\'fan_in\', nonlinearity=\'linear\')\n'"
geffnet/gen_efficientnet.py,2,"b'"""""" Generic Efficient Networks\n\nA generic MobileNet class with building blocks to support a variety of models:\n\n* EfficientNet (B0-B8, L2 + Tensorflow pretrained AutoAug/RandAug/AdvProp/NoisyStudent ports)\n  - EfficientNet: Rethinking Model Scaling for CNNs - https://arxiv.org/abs/1905.11946\n  - CondConv: Conditionally Parameterized Convolutions for Efficient Inference - https://arxiv.org/abs/1904.04971\n  - Adversarial Examples Improve Image Recognition - https://arxiv.org/abs/1911.09665\n  - Self-training with Noisy Student improves ImageNet classification - https://arxiv.org/abs/1911.04252\n\n* EfficientNet-Lite\n\n* MixNet (Small, Medium, and Large)\n  - MixConv: Mixed Depthwise Convolutional Kernels - https://arxiv.org/abs/1907.09595\n\n* MNasNet B1, A1 (SE), Small\n  - MnasNet: Platform-Aware Neural Architecture Search for Mobile - https://arxiv.org/abs/1807.11626\n\n* FBNet-C\n  - FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable NAS - https://arxiv.org/abs/1812.03443\n\n* Single-Path NAS Pixel1\n  - Single-Path NAS: Designing Hardware-Efficient ConvNets - https://arxiv.org/abs/1904.02877\n\n* And likely more...\n\nHacked together by Ross Wightman\n""""""\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .helpers import load_pretrained\nfrom .efficientnet_builder import *\n\n__all__ = [\'GenEfficientNet\', \'mnasnet_050\', \'mnasnet_075\', \'mnasnet_100\', \'mnasnet_b1\', \'mnasnet_140\',\n           \'semnasnet_050\', \'semnasnet_075\', \'semnasnet_100\', \'mnasnet_a1\', \'semnasnet_140\', \'mnasnet_small\',\n           \'mobilenetv2_100\', \'mobilenetv2_140\', \'mobilenetv2_110d\', \'mobilenetv2_120d\',\n           \'fbnetc_100\', \'spnasnet_100\', \'efficientnet_b0\', \'efficientnet_b1\', \'efficientnet_b2\',  \'efficientnet_b3\',\n           \'efficientnet_b4\', \'efficientnet_b5\', \'efficientnet_b6\', \'efficientnet_b7\', \'efficientnet_b8\',\n           \'efficientnet_l2\', \'efficientnet_es\', \'efficientnet_em\', \'efficientnet_el\',\n           \'efficientnet_cc_b0_4e\', \'efficientnet_cc_b0_8e\', \'efficientnet_cc_b1_8e\',\n           \'efficientnet_lite0\', \'efficientnet_lite1\', \'efficientnet_lite2\', \'efficientnet_lite3\', \'efficientnet_lite4\',\n           \'tf_efficientnet_b0\', \'tf_efficientnet_b1\', \'tf_efficientnet_b2\', \'tf_efficientnet_b3\',\n           \'tf_efficientnet_b4\', \'tf_efficientnet_b5\', \'tf_efficientnet_b6\', \'tf_efficientnet_b7\', \'tf_efficientnet_b8\',\n           \'tf_efficientnet_b0_ap\', \'tf_efficientnet_b1_ap\', \'tf_efficientnet_b2_ap\', \'tf_efficientnet_b3_ap\',\n           \'tf_efficientnet_b4_ap\', \'tf_efficientnet_b5_ap\', \'tf_efficientnet_b6_ap\', \'tf_efficientnet_b7_ap\',\n           \'tf_efficientnet_b8_ap\', \'tf_efficientnet_b0_ns\', \'tf_efficientnet_b1_ns\', \'tf_efficientnet_b2_ns\',\n           \'tf_efficientnet_b3_ns\', \'tf_efficientnet_b4_ns\', \'tf_efficientnet_b5_ns\', \'tf_efficientnet_b6_ns\',\n           \'tf_efficientnet_b7_ns\', \'tf_efficientnet_l2_ns\', \'tf_efficientnet_l2_ns_475\',\n           \'tf_efficientnet_es\', \'tf_efficientnet_em\', \'tf_efficientnet_el\',\n           \'tf_efficientnet_cc_b0_4e\', \'tf_efficientnet_cc_b0_8e\', \'tf_efficientnet_cc_b1_8e\',\n           \'tf_efficientnet_lite0\', \'tf_efficientnet_lite1\', \'tf_efficientnet_lite2\', \'tf_efficientnet_lite3\',\n           \'tf_efficientnet_lite4\',\n           \'mixnet_s\', \'mixnet_m\', \'mixnet_l\', \'mixnet_xl\', \'tf_mixnet_s\', \'tf_mixnet_m\', \'tf_mixnet_l\']\n\n\nmodel_urls = {\n    \'mnasnet_050\': None,\n    \'mnasnet_075\': None,\n    \'mnasnet_100\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mnasnet_b1-74cb7081.pth\',\n    \'mnasnet_140\': None,\n    \'mnasnet_small\': None,\n\n    \'semnasnet_050\': None,\n    \'semnasnet_075\': None,\n    \'semnasnet_100\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mnasnet_a1-d9418771.pth\',\n    \'semnasnet_140\': None,\n\n    \'mobilenetv2_100\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_100_ra-b33bc2c4.pth\',\n    \'mobilenetv2_110d\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_110d_ra-77090ade.pth\',\n    \'mobilenetv2_120d\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_120d_ra-5987e2ed.pth\',\n    \'mobilenetv2_140\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_140_ra-21a4e913.pth\',\n\n    \'fbnetc_100\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/fbnetc_100-c345b898.pth\',\n    \'spnasnet_100\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/spnasnet_100-048bc3f4.pth\',\n\n    \'efficientnet_b0\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b0_ra-3dd342df.pth\',\n    \'efficientnet_b1\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b1-533bc792.pth\',\n    \'efficientnet_b2\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b2_ra-bcdf34b7.pth\',\n    \'efficientnet_b3\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra-a5e2fbc7.pth\',\n    \'efficientnet_b4\': None,\n    \'efficientnet_b5\': None,\n    \'efficientnet_b6\': None,\n    \'efficientnet_b7\': None,\n    \'efficientnet_b8\': None,\n    \'efficientnet_l2\': None,\n\n    \'efficientnet_es\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_es_ra-f111e99c.pth\',\n    \'efficientnet_em\': None,\n    \'efficientnet_el\': None,\n\n    \'efficientnet_cc_b0_4e\': None,\n    \'efficientnet_cc_b0_8e\': None,\n    \'efficientnet_cc_b1_8e\': None,\n\n    \'efficientnet_lite0\': None,\n    \'efficientnet_lite1\': None,\n    \'efficientnet_lite2\': None,\n    \'efficientnet_lite3\': None,\n    \'efficientnet_lite4\': None,\n\n    \'tf_efficientnet_b0\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_aa-827b6e33.pth\',\n    \'tf_efficientnet_b1\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_aa-ea7a6ee0.pth\',\n    \'tf_efficientnet_b2\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_aa-60c94f97.pth\',\n    \'tf_efficientnet_b3\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3_aa-84b4657e.pth\',\n    \'tf_efficientnet_b4\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_aa-818f208c.pth\',\n    \'tf_efficientnet_b5\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ra-9a3e5369.pth\',\n    \'tf_efficientnet_b6\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_aa-80ba17e4.pth\',\n    \'tf_efficientnet_b7\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ra-6c08e654.pth\',\n    \'tf_efficientnet_b8\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b8_ra-572d5dd9.pth\',\n\n    \'tf_efficientnet_b0_ap\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ap-f262efe1.pth\',\n    \'tf_efficientnet_b1_ap\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_ap-44ef0a3d.pth\',\n    \'tf_efficientnet_b2_ap\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_ap-2f8e7636.pth\',\n    \'tf_efficientnet_b3_ap\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3_ap-aad25bdd.pth\',\n    \'tf_efficientnet_b4_ap\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ap-dedb23e6.pth\',\n    \'tf_efficientnet_b5_ap\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ap-9e82fae8.pth\',\n    \'tf_efficientnet_b6_ap\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_ap-4ffb161f.pth\',\n    \'tf_efficientnet_b7_ap\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ap-ddb28fec.pth\',\n    \'tf_efficientnet_b8_ap\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b8_ap-00e169fa.pth\',\n\n    \'tf_efficientnet_b0_ns\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ns-c0e6a31c.pth\',\n    \'tf_efficientnet_b1_ns\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_ns-99dd0c41.pth\',\n    \'tf_efficientnet_b2_ns\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_ns-00306e48.pth\',\n    \'tf_efficientnet_b3_ns\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3_ns-9d44bf68.pth\',\n    \'tf_efficientnet_b4_ns\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth\',\n    \'tf_efficientnet_b5_ns\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ns-6f26d0cf.pth\',\n    \'tf_efficientnet_b6_ns\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_ns-51548356.pth\',\n    \'tf_efficientnet_b7_ns\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ns-1dbc32de.pth\',\n    \'tf_efficientnet_l2_ns_475\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_l2_ns_475-bebbd00a.pth\',\n    \'tf_efficientnet_l2_ns\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_l2_ns-df73bb44.pth\',\n\n    \'tf_efficientnet_es\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_es-ca1afbfe.pth\',\n    \'tf_efficientnet_em\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_em-e78cfe58.pth\',\n    \'tf_efficientnet_el\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_el-5143854e.pth\',\n\n    \'tf_efficientnet_cc_b0_4e\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_cc_b0_4e-4362b6b2.pth\',\n    \'tf_efficientnet_cc_b0_8e\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_cc_b0_8e-66184a25.pth\',\n    \'tf_efficientnet_cc_b1_8e\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_cc_b1_8e-f7c79ae1.pth\',\n\n    \'tf_efficientnet_lite0\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite0-0aa007d2.pth\',\n    \'tf_efficientnet_lite1\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite1-bde8b488.pth\',\n    \'tf_efficientnet_lite2\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite2-dcccb7df.pth\',\n    \'tf_efficientnet_lite3\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite3-b733e338.pth\',\n    \'tf_efficientnet_lite4\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite4-741542c3.pth\',\n\n    \'mixnet_s\': \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mixnet_s-a907afbc.pth\',\n    \'mixnet_m\': \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mixnet_m-4647fc68.pth\',\n    \'mixnet_l\': \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mixnet_l-5a9a2ed8.pth\',\n    \'mixnet_xl\': \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mixnet_xl_ra-aac3c00c.pth\',\n\n    \'tf_mixnet_s\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_s-89d3354b.pth\',\n    \'tf_mixnet_m\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_m-0f4d8805.pth\',\n    \'tf_mixnet_l\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_l-6c92e0c8.pth\',\n}\n\n\nclass GenEfficientNet(nn.Module):\n    """""" Generic EfficientNets\n\n    An implementation of mobile optimized networks that covers:\n      * EfficientNet (B0-B8, L2, CondConv, EdgeTPU)\n      * MixNet (Small, Medium, and Large, XL)\n      * MNASNet A1, B1, and small\n      * FBNet C\n      * Single-Path NAS Pixel1\n    """"""\n\n    def __init__(self, block_args, num_classes=1000, in_chans=3, num_features=1280, stem_size=32, fix_stem=False,\n                 channel_multiplier=1.0, channel_divisor=8, channel_min=None,\n                 pad_type=\'\', act_layer=nn.ReLU, drop_rate=0., drop_connect_rate=0.,\n                 se_kwargs=None, norm_layer=nn.BatchNorm2d, norm_kwargs=None,\n                 weight_init=\'goog\'):\n        super(GenEfficientNet, self).__init__()\n        self.drop_rate = drop_rate\n\n        if not fix_stem:\n            stem_size = round_channels(stem_size, channel_multiplier, channel_divisor, channel_min)\n        self.conv_stem = select_conv2d(in_chans, stem_size, 3, stride=2, padding=pad_type)\n        self.bn1 = norm_layer(stem_size, **norm_kwargs)\n        self.act1 = act_layer(inplace=True)\n        in_chs = stem_size\n\n        builder = EfficientNetBuilder(\n            channel_multiplier, channel_divisor, channel_min,\n            pad_type, act_layer, se_kwargs, norm_layer, norm_kwargs, drop_connect_rate)\n        self.blocks = nn.Sequential(*builder(in_chs, block_args))\n        in_chs = builder.in_chs\n\n        self.conv_head = select_conv2d(in_chs, num_features, 1, padding=pad_type)\n        self.bn2 = norm_layer(num_features, **norm_kwargs)\n        self.act2 = act_layer(inplace=True)\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        for n, m in self.named_modules():\n            if weight_init == \'goog\':\n                initialize_weight_goog(m, n)\n            else:\n                initialize_weight_default(m, n)\n\n    def features(self, x):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.blocks(x)\n        x = self.conv_head(x)\n        x = self.bn2(x)\n        x = self.act2(x)\n        return x\n\n    def as_sequential(self):\n        layers = [self.conv_stem, self.bn1, self.act1]\n        layers.extend(self.blocks)\n        layers.extend([\n            self.conv_head, self.bn2, self.act2,\n            self.global_pool, nn.Flatten(), nn.Dropout(self.drop_rate), self.classifier])\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.global_pool(x)\n        x = x.flatten(1)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        return self.classifier(x)\n\n\ndef _create_model(model_kwargs, variant, pretrained=False):\n    as_sequential = model_kwargs.pop(\'as_sequential\', False)\n    model = GenEfficientNet(**model_kwargs)\n    if pretrained:\n        load_pretrained(model, model_urls[variant])\n    if as_sequential:\n        model = model.as_sequential()\n    return model\n\n\ndef _gen_mnasnet_a1(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates a mnasnet-a1 model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet\n    Paper: https://arxiv.org/pdf/1807.11626.pdf.\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    """"""\n    arch_def = [\n        # stage 0, 112x112 in\n        [\'ds_r1_k3_s1_e1_c16_noskip\'],\n        # stage 1, 112x112 in\n        [\'ir_r2_k3_s2_e6_c24\'],\n        # stage 2, 56x56 in\n        [\'ir_r3_k5_s2_e3_c40_se0.25\'],\n        # stage 3, 28x28 in\n        [\'ir_r4_k3_s2_e6_c80\'],\n        # stage 4, 14x14in\n        [\'ir_r2_k3_s1_e6_c112_se0.25\'],\n        # stage 5, 14x14in\n        [\'ir_r3_k5_s2_e6_c160_se0.25\'],\n        # stage 6, 7x7 in\n        [\'ir_r1_k3_s1_e6_c320\'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        act_layer=resolve_act_layer(kwargs, \'relu\'),\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs\n    )\n    model = _create_model(model_kwargs, variant, pretrained)\n    return model\n\n\ndef _gen_mnasnet_b1(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates a mnasnet-b1 model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet\n    Paper: https://arxiv.org/pdf/1807.11626.pdf.\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    """"""\n    arch_def = [\n        # stage 0, 112x112 in\n        [\'ds_r1_k3_s1_c16_noskip\'],\n        # stage 1, 112x112 in\n        [\'ir_r3_k3_s2_e3_c24\'],\n        # stage 2, 56x56 in\n        [\'ir_r3_k5_s2_e3_c40\'],\n        # stage 3, 28x28 in\n        [\'ir_r3_k5_s2_e6_c80\'],\n        # stage 4, 14x14in\n        [\'ir_r2_k3_s1_e6_c96\'],\n        # stage 5, 14x14in\n        [\'ir_r4_k5_s2_e6_c192\'],\n        # stage 6, 7x7 in\n        [\'ir_r1_k3_s1_e6_c320_noskip\']\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        act_layer=resolve_act_layer(kwargs, \'relu\'),\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs\n    )\n    model = _create_model(model_kwargs, variant, pretrained)\n    return model\n\n\ndef _gen_mnasnet_small(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates a mnasnet-b1 model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet\n    Paper: https://arxiv.org/pdf/1807.11626.pdf.\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    """"""\n    arch_def = [\n        [\'ds_r1_k3_s1_c8\'],\n        [\'ir_r1_k3_s2_e3_c16\'],\n        [\'ir_r2_k3_s2_e6_c16\'],\n        [\'ir_r4_k5_s2_e6_c32_se0.25\'],\n        [\'ir_r3_k3_s1_e6_c32_se0.25\'],\n        [\'ir_r3_k5_s2_e6_c88_se0.25\'],\n        [\'ir_r1_k3_s1_e6_c144\']\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=8,\n        channel_multiplier=channel_multiplier,\n        act_layer=resolve_act_layer(kwargs, \'relu\'),\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs\n    )\n    model = _create_model(model_kwargs, variant, pretrained)\n    return model\n\n\ndef _gen_mobilenet_v2(\n        variant, channel_multiplier=1.0, depth_multiplier=1.0, fix_stem_head=False, pretrained=False, **kwargs):\n    """""" Generate MobileNet-V2 network\n    Ref impl: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py\n    Paper: https://arxiv.org/abs/1801.04381\n    """"""\n    arch_def = [\n        [\'ds_r1_k3_s1_c16\'],\n        [\'ir_r2_k3_s2_e6_c24\'],\n        [\'ir_r3_k3_s2_e6_c32\'],\n        [\'ir_r4_k3_s2_e6_c64\'],\n        [\'ir_r3_k3_s1_e6_c96\'],\n        [\'ir_r3_k3_s2_e6_c160\'],\n        [\'ir_r1_k3_s1_e6_c320\'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier=depth_multiplier, fix_first_last=fix_stem_head),\n        num_features=1280 if fix_stem_head else round_channels(1280, channel_multiplier, 8, None),\n        stem_size=32,\n        fix_stem=fix_stem_head,\n        channel_multiplier=channel_multiplier,\n        norm_kwargs=resolve_bn_args(kwargs),\n        act_layer=nn.ReLU6,\n        **kwargs\n    )\n    model = _create_model(model_kwargs, variant, pretrained)\n    return model\n\n\ndef _gen_fbnetc(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """""" FBNet-C\n\n        Paper: https://arxiv.org/abs/1812.03443\n        Ref Impl: https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/modeling/backbone/fbnet_modeldef.py\n\n        NOTE: the impl above does not relate to the \'C\' variant here, that was derived from paper,\n        it was used to confirm some building block details\n    """"""\n    arch_def = [\n        [\'ir_r1_k3_s1_e1_c16\'],\n        [\'ir_r1_k3_s2_e6_c24\', \'ir_r2_k3_s1_e1_c24\'],\n        [\'ir_r1_k5_s2_e6_c32\', \'ir_r1_k5_s1_e3_c32\', \'ir_r1_k5_s1_e6_c32\', \'ir_r1_k3_s1_e6_c32\'],\n        [\'ir_r1_k5_s2_e6_c64\', \'ir_r1_k5_s1_e3_c64\', \'ir_r2_k5_s1_e6_c64\'],\n        [\'ir_r3_k5_s1_e6_c112\', \'ir_r1_k5_s1_e3_c112\'],\n        [\'ir_r4_k5_s2_e6_c184\'],\n        [\'ir_r1_k3_s1_e6_c352\'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=16,\n        num_features=1984,  # paper suggests this, but is not 100% clear\n        channel_multiplier=channel_multiplier,\n        act_layer=resolve_act_layer(kwargs, \'relu\'),\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs\n    )\n    model = _create_model(model_kwargs, variant, pretrained)\n    return model\n\n\ndef _gen_spnasnet(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates the Single-Path NAS model from search targeted for Pixel1 phone.\n\n    Paper: https://arxiv.org/abs/1904.02877\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    """"""\n    arch_def = [\n        # stage 0, 112x112 in\n        [\'ds_r1_k3_s1_c16_noskip\'],\n        # stage 1, 112x112 in\n        [\'ir_r3_k3_s2_e3_c24\'],\n        # stage 2, 56x56 in\n        [\'ir_r1_k5_s2_e6_c40\', \'ir_r3_k3_s1_e3_c40\'],\n        # stage 3, 28x28 in\n        [\'ir_r1_k5_s2_e6_c80\', \'ir_r3_k3_s1_e3_c80\'],\n        # stage 4, 14x14in\n        [\'ir_r1_k5_s1_e6_c96\', \'ir_r3_k5_s1_e3_c96\'],\n        # stage 5, 14x14in\n        [\'ir_r4_k5_s2_e6_c192\'],\n        # stage 6, 7x7 in\n        [\'ir_r1_k3_s1_e6_c320_noskip\']\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        act_layer=resolve_act_layer(kwargs, \'relu\'),\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs\n    )\n    model = _create_model(model_kwargs, variant, pretrained)\n    return model\n\n\ndef _gen_efficientnet(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates an EfficientNet model.\n\n    Ref impl: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py\n    Paper: https://arxiv.org/abs/1905.11946\n\n    EfficientNet params\n    name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)\n    \'efficientnet-b0\': (1.0, 1.0, 224, 0.2),\n    \'efficientnet-b1\': (1.0, 1.1, 240, 0.2),\n    \'efficientnet-b2\': (1.1, 1.2, 260, 0.3),\n    \'efficientnet-b3\': (1.2, 1.4, 300, 0.3),\n    \'efficientnet-b4\': (1.4, 1.8, 380, 0.4),\n    \'efficientnet-b5\': (1.6, 2.2, 456, 0.4),\n    \'efficientnet-b6\': (1.8, 2.6, 528, 0.5),\n    \'efficientnet-b7\': (2.0, 3.1, 600, 0.5),\n    \'efficientnet-b8\': (2.2, 3.6, 672, 0.5),\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer\n      depth_multiplier: multiplier to number of repeats per stage\n\n    """"""\n    arch_def = [\n        [\'ds_r1_k3_s1_e1_c16_se0.25\'],\n        [\'ir_r2_k3_s2_e6_c24_se0.25\'],\n        [\'ir_r2_k5_s2_e6_c40_se0.25\'],\n        [\'ir_r3_k3_s2_e6_c80_se0.25\'],\n        [\'ir_r3_k5_s1_e6_c112_se0.25\'],\n        [\'ir_r4_k5_s2_e6_c192_se0.25\'],\n        [\'ir_r1_k3_s1_e6_c320_se0.25\'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier),\n        num_features=round_channels(1280, channel_multiplier, 8, None),\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        act_layer=resolve_act_layer(kwargs, \'swish\'),\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs,\n    )\n    model = _create_model(model_kwargs, variant, pretrained)\n    return model\n\n\ndef _gen_efficientnet_edge(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):\n    arch_def = [\n        # NOTE `fc` is present to override a mismatch between stem channels and in chs not\n        # present in other models\n        [\'er_r1_k3_s1_e4_c24_fc24_noskip\'],\n        [\'er_r2_k3_s2_e8_c32\'],\n        [\'er_r4_k3_s2_e8_c48\'],\n        [\'ir_r5_k5_s2_e8_c96\'],\n        [\'ir_r4_k5_s1_e8_c144\'],\n        [\'ir_r2_k5_s2_e8_c192\'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier),\n        num_features=round_channels(1280, channel_multiplier, 8, None),\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        act_layer=resolve_act_layer(kwargs, \'relu\'),\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs,\n    )\n    model = _create_model(model_kwargs, variant, pretrained)\n    return model\n\n\ndef _gen_efficientnet_condconv(\n        variant, channel_multiplier=1.0, depth_multiplier=1.0, experts_multiplier=1, pretrained=False, **kwargs):\n    """"""Creates an efficientnet-condconv model.""""""\n    arch_def = [\n      [\'ds_r1_k3_s1_e1_c16_se0.25\'],\n      [\'ir_r2_k3_s2_e6_c24_se0.25\'],\n      [\'ir_r2_k5_s2_e6_c40_se0.25\'],\n      [\'ir_r3_k3_s2_e6_c80_se0.25\'],\n      [\'ir_r3_k5_s1_e6_c112_se0.25_cc4\'],\n      [\'ir_r4_k5_s2_e6_c192_se0.25_cc4\'],\n      [\'ir_r1_k3_s1_e6_c320_se0.25_cc4\'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier, experts_multiplier=experts_multiplier),\n        num_features=round_channels(1280, channel_multiplier, 8, None),\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        act_layer=resolve_act_layer(kwargs, \'swish\'),\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs,\n    )\n    model = _create_model(model_kwargs, variant, pretrained)\n    return model\n\n\ndef _gen_efficientnet_lite(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates an EfficientNet-Lite model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite\n    Paper: https://arxiv.org/abs/1905.11946\n\n    EfficientNet params\n    name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)\n      \'efficientnet-lite0\': (1.0, 1.0, 224, 0.2),\n      \'efficientnet-lite1\': (1.0, 1.1, 240, 0.2),\n      \'efficientnet-lite2\': (1.1, 1.2, 260, 0.3),\n      \'efficientnet-lite3\': (1.2, 1.4, 280, 0.3),\n      \'efficientnet-lite4\': (1.4, 1.8, 300, 0.3),\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer\n      depth_multiplier: multiplier to number of repeats per stage\n    """"""\n    arch_def = [\n        [\'ds_r1_k3_s1_e1_c16\'],\n        [\'ir_r2_k3_s2_e6_c24\'],\n        [\'ir_r2_k5_s2_e6_c40\'],\n        [\'ir_r3_k3_s2_e6_c80\'],\n        [\'ir_r3_k5_s1_e6_c112\'],\n        [\'ir_r4_k5_s2_e6_c192\'],\n        [\'ir_r1_k3_s1_e6_c320\'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier, fix_first_last=True),\n        num_features=1280,\n        stem_size=32,\n        fix_stem=True,\n        channel_multiplier=channel_multiplier,\n        act_layer=nn.ReLU6,\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs,\n    )\n    model = _create_model(model_kwargs, variant, pretrained)\n    return model\n\n\ndef _gen_mixnet_s(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates a MixNet Small model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet\n    Paper: https://arxiv.org/abs/1907.09595\n    """"""\n    arch_def = [\n        # stage 0, 112x112 in\n        [\'ds_r1_k3_s1_e1_c16\'],  # relu\n        # stage 1, 112x112 in\n        [\'ir_r1_k3_a1.1_p1.1_s2_e6_c24\', \'ir_r1_k3_a1.1_p1.1_s1_e3_c24\'],  # relu\n        # stage 2, 56x56 in\n        [\'ir_r1_k3.5.7_s2_e6_c40_se0.5_nsw\', \'ir_r3_k3.5_a1.1_p1.1_s1_e6_c40_se0.5_nsw\'],  # swish\n        # stage 3, 28x28 in\n        [\'ir_r1_k3.5.7_p1.1_s2_e6_c80_se0.25_nsw\', \'ir_r2_k3.5_p1.1_s1_e6_c80_se0.25_nsw\'],  # swish\n        # stage 4, 14x14in\n        [\'ir_r1_k3.5.7_a1.1_p1.1_s1_e6_c120_se0.5_nsw\', \'ir_r2_k3.5.7.9_a1.1_p1.1_s1_e3_c120_se0.5_nsw\'],  # swish\n        # stage 5, 14x14in\n        [\'ir_r1_k3.5.7.9.11_s2_e6_c200_se0.5_nsw\', \'ir_r2_k3.5.7.9_p1.1_s1_e6_c200_se0.5_nsw\'],  # swish\n        # 7x7\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        num_features=1536,\n        stem_size=16,\n        channel_multiplier=channel_multiplier,\n        act_layer=resolve_act_layer(kwargs, \'relu\'),\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs\n    )\n    model = _create_model(model_kwargs, variant, pretrained)\n    return model\n\n\ndef _gen_mixnet_m(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates a MixNet Medium-Large model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet\n    Paper: https://arxiv.org/abs/1907.09595\n    """"""\n    arch_def = [\n        # stage 0, 112x112 in\n        [\'ds_r1_k3_s1_e1_c24\'],  # relu\n        # stage 1, 112x112 in\n        [\'ir_r1_k3.5.7_a1.1_p1.1_s2_e6_c32\', \'ir_r1_k3_a1.1_p1.1_s1_e3_c32\'],  # relu\n        # stage 2, 56x56 in\n        [\'ir_r1_k3.5.7.9_s2_e6_c40_se0.5_nsw\', \'ir_r3_k3.5_a1.1_p1.1_s1_e6_c40_se0.5_nsw\'],  # swish\n        # stage 3, 28x28 in\n        [\'ir_r1_k3.5.7_s2_e6_c80_se0.25_nsw\', \'ir_r3_k3.5.7.9_a1.1_p1.1_s1_e6_c80_se0.25_nsw\'],  # swish\n        # stage 4, 14x14in\n        [\'ir_r1_k3_s1_e6_c120_se0.5_nsw\', \'ir_r3_k3.5.7.9_a1.1_p1.1_s1_e3_c120_se0.5_nsw\'],  # swish\n        # stage 5, 14x14in\n        [\'ir_r1_k3.5.7.9_s2_e6_c200_se0.5_nsw\', \'ir_r3_k3.5.7.9_p1.1_s1_e6_c200_se0.5_nsw\'],  # swish\n        # 7x7\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier, depth_trunc=\'round\'),\n        num_features=1536,\n        stem_size=24,\n        channel_multiplier=channel_multiplier,\n        act_layer=resolve_act_layer(kwargs, \'relu\'),\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs\n    )\n    model = _create_model(model_kwargs, variant, pretrained)\n    return model\n\n\ndef mnasnet_050(pretrained=False, **kwargs):\n    """""" MNASNet B1, depth multiplier of 0.5. """"""\n    model = _gen_mnasnet_b1(\'mnasnet_050\', 0.5, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mnasnet_075(pretrained=False, **kwargs):\n    """""" MNASNet B1, depth multiplier of 0.75. """"""\n    model = _gen_mnasnet_b1(\'mnasnet_075\', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mnasnet_100(pretrained=False, **kwargs):\n    """""" MNASNet B1, depth multiplier of 1.0. """"""\n    model = _gen_mnasnet_b1(\'mnasnet_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mnasnet_b1(pretrained=False, **kwargs):\n    """""" MNASNet B1, depth multiplier of 1.0. """"""\n    return mnasnet_100(pretrained, **kwargs)\n\n\ndef mnasnet_140(pretrained=False, **kwargs):\n    """""" MNASNet B1,  depth multiplier of 1.4 """"""\n    model = _gen_mnasnet_b1(\'mnasnet_140\', 1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef semnasnet_050(pretrained=False, **kwargs):\n    """""" MNASNet A1 (w/ SE), depth multiplier of 0.5 """"""\n    model = _gen_mnasnet_a1(\'semnasnet_050\', 0.5, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef semnasnet_075(pretrained=False, **kwargs):\n    """""" MNASNet A1 (w/ SE),  depth multiplier of 0.75. """"""\n    model = _gen_mnasnet_a1(\'semnasnet_075\', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef semnasnet_100(pretrained=False, **kwargs):\n    """""" MNASNet A1 (w/ SE), depth multiplier of 1.0. """"""\n    model = _gen_mnasnet_a1(\'semnasnet_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mnasnet_a1(pretrained=False, **kwargs):\n    """""" MNASNet A1 (w/ SE), depth multiplier of 1.0. """"""\n    return semnasnet_100(pretrained, **kwargs)\n\n\ndef semnasnet_140(pretrained=False, **kwargs):\n    """""" MNASNet A1 (w/ SE), depth multiplier of 1.4. """"""\n    model = _gen_mnasnet_a1(\'semnasnet_140\', 1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mnasnet_small(pretrained=False, **kwargs):\n    """""" MNASNet Small,  depth multiplier of 1.0. """"""\n    model = _gen_mnasnet_small(\'mnasnet_small\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mobilenetv2_100(pretrained=False, **kwargs):\n    """""" MobileNet V2 w/ 1.0 channel multiplier """"""\n    model = _gen_mobilenet_v2(\'mobilenetv2_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mobilenetv2_140(pretrained=False, **kwargs):\n    """""" MobileNet V2 w/ 1.4 channel multiplier """"""\n    model = _gen_mobilenet_v2(\'mobilenetv2_140\', 1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mobilenetv2_110d(pretrained=False, **kwargs):\n    """""" MobileNet V2 w/ 1.1 channel, 1.2 depth multipliers""""""\n    model = _gen_mobilenet_v2(\n        \'mobilenetv2_110d\', 1.1, depth_multiplier=1.2, fix_stem_head=True, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mobilenetv2_120d(pretrained=False, **kwargs):\n    """""" MobileNet V2 w/ 1.2 channel, 1.4 depth multipliers """"""\n    model = _gen_mobilenet_v2(\n        \'mobilenetv2_120d\', 1.2, depth_multiplier=1.4, fix_stem_head=True, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef fbnetc_100(pretrained=False, **kwargs):\n    """""" FBNet-C """"""\n    if pretrained:\n        # pretrained model trained with non-default BN epsilon\n        kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    model = _gen_fbnetc(\'fbnetc_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef spnasnet_100(pretrained=False, **kwargs):\n    """""" Single-Path NAS Pixel1""""""\n    model = _gen_spnasnet(\'spnasnet_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_b0(pretrained=False, **kwargs):\n    """""" EfficientNet-B0 """"""\n    # NOTE for train set drop_rate=0.2, drop_connect_rate=0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b0\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_b1(pretrained=False, **kwargs):\n    """""" EfficientNet-B1 """"""\n    # NOTE for train set drop_rate=0.2, drop_connect_rate=0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b1\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_b2(pretrained=False, **kwargs):\n    """""" EfficientNet-B2 """"""\n    # NOTE for train set drop_rate=0.3, drop_connect_rate=0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b2\', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_b3(pretrained=False, **kwargs):\n    """""" EfficientNet-B3 """"""\n    # NOTE for train set drop_rate=0.3, drop_connect_rate=0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b3\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_b4(pretrained=False, **kwargs):\n    """""" EfficientNet-B4 """"""\n    # NOTE for train set drop_rate=0.4, drop_connect_rate=0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b4\', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_b5(pretrained=False, **kwargs):\n    """""" EfficientNet-B5 """"""\n    # NOTE for train set drop_rate=0.4, drop_connect_rate=0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b5\', channel_multiplier=1.6, depth_multiplier=2.2, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_b6(pretrained=False, **kwargs):\n    """""" EfficientNet-B6 """"""\n    # NOTE for train set drop_rate=0.5, drop_connect_rate=0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b6\', channel_multiplier=1.8, depth_multiplier=2.6, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_b7(pretrained=False, **kwargs):\n    """""" EfficientNet-B7 """"""\n    # NOTE for train set drop_rate=0.5, drop_connect_rate=0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b7\', channel_multiplier=2.0, depth_multiplier=3.1, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_b8(pretrained=False, **kwargs):\n    """""" EfficientNet-B8 """"""\n    # NOTE for train set drop_rate=0.5, drop_connect_rate=0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b8\', channel_multiplier=2.2, depth_multiplier=3.6, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_l2(pretrained=False, **kwargs):\n    """""" EfficientNet-L2. """"""\n    # NOTE for train, drop_rate should be 0.5\n    model = _gen_efficientnet(\n        \'efficientnet_l2\', channel_multiplier=4.3, depth_multiplier=5.3, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_es(pretrained=False, **kwargs):\n    """""" EfficientNet-Edge Small. """"""\n    model = _gen_efficientnet_edge(\n        \'efficientnet_es\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_em(pretrained=False, **kwargs):\n    """""" EfficientNet-Edge-Medium. """"""\n    model = _gen_efficientnet_edge(\n        \'efficientnet_em\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_el(pretrained=False, **kwargs):\n    """""" EfficientNet-Edge-Large. """"""\n    model = _gen_efficientnet_edge(\n        \'efficientnet_el\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_cc_b0_4e(pretrained=False, **kwargs):\n    """""" EfficientNet-CondConv-B0 w/ 8 Experts """"""\n    # NOTE for train set drop_rate=0.25, drop_connect_rate=0.2\n    model = _gen_efficientnet_condconv(\n        \'efficientnet_cc_b0_4e\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_cc_b0_8e(pretrained=False, **kwargs):\n    """""" EfficientNet-CondConv-B0 w/ 8 Experts """"""\n    # NOTE for train set drop_rate=0.25, drop_connect_rate=0.2\n    model = _gen_efficientnet_condconv(\n        \'efficientnet_cc_b0_8e\', channel_multiplier=1.0, depth_multiplier=1.0, experts_multiplier=2,\n        pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_cc_b1_8e(pretrained=False, **kwargs):\n    """""" EfficientNet-CondConv-B1 w/ 8 Experts """"""\n    # NOTE for train set drop_rate=0.25, drop_connect_rate=0.2\n    model = _gen_efficientnet_condconv(\n        \'efficientnet_cc_b1_8e\', channel_multiplier=1.0, depth_multiplier=1.1, experts_multiplier=2,\n        pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_lite0(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite0 """"""\n    model = _gen_efficientnet_lite(\n        \'efficientnet_lite0\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_lite1(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite1 """"""\n    model = _gen_efficientnet_lite(\n        \'efficientnet_lite1\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_lite2(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite2 """"""\n    model = _gen_efficientnet_lite(\n        \'efficientnet_lite2\', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_lite3(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite3 """"""\n    model = _gen_efficientnet_lite(\n        \'efficientnet_lite3\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef efficientnet_lite4(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite4 """"""\n    model = _gen_efficientnet_lite(\n        \'efficientnet_lite4\', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b0(pretrained=False, **kwargs):\n    """""" EfficientNet-B0 AutoAug. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b0\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b1(pretrained=False, **kwargs):\n    """""" EfficientNet-B1 AutoAug. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b1\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b2(pretrained=False, **kwargs):\n    """""" EfficientNet-B2 AutoAug. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b2\', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b3(pretrained=False, **kwargs):\n    """""" EfficientNet-B3 AutoAug. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b3\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b4(pretrained=False, **kwargs):\n    """""" EfficientNet-B4 AutoAug. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b4\', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b5(pretrained=False, **kwargs):\n    """""" EfficientNet-B5 RandAug. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b5\', channel_multiplier=1.6, depth_multiplier=2.2, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b6(pretrained=False, **kwargs):\n    """""" EfficientNet-B6 AutoAug. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b6\', channel_multiplier=1.8, depth_multiplier=2.6, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b7(pretrained=False, **kwargs):\n    """""" EfficientNet-B7 RandAug. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b7\', channel_multiplier=2.0, depth_multiplier=3.1, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b8(pretrained=False, **kwargs):\n    """""" EfficientNet-B8 RandAug. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b8\', channel_multiplier=2.2, depth_multiplier=3.6, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b0_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B0 AdvProp. Tensorflow compatible variant\n    Paper: Adversarial Examples Improve Image Recognition (https://arxiv.org/abs/1911.09665)\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b0_ap\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b1_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B1 AdvProp. Tensorflow compatible variant\n    Paper: Adversarial Examples Improve Image Recognition (https://arxiv.org/abs/1911.09665)\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b1_ap\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b2_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B2 AdvProp. Tensorflow compatible variant\n    Paper: Adversarial Examples Improve Image Recognition (https://arxiv.org/abs/1911.09665)\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b2_ap\', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b3_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B3 AdvProp. Tensorflow compatible variant\n    Paper: Adversarial Examples Improve Image Recognition (https://arxiv.org/abs/1911.09665)\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b3_ap\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b4_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B4 AdvProp. Tensorflow compatible variant\n    Paper: Adversarial Examples Improve Image Recognition (https://arxiv.org/abs/1911.09665)\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b4_ap\', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b5_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B5 AdvProp. Tensorflow compatible variant\n    Paper: Adversarial Examples Improve Image Recognition (https://arxiv.org/abs/1911.09665)\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b5_ap\', channel_multiplier=1.6, depth_multiplier=2.2, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b6_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B6 AdvProp. Tensorflow compatible variant\n    Paper: Adversarial Examples Improve Image Recognition (https://arxiv.org/abs/1911.09665)\n    """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b6_ap\', channel_multiplier=1.8, depth_multiplier=2.6, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b7_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B7 AdvProp. Tensorflow compatible variant\n    Paper: Adversarial Examples Improve Image Recognition (https://arxiv.org/abs/1911.09665)\n    """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b7_ap\', channel_multiplier=2.0, depth_multiplier=3.1, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b8_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B8 AdvProp. Tensorflow compatible variant\n    Paper: Adversarial Examples Improve Image Recognition (https://arxiv.org/abs/1911.09665)\n    """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b8_ap\', channel_multiplier=2.2, depth_multiplier=3.6, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b0_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B0 NoisyStudent. Tensorflow compatible variant\n    Paper: Self-training with Noisy Student improves ImageNet classification (https://arxiv.org/abs/1911.04252)\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b0_ns\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b1_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B1 NoisyStudent. Tensorflow compatible variant\n    Paper: Self-training with Noisy Student improves ImageNet classification (https://arxiv.org/abs/1911.04252)\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b1_ns\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b2_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B2 NoisyStudent. Tensorflow compatible variant\n    Paper: Self-training with Noisy Student improves ImageNet classification (https://arxiv.org/abs/1911.04252)\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b2_ns\', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b3_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B3 NoisyStudent. Tensorflow compatible variant\n    Paper: Self-training with Noisy Student improves ImageNet classification (https://arxiv.org/abs/1911.04252)\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b3_ns\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b4_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B4 NoisyStudent. Tensorflow compatible variant\n    Paper: Self-training with Noisy Student improves ImageNet classification (https://arxiv.org/abs/1911.04252)\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b4_ns\', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b5_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B5 NoisyStudent. Tensorflow compatible variant\n    Paper: Self-training with Noisy Student improves ImageNet classification (https://arxiv.org/abs/1911.04252)\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b5_ns\', channel_multiplier=1.6, depth_multiplier=2.2, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b6_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B6 NoisyStudent. Tensorflow compatible variant\n    Paper: Self-training with Noisy Student improves ImageNet classification (https://arxiv.org/abs/1911.04252)\n    """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b6_ns\', channel_multiplier=1.8, depth_multiplier=2.6, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_b7_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B7 NoisyStudent. Tensorflow compatible variant\n    Paper: Self-training with Noisy Student improves ImageNet classification (https://arxiv.org/abs/1911.04252)\n    """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b7_ns\', channel_multiplier=2.0, depth_multiplier=3.1, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_l2_ns_475(pretrained=False, **kwargs):\n    """""" EfficientNet-L2 NoisyStudent @ 475x475. Tensorflow compatible variant\n    Paper: Self-training with Noisy Student improves ImageNet classification (https://arxiv.org/abs/1911.04252)\n    """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_l2_ns_475\', channel_multiplier=4.3, depth_multiplier=5.3, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_l2_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-L2 NoisyStudent. Tensorflow compatible variant\n    Paper: Self-training with Noisy Student improves ImageNet classification (https://arxiv.org/abs/1911.04252)\n    """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_l2_ns\', channel_multiplier=4.3, depth_multiplier=5.3, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_es(pretrained=False, **kwargs):\n    """""" EfficientNet-Edge Small. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_edge(\n        \'tf_efficientnet_es\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_em(pretrained=False, **kwargs):\n    """""" EfficientNet-Edge-Medium. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_edge(\n        \'tf_efficientnet_em\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_el(pretrained=False, **kwargs):\n    """""" EfficientNet-Edge-Large. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_edge(\n        \'tf_efficientnet_el\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_cc_b0_4e(pretrained=False, **kwargs):\n    """""" EfficientNet-CondConv-B0 w/ 4 Experts """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_condconv(\n        \'tf_efficientnet_cc_b0_4e\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_cc_b0_8e(pretrained=False, **kwargs):\n    """""" EfficientNet-CondConv-B0 w/ 8 Experts """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_condconv(\n        \'tf_efficientnet_cc_b0_8e\', channel_multiplier=1.0, depth_multiplier=1.0, experts_multiplier=2,\n        pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_cc_b1_8e(pretrained=False, **kwargs):\n    """""" EfficientNet-CondConv-B1 w/ 8 Experts """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_condconv(\n        \'tf_efficientnet_cc_b1_8e\', channel_multiplier=1.0, depth_multiplier=1.1, experts_multiplier=2,\n        pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_lite0(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite0. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_lite(\n        \'tf_efficientnet_lite0\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_lite1(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite1. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_lite(\n        \'tf_efficientnet_lite1\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_lite2(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite2. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_lite(\n        \'tf_efficientnet_lite2\', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_lite3(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite3. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_lite(\n        \'tf_efficientnet_lite3\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_efficientnet_lite4(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite4. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_lite(\n        \'tf_efficientnet_lite4\', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mixnet_s(pretrained=False, **kwargs):\n    """"""Creates a MixNet Small model.\n    """"""\n    # NOTE for train set drop_rate=0.2\n    model = _gen_mixnet_s(\n        \'mixnet_s\', channel_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mixnet_m(pretrained=False, **kwargs):\n    """"""Creates a MixNet Medium model.\n    """"""\n    # NOTE for train set drop_rate=0.25\n    model = _gen_mixnet_m(\n        \'mixnet_m\', channel_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mixnet_l(pretrained=False, **kwargs):\n    """"""Creates a MixNet Large model.\n    """"""\n    # NOTE for train set drop_rate=0.25\n    model = _gen_mixnet_m(\n        \'mixnet_l\', channel_multiplier=1.3, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mixnet_xl(pretrained=False, **kwargs):\n    """"""Creates a MixNet Extra-Large model.\n    Not a paper spec, experimental def by RW w/ depth scaling.\n    """"""\n    # NOTE for train set drop_rate=0.25, drop_connect_rate=0.2\n    model = _gen_mixnet_m(\n        \'mixnet_xl\', channel_multiplier=1.6, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mixnet_xxl(pretrained=False, **kwargs):\n    """"""Creates a MixNet Double Extra Large model.\n    Not a paper spec, experimental def by RW w/ depth scaling.\n    """"""\n    # NOTE for train set drop_rate=0.3, drop_connect_rate=0.2\n    model = _gen_mixnet_m(\n        \'mixnet_xxl\', channel_multiplier=2.4, depth_multiplier=1.3, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_mixnet_s(pretrained=False, **kwargs):\n    """"""Creates a MixNet Small model. Tensorflow compatible variant\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mixnet_s(\n        \'tf_mixnet_s\', channel_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_mixnet_m(pretrained=False, **kwargs):\n    """"""Creates a MixNet Medium model. Tensorflow compatible variant\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mixnet_m(\n        \'tf_mixnet_m\', channel_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_mixnet_l(pretrained=False, **kwargs):\n    """"""Creates a MixNet Large model. Tensorflow compatible variant\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mixnet_m(\n        \'tf_mixnet_l\', channel_multiplier=1.3, pretrained=pretrained, **kwargs)\n    return model\n'"
geffnet/helpers.py,3,"b'import torch\nimport os\nfrom collections import OrderedDict\ntry:\n    from torch.hub import load_state_dict_from_url\nexcept ImportError:\n    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n\n\ndef load_checkpoint(model, checkpoint_path):\n    if checkpoint_path and os.path.isfile(checkpoint_path):\n        print(""=> Loading checkpoint \'{}\'"".format(checkpoint_path))\n        checkpoint = torch.load(checkpoint_path)\n        if isinstance(checkpoint, dict) and \'state_dict\' in checkpoint:\n            new_state_dict = OrderedDict()\n            for k, v in checkpoint[\'state_dict\'].items():\n                if k.startswith(\'module\'):\n                    name = k[7:]  # remove `module.`\n                else:\n                    name = k\n                new_state_dict[name] = v\n            model.load_state_dict(new_state_dict)\n        else:\n            model.load_state_dict(checkpoint)\n        print(""=> Loaded checkpoint \'{}\'"".format(checkpoint_path))\n    else:\n        print(""=> Error: No checkpoint found at \'{}\'"".format(checkpoint_path))\n        raise FileNotFoundError()\n\n\ndef load_pretrained(model, url, filter_fn=None, strict=True):\n    if not url:\n        print(""=> Warning: Pretrained model URL is empty, using random initialization."")\n        return\n\n    state_dict = load_state_dict_from_url(url, progress=False, map_location=\'cpu\')\n\n    input_conv = \'conv_stem\'\n    classifier = \'classifier\'\n    in_chans = getattr(model, input_conv).weight.shape[1]\n    num_classes = getattr(model, classifier).weight.shape[0]\n\n    input_conv_weight = input_conv + \'.weight\'\n    pretrained_in_chans = state_dict[input_conv_weight].shape[1]\n    if in_chans != pretrained_in_chans:\n        if in_chans == 1:\n            print(\'=> Converting pretrained input conv {} from {} to 1 channel\'.format(\n                input_conv_weight, pretrained_in_chans))\n            conv1_weight = state_dict[input_conv_weight]\n            state_dict[input_conv_weight] = conv1_weight.sum(dim=1, keepdim=True)\n        else:\n            print(\'=> Discarding pretrained input conv {} since input channel count != {}\'.format(\n                input_conv_weight, pretrained_in_chans))\n            del state_dict[input_conv_weight]\n            strict = False\n\n    classifier_weight = classifier + \'.weight\'\n    pretrained_num_classes = state_dict[classifier_weight].shape[0]\n    if num_classes != pretrained_num_classes:\n        print(\'=> Discarding pretrained classifier since num_classes != {}\'.format(pretrained_num_classes))\n        del state_dict[classifier_weight]\n        del state_dict[classifier + \'.bias\']\n        strict = False\n\n    if filter_fn is not None:\n        state_dict = filter_fn(state_dict)\n\n    model.load_state_dict(state_dict, strict=strict)\n'"
geffnet/mobilenetv3.py,2,"b'"""""" MobileNet-V3\n\nA PyTorch impl of MobileNet-V3, compatible with TF weights from official impl.\n\nPaper: Searching for MobileNetV3 - https://arxiv.org/abs/1905.02244\n\nHacked together by Ross Wightman\n""""""\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .helpers import load_pretrained\nfrom .efficientnet_builder import *\n\n__all__ = [\'mobilenetv3_rw\', \'mobilenetv3_large_075\', \'mobilenetv3_large_100\', \'mobilenetv3_large_minimal_100\',\n           \'mobilenetv3_small_075\', \'mobilenetv3_small_100\', \'mobilenetv3_small_minimal_100\',\n           \'tf_mobilenetv3_large_075\', \'tf_mobilenetv3_large_100\', \'tf_mobilenetv3_large_minimal_100\',\n           \'tf_mobilenetv3_small_075\', \'tf_mobilenetv3_small_100\', \'tf_mobilenetv3_small_minimal_100\']\n\nmodel_urls = {\n    \'mobilenetv3_rw\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_100-35495452.pth\',\n    \'mobilenetv3_large_075\': None,\n    \'mobilenetv3_large_100\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_large_100_ra-f55367f5.pth\',\n    \'mobilenetv3_large_minimal_100\': None,\n    \'mobilenetv3_small_075\': None,\n    \'mobilenetv3_small_100\': None,\n    \'mobilenetv3_small_minimal_100\': None,\n    \'tf_mobilenetv3_large_075\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_large_075-150ee8b0.pth\',\n    \'tf_mobilenetv3_large_100\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_large_100-427764d5.pth\',\n    \'tf_mobilenetv3_large_minimal_100\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_large_minimal_100-8596ae28.pth\',\n    \'tf_mobilenetv3_small_075\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_small_075-da427f52.pth\',\n    \'tf_mobilenetv3_small_100\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_small_100-37f49e2b.pth\',\n    \'tf_mobilenetv3_small_minimal_100\':\n        \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_small_minimal_100-922a7843.pth\',\n}\n\n\nclass MobileNetV3(nn.Module):\n    """""" MobileNet-V3\n\n    A this model utilizes the MobileNet-v3 specific \'efficient head\', where global pooling is done before the\n    head convolution without a final batch-norm layer before the classifier.\n\n    Paper: https://arxiv.org/abs/1905.02244\n    """"""\n\n    def __init__(self, block_args, num_classes=1000, in_chans=3, stem_size=16, num_features=1280, head_bias=True,\n                 channel_multiplier=1.0, pad_type=\'\', act_layer=HardSwish, drop_rate=0., drop_connect_rate=0.,\n                 se_kwargs=None, norm_layer=nn.BatchNorm2d, norm_kwargs=None, weight_init=\'goog\'):\n        super(MobileNetV3, self).__init__()\n        self.drop_rate = drop_rate\n\n        stem_size = round_channels(stem_size, channel_multiplier)\n        self.conv_stem = select_conv2d(in_chans, stem_size, 3, stride=2, padding=pad_type)\n        self.bn1 = nn.BatchNorm2d(stem_size, **norm_kwargs)\n        self.act1 = act_layer(inplace=True)\n        in_chs = stem_size\n\n        builder = EfficientNetBuilder(\n            channel_multiplier, pad_type=pad_type, act_layer=act_layer, se_kwargs=se_kwargs,\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs, drop_connect_rate=drop_connect_rate)\n        self.blocks = nn.Sequential(*builder(in_chs, block_args))\n        in_chs = builder.in_chs\n\n        self.global_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv_head = select_conv2d(in_chs, num_features, 1, padding=pad_type, bias=head_bias)\n        self.act2 = act_layer(inplace=True)\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        for m in self.modules():\n            if weight_init == \'goog\':\n                initialize_weight_goog(m)\n            else:\n                initialize_weight_default(m)\n\n    def as_sequential(self):\n        layers = [self.conv_stem, self.bn1, self.act1]\n        layers.extend(self.blocks)\n        layers.extend([\n            self.global_pool, self.conv_head, self.act2,\n            nn.Flatten(), nn.Dropout(self.drop_rate), self.classifier])\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.blocks(x)\n        x = self.global_pool(x)\n        x = self.conv_head(x)\n        x = self.act2(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.flatten(1)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        return self.classifier(x)\n\n\ndef _create_model(model_kwargs, variant, pretrained=False):\n    as_sequential = model_kwargs.pop(\'as_sequential\', False)\n    model = MobileNetV3(**model_kwargs)\n    if pretrained and model_urls[variant]:\n        load_pretrained(model, model_urls[variant])\n    if as_sequential:\n        model = model.as_sequential()\n    return model\n\n\ndef _gen_mobilenet_v3_rw(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates a MobileNet-V3 model (RW variant).\n\n    Paper: https://arxiv.org/abs/1905.02244\n\n    This was my first attempt at reproducing the MobileNet-V3 from paper alone. It came close to the\n    eventual Tensorflow reference impl but has a few differences:\n    1. This model has no bias on the head convolution\n    2. This model forces no residual (noskip) on the first DWS block, this is different than MnasNet\n    3. This model always uses ReLU for the SE activation layer, other models in the family inherit their act layer\n       from their parent block\n    4. This model does not enforce divisible by 8 limitation on the SE reduction channel count\n\n    Overall the changes are fairly minor and result in a very small parameter count difference and no\n    top-1/5\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    """"""\n    arch_def = [\n        # stage 0, 112x112 in\n        [\'ds_r1_k3_s1_e1_c16_nre_noskip\'],  # relu\n        # stage 1, 112x112 in\n        [\'ir_r1_k3_s2_e4_c24_nre\', \'ir_r1_k3_s1_e3_c24_nre\'],  # relu\n        # stage 2, 56x56 in\n        [\'ir_r3_k5_s2_e3_c40_se0.25_nre\'],  # relu\n        # stage 3, 28x28 in\n        [\'ir_r1_k3_s2_e6_c80\', \'ir_r1_k3_s1_e2.5_c80\', \'ir_r2_k3_s1_e2.3_c80\'],  # hard-swish\n        # stage 4, 14x14in\n        [\'ir_r2_k3_s1_e6_c112_se0.25\'],  # hard-swish\n        # stage 5, 14x14in\n        [\'ir_r3_k5_s2_e6_c160_se0.25\'],  # hard-swish\n        # stage 6, 7x7 in\n        [\'cn_r1_k1_s1_c960\'],  # hard-swish\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        head_bias=False,  # one of my mistakes\n        channel_multiplier=channel_multiplier,\n        act_layer=resolve_act_layer(kwargs, \'hard_swish\'),\n        se_kwargs=dict(gate_fn=get_act_fn(\'hard_sigmoid\'), reduce_mid=True),\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs,\n    )\n    model = _create_model(model_kwargs, variant, pretrained)\n    return model\n\n\ndef _gen_mobilenet_v3(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates a MobileNet-V3 large/small/minimal models.\n\n    Ref impl: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v3.py\n    Paper: https://arxiv.org/abs/1905.02244\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    """"""\n    if \'small\' in variant:\n        num_features = 1024\n        if \'minimal\' in variant:\n            act_layer = \'relu\'\n            arch_def = [\n                # stage 0, 112x112 in\n                [\'ds_r1_k3_s2_e1_c16\'],\n                # stage 1, 56x56 in\n                [\'ir_r1_k3_s2_e4.5_c24\', \'ir_r1_k3_s1_e3.67_c24\'],\n                # stage 2, 28x28 in\n                [\'ir_r1_k3_s2_e4_c40\', \'ir_r2_k3_s1_e6_c40\'],\n                # stage 3, 14x14 in\n                [\'ir_r2_k3_s1_e3_c48\'],\n                # stage 4, 14x14in\n                [\'ir_r3_k3_s2_e6_c96\'],\n                # stage 6, 7x7 in\n                [\'cn_r1_k1_s1_c576\'],\n            ]\n        else:\n            act_layer = \'hard_swish\'\n            arch_def = [\n                # stage 0, 112x112 in\n                [\'ds_r1_k3_s2_e1_c16_se0.25_nre\'],  # relu\n                # stage 1, 56x56 in\n                [\'ir_r1_k3_s2_e4.5_c24_nre\', \'ir_r1_k3_s1_e3.67_c24_nre\'],  # relu\n                # stage 2, 28x28 in\n                [\'ir_r1_k5_s2_e4_c40_se0.25\', \'ir_r2_k5_s1_e6_c40_se0.25\'],  # hard-swish\n                # stage 3, 14x14 in\n                [\'ir_r2_k5_s1_e3_c48_se0.25\'],  # hard-swish\n                # stage 4, 14x14in\n                [\'ir_r3_k5_s2_e6_c96_se0.25\'],  # hard-swish\n                # stage 6, 7x7 in\n                [\'cn_r1_k1_s1_c576\'],  # hard-swish\n            ]\n    else:\n        num_features = 1280\n        if \'minimal\' in variant:\n            act_layer = \'relu\'\n            arch_def = [\n                # stage 0, 112x112 in\n                [\'ds_r1_k3_s1_e1_c16\'],\n                # stage 1, 112x112 in\n                [\'ir_r1_k3_s2_e4_c24\', \'ir_r1_k3_s1_e3_c24\'],\n                # stage 2, 56x56 in\n                [\'ir_r3_k3_s2_e3_c40\'],\n                # stage 3, 28x28 in\n                [\'ir_r1_k3_s2_e6_c80\', \'ir_r1_k3_s1_e2.5_c80\', \'ir_r2_k3_s1_e2.3_c80\'],\n                # stage 4, 14x14in\n                [\'ir_r2_k3_s1_e6_c112\'],\n                # stage 5, 14x14in\n                [\'ir_r3_k3_s2_e6_c160\'],\n                # stage 6, 7x7 in\n                [\'cn_r1_k1_s1_c960\'],\n            ]\n        else:\n            act_layer = \'hard_swish\'\n            arch_def = [\n                # stage 0, 112x112 in\n                [\'ds_r1_k3_s1_e1_c16_nre\'],  # relu\n                # stage 1, 112x112 in\n                [\'ir_r1_k3_s2_e4_c24_nre\', \'ir_r1_k3_s1_e3_c24_nre\'],  # relu\n                # stage 2, 56x56 in\n                [\'ir_r3_k5_s2_e3_c40_se0.25_nre\'],  # relu\n                # stage 3, 28x28 in\n                [\'ir_r1_k3_s2_e6_c80\', \'ir_r1_k3_s1_e2.5_c80\', \'ir_r2_k3_s1_e2.3_c80\'],  # hard-swish\n                # stage 4, 14x14in\n                [\'ir_r2_k3_s1_e6_c112_se0.25\'],  # hard-swish\n                # stage 5, 14x14in\n                [\'ir_r3_k5_s2_e6_c160_se0.25\'],  # hard-swish\n                # stage 6, 7x7 in\n                [\'cn_r1_k1_s1_c960\'],  # hard-swish\n            ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        num_features=num_features,\n        stem_size=16,\n        channel_multiplier=channel_multiplier,\n        act_layer=resolve_act_layer(kwargs, act_layer),\n        se_kwargs=dict(\n            act_layer=get_act_layer(\'relu\'), gate_fn=get_act_fn(\'hard_sigmoid\'), reduce_mid=True, divisor=8),\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs,\n    )\n    model = _create_model(model_kwargs, variant, pretrained)\n    return model\n\n\ndef mobilenetv3_rw(pretrained=False, **kwargs):\n    """""" MobileNet-V3 RW\n    Attn: See note in gen function for this variant.\n    """"""\n    # NOTE for train set drop_rate=0.2\n    if pretrained:\n        # pretrained model trained with non-default BN epsilon\n        kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    model = _gen_mobilenet_v3_rw(\'mobilenetv3_rw\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mobilenetv3_large_075(pretrained=False, **kwargs):\n    """""" MobileNet V3 Large 0.75""""""\n    # NOTE for train set drop_rate=0.2\n    model = _gen_mobilenet_v3(\'mobilenetv3_large_075\', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mobilenetv3_large_100(pretrained=False, **kwargs):\n    """""" MobileNet V3 Large 1.0 """"""\n    # NOTE for train set drop_rate=0.2\n    model = _gen_mobilenet_v3(\'mobilenetv3_large_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mobilenetv3_large_minimal_100(pretrained=False, **kwargs):\n    """""" MobileNet V3 Large (Minimalistic) 1.0 """"""\n    # NOTE for train set drop_rate=0.2\n    model = _gen_mobilenet_v3(\'mobilenetv3_large_minimal_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mobilenetv3_small_075(pretrained=False, **kwargs):\n    """""" MobileNet V3 Small 0.75 """"""\n    model = _gen_mobilenet_v3(\'mobilenetv3_small_075\', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mobilenetv3_small_100(pretrained=False, **kwargs):\n    """""" MobileNet V3 Small 1.0 """"""\n    model = _gen_mobilenet_v3(\'mobilenetv3_small_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef mobilenetv3_small_minimal_100(pretrained=False, **kwargs):\n    """""" MobileNet V3 Small (Minimalistic) 1.0 """"""\n    model = _gen_mobilenet_v3(\'mobilenetv3_small_minimal_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_mobilenetv3_large_075(pretrained=False, **kwargs):\n    """""" MobileNet V3 Large 0.75. Tensorflow compat variant. """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mobilenet_v3(\'tf_mobilenetv3_large_075\', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_mobilenetv3_large_100(pretrained=False, **kwargs):\n    """""" MobileNet V3 Large 1.0. Tensorflow compat variant. """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mobilenet_v3(\'tf_mobilenetv3_large_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_mobilenetv3_large_minimal_100(pretrained=False, **kwargs):\n    """""" MobileNet V3 Large Minimalistic 1.0. Tensorflow compat variant. """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mobilenet_v3(\'tf_mobilenetv3_large_minimal_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_mobilenetv3_small_075(pretrained=False, **kwargs):\n    """""" MobileNet V3 Small 0.75. Tensorflow compat variant. """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mobilenet_v3(\'tf_mobilenetv3_small_075\', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_mobilenetv3_small_100(pretrained=False, **kwargs):\n    """""" MobileNet V3 Small 1.0. Tensorflow compat variant.""""""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mobilenet_v3(\'tf_mobilenetv3_small_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef tf_mobilenetv3_small_minimal_100(pretrained=False, **kwargs):\n    """""" MobileNet V3 Small Minimalistic 1.0. Tensorflow compat variant. """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mobilenet_v3(\'tf_mobilenetv3_small_minimal_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n'"
geffnet/model_factory.py,0,"b""from .mobilenetv3 import *\nfrom .gen_efficientnet import *\nfrom .helpers import load_checkpoint\n\n\ndef create_model(\n        model_name='mnasnet_100',\n        pretrained=None,\n        num_classes=1000,\n        in_chans=3,\n        checkpoint_path='',\n        **kwargs):\n\n    margs = dict(num_classes=num_classes, in_chans=in_chans, pretrained=pretrained)\n\n    if model_name in globals():\n        create_fn = globals()[model_name]\n        model = create_fn(**margs, **kwargs)\n    else:\n        raise RuntimeError('Unknown model (%s)' % model_name)\n\n    if checkpoint_path and not pretrained:\n        load_checkpoint(model, checkpoint_path)\n\n    return model\n"""
geffnet/version.py,0,"b""__version__ = '0.9.9'\n"""
geffnet/activations/__init__.py,0,"b'from geffnet import config\nfrom geffnet.activations.activations_autofn import *\nfrom geffnet.activations.activations_jit import *\nfrom geffnet.activations.activations import *\n\n\n_ACT_FN_DEFAULT = dict(\n    swish=swish,\n    mish=mish,\n    relu=F.relu,\n    relu6=F.relu6,\n    sigmoid=sigmoid,\n    tanh=tanh,\n    hard_sigmoid=hard_sigmoid,\n    hard_swish=hard_swish,\n)\n\n_ACT_FN_AUTO = dict(\n    swish=swish_auto,\n    mish=mish_auto,\n)\n\n_ACT_FN_JIT = dict(\n    swish=swish_jit,\n    mish=mish_jit,\n    #hard_swish=hard_swish_jit,\n    #hard_sigmoid_jit=hard_sigmoid_jit,\n)\n\n_ACT_LAYER_DEFAULT = dict(\n    swish=Swish,\n    mish=Mish,\n    relu=nn.ReLU,\n    relu6=nn.ReLU6,\n    sigmoid=Sigmoid,\n    tanh=Tanh,\n    hard_sigmoid=HardSigmoid,\n    hard_swish=HardSwish,\n)\n\n_ACT_LAYER_AUTO = dict(\n    swish=SwishAuto,\n    mish=MishAuto,\n)\n\n_ACT_LAYER_JIT = dict(\n    swish=SwishJit,\n    mish=MishJit,\n    #hard_swish=HardSwishJit,\n    #hard_sigmoid=HardSigmoidJit\n)\n\n_OVERRIDE_FN = dict()\n_OVERRIDE_LAYER = dict()\n\n\ndef add_override_act_fn(name, fn):\n    global _OVERRIDE_FN\n    _OVERRIDE_FN[name] = fn\n\n\ndef update_override_act_fn(overrides):\n    assert isinstance(overrides, dict)\n    global _OVERRIDE_FN\n    _OVERRIDE_FN.update(overrides)\n\n\ndef clear_override_act_fn():\n    global _OVERRIDE_FN\n    _OVERRIDE_FN = dict()\n\n\ndef add_override_act_layer(name, fn):\n    _OVERRIDE_LAYER[name] = fn\n\n\ndef update_override_act_layer(overrides):\n    assert isinstance(overrides, dict)\n    global _OVERRIDE_LAYER\n    _OVERRIDE_LAYER.update(overrides)\n\n\ndef clear_override_act_layer():\n    global _OVERRIDE_LAYER\n    _OVERRIDE_LAYER = dict()\n\n\ndef get_act_fn(name=\'relu\'):\n    """""" Activation Function Factory\n    Fetching activation fns by name with this function allows export or torch script friendly\n    functions to be returned dynamically based on current config.\n    """"""\n    if name in _OVERRIDE_FN:\n        return _OVERRIDE_FN[name]\n    if not config.is_exportable() and not config.is_scriptable():\n        # If not exporting or scripting the model, first look for a JIT optimized version\n        # of our activation, then a custom autograd.Function variant before defaulting to\n        # a Python or Torch builtin impl\n        if name in _ACT_FN_JIT:\n            return _ACT_FN_JIT[name]\n        if name in _ACT_FN_AUTO:\n            return _ACT_FN_AUTO[name]\n    return _ACT_FN_DEFAULT[name]\n\n\ndef get_act_layer(name=\'relu\'):\n    """""" Activation Layer Factory\n    Fetching activation layers by name with this function allows export or torch script friendly\n    functions to be returned dynamically based on current config.\n    """"""\n    if name in _OVERRIDE_LAYER:\n        return _OVERRIDE_LAYER[name]\n    if not config.is_exportable() and not config.is_scriptable():\n        if name in _ACT_LAYER_JIT:\n            return _ACT_LAYER_JIT[name]\n        if name in _ACT_LAYER_AUTO:\n            return _ACT_LAYER_AUTO[name]\n    return _ACT_LAYER_DEFAULT[name]\n\n\n'"
geffnet/activations/activations.py,1,"b'from torch import nn as nn\nfrom torch.nn import functional as F\n\n\ndef swish(x, inplace: bool = False):\n    """"""Swish - Described in: https://arxiv.org/abs/1710.05941\n    """"""\n    return x.mul_(x.sigmoid()) if inplace else x.mul(x.sigmoid())\n\n\nclass Swish(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(Swish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return swish(x, self.inplace)\n\n\ndef mish(x, inplace: bool = False):\n    """"""Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n    """"""\n    return x.mul(F.softplus(x).tanh())\n\n\nclass Mish(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(Mish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return mish(x, self.inplace)\n\n\ndef sigmoid(x, inplace: bool = False):\n    return x.sigmoid_() if inplace else x.sigmoid()\n\n\n# PyTorch has this, but not with a consistent inplace argmument interface\nclass Sigmoid(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(Sigmoid, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return x.sigmoid_() if self.inplace else x.sigmoid()\n\n\ndef tanh(x, inplace: bool = False):\n    return x.tanh_() if inplace else x.tanh()\n\n\n# PyTorch has this, but not with a consistent inplace argmument interface\nclass Tanh(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(Tanh, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return x.tanh_() if self.inplace else x.tanh()\n\n\ndef hard_swish(x, inplace: bool = False):\n    inner = F.relu6(x + 3.).div_(6.)\n    return x.mul_(inner) if inplace else x.mul(inner)\n\n\nclass HardSwish(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(HardSwish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return hard_swish(x, self.inplace)\n\n\ndef hard_sigmoid(x, inplace: bool = False):\n    if inplace:\n        return x.add_(3.).clamp_(0., 6.).div_(6.)\n    else:\n        return F.relu6(x + 3.) / 6.\n\n\nclass HardSigmoid(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(HardSigmoid, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return hard_sigmoid(x, self.inplace)\n\n\n'"
geffnet/activations/activations_autofn.py,7,"b'import torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\n\n__all__ = [\'swish_auto\', \'SwishAuto\', \'mish_auto\', \'MishAuto\']\n\n\nclass SwishAutoFn(torch.autograd.Function):\n    """"""Swish - Described in: https://arxiv.org/abs/1710.05941\n    Memory efficient variant from:\n     https://medium.com/the-artificial-impostor/more-memory-efficient-swish-activation-function-e07c22c12a76\n    """"""\n    @staticmethod\n    def forward(ctx, x):\n        result = x.mul(torch.sigmoid(x))\n        ctx.save_for_backward(x)\n        return result\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        x_sigmoid = torch.sigmoid(x)\n        return grad_output.mul(x_sigmoid * (1 + x * (1 - x_sigmoid)))\n\n\ndef swish_auto(x, inplace=False):\n    # inplace ignored\n    return SwishAutoFn.apply(x)\n\n\nclass SwishAuto(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(SwishAuto, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return SwishAutoFn.apply(x)\n\n\nclass MishAutoFn(torch.autograd.Function):\n    """"""Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n    Experimental memory-efficient variant\n    """"""\n\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        y = x.mul(torch.tanh(F.softplus(x)))  # x * tanh(ln(1 + exp(x)))\n        return y\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        x_sigmoid = torch.sigmoid(x)\n        x_tanh_sp = F.softplus(x).tanh()\n        return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n\n\ndef mish_auto(x, inplace=False):\n    # inplace ignored\n    return MishAutoFn.apply(x)\n\n\nclass MishAuto(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(MishAuto, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return MishAutoFn.apply(x)\n\n'"
geffnet/activations/activations_jit.py,14,"b'import torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\n\n__all__ = [\'swish_jit\', \'SwishJit\', \'mish_jit\', \'MishJit\']\n           #\'hard_swish_jit\', \'HardSwishJit\', \'hard_sigmoid_jit\', \'HardSigmoidJit\']\n\n\n@torch.jit.script\ndef swish_jit_fwd(x):\n    return x.mul(torch.sigmoid(x))\n\n\n@torch.jit.script\ndef swish_jit_bwd(x, grad_output):\n    x_sigmoid = torch.sigmoid(x)\n    return grad_output * (x_sigmoid * (1 + x * (1 - x_sigmoid)))\n\n\nclass SwishJitAutoFn(torch.autograd.Function):\n    """""" torch.jit.script optimised Swish\n    Inspired by conversation btw Jeremy Howard & Adam Pazske\n    https://twitter.com/jeremyphoward/status/1188251041835315200\n    """"""\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return swish_jit_fwd(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        return swish_jit_bwd(x, grad_output)\n\n\ndef swish_jit(x, inplace=False):\n    # inplace ignored\n    return SwishJitAutoFn.apply(x)\n\n\nclass SwishJit(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(SwishJit, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return SwishJitAutoFn.apply(x)\n\n\n@torch.jit.script\ndef mish_jit_fwd(x):\n    return x.mul(torch.tanh(F.softplus(x)))\n\n\n@torch.jit.script\ndef mish_jit_bwd(x, grad_output):\n    x_sigmoid = torch.sigmoid(x)\n    x_tanh_sp = F.softplus(x).tanh()\n    return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n\n\nclass MishJitAutoFn(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x):\n        ctx.save_for_backward(x)\n        return mish_jit_fwd(x)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        x = ctx.saved_tensors[0]\n        return mish_jit_bwd(x, grad_output)\n\n\ndef mish_jit(x, inplace=False):\n    # inplace ignored\n    return MishJitAutoFn.apply(x)\n\n\nclass MishJit(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(MishJit, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return MishJitAutoFn.apply(x)\n\n\n# @torch.jit.script\n# def hard_swish_jit(x, inplac: bool = False):\n#     return x.mul(F.relu6(x + 3.).mul_(1./6.))\n#\n#\n# class HardSwishJit(nn.Module):\n#     def __init__(self, inplace: bool = False):\n#         super(HardSwishJit, self).__init__()\n#\n#     def forward(self, x):\n#         return hard_swish_jit(x)\n#\n#\n# @torch.jit.script\n# def hard_sigmoid_jit(x, inplace: bool = False):\n#     return F.relu6(x + 3.).mul(1./6.)\n#\n#\n# class HardSigmoidJit(nn.Module):\n#     def __init__(self, inplace: bool = False):\n#         super(HardSigmoidJit, self).__init__()\n#\n#     def forward(self, x):\n#         return hard_sigmoid_jit(x)\n'"
