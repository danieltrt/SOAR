file_path,api_count,code
avg_checkpoints.py,6,"b'#!/usr/bin/env python\n"""""" Checkpoint Averaging Script\n\nThis script averages all model weights for checkpoints in specified path that match\nthe specified filter wildcard. All checkpoints must be from the exact same model.\n\nFor any hope of decent results, the checkpoints should be from the same or child\n(via resumes) training session. This can be viewed as similar to maintaining running\nEMA (exponential moving average) of the model weights or performing SWA (stochastic\nweight averaging), but post-training.\n\nHacked together by Ross Wightman (https://github.com/rwightman)\n""""""\nimport torch\nimport argparse\nimport os\nimport glob\nimport hashlib\nfrom timm.models.helpers import load_state_dict\n\nparser = argparse.ArgumentParser(description=\'PyTorch Checkpoint Averager\')\nparser.add_argument(\'--input\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to base input folder containing checkpoints\')\nparser.add_argument(\'--filter\', default=\'*.pth.tar\', type=str, metavar=\'WILDCARD\',\n                    help=\'checkpoint filter (path wildcard)\')\nparser.add_argument(\'--output\', default=\'./averaged.pth\', type=str, metavar=\'PATH\',\n                    help=\'output filename\')\nparser.add_argument(\'--no-use-ema\', dest=\'no_use_ema\', action=\'store_true\',\n                    help=\'Force not using ema version of weights (if present)\')\nparser.add_argument(\'--no-sort\', dest=\'no_sort\', action=\'store_true\',\n                    help=\'Do not sort and select by checkpoint metric, also makes ""n"" argument irrelevant\')\nparser.add_argument(\'-n\', type=int, default=10, metavar=\'N\',\n                    help=\'Number of checkpoints to average\')\n\n\ndef checkpoint_metric(checkpoint_path):\n    if not checkpoint_path or not os.path.isfile(checkpoint_path):\n        return {}\n    print(""=> Extracting metric from checkpoint \'{}\'"".format(checkpoint_path))\n    checkpoint = torch.load(checkpoint_path, map_location=\'cpu\')\n    metric = None\n    if \'metric\' in checkpoint:\n        metric = checkpoint[\'metric\']\n    return metric\n\n\ndef main():\n    args = parser.parse_args()\n    # by default use the EMA weights (if present)\n    args.use_ema = not args.no_use_ema\n    # by default sort by checkpoint metric (if present) and avg top n checkpoints\n    args.sort = not args.no_sort\n\n    if os.path.exists(args.output):\n        print(""Error: Output filename ({}) already exists."".format(args.output))\n        exit(1)\n\n    pattern = args.input\n    if not args.input.endswith(os.path.sep) and not args.filter.startswith(os.path.sep):\n        pattern += os.path.sep\n    pattern += args.filter\n    checkpoints = glob.glob(pattern, recursive=True)\n\n    if args.sort:\n        checkpoint_metrics = []\n        for c in checkpoints:\n            metric = checkpoint_metric(c)\n            if metric is not None:\n                checkpoint_metrics.append((metric, c))\n        checkpoint_metrics = list(sorted(checkpoint_metrics))\n        checkpoint_metrics = checkpoint_metrics[-args.n:]\n        print(""Selected checkpoints:"")\n        [print(m, c) for m, c in checkpoint_metrics]\n        avg_checkpoints = [c for m, c in checkpoint_metrics]\n    else:\n        avg_checkpoints = checkpoints\n        print(""Selected checkpoints:"")\n        [print(c) for c in checkpoints]\n\n    avg_state_dict = {}\n    avg_counts = {}\n    for c in avg_checkpoints:\n        new_state_dict = load_state_dict(c, args.use_ema)\n        if not new_state_dict:\n            print(""Error: Checkpoint ({}) doesn\'t exist"".format(args.checkpoint))\n            continue\n\n        for k, v in new_state_dict.items():\n            if k not in avg_state_dict:\n                avg_state_dict[k] = v.clone().to(dtype=torch.float64)\n                avg_counts[k] = 1\n            else:\n                avg_state_dict[k] += v.to(dtype=torch.float64)\n                avg_counts[k] += 1\n\n    for k, v in avg_state_dict.items():\n        v.div_(avg_counts[k])\n\n    # float32 overflow seems unlikely based on weights seen to date, but who knows\n    float32_info = torch.finfo(torch.float32)\n    final_state_dict = {}\n    for k, v in avg_state_dict.items():\n        v = v.clamp(float32_info.min, float32_info.max)\n        final_state_dict[k] = v.to(dtype=torch.float32)\n\n    torch.save(final_state_dict, args.output)\n    with open(args.output, \'rb\') as f:\n        sha_hash = hashlib.sha256(f.read()).hexdigest()\n    print(""=> Saved state_dict to \'{}, SHA256: {}\'"".format(args.output, sha_hash))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
clean_checkpoint.py,2,"b'#!/usr/bin/env python\n"""""" Checkpoint Cleaning Script\n\nTakes training checkpoints with GPU tensors, optimizer state, extra dict keys, etc.\nand outputs a CPU  tensor checkpoint with only the `state_dict` along with SHA256\ncalculation for model zoo compatibility.\n\nHacked together by Ross Wightman (https://github.com/rwightman)\n""""""\nimport torch\nimport argparse\nimport os\nimport hashlib\nimport shutil\nfrom collections import OrderedDict\n\nparser = argparse.ArgumentParser(description=\'PyTorch Checkpoint Cleaner\')\nparser.add_argument(\'--checkpoint\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--output\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'output path\')\nparser.add_argument(\'--use-ema\', dest=\'use_ema\', action=\'store_true\',\n                    help=\'use ema version of weights if present\')\nparser.add_argument(\'--clean-aux-bn\', dest=\'clean_aux_bn\', action=\'store_true\',\n                    help=\'remove auxiliary batch norm layers (from SplitBN training) from checkpoint\')\n\n_TEMP_NAME = \'./_checkpoint.pth\'\n\n\ndef main():\n    args = parser.parse_args()\n\n    if os.path.exists(args.output):\n        print(""Error: Output filename ({}) already exists."".format(args.output))\n        exit(1)\n\n    # Load an existing checkpoint to CPU, strip everything but the state_dict and re-save\n    if args.checkpoint and os.path.isfile(args.checkpoint):\n        print(""=> Loading checkpoint \'{}\'"".format(args.checkpoint))\n        checkpoint = torch.load(args.checkpoint, map_location=\'cpu\')\n\n        new_state_dict = OrderedDict()\n        if isinstance(checkpoint, dict):\n            state_dict_key = \'state_dict_ema\' if args.use_ema else \'state_dict\'\n            if state_dict_key in checkpoint:\n                state_dict = checkpoint[state_dict_key]\n            else:\n                state_dict = checkpoint\n        else:\n            assert False\n        for k, v in state_dict.items():\n            if args.clean_aux_bn and \'aux_bn\' in k:\n                # If all aux_bn keys are removed, the SplitBN layers will end up as normal and\n                # load with the unmodified model using BatchNorm2d.\n                continue\n            name = k[7:] if k.startswith(\'module\') else k\n            new_state_dict[name] = v\n        print(""=> Loaded state_dict from \'{}\'"".format(args.checkpoint))\n\n        torch.save(new_state_dict, _TEMP_NAME)\n        with open(_TEMP_NAME, \'rb\') as f:\n            sha_hash = hashlib.sha256(f.read()).hexdigest()\n\n        if args.output:\n            checkpoint_root, checkpoint_base = os.path.split(args.output)\n            checkpoint_base = os.path.splitext(checkpoint_base)[0]\n        else:\n            checkpoint_root = \'\'\n            checkpoint_base = os.path.splitext(args.checkpoint)[0]\n        final_filename = \'-\'.join([checkpoint_base, sha_hash[:8]]) + \'.pth\'\n        shutil.move(_TEMP_NAME, os.path.join(checkpoint_root, final_filename))\n        print(""=> Saved state_dict to \'{}, SHA256: {}\'"".format(final_filename, sha_hash))\n    else:\n        print(""Error: Checkpoint ({}) doesn\'t exist"".format(args.checkpoint))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
hubconf.py,0,"b""dependencies = ['torch']\nfrom timm.models import registry\n\nglobals().update(registry._model_entrypoints)\n"""
inference.py,3,"b'#!/usr/bin/env python\n""""""PyTorch Inference Script\n\nAn example inference script that outputs top-k class ids for images in a folder into a csv.\n\nHacked together by Ross Wightman (https://github.com/rwightman)\n""""""\nimport os\nimport time\nimport argparse\nimport logging\nimport numpy as np\nimport torch\n\nfrom timm.models import create_model, apply_test_time_pool\nfrom timm.data import Dataset, create_loader, resolve_data_config\nfrom timm.utils import AverageMeter, setup_default_logging\n\ntorch.backends.cudnn.benchmark = True\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Inference\')\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--output_dir\', metavar=\'DIR\', default=\'./\',\n                    help=\'path to output files\')\nparser.add_argument(\'--model\', \'-m\', metavar=\'MODEL\', default=\'dpn92\',\n                    help=\'model architecture (default: dpn92)\')\nparser.add_argument(\'-j\', \'--workers\', default=2, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 2)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=256, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 256)\')\nparser.add_argument(\'--img-size\', default=None, type=int,\n                    metavar=\'N\', help=\'Input image dimension\')\nparser.add_argument(\'--mean\', type=float, nargs=\'+\', default=None, metavar=\'MEAN\',\n                    help=\'Override mean pixel value of dataset\')\nparser.add_argument(\'--std\', type=float, nargs=\'+\', default=None, metavar=\'STD\',\n                    help=\'Override std deviation of of dataset\')\nparser.add_argument(\'--interpolation\', default=\'\', type=str, metavar=\'NAME\',\n                    help=\'Image resize interpolation type (overrides model)\')\nparser.add_argument(\'--num-classes\', type=int, default=1000,\n                    help=\'Number classes in dataset\')\nparser.add_argument(\'--log-freq\', default=10, type=int,\n                    metavar=\'N\', help=\'batch logging frequency (default: 10)\')\nparser.add_argument(\'--checkpoint\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                    help=\'use pre-trained model\')\nparser.add_argument(\'--num-gpu\', type=int, default=1,\n                    help=\'Number of GPUS to use\')\nparser.add_argument(\'--no-test-pool\', dest=\'no_test_pool\', action=\'store_true\',\n                    help=\'disable test time pool\')\nparser.add_argument(\'--topk\', default=5, type=int,\n                    metavar=\'N\', help=\'Top-k to output to CSV\')\n\n\ndef main():\n    setup_default_logging()\n    args = parser.parse_args()\n    # might as well try to do something useful...\n    args.pretrained = args.pretrained or not args.checkpoint\n\n    # create model\n    model = create_model(\n        args.model,\n        num_classes=args.num_classes,\n        in_chans=3,\n        pretrained=args.pretrained,\n        checkpoint_path=args.checkpoint)\n\n    logging.info(\'Model %s created, param count: %d\' %\n                 (args.model, sum([m.numel() for m in model.parameters()])))\n\n    config = resolve_data_config(vars(args), model=model)\n    model, test_time_pool = apply_test_time_pool(model, config, args)\n\n    if args.num_gpu > 1:\n        model = torch.nn.DataParallel(model, device_ids=list(range(args.num_gpu))).cuda()\n    else:\n        model = model.cuda()\n\n    loader = create_loader(\n        Dataset(args.data),\n        input_size=config[\'input_size\'],\n        batch_size=args.batch_size,\n        use_prefetcher=True,\n        interpolation=config[\'interpolation\'],\n        mean=config[\'mean\'],\n        std=config[\'std\'],\n        num_workers=args.workers,\n        crop_pct=1.0 if test_time_pool else config[\'crop_pct\'])\n\n    model.eval()\n\n    k = min(args.topk, args.num_classes)\n    batch_time = AverageMeter()\n    end = time.time()\n    topk_ids = []\n    with torch.no_grad():\n        for batch_idx, (input, _) in enumerate(loader):\n            input = input.cuda()\n            labels = model(input)\n            topk = labels.topk(k)[1]\n            topk_ids.append(topk.cpu().numpy())\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if batch_idx % args.log_freq == 0:\n                logging.info(\'Predict: [{0}/{1}] Time {batch_time.val:.3f} ({batch_time.avg:.3f})\'.format(\n                    batch_idx, len(loader), batch_time=batch_time))\n\n    topk_ids = np.concatenate(topk_ids, axis=0).squeeze()\n\n    with open(os.path.join(args.output_dir, \'./topk_ids.csv\'), \'w\') as out_file:\n        filenames = loader.dataset.filenames()\n        for filename, label in zip(filenames, topk_ids):\n            filename = os.path.basename(filename)\n            out_file.write(\'{0},{1},{2},{3},{4},{5}\\n\'.format(\n                filename, label[0], label[1], label[2], label[3], label[4]))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
setup.py,0,"b'"""""" Setup\n""""""\nfrom setuptools import setup, find_packages\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nexec(open(\'timm/version.py\').read())\nsetup(\n    name=\'timm\',\n    version=__version__,\n    description=\'(Unofficial) PyTorch Image Models\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    url=\'https://github.com/rwightman/pytorch-image-models\',\n    author=\'Ross Wightman\',\n    author_email=\'hello@rwightman.com\',\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        \'Development Status :: 3 - Alpha\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: Apache Software License\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n        \'Topic :: Software Development :: Libraries :: Python Modules\',\n    ],\n\n    # Note that this is a string of words separated by whitespace, not a list.\n    keywords=\'pytorch pretrained models efficientnet mobilenetv3 mnasnet\',\n    packages=find_packages(exclude=[\'convert\']),\n    include_package_data=True,\n    install_requires=[\'torch >= 1.0\', \'torchvision\'],\n    python_requires=\'>=3.6\',\n)\n'"
sotabench.py,1,"b'import torch\nfrom torchbench.image_classification import ImageNet\nfrom timm import create_model\nfrom timm.data import resolve_data_config, create_transform\nfrom timm.models import TestTimePoolHead\nimport os\n\nNUM_GPU = 1\nBATCH_SIZE = 256 * NUM_GPU\n\n\ndef _entry(model_name, paper_model_name, paper_arxiv_id, batch_size=BATCH_SIZE,\n           ttp=False, args=dict(), model_desc=None):\n    return dict(\n        model=model_name,\n        model_description=model_desc,\n        paper_model_name=paper_model_name,\n        paper_arxiv_id=paper_arxiv_id,\n        batch_size=batch_size,\n        ttp=ttp,\n        args=args)\n\n# NOTE For any original PyTorch models, I\'ll remove from this list when you add to sotabench to\n# avoid overlap and confusion. Please contact me.\nmodel_list = [\n    ## Weights ported by myself from other frameworks or trained myself in PyTorch\n    _entry(\'adv_inception_v3\', \'Adversarial Inception V3\', \'1611.01236\',\n           model_desc=\'Ported from official Tensorflow weights\'),\n    _entry(\'ens_adv_inception_resnet_v2\', \'Ensemble Adversarial Inception V3\', \'1705.07204\',\n           model_desc=\'Ported from official Tensorflow weights\'),\n    _entry(\'dpn68\', \'DPN-68 (224x224)\', \'1707.01629\'),\n    _entry(\'dpn68b\', \'DPN-68b (224x224)\', \'1707.01629\'),\n    _entry(\'dpn92\', \'DPN-92 (224x224)\', \'1707.01629\'),\n    _entry(\'dpn98\', \'DPN-98 (224x224)\', \'1707.01629\'),\n    _entry(\'dpn107\', \'DPN-107 (224x224)\', \'1707.01629\'),\n    _entry(\'dpn131\', \'DPN-131 (224x224)\', \'1707.01629\'),\n    _entry(\'dpn68\', \'DPN-68 (320x320, Mean-Max Pooling)\', \'1707.01629\', ttp=True, args=dict(img_size=320)),\n    _entry(\'dpn68b\', \'DPN-68b (320x320, Mean-Max Pooling)\', \'1707.01629\', ttp=True, args=dict(img_size=320)),\n    _entry(\'dpn92\', \'DPN-92 (320x320, Mean-Max Pooling)\', \'1707.01629\',\n           ttp=True, args=dict(img_size=320), batch_size=BATCH_SIZE//2),\n    _entry(\'dpn98\', \'DPN-98 (320x320, Mean-Max Pooling)\', \'1707.01629\',\n           ttp=True, args=dict(img_size=320), batch_size=BATCH_SIZE//2),\n    _entry(\'dpn107\', \'DPN-107 (320x320, Mean-Max Pooling)\', \'1707.01629\',\n           ttp=True, args=dict(img_size=320), batch_size=BATCH_SIZE//4),\n    _entry(\'dpn131\', \'DPN-131 (320x320, Mean-Max Pooling)\', \'1707.01629\',\n           ttp=True, args=dict(img_size=320), batch_size=BATCH_SIZE//4),\n    _entry(\'efficientnet_b0\', \'EfficientNet-B0\', \'1905.11946\'),\n    _entry(\'efficientnet_b1\', \'EfficientNet-B1\', \'1905.11946\'),\n    _entry(\'efficientnet_b2\', \'EfficientNet-B2\', \'1905.11946\',\n           model_desc=\'Trained from scratch in PyTorch w/ RandAugment\'),\n    _entry(\'efficientnet_b2a\', \'EfficientNet-B2 (288x288, 1.0 crop)\', \'1905.11946\',\n           model_desc=\'Trained from scratch in PyTorch w/ RandAugment\'),\n    _entry(\'efficientnet_b3\', \'EfficientNet-B3\', \'1905.11946\',\n           model_desc=\'Trained from scratch in PyTorch w/ RandAugment\'),\n    _entry(\'efficientnet_b3a\', \'EfficientNet-B3 (320x320, 1.0 crop)\', \'1905.11946\',\n           model_desc=\'Trained from scratch in PyTorch w/ RandAugment\'),\n    _entry(\'efficientnet_es\', \'EfficientNet-EdgeTPU-S\', \'1905.11946\',\n           model_desc=\'Trained from scratch in PyTorch w/ RandAugment\'),\n\n    _entry(\'gluon_inception_v3\', \'Inception V3\', \'1512.00567\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnet18_v1b\', \'ResNet-18\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnet34_v1b\', \'ResNet-34\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnet50_v1b\', \'ResNet-50\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnet50_v1c\', \'ResNet-50-C\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnet50_v1d\', \'ResNet-50-D\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnet50_v1s\', \'ResNet-50-S\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnet101_v1b\', \'ResNet-101\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnet101_v1c\', \'ResNet-101-C\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnet101_v1d\', \'ResNet-101-D\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnet101_v1s\', \'ResNet-101-S\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnet152_v1b\', \'ResNet-152\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnet152_v1c\', \'ResNet-152-C\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnet152_v1d\', \'ResNet-152-D\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnet152_v1s\', \'ResNet-152-S\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnext50_32x4d\', \'ResNeXt-50 32x4d\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnext101_32x4d\', \'ResNeXt-101 32x4d\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_resnext101_64x4d\', \'ResNeXt-101 64x4d\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_senet154\', \'SENet-154\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_seresnext50_32x4d\', \'SE-ResNeXt-50 32x4d\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_seresnext101_32x4d\', \'SE-ResNeXt-101 32x4d\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_seresnext101_64x4d\', \'SE-ResNeXt-101 64x4d\', \'1812.01187\', model_desc=\'Ported from GluonCV Model Zoo\'),\n    _entry(\'gluon_xception65\', \'Modified Aligned Xception\', \'1802.02611\', batch_size=BATCH_SIZE//2,\n           model_desc=\'Ported from GluonCV Model Zoo\'),\n\n    _entry(\'mixnet_xl\', \'MixNet-XL\', \'1907.09595\', model_desc=""My own scaling beyond paper\'s MixNet Large""),\n    _entry(\'mixnet_l\', \'MixNet-L\', \'1907.09595\'),\n    _entry(\'mixnet_m\', \'MixNet-M\', \'1907.09595\'),\n    _entry(\'mixnet_s\', \'MixNet-S\', \'1907.09595\'),\n\n    _entry(\'fbnetc_100\', \'FBNet-C\', \'1812.03443\',\n           model_desc=\'Trained in PyTorch with RMSProp, exponential LR decay\'),\n    _entry(\'mnasnet_100\', \'MnasNet-B1\', \'1807.11626\'),\n    _entry(\'semnasnet_100\', \'MnasNet-A1\', \'1807.11626\'),\n    _entry(\'spnasnet_100\', \'Single-Path NAS\', \'1904.02877\',\n           model_desc=\'Trained in PyTorch with SGD, cosine LR decay\'),\n    _entry(\'mobilenetv3_large_100\', \'MobileNet V3-Large 1.0\', \'1905.02244\',\n           model_desc=\'Trained in PyTorch with RMSProp, exponential LR decay, and hyper-params matching \'\n                      \'paper as closely as possible.\'),\n\n    _entry(\'resnet18\', \'ResNet-18\', \'1812.01187\'),\n    _entry(\'resnet26\', \'ResNet-26\', \'1812.01187\', model_desc=\'Block cfg of ResNet-34 w/ Bottleneck\'),\n    _entry(\'resnet26d\', \'ResNet-26-D\', \'1812.01187\',\n           model_desc=\'Block cfg of ResNet-34 w/ Bottleneck, deep stem, and avg-pool in downsample layers.\'),\n    _entry(\'resnet34\', \'ResNet-34\', \'1812.01187\'),\n    _entry(\'resnet50\', \'ResNet-50\', \'1812.01187\', model_desc=\'Trained with AugMix + JSD loss\'),\n    _entry(\'resnet50\', \'ResNet-50 (288x288 Mean-Max Pooling)\', \'1812.01187\',\n           ttp=True, args=dict(img_size=288),\n           model_desc=\'Trained with AugMix + JSD loss\'),\n    _entry(\'resnext50_32x4d\', \'ResNeXt-50 32x4d\', \'1812.01187\'),\n    _entry(\'resnext50d_32x4d\', \'ResNeXt-50-D 32x4d\', \'1812.01187\',\n           model_desc=""\'D\' variant (3x3 deep stem w/ avg-pool downscale). Trained with ""\n                      ""SGD w/ cosine LR decay, random-erasing (gaussian per-pixel noise) and label-smoothing""),\n\n    _entry(\'seresnet18\', \'SE-ResNet-18\', \'1709.01507\'),\n    _entry(\'seresnet34\', \'SE-ResNet-34\', \'1709.01507\'),\n    _entry(\'seresnext26_32x4d\', \'SE-ResNeXt-26 32x4d\', \'1709.01507\',\n           model_desc=\'Block cfg of SE-ResNeXt-34 w/ Bottleneck\'),\n    _entry(\'seresnext26d_32x4d\', \'SE-ResNeXt-26-D 32x4d\', \'1812.01187\',\n           model_desc=\'Block cfg of SE-ResNeXt-34 w/ Bottleneck, deep stem, and avg-pool in downsample layers.\'),\n    _entry(\'seresnext26t_32x4d\', \'SE-ResNeXt-26-T 32x4d\', \'1812.01187\',\n           model_desc=\'Block cfg of SE-ResNeXt-34 w/ Bottleneck, deep tiered stem, and avg-pool in downsample layers.\'),\n    _entry(\'seresnext26tn_32x4d\', \'SE-ResNeXt-26-TN 32x4d\', \'1812.01187\',\n           model_desc=\'Block cfg of SE-ResNeXt-34 w/ Bottleneck, deep tiered narrow stem, and avg-pool in downsample layers.\'),\n\n    _entry(\'skresnet18\', \'SK-ResNet-18\', \'1903.06586\'),\n    _entry(\'skresnet34\', \'SK-ResNet-34\', \'1903.06586\'),\n    _entry(\'skresnext50_32x4d\', \'SKNet-50\', \'1903.06586\'),\n\n    _entry(\'ecaresnetlight\', \'ECA-ResNet-Light\', \'1910.03151\',\n           model_desc=\'A tweaked ResNet50d with ECA attn.\'),\n    _entry(\'ecaresnet50d\', \'ECA-ResNet-50d\', \'1910.03151\',\n           model_desc=\'A ResNet50d with ECA attn\'),\n    _entry(\'ecaresnet101d\', \'ECA-ResNet-101d\', \'1910.03151\',\n           model_desc=\'A ResNet101d with ECA attn\'),\n\n    _entry(\'resnetblur50\', \'ResNet-Blur-50\', \'1904.11486\'),\n\n    _entry(\'tf_efficientnet_b0\', \'EfficientNet-B0 (AutoAugment)\', \'1905.11946\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b1\', \'EfficientNet-B1 (AutoAugment)\', \'1905.11946\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b2\', \'EfficientNet-B2 (AutoAugment)\', \'1905.11946\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b3\', \'EfficientNet-B3 (AutoAugment)\', \'1905.11946\', batch_size=BATCH_SIZE//2,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b4\', \'EfficientNet-B4 (AutoAugment)\', \'1905.11946\', batch_size=BATCH_SIZE//2,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b5\', \'EfficientNet-B5 (RandAugment)\', \'1905.11946\', batch_size=BATCH_SIZE//4,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b6\', \'EfficientNet-B6 (AutoAugment)\', \'1905.11946\', batch_size=BATCH_SIZE//8,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b7\', \'EfficientNet-B7 (RandAugment)\', \'1905.11946\', batch_size=BATCH_SIZE//8,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b8\', \'EfficientNet-B8 (RandAugment)\', \'1905.11946\', batch_size=BATCH_SIZE // 8,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n\n    _entry(\'tf_efficientnet_b0_ap\', \'EfficientNet-B0 (AdvProp)\', \'1911.09665\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b1_ap\', \'EfficientNet-B1 (AdvProp)\', \'1911.09665\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b2_ap\', \'EfficientNet-B2 (AdvProp)\', \'1911.09665\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b3_ap\', \'EfficientNet-B3 (AdvProp)\', \'1911.09665\', batch_size=BATCH_SIZE // 2,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b4_ap\', \'EfficientNet-B4 (AdvProp)\', \'1911.09665\', batch_size=BATCH_SIZE // 2,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b5_ap\', \'EfficientNet-B5 (AdvProp)\', \'1911.09665\', batch_size=BATCH_SIZE // 4,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b6_ap\', \'EfficientNet-B6 (AdvProp)\', \'1911.09665\', batch_size=BATCH_SIZE // 8,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b7_ap\', \'EfficientNet-B7 (AdvProp)\', \'1911.09665\', batch_size=BATCH_SIZE // 8,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b8_ap\', \'EfficientNet-B8 (AdvProp)\', \'1911.09665\', batch_size=BATCH_SIZE // 8,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n\n    _entry(\'tf_efficientnet_b0_ns\', \'EfficientNet-B0 (NoisyStudent)\', \'1911.04252\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b1_ns\', \'EfficientNet-B1 (NoisyStudent)\', \'1911.04252\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b2_ns\', \'EfficientNet-B2 (NoisyStudent)\', \'1911.04252\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b3_ns\', \'EfficientNet-B3 (NoisyStudent)\', \'1911.04252\', batch_size=BATCH_SIZE // 2,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b4_ns\', \'EfficientNet-B4 (NoisyStudent)\', \'1911.04252\', batch_size=BATCH_SIZE // 2,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b5_ns\', \'EfficientNet-B5 (NoisyStudent)\', \'1911.04252\', batch_size=BATCH_SIZE // 4,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b6_ns\', \'EfficientNet-B6 (NoisyStudent)\', \'1911.04252\', batch_size=BATCH_SIZE // 8,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_b7_ns\', \'EfficientNet-B7 (NoisyStudent)\', \'1911.04252\', batch_size=BATCH_SIZE // 8,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_l2_ns_475\', \'EfficientNet-L2 475 (NoisyStudent)\', \'1911.04252\', batch_size=BATCH_SIZE // 16,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_l2_ns\', \'EfficientNet-L2 (NoisyStudent)\', \'1911.04252\', batch_size=BATCH_SIZE // 64,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n\n    _entry(\'tf_efficientnet_cc_b0_4e\', \'EfficientNet-CondConv-B0 4 experts\', \'1904.04971\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_cc_b0_8e\', \'EfficientNet-CondConv-B0 8 experts\', \'1904.04971\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_cc_b1_8e\', \'EfficientNet-CondConv-B1 8 experts\', \'1904.04971\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n\n    _entry(\'tf_efficientnet_es\', \'EfficientNet-EdgeTPU-S\', \'1905.11946\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_em\', \'EfficientNet-EdgeTPU-M\', \'1905.11946\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_el\', \'EfficientNet-EdgeTPU-L\', \'1905.11946\', batch_size=BATCH_SIZE//2,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n\n    _entry(\'tf_efficientnet_lite0\', \'EfficientNet-Lite0\', \'1905.11946\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_lite1\', \'EfficientNet-Lite1\', \'1905.11946\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_lite2\', \'EfficientNet-Lite2\', \'1905.11946\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_lite3\', \'EfficientNet-Lite3\', \'1905.11946\', batch_size=BATCH_SIZE // 2,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_efficientnet_lite4\', \'EfficientNet-Lite4\', \'1905.11946\', batch_size=BATCH_SIZE // 2,\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n\n    _entry(\'tf_inception_v3\', \'Inception V3\', \'1512.00567\', model_desc=\'Ported from official Tensorflow weights\'),\n    _entry(\'tf_mixnet_l\', \'MixNet-L\', \'1907.09595\', model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_mixnet_m\', \'MixNet-M\', \'1907.09595\', model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_mixnet_s\', \'MixNet-S\', \'1907.09595\', model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_mobilenetv3_large_100\', \'MobileNet V3-Large 1.0\', \'1905.02244\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_mobilenetv3_large_075\', \'MobileNet V3-Large 0.75\', \'1905.02244\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_mobilenetv3_large_minimal_100\', \'MobileNet V3-Large Minimal 1.0\', \'1905.02244\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_mobilenetv3_small_100\', \'MobileNet V3-Small 1.0\', \'1905.02244\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_mobilenetv3_small_075\', \'MobileNet V3-Small 0.75\', \'1905.02244\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n    _entry(\'tf_mobilenetv3_small_minimal_100\', \'MobileNet V3-Small Minimal 1.0\', \'1905.02244\',\n           model_desc=\'Ported from official Google AI Tensorflow weights\'),\n\n    ## Cadene ported weights (to remove if Cadene adds sotabench)\n    _entry(\'inception_resnet_v2\', \'Inception ResNet V2\', \'1602.07261\'),\n    _entry(\'inception_v4\', \'Inception V4\', \'1602.07261\'),\n    _entry(\'nasnetalarge\', \'NASNet-A Large\', \'1707.07012\', batch_size=BATCH_SIZE // 4),\n    _entry(\'pnasnet5large\', \'PNASNet-5\', \'1712.00559\', batch_size=BATCH_SIZE // 4),\n    _entry(\'seresnet50\', \'SE-ResNet-50\', \'1709.01507\'),\n    _entry(\'seresnet101\', \'SE-ResNet-101\', \'1709.01507\'),\n    _entry(\'seresnet152\', \'SE-ResNet-152\', \'1709.01507\'),\n    _entry(\'seresnext50_32x4d\', \'SE-ResNeXt-50 32x4d\', \'1709.01507\'),\n    _entry(\'seresnext101_32x4d\', \'SE-ResNeXt-101 32x4d\', \'1709.01507\'),\n    _entry(\'senet154\', \'SENet-154\', \'1709.01507\'),\n    _entry(\'xception\', \'Xception\', \'1610.02357\',  batch_size=BATCH_SIZE//2),\n\n    ## Torchvision weights\n    # _entry(\'densenet121\'),\n    # _entry(\'densenet161\'),\n    # _entry(\'densenet169\'),\n    # _entry(\'densenet201\'),\n    # _entry(\'inception_v3\', paper_model_name=\'Inception V3\', ),\n    # _entry(\'tv_resnet34\', , ),\n    # _entry(\'tv_resnet50\', , ),\n    # _entry(\'resnet101\', , ),\n    # _entry(\'resnet152\', , ),\n    # _entry(\'tv_resnext50_32x4d\', , ),\n    # _entry(\'resnext101_32x8d\', ),\n    # _entry(\'wide_resnet50_2\' , ),\n    # _entry(\'wide_resnet101_2\', , ),\n\n    ## Facebook WSL weights\n    _entry(\'ig_resnext101_32x8d\', \'ResNeXt-101 32x8d\', \'1805.00932\',\n           model_desc=\'Weakly-Supervised pre-training on 1B Instagram hashtag dataset by Facebook Research\'),\n    _entry(\'ig_resnext101_32x16d\', \'ResNeXt-101 32x16d\', \'1805.00932\',\n           model_desc=\'Weakly-Supervised pre-training on 1B Instagram hashtag dataset by Facebook Research\'),\n    _entry(\'ig_resnext101_32x32d\', \'ResNeXt-101 32x32d\', \'1805.00932\', batch_size=BATCH_SIZE // 2,\n           model_desc=\'Weakly-Supervised pre-training on 1B Instagram hashtag dataset by Facebook Research\'),\n    _entry(\'ig_resnext101_32x48d\', \'ResNeXt-101 32x48d\', \'1805.00932\', batch_size=BATCH_SIZE // 4,\n           model_desc=\'Weakly-Supervised pre-training on 1B Instagram hashtag dataset by Facebook Research\'),\n\n    _entry(\'ig_resnext101_32x8d\', \'ResNeXt-101 32x8d (288x288 Mean-Max Pooling)\', \'1805.00932\',\n           ttp=True, args=dict(img_size=288),\n           model_desc=\'Weakly-Supervised pre-training on 1B Instagram hashtag dataset by Facebook Research\'),\n    _entry(\'ig_resnext101_32x16d\', \'ResNeXt-101 32x16d (288x288 Mean-Max Pooling)\', \'1805.00932\',\n           ttp=True, args=dict(img_size=288), batch_size=BATCH_SIZE // 2,\n           model_desc=\'Weakly-Supervised pre-training on 1B Instagram hashtag dataset by Facebook Research\'),\n    _entry(\'ig_resnext101_32x32d\', \'ResNeXt-101 32x32d (288x288 Mean-Max Pooling)\', \'1805.00932\',\n           ttp=True, args=dict(img_size=288), batch_size=BATCH_SIZE // 4,\n           model_desc=\'Weakly-Supervised pre-training on 1B Instagram hashtag dataset by Facebook Research\'),\n    _entry(\'ig_resnext101_32x48d\', \'ResNeXt-101 32x48d (288x288 Mean-Max Pooling)\', \'1805.00932\',\n           ttp=True, args=dict(img_size=288), batch_size=BATCH_SIZE // 8,\n           model_desc=\'Weakly-Supervised pre-training on 1B Instagram hashtag dataset by Facebook Research\'),\n\n    ## Facebook SSL weights\n    _entry(\'ssl_resnet18\', \'ResNet-18\', \'1905.00546\',\n           model_desc=\'Semi-Supervised pre-training on YFCC100M dataset by Facebook Research\'),\n    _entry(\'ssl_resnet50\', \'ResNet-50\', \'1905.00546\',\n           model_desc=\'Semi-Supervised pre-training on YFCC100M dataset by Facebook Research\'),\n    _entry(\'ssl_resnext50_32x4d\', \'ResNeXt-50 32x4d\', \'1905.00546\',\n           model_desc=\'Semi-Supervised pre-training on YFCC100M dataset by Facebook Research\'),\n    _entry(\'ssl_resnext101_32x4d\', \'ResNeXt-101 32x4d\', \'1905.00546\',\n           model_desc=\'Semi-Supervised pre-training on YFCC100M dataset by Facebook Research\'),\n    _entry(\'ssl_resnext101_32x8d\', \'ResNeXt-101 32x8d\', \'1905.00546\',\n           model_desc=\'Semi-Supervised pre-training on YFCC100M dataset by Facebook Research\'),\n    _entry(\'ssl_resnext101_32x16d\', \'ResNeXt-101 32x16d\', \'1905.00546\',\n           model_desc=\'Semi-Supervised pre-training on YFCC100M dataset by Facebook Research\'),\n\n    _entry(\'ssl_resnet50\', \'ResNet-50 (288x288 Mean-Max Pooling)\', \'1905.00546\',\n           ttp=True, args=dict(img_size=288),\n           model_desc=\'Semi-Supervised pre-training on YFCC100M dataset by Facebook Research\'),\n    _entry(\'ssl_resnext50_32x4d\', \'ResNeXt-50 32x4d (288x288 Mean-Max Pooling)\', \'1905.00546\',\n           ttp=True, args=dict(img_size=288),\n           model_desc=\'Semi-Supervised pre-training on YFCC100M dataset by Facebook Research\'),\n    _entry(\'ssl_resnext101_32x4d\', \'ResNeXt-101 32x4d (288x288 Mean-Max Pooling)\', \'1905.00546\',\n           ttp=True, args=dict(img_size=288),\n           model_desc=\'Semi-Supervised pre-training on YFCC100M dataset by Facebook Research\'),\n    _entry(\'ssl_resnext101_32x8d\', \'ResNeXt-101 32x8d (288x288 Mean-Max Pooling)\', \'1905.00546\',\n           ttp=True, args=dict(img_size=288),\n           model_desc=\'Semi-Supervised pre-training on YFCC100M dataset by Facebook Research\'),\n    _entry(\'ssl_resnext101_32x16d\', \'ResNeXt-101 32x16d (288x288 Mean-Max Pooling)\', \'1905.00546\',\n           ttp=True, args=dict(img_size=288), batch_size=BATCH_SIZE // 2,\n           model_desc=\'Semi-Supervised pre-training on YFCC100M dataset by Facebook Research\'),\n\n    ## Facebook SWSL weights\n    _entry(\'swsl_resnet18\', \'ResNet-18\', \'1905.00546\',\n           model_desc=\'Semi-Weakly-Supervised pre-training on 1 billion unlabelled dataset by Facebook Research\'),\n    _entry(\'swsl_resnet50\', \'ResNet-50\', \'1905.00546\',\n           model_desc=\'Semi-Weakly-Supervised pre-training on 1 billion unlabelled dataset by Facebook Research\'),\n    _entry(\'swsl_resnext50_32x4d\', \'ResNeXt-50 32x4d\', \'1905.00546\',\n           model_desc=\'Semi-Weakly-Supervised pre-training on 1 billion unlabelled dataset by Facebook Research\'),\n    _entry(\'swsl_resnext101_32x4d\', \'ResNeXt-101 32x4d\', \'1905.00546\',\n           model_desc=\'Semi-Weakly-Supervised pre-training on 1 billion unlabelled dataset by Facebook Research\'),\n    _entry(\'swsl_resnext101_32x8d\', \'ResNeXt-101 32x8d\', \'1905.00546\',\n           model_desc=\'Semi-Weakly-Supervised pre-training on 1 billion unlabelled dataset by Facebook Research\'),\n    _entry(\'swsl_resnext101_32x16d\', \'ResNeXt-101 32x16d\', \'1905.00546\',\n           model_desc=\'Semi-Weakly-Supervised pre-training on 1 billion unlabelled dataset by Facebook Research\'),\n\n    _entry(\'swsl_resnet50\', \'ResNet-50 (288x288 Mean-Max Pooling)\', \'1905.00546\',\n           ttp=True, args=dict(img_size=288),\n           model_desc=\'Semi-Weakly-Supervised pre-training on 1 billion unlabelled dataset by Facebook Research\'),\n    _entry(\'swsl_resnext50_32x4d\', \'ResNeXt-50 32x4d (288x288 Mean-Max Pooling)\', \'1905.00546\',\n           ttp=True, args=dict(img_size=288),\n           model_desc=\'Semi-Weakly-Supervised pre-training on 1 billion unlabelled dataset by Facebook Research\'),\n    _entry(\'swsl_resnext101_32x4d\', \'ResNeXt-101 32x4d (288x288 Mean-Max Pooling)\', \'1905.00546\',\n           ttp=True, args=dict(img_size=288),\n           model_desc=\'Semi-Weakly-Supervised pre-training on 1 billion unlabelled dataset by Facebook Research\'),\n    _entry(\'swsl_resnext101_32x8d\', \'ResNeXt-101 32x8d (288x288 Mean-Max Pooling)\', \'1905.00546\',\n           ttp=True, args=dict(img_size=288),\n           model_desc=\'Semi-Weakly-Supervised pre-training on 1 billion unlabelled dataset by Facebook Research\'),\n    _entry(\'swsl_resnext101_32x16d\', \'ResNeXt-101 32x16d (288x288 Mean-Max Pooling)\', \'1905.00546\',\n           ttp=True, args=dict(img_size=288), batch_size=BATCH_SIZE // 2,\n           model_desc=\'Semi-Weakly-Supervised pre-training on 1 billion unlabelled dataset by Facebook Research\'),\n\n    ## DLA official impl weights (to remove if sotabench added to source)\n    _entry(\'dla34\', \'DLA-34\', \'1707.06484\'),\n    _entry(\'dla46_c\', \'DLA-46-C\', \'1707.06484\'),\n    _entry(\'dla46x_c\', \'DLA-X-46-C\', \'1707.06484\'),\n    _entry(\'dla60x_c\', \'DLA-X-60-C\', \'1707.06484\'),\n    _entry(\'dla60\', \'DLA-60\', \'1707.06484\'),\n    _entry(\'dla60x\', \'DLA-X-60\', \'1707.06484\'),\n    _entry(\'dla102\', \'DLA-102\', \'1707.06484\'),\n    _entry(\'dla102x\', \'DLA-X-102\', \'1707.06484\'),\n    _entry(\'dla102x2\', \'DLA-X-102 64\', \'1707.06484\'),\n    _entry(\'dla169\', \'DLA-169\', \'1707.06484\'),\n\n    ## Res2Net official impl weights (to remove if sotabench added to source)\n    _entry(\'res2net50_26w_4s\', \'Res2Net-50 26x4s\', \'1904.01169\'),\n    _entry(\'res2net50_14w_8s\', \'Res2Net-50 14x8s\', \'1904.01169\'),\n    _entry(\'res2net50_26w_6s\', \'Res2Net-50 26x6s\', \'1904.01169\'),\n    _entry(\'res2net50_26w_8s\', \'Res2Net-50 26x8s\', \'1904.01169\'),\n    _entry(\'res2net50_48w_2s\', \'Res2Net-50 48x2s\', \'1904.01169\'),\n    _entry(\'res2net101_26w_4s\', \'Res2NeXt-101 26x4s\', \'1904.01169\'),\n    _entry(\'res2next50\', \'Res2NeXt-50\', \'1904.01169\'),\n    _entry(\'dla60_res2net\', \'Res2Net-DLA-60\', \'1904.01169\'),\n    _entry(\'dla60_res2next\', \'Res2NeXt-DLA-60\', \'1904.01169\'),\n\n    ## HRNet official impl weights\n    _entry(\'hrnet_w18_small\', \'HRNet-W18-C-Small-V1\', \'1908.07919\'),\n    _entry(\'hrnet_w18_small_v2\', \'HRNet-W18-C-Small-V2\', \'1908.07919\'),\n    _entry(\'hrnet_w18\', \'HRNet-W18-C\', \'1908.07919\'),\n    _entry(\'hrnet_w30\', \'HRNet-W30-C\', \'1908.07919\'),\n    _entry(\'hrnet_w32\', \'HRNet-W32-C\', \'1908.07919\'),\n    _entry(\'hrnet_w40\', \'HRNet-W40-C\', \'1908.07919\'),\n    _entry(\'hrnet_w44\', \'HRNet-W44-C\', \'1908.07919\'),\n    _entry(\'hrnet_w48\', \'HRNet-W48-C\', \'1908.07919\'),\n    _entry(\'hrnet_w64\', \'HRNet-W64-C\', \'1908.07919\'),\n\n\n    ## SelecSLS official impl weights\n    _entry(\'selecsls42b\', \'SelecSLS-42_B\', \'1907.00837\',\n           model_desc=\'Originally from https://github.com/mehtadushy/SelecSLS-Pytorch\'),\n    _entry(\'selecsls60\', \'SelecSLS-60\', \'1907.00837\',\n           model_desc=\'Originally from https://github.com/mehtadushy/SelecSLS-Pytorch\'),\n    _entry(\'selecsls60b\', \'SelecSLS-60_B\', \'1907.00837\',\n           model_desc=\'Originally from https://github.com/mehtadushy/SelecSLS-Pytorch\'),\n]\n\nfor m in model_list:\n    model_name = m[\'model\']\n    # create model from name\n    model = create_model(model_name, pretrained=True)\n    param_count = sum([m.numel() for m in model.parameters()])\n    print(\'Model %s, %s created. Param count: %d\' % (model_name, m[\'paper_model_name\'], param_count))\n\n    # get appropriate transform for model\'s default pretrained config\n    data_config = resolve_data_config(m[\'args\'], model=model, verbose=True)\n    if m[\'ttp\']:\n        model = TestTimePoolHead(model, model.default_cfg[\'pool_size\'])\n        data_config[\'crop_pct\'] = 1.0\n    input_transform = create_transform(**data_config)\n\n    # Run the benchmark\n    ImageNet.benchmark(\n        model=model,\n        model_description=m.get(\'model_description\', None),\n        paper_model_name=m[\'paper_model_name\'],\n        paper_arxiv_id=m[\'paper_arxiv_id\'],\n        input_transform=input_transform,\n        batch_size=m[\'batch_size\'],\n        num_gpu=NUM_GPU,\n        data_root=os.environ.get(\'IMAGENET_DIR\', \'./imagenet\')\n    )\n\n    torch.cuda.empty_cache()\n\n\n'"
train.py,12,"b'#!/usr/bin/env python\n"""""" ImageNet Training Script\n\nThis is intended to be a lean and easily modifiable ImageNet training script that reproduces ImageNet\ntraining results with some of the latest networks and training techniques. It favours canonical PyTorch\nand standard Python style over trying to be able to \'do it all.\' That said, it offers quite a few speed\nand training result improvements over the usual PyTorch example scripts. Repurpose as you see fit.\n\nThis script was started from an early version of the PyTorch ImageNet example\n(https://github.com/pytorch/examples/tree/master/imagenet)\n\nNVIDIA CUDA specific speedups adopted from NVIDIA Apex examples\n(https://github.com/NVIDIA/apex/tree/master/examples/imagenet)\n\nHacked together by Ross Wightman (https://github.com/rwightman)\n""""""\nimport argparse\nimport time\nimport yaml\nfrom datetime import datetime\n\ntry:\n    from apex import amp\n    from apex.parallel import DistributedDataParallel as DDP\n    from apex.parallel import convert_syncbn_model\n    has_apex = True\nexcept ImportError:\n    from torch.nn.parallel import DistributedDataParallel as DDP\n    has_apex = False\n\nfrom timm.data import Dataset, create_loader, resolve_data_config, FastCollateMixup, mixup_batch, AugMixDataset\nfrom timm.models import create_model, resume_checkpoint, convert_splitbn_model\nfrom timm.utils import *\nfrom timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy, JsdCrossEntropy\nfrom timm.optim import create_optimizer\nfrom timm.scheduler import create_scheduler\n\nimport torch\nimport torch.nn as nn\nimport torchvision.utils\n\ntorch.backends.cudnn.benchmark = True\n\n\n# The first arg parser parses out only the --config argument, this argument is used to\n# load a yaml file containing key-values that override the defaults for the main parser below\nconfig_parser = parser = argparse.ArgumentParser(description=\'Training Config\', add_help=False)\nparser.add_argument(\'-c\', \'--config\', default=\'\', type=str, metavar=\'FILE\',\n                    help=\'YAML config file specifying default arguments\')\n\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Training\')\n# Dataset / Model parameters\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--model\', default=\'resnet101\', type=str, metavar=\'MODEL\',\n                    help=\'Name of model to train (default: ""countception""\')\nparser.add_argument(\'--pretrained\', action=\'store_true\', default=False,\n                    help=\'Start with pretrained version of specified network (if avail)\')\nparser.add_argument(\'--initial-checkpoint\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'Initialize model from this checkpoint (default: none)\')\nparser.add_argument(\'--resume\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'Resume full model and optimizer state from checkpoint (default: none)\')\nparser.add_argument(\'--no-resume-opt\', action=\'store_true\', default=False,\n                    help=\'prevent resume of optimizer state when resuming model\')\nparser.add_argument(\'--num-classes\', type=int, default=1000, metavar=\'N\',\n                    help=\'number of label classes (default: 1000)\')\nparser.add_argument(\'--gp\', default=\'avg\', type=str, metavar=\'POOL\',\n                    help=\'Type of global pool, ""avg"", ""max"", ""avgmax"", ""avgmaxc"" (default: ""avg"")\')\nparser.add_argument(\'--img-size\', type=int, default=None, metavar=\'N\',\n                    help=\'Image patch size (default: None => model default)\')\nparser.add_argument(\'--crop-pct\', default=None, type=float,\n                    metavar=\'N\', help=\'Input image center crop percent (for validation only)\')\nparser.add_argument(\'--mean\', type=float, nargs=\'+\', default=None, metavar=\'MEAN\',\n                    help=\'Override mean pixel value of dataset\')\nparser.add_argument(\'--std\', type=float, nargs=\'+\', default=None, metavar=\'STD\',\n                    help=\'Override std deviation of of dataset\')\nparser.add_argument(\'--interpolation\', default=\'\', type=str, metavar=\'NAME\',\n                    help=\'Image resize interpolation type (overrides model)\')\nparser.add_argument(\'-b\', \'--batch-size\', type=int, default=32, metavar=\'N\',\n                    help=\'input batch size for training (default: 32)\')\nparser.add_argument(\'-vb\', \'--validation-batch-size-multiplier\', type=int, default=1, metavar=\'N\',\n                    help=\'ratio of validation batch size to training batch size (default: 1)\')\nparser.add_argument(\'--drop\', type=float, default=0.0, metavar=\'PCT\',\n                    help=\'Dropout rate (default: 0.)\')\nparser.add_argument(\'--drop-connect\', type=float, default=None, metavar=\'PCT\',\n                    help=\'Drop connect rate, DEPRECATED, use drop-path (default: None)\')\nparser.add_argument(\'--drop-path\', type=float, default=None, metavar=\'PCT\',\n                    help=\'Drop path rate (default: None)\')\nparser.add_argument(\'--drop-block\', type=float, default=None, metavar=\'PCT\',\n                    help=\'Drop block rate (default: None)\')\nparser.add_argument(\'--jsd\', action=\'store_true\', default=False,\n                    help=\'Enable Jensen-Shannon Divergence + CE loss. Use with `--aug-splits`.\')\n# Optimizer parameters\nparser.add_argument(\'--opt\', default=\'sgd\', type=str, metavar=\'OPTIMIZER\',\n                    help=\'Optimizer (default: ""sgd""\')\nparser.add_argument(\'--opt-eps\', default=1e-8, type=float, metavar=\'EPSILON\',\n                    help=\'Optimizer Epsilon (default: 1e-8)\')\nparser.add_argument(\'--momentum\', type=float, default=0.9, metavar=\'M\',\n                    help=\'SGD momentum (default: 0.9)\')\nparser.add_argument(\'--weight-decay\', type=float, default=0.0001,\n                    help=\'weight decay (default: 0.0001)\')\n# Learning rate schedule parameters\nparser.add_argument(\'--sched\', default=\'step\', type=str, metavar=\'SCHEDULER\',\n                    help=\'LR scheduler (default: ""step""\')\nparser.add_argument(\'--lr\', type=float, default=0.01, metavar=\'LR\',\n                    help=\'learning rate (default: 0.01)\')\nparser.add_argument(\'--lr-noise\', type=float, nargs=\'+\', default=None, metavar=\'pct, pct\',\n                    help=\'learning rate noise on/off epoch percentages\')\nparser.add_argument(\'--lr-noise-pct\', type=float, default=0.67, metavar=\'PERCENT\',\n                    help=\'learning rate noise limit percent (default: 0.67)\')\nparser.add_argument(\'--lr-noise-std\', type=float, default=1.0, metavar=\'STDDEV\',\n                    help=\'learning rate noise std-dev (default: 1.0)\')\nparser.add_argument(\'--warmup-lr\', type=float, default=0.0001, metavar=\'LR\',\n                    help=\'warmup learning rate (default: 0.0001)\')\nparser.add_argument(\'--min-lr\', type=float, default=1e-5, metavar=\'LR\',\n                    help=\'lower lr bound for cyclic schedulers that hit 0 (1e-5)\')\nparser.add_argument(\'--epochs\', type=int, default=200, metavar=\'N\',\n                    help=\'number of epochs to train (default: 2)\')\nparser.add_argument(\'--start-epoch\', default=None, type=int, metavar=\'N\',\n                    help=\'manual epoch number (useful on restarts)\')\nparser.add_argument(\'--decay-epochs\', type=float, default=30, metavar=\'N\',\n                    help=\'epoch interval to decay LR\')\nparser.add_argument(\'--warmup-epochs\', type=int, default=3, metavar=\'N\',\n                    help=\'epochs to warmup LR, if scheduler supports\')\nparser.add_argument(\'--cooldown-epochs\', type=int, default=10, metavar=\'N\',\n                    help=\'epochs to cooldown LR at min_lr, after cyclic schedule ends\')\nparser.add_argument(\'--patience-epochs\', type=int, default=10, metavar=\'N\',\n                    help=\'patience epochs for Plateau LR scheduler (default: 10\')\nparser.add_argument(\'--decay-rate\', \'--dr\', type=float, default=0.1, metavar=\'RATE\',\n                    help=\'LR decay rate (default: 0.1)\')\n# Augmentation parameters\nparser.add_argument(\'--color-jitter\', type=float, default=0.4, metavar=\'PCT\',\n                    help=\'Color jitter factor (default: 0.4)\')\nparser.add_argument(\'--aa\', type=str, default=None, metavar=\'NAME\',\n                    help=\'Use AutoAugment policy. ""v0"" or ""original"". (default: None)\'),\nparser.add_argument(\'--aug-splits\', type=int, default=0,\n                    help=\'Number of augmentation splits (default: 0, valid: 0 or >=2)\')\nparser.add_argument(\'--reprob\', type=float, default=0., metavar=\'PCT\',\n                    help=\'Random erase prob (default: 0.)\')\nparser.add_argument(\'--remode\', type=str, default=\'const\',\n                    help=\'Random erase mode (default: ""const"")\')\nparser.add_argument(\'--recount\', type=int, default=1,\n                    help=\'Random erase count (default: 1)\')\nparser.add_argument(\'--resplit\', action=\'store_true\', default=False,\n                    help=\'Do not random erase first (clean) augmentation split\')\nparser.add_argument(\'--mixup\', type=float, default=0.0,\n                    help=\'mixup alpha, mixup enabled if > 0. (default: 0.)\')\nparser.add_argument(\'--mixup-off-epoch\', default=0, type=int, metavar=\'N\',\n                    help=\'turn off mixup after this epoch, disabled if 0 (default: 0)\')\nparser.add_argument(\'--smoothing\', type=float, default=0.1,\n                    help=\'label smoothing (default: 0.1)\')\nparser.add_argument(\'--train-interpolation\', type=str, default=\'random\',\n                    help=\'Training interpolation (random, bilinear, bicubic default: ""random"")\')\n# Batch norm parameters (only works with gen_efficientnet based models currently)\nparser.add_argument(\'--bn-tf\', action=\'store_true\', default=False,\n                    help=\'Use Tensorflow BatchNorm defaults for models that support it (default: False)\')\nparser.add_argument(\'--bn-momentum\', type=float, default=None,\n                    help=\'BatchNorm momentum override (if not None)\')\nparser.add_argument(\'--bn-eps\', type=float, default=None,\n                    help=\'BatchNorm epsilon override (if not None)\')\nparser.add_argument(\'--sync-bn\', action=\'store_true\',\n                    help=\'Enable NVIDIA Apex or Torch synchronized BatchNorm.\')\nparser.add_argument(\'--dist-bn\', type=str, default=\'\',\n                    help=\'Distribute BatchNorm stats between nodes after each epoch (""broadcast"", ""reduce"", or """")\')\nparser.add_argument(\'--split-bn\', action=\'store_true\',\n                    help=\'Enable separate BN layers per augmentation split.\')\n# Model Exponential Moving Average\nparser.add_argument(\'--model-ema\', action=\'store_true\', default=False,\n                    help=\'Enable tracking moving average of model weights\')\nparser.add_argument(\'--model-ema-force-cpu\', action=\'store_true\', default=False,\n                    help=\'Force ema to be tracked on CPU, rank=0 node only. Disables EMA validation.\')\nparser.add_argument(\'--model-ema-decay\', type=float, default=0.9998,\n                    help=\'decay factor for model weights moving average (default: 0.9998)\')\n# Misc\nparser.add_argument(\'--seed\', type=int, default=42, metavar=\'S\',\n                    help=\'random seed (default: 42)\')\nparser.add_argument(\'--log-interval\', type=int, default=50, metavar=\'N\',\n                    help=\'how many batches to wait before logging training status\')\nparser.add_argument(\'--recovery-interval\', type=int, default=0, metavar=\'N\',\n                    help=\'how many batches to wait before writing recovery checkpoint\')\nparser.add_argument(\'-j\', \'--workers\', type=int, default=4, metavar=\'N\',\n                    help=\'how many training processes to use (default: 1)\')\nparser.add_argument(\'--num-gpu\', type=int, default=1,\n                    help=\'Number of GPUS to use\')\nparser.add_argument(\'--save-images\', action=\'store_true\', default=False,\n                    help=\'save images of input bathes every log interval for debugging\')\nparser.add_argument(\'--amp\', action=\'store_true\', default=False,\n                    help=\'use NVIDIA amp for mixed precision training\')\nparser.add_argument(\'--pin-mem\', action=\'store_true\', default=False,\n                    help=\'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\')\nparser.add_argument(\'--no-prefetcher\', action=\'store_true\', default=False,\n                    help=\'disable fast prefetcher\')\nparser.add_argument(\'--output\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to output folder (default: none, current dir)\')\nparser.add_argument(\'--eval-metric\', default=\'top1\', type=str, metavar=\'EVAL_METRIC\',\n                    help=\'Best metric (default: ""top1""\')\nparser.add_argument(\'--tta\', type=int, default=0, metavar=\'N\',\n                    help=\'Test/inference time augmentation (oversampling) factor. 0=None (default: 0)\')\nparser.add_argument(""--local_rank"", default=0, type=int)\nparser.add_argument(\'--use-multi-epochs-loader\', action=\'store_true\', default=False,\n                    help=\'use the multi-epochs-loader to save time at the beginning of every epoch\')\n\n\ndef _parse_args():\n    # Do we have a config file to parse?\n    args_config, remaining = config_parser.parse_known_args()\n    if args_config.config:\n        with open(args_config.config, \'r\') as f:\n            cfg = yaml.safe_load(f)\n            parser.set_defaults(**cfg)\n\n    # The main arg parser parses the rest of the args, the usual\n    # defaults will have been overridden if config file specified.\n    args = parser.parse_args(remaining)\n\n    # Cache the args as a text string to save them in the output dir later\n    args_text = yaml.safe_dump(args.__dict__, default_flow_style=False)\n    return args, args_text\n\n\ndef main():\n    setup_default_logging()\n    args, args_text = _parse_args()\n\n    args.prefetcher = not args.no_prefetcher\n    args.distributed = False\n    if \'WORLD_SIZE\' in os.environ:\n        args.distributed = int(os.environ[\'WORLD_SIZE\']) > 1\n        if args.distributed and args.num_gpu > 1:\n            logging.warning(\'Using more than one GPU per process in distributed mode is not allowed. Setting num_gpu to 1.\')\n            args.num_gpu = 1\n\n    args.device = \'cuda:0\'\n    args.world_size = 1\n    args.rank = 0  # global rank\n    if args.distributed:\n        args.num_gpu = 1\n        args.device = \'cuda:%d\' % args.local_rank\n        torch.cuda.set_device(args.local_rank)\n        torch.distributed.init_process_group(backend=\'nccl\', init_method=\'env://\')\n        args.world_size = torch.distributed.get_world_size()\n        args.rank = torch.distributed.get_rank()\n    assert args.rank >= 0\n\n    if args.distributed:\n        logging.info(\'Training in distributed mode with multiple processes, 1 GPU per process. Process %d, total %d.\'\n                     % (args.rank, args.world_size))\n    else:\n        logging.info(\'Training with a single process on %d GPUs.\' % args.num_gpu)\n\n    torch.manual_seed(args.seed + args.rank)\n\n    model = create_model(\n        args.model,\n        pretrained=args.pretrained,\n        num_classes=args.num_classes,\n        drop_rate=args.drop,\n        drop_connect_rate=args.drop_connect,  # DEPRECATED, use drop_path\n        drop_path_rate=args.drop_path,\n        drop_block_rate=args.drop_block,\n        global_pool=args.gp,\n        bn_tf=args.bn_tf,\n        bn_momentum=args.bn_momentum,\n        bn_eps=args.bn_eps,\n        checkpoint_path=args.initial_checkpoint)\n\n    if args.local_rank == 0:\n        logging.info(\'Model %s created, param count: %d\' %\n                     (args.model, sum([m.numel() for m in model.parameters()])))\n\n    data_config = resolve_data_config(vars(args), model=model, verbose=args.local_rank == 0)\n\n    num_aug_splits = 0\n    if args.aug_splits > 0:\n        assert args.aug_splits > 1, \'A split of 1 makes no sense\'\n        num_aug_splits = args.aug_splits\n\n    if args.split_bn:\n        assert num_aug_splits > 1 or args.resplit\n        model = convert_splitbn_model(model, max(num_aug_splits, 2))\n\n    if args.num_gpu > 1:\n        if args.amp:\n            logging.warning(\n                \'AMP does not work well with nn.DataParallel, disabling. Use distributed mode for multi-GPU AMP.\')\n            args.amp = False\n        model = nn.DataParallel(model, device_ids=list(range(args.num_gpu))).cuda()\n    else:\n        model.cuda()\n\n    optimizer = create_optimizer(args, model)\n\n    use_amp = False\n    if has_apex and args.amp:\n        model, optimizer = amp.initialize(model, optimizer, opt_level=\'O1\')\n        use_amp = True\n    if args.local_rank == 0:\n        logging.info(\'NVIDIA APEX {}. AMP {}.\'.format(\n            \'installed\' if has_apex else \'not installed\', \'on\' if use_amp else \'off\'))\n\n    # optionally resume from a checkpoint\n    resume_state = {}\n    resume_epoch = None\n    if args.resume:\n        resume_state, resume_epoch = resume_checkpoint(model, args.resume)\n    if resume_state and not args.no_resume_opt:\n        if \'optimizer\' in resume_state:\n            if args.local_rank == 0:\n                logging.info(\'Restoring Optimizer state from checkpoint\')\n            optimizer.load_state_dict(resume_state[\'optimizer\'])\n        if use_amp and \'amp\' in resume_state and \'load_state_dict\' in amp.__dict__:\n            if args.local_rank == 0:\n                logging.info(\'Restoring NVIDIA AMP state from checkpoint\')\n            amp.load_state_dict(resume_state[\'amp\'])\n    del resume_state\n\n    model_ema = None\n    if args.model_ema:\n        # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper\n        model_ema = ModelEma(\n            model,\n            decay=args.model_ema_decay,\n            device=\'cpu\' if args.model_ema_force_cpu else \'\',\n            resume=args.resume)\n\n    if args.distributed:\n        if args.sync_bn:\n            assert not args.split_bn\n            try:\n                if has_apex:\n                    model = convert_syncbn_model(model)\n                else:\n                    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\n                if args.local_rank == 0:\n                    logging.info(\n                        \'Converted model to use Synchronized BatchNorm. WARNING: You may have issues if using \'\n                        \'zero initialized BN layers (enabled by default for ResNets) while sync-bn enabled.\')\n            except Exception as e:\n                logging.error(\'Failed to enable Synchronized BatchNorm. Install Apex or Torch >= 1.1\')\n        if has_apex:\n            model = DDP(model, delay_allreduce=True)\n        else:\n            if args.local_rank == 0:\n                logging.info(""Using torch DistributedDataParallel. Install NVIDIA Apex for Apex DDP."")\n            model = DDP(model, device_ids=[args.local_rank])  # can use device str in Torch >= 1.1\n        # NOTE: EMA model does not need to be wrapped by DDP\n\n    lr_scheduler, num_epochs = create_scheduler(args, optimizer)\n    start_epoch = 0\n    if args.start_epoch is not None:\n        # a specified start_epoch will always override the resume epoch\n        start_epoch = args.start_epoch\n    elif resume_epoch is not None:\n        start_epoch = resume_epoch\n    if lr_scheduler is not None and start_epoch > 0:\n        lr_scheduler.step(start_epoch)\n\n    if args.local_rank == 0:\n        logging.info(\'Scheduled epochs: {}\'.format(num_epochs))\n\n    train_dir = os.path.join(args.data, \'train\')\n    if not os.path.exists(train_dir):\n        logging.error(\'Training folder does not exist at: {}\'.format(train_dir))\n        exit(1)\n    dataset_train = Dataset(train_dir)\n\n    collate_fn = None\n    if args.prefetcher and args.mixup > 0:\n        assert not num_aug_splits  # collate conflict (need to support deinterleaving in collate mixup)\n        collate_fn = FastCollateMixup(args.mixup, args.smoothing, args.num_classes)\n\n    if num_aug_splits > 1:\n        dataset_train = AugMixDataset(dataset_train, num_splits=num_aug_splits)\n\n    loader_train = create_loader(\n        dataset_train,\n        input_size=data_config[\'input_size\'],\n        batch_size=args.batch_size,\n        is_training=True,\n        use_prefetcher=args.prefetcher,\n        re_prob=args.reprob,\n        re_mode=args.remode,\n        re_count=args.recount,\n        re_split=args.resplit,\n        color_jitter=args.color_jitter,\n        auto_augment=args.aa,\n        num_aug_splits=num_aug_splits,\n        interpolation=args.train_interpolation,\n        mean=data_config[\'mean\'],\n        std=data_config[\'std\'],\n        num_workers=args.workers,\n        distributed=args.distributed,\n        collate_fn=collate_fn,\n        pin_memory=args.pin_mem,\n        use_multi_epochs_loader=args.use_multi_epochs_loader\n    )\n\n    eval_dir = os.path.join(args.data, \'val\')\n    if not os.path.isdir(eval_dir):\n        eval_dir = os.path.join(args.data, \'validation\')\n        if not os.path.isdir(eval_dir):\n            logging.error(\'Validation folder does not exist at: {}\'.format(eval_dir))\n            exit(1)\n    dataset_eval = Dataset(eval_dir)\n\n    loader_eval = create_loader(\n        dataset_eval,\n        input_size=data_config[\'input_size\'],\n        batch_size=args.validation_batch_size_multiplier * args.batch_size,\n        is_training=False,\n        use_prefetcher=args.prefetcher,\n        interpolation=data_config[\'interpolation\'],\n        mean=data_config[\'mean\'],\n        std=data_config[\'std\'],\n        num_workers=args.workers,\n        distributed=args.distributed,\n        crop_pct=data_config[\'crop_pct\'],\n        pin_memory=args.pin_mem,\n    )\n\n    if args.jsd:\n        assert num_aug_splits > 1  # JSD only valid with aug splits set\n        train_loss_fn = JsdCrossEntropy(num_splits=num_aug_splits, smoothing=args.smoothing).cuda()\n        validate_loss_fn = nn.CrossEntropyLoss().cuda()\n    elif args.mixup > 0.:\n        # smoothing is handled with mixup label transform\n        train_loss_fn = SoftTargetCrossEntropy().cuda()\n        validate_loss_fn = nn.CrossEntropyLoss().cuda()\n    elif args.smoothing:\n        train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing).cuda()\n        validate_loss_fn = nn.CrossEntropyLoss().cuda()\n    else:\n        train_loss_fn = nn.CrossEntropyLoss().cuda()\n        validate_loss_fn = train_loss_fn\n\n    eval_metric = args.eval_metric\n    best_metric = None\n    best_epoch = None\n    saver = None\n    output_dir = \'\'\n    if args.local_rank == 0:\n        output_base = args.output if args.output else \'./output\'\n        exp_name = \'-\'.join([\n            datetime.now().strftime(""%Y%m%d-%H%M%S""),\n            args.model,\n            str(data_config[\'input_size\'][-1])\n        ])\n        output_dir = get_outdir(output_base, \'train\', exp_name)\n        decreasing = True if eval_metric == \'loss\' else False\n        saver = CheckpointSaver(checkpoint_dir=output_dir, decreasing=decreasing)\n        with open(os.path.join(output_dir, \'args.yaml\'), \'w\') as f:\n            f.write(args_text)\n\n    try:\n        for epoch in range(start_epoch, num_epochs):\n            if args.distributed:\n                loader_train.sampler.set_epoch(epoch)\n\n            train_metrics = train_epoch(\n                epoch, model, loader_train, optimizer, train_loss_fn, args,\n                lr_scheduler=lr_scheduler, saver=saver, output_dir=output_dir,\n                use_amp=use_amp, model_ema=model_ema)\n\n            if args.distributed and args.dist_bn in (\'broadcast\', \'reduce\'):\n                if args.local_rank == 0:\n                    logging.info(""Distributing BatchNorm running means and vars"")\n                distribute_bn(model, args.world_size, args.dist_bn == \'reduce\')\n\n            eval_metrics = validate(model, loader_eval, validate_loss_fn, args)\n\n            if model_ema is not None and not args.model_ema_force_cpu:\n                if args.distributed and args.dist_bn in (\'broadcast\', \'reduce\'):\n                    distribute_bn(model_ema, args.world_size, args.dist_bn == \'reduce\')\n\n                ema_eval_metrics = validate(\n                    model_ema.ema, loader_eval, validate_loss_fn, args, log_suffix=\' (EMA)\')\n                eval_metrics = ema_eval_metrics\n\n            if lr_scheduler is not None:\n                # step LR for next epoch\n                lr_scheduler.step(epoch + 1, eval_metrics[eval_metric])\n\n            update_summary(\n                epoch, train_metrics, eval_metrics, os.path.join(output_dir, \'summary.csv\'),\n                write_header=best_metric is None)\n\n            if saver is not None:\n                # save proper checkpoint with eval metric\n                save_metric = eval_metrics[eval_metric]\n                best_metric, best_epoch = saver.save_checkpoint(\n                    model, optimizer, args,\n                    epoch=epoch, model_ema=model_ema, metric=save_metric, use_amp=use_amp)\n\n    except KeyboardInterrupt:\n        pass\n    if best_metric is not None:\n        logging.info(\'*** Best metric: {0} (epoch {1})\'.format(best_metric, best_epoch))\n\n\ndef train_epoch(\n        epoch, model, loader, optimizer, loss_fn, args,\n        lr_scheduler=None, saver=None, output_dir=\'\', use_amp=False, model_ema=None):\n\n    if args.prefetcher and args.mixup > 0 and loader.mixup_enabled:\n        if args.mixup_off_epoch and epoch >= args.mixup_off_epoch:\n            loader.mixup_enabled = False\n\n    batch_time_m = AverageMeter()\n    data_time_m = AverageMeter()\n    losses_m = AverageMeter()\n\n    model.train()\n\n    end = time.time()\n    last_idx = len(loader) - 1\n    num_updates = epoch * len(loader)\n    for batch_idx, (input, target) in enumerate(loader):\n        last_batch = batch_idx == last_idx\n        data_time_m.update(time.time() - end)\n        if not args.prefetcher:\n            input, target = input.cuda(), target.cuda()\n            if args.mixup > 0.:\n                input, target = mixup_batch(\n                    input, target,\n                    alpha=args.mixup, num_classes=args.num_classes, smoothing=args.smoothing,\n                    disable=args.mixup_off_epoch and epoch >= args.mixup_off_epoch)\n\n        output = model(input)\n\n        loss = loss_fn(output, target)\n        if not args.distributed:\n            losses_m.update(loss.item(), input.size(0))\n\n        optimizer.zero_grad()\n        if use_amp:\n            with amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            loss.backward()\n        optimizer.step()\n\n        torch.cuda.synchronize()\n        if model_ema is not None:\n            model_ema.update(model)\n        num_updates += 1\n\n        batch_time_m.update(time.time() - end)\n        if last_batch or batch_idx % args.log_interval == 0:\n            lrl = [param_group[\'lr\'] for param_group in optimizer.param_groups]\n            lr = sum(lrl) / len(lrl)\n\n            if args.distributed:\n                reduced_loss = reduce_tensor(loss.data, args.world_size)\n                losses_m.update(reduced_loss.item(), input.size(0))\n\n            if args.local_rank == 0:\n                logging.info(\n                    \'Train: {} [{:>4d}/{} ({:>3.0f}%)]  \'\n                    \'Loss: {loss.val:>9.6f} ({loss.avg:>6.4f})  \'\n                    \'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  \'\n                    \'({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  \'\n                    \'LR: {lr:.3e}  \'\n                    \'Data: {data_time.val:.3f} ({data_time.avg:.3f})\'.format(\n                        epoch,\n                        batch_idx, len(loader),\n                        100. * batch_idx / last_idx,\n                        loss=losses_m,\n                        batch_time=batch_time_m,\n                        rate=input.size(0) * args.world_size / batch_time_m.val,\n                        rate_avg=input.size(0) * args.world_size / batch_time_m.avg,\n                        lr=lr,\n                        data_time=data_time_m))\n\n                if args.save_images and output_dir:\n                    torchvision.utils.save_image(\n                        input,\n                        os.path.join(output_dir, \'train-batch-%d.jpg\' % batch_idx),\n                        padding=0,\n                        normalize=True)\n\n        if saver is not None and args.recovery_interval and (\n                last_batch or (batch_idx + 1) % args.recovery_interval == 0):\n            saver.save_recovery(\n                model, optimizer, args, epoch, model_ema=model_ema, use_amp=use_amp, batch_idx=batch_idx)\n\n        if lr_scheduler is not None:\n            lr_scheduler.step_update(num_updates=num_updates, metric=losses_m.avg)\n\n        end = time.time()\n        # end for\n\n    if hasattr(optimizer, \'sync_lookahead\'):\n        optimizer.sync_lookahead()\n\n    return OrderedDict([(\'loss\', losses_m.avg)])\n\n\ndef validate(model, loader, loss_fn, args, log_suffix=\'\'):\n    batch_time_m = AverageMeter()\n    losses_m = AverageMeter()\n    top1_m = AverageMeter()\n    top5_m = AverageMeter()\n\n    model.eval()\n\n    end = time.time()\n    last_idx = len(loader) - 1\n    with torch.no_grad():\n        for batch_idx, (input, target) in enumerate(loader):\n            last_batch = batch_idx == last_idx\n            if not args.prefetcher:\n                input = input.cuda()\n                target = target.cuda()\n\n            output = model(input)\n            if isinstance(output, (tuple, list)):\n                output = output[0]\n\n            # augmentation reduction\n            reduce_factor = args.tta\n            if reduce_factor > 1:\n                output = output.unfold(0, reduce_factor, reduce_factor).mean(dim=2)\n                target = target[0:target.size(0):reduce_factor]\n\n            loss = loss_fn(output, target)\n            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n\n            if args.distributed:\n                reduced_loss = reduce_tensor(loss.data, args.world_size)\n                acc1 = reduce_tensor(acc1, args.world_size)\n                acc5 = reduce_tensor(acc5, args.world_size)\n            else:\n                reduced_loss = loss.data\n\n            torch.cuda.synchronize()\n\n            losses_m.update(reduced_loss.item(), input.size(0))\n            top1_m.update(acc1.item(), output.size(0))\n            top5_m.update(acc5.item(), output.size(0))\n\n            batch_time_m.update(time.time() - end)\n            end = time.time()\n            if args.local_rank == 0 and (last_batch or batch_idx % args.log_interval == 0):\n                log_name = \'Test\' + log_suffix\n                logging.info(\n                    \'{0}: [{1:>4d}/{2}]  \'\n                    \'Time: {batch_time.val:.3f} ({batch_time.avg:.3f})  \'\n                    \'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  \'\n                    \'Acc@1: {top1.val:>7.4f} ({top1.avg:>7.4f})  \'\n                    \'Acc@5: {top5.val:>7.4f} ({top5.avg:>7.4f})\'.format(\n                        log_name, batch_idx, last_idx, batch_time=batch_time_m,\n                        loss=losses_m, top1=top1_m, top5=top5_m))\n\n    metrics = OrderedDict([(\'loss\', losses_m.avg), (\'top1\', top1_m.avg), (\'top5\', top5_m.avg)])\n\n    return metrics\n\n\nif __name__ == \'__main__\':\n    main()\n'"
validate.py,7,"b'#!/usr/bin/env python\n"""""" ImageNet Validation Script\n\nThis is intended to be a lean and easily modifiable ImageNet validation script for evaluating pretrained\nmodels or training checkpoints against ImageNet or similarly organized image datasets. It prioritizes\ncanonical PyTorch, standard Python style, and good performance. Repurpose as you see fit.\n\nHacked together by Ross Wightman (https://github.com/rwightman)\n""""""\nimport argparse\nimport os\nimport csv\nimport glob\nimport time\nimport logging\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nfrom collections import OrderedDict\n\ntry:\n    from apex import amp\n    has_apex = True\nexcept ImportError:\n    has_apex = False\n\nfrom timm.models import create_model, apply_test_time_pool, load_checkpoint, is_model, list_models\nfrom timm.data import Dataset, DatasetTar, create_loader, resolve_data_config\nfrom timm.utils import accuracy, AverageMeter, natural_key, setup_default_logging\n\ntorch.backends.cudnn.benchmark = True\n\nparser = argparse.ArgumentParser(description=\'PyTorch ImageNet Validation\')\nparser.add_argument(\'data\', metavar=\'DIR\',\n                    help=\'path to dataset\')\nparser.add_argument(\'--model\', \'-m\', metavar=\'MODEL\', default=\'dpn92\',\n                    help=\'model architecture (default: dpn92)\')\nparser.add_argument(\'-j\', \'--workers\', default=4, type=int, metavar=\'N\',\n                    help=\'number of data loading workers (default: 2)\')\nparser.add_argument(\'-b\', \'--batch-size\', default=256, type=int,\n                    metavar=\'N\', help=\'mini-batch size (default: 256)\')\nparser.add_argument(\'--img-size\', default=None, type=int,\n                    metavar=\'N\', help=\'Input image dimension, uses model default if empty\')\nparser.add_argument(\'--crop-pct\', default=None, type=float,\n                    metavar=\'N\', help=\'Input image center crop pct\')\nparser.add_argument(\'--mean\', type=float, nargs=\'+\', default=None, metavar=\'MEAN\',\n                    help=\'Override mean pixel value of dataset\')\nparser.add_argument(\'--std\', type=float,  nargs=\'+\', default=None, metavar=\'STD\',\n                    help=\'Override std deviation of of dataset\')\nparser.add_argument(\'--interpolation\', default=\'\', type=str, metavar=\'NAME\',\n                    help=\'Image resize interpolation type (overrides model)\')\nparser.add_argument(\'--num-classes\', type=int, default=1000,\n                    help=\'Number classes in dataset\')\nparser.add_argument(\'--class-map\', default=\'\', type=str, metavar=\'FILENAME\',\n                    help=\'path to class to idx mapping file (default: """")\')\nparser.add_argument(\'--log-freq\', default=10, type=int,\n                    metavar=\'N\', help=\'batch logging frequency (default: 10)\')\nparser.add_argument(\'--checkpoint\', default=\'\', type=str, metavar=\'PATH\',\n                    help=\'path to latest checkpoint (default: none)\')\nparser.add_argument(\'--pretrained\', dest=\'pretrained\', action=\'store_true\',\n                    help=\'use pre-trained model\')\nparser.add_argument(\'--num-gpu\', type=int, default=1,\n                    help=\'Number of GPUS to use\')\nparser.add_argument(\'--no-test-pool\', dest=\'no_test_pool\', action=\'store_true\',\n                    help=\'disable test time pool\')\nparser.add_argument(\'--no-prefetcher\', action=\'store_true\', default=False,\n                    help=\'disable fast prefetcher\')\nparser.add_argument(\'--pin-mem\', action=\'store_true\', default=False,\n                    help=\'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\')\nparser.add_argument(\'--amp\', action=\'store_true\', default=False,\n                    help=\'Use AMP mixed precision\')\nparser.add_argument(\'--tf-preprocessing\', action=\'store_true\', default=False,\n                    help=\'Use Tensorflow preprocessing pipeline (require CPU TF installed\')\nparser.add_argument(\'--use-ema\', dest=\'use_ema\', action=\'store_true\',\n                    help=\'use ema version of weights if present\')\nparser.add_argument(\'--torchscript\', dest=\'torchscript\', action=\'store_true\',\n                    help=\'convert model torchscript for inference\')\nparser.add_argument(\'--results-file\', default=\'\', type=str, metavar=\'FILENAME\',\n                    help=\'Output csv file for validation results (summary)\')\n\n\ndef validate(args):\n    # might as well try to validate something\n    args.pretrained = args.pretrained or not args.checkpoint\n    args.prefetcher = not args.no_prefetcher\n\n    # create model\n    model = create_model(\n        args.model,\n        num_classes=args.num_classes,\n        in_chans=3,\n        pretrained=args.pretrained)\n\n    if args.checkpoint:\n        load_checkpoint(model, args.checkpoint, args.use_ema)\n\n    param_count = sum([m.numel() for m in model.parameters()])\n    logging.info(\'Model %s created, param count: %d\' % (args.model, param_count))\n\n    data_config = resolve_data_config(vars(args), model=model)\n    model, test_time_pool = apply_test_time_pool(model, data_config, args)\n\n    if args.torchscript:\n        torch.jit.optimized_execution(True)\n        model = torch.jit.script(model)\n\n    if args.amp:\n        model = amp.initialize(model.cuda(), opt_level=\'O1\')\n    else:\n        model = model.cuda()\n\n    if args.num_gpu > 1:\n        model = torch.nn.DataParallel(model, device_ids=list(range(args.num_gpu)))\n\n    criterion = nn.CrossEntropyLoss().cuda()\n\n    #from torchvision.datasets import ImageNet\n    #dataset = ImageNet(args.data, split=\'val\')\n    if os.path.splitext(args.data)[1] == \'.tar\' and os.path.isfile(args.data):\n        dataset = DatasetTar(args.data, load_bytes=args.tf_preprocessing, class_map=args.class_map)\n    else:\n        dataset = Dataset(args.data, load_bytes=args.tf_preprocessing, class_map=args.class_map)\n\n    crop_pct = 1.0 if test_time_pool else data_config[\'crop_pct\']\n    loader = create_loader(\n        dataset,\n        input_size=data_config[\'input_size\'],\n        batch_size=args.batch_size,\n        use_prefetcher=args.prefetcher,\n        interpolation=data_config[\'interpolation\'],\n        mean=data_config[\'mean\'],\n        std=data_config[\'std\'],\n        num_workers=args.workers,\n        crop_pct=crop_pct,\n        pin_memory=args.pin_mem,\n        tf_preprocessing=args.tf_preprocessing)\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    model.eval()\n    end = time.time()\n    with torch.no_grad():\n        for i, (input, target) in enumerate(loader):\n            if args.no_prefetcher:\n                target = target.cuda()\n                input = input.cuda()\n                if args.fp16:\n                    input = input.half()\n\n            # compute output\n            output = model(input)\n            loss = criterion(output, target)\n\n            # measure accuracy and record loss\n            acc1, acc5 = accuracy(output.data, target, topk=(1, 5))\n            losses.update(loss.item(), input.size(0))\n            top1.update(acc1.item(), input.size(0))\n            top5.update(acc5.item(), input.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % args.log_freq == 0:\n                logging.info(\n                    \'Test: [{0:>4d}/{1}]  \'\n                    \'Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  \'\n                    \'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  \'\n                    \'Acc@1: {top1.val:>7.3f} ({top1.avg:>7.3f})  \'\n                    \'Acc@5: {top5.val:>7.3f} ({top5.avg:>7.3f})\'.format(\n                        i, len(loader), batch_time=batch_time,\n                        rate_avg=input.size(0) / batch_time.avg,\n                        loss=losses, top1=top1, top5=top5))\n\n    results = OrderedDict(\n        top1=round(top1.avg, 4), top1_err=round(100 - top1.avg, 4),\n        top5=round(top5.avg, 4), top5_err=round(100 - top5.avg, 4),\n        param_count=round(param_count / 1e6, 2),\n        img_size=data_config[\'input_size\'][-1],\n        cropt_pct=crop_pct,\n        interpolation=data_config[\'interpolation\'])\n\n    logging.info(\' * Acc@1 {:.3f} ({:.3f}) Acc@5 {:.3f} ({:.3f})\'.format(\n       results[\'top1\'], results[\'top1_err\'], results[\'top5\'], results[\'top5_err\']))\n\n    return results\n\n\ndef main():\n    setup_default_logging()\n    args = parser.parse_args()\n    model_cfgs = []\n    model_names = []\n    if os.path.isdir(args.checkpoint):\n        # validate all checkpoints in a path with same model\n        checkpoints = glob.glob(args.checkpoint + \'/*.pth.tar\')\n        checkpoints += glob.glob(args.checkpoint + \'/*.pth\')\n        model_names = list_models(args.model)\n        model_cfgs = [(args.model, c) for c in sorted(checkpoints, key=natural_key)]\n    else:\n        if args.model == \'all\':\n            # validate all models in a list of names with pretrained checkpoints\n            args.pretrained = True\n            model_names = list_models(pretrained=True)\n            model_cfgs = [(n, \'\') for n in model_names]\n        elif not is_model(args.model):\n            # model name doesn\'t exist, try as wildcard filter\n            model_names = list_models(args.model)\n            model_cfgs = [(n, \'\') for n in model_names]\n\n    if len(model_cfgs):\n        results_file = args.results_file or \'./results-all.csv\'\n        logging.info(\'Running bulk validation on these pretrained models: {}\'.format(\', \'.join(model_names)))\n        results = []\n        try:\n            start_batch_size = args.batch_size\n            for m, c in model_cfgs:\n                batch_size = start_batch_size\n                args.model = m\n                args.checkpoint = c\n                result = OrderedDict(model=args.model)\n                r = {}\n                while not r and batch_size >= args.num_gpu:\n                    try:\n                        args.batch_size = batch_size\n                        print(\'Validating with batch size: %d\' % args.batch_size)\n                        r = validate(args)\n                    except RuntimeError as e:\n                        if batch_size <= args.num_gpu:\n                            print(""Validation failed with no ability to reduce batch size. Exiting."")\n                            raise e\n                        batch_size = max(batch_size // 2, args.num_gpu)\n                        print(""Validation failed, reducing batch size by 50%"")\n                result.update(r)\n                if args.checkpoint:\n                    result[\'checkpoint\'] = args.checkpoint\n                results.append(result)\n        except KeyboardInterrupt as e:\n            pass\n        results = sorted(results, key=lambda x: x[\'top1\'], reverse=True)\n        if len(results):\n            write_results(results_file, results)\n    else:\n        validate(args)\n\n\ndef write_results(results_file, results):\n    with open(results_file, mode=\'w\') as cf:\n        dw = csv.DictWriter(cf, fieldnames=results[0].keys())\n        dw.writeheader()\n        for r in results:\n            dw.writerow(r)\n        cf.flush()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
convert/convert_from_mxnet.py,3,"b'import argparse\nimport hashlib\nimport os\n\nimport mxnet as mx\nimport gluoncv\nimport torch\nfrom timm import create_model\n\nparser = argparse.ArgumentParser(description=\'Convert from MXNet\')\nparser.add_argument(\'--model\', default=\'all\', type=str, metavar=\'MODEL\',\n                    help=\'Name of model to train (default: ""all""\')\n\n\ndef convert(mxnet_name, torch_name):\n    # download and load the pre-trained model\n    net = gluoncv.model_zoo.get_model(mxnet_name, pretrained=True)\n\n    # create corresponding torch model\n    torch_net = create_model(torch_name)\n\n    mxp = [(k, v) for k, v in net.collect_params().items() if \'running\' not in k]\n    torchp = list(torch_net.named_parameters())\n    torch_params = {}\n\n    # convert parameters\n    # NOTE: we are relying on the fact that the order of parameters\n    # are usually exactly the same between these models, thus no key name mapping\n    # is necessary. Asserts will trip if this is not the case.\n    for (tn, tv), (mn, mv) in zip(torchp, mxp):\n        m_split = mn.split(\'_\')\n        t_split = tn.split(\'.\')\n        print(t_split, m_split)\n        print(tv.shape, mv.shape)\n\n        # ensure ordering of BN params match since their sizes are not specific\n        if m_split[-1] == \'gamma\':\n            assert t_split[-1] == \'weight\'\n        if m_split[-1] == \'beta\':\n            assert t_split[-1] == \'bias\'\n\n        # ensure shapes match\n        assert all(t == m for t, m in zip(tv.shape, mv.shape))\n\n        torch_tensor = torch.from_numpy(mv.data().asnumpy())\n        torch_params[tn] = torch_tensor\n\n    # convert buffers (batch norm running stats)\n    mxb = [(k, v) for k, v in net.collect_params().items() if any(x in k for x in [\'running_mean\', \'running_var\'])]\n    torchb = [(k, v) for k, v in torch_net.named_buffers() if \'num_batches\' not in k]\n    for (tn, tv), (mn, mv) in zip(torchb, mxb):\n        print(tn, mn)\n        print(tv.shape, mv.shape)\n\n        # ensure ordering of BN params match since their sizes are not specific\n        if \'running_var\' in tn:\n            assert \'running_var\' in mn\n        if \'running_mean\' in tn:\n            assert \'running_mean\' in mn\n            \n        torch_tensor = torch.from_numpy(mv.data().asnumpy())\n        torch_params[tn] = torch_tensor\n\n    torch_net.load_state_dict(torch_params)\n    torch_filename = \'./%s.pth\' % torch_name\n    torch.save(torch_net.state_dict(), torch_filename)\n    with open(torch_filename, \'rb\') as f:\n        sha_hash = hashlib.sha256(f.read()).hexdigest()\n    final_filename = os.path.splitext(torch_filename)[0] + \'-\' + sha_hash[:8] + \'.pth\'\n    os.rename(torch_filename, final_filename)\n    print(""=> Saved converted model to \'{}, SHA256: {}\'"".format(final_filename, sha_hash))\n\n\ndef map_mx_to_torch_model(mx_name):\n    torch_name = mx_name.lower()\n    if torch_name.startswith(\'se_\'):\n        torch_name = torch_name.replace(\'se_\', \'se\')\n    elif torch_name.startswith(\'senet_\'):\n        torch_name = torch_name.replace(\'senet_\', \'senet\')\n    elif torch_name.startswith(\'inceptionv3\'):\n        torch_name = torch_name.replace(\'inceptionv3\', \'inception_v3\')\n    torch_name = \'gluon_\' + torch_name\n    return torch_name\n\n\nALL = [\'resnet18_v1b\', \'resnet34_v1b\', \'resnet50_v1b\', \'resnet101_v1b\', \'resnet152_v1b\',\n       \'resnet50_v1c\', \'resnet101_v1c\', \'resnet152_v1c\', \'resnet50_v1d\', \'resnet101_v1d\', \'resnet152_v1d\',\n       #\'resnet50_v1e\', \'resnet101_v1e\', \'resnet152_v1e\',\n       \'resnet50_v1s\', \'resnet101_v1s\', \'resnet152_v1s\', \'resnext50_32x4d\', \'resnext101_32x4d\', \'resnext101_64x4d\',\n       \'se_resnext50_32x4d\', \'se_resnext101_32x4d\', \'se_resnext101_64x4d\', \'senet_154\', \'inceptionv3\']\n\n\ndef main():\n    args = parser.parse_args()\n\n    if not args.model or args.model == \'all\':\n        for mx_model in ALL:\n            torch_model = map_mx_to_torch_model(mx_model)\n            convert(mx_model, torch_model)\n    else:\n        mx_model = args.model\n        torch_model = map_mx_to_torch_model(mx_model)\n        convert(mx_model, torch_model)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tests/__init__.py,0,b''
tests/test_models.py,5,"b'import pytest\nimport torch\nimport platform\nimport os\nimport fnmatch\n\nfrom timm import list_models, create_model\n\n\nif \'GITHUB_ACTIONS\' in os.environ and \'Linux\' in platform.system():\n    # GitHub Linux runner is slower and hits memory limits sooner than MacOS, exclude bigger models\n    EXCLUDE_FILTERS = [\'*efficientnet_l2*\', \'*resnext101_32x48d\']\nelse:\n    EXCLUDE_FILTERS = []\nMAX_FWD_SIZE = 384\nMAX_BWD_SIZE = 128\nMAX_FWD_FEAT_SIZE = 448\n\n\n@pytest.mark.timeout(120)\n@pytest.mark.parametrize(\'model_name\', list_models(exclude_filters=EXCLUDE_FILTERS))\n@pytest.mark.parametrize(\'batch_size\', [1])\ndef test_model_forward(model_name, batch_size):\n    """"""Run a single forward pass with each model""""""\n    model = create_model(model_name, pretrained=False)\n    model.eval()\n\n    input_size = model.default_cfg[\'input_size\']\n    if any([x > MAX_FWD_SIZE for x in input_size]):\n        # cap forward test at max res 448 * 448 to keep resource down\n        input_size = tuple([min(x, MAX_FWD_SIZE) for x in input_size])\n    inputs = torch.randn((batch_size, *input_size))\n    outputs = model(inputs)\n\n    assert outputs.shape[0] == batch_size\n    assert not torch.isnan(outputs).any(), \'Output included NaNs\'\n\n\n@pytest.mark.timeout(120)\n# DLA models have an issue TBD, add them to exclusions\n@pytest.mark.parametrize(\'model_name\', list_models(exclude_filters=EXCLUDE_FILTERS + [\'dla*\']))\n@pytest.mark.parametrize(\'batch_size\', [2])\ndef test_model_backward(model_name, batch_size):\n    """"""Run a single forward pass with each model""""""\n    model = create_model(model_name, pretrained=False, num_classes=42)\n    num_params = sum([x.numel() for x in model.parameters()])\n    model.eval()\n\n    input_size = model.default_cfg[\'input_size\']\n    if any([x > MAX_BWD_SIZE for x in input_size]):\n        # cap backward test at 128 * 128 to keep resource usage down\n        input_size = tuple([min(x, MAX_BWD_SIZE) for x in input_size])\n    inputs = torch.randn((batch_size, *input_size))\n    outputs = model(inputs)\n    outputs.mean().backward()\n    num_grad = sum([x.grad.numel() for x in model.parameters() if x.grad is not None])\n\n    assert outputs.shape[-1] == 42\n    assert num_params == num_grad, \'Some parameters are missing gradients\'\n    assert not torch.isnan(outputs).any(), \'Output included NaNs\'\n\n\n@pytest.mark.timeout(120)\n@pytest.mark.parametrize(\'model_name\', list_models())\n@pytest.mark.parametrize(\'batch_size\', [1])\ndef test_model_default_cfgs(model_name, batch_size):\n    """"""Run a single forward pass with each model""""""\n    model = create_model(model_name, pretrained=False)\n    model.eval()\n    state_dict = model.state_dict()\n    cfg = model.default_cfg\n\n    classifier = cfg[\'classifier\']\n    first_conv = cfg[\'first_conv\']\n    pool_size = cfg[\'pool_size\']\n    input_size = model.default_cfg[\'input_size\']\n\n    if all([x <= MAX_FWD_FEAT_SIZE for x in input_size]) and \\\n            not any([fnmatch.fnmatch(model_name, x) for x in EXCLUDE_FILTERS]):\n        # pool size only checked if default res <= 448 * 448 to keep resource down\n        input_size = tuple([min(x, MAX_FWD_FEAT_SIZE) for x in input_size])\n        outputs = model.forward_features(torch.randn((batch_size, *input_size)))\n        assert outputs.shape[-1] == pool_size[-1] and outputs.shape[-2] == pool_size[-2]\n    assert any([k.startswith(classifier) for k in state_dict.keys()]), f\'{classifier} not in model params\'\n    assert any([k.startswith(first_conv) for k in state_dict.keys()]), f\'{first_conv} not in model params\'\n'"
timm/__init__.py,0,"b'from .version import __version__\nfrom .models import create_model, list_models, is_model, list_modules, model_entrypoint\n'"
timm/utils.py,6,"b'from copy import deepcopy\n\nimport torch\nimport math\nimport os\nimport re\nimport shutil\nimport glob\nimport csv\nimport operator\nimport logging\nimport numpy as np\nfrom collections import OrderedDict\ntry:\n    from apex import amp\n    has_apex = True\nexcept ImportError:\n    amp = None\n    has_apex = False\n\nfrom torch import distributed as dist\n\n\ndef unwrap_model(model):\n    if isinstance(model, ModelEma):\n        return unwrap_model(model.ema)\n    else:\n        return model.module if hasattr(model, \'module\') else model\n\n\ndef get_state_dict(model):\n    return unwrap_model(model).state_dict()\n\n\nclass CheckpointSaver:\n    def __init__(\n            self,\n            checkpoint_prefix=\'checkpoint\',\n            recovery_prefix=\'recovery\',\n            checkpoint_dir=\'\',\n            recovery_dir=\'\',\n            decreasing=False,\n            max_history=10):\n\n        # state\n        self.checkpoint_files = []  # (filename, metric) tuples in order of decreasing betterness\n        self.best_epoch = None\n        self.best_metric = None\n        self.curr_recovery_file = \'\'\n        self.last_recovery_file = \'\'\n\n        # config\n        self.checkpoint_dir = checkpoint_dir\n        self.recovery_dir = recovery_dir\n        self.save_prefix = checkpoint_prefix\n        self.recovery_prefix = recovery_prefix\n        self.extension = \'.pth.tar\'\n        self.decreasing = decreasing  # a lower metric is better if True\n        self.cmp = operator.lt if decreasing else operator.gt  # True if lhs better than rhs\n        self.max_history = max_history\n        assert self.max_history >= 1\n\n    def save_checkpoint(self, model, optimizer, args, epoch, model_ema=None, metric=None, use_amp=False):\n        assert epoch >= 0\n        tmp_save_path = os.path.join(self.checkpoint_dir, \'tmp\' + self.extension)\n        last_save_path = os.path.join(self.checkpoint_dir, \'last\' + self.extension)\n        self._save(tmp_save_path, model, optimizer, args, epoch, model_ema, metric, use_amp)\n        if os.path.exists(last_save_path):\n            os.unlink(last_save_path) # required for Windows support.\n        os.rename(tmp_save_path, last_save_path)\n        worst_file = self.checkpoint_files[-1] if self.checkpoint_files else None\n        if (len(self.checkpoint_files) < self.max_history\n                or metric is None or self.cmp(metric, worst_file[1])):\n            if len(self.checkpoint_files) >= self.max_history:\n                self._cleanup_checkpoints(1)\n            filename = \'-\'.join([self.save_prefix, str(epoch)]) + self.extension\n            save_path = os.path.join(self.checkpoint_dir, filename)\n            os.link(last_save_path, save_path)\n            self.checkpoint_files.append((save_path, metric))\n            self.checkpoint_files = sorted(\n                self.checkpoint_files, key=lambda x: x[1],\n                reverse=not self.decreasing)  # sort in descending order if a lower metric is not better\n\n            checkpoints_str = ""Current checkpoints:\\n""\n            for c in self.checkpoint_files:\n                checkpoints_str += \' {}\\n\'.format(c)\n            logging.info(checkpoints_str)\n\n            if metric is not None and (self.best_metric is None or self.cmp(metric, self.best_metric)):\n                self.best_epoch = epoch\n                self.best_metric = metric\n                best_save_path = os.path.join(self.checkpoint_dir, \'model_best\' + self.extension)\n                if os.path.exists(best_save_path):\n                    os.unlink(best_save_path)\n                os.link(last_save_path, best_save_path)\n\n        return (None, None) if self.best_metric is None else (self.best_metric, self.best_epoch)\n\n    def _save(self, save_path, model, optimizer, args, epoch, model_ema=None, metric=None, use_amp=False):\n        save_state = {\n            \'epoch\': epoch,\n            \'arch\': args.model,\n            \'state_dict\': get_state_dict(model),\n            \'optimizer\': optimizer.state_dict(),\n            \'args\': args,\n            \'version\': 2,  # version < 2 increments epoch before save\n        }\n        if use_amp and \'state_dict\' in amp.__dict__:\n            save_state[\'amp\'] = amp.state_dict()\n        if model_ema is not None:\n            save_state[\'state_dict_ema\'] = get_state_dict(model_ema)\n        if metric is not None:\n            save_state[\'metric\'] = metric\n        torch.save(save_state, save_path)\n\n    def _cleanup_checkpoints(self, trim=0):\n        trim = min(len(self.checkpoint_files), trim)\n        delete_index = self.max_history - trim\n        if delete_index <= 0 or len(self.checkpoint_files) <= delete_index:\n            return\n        to_delete = self.checkpoint_files[delete_index:]\n        for d in to_delete:\n            try:\n                logging.debug(""Cleaning checkpoint: {}"".format(d))\n                os.remove(d[0])\n            except Exception as e:\n                logging.error(""Exception \'{}\' while deleting checkpoint"".format(e))\n        self.checkpoint_files = self.checkpoint_files[:delete_index]\n\n    def save_recovery(self, model, optimizer, args, epoch, model_ema=None, use_amp=False, batch_idx=0):\n        assert epoch >= 0\n        filename = \'-\'.join([self.recovery_prefix, str(epoch), str(batch_idx)]) + self.extension\n        save_path = os.path.join(self.recovery_dir, filename)\n        self._save(save_path, model, optimizer, args, epoch, model_ema, use_amp=use_amp)\n        if os.path.exists(self.last_recovery_file):\n            try:\n                logging.debug(""Cleaning recovery: {}"".format(self.last_recovery_file))\n                os.remove(self.last_recovery_file)\n            except Exception as e:\n                logging.error(""Exception \'{}\' while removing {}"".format(e, self.last_recovery_file))\n        self.last_recovery_file = self.curr_recovery_file\n        self.curr_recovery_file = save_path\n\n    def find_recovery(self):\n        recovery_path = os.path.join(self.recovery_dir, self.recovery_prefix)\n        files = glob.glob(recovery_path + \'*\' + self.extension)\n        files = sorted(files)\n        if len(files):\n            return files[0]\n        else:\n            return \'\'\n\n\nclass AverageMeter:\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the accuracy over the k top predictions for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n    return [correct[:k].view(-1).float().sum(0) * 100. / batch_size for k in topk]\n\n\ndef get_outdir(path, *paths, inc=False):\n    outdir = os.path.join(path, *paths)\n    if not os.path.exists(outdir):\n        os.makedirs(outdir)\n    elif inc:\n        count = 1\n        outdir_inc = outdir + \'-\' + str(count)\n        while os.path.exists(outdir_inc):\n            count = count + 1\n            outdir_inc = outdir + \'-\' + str(count)\n            assert count < 100\n        outdir = outdir_inc\n        os.makedirs(outdir)\n    return outdir\n\n\ndef update_summary(epoch, train_metrics, eval_metrics, filename, write_header=False):\n    rowd = OrderedDict(epoch=epoch)\n    rowd.update([(\'train_\' + k, v) for k, v in train_metrics.items()])\n    rowd.update([(\'eval_\' + k, v) for k, v in eval_metrics.items()])\n    with open(filename, mode=\'a\') as cf:\n        dw = csv.DictWriter(cf, fieldnames=rowd.keys())\n        if write_header:  # first iteration (epoch == 1 can\'t be used)\n            dw.writeheader()\n        dw.writerow(rowd)\n\n\ndef natural_key(string_):\n    """"""See http://www.codinghorror.com/blog/archives/001018.html""""""\n    return [int(s) if s.isdigit() else s for s in re.split(r\'(\\d+)\', string_.lower())]\n\n\ndef reduce_tensor(tensor, n):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    rt /= n\n    return rt\n\n\ndef distribute_bn(model, world_size, reduce=False):\n    # ensure every node has the same running bn stats\n    for bn_name, bn_buf in unwrap_model(model).named_buffers(recurse=True):\n        if (\'running_mean\' in bn_name) or (\'running_var\' in bn_name):\n            if reduce:\n                # average bn stats across whole group\n                torch.distributed.all_reduce(bn_buf, op=dist.ReduceOp.SUM)\n                bn_buf /= float(world_size)\n            else:\n                # broadcast bn stats from rank 0 to whole group\n                torch.distributed.broadcast(bn_buf, 0)\n\n\nclass ModelEma:\n    """""" Model Exponential Moving Average\n    Keep a moving average of everything in the model state_dict (parameters and buffers).\n\n    This is intended to allow functionality like\n    https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n\n    A smoothed version of the weights is necessary for some training schemes to perform well.\n    E.g. Google\'s hyper-params for training MNASNet, MobileNet-V3, EfficientNet, etc that use\n    RMSprop with a short 2.4-3 epoch decay period and slow LR decay rate of .96-.99 requires EMA\n    smoothing of weights to match results. Pay attention to the decay constant you are using\n    relative to your update count per epoch.\n\n    To keep EMA from using GPU resources, set device=\'cpu\'. This will save a bit of memory but\n    disable validation of the EMA weights. Validation will have to be done manually in a separate\n    process, or after the training stops converging.\n\n    This class is sensitive where it is initialized in the sequence of model init,\n    GPU assignment and distributed training wrappers.\n    I\'ve tested with the sequence in my own train.py for torch.DataParallel, apex.DDP, and single-GPU.\n    """"""\n    def __init__(self, model, decay=0.9999, device=\'\', resume=\'\'):\n        # make a copy of the model for accumulating moving average of weights\n        self.ema = deepcopy(model)\n        self.ema.eval()\n        self.decay = decay\n        self.device = device  # perform ema on different device from model if set\n        if device:\n            self.ema.to(device=device)\n        self.ema_has_module = hasattr(self.ema, \'module\')\n        if resume:\n            self._load_checkpoint(resume)\n        for p in self.ema.parameters():\n            p.requires_grad_(False)\n\n    def _load_checkpoint(self, checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location=\'cpu\')\n        assert isinstance(checkpoint, dict)\n        if \'state_dict_ema\' in checkpoint:\n            new_state_dict = OrderedDict()\n            for k, v in checkpoint[\'state_dict_ema\'].items():\n                # ema model may have been wrapped by DataParallel, and need module prefix\n                if self.ema_has_module:\n                    name = \'module.\' + k if not k.startswith(\'module\') else k\n                else:\n                    name = k\n                new_state_dict[name] = v\n            self.ema.load_state_dict(new_state_dict)\n            logging.info(""Loaded state_dict_ema"")\n        else:\n            logging.warning(""Failed to find state_dict_ema, starting from loaded model weights"")\n\n    def update(self, model):\n        # correct a mismatch in state dict keys\n        needs_module = hasattr(model, \'module\') and not self.ema_has_module\n        with torch.no_grad():\n            msd = model.state_dict()\n            for k, ema_v in self.ema.state_dict().items():\n                if needs_module:\n                    k = \'module.\' + k\n                model_v = msd[k].detach()\n                if self.device:\n                    model_v = model_v.to(device=self.device)\n                ema_v.copy_(ema_v * self.decay + (1. - self.decay) * model_v)\n\n\nclass FormatterNoInfo(logging.Formatter):\n    def __init__(self, fmt=\'%(levelname)s: %(message)s\'):\n        logging.Formatter.__init__(self, fmt)\n\n    def format(self, record):\n        if record.levelno == logging.INFO:\n            return str(record.getMessage())\n        return logging.Formatter.format(self, record)\n\n\ndef setup_default_logging(default_level=logging.INFO):\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(FormatterNoInfo())\n    logging.root.addHandler(console_handler)\n    logging.root.setLevel(default_level)\n'"
timm/version.py,0,"b""__version__ = '0.1.26'\n"""
timm/data/__init__.py,0,"b'from .constants import *\nfrom .config import resolve_data_config\nfrom .dataset import Dataset, DatasetTar, AugMixDataset\nfrom .transforms import *\nfrom .loader import create_loader\nfrom .transforms_factory import create_transform\nfrom .mixup import mixup_batch, FastCollateMixup\nfrom .auto_augment import RandAugment, AutoAugment, rand_augment_ops, auto_augment_policy,\\\n    rand_augment_transform, auto_augment_transform\n'"
timm/data/auto_augment.py,0,"b'"""""" AutoAugment, RandAugment, and AugMix for PyTorch\n\nThis code implements the searched ImageNet policies with various tweaks and improvements and\ndoes not include any of the search code.\n\nAA and RA Implementation adapted from:\n    https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py\n\nAugMix adapted from:\n    https://github.com/google-research/augmix\n\nPapers:\n    AutoAugment: Learning Augmentation Policies from Data - https://arxiv.org/abs/1805.09501\n    Learning Data Augmentation Strategies for Object Detection - https://arxiv.org/abs/1906.11172\n    RandAugment: Practical automated data augmentation... - https://arxiv.org/abs/1909.13719\n    AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty - https://arxiv.org/abs/1912.02781\n\nHacked together by Ross Wightman\n""""""\nimport random\nimport math\nimport re\nfrom PIL import Image, ImageOps, ImageEnhance, ImageChops\nimport PIL\nimport numpy as np\n\n\n_PIL_VER = tuple([int(x) for x in PIL.__version__.split(\'.\')[:2]])\n\n_FILL = (128, 128, 128)\n\n# This signifies the max integer that the controller RNN could predict for the\n# augmentation scheme.\n_MAX_LEVEL = 10.\n\n_HPARAMS_DEFAULT = dict(\n    translate_const=250,\n    img_mean=_FILL,\n)\n\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n\n\ndef _interpolation(kwargs):\n    interpolation = kwargs.pop(\'resample\', Image.BILINEAR)\n    if isinstance(interpolation, (list, tuple)):\n        return random.choice(interpolation)\n    else:\n        return interpolation\n\n\ndef _check_args_tf(kwargs):\n    if \'fillcolor\' in kwargs and _PIL_VER < (5, 0):\n        kwargs.pop(\'fillcolor\')\n    kwargs[\'resample\'] = _interpolation(kwargs)\n\n\ndef shear_x(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, factor, 0, 0, 1, 0), **kwargs)\n\n\ndef shear_y(img, factor, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, factor, 1, 0), **kwargs)\n\n\ndef translate_x_rel(img, pct, **kwargs):\n    pixels = pct * img.size[0]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_rel(img, pct, **kwargs):\n    pixels = pct * img.size[1]\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n\n\ndef translate_x_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, pixels, 0, 1, 0), **kwargs)\n\n\ndef translate_y_abs(img, pixels, **kwargs):\n    _check_args_tf(kwargs)\n    return img.transform(img.size, Image.AFFINE, (1, 0, 0, 0, 1, pixels), **kwargs)\n\n\ndef rotate(img, degrees, **kwargs):\n    _check_args_tf(kwargs)\n    if _PIL_VER >= (5, 2):\n        return img.rotate(degrees, **kwargs)\n    elif _PIL_VER >= (5, 0):\n        w, h = img.size\n        post_trans = (0, 0)\n        rotn_center = (w / 2.0, h / 2.0)\n        angle = -math.radians(degrees)\n        matrix = [\n            round(math.cos(angle), 15),\n            round(math.sin(angle), 15),\n            0.0,\n            round(-math.sin(angle), 15),\n            round(math.cos(angle), 15),\n            0.0,\n        ]\n\n        def transform(x, y, matrix):\n            (a, b, c, d, e, f) = matrix\n            return a * x + b * y + c, d * x + e * y + f\n\n        matrix[2], matrix[5] = transform(\n            -rotn_center[0] - post_trans[0], -rotn_center[1] - post_trans[1], matrix\n        )\n        matrix[2] += rotn_center[0]\n        matrix[5] += rotn_center[1]\n        return img.transform(img.size, Image.AFFINE, matrix, **kwargs)\n    else:\n        return img.rotate(degrees, resample=kwargs[\'resample\'])\n\n\ndef auto_contrast(img, **__):\n    return ImageOps.autocontrast(img)\n\n\ndef invert(img, **__):\n    return ImageOps.invert(img)\n\n\ndef equalize(img, **__):\n    return ImageOps.equalize(img)\n\n\ndef solarize(img, thresh, **__):\n    return ImageOps.solarize(img, thresh)\n\n\ndef solarize_add(img, add, thresh=128, **__):\n    lut = []\n    for i in range(256):\n        if i < thresh:\n            lut.append(min(255, i + add))\n        else:\n            lut.append(i)\n    if img.mode in (""L"", ""RGB""):\n        if img.mode == ""RGB"" and len(lut) == 256:\n            lut = lut + lut + lut\n        return img.point(lut)\n    else:\n        return img\n\n\ndef posterize(img, bits_to_keep, **__):\n    if bits_to_keep >= 8:\n        return img\n    return ImageOps.posterize(img, bits_to_keep)\n\n\ndef contrast(img, factor, **__):\n    return ImageEnhance.Contrast(img).enhance(factor)\n\n\ndef color(img, factor, **__):\n    return ImageEnhance.Color(img).enhance(factor)\n\n\ndef brightness(img, factor, **__):\n    return ImageEnhance.Brightness(img).enhance(factor)\n\n\ndef sharpness(img, factor, **__):\n    return ImageEnhance.Sharpness(img).enhance(factor)\n\n\ndef _randomly_negate(v):\n    """"""With 50% prob, negate the value""""""\n    return -v if random.random() > 0.5 else v\n\n\ndef _rotate_level_to_arg(level, _hparams):\n    # range [-30, 30]\n    level = (level / _MAX_LEVEL) * 30.\n    level = _randomly_negate(level)\n    return level,\n\n\ndef _enhance_level_to_arg(level, _hparams):\n    # range [0.1, 1.9]\n    return (level / _MAX_LEVEL) * 1.8 + 0.1,\n\n\ndef _enhance_increasing_level_to_arg(level, _hparams):\n    # the \'no change\' level is 1.0, moving away from that towards 0. or 2.0 increases the enhancement blend\n    # range [0.1, 1.9]\n    level = (level / _MAX_LEVEL) * .9\n    level = 1.0 + _randomly_negate(level)\n    return level,\n\n\ndef _shear_level_to_arg(level, _hparams):\n    # range [-0.3, 0.3]\n    level = (level / _MAX_LEVEL) * 0.3\n    level = _randomly_negate(level)\n    return level,\n\n\ndef _translate_abs_level_to_arg(level, hparams):\n    translate_const = hparams[\'translate_const\']\n    level = (level / _MAX_LEVEL) * float(translate_const)\n    level = _randomly_negate(level)\n    return level,\n\n\ndef _translate_rel_level_to_arg(level, hparams):\n    # default range [-0.45, 0.45]\n    translate_pct = hparams.get(\'translate_pct\', 0.45)\n    level = (level / _MAX_LEVEL) * translate_pct\n    level = _randomly_negate(level)\n    return level,\n\n\ndef _posterize_level_to_arg(level, _hparams):\n    # As per Tensorflow TPU EfficientNet impl\n    # range [0, 4], \'keep 0 up to 4 MSB of original image\'\n    # intensity/severity of augmentation decreases with level\n    return int((level / _MAX_LEVEL) * 4),\n\n\ndef _posterize_increasing_level_to_arg(level, hparams):\n    # As per Tensorflow models research and UDA impl\n    # range [4, 0], \'keep 4 down to 0 MSB of original image\',\n    # intensity/severity of augmentation increases with level\n    return 4 - _posterize_level_to_arg(level, hparams)[0],\n\n\ndef _posterize_original_level_to_arg(level, _hparams):\n    # As per original AutoAugment paper description\n    # range [4, 8], \'keep 4 up to 8 MSB of image\'\n    # intensity/severity of augmentation decreases with level\n    return int((level / _MAX_LEVEL) * 4) + 4,\n\n\ndef _solarize_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation decreases with level\n    return int((level / _MAX_LEVEL) * 256),\n\n\ndef _solarize_increasing_level_to_arg(level, _hparams):\n    # range [0, 256]\n    # intensity/severity of augmentation increases with level\n    return 256 - _solarize_level_to_arg(level, _hparams)[0],\n\n\ndef _solarize_add_level_to_arg(level, _hparams):\n    # range [0, 110]\n    return int((level / _MAX_LEVEL) * 110),\n\n\nLEVEL_TO_ARG = {\n    \'AutoContrast\': None,\n    \'Equalize\': None,\n    \'Invert\': None,\n    \'Rotate\': _rotate_level_to_arg,\n    # There are several variations of the posterize level scaling in various Tensorflow/Google repositories/papers\n    \'Posterize\': _posterize_level_to_arg,\n    \'PosterizeIncreasing\': _posterize_increasing_level_to_arg,\n    \'PosterizeOriginal\': _posterize_original_level_to_arg,\n    \'Solarize\': _solarize_level_to_arg,\n    \'SolarizeIncreasing\': _solarize_increasing_level_to_arg,\n    \'SolarizeAdd\': _solarize_add_level_to_arg,\n    \'Color\': _enhance_level_to_arg,\n    \'ColorIncreasing\': _enhance_increasing_level_to_arg,\n    \'Contrast\': _enhance_level_to_arg,\n    \'ContrastIncreasing\': _enhance_increasing_level_to_arg,\n    \'Brightness\': _enhance_level_to_arg,\n    \'BrightnessIncreasing\': _enhance_increasing_level_to_arg,\n    \'Sharpness\': _enhance_level_to_arg,\n    \'SharpnessIncreasing\': _enhance_increasing_level_to_arg,\n    \'ShearX\': _shear_level_to_arg,\n    \'ShearY\': _shear_level_to_arg,\n    \'TranslateX\': _translate_abs_level_to_arg,\n    \'TranslateY\': _translate_abs_level_to_arg,\n    \'TranslateXRel\': _translate_rel_level_to_arg,\n    \'TranslateYRel\': _translate_rel_level_to_arg,\n}\n\n\nNAME_TO_OP = {\n    \'AutoContrast\': auto_contrast,\n    \'Equalize\': equalize,\n    \'Invert\': invert,\n    \'Rotate\': rotate,\n    \'Posterize\': posterize,\n    \'PosterizeIncreasing\': posterize,\n    \'PosterizeOriginal\': posterize,\n    \'Solarize\': solarize,\n    \'SolarizeIncreasing\': solarize,\n    \'SolarizeAdd\': solarize_add,\n    \'Color\': color,\n    \'ColorIncreasing\': color,\n    \'Contrast\': contrast,\n    \'ContrastIncreasing\': contrast,\n    \'Brightness\': brightness,\n    \'BrightnessIncreasing\': brightness,\n    \'Sharpness\': sharpness,\n    \'SharpnessIncreasing\': sharpness,\n    \'ShearX\': shear_x,\n    \'ShearY\': shear_y,\n    \'TranslateX\': translate_x_abs,\n    \'TranslateY\': translate_y_abs,\n    \'TranslateXRel\': translate_x_rel,\n    \'TranslateYRel\': translate_y_rel,\n}\n\n\nclass AugmentOp:\n\n    def __init__(self, name, prob=0.5, magnitude=10, hparams=None):\n        hparams = hparams or _HPARAMS_DEFAULT\n        self.aug_fn = NAME_TO_OP[name]\n        self.level_fn = LEVEL_TO_ARG[name]\n        self.prob = prob\n        self.magnitude = magnitude\n        self.hparams = hparams.copy()\n        self.kwargs = dict(\n            fillcolor=hparams[\'img_mean\'] if \'img_mean\' in hparams else _FILL,\n            resample=hparams[\'interpolation\'] if \'interpolation\' in hparams else _RANDOM_INTERPOLATION,\n        )\n\n        # If magnitude_std is > 0, we introduce some randomness\n        # in the usually fixed policy and sample magnitude from a normal distribution\n        # with mean `magnitude` and std-dev of `magnitude_std`.\n        # NOTE This is my own hack, being tested, not in papers or reference impls.\n        self.magnitude_std = self.hparams.get(\'magnitude_std\', 0)\n\n    def __call__(self, img):\n        if self.prob < 1.0 and random.random() > self.prob:\n            return img\n        magnitude = self.magnitude\n        if self.magnitude_std and self.magnitude_std > 0:\n            magnitude = random.gauss(magnitude, self.magnitude_std)\n        magnitude = min(_MAX_LEVEL, max(0, magnitude))  # clip to valid range\n        level_args = self.level_fn(magnitude, self.hparams) if self.level_fn is not None else tuple()\n        return self.aug_fn(img, *level_args, **self.kwargs)\n\n\ndef auto_augment_policy_v0(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, cannot find a paper reference.\n    policy = [\n        [(\'Equalize\', 0.8, 1), (\'ShearY\', 0.8, 4)],\n        [(\'Color\', 0.4, 9), (\'Equalize\', 0.6, 3)],\n        [(\'Color\', 0.4, 1), (\'Rotate\', 0.6, 8)],\n        [(\'Solarize\', 0.8, 3), (\'Equalize\', 0.4, 7)],\n        [(\'Solarize\', 0.4, 2), (\'Solarize\', 0.6, 2)],\n        [(\'Color\', 0.2, 0), (\'Equalize\', 0.8, 8)],\n        [(\'Equalize\', 0.4, 8), (\'SolarizeAdd\', 0.8, 3)],\n        [(\'ShearX\', 0.2, 9), (\'Rotate\', 0.6, 8)],\n        [(\'Color\', 0.6, 1), (\'Equalize\', 1.0, 2)],\n        [(\'Invert\', 0.4, 9), (\'Rotate\', 0.6, 0)],\n        [(\'Equalize\', 1.0, 9), (\'ShearY\', 0.6, 3)],\n        [(\'Color\', 0.4, 7), (\'Equalize\', 0.6, 0)],\n        [(\'Posterize\', 0.4, 6), (\'AutoContrast\', 0.4, 7)],\n        [(\'Solarize\', 0.6, 8), (\'Color\', 0.6, 9)],\n        [(\'Solarize\', 0.2, 4), (\'Rotate\', 0.8, 9)],\n        [(\'Rotate\', 1.0, 7), (\'TranslateYRel\', 0.8, 9)],\n        [(\'ShearX\', 0.0, 0), (\'Solarize\', 0.8, 4)],\n        [(\'ShearY\', 0.8, 0), (\'Color\', 0.6, 4)],\n        [(\'Color\', 1.0, 0), (\'Rotate\', 0.6, 2)],\n        [(\'Equalize\', 0.8, 4), (\'Equalize\', 0.0, 8)],\n        [(\'Equalize\', 1.0, 4), (\'AutoContrast\', 0.6, 2)],\n        [(\'ShearY\', 0.4, 7), (\'SolarizeAdd\', 0.6, 7)],\n        [(\'Posterize\', 0.8, 2), (\'Solarize\', 0.6, 10)],  # This results in black image with Tpu posterize\n        [(\'Solarize\', 0.6, 8), (\'Equalize\', 0.6, 1)],\n        [(\'Color\', 0.8, 6), (\'Rotate\', 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_v0r(hparams):\n    # ImageNet v0 policy from TPU EfficientNet impl, with variation of Posterize used\n    # in Google research implementation (number of bits discarded increases with magnitude)\n    policy = [\n        [(\'Equalize\', 0.8, 1), (\'ShearY\', 0.8, 4)],\n        [(\'Color\', 0.4, 9), (\'Equalize\', 0.6, 3)],\n        [(\'Color\', 0.4, 1), (\'Rotate\', 0.6, 8)],\n        [(\'Solarize\', 0.8, 3), (\'Equalize\', 0.4, 7)],\n        [(\'Solarize\', 0.4, 2), (\'Solarize\', 0.6, 2)],\n        [(\'Color\', 0.2, 0), (\'Equalize\', 0.8, 8)],\n        [(\'Equalize\', 0.4, 8), (\'SolarizeAdd\', 0.8, 3)],\n        [(\'ShearX\', 0.2, 9), (\'Rotate\', 0.6, 8)],\n        [(\'Color\', 0.6, 1), (\'Equalize\', 1.0, 2)],\n        [(\'Invert\', 0.4, 9), (\'Rotate\', 0.6, 0)],\n        [(\'Equalize\', 1.0, 9), (\'ShearY\', 0.6, 3)],\n        [(\'Color\', 0.4, 7), (\'Equalize\', 0.6, 0)],\n        [(\'PosterizeIncreasing\', 0.4, 6), (\'AutoContrast\', 0.4, 7)],\n        [(\'Solarize\', 0.6, 8), (\'Color\', 0.6, 9)],\n        [(\'Solarize\', 0.2, 4), (\'Rotate\', 0.8, 9)],\n        [(\'Rotate\', 1.0, 7), (\'TranslateYRel\', 0.8, 9)],\n        [(\'ShearX\', 0.0, 0), (\'Solarize\', 0.8, 4)],\n        [(\'ShearY\', 0.8, 0), (\'Color\', 0.6, 4)],\n        [(\'Color\', 1.0, 0), (\'Rotate\', 0.6, 2)],\n        [(\'Equalize\', 0.8, 4), (\'Equalize\', 0.0, 8)],\n        [(\'Equalize\', 1.0, 4), (\'AutoContrast\', 0.6, 2)],\n        [(\'ShearY\', 0.4, 7), (\'SolarizeAdd\', 0.6, 7)],\n        [(\'PosterizeIncreasing\', 0.8, 2), (\'Solarize\', 0.6, 10)],\n        [(\'Solarize\', 0.6, 8), (\'Equalize\', 0.6, 1)],\n        [(\'Color\', 0.8, 6), (\'Rotate\', 0.4, 5)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_original(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501\n    policy = [\n        [(\'PosterizeOriginal\', 0.4, 8), (\'Rotate\', 0.6, 9)],\n        [(\'Solarize\', 0.6, 5), (\'AutoContrast\', 0.6, 5)],\n        [(\'Equalize\', 0.8, 8), (\'Equalize\', 0.6, 3)],\n        [(\'PosterizeOriginal\', 0.6, 7), (\'PosterizeOriginal\', 0.6, 6)],\n        [(\'Equalize\', 0.4, 7), (\'Solarize\', 0.2, 4)],\n        [(\'Equalize\', 0.4, 4), (\'Rotate\', 0.8, 8)],\n        [(\'Solarize\', 0.6, 3), (\'Equalize\', 0.6, 7)],\n        [(\'PosterizeOriginal\', 0.8, 5), (\'Equalize\', 1.0, 2)],\n        [(\'Rotate\', 0.2, 3), (\'Solarize\', 0.6, 8)],\n        [(\'Equalize\', 0.6, 8), (\'PosterizeOriginal\', 0.4, 6)],\n        [(\'Rotate\', 0.8, 8), (\'Color\', 0.4, 0)],\n        [(\'Rotate\', 0.4, 9), (\'Equalize\', 0.6, 2)],\n        [(\'Equalize\', 0.0, 7), (\'Equalize\', 0.8, 8)],\n        [(\'Invert\', 0.6, 4), (\'Equalize\', 1.0, 8)],\n        [(\'Color\', 0.6, 4), (\'Contrast\', 1.0, 8)],\n        [(\'Rotate\', 0.8, 8), (\'Color\', 1.0, 2)],\n        [(\'Color\', 0.8, 8), (\'Solarize\', 0.8, 7)],\n        [(\'Sharpness\', 0.4, 7), (\'Invert\', 0.6, 8)],\n        [(\'ShearX\', 0.6, 5), (\'Equalize\', 1.0, 9)],\n        [(\'Color\', 0.4, 0), (\'Equalize\', 0.6, 3)],\n        [(\'Equalize\', 0.4, 7), (\'Solarize\', 0.2, 4)],\n        [(\'Solarize\', 0.6, 5), (\'AutoContrast\', 0.6, 5)],\n        [(\'Invert\', 0.6, 4), (\'Equalize\', 1.0, 8)],\n        [(\'Color\', 0.6, 4), (\'Contrast\', 1.0, 8)],\n        [(\'Equalize\', 0.8, 8), (\'Equalize\', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy_originalr(hparams):\n    # ImageNet policy from https://arxiv.org/abs/1805.09501 with research posterize variation\n    policy = [\n        [(\'PosterizeIncreasing\', 0.4, 8), (\'Rotate\', 0.6, 9)],\n        [(\'Solarize\', 0.6, 5), (\'AutoContrast\', 0.6, 5)],\n        [(\'Equalize\', 0.8, 8), (\'Equalize\', 0.6, 3)],\n        [(\'PosterizeIncreasing\', 0.6, 7), (\'PosterizeIncreasing\', 0.6, 6)],\n        [(\'Equalize\', 0.4, 7), (\'Solarize\', 0.2, 4)],\n        [(\'Equalize\', 0.4, 4), (\'Rotate\', 0.8, 8)],\n        [(\'Solarize\', 0.6, 3), (\'Equalize\', 0.6, 7)],\n        [(\'PosterizeIncreasing\', 0.8, 5), (\'Equalize\', 1.0, 2)],\n        [(\'Rotate\', 0.2, 3), (\'Solarize\', 0.6, 8)],\n        [(\'Equalize\', 0.6, 8), (\'PosterizeIncreasing\', 0.4, 6)],\n        [(\'Rotate\', 0.8, 8), (\'Color\', 0.4, 0)],\n        [(\'Rotate\', 0.4, 9), (\'Equalize\', 0.6, 2)],\n        [(\'Equalize\', 0.0, 7), (\'Equalize\', 0.8, 8)],\n        [(\'Invert\', 0.6, 4), (\'Equalize\', 1.0, 8)],\n        [(\'Color\', 0.6, 4), (\'Contrast\', 1.0, 8)],\n        [(\'Rotate\', 0.8, 8), (\'Color\', 1.0, 2)],\n        [(\'Color\', 0.8, 8), (\'Solarize\', 0.8, 7)],\n        [(\'Sharpness\', 0.4, 7), (\'Invert\', 0.6, 8)],\n        [(\'ShearX\', 0.6, 5), (\'Equalize\', 1.0, 9)],\n        [(\'Color\', 0.4, 0), (\'Equalize\', 0.6, 3)],\n        [(\'Equalize\', 0.4, 7), (\'Solarize\', 0.2, 4)],\n        [(\'Solarize\', 0.6, 5), (\'AutoContrast\', 0.6, 5)],\n        [(\'Invert\', 0.6, 4), (\'Equalize\', 1.0, 8)],\n        [(\'Color\', 0.6, 4), (\'Contrast\', 1.0, 8)],\n        [(\'Equalize\', 0.8, 8), (\'Equalize\', 0.6, 3)],\n    ]\n    pc = [[AugmentOp(*a, hparams=hparams) for a in sp] for sp in policy]\n    return pc\n\n\ndef auto_augment_policy(name=\'v0\', hparams=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    if name == \'original\':\n        return auto_augment_policy_original(hparams)\n    elif name == \'originalr\':\n        return auto_augment_policy_originalr(hparams)\n    elif name == \'v0\':\n        return auto_augment_policy_v0(hparams)\n    elif name == \'v0r\':\n        return auto_augment_policy_v0r(hparams)\n    else:\n        assert False, \'Unknown AA policy (%s)\' % name\n\n\nclass AutoAugment:\n\n    def __init__(self, policy):\n        self.policy = policy\n\n    def __call__(self, img):\n        sub_policy = random.choice(self.policy)\n        for op in sub_policy:\n            img = op(img)\n        return img\n\n\ndef auto_augment_transform(config_str, hparams):\n    """"""\n    Create a AutoAugment transform\n\n    :param config_str: String defining configuration of auto augmentation. Consists of multiple sections separated by\n    dashes (\'-\'). The first section defines the AutoAugment policy (one of \'v0\', \'v0r\', \'original\', \'originalr\').\n    The remaining sections, not order sepecific determine\n        \'mstd\' -  float std deviation of magnitude noise applied\n    Ex \'original-mstd0.5\' results in AutoAugment with original policy, magnitude_std 0.5\n\n    :param hparams: Other hparams (kwargs) for the AutoAugmentation scheme\n\n    :return: A PyTorch compatible Transform\n    """"""\n    config = config_str.split(\'-\')\n    policy_name = config[0]\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\'(\\d.*)\', c)\n        if len(cs) < 2:\n            continue\n        key, val = cs[:2]\n        if key == \'mstd\':\n            # noise param injected via hparams for now\n            hparams.setdefault(\'magnitude_std\', float(val))\n        else:\n            assert False, \'Unknown AutoAugment config section\'\n    aa_policy = auto_augment_policy(policy_name, hparams=hparams)\n    return AutoAugment(aa_policy)\n\n\n_RAND_TRANSFORMS = [\n    \'AutoContrast\',\n    \'Equalize\',\n    \'Invert\',\n    \'Rotate\',\n    \'Posterize\',\n    \'Solarize\',\n    \'SolarizeAdd\',\n    \'Color\',\n    \'Contrast\',\n    \'Brightness\',\n    \'Sharpness\',\n    \'ShearX\',\n    \'ShearY\',\n    \'TranslateXRel\',\n    \'TranslateYRel\',\n    #\'Cutout\'  # NOTE I\'ve implement this as random erasing separately\n]\n\n\n_RAND_INCREASING_TRANSFORMS = [\n    \'AutoContrast\',\n    \'Equalize\',\n    \'Invert\',\n    \'Rotate\',\n    \'PosterizeIncreasing\',\n    \'SolarizeIncreasing\',\n    \'SolarizeAdd\',\n    \'ColorIncreasing\',\n    \'ContrastIncreasing\',\n    \'BrightnessIncreasing\',\n    \'SharpnessIncreasing\',\n    \'ShearX\',\n    \'ShearY\',\n    \'TranslateXRel\',\n    \'TranslateYRel\',\n    #\'Cutout\'  # NOTE I\'ve implement this as random erasing separately\n]\n\n\n\n# These experimental weights are based loosely on the relative improvements mentioned in paper.\n# They may not result in increased performance, but could likely be tuned to so.\n_RAND_CHOICE_WEIGHTS_0 = {\n    \'Rotate\': 0.3,\n    \'ShearX\': 0.2,\n    \'ShearY\': 0.2,\n    \'TranslateXRel\': 0.1,\n    \'TranslateYRel\': 0.1,\n    \'Color\': .025,\n    \'Sharpness\': 0.025,\n    \'AutoContrast\': 0.025,\n    \'Solarize\': .005,\n    \'SolarizeAdd\': .005,\n    \'Contrast\': .005,\n    \'Brightness\': .005,\n    \'Equalize\': .005,\n    \'Posterize\': 0,\n    \'Invert\': 0,\n}\n\n\ndef _select_rand_weights(weight_idx=0, transforms=None):\n    transforms = transforms or _RAND_TRANSFORMS\n    assert weight_idx == 0  # only one set of weights currently\n    rand_weights = _RAND_CHOICE_WEIGHTS_0\n    probs = [rand_weights[k] for k in transforms]\n    probs /= np.sum(probs)\n    return probs\n\n\ndef rand_augment_ops(magnitude=10, hparams=None, transforms=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    transforms = transforms or _RAND_TRANSFORMS\n    return [AugmentOp(\n        name, prob=0.5, magnitude=magnitude, hparams=hparams) for name in transforms]\n\n\nclass RandAugment:\n    def __init__(self, ops, num_layers=2, choice_weights=None):\n        self.ops = ops\n        self.num_layers = num_layers\n        self.choice_weights = choice_weights\n\n    def __call__(self, img):\n        # no replacement when using weighted choice\n        ops = np.random.choice(\n            self.ops, self.num_layers, replace=self.choice_weights is None, p=self.choice_weights)\n        for op in ops:\n            img = op(img)\n        return img\n\n\ndef rand_augment_transform(config_str, hparams):\n    """"""\n    Create a RandAugment transform\n\n    :param config_str: String defining configuration of random augmentation. Consists of multiple sections separated by\n    dashes (\'-\'). The first section defines the specific variant of rand augment (currently only \'rand\'). The remaining\n    sections, not order sepecific determine\n        \'m\' - integer magnitude of rand augment\n        \'n\' - integer num layers (number of transform ops selected per image)\n        \'w\' - integer probabiliy weight index (index of a set of weights to influence choice of op)\n        \'mstd\' -  float std deviation of magnitude noise applied\n        \'inc\' - integer (bool), use augmentations that increase in severity with magnitude (default: 0)\n    Ex \'rand-m9-n3-mstd0.5\' results in RandAugment with magnitude 9, num_layers 3, magnitude_std 0.5\n    \'rand-mstd1-w0\' results in magnitude_std 1.0, weights 0, default magnitude of 10 and num_layers 2\n\n    :param hparams: Other hparams (kwargs) for the RandAugmentation scheme\n\n    :return: A PyTorch compatible Transform\n    """"""\n    magnitude = _MAX_LEVEL  # default to _MAX_LEVEL for magnitude (currently 10)\n    num_layers = 2  # default to 2 ops per image\n    weight_idx = None  # default to no probability weights for op choice\n    transforms = _RAND_TRANSFORMS\n    config = config_str.split(\'-\')\n    assert config[0] == \'rand\'\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\'(\\d.*)\', c)\n        if len(cs) < 2:\n            continue\n        key, val = cs[:2]\n        if key == \'mstd\':\n            # noise param injected via hparams for now\n            hparams.setdefault(\'magnitude_std\', float(val))\n        elif key == \'inc\':\n            if bool(val):\n                transforms = _RAND_INCREASING_TRANSFORMS\n        elif key == \'m\':\n            magnitude = int(val)\n        elif key == \'n\':\n            num_layers = int(val)\n        elif key == \'w\':\n            weight_idx = int(val)\n        else:\n            assert False, \'Unknown RandAugment config section\'\n    ra_ops = rand_augment_ops(magnitude=magnitude, hparams=hparams, transforms=transforms)\n    choice_weights = None if weight_idx is None else _select_rand_weights(weight_idx)\n    return RandAugment(ra_ops, num_layers, choice_weights=choice_weights)\n\n\n_AUGMIX_TRANSFORMS = [\n    \'AutoContrast\',\n    \'ColorIncreasing\',  # not in paper\n    \'ContrastIncreasing\',  # not in paper\n    \'BrightnessIncreasing\',  # not in paper\n    \'SharpnessIncreasing\',  # not in paper\n    \'Equalize\',\n    \'Rotate\',\n    \'PosterizeIncreasing\',\n    \'SolarizeIncreasing\',\n    \'ShearX\',\n    \'ShearY\',\n    \'TranslateXRel\',\n    \'TranslateYRel\',\n]\n\n\ndef augmix_ops(magnitude=10, hparams=None, transforms=None):\n    hparams = hparams or _HPARAMS_DEFAULT\n    transforms = transforms or _AUGMIX_TRANSFORMS\n    return [AugmentOp(\n        name, prob=1.0, magnitude=magnitude, hparams=hparams) for name in transforms]\n\n\nclass AugMixAugment:\n    """""" AugMix Transform\n    Adapted and improved from impl here: https://github.com/google-research/augmix/blob/master/imagenet.py\n    From paper: \'AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty -\n    https://arxiv.org/abs/1912.02781\n    """"""\n    def __init__(self, ops, alpha=1., width=3, depth=-1, blended=False):\n        self.ops = ops\n        self.alpha = alpha\n        self.width = width\n        self.depth = depth\n        self.blended = blended  # blended mode is faster but not well tested\n\n    def _calc_blended_weights(self, ws, m):\n        ws = ws * m\n        cump = 1.\n        rws = []\n        for w in ws[::-1]:\n            alpha = w / cump\n            cump *= (1 - alpha)\n            rws.append(alpha)\n        return np.array(rws[::-1], dtype=np.float32)\n\n    def _apply_blended(self, img, mixing_weights, m):\n        # This is my first crack and implementing a slightly faster mixed augmentation. Instead\n        # of accumulating the mix for each chain in a Numpy array and then blending with original,\n        # it recomputes the blending coefficients and applies one PIL image blend per chain.\n        # TODO the results appear in the right ballpark but they differ by more than rounding.\n        img_orig = img.copy()\n        ws = self._calc_blended_weights(mixing_weights, m)\n        for w in ws:\n            depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n            ops = np.random.choice(self.ops, depth, replace=True)\n            img_aug = img_orig  # no ops are in-place, deep copy not necessary\n            for op in ops:\n                img_aug = op(img_aug)\n            img = Image.blend(img, img_aug, w)\n        return img\n\n    def _apply_basic(self, img, mixing_weights, m):\n        # This is a literal adaptation of the paper/official implementation without normalizations and\n        # PIL <-> Numpy conversions between every op. It is still quite CPU compute heavy compared to the\n        # typical augmentation transforms, could use a GPU / Kornia implementation.\n        img_shape = img.size[0], img.size[1], len(img.getbands())\n        mixed = np.zeros(img_shape, dtype=np.float32)\n        for mw in mixing_weights:\n            depth = self.depth if self.depth > 0 else np.random.randint(1, 4)\n            ops = np.random.choice(self.ops, depth, replace=True)\n            img_aug = img  # no ops are in-place, deep copy not necessary\n            for op in ops:\n                img_aug = op(img_aug)\n            mixed += mw * np.asarray(img_aug, dtype=np.float32)\n        np.clip(mixed, 0, 255., out=mixed)\n        mixed = Image.fromarray(mixed.astype(np.uint8))\n        return Image.blend(img, mixed, m)\n\n    def __call__(self, img):\n        mixing_weights = np.float32(np.random.dirichlet([self.alpha] * self.width))\n        m = np.float32(np.random.beta(self.alpha, self.alpha))\n        if self.blended:\n            mixed = self._apply_blended(img, mixing_weights, m)\n        else:\n            mixed = self._apply_basic(img, mixing_weights, m)\n        return mixed\n\n\ndef augment_and_mix_transform(config_str, hparams):\n    """""" Create AugMix PyTorch transform\n\n    :param config_str: String defining configuration of random augmentation. Consists of multiple sections separated by\n    dashes (\'-\'). The first section defines the specific variant of rand augment (currently only \'rand\'). The remaining\n    sections, not order sepecific determine\n        \'m\' - integer magnitude (severity) of augmentation mix (default: 3)\n        \'w\' - integer width of augmentation chain (default: 3)\n        \'d\' - integer depth of augmentation chain (-1 is random [1, 3], default: -1)\n        \'b\' - integer (bool), blend each branch of chain into end result without a final blend, less CPU (default: 0)\n        \'mstd\' -  float std deviation of magnitude noise applied (default: 0)\n    Ex \'augmix-m5-w4-d2\' results in AugMix with severity 5, chain width 4, chain depth 2\n\n    :param hparams: Other hparams (kwargs) for the Augmentation transforms\n\n    :return: A PyTorch compatible Transform\n    """"""\n    magnitude = 3\n    width = 3\n    depth = -1\n    alpha = 1.\n    blended = False\n    config = config_str.split(\'-\')\n    assert config[0] == \'augmix\'\n    config = config[1:]\n    for c in config:\n        cs = re.split(r\'(\\d.*)\', c)\n        if len(cs) < 2:\n            continue\n        key, val = cs[:2]\n        if key == \'mstd\':\n            # noise param injected via hparams for now\n            hparams.setdefault(\'magnitude_std\', float(val))\n        elif key == \'m\':\n            magnitude = int(val)\n        elif key == \'w\':\n            width = int(val)\n        elif key == \'d\':\n            depth = int(val)\n        elif key == \'a\':\n            alpha = float(val)\n        elif key == \'b\':\n            blended = bool(val)\n        else:\n            assert False, \'Unknown AugMix config section\'\n    ops = augmix_ops(magnitude=magnitude, hparams=hparams)\n    return AugMixAugment(ops, alpha=alpha, width=width, depth=depth, blended=blended)\n'"
timm/data/config.py,0,"b""import logging\nfrom .constants import *\n\n\ndef resolve_data_config(args, default_cfg={}, model=None, verbose=True):\n    new_config = {}\n    default_cfg = default_cfg\n    if not default_cfg and model is not None and hasattr(model, 'default_cfg'):\n        default_cfg = model.default_cfg\n\n    # Resolve input/image size\n    in_chans = 3\n    if 'chans' in args and args['chans'] is not None:\n        in_chans = args['chans']\n\n    input_size = (in_chans, 224, 224)\n    if 'input_size' in args and args['input_size'] is not None:\n        assert isinstance(args['input_size'], (tuple, list))\n        assert len(args['input_size']) == 3\n        input_size = tuple(args['input_size'])\n        in_chans = input_size[0]  # input_size overrides in_chans\n    elif 'img_size' in args and args['img_size'] is not None:\n        assert isinstance(args['img_size'], int)\n        input_size = (in_chans, args['img_size'], args['img_size'])\n    elif 'input_size' in default_cfg:\n        input_size = default_cfg['input_size']\n    new_config['input_size'] = input_size\n\n    # resolve interpolation method\n    new_config['interpolation'] = 'bicubic'\n    if 'interpolation' in args and args['interpolation']:\n        new_config['interpolation'] = args['interpolation']\n    elif 'interpolation' in default_cfg:\n        new_config['interpolation'] = default_cfg['interpolation']\n\n    # resolve dataset + model mean for normalization\n    new_config['mean'] = IMAGENET_DEFAULT_MEAN\n    if 'mean' in args and args['mean'] is not None:\n        mean = tuple(args['mean'])\n        if len(mean) == 1:\n            mean = tuple(list(mean) * in_chans)\n        else:\n            assert len(mean) == in_chans\n        new_config['mean'] = mean\n    elif 'mean' in default_cfg:\n        new_config['mean'] = default_cfg['mean']\n\n    # resolve dataset + model std deviation for normalization\n    new_config['std'] = IMAGENET_DEFAULT_STD\n    if 'std' in args and args['std'] is not None:\n        std = tuple(args['std'])\n        if len(std) == 1:\n            std = tuple(list(std) * in_chans)\n        else:\n            assert len(std) == in_chans\n        new_config['std'] = std\n    elif 'std' in default_cfg:\n        new_config['std'] = default_cfg['std']\n\n    # resolve default crop percentage\n    new_config['crop_pct'] = DEFAULT_CROP_PCT\n    if 'crop_pct' in args and args['crop_pct'] is not None:\n        new_config['crop_pct'] = args['crop_pct']\n    elif 'crop_pct' in default_cfg:\n        new_config['crop_pct'] = default_cfg['crop_pct']\n\n    if verbose:\n        logging.info('Data processing configuration for current model + dataset:')\n        for n, v in new_config.items():\n            logging.info('\\t%s: %s' % (n, str(v)))\n\n    return new_config\n"""
timm/data/constants.py,0,"b'DEFAULT_CROP_PCT = 0.875\nIMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\nIMAGENET_INCEPTION_MEAN = (0.5, 0.5, 0.5)\nIMAGENET_INCEPTION_STD = (0.5, 0.5, 0.5)\nIMAGENET_DPN_MEAN = (124 / 255, 117 / 255, 104 / 255)\nIMAGENET_DPN_STD = tuple([1 / (.0167 * 255)] * 3)\n'"
timm/data/dataset.py,4,"b'from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch.utils.data as data\n\nimport os\nimport re\nimport torch\nimport tarfile\nfrom PIL import Image\n\n\nIMG_EXTENSIONS = [\'.png\', \'.jpg\', \'.jpeg\']\n\n\ndef natural_key(string_):\n    """"""See http://www.codinghorror.com/blog/archives/001018.html""""""\n    return [int(s) if s.isdigit() else s for s in re.split(r\'(\\d+)\', string_.lower())]\n\n\ndef find_images_and_targets(folder, types=IMG_EXTENSIONS, class_to_idx=None, leaf_name_only=True, sort=True):\n    labels = []\n    filenames = []\n    for root, subdirs, files in os.walk(folder, topdown=False):\n        rel_path = os.path.relpath(root, folder) if (root != folder) else \'\'\n        label = os.path.basename(rel_path) if leaf_name_only else rel_path.replace(os.path.sep, \'_\')\n        for f in files:\n            base, ext = os.path.splitext(f)\n            if ext.lower() in types:\n                filenames.append(os.path.join(root, f))\n                labels.append(label)\n    if class_to_idx is None:\n        # building class index\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels, key=natural_key))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    images_and_targets = zip(filenames, [class_to_idx[l] for l in labels])\n    if sort:\n        images_and_targets = sorted(images_and_targets, key=lambda k: natural_key(k[0]))\n    return images_and_targets, class_to_idx\n\n\ndef load_class_map(filename, root=\'\'):\n    class_to_idx = {}\n    class_map_path = filename\n    if not os.path.exists(class_map_path):\n        class_map_path = os.path.join(root, filename)\n        assert os.path.exists(class_map_path), \'Cannot locate specified class map file (%s)\' % filename\n    class_map_ext = os.path.splitext(filename)[-1].lower()\n    if class_map_ext == \'.txt\':\n        with open(class_map_path) as f:\n            class_to_idx = {v.strip(): k for k, v in enumerate(f)}\n    else:\n        assert False, \'Unsupported class map extension\'\n    return class_to_idx\n\n\nclass Dataset(data.Dataset):\n\n    def __init__(\n            self,\n            root,\n            load_bytes=False,\n            transform=None,\n            class_map=\'\'):\n\n        class_to_idx = None\n        if class_map:\n            class_to_idx = load_class_map(class_map, root)\n        images, class_to_idx = find_images_and_targets(root, class_to_idx=class_to_idx)\n        if len(images) == 0:\n            raise(RuntimeError(""Found 0 images in subfolders of: "" + root + ""\\n""\n                               ""Supported image extensions are: "" + "","".join(IMG_EXTENSIONS)))\n        self.root = root\n        self.samples = images\n        self.imgs = self.samples  # torchvision ImageFolder compat\n        self.class_to_idx = class_to_idx\n        self.load_bytes = load_bytes\n        self.transform = transform\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        img = open(path, \'rb\').read() if self.load_bytes else Image.open(path).convert(\'RGB\')\n        if self.transform is not None:\n            img = self.transform(img)\n        if target is None:\n            target = torch.zeros(1).long()\n        return img, target\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def filenames(self, indices=[], basename=False):\n        if indices:\n            if basename:\n                return [os.path.basename(self.samples[i][0]) for i in indices]\n            else:\n                return [self.samples[i][0] for i in indices]\n        else:\n            if basename:\n                return [os.path.basename(x[0]) for x in self.samples]\n            else:\n                return [x[0] for x in self.samples]\n\n\ndef _extract_tar_info(tarfile, class_to_idx=None, sort=True):\n    files = []\n    labels = []\n    for ti in tarfile.getmembers():\n        if not ti.isfile():\n            continue\n        dirname, basename = os.path.split(ti.path)\n        label = os.path.basename(dirname)\n        ext = os.path.splitext(basename)[1]\n        if ext.lower() in IMG_EXTENSIONS:\n            files.append(ti)\n            labels.append(label)\n    if class_to_idx is None:\n        unique_labels = set(labels)\n        sorted_labels = list(sorted(unique_labels, key=natural_key))\n        class_to_idx = {c: idx for idx, c in enumerate(sorted_labels)}\n    tarinfo_and_targets = zip(files, [class_to_idx[l] for l in labels])\n    if sort:\n        tarinfo_and_targets = sorted(tarinfo_and_targets, key=lambda k: natural_key(k[0].path))\n    return tarinfo_and_targets, class_to_idx\n\n\nclass DatasetTar(data.Dataset):\n\n    def __init__(self, root, load_bytes=False, transform=None, class_map=\'\'):\n\n        class_to_idx = None\n        if class_map:\n            class_to_idx = load_class_map(class_map, root)\n        assert os.path.isfile(root)\n        self.root = root\n        with tarfile.open(root) as tf:  # cannot keep this open across processes, reopen later\n            self.samples, self.class_to_idx = _extract_tar_info(tf, class_to_idx)\n        self.tarfile = None  # lazy init in __getitem__\n        self.load_bytes = load_bytes\n        self.transform = transform\n\n    def __getitem__(self, index):\n        if self.tarfile is None:\n            self.tarfile = tarfile.open(self.root)\n        tarinfo, target = self.samples[index]\n        iob = self.tarfile.extractfile(tarinfo)\n        img = iob.read() if self.load_bytes else Image.open(iob).convert(\'RGB\')\n        if self.transform is not None:\n            img = self.transform(img)\n        if target is None:\n            target = torch.zeros(1).long()\n        return img, target\n\n    def __len__(self):\n        return len(self.samples)\n\n\nclass AugMixDataset(torch.utils.data.Dataset):\n    """"""Dataset wrapper to perform AugMix or other clean/augmentation mixes""""""\n\n    def __init__(self, dataset, num_splits=2):\n        self.augmentation = None\n        self.normalize = None\n        self.dataset = dataset\n        if self.dataset.transform is not None:\n            self._set_transforms(self.dataset.transform)\n        self.num_splits = num_splits\n\n    def _set_transforms(self, x):\n        assert isinstance(x, (list, tuple)) and len(x) == 3, \'Expecting a tuple/list of 3 transforms\'\n        self.dataset.transform = x[0]\n        self.augmentation = x[1]\n        self.normalize = x[2]\n\n    @property\n    def transform(self):\n        return self.dataset.transform\n\n    @transform.setter\n    def transform(self, x):\n        self._set_transforms(x)\n\n    def _normalize(self, x):\n        return x if self.normalize is None else self.normalize(x)\n\n    def __getitem__(self, i):\n        x, y = self.dataset[i]  # all splits share the same dataset base transform\n        x_list = [self._normalize(x)]  # first split only normalizes (this is the \'clean\' split)\n        # run the full augmentation on the remaining splits\n        for _ in range(self.num_splits - 1):\n            x_list.append(self._normalize(self.augmentation(x)))\n        return tuple(x_list), y\n\n    def __len__(self):\n        return len(self.dataset)\n'"
timm/data/distributed_sampler.py,3,"b'import math\nimport torch\nfrom torch.utils.data import Sampler\nimport torch.distributed as dist\n\n\nclass OrderedDistributedSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None):\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        indices = list(range(len(self.dataset)))\n\n        # add extra samples to make it evenly divisible\n        indices += indices[:(self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n'"
timm/data/loader.py,20,"b'import torch.utils.data\nimport numpy as np\n\nfrom .transforms_factory import create_transform\nfrom .constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom .distributed_sampler import OrderedDistributedSampler\nfrom .random_erasing import RandomErasing\nfrom .mixup import FastCollateMixup\n\n\ndef fast_collate(batch):\n    """""" A fast collation function optimized for uint8 images (np array or torch) and int64 targets (labels)""""""\n    assert isinstance(batch[0], tuple)\n    batch_size = len(batch)\n    if isinstance(batch[0][0], tuple):\n        # This branch \'deinterleaves\' and flattens tuples of input tensors into one tensor ordered by position\n        # such that all tuple of position n will end up in a torch.split(tensor, batch_size) in nth position\n        inner_tuple_size = len(batch[0][0])\n        flattened_batch_size = batch_size * inner_tuple_size\n        targets = torch.zeros(flattened_batch_size, dtype=torch.int64)\n        tensor = torch.zeros((flattened_batch_size, *batch[0][0][0].shape), dtype=torch.uint8)\n        for i in range(batch_size):\n            assert len(batch[i][0]) == inner_tuple_size  # all input tensor tuples must be same length\n            for j in range(inner_tuple_size):\n                targets[i + j * batch_size] = batch[i][1]\n                tensor[i + j * batch_size] += torch.from_numpy(batch[i][0][j])\n        return tensor, targets\n    elif isinstance(batch[0][0], np.ndarray):\n        targets = torch.tensor([b[1] for b in batch], dtype=torch.int64)\n        assert len(targets) == batch_size\n        tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n        for i in range(batch_size):\n            tensor[i] += torch.from_numpy(batch[i][0])\n        return tensor, targets\n    elif isinstance(batch[0][0], torch.Tensor):\n        targets = torch.tensor([b[1] for b in batch], dtype=torch.int64)\n        assert len(targets) == batch_size\n        tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n        for i in range(batch_size):\n            tensor[i].copy_(batch[i][0])\n        return tensor, targets\n    else:\n        assert False\n\n\nclass PrefetchLoader:\n\n    def __init__(self,\n                 loader,\n                 mean=IMAGENET_DEFAULT_MEAN,\n                 std=IMAGENET_DEFAULT_STD,\n                 fp16=False,\n                 re_prob=0.,\n                 re_mode=\'const\',\n                 re_count=1,\n                 re_num_splits=0):\n        self.loader = loader\n        self.mean = torch.tensor([x * 255 for x in mean]).cuda().view(1, 3, 1, 1)\n        self.std = torch.tensor([x * 255 for x in std]).cuda().view(1, 3, 1, 1)\n        self.fp16 = fp16\n        if fp16:\n            self.mean = self.mean.half()\n            self.std = self.std.half()\n        if re_prob > 0.:\n            self.random_erasing = RandomErasing(\n                probability=re_prob, mode=re_mode, max_count=re_count, num_splits=re_num_splits)\n        else:\n            self.random_erasing = None\n\n    def __iter__(self):\n        stream = torch.cuda.Stream()\n        first = True\n\n        for next_input, next_target in self.loader:\n            with torch.cuda.stream(stream):\n                next_input = next_input.cuda(non_blocking=True)\n                next_target = next_target.cuda(non_blocking=True)\n                if self.fp16:\n                    next_input = next_input.half().sub_(self.mean).div_(self.std)\n                else:\n                    next_input = next_input.float().sub_(self.mean).div_(self.std)\n                if self.random_erasing is not None:\n                    next_input = self.random_erasing(next_input)\n\n            if not first:\n                yield input, target\n            else:\n                first = False\n\n            torch.cuda.current_stream().wait_stream(stream)\n            input = next_input\n            target = next_target\n\n        yield input, target\n\n    def __len__(self):\n        return len(self.loader)\n\n    @property\n    def sampler(self):\n        return self.loader.sampler\n\n    @property\n    def dataset(self):\n        return self.loader.dataset\n\n    @property\n    def mixup_enabled(self):\n        if isinstance(self.loader.collate_fn, FastCollateMixup):\n            return self.loader.collate_fn.mixup_enabled\n        else:\n            return False\n\n    @mixup_enabled.setter\n    def mixup_enabled(self, x):\n        if isinstance(self.loader.collate_fn, FastCollateMixup):\n            self.loader.collate_fn.mixup_enabled = x\n\n\ndef create_loader(\n        dataset,\n        input_size,\n        batch_size,\n        is_training=False,\n        use_prefetcher=True,\n        re_prob=0.,\n        re_mode=\'const\',\n        re_count=1,\n        re_split=False,\n        color_jitter=0.4,\n        auto_augment=None,\n        num_aug_splits=0,\n        interpolation=\'bilinear\',\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD,\n        num_workers=1,\n        distributed=False,\n        crop_pct=None,\n        collate_fn=None,\n        pin_memory=False,\n        fp16=False,\n        tf_preprocessing=False,\n        use_multi_epochs_loader=False\n):\n    re_num_splits = 0\n    if re_split:\n        # apply RE to second half of batch if no aug split otherwise line up with aug split\n        re_num_splits = num_aug_splits or 2\n    dataset.transform = create_transform(\n        input_size,\n        is_training=is_training,\n        use_prefetcher=use_prefetcher,\n        color_jitter=color_jitter,\n        auto_augment=auto_augment,\n        interpolation=interpolation,\n        mean=mean,\n        std=std,\n        crop_pct=crop_pct,\n        tf_preprocessing=tf_preprocessing,\n        re_prob=re_prob,\n        re_mode=re_mode,\n        re_count=re_count,\n        re_num_splits=re_num_splits,\n        separate=num_aug_splits > 0,\n    )\n\n    sampler = None\n    if distributed:\n        if is_training:\n            sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n        else:\n            # This will add extra duplicate entries to result in equal num\n            # of samples per-process, will slightly alter validation results\n            sampler = OrderedDistributedSampler(dataset)\n\n    if collate_fn is None:\n        collate_fn = fast_collate if use_prefetcher else torch.utils.data.dataloader.default_collate\n\n    loader_class = torch.utils.data.DataLoader\n\n    if use_multi_epochs_loader:\n        loader_class = MultiEpochsDataLoader\n\n    loader = loader_class(\n        dataset,\n        batch_size=batch_size,\n        shuffle=sampler is None and is_training,\n        num_workers=num_workers,\n        sampler=sampler,\n        collate_fn=collate_fn,\n        pin_memory=pin_memory,\n        drop_last=is_training,\n    )\n    if use_prefetcher:\n        loader = PrefetchLoader(\n            loader,\n            mean=mean,\n            std=std,\n            fp16=fp16,\n            re_prob=re_prob if is_training else 0.,\n            re_mode=re_mode,\n            re_count=re_count,\n            re_num_splits=re_num_splits\n        )\n\n    return loader\n\n\nclass MultiEpochsDataLoader(torch.utils.data.DataLoader):\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._DataLoader__initialized = False\n        self.batch_sampler = _RepeatSampler(self.batch_sampler)\n        self._DataLoader__initialized = True\n        self.iterator = super().__iter__()\n\n    def __len__(self):\n        return len(self.batch_sampler.sampler)\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield next(self.iterator)\n\n\nclass _RepeatSampler(object):\n    """""" Sampler that repeats forever.\n\n    Args:\n        sampler (Sampler)\n    """"""\n\n    def __init__(self, sampler):\n        self.sampler = sampler\n\n    def __iter__(self):\n        while True:\n            yield from iter(self.sampler)\n'"
timm/data/mixup.py,4,"b""import numpy as np\nimport torch\n\n\ndef one_hot(x, num_classes, on_value=1., off_value=0., device='cuda'):\n    x = x.long().view(-1, 1)\n    return torch.full((x.size()[0], num_classes), off_value, device=device).scatter_(1, x, on_value)\n\n\ndef mixup_target(target, num_classes, lam=1., smoothing=0.0, device='cuda'):\n    off_value = smoothing / num_classes\n    on_value = 1. - smoothing + off_value\n    y1 = one_hot(target, num_classes, on_value=on_value, off_value=off_value, device=device)\n    y2 = one_hot(target.flip(0), num_classes, on_value=on_value, off_value=off_value, device=device)\n    return lam*y1 + (1. - lam)*y2\n\n\ndef mixup_batch(input, target, alpha=0.2, num_classes=1000, smoothing=0.1, disable=False):\n    lam = 1.\n    if not disable:\n        lam = np.random.beta(alpha, alpha)\n    input = input.mul(lam).add_(1 - lam, input.flip(0))\n    target = mixup_target(target, num_classes, lam, smoothing)\n    return input, target\n\n\nclass FastCollateMixup:\n\n    def __init__(self, mixup_alpha=1., label_smoothing=0.1, num_classes=1000):\n        self.mixup_alpha = mixup_alpha\n        self.label_smoothing = label_smoothing\n        self.num_classes = num_classes\n        self.mixup_enabled = True\n\n    def __call__(self, batch):\n        batch_size = len(batch)\n        lam = 1.\n        if self.mixup_enabled:\n            lam = np.random.beta(self.mixup_alpha, self.mixup_alpha)\n\n        target = torch.tensor([b[1] for b in batch], dtype=torch.int64)\n        target = mixup_target(target, self.num_classes, lam, self.label_smoothing, device='cpu')\n\n        tensor = torch.zeros((batch_size, *batch[0][0].shape), dtype=torch.uint8)\n        for i in range(batch_size):\n            mixed = batch[i][0].astype(np.float32) * lam + \\\n                    batch[batch_size - i - 1][0].astype(np.float32) * (1 - lam)\n            np.round(mixed, out=mixed)\n            tensor[i] += torch.from_numpy(mixed.astype(np.uint8))\n\n        return tensor, target\n"""
timm/data/random_erasing.py,4,"b'import random\nimport math\nimport torch\n\n\ndef _get_pixels(per_pixel, rand_color, patch_size, dtype=torch.float32, device=\'cuda\'):\n    # NOTE I\'ve seen CUDA illegal memory access errors being caused by the normal_()\n    # paths, flip the order so normal is run on CPU if this becomes a problem\n    # Issue has been fixed in master https://github.com/pytorch/pytorch/issues/19508\n    if per_pixel:\n        return torch.empty(patch_size, dtype=dtype, device=device).normal_()\n    elif rand_color:\n        return torch.empty((patch_size[0], 1, 1), dtype=dtype, device=device).normal_()\n    else:\n        return torch.zeros((patch_size[0], 1, 1), dtype=dtype, device=device)\n\n\nclass RandomErasing:\n    """""" Randomly selects a rectangle region in an image and erases its pixels.\n        \'Random Erasing Data Augmentation\' by Zhong et al.\n        See https://arxiv.org/pdf/1708.04896.pdf\n\n        This variant of RandomErasing is intended to be applied to either a batch\n        or single image tensor after it has been normalized by dataset mean and std.\n    Args:\n         probability: Probability that the Random Erasing operation will be performed.\n         min_area: Minimum percentage of erased area wrt input image area.\n         max_area: Maximum percentage of erased area wrt input image area.\n         min_aspect: Minimum aspect ratio of erased area.\n         mode: pixel color mode, one of \'const\', \'rand\', or \'pixel\'\n            \'const\' - erase block is constant color of 0 for all channels\n            \'rand\'  - erase block is same per-channel random (normal) color\n            \'pixel\' - erase block is per-pixel random (normal) color\n        max_count: maximum number of erasing blocks per image, area per box is scaled by count.\n            per-image count is randomly chosen between 1 and this value.\n    """"""\n\n    def __init__(\n            self,\n            probability=0.5, min_area=0.02, max_area=1/3, min_aspect=0.3, max_aspect=None,\n            mode=\'const\', min_count=1, max_count=None, num_splits=0, device=\'cuda\'):\n        self.probability = probability\n        self.min_area = min_area\n        self.max_area = max_area\n        max_aspect = max_aspect or 1 / min_aspect\n        self.log_aspect_ratio = (math.log(min_aspect), math.log(max_aspect))\n        self.min_count = min_count\n        self.max_count = max_count or min_count\n        self.num_splits = num_splits\n        mode = mode.lower()\n        self.rand_color = False\n        self.per_pixel = False\n        if mode == \'rand\':\n            self.rand_color = True  # per block random normal\n        elif mode == \'pixel\':\n            self.per_pixel = True  # per pixel random normal\n        else:\n            assert not mode or mode == \'const\'\n        self.device = device\n\n    def _erase(self, img, chan, img_h, img_w, dtype):\n        if random.random() > self.probability:\n            return\n        area = img_h * img_w\n        count = self.min_count if self.min_count == self.max_count else \\\n            random.randint(self.min_count, self.max_count)\n        for _ in range(count):\n            for attempt in range(10):\n                target_area = random.uniform(self.min_area, self.max_area) * area / count\n                aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n                h = int(round(math.sqrt(target_area * aspect_ratio)))\n                w = int(round(math.sqrt(target_area / aspect_ratio)))\n                if w < img_w and h < img_h:\n                    top = random.randint(0, img_h - h)\n                    left = random.randint(0, img_w - w)\n                    img[:, top:top + h, left:left + w] = _get_pixels(\n                        self.per_pixel, self.rand_color, (chan, h, w),\n                        dtype=dtype, device=self.device)\n                    break\n\n    def __call__(self, input):\n        if len(input.size()) == 3:\n            self._erase(input, *input.size(), input.dtype)\n        else:\n            batch_size, chan, img_h, img_w = input.size()\n            # skip first slice of batch if num_splits is set (for clean portion of samples)\n            batch_start = batch_size // self.num_splits if self.num_splits > 1 else 0\n            for i in range(batch_start, batch_size):\n                self._erase(input[i], chan, img_h, img_w, input.dtype)\n        return input\n'"
timm/data/tf_preprocessing.py,0,"b'# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n""""""ImageNet preprocessing for MnasNet.""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\n\nIMAGE_SIZE = 224\nCROP_PADDING = 32\n\n\ndef distorted_bounding_box_crop(image_bytes,\n                                bbox,\n                                min_object_covered=0.1,\n                                aspect_ratio_range=(0.75, 1.33),\n                                area_range=(0.05, 1.0),\n                                max_attempts=100,\n                                scope=None):\n    """"""Generates cropped_image using one of the bboxes randomly distorted.\n\n    See `tf.image.sample_distorted_bounding_box` for more documentation.\n\n    Args:\n      image_bytes: `Tensor` of binary image data.\n      bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n          where each coordinate is [0, 1) and the coordinates are arranged\n          as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n          image.\n      min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n          area of the image must contain at least this fraction of any bounding\n          box supplied.\n      aspect_ratio_range: An optional list of `float`s. The cropped area of the\n          image must have an aspect ratio = width / height within this range.\n      area_range: An optional list of `float`s. The cropped area of the image\n          must contain a fraction of the supplied image within in this range.\n      max_attempts: An optional `int`. Number of attempts at generating a cropped\n          region of the image of the specified constraints. After `max_attempts`\n          failures, return the entire image.\n      scope: Optional `str` for name scope.\n    Returns:\n      cropped image `Tensor`\n    """"""\n    with tf.name_scope(scope, \'distorted_bounding_box_crop\', [image_bytes, bbox]):\n        shape = tf.image.extract_jpeg_shape(image_bytes)\n        sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n            shape,\n            bounding_boxes=bbox,\n            min_object_covered=min_object_covered,\n            aspect_ratio_range=aspect_ratio_range,\n            area_range=area_range,\n            max_attempts=max_attempts,\n            use_image_if_no_bounding_boxes=True)\n        bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n\n        # Crop the image to the specified bounding box.\n        offset_y, offset_x, _ = tf.unstack(bbox_begin)\n        target_height, target_width, _ = tf.unstack(bbox_size)\n        crop_window = tf.stack([offset_y, offset_x, target_height, target_width])\n        image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n\n        return image\n\n\ndef _at_least_x_are_equal(a, b, x):\n    """"""At least `x` of `a` and `b` `Tensors` are equal.""""""\n    match = tf.equal(a, b)\n    match = tf.cast(match, tf.int32)\n    return tf.greater_equal(tf.reduce_sum(match), x)\n\n\ndef _decode_and_random_crop(image_bytes, image_size, resize_method):\n    """"""Make a random crop of image_size.""""""\n    bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n    image = distorted_bounding_box_crop(\n        image_bytes,\n        bbox,\n        min_object_covered=0.1,\n        aspect_ratio_range=(3. / 4, 4. / 3.),\n        area_range=(0.08, 1.0),\n        max_attempts=10,\n        scope=None)\n    original_shape = tf.image.extract_jpeg_shape(image_bytes)\n    bad = _at_least_x_are_equal(original_shape, tf.shape(image), 3)\n\n    image = tf.cond(\n        bad,\n        lambda: _decode_and_center_crop(image_bytes, image_size),\n        lambda: tf.image.resize([image], [image_size, image_size], resize_method)[0])\n\n    return image\n\n\ndef _decode_and_center_crop(image_bytes, image_size, resize_method):\n    """"""Crops to center of image with padding then scales image_size.""""""\n    shape = tf.image.extract_jpeg_shape(image_bytes)\n    image_height = shape[0]\n    image_width = shape[1]\n\n    padded_center_crop_size = tf.cast(\n        ((image_size / (image_size + CROP_PADDING)) *\n         tf.cast(tf.minimum(image_height, image_width), tf.float32)),\n        tf.int32)\n\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = tf.stack([offset_height, offset_width,\n                            padded_center_crop_size, padded_center_crop_size])\n    image = tf.image.decode_and_crop_jpeg(image_bytes, crop_window, channels=3)\n    image = tf.image.resize([image], [image_size, image_size], resize_method)[0]\n\n    return image\n\n\ndef _flip(image):\n    """"""Random horizontal image flip.""""""\n    image = tf.image.random_flip_left_right(image)\n    return image\n\n\ndef preprocess_for_train(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation=\'bicubic\'):\n    """"""Preprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor`.\n    """"""\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == \'bicubic\' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_random_crop(image_bytes, image_size, resize_method)\n    image = _flip(image)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n\ndef preprocess_for_eval(image_bytes, use_bfloat16, image_size=IMAGE_SIZE, interpolation=\'bicubic\'):\n    """"""Preprocesses the given image for evaluation.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor`.\n    """"""\n    resize_method = tf.image.ResizeMethod.BICUBIC if interpolation == \'bicubic\' else tf.image.ResizeMethod.BILINEAR\n    image = _decode_and_center_crop(image_bytes, image_size, resize_method)\n    image = tf.reshape(image, [image_size, image_size, 3])\n    image = tf.image.convert_image_dtype(\n        image, dtype=tf.bfloat16 if use_bfloat16 else tf.float32)\n    return image\n\n\ndef preprocess_image(image_bytes,\n                     is_training=False,\n                     use_bfloat16=False,\n                     image_size=IMAGE_SIZE,\n                     interpolation=\'bicubic\'):\n    """"""Preprocesses the given image.\n\n    Args:\n      image_bytes: `Tensor` representing an image binary of arbitrary size.\n      is_training: `bool` for whether the preprocessing is for training.\n      use_bfloat16: `bool` for whether to use bfloat16.\n      image_size: image size.\n      interpolation: image interpolation method\n\n    Returns:\n      A preprocessed image `Tensor` with value range of [0, 255].\n    """"""\n    if is_training:\n        return preprocess_for_train(image_bytes, use_bfloat16, image_size, interpolation)\n    else:\n        return preprocess_for_eval(image_bytes, use_bfloat16, image_size, interpolation)\n\n\nclass TfPreprocessTransform:\n\n    def __init__(self, is_training=False, size=224, interpolation=\'bicubic\'):\n        self.is_training = is_training\n        self.size = size[0] if isinstance(size, tuple) else size\n        self.interpolation = interpolation\n        self._image_bytes = None\n        self.process_image = self._build_tf_graph()\n        self.sess = None\n\n    def _build_tf_graph(self):\n        with tf.device(\'/cpu:0\'):\n            self._image_bytes = tf.placeholder(\n                shape=[],\n                dtype=tf.string,\n            )\n            img = preprocess_image(\n                self._image_bytes, self.is_training, False, self.size, self.interpolation)\n        return img\n\n    def __call__(self, image_bytes):\n        if self.sess is None:\n            self.sess = tf.Session()\n        img = self.sess.run(self.process_image, feed_dict={self._image_bytes: image_bytes})\n        img = img.round().clip(0, 255).astype(np.uint8)\n        if img.ndim < 3:\n            img = np.expand_dims(img, axis=-1)\n        img = np.rollaxis(img, 2)  # HWC to CHW\n        return img\n'"
timm/data/transforms.py,2,"b'import torch\nimport torchvision.transforms.functional as F\nfrom PIL import Image\nimport warnings\nimport math\nimport random\nimport numpy as np\n\n\nclass ToNumpy:\n\n    def __call__(self, pil_img):\n        np_img = np.array(pil_img, dtype=np.uint8)\n        if np_img.ndim < 3:\n            np_img = np.expand_dims(np_img, axis=-1)\n        np_img = np.rollaxis(np_img, 2)  # HWC to CHW\n        return np_img\n\n\nclass ToTensor:\n\n    def __init__(self, dtype=torch.float32):\n        self.dtype = dtype\n\n    def __call__(self, pil_img):\n        np_img = np.array(pil_img, dtype=np.uint8)\n        if np_img.ndim < 3:\n            np_img = np.expand_dims(np_img, axis=-1)\n        np_img = np.rollaxis(np_img, 2)  # HWC to CHW\n        return torch.from_numpy(np_img).to(dtype=self.dtype)\n\n\n_pil_interpolation_to_str = {\n    Image.NEAREST: \'PIL.Image.NEAREST\',\n    Image.BILINEAR: \'PIL.Image.BILINEAR\',\n    Image.BICUBIC: \'PIL.Image.BICUBIC\',\n    Image.LANCZOS: \'PIL.Image.LANCZOS\',\n    Image.HAMMING: \'PIL.Image.HAMMING\',\n    Image.BOX: \'PIL.Image.BOX\',\n}\n\n\ndef _pil_interp(method):\n    if method == \'bicubic\':\n        return Image.BICUBIC\n    elif method == \'lanczos\':\n        return Image.LANCZOS\n    elif method == \'hamming\':\n        return Image.HAMMING\n    else:\n        # default bilinear, do we want to allow nearest?\n        return Image.BILINEAR\n\n\n_RANDOM_INTERPOLATION = (Image.BILINEAR, Image.BICUBIC)\n\n\nclass RandomResizedCropAndInterpolation:\n    """"""Crop the given PIL Image to random size and aspect ratio with random interpolation.\n\n    A crop of random size (default: of 0.08 to 1.0) of the original size and a random\n    aspect ratio (default: of 3/4 to 4/3) of the original aspect ratio is made. This crop\n    is finally resized to given size.\n    This is popularly used to train the Inception networks.\n\n    Args:\n        size: expected output size of each edge\n        scale: range of size of the origin size cropped\n        ratio: range of aspect ratio of the origin aspect ratio cropped\n        interpolation: Default: PIL.Image.BILINEAR\n    """"""\n\n    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.),\n                 interpolation=\'bilinear\'):\n        if isinstance(size, tuple):\n            self.size = size\n        else:\n            self.size = (size, size)\n        if (scale[0] > scale[1]) or (ratio[0] > ratio[1]):\n            warnings.warn(""range should be of kind (min, max)"")\n\n        if interpolation == \'random\':\n            self.interpolation = _RANDOM_INTERPOLATION\n        else:\n            self.interpolation = _pil_interp(interpolation)\n        self.scale = scale\n        self.ratio = ratio\n\n    @staticmethod\n    def get_params(img, scale, ratio):\n        """"""Get parameters for ``crop`` for a random sized crop.\n\n        Args:\n            img (PIL Image): Image to be cropped.\n            scale (tuple): range of size of the origin size cropped\n            ratio (tuple): range of aspect ratio of the origin aspect ratio cropped\n\n        Returns:\n            tuple: params (i, j, h, w) to be passed to ``crop`` for a random\n                sized crop.\n        """"""\n        area = img.size[0] * img.size[1]\n\n        for attempt in range(10):\n            target_area = random.uniform(*scale) * area\n            log_ratio = (math.log(ratio[0]), math.log(ratio[1]))\n            aspect_ratio = math.exp(random.uniform(*log_ratio))\n\n            w = int(round(math.sqrt(target_area * aspect_ratio)))\n            h = int(round(math.sqrt(target_area / aspect_ratio)))\n\n            if w <= img.size[0] and h <= img.size[1]:\n                i = random.randint(0, img.size[1] - h)\n                j = random.randint(0, img.size[0] - w)\n                return i, j, h, w\n\n        # Fallback to central crop\n        in_ratio = img.size[0] / img.size[1]\n        if in_ratio < min(ratio):\n            w = img.size[0]\n            h = int(round(w / min(ratio)))\n        elif in_ratio > max(ratio):\n            h = img.size[1]\n            w = int(round(h * max(ratio)))\n        else:  # whole image\n            w = img.size[0]\n            h = img.size[1]\n        i = (img.size[1] - h) // 2\n        j = (img.size[0] - w) // 2\n        return i, j, h, w\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (PIL Image): Image to be cropped and resized.\n\n        Returns:\n            PIL Image: Randomly cropped and resized image.\n        """"""\n        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n        if isinstance(self.interpolation, (tuple, list)):\n            interpolation = random.choice(self.interpolation)\n        else:\n            interpolation = self.interpolation\n        return F.resized_crop(img, i, j, h, w, self.size, interpolation)\n\n    def __repr__(self):\n        if isinstance(self.interpolation, (tuple, list)):\n            interpolate_str = \' \'.join([_pil_interpolation_to_str[x] for x in self.interpolation])\n        else:\n            interpolate_str = _pil_interpolation_to_str[self.interpolation]\n        format_string = self.__class__.__name__ + \'(size={0}\'.format(self.size)\n        format_string += \', scale={0}\'.format(tuple(round(s, 4) for s in self.scale))\n        format_string += \', ratio={0}\'.format(tuple(round(r, 4) for r in self.ratio))\n        format_string += \', interpolation={0})\'.format(interpolate_str)\n        return format_string\n\n\n'"
timm/data/transforms_factory.py,4,"b'"""""" Transforms Factory\nFactory methods for building image transforms for use with TIMM (PyTorch Image Models)\n""""""\nimport math\n\nimport torch\nfrom torchvision import transforms\n\nfrom timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, DEFAULT_CROP_PCT\nfrom timm.data.auto_augment import rand_augment_transform, augment_and_mix_transform, auto_augment_transform\nfrom timm.data.transforms import _pil_interp, RandomResizedCropAndInterpolation, ToNumpy, ToTensor\nfrom timm.data.random_erasing import RandomErasing\n\n\ndef transforms_imagenet_train(\n        img_size=224,\n        scale=(0.08, 1.0),\n        color_jitter=0.4,\n        auto_augment=None,\n        interpolation=\'random\',\n        use_prefetcher=False,\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD,\n        re_prob=0.,\n        re_mode=\'const\',\n        re_count=1,\n        re_num_splits=0,\n        separate=False,\n):\n    """"""\n    If separate==True, the transforms are returned as a tuple of 3 separate transforms\n    for use in a mixing dataset that passes\n     * all data through the first (primary) transform, called the \'clean\' data\n     * a portion of the data through the secondary transform\n     * normalizes and converts the branches above with the third, final transform\n    """"""\n    primary_tfl = [\n        RandomResizedCropAndInterpolation(\n            img_size, scale=scale, interpolation=interpolation),\n        transforms.RandomHorizontalFlip()\n    ]\n\n    secondary_tfl = []\n    if auto_augment:\n        assert isinstance(auto_augment, str)\n        if isinstance(img_size, tuple):\n            img_size_min = min(img_size)\n        else:\n            img_size_min = img_size\n        aa_params = dict(\n            translate_const=int(img_size_min * 0.45),\n            img_mean=tuple([min(255, round(255 * x)) for x in mean]),\n        )\n        if interpolation and interpolation != \'random\':\n            aa_params[\'interpolation\'] = _pil_interp(interpolation)\n        if auto_augment.startswith(\'rand\'):\n            secondary_tfl += [rand_augment_transform(auto_augment, aa_params)]\n        elif auto_augment.startswith(\'augmix\'):\n            aa_params[\'translate_pct\'] = 0.3\n            secondary_tfl += [augment_and_mix_transform(auto_augment, aa_params)]\n        else:\n            secondary_tfl += [auto_augment_transform(auto_augment, aa_params)]\n    elif color_jitter is not None:\n        # color jitter is enabled when not using AA\n        if isinstance(color_jitter, (list, tuple)):\n            # color jitter should be a 3-tuple/list if spec brightness/contrast/saturation\n            # or 4 if also augmenting hue\n            assert len(color_jitter) in (3, 4)\n        else:\n            # if it\'s a scalar, duplicate for brightness, contrast, and saturation, no hue\n            color_jitter = (float(color_jitter),) * 3\n        secondary_tfl += [transforms.ColorJitter(*color_jitter)]\n\n    final_tfl = []\n    if use_prefetcher:\n        # prefetcher and collate will handle tensor conversion and norm\n        final_tfl += [ToNumpy()]\n    else:\n        final_tfl += [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                mean=torch.tensor(mean),\n                std=torch.tensor(std))\n        ]\n        if re_prob > 0.:\n            final_tfl.append(\n                RandomErasing(re_prob, mode=re_mode, max_count=re_count, num_splits=re_num_splits, device=\'cpu\'))\n\n    if separate:\n        return transforms.Compose(primary_tfl), transforms.Compose(secondary_tfl), transforms.Compose(final_tfl)\n    else:\n        return transforms.Compose(primary_tfl + secondary_tfl + final_tfl)\n\n\ndef transforms_imagenet_eval(\n        img_size=224,\n        crop_pct=None,\n        interpolation=\'bilinear\',\n        use_prefetcher=False,\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD):\n    crop_pct = crop_pct or DEFAULT_CROP_PCT\n\n    if isinstance(img_size, tuple):\n        assert len(img_size) == 2\n        if img_size[-1] == img_size[-2]:\n            # fall-back to older behaviour so Resize scales to shortest edge if target is square\n            scale_size = int(math.floor(img_size[0] / crop_pct))\n        else:\n            scale_size = tuple([int(x / crop_pct) for x in img_size])\n    else:\n        scale_size = int(math.floor(img_size / crop_pct))\n\n    tfl = [\n        transforms.Resize(scale_size, _pil_interp(interpolation)),\n        transforms.CenterCrop(img_size),\n    ]\n    if use_prefetcher:\n        # prefetcher and collate will handle tensor conversion and norm\n        tfl += [ToNumpy()]\n    else:\n        tfl += [\n            transforms.ToTensor(),\n            transforms.Normalize(\n                     mean=torch.tensor(mean),\n                     std=torch.tensor(std))\n        ]\n\n    return transforms.Compose(tfl)\n\n\ndef create_transform(\n        input_size,\n        is_training=False,\n        use_prefetcher=False,\n        color_jitter=0.4,\n        auto_augment=None,\n        interpolation=\'bilinear\',\n        mean=IMAGENET_DEFAULT_MEAN,\n        std=IMAGENET_DEFAULT_STD,\n        re_prob=0.,\n        re_mode=\'const\',\n        re_count=1,\n        re_num_splits=0,\n        crop_pct=None,\n        tf_preprocessing=False,\n        separate=False):\n\n    if isinstance(input_size, tuple):\n        img_size = input_size[-2:]\n    else:\n        img_size = input_size\n\n    if tf_preprocessing and use_prefetcher:\n        assert not separate, ""Separate transforms not supported for TF preprocessing""\n        from timm.data.tf_preprocessing import TfPreprocessTransform\n        transform = TfPreprocessTransform(\n            is_training=is_training, size=img_size, interpolation=interpolation)\n    else:\n        if is_training:\n            transform = transforms_imagenet_train(\n                img_size,\n                color_jitter=color_jitter,\n                auto_augment=auto_augment,\n                interpolation=interpolation,\n                use_prefetcher=use_prefetcher,\n                mean=mean,\n                std=std,\n                re_prob=re_prob,\n                re_mode=re_mode,\n                re_count=re_count,\n                re_num_splits=re_num_splits,\n                separate=separate)\n        else:\n            assert not separate, ""Separate transforms not supported for validation preprocessing""\n            transform = transforms_imagenet_eval(\n                img_size,\n                interpolation=interpolation,\n                use_prefetcher=use_prefetcher,\n                mean=mean,\n                std=std,\n                crop_pct=crop_pct)\n\n    return transform\n'"
timm/loss/__init__.py,0,"b'from .cross_entropy import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\nfrom .jsd import JsdCrossEntropy'"
timm/loss/cross_entropy.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass LabelSmoothingCrossEntropy(nn.Module):\n    """"""\n    NLL loss with label smoothing.\n    """"""\n    def __init__(self, smoothing=0.1):\n        """"""\n        Constructor for the LabelSmoothing module.\n        :param smoothing: label smoothing factor\n        """"""\n        super(LabelSmoothingCrossEntropy, self).__init__()\n        assert smoothing < 1.0\n        self.smoothing = smoothing\n        self.confidence = 1. - smoothing\n\n    def forward(self, x, target):\n        logprobs = F.log_softmax(x, dim=-1)\n        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n        nll_loss = nll_loss.squeeze(1)\n        smooth_loss = -logprobs.mean(dim=-1)\n        loss = self.confidence * nll_loss + self.smoothing * smooth_loss\n        return loss.mean()\n\n\nclass SoftTargetCrossEntropy(nn.Module):\n\n    def __init__(self):\n        super(SoftTargetCrossEntropy, self).__init__()\n\n    def forward(self, x, target):\n        loss = torch.sum(-target * F.log_softmax(x, dim=-1), dim=-1)\n        return loss.mean()\n'"
timm/loss/jsd.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .cross_entropy import LabelSmoothingCrossEntropy\n\n\nclass JsdCrossEntropy(nn.Module):\n    """""" Jensen-Shannon Divergence + Cross-Entropy Loss\n\n    Based on impl here: https://github.com/google-research/augmix/blob/master/imagenet.py\n    From paper: \'AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty -\n    https://arxiv.org/abs/1912.02781\n\n    Hacked together by Ross Wightman\n    """"""\n    def __init__(self, num_splits=3, alpha=12, smoothing=0.1):\n        super().__init__()\n        self.num_splits = num_splits\n        self.alpha = alpha\n        if smoothing is not None and smoothing > 0:\n            self.cross_entropy_loss = LabelSmoothingCrossEntropy(smoothing)\n        else:\n            self.cross_entropy_loss = torch.nn.CrossEntropyLoss()\n\n    def __call__(self, output, target):\n        split_size = output.shape[0] // self.num_splits\n        assert split_size * self.num_splits == output.shape[0]\n        logits_split = torch.split(output, split_size)\n\n        # Cross-entropy is only computed on clean images\n        loss = self.cross_entropy_loss(logits_split[0], target[:split_size])\n        probs = [F.softmax(logits, dim=1) for logits in logits_split]\n\n        # Clamp mixture distribution to avoid exploding KL divergence\n        logp_mixture = torch.clamp(torch.stack(probs).mean(axis=0), 1e-7, 1).log()\n        loss += self.alpha * sum([F.kl_div(\n            logp_mixture, p_split, reduction=\'batchmean\') for p_split in probs]) / len(probs)\n        return loss\n'"
timm/models/__init__.py,0,"b'from .inception_v4 import *\nfrom .inception_resnet_v2 import *\nfrom .densenet import *\nfrom .resnet import *\nfrom .dpn import *\nfrom .senet import *\nfrom .xception import *\nfrom .nasnet import *\nfrom .pnasnet import *\nfrom .selecsls import *\nfrom .efficientnet import *\nfrom .mobilenetv3 import *\nfrom .inception_v3 import *\nfrom .gluon_resnet import *\nfrom .gluon_xception import *\nfrom .res2net import *\nfrom .dla import *\nfrom .hrnet import *\nfrom .sknet import *\nfrom .tresnet import *\nfrom .resnest import *\nfrom .regnet import *\n\nfrom .registry import *\nfrom .factory import create_model\nfrom .helpers import load_checkpoint, resume_checkpoint\nfrom .layers import TestTimePoolHead, apply_test_time_pool\nfrom .layers import convert_splitbn_model\n'"
timm/models/densenet.py,7,"b'""""""Pytorch Densenet implementation w/ tweaks\nThis file is a copy of https://github.com/pytorch/vision \'densenet.py\' (BSD-3-Clause) with\nfixed kwargs passthrough and addition of dynamic global avg/max pool.\n""""""\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SelectAdaptivePool2d\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nimport re\n\n__all__ = [\'DenseNet\']\n\n\ndef _cfg(url=\'\'):\n    return {\n        \'url\': url, \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (7, 7),\n        \'crop_pct\': 0.875, \'interpolation\': \'bicubic\',\n        \'mean\': IMAGENET_DEFAULT_MEAN, \'std\': IMAGENET_DEFAULT_STD,\n        \'first_conv\': \'features.conv0\', \'classifier\': \'classifier\',\n    }\n\n\ndefault_cfgs = {\n    \'densenet121\': _cfg(url=\'https://download.pytorch.org/models/densenet121-a639ec97.pth\'),\n    \'densenet169\': _cfg(url=\'https://download.pytorch.org/models/densenet169-b2777c0a.pth\'),\n    \'densenet201\': _cfg(url=\'https://download.pytorch.org/models/densenet201-c1103571.pth\'),\n    \'densenet161\': _cfg(url=\'https://download.pytorch.org/models/densenet161-8d451a50.pth\'),\n}\n\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv1\', nn.Conv2d(num_input_features, bn_size *\n                        growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module(\'norm2\', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(\'relu2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv2\', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                        kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', nn.BatchNorm2d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module(\'pool\', nn.AvgPool2d(kernel_size=2, stride=2))\n\n\nclass DenseNet(nn.Module):\n    r""""""Densenet-BC model class, based on\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n    """"""\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n                 num_init_features=64, bn_size=4, drop_rate=0,\n                 num_classes=1000, in_chans=3, global_pool=\'avg\'):\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        super(DenseNet, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(in_chans, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            (\'norm0\', nn.BatchNorm2d(num_init_features)),\n            (\'relu0\', nn.ReLU(inplace=True)),\n            (\'pool0\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(\n                    num_input_features=num_features, num_output_features=num_features // 2)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', nn.BatchNorm2d(num_features))\n\n        # Linear layer\n        self.num_features = num_features\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.classifier = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n\n    def get_classifier(self):\n        return self.classifier\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.num_classes = num_classes\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.classifier = nn.Linear(\n            self.num_features * self.global_pool.feat_mult(), num_classes) if num_classes else None\n\n    def forward_features(self, x):\n        x = self.features(x)\n        x = F.relu(x, inplace=True)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x).flatten(1)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\ndef _filter_pretrained(state_dict):\n    pattern = re.compile(\n        r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n\n    for key in list(state_dict.keys()):\n        res = pattern.match(key)\n        if res:\n            new_key = res.group(1) + res.group(2)\n            state_dict[new_key] = state_dict[key]\n            del state_dict[key]\n    return state_dict\n\n\n\n@register_model\ndef densenet121(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    r""""""Densenet-121 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    """"""\n    default_cfg = default_cfgs[\'densenet121\']\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\n                     num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans, filter_fn=_filter_pretrained)\n    return model\n\n\n@register_model\ndef densenet169(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    r""""""Densenet-169 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    """"""\n    default_cfg = default_cfgs[\'densenet169\']\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 32, 32),\n                     num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans, filter_fn=_filter_pretrained)\n    return model\n\n\n@register_model\ndef densenet201(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    r""""""Densenet-201 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    """"""\n    default_cfg = default_cfgs[\'densenet201\']\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 48, 32),\n                     num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans, filter_fn=_filter_pretrained)\n    return model\n\n\n@register_model\ndef densenet161(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    r""""""Densenet-201 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`\n    """"""\n    default_cfg = default_cfgs[\'densenet161\']\n    model = DenseNet(num_init_features=96, growth_rate=48, block_config=(6, 12, 36, 24),\n                     num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans, filter_fn=_filter_pretrained)\n    return model\n'"
timm/models/dla.py,5,"b'"""""" Deep Layer Aggregation and DLA w/ Res2Net\nDLA original adapted from Official Pytorch impl at:\nDLA Paper: `Deep Layer Aggregation` - https://arxiv.org/abs/1707.06484\n\nRes2Net additions from: https://github.com/gasvn/Res2Net/\nRes2Net Paper: `Res2Net: A New Multi-scale Backbone Architecture` - https://arxiv.org/abs/1904.01169\n""""""\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SelectAdaptivePool2d\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\n\n__all__ = [\'DLA\']\n\n\ndef _cfg(url=\'\', **kwargs):\n    return {\n        \'url\': url,\n        \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (7, 7),\n        \'crop_pct\': 0.875, \'interpolation\': \'bilinear\',\n        \'mean\': IMAGENET_DEFAULT_MEAN, \'std\': IMAGENET_DEFAULT_STD,\n        \'first_conv\': \'base_layer.0\', \'classifier\': \'fc\',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    \'dla34\': _cfg(url=\'http://dl.yf.io/dla/models/imagenet/dla34-ba72cf86.pth\'),\n    \'dla46_c\': _cfg(url=\'http://dl.yf.io/dla/models/imagenet/dla46_c-2bfd52c3.pth\'),\n    \'dla46x_c\': _cfg(url=\'http://dl.yf.io/dla/models/imagenet/dla46x_c-d761bae7.pth\'),\n    \'dla60x_c\': _cfg(url=\'http://dl.yf.io/dla/models/imagenet/dla60x_c-b870c45c.pth\'),\n    \'dla60\': _cfg(url=\'http://dl.yf.io/dla/models/imagenet/dla60-24839fc4.pth\'),\n    \'dla60x\': _cfg(url=\'http://dl.yf.io/dla/models/imagenet/dla60x-d15cacda.pth\'),\n    \'dla102\': _cfg(url=\'http://dl.yf.io/dla/models/imagenet/dla102-d94d9790.pth\'),\n    \'dla102x\': _cfg(url=\'http://dl.yf.io/dla/models/imagenet/dla102x-ad62be81.pth\'),\n    \'dla102x2\': _cfg(url=\'http://dl.yf.io/dla/models/imagenet/dla102x2-262837b6.pth\'),\n    \'dla169\': _cfg(url=\'http://dl.yf.io/dla/models/imagenet/dla169-0914e092.pth\'),\n    \'dla60_res2net\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net_dla60_4s-d88db7f9.pth\'),\n    \'dla60_res2next\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2next_dla60_4s-d327927b.pth\'),\n}\n\n\nclass DlaBasic(nn.Module):\n    """"""DLA Basic""""""\n    def __init__(self, inplanes, planes, stride=1, dilation=1, **_):\n        super(DlaBasic, self).__init__()\n        self.conv1 = nn.Conv2d(\n            inplanes, planes, kernel_size=3, stride=stride, padding=dilation, bias=False, dilation=dilation)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, stride=1, padding=dilation, bias=False, dilation=dilation)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.stride = stride\n\n    def forward(self, x, residual=None):\n        if residual is None:\n            residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass DlaBottleneck(nn.Module):\n    """"""DLA/DLA-X Bottleneck""""""\n    expansion = 2\n\n    def __init__(self, inplanes, outplanes, stride=1, dilation=1, cardinality=1, base_width=64):\n        super(DlaBottleneck, self).__init__()\n        self.stride = stride\n        mid_planes = int(math.floor(outplanes * (base_width / 64)) * cardinality)\n        mid_planes = mid_planes // self.expansion\n\n        self.conv1 = nn.Conv2d(inplanes, mid_planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_planes)\n        self.conv2 = nn.Conv2d(\n            mid_planes, mid_planes, kernel_size=3, stride=stride, padding=dilation,\n            bias=False, dilation=dilation, groups=cardinality)\n        self.bn2 = nn.BatchNorm2d(mid_planes)\n        self.conv3 = nn.Conv2d(mid_planes, outplanes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(outplanes)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x, residual=None):\n        if residual is None:\n            residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass DlaBottle2neck(nn.Module):\n    """""" Res2Net/Res2NeXT DLA Bottleneck\n    Adapted from https://github.com/gasvn/Res2Net/blob/master/dla.py\n    """"""\n    expansion = 2\n\n    def __init__(self, inplanes, outplanes, stride=1, dilation=1, scale=4, cardinality=8, base_width=4):\n        super(DlaBottle2neck, self).__init__()\n        self.is_first = stride > 1\n        self.scale = scale\n        mid_planes = int(math.floor(outplanes * (base_width / 64)) * cardinality)\n        mid_planes = mid_planes // self.expansion\n        self.width = mid_planes\n\n        self.conv1 = nn.Conv2d(inplanes, mid_planes * scale, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_planes * scale)\n\n        num_scale_convs = max(1, scale - 1)\n        convs = []\n        bns = []\n        for _ in range(num_scale_convs):\n            convs.append(nn.Conv2d(\n                mid_planes, mid_planes, kernel_size=3, stride=stride,\n                padding=dilation, dilation=dilation, groups=cardinality, bias=False))\n            bns.append(nn.BatchNorm2d(mid_planes))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n        if self.is_first:\n            self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1)\n\n        self.conv3 = nn.Conv2d(mid_planes * scale, outplanes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(outplanes)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x, residual=None):\n        if residual is None:\n            residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        spx = torch.split(out, self.width, 1)\n        spo = []\n        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):\n            sp = spx[i] if i == 0 or self.is_first else sp + spx[i]\n            sp = conv(sp)\n            sp = bn(sp)\n            sp = self.relu(sp)\n            spo.append(sp)\n        if self.scale > 1 :\n            spo.append(self.pool(spx[-1]) if self.is_first else spx[-1])\n        out = torch.cat(spo, 1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass DlaRoot(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, residual):\n        super(DlaRoot, self).__init__()\n        self.conv = nn.Conv2d(\n            in_channels, out_channels, 1, stride=1, bias=False, padding=(kernel_size - 1) // 2)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.residual = residual\n\n    def forward(self, *x):\n        children = x\n        x = self.conv(torch.cat(x, 1))\n        x = self.bn(x)\n        if self.residual:\n            x += children[0]\n        x = self.relu(x)\n\n        return x\n\n\nclass DlaTree(nn.Module):\n    def __init__(self, levels, block, in_channels, out_channels, stride=1,\n                 dilation=1, cardinality=1, base_width=64,\n                 level_root=False, root_dim=0, root_kernel_size=1, root_residual=False):\n        super(DlaTree, self).__init__()\n        if root_dim == 0:\n            root_dim = 2 * out_channels\n        if level_root:\n            root_dim += in_channels\n        cargs = dict(dilation=dilation, cardinality=cardinality, base_width=base_width)\n        if levels == 1:\n            self.tree1 = block(in_channels, out_channels, stride, **cargs)\n            self.tree2 = block(out_channels, out_channels, 1, **cargs)\n        else:\n            cargs.update(dict(root_kernel_size=root_kernel_size, root_residual=root_residual))\n            self.tree1 = DlaTree(\n                levels - 1, block, in_channels, out_channels, stride, root_dim=0, **cargs)\n            self.tree2 = DlaTree(\n                levels - 1, block, out_channels, out_channels, root_dim=root_dim + out_channels, **cargs)\n        if levels == 1:\n            self.root = DlaRoot(root_dim, out_channels, root_kernel_size, root_residual)\n        self.level_root = level_root\n        self.root_dim = root_dim\n        self.downsample = nn.MaxPool2d(stride, stride=stride) if stride > 1 else None\n        self.project = None\n        if in_channels != out_channels:\n            self.project = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n        self.levels = levels\n\n    def forward(self, x, residual=None, children=None):\n        children = [] if children is None else children\n        # FIXME the way downsample / project are used here and residual is passed to next level up\n        # the tree, the residual is overridden and some project weights are thus never used and\n        # have no gradients. This appears to be an issue with the original model / weights.\n        bottom = self.downsample(x) if self.downsample is not None else x\n        residual = self.project(bottom) if self.project is not None else bottom\n        if self.level_root:\n            children.append(bottom)\n        x1 = self.tree1(x, residual)\n        if self.levels == 1:\n            x2 = self.tree2(x1)\n            x = self.root(x2, x1, *children)\n        else:\n            children.append(x1)\n            x = self.tree2(x1, children=children)\n        return x\n\n\nclass DLA(nn.Module):\n    def __init__(self, levels, channels, num_classes=1000, in_chans=3, cardinality=1, base_width=64,\n                 block=DlaBottle2neck, residual_root=False, linear_root=False,\n                 drop_rate=0.0, global_pool=\'avg\'):\n        super(DLA, self).__init__()\n        self.channels = channels\n        self.num_classes = num_classes\n        self.cardinality = cardinality\n        self.base_width = base_width\n        self.drop_rate = drop_rate\n\n        self.base_layer = nn.Sequential(\n            nn.Conv2d(in_chans, channels[0], kernel_size=7, stride=1, padding=3, bias=False),\n            nn.BatchNorm2d(channels[0]),\n            nn.ReLU(inplace=True))\n        self.level0 = self._make_conv_level(channels[0], channels[0], levels[0])\n        self.level1 = self._make_conv_level(channels[0], channels[1], levels[1], stride=2)\n        cargs = dict(cardinality=cardinality, base_width=base_width, root_residual=residual_root)\n        self.level2 = DlaTree(levels[2], block, channels[1], channels[2], 2, level_root=False, **cargs)\n        self.level3 = DlaTree(levels[3], block, channels[2], channels[3], 2, level_root=True, **cargs)\n        self.level4 = DlaTree(levels[4], block, channels[3], channels[4], 2, level_root=True, **cargs)\n        self.level5 = DlaTree(levels[5], block, channels[4], channels[5], 2, level_root=True, **cargs)\n\n        self.num_features = channels[-1]\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.fc = nn.Conv2d(self.num_features * self.global_pool.feat_mult(), num_classes, 1, bias=True)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_conv_level(self, inplanes, planes, convs, stride=1, dilation=1):\n        modules = []\n        for i in range(convs):\n            modules.extend([\n                nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride if i == 0 else 1,\n                          padding=dilation, bias=False, dilation=dilation),\n                nn.BatchNorm2d(planes),\n                nn.ReLU(inplace=True)])\n            inplanes = planes\n        return nn.Sequential(*modules)\n\n    def get_classifier(self):\n        return self.fc\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.num_classes = num_classes\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        if num_classes:\n            self.fc = nn.Conv2d(self.num_features * self.global_pool.feat_mult(), num_classes, 1, bias=True)\n        else:\n            self.fc = None\n\n    def forward_features(self, x):\n        x = self.base_layer(x)\n        x = self.level0(x)\n        x = self.level1(x)\n        x = self.level2(x)\n        x = self.level3(x)\n        x = self.level4(x)\n        x = self.level5(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        x = self.fc(x)\n        return x.flatten(1)\n\n\n@register_model\ndef dla60_res2net(pretrained=None, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'dla60_res2net\']\n    model = DLA(levels=(1, 1, 1, 2, 3, 1), channels=(16, 32, 128, 256, 512, 1024),\n                block=DlaBottle2neck, cardinality=1, base_width=28,\n                num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dla60_res2next(pretrained=None, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'dla60_res2next\']\n    model = DLA(levels=(1, 1, 1, 2, 3, 1), channels=(16, 32, 128, 256, 512, 1024),\n                block=DlaBottle2neck, cardinality=8, base_width=4,\n                num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dla34(pretrained=None, num_classes=1000, in_chans=3, **kwargs):  # DLA-34\n    default_cfg = default_cfgs[\'dla34\']\n    model = DLA([1, 1, 1, 2, 2, 1], [16, 32, 64, 128, 256, 512], block=DlaBasic,\n                num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dla46_c(pretrained=None, num_classes=1000, in_chans=3, **kwargs):  # DLA-46-C\n    default_cfg = default_cfgs[\'dla46_c\']\n    model = DLA(levels=[1, 1, 1, 2, 2, 1], channels=[16, 32, 64, 64, 128, 256],\n                block=DlaBottleneck, num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dla46x_c(pretrained=None, num_classes=1000, in_chans=3, **kwargs):  # DLA-X-46-C\n    default_cfg = default_cfgs[\'dla46x_c\']\n    model = DLA(levels=[1, 1, 1, 2, 2, 1], channels=[16, 32, 64, 64, 128, 256],\n                block=DlaBottleneck, cardinality=32, base_width=4,\n                num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dla60x_c(pretrained=None, num_classes=1000, in_chans=3, **kwargs):  # DLA-X-60-C\n    default_cfg = default_cfgs[\'dla60x_c\']\n    model = DLA([1, 1, 1, 2, 3, 1], [16, 32, 64, 64, 128, 256],\n                block=DlaBottleneck, cardinality=32, base_width=4,\n                num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dla60(pretrained=None, num_classes=1000, in_chans=3, **kwargs):  # DLA-60\n    default_cfg = default_cfgs[\'dla60\']\n    model = DLA([1, 1, 1, 2, 3, 1], [16, 32, 128, 256, 512, 1024],\n                block=DlaBottleneck, num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dla60x(pretrained=None, num_classes=1000, in_chans=3, **kwargs):  # DLA-X-60\n    default_cfg = default_cfgs[\'dla60x\']\n    model = DLA([1, 1, 1, 2, 3, 1], [16, 32, 128, 256, 512, 1024],\n                block=DlaBottleneck, cardinality=32, base_width=4,\n                num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dla102(pretrained=None, num_classes=1000, in_chans=3, **kwargs):  # DLA-102\n    default_cfg = default_cfgs[\'dla102\']\n    model = DLA([1, 1, 1, 3, 4, 1], [16, 32, 128, 256, 512, 1024],\n                block=DlaBottleneck, residual_root=True,\n                num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dla102x(pretrained=None, num_classes=1000, in_chans=3, **kwargs):  # DLA-X-102\n    default_cfg = default_cfgs[\'dla102x\']\n    model = DLA([1, 1, 1, 3, 4, 1], [16, 32, 128, 256, 512, 1024],\n                block=DlaBottleneck, cardinality=32, base_width=4, residual_root=True,\n                num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dla102x2(pretrained=None, num_classes=1000, in_chans=3, **kwargs):  # DLA-X-102 64\n    default_cfg = default_cfgs[\'dla102x2\']\n    model = DLA([1, 1, 1, 3, 4, 1], [16, 32, 128, 256, 512, 1024],\n                block=DlaBottleneck, cardinality=64, base_width=4, residual_root=True,\n                num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dla169(pretrained=None, num_classes=1000, in_chans=3, **kwargs):  # DLA-169\n    default_cfg = default_cfgs[\'dla169\']\n    model = DLA([1, 1, 2, 3, 5, 1], [16, 32, 128, 256, 512, 1024],\n                block=DlaBottleneck, residual_root=True,\n                num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n'"
timm/models/dpn.py,5,"b'"""""" PyTorch implementation of DualPathNetworks\nBased on original MXNet implementation https://github.com/cypw/DPNs with\nmany ideas from another PyTorch implementation https://github.com/oyam/pytorch-DPNs.\n\nThis implementation is compatible with the pretrained weights\nfrom cypw\'s MXNet implementation.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SelectAdaptivePool2d\nfrom timm.data import IMAGENET_DPN_MEAN, IMAGENET_DPN_STD\n\n\n__all__ = [\'DPN\']\n\n\ndef _cfg(url=\'\'):\n    return {\n        \'url\': url, \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (7, 7),\n        \'crop_pct\': 0.875, \'interpolation\': \'bicubic\',\n        \'mean\': IMAGENET_DPN_MEAN, \'std\': IMAGENET_DPN_STD,\n        \'first_conv\': \'features.conv1_1.conv\', \'classifier\': \'classifier\',\n    }\n\n\ndefault_cfgs = {\n    \'dpn68\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-dpn-pretrained/releases/download/v0.1/dpn68-66bebafa7.pth\'),\n    \'dpn68b\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-dpn-pretrained/releases/download/v0.1/dpn68b_extra-84854c156.pth\'),\n    \'dpn92\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-dpn-pretrained/releases/download/v0.1/dpn92_extra-b040e4a9b.pth\'),\n    \'dpn98\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-dpn-pretrained/releases/download/v0.1/dpn98-5b90dec4d.pth\'),\n    \'dpn131\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-dpn-pretrained/releases/download/v0.1/dpn131-71dfe43e0.pth\'),\n    \'dpn107\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-dpn-pretrained/releases/download/v0.1/dpn107_extra-1ac7121e2.pth\')\n}\n\n\nclass CatBnAct(nn.Module):\n    def __init__(self, in_chs, activation_fn=nn.ReLU(inplace=True)):\n        super(CatBnAct, self).__init__()\n        self.bn = nn.BatchNorm2d(in_chs, eps=0.001)\n        self.act = activation_fn\n\n    def forward(self, x):\n        x = torch.cat(x, dim=1) if isinstance(x, tuple) else x\n        return self.act(self.bn(x))\n\n\nclass BnActConv2d(nn.Module):\n    def __init__(self, in_chs, out_chs, kernel_size, stride,\n                 padding=0, groups=1, activation_fn=nn.ReLU(inplace=True)):\n        super(BnActConv2d, self).__init__()\n        self.bn = nn.BatchNorm2d(in_chs, eps=0.001)\n        self.act = activation_fn\n        self.conv = nn.Conv2d(in_chs, out_chs, kernel_size, stride, padding, groups=groups, bias=False)\n\n    def forward(self, x):\n        return self.conv(self.act(self.bn(x)))\n\n\nclass InputBlock(nn.Module):\n    def __init__(self, num_init_features, kernel_size=7, in_chans=3,\n                 padding=3, activation_fn=nn.ReLU(inplace=True)):\n        super(InputBlock, self).__init__()\n        self.conv = nn.Conv2d(\n            in_chans, num_init_features, kernel_size=kernel_size, stride=2, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(num_init_features, eps=0.001)\n        self.act = activation_fn\n        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.act(x)\n        x = self.pool(x)\n        return x\n\n\nclass DualPathBlock(nn.Module):\n    def __init__(\n            self, in_chs, num_1x1_a, num_3x3_b, num_1x1_c, inc, groups, block_type=\'normal\', b=False):\n        super(DualPathBlock, self).__init__()\n        self.num_1x1_c = num_1x1_c\n        self.inc = inc\n        self.b = b\n        if block_type == \'proj\':\n            self.key_stride = 1\n            self.has_proj = True\n        elif block_type == \'down\':\n            self.key_stride = 2\n            self.has_proj = True\n        else:\n            assert block_type == \'normal\'\n            self.key_stride = 1\n            self.has_proj = False\n\n        if self.has_proj:\n            # Using different member names here to allow easier parameter key matching for conversion\n            if self.key_stride == 2:\n                self.c1x1_w_s2 = BnActConv2d(\n                    in_chs=in_chs, out_chs=num_1x1_c + 2 * inc, kernel_size=1, stride=2)\n            else:\n                self.c1x1_w_s1 = BnActConv2d(\n                    in_chs=in_chs, out_chs=num_1x1_c + 2 * inc, kernel_size=1, stride=1)\n        self.c1x1_a = BnActConv2d(in_chs=in_chs, out_chs=num_1x1_a, kernel_size=1, stride=1)\n        self.c3x3_b = BnActConv2d(\n            in_chs=num_1x1_a, out_chs=num_3x3_b, kernel_size=3,\n            stride=self.key_stride, padding=1, groups=groups)\n        if b:\n            self.c1x1_c = CatBnAct(in_chs=num_3x3_b)\n            self.c1x1_c1 = nn.Conv2d(num_3x3_b, num_1x1_c, kernel_size=1, bias=False)\n            self.c1x1_c2 = nn.Conv2d(num_3x3_b, inc, kernel_size=1, bias=False)\n        else:\n            self.c1x1_c = BnActConv2d(in_chs=num_3x3_b, out_chs=num_1x1_c + inc, kernel_size=1, stride=1)\n\n    def forward(self, x):\n        x_in = torch.cat(x, dim=1) if isinstance(x, tuple) else x\n        if self.has_proj:\n            if self.key_stride == 2:\n                x_s = self.c1x1_w_s2(x_in)\n            else:\n                x_s = self.c1x1_w_s1(x_in)\n            x_s1 = x_s[:, :self.num_1x1_c, :, :]\n            x_s2 = x_s[:, self.num_1x1_c:, :, :]\n        else:\n            x_s1 = x[0]\n            x_s2 = x[1]\n        x_in = self.c1x1_a(x_in)\n        x_in = self.c3x3_b(x_in)\n        if self.b:\n            x_in = self.c1x1_c(x_in)\n            out1 = self.c1x1_c1(x_in)\n            out2 = self.c1x1_c2(x_in)\n        else:\n            x_in = self.c1x1_c(x_in)\n            out1 = x_in[:, :self.num_1x1_c, :, :]\n            out2 = x_in[:, self.num_1x1_c:, :, :]\n        resid = x_s1 + out1\n        dense = torch.cat([x_s2, out2], dim=1)\n        return resid, dense\n\n\nclass DPN(nn.Module):\n    def __init__(self, small=False, num_init_features=64, k_r=96, groups=32,\n                 b=False, k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128),\n                 num_classes=1000, in_chans=3, drop_rate=0., global_pool=\'avg\', fc_act=nn.ELU()):\n        super(DPN, self).__init__()\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        self.b = b\n        bw_factor = 1 if small else 4\n\n        blocks = OrderedDict()\n\n        # conv1\n        if small:\n            blocks[\'conv1_1\'] = InputBlock(\n                num_init_features, in_chans=in_chans, kernel_size=3, padding=1)\n        else:\n            blocks[\'conv1_1\'] = InputBlock(\n                num_init_features, in_chans=in_chans, kernel_size=7, padding=3)\n\n        # conv2\n        bw = 64 * bw_factor\n        inc = inc_sec[0]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks[\'conv2_1\'] = DualPathBlock(num_init_features, r, r, bw, inc, groups, \'proj\', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[0] + 1):\n            blocks[\'conv2_\' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'normal\', b)\n            in_chs += inc\n\n        # conv3\n        bw = 128 * bw_factor\n        inc = inc_sec[1]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks[\'conv3_1\'] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'down\', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[1] + 1):\n            blocks[\'conv3_\' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'normal\', b)\n            in_chs += inc\n\n        # conv4\n        bw = 256 * bw_factor\n        inc = inc_sec[2]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks[\'conv4_1\'] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'down\', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[2] + 1):\n            blocks[\'conv4_\' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'normal\', b)\n            in_chs += inc\n\n        # conv5\n        bw = 512 * bw_factor\n        inc = inc_sec[3]\n        r = (k_r * bw) // (64 * bw_factor)\n        blocks[\'conv5_1\'] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'down\', b)\n        in_chs = bw + 3 * inc\n        for i in range(2, k_sec[3] + 1):\n            blocks[\'conv5_\' + str(i)] = DualPathBlock(in_chs, r, r, bw, inc, groups, \'normal\', b)\n            in_chs += inc\n        blocks[\'conv5_bn_ac\'] = CatBnAct(in_chs, activation_fn=fc_act)\n        self.num_features = in_chs\n        self.features = nn.Sequential(blocks)\n\n        # Using 1x1 conv for the FC layer to allow the extra pooling scheme\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.classifier = nn.Conv2d(\n            self.num_features * self.global_pool.feat_mult(), num_classes, kernel_size=1, bias=True)\n\n    def get_classifier(self):\n        return self.classifier\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.num_classes = num_classes\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        if num_classes:\n            self.classifier = nn.Conv2d(\n                self.num_features * self.global_pool.feat_mult(), num_classes, kernel_size=1, bias=True)\n        else:\n            self.classifier = None\n\n    def forward_features(self, x):\n        return self.features(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        out = self.classifier(x)\n        return out.flatten(1)\n\n\n@register_model\ndef dpn68(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'dpn68\']\n    model = DPN(\n        small=True, num_init_features=10, k_r=128, groups=32,\n        k_sec=(3, 4, 12, 3), inc_sec=(16, 32, 32, 64),\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dpn68b(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'dpn68b\']\n    model = DPN(\n        small=True, num_init_features=10, k_r=128, groups=32,\n        b=True, k_sec=(3, 4, 12, 3), inc_sec=(16, 32, 32, 64),\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dpn92(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'dpn92\']\n    model = DPN(\n        num_init_features=64, k_r=96, groups=32,\n        k_sec=(3, 4, 20, 3), inc_sec=(16, 32, 24, 128),\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dpn98(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'dpn98\']\n    model = DPN(\n        num_init_features=96, k_r=160, groups=40,\n        k_sec=(3, 6, 20, 3), inc_sec=(16, 32, 32, 128),\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dpn131(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'dpn131\']\n    model = DPN(\n        num_init_features=128, k_r=160, groups=40,\n        k_sec=(4, 8, 28, 3), inc_sec=(16, 32, 32, 128),\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef dpn107(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'dpn107\']\n    model = DPN(\n        num_init_features=128, k_r=200, groups=50,\n        k_sec=(4, 8, 20, 3), inc_sec=(20, 64, 64, 128),\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n'"
timm/models/efficientnet.py,0,"b'"""""" PyTorch EfficientNet Family\n\nAn implementation of EfficienNet that covers variety of related models with efficient architectures:\n\n* EfficientNet (B0-B8, L2 + Tensorflow pretrained AutoAug/RandAug/AdvProp/NoisyStudent weight ports)\n  - EfficientNet: Rethinking Model Scaling for CNNs - https://arxiv.org/abs/1905.11946\n  - CondConv: Conditionally Parameterized Convolutions for Efficient Inference - https://arxiv.org/abs/1904.04971\n  - Adversarial Examples Improve Image Recognition - https://arxiv.org/abs/1911.09665\n  - Self-training with Noisy Student improves ImageNet classification - https://arxiv.org/abs/1911.04252\n\n* MixNet (Small, Medium, and Large)\n  - MixConv: Mixed Depthwise Convolutional Kernels - https://arxiv.org/abs/1907.09595\n\n* MNasNet B1, A1 (SE), Small\n  - MnasNet: Platform-Aware Neural Architecture Search for Mobile - https://arxiv.org/abs/1807.11626\n\n* FBNet-C\n  - FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable NAS - https://arxiv.org/abs/1812.03443\n\n* Single-Path NAS Pixel1\n  - Single-Path NAS: Designing Hardware-Efficient ConvNets - https://arxiv.org/abs/1904.02877\n\n* And likely more...\n\nHacked together by Ross Wightman\n""""""\nfrom .efficientnet_builder import *\nfrom .feature_hooks import FeatureHooks\nfrom .registry import register_model\nfrom .helpers import load_pretrained, adapt_model_from_file\nfrom .layers import SelectAdaptivePool2d\nfrom timm.models.layers import create_conv2d\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n\n\n__all__ = [\'EfficientNet\']\n\n\ndef _cfg(url=\'\', **kwargs):\n    return {\n        \'url\': url, \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (7, 7),\n        \'crop_pct\': 0.875, \'interpolation\': \'bicubic\',\n        \'mean\': IMAGENET_DEFAULT_MEAN, \'std\': IMAGENET_DEFAULT_STD,\n        \'first_conv\': \'conv_stem\', \'classifier\': \'classifier\',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    \'mnasnet_050\': _cfg(url=\'\'),\n    \'mnasnet_075\': _cfg(url=\'\'),\n    \'mnasnet_100\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mnasnet_b1-74cb7081.pth\'),\n    \'mnasnet_140\': _cfg(url=\'\'),\n\n    \'semnasnet_050\': _cfg(url=\'\'),\n    \'semnasnet_075\': _cfg(url=\'\'),\n    \'semnasnet_100\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mnasnet_a1-d9418771.pth\'),\n    \'semnasnet_140\': _cfg(url=\'\'),\n    \'mnasnet_small\': _cfg(url=\'\'),\n\n    \'mobilenetv2_100\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_100_ra-b33bc2c4.pth\'),\n    \'mobilenetv2_110d\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_110d_ra-77090ade.pth\'),\n    \'mobilenetv2_120d\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_120d_ra-5987e2ed.pth\'),\n    \'mobilenetv2_140\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv2_140_ra-21a4e913.pth\'),\n\n    \'fbnetc_100\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/fbnetc_100-c345b898.pth\',\n        interpolation=\'bilinear\'),\n    \'spnasnet_100\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/spnasnet_100-048bc3f4.pth\',\n        interpolation=\'bilinear\'),\n\n    \'efficientnet_b0\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b0_ra-3dd342df.pth\'),\n    \'efficientnet_b1\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b1-533bc792.pth\',\n        input_size=(3, 240, 240), pool_size=(8, 8)),\n    \'efficientnet_b2\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b2_ra-bcdf34b7.pth\',\n        input_size=(3, 260, 260), pool_size=(9, 9)),\n    \'efficientnet_b2a\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b2_ra-bcdf34b7.pth\',\n        input_size=(3, 288, 288), pool_size=(9, 9), crop_pct=1.0),\n    \'efficientnet_b3\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra-a5e2fbc7.pth\',\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n    \'efficientnet_b3a\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b3_ra-a5e2fbc7.pth\',\n        input_size=(3, 320, 320), pool_size=(10, 10), crop_pct=1.0),\n    \'efficientnet_b4\': _cfg(\n        url=\'\', input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.922),\n    \'efficientnet_b5\': _cfg(\n        url=\'\', input_size=(3, 456, 456), pool_size=(15, 15), crop_pct=0.934),\n    \'efficientnet_b6\': _cfg(\n        url=\'\', input_size=(3, 528, 528), pool_size=(17, 17), crop_pct=0.942),\n    \'efficientnet_b7\': _cfg(\n        url=\'\', input_size=(3, 600, 600), pool_size=(19, 19), crop_pct=0.949),\n    \'efficientnet_b8\': _cfg(\n        url=\'\', input_size=(3, 672, 672), pool_size=(21, 21), crop_pct=0.954),\n    \'efficientnet_l2\': _cfg(\n        url=\'\', input_size=(3, 800, 800), pool_size=(25, 25), crop_pct=0.961),\n\n    \'efficientnet_es\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_es_ra-f111e99c.pth\'),\n    \'efficientnet_em\': _cfg(\n        url=\'\', input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n    \'efficientnet_el\': _cfg(\n        url=\'\', input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n\n    \'efficientnet_cc_b0_4e\': _cfg(url=\'\'),\n    \'efficientnet_cc_b0_8e\': _cfg(url=\'\'),\n    \'efficientnet_cc_b1_8e\': _cfg(url=\'\', input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n\n    \'efficientnet_lite0\': _cfg(\n        url=\'\'),\n    \'efficientnet_lite1\': _cfg(\n        url=\'\',\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n    \'efficientnet_lite2\': _cfg(\n        url=\'\',\n        input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890),\n    \'efficientnet_lite3\': _cfg(\n        url=\'\',\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n    \'efficientnet_lite4\': _cfg(\n        url=\'\', input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.922),\n\n    \'efficientnet_b1_pruned\': _cfg(\n        url=\'https://imvl-automl-sh.oss-cn-shanghai.aliyuncs.com/darts/hyperml/hyperml/job_45403/outputs/effnetb1_pruned_9ebb3fe6.pth\',\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882, mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    \'efficientnet_b2_pruned\': _cfg(\n        url=\'https://imvl-automl-sh.oss-cn-shanghai.aliyuncs.com/darts/hyperml/hyperml/job_45403/outputs/effnetb2_pruned_203f55bc.pth\',\n        input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890, mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    \'efficientnet_b3_pruned\': _cfg(\n        url=\'https://imvl-automl-sh.oss-cn-shanghai.aliyuncs.com/darts/hyperml/hyperml/job_45403/outputs/effnetb3_pruned_5abcc29f.pth\',\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904, mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n\n    \'tf_efficientnet_b0\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_aa-827b6e33.pth\',\n        input_size=(3, 224, 224)),\n    \'tf_efficientnet_b1\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_aa-ea7a6ee0.pth\',\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n    \'tf_efficientnet_b2\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_aa-60c94f97.pth\',\n        input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890),\n    \'tf_efficientnet_b3\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3_aa-84b4657e.pth\',\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n    \'tf_efficientnet_b4\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_aa-818f208c.pth\',\n        input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.922),\n    \'tf_efficientnet_b5\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ra-9a3e5369.pth\',\n        input_size=(3, 456, 456), pool_size=(15, 15), crop_pct=0.934),\n    \'tf_efficientnet_b6\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_aa-80ba17e4.pth\',\n        input_size=(3, 528, 528), pool_size=(17, 17), crop_pct=0.942),\n    \'tf_efficientnet_b7\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ra-6c08e654.pth\',\n        input_size=(3, 600, 600), pool_size=(19, 19), crop_pct=0.949),\n    \'tf_efficientnet_b8\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b8_ra-572d5dd9.pth\',\n        input_size=(3, 672, 672), pool_size=(21, 21), crop_pct=0.954),\n\n    \'tf_efficientnet_b0_ap\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ap-f262efe1.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD, input_size=(3, 224, 224)),\n    \'tf_efficientnet_b1_ap\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_ap-44ef0a3d.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n    \'tf_efficientnet_b2_ap\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_ap-2f8e7636.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890),\n    \'tf_efficientnet_b3_ap\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3_ap-aad25bdd.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n    \'tf_efficientnet_b4_ap\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ap-dedb23e6.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.922),\n    \'tf_efficientnet_b5_ap\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ap-9e82fae8.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 456, 456), pool_size=(15, 15), crop_pct=0.934),\n    \'tf_efficientnet_b6_ap\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_ap-4ffb161f.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 528, 528), pool_size=(17, 17), crop_pct=0.942),\n    \'tf_efficientnet_b7_ap\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ap-ddb28fec.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 600, 600), pool_size=(19, 19), crop_pct=0.949),\n    \'tf_efficientnet_b8_ap\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b8_ap-00e169fa.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 672, 672), pool_size=(21, 21), crop_pct=0.954),\n\n    \'tf_efficientnet_b0_ns\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ns-c0e6a31c.pth\',\n        input_size=(3, 224, 224)),\n    \'tf_efficientnet_b1_ns\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b1_ns-99dd0c41.pth\',\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n    \'tf_efficientnet_b2_ns\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b2_ns-00306e48.pth\',\n        input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890),\n    \'tf_efficientnet_b3_ns\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b3_ns-9d44bf68.pth\',\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n    \'tf_efficientnet_b4_ns\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b4_ns-d6313a46.pth\',\n        input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.922),\n    \'tf_efficientnet_b5_ns\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b5_ns-6f26d0cf.pth\',\n        input_size=(3, 456, 456), pool_size=(15, 15), crop_pct=0.934),\n    \'tf_efficientnet_b6_ns\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b6_ns-51548356.pth\',\n        input_size=(3, 528, 528), pool_size=(17, 17), crop_pct=0.942),\n    \'tf_efficientnet_b7_ns\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b7_ns-1dbc32de.pth\',\n        input_size=(3, 600, 600), pool_size=(19, 19), crop_pct=0.949),\n    \'tf_efficientnet_l2_ns_475\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_l2_ns_475-bebbd00a.pth\',\n        input_size=(3, 475, 475), pool_size=(15, 15), crop_pct=0.936),\n    \'tf_efficientnet_l2_ns\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_l2_ns-df73bb44.pth\',\n        input_size=(3, 800, 800), pool_size=(25, 25), crop_pct=0.96),\n\n    \'tf_efficientnet_es\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_es-ca1afbfe.pth\',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 224, 224), ),\n    \'tf_efficientnet_em\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_em-e78cfe58.pth\',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n    \'tf_efficientnet_el\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_el-5143854e.pth\',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904),\n\n    \'tf_efficientnet_cc_b0_4e\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_cc_b0_4e-4362b6b2.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    \'tf_efficientnet_cc_b0_8e\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_cc_b0_8e-66184a25.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    \'tf_efficientnet_cc_b1_8e\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_cc_b1_8e-f7c79ae1.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD,\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882),\n\n    \'tf_efficientnet_lite0\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite0-0aa007d2.pth\',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        interpolation=\'bicubic\',  # should be bilinear but bicubic better match for TF bilinear at low res\n    ),\n    \'tf_efficientnet_lite1\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite1-bde8b488.pth\',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 240, 240), pool_size=(8, 8), crop_pct=0.882,\n        interpolation=\'bicubic\',  # should be bilinear but bicubic better match for TF bilinear at low res\n    ),\n    \'tf_efficientnet_lite2\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite2-dcccb7df.pth\',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 260, 260), pool_size=(9, 9), crop_pct=0.890,\n        interpolation=\'bicubic\',  # should be bilinear but bicubic better match for TF bilinear at low res\n    ),\n    \'tf_efficientnet_lite3\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite3-b733e338.pth\',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 300, 300), pool_size=(10, 10), crop_pct=0.904, interpolation=\'bilinear\'),\n    \'tf_efficientnet_lite4\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_lite4-741542c3.pth\',\n        mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5),\n        input_size=(3, 380, 380), pool_size=(12, 12), crop_pct=0.920, interpolation=\'bilinear\'),\n\n    \'mixnet_s\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mixnet_s-a907afbc.pth\'),\n    \'mixnet_m\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mixnet_m-4647fc68.pth\'),\n    \'mixnet_l\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mixnet_l-5a9a2ed8.pth\'),\n    \'mixnet_xl\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mixnet_xl_ra-aac3c00c.pth\'),\n    \'mixnet_xxl\': _cfg(),\n\n    \'tf_mixnet_s\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_s-89d3354b.pth\'),\n    \'tf_mixnet_m\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_m-0f4d8805.pth\'),\n    \'tf_mixnet_l\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mixnet_l-6c92e0c8.pth\'),\n}\n\n_DEBUG = False\n\n\nclass EfficientNet(nn.Module):\n    """""" (Generic) EfficientNet\n\n    A flexible and performant PyTorch implementation of efficient network architectures, including:\n      * EfficientNet B0-B8, L2\n      * EfficientNet-EdgeTPU\n      * EfficientNet-CondConv\n      * MixNet S, M, L, XL\n      * MnasNet A1, B1, and small\n      * FBNet C\n      * Single-Path NAS Pixel1\n\n    """"""\n\n    def __init__(self, block_args, num_classes=1000, num_features=1280, in_chans=3, stem_size=32,\n                 channel_multiplier=1.0, channel_divisor=8, channel_min=None,\n                 output_stride=32, pad_type=\'\', fix_stem=False, act_layer=nn.ReLU, drop_rate=0., drop_path_rate=0.,\n                 se_kwargs=None, norm_layer=nn.BatchNorm2d, norm_kwargs=None, global_pool=\'avg\'):\n        super(EfficientNet, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n\n        self.num_classes = num_classes\n        self.num_features = num_features\n        self.drop_rate = drop_rate\n        self._in_chs = in_chans\n\n        # Stem\n        if not fix_stem:\n            stem_size = round_channels(stem_size, channel_multiplier, channel_divisor, channel_min)\n        self.conv_stem = create_conv2d(self._in_chs, stem_size, 3, stride=2, padding=pad_type)\n        self.bn1 = norm_layer(stem_size, **norm_kwargs)\n        self.act1 = act_layer(inplace=True)\n        self._in_chs = stem_size\n\n        # Middle stages (IR/ER/DS Blocks)\n        builder = EfficientNetBuilder(\n            channel_multiplier, channel_divisor, channel_min, output_stride, pad_type, act_layer, se_kwargs,\n            norm_layer, norm_kwargs, drop_path_rate, verbose=_DEBUG)\n        self.blocks = nn.Sequential(*builder(self._in_chs, block_args))\n        self.feature_info = builder.features\n        self._in_chs = builder.in_chs\n\n        # Head + Pooling\n        self.conv_head = create_conv2d(self._in_chs, self.num_features, 1, padding=pad_type)\n        self.bn2 = norm_layer(self.num_features, **norm_kwargs)\n        self.act2 = act_layer(inplace=True)\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n\n        # Classifier\n        self.classifier = nn.Linear(self.num_features * self.global_pool.feat_mult(), self.num_classes)\n\n        efficientnet_init_weights(self)\n\n    def as_sequential(self):\n        layers = [self.conv_stem, self.bn1, self.act1]\n        layers.extend(self.blocks)\n        layers.extend([self.conv_head, self.bn2, self.act2, self.global_pool])\n        layers.extend([nn.Flatten(), nn.Dropout(self.drop_rate), self.classifier])\n        return nn.Sequential(*layers)\n\n    def get_classifier(self):\n        return self.classifier\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.num_classes = num_classes\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.classifier = nn.Linear(\n            self.num_features * self.global_pool.feat_mult(), num_classes) if num_classes else None\n\n    def forward_features(self, x):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.blocks(x)\n        x = self.conv_head(x)\n        x = self.bn2(x)\n        x = self.act2(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x)\n        x = x.flatten(1)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        return self.classifier(x)\n\n\nclass EfficientNetFeatures(nn.Module):\n    """""" EfficientNet Feature Extractor\n\n    A work-in-progress feature extraction module for EfficientNet, to use as a backbone for segmentation\n    and object detection models.\n    """"""\n\n    def __init__(self, block_args, out_indices=(0, 1, 2, 3, 4), feature_location=\'bottleneck\',\n                 in_chans=3, stem_size=32, channel_multiplier=1.0, channel_divisor=8, channel_min=None,\n                 output_stride=32, pad_type=\'\', fix_stem=False, act_layer=nn.ReLU, drop_rate=0., drop_path_rate=0.,\n                 se_kwargs=None, norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(EfficientNetFeatures, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n\n        # TODO only create stages needed, currently all stages are created regardless of out_indices\n        num_stages = max(out_indices) + 1\n\n        self.out_indices = out_indices\n        self.feature_location = feature_location\n        self.drop_rate = drop_rate\n        self._in_chs = in_chans\n\n        # Stem\n        if not fix_stem:\n            stem_size = round_channels(stem_size, channel_multiplier, channel_divisor, channel_min)\n        self.conv_stem = create_conv2d(self._in_chs, stem_size, 3, stride=2, padding=pad_type)\n        self.bn1 = norm_layer(stem_size, **norm_kwargs)\n        self.act1 = act_layer(inplace=True)\n        self._in_chs = stem_size\n\n        # Middle stages (IR/ER/DS Blocks)\n        builder = EfficientNetBuilder(\n            channel_multiplier, channel_divisor, channel_min, output_stride, pad_type, act_layer, se_kwargs,\n            norm_layer, norm_kwargs, drop_path_rate, feature_location=feature_location, verbose=_DEBUG)\n        self.blocks = nn.Sequential(*builder(self._in_chs, block_args))\n        self._feature_info = builder.features  # builder provides info about feature channels for each block\n        self._stage_to_feature_idx = {\n            v[\'stage_idx\']: fi for fi, v in self._feature_info.items() if fi in self.out_indices}\n        self._in_chs = builder.in_chs\n\n        efficientnet_init_weights(self)\n        if _DEBUG:\n            for k, v in self._feature_info.items():\n                print(\'Feature idx: {}: Name: {}, Channels: {}\'.format(k, v[\'name\'], v[\'num_chs\']))\n\n        # Register feature extraction hooks with FeatureHooks helper\n        self.feature_hooks = None\n        if feature_location != \'bottleneck\':\n            hooks = [dict(\n                name=self._feature_info[idx][\'module\'],\n                type=self._feature_info[idx][\'hook_type\']) for idx in out_indices]\n            self.feature_hooks = FeatureHooks(hooks, self.named_modules())\n\n    def feature_channels(self, idx=None):\n        """""" Feature Channel Shortcut\n        Returns feature channel count for each output index if idx == None. If idx is an integer, will\n        return feature channel count for that feature block index (independent of out_indices setting).\n        """"""\n        if isinstance(idx, int):\n            return self._feature_info[idx][\'num_chs\']\n        return [self._feature_info[i][\'num_chs\'] for i in self.out_indices]\n\n    def feature_info(self, idx=None):\n        """""" Feature Channel Shortcut\n        Returns feature channel count for each output index if idx == None. If idx is an integer, will\n        return feature channel count for that feature block index (independent of out_indices setting).\n        """"""\n        if isinstance(idx, int):\n            return self._feature_info[idx]\n        return [self._feature_info[i] for i in self.out_indices]\n\n    def forward(self, x):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        if self.feature_hooks is None:\n            features = []\n            for i, b in enumerate(self.blocks):\n                x = b(x)\n                if i in self._stage_to_feature_idx:\n                    features.append(x)\n            return features\n        else:\n            self.blocks(x)\n            return self.feature_hooks.get_output(x.device)\n\n\ndef _create_model(model_kwargs, default_cfg, pretrained=False):\n    if model_kwargs.pop(\'features_only\', False):\n        load_strict = False\n        model_kwargs.pop(\'num_classes\', 0)\n        model_kwargs.pop(\'num_features\', 0)\n        model_kwargs.pop(\'head_conv\', None)\n        model_class = EfficientNetFeatures\n    else:\n        load_strict = True\n        model_class = EfficientNet\n    variant = model_kwargs.pop(\'variant\', \'\')\n    model = model_class(**model_kwargs)\n    model.default_cfg = default_cfg\n    if \'_pruned\' in variant:\n        model = adapt_model_from_file(model, variant)\n    if pretrained:\n        load_pretrained(\n            model,\n            default_cfg,\n            num_classes=model_kwargs.get(\'num_classes\', 0),\n            in_chans=model_kwargs.get(\'in_chans\', 3),\n            strict=load_strict)\n    return model\n\n\ndef _gen_mnasnet_a1(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates a mnasnet-a1 model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet\n    Paper: https://arxiv.org/pdf/1807.11626.pdf.\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    """"""\n    arch_def = [\n        # stage 0, 112x112 in\n        [\'ds_r1_k3_s1_e1_c16_noskip\'],\n        # stage 1, 112x112 in\n        [\'ir_r2_k3_s2_e6_c24\'],\n        # stage 2, 56x56 in\n        [\'ir_r3_k5_s2_e3_c40_se0.25\'],\n        # stage 3, 28x28 in\n        [\'ir_r4_k3_s2_e6_c80\'],\n        # stage 4, 14x14in\n        [\'ir_r2_k3_s1_e6_c112_se0.25\'],\n        # stage 5, 14x14in\n        [\'ir_r3_k5_s2_e6_c160_se0.25\'],\n        # stage 6, 7x7 in\n        [\'ir_r1_k3_s1_e6_c320\'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs\n    )\n    model = _create_model(model_kwargs, default_cfgs[variant], pretrained)\n    return model\n\n\ndef _gen_mnasnet_b1(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates a mnasnet-b1 model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet\n    Paper: https://arxiv.org/pdf/1807.11626.pdf.\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    """"""\n    arch_def = [\n        # stage 0, 112x112 in\n        [\'ds_r1_k3_s1_c16_noskip\'],\n        # stage 1, 112x112 in\n        [\'ir_r3_k3_s2_e3_c24\'],\n        # stage 2, 56x56 in\n        [\'ir_r3_k5_s2_e3_c40\'],\n        # stage 3, 28x28 in\n        [\'ir_r3_k5_s2_e6_c80\'],\n        # stage 4, 14x14in\n        [\'ir_r2_k3_s1_e6_c96\'],\n        # stage 5, 14x14in\n        [\'ir_r4_k5_s2_e6_c192\'],\n        # stage 6, 7x7 in\n        [\'ir_r1_k3_s1_e6_c320_noskip\']\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs\n    )\n    model = _create_model(model_kwargs, default_cfgs[variant], pretrained)\n    return model\n\n\ndef _gen_mnasnet_small(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates a mnasnet-b1 model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet\n    Paper: https://arxiv.org/pdf/1807.11626.pdf.\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    """"""\n    arch_def = [\n        [\'ds_r1_k3_s1_c8\'],\n        [\'ir_r1_k3_s2_e3_c16\'],\n        [\'ir_r2_k3_s2_e6_c16\'],\n        [\'ir_r4_k5_s2_e6_c32_se0.25\'],\n        [\'ir_r3_k3_s1_e6_c32_se0.25\'],\n        [\'ir_r3_k5_s2_e6_c88_se0.25\'],\n        [\'ir_r1_k3_s1_e6_c144\']\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=8,\n        channel_multiplier=channel_multiplier,\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs\n    )\n    model = _create_model(model_kwargs, default_cfgs[variant], pretrained)\n    return model\n\n\ndef _gen_mobilenet_v2(\n        variant, channel_multiplier=1.0, depth_multiplier=1.0, fix_stem_head=False, pretrained=False, **kwargs):\n    """""" Generate MobileNet-V2 network\n    Ref impl: https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet_v2.py\n    Paper: https://arxiv.org/abs/1801.04381\n    """"""\n    arch_def = [\n        [\'ds_r1_k3_s1_c16\'],\n        [\'ir_r2_k3_s2_e6_c24\'],\n        [\'ir_r3_k3_s2_e6_c32\'],\n        [\'ir_r4_k3_s2_e6_c64\'],\n        [\'ir_r3_k3_s1_e6_c96\'],\n        [\'ir_r3_k3_s2_e6_c160\'],\n        [\'ir_r1_k3_s1_e6_c320\'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier=depth_multiplier, fix_first_last=fix_stem_head),\n        num_features=1280 if fix_stem_head else round_channels(1280, channel_multiplier, 8, None),\n        stem_size=32,\n        fix_stem=fix_stem_head,\n        channel_multiplier=channel_multiplier,\n        norm_kwargs=resolve_bn_args(kwargs),\n        act_layer=nn.ReLU6,\n        **kwargs\n    )\n    model = _create_model(model_kwargs, default_cfgs[variant], pretrained)\n    return model\n\n\ndef _gen_fbnetc(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """""" FBNet-C\n\n        Paper: https://arxiv.org/abs/1812.03443\n        Ref Impl: https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/modeling/backbone/fbnet_modeldef.py\n\n        NOTE: the impl above does not relate to the \'C\' variant here, that was derived from paper,\n        it was used to confirm some building block details\n    """"""\n    arch_def = [\n        [\'ir_r1_k3_s1_e1_c16\'],\n        [\'ir_r1_k3_s2_e6_c24\', \'ir_r2_k3_s1_e1_c24\'],\n        [\'ir_r1_k5_s2_e6_c32\', \'ir_r1_k5_s1_e3_c32\', \'ir_r1_k5_s1_e6_c32\', \'ir_r1_k3_s1_e6_c32\'],\n        [\'ir_r1_k5_s2_e6_c64\', \'ir_r1_k5_s1_e3_c64\', \'ir_r2_k5_s1_e6_c64\'],\n        [\'ir_r3_k5_s1_e6_c112\', \'ir_r1_k5_s1_e3_c112\'],\n        [\'ir_r4_k5_s2_e6_c184\'],\n        [\'ir_r1_k3_s1_e6_c352\'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=16,\n        num_features=1984,  # paper suggests this, but is not 100% clear\n        channel_multiplier=channel_multiplier,\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs\n    )\n    model = _create_model(model_kwargs, default_cfgs[variant], pretrained)\n    return model\n\n\ndef _gen_spnasnet(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates the Single-Path NAS model from search targeted for Pixel1 phone.\n\n    Paper: https://arxiv.org/abs/1904.02877\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    """"""\n    arch_def = [\n        # stage 0, 112x112 in\n        [\'ds_r1_k3_s1_c16_noskip\'],\n        # stage 1, 112x112 in\n        [\'ir_r3_k3_s2_e3_c24\'],\n        # stage 2, 56x56 in\n        [\'ir_r1_k5_s2_e6_c40\', \'ir_r3_k3_s1_e3_c40\'],\n        # stage 3, 28x28 in\n        [\'ir_r1_k5_s2_e6_c80\', \'ir_r3_k3_s1_e3_c80\'],\n        # stage 4, 14x14in\n        [\'ir_r1_k5_s1_e6_c96\', \'ir_r3_k5_s1_e3_c96\'],\n        # stage 5, 14x14in\n        [\'ir_r4_k5_s2_e6_c192\'],\n        # stage 6, 7x7 in\n        [\'ir_r1_k3_s1_e6_c320_noskip\']\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs\n    )\n    model = _create_model(model_kwargs, default_cfgs[variant], pretrained)\n    return model\n\n\ndef _gen_efficientnet(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates an EfficientNet model.\n\n    Ref impl: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py\n    Paper: https://arxiv.org/abs/1905.11946\n\n    EfficientNet params\n    name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)\n    \'efficientnet-b0\': (1.0, 1.0, 224, 0.2),\n    \'efficientnet-b1\': (1.0, 1.1, 240, 0.2),\n    \'efficientnet-b2\': (1.1, 1.2, 260, 0.3),\n    \'efficientnet-b3\': (1.2, 1.4, 300, 0.3),\n    \'efficientnet-b4\': (1.4, 1.8, 380, 0.4),\n    \'efficientnet-b5\': (1.6, 2.2, 456, 0.4),\n    \'efficientnet-b6\': (1.8, 2.6, 528, 0.5),\n    \'efficientnet-b7\': (2.0, 3.1, 600, 0.5),\n    \'efficientnet-b8\': (2.2, 3.6, 672, 0.5),\n    \'efficientnet-l2\': (4.3, 5.3, 800, 0.5),\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer\n      depth_multiplier: multiplier to number of repeats per stage\n\n    """"""\n    arch_def = [\n        [\'ds_r1_k3_s1_e1_c16_se0.25\'],\n        [\'ir_r2_k3_s2_e6_c24_se0.25\'],\n        [\'ir_r2_k5_s2_e6_c40_se0.25\'],\n        [\'ir_r3_k3_s2_e6_c80_se0.25\'],\n        [\'ir_r3_k5_s1_e6_c112_se0.25\'],\n        [\'ir_r4_k5_s2_e6_c192_se0.25\'],\n        [\'ir_r1_k3_s1_e6_c320_se0.25\'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier),\n        num_features=round_channels(1280, channel_multiplier, 8, None),\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        act_layer=Swish,\n        norm_kwargs=resolve_bn_args(kwargs),\n        variant=variant,\n        **kwargs,\n    )\n    model = _create_model(model_kwargs, default_cfgs[variant], pretrained)\n    return model\n\n\ndef _gen_efficientnet_edge(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):\n    """""" Creates an EfficientNet-EdgeTPU model\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/edgetpu\n    """"""\n\n    arch_def = [\n        # NOTE `fc` is present to override a mismatch between stem channels and in chs not\n        # present in other models\n        [\'er_r1_k3_s1_e4_c24_fc24_noskip\'],\n        [\'er_r2_k3_s2_e8_c32\'],\n        [\'er_r4_k3_s2_e8_c48\'],\n        [\'ir_r5_k5_s2_e8_c96\'],\n        [\'ir_r4_k5_s1_e8_c144\'],\n        [\'ir_r2_k5_s2_e8_c192\'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier),\n        num_features=round_channels(1280, channel_multiplier, 8, None),\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        norm_kwargs=resolve_bn_args(kwargs),\n        act_layer=nn.ReLU,\n        **kwargs,\n    )\n    model = _create_model(model_kwargs, default_cfgs[variant], pretrained)\n    return model\n\n\ndef _gen_efficientnet_condconv(\n        variant, channel_multiplier=1.0, depth_multiplier=1.0, experts_multiplier=1, pretrained=False, **kwargs):\n    """"""Creates an EfficientNet-CondConv model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv\n    """"""\n    arch_def = [\n      [\'ds_r1_k3_s1_e1_c16_se0.25\'],\n      [\'ir_r2_k3_s2_e6_c24_se0.25\'],\n      [\'ir_r2_k5_s2_e6_c40_se0.25\'],\n      [\'ir_r3_k3_s2_e6_c80_se0.25\'],\n      [\'ir_r3_k5_s1_e6_c112_se0.25_cc4\'],\n      [\'ir_r4_k5_s2_e6_c192_se0.25_cc4\'],\n      [\'ir_r1_k3_s1_e6_c320_se0.25_cc4\'],\n    ]\n    # NOTE unlike official impl, this one uses `cc<x>` option where x is the base number of experts for each stage and\n    # the expert_multiplier increases that on a per-model basis as with depth/channel multipliers\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier, experts_multiplier=experts_multiplier),\n        num_features=round_channels(1280, channel_multiplier, 8, None),\n        stem_size=32,\n        channel_multiplier=channel_multiplier,\n        norm_kwargs=resolve_bn_args(kwargs),\n        act_layer=Swish,\n        **kwargs,\n    )\n    model = _create_model(model_kwargs, default_cfgs[variant], pretrained)\n    return model\n\n\ndef _gen_efficientnet_lite(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates an EfficientNet-Lite model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/lite\n    Paper: https://arxiv.org/abs/1905.11946\n\n    EfficientNet params\n    name: (channel_multiplier, depth_multiplier, resolution, dropout_rate)\n      \'efficientnet-lite0\': (1.0, 1.0, 224, 0.2),\n      \'efficientnet-lite1\': (1.0, 1.1, 240, 0.2),\n      \'efficientnet-lite2\': (1.1, 1.2, 260, 0.3),\n      \'efficientnet-lite3\': (1.2, 1.4, 280, 0.3),\n      \'efficientnet-lite4\': (1.4, 1.8, 300, 0.3),\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer\n      depth_multiplier: multiplier to number of repeats per stage\n    """"""\n    arch_def = [\n        [\'ds_r1_k3_s1_e1_c16\'],\n        [\'ir_r2_k3_s2_e6_c24\'],\n        [\'ir_r2_k5_s2_e6_c40\'],\n        [\'ir_r3_k3_s2_e6_c80\'],\n        [\'ir_r3_k5_s1_e6_c112\'],\n        [\'ir_r4_k5_s2_e6_c192\'],\n        [\'ir_r1_k3_s1_e6_c320\'],\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier, fix_first_last=True),\n        num_features=1280,\n        stem_size=32,\n        fix_stem=True,\n        channel_multiplier=channel_multiplier,\n        act_layer=nn.ReLU6,\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs,\n    )\n    model = _create_model(model_kwargs, default_cfgs[variant], pretrained)\n    return model\n\n\ndef _gen_mixnet_s(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates a MixNet Small model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet\n    Paper: https://arxiv.org/abs/1907.09595\n    """"""\n    arch_def = [\n        # stage 0, 112x112 in\n        [\'ds_r1_k3_s1_e1_c16\'],  # relu\n        # stage 1, 112x112 in\n        [\'ir_r1_k3_a1.1_p1.1_s2_e6_c24\', \'ir_r1_k3_a1.1_p1.1_s1_e3_c24\'],  # relu\n        # stage 2, 56x56 in\n        [\'ir_r1_k3.5.7_s2_e6_c40_se0.5_nsw\', \'ir_r3_k3.5_a1.1_p1.1_s1_e6_c40_se0.5_nsw\'],  # swish\n        # stage 3, 28x28 in\n        [\'ir_r1_k3.5.7_p1.1_s2_e6_c80_se0.25_nsw\', \'ir_r2_k3.5_p1.1_s1_e6_c80_se0.25_nsw\'],  # swish\n        # stage 4, 14x14in\n        [\'ir_r1_k3.5.7_a1.1_p1.1_s1_e6_c120_se0.5_nsw\', \'ir_r2_k3.5.7.9_a1.1_p1.1_s1_e3_c120_se0.5_nsw\'],  # swish\n        # stage 5, 14x14in\n        [\'ir_r1_k3.5.7.9.11_s2_e6_c200_se0.5_nsw\', \'ir_r2_k3.5.7.9_p1.1_s1_e6_c200_se0.5_nsw\'],  # swish\n        # 7x7\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        num_features=1536,\n        stem_size=16,\n        channel_multiplier=channel_multiplier,\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs\n    )\n    model = _create_model(model_kwargs, default_cfgs[variant], pretrained)\n    return model\n\n\ndef _gen_mixnet_m(variant, channel_multiplier=1.0, depth_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates a MixNet Medium-Large model.\n\n    Ref impl: https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet/mixnet\n    Paper: https://arxiv.org/abs/1907.09595\n    """"""\n    arch_def = [\n        # stage 0, 112x112 in\n        [\'ds_r1_k3_s1_e1_c24\'],  # relu\n        # stage 1, 112x112 in\n        [\'ir_r1_k3.5.7_a1.1_p1.1_s2_e6_c32\', \'ir_r1_k3_a1.1_p1.1_s1_e3_c32\'],  # relu\n        # stage 2, 56x56 in\n        [\'ir_r1_k3.5.7.9_s2_e6_c40_se0.5_nsw\', \'ir_r3_k3.5_a1.1_p1.1_s1_e6_c40_se0.5_nsw\'],  # swish\n        # stage 3, 28x28 in\n        [\'ir_r1_k3.5.7_s2_e6_c80_se0.25_nsw\', \'ir_r3_k3.5.7.9_a1.1_p1.1_s1_e6_c80_se0.25_nsw\'],  # swish\n        # stage 4, 14x14in\n        [\'ir_r1_k3_s1_e6_c120_se0.5_nsw\', \'ir_r3_k3.5.7.9_a1.1_p1.1_s1_e3_c120_se0.5_nsw\'],  # swish\n        # stage 5, 14x14in\n        [\'ir_r1_k3.5.7.9_s2_e6_c200_se0.5_nsw\', \'ir_r3_k3.5.7.9_p1.1_s1_e6_c200_se0.5_nsw\'],  # swish\n        # 7x7\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def, depth_multiplier, depth_trunc=\'round\'),\n        num_features=1536,\n        stem_size=24,\n        channel_multiplier=channel_multiplier,\n        norm_kwargs=resolve_bn_args(kwargs),\n        **kwargs\n    )\n    model = _create_model(model_kwargs, default_cfgs[variant], pretrained)\n    return model\n\n\n@register_model\ndef mnasnet_050(pretrained=False, **kwargs):\n    """""" MNASNet B1, depth multiplier of 0.5. """"""\n    model = _gen_mnasnet_b1(\'mnasnet_050\', 0.5, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mnasnet_075(pretrained=False, **kwargs):\n    """""" MNASNet B1, depth multiplier of 0.75. """"""\n    model = _gen_mnasnet_b1(\'mnasnet_075\', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mnasnet_100(pretrained=False, **kwargs):\n    """""" MNASNet B1, depth multiplier of 1.0. """"""\n    model = _gen_mnasnet_b1(\'mnasnet_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mnasnet_b1(pretrained=False, **kwargs):\n    """""" MNASNet B1, depth multiplier of 1.0. """"""\n    return mnasnet_100(pretrained, **kwargs)\n\n\n@register_model\ndef mnasnet_140(pretrained=False, **kwargs):\n    """""" MNASNet B1,  depth multiplier of 1.4 """"""\n    model = _gen_mnasnet_b1(\'mnasnet_140\', 1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef semnasnet_050(pretrained=False, **kwargs):\n    """""" MNASNet A1 (w/ SE), depth multiplier of 0.5 """"""\n    model = _gen_mnasnet_a1(\'semnasnet_050\', 0.5, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef semnasnet_075(pretrained=False, **kwargs):\n    """""" MNASNet A1 (w/ SE),  depth multiplier of 0.75. """"""\n    model = _gen_mnasnet_a1(\'semnasnet_075\', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef semnasnet_100(pretrained=False, **kwargs):\n    """""" MNASNet A1 (w/ SE), depth multiplier of 1.0. """"""\n    model = _gen_mnasnet_a1(\'semnasnet_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mnasnet_a1(pretrained=False, **kwargs):\n    """""" MNASNet A1 (w/ SE), depth multiplier of 1.0. """"""\n    return semnasnet_100(pretrained, **kwargs)\n\n\n@register_model\ndef semnasnet_140(pretrained=False, **kwargs):\n    """""" MNASNet A1 (w/ SE), depth multiplier of 1.4. """"""\n    model = _gen_mnasnet_a1(\'semnasnet_140\', 1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mnasnet_small(pretrained=False, **kwargs):\n    """""" MNASNet Small,  depth multiplier of 1.0. """"""\n    model = _gen_mnasnet_small(\'mnasnet_small\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv2_100(pretrained=False, **kwargs):\n    """""" MobileNet V2 w/ 1.0 channel multiplier """"""\n    model = _gen_mobilenet_v2(\'mobilenetv2_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv2_140(pretrained=False, **kwargs):\n    """""" MobileNet V2 w/ 1.4 channel multiplier """"""\n    model = _gen_mobilenet_v2(\'mobilenetv2_140\', 1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv2_110d(pretrained=False, **kwargs):\n    """""" MobileNet V2 w/ 1.1 channel, 1.2 depth multipliers""""""\n    model = _gen_mobilenet_v2(\n        \'mobilenetv2_110d\', 1.1, depth_multiplier=1.2, fix_stem_head=True, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv2_120d(pretrained=False, **kwargs):\n    """""" MobileNet V2 w/ 1.2 channel, 1.4 depth multipliers """"""\n    model = _gen_mobilenet_v2(\n        \'mobilenetv2_120d\', 1.2, depth_multiplier=1.4, fix_stem_head=True, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef fbnetc_100(pretrained=False, **kwargs):\n    """""" FBNet-C """"""\n    if pretrained:\n        # pretrained model trained with non-default BN epsilon\n        kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    model = _gen_fbnetc(\'fbnetc_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef spnasnet_100(pretrained=False, **kwargs):\n    """""" Single-Path NAS Pixel1""""""\n    model = _gen_spnasnet(\'spnasnet_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b0(pretrained=False, **kwargs):\n    """""" EfficientNet-B0 """"""\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b0\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b1(pretrained=False, **kwargs):\n    """""" EfficientNet-B1 """"""\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b1\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b2(pretrained=False, **kwargs):\n    """""" EfficientNet-B2 """"""\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b2\', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b2a(pretrained=False, **kwargs):\n    """""" EfficientNet-B2 @ 288x288 w/ 1.0 test crop""""""\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b2a\', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b3(pretrained=False, **kwargs):\n    """""" EfficientNet-B3 """"""\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b3\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b3a(pretrained=False, **kwargs):\n    """""" EfficientNet-B3 @ 320x320 w/ 1.0 test crop-pct """"""\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b3a\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b4(pretrained=False, **kwargs):\n    """""" EfficientNet-B4 """"""\n    # NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b4\', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b5(pretrained=False, **kwargs):\n    """""" EfficientNet-B5 """"""\n    # NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b5\', channel_multiplier=1.6, depth_multiplier=2.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b6(pretrained=False, **kwargs):\n    """""" EfficientNet-B6 """"""\n    # NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b6\', channel_multiplier=1.8, depth_multiplier=2.6, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b7(pretrained=False, **kwargs):\n    """""" EfficientNet-B7 """"""\n    # NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b7\', channel_multiplier=2.0, depth_multiplier=3.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b8(pretrained=False, **kwargs):\n    """""" EfficientNet-B8 """"""\n    # NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        \'efficientnet_b8\', channel_multiplier=2.2, depth_multiplier=3.6, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_l2(pretrained=False, **kwargs):\n    """""" EfficientNet-L2.""""""\n    # NOTE for train, drop_rate should be 0.5, drop_path_rate should be 0.2\n    model = _gen_efficientnet(\n        \'efficientnet_l2\', channel_multiplier=4.3, depth_multiplier=5.3, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_es(pretrained=False, **kwargs):\n    """""" EfficientNet-Edge Small. """"""\n    model = _gen_efficientnet_edge(\n        \'efficientnet_es\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_em(pretrained=False, **kwargs):\n    """""" EfficientNet-Edge-Medium. """"""\n    model = _gen_efficientnet_edge(\n        \'efficientnet_em\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_el(pretrained=False, **kwargs):\n    """""" EfficientNet-Edge-Large. """"""\n    model = _gen_efficientnet_edge(\n        \'efficientnet_el\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_cc_b0_4e(pretrained=False, **kwargs):\n    """""" EfficientNet-CondConv-B0 w/ 8 Experts """"""\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    model = _gen_efficientnet_condconv(\n        \'efficientnet_cc_b0_4e\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_cc_b0_8e(pretrained=False, **kwargs):\n    """""" EfficientNet-CondConv-B0 w/ 8 Experts """"""\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    model = _gen_efficientnet_condconv(\n        \'efficientnet_cc_b0_8e\', channel_multiplier=1.0, depth_multiplier=1.0, experts_multiplier=2,\n        pretrained=pretrained, **kwargs)\n    return model\n\n@register_model\ndef efficientnet_cc_b1_8e(pretrained=False, **kwargs):\n    """""" EfficientNet-CondConv-B1 w/ 8 Experts """"""\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    model = _gen_efficientnet_condconv(\n        \'efficientnet_cc_b1_8e\', channel_multiplier=1.0, depth_multiplier=1.1, experts_multiplier=2,\n        pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_lite0(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite0 """"""\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    model = _gen_efficientnet_lite(\n        \'efficientnet_lite0\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_lite1(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite1 """"""\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    model = _gen_efficientnet_lite(\n        \'efficientnet_lite1\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_lite2(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite2 """"""\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    model = _gen_efficientnet_lite(\n        \'efficientnet_lite2\', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_lite3(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite3 """"""\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    model = _gen_efficientnet_lite(\n        \'efficientnet_lite3\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_lite4(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite4 """"""\n    # NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2\n    model = _gen_efficientnet_lite(\n        \'efficientnet_lite4\', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\n\n\n@register_model\ndef efficientnet_b1_pruned(pretrained=False, **kwargs):\n    """""" EfficientNet-B1 Pruned. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    variant = \'efficientnet_b1_pruned\'\n    model = _gen_efficientnet(\n        variant, channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b2_pruned(pretrained=False, **kwargs):\n    """""" EfficientNet-B2 Pruned. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'efficientnet_b2_pruned\', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef efficientnet_b3_pruned(pretrained=False, **kwargs):\n    """""" EfficientNet-B3 Pruned. The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'efficientnet_b3_pruned\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n\n\n@register_model\ndef tf_efficientnet_b0(pretrained=False, **kwargs):\n    """""" EfficientNet-B0. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b0\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b1(pretrained=False, **kwargs):\n    """""" EfficientNet-B1. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b1\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b2(pretrained=False, **kwargs):\n    """""" EfficientNet-B2. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b2\', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b3(pretrained=False, **kwargs):\n    """""" EfficientNet-B3. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b3\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b4(pretrained=False, **kwargs):\n    """""" EfficientNet-B4. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b4\', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b5(pretrained=False, **kwargs):\n    """""" EfficientNet-B5. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b5\', channel_multiplier=1.6, depth_multiplier=2.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b6(pretrained=False, **kwargs):\n    """""" EfficientNet-B6. Tensorflow compatible variant """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b6\', channel_multiplier=1.8, depth_multiplier=2.6, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b7(pretrained=False, **kwargs):\n    """""" EfficientNet-B7. Tensorflow compatible variant """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b7\', channel_multiplier=2.0, depth_multiplier=3.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b8(pretrained=False, **kwargs):\n    """""" EfficientNet-B8. Tensorflow compatible variant """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b8\', channel_multiplier=2.2, depth_multiplier=3.6, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b0_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B0 AdvProp. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b0_ap\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b1_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B1 AdvProp. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b1_ap\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b2_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B2 AdvProp. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b2_ap\', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b3_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B3 AdvProp. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b3_ap\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b4_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B4 AdvProp. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b4_ap\', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b5_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B5 AdvProp. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b5_ap\', channel_multiplier=1.6, depth_multiplier=2.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b6_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B6 AdvProp. Tensorflow compatible variant """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b6_ap\', channel_multiplier=1.8, depth_multiplier=2.6, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b7_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B7 AdvProp. Tensorflow compatible variant """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b7_ap\', channel_multiplier=2.0, depth_multiplier=3.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b8_ap(pretrained=False, **kwargs):\n    """""" EfficientNet-B8 AdvProp. Tensorflow compatible variant """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b8_ap\', channel_multiplier=2.2, depth_multiplier=3.6, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b0_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B0 NoisyStudent. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b0_ns\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b1_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B1 NoisyStudent. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b1_ns\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b2_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B2 NoisyStudent. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b2_ns\', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b3_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B3 NoisyStudent. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b3_ns\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b4_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B4 NoisyStudent. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b4_ns\', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b5_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B5 NoisyStudent. Tensorflow compatible variant """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b5_ns\', channel_multiplier=1.6, depth_multiplier=2.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b6_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B6 NoisyStudent. Tensorflow compatible variant """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b6_ns\', channel_multiplier=1.8, depth_multiplier=2.6, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_b7_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-B7 NoisyStudent. Tensorflow compatible variant """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_b7_ns\', channel_multiplier=2.0, depth_multiplier=3.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_l2_ns_475(pretrained=False, **kwargs):\n    """""" EfficientNet-L2 NoisyStudent @ 475x475. Tensorflow compatible variant """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_l2_ns_475\', channel_multiplier=4.3, depth_multiplier=5.3, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_l2_ns(pretrained=False, **kwargs):\n    """""" EfficientNet-L2 NoisyStudent. Tensorflow compatible variant """"""\n    # NOTE for train, drop_rate should be 0.5\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet(\n        \'tf_efficientnet_l2_ns\', channel_multiplier=4.3, depth_multiplier=5.3, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_es(pretrained=False, **kwargs):\n    """""" EfficientNet-Edge Small. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_edge(\n        \'tf_efficientnet_es\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_em(pretrained=False, **kwargs):\n    """""" EfficientNet-Edge-Medium. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_edge(\n        \'tf_efficientnet_em\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_el(pretrained=False, **kwargs):\n    """""" EfficientNet-Edge-Large. Tensorflow compatible variant  """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_edge(\n        \'tf_efficientnet_el\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_cc_b0_4e(pretrained=False, **kwargs):\n    """""" EfficientNet-CondConv-B0 w/ 4 Experts. Tensorflow compatible variant """"""\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_condconv(\n        \'tf_efficientnet_cc_b0_4e\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_cc_b0_8e(pretrained=False, **kwargs):\n    """""" EfficientNet-CondConv-B0 w/ 8 Experts. Tensorflow compatible variant """"""\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_condconv(\n        \'tf_efficientnet_cc_b0_8e\', channel_multiplier=1.0, depth_multiplier=1.0, experts_multiplier=2,\n        pretrained=pretrained, **kwargs)\n    return model\n\n@register_model\ndef tf_efficientnet_cc_b1_8e(pretrained=False, **kwargs):\n    """""" EfficientNet-CondConv-B1 w/ 8 Experts. Tensorflow compatible variant """"""\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_condconv(\n        \'tf_efficientnet_cc_b1_8e\', channel_multiplier=1.0, depth_multiplier=1.1, experts_multiplier=2,\n        pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_lite0(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite0 """"""\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_lite(\n        \'tf_efficientnet_lite0\', channel_multiplier=1.0, depth_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_lite1(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite1 """"""\n    # NOTE for train, drop_rate should be 0.2, drop_path_rate should be 0.2\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_lite(\n        \'tf_efficientnet_lite1\', channel_multiplier=1.0, depth_multiplier=1.1, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_lite2(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite2 """"""\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_lite(\n        \'tf_efficientnet_lite2\', channel_multiplier=1.1, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_lite3(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite3 """"""\n    # NOTE for train, drop_rate should be 0.3, drop_path_rate should be 0.2\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_lite(\n        \'tf_efficientnet_lite3\', channel_multiplier=1.2, depth_multiplier=1.4, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_efficientnet_lite4(pretrained=False, **kwargs):\n    """""" EfficientNet-Lite4 """"""\n    # NOTE for train, drop_rate should be 0.4, drop_path_rate should be 0.2\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_efficientnet_lite(\n        \'tf_efficientnet_lite4\', channel_multiplier=1.4, depth_multiplier=1.8, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mixnet_s(pretrained=False, **kwargs):\n    """"""Creates a MixNet Small model.\n    """"""\n    model = _gen_mixnet_s(\n        \'mixnet_s\', channel_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mixnet_m(pretrained=False, **kwargs):\n    """"""Creates a MixNet Medium model.\n    """"""\n    model = _gen_mixnet_m(\n        \'mixnet_m\', channel_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mixnet_l(pretrained=False, **kwargs):\n    """"""Creates a MixNet Large model.\n    """"""\n    model = _gen_mixnet_m(\n        \'mixnet_l\', channel_multiplier=1.3, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mixnet_xl(pretrained=False, **kwargs):\n    """"""Creates a MixNet Extra-Large model.\n    Not a paper spec, experimental def by RW w/ depth scaling.\n    """"""\n    model = _gen_mixnet_m(\n        \'mixnet_xl\', channel_multiplier=1.6, depth_multiplier=1.2, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mixnet_xxl(pretrained=False, **kwargs):\n    """"""Creates a MixNet Double Extra Large model.\n    Not a paper spec, experimental def by RW w/ depth scaling.\n    """"""\n    model = _gen_mixnet_m(\n        \'mixnet_xxl\', channel_multiplier=2.4, depth_multiplier=1.3, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mixnet_s(pretrained=False, **kwargs):\n    """"""Creates a MixNet Small model. Tensorflow compatible variant\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mixnet_s(\n        \'tf_mixnet_s\', channel_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mixnet_m(pretrained=False, **kwargs):\n    """"""Creates a MixNet Medium model. Tensorflow compatible variant\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mixnet_m(\n        \'tf_mixnet_m\', channel_multiplier=1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mixnet_l(pretrained=False, **kwargs):\n    """"""Creates a MixNet Large model. Tensorflow compatible variant\n    """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mixnet_m(\n        \'tf_mixnet_l\', channel_multiplier=1.3, pretrained=pretrained, **kwargs)\n    return model\n\n'"
timm/models/efficientnet_blocks.py,3,"b'import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom .layers.activations import sigmoid\nfrom .layers import create_conv2d, drop_path\n\n\n# Defaults used for Google/Tensorflow training of mobile networks /w RMSprop as per\n# papers and TF reference implementations. PT momentum equiv for TF decay is (1 - TF decay)\n# NOTE: momentum varies btw .99 and .9997 depending on source\n# .99 in official TF TPU impl\n# .9997 (/w .999 in search space) for paper\nBN_MOMENTUM_TF_DEFAULT = 1 - 0.99\nBN_EPS_TF_DEFAULT = 1e-3\n_BN_ARGS_TF = dict(momentum=BN_MOMENTUM_TF_DEFAULT, eps=BN_EPS_TF_DEFAULT)\n\n\ndef get_bn_args_tf():\n    return _BN_ARGS_TF.copy()\n\n\ndef resolve_bn_args(kwargs):\n    bn_args = get_bn_args_tf() if kwargs.pop(\'bn_tf\', False) else {}\n    bn_momentum = kwargs.pop(\'bn_momentum\', None)\n    if bn_momentum is not None:\n        bn_args[\'momentum\'] = bn_momentum\n    bn_eps = kwargs.pop(\'bn_eps\', None)\n    if bn_eps is not None:\n        bn_args[\'eps\'] = bn_eps\n    return bn_args\n\n\n_SE_ARGS_DEFAULT = dict(\n    gate_fn=sigmoid,\n    act_layer=None,\n    reduce_mid=False,\n    divisor=1)\n\n\ndef resolve_se_args(kwargs, in_chs, act_layer=None):\n    se_kwargs = kwargs.copy() if kwargs is not None else {}\n    # fill in args that aren\'t specified with the defaults\n    for k, v in _SE_ARGS_DEFAULT.items():\n        se_kwargs.setdefault(k, v)\n    # some models, like MobilNetV3, calculate SE reduction chs from the containing block\'s mid_ch instead of in_ch\n    if not se_kwargs.pop(\'reduce_mid\'):\n        se_kwargs[\'reduced_base_chs\'] = in_chs\n    # act_layer override, if it remains None, the containing block\'s act_layer will be used\n    if se_kwargs[\'act_layer\'] is None:\n        assert act_layer is not None\n        se_kwargs[\'act_layer\'] = act_layer\n    return se_kwargs\n\n\ndef make_divisible(v, divisor=8, min_value=None):\n    min_value = min_value or divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\ndef round_channels(channels, multiplier=1.0, divisor=8, channel_min=None):\n    """"""Round number of filters based on depth multiplier.""""""\n    if not multiplier:\n        return channels\n    channels *= multiplier\n    return make_divisible(channels, divisor, channel_min)\n\n\nclass ChannelShuffle(nn.Module):\n    # FIXME haven\'t used yet\n    def __init__(self, groups):\n        super(ChannelShuffle, self).__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        """"""Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]""""""\n        N, C, H, W = x.size()\n        g = self.groups\n        assert C % g == 0, ""Incompatible group size {} for input channel {}"".format(\n            g, C\n        )\n        return (\n            x.view(N, g, int(C / g), H, W)\n            .permute(0, 2, 1, 3, 4)\n            .contiguous()\n            .view(N, C, H, W)\n        )\n\n\nclass SqueezeExcite(nn.Module):\n    def __init__(self, in_chs, se_ratio=0.25, reduced_base_chs=None,\n                 act_layer=nn.ReLU, gate_fn=sigmoid, divisor=1, **_):\n        super(SqueezeExcite, self).__init__()\n        self.gate_fn = gate_fn\n        reduced_chs = make_divisible((reduced_base_chs or in_chs) * se_ratio, divisor)\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv_reduce = nn.Conv2d(in_chs, reduced_chs, 1, bias=True)\n        self.act1 = act_layer(inplace=True)\n        self.conv_expand = nn.Conv2d(reduced_chs, in_chs, 1, bias=True)\n\n    def forward(self, x):\n        x_se = self.avg_pool(x)\n        x_se = self.conv_reduce(x_se)\n        x_se = self.act1(x_se)\n        x_se = self.conv_expand(x_se)\n        x = x * self.gate_fn(x_se)\n        return x\n\n\nclass ConvBnAct(nn.Module):\n    def __init__(self, in_chs, out_chs, kernel_size,\n                 stride=1, dilation=1, pad_type=\'\', act_layer=nn.ReLU,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(ConvBnAct, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n        self.conv = create_conv2d(in_chs, out_chs, kernel_size, stride=stride, dilation=dilation, padding=pad_type)\n        self.bn1 = norm_layer(out_chs, **norm_kwargs)\n        self.act1 = act_layer(inplace=True)\n\n    def feature_info(self, location):\n        if location == \'expansion\' or location == \'depthwise\':\n            # no expansion or depthwise this block, use act after conv\n            info = dict(module=\'act1\', hook_type=\'forward\', num_chs=self.conv.out_channels)\n        else:  # location == \'bottleneck\'\n            info = dict(module=\'\', hook_type=\'\', num_chs=self.conv.out_channels)\n        return info\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        return x\n\n\nclass DepthwiseSeparableConv(nn.Module):\n    """""" DepthwiseSeparable block\n    Used for DS convs in MobileNet-V1 and in the place of IR blocks that have no expansion\n    (factor of 1.0). This is an alternative to having a IR with an optional first pw conv.\n    """"""\n    def __init__(self, in_chs, out_chs, dw_kernel_size=3,\n                 stride=1, dilation=1, pad_type=\'\', act_layer=nn.ReLU, noskip=False,\n                 pw_kernel_size=1, pw_act=False, se_ratio=0., se_kwargs=None,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None, drop_path_rate=0.):\n        super(DepthwiseSeparableConv, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n        has_se = se_ratio is not None and se_ratio > 0.\n        self.has_residual = (stride == 1 and in_chs == out_chs) and not noskip\n        self.has_pw_act = pw_act  # activation after point-wise conv\n        self.drop_path_rate = drop_path_rate\n\n        self.conv_dw = create_conv2d(\n            in_chs, in_chs, dw_kernel_size, stride=stride, dilation=dilation, padding=pad_type, depthwise=True)\n        self.bn1 = norm_layer(in_chs, **norm_kwargs)\n        self.act1 = act_layer(inplace=True)\n\n        # Squeeze-and-excitation\n        if has_se:\n            se_kwargs = resolve_se_args(se_kwargs, in_chs, act_layer)\n            self.se = SqueezeExcite(in_chs, se_ratio=se_ratio, **se_kwargs)\n        else:\n            self.se = None\n\n        self.conv_pw = create_conv2d(in_chs, out_chs, pw_kernel_size, padding=pad_type)\n        self.bn2 = norm_layer(out_chs, **norm_kwargs)\n        self.act2 = act_layer(inplace=True) if self.has_pw_act else nn.Identity()\n\n    def feature_info(self, location):\n        if location == \'expansion\':\n            # no expansion in this block, use depthwise, before SE\n            info = dict(module=\'act1\', hook_type=\'forward\', num_chs=self.conv_pw.in_channels)\n        elif location == \'depthwise\':  # after SE\n            info = dict(module=\'conv_pw\', hook_type=\'forward_pre\', num_chs=self.conv_pw.in_channels)\n        else:  # location == \'bottleneck\'\n            info = dict(module=\'\', hook_type=\'\', num_chs=self.conv_pw.out_channels)\n        return info\n\n    def forward(self, x):\n        residual = x\n\n        x = self.conv_dw(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n\n        if self.se is not None:\n            x = self.se(x)\n\n        x = self.conv_pw(x)\n        x = self.bn2(x)\n        x = self.act2(x)\n\n        if self.has_residual:\n            if self.drop_path_rate > 0.:\n                x = drop_path(x, self.drop_path_rate, self.training)\n            x += residual\n        return x\n\n\nclass InvertedResidual(nn.Module):\n    """""" Inverted residual block w/ optional SE and CondConv routing""""""\n\n    def __init__(self, in_chs, out_chs, dw_kernel_size=3,\n                 stride=1, dilation=1, pad_type=\'\', act_layer=nn.ReLU, noskip=False,\n                 exp_ratio=1.0, exp_kernel_size=1, pw_kernel_size=1,\n                 se_ratio=0., se_kwargs=None, norm_layer=nn.BatchNorm2d, norm_kwargs=None,\n                 conv_kwargs=None, drop_path_rate=0.):\n        super(InvertedResidual, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n        conv_kwargs = conv_kwargs or {}\n        mid_chs = make_divisible(in_chs * exp_ratio)\n        has_se = se_ratio is not None and se_ratio > 0.\n        self.has_residual = (in_chs == out_chs and stride == 1) and not noskip\n        self.drop_path_rate = drop_path_rate\n\n        # Point-wise expansion\n        self.conv_pw = create_conv2d(in_chs, mid_chs, exp_kernel_size, padding=pad_type, **conv_kwargs)\n        self.bn1 = norm_layer(mid_chs, **norm_kwargs)\n        self.act1 = act_layer(inplace=True)\n\n        # Depth-wise convolution\n        self.conv_dw = create_conv2d(\n            mid_chs, mid_chs, dw_kernel_size, stride=stride, dilation=dilation,\n            padding=pad_type, depthwise=True, **conv_kwargs)\n        self.bn2 = norm_layer(mid_chs, **norm_kwargs)\n        self.act2 = act_layer(inplace=True)\n\n        # Squeeze-and-excitation\n        if has_se:\n            se_kwargs = resolve_se_args(se_kwargs, in_chs, act_layer)\n            self.se = SqueezeExcite(mid_chs, se_ratio=se_ratio, **se_kwargs)\n        else:\n            self.se = None\n\n        # Point-wise linear projection\n        self.conv_pwl = create_conv2d(mid_chs, out_chs, pw_kernel_size, padding=pad_type, **conv_kwargs)\n        self.bn3 = norm_layer(out_chs, **norm_kwargs)\n\n    def feature_info(self, location):\n        if location == \'expansion\':\n            info = dict(module=\'act1\', hook_type=\'forward\', num_chs=self.conv_pw.in_channels)\n        elif location == \'depthwise\':  # after SE\n            info = dict(module=\'conv_pwl\', hook_type=\'forward_pre\', num_chs=self.conv_pwl.in_channels)\n        else:  # location == \'bottleneck\'\n            info = dict(module=\'\', hook_type=\'\', num_chs=self.conv_pwl.out_channels)\n        return info\n\n    def forward(self, x):\n        residual = x\n\n        # Point-wise expansion\n        x = self.conv_pw(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n\n        # Depth-wise convolution\n        x = self.conv_dw(x)\n        x = self.bn2(x)\n        x = self.act2(x)\n\n        # Squeeze-and-excitation\n        if self.se is not None:\n            x = self.se(x)\n\n        # Point-wise linear projection\n        x = self.conv_pwl(x)\n        x = self.bn3(x)\n\n        if self.has_residual:\n            if self.drop_path_rate > 0.:\n                x = drop_path(x, self.drop_path_rate, self.training)\n            x += residual\n\n        return x\n\n\nclass CondConvResidual(InvertedResidual):\n    """""" Inverted residual block w/ CondConv routing""""""\n\n    def __init__(self, in_chs, out_chs, dw_kernel_size=3,\n                 stride=1, dilation=1, pad_type=\'\', act_layer=nn.ReLU, noskip=False,\n                 exp_ratio=1.0, exp_kernel_size=1, pw_kernel_size=1,\n                 se_ratio=0., se_kwargs=None, norm_layer=nn.BatchNorm2d, norm_kwargs=None,\n                 num_experts=0, drop_path_rate=0.):\n\n        self.num_experts = num_experts\n        conv_kwargs = dict(num_experts=self.num_experts)\n\n        super(CondConvResidual, self).__init__(\n            in_chs, out_chs, dw_kernel_size=dw_kernel_size, stride=stride, dilation=dilation, pad_type=pad_type,\n            act_layer=act_layer, noskip=noskip, exp_ratio=exp_ratio, exp_kernel_size=exp_kernel_size,\n            pw_kernel_size=pw_kernel_size, se_ratio=se_ratio, se_kwargs=se_kwargs,\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs, conv_kwargs=conv_kwargs,\n            drop_path_rate=drop_path_rate)\n\n        self.routing_fn = nn.Linear(in_chs, self.num_experts)\n\n    def forward(self, x):\n        residual = x\n\n        # CondConv routing\n        pooled_inputs = F.adaptive_avg_pool2d(x, 1).flatten(1)\n        routing_weights = torch.sigmoid(self.routing_fn(pooled_inputs))\n\n        # Point-wise expansion\n        x = self.conv_pw(x, routing_weights)\n        x = self.bn1(x)\n        x = self.act1(x)\n\n        # Depth-wise convolution\n        x = self.conv_dw(x, routing_weights)\n        x = self.bn2(x)\n        x = self.act2(x)\n\n        # Squeeze-and-excitation\n        if self.se is not None:\n            x = self.se(x)\n\n        # Point-wise linear projection\n        x = self.conv_pwl(x, routing_weights)\n        x = self.bn3(x)\n\n        if self.has_residual:\n            if self.drop_path_rate > 0.:\n                x = drop_path(x, self.drop_path_rate, self.training)\n            x += residual\n        return x\n\n\nclass EdgeResidual(nn.Module):\n    """""" Residual block with expansion convolution followed by pointwise-linear w/ stride""""""\n\n    def __init__(self, in_chs, out_chs, exp_kernel_size=3, exp_ratio=1.0, fake_in_chs=0,\n                 stride=1, dilation=1, pad_type=\'\', act_layer=nn.ReLU, noskip=False, pw_kernel_size=1,\n                 se_ratio=0., se_kwargs=None, norm_layer=nn.BatchNorm2d, norm_kwargs=None,\n                 drop_path_rate=0.):\n        super(EdgeResidual, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n        if fake_in_chs > 0:\n            mid_chs = make_divisible(fake_in_chs * exp_ratio)\n        else:\n            mid_chs = make_divisible(in_chs * exp_ratio)\n        has_se = se_ratio is not None and se_ratio > 0.\n        self.has_residual = (in_chs == out_chs and stride == 1) and not noskip\n        self.drop_path_rate = drop_path_rate\n\n        # Expansion convolution\n        self.conv_exp = create_conv2d(in_chs, mid_chs, exp_kernel_size, padding=pad_type)\n        self.bn1 = norm_layer(mid_chs, **norm_kwargs)\n        self.act1 = act_layer(inplace=True)\n\n        # Squeeze-and-excitation\n        if has_se:\n            se_kwargs = resolve_se_args(se_kwargs, in_chs, act_layer)\n            self.se = SqueezeExcite(mid_chs, se_ratio=se_ratio, **se_kwargs)\n        else:\n            self.se = None\n\n        # Point-wise linear projection\n        self.conv_pwl = create_conv2d(\n            mid_chs, out_chs, pw_kernel_size, stride=stride, dilation=dilation, padding=pad_type)\n        self.bn2 = norm_layer(out_chs, **norm_kwargs)\n\n    def feature_info(self, location):\n        if location == \'expansion\':\n            info = dict(module=\'act1\', hook_type=\'forward\', num_chs=self.conv_exp.out_channels)\n        elif location == \'depthwise\':\n            # there is no depthwise, take after SE, before PWL\n            info = dict(module=\'conv_pwl\', hook_type=\'forward_pre\', num_chs=self.conv_pwl.in_channels)\n        else:  # location == \'bottleneck\'\n            info = dict(module=\'\', hook_type=\'\', num_chs=self.conv_pwl.out_channels)\n        return info\n\n    def forward(self, x):\n        residual = x\n\n        # Expansion convolution\n        x = self.conv_exp(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n\n        # Squeeze-and-excitation\n        if self.se is not None:\n            x = self.se(x)\n\n        # Point-wise linear projection\n        x = self.conv_pwl(x)\n        x = self.bn2(x)\n\n        if self.has_residual:\n            if self.drop_path_rate > 0.:\n                x = drop_path(x, self.drop_path_rate, self.training)\n            x += residual\n\n        return x\n'"
timm/models/efficientnet_builder.py,1,"b'import logging\nimport math\nimport re\nfrom collections.__init__ import OrderedDict\nfrom copy import deepcopy\n\nimport torch.nn as nn\nfrom .layers import CondConv2d, get_condconv_initializer\nfrom .layers.activations import HardSwish, Swish\nfrom .efficientnet_blocks import *\n\n\ndef _parse_ksize(ss):\n    if ss.isdigit():\n        return int(ss)\n    else:\n        return [int(k) for k in ss.split(\'.\')]\n\n\ndef _decode_block_str(block_str):\n    """""" Decode block definition string\n\n    Gets a list of block arg (dicts) through a string notation of arguments.\n    E.g. ir_r2_k3_s2_e1_i32_o16_se0.25_noskip\n\n    All args can exist in any order with the exception of the leading string which\n    is assumed to indicate the block type.\n\n    leading string - block type (\n      ir = InvertedResidual, ds = DepthwiseSep, dsa = DeptwhiseSep with pw act, cn = ConvBnAct)\n    r - number of repeat blocks,\n    k - kernel size,\n    s - strides (1-9),\n    e - expansion ratio,\n    c - output channels,\n    se - squeeze/excitation ratio\n    n - activation fn (\'re\', \'r6\', \'hs\', or \'sw\')\n    Args:\n        block_str: a string representation of block arguments.\n    Returns:\n        A list of block args (dicts)\n    Raises:\n        ValueError: if the string def not properly specified (TODO)\n    """"""\n    assert isinstance(block_str, str)\n    ops = block_str.split(\'_\')\n    block_type = ops[0]  # take the block type off the front\n    ops = ops[1:]\n    options = {}\n    noskip = False\n    for op in ops:\n        # string options being checked on individual basis, combine if they grow\n        if op == \'noskip\':\n            noskip = True\n        elif op.startswith(\'n\'):\n            # activation fn\n            key = op[0]\n            v = op[1:]\n            if v == \'re\':\n                value = nn.ReLU\n            elif v == \'r6\':\n                value = nn.ReLU6\n            elif v == \'hs\':\n                value = HardSwish\n            elif v == \'sw\':\n                value = Swish\n            else:\n                continue\n            options[key] = value\n        else:\n            # all numeric options\n            splits = re.split(r\'(\\d.*)\', op)\n            if len(splits) >= 2:\n                key, value = splits[:2]\n                options[key] = value\n\n    # if act_layer is None, the model default (passed to model init) will be used\n    act_layer = options[\'n\'] if \'n\' in options else None\n    exp_kernel_size = _parse_ksize(options[\'a\']) if \'a\' in options else 1\n    pw_kernel_size = _parse_ksize(options[\'p\']) if \'p\' in options else 1\n    fake_in_chs = int(options[\'fc\']) if \'fc\' in options else 0  # FIXME hack to deal with in_chs issue in TPU def\n\n    num_repeat = int(options[\'r\'])\n    # each type of block has different valid arguments, fill accordingly\n    if block_type == \'ir\':\n        block_args = dict(\n            block_type=block_type,\n            dw_kernel_size=_parse_ksize(options[\'k\']),\n            exp_kernel_size=exp_kernel_size,\n            pw_kernel_size=pw_kernel_size,\n            out_chs=int(options[\'c\']),\n            exp_ratio=float(options[\'e\']),\n            se_ratio=float(options[\'se\']) if \'se\' in options else None,\n            stride=int(options[\'s\']),\n            act_layer=act_layer,\n            noskip=noskip,\n        )\n        if \'cc\' in options:\n            block_args[\'num_experts\'] = int(options[\'cc\'])\n    elif block_type == \'ds\' or block_type == \'dsa\':\n        block_args = dict(\n            block_type=block_type,\n            dw_kernel_size=_parse_ksize(options[\'k\']),\n            pw_kernel_size=pw_kernel_size,\n            out_chs=int(options[\'c\']),\n            se_ratio=float(options[\'se\']) if \'se\' in options else None,\n            stride=int(options[\'s\']),\n            act_layer=act_layer,\n            pw_act=block_type == \'dsa\',\n            noskip=block_type == \'dsa\' or noskip,\n        )\n    elif block_type == \'er\':\n        block_args = dict(\n            block_type=block_type,\n            exp_kernel_size=_parse_ksize(options[\'k\']),\n            pw_kernel_size=pw_kernel_size,\n            out_chs=int(options[\'c\']),\n            exp_ratio=float(options[\'e\']),\n            fake_in_chs=fake_in_chs,\n            se_ratio=float(options[\'se\']) if \'se\' in options else None,\n            stride=int(options[\'s\']),\n            act_layer=act_layer,\n            noskip=noskip,\n        )\n    elif block_type == \'cn\':\n        block_args = dict(\n            block_type=block_type,\n            kernel_size=int(options[\'k\']),\n            out_chs=int(options[\'c\']),\n            stride=int(options[\'s\']),\n            act_layer=act_layer,\n        )\n    else:\n        assert False, \'Unknown block type (%s)\' % block_type\n\n    return block_args, num_repeat\n\n\ndef _scale_stage_depth(stack_args, repeats, depth_multiplier=1.0, depth_trunc=\'ceil\'):\n    """""" Per-stage depth scaling\n    Scales the block repeats in each stage. This depth scaling impl maintains\n    compatibility with the EfficientNet scaling method, while allowing sensible\n    scaling for other models that may have multiple block arg definitions in each stage.\n    """"""\n\n    # We scale the total repeat count for each stage, there may be multiple\n    # block arg defs per stage so we need to sum.\n    num_repeat = sum(repeats)\n    if depth_trunc == \'round\':\n        # Truncating to int by rounding allows stages with few repeats to remain\n        # proportionally smaller for longer. This is a good choice when stage definitions\n        # include single repeat stages that we\'d prefer to keep that way as long as possible\n        num_repeat_scaled = max(1, round(num_repeat * depth_multiplier))\n    else:\n        # The default for EfficientNet truncates repeats to int via \'ceil\'.\n        # Any multiplier > 1.0 will result in an increased depth for every stage.\n        num_repeat_scaled = int(math.ceil(num_repeat * depth_multiplier))\n\n    # Proportionally distribute repeat count scaling to each block definition in the stage.\n    # Allocation is done in reverse as it results in the first block being less likely to be scaled.\n    # The first block makes less sense to repeat in most of the arch definitions.\n    repeats_scaled = []\n    for r in repeats[::-1]:\n        rs = max(1, round((r / num_repeat * num_repeat_scaled)))\n        repeats_scaled.append(rs)\n        num_repeat -= r\n        num_repeat_scaled -= rs\n    repeats_scaled = repeats_scaled[::-1]\n\n    # Apply the calculated scaling to each block arg in the stage\n    sa_scaled = []\n    for ba, rep in zip(stack_args, repeats_scaled):\n        sa_scaled.extend([deepcopy(ba) for _ in range(rep)])\n    return sa_scaled\n\n\ndef decode_arch_def(arch_def, depth_multiplier=1.0, depth_trunc=\'ceil\', experts_multiplier=1, fix_first_last=False):\n    arch_args = []\n    for stack_idx, block_strings in enumerate(arch_def):\n        assert isinstance(block_strings, list)\n        stack_args = []\n        repeats = []\n        for block_str in block_strings:\n            assert isinstance(block_str, str)\n            ba, rep = _decode_block_str(block_str)\n            if ba.get(\'num_experts\', 0) > 0 and experts_multiplier > 1:\n                ba[\'num_experts\'] *= experts_multiplier\n            stack_args.append(ba)\n            repeats.append(rep)\n        if fix_first_last and (stack_idx == 0 or stack_idx == len(arch_def) - 1):\n            arch_args.append(_scale_stage_depth(stack_args, repeats, 1.0, depth_trunc))\n        else:\n            arch_args.append(_scale_stage_depth(stack_args, repeats, depth_multiplier, depth_trunc))\n    return arch_args\n\n\nclass EfficientNetBuilder:\n    """""" Build Trunk Blocks\n\n    This ended up being somewhat of a cross between\n    https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mnasnet_models.py\n    and\n    https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/modeling/backbone/fbnet_builder.py\n\n    """"""\n    def __init__(self, channel_multiplier=1.0, channel_divisor=8, channel_min=None,\n                 output_stride=32, pad_type=\'\', act_layer=None, se_kwargs=None,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None, drop_path_rate=0., feature_location=\'\',\n                 verbose=False):\n        self.channel_multiplier = channel_multiplier\n        self.channel_divisor = channel_divisor\n        self.channel_min = channel_min\n        self.output_stride = output_stride\n        self.pad_type = pad_type\n        self.act_layer = act_layer\n        self.se_kwargs = se_kwargs\n        self.norm_layer = norm_layer\n        self.norm_kwargs = norm_kwargs\n        self.drop_path_rate = drop_path_rate\n        self.feature_location = feature_location\n        assert feature_location in (\'bottleneck\', \'depthwise\', \'expansion\', \'\')\n        self.verbose = verbose\n\n        # state updated during build, consumed by model\n        self.in_chs = None\n        self.features = OrderedDict()\n\n    def _round_channels(self, chs):\n        return round_channels(chs, self.channel_multiplier, self.channel_divisor, self.channel_min)\n\n    def _make_block(self, ba, block_idx, block_count):\n        drop_path_rate = self.drop_path_rate * block_idx / block_count\n        bt = ba.pop(\'block_type\')\n        ba[\'in_chs\'] = self.in_chs\n        ba[\'out_chs\'] = self._round_channels(ba[\'out_chs\'])\n        if \'fake_in_chs\' in ba and ba[\'fake_in_chs\']:\n            # FIXME this is a hack to work around mismatch in origin impl input filters\n            ba[\'fake_in_chs\'] = self._round_channels(ba[\'fake_in_chs\'])\n        ba[\'norm_layer\'] = self.norm_layer\n        ba[\'norm_kwargs\'] = self.norm_kwargs\n        ba[\'pad_type\'] = self.pad_type\n        # block act fn overrides the model default\n        ba[\'act_layer\'] = ba[\'act_layer\'] if ba[\'act_layer\'] is not None else self.act_layer\n        assert ba[\'act_layer\'] is not None\n        if bt == \'ir\':\n            ba[\'drop_path_rate\'] = drop_path_rate\n            ba[\'se_kwargs\'] = self.se_kwargs\n            if self.verbose:\n                logging.info(\'  InvertedResidual {}, Args: {}\'.format(block_idx, str(ba)))\n            if ba.get(\'num_experts\', 0) > 0:\n                block = CondConvResidual(**ba)\n            else:\n                block = InvertedResidual(**ba)\n        elif bt == \'ds\' or bt == \'dsa\':\n            ba[\'drop_path_rate\'] = drop_path_rate\n            ba[\'se_kwargs\'] = self.se_kwargs\n            if self.verbose:\n                logging.info(\'  DepthwiseSeparable {}, Args: {}\'.format(block_idx, str(ba)))\n            block = DepthwiseSeparableConv(**ba)\n        elif bt == \'er\':\n            ba[\'drop_path_rate\'] = drop_path_rate\n            ba[\'se_kwargs\'] = self.se_kwargs\n            if self.verbose:\n                logging.info(\'  EdgeResidual {}, Args: {}\'.format(block_idx, str(ba)))\n            block = EdgeResidual(**ba)\n        elif bt == \'cn\':\n            if self.verbose:\n                logging.info(\'  ConvBnAct {}, Args: {}\'.format(block_idx, str(ba)))\n            block = ConvBnAct(**ba)\n        else:\n            assert False, \'Uknkown block type (%s) while building model.\' % bt\n        self.in_chs = ba[\'out_chs\']  # update in_chs for arg of next block\n\n        return block\n\n    def __call__(self, in_chs, model_block_args):\n        """""" Build the blocks\n        Args:\n            in_chs: Number of input-channels passed to first block\n            model_block_args: A list of lists, outer list defines stages, inner\n                list contains strings defining block configuration(s)\n        Return:\n             List of block stacks (each stack wrapped in nn.Sequential)\n        """"""\n        if self.verbose:\n            logging.info(\'Building model trunk with %d stages...\' % len(model_block_args))\n        self.in_chs = in_chs\n        total_block_count = sum([len(x) for x in model_block_args])\n        total_block_idx = 0\n        current_stride = 2\n        current_dilation = 1\n        feature_idx = 0\n        stages = []\n        # outer list of block_args defines the stacks (\'stages\' by some conventions)\n        for stage_idx, stage_block_args in enumerate(model_block_args):\n            last_stack = stage_idx == (len(model_block_args) - 1)\n            if self.verbose:\n                logging.info(\'Stack: {}\'.format(stage_idx))\n            assert isinstance(stage_block_args, list)\n\n            blocks = []\n            # each stack (stage) contains a list of block arguments\n            for block_idx, block_args in enumerate(stage_block_args):\n                last_block = block_idx == (len(stage_block_args) - 1)\n                extract_features = \'\'  # No features extracted\n                if self.verbose:\n                    logging.info(\' Block: {}\'.format(block_idx))\n\n                # Sort out stride, dilation, and feature extraction details\n                assert block_args[\'stride\'] in (1, 2)\n                if block_idx >= 1:\n                    # only the first block in any stack can have a stride > 1\n                    block_args[\'stride\'] = 1\n\n                do_extract = False\n                if self.feature_location == \'bottleneck\' or self.feature_location == \'depthwise\':\n                    if last_block:\n                        next_stage_idx = stage_idx + 1\n                        if next_stage_idx >= len(model_block_args):\n                            do_extract = True\n                        else:\n                            do_extract = model_block_args[next_stage_idx][0][\'stride\'] > 1\n                elif self.feature_location == \'expansion\':\n                    if block_args[\'stride\'] > 1 or (last_stack and last_block):\n                        do_extract = True\n                if do_extract:\n                    extract_features = self.feature_location\n\n                next_dilation = current_dilation\n                next_output_stride = current_stride\n                if block_args[\'stride\'] > 1:\n                    next_output_stride = current_stride * block_args[\'stride\']\n                    if next_output_stride > self.output_stride:\n                        next_dilation = current_dilation * block_args[\'stride\']\n                        block_args[\'stride\'] = 1\n                        if self.verbose:\n                            logging.info(\'  Converting stride to dilation to maintain output_stride=={}\'.format(\n                                self.output_stride))\n                    else:\n                        current_stride = next_output_stride\n                block_args[\'dilation\'] = current_dilation\n                if next_dilation != current_dilation:\n                    current_dilation = next_dilation\n\n                # create the block\n                block = self._make_block(block_args, total_block_idx, total_block_count)\n                blocks.append(block)\n\n                # stash feature module name and channel info for model feature extraction\n                if extract_features:\n                    feature_info = block.feature_info(extract_features)\n                    if feature_info[\'module\']:\n                        feature_info[\'module\'] = \'blocks.{}.{}.\'.format(stage_idx, block_idx) + feature_info[\'module\']\n                    feature_info[\'stage_idx\'] = stage_idx\n                    feature_info[\'block_idx\'] = block_idx\n                    feature_info[\'reduction\'] = current_stride\n                    self.features[feature_idx] = feature_info\n                    feature_idx += 1\n\n                total_block_idx += 1  # incr global block idx (across all stacks)\n            stages.append(nn.Sequential(*blocks))\n        return stages\n\n\ndef _init_weight_goog(m, n=\'\', fix_group_fanout=True):\n    """""" Weight initialization as per Tensorflow official implementations.\n\n    Args:\n        m (nn.Module): module to init\n        n (str): module name\n        fix_group_fanout (bool): enable correct (matching Tensorflow TPU impl) fanout calculation w/ group convs\n\n    Handles layers in EfficientNet, EfficientNet-CondConv, MixNet, MnasNet, MobileNetV3, etc:\n    * https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mnasnet_model.py\n    * https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_model.py\n    """"""\n    if isinstance(m, CondConv2d):\n        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        if fix_group_fanout:\n            fan_out //= m.groups\n        init_weight_fn = get_condconv_initializer(\n            lambda w: w.data.normal_(0, math.sqrt(2.0 / fan_out)), m.num_experts, m.weight_shape)\n        init_weight_fn(m.weight)\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.Conv2d):\n        fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        if fix_group_fanout:\n            fan_out //= m.groups\n        m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))\n        if m.bias is not None:\n            m.bias.data.zero_()\n    elif isinstance(m, nn.BatchNorm2d):\n        m.weight.data.fill_(1.0)\n        m.bias.data.zero_()\n    elif isinstance(m, nn.Linear):\n        fan_out = m.weight.size(0)  # fan-out\n        fan_in = 0\n        if \'routing_fn\' in n:\n            fan_in = m.weight.size(1)\n        init_range = 1.0 / math.sqrt(fan_in + fan_out)\n        m.weight.data.uniform_(-init_range, init_range)\n        m.bias.data.zero_()\n\n\ndef efficientnet_init_weights(model: nn.Module, init_fn=None):\n    init_fn = init_fn or _init_weight_goog\n    for n, m in model.named_modules():\n        init_fn(m, n)\n\n'"
timm/models/factory.py,0,"b'from .registry import is_model, is_model_in_modules, model_entrypoint\nfrom .helpers import load_checkpoint\n\n\ndef create_model(\n        model_name,\n        pretrained=False,\n        num_classes=1000,\n        in_chans=3,\n        checkpoint_path=\'\',\n        **kwargs):\n    """"""Create a model\n\n    Args:\n        model_name (str): name of model to instantiate\n        pretrained (bool): load pretrained ImageNet-1k weights if true\n        num_classes (int): number of classes for final fully connected layer (default: 1000)\n        in_chans (int): number of input channels / colors (default: 3)\n        checkpoint_path (str): path of checkpoint to load after model is initialized\n\n    Keyword Args:\n        drop_rate (float): dropout rate for training (default: 0.0)\n        global_pool (str): global pool type (default: \'avg\')\n        **: other kwargs are model specific\n    """"""\n    margs = dict(pretrained=pretrained, num_classes=num_classes, in_chans=in_chans)\n\n    # Only EfficientNet and MobileNetV3 models have support for batchnorm params or drop_connect_rate passed as args\n    is_efficientnet = is_model_in_modules(model_name, [\'efficientnet\', \'mobilenetv3\'])\n    if not is_efficientnet:\n        kwargs.pop(\'bn_tf\', None)\n        kwargs.pop(\'bn_momentum\', None)\n        kwargs.pop(\'bn_eps\', None)\n\n    # Parameters that aren\'t supported by all models should default to None in command line args,\n    # remove them if they are present and not set so that non-supporting models don\'t break.\n    if kwargs.get(\'drop_block_rate\', None) is None:\n        kwargs.pop(\'drop_block_rate\', None)\n\n    # handle backwards compat with drop_connect -> drop_path change\n    drop_connect_rate = kwargs.pop(\'drop_connect_rate\', None)\n    if drop_connect_rate is not None and kwargs.get(\'drop_path_rate\', None) is None:\n        print(""WARNING: \'drop_connect\' as an argument is deprecated, please use \'drop_path\'.""\n              "" Setting drop_path to %f."" % drop_connect_rate)\n        kwargs[\'drop_path_rate\'] = drop_connect_rate\n\n    if kwargs.get(\'drop_path_rate\', None) is None:\n        kwargs.pop(\'drop_path_rate\', None)\n\n    if is_model(model_name):\n        create_fn = model_entrypoint(model_name)\n        model = create_fn(**margs, **kwargs)\n    else:\n        raise RuntimeError(\'Unknown model (%s)\' % model_name)\n\n    if checkpoint_path:\n        load_checkpoint(model, checkpoint_path)\n\n    return model\n'"
timm/models/feature_hooks.py,0,"b'from collections import defaultdict, OrderedDict\nfrom functools import partial\n\n\nclass FeatureHooks:\n\n    def __init__(self, hooks, named_modules):\n        # setup feature hooks\n        modules = {k: v for k, v in named_modules}\n        for h in hooks:\n            hook_name = h[\'name\']\n            m = modules[hook_name]\n            hook_fn = partial(self._collect_output_hook, hook_name)\n            if h[\'type\'] == \'forward_pre\':\n                m.register_forward_pre_hook(hook_fn)\n            elif h[\'type\'] == \'forward\':\n                m.register_forward_hook(hook_fn)\n            else:\n                assert False, ""Unsupported hook type""\n        self._feature_outputs = defaultdict(OrderedDict)\n\n    def _collect_output_hook(self, name, *args):\n        x = args[-1]  # tensor we want is last argument, output for fwd, input for fwd_pre\n        if isinstance(x, tuple):\n            x = x[0]  # unwrap input tuple\n        self._feature_outputs[x.device][name] = x\n\n    def get_output(self, device):\n        output = tuple(self._feature_outputs[device].values())[::-1]\n        self._feature_outputs[device] = OrderedDict()  # clear after reading\n        return output\n'"
timm/models/gluon_resnet.py,2,"b'""""""Pytorch impl of MxNet Gluon ResNet/(SE)ResNeXt variants\nThis file evolved from https://github.com/pytorch/vision \'resnet.py\' with (SE)-ResNeXt additions\nand ports of Gluon variations (https://github.com/dmlc/gluon-cv/blob/master/gluoncv/model_zoo/resnet.py) \nby Ross Wightman\n""""""\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SEModule\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\nfrom .resnet import ResNet, Bottleneck, BasicBlock\n\n\ndef _cfg(url=\'\', **kwargs):\n    return {\n        \'url\': url,\n        \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (7, 7),\n        \'crop_pct\': 0.875, \'interpolation\': \'bicubic\',\n        \'mean\': IMAGENET_DEFAULT_MEAN, \'std\': IMAGENET_DEFAULT_STD,\n        \'first_conv\': \'conv1\', \'classifier\': \'fc\',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    \'gluon_resnet18_v1b\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet18_v1b-0757602b.pth\'),\n    \'gluon_resnet34_v1b\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet34_v1b-c6d82d59.pth\'),\n    \'gluon_resnet50_v1b\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet50_v1b-0ebe02e2.pth\'),\n    \'gluon_resnet101_v1b\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet101_v1b-3b017079.pth\'),\n    \'gluon_resnet152_v1b\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet152_v1b-c1edb0dd.pth\'),\n    \'gluon_resnet50_v1c\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet50_v1c-48092f55.pth\'),\n    \'gluon_resnet101_v1c\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet101_v1c-1f26822a.pth\'),\n    \'gluon_resnet152_v1c\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet152_v1c-a3bb0b98.pth\'),\n    \'gluon_resnet50_v1d\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet50_v1d-818a1b1b.pth\'),\n    \'gluon_resnet101_v1d\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet101_v1d-0f9c8644.pth\'),\n    \'gluon_resnet152_v1d\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet152_v1d-bd354e12.pth\'),\n    \'gluon_resnet50_v1e\': _cfg(url=\'\'),\n    \'gluon_resnet101_v1e\': _cfg(url=\'\'),\n    \'gluon_resnet152_v1e\': _cfg(url=\'\'),\n    \'gluon_resnet50_v1s\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet50_v1s-1762acc0.pth\'),\n    \'gluon_resnet101_v1s\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet101_v1s-60fe0cc1.pth\'),\n    \'gluon_resnet152_v1s\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnet152_v1s-dcc41b81.pth\'),\n    \'gluon_resnext50_32x4d\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnext50_32x4d-e6a097c1.pth\'),\n    \'gluon_resnext101_32x4d\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnext101_32x4d-b253c8c4.pth\'),\n    \'gluon_resnext101_64x4d\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_resnext101_64x4d-f9a8e184.pth\'),\n    \'gluon_seresnext50_32x4d\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_seresnext50_32x4d-90cf2d6e.pth\'),\n    \'gluon_seresnext101_32x4d\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_seresnext101_32x4d-cf52900d.pth\'),\n    \'gluon_seresnext101_64x4d\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_seresnext101_64x4d-f9926f93.pth\'),\n    \'gluon_senet154\': _cfg(url=\'https://github.com/rwightman/pytorch-pretrained-gluonresnet/releases/download/v0.1/gluon_senet154-70a1a3c0.pth\'),\n}\n\n\n@register_model\ndef gluon_resnet18_v1b(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet18_v1b\']\n    model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet34_v1b(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet34_v1b\']\n    model = ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet50_v1b(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet50_v1b\']\n    model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet101_v1b(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet101_v1b\']\n    model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet152_v1b(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet152_v1b\']\n    model = ResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet50_v1c(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet50_v1c\']\n    model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans,\n                   stem_width=32, stem_type=\'deep\', **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet101_v1c(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet101_v1c\']\n    model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, in_chans=in_chans,\n                   stem_width=32, stem_type=\'deep\', **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet152_v1c(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet152_v1c\']\n    model = ResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes, in_chans=in_chans,\n                   stem_width=32, stem_type=\'deep\', **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet50_v1d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet50_v1d\']\n    model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans,\n                   stem_width=32, stem_type=\'deep\', avg_down=True, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet101_v1d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet101_v1d\']\n    model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, in_chans=in_chans,\n                   stem_width=32, stem_type=\'deep\', avg_down=True, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet152_v1d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet152_v1d\']\n    model = ResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes, in_chans=in_chans,\n                   stem_width=32, stem_type=\'deep\', avg_down=True, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet50_v1e(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-50-V1e model. No pretrained weights for any \'e\' variants\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet50_v1e\']\n    model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans,\n                   stem_width=64, stem_type=\'deep\', avg_down=True, **kwargs)\n    model.default_cfg = default_cfg\n    #if pretrained:\n    #    load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet101_v1e(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet101_v1e\']\n    model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, in_chans=in_chans,\n                   stem_width=64, stem_type=\'deep\', avg_down=True, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet152_v1e(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet152_v1e\']\n    model = ResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes, in_chans=in_chans,\n                   stem_width=64, stem_type=\'deep\', avg_down=True, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet50_v1s(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet50_v1s\']\n    model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans,\n                   stem_width=64, stem_type=\'deep\', **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet101_v1s(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet101_v1s\']\n    model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, in_chans=in_chans,\n                   stem_width=64, stem_type=\'deep\', **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnet152_v1s(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnet152_v1s\']\n    model = ResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes, in_chans=in_chans,\n                   stem_width=64, stem_type=\'deep\', **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnext50_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNeXt50-32x4d model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnext50_32x4d\']\n    model = ResNet(\n        Bottleneck, [3, 4, 6, 3], cardinality=32, base_width=4,\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnext101_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNeXt-101 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnext101_32x4d\']\n    model = ResNet(\n        Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=4,\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_resnext101_64x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNeXt-101 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_resnext101_64x4d\']\n    model = ResNet(\n        Bottleneck, [3, 4, 23, 3], cardinality=64, base_width=4,\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_seresnext50_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a SEResNeXt50-32x4d model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_seresnext50_32x4d\']\n    model = ResNet(\n        Bottleneck, [3, 4, 6, 3], cardinality=32, base_width=4,\n        num_classes=num_classes, in_chans=in_chans, block_args=dict(attn_layer=SEModule), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_seresnext101_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a SEResNeXt-101-32x4d model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_seresnext101_32x4d\']\n    model = ResNet(\n        Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=4,\n        num_classes=num_classes, in_chans=in_chans, block_args=dict(attn_layer=SEModule), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_seresnext101_64x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a SEResNeXt-101-64x4d model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_seresnext101_64x4d\']\n    block_args = dict(attn_layer=SEModule)\n    model = ResNet(\n        Bottleneck, [3, 4, 23, 3], cardinality=64, base_width=4,\n        num_classes=num_classes, in_chans=in_chans, block_args=block_args, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_senet154(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs an SENet-154 model.\n    """"""\n    default_cfg = default_cfgs[\'gluon_senet154\']\n    block_args = dict(attn_layer=SEModule)\n    model = ResNet(\n        Bottleneck, [3, 8, 36, 3], cardinality=64, base_width=4, stem_type=\'deep\', down_kernel_size=3,\n        block_reduce_first=2, num_classes=num_classes, in_chans=in_chans, block_args=block_args, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n'"
timm/models/gluon_xception.py,2,"b'""""""Pytorch impl of Gluon Xception\nThis is a port of the Gluon Xception code and weights, itself ported from a PyTorch DeepLab impl.\n\nGluon model: (https://gluon-cv.mxnet.io/_modules/gluoncv/model_zoo/xception.html)\nOriginal PyTorch DeepLab impl: https://github.com/jfzhang95/pytorch-deeplab-xception\n\nHacked together by Ross Wightman\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SelectAdaptivePool2d\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\n__all__ = [\'Xception65\', \'Xception71\']\n\ndefault_cfgs = {\n    \'gluon_xception65\': {\n        \'url\': \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gluon_xception-7015a15c.pth\',\n        \'input_size\': (3, 299, 299),\n        \'crop_pct\': 0.875,\n        \'pool_size\': (10, 10),\n        \'interpolation\': \'bicubic\',\n        \'mean\': IMAGENET_DEFAULT_MEAN,\n        \'std\': IMAGENET_DEFAULT_STD,\n        \'num_classes\': 1000,\n        \'first_conv\': \'conv1\',\n        \'classifier\': \'fc\'\n        # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n    },\n    \'gluon_xception71\': {\n        \'url\': \'\',\n        \'input_size\': (3, 299, 299),\n        \'crop_pct\': 0.875,\n        \'pool_size\': (5, 5),\n        \'interpolation\': \'bicubic\',\n        \'mean\': IMAGENET_DEFAULT_MEAN,\n        \'std\': IMAGENET_DEFAULT_STD,\n        \'num_classes\': 1000,\n        \'first_conv\': \'conv1\',\n        \'classifier\': \'fc\'\n        # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n    }\n}\n\n\n"""""" PADDING NOTES\nThe original PyTorch and Gluon impl of these models dutifully reproduced the \naligned padding added to Tensorflow models for Deeplab. This padding was compensating\nfor  Tensorflow \'SAME\' padding. PyTorch symmetric padding behaves the way we\'d want it to. \n\nSo, I\'m phasing out the \'fixed_padding\' ported from TF and replacing with normal \nPyTorch padding, some asserts to validate the equivalence for any scenario we\'d \ncare about before removing altogether.\n""""""\n_USE_FIXED_PAD = False\n\n\ndef _pytorch_padding(kernel_size, stride=1, dilation=1, **_):\n    if _USE_FIXED_PAD:\n        return 0  # FIXME remove once verified\n    else:\n        padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n\n        # FIXME remove once verified\n        fp = _fixed_padding(kernel_size, dilation)\n        assert all(padding == p for p in fp)\n\n        return padding\n\n\ndef _fixed_padding(kernel_size, dilation):\n    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n    pad_total = kernel_size_effective - 1\n    pad_beg = pad_total // 2\n    pad_end = pad_total - pad_beg\n    return [pad_beg, pad_end, pad_beg, pad_end]\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, inplanes, planes, kernel_size=3, stride=1,\n                 dilation=1, bias=False, norm_layer=None, norm_kwargs=None):\n        super(SeparableConv2d, self).__init__()\n        norm_kwargs = norm_kwargs if norm_kwargs is not None else {}\n        self.kernel_size = kernel_size\n        self.dilation = dilation\n\n        padding = _fixed_padding(self.kernel_size, self.dilation)\n        if _USE_FIXED_PAD and any(p > 0 for p in padding):\n            self.fixed_padding = nn.ZeroPad2d(padding)\n        else:\n            self.fixed_padding = None\n\n        # depthwise convolution\n        self.conv_dw = nn.Conv2d(\n            inplanes, inplanes, kernel_size, stride=stride,\n            padding=_pytorch_padding(kernel_size, stride, dilation), dilation=dilation, groups=inplanes, bias=bias)\n        self.bn = norm_layer(num_features=inplanes, **norm_kwargs)\n        # pointwise convolution\n        self.conv_pw = nn.Conv2d(inplanes, planes, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        if self.fixed_padding is not None:\n            # FIXME remove once verified\n            x = self.fixed_padding(x)\n        x = self.conv_dw(x)\n        x = self.bn(x)\n        x = self.conv_pw(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self, inplanes, planes, num_reps, stride=1, dilation=1, norm_layer=None,\n                 norm_kwargs=None, start_with_relu=True, grow_first=True, is_last=False):\n        super(Block, self).__init__()\n        norm_kwargs = norm_kwargs if norm_kwargs is not None else {}\n        if planes != inplanes or stride != 1:\n            self.skip = nn.Sequential()\n            self.skip.add_module(\'conv1\', nn.Conv2d(\n                inplanes, planes, 1, stride=stride, bias=False)),\n            self.skip.add_module(\'bn1\', norm_layer(num_features=planes, **norm_kwargs))\n        else:\n            self.skip = None\n\n        rep = OrderedDict()\n        l = 1\n        filters = inplanes\n        if grow_first:\n            if start_with_relu:\n                rep[\'act%d\' % l] = nn.ReLU(inplace=False)  # NOTE: silent failure if inplace=True here\n            rep[\'conv%d\' % l] = SeparableConv2d(\n                inplanes, planes, 3, 1, dilation, norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n            rep[\'bn%d\' % l] = norm_layer(num_features=planes, **norm_kwargs)\n            filters = planes\n            l += 1\n\n        for _ in range(num_reps - 1):\n            if grow_first or start_with_relu:\n                # FIXME being conservative with inplace here, think it\'s fine to leave True?\n                rep[\'act%d\' % l] = nn.ReLU(inplace=grow_first or not start_with_relu)\n            rep[\'conv%d\' % l] = SeparableConv2d(\n                filters, filters, 3, 1, dilation, norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n            rep[\'bn%d\' % l] = norm_layer(num_features=filters, **norm_kwargs)\n            l += 1\n\n        if not grow_first:\n            rep[\'act%d\' % l] = nn.ReLU(inplace=True)\n            rep[\'conv%d\' % l] = SeparableConv2d(\n                inplanes, planes, 3, 1, dilation, norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n            rep[\'bn%d\' % l] = norm_layer(num_features=planes, **norm_kwargs)\n            l += 1\n\n        if stride != 1:\n            rep[\'act%d\' % l] = nn.ReLU(inplace=True)\n            rep[\'conv%d\' % l] = SeparableConv2d(\n                planes, planes, 3, stride, norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n            rep[\'bn%d\' % l] = norm_layer(num_features=planes, **norm_kwargs)\n            l += 1\n        elif is_last:\n            rep[\'act%d\' % l] = nn.ReLU(inplace=True)\n            rep[\'conv%d\' % l] = SeparableConv2d(\n                planes, planes, 3, 1, dilation, norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n            rep[\'bn%d\' % l] = norm_layer(num_features=planes, **norm_kwargs)\n            l += 1\n        self.rep = nn.Sequential(rep)\n\n    def forward(self, x):\n        skip = x\n        if self.skip is not None:\n            skip = self.skip(skip)\n        x = self.rep(x) + skip\n        return x\n\n\nclass Xception65(nn.Module):\n    """"""Modified Aligned Xception\n    """"""\n\n    def __init__(self, num_classes=1000, in_chans=3, output_stride=32, norm_layer=nn.BatchNorm2d,\n                 norm_kwargs=None, drop_rate=0., global_pool=\'avg\'):\n        super(Xception65, self).__init__()\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        norm_kwargs = norm_kwargs if norm_kwargs is not None else {}\n        if output_stride == 32:\n            entry_block3_stride = 2\n            exit_block20_stride = 2\n            middle_block_dilation = 1\n            exit_block_dilations = (1, 1)\n        elif output_stride == 16:\n            entry_block3_stride = 2\n            exit_block20_stride = 1\n            middle_block_dilation = 1\n            exit_block_dilations = (1, 2)\n        elif output_stride == 8:\n            entry_block3_stride = 1\n            exit_block20_stride = 1\n            middle_block_dilation = 2\n            exit_block_dilations = (2, 4)\n        else:\n            raise NotImplementedError\n\n        # Entry flow\n        self.conv1 = nn.Conv2d(in_chans, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = norm_layer(num_features=32, **norm_kwargs)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = norm_layer(num_features=64)\n\n        self.block1 = Block(\n            64, 128, num_reps=2, stride=2,\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs, start_with_relu=False)\n        self.block2 = Block(\n            128, 256, num_reps=2, stride=2,\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs, start_with_relu=False, grow_first=True)\n        self.block3 = Block(\n            256, 728, num_reps=2, stride=entry_block3_stride,\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs, start_with_relu=True, grow_first=True, is_last=True)\n\n        # Middle flow\n        self.mid = nn.Sequential(OrderedDict([(\'block%d\' % i,  Block(\n            728, 728, num_reps=3, stride=1, dilation=middle_block_dilation,\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs, start_with_relu=True, grow_first=True))\n                                              for i in range(4, 20)]))\n\n        # Exit flow\n        self.block20 = Block(\n            728, 1024, num_reps=2, stride=exit_block20_stride, dilation=exit_block_dilations[0],\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs, start_with_relu=True, grow_first=False, is_last=True)\n\n        self.conv3 = SeparableConv2d(\n            1024, 1536, 3, stride=1, dilation=exit_block_dilations[1],\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n        self.bn3 = norm_layer(num_features=1536, **norm_kwargs)\n\n        self.conv4 = SeparableConv2d(\n            1536, 1536, 3, stride=1, dilation=exit_block_dilations[1],\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n        self.bn4 = norm_layer(num_features=1536, **norm_kwargs)\n\n        self.num_features = 2048\n        self.conv5 = SeparableConv2d(\n            1536, self.num_features, 3, stride=1, dilation=exit_block_dilations[1],\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n        self.bn5 = norm_layer(num_features=self.num_features, **norm_kwargs)\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.fc = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n\n    def get_classifier(self):\n        return self.fc\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.num_classes = num_classes\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.fc = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes) if num_classes else None\n\n    def forward_features(self, x):\n        # Entry flow\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.block1(x)\n        # add relu here\n        x = self.relu(x)\n        # c1 = x\n        x = self.block2(x)\n        # c2 = x\n        x = self.block3(x)\n\n        # Middle flow\n        x = self.mid(x)\n        # c3 = x\n\n        # Exit flow\n        x = self.block20(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.relu(x)\n\n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = self.relu(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x).flatten(1)\n        if self.drop_rate:\n            F.dropout(x, self.drop_rate, training=self.training)\n        x = self.fc(x)\n        return x\n\n\nclass Xception71(nn.Module):\n    """"""Modified Aligned Xception\n    """"""\n\n    def __init__(self, num_classes=1000, in_chans=3, output_stride=32, norm_layer=nn.BatchNorm2d,\n                 norm_kwargs=None, drop_rate=0., global_pool=\'avg\'):\n        super(Xception71, self).__init__()\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        norm_kwargs = norm_kwargs if norm_kwargs is not None else {}\n        if output_stride == 32:\n            entry_block3_stride = 2\n            exit_block20_stride = 2\n            middle_block_dilation = 1\n            exit_block_dilations = (1, 1)\n        elif output_stride == 16:\n            entry_block3_stride = 2\n            exit_block20_stride = 1\n            middle_block_dilation = 1\n            exit_block_dilations = (1, 2)\n        elif output_stride == 8:\n            entry_block3_stride = 1\n            exit_block20_stride = 1\n            middle_block_dilation = 2\n            exit_block_dilations = (2, 4)\n        else:\n            raise NotImplementedError\n        \n        # Entry flow\n        self.conv1 = nn.Conv2d(in_chans, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = norm_layer(num_features=32, **norm_kwargs)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = norm_layer(num_features=64)\n\n        self.block1 = Block(\n            64, 128, num_reps=2, stride=2, norm_layer=norm_layer,\n            norm_kwargs=norm_kwargs, start_with_relu=False)\n        self.block2 = nn.Sequential(*[\n            Block(\n                128, 256, num_reps=2, stride=1, norm_layer=norm_layer,\n                norm_kwargs=norm_kwargs, start_with_relu=False, grow_first=True),\n            Block(\n                256, 256, num_reps=2, stride=2, norm_layer=norm_layer,\n                norm_kwargs=norm_kwargs, start_with_relu=False, grow_first=True),\n            Block(\n                256, 728, num_reps=2, stride=2, norm_layer=norm_layer,\n                norm_kwargs=norm_kwargs, start_with_relu=False, grow_first=True)])\n        self.block3 = Block(\n            728, 728, num_reps=2, stride=entry_block3_stride, norm_layer=norm_layer,\n            norm_kwargs=norm_kwargs, start_with_relu=True, grow_first=True, is_last=True)\n\n        # Middle flow\n        self.mid = nn.Sequential(OrderedDict([(\'block%d\' % i, Block(\n            728, 728, num_reps=3, stride=1, dilation=middle_block_dilation,\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs, start_with_relu=True, grow_first=True))\n                                              for i in range(4, 20)]))\n\n        # Exit flow\n        self.block20 = Block(\n            728, 1024, num_reps=2, stride=exit_block20_stride, dilation=exit_block_dilations[0],\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs, start_with_relu=True, grow_first=False, is_last=True)\n\n        self.conv3 = SeparableConv2d(\n            1024, 1536, 3, stride=1, dilation=exit_block_dilations[1],\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n        self.bn3 = norm_layer(num_features=1536, **norm_kwargs)\n\n        self.conv4 = SeparableConv2d(\n            1536, 1536, 3, stride=1, dilation=exit_block_dilations[1],\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n        self.bn4 = norm_layer(num_features=1536, **norm_kwargs)\n\n        self.num_features = 2048\n        self.conv5 = SeparableConv2d(\n            1536, self.num_features, 3, stride=1, dilation=exit_block_dilations[1],\n            norm_layer=norm_layer, norm_kwargs=norm_kwargs)\n        self.bn5 = norm_layer(num_features=self.num_features, **norm_kwargs)\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.fc = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n\n    def get_classifier(self):\n        return self.fc\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.num_classes = num_classes\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.fc = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes) if num_classes else None\n\n    def forward_features(self, x):\n        # Entry flow\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.block1(x)\n        # add relu here\n        x = self.relu(x)\n        # low_level_feat = x\n        x = self.block2(x)\n        # c2 = x\n        x = self.block3(x)\n\n        # Middle flow\n        x = self.mid(x)\n        # c3 = x\n\n        # Exit flow\n        x = self.block20(x)\n        x = self.relu(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.relu(x)\n\n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = self.relu(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x).flatten(1)\n        if self.drop_rate:\n            F.dropout(x, self.drop_rate, training=self.training)\n        x = self.fc(x)\n        return x\n\n\n@register_model\ndef gluon_xception65(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """""" Modified Aligned Xception-65\n    """"""\n    default_cfg = default_cfgs[\'gluon_xception65\']\n    model = Xception65(num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef gluon_xception71(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """""" Modified Aligned Xception-71\n    """"""\n    default_cfg = default_cfgs[\'gluon_xception71\']\n    model = Xception71(num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n'"
timm/models/helpers.py,4,"b'import torch\nimport torch.nn as nn\nfrom copy import deepcopy\nimport torch.utils.model_zoo as model_zoo\nimport os\nimport logging\nfrom collections import OrderedDict\nfrom timm.models.layers.conv2d_same import Conv2dSame\n\n\ndef load_state_dict(checkpoint_path, use_ema=False):\n    if checkpoint_path and os.path.isfile(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location=\'cpu\')\n        state_dict_key = \'state_dict\'\n        if isinstance(checkpoint, dict):\n            if use_ema and \'state_dict_ema\' in checkpoint:\n                state_dict_key = \'state_dict_ema\'\n        if state_dict_key and state_dict_key in checkpoint:\n            new_state_dict = OrderedDict()\n            for k, v in checkpoint[state_dict_key].items():\n                # strip `module.` prefix\n                name = k[7:] if k.startswith(\'module\') else k\n                new_state_dict[name] = v\n            state_dict = new_state_dict\n        else:\n            state_dict = checkpoint\n        logging.info(""Loaded {} from checkpoint \'{}\'"".format(state_dict_key, checkpoint_path))\n        return state_dict\n    else:\n        logging.error(""No checkpoint found at \'{}\'"".format(checkpoint_path))\n        raise FileNotFoundError()\n\n\ndef load_checkpoint(model, checkpoint_path, use_ema=False, strict=True):\n    state_dict = load_state_dict(checkpoint_path, use_ema)\n    model.load_state_dict(state_dict, strict=strict)\n\n\ndef resume_checkpoint(model, checkpoint_path):\n    other_state = {}\n    resume_epoch = None\n    if os.path.isfile(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location=\'cpu\')\n        if isinstance(checkpoint, dict) and \'state_dict\' in checkpoint:\n            new_state_dict = OrderedDict()\n            for k, v in checkpoint[\'state_dict\'].items():\n                name = k[7:] if k.startswith(\'module\') else k\n                new_state_dict[name] = v\n            model.load_state_dict(new_state_dict)\n            if \'optimizer\' in checkpoint:\n                other_state[\'optimizer\'] = checkpoint[\'optimizer\']\n            if \'amp\' in checkpoint:\n                other_state[\'amp\'] = checkpoint[\'amp\']\n            if \'epoch\' in checkpoint:\n                resume_epoch = checkpoint[\'epoch\']\n                if \'version\' in checkpoint and checkpoint[\'version\'] > 1:\n                    resume_epoch += 1  # start at the next epoch, old checkpoints incremented before save\n            logging.info(""Loaded checkpoint \'{}\' (epoch {})"".format(checkpoint_path, checkpoint[\'epoch\']))\n        else:\n            model.load_state_dict(checkpoint)\n            logging.info(""Loaded checkpoint \'{}\'"".format(checkpoint_path))\n        return other_state, resume_epoch\n    else:\n        logging.error(""No checkpoint found at \'{}\'"".format(checkpoint_path))\n        raise FileNotFoundError()\n\n\ndef load_pretrained(model, cfg=None, num_classes=1000, in_chans=3, filter_fn=None, strict=True):\n    if cfg is None:\n        cfg = getattr(model, \'default_cfg\')\n    if cfg is None or \'url\' not in cfg or not cfg[\'url\']:\n        logging.warning(""Pretrained model URL is invalid, using random initialization."")\n        return\n\n    state_dict = model_zoo.load_url(cfg[\'url\'], progress=False, map_location=\'cpu\')\n\n    if in_chans == 1:\n        conv1_name = cfg[\'first_conv\']\n        logging.info(\'Converting first conv (%s) from 3 to 1 channel\' % conv1_name)\n        conv1_weight = state_dict[conv1_name + \'.weight\']\n        state_dict[conv1_name + \'.weight\'] = conv1_weight.sum(dim=1, keepdim=True)\n    elif in_chans != 3:\n        assert False, ""Invalid in_chans for pretrained weights""\n\n    classifier_name = cfg[\'classifier\']\n    if num_classes == 1000 and cfg[\'num_classes\'] == 1001:\n        # special case for imagenet trained models with extra background class in pretrained weights\n        classifier_weight = state_dict[classifier_name + \'.weight\']\n        state_dict[classifier_name + \'.weight\'] = classifier_weight[1:]\n        classifier_bias = state_dict[classifier_name + \'.bias\']\n        state_dict[classifier_name + \'.bias\'] = classifier_bias[1:]\n    elif num_classes != cfg[\'num_classes\']:\n        # completely discard fully connected for all other differences between pretrained and created model\n        del state_dict[classifier_name + \'.weight\']\n        del state_dict[classifier_name + \'.bias\']\n        strict = False\n\n    if filter_fn is not None:\n        state_dict = filter_fn(state_dict)\n\n    model.load_state_dict(state_dict, strict=strict)\n\n\ndef extract_layer(model, layer):\n    layer = layer.split(\'.\')\n    module = model\n    if hasattr(model, \'module\') and layer[0] != \'module\':\n        module = model.module\n    if not hasattr(model, \'module\') and layer[0] == \'module\':\n        layer = layer[1:]\n    for l in layer:\n        if hasattr(module, l):\n            if not l.isdigit():\n                module = getattr(module, l)\n            else:\n                module = module[int(l)]\n        else:\n            return module\n    return module\n\n\ndef set_layer(model, layer, val):\n    layer = layer.split(\'.\')\n    module = model\n    if hasattr(model, \'module\') and layer[0] != \'module\':\n        module = model.module\n    lst_index = 0\n    module2 = module\n    for l in layer:\n        if hasattr(module2, l):\n            if not l.isdigit():\n                module2 = getattr(module2, l)\n            else:\n                module2 = module2[int(l)]\n            lst_index += 1\n    lst_index -= 1\n    for l in layer[:lst_index]:\n        if not l.isdigit():\n            module = getattr(module, l)\n        else:\n            module = module[int(l)]\n    l = layer[lst_index]\n    setattr(module, l, val)\n\n\ndef adapt_model_from_string(parent_module, model_string):\n    separator = \'***\'\n    state_dict = {}\n    lst_shape = model_string.split(separator)\n    for k in lst_shape:\n        k = k.split(\':\')\n        key = k[0]\n        shape = k[1][1:-1].split(\',\')\n        if shape[0] != \'\':\n            state_dict[key] = [int(i) for i in shape]\n\n    new_module = deepcopy(parent_module)\n    for n, m in parent_module.named_modules():\n        old_module = extract_layer(parent_module, n)\n        if isinstance(old_module, nn.Conv2d) or isinstance(old_module, Conv2dSame):\n            if isinstance(old_module, Conv2dSame):\n                conv = Conv2dSame\n            else:\n                conv = nn.Conv2d\n            s = state_dict[n + \'.weight\']\n            in_channels = s[1]\n            out_channels = s[0]\n            g = 1\n            if old_module.groups > 1:\n                in_channels = out_channels\n                g = in_channels\n            new_conv = conv(\n                in_channels=in_channels, out_channels=out_channels, kernel_size=old_module.kernel_size,\n                bias=old_module.bias is not None, padding=old_module.padding, dilation=old_module.dilation,\n                groups=g, stride=old_module.stride)\n            set_layer(new_module, n, new_conv)\n        if isinstance(old_module, nn.BatchNorm2d):\n            new_bn = nn.BatchNorm2d(\n                num_features=state_dict[n + \'.weight\'][0], eps=old_module.eps, momentum=old_module.momentum,\n                affine=old_module.affine, track_running_stats=True)\n            set_layer(new_module, n, new_bn)\n        if isinstance(old_module, nn.Linear):\n            new_fc = nn.Linear(\n                in_features=state_dict[n + \'.weight\'][1], out_features=old_module.out_features,\n                bias=old_module.bias is not None)\n            set_layer(new_module, n, new_fc)\n    new_module.eval()\n    parent_module.eval()\n\n    return new_module\n\n\ndef adapt_model_from_file(parent_module, model_variant):\n    adapt_file = os.path.join(os.path.dirname(__file__), \'pruned\', model_variant + \'.txt\')\n    with open(adapt_file, \'r\') as f:\n        return adapt_model_from_string(parent_module, f.read().strip())\n'"
timm/models/hrnet.py,2,"b'"""""" HRNet\n\nCopied from https://github.com/HRNet/HRNet-Image-Classification\n\nOriginal header:\n  Copyright (c) Microsoft\n  Licensed under the MIT License.\n  Written by Bin Xiao (Bin.Xiao@microsoft.com)\n  Modified by Ke Sun (sunk@mail.ustc.edu.cn)\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom .helpers import load_pretrained\nfrom .layers import SelectAdaptivePool2d\nfrom .registry import register_model\nfrom .resnet import BasicBlock, Bottleneck  # leveraging ResNet blocks w/ additional features like SE\n\n_BN_MOMENTUM = 0.1\nlogger = logging.getLogger(__name__)\n\n\ndef _cfg(url=\'\', **kwargs):\n    return {\n        \'url\': url,\n        \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (7, 7),\n        \'crop_pct\': 0.875, \'interpolation\': \'bilinear\',\n        \'mean\': IMAGENET_DEFAULT_MEAN, \'std\': IMAGENET_DEFAULT_STD,\n        \'first_conv\': \'conv1\', \'classifier\': \'classifier\',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    \'hrnet_w18_small\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnet_w18_small_v1-f460c6bc.pth\'),\n    \'hrnet_w18_small_v2\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnet_w18_small_v2-4c50a8cb.pth\'),\n    \'hrnet_w18\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w18-8cb57bb9.pth\'),\n    \'hrnet_w30\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w30-8d7f8dab.pth\'),\n    \'hrnet_w32\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w32-90d8c5fb.pth\'),\n    \'hrnet_w40\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w40-7cd397a4.pth\'),\n    \'hrnet_w44\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w44-c9ac8c18.pth\'),\n    \'hrnet_w48\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w48-abd2e6ab.pth\'),\n    \'hrnet_w64\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-hrnet/hrnetv2_w64-b47cc881.pth\'),\n}\n\ncfg_cls = dict(\n    hrnet_w18_small=dict(\n        STEM_WIDTH=64,\n        STAGE1=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=1,\n            BLOCK=\'BOTTLENECK\',\n            NUM_BLOCKS=(1,),\n            NUM_CHANNELS=(32,),\n            FUSE_METHOD=\'SUM\',\n        ),\n        STAGE2=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=2,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(2, 2),\n            NUM_CHANNELS=(16, 32),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE3=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=3,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(2, 2, 2),\n            NUM_CHANNELS=(16, 32, 64),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE4=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=4,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(2, 2, 2, 2),\n            NUM_CHANNELS=(16, 32, 64, 128),\n            FUSE_METHOD=\'SUM\',\n        ),\n    ),\n\n    hrnet_w18_small_v2=dict(\n        STEM_WIDTH=64,\n        STAGE1=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=1,\n            BLOCK=\'BOTTLENECK\',\n            NUM_BLOCKS=(2,),\n            NUM_CHANNELS=(64,),\n            FUSE_METHOD=\'SUM\',\n        ),\n        STAGE2=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=2,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(2, 2),\n            NUM_CHANNELS=(18, 36),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE3=dict(\n            NUM_MODULES=3,\n            NUM_BRANCHES=3,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(2, 2, 2),\n            NUM_CHANNELS=(18, 36, 72),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE4=dict(\n            NUM_MODULES=2,\n            NUM_BRANCHES=4,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(2, 2, 2, 2),\n            NUM_CHANNELS=(18, 36, 72, 144),\n            FUSE_METHOD=\'SUM\',\n        ),\n    ),\n\n    hrnet_w18=dict(\n        STEM_WIDTH=64,\n        STAGE1=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=1,\n            BLOCK=\'BOTTLENECK\',\n            NUM_BLOCKS=(4,),\n            NUM_CHANNELS=(64,),\n            FUSE_METHOD=\'SUM\',\n        ),\n        STAGE2=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=2,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4),\n            NUM_CHANNELS=(18, 36),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE3=dict(\n            NUM_MODULES=4,\n            NUM_BRANCHES=3,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4, 4),\n            NUM_CHANNELS=(18, 36, 72),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE4=dict(\n            NUM_MODULES=3,\n            NUM_BRANCHES=4,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4, 4, 4),\n            NUM_CHANNELS=(18, 36, 72, 144),\n            FUSE_METHOD=\'SUM\',\n        ),\n    ),\n\n    hrnet_w30=dict(\n        STEM_WIDTH=64,\n        STAGE1=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=1,\n            BLOCK=\'BOTTLENECK\',\n            NUM_BLOCKS=(4,),\n            NUM_CHANNELS=(64,),\n            FUSE_METHOD=\'SUM\',\n        ),\n        STAGE2=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=2,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4),\n            NUM_CHANNELS=(30, 60),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE3=dict(\n            NUM_MODULES=4,\n            NUM_BRANCHES=3,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4, 4),\n            NUM_CHANNELS=(30, 60, 120),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE4=dict(\n            NUM_MODULES=3,\n            NUM_BRANCHES=4,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4, 4, 4),\n            NUM_CHANNELS=(30, 60, 120, 240),\n            FUSE_METHOD=\'SUM\',\n        ),\n    ),\n\n    hrnet_w32=dict(\n        STEM_WIDTH=64,\n        STAGE1=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=1,\n            BLOCK=\'BOTTLENECK\',\n            NUM_BLOCKS=(4,),\n            NUM_CHANNELS=(64,),\n            FUSE_METHOD=\'SUM\',\n        ),\n        STAGE2=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=2,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4),\n            NUM_CHANNELS=(32, 64),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE3=dict(\n            NUM_MODULES=4,\n            NUM_BRANCHES=3,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4, 4),\n            NUM_CHANNELS=(32, 64, 128),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE4=dict(\n            NUM_MODULES=3,\n            NUM_BRANCHES=4,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4, 4, 4),\n            NUM_CHANNELS=(32, 64, 128, 256),\n            FUSE_METHOD=\'SUM\',\n        ),\n    ),\n\n    hrnet_w40=dict(\n        STEM_WIDTH=64,\n        STAGE1=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=1,\n            BLOCK=\'BOTTLENECK\',\n            NUM_BLOCKS=(4,),\n            NUM_CHANNELS=(64,),\n            FUSE_METHOD=\'SUM\',\n        ),\n        STAGE2=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=2,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4),\n            NUM_CHANNELS=(40, 80),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE3=dict(\n            NUM_MODULES=4,\n            NUM_BRANCHES=3,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4, 4),\n            NUM_CHANNELS=(40, 80, 160),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE4=dict(\n            NUM_MODULES=3,\n            NUM_BRANCHES=4,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4, 4, 4),\n            NUM_CHANNELS=(40, 80, 160, 320),\n            FUSE_METHOD=\'SUM\',\n        ),\n    ),\n\n    hrnet_w44=dict(\n        STEM_WIDTH=64,\n        STAGE1=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=1,\n            BLOCK=\'BOTTLENECK\',\n            NUM_BLOCKS=(4,),\n            NUM_CHANNELS=(64,),\n            FUSE_METHOD=\'SUM\',\n        ),\n        STAGE2=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=2,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4),\n            NUM_CHANNELS=(44, 88),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE3=dict(\n            NUM_MODULES=4,\n            NUM_BRANCHES=3,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4, 4),\n            NUM_CHANNELS=(44, 88, 176),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE4=dict(\n            NUM_MODULES=3,\n            NUM_BRANCHES=4,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4, 4, 4),\n            NUM_CHANNELS=(44, 88, 176, 352),\n            FUSE_METHOD=\'SUM\',\n        ),\n    ),\n\n    hrnet_w48=dict(\n        STEM_WIDTH=64,\n        STAGE1=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=1,\n            BLOCK=\'BOTTLENECK\',\n            NUM_BLOCKS=(4,),\n            NUM_CHANNELS=(64,),\n            FUSE_METHOD=\'SUM\',\n        ),\n        STAGE2=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=2,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4),\n            NUM_CHANNELS=(48, 96),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE3=dict(\n            NUM_MODULES=4,\n            NUM_BRANCHES=3,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4, 4),\n            NUM_CHANNELS=(48, 96, 192),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE4=dict(\n            NUM_MODULES=3,\n            NUM_BRANCHES=4,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4, 4, 4),\n            NUM_CHANNELS=(48, 96, 192, 384),\n            FUSE_METHOD=\'SUM\',\n        ),\n    ),\n\n    hrnet_w64=dict(\n        STEM_WIDTH=64,\n        STAGE1=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=1,\n            BLOCK=\'BOTTLENECK\',\n            NUM_BLOCKS=(4,),\n            NUM_CHANNELS=(64,),\n            FUSE_METHOD=\'SUM\',\n        ),\n        STAGE2=dict(\n            NUM_MODULES=1,\n            NUM_BRANCHES=2,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4),\n            NUM_CHANNELS=(64, 128),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE3=dict(\n            NUM_MODULES=4,\n            NUM_BRANCHES=3,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4, 4),\n            NUM_CHANNELS=(64, 128, 256),\n            FUSE_METHOD=\'SUM\'\n        ),\n        STAGE4=dict(\n            NUM_MODULES=3,\n            NUM_BRANCHES=4,\n            BLOCK=\'BASIC\',\n            NUM_BLOCKS=(4, 4, 4, 4),\n            NUM_CHANNELS=(64, 128, 256, 512),\n            FUSE_METHOD=\'SUM\',\n        ),\n    )\n)\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n                 num_channels, fuse_method, multi_scale_output=True):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(\n            num_branches, blocks, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(False)\n\n    def _check_branches(self, num_branches, blocks, num_blocks, num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_BLOCKS({})\'.format(\n                num_branches, len(num_blocks))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_CHANNELS({})\'.format(\n                num_branches, len(num_channels))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_INCHANNELS({})\'.format(\n                num_branches, len(num_inchannels))\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n                self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(\n                    self.num_inchannels[branch_index], num_channels[branch_index] * block.expansion,\n                    kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(num_channels[branch_index] * block.expansion, momentum=_BN_MOMENTUM),\n            )\n\n        layers = [block(self.num_inchannels[branch_index], num_channels[branch_index], stride, downsample)]\n        self.num_inchannels[branch_index] = num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index]))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n        for i in range(num_branches):\n            branches.append(self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return nn.Identity()\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(nn.Sequential(\n                        nn.Conv2d(num_inchannels[j], num_inchannels[i], 1, 1, 0, bias=False),\n                        nn.BatchNorm2d(num_inchannels[i], momentum=_BN_MOMENTUM),\n                        nn.Upsample(scale_factor=2 ** (j - i), mode=\'nearest\')))\n                elif j == i:\n                    fuse_layer.append(nn.Identity())\n                else:\n                    conv3x3s = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),\n                                nn.BatchNorm2d(num_outchannels_conv3x3, momentum=_BN_MOMENTUM)))\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),\n                                nn.BatchNorm2d(num_outchannels_conv3x3, momentum=_BN_MOMENTUM),\n                                nn.ReLU(False)))\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nblocks_dict = {\n    \'BASIC\': BasicBlock,\n    \'BOTTLENECK\': Bottleneck\n}\n\n\nclass HighResolutionNet(nn.Module):\n\n    def __init__(self, cfg, in_chans=3, num_classes=1000, global_pool=\'avg\', drop_rate=0.0):\n        super(HighResolutionNet, self).__init__()\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n\n        stem_width = cfg[\'STEM_WIDTH\']\n        self.conv1 = nn.Conv2d(in_chans, stem_width, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(stem_width, momentum=_BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(stem_width, 64, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(64, momentum=_BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.stage1_cfg = cfg[\'STAGE1\']\n        num_channels = self.stage1_cfg[\'NUM_CHANNELS\'][0]\n        block = blocks_dict[self.stage1_cfg[\'BLOCK\']]\n        num_blocks = self.stage1_cfg[\'NUM_BLOCKS\'][0]\n        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n        stage1_out_channel = block.expansion * num_channels\n\n        self.stage2_cfg = cfg[\'STAGE2\']\n        num_channels = self.stage2_cfg[\'NUM_CHANNELS\']\n        block = blocks_dict[self.stage2_cfg[\'BLOCK\']]\n        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition1 = self._make_transition_layer([stage1_out_channel], num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(self.stage2_cfg, num_channels)\n\n        self.stage3_cfg = cfg[\'STAGE3\']\n        num_channels = self.stage3_cfg[\'NUM_CHANNELS\']\n        block = blocks_dict[self.stage3_cfg[\'BLOCK\']]\n        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition2 = self._make_transition_layer(pre_stage_channels, num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(self.stage3_cfg, num_channels)\n\n        self.stage4_cfg = cfg[\'STAGE4\']\n        num_channels = self.stage4_cfg[\'NUM_CHANNELS\']\n        block = blocks_dict[self.stage4_cfg[\'BLOCK\']]\n        num_channels = [num_channels[i] * block.expansion for i in range(len(num_channels))]\n        self.transition3 = self._make_transition_layer(pre_stage_channels, num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(self.stage4_cfg, num_channels, multi_scale_output=True)\n\n        # Classification Head\n        self.num_features = 2048\n        self.incre_modules, self.downsamp_modules, self.final_layer = self._make_head(pre_stage_channels)\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.classifier = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n\n        self.init_weights()\n\n    def _make_head(self, pre_stage_channels):\n        head_block = Bottleneck\n        head_channels = [32, 64, 128, 256]\n\n        # Increasing the #channels on each resolution\n        # from C, 2C, 4C, 8C to 128, 256, 512, 1024\n        incre_modules = []\n        for i, channels in enumerate(pre_stage_channels):\n            incre_modules.append(\n                self._make_layer(head_block, channels, head_channels[i], 1, stride=1))\n        incre_modules = nn.ModuleList(incre_modules)\n\n        # downsampling modules\n        downsamp_modules = []\n        for i in range(len(pre_stage_channels) - 1):\n            in_channels = head_channels[i] * head_block.expansion\n            out_channels = head_channels[i + 1] * head_block.expansion\n            downsamp_module = nn.Sequential(\n                nn.Conv2d(\n                    in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=2, padding=1),\n                nn.BatchNorm2d(out_channels, momentum=_BN_MOMENTUM),\n                nn.ReLU(inplace=True)\n            )\n            downsamp_modules.append(downsamp_module)\n        downsamp_modules = nn.ModuleList(downsamp_modules)\n\n        final_layer = nn.Sequential(\n            nn.Conv2d(\n                in_channels=head_channels[3] * head_block.expansion,\n                out_channels=self.num_features, kernel_size=1, stride=1, padding=0\n            ),\n            nn.BatchNorm2d(self.num_features, momentum=_BN_MOMENTUM),\n            nn.ReLU(inplace=True)\n        )\n\n        return incre_modules, downsamp_modules, final_layer\n\n    def _make_transition_layer(self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(nn.Sequential(\n                        nn.Conv2d(num_channels_pre_layer[i], num_channels_cur_layer[i], 3, 1, 1, bias=False),\n                        nn.BatchNorm2d(num_channels_cur_layer[i], momentum=_BN_MOMENTUM),\n                        nn.ReLU(inplace=True)))\n                else:\n                    transition_layers.append(nn.Identity())\n            else:\n                conv3x3s = []\n                for j in range(i + 1 - num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] if j == i - num_branches_pre else inchannels\n                    conv3x3s.append(nn.Sequential(\n                        nn.Conv2d(inchannels, outchannels, 3, 2, 1, bias=False),\n                        nn.BatchNorm2d(outchannels, momentum=_BN_MOMENTUM),\n                        nn.ReLU(inplace=True)))\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion, momentum=_BN_MOMENTUM),\n            )\n\n        layers = [block(inplanes, planes, stride, downsample)]\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, num_inchannels, multi_scale_output=True):\n        num_modules = layer_config[\'NUM_MODULES\']\n        num_branches = layer_config[\'NUM_BRANCHES\']\n        num_blocks = layer_config[\'NUM_BLOCKS\']\n        num_channels = layer_config[\'NUM_CHANNELS\']\n        block = blocks_dict[layer_config[\'BLOCK\']]\n        fuse_method = layer_config[\'FUSE_METHOD\']\n\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n\n            modules.append(HighResolutionModule(\n                num_branches, block, num_blocks, num_inchannels, num_channels, fuse_method, reset_multi_scale_output)\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def get_classifier(self):\n        return self.classifier\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.num_classes = num_classes\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        num_features = self.num_features * self.global_pool.feat_mult()\n        if num_classes:\n            self.classifier = nn.Linear(num_features, num_classes)\n        else:\n            self.classifier = nn.Identity()\n\n    def forward_features(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(len(self.transition1)):\n            x_list.append(self.transition1[i](x))\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(len(self.transition2)):\n            if not isinstance(self.transition2[i], nn.Identity):\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(len(self.transition3)):\n            if not isinstance(self.transition3[i], nn.Identity):\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage4(x_list)\n\n        # Classification Head\n        y = self.incre_modules[0](y_list[0])\n        for i in range(len(self.downsamp_modules)):\n            y = self.incre_modules[i + 1](y_list[i + 1]) + self.downsamp_modules[i](y)\n        y = self.final_layer(y)\n        return y\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x).flatten(1)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        x = self.classifier(x)\n        return x\n\n\ndef _create_model(variant, pretrained, model_kwargs):\n    if model_kwargs.pop(\'features_only\', False):\n        assert False, \'Not Implemented\'  # TODO\n        load_strict = False\n        model_kwargs.pop(\'num_classes\', 0)\n        model_class = HighResolutionNet\n    else:\n        load_strict = True\n        model_class = HighResolutionNet\n\n    model = model_class(cfg_cls[variant], **model_kwargs)\n    model.default_cfg = default_cfgs[variant]\n    if pretrained:\n        load_pretrained(\n            model,\n            num_classes=model_kwargs.get(\'num_classes\', 0),\n            in_chans=model_kwargs.get(\'in_chans\', 3),\n            strict=load_strict)\n    return model\n\n\n@register_model\ndef hrnet_w18_small(pretrained=True, **kwargs):\n    return _create_model(\'hrnet_w18_small\', pretrained, kwargs)\n\n\n@register_model\ndef hrnet_w18_small_v2(pretrained=True, **kwargs):\n    return _create_model(\'hrnet_w18_small_v2\', pretrained, kwargs)\n\n\n@register_model\ndef hrnet_w18(pretrained=True, **kwargs):\n    return _create_model(\'hrnet_w18\', pretrained, kwargs)\n\n\n@register_model\ndef hrnet_w30(pretrained=True, **kwargs):\n    return _create_model(\'hrnet_w30\', pretrained, kwargs)\n\n\n@register_model\ndef hrnet_w32(pretrained=True, **kwargs):\n    return _create_model(\'hrnet_w32\', pretrained, kwargs)\n\n\n@register_model\ndef hrnet_w40(pretrained=True, **kwargs):\n    return _create_model(\'hrnet_w40\', pretrained, kwargs)\n\n\n@register_model\ndef hrnet_w44(pretrained=True, **kwargs):\n    return _create_model(\'hrnet_w44\', pretrained, kwargs)\n\n\n@register_model\ndef hrnet_w48(pretrained=True, **kwargs):\n    return _create_model(\'hrnet_w48\', pretrained, kwargs)\n\n\n@register_model\ndef hrnet_w64(pretrained=True, **kwargs):\n    return _create_model(\'hrnet_w64\', pretrained, kwargs)\n'"
timm/models/inception_resnet_v2.py,8,"b'"""""" Pytorch Inception-Resnet-V2 implementation\nSourced from https://github.com/Cadene/tensorflow-model-zoo.torch (MIT License) which is\nbased upon Google\'s Tensorflow implementation and pretrained weights (Apache 2.0 License)\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SelectAdaptivePool2d\nfrom timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n\n__all__ = [\'InceptionResnetV2\']\n\ndefault_cfgs = {\n    # ported from http://download.tensorflow.org/models/inception_resnet_v2_2016_08_30.tar.gz\n    \'inception_resnet_v2\': {\n        \'url\': \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/inception_resnet_v2-940b1cd6.pth\',\n        \'num_classes\': 1001, \'input_size\': (3, 299, 299), \'pool_size\': (8, 8),\n        \'crop_pct\': 0.8975, \'interpolation\': \'bicubic\',\n        \'mean\': IMAGENET_INCEPTION_MEAN, \'std\': IMAGENET_INCEPTION_STD,\n        \'first_conv\': \'conv2d_1a.conv\', \'classifier\': \'classif\',\n    },\n    # ported from http://download.tensorflow.org/models/ens_adv_inception_resnet_v2_2017_08_18.tar.gz\n    \'ens_adv_inception_resnet_v2\': {\n        \'url\': \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ens_adv_inception_resnet_v2-2592a550.pth\',\n        \'num_classes\': 1001, \'input_size\': (3, 299, 299), \'pool_size\': (8, 8),\n        \'crop_pct\': 0.8975, \'interpolation\': \'bicubic\',\n        \'mean\': IMAGENET_INCEPTION_MEAN, \'std\': IMAGENET_INCEPTION_STD,\n        \'first_conv\': \'conv2d_1a.conv\', \'classifier\': \'classif\',\n    }\n}\n\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(\n            in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_planes, eps=.001)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Mixed_5b(nn.Module):\n    def __init__(self):\n        super(Mixed_5b, self).__init__()\n\n        self.branch0 = BasicConv2d(192, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(192, 48, kernel_size=1, stride=1),\n            BasicConv2d(48, 64, kernel_size=5, stride=1, padding=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(192, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(192, 64, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block35(nn.Module):\n    def __init__(self, scale=1.0):\n        super(Block35, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(320, 32, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 32, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(320, 32, kernel_size=1, stride=1),\n            BasicConv2d(32, 48, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(48, 64, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.conv2d = nn.Conv2d(128, 320, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_6a(nn.Module):\n    def __init__(self):\n        super(Mixed_6a, self).__init__()\n\n        self.branch0 = BasicConv2d(320, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(320, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Block17(nn.Module):\n    def __init__(self, scale=1.0):\n        super(Block17, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(1088, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 128, kernel_size=1, stride=1),\n            BasicConv2d(128, 160, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(160, 192, kernel_size=(7, 1), stride=1, padding=(3, 0))\n        )\n\n        self.conv2d = nn.Conv2d(384, 1088, kernel_size=1, stride=1)\n        self.relu = nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        out = self.relu(out)\n        return out\n\n\nclass Mixed_7a(nn.Module):\n    def __init__(self):\n        super(Mixed_7a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 384, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1088, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 288, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(288, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch3 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Block8(nn.Module):\n    __constants__ = [\'relu\']  # for pre 1.4 torchscript compat\n\n    def __init__(self, scale=1.0, no_relu=False):\n        super(Block8, self).__init__()\n\n        self.scale = scale\n\n        self.branch0 = BasicConv2d(2080, 192, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(2080, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1, 3), stride=1, padding=(0, 1)),\n            BasicConv2d(224, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))\n        )\n\n        self.conv2d = nn.Conv2d(448, 2080, kernel_size=1, stride=1)\n        self.relu = None if no_relu else nn.ReLU(inplace=False)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        out = self.conv2d(out)\n        out = out * self.scale + x\n        if self.relu is not None:\n            out = self.relu(out)\n        return out\n\n\nclass InceptionResnetV2(nn.Module):\n    def __init__(self, num_classes=1001, in_chans=3, drop_rate=0., global_pool=\'avg\'):\n        super(InceptionResnetV2, self).__init__()\n        self.drop_rate = drop_rate\n        self.num_classes = num_classes\n        self.num_features = 1536\n\n        self.conv2d_1a = BasicConv2d(in_chans, 32, kernel_size=3, stride=2)\n        self.conv2d_2a = BasicConv2d(32, 32, kernel_size=3, stride=1)\n        self.conv2d_2b = BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.maxpool_3a = nn.MaxPool2d(3, stride=2)\n        self.conv2d_3b = BasicConv2d(64, 80, kernel_size=1, stride=1)\n        self.conv2d_4a = BasicConv2d(80, 192, kernel_size=3, stride=1)\n        self.maxpool_5a = nn.MaxPool2d(3, stride=2)\n        self.mixed_5b = Mixed_5b()\n        self.repeat = nn.Sequential(\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17),\n            Block35(scale=0.17)\n        )\n        self.mixed_6a = Mixed_6a()\n        self.repeat_1 = nn.Sequential(\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10),\n            Block17(scale=0.10)\n        )\n        self.mixed_7a = Mixed_7a()\n        self.repeat_2 = nn.Sequential(\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20),\n            Block8(scale=0.20)\n        )\n        self.block8 = Block8(no_relu=True)\n        self.conv2d_7b = BasicConv2d(2080, self.num_features, kernel_size=1, stride=1)\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        # NOTE some variants/checkpoints for this model may have \'last_linear\' as the name for the FC\n        self.classif = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n\n    def get_classifier(self):\n        return self.classif\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.num_classes = num_classes\n        self.classif = nn.Linear(\n            self.num_features * self.global_pool.feat_mult(), num_classes) if num_classes else None\n\n    def forward_features(self, x):\n        x = self.conv2d_1a(x)\n        x = self.conv2d_2a(x)\n        x = self.conv2d_2b(x)\n        x = self.maxpool_3a(x)\n        x = self.conv2d_3b(x)\n        x = self.conv2d_4a(x)\n        x = self.maxpool_5a(x)\n        x = self.mixed_5b(x)\n        x = self.repeat(x)\n        x = self.mixed_6a(x)\n        x = self.repeat_1(x)\n        x = self.mixed_7a(x)\n        x = self.repeat_2(x)\n        x = self.block8(x)\n        x = self.conv2d_7b(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x).flatten(1)\n        if self.drop_rate > 0:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        x = self.classif(x)\n        return x\n\n\n@register_model\ndef inception_resnet_v2(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    r""""""InceptionResnetV2 model architecture from the\n    `""InceptionV4, Inception-ResNet..."" <https://arxiv.org/abs/1602.07261>` paper.\n    """"""\n    default_cfg = default_cfgs[\'inception_resnet_v2\']\n    model = InceptionResnetV2(num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n\n    return model\n\n\n@register_model\ndef ens_adv_inception_resnet_v2(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    r"""""" Ensemble Adversarially trained InceptionResnetV2 model architecture\n    As per https://arxiv.org/abs/1705.07204 and\n    https://github.com/tensorflow/models/tree/master/research/adv_imagenet_models.\n    """"""\n    default_cfg = default_cfgs[\'ens_adv_inception_resnet_v2\']\n    model = InceptionResnetV2(num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n\n    return model\n'"
timm/models/inception_v3.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import trunc_normal_, SelectAdaptivePool2d\nfrom timm.data import IMAGENET_DEFAULT_STD, IMAGENET_DEFAULT_MEAN, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n\n__all__ = []\n\ndef _cfg(url=\'\', **kwargs):\n    return {\n        \'url\': url,\n        \'num_classes\': 1000, \'input_size\': (3, 299, 299), \'pool_size\': (8, 8),\n        \'crop_pct\': 0.875, \'interpolation\': \'bicubic\',\n        \'mean\': IMAGENET_INCEPTION_MEAN, \'std\': IMAGENET_INCEPTION_STD,\n        \'first_conv\': \'Conv2d_1a_3x3\', \'classifier\': \'fc\',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    # original PyTorch weights, ported from Tensorflow but modified\n    \'inception_v3\': _cfg(\n        url=\'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\',\n        has_aux=True),  # checkpoint has aux logit layer weights\n    # my port of Tensorflow SLIM weights (http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz)\n    \'tf_inception_v3\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_inception_v3-e0069de4.pth\',\n        num_classes=1001, has_aux=False),\n    # my port of Tensorflow adversarially trained Inception V3 from\n    # http://download.tensorflow.org/models/adv_inception_v3_2017_08_18.tar.gz\n    \'adv_inception_v3\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/adv_inception_v3-9e27bd63.pth\',\n        num_classes=1001, has_aux=False),\n    # from gluon pretrained models, best performing in terms of accuracy/loss metrics\n    # https://gluon-cv.mxnet.io/model_zoo/classification.html\n    \'gluon_inception_v3\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gluon_inception_v3-9f746940.pth\',\n        mean=IMAGENET_DEFAULT_MEAN,  # also works well with inception defaults\n        std=IMAGENET_DEFAULT_STD,  # also works well with inception defaults\n        has_aux=False,\n    )\n}\n\n\nclass InceptionV3Aux(nn.Module):\n    """"""InceptionV3 with AuxLogits\n    """"""\n\n    def __init__(self, inception_blocks=None, num_classes=1000, in_chans=3, drop_rate=0., global_pool=\'avg\'):\n        super(InceptionV3Aux, self).__init__()\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n\n        if inception_blocks is None:\n            inception_blocks = [\n                BasicConv2d, InceptionA, InceptionB, InceptionC,\n                InceptionD, InceptionE, InceptionAux\n            ]\n        assert len(inception_blocks) == 7\n        conv_block = inception_blocks[0]\n        inception_a = inception_blocks[1]\n        inception_b = inception_blocks[2]\n        inception_c = inception_blocks[3]\n        inception_d = inception_blocks[4]\n        inception_e = inception_blocks[5]\n        inception_aux = inception_blocks[6]\n\n        self.Conv2d_1a_3x3 = conv_block(in_chans, 32, kernel_size=3, stride=2)\n        self.Conv2d_2a_3x3 = conv_block(32, 32, kernel_size=3)\n        self.Conv2d_2b_3x3 = conv_block(32, 64, kernel_size=3, padding=1)\n        self.Conv2d_3b_1x1 = conv_block(64, 80, kernel_size=1)\n        self.Conv2d_4a_3x3 = conv_block(80, 192, kernel_size=3)\n        self.Mixed_5b = inception_a(192, pool_features=32)\n        self.Mixed_5c = inception_a(256, pool_features=64)\n        self.Mixed_5d = inception_a(288, pool_features=64)\n        self.Mixed_6a = inception_b(288)\n        self.Mixed_6b = inception_c(768, channels_7x7=128)\n        self.Mixed_6c = inception_c(768, channels_7x7=160)\n        self.Mixed_6d = inception_c(768, channels_7x7=160)\n        self.Mixed_6e = inception_c(768, channels_7x7=192)\n        self.AuxLogits = inception_aux(768, num_classes)\n        self.Mixed_7a = inception_d(768)\n        self.Mixed_7b = inception_e(1280)\n        self.Mixed_7c = inception_e(2048)\n\n        self.num_features = 2048\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.fc = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                stddev = m.stddev if hasattr(m, \'stddev\') else 0.1\n                trunc_normal_(m.weight, std=stddev)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward_features(self, x):\n        # N x 3 x 299 x 299\n        x = self.Conv2d_1a_3x3(x)\n        # N x 32 x 149 x 149\n        x = self.Conv2d_2a_3x3(x)\n        # N x 32 x 147 x 147\n        x = self.Conv2d_2b_3x3(x)\n        # N x 64 x 147 x 147\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # N x 64 x 73 x 73\n        x = self.Conv2d_3b_1x1(x)\n        # N x 80 x 73 x 73\n        x = self.Conv2d_4a_3x3(x)\n        # N x 192 x 71 x 71\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # N x 192 x 35 x 35\n        x = self.Mixed_5b(x)\n        # N x 256 x 35 x 35\n        x = self.Mixed_5c(x)\n        # N x 288 x 35 x 35\n        x = self.Mixed_5d(x)\n        # N x 288 x 35 x 35\n        x = self.Mixed_6a(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6b(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6c(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6d(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6e(x)\n        # N x 768 x 17 x 17\n        aux = self.AuxLogits(x) if self.training else None\n        # N x 768 x 17 x 17\n        x = self.Mixed_7a(x)\n        # N x 1280 x 8 x 8\n        x = self.Mixed_7b(x)\n        # N x 2048 x 8 x 8\n        x = self.Mixed_7c(x)\n        # N x 2048 x 8 x 8\n        return x, aux\n\n    def get_classifier(self):\n        return self.fc\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.num_classes = num_classes\n        if self.num_classes > 0:\n            self.fc = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n        else:\n            self.fc = nn.Identity()\n\n    def forward(self, x):\n        x, aux = self.forward_features(x)\n        x = self.global_pool(x).flatten(1)\n        if self.drop_rate > 0:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        x = self.fc(x)\n        return x, aux\n\n\nclass InceptionV3(nn.Module):\n    """"""Inception-V3 with no AuxLogits\n    FIXME two class defs are redundant, but less screwing around with torchsript fussyness and inconsistent returns\n    """"""\n\n    def __init__(self, inception_blocks=None, num_classes=1000, in_chans=3, drop_rate=0., global_pool=\'avg\'):\n        super(InceptionV3, self).__init__()\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n\n        if inception_blocks is None:\n            inception_blocks = [\n                BasicConv2d, InceptionA, InceptionB, InceptionC, InceptionD, InceptionE]\n        assert len(inception_blocks) >= 6\n        conv_block = inception_blocks[0]\n        inception_a = inception_blocks[1]\n        inception_b = inception_blocks[2]\n        inception_c = inception_blocks[3]\n        inception_d = inception_blocks[4]\n        inception_e = inception_blocks[5]\n\n        self.Conv2d_1a_3x3 = conv_block(in_chans, 32, kernel_size=3, stride=2)\n        self.Conv2d_2a_3x3 = conv_block(32, 32, kernel_size=3)\n        self.Conv2d_2b_3x3 = conv_block(32, 64, kernel_size=3, padding=1)\n        self.Conv2d_3b_1x1 = conv_block(64, 80, kernel_size=1)\n        self.Conv2d_4a_3x3 = conv_block(80, 192, kernel_size=3)\n        self.Mixed_5b = inception_a(192, pool_features=32)\n        self.Mixed_5c = inception_a(256, pool_features=64)\n        self.Mixed_5d = inception_a(288, pool_features=64)\n        self.Mixed_6a = inception_b(288)\n        self.Mixed_6b = inception_c(768, channels_7x7=128)\n        self.Mixed_6c = inception_c(768, channels_7x7=160)\n        self.Mixed_6d = inception_c(768, channels_7x7=160)\n        self.Mixed_6e = inception_c(768, channels_7x7=192)\n        self.Mixed_7a = inception_d(768)\n        self.Mixed_7b = inception_e(1280)\n        self.Mixed_7c = inception_e(2048)\n\n        self.num_features = 2048\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.fc = nn.Linear(2048, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                stddev = m.stddev if hasattr(m, \'stddev\') else 0.1\n                trunc_normal_(m.weight, std=stddev)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward_features(self, x):\n        # N x 3 x 299 x 299\n        x = self.Conv2d_1a_3x3(x)\n        # N x 32 x 149 x 149\n        x = self.Conv2d_2a_3x3(x)\n        # N x 32 x 147 x 147\n        x = self.Conv2d_2b_3x3(x)\n        # N x 64 x 147 x 147\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # N x 64 x 73 x 73\n        x = self.Conv2d_3b_1x1(x)\n        # N x 80 x 73 x 73\n        x = self.Conv2d_4a_3x3(x)\n        # N x 192 x 71 x 71\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # N x 192 x 35 x 35\n        x = self.Mixed_5b(x)\n        # N x 256 x 35 x 35\n        x = self.Mixed_5c(x)\n        # N x 288 x 35 x 35\n        x = self.Mixed_5d(x)\n        # N x 288 x 35 x 35\n        x = self.Mixed_6a(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6b(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6c(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6d(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6e(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_7a(x)\n        # N x 1280 x 8 x 8\n        x = self.Mixed_7b(x)\n        # N x 2048 x 8 x 8\n        x = self.Mixed_7c(x)\n        # N x 2048 x 8 x 8\n        return  x\n\n    def get_classifier(self):\n        return self.fc\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.num_classes = num_classes\n        if self.num_classes > 0:\n            self.fc = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n        else:\n            self.fc = nn.Identity()\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x).flatten(1)\n        if self.drop_rate > 0:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        x = self.fc(x)\n        return x\n\n\nclass InceptionA(nn.Module):\n\n    def __init__(self, in_channels, pool_features, conv_block=None):\n        super(InceptionA, self).__init__()\n        if conv_block is None:\n            conv_block = BasicConv2d\n        self.branch1x1 = conv_block(in_channels, 64, kernel_size=1)\n\n        self.branch5x5_1 = conv_block(in_channels, 48, kernel_size=1)\n        self.branch5x5_2 = conv_block(48, 64, kernel_size=5, padding=2)\n\n        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, padding=1)\n\n        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=1)\n\n    def _forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n        return outputs\n\n    def forward(self, x):\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionB(nn.Module):\n\n    def __init__(self, in_channels, conv_block=None):\n        super(InceptionB, self).__init__()\n        if conv_block is None:\n            conv_block = BasicConv2d\n        self.branch3x3 = conv_block(in_channels, 384, kernel_size=3, stride=2)\n\n        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, stride=2)\n\n    def _forward(self, x):\n        branch3x3 = self.branch3x3(x)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n\n        outputs = [branch3x3, branch3x3dbl, branch_pool]\n        return outputs\n\n    def forward(self, x):\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionC(nn.Module):\n\n    def __init__(self, in_channels, channels_7x7, conv_block=None):\n        super(InceptionC, self).__init__()\n        if conv_block is None:\n            conv_block = BasicConv2d\n        self.branch1x1 = conv_block(in_channels, 192, kernel_size=1)\n\n        c7 = channels_7x7\n        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=1)\n        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7_3 = conv_block(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n\n        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=1)\n        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_5 = conv_block(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n\n        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n\n    def _forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch7x7 = self.branch7x7_1(x)\n        branch7x7 = self.branch7x7_2(branch7x7)\n        branch7x7 = self.branch7x7_3(branch7x7)\n\n        branch7x7dbl = self.branch7x7dbl_1(x)\n        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n        return outputs\n\n    def forward(self, x):\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionD(nn.Module):\n\n    def __init__(self, in_channels, conv_block=None):\n        super(InceptionD, self).__init__()\n        if conv_block is None:\n            conv_block = BasicConv2d\n        self.branch3x3_1 = conv_block(in_channels, 192, kernel_size=1)\n        self.branch3x3_2 = conv_block(192, 320, kernel_size=3, stride=2)\n\n        self.branch7x7x3_1 = conv_block(in_channels, 192, kernel_size=1)\n        self.branch7x7x3_2 = conv_block(192, 192, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7x3_3 = conv_block(192, 192, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7x3_4 = conv_block(192, 192, kernel_size=3, stride=2)\n\n    def _forward(self, x):\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = self.branch3x3_2(branch3x3)\n\n        branch7x7x3 = self.branch7x7x3_1(x)\n        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n        outputs = [branch3x3, branch7x7x3, branch_pool]\n        return outputs\n\n    def forward(self, x):\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionE(nn.Module):\n\n    def __init__(self, in_channels, conv_block=None):\n        super(InceptionE, self).__init__()\n        if conv_block is None:\n            conv_block = BasicConv2d\n        self.branch1x1 = conv_block(in_channels, 320, kernel_size=1)\n\n        self.branch3x3_1 = conv_block(in_channels, 384, kernel_size=1)\n        self.branch3x3_2a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3_2b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch3x3dbl_1 = conv_block(in_channels, 448, kernel_size=1)\n        self.branch3x3dbl_2 = conv_block(448, 384, kernel_size=3, padding=1)\n        self.branch3x3dbl_3a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3dbl_3b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n\n    def _forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return outputs\n\n    def forward(self, x):\n        outputs = self._forward(x)\n        return torch.cat(outputs, 1)\n\n\nclass InceptionAux(nn.Module):\n\n    def __init__(self, in_channels, num_classes, conv_block=None):\n        super(InceptionAux, self).__init__()\n        if conv_block is None:\n            conv_block = BasicConv2d\n        self.conv0 = conv_block(in_channels, 128, kernel_size=1)\n        self.conv1 = conv_block(128, 768, kernel_size=5)\n        self.conv1.stddev = 0.01\n        self.fc = nn.Linear(768, num_classes)\n        self.fc.stddev = 0.001\n\n    def forward(self, x):\n        # N x 768 x 17 x 17\n        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n        # N x 768 x 5 x 5\n        x = self.conv0(x)\n        # N x 128 x 5 x 5\n        x = self.conv1(x)\n        # N x 768 x 1 x 1\n        # Adaptive average pooling\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        # N x 768 x 1 x 1\n        x = torch.flatten(x, 1)\n        # N x 768\n        x = self.fc(x)\n        # N x 1000\n        return x\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return F.relu(x, inplace=True)\n\n\ndef _inception_v3(variant, pretrained=False, **kwargs):\n    default_cfg = default_cfgs[variant]\n    if kwargs.pop(\'features_only\', False):\n        assert False, \'Not Implemented\'  # TODO\n        load_strict = False\n        model_kwargs.pop(\'num_classes\', 0)\n        model_class = InceptionV3\n    else:\n        aux_logits = kwargs.pop(\'aux_logits\', False)\n        if aux_logits:\n            model_class = InceptionV3Aux\n            load_strict = default_cfg[\'has_aux\']\n        else:\n            model_class = InceptionV3\n            load_strict = not default_cfg[\'has_aux\']\n\n    model = model_class(**kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(\n            model,\n            num_classes=kwargs.get(\'num_classes\', 0),\n            in_chans=kwargs.get(\'in_chans\', 3),\n            strict=load_strict)\n    return model\n\n\n@register_model\ndef inception_v3(pretrained=False, **kwargs):\n    # original PyTorch weights, ported from Tensorflow but modified\n    model = _inception_v3(\'inception_v3\', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_inception_v3(pretrained=False, **kwargs):\n    # my port of Tensorflow SLIM weights (http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz)\n    model = _inception_v3(\'tf_inception_v3\', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef adv_inception_v3(pretrained=False, **kwargs):\n    # my port of Tensorflow adversarially trained Inception V3 from\n    # http://download.tensorflow.org/models/adv_inception_v3_2017_08_18.tar.gz\n    model = _inception_v3(\'adv_inception_v3\', pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef gluon_inception_v3(pretrained=False, **kwargs):\n    # from gluon pretrained models, best performing in terms of accuracy/loss metrics\n    # https://gluon-cv.mxnet.io/model_zoo/classification.html\n    model = _inception_v3(\'gluon_inception_v3\', pretrained=pretrained, **kwargs)\n    return model\n'"
timm/models/inception_v4.py,12,"b'"""""" Pytorch Inception-V4 implementation\nSourced from https://github.com/Cadene/tensorflow-model-zoo.torch (MIT License) which is\nbased upon Google\'s Tensorflow implementation and pretrained weights (Apache 2.0 License)\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SelectAdaptivePool2d\nfrom timm.data import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n\n__all__ = [\'InceptionV4\']\n\ndefault_cfgs = {\n    \'inception_v4\': {\n        \'url\': \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/inceptionv4-8e4777a0.pth\',\n        \'num_classes\': 1001, \'input_size\': (3, 299, 299), \'pool_size\': (8, 8),\n        \'crop_pct\': 0.875, \'interpolation\': \'bicubic\',\n        \'mean\': IMAGENET_INCEPTION_MEAN, \'std\': IMAGENET_INCEPTION_STD,\n        \'first_conv\': \'features.0.conv\', \'classifier\': \'last_linear\',\n    }\n}\n\n\nclass BasicConv2d(nn.Module):\n    def __init__(self, in_planes, out_planes, kernel_size, stride, padding=0):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(\n            in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_planes, eps=0.001)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        return x\n\n\nclass Mixed_3a(nn.Module):\n    def __init__(self):\n        super(Mixed_3a, self).__init__()\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n        self.conv = BasicConv2d(64, 96, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        x0 = self.maxpool(x)\n        x1 = self.conv(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed_4a(nn.Module):\n    def __init__(self):\n        super(Mixed_4a, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(160, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 64, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(64, 64, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            BasicConv2d(64, 96, kernel_size=(3, 3), stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Mixed_5a(nn.Module):\n    def __init__(self):\n        super(Mixed_5a, self).__init__()\n        self.conv = BasicConv2d(192, 192, kernel_size=3, stride=2)\n        self.maxpool = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.conv(x)\n        x1 = self.maxpool(x)\n        out = torch.cat((x0, x1), 1)\n        return out\n\n\nclass Inception_A(nn.Module):\n    def __init__(self):\n        super(Inception_A, self).__init__()\n        self.branch0 = BasicConv2d(384, 96, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(384, 64, kernel_size=1, stride=1),\n            BasicConv2d(64, 96, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(96, 96, kernel_size=3, stride=1, padding=1)\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(384, 96, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Reduction_A(nn.Module):\n    def __init__(self):\n        super(Reduction_A, self).__init__()\n        self.branch0 = BasicConv2d(384, 384, kernel_size=3, stride=2)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(384, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=3, stride=1, padding=1),\n            BasicConv2d(224, 256, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Inception_B(nn.Module):\n    def __init__(self):\n        super(Inception_B, self).__init__()\n        self.branch0 = BasicConv2d(1024, 384, kernel_size=1, stride=1)\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(224, 256, kernel_size=(7, 1), stride=1, padding=(3, 0))\n        )\n\n        self.branch2 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            BasicConv2d(192, 224, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(224, 224, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            BasicConv2d(224, 256, kernel_size=(1, 7), stride=1, padding=(0, 3))\n        )\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(1024, 128, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        x3 = self.branch3(x)\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass Reduction_B(nn.Module):\n    def __init__(self):\n        super(Reduction_B, self).__init__()\n\n        self.branch0 = nn.Sequential(\n            BasicConv2d(1024, 192, kernel_size=1, stride=1),\n            BasicConv2d(192, 192, kernel_size=3, stride=2)\n        )\n\n        self.branch1 = nn.Sequential(\n            BasicConv2d(1024, 256, kernel_size=1, stride=1),\n            BasicConv2d(256, 256, kernel_size=(1, 7), stride=1, padding=(0, 3)),\n            BasicConv2d(256, 320, kernel_size=(7, 1), stride=1, padding=(3, 0)),\n            BasicConv2d(320, 320, kernel_size=3, stride=2)\n        )\n\n        self.branch2 = nn.MaxPool2d(3, stride=2)\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n        x1 = self.branch1(x)\n        x2 = self.branch2(x)\n        out = torch.cat((x0, x1, x2), 1)\n        return out\n\n\nclass Inception_C(nn.Module):\n    def __init__(self):\n        super(Inception_C, self).__init__()\n\n        self.branch0 = BasicConv2d(1536, 256, kernel_size=1, stride=1)\n\n        self.branch1_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch1_1a = BasicConv2d(384, 256, kernel_size=(1, 3), stride=1, padding=(0, 1))\n        self.branch1_1b = BasicConv2d(384, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))\n\n        self.branch2_0 = BasicConv2d(1536, 384, kernel_size=1, stride=1)\n        self.branch2_1 = BasicConv2d(384, 448, kernel_size=(3, 1), stride=1, padding=(1, 0))\n        self.branch2_2 = BasicConv2d(448, 512, kernel_size=(1, 3), stride=1, padding=(0, 1))\n        self.branch2_3a = BasicConv2d(512, 256, kernel_size=(1, 3), stride=1, padding=(0, 1))\n        self.branch2_3b = BasicConv2d(512, 256, kernel_size=(3, 1), stride=1, padding=(1, 0))\n\n        self.branch3 = nn.Sequential(\n            nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False),\n            BasicConv2d(1536, 256, kernel_size=1, stride=1)\n        )\n\n    def forward(self, x):\n        x0 = self.branch0(x)\n\n        x1_0 = self.branch1_0(x)\n        x1_1a = self.branch1_1a(x1_0)\n        x1_1b = self.branch1_1b(x1_0)\n        x1 = torch.cat((x1_1a, x1_1b), 1)\n\n        x2_0 = self.branch2_0(x)\n        x2_1 = self.branch2_1(x2_0)\n        x2_2 = self.branch2_2(x2_1)\n        x2_3a = self.branch2_3a(x2_2)\n        x2_3b = self.branch2_3b(x2_2)\n        x2 = torch.cat((x2_3a, x2_3b), 1)\n\n        x3 = self.branch3(x)\n\n        out = torch.cat((x0, x1, x2, x3), 1)\n        return out\n\n\nclass InceptionV4(nn.Module):\n    def __init__(self, num_classes=1001, in_chans=3, drop_rate=0., global_pool=\'avg\'):\n        super(InceptionV4, self).__init__()\n        self.drop_rate = drop_rate\n        self.num_classes = num_classes\n        self.num_features = 1536\n\n        self.features = nn.Sequential(\n            BasicConv2d(in_chans, 32, kernel_size=3, stride=2),\n            BasicConv2d(32, 32, kernel_size=3, stride=1),\n            BasicConv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            Mixed_3a(),\n            Mixed_4a(),\n            Mixed_5a(),\n            Inception_A(),\n            Inception_A(),\n            Inception_A(),\n            Inception_A(),\n            Reduction_A(),  # Mixed_6a\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Inception_B(),\n            Reduction_B(),  # Mixed_7a\n            Inception_C(),\n            Inception_C(),\n            Inception_C(),\n        )\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.last_linear = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n\n    def get_classifier(self):\n        return self.last_linear\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.num_classes = num_classes\n        self.last_linear = nn.Linear(\n            self.num_features * self.global_pool.feat_mult(), num_classes) if num_classes else None\n\n    def forward_features(self, x):\n        return self.features(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x).flatten(1)\n        if self.drop_rate > 0:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        x = self.last_linear(x)\n        return x\n\n\n@register_model\ndef inception_v4(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'inception_v4\']\n    model = InceptionV4(num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n\n'"
timm/models/mobilenetv3.py,0,"b'\n"""""" MobileNet V3\n\nA PyTorch impl of MobileNet-V3, compatible with TF weights from official impl.\n\nPaper: Searching for MobileNetV3 - https://arxiv.org/abs/1905.02244\n\nHacked together by Ross Wightman\n""""""\n\nfrom .efficientnet_builder import *\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SelectAdaptivePool2d, create_conv2d\nfrom .layers.activations import HardSwish, hard_sigmoid\nfrom .feature_hooks import FeatureHooks\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD\n\n__all__ = [\'MobileNetV3\']\n\n\ndef _cfg(url=\'\', **kwargs):\n    return {\n        \'url\': url, \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (1, 1),\n        \'crop_pct\': 0.875, \'interpolation\': \'bilinear\',\n        \'mean\': IMAGENET_DEFAULT_MEAN, \'std\': IMAGENET_DEFAULT_STD,\n        \'first_conv\': \'conv_stem\', \'classifier\': \'classifier\',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    \'mobilenetv3_large_075\': _cfg(url=\'\'),\n    \'mobilenetv3_large_100\': _cfg(\n        interpolation=\'bicubic\',\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_large_100_ra-f55367f5.pth\'),\n    \'mobilenetv3_small_075\': _cfg(url=\'\'),\n    \'mobilenetv3_small_100\': _cfg(url=\'\'),\n    \'mobilenetv3_rw\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/mobilenetv3_100-35495452.pth\',\n        interpolation=\'bicubic\'),\n    \'tf_mobilenetv3_large_075\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_large_075-150ee8b0.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    \'tf_mobilenetv3_large_100\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_large_100-427764d5.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    \'tf_mobilenetv3_large_minimal_100\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_large_minimal_100-8596ae28.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    \'tf_mobilenetv3_small_075\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_small_075-da427f52.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    \'tf_mobilenetv3_small_100\': _cfg(\n        url= \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_small_100-37f49e2b.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n    \'tf_mobilenetv3_small_minimal_100\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_mobilenetv3_small_minimal_100-922a7843.pth\',\n        mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD),\n}\n\n_DEBUG = False\n\n\nclass MobileNetV3(nn.Module):\n    """""" MobiletNet-V3\n\n    Based on my EfficientNet implementation and building blocks, this model utilizes the MobileNet-v3 specific\n    \'efficient head\', where global pooling is done before the head convolution without a final batch-norm\n    layer before the classifier.\n\n    Paper: https://arxiv.org/abs/1905.02244\n    """"""\n\n    def __init__(self, block_args, num_classes=1000, in_chans=3, stem_size=16, num_features=1280, head_bias=True,\n                 channel_multiplier=1.0, pad_type=\'\', act_layer=nn.ReLU, drop_rate=0., drop_path_rate=0.,\n                 se_kwargs=None, norm_layer=nn.BatchNorm2d, norm_kwargs=None, global_pool=\'avg\'):\n        super(MobileNetV3, self).__init__()\n        \n        self.num_classes = num_classes\n        self.num_features = num_features\n        self.drop_rate = drop_rate\n        self._in_chs = in_chans\n\n        # Stem\n        stem_size = round_channels(stem_size, channel_multiplier)\n        self.conv_stem = create_conv2d(self._in_chs, stem_size, 3, stride=2, padding=pad_type)\n        self.bn1 = norm_layer(stem_size, **norm_kwargs)\n        self.act1 = act_layer(inplace=True)\n        self._in_chs = stem_size\n\n        # Middle stages (IR/ER/DS Blocks)\n        builder = EfficientNetBuilder(\n            channel_multiplier, 8, None, 32, pad_type, act_layer, se_kwargs,\n            norm_layer, norm_kwargs, drop_path_rate, verbose=_DEBUG)\n        self.blocks = nn.Sequential(*builder(self._in_chs, block_args))\n        self.feature_info = builder.features\n        self._in_chs = builder.in_chs\n        \n        # Head + Pooling\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.conv_head = create_conv2d(self._in_chs, self.num_features, 1, padding=pad_type, bias=head_bias)\n        self.act2 = act_layer(inplace=True)\n\n        # Classifier\n        self.classifier = nn.Linear(self.num_features * self.global_pool.feat_mult(), self.num_classes)\n\n        efficientnet_init_weights(self)\n\n    def as_sequential(self):\n        layers = [self.conv_stem, self.bn1, self.act1]\n        layers.extend(self.blocks)\n        layers.extend([self.global_pool, self.conv_head, self.act2])\n        layers.extend([nn.Flatten(), nn.Dropout(self.drop_rate), self.classifier])\n        return nn.Sequential(*layers)\n\n    def get_classifier(self):\n        return self.classifier\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.num_classes = num_classes\n        self.classifier = nn.Linear(\n            self.num_features * self.global_pool.feat_mult(), num_classes) if self.num_classes else None\n\n    def forward_features(self, x):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.blocks(x)\n        x = self.global_pool(x)\n        x = self.conv_head(x)\n        x = self.act2(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = x.flatten(1)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        return self.classifier(x)\n\n\nclass MobileNetV3Features(nn.Module):\n    """""" MobileNetV3 Feature Extractor\n\n    A work-in-progress feature extraction module for MobileNet-V3 to use as a backbone for segmentation\n    and object detection models.\n    """"""\n\n    def __init__(self, block_args, out_indices=(0, 1, 2, 3, 4), feature_location=\'bottleneck\',\n                 in_chans=3, stem_size=16, channel_multiplier=1.0, output_stride=32, pad_type=\'\',\n                 act_layer=nn.ReLU, drop_rate=0., drop_path_rate=0., se_kwargs=None,\n                 norm_layer=nn.BatchNorm2d, norm_kwargs=None):\n        super(MobileNetV3Features, self).__init__()\n        norm_kwargs = norm_kwargs or {}\n\n        # TODO only create stages needed, currently all stages are created regardless of out_indices\n        num_stages = max(out_indices) + 1\n\n        self.out_indices = out_indices\n        self.drop_rate = drop_rate\n        self._in_chs = in_chans\n\n        # Stem\n        stem_size = round_channels(stem_size, channel_multiplier)\n        self.conv_stem = create_conv2d(self._in_chs, stem_size, 3, stride=2, padding=pad_type)\n        self.bn1 = norm_layer(stem_size, **norm_kwargs)\n        self.act1 = act_layer(inplace=True)\n        self._in_chs = stem_size\n\n        # Middle stages (IR/ER/DS Blocks)\n        builder = EfficientNetBuilder(\n            channel_multiplier, 8, None, output_stride, pad_type, act_layer, se_kwargs,\n            norm_layer, norm_kwargs, drop_path_rate, feature_location=feature_location, verbose=_DEBUG)\n        self.blocks = nn.Sequential(*builder(self._in_chs, block_args))\n        self._feature_info = builder.features  # builder provides info about feature channels for each block\n        self._stage_to_feature_idx = {\n            v[\'stage_idx\']: fi for fi, v in self._feature_info.items() if fi in self.out_indices}\n        self._in_chs = builder.in_chs\n\n        efficientnet_init_weights(self)\n        if _DEBUG:\n            for k, v in self._feature_info.items():\n                print(\'Feature idx: {}: Name: {}, Channels: {}\'.format(k, v[\'name\'], v[\'num_chs\']))\n\n        # Register feature extraction hooks with FeatureHooks helper\n        self.feature_hooks = None\n        if feature_location != \'bottleneck\':\n            hooks = [dict(\n                name=self._feature_info[idx][\'module\'],\n                type=self._feature_info[idx][\'hook_type\']) for idx in out_indices]\n            self.feature_hooks = FeatureHooks(hooks, self.named_modules())\n\n    def feature_channels(self, idx=None):\n        """""" Feature Channel Shortcut\n        Returns feature channel count for each output index if idx == None. If idx is an integer, will\n        return feature channel count for that feature block index (independent of out_indices setting).\n        """"""\n        if isinstance(idx, int):\n            return self._feature_info[idx][\'num_chs\']\n        return [self._feature_info[i][\'num_chs\'] for i in self.out_indices]\n\n    def forward(self, x):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        if self.feature_hooks is None:\n            features = []\n            for i, b in enumerate(self.blocks):\n                x = b(x)\n                if i in self._stage_to_feature_idx:\n                    features.append(x)\n            return features\n        else:\n            self.blocks(x)\n            return self.feature_hooks.get_output(x.device)\n\n\ndef _create_model(model_kwargs, default_cfg, pretrained=False):\n    if model_kwargs.pop(\'features_only\', False):\n        load_strict = False\n        model_kwargs.pop(\'num_classes\', 0)\n        model_kwargs.pop(\'num_features\', 0)\n        model_kwargs.pop(\'head_conv\', None)\n        model_class = MobileNetV3Features\n    else:\n        load_strict = True\n        model_class = MobileNetV3\n\n    model = model_class(**model_kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(\n            model,\n            default_cfg,\n            num_classes=model_kwargs.get(\'num_classes\', 0),\n            in_chans=model_kwargs.get(\'in_chans\', 3),\n            strict=load_strict)\n    return model\n\n\ndef _gen_mobilenet_v3_rw(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates a MobileNet-V3 model.\n\n    Ref impl: ?\n    Paper: https://arxiv.org/abs/1905.02244\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    """"""\n    arch_def = [\n        # stage 0, 112x112 in\n        [\'ds_r1_k3_s1_e1_c16_nre_noskip\'],  # relu\n        # stage 1, 112x112 in\n        [\'ir_r1_k3_s2_e4_c24_nre\', \'ir_r1_k3_s1_e3_c24_nre\'],  # relu\n        # stage 2, 56x56 in\n        [\'ir_r3_k5_s2_e3_c40_se0.25_nre\'],  # relu\n        # stage 3, 28x28 in\n        [\'ir_r1_k3_s2_e6_c80\', \'ir_r1_k3_s1_e2.5_c80\', \'ir_r2_k3_s1_e2.3_c80\'],  # hard-swish\n        # stage 4, 14x14in\n        [\'ir_r2_k3_s1_e6_c112_se0.25\'],  # hard-swish\n        # stage 5, 14x14in\n        [\'ir_r3_k5_s2_e6_c160_se0.25\'],  # hard-swish\n        # stage 6, 7x7 in\n        [\'cn_r1_k1_s1_c960\'],  # hard-swish\n    ]\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        head_bias=False,\n        channel_multiplier=channel_multiplier,\n        norm_kwargs=resolve_bn_args(kwargs),\n        act_layer=HardSwish,\n        se_kwargs=dict(gate_fn=hard_sigmoid, reduce_mid=True, divisor=1),\n        **kwargs,\n    )\n    model = _create_model(model_kwargs, default_cfgs[variant], pretrained)\n    return model\n\n\ndef _gen_mobilenet_v3(variant, channel_multiplier=1.0, pretrained=False, **kwargs):\n    """"""Creates a MobileNet-V3 model.\n\n    Ref impl: ?\n    Paper: https://arxiv.org/abs/1905.02244\n\n    Args:\n      channel_multiplier: multiplier to number of channels per layer.\n    """"""\n    if \'small\' in variant:\n        num_features = 1024\n        if \'minimal\' in variant:\n            act_layer = nn.ReLU\n            arch_def = [\n                # stage 0, 112x112 in\n                [\'ds_r1_k3_s2_e1_c16\'],\n                # stage 1, 56x56 in\n                [\'ir_r1_k3_s2_e4.5_c24\', \'ir_r1_k3_s1_e3.67_c24\'],\n                # stage 2, 28x28 in\n                [\'ir_r1_k3_s2_e4_c40\', \'ir_r2_k3_s1_e6_c40\'],\n                # stage 3, 14x14 in\n                [\'ir_r2_k3_s1_e3_c48\'],\n                # stage 4, 14x14in\n                [\'ir_r3_k3_s2_e6_c96\'],\n                # stage 6, 7x7 in\n                [\'cn_r1_k1_s1_c576\'],\n            ]\n        else:\n            act_layer = HardSwish\n            arch_def = [\n                # stage 0, 112x112 in\n                [\'ds_r1_k3_s2_e1_c16_se0.25_nre\'],  # relu\n                # stage 1, 56x56 in\n                [\'ir_r1_k3_s2_e4.5_c24_nre\', \'ir_r1_k3_s1_e3.67_c24_nre\'],  # relu\n                # stage 2, 28x28 in\n                [\'ir_r1_k5_s2_e4_c40_se0.25\', \'ir_r2_k5_s1_e6_c40_se0.25\'],  # hard-swish\n                # stage 3, 14x14 in\n                [\'ir_r2_k5_s1_e3_c48_se0.25\'],  # hard-swish\n                # stage 4, 14x14in\n                [\'ir_r3_k5_s2_e6_c96_se0.25\'],  # hard-swish\n                # stage 6, 7x7 in\n                [\'cn_r1_k1_s1_c576\'],  # hard-swish\n            ]\n    else:\n        num_features = 1280\n        if \'minimal\' in variant:\n            act_layer = nn.ReLU\n            arch_def = [\n                # stage 0, 112x112 in\n                [\'ds_r1_k3_s1_e1_c16\'],\n                # stage 1, 112x112 in\n                [\'ir_r1_k3_s2_e4_c24\', \'ir_r1_k3_s1_e3_c24\'],\n                # stage 2, 56x56 in\n                [\'ir_r3_k3_s2_e3_c40\'],\n                # stage 3, 28x28 in\n                [\'ir_r1_k3_s2_e6_c80\', \'ir_r1_k3_s1_e2.5_c80\', \'ir_r2_k3_s1_e2.3_c80\'],\n                # stage 4, 14x14in\n                [\'ir_r2_k3_s1_e6_c112\'],\n                # stage 5, 14x14in\n                [\'ir_r3_k3_s2_e6_c160\'],\n                # stage 6, 7x7 in\n                [\'cn_r1_k1_s1_c960\'],\n            ]\n        else:\n            act_layer = HardSwish\n            arch_def = [\n                # stage 0, 112x112 in\n                [\'ds_r1_k3_s1_e1_c16_nre\'],  # relu\n                # stage 1, 112x112 in\n                [\'ir_r1_k3_s2_e4_c24_nre\', \'ir_r1_k3_s1_e3_c24_nre\'],  # relu\n                # stage 2, 56x56 in\n                [\'ir_r3_k5_s2_e3_c40_se0.25_nre\'],  # relu\n                # stage 3, 28x28 in\n                [\'ir_r1_k3_s2_e6_c80\', \'ir_r1_k3_s1_e2.5_c80\', \'ir_r2_k3_s1_e2.3_c80\'],  # hard-swish\n                # stage 4, 14x14in\n                [\'ir_r2_k3_s1_e6_c112_se0.25\'],  # hard-swish\n                # stage 5, 14x14in\n                [\'ir_r3_k5_s2_e6_c160_se0.25\'],  # hard-swish\n                # stage 6, 7x7 in\n                [\'cn_r1_k1_s1_c960\'],  # hard-swish\n            ]\n\n    model_kwargs = dict(\n        block_args=decode_arch_def(arch_def),\n        num_features=num_features,\n        stem_size=16,\n        channel_multiplier=channel_multiplier,\n        norm_kwargs=resolve_bn_args(kwargs),\n        act_layer=act_layer,\n        se_kwargs=dict(act_layer=nn.ReLU, gate_fn=hard_sigmoid, reduce_mid=True, divisor=8),\n        **kwargs,\n    )\n    model = _create_model(model_kwargs, default_cfgs[variant], pretrained)\n    return model\n\n\n@register_model\ndef mobilenetv3_large_075(pretrained=False, **kwargs):\n    """""" MobileNet V3 """"""\n    model = _gen_mobilenet_v3(\'mobilenetv3_large_075\', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv3_large_100(pretrained=False, **kwargs):\n    """""" MobileNet V3 """"""\n    model = _gen_mobilenet_v3(\'mobilenetv3_large_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv3_small_075(pretrained=False, **kwargs):\n    """""" MobileNet V3 """"""\n    model = _gen_mobilenet_v3(\'mobilenetv3_small_075\', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv3_small_100(pretrained=False, **kwargs):\n    print(kwargs)\n    """""" MobileNet V3 """"""\n    model = _gen_mobilenet_v3(\'mobilenetv3_small_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef mobilenetv3_rw(pretrained=False, **kwargs):\n    """""" MobileNet V3 """"""\n    if pretrained:\n        # pretrained model trained with non-default BN epsilon\n        kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    model = _gen_mobilenet_v3_rw(\'mobilenetv3_rw\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mobilenetv3_large_075(pretrained=False, **kwargs):\n    """""" MobileNet V3 """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mobilenet_v3(\'tf_mobilenetv3_large_075\', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mobilenetv3_large_100(pretrained=False, **kwargs):\n    """""" MobileNet V3 """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mobilenet_v3(\'tf_mobilenetv3_large_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mobilenetv3_large_minimal_100(pretrained=False, **kwargs):\n    """""" MobileNet V3 """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mobilenet_v3(\'tf_mobilenetv3_large_minimal_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mobilenetv3_small_075(pretrained=False, **kwargs):\n    """""" MobileNet V3 """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mobilenet_v3(\'tf_mobilenetv3_small_075\', 0.75, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mobilenetv3_small_100(pretrained=False, **kwargs):\n    """""" MobileNet V3 """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mobilenet_v3(\'tf_mobilenetv3_small_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n\n\n@register_model\ndef tf_mobilenetv3_small_minimal_100(pretrained=False, **kwargs):\n    """""" MobileNet V3 """"""\n    kwargs[\'bn_eps\'] = BN_EPS_TF_DEFAULT\n    kwargs[\'pad_type\'] = \'same\'\n    model = _gen_mobilenet_v3(\'tf_mobilenetv3_small_minimal_100\', 1.0, pretrained=pretrained, **kwargs)\n    return model\n'"
timm/models/nasnet.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SelectAdaptivePool2d\n\n\n__all__ = [\'NASNetALarge\']\n\ndefault_cfgs = {\n    \'nasnetalarge\': {\n        \'url\': \'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth\',\n        \'input_size\': (3, 331, 331),\n        \'pool_size\': (11, 11),\n        \'crop_pct\': 0.875,\n        \'interpolation\': \'bicubic\',\n        \'mean\': (0.5, 0.5, 0.5),\n        \'std\': (0.5, 0.5, 0.5),\n        \'num_classes\': 1001,\n        \'first_conv\': \'conv0.conv\',\n        \'classifier\': \'last_linear\',\n    },\n}\n\n\nclass MaxPoolPad(nn.Module):\n\n    def __init__(self):\n        super(MaxPoolPad, self).__init__()\n        self.pad = nn.ZeroPad2d((1, 0, 1, 0))\n        self.pool = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.pool(x)\n        x = x[:, :, 1:, 1:]\n        return x\n\n\nclass AvgPoolPad(nn.Module):\n\n    def __init__(self, stride=2, padding=1):\n        super(AvgPoolPad, self).__init__()\n        self.pad = nn.ZeroPad2d((1, 0, 1, 0))\n        self.pool = nn.AvgPool2d(3, stride=stride, padding=padding, count_include_pad=False)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.pool(x)\n        x = x[:, :, 1:, 1:]\n        return x\n\n\nclass SeparableConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, dw_kernel, dw_stride, dw_padding, bias=False):\n        super(SeparableConv2d, self).__init__()\n        self.depthwise_conv2d = nn.Conv2d(\n            in_channels, in_channels, dw_kernel,\n            stride=dw_stride, padding=dw_padding,\n            bias=bias, groups=in_channels)\n        self.pointwise_conv2d = nn.Conv2d(in_channels, out_channels, 1, stride=1, bias=bias)\n\n    def forward(self, x):\n        x = self.depthwise_conv2d(x)\n        x = self.pointwise_conv2d(x)\n        return x\n\n\nclass BranchSeparables(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n        super(BranchSeparables, self).__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, in_channels, kernel_size, stride, padding, bias=bias)\n        self.bn_sep_1 = nn.BatchNorm2d(in_channels, eps=0.001, momentum=0.1, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(in_channels, out_channels, kernel_size, 1, padding, bias=bias)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass BranchSeparablesStem(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, bias=False):\n        super(BranchSeparablesStem, self).__init__()\n        self.relu = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n        self.bn_sep_1 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n        self.relu1 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(out_channels, out_channels, kernel_size, 1, padding, bias=bias)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0.1, affine=True)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.separable_1(x)\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass BranchSeparablesReduction(BranchSeparables):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, z_padding=1, bias=False):\n        BranchSeparables.__init__(self, in_channels, out_channels, kernel_size, stride, padding, bias)\n        self.padding = nn.ZeroPad2d((z_padding, 0, z_padding, 0))\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.padding(x)\n        x = self.separable_1(x)\n        x = x[:, :, 1:, 1:].contiguous()\n        x = self.bn_sep_1(x)\n        x = self.relu1(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass CellStem0(nn.Module):\n    def __init__(self, stem_size, num_channels=42):\n        super(CellStem0, self).__init__()\n        self.num_channels = num_channels\n        self.stem_size = stem_size\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(self.stem_size, self.num_channels, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(self.num_channels, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(self.num_channels, self.num_channels, 5, 2, 2)\n        self.comb_iter_0_right = BranchSeparablesStem(self.stem_size, self.num_channels, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_right = BranchSeparablesStem(self.stem_size, self.num_channels, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_right = BranchSeparablesStem(self.stem_size, self.num_channels, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(self.num_channels, self.num_channels, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x):\n        x1 = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x1)\n        x_comb_iter_0_right = self.comb_iter_0_right(x)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x1)\n        x_comb_iter_1_right = self.comb_iter_1_right(x)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x1)\n        x_comb_iter_2_right = self.comb_iter_2_right(x)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x1)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass CellStem1(nn.Module):\n\n    def __init__(self, stem_size, num_channels):\n        super(CellStem1, self).__init__()\n        self.num_channels = num_channels\n        self.stem_size = stem_size\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(2*self.num_channels, self.num_channels, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(self.num_channels, eps=0.001, momentum=0.1, affine=True))\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module(\'conv\', nn.Conv2d(self.stem_size, self.num_channels//2, 1, stride=1, bias=False))\n        self.path_2 = nn.ModuleList()\n        self.path_2.add_module(\'pad\', nn.ZeroPad2d((0, 1, 0, 1)))\n        self.path_2.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module(\'conv\', nn.Conv2d(self.stem_size, self.num_channels//2, 1, stride=1, bias=False))\n\n        self.final_path_bn = nn.BatchNorm2d(self.num_channels, eps=0.001, momentum=0.1, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(self.num_channels, self.num_channels, 5, 2, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(self.num_channels, self.num_channels, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_right = BranchSeparables(self.num_channels, self.num_channels, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_right = BranchSeparables(self.num_channels, self.num_channels, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(self.num_channels, self.num_channels, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x_conv0, x_stem_0):\n        x_left = self.conv_1x1(x_stem_0)\n\n        x_relu = self.relu(x_conv0)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2.pad(x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n        # final path\n        x_right = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_right)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_left)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_left)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass FirstCell(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(FirstCell, self).__init__()\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential()\n        self.path_1.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.path_2 = nn.ModuleList()\n        self.path_2.add_module(\'pad\', nn.ZeroPad2d((0, 1, 0, 1)))\n        self.path_2.add_module(\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False))\n        self.path_2.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n\n        self.final_path_bn = nn.BatchNorm2d(out_channels_left * 2, eps=0.001, momentum=0.1, affine=True)\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n        self.comb_iter_1_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_1_right = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_3_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n    def forward(self, x, x_prev):\n        x_relu = self.relu(x_prev)\n        # path 1\n        x_path1 = self.path_1(x_relu)\n        # path 2\n        x_path2 = self.path_2.pad(x_relu)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n        # final path\n        x_left = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass NormalCell(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(NormalCell, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 1, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_left, out_channels_left, 3, 1, 1, bias=False)\n\n        self.comb_iter_1_left = BranchSeparables(out_channels_left, out_channels_left, 5, 1, 2, bias=False)\n        self.comb_iter_1_right = BranchSeparables(out_channels_left, out_channels_left, 3, 1, 1, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_3_left = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_left)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_left\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_left)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_left)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_right\n\n        x_out = torch.cat([x_left, x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass ReductionCell0(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(ReductionCell0, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparablesReduction(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = MaxPoolPad()\n        self.comb_iter_1_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = AvgPoolPad()\n        self.comb_iter_2_right = BranchSeparablesReduction(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparablesReduction(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = MaxPoolPad()\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass ReductionCell1(nn.Module):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right, out_channels_right):\n        super(ReductionCell1, self).__init__()\n        self.conv_prev_1x1 = nn.Sequential()\n        self.conv_prev_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_prev_1x1.add_module(\'conv\', nn.Conv2d(in_channels_left, out_channels_left, 1, stride=1, bias=False))\n        self.conv_prev_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001, momentum=0.1, affine=True))\n\n        self.conv_1x1 = nn.Sequential()\n        self.conv_1x1.add_module(\'relu\', nn.ReLU())\n        self.conv_1x1.add_module(\'conv\', nn.Conv2d(in_channels_right, out_channels_right, 1, stride=1, bias=False))\n        self.conv_1x1.add_module(\'bn\', nn.BatchNorm2d(out_channels_right, eps=0.001, momentum=0.1, affine=True))\n\n        self.comb_iter_0_left = BranchSeparables(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n        self.comb_iter_0_right = BranchSeparables(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_1_left = nn.MaxPool2d(3, stride=2, padding=1)\n        self.comb_iter_1_right = BranchSeparables(out_channels_right, out_channels_right, 7, 2, 3, bias=False)\n\n        self.comb_iter_2_left = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n        self.comb_iter_2_right = BranchSeparables(out_channels_right, out_channels_right, 5, 2, 2, bias=False)\n\n        self.comb_iter_3_right = nn.AvgPool2d(3, stride=1, padding=1, count_include_pad=False)\n\n        self.comb_iter_4_left = BranchSeparables(out_channels_right, out_channels_right, 3, 1, 1, bias=False)\n        self.comb_iter_4_right = nn.MaxPool2d(3, stride=2, padding=1)\n\n    def forward(self, x, x_prev):\n        x_left = self.conv_prev_1x1(x_prev)\n        x_right = self.conv_1x1(x)\n\n        x_comb_iter_0_left = self.comb_iter_0_left(x_right)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_left)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_left)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_right = self.comb_iter_3_right(x_comb_iter_0)\n        x_comb_iter_3 = x_comb_iter_3_right + x_comb_iter_1\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_comb_iter_0)\n        x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat([x_comb_iter_1, x_comb_iter_2, x_comb_iter_3, x_comb_iter_4], 1)\n        return x_out\n\n\nclass NASNetALarge(nn.Module):\n    """"""NASNetALarge (6 @ 4032) """"""\n\n    def __init__(self, num_classes=1000, in_chans=1, stem_size=96, num_features=4032, channel_multiplier=2,\n                 drop_rate=0., global_pool=\'avg\'):\n        super(NASNetALarge, self).__init__()\n        self.num_classes = num_classes\n        self.stem_size = stem_size\n        self.num_features = num_features\n        self.channel_multiplier = channel_multiplier\n        self.drop_rate = drop_rate\n\n        channels = self.num_features // 24\n        # 24 is default value for the architecture\n\n        self.conv0 = nn.Sequential()\n        self.conv0.add_module(\'conv\', nn.Conv2d(\n            in_channels=in_chans, out_channels=self.stem_size, kernel_size=3, padding=0, stride=2, bias=False))\n        self.conv0.add_module(\'bn\', nn.BatchNorm2d(self.stem_size, eps=0.001, momentum=0.1, affine=True))\n\n        self.cell_stem_0 = CellStem0(self.stem_size, num_channels=channels // (channel_multiplier ** 2))\n        self.cell_stem_1 = CellStem1(self.stem_size, num_channels=channels // channel_multiplier)\n\n        self.cell_0 = FirstCell(in_channels_left=channels, out_channels_left=channels//2,\n                                in_channels_right=2*channels, out_channels_right=channels)\n        self.cell_1 = NormalCell(in_channels_left=2*channels, out_channels_left=channels,\n                                 in_channels_right=6*channels, out_channels_right=channels)\n        self.cell_2 = NormalCell(in_channels_left=6*channels, out_channels_left=channels,\n                                 in_channels_right=6*channels, out_channels_right=channels)\n        self.cell_3 = NormalCell(in_channels_left=6*channels, out_channels_left=channels,\n                                 in_channels_right=6*channels, out_channels_right=channels)\n        self.cell_4 = NormalCell(in_channels_left=6*channels, out_channels_left=channels,\n                                 in_channels_right=6*channels, out_channels_right=channels)\n        self.cell_5 = NormalCell(in_channels_left=6*channels, out_channels_left=channels,\n                                 in_channels_right=6*channels, out_channels_right=channels)\n\n        self.reduction_cell_0 = ReductionCell0(in_channels_left=6*channels, out_channels_left=2*channels,\n                                               in_channels_right=6*channels, out_channels_right=2*channels)\n\n        self.cell_6 = FirstCell(in_channels_left=6*channels, out_channels_left=channels,\n                                in_channels_right=8*channels, out_channels_right=2*channels)\n        self.cell_7 = NormalCell(in_channels_left=8*channels, out_channels_left=2*channels,\n                                 in_channels_right=12*channels, out_channels_right=2*channels)\n        self.cell_8 = NormalCell(in_channels_left=12*channels, out_channels_left=2*channels,\n                                 in_channels_right=12*channels, out_channels_right=2*channels)\n        self.cell_9 = NormalCell(in_channels_left=12*channels, out_channels_left=2*channels,\n                                 in_channels_right=12*channels, out_channels_right=2*channels)\n        self.cell_10 = NormalCell(in_channels_left=12*channels, out_channels_left=2*channels,\n                                  in_channels_right=12*channels, out_channels_right=2*channels)\n        self.cell_11 = NormalCell(in_channels_left=12*channels, out_channels_left=2*channels,\n                                  in_channels_right=12*channels, out_channels_right=2*channels)\n\n        self.reduction_cell_1 = ReductionCell1(in_channels_left=12*channels, out_channels_left=4*channels,\n                                               in_channels_right=12*channels, out_channels_right=4*channels)\n\n        self.cell_12 = FirstCell(in_channels_left=12*channels, out_channels_left=2*channels,\n                                 in_channels_right=16*channels, out_channels_right=4*channels)\n        self.cell_13 = NormalCell(in_channels_left=16*channels, out_channels_left=4*channels,\n                                  in_channels_right=24*channels, out_channels_right=4*channels)\n        self.cell_14 = NormalCell(in_channels_left=24*channels, out_channels_left=4*channels,\n                                  in_channels_right=24*channels, out_channels_right=4*channels)\n        self.cell_15 = NormalCell(in_channels_left=24*channels, out_channels_left=4*channels,\n                                  in_channels_right=24*channels, out_channels_right=4*channels)\n        self.cell_16 = NormalCell(in_channels_left=24*channels, out_channels_left=4*channels,\n                                  in_channels_right=24*channels, out_channels_right=4*channels)\n        self.cell_17 = NormalCell(in_channels_left=24*channels, out_channels_left=4*channels,\n                                  in_channels_right=24*channels, out_channels_right=4*channels)\n\n        self.relu = nn.ReLU()\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.last_linear = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n\n    def get_classifier(self):\n        return self.last_linear\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.num_classes = num_classes\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        del self.last_linear\n        self.last_linear = nn.Linear(\n            self.num_features * self.global_pool.feat_mult(), num_classes) if num_classes else None\n\n    def forward_features(self, x):\n        x_conv0 = self.conv0(x)\n        x_stem_0 = self.cell_stem_0(x_conv0)\n        x_stem_1 = self.cell_stem_1(x_conv0, x_stem_0)\n\n        x_cell_0 = self.cell_0(x_stem_1, x_stem_0)\n        x_cell_1 = self.cell_1(x_cell_0, x_stem_1)\n        x_cell_2 = self.cell_2(x_cell_1, x_cell_0)\n        x_cell_3 = self.cell_3(x_cell_2, x_cell_1)\n        x_cell_4 = self.cell_4(x_cell_3, x_cell_2)\n        x_cell_5 = self.cell_5(x_cell_4, x_cell_3)\n\n        x_reduction_cell_0 = self.reduction_cell_0(x_cell_5, x_cell_4)\n\n        x_cell_6 = self.cell_6(x_reduction_cell_0, x_cell_4)\n        x_cell_7 = self.cell_7(x_cell_6, x_reduction_cell_0)\n        x_cell_8 = self.cell_8(x_cell_7, x_cell_6)\n        x_cell_9 = self.cell_9(x_cell_8, x_cell_7)\n        x_cell_10 = self.cell_10(x_cell_9, x_cell_8)\n        x_cell_11 = self.cell_11(x_cell_10, x_cell_9)\n\n        x_reduction_cell_1 = self.reduction_cell_1(x_cell_11, x_cell_10)\n\n        x_cell_12 = self.cell_12(x_reduction_cell_1, x_cell_10)\n        x_cell_13 = self.cell_13(x_cell_12, x_reduction_cell_1)\n        x_cell_14 = self.cell_14(x_cell_13, x_cell_12)\n        x_cell_15 = self.cell_15(x_cell_14, x_cell_13)\n        x_cell_16 = self.cell_16(x_cell_15, x_cell_14)\n        x_cell_17 = self.cell_17(x_cell_16, x_cell_15)\n        x = self.relu(x_cell_17)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x).flatten(1)\n        if self.drop_rate > 0:\n            x = F.dropout(x, self.drop_rate, training=self.training)\n        x = self.last_linear(x)\n        return x\n\n\n@register_model\ndef nasnetalarge(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""NASNet-A large model architecture.\n    """"""\n    default_cfg = default_cfgs[\'nasnetalarge\']\n    model = NASNetALarge(num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n\n    return model\n'"
timm/models/pnasnet.py,4,"b'""""""\n pnasnet5large implementation grabbed from Cadene\'s pretrained models\n Additional credit to https://github.com/creafz\n\n https://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/pnasnet.py\n\n""""""\nfrom __future__ import print_function, division, absolute_import\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SelectAdaptivePool2d\n\n__all__ = [\'PNASNet5Large\']\n\ndefault_cfgs = {\n    \'pnasnet5large\': {\n        \'url\': \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/pnasnet5large-bf079911.pth\',\n        \'input_size\': (3, 331, 331),\n        \'pool_size\': (11, 11),\n        \'crop_pct\': 0.875,\n        \'interpolation\': \'bicubic\',\n        \'mean\': (0.5, 0.5, 0.5),\n        \'std\': (0.5, 0.5, 0.5),\n        \'num_classes\': 1001,\n        \'first_conv\': \'conv_0.conv\',\n        \'classifier\': \'last_linear\',\n    },\n}\n\n\nclass MaxPool(nn.Module):\n\n    def __init__(self, kernel_size, stride=1, padding=1, zero_pad=False):\n        super(MaxPool, self).__init__()\n        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0)) if zero_pad else None\n        self.pool = nn.MaxPool2d(kernel_size, stride=stride, padding=padding)\n\n    def forward(self, x):\n        if self.zero_pad:\n            x = self.zero_pad(x)\n        x = self.pool(x)\n        if self.zero_pad:\n            x = x[:, :, 1:, 1:]\n        return x\n\n\nclass SeparableConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, dw_kernel_size, dw_stride,\n                 dw_padding):\n        super(SeparableConv2d, self).__init__()\n        self.depthwise_conv2d = nn.Conv2d(in_channels, in_channels,\n                                          kernel_size=dw_kernel_size,\n                                          stride=dw_stride, padding=dw_padding,\n                                          groups=in_channels, bias=False)\n        self.pointwise_conv2d = nn.Conv2d(in_channels, out_channels,\n                                          kernel_size=1, bias=False)\n\n    def forward(self, x):\n        x = self.depthwise_conv2d(x)\n        x = self.pointwise_conv2d(x)\n        return x\n\n\nclass BranchSeparables(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 stem_cell=False, zero_pad=False):\n        super(BranchSeparables, self).__init__()\n        padding = kernel_size // 2\n        middle_channels = out_channels if stem_cell else in_channels\n        self.zero_pad = nn.ZeroPad2d((1, 0, 1, 0)) if zero_pad else None\n        self.relu_1 = nn.ReLU()\n        self.separable_1 = SeparableConv2d(in_channels, middle_channels,\n                                           kernel_size, dw_stride=stride,\n                                           dw_padding=padding)\n        self.bn_sep_1 = nn.BatchNorm2d(middle_channels, eps=0.001)\n        self.relu_2 = nn.ReLU()\n        self.separable_2 = SeparableConv2d(middle_channels, out_channels,\n                                           kernel_size, dw_stride=1,\n                                           dw_padding=padding)\n        self.bn_sep_2 = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.relu_1(x)\n        if self.zero_pad:\n            x = self.zero_pad(x)\n        x = self.separable_1(x)\n        if self.zero_pad:\n            x = x[:, :, 1:, 1:].contiguous()\n        x = self.bn_sep_1(x)\n        x = self.relu_2(x)\n        x = self.separable_2(x)\n        x = self.bn_sep_2(x)\n        return x\n\n\nclass ReluConvBn(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1):\n        super(ReluConvBn, self).__init__()\n        self.relu = nn.ReLU()\n        self.conv = nn.Conv2d(in_channels, out_channels,\n                              kernel_size=kernel_size, stride=stride,\n                              bias=False)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\nclass FactorizedReduction(nn.Module):\n\n    def __init__(self, in_channels, out_channels):\n        super(FactorizedReduction, self).__init__()\n        self.relu = nn.ReLU()\n        self.path_1 = nn.Sequential(OrderedDict([\n            (\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False)),\n            (\'conv\', nn.Conv2d(in_channels, out_channels // 2,\n                               kernel_size=1, bias=False)),\n        ]))\n        self.path_2 = nn.Sequential(OrderedDict([\n            (\'pad\', nn.ZeroPad2d((0, 1, 0, 1))),\n            (\'avgpool\', nn.AvgPool2d(1, stride=2, count_include_pad=False)),\n            (\'conv\', nn.Conv2d(in_channels, out_channels // 2,\n                               kernel_size=1, bias=False)),\n        ]))\n        self.final_path_bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.relu(x)\n\n        x_path1 = self.path_1(x)\n\n        x_path2 = self.path_2.pad(x)\n        x_path2 = x_path2[:, :, 1:, 1:]\n        x_path2 = self.path_2.avgpool(x_path2)\n        x_path2 = self.path_2.conv(x_path2)\n\n        out = self.final_path_bn(torch.cat([x_path1, x_path2], 1))\n        return out\n\n\nclass CellBase(nn.Module):\n\n    def cell_forward(self, x_left, x_right):\n        x_comb_iter_0_left = self.comb_iter_0_left(x_left)\n        x_comb_iter_0_right = self.comb_iter_0_right(x_left)\n        x_comb_iter_0 = x_comb_iter_0_left + x_comb_iter_0_right\n\n        x_comb_iter_1_left = self.comb_iter_1_left(x_right)\n        x_comb_iter_1_right = self.comb_iter_1_right(x_right)\n        x_comb_iter_1 = x_comb_iter_1_left + x_comb_iter_1_right\n\n        x_comb_iter_2_left = self.comb_iter_2_left(x_right)\n        x_comb_iter_2_right = self.comb_iter_2_right(x_right)\n        x_comb_iter_2 = x_comb_iter_2_left + x_comb_iter_2_right\n\n        x_comb_iter_3_left = self.comb_iter_3_left(x_comb_iter_2)\n        x_comb_iter_3_right = self.comb_iter_3_right(x_right)\n        x_comb_iter_3 = x_comb_iter_3_left + x_comb_iter_3_right\n\n        x_comb_iter_4_left = self.comb_iter_4_left(x_left)\n        if self.comb_iter_4_right:\n            x_comb_iter_4_right = self.comb_iter_4_right(x_right)\n        else:\n            x_comb_iter_4_right = x_right\n        x_comb_iter_4 = x_comb_iter_4_left + x_comb_iter_4_right\n\n        x_out = torch.cat(\n            [x_comb_iter_0, x_comb_iter_1, x_comb_iter_2, x_comb_iter_3,\n             x_comb_iter_4], 1)\n        return x_out\n\n\nclass CellStem0(CellBase):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right,\n                 out_channels_right):\n        super(CellStem0, self).__init__()\n        self.conv_1x1 = ReluConvBn(in_channels_right, out_channels_right,\n                                   kernel_size=1)\n        self.comb_iter_0_left = BranchSeparables(in_channels_left,\n                                                 out_channels_left,\n                                                 kernel_size=5, stride=2,\n                                                 stem_cell=True)\n        self.comb_iter_0_right = nn.Sequential(OrderedDict([\n            (\'max_pool\', MaxPool(3, stride=2)),\n            (\'conv\', nn.Conv2d(in_channels_left, out_channels_left,\n                               kernel_size=1, bias=False)),\n            (\'bn\', nn.BatchNorm2d(out_channels_left, eps=0.001)),\n        ]))\n        self.comb_iter_1_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=7, stride=2)\n        self.comb_iter_1_right = MaxPool(3, stride=2)\n        self.comb_iter_2_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=5, stride=2)\n        self.comb_iter_2_right = BranchSeparables(out_channels_right,\n                                                  out_channels_right,\n                                                  kernel_size=3, stride=2)\n        self.comb_iter_3_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=3)\n        self.comb_iter_3_right = MaxPool(3, stride=2)\n        self.comb_iter_4_left = BranchSeparables(in_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=3, stride=2,\n                                                 stem_cell=True)\n        self.comb_iter_4_right = ReluConvBn(out_channels_right,\n                                            out_channels_right,\n                                            kernel_size=1, stride=2)\n\n    def forward(self, x_left):\n        x_right = self.conv_1x1(x_left)\n        x_out = self.cell_forward(x_left, x_right)\n        return x_out\n\n\nclass Cell(CellBase):\n\n    def __init__(self, in_channels_left, out_channels_left, in_channels_right,\n                 out_channels_right, is_reduction=False, zero_pad=False,\n                 match_prev_layer_dimensions=False):\n        super(Cell, self).__init__()\n\n        # If `is_reduction` is set to `True` stride 2 is used for\n        # convolutional and pooling layers to reduce the spatial size of\n        # the output of a cell approximately by a factor of 2.\n        stride = 2 if is_reduction else 1\n\n        # If `match_prev_layer_dimensions` is set to `True`\n        # `FactorizedReduction` is used to reduce the spatial size\n        # of the left input of a cell approximately by a factor of 2.\n        self.match_prev_layer_dimensions = match_prev_layer_dimensions\n        if match_prev_layer_dimensions:\n            self.conv_prev_1x1 = FactorizedReduction(in_channels_left,\n                                                     out_channels_left)\n        else:\n            self.conv_prev_1x1 = ReluConvBn(in_channels_left,\n                                            out_channels_left, kernel_size=1)\n\n        self.conv_1x1 = ReluConvBn(in_channels_right, out_channels_right,\n                                   kernel_size=1)\n        self.comb_iter_0_left = BranchSeparables(out_channels_left,\n                                                 out_channels_left,\n                                                 kernel_size=5, stride=stride,\n                                                 zero_pad=zero_pad)\n        self.comb_iter_0_right = MaxPool(3, stride=stride, zero_pad=zero_pad)\n        self.comb_iter_1_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=7, stride=stride,\n                                                 zero_pad=zero_pad)\n        self.comb_iter_1_right = MaxPool(3, stride=stride, zero_pad=zero_pad)\n        self.comb_iter_2_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=5, stride=stride,\n                                                 zero_pad=zero_pad)\n        self.comb_iter_2_right = BranchSeparables(out_channels_right,\n                                                  out_channels_right,\n                                                  kernel_size=3, stride=stride,\n                                                  zero_pad=zero_pad)\n        self.comb_iter_3_left = BranchSeparables(out_channels_right,\n                                                 out_channels_right,\n                                                 kernel_size=3)\n        self.comb_iter_3_right = MaxPool(3, stride=stride, zero_pad=zero_pad)\n        self.comb_iter_4_left = BranchSeparables(out_channels_left,\n                                                 out_channels_left,\n                                                 kernel_size=3, stride=stride,\n                                                 zero_pad=zero_pad)\n        if is_reduction:\n            self.comb_iter_4_right = ReluConvBn(out_channels_right,\n                                                out_channels_right,\n                                                kernel_size=1, stride=stride)\n        else:\n            self.comb_iter_4_right = None\n\n    def forward(self, x_left, x_right):\n        x_left = self.conv_prev_1x1(x_left)\n        x_right = self.conv_1x1(x_right)\n        x_out = self.cell_forward(x_left, x_right)\n        return x_out\n\n\nclass PNASNet5Large(nn.Module):\n    def __init__(self, num_classes=1001, in_chans=3, drop_rate=0.5, global_pool=\'avg\'):\n        super(PNASNet5Large, self).__init__()\n        self.num_classes = num_classes\n        self.num_features = 4320\n        self.drop_rate = drop_rate\n\n        self.conv_0 = nn.Sequential(OrderedDict([\n            (\'conv\', nn.Conv2d(in_chans, 96, kernel_size=3, stride=2, bias=False)),\n            (\'bn\', nn.BatchNorm2d(96, eps=0.001))\n        ]))\n        self.cell_stem_0 = CellStem0(in_channels_left=96, out_channels_left=54,\n                                     in_channels_right=96,\n                                     out_channels_right=54)\n        self.cell_stem_1 = Cell(in_channels_left=96, out_channels_left=108,\n                                in_channels_right=270, out_channels_right=108,\n                                match_prev_layer_dimensions=True,\n                                is_reduction=True)\n        self.cell_0 = Cell(in_channels_left=270, out_channels_left=216,\n                           in_channels_right=540, out_channels_right=216,\n                           match_prev_layer_dimensions=True)\n        self.cell_1 = Cell(in_channels_left=540, out_channels_left=216,\n                           in_channels_right=1080, out_channels_right=216)\n        self.cell_2 = Cell(in_channels_left=1080, out_channels_left=216,\n                           in_channels_right=1080, out_channels_right=216)\n        self.cell_3 = Cell(in_channels_left=1080, out_channels_left=216,\n                           in_channels_right=1080, out_channels_right=216)\n        self.cell_4 = Cell(in_channels_left=1080, out_channels_left=432,\n                           in_channels_right=1080, out_channels_right=432,\n                           is_reduction=True, zero_pad=True)\n        self.cell_5 = Cell(in_channels_left=1080, out_channels_left=432,\n                           in_channels_right=2160, out_channels_right=432,\n                           match_prev_layer_dimensions=True)\n        self.cell_6 = Cell(in_channels_left=2160, out_channels_left=432,\n                           in_channels_right=2160, out_channels_right=432)\n        self.cell_7 = Cell(in_channels_left=2160, out_channels_left=432,\n                           in_channels_right=2160, out_channels_right=432)\n        self.cell_8 = Cell(in_channels_left=2160, out_channels_left=864,\n                           in_channels_right=2160, out_channels_right=864,\n                           is_reduction=True)\n        self.cell_9 = Cell(in_channels_left=2160, out_channels_left=864,\n                           in_channels_right=4320, out_channels_right=864,\n                           match_prev_layer_dimensions=True)\n        self.cell_10 = Cell(in_channels_left=4320, out_channels_left=864,\n                            in_channels_right=4320, out_channels_right=864)\n        self.cell_11 = Cell(in_channels_left=4320, out_channels_left=864,\n                            in_channels_right=4320, out_channels_right=864)\n        self.relu = nn.ReLU()\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.last_linear = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n\n    def get_classifier(self):\n        return self.last_linear\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.num_classes = num_classes\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        del self.last_linear\n        if num_classes:\n            self.last_linear = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n        else:\n            self.last_linear = None\n\n    def forward_features(self, x):\n        x_conv_0 = self.conv_0(x)\n        x_stem_0 = self.cell_stem_0(x_conv_0)\n        x_stem_1 = self.cell_stem_1(x_conv_0, x_stem_0)\n        x_cell_0 = self.cell_0(x_stem_0, x_stem_1)\n        x_cell_1 = self.cell_1(x_stem_1, x_cell_0)\n        x_cell_2 = self.cell_2(x_cell_0, x_cell_1)\n        x_cell_3 = self.cell_3(x_cell_1, x_cell_2)\n        x_cell_4 = self.cell_4(x_cell_2, x_cell_3)\n        x_cell_5 = self.cell_5(x_cell_3, x_cell_4)\n        x_cell_6 = self.cell_6(x_cell_4, x_cell_5)\n        x_cell_7 = self.cell_7(x_cell_5, x_cell_6)\n        x_cell_8 = self.cell_8(x_cell_6, x_cell_7)\n        x_cell_9 = self.cell_9(x_cell_7, x_cell_8)\n        x_cell_10 = self.cell_10(x_cell_8, x_cell_9)\n        x_cell_11 = self.cell_11(x_cell_9, x_cell_10)\n        x = self.relu(x_cell_11)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x).flatten(1)\n        if self.drop_rate > 0:\n            x = F.dropout(x, self.drop_rate, training=self.training)\n        x = self.last_linear(x)\n        return x\n\n\n@register_model\ndef pnasnet5large(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    r""""""PNASNet-5 model architecture from the\n    `""Progressive Neural Architecture Search""\n    <https://arxiv.org/abs/1712.00559>`_ paper.\n    """"""\n    default_cfg = default_cfgs[\'pnasnet5large\']\n    model = PNASNet5Large(num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n\n    return model\n'"
timm/models/registry.py,0,"b'import sys\nimport re\nimport fnmatch\nfrom collections import defaultdict\n\n__all__ = [\'list_models\', \'is_model\', \'model_entrypoint\', \'list_modules\', \'is_model_in_modules\']\n\n_module_to_models = defaultdict(set)  # dict of sets to check membership of model in module\n_model_to_module = {}  # mapping of model names to module names\n_model_entrypoints = {}  # mapping of model names to entrypoint fns\n_model_has_pretrained = set()  # set of model names that have pretrained weight url present\n\n\ndef register_model(fn):\n    # lookup containing module\n    mod = sys.modules[fn.__module__]\n    module_name_split = fn.__module__.split(\'.\')\n    module_name = module_name_split[-1] if len(module_name_split) else \'\'\n\n    # add model to __all__ in module\n    model_name = fn.__name__\n    if hasattr(mod, \'__all__\'):\n        mod.__all__.append(model_name)\n    else:\n        mod.__all__ = [model_name]\n\n    # add entries to registry dict/sets\n    _model_entrypoints[model_name] = fn\n    _model_to_module[model_name] = module_name\n    _module_to_models[module_name].add(model_name)\n    has_pretrained = False  # check if model has a pretrained url to allow filtering on this\n    if hasattr(mod, \'default_cfgs\') and model_name in mod.default_cfgs:\n        # this will catch all models that have entrypoint matching cfg key, but miss any aliasing\n        # entrypoints or non-matching combos\n        has_pretrained = \'url\' in mod.default_cfgs[model_name] and \'http\' in mod.default_cfgs[model_name][\'url\']\n    if has_pretrained:\n        _model_has_pretrained.add(model_name)\n    return fn\n\n\ndef _natural_key(string_):\n    return [int(s) if s.isdigit() else s for s in re.split(r\'(\\d+)\', string_.lower())]\n\n\ndef list_models(filter=\'\', module=\'\', pretrained=False, exclude_filters=\'\'):\n    """""" Return list of available model names, sorted alphabetically\n\n    Args:\n        filter (str) - Wildcard filter string that works with fnmatch\n        module (str) - Limit model selection to a specific sub-module (ie \'gen_efficientnet\')\n        pretrained (bool) - Include only models with pretrained weights if True\n        exclude_filters (str or list[str]) - Wildcard filters to exclude models after including them with filter\n\n    Example:\n        model_list(\'gluon_resnet*\') -- returns all models starting with \'gluon_resnet\'\n        model_list(\'*resnext*, \'resnet\') -- returns all models with \'resnext\' in \'resnet\' module\n    """"""\n    if module:\n        models = list(_module_to_models[module])\n    else:\n        models = _model_entrypoints.keys()\n    if filter:\n        models = fnmatch.filter(models, filter)  # include these models\n    if exclude_filters:\n        if not isinstance(exclude_filters, list):\n            exclude_filters = [exclude_filters]\n        for xf in exclude_filters:\n            exclude_models = fnmatch.filter(models, xf)  # exclude these models\n            if len(exclude_models):\n                models = set(models).difference(exclude_models)\n    if pretrained:\n        models = _model_has_pretrained.intersection(models)\n    return list(sorted(models, key=_natural_key))\n\n\ndef is_model(model_name):\n    """""" Check if a model name exists\n    """"""\n    return model_name in _model_entrypoints\n\n\ndef model_entrypoint(model_name):\n    """"""Fetch a model entrypoint for specified model name\n    """"""\n    return _model_entrypoints[model_name]\n\n\ndef list_modules():\n    """""" Return list of module names that contain models / model entrypoints\n    """"""\n    modules = _module_to_models.keys()\n    return list(sorted(modules))\n\n\ndef is_model_in_modules(model_name, module_names):\n    """"""Check if a model exists within a subset of modules\n    Args:\n        model_name (str) - name of model to check\n        module_names (tuple, list, set) - names of modules to search in\n    """"""\n    assert isinstance(module_names, (tuple, list, set))\n    return any(model_name in _module_to_models[n] for n in module_names)\n\n'"
timm/models/regnet.py,2,"b'""""""RegNet\n\nPaper: `Designing Network Design Spaces` - https://arxiv.org/abs/2003.13678\nOriginal Impl: https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py\n\nBased on original PyTorch impl linked above, but re-wrote to use my own blocks (adapted from ResNet here)\nand cleaned up with more descriptive variable names.\n\nWeights from original impl have been modified\n* first layer from BGR -> RGB as most PyTorch models are\n* removed training specific dict entries from checkpoints and keep model state_dict only\n* remap names to match the ones here\n\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SelectAdaptivePool2d, AvgPool2dSame, ConvBnAct, SEModule\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\n\ndef _mcfg(**kwargs):\n    cfg = dict(se_ratio=0., bottle_ratio=1., stem_width=32)\n    cfg.update(**kwargs)\n    return cfg\n\n\n# Model FLOPS = three trailing digits * 10^8\nmodel_cfgs = dict(\n    x_002=_mcfg(w0=24, wa=36.44, wm=2.49, group_w=8, depth=13),\n    x_004=_mcfg(w0=24, wa=24.48, wm=2.54, group_w=16, depth=22),\n    x_006=_mcfg(w0=48, wa=36.97, wm=2.24, group_w=24, depth=16),\n    x_008=_mcfg(w0=56, wa=35.73, wm=2.28, group_w=16, depth=16),\n    x_016=_mcfg(w0=80, wa=34.01, wm=2.25, group_w=24, depth=18),\n    x_032=_mcfg(w0=88, wa=26.31, wm=2.25, group_w=48, depth=25),\n    x_040=_mcfg(w0=96, wa=38.65, wm=2.43, group_w=40, depth=23),\n    x_064=_mcfg(w0=184, wa=60.83, wm=2.07, group_w=56, depth=17),\n    x_080=_mcfg(w0=80, wa=49.56, wm=2.88, group_w=120, depth=23),\n    x_120=_mcfg(w0=168, wa=73.36, wm=2.37, group_w=112, depth=19),\n    x_160=_mcfg(w0=216, wa=55.59, wm=2.1, group_w=128, depth=22),\n    x_320=_mcfg(w0=320, wa=69.86, wm=2.0, group_w=168, depth=23),\n    y_002=_mcfg(w0=24, wa=36.44, wm=2.49, group_w=8, depth=13, se_ratio=0.25),\n    y_004=_mcfg(w0=48, wa=27.89, wm=2.09, group_w=8, depth=16, se_ratio=0.25),\n    y_006=_mcfg(w0=48, wa=32.54, wm=2.32, group_w=16, depth=15, se_ratio=0.25),\n    y_008=_mcfg(w0=56, wa=38.84, wm=2.4, group_w=16, depth=14, se_ratio=0.25),\n    y_016=_mcfg(w0=48, wa=20.71, wm=2.65, group_w=24, depth=27, se_ratio=0.25),\n    y_032=_mcfg(w0=80, wa=42.63, wm=2.66, group_w=24, depth=21, se_ratio=0.25),\n    y_040=_mcfg(w0=96, wa=31.41, wm=2.24, group_w=64, depth=22, se_ratio=0.25),\n    y_064=_mcfg(w0=112, wa=33.22, wm=2.27, group_w=72, depth=25, se_ratio=0.25),\n    y_080=_mcfg(w0=192, wa=76.82, wm=2.19, group_w=56, depth=17, se_ratio=0.25),\n    y_120=_mcfg(w0=168, wa=73.36, wm=2.37, group_w=112, depth=19, se_ratio=0.25),\n    y_160=_mcfg(w0=200, wa=106.23, wm=2.48, group_w=112, depth=18, se_ratio=0.25),\n    y_320=_mcfg(w0=232, wa=115.89, wm=2.53, group_w=232, depth=20, se_ratio=0.25),\n)\n\n\ndef _cfg(url=\'\'):\n    return {\n        \'url\': url, \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (7, 7),\n        \'crop_pct\': 0.875, \'interpolation\': \'bicubic\',\n        \'mean\': IMAGENET_DEFAULT_MEAN, \'std\': IMAGENET_DEFAULT_STD,\n        \'first_conv\': \'stem.conv\', \'classifier\': \'head.fc\',\n    }\n\n\ndefault_cfgs = dict(\n    x_002=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_002-e7e85e5c.pth\'),\n    x_004=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_004-7d0e9424.pth\'),\n    x_006=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_006-85ec1baa.pth\'),\n    x_008=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_008-d8b470eb.pth\'),\n    x_016=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_016-65ca972a.pth\'),\n    x_032=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_032-ed0c7f7e.pth\'),\n    x_040=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_040-73c2a654.pth\'),\n    x_064=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_064-29278baa.pth\'),\n    x_080=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_080-7c7fcab1.pth\'),\n    x_120=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_120-65d5521e.pth\'),\n    x_160=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_160-c98c4112.pth\'),\n    x_320=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_320-8ea38b93.pth\'),\n    y_002=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_002-e68ca334.pth\'),\n    y_004=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_004-0db870e6.pth\'),\n    y_006=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_006-c67e57ec.pth\'),\n    y_008=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_008-dc900dbe.pth\'),\n    y_016=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_016-54367f74.pth\'),\n    y_032=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_032-62b47782.pth\'),\n    y_040=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_040-f0d569f9.pth\'),\n    y_064=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_064-0a48325c.pth\'),\n    y_080=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_080-e7f3eb93.pth\'),\n    y_120=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_120-721ba79a.pth\'),\n    y_160=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_160-d64013cd.pth\'),\n    y_320=_cfg(url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnety_320-ba464b29.pth\'),\n)\n\n\ndef quantize_float(f, q):\n    """"""Converts a float to closest non-zero int divisible by q.""""""\n    return int(round(f / q) * q)\n\n\ndef adjust_widths_groups_comp(widths, bottle_ratios, groups):\n    """"""Adjusts the compatibility of widths and groups.""""""\n    bottleneck_widths = [int(w * b) for w, b in zip(widths, bottle_ratios)]\n    groups = [min(g, w_bot) for g, w_bot in zip(groups, bottleneck_widths)]\n    bottleneck_widths = [quantize_float(w_bot, g) for w_bot, g in zip(bottleneck_widths, groups)]\n    widths = [int(w_bot / b) for w_bot, b in zip(bottleneck_widths, bottle_ratios)]\n    return widths, groups\n\n\ndef generate_regnet(width_slope, width_initial, width_mult, depth, q=8):\n    """"""Generates per block widths from RegNet parameters.""""""\n    assert width_slope >= 0 and width_initial > 0 and width_mult > 1 and width_initial % q == 0\n    widths_cont = np.arange(depth) * width_slope + width_initial\n    width_exps = np.round(np.log(widths_cont / width_initial) / np.log(width_mult))\n    widths = width_initial * np.power(width_mult, width_exps)\n    widths = np.round(np.divide(widths, q)) * q\n    num_stages, max_stage = len(np.unique(widths)), width_exps.max() + 1\n    widths, widths_cont = widths.astype(int).tolist(), widths_cont.tolist()\n    return widths, num_stages, max_stage, widths_cont\n\n\nclass Bottleneck(nn.Module):\n    """""" RegNet Bottleneck\n\n    This is almost exactly the same as a ResNet Bottlneck. The main difference is the SE block is moved from\n    after conv3 to after conv2. Otherwise, it\'s just redefining the arguments for groups/bottleneck channels.\n    """"""\n\n    def __init__(self, in_chs, out_chs, stride=1, bottleneck_ratio=1, group_width=1, se_ratio=0.25,\n                 dilation=1, first_dilation=None, downsample=None, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d,\n                 aa_layer=None, drop_block=None, drop_path=None):\n        super(Bottleneck, self).__init__()\n        bottleneck_chs = int(round(out_chs * bottleneck_ratio))\n        groups = bottleneck_chs // group_width\n        first_dilation = first_dilation or dilation\n\n        cargs = dict(act_layer=act_layer, norm_layer=norm_layer, aa_layer=aa_layer, drop_block=drop_block)\n        self.conv1 = ConvBnAct(in_chs, bottleneck_chs, kernel_size=1, **cargs)\n        self.conv2 = ConvBnAct(\n            bottleneck_chs, bottleneck_chs, kernel_size=3, stride=stride, dilation=first_dilation,\n            groups=groups, **cargs)\n        if se_ratio:\n            se_channels = int(round(in_chs * se_ratio))\n            self.se = SEModule(bottleneck_chs, reduction_channels=se_channels)\n        else:\n            self.se = None\n        cargs[\'act_layer\'] = None\n        self.conv3 = ConvBnAct(bottleneck_chs, out_chs, kernel_size=1, **cargs)\n        self.act3 = act_layer(inplace=True)\n        self.downsample = downsample\n        self.drop_path = drop_path\n\n    def zero_init_last_bn(self):\n        nn.init.zeros_(self.conv3.bn.weight)\n\n    def forward(self, x):\n        shortcut = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        if self.se is not None:\n            x = self.se(x)\n        x = self.conv3(x)\n        if self.drop_path is not None:\n            x = self.drop_path(x)\n        if self.downsample is not None:\n            shortcut = self.downsample(shortcut)\n        x += shortcut\n        x = self.act3(x)\n        return x\n\n\ndef downsample_conv(\n        in_chs, out_chs, kernel_size, stride=1, dilation=1, first_dilation=None, norm_layer=None):\n    norm_layer = norm_layer or nn.BatchNorm2d\n    kernel_size = 1 if stride == 1 and dilation == 1 else kernel_size\n    first_dilation = (first_dilation or dilation) if kernel_size > 1 else 1\n    return ConvBnAct(\n        in_chs, out_chs, kernel_size, stride=stride, dilation=first_dilation, norm_layer=norm_layer, act_layer=None)\n\n\ndef downsample_avg(\n        in_chs, out_chs, kernel_size, stride=1, dilation=1, first_dilation=None, norm_layer=None):\n    """""" AvgPool Downsampling as in \'D\' ResNet variants. This is not in RegNet space but I might experiment.""""""\n    norm_layer = norm_layer or nn.BatchNorm2d\n    avg_stride = stride if dilation == 1 else 1\n    pool = nn.Identity()\n    if stride > 1 or dilation > 1:\n        avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d\n        pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)\n    return nn.Sequential(*[\n        pool, ConvBnAct(in_chs, out_chs, 1, stride=1, norm_layer=norm_layer, act_layer=None)])\n\n\nclass RegStage(nn.Module):\n    """"""Stage (sequence of blocks w/ the same output shape).""""""\n\n    def __init__(self, in_chs, out_chs, stride, depth, block_fn, bottle_ratio, group_width, se_ratio):\n        super(RegStage, self).__init__()\n        block_kwargs = {}  # FIXME setup to pass various aa, norm, act layer common args\n        for i in range(depth):\n            block_stride = stride if i == 0 else 1\n            block_in_chs = in_chs if i == 0 else out_chs\n            if (block_in_chs != out_chs) or (block_stride != 1):\n                proj_block = downsample_conv(block_in_chs, out_chs, 1, stride)\n            else:\n                proj_block = None\n\n            name = ""b{}"".format(i + 1)\n            self.add_module(\n                name, block_fn(\n                    block_in_chs, out_chs, block_stride, bottle_ratio, group_width, se_ratio,\n                    downsample=proj_block, **block_kwargs)\n            )\n\n    def forward(self, x):\n        for block in self.children():\n            x = block(x)\n        return x\n\n\nclass ClassifierHead(nn.Module):\n    """"""Head.""""""\n\n    def __init__(self, in_chs, num_classes, pool_type=\'avg\', drop_rate=0.):\n        super(ClassifierHead, self).__init__()\n        self.drop_rate = drop_rate\n        self.global_pool = SelectAdaptivePool2d(pool_type=pool_type)\n        if num_classes > 0:\n            self.fc = nn.Linear(in_chs, num_classes, bias=True)\n        else:\n            self.fc = nn.Identity()\n\n    def forward(self, x):\n        x = self.global_pool(x).flatten(1)\n        if self.drop_rate:\n            x = F.dropout(x, p=float(self.drop_rate), training=self.training)\n        x = self.fc(x)\n        return x\n\n\nclass RegNet(nn.Module):\n    """"""RegNet model.\n\n    Paper: https://arxiv.org/abs/2003.13678\n    Original Impl: https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py\n    """"""\n\n    def __init__(self, cfg, in_chans=3, num_classes=1000, global_pool=\'avg\', drop_rate=0.,\n                 zero_init_last_bn=True):\n        super().__init__()\n        # TODO add drop block, drop path, anti-aliasing, custom bn/act args\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n\n        # Construct the stem\n        stem_width = cfg[\'stem_width\']\n        self.stem = ConvBnAct(in_chans, stem_width, 3, stride=2)\n        \n        # Construct the stages\n        block_fn = Bottleneck\n        prev_width = stem_width\n        stage_params = self._get_stage_params(cfg)\n        se_ratio = cfg[\'se_ratio\']\n        for i, (d, w, s, br, gw) in enumerate(stage_params):\n            self.add_module(\n                ""s{}"".format(i + 1), RegStage(prev_width, w, s, d, block_fn, br, gw, se_ratio))\n            prev_width = w\n\n        # Construct the head\n        self.num_features = prev_width\n        self.head = ClassifierHead(\n            in_chs=prev_width, num_classes=num_classes, pool_type=global_pool, drop_rate=drop_rate)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, mean=0.0, std=0.01)\n                nn.init.zeros_(m.bias)\n        if zero_init_last_bn:\n            for m in self.modules():\n                if hasattr(m, \'zero_init_last_bn\'):\n                    m.zero_init_last_bn()\n\n    def _get_stage_params(self, cfg, stride=2):\n        # Generate RegNet ws per block\n        w_a, w_0, w_m, d = cfg[\'wa\'], cfg[\'w0\'], cfg[\'wm\'], cfg[\'depth\']\n        widths, num_stages, _, _ = generate_regnet(w_a, w_0, w_m, d)\n\n        # Convert to per stage format\n        stage_widths, stage_depths = np.unique(widths, return_counts=True)\n\n        # Use the same group width, bottleneck mult and stride for each stage\n        stage_groups = [cfg[\'group_w\'] for _ in range(num_stages)]\n        stage_bottle_ratios = [cfg[\'bottle_ratio\'] for _ in range(num_stages)]\n        stage_strides = [stride for _ in range(num_stages)]\n        # FIXME add dilation / output_stride support\n\n        # Adjust the compatibility of ws and gws\n        stage_widths, stage_groups = adjust_widths_groups_comp(stage_widths, stage_bottle_ratios, stage_groups)\n        stage_params = list(zip(stage_depths, stage_widths, stage_strides, stage_bottle_ratios, stage_groups))\n        return stage_params\n\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.head = ClassifierHead(self.num_features, num_classes, pool_type=global_pool, drop_rate=self.drop_rate)\n\n    def forward_features(self, x):\n        for block in list(self.children())[:-1]:\n            x = block(x)\n        return x\n\n    def forward(self, x):\n        for block in self.children():\n            x = block(x)\n        return x\n\n\ndef _regnet(variant, pretrained, **kwargs):\n    load_strict = True\n    model_class = RegNet\n    if kwargs.pop(\'features_only\', False):\n        assert False, \'Not Implemented\'  # TODO\n        load_strict = False\n        kwargs.pop(\'num_classes\', 0)\n    model_cfg = model_cfgs[variant]\n    default_cfg = default_cfgs[variant]\n    model = model_class(model_cfg, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(\n            model, default_cfg,\n            num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3), strict=load_strict)\n    return model\n\n\n@register_model\ndef regnetx_002(pretrained=False, **kwargs):\n    """"""RegNetX-200MF""""""\n    return _regnet(\'x_002\', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_004(pretrained=False, **kwargs):\n    """"""RegNetX-400MF""""""\n    return _regnet(\'x_004\', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_006(pretrained=False, **kwargs):\n    """"""RegNetX-600MF""""""\n    return _regnet(\'x_006\', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_008(pretrained=False, **kwargs):\n    """"""RegNetX-800MF""""""\n    return _regnet(\'x_008\', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_016(pretrained=False, **kwargs):\n    """"""RegNetX-1.6GF""""""\n    return _regnet(\'x_016\', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_032(pretrained=False, **kwargs):\n    """"""RegNetX-3.2GF""""""\n    return _regnet(\'x_032\', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_040(pretrained=False, **kwargs):\n    """"""RegNetX-4.0GF""""""\n    return _regnet(\'x_040\', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_064(pretrained=False, **kwargs):\n    """"""RegNetX-6.4GF""""""\n    return _regnet(\'x_064\', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_080(pretrained=False, **kwargs):\n    """"""RegNetX-8.0GF""""""\n    return _regnet(\'x_080\', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_120(pretrained=False, **kwargs):\n    """"""RegNetX-12GF""""""\n    return _regnet(\'x_120\', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_160(pretrained=False, **kwargs):\n    """"""RegNetX-16GF""""""\n    return _regnet(\'x_160\', pretrained, **kwargs)\n\n\n@register_model\ndef regnetx_320(pretrained=False, **kwargs):\n    """"""RegNetX-32GF""""""\n    return _regnet(\'x_320\', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_002(pretrained=False, **kwargs):\n    """"""RegNetY-200MF""""""\n    return _regnet(\'y_002\', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_004(pretrained=False, **kwargs):\n    """"""RegNetY-400MF""""""\n    return _regnet(\'y_004\', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_006(pretrained=False, **kwargs):\n    """"""RegNetY-600MF""""""\n    return _regnet(\'y_006\', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_008(pretrained=False, **kwargs):\n    """"""RegNetY-800MF""""""\n    return _regnet(\'y_008\', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_016(pretrained=False, **kwargs):\n    """"""RegNetY-1.6GF""""""\n    return _regnet(\'y_016\', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_032(pretrained=False, **kwargs):\n    """"""RegNetY-3.2GF""""""\n    return _regnet(\'y_032\', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_040(pretrained=False, **kwargs):\n    """"""RegNetY-4.0GF""""""\n    return _regnet(\'y_040\', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_064(pretrained=False, **kwargs):\n    """"""RegNetY-6.4GF""""""\n    return _regnet(\'y_064\', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_080(pretrained=False, **kwargs):\n    """"""RegNetY-8.0GF""""""\n    return _regnet(\'y_080\', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_120(pretrained=False, **kwargs):\n    """"""RegNetY-12GF""""""\n    return _regnet(\'y_120\', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_160(pretrained=False, **kwargs):\n    """"""RegNetY-16GF""""""\n    return _regnet(\'y_160\', pretrained, **kwargs)\n\n\n@register_model\ndef regnety_320(pretrained=False, **kwargs):\n    """"""RegNetY-32GF""""""\n    return _regnet(\'y_320\', pretrained, **kwargs)\n'"
timm/models/res2net.py,4,"b'"""""" Res2Net and Res2NeXt\nAdapted from Official Pytorch impl at: https://github.com/gasvn/Res2Net/\nPaper: `Res2Net: A New Multi-scale Backbone Architecture` - https://arxiv.org/abs/1904.01169\n""""""\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .resnet import ResNet\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SEModule\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\n__all__ = []\n\n\ndef _cfg(url=\'\', **kwargs):\n    return {\n        \'url\': url,\n        \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (7, 7),\n        \'crop_pct\': 0.875, \'interpolation\': \'bilinear\',\n        \'mean\': IMAGENET_DEFAULT_MEAN, \'std\': IMAGENET_DEFAULT_STD,\n        \'first_conv\': \'conv1\', \'classifier\': \'fc\',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    \'res2net50_26w_4s\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_4s-06e79181.pth\'),\n    \'res2net50_48w_2s\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_48w_2s-afed724a.pth\'),\n    \'res2net50_14w_8s\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_14w_8s-6527dddc.pth\'),\n    \'res2net50_26w_6s\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_6s-19041792.pth\'),\n    \'res2net50_26w_8s\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net50_26w_8s-2c7c9f12.pth\'),\n    \'res2net101_26w_4s\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2net101_26w_4s-02a759a1.pth\'),\n    \'res2next50\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-res2net/res2next50_4s-6ef7e7bf.pth\'),\n}\n\n\nclass Bottle2neck(nn.Module):\n    """""" Res2Net/Res2NeXT Bottleneck\n    Adapted from https://github.com/gasvn/Res2Net/blob/master/res2net.py\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 cardinality=1, base_width=26, scale=4, dilation=1, first_dilation=None,\n                 act_layer=nn.ReLU, norm_layer=None, attn_layer=None, **_):\n        super(Bottle2neck, self).__init__()\n        self.scale = scale\n        self.is_first = stride > 1 or downsample is not None\n        self.num_scales = max(1, scale - 1)\n        width = int(math.floor(planes * (base_width / 64.0))) * cardinality\n        self.width = width\n        outplanes = planes * self.expansion\n        first_dilation = first_dilation or dilation\n\n        self.conv1 = nn.Conv2d(inplanes, width * scale, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(width * scale)\n\n        convs = []\n        bns = []\n        for i in range(self.num_scales):\n            convs.append(nn.Conv2d(\n                width, width, kernel_size=3, stride=stride, padding=first_dilation,\n                dilation=first_dilation, groups=cardinality, bias=False))\n            bns.append(norm_layer(width))\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList(bns)\n        if self.is_first:\n            # FIXME this should probably have count_include_pad=False, but hurts original weights\n            self.pool = nn.AvgPool2d(kernel_size=3, stride=stride, padding=1)\n\n        self.conv3 = nn.Conv2d(width * scale, outplanes, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(outplanes)\n        self.se = attn_layer(outplanes) if attn_layer is not None else None\n\n        self.relu = act_layer(inplace=True)\n        self.downsample = downsample\n\n    def zero_init_last_bn(self):\n        nn.init.zeros_(self.bn3.weight)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        spx = torch.split(out, self.width, 1)\n        spo = []\n        for i, (conv, bn) in enumerate(zip(self.convs, self.bns)):\n            sp = spx[i] if i == 0 or self.is_first else sp + spx[i]\n            sp = conv(sp)\n            sp = bn(sp)\n            sp = self.relu(sp)\n            spo.append(sp)\n        if self.scale > 1 :\n            spo.append(self.pool(spx[-1]) if self.is_first else spx[-1])\n        out = torch.cat(spo, 1)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.se is not None:\n            out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\n@register_model\ndef res2net50_26w_4s(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a Res2Net-50_26w_4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    default_cfg = default_cfgs[\'res2net50_26w_4s\']\n    res2net_block_args = dict(scale=4)\n    model = ResNet(Bottle2neck, [3, 4, 6, 3], base_width=26,\n                   num_classes=num_classes, in_chans=in_chans, block_args=res2net_block_args, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef res2net101_26w_4s(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a Res2Net-50_26w_4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    default_cfg = default_cfgs[\'res2net101_26w_4s\']\n    res2net_block_args = dict(scale=4)\n    model = ResNet(Bottle2neck, [3, 4, 23, 3], base_width=26,\n                   num_classes=num_classes, in_chans=in_chans, block_args=res2net_block_args, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef res2net50_26w_6s(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a Res2Net-50_26w_4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    default_cfg = default_cfgs[\'res2net50_26w_6s\']\n    res2net_block_args = dict(scale=6)\n    model = ResNet(Bottle2neck, [3, 4, 6, 3], base_width=26,\n                   num_classes=num_classes, in_chans=in_chans, block_args=res2net_block_args, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef res2net50_26w_8s(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a Res2Net-50_26w_4s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    default_cfg = default_cfgs[\'res2net50_26w_8s\']\n    res2net_block_args = dict(scale=8)\n    model = ResNet(Bottle2neck, [3, 4, 6, 3], base_width=26,\n                   num_classes=num_classes, in_chans=in_chans, block_args=res2net_block_args, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef res2net50_48w_2s(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a Res2Net-50_48w_2s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    default_cfg = default_cfgs[\'res2net50_48w_2s\']\n    res2net_block_args = dict(scale=2)\n    model = ResNet(Bottle2neck, [3, 4, 6, 3], base_width=48,\n                   num_classes=num_classes, in_chans=in_chans, block_args=res2net_block_args, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef res2net50_14w_8s(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a Res2Net-50_14w_8s model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    default_cfg = default_cfgs[\'res2net50_14w_8s\']\n    res2net_block_args = dict(scale=8)\n    model = ResNet(Bottle2neck, [3, 4, 6, 3], base_width=14, num_classes=num_classes, in_chans=in_chans,\n                   block_args=res2net_block_args, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef res2next50(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Construct Res2NeXt-50 4s\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    default_cfg = default_cfgs[\'res2next50\']\n    res2net_block_args = dict(scale=4)\n    model = ResNet(Bottle2neck, [3, 4, 6, 3], base_width=4, cardinality=8,\n                   num_classes=num_classes, in_chans=in_chans, block_args=res2net_block_args, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n'"
timm/models/resnest.py,1,"b'"""""" ResNeSt Models\n\nPaper: `ResNeSt: Split-Attention Networks` - https://arxiv.org/abs/2004.08955\n\nAdapted from original PyTorch impl w/ weights at https://github.com/zhanghang1989/ResNeSt by Hang Zhang\n\nModified for torchscript compat, and consistency with timm by Ross Wightman\n""""""\nimport math\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\nfrom timm.models.layers import DropBlock2d\nfrom .helpers import load_pretrained\nfrom .layers import SelectiveKernelConv, ConvBnAct, create_attn\nfrom .layers.split_attn import SplitAttnConv2d\nfrom .registry import register_model\nfrom .resnet import ResNet\n\n\ndef _cfg(url=\'\', **kwargs):\n    return {\n        \'url\': url,\n        \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (7, 7),\n        \'crop_pct\': 0.875, \'interpolation\': \'bilinear\',\n        \'mean\': IMAGENET_DEFAULT_MEAN, \'std\': IMAGENET_DEFAULT_STD,\n        \'first_conv\': \'conv1\', \'classifier\': \'fc\',\n        **kwargs\n    }\n\ndefault_cfgs = {\n    \'resnest14d\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gluon_resnest14-9c8fe254.pth\'),\n    \'resnest26d\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/gluon_resnest26-50eb607c.pth\'),\n    \'resnest50d\': _cfg(\n        url=\'https://hangzh.s3.amazonaws.com/encoding/models/resnest50-528c19ca.pth\'),\n    \'resnest101e\': _cfg(\n        url=\'https://hangzh.s3.amazonaws.com/encoding/models/resnest101-22405ba7.pth\',\n        input_size=(3, 256, 256), pool_size=(8, 8)),\n    \'resnest200e\': _cfg(\n        url=\'https://hangzh.s3.amazonaws.com/encoding/models/resnest200-75117900.pth\',\n        input_size=(3, 320, 320), pool_size=(10, 10)),\n    \'resnest269e\': _cfg(\n        url=\'https://hangzh.s3.amazonaws.com/encoding/models/resnest269-0cc87c48.pth\',\n        input_size=(3, 416, 416), pool_size=(13, 13)),\n    \'resnest50d_4s2x40d\': _cfg(\n        url=\'https://hangzh.s3.amazonaws.com/encoding/models/resnest50_fast_4s2x40d-41d14ed0.pth\',\n        interpolation=\'bicubic\'),\n    \'resnest50d_1s4x24d\': _cfg(\n        url=\'https://hangzh.s3.amazonaws.com/encoding/models/resnest50_fast_1s4x24d-d4a4f76f.pth\',\n        interpolation=\'bicubic\')\n}\n\n\nclass ResNestBottleneck(nn.Module):\n    """"""ResNet Bottleneck\n    """"""\n    # pylint: disable=unused-argument\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 radix=1, cardinality=1, base_width=64, avd=False, avd_first=False, is_first=False,\n                 reduce_first=1, dilation=1, first_dilation=None, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d,\n                 attn_layer=None, aa_layer=None, drop_block=None, drop_path=None):\n        super(ResNestBottleneck, self).__init__()\n        assert reduce_first == 1  # not supported\n        assert attn_layer is None  # not supported\n        assert aa_layer is None  # TODO not yet supported\n        assert drop_path is None  # TODO not yet supported\n\n        group_width = int(planes * (base_width / 64.)) * cardinality\n        first_dilation = first_dilation or dilation\n        if avd and (stride > 1 or is_first):\n            avd_stride = stride\n            stride = 1\n        else:\n            avd_stride = 0\n        self.radix = radix\n        self.drop_block = drop_block\n\n        self.conv1 = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(group_width)\n        self.act1 = act_layer(inplace=True)\n        self.avd_first = nn.AvgPool2d(3, avd_stride, padding=1) if avd_stride > 0 and avd_first else None\n\n        if self.radix >= 1:\n            self.conv2 = SplitAttnConv2d(\n                group_width, group_width, kernel_size=3, stride=stride, padding=first_dilation,\n                dilation=first_dilation, groups=cardinality, radix=radix, norm_layer=norm_layer, drop_block=drop_block)\n            self.bn2 = None  # FIXME revisit, here to satisfy current torchscript fussyness\n            self.act2 = None\n        else:\n            self.conv2 = nn.Conv2d(\n                group_width, group_width, kernel_size=3, stride=stride, padding=first_dilation,\n                dilation=first_dilation, groups=cardinality, bias=False)\n            self.bn2 = norm_layer(group_width)\n            self.act2 = act_layer(inplace=True)\n        self.avd_last = nn.AvgPool2d(3, avd_stride, padding=1) if avd_stride > 0 and not avd_first else None\n\n        self.conv3 = nn.Conv2d(group_width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes*4)\n        self.act3 = act_layer(inplace=True)\n        self.downsample = downsample\n\n    def zero_init_last_bn(self):\n        nn.init.zeros_(self.bn3.weight)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        if self.drop_block is not None:\n            out = self.drop_block(out)\n        out = self.act1(out)\n\n        if self.avd_first is not None:\n            out = self.avd_first(out)\n\n        out = self.conv2(out)\n        if self.bn2 is not None:\n            out = self.bn2(out)\n            if self.drop_block is not None:\n                out = self.drop_block(out)\n            out = self.act2(out)\n\n        if self.avd_last is not None:\n            out = self.avd_last(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        if self.drop_block is not None:\n            out = self.drop_block(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.act3(out)\n        return out\n\n\n@register_model\ndef resnest14d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """""" ResNeSt-14d model. Weights ported from GluonCV.\n    """"""\n    default_cfg = default_cfgs[\'resnest14d\']\n    model = ResNet(\n        ResNestBottleneck, [1, 1, 1, 1], num_classes=num_classes, in_chans=in_chans,\n        stem_type=\'deep\', stem_width=32, avg_down=True, base_width=64, cardinality=1,\n        block_args=dict(radix=2, avd=True, avd_first=False), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnest26d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """""" ResNeSt-26d model. Weights ported from GluonCV.\n    """"""\n    default_cfg = default_cfgs[\'resnest26d\']\n    model = ResNet(\n        ResNestBottleneck, [2, 2, 2, 2], num_classes=num_classes, in_chans=in_chans,\n        stem_type=\'deep\', stem_width=32, avg_down=True, base_width=64, cardinality=1,\n        block_args=dict(radix=2, avd=True, avd_first=False), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnest50d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """""" ResNeSt-50d model. Matches paper ResNeSt-50 model, https://arxiv.org/abs/2004.08955\n    Since this codebase supports all possible variations, \'d\' for deep stem, stem_width 32, avg in downsample.\n    """"""\n    default_cfg = default_cfgs[\'resnest50d\']\n    model = ResNet(\n        ResNestBottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans,\n        stem_type=\'deep\', stem_width=32, avg_down=True, base_width=64, cardinality=1,\n        block_args=dict(radix=2, avd=True, avd_first=False), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnest101e(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """""" ResNeSt-101e model. Matches paper ResNeSt-101 model, https://arxiv.org/abs/2004.08955\n     Since this codebase supports all possible variations, \'e\' for deep stem, stem_width 64, avg in downsample.\n    """"""\n    default_cfg = default_cfgs[\'resnest101e\']\n    model = ResNet(\n        ResNestBottleneck, [3, 4, 23, 3], num_classes=num_classes, in_chans=in_chans,\n        stem_type=\'deep\', stem_width=64, avg_down=True, base_width=64, cardinality=1,\n        block_args=dict(radix=2, avd=True, avd_first=False), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnest200e(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """""" ResNeSt-200e model. Matches paper ResNeSt-200 model, https://arxiv.org/abs/2004.08955\n    Since this codebase supports all possible variations, \'e\' for deep stem, stem_width 64, avg in downsample.\n    """"""\n    default_cfg = default_cfgs[\'resnest200e\']\n    model = ResNet(\n        ResNestBottleneck, [3, 24, 36, 3], num_classes=num_classes, in_chans=in_chans,\n        stem_type=\'deep\', stem_width=64, avg_down=True, base_width=64, cardinality=1,\n        block_args=dict(radix=2, avd=True, avd_first=False), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnest269e(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """""" ResNeSt-269e model. Matches paper ResNeSt-269 model, https://arxiv.org/abs/2004.08955\n    Since this codebase supports all possible variations, \'e\' for deep stem, stem_width 64, avg in downsample.\n    """"""\n    default_cfg = default_cfgs[\'resnest269e\']\n    model = ResNet(\n        ResNestBottleneck, [3, 30, 48, 8], num_classes=num_classes, in_chans=in_chans,\n        stem_type=\'deep\', stem_width=64, avg_down=True, base_width=64, cardinality=1,\n        block_args=dict(radix=2, avd=True, avd_first=False), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnest50d_4s2x40d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""ResNeSt-50 4s2x40d from https://github.com/zhanghang1989/ResNeSt/blob/master/ablation.md\n    """"""\n    default_cfg = default_cfgs[\'resnest50d_4s2x40d\']\n    model = ResNet(\n        ResNestBottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans,\n        stem_type=\'deep\', stem_width=32, avg_down=True, base_width=40, cardinality=2,\n        block_args=dict(radix=4, avd=True, avd_first=True), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnest50d_1s4x24d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""ResNeSt-50 1s4x24d from https://github.com/zhanghang1989/ResNeSt/blob/master/ablation.md\n    """"""\n    default_cfg = default_cfgs[\'resnest50d_1s4x24d\']\n    model = ResNet(\n        ResNestBottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans,\n        stem_type=\'deep\', stem_width=32, avg_down=True, base_width=24, cardinality=4,\n        block_args=dict(radix=1, avd=True, avd_first=True), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n'"
timm/models/resnet.py,19,"b'""""""PyTorch ResNet\n\nThis started as a copy of https://github.com/pytorch/vision \'resnet.py\' (BSD-3-Clause) with\nadditional dropout and dynamic global avg/max pool.\n\nResNeXt, SE-ResNeXt, SENet, and MXNet Gluon stem/downsample variants, tiered stems added by Ross Wightman\n""""""\nimport math\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained, adapt_model_from_file\nfrom .layers import SelectAdaptivePool2d, DropBlock2d, DropPath, AvgPool2dSame, create_attn, BlurPool2d\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\n__all__ = [\'ResNet\', \'BasicBlock\', \'Bottleneck\']  # model_registry will add each entrypoint fn to this\n\n\ndef _cfg(url=\'\', **kwargs):\n    return {\n        \'url\': url,\n        \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (7, 7),\n        \'crop_pct\': 0.875, \'interpolation\': \'bilinear\',\n        \'mean\': IMAGENET_DEFAULT_MEAN, \'std\': IMAGENET_DEFAULT_STD,\n        \'first_conv\': \'conv1\', \'classifier\': \'fc\',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    \'resnet18\': _cfg(url=\'https://download.pytorch.org/models/resnet18-5c106cde.pth\'),\n    \'resnet34\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34-43635321.pth\'),\n    \'resnet26\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet26-9aa10e23.pth\',\n        interpolation=\'bicubic\'),\n    \'resnet26d\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet26d-69e92c46.pth\',\n        interpolation=\'bicubic\'),\n    \'resnet50\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet50_ram-a26f946b.pth\',\n        interpolation=\'bicubic\'),\n    \'resnet50d\': _cfg(\n        url=\'\',\n        interpolation=\'bicubic\'),\n    \'resnet101\': _cfg(url=\'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\'),\n    \'resnet152\': _cfg(url=\'https://download.pytorch.org/models/resnet152-b121ed2d.pth\'),\n    \'tv_resnet34\': _cfg(url=\'https://download.pytorch.org/models/resnet34-333f7ec4.pth\'),\n    \'tv_resnet50\': _cfg(url=\'https://download.pytorch.org/models/resnet50-19c8e357.pth\'),\n    \'wide_resnet50_2\': _cfg(url=\'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\'),\n    \'wide_resnet101_2\': _cfg(url=\'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth\'),\n    \'resnext50_32x4d\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnext50_32x4d_ra-d733960d.pth\',\n        interpolation=\'bicubic\'),\n    \'resnext50d_32x4d\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnext50d_32x4d-103e99f8.pth\',\n        interpolation=\'bicubic\'),\n    \'resnext101_32x4d\': _cfg(url=\'\'),\n    \'resnext101_32x8d\': _cfg(url=\'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\'),\n    \'resnext101_64x4d\': _cfg(url=\'\'),\n    \'tv_resnext50_32x4d\': _cfg(url=\'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\'),\n    \'ig_resnext101_32x8d\': _cfg(url=\'https://download.pytorch.org/models/ig_resnext101_32x8-c38310e5.pth\'),\n    \'ig_resnext101_32x16d\': _cfg(url=\'https://download.pytorch.org/models/ig_resnext101_32x16-c6f796b0.pth\'),\n    \'ig_resnext101_32x32d\': _cfg(url=\'https://download.pytorch.org/models/ig_resnext101_32x32-e4b90b00.pth\'),\n    \'ig_resnext101_32x48d\': _cfg(url=\'https://download.pytorch.org/models/ig_resnext101_32x48-3e41cc8a.pth\'),\n    \'ssl_resnet18\':  _cfg(\n        url=\'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet18-d92f0530.pth\'),\n    \'ssl_resnet50\':  _cfg(\n        url=\'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnet50-08389792.pth\'),\n    \'ssl_resnext50_32x4d\': _cfg(\n        url=\'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext50_32x4-ddb3e555.pth\'),\n    \'ssl_resnext101_32x4d\': _cfg(\n        url=\'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x4-dc43570a.pth\'),\n    \'ssl_resnext101_32x8d\': _cfg(\n        url=\'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x8-2cfe2f8b.pth\'),\n    \'ssl_resnext101_32x16d\': _cfg(\n        url=\'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_supervised_resnext101_32x16-15fffa57.pth\'),\n    \'swsl_resnet18\': _cfg(\n        url=\'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnet18-118f1556.pth\'),\n    \'swsl_resnet50\': _cfg(\n        url=\'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnet50-16a12f1b.pth\'),\n    \'swsl_resnext50_32x4d\': _cfg(\n        url=\'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext50_32x4-72679e44.pth\'),\n    \'swsl_resnext101_32x4d\': _cfg(\n        url=\'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x4-3f87e46b.pth\'),\n    \'swsl_resnext101_32x8d\': _cfg(\n        url=\'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x8-b4712904.pth\'),\n    \'swsl_resnext101_32x16d\': _cfg(\n        url=\'https://dl.fbaipublicfiles.com/semiweaksupervision/model_files/semi_weakly_supervised_resnext101_32x16-f3559a9c.pth\'),\n    \'seresnext26d_32x4d\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext26d_32x4d-80fa48a3.pth\',\n        interpolation=\'bicubic\'),\n    \'seresnext26t_32x4d\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext26t_32x4d-361bc1c4.pth\',\n        interpolation=\'bicubic\'),\n    \'seresnext26tn_32x4d\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext26tn_32x4d-569cb627.pth\',\n        interpolation=\'bicubic\'),\n    \'ecaresnext26tn_32x4d\': _cfg(\n        url=\'\',\n        interpolation=\'bicubic\'),\n    \'ecaresnet18\': _cfg(),\n    \'ecaresnet50\': _cfg(),\n    \'ecaresnetlight\': _cfg(\n        url=\'https://imvl-automl-sh.oss-cn-shanghai.aliyuncs.com/darts/hyperml/hyperml/job_45402/outputs/ECAResNetLight_4f34b35b.pth\',\n        interpolation=\'bicubic\'),\n    \'ecaresnet50d\': _cfg(\n        url=\'https://imvl-automl-sh.oss-cn-shanghai.aliyuncs.com/darts/hyperml/hyperml/job_45402/outputs/ECAResNet50D_833caf58.pth\',\n        interpolation=\'bicubic\'),\n    \'ecaresnet50d_pruned\': _cfg(\n        url=\'https://imvl-automl-sh.oss-cn-shanghai.aliyuncs.com/darts/hyperml/hyperml/job_45899/outputs/ECAResNet50D_P_9c67f710.pth\',\n        interpolation=\'bicubic\'),\n    \'ecaresnet101d\': _cfg(\n        url=\'https://imvl-automl-sh.oss-cn-shanghai.aliyuncs.com/darts/hyperml/hyperml/job_45402/outputs/ECAResNet101D_281c5844.pth\',\n        interpolation=\'bicubic\'),\n    \'ecaresnet101d_pruned\': _cfg(\n        url=\'https://imvl-automl-sh.oss-cn-shanghai.aliyuncs.com/darts/hyperml/hyperml/job_45610/outputs/ECAResNet101D_P_75a3370e.pth\',\n        interpolation=\'bicubic\'),\n    \'resnetblur18\': _cfg(\n        interpolation=\'bicubic\'),\n    \'resnetblur50\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnetblur50-84f4748f.pth\',\n        interpolation=\'bicubic\')\n}\n\n\ndef get_padding(kernel_size, stride, dilation=1):\n    padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n    return padding\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, cardinality=1, base_width=64,\n                 reduce_first=1, dilation=1, first_dilation=None, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d,\n                 attn_layer=None, aa_layer=None, drop_block=None, drop_path=None):\n        super(BasicBlock, self).__init__()\n\n        assert cardinality == 1, \'BasicBlock only supports cardinality of 1\'\n        assert base_width == 64, \'BasicBlock does not support changing base width\'\n        first_planes = planes // reduce_first\n        outplanes = planes * self.expansion\n        first_dilation = first_dilation or dilation\n        use_aa = aa_layer is not None\n\n        self.conv1 = nn.Conv2d(\n            inplanes, first_planes, kernel_size=3, stride=1 if use_aa else stride, padding=first_dilation,\n            dilation=first_dilation, bias=False)\n        self.bn1 = norm_layer(first_planes)\n        self.act1 = act_layer(inplace=True)\n        self.aa = aa_layer(channels=first_planes) if stride == 2 and use_aa else None\n\n        self.conv2 = nn.Conv2d(\n            first_planes, outplanes, kernel_size=3, padding=dilation, dilation=dilation, bias=False)\n        self.bn2 = norm_layer(outplanes)\n\n        self.se = create_attn(attn_layer, outplanes)\n\n        self.act2 = act_layer(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.drop_block = drop_block\n        self.drop_path = drop_path\n\n    def zero_init_last_bn(self):\n        nn.init.zeros_(self.bn2.weight)\n\n    def forward(self, x):\n        residual = x\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        if self.drop_block is not None:\n            x = self.drop_block(x)\n        x = self.act1(x)\n        if self.aa is not None:\n            x = self.aa(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        if self.drop_block is not None:\n            x = self.drop_block(x)\n\n        if self.se is not None:\n            x = self.se(x)\n\n        if self.drop_path is not None:\n            x = self.drop_path(x)\n\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n        x += residual\n        x = self.act2(x)\n\n        return x\n\n\nclass Bottleneck(nn.Module):\n    __constants__ = [\'se\', \'downsample\']  # for pre 1.4 torchscript compat\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, cardinality=1, base_width=64,\n                 reduce_first=1, dilation=1, first_dilation=None, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d,\n                 attn_layer=None, aa_layer=None, drop_block=None, drop_path=None):\n        super(Bottleneck, self).__init__()\n\n        width = int(math.floor(planes * (base_width / 64)) * cardinality)\n        first_planes = width // reduce_first\n        outplanes = planes * self.expansion\n        first_dilation = first_dilation or dilation\n        use_aa = aa_layer is not None\n\n        self.conv1 = nn.Conv2d(inplanes, first_planes, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(first_planes)\n        self.act1 = act_layer(inplace=True)\n\n        self.conv2 = nn.Conv2d(\n            first_planes, width, kernel_size=3, stride=1 if use_aa else stride,\n            padding=first_dilation, dilation=first_dilation, groups=cardinality, bias=False)\n        self.bn2 = norm_layer(width)\n        self.act2 = act_layer(inplace=True)\n        self.aa = aa_layer(channels=width) if stride == 2 and use_aa else None\n\n        self.conv3 = nn.Conv2d(width, outplanes, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(outplanes)\n\n        self.se = create_attn(attn_layer, outplanes)\n\n        self.act3 = act_layer(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.drop_block = drop_block\n        self.drop_path = drop_path\n\n    def zero_init_last_bn(self):\n        nn.init.zeros_(self.bn3.weight)\n\n    def forward(self, x):\n        residual = x\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        if self.drop_block is not None:\n            x = self.drop_block(x)\n        x = self.act1(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        if self.drop_block is not None:\n            x = self.drop_block(x)\n        x = self.act2(x)\n        if self.aa is not None:\n            x = self.aa(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x)\n        if self.drop_block is not None:\n            x = self.drop_block(x)\n\n        if self.se is not None:\n            x = self.se(x)\n\n        if self.drop_path is not None:\n            x = self.drop_path(x)\n\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n        x += residual\n        x = self.act3(x)\n\n        return x\n\n\ndef downsample_conv(\n        in_channels, out_channels, kernel_size, stride=1, dilation=1, first_dilation=None, norm_layer=None):\n    norm_layer = norm_layer or nn.BatchNorm2d\n    kernel_size = 1 if stride == 1 and dilation == 1 else kernel_size\n    first_dilation = (first_dilation or dilation) if kernel_size > 1 else 1\n    p = get_padding(kernel_size, stride, first_dilation)\n\n    return nn.Sequential(*[\n        nn.Conv2d(\n            in_channels, out_channels, kernel_size, stride=stride, padding=p, dilation=first_dilation, bias=False),\n        norm_layer(out_channels)\n    ])\n\n\ndef downsample_avg(\n        in_channels, out_channels, kernel_size, stride=1, dilation=1, first_dilation=None, norm_layer=None):\n    norm_layer = norm_layer or nn.BatchNorm2d\n    avg_stride = stride if dilation == 1 else 1\n    if stride == 1 and dilation == 1:\n        pool = nn.Identity()\n    else:\n        avg_pool_fn = AvgPool2dSame if avg_stride == 1 and dilation > 1 else nn.AvgPool2d\n        pool = avg_pool_fn(2, avg_stride, ceil_mode=True, count_include_pad=False)\n\n    return nn.Sequential(*[\n        pool,\n        nn.Conv2d(in_channels, out_channels, 1, stride=1, padding=0, bias=False),\n        norm_layer(out_channels)\n    ])\n\n\nclass ResNet(nn.Module):\n    """"""ResNet / ResNeXt / SE-ResNeXt / SE-Net\n\n    This class implements all variants of ResNet, ResNeXt, SE-ResNeXt, and SENet that\n      * have > 1 stride in the 3x3 conv layer of bottleneck\n      * have conv-bn-act ordering\n\n    This ResNet impl supports a number of stem and downsample options based on the v1c, v1d, v1e, and v1s\n    variants included in the MXNet Gluon ResNetV1b model. The C and D variants are also discussed in the\n    \'Bag of Tricks\' paper: https://arxiv.org/pdf/1812.01187. The B variant is equivalent to torchvision default.\n\n    ResNet variants (the same modifications can be used in SE/ResNeXt models as well):\n      * normal, b - 7x7 stem, stem_width = 64, same as torchvision ResNet, NVIDIA ResNet \'v1.5\', Gluon v1b\n      * c - 3 layer deep 3x3 stem, stem_width = 32 (32, 32, 64)\n      * d - 3 layer deep 3x3 stem, stem_width = 32 (32, 32, 64), average pool in downsample\n      * e - 3 layer deep 3x3 stem, stem_width = 64 (64, 64, 128), average pool in downsample\n      * s - 3 layer deep 3x3 stem, stem_width = 64 (64, 64, 128)\n      * t - 3 layer deep 3x3 stem, stem width = 32 (24, 48, 64), average pool in downsample\n      * tn - 3 layer deep 3x3 stem, stem width = 32 (24, 32, 64), average pool in downsample\n\n    ResNeXt\n      * normal - 7x7 stem, stem_width = 64, standard cardinality and base widths\n      * same c,d, e, s variants as ResNet can be enabled\n\n    SE-ResNeXt\n      * normal - 7x7 stem, stem_width = 64\n      * same c, d, e, s variants as ResNet can be enabled\n\n    SENet-154 - 3 layer deep 3x3 stem (same as v1c-v1s), stem_width = 64, cardinality=64,\n        reduction by 2 on width of first bottleneck convolution, 3x3 downsample convs after first block\n\n    Parameters\n    ----------\n    block : Block\n        Class for the residual block. Options are BasicBlockGl, BottleneckGl.\n    layers : list of int\n        Numbers of layers in each block\n    num_classes : int, default 1000\n        Number of classification classes.\n    in_chans : int, default 3\n        Number of input (color) channels.\n    cardinality : int, default 1\n        Number of convolution groups for 3x3 conv in Bottleneck.\n    base_width : int, default 64\n        Factor determining bottleneck channels. `planes * base_width / 64 * cardinality`\n    stem_width : int, default 64\n        Number of channels in stem convolutions\n    stem_type : str, default \'\'\n        The type of stem:\n          * \'\', default - a single 7x7 conv with a width of stem_width\n          * \'deep\' - three 3x3 convolution layers of widths stem_width, stem_width, stem_width * 2\n          * \'deep_tiered\' - three 3x3 conv layers of widths stem_width//4 * 3, stem_width//4 * 6, stem_width * 2\n          * \'deep_tiered_narrow\' - three 3x3 conv layers of widths stem_width//4 * 3, stem_width, stem_width * 2\n    block_reduce_first: int, default 1\n        Reduction factor for first convolution output width of residual blocks,\n        1 for all archs except senets, where 2\n    down_kernel_size: int, default 1\n        Kernel size of residual block downsampling path, 1x1 for most archs, 3x3 for senets\n    avg_down : bool, default False\n        Whether to use average pooling for projection skip connection between stages/downsample.\n    output_stride : int, default 32\n        Set the output stride of the network, 32, 16, or 8. Typically used in segmentation.\n    act_layer : nn.Module, activation layer\n    norm_layer : nn.Module, normalization layer\n    aa_layer : nn.Module, anti-aliasing layer\n    drop_rate : float, default 0.\n        Dropout probability before classifier, for training\n    global_pool : str, default \'avg\'\n        Global pooling type. One of \'avg\', \'max\', \'avgmax\', \'catavgmax\'\n    """"""\n    def __init__(self, block, layers, num_classes=1000, in_chans=3,\n                 cardinality=1, base_width=64, stem_width=64, stem_type=\'\',\n                 block_reduce_first=1, down_kernel_size=1, avg_down=False, output_stride=32,\n                 act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d, aa_layer=None, drop_rate=0.0, drop_path_rate=0.,\n                 drop_block_rate=0., global_pool=\'avg\', zero_init_last_bn=True, block_args=None):\n        block_args = block_args or dict()\n        self.num_classes = num_classes\n        deep_stem = \'deep\' in stem_type\n        self.inplanes = stem_width * 2 if deep_stem else 64\n        self.cardinality = cardinality\n        self.base_width = base_width\n        self.drop_rate = drop_rate\n        self.expansion = block.expansion\n        super(ResNet, self).__init__()\n\n        # Stem\n        if deep_stem:\n            stem_chs_1 = stem_chs_2 = stem_width\n            if \'tiered\' in stem_type:\n                stem_chs_1 = 3 * (stem_width // 4)\n                stem_chs_2 = stem_width if \'narrow\' in stem_type else 6 * (stem_width // 4)\n            self.conv1 = nn.Sequential(*[\n                nn.Conv2d(in_chans, stem_chs_1, 3, stride=2, padding=1, bias=False),\n                norm_layer(stem_chs_1),\n                act_layer(inplace=True),\n                nn.Conv2d(stem_chs_1, stem_chs_2, 3, stride=1, padding=1, bias=False),\n                norm_layer(stem_chs_2),\n                act_layer(inplace=True),\n                nn.Conv2d(stem_chs_2, self.inplanes, 3, stride=1, padding=1, bias=False)])\n        else:\n            self.conv1 = nn.Conv2d(in_chans, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.act1 = act_layer(inplace=True)\n        # Stem Pooling\n        if aa_layer is not None:\n            self.maxpool = nn.Sequential(*[\n                nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n                aa_layer(channels=self.inplanes, stride=2)\n            ])\n        else:\n            self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # Feature Blocks\n        dp = DropPath(drop_path_rate) if drop_path_rate else None\n        db_3 = DropBlock2d(drop_block_rate, 7, 0.25) if drop_block_rate else None\n        db_4 = DropBlock2d(drop_block_rate, 7, 1.00) if drop_block_rate else None\n        channels, strides, dilations = [64, 128, 256, 512], [1, 2, 2, 2], [1] * 4\n        if output_stride == 16:\n            strides[3] = 1\n            dilations[3] = 2\n        elif output_stride == 8:\n            strides[2:4] = [1, 1]\n            dilations[2:4] = [2, 4]\n        else:\n            assert output_stride == 32\n        layer_args = list(zip(channels, layers, strides, dilations))\n        layer_kwargs = dict(\n            reduce_first=block_reduce_first, act_layer=act_layer, norm_layer=norm_layer, aa_layer=aa_layer,\n            avg_down=avg_down, down_kernel_size=down_kernel_size, drop_path=dp, **block_args)\n        self.layer1 = self._make_layer(block, *layer_args[0], **layer_kwargs)\n        self.layer2 = self._make_layer(block, *layer_args[1], **layer_kwargs)\n        self.layer3 = self._make_layer(block, drop_block=db_3, *layer_args[2], **layer_kwargs)\n        self.layer4 = self._make_layer(block, drop_block=db_4, *layer_args[3], **layer_kwargs)\n\n        # Head (Pooling and Classifier)\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.num_features = 512 * block.expansion\n        self.fc = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n\n        for n, m in self.named_modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1.)\n                nn.init.constant_(m.bias, 0.)\n        if zero_init_last_bn:\n            for m in self.modules():\n                if hasattr(m, \'zero_init_last_bn\'):\n                    m.zero_init_last_bn()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, reduce_first=1,\n                    avg_down=False, down_kernel_size=1, **kwargs):\n        downsample = None\n        first_dilation = 1 if dilation in (1, 2) else 2\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample_args = dict(\n                in_channels=self.inplanes, out_channels=planes * block.expansion, kernel_size=down_kernel_size,\n                stride=stride, dilation=dilation, first_dilation=first_dilation, norm_layer=kwargs.get(\'norm_layer\'))\n            downsample = downsample_avg(**downsample_args) if avg_down else downsample_conv(**downsample_args)\n\n        block_kwargs = dict(\n            cardinality=self.cardinality, base_width=self.base_width, reduce_first=reduce_first,\n            dilation=dilation, **kwargs)\n        layers = [block(self.inplanes, planes, stride, downsample, first_dilation=first_dilation, **block_kwargs)]\n        self.inplanes = planes * block.expansion\n        layers += [block(self.inplanes, planes, **block_kwargs) for _ in range(1, blocks)]\n\n        return nn.Sequential(*layers)\n\n    def get_classifier(self):\n        return self.fc\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.num_classes = num_classes\n        del self.fc\n        self.fc = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes) if num_classes else None\n\n    def forward_features(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x).flatten(1)\n        if self.drop_rate:\n            x = F.dropout(x, p=float(self.drop_rate), training=self.training)\n        x = self.fc(x)\n        return x\n\n\n@register_model\ndef resnet18(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    """"""\n    default_cfg = default_cfgs[\'resnet18\']\n    model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnet34(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    """"""\n    default_cfg = default_cfgs[\'resnet34\']\n    model = ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnet26(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-26 model.\n    """"""\n    default_cfg = default_cfgs[\'resnet26\']\n    model = ResNet(Bottleneck, [2, 2, 2, 2], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnet26d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-26 v1d model.\n    This is technically a 28 layer ResNet, sticking with \'d\' modifier from Gluon for now.\n    """"""\n    default_cfg = default_cfgs[\'resnet26d\']\n    model = ResNet(\n        Bottleneck, [2, 2, 2, 2], stem_width=32, stem_type=\'deep\', avg_down=True,\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnet50(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    """"""\n    default_cfg = default_cfgs[\'resnet50\']\n    model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnet50d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-50-D model.\n    """"""\n    default_cfg = default_cfgs[\'resnet50d\']\n    model = ResNet(\n        Bottleneck, [3, 4, 6, 3], stem_width=32, stem_type=\'deep\', avg_down=True,\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnet101(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    """"""\n    default_cfg = default_cfgs[\'resnet101\']\n    model = ResNet(Bottleneck, [3, 4, 23, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnet152(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    """"""\n    default_cfg = default_cfgs[\'resnet152\']\n    model = ResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef tv_resnet34(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-34 model with original Torchvision weights.\n    """"""\n    model = ResNet(BasicBlock, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfgs[\'tv_resnet34\']\n    if pretrained:\n        load_pretrained(model, model.default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef tv_resnet50(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-50 model with original Torchvision weights.\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfgs[\'tv_resnet50\']\n    if pretrained:\n        load_pretrained(model, model.default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef wide_resnet50_2(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a Wide ResNet-50-2 model.\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n    """"""\n    model = ResNet(\n        Bottleneck, [3, 4, 6, 3], base_width=128,\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfgs[\'wide_resnet50_2\']\n    if pretrained:\n        load_pretrained(model, model.default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef wide_resnet101_2(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a Wide ResNet-101-2 model.\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same.\n    """"""\n    model = ResNet(\n        Bottleneck, [3, 4, 23, 3], base_width=128,\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfgs[\'wide_resnet101_2\']\n    if pretrained:\n        load_pretrained(model, model.default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnext50_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNeXt50-32x4d model.\n    """"""\n    default_cfg = default_cfgs[\'resnext50_32x4d\']\n    model = ResNet(\n        Bottleneck, [3, 4, 6, 3], cardinality=32, base_width=4,\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnext50d_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNeXt50d-32x4d model. ResNext50 w/ deep stem & avg pool downsample\n    """"""\n    default_cfg = default_cfgs[\'resnext50d_32x4d\']\n    model = ResNet(\n        Bottleneck, [3, 4, 6, 3], cardinality=32, base_width=4,\n        stem_width=32, stem_type=\'deep\', avg_down=True,\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnext101_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNeXt-101 32x4d model.\n    """"""\n    default_cfg = default_cfgs[\'resnext101_32x4d\']\n    model = ResNet(\n        Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=4,\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnext101_32x8d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNeXt-101 32x8d model.\n    """"""\n    default_cfg = default_cfgs[\'resnext101_32x8d\']\n    model = ResNet(\n        Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=8,\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnext101_64x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNeXt101-64x4d model.\n    """"""\n    default_cfg = default_cfgs[\'resnext101_32x4d\']\n    model = ResNet(\n        Bottleneck, [3, 4, 23, 3], cardinality=64, base_width=4,\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef tv_resnext50_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNeXt50-32x4d model with original Torchvision weights.\n    """"""\n    default_cfg = default_cfgs[\'tv_resnext50_32x4d\']\n    model = ResNet(\n        Bottleneck, [3, 4, 6, 3], cardinality=32, base_width=4,\n        num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef ig_resnext101_32x8d(pretrained=True, **kwargs):\n    """"""Constructs a ResNeXt-101 32x8 model pre-trained on weakly-supervised data\n    and finetuned on ImageNet from Figure 5 in\n    `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_\n    Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=8, **kwargs)\n    model.default_cfg = default_cfgs[\'ig_resnext101_32x8d\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef ig_resnext101_32x16d(pretrained=True, **kwargs):\n    """"""Constructs a ResNeXt-101 32x16 model pre-trained on weakly-supervised data\n    and finetuned on ImageNet from Figure 5 in\n    `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_\n    Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=16, **kwargs)\n    model.default_cfg = default_cfgs[\'ig_resnext101_32x16d\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef ig_resnext101_32x32d(pretrained=True, **kwargs):\n    """"""Constructs a ResNeXt-101 32x32 model pre-trained on weakly-supervised data\n    and finetuned on ImageNet from Figure 5 in\n    `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_\n    Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=32, **kwargs)\n    model.default_cfg = default_cfgs[\'ig_resnext101_32x32d\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef ig_resnext101_32x48d(pretrained=True, **kwargs):\n    """"""Constructs a ResNeXt-101 32x48 model pre-trained on weakly-supervised data\n    and finetuned on ImageNet from Figure 5 in\n    `""Exploring the Limits of Weakly Supervised Pretraining"" <https://arxiv.org/abs/1805.00932>`_\n    Weights from https://pytorch.org/hub/facebookresearch_WSL-Images_resnext/\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=48, **kwargs)\n    model.default_cfg = default_cfgs[\'ig_resnext101_32x48d\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef ssl_resnet18(pretrained=True, **kwargs):\n    """"""Constructs a semi-supervised ResNet-18 model pre-trained on YFCC100M dataset and finetuned on ImageNet\n    `""Billion-scale Semi-Supervised Learning for Image Classification"" <https://arxiv.org/abs/1905.00546>`_\n    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    model.default_cfg = default_cfgs[\'ssl_resnet18\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef ssl_resnet50(pretrained=True, **kwargs):\n    """"""Constructs a semi-supervised ResNet-50 model pre-trained on YFCC100M dataset and finetuned on ImageNet\n    `""Billion-scale Semi-Supervised Learning for Image Classification"" <https://arxiv.org/abs/1905.00546>`_\n    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    model.default_cfg = default_cfgs[\'ssl_resnet50\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef ssl_resnext50_32x4d(pretrained=True, **kwargs):\n    """"""Constructs a semi-supervised ResNeXt-50 32x4 model pre-trained on YFCC100M dataset and finetuned on ImageNet\n    `""Billion-scale Semi-Supervised Learning for Image Classification"" <https://arxiv.org/abs/1905.00546>`_\n    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], cardinality=32, base_width=4, **kwargs)\n    model.default_cfg = default_cfgs[\'ssl_resnext50_32x4d\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef ssl_resnext101_32x4d(pretrained=True, **kwargs):\n    """"""Constructs a semi-supervised ResNeXt-101 32x4 model pre-trained on YFCC100M dataset and finetuned on ImageNet\n    `""Billion-scale Semi-Supervised Learning for Image Classification"" <https://arxiv.org/abs/1905.00546>`_\n    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=4, **kwargs)\n    model.default_cfg = default_cfgs[\'ssl_resnext101_32x4d\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef ssl_resnext101_32x8d(pretrained=True, **kwargs):\n    """"""Constructs a semi-supervised ResNeXt-101 32x8 model pre-trained on YFCC100M dataset and finetuned on ImageNet\n    `""Billion-scale Semi-Supervised Learning for Image Classification"" <https://arxiv.org/abs/1905.00546>`_\n    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=8, **kwargs)\n    model.default_cfg = default_cfgs[\'ssl_resnext101_32x8d\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef ssl_resnext101_32x16d(pretrained=True, **kwargs):\n    """"""Constructs a semi-supervised ResNeXt-101 32x16 model pre-trained on YFCC100M dataset and finetuned on ImageNet\n    `""Billion-scale Semi-Supervised Learning for Image Classification"" <https://arxiv.org/abs/1905.00546>`_\n    Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=16, **kwargs)\n    model.default_cfg = default_cfgs[\'ssl_resnext101_32x16d\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef swsl_resnet18(pretrained=True, **kwargs):\n    """"""Constructs a semi-weakly supervised Resnet-18 model pre-trained on 1B weakly supervised\n       image dataset and finetuned on ImageNet.\n       `""Billion-scale Semi-Supervised Learning for Image Classification"" <https://arxiv.org/abs/1905.00546>`_\n       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    """"""\n    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n    model.default_cfg = default_cfgs[\'swsl_resnet18\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef swsl_resnet50(pretrained=True, **kwargs):\n    """"""Constructs a semi-weakly supervised ResNet-50 model pre-trained on 1B weakly supervised\n       image dataset and finetuned on ImageNet.\n       `""Billion-scale Semi-Supervised Learning for Image Classification"" <https://arxiv.org/abs/1905.00546>`_\n       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    model.default_cfg = default_cfgs[\'swsl_resnet50\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef swsl_resnext50_32x4d(pretrained=True, **kwargs):\n    """"""Constructs a semi-weakly supervised ResNeXt-50 32x4 model pre-trained on 1B weakly supervised\n       image dataset and finetuned on ImageNet.\n       `""Billion-scale Semi-Supervised Learning for Image Classification"" <https://arxiv.org/abs/1905.00546>`_\n       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], cardinality=32, base_width=4, **kwargs)\n    model.default_cfg = default_cfgs[\'swsl_resnext50_32x4d\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef swsl_resnext101_32x4d(pretrained=True, **kwargs):\n    """"""Constructs a semi-weakly supervised ResNeXt-101 32x4 model pre-trained on 1B weakly supervised\n       image dataset and finetuned on ImageNet.\n       `""Billion-scale Semi-Supervised Learning for Image Classification"" <https://arxiv.org/abs/1905.00546>`_\n       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=4, **kwargs)\n    model.default_cfg = default_cfgs[\'swsl_resnext101_32x4d\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef swsl_resnext101_32x8d(pretrained=True, **kwargs):\n    """"""Constructs a semi-weakly supervised ResNeXt-101 32x8 model pre-trained on 1B weakly supervised\n       image dataset and finetuned on ImageNet.\n       `""Billion-scale Semi-Supervised Learning for Image Classification"" <https://arxiv.org/abs/1905.00546>`_\n       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=8, **kwargs)\n    model.default_cfg = default_cfgs[\'swsl_resnext101_32x8d\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef swsl_resnext101_32x16d(pretrained=True, **kwargs):\n    """"""Constructs a semi-weakly supervised ResNeXt-101 32x16 model pre-trained on 1B weakly supervised\n       image dataset and finetuned on ImageNet.\n       `""Billion-scale Semi-Supervised Learning for Image Classification"" <https://arxiv.org/abs/1905.00546>`_\n       Weights from https://github.com/facebookresearch/semi-supervised-ImageNet1K-models/\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], cardinality=32, base_width=16, **kwargs)\n    model.default_cfg = default_cfgs[\'swsl_resnext101_32x16d\']\n    if pretrained:\n        load_pretrained(model, num_classes=kwargs.get(\'num_classes\', 0), in_chans=kwargs.get(\'in_chans\', 3))\n    return model\n\n\n@register_model\ndef seresnext26d_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a SE-ResNeXt-26-D model.\n    This is technically a 28 layer ResNet, using the \'D\' modifier from Gluon / bag-of-tricks for\n    combination of deep stem and avg_pool in downsample.\n    """"""\n    default_cfg = default_cfgs[\'seresnext26d_32x4d\']\n    model = ResNet(\n        Bottleneck, [2, 2, 2, 2], cardinality=32, base_width=4, stem_width=32, stem_type=\'deep\', avg_down=True,\n        num_classes=num_classes, in_chans=in_chans, block_args=dict(attn_layer=\'se\'), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef seresnext26t_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a SE-ResNet-26-T model.\n    This is technically a 28 layer ResNet, like a \'D\' bag-of-tricks model but with tiered 24, 48, 64 channels\n    in the deep stem.\n    """"""\n    default_cfg = default_cfgs[\'seresnext26t_32x4d\']\n    model = ResNet(\n        Bottleneck, [2, 2, 2, 2], cardinality=32, base_width=4,\n        stem_width=32, stem_type=\'deep_tiered\', avg_down=True,\n        num_classes=num_classes, in_chans=in_chans, block_args=dict(attn_layer=\'se\'), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef seresnext26tn_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a SE-ResNeXt-26-TN model.\n    This is technically a 28 layer ResNet, like a \'D\' bag-of-tricks model but with tiered 24, 32, 64 channels\n    in the deep stem. The channel number of the middle stem conv is narrower than the \'T\' variant.\n    """"""\n    default_cfg = default_cfgs[\'seresnext26tn_32x4d\']\n    model = ResNet(\n        Bottleneck, [2, 2, 2, 2], cardinality=32, base_width=4,\n        stem_width=32, stem_type=\'deep_tiered_narrow\', avg_down=True,\n        num_classes=num_classes, in_chans=in_chans, block_args=dict(attn_layer=\'se\'), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef ecaresnext26tn_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs an ECA-ResNeXt-26-TN model.\n    This is technically a 28 layer ResNet, like a \'D\' bag-of-tricks model but with tiered 24, 32, 64 channels\n    in the deep stem. The channel number of the middle stem conv is narrower than the \'T\' variant.\n    this model replaces SE module with the ECA module\n    """"""\n    default_cfg = default_cfgs[\'ecaresnext26tn_32x4d\']\n    block_args = dict(attn_layer=\'eca\')\n    model = ResNet(\n        Bottleneck, [2, 2, 2, 2], cardinality=32, base_width=4,\n        stem_width=32, stem_type=\'deep_tiered_narrow\', avg_down=True,\n        num_classes=num_classes, in_chans=in_chans, block_args=block_args, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef ecaresnet18(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """""" Constructs an ECA-ResNet-18 model.\n    """"""\n    default_cfg = default_cfgs[\'ecaresnet18\']\n    block_args = dict(attn_layer=\'eca\')\n    model = ResNet(\n        BasicBlock, [2, 2, 2, 2], num_classes=num_classes, in_chans=in_chans, block_args=block_args, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef ecaresnet50(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs an ECA-ResNet-50 model.\n    """"""\n    default_cfg = default_cfgs[\'ecaresnet50\']\n    block_args = dict(attn_layer=\'eca\')\n    model = ResNet(\n        Bottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans, block_args=block_args, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef ecaresnet50d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-50-D model with eca.\n    """"""\n    default_cfg = default_cfgs[\'ecaresnet50d\']\n    model = ResNet(\n        Bottleneck, [3, 4, 6, 3], stem_width=32, stem_type=\'deep\', avg_down=True,\n        num_classes=num_classes, in_chans=in_chans, block_args=dict(attn_layer=\'eca\'), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef ecaresnet50d_pruned(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-50-D model pruned with eca.\n        The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf\n    """"""\n    variant = \'ecaresnet50d_pruned\'\n    default_cfg = default_cfgs[variant]\n    model = ResNet(\n        Bottleneck, [3, 4, 6, 3], stem_width=32, stem_type=\'deep\', avg_down=True,\n        num_classes=num_classes, in_chans=in_chans, block_args=dict(attn_layer=\'eca\'), **kwargs)\n    model.default_cfg = default_cfg\n    model = adapt_model_from_file(model, variant)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef ecaresnetlight(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-50-D light model with eca.\n    """"""\n    default_cfg = default_cfgs[\'ecaresnetlight\']\n    model = ResNet(\n        Bottleneck, [1, 1, 11, 3], stem_width=32, avg_down=True,\n        num_classes=num_classes, in_chans=in_chans, block_args=dict(attn_layer=\'eca\'), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef ecaresnet101d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-101-D model with eca.\n    """"""\n    default_cfg = default_cfgs[\'ecaresnet101d\']\n    model = ResNet(\n        Bottleneck, [3, 4, 23, 3], stem_width=32, stem_type=\'deep\', avg_down=True,\n        num_classes=num_classes, in_chans=in_chans, block_args=dict(attn_layer=\'eca\'), **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef ecaresnet101d_pruned(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-101-D model pruned with eca.\n       The pruning has been obtained using https://arxiv.org/pdf/2002.08258.pdf\n    """"""\n    variant = \'ecaresnet101d_pruned\'\n    default_cfg = default_cfgs[variant]\n    model = ResNet(\n        Bottleneck, [3, 4, 23, 3], stem_width=32, stem_type=\'deep\', avg_down=True,\n        num_classes=num_classes, in_chans=in_chans, block_args=dict(attn_layer=\'eca\'), **kwargs)\n    model.default_cfg = default_cfg\n    model = adapt_model_from_file(model, variant)\n\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnetblur18(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-18 model with blur anti-aliasing\n    """"""\n    default_cfg = default_cfgs[\'resnetblur18\']\n    model = ResNet(\n        BasicBlock, [2, 2, 2, 2], num_classes=num_classes, in_chans=in_chans, aa_layer=BlurPool2d, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef resnetblur50(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a ResNet-50 model with blur anti-aliasing\n    """"""\n    default_cfg = default_cfgs[\'resnetblur50\']\n    model = ResNet(\n        Bottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans, aa_layer=BlurPool2d, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n'"
timm/models/selecsls.py,4,"b'""""""PyTorch SelecSLS Net example for ImageNet Classification\nLicense: CC BY 4.0 (https://creativecommons.org/licenses/by/4.0/legalcode)\nAuthor: Dushyant Mehta (@mehtadushy)\n\nSelecSLS (core) Network Architecture as proposed in ""XNect: Real-time Multi-person 3D\nHuman Pose Estimation with a Single RGB Camera, Mehta et al.""\nhttps://arxiv.org/abs/1907.00837\n\nBased on ResNet implementation in https://github.com/rwightman/pytorch-image-models\nand SelecSLS Net implementation in https://github.com/mehtadushy/SelecSLS-Pytorch\n""""""\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SelectAdaptivePool2d\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\n__all__ = [\'SelecSLS\']  # model_registry will add each entrypoint fn to this\n\n\ndef _cfg(url=\'\', **kwargs):\n    return {\n        \'url\': url,\n        \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (4, 4),\n        \'crop_pct\': 0.875, \'interpolation\': \'bilinear\',\n        \'mean\': IMAGENET_DEFAULT_MEAN, \'std\': IMAGENET_DEFAULT_STD,\n        \'first_conv\': \'stem\', \'classifier\': \'fc\',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    \'selecsls42\': _cfg(\n        url=\'\',\n        interpolation=\'bicubic\'),\n    \'selecsls42b\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-selecsls/selecsls42b-8af30141.pth\',\n        interpolation=\'bicubic\'),\n    \'selecsls60\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-selecsls/selecsls60-bbf87526.pth\',\n        interpolation=\'bicubic\'),\n    \'selecsls60b\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-selecsls/selecsls60b-94e619b5.pth\',\n        interpolation=\'bicubic\'),\n    \'selecsls84\': _cfg(\n        url=\'\',\n        interpolation=\'bicubic\'),\n}\n\n\ndef conv_bn(in_chs, out_chs, k=3, stride=1, padding=None, dilation=1):\n    if padding is None:\n        padding = ((stride - 1) + dilation * (k - 1)) // 2\n    return nn.Sequential(\n        nn.Conv2d(in_chs, out_chs, k, stride, padding=padding, dilation=dilation, bias=False),\n        nn.BatchNorm2d(out_chs),\n        nn.ReLU(inplace=True)\n    )\n\n\nclass SelecSLSBlock(nn.Module):\n    def __init__(self, in_chs, skip_chs, mid_chs, out_chs, is_first, stride, dilation=1):\n        super(SelecSLSBlock, self).__init__()\n        self.stride = stride\n        self.is_first = is_first\n        assert stride in [1, 2]\n\n        # Process input with 4 conv blocks with the same number of input and output channels\n        self.conv1 = conv_bn(in_chs, mid_chs, 3, stride, dilation=dilation)\n        self.conv2 = conv_bn(mid_chs, mid_chs, 1)\n        self.conv3 = conv_bn(mid_chs, mid_chs // 2, 3)\n        self.conv4 = conv_bn(mid_chs // 2, mid_chs, 1)\n        self.conv5 = conv_bn(mid_chs, mid_chs // 2, 3)\n        self.conv6 = conv_bn(2 * mid_chs + (0 if is_first else skip_chs), out_chs, 1)\n\n    def forward(self, x):\n        assert isinstance(x, list)\n        assert len(x) in [1, 2]\n\n        d1 = self.conv1(x[0])\n        d2 = self.conv3(self.conv2(d1))\n        d3 = self.conv5(self.conv4(d2))\n        if self.is_first:\n            out = self.conv6(torch.cat([d1, d2, d3], 1))\n            return [out, out]\n        else:\n            return [self.conv6(torch.cat([d1, d2, d3, x[1]], 1)), x[1]]\n\n\nclass SelecSLS(nn.Module):\n    """"""SelecSLS42 / SelecSLS60 / SelecSLS84\n\n    Parameters\n    ----------\n    cfg : network config dictionary specifying block type, feature, and head args\n    num_classes : int, default 1000\n        Number of classification classes.\n    in_chans : int, default 3\n        Number of input (color) channels.\n    drop_rate : float, default 0.\n        Dropout probability before classifier, for training\n    global_pool : str, default \'avg\'\n        Global pooling type. One of \'avg\', \'max\', \'avgmax\', \'catavgmax\'\n    """"""\n\n    def __init__(self, cfg, num_classes=1000, in_chans=3, drop_rate=0.0, global_pool=\'avg\'):\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        super(SelecSLS, self).__init__()\n\n        self.stem = conv_bn(in_chans, 32, stride=2)\n        self.features = nn.Sequential(*[cfg[\'block\'](*block_args) for block_args in cfg[\'features\']])\n        self.head = nn.Sequential(*[conv_bn(*conv_args) for conv_args in cfg[\'head\']])\n        self.num_features = cfg[\'num_features\']\n\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.fc = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n\n        for n, m in self.named_modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1.)\n                nn.init.constant_(m.bias, 0.)\n\n    def get_classifier(self):\n        return self.fc\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.num_classes = num_classes\n        del self.fc\n        if num_classes:\n            self.fc = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n        else:\n            self.fc = None\n\n    def forward_features(self, x):\n        x = self.stem(x)\n        x = self.features([x])\n        x = self.head(x[0])\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x).flatten(1)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        x = self.fc(x)\n        return x\n\n\ndef _create_model(variant, pretrained, model_kwargs):\n    cfg = {}\n    if variant.startswith(\'selecsls42\'):\n        cfg[\'block\'] = SelecSLSBlock\n        # Define configuration of the network after the initial neck\n        cfg[\'features\'] = [\n            # in_chs, skip_chs, mid_chs, out_chs, is_first, stride\n            (32, 0, 64, 64, True, 2),\n            (64, 64, 64, 128, False, 1),\n            (128, 0, 144, 144, True, 2),\n            (144, 144, 144, 288, False, 1),\n            (288, 0, 304, 304, True, 2),\n            (304, 304, 304, 480, False, 1),\n        ]\n        # Head can be replaced with alternative configurations depending on the problem\n        if variant == \'selecsls42b\':\n            cfg[\'head\'] = [\n                (480, 960, 3, 2),\n                (960, 1024, 3, 1),\n                (1024, 1280, 3, 2),\n                (1280, 1024, 1, 1),\n            ]\n            cfg[\'num_features\'] = 1024\n        else:\n            cfg[\'head\'] = [\n                (480, 960, 3, 2),\n                (960, 1024, 3, 1),\n                (1024, 1024, 3, 2),\n                (1024, 1280, 1, 1),\n            ]\n            cfg[\'num_features\'] = 1280\n    elif variant.startswith(\'selecsls60\'):\n        cfg[\'block\'] = SelecSLSBlock\n        # Define configuration of the network after the initial neck\n        cfg[\'features\'] = [\n            # in_chs, skip_chs, mid_chs, out_chs, is_first, stride\n            (32, 0, 64, 64, True, 2),\n            (64, 64, 64, 128, False, 1),\n            (128, 0, 128, 128, True, 2),\n            (128, 128, 128, 128, False, 1),\n            (128, 128, 128, 288, False, 1),\n            (288, 0, 288, 288, True, 2),\n            (288, 288, 288, 288, False, 1),\n            (288, 288, 288, 288, False, 1),\n            (288, 288, 288, 416, False, 1),\n        ]\n        # Head can be replaced with alternative configurations depending on the problem\n        if variant == \'selecsls60b\':\n            cfg[\'head\'] = [\n                (416, 756, 3, 2),\n                (756, 1024, 3, 1),\n                (1024, 1280, 3, 2),\n                (1280, 1024, 1, 1),\n            ]\n            cfg[\'num_features\'] = 1024\n        else:\n            cfg[\'head\'] = [\n                (416, 756, 3, 2),\n                (756, 1024, 3, 1),\n                (1024, 1024, 3, 2),\n                (1024, 1280, 1, 1),\n            ]\n            cfg[\'num_features\'] = 1280\n    elif variant == \'selecsls84\':\n        cfg[\'block\'] = SelecSLSBlock\n        # Define configuration of the network after the initial neck\n        cfg[\'features\'] = [\n            # in_chs, skip_chs, mid_chs, out_chs, is_first, stride\n            (32, 0, 64, 64, True, 2),\n            (64, 64, 64, 144, False, 1),\n            (144, 0, 144, 144, True, 2),\n            (144, 144, 144, 144, False, 1),\n            (144, 144, 144, 144, False, 1),\n            (144, 144, 144, 144, False, 1),\n            (144, 144, 144, 304, False, 1),\n            (304, 0, 304, 304, True, 2),\n            (304, 304, 304, 304, False, 1),\n            (304, 304, 304, 304, False, 1),\n            (304, 304, 304, 304, False, 1),\n            (304, 304, 304, 304, False, 1),\n            (304, 304, 304, 512, False, 1),\n        ]\n        # Head can be replaced with alternative configurations depending on the problem\n        cfg[\'head\'] = [\n            (512, 960, 3, 2),\n            (960, 1024, 3, 1),\n            (1024, 1024, 3, 2),\n            (1024, 1280, 3, 1),\n        ]\n        cfg[\'num_features\'] = 1280\n    else:\n        raise ValueError(\'Invalid net configuration \' + variant + \' !!!\')\n\n    model = SelecSLS(cfg, **model_kwargs)\n    model.default_cfg = default_cfgs[variant]\n    if pretrained:\n        load_pretrained(\n            model,\n            num_classes=model_kwargs.get(\'num_classes\', 0),\n            in_chans=model_kwargs.get(\'in_chans\', 3),\n            strict=True)\n    return model\n\n\n@register_model\ndef selecsls42(pretrained=False, **kwargs):\n    """"""Constructs a SelecSLS42 model.\n    """"""\n    return _create_model(\'selecsls42\', pretrained, kwargs)\n\n\n@register_model\ndef selecsls42b(pretrained=False, **kwargs):\n    """"""Constructs a SelecSLS42_B model.\n    """"""\n    return _create_model(\'selecsls42b\', pretrained, kwargs)\n\n\n@register_model\ndef selecsls60(pretrained=False, **kwargs):\n    """"""Constructs a SelecSLS60 model.\n    """"""\n    return _create_model(\'selecsls60\', pretrained, kwargs)\n\n\n@register_model\ndef selecsls60b(pretrained=False, **kwargs):\n    """"""Constructs a SelecSLS60_B model.\n    """"""\n    return _create_model(\'selecsls60b\', pretrained, kwargs)\n\n\n@register_model\ndef selecsls84(pretrained=False, **kwargs):\n    """"""Constructs a SelecSLS84 model.\n    """"""\n    return _create_model(\'selecsls84\', pretrained, kwargs)\n'"
timm/models/senet.py,2,"b'""""""\nSEResNet implementation from Cadene\'s pretrained models\nhttps://github.com/Cadene/pretrained-models.pytorch/blob/master/pretrainedmodels/models/senet.py\nAdditional credit to https://github.com/creafz\n\nOriginal model: https://github.com/hujie-frank/SENet\n\nResNet code gently borrowed from\nhttps://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n""""""\nfrom collections import OrderedDict\nimport math\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SelectAdaptivePool2d\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\n__all__ = [\'SENet\']\n\n\ndef _cfg(url=\'\', **kwargs):\n    return {\n        \'url\': url, \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (7, 7),\n        \'crop_pct\': 0.875, \'interpolation\': \'bilinear\',\n        \'mean\': IMAGENET_DEFAULT_MEAN, \'std\': IMAGENET_DEFAULT_STD,\n        \'first_conv\': \'layer0.conv1\', \'classifier\': \'last_linear\',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    \'senet154\':\n        _cfg(url=\'http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth\'),\n    \'seresnet18\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet18-4bb0ce65.pth\',\n        interpolation=\'bicubic\'),\n    \'seresnet34\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnet34-a4004e63.pth\'),\n    \'seresnet50\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/se_resnet50-ce0d4300.pth\'),\n    \'seresnet101\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/se_resnet101-7e38fcc6.pth\'),\n    \'seresnet152\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/se_resnet152-d17c99b7.pth\'),\n    \'seresnext26_32x4d\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/seresnext26_32x4d-65ebdb501.pth\',\n        interpolation=\'bicubic\'),\n    \'seresnext50_32x4d\':\n        _cfg(url=\'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth\'),\n    \'seresnext101_32x4d\':\n        _cfg(url=\'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth\'),\n}\n\n\ndef _weight_init(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n    elif isinstance(m, nn.BatchNorm2d):\n        nn.init.constant_(m.weight, 1.)\n        nn.init.constant_(m.bias, 0.)\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(\n            channels, channels // reduction, kernel_size=1, padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(\n            channels // reduction, channels, kernel_size=1, padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    """"""\n    Base class for bottlenecks that implements `forward()` method.\n    """"""\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    """"""\n    Bottleneck for SENet154.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(\n            planes * 2, planes * 4, kernel_size=3, stride=stride,\n            padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(\n            planes * 4, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    """"""\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(\n            inplanes, planes, kernel_size=1, bias=False, stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    """"""\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(\n            inplanes, width, kernel_size=1, bias=False, stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(\n            width, width, kernel_size=3, stride=stride, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1, downsample=None):\n        super(SEResNetBlock, self).__init__()\n        self.conv1 = nn.Conv2d(\n            inplanes, planes, kernel_size=3, padding=1, stride=stride, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(\n            planes, planes, kernel_size=3, padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, drop_rate=0.2,\n                 in_chans=3, inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000, global_pool=\'avg\'):\n        """"""\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        """"""\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        if input_3x3:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(in_chans, 64, 3, stride=2, padding=1, bias=False)),\n                (\'bn1\', nn.BatchNorm2d(64)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n                (\'conv2\', nn.Conv2d(64, 64, 3, stride=1, padding=1, bias=False)),\n                (\'bn2\', nn.BatchNorm2d(64)),\n                (\'relu2\', nn.ReLU(inplace=True)),\n                (\'conv3\', nn.Conv2d(64, inplanes, 3, stride=1, padding=1, bias=False)),\n                (\'bn3\', nn.BatchNorm2d(inplanes)),\n                (\'relu3\', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                (\'conv1\', nn.Conv2d(\n                    in_chans, inplanes, kernel_size=7, stride=2, padding=3, bias=False)),\n                (\'bn1\', nn.BatchNorm2d(inplanes)),\n                (\'relu1\', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append((\'pool\', nn.MaxPool2d(3, stride=2, ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.num_features = 512 * block.expansion\n        self.last_linear = nn.Linear(self.num_features, num_classes)\n\n        for m in self.modules():\n            _weight_init(m)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = [block(\n            self.inplanes, planes, groups, reduction, stride, downsample)]\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def get_classifier(self):\n        return self.last_linear\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.num_classes = num_classes\n        self.avg_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        del self.last_linear\n        if num_classes:\n            self.last_linear = nn.Linear(self.num_features * self.avg_pool.feat_mult(), num_classes)\n        else:\n            self.last_linear = None\n\n    def forward_features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x).flatten(1)\n        if self.drop_rate > 0.:\n            x = F.dropout(x, p=self.drop_rate, training=self.training)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.logits(x)\n        return x\n\n\n@register_model\ndef seresnet18(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'seresnet18\']\n    model = SENet(SEResNetBlock, [2, 2, 2, 2], groups=1, reduction=16,\n                  inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef seresnet34(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'seresnet34\']\n    model = SENet(SEResNetBlock, [3, 4, 6, 3], groups=1, reduction=16,\n                  inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef seresnet50(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'seresnet50\']\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef seresnet101(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'seresnet101\']\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef seresnet152(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'seresnet152\']\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef senet154(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'senet154\']\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef seresnext26_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'seresnext26_32x4d\']\n    model = SENet(SEResNeXtBottleneck, [2, 2, 2, 2], groups=32, reduction=16,\n                  inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef seresnext50_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'seresnext50_32x4d\']\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef seresnext101_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'seresnext101_32x4d\']\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n'"
timm/models/sknet.py,0,"b'"""""" Selective Kernel Networks (ResNet base)\n\nPaper: Selective Kernel Networks (https://arxiv.org/abs/1903.06586)\n\nThis was inspired by reading \'Compounding the Performance Improvements...\' (https://arxiv.org/abs/2001.06268)\nand a streamlined impl at https://github.com/clovaai/assembled-cnn but I ended up building something closer\nto the original paper with some modifications of my own to better balance param count vs accuracy.\n\nHacked together by Ross Wightman\n""""""\nimport math\n\nfrom torch import nn as nn\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SelectiveKernelConv, ConvBnAct, create_attn\nfrom .resnet import ResNet\nfrom timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n\n\ndef _cfg(url=\'\', **kwargs):\n    return {\n        \'url\': url,\n        \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (7, 7),\n        \'crop_pct\': 0.875, \'interpolation\': \'bicubic\',\n        \'mean\': IMAGENET_DEFAULT_MEAN, \'std\': IMAGENET_DEFAULT_STD,\n        \'first_conv\': \'conv1\', \'classifier\': \'fc\',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    \'skresnet18\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/skresnet18_ra-4eec2804.pth\'),\n    \'skresnet34\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/skresnet34_ra-bdc0ccde.pth\'),\n    \'skresnet50\': _cfg(),\n    \'skresnet50d\': _cfg(),\n    \'skresnext50_32x4d\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/skresnext50_ra-f40e40bf.pth\'),\n}\n\n\nclass SelectiveKernelBasic(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, cardinality=1, base_width=64,\n                 sk_kwargs=None, reduce_first=1, dilation=1, first_dilation=None, act_layer=nn.ReLU,\n                 norm_layer=nn.BatchNorm2d, attn_layer=None, aa_layer=None, drop_block=None, drop_path=None):\n        super(SelectiveKernelBasic, self).__init__()\n\n        sk_kwargs = sk_kwargs or {}\n        conv_kwargs = dict(drop_block=drop_block, act_layer=act_layer, norm_layer=norm_layer, aa_layer=aa_layer)\n        assert cardinality == 1, \'BasicBlock only supports cardinality of 1\'\n        assert base_width == 64, \'BasicBlock doest not support changing base width\'\n        first_planes = planes // reduce_first\n        outplanes = planes * self.expansion\n        first_dilation = first_dilation or dilation\n\n        self.conv1 = SelectiveKernelConv(\n            inplanes, first_planes, stride=stride, dilation=first_dilation, **conv_kwargs, **sk_kwargs)\n        conv_kwargs[\'act_layer\'] = None\n        self.conv2 = ConvBnAct(\n            first_planes, outplanes, kernel_size=3, dilation=dilation, **conv_kwargs)\n        self.se = create_attn(attn_layer, outplanes)\n        self.act = act_layer(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.drop_block = drop_block\n        self.drop_path = drop_path\n\n    def zero_init_last_bn(self):\n        nn.init.zeros_(self.conv2.bn.weight)\n\n    def forward(self, x):\n        residual = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        if self.se is not None:\n            x = self.se(x)\n        if self.drop_path is not None:\n            x = self.drop_path(x)\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n        x += residual\n        x = self.act(x)\n        return x\n\n\nclass SelectiveKernelBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 cardinality=1, base_width=64, sk_kwargs=None, reduce_first=1, dilation=1, first_dilation=None,\n                 act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d, attn_layer=None, aa_layer=None,\n                 drop_block=None, drop_path=None):\n        super(SelectiveKernelBottleneck, self).__init__()\n\n        sk_kwargs = sk_kwargs or {}\n        conv_kwargs = dict(drop_block=drop_block, act_layer=act_layer, norm_layer=norm_layer, aa_layer=aa_layer)\n        width = int(math.floor(planes * (base_width / 64)) * cardinality)\n        first_planes = width // reduce_first\n        outplanes = planes * self.expansion\n        first_dilation = first_dilation or dilation\n\n        self.conv1 = ConvBnAct(inplanes, first_planes, kernel_size=1, **conv_kwargs)\n        self.conv2 = SelectiveKernelConv(\n            first_planes, width, stride=stride, dilation=first_dilation, groups=cardinality,\n            **conv_kwargs, **sk_kwargs)\n        conv_kwargs[\'act_layer\'] = None\n        self.conv3 = ConvBnAct(width, outplanes, kernel_size=1, **conv_kwargs)\n        self.se = create_attn(attn_layer, outplanes)\n        self.act = act_layer(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        self.drop_block = drop_block\n        self.drop_path = drop_path\n\n    def zero_init_last_bn(self):\n        nn.init.zeros_(self.conv3.bn.weight)\n\n    def forward(self, x):\n        residual = x\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        if self.se is not None:\n            x = self.se(x)\n        if self.drop_path is not None:\n            x = self.drop_path(x)\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n        x += residual\n        x = self.act(x)\n        return x\n\n\n@register_model\ndef skresnet18(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a Selective Kernel ResNet-18 model.\n\n    Different from configs in Select Kernel paper or ""Compounding the Performance Improvements..."" this\n    variation splits the input channels to the selective convolutions to keep param count down.\n    """"""\n    default_cfg = default_cfgs[\'skresnet18\']\n    sk_kwargs = dict(\n        min_attn_channels=16,\n        attn_reduction=8,\n        split_input=True\n    )\n    model = ResNet(\n        SelectiveKernelBasic, [2, 2, 2, 2], num_classes=num_classes, in_chans=in_chans,\n        block_args=dict(sk_kwargs=sk_kwargs), zero_init_last_bn=False, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef skresnet34(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a Selective Kernel ResNet-34 model.\n\n    Different from configs in Select Kernel paper or ""Compounding the Performance Improvements..."" this\n    variation splits the input channels to the selective convolutions to keep param count down.\n    """"""\n    default_cfg = default_cfgs[\'skresnet34\']\n    sk_kwargs = dict(\n        min_attn_channels=16,\n        attn_reduction=8,\n        split_input=True\n    )\n    model = ResNet(\n        SelectiveKernelBasic, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans,\n        block_args=dict(sk_kwargs=sk_kwargs), zero_init_last_bn=False, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef skresnet50(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a Select Kernel ResNet-50 model.\n\n    Different from configs in Select Kernel paper or ""Compounding the Performance Improvements..."" this\n    variation splits the input channels to the selective convolutions to keep param count down.\n    """"""\n    sk_kwargs = dict(\n        split_input=True,\n    )\n    default_cfg = default_cfgs[\'skresnet50\']\n    model = ResNet(\n        SelectiveKernelBottleneck, [3, 4, 6, 3], num_classes=num_classes, in_chans=in_chans,\n        block_args=dict(sk_kwargs=sk_kwargs), zero_init_last_bn=False, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef skresnet50d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a Select Kernel ResNet-50-D model.\n\n    Different from configs in Select Kernel paper or ""Compounding the Performance Improvements..."" this\n    variation splits the input channels to the selective convolutions to keep param count down.\n    """"""\n    sk_kwargs = dict(\n        split_input=True,\n    )\n    default_cfg = default_cfgs[\'skresnet50d\']\n    model = ResNet(\n        SelectiveKernelBottleneck, [3, 4, 6, 3], stem_width=32, stem_type=\'deep\', avg_down=True,\n        num_classes=num_classes, in_chans=in_chans, block_args=dict(sk_kwargs=sk_kwargs),\n        zero_init_last_bn=False, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef skresnext50_32x4d(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    """"""Constructs a Select Kernel ResNeXt50-32x4d model. This should be equivalent to\n    the SKNet-50 model in the Select Kernel Paper\n    """"""\n    default_cfg = default_cfgs[\'skresnext50_32x4d\']\n    model = ResNet(\n        SelectiveKernelBottleneck, [3, 4, 6, 3], cardinality=32, base_width=4,\n        num_classes=num_classes, in_chans=in_chans, zero_init_last_bn=False, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n'"
timm/models/tresnet.py,4,"b'""""""\nTResNet: High Performance GPU-Dedicated Architecture\nhttps://arxiv.org/pdf/2003.13630.pdf\n\nOriginal model: https://github.com/mrT23/TResNet\n\n""""""\nfrom functools import partial\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nfrom .layers import SpaceToDepthModule, AntiAliasDownsampleLayer, SelectAdaptivePool2d\nfrom .registry import register_model\nfrom .helpers import load_pretrained\n\ntry:\n    from inplace_abn import InPlaceABN\n    has_iabn = True\nexcept ImportError:\n    has_iabn = False\n\n__all__ = [\'tresnet_m\', \'tresnet_l\', \'tresnet_xl\']\n\n\ndef _cfg(url=\'\', **kwargs):\n    return {\n        \'url\': url, \'num_classes\': 1000, \'input_size\': (3, 224, 224), \'pool_size\': (7, 7),\n        \'crop_pct\': 0.875, \'interpolation\': \'bilinear\',\n        \'mean\': (0, 0, 0), \'std\': (1, 1, 1),\n        \'first_conv\': \'body.conv1\', \'classifier\': \'head.fc\',\n        **kwargs\n    }\n\n\ndefault_cfgs = {\n    \'tresnet_m\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_m_80_8-dbc13962.pth\'),\n    \'tresnet_l\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_81_5-235b486c.pth\'),\n    \'tresnet_xl\': _cfg(\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_xl_82_0-a2d51b00.pth\'),\n    \'tresnet_m_448\': _cfg(\n        input_size=(3, 448, 448), pool_size=(14, 14),\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_m_448-bc359d10.pth\'),\n    \'tresnet_l_448\': _cfg(\n        input_size=(3, 448, 448), pool_size=(14, 14),\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_l_448-940d0cd1.pth\'),\n    \'tresnet_xl_448\': _cfg(\n        input_size=(3, 448, 448), pool_size=(14, 14),\n        url=\'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-tresnet/tresnet_xl_448-8c1815de.pth\')\n}\n\n\nclass FastGlobalAvgPool2d(nn.Module):\n    def __init__(self, flatten=False):\n        super(FastGlobalAvgPool2d, self).__init__()\n        self.flatten = flatten\n\n    def forward(self, x):\n        if self.flatten:\n            in_size = x.size()\n            return x.view((in_size[0], in_size[1], -1)).mean(dim=2)\n        else:\n            return x.view(x.size(0), x.size(1), -1).mean(-1).view(x.size(0), x.size(1), 1, 1)\n\n    def feat_mult(self):\n        return 1\n\n\nclass FastSEModule(nn.Module):\n\n    def __init__(self, channels, reduction_channels, inplace=True):\n        super(FastSEModule, self).__init__()\n        self.avg_pool = FastGlobalAvgPool2d()\n        self.fc1 = nn.Conv2d(channels, reduction_channels, kernel_size=1, padding=0, bias=True)\n        self.relu = nn.ReLU(inplace=inplace)\n        self.fc2 = nn.Conv2d(reduction_channels, channels, kernel_size=1, padding=0, bias=True)\n        self.activation = nn.Sigmoid()\n\n    def forward(self, x):\n        x_se = self.avg_pool(x)\n        x_se2 = self.fc1(x_se)\n        x_se2 = self.relu(x_se2)\n        x_se = self.fc2(x_se2)\n        x_se = self.activation(x_se)\n        return x * x_se\n\n\ndef IABN2Float(module: nn.Module) -> nn.Module:\n    ""If `module` is IABN don\'t use half precision.""\n    if isinstance(module, InPlaceABN):\n        module.float()\n    for child in module.children():\n        IABN2Float(child)\n    return module\n\n\ndef conv2d_ABN(ni, nf, stride, activation=""leaky_relu"", kernel_size=3, activation_param=1e-2, groups=1):\n    return nn.Sequential(\n        nn.Conv2d(\n            ni, nf, kernel_size=kernel_size, stride=stride, padding=kernel_size // 2, groups=groups, bias=False),\n        InPlaceABN(num_features=nf, activation=activation, activation_param=activation_param)\n    )\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True, anti_alias_layer=None):\n        super(BasicBlock, self).__init__()\n        if stride == 1:\n            self.conv1 = conv2d_ABN(inplanes, planes, stride=1, activation_param=1e-3)\n        else:\n            if anti_alias_layer is None:\n                self.conv1 = conv2d_ABN(inplanes, planes, stride=2, activation_param=1e-3)\n            else:\n                self.conv1 = nn.Sequential(\n                    conv2d_ABN(inplanes, planes, stride=1, activation_param=1e-3),\n                    anti_alias_layer(channels=planes, filt_size=3, stride=2))\n\n        self.conv2 = conv2d_ABN(planes, planes, stride=1, activation=""identity"")\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        reduce_layer_planes = max(planes * self.expansion // 4, 64)\n        self.se = FastSEModule(planes * self.expansion, reduce_layer_planes) if use_se else None\n\n    def forward(self, x):\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        else:\n            residual = x\n\n        out = self.conv1(x)\n        out = self.conv2(out)\n\n        if self.se is not None:\n            out = self.se(out)\n\n        out += residual\n        out = self.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True, anti_alias_layer=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = conv2d_ABN(\n            inplanes, planes, kernel_size=1, stride=1, activation=""leaky_relu"", activation_param=1e-3)\n        if stride == 1:\n            self.conv2 = conv2d_ABN(\n                planes, planes, kernel_size=3, stride=1, activation=""leaky_relu"", activation_param=1e-3)\n        else:\n            if anti_alias_layer is None:\n                self.conv2 = conv2d_ABN(\n                    planes, planes, kernel_size=3, stride=2, activation=""leaky_relu"", activation_param=1e-3)\n            else:\n                self.conv2 = nn.Sequential(\n                    conv2d_ABN(planes, planes, kernel_size=3, stride=1, activation=""leaky_relu"", activation_param=1e-3),\n                    anti_alias_layer(channels=planes, filt_size=3, stride=2))\n\n        self.conv3 = conv2d_ABN(\n            planes, planes * self.expansion, kernel_size=1, stride=1, activation=""identity"")\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n        reduce_layer_planes = max(planes * self.expansion // 8, 64)\n        self.se = FastSEModule(planes, reduce_layer_planes) if use_se else None\n\n    def forward(self, x):\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        else:\n            residual = x\n\n        out = self.conv1(x)\n        out = self.conv2(out)\n        if self.se is not None:\n            out = self.se(out)\n\n        out = self.conv3(out)\n        out = out + residual  # no inplace\n        out = self.relu(out)\n\n        return out\n\n\nclass TResNet(nn.Module):\n    def __init__(self, layers, in_chans=3, num_classes=1000, width_factor=1.0, no_aa_jit=False,\n                 global_pool=\'avg\', drop_rate=0.):\n        if not has_iabn:\n            raise ImportError(\n                ""For TResNet models, please install InplaceABN: ""\n                ""\'pip install git+https://github.com/mapillary/inplace_abn.git@v1.0.11\'"")\n        self.num_classes = num_classes\n        self.drop_rate = drop_rate\n        super(TResNet, self).__init__()\n\n        # JIT layers\n        space_to_depth = SpaceToDepthModule()\n        anti_alias_layer = partial(AntiAliasDownsampleLayer, no_jit=no_aa_jit)\n\n        # TResnet stages\n        self.inplanes = int(64 * width_factor)\n        self.planes = int(64 * width_factor)\n        conv1 = conv2d_ABN(in_chans * 16, self.planes, stride=1, kernel_size=3)\n        layer1 = self._make_layer(\n            BasicBlock, self.planes, layers[0], stride=1, use_se=True, anti_alias_layer=anti_alias_layer)  # 56x56\n        layer2 = self._make_layer(\n            BasicBlock, self.planes * 2, layers[1], stride=2, use_se=True, anti_alias_layer=anti_alias_layer)  # 28x28\n        layer3 = self._make_layer(\n            Bottleneck, self.planes * 4, layers[2], stride=2, use_se=True, anti_alias_layer=anti_alias_layer)  # 14x14\n        layer4 = self._make_layer(\n            Bottleneck, self.planes * 8, layers[3], stride=2, use_se=False, anti_alias_layer=anti_alias_layer)  # 7x7\n\n        # body\n        self.body = nn.Sequential(OrderedDict([\n            (\'SpaceToDepth\', space_to_depth),\n            (\'conv1\', conv1),\n            (\'layer1\', layer1),\n            (\'layer2\', layer2),\n            (\'layer3\', layer3),\n            (\'layer4\', layer4)]))\n\n        # head\n        self.num_features = (self.planes * 8) * Bottleneck.expansion\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)\n        self.head = nn.Sequential(OrderedDict([\n            (\'fc\', nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes))]))\n\n        # model initilization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'leaky_relu\')\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, InPlaceABN):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # residual connections special initialization\n        for m in self.modules():\n            if isinstance(m, BasicBlock):\n                m.conv2[1].weight = nn.Parameter(torch.zeros_like(m.conv2[1].weight))  # BN to zero\n            if isinstance(m, Bottleneck):\n                m.conv3[1].weight = nn.Parameter(torch.zeros_like(m.conv3[1].weight))  # BN to zero\n            if isinstance(m, nn.Linear): m.weight.data.normal_(0, 0.01)\n\n    def _make_layer(self, block, planes, blocks, stride=1, use_se=True, anti_alias_layer=None):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            layers = []\n            if stride == 2:\n                # avg pooling before 1x1 conv\n                layers.append(nn.AvgPool2d(kernel_size=2, stride=2, ceil_mode=True, count_include_pad=False))\n            layers += [conv2d_ABN(\n                self.inplanes, planes * block.expansion, kernel_size=1, stride=1, activation=""identity"")]\n            downsample = nn.Sequential(*layers)\n\n        layers = []\n        layers.append(block(\n            self.inplanes, planes, stride, downsample, use_se=use_se, anti_alias_layer=anti_alias_layer))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(\n                block(self.inplanes, planes, use_se=use_se, anti_alias_layer=anti_alias_layer))\n        return nn.Sequential(*layers)\n\n    def get_classifier(self):\n        return self.head.fc\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool, flatten=True)\n        self.num_classes = num_classes\n        self.head = None\n        if num_classes:\n            self.head = nn.Sequential(OrderedDict([\n                (\'fc\', nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes))]))\n\n    def forward_features(self, x):\n        return self.body(x)\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x)\n        if self.drop_rate:\n            x = F.dropout(x, p=float(self.drop_rate), training=self.training)\n        x = self.head(x)\n        return x\n\n\n@register_model\ndef tresnet_m(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'tresnet_m\']\n    model = TResNet(layers=[3, 4, 11, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef tresnet_l(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'tresnet_l\']\n    model = TResNet(\n        layers=[4, 5, 18, 3], num_classes=num_classes, in_chans=in_chans, width_factor=1.2, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef tresnet_xl(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'tresnet_xl\']\n    model = TResNet(\n        layers=[4, 5, 24, 3], num_classes=num_classes, in_chans=in_chans, width_factor=1.3, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef tresnet_m_448(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'tresnet_m_448\']\n    model = TResNet(layers=[3, 4, 11, 3], num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef tresnet_l_448(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'tresnet_l_448\']\n    model = TResNet(\n        layers=[4, 5, 18, 3], num_classes=num_classes, in_chans=in_chans, width_factor=1.2, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n\n\n@register_model\ndef tresnet_xl_448(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'tresnet_xl_448\']\n    model = TResNet(\n        layers=[4, 5, 24, 3], num_classes=num_classes, in_chans=in_chans, width_factor=1.3, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n    return model\n'"
timm/models/xception.py,2,"b'""""""\nPorted to pytorch thanks to [tstandley](https://github.com/tstandley/Xception-PyTorch)\n\n@author: tstandley\nAdapted by cadene\n\nCreates an Xception Model as defined in:\n\nFrancois Chollet\nXception: Deep Learning with Depthwise Separable Convolutions\nhttps://arxiv.org/pdf/1610.02357.pdf\n\nThis weights ported from the Keras implementation. Achieves the following performance on the validation set:\n\nLoss:0.9173 Prec@1:78.892 Prec@5:94.292\n\nREMEMBER to set your image size to 3x299x299 for both test and validation\n\nnormalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                  std=[0.5, 0.5, 0.5])\n\nThe resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n""""""\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .registry import register_model\nfrom .helpers import load_pretrained\nfrom .layers import SelectAdaptivePool2d\n\n__all__ = [\'Xception\']\n\ndefault_cfgs = {\n    \'xception\': {\n        \'url\': \'https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-cadene/xception-43020ad28.pth\',\n        \'input_size\': (3, 299, 299),\n        \'pool_size\': (10, 10),\n        \'crop_pct\': 0.8975,\n        \'interpolation\': \'bicubic\',\n        \'mean\': (0.5, 0.5, 0.5),\n        \'std\': (0.5, 0.5, 0.5),\n        \'num_classes\': 1000,\n        \'first_conv\': \'conv1\',\n        \'classifier\': \'fc\'\n        # The resize parameter of the validation transform should be 333, and make sure to center crop at 299x299\n    }\n}\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False):\n        super(SeparableConv2d, self).__init__()\n\n        self.conv1 = nn.Conv2d(\n            in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels, 1, 1, 0, 1, 1, bias=bias)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.pointwise(x)\n        return x\n\n\nclass Block(nn.Module):\n    def __init__(self, in_filters, out_filters, reps, strides=1, start_with_relu=True, grow_first=True):\n        super(Block, self).__init__()\n\n        if out_filters != in_filters or strides != 1:\n            self.skip = nn.Conv2d(in_filters, out_filters, 1, stride=strides, bias=False)\n            self.skipbn = nn.BatchNorm2d(out_filters)\n        else:\n            self.skip = None\n\n        self.relu = nn.ReLU(inplace=True)\n        rep = []\n\n        filters = in_filters\n        if grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(in_filters, out_filters, 3, stride=1, padding=1, bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n            filters = out_filters\n\n        for i in range(reps - 1):\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(filters, filters, 3, stride=1, padding=1, bias=False))\n            rep.append(nn.BatchNorm2d(filters))\n\n        if not grow_first:\n            rep.append(self.relu)\n            rep.append(SeparableConv2d(in_filters, out_filters, 3, stride=1, padding=1, bias=False))\n            rep.append(nn.BatchNorm2d(out_filters))\n\n        if not start_with_relu:\n            rep = rep[1:]\n        else:\n            rep[0] = nn.ReLU(inplace=False)\n\n        if strides != 1:\n            rep.append(nn.MaxPool2d(3, strides, 1))\n        self.rep = nn.Sequential(*rep)\n\n    def forward(self, inp):\n        x = self.rep(inp)\n\n        if self.skip is not None:\n            skip = self.skip(inp)\n            skip = self.skipbn(skip)\n        else:\n            skip = inp\n\n        x += skip\n        return x\n\n\nclass Xception(nn.Module):\n    """"""\n    Xception optimized for the ImageNet dataset, as specified in\n    https://arxiv.org/pdf/1610.02357.pdf\n    """"""\n\n    def __init__(self, num_classes=1000, in_chans=3, drop_rate=0., global_pool=\'avg\'):\n        """""" Constructor\n        Args:\n            num_classes: number of classes\n        """"""\n        super(Xception, self).__init__()\n        self.drop_rate = drop_rate\n        self.global_pool = global_pool\n        self.num_classes = num_classes\n        self.num_features = 2048\n\n        self.conv1 = nn.Conv2d(in_chans, 32, 3, 2, 0, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = nn.Conv2d(32, 64, 3, bias=False)\n        self.bn2 = nn.BatchNorm2d(64)\n        # do relu here\n\n        self.block1 = Block(64, 128, 2, 2, start_with_relu=False, grow_first=True)\n        self.block2 = Block(128, 256, 2, 2, start_with_relu=True, grow_first=True)\n        self.block3 = Block(256, 728, 2, 2, start_with_relu=True, grow_first=True)\n\n        self.block4 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n        self.block5 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n        self.block6 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n        self.block7 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n\n        self.block8 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n        self.block9 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n        self.block10 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n        self.block11 = Block(728, 728, 3, 1, start_with_relu=True, grow_first=True)\n\n        self.block12 = Block(728, 1024, 2, 2, start_with_relu=True, grow_first=False)\n\n        self.conv3 = SeparableConv2d(1024, 1536, 3, 1, 1)\n        self.bn3 = nn.BatchNorm2d(1536)\n\n        # do relu here\n        self.conv4 = SeparableConv2d(1536, self.num_features, 3, 1, 1)\n        self.bn4 = nn.BatchNorm2d(self.num_features)\n\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        self.fc = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes)\n\n        # #------- init weights --------\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def get_classifier(self):\n        return self.fc\n\n    def reset_classifier(self, num_classes, global_pool=\'avg\'):\n        self.num_classes = num_classes\n        self.global_pool = SelectAdaptivePool2d(pool_type=global_pool)\n        del self.fc\n        self.fc = nn.Linear(self.num_features * self.global_pool.feat_mult(), num_classes) if num_classes else None\n\n    def forward_features(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = self.block5(x)\n        x = self.block6(x)\n        x = self.block7(x)\n        x = self.block8(x)\n        x = self.block9(x)\n        x = self.block10(x)\n        x = self.block11(x)\n        x = self.block12(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.relu(x)\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = self.global_pool(x).flatten(1)\n        if self.drop_rate:\n            F.dropout(x, self.drop_rate, training=self.training)\n        x = self.fc(x)\n        return x\n\n\n@register_model\ndef xception(pretrained=False, num_classes=1000, in_chans=3, **kwargs):\n    default_cfg = default_cfgs[\'xception\']\n    model = Xception(num_classes=num_classes, in_chans=in_chans, **kwargs)\n    model.default_cfg = default_cfg\n    if pretrained:\n        load_pretrained(model, default_cfg, num_classes, in_chans)\n\n    return model\n'"
timm/optim/__init__.py,0,b'from .nadam import Nadam\nfrom .rmsprop_tf import RMSpropTF\nfrom .adamw import AdamW\nfrom .radam import RAdam\nfrom .novograd import NovoGrad\nfrom .nvnovograd import NvNovoGrad\nfrom .lookahead import Lookahead\nfrom .optim_factory import create_optimizer\n'
timm/optim/adamw.py,5,"b'"""""" AdamW Optimizer\nImpl copied from PyTorch master\n""""""\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer\n\n\nclass AdamW(Optimizer):\n    r""""""Implements AdamW algorithm.\n\n    The original Adam algorithm was proposed in `Adam: A Method for Stochastic Optimization`_.\n    The AdamW variant was proposed in `Decoupled Weight Decay Regularization`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay coefficient (default: 1e-2)\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False)\n\n    .. _Adam\\: A Method for Stochastic Optimization:\n        https://arxiv.org/abs/1412.6980\n    .. _Decoupled Weight Decay Regularization:\n        https://arxiv.org/abs/1711.05101\n    .. _On the Convergence of Adam and Beyond:\n        https://openreview.net/forum?id=ryQu7f-RZ\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=1e-2, amsgrad=False):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, amsgrad=amsgrad)\n        super(AdamW, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(AdamW, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'amsgrad\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n\n                # Perform stepweight decay\n                p.data.mul_(1 - group[\'lr\'] * group[\'weight_decay\'])\n\n                # Perform optimization step\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n                amsgrad = group[\'amsgrad\']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\'max_exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsgrad:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = (max_exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group[\'eps\'])\n                else:\n                    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group[\'eps\'])\n\n                step_size = group[\'lr\'] / bias_correction1\n\n                p.data.addcdiv_(-step_size, exp_avg, denom)\n\n        return loss\n'"
timm/optim/lookahead.py,3,"b'"""""" Lookahead Optimizer Wrapper.\nImplementation modified from: https://github.com/alphadl/lookahead.pytorch\nPaper: `Lookahead Optimizer: k steps forward, 1 step back` - https://arxiv.org/abs/1907.08610\n""""""\nimport torch\nfrom torch.optim.optimizer import Optimizer\nfrom collections import defaultdict\n\n\nclass Lookahead(Optimizer):\n    def __init__(self, base_optimizer, alpha=0.5, k=6):\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f\'Invalid slow update rate: {alpha}\')\n        if not 1 <= k:\n            raise ValueError(f\'Invalid lookahead steps: {k}\')\n        defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n        self.base_optimizer = base_optimizer\n        self.param_groups = self.base_optimizer.param_groups\n        self.defaults = base_optimizer.defaults\n        self.defaults.update(defaults)\n        self.state = defaultdict(dict)\n        # manually add our defaults to the param groups\n        for name, default in defaults.items():\n            for group in self.param_groups:\n                group.setdefault(name, default)\n\n    def update_slow(self, group):\n        for fast_p in group[""params""]:\n            if fast_p.grad is None:\n                continue\n            param_state = self.state[fast_p]\n            if \'slow_buffer\' not in param_state:\n                param_state[\'slow_buffer\'] = torch.empty_like(fast_p.data)\n                param_state[\'slow_buffer\'].copy_(fast_p.data)\n            slow = param_state[\'slow_buffer\']\n            slow.add_(group[\'lookahead_alpha\'], fast_p.data - slow)\n            fast_p.data.copy_(slow)\n\n    def sync_lookahead(self):\n        for group in self.param_groups:\n            self.update_slow(group)\n\n    def step(self, closure=None):\n        #assert id(self.param_groups) == id(self.base_optimizer.param_groups)\n        loss = self.base_optimizer.step(closure)\n        for group in self.param_groups:\n            group[\'lookahead_step\'] += 1\n            if group[\'lookahead_step\'] % group[\'lookahead_k\'] == 0:\n                self.update_slow(group)\n        return loss\n\n    def state_dict(self):\n        fast_state_dict = self.base_optimizer.state_dict()\n        slow_state = {\n            (id(k) if isinstance(k, torch.Tensor) else k): v\n            for k, v in self.state.items()\n        }\n        fast_state = fast_state_dict[\'state\']\n        param_groups = fast_state_dict[\'param_groups\']\n        return {\n            \'state\': fast_state,\n            \'slow_state\': slow_state,\n            \'param_groups\': param_groups,\n        }\n\n    def load_state_dict(self, state_dict):\n        fast_state_dict = {\n            \'state\': state_dict[\'state\'],\n            \'param_groups\': state_dict[\'param_groups\'],\n        }\n        self.base_optimizer.load_state_dict(fast_state_dict)\n\n        # We want to restore the slow state, but share param_groups reference\n        # with base_optimizer. This is a bit redundant but least code\n        slow_state_new = False\n        if \'slow_state\' not in state_dict:\n            print(\'Loading state_dict from optimizer without Lookahead applied.\')\n            state_dict[\'slow_state\'] = defaultdict(dict)\n            slow_state_new = True\n        slow_state_dict = {\n            \'state\': state_dict[\'slow_state\'],\n            \'param_groups\': state_dict[\'param_groups\'],  # this is pointless but saves code\n        }\n        super(Lookahead, self).load_state_dict(slow_state_dict)\n        self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n        if slow_state_new:\n            # reapply defaults to catch missing lookahead specific ones\n            for name, default in self.defaults.items():\n                for group in self.param_groups:\n                    group.setdefault(name, default)\n'"
timm/optim/nadam.py,1,"b'import torch\nfrom torch.optim import Optimizer\n\n\nclass Nadam(Optimizer):\n    """"""Implements Nadam algorithm (a variant of Adam based on Nesterov momentum).\n\n    It has been proposed in `Incorporating Nesterov Momentum into Adam`__.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 2e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        schedule_decay (float, optional): momentum schedule decay (default: 4e-3)\n\n    __ http://cs229.stanford.edu/proj2015/054_report.pdf\n    __ http://www.cs.toronto.edu/~fritz/absps/momentum.pdf\n\n        Originally taken from: https://github.com/pytorch/pytorch/pull/1408\n        NOTE: Has potential issues but does work well on some problems.\n    """"""\n\n    def __init__(self, params, lr=2e-3, betas=(0.9, 0.999), eps=1e-8,\n                 weight_decay=0, schedule_decay=4e-3):\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, schedule_decay=schedule_decay)\n        super(Nadam, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    state[\'m_schedule\'] = 1.\n                    state[\'exp_avg\'] = grad.new().resize_as_(grad).zero_()\n                    state[\'exp_avg_sq\'] = grad.new().resize_as_(grad).zero_()\n\n                # Warming momentum schedule\n                m_schedule = state[\'m_schedule\']\n                schedule_decay = group[\'schedule_decay\']\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n                eps = group[\'eps\']\n                state[\'step\'] += 1\n                t = state[\'step\']\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                momentum_cache_t = beta1 * \\\n                    (1. - 0.5 * (0.96 ** (t * schedule_decay)))\n                momentum_cache_t_1 = beta1 * \\\n                    (1. - 0.5 * (0.96 ** ((t + 1) * schedule_decay)))\n                m_schedule_new = m_schedule * momentum_cache_t\n                m_schedule_next = m_schedule * momentum_cache_t * momentum_cache_t_1\n                state[\'m_schedule\'] = m_schedule_new\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1. - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1. - beta2, grad, grad)\n                exp_avg_sq_prime = exp_avg_sq / (1. - beta2 ** t)\n                denom = exp_avg_sq_prime.sqrt_().add_(eps)\n\n                p.data.addcdiv_(-group[\'lr\'] * (1. - momentum_cache_t) / (1. - m_schedule_new), grad, denom)\n                p.data.addcdiv_(-group[\'lr\'] * momentum_cache_t_1 / (1. - m_schedule_next), exp_avg, denom)\n\n        return loss\n'"
timm/optim/novograd.py,7,"b'""""""NovoGrad Optimizer.\nOriginal impl by Masashi Kimura (Convergence Lab): https://github.com/convergence-lab/novograd\nPaper: `Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks`\n    - https://arxiv.org/abs/1905.11286\n""""""\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\nimport math\n\n\nclass NovoGrad(Optimizer):\n    def __init__(self, params, grad_averaging=False, lr=0.1, betas=(0.95, 0.98), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        super(NovoGrad, self).__init__(params, defaults)\n        self._lr = lr\n        self._beta1 = betas[0]\n        self._beta2 = betas[1]\n        self._eps = eps\n        self._wd = weight_decay\n        self._grad_averaging = grad_averaging\n\n        self._momentum_initialized = False\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        if not self._momentum_initialized:\n            for group in self.param_groups:\n                for p in group[\'params\']:\n                    if p.grad is None:\n                        continue\n                    state = self.state[p]\n                    grad = p.grad.data\n                    if grad.is_sparse:\n                        raise RuntimeError(\'NovoGrad does not support sparse gradients\')\n\n                    v = torch.norm(grad)**2\n                    m = grad/(torch.sqrt(v) + self._eps) + self._wd * p.data\n                    state[\'step\'] = 0\n                    state[\'v\'] = v\n                    state[\'m\'] = m\n                    state[\'grad_ema\'] = None\n            self._momentum_initialized = True\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                state = self.state[p]\n                state[\'step\'] += 1\n\n                step, v, m = state[\'step\'], state[\'v\'], state[\'m\']\n                grad_ema = state[\'grad_ema\']\n\n                grad = p.grad.data\n                g2 = torch.norm(grad)**2\n                grad_ema = g2 if grad_ema is None else grad_ema * \\\n                    self._beta2 + g2 * (1. - self._beta2)\n                grad *= 1.0 / (torch.sqrt(grad_ema) + self._eps)\n\n                if self._grad_averaging:\n                    grad *= (1. - self._beta1)\n\n                g2 = torch.norm(grad)**2\n                v = self._beta2*v + (1. - self._beta2)*g2\n                m = self._beta1*m + (grad / (torch.sqrt(v) + self._eps) + self._wd * p.data)\n                bias_correction1 = 1 - self._beta1 ** step\n                bias_correction2 = 1 - self._beta2 ** step\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n\n                state[\'v\'], state[\'m\']  = v, m\n                state[\'grad_ema\'] = grad_ema\n                p.data.add_(-step_size, m)\n        return loss\n'"
timm/optim/nvnovograd.py,6,"b'"""""" Nvidia NovoGrad Optimizer.\nOriginal impl by Nvidia from Jasper example:\n    - https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/SpeechRecognition/Jasper\nPaper: `Stochastic Gradient Methods with Layer-wise Adaptive Moments for Training of Deep Networks`\n    - https://arxiv.org/abs/1905.11286\n""""""\n\nimport torch\nfrom torch.optim.optimizer import Optimizer\nimport math\n\n\nclass NvNovoGrad(Optimizer):\n    """"""\n    Implements Novograd algorithm.\n\n    Args:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.95, 0.98))\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        grad_averaging: gradient averaging\n        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n            algorithm from the paper `On the Convergence of Adam and Beyond`_\n            (default: False)\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.95, 0.98), eps=1e-8,\n                 weight_decay=0, grad_averaging=False, amsgrad=False):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay,\n                        grad_averaging=grad_averaging,\n                        amsgrad=amsgrad)\n\n        super(NvNovoGrad, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(NvNovoGrad, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'amsgrad\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n            and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Sparse gradients are not supported.\')\n                amsgrad = group[\'amsgrad\']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros([]).to(state[\'exp_avg\'].device)\n                    if amsgrad:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\'max_exp_avg_sq\'] = torch.zeros([]).to(state[\'exp_avg\'].device)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsgrad:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                norm = torch.sum(torch.pow(grad, 2))\n\n                if exp_avg_sq == 0:\n                    exp_avg_sq.copy_(norm)\n                else:\n                    exp_avg_sq.mul_(beta2).add_(1 - beta2, norm)\n\n                if amsgrad:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                grad.div_(denom)\n                if group[\'weight_decay\'] != 0:\n                    grad.add_(group[\'weight_decay\'], p.data)\n                if group[\'grad_averaging\']:\n                    grad.mul_(1 - beta1)\n                exp_avg.mul_(beta1).add_(grad)\n\n                p.data.add_(-group[\'lr\'], exp_avg)\n\n        return loss\n'"
timm/optim/optim_factory.py,1,"b'import torch\nfrom torch import optim as optim\nfrom timm.optim import Nadam, RMSpropTF, AdamW, RAdam, NovoGrad, NvNovoGrad, Lookahead\ntry:\n    from apex.optimizers import FusedNovoGrad, FusedAdam, FusedLAMB, FusedSGD\n    has_apex = True\nexcept ImportError:\n    has_apex = False\n\n\ndef add_weight_decay(model, weight_decay=1e-5, skip_list=()):\n    decay = []\n    no_decay = []\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue  # frozen weights\n        if len(param.shape) == 1 or name.endswith("".bias"") or name in skip_list:\n            no_decay.append(param)\n        else:\n            decay.append(param)\n    return [\n        {\'params\': no_decay, \'weight_decay\': 0.},\n        {\'params\': decay, \'weight_decay\': weight_decay}]\n\n\ndef create_optimizer(args, model, filter_bias_and_bn=True):\n    opt_lower = args.opt.lower()\n    weight_decay = args.weight_decay\n    if \'adamw\' in opt_lower or \'radam\' in opt_lower:\n        # Compensate for the way current AdamW and RAdam optimizers apply LR to the weight-decay\n        # I don\'t believe they follow the paper or original Torch7 impl which schedules weight\n        # decay based on the ratio of current_lr/initial_lr\n        weight_decay /= args.lr\n    if weight_decay and filter_bias_and_bn:\n        parameters = add_weight_decay(model, weight_decay)\n        weight_decay = 0.\n    else:\n        parameters = model.parameters()\n\n    if \'fused\' in opt_lower:\n        assert has_apex and torch.cuda.is_available(), \'APEX and CUDA required for fused optimizers\'\n\n    opt_split = opt_lower.split(\'_\')\n    opt_lower = opt_split[-1]\n    if opt_lower == \'sgd\' or opt_lower == \'nesterov\':\n        optimizer = optim.SGD(\n            parameters, lr=args.lr, momentum=args.momentum, weight_decay=weight_decay, nesterov=True)\n    elif opt_lower == \'momentum\':\n        optimizer = optim.SGD(\n            parameters, lr=args.lr, momentum=args.momentum, weight_decay=weight_decay, nesterov=False)\n    elif opt_lower == \'adam\':\n        optimizer = optim.Adam(\n            parameters, lr=args.lr, weight_decay=weight_decay, eps=args.opt_eps)\n    elif opt_lower == \'adamw\':\n        optimizer = AdamW(\n            parameters, lr=args.lr, weight_decay=weight_decay, eps=args.opt_eps)\n    elif opt_lower == \'nadam\':\n        optimizer = Nadam(\n            parameters, lr=args.lr, weight_decay=weight_decay, eps=args.opt_eps)\n    elif opt_lower == \'radam\':\n        optimizer = RAdam(\n            parameters, lr=args.lr, weight_decay=weight_decay, eps=args.opt_eps)\n    elif opt_lower == \'adadelta\':\n        optimizer = optim.Adadelta(\n            parameters, lr=args.lr, weight_decay=weight_decay, eps=args.opt_eps)\n    elif opt_lower == \'rmsprop\':\n        optimizer = optim.RMSprop(\n            parameters, lr=args.lr, alpha=0.9, eps=args.opt_eps,\n            momentum=args.momentum, weight_decay=weight_decay)\n    elif opt_lower == \'rmsproptf\':\n        optimizer = RMSpropTF(\n            parameters, lr=args.lr, alpha=0.9, eps=args.opt_eps,\n            momentum=args.momentum, weight_decay=weight_decay)\n    elif opt_lower == \'novograd\':\n        optimizer = NovoGrad(parameters, lr=args.lr, weight_decay=weight_decay, eps=args.opt_eps)\n    elif opt_lower == \'nvnovograd\':\n        optimizer = NvNovoGrad(parameters, lr=args.lr, weight_decay=weight_decay, eps=args.opt_eps)\n    elif opt_lower == \'fusedsgd\':\n        optimizer = FusedSGD(\n            parameters, lr=args.lr, momentum=args.momentum, weight_decay=weight_decay, nesterov=True)\n    elif opt_lower == \'fusedmomentum\':\n        optimizer = FusedSGD(\n            parameters, lr=args.lr, momentum=args.momentum, weight_decay=weight_decay, nesterov=False)\n    elif opt_lower == \'fusedadam\':\n        optimizer = FusedAdam(\n            parameters, lr=args.lr, adam_w_mode=False, weight_decay=weight_decay, eps=args.opt_eps)\n    elif opt_lower == \'fusedadamw\':\n        optimizer = FusedAdam(\n            parameters, lr=args.lr, adam_w_mode=True, weight_decay=weight_decay, eps=args.opt_eps)\n    elif opt_lower == \'fusedlamb\':\n        optimizer = FusedLAMB(parameters, lr=args.lr, weight_decay=weight_decay, eps=args.opt_eps)\n    elif opt_lower == \'fusednovograd\':\n        optimizer = FusedNovoGrad(\n            parameters, lr=args.lr, betas=(0.95, 0.98), weight_decay=weight_decay, eps=args.opt_eps)\n    else:\n        assert False and ""Invalid optimizer""\n        raise ValueError\n\n    if len(opt_split) > 1:\n        if opt_split[0] == \'lookahead\':\n            optimizer = Lookahead(optimizer)\n\n    return optimizer\n'"
timm/optim/radam.py,5,"b'""""""RAdam Optimizer.\nImplementation lifted from: https://github.com/LiyuanLucasLiu/RAdam\nPaper: `On the Variance of the Adaptive Learning Rate and Beyond` - https://arxiv.org/abs/1908.03265\n""""""\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\n\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\'RAdam does not support sparse gradients\')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].type_as(p_data_fp32)\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state[\'step\'] += 1\n                buffered = self.buffer[int(state[\'step\'] % 10)]\n                if state[\'step\'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\'step\']\n                    beta2_t = beta2 ** state[\'step\']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\'step\'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it\'s an approximated value\n                    if N_sma >= 5:\n                        step_size = group[\'lr\'] * math.sqrt(\n                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n                                        N_sma_max - 2)) / (1 - beta1 ** state[\'step\'])\n                    else:\n                        step_size = group[\'lr\'] / (1 - beta1 ** state[\'step\'])\n                    buffered[2] = step_size\n\n                if group[\'weight_decay\'] != 0:\n                    p_data_fp32.add_(-group[\'weight_decay\'] * group[\'lr\'], p_data_fp32)\n\n                # more conservative since it\'s an approximated value\n                if N_sma >= 5:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n\n\nclass PlainRAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n\n        super(PlainRAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(PlainRAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\'RAdam does not support sparse gradients\')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].type_as(p_data_fp32)\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state[\'step\'] += 1\n                beta2_t = beta2 ** state[\'step\']\n                N_sma_max = 2 / (1 - beta2) - 1\n                N_sma = N_sma_max - 2 * state[\'step\'] * beta2_t / (1 - beta2_t)\n\n                if group[\'weight_decay\'] != 0:\n                    p_data_fp32.add_(-group[\'weight_decay\'] * group[\'lr\'], p_data_fp32)\n\n                # more conservative since it\'s an approximated value\n                if N_sma >= 5:\n                    step_size = group[\'lr\'] * math.sqrt(\n                        (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (\n                                    N_sma_max - 2)) / (1 - beta1 ** state[\'step\'])\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    step_size = group[\'lr\'] / (1 - beta1 ** state[\'step\'])\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n'"
timm/optim/rmsprop_tf.py,4,"b'import torch\nfrom torch.optim import Optimizer\n\n\nclass RMSpropTF(Optimizer):\n    """"""Implements RMSprop algorithm (TensorFlow style epsilon)\n\n    NOTE: This is a direct cut-and-paste of PyTorch RMSprop with eps applied before sqrt\n    to closer match Tensorflow for matching hyper-params.\n\n    Proposed by G. Hinton in his\n    `course <http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf>`_.\n\n    The centered version first appears in `Generating Sequences\n    With Recurrent Neural Networks <https://arxiv.org/pdf/1308.0850v5.pdf>`_.\n\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): learning rate (default: 1e-2)\n        momentum (float, optional): momentum factor (default: 0)\n        alpha (float, optional): smoothing (decay) constant (default: 0.9)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-10)\n        centered (bool, optional) : if ``True``, compute the centered RMSProp,\n            the gradient is normalized by an estimation of its variance\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        decoupled_decay (bool, optional): decoupled weight decay as per https://arxiv.org/abs/1711.05101\n        lr_in_momentum (bool, optional): learning rate scaling is included in the momentum buffer\n            update as per defaults in Tensorflow\n\n    """"""\n\n    def __init__(self, params, lr=1e-2, alpha=0.9, eps=1e-10, weight_decay=0, momentum=0., centered=False,\n                 decoupled_decay=False, lr_in_momentum=True):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= momentum:\n            raise ValueError(""Invalid momentum value: {}"".format(momentum))\n        if not 0.0 <= weight_decay:\n            raise ValueError(""Invalid weight_decay value: {}"".format(weight_decay))\n        if not 0.0 <= alpha:\n            raise ValueError(""Invalid alpha value: {}"".format(alpha))\n\n        defaults = dict(lr=lr, momentum=momentum, alpha=alpha, eps=eps, centered=centered, weight_decay=weight_decay,\n                        decoupled_decay=decoupled_decay, lr_in_momentum=lr_in_momentum)\n        super(RMSpropTF, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RMSpropTF, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'momentum\', 0)\n            group.setdefault(\'centered\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'RMSprop does not support sparse gradients\')\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    state[\'square_avg\'] = torch.ones_like(p.data)  # PyTorch inits to zero\n                    if group[\'momentum\'] > 0:\n                        state[\'momentum_buffer\'] = torch.zeros_like(p.data)\n                    if group[\'centered\']:\n                        state[\'grad_avg\'] = torch.zeros_like(p.data)\n\n                square_avg = state[\'square_avg\']\n                one_minus_alpha = 1. - group[\'alpha\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    if \'decoupled_decay\' in group and group[\'decoupled_decay\']:\n                        p.data.add_(-group[\'weight_decay\'], p.data)\n                    else:\n                        grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Tensorflow order of ops for updating squared avg\n                square_avg.add_(one_minus_alpha, grad.pow(2) - square_avg)\n                # square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)  # PyTorch original\n\n                if group[\'centered\']:\n                    grad_avg = state[\'grad_avg\']\n                    grad_avg.add_(one_minus_alpha, grad - grad_avg)\n                    # grad_avg.mul_(alpha).add_(1 - alpha, grad)  # PyTorch original\n                    avg = square_avg.addcmul(-1, grad_avg, grad_avg).add(group[\'eps\']).sqrt_()  # eps moved in sqrt\n                else:\n                    avg = square_avg.add(group[\'eps\']).sqrt_()  # eps moved in sqrt\n\n                if group[\'momentum\'] > 0:\n                    buf = state[\'momentum_buffer\']\n                    # Tensorflow accumulates the LR scaling in the momentum buffer\n                    if \'lr_in_momentum\' in group and group[\'lr_in_momentum\']:\n                        buf.mul_(group[\'momentum\']).addcdiv_(group[\'lr\'], grad, avg)\n                        p.data.add_(-buf)\n                    else:\n                        # PyTorch scales the param update by LR\n                        buf.mul_(group[\'momentum\']).addcdiv_(grad, avg)\n                        p.data.add_(-group[\'lr\'], buf)\n                else:\n                    p.data.addcdiv_(-group[\'lr\'], grad, avg)\n\n        return loss\n'"
timm/scheduler/__init__.py,0,b'from .cosine_lr import CosineLRScheduler\nfrom .plateau_lr import PlateauLRScheduler\nfrom .step_lr import StepLRScheduler\nfrom .tanh_lr import TanhLRScheduler\nfrom .scheduler_factory import create_scheduler\n'
timm/scheduler/cosine_lr.py,1,"b'import logging\nimport math\nimport numpy as np\nimport torch\n\nfrom .scheduler import Scheduler\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass CosineLRScheduler(Scheduler):\n    """"""\n    Cosine decay with restarts.\n    This is described in the paper https://arxiv.org/abs/1608.03983.\n\n    Inspiration from\n    https://github.com/allenai/allennlp/blob/master/allennlp/training/learning_rate_schedulers/cosine.py\n    """"""\n\n    def __init__(self,\n                 optimizer: torch.optim.Optimizer,\n                 t_initial: int,\n                 t_mul: float = 1.,\n                 lr_min: float = 0.,\n                 decay_rate: float = 1.,\n                 warmup_t=0,\n                 warmup_lr_init=0,\n                 warmup_prefix=False,\n                 cycle_limit=0,\n                 t_in_epochs=True,\n                 noise_range_t=None,\n                 noise_pct=0.67,\n                 noise_std=1.0,\n                 noise_seed=42,\n                 initialize=True) -> None:\n        super().__init__(\n            optimizer, param_group_field=""lr"",\n            noise_range_t=noise_range_t, noise_pct=noise_pct, noise_std=noise_std, noise_seed=noise_seed,\n            initialize=initialize)\n\n        assert t_initial > 0\n        assert lr_min >= 0\n        if t_initial == 1 and t_mul == 1 and decay_rate == 1:\n            logger.warning(""Cosine annealing scheduler will have no effect on the learning ""\n                           ""rate since t_initial = t_mul = eta_mul = 1."")\n        self.t_initial = t_initial\n        self.t_mul = t_mul\n        self.lr_min = lr_min\n        self.decay_rate = decay_rate\n        self.cycle_limit = cycle_limit\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.warmup_prefix = warmup_prefix\n        self.t_in_epochs = t_in_epochs\n        if self.warmup_t:\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n\n    def _get_lr(self, t):\n        if t < self.warmup_t:\n            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n        else:\n            if self.warmup_prefix:\n                t = t - self.warmup_t\n\n            if self.t_mul != 1:\n                i = math.floor(math.log(1 - t / self.t_initial * (1 - self.t_mul), self.t_mul))\n                t_i = self.t_mul ** i * self.t_initial\n                t_curr = t - (1 - self.t_mul ** i) / (1 - self.t_mul) * self.t_initial\n            else:\n                i = t // self.t_initial\n                t_i = self.t_initial\n                t_curr = t - (self.t_initial * i)\n\n            gamma = self.decay_rate ** i\n            lr_min = self.lr_min * gamma\n            lr_max_values = [v * gamma for v in self.base_values]\n\n            if self.cycle_limit == 0 or (self.cycle_limit > 0 and i < self.cycle_limit):\n                lrs = [\n                    lr_min + 0.5 * (lr_max - lr_min) * (1 + math.cos(math.pi * t_curr / t_i)) for lr_max in lr_max_values\n                ]\n            else:\n                lrs = [self.lr_min for _ in self.base_values]\n\n        return lrs\n\n    def get_epoch_values(self, epoch: int):\n        if self.t_in_epochs:\n            return self._get_lr(epoch)\n        else:\n            return None\n\n    def get_update_values(self, num_updates: int):\n        if not self.t_in_epochs:\n            return self._get_lr(num_updates)\n        else:\n            return None\n\n    def get_cycle_length(self, cycles=0):\n        if not cycles:\n            cycles = self.cycle_limit\n        assert cycles > 0\n        if self.t_mul == 1.0:\n            return self.t_initial * cycles\n        else:\n            return int(math.floor(-self.t_initial * (self.t_mul ** cycles - 1) / (1 - self.t_mul)))\n'"
timm/scheduler/plateau_lr.py,1,"b'import torch\n\nfrom .scheduler import Scheduler\n\n\nclass PlateauLRScheduler(Scheduler):\n    """"""Decay the LR by a factor every time the validation loss plateaus.""""""\n\n    def __init__(self,\n                 optimizer,\n                 decay_rate=0.1,\n                 patience_t=10,\n                 verbose=True,\n                 threshold=1e-4,\n                 cooldown_t=0,\n                 warmup_t=0,\n                 warmup_lr_init=0,\n                 lr_min=0,\n                 mode=\'min\',\n                 initialize=True,\n                 ):\n        super().__init__(optimizer, \'lr\', initialize=initialize)\n\n        self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n            self.optimizer,\n            patience=patience_t,\n            factor=decay_rate,\n            verbose=verbose,\n            threshold=threshold,\n            cooldown=cooldown_t,\n            mode=mode,\n            min_lr=lr_min\n        )\n\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        if self.warmup_t:\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n\n    def state_dict(self):\n        return {\n            \'best\': self.lr_scheduler.best,\n            \'last_epoch\': self.lr_scheduler.last_epoch,\n        }\n\n    def load_state_dict(self, state_dict):\n        self.lr_scheduler.best = state_dict[\'best\']\n        if \'last_epoch\' in state_dict:\n            self.lr_scheduler.last_epoch = state_dict[\'last_epoch\']\n\n    # override the base class step fn completely\n    def step(self, epoch, metric=None):\n        if epoch <= self.warmup_t:\n            lrs = [self.warmup_lr_init + epoch * s for s in self.warmup_steps]\n            super().update_groups(lrs)\n        else:\n            self.lr_scheduler.step(metric, epoch)\n'"
timm/scheduler/scheduler.py,4,"b'from typing import Dict, Any\n\nimport torch\n\n\nclass Scheduler:\n    """""" Parameter Scheduler Base Class\n    A scheduler base class that can be used to schedule any optimizer parameter groups.\n\n    Unlike the builtin PyTorch schedulers, this is intended to be consistently called\n    * At the END of each epoch, before incrementing the epoch count, to calculate next epoch\'s value\n    * At the END of each optimizer update, after incrementing the update count, to calculate next update\'s value\n\n    The schedulers built on this should try to remain as stateless as possible (for simplicity).\n\n    This family of schedulers is attempting to avoid the confusion of the meaning of \'last_epoch\'\n    and -1 values for special behaviour. All epoch and update counts must be tracked in the training\n    code and explicitly passed in to the schedulers on the corresponding step or step_update call.\n\n    Based on ideas from:\n     * https://github.com/pytorch/fairseq/tree/master/fairseq/optim/lr_scheduler\n     * https://github.com/allenai/allennlp/tree/master/allennlp/training/learning_rate_schedulers\n    """"""\n\n    def __init__(self,\n                 optimizer: torch.optim.Optimizer,\n                 param_group_field: str,\n                 noise_range_t=None,\n                 noise_type=\'normal\',\n                 noise_pct=0.67,\n                 noise_std=1.0,\n                 noise_seed=None,\n                 initialize: bool = True) -> None:\n        self.optimizer = optimizer\n        self.param_group_field = param_group_field\n        self._initial_param_group_field = f""initial_{param_group_field}""\n        if initialize:\n            for i, group in enumerate(self.optimizer.param_groups):\n                if param_group_field not in group:\n                    raise KeyError(f""{param_group_field} missing from param_groups[{i}]"")\n                group.setdefault(self._initial_param_group_field, group[param_group_field])\n        else:\n            for i, group in enumerate(self.optimizer.param_groups):\n                if self._initial_param_group_field not in group:\n                    raise KeyError(f""{self._initial_param_group_field} missing from param_groups[{i}]"")\n        self.base_values = [group[self._initial_param_group_field] for group in self.optimizer.param_groups]\n        self.metric = None  # any point to having this for all?\n        self.noise_range_t = noise_range_t\n        self.noise_pct = noise_pct\n        self.noise_type = noise_type\n        self.noise_std = noise_std\n        self.noise_seed = noise_seed if noise_seed is not None else 42\n        self.update_groups(self.base_values)\n\n    def state_dict(self) -> Dict[str, Any]:\n        return {key: value for key, value in self.__dict__.items() if key != \'optimizer\'}\n\n    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n        self.__dict__.update(state_dict)\n\n    def get_epoch_values(self, epoch: int):\n        return None\n\n    def get_update_values(self, num_updates: int):\n        return None\n\n    def step(self, epoch: int, metric: float = None) -> None:\n        self.metric = metric\n        values = self.get_epoch_values(epoch)\n        if values is not None:\n            values = self._add_noise(values, epoch)\n            self.update_groups(values)\n\n    def step_update(self, num_updates: int, metric: float = None):\n        self.metric = metric\n        values = self.get_update_values(num_updates)\n        if values is not None:\n            values = self._add_noise(values, num_updates)\n            self.update_groups(values)\n\n    def update_groups(self, values):\n        if not isinstance(values, (list, tuple)):\n            values = [values] * len(self.optimizer.param_groups)\n        for param_group, value in zip(self.optimizer.param_groups, values):\n            param_group[self.param_group_field] = value\n\n    def _add_noise(self, lrs, t):\n        if self.noise_range_t is not None:\n            if isinstance(self.noise_range_t, (list, tuple)):\n                apply_noise = self.noise_range_t[0] <= t < self.noise_range_t[1]\n            else:\n                apply_noise = t >= self.noise_range_t\n            if apply_noise:\n                g = torch.Generator()\n                g.manual_seed(self.noise_seed + t)\n                if self.noise_type == \'normal\':\n                    while True:\n                        # resample if noise out of percent limit, brute force but shouldn\'t spin much\n                        noise = torch.randn(1, generator=g).item()\n                        if abs(noise) < self.noise_pct:\n                            break\n                else:\n                    noise = 2 * (torch.rand(1, generator=g).item() - 0.5) * self.noise_pct\n                lrs = [v + v * noise for v in lrs]\n        return lrs\n'"
timm/scheduler/scheduler_factory.py,0,"b""from .cosine_lr import CosineLRScheduler\nfrom .tanh_lr import TanhLRScheduler\nfrom .step_lr import StepLRScheduler\nfrom .plateau_lr import PlateauLRScheduler\n\n\ndef create_scheduler(args, optimizer):\n    num_epochs = args.epochs\n\n    if args.lr_noise is not None:\n        if isinstance(args.lr_noise, (list, tuple)):\n            noise_range = [n * num_epochs for n in args.lr_noise]\n            if len(noise_range) == 1:\n                noise_range = noise_range[0]\n        else:\n            noise_range = args.lr_noise * num_epochs\n    else:\n        noise_range = None\n\n    lr_scheduler = None\n    #FIXME expose cycle parms of the scheduler config to arguments\n    if args.sched == 'cosine':\n        lr_scheduler = CosineLRScheduler(\n            optimizer,\n            t_initial=num_epochs,\n            t_mul=1.0,\n            lr_min=args.min_lr,\n            decay_rate=args.decay_rate,\n            warmup_lr_init=args.warmup_lr,\n            warmup_t=args.warmup_epochs,\n            cycle_limit=1,\n            t_in_epochs=True,\n            noise_range_t=noise_range,\n            noise_pct=args.lr_noise_pct,\n            noise_std=args.lr_noise_std,\n            noise_seed=args.seed,\n        )\n        num_epochs = lr_scheduler.get_cycle_length() + args.cooldown_epochs\n    elif args.sched == 'tanh':\n        lr_scheduler = TanhLRScheduler(\n            optimizer,\n            t_initial=num_epochs,\n            t_mul=1.0,\n            lr_min=args.min_lr,\n            warmup_lr_init=args.warmup_lr,\n            warmup_t=args.warmup_epochs,\n            cycle_limit=1,\n            t_in_epochs=True,\n            noise_range_t=noise_range,\n            noise_pct=args.lr_noise_pct,\n            noise_std=args.lr_noise_std,\n            noise_seed=args.seed,\n        )\n        num_epochs = lr_scheduler.get_cycle_length() + args.cooldown_epochs\n    elif args.sched == 'step':\n        lr_scheduler = StepLRScheduler(\n            optimizer,\n            decay_t=args.decay_epochs,\n            decay_rate=args.decay_rate,\n            warmup_lr_init=args.warmup_lr,\n            warmup_t=args.warmup_epochs,\n            noise_range_t=noise_range,\n            noise_pct=args.lr_noise_pct,\n            noise_std=args.lr_noise_std,\n            noise_seed=args.seed,\n        )\n    elif args.sched == 'plateau':\n        lr_scheduler = PlateauLRScheduler(\n            optimizer,\n            decay_rate=args.decay_rate,\n            patience_t=args.patience_epochs,\n            lr_min=args.min_lr,\n            warmup_lr_init=args.warmup_lr,\n            warmup_t=args.warmup_epochs,\n            cooldown_t=args.cooldown_epochs,\n        )\n\n    return lr_scheduler, num_epochs\n"""
timm/scheduler/step_lr.py,1,"b'import math\nimport torch\n\nfrom .scheduler import Scheduler\n\n\nclass StepLRScheduler(Scheduler):\n    """"""\n    """"""\n\n    def __init__(self,\n                 optimizer: torch.optim.Optimizer,\n                 decay_t: float,\n                 decay_rate: float = 1.,\n                 warmup_t=0,\n                 warmup_lr_init=0,\n                 t_in_epochs=True,\n                 noise_range_t=None,\n                 noise_pct=0.67,\n                 noise_std=1.0,\n                 noise_seed=42,\n                 initialize=True,\n                 ) -> None:\n        super().__init__(\n            optimizer, param_group_field=""lr"",\n            noise_range_t=noise_range_t, noise_pct=noise_pct, noise_std=noise_std, noise_seed=noise_seed,\n            initialize=initialize)\n\n        self.decay_t = decay_t\n        self.decay_rate = decay_rate\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.t_in_epochs = t_in_epochs\n        if self.warmup_t:\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in self.base_values]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n\n    def _get_lr(self, t):\n        if t < self.warmup_t:\n            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n        else:\n            lrs = [v * (self.decay_rate ** (t // self.decay_t)) for v in self.base_values]\n        return lrs\n\n    def get_epoch_values(self, epoch: int):\n        if self.t_in_epochs:\n            return self._get_lr(epoch)\n        else:\n            return None\n\n    def get_update_values(self, num_updates: int):\n        if not self.t_in_epochs:\n            return self._get_lr(num_updates)\n        else:\n            return None\n'"
timm/scheduler/tanh_lr.py,1,"b'import logging\nimport math\nimport numpy as np\nimport torch\n\nfrom .scheduler import Scheduler\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass TanhLRScheduler(Scheduler):\n    """"""\n    Hyberbolic-Tangent decay with restarts.\n    This is described in the paper https://arxiv.org/abs/1806.01593\n    """"""\n\n    def __init__(self,\n                 optimizer: torch.optim.Optimizer,\n                 t_initial: int,\n                 lb: float = -6.,\n                 ub: float = 4.,\n                 t_mul: float = 1.,\n                 lr_min: float = 0.,\n                 decay_rate: float = 1.,\n                 warmup_t=0,\n                 warmup_lr_init=0,\n                 warmup_prefix=False,\n                 cycle_limit=0,\n                 t_in_epochs=True,\n                 noise_range_t=None,\n                 noise_pct=0.67,\n                 noise_std=1.0,\n                 noise_seed=42,\n                 initialize=True) -> None:\n        super().__init__(\n            optimizer, param_group_field=""lr"",\n            noise_range_t=noise_range_t, noise_pct=noise_pct, noise_std=noise_std, noise_seed=noise_seed,\n            initialize=initialize)\n\n        assert t_initial > 0\n        assert lr_min >= 0\n        assert lb < ub\n        assert cycle_limit >= 0\n        assert warmup_t >= 0\n        assert warmup_lr_init >= 0\n        self.lb = lb\n        self.ub = ub\n        self.t_initial = t_initial\n        self.t_mul = t_mul\n        self.lr_min = lr_min\n        self.decay_rate = decay_rate\n        self.cycle_limit = cycle_limit\n        self.warmup_t = warmup_t\n        self.warmup_lr_init = warmup_lr_init\n        self.warmup_prefix = warmup_prefix\n        self.t_in_epochs = t_in_epochs\n        if self.warmup_t:\n            t_v = self.base_values if self.warmup_prefix else self._get_lr(self.warmup_t)\n            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t for v in t_v]\n            super().update_groups(self.warmup_lr_init)\n        else:\n            self.warmup_steps = [1 for _ in self.base_values]\n\n    def _get_lr(self, t):\n        if t < self.warmup_t:\n            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n        else:\n            if self.warmup_prefix:\n                t = t - self.warmup_t\n\n            if self.t_mul != 1:\n                i = math.floor(math.log(1 - t / self.t_initial * (1 - self.t_mul), self.t_mul))\n                t_i = self.t_mul ** i * self.t_initial\n                t_curr = t - (1 - self.t_mul ** i) / (1 - self.t_mul) * self.t_initial\n            else:\n                i = t // self.t_initial\n                t_i = self.t_initial\n                t_curr = t - (self.t_initial * i)\n\n            if self.cycle_limit == 0 or (self.cycle_limit > 0 and i < self.cycle_limit):\n                gamma = self.decay_rate ** i\n                lr_min = self.lr_min * gamma\n                lr_max_values = [v * gamma for v in self.base_values]\n\n                tr = t_curr / t_i\n                lrs = [\n                    lr_min + 0.5 * (lr_max - lr_min) * (1 - math.tanh(self.lb * (1. - tr) + self.ub * tr))\n                    for lr_max in lr_max_values\n                ]\n            else:\n                lrs = [self.lr_min * (self.decay_rate ** self.cycle_limit) for _ in self.base_values]\n        return lrs\n\n    def get_epoch_values(self, epoch: int):\n        if self.t_in_epochs:\n            return self._get_lr(epoch)\n        else:\n            return None\n\n    def get_update_values(self, num_updates: int):\n        if not self.t_in_epochs:\n            return self._get_lr(num_updates)\n        else:\n            return None\n\n    def get_cycle_length(self, cycles=0):\n        if not cycles:\n            cycles = self.cycle_limit\n        assert cycles > 0\n        if self.t_mul == 1.0:\n            return self.t_initial * cycles\n        else:\n            return int(math.floor(-self.t_initial * (self.t_mul ** cycles - 1) / (1 - self.t_mul)))\n'"
timm/models/layers/__init__.py,0,"b'from .padding import get_padding\nfrom .pool2d_same import AvgPool2dSame\nfrom .conv2d_same import Conv2dSame\nfrom .conv_bn_act import ConvBnAct\nfrom .mixed_conv2d import MixedConv2d\nfrom .cond_conv2d import CondConv2d, get_condconv_initializer\nfrom .pool2d_same import create_pool2d\nfrom .create_conv2d import create_conv2d\nfrom .create_attn import create_attn\nfrom .selective_kernel import SelectiveKernelConv\nfrom .se import SEModule\nfrom .eca import EcaModule, CecaModule\nfrom .activations import *\nfrom .adaptive_avgmax_pool import \\\n    adaptive_avgmax_pool2d, select_adaptive_pool2d, AdaptiveAvgMaxPool2d, SelectAdaptivePool2d\nfrom .drop import DropBlock2d, DropPath, drop_block_2d, drop_path\nfrom .test_time_pool import TestTimePoolHead, apply_test_time_pool\nfrom .split_batchnorm import SplitBatchNorm2d, convert_splitbn_model\nfrom .anti_aliasing import AntiAliasDownsampleLayer\nfrom .space_to_depth import SpaceToDepthModule\nfrom .blur_pool import BlurPool2d\nfrom .weight_init import trunc_normal_\n'"
timm/models/layers/activations.py,13,"b'"""""" Activations\n\nA collection of activations fn and modules with a common interface so that they can\neasily be swapped. All have an `inplace` arg even if not used.\n\nHacked together by Ross Wightman\n""""""\n\n\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\n\n_USE_MEM_EFFICIENT_ISH = True\nif _USE_MEM_EFFICIENT_ISH:\n    # This version reduces memory overhead of Swish during training by\n    # recomputing torch.sigmoid(x) in backward instead of saving it.\n    @torch.jit.script\n    def swish_jit_fwd(x):\n        return x.mul(torch.sigmoid(x))\n\n\n    @torch.jit.script\n    def swish_jit_bwd(x, grad_output):\n        x_sigmoid = torch.sigmoid(x)\n        return grad_output * (x_sigmoid * (1 + x * (1 - x_sigmoid)))\n\n\n    class SwishJitAutoFn(torch.autograd.Function):\n        """""" torch.jit.script optimised Swish\n        Inspired by conversation btw Jeremy Howard & Adam Pazske\n        https://twitter.com/jeremyphoward/status/1188251041835315200\n        """"""\n\n        @staticmethod\n        def forward(ctx, x):\n            ctx.save_for_backward(x)\n            return swish_jit_fwd(x)\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            x = ctx.saved_tensors[0]\n            return swish_jit_bwd(x, grad_output)\n\n\n    def swish(x, _inplace=False):\n        return SwishJitAutoFn.apply(x)\n\n\n    @torch.jit.script\n    def mish_jit_fwd(x):\n        return x.mul(torch.tanh(F.softplus(x)))\n\n\n    @torch.jit.script\n    def mish_jit_bwd(x, grad_output):\n        x_sigmoid = torch.sigmoid(x)\n        x_tanh_sp = F.softplus(x).tanh()\n        return grad_output.mul(x_tanh_sp + x * x_sigmoid * (1 - x_tanh_sp * x_tanh_sp))\n\n\n    class MishJitAutoFn(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, x):\n            ctx.save_for_backward(x)\n            return mish_jit_fwd(x)\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            x = ctx.saved_tensors[0]\n            return mish_jit_bwd(x, grad_output)\n\n    def mish(x, _inplace=False):\n        return MishJitAutoFn.apply(x)\n\nelse:\n    def swish(x, inplace: bool = False):\n        """"""Swish - Described in: https://arxiv.org/abs/1710.05941\n        """"""\n        return x.mul_(x.sigmoid()) if inplace else x.mul(x.sigmoid())\n\n\n    def mish(x, _inplace: bool = False):\n        """"""Mish: A Self Regularized Non-Monotonic Neural Activation Function - https://arxiv.org/abs/1908.08681\n        """"""\n        return x.mul(F.softplus(x).tanh())\n\n\nclass Swish(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(Swish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return swish(x, self.inplace)\n\n\nclass Mish(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(Mish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return mish(x, self.inplace)\n\n\ndef sigmoid(x, inplace: bool = False):\n    return x.sigmoid_() if inplace else x.sigmoid()\n\n\n# PyTorch has this, but not with a consistent inplace argmument interface\nclass Sigmoid(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(Sigmoid, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return x.sigmoid_() if self.inplace else x.sigmoid()\n\n\ndef tanh(x, inplace: bool = False):\n    return x.tanh_() if inplace else x.tanh()\n\n\n# PyTorch has this, but not with a consistent inplace argmument interface\nclass Tanh(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(Tanh, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return x.tanh_() if self.inplace else x.tanh()\n\n\ndef hard_swish(x, inplace: bool = False):\n    inner = F.relu6(x + 3.).div_(6.)\n    return x.mul_(inner) if inplace else x.mul(inner)\n\n\nclass HardSwish(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(HardSwish, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return hard_swish(x, self.inplace)\n\n\ndef hard_sigmoid(x, inplace: bool = False):\n    if inplace:\n        return x.add_(3.).clamp_(0., 6.).div_(6.)\n    else:\n        return F.relu6(x + 3.) / 6.\n\n\nclass HardSigmoid(nn.Module):\n    def __init__(self, inplace: bool = False):\n        super(HardSigmoid, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, x):\n        return hard_sigmoid(x, self.inplace)\n\n'"
timm/models/layers/adaptive_avgmax_pool.py,3,"b'"""""" PyTorch selectable adaptive pooling\nAdaptive pooling with the ability to select the type of pooling from:\n    * \'avg\' - Average pooling\n    * \'max\' - Max pooling\n    * \'avgmax\' - Sum of average and max pooling re-scaled by 0.5\n    * \'avgmaxc\' - Concatenation of average and max pooling along feature dim, doubles feature dim\n\nBoth a functional and a nn.Module version of the pooling is provided.\n\nAuthor: Ross Wightman (rwightman)\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef adaptive_pool_feat_mult(pool_type=\'avg\'):\n    if pool_type == \'catavgmax\':\n        return 2\n    else:\n        return 1\n\n\ndef adaptive_avgmax_pool2d(x, output_size=1):\n    x_avg = F.adaptive_avg_pool2d(x, output_size)\n    x_max = F.adaptive_max_pool2d(x, output_size)\n    return 0.5 * (x_avg + x_max)\n\n\ndef adaptive_catavgmax_pool2d(x, output_size=1):\n    x_avg = F.adaptive_avg_pool2d(x, output_size)\n    x_max = F.adaptive_max_pool2d(x, output_size)\n    return torch.cat((x_avg, x_max), 1)\n\n\ndef select_adaptive_pool2d(x, pool_type=\'avg\', output_size=1):\n    """"""Selectable global pooling function with dynamic input kernel size\n    """"""\n    if pool_type == \'avg\':\n        x = F.adaptive_avg_pool2d(x, output_size)\n    elif pool_type == \'avgmax\':\n        x = adaptive_avgmax_pool2d(x, output_size)\n    elif pool_type == \'catavgmax\':\n        x = adaptive_catavgmax_pool2d(x, output_size)\n    elif pool_type == \'max\':\n        x = F.adaptive_max_pool2d(x, output_size)\n    else:\n        assert False, \'Invalid pool type: %s\' % pool_type\n    return x\n\n\nclass AdaptiveAvgMaxPool2d(nn.Module):\n    def __init__(self, output_size=1):\n        super(AdaptiveAvgMaxPool2d, self).__init__()\n        self.output_size = output_size\n\n    def forward(self, x):\n        return adaptive_avgmax_pool2d(x, self.output_size)\n\n\nclass AdaptiveCatAvgMaxPool2d(nn.Module):\n    def __init__(self, output_size=1):\n        super(AdaptiveCatAvgMaxPool2d, self).__init__()\n        self.output_size = output_size\n\n    def forward(self, x):\n        return adaptive_catavgmax_pool2d(x, self.output_size)\n\n\nclass SelectAdaptivePool2d(nn.Module):\n    """"""Selectable global pooling layer with dynamic input kernel size\n    """"""\n    def __init__(self, output_size=1, pool_type=\'avg\', flatten=False):\n        super(SelectAdaptivePool2d, self).__init__()\n        self.output_size = output_size\n        self.pool_type = pool_type\n        self.flatten = flatten\n        if pool_type == \'avgmax\':\n            self.pool = AdaptiveAvgMaxPool2d(output_size)\n        elif pool_type == \'catavgmax\':\n            self.pool = AdaptiveCatAvgMaxPool2d(output_size)\n        elif pool_type == \'max\':\n            self.pool = nn.AdaptiveMaxPool2d(output_size)\n        else:\n            if pool_type != \'avg\':\n                assert False, \'Invalid pool type: %s\' % pool_type\n            self.pool = nn.AdaptiveAvgPool2d(output_size)\n\n    def forward(self, x):\n        x = self.pool(x)\n        if self.flatten:\n            x = x.flatten(1)\n        return x\n\n    def feat_mult(self):\n        return adaptive_pool_feat_mult(self.pool_type)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \' (\' \\\n               + \'output_size=\' + str(self.output_size) \\\n               + \', pool_type=\' + self.pool_type + \')\'\n'"
timm/models/layers/anti_aliasing.py,10,"b""import torch\nimport torch.nn.parallel\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass AntiAliasDownsampleLayer(nn.Module):\n    def __init__(self, channels: int = 0, filt_size: int = 3, stride: int = 2, no_jit: bool = False):\n        super(AntiAliasDownsampleLayer, self).__init__()\n        if no_jit:\n            self.op = Downsample(channels, filt_size, stride)\n        else:\n            self.op = DownsampleJIT(channels, filt_size, stride)\n\n        # FIXME I should probably override _apply and clear DownsampleJIT filter cache for .cuda(), .half(), etc calls\n\n    def forward(self, x):\n        return self.op(x)\n\n\n@torch.jit.script\nclass DownsampleJIT(object):\n    def __init__(self, channels: int = 0, filt_size: int = 3, stride: int = 2):\n        self.channels = channels\n        self.stride = stride\n        self.filt_size = filt_size\n        assert self.filt_size == 3\n        assert stride == 2\n        self.filt = {}  # lazy init by device for DataParallel compat\n\n    def _create_filter(self, like: torch.Tensor):\n        filt = torch.tensor([1., 2., 1.], dtype=like.dtype, device=like.device)\n        filt = filt[:, None] * filt[None, :]\n        filt = filt / torch.sum(filt)\n        return filt[None, None, :, :].repeat((self.channels, 1, 1, 1))\n\n    def __call__(self, input: torch.Tensor):\n        input_pad = F.pad(input, (1, 1, 1, 1), 'reflect')\n        filt = self.filt.get(str(input.device), self._create_filter(input))\n        return F.conv2d(input_pad, filt, stride=2, padding=0, groups=input.shape[1])\n\n\nclass Downsample(nn.Module):\n    def __init__(self, channels=None, filt_size=3, stride=2):\n        super(Downsample, self).__init__()\n        self.channels = channels\n        self.filt_size = filt_size\n        self.stride = stride\n\n        assert self.filt_size == 3\n        filt = torch.tensor([1., 2., 1.])\n        filt = filt[:, None] * filt[None, :]\n        filt = filt / torch.sum(filt)\n\n        # self.filt = filt[None, None, :, :].repeat((self.channels, 1, 1, 1))\n        self.register_buffer('filt', filt[None, None, :, :].repeat((self.channels, 1, 1, 1)))\n\n    def forward(self, input):\n        input_pad = F.pad(input, (1, 1, 1, 1), 'reflect')\n        return F.conv2d(input_pad, self.filt, stride=self.stride, padding=0, groups=input.shape[1])\n"""
timm/models/layers/blur_pool.py,7,"b'""""""\nBlurPool layer inspired by\n - Kornia\'s Max_BlurPool2d\n - Making Convolutional Networks Shift-Invariant Again :cite:`zhang2019shiftinvar`\n\nFIXME merge this impl with those in `anti_aliasing.py`\n\nHacked together by Chris Ha and Ross Wightman\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom typing import Dict\nfrom .padding import get_padding\n\n\nclass BlurPool2d(nn.Module):\n    r""""""Creates a module that computes blurs and downsample a given feature map.\n    See :cite:`zhang2019shiftinvar` for more details.\n    Corresponds to the Downsample class, which does blurring and subsampling\n\n    Args:\n        channels = Number of input channels\n        filt_size (int): binomial filter size for blurring. currently supports 3 (default) and 5.\n        stride (int): downsampling filter stride\n\n    Returns:\n        torch.Tensor: the transformed tensor.\n    """"""\n    filt: Dict[str, torch.Tensor]\n\n    def __init__(self, channels, filt_size=3, stride=2) -> None:\n        super(BlurPool2d, self).__init__()\n        assert filt_size > 1\n        self.channels = channels\n        self.filt_size = filt_size\n        self.stride = stride\n        pad_size = [get_padding(filt_size, stride, dilation=1)] * 4\n        self.padding = nn.ReflectionPad2d(pad_size)\n        self._coeffs = torch.tensor((np.poly1d((0.5, 0.5)) ** (self.filt_size - 1)).coeffs)  # for torchscript compat\n        self.filt = {}  # lazy init by device for DataParallel compat\n\n    def _create_filter(self, like: torch.Tensor):\n        blur_filter = (self._coeffs[:, None] * self._coeffs[None, :]).to(dtype=like.dtype, device=like.device)\n        return blur_filter[None, None, :, :].repeat(self.channels, 1, 1, 1)\n\n    def _apply(self, fn):\n        # override nn.Module _apply, reset filter cache if used\n        self.filt = {}\n        super(BlurPool2d, self)._apply(fn)\n\n    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n        C = input_tensor.shape[1]\n        blur_filt = self.filt.get(str(input_tensor.device), self._create_filter(input_tensor))\n        return F.conv2d(\n            self.padding(input_tensor), blur_filt, stride=self.stride, groups=C)\n'"
timm/models/layers/cbam.py,5,"b'"""""" CBAM (sort-of) Attention\n\nExperimental impl of CBAM: Convolutional Block Attention Module: https://arxiv.org/abs/1807.06521\n\nWARNING: Results with these attention layers have been mixed. They can significantly reduce performance on\nsome tasks, especially fine-grained it seems. I may end up removing this impl.\n\nHacked together by Ross Wightman\n""""""\n\nimport torch\nfrom torch import nn as nn\nfrom .conv_bn_act import ConvBnAct\n\n\nclass ChannelAttn(nn.Module):\n    """""" Original CBAM channel attention module, currently avg + max pool variant only.\n    """"""\n    def __init__(self, channels, reduction=16, act_layer=nn.ReLU):\n        super(ChannelAttn, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, 1, bias=False)\n        self.act = act_layer(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, 1, bias=False)\n\n    def forward(self, x):\n        x_avg = self.avg_pool(x)\n        x_max = self.max_pool(x)\n        x_avg = self.fc2(self.act(self.fc1(x_avg)))\n        x_max = self.fc2(self.act(self.fc1(x_max)))\n        x_attn = x_avg + x_max\n        return x * x_attn.sigmoid()\n\n\nclass LightChannelAttn(ChannelAttn):\n    """"""An experimental \'lightweight\' that sums avg + max pool first\n    """"""\n    def __init__(self, channels, reduction=16):\n        super(LightChannelAttn, self).__init__(channels, reduction)\n\n    def forward(self, x):\n        x_pool = 0.5 * self.avg_pool(x) + 0.5 * self.max_pool(x)\n        x_attn = self.fc2(self.act(self.fc1(x_pool)))\n        return x * x_attn.sigmoid()\n\n\nclass SpatialAttn(nn.Module):\n    """""" Original CBAM spatial attention module\n    """"""\n    def __init__(self, kernel_size=7):\n        super(SpatialAttn, self).__init__()\n        self.conv = ConvBnAct(2, 1, kernel_size, act_layer=None)\n\n    def forward(self, x):\n        x_avg = torch.mean(x, dim=1, keepdim=True)\n        x_max = torch.max(x, dim=1, keepdim=True)[0]\n        x_attn = torch.cat([x_avg, x_max], dim=1)\n        x_attn = self.conv(x_attn)\n        return x * x_attn.sigmoid()\n\n\nclass LightSpatialAttn(nn.Module):\n    """"""An experimental \'lightweight\' variant that sums avg_pool and max_pool results.\n    """"""\n    def __init__(self, kernel_size=7):\n        super(LightSpatialAttn, self).__init__()\n        self.conv = ConvBnAct(1, 1, kernel_size, act_layer=None)\n\n    def forward(self, x):\n        x_avg = torch.mean(x, dim=1, keepdim=True)\n        x_max = torch.max(x, dim=1, keepdim=True)[0]\n        x_attn = 0.5 * x_avg + 0.5 * x_max\n        x_attn = self.conv(x_attn)\n        return x * x_attn.sigmoid()\n\n\nclass CbamModule(nn.Module):\n    def __init__(self, channels, spatial_kernel_size=7):\n        super(CbamModule, self).__init__()\n        self.channel = ChannelAttn(channels)\n        self.spatial = SpatialAttn(spatial_kernel_size)\n\n    def forward(self, x):\n        x = self.channel(x)\n        x = self.spatial(x)\n        return x\n\n\nclass LightCbamModule(nn.Module):\n    def __init__(self, channels, spatial_kernel_size=7):\n        super(LightCbamModule, self).__init__()\n        self.channel = LightChannelAttn(channels)\n        self.spatial = LightSpatialAttn(spatial_kernel_size)\n\n    def forward(self, x):\n        x = self.channel(x)\n        x = self.spatial(x)\n        return x\n\n'"
timm/models/layers/cond_conv2d.py,10,"b'"""""" PyTorch Conditionally Parameterized Convolution (CondConv)\n\nPaper: CondConv: Conditionally Parameterized Convolutions for Efficient Inference\n(https://arxiv.org/abs/1904.04971)\n\nHacked together by Ross Wightman\n""""""\n\nimport math\nfrom functools import partial\nimport numpy as np\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\n\nfrom .helpers import tup_pair\nfrom .conv2d_same import conv2d_same\nfrom timm.models.layers.padding import get_padding_value\n\n\ndef get_condconv_initializer(initializer, num_experts, expert_shape):\n    def condconv_initializer(weight):\n        """"""CondConv initializer function.""""""\n        num_params = np.prod(expert_shape)\n        if (len(weight.shape) != 2 or weight.shape[0] != num_experts or\n                weight.shape[1] != num_params):\n            raise (ValueError(\n                \'CondConv variables must have shape [num_experts, num_params]\'))\n        for i in range(num_experts):\n            initializer(weight[i].view(expert_shape))\n    return condconv_initializer\n\n\nclass CondConv2d(nn.Module):\n    """""" Conditionally Parameterized Convolution\n    Inspired by: https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/condconv/condconv_layers.py\n\n    Grouped convolution hackery for parallel execution of the per-sample kernel filters inspired by this discussion:\n    https://github.com/pytorch/pytorch/issues/17983\n    """"""\n    __constants__ = [\'bias\', \'in_channels\', \'out_channels\', \'dynamic_padding\']\n\n    def __init__(self, in_channels, out_channels, kernel_size=3,\n                 stride=1, padding=\'\', dilation=1, groups=1, bias=False, num_experts=4):\n        super(CondConv2d, self).__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = tup_pair(kernel_size)\n        self.stride = tup_pair(stride)\n        padding_val, is_padding_dynamic = get_padding_value(\n            padding, kernel_size, stride=stride, dilation=dilation)\n        self.dynamic_padding = is_padding_dynamic  # if in forward to work with torchscript\n        self.padding = tup_pair(padding_val)\n        self.dilation = tup_pair(dilation)\n        self.groups = groups\n        self.num_experts = num_experts\n\n        self.weight_shape = (self.out_channels, self.in_channels // self.groups) + self.kernel_size\n        weight_num_param = 1\n        for wd in self.weight_shape:\n            weight_num_param *= wd\n        self.weight = torch.nn.Parameter(torch.Tensor(self.num_experts, weight_num_param))\n\n        if bias:\n            self.bias_shape = (self.out_channels,)\n            self.bias = torch.nn.Parameter(torch.Tensor(self.num_experts, self.out_channels))\n        else:\n            self.register_parameter(\'bias\', None)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        init_weight = get_condconv_initializer(\n            partial(nn.init.kaiming_uniform_, a=math.sqrt(5)), self.num_experts, self.weight_shape)\n        init_weight(self.weight)\n        if self.bias is not None:\n            fan_in = np.prod(self.weight_shape[1:])\n            bound = 1 / math.sqrt(fan_in)\n            init_bias = get_condconv_initializer(\n                partial(nn.init.uniform_, a=-bound, b=bound), self.num_experts, self.bias_shape)\n            init_bias(self.bias)\n\n    def forward(self, x, routing_weights):\n        B, C, H, W = x.shape\n        weight = torch.matmul(routing_weights, self.weight)\n        new_weight_shape = (B * self.out_channels, self.in_channels // self.groups) + self.kernel_size\n        weight = weight.view(new_weight_shape)\n        bias = None\n        if self.bias is not None:\n            bias = torch.matmul(routing_weights, self.bias)\n            bias = bias.view(B * self.out_channels)\n        # move batch elements with channels so each batch element can be efficiently convolved with separate kernel\n        x = x.view(1, B * C, H, W)\n        if self.dynamic_padding:\n            out = conv2d_same(\n                x, weight, bias, stride=self.stride, padding=self.padding,\n                dilation=self.dilation, groups=self.groups * B)\n        else:\n            out = F.conv2d(\n                x, weight, bias, stride=self.stride, padding=self.padding,\n                dilation=self.dilation, groups=self.groups * B)\n        out = out.permute([1, 0, 2, 3]).view(B, self.out_channels, out.shape[-2], out.shape[-1])\n\n        # Literal port (from TF definition)\n        # x = torch.split(x, 1, 0)\n        # weight = torch.split(weight, 1, 0)\n        # if self.bias is not None:\n        #     bias = torch.matmul(routing_weights, self.bias)\n        #     bias = torch.split(bias, 1, 0)\n        # else:\n        #     bias = [None] * B\n        # out = []\n        # for xi, wi, bi in zip(x, weight, bias):\n        #     wi = wi.view(*self.weight_shape)\n        #     if bi is not None:\n        #         bi = bi.view(*self.bias_shape)\n        #     out.append(self.conv_fn(\n        #         xi, wi, bi, stride=self.stride, padding=self.padding,\n        #         dilation=self.dilation, groups=self.groups))\n        # out = torch.cat(out, 0)\n        return out\n'"
timm/models/layers/conv2d_same.py,3,"b'"""""" Conv2d w/ Same Padding\n\nHacked together by Ross Wightman\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Tuple, Optional\n\nfrom timm.models.layers.padding import get_padding_value\nfrom .padding import pad_same\n\n\ndef conv2d_same(\n        x, weight: torch.Tensor, bias: Optional[torch.Tensor] = None, stride: Tuple[int, int] = (1, 1),\n        padding: Tuple[int, int] = (0, 0), dilation: Tuple[int, int] = (1, 1), groups: int = 1):\n    x = pad_same(x, weight.shape[-2:], stride, dilation)\n    return F.conv2d(x, weight, bias, stride, (0, 0), dilation, groups)\n\n\nclass Conv2dSame(nn.Conv2d):\n    """""" Tensorflow like \'SAME\' convolution wrapper for 2D convolutions\n    """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        super(Conv2dSame, self).__init__(\n            in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias)\n\n    def forward(self, x):\n        return conv2d_same(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n\n\ndef create_conv2d_pad(in_chs, out_chs, kernel_size, **kwargs):\n    padding = kwargs.pop(\'padding\', \'\')\n    kwargs.setdefault(\'bias\', False)\n    padding, is_dynamic = get_padding_value(padding, kernel_size, **kwargs)\n    if is_dynamic:\n        return Conv2dSame(in_chs, out_chs, kernel_size, **kwargs)\n    else:\n        return nn.Conv2d(in_chs, out_chs, kernel_size, padding=padding, **kwargs)\n\n\n'"
timm/models/layers/conv_bn_act.py,0,"b'"""""" Conv2d + BN + Act\n\nHacked together by Ross Wightman\n""""""\nfrom torch import nn as nn\n\nfrom timm.models.layers import get_padding\n\n\nclass ConvBnAct(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, dilation=1, groups=1,\n                 drop_block=None, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d, aa_layer=None):\n        super(ConvBnAct, self).__init__()\n        padding = get_padding(kernel_size, stride, dilation)  # assuming PyTorch style padding for this block\n        use_aa = aa_layer is not None\n        self.conv = nn.Conv2d(\n            in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=1 if use_aa else stride,\n            padding=padding, dilation=dilation, groups=groups, bias=False)\n        self.bn = norm_layer(out_channels)\n        self.aa = aa_layer(channels=out_channels) if stride == 2 and use_aa else None\n        self.drop_block = drop_block\n        if act_layer is not None:\n            self.act = act_layer(inplace=True)\n        else:\n            self.act = None\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        if self.drop_block is not None:\n            x = self.drop_block(x)\n        if self.act is not None:\n            x = self.act(x)\n        if self.aa is not None:\n            x = self.aa(x)\n        return x\n'"
timm/models/layers/create_attn.py,0,"b'"""""" Select AttentionFactory Method\n\nHacked together by Ross Wightman\n""""""\nimport torch\nfrom .se import SEModule\nfrom .eca import EcaModule, CecaModule\nfrom .cbam import CbamModule, LightCbamModule\n\n\ndef create_attn(attn_type, channels, **kwargs):\n    module_cls = None\n    if attn_type is not None:\n        if isinstance(attn_type, str):\n            attn_type = attn_type.lower()\n            if attn_type == \'se\':\n                module_cls = SEModule\n            elif attn_type == \'eca\':\n                module_cls = EcaModule\n            elif attn_type == \'ceca\':\n                module_cls = CecaModule\n            elif attn_type == \'cbam\':\n                module_cls = CbamModule\n            elif attn_type == \'lcbam\':\n                module_cls = LightCbamModule\n            else:\n                assert False, ""Invalid attn module (%s)"" % attn_type\n        elif isinstance(attn_type, bool):\n            if attn_type:\n                module_cls = SEModule\n        else:\n            module_cls = attn_type\n    if module_cls is not None:\n        return module_cls(channels, **kwargs)\n    return None\n'"
timm/models/layers/create_conv2d.py,1,"b'"""""" Create Conv2d Factory Method\n\nHacked together by Ross Wightman\n""""""\n\nfrom .mixed_conv2d import MixedConv2d\nfrom .cond_conv2d import CondConv2d\nfrom .conv2d_same import create_conv2d_pad\n\n\ndef create_conv2d(in_chs, out_chs, kernel_size, **kwargs):\n    """""" Select a 2d convolution implementation based on arguments\n    Creates and returns one of torch.nn.Conv2d, Conv2dSame, MixedConv2d, or CondConv2d.\n\n    Used extensively by EfficientNet, MobileNetv3 and related networks.\n    """"""\n    assert \'groups\' not in kwargs  # only use \'depthwise\' bool arg\n    if isinstance(kernel_size, list):\n        assert \'num_experts\' not in kwargs  # MixNet + CondConv combo not supported currently\n        # We\'re going to use only lists for defining the MixedConv2d kernel groups,\n        # ints, tuples, other iterables will continue to pass to normal conv and specify h, w.\n        m = MixedConv2d(in_chs, out_chs, kernel_size, **kwargs)\n    else:\n        depthwise = kwargs.pop(\'depthwise\', False)\n        groups = out_chs if depthwise else 1\n        if \'num_experts\' in kwargs and kwargs[\'num_experts\'] > 0:\n            m = CondConv2d(in_chs, out_chs, kernel_size, groups=groups, **kwargs)\n        else:\n            m = create_conv2d_pad(in_chs, out_chs, kernel_size, groups=groups, **kwargs)\n    return m\n'"
timm/models/layers/drop.py,14,"b'"""""" DropBlock, DropPath\n\nPyTorch implementations of DropBlock and DropPath (Stochastic Depth) regularization layers.\n\nPapers:\nDropBlock: A regularization method for convolutional networks (https://arxiv.org/abs/1810.12890)\n\nDeep Networks with Stochastic Depth (https://arxiv.org/abs/1603.09382)\n\nCode:\nDropBlock impl inspired by two Tensorflow impl that I liked:\n - https://github.com/tensorflow/tpu/blob/master/models/official/resnet/resnet_model.py#L74\n - https://github.com/clovaai/assembled-cnn/blob/master/nets/blocks.py\n\nHacked together by Ross Wightman\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport math\n\n\ndef drop_block_2d(\n        x, drop_prob: float = 0.1, block_size: int = 7,  gamma_scale: float = 1.0,\n        with_noise: bool = False, inplace: bool = False, batchwise: bool = False):\n    """""" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n\n    DropBlock with an experimental gaussian noise option. This layer has been tested on a few training\n    runs with success, but needs further validation and possibly optimization for lower runtime impact.\n    """"""\n    B, C, H, W = x.shape\n    total_size = W * H\n    clipped_block_size = min(block_size, min(W, H))\n    # seed_drop_rate, the gamma parameter\n    gamma = gamma_scale * drop_prob * total_size / clipped_block_size ** 2 / (\n        (W - block_size + 1) * (H - block_size + 1))\n\n    # Forces the block to be inside the feature map.\n    w_i, h_i = torch.meshgrid(torch.arange(W).to(x.device), torch.arange(H).to(x.device))\n    valid_block = ((w_i >= clipped_block_size // 2) & (w_i < W - (clipped_block_size - 1) // 2)) & \\\n                  ((h_i >= clipped_block_size // 2) & (h_i < H - (clipped_block_size - 1) // 2))\n    valid_block = torch.reshape(valid_block, (1, 1, H, W)).to(dtype=x.dtype)\n\n    if batchwise:\n        # one mask for whole batch, quite a bit faster\n        uniform_noise = torch.rand((1, C, H, W), dtype=x.dtype, device=x.device)\n    else:\n        uniform_noise = torch.rand_like(x)\n    block_mask = ((2 - gamma - valid_block + uniform_noise) >= 1).to(dtype=x.dtype)\n    block_mask = -F.max_pool2d(\n        -block_mask,\n        kernel_size=clipped_block_size,  # block_size,\n        stride=1,\n        padding=clipped_block_size // 2)\n\n    if with_noise:\n        normal_noise = torch.randn((1, C, H, W), dtype=x.dtype, device=x.device) if batchwise else torch.randn_like(x)\n        if inplace:\n            x.mul_(block_mask).add_(normal_noise * (1 - block_mask))\n        else:\n            x = x * block_mask + normal_noise * (1 - block_mask)\n    else:\n        normalize_scale = (block_mask.numel() / block_mask.to(dtype=torch.float32).sum().add(1e-7)).to(x.dtype)\n        if inplace:\n            x.mul_(block_mask * normalize_scale)\n        else:\n            x = x * block_mask * normalize_scale\n    return x\n\n\ndef drop_block_fast_2d(\n        x: torch.Tensor, drop_prob: float = 0.1, block_size: int = 7,\n        gamma_scale: float = 1.0, with_noise: bool = False, inplace: bool = False, batchwise: bool = False):\n    """""" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n\n    DropBlock with an experimental gaussian noise option. Simplied from above without concern for valid\n    block mask at edges.\n    """"""\n    B, C, H, W = x.shape\n    total_size = W * H\n    clipped_block_size = min(block_size, min(W, H))\n    gamma = gamma_scale * drop_prob * total_size / clipped_block_size ** 2 / (\n            (W - block_size + 1) * (H - block_size + 1))\n\n    if batchwise:\n        # one mask for whole batch, quite a bit faster\n        block_mask = torch.rand((1, C, H, W), dtype=x.dtype, device=x.device) < gamma\n    else:\n        # mask per batch element\n        block_mask = torch.rand_like(x) < gamma\n    block_mask = F.max_pool2d(\n        block_mask.to(x.dtype), kernel_size=clipped_block_size, stride=1, padding=clipped_block_size // 2)\n\n    if with_noise:\n        normal_noise = torch.randn((1, C, H, W), dtype=x.dtype, device=x.device) if batchwise else torch.randn_like(x)\n        if inplace:\n            x.mul_(1. - block_mask).add_(normal_noise * block_mask)\n        else:\n            x = x * (1. - block_mask) + normal_noise * block_mask\n    else:\n        block_mask = 1 - block_mask\n        normalize_scale = (block_mask.numel() / block_mask.to(dtype=torch.float32).sum().add(1e-7)).to(dtype=x.dtype)\n        if inplace:\n            x.mul_(block_mask * normalize_scale)\n        else:\n            x = x * block_mask * normalize_scale\n    return x\n\n\nclass DropBlock2d(nn.Module):\n    """""" DropBlock. See https://arxiv.org/pdf/1810.12890.pdf\n    """"""\n    def __init__(self,\n                 drop_prob=0.1,\n                 block_size=7,\n                 gamma_scale=1.0,\n                 with_noise=False,\n                 inplace=False,\n                 batchwise=False,\n                 fast=True):\n        super(DropBlock2d, self).__init__()\n        self.drop_prob = drop_prob\n        self.gamma_scale = gamma_scale\n        self.block_size = block_size\n        self.with_noise = with_noise\n        self.inplace = inplace\n        self.batchwise = batchwise\n        self.fast = fast  # FIXME finish comparisons of fast vs not\n\n    def forward(self, x):\n        if not self.training or not self.drop_prob:\n            return x\n        if self.fast:\n            return drop_block_fast_2d(\n                x, self.drop_prob, self.block_size, self.gamma_scale, self.with_noise, self.inplace, self.batchwise)\n        else:\n            return drop_block_2d(\n                x, self.drop_prob, self.block_size, self.gamma_scale, self.with_noise, self.inplace, self.batchwise)\n\n\ndef drop_path(x, drop_prob: float = 0., training: bool = False):\n    """"""Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n\n    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n    the original name is misleading as \'Drop Connect\' is a different form of dropout in a separate paper...\n    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I\'ve opted for\n    changing the layer and argument names to \'drop path\' rather than mix DropConnect as a layer name and use\n    \'survival rate\' as the argument.\n\n    """"""\n    if drop_prob == 0. or not training:\n        return x\n    keep_prob = 1 - drop_prob\n    random_tensor = keep_prob + torch.rand((x.size()[0], 1, 1, 1), dtype=x.dtype, device=x.device)\n    random_tensor.floor_()  # binarize\n    output = x.div(keep_prob) * random_tensor\n    return output\n\n\nclass DropPath(nn.ModuleDict):\n    """"""Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n    """"""\n    def __init__(self, drop_prob=None):\n        super(DropPath, self).__init__()\n        self.drop_prob = drop_prob\n\n    def forward(self, x):\n        return drop_path(x, self.drop_prob, self.training)\n'"
timm/models/layers/eca.py,1,"b'""""""\nECA module from ECAnet\n\npaper: ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks\nhttps://arxiv.org/abs/1910.03151\n\nOriginal ECA model borrowed from https://github.com/BangguWu/ECANet\n\nModified circular ECA implementation and adaption for use in timm package\nby Chris Ha https://github.com/VRandme\n\nOriginal License:\n\nMIT License\n\nCopyright (c) 2019 BangguWu, Qilong Wang\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the ""Software""), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n""""""\nimport math\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass EcaModule(nn.Module):\n    """"""Constructs an ECA module.\n\n    Args:\n        channels: Number of channels of the input feature map for use in adaptive kernel sizes\n            for actual calculations according to channel.\n            gamma, beta: when channel is given parameters of mapping function\n            refer to original paper https://arxiv.org/pdf/1910.03151.pdf\n            (default=None. if channel size not given, use k_size given for kernel size.)\n        kernel_size: Adaptive selection of kernel size (default=3)\n    """"""\n    def __init__(self, channels=None, kernel_size=3, gamma=2, beta=1):\n        super(EcaModule, self).__init__()\n        assert kernel_size % 2 == 1\n\n        if channels is not None:\n            t = int(abs(math.log(channels, 2) + beta) / gamma)\n            kernel_size = max(t if t % 2 else t + 1, 3)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False)\n\n    def forward(self, x):\n        # Feature descriptor on the global spatial information\n        y = self.avg_pool(x)\n        # Reshape for convolution\n        y = y.view(x.shape[0], 1, -1)\n        # Two different branches of ECA module\n        y = self.conv(y)\n        # Multi-scale information fusion\n        y = y.view(x.shape[0], -1, 1, 1).sigmoid()\n        return x * y.expand_as(x)\n\n\nclass CecaModule(nn.Module):\n    """"""Constructs a circular ECA module.\n\n    ECA module where the conv uses circular padding rather than zero padding.\n    Unlike the spatial dimension, the channels do not have inherent ordering nor\n    locality. Although this module in essence, applies such an assumption, it is unnecessary\n    to limit the channels on either ""edge"" from being circularly adapted to each other.\n    This will fundamentally increase connectivity and possibly increase performance metrics\n    (accuracy, robustness), without significantly impacting resource metrics\n    (parameter size, throughput,latency, etc)\n\n    Args:\n        channels: Number of channels of the input feature map for use in adaptive kernel sizes\n            for actual calculations according to channel.\n            gamma, beta: when channel is given parameters of mapping function\n            refer to original paper https://arxiv.org/pdf/1910.03151.pdf\n            (default=None. if channel size not given, use k_size given for kernel size.)\n        kernel_size: Adaptive selection of kernel size (default=3)\n    """"""\n\n    def __init__(self, channels=None, kernel_size=3, gamma=2, beta=1):\n        super(CecaModule, self).__init__()\n        assert kernel_size % 2 == 1\n\n        if channels is not None:\n            t = int(abs(math.log(channels, 2) + beta) / gamma)\n            kernel_size = max(t if t % 2 else t + 1, 3)\n\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        #pytorch circular padding mode is buggy as of pytorch 1.4\n        #see https://github.com/pytorch/pytorch/pull/17240\n\n        #implement manual circular padding\n        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=0, bias=False)\n        self.padding = (kernel_size - 1) // 2\n\n    def forward(self, x):\n        # Feature descriptor on the global spatial information\n        y = self.avg_pool(x)\n\n        # Manually implement circular padding, F.pad does not seemed to be bugged\n        y = F.pad(y.view(x.shape[0], 1, -1), (self.padding, self.padding), mode=\'circular\')\n\n        # Two different branches of ECA module\n        y = self.conv(y)\n\n        # Multi-scale information fusion\n        y = y.view(x.shape[0], -1, 1, 1).sigmoid()\n\n        return x * y.expand_as(x)\n'"
timm/models/layers/helpers.py,1,"b'"""""" Layer/Module Helpers\n\nHacked together by Ross Wightman\n""""""\nfrom itertools import repeat\nfrom torch._six import container_abcs\n\n\n# From PyTorch internals\ndef _ntuple(n):\n    def parse(x):\n        if isinstance(x, container_abcs.Iterable):\n            return x\n        return tuple(repeat(x, n))\n    return parse\n\n\ntup_single = _ntuple(1)\ntup_pair = _ntuple(2)\ntup_triple = _ntuple(3)\ntup_quadruple = _ntuple(4)\n\n\n\n\n\n\n'"
timm/models/layers/median_pool.py,3,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.utils import _pair, _quadruple\n\n\nclass MedianPool2d(nn.Module):\n    """""" Median pool (usable as median filter when stride=1) module.\n\n    Args:\n         kernel_size: size of pooling kernel, int or 2-tuple\n         stride: pool stride, int or 2-tuple\n         padding: pool padding, int or 4-tuple (l, r, t, b) as in pytorch F.pad\n         same: override padding and enforce same padding, boolean\n    """"""\n    def __init__(self, kernel_size=3, stride=1, padding=0, same=False):\n        super(MedianPool2d, self).__init__()\n        self.k = _pair(kernel_size)\n        self.stride = _pair(stride)\n        self.padding = _quadruple(padding)  # convert to l, r, t, b\n        self.same = same\n\n    def _padding(self, x):\n        if self.same:\n            ih, iw = x.size()[2:]\n            if ih % self.stride[0] == 0:\n                ph = max(self.k[0] - self.stride[0], 0)\n            else:\n                ph = max(self.k[0] - (ih % self.stride[0]), 0)\n            if iw % self.stride[1] == 0:\n                pw = max(self.k[1] - self.stride[1], 0)\n            else:\n                pw = max(self.k[1] - (iw % self.stride[1]), 0)\n            pl = pw // 2\n            pr = pw - pl\n            pt = ph // 2\n            pb = ph - pt\n            padding = (pl, pr, pt, pb)\n        else:\n            padding = self.padding\n        return padding\n\n    def forward(self, x):\n        x = F.pad(x, self._padding(x), mode=\'reflect\')\n        x = x.unfold(2, self.k[0], self.stride[0]).unfold(3, self.k[1], self.stride[1])\n        x = x.contiguous().view(x.size()[:4] + (-1,)).median(dim=-1)[0]\n        return x\n'"
timm/models/layers/mixed_conv2d.py,2,"b'"""""" PyTorch Mixed Convolution\n\nPaper: MixConv: Mixed Depthwise Convolutional Kernels (https://arxiv.org/abs/1907.09595)\n\nHacked together by Ross Wightman\n""""""\n\nimport torch\nfrom torch import nn as nn\n\nfrom .conv2d_same import create_conv2d_pad\n\n\ndef _split_channels(num_chan, num_groups):\n    split = [num_chan // num_groups for _ in range(num_groups)]\n    split[0] += num_chan - sum(split)\n    return split\n\n\nclass MixedConv2d(nn.ModuleDict):\n    """""" Mixed Grouped Convolution\n\n    Based on MDConv and GroupedConv in MixNet impl:\n      https://github.com/tensorflow/tpu/blob/master/models/official/mnasnet/mixnet/custom_layers.py\n    """"""\n    def __init__(self, in_channels, out_channels, kernel_size=3,\n                 stride=1, padding=\'\', dilation=1, depthwise=False, **kwargs):\n        super(MixedConv2d, self).__init__()\n\n        kernel_size = kernel_size if isinstance(kernel_size, list) else [kernel_size]\n        num_groups = len(kernel_size)\n        in_splits = _split_channels(in_channels, num_groups)\n        out_splits = _split_channels(out_channels, num_groups)\n        self.in_channels = sum(in_splits)\n        self.out_channels = sum(out_splits)\n        for idx, (k, in_ch, out_ch) in enumerate(zip(kernel_size, in_splits, out_splits)):\n            conv_groups = out_ch if depthwise else 1\n            # use add_module to keep key space clean\n            self.add_module(\n                str(idx),\n                create_conv2d_pad(\n                    in_ch, out_ch, k, stride=stride,\n                    padding=padding, dilation=dilation, groups=conv_groups, **kwargs)\n            )\n        self.splits = in_splits\n\n    def forward(self, x):\n        x_split = torch.split(x, self.splits, 1)\n        x_out = [c(x_split[i]) for i, c in enumerate(self.values())]\n        x = torch.cat(x_out, 1)\n        return x\n'"
timm/models/layers/padding.py,1,"b'"""""" Padding Helpers\n\nHacked together by Ross Wightman\n""""""\nimport math\nfrom typing import List, Tuple\n\nimport torch.nn.functional as F\n\n\n# Calculate symmetric padding for a convolution\ndef get_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> int:\n    padding = ((stride - 1) + dilation * (kernel_size - 1)) // 2\n    return padding\n\n\n# Calculate asymmetric TensorFlow-like \'SAME\' padding for a convolution\ndef get_same_padding(x: int, k: int, s: int, d: int):\n    return max((math.ceil(x / s) - 1) * s + (k - 1) * d + 1 - x, 0)\n\n\n# Can SAME padding for given args be done statically?\ndef is_static_pad(kernel_size: int, stride: int = 1, dilation: int = 1, **_):\n    return stride == 1 and (dilation * (kernel_size - 1)) % 2 == 0\n\n\n# Dynamically pad input x with \'SAME\' padding for conv with specified args\ndef pad_same(x, k: List[int], s: List[int], d: List[int] = (1, 1), value: float = 0):\n    ih, iw = x.size()[-2:]\n    pad_h, pad_w = get_same_padding(ih, k[0], s[0], d[0]), get_same_padding(iw, k[1], s[1], d[1])\n    if pad_h > 0 or pad_w > 0:\n        x = F.pad(x, [pad_w // 2, pad_w - pad_w // 2, pad_h // 2, pad_h - pad_h // 2], value=value)\n    return x\n\n\ndef get_padding_value(padding, kernel_size, **kwargs) -> Tuple[Tuple, bool]:\n    dynamic = False\n    if isinstance(padding, str):\n        # for any string padding, the padding will be calculated for you, one of three ways\n        padding = padding.lower()\n        if padding == \'same\':\n            # TF compatible \'SAME\' padding, has a performance and GPU memory allocation impact\n            if is_static_pad(kernel_size, **kwargs):\n                # static case, no extra overhead\n                padding = get_padding(kernel_size, **kwargs)\n            else:\n                # dynamic \'SAME\' padding, has runtime/GPU memory overhead\n                padding = 0\n                dynamic = True\n        elif padding == \'valid\':\n            # \'VALID\' padding, same as padding=0\n            padding = 0\n        else:\n            # Default to PyTorch style \'same\'-ish symmetric padding\n            padding = get_padding(kernel_size, **kwargs)\n    return padding, dynamic\n'"
timm/models/layers/pool2d_same.py,2,"b'"""""" AvgPool2d w/ Same Padding\n\nHacked together by Ross Wightman\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom typing import Union, List, Tuple, Optional\nimport math\n\nfrom .helpers import tup_pair\nfrom .padding import pad_same, get_padding_value\n\n\ndef avg_pool2d_same(x, kernel_size: List[int], stride: List[int], padding: List[int] = (0, 0),\n                    ceil_mode: bool = False, count_include_pad: bool = True):\n    # FIXME how to deal with count_include_pad vs not for external padding?\n    x = pad_same(x, kernel_size, stride)\n    return F.avg_pool2d(x, kernel_size, stride, (0, 0), ceil_mode, count_include_pad)\n\n\nclass AvgPool2dSame(nn.AvgPool2d):\n    """""" Tensorflow like \'SAME\' wrapper for 2D average pooling\n    """"""\n    def __init__(self, kernel_size: int, stride=None, padding=0, ceil_mode=False, count_include_pad=True):\n        kernel_size = tup_pair(kernel_size)\n        stride = tup_pair(stride)\n        super(AvgPool2dSame, self).__init__(kernel_size, stride, (0, 0), ceil_mode, count_include_pad)\n\n    def forward(self, x):\n        return avg_pool2d_same(\n            x, self.kernel_size, self.stride, self.padding, self.ceil_mode, self.count_include_pad)\n\n\ndef max_pool2d_same(\n        x, kernel_size: List[int], stride: List[int], padding: List[int] = (0, 0),\n        dilation: List[int] = (1, 1), ceil_mode: bool = False):\n    x = pad_same(x, kernel_size, stride, value=-float(\'inf\'))\n    return F.max_pool2d(x, kernel_size, stride, (0, 0), dilation, ceil_mode)\n\n\nclass MaxPool2dSame(nn.MaxPool2d):\n    """""" Tensorflow like \'SAME\' wrapper for 2D max pooling\n    """"""\n    def __init__(self, kernel_size: int, stride=None, padding=0, dilation=1, ceil_mode=False, count_include_pad=True):\n        kernel_size = tup_pair(kernel_size)\n        stride = tup_pair(stride)\n        super(MaxPool2dSame, self).__init__(kernel_size, stride, (0, 0), dilation, ceil_mode, count_include_pad)\n\n    def forward(self, x):\n        return max_pool2d_same(x, self.kernel_size, self.stride, self.padding, self.dilation, self.ceil_mode)\n\n\ndef create_pool2d(pool_type, kernel_size, stride=None, **kwargs):\n    stride = stride or kernel_size\n    padding = kwargs.pop(\'padding\', \'\')\n    padding, is_dynamic = get_padding_value(padding, kernel_size, stride=stride, **kwargs)\n    if is_dynamic:\n        if pool_type == \'avg\':\n            return AvgPool2dSame(kernel_size, stride=stride, **kwargs)\n        elif pool_type == \'max\':\n            return MaxPool2dSame(kernel_size, stride=stride, **kwargs)\n        else:\n            assert False, f\'Unsupported pool type {pool_type}\'\n    else:\n        if pool_type == \'avg\':\n            return nn.AvgPool2d(kernel_size, stride=stride, padding=padding, **kwargs)\n        elif pool_type == \'max\':\n            return nn.MaxPool2d(kernel_size, stride=stride, padding=padding, **kwargs)\n        else:\n            assert False, f\'Unsupported pool type {pool_type}\'\n'"
timm/models/layers/se.py,0,"b'from torch import nn as nn\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction=16, act_layer=nn.ReLU, min_channels=8, reduction_channels=None):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        reduction_channels = reduction_channels or max(channels // reduction, min_channels)\n        self.fc1 = nn.Conv2d(\n            channels, reduction_channels, kernel_size=1, padding=0, bias=True)\n        self.act = act_layer(inplace=True)\n        self.fc2 = nn.Conv2d(\n            reduction_channels, channels, kernel_size=1, padding=0, bias=True)\n\n    def forward(self, x):\n        x_se = self.avg_pool(x)\n        x_se = self.fc1(x_se)\n        x_se = self.act(x_se)\n        x_se = self.fc2(x_se)\n        return x * x_se.sigmoid()\n'"
timm/models/layers/selective_kernel.py,5,"b'"""""" Selective Kernel Convolution/Attention\n\nPaper: Selective Kernel Networks (https://arxiv.org/abs/1903.06586)\n\nHacked together by Ross Wightman\n""""""\n\nimport torch\nfrom torch import nn as nn\n\nfrom .conv_bn_act import ConvBnAct\n\n\ndef _kernel_valid(k):\n    if isinstance(k, (list, tuple)):\n        for ki in k:\n            return _kernel_valid(ki)\n    assert k >= 3 and k % 2\n\n\nclass SelectiveKernelAttn(nn.Module):\n    def __init__(self, channels, num_paths=2, attn_channels=32,\n                 act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d):\n        """""" Selective Kernel Attention Module\n\n        Selective Kernel attention mechanism factored out into its own module.\n\n        """"""\n        super(SelectiveKernelAttn, self).__init__()\n        self.num_paths = num_paths\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.fc_reduce = nn.Conv2d(channels, attn_channels, kernel_size=1, bias=False)\n        self.bn = norm_layer(attn_channels)\n        self.act = act_layer(inplace=True)\n        self.fc_select = nn.Conv2d(attn_channels, channels * num_paths, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        assert x.shape[1] == self.num_paths\n        x = torch.sum(x, dim=1)\n        x = self.pool(x)\n        x = self.fc_reduce(x)\n        x = self.bn(x)\n        x = self.act(x)\n        x = self.fc_select(x)\n        B, C, H, W = x.shape\n        x = x.view(B, self.num_paths, C // self.num_paths, H, W)\n        x = torch.softmax(x, dim=1)\n        return x\n\n\nclass SelectiveKernelConv(nn.Module):\n\n    def __init__(self, in_channels, out_channels, kernel_size=None, stride=1, dilation=1, groups=1,\n                 attn_reduction=16, min_attn_channels=32, keep_3x3=True, split_input=False,\n                 drop_block=None, act_layer=nn.ReLU, norm_layer=nn.BatchNorm2d, aa_layer=None):\n        """""" Selective Kernel Convolution Module\n\n        As described in Selective Kernel Networks (https://arxiv.org/abs/1903.06586) with some modifications.\n\n        Largest change is the input split, which divides the input channels across each convolution path, this can\n        be viewed as a grouping of sorts, but the output channel counts expand to the module level value. This keeps\n        the parameter count from ballooning when the convolutions themselves don\'t have groups, but still provides\n        a noteworthy increase in performance over similar param count models without this attention layer. -Ross W\n\n        Args:\n            in_channels (int):  module input (feature) channel count\n            out_channels (int):  module output (feature) channel count\n            kernel_size (int, list): kernel size for each convolution branch\n            stride (int): stride for convolutions\n            dilation (int): dilation for module as a whole, impacts dilation of each branch\n            groups (int): number of groups for each branch\n            attn_reduction (int, float): reduction factor for attention features\n            min_attn_channels (int): minimum attention feature channels\n            keep_3x3 (bool): keep all branch convolution kernels as 3x3, changing larger kernels for dilations\n            split_input (bool): split input channels evenly across each convolution branch, keeps param count lower,\n                can be viewed as grouping by path, output expands to module out_channels count\n            drop_block (nn.Module): drop block module\n            act_layer (nn.Module): activation layer to use\n            norm_layer (nn.Module): batchnorm/norm layer to use\n        """"""\n        super(SelectiveKernelConv, self).__init__()\n        kernel_size = kernel_size or [3, 5]  # default to one 3x3 and one 5x5 branch. 5x5 -> 3x3 + dilation\n        _kernel_valid(kernel_size)\n        if not isinstance(kernel_size, list):\n            kernel_size = [kernel_size] * 2\n        if keep_3x3:\n            dilation = [dilation * (k - 1) // 2 for k in kernel_size]\n            kernel_size = [3] * len(kernel_size)\n        else:\n            dilation = [dilation] * len(kernel_size)\n        self.num_paths = len(kernel_size)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.split_input = split_input\n        if self.split_input:\n            assert in_channels % self.num_paths == 0\n            in_channels = in_channels // self.num_paths\n        groups = min(out_channels, groups)\n\n        conv_kwargs = dict(\n            stride=stride, groups=groups, drop_block=drop_block, act_layer=act_layer, norm_layer=norm_layer,\n            aa_layer=aa_layer)\n        self.paths = nn.ModuleList([\n            ConvBnAct(in_channels, out_channels, kernel_size=k, dilation=d, **conv_kwargs)\n            for k, d in zip(kernel_size, dilation)])\n\n        attn_channels = max(int(out_channels / attn_reduction), min_attn_channels)\n        self.attn = SelectiveKernelAttn(out_channels, self.num_paths, attn_channels)\n        self.drop_block = drop_block\n\n    def forward(self, x):\n        if self.split_input:\n            x_split = torch.split(x, self.in_channels // self.num_paths, 1)\n            x_paths = [op(x_split[i]) for i, op in enumerate(self.paths)]\n        else:\n            x_paths = [op(x) for op in self.paths]\n        x = torch.stack(x_paths, dim=1)\n        x_attn = self.attn(x)\n        x = x * x_attn\n        x = torch.sum(x, dim=1)\n        return x\n'"
timm/models/layers/space_to_depth.py,3,"b'import torch\nimport torch.nn as nn\n\n\nclass SpaceToDepth(nn.Module):\n    def __init__(self, block_size=4):\n        super().__init__()\n        assert block_size == 4\n        self.bs = block_size\n\n    def forward(self, x):\n        N, C, H, W = x.size()\n        x = x.view(N, C, H // self.bs, self.bs, W // self.bs, self.bs)  # (N, C, H//bs, bs, W//bs, bs)\n        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\n        x = x.view(N, C * (self.bs ** 2), H // self.bs, W // self.bs)  # (N, C*bs^2, H//bs, W//bs)\n        return x\n\n\n@torch.jit.script\nclass SpaceToDepthJit(object):\n    def __call__(self, x: torch.Tensor):\n        # assuming hard-coded that block_size==4 for acceleration\n        N, C, H, W = x.size()\n        x = x.view(N, C, H // 4, 4, W // 4, 4)  # (N, C, H//bs, bs, W//bs, bs)\n        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\n        x = x.view(N, C * 16, H // 4, W // 4)  # (N, C*bs^2, H//bs, W//bs)\n        return x\n\n\nclass SpaceToDepthModule(nn.Module):\n    def __init__(self, no_jit=False):\n        super().__init__()\n        if not no_jit:\n            self.op = SpaceToDepthJit()\n        else:\n            self.op = SpaceToDepth()\n\n    def forward(self, x):\n        return self.op(x)\n\n\nclass DepthToSpace(nn.Module):\n\n    def __init__(self, block_size):\n        super().__init__()\n        self.bs = block_size\n\n    def forward(self, x):\n        N, C, H, W = x.size()\n        x = x.view(N, self.bs, self.bs, C // (self.bs ** 2), H, W)  # (N, bs, bs, C//bs^2, H, W)\n        x = x.permute(0, 3, 4, 1, 5, 2).contiguous()  # (N, C//bs^2, H, bs, W, bs)\n        x = x.view(N, C // (self.bs ** 2), H * self.bs, W * self.bs)  # (N, C//bs^2, H * bs, W * bs)\n        return x\n'"
timm/models/layers/split_attn.py,2,"b'"""""" Split Attention Conv2d (for ResNeSt Models)\n\nPaper: `ResNeSt: Split-Attention Networks` - /https://arxiv.org/abs/2004.08955\n\nAdapted from original PyTorch impl at https://github.com/zhanghang1989/ResNeSt\n\nModified for torchscript compat, performance, and consistency with timm by Ross Wightman\n""""""\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass RadixSoftmax(nn.Module):\n    def __init__(self, radix, cardinality):\n        super(RadixSoftmax, self).__init__()\n        self.radix = radix\n        self.cardinality = cardinality\n\n    def forward(self, x):\n        batch = x.size(0)\n        if self.radix > 1:\n            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n            x = F.softmax(x, dim=1)\n            x = x.reshape(batch, -1)\n        else:\n            x = torch.sigmoid(x)\n        return x\n\n\nclass SplitAttnConv2d(nn.Module):\n    """"""Split-Attention Conv2d\n    """"""\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0,\n                 dilation=1, groups=1, bias=False, radix=2, reduction_factor=4,\n                 act_layer=nn.ReLU, norm_layer=None, drop_block=None, **kwargs):\n        super(SplitAttnConv2d, self).__init__()\n        self.radix = radix\n        self.drop_block = drop_block\n        mid_chs = out_channels * radix\n        attn_chs = max(in_channels * radix // reduction_factor, 32)\n\n        self.conv = nn.Conv2d(\n            in_channels, mid_chs, kernel_size, stride, padding, dilation,\n            groups=groups * radix, bias=bias, **kwargs)\n        self.bn0 = norm_layer(mid_chs) if norm_layer is not None else None\n        self.act0 = act_layer(inplace=True)\n        self.fc1 = nn.Conv2d(out_channels, attn_chs, 1, groups=groups)\n        self.bn1 = norm_layer(attn_chs) if norm_layer is not None else None\n        self.act1 = act_layer(inplace=True)\n        self.fc2 = nn.Conv2d(attn_chs, mid_chs, 1, groups=groups)\n        self.rsoftmax = RadixSoftmax(radix, groups)\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn0 is not None:\n            x = self.bn0(x)\n        if self.drop_block is not None:\n            x = self.drop_block(x)\n        x = self.act0(x)\n\n        B, RC, H, W = x.shape\n        if self.radix > 1:\n            x = x.reshape((B, self.radix, RC // self.radix, H, W))\n            x_gap = x.sum(dim=1)\n        else:\n            x_gap = x\n        x_gap = F.adaptive_avg_pool2d(x_gap, 1)\n        x_gap = self.fc1(x_gap)\n        if self.bn1 is not None:\n            x_gap = self.bn1(x_gap)\n        x_gap = self.act1(x_gap)\n        x_attn = self.fc2(x_gap)\n\n        x_attn = self.rsoftmax(x_attn).view(B, -1, 1, 1)\n        if self.radix > 1:\n            out = (x * x_attn.reshape((B, self.radix, RC // self.radix, 1, 1))).sum(dim=1)\n        else:\n            out = x * x_attn\n        return out.contiguous()\n'"
timm/models/layers/split_batchnorm.py,9,"b'"""""" Split BatchNorm\n\nA PyTorch BatchNorm layer that splits input batch into N equal parts and passes each through\na separate BN layer. The first split is passed through the parent BN layers with weight/bias\nkeys the same as the original BN. All other splits pass through BN sub-layers under the \'.aux_bn\'\nnamespace.\n\nThis allows easily removing the auxiliary BN layers after training to efficiently\nachieve the \'Auxiliary BatchNorm\' as described in the AdvProp Paper, section 4.2,\n\'Disentangled Learning via An Auxiliary BN\'\n\nHacked together by Ross Wightman\n""""""\nimport torch\nimport torch.nn as nn\n\n\nclass SplitBatchNorm2d(torch.nn.BatchNorm2d):\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n                 track_running_stats=True, num_splits=2):\n        super().__init__(num_features, eps, momentum, affine, track_running_stats)\n        assert num_splits > 1, \'Should have at least one aux BN layer (num_splits at least 2)\'\n        self.num_splits = num_splits\n        self.aux_bn = nn.ModuleList([\n            nn.BatchNorm2d(num_features, eps, momentum, affine, track_running_stats) for _ in range(num_splits - 1)])\n\n    def forward(self, input: torch.Tensor):\n        if self.training:  # aux BN only relevant while training\n            split_size = input.shape[0] // self.num_splits\n            assert input.shape[0] == split_size * self.num_splits, ""batch size must be evenly divisible by num_splits""\n            split_input = input.split(split_size)\n            x = [super().forward(split_input[0])]\n            for i, a in enumerate(self.aux_bn):\n                x.append(a(split_input[i + 1]))\n            return torch.cat(x, dim=0)\n        else:\n            return super().forward(input)\n\n\ndef convert_splitbn_model(module, num_splits=2):\n    """"""\n    Recursively traverse module and its children to replace all instances of\n    ``torch.nn.modules.batchnorm._BatchNorm`` with `SplitBatchnorm2d`.\n    Args:\n        module (torch.nn.Module): input module\n        num_splits: number of separate batchnorm layers to split input across\n    Example::\n        >>> # model is an instance of torch.nn.Module\n        >>> model = timm.models.convert_splitbn_model(model, num_splits=2)\n    """"""\n    mod = module\n    if isinstance(module, torch.nn.modules.instancenorm._InstanceNorm):\n        return module\n    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n        mod = SplitBatchNorm2d(\n            module.num_features, module.eps, module.momentum, module.affine,\n            module.track_running_stats, num_splits=num_splits)\n        mod.running_mean = module.running_mean\n        mod.running_var = module.running_var\n        mod.num_batches_tracked = module.num_batches_tracked\n        if module.affine:\n            mod.weight.data = module.weight.data.clone().detach()\n            mod.bias.data = module.bias.data.clone().detach()\n        for aux in mod.aux_bn:\n            aux.running_mean = module.running_mean.clone()\n            aux.running_var = module.running_var.clone()\n            aux.num_batches_tracked = module.num_batches_tracked.clone()\n            if module.affine:\n                aux.weight.data = module.weight.data.clone().detach()\n                aux.bias.data = module.bias.data.clone().detach()\n    for name, child in module.named_children():\n        mod.add_module(name, convert_splitbn_model(child, num_splits=num_splits))\n    del module\n    return mod\n'"
timm/models/layers/test_time_pool.py,1,"b'"""""" Test Time Pooling (Average-Max Pool)\n\nHacked together by Ross Wightman\n""""""\n\nimport logging\nfrom torch import nn\nimport torch.nn.functional as F\nfrom .adaptive_avgmax_pool import adaptive_avgmax_pool2d\n\n\nclass TestTimePoolHead(nn.Module):\n    def __init__(self, base, original_pool=7):\n        super(TestTimePoolHead, self).__init__()\n        self.base = base\n        self.original_pool = original_pool\n        base_fc = self.base.get_classifier()\n        if isinstance(base_fc, nn.Conv2d):\n            self.fc = base_fc\n        else:\n            self.fc = nn.Conv2d(\n                self.base.num_features, self.base.num_classes, kernel_size=1, bias=True)\n            self.fc.weight.data.copy_(base_fc.weight.data.view(self.fc.weight.size()))\n            self.fc.bias.data.copy_(base_fc.bias.data.view(self.fc.bias.size()))\n        self.base.reset_classifier(0)  # delete original fc layer\n\n    def forward(self, x):\n        x = self.base.forward_features(x)\n        x = F.avg_pool2d(x, kernel_size=self.original_pool, stride=1)\n        x = self.fc(x)\n        x = adaptive_avgmax_pool2d(x, 1)\n        return x.view(x.size(0), -1)\n\n\ndef apply_test_time_pool(model, config, args):\n    test_time_pool = False\n    if not hasattr(model, \'default_cfg\') or not model.default_cfg:\n        return model, False\n    if not args.no_test_pool and \\\n            config[\'input_size\'][-1] > model.default_cfg[\'input_size\'][-1] and \\\n            config[\'input_size\'][-2] > model.default_cfg[\'input_size\'][-2]:\n        logging.info(\'Target input size %s > pretrained default %s, using test time pooling\' %\n                     (str(config[\'input_size\'][-2:]), str(model.default_cfg[\'input_size\'][-2:])))\n        model = TestTimePoolHead(model, original_pool=model.default_cfg[\'pool_size\'])\n        test_time_pool = True\n    return model, test_time_pool\n'"
timm/models/layers/weight_init.py,3,"b'import torch\nimport math\nimport warnings\n\n\ndef _no_grad_trunc_normal_(tensor, mean, std, a, b):\n    # Cut & paste from PyTorch official master until it\'s in a few official releases - RW\n    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n    def norm_cdf(x):\n        # Computes standard normal cumulative distribution function\n        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\n    if (mean < a - 2 * std) or (mean > b + 2 * std):\n        warnings.warn(""mean is more than 2 std from [a, b] in nn.init.trunc_normal_. ""\n                      ""The distribution of values may be incorrect."",\n                      stacklevel=2)\n\n    with torch.no_grad():\n        # Values are generated by using a truncated uniform distribution and\n        # then using the inverse CDF for the normal distribution.\n        # Get upper and lower cdf values\n        l = norm_cdf((a - mean) / std)\n        u = norm_cdf((b - mean) / std)\n\n        # Uniformly fill tensor with values from [l, u], then translate to\n        # [2l-1, 2u-1].\n        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\n        # Use inverse cdf transform for normal distribution to get truncated\n        # standard normal\n        tensor.erfinv_()\n\n        # Transform to proper mean, std\n        tensor.mul_(std * math.sqrt(2.))\n        tensor.add_(mean)\n\n        # Clamp to ensure it\'s in the proper range\n        tensor.clamp_(min=a, max=b)\n        return tensor\n\n\ndef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n    # type: (Tensor, float, float, float, float) -> Tensor\n    r""""""Fills the input Tensor with values drawn from a truncated\n    normal distribution. The values are effectively drawn from the\n    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n    with values outside :math:`[a, b]` redrawn until they are within\n    the bounds. The method used for generating the random values works\n    best when :math:`a \\leq \\text{mean} \\leq b`.\n    Args:\n        tensor: an n-dimensional `torch.Tensor`\n        mean: the mean of the normal distribution\n        std: the standard deviation of the normal distribution\n        a: the minimum cutoff value\n        b: the maximum cutoff value\n    Examples:\n        >>> w = torch.empty(3, 5)\n        >>> nn.init.trunc_normal_(w)\n    """"""\n    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n'"
