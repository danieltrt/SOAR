file_path,api_count,code
__init__.py,0,b''
app.py,3,"b'import torch\nimport torch.nn as nn \nimport torch.backends.cudnn as cudnn\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport skimage\nimport argparse\n\n\nimport flask \nfrom flask import Flask, request, render_template \n\nfrom skimage import io \nimport numpy as np \nimport json \nimport zipfile\nfrom collections import OrderedDict\n\nimport text_reco.models.craft.craft_utils as craft_utils\nimport text_reco.models.craft.imgproc as img_proc\n\nfrom text_reco.models.craft.craft import CRAFT\nfrom text_reco.models.craft.craft_reader import CraftReader\nfrom text_reco.boxdetect.box_detection import BoxDetect\nfrom text_reco.models.crnn.crnn_run import CRNNReader\n\n#def build_args():\n#    parser = argparse.ArgumentParser()\n#    parser.add_argument(\'--infile\', type = str, help = \'dataset to preprocess\')\n#    args = parser.parse_args()\n#   return args\napp = Flask(__name__)\n\n@app.route(""/"")\n@app.route(""/index"")\ndef index():\n    return flask.render_template(\'index.html\')\n\n@app.route(\'/predict\', methods=[\'POST\'])\ndef make_prediction():\n    if request.method==\'POST\':\n        file_=request.files[\'image\']\n        if not file_:\n            return render_template(\'index.html\', label = ""No file"")\n        crr = CraftReader(file_)\n        boxes, img_res = crr.boxes_detect()\n        results = {}\n        for _, tmp_box in enumerate(boxes):\n            x = int(tmp_box[0][0])\n            y = int(tmp_box[0][1])\n            w = int(np.abs(tmp_box[0][0] - tmp_box[1][0]))\n            h = int(np.abs(tmp_box[0][1] - tmp_box[2][1]))\n            tmp_img =  img_res[y:y+h, x:x+w]\n            tmp_img = Image.fromarray(tmp_img.astype(\'uint8\')).convert(\'L\')\n            tmp_img = crnn.transformer(tmp_img)\n            tmp_img = tmp_img.view(1, *tmp_img.size())\n            tmp_img = Variable(tmp_img)\n            results[\'{}\'.format(_)] = crnn.get_predictions(tmp_img)\n        return render_template(\'index.html\', label = results)\n\nif __name__ == ""__main__"":\n    crnn=CRNNReader()\n\n    app.run(host=\'0.0.0.0\', port=8000, debug = True)\n'"
base/__init__.py,0,b''
base/watchdog.py,0,"b'import redis \n\nclass WatchDogBase():\n    def __init__(self, host = \'localhost\', port = 6379, db = 13):\n        self.host = host\n        self.port = port\n        self.db = db \n\n    def create_base(self):\n        r = redis.StrictRedis(db = self.db)\n        return r\n\n    def update_base(self):\n        pass\n\n    def update_table(self, records):\n        pass\n\n\n    def flush_all(self):\n        r.flushall()\n\n    def flush_db(self):\n        r.flushdb()\n\n\ndef main():\n\n    wb = WatchDogBase()\n    base = wb.create_base()\n\n\n\nif __name__ == ""__main__"":\n    main()\n'"
text_reco/__init__.py,0,b''
text_reco/boxdetect/__init__.py,0,b''
text_reco/boxdetect/box_detection.py,0,"b'# Simple logic \n# box on input - preprocess - sliced image on output\nimport cv2\nimport numpy as np \nclass BoxDetect():\n\n    def __init__(self, boxes):\n        self.boxes = boxes\n        self.n_boxes = len(self.boxes)\n\n    def preprocess(self, image):\n        img_storage = dict()\n        print(""To sa boxy"")\n        print(self.boxes)\n        for el in self.boxes:\n            tmp_img = cv2.rectangle(image, (el[0], el[1]), (el[2], el[3]))\n    \n    @staticmethod\n    def load_box(path):\n        with open(path, \'r\') as outfile:\n            file_ = json.load(outfile)\n        return file_\n\n    @staticmethod \n    def preprocess_box(file_, img):\n        for el in file_.keys():\n            x,y,w,h = cv2.boundingRect(np.array(file_[el]))\n            roi = img[x:x+w, y:y+h]\n            cv2.imshow(\'image\', roi)\n            cv2.waitKey(0)\n\n\n\n'"
text_reco/models/__init__.py,0,b''
build/lib/base/__init__.py,0,b''
build/lib/base/watchdog.py,0,"b'import redis \n\nclass WatchDogBase():\n    def __init__(self, host = \'localhost\', port = 6379, db = 13):\n        self.host = host\n        self.port = port\n        self.db = db \n\n    def create_base(self):\n        r = redis.StrictRedis(db = self.db)\n        return r\n\n    def update_base(self):\n        pass\n\n    def update_table(self, records):\n        pass\n\n\n    def flush_all(self):\n        r.flushall()\n\n    def flush_db(self):\n        r.flushdb()\n\n\ndef main():\n\n    wb = WatchDogBase()\n    base = wb.create_base()\n\n\n\nif __name__ == ""__main__"":\n    main()\n'"
build/lib/text_reco/__init__.py,0,b''
build/lib/text_reco/box_detection.py,0,b'import numpy as np \nimport \n'
text_reco/models/craft/__init__.py,0,b''
text_reco/models/craft/craft.py,6,"b""import torch\nimport torch.nn as nn \nimport torch.nn.functional as F\nfrom text_reco.models.craft.basenet.vgg16_bn import vgg16_bn, init_weights\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channel, mid_channel, out_channel):\n        super(DoubleConv, self).__init__()\n\n        self.conv = nn.Sequential(\n                nn.Conv2d(in_channel + mid_channel, mid_channel,  kernel_size = 1), \n                nn.BatchNorm2d(mid_channel), \n                nn.ReLU(inplace=True), \n                nn.Conv2d(mid_channel, out_channel, kernel_size = 3, padding = 1), \n                nn.BatchNorm2d(out_channel), \n                nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\nclass CRAFT(nn.Module):\n    def __init__(self, pretrained=False, freeze=False):\n        super(CRAFT, self).__init__()\n        self.basenet = vgg16_bn(pretrained, freeze)\n        self.upconv1 = DoubleConv(1024, 512, 256)\n        self.upconv2 = DoubleConv(512, 256, 128)\n        self.upconv3 = DoubleConv(256, 128, 64)\n        self.upconv4 = DoubleConv(128, 64, 32)\n\n        n_classes = 2\n        self.conv_cls = nn.Sequential(\n                nn.Conv2d(32, 32, 3, 1), \n                nn.ReLU(inplace=True), \n                nn.Conv2d(32, 32, 3, 1), \n                nn.ReLU(inplace=True), \n                nn.Conv2d(32, 16, 3, 1), \n                nn.ReLU(inplace=True), \n                nn.Conv2d(16, 16, 1), \n                nn.ReLU(inplace=True), \n                nn.Conv2d(16, n_classes, kernel_size=1),)\n\n        init_weights(self.upconv1.modules())\n        init_weights(self.upconv2.modules())\n        init_weights(self.upconv3.modules())\n        init_weights(self.upconv4.modules())\n        init_weights(self.conv_cls.modules())\n\n    def forward(self, x):\n        sources = self.basenet(x)\n        y = torch.cat([sources[0], sources[1]], dim=1)\n        y = self.upconv1(y)\n\n        y = F.interpolate(y, size = sources[2].size()[2:], mode = 'bilinear', align_corners=False)\n        y = torch.cat([y, sources[2]], dim=1)\n        y = self.upconv2(y)\n\n        y = F.interpolate(y, size = sources[3].size()[2:], mode = 'bilinear', align_corners=False)\n        y = torch.cat([y, sources[3]], dim=1)\n        y = self.upconv3(y)\n\n        y = F.interpolate(y, size = sources[4].size()[2:], mode = 'bilinear', align_corners=False)\n        y = torch.cat([y, sources[4]], dim =1)\n        feature = self.upconv4(y)\n        y = self.conv_cls(feature)\n\n        return y.permute(0, 2, 3, 1), feature\n\n"""
text_reco/models/craft/craft_reader.py,5,"b'import torch\nimport torch.nn as nn \nimport torch.backends.cudnn as cudnn\nfrom  torch.autograd import Variable\nimport text_reco.models.craft.craft_utils as craft_utils\nimport text_reco.models.craft.imgproc as img_proc\nfrom text_reco.models.craft.craft import CRAFT\nfrom text_reco.boxdetect.box_detection import BoxDetect\nfrom text_reco.models.crnn.crnn_run import CRNNReader\n\nfrom PIL import Image\n\nimport cv2 \nfrom skimage import io \nimport numpy as np \nfrom text_reco.models.craft.imgproc import ImageConvert\nimport json \nimport zipfile\nfrom collections import OrderedDict\nfrom skimage import io\nfrom skimage.transform import rescale, resize, downscale_local_mean\n\nclass CraftReader(ImageConvert):\n    def __init__(self,  image):\n        super(CraftReader, self).__init__(image)\n        self.model_path = \'text_reco/models/craft/pretrain/craft_mlt_25k.pth\'\n        self.net = CRAFT()\n        self.net.load_state_dict(self.copyStateDict(torch.load(self.model_path, map_location=\'cpu\')))\n        self.net.eval()\n        self.mag_ratio = 1\n        self.square_size = 1280\n\n    @staticmethod\n    def copyStateDict(state_dict):\n        if list(state_dict.keys())[0].startswith(""module""):\n            start_idx = 1\n        else:\n            start_idx = 0\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            name = ""."".join(k.split(""."")[start_idx:])\n            new_state_dict[name] = v\n        return new_state_dict\n\n    @staticmethod\n    def str2bool(v):\n        return v.lower() in (""yes"", ""y"", ""t"", ""1"")\n        \n    def image_preprocess(self, image):\n        image = self.normalizeMeanVariance(image)\n        image = torch.from_numpy(image).permute(2, 0, 1)\n        image = Variable(image.unsqueeze(0))\n        return image\n\n    def boxes_detect(self):\n        img_resized, target_ratio, size_heatmap = self.resize_aspect_ratio(self.image)\n        ratio_h = ratio_w = 1/ target_ratio\n        x =  self.image_preprocess(img_resized)\n        y, _ = self.net(x)\n        score_text = y[0, :, :, 0].cpu().data.numpy()\n        score_link = y[0, :, :, 1].cpu().data.numpy()\n        boxes = craft_utils.getDetBoxes(textmap =score_text, linkmap = score_link, text_threshold =0.7, link_threshold=0.4, low_text=0.4)\n        print(""Ilosc boxow {}"".format(len(boxes)))\n        boxes = craft_utils.adjustResultCoordinates(boxes, ratio_w, ratio_h)\n        return boxes, img_resized \n\n'"
text_reco/models/craft/craft_utils.py,0,"b'""""""  \nhttps://raw.githubusercontent.com/clovaai/CRAFT-pytorch/master/craft_utils.py\n""""""\nimport numpy as np\nimport math\nimport cv2\ndef getDetBoxes_core(textmap, linkmap, text_threshold, link_threshold, low_text):\n    linkmap = linkmap.copy()\n    textmap = textmap.copy()\n    img_h, img_w = textmap.shape\n\n    ret, text_score = cv2.threshold(textmap, low_text, 1, 0)\n    ret, link_score = cv2.threshold(linkmap, link_threshold, 1, 0)\n    text_score_comb = np.clip(text_score + link_score, 0, 1)\n    nLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(text_score_comb.astype(np.uint8), connectivity=4)\n    det = []\n    mapper = []\n    for k in range(1,nLabels):\n        size = stats[k, cv2.CC_STAT_AREA]\n        if size < 10: continue\n        if np.max(textmap[labels==k]) < text_threshold: continue\n        segmap = np.zeros(textmap.shape, dtype=np.uint8)\n        segmap[labels==k] = 255\n        segmap[np.logical_and(link_score==1, text_score==0)] = 0   # remove link area\n        x, y = stats[k, cv2.CC_STAT_LEFT], stats[k, cv2.CC_STAT_TOP]\n        w, h = stats[k, cv2.CC_STAT_WIDTH], stats[k, cv2.CC_STAT_HEIGHT]\n        niter = int(math.sqrt(size * min(w, h) / (w * h)) * 2)\n        sx, ex, sy, ey = x - niter, x + w + niter + 1, y - niter, y + h + niter + 1\n        if sx < 0 : sx = 0\n        if sy < 0 : sy = 0\n        if ex >= img_w: ex = img_w\n        if ey >= img_h: ey = img_h\n        kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(1 + niter, 1 + niter))\n        segmap[sy:ey, sx:ex] = cv2.dilate(segmap[sy:ey, sx:ex], kernel)\n        np_contours = np.roll(np.array(np.where(segmap!=0)),1,axis=0).transpose().reshape(-1,2)\n        rectangle = cv2.minAreaRect(np_contours)\n        box = cv2.boxPoints(rectangle)\n        w, h = np.linalg.norm(box[0] - box[1]), np.linalg.norm(box[1] - box[2])\n        box_ratio = max(w, h) / (min(w, h) + 1e-5)\n        if abs(1 - box_ratio) <= 0.1:\n            l, r = min(np_contours[:,0]), max(np_contours[:,0])\n            t, b = min(np_contours[:,1]), max(np_contours[:,1])\n            box = np.array([[l, t], [r, t], [r, b], [l, b]], dtype=np.float32)\n        startidx = box.sum(axis=1).argmin()\n        box = np.roll(box, 4-startidx, 0)\n        box = np.array(box)\n        det.append(box)\n        mapper.append(k)\n    return det, labels, mapper\n\ndef getDetBoxes(textmap, linkmap, text_threshold, link_threshold, low_text):\n    boxes, labels, mapper = getDetBoxes_core(textmap, linkmap, text_threshold, link_threshold, low_text)\n    return boxes\n\ndef adjustResultCoordinates(polys, ratio_w, ratio_h, ratio_net = 2):\n    if len(polys) > 0:\n        polys = np.array(polys)\n        for k in range(len(polys)):\n            if polys[k] is not None:\n                polys[k] *= (ratio_w * ratio_net, ratio_h * ratio_net)\n    return polys\n'"
text_reco/models/craft/imgproc.py,0,"b'import numpy as np \nfrom skimage import io \nimport cv2\nclass ImageConvert():\n    def __init__(self, img_array, interpolation =cv2.INTER_LINEAR ,  square_size = 1280,  mag_ratio=1):\n        self.image = io.imread(img_array)\n        self.image =  self.image[:, :, :3]\n        self.image = np.array(self.image)\n        print(""Shape of processed file {}"".format(len(self.image)))\n        self.mean = (0.485, 0.456, 0.406)\n        self.variance =  (0.229, 0.224, 0.225)\n        self.square_size = square_size\n        self.interpolation = interpolation\n        self.mag_ratio = mag_ratio\n\n    def normalizeMeanVariance(self,image):\n        image = image.copy().astype(np.float32)\n        image -= np.array([self.mean[0]  * 255.0, self.mean[1] * 255.0, self.mean[2] * 255.0], dtype = np.float32)\n        image /= np.array([self.variance[0] * 255.0, self.variance[1] * 255.0, self.variance[2] * 255.0], dtype = np.float32)\n        return image\n\n    def resize_aspect_ratio(self, image):\n        height, width, channel = image.shape\n        target_size  = self.mag_ratio * max(height, width)\n        if target_size > self.square_size:\n            target_size = self.square_size\n\n        ratio = target_size / max(height, width)\n        target_h, target_w = int(height * ratio), int(width * ratio)\n        proc = cv2.resize(self.image, (target_w, target_h), interpolation = cv2.INTER_LINEAR)\n\n        target_h32, target_w32 = target_h, target_w\n        if target_h % 32 != 0:\n            target_h32 = target_h + (32 - target_h % 32)\n        if target_w % 32 != 0:\n            target_w32 = target_w + (32 - target_w % 32)\n\n        resized = np.zeros((target_h32, target_w32, channel), dtype = np.float32)\n        resized[0:target_h, 0:target_w, :] = proc\n        target_h, target_w = target_h32, target_w32\n        size_heatmap = (int(target_w/2), int(target_h/2))\n        return resized, ratio, size_heatmap\n\n    def cvt2HeatmapImg(img):\n        img = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n        img = cv2.applyColorMap(img, cv2.COLORMAP_JET)\n        return img\n'"
text_reco/models/crnn/__init__.py,0,b''
text_reco/models/crnn/crnn.py,1,"b'import torch.nn as nn \n\nclass BidirectionalLSTM(nn.Module):\n    def __init__(self, _in, hidden, out):\n        super(BidirectionalLSTM, self).__init__()\n        self.rnn = nn.LSTM(_in, hidden, bidirectional=True)\n        self.embedding = nn.Linear(hidden * 2, out)\n\n    def forward(self, x):\n        recurrent, _ = self.rnn(x)\n        T, b, h = recurrent.size()\n        t_rec = recurrent.view(T * b, h)\n        output = self.embedding(t_rec) \n        output = output.view(T, b, -1)\n        return output\n\nclass CRNN(nn.Module):\n    def __init__(self, imgh, nc, nclass, nh, n_rnn=2, leakyReLU = False):\n        super(CRNN, self).__init__()\n        ks = [3, 3, 3, 3, 3, 3, 2]\n        ps = [1, 1, 1, 1, 1, 1, 0]\n        ss = [1, 1, 1, 1, 1, 1, 1]\n        nm = [64, 128, 256, 256, 512, 512, 512]\n        cnn = nn.Sequential()\n        def convRelu(i, batchNormalization=False):\n            nIn = nc if i == 0 else nm[i - 1]\n            nOut = nm[i]\n            cnn.add_module(\'conv{0}\'.format(i),\n                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n            if batchNormalization:\n                cnn.add_module(\'batchnorm{0}\'.format(i), nn.BatchNorm2d(nOut))\n            if leakyReLU:\n                cnn.add_module(\'relu{0}\'.format(i),\n                               nn.LeakyReLU(0.2, inplace=True))\n            else:\n                cnn.add_module(\'relu{0}\'.format(i), nn.ReLU(True))\n        convRelu(0)\n        cnn.add_module(\'pooling{0}\'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n        convRelu(1)\n        cnn.add_module(\'pooling{0}\'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n        convRelu(2, True)\n        convRelu(3)\n        cnn.add_module(\'pooling{0}\'.format(2),\n                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n        convRelu(4, True)\n        convRelu(5)\n        cnn.add_module(\'pooling{0}\'.format(3),\n                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n        convRelu(6, True)  # 512x1x16\n        self.cnn = cnn\n        self.rnn = nn.Sequential(\n            BidirectionalLSTM(512, nh, nh),\n            BidirectionalLSTM(nh, nh, nclass))\n\n    def forward(self, input):\n        conv = self.cnn(input)\n        b, c, h, w = conv.size()\n        assert h == 1, ""the height of conv must be 1""\n        conv = conv.squeeze(2)\n        conv = conv.permute(2, 0, 1)  # [w, b, c]\n        output = self.rnn(conv)\n        return output\n'"
text_reco/models/crnn/crnn_run.py,3,"b""from PIL import Image\nfrom skimage import io\nimport text_reco.models.crnn.crnn as crnn\n\nimport torch\nfrom torch.autograd import Variable\nimport text_reco.models.crnn.utils  as utils\nimport text_reco.models.crnn.preprocess as preprocess\n\nclass CRNNReader():\n    def __init__(self, model_path= 'text_reco/models/crnn/pretrain/crnn.pth'):\n        self.model_path = model_path\n        self.model = crnn.CRNN(32, 1,37, 256)\n        self.model = self.model.float()\n        self.model.load_state_dict(torch.load(self.model_path))\n        self.model.eval()\n        self.alphabet = '0123456789abcdefghijklmnopqrstuv2xyz'\n        self.transformer =  preprocess.resizeNormalize((100, 32))\n        self.converter = utils.strLabelConverter(self.alphabet)\n\n    def get_predictions(self, img):\n        self.model = self.model.float()\n        img = img.float()\n        predictions = self.model(img)\n        _, predictions = predictions.max(2)\n        predictions = predictions.transpose(1, 0).contiguous().view(-1)\n        pred_size = Variable(torch.IntTensor([predictions.size(0)]))\n        results =  self.converter.decode(predictions.data, pred_size.data, raw=False)\n        return results\n"""
text_reco/models/crnn/preprocess.py,6,"b""import skimage\n\nimport random\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import sampler\nimport torchvision.transforms as transforms\nimport lmdb\nimport six\nimport sys\nfrom PIL import Image\nimport numpy as np\n\nclass lmdbDataset(Dataset):\n    def __init__(self, root=None, transform=None, target_transform=None):\n        self.env = lmdb.open(\n            root,\n            max_readers=1,\n            readonly=True,\n            lock=False,\n            readahead=False,\n            meminit=False)\n\n        if not self.env:\n            print('cannot creat lmdb from %s' % (root))\n            sys.exit(0)\n\n        with self.env.begin(write=False) as txn:\n            nSamples = int(txn.get('num-samples'))\n            self.nSamples = nSamples\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return self.nSamples\n\n    def __getitem__(self, index):\n        assert index <= len(self), 'index range error'\n        index += 1\n        with self.env.begin(write=False) as txn:\n            img_key = 'image-%09d' % index\n            imgbuf = txn.get(img_key)\n            buf = six.BytesIO()\n            buf.write(imgbuf)\n            buf.seek(0)\n            try:\n                img = Image.open(buf).convert('L')\n            except IOError:\n                print('Corrupted image for %d' % index)\n                return self[index + 1]\n\n            if self.transform is not None:\n                img = self.transform(img)\n            label_key = 'label-%09d' % index\n            label = str(txn.get(label_key))\n            if self.target_transform is not None:\n                label = self.target_transform(label)\n        return (img, label)\n\nclass resizeNormalize(object):\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n        self.toTensor = transforms.ToTensor()\n\n    def __call__(self, img):\n        img = img.resize(self.size , self.interpolation)\n        img = self.toTensor(img)\n        img.sub_(0.5).div_(0.5)\n        return img\n\nclass randomSequentialSampler(sampler.Sampler):\n    def __init__(self, data_source, batch_size):\n        self.num_samples = len(data_source)\n        self.batch_size = batch_size\n\n    def __iter__(self):\n        n_batch = len(self) // self.batch_size\n        tail = len(self) % self.batch_size\n        index = torch.LongTensor(len(self)).fill_(0)\n        for i in range(n_batch):\n            random_start = random.randint(0, len(self) - self.batch_size)\n            batch_index = random_start + torch.range(0, self.batch_size - 1)\n            index[i * self.batch_size:(i + 1) * self.batch_size] = batch_index\n        if tail:\n            random_start = random.randint(0, len(self) - self.batch_size)\n            tail_index = random_start + torch.range(0, tail - 1)\n            index[(i + 1) * self.batch_size:] = tail_index\n\n        return iter(index)\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass alignCollate(object):\n\n    def __init__(self, imgH=32, imgW=100, keep_ratio=False, min_ratio=1):\n        self.imgH = imgH\n        self.imgW = imgW\n        self.keep_ratio = keep_ratio\n        self.min_ratio = min_ratio\n\n    def __call__(self, batch):\n        images, labels = zip(*batch)\n\n        imgH = self.imgH\n        imgW = self.imgW\n        if self.keep_ratio:\n            ratios = []\n            for image in images:\n                w, h = image.size\n                ratios.append(w / float(h))\n            ratios.sort()\n            max_ratio = ratios[-1]\n            imgW = int(np.floor(max_ratio * imgH))\n            imgW = max(imgH * self.min_ratio, imgW)  # assure imgH >= imgW\n\n        transform = resizeNormalize((imgW, imgH))\n        images = [transform(image) for image in images]\n        images = torch.cat([t.unsqueeze(0) for t in images], 0)\n\n        return images, labels \n"""
text_reco/models/crnn/utils.py,11,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport collections\n\n\nclass strLabelConverter(object):\n    """"""Convert between str and label.\n\n    NOTE:\n        Insert `blank` to the alphabet for CTC.\n\n    Args:\n        alphabet (str): set of the possible characters.\n        ignore_case (bool, default=True): whether or not to ignore all of the case.\n    """"""\n\n    def __init__(self, alphabet, ignore_case=True):\n        self._ignore_case = ignore_case\n        if self._ignore_case:\n            alphabet = alphabet.lower()\n        self.alphabet = alphabet + \'-\'  # for `-1` index\n\n        self.dict = {}\n        for i, char in enumerate(alphabet):\n            # NOTE: 0 is reserved for \'blank\' required by wrap_ctc\n            self.dict[char] = i + 1\n\n    def encode(self, text):\n        """"""Support batch or single str.\n\n        Args:\n            text (str or list of str): texts to convert.\n\n        Returns:\n            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n            torch.IntTensor [n]: length of each text.\n        """"""\n        if isinstance(text, str):\n            text = [\n                self.dict[char.lower() if self._ignore_case else char]\n                for char in text\n            ]\n            length = [len(text)]\n        elif isinstance(text, collections.Iterable):\n            length = [len(s) for s in text]\n            text = \'\'.join(text)\n            text, _ = self.encode(text)\n        return (torch.IntTensor(text), torch.IntTensor(length))\n\n    def decode(self, t, length, raw=False):\n        """"""Decode encoded texts back into strs.\n\n        Args:\n            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n            torch.IntTensor [n]: length of each text.\n\n        Raises:\n            AssertionError: when the texts and its length does not match.\n\n        Returns:\n            text (str or list of str): texts to convert.\n        """"""\n        if length.numel() == 1:\n            length = length[0]\n            assert t.numel() == length, ""text with length: {} does not match declared length: {}"".format(t.numel(), length)\n            if raw:\n                return \'\'.join([self.alphabet[i - 1] for i in t])\n            else:\n                char_list = []\n                for i in range(length):\n                    if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):\n                        char_list.append(self.alphabet[t[i] - 1])\n                return \'\'.join(char_list)\n        else:\n            # batch mode\n            assert t.numel() == length.sum(), ""texts with length: {} does not match declared length: {}"".format(t.numel(), length.sum())\n            texts = []\n            index = 0\n            for i in range(length.numel()):\n                l = length[i]\n                texts.append(\n                    self.decode(\n                        t[index:index + l], torch.IntTensor([l]), raw=raw))\n                index += l\n            return texts\n\n\nclass averager(object):\n    """"""Compute average for `torch.Variable` and `torch.Tensor`. """"""\n\n    def __init__(self):\n        self.reset()\n\n    def add(self, v):\n        if isinstance(v, Variable):\n            count = v.data.numel()\n            v = v.data.sum()\n        elif isinstance(v, torch.Tensor):\n            count = v.numel()\n            v = v.sum()\n\n        self.n_count += count\n        self.sum += v\n\n    def reset(self):\n        self.n_count = 0\n        self.sum = 0\n\n    def val(self):\n        res = 0\n        if self.n_count != 0:\n            res = self.sum / float(self.n_count)\n        return res\n\ndef oneHot(v, v_length, nc):\n    batchSize = v_length.size(0)\n    maxLength = v_length.max()\n    v_onehot = torch.FloatTensor(batchSize, maxLength, nc).fill_(0)\n    acc = 0\n    for i in range(batchSize):\n        length = v_length[i]\n        label = v[acc:acc + length].view(-1, 1).long()\n        v_onehot[i, :length].scatter_(1, label, 1.0)\n        acc += length\n    return v_onehot\n\n\ndef loadData(v, data):\n    v.data.resize_(data.size()).copy_(data)\n\n\ndef prettyPrint(v):\n    print(\'Size {0}, Type: {1}\'.format(str(v.size()), v.data.type()))\n    print(\'| Max: %f | Min: %f | Mean: %f\' % (v.max().data[0], v.min().data[0],\n                                              v.mean().data[0]))\n\n\ndef assureRatio(img):\n    """"""Ensure imgH <= imgW.""""""\n    b, c, h, w = img.size()\n    if h > w:\n        main = nn.UpsamplingBilinear2d(size=(h, h), scale_factor=None)\n        img = main(img)\n    return img\n'"
build/lib/text_reco/boxdetect/__init__.py,0,b''
build/lib/text_reco/boxdetect/box_detection.py,0,"b'# Simple logic \n# box on input - preprocess - sliced image on output\nimport cv2\nimport numpy as np \nclass BoxDetect():\n\n    def __init__(self, boxes):\n        self.boxes = boxes\n        self.n_boxes = len(self.boxes)\n\n    def preprocess(self, image):\n        img_storage = dict()\n        print(""To sa boxy"")\n        print(self.boxes)\n        for el in self.boxes:\n            tmp_img = cv2.rectangle(image, (el[0], el[1]), (el[2], el[3]))\n    \n    @staticmethod\n    def load_box(path):\n        with open(path, \'r\') as outfile:\n            file_ = json.load(outfile)\n        return file_\n\n    @staticmethod \n    def preprocess_box(file_, img):\n        for el in file_.keys():\n            x,y,w,h = cv2.boundingRect(np.array(file_[el]))\n            roi = img[x:x+w, y:y+h]\n            cv2.imshow(\'image\', roi)\n            cv2.waitKey(0)\n\n\n\n'"
build/lib/text_reco/models/__init__.py,0,b''
text_reco/models/craft/basenet/__init__.py,0,b''
text_reco/models/craft/basenet/vgg16_bn.py,9,"b'# https://github.com/clovaai/CRAFT-pytorch/blob/master/basenet/vgg16_bn.py\n\n\n# Imports \n\nfrom collections import namedtuple\n\nimport torch\nimport torch.nn as nn \nimport torch.nn.init as init\nfrom torchvision import models\nfrom torchvision.models.vgg import model_urls\n\ndef init_weights(modules):\n    # https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/ \n    for m in modules:\n        if isinstance(m, nn.Conv2d):\n            init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.zero_()\n       \n        elif isinstance(m, nn.BatchNorm2d):\n            m.weight.data.fill_(1)\n            m.bias.data.zero_()\n\n        elif isinstance(m, nn.Linear):\n            m.weight.data.normal_(0, 0.01)\n            m.bias.data.zero_()\n\n            \nclass vgg16_bn(torch.nn.Module):\n\n    def __init__(self, pretrained =  True, freeze = True):\n        super(vgg16_bn, self).__init__()\n        model_urls[\'vgg16_bn\'] = model_urls[\'vgg16_bn\'].replace(\'https://\', \'http://\')\n        vgg_pretrained_features = models.vgg16_bn(pretrained=pretrained).features\n        self.slice1 =  torch.nn.Sequential()\n        self.slice2 =  torch.nn.Sequential()\n        self.slice3 =  torch.nn.Sequential()\n        self.slice4 =  torch.nn.Sequential()\n        self.slice5 =  torch.nn.Sequential()\n\n        for x in range(12):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(12, 19):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(19, 29):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(29, 39):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n\n        self.slice5 = torch.nn.Sequential(\n                nn.MaxPool2d(3, 1, 1), \n                nn.Conv2d(512, 1024, kernel_size = 3, padding = 6, dilation = 6), \n                nn.Conv2d(1024, 1024, 1))\n\n        if not pretrained:\n            init_weights(self.slice1.modules())\n            init_weights(self.slice2.modules())\n            init_weights(self.slice3.modules())\n            init_weights(self.slice4.modules())\n\n        init_weights(self.slice5.modules())\n\n\n        if freeze:\n            for param in self.slice1.parameters():\n                param.requires_grad = False\n\n\n# Define forward pass\n\n\n    def forward(self, x):\n        h = self.slice1(x)\n        h_relu2_2 = h\n        h = self.slice2(h)\n        h_relu3_2 = h\n        h = self.slice3(h)\n        h_relu4_3 = h\n        h = self.slice4(h)\n        h_relu5_3 = h\n        h = self.slice5(h)\n        h_fc7 = h\n        vgg_outputs =  namedtuple(""VggOutputs"", [\'fc7\', \'relu5_3\', \'relu4_3\', \'relu3_2\', \'relu2_2\'])\n        out = vgg_outputs(h_fc7 ,h_relu5_3, h_relu4_3, h_relu3_2, h_relu2_2)\n        return out\n        \n'"
build/lib/text_reco/models/craft/__init__.py,0,b''
build/lib/text_reco/models/craft/craft.py,6,"b""import torch\nimport torch.nn as nn \nimport torch.nn.functional as F\nfrom text_reco.models.craft.basenet.vgg16_bn import vgg16_bn, init_weights\n\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channel, mid_channel, out_channel):\n        super(DoubleConv, self).__init__()\n\n        self.conv = nn.Sequential(\n                nn.Conv2d(in_channel + mid_channel, mid_channel,  kernel_size = 1), \n                nn.BatchNorm2d(mid_channel), \n                nn.ReLU(inplace=True), \n                nn.Conv2d(mid_channel, out_channel, kernel_size = 3, padding = 1), \n                nn.BatchNorm2d(out_channel), \n                nn.ReLU(inplace=True))\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\nclass CRAFT(nn.Module):\n    def __init__(self, pretrained=False, freeze=False):\n        super(CRAFT, self).__init__()\n        self.basenet = vgg16_bn(pretrained, freeze)\n        self.upconv1 = DoubleConv(1024, 512, 256)\n        self.upconv2 = DoubleConv(512, 256, 128)\n        self.upconv3 = DoubleConv(256, 128, 64)\n        self.upconv4 = DoubleConv(128, 64, 32)\n\n        n_classes = 2\n        self.conv_cls = nn.Sequential(\n                nn.Conv2d(32, 32, 3, 1), \n                nn.ReLU(inplace=True), \n                nn.Conv2d(32, 32, 3, 1), \n                nn.ReLU(inplace=True), \n                nn.Conv2d(32, 16, 3, 1), \n                nn.ReLU(inplace=True), \n                nn.Conv2d(16, 16, 1), \n                nn.ReLU(inplace=True), \n                nn.Conv2d(16, n_classes, kernel_size=1),)\n\n        init_weights(self.upconv1.modules())\n        init_weights(self.upconv2.modules())\n        init_weights(self.upconv3.modules())\n        init_weights(self.upconv4.modules())\n        init_weights(self.conv_cls.modules())\n\n    def forward(self, x):\n        sources = self.basenet(x)\n        y = torch.cat([sources[0], sources[1]], dim=1)\n        y = self.upconv1(y)\n\n        y = F.interpolate(y, size = sources[2].size()[2:], mode = 'bilinear', align_corners=False)\n        y = torch.cat([y, sources[2]], dim=1)\n        y = self.upconv2(y)\n\n        y = F.interpolate(y, size = sources[3].size()[2:], mode = 'bilinear', align_corners=False)\n        y = torch.cat([y, sources[3]], dim=1)\n        y = self.upconv3(y)\n\n        y = F.interpolate(y, size = sources[4].size()[2:], mode = 'bilinear', align_corners=False)\n        y = torch.cat([y, sources[4]], dim =1)\n        feature = self.upconv4(y)\n        y = self.conv_cls(feature)\n\n        return y.permute(0, 2, 3, 1), feature\n\n"""
build/lib/text_reco/models/craft/craft_reader.py,5,"b'import torch\nimport torch.nn as nn \nimport torch.backends.cudnn as cudnn\nfrom  torch.autograd import Variable\nimport text_reco.models.craft.craft_utils as craft_utils\nimport text_reco.models.craft.imgproc as img_proc\nfrom text_reco.models.craft.craft import CRAFT\nfrom text_reco.boxdetect.box_detection import BoxDetect\nfrom text_reco.models.crnn.crnn_run import CRNNReader\n\nfrom PIL import Image\n\nimport cv2 \nfrom skimage import io \nimport numpy as np \nfrom text_reco.models.craft.imgproc import ImageConvert\nimport json \nimport zipfile\nfrom collections import OrderedDict\nfrom skimage import io\nfrom skimage.transform import rescale, resize, downscale_local_mean\n\nclass CraftReader(ImageConvert):\n    def __init__(self,  image):\n        super(CraftReader, self).__init__(image)\n        self.model_path = \'text_reco/models/craft/pretrain/craft_mlt_25k.pth\'\n        self.net = CRAFT()\n        self.net.load_state_dict(self.copyStateDict(torch.load(self.model_path)))\n        self.net.eval()\n        self.mag_ratio = 1\n        self.square_size = 1280\n\n    @staticmethod\n    def copyStateDict(state_dict):\n        if list(state_dict.keys())[0].startswith(""module""):\n            start_idx = 1\n        else:\n            start_idx = 0\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            name = ""."".join(k.split(""."")[start_idx:])\n            new_state_dict[name] = v\n        return new_state_dict\n\n    @staticmethod\n    def str2bool(v):\n        return v.lower() in (""yes"", ""y"", ""t"", ""1"")\n        \n    def image_preprocess(self, image):\n        image = self.normalizeMeanVariance(image)\n        image = torch.from_numpy(image).permute(2, 0, 1)\n        image = Variable(image.unsqueeze(0))\n        return image\n\n    def boxes_detect(self):\n        img_resized, target_ratio, size_heatmap = self.resize_aspect_ratio(self.image)\n        ratio_h = ratio_w = 1/ target_ratio\n        x =  self.image_preprocess(img_resized)\n        y, _ = self.net(x)\n        score_text = y[0, :, :, 0].cpu().data.numpy()\n        score_link = y[0, :, :, 1].cpu().data.numpy()\n        boxes = craft_utils.getDetBoxes(textmap =score_text, linkmap = score_link, text_threshold =0.7, link_threshold=0.4, low_text=0.4)\n        print(""Ilosc boxow {}"".format(len(boxes)))\n        boxes = craft_utils.adjustResultCoordinates(boxes, ratio_w, ratio_h)\n        return boxes, img_resized \n\n'"
build/lib/text_reco/models/craft/craft_utils.py,0,"b'""""""  \nhttps://raw.githubusercontent.com/clovaai/CRAFT-pytorch/master/craft_utils.py\n""""""\nimport numpy as np\nimport cv2\nimport math\n\ndef getDetBoxes_core(textmap, linkmap, text_threshold, link_threshold, low_text):\n    linkmap = linkmap.copy()\n    textmap = textmap.copy()\n    img_h, img_w = textmap.shape\n\n    ret, text_score = cv2.threshold(textmap, low_text, 1, 0)\n    ret, link_score = cv2.threshold(linkmap, link_threshold, 1, 0)\n    text_score_comb = np.clip(text_score + link_score, 0, 1)\n    nLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(text_score_comb.astype(np.uint8), connectivity=4)\n    det = []\n    mapper = []\n    for k in range(1,nLabels):\n        size = stats[k, cv2.CC_STAT_AREA]\n        if size < 10: continue\n        if np.max(textmap[labels==k]) < text_threshold: continue\n        segmap = np.zeros(textmap.shape, dtype=np.uint8)\n        segmap[labels==k] = 255\n        segmap[np.logical_and(link_score==1, text_score==0)] = 0   # remove link area\n        x, y = stats[k, cv2.CC_STAT_LEFT], stats[k, cv2.CC_STAT_TOP]\n        w, h = stats[k, cv2.CC_STAT_WIDTH], stats[k, cv2.CC_STAT_HEIGHT]\n        niter = int(math.sqrt(size * min(w, h) / (w * h)) * 2)\n        sx, ex, sy, ey = x - niter, x + w + niter + 1, y - niter, y + h + niter + 1\n        if sx < 0 : sx = 0\n        if sy < 0 : sy = 0\n        if ex >= img_w: ex = img_w\n        if ey >= img_h: ey = img_h\n        kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(1 + niter, 1 + niter))\n        segmap[sy:ey, sx:ex] = cv2.dilate(segmap[sy:ey, sx:ex], kernel)\n        np_contours = np.roll(np.array(np.where(segmap!=0)),1,axis=0).transpose().reshape(-1,2)\n        rectangle = cv2.minAreaRect(np_contours)\n        box = cv2.boxPoints(rectangle)\n        w, h = np.linalg.norm(box[0] - box[1]), np.linalg.norm(box[1] - box[2])\n        box_ratio = max(w, h) / (min(w, h) + 1e-5)\n        if abs(1 - box_ratio) <= 0.1:\n            l, r = min(np_contours[:,0]), max(np_contours[:,0])\n            t, b = min(np_contours[:,1]), max(np_contours[:,1])\n            box = np.array([[l, t], [r, t], [r, b], [l, b]], dtype=np.float32)\n        startidx = box.sum(axis=1).argmin()\n        box = np.roll(box, 4-startidx, 0)\n        box = np.array(box)\n        det.append(box)\n        mapper.append(k)\n    return det, labels, mapper\n\ndef getDetBoxes(textmap, linkmap, text_threshold, link_threshold, low_text):\n    boxes, labels, mapper = getDetBoxes_core(textmap, linkmap, text_threshold, link_threshold, low_text)\n    return boxes\n\ndef adjustResultCoordinates(polys, ratio_w, ratio_h, ratio_net = 2):\n    if len(polys) > 0:\n        polys = np.array(polys)\n        for k in range(len(polys)):\n            if polys[k] is not None:\n                polys[k] *= (ratio_w * ratio_net, ratio_h * ratio_net)\n    return polys\n'"
build/lib/text_reco/models/craft/file_utils.py,0,"b'import os \nimport numpy as np \nimport cv2 \nfrom PIL import Image\n\n\n\nclass DataLoader():\n\n\n    def __init__(self, _file):\n\n        self._file = _file\n        self.extensions = [\'.pdf\', \'.tif\', \'.png\']\n\n    \n\n    def load_image(self):\n\n        try:\n            _img = Image.open(self._file)\n            return _img\n        except:\n            ValueError(""File does not exist!"")\n           \n\ndef main():\n    pass\n\n\nif __name__ == ""__main__"":\n    main()\n'"
build/lib/text_reco/models/craft/imgproc.py,0,"b'import numpy as np \nfrom skimage import io \nimport cv2 \n\nclass ImageConvert():\n    def __init__(self, img_array, interpolation =cv2.INTER_LINEAR ,  square_size = 1280,  mag_ratio=1):\n        self.image = io.imread(img_array)\n        self.image =  self.image[:, :, :3]\n        self.image = np.array(self.image)\n        print(""Shape of processed file {}"".format(len(self.image)))\n        self.mean = (0.485, 0.456, 0.406)\n        self.variance =  (0.229, 0.224, 0.225)\n        self.square_size = square_size\n        self.interpolation = interpolation\n        self.mag_ratio = mag_ratio\n\n    def normalizeMeanVariance(self,image):\n        image = image.copy().astype(np.float32)\n        image -= np.array([self.mean[0]  * 255.0, self.mean[1] * 255.0, self.mean[2] * 255.0], dtype = np.float32)\n        image /= np.array([self.variance[0] * 255.0, self.variance[1] * 255.0, self.variance[2] * 255.0], dtype = np.float32)\n        return image\n\n    def resize_aspect_ratio(self, image):\n        height, width, channel = image.shape\n        target_size  = self.mag_ratio * max(height, width)\n        if target_size > self.square_size:\n            target_size = self.square_size\n\n        ratio = target_size / max(height, width)\n        target_h, target_w = int(height * ratio), int(width * ratio)\n        proc = cv2.resize(self.image, (target_w, target_h), interpolation = cv2.INTER_LINEAR)\n\n        target_h32, target_w32 = target_h, target_w\n        if target_h % 32 != 0:\n            target_h32 = target_h + (32 - target_h % 32)\n        if target_w % 32 != 0:\n            target_w32 = target_w + (32 - target_w % 32)\n\n        resized = np.zeros((target_h32, target_w32, channel), dtype = np.float32)\n        resized[0:target_h, 0:target_w, :] = proc\n        target_h, target_w = target_h32, target_w32\n        size_heatmap = (int(target_w/2), int(target_h/2))\n        return resized, ratio, size_heatmap\n\n    def cvt2HeatmapImg(img):\n        img = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n        img = cv2.applyColorMap(img, cv2.COLORMAP_JET)\n        return img\n'"
build/lib/text_reco/models/crnn/__init__.py,0,b''
build/lib/text_reco/models/crnn/crnn.py,1,"b'import torch.nn as nn \n\nclass BidirectionalLSTM(nn.Module):\n    def __init__(self, _in, hidden, out):\n        super(BidirectionalLSTM, self).__init__()\n        self.rnn = nn.LSTM(_in, hidden, bidirectional=True)\n        self.embedding = nn.Linear(hidden * 2, out)\n\n    def forward(self, x):\n        recurrent, _ = self.rnn(x)\n        T, b, h = recurrent.size()\n        t_rec = recurrent.view(T * b, h)\n        output = self.embedding(t_rec) \n        output = output.view(T, b, -1)\n        return output\n\nclass CRNN(nn.Module):\n    def __init__(self, imgh, nc, nclass, nh, n_rnn=2, leakyReLU = False):\n        super(CRNN, self).__init__()\n        ks = [3, 3, 3, 3, 3, 3, 2]\n        ps = [1, 1, 1, 1, 1, 1, 0]\n        ss = [1, 1, 1, 1, 1, 1, 1]\n        nm = [64, 128, 256, 256, 512, 512, 512]\n        cnn = nn.Sequential()\n        def convRelu(i, batchNormalization=False):\n            nIn = nc if i == 0 else nm[i - 1]\n            nOut = nm[i]\n            cnn.add_module(\'conv{0}\'.format(i),\n                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n            if batchNormalization:\n                cnn.add_module(\'batchnorm{0}\'.format(i), nn.BatchNorm2d(nOut))\n            if leakyReLU:\n                cnn.add_module(\'relu{0}\'.format(i),\n                               nn.LeakyReLU(0.2, inplace=True))\n            else:\n                cnn.add_module(\'relu{0}\'.format(i), nn.ReLU(True))\n        convRelu(0)\n        cnn.add_module(\'pooling{0}\'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n        convRelu(1)\n        cnn.add_module(\'pooling{0}\'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n        convRelu(2, True)\n        convRelu(3)\n        cnn.add_module(\'pooling{0}\'.format(2),\n                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n        convRelu(4, True)\n        convRelu(5)\n        cnn.add_module(\'pooling{0}\'.format(3),\n                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n        convRelu(6, True)  # 512x1x16\n        self.cnn = cnn\n        self.rnn = nn.Sequential(\n            BidirectionalLSTM(512, nh, nh),\n            BidirectionalLSTM(nh, nh, nclass))\n\n    def forward(self, input):\n        conv = self.cnn(input)\n        b, c, h, w = conv.size()\n        assert h == 1, ""the height of conv must be 1""\n        conv = conv.squeeze(2)\n        conv = conv.permute(2, 0, 1)  # [w, b, c]\n        output = self.rnn(conv)\n        return output\n'"
build/lib/text_reco/models/crnn/crnn_run.py,3,"b""from PIL import Image\nfrom skimage import io\nimport text_reco.models.crnn.crnn as crnn\n\nimport torch\nfrom torch.autograd import Variable\nimport text_reco.models.crnn.utils  as utils\nimport text_reco.models.crnn.preprocess as preprocess\n\nclass CRNNReader():\n    def __init__(self, model_path= 'text_reco/models/crnn/pretrain/crnn.pth'):\n        self.model_path = model_path\n        self.model = crnn.CRNN(32, 1,37, 256)\n        self.model = self.model.float()\n        self.model.load_state_dict(torch.load(self.model_path))\n        self.model.eval()\n        self.alphabet = '0123456789abcdefghijklmnopqrstuv2xyz'\n        self.transformer =  preprocess.resizeNormalize((100, 32))\n        self.converter = utils.strLabelConverter(self.alphabet)\n\n    def get_predictions(self, img):\n        self.model = self.model.float()\n        img = img.float()\n        predictions = self.model(img)\n        _, predictions = predictions.max(2)\n        predictions = predictions.transpose(1, 0).contiguous().view(-1)\n        pred_size = Variable(torch.IntTensor([predictions.size(0)]))\n        results =  self.converter.decode(predictions.data, pred_size.data, raw=False)\n        return results\n"""
build/lib/text_reco/models/crnn/preprocess.py,6,"b""import skimage\n\nimport random\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import sampler\nimport torchvision.transforms as transforms\nimport lmdb\nimport six\nimport sys\nfrom PIL import Image\nimport numpy as np\n\nclass lmdbDataset(Dataset):\n    def __init__(self, root=None, transform=None, target_transform=None):\n        self.env = lmdb.open(\n            root,\n            max_readers=1,\n            readonly=True,\n            lock=False,\n            readahead=False,\n            meminit=False)\n\n        if not self.env:\n            print('cannot creat lmdb from %s' % (root))\n            sys.exit(0)\n\n        with self.env.begin(write=False) as txn:\n            nSamples = int(txn.get('num-samples'))\n            self.nSamples = nSamples\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return self.nSamples\n\n    def __getitem__(self, index):\n        assert index <= len(self), 'index range error'\n        index += 1\n        with self.env.begin(write=False) as txn:\n            img_key = 'image-%09d' % index\n            imgbuf = txn.get(img_key)\n            buf = six.BytesIO()\n            buf.write(imgbuf)\n            buf.seek(0)\n            try:\n                img = Image.open(buf).convert('L')\n            except IOError:\n                print('Corrupted image for %d' % index)\n                return self[index + 1]\n\n            if self.transform is not None:\n                img = self.transform(img)\n            label_key = 'label-%09d' % index\n            label = str(txn.get(label_key))\n            if self.target_transform is not None:\n                label = self.target_transform(label)\n        return (img, label)\n\nclass resizeNormalize(object):\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n        self.toTensor = transforms.ToTensor()\n\n    def __call__(self, img):\n        img = img.resize(self.size , self.interpolation)\n        img = self.toTensor(img)\n        img.sub_(0.5).div_(0.5)\n        return img\n\nclass randomSequentialSampler(sampler.Sampler):\n    def __init__(self, data_source, batch_size):\n        self.num_samples = len(data_source)\n        self.batch_size = batch_size\n\n    def __iter__(self):\n        n_batch = len(self) // self.batch_size\n        tail = len(self) % self.batch_size\n        index = torch.LongTensor(len(self)).fill_(0)\n        for i in range(n_batch):\n            random_start = random.randint(0, len(self) - self.batch_size)\n            batch_index = random_start + torch.range(0, self.batch_size - 1)\n            index[i * self.batch_size:(i + 1) * self.batch_size] = batch_index\n        if tail:\n            random_start = random.randint(0, len(self) - self.batch_size)\n            tail_index = random_start + torch.range(0, tail - 1)\n            index[(i + 1) * self.batch_size:] = tail_index\n\n        return iter(index)\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass alignCollate(object):\n\n    def __init__(self, imgH=32, imgW=100, keep_ratio=False, min_ratio=1):\n        self.imgH = imgH\n        self.imgW = imgW\n        self.keep_ratio = keep_ratio\n        self.min_ratio = min_ratio\n\n    def __call__(self, batch):\n        images, labels = zip(*batch)\n\n        imgH = self.imgH\n        imgW = self.imgW\n        if self.keep_ratio:\n            ratios = []\n            for image in images:\n                w, h = image.size\n                ratios.append(w / float(h))\n            ratios.sort()\n            max_ratio = ratios[-1]\n            imgW = int(np.floor(max_ratio * imgH))\n            imgW = max(imgH * self.min_ratio, imgW)  # assure imgH >= imgW\n\n        transform = resizeNormalize((imgW, imgH))\n        images = [transform(image) for image in images]\n        images = torch.cat([t.unsqueeze(0) for t in images], 0)\n\n        return images, labels \n"""
build/lib/text_reco/models/crnn/utils.py,11,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport collections\n\n\nclass strLabelConverter(object):\n    """"""Convert between str and label.\n\n    NOTE:\n        Insert `blank` to the alphabet for CTC.\n\n    Args:\n        alphabet (str): set of the possible characters.\n        ignore_case (bool, default=True): whether or not to ignore all of the case.\n    """"""\n\n    def __init__(self, alphabet, ignore_case=True):\n        self._ignore_case = ignore_case\n        if self._ignore_case:\n            alphabet = alphabet.lower()\n        self.alphabet = alphabet + \'-\'  # for `-1` index\n\n        self.dict = {}\n        for i, char in enumerate(alphabet):\n            # NOTE: 0 is reserved for \'blank\' required by wrap_ctc\n            self.dict[char] = i + 1\n\n    def encode(self, text):\n        """"""Support batch or single str.\n\n        Args:\n            text (str or list of str): texts to convert.\n\n        Returns:\n            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n            torch.IntTensor [n]: length of each text.\n        """"""\n        if isinstance(text, str):\n            text = [\n                self.dict[char.lower() if self._ignore_case else char]\n                for char in text\n            ]\n            length = [len(text)]\n        elif isinstance(text, collections.Iterable):\n            length = [len(s) for s in text]\n            text = \'\'.join(text)\n            text, _ = self.encode(text)\n        return (torch.IntTensor(text), torch.IntTensor(length))\n\n    def decode(self, t, length, raw=False):\n        """"""Decode encoded texts back into strs.\n\n        Args:\n            torch.IntTensor [length_0 + length_1 + ... length_{n - 1}]: encoded texts.\n            torch.IntTensor [n]: length of each text.\n\n        Raises:\n            AssertionError: when the texts and its length does not match.\n\n        Returns:\n            text (str or list of str): texts to convert.\n        """"""\n        if length.numel() == 1:\n            length = length[0]\n            assert t.numel() == length, ""text with length: {} does not match declared length: {}"".format(t.numel(), length)\n            if raw:\n                return \'\'.join([self.alphabet[i - 1] for i in t])\n            else:\n                char_list = []\n                for i in range(length):\n                    if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):\n                        char_list.append(self.alphabet[t[i] - 1])\n                return \'\'.join(char_list)\n        else:\n            # batch mode\n            assert t.numel() == length.sum(), ""texts with length: {} does not match declared length: {}"".format(t.numel(), length.sum())\n            texts = []\n            index = 0\n            for i in range(length.numel()):\n                l = length[i]\n                texts.append(\n                    self.decode(\n                        t[index:index + l], torch.IntTensor([l]), raw=raw))\n                index += l\n            return texts\n\n\nclass averager(object):\n    """"""Compute average for `torch.Variable` and `torch.Tensor`. """"""\n\n    def __init__(self):\n        self.reset()\n\n    def add(self, v):\n        if isinstance(v, Variable):\n            count = v.data.numel()\n            v = v.data.sum()\n        elif isinstance(v, torch.Tensor):\n            count = v.numel()\n            v = v.sum()\n\n        self.n_count += count\n        self.sum += v\n\n    def reset(self):\n        self.n_count = 0\n        self.sum = 0\n\n    def val(self):\n        res = 0\n        if self.n_count != 0:\n            res = self.sum / float(self.n_count)\n        return res\n\ndef oneHot(v, v_length, nc):\n    batchSize = v_length.size(0)\n    maxLength = v_length.max()\n    v_onehot = torch.FloatTensor(batchSize, maxLength, nc).fill_(0)\n    acc = 0\n    for i in range(batchSize):\n        length = v_length[i]\n        label = v[acc:acc + length].view(-1, 1).long()\n        v_onehot[i, :length].scatter_(1, label, 1.0)\n        acc += length\n    return v_onehot\n\n\ndef loadData(v, data):\n    v.data.resize_(data.size()).copy_(data)\n\n\ndef prettyPrint(v):\n    print(\'Size {0}, Type: {1}\'.format(str(v.size()), v.data.type()))\n    print(\'| Max: %f | Min: %f | Mean: %f\' % (v.max().data[0], v.min().data[0],\n                                              v.mean().data[0]))\n\n\ndef assureRatio(img):\n    """"""Ensure imgH <= imgW.""""""\n    b, c, h, w = img.size()\n    if h > w:\n        main = nn.UpsamplingBilinear2d(size=(h, h), scale_factor=None)\n        img = main(img)\n    return img\n'"
build/lib/text_reco/models/craft/basenet/__init__.py,0,b''
build/lib/text_reco/models/craft/basenet/vgg16_bn.py,9,"b'# https://github.com/clovaai/CRAFT-pytorch/blob/master/basenet/vgg16_bn.py\n\n\n# Imports \n\nfrom collections import namedtuple\n\nimport torch\nimport torch.nn as nn \nimport torch.nn.init as init\nfrom torchvision import models\nfrom torchvision.models.vgg import model_urls\n\ndef init_weights(modules):\n    # https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/ \n    for m in modules:\n        if isinstance(m, nn.Conv2d):\n            init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.zero_()\n       \n        elif isinstance(m, nn.BatchNorm2d):\n            m.weight.data.fill_(1)\n            m.bias.data.zero_()\n\n        elif isinstance(m, nn.Linear):\n            m.weight.data.normal_(0, 0.01)\n            m.bias.data.zero_()\n\n            \nclass vgg16_bn(torch.nn.Module):\n\n    def __init__(self, pretrained =  True, freeze = True):\n        super(vgg16_bn, self).__init__()\n        model_urls[\'vgg16_bn\'] = model_urls[\'vgg16_bn\'].replace(\'https://\', \'http://\')\n        vgg_pretrained_features = models.vgg16_bn(pretrained=pretrained).features\n        self.slice1 =  torch.nn.Sequential()\n        self.slice2 =  torch.nn.Sequential()\n        self.slice3 =  torch.nn.Sequential()\n        self.slice4 =  torch.nn.Sequential()\n        self.slice5 =  torch.nn.Sequential()\n\n        for x in range(12):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(12, 19):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(19, 29):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(29, 39):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n\n        self.slice5 = torch.nn.Sequential(\n                nn.MaxPool2d(3, 1, 1), \n                nn.Conv2d(512, 1024, kernel_size = 3, padding = 6, dilation = 6), \n                nn.Conv2d(1024, 1024, 1))\n\n        if not pretrained:\n            init_weights(self.slice1.modules())\n            init_weights(self.slice2.modules())\n            init_weights(self.slice3.modules())\n            init_weights(self.slice4.modules())\n\n        init_weights(self.slice5.modules())\n\n\n        if freeze:\n            for param in self.slice1.parameters():\n                param.requires_grad = False\n\n\n# Define forward pass\n\n\n    def forward(self, x):\n        h = self.slice1(x)\n        h_relu2_2 = h\n        h = self.slice2(h)\n        h_relu3_2 = h\n        h = self.slice3(h)\n        h_relu4_3 = h\n        h = self.slice4(h)\n        h_relu5_3 = h\n        h = self.slice5(h)\n        h_fc7 = h\n        vgg_outputs =  namedtuple(""VggOutputs"", [\'fc7\', \'relu5_3\', \'relu4_3\', \'relu3_2\', \'relu2_2\'])\n        out = vgg_outputs(h_fc7 ,h_relu5_3, h_relu4_3, h_relu3_2, h_relu2_2)\n        return out\n        \n'"
