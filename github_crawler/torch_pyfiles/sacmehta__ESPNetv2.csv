file_path,api_count,code
imagenet/LRSchedule.py,0,"b'\n#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nclass MyLRScheduler(object):\n    \'\'\'\n    CLass that defines cyclic learning rate that decays the learning rate linearly till the end of cycle and then restarts\n    at the maximum value.\n    \'\'\'\n    def __init__(self, initial=0.1, cycle_len=5, steps=[51, 101, 131, 161, 191, 221, 251, 281], gamma=2):\n        super(MyLRScheduler, self).__init__()\n        assert len(steps) > 1, \'Please specify step intervals.\'\n        self.min_lr = initial # minimum learning rate\n        self.m = cycle_len\n        self.steps = steps\n        self.warm_up_interval = 1 # we do not start from max value for the first epoch, because some time it diverges\n        self.counter = 0\n        self.decayFactor = gamma # factor by which we should decay learning rate\n        self.count_cycles = 0\n        self.step_counter = 0\n        self.stepping = True\n        print(\'Using Cyclic LR Scheduler with warm restarts\')\n\n    def get_lr(self, epoch):\n        if epoch%self.steps[self.step_counter] == 0 and epoch > 1 and self.stepping:\n            self.min_lr = self.min_lr / self.decayFactor\n            self.count_cycles = 0\n            if self.step_counter < len(self.steps) - 1:\n                self.step_counter += 1\n            else:\n                self.stepping = False\n        current_lr = self.min_lr\n        # warm-up or cool-down phase\n        if self.count_cycles < self.warm_up_interval:\n            self.count_cycles += 1\n            # We do not need warm up after first step.\n            # so, we set warm up interval to 0 after first step\n            if self.count_cycles == self.warm_up_interval:\n                self.warm_up_interval = 0\n        else:\n            #Cyclic learning rate with warm restarts\n            # max_lr (= min_lr * step_size) is decreased to min_lr using linear decay before\n            # it is set to max value at the end of cycle.\n            if self.counter >= self.m:\n                self.counter = 0\n            current_lr = round((self.min_lr * self.m) - (self.counter * self.min_lr), 5)\n            self.counter += 1\n            self.count_cycles += 1\n        return current_lr\n\n\nif __name__ == \'__main__\':\n    lrSched = MyLRScheduler(0.1)#MyLRScheduler(0.1, 5, [51, 101, 131, 161, 191, 221, 251, 281, 311, 341, 371])\n    max_epochs = 300\n    for i in range(max_epochs):\n        print(i, lrSched.get_lr(i))\n'"
imagenet/Model.py,5,"b'from torch.nn import init\nimport torch.nn.functional as F\nfrom cnn_utils import *\nimport math\nimport torch\n\n#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nclass EESP(nn.Module):\n    \'\'\'\n    This class defines the EESP block, which is based on the following principle\n        REDUCE ---> SPLIT ---> TRANSFORM --> MERGE\n    \'\'\'\n\n    def __init__(self, nIn, nOut, stride=1, k=4, r_lim=7, down_method=\'esp\'): #down_method --> [\'avg\' or \'esp\']\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param stride: factor by which we should skip (useful for down-sampling). If 2, then down-samples the feature map by 2\n        :param k: # of parallel branches\n        :param r_lim: A maximum value of receptive field allowed for EESP block\n        :param down_method: Downsample or not (equivalent to say stride is 2 or not)\n        \'\'\'\n        super().__init__()\n        self.stride = stride\n        n = int(nOut / k)\n        n1 = nOut - (k - 1) * n\n        assert down_method in [\'avg\', \'esp\'], \'One of these is suppported (avg or esp)\'\n        assert n == n1, ""n(={}) and n1(={}) should be equal for Depth-wise Convolution "".format(n, n1)\n        self.proj_1x1 = CBR(nIn, n, 1, stride=1, groups=k)\n\n        # (For convenience) Mapping between dilation rate and receptive field for a 3x3 kernel\n        map_receptive_ksize = {3: 1, 5: 2, 7: 3, 9: 4, 11: 5, 13: 6, 15: 7, 17: 8}\n        self.k_sizes = list()\n        for i in range(k):\n            ksize = int(3 + 2 * i)\n            # After reaching the receptive field limit, fall back to the base kernel size of 3 with a dilation rate of 1\n            ksize = ksize if ksize <= r_lim else 3\n            self.k_sizes.append(ksize)\n        # sort (in ascending order) these kernel sizes based on their receptive field\n        # This enables us to ignore the kernels (3x3 in our case) with the same effective receptive field in hierarchical\n        # feature fusion because kernels with 3x3 receptive fields does not have gridding artifact.\n        self.k_sizes.sort()\n        self.spp_dw = nn.ModuleList()\n        for i in range(k):\n            d_rate = map_receptive_ksize[self.k_sizes[i]]\n            self.spp_dw.append(CDilated(n, n, kSize=3, stride=stride, groups=n, d=d_rate))\n        # Performing a group convolution with K groups is the same as performing K point-wise convolutions\n        self.conv_1x1_exp = CB(nOut, nOut, 1, 1, groups=k)\n        self.br_after_cat = BR(nOut)\n        self.module_act = nn.PReLU(nOut)\n        self.downAvg = True if down_method == \'avg\' else False\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n\n        # Reduce --> project high-dimensional feature maps to low-dimensional space\n        output1 = self.proj_1x1(input)\n        output = [self.spp_dw[0](output1)]\n        # compute the output for each branch and hierarchically fuse them\n        # i.e. Split --> Transform --> HFF\n        for k in range(1, len(self.spp_dw)):\n            out_k = self.spp_dw[k](output1)\n            # HFF\n            out_k = out_k + output[k - 1]\n            output.append(out_k)\n        # Merge\n        expanded = self.conv_1x1_exp( # learn linear combinations using group point-wise convolutions\n            self.br_after_cat( # apply batch normalization followed by activation function (PRelu in this case)\n                torch.cat(output, 1) # concatenate the output of different branches\n            )\n        )\n        del output\n        # if down-sampling, then return the concatenated vector\n        # because Downsampling function will combine it with avg. pooled feature map and then threshold it\n        if self.stride == 2 and self.downAvg:\n            return expanded\n\n        # if dimensions of input and concatenated vector are the same, add them (RESIDUAL LINK)\n        if expanded.size() == input.size():\n            expanded = expanded + input\n\n        # Threshold the feature map using activation function (PReLU in this case)\n        return self.module_act(expanded)\n\n\nclass DownSampler(nn.Module):\n    \'\'\'\n    Down-sampling fucntion that has three parallel branches: (1) avg pooling,\n    (2) EESP block with stride of 2 and (3) efficient long-range connection with the input.\n    The output feature maps of branches from (1) and (2) are concatenated and then additively fused with (3) to produce\n    the final output.\n    \'\'\'\n\n    def __init__(self, nin, nout, k=4, r_lim=9, reinf=True):\n        \'\'\'\n            :param nin: number of input channels\n            :param nout: number of output channels\n            :param k: # of parallel branches\n            :param r_lim: A maximum value of receptive field allowed for EESP block\n            :param reinf: Use long range shortcut connection with the input or not.\n        \'\'\'\n        super().__init__()\n        nout_new = nout - nin\n        self.eesp = EESP(nin, nout_new, stride=2, k=k, r_lim=r_lim, down_method=\'avg\')\n        self.avg = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n        if reinf:\n            self.inp_reinf = nn.Sequential(\n                CBR(config_inp_reinf, config_inp_reinf, 3, 1),\n                CB(config_inp_reinf, nout, 1, 1)\n            )\n        self.act =  nn.PReLU(nout)\n\n    def forward(self, input, input2=None):\n        \'\'\'\n        :param input: input feature map\n        :return: feature map down-sampled by a factor of 2\n        \'\'\'\n        avg_out = self.avg(input)\n        eesp_out = self.eesp(input)\n        output = torch.cat([avg_out, eesp_out], 1)\n\n        if input2 is not None:\n            #assuming the input is a square image\n            # Shortcut connection with the input image\n            w1 = avg_out.size(2)\n            while True:\n                input2 = F.avg_pool2d(input2, kernel_size=3, padding=1, stride=2)\n                w2 = input2.size(2)\n                if w2 == w1:\n                    break\n            output = output + self.inp_reinf(input2)\n\n        return self.act(output)\n\nclass EESPNet(nn.Module):\n    \'\'\'\n    This class defines the ESPNetv2 architecture for the ImageNet classification\n    \'\'\'\n\n    def __init__(self, classes=1000, s=1):\n        \'\'\'\n        :param classes: number of classes in the dataset. Default is 1000 for the ImageNet dataset\n        :param s: factor that scales the number of output feature maps\n        \'\'\'\n        super().__init__()\n        reps = [0, 3, 7, 3]  # how many times EESP blocks should be repeated at each spatial level.\n        channels = 3\n\n        r_lim = [13, 11, 9, 7, 5]  # receptive field at each spatial level\n        K = [4]*len(r_lim) # No. of parallel branches at different levels\n\n        base = 32 #base configuration\n        config_len = 5\n        config = [base] * config_len\n        base_s = 0\n        for i in range(config_len):\n            if i== 0:\n                base_s = int(base * s)\n                base_s = math.ceil(base_s / K[0]) * K[0]\n                config[i] = base if base_s > base else base_s\n            else:\n                config[i] = base_s * pow(2, i)\n        if s <= 1.5:\n            config.append(1024)\n        elif s <= 2.0:\n            config.append(1280)\n        else:\n            ValueError(\'Configuration not supported\')\n\n\n        global config_inp_reinf\n        config_inp_reinf = 3\n        self.input_reinforcement = True # True for the shortcut connection with input\n\n        assert len(K) == len(r_lim), \'Length of branching factor array and receptive field array should be the same.\'\n\n        self.level1 = CBR(channels, config[0], 3, 2)  # 112 L1\n\n        self.level2_0 = DownSampler(config[0], config[1], k=K[0], r_lim=r_lim[0], reinf=self.input_reinforcement)  # out = 56\n\n        self.level3_0 = DownSampler(config[1], config[2], k=K[1], r_lim=r_lim[1], reinf=self.input_reinforcement) # out = 28\n        self.level3 = nn.ModuleList()\n        for i in range(reps[1]):\n            self.level3.append(EESP(config[2], config[2], stride=1, k=K[2], r_lim=r_lim[2]))\n\n        self.level4_0 = DownSampler(config[2], config[3], k=K[2], r_lim=r_lim[2], reinf=self.input_reinforcement) #out = 14\n        self.level4 = nn.ModuleList()\n        for i in range(reps[2]):\n            self.level4.append(EESP(config[3], config[3], stride=1, k=K[3], r_lim=r_lim[3]))\n\n        self.level5_0 = DownSampler(config[3], config[4], k=K[3], r_lim=r_lim[3]) #7\n        self.level5 = nn.ModuleList()\n        for i in range(reps[3]):\n            self.level5.append(EESP(config[4], config[4], stride=1, k=K[4], r_lim=r_lim[4]))\n\n        # expand the feature maps using depth-wise convolution followed by group point-wise convolution\n        self.level5.append(CBR(config[4], config[4], 3, 1, groups=config[4]))\n        self.level5.append(CBR(config[4], config[5], 1, 1, groups=K[4]))\n\n        self.classifier = nn.Linear(config[5], classes)\n        self.init_params()\n\n    def init_params(self):\n        \'\'\'\n        Function to initialze the parameters\n        \'\'\'\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal_(m.weight, std=0.001)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n    def forward(self, input, p=0.2):\n        \'\'\'\n        :param input: Receives the input RGB image\n        :return: a C-dimensional vector, C=# of classes\n        \'\'\'\n        out_l1 = self.level1(input)  # 112\n        if not self.input_reinforcement:\n            del input\n            input = None\n\n        out_l2 = self.level2_0(out_l1, input)  # 56\n\n        out_l3_0 = self.level3_0(out_l2, input)  # down-sample\n        for i, layer in enumerate(self.level3):\n            if i == 0:\n                out_l3 = layer(out_l3_0)\n            else:\n                out_l3 = layer(out_l3)\n\n        out_l4_0 = self.level4_0(out_l3, input)  # down-sample\n        for i, layer in enumerate(self.level4):\n            if i == 0:\n                out_l4 = layer(out_l4_0)\n            else:\n                out_l4 = layer(out_l4)\n\n        out_l5_0 = self.level5_0(out_l4)  # down-sample\n        for i, layer in enumerate(self.level5):\n            if i == 0:\n                out_l5 = layer(out_l5_0)\n            else:\n                out_l5 = layer(out_l5)\n\n        output_g = F.adaptive_avg_pool2d(out_l5, output_size=1)\n        output_g = F.dropout(output_g, p=p, training=self.training)\n        output_1x1 = output_g.view(output_g.size(0), -1)\n\n        return self.classifier(output_1x1)\n\n\nif __name__ == \'__main__\':\n    input = torch.Tensor(1, 3, 224, 224).cuda()\n    model = EESPNet(classes=1000, s=1.0)\n    out = model(input)\n    print(\'Output size\')\n    print(out.size())\n'"
imagenet/cnn_utils.py,1,"b'import torch.nn as nn\n\n#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nclass CBR(nn.Module):\n    \'\'\'\n    This class defines the convolution layer with batch normalization and PReLU activation\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, groups=1):\n        \'\'\'\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: stride rate for down-sampling. Default is 1\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False, groups=groups)\n        self.bn = nn.BatchNorm2d(nOut)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        # output = self.conv1(output)\n        output = self.bn(output)\n        output = self.act(output)\n        return output\n\n\nclass BR(nn.Module):\n    \'\'\'\n        This class groups the batch normalization and PReLU activation\n    \'\'\'\n\n    def __init__(self, nOut):\n        \'\'\'\n        :param nOut: output feature maps\n        \'\'\'\n        super().__init__()\n        self.bn = nn.BatchNorm2d(nOut)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: normalized and thresholded feature map\n        \'\'\'\n        output = self.bn(input)\n        output = self.act(output)\n        return output\n\n\nclass CB(nn.Module):\n    \'\'\'\n       This class groups the convolution and batch normalization\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, groups=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optinal stide for down-sampling\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False,\n                              groups=groups)\n        self.bn = nn.BatchNorm2d(nOut)\n\n    def forward(self, input):\n        \'\'\'\n\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        output = self.bn(output)\n        return output\n\n\nclass C(nn.Module):\n    \'\'\'\n    This class is for a convolutional layer.\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, groups=1):\n        \'\'\'\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False,\n                              groups=groups)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        return output\n\n\nclass CDilated(nn.Module):\n    \'\'\'\n    This class defines the dilated convolution.\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1, groups=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        :param d: optional dilation rate\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2) * d\n        self.conv = nn.Conv2d(nIn, nOut,kSize, stride=stride, padding=padding, bias=False,\n                              dilation=d, groups=groups)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        return output\n\nclass CDilatedB(nn.Module):\n    \'\'\'\n    This class defines the dilated convolution with batch normalization.\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1, groups=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        :param d: optional dilation rate\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2) * d\n        self.conv = nn.Conv2d(nIn, nOut,kSize, stride=stride, padding=padding, bias=False,\n                              dilation=d, groups=groups)\n        self.bn = nn.BatchNorm2d(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        return self.bn(self.conv(input))\n'"
imagenet/evaluate.py,9,"b'import argparse\nimport time\nfrom torch.autograd import Variable\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport Model as Net\nimport numpy as np\nfrom utils import *\nimport os\nimport torchvision.models as preModels\n\ncudnn.benchmark = True\n\n\'\'\'\nThis file is mostly adapted from the PyTorch ImageNet example\n\'\'\'\n\n#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\ndef main(args):\n\n    model = Net.EESPNet(classes=1000, s=args.s)\n    model = torch.nn.DataParallel(model).cuda()\n    if not os.path.isfile(args.weightFile):\n        print(\'Weight file does not exist\')\n        exit(-1)\n    dict_model = torch.load(args.weightFile)\n    model.load_state_dict(dict_model)\n\n\n    n_params = sum([np.prod(p.size()) for p in model.parameters()])\n    print(\'Parameters: \' + str(n_params))\n\n    # Data loading code\n    valdir = os.path.join(args.data, \'val\')\n    traindir = os.path.join(args.data, \'train\')\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n\n    val_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(valdir, transforms.Compose([\n            transforms.Resize(int(args.inpSize/0.875)),\n            transforms.CenterCrop(args.inpSize),\n            transforms.ToTensor(),\n            normalize,\n        ])),\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    validate(val_loader, model)\n    return\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'ESPNetv2 Training on the ImageNet\')\n    parser.add_argument(\'--data\', default=\'/home/ubuntu/ILSVRC2015/Data/CLS-LOC/\', help=\'path to dataset\')\n    parser.add_argument(\'--workers\', default=12, type=int, help=\'number of data loading workers (default: 4)\')\n    parser.add_argument(\'-b\', \'--batch-size\', default=512, type=int,\n                        metavar=\'N\', help=\'mini-batch size (default: 256)\')\n    parser.add_argument(\'--print-freq\', \'-p\', default=10, type=int,\n                        metavar=\'N\', help=\'print frequency (default: 10)\')\n    parser.add_argument(\'--s\', default=1, type=float,\n                        help=\'Factor by which output channels should be reduced (s > 1 for increasing the dims while < 1 for decreasing)\')\n    parser.add_argument(\'--weightFile\', type=str, default=\'\', help=\'weight file\')\n    parser.add_argument(\'--inpSize\', default=224, type=int,\n                        help=\'Input size\')\n\n\n    args = parser.parse_args()\n    args.parallel = True\n\n    main(args)\n'"
imagenet/main.py,13,"b'import argparse\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport Model as Net\nimport numpy as np\nfrom utils import *\nimport random\nimport os\nfrom LRSchedule import MyLRScheduler\n\ncudnn.benchmark = True\n\n\n#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\ndef compute_params(model):\n    return sum([np.prod(p.size()) for p in model.parameters()])\n\ndef main(args):\n    best_prec1 = 0.0\n\n    if not os.path.isdir(args.savedir):\n        os.mkdir(args.savedir)\n\n    # create model\n    model = Net.EESPNet(classes=1000, s=args.s)\n    print(\'Network Parameters: \' + str(compute_params(model)))\n\n    #check if the cuda is available or not\n    cuda_available = torch.cuda.is_available()\n\n    num_gpus = torch.cuda.device_count()\n    if num_gpus >= 1:\n        model = torch.nn.DataParallel(model)\n\n    if cuda_available:\n        model = model.cuda()\n\n\n    logFileLoc = args.savedir + \'logs.txt\'\n    if os.path.isfile(logFileLoc):\n        logger = open(logFileLoc, \'a\')\n    else:\n        logger = open(logFileLoc, \'w\')\n        logger.write(""\\n%s\\t%s\\t%s\\t%s\\t%s\\t"" % (\'Epoch\', \'Loss(Tr)\', \'Loss(val)\', \'top1 (tr)\', \'top1 (val\'))\n\n    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n                                momentum=args.momentum,\n                                weight_decay=args.weight_decay, nesterov=True)\n\n    # optionally resume from a checkpoint\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint[\'epoch\']\n            best_prec1 = checkpoint[\'best_prec1\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                  .format(args.resume, checkpoint[\'epoch\']))\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\n    # Data loading code\n    traindir = os.path.join(args.data, \'train\')\n    valdir = os.path.join(args.data, \'val\')\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n\n    train_loader1 = torch.utils.data.DataLoader(\n        datasets.ImageFolder(\n            traindir,\n            transforms.Compose([\n                transforms.RandomResizedCrop(args.inpSize),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                normalize,\n            ])),\n        batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n\n\n    val_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(valdir, transforms.Compose([\n            transforms.Resize(int(args.inpSize/0.875)),\n            transforms.CenterCrop(args.inpSize),\n            transforms.ToTensor(),\n            normalize,\n        ])),\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n   # global customLR\n    # steps at which we should decrease the learning rate\n    step_sizes = [51, 101, 131, 161, 191, 221, 251, 281]\n\n    #ImageNet experiments consume a lot of time\n    # Just for safety, store the checkpoint before we decrease the learning rate\n    # i.e.  store the model at step -1\n    step_store = list()\n    for step in step_sizes:\n        step_store.append(step-1)\n\n\n    customLR = MyLRScheduler(args.lr, 5, step_sizes)\n    #set up the variables in case of resuming\n    if args.start_epoch != 0:\n        for epoch in range(args.start_epoch):\n            customLR.get_lr(epoch)\n\n    for epoch in range(args.start_epoch, args.epochs):\n        lr_log = customLR.get_lr(epoch)\n        # set the optimizer with the learning rate\n        # This can be done inside the MyLRScheduler\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lr_log\n        print(""LR for epoch {} = {:.5f}"".format(epoch, lr_log))\n        train_prec1, train_loss = train(train_loader1, model, optimizer, epoch)\n        # evaluate on validation set\n        val_prec1, val_loss = validate(val_loader, model)\n\n        # remember best prec@1 and save checkpoint\n        is_best = val_prec1.item() > best_prec1\n        best_prec1 = max(val_prec1.item(), best_prec1)\n        back_check = True if epoch in step_store else False #backup checkpoint or not\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'arch\': \'ESPNet\',\n            \'state_dict\': model.state_dict(),\n            \'best_prec1\': best_prec1,\n            \'optimizer\': optimizer.state_dict(),\n        }, is_best, back_check, epoch, args.savedir)\n\n        logger.write(""\\n%d\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.7f"" % (epoch, train_loss, val_loss, train_prec1,\n                                                                       val_prec1, lr_log))\n        logger.flush()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'ESPNetv2 Training on the ImageNet\')\n    parser.add_argument(\'--data\', default=\'/home/ubuntu/ILSVRC2015/Data/CLS-LOC/\', help=\'path to dataset\')\n    parser.add_argument(\'--workers\', default=12, type=int, help=\'number of data loading workers (default: 4)\')\n    parser.add_argument(\'--epochs\', default=300, type=int, help=\'number of total epochs to run\')\n    parser.add_argument(\'--start-epoch\', default=0, type=int, help=\'manual epoch number (useful on restarts)\')\n    parser.add_argument(\'--batch-size\', default=512, type=int, help=\'mini-batch size (default: 512)\')\n    parser.add_argument(\'--lr\', default=0.1, type=float, help=\'initial learning rate\')\n    parser.add_argument(\'--momentum\', default=0.9, type=float, help=\'momentum\')\n    parser.add_argument(\'--weight-decay\', default=4e-5, type=float, help=\'weight decay (default: 4e-5)\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, help=\'path to latest checkpoint (default: none)\')\n    parser.add_argument(\'--savedir\', type=str, default=\'./results\', help=\'Location to save the results\')\n    parser.add_argument(\'--s\', default=1, type=float, help=\'Factor by which output channels should be reduced (s > 1 \'\n                                                           \'for increasing the dims while < 1 for decreasing)\')\n    parser.add_argument(\'--inpSize\', default=224, type=int, help=\'Input image size (default: 224 x 224)\')\n\n\n    args = parser.parse_args()\n    args.parallel = True\n    cudnn.deterministic = True\n    random.seed(1882)\n    torch.manual_seed(1882)\n\n    args.savedir = args.savedir + \'_s_\' + str(args.s) + \'_inp_\' + str(args.inpSize) + os.sep\n\n    main(args)\n'"
imagenet/utils.py,5,"b'import os\nimport torch\nimport shutil\nimport torch.nn.functional as F\nimport time\n\n\n#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\n\'\'\'\nThis file is mostly adapted from the PyTorch ImageNet example\n\'\'\'\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\'\'\'\nUtility to save checkpoint or not\n\'\'\'\ndef save_checkpoint(state, is_best, back_check, epoch, dir):\n    check_pt_file = dir + os.sep + \'checkpoint.pth.tar\'\n    torch.save(state, check_pt_file)\n    if is_best:\n        #We only need best models weight and not check point states, etc.\n        torch.save(state[\'state_dict\'], dir + os.sep + \'model_best.pth\')\n    if back_check:\n        shutil.copyfile(check_pt_file, dir + os.sep + \'checkpoint_back\' + str(epoch) + \'.pth.tar\')\n\n\'\'\'\nCross entropy loss function\n\'\'\'\ndef loss_fn(outputs, labels):\n\n    return F.cross_entropy(outputs, labels)\n\n\'\'\'\nTraining loop\n\'\'\'\ndef train(train_loader, model, optimizer, epoch):\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(train_loader):\n\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        input = input.cuda(non_blocking=True)\n        target = target.cuda(non_blocking=True)\n\n        # compute output\n        output = model(input)\n\n        # compute loss\n        loss = loss_fn(output, target)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n        #losses.update(loss.data[0], input.size(0))\n        losses.update(loss.item(), input.size(0))\n        top1.update(prec1[0], input.size(0))\n        top5.update(prec5[0], input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % 100 == 0: #print after every 100 batches\n            print(""Epoch: %d[%d/%d]\\t\\tBatch Time:%.4f\\t\\tLoss:%.4f\\t\\ttop1:%.4f (%.4f)\\t\\ttop5:%.4f (%.4f)"" %\n                  (epoch, i, len(train_loader), batch_time.avg, losses.avg, top1.val, top1.avg, top5.val, top5.avg))\n\n\n    return top1.avg, losses.avg\n\n\'\'\'\nValidation loop\n\'\'\'\ndef validate(val_loader, model):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    # with torch.no_grad():\n    end = time.time()\n    with torch.no_grad():\n        for i, (input, target) in enumerate(val_loader):\n            input = input.cuda(non_blocking=True)\n            target = target.cuda(non_blocking=True)\n\n            # compute output\n            output = model(input)\n            loss = loss_fn(output, target)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(output, target, topk=(1, 5))\n\n            # replace if using pytorch version < 0.4\n            #losses.update(loss.data[0], input.size(0))\n            losses.update(loss.item(), input.size(0))\n            top1.update(prec1[0], input.size(0))\n            top5.update(prec5[0], input.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % 100 == 0: # print after every 100 batches\n                print(""Batch:[%d/%d]\\t\\tBatchTime:%.3f\\t\\tLoss:%.3f\\t\\ttop1:%.3f (%.3f)\\t\\ttop5:%.3f(%.3f)"" %\n                      (i, len(val_loader), batch_time.avg, losses.avg, top1.val, top1.avg, top5.val, top5.avg))\n\n        print(\' * Prec@1:%.3f Prec@5:%.3f\' % (top1.avg, top5.avg))\n\n        return top1.avg, losses.avg'"
segmentation/DataSet.py,2,"b'import cv2\nimport torch.utils.data\nimport numpy as np\n\n\n#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\n\nclass MyDataset(torch.utils.data.Dataset):\n    \'\'\'\n    Class to load the dataset\n    \'\'\'\n    def __init__(self, imList, labelList, transform=None):\n        \'\'\'\n        :param imList: image list (Note that these lists have been processed and pickled using the loadData.py)\n        :param labelList: label list (Note that these lists have been processed and pickled using the loadData.py)\n        :param transform: Type of transformation. SEe Transforms.py for supported transformations\n        \'\'\'\n        self.imList = imList\n        self.labelList = labelList\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.imList)\n\n    def __getitem__(self, idx):\n        \'\'\'\n\n        :param idx: Index of the image file\n        :return: returns the image and corresponding label file.\n        \'\'\'\n        image_name = self.imList[idx]\n        label_name = self.labelList[idx]\n        image = cv2.imread(image_name)\n        label = cv2.imread(label_name, 0)\n        # if you have 255 label in your label files, map it to the background class (19) in the Cityscapes dataset\n        if 255 in np.unique(label):\n            label[label==255] = 19\n\n        if self.transform:\n            [image, label] = self.transform(image, label)\n        return (image, label)\n'"
segmentation/IOUEval.py,0,"b'import numpy as np\n\n#adapted from https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/score.py\n\n#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nclass iouEval:\n    def __init__(self, nClasses):\n        self.nClasses = nClasses\n        self.reset()\n\n    def reset(self):\n        self.overall_acc = 0\n        self.per_class_acc = np.zeros(self.nClasses, dtype=np.float32)\n        self.per_class_iu = np.zeros(self.nClasses, dtype=np.float32)\n        self.mIOU = 0\n        self.batchCount = 1\n\n    def fast_hist(self, a, b):\n        k = (a >= 0) & (a < self.nClasses)\n        return np.bincount(self.nClasses * a[k].astype(int) + b[k], minlength=self.nClasses ** 2).reshape(self.nClasses, self.nClasses)\n\n    def compute_hist(self, predict, gth):\n        hist = self.fast_hist(gth, predict)\n        return hist\n\n    def addBatch(self, predict, gth):\n        predict = predict.cpu().numpy().flatten()\n        gth = gth.cpu().numpy().flatten()\n\n        epsilon = 0.00000001\n        hist = self.compute_hist(predict, gth)\n        overall_acc = np.diag(hist).sum() / (hist.sum() + epsilon)\n        per_class_acc = np.diag(hist) / (hist.sum(1) + epsilon)\n        per_class_iu = np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist) + epsilon)\n        mIou = np.nanmean(per_class_iu)\n\n        self.overall_acc +=overall_acc\n        self.per_class_acc += per_class_acc\n        self.per_class_iu += per_class_iu\n        self.mIOU += mIou\n        self.batchCount += 1\n\n    def getMetric(self):\n        overall_acc = self.overall_acc/self.batchCount\n        per_class_acc = self.per_class_acc / self.batchCount\n        per_class_iu = self.per_class_iu / self.batchCount\n        mIOU = self.mIOU / self.batchCount\n\n        return overall_acc, per_class_acc, per_class_iu, mIOU'"
segmentation/Transforms.py,3,"b'import numpy as np\nimport torch\nimport random\nimport cv2\n\n\n#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\n\nclass Scale(object):\n    """"""\n    Randomly crop and resize the given PIL image with a probability of 0.5\n    """"""\n    def __init__(self, wi, he):\n        \'\'\'\n\n        :param wi: width after resizing\n        :param he: height after reszing\n        \'\'\'\n        self.w = wi\n        self.h = he\n\n    def __call__(self, img, label):\n        \'\'\'\n        :param img: RGB image\n        :param label: semantic label image\n        :return: resized images\n        \'\'\'\n        #bilinear interpolation for RGB image\n        img = cv2.resize(img, (self.w, self.h))\n        # nearest neighbour interpolation for label image\n        label = cv2.resize(label, (self.w, self.h), interpolation=cv2.INTER_NEAREST)\n\n        return [img, label]\n\n\n\nclass RandomCropResize(object):\n    """"""\n    Randomly crop and resize the given PIL image with a probability of 0.5\n    """"""\n    def __init__(self, size):\n        \'\'\'\n        :param crop_area: area to be cropped (this is the max value and we select between o and crop area\n        \'\'\'\n        self.size = size\n\n    def __call__(self, img, label):\n        h, w = img.shape[:2]\n        x1 = random.randint(0, int(w*0.1)) # 25% to 10%\n        y1 = random.randint(0, int(h*0.1))\n\n        img_crop = img[y1:h-y1, x1:w-x1]\n        label_crop = label[y1:h-y1, x1:w-x1]\n\n        img_crop = cv2.resize(img_crop, self.size)\n        label_crop = cv2.resize(label_crop, self.size, interpolation=cv2.INTER_NEAREST)\n        return img_crop, label_crop\n\nclass RandomCrop(object):\n    \'\'\'\n    This class if for random cropping\n    \'\'\'\n    def __init__(self, cropArea):\n        \'\'\'\n        :param cropArea: amount of cropping (in pixels)\n        \'\'\'\n        self.crop = cropArea\n\n    def __call__(self, img, label):\n\n        if random.random() < 0.5:\n            h, w = img.shape[:2]\n            img_crop = img[self.crop:h-self.crop, self.crop:w-self.crop]\n            label_crop = label[self.crop:h-self.crop, self.crop:w-self.crop]\n            return img_crop, label_crop\n        else:\n            return [img, label]\n\n\n\nclass RandomFlip(object):\n    """"""Randomly horizontally flips the given PIL.Image with a probability of 0.5\n    """"""\n\n    def __call__(self, image, label):\n        if random.random() < 0.5:\n            x1 = 0#random.randint(0, 1) #if you want to do vertical flip, uncomment this line\n            if x1 == 0:\n                image = cv2.flip(image, 0) # horizontal flip\n                label = cv2.flip(label, 0) # horizontal flip\n            else:\n                image = cv2.flip(image, 1) # veritcal flip\n                label = cv2.flip(label, 1)  # veritcal flip\n        return [image, label]\n\n\nclass Normalize(object):\n    """"""Given mean: (R, G, B) and std: (R, G, B),\n    will normalize each channel of the torch.*Tensor, i.e.\n    channel = (channel - mean) / std\n    """"""\n\n    def __init__(self, mean, std):\n        \'\'\'\n        :param mean: global mean computed from dataset\n        :param std: global std computed from dataset\n        \'\'\'\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, image, label):\n        image = image.astype(np.float32)\n        for i in range(3):\n            image[:,:,i] -= self.mean[i]\n        for i in range(3):\n            image[:,:, i] /= self.std[i]\n\n        return [image, label]\n\nclass ToTensor(object):\n    \'\'\'\n    This class converts the data to tensor so that it can be processed by PyTorch\n    \'\'\'\n    def __init__(self, scale=1):\n        \'\'\'\n        :param scale: ESPNet-C\'s output is 1/8th of original image size, so set this parameter accordingly\n        \'\'\'\n        self.scale = scale # original images are 2048 x 1024\n\n    def __call__(self, image, label):\n\n        if self.scale != 1:\n            h, w = label.shape[:2]\n            image = cv2.resize(image, (int(w), int(h)))\n            label = cv2.resize(label, (int(w/self.scale), int(h/self.scale)), interpolation=cv2.INTER_NEAREST)\n\n        image = image.transpose((2,0,1))\n\n        image_tensor = torch.from_numpy(image).div(255)\n        label_tensor =  torch.LongTensor(np.array(label, dtype=np.int)) #torch.from_numpy(label)\n\n        return [image_tensor, label_tensor]\n\nclass Compose(object):\n    """"""Composes several transforms together.\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, *args):\n        for t in self.transforms:\n            args = t(*args)\n        return args\n'"
segmentation/gen_cityscapes.py,3,"b'import numpy as np\nimport torch\nimport glob\n\nimport cv2\nimport os\nfrom argparse import ArgumentParser\nfrom cnn import SegmentationModel as net\nfrom torch import nn\n\n\n#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\npallete = [[128, 64, 128],\n           [244, 35, 232],\n           [70, 70, 70],\n           [102, 102, 156],\n           [190, 153, 153],\n           [153, 153, 153],\n           [250, 170, 30],\n           [220, 220, 0],\n           [107, 142, 35],\n           [152, 251, 152],\n           [70, 130, 180],\n           [220, 20, 60],\n           [255, 0, 0],\n           [0, 0, 142],\n           [0, 0, 70],\n           [0, 60, 100],\n           [0, 80, 100],\n           [0, 0, 230],\n           [119, 11, 32],\n           [0, 0, 0]]\n\n\ndef relabel(img):\n    \'\'\'\n    This function relabels the predicted labels so that cityscape dataset can process\n    :param img:\n    :return:\n    \'\'\'\n    img[img == 19] = 255\n    img[img == 18] = 33\n    img[img == 17] = 32\n    img[img == 16] = 31\n    img[img == 15] = 28\n    img[img == 14] = 27\n    img[img == 13] = 26\n    img[img == 12] = 25\n    img[img == 11] = 24\n    img[img == 10] = 23\n    img[img == 9] = 22\n    img[img == 8] = 21\n    img[img == 7] = 20\n    img[img == 6] = 19\n    img[img == 5] = 17\n    img[img == 4] = 13\n    img[img == 3] = 12\n    img[img == 2] = 11\n    img[img == 1] = 8\n    img[img == 0] = 7\n    img[img == 255] = 0\n    return img\n\n\ndef evaluateModel(args, model, image_list):\n    # gloabl mean and std values\n    mean = [72.3923111, 82.90893555, 73.15840149]\n    std = [45.3192215, 46.15289307, 44.91483307]\n\n    model.eval()\n    for i, imgName in enumerate(image_list):\n        img = cv2.imread(imgName)\n        if args.overlay:\n            img_orig = np.copy(img)\n\n        img = img.astype(np.float32)\n        for j in range(3):\n            img[:, :, j] -= mean[j]\n        for j in range(3):\n            img[:, :, j] /= std[j]\n\n        # resize the image to 1024x512x3\n        img = cv2.resize(img, (args.inWidth, args.inHeight))\n        if args.overlay:\n            img_orig = cv2.resize(img_orig, (args.inWidth, args.inHeight))\n\n        img /= 255\n        img = img.transpose((2, 0, 1))\n        img_tensor = torch.from_numpy(img)\n        img_tensor = torch.unsqueeze(img_tensor, 0)  # add a batch dimension\n        if args.gpu:\n            img_tensor = img_tensor.cuda()\n        img_out = model(img_tensor)\n\n        classMap_numpy = img_out[0].max(0)[1].byte().cpu().data.numpy()\n        # upsample the feature maps to the same size as the input image using Nearest neighbour interpolation\n        # upsample the feature map from 1024x512 to 2048x1024\n        classMap_numpy = cv2.resize(classMap_numpy, (args.inWidth*2, args.inHeight*2), interpolation=cv2.INTER_NEAREST)\n        if i % 100 == 0 and i > 0:\n            print(\'Processed [{}/{}]\'.format(i, len(image_list)))\n\n        name = imgName.split(\'/\')[-1]\n\n        if args.colored:\n            classMap_numpy_color = np.zeros((img.shape[1], img.shape[2], img.shape[0]), dtype=np.uint8)\n            for idx in range(len(pallete)):\n                [r, g, b] = pallete[idx]\n                classMap_numpy_color[classMap_numpy == idx] = [b, g, r]\n            cv2.imwrite(args.savedir + os.sep + \'c_\' + name.replace(args.img_extn, \'png\'), classMap_numpy_color)\n            if args.overlay:\n                overlayed = cv2.addWeighted(img_orig, 0.5, classMap_numpy_color, 0.5, 0)\n                cv2.imwrite(args.savedir + os.sep + \'over_\' + name.replace(args.img_extn, \'jpg\'), overlayed)\n\n        if args.cityFormat:\n            classMap_numpy = relabel(classMap_numpy.astype(np.uint8))\n\n\n        cv2.imwrite(args.savedir + os.sep + name.replace(args.img_extn, \'png\'), classMap_numpy)\n\n\ndef main(args):\n    # read all the images in the folder\n    image_list = glob.glob(args.data_dir + os.sep + \'*.\' + args.img_extn)\n\n    modelA = net.EESPNet_Seg(args.classes, s=args.s)\n    if not os.path.isfile(args.pretrained):\n        print(\'Pre-trained model file does not exist. Please check ./pretrained_models folder\')\n        exit(-1)\n    modelA = nn.DataParallel(modelA)\n    modelA.load_state_dict(torch.load(args.pretrained))\n    if args.gpu:\n        modelA = modelA.cuda()\n\n    # set to evaluation mode\n    modelA.eval()\n\n    if not os.path.isdir(args.savedir):\n        os.mkdir(args.savedir)\n\n    evaluateModel(args, modelA, image_list)\n\n\nif __name__ == \'__main__\':\n    parser = ArgumentParser()\n    parser.add_argument(\'--model\', default=""ESPNetv2"", help=\'Model name\')\n    parser.add_argument(\'--data_dir\', default=""./data"", help=\'Data directory\')\n    parser.add_argument(\'--img_extn\', default=""png"", help=\'RGB Image format\')\n    parser.add_argument(\'--inWidth\', type=int, default=1024, help=\'Width of RGB image\')\n    parser.add_argument(\'--inHeight\', type=int, default=512, help=\'Height of RGB image\')\n    parser.add_argument(\'--savedir\', default=\'./results\', help=\'directory to save the results\')\n    parser.add_argument(\'--gpu\', default=True, type=bool, help=\'Run on CPU or GPU. If TRUE, then GPU.\')\n    parser.add_argument(\'--pretrained\', default=\'\', help=\'Pretrained weights directory.\')\n    parser.add_argument(\'--s\', default=0.5, type=float, help=\'scale\')\n    parser.add_argument(\'--cityFormat\', default=True, type=bool, help=\'If you want to convert to cityscape \'\n                                                                       \'original label ids\')\n    parser.add_argument(\'--colored\', default=False, type=bool, help=\'If you want to visualize the \'\n                                                                   \'segmentation masks in color\')\n    parser.add_argument(\'--overlay\', default=False, type=bool, help=\'If you want to visualize the \'\n                                                                   \'segmentation masks overlayed on top of RGB image\')\n    parser.add_argument(\'--classes\', default=20, type=int, help=\'Number of classes in the dataset. 20 for Cityscapes\')\n\n    args = parser.parse_args()\n    if args.overlay:\n        args.colored = True # This has to be true if you want to overlay\n    main(args)\n'"
segmentation/loadData.py,0,"b'import numpy as np\nimport cv2\nimport pickle\n\n\n#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nclass LoadData:\n    \'\'\'\n    Class to laod the data\n    \'\'\'\n    def __init__(self, data_dir, classes, cached_data_file, normVal=1.10):\n        \'\'\'\n        :param data_dir: directory where the dataset is kept\n        :param classes: number of classes in the dataset\n        :param cached_data_file: location where cached file has to be stored\n        :param normVal: normalization value, as defined in ERFNet paper\n        \'\'\'\n        self.data_dir = data_dir\n        self.classes = classes\n        self.classWeights = np.ones(self.classes, dtype=np.float32)\n        self.normVal = normVal\n        self.mean = np.zeros(3, dtype=np.float32)\n        self.std = np.zeros(3, dtype=np.float32)\n        self.trainImList = list()\n        self.valImList = list()\n        self.trainAnnotList = list()\n        self.valAnnotList = list()\n        self.cached_data_file = cached_data_file\n\n    def compute_class_weights(self, histogram):\n        \'\'\'\n        Helper function to compute the class weights\n        :param histogram: distribution of class samples\n        :return: None, but updates the classWeights variable\n        \'\'\'\n        normHist = histogram / np.sum(histogram)\n        for i in range(self.classes):\n            self.classWeights[i] = 1 / (np.log(self.normVal + normHist[i]))\n\n    def readFile(self, fileName, trainStg=False):\n        \'\'\'\n        Function to read the data\n        :param fileName: file that stores the image locations\n        :param trainStg: if processing training or validation data\n        :return: 0 if successful\n        \'\'\'\n        if trainStg == True:\n            global_hist = np.zeros(self.classes, dtype=np.float32)\n\n        no_files = 0\n        min_val_al = 0\n        max_val_al = 0\n        with open(self.data_dir + \'/\' + fileName, \'r\') as textFile:\n            for line in textFile:\n                # we expect the text file to contain the data in following format\n                # <RGB Image>, <Label Image>\n                line_arr = line.split(\',\')\n                img_file = ((self.data_dir).strip() + \'/\' + line_arr[0].strip()).strip()\n                label_file = ((self.data_dir).strip() + \'/\' + line_arr[1].strip()).strip()\n                label_img = cv2.imread(label_file, 0)\n                unique_values = np.unique(label_img)\n                # if you have 255 label in your label files, map it to the background class (19) in the Cityscapes dataset\n                if 255 in unique_values:\n\t                label_img[label_img==255] = 19\n                \n                max_val = max(unique_values)\n                min_val = min(unique_values)\n\n                max_val_al = max(max_val, max_val_al)\n                min_val_al = min(min_val, min_val_al)\n\n                if trainStg == True:\n                    hist = np.histogram(label_img, self.classes)\n                    global_hist += hist[0]\n\n                    rgb_img = cv2.imread(img_file)\n                    self.mean[0] += np.mean(rgb_img[:,:,0])\n                    self.mean[1] += np.mean(rgb_img[:, :, 1])\n                    self.mean[2] += np.mean(rgb_img[:, :, 2])\n\n                    self.std[0] += np.std(rgb_img[:, :, 0])\n                    self.std[1] += np.std(rgb_img[:, :, 1])\n                    self.std[2] += np.std(rgb_img[:, :, 2])\n\n                    self.trainImList.append(img_file)\n                    self.trainAnnotList.append(label_file)\n                else:\n                    self.valImList.append(img_file)\n                    self.valAnnotList.append(label_file)\n\n                if max_val > (self.classes - 1) or min_val < 0:\n                    print(\'Labels can take value between 0 and number of classes {}.\'.format(self.classes-1))\n                    print(\'You have following values as class labels:\')\n                    print(unique_values)\n                    print(\'Some problem with labels. Please check image file: {}\'.format(label_file))\n                    print(\'Exiting!!\')\n                    exit()\n                no_files += 1\n\n        if trainStg == True:\n            # divide the mean and std values by the sample space size\n            self.mean /= no_files\n            self.std /= no_files\n\n            #compute the class imbalance information\n            self.compute_class_weights(global_hist)\n        return 0\n\n    def processData(self):\n        \'\'\'\n        main.py calls this function\n        We expect train.txt and val.txt files to be inside the data directory.\n        :return:\n        \'\'\'\n        print(\'Processing training data\')\n        return_val = self.readFile(\'train.txt\', True)\n\n        print(\'Processing validation data\')\n        return_val1 = self.readFile(\'val.txt\')\n\n        print(\'Pickling data\')\n        if return_val == 0 and return_val1 == 0:\n            data_dict = dict()\n            data_dict[\'trainIm\'] = self.trainImList\n            data_dict[\'trainAnnot\'] = self.trainAnnotList\n            data_dict[\'valIm\'] = self.valImList\n            data_dict[\'valAnnot\'] = self.valAnnotList\n\n            data_dict[\'mean\'] = self.mean\n            data_dict[\'std\'] = self.std\n            data_dict[\'classWeights\'] = self.classWeights\n\n            pickle.dump(data_dict, open(self.cached_data_file, ""wb""))\n            return data_dict\n        return None\n\n\n\n\n'"
segmentation/main.py,17,"b'import loadData as ld\nimport os\nimport torch\nimport pickle\nfrom cnn import SegmentationModel as net\nimport torch.backends.cudnn as cudnn\nimport Transforms as myTransforms\nimport DataSet as myDataLoader\nfrom argparse import ArgumentParser\nfrom train_utils import train, val, netParams, save_checkpoint, poly_lr_scheduler\nimport torch.optim.lr_scheduler\n\n\n#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\ndef trainValidateSegmentation(args):\n    \'\'\'\n    Main function for trainign and validation\n    :param args: global arguments\n    :return: None\n    \'\'\'\n\n    # load the model\n    cuda_available = torch.cuda.is_available()\n    num_gpus = torch.cuda.device_count()\n    model = net.EESPNet_Seg(args.classes, s=args.s, pretrained=args.pretrained, gpus=num_gpus)\n\n    if num_gpus >= 1:\n        model = torch.nn.DataParallel(model)\n\n    args.savedir = args.savedir + str(args.s) + \'/\'\n\n    # create the directory if not exist\n    if not os.path.exists(args.savedir):\n        os.mkdir(args.savedir)\n\n    # check if processed data file exists or not\n    if not os.path.isfile(args.cached_data_file):\n        dataLoad = ld.LoadData(args.data_dir, args.classes, args.cached_data_file)\n        data = dataLoad.processData()\n        if data is None:\n            print(\'Error while pickling data. Please check.\')\n            exit(-1)\n    else:\n        data = pickle.load(open(args.cached_data_file, ""rb""))\n\n\n\n    if cuda_available:\n        args.onGPU = True\n        model = model.cuda()\n\n    total_paramters = netParams(model)\n    print(\'Total network parameters: \' + str(total_paramters))\n\n    # define optimization criteria\n    weight = torch.from_numpy(data[\'classWeights\']) # convert the numpy array to torch\n    if args.onGPU:\n        weight = weight.cuda()\n\n    criteria = torch.nn.CrossEntropyLoss(weight) #weight\n\n    if args.onGPU:\n        criteria = criteria.cuda()\n\n    print(\'Data statistics\')\n    print(data[\'mean\'], data[\'std\'])\n    print(data[\'classWeights\'])\n\n    #compose the data with transforms\n    trainDataset_main = myTransforms.Compose([\n        myTransforms.Normalize(mean=data[\'mean\'], std=data[\'std\']),\n        myTransforms.RandomCropResize(size=(args.inWidth, args.inHeight)),\n        myTransforms.RandomFlip(),\n        #myTransforms.RandomCrop(64).\n        myTransforms.ToTensor(args.scaleIn),\n        #\n    ])\n\n    trainDataset_scale1 = myTransforms.Compose([\n        myTransforms.Normalize(mean=data[\'mean\'], std=data[\'std\']),\n        myTransforms.RandomCropResize(size=(int(args.inWidth*1.5), int(1.5*args.inHeight))),\n        myTransforms.RandomFlip(),\n        myTransforms.ToTensor(args.scaleIn),\n        #\n    ])\n\n    trainDataset_scale2 = myTransforms.Compose([\n        myTransforms.Normalize(mean=data[\'mean\'], std=data[\'std\']),\n        myTransforms.RandomCropResize(size=(int(args.inWidth*1.25), int(1.25*args.inHeight))), # 1536, 768\n        myTransforms.RandomFlip(),\n        myTransforms.ToTensor(args.scaleIn),\n        #\n    ])\n\n    trainDataset_scale3 = myTransforms.Compose([\n        myTransforms.Normalize(mean=data[\'mean\'], std=data[\'std\']),\n        myTransforms.RandomCropResize(size=(int(args.inWidth*0.75), int(0.75*args.inHeight))),\n        myTransforms.RandomFlip(),\n        myTransforms.ToTensor(args.scaleIn),\n        #\n    ])\n\n    trainDataset_scale4 = myTransforms.Compose([\n        myTransforms.Normalize(mean=data[\'mean\'], std=data[\'std\']),\n        myTransforms.RandomCropResize(size=(int(args.inWidth*0.5), int(0.5*args.inHeight))),\n        myTransforms.RandomFlip(),\n        myTransforms.ToTensor(args.scaleIn),\n        #\n    ])\n\n\n    valDataset = myTransforms.Compose([\n        myTransforms.Normalize(mean=data[\'mean\'], std=data[\'std\']),\n        myTransforms.Scale(1024, 512),\n        myTransforms.ToTensor(args.scaleIn),\n        #\n    ])\n\n    # since we training from scratch, we create data loaders at different scales\n    # so that we can generate more augmented data and prevent the network from overfitting\n\n    trainLoader = torch.utils.data.DataLoader(\n        myDataLoader.MyDataset(data[\'trainIm\'], data[\'trainAnnot\'], transform=trainDataset_main),\n        batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n\n    trainLoader_scale1 = torch.utils.data.DataLoader(\n        myDataLoader.MyDataset(data[\'trainIm\'], data[\'trainAnnot\'], transform=trainDataset_scale1),\n        batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n\n    trainLoader_scale2 = torch.utils.data.DataLoader(\n        myDataLoader.MyDataset(data[\'trainIm\'], data[\'trainAnnot\'], transform=trainDataset_scale2),\n        batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n\n    trainLoader_scale3 = torch.utils.data.DataLoader(\n        myDataLoader.MyDataset(data[\'trainIm\'], data[\'trainAnnot\'], transform=trainDataset_scale3),\n        batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n\n    trainLoader_scale4 = torch.utils.data.DataLoader(\n        myDataLoader.MyDataset(data[\'trainIm\'], data[\'trainAnnot\'], transform=trainDataset_scale4),\n        batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, pin_memory=True)\n\n    valLoader = torch.utils.data.DataLoader(\n        myDataLoader.MyDataset(data[\'valIm\'], data[\'valAnnot\'], transform=valDataset),\n        batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n\n    if args.onGPU:\n        cudnn.benchmark = True\n\n    start_epoch = 0\n    best_val = 0\n    lr = args.lr\n\n    optimizer = torch.optim.Adam(model.parameters(), lr, (0.9, 0.999), eps=1e-08, weight_decay=5e-4)\n    # we step the loss by 2 after step size is reached\n    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.step_loss, gamma=0.5)\n\n\n\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            start_epoch = checkpoint[\'epoch\']\n            best_val = checkpoint[\'best_val\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print(""=> loaded checkpoint \'{}\' (epoch {})""\n                .format(args.resume, checkpoint[\'epoch\']))\n        else:\n            print(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\n\n    logFileLoc = args.savedir + args.logFile\n    if os.path.isfile(logFileLoc):\n        logger = open(logFileLoc, \'a\')\n    else:\n        logger = open(logFileLoc, \'w\')\n        logger.write(""Parameters: %s"" % (str(total_paramters)))\n        logger.write(""\\n%s\\t%s\\t%s\\t%s\\t%s\\t"" % (\'Epoch\', \'Loss(Tr)\', \'Loss(val)\', \'mIOU (tr)\', \'mIOU (val\'))\n    logger.flush()\n\n\n\n    for epoch in range(start_epoch, args.max_epochs):\n\n        #scheduler.step(epoch)\n        poly_lr_scheduler(args, optimizer, epoch)\n        lr = 0\n        for param_group in optimizer.param_groups:\n            lr = param_group[\'lr\']\n        print(""Learning rate: "" +  str(lr))\n\n        # train for one epoch\n        # We consider 1 epoch with all the training data (at different scales)\n        train(args, trainLoader_scale1, model, criteria, optimizer, epoch)\n        train(args, trainLoader_scale2, model, criteria, optimizer, epoch)\n        train(args, trainLoader_scale4, model, criteria, optimizer, epoch)\n        train(args, trainLoader_scale3, model, criteria, optimizer, epoch)\n        lossTr, overall_acc_tr, per_class_acc_tr, per_class_iu_tr, mIOU_tr = train(args, trainLoader, model, criteria, optimizer, epoch)\n\n        # evaluate on validation set\n        lossVal, overall_acc_val, per_class_acc_val, per_class_iu_val, mIOU_val = val(args, valLoader, model, criteria)\n\n        is_best = mIOU_val > best_val\n        best_val = max(mIOU_val, best_val)\n        \n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'state_dict\': model.state_dict(),\n            \'optimizer\': optimizer.state_dict(),\n            \'lr\': lr,\n            \'best_val\': best_val,\n        }, args.savedir + \'checkpoint.pth.tar\')\n\n        #save the model also\n        if is_best:\n            model_file_name = args.savedir + os.sep + \'model_best.pth\'\n            torch.save(model.state_dict(), model_file_name)\n\n        with open(args.savedir + \'acc_\' + str(epoch) + \'.txt\', \'w\') as log:\n            log.write(""\\nEpoch: %d\\t Overall Acc (Tr): %.4f\\t Overall Acc (Val): %.4f\\t mIOU (Tr): %.4f\\t mIOU (Val): %.4f"" % (epoch, overall_acc_tr, overall_acc_val, mIOU_tr, mIOU_val))\n            log.write(\'\\n\')\n            log.write(\'Per Class Training Acc: \' + str(per_class_acc_tr))\n            log.write(\'\\n\')\n            log.write(\'Per Class Validation Acc: \' + str(per_class_acc_val))\n            log.write(\'\\n\')\n            log.write(\'Per Class Training mIOU: \' + str(per_class_iu_tr))\n            log.write(\'\\n\')\n            log.write(\'Per Class Validation mIOU: \' + str(per_class_iu_val))\n\n        logger.write(""\\n%d\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.7f"" % (epoch, lossTr, lossVal, mIOU_tr, mIOU_val, lr))\n        logger.flush()\n        print(""Epoch : "" + str(epoch) + \' Details\')\n        print(""\\nEpoch No.: %d\\tTrain Loss = %.4f\\tVal Loss = %.4f\\t mIOU(tr) = %.4f\\t mIOU(val) = %.4f"" % (epoch, lossTr, lossVal, mIOU_tr, mIOU_val))\n    logger.close()\n\n\nif __name__ == \'__main__\':\n\n    parser = ArgumentParser()\n    parser.add_argument(\'--model\', default=""ESPNetv2"", help=\'Model name\')\n    parser.add_argument(\'--data_dir\', default=""./city"", help=\'Data directory\')\n    parser.add_argument(\'--inWidth\', type=int, default=1024, help=\'Width of RGB image\')\n    parser.add_argument(\'--inHeight\', type=int, default=512, help=\'Height of RGB image\')\n    parser.add_argument(\'--scaleIn\', type=int, default=1, help=\'For ESPNet-C, scaleIn=8. For ESPNet, scaleIn=1\')\n    parser.add_argument(\'--max_epochs\', type=int, default=300, help=\'Max. number of epochs\')\n    parser.add_argument(\'--num_workers\', type=int, default=12, help=\'No. of parallel threads\')\n    parser.add_argument(\'--batch_size\', type=int, default=10, help=\'Batch size. 12 for ESPNet-C and 6 for ESPNet. \'\n                                                                   \'Change as per the GPU memory\')\n    parser.add_argument(\'--step_loss\', type=int, default=100, help=\'Decrease learning rate after how many epochs.\')\n    parser.add_argument(\'--lr\', type=float, default=5e-4, help=\'Initial learning rate\')\n    parser.add_argument(\'--savedir\', default=\'./results_espnetv2_\', help=\'directory to save the results\')\n    parser.add_argument(\'--resume\', type=str, default=\'\', help=\'Use this flag to load last checkpoint for training\')  #\n    parser.add_argument(\'--classes\', type=int, default=20, help=\'No of classes in the dataset. 20 for cityscapes\')\n    parser.add_argument(\'--cached_data_file\', default=\'city.p\', help=\'Cached file name\')\n    parser.add_argument(\'--logFile\', default=\'trainValLog.txt\', help=\'File that stores the training and validation logs\')\n    parser.add_argument(\'--pretrained\', default=\'\', help=\'Pretrained ESPNetv2 weights.\')\n    parser.add_argument(\'--s\', default=1, type=float, help=\'scaling parameter\')\n\n    trainValidateSegmentation(parser.parse_args())\n\n'"
segmentation/train_utils.py,4,"b'#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nfrom IOUEval import iouEval\nimport time\nimport torch\nimport numpy as np\n\n\ndef poly_lr_scheduler(args, optimizer, epoch, power=0.9):\n    lr = round(args.lr * (1 - epoch / args.max_epochs) ** power, 8)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n    return lr\n\ndef val(args, val_loader, model, criterion):\n    \'\'\'\n    :param args: general arguments\n    :param val_loader: loaded for validation dataset\n    :param model: model\n    :param criterion: loss function\n    :return: average epoch loss, overall pixel-wise accuracy, per class accuracy, per class iu, and mIOU\n    \'\'\'\n    #switch to evaluation mode\n    model.eval()\n\n    iouEvalVal = iouEval(args.classes)\n\n    epoch_loss = []\n\n    total_batches = len(val_loader)\n    for i, (input, target) in enumerate(val_loader):\n        start_time = time.time()\n\n        if args.onGPU:\n            input = input.cuda(non_blocking=True) #torch.autograd.Variable(input, volatile=True)\n            target = target.cuda(non_blocking=True)#torch.autograd.Variable(target, volatile=True)\n\n        # run the mdoel\n        output1 = model(input)\n\n        # compute the loss\n        loss = criterion(output1, target)\n\n        epoch_loss.append(loss.item())\n\n        time_taken = time.time() - start_time\n\n        # compute the confusion matrix\n        iouEvalVal.addBatch(output1.max(1)[1].data, target.data)\n\n        print(\'[%d/%d] loss: %.3f time: %.2f\' % (i, total_batches, loss.item(), time_taken))\n\n    average_epoch_loss_val = sum(epoch_loss) / len(epoch_loss)\n\n    overall_acc, per_class_acc, per_class_iu, mIOU = iouEvalVal.getMetric()\n\n    return average_epoch_loss_val, overall_acc, per_class_acc, per_class_iu, mIOU\n\ndef train(args, train_loader, model, criterion, optimizer, epoch):\n    \'\'\'\n    :param args: general arguments\n    :param train_loader: loaded for training dataset\n    :param model: model\n    :param criterion: loss function\n    :param optimizer: optimization algo, such as ADAM or SGD\n    :param epoch: epoch number\n    :return: average epoch loss, overall pixel-wise accuracy, per class accuracy, per class iu, and mIOU\n    \'\'\'\n    # switch to train mode\n    model.train()\n\n    iouEvalTrain = iouEval(args.classes)\n\n    epoch_loss = []\n\n    total_batches = len(train_loader)\n    for i, (input, target) in enumerate(train_loader):\n        start_time = time.time()\n\n        if args.onGPU:\n            input = input.cuda(non_blocking=True) #torch.autograd.Variable(input, volatile=True)\n            target = target.cuda(non_blocking=True)\n\n        #run the mdoel\n        output1, output2 = model(input)\n\n        #set the grad to zero\n        optimizer.zero_grad()\n        loss1 = criterion(output1, target)\n        loss2 = criterion(output2, target)\n        loss = loss1 + loss2\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss.append(loss.item())\n        time_taken = time.time() - start_time\n\n        #compute the confusion matrix\n        iouEvalTrain.addBatch(output1.max(1)[1].data, target.data)\n\n        print(\'[%d/%d] loss: %.3f time:%.2f\' % (i, total_batches, loss.item(), time_taken))\n\n    average_epoch_loss_train = sum(epoch_loss) / len(epoch_loss)\n\n    overall_acc, per_class_acc, per_class_iu, mIOU = iouEvalTrain.getMetric()\n\n    return average_epoch_loss_train, overall_acc, per_class_acc, per_class_iu, mIOU\n\ndef save_checkpoint(state, filenameCheckpoint=\'checkpoint.pth.tar\'):\n    \'\'\'\n    helper function to save the checkpoint\n    :param state: model state\n    :param filenameCheckpoint: where to save the checkpoint\n    :return: nothing\n    \'\'\'\n    torch.save(state, filenameCheckpoint)\n\ndef netParams(model):\n    \'\'\'\n    helper function to see total network parameters\n    :param model: model\n    :return: total network parameters\n    \'\'\'\n    return np.sum([np.prod(parameter.size()) for parameter in model.parameters()])'"
segmentation/cnn/Model.py,5,"b'from torch.nn import init\nimport torch.nn.functional as F\nfrom cnn.cnn_utils import *\nimport math\nimport torch\n\n__author__ = ""Sachin Mehta""\n__version__ = ""1.0.1""\n__maintainer__ = ""Sachin Mehta""\n\nclass EESP(nn.Module):\n    \'\'\'\n    This class defines the EESP block, which is based on the following principle\n        REDUCE ---> SPLIT ---> TRANSFORM --> MERGE\n    \'\'\'\n\n    def __init__(self, nIn, nOut, stride=1, k=4, r_lim=7, down_method=\'esp\'): #down_method --> [\'avg\' or \'esp\']\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param stride: factor by which we should skip (useful for down-sampling). If 2, then down-samples the feature map by 2\n        :param k: # of parallel branches\n        :param r_lim: A maximum value of receptive field allowed for EESP block\n        :param g: number of groups to be used in the feature map reduction step.\n        \'\'\'\n        super().__init__()\n        self.stride = stride\n        n = int(nOut / k)\n        n1 = nOut - (k - 1) * n\n        assert down_method in [\'avg\', \'esp\'], \'One of these is suppported (avg or esp)\'\n        assert n == n1, ""n(={}) and n1(={}) should be equal for Depth-wise Convolution "".format(n, n1)\n        #assert nIn%k == 0, ""Number of input channels ({}) should be divisible by # of branches ({})"".format(nIn, k)\n        #assert n % k == 0, ""Number of output channels ({}) should be divisible by # of branches ({})"".format(n, k)\n        self.proj_1x1 = CBR(nIn, n, 1, stride=1, groups=k)\n\n        # (For convenience) Mapping between dilation rate and receptive field for a 3x3 kernel\n        map_receptive_ksize = {3: 1, 5: 2, 7: 3, 9: 4, 11: 5, 13: 6, 15: 7, 17: 8}\n        self.k_sizes = list()\n        for i in range(k):\n            ksize = int(3 + 2 * i)\n            # After reaching the receptive field limit, fall back to the base kernel size of 3 with a dilation rate of 1\n            ksize = ksize if ksize <= r_lim else 3\n            self.k_sizes.append(ksize)\n        # sort (in ascending order) these kernel sizes based on their receptive field\n        # This enables us to ignore the kernels (3x3 in our case) with the same effective receptive field in hierarchical\n        # feature fusion because kernels with 3x3 receptive fields does not have gridding artifact.\n        self.k_sizes.sort()\n        self.spp_dw = nn.ModuleList()\n        #self.bn = nn.ModuleList()\n        for i in range(k):\n            d_rate = map_receptive_ksize[self.k_sizes[i]]\n            self.spp_dw.append(CDilated(n, n, kSize=3, stride=stride, groups=n, d=d_rate))\n            #self.bn.append(nn.BatchNorm2d(n))\n        self.conv_1x1_exp = CB(nOut, nOut, 1, 1, groups=k)\n        self.br_after_cat = BR(nOut)\n        self.module_act = nn.PReLU(nOut)\n        self.downAvg = True if down_method == \'avg\' else False\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n\n        # Reduce --> project high-dimensional feature maps to low-dimensional space\n        output1 = self.proj_1x1(input)\n        output = [self.spp_dw[0](output1)]\n        # compute the output for each branch and hierarchically fuse them\n        # i.e. Split --> Transform --> HFF\n        for k in range(1, len(self.spp_dw)):\n            out_k = self.spp_dw[k](output1)\n            # HFF\n            # We donot combine the branches that have the same effective receptive (3x3 in our case)\n            # because there are no holes in those kernels.\n            out_k = out_k + output[k - 1]\n            #apply batch norm after fusion and then append to the list\n            output.append(out_k)\n        # Merge\n        expanded = self.conv_1x1_exp( # Aggregate the feature maps using point-wise convolution\n            self.br_after_cat( # apply batch normalization followed by activation function (PRelu in this case)\n                torch.cat(output, 1) # concatenate the output of different branches\n            )\n        )\n        del output\n        # if down-sampling, then return the concatenated vector\n        # as Downsampling function will combine it with avg. pooled feature map and then threshold it\n        if self.stride == 2 and self.downAvg:\n            return expanded\n\n        # if dimensions of input and concatenated vector are the same, add them (RESIDUAL LINK)\n        if expanded.size() == input.size():\n            expanded = expanded + input\n\n        # Threshold the feature map using activation function (PReLU in this case)\n        return self.module_act(expanded)\n\n\nclass DownSampler(nn.Module):\n    \'\'\'\n    Down-sampling fucntion that has two parallel branches: (1) avg pooling\n    and (2) EESP block with stride of 2. The output feature maps of these branches\n    are then concatenated and thresholded using an activation function (PReLU in our\n    case) to produce the final output.\n    \'\'\'\n\n    def __init__(self, nin, nout, k=4, r_lim=9, reinf=True):\n        \'\'\'\n            :param nin: number of input channels\n            :param nout: number of output channels\n            :param k: # of parallel branches\n            :param r_lim: A maximum value of receptive field allowed for EESP block\n            :param g: number of groups to be used in the feature map reduction step.\n        \'\'\'\n        super().__init__()\n        nout_new = nout - nin\n        self.eesp = EESP(nin, nout_new, stride=2, k=k, r_lim=r_lim, down_method=\'avg\')\n        self.avg = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n        if reinf:\n            self.inp_reinf = nn.Sequential(\n                CBR(config_inp_reinf, config_inp_reinf, 3, 1),\n                CB(config_inp_reinf, nout, 1, 1)\n            )\n        self.act =  nn.PReLU(nout)\n\n    def forward(self, input, input2=None):\n        \'\'\'\n        :param input: input feature map\n        :return: feature map down-sampled by a factor of 2\n        \'\'\'\n        avg_out = self.avg(input)\n        eesp_out = self.eesp(input)\n        output = torch.cat([avg_out, eesp_out], 1)\n        if input2 is not None:\n            #assuming the input is a square image\n            w1 = avg_out.size(2)\n            while True:\n                input2 = F.avg_pool2d(input2, kernel_size=3, padding=1, stride=2)\n                w2 = input2.size(2)\n                if w2 == w1:\n                    break\n            output = output + self.inp_reinf(input2)\n\n        return self.act(output) #self.act(output)\n\nclass EESPNet(nn.Module):\n    \'\'\'\n    This class defines the ESPNetv2 architecture for the ImageNet classification\n    \'\'\'\n\n    def __init__(self, classes=20, s=1):\n        \'\'\'\n        :param classes: number of classes in the dataset. Default is 20 for the cityscapes\n        :param s: factor that scales the number of output feature maps\n        \'\'\'\n        super().__init__()\n        reps = [0, 3, 7, 3]  # how many times EESP blocks should be repeated.\n        channels = 3\n\n        r_lim = [13, 11, 9, 7, 5]  # receptive field at each spatial level\n        K = [4]*len(r_lim) # No. of parallel branches at different levels\n\n        base = 32 #base configuration\n        config_len = 5\n        config = [base] * config_len\n        base_s = 0\n        for i in range(config_len):\n            if i== 0:\n                base_s = int(base * s)\n                base_s = math.ceil(base_s / K[0]) * K[0]\n                config[i] = base if base_s > base else base_s\n            else:\n                config[i] = base_s * pow(2, i)\n        if s <= 1.5:\n            config.append(1024)\n        elif s in [1.5, 2]:\n            config.append(1280)\n        else:\n            ValueError(\'Configuration not supported\')\n\n        #print(\'Config: \', config)\n\n        global config_inp_reinf\n        config_inp_reinf = 3\n        self.input_reinforcement = True\n        assert len(K) == len(r_lim), \'Length of branching factor array and receptive field array should be the same.\'\n\n        self.level1 = CBR(channels, config[0], 3, 2)  # 112 L1\n\n        self.level2_0 = DownSampler(config[0], config[1], k=K[0], r_lim=r_lim[0], reinf=self.input_reinforcement)  # out = 56\n        self.level3_0 = DownSampler(config[1], config[2], k=K[1], r_lim=r_lim[1], reinf=self.input_reinforcement) # out = 28\n        self.level3 = nn.ModuleList()\n        for i in range(reps[1]):\n            self.level3.append(EESP(config[2], config[2], stride=1, k=K[2], r_lim=r_lim[2]))\n\n        self.level4_0 = DownSampler(config[2], config[3], k=K[2], r_lim=r_lim[2], reinf=self.input_reinforcement) #out = 14\n        self.level4 = nn.ModuleList()\n        for i in range(reps[2]):\n            self.level4.append(EESP(config[3], config[3], stride=1, k=K[3], r_lim=r_lim[3]))\n\n        self.level5_0 = DownSampler(config[3], config[4], k=K[3], r_lim=r_lim[3]) #7\n        self.level5 = nn.ModuleList()\n        for i in range(reps[3]):\n            self.level5.append(EESP(config[4], config[4], stride=1, k=K[4], r_lim=r_lim[4]))\n\n        # expand the feature maps using depth-wise separable convolution\n        self.level5.append(CBR(config[4], config[4], 3, 1, groups=config[4]))\n        self.level5.append(CBR(config[4], config[5], 1, 1, groups=K[4]))\n\n\n\n        #self.level5_exp = nn.ModuleList()\n        #assert config[5]%config[4] == 0, \'{} should be divisible by {}\'.format(config[5], config[4])\n        #gr = int(config[5]/config[4])\n        #for i in range(gr):\n        #    self.level5_exp.append(CBR(config[4], config[4], 1, 1, groups=pow(2, i)))\n\n        self.classifier = nn.Linear(config[5], classes)\n        self.init_params()\n\n    def init_params(self):\n        \'\'\'\n        Function to initialze the parameters\n        \'\'\'\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal_(m.weight, std=0.001)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n    def forward(self, input, p=0.2, seg=True):\n        \'\'\'\n        :param input: Receives the input RGB image\n        :return: a C-dimensional vector, C=# of classes\n        \'\'\'\n        out_l1 = self.level1(input)  # 112\n        if not self.input_reinforcement:\n            del input\n            input = None\n\n        out_l2 = self.level2_0(out_l1, input)  # 56\n\n        out_l3_0 = self.level3_0(out_l2, input)  # out_l2_inp_rein\n        for i, layer in enumerate(self.level3):\n            if i == 0:\n                out_l3 = layer(out_l3_0)\n            else:\n                out_l3 = layer(out_l3)\n\n        out_l4_0 = self.level4_0(out_l3, input)  # down-sampled\n        for i, layer in enumerate(self.level4):\n            if i == 0:\n                out_l4 = layer(out_l4_0)\n            else:\n                out_l4 = layer(out_l4)\n\n        if not seg:\n            out_l5_0 = self.level5_0(out_l4)  # down-sampled\n            for i, layer in enumerate(self.level5):\n                if i == 0:\n                    out_l5 = layer(out_l5_0)\n                else:\n                    out_l5 = layer(out_l5)\n\n            #out_e = []\n            #for layer in self.level5_exp:\n            #    out_e.append(layer(out_l5))\n            #out_exp = torch.cat(out_e, dim=1)\n\n\n\n            output_g = F.adaptive_avg_pool2d(out_l5, output_size=1)\n            output_g = F.dropout(output_g, p=p, training=self.training)\n            output_1x1 = output_g.view(output_g.size(0), -1)\n\n            return self.classifier(output_1x1)\n        return out_l1, out_l2, out_l3, out_l4\n\n\n'"
segmentation/cnn/SegmentationModel.py,6,"b'#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nimport torch\nfrom torch import nn\n\nfrom cnn.Model import EESPNet, EESP\nimport os\nimport torch.nn.functional as F\nfrom cnn.cnn_utils import *\n\nclass EESPNet_Seg(nn.Module):\n    def __init__(self, classes=20, s=1, pretrained=None, gpus=1):\n        super().__init__()\n        classificationNet = EESPNet(classes=1000, s=s)\n        if gpus >=1:\n            classificationNet = nn.DataParallel(classificationNet)\n        # load the pretrained weights\n        if pretrained:\n            if not os.path.isfile(pretrained):\n                print(\'Weight file does not exist. Training without pre-trained weights\')\n            print(\'Model initialized with pretrained weights\')\n            classificationNet.load_state_dict(torch.load(pretrained))\n\n        self.net = classificationNet.module\n\n        del classificationNet\n        # delete last few layers\n        del self.net.classifier\n        del self.net.level5\n        del self.net.level5_0\n        if s <=0.5:\n            p = 0.1\n        else:\n            p=0.2\n\n        self.proj_L4_C = CBR(self.net.level4[-1].module_act.num_parameters, self.net.level3[-1].module_act.num_parameters, 1, 1)\n        pspSize = 2*self.net.level3[-1].module_act.num_parameters\n        self.pspMod = nn.Sequential(EESP(pspSize, pspSize //2, stride=1, k=4, r_lim=7),\n                PSPModule(pspSize // 2, pspSize //2))\n        self.project_l3 = nn.Sequential(nn.Dropout2d(p=p), C(pspSize // 2, classes, 1, 1))\n        self.act_l3 = BR(classes)\n        self.project_l2 = CBR(self.net.level2_0.act.num_parameters + classes, classes, 1, 1)\n        self.project_l1 = nn.Sequential(nn.Dropout2d(p=p), C(self.net.level1.act.num_parameters + classes, classes, 1, 1))\n\n    def hierarchicalUpsample(self, x, factor=3):\n        for i in range(factor):\n            x = F.interpolate(x, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        return x\n\n\n    def forward(self, input):\n        out_l1, out_l2, out_l3, out_l4 = self.net(input, seg=True)\n        out_l4_proj = self.proj_L4_C(out_l4)\n        up_l4_to_l3 = F.interpolate(out_l4_proj, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        merged_l3_upl4 = self.pspMod(torch.cat([out_l3, up_l4_to_l3], 1))\n        proj_merge_l3_bef_act = self.project_l3(merged_l3_upl4)\n        proj_merge_l3 = self.act_l3(proj_merge_l3_bef_act)\n        out_up_l3 = F.interpolate(proj_merge_l3, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        merge_l2 = self.project_l2(torch.cat([out_l2, out_up_l3], 1))\n        out_up_l2 = F.interpolate(merge_l2, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        merge_l1 = self.project_l1(torch.cat([out_l1, out_up_l2], 1))\n        if self.training:\n            return F.interpolate(merge_l1, scale_factor=2, mode=\'bilinear\', align_corners=True), self.hierarchicalUpsample(proj_merge_l3_bef_act)\n        else:\n            return F.interpolate(merge_l1, scale_factor=2, mode=\'bilinear\', align_corners=True)\n\n\nif __name__ == \'__main__\':\n    input = torch.Tensor(1, 3, 512, 1024).cuda()\n    net = EESPNet_Seg(classes=20, s=2).cuda()\n    out_x_8 = net(input)\n    print(out_x_8.size())\n\n'"
segmentation/cnn/cnn_utils.py,3,"b'import torch.nn as nn\nimport torch\nimport torch.nn.functional as F\n\n\n__author__ = ""Sachin Mehta""\n__version__ = ""1.0.1""\n__maintainer__ = ""Sachin Mehta""\n\n\nclass PSPModule(nn.Module):\n    def __init__(self, features, out_features=1024, sizes=(1, 2, 4, 8)):\n        super().__init__()\n        self.stages = []\n        self.stages = nn.ModuleList([C(features, features, 3, 1, groups=features) for size in sizes])\n        self.project = CBR(features * (len(sizes) + 1), out_features, 1, 1)\n \n    def forward(self, feats):\n        h, w = feats.size(2), feats.size(3)\n        out = [feats]\n        for stage in self.stages:\n            feats = F.avg_pool2d(feats, kernel_size=3, stride=2, padding=1)\n            upsampled = F.interpolate(input=stage(feats), size=(h, w), mode=\'bilinear\', align_corners=True)\n            out.append(upsampled)\n        return self.project(torch.cat(out, dim=1))\n\nclass CBR(nn.Module):\n    \'\'\'\n    This class defines the convolution layer with batch normalization and PReLU activation\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, groups=1):\n        \'\'\'\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: stride rate for down-sampling. Default is 1\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False, groups=groups)\n        self.bn = nn.BatchNorm2d(nOut)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        # output = self.conv1(output)\n        output = self.bn(output)\n        output = self.act(output)\n        return output\n\n\nclass BR(nn.Module):\n    \'\'\'\n        This class groups the batch normalization and PReLU activation\n    \'\'\'\n\n    def __init__(self, nOut):\n        \'\'\'\n        :param nOut: output feature maps\n        \'\'\'\n        super().__init__()\n        self.bn = nn.BatchNorm2d(nOut)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: normalized and thresholded feature map\n        \'\'\'\n        output = self.bn(input)\n        output = self.act(output)\n        return output\n\n\nclass CB(nn.Module):\n    \'\'\'\n       This class groups the convolution and batch normalization\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, groups=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optinal stide for down-sampling\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False,\n                              groups=groups)\n        self.bn = nn.BatchNorm2d(nOut)\n\n    def forward(self, input):\n        \'\'\'\n\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        output = self.bn(output)\n        return output\n\n\nclass C(nn.Module):\n    \'\'\'\n    This class is for a convolutional layer.\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, groups=1):\n        \'\'\'\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False,\n                              groups=groups)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        return output\n\n\nclass CDilated(nn.Module):\n    \'\'\'\n    This class defines the dilated convolution.\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1, groups=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        :param d: optional dilation rate\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2) * d\n        self.conv = nn.Conv2d(nIn, nOut,kSize, stride=stride, padding=padding, bias=False,\n                              dilation=d, groups=groups)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        return output\n\nclass CDilatedB(nn.Module):\n    \'\'\'\n    This class defines the dilated convolution with batch normalization.\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1, groups=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        :param d: optional dilation rate\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2) * d\n        self.conv = nn.Conv2d(nIn, nOut,kSize, stride=stride, padding=padding, bias=False,\n                              dilation=d, groups=groups)\n        self.bn = nn.BatchNorm2d(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        return self.bn(self.conv(input))\n'"
