file_path,api_count,code
detection_demo.py,9,"b'import sys\nimport os\nimport argparse\nimport numpy as np\nimport torch\nimport torchvision\nfrom utilities.print_utils import *\nfrom model.detection.ssd import ssd\nimport os\nfrom model.detection.box_predictor import BoxPredictor\nfrom PIL import Image\nimport cv2\nfrom utilities.color_map import VOCColormap\nimport glob\nimport gc\n\n\nCOLOR_MAP = []\nfor cmap in VOCColormap().get_color_map():\n    r, g, b = cmap\n    COLOR_MAP.append((int(r), int(g), int(b)))\n\nFONT_SIZE = cv2.FONT_HERSHEY_PLAIN\nLABEL_COLOR = [255, 255, 255]\nTEXT_THICKNESS = 1\nRECT_BORDER_THICKNESS=2\n\n\ndef main(args):\n    if args.im_size in [300, 512]:\n        from model.detection.ssd_config import get_config\n        cfg = get_config(args.im_size)\n    else:\n        print_error_message(\'{} image size not supported\'.format(args.im_size))\n\n    if args.dataset in [\'voc\', \'pascal\']:\n        from data_loader.detection.voc import VOC_CLASS_LIST\n        num_classes = len(VOC_CLASS_LIST)\n        object_names = VOC_CLASS_LIST\n    elif args.dataset == \'coco\':\n        from data_loader.detection.coco import COCO_CLASS_LIST\n        num_classes = len(COCO_CLASS_LIST)\n        object_names = COCO_CLASS_LIST\n\n\n    else:\n        print_error_message(\'{} dataset not supported.\'.format(args.dataset))\n        exit(-1)\n\n    cfg.NUM_CLASSES = num_classes\n    # discard the boxes that have prediction score less than this value\n    cfg.conf_threshold = args.conf_threshold\n\n    # -----------------------------------------------------------------------------\n    # Model\n    # -----------------------------------------------------------------------------\n    model = ssd(args, cfg)\n    if args.weights_test:\n        weight_dict = torch.load(args.weights_test, map_location=\'cpu\')\n        model.load_state_dict(weight_dict)\n    else:\n        print_error_message(""Please provide the location of weight files using --weights argument"")\n\n    num_gpus = torch.cuda.device_count()\n    device = \'cuda\' if num_gpus >= 1 else \'cpu\'\n\n    if num_gpus >= 1:\n        #model = torch.nn.DataParallel(model)\n        model = model.to(device)\n        if torch.backends.cudnn.is_available():\n            import torch.backends.cudnn as cudnn\n            cudnn.benchmark = True\n            cudnn.deterministic = True\n\n    predictor = BoxPredictor(cfg=cfg, device=device)\n\n    if args.live:\n        main_live(predictor=predictor, model=model, object_names=object_names, stream_url=args.stream, device=device)\n    else:\n        if not os.path.isdir(args.save_dir):\n            os.makedirs(args.save_dir)\n        main_images(predictor=predictor, model=model, object_names=object_names,\n                    in_dir=args.im_dir, out_dir=args.save_dir, device=device)\n\n\ndef main_images(predictor, model, object_names, in_dir, out_dir, device=\'cuda\'):\n    png_file_names = glob.glob(in_dir + os.sep + \'*.png\')\n    jpg_file_names = glob.glob(in_dir + os.sep + \'*.jpg\')\n    file_names = png_file_names + jpg_file_names\n\n    if len(file_names) == 0:\n        print_error_message(\'No image files in the folder\')\n\n    # model in eval mode\n    model.eval()\n    with torch.no_grad():\n        for img_name in file_names:\n            image = cv2.imread(img_name)\n            height, width, _ = image.shape\n\n            start_time = time.time()\n\n            output = predictor.predict(model, image, is_scaling=False)\n            if device == \'cuda\':\n                torch.cuda.synchronize()\n            prediction_time = (time.time() - start_time) * 1000  # convert to millis\n\n            start_time = time.time()\n            boxes, labels, scores = [o.to(""cpu"").numpy() for o in output]\n            for label, score, coords in zip(labels, scores, boxes):\n                r, g, b = COLOR_MAP[label]\n                #c1 = (int(coords[0]), int(coords[1]))\n                #c2 = (int(coords[2]), int(coords[3]))\n                c1 = (int(coords[0] * width), int(coords[1] * height))\n                c2 = (int(coords[2] * width), int(coords[3] * height))\n\n                cv2.rectangle(image, c1, c2, (r, g, b), thickness=RECT_BORDER_THICKNESS)\n                label_text = \'{label}: {score:.2f}\'.format(label=object_names[label], score=score)\n                t_size = cv2.getTextSize(label_text, FONT_SIZE, 1, TEXT_THICKNESS)[0]\n                c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n                cv2.rectangle(image, c1, c2, (r, g, b), -1)\n                cv2.putText(image, label_text, (c1[0], c1[1] + t_size[1] + 4), FONT_SIZE, 1, LABEL_COLOR, TEXT_THICKNESS)\n\n            annot_time = (time.time() - start_time) * 1000  # convert to millis\n            print_log_message(\n                \'Prediction time: {:.2f} ms, Annotation time: {:.2f} ms, Total time: {:.2f} ms\'.format(prediction_time,\n                                                                                                       annot_time,\n                                                                                                       prediction_time + annot_time))\n\n            new_file_name = \'{}/{}\'.format(out_dir, img_name.split(\'/\')[-1])\n            cv2.imwrite(new_file_name, image)\n            gc.collect()\n\n\ndef main_live(predictor, model, object_names, stream_url, device=\'cuda\'):\n\n    capture_device = cv2.VideoCapture(stream_url)\n    capture_device.set(3, 1920)\n    capture_device.set(4, 1080)\n\n    # model in eval mode\n    model.eval()\n    with torch.no_grad():\n        while True:\n            ret, image = capture_device.read()\n            if image is None:\n                continue\n\n            height, width, _ = image.shape\n\n            start_time = time.time()\n\n            output = predictor.predict(model, image, is_scaling=False)\n            if device == \'cuda\':\n                torch.cuda.synchronize()\n            # box prediction time\n            prediction_time = (time.time() - start_time) * 1000\n\n            start_time = time.time()\n            boxes, labels, scores = [o.to(""cpu"").numpy() for o in output]\n\n            for label, score, coords in zip(labels, scores, boxes):\n                r, g, b = COLOR_MAP[label]\n                #c1 = (int(coords[0]), int(coords[1]))\n                #c2 = (int(coords[2]), int(coords[3]))\n\n                c1 = (int(coords[0] * width), int(coords[1] * height))\n                c2 = (int(coords[2] * width), int(coords[3] * height))\n\n                cv2.rectangle(image, c1, c2, (r, g, b), thickness=RECT_BORDER_THICKNESS)\n                label_text = \'{} {}\'.format(object_names[label], int(score*100))\n                t_size = cv2.getTextSize(label_text, FONT_SIZE, 1, TEXT_THICKNESS)[0]\n                c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n                cv2.rectangle(image, c1, c2, (r, g, b), -1)\n                cv2.putText(image, label_text, (c1[0], c1[1] + t_size[1] + 4), FONT_SIZE, 1, [255, 255, 255], TEXT_THICKNESS)\n\n            # box annotation time\n            annot_time = (time.time() - start_time) * 1000  # convert to millis\n            print_log_message(\n                \'Prediction time: {:.2f} ms, Annotation time: {:.2f} ms, Total time: {:.2f} ms\'.format(prediction_time,\n                                                                                                       annot_time,\n                                                                                                       prediction_time + annot_time))\n\n            cv2.imshow(\'EdgeNets\', image)\n            if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                break\n            gc.collect()\n\n        capture_device.release()\n        cv2.destroyAllWindows()\n\n\nif __name__ == \'__main__\':\n    from commons.general_details import detection_datasets\n\n    parser = argparse.ArgumentParser(description=\'Training detection network\')\n    parser.add_argument(\'--model\', default=\'espnetv2\', choices=[\'espnetv2\'], type=str,\n                        help=\'initialized model path\')\n    parser.add_argument(\'--s\', default=2.0, type=float, help=\'Model scale factor\')\n    parser.add_argument(\'--dataset\', default=\'coco\', choices=detection_datasets,\n                        help=\'Name of the dataset is required to retrieve the correct object list\')\n    parser.add_argument(\'--weights-test\', default=\'\', help=\'model weights\')\n    parser.add_argument(\'--im-size\', default=512, type=int, help=\'Image size for training\')\n    parser.add_argument(\'--im-dir\', default=\'./sample_images\', type=str, help=\'Image file\')\n    parser.add_argument(\'--save-dir\', default=\'vis_detect\', type=str, help=\'Directory where results will be stored\')\n    parser.add_argument(\'--live\', action=\'store_true\', default=False, help=""Live detection"")\n    parser.add_argument(\'--conf-threshold\', type=float, default=0.3, help=\'Ignore boxes with value < conf-threshold\')\n    parser.add_argument(\'--stream\', type=str, default=0, help=\'Stream url for live detection.\')\n    args = parser.parse_args()\n\n    # This key is used to load the ImageNet weights while training. So, set to empty to avoid errors\n    args.weights = \'\'\n\n    if not args.weights_test:\n        from model.weight_locations.detection import model_weight_map\n\n        model_key = \'{}_{}\'.format(args.model, args.s)\n        dataset_key = \'{}_{}x{}\'.format(args.dataset, args.im_size, args.im_size)\n        assert model_key in model_weight_map.keys(), \'{} does not exist\'.format(model_key)\n        assert dataset_key in model_weight_map[model_key].keys(), \'{} does not exist\'.format(dataset_key)\n        args.weights_test = model_weight_map[model_key][dataset_key][\'weights\']\n        if not os.path.isfile(args.weights_test):\n            print_error_message(\'weight file does not exist: {}\'.format(args.weights_test))\n\n    main(args)'"
segmentation_demo.py,5,"b'import torch\nfrom utilities.print_utils import *\nimport os\nfrom PIL import Image\nfrom utilities.color_map import VOCColormap\nimport glob\nfrom torchvision.transforms import functional as F\nfrom transforms.classification.data_transforms import MEAN, STD\nfrom tqdm import tqdm\nfrom argparse import ArgumentParser\n\nCOLOR_MAP = VOCColormap().get_color_map()\nIMAGE_EXTENSIONS = [\'.jpg\', \'.png\', \'.jpeg\']\n\ndef data_transform(img, im_size):\n    img = img.resize(im_size, Image.BILINEAR)\n    img = F.to_tensor(img)  # convert to tensor (values between 0 and 1)\n    img = F.normalize(img, MEAN, STD)  # normalize the tensor\n    return img\n\ndef run_segmentation(args, model, image_list, device):\n    im_size = tuple(args.im_size)\n\n    model.eval()\n    with torch.no_grad():\n        for imgName in tqdm(image_list):\n            img = Image.open(imgName).convert(\'RGB\')\n            img_clone = img.copy()\n            w, h = img.size\n\n            img = data_transform(img, im_size)\n            img = img.unsqueeze(0)  # add a batch dimension\n            img = img.to(device)\n            img_out = model(img)\n            img_out = img_out.squeeze(0)  # remove the batch dimension\n            img_out = img_out.max(0)[1].byte()  # get the label map\n            img_out = img_out.to(device=\'cpu\').numpy()\n\n            img_out = Image.fromarray(img_out)\n            # resize to original size\n            img_out = img_out.resize((w, h), Image.NEAREST)\n\n            # pascal dataset accepts colored segmentations\n            img_out.putpalette(COLOR_MAP)\n            img_out = img_out.convert(\'RGB\')\n\n            # save the segmentation mask\n            name = imgName.split(\'/\')[-1]\n            img_extn = imgName.split(\'.\')[-1]\n            name = \'{}/{}\'.format(args.savedir, name.replace(img_extn, \'png\'))\n            blended = Image.blend(img_clone, img_out, alpha=0.7)\n            blended.save(name)\n            #img_out.save(name)\n\ndef main(args):\n    # read all the images in the folder\n    if args.dataset == \'city\':\n        from data_loader.segmentation.cityscapes import CITYSCAPE_CLASS_LIST\n        seg_classes = len(CITYSCAPE_CLASS_LIST)\n    elif args.dataset == \'pascal\':\n        from data_loader.segmentation.voc import VOC_CLASS_LIST\n        seg_classes = len(VOC_CLASS_LIST)\n    else:\n        print_error_message(\'{} dataset not yet supported\'.format(args.dataset))\n\n    image_list = []\n    for extn in IMAGE_EXTENSIONS:\n        image_list = image_list +  glob.glob(args.data_path + os.sep + \'*\' + extn)\n\n    if len(image_list) == 0:\n        print_error_message(\'No files in directory: {}\'.format(args.data_path))\n\n    print_info_message(\'# of images used for demonstration: {}\'.format(len(image_list)))\n\n    if args.model == \'espnetv2\':\n        from model.segmentation.espnetv2 import espnetv2_seg\n        args.classes = seg_classes\n        model = espnetv2_seg(args)\n    elif args.model == \'dicenet\':\n        from model.segmentation.dicenet import dicenet_seg\n        model = dicenet_seg(args, classes=seg_classes)\n    else:\n        print_error_message(\'{} network not yet supported\'.format(args.model))\n        exit(-1)\n\n\n    if args.weights_test:\n        print_info_message(\'Loading model weights\')\n        weight_dict = torch.load(args.weights_test, map_location=torch.device(\'cpu\'))\n        model.load_state_dict(weight_dict)\n        print_info_message(\'Weight loaded successfully\')\n    else:\n        print_error_message(\'weight file does not exist or not specified. Please check: {}\', format(args.weights_test))\n\n    num_gpus = torch.cuda.device_count()\n    device = \'cuda\' if num_gpus > 0 else \'cpu\'\n    model = model.to(device=device)\n\n    if torch.backends.cudnn.is_available():\n        import torch.backends.cudnn as cudnn\n        cudnn.benchmark = True\n        cudnn.deterministic = True\n\n    run_segmentation(args, model, image_list, device=device)\n\n\nif __name__ == \'__main__\':\n    from commons.general_details import segmentation_models, segmentation_datasets\n\n    parser = ArgumentParser()\n    # mdoel details\n    parser.add_argument(\'--model\', default=""espnetv2"", choices=segmentation_models, help=\'Model name\')\n    parser.add_argument(\'--weights-test\', default=\'\', help=\'Pretrained weights directory.\')\n    parser.add_argument(\'--s\', default=2.0, type=float, help=\'scale\')\n    # dataset details\n    parser.add_argument(\'--dataset\', default=\'pascal\', choices=segmentation_datasets, help=\'Dataset name. \'\n                                                                                           \'This is required to retrieve the correct segmentation model weights\')\n    parser.add_argument(\'--data-path\', default=\'./sample_images\', type=str, help=\'Image folder location\')\n    # input details\n    parser.add_argument(\'--im-size\', type=int, nargs=""+"", default=[384, 384], help=\'Image size for testing (W x H)\')\n    parser.add_argument(\'--model-width\', default=224, type=int, help=\'Model width\')\n    parser.add_argument(\'--model-height\', default=224, type=int, help=\'Model height\')\n    parser.add_argument(\'--channels\', default=3, type=int, help=\'Input channels\')\n    parser.add_argument(\'--num-classes\', default=1000, type=int,\n                        help=\'ImageNet classes. Required for loading the base network\')\n\n    args = parser.parse_args()\n\n    if not args.weights_test:\n        from model.weight_locations.segmentation import model_weight_map\n\n        model_key = \'{}_{}\'.format(args.model, args.s)\n        dataset_key = \'{}_{}x{}\'.format(args.dataset, args.im_size[0], args.im_size[1])\n        assert model_key in model_weight_map.keys(), \'{} does not exist\'.format(model_key)\n        assert dataset_key in model_weight_map[model_key].keys(), \'{} does not exist\'.format(dataset_key)\n        args.weights_test = model_weight_map[model_key][dataset_key][\'weights\']\n        if not os.path.isfile(args.weights_test):\n            print_error_message(\'weight file does not exist: {}\'.format(args.weights_test))\n\n    # set-up results path\n    args.savedir = \'segmentation_results/\'\n    if not os.path.isdir(args.savedir):\n        os.makedirs(args.savedir)\n\n    # This key is used to load the ImageNet weights while training. So, set to empty to avoid errors\n    args.weights = \'\'\n\n    main(args)'"
test_classification.py,5,"b'import argparse\nimport torch\nfrom utilities.utils import model_parameters, compute_flops\nfrom utilities.train_eval_classification import validate\nimport os\nfrom data_loader.classification.imagenet import val_loader as loader\nfrom utilities.print_utils import *\n#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\n\ndef main(args):\n    # create model\n    if args.model == \'dicenet\':\n        from model.classification import dicenet as net\n        model = net.CNNModel(args)\n    elif args.model == \'espnetv2\':\n        from model.classification import espnetv2 as net\n        model = net.EESPNet(args)\n    elif args.model == \'shufflenetv2\':\n        from model.classification import shufflenetv2 as net\n        model = net.CNNModel(args)\n    else:\n        NotImplementedError(\'Model {} not yet implemented\'.format(args.model))\n        exit()\n\n    num_params = model_parameters(model)\n    flops = compute_flops(model)\n    print_info_message(\'FLOPs: {:.2f} million\'.format(flops))\n    print_info_message(\'Network Parameters: {:.2f} million\'.format(num_params))\n\n\n    if not args.weights:\n        print_info_message(\'Grabbing location of the ImageNet weights from the weight dictionary\')\n        from model.weight_locations.classification import model_weight_map\n\n        weight_file_key = \'{}_{}\'.format(args.model, args.s)\n        assert weight_file_key in model_weight_map.keys(), \'{} does not exist\'.format(weight_file_key)\n        args.weights = model_weight_map[weight_file_key]\n\n    num_gpus = torch.cuda.device_count()\n    device = \'cuda\' if num_gpus >=1 else \'cpu\'\n    weight_dict = torch.load(args.weights, map_location=torch.device(device))\n    model.load_state_dict(weight_dict)\n\n    if num_gpus >= 1:\n        model = torch.nn.DataParallel(model)\n        model = model.cuda()\n        if torch.backends.cudnn.is_available():\n            import torch.backends.cudnn as cudnn\n            cudnn.benchmark = True\n            cudnn.deterministic = True\n\n    # Data loading code\n    val_loader = loader(args)\n    validate(val_loader, model, criteria=None, device=device)\n\n\nif __name__ == \'__main__\':\n    from commons.general_details import classification_models, classification_datasets\n\n    parser = argparse.ArgumentParser(description=\'Testing efficient networks\')\n    parser.add_argument(\'--workers\', default=4, type=int, help=\'number of data loading workers (default: 4)\')\n    parser.add_argument(\'--data\', default=\'\', help=\'path to dataset\')\n    parser.add_argument(\'--dataset\', default=\'imagenet\', help=\'Name of the dataset\', choices=classification_datasets)\n    parser.add_argument(\'--batch-size\', default=512, type=int, help=\'mini-batch size (default: 512)\')\n    parser.add_argument(\'--num-classes\', default=1000, type=int, help=\'# of classes in the dataset\')\n    parser.add_argument(\'--s\', default=1, type=float, help=\'Width scaling factor\')\n    parser.add_argument(\'--weights\', type=str, default=\'\', help=\'weight file\')\n    parser.add_argument(\'--inpSize\', default=224, type=int, help=\'Input size\')\n    ##Select a model\n    parser.add_argument(\'--model\', default=\'dicenet\', choices=classification_models, help=\'Which model?\')\n    parser.add_argument(\'--model-width\', default=224, type=int, help=\'Model width\')\n    parser.add_argument(\'--model-height\', default=224, type=int, help=\'Model height\')\n    parser.add_argument(\'--channels\', default=3, type=int, help=\'Input channels\')\n\n    args = parser.parse_args()\n    main(args)\n'"
test_detection.py,7,"b'import argparse\nimport torch\nfrom utilities.utils import model_parameters, compute_flops\nfrom tqdm import tqdm\nfrom utilities.metrics.evaluate_detection import evaluate\nfrom utilities.print_utils import *\nfrom model.detection.ssd import ssd\nimport os\nfrom model.detection.box_predictor import BoxPredictor\n\n\ndef eval(model, dataset, predictor):\n    model.eval()\n    predictions = {}\n    with torch.no_grad():\n        for i in tqdm(range(len(dataset))):\n            image = dataset.get_image(i)\n            output = predictor.predict(model, image)\n            boxes, labels, scores = [o.to(""cpu"").numpy() for o in output]\n            predictions[i] = (boxes, labels, scores)\n\n    predictions = [predictions[i] for i in predictions.keys()]\n    return predictions\n\n\ndef main(args):\n    if args.im_size in [300, 512]:\n        from model.detection.ssd_config import get_config\n        cfg = get_config(args.im_size)\n    else:\n        print_error_message(\'{} image size not supported\'.format(args.im_size))\n\n    if args.dataset in [\'voc\', \'pascal\']:\n        from data_loader.detection.voc import VOC_CLASS_LIST\n        num_classes = len(VOC_CLASS_LIST)\n    elif args.dataset == \'coco\':\n        from data_loader.detection.coco import COCO_CLASS_LIST\n        num_classes = len(COCO_CLASS_LIST)\n    else:\n        print_error_message(\'{} dataset not supported.\'.format(args.dataset))\n        exit(-1)\n\n    cfg.NUM_CLASSES = num_classes\n\n    # -----------------------------------------------------------------------------\n    # Model\n    # -----------------------------------------------------------------------------\n    model = ssd(args, cfg)\n\n    if args.weights_test:\n        weight_dict = torch.load(args.weights_test, map_location=\'cpu\')\n        model.load_state_dict(weight_dict)\n\n    num_params = model_parameters(model)\n    flops = compute_flops(model, input=torch.Tensor(1, 3, cfg.image_size, cfg.image_size))\n    print_info_message(\'FLOPs for an input of size {}x{}: {:.2f} million\'.format(cfg.image_size, cfg.image_size, flops))\n    print_info_message(\'Network Parameters: {:.2f} million\'.format(num_params))\n\n    num_gpus = torch.cuda.device_count()\n    device = \'cuda\' if num_gpus >= 1 else \'cpu\'\n\n    if num_gpus >= 1:\n        model = torch.nn.DataParallel(model)\n        model = model.to(device)\n        if torch.backends.cudnn.is_available():\n            import torch.backends.cudnn as cudnn\n            cudnn.benchmark = True\n            cudnn.deterministic = True\n\n    # -----------------------------------------------------------------------------\n    # Dataset\n    # -----------------------------------------------------------------------------\n    if args.dataset in [\'voc\', \'pascal\']:\n        from data_loader.detection.voc import VOCDataset, VOC_CLASS_LIST\n        dataset_class = VOCDataset(root_dir=args.data_path, transform=None, is_training=False,\n                                   split=""VOC2007"")\n        class_names = VOC_CLASS_LIST\n    else:\n        from data_loader.detection.coco import COCOObjectDetection, COCO_CLASS_LIST\n        dataset_class = COCOObjectDetection(root_dir=args.data_path, transform=None,\n                                            is_training=False)\n        class_names = COCO_CLASS_LIST\n\n    # -----------------------------------------------------------------------------\n    # Evaluate\n    # -----------------------------------------------------------------------------\n    predictor = BoxPredictor(cfg=cfg, device=device)\n    predictions = eval(model=model, dataset=dataset_class, predictor=predictor)\n\n    result_info = evaluate(dataset=dataset_class, predictions=predictions, output_dir=args.save_dir,\n                           dataset_name=args.dataset)\n\n    # -----------------------------------------------------------------------------\n    # Results\n    # -----------------------------------------------------------------------------\n    if args.dataset in [\'voc\', \'pascal\']:\n        mAP = result_info[\'map\']\n        ap = result_info[\'ap\']\n        for i, c_name in enumerate(class_names):\n            if i == 0:  # skip the background class\n                continue\n            print_info_message(\'{}: {}\'.format(c_name, ap[i]))\n\n        print_info_message(\'* mAP: {}\'.format(mAP))\n    elif args.dataset == \'coco\':\n        print_info_message(\'AP_IoU=0.50:0.95: {}\'.format(result_info.stats[0]))\n        print_info_message(\'AP_IoU=0.50: {}\'.format(result_info.stats[1]))\n        print_info_message(\'AP_IoU=0.75: {}\'.format(result_info.stats[2]))\n    else:\n        print_error_message(\'{} not supported\'.format(args.dataset))\n\n    print_log_message(\'Done\')\n\n\nif __name__ == \'__main__\':\n    from commons.general_details import detection_datasets, detection_models\n\n    parser = argparse.ArgumentParser(description=\'Training detection network\')\n    parser.add_argument(\'--resume\', action=\'store_true\', help=\'resume from checkpoint\')\n    parser.add_argument(\'--model\', default=\'espnetv2\', choices=detection_models, type=str,\n                        help=\'initialized model path\')\n    parser.add_argument(\'--s\', default=2.0, type=float, help=\'Model scale factor\')\n    parser.add_argument(\'--dataset\', default=\'pascal\', choices=detection_datasets, help=\'Name of the dataset\')\n    parser.add_argument(\'--data-path\', default=\'\', help=\'Dataset path\')\n    parser.add_argument(\'--weights-test\', default=\'\', help=\'model weights\')\n    parser.add_argument(\'--im-size\', default=300, type=int, help=\'Image size for training\')\n    # dimension wise network related params\n    parser.add_argument(\'--model-width\', default=224, type=int, help=\'Model width\')\n    parser.add_argument(\'--model-height\', default=224, type=int, help=\'Model height\')\n\n    args = parser.parse_args()\n\n    if not args.weights_test:\n        from model.weight_locations.detection import model_weight_map\n\n        model_key = \'{}_{}\'.format(args.model, args.s)\n        dataset_key = \'{}_{}x{}\'.format(args.dataset, args.im_size, args.im_size)\n        assert model_key in model_weight_map.keys(), \'{} does not exist\'.format(model_key)\n        assert dataset_key in model_weight_map[model_key].keys(), \'{} does not exist\'.format(dataset_key)\n        args.weights_test = model_weight_map[model_key][dataset_key][\'weights\']\n        if not os.path.isfile(args.weights_test):\n            print_error_message(\'weight file does not exist: {}\'.format(args.weights_test))\n\n    # This key is used to load the ImageNet weights while training. So, set to empty to avoid errors\n    args.weights = \'\'\n\n    args.save_dir = \'results_detection_{}_{}/{}_{}/\'.format(args.model, args.s, args.dataset, args.im_size)\n\n    if not os.path.isdir(args.save_dir):\n        os.makedirs(args.save_dir)\n\n    main(args)\n'"
test_segmentation.py,3,"b'import torch\nimport glob\nimport os\nfrom argparse import ArgumentParser\nfrom PIL import Image\nfrom torchvision.transforms import functional as F\nfrom tqdm import tqdm\nfrom utilities.print_utils import *\nfrom transforms.classification.data_transforms import MEAN, STD\nfrom utilities.utils import model_parameters, compute_flops\n\n# ===========================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\n\ndef relabel(img):\n    \'\'\'\n    This function relabels the predicted labels so that cityscape dataset can process\n    :param img:\n    :return:\n    \'\'\'\n    img[img == 19] = 255\n    img[img == 18] = 33\n    img[img == 17] = 32\n    img[img == 16] = 31\n    img[img == 15] = 28\n    img[img == 14] = 27\n    img[img == 13] = 26\n    img[img == 12] = 25\n    img[img == 11] = 24\n    img[img == 10] = 23\n    img[img == 9] = 22\n    img[img == 8] = 21\n    img[img == 7] = 20\n    img[img == 6] = 19\n    img[img == 5] = 17\n    img[img == 4] = 13\n    img[img == 3] = 12\n    img[img == 2] = 11\n    img[img == 1] = 8\n    img[img == 0] = 7\n    img[img == 255] = 0\n    return img\n\n\ndef data_transform(img, im_size):\n    img = img.resize(im_size, Image.BILINEAR)\n    img = F.to_tensor(img)  # convert to tensor (values between 0 and 1)\n    img = F.normalize(img, MEAN, STD)  # normalize the tensor\n    return img\n\n\ndef evaluate(args, model, image_list, device):\n    im_size = tuple(args.im_size)\n\n    # get color map for pascal dataset\n    if args.dataset == \'pascal\':\n        from utilities.color_map import VOCColormap\n        cmap = VOCColormap().get_color_map_voc()\n    else:\n        cmap = None\n\n    model.eval()\n    for i, imgName in tqdm(enumerate(image_list)):\n        img = Image.open(imgName).convert(\'RGB\')\n        w, h = img.size\n\n        img = data_transform(img, im_size)\n        img = img.unsqueeze(0)  # add a batch dimension\n        img = img.to(device)\n        img_out = model(img)\n        img_out = img_out.squeeze(0)  # remove the batch dimension\n        img_out = img_out.max(0)[1].byte()  # get the label map\n        img_out = img_out.to(device=\'cpu\').numpy()\n\n        if args.dataset == \'city\':\n            # cityscape uses different IDs for training and testing\n            # so, change from Train IDs to actual IDs\n            img_out = relabel(img_out)\n\n        img_out = Image.fromarray(img_out)\n        # resize to original size\n        img_out = img_out.resize((w, h), Image.NEAREST)\n\n        # pascal dataset accepts colored segmentations\n        if args.dataset == \'pascal\':\n            img_out.putpalette(cmap)\n\n        # save the segmentation mask\n        name = imgName.split(\'/\')[-1]\n        img_extn = imgName.split(\'.\')[-1]\n        name = \'{}/{}\'.format(args.savedir, name.replace(img_extn, \'png\'))\n        img_out.save(name)\n\n\ndef main(args):\n    # read all the images in the folder\n    if args.dataset == \'city\':\n        image_path = os.path.join(args.data_path, ""leftImg8bit"", args.split, ""*"", ""*.png"")\n        image_list = glob.glob(image_path)\n        from data_loader.segmentation.cityscapes import CITYSCAPE_CLASS_LIST\n        seg_classes = len(CITYSCAPE_CLASS_LIST)\n    elif args.dataset == \'pascal\':\n        from data_loader.segmentation.voc import VOC_CLASS_LIST\n        seg_classes = len(VOC_CLASS_LIST)\n        data_file = os.path.join(args.data_path, \'VOC2012\', \'list\', \'{}.txt\'.format(args.split))\n        if not os.path.isfile(data_file):\n            print_error_message(\'{} file does not exist\'.format(data_file))\n        image_list = []\n        with open(data_file, \'r\') as lines:\n            for line in lines:\n                rgb_img_loc = \'{}/{}/{}\'.format(args.data_path, \'VOC2012\', line.split()[0])\n                if not os.path.isfile(rgb_img_loc):\n                    print_error_message(\'{} image file does not exist\'.format(rgb_img_loc))\n                image_list.append(rgb_img_loc)\n    else:\n        print_error_message(\'{} dataset not yet supported\'.format(args.dataset))\n\n    if len(image_list) == 0:\n        print_error_message(\'No files in directory: {}\'.format(image_path))\n\n    print_info_message(\'# of images for testing: {}\'.format(len(image_list)))\n\n    if args.model == \'espnetv2\':\n        from model.segmentation.espnetv2 import espnetv2_seg\n        args.classes = seg_classes\n        model = espnetv2_seg(args)\n    elif args.model == \'dicenet\':\n        from model.segmentation.dicenet import dicenet_seg\n        model = dicenet_seg(args, classes=seg_classes)\n    else:\n        print_error_message(\'{} network not yet supported\'.format(args.model))\n        exit(-1)\n\n    # mdoel information\n    num_params = model_parameters(model)\n    flops = compute_flops(model, input=torch.Tensor(1, 3, args.im_size[0], args.im_size[1]))\n    print_info_message(\'FLOPs for an input of size {}x{}: {:.2f} million\'.format(args.im_size[0], args.im_size[1], flops))\n    print_info_message(\'# of parameters: {}\'.format(num_params))\n\n    if args.weights_test:\n        print_info_message(\'Loading model weights\')\n        weight_dict = torch.load(args.weights_test, map_location=torch.device(\'cpu\'))\n        model.load_state_dict(weight_dict)\n        print_info_message(\'Weight loaded successfully\')\n    else:\n        print_error_message(\'weight file does not exist or not specified. Please check: {}\', format(args.weights_test))\n\n    num_gpus = torch.cuda.device_count()\n    device = \'cuda\' if num_gpus > 0 else \'cpu\'\n    model = model.to(device=device)\n\n    evaluate(args, model, image_list, device=device)\n\n\nif __name__ == \'__main__\':\n    from commons.general_details import segmentation_models, segmentation_datasets\n\n    parser = ArgumentParser()\n    # mdoel details\n    parser.add_argument(\'--model\', default=""espnetv2"", choices=segmentation_models, help=\'Model name\')\n    parser.add_argument(\'--weights-test\', default=\'\', help=\'Pretrained weights directory.\')\n    parser.add_argument(\'--s\', default=2.0, type=float, help=\'scale\')\n    # dataset details\n    parser.add_argument(\'--data-path\', default="""", help=\'Data directory\')\n    parser.add_argument(\'--dataset\', default=\'city\', choices=segmentation_datasets, help=\'Dataset name\')\n    # input details\n    parser.add_argument(\'--im-size\', type=int, nargs=""+"", default=[512, 256], help=\'Image size for testing (W x H)\')\n    parser.add_argument(\'--split\', default=\'val\', choices=[\'val\', \'test\'], help=\'data split\')\n    parser.add_argument(\'--model-width\', default=224, type=int, help=\'Model width\')\n    parser.add_argument(\'--model-height\', default=224, type=int, help=\'Model height\')\n    parser.add_argument(\'--channels\', default=3, type=int, help=\'Input channels\')\n    parser.add_argument(\'--num-classes\', default=1000, type=int,\n                        help=\'ImageNet classes. Required for loading the base network\')\n\n    args = parser.parse_args()\n\n    if not args.weights_test:\n        from model.weight_locations.segmentation import model_weight_map\n\n        model_key = \'{}_{}\'.format(args.model, args.s)\n        dataset_key = \'{}_{}x{}\'.format(args.dataset, args.im_size[0], args.im_size[1])\n        assert model_key in model_weight_map.keys(), \'{} does not exist\'.format(model_key)\n        assert dataset_key in model_weight_map[model_key].keys(), \'{} does not exist\'.format(dataset_key)\n        args.weights_test = model_weight_map[model_key][dataset_key][\'weights\']\n        if not os.path.isfile(args.weights_test):\n            print_error_message(\'weight file does not exist: {}\'.format(args.weights_test))\n\n    # set-up results path\n    if args.dataset == \'city\':\n        args.savedir = \'{}_{}_{}/results\'.format(\'results\', args.dataset, args.split)\n    elif args.dataset == \'pascal\':\n        args.savedir = \'{}_{}/results/VOC2012/Segmentation/comp6_{}_cls\'.format(\'results\', args.dataset, args.split)\n    else:\n        print_error_message(\'{} dataset not yet supported\'.format(args.dataset))\n\n    if not os.path.isdir(args.savedir):\n        os.makedirs(args.savedir)\n\n    # This key is used to load the ImageNet weights while training. So, set to empty to avoid errors\n    args.weights = \'\'\n\n    main(args)\n'"
train_classification.py,19,"b'import torch\nimport argparse\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\nfrom data_loader.classification import imagenet as img_loader\nimport random\nimport os\nfrom torch.utils.tensorboard import SummaryWriter\nimport time\nfrom utilities.utils import model_parameters, compute_flops\nfrom utilities.utils import save_checkpoint\nimport numpy as np\nfrom utilities.print_utils import *\nfrom torch import nn\n\n# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\n\ndef main(args):\n    # -----------------------------------------------------------------------------\n    # Create model\n    # -----------------------------------------------------------------------------\n    if args.model == \'dicenet\':\n        from model.classification import dicenet as net\n        model = net.CNNModel(args)\n    elif args.model == \'espnetv2\':\n        from model.classification import espnetv2 as net\n        model = net.EESPNet(args)\n    elif args.model == \'shufflenetv2\':\n        from model.classification import shufflenetv2 as net\n        model = net.CNNModel(args)\n    else:\n        print_error_message(\'Model {} not yet implemented\'.format(args.model))\n        exit()\n\n    if args.finetune:\n        # laod the weights for finetuning\n        if os.path.isfile(args.weights_ft):\n            pretrained_dict = torch.load(args.weights_ft, map_location=torch.device(\'cpu\'))\n            print_info_message(\'Loading pretrained basenet model weights\')\n            model_dict = model.state_dict()\n\n            overlap_dict = {k: v for k, v in model_dict.items() if k in pretrained_dict}\n\n            total_size_overlap = 0\n            for k, v in enumerate(overlap_dict):\n                total_size_overlap += torch.numel(overlap_dict[v])\n\n            total_size_pretrain = 0\n            for k, v in enumerate(pretrained_dict):\n                total_size_pretrain += torch.numel(pretrained_dict[v])\n\n            if len(overlap_dict) == 0:\n                print_error_message(\'No overlaping weights between model file and pretrained weight file. Please check\')\n\n            print_info_message(\'Overlap ratio of weights: {:.2f} %\'.format(\n                (total_size_overlap * 100.0) / total_size_pretrain))\n\n            model_dict.update(overlap_dict)\n            model.load_state_dict(model_dict, strict=False)\n            print_info_message(\'Pretrained basenet model loaded!!\')\n        else:\n            print_error_message(\'Unable to find the weights: {}\'.format(args.weights_ft))\n\n    # -----------------------------------------------------------------------------\n    # Writer for logging\n    # -----------------------------------------------------------------------------\n    if not os.path.isdir(args.savedir):\n        os.makedirs(args.savedir)\n    writer = SummaryWriter(log_dir=args.savedir, comment=\'Training and Validation logs\')\n    try:\n        writer.add_graph(model, input_to_model=torch.randn(1, 3, args.inpSize, args.inpSize))\n    except:\n        print_log_message(""Not able to generate the graph. Likely because your model is not supported by ONNX"")\n\n    # network properties\n    num_params = model_parameters(model)\n    flops = compute_flops(model, input=torch.Tensor(1, 3, args.inpSize, args.inpSize))\n    print_info_message(\'FLOPs: {:.2f} million\'.format(flops))\n    print_info_message(\'Network Parameters: {:.2f} million\'.format(num_params))\n\n    # -----------------------------------------------------------------------------\n    # Optimizer\n    # -----------------------------------------------------------------------------\n\n    optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momentum, weight_decay=args.weight_decay)\n\n    # optionally resume from a checkpoint\n    best_acc = 0.0\n    num_gpus = torch.cuda.device_count()\n    device = \'cuda\' if num_gpus >= 1 else \'cpu\'\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print_info_message(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume, map_location=torch.device(\'cpu\'))\n            args.start_epoch = checkpoint[\'epoch\']\n            best_acc = checkpoint[\'best_prec1\']\n            model.load_state_dict(checkpoint[\'state_dict\'], map_location=torch.device(device))\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print_info_message(""=> loaded checkpoint \'{}\' (epoch {})""\n                               .format(args.resume, checkpoint[\'epoch\']))\n        else:\n            print_warning_message(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\n    # -----------------------------------------------------------------------------\n    # Loss Fn\n    # -----------------------------------------------------------------------------\n    if args.dataset == \'imagenet\':\n        criterion = nn.CrossEntropyLoss()\n        acc_metric = \'Top-1\'\n    elif args.dataset == \'coco\':\n        criterion = nn.BCEWithLogitsLoss()\n        acc_metric = \'F1\'\n    else:\n        print_error_message(\'{} dataset not yet supported\'.format(args.dataset))\n\n    if num_gpus >= 1:\n        model = torch.nn.DataParallel(model)\n        model = model.cuda()\n        criterion = criterion.cuda()\n        if torch.backends.cudnn.is_available():\n            import torch.backends.cudnn as cudnn\n            cudnn.benchmark = True\n            cudnn.deterministic = True\n\n    # -----------------------------------------------------------------------------\n    # Data Loaders\n    # -----------------------------------------------------------------------------\n    # Data loading code\n    if args.dataset == \'imagenet\':\n        train_loader, val_loader = img_loader.data_loaders(args)\n        # import the loaders too\n        from utilities.train_eval_classification import train, validate\n    elif args.dataset == \'coco\':\n        from data_loader.classification.coco import COCOClassification\n        train_dataset = COCOClassification(root=args.data, split=\'train\', year=\'2017\', inp_size=args.inpSize,\n                                           scale=args.scale, is_training=True)\n        val_dataset = COCOClassification(root=args.data, split=\'val\', year=\'2017\', inp_size=args.inpSize,\n                                         is_training=False)\n\n        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n                                                   pin_memory=True, num_workers=args.workers)\n        val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n                                                 pin_memory=True, num_workers=args.workers)\n\n        # import the loaders too\n        from utilities.train_eval_classification import train_multi as train\n        from utilities.train_eval_classification import validate_multi as validate\n    else:\n        print_error_message(\'{} dataset not yet supported\'.format(args.dataset))\n\n    # -----------------------------------------------------------------------------\n    # LR schedulers\n    # -----------------------------------------------------------------------------\n    if args.scheduler == \'fixed\':\n        step_sizes = args.steps\n        from utilities.lr_scheduler import FixedMultiStepLR\n        lr_scheduler = FixedMultiStepLR(base_lr=args.lr, steps=step_sizes, gamma=args.lr_decay)\n    elif args.scheduler == \'clr\':\n        from utilities.lr_scheduler import CyclicLR\n        step_sizes = args.steps\n        lr_scheduler = CyclicLR(min_lr=args.lr, cycle_len=5, steps=step_sizes, gamma=args.lr_decay)\n    elif args.scheduler == \'poly\':\n        from utilities.lr_scheduler import PolyLR\n        lr_scheduler = PolyLR(base_lr=args.lr, max_epochs=args.epochs)\n    elif args.scheduler == \'linear\':\n        from utilities.lr_scheduler import LinearLR\n        lr_scheduler = LinearLR(base_lr=args.lr, max_epochs=args.epochs)\n    elif args.scheduler == \'hybrid\':\n        from utilities.lr_scheduler import HybirdLR\n        lr_scheduler = HybirdLR(base_lr=args.lr, max_epochs=args.epochs, clr_max=args.clr_max)\n    else:\n        print_error_message(\'Scheduler ({}) not yet implemented\'.format(args.scheduler))\n        exit()\n\n    print_info_message(lr_scheduler)\n\n    # set up the epoch variable in case resuming training\n    if args.start_epoch != 0:\n        for epoch in range(args.start_epoch):\n            lr_scheduler.step(epoch)\n\n    with open(args.savedir + os.sep + \'arguments.json\', \'w\') as outfile:\n        import json\n        arg_dict = vars(args)\n        arg_dict[\'model_params\'] = \'{} \'.format(num_params)\n        arg_dict[\'flops\'] = \'{} \'.format(flops)\n        json.dump(arg_dict, outfile)\n\n    # -----------------------------------------------------------------------------\n    # Training and Val Loop\n    # -----------------------------------------------------------------------------\n\n    extra_info_ckpt = args.model + \'_\' + str(args.s)\n    for epoch in range(args.start_epoch, args.epochs):\n        lr_log = lr_scheduler.step(epoch)\n        # set the optimizer with the learning rate\n        # This can be done inside the MyLRScheduler\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lr_log\n        print_info_message(""LR for epoch {} = {:.5f}"".format(epoch, lr_log))\n        train_acc, train_loss = train(data_loader=train_loader, model=model, criteria=criterion, optimizer=optimizer,\n                                      epoch=epoch, device=device)\n        # evaluate on validation set\n        val_acc, val_loss = validate(data_loader=val_loader, model=model, criteria=criterion, device=device)\n\n        # remember best prec@1 and save checkpoint\n        is_best = val_acc > best_acc\n        best_acc = max(val_acc, best_acc)\n\n        weights_dict = model.module.state_dict() if device == \'cuda\' else model.state_dict()\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'state_dict\': weights_dict,\n            \'best_prec1\': best_acc,\n            \'optimizer\': optimizer.state_dict(),\n        }, is_best, args.savedir, extra_info_ckpt)\n\n        writer.add_scalar(\'Classification/LR/learning_rate\', lr_log, epoch)\n        writer.add_scalar(\'Classification/Loss/Train\', train_loss, epoch)\n        writer.add_scalar(\'Classification/Loss/Val\', val_loss, epoch)\n        writer.add_scalar(\'Classification/{}/Train\'.format(acc_metric), train_acc, epoch)\n        writer.add_scalar(\'Classification/{}/Val\'.format(acc_metric), val_acc, epoch)\n        writer.add_scalar(\'Classification/Complexity/Top1_vs_flops\', best_acc, round(flops, 2))\n        writer.add_scalar(\'Classification/Complexity/Top1_vs_params\', best_acc, round(num_params, 2))\n\n    writer.close()\n\n\nif __name__ == \'__main__\':\n    from commons.general_details import classification_models, classification_datasets, classification_exp_choices, \\\n        classification_schedulers\n\n    parser = argparse.ArgumentParser(description=\'Training efficient networks\')\n    # General settings\n    parser.add_argument(\'--workers\', default=12, type=int, help=\'number of data loading workers (default: 4)\')\n    parser.add_argument(\'--batch-size\', default=512, type=int, help=\'mini-batch size (default: 512)\')\n\n    # Dataset related settings\n    parser.add_argument(\'--data\', default=\'\', help=\'path to dataset\')\n    parser.add_argument(\'--dataset\', default=\'imagenet\', help=\'Name of the dataset\', choices=classification_datasets)\n\n    # LR scheduler settings\n    parser.add_argument(\'--epochs\', default=300, type=int, help=\'number of total epochs to run\')\n    parser.add_argument(\'--start-epoch\', default=0, type=int, help=\'manual epoch number (useful on restarts)\')\n    parser.add_argument(\'--clr-max\', default=61, type=int, help=\'Max. epochs for CLR in Hybrid scheduler\')\n    parser.add_argument(\'--steps\', default=[51, 101, 131, 161, 191, 221, 251, 281], type=int, nargs=""+"",\n                        help=\'steps at which lr should be decreased. Only used for Cyclic and Fixed LR\')\n    parser.add_argument(\'--scheduler\', default=\'clr\', choices=classification_schedulers,\n                        help=\'Learning rate scheduler\')\n    parser.add_argument(\'--lr\', default=0.1, type=float, help=\'initial learning rate\')\n    parser.add_argument(\'--lr-decay\', default=0.5, type=float, help=\'factor by which lr should be decreased\')\n\n    # optimizer settings\n    parser.add_argument(\'--momentum\', default=0.9, type=float, help=\'momentum\')\n    parser.add_argument(\'--weight-decay\', default=4e-5, type=float, help=\'weight decay (default: 4e-5)\')\n    parser.add_argument(\'--resume\', default=\'\', type=str, help=\'path to latest checkpoint (default: none)\')\n    parser.add_argument(\'--savedir\', type=str, default=\'results_classification\', help=\'Location to save the results\')\n\n    # Model settings\n    parser.add_argument(\'--s\', default=1.0, type=float, help=\'Factor by which output channels should be scaled (s > 1 \'\n                                                           \'for increasing the dims while < 1 for decreasing)\')\n    parser.add_argument(\'--inpSize\', default=224, type=int, help=\'Input image size (default: 224 x 224)\')\n    parser.add_argument(\'--scale\', default=[0.2, 1.0], type=float, nargs=""+"", help=\'Scale for data augmentation\')\n    parser.add_argument(\'--model\', default=\'shuffle_vw\', choices=classification_models,\n                        help=\'Which model? basic= basic CNN model, res=resnet style)\')\n    parser.add_argument(\'--channels\', default=3, type=int, help=\'Input channels\')\n    # DiceNet related settings\n    parser.add_argument(\'--model-width\', default=224, type=int, help=\'Model width\')\n    parser.add_argument(\'--model-height\', default=224, type=int, help=\'Model height\')\n\n    ## Experiment related settings\n    parser.add_argument(\'--exp-type\', type=str, choices=classification_exp_choices, default=\'main\',\n                        help=\'Experiment type\')\n    parser.add_argument(\'--finetune\', action=\'store_true\', default=False, help=\'Finetune the model\')\n\n    args = parser.parse_args()\n\n    assert len(args.scale) == 2\n    args.scale = tuple(args.scale)\n\n    random.seed(1882)\n    torch.manual_seed(1882)\n\n    timestr = time.strftime(""%Y%m%d-%H%M%S"")\n    args.savedir = \'{}_{}/model_{}_{}/aug_{}_{}/s_{}_inp_{}_sch_{}/{}/\'.format(args.savedir, args.exp_type, args.model,\n                                                                               args.dataset, args.scale[0],\n                                                                               args.scale[1],\n                                                                               args.s, args.inpSize, args.scheduler,\n                                                                               timestr)\n\n    # if you want to finetune ImageNet model on other dataset, say MS-COCO classification\n    if args.finetune:\n        print_info_message(\'Grabbing location of the ImageNet weights from the weight dictionary\')\n        from model.weight_locations.classification import model_weight_map\n\n        weight_file_key = \'{}_{}\'.format(args.model, args.s)\n        assert weight_file_key in model_weight_map.keys(), \'{} does not exist\'.format(weight_file_key)\n        args.weights_ft = model_weight_map[weight_file_key]\n\n\n    if args.dataset == \'imagenet\':\n        args.num_classes = 1000\n    elif args.dataset == \'coco\':\n        from data_loader.classification.coco import COCO_CLASS_LIST\n        args.num_classes = len(COCO_CLASS_LIST)\n\n    main(args)\n'"
train_detection.py,13,"b'import argparse\nimport os\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom data_loader.detection.augmentation import TrainTransform, ValTransform\nfrom model.detection.match_priors import MatchPrior\nfrom model.detection.generate_priors import PriorBox\nfrom loss_fns.multi_box_loss import MultiBoxLoss\nfrom utilities.train_eval_detect import train, validate\nfrom utilities.utils import save_checkpoint, model_parameters, compute_flops\nimport math\nfrom torch.utils.tensorboard import SummaryWriter\nfrom utilities.print_utils import *\nfrom model.detection.ssd import ssd\n\ndef main(args):\n    if args.im_size in [300, 512]:\n        from model.detection.ssd_config import get_config\n        cfg = get_config(args.im_size)\n    else:\n        print_error_message(\'{} image size not supported\'.format(args.im_size))\n\n    # -----------------------------------------------------------------------------\n    # Dataset\n    # -----------------------------------------------------------------------------\n    train_transform = TrainTransform(cfg.image_size)\n    target_transform = MatchPrior(PriorBox(cfg)(), cfg.center_variance, cfg.size_variance, cfg.iou_threshold)\n    val_transform = ValTransform(cfg.image_size)\n\n    if args.dataset in [\'voc\', \'pascal\']:\n        from data_loader.detection.voc import VOCDataset, VOC_CLASS_LIST\n        train_dataset_2007 = VOCDataset(root_dir=args.data_path, transform=train_transform,\n                                        target_transform=target_transform,\n                                        is_training=True, split=""VOC2007"")\n        train_dataset_2012 = VOCDataset(root_dir=args.data_path, transform=train_transform,\n                                        target_transform=target_transform,\n                                        is_training=True, split=""VOC2012"")\n        train_dataset = torch.utils.data.ConcatDataset([train_dataset_2007, train_dataset_2012])\n        val_dataset = VOCDataset(root_dir=args.data_path, transform=val_transform, target_transform=target_transform,\n                                 is_training=False, split=""VOC2007"")\n        num_classes = len(VOC_CLASS_LIST)\n    elif args.dataset == \'coco\':\n        from data_loader.detection.coco import COCOObjectDetection, COCO_CLASS_LIST\n        train_dataset = COCOObjectDetection(root_dir=args.data_path, transform=train_transform,\n                                            target_transform=target_transform, is_training=True)\n        val_dataset = COCOObjectDetection(root_dir=args.data_path, transform=val_transform, target_transform=target_transform, is_training=False)\n        num_classes = len(COCO_CLASS_LIST)\n    else:\n        print_error_message(\'{} dataset is not supported yet\'.format(args.dataset))\n        exit()\n    cfg.NUM_CLASSES = num_classes\n\n    # -----------------------------------------------------------------------------\n    # Dataset loader\n    # -----------------------------------------------------------------------------\n    print_info_message(\'Training samples: {}\'.format(len(train_dataset)))\n    print_info_message(\'Validation samples: {}\'.format(len(val_dataset)))\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers,\n                              pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers,\n                            pin_memory=True)\n    # -----------------------------------------------------------------------------\n    # Model\n    # -----------------------------------------------------------------------------\n    model = ssd(args, cfg)\n    if args.finetune:\n        if os.path.isfile(args.finetune):\n            print_info_message(\'Loading weights for finetuning from {}\'.format(args.finetune))\n            weight_dict = torch.load(args.finetune, map_location=torch.device(device=\'cpu\'))\n            model.load_state_dict(weight_dict)\n            print_info_message(\'Done\')\n        else:\n            print_warning_message(\'No file for finetuning. Please check.\')\n\n    if args.freeze_bn:\n        print_info_message(\'Freezing batch normalization layers\')\n        for m in model.modules():\n            if isinstance(m, torch.nn.BatchNorm2d):\n                m.eval()\n                m.weight.requires_grad = False\n                m.bias.requires_grad = False\n    # -----------------------------------------------------------------------------\n    # Optimizer and Criterion\n    # -----------------------------------------------------------------------------\n    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.wd)\n\n    criterion = MultiBoxLoss(neg_pos_ratio=cfg.neg_pos_ratio)\n\n    # writer for logs\n    writer = SummaryWriter(log_dir=args.save, comment=\'Training and Validation logs\')\n    try:\n        writer.add_graph(model, input_to_model=torch.Tensor(1, 3, cfg.image_size, cfg.image_size))\n    except:\n        print_log_message(""Not able to generate the graph. Likely because your model is not supported by ONNX"")\n\n    #model stats\n    num_params = model_parameters(model)\n    flops = compute_flops(model, input=torch.Tensor(1, 3, cfg.image_size, cfg.image_size))\n    print_info_message(\'FLOPs for an input of size {}x{}: {:.2f} million\'.format(cfg.image_size, cfg.image_size, flops))\n    print_info_message(\'Network Parameters: {:.2f} million\'.format(num_params))\n\n    num_gpus = torch.cuda.device_count()\n    device = \'cuda\' if num_gpus >= 1 else \'cpu\'\n\n    min_val_loss = float(\'inf\')\n    start_epoch = 0  # start from epoch 0 or last epoch\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print_info_message(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.checkpoint, map_location=torch.device(\'cpu\'))\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            min_val_loss = checkpoint[\'min_loss\']\n            start_epoch = checkpoint[\'epoch\']\n        else:\n            print_warning_message(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\n    if num_gpus >= 1:\n        model = torch.nn.DataParallel(model)\n        model = model.to(device)\n        if torch.backends.cudnn.is_available():\n            import torch.backends.cudnn as cudnn\n            cudnn.benchmark = True\n            cudnn.deterministic = True\n\n    # -----------------------------------------------------------------------------\n    # Scheduler\n    # -----------------------------------------------------------------------------\n    if args.lr_type == \'poly\':\n        from utilities.lr_scheduler import PolyLR\n        lr_scheduler = PolyLR(base_lr=args.lr, max_epochs=args.epochs, power=args.power)\n    elif args.lr_type == \'hybrid\':\n        from utilities.lr_scheduler import HybirdLR\n        lr_scheduler = HybirdLR(base_lr=args.lr, max_epochs=args.epochs, clr_max=args.clr_max, cycle_len=args.cycle_len)\n    elif args.lr_type == \'clr\':\n        from utilities.lr_scheduler import CyclicLR\n        lr_scheduler = CyclicLR(min_lr=args.lr, cycle_len=args.cycle_len, steps=args.steps, gamma=args.gamma, step=True)\n    elif args.lr_type == \'cosine\':\n        from utilities.lr_scheduler import CosineLR\n        lr_scheduler = CosineLR(base_lr=args.lr, max_epochs=args.epochs)\n    else:\n        print_error_message(\'{} scheduler not yet supported\'.format(args.lr_type))\n        exit()\n\n    print_info_message(lr_scheduler)\n\n    # -----------------------------------------------------------------------------\n    # Training and validation loop\n    # -----------------------------------------------------------------------------\n\n    extra_info_ckpt = \'{}_{}\'.format(args.model, args.s)\n    for epoch in range(start_epoch, args.epochs):\n        curr_lr = lr_scheduler.step(epoch)\n        optimizer.param_groups[0][\'lr\'] = curr_lr\n\n        print_info_message(\'Running epoch {} at LR {}\'.format(epoch, curr_lr))\n        train_loss, train_cl_loss, train_loc_loss = train(train_loader, model, criterion, optimizer, device, epoch=epoch)\n        val_loss, val_cl_loss, val_loc_loss = validate(val_loader, model, criterion, device, epoch=epoch)\n        # Save checkpoint\n        is_best = val_loss < min_val_loss\n        min_val_loss = min(val_loss, min_val_loss)\n\n        weights_dict = model.module.state_dict() if device == \'cuda\' else model.state_dict()\n        save_checkpoint({\n            \'epoch\': epoch,\n            \'model\': args.model,\n            \'state_dict\': weights_dict,\n            \'min_loss\': min_val_loss\n        }, is_best, args.save, extra_info_ckpt)\n\n        writer.add_scalar(\'Detection/LR/learning_rate\', round(curr_lr, 6), epoch)\n        writer.add_scalar(\'Detection/Loss/train\', train_loss, epoch)\n        writer.add_scalar(\'Detection/Loss/val\', val_loss, epoch)\n        writer.add_scalar(\'Detection/Loss/train_cls\', train_cl_loss, epoch)\n        writer.add_scalar(\'Detection/Loss/val_cls\', val_cl_loss, epoch)\n        writer.add_scalar(\'Detection/Loss/train_loc\', train_loc_loss, epoch)\n        writer.add_scalar(\'Detection/Loss/val_loc\', val_loc_loss, epoch)\n        writer.add_scalar(\'Detection/Complexity/Flops\', min_val_loss, math.ceil(flops))\n        writer.add_scalar(\'Detection/Complexity/Params\', min_val_loss, math.ceil(num_params))\n\n    writer.close()\n\n\nif __name__ == \'__main__\':\n    from commons.general_details import detection_datasets, detection_models, detection_schedulers\n\n    parser = argparse.ArgumentParser(description=\'Training detection network\')\n    ### MODEL RELATED PARAMS\n    parser.add_argument(\'--resume\', action=\'store_true\', help=\'resume from checkpoint\')\n    parser.add_argument(\'--model\', default=\'espnetv2\', choices=detection_models, type=str, help=\'initialized model path\')\n    parser.add_argument(\'--s\', default=2.0, type=float, help=\'Model scale factor\')\n    parser.add_argument(\'--channels\', default=3, type=int, help=\'Input channels\')\n    # dimension wise network related params\n    parser.add_argument(\'--model-width\', default=224, type=int, help=\'Model width\')\n    parser.add_argument(\'--model-height\', default=224, type=int, help=\'Model height\')\n\n    ### General configuration such as dataset path, etc\n    parser.add_argument(\'--save\', default=\'results_detection\', type=str, help=\'results path\')\n    parser.add_argument(\'--dataset\', default=\'pascal\', choices=detection_datasets, help=\'Name of the dataset\')\n    parser.add_argument(\'--data-path\', default=\'\', help=\'Dataset path\')\n\n    #### OPTIMIZER related settings\n    parser.add_argument(\'--momentum\', default=0.9, type=float, help=\'Momentum value for optim\')\n    parser.add_argument(\'--wd\', default=5e-4, type=float, help=\'Weight decay for SGD\')\n    parser.add_argument(\'--gamma\', default=0.1, type=float, help=\'Gamma update for SGD\')\n    parser.add_argument(\'--power\', default=0.9, type=float, help=\'Power for Polynomial LR\')\n    parser.add_argument(\'--lr-type\', default=\'clr\', type=str, choices=detection_schedulers, help=\'LR scheduler\')\n    parser.add_argument(\'--lr\', default=1e-2, type=float, help=\'initial learning rate\')\n    parser.add_argument(\'--lr-mult\', default=1, type=int, help=\'Factor by which base lr should be increased\')\n    # Hybrid LR hyperparameters\n    parser.add_argument(\'--clr-max\', default=160, type=int, help=\'Max CLR epochs (only for hybrid)\')\n    parser.add_argument(\'--cycle-len\', default=5, type=int, help=\'Cycle length for CLR\')\n    # CLR/Multi-step LR related hyper-parameters\n    parser.add_argument(\'--steps\', default=[51, 161, 201], type=int, nargs=""+"",\n                        help=\'steps at which lr should be decreased. Only used for Cyclic and Fixed LR\')\n\n    # general training parameters\n    parser.add_argument(\'--batch-size\', type=int, default=32, help=\'Batch size\')\n    parser.add_argument(\'--workers\', type=int, default=4, help=\'Number of workers for laoding data\')\n    parser.add_argument(\'--epochs\', default=240, type=int, help=\'Max number of epochs\')\n    parser.add_argument(\'--weights\', default=\'\', type=str, help=\'Location of pretrained weights\')\n    parser.add_argument(\'--im-size\', default=300, type=int, help=\'Image size for training\')\n    # finetune the model\n    parser.add_argument(\'--finetune\', default=\'\', type=str, help=\'finetune\')\n    parser.add_argument(\'--freeze-bn\', action=\'store_true\', default=False, help=\'Freeze BN params or not\')\n\n    args = parser.parse_args()\n\n    if not args.weights:\n        print_info_message(\'Loading weights using the weight dictionary\')\n        from model.weight_locations.classification import model_weight_map\n        weight_file_key = \'{}_{}\'.format(args.model, args.s)\n        assert weight_file_key in model_weight_map.keys(), \'{} does not exist\'.format(weight_file_key)\n        args.weights = model_weight_map[weight_file_key]\n\n    timestr = time.strftime(""%Y%m%d-%H%M%S"")\n    args.save = \'{}/model_{}_{}/s_{}_sch_{}_im_{}/{}/\'.format(args.save, args.model, args.dataset, args.s,\n                                                                args.lr_type, args.im_size, timestr)\n\n    if not os.path.exists(args.save):\n        os.makedirs(args.save)\n\n    main(args)\n'"
train_segmentation.py,16,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nimport argparse\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom utilities.utils import save_checkpoint, model_parameters, compute_flops\nfrom utilities.train_eval_seg import train_seg as train\nfrom utilities.train_eval_seg import val_seg as val\nfrom torch.utils.tensorboard import SummaryWriter\nfrom loss_fns.segmentation_loss import SegmentationLoss\nimport random\nimport math\nimport time\nimport numpy as np\nfrom utilities.print_utils import *\n\n\ndef main(args):\n    crop_size = args.crop_size\n    assert isinstance(crop_size, tuple)\n    print_info_message(\'Running Model at image resolution {}x{} with batch size {}\'.format(crop_size[0], crop_size[1],\n                                                                                           args.batch_size))\n    if not os.path.isdir(args.savedir):\n        os.makedirs(args.savedir)\n\n    if args.dataset == \'pascal\':\n        from data_loader.segmentation.voc import VOCSegmentation, VOC_CLASS_LIST\n        train_dataset = VOCSegmentation(root=args.data_path, train=True, crop_size=crop_size, scale=args.scale,\n                                        coco_root_dir=args.coco_path)\n        val_dataset = VOCSegmentation(root=args.data_path, train=False, crop_size=crop_size, scale=args.scale)\n        seg_classes = len(VOC_CLASS_LIST)\n        class_wts = torch.ones(seg_classes)\n    elif args.dataset == \'city\':\n        from data_loader.segmentation.cityscapes import CityscapesSegmentation, CITYSCAPE_CLASS_LIST\n        train_dataset = CityscapesSegmentation(root=args.data_path, train=True, size=crop_size, scale=args.scale,\n                                               coarse=args.coarse)\n        val_dataset = CityscapesSegmentation(root=args.data_path, train=False, size=crop_size, scale=args.scale,\n                                             coarse=False)\n        seg_classes = len(CITYSCAPE_CLASS_LIST)\n        class_wts = torch.ones(seg_classes)\n        class_wts[0] = 2.8149201869965\n        class_wts[1] = 6.9850029945374\n        class_wts[2] = 3.7890393733978\n        class_wts[3] = 9.9428062438965\n        class_wts[4] = 9.7702074050903\n        class_wts[5] = 9.5110931396484\n        class_wts[6] = 10.311357498169\n        class_wts[7] = 10.026463508606\n        class_wts[8] = 4.6323022842407\n        class_wts[9] = 9.5608062744141\n        class_wts[10] = 7.8698215484619\n        class_wts[11] = 9.5168733596802\n        class_wts[12] = 10.373730659485\n        class_wts[13] = 6.6616044044495\n        class_wts[14] = 10.260489463806\n        class_wts[15] = 10.287888526917\n        class_wts[16] = 10.289801597595\n        class_wts[17] = 10.405355453491\n        class_wts[18] = 10.138095855713\n        class_wts[19] = 0.0\n    else:\n        print_error_message(\'Dataset: {} not yet supported\'.format(args.dataset))\n        exit(-1)\n\n    print_info_message(\'Training samples: {}\'.format(len(train_dataset)))\n    print_info_message(\'Validation samples: {}\'.format(len(val_dataset)))\n\n    if args.model == \'espnetv2\':\n        from model.segmentation.espnetv2 import espnetv2_seg\n        args.classes = seg_classes\n        model = espnetv2_seg(args)\n    elif args.model == \'dicenet\':\n        from model.segmentation.dicenet import dicenet_seg\n        model = dicenet_seg(args, classes=seg_classes)\n    else:\n        print_error_message(\'Arch: {} not yet supported\'.format(args.model))\n        exit(-1)\n\n    if args.finetune:\n        if os.path.isfile(args.finetune):\n            print_info_message(\'Loading weights for finetuning from {}\'.format(args.finetune))\n            weight_dict = torch.load(args.finetune, map_location=torch.device(device=\'cpu\'))\n            model.load_state_dict(weight_dict)\n            print_info_message(\'Done\')\n        else:\n            print_warning_message(\'No file for finetuning. Please check.\')\n\n    if args.freeze_bn:\n        print_info_message(\'Freezing batch normalization layers\')\n        for m in model.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n                m.weight.requires_grad = False\n                m.bias.requires_grad = False\n\n    num_gpus = torch.cuda.device_count()\n    device = \'cuda\' if num_gpus > 0 else \'cpu\'\n\n    train_params = [{\'params\': model.get_basenet_params(), \'lr\': args.lr},\n                    {\'params\': model.get_segment_params(), \'lr\': args.lr * args.lr_mult}]\n\n    optimizer = optim.SGD(train_params, momentum=args.momentum, weight_decay=args.weight_decay)\n\n    num_params = model_parameters(model)\n    flops = compute_flops(model, input=torch.Tensor(1, 3, crop_size[0], crop_size[1]))\n    print_info_message(\'FLOPs for an input of size {}x{}: {:.2f} million\'.format(crop_size[0], crop_size[1], flops))\n    print_info_message(\'Network Parameters: {:.2f} million\'.format(num_params))\n\n    writer = SummaryWriter(log_dir=args.savedir, comment=\'Training and Validation logs\')\n    try:\n        writer.add_graph(model, input_to_model=torch.Tensor(1, 3, crop_size[0], crop_size[1]))\n    except:\n        print_log_message(""Not able to generate the graph. Likely because your model is not supported by ONNX"")\n\n    start_epoch = 0\n    best_miou = 0.0\n    if args.resume:\n        if os.path.isfile(args.resume):\n            print_info_message(""=> loading checkpoint \'{}\'"".format(args.resume))\n            checkpoint = torch.load(args.resume, map_location=torch.device(\'cpu\'))\n            start_epoch = checkpoint[\'epoch\']\n            best_miou = checkpoint[\'best_miou\']\n            model.load_state_dict(checkpoint[\'state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            print_info_message(""=> loaded checkpoint \'{}\' (epoch {})""\n                               .format(args.resume, checkpoint[\'epoch\']))\n        else:\n            print_warning_message(""=> no checkpoint found at \'{}\'"".format(args.resume))\n\n    #criterion = nn.CrossEntropyLoss(weight=class_wts, reduction=\'none\', ignore_index=args.ignore_idx)\n    criterion = SegmentationLoss(n_classes=seg_classes, loss_type=args.loss_type,\n                                 device=device, ignore_idx=args.ignore_idx,\n                                 class_wts=class_wts.to(device))\n\n    if num_gpus >= 1:\n        if num_gpus == 1:\n            # for a single GPU, we do not need DataParallel wrapper for Criteria.\n            # So, falling back to its internal wrapper\n            from torch.nn.parallel import DataParallel\n            model = DataParallel(model)\n            model = model.cuda()\n            criterion = criterion.cuda()\n        else:\n            from utilities.parallel_wrapper import DataParallelModel, DataParallelCriteria\n            model = DataParallelModel(model)\n            model = model.cuda()\n            criterion = DataParallelCriteria(criterion)\n            criterion = criterion.cuda()\n\n        if torch.backends.cudnn.is_available():\n            import torch.backends.cudnn as cudnn\n            cudnn.benchmark = True\n            cudnn.deterministic = True\n\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n                                               pin_memory=True, num_workers=args.workers)\n    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False,\n                                             pin_memory=True, num_workers=args.workers)\n\n    if args.scheduler == \'fixed\':\n        step_size = args.step_size\n        step_sizes = [step_size * i for i in range(1, int(math.ceil(args.epochs / step_size)))]\n        from utilities.lr_scheduler import FixedMultiStepLR\n        lr_scheduler = FixedMultiStepLR(base_lr=args.lr, steps=step_sizes, gamma=args.lr_decay)\n    elif args.scheduler == \'clr\':\n        step_size = args.step_size\n        step_sizes = [step_size * i for i in range(1, int(math.ceil(args.epochs / step_size)))]\n        from utilities.lr_scheduler import CyclicLR\n        lr_scheduler = CyclicLR(min_lr=args.lr, cycle_len=5, steps=step_sizes, gamma=args.lr_decay)\n    elif args.scheduler == \'poly\':\n        from utilities.lr_scheduler import PolyLR\n        lr_scheduler = PolyLR(base_lr=args.lr, max_epochs=args.epochs, power=args.power)\n    elif args.scheduler == \'hybrid\':\n        from utilities.lr_scheduler import HybirdLR\n        lr_scheduler = HybirdLR(base_lr=args.lr, max_epochs=args.epochs, clr_max=args.clr_max,\n                                cycle_len=args.cycle_len)\n    elif args.scheduler == \'linear\':\n        from utilities.lr_scheduler import LinearLR\n        lr_scheduler = LinearLR(base_lr=args.lr, max_epochs=args.epochs)\n    else:\n        print_error_message(\'{} scheduler Not supported\'.format(args.scheduler))\n        exit()\n\n    print_info_message(lr_scheduler)\n\n    with open(args.savedir + os.sep + \'arguments.json\', \'w\') as outfile:\n        import json\n        arg_dict = vars(args)\n        arg_dict[\'model_params\'] = \'{} \'.format(num_params)\n        arg_dict[\'flops\'] = \'{} \'.format(flops)\n        json.dump(arg_dict, outfile)\n\n    extra_info_ckpt = \'{}_{}_{}\'.format(args.model, args.s, crop_size[0])\n    for epoch in range(start_epoch, args.epochs):\n        lr_base = lr_scheduler.step(epoch)\n        # set the optimizer with the learning rate\n        # This can be done inside the MyLRScheduler\n        lr_seg = lr_base * args.lr_mult\n        optimizer.param_groups[0][\'lr\'] = lr_base\n        optimizer.param_groups[1][\'lr\'] = lr_seg\n\n        print_info_message(\n            \'Running epoch {} with learning rates: base_net {:.6f}, segment_net {:.6f}\'.format(epoch, lr_base, lr_seg))\n        miou_train, train_loss = train(model, train_loader, optimizer, criterion, seg_classes, epoch, device=device)\n        miou_val, val_loss = val(model, val_loader, criterion, seg_classes, device=device)\n\n        # remember best miou and save checkpoint\n        is_best = miou_val > best_miou\n        best_miou = max(miou_val, best_miou)\n\n        weights_dict = model.module.state_dict() if device == \'cuda\' else model.state_dict()\n        save_checkpoint({\n            \'epoch\': epoch + 1,\n            \'arch\': args.model,\n            \'state_dict\': weights_dict,\n            \'best_miou\': best_miou,\n            \'optimizer\': optimizer.state_dict(),\n        }, is_best, args.savedir, extra_info_ckpt)\n\n        writer.add_scalar(\'Segmentation/LR/base\', round(lr_base, 6), epoch)\n        writer.add_scalar(\'Segmentation/LR/seg\', round(lr_seg, 6), epoch)\n        writer.add_scalar(\'Segmentation/Loss/train\', train_loss, epoch)\n        writer.add_scalar(\'Segmentation/Loss/val\', val_loss, epoch)\n        writer.add_scalar(\'Segmentation/mIOU/train\', miou_train, epoch)\n        writer.add_scalar(\'Segmentation/mIOU/val\', miou_val, epoch)\n        writer.add_scalar(\'Segmentation/Complexity/Flops\', best_miou, math.ceil(flops))\n        writer.add_scalar(\'Segmentation/Complexity/Params\', best_miou, math.ceil(num_params))\n\n    writer.close()\n\n\nif __name__ == ""__main__"":\n    from commons.general_details import segmentation_models, segmentation_schedulers, segmentation_loss_fns, \\\n        segmentation_datasets\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--resume\', type=str, default=None, help=\'path to checkpoint to resume from\')\n    parser.add_argument(\'--workers\', type=int, default=4, help=\'number of data loading workers\')\n    parser.add_argument(\'--ignore-idx\', type=int, default=255, help=\'Index or label to be ignored during training\')\n\n    # model details\n    parser.add_argument(\'--freeze-bn\', action=\'store_true\', default=False, help=\'Freeze BN params or not\')\n\n    # dataset and result directories\n    parser.add_argument(\'--dataset\', type=str, default=\'pascal\', choices=segmentation_datasets, help=\'Datasets\')\n    parser.add_argument(\'--data-path\', type=str, default=\'\', help=\'dataset path\')\n    parser.add_argument(\'--coco-path\', type=str, default=\'\', help=\'MS COCO dataset path\')\n    parser.add_argument(\'--savedir\', type=str, default=\'./results_segmentation\', help=\'Location to save the results\')\n    ## only for cityscapes\n    parser.add_argument(\'--coarse\', action=\'store_true\', default=False, help=\'Want to use coarse annotations or not\')\n\n    # scheduler details\n    parser.add_argument(\'--scheduler\', default=\'hybrid\', choices=segmentation_schedulers,\n                        help=\'Learning rate scheduler (fixed, clr, poly)\')\n    parser.add_argument(\'--epochs\', type=int, default=100, help=\'num of training epochs\')\n    parser.add_argument(\'--step-size\', default=51, type=int, help=\'steps at which lr should be decreased\')\n    parser.add_argument(\'--lr\', default=9e-3, type=float, help=\'initial learning rate\')\n    parser.add_argument(\'--lr-mult\', default=10.0, type=float, help=\'initial learning rate\')\n    parser.add_argument(\'--lr-decay\', default=0.5, type=float, help=\'factor by which lr should be decreased\')\n    parser.add_argument(\'--momentum\', default=0.9, type=float, help=\'momentum\')\n    parser.add_argument(\'--weight-decay\', default=4e-5, type=float, help=\'weight decay (default: 4e-5)\')\n    # for Polynomial LR\n    parser.add_argument(\'--power\', default=0.9, type=float, help=\'power factor for Polynomial LR\')\n\n    # for hybrid LR\n    parser.add_argument(\'--clr-max\', default=61, type=int, help=\'Max number of epochs for cylic LR before \'\n                                                                \'changing last cycle to linear\')\n    parser.add_argument(\'--cycle-len\', default=5, type=int, help=\'Duration of cycle\')\n\n    # input details\n    parser.add_argument(\'--batch-size\', type=int, default=40, help=\'list of batch sizes\')\n    parser.add_argument(\'--crop-size\', type=int, nargs=\'+\', default=[256, 256],\n                        help=\'list of image crop sizes, with each item storing the crop size (should be a tuple).\')\n    parser.add_argument(\'--loss-type\', default=\'ce\', choices=segmentation_loss_fns, help=\'Loss function (ce or miou)\')\n\n    # model related params\n    parser.add_argument(\'--s\', type=float, default=2.0, help=\'Factor by which channels will be scaled\')\n    parser.add_argument(\'--model\', default=\'espnet\', choices=segmentation_models,\n                        help=\'Which model? basic= basic CNN model, res=resnet style)\')\n    parser.add_argument(\'--channels\', default=3, type=int, help=\'Input channels\')\n    parser.add_argument(\'--num-classes\', default=1000, type=int,\n                        help=\'ImageNet classes. Required for loading the base network\')\n    parser.add_argument(\'--finetune\', default=\'\', type=str, help=\'Finetune the segmentation model\')\n    parser.add_argument(\'--model-width\', default=224, type=int, help=\'Model width\')\n    parser.add_argument(\'--model-height\', default=224, type=int, help=\'Model height\')\n\n    args = parser.parse_args()\n\n    random.seed(1882)\n    torch.manual_seed(1882)\n\n    if args.dataset == \'pascal\':\n        args.scale = (0.5, 2.0)\n    elif args.dataset == \'city\':\n        if args.crop_size[0] == 512:\n            args.scale = (0.25, 0.5)\n        elif args.crop_size[0] == 1024:\n            args.scale = (0.35, 1.0)  # 0.75 # 0.5 -- 59+\n        elif args.crop_size[0] == 2048:\n            args.scale = (1.0, 2.0)\n        else:\n            print_error_message(\'Select image size from 512x256, 1024x512, 2048x1024\')\n        print_log_message(\'Using scale = ({}, {})\'.format(args.scale[0], args.scale[1]))\n    else:\n        print_error_message(\'{} dataset not yet supported\'.format(args.dataset))\n\n    if not args.finetune:\n        from model.weight_locations.classification import model_weight_map\n\n        weight_file_key = \'{}_{}\'.format(args.model, args.s)\n        assert weight_file_key in model_weight_map.keys(), \'{} does not exist\'.format(weight_file_key)\n        args.weights = model_weight_map[weight_file_key]\n    else:\n        args.weights = \'\'\n        assert os.path.isfile(args.finetune), \'{} weight file does not exist\'.format(args.finetune)\n\n    assert len(args.crop_size) == 2, \'crop-size argument must contain 2 values\'\n    assert args.data_path != \'\', \'Dataset path is an empty string. Please check.\'\n\n    args.crop_size = tuple(args.crop_size)\n    timestr = time.strftime(""%Y%m%d-%H%M%S"")\n    args.savedir = \'{}/model_{}_{}/s_{}_sch_{}_loss_{}_res_{}_sc_{}_{}/{}\'.format(args.savedir, args.model, args.dataset, args.s,\n                                                                         args.scheduler,\n                                                                         args.loss_type, args.crop_size[0], args.scale[0], args.scale[1], timestr)\n    main(args)\n'"
commons/__init__.py,0,b''
commons/general_details.py,0,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\n# classification related details\nclassification_datasets = [\'imagenet\', \'coco\']\nclassification_schedulers = [\'fixed\', \'clr\', \'hybrid\', \'linear\', \'poly\']\nclassification_models = [\'espnetv2\', \'dicenet\', \'shufflenetv2\']\nclassification_exp_choices = [\'main\', \'ablation\']\n\n# segmentation related details\nsegmentation_schedulers = [\'poly\', \'fixed\', \'clr\', \'linear\', \'hybrid\']\nsegmentation_datasets = [\'pascal\', \'city\']\nsegmentation_models = [\'espnetv2\', \'dicenet\']\nsegmentation_loss_fns = [\'ce\', \'bce\']\n\n\n# detection related details\n\ndetection_datasets = [\'coco\', \'pascal\']\ndetection_models = [\'espnetv2\', \'dicenet\']\ndetection_schedulers = [\'poly\', \'hybrid\', \'clr\', \'cosine\']\n'"
data_loader/__init__.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n'"
loss_fns/__init__.py,0,b''
loss_fns/multi_box_loss.py,3,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nfrom utilities import box_utils\n\n\nclass MultiBoxLoss(nn.Module):\n    def __init__(self, neg_pos_ratio):\n        super(MultiBoxLoss, self).__init__()\n        self.neg_pos_ratio = neg_pos_ratio\n\n\n    def forward(self, confidence, predicted_locations, labels, gt_locations):\n        """"""Compute classification loss and smooth l1 loss.\n\n        Args:\n            confidence (batch_size, num_priors, num_classes): class predictions.\n            predicted_locations (batch_size, num_priors, 4): predicted locations.\n            labels (batch_size, num_priors): real labels of all the priors.\n            gt_locations (batch_size, num_priors, 4): real boxes corresponding all the priors.\n        """"""\n        num_classes = confidence.size(2)\n        with torch.no_grad():\n            # derived from cross_entropy=sum(log(p))\n            loss = -F.log_softmax(confidence, dim=2)[:, :, 0]\n            mask = box_utils.hard_negative_mining(loss, labels, self.neg_pos_ratio)\n\n        confidence = confidence[mask, :]\n        classification_loss = F.cross_entropy(confidence.reshape(-1, num_classes), labels[mask], reduction=""sum"")\n        pos_mask = labels > 0\n        predicted_locations = predicted_locations[pos_mask, :].view(-1, 4)\n        gt_locations = gt_locations[pos_mask, :].view(-1, 4)\n        smooth_l1_loss = F.smooth_l1_loss(predicted_locations, gt_locations, reduction=""sum"")\n        num_pos = gt_locations.size(0)\n        return smooth_l1_loss / num_pos, classification_loss / num_pos\n\n'"
loss_fns/segmentation_loss.py,1,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nimport torch\nfrom torch import nn\n\n\nclass SegmentationLoss(nn.Module):\n    def __init__(self, n_classes=21, loss_type=\'ce\', device=\'cuda\', ignore_idx=255, class_wts=None):\n        super(SegmentationLoss, self).__init__()\n        self.loss_type = loss_type\n        self.n_classes = n_classes\n        self.device = device\n        self.ignore_idx = ignore_idx\n        self.smooth = 1e-6\n        self.class_wts = class_wts\n\n        if self.loss_type == \'bce\':\n            self.loss_fn = nn.BCEWithLogitsLoss(weight=self.class_wts)\n        else:\n            self.loss_fn = nn.CrossEntropyLoss(ignore_index=self.ignore_idx, weight=self.class_wts)\n\n    def convert_to_one_hot(self, x):\n        n, h, w = x.size()\n        # remove the 255 index\n        x[x == self.ignore_idx] = self.n_classes\n        x = x.unsqueeze(1)\n\n        # convert to one hot vector\n        x_one_hot = torch.zeros(n, self.n_classes + 1, h, w).to(device=self.device)\n        x_one_hot = x_one_hot.scatter_(1, x, 1)\n\n        return x_one_hot[:, :self.n_classes, :, :].contiguous()\n\n    def forward(self, inputs, target):\n        if isinstance(inputs, tuple):\n            tuple_len = len(inputs)\n            assert tuple_len == 2\n            loss = 0\n            for i in range(tuple_len):\n                if target.dim() == 3 and self.loss_type == \'bce\':\n                    target = self.convert_to_one_hot(target)\n                loss_ = self.loss_fn(inputs[i], target)\n                loss += loss_\n        else:\n            if target.dim() == 3 and self.loss_type == \'bce\':\n                target = self.convert_to_one_hot(target)\n            return self.loss_fn(inputs, target)\n        return loss\n'"
model/__init__.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================'"
nn_layers/__init__.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================'"
nn_layers/cnn_utils.py,1,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nimport torch\nfrom torch import nn\n\n# helper function for activations\ndef activation_fn(features, name=\'prelu\', inplace=True):\n    \'\'\'\n    :param features: # of features (only for PReLU)\n    :param name: activation name (prelu, relu, selu)\n    :param inplace: Inplace operation or not\n    :return:\n    \'\'\'\n    if name == \'relu\':\n        return nn.ReLU(inplace=True)\n    elif name == \'selu\':\n        return nn.SELU(inplace=inplace)\n    elif name == \'prelu\':\n        return nn.PReLU(features)\n    else:\n        NotImplementedError(\'Not implemented yet\')\n        exit()\n\nclass CBR(nn.Module):\n    \'\'\'\n    This class defines the convolution layer with batch normalization and activation function\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, dilation=1, groups=1, act_name=\'prelu\'):\n        \'\'\'\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: stride rate for down-sampling. Default is 1\n        :param groups: # of groups for group-wise convolution\n        :param act_name: Name of the activation function\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)*dilation\n        self.cbr = nn.Sequential(\n            nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False, groups=groups, dilation=dilation),\n            nn.BatchNorm2d(nOut),\n            activation_fn(features=nOut, name=act_name)\n        )\n\n    def forward(self, x):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        return self.cbr(x)\n\nclass CB(nn.Module):\n    \'\'\'\n    This class implements convolution layer followed by batch normalization\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, dilation=1, groups=1):\n        \'\'\'\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: stride rate for down-sampling. Default is 1\n        :param groups: # of groups for group-wise convolution\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)*dilation\n        self.cb = nn.Sequential(\n            nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False, groups=groups, dilation=1),\n            nn.BatchNorm2d(nOut),\n        )\n\n    def forward(self, x):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        return self.cb(x)\n\n\nclass BR(nn.Module):\n    \'\'\'\n    This class implements batch normalization and  activation function\n    \'\'\'\n    def __init__(self, nOut, act_name=\'prelu\'):\n        \'\'\'\n        :param nIn: number of input channels\n        :param act_name: Name of the activation function\n        \'\'\'\n        super().__init__()\n        self.br = nn.Sequential(\n            nn.BatchNorm2d(nOut),\n            activation_fn(nOut, name=act_name)\n        )\n\n    def forward(self, x):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        return self.br(x)\n\n\nclass Shuffle(nn.Module):\n    \'\'\'\n    This class implements Channel Shuffling\n    \'\'\'\n    def __init__(self, groups):\n        \'\'\'\n        :param groups: # of groups for shuffling\n        \'\'\'\n        super().__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        batchsize, num_channels, height, width = x.data.size()\n        channels_per_group = num_channels // self.groups\n        x = x.view(batchsize, self.groups, channels_per_group, height, width)\n        x = torch.transpose(x, 1, 2).contiguous()\n        x = x.view(batchsize, -1, height, width)\n        return x\n\n\nclass DWConv(nn.Module):\n    def __init__(self, nin):\n        super(DWConv, self).__init__()\n        self.dw_layer = nn.Sequential(\n            nn.Conv2d(nin, nin, kernel_size=3, stride=1, padding=1, bias=False, groups=nin),\n            nn.BatchNorm2d(nin),\n            nn.PReLU(nin)\n        )\n\n    def forward(self, x):\n        return self.dw_layer(x)'"
nn_layers/dice.py,5,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom nn_layers.cnn_utils import activation_fn, CBR, Shuffle, BR\nimport math\n\n\nclass DICE(nn.Module):\n    \'\'\'\n    This class implements the volume-wise seperable convolutions\n    \'\'\'\n    def __init__(self, channel_in, channel_out, height, width, kernel_size=3, dilation=[1, 1, 1], shuffle=True):\n        \'\'\'\n        :param channel_in: # of input channels\n        :param channel_out: # of output channels\n        :param height: Height of the input volume\n        :param width: Width of the input volume\n        :param kernel_size: Kernel size. We use the same kernel size of 3 for each dimension. Larger kernel size would increase the FLOPs and Parameters\n        :param dilation: It\'s a list with 3 elements, each element corresponding to a dilation rate for each dimension.\n        :param shuffle: Shuffle the feature maps in the volume-wise separable convolutions\n        \'\'\'\n        super().__init__()\n        assert len(dilation) == 3\n        padding_1 = int((kernel_size - 1) / 2) *dilation[0]\n        padding_2 = int((kernel_size - 1) / 2) *dilation[1]\n        padding_3 = int((kernel_size - 1) / 2) *dilation[2]\n        self.conv_channel = nn.Conv2d(channel_in, channel_in, kernel_size=kernel_size, stride=1, groups=channel_in,\n                                      padding=padding_1, bias=False, dilation=dilation[0])\n        self.conv_width = nn.Conv2d(width, width, kernel_size=kernel_size, stride=1, groups=width,\n                               padding=padding_2, bias=False, dilation=dilation[1])\n        self.conv_height = nn.Conv2d(height, height, kernel_size=kernel_size, stride=1, groups=height,\n                               padding=padding_3, bias=False, dilation=dilation[2])\n\n        self.br_act = BR(3*channel_in)\n        self.weight_avg_layer = CBR(3*channel_in, channel_in, kSize=1, stride=1, groups=channel_in)\n\n        # project from channel_in to Channel_out\n        groups_proj = math.gcd(channel_in, channel_out)\n        self.proj_layer = CBR(channel_in, channel_out, kSize=3, stride=1, groups=groups_proj)\n        self.linear_comb_layer = nn.Sequential(\n            nn.AdaptiveAvgPool2d(output_size=1),\n            nn.Conv2d(channel_in, channel_in // 4, kernel_size=1, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(channel_in //4, channel_out, kernel_size=1, bias=False),\n            nn.Sigmoid()\n        )\n\n        self.vol_shuffle = Shuffle(3)\n\n        self.width = width\n        self.height = height\n        self.channel_in = channel_in\n        self.channel_out = channel_out\n        self.shuffle = shuffle\n        self.ksize=kernel_size\n        self.dilation = dilation\n\n    def forward(self, x):\n        \'\'\'\n        :param x: input of dimension C x H x W\n        :return: output of dimension C1 x H x W\n        \'\'\'\n        bsz, channels, height, width = x.size()\n        # process across channel. Input: C x H x W, Output: C x H x W\n        out_ch_wise = self.conv_channel(x)\n\n        # process across height. Input: H x C x W, Output: C x H x W\n        x_h_wise = x.clone()\n        if height != self.height:\n            if height < self.height:\n                x_h_wise = F.interpolate(x_h_wise, mode=\'bilinear\', size=(self.height, width), align_corners=True)\n            else:\n                x_h_wise = F.adaptive_avg_pool2d(x_h_wise, output_size=(self.height, width))\n\n        x_h_wise = x_h_wise.transpose(1, 2).contiguous()\n        out_h_wise = self.conv_height(x_h_wise).transpose(1, 2).contiguous()\n\n        h_wise_height = out_h_wise.size(2)\n        if height != h_wise_height:\n            if h_wise_height < height:\n                out_h_wise = F.interpolate(out_h_wise, mode=\'bilinear\', size=(height, width), align_corners=True)\n            else:\n                out_h_wise = F.adaptive_avg_pool2d(out_h_wise, output_size=(height, width))\n\n        # process across width: Input: W x H x C, Output: C x H x W\n        x_w_wise = x.clone()\n        if width != self.width:\n            if width < self.width:\n                x_w_wise = F.interpolate(x_w_wise, mode=\'bilinear\', size=(height, self.width), align_corners=True)\n            else:\n                x_w_wise = F.adaptive_avg_pool2d(x_w_wise, output_size=(height, self.width))\n\n        x_w_wise = x_w_wise.transpose(1, 3).contiguous()\n        out_w_wise = self.conv_width(x_w_wise).transpose(1, 3).contiguous()\n        w_wise_width = out_w_wise.size(3)\n        if width != w_wise_width:\n            if w_wise_width < width:\n                out_w_wise = F.interpolate(out_w_wise, mode=\'bilinear\', size=(height, width), align_corners=True)\n            else:\n                out_w_wise = F.adaptive_avg_pool2d(out_w_wise, output_size=(height, width))\n\n        # Merge. Output will be 3C x H X W\n        outputs = torch.cat((out_ch_wise, out_h_wise, out_w_wise), 1)\n        outputs = self.br_act(outputs)\n        if self.shuffle:\n            outputs = self.vol_shuffle(outputs)\n        outputs = self.weight_avg_layer(outputs)\n        linear_wts = self.linear_comb_layer(outputs)\n        proj_out = self.proj_layer(outputs)\n        return proj_out * linear_wts\n\n    def __repr__(self):\n        s = \'{name}(in_channels={channel_in}, out_channels={channel_out}, kernel_size={ksize}, vol_shuffle={shuffle}, \' \\\n            \'width={width}, height={height}, dilation={dilation})\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass StridedDICE(nn.Module):\n    \'\'\'\n    This class implements the strided volume-wise seperable convolutions\n    \'\'\'\n    def __init__(self, channel_in, height, width, kernel_size=3, dilation=[1,1,1], shuffle=True):\n        \'\'\'\n        :param channel_in: # of input channels\n        :param channel_out: # of output channels\n        :param height: Height of the input volume\n        :param width: Width of the input volume\n        :param kernel_size: Kernel size. We use the same kernel size of 3 for each dimension. Larger kernel size would increase the FLOPs and Parameters\n        :param dilation: It\'s a list with 3 elements, each element corresponding to a dilation rate for each dimension.\n        :param shuffle: Shuffle the feature maps in the volume-wise separable convolutions\n        \'\'\'\n        super().__init__()\n        assert len(dilation) == 3\n\n        self.left_layer = nn.Sequential(CBR(channel_in, channel_in, 3, stride=2, groups=channel_in),\n                                        CBR(channel_in, channel_in, 1, 1)\n                                        )\n        self.right_layer =  nn.Sequential(\n            nn.AvgPool2d(kernel_size=3, padding=1, stride=2),\n            DICE(channel_in, channel_in, height, width, kernel_size=kernel_size, dilation=dilation,\n                 shuffle=shuffle),\n            CBR(channel_in, channel_in, 1, 1)\n        )\n        self.shuffle = Shuffle(groups=2)\n\n        self.width = width\n        self.height = height\n        self.channel_in = channel_in\n        self.channel_out = 2*channel_in\n        self.ksize = kernel_size\n\n    def forward(self, x):\n        x_left = self.left_layer(x)\n        x_right = self.right_layer(x)\n        concat = torch.cat([x_left, x_right], 1)\n        return self.shuffle(concat)\n\n    def __repr__(self):\n        s = \'{name}(in_channels={channel_in}, out_channels={channel_out}, kernel_size={ksize}, \' \\\n            \'width={width}, height={height})\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\nif __name__ == \'__main__\':\n    import numpy as np\n    channel_in = 3\n    channel_out = 30\n    width = 112\n    height = 112\n    bsz = 2\n    input = torch.Tensor(bsz, channel_in, height, width).fill_(1)\n    model = DICE(channel_in, channel_out, width, height, shuffle=True)\n    model.eval()\n\n    input = torch.Tensor(bsz, channel_in, 56, 56).fill_(1)\n    out = model(input)\n\n    n_params = sum([np.prod(p.size()) for p in model.parameters()])\n    print(\'Params: {}\'.format(n_params))\n'"
nn_layers/dwise_conv.py,2,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nimport torch\nfrom torch import nn\nfrom nn_layers.cnn_utils import CBR, CB\n\nclass Shuffle(nn.Module):\n    \'\'\'\n    This class implements Channel Shuffling\n    \'\'\'\n    def __init__(self, groups):\n        \'\'\'\n        :param groups: # of groups for shuffling\n        \'\'\'\n        super().__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        batchsize, num_channels, height, width = x.data.size()\n        channels_per_group = num_channels // self.groups\n        x = x.view(batchsize, self.groups, channels_per_group, height, width)\n        x = torch.transpose(x, 1, 2).contiguous()\n        x = x.view(batchsize, -1, height, width)\n        return x\n\n\nclass DWSepConv(nn.Module):\n    \'\'\'\n    This class implements the volume-wise seperable convolutions\n    \'\'\'\n    def __init__(self, channel_in, channel_out,  kernel_size=3, stride=1, dilation=1):\n        super().__init__()\n        self.dwise_layer = nn.Sequential(\n                                CB(channel_in, channel_in, kernel_size, stride=stride, dilation=dilation, groups=channel_in),\n                                CBR(channel_in, channel_out, 1, 1, groups=1)\n                            )\n        self.channel_in = channel_in\n        self.channel_out = channel_out\n        self.ksize=kernel_size\n        self.dilation = dilation\n\n    def forward(self, x):\n        return self.dwise_layer(x)\n\n    def __repr__(self):\n        s = \'{name}(in_channels={channel_in}, out_channels={channel_out}, kernel_size={ksize}, dilation={dilation})\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass StridedDWise(nn.Module):\n    def __init__(self, channel_in, kernel_size=3, dilation=1):\n        super().__init__()\n        self.pool_layer = CBR(channel_in, channel_in, 3, stride=2, groups=channel_in)\n        self.dwise_layer = DWSepConv(channel_in, channel_in, kernel_size=kernel_size, dilation=dilation)\n        self.channel_in = channel_in\n        self.channel_out = 2*channel_in\n        self.ksize = kernel_size\n\n    def forward(self, x):\n        x = self.pool_layer(x)\n        return torch.cat([x, self.dwise_layer(x)], 1)\n\n    def __repr__(self):\n        s = \'{name}(in_channels={channel_in}, out_channels={channel_out}, kernel_size={ksize})\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n'"
nn_layers/eesp.py,4,"b'from torch.nn import init\nimport torch.nn.functional as F\nfrom nn_layers.espnet_utils import *\nimport math\nimport torch\nfrom model.classification import espnetv2_config as config\n\n#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nconfig_inp_reinf = config.config_inp_reinf\n\nclass EESP(nn.Module):\n    \'\'\'\n    This class defines the EESP block, which is based on the following principle\n        REDUCE ---> SPLIT ---> TRANSFORM --> MERGE\n    \'\'\'\n\n    def __init__(self, nIn, nOut, stride=1, k=4, r_lim=7, down_method=\'esp\'): #down_method --> [\'avg\' or \'esp\']\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param stride: factor by which we should skip (useful for down-sampling). If 2, then down-samples the feature map by 2\n        :param k: # of parallel branches\n        :param r_lim: A maximum value of receptive field allowed for EESP block\n        :param down_method: Downsample or not (equivalent to say stride is 2 or not)\n        \'\'\'\n        super().__init__()\n        self.stride = stride\n        n = int(nOut / k)\n        n1 = nOut - (k - 1) * n\n        assert down_method in [\'avg\', \'esp\'], \'One of these is suppported (avg or esp)\'\n        assert n == n1, ""n(={}) and n1(={}) should be equal for Depth-wise Convolution "".format(n, n1)\n        self.proj_1x1 = CBR(nIn, n, 1, stride=1, groups=k)\n\n        # (For convenience) Mapping between dilation rate and receptive field for a 3x3 kernel\n        map_receptive_ksize = {3: 1, 5: 2, 7: 3, 9: 4, 11: 5, 13: 6, 15: 7, 17: 8}\n        self.k_sizes = list()\n        for i in range(k):\n            ksize = int(3 + 2 * i)\n            # After reaching the receptive field limit, fall back to the base kernel size of 3 with a dilation rate of 1\n            ksize = ksize if ksize <= r_lim else 3\n            self.k_sizes.append(ksize)\n        # sort (in ascending order) these kernel sizes based on their receptive field\n        # This enables us to ignore the kernels (3x3 in our case) with the same effective receptive field in hierarchical\n        # feature fusion because kernels with 3x3 receptive fields does not have gridding artifact.\n        self.k_sizes.sort()\n        self.spp_dw = nn.ModuleList()\n        for i in range(k):\n            d_rate = map_receptive_ksize[self.k_sizes[i]]\n            self.spp_dw.append(CDilated(n, n, kSize=3, stride=stride, groups=n, d=d_rate))\n        # Performing a group convolution with K groups is the same as performing K point-wise convolutions\n        self.conv_1x1_exp = CB(nOut, nOut, 1, 1, groups=k)\n        self.br_after_cat = BR(nOut)\n        self.module_act = nn.PReLU(nOut)\n        self.downAvg = True if down_method == \'avg\' else False\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n\n        # Reduce --> project high-dimensional feature maps to low-dimensional space\n        output1 = self.proj_1x1(input)\n        output = [self.spp_dw[0](output1)]\n        # compute the output for each branch and hierarchically fuse them\n        # i.e. Split --> Transform --> HFF\n        for k in range(1, len(self.spp_dw)):\n            out_k = self.spp_dw[k](output1)\n            # HFF\n            out_k = out_k + output[k - 1]\n            output.append(out_k)\n        # Merge\n        expanded = self.conv_1x1_exp( # learn linear combinations using group point-wise convolutions\n            self.br_after_cat( # apply batch normalization followed by activation function (PRelu in this case)\n                torch.cat(output, 1) # concatenate the output of different branches\n            )\n        )\n        del output\n        # if down-sampling, then return the concatenated vector\n        # because Downsampling function will combine it with avg. pooled feature map and then threshold it\n        if self.stride == 2 and self.downAvg:\n            return expanded\n\n        # if dimensions of input and concatenated vector are the same, add them (RESIDUAL LINK)\n        if expanded.size() == input.size():\n            expanded = expanded + input\n\n        # Threshold the feature map using activation function (PReLU in this case)\n        return self.module_act(expanded)\n\n\nclass DownSampler(nn.Module):\n    \'\'\'\n    Down-sampling fucntion that has three parallel branches: (1) avg pooling,\n    (2) EESP block with stride of 2 and (3) efficient long-range connection with the input.\n    The output feature maps of branches from (1) and (2) are concatenated and then additively fused with (3) to produce\n    the final output.\n    \'\'\'\n\n    def __init__(self, nin, nout, k=4, r_lim=9, reinf=True):\n        \'\'\'\n            :param nin: number of input channels\n            :param nout: number of output channels\n            :param k: # of parallel branches\n            :param r_lim: A maximum value of receptive field allowed for EESP block\n            :param reinf: Use long range shortcut connection with the input or not.\n        \'\'\'\n        super().__init__()\n        nout_new = nout - nin\n        self.eesp = EESP(nin, nout_new, stride=2, k=k, r_lim=r_lim, down_method=\'avg\')\n        self.avg = nn.AvgPool2d(kernel_size=3, padding=1, stride=2)\n        if reinf:\n            self.inp_reinf = nn.Sequential(\n                CBR(config_inp_reinf, config_inp_reinf, 3, 1),\n                CB(config_inp_reinf, nout, 1, 1)\n            )\n        self.act =  nn.PReLU(nout)\n\n    def forward(self, input, input2=None):\n        \'\'\'\n        :param input: input feature map\n        :return: feature map down-sampled by a factor of 2\n        \'\'\'\n        avg_out = self.avg(input)\n        eesp_out = self.eesp(input)\n        output = torch.cat([avg_out, eesp_out], 1)\n\n        if input2 is not None:\n            #assuming the input is a square image\n            # Shortcut connection with the input image\n            w1 = avg_out.size(2)\n            while True:\n                input2 = F.avg_pool2d(input2, kernel_size=3, padding=1, stride=2)\n                w2 = input2.size(2)\n                if w2 == w1:\n                    break\n            output = output + self.inp_reinf(input2)\n\n        return self.act(output)'"
nn_layers/efficient_dwise_conv.py,4,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom nn_layers.cnn_utils import activation_fn, CBR, Shuffle, BR\nimport math\n\n\nclass EffDWSepConv(nn.Module):\n    \'\'\'\n    This class implements the volume-wise seperable convolutions\n    \'\'\'\n    def __init__(self, channel_in, channel_out, kernel_size=3):\n        super().__init__()\n        self.conv_channel = CBR(channel_in, channel_in, kSize=kernel_size, stride=1, groups=channel_in)\n\n        # project from channel_in to Channel_out\n        groups_proj = math.gcd(channel_in, channel_out)\n        self.proj_layer = CBR(channel_in, channel_out, kSize=3, stride=1, groups=groups_proj)\n\n        self.linear_comb_layer = nn.Sequential(\n            nn.AdaptiveAvgPool2d(output_size=1),\n            nn.Conv2d(channel_in, channel_out, kernel_size=1, bias=False),\n            nn.Sigmoid()\n        )\n\n        self.channel_in = channel_in\n        self.channel_out = channel_out\n        self.ksize=kernel_size\n\n    def forward(self, x):\n        \'\'\'\n        :param x: input of dimension C x H x W\n        :return: output of dimension C1 x H x W\n        \'\'\'\n        bsz, channels, height, width = x.size()\n        x = self.conv_channel(x)\n        proj_out  =self.proj_layer(x)\n        linear_comb_out = self.linear_comb_layer(x)\n        return proj_out * linear_comb_out\n\n    def __repr__(self):\n        s = \'{name}(in_channels={channel_in}, out_channels={channel_out}, kernel_size={ksize})\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass StridedEffDWise(nn.Module):\n    \'\'\'\n    This class implements the strided volume-wise seperable convolutions\n    \'\'\'\n    def __init__(self, channel_in, kernel_size=3):\n        \'\'\'\n        :param channel_in: # of input channels\n        :param channel_out: # of output channels\n        :param height: Height of the input volume\n        :param width: Width of the input volume\n        :param kernel_size: Kernel size. We use the same kernel size of 3 for each dimension. Larger kernel size would increase the FLOPs and Parameters\n        :param dilation: It\'s a list with 3 elements, each element corresponding to a dilation rate for each dimension.\n        :param shuffle: Shuffle the feature maps in the volume-wise separable convolutions\n        :param weight_avg: Waighted average for fusing the feature maps in volume-wise separable convolutions\n        :param res_conn: Residual connection in the volume-wise separabel convolutions\n        :param proj: Want to project the feature maps from channel_in to channel_out or not\n        \'\'\'\n        super().__init__()\n\n        self.pool_layer = CBR(channel_in, channel_in, 3, stride=2, groups=channel_in)\n        self.dw_layer =  EffDWSepConv(channel_in, channel_in, kernel_size=kernel_size)\n        self.channel_in = channel_in\n        self.channel_out = 2*channel_in\n        self.ksize = kernel_size\n\n    def forward(self, x):\n        x = self.pool_layer(x)\n        return torch.cat([x, self.dw_layer(x)], 1)\n\n    def __repr__(self):\n        s = \'{name}(in_channels={channel_in}, out_channels={channel_out}, kernel_size={ksize}, \' \\\n            \'width={width}, height={height})\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\nif __name__ == \'__main__\':\n    import numpy as np\n    channel_in = 3\n    channel_out = 30\n    width = 112\n    height = 112\n    bsz = 2\n    input = torch.Tensor(bsz, channel_in, height, width)._fill_(1)\n    model = EffDWSepConv(channel_in, channel_out)\n    model.eval()\n\n    input = torch.Tensor(bsz, channel_in, 56, 56)._fill_(1)\n    out = model(input)\n\n    n_params = sum([np.prod(p.size()) for p in model.parameters()])\n    print(\'Params: {}\'.format(n_params))\n'"
nn_layers/efficient_pt.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nfrom torch import nn\nimport math\nfrom nn_layers.cnn_utils import CBR\n\nclass EfficientPWConv(nn.Module):\n    def __init__(self, nin, nout):\n        super(EfficientPWConv, self).__init__()\n        self.wt_layer = nn.Sequential(\n                        nn.AdaptiveAvgPool2d(output_size=1),\n                        nn.Conv2d(nin, nout, kernel_size=1, stride=1, padding=0, groups=1, bias=False),\n                        nn.Sigmoid()\n                    )\n\n        self.groups = math.gcd(nin, nout)\n        self.expansion_layer = CBR(nin, nout, kSize=3, stride=1, groups=self.groups)\n\n        self.out_size = nout\n        self.in_size = nin\n\n    def forward(self, x):\n        wts = self.wt_layer(x)\n        x = self.expansion_layer(x)\n        x = x * wts\n        return x\n\n    def __repr__(self):\n        s = \'{name}(in_channels={in_size}, out_channels={out_size})\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n'"
nn_layers/efficient_pyramid_pool.py,2,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nimport torch\nfrom torch import nn\nimport math\nfrom torch.nn import functional as F\nfrom nn_layers.cnn_utils import CBR, BR, Shuffle\n\nclass EfficientPyrPool(nn.Module):\n    """"""Efficient Pyramid Pooling Module""""""\n\n    def __init__(self, in_planes, proj_planes, out_planes, scales=[2.0, 1.5, 1.0, 0.5, 0.1], last_layer_br=True):\n        super(EfficientPyrPool, self).__init__()\n        self.stages = nn.ModuleList()\n        scales.sort(reverse=True)\n\n        self.projection_layer = CBR(in_planes, proj_planes, 1, 1)\n        for _ in enumerate(scales):\n            self.stages.append(nn.Conv2d(proj_planes, proj_planes, kernel_size=3, stride=1, padding=1, bias=False, groups=proj_planes))\n\n        self.merge_layer = nn.Sequential(\n            # perform one big batch normalization instead of p small ones\n            BR(proj_planes * len(scales)),\n            Shuffle(groups=len(scales)),\n            CBR(proj_planes * len(scales), proj_planes, 3, 1, groups=proj_planes),\n            nn.Conv2d(proj_planes, out_planes, kernel_size=1, stride=1, bias=not last_layer_br),\n        )\n        if last_layer_br:\n            self.br = BR(out_planes)\n        self.last_layer_br = last_layer_br\n        self.scales = scales\n\n    def forward(self, x):\n        hs = []\n        x = self.projection_layer(x)\n        height, width = x.size()[2:]\n        for i, stage in enumerate(self.stages):\n            h_s = int(math.ceil(height * self.scales[i]))\n            w_s = int(math.ceil(width * self.scales[i]))\n            h_s = h_s if h_s > 5 else 5\n            w_s = w_s if w_s > 5 else 5\n            if self.scales[i] < 1.0:\n                h = F.adaptive_avg_pool2d(x, output_size=(h_s, w_s))\n                h = stage(h)\n                h = F.interpolate(h, (height, width), mode=\'bilinear\', align_corners=True)\n            elif self.scales[i] > 1.0:\n                h = F.interpolate(x, (h_s, w_s), mode=\'bilinear\', align_corners=True)\n                h = stage(h)\n                h = F.adaptive_avg_pool2d(h, output_size=(height, width))\n            else:\n                h = stage(x)\n            hs.append(h)\n\n        out = torch.cat(hs, dim=1)\n        out = self.merge_layer(out)\n        if self.last_layer_br:\n            return self.br(out)\n        return out'"
nn_layers/espnet_utils.py,1,"b'import torch.nn as nn\n\n#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nclass CBR(nn.Module):\n    \'\'\'\n    This class defines the convolution layer with batch normalization and PReLU activation\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, groups=1):\n        \'\'\'\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: stride rate for down-sampling. Default is 1\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False, groups=groups)\n        self.bn = nn.BatchNorm2d(nOut)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        # output = self.conv1(output)\n        output = self.bn(output)\n        output = self.act(output)\n        return output\n\n\nclass BR(nn.Module):\n    \'\'\'\n        This class groups the batch normalization and PReLU activation\n    \'\'\'\n\n    def __init__(self, nOut):\n        \'\'\'\n        :param nOut: output feature maps\n        \'\'\'\n        super().__init__()\n        self.bn = nn.BatchNorm2d(nOut)\n        self.act = nn.PReLU(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: normalized and thresholded feature map\n        \'\'\'\n        output = self.bn(input)\n        output = self.act(output)\n        return output\n\n\nclass CB(nn.Module):\n    \'\'\'\n       This class groups the convolution and batch normalization\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, groups=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optinal stide for down-sampling\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False,\n                              groups=groups)\n        self.bn = nn.BatchNorm2d(nOut)\n\n    def forward(self, input):\n        \'\'\'\n\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        output = self.bn(output)\n        return output\n\n\nclass C(nn.Module):\n    \'\'\'\n    This class is for a convolutional layer.\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, groups=1):\n        \'\'\'\n\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2)\n        self.conv = nn.Conv2d(nIn, nOut, kSize, stride=stride, padding=padding, bias=False,\n                              groups=groups)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        return output\n\n\nclass CDilated(nn.Module):\n    \'\'\'\n    This class defines the dilated convolution.\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1, groups=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        :param d: optional dilation rate\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2) * d\n        self.conv = nn.Conv2d(nIn, nOut,kSize, stride=stride, padding=padding, bias=False,\n                              dilation=d, groups=groups)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        output = self.conv(input)\n        return output\n\nclass CDilatedB(nn.Module):\n    \'\'\'\n    This class defines the dilated convolution with batch normalization.\n    \'\'\'\n\n    def __init__(self, nIn, nOut, kSize, stride=1, d=1, groups=1):\n        \'\'\'\n        :param nIn: number of input channels\n        :param nOut: number of output channels\n        :param kSize: kernel size\n        :param stride: optional stride rate for down-sampling\n        :param d: optional dilation rate\n        \'\'\'\n        super().__init__()\n        padding = int((kSize - 1) / 2) * d\n        self.conv = nn.Conv2d(nIn, nOut,kSize, stride=stride, padding=padding, bias=False,\n                              dilation=d, groups=groups)\n        self.bn = nn.BatchNorm2d(nOut)\n\n    def forward(self, input):\n        \'\'\'\n        :param input: input feature map\n        :return: transformed feature map\n        \'\'\'\n        return self.bn(self.conv(input))\n'"
transforms/__init__.py,0,b''
utilities/__init__.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__license__ = ""MIT""\n__maintainer__ = ""Sachin Mehta""\n#============================================'"
utilities/box_utils.py,12,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nimport torch\nimport math\n\ndef convert_locations_to_boxes(locations, priors, center_variance,\n                               size_variance):\n    """"""Convert regressional location results of SSD into boxes in the form of (center_x, center_y, h, w).\n\n    The conversion:\n        $$predicted\\_center * center_variance = \\frac {real\\_center - prior\\_center} {prior\\_hw}$$\n        $$exp(predicted\\_hw * size_variance) = \\frac {real\\_hw} {prior\\_hw}$$\n    We do it in the inverse direction here.\n    Args:\n        locations (batch_size, num_priors, 4): the regression output of SSD. It will contain the outputs as well.\n        priors (num_priors, 4) or (batch_size/1, num_priors, 4): prior boxes.\n        center_variance: a float used to change the scale of center.\n        size_variance: a float used to change of scale of size.\n    Returns:\n        boxes:  priors: [[center_x, center_y, h, w]]. All the values\n            are relative to the image size.\n    """"""\n    # priors can have one dimension less.\n    if priors.dim() + 1 == locations.dim():\n        priors = priors.unsqueeze(0)\n    return torch.cat((\n        locations[..., :2] * center_variance * priors[..., 2:] + priors[..., :2],\n        torch.exp(locations[..., 2:] * size_variance) * priors[..., 2:]\n    ), dim=locations.dim() - 1)\n\n\ndef convert_boxes_to_locations(center_form_boxes, center_form_priors, center_variance, size_variance):\n    # priors can have one dimension less\n    if center_form_priors.dim() + 1 == center_form_boxes.dim():\n        center_form_priors = center_form_priors.unsqueeze(0)\n    return torch.cat((\n        (center_form_boxes[..., :2] - center_form_priors[..., :2]) / center_form_priors[..., 2:] / center_variance,\n        torch.log(center_form_boxes[..., 2:] / center_form_priors[..., 2:]) / size_variance\n    ), dim=center_form_boxes.dim() - 1)\n\n\ndef area_of(left_top, right_bottom) -> torch.Tensor:\n    """"""Compute the areas of rectangles given two corners.\n\n    Args:\n        left_top (N, 2): left top corner.\n        right_bottom (N, 2): right bottom corner.\n\n    Returns:\n        area (N): return the area.\n    """"""\n    hw = torch.clamp(right_bottom - left_top, min=0.0)\n    return hw[..., 0] * hw[..., 1]\n\n\ndef iou_of(boxes0, boxes1, eps=1e-5):\n    """"""Return intersection-over-union (Jaccard index) of boxes.\n\n    Args:\n        boxes0 (N, 4): ground truth boxes.\n        boxes1 (N or 1, 4): predicted boxes.\n        eps: a small number to avoid 0 as denominator.\n    Returns:\n        iou (N): IoU values.\n    """"""\n    overlap_left_top = torch.max(boxes0[..., :2], boxes1[..., :2])\n    overlap_right_bottom = torch.min(boxes0[..., 2:], boxes1[..., 2:])\n\n    overlap_area = area_of(overlap_left_top, overlap_right_bottom)\n    area0 = area_of(boxes0[..., :2], boxes0[..., 2:])\n    area1 = area_of(boxes1[..., :2], boxes1[..., 2:])\n    return overlap_area / (area0 + area1 - overlap_area + eps)\n\n\ndef assign_priors(gt_boxes, gt_labels, corner_form_priors,\n                  iou_threshold):\n    """"""Assign ground truth boxes and targets to priors.\n\n    Args:\n        gt_boxes (num_targets, 4): ground truth boxes.\n        gt_labels (num_targets): labels of targets.\n        priors (num_priors, 4): corner form priors\n    Returns:\n        boxes (num_priors, 4): real values for priors.\n        labels (num_priros): labels for priors.\n    """"""\n    # size: num_priors x num_targets\n    # in coco there are images where there are no labels, so handling here such cases\n    if gt_labels.nelement() == 0:\n        gt_boxes = torch.zeros((1, 4), dtype=torch.float32, device=corner_form_priors.device)\n        gt_labels = torch.zeros(1, dtype=torch.int64, device=corner_form_priors.device)\n\n    ious = iou_of(gt_boxes.unsqueeze(0), corner_form_priors.unsqueeze(1))\n    # size: num_priors\n    best_target_per_prior, best_target_per_prior_index = ious.max(1)\n    # size: num_targets\n    best_prior_per_target, best_prior_per_target_index = ious.max(0)\n\n    for target_index, prior_index in enumerate(best_prior_per_target_index):\n        best_target_per_prior_index[prior_index] = target_index\n    # 2.0 is used to make sure every target has a prior assigned\n    best_target_per_prior.index_fill_(0, best_prior_per_target_index, 2)\n    # size: num_priors\n    labels = gt_labels[best_target_per_prior_index]\n    labels[best_target_per_prior < iou_threshold] = 0  # the backgournd id\n    boxes = gt_boxes[best_target_per_prior_index]\n    return boxes, labels\n\n\ndef hard_negative_mining(loss, labels, neg_pos_ratio):\n    """"""\n    It used to suppress the presence of a large number of negative prediction.\n    It works on image level not batch level.\n    For any example/image, it keeps all the positive predictions and\n     cut the number of negative predictions to make sure the ratio\n     between the negative examples and positive examples is no more\n     the given ratio for an image.\n\n    Args:\n        loss (N, num_priors): the loss for each example.\n        labels (N, num_priors): the labels.\n        neg_pos_ratio:  the ratio between the negative examples and positive examples.\n    """"""\n    pos_mask = labels > 0\n    num_pos = pos_mask.long().sum(dim=1, keepdim=True)\n    num_neg = num_pos * neg_pos_ratio\n\n    loss[pos_mask] = -math.inf\n    _, indexes = loss.sort(dim=1, descending=True)\n    _, orders = indexes.sort(dim=1)\n    neg_mask = orders < num_neg\n    return pos_mask | neg_mask\n\n\ndef center_form_to_corner_form(locations):\n    return torch.cat((locations[..., :2] - locations[..., 2:] / 2,\n                      locations[..., :2] + locations[..., 2:] / 2), locations.dim() - 1)\n\n\ndef corner_form_to_center_form(boxes):\n    return torch.cat((\n        (boxes[..., :2] + boxes[..., 2:]) / 2,\n        boxes[..., 2:] - boxes[..., :2]\n    ), boxes.dim() - 1)'"
utilities/color_map.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nimport numpy as np\n\n\nclass VOCColormap(object):\n    def __init__(self, n=256, num_classes=21, normalized=False):\n        super().__init__()\n        self.n = n\n        self.normalized = normalized\n        self.num_classes = num_classes\n\n    def get_color_map(self):\n        def bitget(byteval, idx):\n            return ((byteval & (1 << idx)) != 0)\n\n        dtype = \'float32\' if self.normalized else \'uint8\'\n        cmap = np.zeros((self.n, 3), dtype=dtype)\n        for i in range(self.n):\n            r = g = b = 0\n            c = i\n            for j in range(8):\n                r = r | (bitget(c, 0) << 7 - j)\n                g = g | (bitget(c, 1) << 7 - j)\n                b = b | (bitget(c, 2) << 7 - j)\n                c = c >> 3\n\n            cmap[i] = np.array([r, g, b])\n        cmap = cmap / 255 if self.normalized else cmap\n        return cmap\n\n    def get_color_map_voc(self):\n        cmap = self.get_color_map()\n        cmap = np.asarray(cmap)\n        cmap = cmap.flatten()\n        cmap = cmap.tolist()\n        return cmap\n\n#adepallete = [0,0,0,120,120,120,180,120,120,6,230,230,80,50,50,4,200,3,120,120,80,140,140,140,204,5,255,230,230,230,4,250,7,224,5,255,235,255,7,150,5,61,120,120,70,8,255,51,255,6,82,143,255,140,204,255,4,255,51,7,204,70,3,0,102,200,61,230,250,255,6,51,11,102,255,255,7,71,255,9,224,9,7,230,220,220,220,255,9,92,112,9,255,8,255,214,7,255,224,255,184,6,10,255,71,255,41,10,7,255,255,224,255,8,102,8,255,255,61,6,255,194,7,255,122,8,0,255,20,255,8,41,255,5,153,6,51,255,235,12,255,160,150,20,0,163,255,140,140,140,250,10,15,20,255,0,31,255,0,255,31,0,255,224,0,153,255,0,0,0,255,255,71,0,0,235,255,0,173,255,31,0,255,11,200,200,255,82,0,0,255,245,0,61,255,0,255,112,0,255,133,255,0,0,255,163,0,255,102,0,194,255,0,0,143,255,51,255,0,0,82,255,0,255,41,0,255,173,10,0,255,173,255,0,0,255,153,255,92,0,255,0,255,255,0,245,255,0,102,255,173,0,255,0,20,255,184,184,0,31,255,0,255,61,0,71,255,255,0,204,0,255,194,0,255,82,0,10,255,0,112,255,51,0,255,0,194,255,0,122,255,0,255,163,255,153,0,0,255,10,255,112,0,143,255,0,82,0,255,163,255,0,255,235,0,8,184,170,133,0,255,0,255,92,184,0,255,255,0,31,0,184,255,0,214,255,255,0,112,92,255,0,0,224,255,112,224,255,70,184,160,163,0,255,153,0,255,71,255,0,255,0,163,255,204,0,255,0,143,0,255,235,133,255,0,255,0,235,245,0,255,255,0,122,255,245,0,10,190,212,214,255,0,0,204,255,20,0,255,255,255,0,0,153,255,0,41,255,0,255,204,41,0,255,41,255,0,173,0,255,0,245,255,71,0,255,122,0,255,0,255,184,0,92,255,184,255,0,0,133,255,255,214,0,25,194,194,102,255,0,92,0,255]\n\n#citypallete = [\n#128,64,128,244,35,232,70,70,70,102,102,156,190,153,153,153,153,153,250,170,30,220,220,0,107,142,35,152,251,152,70,130,180,220,20,60,255,0,0,0,0,142,0,0,70,0,60,100,0,80,100,0,0,230,119,11,32,128,192,0,0,64,128,128,64,128,0,192,128,128,192,128,64,64,0,192,64,0,64,192,0,192,192,0,64,64,128,192,64,128,64,192,128,192,192,128,0,0,64,128,0,64,0,128,64,128,128,64,0,0,192,128,0,192,0,128,192,128,128,192,64,0,64,192,0,64,64,128,64,192,128,64,64,0,192,192,0,192,64,128,192,192,128,192,0,64,64,128,64,64,0,192,64,128,192,64,0,64,192,128,64,192,0,192,192,128,192,192,64,64,64,192,64,64,64,192,64,192,192,64,64,64,192,192,64,192,64,192,192,192,192,192,32,0,0,160,0,0,32,128,0,160,128,0,32,0,128,160,0,128,32,128,128,160,128,128,96,0,0,224,0,0,96,128,0,224,128,0,96,0,128,224,0,128,96,128,128,224,128,128,32,64,0,160,64,0,32,192,0,160,192,0,32,64,128,160,64,128,32,192,128,160,192,128,96,64,0,224,64,0,96,192,0,224,192,0,96,64,128,224,64,128,96,192,128,224,192,128,32,0,64,160,0,64,32,128,64,160,128,64,32,0,192,160,0,192,32,128,192,160,128,192,96,0,64,224,0,64,96,128,64,224,128,64,96,0,192,224,0,192,96,128,192,224,128,192,32,64,64,160,64,64,32,192,64,160,192,64,32,64,192,160,64,192,32,192,192,160,192,192,96,64,64,224,64,64,96,192,64,224,192,64,96,64,192,224,64,192,96,192,192,224,192,192,0,32,0,128,32,0,0,160,0,128,160,0,0,32,128,128,32,128,0,160,128,128,160,128,64,32,0,192,32,0,64,160,0,192,160,0,64,32,128,192,32,128,64,160,128,192,160,128,0,96,0,128,96,0,0,224,0,128,224,0,0,96,128,128,96,128,0,224,128,128,224,128,64,96,0,192,96,0,64,224,0,192,224,0,64,96,128,192,96,128,64,224,128,192,224,128,0,32,64,128,32,64,0,160,64,128,160,64,0,32,192,128,32,192,0,160,192,128,160,192,64,32,64,192,32,64,64,160,64,192,160,64,64,32,192,192,32,192,64,160,192,192,160,192,0,96,64,128,96,64,0,224,64,128,224,64,0,96,192,128,96,192,0,224,192,128,224,192,64,96,64,192,96,64,64,224,64,192,224,64,64,96,192,192,96,192,64,224,192,192,224,192,32,32,0,160,32,0,32,160,0,160,160,0,32,32,128,160,32,128,32,160,128,160,160,128,96,32,0,224,32,0,96,160,0,224,160,0,96,32,128,224,32,128,96,160,128,224,160,128,32,96,0,160,96,0,32,224,0,160,224,0,32,96,128,160,96,128,32,224,128,160,224,128,96,96,0,224,96,0,96,224,0,224,224,0,96,96,128,224,96,128,96,224,128,224,224,128,32,32,64,160,32,64,32,160,64,160,160,64,32,32,192,160,32,192,32,160,192,160,160,192,96,32,64,224,32,64,96,160,64,224,160,64,96,32,192,224,32,192,96,160,192,224,160,192,32,96,64,160,96,64,32,224,64,160,224,64,32,96,192,160,96,192,32,224,192,160,224,192,96,96,64,224,96,64,96,224,64,224,224,64,96,96,192,224,96,192,96,224,192,0,0,0]\n'"
utilities/flops_compute.py,8,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nimport torch\n\n\'\'\'\nThis code is taken from here: https://github.com/warmspringwinds/pytorch-segmentation-detection/blob/master/pytorch_segmentation_detection/utils/flops_benchmark.py\n\nChanges: We adapted this code to support group convolutions (as well as depth-wise convolutions).\nFLOPs are verified for ShuffleNet.\n\'\'\'\n\n\n# ---- Public functions\n\ndef add_flops_counting_methods(net_main_module):\n    """"""Adds flops counting functions to an existing model. After that\n    the flops count should be activated and the model should be run on an input\n    image.\n\n    Example:\n\n    fcn = add_flops_counting_methods(fcn)\n    fcn = fcn.cuda().train()\n    fcn.start_flops_count()\n\n\n    _ = fcn(batch)\n\n    fcn.compute_average_flops_cost() / 1e9 / 2 # Result in GFLOPs per image in batch\n\n    Important: dividing by 2 only works for resnet models -- see below for the details\n    of flops computation.\n\n    Attention: we are counting multiply-add as two flops in this work, because in\n    most resnet models convolutions are bias-free (BN layers act as bias there)\n    and it makes sense to count muliply and add as separate flops therefore.\n    This is why in the above example we divide by 2 in order to be consistent with\n    most modern benchmarks. For example in ""Spatially Adaptive Computatin Time for Residual\n    Networks"" by Figurnov et al multiply-add was counted as two flops.\n\n    This module computes the average flops which is necessary for dynamic networks which\n    have different number of executed layers. For static networks it is enough to run the network\n    once and get statistics (above example).\n\n    Implementation:\n    The module works by adding batch_count to the main module which tracks the sum\n    of all batch sizes that were run through the network.\n\n    Also each convolutional layer of the network tracks the overall number of flops\n    performed.\n\n    The parameters are updated with the help of registered hook-functions which\n    are being called each time the respective layer is executed.\n\n    Parameters\n    ----------\n    net_main_module : torch.nn.Module\n        Main module containing network\n\n    Returns\n    -------\n    net_main_module : torch.nn.Module\n        Updated main module with new methods/attributes that are used\n        to compute flops.\n    """"""\n\n    # adding additional methods to the existing module object,\n    # this is done this way so that each function has access to self object\n    net_main_module.start_flops_count = start_flops_count.__get__(net_main_module)\n    net_main_module.stop_flops_count = stop_flops_count.__get__(net_main_module)\n    net_main_module.reset_flops_count = reset_flops_count.__get__(net_main_module)\n    net_main_module.compute_average_flops_cost = compute_average_flops_cost.__get__(net_main_module)\n\n    net_main_module.reset_flops_count()\n\n    # Adding varialbles necessary for masked flops computation\n    net_main_module.apply(add_flops_mask_variable_or_reset)\n\n    return net_main_module\n\n\ndef compute_average_flops_cost(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Returns current mean flops consumption per image.\n\n    """"""\n\n    batches_count = self.__batch_counter__\n\n    flops_sum = 0\n\n    for module in self.modules():\n\n        if isinstance(module, torch.nn.Conv2d):\n            flops_sum += module.__flops__\n\n    return flops_sum / batches_count\n\n\ndef start_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Activates the computation of mean flops consumption per image.\n    Call it before you run the network.\n\n    """"""\n\n    add_batch_counter_hook_function(self)\n\n    self.apply(add_flops_counter_hook_function)\n\n\ndef stop_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Stops computing the mean flops consumption per image.\n    Call whenever you want to pause the computation.\n\n    """"""\n\n    remove_batch_counter_hook_function(self)\n\n    self.apply(remove_flops_counter_hook_function)\n\n\ndef reset_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Resets statistics computed so far.\n\n    """"""\n\n    add_batch_counter_variables_or_reset(self)\n\n    self.apply(add_flops_counter_variable_or_reset)\n\n\ndef add_flops_mask(module, mask):\n    def add_flops_mask_func(module):\n        if isinstance(module, torch.nn.Conv2d):\n            module.__mask__ = mask\n\n    module.apply(add_flops_mask_func)\n\n\ndef remove_flops_mask(module):\n    module.apply(add_flops_mask_variable_or_reset)\n\n\n# ---- Internal functions\n\n\ndef conv_flops_counter_hook(conv_module, input, output):\n    # Can have multiple inputs, getting the first one\n    # print(conv_module)\n    input = input[0]\n    batch_size = input.size()[0]\n    output_height, output_width = output.size()[2:]\n\n    kernel_height, kernel_width = conv_module.kernel_size\n    in_channels = conv_module.in_channels\n    out_channels = conv_module.out_channels\n    groups = conv_module.groups if conv_module.groups else 1\n\n    # We count multiply-add as 2 flops\n    conv_per_position_flops = (2 * kernel_height * kernel_width * in_channels * out_channels) / groups\n\n    active_elements_count = batch_size * output_height * output_width\n\n    if conv_module.__mask__ is not None:\n        # (b, 1, h, w)\n        flops_mask = conv_module.__mask__.expand(batch_size, 1, output_height, output_width)\n        active_elements_count = flops_mask.sum()\n\n    overall_conv_flops = conv_per_position_flops * active_elements_count\n\n    bias_flops = 0\n\n    if conv_module.bias is not None:\n        bias_flops = out_channels * active_elements_count\n\n    overall_flops = overall_conv_flops + bias_flops\n\n    conv_module.__flops__ += overall_flops\n\n\ndef batch_counter_hook(module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n\n    batch_size = input.size()[0]\n\n    module.__batch_counter__ += batch_size\n\n\ndef add_batch_counter_variables_or_reset(module):\n    module.__batch_counter__ = 0\n\n\ndef add_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        return\n\n    handle = module.register_forward_hook(batch_counter_hook)\n    module.__batch_counter_handle__ = handle\n\n\ndef remove_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        module.__batch_counter_handle__.remove()\n\n        del module.__batch_counter_handle__\n\n\ndef add_flops_counter_variable_or_reset(module):\n    if isinstance(module, torch.nn.Conv2d):\n        module.__flops__ = 0\n\n\ndef add_flops_counter_hook_function(module):\n    if isinstance(module, torch.nn.Conv2d):\n\n        if hasattr(module, \'__flops_handle__\'):\n            return\n\n        handle = module.register_forward_hook(conv_flops_counter_hook)\n        module.__flops_handle__ = handle\n\n\ndef remove_flops_counter_hook_function(module):\n    if isinstance(module, torch.nn.Conv2d):\n\n        if hasattr(module, \'__flops_handle__\'):\n            module.__flops_handle__.remove()\n\n            del module.__flops_handle__\n\n\n# --- Masked flops counting\n\n\n# Also being run in the initialization\ndef add_flops_mask_variable_or_reset(module):\n    if isinstance(module, torch.nn.Conv2d):\n        module.__mask__ = None\n\n'"
utilities/lr_scheduler.py,0,"b'import bisect\nimport math\n#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nclass CyclicLR(object):\n    \'\'\'\n    CLass that defines cyclic learning rate with warm restarts that decays the learning rate linearly till the end of cycle and then restarts\n    at the maximum value.\n    See https://arxiv.org/abs/1811.11431 for more details\n    \'\'\'\n    def __init__(self, min_lr=0.1, cycle_len=5, steps=[51, 101, 131, 161, 191, 221, 251, 281], gamma=0.5, step=True):\n        super(CyclicLR, self).__init__()\n        assert len(steps) > 0, \'Please specify step intervals.\'\n        assert 0 < gamma <= 1, \'Learing rate decay factor should be between 0 and 1\'\n        self.min_lr = min_lr # minimum learning rate\n        self.m = cycle_len\n        self.steps = steps\n        self.warm_up_interval = 1 # we do not start from max value for the first epoch, because some time it diverges\n        self.counter = 0\n        self.decayFactor = gamma # factor by which we should decay learning rate\n        self.count_cycles = 0\n        self.step_counter = 0\n        self.stepping = step\n\n    def step(self, epoch):\n        if epoch%self.steps[self.step_counter] == 0 and epoch > 1 and self.stepping:\n            self.min_lr = self.min_lr * self.decayFactor\n            self.count_cycles = 0\n            if self.step_counter < len(self.steps) - 1:\n                self.step_counter += 1\n            else:\n                self.stepping = False\n        current_lr = self.min_lr\n        # warm-up or cool-down phase\n        if self.count_cycles < self.warm_up_interval:\n            self.count_cycles += 1\n            # We do not need warm up after first step.\n            # so, we set warm up interval to 0 after first step\n            if self.count_cycles == self.warm_up_interval:\n                self.warm_up_interval = 0\n        else:\n            #Cyclic learning rate with warm restarts\n            # max_lr (= min_lr * step_size) is decreased to min_lr using linear decay before\n            # it is set to max value at the end of cycle.\n            if self.counter >= self.m:\n                self.counter = 0\n            current_lr = round((self.min_lr * self.m) - (self.counter * self.min_lr), 5)\n            self.counter += 1\n            self.count_cycles += 1\n        return current_lr\n\n    def __repr__(self):\n        fmt_str = \'Scheduler \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Min. base LR: {}\\n\'.format(self.min_lr)\n        fmt_str += \'    Max. base LR: {}\\n\'.format(self.min_lr * self.m)\n        fmt_str += \'    Step interval: {}\\n\'.format(self.steps)\n        fmt_str += \'    Decay lr at each step by {}\\n\'.format(self.decayFactor)\n        return fmt_str\n\n\nclass FixedMultiStepLR(object):\n    \'\'\'\n        Fixed LR scheduler with steps\n    \'\'\'\n    def __init__(self, base_lr=0.1, steps=[30, 60, 90], gamma=0.1, step=True):\n        super(FixedMultiStepLR, self).__init__()\n        assert len(steps) > 1, \'Please specify step intervals.\'\n        self.base_lr = base_lr\n        self.steps = steps\n        self.decayFactor = gamma # factor by which we should decay learning rate\n        self.stepping = step\n        print(\'Using Fixed LR Scheduler\')\n\n    def step(self, epoch):\n        return round(self.base_lr * (self.decayFactor ** bisect.bisect(self.steps, epoch)), 5)\n\n    def __repr__(self):\n        fmt_str = \'Scheduler \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Base LR: {}\\n\'.format(self.base_lr)\n        fmt_str += \'    Step interval: {}\\n\'.format(self.steps)\n        fmt_str += \'    Decay lr at each step by {}\\n\'.format(self.decayFactor)\n        return fmt_str\n\n\nclass PolyLR(object):\n    \'\'\'\n        Polynomial LR scheduler with steps\n    \'\'\'\n    def __init__(self, base_lr, max_epochs, power=0.99):\n        super(PolyLR, self).__init__()\n        assert 0 < power < 1\n        self.base_lr = base_lr\n        self.power = power\n        self.max_epochs = max_epochs\n\n    def step(self, epoch):\n        curr_lr = self.base_lr * (1 - (float(epoch) / self.max_epochs)) ** self.power\n        return round(curr_lr, 6)\n\n    def __repr__(self):\n        fmt_str = \'Scheduler \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Total Epochs: {}\\n\'.format(self.max_epochs)\n        fmt_str += \'    Base LR: {}\\n\'.format(self.base_lr)\n        fmt_str += \'    Power: {}\\n\'.format(self.power)\n        return fmt_str\n\n\nclass LinearLR(object):\n    def __init__(self, base_lr, max_epochs):\n        super(LinearLR, self).__init__()\n        self.base_lr = base_lr\n        self.max_epochs = max_epochs\n\n    def step(self, epoch):\n        curr_lr = self.base_lr - (self.base_lr * (epoch / (self.max_epochs)))\n        return round(curr_lr, 6)\n\n    def __repr__(self):\n        fmt_str = \'Scheduler \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Total Epochs: {}\\n\'.format(self.max_epochs)\n        fmt_str += \'    Base LR: {}\\n\'.format(self.base_lr)\n        return fmt_str\n\n\nclass HybirdLR(object):\n    def __init__(self, base_lr, clr_max, max_epochs, cycle_len=5):\n        super(HybirdLR, self).__init__()\n        self.linear_epochs = max_epochs - clr_max + 1\n        steps = [clr_max]\n        self.clr = CyclicLR(min_lr=base_lr, cycle_len=cycle_len, steps=steps, gamma=1)\n        self.decay_lr = LinearLR(base_lr=base_lr, max_epochs=self.linear_epochs)\n        self.cyclic_epochs = clr_max\n\n        self.base_lr=base_lr\n        self.max_epochs = max_epochs\n        self.clr_max = clr_max\n        self.cycle_len = cycle_len\n\n    def step(self, epoch):\n        if epoch < self.cyclic_epochs:\n            curr_lr = self.clr.step(epoch)\n        else:\n            curr_lr = self.decay_lr.step(epoch - self.cyclic_epochs + 1)\n        return round(curr_lr, 6)\n\n    def __repr__(self):\n        fmt_str = \'Scheduler \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Total Epochs: {}\\n\'.format(self.max_epochs)\n        fmt_str += \'    Cycle with length of {}: {}\\n\'.format(self.cycle_len , int(self.clr_max/self.cycle_len))\n        fmt_str += \'    Base LR with {} cycle length: {}\\n\'.format(self.cycle_len, self.base_lr)\n        fmt_str += \'    Cycle with length of {}: {}\\n\'.format(self.linear_epochs, 1)\n        fmt_str += \'    Base LR with {} cycle length: {}\\n\'.format(self.linear_epochs, self.base_lr)\n        return fmt_str\n\n\nclass CosineLR(object):\n    def __init__(self, base_lr, max_epochs):\n        super(CosineLR, self).__init__()\n        self.base_lr = base_lr\n        self.max_epochs = max_epochs\n\n    def step(self, epoch):\n        return round(self.base_lr * (1 + math.cos(math.pi * epoch / self.max_epochs)) / 2, 6)\n\n    def __repr__(self):\n        fmt_str = \'Scheduler \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Total Epochs: {}\\n\'.format(self.max_epochs)\n        fmt_str += \'    Base LR : {}\\n\'.format(self.base_lr)\n        return fmt_str\n\n\nif __name__ == \'__main__\':\n    max_epochs = 100\n    lrSched = PolyLR(0.007, max_epochs=max_epochs)\n    #lrSched = CosineLR(0.01, max_epochs=200)\n    #for i in range(max_epochs):\n    #    print(i, lrSched.step(i))\n    #exit()\n    lrSched = HybirdLR(0.007, clr_max=51, max_epochs=max_epochs)\n    for i in range(max_epochs):\n        print(i, lrSched.step(i))\n    exit()\n    lrSched = PolyLR(0.007, max_epochs=50)\n    for i in range(50):\n        print(i, lrSched.step(i))\n\n    max_epochs = 240\n    lrSched = CyclicLR(min_lr=0.01, steps=[51, 161, 201], gamma=0.1)\n    print(lrSched)\n    for i in range(max_epochs):\n        print(i, lrSched.step(i))\n'"
utilities/nms.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nfrom torchvision.ops import nms as _nms\n\n\ndef nms(box_scores, nms_threshold, top_k=200):\n    """"""\n    Args:\n        box_scores (N, 5): boxes in corner-form and probabilities.\n        nms_threshold: intersection over union threshold.\n        top_k: keep top_k results. If k <= 0, keep all the results.\n        candidate_size: only consider the candidates with the highest scores.\n    Returns:\n         picked: a list of indexes of the kept boxes\n    """"""\n    scores = box_scores[:, -1]\n    boxes = box_scores[:, :-1]\n    keep = _nms(boxes, scores, nms_threshold)\n    if top_k > 0:\n        keep = keep[:top_k]\n    return box_scores[keep, :]\n\n#\n# from utilities.box_utils import iou_of\n# def nms1(box_scores, nms_threshold, top_k=200, k=-1):\n#     """"""\n#     Args:\n#         box_scores (N, 5): boxes in corner-form and probabilities.\n#         nms_threshold: intersection over union threshold.\n#         top_k: keep top_k results. If k <= 0, keep all the results.\n#         candidate_size: only consider the candidates with the highest scores.\n#     Returns:\n#          picked: a list of indexes of the kept boxes\n#     """"""\n#     scores = box_scores[:, -1]\n#     boxes = box_scores[:, :-1]\n#     scores = scores.to(\'cpu\')\n#     boxes = boxes.to(\'cpu\')\n#     keep = []\n#     _, indexes = scores.sort(descending=True)\n#     indexes = indexes[:top_k]\n#     while len(indexes) > 0:\n#         current = indexes[0]\n#         keep.append(current.item())\n#         if 0 < k == len(keep) or len(indexes) == 1:\n#             break\n#         current_box = boxes[current, :]\n#         indexes = indexes[1:]\n#         rest_boxes = boxes[indexes, :]\n#         iou = iou_of(\n#             rest_boxes,\n#             current_box.unsqueeze(0),\n#         )\n#         indexes = indexes[iou <= nms_threshold]\n#     return box_scores[keep, :]'"
utilities/parallel_wrapper.py,6,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nimport torch\nimport threading\nfrom torch import nn\nfrom torch.cuda._utils import _get_device_index\nfrom torch.nn.parallel.parallel_apply import get_a_var\nfrom torch.nn.parallel.data_parallel import DataParallel\n\n\'\'\'\nThis class defines data parallel wrappers for mdoel and criterian parallelism.\nThese are basically adapted from PyTroch\'s DataParallel wrapper\'\'\'\n\nclass DataParallelModel(DataParallel):\n\n    def forward(self, *inputs, **kwargs):\n        \'\'\' The only difference between this and PyTorch\'s native implementation is that\n        we do not need gather function because we will perform gathering inside Criterian\n        wrapper.\'\'\'\n        if not self.device_ids:\n            return self.module(*inputs, **kwargs)\n        inputs, kwargs = self.scatter(inputs, kwargs, self.device_ids)\n        if len(self.device_ids) == 1:\n            return self.module(*inputs[0], **kwargs[0])\n        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n        return self.parallel_apply(replicas, inputs, kwargs)\n\n\nclass DataParallelCriteria(DataParallel):\n\n    def forward(self, inputs, *targets, **kwargs):\n        \'\'\'\n        Input is already sliced, so slice only target\n        \'\'\'\n        if not self.device_ids:\n            return self.module(inputs, *targets, **kwargs)\n        if len(self.device_ids) == 1:\n            return self.module(inputs, *targets[0], **kwargs[0])\n        targets, kwargs = self.scatter(targets, kwargs, self.device_ids)\n        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\n        outputs = parallel_apply_criteria(replicas, inputs, targets, kwargs)\n        return self.gather(outputs, self.output_device)\n\n\ndef parallel_apply_criteria(modules, inputs, targets, kwargs_tup=None, devices=None):\n    assert len(modules) == len(inputs)\n    assert len(targets) == len(inputs)\n    if kwargs_tup:\n        assert len(modules) == len(kwargs_tup)\n    else:\n        kwargs_tup = ({},) * len(modules)\n    if devices is not None:\n        assert len(modules) == len(devices)\n    else:\n        devices = [None] * len(modules)\n\n    lock = threading.Lock()\n    results = {}\n    grad_enabled = torch.is_grad_enabled()\n\n    def _worker(i, module, input, target, kwargs, device=None):\n        torch.set_grad_enabled(grad_enabled)\n        if device is None:\n            device = get_a_var(input).get_device()\n        try:\n            with torch.cuda.device(device):\n                # PyTorch\'s native implementation convert to tuple to avoid further slicing\n                # Just extract the tensor out of the tuple to compute loss\n                if isinstance(input, (list, tuple)):\n                    input = input[0]\n                if isinstance(target, (list, tuple)):\n                    target = target[0]\n                assert target.device == input.device\n                if module.device != input.device:\n                    module = module.to(input.device)\n                output = module(input, target, **kwargs)\n            with lock:\n                results[i] = output\n        except Exception as e:\n            with lock:\n                results[i] = e\n\n    if len(modules) > 1:\n        threads = [threading.Thread(target=_worker,\n                                    args=(i, module, input, target,\n                                          kwargs, device),)\n                   for i, (module, input, target, kwargs, device) in\n                   enumerate(zip(modules, inputs, targets, kwargs_tup, devices))]\n\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n    else:\n        _worker(0, modules[0], inputs[0], targets[0], kwargs_tup[0], devices[0])\n\n    outputs = []\n    for i in range(len(inputs)):\n        output = results[i]\n        if isinstance(output, Exception):\n            raise output\n        outputs.append(output)\n    return outputs\n\n'"
utilities/print_utils.py,0,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\nimport time\n\ntext_colors = {\n               \'logs\': \'\\033[34m\', # 033 is the escape code and 34 is the color code\n               \'info\': \'\\033[32m\',\n               \'warning\': \'\\033[33m\',\n               \'error\': \'\\033[31m\',\n               \'bold\': \'\\033[1m\',\n               \'end_color\': \'\\033[0m\'\n               }\n\n\ndef get_curr_time_stamp():\n    return time.strftime(""%Y-%m-%d %H:%M:%S"")\n\n\ndef print_error_message(message):\n    time_stamp = get_curr_time_stamp()\n    error_str = text_colors[\'error\'] + text_colors[\'bold\'] + \'ERROR  \' + text_colors[\'end_color\']\n    print(\'{} - {} - {}\'.format(time_stamp, error_str, message))\n    print(\'{} - {} - {}\'.format(time_stamp, error_str, \'Exiting!!!\'))\n    exit(-1)\n\n\ndef print_log_message(message):\n    time_stamp = get_curr_time_stamp()\n    log_str = text_colors[\'logs\'] + text_colors[\'bold\'] + \'LOGS   \' + text_colors[\'end_color\']\n    print(\'{} - {} - {}\'.format(time_stamp, log_str, message))\n\n\ndef print_warning_message(message):\n    time_stamp = get_curr_time_stamp()\n    warn_str = text_colors[\'warning\'] + text_colors[\'bold\'] + \'WARNING\' + text_colors[\'end_color\']\n    print(\'{} - {} - {}\'.format(time_stamp, warn_str, message))\n\n\ndef print_info_message(message):\n    time_stamp = get_curr_time_stamp()\n    info_str = text_colors[\'info\'] + text_colors[\'bold\'] + \'INFO   \' + text_colors[\'end_color\']\n    print(\'{} - {} - {}\'.format(time_stamp, info_str, message))\n\n\nif __name__ == \'__main__\':\n    print_log_message(\'Testing\')\n    print_warning_message(\'Testing\')\n    print_info_message(\'Testing\')\n    print_error_message(\'Testing\')\n'"
utilities/train_eval_classification.py,4,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nimport time\nimport torch\nfrom utilities.utils import AverageMeter\nfrom utilities.metrics.classification_accuracy import accuracy\nfrom torch.nn import functional as F\nfrom utilities.print_utils import *\n\n\'\'\'\nTraining loop\n\'\'\'\ndef train(data_loader, model, criteria, optimizer, epoch, device=\'cuda\'):\n\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n\n    # switch to train mode\n    model.train()\n\n    end = time.time()\n    for i, (input, target) in enumerate(data_loader):\n\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        input = input.to(device)\n        target = target.to(device)\n\n        # compute output\n        output = model(input)\n\n        # compute loss\n        loss = criteria(output, target)\n\n        # measure accuracy and record loss\n        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n        #losses.update(loss.data[0], input.size(0))\n        losses.update(loss.item(), input.size(0))\n        top1.update(prec1[0].item(), input.size(0))\n        top5.update(prec5[0].item(), input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % 10 == 0: #print after every 100 batches\n            print_log_message(""Epoch: %d[%d/%d]\\t\\tBatch Time:%.4f\\t\\tLoss:%.4f\\t\\ttop1:%.4f (%.4f)\\t\\ttop5:%.4f (%.4f)"" %\n                              (epoch, i, len(data_loader), batch_time.avg, losses.avg, top1.val, top1.avg, top5.val, top5.avg))\n\n\n    return top1.avg, losses.avg\n\n\'\'\'\nValidation loop\n\'\'\'\ndef validate(data_loader, model, criteria=None, device=\'cuda\'):\n    batch_time = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    if criteria:\n        losses = AverageMeter()\n    # switch to evaluate mode\n    model.eval()\n\n    # with torch.no_grad():\n    end = time.time()\n    with torch.no_grad():\n        for i, (input, target) in enumerate(data_loader):\n            input = input.to(device)\n            target = target.to(device)\n\n            # compute output\n            output = model(input)\n            if criteria:\n                loss = criteria(output, target)\n\n            # measure accuracy and record loss\n            prec1, prec5 = accuracy(output, target, topk=(1, 5))\n\n            if criteria:\n                losses.update(loss.item(), input.size(0))\n            top1.update(prec1[0].item(), input.size(0))\n            top5.update(prec5[0].item(), input.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % 10 == 0 and criteria: # print after every 100 batches\n                print_log_message(""Batch:[%d/%d]\\t\\tBatchTime:%.3f\\t\\tLoss:%.3f\\t\\ttop1:%.3f (%.3f)\\t\\ttop5:%.3f(%.3f)"" %\n                                  (i, len(data_loader), batch_time.avg, losses.avg, top1.val, top1.avg, top5.val, top5.avg))\n            elif i % 10:\n                print_log_message(\n                    ""Batch:[%d/%d]\\t\\tBatchTime:%.3f\\t\\ttop1:%.3f (%.3f)\\t\\ttop5:%.3f(%.3f)"" %\n                    (i, len(data_loader), batch_time.avg, top1.val, top1.avg, top5.val, top5.avg))\n\n\n        print_info_message(\' * Prec@1:%.3f Prec@5:%.3f\' % (top1.avg, top5.avg))\n\n        if criteria:\n            return top1.avg, losses.avg\n        else:\n            return top1.avg\n\n\ndef train_multi(data_loader, model, criteria, optimizer, epoch, device=\'cuda\'):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    prec = AverageMeter()\n    rec = AverageMeter()\n\n    # switch to train mode\n    model.train()\n    end = time.time()\n\n\n    tp, fp, fn, tn, count = 0, 0, 0, 0, 0\n    p_o, r_o, f_o = 0.0, 0.0, 0.0\n    for i, (input, target) in enumerate(data_loader):\n\n        target = target.to(device=device)\n        target = target.max(dim=1)[0]\n        # compute output\n        output = model(input)\n        loss = criteria(output, target.float()) * 80.0\n\n        # measure accuracy and record loss\n        pred = output.gt(0.0).long()\n\n        tp += (pred + target).eq(2).sum(dim=0)\n        fp += (pred - target).eq(1).sum(dim=0)\n        fn += (pred - target).eq(-1).sum(dim=0)\n        tn += (pred + target).eq(0).sum(dim=0)\n        count += input.size(0)\n\n        this_tp = (pred + target).eq(2).sum()\n        this_fp = (pred - target).eq(1).sum()\n        this_fn = (pred - target).eq(-1).sum()\n        this_tn = (pred + target).eq(0).sum()\n        this_acc = (this_tp + this_tn).float() / (this_tp + this_tn + this_fp + this_fn).float()\n\n        this_prec = this_tp.float() / (this_tp + this_fp).float() * 100.0 if this_tp + this_fp != 0 else 0.0\n        this_rec = this_tp.float() / (this_tp + this_fn).float() * 100.0 if this_tp + this_fn != 0 else 0.0\n\n        losses.update(float(loss), input.size(0))\n        prec.update(float(this_prec), input.size(0))\n        rec.update(float(this_rec), input.size(0))\n\n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        p_c = [float(tp[i].float() / (tp[i] + fp[i]).float()) * 100.0 if tp[i] > 0 else 0.0 for i in range(len(tp))]\n        r_c = [float(tp[i].float() / (tp[i] + fn[i]).float()) * 100.0 if tp[i] > 0 else 0.0 for i in range(len(tp))]\n        f_c = [2 * p_c[i] * r_c[i] / (p_c[i] + r_c[i]) if tp[i] > 0 else 0.0 for i in range(len(tp))]\n\n        p_o = tp.sum().float() / (tp + fp).sum().float() * 100.0\n        r_o = tp.sum().float() / (tp + fn).sum().float() * 100.0\n        f_o = 2 * p_o * r_o / (p_o + r_o)\n\n        if i % 100 == 0:\n            print_log_message(\'Epoch: [{0}][{1}/{2}]\\t\'\n                  \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Precision {prec.val:.2f} ({prec.avg:.2f})\\t\'\n                  \'Recall {rec.val:.2f} ({rec.avg:.2f})\'.format(\n                epoch, i, len(data_loader), batch_time=batch_time,\n                loss=losses, prec=prec, rec=rec))\n\n    return f_o, losses.avg\n\n\ndef validate_multi(data_loader, model, criteria, device=\'cuda\'):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    prec = AverageMeter()\n    rec = AverageMeter()\n\n    # switch to evaluate mode\n    model.eval()\n\n    end = time.time()\n    tp, fp, fn, tn, count = 0, 0, 0, 0, 0\n    tp_size, fn_size = 0, 0\n    with torch.no_grad():\n        for i, (input, target) in enumerate(data_loader):\n            input = input.to(device=device)\n\n            target = target.to(device=device)\n            original_target = target\n            target = target.max(dim=1)[0]\n\n            # compute output\n            output = model(input)\n            loss = criteria(output, target.float())\n\n            # measure accuracy and record loss\n            pred = output.data.gt(0.0).long()\n\n            tp += (pred + target).eq(2).sum(dim=0)\n            fp += (pred - target).eq(1).sum(dim=0)\n            fn += (pred - target).eq(-1).sum(dim=0)\n            tn += (pred + target).eq(0).sum(dim=0)\n            three_pred = pred.unsqueeze(1).expand(-1, 3, -1)  # n, 3, 80\n            tp_size += (three_pred + original_target).eq(2).sum(dim=0)\n            fn_size += (three_pred - original_target).eq(-1).sum(dim=0)\n            count += input.size(0)\n\n            this_tp = (pred + target).eq(2).sum()\n            this_fp = (pred - target).eq(1).sum()\n            this_fn = (pred - target).eq(-1).sum()\n            this_tn = (pred + target).eq(0).sum()\n            this_acc = (this_tp + this_tn).float() / (this_tp + this_tn + this_fp + this_fn).float()\n\n            this_prec = this_tp.float() / (this_tp + this_fp).float() * 100.0 if this_tp + this_fp != 0 else 0.0\n            this_rec = this_tp.float() / (this_tp + this_fn).float() * 100.0 if this_tp + this_fn != 0 else 0.0\n\n            losses.update(float(loss), input.size(0))\n            prec.update(float(this_prec), input.size(0))\n            rec.update(float(this_rec), input.size(0))\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            p_c = [float(tp[i].float() / (tp[i] + fp[i]).float()) * 100.0 if tp[i] > 0 else 0.0 for i in range(len(tp))]\n            r_c = [float(tp[i].float() / (tp[i] + fn[i]).float()) * 100.0 if tp[i] > 0 else 0.0 for i in range(len(tp))]\n            f_c = [2 * p_c[i] * r_c[i] / (p_c[i] + r_c[i]) if tp[i] > 0 else 0.0 for i in range(len(tp))]\n\n            mean_p_c = sum(p_c) / len(p_c)\n            mean_r_c = sum(r_c) / len(r_c)\n            mean_f_c = sum(f_c) / len(f_c)\n\n            p_o = tp.sum().float() / (tp + fp).sum().float() * 100.0\n            r_o = tp.sum().float() / (tp + fn).sum().float() * 100.0\n            f_o = 2 * p_o * r_o / (p_o + r_o)\n\n            if i % 100 == 0:\n                print_log_message(\'Test: [{0}/{1}]\\t\'\n                      \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                      \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                      \'Precision {prec.val:.2f} ({prec.avg:.2f})\\t\'\n                      \'Recall {rec.val:.2f} ({rec.avg:.2f})\'.format(\n                    i, len(data_loader), batch_time=batch_time, loss=losses,\n                    prec=prec, rec=rec))\n                print(\'P_C {:.2f} R_C {:.2f} F_C {:.2f} P_O {:.2f} R_O {:.2f} F_O {:.2f}\'\n                      .format(mean_p_c, mean_r_c, mean_f_c, p_o, r_o, f_o))\n\n        print_info_message(\' * P_C {:.2f} R_C {:.2f} F_C {:.2f} P_O {:.2f} R_O {:.2f} F_O {:.2f}\'\n              .format(mean_p_c, mean_r_c, mean_f_c, p_o, r_o, f_o))\n        return f_o, losses.avg'"
utilities/train_eval_detect.py,1,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nfrom utilities.utils import AverageMeter\nimport time\nimport torch\nfrom utilities.print_utils import *\n\n\ndef train(data_loader, model, criterion, optimizer, device, epoch=-1):\n    model.train()\n\n    train_loss = AverageMeter()\n    train_cl_loss = AverageMeter()\n    train_loc_loss = AverageMeter()\n    batch_time = AverageMeter()\n    end = time.time()\n\n    for batch_idx, (images, boxes, labels) in enumerate(data_loader):\n        images = images.to(device)\n        boxes = boxes.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        confidences, locations = model(images)\n        regression_loss, classification_loss = criterion(confidences, locations, labels, boxes)\n\n        loss = regression_loss + classification_loss\n        loss.backward()\n        optimizer.step()\n\n        N = images.size(0)\n        train_loss.update(loss.item(), N)\n        train_cl_loss.update(classification_loss.item(), N)\n        train_loc_loss.update(regression_loss.item(), N)\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if batch_idx and batch_idx % 10 == 0:\n            print_log_message(\n                ""Epoch: %d[%d/%d]\\t\\tBatch Time:%.4f\\t\\tCls Loss: %.4f(%.4f)\\t \\tLoc Loss: %.4f(%.4f)\\t\\tTotal Loss: %.4f(%.4f)"" %\n                (epoch, batch_idx, len(data_loader), batch_time.avg,\n                 train_cl_loss.val, train_cl_loss.avg,\n                 train_loc_loss.val, train_loc_loss.avg,\n                 train_loss.val, train_loss.avg\n                 ))\n\n    return train_loss.avg, train_cl_loss.avg, train_loc_loss.avg\n\n\ndef validate(data_loader, model, criterion, device, epoch):\n    model.eval()\n\n    val_loss = AverageMeter()\n    val_cl_loss = AverageMeter()\n    val_loc_loss = AverageMeter()\n    batch_time = AverageMeter()\n    end = time.time()\n\n    with torch.no_grad():\n        for batch_idx, (images, boxes, labels) in enumerate(data_loader):\n            images = images.to(device)\n            boxes = boxes.to(device)\n            labels = labels.to(device)\n            confidences, locations = model(images)\n            regression_loss, classification_loss = criterion(confidences, locations, labels, boxes)\n\n            loss = regression_loss + classification_loss\n\n            N = images.size(0)\n            val_loss.update(loss.item(), N)\n            val_cl_loss.update(classification_loss.item(), N)\n            val_loc_loss.update(regression_loss.item(), N)\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if batch_idx and batch_idx % 10 == 0:\n                print_log_message(\n                    ""Epoch: %d[%d/%d]\\t\\tBatch Time:%.4f\\t\\tCls Loss: %.4f(%.4f)\\t \\tLoc Loss: %.4f(%.4f)\\t\\tTotal Loss: %.4f(%.4f)"" %\n                    (epoch, batch_idx, len(data_loader), batch_time.avg,\n                     val_cl_loss.val, val_cl_loss.avg,\n                     val_loc_loss.val, val_loc_loss.avg,\n                     val_loss.val, val_loss.avg\n                     ))\n        return val_loss.avg, val_cl_loss.avg, val_loc_loss.avg\n\n'"
utilities/train_eval_seg.py,2,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nimport torch\nfrom utilities.utils import AverageMeter\nimport time\nfrom utilities.metrics.segmentation_miou import MIOU\nfrom utilities.print_utils import *\nfrom torch.nn.parallel import gather\n\ndef train_seg(model, dataset_loader, optimizer, criterion, num_classes, epoch, device=\'cuda\'):\n    losses = AverageMeter()\n    batch_time = AverageMeter()\n    inter_meter = AverageMeter()\n    union_meter = AverageMeter()\n    end = time.time()\n    model.train()\n\n    miou_class = MIOU(num_classes=num_classes)\n\n    for i, (inputs, target) in enumerate(dataset_loader):\n        inputs = inputs.to(device=device)\n        target = target.to(device=device)\n\n        outputs = model(inputs)\n\n        if device == \'cuda\':\n            loss = criterion(outputs, target).mean()\n            if isinstance(outputs, (list, tuple)):\n                target_dev = outputs[0].device\n                outputs = gather(outputs, target_device=target_dev)\n        else:\n            loss = criterion(outputs, target)\n\n        inter, union = miou_class.get_iou(outputs, target)\n\n        inter_meter.update(inter)\n        union_meter.update(union)\n\n        losses.update(loss.item(), inputs.size(0))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % 10 == 0:  # print after every 100 batches\n            iou = inter_meter.sum / (union_meter.sum + 1e-10)\n            miou = iou.mean() * 100\n            print_log_message(""Epoch: %d[%d/%d]\\t\\tBatch Time:%.4f\\t\\tLoss:%.4f\\t\\tmiou:%.4f"" %\n                  (epoch, i, len(dataset_loader), batch_time.avg, losses.avg, miou))\n\n    iou = inter_meter.sum / (union_meter.sum + 1e-10)\n    miou = iou.mean() * 100\n    return miou, losses.avg\n\n\ndef val_seg(model, dataset_loader, criterion=None, num_classes=21, device=\'cuda\'):\n    model.eval()\n    inter_meter = AverageMeter()\n    union_meter = AverageMeter()\n    batch_time = AverageMeter()\n    end = time.time()\n\n    miou_class = MIOU(num_classes=num_classes)\n\n    if criterion:\n        losses = AverageMeter()\n\n    with torch.no_grad():\n        for i, (inputs, target) in enumerate(dataset_loader):\n            inputs = inputs.to(device=device)\n            target = target.to(device=device)\n            outputs = model(inputs)\n\n            if criterion:\n                if device == \'cuda\':\n                    loss = criterion(outputs, target).mean()\n                    if isinstance(outputs, (list, tuple)):\n                        target_dev = outputs[0].device\n                        outputs = gather(outputs, target_device=target_dev)\n                else:\n                    loss = criterion(outputs, target)\n                losses.update(loss.item(), inputs.size(0))\n\n            inter, union = miou_class.get_iou(outputs, target)\n            inter_meter.update(inter)\n            union_meter.update(union)\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % 10 == 0:  # print after every 100 batches\n                iou = inter_meter.sum / (union_meter.sum + 1e-10)\n                miou = iou.mean() * 100\n                loss_ = losses.avg if criterion is not None else 0\n                print_log_message(""[%d/%d]\\t\\tBatch Time:%.4f\\t\\tLoss:%.4f\\t\\tmiou:%.4f"" %\n                      (i, len(dataset_loader), batch_time.avg, loss_, miou))\n\n    iou = inter_meter.sum / (union_meter.sum + 1e-10)\n    miou = iou.mean() * 100\n\n    print_info_message(\'Mean IoU: {0:.2f}\'.format(miou))\n    if criterion:\n        return miou, losses.avg\n    else:\n        return miou, 0'"
utilities/utils.py,4,"b'\nimport os\nimport torch\nfrom utilities.print_utils import print_info_message\nimport numpy as np\n\n#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\n\'\'\'\nThis file is mostly adapted from the PyTorch ImageNet example\n\'\'\'\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\'\'\'\nUtility to save checkpoint or not\n\'\'\'\ndef save_checkpoint(state, is_best, dir, extra_info=\'model\', epoch=-1):\n    check_pt_file = dir + os.sep + str(extra_info) + \'_checkpoint.pth.tar\'\n    torch.save(state, check_pt_file)\n    if is_best:\n        #We only need best models weight and not check point states, etc.\n        torch.save(state[\'state_dict\'], dir + os.sep + str(extra_info) + \'_best.pth\')\n    if epoch != -1:\n        torch.save(state[\'state_dict\'], dir + os.sep + str(extra_info) + \'_ep_\' + str(epoch) + \'.pth\')\n\n    print_info_message(\'Checkpoint saved at: {}\'.format(check_pt_file))\n\n\n\'\'\'\nFunction to compute model parameters\n\'\'\'\ndef model_parameters(model):\n    return sum([np.prod(p.size()) for p in model.parameters()])/ 1e6\n\n\'\'\'\nfunction to compute flops\n\'\'\'\ndef compute_flops(model, input=None):\n    from utilities.flops_compute import add_flops_counting_methods\n    input = input if input is not None else torch.Tensor(1, 3, 224, 224)\n    model = add_flops_counting_methods(model)\n    model.eval()\n    model.start_flops_count()\n\n    _ = model(input)\n\n    flops = model.compute_average_flops_cost()  # + (model.classifier.in_features * model.classifier.out_features)\n    flops = flops / 1e6 / 2\n    return flops'"
data_loader/classification/__init__.py,0,b''
data_loader/classification/coco.py,3,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nimport os\nimport torch.utils.data\nimport numpy as np\nfrom PIL import Image\nfrom pycocotools.coco import COCO\nfrom torchvision.transforms import Compose, RandomResizedCrop, RandomHorizontalFlip, ToTensor, Normalize, Resize\nfrom transforms.classification.data_transforms import MEAN, STD\nfrom torch.utils import data\n\nCOCO_CLASS_LIST = [\'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\',\n                   \'train\', \'truck\', \'boat\', \'traffic light\', \'fire\', \'hydrant\',\n                   \'stop sign\', \'parking meter\', \'bench\', \'bird\', \'cat\', \'dog\',\n                   \'horse\', \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\',\n                   \'giraffe\', \'backpack\', \'umbrella\', \'handbag\', \'tie\',\n                   \'suitcase\', \'frisbee\', \'skis\', \'snowboard\', \'sports ball\',\n                   \'kite\', \'baseball bat\', \'baseball glove\', \'skateboard\',\n                   \'surfboard\', \'tennis racket\', \'bottle\', \'wine glass\', \'cup\',\n                   \'fork\', \'knife\', \'spoon\', \'bowl\', \'banana\', \'apple\',\n                   \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hot dog\', \'pizza\',\n                   \'donut\', \'cake\', \'chair\', \'couch\', \'potted plant\', \'bed\',\n                   \'dining table\', \'toilet\', \'tv\', \'laptop\', \'mouse\', \'remote\',\n                   \'keyboard\', \'cell phone\', \'microwave oven\', \'toaster\', \'sink\',\n                   \'refrigerator\', \'book\', \'clock\', \'vase\', \'scissors\',\n                   \'teddy bear\', \'hair drier\', \'toothbrush\'\n                   ]\n\n\nclass COCOClassification(data.Dataset):\n    def __init__(self, root, split=\'train\', year=\'2017\', inp_size=224, scale=(0.2, 1.0), is_training=True):\n        super(COCOClassification, self).__init__()\n        ann_file = os.path.join(root, \'annotations/instances_{}{}.json\'.format(split, year))\n        self.root = root\n        self.img_dir = os.path.join(root, \'images/{}{}\'.format(split, year))\n        self.coco = COCO(ann_file)\n        self.ids = list(self.coco.imgs.keys())\n        self.cat2cat = dict()\n        for cat in self.coco.cats.keys():\n            self.cat2cat[cat] = len(self.cat2cat)\n        self.transform = self.transforms(inp_size=inp_size, inp_scale=scale, is_training=is_training)\n\n\n    def transforms(self, inp_size, inp_scale, is_training):\n\n        if is_training:\n            return Compose(\n                [\n                    RandomResizedCrop(inp_size, scale=inp_scale),\n                    RandomHorizontalFlip(),\n                    ToTensor(),\n                    Normalize(mean=MEAN, std=STD)\n                ]\n            )\n        else:\n            return Compose(\n                [\n                    Resize(size=(inp_size, inp_size)),\n                    ToTensor(),\n                    Normalize(mean=MEAN, std=STD)\n                ]\n            )\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, index):\n        coco = self.coco\n        img_id = self.ids[index]\n        ann_ids = coco.getAnnIds(imgIds=img_id)\n        target = coco.loadAnns(ann_ids)\n\n        output = torch.zeros((3, len(COCO_CLASS_LIST)), dtype=torch.long)\n        for obj in target:\n            if obj[\'area\'] < 32 * 32:\n                output[0][self.cat2cat[obj[\'category_id\']]] = 1\n            elif obj[\'area\'] < 96 * 96:\n                output[1][self.cat2cat[obj[\'category_id\']]] = 1\n            else:\n                output[2][self.cat2cat[obj[\'category_id\']]] = 1\n        target = output\n\n        path = coco.loadImgs(img_id)[0][\'file_name\']\n        img = Image.open(os.path.join(self.img_dir, path)).convert(\'RGB\')\n        if self.transform is not None:\n            img = self.transform(img)\n\n        return img, target\n'"
data_loader/classification/custom_data_loader.py,2,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\nimport torch\nimport torch.utils.data as data\nimport os\nfrom PIL import Image\nfrom torchvision.transforms import Compose, RandomResizedCrop, RandomHorizontalFlip, ToTensor, Normalize, Resize\nimport numpy as np\nfrom transforms.classification.data_transforms import MEAN, STD\n\n\'\'\'\nThis is a (untested) sample file for supporting custom datasets\n\'\'\'\n\n# let us assume that custom dataset has two classes, ""Person"" and ""Background"". Background is everything except persons\nCUSTOM_DATASET_CLASS_LIST = [\'background\', \'person\']\n\n\nclass CustomClassificationDataset(torch.utils.data.Dataset):\n\n    def __init__(self, root, inp_size=224, scale=(0.2, 1.0), is_training=True, debug_labels=True):\n        super(CustomClassificationDataset, self).__init__()\n        # let us assume that you have data directory set-up like this\n        # + vision_datasets\n        # +++++ train.txt // each line in this file contains mapping about RGB image and class <image1.jpg>, Class_id\n        # +++++ val.txt // each line in this file contains mapping about RGB image and class <image1.jpg>, Class_id\n        # +++++ images // This directory contains RGB images\n        # +++++++++ image1.jpg\n        # +++++++++ image2.jpg\n\n        rgb_root_dir = os.path.join(root, \'images\')\n        self.train = is_training\n        if self.train:\n            data_file = os.path.join(root, \'train.txt\')\n        else:\n            data_file = os.path.join(root, \'val.txt\')\n\n        self.images = []\n        self.labels = []\n        with open(data_file, \'r\') as lines:\n            for line in lines:\n                # line is a comma separated file that contains mapping between RGB image and class iD\n                # <image1.jpg>, Class_ID\n                line_split = line.split(\',\') # index 0 contains rgb location and index 1 contains label id\n                rgb_img_loc = rgb_root_dir + os.sep + line_split[0].strip()\n                label_id = int(line_split[1].strip()) #strip to remove spaces\n                assert os.path.isfile(rgb_img_loc)\n                self.images.append(rgb_img_loc)\n\n                if debug_labels:\n                    # let us also check that label index  only contain labels as listed in CUSTOM_DATASET_CLASS_LIST\n                    assert 0 <= label_id < len(CUSTOM_DATASET_CLASS_LIST), ""Label should contain values only between 0 "" \\\n                                                    ""and {}"".format(len(CUSTOM_DATASET_CLASS_LIST) - 1)\n                self.labels.append(label_id)\n\n        self.transform = self.transforms(inp_size=inp_size, inp_scale=scale, is_training=is_training)\n\n    def transforms(self, inp_size, inp_scale, is_training):\n        if is_training:\n            return Compose(\n                [\n                    RandomResizedCrop(inp_size, scale=inp_scale),\n                    RandomHorizontalFlip(),\n                    ToTensor(),\n                    Normalize(mean=MEAN, std=STD)\n                ]\n            )\n        else:\n            return Compose(\n                [\n                    Resize(size=(inp_size, inp_size)),\n                    ToTensor(),\n                    Normalize(mean=MEAN, std=STD)\n                ]\n            )\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        rgb_img = Image.open(self.images[index]).convert(\'RGB\')\n        label_id = self.labels[index]\n\n        if self.transform is not None:\n            rgb_img = self.transform(rgb_img)\n\n        return rgb_img, label_id\n'"
data_loader/classification/imagenet.py,2,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nimport os\nimport torch\nimport torchvision.datasets as datasets\nfrom torchvision import transforms\nfrom transforms.classification.data_transforms import Lighting, normalize\n\n\ndef train_transforms(inp_size, scale):\n    return transforms.Compose([\n        transforms.RandomResizedCrop(inp_size, scale=scale),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n\ndef val_transforms(inp_size):\n    return transforms.Compose([\n        transforms.Resize(int(inp_size / 0.875)),\n        transforms.CenterCrop(inp_size),\n        transforms.ToTensor(),\n        normalize,\n    ])\n\n#helper function for the loading the training data\ndef train_loader(args):\n    traindir = os.path.join(args.data, \'train\')\n    train_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(traindir, train_transforms(args.inpSize, scale=args.scale)),\n        batch_size=args.batch_size, shuffle=True,\n        num_workers=args.workers, pin_memory=True)\n\n    return train_loader\n\n\n#helper function for the loading the validation data\ndef val_loader(args):\n    valdir = os.path.join(args.data, \'val\')\n    val_loader = torch.utils.data.DataLoader(\n        datasets.ImageFolder(valdir, val_transforms(args.inpSize)),\n        batch_size=args.batch_size, shuffle=False,\n        num_workers=args.workers, pin_memory=True)\n\n    return val_loader\n\n# helped function for loading trianing and validation data\ndef data_loaders(args):\n    tr_loader = train_loader(args)\n    vl_loader = val_loader(args)\n    return tr_loader, vl_loader\n\n'"
data_loader/detection/__init__.py,0,b''
data_loader/detection/augmentation.py,0,"b'# from https://github.com/amdegroot/ssd.pytorch\n\nfrom transforms.detection.data_transforms import ConvertFromInts, PhotometricDistort, Expand, RandomSampleCrop, \\\n    RandomFlipping, ToPercentCoords, Resize, Normalize, ToTensor, Compose\n\n\nclass TrainTransform:\n    \'\'\'\n    Transformation for training set\n    \'\'\'\n    def __init__(self, size):\n        """"""\n        Args:\n            size: the size the of final image.\n        """"""\n        self.size = size\n        self.augment = Compose([\n            ConvertFromInts(),\n            PhotometricDistort(),\n            Expand(),\n            RandomSampleCrop(),\n            RandomFlipping(),\n            ToPercentCoords(),\n            Resize(self.size),\n            Normalize(),\n            ToTensor(),\n        ])\n\n    def __call__(self, img, boxes, labels):\n        """"""\n\n        Args:\n            img: the output of cv.imread in RGB layout.\n            boxes: boundding boxes in the form of (x1, y1, x2, y2).\n            labels: labels of boxes.\n        """"""\n        return self.augment(img, boxes, labels)\n\n\nclass ValTransform:\n    \'\'\'\n    Transformation for validation set\n    \'\'\'\n\n    def __init__(self, size):\n        self.transform = Compose([\n            ToPercentCoords(),\n            Resize(size),\n            Normalize(),\n            ToTensor(),\n        ])\n\n    def __call__(self, image, boxes, labels):\n        return self.transform(image, boxes, labels)\n\n\nclass TestTransform:\n    \'\'\'\n    Transformation for test set\n    \'\'\'\n\n    def __init__(self, size):\n        self.transform = Compose([\n            Resize(size),\n            Normalize(),\n            ToTensor()\n        ])\n\n    def __call__(self, image):\n        image, _, _ = self.transform(image)\n        return image\n'"
data_loader/detection/coco.py,2,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nimport os\nimport torch.utils.data\nimport numpy as np\nfrom PIL import Image\nfrom pycocotools.coco import COCO\n\nCOCO_CLASS_LIST = [\'__background__\',\n                   \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\',\n                   \'train\', \'truck\', \'boat\', \'traffic light\', \'fire hydrant\',\n                   \'stop sign\', \'parking meter\', \'bench\', \'bird\', \'cat\', \'dog\',\n                   \'horse\', \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\',\n                   \'giraffe\', \'backpack\', \'umbrella\', \'handbag\', \'tie\',\n                   \'suitcase\', \'frisbee\', \'skis\', \'snowboard\', \'sports ball\',\n               \'kite\', \'baseball bat\', \'baseball glove\', \'skateboard\',\n               \'surfboard\', \'tennis racket\', \'bottle\', \'wine glass\', \'cup\',\n               \'fork\', \'knife\', \'spoon\', \'bowl\', \'banana\', \'apple\',\n               \'sandwich\', \'orange\', \'broccoli\', \'carrot\', \'hot dog\', \'pizza\',\n               \'donut\', \'cake\', \'chair\', \'couch\', \'potted plant\', \'bed\',\n               \'dining table\', \'toilet\', \'tv\', \'laptop\', \'mouse\', \'remote\',\n               \'keyboard\', \'cell phone\', \'microwave\', \'oven\', \'toaster\', \'sink\',\n               \'refrigerator\', \'book\', \'clock\', \'vase\', \'scissors\',\n               \'teddy bear\', \'hair drier\', \'toothbrush\'\n                ]\n\nclass COCOObjectDetection(torch.utils.data.Dataset):\n\n    def __init__(self, root_dir, transform=None, target_transform=None, is_training=True):\n        super(COCOObjectDetection, self).__init__()\n        if is_training:\n            split = \'train\'\n            year = \'2017\'\n        else:\n            split = \'val\'\n            year = \'2017\'\n        ann_file = os.path.join(root_dir, \'annotations/instances_{}{}.json\'.format(split, year))\n        self.coco = COCO(ann_file)\n        self.root_dir = root_dir\n        self.img_dir = os.path.join(root_dir, \'images/{}{}\'.format(split, year))\n        self.transform = transform\n        self.target_transform = target_transform\n        if is_training:\n            #Remove image IDS that don\'t have annotations from training set\n            self.ids = list(self.coco.imgToAnns.keys())\n        else:\n            # use all images from Validation Set\n            self.ids = list(self.coco.imgs.keys())\n        coco_categories = sorted(self.coco.getCatIds())\n        self.coco_id_to_contiguous_id = {coco_id: i + 1 for i, coco_id in enumerate(coco_categories)}\n        self.contiguous_id_to_coco_id = {v: k for k, v in self.coco_id_to_contiguous_id.items()}\n        self.CLASSES = COCO_CLASS_LIST\n\n    def __getitem__(self, index):\n        img_id = self.ids[index]\n        boxes, labels = self._get_annotation(img_id)\n        image = self._get_image(img_id)\n        if self.transform:\n            image, boxes, labels = self.transform(image, boxes, labels)\n        if self.target_transform:\n            boxes, labels = self.target_transform(boxes, labels)\n        return image, boxes, labels\n\n    def get_image(self, index):\n        image_id = self.ids[index]\n        image = self._get_image(image_id)\n        if self.transform:\n            image, _ = self.transform(image)\n        return image\n\n    def get_annotation(self, index):\n        image_id = self.ids[index]\n        return image_id, self._get_annotation(image_id)\n\n    def __len__(self):\n        return len(self.ids)\n\n    def _get_annotation(self, image_id):\n        ann_ids = self.coco.getAnnIds(imgIds=image_id)\n        ann = self.coco.loadAnns(ann_ids)\n        # filter crowd annotations\n        ann = [obj for obj in ann if obj[""iscrowd""] == 0]\n        boxes = np.array([self._xywh2xyxy(obj[""bbox""]) for obj in ann], np.float32).reshape((-1, 4))\n        labels = np.array([self.coco_id_to_contiguous_id[obj[""category_id""]] for obj in ann], np.int64).reshape((-1,))\n        # remove invalid boxes\n        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n        boxes = boxes[keep]\n        labels = labels[keep]\n        return boxes, labels\n\n    def _xywh2xyxy(self, box):\n        x1, y1, w, h = box\n        return [x1, y1, x1 + w, y1 + h]\n\n    def _get_image(self, image_id):\n        file_name = self.coco.loadImgs(image_id)[0][\'file_name\']\n        image_file = os.path.join(self.img_dir, file_name)\n        image = Image.open(image_file).convert(""RGB"")\n        image = np.array(image)\n        return image\n'"
data_loader/detection/voc.py,1,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nimport os\nfrom torch.utils import data\nimport numpy as np\nimport xml.etree.ElementTree as ET\nfrom PIL import Image\n\nVOC_CLASS_LIST = [\'__background__\',\n                   \'aeroplane\', \'bicycle\', \'bird\', \'boat\',\n                   \'bottle\', \'bus\', \'car\', \'cat\', \'chair\',\n                   \'cow\', \'diningtable\', \'dog\', \'horse\',\n                   \'motorbike\', \'person\', \'pottedplant\',\n                   \'sheep\', \'sofa\', \'train\', \'tvmonitor\']\n\nclass VOCDataset(data.Dataset):\n\n    def __init__(self, root_dir, split, transform=None, target_transform=None, keep_difficult=False, is_training=True):\n        \'\'\'\n        Dataset for PASCAL VOC\n        :param root_dir: the root of the VOCdevkit\n        :param split: which split (VOC2007 or VOC2012)\n        :param transform: Image transforms\n        :param target_transform: Box transforms\n        :param keep_difficult: Keep difficult or not\n        :param is_training: True if Training else False\n        \'\'\'\n        super(VOCDataset, self).__init__()\n        if is_training:\n            search_paths = os.path.join(root_dir, split, ""ImageSets"", ""Main"", ""trainval.txt"")\n        else:\n            search_paths = os.path.join(root_dir, split, ""ImageSets"", ""Main"", ""test.txt"")\n\n        self.ids = VOCDataset._read_image_ids(search_paths)\n\n        self.data_dir = root_dir\n        self.split = split\n        self.transform = transform\n        self.target_transform = target_transform\n        self.keep_difficult = keep_difficult\n        self.CLASSES = VOC_CLASS_LIST\n\n        self.class_dict = {class_name: i for i, class_name in enumerate(self.CLASSES)}\n\n    def __getitem__(self, index):\n        image_id = self.ids[index]\n        boxes, labels, is_difficult = self._get_annotation(image_id)\n        if not self.keep_difficult:\n            boxes = boxes[is_difficult == 0]\n            labels = labels[is_difficult == 0]\n        image = self._read_image(image_id)\n        if self.transform:\n            image, boxes, labels = self.transform(image, boxes, labels)\n        if self.target_transform:\n            boxes, labels = self.target_transform(boxes, labels)\n        return image, boxes, labels\n\n    def get_image(self, index):\n        image_id = self.ids[index]\n        image = self._read_image(image_id)\n        if self.transform:\n            image, _ = self.transform(image)\n        return image\n\n    def get_annotation(self, index):\n        image_id = self.ids[index]\n        return image_id, self._get_annotation(image_id)\n\n    def __len__(self):\n        return len(self.ids)\n\n    @staticmethod\n    def _read_image_ids(image_sets_file):\n        ids = []\n        with open(image_sets_file) as f:\n            for line in f:\n                ids.append(line.rstrip())\n        return ids\n\n    def _get_annotation(self, image_id):\n        annotation_file = os.path.join(self.data_dir, self.split, ""Annotations"", ""%s.xml"" % image_id)\n        objects = ET.parse(annotation_file).findall(""object"")\n        boxes = []\n        labels = []\n        is_difficult = []\n        for obj in objects:\n            class_name = obj.find(\'name\').text.lower().strip()\n            bbox = obj.find(\'bndbox\')\n            # VOC dataset format follows Matlab, in which indexes start from 0\n            x1 = float(bbox.find(\'xmin\').text) - 1\n            y1 = float(bbox.find(\'ymin\').text) - 1\n            x2 = float(bbox.find(\'xmax\').text) - 1\n            y2 = float(bbox.find(\'ymax\').text) - 1\n            boxes.append([x1, y1, x2, y2])\n            labels.append(self.class_dict[class_name])\n            is_difficult_str = obj.find(\'difficult\').text\n            is_difficult.append(int(is_difficult_str) if is_difficult_str else 0)\n\n        return (np.array(boxes, dtype=np.float32),\n                np.array(labels, dtype=np.int64),\n                np.array(is_difficult, dtype=np.uint8))\n\n    def _read_image(self, image_id):\n        image_file = os.path.join(self.data_dir, self.split, ""JPEGImages"", ""%s.jpg"" % image_id)\n        image = Image.open(image_file).convert(""RGB"")\n        image = np.array(image)\n        return image\n'"
data_loader/segmentation/__init__.py,0,b''
data_loader/segmentation/cityscapes.py,1,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\nimport torch\nimport torch.utils.data as data\nimport os\nfrom PIL import Image\nfrom transforms.segmentation.data_transforms import RandomFlip, RandomCrop, RandomScale, Normalize, Resize, Compose\n\nCITYSCAPE_CLASS_LIST = [\'road\', \'sidewalk\', \'building\', \'wall\', \'fence\', \'pole\', \'traffic light\', \'traffic sign\',\n                        \'vegetation\', \'terrain\', \'sky\', \'person\', \'rider\', \'car\', \'truck\', \'bus\', \'train\', \'motorcycle\',\n                        \'bicycle\', \'background\']\n\n\nclass CityscapesSegmentation(data.Dataset):\n\n    def __init__(self, root, train=True, scale=(0.5, 2.0), size=(1024, 512), ignore_idx=255, coarse=True):\n\n        self.train = train\n        if self.train:\n            data_file = os.path.join(root, \'train.txt\')\n            if coarse:\n                coarse_data_file = os.path.join(root, \'train_coarse.txt\')\n        else:\n            data_file = os.path.join(root, \'val.txt\')\n\n        self.images = []\n        self.masks = []\n        with open(data_file, \'r\') as lines:\n            for line in lines:\n                line_split = line.split(\',\')\n                rgb_img_loc = root + os.sep + line_split[0].rstrip()\n                label_img_loc = root + os.sep + line_split[1].rstrip()\n                assert os.path.isfile(rgb_img_loc)\n                assert os.path.isfile(label_img_loc)\n                self.images.append(rgb_img_loc)\n                self.masks.append(label_img_loc)\n\n        # if you want to use Coarse data for training\n        if train and coarse and os.path.isfile(coarse_data_file):\n            with open(coarse_data_file, \'r\') as lines:\n                for line in lines:\n                    line_split = line.split(\',\')\n                    rgb_img_loc = root + os.sep + line_split[0].rstrip()\n                    label_img_loc = root + os.sep + line_split[1].rstrip()\n                    assert os.path.isfile(rgb_img_loc)\n                    assert os.path.isfile(label_img_loc)\n                    self.images.append(rgb_img_loc)\n                    self.masks.append(label_img_loc)\n\n        if isinstance(size, tuple):\n            self.size = size\n        else:\n            self.size = (size, size)\n\n        if isinstance(scale, tuple):\n            self.scale = scale\n        else:\n            self.scale = (scale, scale)\n\n        self.train_transforms, self.val_transforms = self.transforms()\n        self.ignore_idx = ignore_idx\n\n    def transforms(self):\n        train_transforms = Compose(\n            [\n                RandomScale(scale=self.scale),\n                RandomCrop(crop_size=self.size),\n                RandomFlip(),\n                Normalize()\n            ]\n        )\n        val_transforms = Compose(\n            [\n                Resize(size=self.size),\n                Normalize()\n            ]\n        )\n        return train_transforms, val_transforms\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        rgb_img = Image.open(self.images[index]).convert(\'RGB\')\n        label_img = Image.open(self.masks[index])\n\n        if self.train:\n            rgb_img, label_img = self.train_transforms(rgb_img, label_img)\n        else:\n            rgb_img, label_img = self.val_transforms(rgb_img, label_img)\n\n        return rgb_img, label_img\n'"
data_loader/segmentation/coco.py,1,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nfrom pycocotools.coco import COCO\nfrom pycocotools import mask\nfrom tqdm import tqdm\nimport numpy as np\nimport torch\nimport os\nfrom PIL import Image\nimport numba as numba\nfrom numba import prange\nfrom torch.utils import data\n\n\nclass COCOSegmentation(data.Dataset):\n    # these are the same as the PASCAL VOC dataset\n    CAT_LIST = [0, 5, 2, 16, 9, 44, 6, 3, 17, 62, 21, 67, 18, 19, 4, 1, 64, 20, 63, 7, 72]\n\n    def __init__(self, root_dir, split=\'train\', year=\'2017\'):\n        super(COCOSegmentation, self).__init__()\n        ann_file = os.path.join(root_dir, \'annotations/instances_{}{}.json\'.format(split, year))\n        self.img_dir = os.path.join(root_dir, \'images/{}{}\'.format(split, year))\n        self.split = split\n        self.coco = COCO(ann_file)\n        self.coco_mask = mask\n        self.ids = list(self.coco.imgs.keys())\n\n    def generate_img_mask_pair(self, index):\n        coco = self.coco\n        img_id = self.ids[index]\n        img_metadata = coco.loadImgs(img_id)[0]\n        path = img_metadata[\'file_name\']\n\n        if img_metadata[\'height\'] < 256 or img_metadata[\'width\'] < 256:\n            return None, None, None\n\n        try:\n            _img = Image.open(os.path.join(self.img_dir, path)).convert(\'RGB\')\n            cocotarget = coco.loadAnns(coco.getAnnIds(imgIds=img_id))\n\n            _target = Image.fromarray(self._gen_seg_mask(cocotarget, img_metadata[\'height\'], img_metadata[\'width\']))\n\n            len_uniq_vals = len(np.unique(_target))\n            if len_uniq_vals < 2:\n                return None, None, None\n\n            return path, _img, _target\n        except OSError:\n            return None, None, None\n\n    def _gen_seg_mask(self, target, h, w):\n        mask = np.zeros((h, w), dtype=np.uint8)\n        coco_mask = self.coco_mask\n        for instance in target:\n            rle = coco_mask.frPyObjects(instance[\'segmentation\'], h, w)\n            m = coco_mask.decode(rle)\n            cat = instance[\'category_id\']\n            if cat in self.CAT_LIST:\n                c = self.CAT_LIST.index(cat)\n            else:\n                continue\n            if len(m.shape) < 3:\n                mask[:, :] += (mask == 0) * (m * c)\n            else:\n                mask[:, :] += (mask == 0) * (((np.sum(m, axis=2)) > 0) * c).astype(np.uint8)\n        return mask\n\n\n@numba.jit(target=\'cpu\')\ndef generate_pairs(coco, rgb_dir_name, mask_dir_name, save_dir_rgb, save_dir_mask):\n    lines = []\n    for i in tqdm(prange(len(coco.ids))):\n        fname, img, mask = coco.generate_img_mask_pair(i)\n        if fname is None:\n            continue\n        img.save(os.path.join(save_dir_rgb, fname))\n\n        mask_fname = fname.split(\'.\')[0] + \'.png\'\n        mask.save(os.path.join(save_dir_mask, mask_fname))\n        loc_pair = \'{}/{} {}/{}\\n\'.format(rgb_dir_name, fname, mask_dir_name, mask_fname)\n        lines.append(loc_pair)\n\n    return lines\n\n\nif __name__ == ""__main__"":\n    root_dir = \'../../../vision_datasets/coco/\'\n    save_root_dir = \'../../../vision_datasets/coco_preprocess\'\n    rgb_dir_name = \'rgb_train\'\n    mask_dir_name = \'annot_train\'\n\n    # process the training split\n    split = \'train\'\n    year = \'2017\'\n\n    save_dir_rgb = save_root_dir + os.sep + rgb_dir_name\n    save_dir_mask = save_root_dir + os.sep + mask_dir_name\n    if not os.path.isdir(save_dir_rgb):\n        os.makedirs(save_dir_rgb)\n\n    if not os.path.isdir(save_dir_mask):\n        os.makedirs(save_dir_mask)\n\n    coco = COCOSegmentation(root_dir, split=split, year=year)\n\n    lines = generate_pairs(coco, rgb_dir_name=rgb_dir_name, mask_dir_name=mask_dir_name,\n                           save_dir_rgb=save_dir_rgb, save_dir_mask=save_dir_mask)\n\n    with open(save_root_dir + os.sep + \'{}_{}.txt\'.format(split, year), \'w\') as txt_file:\n        for line in lines:\n            txt_file.write(line)\n'"
data_loader/segmentation/custom_dataset_loader.py,2,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\nimport torch\nimport torch.utils.data as data\nimport os\nfrom PIL import Image\nfrom transforms.segmentation.data_transforms import RandomFlip, RandomCrop, RandomScale, Normalize, Resize, Compose\nimport numpy as np\n\n\'\'\'\nThis is a (untested) sample file for supporting custom datasets\n\'\'\'\n\n# let us assume that custom dataset has two classes, ""Person"" and ""Background"". Background is everything except persons\nCUSTOM_DATASET_CLASS_LIST = [\'background\', \'person\']\n\n\nclass CustomSegmentationDataset(torch.utils.data.Dataset):\n\n    def __init__(self, root, train=True, scale=(0.5, 1.0), crop_size=(513, 513), ignore_idx=255, debug_labels=True):\n        super(CustomSegmentationDataset, self).__init__()\n        # let us assume that you have data directory set-up like this\n        # + vision_datasets\n        # +++++ train.txt // each line in this file contains mapping about RGB image and mask image <image1.jpg>, <image1.png>\n        # +++++ val.txt // each line in this file contains mapping about RGB image and mask image <image1.jpg>, <image1.png>\n        # +++++ images // This directory contains RGB images\n        # +++++++++ image1.jpg\n        # +++++++++ image2.jpg\n        # +++++ annotations // This directory contains segmentation mask images\n        # +++++++++ image1.png\n        # +++++++++ image2.png\n\n        rgb_root_dir = os.path.join(root, \'images\')\n        label_root_dir = os.path.join(root, \'annotations\')\n        self.train = train\n        if self.train:\n            data_file = os.path.join(root, \'train.txt\')\n        else:\n            data_file = os.path.join(root, \'val.txt\')\n\n        self.images = []\n        self.masks = []\n        with open(data_file, \'r\') as lines:\n            for line in lines:\n                # line is a comma separated file that contains mapping between RGB image and mask image\n                # <image1.jpg>, <image1.png>\n                line_split = line.split(\',\')\n                rgb_img_loc = rgb_root_dir + os.sep + line_split[0].strip()\n                label_img_loc = label_root_dir + os.sep + line_split[1].strip()\n                assert os.path.isfile(rgb_img_loc)\n                assert os.path.isfile(label_img_loc)\n                self.images.append(rgb_img_loc)\n\n                if debug_labels:\n                    # let us also check that label file only contain labels as listed in CUSTOM_DATASET_CLASS_LIST\n                    label_img = Image.open(label_img_loc)\n                    label_img = np.asarray(label_img)\n                    unique_values = np.unique(label_img)\n                    min_val = np.min(unique_values)\n                    max_val = np.max(unique_values)\n                    assert 0 <= min_val <= max_val < len(\n                        CUSTOM_DATASET_CLASS_LIST), ""Label image at {} has following values. "" \\\n                                                    ""However, it should contain values only between 0 "" \\\n                                                    ""and {}"".format(unique_values.tolist(),\n                                                                    len(CUSTOM_DATASET_CLASS_LIST) - 1)\n                self.masks.append(label_img_loc)\n\n        if isinstance(crop_size, tuple):\n            self.crop_size = crop_size\n        else:\n            self.crop_size = (crop_size, crop_size)\n\n        if isinstance(scale, tuple):\n            self.scale = scale\n        else:\n            self.scale = (scale, scale)\n\n        self.train_transforms, self.val_transforms = self.transforms()\n        self.ignore_idx = ignore_idx\n\n    def transforms(self):\n        train_transforms = Compose(\n            [\n                RandomFlip(),\n                RandomScale(scale=self.scale),\n                RandomCrop(crop_size=self.crop_size),\n                Normalize()\n            ]\n        )\n        val_transforms = Compose(\n            [\n                Resize(size=self.crop_size),\n                Normalize()\n            ]\n        )\n        return train_transforms, val_transforms\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        rgb_img = Image.open(self.images[index]).convert(\'RGB\')\n        label_img = Image.open(self.masks[index])\n\n        if self.train:\n            rgb_img, label_img = self.train_transforms(rgb_img, label_img)\n        else:\n            rgb_img, label_img = self.val_transforms(rgb_img, label_img)\n\n        return rgb_img, label_img\n'"
data_loader/segmentation/voc.py,2,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\nimport torch\nimport torch.utils.data as data\nimport os\nfrom PIL import Image\nfrom transforms.segmentation.data_transforms import RandomFlip, RandomCrop, RandomScale, Normalize, Resize, Compose\n\n\nVOC_CLASS_LIST = [\'background\', \'aeroplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\', \'bus\',\n                   \'car\', \'cat\', \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\',\n                   \'motorbike\', \'person\', \'potted-plant\', \'sheep\', \'sofa\', \'train\',\n                   \'tv/monitor\']\n\nclass VOCSegmentation(torch.utils.data.Dataset):\n\n    def __init__(self, root, train=True, scale=(0.5, 1.0), crop_size=(513, 513), ignore_idx=255, coco_root_dir=\'\'):\n        super(VOCSegmentation, self).__init__()\n        voc_root_dir = os.path.join(root, \'VOC2012\')\n        voc_list_dir = os.path.join(voc_root_dir, \'list\')\n\n        self.train = train\n        if self.train:\n            data_file = os.path.join(voc_list_dir, \'train_aug.txt\')\n            if coco_root_dir:\n                data_file_coco = os.path.join(coco_root_dir, \'train_2017.txt\')\n        else:\n            data_file = os.path.join(voc_list_dir, \'val.txt\')\n\n        self.images = []\n        self.masks = []\n        with open(data_file, \'r\') as lines:\n            for line in lines:\n                rgb_img_loc = voc_root_dir + os.sep  + line.split()[0]\n                label_img_loc = voc_root_dir + os.sep + line.split()[1]\n                assert os.path.isfile(rgb_img_loc)\n                assert os.path.isfile(label_img_loc)\n                self.images.append(rgb_img_loc)\n                self.masks.append(label_img_loc)\n\n        if self.train and coco_root_dir:\n            with open(data_file_coco, \'r\') as lines:\n                for line in lines:\n                    rgb_img_loc = coco_root_dir + os.sep + line.split()[0]\n                    label_img_loc = coco_root_dir + os.sep + line.split()[1]\n                    assert os.path.isfile(rgb_img_loc)\n                    assert os.path.isfile(label_img_loc)\n                    self.images.append(rgb_img_loc)\n                    self.masks.append(label_img_loc)\n\n        if isinstance(crop_size, tuple):\n            self.crop_size = crop_size\n        else:\n            self.crop_size = (crop_size, crop_size)\n\n        if isinstance(scale, tuple):\n            self.scale = scale\n        else:\n            self.scale = (scale, scale)\n\n        self.train_transforms, self.val_transforms = self.transforms()\n        self.ignore_idx = ignore_idx\n\n    def transforms(self):\n        train_transforms = Compose(\n            [\n                RandomFlip(),\n                RandomScale(scale=self.scale),\n                RandomCrop(crop_size=self.crop_size),\n                Normalize()\n            ]\n        )\n        val_transforms = Compose(\n            [\n                Resize(size=self.crop_size),\n                Normalize()\n            ]\n        )\n        return train_transforms, val_transforms\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, index):\n        rgb_img = Image.open(self.images[index]).convert(\'RGB\')\n        label_img = Image.open(self.masks[index])\n\n        if self.train:\n            rgb_img, label_img = self.train_transforms(rgb_img, label_img)\n        else:\n            rgb_img, label_img = self.val_transforms(rgb_img, label_img)\n\n        return rgb_img, label_img\n\n\n\n'"
model/classification/__init__.py,0,b''
model/classification/dicenet.py,3,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nimport torch\nfrom torch.nn import init\nfrom torch import nn\nfrom nn_layers.cnn_utils import CBR, Shuffle\nfrom nn_layers.dice import DICE, StridedDICE\nfrom model.classification import dicenet_config as config_all\nfrom utilities.print_utils import *\n\n\nclass ShuffleDICEBlock(nn.Module):\n    def __init__(self, inplanes, outplanes, height, width, c_tag=0.5, groups=2):\n        super(ShuffleDICEBlock, self).__init__()\n        self.left_part = round(c_tag * inplanes)\n        self.right_part_in = inplanes - self.left_part\n        self.right_part_out = outplanes - self.left_part\n\n        self.layer_right = nn.Sequential(\n            CBR(self.right_part_in, self.right_part_out, 1, 1),\n            DICE(channel_in=self.right_part_out, channel_out=self.right_part_out, height=height, width=width)\n        )\n\n        self.inplanes = inplanes\n        self.outplanes = outplanes\n        self.groups = groups\n        self.shuffle = Shuffle(groups=2)\n\n    def forward(self, x):\n        left = x[:, :self.left_part, :, :]\n        right = x[:, self.left_part:, :, :]\n\n        right = self.layer_right(right)\n\n        return self.shuffle(torch.cat((left, right), 1))\n\n    def __repr__(self):\n        s = \'{name}(in_channels={inplanes}, out_channels={outplanes})\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass CNNModel(nn.Module):\n    def __init__(self, args):\n        super(CNNModel, self).__init__()\n\n        # ====================\n        # Network configuraiton\n        # ====================\n        try:\n            num_classes = args.num_classes\n        except:\n            # if not specified, default to 1000 for imageNet\n            num_classes = 1000 # 1000 for imagenet\n\n        try:\n            channels_in = args.channels\n        except:\n            # if not specified, default to RGB (3)\n            channels_in = 3\n\n        width = args.model_width\n        height = args.model_height\n        s = args.s\n        if not s in config_all.sc_ch_dict.keys():\n            print_error_message(\'Model at scale s={} is not suppoerted yet\'.format(s))\n            exit(-1)\n\n        out_channel_map = config_all.sc_ch_dict[s]\n        reps_at_each_level = config_all.rep_layers\n\n        assert width % 32 == 0, \'Input image width should be divisible by 32\'\n        assert height % 32 == 0, \'Input image height should be divisible by 32\'\n\n        # ====================\n        # Network architecture\n        # ====================\n\n        # output size will be 112 x 112\n        width = int(width / 2)\n        height = int(height / 2)\n        self.level1 = CBR(channels_in, out_channel_map[0], 3, 2)\n        width = int(width / 2)\n        height = int(height / 2)\n        self.level2 = nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n\n        # output size will be 28 x 28\n        width = int(width / 2)\n        height = int(height / 2)\n        level3 = nn.ModuleList()\n        level3.append(StridedDICE(channel_in=out_channel_map[1], height=height, width=width))\n        for i in range(reps_at_each_level[1]):\n            if i == 0:\n                level3.append(ShuffleDICEBlock(2 * out_channel_map[1], out_channel_map[2], width=width, height=height))\n            else:\n                level3.append(ShuffleDICEBlock(out_channel_map[2], out_channel_map[2], width=width, height=height))\n        self.level3 = nn.Sequential(*level3)\n\n        # output size will be 14 x 14\n        level4 = nn.ModuleList()\n        width = int(width / 2)\n        height = int(height / 2)\n        level4.append(StridedDICE(channel_in=out_channel_map[2], width=width, height=height))\n        for i in range(reps_at_each_level[2]):\n            if i == 0:\n                level4.append(ShuffleDICEBlock(2 * out_channel_map[2], out_channel_map[3], width=width, height=height))\n            else:\n                level4.append(ShuffleDICEBlock(out_channel_map[3], out_channel_map[3], width=width, height=height))\n        self.level4 = nn.Sequential(*level4)\n\n        # output size will be 7 x 7\n        level5 = nn.ModuleList()\n        width = int(width / 2)\n        height = int(height / 2)\n        level5.append(StridedDICE(channel_in=out_channel_map[3], width=width, height=height))\n        for i in range(reps_at_each_level[3]):\n            if i == 0:\n                level5.append(ShuffleDICEBlock(2 * out_channel_map[3], out_channel_map[4], width=width, height=height))\n            else:\n                level5.append(ShuffleDICEBlock(out_channel_map[4], out_channel_map[4], width=width, height=height))\n        self.level5 = nn.Sequential(*level5)\n\n        # classification layer\n        if s > 1:\n            self.drop_layer = nn.Dropout(p=0.2)\n        else:\n            self.drop_layer = nn.Dropout(p=0.1)\n\n        # We use four groups in Grouped linear transformation\n        # introduced in Pyramidal Recurrent Unit for Language Modeling\n        # https://arxiv.org/abs/1808.09029\n        groups = 4\n\n        self.classifier = nn.Sequential(\n            nn.Conv2d(out_channel_map[4], out_channel_map[5], kernel_size=1, groups=groups, bias=False),\n            self.drop_layer,\n            nn.Conv2d(out_channel_map[5], num_classes, 1, padding=0, bias=True)\n        )\n\n        self.global_pool = nn.AdaptiveAvgPool2d(output_size=1)\n\n        self.out_channel_map = out_channel_map\n\n        self.init_params()\n\n    def init_params(self):\n        \'\'\'\n        Function to initialze the parameters\n        \'\'\'\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal_(m.weight, std=0.001)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        \'\'\'\n        :param x: Receives the input RGB image\n        :return: a C-dimensional vector, C=# of classes\n        \'\'\'\n        x = self.level1(x)  # 112\n        x = self.level2(x)  # 56\n        x = self.level3(x) # 28\n        x = self.level4(x) # 14\n        x = self.level5(x) # 7\n        x = self.global_pool(x)\n        x = self.classifier(x)\n        x = x.view(x.size(0), -1)\n        return x\n\n\nif __name__ == \'__main__\':\n\n    from utilities.utils import compute_flops, model_parameters\n    import torch\n    import argparse\n    parser = argparse.ArgumentParser(description=\'Testing\')\n    args = parser.parse_args()\n\n    for scale in config_all.sc_ch_dict.keys():\n        for size in [224]:\n            #args.num_classes = 1000\n            imSz = size\n            args.s = scale\n            args.channels = 3\n            args.model_width = 224\n            args.model_height = 224\n\n            model = CNNModel(args)\n            input = torch.randn(1, 3, size, size)\n            print_info_message(\'Scale: {}, ImSize: {}\'.format(scale, size))\n            print_info_message(\'Flops: {:.2f} million\'.format(compute_flops(model, input)))\n            print_info_message(\'Params: {:.2f} million\'.format(model_parameters(model)))\n            print(\'\\n\')\n            #exit()\n'"
model/classification/dicenet_config.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nsc_ch_dict = {\n            #0.1 : [8, 8, 16, 32, 64, 512],\n            0.2: [16, 16, 32, 64, 128, 1024],\n            0.5: [24, 24, 48, 96, 192, 1024],\n            0.75: [24, 24, 86, 172, 344, 1024],\n            1.0: [24, 24, 116, 232, 464, 1024],\n            1.25: [24, 24, 144, 288, 576, 1024],\n            1.5: [24, 24, 176, 352, 704, 1024],\n            1.75: [24, 24, 210, 420, 840, 1024],\n            2.0: [24, 24, 244, 488, 976, 1024],\n            2.4: [24, 24, 278, 556, 1112, 1280],\n            #3.0: [48, 48, 384, 768, 1536, 2048]\n        }\n\nrep_layers = [0, 3, 7, 3]\n\n\n#RESULTS\n################################################################################\n#   s   #   FLOPs   #   top-1   #  Scale Aug #  Params   # Closest and best SOTA (design)\n################################################################################\n#  0.2  #   14 M    #   46.19   # (0.20-1.0) #   1.13 M  # 45.5 (MobileNetv2 with 12 MFLOPs and 1.7M Params)\n#  0.5  #   24 M    #   54.04   # (0.20-1.0) #   1.21 M  #\n# 0.75  #   46 M    #   62.80   # (0.20-1.0) #   1.50 M  # 60.3 (ShuffleNetv2 with 41 MFLOPs)\n# 1.00  #   70 M    #   66.45   # (0.20-1.0) #   1.81 M  # 63.9 (MobileNetv2 with 71 MFLOPs and 2.0 M Params) Note: 65.9 (FBNEt with 73 MFLOPs and 2.6 M Params)\n# 1.25  #   98 M    #   67.77   # (0.08-1.0) #   2.16 M  # 65.3 (MobileNetv2 with 99 MFLOPs and 3.5 M Params) Note: 67.0 (FBNEt with 92 MFLOPs and 4.2 M Params)\n# 1.50  #  135 M    #   69.51   # (0.08-1.0) #   2.65 M  # 69.4 (ShuffleNetv2 with 146 MFLOPs)\n# 1.50  #  182 M    #   70.26   # (0.08-1.0) #   3.26 M  #\n# 2.00  #  236 M    #   71.00   # (0.08-1.0) #   3.98 M  # 70.7 (MobileNetv2 with 221 MFLOPs and 3.5 M Params)\n\n\n\n\n'"
model/classification/espnetv2.py,3,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nimport torch\nfrom torch import nn\nfrom nn_layers.eesp import DownSampler, EESP\nfrom nn_layers.espnet_utils import CBR\nfrom torch.nn import init\nimport torch.nn.functional as F\nfrom model.classification import espnetv2_config as config_all\nfrom utilities.print_utils import *\n\nclass EESPNet(nn.Module):\n    \'\'\'\n    This class defines the ESPNetv2 architecture for the ImageNet classification\n    \'\'\'\n\n    def __init__(self, args):\n        \'\'\'\n        :param classes: number of classes in the dataset. Default is 1000 for the ImageNet dataset\n        :param s: factor that scales the number of output feature maps\n        \'\'\'\n        super().__init__()\n\n        # ====================\n        # Network configuraiton\n        # ====================\n        try:\n            num_classes = args.num_classes\n        except:\n            # if not specified, default to 1000 for imageNet\n            num_classes = 1000  # 1000 for imagenet\n\n        try:\n            channels_in = args.channels\n        except:\n            # if not specified, default to RGB (3)\n            channels_in = 3\n\n        s = args.s\n        if not s in config_all.sc_ch_dict.keys():\n            print_error_message(\'Model at scale s={} is not suppoerted yet\'.format(s))\n            exit(-1)\n\n        out_channel_map = config_all.sc_ch_dict[args.s]\n        reps_at_each_level = config_all.rep_layers\n\n        recept_limit = config_all.recept_limit  # receptive field at each spatial level\n        K = [config_all.branches]*len(recept_limit) # No. of parallel branches at different level\n\n        # True for the shortcut connection with input\n        self.input_reinforcement = config_all.input_reinforcement\n\n        assert len(K) == len(recept_limit), \'Length of branching factor array and receptive field array should be the same.\'\n\n        self.level1 = CBR(channels_in, out_channel_map[0], 3, 2)  # 112 L1\n\n        self.level2_0 = DownSampler(out_channel_map[0], out_channel_map[1], k=K[0], r_lim=recept_limit[0], reinf=self.input_reinforcement)  # out = 56\n\n        self.level3_0 = DownSampler(out_channel_map[1], out_channel_map[2], k=K[1], r_lim=recept_limit[1], reinf=self.input_reinforcement) # out = 28\n        self.level3 = nn.ModuleList()\n        for i in range(reps_at_each_level[1]):\n            self.level3.append(EESP(out_channel_map[2], out_channel_map[2], stride=1, k=K[2], r_lim=recept_limit[2]))\n\n        self.level4_0 = DownSampler(out_channel_map[2], out_channel_map[3], k=K[2], r_lim=recept_limit[2], reinf=self.input_reinforcement) #out = 14\n        self.level4 = nn.ModuleList()\n        for i in range(reps_at_each_level[2]):\n            self.level4.append(EESP(out_channel_map[3], out_channel_map[3], stride=1, k=K[3], r_lim=recept_limit[3]))\n\n        self.level5_0 = DownSampler(out_channel_map[3], out_channel_map[4], k=K[3], r_lim=recept_limit[3]) #7\n        self.level5 = nn.ModuleList()\n        for i in range(reps_at_each_level[3]):\n            self.level5.append(EESP(out_channel_map[4], out_channel_map[4], stride=1, k=K[4], r_lim=recept_limit[4]))\n\n        # expand the feature maps using depth-wise convolution followed by group point-wise convolution\n        self.level5.append(CBR(out_channel_map[4], out_channel_map[4], 3, 1, groups=out_channel_map[4]))\n        self.level5.append(CBR(out_channel_map[4], out_channel_map[5], 1, 1, groups=K[4]))\n\n        self.classifier = nn.Linear(out_channel_map[5], num_classes)\n        self.config = out_channel_map\n        self.init_params()\n\n    def init_params(self):\n        \'\'\'\n        Function to initialze the parameters\n        \'\'\'\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal_(m.weight, std=0.001)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n    def forward(self, input, p=0.2):\n        \'\'\'\n        :param input: Receives the input RGB image\n        :return: a C-dimensional vector, C=# of classes\n        \'\'\'\n        out_l1 = self.level1(input)  # 112\n        if not self.input_reinforcement:\n            del input\n            input = None\n\n        out_l2 = self.level2_0(out_l1, input)  # 56\n\n        out_l3_0 = self.level3_0(out_l2, input)  # down-sample\n        for i, layer in enumerate(self.level3):\n            if i == 0:\n                out_l3 = layer(out_l3_0)\n            else:\n                out_l3 = layer(out_l3)\n\n        out_l4_0 = self.level4_0(out_l3, input)  # down-sample\n        for i, layer in enumerate(self.level4):\n            if i == 0:\n                out_l4 = layer(out_l4_0)\n            else:\n                out_l4 = layer(out_l4)\n\n        out_l5_0 = self.level5_0(out_l4)  # down-sample\n        for i, layer in enumerate(self.level5):\n            if i == 0:\n                out_l5 = layer(out_l5_0)\n            else:\n                out_l5 = layer(out_l5)\n\n        output_g = F.adaptive_avg_pool2d(out_l5, output_size=1)\n        output_g = F.dropout(output_g, p=p, training=self.training)\n        output_1x1 = output_g.view(output_g.size(0), -1)\n\n        return self.classifier(output_1x1)\n\n\nif __name__ == \'__main__\':\n    from utilities.utils import compute_flops, model_parameters\n    import torch\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\'Testing\')\n    args = parser.parse_args()\n\n    for scale in config_all.sc_ch_dict.keys():\n        for size in [224]:\n            # args.num_classes = 1000\n            imSz = size\n            args.s = scale\n            args.channels = 3\n\n            model = EESPNet(args)\n            input = torch.randn(1, 3, size, size)\n            print_info_message(\'Scale: {}, ImSize: {}x{}\'.format(scale, size, size))\n            print_info_message(\'Flops: {:.2f} million\'.format(compute_flops(model, input)))\n            print_info_message(\'Params: {:.2f} million\'.format(model_parameters(model)))\n            print(\'\\n\')\n'"
model/classification/espnetv2_config.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nsc_ch_dict = {\n            0.5: [16, 32, 64, 128, 256, 1024],\n            1.0: [32, 64, 128, 256, 512, 1024],\n            1.25: [32, 80, 160, 320, 640, 1024],\n            1.5: [32, 96, 192, 384, 768, 1024],\n            2.0: [32, 128, 256, 512, 1024, 1280]\n        }\n\nrep_layers = [0, 3, 7, 3]\n\n# limits for the receptive field at each spatial level\nrecept_limit = [13, 11, 9, 7, 5]\nbranches = 4\n\n# input reinforcement related parameters\nconfig_inp_reinf = 3\ninput_reinforcement = True'"
model/classification/shufflenetv2.py,4,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nimport torch\nfrom torch.nn import init\nfrom torch import nn\nfrom nn_layers.cnn_utils import CBR, Shuffle\nfrom nn_layers.dwise_conv import DWSepConv\nfrom model.classification import shufflenetv2_config as config_all\nfrom utilities.print_utils import *\n\n\nclass ShuffleBlock(nn.Module):\n    def __init__(self, inplanes, outplanes, c_tag=0.5, groups=2):\n        super(ShuffleBlock, self).__init__()\n        self.left_part = round(c_tag * inplanes)\n        self.right_part_in = inplanes - self.left_part\n        self.right_part_out = outplanes - self.left_part\n\n        self.layer_right = nn.Sequential(\n            CBR(self.right_part_in, self.right_part_out, 1, 1),\n            DWSepConv(channel_in=self.right_part_out, channel_out=self.right_part_out)\n        )\n\n        self.inplanes = inplanes\n        self.outplanes = outplanes\n        self.groups = groups\n        self.shuffle = Shuffle(groups=2)\n\n    def forward(self, x):\n        left = x[:, :self.left_part, :, :]\n        right = x[:, self.left_part:, :, :]\n\n        right = self.layer_right(right)\n        return self.shuffle(torch.cat((left, right), 1))\n\n    def __repr__(self):\n        s = \'{name}(in_channels={inplanes}, out_channels={outplanes})\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass StridedShuffleBlock(nn.Module):\n    def __init__(self, inplanes, groups=2):\n        super(StridedShuffleBlock, self).__init__()\n\n        self.layer_right = nn.Sequential(\n            CBR(inplanes, inplanes, 1, 1),\n            DWSepConv(inplanes, inplanes, stride=2)\n        )\n        self.layer_left = DWSepConv(inplanes, inplanes, stride=2)\n\n        self.groups = groups\n        self.inplanes = inplanes\n        self.out_size = 2*inplanes\n        self.shuffle = Shuffle(groups=2)\n\n    def forward(self, x):\n        out_l = self.layer_left(x)\n        out_r = self.layer_right(x)\n        return self.shuffle(torch.cat((out_l, out_r), 1))\n\n    def __repr__(self):\n        s = \'{name}(in_channels={inplanes}, out_channels={out_size})\'\n        return s.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass CNNModel(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n\n        # ====================\n        # Network configuraiton\n        # ====================\n        num_classes = args.num_classes\n        channels_in = args.channels\n        s = args.s\n        if not s in config_all.sc_ch_dict.keys():\n            print_error_message(\'Model at scale s={} is not suppoerted yet\'.format(s))\n            exit()\n\n        out_channel_map = config_all.sc_ch_dict[s]\n        reps_at_each_level = config_all.rep_layers\n\n        # ====================\n        # Network architecture\n        # ====================\n\n        # output size will be 112 x 112\n        self.level1 = CBR(channels_in, out_channel_map[0], 3, 2)\n        self.level2 = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        # output size will be 28 x 28\n        level3 = nn.ModuleList()\n        level3.append(StridedShuffleBlock(out_channel_map[1]))\n        for i in range(reps_at_each_level[1]):\n            if i == 0:\n                level3.append(ShuffleBlock(2*out_channel_map[1], out_channel_map[2]))\n            else:\n                level3.append(ShuffleBlock(out_channel_map[2], out_channel_map[2]))\n        self.level3 = nn.Sequential(*level3)\n\n        # output size will be 14 x 14\n        level4 = nn.ModuleList()\n        level4.append(StridedShuffleBlock(out_channel_map[2]))\n        for i in range(reps_at_each_level[2]):\n            if i == 0:\n                level4.append(ShuffleBlock(2*out_channel_map[2], out_channel_map[3]))\n            else:\n                level4.append(ShuffleBlock(out_channel_map[3], out_channel_map[3]))\n        self.level4 = nn.Sequential(*level4)\n\n        # output size will be 7 x 7\n        level5 = nn.ModuleList()\n        level5.append(StridedShuffleBlock(out_channel_map[3]))\n        for i in range(reps_at_each_level[3]):\n            if i == 0:\n                level5.append(ShuffleBlock(2*out_channel_map[3], out_channel_map[4]))\n            else:\n                level5.append(ShuffleBlock(out_channel_map[4], out_channel_map[4]))\n        level5.append(CBR(out_channel_map[4], out_channel_map[5], 1, 1))\n        self.level5 = nn.Sequential(*level5)\n\n        # classification layer\n        self.classifier = nn.Conv2d(out_channel_map[5], num_classes, 1, padding=0, bias=True)\n        self.drop_enable = False\n        if s > 1:\n            self.drop_layer = nn.Dropout(p=0.2)\n            self.drop_enable = True\n\n        self.global_pool = nn.AdaptiveAvgPool2d(output_size=1)\n\n        self.init_params()\n\n    def init_params(self):\n        \'\'\'\n        Function to initialze the parameters\n        \'\'\'\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal_(m.weight, std=0.001)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        \'\'\'\n        :param x: Receives the input RGB image\n        :return: a C-dimensional vector, C=# of classes\n        \'\'\'\n        x = self.level1(x)  # 112\n        x = self.level2(x)  # 56\n        x = self.level3(x)  # 28\n        x = self.level4(x)  # 14\n        x = self.level5(x)  # 7\n        x = self.global_pool(x)\n        if self.drop_enable:\n            x = self.drop_layer(x)\n        x = self.classifier(x)\n        x = x.view(x.size(0), -1)\n        return x\n\n\nif __name__ == \'__main__\':\n\n    from utilities.utils import compute_flops, model_parameters\n    import torch\n    import argparse\n    parser = argparse.ArgumentParser(description=\'Testing\')\n    args = parser.parse_args()\n\n    for scale in config_all.sc_ch_dict.keys():\n        for size in [224]:\n            args.num_classes = 1000\n            args.s = scale\n            args.channels = 3\n\n            model = CNNModel(args)\n            input = torch.randn(1, 3, size, size)\n            print_info_message(\'Scale: {}, ImSize: {}\'.format(scale, size))\n            print_info_message(\'Flops: {:.2f} million\'.format(compute_flops(model, input)))\n            print_info_message(\'Params: {:.2f} million\'.format(model_parameters(model)))\n            print(\'\\n\')\n'"
model/classification/shufflenetv2_config.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nsc_ch_dict = {\n            0.1: [8, 8, 16, 32, 64, 512],\n            0.5: [24, 24, 48, 96, 192, 1024],  # This is the same as ShuffleNetv2_0.5\n            1.0: [24, 24, 116, 232, 464, 1024],\n            1.5: [24, 24, 176, 352, 704, 1024],\n            2.0: [24, 24, 244, 488, 976, 2048]\n        }\n\nrep_layers = [0, 3, 7, 3]'"
model/detection/__init__.py,0,b''
model/detection/box_predictor.py,7,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nimport torch\nfrom model.detection.generate_priors import PriorBox\nfrom utilities import box_utils\nfrom utilities.nms import nms\nfrom data_loader.detection.augmentation import TestTransform\n\nclass BoxPredictor(object):\n\n    def __init__(self, cfg, device=\'cpu\'):\n        super(BoxPredictor, self).__init__()\n        #device = \'cuda\' if torch.cuda.device_count() > 1 else \'cpu\'\n        self.priors = PriorBox(cfg)().to(device)\n        self.center_var = cfg.center_variance\n        self.size_var = cfg.size_variance\n        self.filter_threshold = cfg.conf_threshold\n        self.nms_threshold = cfg.iou_threshold\n        self.top_k = cfg.top_k\n        self.nms_threshold = cfg.iou_threshold\n        self.top_k = cfg.top_k\n        self.transform = TestTransform(size=cfg.image_size)\n        self.softmax = torch.nn.Softmax(dim=2)\n        self.device = device\n\n    def predict(self, model, image, is_scaling=True):\n        height, width, _ = image.shape\n        image = self.transform(image)\n        images = image.unsqueeze(0)\n        images = images.to(self.device)\n        with torch.no_grad():\n            confidences, locations = model(images)\n            scores = self.softmax(confidences)\n            boxes = box_utils.convert_locations_to_boxes(\n                locations, self.priors, self.center_var, self.size_var\n            )\n            boxes = box_utils.center_form_to_corner_form(boxes)\n\n        boxes = boxes[0]\n        scores = scores[0]\n\n        filtered_box_probs = []\n        filtered_labels = []\n        for class_index in range(1, scores.size(1)):\n            probs = scores[:, class_index]\n            mask = probs > self.filter_threshold\n            probs = probs[mask]\n            if probs.size(0) == 0:\n                continue\n            masked_boxes = boxes[mask, :]\n            box_and_probs = torch.cat((masked_boxes, probs.reshape(-1, 1)), dim=1)\n            box_and_probs = nms(box_and_probs, self.nms_threshold, top_k=self.top_k)\n            filtered_box_probs.append(box_and_probs)\n            filtered_labels.extend([class_index] * box_and_probs.size(0))\n        # no object detected\n        if not filtered_box_probs:\n            return torch.empty(0, 4), torch.empty(0), torch.empty(0)\n        # concatenate all results\n        filtered_box_probs = torch.cat(filtered_box_probs)\n        if is_scaling:\n            filtered_box_probs[:, 0] *= width\n            filtered_box_probs[:, 1] *= height\n            filtered_box_probs[:, 2] *= width\n            filtered_box_probs[:, 3] *= height\n        return filtered_box_probs[:, :4], torch.tensor(filtered_labels), filtered_box_probs[:, 4]\n'"
model/detection/dicenet.py,10,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nfrom torch.nn import init\nimport torch.nn.functional as F\nimport torch\nfrom torch import nn\nfrom model.classification.dicenet import CNNModel\n\n\nclass SSDNet300(nn.Module):\n\n    def __init__(self, args, extra_layer):\n        \'\'\'\n        :param classes: number of classes in the dataset. Default is 1000 for the ImageNet dataset\n        :param s: factor that scales the number of output feature maps\n        \'\'\'\n        super(SSDNet300, self).__init__()\n\n        # =============================================================\n        #                       BASE NETWORK\n        # =============================================================\n\n        self.basenet = CNNModel(args)\n\n        # delete the classification layer\n        del self.basenet.classifier\n\n        # retrive the basenet configuration\n        base_net_config = self.basenet.out_channel_map\n        config = base_net_config[:4] + [base_net_config[5]]\n\n        # add configuration for SSD version\n        config += [1024, 512, 256]\n\n        # =============================================================\n        #                EXTRA LAYERS for DETECTION\n        # =============================================================\n\n        self.extra_level6 = extra_layer(config[4], config[5])\n        self.extra_level7 = extra_layer(config[5], config[6])\n\n        self.extra_level8 = nn.Sequential(\n            nn.Conv2d(config[6], config[6], kernel_size=3, stride=2, bias=False, padding=1),\n            nn.BatchNorm2d(config[6]),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(config[6], config[7], kernel_size=2, stride=2, bias=False),\n            nn.ReLU(inplace=True)\n        )\n\n        # =============================================================\n        #                EXTRA LAYERS for Bottom-up decoding\n        # =============================================================\n\n        from nn_layers.efficient_pyramid_pool import EfficientPyrPool\n\n        in_features = config[5] + config[6]\n        out_features = config[5]\n        red_factor = 5\n        self.bu_3x3_5x5 = EfficientPyrPool(in_planes=in_features, proj_planes=out_features // red_factor,\n                                           out_planes=out_features)\n\n        in_features = config[4] + config[5]\n        out_features = config[4]\n        self.bu_5x5_10x10 = EfficientPyrPool(in_planes=in_features, proj_planes=out_features // red_factor,\n                                           out_planes=out_features)\n\n        in_features = config[4] + config[3]\n        out_features = config[3]\n        self.bu_10x10_19x19 = EfficientPyrPool(in_planes=in_features, proj_planes=out_features // red_factor,\n                                             out_planes=out_features)\n\n        in_features = config[3] + config[2]\n        out_features = config[2]\n        self.bu_19x19_38x38 = EfficientPyrPool(in_planes=in_features, proj_planes=out_features // red_factor,\n                                               out_planes=out_features)\n\n        self.config = config\n\n    def up_sample(self, x, size):\n        return F.interpolate(x, size=(size[2], size[3]), align_corners=True, mode=\'bilinear\')\n\n    def forward(self, x, is_train=True):\n        \'\'\'\n        :param x: Receives the input RGB image\n        :return: a C-dimensional vector, C=# of classes\n        \'\'\'\n\n        out_150x150 = self.basenet.level1(x)\n        out_75x75 = self.basenet.level2(out_150x150)\n        out_38x38 = self.basenet.level3(out_75x75)\n        out_19x19 = self.basenet.level4(out_38x38)\n        out_10x10 = self.basenet.level5(out_19x19)\n\n        # Detection network\'s extra layers\n        out_5x5 = self.extra_level6(out_10x10)\n        out_3x3 = self.extra_level7(out_5x5)\n        out_1x1 = self.extra_level8(out_3x3)\n\n\n        # bottom-up decoding\n        ## 3x3 and 5x5\n        out_3x3_5x5 = self.up_sample(out_3x3, out_5x5.size())\n        out_3x3_5x5 = torch.cat((out_3x3_5x5, out_5x5), dim=1)\n        out_5x5_epp = self.bu_3x3_5x5(out_3x3_5x5)\n\n        ## 5x5 and 10x10\n        out_5x5_10x10 = self.up_sample(out_5x5_epp, out_10x10.size())\n        out_5x5_10x10 = torch.cat((out_5x5_10x10, out_10x10), dim=1)\n        out_10x10_epp = self.bu_5x5_10x10(out_5x5_10x10)\n\n        ## 10x10 and 19x19\n        out_10x10_19x19 = self.up_sample(out_10x10_epp, out_19x19.size())\n        out_10x10_19x19 = torch.cat((out_10x10_19x19, out_19x19), dim=1)\n        out_19x19_epp = self.bu_10x10_19x19(out_10x10_19x19)\n\n        ## 19x19 and 38x38\n        out_19x19_38x38 = self.up_sample(out_19x19_epp, out_38x38.size())\n        out_19x19_38x38 = torch.cat((out_19x19_38x38, out_38x38), dim=1)\n        out_38x38_epp = self.bu_19x19_38x38(out_19x19_38x38)\n\n        return out_38x38_epp, out_19x19_epp, out_10x10_epp, out_5x5_epp, out_3x3, out_1x1\n\n\n\nclass SSDNet512(nn.Module):\n\n    def __init__(self, args, extra_layer):\n        \'\'\'\n        :param classes: number of classes in the dataset. Default is 1000 for the ImageNet dataset\n        :param s: factor that scales the number of output feature maps\n        \'\'\'\n        super(SSDNet512, self).__init__()\n\n        # =============================================================\n        #                       BASE NETWORK\n        # =============================================================\n\n        self.basenet = CNNModel(args)\n\n        # delete the classification layer\n        del self.basenet.classifier\n\n        # retrive the basenet configuration\n        base_net_config = self.basenet.out_channel_map\n        config = base_net_config[:4] + [base_net_config[5]]\n\n        # add configuration for SSD version\n        config += [1024, 512, 256, 128]\n\n        # =============================================================\n        #                EXTRA LAYERS for DETECTION\n        # =============================================================\n\n        self.extra_level6 = extra_layer(config[4], config[5])\n        self.extra_level7 = extra_layer(config[5], config[6])\n        self.extra_level8 = extra_layer(config[6], config[7])\n\n        self.extra_level9 = nn.Sequential(\n            nn.Conv2d(config[7], config[8], kernel_size=3, stride=2, bias=False, padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n        # =============================================================\n        #                EXTRA LAYERS for Bottom-up decoding\n        # =============================================================\n\n        from nn_layers.efficient_pyramid_pool import EfficientPyrPool\n\n        in_features = config[5] + config[6]\n        out_features = config[5]\n        red_factor = 5\n        self.bu_4x4_8x8 = EfficientPyrPool(in_planes=in_features, proj_planes=out_features // red_factor,\n                                           out_planes=out_features)\n\n        in_features = config[4] + config[5]\n        out_features = config[4]\n        self.bu_8x8_16x16 = EfficientPyrPool(in_planes=in_features, proj_planes=out_features // red_factor,\n                                             out_planes=out_features)\n\n        in_features = config[4] + config[3]\n        out_features = config[3]\n        self.bu_16x16_32x32 = EfficientPyrPool(in_planes=in_features, proj_planes=out_features // red_factor,\n                                               out_planes=out_features)\n\n        in_features = config[3] + config[2]\n        out_features = config[2]\n        self.bu_32x32_64x64 = EfficientPyrPool(in_planes=in_features, proj_planes=out_features // red_factor,\n                                               out_planes=out_features)\n\n        self.config = config\n\n    def up_sample(self, x, size):\n        return F.interpolate(x, size=(size[2], size[3]), align_corners=True, mode=\'bilinear\')\n\n    def forward(self, x, is_train=True):\n        \'\'\'\n        :param x: Receives the input RGB image\n        :return: a C-dimensional vector, C=# of classes\n        \'\'\'\n\n        out_256x256 = self.basenet.level1(x)\n        out_128x128 = self.basenet.level2(out_256x256)\n        out_64x64 = self.basenet.level3(out_128x128)\n        out_32x32 = self.basenet.level4(out_64x64)\n        out_16x16 = self.basenet.level5(out_32x32)\n\n        # Detection network\'s extra layers\n        out_8x8 = self.extra_level6(out_16x16)\n        out_4x4 = self.extra_level7(out_8x8)\n        out_2x2 = self.extra_level8(out_4x4)\n        out_1x1 = self.extra_level9(out_2x2)\n\n        # bottom-up decoding\n        ## 3x3 and 5x5\n        out_4x4_8x8 = self.up_sample(out_4x4, out_8x8.size())\n        out_4x4_8x8 = torch.cat((out_4x4_8x8, out_8x8), dim=1)\n        out_8x8_epp = self.bu_4x4_8x8(out_4x4_8x8)\n\n        ## 5x5 and 10x10\n        out_8x8_16x16 = self.up_sample(out_8x8_epp, out_16x16.size())\n        out_8x8_16x16 = torch.cat((out_8x8_16x16, out_16x16), dim=1)\n        out_16x16_epp = self.bu_8x8_16x16(out_8x8_16x16)\n\n        ## 10x10 and 19x19\n        out_16x16_32x32 = self.up_sample(out_16x16_epp, out_32x32.size())\n        out_16x16_32x32 = torch.cat((out_16x16_32x32, out_32x32), dim=1)\n        out_32x32_epp = self.bu_16x16_32x32(out_16x16_32x32)\n\n        ## 19x19 and 38x38\n        out_32x32_64x64 = self.up_sample(out_32x32_epp, out_64x64.size())\n        out_32x32_64x64 = torch.cat((out_32x32_64x64, out_64x64), dim=1)\n        out_64x64_epp = self.bu_32x32_64x64(out_32x32_64x64)\n\n        return out_64x64_epp, out_32x32_epp, out_16x16_epp, out_8x8_epp, out_4x4, out_2x2, out_1x1\n'"
model/detection/espnetv2.py,6,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nfrom torch.nn import init\nimport torch.nn.functional as F\nimport torch\nfrom torch import nn\nfrom model.classification.espnetv2 import EESPNet\n\nclass ESPNetv2SSD(nn.Module):\n\n    def __init__(self, args, extra_layer):\n        \'\'\'\n        :param classes: number of classes in the dataset. Default is 1000 for the ImageNet dataset\n        :param s: factor that scales the number of output feature maps\n        \'\'\'\n        super(ESPNetv2SSD, self).__init__()\n\n        # =============================================================\n        #                       BASE NETWORK\n        # =============================================================\n\n        self.basenet = EESPNet(args)\n        # delete the classification layer\n        del self.basenet.classifier\n        # delte the last layer in level 5\n        #del self.basenet.level5[4]\n        #del self.basenet.level5[3]\n\n\n        # retrive the basenet configuration\n        base_net_config = self.basenet.config\n        config = base_net_config[:4] + [base_net_config[5]]\n\n        # add configuration for SSD version\n        config += [512, 256, 128]\n\n        # =============================================================\n        #                EXTRA LAYERS for DETECTION\n        # =============================================================\n\n        self.extra_level6 = extra_layer(config[4], config[5]) #\n\n        self.extra_level7 = extra_layer(config[5], config[6])\n\n        self.extra_level8 = nn.Sequential(\n            nn.AdaptiveAvgPool2d(output_size=1),\n            nn.Conv2d(config[6], config[7], kernel_size=1, stride=1, bias=False),\n            nn.ReLU(inplace=True)\n        )\n\n        # =============================================================\n        #                EXTRA LAYERS for Bottom-up decoding\n        # =============================================================\n\n        from nn_layers.efficient_pyramid_pool import EfficientPyrPool\n\n        in_features = config[5] + config[6]\n        out_features = config[5]\n        red_factor = 5\n        self.bu_3x3_5x5 = EfficientPyrPool(in_planes=in_features, proj_planes=out_features // red_factor,\n                                           out_planes=out_features, scales=[2.0, 1.0])\n\n        in_features = config[4] + config[5]\n        out_features = config[4]\n        self.bu_5x5_10x10 = EfficientPyrPool(in_planes=in_features, proj_planes=out_features // red_factor,\n                                           out_planes=out_features, scales=[2.0, 1.0, 0.5])\n\n        in_features = config[4] + config[3]\n        out_features = config[3]\n        self.bu_10x10_19x19 = EfficientPyrPool(in_planes=in_features, proj_planes=out_features // red_factor,\n                                             out_planes=out_features, scales=[2.0, 1.0, 0.5, 0.25])\n\n        in_features = config[3] + config[2]\n        out_features = config[2]\n        self.bu_19x19_38x38 = EfficientPyrPool(in_planes=in_features, proj_planes=out_features // red_factor,\n                                               out_planes=out_features, scales=[2.0, 1.0, 0.5, 0.25])\n\n        self.config = config\n\n    def up_sample(self, x, size):\n        return F.interpolate(x, size=(size[2], size[3]), align_corners=True, mode=\'bilinear\')\n\n    def forward(self, x, is_train=True):\n        \'\'\'\n        :param x: Receives the input RGB image\n        :return: a C-dimensional vector, C=# of classes\n        \'\'\'\n        out_150x150 = self.basenet.level1(x)  # 112\n        if not self.basenet.input_reinforcement:\n            del x\n            x = None\n\n        out_75x75 = self.basenet.level2_0(out_150x150, x)  # 56\n\n        out_38x38 = self.basenet.level3_0(out_75x75, x)  # down-sample\n        for i, layer in enumerate(self.basenet.level3):\n            out_38x38 = layer(out_38x38)\n\n        # Detection network\n        out_19x19 = self.basenet.level4_0(out_38x38, x)  # down-sample\n        for i, layer in enumerate(self.basenet.level4):\n            out_19x19 = layer(out_19x19)\n\n        out_10x10 = self.basenet.level5_0(out_19x19, x)  # down-sample\n        for i, layer in enumerate(self.basenet.level5):\n            out_10x10 = layer(out_10x10)\n\n        # Detection network\'s extra layers\n        out_5x5 = self.extra_level6(out_10x10)\n        out_3x3 = self.extra_level7(out_5x5)\n        out_1x1 = self.extra_level8(out_3x3)\n\n\n        # bottom-up decoding\n        ## 3x3 and 5x5\n        out_3x3_5x5 = self.up_sample(out_3x3, out_5x5.size())\n        out_3x3_5x5 = torch.cat((out_3x3_5x5, out_5x5), dim=1)\n        out_5x5_epp = self.bu_3x3_5x5(out_3x3_5x5)\n\n        ## 5x5 and 10x10\n        out_5x5_10x10 = self.up_sample(out_5x5_epp, out_10x10.size())\n        out_5x5_10x10 = torch.cat((out_5x5_10x10, out_10x10), dim=1)\n        out_10x10_epp = self.bu_5x5_10x10(out_5x5_10x10)\n\n        ## 10x10 and 19x19\n        out_10x10_19x19 = self.up_sample(out_10x10_epp, out_19x19.size())\n        out_10x10_19x19 = torch.cat((out_10x10_19x19, out_19x19), dim=1)\n        out_19x19_epp = self.bu_10x10_19x19(out_10x10_19x19)\n\n        ## 19x19 and 38x38\n        out_19x19_38x38 = self.up_sample(out_19x19_epp, out_38x38.size())\n        out_19x19_38x38 = torch.cat((out_19x19_38x38, out_38x38), dim=1)\n        out_38x38_epp = self.bu_19x19_38x38(out_19x19_38x38)\n\n        return out_38x38_epp, out_19x19_epp, out_10x10_epp, out_5x5_epp, out_3x3, out_1x1\n'"
model/detection/generate_priors.py,2,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nfrom itertools import product\n\nimport torch\nimport torch.nn as nn\nfrom math import sqrt\n\nclass PriorBox(nn.Module):\n    def __init__(self, cfg):\n        super(PriorBox, self).__init__()\n        self.image_size = cfg.image_size\n        self.feature_maps = cfg.feature_maps\n        self.min_sizes = cfg.min_sizes\n        self.max_sizes = cfg.max_sizes\n        self.strides = cfg.strides\n        self.aspect_ratios = cfg.aspect_ratio\n        self.clip = cfg.clip\n\n    def forward(self):\n        """"""Generate SSD Prior Boxes.\n            It returns the center, height and width of the priors. The values are relative to the image size\n            Returns:\n                priors (num_priors, 4): The prior boxes represented as [[center_x, center_y, w, h]]. All the values\n                    are relative to the image size.\n        """"""\n        priors = []\n        for k, f in enumerate(self.feature_maps):\n            scale = self.image_size / self.strides[k]\n            for i, j in product(range(f), repeat=2):\n                # unit center x,y\n                cx = (j + 0.5) / scale\n                cy = (i + 0.5) / scale\n\n                # small sized square box\n                size = self.min_sizes[k]\n                h = w = size / self.image_size\n                priors.append([cx, cy, w, h])\n\n                # big sized square box\n                size = sqrt(self.min_sizes[k] * self.max_sizes[k])\n                h = w = size / self.image_size\n                priors.append([cx, cy, w, h])\n\n                # change h/w ratio of the small sized box\n                size = self.min_sizes[k]\n                h = w = size / self.image_size\n                for ratio in self.aspect_ratios[k]:\n                    ratio = sqrt(ratio)\n                    priors.append([cx, cy, w * ratio, h / ratio])\n                    priors.append([cx, cy, w / ratio, h * ratio])\n\n        priors = torch.Tensor(priors)\n        if self.clip:\n            priors.clamp_(max=1.0, min=0)\n        return priors\n\n\nif __name__ == \'__main__\':\n    from model.detection.ssd_config import SSD300Configuration as cfg\n    center_form_priors = PriorBox(cfg)()\n    from utilities import box_utils\n    corner_form_priors = box_utils.center_form_to_corner_form(center_form_priors)\n    print(corner_form_priors)\n\n'"
model/detection/match_priors.py,2,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nimport torch\nfrom utilities import box_utils\nimport numpy as np\n\nclass MatchPrior(object):\n    def __init__(self, center_form_priors, center_variance, size_variance, iou_threshold):\n        self.center_form_priors = center_form_priors\n        self.corner_form_priors = box_utils.center_form_to_corner_form(center_form_priors)\n        self.center_variance = center_variance\n        self.size_variance = size_variance\n        self.iou_threshold = iou_threshold\n\n    def __call__(self, gt_boxes, gt_labels):\n        if type(gt_boxes) is np.ndarray:\n            gt_boxes = torch.from_numpy(gt_boxes)\n        if type(gt_labels) is np.ndarray:\n            gt_labels = torch.from_numpy(gt_labels)\n        boxes, labels = box_utils.assign_priors(gt_boxes, gt_labels,\n                                                self.corner_form_priors, self.iou_threshold)\n        boxes = box_utils.corner_form_to_center_form(boxes)\n        locations = box_utils.convert_boxes_to_locations(boxes, self.center_form_priors, self.center_variance, self.size_variance)\n        return locations, labels'"
model/detection/ssd.py,7,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nimport torch\nfrom torch import nn\nfrom utilities.print_utils import *\nfrom nn_layers.cnn_utils import CBR\nfrom model.detection.espnetv2 import ESPNetv2SSD\n\n\nclass SSDExtraLayers(nn.Module):\n    def __init__(self, nin, nout, ksize=3):\n        super(SSDExtraLayers, self).__init__()\n        self.layer = nn.Sequential(\n            CBR(nin, nin, stride=2, kSize=ksize, groups=nin),\n            CBR(nin, nout, kSize=1)\n        )\n\n    def forward(self, x):\n        return self.layer(x)\n\nclass SSD300(nn.Module):\n    def __init__(self, args, cfg):\n        super(SSD300, self).__init__()\n        if args.model == \'espnetv2\':\n            self.base_net = ESPNetv2SSD(args, extra_layer=SSDExtraLayers)\n        elif args.model == \'dicenet\':\n            from model.detection.dicenet import SSDNet300\n            self.base_net = SSDNet300(args, extra_layer=SSDExtraLayers)\n        else:\n            print_error_message(\'{} model not yet supported\'.format(args.model))\n\n        self.num_classes = cfg.NUM_CLASSES\n\n        self.in_channels = self.base_net.config[-6:]\n        self.loc_layers = nn.ModuleList()\n        self.cls_layers = nn.ModuleList()\n        num_anchors = cfg.box_per_location\n\n        for i in range(len(self.in_channels)):\n            self.loc_layers += [nn.Conv2d(self.in_channels[i], num_anchors[i] * 4, kernel_size=1)]\n            self.cls_layers += [nn.Conv2d(self.in_channels[i], num_anchors[i] * self.num_classes, kernel_size=1)]\n        self.init_params()\n\n    def init_params(self):\n        \'\'\'\n        Function to initialze the parameters\n        \'\'\'\n        print_info_message(\'Initializaing Conv Layers with Xavier Unifrom\')\n        # initializing matters a lot\n        # changing to Kaiming He\'s init functionaity, does not let the model to converge.\n        # probably because, smooth function struggles with that initialization.\n        # XAVIER Unifrom Rocks here\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n\n    def forward(self, x):\n        loc_preds = []\n        cls_preds = []\n        fms = self.base_net(x)\n        for i, fm in enumerate(fms):\n            loc_pred = self.loc_layers[i](fm)\n            cls_pred = self.cls_layers[i](fm)\n\n            loc_pred = loc_pred.permute(0, 2, 3, 1).contiguous()\n            loc_pred = loc_pred.view(loc_pred.size(0), -1,4)  # [N, 9*4,H,W] -> [N,H,W, 9*4] -> [N,H*W*9, 4]\n\n            cls_pred = cls_pred.permute(0, 2, 3, 1).contiguous()\n            cls_pred = cls_pred.view(cls_pred.size(0), -1,self.num_classes)  # [N,9*NC,H,W] -> [N,H,W,9*NC] -> [N,H*W*9,NC]\n\n            loc_preds.append(loc_pred)\n            cls_preds.append(cls_pred)\n\n        locations = torch.cat(loc_preds, 1)\n        confidences = torch.cat(cls_preds, 1)\n        return confidences, locations\n\n\nclass SSD512(nn.Module):\n    def __init__(self, args, cfg):\n        super(SSD512, self).__init__()\n        if args.model == \'espnetv2\':\n            self.base_net = ESPNetv2SSD(args, extra_layer=SSDExtraLayers)\n        elif args.model == \'dicenet\':\n            from model.detection.dicenet import SSDNet512\n            self.base_net = SSDNet512(args, extra_layer=SSDExtraLayers)\n        else:\n            print_error_message(\'{} model not yet supported\'.format(args.model))\n\n        self.num_classes = cfg.NUM_CLASSES\n\n        self.in_channels = self.base_net.config[-6:]\n        self.loc_layers = nn.ModuleList()\n        self.cls_layers = nn.ModuleList()\n        num_anchors = cfg.box_per_location\n\n        for i in range(len(self.in_channels)):\n            self.loc_layers += [nn.Conv2d(self.in_channels[i], num_anchors[i] * 4, kernel_size=1)]\n            self.cls_layers += [nn.Conv2d(self.in_channels[i], num_anchors[i] * self.num_classes, kernel_size=1)]\n\n        self.init_params()\n\n    def init_params(self):\n        \'\'\'\n        Function to initialze the parameters\n        \'\'\'\n        print_info_message(\'Initializaing Conv Layers with Xavier Unifrom\')\n        # initializing matters a lot\n        # changing to Kaiming He\'s init functionaity, does not let the model to converge.\n        # probably because, smooth function struggles with that initialization.\n        # XAVIER Unifrom Rocks here\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_uniform_(m.weight)\n\n    def forward(self, x):\n        loc_preds = []\n        cls_preds = []\n        fms = self.base_net(x)\n        for i, fm in enumerate(fms):\n            loc_pred = self.loc_layers[i](fm)\n            cls_pred = self.cls_layers[i](fm)\n\n            loc_pred = loc_pred.permute(0, 2, 3, 1).contiguous()\n            loc_pred = loc_pred.view(loc_pred.size(0), -1,4)  # [N, 9*4,H,W] -> [N,H,W, 9*4] -> [N,H*W*9, 4]\n\n            cls_pred = cls_pred.permute(0, 2, 3, 1).contiguous()\n            cls_pred = cls_pred.view(cls_pred.size(0), -1,self.num_classes)  # [N,9*NC,H,W] -> [N,H,W,9*NC] -> [N,H*W*9,NC]\n\n            loc_preds.append(loc_pred)\n            cls_preds.append(cls_pred)\n\n        locations = torch.cat(loc_preds, 1)\n        confidences = torch.cat(cls_preds, 1)\n        return confidences, locations\n\n\ndef ssd(config, cfg, *args, **kwargs):\n    weights = config.weights\n\n    if config.im_size == 512:\n        model = SSD512(config, cfg)\n    elif config.im_size == 300:\n        model = SSD300(config, cfg)\n    else:\n        print_error_message(\'{} image size not supported\'.format(config.im_size))\n    if weights:\n        import os\n        if not os.path.isfile(weights):\n            print_error_message(\'Weight file does not exist at {}. Please check. Exiting!!\'.format(weights))\n            exit(-1)\n        num_gpus = torch.cuda.device_count()\n        device = \'cuda\' if num_gpus >= 1 else \'cpu\'\n        pretrained_dict = torch.load(weights, map_location=torch.device(device))\n        print_info_message(\'Loading pretrained base model weights\')\n        basenet_dict = model.base_net.basenet.state_dict()\n        model_dict = model.state_dict()\n        overlap_dict = {k: v for k, v in pretrained_dict.items() if k in basenet_dict}\n        if len(overlap_dict) == 0:\n            print_error_message(\'No overlaping weights between model file and pretrained weight file. Please check\')\n            exit()\n        print_info_message(\n            \'{:.2f} % of basenet weights copied to detectnet\'.format(len(overlap_dict) * 1.0 / len(model_dict) * 100))\n        basenet_dict.update(overlap_dict)\n        model.base_net.basenet.load_state_dict(basenet_dict)\n        print_info_message(\'Pretrained base model loaded!!\')\n    else:\n        print_warning_message(\'Training from scratch!!. If you are testing, ignore this message.\'\n                              \' For testing, we do not load weights here.\')\n    return model\n\n\nif __name__ == ""__main__"":\n    from utilities.utils import compute_flops, model_parameters\n    import torch\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\'Testing\')\n    args = parser.parse_args()\n    args.s = 2.0\n    args.channels = 3\n    args.model_width = 224\n    args.model_height = 224\n    args.model = \'espnetv2\'\n    args.weights = \'\'\n    args.im_size = 300\n\n    if args.im_size == 512:\n        from model.detection.ssd_config import SSD512Configuration as cfg\n        cfg.NUM_CLASSES = 81\n    elif args.im_size == 300:\n        from model.detection.ssd_config import SSD300Configuration as cfg\n        cfg.NUM_CLASSES = 81\n    else:\n        print_error_message(\'not supported\')\n    inputs = torch.randn(1, 3, args.im_size, args.im_size)\n    net = ssd(args, cfg)\n    loc_preds, cls_preds = net(inputs)\n\n    print_info_message(compute_flops(net, input=inputs))\n    print_info_message(model_parameters(net))\n'"
model/detection/ssd_config.py,0,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\n\'\'\'\nThis file contains the standard SSD configuration\n\'\'\'\nimport numpy as np\nimport math\n\nclass SSD300Configuration(object):\n    # match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5)\n    iou_threshold = 0.45\n    neg_pos_ratio = 3\n    center_variance = 0.1\n    size_variance = 0.2\n    image_size = 300\n\n\n    # PRIOR related settings\n    strides = [8, 16, 32, 64, 100, 300]\n    m = len(strides)\n\n    feature_maps = []\n    for stride in strides:\n        temp = int(math.ceil(image_size/ stride))\n        feature_maps.append(temp)\n    #feature_maps = [38, 19, 10, 5, 3, 1]\n\n\n    s_max_size = int(math.ceil(1.05 * image_size))\n    s_min_size = int(math.ceil(0.1 * image_size))\n    sizes = [int(k) for k in np.linspace(s_min_size, s_max_size, m+1)]\n    min_sizes = sizes[:m]\n    max_sizes = sizes[1:]\n    #min_sizes = [30, 60, 111, 162, 213, 264]\n    #max_sizes = [60, 111, 162, 213, 264, 315]\n\n    # aspect ratio contains a list of pair (e.g. [2, 2] or [2,3] or single valued list e.g. [2,]\n    # This has a relationship with # of boxes per location. For example, [2,] means that 4 (=2*2) boxes per location\n    # [2, 3] means that 6=(2*2) boxes per location\n    aspect_ratio = [[2, 3]] * m\n\n    box_per_location = []  # number of boxes per feature map location\n    for pair in aspect_ratio:\n        if len(pair) == 1:\n            box_per_location.append(pair[0] * pair[0])\n        else:\n            box_per_location.append(np.prod(pair))\n\n    assert len(feature_maps) == len(strides) == len(min_sizes) == len(max_sizes) == len(aspect_ratio)\n\n    clip = True\n\n    # test specific options\n    nms_threshold = 0.45\n    conf_threshold = 0.01 # change this value during demo\n    top_k = 200 # MAX detections per class\n    max_per_image = -1\n\n\nclass SSD512Configuration(object):\n    # match default boxes to any ground truth with jaccard overlap higher than a threshold (0.5)\n    iou_threshold = 0.45\n    neg_pos_ratio = 3\n    center_variance = 0.1\n    size_variance = 0.2\n    image_size = 512\n\n\n    # PRIOR related settings\n    strides = [8, 16, 32, 64, 128, 512]\n    m = len(strides)\n    feature_maps = []\n    for stride in strides:\n        temp = int(math.ceil(image_size / stride))\n        feature_maps.append(temp)\n\n    #min_sizes = [36, 77, 154, 230, 307,  461]\n    #max_sizes = [77, 154, 230, 307, 384, 538]\n    s_max_size = int(math.ceil(1.05 * image_size))\n    s_min_size = int(math.ceil(0.1 * image_size))\n    sizes = [int(k) for k in np.linspace(s_min_size, s_max_size, m + 1)]\n    min_sizes = sizes[:m]\n    max_sizes = sizes[1:]\n\n    # aspect ratio contains a list of pair (e.g. [2, 2] or [2,3] or single valued list e.g. [2,]\n    # This has a relationship with # of boxes per location. For example, [2,] means that 4 (=2*2) boxes per location\n    # [2, 3] means that 6=(2*2) boxes per location\n    aspect_ratio = [[2, 3]] * m\n    box_per_location = []  # number of boxes per feature map location\n    for pair in aspect_ratio:\n        if len(pair) == 1:\n            box_per_location.append(pair[0] * pair[0])\n        else:\n            box_per_location.append(np.prod(pair))\n    clip = True\n\n    assert len(feature_maps) == len(strides) == len(min_sizes) == len(max_sizes) == len(aspect_ratio)\n\n    # test specific options\n    nms_threshold = 0.45\n    conf_threshold = 0.01 # change this value during demo\n    top_k = 200 # MAX detections per class\n    max_per_image = -1\n\n\n\ndef get_config(im_size):\n    if im_size == 300:\n        return SSD300Configuration()\n    elif im_size == 512:\n        return SSD512Configuration()\n    else:\n        print(\'{} image size not supported\'.format(im_size))\n\n\n'"
model/segmentation/__init__.py,0,b''
model/segmentation/dicenet.py,6,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nimport torch\nfrom torch.nn import init\nfrom nn_layers.espnet_utils import *\nfrom nn_layers.efficient_pyramid_pool import EfficientPyrPool\nfrom nn_layers.efficient_pt import EfficientPWConv\nfrom model.classification.dicenet import CNNModel\nfrom utilities.print_utils import *\nfrom torch.nn import functional as F\n\n\nclass DiCENetSegmentation(nn.Module):\n    \'\'\'\n    This class defines the ESPNetv2 architecture for the Semantic Segmenation\n    \'\'\'\n\n    def __init__(self, args, classes=21, dataset=\'pascal\'):\n        super().__init__()\n\n        # =============================================================\n        #                       BASE NETWORK\n        # =============================================================\n        self.base_net = CNNModel(args) #imagenet model\n        del self.base_net.classifier\n        del self.base_net.level5\n\n        config = self.base_net.out_channel_map\n\n        #=============================================================\n        #                   SEGMENTATION NETWORK\n        #=============================================================\n        dec_feat_dict={\n            \'pascal\': 16,\n            \'city\': 16,\n            \'coco\': 32\n        }\n        base_dec_planes = dec_feat_dict[dataset]\n        dec_planes = [4*base_dec_planes, 3*base_dec_planes, 2*base_dec_planes, classes]\n        pyr_plane_proj = min(classes //2, base_dec_planes)\n\n        # dimensions in variable names are shown for an input of 256x256\n        self.eff_pool_16x16 = EfficientPyrPool(in_planes=config[3], proj_planes=pyr_plane_proj,\n                                               out_planes=dec_planes[0])\n        self.eff_pool_32x32 = EfficientPyrPool(in_planes=dec_planes[0], proj_planes=pyr_plane_proj,\n                                               out_planes=dec_planes[1])\n        self.eff_pool_64x64 = EfficientPyrPool(in_planes=dec_planes[1], proj_planes=pyr_plane_proj,\n                                               out_planes=dec_planes[2])\n        self.eff_pool_128x128 = EfficientPyrPool(in_planes=dec_planes[2], proj_planes=pyr_plane_proj,\n                                                 out_planes=dec_planes[3], last_layer_br=False)\n\n        self.proj_enc_32x32 = EfficientPWConv(config[2], dec_planes[0])\n        self.proj_enc_64x64 = EfficientPWConv(config[1], dec_planes[1])\n        self.proj_enc_128x128 = EfficientPWConv(config[0], dec_planes[2])\n\n        self.bu_br_32x32 = nn.Sequential(nn.BatchNorm2d(dec_planes[0]),\n                                         nn.PReLU(dec_planes[0])\n                                         )\n        self.bu_br_64x64 = nn.Sequential(nn.BatchNorm2d(dec_planes[1]),\n                                         nn.PReLU(dec_planes[1])\n                                         )\n        self.bu_br_128x128 = nn.Sequential(nn.BatchNorm2d(dec_planes[2]),\n                                           nn.PReLU(dec_planes[2])\n                                           )\n\n        self.upsample = nn.Upsample(scale_factor=2, mode=\'bilinear\', align_corners=True)\n\n        self.init_params()\n\n    def init_params(self):\n        \'\'\'\n        Function to initialze the parameters\n        \'\'\'\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal_(m.weight, std=0.001)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n    def get_basenet_params(self):\n        modules_base = [self.base_net]\n        for i in range(len(modules_base)):\n            for m in modules_base[i].named_modules():\n                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], nn.BatchNorm2d) or isinstance(m[1], nn.PReLU):\n                    for p in m[1].parameters():\n                        if p.requires_grad:\n                            yield p\n\n    def get_segment_params(self):\n        modules_seg = [self.eff_pool_16x16, self.eff_pool_32x32, self.eff_pool_64x64, self.eff_pool_128x128,\n                       self.proj_enc_128x128, self.proj_enc_64x64, self.proj_enc_32x32,\n                       self.bu_br_128x128, self.bu_br_64x64, self.bu_br_32x32]\n        for i in range(len(modules_seg)):\n            for m in modules_seg[i].named_modules():\n                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], nn.BatchNorm2d) or isinstance(m[1], nn.PReLU):\n                    for p in m[1].parameters():\n                        if p.requires_grad:\n                            yield p\n\n    def forward(self, x):\n        \'\'\'\n        :param x: Receives the input RGB image\n        :return: a C-dimensional vector, C=# of classes\n        \'\'\'\n        x_size = (x.size(2), x.size(3))\n        # dimensions/names are shown for an input of 256x256\n        enc_out_128x128 = self.base_net.level1(x)\n        enc_out_64x64 = self.base_net.level2(enc_out_128x128)\n        enc_out_32x32 = self.base_net.level3(enc_out_64x64)\n        enc_out_16x16 = self.base_net.level4(enc_out_32x32)\n\n        # bottom-up decoding\n        bu_out_16x16 = self.eff_pool_16x16(enc_out_16x16)\n\n        # Decoding block\n        bu_out_32x32 = self.upsample(bu_out_16x16)\n        # project encoder\n        enc_out_32x32 = self.proj_enc_32x32(enc_out_32x32)\n        # merge encoder and decoder with identity connection\n        bu_out_32x32 = enc_out_32x32 + bu_out_32x32\n        # normalize\n        bu_out_32x32 = self.bu_br_32x32(bu_out_32x32)\n        # compute pyramid features\n        bu_out_32x32 = self.eff_pool_32x32(bu_out_32x32)\n\n        #decoding block\n        bu_out_64x64 = self.upsample(bu_out_32x32)\n        # project encoder\n        enc_out_64x64 = self.proj_enc_64x64(enc_out_64x64)\n        # merge encoder and decoder with identity connection\n        bu_out_64x64 = enc_out_64x64 + bu_out_64x64\n        # normalize\n        bu_out_64x64 = self.bu_br_64x64(bu_out_64x64)\n        # compute pyramid features\n        bu_out_64x64 = self.eff_pool_64x64(bu_out_64x64)\n\n        # decoding block\n        bu_out_128x128 = self.upsample(bu_out_64x64)\n        #project encoder\n        enc_out_128x128 = self.proj_enc_128x128(enc_out_128x128)\n        # merge encoder and decoder with identity connection\n        bu_out_128x128 = enc_out_128x128 + bu_out_128x128\n        #normalize\n        bu_out_128x128 = self.bu_br_128x128(bu_out_128x128)\n        # compute pyramid features\n        bu_out_128x128  = self.eff_pool_128x128(bu_out_128x128)\n\n        #upsample to the same size as the input\n        return F.interpolate(bu_out_128x128, size=x_size, mode=\'bilinear\', align_corners=True) #nn.Upsample(x_size, mode=\'bilinear\', align_corners=True)(bu_out_128x128)\n\n\ndef dicenet_seg(args, classes):\n    weights = args.weights\n    model = DiCENetSegmentation(args, classes=classes)\n    if weights:\n        import os\n        if os.path.isfile(weights):\n            num_gpus = torch.cuda.device_count()\n            device = \'cuda\' if num_gpus >= 1 else \'cpu\'\n            pretrained_dict = torch.load(weights, map_location=torch.device(device))\n        else:\n            print_error_message(\'Weight file does not exist at {}. Please check. Exiting!!\'.format(weights))\n            exit()\n        print_log_message(\'Loading pretrained basenet model weights\')\n        basenet_dict = model.base_net.state_dict()\n        model_dict = model.state_dict()\n        overlap_dict = {k: v for k, v in pretrained_dict.items() if k in basenet_dict}\n        if len(overlap_dict) == 0:\n            print_error_message(\'No overlaping weights between model file and pretrained weight file. Please check\')\n            #exit()\n        print_log_message(\'{:.2f} % of weights copied from basenet to segnet\'.format(len(overlap_dict) * 1.0/len(model_dict) * 100))\n        basenet_dict.update(overlap_dict)\n        model.base_net.load_state_dict(basenet_dict)\n        print_log_message(\'Pretrained basenet model loaded!!\')\n    else:\n        print_warning_message(\'Training from scratch!!\')\n    return model\n\nif __name__ == ""__main__"":\n\n    from utilities.utils import compute_flops, model_parameters\n    import torch\n    import argparse\n    import os\n\n    parser = argparse.ArgumentParser(description=\'Testing\')\n    args = parser.parse_args()\n    from model.weight_locations import segmentation\n\n    args.num_classes = 1000\n    args.weights = \'\'\n    args.dataset = \'pascal\'\n    args.channels = 3\n    args.model_width = 224\n    args.model_height = 224\n\n    for scale in segmentation.dicenet_scales:\n        args.s = scale\n        weight_map_dict = segmentation.model_weight_map[\'dicenet_{}\'.format(scale)]\n        if args.dataset == \'pascal\':\n            for size in [256, 384]:\n                model = dicenet_seg(args, classes=21)\n                input = torch.Tensor(1, 3, size, size)\n                print_info_message(\'Scale: {}, Input: {}, FLOPs: {}, Params: {}\'.format(scale, size,\n                                                                                        compute_flops(model, input=input),\n                                                                                        model_parameters(model)))\n\n\n        #for size in [224]:\n        #input = torch.Tensor(1, 3, 256, 256)\n        #model = dicenet_seg(args, classes=21)\n        #from utilities.utils import compute_flops, model_parameters\n        #print(compute_flops(model, input=input))\n        #print(model_parameters(model))\n        #out = model(input)\n        #print(out.size())\n\n'"
model/segmentation/espnetv2.py,5,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nimport torch\nfrom torch.nn import init\nfrom nn_layers.espnet_utils import *\nfrom nn_layers.efficient_pyramid_pool import EfficientPyrPool\nfrom nn_layers.efficient_pt import EfficientPWConv\nfrom model.classification.espnetv2 import EESPNet\nfrom utilities.print_utils import *\nfrom torch.nn import functional as F\n\n\nclass ESPNetv2Segmentation(nn.Module):\n    \'\'\'\n    This class defines the ESPNetv2 architecture for the Semantic Segmenation\n    \'\'\'\n\n    def __init__(self, args, classes=21, dataset=\'pascal\'):\n        super().__init__()\n\n        # =============================================================\n        #                       BASE NETWORK\n        # =============================================================\n        self.base_net = EESPNet(args) #imagenet model\n        del self.base_net.classifier\n        del self.base_net.level5\n        del self.base_net.level5_0\n        config = self.base_net.config\n\n        #=============================================================\n        #                   SEGMENTATION NETWORK\n        #=============================================================\n        dec_feat_dict={\n            \'pascal\': 16,\n            \'city\': 16,\n            \'coco\': 32\n        }\n        base_dec_planes = dec_feat_dict[dataset]\n        dec_planes = [4*base_dec_planes, 3*base_dec_planes, 2*base_dec_planes, classes]\n        pyr_plane_proj = min(classes //2, base_dec_planes)\n\n        self.bu_dec_l1 = EfficientPyrPool(in_planes=config[3], proj_planes=pyr_plane_proj,\n                                          out_planes=dec_planes[0])\n        self.bu_dec_l2 = EfficientPyrPool(in_planes=dec_planes[0], proj_planes=pyr_plane_proj,\n                                          out_planes=dec_planes[1])\n        self.bu_dec_l3 = EfficientPyrPool(in_planes=dec_planes[1], proj_planes=pyr_plane_proj,\n                                          out_planes=dec_planes[2])\n        self.bu_dec_l4 = EfficientPyrPool(in_planes=dec_planes[2], proj_planes=pyr_plane_proj,\n                                          out_planes=dec_planes[3], last_layer_br=False)\n\n        self.merge_enc_dec_l2 = EfficientPWConv(config[2], dec_planes[0])\n        self.merge_enc_dec_l3 = EfficientPWConv(config[1], dec_planes[1])\n        self.merge_enc_dec_l4 = EfficientPWConv(config[0], dec_planes[2])\n\n        self.bu_br_l2 = nn.Sequential(nn.BatchNorm2d(dec_planes[0]),\n                                      nn.PReLU(dec_planes[0])\n                                      )\n        self.bu_br_l3 = nn.Sequential(nn.BatchNorm2d(dec_planes[1]),\n                                      nn.PReLU(dec_planes[1])\n                                      )\n        self.bu_br_l4 = nn.Sequential(nn.BatchNorm2d(dec_planes[2]),\n                                      nn.PReLU(dec_planes[2])\n                                      )\n\n        #self.upsample =  nn.Upsample(scale_factor=2, mode=\'bilinear\', align_corners=True)\n\n        self.init_params()\n\n    def upsample(self, x):\n        return F.interpolate(x, scale_factor=2, mode=\'bilinear\', align_corners=True)\n\n    def init_params(self):\n        \'\'\'\n        Function to initialze the parameters\n        \'\'\'\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                init.constant_(m.weight, 1)\n                init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                init.normal_(m.weight, std=0.001)\n                if m.bias is not None:\n                    init.constant_(m.bias, 0)\n\n    def get_basenet_params(self):\n        modules_base = [self.base_net]\n        for i in range(len(modules_base)):\n            for m in modules_base[i].named_modules():\n                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], nn.BatchNorm2d) or isinstance(m[1], nn.PReLU):\n                    for p in m[1].parameters():\n                        if p.requires_grad:\n                            yield p\n\n    def get_segment_params(self):\n        modules_seg = [self.bu_dec_l1, self.bu_dec_l2, self.bu_dec_l3, self.bu_dec_l4,\n                       self.merge_enc_dec_l4, self.merge_enc_dec_l3, self.merge_enc_dec_l2,\n                       self.bu_br_l4, self.bu_br_l3, self.bu_br_l2]\n        for i in range(len(modules_seg)):\n            for m in modules_seg[i].named_modules():\n                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], nn.BatchNorm2d) or isinstance(m[1], nn.PReLU):\n                    for p in m[1].parameters():\n                        if p.requires_grad:\n                            yield p\n\n    def forward(self, x):\n        \'\'\'\n        :param x: Receives the input RGB image\n        :return: a C-dimensional vector, C=# of classes\n        \'\'\'\n        x_size = (x.size(2), x.size(3))\n        enc_out_l1 = self.base_net.level1(x)  # 112\n        if not self.base_net.input_reinforcement:\n            del x\n            x = None\n\n        enc_out_l2 = self.base_net.level2_0(enc_out_l1, x)  # 56\n\n        enc_out_l3_0 = self.base_net.level3_0(enc_out_l2, x)  # down-sample\n        for i, layer in enumerate(self.base_net.level3):\n            if i == 0:\n                enc_out_l3 = layer(enc_out_l3_0)\n            else:\n                enc_out_l3 = layer(enc_out_l3)\n\n        enc_out_l4_0 = self.base_net.level4_0(enc_out_l3, x)  # down-sample\n        for i, layer in enumerate(self.base_net.level4):\n            if i == 0:\n                enc_out_l4 = layer(enc_out_l4_0)\n            else:\n                enc_out_l4 = layer(enc_out_l4)\n\n        # bottom-up decoding\n        bu_out = self.bu_dec_l1(enc_out_l4)\n\n        # Decoding block\n        bu_out = self.upsample(bu_out)\n        enc_out_l3_proj = self.merge_enc_dec_l2(enc_out_l3)\n        bu_out = enc_out_l3_proj + bu_out\n        bu_out = self.bu_br_l2(bu_out)\n        bu_out = self.bu_dec_l2(bu_out)\n\n        #decoding block\n        bu_out = self.upsample(bu_out)\n        enc_out_l2_proj = self.merge_enc_dec_l3(enc_out_l2)\n        bu_out = enc_out_l2_proj + bu_out\n        bu_out = self.bu_br_l3(bu_out)\n        bu_out = self.bu_dec_l3(bu_out)\n\n        # decoding block\n        bu_out = self.upsample(bu_out)\n        enc_out_l1_proj = self.merge_enc_dec_l4(enc_out_l1)\n        bu_out = enc_out_l1_proj + bu_out\n        bu_out = self.bu_br_l4(bu_out)\n        bu_out  = self.bu_dec_l4(bu_out)\n\n        return F.interpolate(bu_out, size=x_size, mode=\'bilinear\', align_corners=True)\n\n\ndef espnetv2_seg(args):\n    classes = args.classes\n    scale=args.s\n    weights = args.weights\n    dataset=args.dataset\n    model = ESPNetv2Segmentation(args, classes=classes, dataset=dataset)\n    if weights:\n        import os\n        if os.path.isfile(weights):\n            num_gpus = torch.cuda.device_count()\n            device = \'cuda\' if num_gpus >= 1 else \'cpu\'\n            pretrained_dict = torch.load(weights, map_location=torch.device(device))\n        else:\n            print_error_message(\'Weight file does not exist at {}. Please check. Exiting!!\'.format(weights))\n            exit()\n        print_info_message(\'Loading pretrained basenet model weights\')\n        basenet_dict = model.base_net.state_dict()\n        model_dict = model.state_dict()\n        overlap_dict = {k: v for k, v in pretrained_dict.items() if k in basenet_dict}\n        if len(overlap_dict) == 0:\n            print_error_message(\'No overlaping weights between model file and pretrained weight file. Please check\')\n            exit()\n        print_info_message(\'{:.2f} % of weights copied from basenet to segnet\'.format(len(overlap_dict) * 1.0/len(model_dict) * 100))\n        basenet_dict.update(overlap_dict)\n        model.base_net.load_state_dict(basenet_dict)\n        print_info_message(\'Pretrained basenet model loaded!!\')\n    else:\n        print_warning_message(\'Training from scratch!!\')\n    return model\n\nif __name__ == ""__main__"":\n\n    from utilities.utils import compute_flops, model_parameters\n    import torch\n    import argparse\n\n    parser = argparse.ArgumentParser(description=\'Testing\')\n    args = parser.parse_args()\n\n    args.classes = 21\n    args.s = 2.0\n    args.weights=\'../classification/model_zoo/espnet/espnetv2_s_2.0_imagenet_224x224.pth\'\n    args.dataset=\'pascal\'\n\n    input = torch.Tensor(1, 3, 384, 384)\n    model = espnetv2_seg(args)\n    from utilities.utils import compute_flops, model_parameters\n    print_info_message(compute_flops(model, input=input))\n    print_info_message(model_parameters(model))\n    out = model(input)\n    print_info_message(out.size())\n'"
model/weight_locations/__init__.py,0,b''
model/weight_locations/classification.py,0,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nmodel_weight_map = {}\n# key is of the form <model-name_model-scale>\n\n## ESPNetv2 models\nespnetv2_scales = [0.5, 1.0, 1.25, 1.5, 2.0]\nfor sc in espnetv2_scales:\n    model_weight_map[\'espnetv2_{}\'.format(sc)] = \'model/classification/model_zoo/espnetv2/espnetv2_s_{}_imagenet_224x224.pth\'.format(sc)\n\n#DiceNet Models\ndicenet_scales  = [0.2, 0.5, 0.75, 1.0, 1.5, 1.25, 1.75, 2.0]\nfor sc in dicenet_scales:\n    model_weight_map[\'dicenet_{}\'.format(sc)] = \'model/classification/model_zoo/dicenet/dicenet_s_{}_imagenet_224x224.pth\'.format(sc)\n\n\n## ShuffleNetv2 models\nshufflenetv2_scales = [0.5]\nfor sc in shufflenetv2_scales:\n    model_weight_map[\'shufflenetv2_{}\'.format(sc)] = \'model/classification/model_zoo/shufflenetv2/shufflenetv2_s_{}_imagenet_224x224.pth\'.format(sc)'"
model/weight_locations/detection.py,0,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nmodel_weight_map = {}\n# key is of the form <model-name_model-scale>\n\n# ESPNetv2\nespnetv2_scales = [2.0]\nfor scale in espnetv2_scales:\n    model_weight_map[\'espnetv2_{}\'.format(scale)] = {\n        \'pascal_300x300\':\n        {\n            \'weights\': \'model/detection/model_zoo/espnetv2/espnetv2_s_{}_pascal_300x300.pth\'.format(scale)\n        },\n        \'pascal_512x512\':\n            {\n                \'weights\': \'model/detection/model_zoo/espnetv2/espnetv2_s_{}_pascal_512x512.pth\'.format(scale)\n            },\n        \'coco_300x300\':\n            {\n                \'weights\': \'model/detection/model_zoo/espnetv2/espnetv2_s_{}_coco_300x300.pth\'.format(scale)\n            },\n        \'coco_512x512\':\n            {\n                \'weights\': \'model/detection/model_zoo/espnetv2/espnetv2_s_{}_coco_512x512.pth\'.format(scale)\n            }\n    }'"
model/weight_locations/segmentation.py,0,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nmodel_weight_map = {}\n# key is of the form <model-name_model-scale>\n\n\n#ESPNetv2\nespnetv2_scales = [0.5, 1.0, 1.5, 2.0]\nfor scale in espnetv2_scales:\n    model_weight_map[\'espnetv2_{}\'.format(scale)] = {\n        \'pascal_256x256\':\n            {\n                \'weights\': \'model/segmentation/model_zoo/espnetv2/espnetv2_s_{}_pascal_256x256.pth\'.format(scale)\n            },\n        \'pascal_384x384\':\n            {\n                \'weights\': \'model/segmentation/model_zoo/espnetv2/espnetv2_s_{}_pascal_384x384.pth\'.format(scale)\n            },\n        \'city_1024x512\': {\n            \'weights\': \'model/segmentation/model_zoo/espnetv2/espnetv2_s_{}_city_1024x512.pth\'.format(scale)\n        },\n        \'city_512x256\': {\n            \'weights\': \'model/segmentation/model_zoo/espnetv2/espnetv2_s_{}_city_512x256.pth\'.format(scale)\n        }\n    }\n\n#DiCENet\n\ndicenet_scales = [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]\nfor scale in dicenet_scales:\n    model_weight_map[\'dicenet_{}\'.format(scale)] = {\n        \'pascal_256x256\':\n        {\n            \'weights\': \'model/segmentation/model_zoo/dicenet/dicenet_s_{}_pascal_256x256.pth\'.format(scale)\n        },\n        \'pascal_384x384\':\n        {\n            \'weights\': \'model/segmentation/model_zoo/dicenet/dicenet_s_{}_pascal_384x384.pth\'.format(scale)\n        },\n        \'city_1024x512\': {\n            \'weights\': \'model/segmentation/model_zoo/dicenet/dicenet_s_{}_city_1024x512.pth\'.format(scale)\n        },\n        \'city_512x256\': {\n            \'weights\': \'model/segmentation/model_zoo/dicenet/dicenet_s_{}_city_512x256.pth\'.format(scale)\n        }\n    }\n'"
transforms/classification/__init__.py,0,b''
transforms/classification/data_transforms.py,2,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nfrom torchvision import transforms\nimport torch\n\n# Normalization PARAMETERS for the IMAGENET dataset\nMEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]\nnormalize = transforms.Normalize(mean=MEAN, std=STD)\n\nimagenet_pca = {\n    \'eigval\': torch.Tensor([0.2175, 0.0188, 0.0045]),\n    \'eigvec\': torch.Tensor([\n        [-0.5675, 0.7192, 0.4009],\n        [-0.5808, -0.0045, -0.8140],\n        [-0.5836, -0.6948, 0.4203],\n    ])\n}\n\n\n# Lighting data augmentation take from here - https://github.com/eladhoffer/convNet.pytorch/blob/master/preprocess.py\nclass Lighting(object):\n    """"""Lighting noise(AlexNet - style PCA - based noise)""""""\n\n    def __init__(self, alphastd):\n        self.alphastd = alphastd\n        self.eigval = imagenet_pca[\'eigval\']\n        self.eigvec = imagenet_pca[\'eigvec\']\n\n    def __call__(self, img):\n        if self.alphastd == 0:\n            return img\n\n        alpha = img.new().resize_(3).normal_(0, self.alphastd)\n        rgb = self.eigvec.type_as(img).clone() \\\n            .mul(alpha.view(1, 3).expand(3, 3)) \\\n            .mul(self.eigval.view(1, 3).expand(3, 3)) \\\n            .sum(1).squeeze()\n        return img.add(rgb.view(3, 1, 1).expand_as(img))\n'"
transforms/detection/__init__.py,0,b''
transforms/detection/data_transforms.py,1,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\n# from https://github.com/amdegroot/ssd.pytorch\n\nimport torch\nimport cv2\nimport numpy as np\nfrom transforms.classification.data_transforms import MEAN as MEAN_FLOAT\nfrom transforms.classification.data_transforms import STD as STD_FLOAT\nfrom numpy import random\nimport math\n\nMEAN = np.array([m * 255 for m in MEAN_FLOAT])\nSTD = np.array([s * 255 for s in STD_FLOAT])\n\n\ndef intersect(box_a, box_b):\n    max_xy = np.minimum(box_a[:, 2:], box_b[2:])\n    min_xy = np.maximum(box_a[:, :2], box_b[:2])\n    inter = np.clip((max_xy - min_xy), a_min=0, a_max=np.inf)\n    return inter[:, 0] * inter[:, 1]\n\n\ndef jaccard_numpy(box_a, box_b):\n    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n    is simply the intersection over union of two boxes.\n    E.g.:\n        A \xe2\x88\xa9 B / A \xe2\x88\xaa B = A \xe2\x88\xa9 B / (area(A) + area(B) - A \xe2\x88\xa9 B)\n    Args:\n        box_a: Multiple bounding boxes, Shape: [num_boxes,4]\n        box_b: Single bounding box, Shape: [4]\n    Return:\n        jaccard overlap: Shape: [box_a.shape[0], box_a.shape[1]]\n    """"""\n    inter = intersect(box_a, box_b)\n    area_a = ((box_a[:, 2] - box_a[:, 0]) *\n              (box_a[:, 3] - box_a[:, 1]))  # [A,B]\n    area_b = ((box_b[2] - box_b[0]) *\n              (box_b[3] - box_b[1]))  # [A,B]\n    union = area_a + area_b - inter\n    return inter / union  # [A,B]\n\n\nclass ConvertFromInts(object):\n    def __call__(self, image, boxes=None, labels=None):\n        return image.astype(np.float32), boxes, labels\n\n\nclass Normalize(object):\n    def __call__(self, image, boxes=None, labels=None):\n        image = image.astype(np.float32)\n        image -= MEAN\n        image /= STD\n        return image.astype(np.float32), boxes, labels\n\n\nclass ToAbsoluteCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] *= width\n        boxes[:, 2] *= width\n        boxes[:, 1] *= height\n        boxes[:, 3] *= height\n\n        return image, boxes, labels\n\n\nclass ToPercentCoords(object):\n    def __call__(self, image, boxes=None, labels=None):\n        height, width, channels = image.shape\n        boxes[:, 0] /= width\n        boxes[:, 2] /= width\n        boxes[:, 1] /= height\n        boxes[:, 3] /= height\n\n        return image, boxes, labels\n\n\nclass Expand(object):\n    def __init__(self):\n        self.mean = MEAN\n\n    def __call__(self, image, boxes, labels):\n        if random.randint(2):\n            return image, boxes, labels\n\n        height, width, depth = image.shape\n        ratio = random.uniform(1, 4)\n        left = random.uniform(0, width * ratio - width)\n        top = random.uniform(0, height * ratio - height)\n\n        expand_image = np.zeros(\n            (int(height * ratio), int(width * ratio), depth),\n            dtype=image.dtype)\n        expand_image[:, :, :] = self.mean\n        expand_image[int(top):int(top + height),\n        int(left):int(left + width)] = image\n        image = expand_image\n\n        boxes = boxes.copy()\n        boxes[:, :2] += (int(left), int(top))\n        boxes[:, 2:] += (int(left), int(top))\n\n        return image, boxes, labels\n\n\nclass Resize(object):\n    def __init__(self, size=300):\n        self.size = size\n\n    def __call__(self, image, boxes=None, labels=None):\n        image = cv2.resize(image, (self.size,\n                                   self.size))\n        return image, boxes, labels\n\n\nclass RandomSaturation(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 1] *= random.uniform(self.lower, self.upper)\n\n        return image, boxes, labels\n\n\nclass RandomHue(object):\n    def __init__(self, delta=18.0):\n        assert delta >= 0.0 and delta <= 360.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            image[:, :, 0] += random.uniform(-self.delta, self.delta)\n            image[:, :, 0][image[:, :, 0] > 360.0] -= 360.0\n            image[:, :, 0][image[:, :, 0] < 0.0] += 360.0\n        return image, boxes, labels\n\n\nclass RandomLightingNoise(object):\n    def __init__(self):\n        self.perms = ((0, 1, 2), (0, 2, 1),\n                      (1, 0, 2), (1, 2, 0),\n                      (2, 0, 1), (2, 1, 0))\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            swap = self.perms[random.randint(len(self.perms))]\n            shuffle = SwapChannels(swap)  # shuffle channels\n            image = shuffle(image)\n        return image, boxes, labels\n\n\nclass ConvertColor(object):\n    def __init__(self, current, transform):\n        self.transform = transform\n        self.current = current\n\n    def __call__(self, image, boxes=None, labels=None):\n        if self.current == \'BGR\' and self.transform == \'HSV\':\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        elif self.current == \'RGB\' and self.transform == \'HSV\':\n            image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n        elif self.current == \'BGR\' and self.transform == \'RGB\':\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif self.current == \'HSV\' and self.transform == \'BGR\':\n            image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n        elif self.current == \'HSV\' and self.transform == ""RGB"":\n            image = cv2.cvtColor(image, cv2.COLOR_HSV2RGB)\n        else:\n            raise NotImplementedError\n        return image, boxes, labels\n\n\nclass RandomContrast(object):\n    def __init__(self, lower=0.5, upper=1.5):\n        self.lower = lower\n        self.upper = upper\n        assert self.upper >= self.lower, ""contrast upper must be >= lower.""\n        assert self.lower >= 0, ""contrast lower must be non-negative.""\n\n    # expects float image\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            alpha = random.uniform(self.lower, self.upper)\n            image *= alpha\n        return image, boxes, labels\n\n\nclass RandomBrightness(object):\n    def __init__(self, delta=32):\n        assert delta >= 0.0\n        assert delta <= 255.0\n        self.delta = delta\n\n    def __call__(self, image, boxes=None, labels=None):\n        if random.randint(2):\n            delta = random.uniform(-self.delta, self.delta)\n            image += delta\n        return image, boxes, labels\n\n\nclass ToTensor(object):\n    def __call__(self, cvimage, boxes=None, labels=None):\n        return torch.from_numpy(cvimage.astype(np.float32)).permute(2, 0, 1), boxes, labels\n\n\nclass RandomSampleCrop(object):\n    """"""Crop\n    Arguments:\n        img (Image): the image being input during training\n        boxes (Tensor): the original bounding boxes in pt form\n        labels (Tensor): the class labels for each bbox\n        mode (float tuple): the min and max jaccard overlaps\n    Return:\n        (img, boxes, classes)\n            img (Image): the cropped image\n            boxes (Tensor): the adjusted bounding boxes in pt form\n            labels (Tensor): the class labels for each bbox\n    """"""\n\n    def __init__(self):\n        self.sample_options = (\n            # using entire original input image\n            None,\n            # sample a patch s.t. MIN jaccard w/ obj in .1,.3,.4,.7,.9\n            (0.1, None),\n            (0.3, None),\n            (0.7, None),\n            (0.9, None),\n            # randomly sample a patch\n            (None, None),\n        )\n\n    def __call__(self, image, boxes=None, labels=None):\n        # guard against no boxes\n        if boxes is not None and boxes.shape[0] == 0:\n            return image, boxes, labels\n        height, width = image.shape[:2]\n        while True:\n            # randomly choose a mode\n            mode = random.choice(self.sample_options)\n            if mode is None:\n                return image, boxes, labels\n\n            min_iou, max_iou = mode\n            if min_iou is None:\n                min_iou = float(\'-inf\')\n            if max_iou is None:\n                max_iou = float(\'inf\')\n\n            # max trails (50)\n            for _ in range(50):\n                current_image = image\n\n                w = random.uniform(0.3 * width, width)\n                h = random.uniform(0.3 * height, height)\n\n                # aspect ratio constraint b/t .5 & 2\n                if h / w < 0.5 or h / w > 2:\n                    continue\n\n                left = random.uniform(width - w)\n                top = random.uniform(height - h)\n\n                # convert to integer rect x1,y1,x2,y2\n                rect = np.array([int(left), int(top), int(left + w), int(top + h)])\n\n                # calculate IoU (jaccard overlap) b/t the cropped and gt boxes\n                overlap = jaccard_numpy(boxes, rect)\n\n                # is min and max overlap constraint satisfied? if not try again\n                if overlap.min() < min_iou and max_iou < overlap.max():\n                    continue\n\n                # cut the crop from the image\n                current_image = current_image[rect[1]:rect[3], rect[0]:rect[2],\n                                :]\n\n                # keep overlap with gt box IF center in sampled patch\n                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n\n                # mask in all gt boxes that above and to the left of centers\n                m1 = (rect[0] < centers[:, 0]) * (rect[1] < centers[:, 1])\n\n                # mask in all gt boxes that under and to the right of centers\n                m2 = (rect[2] > centers[:, 0]) * (rect[3] > centers[:, 1])\n\n                # mask in that both m1 and m2 are true\n                mask = m1 * m2\n\n                # have any valid boxes? try again if not\n                if not mask.any():\n                    continue\n\n                # take only matching gt boxes\n                current_boxes = boxes[mask, :].copy()\n\n                # take only matching gt labels\n                current_labels = labels[mask]\n\n                # should we use the box left and top corner or the crop\'s\n                current_boxes[:, :2] = np.maximum(current_boxes[:, :2],\n                                                  rect[:2])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, :2] -= rect[:2]\n\n                current_boxes[:, 2:] = np.minimum(current_boxes[:, 2:],\n                                                  rect[2:])\n                # adjust to crop (by substracting crop\'s left,top)\n                current_boxes[:, 2:] -= rect[:2]\n\n                return current_image, current_boxes, current_labels\n\n\nclass SwapChannels(object):\n    """"""Transforms a tensorized image by swapping the channels in the order\n     specified in the swap tuple.\n    Args:\n        swaps (int triple): final order of channels\n            eg: (2, 1, 0)\n    """"""\n\n    def __init__(self, swaps):\n        self.swaps = swaps\n\n    def __call__(self, image):\n        """"""\n        Args:\n            image (Tensor): image tensor to be transformed\n        Return:\n            a tensor with channels swapped according to swap\n        """"""\n        image = image[:, :, self.swaps]\n        return image\n\nclass RandomFlipping(object):\n    def __call__(self, image, boxes, classes):\n        if random.random() < 0.5:\n            width = image.shape[1]\n            image = image[:, ::-1]\n            boxes = boxes.copy()\n            boxes[:, 0::2] = width - boxes[:, 2::-2]\n        return image, boxes, classes\n\n\nclass PhotometricDistort(object):\n    def __init__(self):\n        self.pd = [\n            RandomContrast(),  # RGB\n            ConvertColor(current=""RGB"", transform=\'HSV\'),  # HSV\n            RandomSaturation(),  # HSV\n            RandomHue(),  # HSV\n            ConvertColor(current=\'HSV\', transform=\'RGB\'),  # RGB\n            RandomContrast()  # RGB\n        ]\n        self.rand_brightness = RandomBrightness()\n        self.rand_light_noise = RandomLightingNoise()\n\n    def __call__(self, image, boxes, labels):\n        im = image.copy()\n        im, boxes, labels = self.rand_brightness(im, boxes, labels)\n        if random.random() < 0.5:\n            distort = Compose(self.pd[:-1])\n        else:\n            distort = Compose(self.pd[1:])\n        im, boxes, labels = distort(im, boxes, labels)\n        return self.rand_light_noise(im, boxes, labels)\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, boxes=None, labels=None):\n        for t in self.transforms:\n            img, boxes, labels = t(img, boxes, labels)\n        return img, boxes, labels\n'"
transforms/segmentation/__init__.py,0,b''
transforms/segmentation/data_transforms.py,1,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nimport random\nfrom PIL import Image\nimport math\nimport torch\nimport numpy as np\nimport numbers\nfrom torchvision.transforms import Pad\nfrom torchvision.transforms import functional as F\nfrom transforms.classification.data_transforms import MEAN, STD\n\nclass Normalize(object):\n    \'\'\'\n        Normalize the tensors\n    \'\'\'\n    def __call__(self, rgb_img, label_img=None):\n        rgb_img = F.to_tensor(rgb_img) # convert to tensor (values between 0 and 1)\n        rgb_img = F.normalize(rgb_img, MEAN, STD) # normalize the tensor\n        label_img = torch.LongTensor(np.array(label_img).astype(np.int64))\n        return rgb_img, label_img\n\n\nclass RandomFlip(object):\n    \'\'\'\n        Random Flipping\n    \'\'\'\n    def __call__(self, rgb_img, label_img):\n        if random.random() < 0.5:\n            rgb_img = rgb_img.transpose(Image.FLIP_LEFT_RIGHT)\n            label_img = label_img.transpose(Image.FLIP_LEFT_RIGHT)\n        return rgb_img, label_img\n\n\nclass RandomScale(object):\n    \'\'\'\n    Random scale, where scale is logrithmic\n    \'\'\'\n    def __init__(self, scale=(0.5, 1.0)):\n        if isinstance(scale, tuple):\n            self.scale = scale\n        else:\n            self.scale = (scale, scale)\n\n    def __call__(self, rgb_img, label_img):\n        w, h = rgb_img.size\n        rand_log_scale = math.log(self.scale[0], 2) + random.random() * (math.log(self.scale[1], 2) - math.log(self.scale[0], 2))\n        random_scale = math.pow(2, rand_log_scale)\n        new_size = (int(round(w * random_scale)), int(round(h * random_scale)))\n        rgb_img = rgb_img.resize(new_size, Image.ANTIALIAS)\n        label_img = label_img.resize(new_size, Image.NEAREST)\n        return rgb_img, label_img\n\n\nclass RandomCrop(object):\n    \'\'\'\n    Randomly crop the image\n    \'\'\'\n    def __init__(self, crop_size, ignore_idx=255):\n        if isinstance(crop_size, numbers.Number):\n            self.crop_size = (int(crop_size), int(crop_size))\n        else:\n            self.crop_size = crop_size\n        self.ignore_idx = ignore_idx\n\n    @staticmethod\n    def get_params(img, output_size):\n        w, h = img.size\n        tw, th = output_size\n        if w == tw and h == th:\n            return 0, 0, h, w\n        i = random.randint(0, h - th)\n        j = random.randint(0, w - tw)\n        return i, j, th, tw\n\n    def __call__(self, rgb_img, label_img):\n        w, h = rgb_img.size\n        pad_along_w = max(0, int((1 + self.crop_size[0] - w) / 2))\n        pad_along_h = max(0, int((1 + self.crop_size[1] - h) / 2))\n        # padd the images\n        rgb_img = Pad(padding=(pad_along_w, pad_along_h), fill=0, padding_mode=\'constant\')(rgb_img)\n        label_img = Pad(padding=(pad_along_w, pad_along_h), fill=self.ignore_idx, padding_mode=\'constant\')(label_img)\n\n        i, j, h, w = self.get_params(rgb_img, self.crop_size)\n        rgb_img = F.crop(rgb_img, i, j, h, w)\n        label_img = F.crop(label_img, i, j, h, w)\n        return rgb_img, label_img\n\n\nclass RandomResizedCrop(object):\n    \'\'\'\n    Randomly crop the image and then resize it\n    \'\'\'\n    def __init__(self, size, scale=(0.5, 1.0), ignore_idx=255):\n        if isinstance(size, numbers.Number):\n            self.size = (int(size), int(size))\n        else:\n            self.size = size\n\n        if isinstance(scale, tuple):\n            self.scale = scale\n        else:\n            self.scale = (scale, scale)\n\n        self.ignore_idx = ignore_idx\n\n    @staticmethod\n    def get_params(img, output_size):\n        w, h = img.size\n        tw, th = output_size\n        if w == tw and h == th:\n            return 0, 0, h, w\n        i = random.randint(0, h - th)\n        j = random.randint(0, w - tw)\n        return i, j, th, tw\n\n    def __call__(self, rgb_img, label_img):\n        w, h = rgb_img.size\n\n        rand_log_scale = math.log(self.scale[0], 2) + random.random() * (\n                    math.log(self.scale[1], 2) - math.log(self.scale[0], 2))\n        random_scale = math.pow(2, rand_log_scale)\n        crop_size = (int(round(w * random_scale)), int(round(h * random_scale)))\n\n        i, j, h, w = self.get_params(rgb_img, crop_size)\n        rgb_img = F.crop(rgb_img, i, j, h, w)\n        label_img = F.crop(label_img, i, j, h, w)\n\n        rgb_img = rgb_img.resize(self.size, Image.ANTIALIAS)\n        label_img = label_img.resize(self.size, Image.NEAREST)\n\n        return rgb_img, label_img\n\n\nclass Resize(object):\n    \'\'\'\n        Resize the images\n    \'\'\'\n    def __init__(self, size=(512, 512)):\n        if isinstance(size, tuple):\n            self.size = size\n        else:\n            self.size = (size, size)\n\n    def __call__(self, rgb_img, label_img):\n        rgb_img = rgb_img.resize(self.size, Image.BILINEAR)\n        label_img = label_img.resize(self.size, Image.NEAREST)\n        return rgb_img, label_img\n\n\nclass Compose(object):\n    """"""Composes several transforms together.\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, *args):\n        for t in self.transforms:\n            args = t(*args)\n        return args\n'"
utilities/metrics/__init__.py,0,b''
utilities/metrics/classification_accuracy.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nimport torch\n\ndef accuracy(output, target, topk=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res'"
utilities/metrics/coco.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nimport json\nimport os\n\n\ndef coco_evaluation(dataset, predictions, output_dir):\n    coco_results = []\n    for i, (boxes, labels, scores) in enumerate(predictions):\n        image_id, annotation = dataset.get_annotation(i)\n        class_mapper = dataset.contiguous_id_to_coco_id\n        if labels.shape[0] == 0:\n            continue\n\n        boxes = boxes.tolist()\n        labels = labels.tolist()\n        scores = scores.tolist()\n        coco_results.extend(\n            [\n                {\n                    ""image_id"": image_id,\n                    ""category_id"": class_mapper[labels[k]],\n                    ""bbox"": [box[0], box[1], box[2] - box[0], box[3] - box[1]],  # to xywh format\n                    ""score"": scores[k],\n                }\n                for k, box in enumerate(boxes)\n            ]\n        )\n    iou_type = \'bbox\'\n    if output_dir:\n        json_result_file = os.path.join(output_dir, iou_type + "".json"")\n        with open(json_result_file, ""w"") as f:\n            json.dump(coco_results, f)\n    from pycocotools.cocoeval import COCOeval\n    coco_gt = dataset.coco\n    coco_dt = coco_gt.loadRes(json_result_file)\n    coco_eval = COCOeval(coco_gt, coco_dt, iou_type)\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    coco_eval.summarize()\n    return coco_eval\n'"
utilities/metrics/evaluate_detection.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nfrom .coco import coco_evaluation\nfrom .voc import voc_evaluation\n\ndef evaluate(dataset, dataset_name, predictions, output_dir):\n    """"""evaluate dataset using different methods based on dataset type.\n    Args:\n        dataset: Dataset object\n        predictions(list[(boxes, labels, scores)]): Each item in the list represents the\n            prediction results for one image. And the index should match the dataset index.\n        output_dir: output folder, to save evaluation files or results.\n    Returns:\n        evaluation result\n    """"""\n    args = dict(\n        dataset=dataset, predictions=predictions, output_dir=output_dir\n    )\n    if dataset_name in [\'voc\', \'pascal\']:\n        evaluation_result = voc_evaluation(**args)\n        return evaluation_result\n    elif dataset_name == \'coco\':\n        evaluation_result = coco_evaluation(**args)\n        return evaluation_result\n    else:\n        raise NotImplementedError\n        exit()\n'"
utilities/metrics/segmentation_miou.py,10,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\nimport numpy as np\nimport torch\n\nclass MIOU(object):\n    def __init__(self, num_classes=21):\n        self.num_classes = num_classes\n        self.epsilon = 1e-6\n\n    def get_iou(self, output, target):\n        if isinstance(output, tuple):\n            output = output[0]\n\n        _, pred = torch.max(output, 1)\n\n        # histc in torch is implemented only for cpu tensors, so move your tensors to CPU\n        if pred.device == torch.device(\'cuda\'):\n            pred = pred.cpu()\n        if target.device == torch.device(\'cuda\'):\n            target = target.cpu()\n\n        pred = pred.type(torch.ByteTensor)\n        target = target.type(torch.ByteTensor)\n\n        # shift by 1 so that 255 is 0\n        pred += 1\n        target += 1\n\n        pred = pred * (target > 0)\n        inter = pred * (pred == target)\n        area_inter = torch.histc(inter.float(), bins=self.num_classes, min=1, max=self.num_classes)\n        area_pred = torch.histc(pred.float(), bins=self.num_classes, min=1, max=self.num_classes)\n        area_mask = torch.histc(target.float(), bins=self.num_classes, min=1, max=self.num_classes)\n        area_union = area_pred + area_mask - area_inter + self.epsilon\n\n        return area_inter.numpy(), area_union.numpy()\n\n\nif __name__ == \'__main__\':\n    from utilities.utils import AverageMeter\n    inter = AverageMeter()\n    union = AverageMeter()\n    a = torch.Tensor(1, 21, 224, 224).random_(254, 256)\n    b = torch.Tensor(1, 21, 224, 224).random_(254, 256 )\n\n    m = MIOU()\n    print(m.get_iou(a, b))\n'"
utilities/metrics/voc.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nimport os\nfrom datetime import datetime\n\nimport numpy as np\n\nfrom .voc_helper import eval_detection_voc\n\n\ndef voc_evaluation(dataset, predictions, output_dir=None):\n    class_names = dataset.CLASSES\n\n    pred_boxes_list = []\n    pred_labels_list = []\n    pred_scores_list = []\n    gt_boxes_list = []\n    gt_labels_list = []\n    gt_difficults = []\n\n    for i in range(len(dataset)):\n        image_id, annotation = dataset.get_annotation(i)\n        gt_boxes, gt_labels, is_difficult = annotation\n        gt_boxes_list.append(gt_boxes)\n        gt_labels_list.append(gt_labels)\n        gt_difficults.append(is_difficult.astype(np.bool))\n\n        boxes, labels, scores = predictions[i]\n        pred_boxes_list.append(boxes)\n        pred_labels_list.append(labels)\n        pred_scores_list.append(scores)\n    result = eval_detection_voc(pred_bboxes=pred_boxes_list,\n                                pred_labels=pred_labels_list,\n                                pred_scores=pred_scores_list,\n                                gt_bboxes=gt_boxes_list,\n                                gt_labels=gt_labels_list,\n                                gt_difficults=gt_difficults,\n                                iou_thresh=0.5,\n                                use_07_metric=True)\n    if output_dir:\n        result_str = ""mAP: {:.4f}\\n"".format(result[""map""])\n        for i, ap in enumerate(result[""ap""]):\n            if i == 0:  # skip background\n                continue\n            result_str += ""{:<16}: {:.4f}\\n"".format(class_names[i], ap)\n        result_path = os.path.join(output_dir, ""result_{}.txt"".format(datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\')))\n        with open(result_path, ""w"") as f:\n            f.write(result_str)\n    return result'"
utilities/metrics/voc_helper.py,0,"b'#============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n#============================================\n\nfrom collections import defaultdict\nimport itertools\nimport numpy as np\nimport six\n\n\ndef bbox_iou(bbox_a, bbox_b):\n    """"""Calculate the Intersection of Unions (IoUs) between bounding boxes.\n    IoU is calculated as a ratio of area of the intersection\n    and area of the union.\n    This function accepts both :obj:`numpy.ndarray` and :obj:`cupy.ndarray` as\n    inputs. Please note that both :obj:`bbox_a` and :obj:`bbox_b` need to be\n    same type.\n    The output is same type as the type of the inputs.\n    Args:\n        bbox_a (array): An array whose shape is :math:`(N, 4)`.\n            :math:`N` is the number of bounding boxes.\n            The dtype should be :obj:`numpy.float32`.\n        bbox_b (array): An array similar to :obj:`bbox_a`,\n            whose shape is :math:`(K, 4)`.\n            The dtype should be :obj:`numpy.float32`.\n    Returns:\n        array:\n        An array whose shape is :math:`(N, K)`. \\\n        An element at index :math:`(n, k)` contains IoUs between \\\n        :math:`n` th bounding box in :obj:`bbox_a` and :math:`k` th bounding \\\n        box in :obj:`bbox_b`.\n    """"""\n    if bbox_a.shape[1] != 4 or bbox_b.shape[1] != 4:\n        raise IndexError\n\n    # top left\n    tl = np.maximum(bbox_a[:, None, :2], bbox_b[:, :2])\n    # bottom right\n    br = np.minimum(bbox_a[:, None, 2:], bbox_b[:, 2:])\n\n    area_i = np.prod(br - tl, axis=2) * (tl < br).all(axis=2)\n    area_a = np.prod(bbox_a[:, 2:] - bbox_a[:, :2], axis=1)\n    area_b = np.prod(bbox_b[:, 2:] - bbox_b[:, :2], axis=1)\n    return area_i / (area_a[:, None] + area_b - area_i)\n\n\ndef eval_detection_voc(\n        pred_bboxes,\n        pred_labels,\n        pred_scores,\n        gt_bboxes,\n        gt_labels,\n        gt_difficults=None,\n        iou_thresh=0.5,\n        use_07_metric=False):\n    """"""Calculate average precisions based on evaluation code of PASCAL VOC.\n\n    This function evaluates predicted bounding boxes obtained from a dataset\n    which has :math:`N` images by using average precision for each class.\n    The code is based on the evaluation code used in PASCAL VOC Challenge.\n\n    Args:\n        pred_bboxes (iterable of numpy.ndarray): An iterable of :math:`N`\n            sets of bounding boxes.\n            Its index corresponds to an index for the base dataset.\n            Each element of :obj:`pred_bboxes` is a set of coordinates\n            of bounding boxes. This is an array whose shape is :math:`(R, 4)`,\n            where :math:`R` corresponds\n            to the number of bounding boxes, which may vary among boxes.\n            The second axis corresponds to\n            :math:`y_{min}, x_{min}, y_{max}, x_{max}` of a bounding box.\n        pred_labels (iterable of numpy.ndarray): An iterable of labels.\n            Similar to :obj:`pred_bboxes`, its index corresponds to an\n            index for the base dataset. Its length is :math:`N`.\n        pred_scores (iterable of numpy.ndarray): An iterable of confidence\n            scores for predicted bounding boxes. Similar to :obj:`pred_bboxes`,\n            its index corresponds to an index for the base dataset.\n            Its length is :math:`N`.\n        gt_bboxes (iterable of numpy.ndarray): An iterable of ground truth\n            bounding boxes\n            whose length is :math:`N`. An element of :obj:`gt_bboxes` is a\n            bounding box whose shape is :math:`(R, 4)`. Note that the number of\n            bounding boxes in each image does not need to be same as the number\n            of corresponding predicted boxes.\n        gt_labels (iterable of numpy.ndarray): An iterable of ground truth\n            labels which are organized similarly to :obj:`gt_bboxes`.\n        gt_difficults (iterable of numpy.ndarray): An iterable of boolean\n            arrays which is organized similarly to :obj:`gt_bboxes`.\n            This tells whether the\n            corresponding ground truth bounding box is difficult or not.\n            By default, this is :obj:`None`. In that case, this function\n            considers all bounding boxes to be not difficult.\n        iou_thresh (float): A prediction is correct if its Intersection over\n            Union with the ground truth is above this value.\n        use_07_metric (bool): Whether to use PASCAL VOC 2007 evaluation metric\n            for calculating average precision. The default value is\n            :obj:`False`.\n\n    Returns:\n        dict:\n\n        The keys, value-types and the description of the values are listed\n        below.\n\n        * **ap** (*numpy.ndarray*): An array of average precisions. \\\n            The :math:`l`-th value corresponds to the average precision \\\n            for class :math:`l`. If class :math:`l` does not exist in \\\n            either :obj:`pred_labels` or :obj:`gt_labels`, the corresponding \\\n            value is set to :obj:`numpy.nan`.\n        * **map** (*float*): The average of Average Precisions over classes.\n\n    """"""\n\n    prec, rec = calc_detection_voc_prec_rec(pred_bboxes,\n                                            pred_labels,\n                                            pred_scores,\n                                            gt_bboxes,\n                                            gt_labels,\n                                            gt_difficults,\n                                            iou_thresh=iou_thresh)\n\n    ap = calc_detection_voc_ap(prec, rec, use_07_metric=use_07_metric)\n\n    return {\'ap\': ap, \'map\': np.nanmean(ap)}\n\n\ndef calc_detection_voc_prec_rec(\n        pred_bboxes, pred_labels, pred_scores, gt_bboxes, gt_labels,\n        gt_difficults=None,\n        iou_thresh=0.5):\n    """"""Calculate precision and recall based on evaluation code of PASCAL VOC.\n\n    This function calculates precision and recall of\n    predicted bounding boxes obtained from a dataset which has :math:`N`\n    images.\n    The code is based on the evaluation code used in PASCAL VOC Challenge.\n\n    Args:\n        pred_bboxes (iterable of numpy.ndarray): An iterable of :math:`N`\n            sets of bounding boxes.\n            Its index corresponds to an index for the base dataset.\n            Each element of :obj:`pred_bboxes` is a set of coordinates\n            of bounding boxes. This is an array whose shape is :math:`(R, 4)`,\n            where :math:`R` corresponds\n            to the number of bounding boxes, which may vary among boxes.\n            The second axis corresponds to\n            :math:`y_{min}, x_{min}, y_{max}, x_{max}` of a bounding box.\n        pred_labels (iterable of numpy.ndarray): An iterable of labels.\n            Similar to :obj:`pred_bboxes`, its index corresponds to an\n            index for the base dataset. Its length is :math:`N`.\n        pred_scores (iterable of numpy.ndarray): An iterable of confidence\n            scores for predicted bounding boxes. Similar to :obj:`pred_bboxes`,\n            its index corresponds to an index for the base dataset.\n            Its length is :math:`N`.\n        gt_bboxes (iterable of numpy.ndarray): An iterable of ground truth\n            bounding boxes\n            whose length is :math:`N`. An element of :obj:`gt_bboxes` is a\n            bounding box whose shape is :math:`(R, 4)`. Note that the number of\n            bounding boxes in each image does not need to be same as the number\n            of corresponding predicted boxes.\n        gt_labels (iterable of numpy.ndarray): An iterable of ground truth\n            labels which are organized similarly to :obj:`gt_bboxes`.\n        gt_difficults (iterable of numpy.ndarray): An iterable of boolean\n            arrays which is organized similarly to :obj:`gt_bboxes`.\n            This tells whether the\n            corresponding ground truth bounding box is difficult or not.\n            By default, this is :obj:`None`. In that case, this function\n            considers all bounding boxes to be not difficult.\n        iou_thresh (float): A prediction is correct if its Intersection over\n            Union with the ground truth is above this value..\n\n    Returns:\n        tuple of two lists:\n        This function returns two lists: :obj:`prec` and :obj:`rec`.\n\n        * :obj:`prec`: A list of arrays. :obj:`prec[l]` is precision \\\n            for class :math:`l`. If class :math:`l` does not exist in \\\n            either :obj:`pred_labels` or :obj:`gt_labels`, :obj:`prec[l]` is \\\n            set to :obj:`None`.\n        * :obj:`rec`: A list of arrays. :obj:`rec[l]` is recall \\\n            for class :math:`l`. If class :math:`l` that is not marked as \\\n            difficult does not exist in \\\n            :obj:`gt_labels`, :obj:`rec[l]` is \\\n            set to :obj:`None`.\n\n    """"""\n\n    pred_bboxes = iter(pred_bboxes)\n    pred_labels = iter(pred_labels)\n    pred_scores = iter(pred_scores)\n    gt_bboxes = iter(gt_bboxes)\n    gt_labels = iter(gt_labels)\n    if gt_difficults is None:\n        gt_difficults = itertools.repeat(None)\n    else:\n        gt_difficults = iter(gt_difficults)\n\n    n_pos = defaultdict(int)\n    score = defaultdict(list)\n    match = defaultdict(list)\n\n    for pred_bbox, pred_label, pred_score, gt_bbox, gt_label, gt_difficult in \\\n            six.moves.zip(\n                pred_bboxes, pred_labels, pred_scores,\n                gt_bboxes, gt_labels, gt_difficults):\n\n        if gt_difficult is None:\n            gt_difficult = np.zeros(gt_bbox.shape[0], dtype=bool)\n\n        for l in np.unique(np.concatenate((pred_label, gt_label)).astype(int)):\n            pred_mask_l = pred_label == l\n            pred_bbox_l = pred_bbox[pred_mask_l]\n            pred_score_l = pred_score[pred_mask_l]\n            # sort by score\n            order = pred_score_l.argsort()[::-1]\n            pred_bbox_l = pred_bbox_l[order]\n            pred_score_l = pred_score_l[order]\n\n            gt_mask_l = gt_label == l\n            gt_bbox_l = gt_bbox[gt_mask_l]\n            gt_difficult_l = gt_difficult[gt_mask_l]\n\n            n_pos[l] += np.logical_not(gt_difficult_l).sum()\n            score[l].extend(pred_score_l)\n\n            if len(pred_bbox_l) == 0:\n                continue\n            if len(gt_bbox_l) == 0:\n                match[l].extend((0,) * pred_bbox_l.shape[0])\n                continue\n\n            # VOC evaluation follows integer typed bounding boxes.\n            pred_bbox_l = pred_bbox_l.copy()\n            pred_bbox_l[:, 2:] += 1\n            gt_bbox_l = gt_bbox_l.copy()\n            gt_bbox_l[:, 2:] += 1\n\n            iou = bbox_iou(pred_bbox_l, gt_bbox_l)\n            gt_index = iou.argmax(axis=1)\n            # set -1 if there is no matching ground truth\n            gt_index[iou.max(axis=1) < iou_thresh] = -1\n            del iou\n\n            selec = np.zeros(gt_bbox_l.shape[0], dtype=bool)\n            for gt_idx in gt_index:\n                if gt_idx >= 0:\n                    if gt_difficult_l[gt_idx]:\n                        match[l].append(-1)\n                    else:\n                        if not selec[gt_idx]:\n                            match[l].append(1)\n                        else:\n                            match[l].append(0)\n                    selec[gt_idx] = True\n                else:\n                    match[l].append(0)\n\n    for iter_ in (\n            pred_bboxes, pred_labels, pred_scores,\n            gt_bboxes, gt_labels, gt_difficults):\n        if next(iter_, None) is not None:\n            raise ValueError(\'Length of input iterables need to be same.\')\n\n    n_fg_class = max(n_pos.keys()) + 1\n    prec = [None] * n_fg_class\n    rec = [None] * n_fg_class\n\n    for l in n_pos.keys():\n        score_l = np.array(score[l])\n        match_l = np.array(match[l], dtype=np.int8)\n\n        order = score_l.argsort()[::-1]\n        match_l = match_l[order]\n\n        tp = np.cumsum(match_l == 1)\n        fp = np.cumsum(match_l == 0)\n\n        # If an element of fp + tp is 0,\n        # the corresponding element of prec[l] is nan.\n        prec[l] = tp / (fp + tp)\n        # If n_pos[l] is 0, rec[l] is None.\n        if n_pos[l] > 0:\n            rec[l] = tp / n_pos[l]\n\n    return prec, rec\n\n\ndef calc_detection_voc_ap(prec, rec, use_07_metric=False):\n    """"""Calculate average precisions based on evaluation code of PASCAL VOC.\n\n    This function calculates average precisions\n    from given precisions and recalls.\n    The code is based on the evaluation code used in PASCAL VOC Challenge.\n\n    Args:\n        prec (list of numpy.array): A list of arrays.\n            :obj:`prec[l]` indicates precision for class :math:`l`.\n            If :obj:`prec[l]` is :obj:`None`, this function returns\n            :obj:`numpy.nan` for class :math:`l`.\n        rec (list of numpy.array): A list of arrays.\n            :obj:`rec[l]` indicates recall for class :math:`l`.\n            If :obj:`rec[l]` is :obj:`None`, this function returns\n            :obj:`numpy.nan` for class :math:`l`.\n        use_07_metric (bool): Whether to use PASCAL VOC 2007 evaluation metric\n            for calculating average precision. The default value is\n            :obj:`False`.\n\n    Returns:\n        ~numpy.ndarray:\n        This function returns an array of average precisions.\n        The :math:`l`-th value corresponds to the average precision\n        for class :math:`l`. If :obj:`prec[l]` or :obj:`rec[l]` is\n        :obj:`None`, the corresponding value is set to :obj:`numpy.nan`.\n\n    """"""\n\n    n_fg_class = len(prec)\n    ap = np.empty(n_fg_class)\n    for l in six.moves.range(n_fg_class):\n        if prec[l] is None or rec[l] is None:\n            ap[l] = np.nan\n            continue\n\n        if use_07_metric:\n            # 11 point metric\n            ap[l] = 0\n            for t in np.arange(0., 1.1, 0.1):\n                if np.sum(rec[l] >= t) == 0:\n                    p = 0\n                else:\n                    p = np.max(np.nan_to_num(prec[l])[rec[l] >= t])\n                ap[l] += p / 11\n        else:\n            # correct AP calculation\n            # first append sentinel values at the end\n            mpre = np.concatenate(([0], np.nan_to_num(prec[l]), [0]))\n            mrec = np.concatenate(([0], rec[l], [1]))\n\n            mpre = np.maximum.accumulate(mpre[::-1])[::-1]\n\n            # to calculate area under PR curve, look for points\n            # where X axis (recall) changes value\n            i = np.where(mrec[1:] != mrec[:-1])[0]\n\n            # and sum (\\Delta recall) * prec\n            ap[l] = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n\n    return ap\n'"
data_loader/segmentation/cityscape_scripts/__init__.py,0,b''
data_loader/segmentation/cityscape_scripts/generate_mappings.py,0,"b'# ============================================\n__author__ = ""Sachin Mehta""\n__maintainer__ = ""Sachin Mehta""\n# ============================================\n\nimport glob\nimport os\nfrom utilities.print_utils import *\n\ndef get_mappings(root_dir, files, annot_name):\n    pairs = []\n    for f in files:\n        f = f.replace(root_dir, \'/\')\n        img_f = f.replace(annot_name, \'leftImg8bit\')\n        img_f = img_f.replace(\'_labelTrainIds.png\', \'.png\')\n        if not os.path.isfile(root_dir + img_f):\n            print_error_message(\'{} file does not exist. Please check\'.format(root_dir + img_f))\n            exit()\n        line = img_f + \',\'  + f\n        pairs.append(line)\n    return pairs\n\ndef main(cityscapesPath, split):\n    searchFine = os.path.join(cityscapesPath, ""gtFine"", split, ""*"", \'*_labelTrainIds.png\')\n    filesFine = glob.glob(searchFine)\n    filesFine.sort()\n\n    if not filesFine:\n        print_warning_message(""Did not find any files. Please check root directory: {}."".format(cityscapesPath))\n        fine_pairs = []\n    else:\n        print_info_message(\'{} files found for {} split\'.format(len(filesFine), split))\n        fine_pairs = get_mappings(cityscapesPath, filesFine, \'gtFine\')\n\n    if not fine_pairs:\n        print_error_message(\'No pair exist. Exiting\')\n        exit()\n    else:\n        print_info_message(\'Creating train and val files.\')\n    f_name = split + \'.txt\'\n    with open(os.path.join(cityscapesPath, f_name), \'w\') as txtFile:\n        for pair in fine_pairs:\n            txtFile.write(pair + \'\\n\')\n    print_info_message(\'{} created in {} with {} pairs\'.format(f_name, cityscapesPath, len(fine_pairs)))\n\n    if split == \'train\':\n        split_orig = split\n        split = split + \'_extra\'\n        searchCoarse = os.path.join(cityscapesPath, ""gtCoarse"", split, ""*"", \'*_labelTrainIds.png\')\n        filesCoarse = glob.glob(searchCoarse)\n        filesCoarse.sort()\n        if not filesCoarse:\n            print_warning_message(""Did not find any files. Please check root directory: {}."".format(cityscapesPath))\n            course_pairs = []\n        else:\n            print_info_message(\'{} files found for {} split\'.format(len(filesCoarse), split))\n            course_pairs = get_mappings(cityscapesPath, filesCoarse, \'gtCoarse\')\n        if not course_pairs:\n            print_warning_message(\'No pair exist for coarse data\')\n            return\n        else:\n            print_info_message(\'Creating train and val files.\')\n        f_name = split_orig + \'_coarse.txt\'\n        with open(os.path.join(cityscapesPath, f_name), \'w\') as txtFile:\n            for pair in course_pairs:\n                txtFile.write(pair + \'\\n\')\n        print_info_message(\'{} created in {} with {} pairs\'.format(f_name, cityscapesPath, len(course_pairs)))\n\nif __name__ == \'__main__\':\n    cityscapes_path = \'../../../vision_datasets/cityscapes/\'\n    main(cityscapes_path, ""train"")\n    main(cityscapes_path, ""val"")'"
data_loader/segmentation/cityscape_scripts/process_cityscapes.py,0,"b'#!/usr/bin/python\n#\n# Converts the polygonal annotations of the Cityscapes dataset\n# to images, where pixel values encode ground truth classes.\n#\n# The Cityscapes downloads already include such images\n#   a) *color.png             : the class is encoded by its color\n#   b) *labelIds.png          : the class is encoded by its ID\n#   c) *instanceIds.png       : the class and the instance are encoded by an instance ID\n# \n# With this tool, you can generate option\n#   d) *labelTrainIds.png     : the class is encoded by its training ID\n# This encoding might come handy for training purposes. You can use\n# the file labes.py to define the training IDs that suit your needs.\n# Note however, that once you submit or evaluate results, the regular\n# IDs are needed.\n#\n# Uses the converter tool in \'json2labelImg.py\'\n# Uses the mapping defined in \'labels.py\'\n#\n\n# python imports\nfrom __future__ import print_function, absolute_import, division\nimport os, glob, sys\nfrom PIL import Image\nfrom PIL import ImageDraw\n## annotation file\nimport os\nimport json\nfrom collections import namedtuple\n\n# get current date and time\nimport datetime\nimport locale\n\n# A point in a polygon\nPoint = namedtuple(\'Point\', [\'x\', \'y\'])\n\nfrom abc import ABCMeta, abstractmethod\n\n# Type of an object\nclass CsObjectType():\n    POLY = 1 # polygon\n    BBOX = 2 # bounding box\n\n# Abstract base class for annotation objects\nclass CsObject:\n    __metaclass__ = ABCMeta\n\n    def __init__(self, objType):\n        self.objectType = objType\n        # the label\n        self.label    = """"\n\n        # If deleted or not\n        self.deleted  = 0\n        # If verified or not\n        self.verified = 0\n        # The date string\n        self.date     = """"\n        # The username\n        self.user     = """"\n        # Draw the object\n        # Not read from or written to JSON\n        # Set to False if deleted object\n        # Might be set to False by the application for other reasons\n        self.draw     = True\n\n    @abstractmethod\n    def __str__(self): pass\n\n    @abstractmethod\n    def fromJsonText(self, jsonText, objId=-1): pass\n\n    @abstractmethod\n    def toJsonText(self): pass\n\n    def updateDate( self ):\n        try:\n            locale.setlocale( locale.LC_ALL , \'en_US.utf8\' )\n        except locale.Error:\n            locale.setlocale( locale.LC_ALL , \'en_US\' )\n        except locale.Error:\n            locale.setlocale( locale.LC_ALL , \'us_us.utf8\' )\n        except locale.Error:\n            locale.setlocale( locale.LC_ALL , \'us_us\' )\n        except:\n            pass\n        self.date = datetime.datetime.now().strftime(""%d-%b-%Y %H:%M:%S"")\n\n    # Mark the object as deleted\n    def delete(self):\n        self.deleted = 1\n        self.draw    = False\n\n# Class that contains the information of a single annotated object as polygon\nclass CsPoly(CsObject):\n    # Constructor\n    def __init__(self):\n        CsObject.__init__(self, CsObjectType.POLY)\n        # the polygon as list of points\n        self.polygon    = []\n        # the object ID\n        self.id         = -1\n\n    def __str__(self):\n        polyText = """"\n        if self.polygon:\n            if len(self.polygon) <= 4:\n                for p in self.polygon:\n                    polyText += \'({},{}) \'.format( p.x , p.y )\n            else:\n                polyText += \'({},{}) ({},{}) ... ({},{}) ({},{})\'.format(\n                    self.polygon[ 0].x , self.polygon[ 0].y ,\n                    self.polygon[ 1].x , self.polygon[ 1].y ,\n                    self.polygon[-2].x , self.polygon[-2].y ,\n                    self.polygon[-1].x , self.polygon[-1].y )\n        else:\n            polyText = ""none""\n        text = ""Object: {} - {}"".format( self.label , polyText )\n        return text\n\n    def fromJsonText(self, jsonText, objId):\n        self.id = objId\n        self.label = str(jsonText[\'label\'])\n        self.polygon = [ Point(p[0],p[1]) for p in jsonText[\'polygon\'] ]\n        if \'deleted\' in jsonText.keys():\n            self.deleted = jsonText[\'deleted\']\n        else:\n            self.deleted = 0\n        if \'verified\' in jsonText.keys():\n            self.verified = jsonText[\'verified\']\n        else:\n            self.verified = 1\n        if \'user\' in jsonText.keys():\n            self.user = jsonText[\'user\']\n        else:\n            self.user = \'\'\n        if \'date\' in jsonText.keys():\n            self.date = jsonText[\'date\']\n        else:\n            self.date = \'\'\n        if self.deleted == 1:\n            self.draw = False\n        else:\n            self.draw = True\n\n    def toJsonText(self):\n        objDict = {}\n        objDict[\'label\'] = self.label\n        objDict[\'id\'] = self.id\n        objDict[\'deleted\'] = self.deleted\n        objDict[\'verified\'] = self.verified\n        objDict[\'user\'] = self.user\n        objDict[\'date\'] = self.date\n        objDict[\'polygon\'] = []\n        for pt in self.polygon:\n            objDict[\'polygon\'].append([pt.x, pt.y])\n\n        return objDict\n\n# Class that contains the information of a single annotated object as bounding box\nclass CsBbox(CsObject):\n    # Constructor\n    def __init__(self):\n        CsObject.__init__(self, CsObjectType.BBOX)\n        # the polygon as list of points\n        self.bbox  = []\n        self.bboxVis  = []\n\n        # the ID of the corresponding object\n        self.instanceId = -1\n\n    def __str__(self):\n        bboxText = """"\n        bboxText += \'[(x1: {}, y1: {}), (w: {}, h: {})]\'.format(\n            self.bbox[0] , self.bbox[1] ,  self.bbox[2] ,  self.bbox[3] )\n\n        bboxVisText = """"\n        bboxVisText += \'[(x1: {}, y1: {}), (w: {}, h: {})]\'.format(\n            self.bboxVis[0] , self.bboxVis[1] , self.bboxVis[2], self.bboxVis[3] )\n\n        text = ""Object: {} - bbox {} - visible {}"".format( self.label , bboxText, bboxVisText )\n        return text\n\n    def fromJsonText(self, jsonText, objId=-1):\n        self.bbox = jsonText[\'bbox\']\n        self.bboxVis = jsonText[\'bboxVis\']\n        self.label = str(jsonText[\'label\'])\n        self.instanceId = jsonText[\'instanceId\']\n\n    def toJsonText(self):\n        objDict = {}\n        objDict[\'label\'] = self.label\n        objDict[\'instanceId\'] = self.instanceId\n        objDict[\'bbox\'] = self.bbox\n        objDict[\'bboxVis\'] = self.bboxVis\n\n        return objDict\n\n# The annotation of a whole image (doesn\'t support mixed annotations, i.e. combining CsPoly and CsBbox)\nclass Annotation:\n    # Constructor\n    def __init__(self, objType=CsObjectType.POLY):\n        # the width of that image and thus of the label image\n        self.imgWidth  = 0\n        # the height of that image and thus of the label image\n        self.imgHeight = 0\n        # the list of objects\n        self.objects = []\n        assert objType in CsObjectType.__dict__.values()\n        self.objectType = objType\n\n    def toJson(self):\n        return json.dumps(self, default=lambda o: o.__dict__, sort_keys=True, indent=4)\n\n    def fromJsonText(self, jsonText):\n        jsonDict = json.loads(jsonText)\n        self.imgWidth  = int(jsonDict[\'imgWidth\'])\n        self.imgHeight = int(jsonDict[\'imgHeight\'])\n        self.objects   = []\n        for objId, objIn in enumerate(jsonDict[ \'objects\' ]):\n            if self.objectType == CsObjectType.POLY:\n                obj = CsPoly()\n            elif self.objectType == CsObjectType.BBOX:\n                obj = CsBbox()\n            obj.fromJsonText(objIn, objId)\n            self.objects.append(obj)\n\n    def toJsonText(self):\n        jsonDict = {}\n        jsonDict[\'imgWidth\'] = self.imgWidth\n        jsonDict[\'imgHeight\'] = self.imgHeight\n        jsonDict[\'objects\'] = []\n        for obj in self.objects:\n            objDict = obj.toJsonText()\n            jsonDict[\'objects\'].append(objDict)\n\n        return jsonDict\n\n    # Read a json formatted polygon file and return the annotation\n    def fromJsonFile(self, jsonFile):\n        if not os.path.isfile(jsonFile):\n            print(\'Given json file not found: {}\'.format(jsonFile))\n            return\n        with open(jsonFile, \'r\') as f:\n            jsonText = f.read()\n            self.fromJsonText(jsonText)\n\n    def toJsonFile(self, jsonFile):\n        with open(jsonFile, \'w\') as f:\n            f.write(self.toJson())\n\n## from labels.py file\n# a label and all meta information\nLabel = namedtuple( \'Label\' , [\n\n    \'name\'        , # The identifier of this label, e.g. \'car\', \'person\', ... .\n                    # We use them to uniquely name a class\n\n    \'id\'          , # An integer ID that is associated with this label.\n                    # The IDs are used to represent the label in ground truth images\n                    # An ID of -1 means that this label does not have an ID and thus\n                    # is ignored when creating ground truth images (e.g. license plate).\n                    # Do not modify these IDs, since exactly these IDs are expected by the\n                    # evaluation server.\n\n    \'trainId\'     , # Feel free to modify these IDs as suitable for your method. Then create\n                    # ground truth images with train IDs, using the tools provided in the\n                    # \'preparation\' folder. However, make sure to validate or submit results\n                    # to our evaluation server using the regular IDs above!\n                    # For trainIds, multiple labels might have the same ID. Then, these labels\n                    # are mapped to the same class in the ground truth images. For the inverse\n                    # mapping, we use the label that is defined first in the list below.\n                    # For example, mapping all void-type classes to the same ID in training,\n                    # might make sense for some approaches.\n                    # Max value is 255!\n\n    \'category\'    , # The name of the category that this label belongs to\n\n    \'categoryId\'  , # The ID of this category. Used to create ground truth images\n                    # on category level.\n\n    \'hasInstances\', # Whether this label distinguishes between single instances or not\n\n    \'ignoreInEval\', # Whether pixels having this class as ground truth label are ignored\n                    # during evaluations or not\n\n    \'color\'       , # The color of this label\n    ] )\n\n\n#--------------------------------------------------------------------------------\n# A list of all labels\n#--------------------------------------------------------------------------------\n\n# Please adapt the train IDs as appropriate for your approach.\n# Note that you might want to ignore labels with ID 255 during training.\n# Further note that the current train IDs are only a suggestion. You can use whatever you like.\n# Make sure to provide your results using the original IDs and not the training IDs.\n# Note that many IDs are ignored in evaluation and thus you never need to predict these!\n\nlabels = [\n    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n    Label(  \'unlabeled\'            ,  0 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'ego vehicle\'          ,  1 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'rectification border\' ,  2 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'out of roi\'           ,  3 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'static\'               ,  4 ,      255 , \'void\'            , 0       , False        , True         , (  0,  0,  0) ),\n    Label(  \'dynamic\'              ,  5 ,      255 , \'void\'            , 0       , False        , True         , (111, 74,  0) ),\n    Label(  \'ground\'               ,  6 ,      255 , \'void\'            , 0       , False        , True         , ( 81,  0, 81) ),\n    Label(  \'road\'                 ,  7 ,        0 , \'flat\'            , 1       , False        , False        , (128, 64,128) ),\n    Label(  \'sidewalk\'             ,  8 ,        1 , \'flat\'            , 1       , False        , False        , (244, 35,232) ),\n    Label(  \'parking\'              ,  9 ,      255 , \'flat\'            , 1       , False        , True         , (250,170,160) ),\n    Label(  \'rail track\'           , 10 ,      255 , \'flat\'            , 1       , False        , True         , (230,150,140) ),\n    Label(  \'building\'             , 11 ,        2 , \'construction\'    , 2       , False        , False        , ( 70, 70, 70) ),\n    Label(  \'wall\'                 , 12 ,        3 , \'construction\'    , 2       , False        , False        , (102,102,156) ),\n    Label(  \'fence\'                , 13 ,        4 , \'construction\'    , 2       , False        , False        , (190,153,153) ),\n    Label(  \'guard rail\'           , 14 ,      255 , \'construction\'    , 2       , False        , True         , (180,165,180) ),\n    Label(  \'bridge\'               , 15 ,      255 , \'construction\'    , 2       , False        , True         , (150,100,100) ),\n    Label(  \'tunnel\'               , 16 ,      255 , \'construction\'    , 2       , False        , True         , (150,120, 90) ),\n    Label(  \'pole\'                 , 17 ,        5 , \'object\'          , 3       , False        , False        , (153,153,153) ),\n    Label(  \'polegroup\'            , 18 ,      255 , \'object\'          , 3       , False        , True         , (153,153,153) ),\n    Label(  \'traffic light\'        , 19 ,        6 , \'object\'          , 3       , False        , False        , (250,170, 30) ),\n    Label(  \'traffic sign\'         , 20 ,        7 , \'object\'          , 3       , False        , False        , (220,220,  0) ),\n    Label(  \'vegetation\'           , 21 ,        8 , \'nature\'          , 4       , False        , False        , (107,142, 35) ),\n    Label(  \'terrain\'              , 22 ,        9 , \'nature\'          , 4       , False        , False        , (152,251,152) ),\n    Label(  \'sky\'                  , 23 ,       10 , \'sky\'             , 5       , False        , False        , ( 70,130,180) ),\n    Label(  \'person\'               , 24 ,       11 , \'human\'           , 6       , True         , False        , (220, 20, 60) ),\n    Label(  \'rider\'                , 25 ,       12 , \'human\'           , 6       , True         , False        , (255,  0,  0) ),\n    Label(  \'car\'                  , 26 ,       13 , \'vehicle\'         , 7       , True         , False        , (  0,  0,142) ),\n    Label(  \'truck\'                , 27 ,       14 , \'vehicle\'         , 7       , True         , False        , (  0,  0, 70) ),\n    Label(  \'bus\'                  , 28 ,       15 , \'vehicle\'         , 7       , True         , False        , (  0, 60,100) ),\n    Label(  \'caravan\'              , 29 ,      255 , \'vehicle\'         , 7       , True         , True         , (  0,  0, 90) ),\n    Label(  \'trailer\'              , 30 ,      255 , \'vehicle\'         , 7       , True         , True         , (  0,  0,110) ),\n    Label(  \'train\'                , 31 ,       16 , \'vehicle\'         , 7       , True         , False        , (  0, 80,100) ),\n    Label(  \'motorcycle\'           , 32 ,       17 , \'vehicle\'         , 7       , True         , False        , (  0,  0,230) ),\n    Label(  \'bicycle\'              , 33 ,       18 , \'vehicle\'         , 7       , True         , False        , (119, 11, 32) ),\n    Label(  \'license plate\'        , -1 ,       -1 , \'vehicle\'         , 7       , False        , True         , (  0,  0,142) ),\n]\n\n\n#--------------------------------------------------------------------------------\n# Create dictionaries for a fast lookup\n#--------------------------------------------------------------------------------\n\n# Please refer to the main method below for example usages!\n\n# name to label object\nname2label      = { label.name    : label for label in labels           }\n# id to label object\nid2label        = { label.id      : label for label in labels           }\n# trainId to label object\ntrainId2label   = { label.trainId : label for label in reversed(labels) }\n# category to list of label objects\ncategory2labels = {}\nfor label in labels:\n    category = label.category\n    if category in category2labels:\n        category2labels[category].append(label)\n    else:\n        category2labels[category] = [label]\n\n# Convert the given annotation to a label image\ndef createLabelImage(annotation, encoding, outline=None):\n    # the size of the image\n    size = ( annotation.imgWidth , annotation.imgHeight )\n\n    # the background\n    if encoding == ""ids"":\n        background = name2label[\'unlabeled\'].id\n    elif encoding == ""trainIds"":\n        background = name2label[\'unlabeled\'].trainId\n    elif encoding == ""color"":\n        background = name2label[\'unlabeled\'].color\n    else:\n        print(""Unknown encoding \'{}\'"".format(encoding))\n        return None\n\n    # this is the image that we want to create\n    if encoding == ""color"":\n        labelImg = Image.new(""RGBA"", size, background)\n    else:\n        labelImg = Image.new(""L"", size, background)\n\n    # a drawer to draw into the image\n    drawer = ImageDraw.Draw( labelImg )\n\n    # loop over all objects\n    for obj in annotation.objects:\n        label   = obj.label\n        polygon = obj.polygon\n\n        # If the object is deleted, skip it\n        if obj.deleted:\n            continue\n\n        # If the label is not known, but ends with a \'group\' (e.g. cargroup)\n        # try to remove the s and see if that works\n        if ( not label in name2label ) and label.endswith(\'group\'):\n            label = label[:-len(\'group\')]\n\n        if not label in name2label:\n            print( ""Label \'{}\' not known."".format(label) )\n\n        # If the ID is negative that polygon should not be drawn\n        if name2label[label].id < 0:\n            continue\n\n        if encoding == ""ids"":\n            val = name2label[label].id\n        elif encoding == ""trainIds"":\n            val = name2label[label].trainId\n        elif encoding == ""color"":\n            val = name2label[label].color\n\n        try:\n            if outline:\n                drawer.polygon( polygon, fill=val, outline=outline )\n            else:\n                drawer.polygon( polygon, fill=val )\n        except:\n            print(""Failed to draw polygon with label {}"".format(label))\n            raise\n\n    return labelImg\n\n# A method that does all the work\n# inJson is the filename of the json file\n# outImg is the filename of the label image that is generated\n# encoding can be set to\n#     - ""ids""      : classes are encoded using the regular label IDs\n#     - ""trainIds"" : classes are encoded using the training IDs\n#     - ""color""    : classes are encoded using the corresponding colors\ndef json2labelImg(inJson,outImg,encoding=""ids""):\n    annotation = Annotation()\n    annotation.fromJsonFile(inJson)\n    labelImg   = createLabelImage( annotation , encoding )\n    labelImg.save( outImg )\n\n# The main method\ndef main(cityscapesPath):\n    # how to search for all ground truth\n    searchFine   = os.path.join( cityscapesPath , ""gtFine""   , ""*"" , ""*"" , ""*_gt*_polygons.json"" )\n    searchCoarse = os.path.join( cityscapesPath , ""gtCoarse"" , ""*"" , ""*"" , ""*_gt*_polygons.json"" )\n\n    # search files\n    filesFine = glob.glob( searchFine )\n    filesFine.sort()\n    filesCoarse = glob.glob( searchCoarse )\n    filesCoarse.sort()\n\n    # concatenate fine and coarse\n    files = filesFine + filesCoarse\n    # files = filesFine # use this line if fine is enough for now.\n\n    # quit if we did not find anything\n    if not files:\n        print( ""Did not find any files. Please consult the README."" )\n\n    # a bit verbose\n    print(""Processing {} annotation files"".format(len(files)))\n\n    # iterate through files\n    progress = 0\n    print(""Progress: {:>3} %"".format( progress * 100 / len(files) ), end=\' \')\n    for f in files:\n        # create the output filename\n        dst = f.replace( ""_polygons.json"" , ""_labelTrainIds.png"" )\n\n        # do the conversion\n        try:\n            json2labelImg( f , dst , ""trainIds"" )\n        except:\n            print(""Failed to convert: {}"".format(f))\n            raise\n\n        # status\n        progress += 1\n        print(""\\rProgress: {:>3} %"".format( progress * 100 / len(files) ), end=\' \')\n        sys.stdout.flush()\n\n\n# call the main\nif __name__ == ""__main__"":\n    cityscapes_path = \'../../../vision_datasets/cityscapes/\'\n    main(cityscapes_path)\n'"
model/classification/model_zoo/espnetv2/__init__.py,0,b''
model/classification/model_zoo/shufflenetv2/__init__.py,0,b''
