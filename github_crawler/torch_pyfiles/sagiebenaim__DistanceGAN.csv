file_path,api_count,code
test.py,0,"b""import os\nimport os\n\nfrom cyclegan_arch.cyclegan_arch_options.test_options import TestOptions\n\nopt = TestOptions().parse()  # set CUDA_VISIBLE_DEVICES before import torch\n\nfrom cyclegan_arch.data.data_loader import CreateDataLoader\nfrom cyclegan_arch.models import create_model\nfrom cyclegan_arch.util.visualizer import Visualizer\nfrom cyclegan_arch.util import html\n\nopt.nThreads = 1   # test code only supports nThreads=1\nopt.batchSize = 1  #test code only supports batchSize=1\nopt.serial_batches = True # no shuffle\n\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\n\nmodel = create_model(opt)\nvisualizer = Visualizer(opt)\n# create website\nweb_dir = os.path.join(opt.results_dir, opt.name, '%s_%s' % (opt.phase, opt.which_epoch))\nwebpage = html.HTML(web_dir, 'Experiment = %s, Phase = %s, Epoch = %s' % (opt.name, opt.phase, opt.which_epoch))\n# test\nfor i, data in enumerate(dataset):\n    if i >= opt.how_many:\n        break\n    model.set_input(data)\n    model.test()\n    visuals = model.get_current_visuals()\n    img_path = model.get_image_paths()\n    print('process image... %s' % img_path)\n    visualizer.save_images(webpage, visuals, img_path)\n\nwebpage.save()\n"""
train.py,0,"b""import time\nfrom cyclegan_arch.cyclegan_arch_options.train_options import TrainOptions\nopt = TrainOptions().parse()  # set CUDA_VISIBLE_DEVICES before import torch\n\nfrom cyclegan_arch.data.data_loader import CreateDataLoader\nfrom cyclegan_arch.models import create_model\nfrom cyclegan_arch.util.visualizer import Visualizer\n\ndata_loader = CreateDataLoader(opt)\ndataset = data_loader.load_data()\ndataset_size = len(data_loader)\nprint('#training images = %d' % dataset_size)\n\nmodel = create_model(opt, dataset)\nvisualizer = Visualizer(opt)\n\ntotal_steps = 0\n\nfor epoch in range(1, opt.niter + opt.niter_decay + 1):\n    epoch_start_time = time.time()\n    for i, data in enumerate(dataset):\n        iter_start_time = time.time()\n        total_steps += opt.batchSize\n        epoch_iter = total_steps - dataset_size * (epoch - 1)\n\n        # If data doesn't devide batch size this causes batches of size 1\n        # which are problamatic for distance comparison\n        if len(data['A']) != opt.batchSize or len(data['B']) != opt.batchSize:\n            continue\n\n        model.set_input(data)\n        model.optimize_parameters()\n\n        if total_steps % opt.display_freq == 0:\n            visualizer.display_current_results(model.get_current_visuals(), epoch)\n\n        if total_steps % opt.print_freq == 0:\n            errors = model.get_current_errors()\n            t = (time.time() - iter_start_time) / opt.batchSize\n            visualizer.print_current_errors(epoch, epoch_iter, errors, t)\n            if opt.display_id > 0:\n                visualizer.plot_current_errors(epoch, float(epoch_iter)/dataset_size, opt, errors)\n\n        if total_steps % opt.save_latest_freq == 0:\n            print('saving the latest model (epoch %d, total_steps %d)' %\n                  (epoch, total_steps))\n            model.save('latest')\n\n    if epoch % opt.save_epoch_freq == 0:\n        print('saving the model at the end of epoch %d, iters %d' %\n              (epoch, total_steps))\n        model.save('latest')\n        model.save(epoch)\n\n    print('End of epoch %d / %d \\t Time Taken: %d sec' %\n          (epoch, opt.niter + opt.niter_decay, time.time() - epoch_start_time))\n\n    if epoch > opt.niter:\n        model.update_learning_rate()\n"""
cyclegan_arch/__init__.py,0,b''
cyclegan_arch/base_model.py,4,"b""import os\nimport torch\n\nclass BaseModel(object):\n    def name(self):\n        return 'BaseModel'\n\n    def get_image_paths(self):\n        pass\n\n    def initialize(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.isTrain = opt.isTrain\n        self.Tensor = torch.cuda.FloatTensor if self.gpu_ids else torch.Tensor\n        self.save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n\n    def set_input(self, input):\n        self.input = input\n\n    def forward(self):\n        pass\n\n    # used in test time, no backprop\n    def test(self):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        return self.input\n\n    def get_current_errors(self):\n        return {}\n\n    def save(self, label):\n        pass\n\n    # helper saving function that can be used by subclasses\n    def save_network(self, network, network_label, epoch_label, gpu_ids):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        torch.save(network.cpu().state_dict(), save_path)\n        if len(gpu_ids) and torch.cuda.is_available():\n            network.cuda(device_id=gpu_ids[0])\n\n    # helper loading function that can be used by subclasses\n    def load_network(self, network, network_label, epoch_label):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        network.load_state_dict(torch.load(save_path))\n\n    def update_learning_rate(self):\n        pass\n\n    def as_np(self, data):\n        return data.cpu().data.numpy()\n"""
cyclegan_arch/cycle_gan_model.py,3,"b""from collections import OrderedDict\n\nimport torch\nfrom torch.autograd import Variable\n\nfrom util import util as util\nfrom .gan_model import GANModel\n\n\nclass CycleGANModel(GANModel):\n    def name(self):\n        return 'CycleGANModel'\n\n    def initialize(self, opt):\n\n        GANModel.initialize(self, opt)\n\n        if self.isTrain:\n            self.criterionCycle = torch.nn.L1Loss()\n            self.criterionIdt = torch.nn.L1Loss()\n\n    def test(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        self.fake_B = self.netG_A.forward(self.real_A)\n        self.rec_A = self.netG_B.forward(self.fake_B)\n\n        self.real_B = Variable(self.input_B, volatile=True)\n        self.fake_A = self.netG_B.forward(self.real_B)\n        self.rec_B  = self.netG_A.forward(self.fake_A)\n\n    def backward_G(self):\n        lambda_idt = self.opt.identity\n        lambda_A = self.opt.lambda_A\n        lambda_B = self.opt.lambda_B\n        # Identity loss\n        if lambda_idt > 0:\n            # G_A should be identity if real_B is fed.\n            self.idt_A = self.netG_A.forward(self.real_B)\n            self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt\n            # G_B should be identity if real_A is fed.\n            self.idt_B = self.netG_B.forward(self.real_A)\n            self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt\n        else:\n            self.loss_idt_A = 0\n            self.loss_idt_B = 0\n\n        # GAN loss\n        # D_A(G_A(A))\n        self.fake_B = self.netG_A.forward(self.real_A)\n        pred_fake = self.netD_A.forward(self.fake_B)\n        self.loss_G_A = self.criterionGAN(pred_fake, True)\n        # D_B(G_B(B))\n        self.fake_A = self.netG_B.forward(self.real_B)\n        pred_fake = self.netD_B.forward(self.fake_A)\n        self.loss_G_B = self.criterionGAN(pred_fake, True)\n        # Forward cycle loss\n        self.rec_A = self.netG_B.forward(self.fake_B)\n        self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A\n        # Backward cycle loss\n        self.rec_B = self.netG_A.forward(self.fake_A)\n        self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B\n        # combined loss\n        self.loss_G = self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B\n        self.loss_G.backward()\n\n\n    def get_current_errors(self):\n        D_A = self.loss_D_A.data[0]\n        G_A = self.loss_G_A.data[0]\n        Cyc_A = self.loss_cycle_A.data[0]\n        D_B = self.loss_D_B.data[0]\n        G_B = self.loss_G_B.data[0]\n        Cyc_B = self.loss_cycle_B.data[0]\n        if self.opt.identity > 0.0:\n            idt_A = self.loss_idt_A.data[0]\n            idt_B = self.loss_idt_B.data[0]\n            return OrderedDict([('D_A', D_A), ('G_A', G_A), ('Cyc_A', Cyc_A), ('idt_A', idt_A),\n                                ('D_B', D_B), ('G_B', G_B), ('Cyc_B', Cyc_B), ('idt_B', idt_B)])\n        else:\n            return OrderedDict([('D_A', D_A), ('G_A', G_A), ('Cyc_A', Cyc_A),\n                                ('D_B', D_B), ('G_B', G_B), ('Cyc_B', Cyc_B)])\n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n        rec_A  = util.tensor2im(self.rec_A.data)\n        real_B = util.tensor2im(self.real_B.data)\n        fake_A = util.tensor2im(self.fake_A.data)\n        rec_B  = util.tensor2im(self.rec_B.data)\n        if self.opt.identity > 0.0:\n            idt_A = util.tensor2im(self.idt_A.data)\n            idt_B = util.tensor2im(self.idt_B.data)\n            return OrderedDict([('real_A', real_A), ('fake_B', fake_B), ('rec_A', rec_A), ('idt_B', idt_B),\n                                ('real_B', real_B), ('fake_A', fake_A), ('rec_B', rec_B), ('idt_A', idt_A)])\n        else:\n            return OrderedDict([('real_A', real_A), ('fake_B', fake_B), ('rec_A', rec_A),\n                                ('real_B', real_B), ('fake_A', fake_A), ('rec_B', rec_B)])\n"""
cyclegan_arch/distance_gan_model.py,13,"b""from collections import OrderedDict\n\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\nfrom util import util as util\nfrom .cycle_gan_model import CycleGANModel\n\n\nclass DistanceGANModel(CycleGANModel):\n    def __init__(self, dataset):\n        super(CycleGANModel, self).__init__()\n        self.dataset = dataset\n\n    def name(self):\n        return 'DistanceGANModel'\n\n    def initialize(self, opt):\n\n        CycleGANModel.initialize(self, opt)\n\n        self.use_self_distance = opt.use_self_distance\n        self.normalize_distances = not opt.unnormalized_distances\n\n        if self.isTrain and self.normalize_distances:\n            self.set_expectation_and_std()\n\n\n    def distance(self, A, B):\n        return torch.mean(torch.abs(A - B))\n\n    def get_individual_distance_loss(self, A_i, A_j, AB_i, AB_j,\n                                     B_i, B_j, BA_i, BA_j):\n\n        distance_in_A = self.distance(A_i, A_j)\n        distance_in_AB = self.distance(AB_i, AB_j)\n        distance_in_B = self.distance(B_i, B_j)\n        distance_in_BA = self.distance(BA_i, BA_j)\n\n        if self.normalize_distances:\n            distance_in_A = (distance_in_A - self.expectation_A) / self.std_A\n            distance_in_AB = (distance_in_AB - self.expectation_B) / self.std_B\n            distance_in_B = (distance_in_B - self.expectation_B) / self.std_B\n            distance_in_BA = (distance_in_BA - self.expectation_A) / self.std_A\n\n        return torch.abs(distance_in_A - distance_in_AB), torch.abs(distance_in_B - distance_in_BA)\n\n    def get_self_distances(self):\n\n        A_half_1, A_half_2 = torch.chunk(self.real_A, 2, dim=2)\n        B_half_1, B_half_2 = torch.chunk(self.real_B, 2, dim=2)\n        AB_half_1, AB_half_2 = torch.chunk(self.fake_B, 2, dim=2)\n        BA_half_1, BA_half_2 = torch.chunk(self.fake_A, 2, dim=2)\n\n        l_distance_A, l_distance_B = \\\n            self.get_individual_distance_loss(A_half_1, A_half_2,\n                                              AB_half_1, AB_half_2,\n                                              B_half_1, B_half_2,\n                                              BA_half_1, BA_half_2)\n\n        return l_distance_A, l_distance_B\n\n    def get_distance_losses(self):\n\n        As = torch.split(self.real_A, 1)\n        Bs = torch.split(self.real_B, 1)\n        ABs = torch.split(self.fake_B, 1)\n        BAs = torch.split(self.fake_A, 1)\n\n        loss_distance_A = 0.0\n        loss_distance_B = 0.0\n        num_pairs = 0\n        min_length = min(len(As), len(Bs))\n\n        for i in xrange(min_length - 1):\n            for j in xrange(i + 1, min_length):\n                num_pairs += 1\n                loss_distance_A_ij, loss_distance_B_ij = \\\n                    self.get_individual_distance_loss(As[i], As[j],\n                                                      ABs[i], ABs[j],\n                                                      Bs[i], Bs[j],\n                                                      BAs[i], BAs[j])\n\n                loss_distance_A += loss_distance_A_ij\n                loss_distance_B += loss_distance_B_ij\n\n        loss_distance_A = loss_distance_A / num_pairs\n        loss_distance_B = loss_distance_B / num_pairs\n\n        return loss_distance_A, loss_distance_B\n\n    def get_std(self, num_items, vars, expectation):\n\n        num_pairs = 0\n        std_sum = 0.0\n\n        # If self distance computed std for top and bottom half\n        if self.use_self_distance:\n            for i in xrange(num_items):\n                var_half_1, var_half_2 = torch.chunk(vars[i], 2, dim=2)\n                std_sum += np.square(self.as_np(self.distance(var_half_1, var_half_2)) - expectation)\n            return np.sqrt(std_sum / num_items)\n\n        # Otherwise compute std for all pairs of images\n        for i in xrange(num_items - 1):\n            for j in xrange(i + 1, num_items):\n                num_pairs += 1\n                std_sum += np.square(self.as_np(self.distance(vars[i], vars[j])) - expectation)\n\n        return np.sqrt(std_sum / num_pairs)\n\n    def get_expectation(self, num_items, vars):\n\n        num_pairs = 0\n        distance_sum = 0.0\n\n        # If self distance computed expectation for top and bottom half\n        if self.use_self_distance:\n            for i in xrange(num_items):\n                # Split image to top and bottom half\n                var_half_1, var_half_2 = torch.chunk(vars[i], 2, dim=2)\n                distance_sum += self.as_np(self.distance(var_half_1, var_half_2))\n            return distance_sum / num_items\n\n        # Otherwise compute expectation for all pairs of images\n        for i in xrange(num_items - 1):\n            for j in xrange(i + 1, num_items):\n                num_pairs += 1\n                distance_sum += self.as_np(self.distance(vars[i], vars[j]))\n\n        return distance_sum / num_pairs\n\n    def set_expectation_and_std(self):\n\n        max_items = self.opt.max_items\n\n        A_vars = []\n        B_vars = []\n        num_vars_A = 0\n        num_vars_B = 0\n        for i, data in enumerate(self.dataset):\n\n            if (self.dataset.stop_A and self.dataset.stop_B) or i >= max_items:\n                break\n\n            if not self.dataset.stop_A:\n                A = Variable(data['A'], volatile=True)\n\n                # If reached end of dataset, variable sizes may be different\n                # We check this and not take these variables into account\n                # when calculating expectation and std\n                if A.size()[0] != self.opt.batchSize:\n                    continue\n\n                A_vars.append(A)\n                num_vars_A += 1\n\n            if not self.dataset.stop_B:\n                B = Variable(data['B'], volatile=True)\n\n                if B.size()[0] != self.opt.batchSize:\n                    continue\n\n                B_vars.append(B)\n                num_vars_B += 1\n\n        self.expectation_A = self.get_expectation(num_vars_A, A_vars)[0].astype(float)\n        self.expectation_B = self.get_expectation(num_vars_B, B_vars)[0].astype(float)\n        self.std_A = self.get_std(num_vars_A, A_vars, self.expectation_A)[0].astype(float)\n        self.std_B = self.get_std(num_vars_B, B_vars, self.expectation_B)[0].astype(float)\n\n        print('Expectation for dataset A: %f' % self.expectation_A)\n        print('Expectation for dataset B: %f' % self.expectation_B)\n        print('Std for dataset A: %f' % self.std_A)\n        print('Std for dataset B: %f' % self.std_B)\n\n    def backward_G(self):\n\n        # D_A(G_A(A))\n        self.fake_B = self.netG_A.forward(self.real_A)\n        pred_fake = self.netD_A.forward(self.fake_B)\n        self.loss_G_A = self.criterionGAN(pred_fake, True)\n        # D_B(G_B(B))\n        self.fake_A = self.netG_B.forward(self.real_B)\n        pred_fake = self.netD_B.forward(self.fake_A)\n        self.loss_G_B = self.criterionGAN(pred_fake, True)\n\n        if self.use_self_distance:\n            self.loss_distance_A, self.loss_distance_B = self.get_self_distances()\n        else:\n            self.loss_distance_A, self.loss_distance_B = self.get_distance_losses()\n\n        self.loss_distance_A *= self.opt.lambda_distance_A\n        self.loss_distance_B *= self.opt.lambda_distance_B\n\n        if self.A_to_B:\n            self.loss_G = self.loss_G_A + self.loss_distance_A\n        elif self.B_to_A:\n            self.loss_G = self.loss_G_B + self.loss_distance_B\n        else:\n            self.loss_G = self.loss_G_A + self.loss_G_B + self.loss_distance_A + self.loss_distance_B\n\n        if self.opt.use_cycle_loss:\n            # Forward cycle loss\n            self.rec_A = self.netG_B.forward(self.fake_B)\n            self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * self.opt.lambda_A\n            # Backward cycle loss\n            self.rec_B = self.netG_A.forward(self.fake_A)\n            self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * self.opt.lambda_B\n\n            self.loss_G += self.loss_cycle_A + self.loss_cycle_B\n\n        self.loss_G.backward()\n\n    def get_current_errors(self):\n        if self.opt.use_cycle_loss:\n            return CycleGANModel.get_current_errors(self)\n\n        D_A = self.loss_D_A.data[0]\n        G_A = self.loss_G_A.data[0]\n        Dist_A = self.loss_distance_A.data[0]\n\n        D_B = self.loss_D_B.data[0]\n        G_B = self.loss_G_B.data[0]\n        Dist_B = self.loss_distance_B.data[0]\n\n        if self.A_to_B:\n            return OrderedDict([('D_A', D_A), ('G_A', G_A), ('Dist_A', Dist_A)])\n        elif self.B_to_A:\n            return OrderedDict([('D_B', D_B), ('G_B', G_B), ('Dist_B', Dist_B)])\n\n        return OrderedDict([('D_A', D_A), ('G_A', G_A), ('Dist_A', Dist_A),\n                            ('D_B', D_B), ('G_B', G_B), ('Dist_B', Dist_B)])\n\n    def get_current_visuals(self):\n        if self.opt.use_cycle_loss:\n            return CycleGANModel.get_current_visuals(self)\n\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n\n        real_B = util.tensor2im(self.real_B.data)\n        fake_A = util.tensor2im(self.fake_A.data)\n\n        if self.A_to_B:\n            return OrderedDict([('real_A', real_A), ('fake_B', fake_B)])\n        elif self.B_to_A:\n            return OrderedDict([('real_B', real_B), ('fake_A', fake_A)])\n\n        return OrderedDict([('real_A', real_A), ('fake_B', fake_B),\n                            ('real_B', real_B), ('fake_A', fake_A)])\n\n    def test(self):\n        if self.opt.use_cycle_loss:\n            return CycleGANModel.test(self)\n\n        self.real_A = Variable(self.input_A, volatile=True)\n        self.fake_B = self.netG_A.forward(self.real_A)\n\n        self.real_B = Variable(self.input_B, volatile=True)\n        self.fake_A = self.netG_B.forward(self.real_B)\n"""
cyclegan_arch/gan_model.py,4,"b""import itertools\nfrom collections import OrderedDict\n\nimport torch\nfrom torch.autograd import Variable\n\nfrom util import util as util\nfrom util.image_pool import ImagePool\nfrom . import networks\nfrom .base_model import BaseModel\n\n\nclass GANModel(BaseModel):\n    def name(self):\n        return 'GANModel'\n\n    def initialize(self, opt):\n        BaseModel.initialize(self, opt)\n\n        self.A_to_B = opt.A_to_B\n        self.B_to_A = opt.B_to_A\n\n        nb = opt.batchSize\n        size = opt.fineSize\n        self.input_A = self.Tensor(nb, opt.input_nc, size, size)\n        self.input_B = self.Tensor(nb, opt.output_nc, size, size)\n\n        # load/define networks\n        self.netG_A = networks.define_G(opt.input_nc, opt.output_nc,\n                                     opt.ngf, opt.which_model_netG, opt.norm, opt.use_dropout, self.gpu_ids)\n        self.netG_B = networks.define_G(opt.output_nc, opt.input_nc,\n                                    opt.ngf, opt.which_model_netG, opt.norm, opt.use_dropout, self.gpu_ids)\n\n        if self.isTrain:\n            use_sigmoid = opt.no_lsgan\n            self.netD_A = networks.define_D(opt.output_nc, opt.ndf,\n                                         opt.which_model_netD,\n                                         opt.n_layers_D, opt.norm, use_sigmoid, self.gpu_ids)\n            self.netD_B = networks.define_D(opt.input_nc, opt.ndf,\n                                         opt.which_model_netD,\n                                         opt.n_layers_D, opt.norm, use_sigmoid, self.gpu_ids)\n        if not self.isTrain or opt.continue_train:\n            which_epoch = opt.which_epoch\n            self.load_network(self.netG_A, 'G_A', which_epoch)\n            self.load_network(self.netG_B, 'G_B', which_epoch)\n            if self.isTrain:\n                self.load_network(self.netD_A, 'D_A', which_epoch)\n                self.load_network(self.netD_B, 'D_B', which_epoch)\n\n        if self.isTrain:\n            self.old_lr = opt.lr\n            self.fake_A_pool = ImagePool(opt.pool_size)\n            self.fake_B_pool = ImagePool(opt.pool_size)\n            # define loss functions\n            self.criterionGAN = networks.GANLoss(use_lsgan=not opt.no_lsgan, tensor=self.Tensor)\n            # initialize optimizers\n            self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D_A = torch.optim.Adam(self.netD_A.parameters(),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n            self.optimizer_D_B = torch.optim.Adam(self.netD_B.parameters(),\n                                                lr=opt.lr, betas=(opt.beta1, 0.999))\n\n            print('---------- Networks initialized -------------')\n            networks.print_network(self.netG_A)\n            networks.print_network(self.netG_B)\n            networks.print_network(self.netD_A)\n            networks.print_network(self.netD_B)\n            print('-----------------------------------------------')\n\n    def set_input(self, input):\n        AtoB = self.opt.which_direction == 'AtoB'\n        input_A = input['A' if AtoB else 'B']\n        input_B = input['B' if AtoB else 'A']\n        self.input_A.resize_(input_A.size()).copy_(input_A)\n        self.input_B.resize_(input_B.size()).copy_(input_B)\n        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n\n    def forward(self):\n        self.real_A = Variable(self.input_A)\n        self.real_B = Variable(self.input_B)\n\n    def test(self):\n        self.real_A = Variable(self.input_A, volatile=True)\n        self.fake_B = self.netG_A.forward(self.real_A)\n\n        self.real_B = Variable(self.input_B, volatile=True)\n        self.fake_A = self.netG_B.forward(self.real_B)\n\n    def backward_D_basic(self, netD, real, fake):\n        # Real\n        pred_real = netD.forward(real)\n        loss_D_real = self.criterionGAN(pred_real, True)\n        # Fake\n        pred_fake = netD.forward(fake.detach())\n        loss_D_fake = self.criterionGAN(pred_fake, False)\n        # Combined loss\n        loss_D = (loss_D_real + loss_D_fake) * 0.5\n        # backward\n        loss_D.backward()\n        return loss_D\n\n    def backward_D_A(self):\n        fake_B = self.fake_B_pool.query(self.fake_B)\n        self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)\n\n    def backward_D_B(self):\n        fake_A = self.fake_A_pool.query(self.fake_A)\n        self.loss_D_B =  self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n\n    def backward_G(self):\n\n        # D_A(G_A(A))\n        self.fake_B = self.netG_A.forward(self.real_A)\n        pred_fake = self.netD_A.forward(self.fake_B)\n        self.loss_G_A = self.criterionGAN(pred_fake, True)\n\n        # D_B(G_B(B))\n        self.fake_A = self.netG_B.forward(self.real_B)\n        pred_fake = self.netD_B.forward(self.fake_A)\n        self.loss_G_B = self.criterionGAN(pred_fake, True)\n\n        # Full loss\n        if self.A_to_B:\n            self.loss_G = self.loss_G_A\n        elif self.B_to_A:\n            self.loss_G = self.loss_G_B\n        else:\n            self.loss_G = self.loss_G_A + self.loss_G_B\n\n        self.loss_G.backward()\n\n    def optimize_parameters(self):\n        # forward\n        self.forward()\n\n        # G_A and G_B\n        self.optimizer_G.zero_grad()\n        self.backward_G()\n        self.optimizer_G.step()\n\n        # D_A\n        if not self.B_to_A:\n            self.optimizer_D_A.zero_grad()\n            self.backward_D_A()\n            self.optimizer_D_A.step()\n\n        # D_B\n        if not self.A_to_B:\n            self.optimizer_D_B.zero_grad()\n            self.backward_D_B()\n            self.optimizer_D_B.step()\n\n\n    def get_current_errors(self):\n        D_A = self.loss_D_A.data[0]\n        G_A = self.loss_G_A.data[0]\n\n        D_B = self.loss_D_B.data[0]\n        G_B = self.loss_G_B.data[0]\n\n        return OrderedDict([('D_A', D_A), ('G_A', G_A),\n                            ('D_B', D_B), ('G_B', G_B)])\n\n    def get_current_visuals(self):\n        real_A = util.tensor2im(self.real_A.data)\n        fake_B = util.tensor2im(self.fake_B.data)\n\n        real_B = util.tensor2im(self.real_B.data)\n        fake_A = util.tensor2im(self.fake_A.data)\n\n        return OrderedDict([('real_A', real_A), ('fake_B', fake_B),\n                            ('real_B', real_B), ('fake_A', fake_A)])\n\n    def save(self, label):\n        self.save_network(self.netG_A, 'G_A', label, self.gpu_ids)\n        self.save_network(self.netD_A, 'D_A', label, self.gpu_ids)\n        self.save_network(self.netG_B, 'G_B', label, self.gpu_ids)\n        self.save_network(self.netD_B, 'D_B', label, self.gpu_ids)\n\n    def update_learning_rate(self):\n        lrd = self.opt.lr / self.opt.niter_decay\n        lr = self.old_lr - lrd\n        for param_group in self.optimizer_D_A.param_groups:\n            param_group['lr'] = lr\n        for param_group in self.optimizer_D_B.param_groups:\n            param_group['lr'] = lr\n        for param_group in self.optimizer_G.param_groups:\n            param_group['lr'] = lr\n\n        print('update learning rate: %f -> %f' % (self.old_lr, lr))\n        self.old_lr = lr\n\n    def get_image_paths(self):\n        return self.image_paths"""
cyclegan_arch/models.py,0,"b'\ndef create_model(opt, dataset=None):\n\n    print(opt.model)\n\n    if opt.model == \'gan\':\n        from .gan_model import GANModel\n        model = GANModel()\n    elif opt.model == \'cycle_gan\':\n        from .cycle_gan_model import CycleGANModel\n        model = CycleGANModel()\n    elif opt.model == ""distance_gan"":\n        from .distance_gan_model import DistanceGANModel\n        model = DistanceGANModel(dataset)\n    else:\n        raise ValueError(""Model [%s] not recognized."" % opt.model)\n\n    model.initialize(opt)\n    print(""model [%s] was created"" % (model.name()))\n    return model\n'"
cyclegan_arch/networks.py,9,"b""import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\n###############################################################################\n# Functions\n###############################################################################\n\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm2d') != -1 or  classname.find('InstanceNorm2d') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\ndef get_norm_layer(norm_type):\n    if norm_type == 'batch':\n        norm_layer = nn.BatchNorm2d\n    elif norm_type == 'instance':\n        norm_layer = nn.InstanceNorm2d\n    else:\n        print('normalization layer [%s] is not found' % norm)\n    return norm_layer\n\ndef define_G(input_nc, output_nc, ngf, which_model_netG, norm='batch', use_dropout=False, gpu_ids=[]):\n    netG = None\n    use_gpu = len(gpu_ids) > 0\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if use_gpu:\n        assert(torch.cuda.is_available())\n\n    if which_model_netG == 'resnet_9blocks':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9, gpu_ids=gpu_ids)\n    elif which_model_netG == 'resnet_6blocks':\n        netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6, gpu_ids=gpu_ids)\n    elif which_model_netG == 'unet_128':\n        netG = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout, gpu_ids=gpu_ids)\n    elif which_model_netG == 'unet_256':\n        netG = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout, gpu_ids=gpu_ids)\n    else:\n        print('Generator model name [%s] is not recognized' % which_model_netG)\n    if len(gpu_ids) > 0:\n        netG.cuda(device_id=gpu_ids[0])\n    netG.apply(weights_init)\n    return netG\n\n\ndef define_D(input_nc, ndf, which_model_netD,\n             n_layers_D=3, norm='batch', use_sigmoid=False, gpu_ids=[]):\n    netD = None\n    use_gpu = len(gpu_ids) > 0\n    norm_layer = get_norm_layer(norm_type=norm)\n\n    if use_gpu:\n        assert(torch.cuda.is_available())\n    if which_model_netD == 'basic':\n        netD = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)\n    elif which_model_netD == 'n_layers':\n        netD = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid, gpu_ids=gpu_ids)\n    else:\n        print('Discriminator model name [%s] is not recognized' %\n              which_model_netD)\n    if use_gpu:\n        netD.cuda(device_id=gpu_ids[0])\n    netD.apply(weights_init)\n    return netD\n\n\ndef print_network(net):\n    num_params = 0\n    for param in net.parameters():\n        num_params += param.numel()\n    print(net)\n    print('Total number of parameters: %d' % num_params)\n\n\n##############################################################################\n# Classes\n##############################################################################\n\n\n# Defines the GAN loss which uses either LSGAN or the regular GAN.\n# When LSGAN is used, it is basically same as MSELoss,\n# but it abstracts away the need to create the target label tensor\n# that has the same size as the input\nclass GANLoss(nn.Module):\n    def __init__(self, use_lsgan=True, target_real_label=1.0, target_fake_label=0.0,\n                 tensor=torch.FloatTensor):\n        super(GANLoss, self).__init__()\n        self.real_label = target_real_label\n        self.fake_label = target_fake_label\n        self.real_label_var = None\n        self.fake_label_var = None\n        self.Tensor = tensor\n        if use_lsgan:\n            self.loss = nn.MSELoss()\n        else:\n            self.loss = nn.BCELoss()\n\n    def get_target_tensor(self, input, target_is_real):\n        target_tensor = None\n        if target_is_real:\n            create_label = ((self.real_label_var is None) or\n                            (self.real_label_var.numel() != input.numel()))\n            if create_label:\n                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n                self.real_label_var = Variable(real_tensor, requires_grad=False)\n            target_tensor = self.real_label_var\n        else:\n            create_label = ((self.fake_label_var is None) or\n                            (self.fake_label_var.numel() != input.numel()))\n            if create_label:\n                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n            target_tensor = self.fake_label_var\n        return target_tensor\n\n    def __call__(self, input, target_is_real):\n        target_tensor = self.get_target_tensor(input, target_is_real)\n        return self.loss(input, target_tensor)\n\n\n# Defines the generator that consists of Resnet blocks between a few\n# downsampling/upsampling operations.\n# Code and idea originally from Justin Johnson's architecture.\n# https://github.com/jcjohnson/fast-neural-style/\nclass ResnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, gpu_ids=[]):\n        assert(n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.ngf = ngf\n        self.gpu_ids = gpu_ids\n\n        model = [nn.Conv2d(input_nc, ngf, kernel_size=7, padding=3),\n                 norm_layer(ngf, affine=True),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):\n            mult = 2**i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n                                stride=2, padding=1),\n                      norm_layer(ngf * mult * 2, affine=True),\n                      nn.ReLU(True)]\n\n        mult = 2**n_downsampling\n        for i in range(n_blocks):\n            model += [ResnetBlock(ngf * mult, 'zero', norm_layer=norm_layer, use_dropout=use_dropout)]\n\n        for i in range(n_downsampling):\n            mult = 2**(n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1),\n                      norm_layer(int(ngf * mult / 2), affine=True),\n                      nn.ReLU(True)]\n\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=3)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        if self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            return self.model(input)\n\n\n# Define a resnet block\nclass ResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, use_dropout):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout):\n        conv_block = []\n        p = 0\n        # TODO: support padding types\n        assert(padding_type == 'zero')\n        p = 1\n\n        # TODO: InstanceNorm\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n                       norm_layer(dim, affine=True),\n                       nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p),\n                       norm_layer(dim, affine=True)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\n\n# Defines the Unet generator.\n# |num_downs|: number of downsamplings in UNet. For example,\n# if |num_downs| == 7, image of size 128x128 will become of size 1x1\n# at the bottleneck\nclass UnetGenerator(nn.Module):\n    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n                 norm_layer=nn.BatchNorm2d, use_dropout=False, gpu_ids=[]):\n        super(UnetGenerator, self).__init__()\n        self.gpu_ids = gpu_ids\n\n        # currently support only input_nc == output_nc\n        assert(input_nc == output_nc)\n\n        # construct unet structure\n        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, innermost=True)\n        for i in range(num_downs - 5):\n            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, unet_block, norm_layer=norm_layer)\n        unet_block = UnetSkipConnectionBlock(output_nc, ngf, unet_block, outermost=True, norm_layer=norm_layer)\n\n        self.model = unet_block\n\n    def forward(self, input):\n        if  self.gpu_ids and isinstance(input.data, torch.cuda.FloatTensor):\n            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            return self.model(input)\n\n\n# Defines the submodule with skip connection.\n# X -------------------identity---------------------- X\n#   |-- downsampling -- |submodule| -- upsampling --|\nclass UnetSkipConnectionBlock(nn.Module):\n    def __init__(self, outer_nc, inner_nc,\n                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n        super(UnetSkipConnectionBlock, self).__init__()\n        self.outermost = outermost\n\n        downconv = nn.Conv2d(outer_nc, inner_nc, kernel_size=4,\n                             stride=2, padding=1)\n        downrelu = nn.LeakyReLU(0.2, True)\n        downnorm = norm_layer(inner_nc, affine=True)\n        uprelu = nn.ReLU(True)\n        upnorm = norm_layer(outer_nc, affine=True)\n\n        if outermost:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1)\n            down = [downconv]\n            up = [uprelu, upconv, nn.Tanh()]\n            model = down + [submodule] + up\n        elif innermost:\n            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1)\n            down = [downrelu, downconv]\n            up = [uprelu, upconv, upnorm]\n            model = down + up\n        else:\n            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n                                        kernel_size=4, stride=2,\n                                        padding=1)\n            down = [downrelu, downconv, downnorm]\n            up = [uprelu, upconv, upnorm]\n\n            if use_dropout:\n                model = down + [submodule] + up + [nn.Dropout(0.5)]\n            else:\n                model = down + [submodule] + up\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        if self.outermost:\n            return self.model(x)\n        else:\n            return torch.cat([self.model(x), x], 1)\n\n\n# Defines the PatchGAN discriminator with the specified arguments.\nclass NLayerDiscriminator(nn.Module):\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d, use_sigmoid=False, gpu_ids=[]):\n        super(NLayerDiscriminator, self).__init__()\n        self.gpu_ids = gpu_ids\n\n        kw = 4\n        padw = int(np.ceil((kw-1)/2))\n        sequence = [\n            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):\n            nf_mult_prev = nf_mult\n            nf_mult = min(2**n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                                kernel_size=kw, stride=2, padding=padw),\n                # TODO: use InstanceNorm\n                norm_layer(ndf * nf_mult, affine=True),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2**n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n                            kernel_size=kw, stride=1, padding=padw),\n            # TODO: useInstanceNorm\n            norm_layer(ndf * nf_mult, affine=True),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n\n        if use_sigmoid:\n            sequence += [nn.Sigmoid()]\n\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        if len(self.gpu_ids)  and isinstance(input.data, torch.cuda.FloatTensor):\n            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n        else:\n            return self.model(input)\n"""
datasets/combine_A_and_B.py,0,"b""from pdb import set_trace as st\nimport os\nimport numpy as np\nimport cv2\nimport argparse\n\nparser = argparse.ArgumentParser('create image pairs')\nparser.add_argument('--fold_A', dest='fold_A', help='input directory for image A', type=str, default='../dataset/50kshoes_edges')\nparser.add_argument('--fold_B', dest='fold_B', help='input directory for image B', type=str, default='../dataset/50kshoes_jpg')\nparser.add_argument('--fold_AB', dest='fold_AB', help='output directory', type=str, default='../dataset/test_AB')\nparser.add_argument('--num_imgs', dest='num_imgs', help='number of images',type=int, default=1000000)\nparser.add_argument('--use_AB', dest='use_AB', help='if true: (0001_A, 0001_B) to (0001_AB)',action='store_true')\nargs = parser.parse_args()\n\nfor arg in vars(args):\n    print('[%s] = ' % arg,  getattr(args, arg))\n\nsplits = os.listdir(args.fold_A)\n\nfor sp in splits:\n    img_fold_A = os.path.join(args.fold_A, sp)\n    img_fold_B = os.path.join(args.fold_B, sp)\n    img_list = os.listdir(img_fold_A)\n    if args.use_AB: \n        img_list = [img_path for img_path in img_list if '_A.' in img_path]\n\n    num_imgs = min(args.num_imgs, len(img_list))\n    print('split = %s, use %d/%d images' % (sp, num_imgs, len(img_list)))\n    img_fold_AB = os.path.join(args.fold_AB, sp)\n    if not os.path.isdir(img_fold_AB):\n        os.makedirs(img_fold_AB)\n    print('split = %s, number of images = %d' % (sp, num_imgs))\n    for n in range(num_imgs):\n        name_A = img_list[n]\n        path_A = os.path.join(img_fold_A, name_A)\n        if args.use_AB:\n            name_B = name_A.replace('_A.', '_B.')\n        else:\n            name_B = name_A\n        path_B = os.path.join(img_fold_B, name_B)\n        if os.path.isfile(path_A) and os.path.isfile(path_B):\n            name_AB = name_A\n            if args.use_AB:\n                name_AB = name_AB.replace('_A.', '.') # remove _A\n            path_AB = os.path.join(img_fold_AB, name_AB)\n            im_A = cv2.imread(path_A, cv2.CV_LOAD_IMAGE_COLOR)\n            im_B = cv2.imread(path_B, cv2.CV_LOAD_IMAGE_COLOR)\n            im_AB = np.concatenate([im_A, im_B], 1)\n            cv2.imwrite(path_AB, im_AB)\n"""
datasets/download.py,0,"b'""""""\nModification of https://github.com/carpedm20/DCGAN-tensorflow/blob/master/download.py\nModification of https://github.com/faceteam/facescrub/download.py\nDownloads the following:\n- Celeb-A dataset\n- Pix2Pix - Edges2Handbags dataset\n- Pix2pix - Edges2Shoes dataset\n- Facescrub dataset\n""""""\n\nfrom __future__ import print_function\nimport os\nfrom os.path import join, exists\nimport multiprocessing\nimport hashlib\nimport cv2\nimport sys\nimport zipfile\nimport argparse\nfrom six.moves import urllib\n\nparser = argparse.ArgumentParser(description=\'Download dataset for DistanceGAN.\')\nparser.add_argument(\'datasets\', metavar=\'N\', type=str, nargs=\'+\', choices=[\'celebA\', \'edges2handbags\', \'edges2shoes\', \'horse2zebra\'],\n                    help=\'name of dataset to download [celebA, edges2handbags, edges2shoes, horse2zebra]\')\n\n\ndef download(url, path):\n    filename = url.split(\'/\')[-1]\n    filepath = os.path.join(path, filename)\n    u = urllib.request.urlopen(url)\n    f = open(filepath, \'wb\')\n    filesize = int(u.headers[""Content-Length""])\n    print(""Downloading: %s Bytes: %s"" % (filename, filesize))\n\n    downloaded = 0\n    block_sz = 8192\n    status_width = 70\n    while True:\n        buf = u.read(block_sz)\n        if not buf:\n            print(\'\')\n            break\n        else:\n            print(\'\', end=\'\\r\')\n        downloaded += len(buf)\n        f.write(buf)\n        status = ((""[%-"" + str(status_width + 1) + ""s] %3.2f%%"") %\n                  (\'=\' * int(float(downloaded) / filesize * status_width) + \'>\', downloaded * 100. / filesize))\n        print(status, end=\'\')\n        sys.stdout.flush()\n    f.close()\n    return filepath\n\n\ndef unzip(filepath):\n    print(""Extracting: "" + filepath)\n    dirpath = os.path.dirname(filepath)\n    with zipfile.ZipFile(filepath) as zf:\n        zf.extractall(dirpath)\n    os.remove(filepath)\n\n\ndef download_celeb_a(dirpath):\n    data_dir = \'celebA\'\n    if os.path.exists(os.path.join(dirpath, data_dir)):\n        print(\'Found Celeb-A - skip\')\n        return\n    url = \'https://drive.google.com/open?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM\'\n    filepath = download(url, dirpath)\n    zip_dir = \'\'\n    with zipfile.ZipFile(filepath) as zf:\n        zip_dir = zf.namelist()[0]\n        zf.extractall(dirpath)\n    os.remove(filepath)\n    os.rename(os.path.join(dirpath, zip_dir), os.path.join(dirpath, data_dir))\n\n    attribute_url = \'https://www.dropbox.com/sh/8oqt9vytwxb3s4r/AAB06FXaQRUNtjW9ntaoPGvCa?dl=0\'\n    filepath = download(attribute_url, dirpath)\n\n\n\ndef download_pix2pix(category):\n    CMD = \'bash ./datasets/download_pix2pix.sh ""%s""\'\n    res = os.system(CMD % (category))\n\ndef download_cyclegan(category):\n    CMD = \'bash ./datasets/download_cyclegan.sh ""%s""\'\n    res = os.system(CMD % (category))\n\n\ndef preprocess_facescrub(dirpath):\n    data_dir = os.path.join(dirpath, \'facescrub\')\n    if os.path.exists(data_dir):\n        print(\'Found Facescrub\')\n    else:\n        os.mkdir(data_dir)\n    files = [\'./datasets/facescrub_actors.txt\',\n             \'./datasets/facescrub_actresses.txt\']\n    for f in files:\n        with open(f, \'r\') as fd:\n            # strip first line\n            fd.readline()\n            names = []\n            urls = []\n            bboxes = []\n            genders = []\n            for line in fd.readlines():\n                gender = f.split(""_"")[1].split(""."")[0]\n                components = line.split(\'\\t\')\n                assert(len(components) == 6)\n                name = components[0].replace(\' \', \'_\')\n                url = components[3]\n                bbox = [int(_) for _ in components[4].split(\',\')]\n                names.append(name)\n                urls.append(url)\n                bboxes.append(bbox)\n                genders.append(gender)\n        # every name gets a task\n        last_name = names[0]\n        task_names = []\n        task_urls = []\n        task_bboxes = []\n        task_genders = []\n        tasks = []\n        for i in range(len(names)):\n            if names[i] == last_name:\n                task_names.append(names[i])\n                task_urls.append(urls[i])\n                task_bboxes.append(bboxes[i])\n                task_genders.append(genders[i])\n            else:\n                tasks.append(\n                    (data_dir, task_genders, task_names, task_urls, task_bboxes))\n                task_names = [names[i]]\n                task_urls = [urls[i]]\n                task_bboxes = [bboxes[i]]\n                task_genders = [genders[i]]\n                last_name = names[i]\n        tasks.append(\n            (data_dir, task_genders, task_names, task_urls, task_bboxes))\n\n        pool_size = multiprocessing.cpu_count()\n        pool = multiprocessing.Pool(processes=pool_size, maxtasksperchild=2)\n        pool.map(download_facescrub, tasks)\n        pool.close()\n        pool.join()\n\n\ndef download_facescrub((data_dir, genders, names, urls, bboxes)):\n    """"""\n        download from urls into folder names using wget\n    """"""\n\n    assert(len(names) == len(urls))\n    assert(len(names) == len(bboxes))\n    # download using external wget\n    CMD = \'wget -c -t 1 -T 3 ""%s"" -O ""%s""\'\n    for i in range(len(names)):\n        directory = join(data_dir, genders[i])\n\n        if not exists(directory):\n            print(directory)\n            os.mkdir(directory)\n        fname = hashlib.sha1(urls[i]).hexdigest() + ""_"" + names[i] + \'.jpg\'\n        dst = join(directory, fname)\n        print(""downloading"", dst)\n        if exists(dst):\n            print(""already downloaded, skipping..."")\n            continue\n        else:\n            res = os.system(CMD % (urls[i], dst))\n        # get face\n        face_directory = join(directory, \'face\')\n        if not exists(face_directory):\n            os.mkdir(face_directory)\n        img = cv2.imread(dst)\n        if img is None:\n            # no image data\n            os.remove(dst)\n        else:\n            face_path = join(face_directory, fname)\n            face = img[bboxes[i][1]:bboxes[i][3], bboxes[i][0]:bboxes[i][2]]\n            cv2.imwrite(face_path, face)\n            #write bbox to file\n            with open(join(directory, \'_bboxes.txt\'), \'a\') as fd:\n                bbox_str = \',\'.join([str(_) for _ in bboxes[i]])\n                fd.write(\'%s %s\\n\' % (fname, bbox_str))\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n\n    if \'celebA\' in args.datasets:\n        download_celeb_a(\'./datasets/\')\n    if \'edges2handbags\' in args.datasets:\n        download_pix2pix(\'edges2handbags\')\n    if \'edges2shoes\' in args.datasets:\n        download_pix2pix(\'edges2shoes\')\n    if \'horse2zebra\' in args.datasets:\n        download_cyclegan(\'horse2zebra\')\n\n\n'"
discogan_arch/__init__.py,0,b''
discogan_arch/dataset.py,0,"b""import os\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom scipy.misc import imresize\nimport scipy.io\n\n\ndataset_path = './datasets/'\ncelebA_path = os.path.join(dataset_path, 'celebA')\nhandbag_path = os.path.join(dataset_path, 'edges2handbags')\nshoe_path = os.path.join(dataset_path, 'edges2shoes')\nfacescrub_path = os.path.join(dataset_path, 'facescrub')\nchair_path = os.path.join(dataset_path, 'rendered_chairs')\nface_3d_path = os.path.join(dataset_path, 'PublicMM1', '05_renderings')\nface_real_path = os.path.join(dataset_path, 'real_face')\ncar_path = os.path.join(dataset_path, 'data', 'cars')\n\ndef shuffle_data(da, db):\n    a_idx = range(len(da))\n    np.random.shuffle( a_idx )\n\n    b_idx = range(len(db))\n    np.random.shuffle(b_idx)\n\n    shuffled_da = np.array(da)[ np.array(a_idx) ]\n    shuffled_db = np.array(db)[ np.array(b_idx) ]\n\n    return shuffled_da, shuffled_db\n\ndef read_images( filenames, domain=None, image_size=64):\n\n    images = []\n\n    for fn in filenames:\n        image = cv2.imread(fn)\n        if image is None:\n            continue\n\n        if domain == 'A':\n            kernel = np.ones((3,3), np.uint8)\n            image = image[:, :256, :]\n            image = 255. - image\n            image = cv2.dilate( image, kernel, iterations=1 )\n            image = 255. - image\n        elif domain == 'B':\n            image = image[:, 256:, :]\n\n        image = cv2.resize(image, (image_size,image_size))\n        image = image.astype(np.float32) / 255.\n        image = image.transpose(2,0,1)\n        images.append( image )\n\n    images = np.stack( images )\n    return images\n\ndef read_attr_file( attr_path, image_dir ):\n    f = open( attr_path )\n    lines = f.readlines()\n    lines = map(lambda line: line.strip(), lines)\n    columns = ['image_path'] + lines[1].split()\n    lines = lines[2:]\n\n    items = map(lambda line: line.split(), lines)\n    df = pd.DataFrame( items, columns=columns )\n    df['image_path'] = df['image_path'].map( lambda x: os.path.join( image_dir, x ) )\n\n    return df\n\ndef get_celebA_files(style_A, style_B, constraint, constraint_type, test=False, n_test=200):\n    attr_file = os.path.join( celebA_path, 'list_attr_celeba.txt' )\n    image_dir = os.path.join( celebA_path, 'img_align_celeba' )\n    image_data = read_attr_file( attr_file, image_dir )\n\n    if constraint:\n        image_data = image_data[ image_data[constraint] == constraint_type]\n\n    style_A_data = image_data[ image_data[style_A] == '1']['image_path'].values\n    if style_B:\n        style_B_data = image_data[ image_data[style_B] == '1']['image_path'].values\n    else:\n        style_B_data = image_data[ image_data[style_A] == '-1']['image_path'].values\n\n    if test == False:\n        return style_A_data[:-n_test], style_B_data[:-n_test]\n    if test == True:\n        return style_A_data[-n_test:], style_B_data[-n_test:]\n\n\ndef get_edge2photo_files(item='edges2handbags', test=False):\n    if item == 'edges2handbags':\n        item_path = handbag_path\n    elif item == 'edges2shoes':\n        item_path = shoe_path\n\n    if test == True:\n        item_path = os.path.join( item_path, 'val' )\n    else:\n        item_path = os.path.join( item_path, 'train' )\n\n    image_paths = map(lambda x: os.path.join( item_path, x ), os.listdir( item_path ))\n\n    if test == True:\n        return [image_paths, image_paths]\n    else:\n        n_images = len( image_paths )\n        return [image_paths[:n_images/2], image_paths[n_images/2:]]\n\n\ndef get_facescrub_files(test=False, n_test=200):\n    actor_path = os.path.join(facescrub_path, 'actors', 'face' )\n    actress_path = os.path.join( facescrub_path, 'actresses', 'face' )\n\n    actor_files = map(lambda x: os.path.join( actor_path, x ), os.listdir( actor_path ) )\n    actress_files = map(lambda x: os.path.join( actress_path, x ), os.listdir( actress_path ) )\n\n    if test == False:\n        return actor_files[:-n_test], actress_files[:-n_test]\n    else:\n        return actor_files[-n_test:], actress_files[-n_test:]\n\n\ndef get_chairs(test=False, half=None, ver=360, angle_info=False):\n    chair_ids = os.listdir( chair_path )\n    if test:\n        current_ids = chair_ids[-10:]\n    else:\n        if half is None: current_ids = chair_ids[:-10]\n        elif half == 'first': current_ids = chair_ids[:-10][:len(chair_ids)/2]\n        elif half == 'last': current_ids = chair_ids[:-10][len(chair_ids)/2:]\n\n    chair_paths = []\n\n    for chair in current_ids:\n        current_path = os.path.join( chair_path, chair, 'renders' )\n        if not os.path.exists( current_path ): continue\n        filenames = filter(lambda x: x.endswith('.png'), os.listdir( current_path ))\n\n        for filename in filenames:\n            angle = int(filename.split('_')[3][1:])\n            filepath = os.path.join(current_path, filename)\n\n            if ver == 180:\n                if angle > 180 and angle < 360: chair_paths.append(filepath)\n            if ver == 360:\n                chair_paths.append(filepath)\n\n    return chair_paths\n\ndef get_cars(test=False, ver=360, interval=1, half=None, angle_info=False, image_size=64, gray=True):\n    car_files = map(lambda x: os.path.join(car_path, x), os.listdir( car_path ))\n    car_files = filter(lambda x: x.endswith('.mat'), car_files)\n\n    car_idx = map(lambda x: int(x.split('car_')[1].split('_mesh')[0]), car_files )\n    car_df = pd.DataFrame( {'idx': car_idx, 'path': car_files}).sort_values(by='idx')\n\n    car_files = car_df['path'].values\n\n    if not test:\n        car_files = car_files[:-14]\n    else:\n        car_files = car_files[-14:]\n\n    car_images = []\n    classes = []\n\n    n_cars = len(car_files)\n    car_idx = 0\n    for car_file in car_files:\n        if not car_file.endswith('.mat'): continue\n        car_mat = scipy.io.loadmat(car_file)\n        car_ims = car_mat['im']\n        car_idx += 1\n\n        if half == 'first':\n            if car_idx > n_cars / 2:\n                break\n        elif half == 'last':\n            if car_idx <= n_cars / 2:\n                continue\n\n        if ver == 360:\n            for idx,i in enumerate(range(24)):\n                car_image = car_ims[:,:,:,i,3]\n                car_image = cv2.resize(car_image, (image_size,image_size))\n                if gray:\n                    car_image = cv2.cvtColor(car_image, cv2.COLOR_BGR2GRAY)\n                    car_image = np.repeat(car_image[:,:,None], 3, 2)\n                car_image = car_image.transpose(2,0,1)\n                car_image = car_image.astype(np.float32)/255.\n                car_images.append( car_image )\n                if angle_info:\n                    classes.append(idx)\n\n        elif ver == 180:\n            for idx,i in enumerate(range(5,-1,-1) + range(23,18,-1)):\n                car_image = car_ims[:,:,:,i,3]\n                car_image = cv2.resize(car_image, (image_size,image_size))\n                if gray:\n                    car_image = cv2.cvtColor(car_image, cv2.COLOR_BGR2GRAY)\n                    car_image = np.repeat(car_image[:,:,None], 3, 2)\n                car_image = car_image.transpose(2,0,1)\n                car_image = car_image.astype(np.float32)/255.\n                car_images.append( car_image )\n\t\tif angle_info:\n\t\t    classes.append( idx )\n\n        elif ver == 90:\n            for idx,i in enumerate(range(5,-1,-1)):\n                car_image = car_ims[:,:,:,i,3]\n                car_image = cv2.resize(car_image, (image_size,image_size))\n                car_image = car_image.transpose(2,0,1)\n                car_image = car_image.astype(np.float32)/255.\n                car_images.append( car_image )\n                if angle_info:\n                    classes.append( idx )\n\n    car_images = car_images[::interval]\n\n    if angle_info:\n        return np.stack(car_images), np.array(classes)\n\n    return np.stack( car_images )\n\ndef get_faces_3d(test=False, half=None):\n    files = os.listdir( face_3d_path )\n    image_files = filter(lambda x: x.endswith('.png'), files)\n\n    df = pd.DataFrame({'image_path': image_files})\n    df['id'] = df['image_path'].map(lambda x: x.split('/')[-1][:20])\n    unique_ids = df['id'].unique()\n\n    if not test:\n        if half is None:\n            current_ids = unique_ids[:8]\n        if half == 'first':\n            current_ids = unique_ids[:4]\n        if half == 'last':\n            current_ids = unique_ids[4:8]\n    else:\n        current_ids = unique_ids[8:]\n\n    groups = df.groupby('id')\n    image_paths = []\n\n    for current_id in current_ids:\n        image_paths += groups.get_group(current_id)['image_path'].tolist()\n\n    image_paths = [os.path.join(face_3d_path, path) for path in image_paths]\n    return image_paths\n"""
discogan_arch/disco_gan_angle_pairing_model.py,1,"b'from dataset import *\nfrom disco_gan_model import DiscoGAN\nfrom discogan_arch_options.options import AnglePairingOptions\nfrom model import *\n\n\nclass DiscoGANAnglePairing(DiscoGAN):\n\n    def get_data(self):\n        if self.args.task_name == \'car2car\':\n            data_A = get_cars(test=False, ver=180, half=\'first\', image_size=self.args.image_size)\n            data_B = get_cars(test=False, ver=180, half=\'last\', image_size=self.args.image_size)\n\n            test_A = test_B = get_cars(test=True, ver=180, image_size=self.args.image_size)\n\n        elif self.args.task_name == \'face2face\':\n            data_A = get_faces_3d(test=False, half=\'first\')\n            data_B = get_faces_3d(test=False, half=\'last\')\n\n            test_A = test_B = get_faces_3d(test=True)\n\n        elif self.args.task_name == \'chair2chair\':\n            data_A = get_chairs(test=False, half=\'first\', ver=360)\n            data_B = get_chairs(test=False, half=\'last\', ver=360)\n\n            test_A = test_B = get_chairs(test=True, ver=360)\n\n        elif self.args.task_name == \'chair2car\':\n            data_A = get_chairs(test=False, half=None, ver=180)\n            data_B = get_cars(test=False, half=None, ver=180)\n\n            test_A = get_chairs(test=True, ver=180)\n            test_B = get_cars(test=True, ver=180)\n\n        elif self.args.task_name == \'chair2face\':\n            data_A = get_chairs(test=False, half=None, ver=180)\n            data_B = get_faces_3d(test=False, half=None)\n\n            test_A = get_chairs(test=True, ver=180)\n            test_B = get_faces_3d(test=True)\n\n        elif self.args.task_name == \'car2face\':\n            data_A = get_cars(test=False, ver=180, half=None)\n            data_B = get_faces_3d(test=False, half=None)\n\n            test_A = get_cars(test=True, ver=180)\n            test_B = get_faces_3d(test=True)\n\n        return data_A, data_B, test_A, test_B\n\n    def get_fm_loss(self, real_feats, fake_feats):\n        losses = 0\n        for real_feat, fake_feat in zip(real_feats[1:], fake_feats[1:]):\n            l2 = (real_feat.mean(0) - fake_feat.mean(0)) * (real_feat.mean(0) - fake_feat.mean(0))\n            loss = self.feat_criterion(l2, Variable(torch.ones(l2.size())).cuda())\n            losses += loss\n\n        return losses\n\n    def get_test_images(self):\n        if self.args.task_name.startswith(\'car\') and self.args.task_name.endswith(\'car\'):\n            self.test_A = self.test_style_A\n            self.test_B = self.test_style_B\n\n        if self.args.task_name.startswith(\'car\') and not self.args.task_name.endswith(\'car\'):\n            self.test_A = self.test_style_A\n            self.test_B = read_images(self.test_style_B, None, self.args.image_size)\n\n        if not self.args.task_name.startswith(\'car\') and not self.args.task_name.endswith(\'car\'):\n            self.test_A = read_images(self.test_style_A, None, self.args.image_size)\n            self.test_B = read_images(self.test_style_B, None, self.args.image_size)\n\n    def get_images(self):\n        if self.args.task_name.startswith(\'car\') and self.args.task_name.endswith(\'car\'):\n            A = self.A_path\n            B = self.B_path\n\n        if self.args.task_name.startswith(\'car\') and not self.args.task_name.endswith(\'car\'):\n            A = self.A_path\n            B = read_images(self.B_path, None, self.args.image_size)\n\n        if not self.args.task_name.startswith(\'car\') and not self.args.task_name.endswith(\'car\'):\n            A = read_images(self.A_path, None, self.args.image_size)\n            B = read_images(self.B_path, None, self.args.image_size)\n\n        return A, B\n\n\n    def __init__(self):\n        options = AnglePairingOptions()\n        options.initialize()\n        self.args = options.parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    model = DiscoGANAnglePairing()\n    model.run()'"
discogan_arch/disco_gan_model.py,13,"b'from itertools import chain\n\nimport scipy\nimport torch.optim as optim\nfrom progressbar import ETA, Bar, Percentage, ProgressBar\n\nfrom dataset import *\nfrom discogan_arch_options.options import Options\nfrom model import *\n\n\nclass DiscoGAN(object):\n\n    def as_np(self, data):\n        return data.cpu().data.numpy()\n\n    def get_data(self):\n        # celebA / edges2shoes / edges2handbags / ...\n        if self.args.task_name == \'facescrub\':\n            data_A, data_B = get_facescrub_files(test=False, n_test=self.args.n_test)\n            test_A, test_B = get_facescrub_files(test=True, n_test=self.args.n_test)\n\n        elif self.args.task_name == \'celebA\':\n            data_A, data_B = get_celebA_files(style_A=self.args.style_A, style_B=self.args.style_B, constraint=self.args.constraint,\n                                              constraint_type=self.args.constraint_type, test=False, n_test=self.args.n_test)\n            test_A, test_B = get_celebA_files(style_A=self.args.style_A, style_B=self.args.style_B, constraint=self.args.constraint,\n                                              constraint_type=self.args.constraint_type, test=True, n_test=self.args.n_test)\n\n        elif self.args.task_name == \'edges2shoes\':\n            data_A, data_B = get_edge2photo_files(item=\'edges2shoes\', test=False)\n            test_A, test_B = get_edge2photo_files(item=\'edges2shoes\', test=True)\n\n        elif self.args.task_name == \'edges2handbags\':\n            data_A, data_B = get_edge2photo_files(item=\'edges2handbags\', test=False)\n            test_A, test_B = get_edge2photo_files(item=\'edges2handbags\', test=True)\n\n        elif self.args.task_name == \'handbags2shoes\':\n            data_A_1, data_A_2 = get_edge2photo_files(item=\'edges2handbags\', test=False)\n            test_A_1, test_A_2 = get_edge2photo_files(item=\'edges2handbags\', test=True)\n\n            data_A = np.hstack([data_A_1, data_A_2])\n            test_A = np.hstack([test_A_1, test_A_2])\n\n            data_B_1, data_B_2 = get_edge2photo_files(item=\'edges2shoes\', test=False)\n            test_B_1, test_B_2 = get_edge2photo_files(item=\'edges2shoes\', test=True)\n\n            data_B = np.hstack([data_B_1, data_B_2])\n            test_B = np.hstack([test_B_1, test_B_2])\n\n        return data_A, data_B, test_A, test_B\n\n    def get_fm_loss(self, real_feats, fake_feats):\n        losses = 0\n        for real_feat, fake_feat in zip(real_feats, fake_feats):\n            l2 = (real_feat.mean(0) - fake_feat.mean(0)) * (real_feat.mean(0) - fake_feat.mean(0))\n            l2_label = Variable(torch.ones(l2.size()))\n            if self.cuda:\n                l2_label = l2_label.cuda()\n            loss = self.feat_criterion(l2, l2_label)\n            losses += loss\n\n        return losses\n\n    def get_gan_loss(self, dis_real, dis_fake):\n        labels_dis_real = Variable(torch.ones([dis_real.size()[0], 1]))\n        labels_dis_fake = Variable(torch.zeros([dis_fake.size()[0], 1]))\n        labels_gen = Variable(torch.ones([dis_fake.size()[0], 1]))\n\n        if self.cuda:\n            labels_dis_real = labels_dis_real.cuda()\n            labels_dis_fake = labels_dis_fake.cuda()\n            labels_gen = labels_gen.cuda()\n\n        dis_loss = self.gan_criterion(dis_real, labels_dis_real) * 0.5 + self.gan_criterion(dis_fake, labels_dis_fake) * 0.5\n        gen_loss = self.gan_criterion(dis_fake, labels_gen)\n\n        return dis_loss, gen_loss\n\n    def get_test_images(self):\n\n        if self.args.task_name.startswith(\'edges2\'):\n            self.test_A = read_images(self.test_style_A, \'A\', self.args.image_size)\n            self.test_B = read_images(self.test_style_B, \'B\', self.args.image_size)\n        elif self.args.task_name == \'handbags2shoes\' or self.args.task_name == \'shoes2handbags\':\n            self.test_A = read_images(self.test_style_A, \'B\', self.args.image_size)\n            self.test_B = read_images(self.test_style_B, \'B\', self.args.image_size)\n        else:\n            self.test_A = read_images(self.test_style_A, None, self.args.image_size)\n            self.test_B = read_images(self.test_style_B, None, self.args.image_size)\n\n    def get_images(self):\n\n        if self.args.task_name.startswith(\'edges2\'):\n            A = read_images(self.A_path, \'A\', self.args.image_size)\n            B = read_images(self.B_path, \'B\', self.args.image_size)\n        elif self.args.task_name == \'handbags2shoes\' or self.args.task_name == \'shoes2handbags\':\n            A = read_images(self.A_path, \'B\', self.args.image_size)\n            B = read_images(self.B_path, \'B\', self.args.image_size)\n        else:\n            A = read_images(self.A_path, None, self.args.image_size)\n            B = read_images(self.B_path, None, self.args.image_size)\n\n        return A, B\n\n    def __init__(self):\n        options = Options()\n        options.initialize()\n        self.args = options.parser.parse_args()\n\n    def initialize(self):\n\n        self.cuda = self.args.cuda\n        if self.cuda == \'true\':\n            self.cuda = True\n        else:\n            self.cuda = False\n\n        self.result_path = os.path.join(self.args.result_path, self.args.task_name)\n        if self.args.style_A:\n            self.result_path = os.path.join(self.result_path, self.args.style_A)\n        self.result_path = os.path.join(self.result_path, self.args.model_arch)\n\n        self.model_path = os.path.join(self.args.model_path, self.args.task_name)\n        if self.args.style_A:\n            self.model_path = os.path.join(self.model_path, self.args.style_A)\n        self.model_path = os.path.join(self.model_path, self.args.model_arch)\n\n        self.data_style_A, self.data_style_B, self.test_style_A, self.test_style_B = self.get_data()\n\n        self.get_test_images()\n        self.test_A = Variable(torch.FloatTensor(self.test_A), volatile=True)\n        self.test_B = Variable(torch.FloatTensor(self.test_B), volatile=True)\n\n        if not os.path.exists(self.result_path):\n            os.makedirs(self.result_path)\n        if not os.path.exists(self.model_path):\n            os.makedirs(self.model_path)\n\n        self.generator_A = Generator(num_layers=self.args.num_layers)\n        self.generator_B = Generator(num_layers=self.args.num_layers)\n        self.discriminator_A = Discriminator()\n        self.discriminator_B = Discriminator()\n\n        if self.cuda:\n            self.test_A = self.test_A.cuda()\n            self.test_B = self.test_B.cuda()\n            self.generator_A = self.generator_A.cuda()\n            self.generator_B = self.generator_B.cuda()\n            self.discriminator_A = self.discriminator_A.cuda()\n            self.discriminator_B = self.discriminator_B.cuda()\n\n        data_size = min(len(self.data_style_A), len(self.data_style_B))\n        self.n_batches = (data_size // self.args.batch_size)\n\n        self.recon_criterion = nn.MSELoss()\n        self.gan_criterion = nn.BCELoss()\n        self.feat_criterion = nn.HingeEmbeddingLoss()\n\n        gen_params = chain(self.generator_A.parameters(), self.generator_B.parameters())\n        dis_params = chain(self.discriminator_A.parameters(), self.discriminator_B.parameters())\n\n        self.optim_gen = optim.Adam(gen_params, lr=self.args.learning_rate, betas=(0.5, 0.999), weight_decay=0.00001)\n        self.optim_dis = optim.Adam(dis_params, lr=self.args.learning_rate, betas=(0.5, 0.999), weight_decay=0.00001)\n\n    def run(self):\n\n        self.initialize()\n        self.iters = 0\n\n        for epoch in range(self.args.epoch_size):\n            data_style_A, data_style_B = shuffle_data(self.data_style_A, self.data_style_B)\n\n            widgets = [\'epoch #%d|\' % epoch, Percentage(), Bar(), ETA()]\n            pbar = ProgressBar(maxval=self.n_batches, widgets=widgets)\n            pbar.start()\n\n            for i in range(self.n_batches):\n\n                pbar.update(i)\n\n                self.generator_A.zero_grad()\n                self.generator_B.zero_grad()\n                self.discriminator_A.zero_grad()\n                self.discriminator_B.zero_grad()\n\n                self.A_path = data_style_A[i * self.args.batch_size: (i + 1) * self.args.batch_size]\n                self.B_path = data_style_B[i * self.args.batch_size: (i + 1) * self.args.batch_size]\n\n                A, B = self.get_images()\n                A = Variable(torch.FloatTensor(A))\n                B = Variable(torch.FloatTensor(B))\n\n                if self.cuda:\n                    A = A.cuda()\n                    B = B.cuda()\n\n                AB = self.generator_B(A)\n                BA = self.generator_A(B)\n\n                ABA = self.generator_A(AB)\n                BAB = self.generator_B(BA)\n\n                # Reconstruction Loss\n                self.recon_loss_A = self.recon_criterion(ABA, A)\n                self.recon_loss_B = self.recon_criterion(BAB, B)\n\n                # Real/Fake GAN Loss (A)\n                A_dis_real, A_feats_real = self.discriminator_A(A)\n                A_dis_fake, A_feats_fake = self.discriminator_A(BA)\n\n                self.dis_loss_A, self.gen_loss_A = self.get_gan_loss(A_dis_real, A_dis_fake)\n                self.fm_loss_A = self.get_fm_loss(A_feats_real, A_feats_fake)\n\n                # Real/Fake GAN Loss (B)\n                B_dis_real, B_feats_real = self.discriminator_B(B)\n                B_dis_fake, B_feats_fake = self.discriminator_B(AB)\n\n                self.dis_loss_B, self.gen_loss_B = self.get_gan_loss(B_dis_real, B_dis_fake)\n                self.fm_loss_B = self.get_fm_loss(B_feats_real, B_feats_fake)\n\n                # Total Loss\n                if self.iters < self.args.gan_curriculum:\n                    rate = self.args.starting_rate\n                else:\n                    rate = self.args.default_rate\n\n                self.gen_loss_A_total = (self.gen_loss_B * 0.1 + self.fm_loss_B * 0.9) * (1. - rate) + self.recon_loss_A * rate\n                self.gen_loss_B_total = (self.gen_loss_A * 0.1 + self.fm_loss_A * 0.9) * (1. - rate) + self.recon_loss_B * rate\n\n                if self.args.model_arch == \'discogan\':\n                    self.gen_loss = self.gen_loss_A_total + self.gen_loss_B_total\n                    self.dis_loss = self.dis_loss_A + self.dis_loss_B\n                elif self.args.model_arch == \'recongan\':\n                    self.gen_loss = self.gen_loss_A_total\n                    self.dis_loss = self.dis_loss_B\n                elif self.args.model_arch == \'recongan_reverse\':\n                    self.gen_loss = self.gen_loss_B_total\n                    self.dis_loss = self.dis_loss_A\n                elif self.args.model_arch == \'gan\':\n                    self.gen_loss = (self.gen_loss_B * 0.1 + self.fm_loss_B * 0.9)\n                    self.dis_loss = self.dis_loss_B\n\n                self.finish_iteration()\n                self.iters += 1\n\n    def finish_iteration(self):\n\n        if self.iters % self.args.update_interval == 0:\n            self.dis_loss.backward()\n            self.optim_dis.step()\n        else:\n            self.gen_loss.backward()\n            self.optim_gen.step()\n\n        if self.iters % self.args.log_interval == 0:\n            print ""---------------------""\n            print ""GEN Loss:"", self.as_np(self.gen_loss_A.mean()), self.as_np(self.gen_loss_B.mean())\n            print ""Feature Matching Loss:"", self.as_np(self.fm_loss_A.mean()), self.as_np(self.fm_loss_B.mean())\n            print ""DIS Loss:"", self.as_np(self.dis_loss_A.mean()), self.as_np(self.dis_loss_B.mean())\n            print ""RECON Loss:"", self.as_np(self.recon_loss_A.mean()), self.as_np(self.recon_loss_B.mean())\n\n        if self.iters % self.args.image_save_interval == 0:\n            AB = self.generator_B(self.test_A)\n            BA = self.generator_A(self.test_B)\n            ABA = self.generator_A(AB)\n            BAB = self.generator_B(BA)\n\n            n_testset = min(self.test_A.size()[0], self.test_B.size()[0])\n            subdir_path = os.path.join(self.result_path, str(self.iters / self.args.image_save_interval))\n\n            if not os.path.exists(subdir_path):\n                os.makedirs(subdir_path)\n\n            for im_idx in range(n_testset):\n                A_val = self.test_A[im_idx].cpu().data.numpy().transpose(1, 2, 0) * 255.\n                B_val = self.test_B[im_idx].cpu().data.numpy().transpose(1, 2, 0) * 255.\n                BA_val = BA[im_idx].cpu().data.numpy().transpose(1, 2, 0) * 255.\n                ABA_val = ABA[im_idx].cpu().data.numpy().transpose(1, 2, 0) * 255.\n                AB_val = AB[im_idx].cpu().data.numpy().transpose(1, 2, 0) * 255.\n                BAB_val = BAB[im_idx].cpu().data.numpy().transpose(1, 2, 0) * 255.\n\n                filename_prefix = os.path.join(subdir_path, str(im_idx))\n                scipy.misc.imsave(filename_prefix + \'.A.jpg\', A_val.astype(np.uint8)[:, :, ::-1])\n                scipy.misc.imsave(filename_prefix + \'.B.jpg\', B_val.astype(np.uint8)[:, :, ::-1])\n                scipy.misc.imsave(filename_prefix + \'.BA.jpg\', BA_val.astype(np.uint8)[:, :, ::-1])\n                scipy.misc.imsave(filename_prefix + \'.AB.jpg\', AB_val.astype(np.uint8)[:, :, ::-1])\n                scipy.misc.imsave(filename_prefix + \'.ABA.jpg\', ABA_val.astype(np.uint8)[:, :, ::-1])\n                scipy.misc.imsave(filename_prefix + \'.BAB.jpg\', BAB_val.astype(np.uint8)[:, :, ::-1])\n\n\n        if self.iters % self.args.model_save_interval == 0:\n            torch.save(self.generator_A,\n                       os.path.join(self.model_path, \'model_gen_A-\' + str(self.iters / self.args.model_save_interval)))\n            torch.save(self.generator_B,\n                       os.path.join(self.model_path, \'model_gen_B-\' + str(self.iters / self.args.model_save_interval)))\n            torch.save(self.discriminator_A,\n                       os.path.join(self.model_path, \'model_dis_A-\' + str(self.iters / self.args.model_save_interval)))\n            torch.save(self.discriminator_B,\n                       os.path.join(self.model_path, \'model_dis_B-\' + str(self.iters / self.args.model_save_interval)))\n\nif __name__ == ""__main__"":\n    model = DiscoGAN()\n    model.run()'"
discogan_arch/distance_gan_angle_pairing_model.py,1,"b'from dataset import *\nfrom discogan_arch_options.options import AnglePairingOptions\nfrom distance_gan_model import DistanceGAN\nfrom model import *\n\n\nclass DistanceGANAnglePairing(DistanceGAN):\n\n    def get_data(self):\n        if self.args.task_name == \'car2car\':\n            data_A = get_cars(test=False, ver=180, half=\'first\', image_size=self.args.image_size)\n            data_B = get_cars(test=False, ver=180, half=\'last\', image_size=self.args.image_size)\n\n            test_A = test_B = get_cars(test=True, ver=180, image_size=self.args.image_size)\n\n        elif self.args.task_name == \'face2face\':\n            data_A = get_faces_3d(test=False, half=\'first\')\n            data_B = get_faces_3d(test=False, half=\'last\')\n\n            test_A = test_B = get_faces_3d(test=True)\n\n        elif self.args.task_name == \'chair2chair\':\n            data_A = get_chairs(test=False, half=\'first\', ver=360)\n            data_B = get_chairs(test=False, half=\'last\', ver=360)\n\n            test_A = test_B = get_chairs(test=True, ver=360)\n\n        elif self.args.task_name == \'chair2car\':\n            data_A = get_chairs(test=False, half=None, ver=180)\n            data_B = get_cars(test=False, half=None, ver=180)\n\n            test_A = get_chairs(test=True, ver=180)\n            test_B = get_cars(test=True, ver=180)\n\n        elif self.args.task_name == \'chair2face\':\n            data_A = get_chairs(test=False, half=None, ver=180)\n            data_B = get_faces_3d(test=False, half=None)\n\n            test_A = get_chairs(test=True, ver=180)\n            test_B = get_faces_3d(test=True)\n\n        elif self.args.task_name == \'car2face\':\n            data_A = get_cars(test=False, ver=180, half=None)\n            data_B = get_faces_3d(test=False, half=None)\n\n            test_A = get_cars(test=True, ver=180)\n            test_B = get_faces_3d(test=True)\n\n        return data_A, data_B, test_A, test_B\n\n    def get_fm_loss(self, real_feats, fake_feats):\n        losses = 0\n        for real_feat, fake_feat in zip(real_feats[1:], fake_feats[1:]):\n            l2 = (real_feat.mean(0) - fake_feat.mean(0)) * (real_feat.mean(0) - fake_feat.mean(0))\n            loss = self.feat_criterion(l2, Variable(torch.ones(l2.size())).cuda())\n            losses += loss\n\n        return losses\n\n    def get_test_images(self):\n        if self.args.task_name.startswith(\'car\') and self.args.task_name.endswith(\'car\'):\n            self.test_A = self.test_style_A\n            self.test_B = self.test_style_B\n\n        if self.args.task_name.startswith(\'car\') and not self.args.task_name.endswith(\'car\'):\n            self.test_A = self.test_style_A\n            self.test_B = read_images(self.test_style_B, None, self.args.image_size)\n\n        if not self.args.task_name.startswith(\'car\') and not self.args.task_name.endswith(\'car\'):\n            self.test_A = read_images(self.test_style_A, None, self.args.image_size)\n            self.test_B = read_images(self.test_style_B, None, self.args.image_size)\n\n    def get_images(self):\n        if self.args.task_name.startswith(\'car\') and self.args.task_name.endswith(\'car\'):\n            A = self.A_path\n            B = self.B_path\n\n        if self.args.task_name.startswith(\'car\') and not self.args.task_name.endswith(\'car\'):\n            A = self.A_path\n            B = read_images(self.B_path, None, self.args.image_size)\n\n        if not self.args.task_name.startswith(\'car\') and not self.args.task_name.endswith(\'car\'):\n            A = read_images(self.A_path, None, self.args.image_size)\n            B = read_images(self.B_path, None, self.args.image_size)\n\n        return A, B\n\n\n    def __init__(self):\n        options = AnglePairingOptions()\n        options.initialize()\n        self.args = options.parser.parse_args()\n\n\nif __name__ == ""__main__"":\n    model = DistanceGANAnglePairing()\n    model.run()'"
discogan_arch/distance_gan_model.py,20,"b'from dataset import *\nfrom disco_gan_model import DiscoGAN\nfrom model import *\nimport scipy\nfrom progressbar import ETA, Bar, Percentage, ProgressBar\n\nclass DistanceGAN(DiscoGAN):\n\n    def distance(self, A, B):\n        return torch.mean(torch.abs(A - B))\n\n    def get_individual_distance_loss(self, A_i, A_j, AB_i, AB_j,\n                                     B_i, B_j, BA_i, BA_j):\n\n        distance_in_A = self.distance(A_i, A_j)\n        distance_in_AB = self.distance(AB_i, AB_j)\n        distance_in_B = self.distance(B_i, B_j)\n        distance_in_BA = self.distance(BA_i, BA_j)\n\n        if self.normalize_distances:\n            distance_in_A = (distance_in_A - self.expectation_A) / self.std_A\n            distance_in_AB = (distance_in_AB - self.expectation_B) / self.std_B\n            distance_in_B = (distance_in_B - self.expectation_B) / self.std_B\n            distance_in_BA = (distance_in_BA - self.expectation_A) / self.std_A\n\n        return torch.abs(distance_in_A - distance_in_AB), torch.abs(distance_in_B - distance_in_BA)\n\n    def get_self_distances(self, A, B, AB, BA):\n\n        A_half_1, A_half_2 = torch.chunk(A, 2, dim=2)\n        B_half_1, B_half_2 = torch.chunk(B, 2, dim=2)\n        AB_half_1, AB_half_2 = torch.chunk(AB, 2, dim=2)\n        BA_half_1, BA_half_2 = torch.chunk(BA, 2, dim=2)\n\n        l_distance_A, l_distance_B = \\\n            self.get_individual_distance_loss(A_half_1, A_half_2,\n                                              AB_half_1, AB_half_2,\n                                              B_half_1, B_half_2,\n                                              BA_half_1, BA_half_2)\n\n        return l_distance_A, l_distance_B\n\n    def get_distance_losses(self, A, B, AB, BA):\n\n        As = torch.split(A, 1)\n        Bs = torch.split(B, 1)\n        ABs = torch.split(AB, 1)\n        BAs = torch.split(BA, 1)\n\n        loss_distance_A = 0.0\n        loss_distance_B = 0.0\n        num_pairs = 0\n        min_length = min(len(As), len(Bs))\n\n        for i in xrange(min_length - 1):\n            for j in xrange(i + 1, min_length):\n                num_pairs += 1\n                loss_distance_A_ij, loss_distance_B_ij = \\\n                    self.get_individual_distance_loss(As[i], As[j],\n                                                      ABs[i], ABs[j],\n                                                      Bs[i], Bs[j],\n                                                      BAs[i], BAs[j])\n\n                loss_distance_A += loss_distance_A_ij\n                loss_distance_B += loss_distance_B_ij\n\n        loss_distance_A = loss_distance_A / num_pairs\n        loss_distance_B = loss_distance_B / num_pairs\n\n        return loss_distance_A, loss_distance_B\n\n    def get_std(self, num_items, vars, expectation):\n\n        num_pairs = 0\n        std_sum = 0.0\n\n        # If self distance computed std for top and bottom half\n        if self.args.use_self_distance:\n            for i in xrange(num_items):\n                var_half_1, var_half_2 = torch.chunk(vars[i], 2, dim=2)\n                std_sum += np.square(self.as_np(self.distance(var_half_1, var_half_2)) - expectation)\n            return np.sqrt(std_sum / num_items)\n\n        # Otherwise compute std for all pairs of images\n        for i in xrange(num_items - 1):\n            for j in xrange(i + 1, num_items):\n                num_pairs += 1\n                std_sum += np.square(self.as_np(self.distance(vars[i], vars[j])) - expectation)\n\n        return np.sqrt(std_sum / num_pairs)\n\n    def get_expectation(self, num_items, vars):\n\n        num_pairs = 0\n        distance_sum = 0.0\n\n        # If self distance computed expectation for top and bottom half\n        if self.args.use_self_distance:\n            for i in xrange(num_items):\n                # Split image to top and bottom half\n                var_half_1, var_half_2 = torch.chunk(vars[i], 2, dim=2)\n                distance_sum += self.as_np(self.distance(var_half_1, var_half_2))\n            return distance_sum / num_items\n\n        # Otherwise compute expectation for all pairs of images\n        for i in xrange(num_items - 1):\n            for j in xrange(i + 1, num_items):\n                num_pairs += 1\n                distance_sum += self.as_np(self.distance(vars[i], vars[j]))\n\n        return distance_sum / num_pairs\n\n    def set_expectation_and_std(self):\n\n        max_items = self.args.max_items\n\n        data_style_A, data_style_B = shuffle_data(self.data_style_A, self.data_style_B)\n\n        if max_items < len(data_style_A):\n            self.A_path = data_style_A[0:max_items]\n        else:\n            self.A_path = data_style_A\n\n        if max_items < len(data_style_B):\n            self.B_path = data_style_B[0:max_items]\n        else:\n            self.B_path = data_style_B\n\n        dataset_A, dataset_B = self.get_images()\n\n        A_vars = []\n        num_vars_A = 0\n\n        for step_data_a, data in enumerate(dataset_A):\n\n            if step_data_a >= max_items:\n                break\n\n            A = Variable(torch.FloatTensor(data), volatile=True)\n            if self.cuda:\n                A = A.cuda()\n\n            A_vars.append(A)\n            num_vars_A += 1\n\n        B_vars = []\n        num_vars_B = 0\n        for step_data_b, data in enumerate(dataset_B):\n\n            if step_data_b >= max_items:\n                break\n\n            B = Variable(torch.FloatTensor(data), volatile=True)\n            if self.cuda:\n                B = B.cuda()\n\n            B_vars.append(B)\n            num_vars_B += 1\n\n\n        self.expectation_A = self.get_expectation(num_vars_A, A_vars)[0].astype(float)\n        self.expectation_B = self.get_expectation(num_vars_B, B_vars)[0].astype(float)\n        self.std_A = self.get_std(num_vars_A, A_vars, self.expectation_A)[0].astype(float)\n        self.std_B = self.get_std(num_vars_B, B_vars, self.expectation_B)[0].astype(float)\n\n        print(\'Expectation for dataset A: %f\' % self.expectation_A)\n        print(\'Expectation for dataset B: %f\' % self.expectation_B)\n        print(\'Std for dataset A: %f\' % self.std_A)\n        print(\'Std for dataset B: %f\' % self.std_B)\n\n    def run(self):\n\n        self.initialize()\n        self.normalize_distances = not self.args.unnormalized_distances\n\n        if self.normalize_distances:\n            self.set_expectation_and_std()\n\n        self.iters = 0\n\n        for epoch in range(self.args.epoch_size):\n            data_style_A, data_style_B = shuffle_data(self.data_style_A, self.data_style_B)\n\n            widgets = [\'epoch #%d|\' % epoch, Percentage(), Bar(), ETA()]\n            pbar = ProgressBar(maxval=self.n_batches, widgets=widgets)\n            pbar.start()\n\n            for i in range(self.n_batches):\n\n                pbar.update(i)\n\n                self.generator_A.zero_grad()\n                self.generator_B.zero_grad()\n                self.discriminator_A.zero_grad()\n                self.discriminator_B.zero_grad()\n\n                self.A_path = data_style_A[i * self.args.batch_size: (i + 1) * self.args.batch_size]\n                self.B_path = data_style_B[i * self.args.batch_size: (i + 1) * self.args.batch_size]\n\n                A, B = self.get_images()\n                A = Variable(torch.FloatTensor(A))\n                B = Variable(torch.FloatTensor(B))\n\n                if self.cuda:\n                    A = A.cuda()\n                    B = B.cuda()\n\n                AB = self.generator_B(A)\n                BA = self.generator_A(B)\n\n                if self.args.use_reconst_loss:\n                    ABA = self.generator_A(AB)\n                    BAB = self.generator_B(BA)\n\n                    # Reconstruction Loss\n                    self.recon_loss_A = self.recon_criterion(ABA, A)\n                    self.recon_loss_B = self.recon_criterion(BAB, B)\n\n                if not self.args.use_self_distance:\n                    self.loss_distance_A, self.loss_distance_B = self.get_distance_losses(A, B, AB, BA)\n                else:\n                    self.loss_distance_A, self.loss_distance_B = self.get_self_distances(A, B, AB, BA)\n\n                # Real/Fake GAN Loss (A)\n                A_dis_real, A_feats_real = self.discriminator_A(A)\n                A_dis_fake, A_feats_fake = self.discriminator_A(BA)\n\n                self.dis_loss_A, self.gen_loss_A = self.get_gan_loss(A_dis_real, A_dis_fake)\n                self.fm_loss_A = self.get_fm_loss(A_feats_real, A_feats_fake)\n\n                # Real/Fake GAN Loss (B)\n                B_dis_real, B_feats_real = self.discriminator_B(B)\n                B_dis_fake, B_feats_fake = self.discriminator_B(AB)\n\n                self.dis_loss_B, self.gen_loss_B = self.get_gan_loss(B_dis_real, B_dis_fake)\n                self.fm_loss_B = self.get_fm_loss(B_feats_real, B_feats_fake)\n\n                # Total Loss\n                if self.iters < self.args.gan_curriculum:\n                    rate = self.args.starting_rate\n                else:\n                    rate = self.args.default_rate\n\n                non_gan_loss_A = self.loss_distance_A\n                non_gan_loss_B = self.loss_distance_B\n\n                if self.args.use_reconst_loss:\n                    non_gan_loss_A += self.recon_loss_A\n                    non_gan_loss_B += self.recon_loss_B\n\n                self.gen_loss_A_total = (self.gen_loss_B * 0.1 + self.fm_loss_B * 0.9) * (1. - rate) + non_gan_loss_A * rate\n                self.gen_loss_B_total = (self.gen_loss_A * 0.1 + self.fm_loss_A * 0.9) * (1. - rate) + non_gan_loss_B * rate\n\n                if self.args.model_arch == \'distancegan\':\n                    self.gen_loss = self.gen_loss_A_total + self.gen_loss_B_total\n                    self.dis_loss = self.dis_loss_A + self.dis_loss_B\n                elif self.args.model_arch == \'distance_A_to_B\':\n                    self.gen_loss = self.gen_loss_A_total\n                    self.dis_loss = self.dis_loss_B\n                elif self.args.model_arch == \'distance_B_to_A\':\n                    self.gen_loss = self.gen_loss_B_total\n                    self.dis_loss = self.dis_loss_A\n                elif self.args.model_arch == \'gan\':\n                    self.gen_loss = (self.gen_loss_B * 0.1 + self.fm_loss_B * 0.9)\n                    self.dis_loss = self.dis_loss_B\n\n                self.finish_iteration()\n                self.iters += 1\n\n    def finish_iteration(self):\n\n        if self.iters % self.args.update_interval == 0:\n            self.dis_loss.backward()\n            self.optim_dis.step()\n        else:\n            self.gen_loss.backward()\n            self.optim_gen.step()\n\n        if self.iters % self.args.log_interval == 0:\n            print ""---------------------""\n            print ""GEN Loss:"", self.as_np(self.gen_loss_A.mean()), self.as_np(self.gen_loss_B.mean())\n            print ""Feature Matching Loss:"", self.as_np(self.fm_loss_A.mean()), self.as_np(self.fm_loss_B.mean())\n            print ""DIS Loss:"", self.as_np(self.dis_loss_A.mean()), self.as_np(self.dis_loss_B.mean())\n            print ""Distance Loss:"", self.as_np(self.loss_distance_A.mean()), self.as_np(self.loss_distance_B.mean())\n            if self.args.use_reconst_loss:\n                print ""RECON Loss:"", self.as_np(self.recon_loss_A.mean()), self.as_np(self.recon_loss_B.mean())\n\n        if self.iters % self.args.image_save_interval == 0:\n            AB = self.generator_B(self.test_A)\n            BA = self.generator_A(self.test_B)\n\n            if self.args.use_reconst_loss:\n                ABA = self.generator_A(AB)\n                BAB = self.generator_B(BA)\n\n            n_testset = min(self.test_A.size()[0], self.test_B.size()[0])\n            subdir_path = os.path.join(self.result_path, str(self.iters / self.args.image_save_interval))\n\n            if not os.path.exists(subdir_path):\n                os.makedirs(subdir_path)\n\n            for im_idx in range(n_testset):\n                A_val = self.test_A[im_idx].cpu().data.numpy().transpose(1, 2, 0) * 255.\n                B_val = self.test_B[im_idx].cpu().data.numpy().transpose(1, 2, 0) * 255.\n                BA_val = BA[im_idx].cpu().data.numpy().transpose(1, 2, 0) * 255.\n                AB_val = AB[im_idx].cpu().data.numpy().transpose(1, 2, 0) * 255.\n\n                filename_prefix = os.path.join(subdir_path, str(im_idx))\n                scipy.misc.imsave(filename_prefix + \'.A.jpg\', A_val.astype(np.uint8)[:, :, ::-1])\n                scipy.misc.imsave(filename_prefix + \'.B.jpg\', B_val.astype(np.uint8)[:, :, ::-1])\n                scipy.misc.imsave(filename_prefix + \'.BA.jpg\', BA_val.astype(np.uint8)[:, :, ::-1])\n                scipy.misc.imsave(filename_prefix + \'.AB.jpg\', AB_val.astype(np.uint8)[:, :, ::-1])\n\n                if self.args.use_reconst_loss:\n                    ABA_val = ABA[im_idx].cpu().data.numpy().transpose(1, 2, 0) * 255.\n                    BAB_val = BAB[im_idx].cpu().data.numpy().transpose(1, 2, 0) * 255.\n                    scipy.misc.imsave(filename_prefix + \'.ABA.jpg\', ABA_val.astype(np.uint8)[:, :, ::-1])\n                    scipy.misc.imsave(filename_prefix + \'.BAB.jpg\', BAB_val.astype(np.uint8)[:, :, ::-1])\n\n        if self.iters % self.args.model_save_interval == 0:\n            torch.save(self.generator_A,\n                       os.path.join(self.model_path, \'model_gen_A-\' + str(self.iters / self.args.model_save_interval)))\n            torch.save(self.generator_B,\n                       os.path.join(self.model_path, \'model_gen_B-\' + str(self.iters / self.args.model_save_interval)))\n            torch.save(self.discriminator_A,\n                       os.path.join(self.model_path, \'model_dis_A-\' + str(self.iters / self.args.model_save_interval)))\n            torch.save(self.discriminator_B,\n                       os.path.join(self.model_path, \'model_dis_B-\' + str(self.iters / self.args.model_save_interval)))\n\nif __name__ == ""__main__"":\n    model = DistanceGAN()\n    model.run()'"
discogan_arch/model.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\n\nimport numpy as np\n\nkernel_sizes = [4,3,3]\nstrides = [2,2,1]\npaddings=[0,0,1]\n\nlatent_dim = 300\n\nclass Discriminator(nn.Module):\n    def __init__(\n            self,\n            ):\n\n        super(Discriminator, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, 4, 2, 1, bias=False)\n        self.relu1 = nn.LeakyReLU(0.2, inplace=True)\n\n        self.conv2 = nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False)\n        self.bn2 = nn.BatchNorm2d(64 * 2)\n        self.relu2 = nn.LeakyReLU(0.2, inplace=True)\n\n        self.conv3 = nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False)\n        self.bn3 = nn.BatchNorm2d(64 * 4)\n        self.relu3 = nn.LeakyReLU(0.2, inplace=True)\n\n        self.conv4 = nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False)\n        self.bn4 = nn.BatchNorm2d(64 * 8)\n        self.relu4 = nn.LeakyReLU(0.2, inplace=True)\n\n        self.conv5 = nn.Conv2d(64 * 8, 1, 4, 1, 0, bias=False)\n\n    def forward(self, input):\n        conv1 = self.conv1( input )\n        relu1 = self.relu1( conv1 )\n\n        conv2 = self.conv2( relu1 )\n        bn2 = self.bn2( conv2 )\n        relu2 = self.relu2( bn2 )\n\n        conv3 = self.conv3( relu2 )\n        bn3 = self.bn3( conv3 )\n        relu3 = self.relu3( bn3 )\n\n        conv4 = self.conv4( relu3 )\n        bn4 = self.bn4( conv4 )\n        relu4 = self.relu4( bn4 )\n\n        conv5 = self.conv5( relu4 )\n\n        return torch.sigmoid( conv5 ), [relu2, relu3, relu4]\n\nclass Generator(nn.Module):\n    def __init__(\n            self,\n            num_layers=4\n            ):\n\n        super(Generator, self).__init__()\n\n        if num_layers == 5:\n            self.main = nn.Sequential(\n                nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64 * 2),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64 * 4),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64 * 8),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(64 * 8, 100, 4, 1, 0, bias=False),\n                nn.BatchNorm2d(100),\n                nn.LeakyReLU(0.2, inplace=True),\n\n                nn.ConvTranspose2d(100, 64 * 8, 4, 1, 0, bias=False),\n                nn.BatchNorm2d(64 * 8),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64 * 4),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64 * 2),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(64 * 2,     64, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(    64,      3, 4, 2, 1, bias=False),\n                nn.Sigmoid()\n            )\n\n\n        if num_layers == 4:\n            self.main = nn.Sequential(\n                nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64 * 2),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64 * 4),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(64 * 4, 64 * 8, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64 * 8),\n                nn.LeakyReLU(0.2, inplace=True),\n\n                nn.ConvTranspose2d(64 * 8, 64 * 4, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64 * 4),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64 * 2),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(64 * 2,     64, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(    64,      3, 4, 2, 1, bias=False),\n                nn.Sigmoid()\n            )\n\n        if num_layers == 3:\n            self.main = nn.Sequential(\n                nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(64, 64 * 2, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64 * 2),\n                nn.LeakyReLU(0.2, inplace=True),\n                nn.Conv2d(64 * 2, 64 * 4, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64 * 4),\n                nn.LeakyReLU(0.2, inplace=True),\n\n                nn.ConvTranspose2d(64 * 4, 64 * 2, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64 * 2),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(64 * 2,     64, 4, 2, 1, bias=False),\n                nn.BatchNorm2d(64),\n                nn.ReLU(True),\n                nn.ConvTranspose2d(    64,      3, 4, 2, 1, bias=False),\n                nn.Sigmoid()\n            )\n\n    def forward(self, input):\n        return self.main( input )'"
scripts/__init__.py,0,b''
cyclegan_arch/cyclegan_arch_options/__init__.py,0,b''
cyclegan_arch/cyclegan_arch_options/base_options.py,0,"b'import argparse\nimport os\n\nfrom ..util import util\n\n\nclass BaseOptions(object):\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.initialized = False\n\n    def initialize(self):\n        self.parser.add_argument(\'--dataroot\', required=True, help=\'path to images (should have subfolders trainA, trainB, valA, valB, etc)\')\n        self.parser.add_argument(\'--batchSize\', type=int, default=2, help=\'input batch size\')\n        self.parser.add_argument(\'--loadSize\', type=int, default=286, help=\'scale images to this size\')\n        self.parser.add_argument(\'--fineSize\', type=int, default=256, help=\'then crop to this size\')\n        self.parser.add_argument(\'--input_nc\', type=int, default=3, help=\'# of input image channels\')\n        self.parser.add_argument(\'--output_nc\', type=int, default=3, help=\'# of output image channels\')\n        self.parser.add_argument(\'--ngf\', type=int, default=64, help=\'# of gen filters in first conv layer\')\n        self.parser.add_argument(\'--ndf\', type=int, default=64, help=\'# of discrim filters in first conv layer\')\n        self.parser.add_argument(\'--which_model_netD\', type=str, default=\'basic\', help=\'selects model to use for netD\')\n        self.parser.add_argument(\'--which_model_netG\', type=str, default=\'resnet_9blocks\', help=\'selects model to use for netG\')\n        self.parser.add_argument(\'--n_layers_D\', type=int, default=3, help=\'only used if which_model_netD==n_layers\')\n        self.parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2\')\n        self.parser.add_argument(\'--name\', type=str, default=\'experiment_name\', help=\'name of the experiment. It decides where to store samples and models\')\n        self.parser.add_argument(\'--align_data\', action=\'store_true\',\n                                help=\'if True, the datasets are loaded from ""test"" and ""train"" directories and the data pairs are aligned\')\n        self.parser.add_argument(\'--model\', type=str, default=\'distance_gan\',\n                                 help=\'chooses which model to use. cycle_gan, one_direction_test, pix2pix, ...\')\n        self.parser.add_argument(\'--which_direction\', type=str, default=\'AtoB\', help=\'AtoB or BtoA\')\n        self.parser.add_argument(\'--nThreads\', default=2, type=int, help=\'# threads for loading data\')\n        self.parser.add_argument(\'--checkpoints_dir\', type=str, default=\'./checkpoints\', help=\'models are saved here\')\n        self.parser.add_argument(\'--norm\', type=str, default=\'instance\', help=\'instance normalization or batch normalization\')\n        self.parser.add_argument(\'--serial_batches\', action=\'store_true\', help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n        self.parser.add_argument(\'--display_winsize\', type=int, default=256,  help=\'display window size\')\n        self.parser.add_argument(\'--display_id\', type=int, default=1, help=\'window id of the web display\')\n        self.parser.add_argument(\'--identity\', type=float, default=0.0, help=\'use identity mapping. Setting identity other than 1 has an effect of scaling the weight of the identity mapping loss. For example, if the weight of the identity loss should be 10 times smaller than the weight of the reconstruction loss, please set optidentity = 0.1\')\n        self.parser.add_argument(\'--use_dropout\', action=\'store_true\', help=\'use dropout for the generator\')\n        self.parser.add_argument(\'--max_dataset_size\', type=int, default=float(""inf""), help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n        self.parser.add_argument(\'--use_self_distance\', action=\'store_true\', help=""use distance for top and bottom half of the image"")\n        self.parser.add_argument(\'--A_to_B\', action=\'store_true\', help=\'only train from A to B\')\n        self.parser.add_argument(\'--B_to_A\', action=\'store_true\', help=\'only train from B to A\')\n        self.parser.add_argument(\'--unnormalized_distances\', action=\'store_true\', help=\'do not normalize distances by expecatation and std\')\n        self.parser.add_argument(\'--use_cycle_loss\', action=\'store_true\', help=\'add cycle loss in addition to distance loss\')\n\n\n        self.initialized = True\n\n    def parse(self):\n        if not self.initialized:\n            self.initialize()\n        self.opt = self.parser.parse_args()\n        self.opt.isTrain = self.isTrain   # train or test\n\n        str_ids = self.opt.gpu_ids.split(\',\')\n        self.opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                self.opt.gpu_ids.append(id)\n\n        args = vars(self.opt)\n\n        print(\'------------ Options -------------\')\n        for k, v in sorted(args.items()):\n            print(\'%s: %s\' % (str(k), str(v)))\n        print(\'-------------- End ----------------\')\n\n        # save to the disk\n        expr_dir =  os.path.join(self.opt.checkpoints_dir, self.opt.name)\n        util.mkdirs(expr_dir)\n        file_name = os.path.join(expr_dir, \'opt.txt\')\n        with open(file_name, \'wt\') as opt_file:\n            opt_file.write(\'------------ Options -------------\\n\')\n            for k, v in sorted(args.items()):\n                opt_file.write(\'%s: %s\\n\' % (str(k), str(v)))\n            opt_file.write(\'-------------- End ----------------\\n\')\n        return self.opt\n'"
cyclegan_arch/cyclegan_arch_options/test_options.py,0,"b'from .base_options import BaseOptions\n\nclass TestOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument(\'--ntest\', type=int, default=float(""inf""), help=\'# of test examples.\')\n        self.parser.add_argument(\'--results_dir\', type=str, default=\'./results\', help=\'saves results_cycle here.\')\n        self.parser.add_argument(\'--aspect_ratio\', type=float, default=1.0, help=\'aspect ratio of result images\')\n        self.parser.add_argument(\'--phase\', type=str, default=\'test\', help=\'train, val, test, etc\')\n        self.parser.add_argument(\'--which_epoch\', type=str, default=\'latest\', help=\'which epoch to load? set to latest to use latest cached model\')\n        self.parser.add_argument(\'--how_many\', type=int, default=50, help=\'how many test images to run\')\n        self.isTrain = False\n'"
cyclegan_arch/cyclegan_arch_options/train_options.py,0,"b""from .base_options import BaseOptions\n\nclass TrainOptions(BaseOptions):\n    def initialize(self):\n        BaseOptions.initialize(self)\n        self.parser.add_argument('--display_freq', type=int, default=100, help='frequency of showing training results_cycle on screen')\n        self.parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results_cycle on console')\n        self.parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results_cycle')\n        self.parser.add_argument('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n        self.parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n        self.parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n        self.parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n        self.parser.add_argument('--niter', type=int, default=100, help='# of iter at starting learning rate')\n        self.parser.add_argument('--niter_decay', type=int, default=100, help='# of iter to linearly decay learning rate to zero')\n        self.parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n        self.parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n        self.parser.add_argument('--no_lsgan', action='store_true', help='do *not* use least square GAN, if false, use vanilla GAN')\n        self.parser.add_argument('--lambda_A', type=float, default=10.0, help='weight for cycle loss (A -> B -> A)')\n        self.parser.add_argument('--lambda_B', type=float, default=10.0, help='weight for cycle loss (B -> A -> B)')\n        self.parser.add_argument('--lambda_distance_A', type=float, default=1, help='weight for distance loss (A -> B)')\n        self.parser.add_argument('--lambda_distance_B', type=float, default=1, help='weight for distance loss (B -> A)')\n        self.parser.add_argument('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n        self.parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results_cycle to [opt.checkpoints_dir]/[opt.name]/web/')\n        self.parser.add_argument('--no_flip'  , action='store_true', help='if specified, do not flip the images for data argumentation')\n        self.parser.add_argument('--max_items', type=int, default=900, help='maximum number of items to use for expectation and std calculation')\n\n        self.isTrain = True\n"""
cyclegan_arch/data/__init__.py,0,b''
cyclegan_arch/data/aligned_data_loader.py,3,"b""import random\nimport torch.utils.data\nimport torchvision.transforms as transforms\nfrom data.base_data_loader import BaseDataLoader\nfrom data.image_folder import ImageFolder\nfrom future_builtins import object\n\nclass PairedData(object):\n    def __init__(self, data_loader, fineSize, max_dataset_size, flip):\n        self.data_loader = data_loader\n        self.fineSize = fineSize\n        self.max_dataset_size = max_dataset_size\n        self.flip = flip\n\n    def __iter__(self):\n        self.data_loader_iter = iter(self.data_loader)\n        self.iter = 0\n        return self\n\n    def __next__(self):\n        self.iter += 1\n        if self.iter > self.max_dataset_size:\n            raise StopIteration\n\n        AB, AB_paths = next(self.data_loader_iter)\n        w_total = AB.size(3)\n        w = int(w_total / 2)\n        h = AB.size(2)\n\n        w_offset = random.randint(0, max(0, w - self.fineSize - 1))\n        h_offset = random.randint(0, max(0, h - self.fineSize - 1))\n        A = AB[:, :, h_offset:h_offset + self.fineSize,\n               w_offset:w_offset + self.fineSize]\n        B = AB[:, :, h_offset:h_offset + self.fineSize,\n               w + w_offset:w + w_offset + self.fineSize]\n\n        if self.flip and random.random() < 0.5:\n            idx = [i for i in range(A.size(3) - 1, -1, -1)]\n            idx = torch.LongTensor(idx)\n            A = A.index_select(3, idx)\n            B = B.index_select(3, idx)\n\n        return {'A': A, 'A_paths': AB_paths, 'B': B, 'B_paths': AB_paths}\n\n\nclass AlignedDataLoader(BaseDataLoader):\n    def initialize(self, opt):\n        BaseDataLoader.initialize(self, opt)\n        self.fineSize = opt.fineSize\n\n        transformations = [\n            # TODO: Scale\n            transforms.Scale(opt.loadSize),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5),\n                                 (0.5, 0.5, 0.5))]\n        transform = transforms.Compose(transformations)\n\n        # Dataset A\n        dataset = ImageFolder(root=opt.dataroot + '/' + opt.phase,\n                              transform=transform, return_paths=True)\n        data_loader = torch.utils.data.DataLoader(\n            dataset,\n            batch_size=self.opt.batchSize,\n            shuffle=not self.opt.serial_batches,\n            num_workers=int(self.opt.nThreads))\n\n        self.dataset = dataset\n\n        flip = opt.isTrain and not opt.no_flip\n        self.paired_data = PairedData(data_loader, opt.fineSize, \n                                      opt.max_dataset_size, flip)\n\n    def name(self):\n        return 'AlignedDataLoader'\n\n    def load_data(self):\n        return self.paired_data\n\n    def __len__(self):\n        return min(len(self.dataset), self.opt.max_dataset_size)\n"""
cyclegan_arch/data/base_data_loader.py,0,"b'\nclass BaseDataLoader(object):\n    def __init__(self):\n        pass\n    \n    def initialize(self, opt):\n        self.opt = opt\n        pass\n\n    def load_data(self):\n        return None\n\n        \n        \n'"
cyclegan_arch/data/data_loader.py,0,b'\ndef CreateDataLoader(opt):\n    if opt.align_data > 0:\n        from cyclegan_arch.data.aligned_data_loader import AlignedDataLoader\n        data_loader = AlignedDataLoader()\n    else:\n        from unaligned_data_loader import UnalignedDataLoader\n        data_loader = UnalignedDataLoader()\n    print(data_loader.name())\n    data_loader.initialize(opt)\n    return data_loader\n'
cyclegan_arch/data/image_folder.py,1,"b'################################################################################\n# Code from\n# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n# Modified the original code so that it also loads images from the current\n# directory as well as the subdirectories\n################################################################################\n\nimport torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef make_dataset(dir):\n    images = []\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, _, fnames in sorted(os.walk(dir)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                images.append(path)\n\n    return images\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\n\nclass ImageFolder(data.Dataset):\n\n    def __init__(self, root, transform=None, return_paths=False,\n                 loader=default_loader):\n        imgs = make_dataset(root)\n        if len(imgs) == 0:\n            raise(RuntimeError(""Found 0 images in: "" + root + ""\\n""\n                               ""Supported image extensions are: "" + "","".join(IMG_EXTENSIONS)))\n\n        self.root = root\n        self.imgs = imgs\n        self.transform = transform\n        self.return_paths = return_paths\n        self.loader = loader\n\n    def __getitem__(self, index):\n        path = self.imgs[index]\n        img = self.loader(path)\n        if self.transform is not None:\n            img = self.transform(img)\n        if self.return_paths:\n            return img, path\n        else:\n            return img\n\n    def __len__(self):\n        return len(self.imgs)\n'"
cyclegan_arch/data/unaligned_data_loader.py,4,"b""import random\nimport torch.utils.data\nimport torchvision.transforms as transforms\nfrom base_data_loader import BaseDataLoader\nfrom image_folder import ImageFolder\nfrom builtins import object\n\nclass PairedData(object):\n    def __init__(self, data_loader_A, data_loader_B, max_dataset_size, flip):\n        self.data_loader_A = data_loader_A\n        self.data_loader_B = data_loader_B\n        self.stop_A = False\n        self.stop_B = False\n        self.max_dataset_size = max_dataset_size\n        self.flip = flip\n\n    def __iter__(self):\n        self.stop_A = False\n        self.stop_B = False\n        self.data_loader_A_iter = iter(self.data_loader_A)\n        self.data_loader_B_iter = iter(self.data_loader_B)\n        self.iter = 0\n        return self\n\n    def __next__(self):\n        A, A_paths = None, None\n        B, B_paths = None, None\n        try:\n            A, A_paths = next(self.data_loader_A_iter)\n        except StopIteration:\n            if A is None or A_paths is None:\n                self.stop_A = True\n                self.data_loader_A_iter = iter(self.data_loader_A)\n                A, A_paths = next(self.data_loader_A_iter)\n\n        try:\n            B, B_paths = next(self.data_loader_B_iter)\n        except StopIteration:\n            if B is None or B_paths is None:\n                self.stop_B = True\n                self.data_loader_B_iter = iter(self.data_loader_B)\n                B, B_paths = next(self.data_loader_B_iter)\n\n        if (self.stop_A and self.stop_B) or self.iter > self.max_dataset_size:\n            self.stop_A = False\n            self.stop_B = False\n            raise StopIteration()\n        else:\n            self.iter += 1\n            if self.flip and random.random() < 0.5:\n                idx = [i for i in range(A.size(3) - 1, -1, -1)]\n                idx = torch.LongTensor(idx)\n                A = A.index_select(3, idx)\n                B = B.index_select(3, idx)\n            return {'A': A, 'A_paths': A_paths,\n                    'B': B, 'B_paths': B_paths}\n\nclass UnalignedDataLoader(BaseDataLoader):\n    def initialize(self, opt):\n        BaseDataLoader.initialize(self, opt)\n        transformations = [transforms.Scale(opt.loadSize),\n                           transforms.RandomCrop(opt.fineSize),\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.5, 0.5, 0.5),\n                                                (0.5, 0.5, 0.5))]\n        transform = transforms.Compose(transformations)\n\n        # Dataset A\n        dataset_A = ImageFolder(root=opt.dataroot + '/' + opt.phase + 'A',\n                                transform=transform, return_paths=True)\n        data_loader_A = torch.utils.data.DataLoader(\n            dataset_A,\n            batch_size=self.opt.batchSize,\n            shuffle=not self.opt.serial_batches,\n            num_workers=int(self.opt.nThreads))\n\n        # Dataset B\n        dataset_B = ImageFolder(root=opt.dataroot + '/' + opt.phase + 'B',\n                                transform=transform, return_paths=True)\n        data_loader_B = torch.utils.data.DataLoader(\n            dataset_B,\n            batch_size=self.opt.batchSize,\n            shuffle=not self.opt.serial_batches,\n            num_workers=int(self.opt.nThreads))\n        self.dataset_A = dataset_A\n        self.dataset_B = dataset_B\n        flip = opt.isTrain and not opt.no_flip\n        self.paired_data = PairedData(data_loader_A, data_loader_B, \n                                      self.opt.max_dataset_size, flip)\n\n    def name(self):\n        return 'UnalignedDataLoader'\n\n    def load_data(self):\n        return self.paired_data\n\n    def __len__(self):\n        return min(max(len(self.dataset_A), len(self.dataset_B)), self.opt.max_dataset_size)\n"""
cyclegan_arch/mnist_to_svhn/__init__.py,0,b''
cyclegan_arch/mnist_to_svhn/data_loader.py,4,"b'import torch\nfrom torchvision import datasets\nfrom torchvision import transforms\n\ndef get_loader(config):\n    """"""Builds and returns Dataloader for MNIST and SVHN dataset.""""""\n    \n    transform = transforms.Compose([\n                    transforms.Scale(config.image_size),\n                    transforms.ToTensor(),\n                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    \n    svhn = datasets.SVHN(root=config.svhn_path, download=True, transform=transform, split=\'train\')\n    mnist = datasets.MNIST(root=config.mnist_path, download=True, transform=transform, train=True)\n\n    svhn_test = datasets.SVHN(root=config.svhn_path, download=True, transform=transform, split=\'test\')\n    mnist_test = datasets.MNIST(root=config.mnist_path, download=True, transform=transform, train=False)\n\n    svhn_loader = torch.utils.data.DataLoader(dataset=svhn,\n                                              batch_size=config.batch_size,\n                                              shuffle=True,\n                                              num_workers=config.num_workers)\n\n    mnist_loader = torch.utils.data.DataLoader(dataset=mnist,\n                                               batch_size=config.batch_size,\n                                               shuffle=True,\n                                               num_workers=config.num_workers)\n\n\n    svhn_test_loader = torch.utils.data.DataLoader(dataset=svhn_test,\n                                              batch_size=config.batch_size,\n                                              shuffle=False,\n                                              num_workers=config.num_workers)\n\n    mnist_test_loader = torch.utils.data.DataLoader(dataset=mnist_test,\n                                               batch_size=config.batch_size,\n                                               shuffle=False,\n                                               num_workers=config.num_workers)\n\n    return svhn_loader, mnist_loader, svhn_test_loader, mnist_test_loader'"
cyclegan_arch/mnist_to_svhn/main.py,1,"b""import argparse\nimport os\nfrom solver import Solver\nfrom torch.backends import cudnn\nfrom data_loader import get_loader\n\ndef str2bool(v):\n    return v.lower() in ('true')\n\ndef main(config):\n\n    svhn_loader, mnist_loader, svhn_test_loader, mnist_test_loader = get_loader(config)\n\n    solver = Solver(config, svhn_loader, mnist_loader)\n    cudnn.benchmark = True\n\n    # create directories if not exist\n    if not os.path.exists(config.model_path):\n        os.makedirs(config.model_path)\n    if not os.path.exists(config.sample_path):\n        os.makedirs(config.sample_path)\n    \n    if config.mode == 'train':\n        solver.train(svhn_test_loader, mnist_test_loader)\n    elif config.mode == 'sample':\n        solver.sample()\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    \n    # model hyper-parameters\n    parser.add_argument('--image_size', type=int, default=32)\n    parser.add_argument('--g_conv_dim', type=int, default=64)\n    parser.add_argument('--d_conv_dim', type=int, default=64)\n    parser.add_argument('--use_reconst_loss', required=True, type=str2bool)\n    parser.add_argument('--use_distance_loss', required=False, type=str2bool)\n    parser.add_argument('--num_classes', type=int, default=10)\n    \n    # training hyper-parameters\n    parser.add_argument('--train_iters', type=int, default=40000)\n    parser.add_argument('--batch_size', type=int, default=16)\n    parser.add_argument('--num_workers', type=int, default=2)\n    parser.add_argument('--lr', type=float, default=0.0002)\n    parser.add_argument('--beta1', type=float, default=0.5)\n    parser.add_argument('--beta2', type=float, default=0.999)\n    parser.add_argument('--lambda_distance_A', type=float, default=0.05)\n    parser.add_argument('--lambda_distance_B', type=float, default=0.1)\n    parser.add_argument('--use_self_distance', required=False, type=str2bool)\n    parser.add_argument('--max_items', type=int, default=400)\n    parser.add_argument('--unnormalized_distances', required=False, type=str2bool)\n    \n    # misc\n    parser.add_argument('--mode', type=str, default='train')\n    parser.add_argument('--model_path', type=str, default='./models')\n    parser.add_argument('--sample_path', type=str, default='./samples')\n    parser.add_argument('--mnist_path', type=str, default='./mnist')\n    parser.add_argument('--log_path', type=str, default='./logs')\n    parser.add_argument('--svhn_path', type=str, default='./svhn')\n    parser.add_argument('--log_step', type=int , default=10)\n    parser.add_argument('--sample_step', type=int , default=500)\n\n    config = parser.parse_args()\n    print(config)\n    main(config)"""
cyclegan_arch/mnist_to_svhn/model.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n    """"""Custom deconvolutional layer for simplicity.""""""\n    layers = []\n    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=False))\n    if bn:\n        layers.append(nn.BatchNorm2d(c_out))\n    return nn.Sequential(*layers)\n\ndef conv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n    """"""Custom convolutional layer for simplicity.""""""\n    layers = []\n    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=False))\n    if bn:\n        layers.append(nn.BatchNorm2d(c_out))\n    return nn.Sequential(*layers)\n\nclass G12(nn.Module):\n    """"""Generator for transfering from mnist to svhn""""""\n    def __init__(self, conf, conv_dim=64, svhn_input=None):\n        super(G12, self).__init__()\n\n        self.config = conf\n\n        # encoding blocks\n        self.conv1 = conv(1, conv_dim, 4)\n        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n        \n        # residual blocks\n        self.conv3 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n        self.conv4 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n        \n        # decoding blocks\n        self.deconv1 = deconv(conv_dim*2, conv_dim, 4)\n        self.deconv2 = deconv(conv_dim, 3, 4, bn=False)\n        \n    def forward(self, x):\n        out_1 = F.leaky_relu(self.conv1(x), 0.05)      # (?, 64, 16, 16)\n        out_2 = F.leaky_relu(self.conv2(out_1), 0.05)    # (?, 128, 8, 8)\n        \n        out_3 = F.leaky_relu(self.conv3(out_2), 0.05)    # ( "" )\n        out_4 = F.leaky_relu(self.conv4(out_3), 0.05)    # ( "" )\n        \n        out_5 = F.leaky_relu(self.deconv1(out_4), 0.05)  # (?, 64, 16, 16)\n        out = F.tanh(self.deconv2(out_5))              # (?, 3, 32, 32)\n\n        return out\n    \nclass G21(nn.Module):\n    """"""Generator for transfering from svhn to mnist""""""\n    def __init__(self, conf,  conv_dim=64, svhn_input=None):\n        super(G21, self).__init__()\n\n        self.config = conf\n\n        # encoding blocks\n        self.conv1 = conv(3, conv_dim, 4)\n        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n        \n        # residual blocks\n        self.conv3 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n        self.conv4 = conv(conv_dim*2, conv_dim*2, 3, 1, 1)\n        \n        # decoding blocks\n        self.deconv1 = deconv(conv_dim*2, conv_dim, 4)\n        self.deconv2 = deconv(conv_dim, 1, 4, bn=False)\n        \n    def forward(self, x):\n        out_1 = F.leaky_relu(self.conv1(x), 0.05)      # (?, 64, 16, 16)\n        out_2 = F.leaky_relu(self.conv2(out_1), 0.05)    # (?, 128, 8, 8)\n        \n        out_3 = F.leaky_relu(self.conv3(out_2), 0.05)    # ( "" )\n        out_4 = F.leaky_relu(self.conv4(out_3), 0.05)    # ( "" )\n        \n        out_5 = F.leaky_relu(self.deconv1(out_4), 0.05)  # (?, 64, 16, 16)\n        out = F.tanh(self.deconv2(out_5))              # (?, 1, 32, 32)\n\n        return out\n    \nclass D1(nn.Module):\n    """"""Discriminator for mnist.""""""\n    def __init__(self, conv_dim=64):\n        super(D1, self).__init__()\n        self.conv1 = conv(1, conv_dim, 4, bn=False)\n        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n        self.fc = conv(conv_dim*4, 1, 4, 1, 0, False)\n        \n    def forward(self, x):\n        out = F.leaky_relu(self.conv1(x), 0.05)    # (?, 64, 16, 16)\n        out = F.leaky_relu(self.conv2(out), 0.05)  # (?, 128, 8, 8)\n        out = F.leaky_relu(self.conv3(out), 0.05)  # (?, 256, 4, 4)\n        out = self.fc(out).squeeze()\n        return out\n\nclass D2(nn.Module):\n    """"""Discriminator for svhn.""""""\n    def __init__(self, conv_dim=64):\n        super(D2, self).__init__()\n        self.conv1 = conv(3, conv_dim, 4, bn=False)\n        self.conv2 = conv(conv_dim, conv_dim*2, 4)\n        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n        self.fc = conv(conv_dim*4, 1, 4, 1, 0, False)\n        \n    def forward(self, x):\n        out = F.leaky_relu(self.conv1(x), 0.05)    # (?, 64, 16, 16)\n        out = F.leaky_relu(self.conv2(out), 0.05)  # (?, 128, 8, 8)\n        out = F.leaky_relu(self.conv3(out), 0.05)  # (?, 256, 4, 4)\n        out = self.fc(out).squeeze()\n        return out'"
cyclegan_arch/mnist_to_svhn/solver.py,26,"b'import torch\nimport torch.nn as nn\nimport torchvision\nimport os\nimport pickle\nimport scipy.io\nimport numpy as np\nimport torch.nn.functional as F\n\nfrom torch.autograd import Variable\nfrom torch import optim\nfrom model import G12, G21\nfrom model import D1, D2\n\nclass Solver(object):\n    def __init__(self, config, svhn_loader, mnist_loader):\n        self.svhn_loader = svhn_loader\n        self.mnist_loader = mnist_loader\n        self.g12 = None\n        self.g21 = None\n        self.d1 = None\n        self.d2 = None\n        self.g_optimizer = None\n        self.d_optimizer = None\n        self.use_reconst_loss = config.use_reconst_loss\n        self.use_distance_loss = config.use_distance_loss\n        self.use_self_distance = config.use_self_distance\n        self.num_classes = config.num_classes\n        self.beta1 = config.beta1\n        self.beta2 = config.beta2\n        self.g_conv_dim = config.g_conv_dim\n        self.d_conv_dim = config.d_conv_dim\n        self.train_iters = config.train_iters\n        self.batch_size = config.batch_size\n        self.lr = config.lr\n        self.log_step = config.log_step\n        self.sample_step = config.sample_step\n        self.sample_path = config.sample_path\n        self.model_path = config.model_path\n        self.lambda_distance_A = config.lambda_distance_A\n        self.lambda_distance_B = config.lambda_distance_B\n        self.config = config\n        self.build_model()\n        \n    def build_model(self):\n        """"""Builds a generator and a discriminator.""""""\n        self.g12 = G12(self.config, conv_dim=self.g_conv_dim)\n        self.g21 = G21(self.config, conv_dim=self.g_conv_dim)\n        self.d1 = D1(conv_dim=self.d_conv_dim)\n        self.d2 = D2(conv_dim=self.d_conv_dim)\n        \n        g_params = list(self.g12.parameters()) + list(self.g21.parameters())\n        d_params = list(self.d1.parameters()) + list(self.d2.parameters())\n        \n        self.g_optimizer = optim.Adam(g_params, self.lr, [self.beta1, self.beta2])\n        self.d_optimizer = optim.Adam(d_params, self.lr, [self.beta1, self.beta2])\n        \n        if torch.cuda.is_available():\n            self.g12.cuda()\n            self.g21.cuda()\n            self.d1.cuda()\n            self.d2.cuda()\n    \n    def merge_images(self, sources, targets, k=10):\n        _, _, h, w = sources.shape\n        row = int(np.sqrt(self.batch_size))\n        merged = np.zeros([3, row*h, row*w*2])\n        for idx, (s, t) in enumerate(zip(sources, targets)):\n            i = idx // row\n            j = idx % row\n            merged[:, i*h:(i+1)*h, (j*2)*h:(j*2+1)*h] = s\n            merged[:, i*h:(i+1)*h, (j*2+1)*h:(j*2+2)*h] = t\n        return merged.transpose(1, 2, 0)\n    \n    def to_var(self, x):\n        """"""Converts numpy to variable.""""""\n        if torch.cuda.is_available():\n            x = x.cuda()\n        return Variable(x)\n    \n    def to_data(self, x):\n        """"""Converts variable to numpy.""""""\n        if torch.cuda.is_available():\n            x = x.cpu()\n        return x.data.numpy()\n    \n    def reset_grad(self):\n        """"""Zeros the gradient buffers.""""""\n        self.g_optimizer.zero_grad()\n        self.d_optimizer.zero_grad()\n\n    def distance(self, A, B):\n        return torch.mean(torch.abs(A - B))\n\n    def get_individual_distance_loss(self, A_i, A_j, AB_i, AB_j, A_to_AB):\n\n        distance_in_A = self.distance(A_i, A_j)\n        distance_in_AB = self.distance(AB_i, AB_j)\n\n        if self.normalize_distances:\n            if A_to_AB:\n                distance_in_A = (distance_in_A - self.expectation_A) / self.std_A\n                distance_in_AB = (distance_in_AB - self.expectation_B) / self.std_B\n            else:\n                distance_in_A = (distance_in_A - self.expectation_B) / self.std_B\n                distance_in_AB = (distance_in_AB - self.expectation_A) / self.std_A\n\n        return torch.abs(distance_in_A - distance_in_AB)\n\n    def get_self_distances(self, A, AB, A_to_AB=True):\n\n        A_half_1, A_half_2 = torch.chunk(A, 2, dim=2)\n        AB_half_1, AB_half_2 = torch.chunk(AB, 2, dim=2)\n\n        l_distance_A = \\\n            self.get_individual_distance_loss(A_half_1, A_half_2,\n                                              AB_half_1, AB_half_2, A_to_AB)\n\n        return l_distance_A\n\n    def get_distance_losses(self, A, AB, A_to_AB=True ):\n\n        As = torch.split(A, 1)\n        ABs = torch.split(AB, 1)\n\n        loss_distance_A = 0.0\n        num_pairs = 0\n        min_length = len(As)\n\n        for i in xrange(min_length - 1):\n            for j in xrange(i + 1, min_length):\n                num_pairs += 1\n                loss_distance_A_ij = \\\n                    self.get_individual_distance_loss(As[i], As[j],\n                                                      ABs[i], ABs[j], A_to_AB)\n                loss_distance_A += loss_distance_A_ij\n\n        loss_distance_A = loss_distance_A / num_pairs\n        return loss_distance_A\n\n    def get_std(self, num_items, vars, expectation):\n\n        num_pairs = 0\n        std_sum = 0.0\n\n        # If self distance computed std for top and bottom half\n        if self.use_self_distance:\n            for i in xrange(num_items):\n                var_half_1, var_half_2 = torch.chunk(vars[i], 2, dim=2)\n                std_sum += np.square(self.as_np(self.distance(var_half_1, var_half_2)) - expectation)\n            return np.sqrt(std_sum / num_items)\n\n        # Otherwise compute std for all pairs of images\n        for i in xrange(num_items - 1):\n            for j in xrange(i + 1, num_items):\n                num_pairs += 1\n                std_sum += np.square(self.as_np(self.distance(vars[i], vars[j])) - expectation)\n\n        return np.sqrt(std_sum / num_pairs)\n\n    def get_expectation(self, num_items, vars):\n\n        num_pairs = 0\n        distance_sum = 0.0\n\n        # If self distance computed expectation for top and bottom half\n        if self.use_self_distance:\n            for i in xrange(num_items):\n                # Split image to top and bottom half\n                var_half_1, var_half_2 = torch.chunk(vars[i], 2, dim=2)\n                distance_sum += self.as_np(self.distance(var_half_1, var_half_2))\n            return distance_sum / num_items\n\n        # Otherwise compute expectation for all pairs of images\n        for i in xrange(num_items - 1):\n            for j in xrange(i + 1, num_items):\n                num_pairs += 1\n                distance_sum += self.as_np(self.distance(vars[i], vars[j]))\n\n        return distance_sum / num_pairs\n\n    def set_expectation_and_std(self):\n\n        max_items = self.config.max_items\n\n        A_vars = []\n        B_vars = []\n        num_vars_A = 0\n        num_vars_B = 0\n\n        mnist_iter = iter(self.mnist_loader)\n        for step in range(len(mnist_iter)):\n\n            if step >= max_items:\n                break\n\n            mnist, m_labels = mnist_iter.next()\n            A = Variable(mnist, volatile=True)\n\n            if A.size()[0] != self.config.batch_size:\n                continue\n\n            A_vars.append(A)\n            num_vars_A += 1\n\n        svhn_iter = iter(self.svhn_loader)\n        for step in range(len(svhn_iter)):\n\n            if step >= max_items:\n                break\n\n            svhn, s_labels = svhn_iter.next()\n            B = Variable(svhn, volatile=True)\n\n            if B.size()[0] != self.config.batch_size:\n                continue\n\n            B_vars.append(B)\n            num_vars_B +=1\n\n        self.expectation_A = self.get_expectation(num_vars_A, A_vars)[0].astype(float)\n        self.expectation_B = self.get_expectation(num_vars_B, B_vars)[0].astype(float)\n        self.std_A = self.get_std(num_vars_A, A_vars, self.expectation_A)[0].astype(float)\n        self.std_B = self.get_std(num_vars_B, B_vars, self.expectation_B)[0].astype(float)\n\n        print(\'Expectation for dataset A: %f\' % self.expectation_A)\n        print(\'Expectation for dataset B: %f\' % self.expectation_B)\n        print(\'Std for dataset A: %f\' % self.std_A)\n        print(\'Std for dataset B: %f\' % self.std_B)\n\n\n    def as_np(self, data):\n        return data.cpu().data.numpy()\n\n    def train(self, svhn_test_loader, mnist_test_loader):\n        svhn_iter = iter(self.svhn_loader)\n        mnist_iter = iter(self.mnist_loader)\n        iter_per_epoch = min(len(svhn_iter), len(mnist_iter)) -1\n        \n        # fixed mnist and svhn for sampling\n        svhn_test_iter = iter(svhn_test_loader)\n        mnist_test_iter = iter(mnist_test_loader)\n        fixed_svhn = self.to_var(svhn_test_iter.next()[0])\n        fixed_mnist = self.to_var(mnist_test_iter.next()[0])\n\n        self.normalize_distances = not self.config.unnormalized_distances\n\n        if (self.use_self_distance or self.use_distance_loss) and self.normalize_distances:\n            self.set_expectation_and_std()\n        \n        for step in range(self.train_iters+1):\n            # reset data_iter for each epoch\n            if (step+1) % iter_per_epoch == 0:\n                mnist_iter = iter(self.mnist_loader)\n                svhn_iter = iter(self.svhn_loader)\n            \n            # load svhn and mnist dataset\n            svhn, s_labels = svhn_iter.next() \n            svhn, s_labels = self.to_var(svhn), self.to_var(s_labels).long().squeeze()\n            mnist, m_labels = mnist_iter.next() \n            mnist, m_labels = self.to_var(mnist), self.to_var(m_labels)\n            \n            #============ train D ============#\n            \n            # train with real images\n            self.reset_grad()\n            out = self.d1(mnist)\n            d1_loss = torch.mean((out-1)**2)\n            \n            out = self.d2(svhn)\n            d2_loss = torch.mean((out-1)**2)\n            \n            d_mnist_loss = d1_loss\n            d_svhn_loss = d2_loss\n            d_real_loss = d1_loss + d2_loss\n            d_real_loss.backward()\n            self.d_optimizer.step()\n            \n            # train with fake images\n            self.reset_grad()\n            fake_svhn = self.g12(mnist)\n            out = self.d2(fake_svhn)\n            d2_loss = torch.mean(out**2)\n            \n            fake_mnist = self.g21(svhn)\n            out = self.d1(fake_mnist)\n            d1_loss = torch.mean(out**2)\n            \n            d_fake_loss = d1_loss + d2_loss\n            d_fake_loss.backward()\n            self.d_optimizer.step()\n            \n            #============ train G ============#\n            \n            # train mnist-svhn-mnist cycle\n            self.reset_grad()\n            fake_svhn = self.g12(mnist)\n            out_svhn = self.d2(fake_svhn)\n            reconst_mnist = self.g21(fake_svhn)\n\n            gen_loss_A = torch.mean((out_svhn-1)**2)\n            g_loss = gen_loss_A\n\n            if self.use_reconst_loss:\n                reconst_loss_A = torch.mean((mnist - reconst_mnist) ** 2)\n                g_loss += reconst_loss_A\n\n            if self.use_distance_loss:\n                dist_A = self.get_distance_losses(mnist, fake_svhn, A_to_AB=True) * self.lambda_distance_A\n                g_loss += dist_A\n            elif self.use_self_distance:\n                dist_A = self.get_self_distances(mnist, fake_svhn, A_to_AB=True) * self.lambda_distance_A\n                g_loss += dist_A\n\n            g_loss.backward()\n            self.g_optimizer.step()\n\n            # train svhn-mnist-svhn cycle\n            self.reset_grad()\n            fake_mnist  = self.g21(svhn)\n            out_mnist = self.d1(fake_mnist)\n            reconst_svhn = self.g12(fake_mnist)\n\n            gen_loss_B = torch.mean((out_mnist - 1) ** 2)\n            g_loss = gen_loss_B\n\n            if self.use_reconst_loss:\n                reconst_loss_B = torch.mean((svhn - reconst_svhn) ** 2)\n                g_loss += reconst_loss_B\n\n            if self.use_distance_loss:\n                dist_B = self.get_distance_losses(svhn, fake_mnist, A_to_AB=False) * self.lambda_distance_B\n                g_loss += dist_B\n            elif self.use_self_distance:\n                dist_B = self.get_self_distances(svhn, fake_mnist, A_to_AB=False) * self.lambda_distance_B\n                g_loss += dist_B\n\n            g_loss.backward()\n            self.g_optimizer.step()\n\n            # print the log info\n            if (step+1) % self.log_step == 0:\n\n                print(\'Step [%d/%d], d_real_loss: %.4f, d_mnist_loss: %.4f, d_svhn_loss: %.4f, \'\n                      \'d_fake_loss: %.4f, gen_loss_A: %.4f, gen_loss_B: %.4f,\'\n                      %(step+1, self.train_iters, d_real_loss.data[0], d_mnist_loss.data[0],\n                        d_svhn_loss.data[0], d_fake_loss.data[0], gen_loss_A.data[0],gen_loss_B.data[0]))\n\n                if self.use_reconst_loss:\n                    print (\'reconst_loss_A: %.4f, recons_loss_B: %.4f, \' %\n                           (reconst_loss_A.data[0], reconst_loss_B.data[0]))\n                if self.use_distance_loss or self.use_self_distance:\n                    print  (\'dist_loss_A: %.4f, dist_loss_B: %.4f, \' %\n                            (dist_A.data[0], dist_B.data[0]))\n\n\n            # save the sampled images\n            if (step+1) % self.sample_step == 0:\n                fake_svhn = self.g12(fixed_mnist)\n                fake_mnist = self.g21(fixed_svhn)\n                \n                mnist, fake_mnist = self.to_data(fixed_mnist), self.to_data(fake_mnist)\n                svhn , fake_svhn = self.to_data(fixed_svhn), self.to_data(fake_svhn)\n                \n                merged = self.merge_images(mnist, fake_svhn)\n                path = os.path.join(self.sample_path, \'sample-%d-m-s.png\' %(step+1))\n                scipy.misc.imsave(path, merged)\n                print (\'saved %s\' %path)\n                \n                merged = self.merge_images(svhn, fake_mnist)\n                path = os.path.join(self.sample_path, \'sample-%d-s-m.png\' %(step+1))\n                scipy.misc.imsave(path, merged)\n                print (\'saved %s\' %path)\n            \n            if (step+1) % 5000 == 0:\n                # save the model parameters for each epoch\n                g12_path = os.path.join(self.model_path, \'g12-%d.pkl\' %(step+1))\n                g21_path = os.path.join(self.model_path, \'g21-%d.pkl\' %(step+1))\n                d1_path = os.path.join(self.model_path, \'d1-%d.pkl\' %(step+1))\n                d2_path = os.path.join(self.model_path, \'d2-%d.pkl\' %(step+1))\n                torch.save(self.g12.state_dict(), g12_path)\n                torch.save(self.g21.state_dict(), g21_path)\n                torch.save(self.d1.state_dict(), d1_path)\n                torch.save(self.d2.state_dict(), d2_path)\n'"
cyclegan_arch/util/__init__.py,0,b''
cyclegan_arch/util/html.py,0,"b'import dominate\nfrom dominate.tags import *\nimport os\n\n\nclass HTML:\n    def __init__(self, web_dir, title, reflesh=0):\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n        # print(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if reflesh > 0:\n            with self.doc.head:\n                meta(http_equiv=""reflesh"", content=str(reflesh))\n\n    def get_image_dir(self):\n        return self.img_dir\n\n    def add_header(self, str):\n        with self.doc:\n            h3(str)\n\n    def add_table(self, border=1):\n        self.t = table(border=border, style=""table-layout: fixed;"")\n        self.doc.add(self.t)\n\n    def add_images(self, ims, txts, links, width=400):\n        self.add_table()\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % width, src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims = []\n    txts = []\n    links = []\n    for n in range(4):\n        ims.append(\'image_%d.png\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.png\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
cyclegan_arch/util/image_pool.py,3,"b'import random\nimport numpy as np\nimport torch\nfrom pdb import set_trace as st\nfrom torch.autograd import Variable\nclass ImagePool():\n    def __init__(self, pool_size):\n        self.pool_size = pool_size\n        if self.pool_size > 0:\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        if self.pool_size == 0:\n            return images\n        return_images = []\n        for image in images.data:\n            image = torch.unsqueeze(image, 0)\n            if self.num_imgs < self.pool_size:\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:\n                    random_id = random.randint(0, self.pool_size-1)\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:\n                    return_images.append(image)\n        return_images = Variable(torch.cat(return_images, 0))\n        return return_images\n'"
cyclegan_arch/util/png.py,0,"b'import struct\nimport zlib\n\ndef encode(buf, width, height):\n  """""" buf: must be bytes or a bytearray in py3, a regular string in py2. formatted RGBRGB... """"""\n  assert (width * height * 3 == len(buf))\n  bpp = 3\n\n  def raw_data():\n    # reverse the vertical line order and add null bytes at the start\n    row_bytes = width * bpp\n    for row_start in range((height - 1) * width * bpp, -1, -row_bytes):\n      yield b\'\\x00\'\n      yield buf[row_start:row_start + row_bytes]\n\n  def chunk(tag, data):\n    return [\n        struct.pack(""!I"", len(data)),\n        tag,\n        data,\n        struct.pack(""!I"", 0xFFFFFFFF & zlib.crc32(data, zlib.crc32(tag)))\n      ]\n\n  SIGNATURE = b\'\\x89PNG\\r\\n\\x1a\\n\'\n  COLOR_TYPE_RGB = 2\n  COLOR_TYPE_RGBA = 6\n  bit_depth = 8\n  return b\'\'.join(\n      [ SIGNATURE ] +\n      chunk(b\'IHDR\', struct.pack(""!2I5B"", width, height, bit_depth, COLOR_TYPE_RGB, 0, 0, 0)) +\n      chunk(b\'IDAT\', zlib.compress(b\'\'.join(raw_data()), 9)) +\n      chunk(b\'IEND\', b\'\')\n    )\n'"
cyclegan_arch/util/util.py,1,"b'from __future__ import print_function\nimport torch\nimport numpy as np\nfrom PIL import Image\nimport inspect, re\nimport numpy as np\nimport os\nimport collections\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(image_tensor, imtype=np.uint8):\n    image_numpy = image_tensor[0].cpu().float().numpy()\n    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    return image_numpy.astype(imtype)\n\n\ndef diagnose_network(net, name=\'network\'):\n    mean = 0.0\n    count = 0\n    for param in net.parameters():\n        if param.grad is not None:\n            mean += torch.mean(torch.abs(param.grad.data))\n            count += 1\n    if count > 0:\n        mean = mean / count\n    print(name)\n    print(mean)\n\n\ndef save_image(image_numpy, image_path):\n    image_pil = Image.fromarray(image_numpy)\n    image_pil.save(image_path)\n\ndef info(object, spacing=10, collapse=1):\n    """"""Print methods and doc strings.\n    Takes module, class, list, dictionary, or string.""""""\n    methodList = [e for e in dir(object) if isinstance(getattr(object, e), collections.Callable)]\n    processFunc = collapse and (lambda s: "" "".join(s.split())) or (lambda s: s)\n    print( ""\\n"".join([""%s %s"" %\n                     (method.ljust(spacing),\n                      processFunc(str(getattr(object, method).__doc__)))\n                     for method in methodList]) )\n\ndef varname(p):\n    for line in inspect.getframeinfo(inspect.currentframe().f_back)[3]:\n        m = re.search(r\'\\bvarname\\s*\\(\\s*([A-Za-z_][A-Za-z0-9_]*)\\s*\\)\', line)\n        if m:\n            return m.group(1)\n\ndef print_numpy(x, val=True, shp=False):\n    x = x.astype(np.float64)\n    if shp:\n        print(\'shape,\', x.shape)\n    if val:\n        x = x.flatten()\n        print(\'mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f\' % (\n            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))\n\n\ndef mkdirs(paths):\n    if isinstance(paths, list) and not isinstance(paths, str):\n        for path in paths:\n            mkdir(path)\n    else:\n        mkdir(paths)\n\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n'"
cyclegan_arch/util/visualizer.py,0,"b""import numpy as np\nimport os\nimport ntpath\nimport time\nfrom . import util\nfrom . import html\n\nclass Visualizer():\n    def __init__(self, opt):\n        # self.opt = opt\n        self.display_id = opt.display_id\n        self.use_html = opt.isTrain and not opt.no_html\n        self.win_size = opt.display_winsize\n        self.name = opt.name\n        if self.display_id > 0:\n            import visdom\n            self.vis = visdom.Visdom()\n\n        if self.use_html:\n            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, 'web')\n            self.img_dir = os.path.join(self.web_dir, 'images')\n            print('create web directory %s...' % self.web_dir)\n            util.mkdirs([self.web_dir, self.img_dir])\n\n\n    # |visuals|: dictionary of images to display or save\n    def display_current_results(self, visuals, epoch):\n        if self.display_id > 0: # show images in the browser\n            idx = 1\n            for label, image_numpy in visuals.items():\n                #image_numpy = np.flipud(image_numpy)\n                self.vis.image(image_numpy.transpose([2,0,1]), opts=dict(title=label),\n                                   win=self.display_id + idx)\n                idx += 1\n\n        if self.use_html: # save images to a html file\n            for label, image_numpy in visuals.items():\n                img_path = os.path.join(self.img_dir, 'epoch%.3d_%s.png' % (epoch, label))\n                util.save_image(image_numpy, img_path)\n            # update website\n            webpage = html.HTML(self.web_dir, 'Experiment name = %s' % self.name, reflesh=1)\n            for n in range(epoch, 0, -1):\n                webpage.add_header('epoch [%d]' % n)\n                ims = []\n                txts = []\n                links = []\n\n                for label, image_numpy in visuals.items():\n                    img_path = 'epoch%.3d_%s.png' % (n, label)\n                    ims.append(img_path)\n                    txts.append(label)\n                    links.append(img_path)\n                webpage.add_images(ims, txts, links, width=self.win_size)\n            webpage.save()\n\n    # errors: dictionary of error labels and values\n    def plot_current_errors(self, epoch, counter_ratio, opt, errors):\n        if not hasattr(self, 'plot_data'):\n            self.plot_data = {'X':[],'Y':[], 'legend':list(errors.keys())}\n        self.plot_data['X'].append(epoch + counter_ratio)\n        self.plot_data['Y'].append([errors[k] for k in self.plot_data['legend']])\n        self.vis.line(\n            X=np.stack([np.array(self.plot_data['X'])]*len(self.plot_data['legend']),1),\n            Y=np.array(self.plot_data['Y']),\n            opts={\n                'title': self.name + ' loss over time',\n                'legend': self.plot_data['legend'],\n                'xlabel': 'epoch',\n                'ylabel': 'loss'},\n            win=self.display_id)\n\n    # errors: same format as |errors| of plotCurrentErrors\n    def print_current_errors(self, epoch, i, errors, t):\n        message = '(epoch: %d, iters: %d, time: %.3f) ' % (epoch, i, t)\n        for k, v in errors.items():\n            message += '%s: %.3f ' % (k, v)\n\n        print(message)\n\n    # save image to the disk\n    def save_images(self, webpage, visuals, image_path):\n        image_dir = webpage.get_image_dir()\n        short_path = ntpath.basename(image_path[0])\n        name = os.path.splitext(short_path)[0]\n\n        webpage.add_header(name)\n        ims = []\n        txts = []\n        links = []\n\n        for label, image_numpy in visuals.items():\n            image_name = '%s_%s.png' % (name, label)\n            save_path = os.path.join(image_dir, image_name)\n            util.save_image(image_numpy, save_path)\n\n            ims.append(image_name)\n            txts.append(label)\n            links.append(image_name)\n        webpage.add_images(ims, txts, links, width=self.win_size)\n"""
discogan_arch/discogan_arch_options/__init__.py,0,b''
discogan_arch/discogan_arch_options/options.py,0,"b'import argparse\n\nclass Options(object):\n    def __init__(self):\n        self.parser = argparse.ArgumentParser(description=\'PyTorch implementation of DistanceGAN based on DiscoGAN\')\n        self.initialized = False\n\n    def initialize(self):\n\n        self.parser.add_argument(\'--cuda\', type=str, default=\'true\', help=\'Set cuda usage\')\n        self.parser.add_argument(\'--task_name\', type=str, default=\'facescrub\', help=\'Set data name\')\n        self.parser.add_argument(\'--epoch_size\', type=int, default=5000, help=\'Set epoch size\')\n        self.parser.add_argument(\'--batch_size\', type=int, default=64, help=\'Set batch size\')\n        self.parser.add_argument(\'--learning_rate\', type=float, default=0.0002, help=\'Set learning rate for optimizer\')\n        self.parser.add_argument(\'--model_arch\', type=str, default=\'distancegan\',\n                            help=\'choose among gan/recongan/discogan. gan - standard GAN, recongan - GAN with reconstruction, discogan - DiscoGAN.\')\n        self.parser.add_argument(\'--image_size\', type=int, default=64,\n                            help=\'Image size. 64 for every experiment in the paper\')\n        self.parser.add_argument(\'--gan_curriculum\', type=int, default=10000,\n                            help=\'Strong GAN loss for certain period at the beginning\')\n        self.parser.add_argument(\'--starting_rate\', type=float, default=0.01,\n                            help=\'Set the lambda weight between GAN loss and Recon loss during curriculum period at the beginning. We used the 0.01 weight.\')\n        self.parser.add_argument(\'--default_rate\', type=float, default=0.5,\n                            help=\'Set the lambda weight between GAN loss and Recon loss after curriculum period. We used the 0.5 weight.\')\n        self.parser.add_argument(\'--style_A\', type=str, default=None,\n                            help=\'Style for CelebA dataset. Could be any attributes in celebA (Young, Male, Blond_Hair, Wearing_Hat ...)\')\n        self.parser.add_argument(\'--style_B\', type=str, default=None,\n                            help=\'Style for CelebA dataset. Could be any attributes in celebA (Young, Male, Blond_Hair, Wearing_Hat ...)\')\n        self.parser.add_argument(\'--constraint\', type=str, default=None,\n                            help=\'Constraint for celebA dataset. Only images satisfying this constraint is used. For example, if --constraint=Male, and --constraint_type=1, only male images are used for both style/domain.\')\n        self.parser.add_argument(\'--constraint_type\', type=str, default=None,\n                            help=\'Used along with --constraint. If --constraint_type=1, only images satisfying the constraint are used. If --constraint_type=-1, only images not satisfying the constraint are used.\')\n        self.parser.add_argument(\'--n_test\', type=int, default=200, help=\'Number of test data.\')\n        self.parser.add_argument(\'--update_interval\', type=int, default=3, help=\'\')\n        self.parser.add_argument(\'--log_interval\', type=int, default=50,\n                            help=\'Print loss values every log_interval iterations.\')\n        self.parser.add_argument(\'--image_save_interval\', type=int, default=1000,\n                            help=\'Save test results every log_interval iterations.\')\n        self.parser.add_argument(\'--model_save_interval\', type=int, default=10000,\n                            help=\'Save models every log_interval iterations.\')\n        self.parser.add_argument(\'--result_path\', type=str, default=\'./results/\')\n        self.parser.add_argument(\'--model_path\', type=str, default=\'./models/\')\n        self.parser.add_argument(\'--use_self_distance\', action=\'store_true\', help=""use distance for top and bottom half of the image"")\n        self.parser.add_argument(\'--unnormalized_distances\', action=\'store_true\', help=\'do not normalize distances by expecatation and std\')\n        self.parser.add_argument(\'--max_items\', type=int, default=900, help=\'maximum number of items to use for expectation and std calculation\')\n        self.parser.add_argument(\'--use_reconst_loss\', action=\'store_true\',\n                                 help=\'add reconstruction loss in addition to distance loss\')\n        self.parser.add_argument(\'--num_layers\', type=int, default=4, help=\'Number of convolutional layers in G (equal number of deconvolutional layers exist)\')\n\n\nclass AnglePairingOptions(Options):\n\n    def initialize(self):\n\n        self.parser.add_argument(\'--cuda\', type=str, default=\'true\', help=\'Set cuda usage\')\n        self.parser.add_argument(\'--task_name\', type=str, default=\'car2car\', help=\'Set data name\')\n        self.parser.add_argument(\'--epoch_size\', type=int, default=10000, help=\'Set epoch size\')\n        self.parser.add_argument(\'--batch_size\', type=int, default=64, help=\'Set batch size\')\n        self.parser.add_argument(\'--learning_rate\', type=float, default=0.0002, help=\'Set learning rate for optimizer\')\n        self.parser.add_argument(\'--model_arch\', type=str, default=\'distancegan\',\n                            help=\'choose among gan/recongan/discogan. gan - standard GAN, recongan - GAN with reconstruction, discogan - DiscoGAN.\')\n        self.parser.add_argument(\'--image_size\', type=int, default=64,\n                            help=\'Image size. 64 for every experiment in the paper\')\n        self.parser.add_argument(\'--gan_curriculum\', type=int, default=10000,\n                            help=\'Strong GAN loss for certain period at the beginning\')\n        self.parser.add_argument(\'--starting_rate\', type=float, default=0.9,\n                            help=\'Set the lambda weight between GAN loss and Recon loss during curriculum period at the beginning. We used the 0.01 weight.\')\n        self.parser.add_argument(\'--default_rate\', type=float, default=0.9,\n                            help=\'Set the lambda weight between GAN loss and Recon loss after curriculum period. We used the 0.5 weight.\')\n        self.parser.add_argument(\'--style_A\', type=str, default=None,\n                            help=\'Style for CelebA dataset. Could be any attributes in celebA (Young, Male, Blond_Hair, Wearing_Hat ...)\')\n        self.parser.add_argument(\'--style_B\', type=str, default=None,\n                            help=\'Style for CelebA dataset. Could be any attributes in celebA (Young, Male, Blond_Hair, Wearing_Hat ...)\')\n        self.parser.add_argument(\'--constraint\', type=str, default=None,\n                            help=\'Constraint for celebA dataset. Only images satisfying this constraint is used. For example, if --constraint=Male, and --constraint_type=1, only male images are used for both style/domain.\')\n        self.parser.add_argument(\'--constraint_type\', type=str, default=None,\n                            help=\'Used along with --constraint. If --constraint_type=1, only images satisfying the constraint are used. If --constraint_type=-1, only images not satisfying the constraint are used.\')\n        self.parser.add_argument(\'--n_test\', type=int, default=200, help=\'Number of test data.\')\n        self.parser.add_argument(\'--update_interval\', type=int, default=3, help=\'\')\n        self.parser.add_argument(\'--log_interval\', type=int, default=50,\n                            help=\'Print loss values every log_interval iterations.\')\n        self.parser.add_argument(\'--image_save_interval\', type=int, default=500,\n                            help=\'Save test results every log_interval iterations.\')\n        self.parser.add_argument(\'--model_save_interval\', type=int, default=10000,\n                            help=\'Save models every log_interval iterations.\')\n        self.parser.add_argument(\'--result_path\', type=str, default=\'./results/\')\n        self.parser.add_argument(\'--model_path\', type=str, default=\'./models/\')\n        self.parser.add_argument(\'--log_path\', type=str, default=\'./logs/\')\n        self.parser.add_argument(\'--use_self_distance\', action=\'store_true\', help=""use distance for top and bottom half of the image"")\n        self.parser.add_argument(\'--unnormalized_distances\', action=\'store_true\', help=\'do not normalize distances by expecatation and std\')\n        self.parser.add_argument(\'--max_items\', type=int, default=900, help=\'maximum number of items to use for expectation and std calculation\')\n        self.parser.add_argument(\'--use_reconst_loss\', action=\'store_true\',\n                                 help=\'add reconstruction loss in addition to distance loss\')\n        self.parser.add_argument(\'--num_layers\', type=int, default=5,\n                                 help=\'Number of convolutional layers in G (equal number of deconvolutional layers exist)\')\n\n'"
