file_path,api_count,code
train_cityscapes.py,0,"b""import mxnet as mx\nimport random\nfrom mxnet.gluon.data.vision import transforms\nfrom functools import partial\nfrom gluoncv.utils import LRScheduler\nfrom easydict import EasyDict as edict\nfrom albumentations import (\n    Compose, HorizontalFlip, ShiftScaleRotate, PadIfNeeded, RandomCrop,\n    RGBShift, RandomBrightness, RandomContrast\n)\n\nfrom adaptis.engine.trainer import AdaptISTrainer, init_proposals_head\nfrom adaptis.model.cityscapes.models import get_cityscapes_model\nfrom adaptis.model.losses import NormalizedFocalLossSigmoid, NormalizedFocalLossSoftmax, AdaptISProposalsLossIoU\nfrom adaptis.model.metrics import AdaptiveIoU\nfrom adaptis.data.cityscapes import CityscapesDataset\nfrom adaptis.utils.exp import init_experiment\nfrom adaptis.utils.log import logger\n\n\ndef add_exp_args(parser):\n    parser.add_argument('--dataset-path', type=str, help='Path to the dataset')\n    return parser\n\n\ndef init_model():\n    model_cfg = edict()\n    model_cfg.syncbn = True\n    model_cfg.crop_size = (400, 720)\n\n    model_cfg.input_normalization = {\n        'mean': [.485, .456, .406],\n        'std': [.229, .224, .225]\n    }\n\n    model_cfg.input_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(model_cfg.input_normalization['mean'],\n                             model_cfg.input_normalization['std']),\n    ])\n\n    if args.ngpus > 1 and model_cfg.syncbn:\n        norm_layer = partial(mx.gluon.contrib.nn.SyncBatchNorm, num_devices=args.ngpus)\n    else:\n        norm_layer = mx.gluon.nn.BatchNorm\n\n    model = get_cityscapes_model(num_classes=19, norm_layer=norm_layer,\n                                 backbone='resnet50')\n    model.initialize(mx.init.Xavier(rnd_type='gaussian', magnitude=2), ctx=mx.cpu(0))\n    model.feature_extractor.load_pretrained_weights()\n\n    return model, model_cfg\n\n\ndef train(model, model_cfg, args, train_proposals, start_epoch=0):\n    args.val_batch_size = args.batch_size\n    args.input_normalization = model_cfg.input_normalization\n    crop_size = model_cfg.crop_size\n\n    loss_cfg = edict()\n    loss_cfg.instance_loss = NormalizedFocalLossSigmoid(alpha=0.25, gamma=2)\n    loss_cfg.instance_loss_weight = 1.0 if not train_proposals else 0.0\n\n    if not train_proposals:\n        num_epochs = 250\n        num_points = 6\n\n        loss_cfg.segmentation_loss = NormalizedFocalLossSoftmax(ignore_label=-1, gamma=1)\n        loss_cfg.segmentation_loss_weight = 0.75\n    else:\n        num_epochs = 8\n        num_points = 48\n\n        loss_cfg.proposals_loss = AdaptISProposalsLossIoU(args.batch_size)\n        loss_cfg.proposals_loss_weight = 1.0\n\n    train_augmentator = Compose([\n        HorizontalFlip(),\n        ShiftScaleRotate(shift_limit=0.03, scale_limit=0,\n                         rotate_limit=(-3, 3), border_mode=0, p=0.75),\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size),\n        RandomBrightness(limit=(-0.25, 0.25), p=0.75),\n        RandomContrast(limit=(-0.15, 0.4), p=0.75),\n        RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.75)\n    ], p=1.0)\n\n    val_augmentator = Compose([\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size)\n    ], p=1.0)\n\n    def scale_func(image_shape):\n        return random.uniform(0.85, 1.15)\n\n    trainset = CityscapesDataset(\n        args.dataset_path,\n        split='train',\n        num_points=num_points,\n        augmentator=train_augmentator,\n        with_segmentation=True,\n        points_from_one_object=train_proposals,\n        input_transform=model_cfg.input_transform,\n        min_object_area=80,\n        sample_ignore_object_prob=0.025,\n        keep_background_prob=0.05,\n        image_rescale=scale_func,\n        use_jpeg=False\n    )\n\n    valset = CityscapesDataset(\n        args.dataset_path,\n        split='test',\n        augmentator=val_augmentator,\n        num_points=num_points,\n        with_segmentation=True,\n        points_from_one_object=train_proposals,\n        input_transform=model_cfg.input_transform,\n        min_object_area=80,\n        image_rescale=scale_func,\n        use_jpeg=False\n    )\n\n    if not train_proposals:\n        optimizer_params = {\n            'learning_rate': 0.01,\n            'momentum': 0.9, 'wd': 1e-4\n        }\n        lr_scheduler = partial(LRScheduler, mode='poly', baselr=optimizer_params['learning_rate'],\n                               nepochs=num_epochs)\n    else:\n        optimizer_params = {\n            'learning_rate': 5e-4,\n            'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8\n        }\n        lr_scheduler = partial(LRScheduler, mode='cosine',\n                               baselr=optimizer_params['learning_rate'],\n                               nepochs=num_epochs)\n\n    trainer = AdaptISTrainer(args, model, model_cfg, loss_cfg,\n                             trainset, valset,\n                             optimizer='sgd' if not train_proposals else 'adam',\n                             optimizer_params=optimizer_params,\n                             lr_scheduler=lr_scheduler,\n                             checkpoint_interval=40 if not train_proposals else 2,\n                             image_dump_interval=100 if not train_proposals else -1,\n                             train_proposals=train_proposals,\n                             hybridize_model=not train_proposals,\n                             metrics=[AdaptiveIoU()])\n\n    logger.info(f'Starting Epoch: {start_epoch}')\n    logger.info(f'Total Epochs: {num_epochs}')\n    for epoch in range(start_epoch, num_epochs):\n        trainer.training(epoch)\n        trainer.validation(epoch)\n\n\nif __name__ == '__main__':\n    args = init_experiment('cityscapes', add_exp_args, script_path=__file__)\n\n    model, model_cfg = init_model()\n    train(model, model_cfg, args, train_proposals=False,\n          start_epoch=args.start_epoch)\n    init_proposals_head(model, args.ctx)\n    train(model, model_cfg, args, train_proposals=True)\n"""
train_toy.py,0,"b""import mxnet as mx\nfrom mxnet.gluon.data.vision import transforms\nfrom functools import partial\nfrom gluoncv.utils import LRScheduler\nfrom easydict import EasyDict as edict\nfrom albumentations import Compose, Blur, Flip, IAAAdditiveGaussianNoise\n\nfrom adaptis.engine.trainer import AdaptISTrainer, init_proposals_head\nfrom adaptis.model.toy.models import get_unet_model\nfrom adaptis.model.losses import NormalizedFocalLossSigmoid, NormalizedFocalLossSoftmax, AdaptISProposalsLossIoU\nfrom adaptis.model.metrics import AdaptiveIoU\nfrom adaptis.data.toy import ToyDataset\nfrom adaptis.utils.exp import init_experiment\nfrom adaptis.utils.log import logger\n\n\ndef add_exp_args(parser):\n    parser.add_argument('--dataset-path', type=str, help='Path to the dataset')\n    return parser\n\n\ndef init_model():\n    model_cfg = edict()\n    model_cfg.syncbn = True\n\n    model_cfg.input_normalization = {\n        'mean': [0.5, 0.5, 0.5],\n        'std': [0.5, 0.5, 0.5]\n    }\n\n    model_cfg.input_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(model_cfg.input_normalization['mean'],\n                             model_cfg.input_normalization['std']),\n    ])\n\n    if args.ngpus > 1 and model_cfg.syncbn:\n        norm_layer = partial(mx.gluon.contrib.nn.SyncBatchNorm, num_devices=args.ngpus)\n    else:\n        norm_layer = mx.gluon.nn.BatchNorm\n\n    model = get_unet_model(norm_layer)\n    model.initialize(mx.init.Xavier(rnd_type='gaussian', magnitude=1), ctx=mx.cpu(0))\n\n    return model, model_cfg\n\n\ndef train(model, model_cfg, args, train_proposals, start_epoch=0):\n    loss_cfg = edict()\n    loss_cfg.instance_loss = NormalizedFocalLossSigmoid(alpha=0.50, gamma=2)\n    loss_cfg.instance_loss_weight = 1.0 if not train_proposals else 0.0\n\n    if not train_proposals:\n        num_epochs = 160\n        num_points = 12\n\n        loss_cfg.segmentation_loss = NormalizedFocalLossSoftmax(ignore_label=-1, gamma=1)\n        loss_cfg.segmentation_loss_weight = 0.75\n    else:\n        num_epochs = 10\n        num_points = 32\n\n        loss_cfg.proposals_loss = AdaptISProposalsLossIoU(args.batch_size)\n        loss_cfg.proposals_loss_weight = 1.0\n\n    args.val_batch_size = args.batch_size\n    args.input_normalization = model_cfg.input_normalization\n\n    train_augmentator = Compose([\n        Blur(blur_limit=(2, 4)),\n        IAAAdditiveGaussianNoise(scale=(10, 40), p=0.5),\n        Flip()\n    ], p=1.0)\n\n    trainset = ToyDataset(\n        args.dataset_path,\n        split='train',\n        num_points=num_points,\n        augmentator=train_augmentator,\n        with_segmentation=True,\n        points_from_one_object=train_proposals,\n        input_transform=model_cfg.input_transform\n    )\n\n    valset = ToyDataset(\n        args.dataset_path,\n        split='test',\n        augmentator=train_augmentator,\n        num_points=num_points,\n        with_segmentation=True,\n        points_from_one_object=train_proposals,\n        input_transform=model_cfg.input_transform\n    )\n\n    optimizer_params = {\n        'learning_rate': 5e-4,\n        'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8\n    }\n\n    if not train_proposals:\n        lr_scheduler = partial(LRScheduler, mode='cosine',\n                               baselr=optimizer_params['learning_rate'],\n                               nepochs=num_epochs)\n    else:\n        lr_scheduler = partial(LRScheduler, mode='cosine',\n                               baselr=optimizer_params['learning_rate'],\n                               nepochs=num_epochs)\n\n    trainer = AdaptISTrainer(args, model, model_cfg, loss_cfg,\n                             trainset, valset,\n                             optimizer='adam',\n                             optimizer_params=optimizer_params,\n                             lr_scheduler=lr_scheduler,\n                             checkpoint_interval=40 if not train_proposals else 5,\n                             image_dump_interval=600 if not train_proposals else -1,\n                             train_proposals=train_proposals,\n                             hybridize_model=not train_proposals,\n                             metrics=[AdaptiveIoU()])\n\n    logger.info(f'Starting Epoch: {start_epoch}')\n    logger.info(f'Total Epochs: {num_epochs}')\n    for epoch in range(start_epoch, num_epochs):\n        trainer.training(epoch)\n        trainer.validation(epoch)\n\n\nif __name__ == '__main__':\n    args = init_experiment('toy', add_exp_args, script_path=__file__)\n\n    model, model_cfg = init_model()\n    train(model, model_cfg, args, train_proposals=False,\n          start_epoch=args.start_epoch)\n    init_proposals_head(model, args.ctx)\n    train(model, model_cfg, args, train_proposals=True)\n"""
train_toy_v2.py,0,"b""import mxnet as mx\nfrom mxnet.gluon.data.vision import transforms\nfrom functools import partial\nfrom gluoncv.utils import LRScheduler\nfrom easydict import EasyDict as edict\nfrom albumentations import Compose, Flip\n\nfrom adaptis.engine.trainer import AdaptISTrainer, init_proposals_head\nfrom adaptis.model.toy.models import get_unet_model\nfrom adaptis.model.losses import NormalizedFocalLossSigmoid, NormalizedFocalLossSoftmax, AdaptISProposalsLossIoU\nfrom adaptis.model.metrics import AdaptiveIoU\nfrom adaptis.data.toy import ToyDataset\nfrom adaptis.utils.exp import init_experiment\nfrom adaptis.utils.log import logger\n\n\ndef add_exp_args(parser):\n    parser.add_argument('--dataset-path', type=str, help='Path to the dataset')\n    return parser\n\n\ndef init_model():\n    model_cfg = edict()\n    model_cfg.syncbn = True\n\n    model_cfg.input_normalization = {\n        'mean': [0.5, 0.5, 0.5],\n        'std': [0.5, 0.5, 0.5]\n    }\n\n    model_cfg.input_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(model_cfg.input_normalization['mean'],\n                             model_cfg.input_normalization['std']),\n    ])\n\n    if args.ngpus > 1 and model_cfg.syncbn:\n        norm_layer = partial(mx.gluon.contrib.nn.SyncBatchNorm, num_devices=args.ngpus)\n    else:\n        norm_layer = mx.gluon.nn.BatchNorm\n\n    model = get_unet_model(norm_layer)\n    model.initialize(mx.init.Xavier(rnd_type='gaussian', magnitude=1), ctx=mx.cpu(0))\n\n    return model, model_cfg\n\n\ndef train(model, model_cfg, args, train_proposals, start_epoch=0):\n    loss_cfg = edict()\n    loss_cfg.instance_loss = NormalizedFocalLossSigmoid(alpha=0.50, gamma=2)\n    loss_cfg.instance_loss_weight = 1.0 if not train_proposals else 0.0\n\n    if not train_proposals:\n        num_epochs = 160\n        num_points = 12\n\n        loss_cfg.segmentation_loss = NormalizedFocalLossSoftmax(ignore_label=-1, gamma=1)\n        loss_cfg.segmentation_loss_weight = 0.75\n    else:\n        num_epochs = 10\n        num_points = 32\n\n        loss_cfg.proposals_loss = AdaptISProposalsLossIoU(args.batch_size)\n        loss_cfg.proposals_loss_weight = 1.0\n\n    args.val_batch_size = args.batch_size\n    args.input_normalization = model_cfg.input_normalization\n\n    train_augmentator = Compose([\n        Flip()\n    ], p=1.0)\n\n    trainset = ToyDataset(\n        args.dataset_path,\n        split='train',\n        num_points=num_points,\n        augmentator=train_augmentator,\n        with_segmentation=True,\n        points_from_one_object=train_proposals,\n        input_transform=model_cfg.input_transform,\n        epoch_len=10000\n    )\n\n    valset = ToyDataset(\n        args.dataset_path,\n        split='test',\n        augmentator=None,\n        num_points=num_points,\n        with_segmentation=True,\n        points_from_one_object=train_proposals,\n        input_transform=model_cfg.input_transform\n    )\n\n    optimizer_params = {\n        'learning_rate': 5e-4,\n        'beta1': 0.9, 'beta2': 0.999, 'epsilon': 1e-8\n    }\n\n    if not train_proposals:\n        lr_scheduler = partial(LRScheduler, mode='cosine',\n                               baselr=optimizer_params['learning_rate'],\n                               nepochs=num_epochs)\n    else:\n        lr_scheduler = partial(LRScheduler, mode='cosine',\n                               baselr=optimizer_params['learning_rate'],\n                               nepochs=num_epochs)\n\n    trainer = AdaptISTrainer(args, model, model_cfg, loss_cfg,\n                             trainset, valset,\n                             optimizer='adam',\n                             optimizer_params=optimizer_params,\n                             lr_scheduler=lr_scheduler,\n                             checkpoint_interval=40 if not train_proposals else 5,\n                             image_dump_interval=200 if not train_proposals else -1,\n                             train_proposals=train_proposals,\n                             hybridize_model=not train_proposals,\n                             metrics=[AdaptiveIoU()])\n\n    logger.info(f'Starting Epoch: {start_epoch}')\n    logger.info(f'Total Epochs: {num_epochs}')\n    for epoch in range(start_epoch, num_epochs):\n        trainer.training(epoch)\n        trainer.validation(epoch)\n\n\nif __name__ == '__main__':\n    args = init_experiment('toy_v2', add_exp_args, script_path=__file__)\n\n    model, model_cfg = init_model()\n    train(model, model_cfg, args, train_proposals=False,\n          start_epoch=args.start_epoch)\n    init_proposals_head(model, args.ctx)\n    train(model, model_cfg, args, train_proposals=True)\n"""
adaptis/coco/panoptic_metric.py,0,"b'# Based on ""COCO 2018 Panoptic Segmentation Task API"":\n# https://github.com/cocodataset/panopticapi/blob/master/evaluation.py\nimport numpy as np\nfrom collections import defaultdict\nfrom .utils import rgb2id\n\nVOID = 0\nOFFSET = 255 * 255 * 255\n\n\nclass PQStatCat():\n    def __init__(self):\n        self.iou = 0.0\n        self.tp = 0\n        self.fp = 0\n        self.fn = 0\n\n    def __iadd__(self, pq_stat_cat):\n        self.iou += pq_stat_cat.iou\n        self.tp += pq_stat_cat.tp\n        self.fp += pq_stat_cat.fp\n        self.fn += pq_stat_cat.fn\n        return self\n\n\nclass PQStat():\n    def __init__(self):\n        self.pq_per_cat = defaultdict(PQStatCat)\n\n    def __getitem__(self, i):\n        return self.pq_per_cat[i]\n\n    def __iadd__(self, pq_stat):\n        for label, pq_stat_cat in pq_stat.pq_per_cat.items():\n            self.pq_per_cat[label] += pq_stat_cat\n        return self\n\n    def pq_average(self, categories, isthing):\n        pq, sq, rq, n = 0, 0, 0, 0\n        per_class_results = {}\n        for label, label_info in categories.items():\n            if isthing is not None:\n                cat_isthing = label_info[\'isthing\'] == 1\n                if isthing != cat_isthing:\n                    continue\n            iou = self.pq_per_cat[label].iou\n            tp = self.pq_per_cat[label].tp\n            fp = self.pq_per_cat[label].fp\n            fn = self.pq_per_cat[label].fn\n            if tp + fp + fn == 0:\n                per_class_results[label] = {\'pq\': 0.0, \'sq\': 0.0, \'rq\': 0.0}\n                continue\n            n += 1\n            pq_class = iou / (tp + 0.5 * fp + 0.5 * fn)\n            sq_class = iou / tp if tp != 0 else 0\n            rq_class = tp / (tp + 0.5 * fp + 0.5 * fn)\n            per_class_results[label] = {\'pq\': pq_class, \'sq\': sq_class, \'rq\': rq_class}\n            pq += pq_class\n            sq += sq_class\n            rq += rq_class\n\n        return {\'pq\': pq / n, \'sq\': sq / n, \'rq\': rq / n, \'n\': n}, per_class_results\n\n\ndef pq_compute(pq_stat, pred_sample, gt_sample, categories):\n    gt_segms = {el[\'id\']: el for el in gt_sample[\'segments_info\']}\n    pred_segms = {el[\'id\']: el for el in pred_sample[\'segments_info\']}\n\n    pan_gt = rgb2id(gt_sample[\'annotation\'])\n    pan_pred = rgb2id(pred_sample[\'annotation\'])\n\n    pred_labels_set = set(el[\'id\'] for el in pred_sample[\'segments_info\'])\n    labels, labels_cnt = np.unique(pan_pred, return_counts=True)\n\n    for label, label_cnt in zip(labels, labels_cnt):\n        if label not in pred_segms:\n            if label == VOID:\n                continue\n            raise KeyError(\n                \'In the image segment with ID {} is presented in PNG and not presented in JSON.\'.format(label))\n        pred_segms[label][\'area\'] = label_cnt\n        pred_labels_set.remove(label)\n        if pred_segms[label][\'category_id\'] not in categories:\n            raise KeyError(\n                \'In the image segment with ID {} has unknown category_id {}.\'.format(label, pred_segms[label][\n                                                                                                    \'category_id\']))\n\n    if len(pred_labels_set) != 0:\n        raise KeyError(\n            \'In the image the following segment IDs {} are presented in JSON and not presented in PNG.\'.format(\n                list(pred_labels_set)))\n\n    # confusion matrix calculation\n    pan_gt_pred = pan_gt.astype(np.uint64) * OFFSET + pan_pred.astype(np.uint64)\n    gt_pred_map = {}\n    labels, labels_cnt = np.unique(pan_gt_pred, return_counts=True)\n    for label, intersection in zip(labels, labels_cnt):\n        gt_id = label // OFFSET\n        pred_id = label % OFFSET\n        gt_pred_map[(gt_id, pred_id)] = intersection\n\n    # count all matched pairs\n    gt_matched = set()\n    pred_matched = set()\n    for label_tuple, intersection in gt_pred_map.items():\n        gt_label, pred_label = label_tuple\n        if gt_label not in gt_segms:\n            continue\n        if pred_label not in pred_segms:\n            continue\n        if gt_segms[gt_label][\'iscrowd\'] == 1:\n            continue\n        if gt_segms[gt_label][\'category_id\'] != pred_segms[pred_label][\'category_id\']:\n            continue\n\n        union = pred_segms[pred_label][\'area\'] + gt_segms[gt_label][\'area\'] - intersection - gt_pred_map.get(\n            (VOID, pred_label), 0)\n        iou = intersection / union\n        if iou > 0.5:\n            pq_stat[gt_segms[gt_label][\'category_id\']].tp += 1\n            pq_stat[gt_segms[gt_label][\'category_id\']].iou += iou\n            gt_matched.add(gt_label)\n            pred_matched.add(pred_label)\n\n    # count false positives\n    crowd_labels_dict = {}\n    for gt_label, gt_info in gt_segms.items():\n        if gt_label in gt_matched:\n            continue\n        # crowd segments are ignored\n        if gt_info[\'iscrowd\'] == 1:\n            crowd_labels_dict[gt_info[\'category_id\']] = gt_label\n            continue\n        pq_stat[gt_info[\'category_id\']].fn += 1\n\n    # count false positives\n    for pred_label, pred_info in pred_segms.items():\n        if pred_label in pred_matched:\n            continue\n        # intersection of the segment with VOID\n        intersection = gt_pred_map.get((VOID, pred_label), 0)\n        # plus intersection with corresponding CROWD region if it exists\n        if pred_info[\'category_id\'] in crowd_labels_dict:\n            intersection += gt_pred_map.get((crowd_labels_dict[pred_info[\'category_id\']], pred_label), 0)\n        # predicted segment is ignored if more than half of the segment correspond to VOID and CROWD regions\n        if intersection / pred_info[\'area\'] > 0.5:\n            continue\n        pq_stat[pred_info[\'category_id\']].fp += 1\n\n    return pq_stat\n\n\ndef print_pq_stat(pq_stat, categories):\n    metrics = [(""All"", None), (""Things"", True), (""Stuff"", False)]\n    results = {}\n    for name, isthing in metrics:\n        results[name], per_class_results = pq_stat.pq_average(categories, isthing=isthing)\n        if name == \'All\':\n            results[\'per_class\'] = per_class_results\n    print(""{:10s}| {:>6s}  {:>6s}  {:>6s} {:>5s}"".format("""", ""PQ"", ""SQ"", ""RQ"", ""N""))\n    print(""-"" * (10 + 7 * 4))\n\n    for name, _isthing in metrics:\n        print(""{:10s}| {:6.2f}  {:6.2f}  {:6.2f} {:5d}"".format(\n            name,\n            100 * results[name][\'pq\'],\n            100 * results[name][\'sq\'],\n            100 * results[name][\'rq\'],\n            results[name][\'n\'])\n        )\n'"
adaptis/coco/utils.py,0,"b'# Based on ""COCO 2018 Panoptic Segmentation Task API"":\n# https://github.com/cocodataset/panopticapi/blob/master/utils.py\nimport numpy as np\n\n\nclass IdGenerator():\n    \'\'\'\n    The class is designed to generate unique IDs that have meaningful RGB encoding.\n    Given semantic category unique ID will be generated and its RGB encoding will\n    have color close to the predefined semantic category color.\n    The RGB encoding used is ID = R * 256 * G + 256 * 256 + B.\n    Class constructor takes dictionary {id: category_info}, where all semantic\n    class ids are presented and category_info record is a dict with fields\n    \'isthing\' and \'color\'\n    \'\'\'\n\n    def __init__(self, categories):\n        self.taken_colors = set([0, 0, 0])\n        self.categories = categories\n        for category in self.categories.values():\n            if category[\'isthing\'] == 0:\n                self.taken_colors.add(tuple(category[\'color\']))\n\n    def get_color(self, cat_id):\n        def random_color(base, max_dist=30):\n            new_color = base + np.random.randint(low=-max_dist,\n                                                 high=max_dist + 1,\n                                                 size=3)\n            return tuple(np.maximum(0, np.minimum(255, new_color)))\n\n        category = self.categories[cat_id]\n        if category[\'isthing\'] == 0:\n            return category[\'color\']\n        base_color_array = category[\'color\']\n        base_color = tuple(base_color_array)\n        if base_color not in self.taken_colors:\n            self.taken_colors.add(base_color)\n            return base_color\n        else:\n            while True:\n                color = random_color(base_color_array)\n                if color not in self.taken_colors:\n                    self.taken_colors.add(color)\n                    return color\n\n    def get_id(self, cat_id):\n        color = self.get_color(cat_id)\n        return rgb2id(color)\n\n    def get_id_and_color(self, cat_id):\n        color = self.get_color(cat_id)\n        return rgb2id(color), color\n\n\ndef rgb2id(color):\n    if isinstance(color, np.ndarray) and len(color.shape) == 3:\n        if color.dtype == np.uint8:\n            color = color.astype(np.int32)\n        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n    return int(color[0] + 256 * color[1] + 256 * 256 * color[2])\n\n\ndef id2rgb(id_map):\n    if isinstance(id_map, np.ndarray):\n        id_map_copy = id_map.copy()\n        rgb_shape = tuple(list(id_map.shape) + [3])\n        rgb_map = np.zeros(rgb_shape, dtype=np.uint8)\n        for i in range(3):\n            rgb_map[..., i] = id_map_copy % 256\n            id_map_copy //= 256\n        return rgb_map\n    color = []\n    for _ in range(3):\n        color.append(id_map % 256)\n        id_map //= 256\n    return color\n'"
adaptis/data/base.py,0,"b""import cv2\nimport random\nimport numpy as np\nimport mxnet as mx\nfrom mxnet import gluon\nfrom scipy.ndimage import measurements\nfrom adaptis.coco.utils import IdGenerator, rgb2id\nfrom adaptis.utils.vis import get_palette\n\n\nclass AdaptISDataset(gluon.data.dataset.Dataset):\n    def __init__(self,\n                 with_segmentation=False,\n                 points_from_one_object=False,\n                 augmentator=None,\n                 num_points=1,\n                 input_transform=None,\n                 image_rescale=None,\n                 min_object_area=0,\n                 min_ignore_object_area=10,\n                 keep_background_prob=0.0,\n                 sample_ignore_object_prob=0.0,\n                 epoch_len=-1):\n        super(AdaptISDataset, self).__init__()\n        self.epoch_len = epoch_len\n        self.num_points = num_points\n        self.with_segmentation = with_segmentation\n        self.points_from_one_object = points_from_one_object\n        self.input_transform = input_transform\n        self.augmentator = augmentator\n        self.image_rescale = image_rescale\n        self.min_object_area = min_object_area\n        self.min_ignore_object_area = min_ignore_object_area\n        self.keep_background_prob = keep_background_prob\n        self.sample_ignore_object_prob = sample_ignore_object_prob\n\n        if isinstance(self.image_rescale, (float, int)):\n            scale = self.image_rescale\n            self.image_rescale = lambda shape: scale\n\n        self.dataset_samples = None\n        self._precise_masks = ['instances_mask', 'semantic_segmentation']\n        self._from_dataset_mapping = None\n        self._to_dataset_mapping = None\n\n    def __getitem__(self, index):\n        if self.epoch_len > 0:\n            index = random.randrange(0, len(self.dataset_samples))\n\n        sample = self.get_sample(index)\n        self.check_sample_types(sample)\n\n        sample = self.rescale_sample(sample)\n        sample = self.augment_sample(sample)\n        sample = self.exclude_small_objects(sample)\n        if self.sample_ignore_object_prob > 0:\n            sample = self.determine_ignored_regions(sample)\n\n        objects_id = [obj_id for obj_id, obj_info in sample['instances_info'].items()\n                      if not obj_info['ignore']]\n\n        if len(objects_id) > 0:\n            points, points_masks = [], []\n            mask, indices, is_ignored = self.get_random_object(sample, objects_id)\n            for i in range(self.num_points):\n                if is_ignored:\n                    mask[mask > 0.5] = -1\n\n                point_coord = indices[np.random.randint(0, indices.shape[0])]\n                points_masks.append(mask)\n                points.append(point_coord)\n                if not self.points_from_one_object:\n                    mask, indices, is_ignored = self.get_random_object(sample, objects_id)\n\n            points_masks = np.array(points_masks)\n            points = np.array(points)\n        else:\n            height, width = sample['instances_mask'].shape[:2]\n            points_masks = np.full((self.num_points, 1, height, width), -1, dtype=np.float32)\n            points = np.zeros((self.num_points, 2), dtype=np.float32)\n\n        image = mx.ndarray.array(sample['image'], mx.cpu(0))\n        if self.input_transform is not None:\n            image = self.input_transform(image)\n\n        output = {\n            'images': image,\n            'points': points.astype(np.float32),\n            'instances': points_masks\n        }\n        if self.with_segmentation:\n            output['semantic'] = sample['semantic_segmentation']\n\n        return output\n\n    def check_sample_types(self, sample):\n        assert sample['image'].dtype == 'uint8'\n        assert sample['instances_mask'].dtype == 'int32'\n        if 'semantic_segmentation' in sample:\n            assert sample['semantic_segmentation'].dtype == 'int32'\n\n    def rescale_sample(self, sample):\n        if self.image_rescale is None:\n            return sample\n\n        image = sample['image']\n        scale = self.image_rescale(image.shape)\n        image = cv2.resize(image, (0, 0), fx=scale, fy=scale)\n        new_size = (image.shape[1], image.shape[0])\n\n        sample['image'] = image\n        for mask_name in self._precise_masks:\n            if mask_name not in sample:\n                continue\n            sample[mask_name] = cv2.resize(sample[mask_name], new_size,\n                                           interpolation=cv2.INTER_NEAREST)\n\n        return sample\n\n    def augment_sample(self, sample):\n        if self.augmentator is None:\n            return sample\n\n        if 'semantic_segmentation' in sample:\n            sample['semantic_segmentation'] += 1\n\n        masks_to_augment = [mask_name for mask_name in self._precise_masks if mask_name in sample]\n        masks = [sample[mask_name] for mask_name in masks_to_augment]\n\n        valid_augmentation = False\n        while not valid_augmentation:\n            aug_output = self.augmentator(image=sample['image'], masks=masks)\n            valid_augmentation = self.check_augmented_sample(sample, aug_output, masks_to_augment)\n\n        sample['image'] = aug_output['image']\n        for mask_name, mask in zip(masks_to_augment, aug_output['masks']):\n            sample[mask_name] = mask\n\n        if 'semantic_segmentation' in sample:\n            sample['semantic_segmentation'] -= 1\n\n        sample_ids = set(get_unique_labels(sample['instances_mask'], exclude_zero=True))\n        instances_info = sample['instances_info']\n        instances_info = {sample_id: sample_info for sample_id, sample_info in instances_info.items()\n                          if sample_id in sample_ids}\n        sample['instances_info'] = instances_info\n\n        return sample\n\n    def check_augmented_sample(self, sample, aug_output, masks_to_augment):\n        if self.keep_background_prob < 0.0 or random.random() < self.keep_background_prob:\n            return True\n\n        aug_instances_mask = aug_output['masks'][masks_to_augment.index('instances_mask')]\n        aug_sample_ids = set(get_unique_labels(aug_instances_mask, exclude_zero=True))\n        num_objects_after_aug = len([obj_id for obj_id in aug_sample_ids\n                                     if not sample['instances_info'][obj_id]['ignore']])\n\n        return num_objects_after_aug > 0\n\n    def exclude_small_objects(self, sample):\n        if self.min_object_area <= 0:\n            return sample\n\n        for obj_id, obj_info in sample['instances_info'].items():\n            if not obj_info['ignore']:\n                obj_area = (sample['instances_mask'] == obj_id).sum()\n                if obj_area < self.min_object_area:\n                    obj_info['ignore'] = True\n\n        return sample\n\n    def determine_ignored_regions(self, sample):\n        ignore_ids = []\n        ignore_mask = np.zeros_like(sample['instances_mask'])\n        for obj_id, obj_info in sample['instances_info'].items():\n            if obj_info['ignore']:\n                obj_mask = sample['instances_mask'] == obj_id\n                if np.sum(obj_mask) < self.min_ignore_object_area:\n                    continue\n                ignore_mask[obj_mask] = obj_id\n                ignore_ids.append(obj_id)\n\n        if 'semantic_segmentation' in sample:\n            current_id = max(ignore_ids, default=0) + 1\n            ss_ignore = sample['semantic_segmentation'] == -1\n            ss_ignore_labeled, _ = measurements.label(ss_ignore, np.ones((3, 3)))\n            ss_ignore_ids = get_unique_labels(ss_ignore_labeled, exclude_zero=True)\n\n            for region_id in ss_ignore_ids:\n                region_mask = ss_ignore_labeled == region_id\n\n                region_mask_area = region_mask.sum()\n                if region_mask_area < self.min_ignore_object_area:\n                    continue\n\n                if ignore_mask[region_mask].sum() > 0:\n                    continue\n\n                ignore_mask[region_mask] = current_id\n                ignore_ids.append(current_id)\n                current_id += 1\n\n        sample['ignore_mask'] = ignore_mask\n        sample['ignore_ids'] = ignore_ids\n\n        return sample\n\n    def get_random_object(self, sample, objects_id):\n        if sample.get('ignore_ids', []) and random.random() < self.sample_ignore_object_prob:\n            random_id = random.choice(sample['ignore_ids'])\n            mask = sample['ignore_mask'] == random_id\n            is_ignored = True\n        else:\n            random_id = random.choice(objects_id)\n            mask = sample['instances_mask'] == random_id\n            is_ignored = False\n\n        indices = np.argwhere(mask)\n        mask = mask.astype(np.float32)[np.newaxis, :]\n\n        return mask, indices, is_ignored\n\n    def convert_to_coco_format(self, sample, use_id_generator=False, min_segment_area=1):\n        instances_mask = sample['instances_mask']\n        panoptic_labels = np.zeros(instances_mask.shape[:2] + (3,), dtype=np.uint8)\n        segments = []\n\n        semantic_labels = sample['semantic_segmentation'].copy()\n        semantic_labels[instances_mask > 0] = -1\n        semantic_ids = np.array(get_unique_labels(semantic_labels + 1)) - 1\n\n        if use_id_generator:\n            if not hasattr(self, '_categories'):\n                self._categories = self._generate_coco_categories()\n            categories = {el['id']: el for el in self._categories}\n            id_generator = IdGenerator(categories)\n        else:\n            palette = get_palette(len(semantic_labels) + len(sample['instances_info']) + 1)[1:].astype(int)\n\n        def add_segment(label_id, class_id, isthing):\n            if class_id == -1 or (not isthing and class_id >= self.things_offset):\n                return\n\n            segment_mask = (instances_mask if isthing else semantic_labels) == label_id\n            segment_area = segment_mask.sum()\n            if segment_area < min_segment_area:\n                return\n\n            category_id = self.to_dataset_mapping[class_id]\n            if use_id_generator:\n                segment_id, color = id_generator.get_id_and_color(category_id)\n            else:\n                color = palette[len(segments)]\n                segment_id = rgb2id(color)\n\n            panoptic_labels[segment_mask] = color\n            segments.append({\n                'id': segment_id,\n                'category_id': category_id,\n                'iscrowd': 0,\n                'area': int(segment_area)\n            })\n\n        for label_id in semantic_ids:\n            add_segment(label_id, label_id, False)\n\n        for inst_id, instance in sample['instances_info'].items():\n            add_segment(inst_id, instance['class_id'], True)\n\n        coco_sample = {\n            'annotation': panoptic_labels,\n            'segments_info': segments\n        }\n\n        if 'image' in sample:\n            coco_sample['image'] = sample['image']\n        return coco_sample\n\n    def _generate_coco_categories(self):\n        categories = []\n\n        palette = get_palette(self.num_classes + 1)[1:].astype(int)\n        for indx, stuff_label in enumerate(self.stuff_labels):\n            categories.append({\n                'id': stuff_label,\n                'isthing': 0,\n                'color': tuple(palette[indx])\n            })\n\n        for indx, thing_label in enumerate(self.things_labels):\n            categories.append({\n                'id': thing_label,\n                'isthing': 1,\n                'color': tuple(palette[self.things_offset + indx])\n            })\n\n        return categories\n\n    def get_sample(self, index):\n        raise NotImplementedError\n\n    @property\n    def stuff_labels(self):\n        raise NotImplementedError\n\n    @property\n    def things_labels(self):\n        raise NotImplementedError\n\n    @property\n    def from_dataset_mapping(self):\n        if self._from_dataset_mapping is None:\n            dataset_labels = self.stuff_labels + self.things_labels\n            mapping = {label: indx for indx, label in enumerate(dataset_labels)}\n            self._from_dataset_mapping = mapping\n\n        return self._from_dataset_mapping\n\n    @property\n    def to_dataset_mapping(self):\n        if self._to_dataset_mapping is None:\n            mapping = {indx: label for label, indx in self.from_dataset_mapping.items()}\n            self._to_dataset_mapping = mapping\n\n        return self._to_dataset_mapping\n\n    @property\n    def stuff_offset(self):\n        return 0\n\n    @property\n    def things_offset(self):\n        return len(self.stuff_labels)\n\n    @property\n    def num_classes(self):\n        return len(self.from_dataset_mapping)\n\n    def __len__(self):\n        if self.epoch_len > 0:\n            return self.epoch_len\n        else:\n            return len(self.dataset_samples)\n\n\ndef get_unique_labels(x, exclude_zero=False):\n    obj_sizes = np.bincount(x.flatten())\n    labels = np.nonzero(obj_sizes)[0].tolist()\n\n    if exclude_zero:\n        labels = [x for x in labels if x != 0]\n    return labels\n"""
adaptis/data/cityscapes.py,0,"b""import cv2\nimport numpy as np\nfrom pathlib import Path\nfrom .base import AdaptISDataset, get_unique_labels\n\n\nclass CityscapesDataset(AdaptISDataset):\n    def __init__(self, dataset_path, split='train', use_jpeg=False, **kwargs):\n        super(CityscapesDataset, self).__init__(**kwargs)\n\n        self.dataset_path = Path(dataset_path)\n        self.dataset_split = split\n\n        images_path = self.dataset_path / ('leftImgJPG' if use_jpeg else 'leftImg8bit') / split\n        gt_path = self.dataset_path / 'gtFine' / split\n\n        images_mask = '*leftImg8bit.jpg' if use_jpeg else '*leftImg8bit.png'\n        images_list = sorted(images_path.rglob(images_mask))\n\n        self.dataset_samples = []\n        for image_path in images_list:\n            image_name = str(image_path.relative_to(images_path))\n            instances_name = image_name.replace(images_mask[1:], 'gtFine_instanceIds.png')\n            instances_path = str(gt_path / instances_name)\n\n            semantic_name = image_name.replace(images_mask[1:], 'gtFine_labelIds.png')\n            semantic_path = str(gt_path / semantic_name)\n\n            self.dataset_samples.append((str(image_path), instances_path, semantic_path))\n\n        total_classes = 34\n        self._semseg_mapping = np.ones(total_classes, dtype=np.int32)\n        for i in range(total_classes):\n            self._semseg_mapping[i] = self.from_dataset_mapping.get(i, -1)\n\n    def get_sample(self, index):\n        image_path, instances_path, semantic_path = self.dataset_samples[index]\n\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        instance_map = cv2.imread(instances_path, cv2.IMREAD_UNCHANGED).astype(np.int32)\n        label_map = cv2.imread(semantic_path, cv2.IMREAD_UNCHANGED)\n        label_map = self._semseg_mapping[label_map]\n\n        instances_info = dict()\n        instances_ids = get_unique_labels(instance_map)\n        for obj_id in instances_ids:\n            if obj_id < 1000:\n                continue\n\n            class_id = obj_id // 1000\n            mapped_class_id = self._semseg_mapping[class_id]\n            ignore = mapped_class_id == -1\n\n            instances_info[obj_id] = {\n                'class_id': mapped_class_id, 'ignore': ignore\n            }\n\n        iscrowd = np.logical_and(instance_map < 1000, label_map >= self.things_offset)\n        iscrowd_area = np.sum(iscrowd)\n        if iscrowd_area > 0:\n            iscrowd_labels = get_unique_labels(label_map[iscrowd])\n            iscrowd_labels = [label for label in iscrowd_labels if label >= self.things_offset]\n\n            new_obj_id = max(instances_info.keys(), default=0) + 1\n            for class_id in iscrowd_labels:\n                iscrowd_mask = np.logical_and(iscrowd, label_map == class_id)\n                instance_map[iscrowd_mask] = new_obj_id\n                instances_info[new_obj_id] = {'class_id': class_id, 'ignore': True}\n                new_obj_id += 1\n\n        instance_map[instance_map < 1000] = 0\n\n        sample = {\n            'image': image,\n            'instances_mask': instance_map,\n            'instances_info': instances_info,\n            'semantic_segmentation': label_map\n        }\n\n        return sample\n\n    @property\n    def stuff_labels(self):\n        return [7, 8, 11, 12, 13, 17, 19, 20, 21, 22, 23]\n\n    @property\n    def things_labels(self):\n        return [24, 25, 26, 27, 28, 31, 32, 33]\n"""
adaptis/data/coco.py,0,"b""import cv2\nimport json\nimport numpy as np\nfrom pathlib import Path\nfrom .base import AdaptISDataset\n\n\nclass CocoDataset(AdaptISDataset):\n    def __init__(self, dataset_path, split='train', **kwargs):\n        super(CocoDataset, self).__init__(**kwargs)\n        self.split = split\n        self.dataset_path = Path(dataset_path)\n\n        self.load_samples()\n\n    def load_samples(self):\n        annotation_path = self.dataset_path / 'annotations' / f'panoptic_{self.split}.json'\n        self.labels_path = self.dataset_path / 'annotations' / f'panoptic_{self.split}'\n        self.images_path = self.dataset_path / self.split\n\n        with open(annotation_path, 'r') as f:\n            annotation = json.load(f)\n\n        self.dataset_samples = annotation['annotations']\n\n        self._categories = annotation['categories']\n        self._stuff_labels = [x['id'] for x in self._categories if x['isthing'] == 0]\n        self._things_labels = [x['id'] for x in self._categories if x['isthing'] == 1]\n        self._things_labels_set = set(self._things_labels)\n\n    def get_sample(self, index, coco_format=False):\n        dataset_sample = self.dataset_samples[index]\n\n        image_path = self.images_path / self.get_image_name(dataset_sample['file_name'])\n        label_path = self.labels_path / dataset_sample['file_name']\n\n        image = cv2.imread(str(image_path))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        label = cv2.imread(str(label_path), cv2.IMREAD_UNCHANGED).astype(np.int32)\n        label = 256 * 256 * label[:, :, 0] + 256 * label[:, :, 1] + label[:, :, 2]\n\n        if coco_format:\n            return {\n                'image': image,\n                'annotation': label,\n                'segments_info': dataset_sample['segments_info']\n            }\n\n        label_map = np.full_like(label, -1)\n        for segment in dataset_sample['segments_info']:\n            label_map[label == segment['id']] = self.from_dataset_mapping[segment['category_id']]\n\n        instance_map = np.full_like(label, 0)\n        instances_info = dict()\n        for segment in dataset_sample['segments_info']:\n            class_id = segment['category_id']\n            if class_id not in self._things_labels_set:\n                continue\n\n            mapped_class_id = self.from_dataset_mapping[class_id]\n            obj_id = segment['id']\n            instance_map[label == obj_id] = obj_id\n\n            ignore = segment['iscrowd'] == 1\n            instances_info[obj_id] = {\n                'class_id': mapped_class_id, 'ignore': ignore\n            }\n\n        sample = {\n            'image': image,\n            'instances_mask': instance_map,\n            'instances_info': instances_info,\n            'semantic_segmentation': label_map\n        }\n\n        return sample\n\n    @classmethod\n    def get_image_name(cls, panoptic_name):\n        return panoptic_name.replace('.png', '.jpg')\n\n    @property\n    def stuff_labels(self):\n        return self._stuff_labels\n\n    @property\n    def things_labels(self):\n        return self._things_labels\n"""
adaptis/data/toy.py,0,"b""import cv2\nimport numpy as np\nfrom pathlib import Path\nfrom .base import AdaptISDataset, get_unique_labels\n\n\nclass ToyDataset(AdaptISDataset):\n    def __init__(self, dataset_path, split='train', **kwargs):\n        super(ToyDataset, self).__init__(**kwargs)\n\n        self.dataset_path = Path(dataset_path)\n        self.dataset_split = split\n\n        self.dataset_samples = []\n        images_path = sorted((self.dataset_path / split).rglob('*rgb.png'))\n        for image_path in images_path:\n            image_path = str(image_path)\n            mask_path = image_path.replace('rgb.png', 'im.png')\n            self.dataset_samples.append((image_path, mask_path))\n\n    def get_sample(self, index):\n        image_path, mask_path = self.dataset_samples[index]\n\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        instances_mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED).astype(np.int32)\n\n        sample = {'image': image}\n        if self.with_segmentation:\n            semantic_segmentation = (instances_mask > 0).astype(np.int32)\n            sample['semantic_segmentation'] = semantic_segmentation\n        else:\n            instances_mask += 1\n\n        instances_ids = get_unique_labels(instances_mask, exclude_zero=True)\n        instances_info = {\n            x: {'class_id': 1, 'ignore': False}\n            for x in instances_ids\n        }\n\n        sample.update({\n            'instances_mask': instances_mask,\n            'instances_info': instances_info,\n        })\n\n        return sample\n\n    @property\n    def stuff_labels(self):\n        return [0]\n\n    @property\n    def things_labels(self):\n        return [1]\n"""
adaptis/engine/trainer.py,0,"b'import os\nimport cv2\nimport mxnet as mx\nimport numpy as np\nfrom mxnet import gluon, autograd\nfrom tqdm import tqdm\nimport logging\n\nfrom copy import deepcopy\nfrom gluoncv.utils.viz.segmentation import DeNormalize\nfrom collections import defaultdict\n\nfrom adaptis.utils.log import logger, TqdmToLogger, SummaryWriterAvg\nfrom adaptis.utils.vis import draw_probmap, draw_points, visualize_mask\nfrom adaptis.utils.misc import save_checkpoint, get_dict_batchify_fn\n\n\nclass AdaptISTrainer(object):\n    def __init__(self, args, model, model_cfg, loss_cfg,\n                 trainset, valset, optimizer_params,\n                 optimizer=\'adam\',\n                 image_dump_interval=200,\n                 checkpoint_interval=10,\n                 tb_dump_period=25,\n                 lr_scheduler=None,\n                 metrics=None,\n                 additional_val_metrics=None,\n                 train_proposals=False,\n                 hybridize_model=True):\n        self.args = args\n        self.model_cfg = model_cfg\n        self.loss_cfg = loss_cfg\n        self.val_loss_cfg = deepcopy(loss_cfg)\n        self.tb_dump_period = tb_dump_period\n\n        if metrics is None:\n            metrics = []\n        self.train_metrics = metrics\n        self.val_metrics = deepcopy(metrics)\n        if additional_val_metrics is not None:\n            self.val_metrics.extend(additional_val_metrics)\n\n        self.hybridize_model = hybridize_model\n        self.checkpoint_interval = checkpoint_interval\n        self.train_proposals = train_proposals\n        self.task_prefix = \'\'\n\n        self.trainset = trainset\n        self.valset = valset\n\n        self.train_data = gluon.data.DataLoader(\n            trainset, args.batch_size, shuffle=True,\n            last_batch=\'rollover\',\n            batchify_fn=get_dict_batchify_fn(args.workers),\n            thread_pool=args.thread_pool,\n            num_workers=args.workers)\n\n        self.val_data = gluon.data.DataLoader(\n            valset, args.val_batch_size,\n            batchify_fn=get_dict_batchify_fn(args.workers),\n            last_batch=\'rollover\',\n            thread_pool=args.thread_pool,\n            num_workers=args.workers)\n\n        logger.info(model)\n        model.cast(args.dtype)\n        model.collect_params().reset_ctx(ctx=args.ctx)\n\n        self.net = model\n        self.evaluator = None\n        if args.weights is not None:\n            if os.path.isfile(args.weights):\n                model.load_parameters(args.weights, ctx=args.ctx, allow_missing=True)\n                args.weights = None\n            else:\n                raise RuntimeError(f""=> no checkpoint found at \'{args.weights}\'"")\n\n        self.lr_scheduler = None\n        if lr_scheduler is not None:\n            self.lr_scheduler = lr_scheduler(niters=len(self.train_data))\n            optimizer_params[\'lr_scheduler\'] = self.lr_scheduler\n\n        kv = mx.kv.create(args.kvstore)\n        if not train_proposals:\n            train_params = self.net.collect_params()\n        else:\n            train_params = self.net.proposals_head.collect_params()\n            self.task_prefix = \'proposals\'\n\n        self.trainer = gluon.Trainer(train_params,\n                                     optimizer, optimizer_params,\n                                     kvstore=kv, update_on_kvstore=len(args.ctx) > 1)\n\n        self.tqdm_out = TqdmToLogger(logger, level=logging.INFO)\n        if args.input_normalization:\n            self.denormalizator = DeNormalize(args.input_normalization[\'mean\'],\n                                              args.input_normalization[\'std\'])\n        else:\n            self.denormalizator = lambda x: x\n\n        self.sw = None\n        self.image_dump_interval = image_dump_interval\n\n    def training(self, epoch):\n        if self.sw is None:\n            self.sw = SummaryWriterAvg(logdir=str(self.args.logs_path),\n                                       flush_secs=10, dump_period=self.tb_dump_period)\n\n        log_prefix = \'Train\' + self.task_prefix.capitalize()\n        tbar = tqdm(self.train_data, file=self.tqdm_out, ncols=100)\n        train_loss = 0.0\n        hybridize = False\n\n        for metric in self.train_metrics:\n            metric.reset_epoch_stats()\n\n        for i, batch_data in enumerate(tbar):\n            if self.lr_scheduler is not None:\n                self.lr_scheduler.update(i, epoch)\n            global_step = epoch * len(self.train_data) + i\n\n            losses, losses_logging, splitted_batch_data, outputs = \\\n                self.batch_forward(batch_data)\n\n            autograd.backward(losses)\n            self.trainer.step(1, ignore_stale_grad=True)\n\n            batch_loss = sum(loss.asnumpy().mean() for loss in losses) / len(losses)\n            train_loss += batch_loss\n\n            for loss_name, loss_values in losses_logging.items():\n                self.sw.add_scalar(tag=f\'{log_prefix}Losses/{loss_name}\', value=np.array(loss_values).mean(),\n                                   global_step=global_step)\n            self.sw.add_scalar(tag=f\'{log_prefix}Losses/overall\', value=batch_loss, global_step=global_step)\n\n            for k, v in self.loss_cfg.items():\n                if \'_loss\' in k and hasattr(v, \'log_states\') and self.loss_cfg.get(k + \'_weight\', 0.0) > 0:\n                    v.log_states(self.sw, f\'{log_prefix}Losses/{k}\', global_step)\n\n            if self.image_dump_interval > 0 and global_step % self.image_dump_interval == 0:\n                self.save_visualization(splitted_batch_data, outputs, global_step, prefix=\'images/train\')\n\n            self.sw.add_scalar(tag=f\'{log_prefix}States/learning_rate\', value=self.trainer.learning_rate,\n                               global_step=global_step)\n\n            tbar.set_description(f\'Epoch {epoch}, training loss {train_loss/(i+1):.6f}\')\n            for metric in self.train_metrics:\n                metric.log_states(self.sw, f\'{log_prefix}Metrics/{metric.name}\', global_step)\n            mx.nd.waitall()\n\n            if self.hybridize_model and not hybridize:\n                self.net.hybridize()\n                hybridize = True\n\n        for metric in self.train_metrics:\n            self.sw.add_scalar(tag=f\'{log_prefix}Metrics/{metric.name}\', value=metric.get_epoch_value(),\n                               global_step=epoch, disable_avg=True)\n\n        save_checkpoint(self.net, self.args.checkpoints_path, prefix=self.task_prefix, epoch=None)\n        if epoch % self.checkpoint_interval == 0:\n            save_checkpoint(self.net, self.args.checkpoints_path, prefix=self.task_prefix, epoch=epoch)\n\n    def validation(self, epoch):\n        if self.sw is None:\n            self.sw = SummaryWriterAvg(logdir=str(self.args.logs_path),\n                                       flush_secs=10, dump_period=self.tb_dump_period)\n\n        log_prefix = \'Val\' + self.task_prefix.capitalize()\n        tbar = tqdm(self.val_data, file=self.tqdm_out, ncols=100)\n\n        for metric in self.val_metrics:\n            metric.reset_epoch_stats()\n\n        num_batches = 0\n        val_loss = 0\n        losses_logging = defaultdict(list)\n        for i, batch_data in enumerate(tbar):\n            global_step = epoch * len(self.val_data) + i\n            losses, batch_losses_logging, splitted_batch_data, outputs = \\\n                self.batch_forward(batch_data, validation=True)\n\n            for loss_name, loss_values in batch_losses_logging.items():\n                losses_logging[loss_name].extend(loss_values)\n\n            batch_loss = sum(loss.asnumpy()[0] for loss in losses) / len(losses)\n            val_loss += batch_loss\n            num_batches += 1\n\n            tbar.set_description(f\'Epoch {epoch}, validation loss: {val_loss/num_batches:.6f}\')\n            for metric in self.val_metrics:\n                metric.log_states(self.sw, f\'{log_prefix}Metrics/{metric.name}\', global_step)\n\n        for loss_name, loss_values in losses_logging.items():\n            self.sw.add_scalar(tag=f\'{log_prefix}Losses/{loss_name}\', value=np.array(loss_values).mean(),\n                               global_step=epoch, disable_avg=True)\n\n        for metric in self.val_metrics:\n            self.sw.add_scalar(tag=f\'{log_prefix}Metrics/{metric.name}\', value=metric.get_epoch_value(),\n                               global_step=epoch, disable_avg=True)\n        self.sw.add_scalar(tag=f\'{log_prefix}Losses/overall\', value=val_loss / num_batches,\n                           global_step=epoch, disable_avg=True)\n\n    def batch_forward(self, batch_data, validation=False):\n        splitted_batch = {k: gluon.utils.split_and_load(v, ctx_list=self.args.ctx, even_split=False)\n                               for k, v in batch_data.items()}\n        if \'instances\' in splitted_batch:\n            splitted_batch[\'instances\'] = [masks.reshape(shape=(-3, -2))\n                                           for masks in splitted_batch[\'instances\']]\n\n        metrics = self.val_metrics if validation else self.train_metrics\n\n        losses_logging = defaultdict(list)\n        with autograd.record(True) if not validation else autograd.pause(False):\n            outputs = [self.net(image, points)\n                       for image, points in zip(splitted_batch[\'images\'], splitted_batch[\'points\'])]\n\n            losses = []\n            for ictx, ctx_output in enumerate(outputs):\n                loss = 0.0\n                loss = self.add_loss(\'instance_loss\', loss, losses_logging, validation,\n                                     lambda: (ctx_output.instances, splitted_batch[\'instances\'][ictx]))\n                loss = self.add_loss(\'segmentation_loss\', loss, losses_logging, validation,\n                                     lambda: (ctx_output.semantic, splitted_batch[\'semantic\'][ictx]))\n                loss = self.add_loss(\'proposals_loss\', loss, losses_logging, validation,\n                                     lambda: (ctx_output.instances, ctx_output.proposals, splitted_batch[\'instances\'][ictx]))\n\n                with autograd.pause():\n                    for m in metrics:\n                        m.update(*(getattr(ctx_output, x) for x in m.pred_outputs),\n                                 *(splitted_batch[x][ictx] for x in m.gt_outputs))\n\n                losses.append(loss)\n\n        return losses, losses_logging, splitted_batch, outputs\n\n    def add_loss(self, loss_name, total_loss, losses_logging, validation, lambda_loss_inputs):\n        loss_cfg = self.loss_cfg if not validation else self.val_loss_cfg\n        loss_weight = loss_cfg.get(loss_name + \'_weight\', 0.0)\n        if loss_weight > 0.0:\n            loss_criterion = loss_cfg.get(loss_name)\n            loss = loss_criterion(*lambda_loss_inputs())\n            loss = mx.nd.mean(loss)\n            losses_logging[loss_name].append(loss.asnumpy())\n            loss = loss_weight * loss\n            total_loss = total_loss + loss\n\n        return total_loss\n\n    def save_visualization(self, splitted_batch_data, outputs, global_step, prefix=\'images\'):\n        outputs = outputs[0]\n\n        output_images_path = self.args.logs_path / prefix\n        if self.task_prefix:\n            output_images_path /= self.task_prefix\n\n        if not output_images_path.exists():\n            output_images_path.mkdir(parents=True)\n        image_name_prefix = f\'{global_step:06d}\'\n\n        def _save_image(suffix, image):\n            cv2.imwrite(str(output_images_path / f\'{image_name_prefix}_{suffix}.jpg\'),\n                        image, [cv2.IMWRITE_JPEG_QUALITY, 85])\n\n        images = splitted_batch_data[\'images\']\n        points = splitted_batch_data[\'points\']\n        instance_masks = splitted_batch_data[\'instances\']\n\n        image_blob, points = images[0][0], points[0][0]\n        image = self.denormalizator(image_blob.as_in_context(mx.cpu(0))).asnumpy() * 255\n        image = image.transpose((1, 2, 0))\n\n        gt_instance_masks = instance_masks[0].asnumpy()\n        predicted_instance_masks = mx.nd.sigmoid(outputs.instances).asnumpy()\n\n        if \'semantic\' in splitted_batch_data:\n            segmentation_labels = splitted_batch_data[\'semantic\']\n            gt_segmentation = segmentation_labels[0][0].asnumpy() + 1\n            predicted_label = mx.nd.squeeze(mx.nd.argmax(outputs.semantic[0], 0)).asnumpy() + 1\n\n            if len(gt_segmentation.shape) == 3:\n                area_weights = gt_segmentation[1] ** self.loss_cfg.segmentation_loss._area_gamma\n                area_weights -= area_weights.min()\n                area_weights /= area_weights.max()\n                area_weights = draw_probmap(area_weights)\n\n                _save_image(\'area_weights\', area_weights[:, :, ::-1])\n                gt_segmentation = gt_segmentation[0]\n\n            gt_mask = visualize_mask(gt_segmentation.astype(np.int32), self.trainset.num_classes + 1)\n            predicted_mask = visualize_mask(predicted_label.astype(np.int32), self.trainset.num_classes + 1)\n            result = np.hstack((image, gt_mask, predicted_mask)).astype(np.uint8)\n\n            _save_image(\'semantic_segmentation\', result[:, :, ::-1])\n\n        points = points.asnumpy()\n        gt_masks = np.squeeze(gt_instance_masks[:points.shape[0]])\n        predicted_masks = np.squeeze(predicted_instance_masks[:points.shape[0]])\n\n        viz_image = []\n        for gt_mask, point, predicted_mask in zip(gt_masks, points, predicted_masks):\n            timage = draw_points(image, [point], (0, 255, 0))\n            gt_mask[gt_mask < 0] = 0.25\n            gt_mask = draw_probmap(gt_mask)\n            predicted_mask = draw_probmap(predicted_mask)\n            viz_image.append(np.hstack((timage, gt_mask, predicted_mask)))\n        viz_image = np.vstack(viz_image)\n\n        result = viz_image.astype(np.uint8)\n        _save_image(\'instance_segmentation\', result[:, :, ::-1])\n\n\ndef init_proposals_head(model, ctx):\n    model.hybridize(False)\n    model.collect_params().setattr(\'grad_req\', \'null\')\n    model.add_proposals_head(ctx)\n'"
adaptis/inference/adaptis_sampling.py,0,"b'import cv2\nimport math\nimport random\nimport numpy as np\nfrom bridson import poisson_disc_samples\nfrom adaptis.inference.cython_utils.utils import find_local_maxima\n\n\ndef get_panoptic_segmentation(pmodel, image, ignore_mask=None, use_flip=True,\n                              crop_size=None, min_overlap=0.2,\n                              stuff_prob_mult=1.0, min_things_prob=0.0, min_stuff_prob=0.0,\n                              sampling_algorithm=\'proposals\', **sampling_params):\n    image_height, image_width = image.shape[:2]\n    things_prob = np.zeros((len(pmodel.to_dataset_mapping) - pmodel.things_offset, image_height, image_width), dtype=np.float32)\n    stuff_prob = np.zeros((pmodel.things_offset, image_height, image_width), dtype=np.float32)\n    counts = np.zeros((image_height, image_width), dtype=np.float32)\n\n    if crop_size is None:\n        crop_size_h, crop_size_w = image_height, image_width\n    elif isinstance(crop_size, tuple):\n        crop_size_h, crop_size_w = crop_size\n    else:\n        crop_size_h, crop_size_w = crop_size, crop_size\n\n    x_offsets, x_overlap = get_offsets(image_width, crop_size_w, min_overlap)\n    y_offsets, y_overlap = get_offsets(image_height, crop_size_h, min_overlap)\n    x_osize = int(x_overlap * crop_size_w)\n    y_osize = int(y_overlap * crop_size_h)\n\n    masks = [None]\n    masks_counts = [None]\n    proposals_info = None\n    occupied = np.full(image.shape[:2], 0, dtype=np.int32)\n    for dy in y_offsets:\n        for dx in x_offsets:\n            crop_image = image[dy:dy + crop_size_h, dx:dx + crop_size_w]\n\n            cfeatures = pmodel.get_features(crop_image, use_flip)\n\n            cthings_prob, cstuff_prob = pmodel.get_semantic_segmentation(cfeatures, use_flip=use_flip,\n                                                                         output_height=crop_size_h, output_width=crop_size_w)\n\n            if crop_size is not None:\n                things_prob[:, dy:dy + crop_size_h, dx:dx + crop_size_w] += cthings_prob\n                stuff_prob[:, dy:dy + crop_size_h, dx:dx + crop_size_w] += cstuff_prob\n                counts[dy:dy + crop_size_h, dx:dx + crop_size_w] += 1\n\n            if ignore_mask is not None:\n                cignore_mask = ignore_mask[dy:dy + crop_size_h, dx:dx + crop_size_w]\n            else:\n                cignore_mask = None\n\n            if sampling_algorithm == \'proposals\':\n                crop_masks, proposals_info = \\\n                    predict_instances_with_proposals(pmodel, cfeatures, cthings_prob,\n                                                     cignore_mask, crop_image.shape,\n                                                     use_flip=use_flip, **sampling_params)\n            elif sampling_algorithm == \'random\':\n                crop_masks, proposals_info = \\\n                    predict_instances_random(pmodel, cfeatures, cthings_prob,\n                                             cignore_mask, crop_image.shape,\n                                             use_flip=use_flip, **sampling_params)\n            else:\n                assert False, ""Unknown sampling algorithm""\n\n            if crop_masks:\n                crop_final_map = np.array(crop_masks).argmax(axis=0)\n\n            if crop_size is None:\n                things_prob, stuff_prob = cthings_prob, cstuff_prob\n                masks = [None] + crop_masks\n                break\n\n            occupied_left = occupied[dy:dy + crop_size_h, dx:dx + x_osize]\n            occupied_top = occupied[dy:dy + y_osize, dx:dx + crop_size_w]\n            left_labels, left_sizes = np.unique(occupied_left.flatten(), return_counts=True)\n            top_labels, top_sizes = np.unique(occupied_top.flatten(), return_counts=True)\n\n            for i in range(len(crop_masks)):\n                obj_mask = crop_final_map == i\n                matched = False\n\n                if dy > 0:\n                    obj_mask_top = obj_mask[:y_osize, :]\n\n                    for ilabel, isize in zip(top_labels, top_sizes):\n                        inter_area = np.logical_and(obj_mask_top, occupied_top == ilabel).sum()\n                        union_area = np.logical_or(obj_mask_top, occupied_top == ilabel).sum()\n                        iou = inter_area / union_area\n\n                        if iou > 0.5:\n                            matched = True\n                            masks[ilabel][dy:dy + crop_size_h, dx:dx + crop_size_w] += crop_masks[i]\n                            masks_counts[ilabel][dy:dy + crop_size_h, dx:dx + crop_size_w] += 1\n                            occupied[dy:dy + crop_size_h, dx:dx + crop_size_w][obj_mask] = ilabel\n                            break\n\n                if dx > 0 and not matched:\n                    obj_mask_left = obj_mask[:, :x_osize]\n\n                    for ilabel, isize in zip(left_labels, left_sizes):\n                        inter_area = np.logical_and(obj_mask_left, occupied_left == ilabel).sum()\n                        union_area = np.logical_or(obj_mask_left, occupied_left == ilabel).sum()\n                        iou = inter_area / union_area\n\n                        if iou > 0.3:\n                            matched = True\n                            masks[ilabel][dy:dy + crop_size_h, dx:dx + crop_size_w] += crop_masks[i]\n                            masks_counts[ilabel][dy:dy + crop_size_h, dx:dx + crop_size_w] += 1\n                            occupied[dy:dy + crop_size_h, dx:dx + crop_size_w][obj_mask] = ilabel\n                            break\n\n                if not matched:\n                    new_mask = np.zeros((image_height, image_width), dtype=np.float32)\n                    new_mask[dy:dy + crop_size_h, dx:dx + crop_size_w] = crop_masks[i]\n                    new_mask_counts = np.zeros_like(new_mask)\n                    new_mask_counts[dy:dy + crop_size_h, dx:dx + crop_size_w] = 1\n\n                    masks.append(new_mask)\n                    masks_counts.append(new_mask_counts)\n                    occupied[dy:dy + crop_size_h, dx:dx + crop_size_w][obj_mask] = len(masks) - 1\n\n    if crop_size is not None:\n        things_prob /= counts\n        stuff_prob /= counts\n\n    object_labels = [None]\n    masks[0] = stuff_prob_mult * (1 - things_prob.sum(axis=0))\n    for i in range(1, len(masks)):\n        if crop_size is not None:\n            masks[i] /= np.maximum(masks_counts[i], 1)\n        obj_mask = masks[i] > sampling_params[\'thresh1\']\n        obj_label = things_prob[:, obj_mask].mean(axis=1).argmax()\n        object_labels.append(obj_label)\n\n    if len(masks) > 1:\n        tmasks = np.array(masks)\n        not_found_mask = tmasks[1:, :, :].max(axis=0) < min_things_prob\n        final_map = tmasks.argmax(axis=0)\n        final_map[not_found_mask] = 0\n    else:\n        final_map = np.array(masks).argmax(axis=0)\n\n    semantic_segmentation = np.concatenate((stuff_prob, things_prob), axis=0).argmax(axis=0)\n    semantic_segmentation[stuff_prob.max(axis=0) < min_stuff_prob] = -1\n    if ignore_mask is not None:\n        semantic_segmentation[ignore_mask > 0] = -1\n\n    things_offset = stuff_prob.shape[0]\n    instances_mask = final_map\n    instances_info = {\n        indx + 1: {\'class_id\': things_offset + label, \'ignore\': False}\n        for indx, label in enumerate(object_labels[1:])\n    }\n\n    return {\n        \'image\': None, \'semantic_segmentation\': semantic_segmentation,\n        \'instances_mask\': instances_mask, \'instances_info\': instances_info,\n        \'masks\': masks, \'proposals_info\': proposals_info\n    }\n\n\ndef predict_instances_with_proposals(pmodel, features,\n                                     instances_prob, ignore_mask,\n                                     image_shape,\n                                     sampling_mask=None, use_flip=False,\n                                     thresh1=0.4, thresh2=0.4, ithresh=0.6,\n                                     fl_prob=0.35, fl_eps=0.003, fl_blur=5, fl_step=0.025,\n                                     cut_radius=-1, max_iters=500):\n    point_invariant_states = pmodel.get_point_invariant_states(features)\n    output_height, output_width = image_shape[:2]\n\n    if ignore_mask is not None:\n        instances_prob = instances_prob * (1 - ignore_mask)\n\n    proposals_map = pmodel.get_proposals_map(features[0],\n                                             width=image_shape[1], height=image_shape[0])[0, 0]\n    if use_flip:\n        proposals_map_flipped = pmodel.get_proposals_map(features[1],\n                                                         width=image_shape[1], height=image_shape[0])[0, 0]\n        proposals_map = 0.5 * (proposals_map + proposals_map_flipped[:, ::-1])\n\n    proposals_map[instances_prob.max(axis=0) < ithresh] = 0\n\n    masks = []\n    occupied = np.zeros(image_shape[:2], dtype=np.int32)\n    if ignore_mask is not None:\n        occupied[ignore_mask == 1] = 1\n\n    pmap = proposals_map.copy()\n    if sampling_mask is not None:\n        pmap *= sampling_mask\n        occupied[sampling_mask == 0] = 1\n\n    if fl_blur > 1:\n        pmap = cv2.blur(pmap, (fl_blur, fl_blur))\n    colors, candidates = find_local_maxima(pmap, prob_thresh=fl_prob, eps=fl_eps, step_size=fl_step)\n\n    for i in range(max_iters):\n        if i >= len(candidates):\n            break\n\n        best_point = candidates[i]\n        if occupied[best_point]:\n            continue\n\n        p = pmodel.get_instance_masks(features[0], [best_point], states=point_invariant_states[0],\n                            width=image_shape[1], height=image_shape[0])[0]\n        if use_flip:\n            flipped_point = (best_point[0], output_width - best_point[1])\n            p_flipped = pmodel.get_instance_masks(features[1], [flipped_point], states=point_invariant_states[1],\n                                        width=image_shape[1], height=image_shape[0])[0]\n            p = 0.5 * (p + p_flipped[:, ::-1])\n\n        if cut_radius > 0:\n            p = cut_prob_with_radius(p, best_point, cut_radius)\n\n        obj_mask = p > thresh1\n        obj_area = obj_mask.sum()\n        if obj_area < 10:\n            continue\n\n        inter_score = occupied[obj_mask].sum() / obj_area\n        if inter_score >= thresh2:\n            continue\n\n        masks.append(p)\n        occupied[obj_mask == 1] = 1\n        kernel = np.ones((3, 3), np.uint8)\n        obj_mask = cv2.dilate(obj_mask.astype(np.uint8), kernel, iterations=2)\n        proposals_map[obj_mask == 1] = 0\n\n    return masks, (pmap, colors, candidates)\n\n\ndef predict_instances_random(pmodel, features, instances_prob, ignore_mask, image_shape,\n                         thresh1=0.5, thresh2=0.6,\n                         num_candidates=5, num_iters=40,\n                         cut_radius=-1, use_flip=False,\n                         ithresh=0.5):\n    point_invariant_states = pmodel.get_point_invariant_states(features)\n    output_height, output_width = image_shape[:2]\n\n    if ignore_mask is not None:\n        instances_prob = instances_prob * (1 - ignore_mask)\n\n    result_map = np.full((output_height, output_width), -1, dtype=np.int)\n    result_map[instances_prob.max(axis=0) < ithresh] = 0\n\n    last_color = 1\n    masks = []\n    for i in range(num_iters):\n        if last_color == 0:\n            points = get_random_points(output_height, output_width, num_candidates)\n        else:\n            points = sample_points_with_mask(result_map == -1, num_candidates)\n            if points is None:\n                break\n\n        pmaps = pmodel.get_instance_masks(features[0], points, states=point_invariant_states[0],\n                                width=output_width, height=output_height)\n        if use_flip:\n            flipped_points = points.copy()\n            flipped_points[:, 1] = output_width - flipped_points[:, 1]\n            pmaps_flipped = pmodel.get_instance_masks(features[1], flipped_points, states=point_invariant_states[1],\n                                            width=output_width, height=output_height)\n\n            pmaps = 0.5 * (pmaps + pmaps_flipped[:, :, ::-1])\n\n        if last_color == 0:\n            best_point_id = pmaps.mean(axis=(1, 2)).argmax()\n        else:\n            tmp = [get_map_score(x) for x in pmaps]\n            best_point_id = np.argmax(tmp)\n\n        pmap = pmaps[best_point_id]\n        if cut_radius > 0:\n            pmap = cut_prob_with_radius(pmap, points[best_point_id], cut_radius)\n\n        pmap_mask = pmap > thresh1\n        pmap_area = pmap_mask.sum()\n\n        uncovered_area = (result_map[pmap_mask] == -1).sum()\n\n        if uncovered_area / pmap_area < thresh2:\n            continue\n\n        result_map[pmap_mask] = last_color\n\n        last_color += 1\n        masks.append(pmap)\n\n    return masks, None\n\n\ndef get_random_points(width, height, num_points):\n    points = poisson_disc_samples(width=width, height=height, r=10)\n    random.shuffle(points)\n    return np.round(points[:num_points])\n\n\ndef get_map_score(prob_map, thresh=0.2):\n    mask = prob_map > thresh\n\n    if mask.sum() > 0:\n        return prob_map[mask].mean()\n    else:\n        return 0\n\n\ndef sample_points_with_mask(mask, num_points):\n    possible_points = np.where(mask)\n    num_possible_points = possible_points[0].shape[0]\n\n    if num_possible_points == 0:\n        return None\n\n    rindx = random.sample(list(range(num_possible_points)),\n                          k=min(num_points, num_possible_points))\n    points = []\n    for j in rindx:\n        points.append((possible_points[0][j], possible_points[1][j]))\n    points = np.array(points)\n\n    return points\n\n\ndef cut_prob_with_radius(prob, p, radius):\n    mask = np.zeros_like(prob, dtype=np.float32)\n\n    mask[max(p[0] - radius, 0):min(p[0] + radius, mask.shape[0]),\n    max(p[1] - radius, 0):min(p[1] + radius, mask.shape[1])] = 1\n\n    return prob * mask\n\n\ndef get_offsets(length, crop_size, min_overlap_ratio=0.2):\n    if length == crop_size:\n        return [0], 1.0\n\n    N = (length / crop_size - min_overlap_ratio) / (1 - min_overlap_ratio)\n    N = math.ceil(N)\n\n    overlap_ratio = (N - length / crop_size) / (N - 1)\n    overlap_width = int(crop_size * overlap_ratio)\n\n    offsets = [0]\n    for i in range(1, N):\n        new_offset = offsets[-1] + crop_size - overlap_width\n        if new_offset + crop_size > length:\n            new_offset = length - crop_size\n\n        offsets.append(new_offset)\n\n    return offsets, overlap_ratio\n'"
adaptis/inference/prediction_model.py,0,"b'import mxnet as mx\nimport numpy as np\nfrom mxnet.gluon.data.vision import transforms\n\n\nclass AdaptISPrediction(object):\n    def __init__(self, net, dataset,\n                 input_transforms=None):\n\n        if input_transforms is None:\n            input_transforms = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n            ])\n        self.input_transforms = input_transforms\n\n        self.net = net\n        self.things_offset = dataset.things_offset\n        self.to_dataset_mapping = dataset.to_dataset_mapping\n        self.to_dataset_mapping_list = [-1] * (max(self.to_dataset_mapping.keys()) + 1)\n        for k, v in self.to_dataset_mapping.items():\n            self.to_dataset_mapping_list[k] = v\n\n    def load_parameters(self, weights_path, ctx):\n        self.ctx = ctx\n        self.net.load_parameters(weights_path, ctx=ctx)\n\n    def get_features(self, image, use_flip=False):\n        data = mx.nd.reshape(self.input_transforms(mx.nd.array(image)),\n                             shape=(-4, 1, -1, -2))\n        data = mx.nd.array(data, ctx=self.ctx)\n        features = self.net.feature_extractor(data)\n\n        if use_flip:\n            features_flipped = self.net.feature_extractor(data[:, :, :, ::-1])\n            return [features, features_flipped]\n        else:\n            return [features]\n\n    def get_point_invariant_states(self, features):\n        states = [self.net.adaptis_head.get_point_invariant_states(mx.nd, f[0])\n                  for f in features]\n        return states\n\n    def get_instance_masks(self, features, points, states=None, width=None, height=None):\n        points = np.array(points).reshape((1, -1, 2)).astype(np.float32)\n        points = mx.nd.array(points, ctx=self.ctx, dtype=np.float32)\n\n        if states is not None:\n            out = self.net.adaptis_head.get_instances_maps(mx.nd, points, *states)\n        else:\n            out = self.net.adaptis_head(features[0], points)\n        out = mx.nd.sigmoid(out)\n\n        if (height != out.shape[2] or width != out.shape[3]) and width is not None:\n            out = mx.nd.contrib.BilinearResize2D(out, width=width, height=height)\n\n        return np.squeeze(out.asnumpy(), axis=1)\n\n    def get_semantic_segmentation(self, features, use_flip=False, output_height=None, output_width=None):\n        smap = self._get_semantic_segmentation(features[0], height=output_height, width=output_width)\n\n        if use_flip:\n            smap_flipped = self._get_semantic_segmentation(features[1], height=output_height, width=output_width)\n            smap = 0.5 * (smap + smap_flipped[:, :, :, ::-1])\n\n        instances_prob = smap[0, self.things_offset:, :, :]\n        stuff_prob = smap[0, :self.things_offset, :, :]\n\n        return instances_prob, stuff_prob\n\n    def _get_semantic_segmentation(self, features, width=None, height=None):\n        out = self.net.segmentation_head(*features)\n        out = mx.nd.softmax(out, axis=1)\n\n        if (height != out.shape[2] or width != out.shape[3]) and width is not None:\n            out = mx.nd.contrib.BilinearResize2D(out, width=width, height=height)\n\n        return out.asnumpy()\n\n    def get_proposals_map(self, features, width=None, height=None, power=2):\n        out = self.net.proposals_head(*features)\n\n        if (height != out.shape[2] or width != out.shape[3]) and width is not None:\n            out = mx.nd.contrib.BilinearResize2D(out, width=width, height=height)\n\n        result = mx.nd.sigmoid(out).asnumpy()\n        critic_map = result ** power\n        return critic_map\n'"
adaptis/model/adaptis.py,0,"b""import mxnet as mx\nfrom adaptis.utils.block import NamedHybridBlock\n\n\nclass AdaptIS(NamedHybridBlock):\n    def __init__(self,\n                 feature_extractor,\n                 adaptis_head,\n                 segmentation_head=None,\n                 proposal_head=None,\n                 with_proposals=False,\n                 spatial_scale=1.0):\n        super(AdaptIS, self).__init__()\n\n        self.with_proposals = with_proposals\n        self.spatial_scale = spatial_scale\n        self._image_shape = None\n        object.__setattr__(self, '_proposals_head', proposal_head)\n\n        with self.name_scope():\n            self.feature_extractor = feature_extractor\n            self.adaptis_head = adaptis_head\n            self.segmentation_head = segmentation_head\n\n            if with_proposals:\n                self.add_proposals_head()\n            else:\n                self.proposals_head = None\n\n    def hybrid_forward(self, F, x, points):\n        if hasattr(x, 'shape'):\n            self._image_shape = x.shape[2:]\n\n        backbone_features = self.feature_extractor(x)\n        instance_out = self.adaptis_head(backbone_features[0], points)\n        if self.spatial_scale != 1.0:\n            instance_out = F.contrib.BilinearResize2D(instance_out,\n                                                      height=self._image_shape[0], width=self._image_shape[1])\n\n        outputs = [('instances', instance_out)]\n        if self.segmentation_head is not None:\n            segmentation_out = self.segmentation_head(*backbone_features)\n            if self.spatial_scale != 1.0:\n                segmentation_out = F.contrib.BilinearResize2D(segmentation_out,\n                                                              height=self._image_shape[0], width=self._image_shape[1])\n            outputs.append(('semantic', segmentation_out))\n\n        if self.proposals_head is not None:\n            backbone_features = [x.detach() for x in backbone_features]\n            proposals_out = self.proposals_head(*backbone_features)\n            proposals_out = self.adaptis_head.eqf(proposals_out, points.detach())\n            outputs.append(('proposals', proposals_out))\n        return self.make_named_outputs(outputs)\n\n    def add_proposals_head(self, ctx=None, initializer=mx.init.Xavier()):\n        if getattr(self, 'proposals_head', None) is None:\n            self.proposals_head = self._proposals_head\n            self.with_proposals = True\n\n            if ctx is not None:\n                self.proposals_head.initialize(initializer, ctx=ctx)\n"""
adaptis/model/basic_blocks.py,0,"b""from mxnet import gluon\n\n\nclass SimpleConvController(gluon.HybridBlock):\n    def __init__(self, num_layers, latent_size, norm_layer,\n                 kernel_size=1, activation='relu'):\n        super(SimpleConvController, self).__init__()\n\n        self.return_map = True\n        with self.name_scope():\n            self.layers = gluon.nn.HybridSequential()\n            for i in range(num_layers):\n                self.layers.add(\n                    gluon.nn.Conv2D(latent_size, kernel_size, activation=activation),\n                    norm_layer()\n                )\n\n    def hybrid_forward(self, F, features):\n        return self.layers(features)\n\n\nclass FCController(gluon.HybridBlock):\n    def __init__(self, layers_sizes, activation='relu',\n                 norm_layer=gluon.nn.BatchNorm):\n        super(FCController, self).__init__()\n\n        self.return_map = False\n        with self.name_scope():\n            fc_net = gluon.nn.HybridSequential()\n            for i, num_units in enumerate(layers_sizes):\n                fc_net.add(gluon.nn.Dense(num_units))\n\n                if isinstance(activation, str):\n                    fc_net.add(gluon.nn.Activation(activation))\n                else:\n                    fc_net.add(activation())\n\n                if norm_layer is not None:\n                    fc_net.add(norm_layer())\n\n            self.fc_net = fc_net\n\n    def hybrid_forward(self, F, x):\n        return self.fc_net(x)\n\n\nclass ConvHead(gluon.HybridBlock):\n    def __init__(self, num_outputs, channels=32, num_layers=1,\n                 kernel_size=3, padding=1,\n                 norm_layer=gluon.nn.BatchNorm):\n        super(ConvHead, self).__init__()\n\n        with self.name_scope():\n            self.layers = gluon.nn.HybridSequential()\n            for i in range(num_layers):\n                self.layers.add(\n                    gluon.nn.Conv2D(channels=channels, kernel_size=kernel_size,\n                                    padding=padding, activation='relu'),\n                    norm_layer(in_channels=channels)\n                )\n            self.layers.add(\n                gluon.nn.Conv2D(channels=num_outputs, kernel_size=1, padding=0)\n            )\n\n    def hybrid_forward(self, F, *inputs):\n        x = inputs[0]\n        return self.layers(x)\n\n\nclass SepConvHead(gluon.HybridBlock):\n    def __init__(self, num_outputs, channels, in_channels, num_layers=1,\n                 kernel_size=3, padding=1, dropout_ratio=0.0, dropout_indx=0,\n                 norm_layer=gluon.nn.BatchNorm):\n        super(SepConvHead, self).__init__()\n\n        with self.name_scope():\n            self.layers = gluon.nn.HybridSequential()\n\n            for i in range(num_layers):\n                self.layers.add(\n                    SeparableConv2D(channels,\n                                    in_channels=in_channels if i == 0 else channels,\n                                    dw_kernel=kernel_size, dw_padding=padding,\n                                    norm_layer=norm_layer, activation='relu')\n                )\n                if dropout_ratio > 0 and dropout_indx == i:\n                    self.layers.add(gluon.nn.Dropout(dropout_ratio))\n\n            self.layers.add(\n                gluon.nn.Conv2D(channels=num_outputs, kernel_size=1, padding=0)\n            )\n\n    def hybrid_forward(self, F, *inputs):\n        x = inputs[0]\n\n        return self.layers(x)\n\n\nclass SeparableConv2D(gluon.HybridBlock):\n    def __init__(self, channels, in_channels, dw_kernel, dw_padding, dw_stride=1,\n                 activation=None, use_bias=False, norm_layer=None):\n        super(SeparableConv2D, self).__init__()\n        self.body = gluon.nn.HybridSequential(prefix='')\n        self.body.add(gluon.nn.Conv2D(in_channels, kernel_size=dw_kernel,\n                                strides=dw_stride, padding=dw_padding,\n                                use_bias=use_bias,\n                                groups=in_channels))\n        self.body.add(gluon.nn.Conv2D(channels, kernel_size=1, strides=1, use_bias=use_bias))\n\n        if norm_layer:\n            self.body.add(norm_layer())\n        if activation:\n            self.body.add(gluon.nn.Activation(activation))\n\n    def hybrid_forward(self, F, x):\n        return self.body(x)\n"""
adaptis/model/losses.py,0,"b""import numpy as np\nimport mxnet as mx\nfrom mxnet import gluon\nfrom mxnet import nd\nfrom mxnet.gluon.loss import Loss, _apply_weighting, _reshape_like\nfrom adaptis.model.metrics import AdaptiveIoU, _compute_iou\n\n\nclass NormalizedFocalLossSoftmax(Loss):\n    def __init__(self, sparse_label=True, batch_axis=0, ignore_label=-1,\n                 size_average=True, detach_delimeter=True, gamma=2, eps=1e-10, **kwargs):\n        super(NormalizedFocalLossSoftmax, self).__init__(None, batch_axis, **kwargs)\n        self._sparse_label = sparse_label\n        self._ignore_label = ignore_label\n        self._size_average = size_average\n        self._detach_delimeter = detach_delimeter\n        self._eps = eps\n        self._gamma = gamma\n        self._k_sum = 0\n\n    def hybrid_forward(self, F, pred, label):\n        label = F.expand_dims(label, axis=1)\n        softmaxout = F.softmax(pred, axis=1)\n\n        t = label != self._ignore_label\n        pt = F.pick(softmaxout, label, axis=1, keepdims=True)\n        pt = F.where(t, pt, F.ones_like(pt))\n        beta = (1 - pt) ** self._gamma\n\n        t_sum = F.cast(F.sum(t, axis=(-2, -1), keepdims=True), 'float32')\n        beta_sum = F.sum(beta, axis=(-2, -1), keepdims=True)\n        mult = t_sum / (beta_sum + self._eps)\n        if self._detach_delimeter:\n            mult = mult.detach()\n        beta = F.broadcast_mul(beta, mult)\n        self._k_sum = 0.9 * self._k_sum + 0.1 * mult.asnumpy().mean()\n\n        loss = -beta * F.log(F.minimum(pt + self._eps, 1))\n\n        if self._size_average:\n            bsum = F.sum(t_sum, axis=self._batch_axis, exclude=True)\n            loss = F.sum(loss, axis=self._batch_axis, exclude=True) / (bsum + self._eps)\n        else:\n            loss = F.sum(loss, axis=self._batch_axis, exclude=True)\n\n        return loss\n\n    def log_states(self, sw, name, global_step):\n        sw.add_scalar(tag=name + '_k', value=self._k_sum, global_step=global_step)\n\n\nclass NormalizedFocalLossSigmoid(gluon.loss.Loss):\n    def __init__(self, axis=-1, alpha=0.25, gamma=2,\n                 from_logits=False, batch_axis=0,\n                 weight=None, size_average=True, detach_delimeter=True,\n                 eps=1e-12, scale=1.0,\n                 ignore_label=-1, **kwargs):\n        super(NormalizedFocalLossSigmoid, self).__init__(weight, batch_axis, **kwargs)\n        self._axis = axis\n        self._alpha = alpha\n        self._gamma = gamma\n        self._ignore_label = ignore_label\n\n        self._scale = scale\n        self._from_logits = from_logits\n        self._eps = eps\n        self._size_average = size_average\n        self._detach_delimeter = detach_delimeter\n        self._k_sum = 0\n\n    def hybrid_forward(self, F, pred, label, sample_weight=None):\n        one_hot = label > 0\n        t = F.ones_like(one_hot)\n\n        if not self._from_logits:\n            pred = F.sigmoid(pred)\n\n        alpha = F.where(one_hot, self._alpha * t, (1 - self._alpha) * t)\n        pt = F.where(one_hot, pred, 1 - pred)\n        pt = F.where(label != self._ignore_label, pt, F.ones_like(pt))\n\n        beta = (1 - pt) ** self._gamma\n\n        t_sum = F.sum(t, axis=(-2, -1), keepdims=True)\n        beta_sum = F.sum(beta, axis=(-2, -1), keepdims=True)\n        mult = t_sum / (beta_sum + self._eps)\n        if self._detach_delimeter:\n            mult = mult.detach()\n        beta = F.broadcast_mul(beta, mult)\n\n        ignore_area = F.sum(label == -1, axis=0, exclude=True).asnumpy()\n        sample_mult = F.mean(mult, axis=0, exclude=True).asnumpy()\n        if np.any(ignore_area == 0):\n            self._k_sum = 0.9 * self._k_sum + 0.1 * sample_mult[ignore_area == 0].mean()\n\n        loss = -alpha * beta * F.log(F.minimum(pt + self._eps, 1))\n        sample_weight = label != self._ignore_label\n\n        loss = _apply_weighting(F, loss, self._weight, sample_weight)\n        if self._size_average:\n            bsum = F.sum(sample_weight, axis=self._batch_axis, exclude=True)\n            loss = F.sum(loss, axis=self._batch_axis, exclude=True) / (bsum + self._eps)\n        else:\n            loss = F.sum(loss, axis=self._batch_axis, exclude=True)\n\n        return self._scale * loss\n\n    def log_states(self, sw, name, global_step):\n        sw.add_scalar(tag=name + '_k', value=self._k_sum, global_step=global_step)\n\n\nclass FocalLoss(gluon.loss.Loss):\n    def __init__(self, axis=-1, alpha=0.25, gamma=2,\n                 from_logits=False, batch_axis=0,\n                 weight=None, num_class=None,\n                 eps=1e-9, size_average=True, scale=1.0, **kwargs):\n        super(FocalLoss, self).__init__(weight, batch_axis, **kwargs)\n        self._axis = axis\n        self._alpha = alpha\n        self._gamma = gamma\n\n        self._scale = scale\n        self._num_class = num_class\n        self._from_logits = from_logits\n        self._eps = eps\n        self._size_average = size_average\n\n    def hybrid_forward(self, F, pred, label, sample_weight=None):\n        if not self._from_logits:\n            pred = F.sigmoid(pred)\n\n        one_hot = label > 0\n        pt = F.where(one_hot, pred, 1 - pred)\n\n        t = label != -1\n        alpha = F.where(one_hot, self._alpha * t, (1 - self._alpha) * t)\n        beta = (1 - pt) ** self._gamma\n\n        loss = -alpha * beta * F.log(F.minimum(pt + self._eps, 1))\n        sample_weight = label != -1\n\n        loss = _apply_weighting(F, loss, self._weight, sample_weight)\n        if self._size_average:\n            tsum = F.sum(label == 1, axis=self._batch_axis, exclude=True)\n            loss = F.sum(loss, axis=self._batch_axis, exclude=True) / (tsum + self._eps)\n        else:\n            loss = F.sum(loss, axis=self._batch_axis, exclude=True)\n\n        return self._scale * loss\n\n\nclass SoftmaxCrossEntropyLoss(Loss):\n    def __init__(self, sparse_label=True, batch_axis=0, ignore_label=-1,\n                 size_average=True, grad_scale=1.0, **kwargs):\n        super(SoftmaxCrossEntropyLoss, self).__init__(None, batch_axis, **kwargs)\n        self._sparse_label = sparse_label\n        self._ignore_label = ignore_label\n        self._size_average = size_average\n        self._grad_scale = grad_scale\n\n    def hybrid_forward(self, F, pred, label):\n        softmaxout = F.SoftmaxOutput(\n            pred, label.astype(pred.dtype), ignore_label=self._ignore_label,\n            multi_output=self._sparse_label,\n            use_ignore=True, normalization='valid' if self._size_average else 'null',\n            grad_scale=self._grad_scale,\n        )\n        loss = -F.pick(F.log(softmaxout), label, axis=1, keepdims=True)\n        loss = F.where(label.expand_dims(axis=1) == self._ignore_label,\n                       F.zeros_like(loss), loss)\n        return F.mean(loss, axis=self._batch_axis, exclude=True)\n\n\nclass SigmoidBinaryCrossEntropyLoss(Loss):\n    def __init__(self, from_sigmoid=False, weight=None, batch_axis=0, ignore_label=-1, **kwargs):\n        super(SigmoidBinaryCrossEntropyLoss, self).__init__(\n            weight, batch_axis, **kwargs)\n        self._from_sigmoid = from_sigmoid\n        self._ignore_label = ignore_label\n\n    def hybrid_forward(self, F, pred, label):\n        label = _reshape_like(F, label, pred)\n        sample_weight = label != self._ignore_label\n        label = F.where(sample_weight, label, F.zeros_like(label))\n\n        if not self._from_sigmoid:\n            loss = F.relu(pred) - pred * label + \\\n                F.Activation(-F.abs(pred), act_type='softrelu')\n        else:\n            eps = 1e-12\n            loss = -(F.log(pred + eps) * label\n                     + F.log(1. - pred + eps) * (1. - label))\n\n        loss = _apply_weighting(F, loss, self._weight, sample_weight)\n        return F.mean(loss, axis=self._batch_axis, exclude=True)\n\n\nclass AdaptISProposalsLossIoU(gluon.HybridBlock):\n    def __init__(self, batch_size, bad_percentile=30, init_iou_thresh=0.35,\n                 from_logits=True, ignore_label=-1):\n        super(AdaptISProposalsLossIoU, self).__init__()\n        self._batch_size = batch_size\n        self._bad_percentile = bad_percentile\n        self._from_logits = from_logits\n        self._iou_metric = AdaptiveIoU(init_thresh=init_iou_thresh,\n                                       from_logits=False, ignore_label=ignore_label)\n        self._ignore_label = ignore_label\n        self._ce_loss = SigmoidBinaryCrossEntropyLoss()\n\n    def hybrid_forward(self, F, pred_inst_maps, pred_proposals, gt_inst_masks):\n        with mx.autograd.pause():\n            if self._from_logits:\n                pred_inst_maps = mx.nd.sigmoid(pred_inst_maps)\n\n            self._iou_metric.update(pred_inst_maps, gt_inst_masks)\n            pred_masks = pred_inst_maps > self._iou_metric.iou_thresh\n\n            batch_iou = _compute_iou(pred_masks, gt_inst_masks > 0, gt_inst_masks == self._ignore_label,\n                                     keep_ignore=True)\n            batch_iou = batch_iou.reshape((self._batch_size, -1))\n\n            prob_score = mx.nd.sum(pred_inst_maps * gt_inst_masks, axis=0, exclude=True)\n            prob_score = prob_score / mx.nd.maximum(mx.nd.sum(gt_inst_masks, axis=0, exclude=True), 1)\n            prob_score = prob_score.asnumpy().reshape((self._batch_size, -1))\n\n        labels = []\n        for i in range(self._batch_size):\n            obj_iou = batch_iou[i]\n            if obj_iou.min() < 0:\n                labels.append([-1] * len(obj_iou))\n                continue\n\n            obj_score = obj_iou * prob_score[i]\n            if obj_iou.max() - obj_iou.min() > 0.1:\n                th = np.percentile(obj_score, self._bad_percentile) * 0.95\n            else:\n                th = 0.99 * obj_score.min()\n\n            good_points = obj_score > th\n            gp_min_score = obj_score[good_points].min()\n            gp_max_score = obj_score[good_points].max()\n\n            obj_labels = (obj_score > th).astype(np.float32)\n            if gp_max_score - gp_min_score > 1e-3:\n                prob = (obj_score[good_points] - gp_min_score) / (gp_max_score - gp_min_score)\n                obj_labels[good_points] = 0.7 + 0.3 * prob\n            labels.append(obj_labels.tolist())\n\n        labels = np.array(labels)\n        labels = nd.array(labels).as_in_context(pred_proposals.context)\n\n        return self._ce_loss(pred_proposals, labels)\n\n    def log_states(self, sw, name, global_step):\n        sw.add_scalar(tag=name + '_iou_thresh', value=self._iou_metric.iou_thresh, global_step=global_step)\n"""
adaptis/model/metrics.py,0,"b""import mxnet as mx\nimport numpy as np\n\n\nclass TrainMetric(object):\n    def __init__(self, pred_outputs, gt_outputs):\n        self.pred_outputs = pred_outputs\n        self.gt_outputs = gt_outputs\n\n    def update(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def get_epoch_value(self):\n        raise NotImplementedError\n\n    def reset_epoch_stats(self):\n        raise NotImplementedError\n\n    def log_states(self, sw, tag_prefix, global_step):\n        pass\n\n    @property\n    def name(self):\n        return type(self).__name__\n\n\nclass AdaptiveIoU(TrainMetric):\n    def __init__(self, init_thresh=0.4, thresh_step=0.025, thresh_beta=0.99, iou_beta=0.9,\n                 ignore_label=-1, from_logits=True,\n                 pred_output='instances', gt_output='instances'):\n        super().__init__(pred_outputs=(pred_output,), gt_outputs=(gt_output,))\n        self._ignore_label = ignore_label\n        self._from_logits = from_logits\n        self._iou_thresh = init_thresh\n        self._thresh_step = thresh_step\n        self._thresh_beta = thresh_beta\n        self._iou_beta = iou_beta\n        self._ema_iou = 0.0\n        self._epoch_iou_sum = 0.0\n        self._epoch_batch_count = 0\n\n    def update(self, pred, gt):\n        gt_mask = gt > 0\n        if self._from_logits:\n            pred = mx.nd.sigmoid(pred)\n\n        gt_mask_area = mx.nd.sum(gt_mask, axis=0, exclude=True).asnumpy()\n        if np.all(gt_mask_area == 0):\n            return\n\n        ignore_mask = gt == self._ignore_label\n        max_iou = _compute_iou(pred > self._iou_thresh, gt_mask, ignore_mask).mean()\n        best_thresh = self._iou_thresh\n        for t in [best_thresh - self._thresh_step, best_thresh + self._thresh_step]:\n            temp_iou = _compute_iou(pred > t, gt_mask, ignore_mask).mean()\n            if temp_iou > max_iou:\n                max_iou = temp_iou\n                best_thresh = t\n\n        self._iou_thresh = self._thresh_beta * self._iou_thresh + (1 - self._thresh_beta) * best_thresh\n        self._ema_iou = self._iou_beta * self._ema_iou + (1 - self._iou_beta) * max_iou\n        self._epoch_iou_sum += max_iou\n        self._epoch_batch_count += 1\n\n    def get_epoch_value(self):\n        if self._epoch_batch_count > 0:\n            return self._epoch_iou_sum / self._epoch_batch_count\n        else:\n            return 0.0\n\n    def reset_epoch_stats(self):\n        self._epoch_iou_sum = 0.0\n        self._epoch_batch_count = 0\n\n    def log_states(self, sw, tag_prefix, global_step):\n        sw.add_scalar(tag=tag_prefix + '_ema_iou', value=self._ema_iou, global_step=global_step)\n        sw.add_scalar(tag=tag_prefix + '_iou_thresh', value=self._iou_thresh, global_step=global_step)\n\n    @property\n    def iou_thresh(self):\n        return self._iou_thresh\n\n\ndef _compute_iou(pred_mask, gt_mask, ignore_mask, keep_ignore=False):\n    pred_mask = mx.nd.where(ignore_mask, mx.nd.zeros_like(pred_mask), pred_mask)\n    union = mx.nd.mean(mx.nd.logical_or(pred_mask, gt_mask), axis=0, exclude=True).asnumpy()\n    intersection = mx.nd.mean(mx.nd.logical_and(pred_mask, gt_mask), axis=0, exclude=True).asnumpy()\n    nonzero = union > 0\n\n    iou = intersection[nonzero] / union[nonzero]\n    if not keep_ignore:\n        return iou\n    else:\n        result = np.full_like(intersection, -1)\n        result[nonzero] = iou\n        return result\n\n"""
adaptis/model/ops.py,0,"b'import mxnet as mx\nfrom mxnet import gluon\n\n\nclass AdaIN(gluon.HybridBlock):\n    def __init__(self, channels, norm=True):\n        super(AdaIN, self).__init__()\n        self.channels = channels\n        self.norm = norm\n\n        with self.name_scope():\n            self.affine_scale = gluon.nn.Dense(channels,\n                                               bias_initializer=mx.init.Constant(1),\n                                               use_bias=True, flatten=True)\n            self.affine_bias = gluon.nn.Dense(channels,\n                                             use_bias=True, flatten=True)\n\n    def hybrid_forward(self, F, x, w):\n        ys = self.affine_scale(w)\n        yb = self.affine_bias(w)\n        ys = F.expand_dims(ys, axis=2)\n        yb = F.expand_dims(yb, axis=2)\n\n        xm = F.reshape(x, shape=(0, 0, -1))  # (N,C,H,W) --> (N,C,K)\n        if self.norm:\n            xm_mean = F.mean(xm, axis=2, keepdims=True)\n            xm_centered = F.broadcast_minus(xm, xm_mean)\n            xm_std_rev = F.rsqrt(F.mean(F.square(xm_centered), axis=2, keepdims=True))  # 1 / std (rsqrt, not sqrt)\n            xm_norm = F.broadcast_mul(xm_centered, xm_std_rev)\n        else:\n            xm_norm = xm\n\n        xm_scaled = F.broadcast_plus(F.broadcast_mul(xm_norm, ys), yb)\n        return F.reshape_like(xm_scaled, x)\n\n\nclass AppendCoordFeatures(gluon.HybridBlock):\n    def __init__(self, norm_radius, append_dist=True, spatial_scale=1.0):\n        super(AppendCoordFeatures, self).__init__()\n        self.xs = None\n        self.spatial_scale = spatial_scale\n        self.norm_radius = norm_radius\n        self.append_dist = append_dist\n\n    def _ctx_kwarg(self, x):\n        if isinstance(x, mx.nd.NDArray):\n            return {""ctx"": x.context}\n        return {}\n\n    def get_coord_features(self, F, points, rows, cols, batch_size,\n                           **ctx_kwarg):\n        row_array = F.arange(start=0, stop=rows, step=1, **ctx_kwarg)\n        col_array = F.arange(start=0, stop=cols, step=1, **ctx_kwarg)\n        coord_rows = F.repeat(F.reshape(row_array, (1, 1, rows, 1)), repeats=cols, axis=3)\n        coord_cols = F.repeat(F.reshape(col_array, (1, 1, 1, cols)), repeats=rows, axis=2)\n\n        coord_rows = F.repeat(coord_rows, repeats=batch_size, axis=0)\n        coord_cols = F.repeat(coord_cols, repeats=batch_size, axis=0)\n\n        coords = F.concat(coord_rows, coord_cols, dim=1)\n\n        add_xy = F.reshape(points * self.spatial_scale, shape=(0, 0, 1))\n        add_xy = F.reshape(F.repeat(add_xy, rows * cols, axis=2),\n                           shape=(0, 0, rows, cols))\n\n        coords = (coords - add_xy) / (self.norm_radius * self.spatial_scale)\n        if self.append_dist:\n            dist = F.sqrt(F.sum(F.square(coords), axis=1, keepdims=1))\n            coord_features = F.concat(coords, dist, dim=1)\n        else:\n            coord_features = coords\n\n        coord_features = F.clip(coord_features, a_min=-1, a_max=1)\n        return coord_features\n\n    def hybrid_forward(self, F, x, coords):\n        if isinstance(x, mx.nd.NDArray):\n            self.xs = x.shape\n\n        batch_size, rows, cols = self.xs[0], self.xs[2], self.xs[3]\n        coord_features = self.get_coord_features(F, coords, rows, cols, batch_size,\n                                                 **self._ctx_kwarg(x))\n        return F.concat(coord_features, x, dim=1)\n\n\nclass ExtractQueryFeatures(gluon.HybridBlock):\n    def __init__(self, extraction_method=\'ROIPooling\', spatial_scale=1.0, eps=1e-4):\n        super(ExtractQueryFeatures, self).__init__()\n        self.cshape = None\n        self.extraction_method = extraction_method\n        self.spatial_scale = spatial_scale\n        self.eps = eps\n\n    def _ctx_kwarg(self, x):\n        if isinstance(x, mx.nd.NDArray):\n            return {""ctx"": x.context}\n        return {}\n\n    def hybrid_forward(self, F, x, coords):\n        if isinstance(coords, mx.nd.NDArray):\n            self.cshape = coords.shape\n\n        batch_size, num_points = self.cshape[0], self.cshape[1]\n        ctx_kwarg = self._ctx_kwarg(coords)\n\n        coords = F.reverse(F.reshape(coords, shape=(-1, 2)), axis=1)\n        if self.extraction_method == \'ROIAlign\':\n            coords = coords - 0.5 / self.spatial_scale\n            coords2 = coords\n        else:\n            coords2 = coords\n        rois = F.concat(coords, coords2, dim=1)\n\n        bi = F.arange(0, batch_size, **ctx_kwarg)\n        bi = F.repeat(bi, num_points, axis=0)\n        bi = F.reshape(bi, shape=(-1, 1))\n        rois = F.concat(bi, rois)\n\n        if self.extraction_method == \'ROIPooling\':\n            w = F.ROIPooling(x, rois,\n                             pooled_size=(1, 1), spatial_scale=self.spatial_scale)\n        elif self.extraction_method == \'ROIAlign\':\n            w = F.contrib.ROIAlign(x, rois,\n                             pooled_size=(1, 1), spatial_scale=self.spatial_scale)\n        else:\n            assert False\n\n        w = F.reshape(w, shape=(0, -1))\n\n        return w\n\n\nclass AverageMaskedFeatures(gluon.HybridBlock):\n    def __init__(self, spatial_scale=1.0):\n        super(AverageMaskedFeatures, self).__init__()\n        self.xshape = None\n        self.mshape = None\n        self.spatial_scale = spatial_scale\n\n    def hybrid_forward(self, F, x, masks):\n        if isinstance(masks, mx.nd.NDArray):\n            self.xshape = x.shape\n            self.mshape = masks.shape\n\n        batch_size = self.xshape[0]\n        num_masks = self.mshape[0]\n        num_points = num_masks // batch_size\n\n        masks = F.expand_dims(masks, axis=1)\n        if self.spatial_scale < 1.0:\n            masks = mx.nd.contrib.BilinearResize2D(masks,\n                                                   height=self.xshape[2], width=self.xshape[3])\n\n        xr = F.repeat(x, num_points, axis=0)\n        xr = F.broadcast_mul(xr, masks)\n\n        ws = F.sum(xr, axis=(0, 1), exclude=True)\n        masks_sum = F.sum(masks, axis=(0, 1), exclude=True) + 1e-6\n        w = F.broadcast_div(ws, masks_sum)\n\n        return w\n'"
adaptis/utils/args.py,0,"b'import mxnet as mx\nimport argparse\n\n\ndef get_common_arguments():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--workers\', type=int, default=4,\n                        metavar=\'N\', help=\'Dataloader threads\')\n\n    parser.add_argument(\'--thread-pool\', action=\'store_true\', default=False,\n                        help=\'use ThreadPool for dataloader workers\')\n\n    parser.add_argument(\'--ngpus\', type=int,\n                        default=len(mx.test_utils.list_gpus()),\n                        help=\'number of GPUs\')\n    parser.add_argument(\'--gpus\', type=str, default=\'\', required=False)\n\n    parser.add_argument(\'--kvstore\', type=str, default=\'device\',\n                        help=\'kvstore to use for trainer/module.\')\n\n    parser.add_argument(\'--dtype\', type=str, default=\'float32\',\n                        help=\'data type for training. default is float32\')\n\n    parser.add_argument(\'--batch-size\', type=int, default=8)\n\n    parser.add_argument(\'--exp-name\', type=str, default=\'\',\n                        help=\'experiment name\')\n\n    return parser\n\n\ndef get_train_arguments():\n    parser = get_common_arguments()\n    parser.add_argument(\'--start-epoch\', type=int, default=0,\n                        help=\'Start epoch for learning schedule and for logging\')\n\n    parser.add_argument(\'--weights\', type=str, default=None,\n                        help=\'Put the path to resuming file if needed\')\n\n    parser.add_argument(\'--val-batch-size\', type=int, default=8)\n\n    parser.add_argument(\'--no-exp\', action=\'store_true\', default=False,\n                        help=""Don\'t create exps dir"")\n\n    return parser\n'"
adaptis/utils/block.py,0,"b""from mxnet import gluon\nfrom collections import namedtuple\n\n\nclass NamedHybridBlock(gluon.HybridBlock):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n        self._outputs_type = None\n\n    def make_named_outputs(self, outputs):\n        keys = [x[0] for x in outputs]\n        values = [x[1] for x in outputs]\n\n        named_outputs = namedtuple('outputs', keys)(*values)\n        self._outputs_type = type(named_outputs)\n        return named_outputs\n\n    def __call__(self, *args):\n        out = super().__call__(*args)\n        if not isinstance(out, self._outputs_type):\n            out = self._outputs_type(*out)\n        return out\n"""
adaptis/utils/exp.py,0,"b""import logging\nimport mxnet as mx\nfrom datetime import datetime\nfrom pathlib import Path\nimport shutil\n\nfrom .log import logger\nfrom .args import get_train_arguments\n\n\ndef init_experiment(experiment_name, add_exp_args, script_path=None):\n    parser = get_train_arguments()\n    parser = add_exp_args(parser)\n    args = parser.parse_args()\n\n    experiments_path = Path('./experiments') / experiment_name\n    experiments_path.mkdir(parents=True, exist_ok=True)\n\n    exp_indx = find_last_exp_indx(experiments_path)\n    experiment_name = f'{exp_indx:03d}'\n    if args.exp_name:\n        experiment_name += f'_{args.exp_name}'\n\n    experiment_path = experiments_path / experiment_name\n\n    args.logs_path = experiment_path / 'logs'\n    args.run_path = experiment_path\n    args.checkpoints_path = experiment_path / 'checkpoints'\n\n    experiment_path.mkdir(parents=True)\n    if script_path is not None:\n        temp_script_name = Path(script_path).stem + datetime.strftime(datetime.today(), '_%Y-%m-%d_%H-%M-%S.py')\n        shutil.copy(script_path, experiment_path / temp_script_name)\n\n    if not args.checkpoints_path.exists():\n        args.checkpoints_path.mkdir(parents=True)\n    if not args.logs_path.exists():\n        args.logs_path.mkdir(parents=True)\n\n    stdout_log_path = args.logs_path / 'train_log.txt'\n\n    if stdout_log_path is not None:\n        fh = logging.FileHandler(str(stdout_log_path))\n        formatter = logging.Formatter(fmt='(%(levelname)s) %(asctime)s: %(message)s',\n                                      datefmt='%Y-%m-%d %H:%M:%S')\n        fh.setFormatter(formatter)\n        logger.addHandler(fh)\n\n    if args.gpus:\n        args.ctx = [mx.gpu(int(i)) for i in args.gpus.split(',')]\n        args.ngpus = len(args.ctx)\n    else:\n        args.ctx = [mx.gpu(i) for i in range(args.ngpus)]\n    logger.info(f'Number of GPUs: {args.ngpus}')\n\n    if args.ngpus < 2:\n        args.syncbn = False\n\n    logger.info(args)\n\n    return args\n\n\ndef find_last_exp_indx(exp_parent_path):\n    indx = 0\n    for x in exp_parent_path.iterdir():\n        if not x.is_dir():\n            continue\n\n        exp_name = x.stem\n        if exp_name[:3].isnumeric():\n            indx = max(indx, int(exp_name[:3]) + 1)\n\n    return indx\n"""
adaptis/utils/log.py,0,"b""import io\nimport time\nimport logging\nfrom mxboard import SummaryWriter\n\nLOGGER_NAME = 'root'\n\nhandler = logging.StreamHandler()\n\nlogger = logging.getLogger(LOGGER_NAME)\nlogger.setLevel(logging.INFO)\nlogger.addHandler(handler)\n\n\nclass TqdmToLogger(io.StringIO):\n    logger = None\n    level = None\n    buf = ''\n\n    def __init__(self, logger, level=None, mininterval=5):\n        super(TqdmToLogger, self).__init__()\n        self.logger = logger\n        self.level = level or logging.INFO\n        self.mininterval = mininterval\n        self.last_time = 0\n\n    def write(self, buf):\n        self.buf = buf.strip('\\r\\n\\t ')\n \n    def flush(self):\n        if len(self.buf) > 0 and time.time() - self.last_time > self.mininterval:\n            self.logger.log(self.level, self.buf)\n            self.last_time = time.time()\n\n\nclass SummaryWriterAvg(SummaryWriter):\n    def __init__(self, *args, dump_period=20, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._dump_period = dump_period\n        self._avg_scalars = dict()\n\n    def add_scalar(self, tag, value, global_step=None, disable_avg=False):\n        if disable_avg or isinstance(value, (tuple, list, dict)):\n            super().add_scalar(tag, value, global_step=global_step)\n        else:\n            if tag not in self._avg_scalars:\n                self._avg_scalars[tag] = ScalarAccumulator(self._dump_period)\n            avg_scalar = self._avg_scalars[tag]\n            avg_scalar.add(value)\n\n            if avg_scalar.is_full():\n                super().add_scalar(tag, avg_scalar.value,\n                                   global_step=global_step)\n                avg_scalar.reset()\n\n\nclass ScalarAccumulator(object):\n    def __init__(self, period):\n        self.sum = 0\n        self.cnt = 0\n        self.period = period\n\n    def add(self, value):\n        self.sum += value\n        self.cnt += 1\n\n    @property\n    def value(self):\n        if self.cnt > 0:\n            return self.sum / self.cnt\n        else:\n            return 0\n\n    def reset(self):\n        self.cnt = 0\n        self.sum = 0\n\n    def is_full(self):\n        return self.cnt >= self.period\n\n    def __len__(self):\n        return self.cnt\n"""
adaptis/utils/misc.py,0,"b""import numpy as np\nfrom .log import logger\nfrom functools import partial\nfrom mxnet.gluon.data.dataloader import default_mp_batchify_fn, default_batchify_fn\n\n\ndef save_checkpoint(net, checkpoints_path, epoch=None, prefix='', verbose=True):\n    if epoch is None:\n        checkpoint_name = 'last_checkpoint.params'\n    else:\n        checkpoint_name = f'{epoch:03d}.params'\n\n    if prefix:\n        checkpoint_name = f'{prefix}_{checkpoint_name}'\n\n    if not checkpoints_path.exists():\n        checkpoints_path.mkdir(parents=True)\n\n    checkpoint_path = checkpoints_path / checkpoint_name\n    if verbose:\n        logger.info(f'Save checkpoint to {str(checkpoint_path)}')\n    net.save_parameters(str(checkpoint_path))\n\n\ndef get_unique_labels(mask):\n    return np.nonzero(np.bincount(mask.flatten() + 1))[0] - 1\n\n\ndef get_dict_batchify_fn(num_workers):\n    base_batchify_fn = default_mp_batchify_fn if num_workers > 0 else default_batchify_fn\n\n    return partial(dict_batchify_fn, base_batchify_fn=base_batchify_fn)\n\n\ndef dict_batchify_fn(data, base_batchify_fn):\n    if isinstance(data[0], dict):\n        ret = {k: [] for k in data[0].keys()}\n        for x in data:\n            for k, v in x.items():\n                ret[k].append(v)\n        return {k: base_batchify_fn(v) for k, v in ret.items()}\n    else:\n        return base_batchify_fn(data)\n"""
adaptis/utils/vis.py,0,"b'from functools import lru_cache\nimport numpy as np\nimport cv2\n\n\ndef visualize_instances(imask, bg_color=255,\n                        boundaries_color=None, boundaries_width=1, boundaries_alpha=0.8):\n    num_objects = imask.max() + 1\n    palette = get_palette(num_objects)\n    if bg_color is not None:\n        palette[0] = bg_color\n\n    result = palette[imask].astype(np.uint8)\n    if boundaries_color is not None:\n        boundaries_mask = get_boundaries(imask, boundaries_width=boundaries_width)\n        tresult = result.astype(np.float32)\n        tresult[boundaries_mask] = boundaries_color\n        tresult = tresult * boundaries_alpha + (1 - boundaries_alpha) * result\n        result = tresult.astype(np.uint8)\n\n    return result\n\n\n@lru_cache(maxsize=16)\ndef get_palette(num_cls):\n    palette = np.zeros(3 * num_cls, dtype=np.int32)\n\n    for j in range(0, num_cls):\n        lab = j\n        i = 0\n\n        while lab > 0:\n            palette[j*3 + 0] |= (((lab >> 0) & 1) << (7-i))\n            palette[j*3 + 1] |= (((lab >> 1) & 1) << (7-i))\n            palette[j*3 + 2] |= (((lab >> 2) & 1) << (7-i))\n            i = i + 1\n            lab >>= 3\n\n    return palette.reshape((-1, 3))\n\n\ndef visualize_mask(mask, num_cls):\n    palette = get_palette(num_cls)\n    mask[mask == -1] = 0\n\n    return palette[mask].astype(np.uint8)\n\n\ndef visualize_proposals(proposals_info, point_color=(255, 0, 0), point_radius=1):\n    proposal_map, colors, candidates = proposals_info\n\n    proposal_map = draw_probmap(proposal_map)\n    for x, y in candidates:\n        proposal_map = cv2.circle(proposal_map, (y, x), point_radius, point_color, -1)\n\n    return proposal_map\n\n\ndef draw_probmap(x):\n    return cv2.applyColorMap((x * 255).astype(np.uint8), cv2.COLORMAP_HOT)\n\n\ndef draw_points(image, points, color, radius=3):\n    image = image.copy()\n    for p in points:\n        image = cv2.circle(image, (int(p[1]), int(p[0])), radius, color, -1)\n\n    return image\n\n\ndef draw_instance_map(x, palette=None):\n    num_colors = x.max() + 1\n    if palette is None:\n        palette = get_palette(num_colors)\n\n    return palette[x].astype(np.uint8)\n\n\ndef blend_mask(image, mask, alpha=0.6):\n    if mask.min() == -1:\n        mask = mask.copy() + 1\n\n    imap = draw_instance_map(mask)\n    result = (image * (1 - alpha) + alpha * imap).astype(np.uint8)\n    return result\n\n\ndef get_boundaries(instances_masks, boundaries_width=1):\n    boundaries = np.zeros((instances_masks.shape[0], instances_masks.shape[1]), dtype=np.bool)\n\n    for obj_id in np.unique(instances_masks.flatten()):\n        if obj_id == 0:\n            continue\n\n        obj_mask = instances_masks == obj_id\n        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n        inner_mask = cv2.erode(obj_mask.astype(np.uint8), kernel, iterations=boundaries_width).astype(np.bool)\n\n        obj_boundary = np.logical_xor(obj_mask, np.logical_and(inner_mask, obj_mask))\n        boundaries = np.logical_or(boundaries, obj_boundary)\n    return boundaries\n'"
adaptis/inference/cython_utils/setup.py,0,"b'from distutils.core import setup, Extension\nfrom Cython.Build import cythonize\nimport numpy\n\nextensions = [\n    Extension(""utils"", [""utils.pyx""],\n              include_dirs=[numpy.get_include()],\n              extra_compile_args=[\'-O3\'],\n              language=\'c++11\'),\n]\n\nsetup(\n    ext_modules=cythonize(extensions),\n)'"
adaptis/model/cityscapes/deeplab_v3.py,0,"b'from mxnet import gluon\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.nn import HybridBlock\nfrom adaptis.model.basic_blocks import SeparableConv2D\nfrom .resnet import ResNetBackbone\n\n\nclass DeepLabV3Plus(gluon.HybridBlock):\n    def __init__(self, backbone=\'resnet50\', backbone_lr_mult=0.1, **kwargs):\n        super(DeepLabV3Plus, self).__init__()\n\n        self._c1_shape = None\n        self.backbone_name = backbone\n        self.backbone_lr_mult = backbone_lr_mult\n        self._kwargs = kwargs\n\n        with self.name_scope():\n            self.backbone = ResNetBackbone(backbone=self.backbone_name, pretrained_base=False, **kwargs)\n\n            self.head = _DeepLabHead(256, in_filters=256 + 32, **kwargs)\n            self.skip_project = _SkipProject(32, **kwargs)\n            self.aspp = _ASPP(2048, [12, 24, 36], **kwargs)\n\n    def load_pretrained_weights(self):\n        pretrained = ResNetBackbone(backbone=self.backbone_name, pretrained_base=True, **self._kwargs)\n        backbone_params = self.backbone.collect_params()\n        pretrained_weights = pretrained.collect_params()\n        for k, v in pretrained_weights.items():\n            param_name = backbone_params.prefix + k[len(pretrained_weights.prefix):]\n            backbone_params[param_name].set_data(v.data())\n\n        self.backbone.collect_params().setattr(\'lr_mult\', self.backbone_lr_mult)\n\n    def hybrid_forward(self, F, x):\n        c1, _, c3, c4 = self.backbone(x)\n        c1 = self.skip_project(c1)\n\n        if hasattr(c1, \'shape\'):\n            self._c1_shape = c1.shape\n\n        x = self.aspp(c4)\n        x = F.contrib.BilinearResize2D(x, height=self._c1_shape[2], width=self._c1_shape[3])\n        x = F.concat(x, c1, dim=1)\n        x = self.head(x)\n\n        return x,\n\n\nclass _SkipProject(HybridBlock):\n    def __init__(self, out_channels, norm_layer=nn.BatchNorm):\n        super(_SkipProject, self).__init__()\n\n        with self.name_scope():\n            self.skip_project = nn.HybridSequential()\n            self.skip_project.add(nn.Conv2D(out_channels, kernel_size=1, use_bias=False))\n            self.skip_project.add(norm_layer(in_channels=out_channels))\n            self.skip_project.add(nn.Activation(""relu""))\n\n    def hybrid_forward(self, F, x):\n        return self.skip_project(x)\n\n\nclass _DeepLabHead(HybridBlock):\n    def __init__(self, output_channels, in_filters, norm_layer=nn.BatchNorm):\n        super(_DeepLabHead, self).__init__()\n        with self.name_scope():\n            self.block = nn.HybridSequential()\n\n            self.block.add(SeparableConv2D(256, in_channels=in_filters, dw_kernel=3, dw_padding=1,\n                                           activation=\'relu\', norm_layer=norm_layer))\n            self.block.add(SeparableConv2D(256, in_channels=256, dw_kernel=3, dw_padding=1,\n                                           activation=\'relu\', norm_layer=norm_layer))\n\n            self.block.add(nn.Conv2D(channels=output_channels,\n                                     kernel_size=1))\n\n    def hybrid_forward(self, F, x):\n        return self.block(x)\n\n\nclass _ASPP(nn.HybridBlock):\n    def __init__(self, in_channels, atrous_rates, out_channels=256,\n                 project_dropout=0.5,\n                 norm_layer=nn.BatchNorm):\n        super(_ASPP, self).__init__()\n\n        b0 = nn.HybridSequential()\n        with b0.name_scope():\n            b0.add(nn.Conv2D(in_channels=in_channels, channels=out_channels,\n                             kernel_size=1, use_bias=False))\n            b0.add(norm_layer(in_channels=out_channels))\n            b0.add(nn.Activation(""relu""))\n\n        rate1, rate2, rate3 = tuple(atrous_rates)\n        b1 = _ASPPConv(in_channels, out_channels, rate1, norm_layer)\n        b2 = _ASPPConv(in_channels, out_channels, rate2, norm_layer)\n        b3 = _ASPPConv(in_channels, out_channels, rate3, norm_layer)\n        b4 = _AsppPooling(in_channels, out_channels, norm_layer=norm_layer)\n\n        self.concurent = gluon.contrib.nn.HybridConcurrent(axis=1)\n        with self.concurent.name_scope():\n            self.concurent.add(b0)\n            self.concurent.add(b1)\n            self.concurent.add(b2)\n            self.concurent.add(b3)\n            self.concurent.add(b4)\n\n        self.project = nn.HybridSequential()\n        with self.project.name_scope():\n            self.project.add(nn.Conv2D(in_channels=5*out_channels, channels=out_channels,\n                                       kernel_size=1, use_bias=False))\n            self.project.add(norm_layer(in_channels=out_channels))\n            self.project.add(nn.Activation(""relu""))\n            if project_dropout > 0:\n                self.project.add(nn.Dropout(project_dropout))\n\n    def hybrid_forward(self, F, x):\n        return self.project(self.concurent(x))\n\n\nclass _AsppPooling(nn.HybridBlock):\n    def __init__(self, in_channels, out_channels, norm_layer):\n        super(_AsppPooling, self).__init__()\n        self._out_h = None\n        self._out_w = None\n        self.gap = nn.HybridSequential()\n        with self.gap.name_scope():\n            self.gap.add(nn.GlobalAvgPool2D())\n            self.gap.add(nn.Conv2D(in_channels=in_channels, channels=out_channels,\n                                   kernel_size=1, use_bias=False))\n            self.gap.add(norm_layer(in_channels=out_channels))\n            self.gap.add(nn.Activation(""relu""))\n\n    def hybrid_forward(self, F, x):\n        if hasattr(x, \'shape\'):\n            _, _, h, w = x.shape\n            self._out_h = h\n            self._out_w = w\n        else:\n            h, w = self._out_h, self._out_w\n            assert h is not None\n\n        pool = self.gap(x)\n        return F.contrib.BilinearResize2D(pool, height=h, width=w)\n\n\ndef _ASPPConv(in_channels, out_channels, atrous_rate, norm_layer):\n    block = nn.HybridSequential()\n    with block.name_scope():\n        block.add(nn.Conv2D(in_channels=in_channels, channels=out_channels,\n                            kernel_size=3, padding=atrous_rate,\n                            dilation=atrous_rate, use_bias=False))\n        block.add(norm_layer(in_channels=out_channels))\n        block.add(nn.Activation(\'relu\'))\n    return block\n'"
adaptis/model/cityscapes/models.py,0,"b""import mxnet as mx\nfrom mxnet import gluon\n\nfrom adaptis.model.cityscapes.deeplab_v3 import DeepLabV3Plus\nfrom adaptis.model.adaptis import AdaptIS\nfrom adaptis.model.ops import AdaIN, ExtractQueryFeatures, AppendCoordFeatures\nfrom adaptis.model.basic_blocks import SepConvHead, FCController, SeparableConv2D\nfrom .resnet_fpn import SemanticFPNHead, ResNetFPN\n\n\ndef get_cityscapes_model(num_classes, norm_layer, backbone='resnet50',\n                         with_proposals=False):\n    model = AdaptIS(\n        feature_extractor=DeepLabV3Plus(backbone=backbone, norm_layer=norm_layer),\n        adaptis_head=CityscapesAdaptISHead(\n            FCController(3 * [128], norm_layer=norm_layer),\n            ch=128, norm_radius=280,\n            spatial_scale=1.0/4.0,\n            norm_layer=norm_layer\n        ),\n        segmentation_head=SepConvHead(num_classes, channels=192, in_channels=256, num_layers=2, norm_layer=norm_layer),\n        proposal_head=SepConvHead(1, channels=128, in_channels=256, num_layers=2,\n                                  dropout_ratio=0.5, dropout_indx=0, norm_layer=norm_layer),\n        with_proposals=with_proposals,\n        spatial_scale=1.0/4.0\n    )\n\n    return model\n\n\ndef get_fpn_model(num_classes, norm_layer, backbone='resnet50',\n                         with_proposals=False):\n    model = AdaptIS(\n        feature_extractor=ResNetFPN(backbone=backbone, norm_layer=norm_layer),\n        adaptis_head=CityscapesAdaptISHead(\n            FCController(3 * [128], norm_layer=norm_layer),\n            ch=128, norm_radius=280,\n            spatial_scale=1.0/4.0,\n            norm_layer=norm_layer\n        ),\n        segmentation_head=SemanticFPNHead(num_classes, output_channels=256, norm_layer=norm_layer),\n        proposal_head=SepConvHead(1, channels=128, in_channels=256, num_layers=2,\n                                  dropout_ratio=0.5, dropout_indx=0, norm_layer=norm_layer),\n        with_proposals=with_proposals,\n        spatial_scale=1.0/4.0\n    )\n    return model\n\n\nclass CityscapesAdaptISHead(gluon.HybridBlock):\n    def __init__(self, controller_net, ch=128, norm_radius=190, spatial_scale=0.25,\n                 norm_layer=gluon.nn.BatchNorm):\n        super(CityscapesAdaptISHead, self).__init__()\n\n        self.num_points = None\n        with self.name_scope():\n            self.eqf = ExtractQueryFeatures(extraction_method='ROIAlign', spatial_scale=spatial_scale)\n            self.controller_net = controller_net\n\n            self.add_coord_features = AppendCoordFeatures(norm_radius=norm_radius, spatial_scale=spatial_scale)\n\n            self.block0 = gluon.nn.HybridSequential()\n            self.block0.add(\n                gluon.nn.Conv2D(channels=ch, kernel_size=3, padding=1, activation='relu'),\n                norm_layer(in_channels=ch),\n                SeparableConv2D(ch, in_channels=ch, dw_kernel=3, dw_padding=1,\n                                norm_layer=norm_layer, activation='relu'),\n                gluon.nn.Conv2D(channels=ch, kernel_size=1, padding=0),\n                gluon.nn.LeakyReLU(0.2)\n            )\n\n            self.adain = AdaIN(ch)\n\n            self.block1 = gluon.nn.HybridSequential()\n            for i in range(3):\n                self.block1.add(\n                    SeparableConv2D(ch // (2 ** i), in_channels=min(ch, 2 * ch // (2 ** i)),\n                                    dw_kernel=3, dw_padding=1,\n                                    norm_layer=norm_layer, activation='relu'),\n                )\n            self.block1.add(gluon.nn.Conv2D(channels=1, kernel_size=1))\n\n    def hybrid_forward(self, F, p1_features, points):\n        adaptive_input, controller_input = self.get_point_invariant_states(F, p1_features)\n        return self.get_instances_maps(F, points, adaptive_input, controller_input)\n\n    def get_point_invariant_states(self, F, backbone_features):\n        adaptive_input = backbone_features\n\n        if getattr(self.controller_net, 'return_map', False):\n            controller_input = self.controller_net(backbone_features)\n        else:\n            controller_input = backbone_features\n\n        return adaptive_input, controller_input\n\n    def get_instances_maps(self, F, points, adaptive_input, controller_input):\n        if isinstance(points, mx.nd.NDArray):\n            self.num_points = points.shape[1]\n\n        if getattr(self.controller_net, 'return_map', False):\n            w = self.eqf(controller_input, points)\n        else:\n            w = self.eqf(controller_input, points)\n            w = self.controller_net(w)\n\n        points = F.reshape(points, shape=(-1, 2))\n        x = F.repeat(adaptive_input, self.num_points, axis=0)\n        x = self.add_coord_features(x, points)\n\n        x = self.block0(x)\n        x = self.adain(x, w)\n        x = self.block1(x)\n\n        return x\n"""
adaptis/model/cityscapes/resnet.py,0,"b""import mxnet as mx\nfrom gluoncv.model_zoo.resnetv1b import resnet50_v1s, resnet101_v1s, resnet152_v1s\n\n\nclass ResNetBackbone(mx.gluon.HybridBlock):\n    def __init__(self, backbone='resnet50', pretrained_base=True,dilated=True, **kwargs):\n        super(ResNetBackbone, self).__init__()\n\n        with self.name_scope():\n            if backbone == 'resnet50':\n                pretrained = resnet50_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n            elif backbone == 'resnet101':\n                pretrained = resnet101_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n            elif backbone == 'resnet152':\n                pretrained = resnet152_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n            else:\n                raise RuntimeError(f'unknown backbone: {backbone}')\n\n            self.conv1 = pretrained.conv1\n            self.bn1 = pretrained.bn1\n            self.relu = pretrained.relu\n            self.maxpool = pretrained.maxpool\n            self.layer1 = pretrained.layer1\n            self.layer2 = pretrained.layer2\n            self.layer3 = pretrained.layer3\n            self.layer4 = pretrained.layer4\n\n    def hybrid_forward(self, F, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        c1 = self.layer1(x)\n        c2 = self.layer2(c1)\n        c3 = self.layer3(c2)\n        c4 = self.layer4(c3)\n\n        return c1, c2, c3, c4\n"""
adaptis/model/cityscapes/resnet_fpn.py,0,"b""import mxnet as mx\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.nn import HybridBlock\nfrom .resnet import ResNetBackbone\n\n\nclass ResNetFPN(mx.gluon.HybridBlock):\n    def __init__(self, backbone= 'resnet50', backbone_lr_mult=0.1, **kwargs):\n        super(ResNetFPN, self).__init__()\n\n        self.backbone_name = backbone\n        self.backbone_lr_mult = backbone_lr_mult\n        self._kwargs = kwargs\n\n        with self.name_scope():\n            self.backbone = ResNetBackbone(backbone=self.backbone_name, pretrained_base=False, dilated=False, **kwargs)\n\n            self.head = _FPNHead(output_channels=256, **kwargs)\n\n    def load_pretrained_weights(self):\n        pretrained = ResNetBackbone(backbone=self.backbone_name, pretrained_base=True, dilated=False, **self._kwargs)\n        backbone_params = self.backbone.collect_params()\n        pretrained_weights = pretrained.collect_params()\n        for k, v in pretrained_weights.items():\n            param_name = backbone_params.prefix + k[len(pretrained_weights.prefix):]\n            backbone_params[param_name].set_data(v.data())\n\n        self.backbone.collect_params().setattr('lr_mult', self.backbone_lr_mult)\n\n    def hybrid_forward(self,F, x):\n        c1, c2, c3, c4 = self.backbone(x)\n        p1, p2, p3, p4 = self.head(c1, c2, c3, c4)\n\n        return p1, p2, p3, p4\n\n\nclass _FPNHead(HybridBlock):\n    def __init__(self, output_channels=256, norm_layer=nn.BatchNorm):\n        super(_FPNHead, self).__init__()\n        self._hdsize = {}\n\n        with self.name_scope():\n            self.block4 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n            self.block3 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n            self.block2 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n            self.block1 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n\n    def hybrid_forward(self, F, c1, c2, c3, c4):\n        p4 = self.block4(c4)\n        p3 = self._resize_as(F, 'id_1', p4, c3) + self.block3(c3)\n        p2 = self._resize_as(F, 'id_2', p3, c2) + self.block2(c2)\n        p1 = self._resize_as(F, 'id_3', p2, c1) + self.block1(c1)\n\n        return p1, p2, p3, p4\n\n    def _resize_as(self, F, name, x, y):\n        h_key = name + '_h'\n        w_key = name + '_w'\n\n        if hasattr(y, 'shape'):\n            _, _, h, w = y.shape\n            _, _, h2, w2 = x.shape\n\n            if h == h2 and w == w2:\n                h = 0\n                w = 0\n\n            self._hdsize[h_key] = h\n            self._hdsize[w_key] = w\n        else:\n            h, w = self._hdsize[h_key], self._hdsize[w_key]\n\n        if h == 0 and w == 0:\n            return x\n        else:\n            return F.contrib.BilinearResize2D(x, h, w)\n\n\nclass SemanticFPNHead(HybridBlock):\n    def __init__(self, num_classes, output_channels=128, norm_layer=nn.BatchNorm):\n        super(SemanticFPNHead, self).__init__()\n        self._hdsize = {}\n\n        with self.name_scope():\n            self.block4_1 = ConvBlock(output_channels, kernel_size=3, norm_layer=norm_layer)\n            self.block4_2 = ConvBlock(output_channels, kernel_size=3, norm_layer=norm_layer)\n            self.block4_3 = ConvBlock(output_channels, kernel_size=3, norm_layer=norm_layer)\n\n            self.block3_1 = ConvBlock(output_channels, kernel_size=3, norm_layer=norm_layer)\n            self.block3_2 = ConvBlock(output_channels, kernel_size=3, norm_layer=norm_layer)\n\n            self.block2 = ConvBlock(output_channels, kernel_size=3, norm_layer=norm_layer)\n            self.block1 = ConvBlock(output_channels, kernel_size=1, norm_layer=norm_layer)\n\n            self.postprocess_block = nn.Conv2D(num_classes, kernel_size=1)\n\n    def hybrid_forward(self, F, c1, c2, c3, c4):\n        out4 = self._resize_as(F, 'id_1', self.block4_1(c4), c3)\n        out4 = self._resize_as(F, 'id_2', self.block4_2(out4), c2)\n        out4 = self._resize_as(F, 'id_3', self.block4_3(out4), c1)\n\n        out3 = self._resize_as(F, 'id_4', self.block3_1(c3), c2)\n        out3 = self._resize_as(F, 'id_5', self.block3_2(out3), c1)\n\n        out2 = self._resize_as(F, 'id_6', self.block2(c2), c1)\n\n        out1 = self.block1(c1)\n\n        out = out1 + out2 + out3 + out4\n\n        out = self.postprocess_block(out)\n\n        return out\n\n    def _resize_as(self, F,name, x, y):\n        h_key = name + '_h'\n        w_key = name + '_w'\n\n        if hasattr(y, 'shape'):\n            _, _, h, w = y.shape\n            _, _, h2, w2 = x.shape\n\n            if h == h2 and w == w2:\n                h = 0\n                w = 0\n\n            self._hdsize[h_key]=h\n            self._hdsize[w_key]=w\n        else:\n            h, w = self._hdsize[h_key], self._hdsize[w_key]\n\n        if h == 0 and w == 0:\n            return x\n        else:\n            return F.contrib.BilinearResize2D(x,h,w)\n\n\nclass ConvBlock(HybridBlock):\n    def __init__(self, output_channels, kernel_size, activation='relu', norm_layer=nn.BatchNorm):\n        super().__init__()\n        self.body = nn.HybridSequential()\n        self.body.add(\n            nn.Conv2D(output_channels, kernel_size=kernel_size, activation=activation),\n            norm_layer(in_channels=output_channels)\n        )\n\n    def hybrid_forward(self, F, x):\n        return self.body(x)\n"""
adaptis/model/toy/models.py,0,"b""import mxnet as mx\nfrom mxnet import gluon\n\nfrom adaptis.model.adaptis import AdaptIS\nfrom .unet import UNet\nfrom adaptis.model.basic_blocks import ConvHead, SimpleConvController\nfrom adaptis.model.ops import AdaIN, ExtractQueryFeatures, AppendCoordFeatures\n\n\ndef get_unet_model(norm_layer, channel_width=32, max_width=512, with_proposals=False, rescale_output=(0.2, -1.7)):\n    return AdaptIS(\n        feature_extractor=UNet(num_blocks=4, first_channels=channel_width, max_width=max_width,\n                               norm_layer=norm_layer),\n        adaptis_head=ToyAdaptISHead(\n            SimpleConvController(3, channel_width, norm_layer=norm_layer),\n            ch=channel_width, norm_radius=42, with_coord_features=True,\n            norm_layer=norm_layer,\n            rescale_output=rescale_output\n        ),\n        segmentation_head=ConvHead(2, channels=32, num_layers=3, norm_layer=norm_layer),\n        proposal_head=ConvHead(1, channels=32, num_layers=2, norm_layer=norm_layer),\n        with_proposals=with_proposals\n    )\n\n\nclass ToyAdaptISHead(gluon.HybridBlock):\n    def __init__(self, controller_net, ch=32, norm_radius=42,\n                 with_coord_features=True, norm_layer=gluon.nn.BatchNorm,\n                 rescale_output=None):\n        super(ToyAdaptISHead, self).__init__()\n\n        self.num_points = None\n        self.with_coord_features = with_coord_features\n        self.rescale_output = rescale_output\n\n        with self.name_scope():\n            self.eqf = ExtractQueryFeatures(extraction_method='ROIAlign', spatial_scale=1.0)\n            self.controller_net = controller_net\n\n            if self.with_coord_features:\n                self.add_coord_features = AppendCoordFeatures(norm_radius=norm_radius, spatial_scale=1.0)\n\n            self.block0 = gluon.nn.HybridSequential()\n            for i in range(3):\n                self.block0.add(\n                    gluon.nn.Conv2D(channels=ch, kernel_size=3, padding=1, activation='relu'),\n                    norm_layer(in_channels=ch)\n                )\n\n            self.adain = AdaIN(ch)\n\n            self.block1 = gluon.nn.HybridSequential()\n            for i in range(1):\n                self.block1.add(\n                    gluon.nn.Conv2D(channels=ch // 2, kernel_size=3, padding=1, activation='relu'),\n                    norm_layer(in_channels=ch // 2)\n                )\n            self.block1.add(gluon.nn.Conv2D(channels=1, kernel_size=1))\n\n    def hybrid_forward(self, F, backbone_features, points):\n        adaptive_input, controller_input = self.get_point_invariant_states(F, backbone_features)\n        return self.get_instances_maps(F, points, adaptive_input, controller_input)\n\n    def get_point_invariant_states(self, F, backbone_features):\n        adaptive_input = backbone_features\n\n        if getattr(self.controller_net, 'return_map', False):\n            controller_input = self.controller_net(backbone_features)\n        else:\n            controller_input = backbone_features\n\n        return adaptive_input, controller_input\n\n    def get_instances_maps(self, F, points, adaptive_input, controller_input):\n        if isinstance(points, mx.nd.NDArray):\n            self.num_points = points.shape[1]\n\n        if getattr(self.controller_net, 'return_map', False):\n            w = self.eqf(controller_input, points)\n        else:\n            w = self.eqf(controller_input, points)\n            w = self.controller_net(w)\n\n        points = F.reshape(points, shape=(-1, 2))\n        x = F.repeat(adaptive_input, self.num_points, axis=0)\n        if self.with_coord_features:\n            x = self.add_coord_features(x, points)\n\n        x = self.block0(x)\n        x = self.adain(x, w)\n        x = self.block1(x)\n\n        if self.rescale_output:\n            scale, bias = self.rescale_output\n            x = scale * x + bias\n\n        return x\n"""
adaptis/model/toy/unet.py,0,"b""import mxnet as mx\nfrom mxnet import gluon\n\n\nclass UNet(gluon.nn.HybridBlock):\n    def __init__(self, num_blocks=4, first_channels=64, max_width=512,\n                 norm_layer=gluon.nn.BatchNorm, **kwargs):\n        super(UNet, self).__init__(**kwargs)\n\n        self.num_blocks = num_blocks\n        with self.name_scope():\n            self.image_bn = norm_layer(in_channels=3)\n\n            prev_ch = 0\n            for i in range(num_blocks + 1):\n                if i == 0:\n                    dblock = down_block(first_channels, norm_layer=norm_layer)\n                else:\n                    dblock = gluon.nn.HybridSequential()\n                    dblock_ch = min(first_channels * (2 ** i), max_width)\n                    dblock.add(\n                        gluon.nn.MaxPool2D(2, 2, ceil_mode=True),\n                        down_block(dblock_ch, norm_layer=norm_layer)\n                    )\n                    prev_ch = dblock_ch\n                setattr(self, f'd{i}', dblock)\n\n            for i in range(num_blocks):\n                uindx = self.num_blocks - i - 1\n                block_width = first_channels * (2 ** uindx)\n                block_ch = min(block_width, max_width)\n                block_shrink = uindx != 0 and block_width <= max_width\n                ublock = up_block(block_ch, shrink=block_shrink,\n                                  in_channels=prev_ch,\n                                  norm_layer=norm_layer)\n                prev_ch = block_ch\n                if block_shrink:\n                    prev_ch //= 2\n                setattr(self, f'u{uindx}', ublock)\n\n            self.fe = gluon.nn.HybridSequential()\n            self.fe.add(\n                gluon.nn.Conv2D(channels=first_channels, kernel_size=3, padding=1, activation='relu'),\n                norm_layer(in_channels=first_channels),\n                gluon.nn.Conv2D(channels=first_channels, kernel_size=3, padding=1, activation='relu'),\n                norm_layer(in_channels=first_channels)\n            )\n\n    def hybrid_forward(self, F, x):\n        x = self.image_bn(x)\n\n        d_outs = []\n        for i in range(self.num_blocks + 1):\n            x = getattr(self, f'd{i}')(x)\n            d_outs.append(x)\n\n        for i in range(self.num_blocks):\n            u_indx = self.num_blocks - i - 1\n            x = getattr(self, f'u{u_indx}')(x, d_outs[u_indx])\n\n        return self.fe(x),\n\n\ndef down_block(channels, norm_layer=gluon.nn.BatchNorm):\n    out = gluon.nn.HybridSequential()\n\n    out.add(\n        ConvBlock(channels, 3, norm_layer=norm_layer),\n        ConvBlock(channels, 3, norm_layer=norm_layer)\n    )\n    return out\n\n\nclass up_block(gluon.nn.HybridBlock):\n    def __init__(self, channels, shrink=True, norm_layer=gluon.nn.BatchNorm, in_channels=0, **kwargs):\n        super(up_block, self).__init__(**kwargs)\n\n        self.upsampler = gluon.nn.Conv2DTranspose(channels=channels, kernel_size=4, strides=2,\n                                                  in_channels=in_channels,\n                                                  padding=1, use_bias=False, groups=channels,\n                                                  weight_initializer=mx.init.Bilinear())\n        self.upsampler.collect_params().setattr('grad_req', 'null')\n\n        self.conv1 = ConvBlock(channels, 1, norm_layer=norm_layer)\n        self.conv3_0 = ConvBlock(channels, 3, norm_layer=norm_layer)\n        if shrink:\n            self.conv3_1 = ConvBlock(channels // 2, 3, norm_layer=norm_layer)\n        else:\n            self.conv3_1 = ConvBlock(channels, 3, norm_layer=norm_layer)\n\n    def hybrid_forward(self, F, x, s):\n        x = self.upsampler(x)\n        x = self.conv1(x)\n        x = F.relu(x)\n\n        x = F.Crop(*[x, s], center_crop=True)\n        x = F.concat(s, x, dim=1)\n\n        x = self.conv3_0(x)\n        x = self.conv3_1(x)\n\n        return x\n\n\ndef ConvBlock(channels, kernel_size, norm_layer=gluon.nn.BatchNorm):\n    out = gluon.nn.HybridSequential()\n\n    out.add(\n        gluon.nn.Conv2D(channels, kernel_size, padding=kernel_size // 2, use_bias=False),\n        gluon.nn.Activation('relu'),\n        norm_layer()\n    )\n    return out\n"""
