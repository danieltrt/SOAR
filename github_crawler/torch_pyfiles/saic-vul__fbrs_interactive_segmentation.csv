file_path,api_count,code
demo.py,3,"b'import argparse\nimport tkinter as tk\n\nimport torch\n\nfrom isegm.utils import exp\nfrom isegm.inference import utils\nfrom interactive_demo.app import InteractiveDemoApp\n\n\ndef main():\n    args, cfg = parse_args()\n\n    torch.backends.cudnn.deterministic = True\n    checkpoint_path = utils.find_checkpoint(cfg.INTERACTIVE_MODELS_PATH, args.checkpoint)\n    model = utils.load_is_model(checkpoint_path, args.device, cpu_dist_maps=True, norm_radius=args.norm_radius)\n\n    root = tk.Tk()\n    root.minsize(960, 480)\n    app = InteractiveDemoApp(root, args, model)\n    root.deiconify()\n    app.mainloop()\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--checkpoint\', type=str, required=True,\n                        help=\'The path to the checkpoint. \'\n                             \'This can be a relative path (relative to cfg.INTERACTIVE_MODELS_PATH) \'\n                             \'or an absolute path. The file extension can be omitted.\')\n\n    parser.add_argument(\'--gpu\', type=int, default=0,\n                        help=\'Id of GPU to use.\')\n\n    parser.add_argument(\'--cpu\', action=\'store_true\', default=False,\n                        help=\'Use only CPU for inference.\')\n\n    parser.add_argument(\'--limit-longest-size\', type=int, default=800,\n                        help=\'If the largest side of an image exceeds this value, \'\n                             \'it is resized so that its largest side is equal to this value.\')\n\n    parser.add_argument(\'--norm-radius\', type=int, default=260)\n\n    parser.add_argument(\'--cfg\', type=str, default=""config.yml"",\n                        help=\'The path to the config file.\')\n\n    args = parser.parse_args()\n    if args.cpu:\n        args.device =torch.device(\'cpu\')\n    else:\n        args.device = torch.device(f\'cuda:{args.gpu}\')\n    cfg = exp.load_config_file(args.cfg, return_edict=True)\n\n    return args, cfg\n\n\nif __name__ == \'__main__\':\n    main()\n'"
train.py,2,"b'import argparse\nimport importlib.util\n\nimport torch\nfrom isegm.utils.exp import init_experiment\n\n\ndef main():\n    args = parse_args()\n    model_script = load_module(args.model_path)\n\n    cfg = init_experiment(args)\n\n    torch.backends.cudnn.benchmark = True\n    torch.multiprocessing.set_sharing_strategy(\'file_system\')\n    model_script.main(cfg)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'model_path\', type=str,\n                        help=\'Path to the model script.\')\n\n    parser.add_argument(\'--exp-name\', type=str, default=\'\',\n                        help=\'Here you can specify the name of the experiment. \'\n                             \'It will be added as a suffix to the experiment folder.\')\n\n    parser.add_argument(\'--workers\', type=int, default=4,\n                        metavar=\'N\', help=\'Dataloader threads.\')\n\n    parser.add_argument(\'--batch-size\', type=int, default=-1,\n                        help=\'You can override model batch size by specify positive number.\')\n\n    parser.add_argument(\'--ngpus\', type=int, default=1,\n                        help=\'Number of GPUs. \'\n                             \'If you only specify ""--gpus"" argument, the ngpus value will be calculated automatically. \'\n                             \'You should use either this argument or ""--gpus"".\')\n\n    parser.add_argument(\'--gpus\', type=str, default=\'\', required=False,\n                        help=\'Ids of used GPUs. You should use either this argument or ""--ngpus"".\')\n\n    parser.add_argument(\'--resume-exp\', type=str, default=None,\n                        help=\'The prefix of the name of the experiment to be continued. \'\n                             \'If you use this field, you must specify the ""--resume-prefix"" argument.\')\n\n    parser.add_argument(\'--resume-prefix\', type=str, default=\'latest\',\n                        help=\'The prefix of the name of the checkpoint to be loaded.\')\n\n    parser.add_argument(\'--start-epoch\', type=int, default=0,\n                        help=\'The number of the starting epoch from which training will continue. \'\n                             \'(it is important for correct logging and learning rate)\')\n\n    parser.add_argument(\'--weights\', type=str, default=None,\n                        help=\'Model weights will be loaded from the specified path if you use this argument.\')\n\n    return parser.parse_args()\n\n\ndef load_module(script_path):\n    spec = importlib.util.spec_from_file_location(""model_script"", script_path)\n    model_script = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(model_script)\n\n    return model_script\n\n\nif __name__ == \'__main__\':\n    main()'"
interactive_demo/app.py,0,"b'import tkinter as tk\nfrom tkinter import messagebox, filedialog, ttk\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport os\n\nfrom interactive_demo.canvas import CanvasImage\nfrom interactive_demo.controller import InteractiveController\nfrom interactive_demo.wrappers import BoundedNumericalEntry, FocusHorizontalScale, FocusCheckButton, \\\n    FocusButton, FocusLabelFrame\n\nclass InteractiveDemoApp(ttk.Frame):\n    def __init__(self, master, args, model):\n        super().__init__(master)\n        self.master = master\n        master.title(""Interactive Segmentation with f-BRS"")\n        master.withdraw()\n        master.update_idletasks()\n        x = (master.winfo_screenwidth() - master.winfo_reqwidth()) / 2\n        y = (master.winfo_screenheight() - master.winfo_reqheight()) / 2\n        master.geometry(""+%d+%d"" % (x, y))\n        self.pack(fill=""both"", expand=True)\n        self.filename = \'\'\n        self.filenames = []\n        self.current_file_index = 0\n\n        self.brs_modes = [\'NoBRS\', \'RGB-BRS\', \'DistMap-BRS\', \'f-BRS-A\', \'f-BRS-B\', \'f-BRS-C\']\n        self.limit_longest_size = args.limit_longest_size\n\n        self.controller = InteractiveController(model, args.device,\n                                                predictor_params={\'brs_mode\': \'NoBRS\'},\n                                                update_image_callback=self._update_image)\n\n        self._init_state()\n        self._add_menu()\n        self._add_canvas()\n        self._add_buttons()\n\n        master.bind(\'<space>\', lambda event: self.controller.finish_object())\n        master.bind(\'a\', lambda event: self.controller.partially_finish_object())\n        master.bind(\'<Key-Right>\', self._set_next_image)\n        master.bind(\'<Key-Left>\', self._set_forward_image)\n        master.bind(\'<Control-Key-s>\', self._save_mask_force)\n\n        self.state[\'zoomin_params\'][\'skip_clicks\'].trace(mode=\'w\', callback=self._reset_predictor)\n        self.state[\'zoomin_params\'][\'target_size\'].trace(mode=\'w\', callback=self._reset_predictor)\n        self.state[\'zoomin_params\'][\'expansion_ratio\'].trace(mode=\'w\', callback=self._reset_predictor)\n        self.state[\'predictor_params\'][\'net_clicks_limit\'].trace(mode=\'w\', callback=self._change_brs_mode)\n        self.state[\'lbfgs_max_iters\'].trace(mode=\'w\', callback=self._change_brs_mode)\n        self._reset_predictor()\n\n    def _init_state(self):\n        self.state = {\n            \'zoomin_params\': {\n                \'use_zoom_in\': tk.BooleanVar(value=True),\n                \'skip_clicks\': tk.IntVar(value=1),\n                \'target_size\': tk.IntVar(value=min(480, self.limit_longest_size)),\n                \'expansion_ratio\': tk.DoubleVar(value=1.4)\n            },\n\n            \'predictor_params\': {\n                \'net_clicks_limit\': tk.IntVar(value=8)\n            },\n            \'brs_mode\': tk.StringVar(value=\'f-BRS-B\'),\n            \'prob_thresh\': tk.DoubleVar(value=0.5),\n            \'lbfgs_max_iters\': tk.IntVar(value=20),\n\n            \'alpha_blend\': tk.DoubleVar(value=0.5),\n            \'click_radius\': tk.IntVar(value=3),\n        }\n\n    def _add_menu(self):\n        self.menubar = FocusLabelFrame(self, bd=1)\n        self.menubar.pack(side=tk.TOP, fill=\'x\')\n\n        button = FocusButton(self.menubar, text=\'Load image\', command=self._load_image_callback)\n        button.pack(side=tk.LEFT)\n        button = FocusButton(self.menubar, text=\'Save mask\', command=self._save_mask_callback)\n        button.pack(side=tk.LEFT)\n        button = FocusButton(self.menubar, text=\'About\', command=self._about_callback)\n        button.pack(side=tk.LEFT)\n        button = FocusButton(self.menubar, text=\'Exit\', command=self.master.quit)\n        button.pack(side=tk.LEFT)\n\n    def _add_canvas(self):\n        self.canvas_frame = FocusLabelFrame(self, text=""Image"")\n        self.canvas_frame.rowconfigure(0, weight=1)\n        self.canvas_frame.columnconfigure(0, weight=1)\n\n        self.canvas = tk.Canvas(self.canvas_frame, highlightthickness=0, cursor=""hand1"", width=400, height=400)\n        self.canvas.grid(row=0, column=0, sticky=\'nswe\', padx=5, pady=5)\n\n        self.image_on_canvas = None\n        self.canvas_frame.pack(side=tk.LEFT, fill=""both"", expand=True, padx=5, pady=5)\n\n    def _add_buttons(self):\n        self.control_frame = FocusLabelFrame(self, text=""Controls"")\n        self.control_frame.pack(side=tk.TOP, fill=\'x\', padx=5, pady=5)\n        master = self.control_frame\n\n        self.clicks_options_frame = FocusLabelFrame(master, text=""Clicks management"")\n        self.clicks_options_frame.pack(side=tk.TOP, fill=tk.X, padx=10, pady=3)\n        self.finish_object_button = \\\n            FocusButton(self.clicks_options_frame, text=\'Finish\\nobject\', bg=\'#b6d7a8\', fg=\'black\', width=10, height=2,\n                        state=tk.DISABLED, command=self.controller.finish_object)\n        self.finish_object_button.pack(side=tk.LEFT, fill=tk.X, padx=10, pady=3)\n        self.undo_click_button = \\\n            FocusButton(self.clicks_options_frame, text=\'Undo click\', bg=\'#ffe599\', fg=\'black\', width=10, height=2,\n                        state=tk.DISABLED, command=self.controller.undo_click)\n        self.undo_click_button.pack(side=tk.LEFT, fill=tk.X, padx=10, pady=3)\n        self.reset_clicks_button = \\\n            FocusButton(self.clicks_options_frame, text=\'Reset clicks\', bg=\'#ea9999\', fg=\'black\', width=10, height=2,\n                        state=tk.DISABLED, command=self._reset_last_object)\n        self.reset_clicks_button.pack(side=tk.LEFT, fill=tk.X, padx=10, pady=3)\n\n        self.zoomin_options_frame = FocusLabelFrame(master, text=""ZoomIn options"")\n        self.zoomin_options_frame.pack(side=tk.TOP, fill=tk.X, padx=10, pady=3)\n        FocusCheckButton(self.zoomin_options_frame, text=\'Use ZoomIn\', command=self._reset_predictor,\n                         variable=self.state[\'zoomin_params\'][\'use_zoom_in\']).grid(rowspan=3, column=0, padx=10)\n        tk.Label(self.zoomin_options_frame, text=""Skip clicks"").grid(row=0, column=1, pady=1, sticky=\'e\')\n        tk.Label(self.zoomin_options_frame, text=""Target size"").grid(row=1, column=1, pady=1, sticky=\'e\')\n        tk.Label(self.zoomin_options_frame, text=""Expand ratio"").grid(row=2, column=1, pady=1, sticky=\'e\')\n        BoundedNumericalEntry(self.zoomin_options_frame, variable=self.state[\'zoomin_params\'][\'skip_clicks\'],\n                              min_value=0, max_value=None, vartype=int,\n                              name=\'zoom_in_skip_clicks\').grid(row=0, column=2, padx=10, pady=1, sticky=\'w\')\n        BoundedNumericalEntry(self.zoomin_options_frame, variable=self.state[\'zoomin_params\'][\'target_size\'],\n                              min_value=100, max_value=self.limit_longest_size, vartype=int,\n                              name=\'zoom_in_target_size\').grid(row=1, column=2, padx=10, pady=1, sticky=\'w\')\n        BoundedNumericalEntry(self.zoomin_options_frame, variable=self.state[\'zoomin_params\'][\'expansion_ratio\'],\n                              min_value=1.0, max_value=2.0, vartype=float,\n                              name=\'zoom_in_expansion_ratio\').grid(row=2, column=2, padx=10, pady=1, sticky=\'w\')\n        self.zoomin_options_frame.columnconfigure((0, 1, 2), weight=1)\n\n        self.brs_options_frame = FocusLabelFrame(master, text=""BRS options"")\n        self.brs_options_frame.pack(side=tk.TOP, fill=tk.X, padx=10, pady=3)\n        menu = tk.OptionMenu(self.brs_options_frame, self.state[\'brs_mode\'],\n                             *self.brs_modes, command=self._change_brs_mode)\n        menu.config(width=11)\n        menu.grid(rowspan=2, column=0, padx=10)\n        self.net_clicks_label = tk.Label(self.brs_options_frame, text=""Network clicks"")\n        self.net_clicks_label.grid(row=0, column=1, pady=2, sticky=\'e\')\n        self.net_clicks_entry = BoundedNumericalEntry(self.brs_options_frame,\n                                                      variable=self.state[\'predictor_params\'][\'net_clicks_limit\'],\n                                                      min_value=0, max_value=None, vartype=int, allow_inf=True,\n                                                      name=\'net_clicks_limit\')\n        self.net_clicks_entry.grid(row=0, column=2, padx=10, pady=2, sticky=\'w\')\n        tk.Label(self.brs_options_frame, text=""L-BFGS\\nmax iterations"").grid(row=1, column=1, pady=2, sticky=\'e\')\n        BoundedNumericalEntry(self.brs_options_frame, variable=self.state[\'lbfgs_max_iters\'],\n                              min_value=1, max_value=1000, vartype=int,\n                              name=\'lbfgs_max_iters\').grid(row=1, column=2, padx=10, pady=2, sticky=\'w\')\n        self.brs_options_frame.columnconfigure((0, 1), weight=1)\n\n        self.prob_thresh_frame = FocusLabelFrame(master, text=""Predictions threshold"")\n        self.prob_thresh_frame.pack(side=tk.TOP, fill=tk.X, padx=10, pady=3)\n        FocusHorizontalScale(self.prob_thresh_frame, from_=0.0, to=1.0, command=self._update_prob_thresh,\n                             variable=self.state[\'prob_thresh\']).pack(padx=10)\n\n        self.alpha_blend_frame = FocusLabelFrame(master, text=""Alpha blending coefficient"")\n        self.alpha_blend_frame.pack(side=tk.TOP, fill=tk.X, padx=10, pady=3)\n        FocusHorizontalScale(self.alpha_blend_frame, from_=0.0, to=1.0, command=self._update_blend_alpha,\n                             variable=self.state[\'alpha_blend\']).pack(padx=10, anchor=tk.CENTER)\n\n        self.click_radius_frame = FocusLabelFrame(master, text=""Visualisation click radius"")\n        self.click_radius_frame.pack(side=tk.TOP, fill=tk.X, padx=10, pady=3)\n        FocusHorizontalScale(self.click_radius_frame, from_=0, to=7, resolution=1, command=self._update_click_radius,\n                             variable=self.state[\'click_radius\']).pack(padx=10, anchor=tk.CENTER)\n\n    def _set_next_image(self, event):\n        if self.current_file_index < len(self.filenames):\n            self.current_file_index += 1\n            self._set_image(self.current_file_index)\n\n    def _set_forward_image(self, event):\n        if self.current_file_index > 0:\n            self.current_file_index -= 1\n            self._set_image(self.current_file_index)\n\n    def _save_mask_force(self, event):\n        self.menubar.focus_set()\n        if self._check_entry(self):\n            mask = self.controller.result_mask\n            if mask is None:\n                return\n            if mask.max() < 256:\n                mask = mask.astype(np.uint8)\n            cv2.imwrite(\'{}.png\'.format(self.filenames[self.current_file_index]), mask)\n\n    def _set_image(self, value):\n        image = cv2.cvtColor(cv2.imread(self.filenames[value]), cv2.COLOR_BGR2RGB)\n        self.filename = os.path.basename(self.filenames[value])\n        self.controller.set_image(image)\n\n    def _load_image_callback(self):\n        self.menubar.focus_set()\n        if self._check_entry(self):\n            self.filenames = filedialog.askopenfilenames(parent=self.master, filetypes=[\n                (""Images"", ""*.jpg *.JPG *.jpeg *.png *.bmp *.tiff""),\n                (""All files"", ""*.*""),\n            ], title=""Chose an image"")\n            if len(self.filenames) > 0:\n                self._set_image(0)\n\n    def _save_mask_callback(self):\n        self.menubar.focus_set()\n        if self._check_entry(self):\n            mask = self.controller.result_mask\n            if mask is None:\n                return\n\n            filename = filedialog.asksaveasfilename(parent=self.master, initialfile=\'{}.png\'.format(self.filename), filetypes=[\n                (""PNG image"", ""*.png""),\n                (""BMP image"", ""*.bmp""),\n                (""All files"", ""*.*""),\n            ], title=""Save current mask as..."")\n\n            if len(filename) > 0:\n                if mask.max() < 256:\n                    mask = mask.astype(np.uint8)\n                cv2.imwrite(filename, mask)\n\n    def _about_callback(self):\n        self.menubar.focus_set()\n\n        text = [\n            ""Developed by:"",\n            ""K.Sofiiuk and I. Petrov"",\n            ""MPL-2.0 License, 2020""\n        ]\n\n        messagebox.showinfo(""About Demo"", \'\\n\'.join(text))\n\n    def _reset_last_object(self):\n        self.state[\'alpha_blend\'].set(0.5)\n        self.state[\'prob_thresh\'].set(0.5)\n        self.controller.reset_last_object()\n\n    def _update_prob_thresh(self, value):\n        if self.controller.is_incomplete_mask:\n            self.controller.prob_thresh = self.state[\'prob_thresh\'].get()\n            self._update_image()\n\n    def _update_blend_alpha(self, value):\n        self._update_image()\n\n    def _update_click_radius(self, *args):\n        if self.image_on_canvas is None:\n            return\n\n        self._update_image()\n\n    def _change_brs_mode(self, *args):\n        if self.state[\'brs_mode\'].get() == \'NoBRS\':\n            self.net_clicks_entry.set(\'INF\')\n            self.net_clicks_entry.configure(state=tk.DISABLED)\n            self.net_clicks_label.configure(state=tk.DISABLED)\n        else:\n            if self.net_clicks_entry.get() == \'INF\':\n                self.net_clicks_entry.set(8)\n            self.net_clicks_entry.configure(state=tk.NORMAL)\n            self.net_clicks_label.configure(state=tk.NORMAL)\n\n        self._reset_predictor()\n\n    def _reset_predictor(self):\n        brs_mode = self.state[\'brs_mode\'].get()\n        prob_thresh = self.state[\'prob_thresh\'].get()\n        net_clicks_limit = None if brs_mode == \'NoBRS\' else self.state[\'predictor_params\'][\'net_clicks_limit\'].get()\n\n        if self.state[\'zoomin_params\'][\'use_zoom_in\'].get():\n            zoomin_params = {\n                \'skip_clicks\': self.state[\'zoomin_params\'][\'skip_clicks\'].get(),\n                \'target_size\': self.state[\'zoomin_params\'][\'target_size\'].get(),\n                \'expansion_ratio\': self.state[\'zoomin_params\'][\'expansion_ratio\'].get()\n            }\n        else:\n            zoomin_params = None\n\n        predictor_params = {\n            \'brs_mode\': brs_mode,\n            \'prob_thresh\': prob_thresh,\n            \'zoom_in_params\': zoomin_params,\n            \'predictor_params\': {\n                \'net_clicks_limit\': net_clicks_limit,\n                \'max_size\': self.limit_longest_size\n            },\n            \'brs_opt_func_params\': {\'min_iou_diff\': 1e-3},\n            \'lbfgs_params\': {\'maxfun\': self.state[\'lbfgs_max_iters\'].get()}\n        }\n        self.controller.reset_predictor(predictor_params)\n\n    def _click_callback(self, is_positive, x, y):\n        self.canvas.focus_set()\n\n        if self.image_on_canvas is None:\n            messagebox.showwarning(""Warning"", ""Please, load an image first"")\n            return\n\n        if self._check_entry(self):\n            self.controller.add_click(x, y, is_positive)\n\n    def _update_image(self, reset_canvas=False):\n        image = self.controller.get_visualization(alpha_blend=self.state[\'alpha_blend\'].get(),\n                                                  click_radius=self.state[\'click_radius\'].get())\n        if self.image_on_canvas is None:\n            self.image_on_canvas = CanvasImage(self.canvas_frame, self.canvas)\n            self.image_on_canvas.register_click_callback(self._click_callback)\n\n        self._set_click_dependent_widgets_state()\n        if image is not None:\n            self.image_on_canvas.reload_image(Image.fromarray(image), reset_canvas)\n\n    def _set_click_dependent_widgets_state(self):\n        after_1st_click_state = tk.NORMAL if self.controller.is_incomplete_mask else tk.DISABLED\n        before_1st_click_state = tk.DISABLED if self.controller.is_incomplete_mask else tk.NORMAL\n\n        self.finish_object_button.configure(state=after_1st_click_state)\n        self.undo_click_button.configure(state=after_1st_click_state)\n        self.reset_clicks_button.configure(state=after_1st_click_state)\n        self.zoomin_options_frame.set_frame_state(before_1st_click_state)\n        self.brs_options_frame.set_frame_state(before_1st_click_state)\n\n        if self.state[\'brs_mode\'].get() == \'NoBRS\':\n            self.net_clicks_entry.configure(state=tk.DISABLED)\n            self.net_clicks_label.configure(state=tk.DISABLED)\n\n    def _check_entry(self, widget):\n        all_checked = True\n        if widget.winfo_children is not None:\n            for w in widget.winfo_children():\n                all_checked = all_checked and self._check_entry(w)\n\n        if getattr(widget, ""_check_bounds"", None) is not None:\n            all_checked = all_checked and widget._check_bounds(widget.get(), \'-1\')\n\n        return all_checked\n'"
interactive_demo/canvas.py,0,"b'# -*- coding: utf-8 -*-\n"""""" Adopted from https://github.com/foobar167/junkyard/blob/master/manual_image_annotation1/polygon/gui_canvas.py """"""\nimport os\nimport sys\nimport time\nimport math\nimport tkinter as tk\n\nfrom tkinter import ttk\nfrom PIL import Image, ImageTk\n\n\ndef handle_exception(exit_code=0):\n    """""" Use: @land.logger.handle_exception(0)\n        before every function which could cast an exception """"""\n\n    def wrapper(func):\n        def inner(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except:\n                if exit_code != 0:  # if zero, don\'t exit from the program\n                    sys.exit(exit_code)  # exit from the program\n\n        return inner\n\n    return wrapper\n\n\nclass AutoScrollbar(ttk.Scrollbar):\n    """""" A scrollbar that hides itself if it\'s not needed. Works only for grid geometry manager """"""\n\n    def set(self, lo, hi):\n        if float(lo) <= 0.0 and float(hi) >= 1.0:\n            self.grid_remove()\n        else:\n            self.grid()\n            ttk.Scrollbar.set(self, lo, hi)\n\n    @handle_exception(1)\n    def pack(self, **kw):\n        raise tk.TclError(\'Cannot use pack with the widget \' + self.__class__.__name__)\n\n    @handle_exception(1)\n    def place(self, **kw):\n        raise tk.TclError(\'Cannot use place with the widget \' + self.__class__.__name__)\n\n\nclass CanvasImage:\n    """""" Display and zoom image """"""\n\n    def __init__(self, canvas_frame, canvas):\n        """""" Initialize the ImageFrame """"""\n        self.current_scale = 1.0  # scale for the canvas image zoom, public for outer classes\n        self.__delta = 1.2  # zoom magnitude\n        self.__previous_state = 0  # previous state of the keyboard\n        # Create ImageFrame in placeholder widget\n        self.__imframe = canvas_frame\n        # Vertical and horizontal scrollbars for canvas\n        self.hbar = AutoScrollbar(canvas_frame, orient=\'horizontal\')\n        self.vbar = AutoScrollbar(canvas_frame, orient=\'vertical\')\n        self.hbar.grid(row=1, column=0, sticky=\'we\')\n        self.vbar.grid(row=0, column=1, sticky=\'ns\')\n        # Add scroll bars to canvas\n        self.canvas = canvas\n        self.canvas.configure(xscrollcommand=self.hbar.set, yscrollcommand=self.vbar.set)\n        self.hbar.configure(command=self.__scroll_x)  # bind scrollbars to the canvas\n        self.vbar.configure(command=self.__scroll_y)\n        # Bind events to the Canvas\n        self.canvas.bind(\'<Configure>\', lambda event: self.__size_changed())  # canvas is resized\n        self.canvas.bind(\'<Button-1>\', self.__left_mouse_button)  # remember canvas position\n        self.canvas.bind(\'<ButtonPress-3>\', self.__right_mouse_button_pressed)  # remember canvas position\n        self.canvas.bind(\'<ButtonRelease-3>\', self.__right_mouse_button_released)  # remember canvas position\n        self.canvas.bind(\'<B3-Motion>\', self.__right_mouse_button_motion)  # move canvas to the new position\n        self.canvas.bind(\'<MouseWheel>\', self.__wheel)  # zoom for Windows and MacOS, but not Linux\n        self.canvas.bind(\'<Button-5>\', self.__wheel)  # zoom for Linux, wheel scroll down\n        self.canvas.bind(\'<Button-4>\', self.__wheel)  # zoom for Linux, wheel scroll up\n        # Handle keystrokes in idle mode, because program slows down on a weak computers,\n        # when too many key stroke events in the same time\n        self.canvas.bind(\'<Key>\', lambda event: self.canvas.after_idle(self.__keystroke, event))\n        self.container = None\n\n        self._click_callback = None\n\n    def register_click_callback(self,  click_callback):\n        self._click_callback = click_callback\n\n    def reload_image(self, image, reset_canvas=True):\n        self.__original_image = image.copy()\n        self.__current_image = image.copy()\n\n        if reset_canvas:\n            self.imwidth, self.imheight = self.__original_image.size\n            self.__min_side = min(self.imwidth, self.imheight)  # get the smaller image side\n\n            scale = min(self.canvas.winfo_width() / self.imwidth, self.canvas.winfo_height() / self.imheight)\n            if self.container:\n                self.canvas.delete(self.container)\n\n            self.container = self.canvas.create_rectangle((0, 0, scale * self.imwidth, scale * self.imheight), width=0)\n            self.current_scale = scale\n            self._reset_canvas_offset()\n\n        self.__show_image()  # show image on the canvas\n        self.canvas.focus_set()  # set focus on the canvas\n\n    def grid(self, **kw):\n        """""" Put CanvasImage widget on the parent widget """"""\n        self.__imframe.grid(**kw)  # place CanvasImage widget on the grid\n        self.__imframe.grid(sticky=\'nswe\')  # make frame container sticky\n        self.__imframe.rowconfigure(0, weight=1)  # make canvas expandable\n        self.__imframe.columnconfigure(0, weight=1)\n\n    def __show_image(self):\n        box_image = self.canvas.coords(self.container)  # get image area\n        box_canvas = (self.canvas.canvasx(0),  # get visible area of the canvas\n                      self.canvas.canvasy(0),\n                      self.canvas.canvasx(self.canvas.winfo_width()),\n                      self.canvas.canvasy(self.canvas.winfo_height()))\n        box_img_int = tuple(map(int, box_image))  # convert to integer or it will not work properly\n        # Get scroll region box\n        box_scroll = [min(box_img_int[0], box_canvas[0]), min(box_img_int[1], box_canvas[1]),\n                      max(box_img_int[2], box_canvas[2]), max(box_img_int[3], box_canvas[3])]\n        # Horizontal part of the image is in the visible area\n        if box_scroll[0] == box_canvas[0] and box_scroll[2] == box_canvas[2]:\n            box_scroll[0] = box_img_int[0]\n            box_scroll[2] = box_img_int[2]\n        # Vertical part of the image is in the visible area\n        if box_scroll[1] == box_canvas[1] and box_scroll[3] == box_canvas[3]:\n            box_scroll[1] = box_img_int[1]\n            box_scroll[3] = box_img_int[3]\n        # Convert scroll region to tuple and to integer\n        self.canvas.configure(scrollregion=tuple(map(int, box_scroll)))  # set scroll region\n        x1 = max(box_canvas[0] - box_image[0], 0)  # get coordinates (x1,y1,x2,y2) of the image tile\n        y1 = max(box_canvas[1] - box_image[1], 0)\n        x2 = min(box_canvas[2], box_image[2]) - box_image[0]\n        y2 = min(box_canvas[3], box_image[3]) - box_image[1]\n\n        if int(x2 - x1) > 0 and int(y2 - y1) > 0:  # show image if it in the visible area\n            border_width = 2\n            sx1, sx2 = x1 / self.current_scale, x2 / self.current_scale\n            sy1, sy2 = y1 / self.current_scale, y2 / self.current_scale\n            crop_x, crop_y = max(0, math.floor(sx1 - border_width)), max(0, math.floor(sy1 - border_width))\n            crop_w, crop_h = math.ceil(sx2 - sx1 + 2 * border_width), math.ceil(sy2 - sy1 + 2 * border_width)\n            crop_w = min(crop_w, self.__original_image.width - crop_x)\n            crop_h = min(crop_h, self.__original_image.height - crop_y)\n\n            __current_image = self.__original_image.crop((crop_x, crop_y,\n                                                          crop_x + crop_w, crop_y + crop_h))\n            crop_zw = int(round(crop_w * self.current_scale))\n            crop_zh = int(round(crop_h * self.current_scale))\n            zoom_sx, zoom_sy = crop_zw / crop_w, crop_zh / crop_h\n            crop_zx, crop_zy = crop_x * zoom_sx, crop_y * zoom_sy\n            self.real_scale = (zoom_sx, zoom_sy)\n\n            interpolation = Image.NEAREST if self.current_scale > 2.0 else Image.ANTIALIAS\n            __current_image = __current_image.resize((crop_zw, crop_zh), interpolation)\n            zx1, zy1 = x1 - crop_zx, y1 - crop_zy\n            zx2 = min(zx1 + self.canvas.winfo_width(), __current_image.width)\n            zy2 = min(zy1 + self.canvas.winfo_height(), __current_image.height)\n\n            self.__current_image = __current_image.crop((zx1, zy1, zx2, zy2))\n\n            imagetk = ImageTk.PhotoImage(self.__current_image)\n            imageid = self.canvas.create_image(max(box_canvas[0], box_img_int[0]),\n                                               max(box_canvas[1], box_img_int[1]),\n                                               anchor=\'nw\', image=imagetk)\n            self.canvas.lower(imageid)  # set image into background\n            self.canvas.imagetk = imagetk  # keep an extra reference to prevent garbage-collection\n\n    def _get_click_coordinates(self, event):\n        x = self.canvas.canvasx(event.x)  # get coordinates of the event on the canvas\n        y = self.canvas.canvasy(event.y)\n\n        if self.outside(x, y):\n            return None\n\n        box_image = self.canvas.coords(self.container)\n        x = max(x - box_image[0], 0)\n        y = max(y - box_image[1], 0)\n\n        x = int(x / self.real_scale[0])\n        y = int(y / self.real_scale[1])\n\n        return x, y\n\n    # ================================================ Canvas Routines =================================================\n    def _reset_canvas_offset(self):\n        self.canvas.configure(scrollregion=(0, 0, 5000, 5000))\n        self.canvas.scan_mark(0, 0)\n        self.canvas.scan_dragto(int(self.canvas.canvasx(0)), int(self.canvas.canvasy(0)), gain=1)\n\n    def _change_canvas_scale(self, relative_scale, x=0, y=0):\n        new_scale = self.current_scale * relative_scale\n\n        if new_scale > 20:\n            return\n\n        if new_scale * self.__original_image.width < self.canvas.winfo_width() and \\\n           new_scale * self.__original_image.height < self.canvas.winfo_height():\n            return\n\n        self.current_scale = new_scale\n        self.canvas.scale(\'all\', x, y, relative_scale, relative_scale)  # rescale all objects\n\n    # noinspection PyUnusedLocal\n    def __scroll_x(self, *args, **kwargs):\n        """""" Scroll canvas horizontally and redraw the image """"""\n        self.canvas.xview(*args)  # scroll horizontally\n        self.__show_image()  # redraw the image\n\n    # noinspection PyUnusedLocal\n    def __scroll_y(self, *args, **kwargs):\n        """""" Scroll canvas vertically and redraw the image """"""\n        self.canvas.yview(*args)  # scroll vertically\n        self.__show_image()  # redraw the image\n\n    def __size_changed(self):\n        new_scale_w = self.canvas.winfo_width() / (self.current_scale * self.__original_image.width)\n        new_scale_h = self.canvas.winfo_height() / (self.current_scale * self.__original_image.height)\n        new_scale = min(new_scale_w, new_scale_h)\n        if new_scale > 1.0:\n            self._change_canvas_scale(new_scale)\n        self.__show_image()\n\n    # ================================================ Mouse callbacks =================================================\n    def __wheel(self, event):\n        """""" Zoom with mouse wheel """"""\n        x = self.canvas.canvasx(event.x)  # get coordinates of the event on the canvas\n        y = self.canvas.canvasy(event.y)\n        if self.outside(x, y): return  # zoom only inside image area\n\n        scale = 1.0\n        # Respond to Linux (event.num) or Windows (event.delta) wheel event\n        if event.num == 5 or event.delta == -120:  # scroll down, zoom out, smaller\n            scale /= self.__delta\n        if event.num == 4 or event.delta == 120:  # scroll up, zoom in, bigger\n            scale *= self.__delta\n\n        self._change_canvas_scale(scale, x, y)\n        self.__show_image()\n\n    def __left_mouse_button(self, event):\n        if self._click_callback is None:\n            return\n\n        coords = self._get_click_coordinates(event)\n\n        if coords is not None:\n            self._click_callback(is_positive=True, x=coords[0], y=coords[1])\n\n    def __right_mouse_button_pressed(self, event):\n        """""" Remember previous coordinates for scrolling with the mouse """"""\n        self._last_rb_click_time = time.time()\n        self._last_rb_click_event = event\n        self.canvas.scan_mark(event.x, event.y)\n\n    def __right_mouse_button_released(self, event):\n        time_delta = time.time() - self._last_rb_click_time\n        move_delta = math.sqrt((event.x - self._last_rb_click_event.x) ** 2 +\n                               (event.y - self._last_rb_click_event.y) ** 2)\n        if time_delta > 0.5 or move_delta > 3:\n            return\n\n        if self._click_callback is None:\n            return\n\n        coords = self._get_click_coordinates(self._last_rb_click_event)\n\n        if coords is not None:\n            self._click_callback(is_positive=False, x=coords[0], y=coords[1])\n\n    def __right_mouse_button_motion(self, event):\n        """""" Drag (move) canvas to the new position """"""\n        move_delta = math.sqrt((event.x - self._last_rb_click_event.x) ** 2 +\n                               (event.y - self._last_rb_click_event.y) ** 2)\n        if move_delta > 3:\n            self.canvas.scan_dragto(event.x, event.y, gain=1)\n            self.__show_image()  # zoom tile and show it on the canvas\n\n    def outside(self, x, y):\n        """""" Checks if the point (x,y) is outside the image area """"""\n        bbox = self.canvas.coords(self.container)  # get image area\n        if bbox[0] < x < bbox[2] and bbox[1] < y < bbox[3]:\n            return False  # point (x,y) is inside the image area\n        else:\n            return True  # point (x,y) is outside the image area\n\n    # ================================================= Keys Callback ==================================================\n    def __keystroke(self, event):\n        """""" Scrolling with the keyboard.\n            Independent from the language of the keyboard, CapsLock, <Ctrl>+<key>, etc. """"""\n        if event.state - self.__previous_state == 4:  # means that the Control key is pressed\n            pass  # do nothing if Control key is pressed\n        else:\n            self.__previous_state = event.state  # remember the last keystroke state\n            # Up, Down, Left, Right keystrokes\n            self.keycodes = {}  # init key codes\n            if os.name == \'nt\':  # Windows OS\n                self.keycodes = {\n                    \'d\': [68, 39, 102],\n                    \'a\': [65, 37, 100],\n                    \'w\': [87, 38, 104],\n                    \'s\': [83, 40, 98],\n                }\n            else:  # Linux OS\n                self.keycodes = {\n                    \'d\': [40, 114, 85],\n                    \'a\': [38, 113, 83],\n                    \'w\': [25, 111, 80],\n                    \'s\': [39, 116, 88],\n                }\n            if event.keycode in self.keycodes[\'d\']:  # scroll right, keys \'d\' or \'Right\'\n                self.__scroll_x(\'scroll\', 1, \'unit\', event=event)\n            elif event.keycode in self.keycodes[\'a\']:  # scroll left, keys \'a\' or \'Left\'\n                self.__scroll_x(\'scroll\', -1, \'unit\', event=event)\n            elif event.keycode in self.keycodes[\'w\']:  # scroll up, keys \'w\' or \'Up\'\n                self.__scroll_y(\'scroll\', -1, \'unit\', event=event)\n            elif event.keycode in self.keycodes[\'s\']:  # scroll down, keys \'s\' or \'Down\'\n                self.__scroll_y(\'scroll\', 1, \'unit\', event=event)\n'"
interactive_demo/controller.py,1,"b""import torch\nimport numpy as np\nfrom torchvision import transforms\n\nfrom isegm.inference import clicker\nfrom isegm.inference.predictors import get_predictor\nfrom isegm.utils.vis import draw_with_blend_and_clicks\n\n\nclass InteractiveController:\n    def __init__(self, net, device, predictor_params, update_image_callback, prob_thresh=0.5):\n        self.net = net.to(device)\n        self.prob_thresh = prob_thresh\n        self.clicker = clicker.Clicker()\n        self.states = []\n        self.probs_history = []\n        self.object_count = 0\n        self._result_mask = None\n\n        self.image = None\n        self.image_nd = None\n        self.predictor = None\n        self.device = device\n        self.update_image_callback = update_image_callback\n        self.predictor_params = predictor_params\n        self.reset_predictor()\n\n    def set_image(self, image):\n        input_transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize([.485, .456, .406], [.229, .224, .225])\n        ])\n\n        self.image = image\n        self.image_nd = input_transform(image).to(self.device)\n        self._result_mask = np.zeros(image.shape[:2], dtype=np.uint16)\n        self.object_count = 0\n        self.reset_last_object(update_image=False)\n        self.update_image_callback(reset_canvas=True)\n\n    def add_click(self, x, y, is_positive):\n        self.states.append({\n            'clicker': self.clicker.get_state(),\n            'predictor': self.predictor.get_states()\n        })\n\n        click = clicker.Click(is_positive=is_positive, coords=(y, x))\n        self.clicker.add_click(click)\n        pred = self.predictor.get_prediction(self.clicker)\n        torch.cuda.empty_cache()\n\n        if self.probs_history:\n            self.probs_history.append((self.probs_history[-1][0], pred))\n        else:\n            self.probs_history.append((np.zeros_like(pred), pred))\n\n        self.update_image_callback()\n\n    def undo_click(self):\n        if not self.states:\n            return\n\n        prev_state = self.states.pop()\n        self.clicker.set_state(prev_state['clicker'])\n        self.predictor.set_states(prev_state['predictor'])\n        self.probs_history.pop()\n        self.update_image_callback()\n\n    def partially_finish_object(self):\n        object_prob = self.current_object_prob\n        if object_prob is None:\n            return\n\n        self.probs_history.append((object_prob, np.zeros_like(object_prob)))\n        self.states.append(self.states[-1])\n\n        self.clicker.reset_clicks()\n        self.reset_predictor()\n        self.update_image_callback()\n\n    def finish_object(self):\n        object_prob = self.current_object_prob\n        if object_prob is None:\n            return\n\n        self.object_count += 1\n        object_mask = object_prob > self.prob_thresh\n        self._result_mask[object_mask] = self.object_count\n        self.reset_last_object()\n\n    def reset_last_object(self, update_image=True):\n        self.states = []\n        self.probs_history = []\n        self.clicker.reset_clicks()\n        self.reset_predictor()\n        if update_image:\n            self.update_image_callback()\n\n    def reset_predictor(self, predictor_params=None):\n        if predictor_params is not None:\n            self.predictor_params = predictor_params\n        self.predictor = get_predictor(self.net, device=self.device,\n                                       **self.predictor_params)\n        if self.image_nd is not None:\n            self.predictor.set_input_image(self.image_nd)\n\n    @property\n    def current_object_prob(self):\n        if self.probs_history:\n            current_prob_total, current_prob_additive = self.probs_history[-1]\n            return np.maximum(current_prob_total, current_prob_additive)\n        else:\n            return None\n\n    @property\n    def is_incomplete_mask(self):\n        return len(self.probs_history) > 0\n\n    @property\n    def result_mask(self):\n        return self._result_mask.copy()\n\n    def get_visualization(self, alpha_blend, click_radius):\n        if self.image is None:\n            return None\n\n        results_mask_for_vis = self.result_mask\n        if self.probs_history:\n            results_mask_for_vis[self.current_object_prob > self.prob_thresh] = self.object_count + 1\n\n        vis = draw_with_blend_and_clicks(self.image, mask=results_mask_for_vis, alpha=alpha_blend,\n                                         clicks_list=self.clicker.clicks_list, radius=click_radius)\n        if self.probs_history:\n            total_mask = self.probs_history[-1][0] > self.prob_thresh\n            results_mask_for_vis[np.logical_not(total_mask)] = 0\n            vis = draw_with_blend_and_clicks(vis, mask=results_mask_for_vis, alpha=alpha_blend)\n\n        return vis\n"""
interactive_demo/wrappers.py,0,"b'import tkinter as tk\nfrom tkinter import messagebox, ttk\n\n\nclass BoundedNumericalEntry(tk.Entry):\n    def __init__(self, master=None, min_value=None, max_value=None, variable=None,\n                 vartype=float, width=7, allow_inf=False,  **kwargs):\n        if variable is None:\n            if vartype == float:\n                self.var = tk.DoubleVar()\n            elif vartype == int:\n                self.var = tk.IntVar()\n            else:\n                self.var = tk.StringVar()\n        else:\n            self.var = variable\n\n        self.fake_var = tk.StringVar(value=self.var.get())\n        self.vartype = vartype\n        self.old_value = self.var.get()\n        self.allow_inf = allow_inf\n\n        self.min_value, self.max_value = min_value, max_value\n        self.get, self.set = self.fake_var.get, self.fake_var.set\n\n        self.validate_command = master.register(self._check_bounds)\n        tk.Entry.__init__(self, master, textvariable=self.fake_var, validate=""focus"", width=width,\n                          vcmd=(self.validate_command, \'%P\', \'%d\'), **kwargs)\n\n    def _check_bounds(self, instr, action_type):\n        if self.allow_inf and instr == \'INF\':\n            self.fake_var.set(\'INF\')\n            return True\n\n        if action_type == \'-1\':\n            try:\n                new_value = self.vartype(instr)\n            except ValueError:\n                pass\n            else:\n                if (self.min_value is None or new_value >= self.min_value) and \\\n                        (self.max_value is None or new_value <= self.max_value):\n                    if new_value != self.old_value:\n                        self.old_value = self.vartype(self.fake_var.get())\n                        self.delete(0, tk.END)\n                        self.insert(0, str(self.old_value))\n                        self.var.set(self.old_value)\n                    return True\n        self.delete(0, tk.END)\n        self.insert(0, str(self.old_value))\n        mn = \'-inf\' if self.min_value is None else str(self.min_value)\n        mx = \'+inf\' if self.max_value is None else str(self.max_value)\n        messagebox.showwarning(""Incorrect value in input field"", f""Value for {self._name} should be in ""\n                               f""[{mn}; {mx}] and of type {self.vartype.__name__}"")\n\n        return False\n\n\nclass FocusHorizontalScale(tk.Scale):\n    def __init__(self, *args, highlightthickness=0, sliderrelief=tk.GROOVE, resolution=0.01,\n                 sliderlength=20, length=200, **kwargs):\n        tk.Scale.__init__(self, *args, orient=tk.HORIZONTAL, highlightthickness=highlightthickness,\n                          sliderrelief=sliderrelief, resolution=resolution,\n                          sliderlength=sliderlength, length=length, **kwargs)\n        self.bind(""<1>"", lambda event: self.focus_set())\n\n\nclass FocusCheckButton(tk.Checkbutton):\n    def __init__(self, *args, highlightthickness=0, **kwargs):\n        tk.Checkbutton.__init__(self, *args, highlightthickness=highlightthickness, **kwargs)\n        self.bind(""<1>"", lambda event: self.focus_set())\n\n\nclass FocusButton(tk.Button):\n    def __init__(self, *args, highlightthickness=0, **kwargs):\n        tk.Button.__init__(self, *args, highlightthickness=highlightthickness, **kwargs)\n        self.bind(""<1>"", lambda event: self.focus_set())\n\n\nclass FocusLabelFrame(ttk.LabelFrame):\n    def __init__(self, *args, highlightthickness=0, relief=tk.RIDGE, borderwidth=2, **kwargs):\n        tk.LabelFrame.__init__(self, *args, highlightthickness=highlightthickness, relief=relief,\n                               borderwidth=borderwidth, **kwargs)\n        self.bind(""<1>"", lambda event: self.focus_set())\n\n    def set_frame_state(self, state):\n        def set_widget_state(widget, state):\n            if widget.winfo_children is not None:\n                for w in widget.winfo_children():\n                    w.configure(state=state)\n\n        set_widget_state(self, state)\n'"
scripts/convert_weights_mx2pt.py,2,"b""import argparse\nfrom pathlib import Path\nfrom collections import OrderedDict\nimport mxnet as mx\nimport torch\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('checkpoint', type=str, help='The path to the MXNet checkpoint.')\n\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    mx_weights = mx.nd.load(args.checkpoint)\n\n    pt_weights = OrderedDict()\n    for weight_name, data in mx_weights.items():\n        pt_weight = torch.tensor(data.asnumpy(), device='cuda:0')\n        pt_name = convert_mx2pt(weight_name, mx_weights)\n        pt_weights[pt_name] = pt_weight\n\n    mx_weights_path = Path(args.checkpoint)\n    pt_weights_path = mx_weights_path.parent / (mx_weights_path.stem + '.pth')\n    with open(pt_weights_path, 'wb') as f:\n        torch.save(pt_weights, f)\n    print(f'Converted weights saved to {pt_weights_path}')\n\n\ndef convert_mx2pt(x, weights_names):\n    if x.endswith('.beta') and x[:-4] + 'running_var' in weights_names:\n        x = x.replace('.beta', '.bias')\n    if x.endswith('.gamma') and x[:-5] + 'running_var' in weights_names:\n        x = x.replace('.gamma', '.weight')\n\n    return x\n\n\nif __name__ == '__main__':\n    main()\n"""
scripts/evaluate_model.py,2,"b'import sys\nimport pickle\nimport argparse\nfrom pathlib import Path\n\nimport torch\nimport numpy as np\n\nsys.path.insert(0, \'.\')\nfrom isegm.inference import utils\nfrom isegm.utils.exp import load_config_file\nfrom isegm.inference.predictors import get_predictor\nfrom isegm.inference.evaluation import evaluate_dataset\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'mode\', choices=[\'NoBRS\', \'RGB-BRS\', \'DistMap-BRS\',\n                                         \'f-BRS-A\', \'f-BRS-B\', \'f-BRS-C\'],\n                        help=\'\')\n    parser.add_argument(\'--checkpoint\', type=str, required=True,\n                        help=\'The path to the checkpoint. \'\n                             \'This can be a relative path (relative to cfg.INTERACTIVE_MODELS_PATH) \'\n                             \'or an absolute path. The file extension can be omitted.\')\n    parser.add_argument(\'--datasets\', type=str, default=\'GrabCut,Berkeley,DAVIS,COCO_MVal,SBD\',\n                        help=\'List of datasets on which the model should be tested. \'\n                             \'Datasets are separated by a comma. Possible choices: \'\n                             \'GrabCut, Berkeley, DAVIS, COCO_MVal, SBD\')\n    parser.add_argument(\'--n-clicks\', type=int, default=20,\n                        help=\'Maximum number of clicks for the NoC metric.\')\n    parser.add_argument(\'--gpus\', type=str, default=\'0\',\n                        help=\'ID of used GPU.\')\n    parser.add_argument(\'--cpu\', action=\'store_true\', default=False,\n                        help=\'Use only CPU for inference.\')\n    parser.add_argument(\'--thresh\', type=float, required=False, default=0.49,\n                        help=\'The segmentation mask is obtained from the probability outputs using this threshold.\')\n    parser.add_argument(\'--target-iou\', type=float, default=0.90,\n                        help=\'Target IoU threshold for the NoC metric. (min possible value = 0.8)\')\n    parser.add_argument(\'--norm-radius\', type=int, default=260)\n    parser.add_argument(\'--clicks-limit\', type=int, default=None)\n    parser.add_argument(\'--config-path\', type=str, default=\'./config.yml\',\n                        help=\'The path to the config file.\')\n    parser.add_argument(\'--logs-path\', type=str, default=\'\',\n                        help=\'The path to the evaluation logs. Default path: cfg.EXPS_PATH/evaluation_logs.\')\n\n    args = parser.parse_args()\n    if args.cpu:\n        args.device = torch.device(\'cpu\')\n    else:\n        args.device = torch.device(f""cuda:{args.gpus.split(\',\')[0]}"")\n    args.target_iou = max(0.8, args.target_iou)\n\n    cfg = load_config_file(args.config_path, return_edict=True)\n\n    if args.logs_path == \'\':\n        args.logs_path = Path(cfg.EXPS_PATH) / \'evaluation_logs\'\n    else:\n        args.logs_path = Path(args.logs_path)\n\n    return args, cfg\n\n\ndef main():\n    args, cfg = parse_args()\n\n    checkpoint_path = utils.find_checkpoint(cfg.INTERACTIVE_MODELS_PATH, args.checkpoint)\n    model = utils.load_is_model(checkpoint_path, args.device, norm_radius=args.norm_radius)\n\n    eval_exp_name = get_eval_exp_name(args)\n    eval_exp_path = args.logs_path / eval_exp_name\n    eval_exp_path.mkdir(parents=True, exist_ok=True)\n    predictor_params = None\n    if args.clicks_limit is not None:\n        if args.clicks_limit == -1:\n            args.clicks_limit = args.n_clicks\n        predictor_params = {\'net_clicks_limit\': args.clicks_limit}\n\n    print_header = True\n    for dataset_name in args.datasets.split(\',\'):\n        dataset = utils.get_dataset(dataset_name, cfg)\n\n        zoom_in_target_size = 600 if dataset_name == \'DAVIS\' else 400\n        predictor = get_predictor(model, args.mode, args.device,\n                                  prob_thresh=args.thresh,\n                                  zoom_in_params={\'target_size\': zoom_in_target_size},\n                                  predictor_params=predictor_params)\n\n        dataset_results = evaluate_dataset(dataset, predictor, pred_thr=args.thresh,\n                                           max_iou_thr=args.target_iou,\n                                           max_clicks=args.n_clicks)\n\n        save_results(args, dataset_name, eval_exp_path, dataset_results,\n                     print_header=print_header)\n        print_header = False\n\n\ndef get_eval_exp_name(args):\n    if \':\' in args.checkpoint:\n        model_name, checkpoint_prefix = args.checkpoint.split(\':\')\n        model_name = model_name.split(\'/\')[-1]\n\n        return f""{model_name}_{checkpoint_prefix}""\n    else:\n        return Path(args.checkpoint).stem\n\n\ndef save_results(args, dataset_name, eval_exp_path, dataset_results, print_header=True):\n    all_ious, elapsed_time = dataset_results\n    mean_spc, mean_spi = utils.get_time_metrics(all_ious, elapsed_time)\n\n    iou_thrs = np.arange(0.8, min(0.95, args.target_iou) + 0.001, 0.05).tolist()\n    noc_list, over_max_list = utils.compute_noc_metric(all_ious, iou_thrs=iou_thrs, max_clicks=args.n_clicks)\n    header, table_row = utils.get_results_table(noc_list, over_max_list, args.mode, dataset_name,\n                                                mean_spc, elapsed_time, args.n_clicks,\n                                                model_name=eval_exp_path.stem)\n    target_iou_int = int(args.target_iou * 100)\n    if target_iou_int not in [80, 85, 90]:\n        noc_list, over_max_list = utils.compute_noc_metric(all_ious, iou_thrs=[args.target_iou],\n                                                           max_clicks=args.n_clicks)\n        table_row += f\' NoC@{args.target_iou:.1%} = {noc_list[0]:.2f};\'\n        table_row += f\' >={args.n_clicks}@{args.target_iou:.1%} = {over_max_list[0]}\'\n\n    if print_header:\n        print(header)\n    print(table_row)\n\n    log_path = eval_exp_path / f\'results_{args.mode}_{args.n_clicks}.txt\'\n    if log_path.exists():\n        with open(log_path, \'a\') as f:\n            f.write(table_row + \'\\n\')\n    else:\n        with open(log_path, \'w\') as f:\n            f.write(header + \'\\n\')\n            f.write(table_row + \'\\n\')\n\n    ious_path = eval_exp_path / \'all_ious\'\n    ious_path.mkdir(exist_ok=True)\n    with open(ious_path / f\'{dataset_name}_{args.mode}_{args.n_clicks}.pkl\', \'wb\') as fp:\n        pickle.dump(all_ious, fp)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
isegm/data/base.py,1,"b""import random\nimport pickle\n\nimport cv2\nimport numpy as np\nimport torch\nfrom torchvision import transforms\n\nfrom .points_sampler import SinglePointSampler\n\n\nclass ISDataset(torch.utils.data.dataset.Dataset):\n    def __init__(self,\n                 points_from_one_object=True,\n                 augmentator=None,\n                 num_masks=1,\n                 points_sampler=SinglePointSampler(ignore_object_prob=0.0),\n                 input_transform=None,\n                 image_rescale=None,\n                 min_object_area=0,\n                 min_ignore_object_area=10,\n                 keep_background_prob=0.0,\n                 with_image_info=False,\n                 samples_scores_path=None,\n                 samples_scores_gamma=1.0,\n                 zoom_in=None,\n                 epoch_len=-1):\n        super(ISDataset, self).__init__()\n        self.epoch_len = epoch_len\n        self.num_masks = num_masks\n        self.points_from_one_object = points_from_one_object\n        self.input_transform = input_transform\n        self.augmentator = augmentator\n        self.image_rescale = image_rescale\n        self.min_object_area = min_object_area\n        self.min_ignore_object_area = min_ignore_object_area\n        self.keep_background_prob = keep_background_prob\n        self.points_sampler = points_sampler\n        self.with_image_info = with_image_info\n        self.samples_precomputed_scores = self._load_samples_scores(samples_scores_path, samples_scores_gamma)\n        self.zoom_in = zoom_in\n        if isinstance(self.image_rescale, (float, int)):\n            scale = self.image_rescale\n            self.image_rescale = lambda shape: scale\n\n        if input_transform is None:\n            input_transform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n            ])\n        self.input_transform = input_transform\n\n        self.dataset_samples = None\n        self._precise_masks = ['instances_mask']\n        self._from_dataset_mapping = None\n        self._to_dataset_mapping = None\n\n    def __getitem__(self, index):\n        if self.samples_precomputed_scores is not None:\n            index = np.random.choice(self.samples_precomputed_scores['indices'],\n                                     p=self.samples_precomputed_scores['probs'])\n        else:\n            if self.epoch_len > 0:\n                index = random.randrange(0, len(self.dataset_samples))\n\n        sample = self.get_sample(index)\n        self.check_sample_types(sample)\n\n        sample = self.rescale_sample(sample)\n        sample, use_zoom_in = self.check_zoom_in(sample)\n        sample = self.augment_sample(sample, use_zoom_in=use_zoom_in)\n\n        if use_zoom_in:\n            sample = self.zoom_in(sample)\n        else:\n            sample = self.exclude_small_objects(sample)\n\n        sample['objects_ids'] = [obj_id for obj_id, obj_info in sample['instances_info'].items()\n                                 if not obj_info['ignore']]\n\n        points, masks = [], []\n        self.points_sampler.sample_object(sample)\n        for i in range(self.num_masks):\n            points.extend(self.points_sampler.sample_points())\n            masks.append(self.points_sampler.selected_mask)\n            if not self.points_from_one_object:\n                self.points_sampler.sample_object(sample)\n\n        masks = np.array(masks)\n        points = np.array(points, dtype=np.float32)\n\n        image = self.input_transform(sample['image'])\n\n        output = {\n            'images': image,\n            'points': points.astype(np.float32),\n            'instances': masks\n        }\n\n        if self.with_image_info and 'image_id' in sample:\n            output['image_info'] = sample['image_id']\n\n        return output\n\n    def check_sample_types(self, sample):\n        assert sample['image'].dtype == 'uint8'\n        assert sample['instances_mask'].dtype == 'int32'\n\n    def rescale_sample(self, sample):\n        if self.image_rescale is None:\n            return sample\n\n        image = sample['image']\n        scale = self.image_rescale(image.shape)\n        image = cv2.resize(image, (0, 0), fx=scale, fy=scale)\n        new_size = (image.shape[1], image.shape[0])\n\n        sample['image'] = image\n        for mask_name in self._precise_masks:\n            if mask_name not in sample:\n                continue\n            sample[mask_name] = cv2.resize(sample[mask_name], new_size,\n                                           interpolation=cv2.INTER_NEAREST)\n\n        return sample\n\n    def check_zoom_in(self, sample):\n        use_zoom_in = self.zoom_in is not None and random.random() < self.zoom_in.p\n        if use_zoom_in:\n            sample = self.exclude_small_objects(sample)\n            num_objects = len([x for x in sample['instances_info'].values() if not x['ignore']])\n            if num_objects == 0:\n                use_zoom_in = False\n\n        return sample, use_zoom_in\n\n    def augment_sample(self, sample, use_zoom_in=False):\n        augmentator = self.augmentator if not use_zoom_in else self.zoom_in.augmentator\n        if augmentator is None:\n            return sample\n\n        masks_to_augment = [mask_name for mask_name in self._precise_masks if mask_name in sample]\n        masks = [sample[mask_name] for mask_name in masks_to_augment]\n\n        valid_augmentation = False\n        while not valid_augmentation:\n            aug_output = augmentator(image=sample['image'], masks=masks)\n            valid_augmentation = self.check_augmented_sample(sample, aug_output, masks_to_augment)\n\n        sample['image'] = aug_output['image']\n        for mask_name, mask in zip(masks_to_augment, aug_output['masks']):\n            sample[mask_name] = mask\n\n        sample_ids = set(get_unique_labels(sample['instances_mask'], exclude_zero=True))\n        instances_info = sample['instances_info']\n        instances_info = {sample_id: sample_info for sample_id, sample_info in instances_info.items()\n                          if sample_id in sample_ids}\n        sample['instances_info'] = instances_info\n\n        return sample\n\n    def check_augmented_sample(self, sample, aug_output, masks_to_augment):\n        if self.keep_background_prob < 0.0 or random.random() < self.keep_background_prob:\n            return True\n\n        aug_instances_mask = aug_output['masks'][masks_to_augment.index('instances_mask')]\n        aug_sample_ids = set(get_unique_labels(aug_instances_mask, exclude_zero=True))\n        num_objects_after_aug = len([obj_id for obj_id in aug_sample_ids\n                                     if not sample['instances_info'][obj_id]['ignore']])\n\n        return num_objects_after_aug > 0\n\n    def exclude_small_objects(self, sample):\n        if self.min_object_area <= 0:\n            return sample\n\n        for obj_id, obj_info in sample['instances_info'].items():\n            if not obj_info['ignore']:\n                obj_area = (sample['instances_mask'] == obj_id).sum()\n                if obj_area < self.min_object_area:\n                    obj_info['ignore'] = True\n\n        return sample\n\n    @staticmethod\n    def _load_samples_scores(samples_scores_path, samples_scores_gamma):\n        if samples_scores_path is None:\n            return None\n\n        with open(samples_scores_path, 'rb') as f:\n            images_scores = pickle.load(f)\n\n        probs = np.array([(1.0 - x[2]) ** samples_scores_gamma for x in images_scores])\n        probs /= probs.sum()\n        samples_scores = {\n            'indices': [x[0] for x in images_scores],\n            'probs': probs\n        }\n        print(f'Loaded {len(probs)} weights with gamma={samples_scores_gamma}')\n        return samples_scores\n\n    def get_sample(self, index):\n        raise NotImplementedError\n\n    def __len__(self):\n        if self.epoch_len > 0:\n            return self.epoch_len\n        else:\n            return len(self.dataset_samples)\n\n\ndef get_unique_labels(x, exclude_zero=False):\n    obj_sizes = np.bincount(x.flatten())\n    labels = np.nonzero(obj_sizes)[0].tolist()\n\n    if exclude_zero:\n        labels = [x for x in labels if x != 0]\n    return labels\n"""
isegm/data/berkeley.py,0,"b""from .grabcut import GrabCutDataset\n\n\nclass BerkeleyDataset(GrabCutDataset):\n    def __init__(self, dataset_path, **kwargs):\n        super().__init__(dataset_path, images_dir_name='images', masks_dir_name='masks', **kwargs)\n"""
isegm/data/davis.py,0,"b""from pathlib import Path\n\nimport cv2\nimport numpy as np\n\nfrom .base import ISDataset\n\n\nclass DavisDataset(ISDataset):\n    def __init__(self, dataset_path,\n                 images_dir_name='img', masks_dir_name='gt',\n                 **kwargs):\n        super(DavisDataset, self).__init__(**kwargs)\n\n        self.dataset_path = Path(dataset_path)\n        self._images_path = self.dataset_path / images_dir_name\n        self._insts_path = self.dataset_path / masks_dir_name\n\n        self.dataset_samples = [x.name for x in sorted(self._images_path.glob('*.*'))]\n        self._masks_paths = {x.stem: x for x in self._insts_path.glob('*.*')}\n\n    def get_sample(self, index):\n        image_name = self.dataset_samples[index]\n        image_path = str(self._images_path / image_name)\n        mask_path = str(self._masks_paths[image_name.split('.')[0]])\n\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        instances_mask = np.max(cv2.imread(mask_path).astype(np.int32), axis=2)\n        instances_mask[instances_mask > 0] = 1\n\n        instances_ids = [1]\n\n        instances_info = {\n            x: {'ignore': False}\n            for x in instances_ids\n        }\n\n        return {\n            'image': image,\n            'instances_mask': instances_mask,\n            'instances_info': instances_info,\n            'image_id': index\n        }\n"""
isegm/data/grabcut.py,0,"b""from pathlib import Path\n\nimport cv2\nimport numpy as np\n\nfrom .base import ISDataset\n\n\nclass GrabCutDataset(ISDataset):\n    def __init__(self, dataset_path,\n                 images_dir_name='data_GT', masks_dir_name='boundary_GT',\n                 **kwargs):\n        super(GrabCutDataset, self).__init__(**kwargs)\n\n        self.dataset_path = Path(dataset_path)\n        self._images_path = self.dataset_path / images_dir_name\n        self._insts_path = self.dataset_path / masks_dir_name\n\n        self.dataset_samples = [x.name for x in sorted(self._images_path.glob('*.*'))]\n        self._masks_paths = {x.stem: x for x in self._insts_path.glob('*.*')}\n\n    def get_sample(self, index):\n        image_name = self.dataset_samples[index]\n        image_path = str(self._images_path / image_name)\n        mask_path = str(self._masks_paths[image_name.split('.')[0]])\n\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        instances_mask = cv2.imread(mask_path)[:, :, 0].astype(np.int32)\n        instances_mask[instances_mask == 128] = -1\n        instances_mask[instances_mask > 128] = 1\n\n        instances_ids = [1]\n\n        instances_info = {\n            x: {'ignore': False}\n            for x in instances_ids\n        }\n\n        return {\n            'image': image,\n            'instances_mask': instances_mask,\n            'instances_info': instances_info,\n            'image_id': index\n        }\n"""
isegm/data/lvis.py,0,"b""import json\nimport random\nfrom collections import defaultdict\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\n\nfrom .base import ISDataset\n\n\nclass LvisDataset(ISDataset):\n    def __init__(self, dataset_path, split='train',\n                 max_overlap_ratio=0.5,\n                 **kwargs):\n        super(LvisDataset, self).__init__(**kwargs)\n        dataset_path = Path(dataset_path)\n        train_categories_path = dataset_path / 'train_categories.json'\n        self._split_path = dataset_path / split\n        self.split = split\n        self.max_overlap_ratio = max_overlap_ratio\n\n        with open(self._split_path / f'lvis_{self.split}.json', 'r') as f:\n            json_annotation = json.loads(f.read())\n\n        self.annotations = defaultdict(list)\n        for x in json_annotation['annotations']:\n            self.annotations[x['image_id']].append(x)\n\n        if not train_categories_path.exists():\n            self.generate_train_categories(dataset_path, train_categories_path)\n        self.dataset_samples = [x for x in json_annotation['images']\n                                if len(self.annotations[x['id']]) > 0]\n\n    def get_sample(self, index):\n        image_info = self.dataset_samples[index]\n        image_id, image_filename = image_info['id'], image_info['file_name']\n        image_filename = image_filename.split('_')[-1]\n        image_annotations = self.annotations[image_id]\n        random.shuffle(image_annotations)\n\n        image_path = self._split_path / 'images' / image_filename\n        image = cv2.imread(str(image_path))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        instances_mask = None\n        masks_info = {}\n        instances_area = defaultdict(int)\n        for indx, obj_annotation in enumerate(image_annotations):\n            mask = self.get_mask_from_polygon(obj_annotation, image)\n            object_mask = mask > 0\n            object_area = object_mask.sum()\n\n            if instances_mask is None:\n                instances_mask = np.zeros_like(object_mask, dtype=np.int32)\n\n            overlap_ids = np.bincount(instances_mask[object_mask].flatten())\n            overlap_areas = [overlap_area / instances_area[inst_id] for inst_id, overlap_area in enumerate(overlap_ids)\n                             if overlap_area > 0 and inst_id > 0]\n            overlap_ratio = np.logical_and(object_mask, instances_mask > 0).sum() / object_area\n            if overlap_areas:\n                overlap_ratio = max(overlap_ratio, max(overlap_areas))\n            if overlap_ratio > self.max_overlap_ratio:\n                continue\n\n            instance_id = indx + 1\n            instances_mask[object_mask] = instance_id\n            instances_area[instance_id] = object_area\n\n            masks_info[instance_id] = {\n                'ignore': False\n            }\n\n        return {\n            'image': image,\n            'instances_mask': instances_mask,\n            'instances_info': masks_info\n        }\n\n    @staticmethod\n    def get_mask_from_polygon(annotation, image):\n        mask = np.zeros(image.shape[:2], dtype=np.int32)\n        for contour_points in annotation['segmentation']:\n            contour_points = np.array(contour_points).reshape((-1, 2))\n            contour_points = np.round(contour_points).astype(np.int32)[np.newaxis, :]\n            cv2.fillPoly(mask, contour_points, 1)\n\n        return mask\n\n    @staticmethod\n    def generate_train_categories(dataset_path, train_categories_path):\n        with open(dataset_path / 'train/lvis_train.json', 'r') as f:\n            annotation = json.load(f)\n\n        with open(train_categories_path, 'w') as f:\n            json.dump(annotation['categories'], f, indent=1)\n"""
isegm/data/points_sampler.py,0,"b""import random\n\nimport cv2\nimport numpy as np\n\n\nclass BasePointSampler(object):\n    def __init__(self, ignore_object_prob=0.0):\n        self.ignore_object_prob = ignore_object_prob\n        self._selected_mask = None\n        self._selected_indices = None\n        self._is_selected_ignore = False\n\n    def sample_object(self, dataset_sample):\n        raise NotImplementedError\n\n    def sample_points(self):\n        raise NotImplementedError\n\n    @property\n    def selected_mask(self):\n        assert self._selected_mask is not None\n        return self._selected_mask\n\n    @selected_mask.setter\n    def selected_mask(self, mask):\n        if self._is_selected_ignore:\n            mask = mask[mask > 0.5] = -1\n        self._selected_mask = mask[np.newaxis, :].astype(np.float32)\n\n\nclass SinglePointSampler(BasePointSampler):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def sample_object(self, dataset_sample):\n        if len(dataset_sample['objects_ids']) == 0:\n            self._selected_mask = np.full(dataset_sample['instances_mask'].shape[:2], -1, dtype=np.float32)\n            self._selected_indices = None\n            self._is_selected_ignore = False\n        else:\n            if dataset_sample.get('ignore_ids', []) and random.random() < self.ignore_object_prob:\n                random_id = random.choice(dataset_sample['ignore_ids'])\n                mask = dataset_sample['ignore_mask'] == random_id\n                self._is_selected_ignore = True\n            else:\n                random_id = random.choice(dataset_sample['objects_ids'])\n                mask = dataset_sample['instances_mask'] == random_id\n                self._is_selected_ignore = False\n\n            self._selected_indices = np.argwhere(mask)\n            self.selected_mask = mask\n\n    def sample_points(self):\n        assert self._selected_mask is not None\n        if self._selected_indices is None:\n            return [(-1, -1)]\n        else:\n            num_indices = self._selected_indices.shape[0]\n            point = self._selected_indices[np.random.randint(0, num_indices)]\n            return [point]\n\n\nclass MultiPointSampler(BasePointSampler):\n    def __init__(self, max_num_points, prob_gamma=0.7, expand_ratio=0.1,\n                 positive_erode_prob=0.9, positive_erode_iters=3,\n                 negative_bg_prob=0.1, negative_other_prob=0.4, negative_border_prob=0.5,\n                 merge_objects_prob=0.0, max_num_merged_objects=2,\n                 **kwargs):\n        kwargs['ignore_object_prob'] = 0.0\n        super().__init__(**kwargs)\n        self.max_num_points = max_num_points\n        self.expand_ratio = expand_ratio\n        self.positive_erode_prob = positive_erode_prob\n        self.positive_erode_iters = positive_erode_iters\n        self.merge_objects_prob = merge_objects_prob\n        if max_num_merged_objects == -1:\n            max_num_merged_objects = max_num_points\n        self.max_num_merged_objects = max_num_merged_objects\n\n        self.neg_strategies = ['bg', 'other', 'border']\n        self.neg_strategies_prob = [negative_bg_prob, negative_other_prob, negative_border_prob]\n        assert sum(self.neg_strategies_prob) == 1.0\n\n        self._pos_probs = self._generate_probs(max_num_points, gamma=prob_gamma)\n        self._neg_probs = self._generate_probs(max_num_points + 1, gamma=prob_gamma)\n        self._neg_indices = None\n\n    def sample_object(self, dataset_sample):\n        if len(dataset_sample['objects_ids']) == 0:\n            self.selected_mask = np.zeros_like(dataset_sample['instances_mask'])\n            self._selected_indices = [[]]\n            bg_indices = np.argwhere(dataset_sample['instances_mask'] == 0)\n            self._neg_indices = {strategy: bg_indices for strategy in self.neg_strategies}\n            return\n\n        if len(dataset_sample['objects_ids']) > 1 and random.random() < self.merge_objects_prob:\n            max_selected_objects = min(len(dataset_sample['objects_ids']), self.max_num_merged_objects)\n            num_selected_objects = np.random.randint(2, max_selected_objects + 1)\n\n            random_ids = random.sample(dataset_sample['objects_ids'], num_selected_objects)\n            masks = [dataset_sample['instances_mask'] == obj_id for obj_id in random_ids]\n            self._selected_indices = [np.argwhere(self._positive_erode(x)) for x in masks]\n            mask = masks[0]\n            for x in masks[1:]:\n                mask = np.logical_or(mask, x)\n        else:\n            random_id = random.choice(dataset_sample['objects_ids'])\n            mask = dataset_sample['instances_mask'] == random_id\n            self._selected_indices = [np.argwhere(self._positive_erode(mask))]\n\n        self.selected_mask = mask\n        neg_indices_bg = np.argwhere(np.logical_not(mask))\n        neg_indices_border = np.argwhere(self._get_border_mask(mask))\n        if len(dataset_sample['objects_ids']) <= len(self._selected_indices):\n            neg_indices_other = neg_indices_bg\n        else:\n            other_objects_mask = np.logical_and(dataset_sample['instances_mask'] > 0,\n                                                np.logical_not(mask))\n            neg_indices_other = np.argwhere(other_objects_mask)\n\n        self._neg_indices = {\n            'bg': neg_indices_bg,\n            'other': neg_indices_other,\n            'border': neg_indices_border\n        }\n\n    def sample_points(self):\n        assert self._selected_mask is not None\n        if len(self._selected_indices) == 1:\n            pos_points = self._sample_points(self._selected_indices[0], is_negative=False)\n        else:\n            each_obj_points = [self._sample_points(indices, is_negative=False)\n                               for indices in self._selected_indices]\n            pos_points = [obj_points[0] for obj_points in each_obj_points]\n\n            other_points_union = []\n            for obj_points in each_obj_points:\n                other_points_union.extend([t for t in obj_points[1:] if t[0] >= 0])\n\n            num_additional_points = min(len(other_points_union), self.max_num_points - len(pos_points))\n            if num_additional_points > 0:\n                additional_points = random.sample(other_points_union, num_additional_points)\n                assert num_additional_points + len(pos_points) <= self.max_num_points\n                pos_points.extend(additional_points)\n\n            random.shuffle(pos_points)\n            if len(pos_points) < self.max_num_points:\n                pos_points.extend([(-1, -1)] * (self.max_num_points - len(pos_points)))\n\n        negative_strategy = np.random.choice(self.neg_strategies, p=self.neg_strategies_prob)\n        neg_points = self._sample_points(self._neg_indices[negative_strategy], is_negative=True)\n\n        return pos_points + neg_points\n\n    def _sample_points(self, indices, is_negative=False):\n        if is_negative:\n            num_points = np.random.choice(np.arange(self.max_num_points + 1), p=self._neg_probs)\n        else:\n            num_points = 1 + np.random.choice(np.arange(self.max_num_points), p=self._pos_probs)\n\n        points = []\n        num_indices = len(indices)\n        for j in range(self.max_num_points):\n            point_coord = indices[np.random.randint(0, num_indices)] if j < num_points and num_indices > 0 else (-1, -1)\n            points.append(point_coord)\n\n        return points\n\n    def _positive_erode(self, mask):\n        if random.random() > self.positive_erode_prob:\n            return mask\n\n        kernel = np.ones((3, 3), np.uint8)\n        eroded_mask = cv2.erode(mask.astype(np.uint8),\n                                kernel, iterations=self.positive_erode_iters).astype(np.bool)\n\n        if eroded_mask.sum() > 10:\n            return eroded_mask\n        else:\n            return mask\n\n    def _get_border_mask(self, mask):\n        expand_r = int(np.ceil(self.expand_ratio * np.sqrt(mask.sum())))\n        kernel = np.ones((3, 3), np.uint8)\n        expanded_mask = cv2.dilate(mask.astype(np.uint8), kernel, iterations=expand_r)\n        expanded_mask[mask] = 0\n        return expanded_mask\n\n    @staticmethod\n    def _generate_probs(max_num_points, gamma):\n        probs = []\n        last_value = 1\n        for i in range(max_num_points):\n            probs.append(last_value)\n            last_value *= gamma\n\n        probs = np.array(probs)\n        probs /= probs.sum()\n\n        return probs\n"""
isegm/data/sbd.py,0,"b""import pickle as pkl\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nfrom scipy.io import loadmat\n\nfrom isegm.utils.misc import get_bbox_from_mask\nfrom .base import ISDataset, get_unique_labels\n\n\nclass SBDDataset(ISDataset):\n    def __init__(self, dataset_path, split='train', buggy_mask_thresh=0.08, **kwargs):\n        super(SBDDataset, self).__init__(**kwargs)\n        assert split in {'train', 'val'}\n\n        self.dataset_path = Path(dataset_path)\n        self.dataset_split = split\n        self._images_path = self.dataset_path / 'img'\n        self._insts_path = self.dataset_path / 'inst'\n        self._buggy_objects = dict()\n        self._buggy_mask_thresh = buggy_mask_thresh\n\n        with open(self.dataset_path / f'{split}.txt', 'r') as f:\n            self.dataset_samples = [x.strip() for x in f.readlines()]\n\n    def get_sample(self, index):\n        image_name = self.dataset_samples[index]\n        image_path = str(self._images_path / f'{image_name}.jpg')\n        inst_info_path = str(self._insts_path / f'{image_name}.mat')\n\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        instances_mask = loadmat(str(inst_info_path))['GTinst'][0][0][0].astype(np.int32)\n        instances_mask = self.remove_buggy_masks(index, instances_mask)\n        instances_ids = get_unique_labels(instances_mask, exclude_zero=True)\n\n        instances_info = {\n            x: {'ignore': False}\n            for x in instances_ids\n        }\n\n        return {\n            'image': image,\n            'instances_mask': instances_mask,\n            'instances_info': instances_info,\n            'image_id': index\n        }\n\n    def remove_buggy_masks(self, index, instances_mask):\n        if self._buggy_mask_thresh > 0.0:\n            buggy_image_objects = self._buggy_objects.get(index, None)\n            if buggy_image_objects is None:\n                buggy_image_objects = []\n                instances_ids = get_unique_labels(instances_mask, exclude_zero=True)\n                for obj_id in instances_ids:\n                    obj_mask = instances_mask == obj_id\n                    mask_area = obj_mask.sum()\n                    bbox = get_bbox_from_mask(obj_mask)\n                    bbox_area = (bbox[1] - bbox[0] + 1) * (bbox[3] - bbox[2] + 1)\n                    obj_area_ratio = mask_area / bbox_area\n                    if obj_area_ratio < self._buggy_mask_thresh:\n                        buggy_image_objects.append(obj_id)\n\n                self._buggy_objects[index] = buggy_image_objects\n            for obj_id in buggy_image_objects:\n                instances_mask[instances_mask == obj_id] = 0\n\n        return instances_mask\n\n\nclass SBDEvaluationDataset(ISDataset):\n    def __init__(self, dataset_path, split='val', **kwargs):\n        super(SBDEvaluationDataset, self).__init__(**kwargs)\n        assert split in {'train', 'val'}\n\n        self.dataset_path = Path(dataset_path)\n        self.dataset_split = split\n        self._images_path = self.dataset_path / 'img'\n        self._insts_path = self.dataset_path / 'inst'\n\n        with open(self.dataset_path / f'{split}.txt', 'r') as f:\n            self.dataset_samples = [x.strip() for x in f.readlines()]\n\n        self.dataset_samples = self.get_sbd_images_and_ids_list()\n\n    def get_sample(self, index):\n        image_name, instance_id = self.dataset_samples[index]\n        image_path = str(self._images_path / f'{image_name}.jpg')\n        inst_info_path = str(self._insts_path / f'{image_name}.mat')\n\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        instances_mask = loadmat(str(inst_info_path))['GTinst'][0][0][0].astype(np.int32)\n        instances_mask[instances_mask != instance_id] = 0\n        instances_mask[instances_mask > 0] = 1\n\n        instances_ids = [1]\n        instances_info = {\n            x: {'ignore': False}\n            for x in instances_ids\n        }\n\n        return {\n            'image': image,\n            'instances_mask': instances_mask,\n            'instances_info': instances_info,\n            'image_id': index\n        }\n\n    def get_sbd_images_and_ids_list(self):\n        pkl_path = self.dataset_path / f'{self.dataset_split}_images_and_ids_list.pkl'\n\n        if pkl_path.exists():\n            with open(str(pkl_path), 'rb') as fp:\n                images_and_ids_list = pkl.load(fp)\n        else:\n            images_and_ids_list = []\n\n            for sample in self.dataset_samples:\n                inst_info_path = str(self._insts_path / f'{sample}.mat')\n                instances_mask = loadmat(str(inst_info_path))['GTinst'][0][0][0].astype(np.int32)\n                instances_ids = get_unique_labels(instances_mask, exclude_zero=True)\n\n                for instances_id in instances_ids:\n                    images_and_ids_list.append((sample, instances_id))\n\n            with open(str(pkl_path), 'wb') as fp:\n                pkl.dump(images_and_ids_list, fp)\n\n        return images_and_ids_list\n"""
isegm/data/zoom_in.py,0,"b""import random\nfrom copy import deepcopy\n\nimport cv2\nimport numpy as np\n\nfrom isegm.utils.misc import get_bbox_from_mask, expand_bbox, clamp_bbox\nfrom albumentations import RandomCrop, CenterCrop\n\n\nclass ZoomIn(object):\n    def __init__(self, augmentator, p=0.2, min_expand=1.25, max_expand=2.0):\n        assert augmentator is not None\n        self.augmentator, self.crop_size = self.get_zoom_in_augmentator(augmentator)\n        self.p = p\n        self.min_expand = min_expand\n        self.max_expand = max_expand\n\n    def __call__(self, sample):\n        instances_info = {\n            obj_id: obj_info for obj_id, obj_info in sample['instances_info'].items()\n            if not obj_info['ignore']\n        }\n\n        obj_id, obj_info = random.choice(list(instances_info.items()))\n        sample['instances_info'] = {obj_id: obj_info}\n        obj_mask = sample['instances_mask'] == obj_id\n\n        crop_height, crop_width = self.crop_size\n\n        obj_bbox = get_bbox_from_mask(obj_mask)\n        obj_bbox = fit_bbox_ratio(obj_bbox, crop_height / crop_width)\n\n        expand_k = np.random.uniform(self.min_expand, self.max_expand)\n        obj_bbox = expand_bbox(obj_bbox, expand_ratio=expand_k)\n        obj_bbox = clamp_bbox(obj_bbox,\n                              0, sample['image'].shape[0] - 1,\n                              0, sample['image'].shape[1] - 1)\n\n        sample['image'] = sample['image'][obj_bbox[0]:obj_bbox[1] + 1, obj_bbox[2]:obj_bbox[3]+1, :]\n        sample['instances_mask'] = sample['instances_mask'][obj_bbox[0]:obj_bbox[1] + 1, obj_bbox[2]:obj_bbox[3]+1]\n\n        sample['image'] = cv2.resize(sample['image'], (crop_width, crop_height))\n        sample['instances_mask'] = cv2.resize(sample['instances_mask'], (crop_width, crop_height),\n                                              interpolation=cv2.INTER_NEAREST)\n\n        return sample\n\n    @staticmethod\n    def get_zoom_in_augmentator(augmentator):\n        crop_augs = (RandomCrop, CenterCrop)\n        zoom_in_augmentator = deepcopy(augmentator)\n        zoom_in_augmentator.transforms = [\n            x for x in zoom_in_augmentator.transforms\n            if not isinstance(x, crop_augs)\n        ]\n\n        crop_height, crop_width = None, None\n        for x in augmentator.transforms:\n            if isinstance(x, crop_augs):\n                crop_height, crop_width = x.height, x.width\n                break\n\n        assert crop_height is not None\n        return zoom_in_augmentator, (crop_height, crop_width)\n\n\ndef fit_bbox_ratio(bbox, target_ratio):\n    rmin, rmax, cmin, cmax = bbox\n\n    bb_rc = 0.5 * (rmax + rmin)\n    bb_cc = 0.5 * (cmax + cmin)\n    bb_height = rmax - rmin + 1\n    bb_width = cmax - cmin + 1\n    bb_ratio = bb_height / bb_width\n\n    if bb_ratio < target_ratio:\n        bb_height = target_ratio * bb_width\n    else:\n        bb_width = bb_height / target_ratio\n\n    rmin = bb_rc - 0.5 * bb_height\n    rmax = bb_rc + 0.5 * bb_height\n    cmin = bb_cc - 0.5 * bb_width\n    cmax = bb_cc + 0.5 * bb_width\n\n    return rmin, rmax, cmin, cmax\n"""
isegm/engine/trainer.py,11,"b'import os\nimport logging\nfrom copy import deepcopy\nfrom collections import defaultdict\n\nimport cv2\nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader\nfrom torchvision.transforms import Normalize\n\nfrom isegm.utils.log import logger, TqdmToLogger, SummaryWriterAvg\nfrom isegm.utils.vis import draw_probmap, draw_points\nfrom isegm.utils.misc import save_checkpoint\n\n\nclass ISTrainer(object):\n    def __init__(self, model, cfg, model_cfg, loss_cfg,\n                 trainset, valset,\n                 optimizer=\'adam\',\n                 optimizer_params=None,\n                 image_dump_interval=200,\n                 checkpoint_interval=10,\n                 tb_dump_period=25,\n                 max_interactive_points=0,\n                 lr_scheduler=None,\n                 metrics=None,\n                 additional_val_metrics=None,\n                 backbone_lr_mult=0.1,\n                 net_inputs=(\'images\', \'points\')):\n        self.cfg = cfg\n        self.model_cfg = model_cfg\n        self.max_interactive_points = max_interactive_points\n        self.loss_cfg = loss_cfg\n        self.val_loss_cfg = deepcopy(loss_cfg)\n        self.tb_dump_period = tb_dump_period\n        self.net_inputs = net_inputs\n\n        if metrics is None:\n            metrics = []\n        self.train_metrics = metrics\n        self.val_metrics = deepcopy(metrics)\n        if additional_val_metrics is not None:\n            self.val_metrics.extend(additional_val_metrics)\n\n        self.checkpoint_interval = checkpoint_interval\n        self.image_dump_interval = image_dump_interval\n        self.task_prefix = \'\'\n        self.sw = None\n\n        self.trainset = trainset\n        self.valset = valset\n\n        self.train_data = DataLoader(\n            trainset, cfg.batch_size, shuffle=True,\n            drop_last=True, pin_memory=True,\n            num_workers=cfg.workers\n        )\n\n        self.val_data = DataLoader(\n            valset, cfg.val_batch_size, shuffle=False,\n            drop_last=True, pin_memory=True,\n            num_workers=cfg.workers\n        )\n\n        backbone_params, other_params = model.get_trainable_params()\n        opt_params = [\n            {\'params\': backbone_params, \'lr\': backbone_lr_mult * optimizer_params[\'lr\']},\n            {\'params\': other_params}\n        ]\n        if optimizer.lower() == \'adam\':\n            self.optim = torch.optim.Adam(opt_params, **optimizer_params)\n        elif optimizer.lower() == \'adamw\':\n            self.optim = torch.optim.AdamW(opt_params, **optimizer_params)\n        elif optimizer.lower() == \'sgd\':\n            self.optim = torch.optim.SGD(opt_params, **optimizer_params)\n        else:\n            raise NotImplementedError\n\n        if cfg.multi_gpu:\n            model = _CustomDP(model, device_ids=cfg.gpu_ids, output_device=cfg.gpu_ids[0])\n\n        logger.info(model)\n        self.device = cfg.device\n        self.net = model.to(self.device)\n        self.lr = optimizer_params[\'lr\']\n\n        if lr_scheduler is not None:\n            self.lr_scheduler = lr_scheduler(optimizer=self.optim)\n            if cfg.start_epoch > 0:\n                for _ in range(cfg.start_epoch):\n                    self.lr_scheduler.step()\n\n        self.tqdm_out = TqdmToLogger(logger, level=logging.INFO)\n        if cfg.input_normalization:\n            mean = torch.tensor(cfg.input_normalization[\'mean\'], dtype=torch.float32)\n            std = torch.tensor(cfg.input_normalization[\'std\'], dtype=torch.float32)\n\n            self.denormalizator = Normalize((-mean / std), (1.0 / std))\n        else:\n            self.denormalizator = lambda x: x\n\n        self._load_weights()\n\n    def training(self, epoch):\n        if self.sw is None:\n            self.sw = SummaryWriterAvg(log_dir=str(self.cfg.LOGS_PATH),\n                                       flush_secs=10, dump_period=self.tb_dump_period)\n\n        log_prefix = \'Train\' + self.task_prefix.capitalize()\n        tbar = tqdm(self.train_data, file=self.tqdm_out, ncols=100)\n        train_loss = 0.0\n\n        for metric in self.train_metrics:\n            metric.reset_epoch_stats()\n\n        self.net.train()\n        for i, batch_data in enumerate(tbar):\n            global_step = epoch * len(self.train_data) + i\n\n            loss, losses_logging, splitted_batch_data, outputs = \\\n                self.batch_forward(batch_data)\n\n            self.optim.zero_grad()\n            loss.backward()\n            self.optim.step()\n\n            batch_loss = loss.item()\n            train_loss += batch_loss\n\n            for loss_name, loss_values in losses_logging.items():\n                self.sw.add_scalar(tag=f\'{log_prefix}Losses/{loss_name}\',\n                                   value=np.array(loss_values).mean(),\n                                   global_step=global_step)\n            self.sw.add_scalar(tag=f\'{log_prefix}Losses/overall\',\n                               value=batch_loss,\n                               global_step=global_step)\n\n            for k, v in self.loss_cfg.items():\n                if \'_loss\' in k and hasattr(v, \'log_states\') and self.loss_cfg.get(k + \'_weight\', 0.0) > 0:\n                    v.log_states(self.sw, f\'{log_prefix}Losses/{k}\', global_step)\n\n            if self.image_dump_interval > 0 and global_step % self.image_dump_interval == 0:\n                self.save_visualization(splitted_batch_data, outputs, global_step, prefix=\'train\')\n\n            self.sw.add_scalar(tag=f\'{log_prefix}States/learning_rate\',\n                               value=self.lr if self.lr_scheduler is None else self.lr_scheduler.get_lr()[-1],\n                               global_step=global_step)\n\n            tbar.set_description(f\'Epoch {epoch}, training loss {train_loss/(i+1):.6f}\')\n            for metric in self.train_metrics:\n                metric.log_states(self.sw, f\'{log_prefix}Metrics/{metric.name}\', global_step)\n\n        for metric in self.train_metrics:\n            self.sw.add_scalar(tag=f\'{log_prefix}Metrics/{metric.name}\',\n                               value=metric.get_epoch_value(),\n                               global_step=epoch, disable_avg=True)\n\n        save_checkpoint(self.net, self.cfg.CHECKPOINTS_PATH, prefix=self.task_prefix,\n                        epoch=None, multi_gpu=self.cfg.multi_gpu)\n        if epoch % self.checkpoint_interval == 0:\n            save_checkpoint(self.net, self.cfg.CHECKPOINTS_PATH, prefix=self.task_prefix,\n                            epoch=epoch, multi_gpu=self.cfg.multi_gpu)\n\n        if self.lr_scheduler is not None:\n            self.lr_scheduler.step()\n\n    def validation(self, epoch):\n        if self.sw is None:\n            self.sw = SummaryWriterAvg(log_dir=str(self.cfg.LOGS_PATH),\n                                       flush_secs=10, dump_period=self.tb_dump_period)\n\n        log_prefix = \'Val\' + self.task_prefix.capitalize()\n        tbar = tqdm(self.val_data, file=self.tqdm_out, ncols=100)\n\n        for metric in self.val_metrics:\n            metric.reset_epoch_stats()\n\n        num_batches = 0\n        val_loss = 0\n        losses_logging = defaultdict(list)\n\n        self.net.eval()\n        for i, batch_data in enumerate(tbar):\n            global_step = epoch * len(self.val_data) + i\n            loss, batch_losses_logging, splitted_batch_data, outputs = \\\n                self.batch_forward(batch_data, validation=True)\n\n            for loss_name, loss_values in batch_losses_logging.items():\n                losses_logging[loss_name].extend(loss_values)\n\n            batch_loss = loss.item()\n            val_loss += batch_loss\n            num_batches += 1\n\n            tbar.set_description(f\'Epoch {epoch}, validation loss: {val_loss/num_batches:.6f}\')\n            for metric in self.val_metrics:\n                metric.log_states(self.sw, f\'{log_prefix}Metrics/{metric.name}\', global_step)\n\n        for loss_name, loss_values in losses_logging.items():\n            self.sw.add_scalar(tag=f\'{log_prefix}Losses/{loss_name}\', value=np.array(loss_values).mean(),\n                               global_step=epoch, disable_avg=True)\n\n        for metric in self.val_metrics:\n            self.sw.add_scalar(tag=f\'{log_prefix}Metrics/{metric.name}\', value=metric.get_epoch_value(),\n                               global_step=epoch, disable_avg=True)\n        self.sw.add_scalar(tag=f\'{log_prefix}Losses/overall\', value=val_loss / num_batches,\n                           global_step=epoch, disable_avg=True)\n\n    def batch_forward(self, batch_data, validation=False):\n        if \'instances\' in batch_data:\n            batch_size, num_points, c, h, w = batch_data[\'instances\'].size()\n            batch_data[\'instances\'] = batch_data[\'instances\'].view(batch_size * num_points, c, h, w)\n        metrics = self.val_metrics if validation else self.train_metrics\n        losses_logging = defaultdict(list)\n        with torch.set_grad_enabled(not validation):\n            batch_data = {k: v.to(self.device) for k, v in batch_data.items()}\n            image, points = batch_data[\'images\'], batch_data[\'points\']\n\n            output = self.net(image, points)\n\n            loss = 0.0\n            loss = self.add_loss(\'instance_loss\', loss, losses_logging, validation,\n                                 lambda: (output[\'instances\'], batch_data[\'instances\']))\n            loss = self.add_loss(\'instance_aux_loss\', loss, losses_logging, validation,\n                                 lambda: (output[\'instances_aux\'], batch_data[\'instances\']))\n            with torch.no_grad():\n                for m in metrics:\n                    m.update(*(output.get(x) for x in m.pred_outputs),\n                             *(batch_data[x] for x in m.gt_outputs))\n        return loss, losses_logging, batch_data, output\n\n    def add_loss(self, loss_name, total_loss, losses_logging, validation, lambda_loss_inputs):\n        loss_cfg = self.loss_cfg if not validation else self.val_loss_cfg\n        loss_weight = loss_cfg.get(loss_name + \'_weight\', 0.0)\n        if loss_weight > 0.0:\n            loss_criterion = loss_cfg.get(loss_name)\n            loss = loss_criterion(*lambda_loss_inputs())\n            loss = torch.mean(loss)\n            losses_logging[loss_name].append(loss.detach().cpu().numpy())\n            loss = loss_weight * loss\n            total_loss = total_loss + loss\n\n        return total_loss\n\n    def save_visualization(self, splitted_batch_data, outputs, global_step, prefix):\n        output_images_path = self.cfg.VIS_PATH / prefix\n        if self.task_prefix:\n            output_images_path /= self.task_prefix\n\n        if not output_images_path.exists():\n            output_images_path.mkdir(parents=True)\n        image_name_prefix = f\'{global_step:06d}\'\n\n        def _save_image(suffix, image):\n            cv2.imwrite(str(output_images_path / f\'{image_name_prefix}_{suffix}.jpg\'),\n                        image, [cv2.IMWRITE_JPEG_QUALITY, 85])\n\n        images = splitted_batch_data[\'images\']\n        points = splitted_batch_data[\'points\']\n        instance_masks = splitted_batch_data[\'instances\']\n\n        image_blob, points = images[0], points[0]\n        image = self.denormalizator(image_blob).cpu().numpy() * 255\n        image = image.transpose((1, 2, 0))\n\n        gt_instance_masks = instance_masks.cpu().numpy()\n        predicted_instance_masks = torch.sigmoid(outputs[\'instances\']).detach().cpu().numpy()\n\n        points = points.detach().cpu().numpy()\n        if self.max_interactive_points > 0:\n            points = points.reshape((-1, 2 * self.max_interactive_points, 2))\n        else:\n            points = points.reshape((-1, 1, 2))\n\n        num_masks = points.shape[0]\n        gt_masks = np.squeeze(gt_instance_masks[:num_masks], axis=1)\n        predicted_masks = np.squeeze(predicted_instance_masks[:num_masks], axis=1)\n\n        viz_image = []\n        for gt_mask, point, predicted_mask in zip(gt_masks, points, predicted_masks):\n            timage = draw_points(image, point[:max(1, self.max_interactive_points)], (0, 255, 0))\n            if self.max_interactive_points > 0:\n                timage = draw_points(timage, point[self.max_interactive_points:], (0, 0, 255))\n\n            gt_mask[gt_mask < 0] = 0.25\n            gt_mask = draw_probmap(gt_mask)\n            predicted_mask = draw_probmap(predicted_mask)\n            viz_image.append(np.hstack((timage, gt_mask, predicted_mask)))\n        viz_image = np.vstack(viz_image)\n\n        result = viz_image.astype(np.uint8)\n        _save_image(\'instance_segmentation\', result[:, :, ::-1])\n\n    def _load_weights(self):\n        if self.cfg.weights is not None:\n            if os.path.isfile(self.cfg.weights):\n                self.net.load_weights(self.cfg.weights)\n                self.cfg.weights = None\n            else:\n                raise RuntimeError(f""=> no checkpoint found at \'{self.cfg.weights}\'"")\n        elif self.cfg.resume_exp is not None:\n            checkpoints = list(self.cfg.CHECKPOINTS_PATH.glob(f\'{self.cfg.resume_prefix}*.pth\'))\n            assert len(checkpoints) == 1\n\n            checkpoint_path = checkpoints[0]\n            logger.info(f\'Load checkpoint from path: {checkpoint_path}\')\n            self.net.load_weights(str(checkpoint_path))\n        self.net = self.net.to(self.device)\n\n\nclass _CustomDP(torch.nn.DataParallel):\n    def __getattr__(self, name):\n        try:\n            return super().__getattr__(name)\n        except AttributeError:\n            return getattr(self.module, name)\n'"
isegm/inference/clicker.py,0,"b""from collections import namedtuple\n\nimport numpy as np\nfrom copy import deepcopy\nfrom scipy.ndimage import distance_transform_edt\n\nClick = namedtuple('Click', ['is_positive', 'coords'])\n\n\nclass Clicker(object):\n    def __init__(self, gt_mask=None, init_clicks=None, ignore_label=-1):\n        if gt_mask is not None:\n            self.gt_mask = gt_mask == 1\n            self.not_ignore_mask = gt_mask != ignore_label\n        else:\n            self.gt_mask = None\n\n        self.reset_clicks()\n\n        if init_clicks is not None:\n            for click in init_clicks:\n                self.add_click(click)\n\n    def make_next_click(self, pred_mask):\n        assert self.gt_mask is not None\n        click = self._get_click(pred_mask)\n        self.add_click(click)\n\n    def get_clicks(self, clicks_limit=None):\n        return self.clicks_list[:clicks_limit]\n\n    def _get_click(self, pred_mask, padding=True):\n        fn_mask = np.logical_and(np.logical_and(self.gt_mask, np.logical_not(pred_mask)), self.not_ignore_mask)\n        fp_mask = np.logical_and(np.logical_and(np.logical_not(self.gt_mask), pred_mask), self.not_ignore_mask)\n\n        if padding:\n            fn_mask = np.pad(fn_mask, ((1, 1), (1, 1)), 'constant')\n            fp_mask = np.pad(fp_mask, ((1, 1), (1, 1)), 'constant')\n\n        fn_mask_dt = distance_transform_edt(fn_mask)\n        fp_mask_dt = distance_transform_edt(fp_mask)\n\n        if padding:\n            fn_mask_dt = fn_mask_dt[1:-1, 1:-1]\n            fp_mask_dt = fp_mask_dt[1:-1, 1:-1]\n\n        fn_mask_dt = fn_mask_dt * self.not_clicked_map\n        fp_mask_dt = fp_mask_dt * self.not_clicked_map\n\n        fn_max_dist = np.max(fn_mask_dt)\n        fp_max_dist = np.max(fp_mask_dt)\n\n        is_positive = fn_max_dist > fp_max_dist\n        if is_positive:\n            coords_y, coords_x = np.where(fn_mask_dt == fn_max_dist)  # coords is [y, x]\n        else:\n            coords_y, coords_x = np.where(fp_mask_dt == fp_max_dist)  # coords is [y, x]\n\n        return Click(is_positive=is_positive, coords=(coords_y[0], coords_x[0]))\n\n    def add_click(self, click):\n        coords = click.coords\n\n        if click.is_positive:\n            self.num_pos_clicks += 1\n        else:\n            self.num_neg_clicks += 1\n\n        self.clicks_list.append(click)\n        if self.gt_mask is not None:\n            self.not_clicked_map[coords[0], coords[1]] = False\n\n    def _remove_last_click(self):\n        click = self.clicks_list.pop()\n        coords = click.coords\n\n        if click.is_positive:\n            self.num_pos_clicks -= 1\n        else:\n            self.num_neg_clicks -= 1\n\n        if self.gt_mask is not None:\n            self.not_clicked_map[coords[0], coords[1]] = True\n\n    def reset_clicks(self):\n        if self.gt_mask is not None:\n            self.not_clicked_map = np.ones_like(self.gt_mask, dtype=np.bool)\n\n        self.num_pos_clicks = 0\n        self.num_neg_clicks = 0\n\n        self.clicks_list = []\n\n    def get_state(self):\n        return deepcopy(self.clicks_list)\n\n    def set_state(self, state):\n        self.reset_clicks()\n        for click in state:\n            self.add_click(click)\n\n    def __len__(self):\n        return len(self.clicks_list)\n"""
isegm/inference/evaluation.py,2,"b""from time import time\n\nimport numpy as np\nimport torch\n\nfrom isegm.inference import utils\nfrom isegm.inference.clicker import Clicker\n\ntry:\n    get_ipython()\n    from tqdm import tqdm_notebook as tqdm\nexcept NameError:\n    from tqdm import tqdm\n\n\ndef evaluate_dataset(dataset, predictor, oracle_eval=False, **kwargs):\n    all_ious = []\n\n    start_time = time()\n    for index in tqdm(range(len(dataset)), leave=False):\n        sample = dataset.get_sample(index)\n        item = dataset[index]\n\n        if oracle_eval:\n            gt_mask = torch.tensor(sample['instances_mask'], dtype=torch.float32)\n            gt_mask = gt_mask.unsqueeze(0).unsqueeze(0)\n            predictor.opt_functor.mask_loss.set_gt_mask(gt_mask)\n        _, sample_ious, _ = evaluate_sample(item['images'], sample['instances_mask'], predictor, **kwargs)\n        all_ious.append(sample_ious)\n    end_time = time()\n    elapsed_time = end_time - start_time\n\n    return all_ious, elapsed_time\n\n\ndef evaluate_sample(image_nd, instances_mask, predictor, max_iou_thr,\n                    pred_thr=0.49, max_clicks=20):\n    clicker = Clicker(gt_mask=instances_mask)\n    pred_mask = np.zeros_like(instances_mask)\n    ious_list = []\n\n    with torch.no_grad():\n        predictor.set_input_image(image_nd)\n\n        for click_number in range(max_clicks):\n            clicker.make_next_click(pred_mask)\n            pred_probs = predictor.get_prediction(clicker)\n            pred_mask = pred_probs > pred_thr\n\n            iou = utils.get_iou(instances_mask, pred_mask)\n            ious_list.append(iou)\n\n            if iou >= max_iou_thr:\n                break\n\n        return clicker.clicks_list, np.array(ious_list, dtype=np.float32), pred_probs\n"""
isegm/inference/utils.py,1,"b'from datetime import timedelta\nfrom pathlib import Path\n\nimport torch\nimport numpy as np\n\nfrom isegm.model.is_deeplab_model import get_deeplab_model\nfrom isegm.model.is_hrnet_model import get_hrnet_model\nfrom isegm.data.berkeley import BerkeleyDataset\nfrom isegm.data.grabcut import GrabCutDataset\nfrom isegm.data.davis import DavisDataset\nfrom isegm.data.sbd import SBDEvaluationDataset\n\n\ndef get_time_metrics(all_ious, elapsed_time):\n    n_images = len(all_ious)\n    n_clicks = sum(map(len, all_ious))\n\n    mean_spc = elapsed_time / n_clicks\n    mean_spi = elapsed_time / n_images\n\n    return mean_spc, mean_spi\n\n\ndef load_is_model(checkpoint, device, backbone=\'auto\', **kwargs):\n    if isinstance(checkpoint, (str, Path)):\n        state_dict = torch.load(checkpoint, map_location=\'cpu\')\n    else:\n        state_dict = checkpoint\n\n    if backbone == \'auto\':\n        for k in state_dict.keys():\n            if \'feature_extractor.stage2.0.branches\' in k:\n                return load_hrnet_is_model(state_dict, device, backbone, **kwargs)\n        return load_deeplab_is_model(state_dict, device, backbone, **kwargs)\n    elif \'resnet\' in backbone:\n        return load_deeplab_is_model(state_dict, device, backbone, **kwargs)\n    elif \'hrnet\' in backbone:\n        return load_hrnet_is_model(state_dict, device, backbone, **kwargs)\n    else:\n        raise NotImplementedError(\'Unknown backbone\')\n\n\ndef load_hrnet_is_model(state_dict, device, backbone=\'auto\', width=48, ocr_width=256,\n                        small=False, cpu_dist_maps=False, norm_radius=260):\n    if backbone == \'auto\':\n        num_fe_weights = len([x for x in state_dict.keys() if \'feature_extractor.\' in x])\n        small = num_fe_weights < 1800\n\n        ocr_f_down = [v for k, v in state_dict.items() if \'object_context_block.f_down.1.0.bias\' in k]\n        assert len(ocr_f_down) == 1\n        ocr_width = ocr_f_down[0].shape[0]\n\n        s2_conv1_w = [v for k, v in state_dict.items() if \'stage2.0.branches.0.0.conv1.weight\' in k]\n        assert  len(s2_conv1_w) == 1\n        width = s2_conv1_w[0].shape[0]\n\n    model = get_hrnet_model(width=width, ocr_width=ocr_width, small=small,\n                            with_aux_output=False, cpu_dist_maps=cpu_dist_maps,\n                            norm_radius=norm_radius)\n\n    model.load_state_dict(state_dict, strict=False)\n    for param in model.parameters():\n        param.requires_grad = False\n    model.to(device)\n    model.eval()\n\n    return model\n\n\ndef load_deeplab_is_model(state_dict, device, backbone=\'auto\', deeplab_ch=128, aspp_dropout=0.2,\n                          cpu_dist_maps=False, norm_radius=260):\n    if backbone == \'auto\':\n        num_backbone_params = len([x for x in state_dict.keys()\n                                   if \'feature_extractor.backbone\' in x and not(\'num_batches_tracked\' in x)])\n\n        if num_backbone_params <= 181:\n            backbone = \'resnet34\'\n        elif num_backbone_params <= 276:\n            backbone = \'resnet50\'\n        elif num_backbone_params <= 531:\n            backbone = \'resnet101\'\n        else:\n            raise NotImplementedError(\'Unknown backbone\')\n\n        if \'aspp_dropout\' in state_dict:\n            aspp_dropout = float(state_dict[\'aspp_dropout\'].cpu().numpy())\n        else:\n            aspp_project_weight = [v for k, v in state_dict.items() if \'aspp.project.0.weight\' in k][0]\n            deeplab_ch = aspp_project_weight.size(0)\n            if deeplab_ch == 256:\n                aspp_dropout = 0.5\n\n    model = get_deeplab_model(backbone=backbone, deeplab_ch=deeplab_ch,\n                              aspp_dropout=aspp_dropout, cpu_dist_maps=cpu_dist_maps,\n                              norm_radius=norm_radius)\n\n    model.load_state_dict(state_dict, strict=False)\n    for param in model.parameters():\n        param.requires_grad = False\n    model.to(device)\n    model.eval()\n\n    return model\n\n\ndef get_dataset(dataset_name, cfg):\n    if dataset_name == \'GrabCut\':\n        dataset = GrabCutDataset(cfg.GRABCUT_PATH)\n    elif dataset_name == \'Berkeley\':\n        dataset = BerkeleyDataset(cfg.BERKELEY_PATH)\n    elif dataset_name == \'DAVIS\':\n        dataset = DavisDataset(cfg.DAVIS_PATH)\n    elif dataset_name == \'COCO_MVal\':\n        dataset = DavisDataset(cfg.COCO_MVAL_PATH)\n    elif dataset_name == \'SBD\':\n        dataset = SBDEvaluationDataset(cfg.SBD_PATH)\n    elif dataset_name == \'SBD_Train\':\n        dataset = SBDEvaluationDataset(cfg.SBD_PATH, split=\'train\')\n    else:\n        dataset = None\n\n    return dataset\n\n\ndef get_iou(gt_mask, pred_mask, ignore_label=-1):\n    ignore_gt_mask_inv = gt_mask != ignore_label\n    obj_gt_mask = gt_mask == 1\n\n    intersection = np.logical_and(np.logical_and(pred_mask, obj_gt_mask), ignore_gt_mask_inv).sum()\n    union = np.logical_and(np.logical_or(pred_mask, obj_gt_mask), ignore_gt_mask_inv).sum()\n\n    return intersection / union\n\n\ndef compute_noc_metric(all_ious, iou_thrs, max_clicks=20):\n    def _get_noc(iou_arr, iou_thr):\n        vals = iou_arr >= iou_thr\n        return np.argmax(vals) + 1 if np.any(vals) else max_clicks\n\n    noc_list = []\n    over_max_list = []\n    for iou_thr in iou_thrs:\n        scores_arr = np.array([_get_noc(iou_arr, iou_thr)\n                               for iou_arr in all_ious], dtype=np.int)\n\n        score = scores_arr.mean()\n        over_max = (scores_arr == max_clicks).sum()\n\n        noc_list.append(score)\n        over_max_list.append(over_max)\n\n    return noc_list, over_max_list\n\n\ndef find_checkpoint(weights_folder, checkpoint_name):\n    weights_folder = Path(weights_folder)\n    if \':\' in checkpoint_name:\n        model_name, checkpoint_name = checkpoint_name.split(\':\')\n        models_candidates = [x for x in weights_folder.glob(f\'{model_name}*\') if x.is_dir()]\n        assert len(models_candidates) == 1\n        model_folder = models_candidates[0]\n    else:\n        model_folder = weights_folder\n\n    if checkpoint_name.endswith(\'.pth\'):\n        if Path(checkpoint_name).exists():\n            checkpoint_path = checkpoint_name\n        else:\n            checkpoint_path = weights_folder / checkpoint_name\n    else:\n        model_checkpoints = list(model_folder.rglob(f\'{checkpoint_name}*.pth\'))\n        assert len(model_checkpoints) == 1\n        checkpoint_path = model_checkpoints[0]\n\n    return str(checkpoint_path)\n\n\ndef get_results_table(noc_list, over_max_list, brs_type, dataset_name, mean_spc, elapsed_time,\n                      n_clicks=20, model_name=None):\n    table_header = (f\'|{""BRS Type"":^13}|{""Dataset"":^11}|\'\n                    f\'{""NoC@80%"":^9}|{""NoC@85%"":^9}|{""NoC@90%"":^9}|\'\n                    f\'{"">=""+str(n_clicks)+""@85%"":^9}|{"">=""+str(n_clicks)+""@90%"":^9}|\'\n                    f\'{""SPC,s"":^7}|{""Time"":^9}|\')\n    row_width = len(table_header)\n\n    header = f\'Eval results for model: {model_name}\\n\' if model_name is not None else \'\'\n    header += \'-\' * row_width + \'\\n\'\n    header += table_header + \'\\n\' + \'-\' * row_width\n\n    eval_time = str(timedelta(seconds=int(elapsed_time)))\n    table_row = f\'|{brs_type:^13}|{dataset_name:^11}|\'\n    table_row += f\'{noc_list[0]:^9.2f}|\'\n    table_row += f\'{noc_list[1]:^9.2f}|\' if len(noc_list) > 1 else f\'{""?"":^9}|\'\n    table_row += f\'{noc_list[2]:^9.2f}|\' if len(noc_list) > 2 else f\'{""?"":^9}|\'\n    table_row += f\'{over_max_list[1]:^9}|\' if len(noc_list) > 1 else f\'{""?"":^9}|\'\n    table_row += f\'{over_max_list[2]:^9}|\' if len(noc_list) > 2 else f\'{""?"":^9}|\'\n    table_row += f\'{mean_spc:^7.3f}|{eval_time:^9}|\'\n\n    return header, table_row'"
isegm/model/initializer.py,3,"b'import torch\nimport torch.nn as nn\nimport numpy as np\n\n\nclass Initializer(object):\n    def __init__(self, local_init=True, gamma=None):\n        self.local_init = local_init\n        self.gamma = gamma\n\n    def __call__(self, m):\n        if getattr(m, \'__initialized\', False):\n            return\n\n        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d,\n                          nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d,\n                          nn.GroupNorm, nn.SyncBatchNorm)) or \'BatchNorm\' in m.__class__.__name__:\n            if m.weight is not None:\n                self._init_gamma(m.weight.data)\n            if m.bias is not None:\n                self._init_beta(m.bias.data)\n        else:\n            if getattr(m, \'weight\', None) is not None:\n                self._init_weight(m.weight.data)\n            if getattr(m, \'bias\', None) is not None:\n                self._init_bias(m.bias.data)\n\n        if self.local_init:\n            object.__setattr__(m, \'__initialized\', True)\n\n    def _init_weight(self, data):\n        nn.init.uniform_(data, -0.07, 0.07)\n\n    def _init_bias(self, data):\n        nn.init.constant_(data, 0)\n\n    def _init_gamma(self, data):\n        if self.gamma is None:\n            nn.init.constant_(data, 1.0)\n        else:\n            nn.init.normal_(data, 1.0, self.gamma)\n\n    def _init_beta(self, data):\n        nn.init.constant_(data, 0)\n\n\nclass Bilinear(Initializer):\n    def __init__(self, scale, groups, in_channels, **kwargs):\n        super().__init__(**kwargs)\n        self.scale = scale\n        self.groups = groups\n        self.in_channels = in_channels\n\n    def _init_weight(self, data):\n        """"""Reset the weight and bias.""""""\n        bilinear_kernel = self.get_bilinear_kernel(self.scale)\n        weight = torch.zeros_like(data)\n        for i in range(self.in_channels):\n            if self.groups == 1:\n                j = i\n            else:\n                j = 0\n            weight[i, j] = bilinear_kernel\n        data[:] = weight\n\n    @staticmethod\n    def get_bilinear_kernel(scale):\n        """"""Generate a bilinear upsampling kernel.""""""\n        kernel_size = 2 * scale - scale % 2\n        scale = (kernel_size + 1) // 2\n        center = scale - 0.5 * (1 + kernel_size % 2)\n\n        og = np.ogrid[:kernel_size, :kernel_size]\n        kernel = (1 - np.abs(og[0] - center) / scale) * (1 - np.abs(og[1] - center) / scale)\n\n        return torch.tensor(kernel, dtype=torch.float32)\n\n\nclass XavierGluon(Initializer):\n    def __init__(self, rnd_type=\'uniform\', factor_type=\'avg\', magnitude=3, **kwargs):\n        super().__init__(**kwargs)\n\n        self.rnd_type = rnd_type\n        self.factor_type = factor_type\n        self.magnitude = float(magnitude)\n\n    def _init_weight(self, arr):\n        fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(arr)\n\n        if self.factor_type == \'avg\':\n            factor = (fan_in + fan_out) / 2.0\n        elif self.factor_type == \'in\':\n            factor = fan_in\n        elif self.factor_type == \'out\':\n            factor = fan_out\n        else:\n            raise ValueError(\'Incorrect factor type\')\n        scale = np.sqrt(self.magnitude / factor)\n\n        if self.rnd_type == \'uniform\':\n            nn.init.uniform_(arr, -scale, scale)\n        elif self.rnd_type == \'gaussian\':\n            nn.init.normal_(arr, 0, scale)\n        else:\n            raise ValueError(\'Unknown random type\')\n'"
isegm/model/is_deeplab_model.py,6,"b""import torch\nimport torch.nn as nn\n\nfrom isegm.model.ops import DistMaps\nfrom .modeling.deeplab_v3 import DeepLabV3Plus\nfrom .modeling.basic_blocks import SepConvHead\n\n\ndef get_deeplab_model(backbone='resnet50', deeplab_ch=256, aspp_dropout=0.5,\n                      norm_layer=nn.BatchNorm2d, backbone_norm_layer=None,\n                      use_rgb_conv=True, cpu_dist_maps=False,\n                      norm_radius=260):\n    model = DistMapsModel(\n        feature_extractor=DeepLabV3Plus(backbone=backbone,\n                                        ch=deeplab_ch,\n                                        project_dropout=aspp_dropout,\n                                        norm_layer=norm_layer,\n                                        backbone_norm_layer=backbone_norm_layer),\n        head=SepConvHead(1, in_channels=deeplab_ch, mid_channels=deeplab_ch // 2,\n                         num_layers=2, norm_layer=norm_layer),\n        use_rgb_conv=use_rgb_conv,\n        norm_layer=norm_layer,\n        norm_radius=norm_radius,\n        cpu_dist_maps=cpu_dist_maps\n    )\n\n    return model\n\n\nclass DistMapsModel(nn.Module):\n    def __init__(self, feature_extractor, head, norm_layer=nn.BatchNorm2d, use_rgb_conv=True,\n                 cpu_dist_maps=False, norm_radius=260):\n        super(DistMapsModel, self).__init__()\n\n        if use_rgb_conv:\n            self.rgb_conv = nn.Sequential(\n                nn.Conv2d(in_channels=5, out_channels=8, kernel_size=1),\n                nn.LeakyReLU(negative_slope=0.2),\n                norm_layer(8),\n                nn.Conv2d(in_channels=8, out_channels=3, kernel_size=1),\n            )\n        else:\n            self.rgb_conv = None\n\n        self.dist_maps = DistMaps(norm_radius=norm_radius, spatial_scale=1.0,\n                                  cpu_mode=cpu_dist_maps)\n        self.feature_extractor = feature_extractor\n        self.head = head\n\n    def forward(self, image, points):\n        coord_features = self.dist_maps(image, points)\n\n        if self.rgb_conv is not None:\n            x = self.rgb_conv(torch.cat((image, coord_features), dim=1))\n        else:\n            c1, c2 = torch.chunk(coord_features, 2, dim=1)\n            c3 = torch.ones_like(c1)\n            coord_features = torch.cat((c1, c2, c3), dim=1)\n            x = 0.8 * image * coord_features + 0.2 * image\n\n        backbone_features = self.feature_extractor(x)\n        instance_out = self.head(backbone_features[0])\n        instance_out = nn.functional.interpolate(instance_out, size=image.size()[2:],\n                                                 mode='bilinear', align_corners=True)\n\n        return {'instances': instance_out}\n\n    def load_weights(self, path_to_weights):\n        current_state_dict = self.state_dict()\n        new_state_dict = torch.load(path_to_weights, map_location='cpu')\n        current_state_dict.update(new_state_dict)\n        self.load_state_dict(current_state_dict)\n\n    def get_trainable_params(self):\n        backbone_params = nn.ParameterList()\n        other_params = nn.ParameterList()\n\n        for name, param in self.named_parameters():\n            if param.requires_grad:\n                if 'backbone' in name:\n                    backbone_params.append(param)\n                else:\n                    other_params.append(param)\n        return backbone_params, other_params\n\n\n"""
isegm/model/is_hrnet_model.py,6,"b""import torch\nimport torch.nn as nn\n\nfrom isegm.model.ops import DistMaps\nfrom .modeling.hrnet_ocr import HighResolutionNet\n\n\ndef get_hrnet_model(width=48, ocr_width=256, small=False, norm_radius=260,\n                    use_rgb_conv=True, with_aux_output=False, cpu_dist_maps=False,\n                    norm_layer=nn.BatchNorm2d):\n    model = DistMapsHRNetModel(\n        feature_extractor=HighResolutionNet(width=width, ocr_width=ocr_width, small=small,\n                                            num_classes=1, norm_layer=norm_layer),\n        use_rgb_conv=use_rgb_conv,\n        with_aux_output=with_aux_output,\n        norm_layer=norm_layer,\n        norm_radius=norm_radius,\n        cpu_dist_maps=cpu_dist_maps\n    )\n\n    return model\n\n\nclass DistMapsHRNetModel(nn.Module):\n    def __init__(self, feature_extractor, use_rgb_conv=True, with_aux_output=False,\n                 norm_layer=nn.BatchNorm2d, norm_radius=260, cpu_dist_maps=False):\n        super(DistMapsHRNetModel, self).__init__()\n        self.with_aux_output = with_aux_output\n\n        if use_rgb_conv:\n            self.rgb_conv = nn.Sequential(\n                nn.Conv2d(in_channels=5, out_channels=8, kernel_size=1),\n                nn.LeakyReLU(negative_slope=0.2),\n                norm_layer(8),\n                nn.Conv2d(in_channels=8, out_channels=3, kernel_size=1),\n            )\n        else:\n            self.rgb_conv = None\n\n        self.dist_maps = DistMaps(norm_radius=norm_radius, spatial_scale=1.0, cpu_mode=cpu_dist_maps)\n        self.feature_extractor = feature_extractor\n\n    def forward(self, image, points):\n        coord_features = self.dist_maps(image, points)\n\n        if self.rgb_conv is not None:\n            x = self.rgb_conv(torch.cat((image, coord_features), dim=1))\n        else:\n            c1, c2 = torch.chunk(coord_features, 2, dim=1)\n            c3 = torch.ones_like(c1)\n            coord_features = torch.cat((c1, c2, c3), dim=1)\n            x = 0.8 * image * coord_features + 0.2 * image\n\n        feature_extractor_out = self.feature_extractor(x)\n        instance_out = feature_extractor_out[0]\n        instance_out = nn.functional.interpolate(instance_out, size=image.size()[2:],\n                                                 mode='bilinear', align_corners=True)\n        outputs = {'instances': instance_out}\n        if self.with_aux_output:\n            instance_aux_out = feature_extractor_out[1]\n            instance_aux_out = nn.functional.interpolate(instance_aux_out, size=image.size()[2:],\n                                                         mode='bilinear', align_corners=True)\n            outputs['instances_aux'] = instance_aux_out\n\n        return outputs\n\n    def load_weights(self, path_to_weights):\n        current_state_dict = self.state_dict()\n        new_state_dict = torch.load(path_to_weights)\n        current_state_dict.update(new_state_dict)\n        self.load_state_dict(current_state_dict)\n\n    def get_trainable_params(self):\n        backbone_params = nn.ParameterList()\n        other_params = nn.ParameterList()\n        other_params_keys = []\n        nonbackbone_keywords = ['rgb_conv', 'aux_head', 'cls_head', 'conv3x3_ocr', 'ocr_distri_head']\n\n        for name, param in self.named_parameters():\n            if param.requires_grad:\n                if any(x in name for x in nonbackbone_keywords):\n                    other_params.append(param)\n                    other_params_keys.append(name)\n                else:\n                    backbone_params.append(param)\n        print('Nonbackbone params:', sorted(other_params_keys))\n        return backbone_params, other_params\n"""
isegm/model/losses.py,25,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom isegm.utils import misc\n\n\nclass NormalizedFocalLossSigmoid(nn.Module):\n    def __init__(self, axis=-1, alpha=0.25, gamma=2,\n                 from_logits=False, batch_axis=0,\n                 weight=None, size_average=True, detach_delimeter=True,\n                 eps=1e-12, scale=1.0,\n                 ignore_label=-1):\n        super(NormalizedFocalLossSigmoid, self).__init__()\n        self._axis = axis\n        self._alpha = alpha\n        self._gamma = gamma\n        self._ignore_label = ignore_label\n        self._weight = weight if weight is not None else 1.0\n        self._batch_axis = batch_axis\n\n        self._scale = scale\n        self._from_logits = from_logits\n        self._eps = eps\n        self._size_average = size_average\n        self._detach_delimeter = detach_delimeter\n        self._k_sum = 0\n\n    def forward(self, pred, label, sample_weight=None):\n        one_hot = label > 0\n        sample_weight = label != self._ignore_label\n\n        if not self._from_logits:\n            pred = torch.sigmoid(pred)\n\n        alpha = torch.where(one_hot, self._alpha * sample_weight, (1 - self._alpha) * sample_weight)\n        pt = torch.where(one_hot, pred, 1 - pred)\n        pt = torch.where(sample_weight, pt, torch.ones_like(pt))\n\n        beta = (1 - pt) ** self._gamma\n\n        sw_sum = torch.sum(sample_weight, dim=(-2, -1), keepdim=True)\n        beta_sum = torch.sum(beta, dim=(-2, -1), keepdim=True)\n        mult = sw_sum / (beta_sum + self._eps)\n        if self._detach_delimeter:\n            mult = mult.detach()\n        beta = beta * mult\n\n        ignore_area = torch.sum(label == self._ignore_label, dim=tuple(range(1, label.dim()))).cpu().numpy()\n        sample_mult = torch.mean(mult, dim=tuple(range(1, mult.dim()))).cpu().numpy()\n        if np.any(ignore_area == 0):\n            self._k_sum = 0.9 * self._k_sum + 0.1 * sample_mult[ignore_area == 0].mean()\n\n        loss = -alpha * beta * torch.log(torch.min(pt + self._eps, torch.ones(1, dtype=torch.float).to(pt.device)))\n        loss = self._weight * (loss * sample_weight)\n\n        if self._size_average:\n            bsum = torch.sum(sample_weight, dim=misc.get_dims_with_exclusion(sample_weight.dim(), self._batch_axis))\n            loss = torch.sum(loss, dim=misc.get_dims_with_exclusion(loss.dim(), self._batch_axis)) / (bsum + self._eps)\n        else:\n            loss = torch.sum(loss, dim=misc.get_dims_with_exclusion(loss.dim(), self._batch_axis))\n\n        return self._scale * loss\n\n    def log_states(self, sw, name, global_step):\n        sw.add_scalar(tag=name + '_k', value=self._k_sum, global_step=global_step)\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self, axis=-1, alpha=0.25, gamma=2,\n                 from_logits=False, batch_axis=0,\n                 weight=None, num_class=None,\n                 eps=1e-9, size_average=True, scale=1.0):\n        super(FocalLoss, self).__init__()\n        self._axis = axis\n        self._alpha = alpha\n        self._gamma = gamma\n        self._weight = weight if weight is not None else 1.0\n        self._batch_axis = batch_axis\n\n        self._scale = scale\n        self._num_class = num_class\n        self._from_logits = from_logits\n        self._eps = eps\n        self._size_average = size_average\n\n    def forward(self, pred, label, sample_weight=None):\n        if not self._from_logits:\n            pred = F.sigmoid(pred)\n\n        one_hot = label > 0\n        pt = torch.where(one_hot, pred, 1 - pred)\n\n        t = label != -1\n        alpha = torch.where(one_hot, self._alpha * t, (1 - self._alpha) * t)\n        beta = (1 - pt) ** self._gamma\n\n        loss = -alpha * beta * torch.log(torch.min(pt + self._eps, torch.ones(1, dtype=torch.float).to(pt.device)))\n        sample_weight = label != -1\n\n        loss = self._weight * (loss * sample_weight)\n\n        if self._size_average:\n            tsum = torch.sum(label == 1, dim=misc.get_dims_with_exclusion(label.dim(), self._batch_axis))\n            loss = torch.sum(loss, dim=misc.get_dims_with_exclusion(loss.dim(), self._batch_axis)) / (tsum + self._eps)\n        else:\n            loss = torch.sum(loss, dim=misc.get_dims_with_exclusion(loss.dim(), self._batch_axis))\n\n        return self._scale * loss\n\n\nclass SigmoidBinaryCrossEntropyLoss(nn.Module):\n    def __init__(self, from_sigmoid=False, weight=None, batch_axis=0, ignore_label=-1):\n        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n        self._from_sigmoid = from_sigmoid\n        self._ignore_label = ignore_label\n        self._weight = weight if weight is not None else 1.0\n        self._batch_axis = batch_axis\n\n    def forward(self, pred, label):\n        label = label.view(pred.size())\n        sample_weight = label != self._ignore_label\n        label = torch.where(sample_weight, label, torch.zeros_like(label))\n\n        if not self._from_sigmoid:\n            loss = torch.relu(pred) - pred * label + F.softplus(-torch.abs(pred))\n        else:\n            eps = 1e-12\n            loss = -(torch.log(pred + eps) * label\n                     + torch.log(1. - pred + eps) * (1. - label))\n\n        loss = self._weight * (loss * sample_weight)\n        return torch.mean(loss, dim=misc.get_dims_with_exclusion(loss.dim(), self._batch_axis))\n"""
isegm/model/metrics.py,5,"b""import torch\nimport numpy as np\n\nfrom isegm.utils import misc\n\n\nclass TrainMetric(object):\n    def __init__(self, pred_outputs, gt_outputs):\n        self.pred_outputs = pred_outputs\n        self.gt_outputs = gt_outputs\n\n    def update(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def get_epoch_value(self):\n        raise NotImplementedError\n\n    def reset_epoch_stats(self):\n        raise NotImplementedError\n\n    def log_states(self, sw, tag_prefix, global_step):\n        pass\n\n    @property\n    def name(self):\n        return type(self).__name__\n\n\nclass AdaptiveIoU(TrainMetric):\n    def __init__(self, init_thresh=0.4, thresh_step=0.025, thresh_beta=0.99, iou_beta=0.9,\n                 ignore_label=-1, from_logits=True,\n                 pred_output='instances', gt_output='instances'):\n        super().__init__(pred_outputs=(pred_output,), gt_outputs=(gt_output,))\n        self._ignore_label = ignore_label\n        self._from_logits = from_logits\n        self._iou_thresh = init_thresh\n        self._thresh_step = thresh_step\n        self._thresh_beta = thresh_beta\n        self._iou_beta = iou_beta\n        self._ema_iou = 0.0\n        self._epoch_iou_sum = 0.0\n        self._epoch_batch_count = 0\n\n    def update(self, pred, gt):\n        gt_mask = gt > 0\n        if self._from_logits:\n            pred = torch.sigmoid(pred)\n\n        gt_mask_area = torch.sum(gt_mask, dim=(1, 2)).detach().cpu().numpy()\n        if np.all(gt_mask_area == 0):\n            return\n\n        ignore_mask = gt == self._ignore_label\n        max_iou = _compute_iou(pred > self._iou_thresh, gt_mask, ignore_mask).mean()\n        best_thresh = self._iou_thresh\n        for t in [best_thresh - self._thresh_step, best_thresh + self._thresh_step]:\n            temp_iou = _compute_iou(pred > t, gt_mask, ignore_mask).mean()\n            if temp_iou > max_iou:\n                max_iou = temp_iou\n                best_thresh = t\n\n        self._iou_thresh = self._thresh_beta * self._iou_thresh + (1 - self._thresh_beta) * best_thresh\n        self._ema_iou = self._iou_beta * self._ema_iou + (1 - self._iou_beta) * max_iou\n        self._epoch_iou_sum += max_iou\n        self._epoch_batch_count += 1\n\n    def get_epoch_value(self):\n        if self._epoch_batch_count > 0:\n            return self._epoch_iou_sum / self._epoch_batch_count\n        else:\n            return 0.0\n\n    def reset_epoch_stats(self):\n        self._epoch_iou_sum = 0.0\n        self._epoch_batch_count = 0\n\n    def log_states(self, sw, tag_prefix, global_step):\n        sw.add_scalar(tag=tag_prefix + '_ema_iou', value=self._ema_iou, global_step=global_step)\n        sw.add_scalar(tag=tag_prefix + '_iou_thresh', value=self._iou_thresh, global_step=global_step)\n\n    @property\n    def iou_thresh(self):\n        return self._iou_thresh\n\n\ndef _compute_iou(pred_mask, gt_mask, ignore_mask=None, keep_ignore=False):\n    if ignore_mask is not None:\n        pred_mask = torch.where(ignore_mask, torch.zeros_like(pred_mask), pred_mask)\n\n    reduction_dims = misc.get_dims_with_exclusion(gt_mask.dim(), 0)\n    union = torch.mean((pred_mask | gt_mask).float(), dim=reduction_dims).detach().cpu().numpy()\n    intersection = torch.mean((pred_mask & gt_mask).float(), dim=reduction_dims).detach().cpu().numpy()\n    nonzero = union > 0\n\n    iou = intersection[nonzero] / union[nonzero]\n    if not keep_ignore:\n        return iou\n    else:\n        result = np.full_like(intersection, -1)\n        result[nonzero] = iou\n        return result\n"""
isegm/model/ops.py,6,"b'import torch\nfrom torch import nn as nn\nimport numpy as np\n\nimport isegm.model.initializer as initializer\nfrom isegm.utils.cython import get_dist_maps\n\n\ndef select_activation_function(activation):\n    if isinstance(activation, str):\n        if activation.lower() == \'relu\':\n            return nn.ReLU\n        elif activation.lower() == \'softplus\':\n            return nn.Softplus\n        else:\n            raise ValueError(f""Unknown activation type {activation}"")\n    elif isinstance(activation, nn.Module):\n        return activation\n    else:\n        raise ValueError(f""Unknown activation type {activation}"")\n\n\nclass BilinearConvTranspose2d(nn.ConvTranspose2d):\n    def __init__(self, in_channels, out_channels, scale, groups=1):\n        kernel_size = 2 * scale - scale % 2\n        self.scale = scale\n\n        super().__init__(\n            in_channels, out_channels,\n            kernel_size=kernel_size,\n            stride=scale,\n            padding=1,\n            groups=groups,\n            bias=False)\n\n        self.apply(initializer.Bilinear(scale=scale, in_channels=in_channels, groups=groups))\n\n\nclass DistMaps(nn.Module):\n    def __init__(self, norm_radius, spatial_scale=1.0, cpu_mode=False):\n        super(DistMaps, self).__init__()\n        self.spatial_scale = spatial_scale\n        self.norm_radius = norm_radius\n        self.cpu_mode = cpu_mode\n\n    def get_coord_features(self, points, batchsize, rows, cols):\n        if self.cpu_mode:\n            coords = []\n            for i in range(batchsize):\n                norm_delimeter = self.spatial_scale * self.norm_radius\n                coords.append(get_dist_maps(points[i].cpu().float().numpy(), rows, cols,\n                                            norm_delimeter))\n            coords = torch.from_numpy(np.stack(coords, axis=0)).to(points.device).float()\n        else:\n            num_points = points.shape[1] // 2\n            points = points.view(-1, 2)\n            invalid_points = torch.max(points, dim=1, keepdim=False)[0] < 0\n            row_array = torch.arange(start=0, end=rows, step=1, dtype=torch.float32, device=points.device)\n            col_array = torch.arange(start=0, end=cols, step=1, dtype=torch.float32, device=points.device)\n\n            coord_rows, coord_cols = torch.meshgrid(row_array, col_array)\n            coords = torch.stack((coord_rows, coord_cols), dim=0).unsqueeze(0).repeat(points.size(0), 1, 1, 1)\n\n            add_xy = (points * self.spatial_scale).view(points.size(0), points.size(1), 1, 1)\n            coords.add_(-add_xy)\n            coords.div_(self.norm_radius * self.spatial_scale)\n            coords.mul_(coords)\n\n            coords[:, 0] += coords[:, 1]\n            coords = coords[:, :1]\n\n            coords[invalid_points, :, :, :] = 1e6\n\n            coords = coords.view(-1, num_points, 1, rows, cols)\n            coords = coords.min(dim=1)[0]  # -> (bs * num_masks * 2) x 1 x h x w\n            coords = coords.view(-1, 2, rows, cols)\n\n        coords.sqrt_().mul_(2).tanh_()\n\n        return coords\n\n    def forward(self, x, coords):\n        return self.get_coord_features(coords, x.shape[0], x.shape[2], x.shape[3])\n'"
isegm/utils/exp.py,2,"b'import os\nimport sys\nimport shutil\nimport pprint\nfrom pathlib import Path\nfrom datetime import datetime\n\nimport yaml\nimport torch\nfrom easydict import EasyDict as edict\n\nfrom .log import logger, add_logging\n\n\ndef init_experiment(args):\n    model_path = Path(args.model_path)\n    ftree = get_model_family_tree(model_path)\n    if ftree is None:\n        print(\'Models can only be located in the ""models"" directory in the root of the repository\')\n        sys.exit(1)\n\n    cfg = load_config(model_path)\n    update_config(cfg, args)\n\n    experiments_path = Path(cfg.EXPS_PATH)\n    exp_parent_path = experiments_path / \'/\'.join(ftree)\n    exp_parent_path.mkdir(parents=True, exist_ok=True)\n\n    if cfg.resume_exp:\n        exp_path = find_resume_exp(exp_parent_path, cfg.resume_exp)\n    else:\n        last_exp_indx = find_last_exp_indx(exp_parent_path)\n        exp_name = f\'{last_exp_indx:03d}\'\n        if cfg.exp_name:\n            exp_name += \'_\' + cfg.exp_name\n        exp_path = exp_parent_path / exp_name\n        exp_path.mkdir(parents=True)\n\n    cfg.EXP_PATH = exp_path\n    cfg.CHECKPOINTS_PATH = exp_path / \'checkpoints\'\n    cfg.VIS_PATH = exp_path / \'vis\'\n    cfg.LOGS_PATH = exp_path / \'logs\'\n\n    cfg.LOGS_PATH.mkdir(exist_ok=True)\n    cfg.CHECKPOINTS_PATH.mkdir(exist_ok=True)\n    cfg.VIS_PATH.mkdir(exist_ok=True)\n\n    dst_script_path = exp_path / (model_path.stem + datetime.strftime(datetime.today(), \'_%Y-%m-%d-%H-%M-%S.py\'))\n    shutil.copy(model_path, dst_script_path)\n\n    if cfg.gpus != \'\':\n        gpu_ids = [int(id) for id in cfg.gpus.split(\',\')]\n    else:\n        gpu_ids = list(range(cfg.ngpus))\n        cfg.gpus = \',\'.join([str(id) for id in gpu_ids])\n    cfg.gpu_ids = gpu_ids\n    cfg.ngpus = len(gpu_ids)\n    cfg.multi_gpu = cfg.ngpus > 1\n\n    if cfg.multi_gpu:\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = cfg.gpus\n        ngpus = torch.cuda.device_count()\n        assert ngpus == cfg.ngpus\n    cfg.device = torch.device(f\'cuda:{cfg.gpu_ids[0]}\')\n\n    add_logging(cfg.LOGS_PATH, prefix=\'train_\')\n\n    logger.info(f\'Number of GPUs: {len(cfg.gpu_ids)}\')\n    logger.info(\'Run experiment with config:\')\n    logger.info(pprint.pformat(cfg, indent=4))\n\n    return cfg\n\n\ndef get_model_family_tree(model_path, terminate_name=\'models\'):\n    model_name = model_path.stem\n    family_tree = [model_name]\n    for x in model_path.parents:\n        if x.stem == terminate_name:\n            break\n        family_tree.append(x.stem)\n    else:\n        return None\n\n    return family_tree[::-1]\n\n\ndef find_last_exp_indx(exp_parent_path):\n    indx = 0\n    for x in exp_parent_path.iterdir():\n        if not x.is_dir():\n            continue\n\n        exp_name = x.stem\n        if exp_name[:3].isnumeric():\n            indx = max(indx, int(exp_name[:3]) + 1)\n\n    return indx\n\n\ndef find_resume_exp(exp_parent_path, exp_pattern):\n    candidates = sorted(exp_parent_path.glob(f\'{exp_pattern}*\'))\n    if len(candidates) == 0:\n        print(f\'No experiments could be found that satisfies the pattern = ""*{exp_pattern}""\')\n        sys.exit(1)\n    elif len(candidates) > 1:\n        print(\'More than one experiment found:\')\n        for x in candidates:\n            print(x)\n        sys.exit(1)\n    else:\n        exp_path = candidates[0]\n        print(f\'Continue with experiment ""{exp_path}""\')\n\n    return exp_path\n\n\ndef update_config(cfg, args):\n    for param_name, value in vars(args).items():\n        if param_name.lower() in cfg or param_name.upper() in cfg:\n            continue\n        cfg[param_name] = value\n\n\ndef load_config(model_path):\n    model_name = model_path.stem\n    config_path = model_path.parent / (model_name + \'.yml\')\n\n    if config_path.exists():\n        cfg = load_config_file(config_path)\n    else:\n        cfg = dict()\n\n    cwd = Path.cwd()\n    config_parent = config_path.parent.absolute()\n    while len(config_parent.parents) > 0:\n        config_path = config_parent / \'config.yml\'\n\n        if config_path.exists():\n            local_config = load_config_file(config_path, model_name=model_name)\n            cfg.update({k: v for k, v in local_config.items() if k not in cfg})\n\n        if config_parent.absolute() == cwd:\n            break\n        config_parent = config_parent.parent\n\n    return edict(cfg)\n\n\ndef load_config_file(config_path, model_name=None, return_edict=False):\n    with open(config_path, \'r\') as f:\n        cfg = yaml.safe_load(f)\n\n    if \'SUBCONFIGS\' in cfg:\n        if model_name is not None and model_name in cfg[\'SUBCONFIGS\']:\n            cfg.update(cfg[\'SUBCONFIGS\'][model_name])\n        del cfg[\'SUBCONFIGS\']\n\n    return edict(cfg) if return_edict else cfg\n'"
isegm/utils/log.py,1,"b""import io\nimport time\nimport logging\nfrom datetime import datetime\n\nimport numpy as np\nfrom torch.utils.tensorboard import SummaryWriter\n\nLOGGER_NAME = 'root'\nLOGGER_DATEFMT = '%Y-%m-%d %H:%M:%S'\n\nhandler = logging.StreamHandler()\n\nlogger = logging.getLogger(LOGGER_NAME)\nlogger.setLevel(logging.INFO)\nlogger.addHandler(handler)\n\n\ndef add_logging(logs_path, prefix):\n    log_name = prefix + datetime.strftime(datetime.today(), '%Y-%m-%d_%H-%M-%S') + '.log'\n    stdout_log_path = logs_path / log_name\n\n    fh = logging.FileHandler(str(stdout_log_path))\n    formatter = logging.Formatter(fmt='(%(levelname)s) %(asctime)s: %(message)s',\n                                  datefmt=LOGGER_DATEFMT)\n    fh.setFormatter(formatter)\n    logger.addHandler(fh)\n\n\nclass TqdmToLogger(io.StringIO):\n    logger = None\n    level = None\n    buf = ''\n\n    def __init__(self, logger, level=None, mininterval=5):\n        super(TqdmToLogger, self).__init__()\n        self.logger = logger\n        self.level = level or logging.INFO\n        self.mininterval = mininterval\n        self.last_time = 0\n\n    def write(self, buf):\n        self.buf = buf.strip('\\r\\n\\t ')\n \n    def flush(self):\n        if len(self.buf) > 0 and time.time() - self.last_time > self.mininterval:\n            self.logger.log(self.level, self.buf)\n            self.last_time = time.time()\n\n\nclass SummaryWriterAvg(SummaryWriter):\n    def __init__(self, *args, dump_period=20, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._dump_period = dump_period\n        self._avg_scalars = dict()\n\n    def add_scalar(self, tag, value, global_step=None, disable_avg=False):\n        if disable_avg or isinstance(value, (tuple, list, dict)):\n            super().add_scalar(tag, np.array(value), global_step=global_step)\n        else:\n            if tag not in self._avg_scalars:\n                self._avg_scalars[tag] = ScalarAccumulator(self._dump_period)\n            avg_scalar = self._avg_scalars[tag]\n            avg_scalar.add(value)\n\n            if avg_scalar.is_full():\n                super().add_scalar(tag, avg_scalar.value,\n                                   global_step=global_step)\n                avg_scalar.reset()\n\n\nclass ScalarAccumulator(object):\n    def __init__(self, period):\n        self.sum = 0\n        self.cnt = 0\n        self.period = period\n\n    def add(self, value):\n        self.sum += value\n        self.cnt += 1\n\n    @property\n    def value(self):\n        if self.cnt > 0:\n            return self.sum / self.cnt\n        else:\n            return 0\n\n    def reset(self):\n        self.cnt = 0\n        self.sum = 0\n\n    def is_full(self):\n        return self.cnt >= self.period\n\n    def __len__(self):\n        return self.cnt\n"""
isegm/utils/misc.py,1,"b""from functools import partial\n\nimport torch\nimport numpy as np\n\nfrom .log import logger\n\n\ndef get_dims_with_exclusion(dim, exclude=None):\n    dims = list(range(dim))\n    if exclude is not None:\n        dims.remove(exclude)\n\n    return dims\n\n\ndef save_checkpoint(net, checkpoints_path, epoch=None, prefix='', verbose=True, multi_gpu=False):\n    if epoch is None:\n        checkpoint_name = 'last_checkpoint.pth'\n    else:\n        checkpoint_name = f'{epoch:03d}.pth'\n\n    if prefix:\n        checkpoint_name = f'{prefix}_{checkpoint_name}'\n\n    if not checkpoints_path.exists():\n        checkpoints_path.mkdir(parents=True)\n\n    checkpoint_path = checkpoints_path / checkpoint_name\n    if verbose:\n        logger.info(f'Save checkpoint to {str(checkpoint_path)}')\n\n    state_dict = net.module.state_dict() if multi_gpu else net.state_dict()\n    torch.save(state_dict, str(checkpoint_path))\n\n\ndef get_unique_labels(mask):\n    return np.nonzero(np.bincount(mask.flatten() + 1))[0] - 1\n\n\ndef get_bbox_from_mask(mask):\n    rows = np.any(mask, axis=1)\n    cols = np.any(mask, axis=0)\n    rmin, rmax = np.where(rows)[0][[0, -1]]\n    cmin, cmax = np.where(cols)[0][[0, -1]]\n\n    return rmin, rmax, cmin, cmax\n\n\ndef expand_bbox(bbox, expand_ratio, min_crop_size=None):\n    rmin, rmax, cmin, cmax = bbox\n    rcenter = 0.5 * (rmin + rmax)\n    ccenter = 0.5 * (cmin + cmax)\n    height = expand_ratio * (rmax - rmin + 1)\n    width = expand_ratio * (cmax - cmin + 1)\n    if min_crop_size is not None:\n        height = max(height, min_crop_size)\n        width = max(width, min_crop_size)\n\n    rmin = int(round(rcenter - 0.5 * height))\n    rmax = int(round(rcenter + 0.5 * height))\n    cmin = int(round(ccenter - 0.5 * width))\n    cmax = int(round(ccenter + 0.5 * width))\n\n    return rmin, rmax, cmin, cmax\n\n\ndef clamp_bbox(bbox, rmin, rmax, cmin, cmax):\n    return (max(rmin, bbox[0]), min(rmax, bbox[1]),\n            max(cmin, bbox[2]), min(cmax, bbox[3]))\n\n\ndef get_bbox_iou(b1, b2):\n    h_iou = get_segments_iou(b1[:2], b2[:2])\n    w_iou = get_segments_iou(b1[2:4], b2[2:4])\n    return h_iou * w_iou\n\n\ndef get_segments_iou(s1, s2):\n    a, b = s1\n    c, d = s2\n    intersection = max(0, min(b, d) - max(a, c) + 1)\n    union = max(1e-6, max(b, d) - min(a, c) + 1)\n    return intersection / union\n"""
isegm/utils/vis.py,0,"b'from functools import lru_cache\n\nimport cv2\nimport numpy as np\n\n\ndef visualize_instances(imask, bg_color=255,\n                        boundaries_color=None, boundaries_width=1, boundaries_alpha=0.8):\n    num_objects = imask.max() + 1\n    palette = get_palette(num_objects)\n    if bg_color is not None:\n        palette[0] = bg_color\n\n    result = palette[imask].astype(np.uint8)\n    if boundaries_color is not None:\n        boundaries_mask = get_boundaries(imask, boundaries_width=boundaries_width)\n        tresult = result.astype(np.float32)\n        tresult[boundaries_mask] = boundaries_color\n        tresult = tresult * boundaries_alpha + (1 - boundaries_alpha) * result\n        result = tresult.astype(np.uint8)\n\n    return result\n\n\n@lru_cache(maxsize=16)\ndef get_palette(num_cls):\n    palette = np.zeros(3 * num_cls, dtype=np.int32)\n\n    for j in range(0, num_cls):\n        lab = j\n        i = 0\n\n        while lab > 0:\n            palette[j*3 + 0] |= (((lab >> 0) & 1) << (7-i))\n            palette[j*3 + 1] |= (((lab >> 1) & 1) << (7-i))\n            palette[j*3 + 2] |= (((lab >> 2) & 1) << (7-i))\n            i = i + 1\n            lab >>= 3\n\n    return palette.reshape((-1, 3))\n\n\ndef visualize_mask(mask, num_cls):\n    palette = get_palette(num_cls)\n    mask[mask == -1] = 0\n\n    return palette[mask].astype(np.uint8)\n\n\ndef visualize_proposals(proposals_info, point_color=(255, 0, 0), point_radius=1):\n    proposal_map, colors, candidates = proposals_info\n\n    proposal_map = draw_probmap(proposal_map)\n    for x, y in candidates:\n        proposal_map = cv2.circle(proposal_map, (y, x), point_radius, point_color, -1)\n\n    return proposal_map\n\n\ndef draw_probmap(x):\n    return cv2.applyColorMap((x * 255).astype(np.uint8), cv2.COLORMAP_HOT)\n\n\ndef draw_points(image, points, color, radius=3):\n    image = image.copy()\n    for p in points:\n        image = cv2.circle(image, (int(p[1]), int(p[0])), radius, color, -1)\n\n    return image\n\n\ndef draw_instance_map(x, palette=None):\n    num_colors = x.max() + 1\n    if palette is None:\n        palette = get_palette(num_colors)\n\n    return palette[x].astype(np.uint8)\n\n\ndef blend_mask(image, mask, alpha=0.6):\n    if mask.min() == -1:\n        mask = mask.copy() + 1\n\n    imap = draw_instance_map(mask)\n    result = (image * (1 - alpha) + alpha * imap).astype(np.uint8)\n    return result\n\n\ndef get_boundaries(instances_masks, boundaries_width=1):\n    boundaries = np.zeros((instances_masks.shape[0], instances_masks.shape[1]), dtype=np.bool)\n\n    for obj_id in np.unique(instances_masks.flatten()):\n        if obj_id == 0:\n            continue\n\n        obj_mask = instances_masks == obj_id\n        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n        inner_mask = cv2.erode(obj_mask.astype(np.uint8), kernel, iterations=boundaries_width).astype(np.bool)\n\n        obj_boundary = np.logical_xor(obj_mask, np.logical_and(inner_mask, obj_mask))\n        boundaries = np.logical_or(boundaries, obj_boundary)\n    return boundaries\n    \n \ndef draw_with_blend_and_clicks(img, mask=None, alpha=0.6, clicks_list=None, pos_color=(0, 255, 0),\n                               neg_color=(255, 0, 0), radius=4):\n    result = img.copy()\n\n    if mask is not None:\n        palette = get_palette(np.max(mask) + 1)\n        rgb_mask = palette[mask.astype(np.uint8)]\n\n        mask_region = (mask > 0).astype(np.uint8)\n        result = result * (1 - mask_region[:, :, np.newaxis]) + \\\n            (1 - alpha) * mask_region[:, :, np.newaxis] * result + \\\n            alpha * rgb_mask\n        result = result.astype(np.uint8)\n\n        # result = (result * (1 - alpha) + alpha * rgb_mask).astype(np.uint8)\n\n    if clicks_list is not None and len(clicks_list) > 0:\n        pos_points = [click.coords for click in clicks_list if click.is_positive]\n        neg_points = [click.coords for click in clicks_list if not click.is_positive]\n\n        result = draw_points(result, pos_points, pos_color, radius=radius)\n        result = draw_points(result, neg_points, neg_color, radius=radius)\n\n    return result\n\n'"
models/sbd/hrnet18_ocr64.py,1,"b""import random\nfrom functools import partial\n\nimport torch\nfrom torchvision import transforms\nfrom easydict import EasyDict as edict\nfrom albumentations import (\n    Compose, ShiftScaleRotate, PadIfNeeded, RandomCrop,\n    RGBShift, RandomBrightnessContrast, RandomRotate90, Flip\n)\n\nfrom isegm.engine.trainer import ISTrainer\nfrom isegm.model.is_hrnet_model import get_hrnet_model\nfrom isegm.model.losses import SigmoidBinaryCrossEntropyLoss\nfrom isegm.model.metrics import AdaptiveIoU\nfrom isegm.data.sbd import SBDDataset\nfrom isegm.data.points_sampler import MultiPointSampler\nfrom isegm.utils.log import logger\nfrom isegm.model import initializer\n\n\ndef main(cfg):\n    model, model_cfg = init_model(cfg)\n    train(model, cfg, model_cfg, start_epoch=cfg.start_epoch)\n\n\ndef init_model(cfg):\n    model_cfg = edict()\n    model_cfg.crop_size = (320, 480)\n    model_cfg.input_normalization = {\n        'mean': [.485, .456, .406],\n        'std': [.229, .224, .225]\n    }\n    model_cfg.num_max_points = 10\n\n    model_cfg.input_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(model_cfg.input_normalization['mean'],\n                             model_cfg.input_normalization['std']),\n    ])\n\n    model = get_hrnet_model(width=18, ocr_width=64, with_aux_output=True)\n\n    model.to(cfg.device)\n    model.apply(initializer.XavierGluon(rnd_type='gaussian', magnitude=2.0))\n    model.feature_extractor.load_pretrained_weights(cfg.IMAGENET_PRETRAINED_MODELS.HRNETV2_W18)\n\n    return model, model_cfg\n\n\ndef train(model, cfg, model_cfg, start_epoch=0):\n    cfg.batch_size = 32 if cfg.batch_size < 1 else cfg.batch_size\n    cfg.val_batch_size = cfg.batch_size\n\n    cfg.input_normalization = model_cfg.input_normalization\n    crop_size = model_cfg.crop_size\n\n    loss_cfg = edict()\n    loss_cfg.instance_loss = SigmoidBinaryCrossEntropyLoss()\n    loss_cfg.instance_loss_weight = 1.0\n    loss_cfg.instance_aux_loss = SigmoidBinaryCrossEntropyLoss()\n    loss_cfg.instance_aux_loss_weight = 0.4\n\n    num_epochs = 120\n    num_masks = 1\n\n    train_augmentator = Compose([\n        Flip(),\n        RandomRotate90(),\n        ShiftScaleRotate(shift_limit=0.03, scale_limit=0,\n                         rotate_limit=(-3, 3), border_mode=0, p=0.75),\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size),\n        RandomBrightnessContrast(brightness_limit=(-0.25, 0.25), contrast_limit=(-0.15, 0.4), p=0.75),\n        RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.75)\n    ], p=1.0)\n\n    val_augmentator = Compose([\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size)\n    ], p=1.0)\n\n    def scale_func(image_shape):\n        return random.uniform(0.75, 1.25)\n\n    points_sampler = MultiPointSampler(model_cfg.num_max_points, prob_gamma=0.7,\n                                       merge_objects_prob=0.15,\n                                       max_num_merged_objects=2)\n\n    trainset = SBDDataset(\n        cfg.SBD_PATH,\n        split='train',\n        num_masks=num_masks,\n        augmentator=train_augmentator,\n        points_from_one_object=False,\n        input_transform=model_cfg.input_transform,\n        min_object_area=80,\n        keep_background_prob=0.0,\n        image_rescale=scale_func,\n        points_sampler=points_sampler,\n        samples_scores_path='./models/sbd/sbd_samples_weights.pkl',\n        samples_scores_gamma=1.25\n    )\n\n    valset = SBDDataset(\n        cfg.SBD_PATH,\n        split='val',\n        augmentator=val_augmentator,\n        num_masks=num_masks,\n        points_from_one_object=False,\n        input_transform=model_cfg.input_transform,\n        min_object_area=80,\n        image_rescale=scale_func,\n        points_sampler=points_sampler\n    )\n\n    optimizer_params = {\n        'lr': 5e-4, 'betas': (0.9, 0.999), 'eps': 1e-8\n    }\n\n    lr_scheduler = partial(torch.optim.lr_scheduler.MultiStepLR,\n                           milestones=[100], gamma=0.1)\n    trainer = ISTrainer(model, cfg, model_cfg, loss_cfg,\n                        trainset, valset,\n                        optimizer='adam',\n                        optimizer_params=optimizer_params,\n                        lr_scheduler=lr_scheduler,\n                        checkpoint_interval=5,\n                        image_dump_interval=100,\n                        metrics=[AdaptiveIoU()],\n                        max_interactive_points=model_cfg.num_max_points)\n    logger.info(f'Starting Epoch: {start_epoch}')\n    logger.info(f'Total Epochs: {num_epochs}')\n    for epoch in range(start_epoch, num_epochs):\n        trainer.training(epoch)\n        trainer.validation(epoch)\n"""
models/sbd/hrnet32_ocr128.py,1,"b""import random\nfrom functools import partial\n\nimport torch\nfrom torchvision import transforms\nfrom easydict import EasyDict as edict\nfrom albumentations import (\n    Compose, ShiftScaleRotate, PadIfNeeded, RandomCrop,\n    RGBShift, RandomBrightnessContrast, RandomRotate90, Flip\n)\n\nfrom isegm.engine.trainer import ISTrainer\nfrom isegm.model.is_hrnet_model import get_hrnet_model\nfrom isegm.model.losses import SigmoidBinaryCrossEntropyLoss\nfrom isegm.model.metrics import AdaptiveIoU\nfrom isegm.data.sbd import SBDDataset\nfrom isegm.data.points_sampler import MultiPointSampler\nfrom isegm.utils.log import logger\nfrom isegm.model import initializer\n\n\ndef main(cfg):\n    model, model_cfg = init_model(cfg)\n    train(model, cfg, model_cfg, start_epoch=cfg.start_epoch)\n\n\ndef init_model(cfg):\n    model_cfg = edict()\n    model_cfg.crop_size = (320, 480)\n    model_cfg.input_normalization = {\n        'mean': [.485, .456, .406],\n        'std': [.229, .224, .225]\n    }\n    model_cfg.num_max_points = 10\n\n    model_cfg.input_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(model_cfg.input_normalization['mean'],\n                             model_cfg.input_normalization['std']),\n    ])\n\n    model = get_hrnet_model(width=32, ocr_width=128, with_aux_output=True)\n\n    model.to(cfg.device)\n    model.apply(initializer.XavierGluon(rnd_type='gaussian', magnitude=2.0))\n    model.feature_extractor.load_pretrained_weights(cfg.IMAGENET_PRETRAINED_MODELS.HRNETV2_W32)\n\n    return model, model_cfg\n\n\ndef train(model, cfg, model_cfg, start_epoch=0):\n    cfg.batch_size = 32 if cfg.batch_size < 1 else cfg.batch_size\n    cfg.val_batch_size = cfg.batch_size\n\n    cfg.input_normalization = model_cfg.input_normalization\n    crop_size = model_cfg.crop_size\n\n    loss_cfg = edict()\n    loss_cfg.instance_loss = SigmoidBinaryCrossEntropyLoss()\n    loss_cfg.instance_loss_weight = 1.0\n    loss_cfg.instance_aux_loss = SigmoidBinaryCrossEntropyLoss()\n    loss_cfg.instance_aux_loss_weight = 0.4\n\n    num_epochs = 120\n    num_masks = 1\n\n    train_augmentator = Compose([\n        Flip(),\n        RandomRotate90(),\n        ShiftScaleRotate(shift_limit=0.03, scale_limit=0,\n                         rotate_limit=(-3, 3), border_mode=0, p=0.75),\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size),\n        RandomBrightnessContrast(brightness_limit=(-0.25, 0.25), contrast_limit=(-0.15, 0.4), p=0.75),\n        RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.75)\n    ], p=1.0)\n\n    val_augmentator = Compose([\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size)\n    ], p=1.0)\n\n    def scale_func(image_shape):\n        return random.uniform(0.75, 1.25)\n\n    points_sampler = MultiPointSampler(model_cfg.num_max_points, prob_gamma=0.7,\n                                       merge_objects_prob=0.15,\n                                       max_num_merged_objects=2)\n\n    trainset = SBDDataset(\n        cfg.SBD_PATH,\n        split='train',\n        num_masks=num_masks,\n        augmentator=train_augmentator,\n        points_from_one_object=False,\n        input_transform=model_cfg.input_transform,\n        min_object_area=80,\n        keep_background_prob=0.0,\n        image_rescale=scale_func,\n        points_sampler=points_sampler,\n        samples_scores_path='./models/sbd/sbd_samples_weights.pkl',\n        samples_scores_gamma=1.25\n    )\n\n    valset = SBDDataset(\n        cfg.SBD_PATH,\n        split='val',\n        augmentator=val_augmentator,\n        num_masks=num_masks,\n        points_from_one_object=False,\n        input_transform=model_cfg.input_transform,\n        min_object_area=80,\n        image_rescale=scale_func,\n        points_sampler=points_sampler\n    )\n\n    optimizer_params = {\n        'lr': 5e-4, 'betas': (0.9, 0.999), 'eps': 1e-8\n    }\n\n    lr_scheduler = partial(torch.optim.lr_scheduler.MultiStepLR,\n                           milestones=[100], gamma=0.1)\n    trainer = ISTrainer(model, cfg, model_cfg, loss_cfg,\n                        trainset, valset,\n                        optimizer='adam',\n                        optimizer_params=optimizer_params,\n                        lr_scheduler=lr_scheduler,\n                        checkpoint_interval=5,\n                        image_dump_interval=100,\n                        metrics=[AdaptiveIoU()],\n                        max_interactive_points=model_cfg.num_max_points)\n    logger.info(f'Starting Epoch: {start_epoch}')\n    logger.info(f'Total Epochs: {num_epochs}')\n    for epoch in range(start_epoch, num_epochs):\n        trainer.training(epoch)\n        trainer.validation(epoch)\n"""
models/sbd/hrnet48_ocr128.py,1,"b""import random\nfrom functools import partial\n\nimport torch\nfrom torchvision import transforms\nfrom easydict import EasyDict as edict\nfrom albumentations import (\n    Compose, ShiftScaleRotate, PadIfNeeded, RandomCrop,\n    RGBShift, RandomBrightnessContrast, RandomRotate90, Flip\n)\n\nfrom isegm.engine.trainer import ISTrainer\nfrom isegm.model.is_hrnet_model import get_hrnet_model\nfrom isegm.model.losses import SigmoidBinaryCrossEntropyLoss\nfrom isegm.model.metrics import AdaptiveIoU\nfrom isegm.data.sbd import SBDDataset\nfrom isegm.data.points_sampler import MultiPointSampler\nfrom isegm.utils.log import logger\nfrom isegm.model import initializer\n\n\ndef main(cfg):\n    model, model_cfg = init_model(cfg)\n    train(model, cfg, model_cfg, start_epoch=cfg.start_epoch)\n\n\ndef init_model(cfg):\n    model_cfg = edict()\n    model_cfg.crop_size = (320, 480)\n    model_cfg.input_normalization = {\n        'mean': [.485, .456, .406],\n        'std': [.229, .224, .225]\n    }\n    model_cfg.num_max_points = 10\n\n    model_cfg.input_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(model_cfg.input_normalization['mean'],\n                             model_cfg.input_normalization['std']),\n    ])\n\n    model = get_hrnet_model(width=48, ocr_width=128, with_aux_output=True)\n\n    model.to(cfg.device)\n    model.apply(initializer.XavierGluon(rnd_type='gaussian', magnitude=2.0))\n    model.feature_extractor.load_pretrained_weights(cfg.IMAGENET_PRETRAINED_MODELS.HRNETV2_W32)\n\n    return model, model_cfg\n\n\ndef train(model, cfg, model_cfg, start_epoch=0):\n    cfg.batch_size = 32 if cfg.batch_size < 1 else cfg.batch_size\n    cfg.val_batch_size = cfg.batch_size\n\n    cfg.input_normalization = model_cfg.input_normalization\n    crop_size = model_cfg.crop_size\n\n    loss_cfg = edict()\n    loss_cfg.instance_loss = SigmoidBinaryCrossEntropyLoss()\n    loss_cfg.instance_loss_weight = 1.0\n    loss_cfg.instance_aux_loss = SigmoidBinaryCrossEntropyLoss()\n    loss_cfg.instance_aux_loss_weight = 0.4\n\n    num_epochs = 120\n    num_masks = 1\n\n    train_augmentator = Compose([\n        Flip(),\n        RandomRotate90(),\n        ShiftScaleRotate(shift_limit=0.03, scale_limit=0,\n                         rotate_limit=(-3, 3), border_mode=0, p=0.75),\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size),\n        RandomBrightnessContrast(brightness_limit=(-0.25, 0.25), contrast_limit=(-0.15, 0.4), p=0.75),\n        RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.75)\n    ], p=1.0)\n\n    val_augmentator = Compose([\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size)\n    ], p=1.0)\n\n    def scale_func(image_shape):\n        return random.uniform(0.75, 1.25)\n\n    points_sampler = MultiPointSampler(model_cfg.num_max_points, prob_gamma=0.7,\n                                       merge_objects_prob=0.15,\n                                       max_num_merged_objects=2)\n\n    trainset = SBDDataset(\n        cfg.SBD_PATH,\n        split='train',\n        num_masks=num_masks,\n        augmentator=train_augmentator,\n        points_from_one_object=False,\n        input_transform=model_cfg.input_transform,\n        min_object_area=80,\n        keep_background_prob=0.0,\n        image_rescale=scale_func,\n        points_sampler=points_sampler,\n        samples_scores_path='./models/sbd/sbd_samples_weights.pkl',\n        samples_scores_gamma=1.25\n    )\n\n    valset = SBDDataset(\n        cfg.SBD_PATH,\n        split='val',\n        augmentator=val_augmentator,\n        num_masks=num_masks,\n        points_from_one_object=False,\n        input_transform=model_cfg.input_transform,\n        min_object_area=80,\n        image_rescale=scale_func,\n        points_sampler=points_sampler\n    )\n\n    optimizer_params = {\n        'lr': 5e-4, 'betas': (0.9, 0.999), 'eps': 1e-8\n    }\n\n    lr_scheduler = partial(torch.optim.lr_scheduler.MultiStepLR,\n                           milestones=[100], gamma=0.1)\n    trainer = ISTrainer(model, cfg, model_cfg, loss_cfg,\n                        trainset, valset,\n                        optimizer='adam',\n                        optimizer_params=optimizer_params,\n                        lr_scheduler=lr_scheduler,\n                        checkpoint_interval=5,\n                        image_dump_interval=100,\n                        metrics=[AdaptiveIoU()],\n                        max_interactive_points=model_cfg.num_max_points)\n    logger.info(f'Starting Epoch: {start_epoch}')\n    logger.info(f'Total Epochs: {num_epochs}')\n    for epoch in range(start_epoch, num_epochs):\n        trainer.training(epoch)\n        trainer.validation(epoch)\n"""
models/sbd/r101_dh256.py,1,"b""import random\nfrom functools import partial\n\nimport torch\nfrom torchvision import transforms\nfrom easydict import EasyDict as edict\nfrom albumentations import (\n    Compose, ShiftScaleRotate, PadIfNeeded, RandomCrop,\n    RGBShift, RandomBrightnessContrast, RandomRotate90, Flip\n)\n\nfrom isegm.engine.trainer import ISTrainer\nfrom isegm.model.is_deeplab_model import get_deeplab_model\nfrom isegm.model.losses import SigmoidBinaryCrossEntropyLoss\nfrom isegm.model.metrics import AdaptiveIoU\nfrom isegm.data.sbd import SBDDataset\nfrom isegm.data.points_sampler import MultiPointSampler\nfrom isegm.utils.log import logger\nfrom isegm.model import initializer\n\n\ndef main(cfg):\n    model, model_cfg = init_model(cfg)\n    train(model, cfg, model_cfg, start_epoch=cfg.start_epoch)\n\n\ndef init_model(cfg):\n    model_cfg = edict()\n    model_cfg.crop_size = (320, 480)\n    model_cfg.input_normalization = {\n        'mean': [.485, .456, .406],\n        'std': [.229, .224, .225]\n    }\n    model_cfg.num_max_points = 10\n\n    model_cfg.input_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(model_cfg.input_normalization['mean'],\n                             model_cfg.input_normalization['std']),\n    ])\n\n    model = get_deeplab_model(backbone='resnet101', deeplab_ch=256, aspp_dropout=0.50)\n\n    model.to(cfg.device)\n    model.apply(initializer.XavierGluon(rnd_type='gaussian', magnitude=2.0))\n    model.feature_extractor.load_pretrained_weights()\n\n    return model, model_cfg\n\n\ndef train(model, cfg, model_cfg, start_epoch=0):\n    cfg.batch_size = 28 if cfg.batch_size < 1 else cfg.batch_size\n    cfg.val_batch_size = cfg.batch_size\n\n    cfg.input_normalization = model_cfg.input_normalization\n    crop_size = model_cfg.crop_size\n\n    loss_cfg = edict()\n    loss_cfg.instance_loss = SigmoidBinaryCrossEntropyLoss()\n    loss_cfg.instance_loss_weight = 1.0\n\n    num_epochs = 120\n    num_masks = 1\n\n    train_augmentator = Compose([\n        Flip(),\n        RandomRotate90(),\n        ShiftScaleRotate(shift_limit=0.03, scale_limit=0,\n                         rotate_limit=(-3, 3), border_mode=0, p=0.75),\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size),\n        RandomBrightnessContrast(brightness_limit=(-0.25, 0.25), contrast_limit=(-0.15, 0.4), p=0.75),\n        RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.75)\n    ], p=1.0)\n\n    val_augmentator = Compose([\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size)\n    ], p=1.0)\n\n    def scale_func(image_shape):\n        return random.uniform(0.75, 1.25)\n\n    points_sampler = MultiPointSampler(model_cfg.num_max_points, prob_gamma=0.7,\n                                       merge_objects_prob=0.15,\n                                       max_num_merged_objects=2)\n\n    trainset = SBDDataset(\n        cfg.SBD_PATH,\n        split='train',\n        num_masks=num_masks,\n        augmentator=train_augmentator,\n        points_from_one_object=False,\n        input_transform=model_cfg.input_transform,\n        min_object_area=80,\n        keep_background_prob=0.0,\n        image_rescale=scale_func,\n        points_sampler=points_sampler,\n        samples_scores_path='./models/sbd/sbd_samples_weights.pkl',\n        samples_scores_gamma=1.25\n    )\n\n    valset = SBDDataset(\n        cfg.SBD_PATH,\n        split='val',\n        augmentator=val_augmentator,\n        num_masks=num_masks,\n        points_from_one_object=False,\n        input_transform=model_cfg.input_transform,\n        min_object_area=80,\n        image_rescale=scale_func,\n        points_sampler=points_sampler\n    )\n\n    optimizer_params = {\n        'lr': 5e-4, 'betas': (0.9, 0.999), 'eps': 1e-8\n    }\n\n    lr_scheduler = partial(torch.optim.lr_scheduler.MultiStepLR,\n                           milestones=[100], gamma=0.1)\n    trainer = ISTrainer(model, cfg, model_cfg, loss_cfg,\n                        trainset, valset,\n                        optimizer_params=optimizer_params,\n                        lr_scheduler=lr_scheduler,\n                        checkpoint_interval=5,\n                        image_dump_interval=200,\n                        metrics=[AdaptiveIoU()],\n                        max_interactive_points=model_cfg.num_max_points)\n    logger.info(f'Starting Epoch: {start_epoch}')\n    logger.info(f'Total Epochs: {num_epochs}')\n    for epoch in range(start_epoch, num_epochs):\n        trainer.training(epoch)\n        trainer.validation(epoch)\n"""
models/sbd/r34_dh128.py,1,"b""import random\nfrom functools import partial\n\nimport torch\nfrom torchvision import transforms\nfrom easydict import EasyDict as edict\nfrom albumentations import (\n    Compose, ShiftScaleRotate, PadIfNeeded, RandomCrop,\n    RGBShift, RandomBrightnessContrast, RandomRotate90, Flip\n)\n\nfrom isegm.engine.trainer import ISTrainer\nfrom isegm.model.is_deeplab_model import get_deeplab_model\nfrom isegm.model.losses import SigmoidBinaryCrossEntropyLoss\nfrom isegm.model.metrics import AdaptiveIoU\nfrom isegm.data.sbd import SBDDataset\nfrom isegm.data.points_sampler import MultiPointSampler\nfrom isegm.utils.log import logger\nfrom isegm.model import initializer\n\n\ndef main(cfg):\n    model, model_cfg = init_model(cfg)\n    train(model, cfg, model_cfg, start_epoch=cfg.start_epoch)\n\n\ndef init_model(cfg):\n    model_cfg = edict()\n    model_cfg.crop_size = (320, 480)\n    model_cfg.input_normalization = {\n        'mean': [.485, .456, .406],\n        'std': [.229, .224, .225]\n    }\n    model_cfg.num_max_points = 10\n\n    model_cfg.input_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(model_cfg.input_normalization['mean'],\n                             model_cfg.input_normalization['std']),\n    ])\n\n    model = get_deeplab_model(backbone='resnet34', deeplab_ch=128, aspp_dropout=0.20)\n\n    model.to(cfg.device)\n    model.apply(initializer.XavierGluon(rnd_type='gaussian', magnitude=2.0))\n    model.feature_extractor.load_pretrained_weights()\n\n    return model, model_cfg\n\n\ndef train(model, cfg, model_cfg, start_epoch=0):\n    cfg.batch_size = 28 if cfg.batch_size < 1 else cfg.batch_size\n    cfg.val_batch_size = cfg.batch_size\n\n    cfg.input_normalization = model_cfg.input_normalization\n    crop_size = model_cfg.crop_size\n\n    loss_cfg = edict()\n    loss_cfg.instance_loss = SigmoidBinaryCrossEntropyLoss()\n    loss_cfg.instance_loss_weight = 1.0\n\n    num_epochs = 120\n    num_masks = 1\n\n    train_augmentator = Compose([\n        Flip(),\n        RandomRotate90(),\n        ShiftScaleRotate(shift_limit=0.03, scale_limit=0,\n                         rotate_limit=(-3, 3), border_mode=0, p=0.75),\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size),\n        RandomBrightnessContrast(brightness_limit=(-0.25, 0.25), contrast_limit=(-0.15, 0.4), p=0.75),\n        RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.75)\n    ], p=1.0)\n\n    val_augmentator = Compose([\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size)\n    ], p=1.0)\n\n    def scale_func(image_shape):\n        return random.uniform(0.75, 1.25)\n\n    points_sampler = MultiPointSampler(model_cfg.num_max_points, prob_gamma=0.7,\n                                       merge_objects_prob=0.15,\n                                       max_num_merged_objects=2)\n\n    trainset = SBDDataset(\n        cfg.SBD_PATH,\n        split='train',\n        num_masks=num_masks,\n        augmentator=train_augmentator,\n        points_from_one_object=False,\n        input_transform=model_cfg.input_transform,\n        min_object_area=80,\n        keep_background_prob=0.0,\n        image_rescale=scale_func,\n        points_sampler=points_sampler,\n        samples_scores_path='./models/sbd/sbd_samples_weights.pkl',\n        samples_scores_gamma=1.25\n    )\n\n    valset = SBDDataset(\n        cfg.SBD_PATH,\n        split='val',\n        augmentator=val_augmentator,\n        num_masks=num_masks,\n        points_from_one_object=False,\n        input_transform=model_cfg.input_transform,\n        min_object_area=80,\n        image_rescale=scale_func,\n        points_sampler=points_sampler\n    )\n\n    optimizer_params = {\n        'lr': 5e-4, 'betas': (0.9, 0.999), 'eps': 1e-8\n    }\n\n    lr_scheduler = partial(torch.optim.lr_scheduler.MultiStepLR,\n                           milestones=[100], gamma=0.1)\n    trainer = ISTrainer(model, cfg, model_cfg, loss_cfg,\n                        trainset, valset,\n                        optimizer_params=optimizer_params,\n                        lr_scheduler=lr_scheduler,\n                        checkpoint_interval=5,\n                        image_dump_interval=200,\n                        metrics=[AdaptiveIoU()],\n                        max_interactive_points=model_cfg.num_max_points)\n    logger.info(f'Starting Epoch: {start_epoch}')\n    logger.info(f'Total Epochs: {num_epochs}')\n    for epoch in range(start_epoch, num_epochs):\n        trainer.training(epoch)\n        trainer.validation(epoch)\n"""
models/sbd/r50_dh128.py,1,"b""import random\nfrom functools import partial\n\nimport torch\nfrom torchvision import transforms\nfrom easydict import EasyDict as edict\nfrom albumentations import (\n    Compose, ShiftScaleRotate, PadIfNeeded, RandomCrop,\n    RGBShift, RandomBrightnessContrast, RandomRotate90, Flip\n)\n\nfrom isegm.engine.trainer import ISTrainer\nfrom isegm.model.is_deeplab_model import get_deeplab_model\nfrom isegm.model.losses import SigmoidBinaryCrossEntropyLoss\nfrom isegm.model.metrics import AdaptiveIoU\nfrom isegm.data.sbd import SBDDataset\nfrom isegm.data.points_sampler import MultiPointSampler\nfrom isegm.utils.log import logger\nfrom isegm.model import initializer\n\n\ndef main(cfg):\n    model, model_cfg = init_model(cfg)\n    train(model, cfg, model_cfg, start_epoch=cfg.start_epoch)\n\n\ndef init_model(cfg):\n    model_cfg = edict()\n    model_cfg.crop_size = (320, 480)\n    model_cfg.input_normalization = {\n        'mean': [.485, .456, .406],\n        'std': [.229, .224, .225]\n    }\n    model_cfg.num_max_points = 10\n\n    model_cfg.input_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(model_cfg.input_normalization['mean'],\n                             model_cfg.input_normalization['std']),\n    ])\n\n    model = get_deeplab_model(backbone='resnet50', deeplab_ch=128, aspp_dropout=0.20)\n\n    model.to(cfg.device)\n    model.apply(initializer.XavierGluon(rnd_type='gaussian', magnitude=2.0))\n    model.feature_extractor.load_pretrained_weights()\n\n    return model, model_cfg\n\n\ndef train(model, cfg, model_cfg, start_epoch=0):\n    cfg.batch_size = 28 if cfg.batch_size < 1 else cfg.batch_size\n    cfg.val_batch_size = cfg.batch_size\n\n    cfg.input_normalization = model_cfg.input_normalization\n    crop_size = model_cfg.crop_size\n\n    loss_cfg = edict()\n    loss_cfg.instance_loss = SigmoidBinaryCrossEntropyLoss()\n    loss_cfg.instance_loss_weight = 1.0\n\n    num_epochs = 120\n    num_masks = 1\n\n    train_augmentator = Compose([\n        Flip(),\n        RandomRotate90(),\n        ShiftScaleRotate(shift_limit=0.03, scale_limit=0,\n                         rotate_limit=(-3, 3), border_mode=0, p=0.75),\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size),\n        RandomBrightnessContrast(brightness_limit=(-0.25, 0.25), contrast_limit=(-0.15, 0.4), p=0.75),\n        RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=0.75)\n    ], p=1.0)\n\n    val_augmentator = Compose([\n        PadIfNeeded(min_height=crop_size[0], min_width=crop_size[1], border_mode=0),\n        RandomCrop(*crop_size)\n    ], p=1.0)\n\n    def scale_func(image_shape):\n        return random.uniform(0.75, 1.25)\n\n    points_sampler = MultiPointSampler(model_cfg.num_max_points, prob_gamma=0.7,\n                                       merge_objects_prob=0.15,\n                                       max_num_merged_objects=2)\n\n    trainset = SBDDataset(\n        cfg.SBD_PATH,\n        split='train',\n        num_masks=num_masks,\n        augmentator=train_augmentator,\n        points_from_one_object=False,\n        input_transform=model_cfg.input_transform,\n        min_object_area=80,\n        keep_background_prob=0.0,\n        image_rescale=scale_func,\n        points_sampler=points_sampler,\n        samples_scores_path='./models/sbd/sbd_samples_weights.pkl',\n        samples_scores_gamma=1.25\n    )\n\n    valset = SBDDataset(\n        cfg.SBD_PATH,\n        split='val',\n        augmentator=val_augmentator,\n        num_masks=num_masks,\n        points_from_one_object=False,\n        input_transform=model_cfg.input_transform,\n        min_object_area=80,\n        image_rescale=scale_func,\n        points_sampler=points_sampler\n    )\n\n    optimizer_params = {\n        'lr': 5e-4, 'betas': (0.9, 0.999), 'eps': 1e-8\n    }\n\n    lr_scheduler = partial(torch.optim.lr_scheduler.MultiStepLR,\n                           milestones=[100], gamma=0.1)\n    trainer = ISTrainer(model, cfg, model_cfg, loss_cfg,\n                        trainset, valset,\n                        optimizer_params=optimizer_params,\n                        lr_scheduler=lr_scheduler,\n                        checkpoint_interval=5,\n                        image_dump_interval=200,\n                        metrics=[AdaptiveIoU()],\n                        max_interactive_points=model_cfg.num_max_points)\n    logger.info(f'Starting Epoch: {start_epoch}')\n    logger.info(f'Total Epochs: {num_epochs}')\n    for epoch in range(start_epoch, num_epochs):\n        trainer.training(epoch)\n        trainer.validation(epoch)\n"""
isegm/inference/predictors/__init__.py,0,"b""from .base import BasePredictor\nfrom .brs import InputBRSPredictor, FeatureBRSPredictor, HRNetFeatureBRSPredictor\nfrom .brs_functors import InputOptimizer, ScaleBiasOptimizer\nfrom isegm.inference.transforms import ZoomIn\nfrom isegm.model.is_hrnet_model import DistMapsHRNetModel\n\n\ndef get_predictor(net, brs_mode, device,\n                  prob_thresh=0.49,\n                  with_flip=True,\n                  zoom_in_params=dict(),\n                  predictor_params=None,\n                  brs_opt_func_params=None,\n                  lbfgs_params=None):\n    lbfgs_params_ = {\n        'm': 20,\n        'factr': 0,\n        'pgtol': 1e-8,\n        'maxfun': 20,\n    }\n\n    predictor_params_ = {\n        'optimize_after_n_clicks': 1\n    }\n\n    if zoom_in_params is not None:\n        zoom_in = ZoomIn(**zoom_in_params)\n    else:\n        zoom_in = None\n\n    if lbfgs_params is not None:\n        lbfgs_params_.update(lbfgs_params)\n    lbfgs_params_['maxiter'] = 2 * lbfgs_params_['maxfun']\n\n    if brs_opt_func_params is None:\n        brs_opt_func_params = dict()\n\n    if brs_mode == 'NoBRS':\n        if predictor_params is not None:\n            predictor_params_.update(predictor_params)\n        predictor = BasePredictor(net, device, zoom_in=zoom_in, with_flip=with_flip, **predictor_params_)\n    elif brs_mode.startswith('f-BRS'):\n        predictor_params_.update({\n            'net_clicks_limit': 8,\n        })\n        if predictor_params is not None:\n            predictor_params_.update(predictor_params)\n\n        insertion_mode = {\n            'f-BRS-A': 'after_c4',\n            'f-BRS-B': 'after_aspp',\n            'f-BRS-C': 'after_deeplab'\n        }[brs_mode]\n\n        opt_functor = ScaleBiasOptimizer(prob_thresh=prob_thresh,\n                                         with_flip=with_flip,\n                                         optimizer_params=lbfgs_params_,\n                                         **brs_opt_func_params)\n\n        if isinstance(net, DistMapsHRNetModel):\n            FeaturePredictor = HRNetFeatureBRSPredictor\n            insertion_mode = {'after_c4': 'A', 'after_aspp': 'A', 'after_deeplab': 'C'}[insertion_mode]\n        else:\n            FeaturePredictor = FeatureBRSPredictor\n\n        predictor = FeaturePredictor(net, device,\n                                     opt_functor=opt_functor,\n                                     with_flip=with_flip,\n                                     insertion_mode=insertion_mode,\n                                     zoom_in=zoom_in,\n                                     **predictor_params_)\n    elif brs_mode == 'RGB-BRS' or brs_mode == 'DistMap-BRS':\n        use_dmaps = brs_mode == 'DistMap-BRS'\n\n        predictor_params_.update({\n            'net_clicks_limit': 5,\n        })\n        if predictor_params is not None:\n            predictor_params_.update(predictor_params)\n\n        opt_functor = InputOptimizer(prob_thresh=prob_thresh,\n                                     with_flip=with_flip,\n                                     optimizer_params=lbfgs_params_,\n                                     **brs_opt_func_params)\n\n        predictor = InputBRSPredictor(net, device,\n                                      optimize_target='dmaps' if use_dmaps else 'rgb',\n                                      opt_functor=opt_functor,\n                                      with_flip=with_flip,\n                                      zoom_in=zoom_in,\n                                      **predictor_params_)\n    else:\n        raise NotImplementedError\n\n    return predictor\n"""
isegm/inference/predictors/base.py,2,"b""import torch\nimport torch.nn.functional as F\n\nfrom isegm.inference.transforms import AddHorizontalFlip, SigmoidForPred, LimitLongestSide\n\n\nclass BasePredictor(object):\n    def __init__(self, net, device,\n                 net_clicks_limit=None,\n                 with_flip=False,\n                 zoom_in=None,\n                 max_size=None,\n                 **kwargs):\n        self.net = net\n        self.with_flip = with_flip\n        self.net_clicks_limit = net_clicks_limit\n        self.original_image = None\n        self.device = device\n        self.zoom_in = zoom_in\n\n        self.transforms = [zoom_in] if zoom_in is not None else []\n        if max_size is not None:\n            self.transforms.append(LimitLongestSide(max_size=max_size))\n        self.transforms.append(SigmoidForPred())\n        if with_flip:\n            self.transforms.append(AddHorizontalFlip())\n\n    def set_input_image(self, image_nd):\n        for transform in self.transforms:\n            transform.reset()\n        self.original_image = image_nd.to(self.device)\n        if len(self.original_image.shape) == 3:\n            self.original_image = self.original_image.unsqueeze(0)\n\n    def get_prediction(self, clicker):\n        clicks_list = clicker.get_clicks()\n\n        image_nd, clicks_lists, is_image_changed = self.apply_transforms(\n            self.original_image, [clicks_list]\n        )\n\n        pred_logits = self._get_prediction(image_nd, clicks_lists, is_image_changed)\n        prediction = F.interpolate(pred_logits, mode='bilinear', align_corners=True,\n                                   size=image_nd.size()[2:])\n\n        for t in reversed(self.transforms):\n            prediction = t.inv_transform(prediction)\n\n        if self.zoom_in is not None and self.zoom_in.check_possible_recalculation():\n            return self.get_prediction(clicker)\n\n        return prediction.cpu().numpy()[0, 0]\n\n    def _get_prediction(self, image_nd, clicks_lists, is_image_changed):\n        points_nd = self.get_points_nd(clicks_lists)\n        return self.net(image_nd, points_nd)['instances']\n\n    def _get_transform_states(self):\n        return [x.get_state() for x in self.transforms]\n\n    def _set_transform_states(self, states):\n        assert len(states) == len(self.transforms)\n        for state, transform in zip(states, self.transforms):\n            transform.set_state(state)\n\n    def apply_transforms(self, image_nd, clicks_lists):\n        is_image_changed = False\n        for t in self.transforms:\n            image_nd, clicks_lists = t.transform(image_nd, clicks_lists)\n            is_image_changed |= t.image_changed\n\n        return image_nd, clicks_lists, is_image_changed\n\n    def get_points_nd(self, clicks_lists):\n        total_clicks = []\n        num_pos_clicks = [sum(x.is_positive for x in clicks_list) for clicks_list in clicks_lists]\n        num_neg_clicks = [len(clicks_list) - num_pos for clicks_list, num_pos in zip(clicks_lists, num_pos_clicks)]\n        num_max_points = max(num_pos_clicks + num_neg_clicks)\n        if self.net_clicks_limit is not None:\n            num_max_points = min(self.net_clicks_limit, num_max_points)\n        num_max_points = max(1, num_max_points)\n\n        for clicks_list in clicks_lists:\n            clicks_list = clicks_list[:self.net_clicks_limit]\n            pos_clicks = [click.coords for click in clicks_list if click.is_positive]\n            pos_clicks = pos_clicks + (num_max_points - len(pos_clicks)) * [(-1, -1)]\n\n            neg_clicks = [click.coords for click in clicks_list if not click.is_positive]\n            neg_clicks = neg_clicks + (num_max_points - len(neg_clicks)) * [(-1, -1)]\n            total_clicks.append(pos_clicks + neg_clicks)\n\n        return torch.tensor(total_clicks, device=self.device)\n\n    def get_states(self):\n        return {'transform_states': self._get_transform_states()}\n\n    def set_states(self, states):\n        self._set_transform_states(states['transform_states'])\n"""
isegm/inference/predictors/brs.py,19,"b""import torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\n\nfrom .base import BasePredictor\nfrom isegm.model.is_hrnet_model import DistMapsHRNetModel\n\n\nclass BRSBasePredictor(BasePredictor):\n    def __init__(self, model, device, opt_functor, optimize_after_n_clicks=1, **kwargs):\n        super().__init__(model, device, **kwargs)\n        self.optimize_after_n_clicks = optimize_after_n_clicks\n        self.opt_functor = opt_functor\n\n        self.opt_data = None\n        self.input_data = None\n\n    def set_input_image(self, image_nd):\n        super().set_input_image(image_nd)\n        self.opt_data = None\n        self.input_data = None\n\n    def _get_clicks_maps_nd(self, clicks_lists, image_shape, radius=1):\n        pos_clicks_map = np.zeros((len(clicks_lists), 1) + image_shape, dtype=np.float32)\n        neg_clicks_map = np.zeros((len(clicks_lists), 1) + image_shape, dtype=np.float32)\n\n        for list_indx, clicks_list in enumerate(clicks_lists):\n            for click in clicks_list:\n                y, x = click.coords\n                y, x = int(round(y)), int(round(x))\n                y1, x1 = y - radius, x - radius\n                y2, x2 = y + radius + 1, x + radius + 1\n\n                if click.is_positive:\n                    pos_clicks_map[list_indx, 0, y1:y2, x1:x2] = True\n                else:\n                    neg_clicks_map[list_indx, 0, y1:y2, x1:x2] = True\n\n        with torch.no_grad():\n            pos_clicks_map = torch.from_numpy(pos_clicks_map).to(self.device)\n            neg_clicks_map = torch.from_numpy(neg_clicks_map).to(self.device)\n\n        return pos_clicks_map, neg_clicks_map\n\n    def get_states(self):\n        return {'transform_states': self._get_transform_states(), 'opt_data': self.opt_data}\n\n    def set_states(self, states):\n        self._set_transform_states(states['transform_states'])\n        self.opt_data = states['opt_data']\n\n\nclass FeatureBRSPredictor(BRSBasePredictor):\n    def __init__(self, model, device, opt_functor, insertion_mode='after_deeplab', **kwargs):\n        super().__init__(model, device, opt_functor=opt_functor, **kwargs)\n        self.insertion_mode = insertion_mode\n        self._c1_features = None\n\n        if self.insertion_mode == 'after_deeplab':\n            self.num_channels = model.feature_extractor.ch\n        elif self.insertion_mode == 'after_c4':\n            self.num_channels = model.feature_extractor.aspp_in_channels\n        elif self.insertion_mode == 'after_aspp':\n            self.num_channels = model.feature_extractor.ch + 32\n        else:\n            raise NotImplementedError\n\n    def _get_prediction(self, image_nd, clicks_lists, is_image_changed):\n        points_nd = self.get_points_nd(clicks_lists)\n        pos_mask, neg_mask = self._get_clicks_maps_nd(clicks_lists, image_nd.shape[2:])\n\n        num_clicks = len(clicks_lists[0])\n        bs = image_nd.shape[0] // 2 if self.with_flip else image_nd.shape[0]\n\n        if self.opt_data is None or self.opt_data.shape[0] // (2 * self.num_channels) != bs:\n            self.opt_data = np.zeros((bs * 2 * self.num_channels), dtype=np.float32)\n\n        if num_clicks <= self.net_clicks_limit or is_image_changed or self.input_data is None:\n            self.input_data = self._get_head_input(image_nd, points_nd)\n\n        def get_prediction_logits(scale, bias):\n            scale = scale.view(bs, -1, 1, 1)\n            bias = bias.view(bs, -1, 1, 1)\n            if self.with_flip:\n                scale = scale.repeat(2, 1, 1, 1)\n                bias = bias.repeat(2, 1, 1, 1)\n\n            scaled_backbone_features = self.input_data * scale\n            scaled_backbone_features = scaled_backbone_features + bias\n            if self.insertion_mode == 'after_c4':\n                x = self.net.feature_extractor.aspp(scaled_backbone_features)\n                x = F.interpolate(x, mode='bilinear', size=self._c1_features.size()[2:],\n                                  align_corners=True)\n                x = torch.cat((x, self._c1_features), dim=1)\n                scaled_backbone_features = self.net.feature_extractor.head(x)\n            elif self.insertion_mode == 'after_aspp':\n                scaled_backbone_features = self.net.feature_extractor.head(scaled_backbone_features)\n\n            pred_logits = self.net.head(scaled_backbone_features)\n            pred_logits = F.interpolate(pred_logits, size=image_nd.size()[2:], mode='bilinear',\n                                        align_corners=True)\n            return pred_logits\n\n        self.opt_functor.init_click(get_prediction_logits, pos_mask, neg_mask, self.device)\n        if num_clicks > self.optimize_after_n_clicks:\n            opt_result = fmin_l_bfgs_b(func=self.opt_functor, x0=self.opt_data,\n                                       **self.opt_functor.optimizer_params)\n            self.opt_data = opt_result[0]\n\n        with torch.no_grad():\n            if self.opt_functor.best_prediction is not None:\n                opt_pred_logits = self.opt_functor.best_prediction\n            else:\n                opt_data_nd = torch.from_numpy(self.opt_data).to(self.device)\n                opt_vars, _ = self.opt_functor.unpack_opt_params(opt_data_nd)\n                opt_pred_logits = get_prediction_logits(*opt_vars)\n\n        return opt_pred_logits\n\n    def _get_head_input(self, image_nd, points):\n        with torch.no_grad():\n            coord_features = self.net.dist_maps(image_nd, points)\n            x = self.net.rgb_conv(torch.cat((image_nd, coord_features), dim=1))\n            if self.insertion_mode == 'after_c4' or self.insertion_mode == 'after_aspp':\n                c1, _, c3, c4 = self.net.feature_extractor.backbone(x)\n                c1 = self.net.feature_extractor.skip_project(c1)\n\n                if self.insertion_mode == 'after_aspp':\n                    x = self.net.feature_extractor.aspp(c4)\n                    x = F.interpolate(x, size=c1.size()[2:], mode='bilinear', align_corners=True)\n                    x = torch.cat((x, c1), dim=1)\n                    backbone_features = x\n                else:\n                    backbone_features = c4\n                    self._c1_features = c1\n            else:\n                backbone_features = self.net.feature_extractor(x)[0]\n\n        return backbone_features\n\n\nclass HRNetFeatureBRSPredictor(BRSBasePredictor):\n    def __init__(self, model, device, opt_functor, insertion_mode='A', **kwargs):\n        super().__init__(model, device, opt_functor=opt_functor, **kwargs)\n        self.insertion_mode = insertion_mode\n        self._c1_features = None\n\n        if self.insertion_mode == 'A':\n            self.num_channels = sum(k * model.feature_extractor.width for k in [1, 2, 4, 8])\n        elif self.insertion_mode == 'C':\n            self.num_channels = 2 * model.feature_extractor.ocr_width\n        else:\n            raise NotImplementedError\n\n    def _get_prediction(self, image_nd, clicks_lists, is_image_changed):\n        points_nd = self.get_points_nd(clicks_lists)\n        pos_mask, neg_mask = self._get_clicks_maps_nd(clicks_lists, image_nd.shape[2:])\n        num_clicks = len(clicks_lists[0])\n        bs = image_nd.shape[0] // 2 if self.with_flip else image_nd.shape[0]\n\n        if self.opt_data is None or self.opt_data.shape[0] // (2 * self.num_channels) != bs:\n            self.opt_data = np.zeros((bs * 2 * self.num_channels), dtype=np.float32)\n\n        if num_clicks <= self.net_clicks_limit or is_image_changed or self.input_data is None:\n            self.input_data = self._get_head_input(image_nd, points_nd)\n\n        def get_prediction_logits(scale, bias):\n            scale = scale.view(bs, -1, 1, 1)\n            bias = bias.view(bs, -1, 1, 1)\n            if self.with_flip:\n                scale = scale.repeat(2, 1, 1, 1)\n                bias = bias.repeat(2, 1, 1, 1)\n\n            scaled_backbone_features = self.input_data * scale\n            scaled_backbone_features = scaled_backbone_features + bias\n            if self.insertion_mode == 'A':\n                out_aux = self.net.feature_extractor.aux_head(scaled_backbone_features)\n                feats = self.net.feature_extractor.conv3x3_ocr(scaled_backbone_features)\n\n                context = self.net.feature_extractor.ocr_gather_head(feats, out_aux)\n                feats = self.net.feature_extractor.ocr_distri_head(feats, context)\n                pred_logits = self.net.feature_extractor.cls_head(feats)\n            elif self.insertion_mode == 'C':\n                pred_logits = self.net.feature_extractor.cls_head(scaled_backbone_features)\n            else:\n                raise NotImplementedError\n\n            pred_logits = F.interpolate(pred_logits, size=image_nd.size()[2:], mode='bilinear',\n                                        align_corners=True)\n            return pred_logits\n\n        self.opt_functor.init_click(get_prediction_logits, pos_mask, neg_mask, self.device)\n        if num_clicks > self.optimize_after_n_clicks:\n            opt_result = fmin_l_bfgs_b(func=self.opt_functor, x0=self.opt_data,\n                                       **self.opt_functor.optimizer_params)\n            self.opt_data = opt_result[0]\n\n        with torch.no_grad():\n            if self.opt_functor.best_prediction is not None:\n                opt_pred_logits = self.opt_functor.best_prediction\n            else:\n                opt_data_nd = torch.from_numpy(self.opt_data).to(self.device)\n                opt_vars, _ = self.opt_functor.unpack_opt_params(opt_data_nd)\n                opt_pred_logits = get_prediction_logits(*opt_vars)\n\n        return opt_pred_logits\n\n    def _get_head_input(self, image_nd, points):\n        with torch.no_grad():\n            coord_features = self.net.dist_maps(image_nd, points)\n            x = self.net.rgb_conv(torch.cat((image_nd, coord_features), dim=1))\n            feats = self.net.feature_extractor.compute_hrnet_feats(x)\n            if self.insertion_mode == 'A':\n                backbone_features = feats\n            elif self.insertion_mode == 'C':\n                out_aux = self.net.feature_extractor.aux_head(feats)\n                feats = self.net.feature_extractor.conv3x3_ocr(feats)\n\n                context = self.net.feature_extractor.ocr_gather_head(feats, out_aux)\n                backbone_features = self.net.feature_extractor.ocr_distri_head(feats, context)\n            else:\n                raise NotImplementedError\n\n        return backbone_features\n\n\nclass InputBRSPredictor(BRSBasePredictor):\n    def __init__(self, model, device, opt_functor, optimize_target='rgb', **kwargs):\n        super().__init__(model, device, opt_functor=opt_functor, **kwargs)\n        self.optimize_target = optimize_target\n\n    def _get_prediction(self, image_nd, clicks_lists, is_image_changed):\n        points_nd = self.get_points_nd(clicks_lists)\n        pos_mask, neg_mask = self._get_clicks_maps_nd(clicks_lists, image_nd.shape[2:])\n        num_clicks = len(clicks_lists[0])\n\n        if self.opt_data is None or is_image_changed:\n            opt_channels = 2 if self.optimize_target == 'dmaps' else 3\n            bs = image_nd.shape[0] // 2 if self.with_flip else image_nd.shape[0]\n            self.opt_data = torch.zeros((bs, opt_channels, image_nd.shape[2], image_nd.shape[3]),\n                                        device=self.device, dtype=torch.float32)\n\n        def get_prediction_logits(opt_bias):\n            input_image = image_nd\n            if self.optimize_target == 'rgb':\n                input_image = input_image + opt_bias\n            dmaps = self.net.dist_maps(input_image, points_nd)\n            if self.optimize_target == 'dmaps':\n                dmaps = dmaps + opt_bias\n\n            x = self.net.rgb_conv(torch.cat((input_image, dmaps), dim=1))\n            if self.optimize_target == 'all':\n                x = x + opt_bias\n\n            if isinstance(self.net, DistMapsHRNetModel):\n                pred_logits = self.net.feature_extractor(x)[0]\n            else:\n                backbone_features = self.net.feature_extractor(x)\n                pred_logits = self.net.head(backbone_features[0])\n            pred_logits = F.interpolate(pred_logits, size=image_nd.size()[2:], mode='bilinear', align_corners=True)\n\n            return pred_logits\n\n        self.opt_functor.init_click(get_prediction_logits, pos_mask, neg_mask, self.device,\n                                    shape=self.opt_data.shape)\n        if num_clicks > self.optimize_after_n_clicks:\n            opt_result = fmin_l_bfgs_b(func=self.opt_functor, x0=self.opt_data.cpu().numpy().ravel(),\n                                       **self.opt_functor.optimizer_params)\n\n            self.opt_data = torch.from_numpy(opt_result[0]).view(self.opt_data.shape).to(self.device)\n\n        with torch.no_grad():\n            if self.opt_functor.best_prediction is not None:\n                opt_pred_logits = self.opt_functor.best_prediction\n            else:\n                opt_vars, _ = self.opt_functor.unpack_opt_params(self.opt_data)\n                opt_pred_logits = get_prediction_logits(*opt_vars)\n\n        return opt_pred_logits\n"""
isegm/inference/predictors/brs_functors.py,12,"b""import torch\nimport numpy as np\n\nfrom isegm.model.metrics import _compute_iou\nfrom .brs_losses import BRSMaskLoss\n\n\nclass BaseOptimizer:\n    def __init__(self, optimizer_params,\n                 prob_thresh=0.49,\n                 reg_weight=1e-3,\n                 min_iou_diff=0.01,\n                 brs_loss=BRSMaskLoss(),\n                 with_flip=False,\n                 flip_average=False,\n                 **kwargs):\n        self.brs_loss = brs_loss\n        self.optimizer_params = optimizer_params\n        self.prob_thresh = prob_thresh\n        self.reg_weight = reg_weight\n        self.min_iou_diff = min_iou_diff\n        self.with_flip = with_flip\n        self.flip_average = flip_average\n\n        self.best_prediction = None\n        self._get_prediction_logits = None\n        self._opt_shape = None\n        self._best_loss = None\n        self._click_masks = None\n        self._last_mask = None\n        self.device = None\n\n    def init_click(self, get_prediction_logits, pos_mask, neg_mask, device, shape=None):\n        self.best_prediction = None\n        self._get_prediction_logits = get_prediction_logits\n        self._click_masks = (pos_mask, neg_mask)\n        self._opt_shape = shape\n        self._last_mask = None\n        self.device = device\n\n    def __call__(self, x):\n        opt_params = torch.from_numpy(x).float().to(self.device)\n        opt_params.requires_grad_(True)\n\n        with torch.enable_grad():\n            opt_vars, reg_loss = self.unpack_opt_params(opt_params)\n            result_before_sigmoid = self._get_prediction_logits(*opt_vars)\n            result = torch.sigmoid(result_before_sigmoid)\n\n            pos_mask, neg_mask = self._click_masks\n            if self.with_flip and self.flip_average:\n                result, result_flipped = torch.chunk(result, 2, dim=0)\n                result = 0.5 * (result + torch.flip(result_flipped, dims=[3]))\n                pos_mask, neg_mask = pos_mask[:result.shape[0]], neg_mask[:result.shape[0]]\n\n            loss, f_max_pos, f_max_neg = self.brs_loss(result, pos_mask, neg_mask)\n            loss = loss + reg_loss\n\n        f_val = loss.detach().cpu().numpy()\n        if self.best_prediction is None or f_val < self._best_loss:\n            self.best_prediction = result_before_sigmoid.detach()\n            self._best_loss = f_val\n\n        if f_max_pos < (1 - self.prob_thresh) and f_max_neg < self.prob_thresh:\n            return [f_val, np.zeros_like(x)]\n\n        current_mask = result > self.prob_thresh\n        if self._last_mask is not None and self.min_iou_diff > 0:\n            diff_iou = _compute_iou(current_mask, self._last_mask)\n            if len(diff_iou) > 0 and diff_iou.mean() > 1 - self.min_iou_diff:\n                return [f_val, np.zeros_like(x)]\n        self._last_mask = current_mask\n\n        loss.backward()\n        f_grad = opt_params.grad.cpu().numpy().ravel().astype(np.float)\n\n        return [f_val, f_grad]\n\n    def unpack_opt_params(self, opt_params):\n        raise NotImplementedError\n\n\nclass InputOptimizer(BaseOptimizer):\n    def unpack_opt_params(self, opt_params):\n        opt_params = opt_params.view(self._opt_shape)\n        if self.with_flip:\n            opt_params_flipped = torch.flip(opt_params, dims=[3])\n            opt_params = torch.cat([opt_params, opt_params_flipped], dim=0)\n        reg_loss = self.reg_weight * torch.sum(opt_params**2)\n\n        return (opt_params,), reg_loss\n\n\nclass ScaleBiasOptimizer(BaseOptimizer):\n    def __init__(self, *args, scale_act=None, reg_bias_weight=10.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.scale_act = scale_act\n        self.reg_bias_weight = reg_bias_weight\n\n    def unpack_opt_params(self, opt_params):\n        scale, bias = torch.chunk(opt_params, 2, dim=0)\n        reg_loss = self.reg_weight * (torch.sum(scale**2) + self.reg_bias_weight * torch.sum(bias**2))\n\n        if self.scale_act == 'tanh':\n            scale = torch.tanh(scale)\n        elif self.scale_act == 'sin':\n            scale = torch.sin(scale)\n\n        return (1 + scale, bias), reg_loss\n"""
isegm/inference/predictors/brs_losses.py,12,"b""import torch\n\nfrom isegm.model.losses import SigmoidBinaryCrossEntropyLoss\n\n\nclass BRSMaskLoss(torch.nn.Module):\n    def __init__(self, eps=1e-5):\n        super().__init__()\n        self._eps = eps\n\n    def forward(self, result, pos_mask, neg_mask):\n        pos_diff = (1 - result) * pos_mask\n        pos_target = torch.sum(pos_diff ** 2)\n        pos_target = pos_target / (torch.sum(pos_mask) + self._eps)\n\n        neg_diff = result * neg_mask\n        neg_target = torch.sum(neg_diff ** 2)\n        neg_target = neg_target / (torch.sum(neg_mask) + self._eps)\n        \n        loss = pos_target + neg_target\n\n        with torch.no_grad():\n            f_max_pos = torch.max(torch.abs(pos_diff)).item()\n            f_max_neg = torch.max(torch.abs(neg_diff)).item()\n\n        return loss, f_max_pos, f_max_neg\n\n\nclass OracleMaskLoss(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.gt_mask = None\n        self.loss = SigmoidBinaryCrossEntropyLoss(from_sigmoid=True)\n        self.predictor = None\n        self.history = []\n\n    def set_gt_mask(self, gt_mask):\n        self.gt_mask = gt_mask\n        self.history = []\n\n    def forward(self, result, pos_mask, neg_mask):\n        gt_mask = self.gt_mask.to(result.device)\n        if self.predictor.object_roi is not None:\n            r1, r2, c1, c2 = self.predictor.object_roi[:4]\n            gt_mask = gt_mask[:, :, r1:r2 + 1, c1:c2 + 1]\n            gt_mask = torch.nn.functional.interpolate(gt_mask, result.size()[2:],  mode='bilinear', align_corners=True)\n\n        if result.shape[0] == 2:\n            gt_mask_flipped = torch.flip(gt_mask, dims=[3])\n            gt_mask = torch.cat([gt_mask, gt_mask_flipped], dim=0)\n\n        loss = self.loss(result, gt_mask)\n        self.history.append(loss.detach().cpu().numpy()[0])\n\n        if len(self.history) > 5 and abs(self.history[-5] - self.history[-1]) < 1e-5:\n            return 0, 0, 0\n\n        return loss, 1.0, 1.0\n"""
isegm/inference/transforms/__init__.py,0,b'from .base import SigmoidForPred\nfrom .flip import AddHorizontalFlip\nfrom .zoom_in import ZoomIn\nfrom .limit_longest_side import LimitLongestSide\nfrom .crops import Crops\n'
isegm/inference/transforms/base.py,1,"b'import torch\n\n\nclass BaseTransform(object):\n    def __init__(self):\n        self.image_changed = False\n\n    def transform(self, image_nd, clicks_lists):\n        raise NotImplementedError\n\n    def inv_transform(self, prob_map):\n        raise NotImplementedError\n\n    def reset(self):\n        raise NotImplementedError\n\n    def get_state(self):\n        raise NotImplementedError\n\n    def set_state(self, state):\n        raise NotImplementedError\n\n\nclass SigmoidForPred(BaseTransform):\n    def transform(self, image_nd, clicks_lists):\n        return image_nd, clicks_lists\n\n    def inv_transform(self, prob_map):\n        return torch.sigmoid(prob_map)\n\n    def reset(self):\n        pass\n\n    def get_state(self):\n        return None\n\n    def set_state(self, state):\n        pass\n'"
isegm/inference/transforms/crops.py,4,"b'import math\n\nimport torch\nimport numpy as np\n\nfrom isegm.inference.clicker import Click\nfrom .base import BaseTransform\n\n\nclass Crops(BaseTransform):\n    def __init__(self, crop_size=(320, 480), min_overlap=0.2):\n        super().__init__()\n        self.crop_height, self.crop_width = crop_size\n        self.min_overlap = min_overlap\n\n        self.x_offsets = None\n        self.y_offsets = None\n        self._counts = None\n\n    def transform(self, image_nd, clicks_lists):\n        assert image_nd.shape[0] == 1 and len(clicks_lists) == 1\n        image_height, image_width = image_nd.shape[2:4]\n        self._counts = None\n\n        if image_height < self.crop_height or image_width < self.crop_width:\n            return image_nd, clicks_lists\n\n        self.x_offsets = get_offsets(image_width, self.crop_width, self.min_overlap)\n        self.y_offsets = get_offsets(image_height, self.crop_height, self.min_overlap)\n        self._counts = np.zeros((image_height, image_width))\n\n        image_crops = []\n        for dy in self.y_offsets:\n            for dx in self.x_offsets:\n                self._counts[dy:dy + self.crop_height, dx:dx + self.crop_width] += 1\n                image_crop = image_nd[:, :, dy:dy + self.crop_height, dx:dx + self.crop_width]\n                image_crops.append(image_crop)\n        image_crops = torch.cat(image_crops, dim=0)\n        self._counts = torch.tensor(self._counts, device=image_nd.device, dtype=torch.float32)\n\n        clicks_list = clicks_lists[0]\n        clicks_lists = []\n        for dy in self.y_offsets:\n            for dx in self.x_offsets:\n                crop_clicks = [Click(is_positive=x.is_positive, coords=(x.coords[0] - dy, x.coords[1] - dx))\n                               for x in clicks_list]\n                clicks_lists.append(crop_clicks)\n\n        return image_crops, clicks_lists\n\n    def inv_transform(self, prob_map):\n        if self._counts is None:\n            return prob_map\n\n        new_prob_map = torch.zeros((1, 1, *self._counts.shape),\n                                   dtype=prob_map.dtype, device=prob_map.device)\n\n        crop_indx = 0\n        for dy in self.y_offsets:\n            for dx in self.x_offsets:\n                new_prob_map[0, 0, dy:dy + self.crop_height, dx:dx + self.crop_width] += prob_map[crop_indx, 0]\n                crop_indx += 1\n        new_prob_map = torch.div(new_prob_map, self._counts)\n\n        return new_prob_map\n\n    def get_state(self):\n        return self.x_offsets, self.y_offsets, self._counts\n\n    def set_state(self, state):\n        self.x_offsets, self.y_offsets, self._counts = state\n\n    def reset(self):\n        self.x_offsets = None\n        self.y_offsets = None\n        self._counts = None\n\n\ndef get_offsets(length, crop_size, min_overlap_ratio=0.2):\n    if length == crop_size:\n        return [0]\n\n    N = (length / crop_size - min_overlap_ratio) / (1 - min_overlap_ratio)\n    N = math.ceil(N)\n\n    overlap_ratio = (N - length / crop_size) / (N - 1)\n    overlap_width = int(crop_size * overlap_ratio)\n\n    offsets = [0]\n    for i in range(1, N):\n        new_offset = offsets[-1] + crop_size - overlap_width\n        if new_offset + crop_size > length:\n            new_offset = length - crop_size\n\n        offsets.append(new_offset)\n\n    return offsets\n'"
isegm/inference/transforms/flip.py,2,"b'import torch\n\nfrom isegm.inference.clicker import Click\nfrom .base import BaseTransform\n\n\nclass AddHorizontalFlip(BaseTransform):\n    def transform(self, image_nd, clicks_lists):\n        assert len(image_nd.shape) == 4\n        image_nd = torch.cat([image_nd, torch.flip(image_nd, dims=[3])], dim=0)\n\n        image_width = image_nd.shape[3]\n        clicks_lists_flipped = []\n        for clicks_list in clicks_lists:\n            clicks_list_flipped = [Click(is_positive=click.is_positive,\n                                         coords=(click.coords[0], image_width - click.coords[1] - 1))\n                                   for click in clicks_list]\n            clicks_lists_flipped.append(clicks_list_flipped)\n        clicks_lists = clicks_lists + clicks_lists_flipped\n\n        return image_nd, clicks_lists\n\n    def inv_transform(self, prob_map):\n        assert len(prob_map.shape) == 4 and prob_map.shape[0] % 2 == 0\n        num_maps = prob_map.shape[0] // 2\n        prob_map, prob_map_flipped = prob_map[:num_maps], prob_map[num_maps:]\n\n        return 0.5 * (prob_map + torch.flip(prob_map_flipped, dims=[3]))\n\n    def get_state(self):\n        return None\n\n    def set_state(self, state):\n        pass\n\n    def reset(self):\n        pass\n'"
isegm/inference/transforms/limit_longest_side.py,0,"b'from .zoom_in import ZoomIn, get_roi_image_nd\n\n\nclass LimitLongestSide(ZoomIn):\n    def __init__(self, max_size=800):\n        super().__init__(target_size=max_size, skip_clicks=0)\n\n    def transform(self, image_nd, clicks_lists):\n        assert image_nd.shape[0] == 1 and len(clicks_lists) == 1\n        image_max_size = max(image_nd.shape[2:4])\n        self.image_changed = False\n\n        if image_max_size <= self.target_size:\n            return image_nd, clicks_lists\n        self._input_image = image_nd\n\n        self._object_roi = (0, image_nd.shape[2] - 1, 0, image_nd.shape[3] - 1)\n        self._roi_image = get_roi_image_nd(image_nd, self._object_roi, self.target_size)\n        self.image_changed = True\n\n        tclicks_lists = [self._transform_clicks(clicks_lists[0])]\n        return self._roi_image, tclicks_lists\n'"
isegm/inference/transforms/zoom_in.py,4,"b""import torch\n\nfrom isegm.inference.clicker import Click\nfrom isegm.utils.misc import get_bbox_iou, get_bbox_from_mask, expand_bbox, clamp_bbox\nfrom .base import BaseTransform\n\n\nclass ZoomIn(BaseTransform):\n    def __init__(self,\n                 target_size=400,\n                 skip_clicks=1,\n                 expansion_ratio=1.4,\n                 min_crop_size=200,\n                 recompute_thresh_iou=0.5,\n                 prob_thresh=0.50):\n        super().__init__()\n        self.target_size = target_size\n        self.min_crop_size = min_crop_size\n        self.skip_clicks = skip_clicks\n        self.expansion_ratio = expansion_ratio\n        self.recompute_thresh_iou = recompute_thresh_iou\n        self.prob_thresh = prob_thresh\n\n        self._input_image_shape = None\n        self._prev_probs = None\n        self._object_roi = None\n        self._roi_image = None\n\n    def transform(self, image_nd, clicks_lists):\n        assert image_nd.shape[0] == 1 and len(clicks_lists) == 1\n        self.image_changed = False\n\n        clicks_list = clicks_lists[0]\n        if len(clicks_list) <= self.skip_clicks:\n            return image_nd, clicks_lists\n\n        self._input_image_shape = image_nd.shape\n\n        current_object_roi = None\n        if self._prev_probs is not None:\n            current_pred_mask = (self._prev_probs > self.prob_thresh)[0, 0]\n            if current_pred_mask.sum() > 0:\n                current_object_roi = get_object_roi(current_pred_mask, clicks_list,\n                                                    self.expansion_ratio, self.min_crop_size)\n\n        if current_object_roi is None:\n            return image_nd, clicks_lists\n\n        update_object_roi = False\n        if self._object_roi is None:\n            update_object_roi = True\n        elif not check_object_roi(self._object_roi, clicks_list):\n            update_object_roi = True\n        elif get_bbox_iou(current_object_roi, self._object_roi) < self.recompute_thresh_iou:\n            update_object_roi = True\n\n        if update_object_roi:\n            self._object_roi = current_object_roi\n            self._roi_image = get_roi_image_nd(image_nd, self._object_roi, self.target_size)\n            self.image_changed = True\n\n        tclicks_lists = [self._transform_clicks(clicks_list)]\n        return self._roi_image.to(image_nd.device), tclicks_lists\n\n    def inv_transform(self, prob_map):\n        if self._object_roi is None:\n            self._prev_probs = prob_map.cpu().numpy()\n            return prob_map\n\n        assert prob_map.shape[0] == 1\n        rmin, rmax, cmin, cmax = self._object_roi\n        prob_map = torch.nn.functional.interpolate(prob_map, size=(rmax - rmin + 1, cmax - cmin + 1),\n                                                   mode='bilinear', align_corners=True)\n\n        if self._prev_probs is not None:\n            new_prob_map = torch.zeros(*self._prev_probs.shape, device=prob_map.device, dtype=prob_map.dtype)\n            new_prob_map[:, :, rmin:rmax + 1, cmin:cmax + 1] = prob_map\n        else:\n            new_prob_map = prob_map\n\n        self._prev_probs = new_prob_map.cpu().numpy()\n\n        return new_prob_map\n\n    def check_possible_recalculation(self):\n        if self._prev_probs is None or self._object_roi is not None or self.skip_clicks > 0:\n            return False\n\n        pred_mask = (self._prev_probs > self.prob_thresh)[0, 0]\n        if pred_mask.sum() > 0:\n            possible_object_roi = get_object_roi(pred_mask, [],\n                                                 self.expansion_ratio, self.min_crop_size)\n            image_roi = (0, self._input_image_shape[2] - 1, 0, self._input_image_shape[3] - 1)\n            if get_bbox_iou(possible_object_roi, image_roi) < 0.50:\n                return True\n        return False\n\n    def get_state(self):\n        roi_image = self._roi_image.cpu() if self._roi_image is not None else None\n        return self._input_image_shape, self._object_roi, self._prev_probs, roi_image, self.image_changed\n\n    def set_state(self, state):\n        self._input_image_shape, self._object_roi, self._prev_probs, self._roi_image, self.image_changed = state\n\n    def reset(self):\n        self._input_image_shape = None\n        self._object_roi = None\n        self._prev_probs = None\n        self._roi_image = None\n        self.image_changed = False\n\n    def _transform_clicks(self, clicks_list):\n        if self._object_roi is None:\n            return clicks_list\n\n        rmin, rmax, cmin, cmax = self._object_roi\n        crop_height, crop_width = self._roi_image.shape[2:]\n\n        transformed_clicks = []\n        for click in clicks_list:\n            new_r = crop_height * (click.coords[0] - rmin) / (rmax - rmin + 1)\n            new_c = crop_width * (click.coords[1] - cmin) / (cmax - cmin + 1)\n            transformed_clicks.append(Click(is_positive=click.is_positive, coords=(new_r, new_c)))\n        return transformed_clicks\n\n\ndef get_object_roi(pred_mask, clicks_list, expansion_ratio, min_crop_size):\n    pred_mask = pred_mask.copy()\n\n    for click in clicks_list:\n        if click.is_positive:\n            pred_mask[int(click.coords[0]), int(click.coords[1])] = 1\n\n    bbox = get_bbox_from_mask(pred_mask)\n    bbox = expand_bbox(bbox, expansion_ratio, min_crop_size)\n    h, w = pred_mask.shape[0], pred_mask.shape[1]\n    bbox = clamp_bbox(bbox, 0, h - 1, 0, w - 1)\n\n    return bbox\n\n\ndef get_roi_image_nd(image_nd, object_roi, target_size):\n    rmin, rmax, cmin, cmax = object_roi\n\n    height = rmax - rmin + 1\n    width = cmax - cmin + 1\n\n    if isinstance(target_size, tuple):\n        new_height, new_width = target_size\n    else:\n        scale = target_size / max(height, width)\n        new_height = int(round(height * scale))\n        new_width = int(round(width * scale))\n\n    with torch.no_grad():\n        roi_image_nd = image_nd[:, :, rmin:rmax + 1, cmin:cmax + 1]\n        roi_image_nd = torch.nn.functional.interpolate(roi_image_nd, size=(new_height, new_width),\n                                                       mode='bilinear', align_corners=True)\n\n    return roi_image_nd\n\n\ndef check_object_roi(object_roi, clicks_list):\n    for click in clicks_list:\n        if click.is_positive:\n            if click.coords[0] < object_roi[0] or click.coords[0] >= object_roi[1]:\n                return False\n            if click.coords[1] < object_roi[2] or click.coords[1] >= object_roi[3]:\n                return False\n\n    return True\n"""
isegm/model/modeling/basic_blocks.py,1,"b""import torch.nn as nn\n\nfrom isegm.model import ops\n\n\nclass ConvHead(nn.Module):\n    def __init__(self, out_channels, in_channels=32, num_layers=1,\n                 kernel_size=3, padding=1,\n                 norm_layer=nn.BatchNorm2d):\n        super(ConvHead, self).__init__()\n        convhead = []\n\n        for i in range(num_layers):\n            convhead.extend([\n                nn.Conv2d(in_channels, in_channels, kernel_size, padding=padding),\n                nn.ReLU(),\n                norm_layer(in_channels) if norm_layer is not None else nn.Identity()\n            ])\n        convhead.append(nn.Conv2d(in_channels, out_channels, 1, padding=0))\n\n        self.convhead = nn.Sequential(*convhead)\n\n    def forward(self, *inputs):\n        return self.convhead(inputs[0])\n\n\nclass SepConvHead(nn.Module):\n    def __init__(self, num_outputs, in_channels, mid_channels, num_layers=1,\n                 kernel_size=3, padding=1, dropout_ratio=0.0, dropout_indx=0,\n                 norm_layer=nn.BatchNorm2d):\n        super(SepConvHead, self).__init__()\n\n        sepconvhead = []\n\n        for i in range(num_layers):\n            sepconvhead.append(\n                SeparableConv2d(in_channels=in_channels if i == 0 else mid_channels,\n                                out_channels=mid_channels,\n                                dw_kernel=kernel_size, dw_padding=padding,\n                                norm_layer=norm_layer, activation='relu')\n            )\n            if dropout_ratio > 0 and dropout_indx == i:\n                sepconvhead.append(nn.Dropout(dropout_ratio))\n\n        sepconvhead.append(\n            nn.Conv2d(in_channels=mid_channels, out_channels=num_outputs, kernel_size=1, padding=0)\n        )\n\n        self.layers = nn.Sequential(*sepconvhead)\n\n    def forward(self, *inputs):\n        x = inputs[0]\n\n        return self.layers(x)\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, dw_kernel, dw_padding, dw_stride=1,\n                 activation=None, use_bias=False, norm_layer=None):\n        super(SeparableConv2d, self).__init__()\n        _activation = ops.select_activation_function(activation)\n        self.body = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels, kernel_size=dw_kernel, stride=dw_stride,\n                      padding=dw_padding, bias=use_bias, groups=in_channels),\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=use_bias),\n            norm_layer(out_channels) if norm_layer is not None else nn.Identity(),\n            _activation()\n        )\n\n    def forward(self, x):\n        return self.body(x)\n"""
isegm/model/modeling/deeplab_v3.py,4,"b'from contextlib import ExitStack\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nfrom .basic_blocks import SeparableConv2d\nfrom .resnet import ResNetBackbone\nfrom isegm.model import ops\n\n\nclass DeepLabV3Plus(nn.Module):\n    def __init__(self, backbone=\'resnet50\', norm_layer=nn.BatchNorm2d,\n                 backbone_norm_layer=None,\n                 ch=256,\n                 project_dropout=0.5,\n                 inference_mode=False,\n                 **kwargs):\n        super(DeepLabV3Plus, self).__init__()\n        if backbone_norm_layer is None:\n            backbone_norm_layer = norm_layer\n\n        self.backbone_name = backbone\n        self.norm_layer = norm_layer\n        self.backbone_norm_layer = backbone_norm_layer\n        self.inference_mode = False\n        self.ch = ch\n        self.aspp_in_channels = 2048\n        self.skip_project_in_channels = 256  # layer 1 out_channels\n\n        self._kwargs = kwargs\n        if backbone == \'resnet34\':\n            self.aspp_in_channels = 512\n            self.skip_project_in_channels = 64\n\n        self.backbone = ResNetBackbone(backbone=self.backbone_name, pretrained_base=False,\n                                       norm_layer=self.backbone_norm_layer, **kwargs)\n\n        self.head = _DeepLabHead(in_channels=ch + 32, mid_channels=ch, out_channels=ch,\n                                 norm_layer=self.norm_layer)\n        self.skip_project = _SkipProject(self.skip_project_in_channels, 32, norm_layer=self.norm_layer)\n        self.aspp = _ASPP(in_channels=self.aspp_in_channels,\n                          atrous_rates=[12, 24, 36],\n                          out_channels=ch,\n                          project_dropout=project_dropout,\n                          norm_layer=self.norm_layer)\n\n        if inference_mode:\n            self.set_prediction_mode()\n\n    def load_pretrained_weights(self):\n        pretrained = ResNetBackbone(backbone=self.backbone_name, pretrained_base=True,\n                                    norm_layer=self.backbone_norm_layer, **self._kwargs)\n        backbone_state_dict = self.backbone.state_dict()\n        pretrained_state_dict = pretrained.state_dict()\n\n        backbone_state_dict.update(pretrained_state_dict)\n        self.backbone.load_state_dict(backbone_state_dict)\n\n        if self.inference_mode:\n            for param in self.backbone.parameters():\n                param.requires_grad = False\n\n    def set_prediction_mode(self):\n        self.inference_mode = True\n        self.eval()\n\n    def forward(self, x):\n        with ExitStack() as stack:\n            if self.inference_mode:\n                stack.enter_context(torch.no_grad())\n\n            c1, _, c3, c4 = self.backbone(x)\n            c1 = self.skip_project(c1)\n\n            x = self.aspp(c4)\n            x = F.interpolate(x, c1.size()[2:], mode=\'bilinear\', align_corners=True)\n            x = torch.cat((x, c1), dim=1)\n            x = self.head(x)\n\n        return x,\n\n\nclass _SkipProject(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer=nn.BatchNorm2d):\n        super(_SkipProject, self).__init__()\n        _activation = ops.select_activation_function(""relu"")\n\n        self.skip_project = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n            norm_layer(out_channels),\n            _activation()\n        )\n\n    def forward(self, x):\n        return self.skip_project(x)\n\n\nclass _DeepLabHead(nn.Module):\n    def __init__(self, out_channels, in_channels, mid_channels=256, norm_layer=nn.BatchNorm2d):\n        super(_DeepLabHead, self).__init__()\n\n        self.block = nn.Sequential(\n            SeparableConv2d(in_channels=in_channels, out_channels=mid_channels, dw_kernel=3,\n                            dw_padding=1, activation=\'relu\', norm_layer=norm_layer),\n            SeparableConv2d(in_channels=mid_channels, out_channels=mid_channels, dw_kernel=3,\n                            dw_padding=1, activation=\'relu\', norm_layer=norm_layer),\n            nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=1)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass _ASPP(nn.Module):\n    def __init__(self, in_channels, atrous_rates, out_channels=256,\n                 project_dropout=0.5, norm_layer=nn.BatchNorm2d):\n        super(_ASPP, self).__init__()\n\n        b0 = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=False),\n            norm_layer(out_channels),\n            nn.ReLU()\n        )\n\n        rate1, rate2, rate3 = tuple(atrous_rates)\n        b1 = _ASPPConv(in_channels, out_channels, rate1, norm_layer)\n        b2 = _ASPPConv(in_channels, out_channels, rate2, norm_layer)\n        b3 = _ASPPConv(in_channels, out_channels, rate3, norm_layer)\n        b4 = _AsppPooling(in_channels, out_channels, norm_layer=norm_layer)\n\n        self.concurent = nn.ModuleList([b0, b1, b2, b3, b4])\n\n        project = [\n            nn.Conv2d(in_channels=5*out_channels, out_channels=out_channels,\n                      kernel_size=1, bias=False),\n            norm_layer(out_channels),\n            nn.ReLU()\n        ]\n        if project_dropout > 0:\n            project.append(nn.Dropout(project_dropout))\n        self.project = nn.Sequential(*project)\n\n    def forward(self, x):\n        x = torch.cat([block(x) for block in self.concurent], dim=1)\n\n        return self.project(x)\n\n\nclass _AsppPooling(nn.Module):\n    def __init__(self, in_channels, out_channels, norm_layer):\n        super(_AsppPooling, self).__init__()\n\n        self.gap = nn.Sequential(\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n                      kernel_size=1, bias=False),\n            norm_layer(out_channels),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        pool = self.gap(x)\n        return F.interpolate(pool, x.size()[2:], mode=\'bilinear\', align_corners=True)\n\n\ndef _ASPPConv(in_channels, out_channels, atrous_rate, norm_layer):\n    block = nn.Sequential(\n        nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n                  kernel_size=3, padding=atrous_rate,\n                  dilation=atrous_rate, bias=False),\n        norm_layer(out_channels),\n        nn.ReLU()\n    )\n\n    return block\n'"
isegm/model/modeling/hrnet_ocr.py,5,"b'import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch._utils\nimport torch.nn.functional as F\nfrom .ocr import SpatialOCR_Module, SpatialGather_Module\nfrom .resnetv1b import BasicBlockV1b, BottleneckV1b\n\nrelu_inplace = True\n\n\nclass HighResolutionModule(nn.Module):\n    def __init__(self, num_branches, blocks, num_blocks, num_inchannels,\n                 num_channels, fuse_method,multi_scale_output=True,\n                 norm_layer=nn.BatchNorm2d, align_corners=True):\n        super(HighResolutionModule, self).__init__()\n        self._check_branches(num_branches, num_blocks, num_inchannels, num_channels)\n\n        self.num_inchannels = num_inchannels\n        self.fuse_method = fuse_method\n        self.num_branches = num_branches\n        self.norm_layer = norm_layer\n        self.align_corners = align_corners\n\n        self.multi_scale_output = multi_scale_output\n\n        self.branches = self._make_branches(\n            num_branches, blocks, num_blocks, num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(inplace=relu_inplace)\n\n    def _check_branches(self, num_branches, num_blocks, num_inchannels, num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_BLOCKS({})\'.format(\n                num_branches, len(num_blocks))\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_CHANNELS({})\'.format(\n                num_branches, len(num_channels))\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_inchannels):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_INCHANNELS({})\'.format(\n                num_branches, len(num_inchannels))\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self, branch_index, block, num_blocks, num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n                self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.num_inchannels[branch_index],\n                          num_channels[branch_index] * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                self.norm_layer(num_channels[branch_index] * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.num_inchannels[branch_index],\n                            num_channels[branch_index], stride,\n                            downsample=downsample, norm_layer=self.norm_layer))\n        self.num_inchannels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(block(self.num_inchannels[branch_index],\n                                num_channels[branch_index],\n                                norm_layer=self.norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        num_inchannels = self.num_inchannels\n        fuse_layers = []\n        for i in range(num_branches if self.multi_scale_output else 1):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(nn.Sequential(\n                        nn.Conv2d(in_channels=num_inchannels[j],\n                                  out_channels=num_inchannels[i],\n                                  kernel_size=1,\n                                  bias=False),\n                        self.norm_layer(num_inchannels[i])))\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv3x3s = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            num_outchannels_conv3x3 = num_inchannels[i]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          kernel_size=3, stride=2, padding=1, bias=False),\n                                self.norm_layer(num_outchannels_conv3x3)))\n                        else:\n                            num_outchannels_conv3x3 = num_inchannels[j]\n                            conv3x3s.append(nn.Sequential(\n                                nn.Conv2d(num_inchannels[j],\n                                          num_outchannels_conv3x3,\n                                          kernel_size=3, stride=2, padding=1, bias=False),\n                                self.norm_layer(num_outchannels_conv3x3),\n                                nn.ReLU(inplace=relu_inplace)))\n                    fuse_layer.append(nn.Sequential(*conv3x3s))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def get_num_inchannels(self):\n        return self.num_inchannels\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n        for i in range(len(self.fuse_layers)):\n            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n            for j in range(1, self.num_branches):\n                if i == j:\n                    y = y + x[j]\n                elif j > i:\n                    width_output = x[i].shape[-1]\n                    height_output = x[i].shape[-2]\n                    y = y + F.interpolate(\n                        self.fuse_layers[i][j](x[j]),\n                        size=[height_output, width_output],\n                        mode=\'bilinear\', align_corners=self.align_corners)\n                else:\n                    y = y + self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n\n        return x_fuse\n\n\nclass HighResolutionNet(nn.Module):\n    def __init__(self, width, num_classes, ocr_width=256, small=False,\n                 norm_layer=nn.BatchNorm2d, align_corners=True):\n        super(HighResolutionNet, self).__init__()\n        self.norm_layer = norm_layer\n        self.width = width\n        self.ocr_width = ocr_width\n        self.align_corners = align_corners\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn1 = norm_layer(64)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)\n        self.bn2 = norm_layer(64)\n        self.relu = nn.ReLU(inplace=relu_inplace)\n\n        num_blocks = 2 if small else 4\n\n        stage1_num_channels = 64\n        self.layer1 = self._make_layer(BottleneckV1b, 64, stage1_num_channels, blocks=num_blocks)\n        stage1_out_channel = BottleneckV1b.expansion * stage1_num_channels\n\n        self.stage2_num_branches = 2\n        num_channels = [width, 2 * width]\n        num_inchannels = [\n            num_channels[i] * BasicBlockV1b.expansion for i in range(len(num_channels))]\n        self.transition1 = self._make_transition_layer(\n            [stage1_out_channel], num_inchannels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            BasicBlockV1b, num_inchannels=num_inchannels, num_modules=1, num_branches=self.stage2_num_branches,\n            num_blocks=2 * [num_blocks], num_channels=num_channels)\n\n        self.stage3_num_branches = 3\n        num_channels = [width, 2 * width, 4 * width]\n        num_inchannels = [\n            num_channels[i] * BasicBlockV1b.expansion for i in range(len(num_channels))]\n        self.transition2 = self._make_transition_layer(\n            pre_stage_channels, num_inchannels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            BasicBlockV1b, num_inchannels=num_inchannels,\n            num_modules=3 if small else 4, num_branches=self.stage3_num_branches,\n            num_blocks=3 * [num_blocks], num_channels=num_channels)\n\n        self.stage4_num_branches = 4\n        num_channels = [width, 2 * width, 4 * width, 8 * width]\n        num_inchannels = [\n            num_channels[i] * BasicBlockV1b.expansion for i in range(len(num_channels))]\n        self.transition3 = self._make_transition_layer(\n            pre_stage_channels, num_inchannels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            BasicBlockV1b, num_inchannels=num_inchannels, num_modules=2 if small else 3,\n            num_branches=self.stage4_num_branches,\n            num_blocks=4 * [num_blocks], num_channels=num_channels)\n\n        last_inp_channels = np.int(np.sum(pre_stage_channels))\n        ocr_mid_channels = 2 * ocr_width\n        ocr_key_channels = ocr_width\n\n        self.conv3x3_ocr = nn.Sequential(\n            nn.Conv2d(last_inp_channels, ocr_mid_channels,\n                      kernel_size=3, stride=1, padding=1),\n            norm_layer(ocr_mid_channels),\n            nn.ReLU(inplace=relu_inplace),\n        )\n        self.ocr_gather_head = SpatialGather_Module(num_classes)\n\n        self.ocr_distri_head = SpatialOCR_Module(in_channels=ocr_mid_channels,\n                                                 key_channels=ocr_key_channels,\n                                                 out_channels=ocr_mid_channels,\n                                                 scale=1,\n                                                 dropout=0.05,\n                                                 norm_layer=norm_layer,\n                                                 align_corners=align_corners)\n        self.cls_head = nn.Conv2d(\n            ocr_mid_channels, num_classes, kernel_size=1, stride=1, padding=0, bias=True)\n\n        self.aux_head = nn.Sequential(\n            nn.Conv2d(last_inp_channels, last_inp_channels,\n                      kernel_size=1, stride=1, padding=0),\n            norm_layer(last_inp_channels),\n            nn.ReLU(inplace=relu_inplace),\n            nn.Conv2d(last_inp_channels, num_classes,\n                      kernel_size=1, stride=1, padding=0, bias=True)\n        )\n\n    def _make_transition_layer(\n            self, num_channels_pre_layer, num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(nn.Sequential(\n                        nn.Conv2d(num_channels_pre_layer[i],\n                                  num_channels_cur_layer[i],\n                                  kernel_size=3,\n                                  stride=1,\n                                  padding=1,\n                                  bias=False),\n                        self.norm_layer(num_channels_cur_layer[i]),\n                        nn.ReLU(inplace=relu_inplace)))\n                else:\n                    transition_layers.append(None)\n            else:\n                conv3x3s = []\n                for j in range(i + 1 - num_branches_pre):\n                    inchannels = num_channels_pre_layer[-1]\n                    outchannels = num_channels_cur_layer[i] \\\n                        if j == i - num_branches_pre else inchannels\n                    conv3x3s.append(nn.Sequential(\n                        nn.Conv2d(inchannels, outchannels,\n                                  kernel_size=3, stride=2, padding=1, bias=False),\n                        self.norm_layer(outchannels),\n                        nn.ReLU(inplace=relu_inplace)))\n                transition_layers.append(nn.Sequential(*conv3x3s))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                self.norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(inplanes, planes, stride,\n                            downsample=downsample, norm_layer=self.norm_layer))\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(inplanes, planes, norm_layer=self.norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, block, num_inchannels,\n                    num_modules, num_branches, num_blocks, num_channels,\n                    fuse_method=\'SUM\',\n                    multi_scale_output=True):\n        modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used last module\n            if not multi_scale_output and i == num_modules - 1:\n                reset_multi_scale_output = False\n            else:\n                reset_multi_scale_output = True\n            modules.append(\n                HighResolutionModule(num_branches,\n                                     block,\n                                     num_blocks,\n                                     num_inchannels,\n                                     num_channels,\n                                     fuse_method,\n                                     reset_multi_scale_output,\n                                     norm_layer=self.norm_layer,\n                                     align_corners=self.align_corners)\n            )\n            num_inchannels = modules[-1].get_num_inchannels()\n\n        return nn.Sequential(*modules), num_inchannels\n\n    def forward(self, x):\n        feats = self.compute_hrnet_feats(x)\n        out_aux = self.aux_head(feats)\n        feats = self.conv3x3_ocr(feats)\n\n        context = self.ocr_gather_head(feats, out_aux)\n        feats = self.ocr_distri_head(feats, context)\n        out = self.cls_head(feats)\n\n        return [out, out_aux]\n\n    def compute_hrnet_feats(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_num_branches):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_num_branches):\n            if self.transition2[i] is not None:\n                if i < self.stage2_num_branches:\n                    x_list.append(self.transition2[i](y_list[i]))\n                else:\n                    x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_num_branches):\n            if self.transition3[i] is not None:\n                if i < self.stage3_num_branches:\n                    x_list.append(self.transition3[i](y_list[i]))\n                else:\n                    x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        x = self.stage4(x_list)\n\n        # Upsampling\n        x0_h, x0_w = x[0].size(2), x[0].size(3)\n        x1 = F.interpolate(x[1], size=(x0_h, x0_w),\n                           mode=\'bilinear\', align_corners=self.align_corners)\n        x2 = F.interpolate(x[2], size=(x0_h, x0_w),\n                           mode=\'bilinear\', align_corners=self.align_corners)\n        x3 = F.interpolate(x[3], size=(x0_h, x0_w),\n                           mode=\'bilinear\', align_corners=self.align_corners)\n\n        return torch.cat([x[0], x1, x2, x3], 1)\n\n    def load_pretrained_weights(self, pretrained_path=\'\'):\n        model_dict = self.state_dict()\n\n        if not os.path.exists(pretrained_path):\n            print(f\'\\nFile ""{pretrained_path}"" does not exist.\')\n            print(\'You need to specify the correct path to the pre-trained weights.\\n\'\n                  \'You can download the weights for HRNet from the repository:\\n\'\n                  \'https://github.com/HRNet/HRNet-Image-Classification\')\n            exit(1)\n        pretrained_dict = torch.load(pretrained_path, map_location={\'cuda:0\': \'cpu\'})\n        pretrained_dict = {k.replace(\'last_layer\', \'aux_head\').replace(\'model.\', \'\'): v for k, v in\n                           pretrained_dict.items()}\n\n        print(\'model_dict-pretrained_dict:\', sorted(list(set(model_dict) - set(pretrained_dict))))\n        print(\'pretrained_dict-model_dict:\', sorted(list(set(pretrained_dict) - set(model_dict))))\n\n        pretrained_dict = {k: v for k, v in pretrained_dict.items()\n                           if k in model_dict.keys()}\n\n        model_dict.update(pretrained_dict)\n        self.load_state_dict(model_dict)\n'"
isegm/model/modeling/ocr.py,7,"b'import torch\nimport torch.nn as nn\nimport torch._utils\nimport torch.nn.functional as F\n\n\nclass SpatialGather_Module(nn.Module):\n    """"""\n        Aggregate the context features according to the initial\n        predicted probability distribution.\n        Employ the soft-weighted method to aggregate the context.\n    """"""\n\n    def __init__(self, cls_num=0, scale=1):\n        super(SpatialGather_Module, self).__init__()\n        self.cls_num = cls_num\n        self.scale = scale\n\n    def forward(self, feats, probs):\n        batch_size, c, h, w = probs.size(0), probs.size(1), probs.size(2), probs.size(3)\n        probs = probs.view(batch_size, c, -1)\n        feats = feats.view(batch_size, feats.size(1), -1)\n        feats = feats.permute(0, 2, 1)  # batch x hw x c\n        probs = F.softmax(self.scale * probs, dim=2)  # batch x k x hw\n        ocr_context = torch.matmul(probs, feats) \\\n            .permute(0, 2, 1).unsqueeze(3)  # batch x k x c\n        return ocr_context\n\n\nclass SpatialOCR_Module(nn.Module):\n    """"""\n    Implementation of the OCR module:\n    We aggregate the global object representation to update the representation for each pixel.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 key_channels,\n                 out_channels,\n                 scale=1,\n                 dropout=0.1,\n                 norm_layer=nn.BatchNorm2d,\n                 align_corners=True):\n        super(SpatialOCR_Module, self).__init__()\n        self.object_context_block = ObjectAttentionBlock2D(in_channels, key_channels, scale,\n                                                           norm_layer, align_corners)\n        _in_channels = 2 * in_channels\n\n        self.conv_bn_dropout = nn.Sequential(\n            nn.Conv2d(_in_channels, out_channels, kernel_size=1, padding=0, bias=False),\n            nn.Sequential(norm_layer(out_channels), nn.ReLU(inplace=True)),\n            nn.Dropout2d(dropout)\n        )\n\n    def forward(self, feats, proxy_feats):\n        context = self.object_context_block(feats, proxy_feats)\n\n        output = self.conv_bn_dropout(torch.cat([context, feats], 1))\n\n        return output\n\n\nclass ObjectAttentionBlock2D(nn.Module):\n    \'\'\'\n    The basic implementation for object context block\n    Input:\n        N X C X H X W\n    Parameters:\n        in_channels       : the dimension of the input feature map\n        key_channels      : the dimension after the key/query transform\n        scale             : choose the scale to downsample the input feature maps (save memory cost)\n        bn_type           : specify the bn type\n    Return:\n        N X C X H X W\n    \'\'\'\n\n    def __init__(self,\n                 in_channels,\n                 key_channels,\n                 scale=1,\n                 norm_layer=nn.BatchNorm2d,\n                 align_corners=True):\n        super(ObjectAttentionBlock2D, self).__init__()\n        self.scale = scale\n        self.in_channels = in_channels\n        self.key_channels = key_channels\n        self.align_corners = align_corners\n\n        self.pool = nn.MaxPool2d(kernel_size=(scale, scale))\n        self.f_pixel = nn.Sequential(\n            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n                      kernel_size=1, stride=1, padding=0, bias=False),\n            nn.Sequential(norm_layer(self.key_channels), nn.ReLU(inplace=True)),\n            nn.Conv2d(in_channels=self.key_channels, out_channels=self.key_channels,\n                      kernel_size=1, stride=1, padding=0, bias=False),\n            nn.Sequential(norm_layer(self.key_channels), nn.ReLU(inplace=True))\n        )\n        self.f_object = nn.Sequential(\n            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n                      kernel_size=1, stride=1, padding=0, bias=False),\n            nn.Sequential(norm_layer(self.key_channels), nn.ReLU(inplace=True)),\n            nn.Conv2d(in_channels=self.key_channels, out_channels=self.key_channels,\n                      kernel_size=1, stride=1, padding=0, bias=False),\n            nn.Sequential(norm_layer(self.key_channels), nn.ReLU(inplace=True))\n        )\n        self.f_down = nn.Sequential(\n            nn.Conv2d(in_channels=self.in_channels, out_channels=self.key_channels,\n                      kernel_size=1, stride=1, padding=0, bias=False),\n            nn.Sequential(norm_layer(self.key_channels), nn.ReLU(inplace=True))\n        )\n        self.f_up = nn.Sequential(\n            nn.Conv2d(in_channels=self.key_channels, out_channels=self.in_channels,\n                      kernel_size=1, stride=1, padding=0, bias=False),\n            nn.Sequential(norm_layer(self.in_channels), nn.ReLU(inplace=True))\n        )\n\n    def forward(self, x, proxy):\n        batch_size, h, w = x.size(0), x.size(2), x.size(3)\n        if self.scale > 1:\n            x = self.pool(x)\n\n        query = self.f_pixel(x).view(batch_size, self.key_channels, -1)\n        query = query.permute(0, 2, 1)\n        key = self.f_object(proxy).view(batch_size, self.key_channels, -1)\n        value = self.f_down(proxy).view(batch_size, self.key_channels, -1)\n        value = value.permute(0, 2, 1)\n\n        sim_map = torch.matmul(query, key)\n        sim_map = (self.key_channels ** -.5) * sim_map\n        sim_map = F.softmax(sim_map, dim=-1)\n\n        # add bg context ...\n        context = torch.matmul(sim_map, value)\n        context = context.permute(0, 2, 1).contiguous()\n        context = context.view(batch_size, self.key_channels, *x.size()[2:])\n        context = self.f_up(context)\n        if self.scale > 1:\n            context = F.interpolate(input=context, size=(h, w),\n                                    mode=\'bilinear\', align_corners=self.align_corners)\n\n        return context\n'"
isegm/model/modeling/resnet.py,1,"b""import torch\nfrom .resnetv1b import resnet34_v1b, resnet50_v1s, resnet101_v1s, resnet152_v1s\n\n\nclass ResNetBackbone(torch.nn.Module):\n    def __init__(self, backbone='resnet50', pretrained_base=True, dilated=True, **kwargs):\n        super(ResNetBackbone, self).__init__()\n\n        if backbone == 'resnet34':\n            pretrained = resnet34_v1b(pretrained=pretrained_base, dilated=dilated, **kwargs)\n        elif backbone == 'resnet50':\n            pretrained = resnet50_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n        elif backbone == 'resnet101':\n            pretrained = resnet101_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n        elif backbone == 'resnet152':\n            pretrained = resnet152_v1s(pretrained=pretrained_base, dilated=dilated, **kwargs)\n        else:\n            raise RuntimeError(f'unknown backbone: {backbone}')\n\n        self.conv1 = pretrained.conv1\n        self.bn1 = pretrained.bn1\n        self.relu = pretrained.relu\n        self.maxpool = pretrained.maxpool\n        self.layer1 = pretrained.layer1\n        self.layer2 = pretrained.layer2\n        self.layer3 = pretrained.layer3\n        self.layer4 = pretrained.layer4\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        c1 = self.layer1(x)\n        c2 = self.layer2(c1)\n        c3 = self.layer3(c2)\n        c4 = self.layer4(c3)\n\n        return c1, c2, c3, c4\n"""
isegm/model/modeling/resnetv1b.py,5,"b'import torch\nimport torch.nn as nn\nGLUON_RESNET_TORCH_HUB = \'rwightman/pytorch-pretrained-gluonresnet\'\n\n\nclass BasicBlockV1b(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None,\n                 previous_dilation=1, norm_layer=nn.BatchNorm2d):\n        super(BasicBlockV1b, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n                               padding=dilation, dilation=dilation, bias=False)\n        self.bn1 = norm_layer(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n                               padding=previous_dilation, dilation=previous_dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = out + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass BottleneckV1b(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None,\n                 previous_dilation=1, norm_layer=nn.BatchNorm2d):\n        super(BottleneckV1b, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = norm_layer(planes)\n\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=dilation, dilation=dilation, bias=False)\n        self.bn2 = norm_layer(planes)\n\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = norm_layer(planes * self.expansion)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = out + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNetV1b(nn.Module):\n    """""" Pre-trained ResNetV1b Model, which produces the strides of 8 featuremaps at conv5.\n\n    Parameters\n    ----------\n    block : Block\n        Class for the residual block. Options are BasicBlockV1, BottleneckV1.\n    layers : list of int\n        Numbers of layers in each block\n    classes : int, default 1000\n        Number of classification classes.\n    dilated : bool, default False\n        Applying dilation strategy to pretrained ResNet yielding a stride-8 model,\n        typically used in Semantic Segmentation.\n    norm_layer : object\n        Normalization layer used (default: :class:`nn.BatchNorm2d`)\n    deep_stem : bool, default False\n        Whether to replace the 7x7 conv1 with 3 3x3 convolution layers.\n    avg_down : bool, default False\n        Whether to use average pooling for projection skip connection between stages/downsample.\n    final_drop : float, default 0.0\n        Dropout ratio before the final classification layer.\n\n    Reference:\n        - He, Kaiming, et al. ""Deep residual learning for image recognition.""\n        Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.\n\n        - Yu, Fisher, and Vladlen Koltun. ""Multi-scale context aggregation by dilated convolutions.""\n    """"""\n    def __init__(self, block, layers, classes=1000, dilated=True, deep_stem=False, stem_width=32,\n                 avg_down=False, final_drop=0.0, norm_layer=nn.BatchNorm2d):\n        self.inplanes = stem_width*2 if deep_stem else 64\n        super(ResNetV1b, self).__init__()\n        if not deep_stem:\n            self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        else:\n            self.conv1 = nn.Sequential(\n                nn.Conv2d(3, stem_width, kernel_size=3, stride=2, padding=1, bias=False),\n                norm_layer(stem_width),\n                nn.ReLU(True),\n                nn.Conv2d(stem_width, stem_width, kernel_size=3, stride=1, padding=1, bias=False),\n                norm_layer(stem_width),\n                nn.ReLU(True),\n                nn.Conv2d(stem_width, 2*stem_width, kernel_size=3, stride=1, padding=1, bias=False)\n            )\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(True)\n        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], avg_down=avg_down,\n                                       norm_layer=norm_layer)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, avg_down=avg_down,\n                                       norm_layer=norm_layer)\n        if dilated:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2,\n                                           avg_down=avg_down, norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4,\n                                           avg_down=avg_down, norm_layer=norm_layer)\n        else:\n            self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                           avg_down=avg_down, norm_layer=norm_layer)\n            self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                           avg_down=avg_down, norm_layer=norm_layer)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.drop = None\n        if final_drop > 0.0:\n            self.drop = nn.Dropout(final_drop)\n        self.fc = nn.Linear(512 * block.expansion, classes)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1,\n                    avg_down=False, norm_layer=nn.BatchNorm2d):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = []\n            if avg_down:\n                if dilation == 1:\n                    downsample.append(\n                        nn.AvgPool2d(kernel_size=stride, stride=stride, ceil_mode=True, count_include_pad=False)\n                    )\n                else:\n                    downsample.append(\n                        nn.AvgPool2d(kernel_size=1, stride=1, ceil_mode=True, count_include_pad=False)\n                    )\n                downsample.extend([\n                    nn.Conv2d(self.inplanes, out_channels=planes * block.expansion,\n                              kernel_size=1, stride=1, bias=False),\n                    norm_layer(planes * block.expansion)\n                ])\n                downsample = nn.Sequential(*downsample)\n            else:\n                downsample = nn.Sequential(\n                    nn.Conv2d(self.inplanes, out_channels=planes * block.expansion,\n                              kernel_size=1, stride=stride, bias=False),\n                    norm_layer(planes * block.expansion)\n                )\n\n        layers = []\n        if dilation in (1, 2):\n            layers.append(block(self.inplanes, planes, stride, dilation=1, downsample=downsample,\n                                previous_dilation=dilation, norm_layer=norm_layer))\n        elif dilation == 4:\n            layers.append(block(self.inplanes, planes, stride, dilation=2, downsample=downsample,\n                                previous_dilation=dilation, norm_layer=norm_layer))\n        else:\n            raise RuntimeError(""=> unknown dilation size: {}"".format(dilation))\n\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, dilation=dilation,\n                                previous_dilation=dilation, norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        if self.drop is not None:\n            x = self.drop(x)\n        x = self.fc(x)\n\n        return x\n\n\ndef _safe_state_dict_filtering(orig_dict, model_dict_keys):\n    filtered_orig_dict = {}\n    for k, v in orig_dict.items():\n        if k in model_dict_keys:\n            filtered_orig_dict[k] = v\n        else:\n            print(f""[ERROR] Failed to load <{k}> in backbone"")\n    return filtered_orig_dict\n\n\ndef resnet34_v1b(pretrained=False, **kwargs):\n    model = ResNetV1b(BasicBlockV1b, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model_dict = model.state_dict()\n        filtered_orig_dict = _safe_state_dict_filtering(\n            torch.hub.load(GLUON_RESNET_TORCH_HUB, \'gluon_resnet34_v1b\', pretrained=True).state_dict(),\n            model_dict.keys()\n        )\n        model_dict.update(filtered_orig_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet50_v1s(pretrained=False, **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 4, 6, 3], deep_stem=True, stem_width=64, **kwargs)\n    if pretrained:\n        model_dict = model.state_dict()\n        filtered_orig_dict = _safe_state_dict_filtering(\n            torch.hub.load(GLUON_RESNET_TORCH_HUB, \'gluon_resnet50_v1s\', pretrained=True).state_dict(),\n            model_dict.keys()\n        )\n        model_dict.update(filtered_orig_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet101_v1s(pretrained=False, **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 4, 23, 3], deep_stem=True, stem_width=64, **kwargs)\n    if pretrained:\n        model_dict = model.state_dict()\n        filtered_orig_dict = _safe_state_dict_filtering(\n            torch.hub.load(GLUON_RESNET_TORCH_HUB, \'gluon_resnet101_v1s\', pretrained=True).state_dict(),\n            model_dict.keys()\n        )\n        model_dict.update(filtered_orig_dict)\n        model.load_state_dict(model_dict)\n    return model\n\n\ndef resnet152_v1s(pretrained=False, **kwargs):\n    model = ResNetV1b(BottleneckV1b, [3, 8, 36, 3], deep_stem=True, stem_width=64, **kwargs)\n    if pretrained:\n        model_dict = model.state_dict()\n        filtered_orig_dict = _safe_state_dict_filtering(\n            torch.hub.load(GLUON_RESNET_TORCH_HUB, \'gluon_resnet152_v1s\', pretrained=True).state_dict(),\n            model_dict.keys()\n        )\n        model_dict.update(filtered_orig_dict)\n        model.load_state_dict(model_dict)\n    return model\n'"
isegm/model/syncbn/__init__.py,0,b''
isegm/utils/cython/__init__.py,0,b'# noinspection PyUnresolvedReferences\nfrom .dist_maps import get_dist_maps'
isegm/utils/cython/dist_maps.py,0,"b'import pyximport; pyximport.install(pyximport=True, language_level=3)\n# noinspection PyUnresolvedReferences\nfrom ._get_dist_maps import get_dist_maps'"
isegm/model/syncbn/modules/__init__.py,0,b''
isegm/model/syncbn/modules/functional/__init__.py,0,b'from .syncbn import batchnorm2d_sync\n'
isegm/model/syncbn/modules/functional/_csrc.py,3,"b'""""""\n/*****************************************************************************/\n\nExtension module loader\n\ncode referenced from : https://github.com/facebookresearch/maskrcnn-benchmark\n\n/*****************************************************************************/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport os.path\n\nimport torch\n\ntry:\n    from torch.utils.cpp_extension import load\n    from torch.utils.cpp_extension import CUDA_HOME\nexcept ImportError:\n    raise ImportError(\n        ""The cpp layer extensions requires PyTorch 0.4 or higher"")\n\n\ndef _load_C_extensions():\n    this_dir = os.path.dirname(os.path.abspath(__file__))\n    this_dir = os.path.join(this_dir, ""csrc"")\n\n    main_file = glob.glob(os.path.join(this_dir, ""*.cpp""))\n    sources_cpu = glob.glob(os.path.join(this_dir, ""cpu"", ""*.cpp""))\n    sources_cuda = glob.glob(os.path.join(this_dir, ""cuda"", ""*.cu""))\n\n    sources = main_file + sources_cpu\n\n    extra_cflags = []\n    extra_cuda_cflags = []\n    if torch.cuda.is_available() and CUDA_HOME is not None:\n        sources.extend(sources_cuda)\n        extra_cflags = [""-O3"", ""-DWITH_CUDA""]\n        extra_cuda_cflags = [""--expt-extended-lambda""]\n    sources = [os.path.join(this_dir, s) for s in sources]\n    extra_include_paths = [this_dir]\n    return load(\n        name=""ext_lib"",\n        sources=sources,\n        extra_cflags=extra_cflags,\n        extra_include_paths=extra_include_paths,\n        extra_cuda_cflags=extra_cuda_cflags,\n    )\n\n\n_backend = _load_C_extensions()\n'"
isegm/model/syncbn/modules/functional/syncbn.py,3,"b'""""""\n/*****************************************************************************/\n\nBatchNorm2dSync with multi-gpu\n\ncode referenced from : https://github.com/mapillary/inplace_abn\n\n/*****************************************************************************/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport torch.cuda.comm as comm\nfrom torch.autograd import Function\nfrom torch.autograd.function import once_differentiable\nfrom ._csrc import _backend\n\n\ndef _count_samples(x):\n    count = 1\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            count *= s\n    return count\n\n\nclass BatchNorm2dSyncFunc(Function):\n\n    @staticmethod\n    def forward(ctx, x, weight, bias, running_mean, running_var,\n                extra, compute_stats=True, momentum=0.1, eps=1e-05):\n        def _parse_extra(ctx, extra):\n            ctx.is_master = extra[""is_master""]\n            if ctx.is_master:\n                ctx.master_queue = extra[""master_queue""]\n                ctx.worker_queues = extra[""worker_queues""]\n                ctx.worker_ids = extra[""worker_ids""]\n            else:\n                ctx.master_queue = extra[""master_queue""]\n                ctx.worker_queue = extra[""worker_queue""]\n        # Save context\n        if extra is not None:\n            _parse_extra(ctx, extra)\n        ctx.compute_stats = compute_stats\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.affine = weight is not None and bias is not None\n        if ctx.compute_stats:\n            N = _count_samples(x) * (ctx.master_queue.maxsize + 1)\n            assert N > 1\n            # 1. compute sum(x) and sum(x^2)\n            xsum, xsqsum = _backend.syncbn_sum_sqsum(x.detach())\n            if ctx.is_master:\n                xsums, xsqsums = [xsum], [xsqsum]\n                # master : gatther all sum(x) and sum(x^2) from slaves\n                for _ in range(ctx.master_queue.maxsize):\n                    xsum_w, xsqsum_w = ctx.master_queue.get()\n                    ctx.master_queue.task_done()\n                    xsums.append(xsum_w)\n                    xsqsums.append(xsqsum_w)\n                xsum = comm.reduce_add(xsums)\n                xsqsum = comm.reduce_add(xsqsums)\n                mean = xsum / N\n                sumvar = xsqsum - xsum * mean\n                var = sumvar / N\n                uvar = sumvar / (N - 1)\n                # master : broadcast global mean, variance to all slaves\n                tensors = comm.broadcast_coalesced(\n                    (mean, uvar, var), [mean.get_device()] + ctx.worker_ids)\n                for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                    queue.put(ts)\n            else:\n                # slave : send sum(x) and sum(x^2) to master\n                ctx.master_queue.put((xsum, xsqsum))\n                # slave : get global mean and variance\n                mean, uvar, var = ctx.worker_queue.get()\n                ctx.worker_queue.task_done()\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * uvar)\n            ctx.N = N\n            ctx.save_for_backward(x, weight, bias, mean, var)\n        else:\n            mean, var = running_mean, running_var\n\n        # do batch norm forward\n        z = _backend.syncbn_forward(x, weight, bias, mean, var,\n                                    ctx.affine, ctx.eps)\n        return z\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        x, weight, bias, mean, var = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # 1. compute \\sum(\\frac{dJ}{dy_i}) and \\sum(\\frac{dJ}{dy_i}*\\hat{x_i})\n        sum_dz, sum_dz_xhat = _backend.syncbn_backward_xhat(\n            dz, x, mean, var, ctx.eps)\n        if ctx.is_master:\n            sum_dzs, sum_dz_xhats = [sum_dz], [sum_dz_xhat]\n            # master : gatther from slaves\n            for _ in range(ctx.master_queue.maxsize):\n                sum_dz_w, sum_dz_xhat_w = ctx.master_queue.get()\n                ctx.master_queue.task_done()\n                sum_dzs.append(sum_dz_w)\n                sum_dz_xhats.append(sum_dz_xhat_w)\n            # master : compute global stats\n            sum_dz = comm.reduce_add(sum_dzs)\n            sum_dz_xhat = comm.reduce_add(sum_dz_xhats)\n            sum_dz /= ctx.N\n            sum_dz_xhat /= ctx.N\n            # master : broadcast global stats\n            tensors = comm.broadcast_coalesced(\n                (sum_dz, sum_dz_xhat), [mean.get_device()] + ctx.worker_ids)\n            for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                queue.put(ts)\n        else:\n            # slave : send to master\n            ctx.master_queue.put((sum_dz, sum_dz_xhat))\n            # slave : get global stats\n            sum_dz, sum_dz_xhat = ctx.worker_queue.get()\n            ctx.worker_queue.task_done()\n\n        # do batch norm backward\n        dx, dweight, dbias = _backend.syncbn_backward(\n            dz, x, weight, bias, mean, var, sum_dz, sum_dz_xhat,\n            ctx.affine, ctx.eps)\n\n        return dx, dweight, dbias, \\\n            None, None, None, None, None, None\n\nbatchnorm2d_sync = BatchNorm2dSyncFunc.apply\n\n__all__ = [""batchnorm2d_sync""]\n'"
isegm/model/syncbn/modules/nn/__init__.py,0,b'from .syncbn import *\n'
isegm/model/syncbn/modules/nn/syncbn.py,8,"b'""""""\n/*****************************************************************************/\n\nBatchNorm2dSync with multi-gpu\n\n/*****************************************************************************/\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\ntry:\n    # python 3\n    from queue import Queue\nexcept ImportError:\n    # python 2\n    from Queue import Queue\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.nn.parameter import Parameter\nfrom isegm.model.syncbn.modules.functional import batchnorm2d_sync\n\n\nclass _BatchNorm(nn.Module):\n    """"""\n    Customized BatchNorm from nn.BatchNorm\n    >> added freeze attribute to enable bn freeze.\n    """"""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n                 track_running_stats=True):\n        super(_BatchNorm, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        self.freezed = False\n        if self.affine:\n            self.weight = Parameter(torch.Tensor(num_features))\n            self.bias = Parameter(torch.Tensor(num_features))\n        else:\n            self.register_parameter(\'weight\', None)\n            self.register_parameter(\'bias\', None)\n        if self.track_running_stats:\n            self.register_buffer(\'running_mean\', torch.zeros(num_features))\n            self.register_buffer(\'running_var\', torch.ones(num_features))\n        else:\n            self.register_parameter(\'running_mean\', None)\n            self.register_parameter(\'running_var\', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        if self.track_running_stats:\n            self.running_mean.zero_()\n            self.running_var.fill_(1)\n        if self.affine:\n            self.weight.data.uniform_()\n            self.bias.data.zero_()\n\n    def _check_input_dim(self, input):\n        return NotImplemented\n\n    def forward(self, input):\n        self._check_input_dim(input)\n\n        compute_stats = not self.freezed and \\\n            self.training and self.track_running_stats\n\n        ret = F.batch_norm(input, self.running_mean, self.running_var,\n                           self.weight, self.bias, compute_stats,\n                           self.momentum, self.eps)\n        return ret\n\n    def extra_repr(self):\n        return \'{num_features}, eps={eps}, momentum={momentum}, \'\\\n               \'affine={affine}, \' \\\n               \'track_running_stats={track_running_stats}\'.format(\n                   **self.__dict__)\n\n\nclass BatchNorm2dNoSync(_BatchNorm):\n    """"""\n    Equivalent to nn.BatchNorm2d\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError(\'expected 4D input (got {}D input)\'\n                             .format(input.dim()))\n\n\nclass BatchNorm2dSync(BatchNorm2dNoSync):\n    """"""\n    BatchNorm2d with automatic multi-GPU Sync\n    """"""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n                 track_running_stats=True):\n        super(BatchNorm2dSync, self).__init__(\n            num_features, eps=eps, momentum=momentum, affine=affine,\n            track_running_stats=track_running_stats)\n        self.sync_enabled = True\n        self.devices = list(range(torch.cuda.device_count()))\n        if len(self.devices) > 1:\n            # Initialize queues\n            self.worker_ids = self.devices[1:]\n            self.master_queue = Queue(len(self.worker_ids))\n            self.worker_queues = [Queue(1) for _ in self.worker_ids]\n\n    def forward(self, x):\n        compute_stats = not self.freezed and \\\n            self.training and self.track_running_stats\n        if self.sync_enabled and compute_stats and len(self.devices) > 1:\n            if x.get_device() == self.devices[0]:\n                # Master mode\n                extra = {\n                    ""is_master"": True,\n                    ""master_queue"": self.master_queue,\n                    ""worker_queues"": self.worker_queues,\n                    ""worker_ids"": self.worker_ids\n                }\n            else:\n                # Worker mode\n                extra = {\n                    ""is_master"": False,\n                    ""master_queue"": self.master_queue,\n                    ""worker_queue"": self.worker_queues[\n                        self.worker_ids.index(x.get_device())]\n                }\n            return batchnorm2d_sync(x, self.weight, self.bias,\n                                    self.running_mean, self.running_var,\n                                    extra, compute_stats, self.momentum,\n                                    self.eps)\n        return super(BatchNorm2dSync, self).forward(x)\n\n    def __repr__(self):\n        """"""repr""""""\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n            \'affine={affine}, \' \\\n            \'track_running_stats={track_running_stats},\' \\\n            \'devices={devices})\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n#BatchNorm2d = BatchNorm2dNoSync\nBatchNorm2d = BatchNorm2dSync\n'"
