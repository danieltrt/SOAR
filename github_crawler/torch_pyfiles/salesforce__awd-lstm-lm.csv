file_path,api_count,code
data.py,1,"b'import os\nimport torch\n\nfrom collections import Counter\n\n\nclass Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n        self.counter = Counter()\n        self.total = 0\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        token_id = self.word2idx[word]\n        self.counter[token_id] += 1\n        self.total += 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\nclass Corpus(object):\n    def __init__(self, path):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(os.path.join(path, \'train.txt\'))\n        self.valid = self.tokenize(os.path.join(path, \'valid.txt\'))\n        self.test = self.tokenize(os.path.join(path, \'test.txt\'))\n\n    def tokenize(self, path):\n        """"""Tokenizes a text file.""""""\n        assert os.path.exists(path)\n        # Add words to the dictionary\n        with open(path, \'r\') as f:\n            tokens = 0\n            for line in f:\n                words = line.split() + [\'<eos>\']\n                tokens += len(words)\n                for word in words:\n                    self.dictionary.add_word(word)\n\n        # Tokenize file content\n        with open(path, \'r\') as f:\n            ids = torch.LongTensor(tokens)\n            token = 0\n            for line in f:\n                words = line.split() + [\'<eos>\']\n                for word in words:\n                    ids[token] = self.dictionary.word2idx[word]\n                    token += 1\n\n        return ids\n'"
embed_regularize.py,3,"b""import numpy as np\n\nimport torch\n\ndef embedded_dropout(embed, words, dropout=0.1, scale=None):\n  if dropout:\n    mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\n    masked_embed_weight = mask * embed.weight\n  else:\n    masked_embed_weight = embed.weight\n  if scale:\n    masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n\n  padding_idx = embed.padding_idx\n  if padding_idx is None:\n      padding_idx = -1\n\n  X = torch.nn.functional.embedding(words, masked_embed_weight,\n    padding_idx, embed.max_norm, embed.norm_type,\n    embed.scale_grad_by_freq, embed.sparse\n  )\n  return X\n\nif __name__ == '__main__':\n  V = 50\n  h = 4\n  bptt = 10\n  batch_size = 2\n\n  embed = torch.nn.Embedding(V, h)\n\n  words = np.random.random_integers(low=0, high=V-1, size=(batch_size, bptt))\n  words = torch.LongTensor(words)\n\n  origX = embed(words)\n  X = embedded_dropout(embed, words)\n\n  print(origX)\n  print(X)\n"""
finetune.py,11,"b'import argparse\nimport time\nimport math\nimport numpy as np\nnp.random.seed(331)\nimport torch\nimport torch.nn as nn\n\nimport data\nimport model\n\nfrom utils import batchify, get_batch, repackage_hidden\n\nparser = argparse.ArgumentParser(description=\'PyTorch PennTreeBank RNN/LSTM Language Model\')\nparser.add_argument(\'--data\', type=str, default=\'data/penn/\',\n                    help=\'location of the data corpus\')\nparser.add_argument(\'--model\', type=str, default=\'LSTM\',\n                    help=\'type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU)\')\nparser.add_argument(\'--emsize\', type=int, default=400,\n                    help=\'size of word embeddings\')\nparser.add_argument(\'--nhid\', type=int, default=1150,\n                    help=\'number of hidden units per layer\')\nparser.add_argument(\'--nlayers\', type=int, default=3,\n                    help=\'number of layers\')\nparser.add_argument(\'--lr\', type=float, default=30,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--clip\', type=float, default=0.25,\n                    help=\'gradient clipping\')\nparser.add_argument(\'--epochs\', type=int, default=8000,\n                    help=\'upper epoch limit\')\nparser.add_argument(\'--batch_size\', type=int, default=80, metavar=\'N\',\n                    help=\'batch size\')\nparser.add_argument(\'--bptt\', type=int, default=70,\n                    help=\'sequence length\')\nparser.add_argument(\'--dropout\', type=float, default=0.4,\n                    help=\'dropout applied to layers (0 = no dropout)\')\nparser.add_argument(\'--dropouth\', type=float, default=0.3,\n                    help=\'dropout for rnn layers (0 = no dropout)\')\nparser.add_argument(\'--dropouti\', type=float, default=0.65,\n                    help=\'dropout for input embedding layers (0 = no dropout)\')\nparser.add_argument(\'--dropoute\', type=float, default=0.1,\n                    help=\'dropout to remove words from embedding layer (0 = no dropout)\')\nparser.add_argument(\'--wdrop\', type=float, default=0.5,\n                    help=\'amount of weight dropout to apply to the RNN hidden to hidden matrix\')\nparser.add_argument(\'--tied\', action=\'store_false\',\n                    help=\'tie the word embedding and softmax weights\')\nparser.add_argument(\'--seed\', type=int, default=1111,\n                    help=\'random seed\')\nparser.add_argument(\'--nonmono\', type=int, default=5,\n                    help=\'random seed\')\nparser.add_argument(\'--cuda\', action=\'store_false\',\n                    help=\'use CUDA\')\nparser.add_argument(\'--log-interval\', type=int, default=200, metavar=\'N\',\n                    help=\'report interval\')\nrandomhash = \'\'.join(str(time.time()).split(\'.\'))\nparser.add_argument(\'--save\', type=str,  default=randomhash+\'.pt\',\n                    help=\'path to save the final model\')\nparser.add_argument(\'--alpha\', type=float, default=2,\n                    help=\'alpha L2 regularization on RNN activation (alpha = 0 means no regularization)\')\nparser.add_argument(\'--beta\', type=float, default=1,\n                    help=\'beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\')\nparser.add_argument(\'--wdecay\', type=float, default=1.2e-6,\n                    help=\'weight decay applied to all weights\')\nargs = parser.parse_args()\n\n# Set the random seed manually for reproducibility.\ntorch.manual_seed(args.seed)\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n    else:\n        torch.cuda.manual_seed(args.seed)\n\n###############################################################################\n# Load data\n###############################################################################\n\ncorpus = data.Corpus(args.data)\n\neval_batch_size = 10\ntest_batch_size = 1\ntrain_data = batchify(corpus.train, args.batch_size, args)\nval_data = batchify(corpus.valid, eval_batch_size, args)\ntest_data = batchify(corpus.test, test_batch_size, args)\n\n###############################################################################\n# Build the model\n###############################################################################\n\nntokens = len(corpus.dictionary)\nmodel = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.dropouth, args.dropouti, args.dropoute, args.wdrop, args.tied)\nif args.cuda:\n    model.cuda()\ntotal_params = sum(x.size()[0] * x.size()[1] if len(x.size()) > 1 else x.size()[0] for x in model.parameters())\nprint(\'Args:\', args)\nprint(\'Model total parameters:\', total_params)\n\ncriterion = nn.CrossEntropyLoss()\n\n###############################################################################\n# Training code\n###############################################################################\n\ndef evaluate(data_source, batch_size=10):\n    # Turn on evaluation mode which disables dropout.\n    if args.model == \'QRNN\': model.reset()\n    model.eval()\n    total_loss = 0\n    ntokens = len(corpus.dictionary)\n    hidden = model.init_hidden(batch_size)\n    for i in range(0, data_source.size(0) - 1, args.bptt):\n        data, targets = get_batch(data_source, i, args, evaluation=True)\n        output, hidden = model(data, hidden)\n        output_flat = output.view(-1, ntokens)\n        total_loss += len(data) * criterion(output_flat, targets).data\n        hidden = repackage_hidden(hidden)\n    return total_loss[0] / len(data_source)\n\n\ndef train():\n    # Turn on training mode which enables dropout.\n    if args.model == \'QRNN\': model.reset()\n    total_loss = 0\n    start_time = time.time()\n    ntokens = len(corpus.dictionary)\n    hidden = model.init_hidden(args.batch_size)\n    batch, i = 0, 0\n    while i < train_data.size(0) - 1 - 1:\n        bptt = args.bptt if np.random.random() < 0.95 else args.bptt / 2.\n        # Prevent excessively small or negative sequence lengths\n        seq_len = max(5, int(np.random.normal(bptt, 5)))\n        # There\'s a very small chance that it could select a very long sequence length resulting in OOM\n        seq_len = min(seq_len, args.bptt + 10)\n\n        lr2 = optimizer.param_groups[0][\'lr\']\n        optimizer.param_groups[0][\'lr\'] = lr2 * seq_len / args.bptt\n        model.train()\n        data, targets = get_batch(train_data, i, args, seq_len=seq_len)\n\n        # Starting each batch, we detach the hidden state from how it was previously produced.\n        # If we didn\'t, the model would try backpropagating all the way to start of the dataset.\n        hidden = repackage_hidden(hidden)\n        optimizer.zero_grad()\n\n        output, hidden, rnn_hs, dropped_rnn_hs = model(data, hidden, return_h=True)\n        raw_loss = criterion(output.view(-1, ntokens), targets)\n\n        loss = raw_loss\n        # Activiation Regularization\n        loss = loss + sum(args.alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n        # Temporal Activation Regularization (slowness)\n        loss = loss + sum(args.beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n        loss.backward()\n\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        torch.nn.utils.clip_grad_norm(model.parameters(), args.clip)\n        optimizer.step()\n\n        total_loss += raw_loss.data\n        optimizer.param_groups[0][\'lr\'] = lr2\n        if batch % args.log_interval == 0 and batch > 0:\n            cur_loss = total_loss[0] / args.log_interval\n            elapsed = time.time() - start_time\n            print(\'| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | \'\n                    \'loss {:5.2f} | ppl {:8.2f}\'.format(\n                epoch, batch, len(train_data) // args.bptt, optimizer.param_groups[0][\'lr\'],\n                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n            total_loss = 0\n            start_time = time.time()\n        ###\n        batch += 1\n        i += seq_len\n\n\n# Load the best saved model.\nwith open(args.save, \'rb\') as f:\n    model = torch.load(f)\n\n\n# Loop over epochs.\nlr = args.lr\nstored_loss = evaluate(val_data)\nbest_val_loss = []\n# At any point you can hit Ctrl + C to break out of training early.\ntry:\n    #optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, weight_decay=args.wdecay)\n    optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n    for epoch in range(1, args.epochs+1):\n        epoch_start_time = time.time()\n        train()\n        if \'t0\' in optimizer.param_groups[0]:\n            tmp = {}\n            for prm in model.parameters():\n                tmp[prm] = prm.data.clone()\n                prm.data = optimizer.state[prm][\'ax\'].clone()\n\n            val_loss2 = evaluate(val_data)\n            print(\'-\' * 89)\n            print(\'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | \'\n                    \'valid ppl {:8.2f}\'.format(epoch, (time.time() - epoch_start_time),\n                                               val_loss2, math.exp(val_loss2)))\n            print(\'-\' * 89)\n\n            if val_loss2 < stored_loss:\n                with open(args.save, \'wb\') as f:\n                    torch.save(model, f)\n                print(\'Saving Averaged!\')\n                stored_loss = val_loss2\n\n            for prm in model.parameters():\n                prm.data = tmp[prm].clone()\n\n        if (len(best_val_loss)>args.nonmono and val_loss2 > min(best_val_loss[:-args.nonmono])):\n            print(\'Done!\')\n            import sys\n            sys.exit(1)\n            optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n            #optimizer.param_groups[0][\'lr\'] /= 2.\n        best_val_loss.append(val_loss2)\n\nexcept KeyboardInterrupt:\n    print(\'-\' * 89)\n    print(\'Exiting from training early\')\n\n# Load the best saved model.\nwith open(args.save, \'rb\') as f:\n    model = torch.load(f)\n    \n# Run on test data.\ntest_loss = evaluate(test_data, test_batch_size)\nprint(\'=\' * 89)\nprint(\'| End of training | test loss {:5.2f} | test ppl {:8.2f}\'.format(\n    test_loss, math.exp(test_loss)))\nprint(\'=\' * 89)\n'"
generate.py,7,"b'###############################################################################\n# Language Modeling on Penn Tree Bank\n#\n# This file generates new sentences sampled from the language model\n#\n###############################################################################\n\nimport argparse\n\nimport torch\nfrom torch.autograd import Variable\n\nimport data\n\nparser = argparse.ArgumentParser(description=\'PyTorch PTB Language Model\')\n\n# Model parameters.\nparser.add_argument(\'--data\', type=str, default=\'./data/penn\',\n                    help=\'location of the data corpus\')\nparser.add_argument(\'--model\', type=str, default=\'LSTM\',\n                    help=\'type of recurrent net (LSTM, QRNN)\')\nparser.add_argument(\'--checkpoint\', type=str, default=\'./model.pt\',\n                    help=\'model checkpoint to use\')\nparser.add_argument(\'--outf\', type=str, default=\'generated.txt\',\n                    help=\'output file for generated text\')\nparser.add_argument(\'--words\', type=int, default=\'1000\',\n                    help=\'number of words to generate\')\nparser.add_argument(\'--seed\', type=int, default=1111,\n                    help=\'random seed\')\nparser.add_argument(\'--cuda\', action=\'store_true\',\n                    help=\'use CUDA\')\nparser.add_argument(\'--temperature\', type=float, default=1.0,\n                    help=\'temperature - higher will increase diversity\')\nparser.add_argument(\'--log-interval\', type=int, default=100,\n                    help=\'reporting interval\')\nargs = parser.parse_args()\n\n# Set the random seed manually for reproducibility.\ntorch.manual_seed(args.seed)\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n    else:\n        torch.cuda.manual_seed(args.seed)\n\nif args.temperature < 1e-3:\n    parser.error(""--temperature has to be greater or equal 1e-3"")\n\nwith open(args.checkpoint, \'rb\') as f:\n    model = torch.load(f)\nmodel.eval()\nif args.model == \'QRNN\':\n    model.reset()\n\nif args.cuda:\n    model.cuda()\nelse:\n    model.cpu()\n\ncorpus = data.Corpus(args.data)\nntokens = len(corpus.dictionary)\nhidden = model.init_hidden(1)\ninput = Variable(torch.rand(1, 1).mul(ntokens).long(), volatile=True)\nif args.cuda:\n    input.data = input.data.cuda()\n\nwith open(args.outf, \'w\') as outf:\n    for i in range(args.words):\n        output, hidden = model(input, hidden)\n        word_weights = output.squeeze().data.div(args.temperature).exp().cpu()\n        word_idx = torch.multinomial(word_weights, 1)[0]\n        input.data.fill_(word_idx)\n        word = corpus.dictionary.idx2word[word_idx]\n\n        outf.write(word + (\'\\n\' if i % 20 == 19 else \' \'))\n\n        if i % args.log_interval == 0:\n            print(\'| Generated {}/{} words\'.format(i, args.words))\n'"
locked_dropout.py,2,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nclass LockedDropout(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, dropout=0.5):\n        if not self.training or not dropout:\n            return x\n        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - dropout)\n        mask = Variable(m, requires_grad=False) / (1 - dropout)\n        mask = mask.expand_as(x)\n        return mask * x\n'"
main.py,12,"b'import argparse\nimport time\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nimport data\nimport model\n\nfrom utils import batchify, get_batch, repackage_hidden\n\nparser = argparse.ArgumentParser(description=\'PyTorch PennTreeBank RNN/LSTM Language Model\')\nparser.add_argument(\'--data\', type=str, default=\'data/penn/\',\n                    help=\'location of the data corpus\')\nparser.add_argument(\'--model\', type=str, default=\'LSTM\',\n                    help=\'type of recurrent net (LSTM, QRNN, GRU)\')\nparser.add_argument(\'--emsize\', type=int, default=400,\n                    help=\'size of word embeddings\')\nparser.add_argument(\'--nhid\', type=int, default=1150,\n                    help=\'number of hidden units per layer\')\nparser.add_argument(\'--nlayers\', type=int, default=3,\n                    help=\'number of layers\')\nparser.add_argument(\'--lr\', type=float, default=30,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--clip\', type=float, default=0.25,\n                    help=\'gradient clipping\')\nparser.add_argument(\'--epochs\', type=int, default=8000,\n                    help=\'upper epoch limit\')\nparser.add_argument(\'--batch_size\', type=int, default=80, metavar=\'N\',\n                    help=\'batch size\')\nparser.add_argument(\'--bptt\', type=int, default=70,\n                    help=\'sequence length\')\nparser.add_argument(\'--dropout\', type=float, default=0.4,\n                    help=\'dropout applied to layers (0 = no dropout)\')\nparser.add_argument(\'--dropouth\', type=float, default=0.3,\n                    help=\'dropout for rnn layers (0 = no dropout)\')\nparser.add_argument(\'--dropouti\', type=float, default=0.65,\n                    help=\'dropout for input embedding layers (0 = no dropout)\')\nparser.add_argument(\'--dropoute\', type=float, default=0.1,\n                    help=\'dropout to remove words from embedding layer (0 = no dropout)\')\nparser.add_argument(\'--wdrop\', type=float, default=0.5,\n                    help=\'amount of weight dropout to apply to the RNN hidden to hidden matrix\')\nparser.add_argument(\'--seed\', type=int, default=1111,\n                    help=\'random seed\')\nparser.add_argument(\'--nonmono\', type=int, default=5,\n                    help=\'random seed\')\nparser.add_argument(\'--cuda\', action=\'store_false\',\n                    help=\'use CUDA\')\nparser.add_argument(\'--log-interval\', type=int, default=200, metavar=\'N\',\n                    help=\'report interval\')\nrandomhash = \'\'.join(str(time.time()).split(\'.\'))\nparser.add_argument(\'--save\', type=str,  default=randomhash+\'.pt\',\n                    help=\'path to save the final model\')\nparser.add_argument(\'--alpha\', type=float, default=2,\n                    help=\'alpha L2 regularization on RNN activation (alpha = 0 means no regularization)\')\nparser.add_argument(\'--beta\', type=float, default=1,\n                    help=\'beta slowness regularization applied on RNN activiation (beta = 0 means no regularization)\')\nparser.add_argument(\'--wdecay\', type=float, default=1.2e-6,\n                    help=\'weight decay applied to all weights\')\nparser.add_argument(\'--resume\', type=str,  default=\'\',\n                    help=\'path of model to resume\')\nparser.add_argument(\'--optimizer\', type=str,  default=\'sgd\',\n                    help=\'optimizer to use (sgd, adam)\')\nparser.add_argument(\'--when\', nargs=""+"", type=int, default=[-1],\n                    help=\'When (which epochs) to divide the learning rate by 10 - accepts multiple\')\nargs = parser.parse_args()\nargs.tied = True\n\n# Set the random seed manually for reproducibility.\nnp.random.seed(args.seed)\ntorch.manual_seed(args.seed)\nif torch.cuda.is_available():\n    if not args.cuda:\n        print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n    else:\n        torch.cuda.manual_seed(args.seed)\n\n###############################################################################\n# Load data\n###############################################################################\n\ndef model_save(fn):\n    with open(fn, \'wb\') as f:\n        torch.save([model, criterion, optimizer], f)\n\ndef model_load(fn):\n    global model, criterion, optimizer\n    with open(fn, \'rb\') as f:\n        model, criterion, optimizer = torch.load(f)\n\nimport os\nimport hashlib\nfn = \'corpus.{}.data\'.format(hashlib.md5(args.data.encode()).hexdigest())\nif os.path.exists(fn):\n    print(\'Loading cached dataset...\')\n    corpus = torch.load(fn)\nelse:\n    print(\'Producing dataset...\')\n    corpus = data.Corpus(args.data)\n    torch.save(corpus, fn)\n\neval_batch_size = 10\ntest_batch_size = 1\ntrain_data = batchify(corpus.train, args.batch_size, args)\nval_data = batchify(corpus.valid, eval_batch_size, args)\ntest_data = batchify(corpus.test, test_batch_size, args)\n\n###############################################################################\n# Build the model\n###############################################################################\n\nfrom splitcross import SplitCrossEntropyLoss\ncriterion = None\n\nntokens = len(corpus.dictionary)\nmodel = model.RNNModel(args.model, ntokens, args.emsize, args.nhid, args.nlayers, args.dropout, args.dropouth, args.dropouti, args.dropoute, args.wdrop, args.tied)\n###\nif args.resume:\n    print(\'Resuming model ...\')\n    model_load(args.resume)\n    optimizer.param_groups[0][\'lr\'] = args.lr\n    model.dropouti, model.dropouth, model.dropout, args.dropoute = args.dropouti, args.dropouth, args.dropout, args.dropoute\n    if args.wdrop:\n        from weight_drop import WeightDrop\n        for rnn in model.rnns:\n            if type(rnn) == WeightDrop: rnn.dropout = args.wdrop\n            elif rnn.zoneout > 0: rnn.zoneout = args.wdrop\n###\nif not criterion:\n    splits = []\n    if ntokens > 500000:\n        # One Billion\n        # This produces fairly even matrix mults for the buckets:\n        # 0: 11723136, 1: 10854630, 2: 11270961, 3: 11219422\n        splits = [4200, 35000, 180000]\n    elif ntokens > 75000:\n        # WikiText-103\n        splits = [2800, 20000, 76000]\n    print(\'Using\', splits)\n    criterion = SplitCrossEntropyLoss(args.emsize, splits=splits, verbose=False)\n###\nif args.cuda:\n    model = model.cuda()\n    criterion = criterion.cuda()\n###\nparams = list(model.parameters()) + list(criterion.parameters())\ntotal_params = sum(x.size()[0] * x.size()[1] if len(x.size()) > 1 else x.size()[0] for x in params if x.size())\nprint(\'Args:\', args)\nprint(\'Model total parameters:\', total_params)\n\n###############################################################################\n# Training code\n###############################################################################\n\ndef evaluate(data_source, batch_size=10):\n    # Turn on evaluation mode which disables dropout.\n    model.eval()\n    if args.model == \'QRNN\': model.reset()\n    total_loss = 0\n    ntokens = len(corpus.dictionary)\n    hidden = model.init_hidden(batch_size)\n    for i in range(0, data_source.size(0) - 1, args.bptt):\n        data, targets = get_batch(data_source, i, args, evaluation=True)\n        output, hidden = model(data, hidden)\n        total_loss += len(data) * criterion(model.decoder.weight, model.decoder.bias, output, targets).data\n        hidden = repackage_hidden(hidden)\n    return total_loss.item() / len(data_source)\n\n\ndef train():\n    # Turn on training mode which enables dropout.\n    if args.model == \'QRNN\': model.reset()\n    total_loss = 0\n    start_time = time.time()\n    ntokens = len(corpus.dictionary)\n    hidden = model.init_hidden(args.batch_size)\n    batch, i = 0, 0\n    while i < train_data.size(0) - 1 - 1:\n        bptt = args.bptt if np.random.random() < 0.95 else args.bptt / 2.\n        # Prevent excessively small or negative sequence lengths\n        seq_len = max(5, int(np.random.normal(bptt, 5)))\n        # There\'s a very small chance that it could select a very long sequence length resulting in OOM\n        # seq_len = min(seq_len, args.bptt + 10)\n\n        lr2 = optimizer.param_groups[0][\'lr\']\n        optimizer.param_groups[0][\'lr\'] = lr2 * seq_len / args.bptt\n        model.train()\n        data, targets = get_batch(train_data, i, args, seq_len=seq_len)\n\n        # Starting each batch, we detach the hidden state from how it was previously produced.\n        # If we didn\'t, the model would try backpropagating all the way to start of the dataset.\n        hidden = repackage_hidden(hidden)\n        optimizer.zero_grad()\n\n        output, hidden, rnn_hs, dropped_rnn_hs = model(data, hidden, return_h=True)\n        raw_loss = criterion(model.decoder.weight, model.decoder.bias, output, targets)\n\n        loss = raw_loss\n        # Activiation Regularization\n        if args.alpha: loss = loss + sum(args.alpha * dropped_rnn_h.pow(2).mean() for dropped_rnn_h in dropped_rnn_hs[-1:])\n        # Temporal Activation Regularization (slowness)\n        if args.beta: loss = loss + sum(args.beta * (rnn_h[1:] - rnn_h[:-1]).pow(2).mean() for rnn_h in rnn_hs[-1:])\n        loss.backward()\n\n        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n        if args.clip: torch.nn.utils.clip_grad_norm_(params, args.clip)\n        optimizer.step()\n\n        total_loss += raw_loss.data\n        optimizer.param_groups[0][\'lr\'] = lr2\n        if batch % args.log_interval == 0 and batch > 0:\n            cur_loss = total_loss.item() / args.log_interval\n            elapsed = time.time() - start_time\n            print(\'| epoch {:3d} | {:5d}/{:5d} batches | lr {:05.5f} | ms/batch {:5.2f} | \'\n                    \'loss {:5.2f} | ppl {:8.2f} | bpc {:8.3f}\'.format(\n                epoch, batch, len(train_data) // args.bptt, optimizer.param_groups[0][\'lr\'],\n                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss), cur_loss / math.log(2)))\n            total_loss = 0\n            start_time = time.time()\n        ###\n        batch += 1\n        i += seq_len\n\n# Loop over epochs.\nlr = args.lr\nbest_val_loss = []\nstored_loss = 100000000\n\n# At any point you can hit Ctrl + C to break out of training early.\ntry:\n    optimizer = None\n    # Ensure the optimizer is optimizing params, which includes both the model\'s weights as well as the criterion\'s weight (i.e. Adaptive Softmax)\n    if args.optimizer == \'sgd\':\n        optimizer = torch.optim.SGD(params, lr=args.lr, weight_decay=args.wdecay)\n    if args.optimizer == \'adam\':\n        optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay=args.wdecay)\n    for epoch in range(1, args.epochs+1):\n        epoch_start_time = time.time()\n        train()\n        if \'t0\' in optimizer.param_groups[0]:\n            tmp = {}\n            for prm in model.parameters():\n                tmp[prm] = prm.data.clone()\n                prm.data = optimizer.state[prm][\'ax\'].clone()\n\n            val_loss2 = evaluate(val_data)\n            print(\'-\' * 89)\n            print(\'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | \'\n                \'valid ppl {:8.2f} | valid bpc {:8.3f}\'.format(\n                    epoch, (time.time() - epoch_start_time), val_loss2, math.exp(val_loss2), val_loss2 / math.log(2)))\n            print(\'-\' * 89)\n\n            if val_loss2 < stored_loss:\n                model_save(args.save)\n                print(\'Saving Averaged!\')\n                stored_loss = val_loss2\n\n            for prm in model.parameters():\n                prm.data = tmp[prm].clone()\n\n        else:\n            val_loss = evaluate(val_data, eval_batch_size)\n            print(\'-\' * 89)\n            print(\'| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | \'\n                \'valid ppl {:8.2f} | valid bpc {:8.3f}\'.format(\n              epoch, (time.time() - epoch_start_time), val_loss, math.exp(val_loss), val_loss / math.log(2)))\n            print(\'-\' * 89)\n\n            if val_loss < stored_loss:\n                model_save(args.save)\n                print(\'Saving model (new best validation)\')\n                stored_loss = val_loss\n\n            if args.optimizer == \'sgd\' and \'t0\' not in optimizer.param_groups[0] and (len(best_val_loss)>args.nonmono and val_loss > min(best_val_loss[:-args.nonmono])):\n                print(\'Switching to ASGD\')\n                optimizer = torch.optim.ASGD(model.parameters(), lr=args.lr, t0=0, lambd=0., weight_decay=args.wdecay)\n\n            if epoch in args.when:\n                print(\'Saving model before learning rate decreased\')\n                model_save(\'{}.e{}\'.format(args.save, epoch))\n                print(\'Dividing learning rate by 10\')\n                optimizer.param_groups[0][\'lr\'] /= 10.\n\n            best_val_loss.append(val_loss)\n\nexcept KeyboardInterrupt:\n    print(\'-\' * 89)\n    print(\'Exiting from training early\')\n\n# Load the best saved model.\nmodel_load(args.save)\n\n# Run on test data.\ntest_loss = evaluate(test_data, test_batch_size)\nprint(\'=\' * 89)\nprint(\'| End of training | test loss {:5.2f} | test ppl {:8.2f} | test bpc {:8.3f}\'.format(\n    test_loss, math.exp(test_loss), test_loss / math.log(2)))\nprint(\'=\' * 89)\n'"
model.py,4,"b'import torch\nimport torch.nn as nn\n\nfrom embed_regularize import embedded_dropout\nfrom locked_dropout import LockedDropout\nfrom weight_drop import WeightDrop\n\nclass RNNModel(nn.Module):\n    """"""Container module with an encoder, a recurrent module, and a decoder.""""""\n\n    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, dropouth=0.5, dropouti=0.5, dropoute=0.1, wdrop=0, tie_weights=False):\n        super(RNNModel, self).__init__()\n        self.lockdrop = LockedDropout()\n        self.idrop = nn.Dropout(dropouti)\n        self.hdrop = nn.Dropout(dropouth)\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        assert rnn_type in [\'LSTM\', \'QRNN\', \'GRU\'], \'RNN type is not supported\'\n        if rnn_type == \'LSTM\':\n            self.rnns = [torch.nn.LSTM(ninp if l == 0 else nhid, nhid if l != nlayers - 1 else (ninp if tie_weights else nhid), 1, dropout=0) for l in range(nlayers)]\n            if wdrop:\n                self.rnns = [WeightDrop(rnn, [\'weight_hh_l0\'], dropout=wdrop) for rnn in self.rnns]\n        if rnn_type == \'GRU\':\n            self.rnns = [torch.nn.GRU(ninp if l == 0 else nhid, nhid if l != nlayers - 1 else ninp, 1, dropout=0) for l in range(nlayers)]\n            if wdrop:\n                self.rnns = [WeightDrop(rnn, [\'weight_hh_l0\'], dropout=wdrop) for rnn in self.rnns]\n        elif rnn_type == \'QRNN\':\n            from torchqrnn import QRNNLayer\n            self.rnns = [QRNNLayer(input_size=ninp if l == 0 else nhid, hidden_size=nhid if l != nlayers - 1 else (ninp if tie_weights else nhid), save_prev_x=True, zoneout=0, window=2 if l == 0 else 1, output_gate=True) for l in range(nlayers)]\n            for rnn in self.rnns:\n                rnn.linear = WeightDrop(rnn.linear, [\'weight\'], dropout=wdrop)\n        print(self.rnns)\n        self.rnns = torch.nn.ModuleList(self.rnns)\n        self.decoder = nn.Linear(nhid, ntoken)\n\n        # Optionally tie weights as in:\n        # ""Using the Output Embedding to Improve Language Models"" (Press & Wolf 2016)\n        # https://arxiv.org/abs/1608.05859\n        # and\n        # ""Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling"" (Inan et al. 2016)\n        # https://arxiv.org/abs/1611.01462\n        if tie_weights:\n            #if nhid != ninp:\n            #    raise ValueError(\'When using the tied flag, nhid must be equal to emsize\')\n            self.decoder.weight = self.encoder.weight\n\n        self.init_weights()\n\n        self.rnn_type = rnn_type\n        self.ninp = ninp\n        self.nhid = nhid\n        self.nlayers = nlayers\n        self.dropout = dropout\n        self.dropouti = dropouti\n        self.dropouth = dropouth\n        self.dropoute = dropoute\n        self.tie_weights = tie_weights\n\n    def reset(self):\n        if self.rnn_type == \'QRNN\': [r.reset() for r in self.rnns]\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.fill_(0)\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, input, hidden, return_h=False):\n        emb = embedded_dropout(self.encoder, input, dropout=self.dropoute if self.training else 0)\n        #emb = self.idrop(emb)\n\n        emb = self.lockdrop(emb, self.dropouti)\n\n        raw_output = emb\n        new_hidden = []\n        #raw_output, hidden = self.rnn(emb, hidden)\n        raw_outputs = []\n        outputs = []\n        for l, rnn in enumerate(self.rnns):\n            current_input = raw_output\n            raw_output, new_h = rnn(raw_output, hidden[l])\n            new_hidden.append(new_h)\n            raw_outputs.append(raw_output)\n            if l != self.nlayers - 1:\n                #self.hdrop(raw_output)\n                raw_output = self.lockdrop(raw_output, self.dropouth)\n                outputs.append(raw_output)\n        hidden = new_hidden\n\n        output = self.lockdrop(raw_output, self.dropout)\n        outputs.append(output)\n\n        result = output.view(output.size(0)*output.size(1), output.size(2))\n        if return_h:\n            return result, hidden, raw_outputs, outputs\n        return result, hidden\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters()).data\n        if self.rnn_type == \'LSTM\':\n            return [(weight.new(1, bsz, self.nhid if l != self.nlayers - 1 else (self.ninp if self.tie_weights else self.nhid)).zero_(),\n                    weight.new(1, bsz, self.nhid if l != self.nlayers - 1 else (self.ninp if self.tie_weights else self.nhid)).zero_())\n                    for l in range(self.nlayers)]\n        elif self.rnn_type == \'QRNN\' or self.rnn_type == \'GRU\':\n            return [weight.new(1, bsz, self.nhid if l != self.nlayers - 1 else (self.ninp if self.tie_weights else self.nhid)).zero_()\n                    for l in range(self.nlayers)]\n'"
pointer.py,14,"b""import argparse\nimport time\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nimport data\nimport model\n\nfrom utils import batchify, get_batch, repackage_hidden\n\nparser = argparse.ArgumentParser(description='PyTorch PennTreeBank RNN/LSTM Language Model')\nparser.add_argument('--data', type=str, default='data/penn',\n                    help='location of the data corpus')\nparser.add_argument('--model', type=str, default='LSTM',\n                    help='type of recurrent net (LSTM, QRNN)')\nparser.add_argument('--save', type=str,default='best.pt',\n                    help='model to use the pointer over')\nparser.add_argument('--cuda', action='store_false',\n                    help='use CUDA')\nparser.add_argument('--bptt', type=int, default=5000,\n                    help='sequence length')\nparser.add_argument('--window', type=int, default=3785,\n                    help='pointer window length')\nparser.add_argument('--theta', type=float, default=0.6625523432485668,\n                    help='mix between uniform distribution and pointer softmax distribution over previous words')\nparser.add_argument('--lambdasm', type=float, default=0.12785920428335693,\n                    help='linear mix between only pointer (1) and only vocab (0) distribution')\nargs = parser.parse_args()\n\n###############################################################################\n# Load data\n###############################################################################\n\ncorpus = data.Corpus(args.data)\n\neval_batch_size = 1\ntest_batch_size = 1\n#train_data = batchify(corpus.train, args.batch_size)\nval_data = batchify(corpus.valid, test_batch_size, args)\ntest_data = batchify(corpus.test, test_batch_size, args)\n\n###############################################################################\n# Build the model\n###############################################################################\n\nntokens = len(corpus.dictionary)\ncriterion = nn.CrossEntropyLoss()\n\ndef one_hot(idx, size, cuda=True):\n    a = np.zeros((1, size), np.float32)\n    a[0][idx] = 1\n    v = Variable(torch.from_numpy(a))\n    if cuda: v = v.cuda()\n    return v\n\ndef evaluate(data_source, batch_size=10, window=args.window):\n    # Turn on evaluation mode which disables dropout.\n    if args.model == 'QRNN': model.reset()\n    model.eval()\n    total_loss = 0\n    ntokens = len(corpus.dictionary)\n    hidden = model.init_hidden(batch_size)\n    next_word_history = None\n    pointer_history = None\n    for i in range(0, data_source.size(0) - 1, args.bptt):\n        if i > 0: print(i, len(data_source), math.exp(total_loss / i))\n        data, targets = get_batch(data_source, i, evaluation=True, args=args)\n        output, hidden, rnn_outs, _ = model(data, hidden, return_h=True)\n        rnn_out = rnn_outs[-1].squeeze()\n        output_flat = output.view(-1, ntokens)\n        ###\n        # Fill pointer history\n        start_idx = len(next_word_history) if next_word_history is not None else 0\n        next_word_history = torch.cat([one_hot(t.data[0], ntokens) for t in targets]) if next_word_history is None else torch.cat([next_word_history, torch.cat([one_hot(t.data[0], ntokens) for t in targets])])\n        #print(next_word_history)\n        pointer_history = Variable(rnn_out.data) if pointer_history is None else torch.cat([pointer_history, Variable(rnn_out.data)], dim=0)\n        #print(pointer_history)\n        ###\n        # Built-in cross entropy\n        # total_loss += len(data) * criterion(output_flat, targets).data[0]\n        ###\n        # Manual cross entropy\n        # softmax_output_flat = torch.nn.functional.softmax(output_flat)\n        # soft = torch.gather(softmax_output_flat, dim=1, index=targets.view(-1, 1))\n        # entropy = -torch.log(soft)\n        # total_loss += len(data) * entropy.mean().data[0]\n        ###\n        # Pointer manual cross entropy\n        loss = 0\n        softmax_output_flat = torch.nn.functional.softmax(output_flat)\n        for idx, vocab_loss in enumerate(softmax_output_flat):\n            p = vocab_loss\n            if start_idx + idx > window:\n                valid_next_word = next_word_history[start_idx + idx - window:start_idx + idx]\n                valid_pointer_history = pointer_history[start_idx + idx - window:start_idx + idx]\n                logits = torch.mv(valid_pointer_history, rnn_out[idx])\n                theta = args.theta\n                ptr_attn = torch.nn.functional.softmax(theta * logits).view(-1, 1)\n                ptr_dist = (ptr_attn.expand_as(valid_next_word) * valid_next_word).sum(0).squeeze()\n                lambdah = args.lambdasm\n                p = lambdah * ptr_dist + (1 - lambdah) * vocab_loss\n            ###\n            target_loss = p[targets[idx].data]\n            loss += (-torch.log(target_loss)).data[0]\n        total_loss += loss / batch_size\n        ###\n        hidden = repackage_hidden(hidden)\n        next_word_history = next_word_history[-window:]\n        pointer_history = pointer_history[-window:]\n    return total_loss / len(data_source)\n\n# Load the best saved model.\nwith open(args.save, 'rb') as f:\n    if not args.cuda:\n        model = torch.load(f, map_location=lambda storage, loc: storage)\n    else:\n        model = torch.load(f)\nprint(model)\n\n# Run on val data.\nval_loss = evaluate(val_data, test_batch_size)\nprint('=' * 89)\nprint('| End of pointer | val loss {:5.2f} | val ppl {:8.2f}'.format(\n    val_loss, math.exp(val_loss)))\nprint('=' * 89)\n\n# Run on test data.\ntest_loss = evaluate(test_data, test_batch_size)\nprint('=' * 89)\nprint('| End of pointer | test loss {:5.2f} | test ppl {:8.2f}'.format(\n    test_loss, math.exp(test_loss)))\nprint('=' * 89)\n"""
splitcross.py,31,"b""from collections import defaultdict\n\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\n\n\nclass SplitCrossEntropyLoss(nn.Module):\n    r'''SplitCrossEntropyLoss calculates an approximate softmax'''\n    def __init__(self, hidden_size, splits, verbose=False):\n        # We assume splits is [0, split1, split2, N] where N >= |V|\n        # For example, a vocab of 1000 words may have splits [0] + [100, 500] + [inf]\n        super(SplitCrossEntropyLoss, self).__init__()\n        self.hidden_size = hidden_size\n        self.splits = [0] + splits + [100 * 1000000]\n        self.nsplits = len(self.splits) - 1\n        self.stats = defaultdict(list)\n        self.verbose = verbose\n        # Each of the splits that aren't in the head require a pretend token, we'll call them tombstones\n        # The probability given to this tombstone is the probability of selecting an item from the represented split\n        if self.nsplits > 1:\n            self.tail_vectors = nn.Parameter(torch.zeros(self.nsplits - 1, hidden_size))\n            self.tail_bias = nn.Parameter(torch.zeros(self.nsplits - 1))\n\n    def logprob(self, weight, bias, hiddens, splits=None, softmaxed_head_res=None, verbose=False):\n        # First we perform the first softmax on the head vocabulary and the tombstones\n        if softmaxed_head_res is None:\n            start, end = self.splits[0], self.splits[1]\n            head_weight = None if end - start == 0 else weight[start:end]\n            head_bias = None if end - start == 0 else bias[start:end]\n            # We only add the tombstones if we have more than one split\n            if self.nsplits > 1:\n                head_weight = self.tail_vectors if head_weight is None else torch.cat([head_weight, self.tail_vectors])\n                head_bias = self.tail_bias if head_bias is None else torch.cat([head_bias, self.tail_bias])\n\n            # Perform the softmax calculation for the word vectors in the head for all splits\n            # We need to guard against empty splits as torch.cat does not like random lists\n            head_res = torch.nn.functional.linear(hiddens, head_weight, bias=head_bias)\n            softmaxed_head_res = torch.nn.functional.log_softmax(head_res, dim=-1)\n\n        if splits is None:\n            splits = list(range(self.nsplits))\n\n        results = []\n        running_offset = 0\n        for idx in splits:\n\n            # For those targets in the head (idx == 0) we only need to return their loss\n            if idx == 0:\n                results.append(softmaxed_head_res[:, :-(self.nsplits - 1)])\n\n            # If the target is in one of the splits, the probability is the p(tombstone) * p(word within tombstone)\n            else:\n                start, end = self.splits[idx], self.splits[idx + 1]\n                tail_weight = weight[start:end]\n                tail_bias = bias[start:end]\n\n                # Calculate the softmax for the words in the tombstone\n                tail_res = torch.nn.functional.linear(hiddens, tail_weight, bias=tail_bias)\n\n                # Then we calculate p(tombstone) * p(word in tombstone)\n                # Adding is equivalent to multiplication in log space\n                head_entropy = (softmaxed_head_res[:, -idx]).contiguous()\n                tail_entropy = torch.nn.functional.log_softmax(tail_res, dim=-1)\n                results.append(head_entropy.view(-1, 1) + tail_entropy)\n\n        if len(results) > 1:\n            return torch.cat(results, dim=1)\n        return results[0]\n\n    def split_on_targets(self, hiddens, targets):\n        # Split the targets into those in the head and in the tail\n        split_targets = []\n        split_hiddens = []\n\n        # Determine to which split each element belongs (for each start split value, add 1 if equal or greater)\n        # This method appears slower at least for WT-103 values for approx softmax\n        #masks = [(targets >= self.splits[idx]).view(1, -1) for idx in range(1, self.nsplits)]\n        #mask = torch.sum(torch.cat(masks, dim=0), dim=0)\n        ###\n        # This is equally fast for smaller splits as method below but scales linearly\n        mask = None\n        for idx in range(1, self.nsplits):\n            partial_mask = targets >= self.splits[idx]\n            mask = mask + partial_mask if mask is not None else partial_mask\n        ###\n        #masks = torch.stack([targets] * (self.nsplits - 1))\n        #mask = torch.sum(masks >= self.split_starts, dim=0)\n        for idx in range(self.nsplits):\n            # If there are no splits, avoid costly masked select\n            if self.nsplits == 1:\n                split_targets, split_hiddens = [targets], [hiddens]\n                continue\n            # If all the words are covered by earlier targets, we have empties so later stages don't freak out\n            if sum(len(t) for t in split_targets) == len(targets):\n                split_targets.append([])\n                split_hiddens.append([])\n                continue\n            # Are you in our split?\n            tmp_mask = mask == idx\n            split_targets.append(torch.masked_select(targets, tmp_mask))\n            split_hiddens.append(hiddens.masked_select(tmp_mask.unsqueeze(1).expand_as(hiddens)).view(-1, hiddens.size(1)))\n        return split_targets, split_hiddens\n\n    def forward(self, weight, bias, hiddens, targets, verbose=False):\n        if self.verbose or verbose:\n            for idx in sorted(self.stats):\n                print('{}: {}'.format(idx, int(np.mean(self.stats[idx]))), end=', ')\n            print()\n\n        total_loss = None\n        if len(hiddens.size()) > 2: hiddens = hiddens.view(-1, hiddens.size(2))\n\n        split_targets, split_hiddens = self.split_on_targets(hiddens, targets)\n\n        # First we perform the first softmax on the head vocabulary and the tombstones\n        start, end = self.splits[0], self.splits[1]\n        head_weight = None if end - start == 0 else weight[start:end]\n        head_bias = None if end - start == 0 else bias[start:end]\n\n        # We only add the tombstones if we have more than one split\n        if self.nsplits > 1:\n            head_weight = self.tail_vectors if head_weight is None else torch.cat([head_weight, self.tail_vectors])\n            head_bias = self.tail_bias if head_bias is None else torch.cat([head_bias, self.tail_bias])\n\n        # Perform the softmax calculation for the word vectors in the head for all splits\n        # We need to guard against empty splits as torch.cat does not like random lists\n        combo = torch.cat([split_hiddens[i] for i in range(self.nsplits) if len(split_hiddens[i])])\n        ###\n        all_head_res = torch.nn.functional.linear(combo, head_weight, bias=head_bias)\n        softmaxed_all_head_res = torch.nn.functional.log_softmax(all_head_res, dim=-1)\n        if self.verbose or verbose:\n            self.stats[0].append(combo.size()[0] * head_weight.size()[0])\n\n        running_offset = 0\n        for idx in range(self.nsplits):\n            # If there are no targets for this split, continue\n            if len(split_targets[idx]) == 0: continue\n\n            # For those targets in the head (idx == 0) we only need to return their loss\n            if idx == 0:\n                softmaxed_head_res = softmaxed_all_head_res[running_offset:running_offset + len(split_hiddens[idx])]\n                entropy = -torch.gather(softmaxed_head_res, dim=1, index=split_targets[idx].view(-1, 1))\n            # If the target is in one of the splits, the probability is the p(tombstone) * p(word within tombstone)\n            else:\n                softmaxed_head_res = softmaxed_all_head_res[running_offset:running_offset + len(split_hiddens[idx])]\n\n                if self.verbose or verbose:\n                    start, end = self.splits[idx], self.splits[idx + 1]\n                    tail_weight = weight[start:end]\n                    self.stats[idx].append(split_hiddens[idx].size()[0] * tail_weight.size()[0])\n\n                # Calculate the softmax for the words in the tombstone\n                tail_res = self.logprob(weight, bias, split_hiddens[idx], splits=[idx], softmaxed_head_res=softmaxed_head_res)\n\n                # Then we calculate p(tombstone) * p(word in tombstone)\n                # Adding is equivalent to multiplication in log space\n                head_entropy = softmaxed_head_res[:, -idx]\n                # All indices are shifted - if the first split handles [0,...,499] then the 500th in the second split will be 0 indexed\n                indices = (split_targets[idx] - self.splits[idx]).view(-1, 1)\n                # Warning: if you don't squeeze, you get an N x 1 return, which acts oddly with broadcasting\n                tail_entropy = torch.gather(torch.nn.functional.log_softmax(tail_res, dim=-1), dim=1, index=indices).squeeze()\n                entropy = -(head_entropy + tail_entropy)\n            ###\n            running_offset += len(split_hiddens[idx])\n            total_loss = entropy.float().sum() if total_loss is None else total_loss + entropy.float().sum()\n\n        return (total_loss / len(targets)).type_as(weight)\n\n\nif __name__ == '__main__':\n    np.random.seed(42)\n    torch.manual_seed(42)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(42)\n\n    V = 8\n    H = 10\n    N = 100\n    E = 10\n\n    embed = torch.nn.Embedding(V, H)\n    crit = SplitCrossEntropyLoss(hidden_size=H, splits=[V // 2])\n    bias = torch.nn.Parameter(torch.ones(V))\n    optimizer = torch.optim.SGD(list(embed.parameters()) + list(crit.parameters()), lr=1)\n\n    for _ in range(E):\n        prev = torch.autograd.Variable((torch.rand(N, 1) * 0.999 * V).int().long())\n        x = torch.autograd.Variable((torch.rand(N, 1) * 0.999 * V).int().long())\n        y = embed(prev).squeeze()\n        c = crit(embed.weight, bias, y, x.view(N))\n        print('Crit', c.exp().data[0])\n\n        logprobs = crit.logprob(embed.weight, bias, y[:2]).exp()\n        print(logprobs)\n        print(logprobs.sum(dim=1))\n\n        optimizer.zero_grad()\n        c.backward()\n        optimizer.step()\n"""
utils.py,1,"b'import torch\n\n\ndef repackage_hidden(h):\n    """"""Wraps hidden states in new Tensors,\n    to detach them from their history.""""""\n    if isinstance(h, torch.Tensor):\n        return h.detach()\n    else:\n        return tuple(repackage_hidden(v) for v in h)\n\n\ndef batchify(data, bsz, args):\n    # Work out how cleanly we can divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn\'t cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1).t().contiguous()\n    if args.cuda:\n        data = data.cuda()\n    return data\n\n\ndef get_batch(source, i, args, seq_len=None, evaluation=False):\n    seq_len = min(seq_len if seq_len else args.bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].view(-1)\n    return data, target\n'"
weight_drop.py,9,"b""import torch\nfrom torch.nn import Parameter\nfrom functools import wraps\n\nclass WeightDrop(torch.nn.Module):\n    def __init__(self, module, weights, dropout=0, variational=False):\n        super(WeightDrop, self).__init__()\n        self.module = module\n        self.weights = weights\n        self.dropout = dropout\n        self.variational = variational\n        self._setup()\n\n    def widget_demagnetizer_y2k_edition(*args, **kwargs):\n        # We need to replace flatten_parameters with a nothing function\n        # It must be a function rather than a lambda as otherwise pickling explodes\n        # We can't write boring code though, so ... WIDGET DEMAGNETIZER Y2K EDITION!\n        # (\xe2\x95\xaf\xc2\xb0\xe2\x96\xa1\xc2\xb0\xef\xbc\x89\xe2\x95\xaf\xef\xb8\xb5 \xe2\x94\xbb\xe2\x94\x81\xe2\x94\xbb\n        return\n\n    def _setup(self):\n        # Terrible temporary solution to an issue regarding compacting weights re: CUDNN RNN\n        if issubclass(type(self.module), torch.nn.RNNBase):\n            self.module.flatten_parameters = self.widget_demagnetizer_y2k_edition\n\n        for name_w in self.weights:\n            print('Applying weight drop of {} to {}'.format(self.dropout, name_w))\n            w = getattr(self.module, name_w)\n            del self.module._parameters[name_w]\n            self.module.register_parameter(name_w + '_raw', Parameter(w.data))\n\n    def _setweights(self):\n        for name_w in self.weights:\n            raw_w = getattr(self.module, name_w + '_raw')\n            w = None\n            if self.variational:\n                mask = torch.autograd.Variable(torch.ones(raw_w.size(0), 1))\n                if raw_w.is_cuda: mask = mask.cuda()\n                mask = torch.nn.functional.dropout(mask, p=self.dropout, training=True)\n                w = mask.expand_as(raw_w) * raw_w\n            else:\n                w = torch.nn.functional.dropout(raw_w, p=self.dropout, training=self.training)\n            setattr(self.module, name_w, w)\n\n    def forward(self, *args):\n        self._setweights()\n        return self.module.forward(*args)\n\nif __name__ == '__main__':\n    import torch\n    from weight_drop import WeightDrop\n\n    # Input is (seq, batch, input)\n    x = torch.autograd.Variable(torch.randn(2, 1, 10)).cuda()\n    h0 = None\n\n    ###\n\n    print('Testing WeightDrop')\n    print('=-=-=-=-=-=-=-=-=-=')\n\n    ###\n\n    print('Testing WeightDrop with Linear')\n\n    lin = WeightDrop(torch.nn.Linear(10, 10), ['weight'], dropout=0.9)\n    lin.cuda()\n    run1 = [x.sum() for x in lin(x).data]\n    run2 = [x.sum() for x in lin(x).data]\n\n    print('All items should be different')\n    print('Run 1:', run1)\n    print('Run 2:', run2)\n\n    assert run1[0] != run2[0]\n    assert run1[1] != run2[1]\n\n    print('---')\n\n    ###\n\n    print('Testing WeightDrop with LSTM')\n\n    wdrnn = WeightDrop(torch.nn.LSTM(10, 10), ['weight_hh_l0'], dropout=0.9)\n    wdrnn.cuda()\n\n    run1 = [x.sum() for x in wdrnn(x, h0)[0].data]\n    run2 = [x.sum() for x in wdrnn(x, h0)[0].data]\n\n    print('First timesteps should be equal, all others should differ')\n    print('Run 1:', run1)\n    print('Run 2:', run2)\n\n    # First time step, not influenced by hidden to hidden weights, should be equal\n    assert run1[0] == run2[0]\n    # Second step should not\n    assert run1[1] != run2[1]\n\n    print('---')\n"""
data/enwik8/prep_enwik8.py,0,"b""#!/usr/bin/env python\n# coding=utf-8\n\nimport os\nimport sys\nimport zipfile\n\nif os.path.exists('train.txt'):\n    print('Tokenized enwik8 already exists - skipping processing')\n    sys.exit()\n\ndata = zipfile.ZipFile('enwik8.zip').read('enwik8')\n\nprint('Length of enwik8: {}'.format(len(data)))\n\nnum_test_chars = 5000000\n\ntrain_data = data[: -2 * num_test_chars]\nvalid_data = data[-2 * num_test_chars: -num_test_chars]\ntest_data = data[-num_test_chars:]\n\nfor fn, part in [('train.txt', train_data), ('valid.txt', valid_data), ('test.txt', test_data)]:\n    print('{} will have {} bytes'.format(fn, len(part)))\n    print('- Tokenizing...')\n    part_str = ' '.join([str(c) if c != ord('\\n') else '\\n' for c in part])\n    print('- Writing...')\n    f = open(fn, 'w').write(part_str)\n    f = open(fn + '.raw', 'wb').write(part)\n"""
