file_path,api_count,code
__init__.py,0,b''
dataset.py,0,"b""import json\nfrom collections import defaultdict\nimport numpy as np\nfrom tqdm import tqdm\nfrom stanza.nlp.corenlp import CoreNLPClient\n\n\nclient = None\n\n\ndef annotate(sent):\n    global client\n    if client is None:\n        client = CoreNLPClient(default_annotators='ssplit,tokenize'.split(','))\n    words = []\n    for sent in client.annotate(sent).sentences:\n        for tok in sent:\n            words.append(tok.word)\n    return words\n\n\nclass Turn:\n\n    def __init__(self, turn_id, transcript, turn_label, belief_state, system_acts, system_transcript, num=None):\n        self.id = turn_id\n        self.transcript = transcript\n        self.turn_label = turn_label\n        self.belief_state = belief_state\n        self.system_acts = system_acts\n        self.system_transcript = system_transcript\n        self.num = num or {}\n\n    def to_dict(self):\n        return {'turn_id': self.id, 'transcript': self.transcript, 'turn_label': self.turn_label, 'belief_state': self.belief_state, 'system_acts': self.system_acts, 'system_transcript': self.system_transcript, 'num': self.num}\n\n    @classmethod\n    def from_dict(cls, d):\n        return cls(**d)\n\n    @classmethod\n    def annotate_raw(cls, raw):\n        system_acts = []\n        for a in raw['system_acts']:\n            if isinstance(a, list):\n                s, v = a\n                system_acts.append(['inform'] + s.split() + ['='] + v.split())\n            else:\n                system_acts.append(['request'] + a.split())\n        # NOTE: fix inconsistencies in data label\n        fix = {'centre': 'center', 'areas': 'area', 'phone number': 'number'}\n        return cls(\n            turn_id=raw['turn_idx'],\n            transcript=annotate(raw['transcript']),\n            system_acts=system_acts,\n            turn_label=[[fix.get(s.strip(), s.strip()), fix.get(v.strip(), v.strip())] for s, v in raw['turn_label']],\n            belief_state=raw['belief_state'],\n            system_transcript=raw['system_transcript'],\n        )\n\n    def numericalize_(self, vocab):\n        self.num['transcript'] = vocab.word2index(['<sos>'] + [w.lower() for w in self.transcript + ['<eos>']], train=True)\n        self.num['system_acts'] = [vocab.word2index(['<sos>'] + [w.lower() for w in a] + ['<eos>'], train=True) for a in self.system_acts + [['<sentinel>']]]\n\n\nclass Dialogue:\n\n    def __init__(self, dialogue_id, turns):\n        self.id = dialogue_id\n        self.turns = turns\n\n    def __len__(self):\n        return len(self.turns)\n\n    def to_dict(self):\n        return {'dialogue_id': self.id, 'turns': [t.to_dict() for t in self.turns]}\n\n    @classmethod\n    def from_dict(cls, d):\n        return cls(d['dialogue_id'], [Turn.from_dict(t) for t in d['turns']])\n\n    @classmethod\n    def annotate_raw(cls, raw):\n        return cls(raw['dialogue_idx'], [Turn.annotate_raw(t) for t in raw['dialogue']])\n\n\nclass Dataset:\n\n    def __init__(self, dialogues):\n        self.dialogues = dialogues\n\n    def __len__(self):\n        return len(self.dialogues)\n\n    def iter_turns(self):\n        for d in self.dialogues:\n            for t in d.turns:\n                yield t\n\n    def to_dict(self):\n        return {'dialogues': [d.to_dict() for d in self.dialogues]}\n\n    @classmethod\n    def from_dict(cls, d):\n        return cls([Dialogue.from_dict(dd) for dd in d['dialogues']])\n\n    @classmethod\n    def annotate_raw(cls, fname):\n        with open(fname) as f:\n            data = json.load(f)\n            return cls([Dialogue.annotate_raw(d) for d in tqdm(data)])\n\n    def numericalize_(self, vocab):\n        for t in self.iter_turns():\n            t.numericalize_(vocab)\n\n    def extract_ontology(self):\n        slots = set()\n        values = defaultdict(set)\n        for t in self.iter_turns():\n            for s, v in t.turn_label:\n                slots.add(s.lower())\n                values[s].add(v.lower())\n        return Ontology(sorted(list(slots)), {k: sorted(list(v)) for k, v in values.items()})\n\n    def batch(self, batch_size, shuffle=False):\n        turns = list(self.iter_turns())\n        if shuffle:\n            np.random.shuffle(turns)\n        for i in tqdm(range(0, len(turns), batch_size)):\n            yield turns[i:i+batch_size]\n\n    def evaluate_preds(self, preds):\n        request = []\n        inform = []\n        joint_goal = []\n        fix = {'centre': 'center', 'areas': 'area', 'phone number': 'number'}\n        i = 0\n        for d in self.dialogues:\n            pred_state = {}\n            for t in d.turns:\n                gold_request = set([(s, v) for s, v in t.turn_label if s == 'request'])\n                gold_inform = set([(s, v) for s, v in t.turn_label if s != 'request'])\n                pred_request = set([(s, v) for s, v in preds[i] if s == 'request'])\n                pred_inform = set([(s, v) for s, v in preds[i] if s != 'request'])\n                request.append(gold_request == pred_request)\n                inform.append(gold_inform == pred_inform)\n\n                gold_recovered = set()\n                pred_recovered = set()\n                for s, v in pred_inform:\n                    pred_state[s] = v\n                for b in t.belief_state:\n                    for s, v in b['slots']:\n                        if b['act'] != 'request':\n                            gold_recovered.add((b['act'], fix.get(s.strip(), s.strip()), fix.get(v.strip(), v.strip())))\n                for s, v in pred_state.items():\n                    pred_recovered.add(('inform', s, v))\n                joint_goal.append(gold_recovered == pred_recovered)\n                i += 1\n        return {'turn_inform': np.mean(inform), 'turn_request': np.mean(request), 'joint_goal': np.mean(joint_goal)}\n\n    def record_preds(self, preds, to_file):\n        data = self.to_dict()\n        i = 0\n        for d in data['dialogues']:\n            for t in d['turns']:\n                t['pred'] = sorted(list(preds[i]))\n                i += 1\n        with open(to_file, 'wt') as f:\n            json.dump(data, f)\n\n\nclass Ontology:\n\n    def __init__(self, slots=None, values=None, num=None):\n        self.slots = slots or []\n        self.values = values or {}\n        self.num = num or {}\n\n    def __add__(self, another):\n        new_slots = sorted(list(set(self.slots + another.slots)))\n        new_values = {s: sorted(list(set(self.values.get(s, []) + another.values.get(s, [])))) for s in new_slots}\n        return Ontology(new_slots, new_values)\n\n    def __radd__(self, another):\n        return self if another == 0 else self.__add__(another)\n\n    def to_dict(self):\n        return {'slots': self.slots, 'values': self.values, 'num': self.num}\n\n    def numericalize_(self, vocab):\n        self.num = {}\n        for s, vs in self.values.items():\n            self.num[s] = [vocab.word2index(annotate('{} = {}'.format(s, v)) + ['<eos>'], train=True) for v in vs]\n\n    @classmethod\n    def from_dict(cls, d):\n        return cls(**d)\n"""
evaluate.py,0,"b""import os\nimport json\nimport logging\nfrom argparse import ArgumentParser, Namespace\nfrom pprint import pprint\nfrom utils import load_dataset, load_model\n\n\nif __name__ == '__main__':\n    parser = ArgumentParser()\n    parser.add_argument('dsave', help='save location of model')\n    parser.add_argument('--split', help='split to evaluate on', default='dev')\n    parser.add_argument('--gpu', type=int, help='gpu to use', default=None)\n    parser.add_argument('--fout', help='optional save file to store the predictions')\n    args = parser.parse_args()\n\n    logging.basicConfig(level=logging.INFO)\n\n    with open(os.path.join(args.dsave, 'config.json')) as f:\n        args_save = Namespace(**json.load(f))\n        args_save.gpu = args.gpu\n    pprint(args_save)\n\n    dataset, ontology, vocab, Eword = load_dataset()\n\n    model = load_model(args_save.model, args_save, ontology, vocab)\n    model.load_best_save(directory=args.dsave)\n    if args.gpu is not None:\n        model.cuda(args.gpu)\n\n    logging.info('Making predictions for {} dialogues and {} turns'.format(len(dataset[args.split]), len(list(dataset[args.split].iter_turns()))))\n    preds = model.run_pred(dataset[args.split], args_save)\n    pprint(dataset[args.split].evaluate_preds(preds))\n\n    if args.fout:\n        with open(args.fout, 'wt') as f:\n            # predictions is a list of sets, need to convert to list of lists to make it JSON serializable\n            json.dump([list(p) for p in preds], f, indent=2)\n"""
preprocess_data.py,0,"b""#!/usr/bin/env python\nimport os\nimport json\nimport logging\nimport requests\nfrom tqdm import tqdm\nfrom vocab import Vocab\nfrom embeddings import GloveEmbedding, KazumaCharEmbedding\nfrom dataset import Dataset, Ontology\n\n\nroot_dir = os.path.dirname(__file__)\ndata_dir = os.path.join(root_dir, 'data', 'woz')\n\n\ndraw = os.path.join(data_dir, 'raw')\ndann = os.path.join(data_dir, 'ann')\n\nsplits = ['dev', 'train', 'test']\n\n\ndef download(url, to_file):\n    r = requests.get(url, stream=True)\n    with open(to_file, 'wb') as f:\n        for chunk in r.iter_content(chunk_size=1024):\n            if chunk:\n                f.write(chunk)\n\n\ndef missing_files(d, files):\n    return not all([os.path.isfile(os.path.join(d, '{}.json'.format(s))) for s in files])\n\n\nif __name__ == '__main__':\n    if missing_files(draw, splits):\n        if not os.path.isdir(draw):\n            os.makedirs(draw)\n        download('https://github.com/nmrksic/neural-belief-tracker/raw/master/data/woz/woz_train_en.json', os.path.join(draw, 'train.json'))\n        download('https://github.com/nmrksic/neural-belief-tracker/raw/master/data/woz/woz_validate_en.json', os.path.join(draw, 'dev.json'))\n        download('https://github.com/nmrksic/neural-belief-tracker/raw/master/data/woz/woz_test_en.json', os.path.join(draw, 'test.json'))\n\n    if missing_files(dann, files=splits + ['ontology', 'vocab', 'emb']):\n        if not os.path.isdir(dann):\n            os.makedirs(dann)\n        dataset = {}\n        ontology = Ontology()\n        vocab = Vocab()\n        vocab.word2index(['<sos>', '<eos>'], train=True)\n        for s in splits:\n            fname = '{}.json'.format(s)\n            logging.warn('Annotating {}'.format(s))\n            dataset[s] = Dataset.annotate_raw(os.path.join(draw, fname))\n            dataset[s].numericalize_(vocab)\n            ontology = ontology + dataset[s].extract_ontology()\n            with open(os.path.join(dann, fname), 'wt') as f:\n                json.dump(dataset[s].to_dict(), f)\n        ontology.numericalize_(vocab)\n        with open(os.path.join(dann, 'ontology.json'), 'wt') as f:\n            json.dump(ontology.to_dict(), f)\n        with open(os.path.join(dann, 'vocab.json'), 'wt') as f:\n            json.dump(vocab.to_dict(), f)\n\n        logging.warn('Computing word embeddings')\n        embeddings = [GloveEmbedding(), KazumaCharEmbedding()]\n        E = []\n        for w in tqdm(vocab._index2word):\n            e = []\n            for emb in embeddings:\n                e += emb.emb(w, default='zero')\n            E.append(e)\n        with open(os.path.join(dann, 'emb.json'), 'wt') as f:\n            json.dump(E, f)\n"""
train.py,1,"b""#!/usr/bin/env python3\nfrom argparse import ArgumentDefaultsHelpFormatter, ArgumentParser\nfrom utils import load_dataset, get_models, load_model\nimport os\nimport logging\nimport numpy as np\nfrom pprint import pprint\nimport torch\nfrom random import seed\n\n\ndef run(args):\n    pprint(args)\n    logging.basicConfig(level=logging.INFO)\n\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    seed(args.seed)\n\n    dataset, ontology, vocab, Eword = load_dataset()\n\n    model = load_model(args.model, args, ontology, vocab)\n    model.save_config()\n    model.load_emb(Eword)\n\n    model = model.to(model.device)\n    if not args.test:\n        logging.info('Starting train')\n        model.run_train(dataset['train'], dataset['dev'], args)\n    if args.resume:\n        model.load_best_save(directory=args.resume)\n    else:\n        model.load_best_save(directory=args.dout)\n    model = model.to(model.device)\n    logging.info('Running dev evaluation')\n    dev_out = model.run_eval(dataset['dev'], args)\n    pprint(dev_out)\n\n\ndef get_args():\n    parser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\n    parser.add_argument('--dexp', help='root experiment folder', default='exp')\n    parser.add_argument('--model', help='which model to use', default='glad', choices=get_models())\n    parser.add_argument('--epoch', help='max epoch to run for', default=50, type=int)\n    parser.add_argument('--demb', help='word embedding size', default=400, type=int)\n    parser.add_argument('--dhid', help='hidden state size', default=200, type=int)\n    parser.add_argument('--batch_size', help='batch size', default=50, type=int)\n    parser.add_argument('--lr', help='learning rate', default=1e-3, type=float)\n    parser.add_argument('--stop', help='slot to early stop on', default='joint_goal')\n    parser.add_argument('--resume', help='save directory to resume from')\n    parser.add_argument('-n', '--nick', help='nickname for model', default='default')\n    parser.add_argument('--seed', default=42, help='random seed', type=int)\n    parser.add_argument('--test', action='store_true', help='run in evaluation only mode')\n    parser.add_argument('--gpu', type=int, help='which GPU to use')\n    parser.add_argument('--dropout', nargs='*', help='dropout rates', default=['emb=0.2', 'local=0.2', 'global=0.2'])\n    args = parser.parse_args()\n    args.dout = os.path.join(args.dexp, args.model, args.nick)\n    args.dropout = {d.split('=')[0]: float(d.split('=')[1]) for d in args.dropout}\n    if not os.path.isdir(args.dout):\n        os.makedirs(args.dout)\n    return args\n\n\nif __name__ == '__main__':\n    args = get_args()\n    run(args)\n"""
utils.py,0,"b""import json\nimport logging\nimport os\nfrom pprint import pformat\nfrom importlib import import_module\nfrom vocab import Vocab\nfrom dataset import Dataset, Ontology\nfrom preprocess_data import dann\n\n\ndef load_dataset(splits=('train', 'dev', 'test')):\n    with open(os.path.join(dann, 'ontology.json')) as f:\n        ontology = Ontology.from_dict(json.load(f))\n    with open(os.path.join(dann, 'vocab.json')) as f:\n        vocab = Vocab.from_dict(json.load(f))\n    with open(os.path.join(dann, 'emb.json')) as f:\n        E = json.load(f)\n    dataset = {}\n    for split in splits:\n        with open(os.path.join(dann, '{}.json'.format(split))) as f:\n            logging.warn('loading split {}'.format(split))\n            dataset[split] = Dataset.from_dict(json.load(f))\n\n    logging.info('dataset sizes: {}'.format(pformat({k: len(v) for k, v in dataset.items()})))\n    return dataset, ontology, vocab, E\n\n\ndef get_models():\n    return [m.replace('.py', '') for m in os.listdir('models') if not m.startswith('_') and m != 'model']\n\n\ndef load_model(model, *args, **kwargs):\n    Model = import_module('models.{}'.format(model)).Model\n    model = Model(*args, **kwargs)\n    logging.info('loaded model {}'.format(Model))\n    return model\n"""
models/__init__.py,0,b''
models/glad.py,13,"b'import torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.nn import functional as F\nimport numpy as np\nimport logging\nimport os\nimport re\nimport json\nfrom collections import defaultdict\nfrom pprint import pformat\n\n\ndef pad(seqs, emb, device, pad=0):\n    lens = [len(s) for s in seqs]\n    max_len = max(lens)\n    padded = torch.LongTensor([s + (max_len-l) * [pad] for s, l in zip(seqs, lens)])\n    return emb(padded.to(device)), lens\n\n\ndef run_rnn(rnn, inputs, lens):\n    # sort by lens\n    order = np.argsort(lens)[::-1].tolist()\n    reindexed = inputs.index_select(0, inputs.data.new(order).long())\n    reindexed_lens = [lens[i] for i in order]\n    packed = nn.utils.rnn.pack_padded_sequence(reindexed, reindexed_lens, batch_first=True)\n    outputs, _ = rnn(packed)\n    padded, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True, padding_value=0.)\n    reverse_order = np.argsort(order).tolist()\n    recovered = padded.index_select(0, inputs.data.new(reverse_order).long())\n    # reindexed_lens = [lens[i] for i in order]\n    # recovered_lens = [reindexed_lens[i] for i in reverse_order]\n    # assert recovered_lens == lens\n    return recovered\n\n\ndef attend(seq, cond, lens):\n    """"""\n    attend over the sequences `seq` using the condition `cond`.\n    """"""\n    scores = cond.unsqueeze(1).expand_as(seq).mul(seq).sum(2)\n    max_len = max(lens)\n    for i, l in enumerate(lens):\n        if l < max_len:\n            scores.data[i, l:] = -np.inf\n    scores = F.softmax(scores, dim=1)\n    context = scores.unsqueeze(2).expand_as(seq).mul(seq).sum(1)\n    return context, scores\n\n\nclass FixedEmbedding(nn.Embedding):\n    """"""\n    this is the same as `nn.Embedding` but detaches the result from the graph and has dropout after lookup.\n    """"""\n\n    def __init__(self, *args, dropout=0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.dropout = dropout\n\n    def forward(self, *args, **kwargs):\n        out = super().forward(*args, **kwargs)\n        out.detach_()\n        return F.dropout(out, self.dropout, self.training)\n\n\nclass SelfAttention(nn.Module):\n    """"""\n    scores each element of the sequence with a linear layer and uses the normalized scores to compute a context over the sequence.\n    """"""\n\n    def __init__(self, d_hid, dropout=0.):\n        super().__init__()\n        self.scorer = nn.Linear(d_hid, 1)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, inp, lens):\n        batch_size, seq_len, d_feat = inp.size()\n        inp = self.dropout(inp)\n        scores = self.scorer(inp.contiguous().view(-1, d_feat)).view(batch_size, seq_len)\n        max_len = max(lens)\n        for i, l in enumerate(lens):\n            if l < max_len:\n                scores.data[i, l:] = -np.inf\n        scores = F.softmax(scores, dim=1)\n        context = scores.unsqueeze(2).expand_as(inp).mul(inp).sum(1)\n        return context\n\n\nclass GLADEncoder(nn.Module):\n    """"""\n    the GLAD encoder described in https://arxiv.org/abs/1805.09655.\n    """"""\n\n    def __init__(self, din, dhid, slots, dropout=None):\n        super().__init__()\n        self.dropout = dropout or {}\n        self.global_rnn = nn.LSTM(din, dhid, bidirectional=True, batch_first=True)\n        self.global_selfattn = SelfAttention(2 * dhid, dropout=self.dropout.get(\'selfattn\', 0.))\n        for s in slots:\n            setattr(self, \'{}_rnn\'.format(s), nn.LSTM(din, dhid, bidirectional=True, batch_first=True, dropout=self.dropout.get(\'rnn\', 0.)))\n            setattr(self, \'{}_selfattn\'.format(s), SelfAttention(2*dhid, dropout=self.dropout.get(\'selfattn\', 0.)))\n        self.slots = slots\n        self.beta_raw = nn.Parameter(torch.Tensor(len(slots)))\n        nn.init.uniform_(self.beta_raw, -0.01, 0.01)\n\n    def beta(self, slot):\n        return F.sigmoid(self.beta_raw[self.slots.index(slot)])\n\n    def forward(self, x, x_len, slot, default_dropout=0.2):\n        local_rnn = getattr(self, \'{}_rnn\'.format(slot))\n        local_selfattn = getattr(self, \'{}_selfattn\'.format(slot))\n        beta = self.beta(slot)\n        local_h = run_rnn(local_rnn, x, x_len)\n        global_h = run_rnn(self.global_rnn, x, x_len)\n        h = F.dropout(local_h, self.dropout.get(\'local\', default_dropout), self.training) * beta + F.dropout(global_h, self.dropout.get(\'global\', default_dropout), self.training) * (1-beta)\n        c = F.dropout(local_selfattn(h, x_len), self.dropout.get(\'local\', default_dropout), self.training) * beta + F.dropout(self.global_selfattn(h, x_len), self.dropout.get(\'global\', default_dropout), self.training) * (1-beta)\n        return h, c\n\n\nclass Model(nn.Module):\n    """"""\n    the GLAD model described in https://arxiv.org/abs/1805.09655.\n    """"""\n\n    def __init__(self, args, ontology, vocab):\n        super().__init__()\n        self.optimizer = None\n        self.args = args\n        self.vocab = vocab\n        self.ontology = ontology\n        self.emb_fixed = FixedEmbedding(len(vocab), args.demb, dropout=args.dropout.get(\'emb\', 0.2))\n\n        self.utt_encoder = GLADEncoder(args.demb, args.dhid, self.ontology.slots, dropout=args.dropout)\n        self.act_encoder = GLADEncoder(args.demb, args.dhid, self.ontology.slots, dropout=args.dropout)\n        self.ont_encoder = GLADEncoder(args.demb, args.dhid, self.ontology.slots, dropout=args.dropout)\n        self.utt_scorer = nn.Linear(2 * args.dhid, 1)\n        self.score_weight = nn.Parameter(torch.Tensor([0.5]))\n\n    @property\n    def device(self):\n        if self.args.gpu is not None and torch.cuda.is_available():\n            return torch.device(\'cuda\')\n        else:\n            return torch.device(\'cpu\')\n\n    def set_optimizer(self):\n        self.optimizer = optim.Adam(self.parameters(), lr=self.args.lr)\n\n    def load_emb(self, Eword):\n        new = self.emb_fixed.weight.data.new\n        self.emb_fixed.weight.data.copy_(new(Eword))\n\n    def forward(self, batch):\n        # convert to variables and look up embeddings\n        eos = self.vocab.word2index(\'<eos>\')\n        utterance, utterance_len = pad([e.num[\'transcript\'] for e in batch], self.emb_fixed, self.device, pad=eos)\n        acts = [pad(e.num[\'system_acts\'], self.emb_fixed, self.device, pad=eos) for e in batch]\n        ontology = {s: pad(v, self.emb_fixed, self.device, pad=eos) for s, v in self.ontology.num.items()}\n\n        ys = {}\n        for s in self.ontology.slots:\n            # for each slot, compute the scores for each value\n            H_utt, c_utt = self.utt_encoder(utterance, utterance_len, slot=s)\n            _, C_acts = list(zip(*[self.act_encoder(a, a_len, slot=s) for a, a_len in acts]))\n            _, C_vals = self.ont_encoder(ontology[s][0], ontology[s][1], slot=s)\n\n            # compute the utterance score\n            y_utts = []\n            q_utts = []\n            for c_val in C_vals:\n                q_utt, _ = attend(H_utt, c_val.unsqueeze(0).expand(len(batch), *c_val.size()), lens=utterance_len)\n                q_utts.append(q_utt)\n            y_utts = self.utt_scorer(torch.stack(q_utts, dim=1)).squeeze(2)\n\n            # compute the previous action score\n            q_acts = []\n            for i, C_act in enumerate(C_acts):\n                q_act, _ = attend(C_act.unsqueeze(0), c_utt[i].unsqueeze(0), lens=[C_act.size(0)])\n                q_acts.append(q_act)\n            y_acts = torch.cat(q_acts, dim=0).mm(C_vals.transpose(0, 1))\n\n            # combine the scores\n            ys[s] = F.sigmoid(y_utts + self.score_weight * y_acts)\n\n        if self.training:\n            # create label variable and compute loss\n            labels = {s: [len(self.ontology.values[s]) * [0] for i in range(len(batch))] for s in self.ontology.slots}\n            for i, e in enumerate(batch):\n                for s, v in e.turn_label:\n                    labels[s][i][self.ontology.values[s].index(v)] = 1\n            labels = {s: torch.Tensor(m).to(self.device) for s, m in labels.items()}\n\n            loss = 0\n            for s in self.ontology.slots:\n                loss += F.binary_cross_entropy(ys[s], labels[s])\n        else:\n            loss = torch.Tensor([0]).to(self.device)\n        return loss, {s: v.data.tolist() for s, v in ys.items()}\n\n    def get_train_logger(self):\n        logger = logging.getLogger(\'train-{}\'.format(self.__class__.__name__))\n        formatter = logging.Formatter(\'%(asctime)s [%(threadName)-12.12s] [%(levelname)-5.5s]  %(message)s\')\n        file_handler = logging.FileHandler(os.path.join(self.args.dout, \'train.log\'))\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n        return logger\n\n    def run_train(self, train, dev, args):\n        track = defaultdict(list)\n        iteration = 0\n        best = {}\n        logger = self.get_train_logger()\n        if self.optimizer is None:\n            self.set_optimizer()\n\n        for epoch in range(args.epoch):\n            logger.info(\'starting epoch {}\'.format(epoch))\n\n            # train and update parameters\n            self.train()\n            for batch in train.batch(batch_size=args.batch_size, shuffle=True):\n                iteration += 1\n                self.zero_grad()\n                loss, scores = self.forward(batch)\n                loss.backward()\n                self.optimizer.step()\n                track[\'loss\'].append(loss.item())\n\n            # evalute on train and dev\n            summary = {\'iteration\': iteration, \'epoch\': epoch}\n            for k, v in track.items():\n                summary[k] = sum(v) / len(v)\n            summary.update({\'eval_train_{}\'.format(k): v for k, v in self.run_eval(train, args).items()})\n            summary.update({\'eval_dev_{}\'.format(k): v for k, v in self.run_eval(dev, args).items()})\n\n            # do early stopping saves\n            stop_key = \'eval_dev_{}\'.format(args.stop)\n            train_key = \'eval_train_{}\'.format(args.stop)\n            if best.get(stop_key, 0) <= summary[stop_key]:\n                best_dev = \'{:f}\'.format(summary[stop_key])\n                best_train = \'{:f}\'.format(summary[train_key])\n                best.update(summary)\n                self.save(\n                    best,\n                    identifier=\'epoch={epoch},iter={iteration},train_{key}={train},dev_{key}={dev}\'.format(\n                        epoch=epoch, iteration=iteration, train=best_train, dev=best_dev, key=args.stop,\n                    )\n                )\n                self.prune_saves()\n                dev.record_preds(\n                    preds=self.run_pred(dev, self.args),\n                    to_file=os.path.join(self.args.dout, \'dev.pred.json\'),\n                )\n            summary.update({\'best_{}\'.format(k): v for k, v in best.items()})\n            logger.info(pformat(summary))\n            track.clear()\n\n    def extract_predictions(self, scores, threshold=0.5):\n        batch_size = len(list(scores.values())[0])\n        predictions = [set() for i in range(batch_size)]\n        for s in self.ontology.slots:\n            for i, p in enumerate(scores[s]):\n                triggered = [(s, v, p_v) for v, p_v in zip(self.ontology.values[s], p) if p_v > threshold]\n                if s == \'request\':\n                    # we can have multiple requests predictions\n                    predictions[i] |= set([(s, v) for s, v, p_v in triggered])\n                elif triggered:\n                    # only extract the top inform prediction\n                    sort = sorted(triggered, key=lambda tup: tup[-1], reverse=True)\n                    predictions[i].add((sort[0][0], sort[0][1]))\n        return predictions\n\n    def run_pred(self, dev, args):\n        self.eval()\n        predictions = []\n        for batch in dev.batch(batch_size=args.batch_size):\n            loss, scores = self.forward(batch)\n            predictions += self.extract_predictions(scores)\n        return predictions\n\n    def run_eval(self, dev, args):\n        predictions = self.run_pred(dev, args)\n        return dev.evaluate_preds(predictions)\n\n    def save_config(self):\n        fname = \'{}/config.json\'.format(self.args.dout)\n        with open(fname, \'wt\') as f:\n            logging.info(\'saving config to {}\'.format(fname))\n            json.dump(vars(self.args), f, indent=2)\n\n    @classmethod\n    def load_config(cls, fname, ontology, **kwargs):\n        with open(fname) as f:\n            logging.info(\'loading config from {}\'.format(fname))\n            args = object()\n            for k, v in json.load(f):\n                setattr(args, k, kwargs.get(k, v))\n        return cls(args, ontology)\n\n    def save(self, summary, identifier):\n        fname = \'{}/{}.t7\'.format(self.args.dout, identifier)\n        logging.info(\'saving model to {}\'.format(fname))\n        state = {\n            \'args\': vars(self.args),\n            \'model\': self.state_dict(),\n            \'summary\': summary,\n            \'optimizer\': self.optimizer.state_dict(),\n        }\n        torch.save(state, fname)\n\n    def load(self, fname):\n        logging.info(\'loading model from {}\'.format(fname))\n        state = torch.load(fname)\n        self.load_state_dict(state[\'model\'])\n        self.set_optimizer()\n        self.optimizer.load_state_dict(state[\'optimizer\'])\n\n    def get_saves(self, directory=None):\n        if directory is None:\n            directory = self.args.dout\n        files = [f for f in os.listdir(directory) if f.endswith(\'.t7\')]\n        scores = []\n        for fname in files:\n            re_str = r\'dev_{}=([0-9\\.]+)\'.format(self.args.stop)\n            dev_acc = re.findall(re_str, fname)\n            if dev_acc:\n                score = float(dev_acc[0].strip(\'.\'))\n                scores.append((score, os.path.join(directory, fname)))\n        if not scores:\n            raise Exception(\'No files found!\')\n        scores.sort(key=lambda tup: tup[0], reverse=True)\n        return scores\n\n    def prune_saves(self, n_keep=5):\n        scores_and_files = self.get_saves()\n        if len(scores_and_files) > n_keep:\n            for score, fname in scores_and_files[n_keep:]:\n                os.remove(fname)\n\n    def load_best_save(self, directory):\n        if directory is None:\n            directory = self.args.dout\n\n        scores_and_files = self.get_saves(directory=directory)\n        if scores_and_files:\n            assert scores_and_files, \'no saves exist at {}\'.format(directory)\n            score, fname = scores_and_files[0]\n            self.load(fname)\n'"
