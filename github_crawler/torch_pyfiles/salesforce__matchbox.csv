file_path,api_count,code
setup.py,0,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nfrom setuptools import setup, find_packages\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# Get the long description from the README file\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nsetup(\n    name=\'matchbox\',\n    version=\'0.1.0\',\n\n    description=\'Dispatch-driven autobatching for imperative deep learning\',\n    long_description=long_description,\n\n    # The project\'s main homepage.\n    url=\'https://github.com/salesforce/matchbox\',\n\n    # Author details\n    author=\'James Bradbury\',\n    author_email=\'james.bradbury@salesforce.com\',\n\n    # Choose your license\n    license=\'BSD-3\',\n\n    # See https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        \'Development Status :: 3 - Alpha\',\n\n        # Indicate who your project is intended for\n        \'Intended Audience :: Developers\',\n\n        # Pick your license as you wish (should match ""license"" above)\n        \'License :: OSI Approved :: BSD-3 License\',\n\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        \'Programming Language :: Python :: 3.6\',\n    ],\n\n    # What does your project relate to?\n    keywords=\'pytorch deep_learning nlp batching autobatching\',\n\n    # You can just specify the packages manually here if your project is\n    # simple. Or you can use find_packages().\n    packages=find_packages(exclude=[\'test\', \'examples\']),\n\n    # List run-time dependencies here.  These will be installed by pip when\n    # your project is installed. For an analysis of ""install_requires"" vs pip\'s\n    # requirements files see:\n    # https://packaging.python.org/en/latest/requirements.html\n    install_requires=[\'astor\', \'gast\'],\n\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). You can install these using the following syntax,\n    # for example:\n    # $ pip install -e .[dev,test]\n    extras_require={\n        \'data\': [\'torchtext\', \'six\'],\n        \'test\': [\'pytest\', \'numpy\'],\n    },\n\n    # If there are data files included in your packages that need to be\n    # installed, specify them here.  If using Python 2.6 or less, then these\n    # have to be included in MANIFEST.in as well.\n    package_data={\n    },\n\n    # Although \'package_data\' is the preferred approach, in some case you may\n    # need to place data files outside of your packages. See:\n    # http://docs.python.org/3.4/distutils/setupscript.html#installing-additional-files # noqa\n    # In this case, \'data_file\' will be installed into \'<sys.prefix>/my_data\'\n    data_files=[],\n\n    # To provide executable scripts, use entry points in preference to the\n    # ""scripts"" keyword. Entry points provide cross-platform support and allow\n    # pip to create the appropriate form of executable for the target platform.\n    entry_points={\n    },\n)\n'"
examples/test_transformer.py,4,"b""# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport argparse\nimport random\nfrom collections import namedtuple\n\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\nimport matchbox\nfrom matchbox import MaskedBatch\nfrom matchbox import functional as F\nfrom matchbox.test_utils import mb_test, mb_rand, mb_assert\nfrom matchbox.data import MaskedBatchField\n\nfrom transformer import *\n\ndef test_LayerNorm():\n    mb_test(LayerNorm(2),\n            (4, (True, 3), (False, 2)))\n\ndef test_FeedForward():\n    mb_test(FeedForward(2, 3, 0),\n            (4, (True, 3), (False, 2)))\n\ndef test_ResidualBlock():\n    mb_test(ResidualBlock(FeedForward(2, 3, 0), 2, 0),\n            (4, (True, 3), (False, 2)))\n\ndef test_Attention():\n    mb_test(Attention(2, 0, False),\n            (4, (True, 3), (False, 2)), 0, 0)\n    mb_test(Attention(2, 0, False),\n            (4, (True, 3), (False, 2)), (4, (True, 3), (False, 2)), 1)\n    mb_test(Attention(2, 0, True),\n            (4, (True, 3), (False, 2)), 0, 0)\n\ndef test_MultiHead():\n    mb_test(MultiHead(Attention(6, 0, False), 6, 6, 3),\n            (4, (True, 3), (False, 6)), (4, (True, 3), (False, 6)), 1)\n    mb_test(MultiHead(Attention(6, 0, True), 6, 6, 3),\n            (4, (True, 3), (False, 6)), 0, 0)\n\ndef test_EncoderLayer():\n    args = argparse.Namespace()\n    args.__dict__.update(d_model=6, d_hidden=6, n_heads=3, drop_ratio=0)\n    mb_test(EncoderLayer(args),\n            (4, (True, 3), (False, 6)))\n\ndef test_DecoderLayer():\n    args = argparse.Namespace()\n    args.__dict__.update(d_model=6, d_hidden=6, n_heads=3, drop_ratio=0)\n    mb_test(DecoderLayer(args),\n            (4, (True, 3), (False, 6)), (4, (True, 3), (False, 6)))\n\ndef test_posenc():\n    mb_test(lambda x: x + positional_encodings_like(x),\n            (4, (True, 3), (False, 6)))\n\ndef test_Encoder():\n    args = argparse.Namespace()\n    args.__dict__.update(d_model=6, d_hidden=6, n_heads=3, drop_ratio=0,\n                         n_layers=2)\n    field = MaskedBatchField()\n    field.out = nn.Linear(args.d_model, 5)\n    xs = [Variable(torch.LongTensor(1, random.randint(1, 3)).random_(5))\n          for i in range(4)]\n    xb = MaskedBatch.fromlist(xs, (True,))\n    mb_assert(Encoder(field, args),\n              (xs,), (xb,), 4)\n\ndef test_Transformer():\n    args = argparse.Namespace()\n    B, T, C, V = 4, 3, 6, 5\n    args.__dict__.update(d_model=C, d_hidden=C, n_heads=3, drop_ratio=0,\n                         n_layers=2, length_ratio=1.5)\n    field = MaskedBatchField()\n    field.vocab = list(range(V))\n    xs = [Variable(torch.LongTensor(1, random.randint(1, T)).random_(V))\n          for i in range(B)]\n    ys = [Variable(torch.LongTensor(1, random.randint(2, T)).random_(V))\n          for i in range(B)]\n    xb = MaskedBatch.fromlist(xs, (True,))\n    yb = MaskedBatch.fromlist(ys, (True,))\n    model = Transformer(field, field, args)\n    mb_assert(model,\n              (xs, ys), (xb, yb), B)\n    def loss(x, y):\n        b = namedtuple('_batch', ('src', 'trg'))(x, y)\n        return model.loss(b, reduce=False)\n    mb_assert(loss,\n              (xs, ys), (xb, yb), B)\n"""
examples/transformer.py,8,"b""# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\nfrom torch import nn\nfrom torchtext import data, datasets\n\nimport matchbox\nfrom matchbox import functional as F\nfrom matchbox.data import MaskedBatchField\n\nimport argparse\nimport math\nimport random\nimport time\n\ndef positional_encodings_like(x, t=None):\n    T, D = x.maxsize(-2), x.maxsize(-1)\n    positions = torch.arange(0, T, out=x.new(T)) if t is None else t\n    channels = torch.arange(0, D, 2, out=x.new(D)) / D\n    channels = 1 / (10000 ** channels)\n    encodings = positions.unsqueeze(-1) @ channels.unsqueeze(0)\n    encodings = torch.stack((encodings.sin(), encodings.cos()), -1)\n    encodings = encodings.contiguous().view(*encodings.size()[:-2], -1)\n\n    if encodings.dim() == 2:\n        encodings = encodings.unsqueeze(0).expand(x.maxsize())\n\n    return encodings\n\nclass LayerNorm(nn.Module):\n\n    def __init__(self, d_model, eps=1e-6):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(d_model))\n        self.beta = nn.Parameter(torch.zeros(d_model))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\nclass FeedForward(nn.Module):\n\n    def __init__(self, d_model, d_hidden, drop_ratio):\n        super().__init__()\n        self.linear1 = nn.Linear(d_model, d_hidden)\n        self.linear2 = nn.Linear(d_hidden, d_model)\n        self.dropout = nn.Dropout(drop_ratio)\n\n    def forward(self, x):\n        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n\nclass ResidualBlock(nn.Module):\n\n    def __init__(self, layer, d_model, drop_ratio):\n        super().__init__()\n        self.layer = layer\n        self.dropout = nn.Dropout(drop_ratio)\n        self.norm = LayerNorm(d_model)\n\n    def forward(self, *x):\n        return x[0] + self.dropout(self.norm(self.layer(*x)))\n\nclass Attention(nn.Module):\n\n    def __init__(self, d_key, drop_ratio, causal=False):\n        super().__init__()\n        self.scale = math.sqrt(d_key)\n        self.dropout = nn.Dropout(drop_ratio)\n        self.causal = causal\n\n    def forward(self, query, key, value):\n        dot_products = query @ key.transpose(1, 2)\n\n        if self.causal and query.dim() == 3:\n            dot_products = dot_products.causal_mask(in_dim=2, out_dim=1)\n\n        return self.dropout((dot_products / self.scale).softmax()) @ value\n\nclass MultiHead(nn.Module):\n\n    def __init__(self, attention, d_key, d_value, n_heads):\n        super().__init__()\n        self.attention = attention\n        self.wq = nn.Linear(d_key, d_key)\n        self.wk = nn.Linear(d_key, d_key)\n        self.wv = nn.Linear(d_value, d_value)\n        self.wo = nn.Linear(d_value, d_key)\n        self.n_heads = n_heads\n\n    def forward(self, query, key, value):\n        query, key, value = self.wq(query), self.wk(key), self.wv(value)\n        # B x T x D -> B x T x (D/N) x N -> (B*N) x T x (D/N)\n        query, key, value = (x.split_dim(-1, self.n_heads).join_dims(0, -1)\n                             for x in (query, key, value))\n        outputs = self.attention(query, key, value)\n        # (B*N) x T x (D/N) -> B x N x T x (D/N) -> B x T x D\n        outputs = outputs.split_dim(0, self.n_heads).join_dims(-1, 1)\n        return self.wo(outputs)\n\nclass EncoderLayer(nn.Module):\n\n    def __init__(self, args):\n        super().__init__()\n        self.selfattn = ResidualBlock(\n            MultiHead(Attention(args.d_model, args.drop_ratio),\n                args.d_model, args.d_model, args.n_heads),\n            args.d_model, args.drop_ratio)\n        self.feedforward = ResidualBlock(\n            FeedForward(args.d_model, args.d_hidden, args.drop_ratio),\n            args.d_model, args.drop_ratio)\n\n    def forward(self, x):\n        x = self.selfattn(x, x, x)\n        return self.feedforward(x)\n\nclass DecoderLayer(nn.Module):\n\n    def __init__(self, args):\n        super().__init__()\n        self.selfattn = ResidualBlock(\n            MultiHead(Attention(args.d_model, args.drop_ratio, True),\n                      args.d_model, args.d_model, args.n_heads),\n            args.d_model, args.drop_ratio)\n\n        self.attention = ResidualBlock(\n            MultiHead(Attention(args.d_model, args.drop_ratio),\n                      args.d_model, args.d_model, args.n_heads),\n            args.d_model, args.drop_ratio)\n\n        self.feedforward = ResidualBlock(\n            FeedForward(args.d_model, args.d_hidden, args.drop_ratio),\n            args.d_model, args.drop_ratio)\n\n    def forward(self, x, encoding):\n        x = self.selfattn(x, x, x)\n        x = self.attention(x, encoding, encoding)\n        return self.feedforward(x)\n\nclass Encoder(nn.Module):\n\n    def __init__(self, field, args):\n        super().__init__()\n        self.out = field.out\n        self.layers = nn.ModuleList(\n            [EncoderLayer(args) for i in range(args.n_layers)])\n        self.dropout = nn.Dropout(args.drop_ratio)\n        self.field = field\n        self.d_model = args.d_model\n\n    def forward(self, x):\n        x = F.embedding(x, self.out.weight * math.sqrt(self.d_model))\n        x += positional_encodings_like(x)\n        x = self.dropout(x)\n        encoding = []\n        for layer in self.layers:\n            x = layer(x)\n            encoding.append(x)\n        return encoding\n\nclass Decoder(nn.Module):\n\n    def __init__(self, field, args):\n        super().__init__()\n        self.out = field.out\n        self.layers = nn.ModuleList(\n            [DecoderLayer(args) for i in range(args.n_layers)])\n        self.dropout = nn.Dropout(args.drop_ratio)\n        self.d_model = args.d_model\n        self.field = field\n        self.length_ratio = args.length_ratio\n\n    def forward(self, x, encoding):\n        x = F.embedding(x, self.out.weight * math.sqrt(self.d_model))\n        x += positional_encodings_like(x)\n        x = self.dropout(x)\n\n        for l, (layer, enc) in enumerate(zip(self.layers, encoding)):\n            x = layer(x, enc)\n        return self.out(x)\n\nclass Transformer(nn.Module):\n\n    def __init__(self, src, trg, args):\n        super().__init__()\n        for field in set((src, trg)):\n            field.out = nn.Linear(args.d_model, len(field.vocab))\n        self.encoder = Encoder(src, args)\n        self.decoder = Decoder(trg, args)\n\n    def forward(self, encoder_inputs, decoder_inputs, decoding=False, beam=1,\n                alpha=0.6, return_probs=False):\n        encoding = self.encoder(encoder_inputs)\n\n        if (return_probs and decoding) or (not decoding):\n            out = self.decoder(decoder_inputs, encoding)\n\n        if decoding:\n            if beam == 1:\n                output = self.decoder.greedy(encoding)\n            else:\n                output = self.decoder.beam_search(encoding, beam, alpha)\n\n            if return_probs:\n                return output, out\n            return output\n\n        return out\n\n    def loss(self, batch, reduce=True, unbatch=False):\n        if unbatch:\n            loss = 0\n            for src, trg in zip(batch.src.examples(), batch.trg.examples()):\n                logits = self(src, trg[:, :-1])\n                loss += F.cross_entropy(logits, trg[:, 1:], reduce=reduce)\n            return loss\n        logits = self(batch.src, batch.trg[:, :-1])\n        return F.cross_entropy(logits, batch.trg[:, 1:], reduce=reduce)\n\nif __name__ == '__main__':\n    import sys\n    unbatch = sys.argv[1] == '1'\n    small = sys.argv[2] == '1'\n    if sys.argv[3] == '1':\n        TEXT = data.Field(batch_first=True)\n    else:\n        TEXT = MaskedBatchField(batch_first=True)\n    train, dev, test = datasets.IWSLT.splits(('.de', '.en'), (TEXT, TEXT))\n    TEXT.build_vocab(train, max_size=50000)\n    random.seed(0)\n    torch.manual_seed(0)\n    train_iter = data.BucketIterator(\n        train, batch_size=32, device=0 if torch.cuda.is_available() else -1)\n    args = argparse.Namespace()\n    args.__dict__.update(d_model=8 if small else 512,\n                         d_hidden=1 if small else 2048,\n                         n_heads=8, drop_ratio=0,\n                         n_layers=6, length_ratio=1.5)\n    model = Transformer(TEXT, TEXT, args)\n    if torch.cuda.is_available(): model.cuda()\n    for i, b in enumerate(train_iter):\n        if i == 1:\n            t = time.time()\n        if i == 2:\n            print(time.time() - t)\n            break\n        model.zero_grad()\n        loss = model.loss(b, unbatch=unbatch)\n        loss.backward()\n"""
matchbox/__init__.py,0,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\n\nfrom .compat import TENSOR_TYPE\n\nclass MaskedBatch(object):\n\n    def __init__(self, data, mask, dims):\n        if data.dim() != mask.dim() or mask.dim() != len(dims) + 1:\n            raise ValueError(""malformed MaskedBatch {} with:\\n data: ""\n                             "" {}\\n mask: {}"".format(\n                repr(dims), repr(data), repr(mask)))\n        if isinstance(mask, TENSOR_TYPE) and mask.requires_grad:\n            raise ValueError(""mask cannot require grad"")\n        self.data = data\n        self.mask = mask\n        self.dims = dims\n\n    @classmethod\n    def fromlist(cls, examples, dims):\n        # TODO do some validation\n        bs = len(examples)\n        sizes = [max(x.size(d + 1) for x in examples)\n                 for d in range(len(dims))]\n        data = examples[0].new(bs, *sizes).zero_()\n        mask_sizes = [s if dims[d] else 1 for d, s in enumerate(sizes)]\n        mask = examples[0].new(bs, *mask_sizes).zero_()\n        mask.requires_grad = False\n        for i, x in enumerate(examples):\n            inds = [slice(0, x.size(d + 1)) if b else slice(None)\n                    for d, b in enumerate(dims)]\n            data[(slice(i, i + 1), *inds)] = x\n            mask[(slice(i, i + 1), *inds)] = 1\n        return cls(data, mask, dims)\n\n    def examples(self):\n        data, mask, dims = self.data, self.mask.data.long(), self.dims\n        for i in range(data.maxsize(0)):\n            inds = tuple(slice(0, mask[i].sum(d, keepdim=True)[\n                tuple(0 for _ in dims)])\n                if b else slice(None) for d, b in enumerate(dims))\n            yield data[(slice(i, i + 1), *inds)]\n\n    def __repr__(self):\n        return ""MaskedBatch {} with:\\n data: {}\\n mask: {}"".format(\n            repr(self.dims), repr(self.data), repr(self.mask))\n\n    def cuda(self, *args, **kwargs):\n        data = self.data.cuda(*args, **kwargs)\n        mask = self.mask.cuda(*args, **kwargs)\n        return self.__class__(data, mask, self.dims)\n\n    @property\n    def is_cuda(self):\n        return self.data.is_cuda\n\n    def get_device(self):\n        return self.data.get_device()\n\n    def dim(self):\n        return self.data.dim()\n\n    def size(self, dim=None):\n        if dim is None:\n            if any(self.dims):\n                raise ValueError(""use size_as_tensor for dynamic dimensions"")\n            return self.data.size()\n        if dim < 0:\n            dim += self.dim()\n        if dim == 0 or not self.dims[dim - 1]:\n            return self.data.size(dim)\n        raise ValueError(""use size_as_tensor for dynamic dimensions"")\n\n    @property\n    def shape(self):\n        return self.size()\n\n    def new(self, *sizes):\n        return self.data.new(*sizes)\n\n    def type(self, dtype=None, non_blocking=False, **kwargs):\n        if dtype:\n            data = self.data.type(dtype, non_blocking, **kwargs)\n            mask = self.mask.type(dtype, non_blocking, **kwargs)\n            return self.__class__(data, mask, self.dims)\n        else:\n            return self.data.type()\n\n    def __bool__(self):\n        if self.data.nelement() > 1:\n            raise ValueError(""bool value of MaskedBatch with more than one ""\n                             ""value is ambiguous; use .any() or .all() or wrap ""\n                             ""code containing control flow in @batch."")\n        return bool(self.data)\n\nfrom . import functional\nfrom .macro import batch\n\ntry:\n    from . import data\nexcept ImportError:\n    pass\n\n# global mask stack for control flow; not thread-safe\n_EXECUTION_MASKS = [None]\nEXECUTION_MASK = None\n\ndef push_execution_mask(mask):\n    global EXECUTION_MASK\n    if EXECUTION_MASK is not None:\n        EXECUTION_MASK = EXECUTION_MASK * mask\n    else:\n        EXECUTION_MASK = mask\n    _EXECUTION_MASKS.append(EXECUTION_MASK)\n    EXECUTION_MASK = mask\n\ndef pop_execution_mask():\n    global EXECUTION_MASK\n    _EXECUTION_MASKS.pop()\n    EXECUTION_MASK = _EXECUTION_MASKS[-1]\n'"
matchbox/compat.py,9,"b""# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\n\nif torch.__version__ < '0.4':\n    MAYBE_VARIABLE = TENSOR_TYPE = torch.autograd.Variable\n\n    def _var_method(method_name):\n        def inner(self, *args, **kwargs):\n            t = getattr(self.data, method_name)(*args, **kwargs)\n            return torch.autograd.Variable(t)\n        return inner\n    TENSOR_TYPE.new_empty = TENSOR_TYPE.new = _var_method('new')\n\n    def _new_zeros(self, *sizes):\n        return self.new(*sizes).zero_()\n    TENSOR_TYPE.new_zeros = _new_zeros\n\n    def _new_ones(self, *sizes):\n        return self.new(*sizes).fill_(1)\n    TENSOR_TYPE.new_ones = _new_ones\n\n    def _where(cond, x, y):\n        cond = cond.type_as(x)\n        return x * cond + y * (1 - cond)\n    torch.where = _where\n\n    _old_arange = torch.arange\n    def _new_arange(*args, out=None):\n        if isinstance(out, torch.autograd.Variable):\n            torch.arange(*args, out=out.data)\n            return out\n        return _old_arange(*args, out=out)\n    torch.arange = _new_arange\n\nelse:\n    def identity(x): return x\n    MAYBE_VARIABLE = identity\n    TENSOR_TYPE = torch.Tensor\n"""
matchbox/data.py,1,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\nfrom torch.autograd import Variable\nfrom torchtext import data\nimport six\n\nfrom . import MaskedBatch\n\nclass MaskedBatchField(data.Field):\n\n    def process(self, batch, device, train):\n        """"""Process a list of examples to create a matchbox.MaskedBatch.\n\n        Args:\n            batch (list(object)): A list of examples comprising a batch.\n        Returns:\n            matchbox.MaskedBatch: Processed batch given the input and custom\n                postprocessing Pipeline.\n        """"""\n        batch = list(batch)\n        if self.sequential:\n            if self.fix_length is not None:\n                raise ValueError(""cannot use fix_length with Matchbox"")\n            batch = [([] if self.init_token is None else [self.init_token]) +\n                     list(x) +\n                     ([] if self.eos_token is None else [self.eos_token])\n                     for x in batch]\n\n        if self.use_vocab:\n            if self.sequential:\n                batch = [[self.vocab.stoi[x] for x in ex] for ex in batch]\n            else:\n                batch = [self.vocab.stoi[x] for x in batch]\n\n            if self.postprocessing is not None:\n                batch = self.postprocessing(batch, self.vocab, train)\n        else:\n            if self.tensor_type not in self.tensor_types:\n                raise ValueError(\n                    ""Specified Field tensor_type {} can not be used with ""\n                    ""use_vocab=False because we do not know how to numericalize it. ""\n                    ""Please raise an issue at ""\n                    ""https://github.com/pytorch/text/issues"".format(self.tensor_type))\n            numericalization_func = self.tensor_types[self.tensor_type]\n            batch = [numericalization_func(x) if isinstance(x, six.string_types)\n                   else x for x in batch]\n            if self.postprocessing is not None:\n                batch = self.postprocessing(batch, None, train)\n\n        batch = [Variable(self.tensor_type(x).unsqueeze(0), volatile=not train) for x in batch]\n        if self.sequential and not self.batch_first:\n            raise ValueError(""Matchbox requires batch_first for sequential Fields"")\n        dims = (True,) if self.sequential else ()\n        batch = MaskedBatch.fromlist(batch, dims)\n        if device != -1:\n            batch = batch.cuda()\n        return batch\n'"
matchbox/macro.py,0,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nfrom collections import defaultdict\n\nimport astor\nimport gast\n\nfrom .recompile import compile_function, code_to_ast\n\ndef pushmask(mask_expr):\n    return gast.Expr(gast.Call(\n        gast.Attribute(gast.Name(\'matchbox\', gast.Load(), None),\n                       gast.Name(\'push_execution_mask\', gast.Load(), None),\n                       gast.Load()),\n        [mask_expr], []))\n\npopmask = gast.Expr(gast.Call(\n    gast.Attribute(gast.Name(\'matchbox\', gast.Load(), None),\n                   gast.Name(\'pop_execution_mask\', gast.Load(), None),\n                   gast.Load()),\n    [], []))\n\ndef any_active(mask_expr):\n    return gast.Call(gast.Attribute( # TODO any over dim 0\n        mask_expr, gast.Name(\'any\', gast.Load(), None), gast.Load()), [], [])\n\nclass FuseAttributes(gast.NodeTransformer):\n    \'\'\'Transform foo.bar to foo_DOT_bar\'\'\'\n    def visit_Attribute(self, node):\n        self.generic_visit(node)\n        if not isinstance(node.value, gast.Name):\n            return node\n        attrname = node.attr if isinstance(node.attr, str) else node.attr.id\n        return gast.Name(node.value.id + \'_DOT_\' + attrname,\n                         node.value.ctx, None)\n\nclass SplitAttributes(gast.NodeTransformer):\n    \'\'\'Transform foo_DOT_bar to foo.bar\'\'\'\n    def visit_Name(self, node):\n        if \'_DOT_\' not in node.id:\n            return node\n        value, attr = node.id.split(\'_DOT_\')\n        return gast.Attribute(gast.Name(value, node.ctx, None),\n                              attr, node.ctx)\n\nclass ExecutionMasking(gast.NodeTransformer):\n    def __init__(self):\n        super().__init__()\n\n    def visit_FunctionDef(self, node):\n        node = self.generic_visit(node)\n        def is_batch_decorator(d):\n            if isinstance(d, gast.Name):\n                return d.id == \'batch\'\n            elif isinstance(d, gast.Attribute):\n                return is_batch_decorator(d.attr)\n            return d == \'batch\'\n        node.decorator_list = [d for d in node.decorator_list\n                               if not is_batch_decorator(d)]\n        return node\n\n    def visit_For(self, node):\n        node = self.generic_visit(node)\n        return self.synchronize_lcds(node)\n\n    def visit_If(self, node):\n        node = self.generic_visit(node)\n        node = self.add_mask(node, node.test)\n        nodes = [node]\n        if len(node.orelse) > 0:\n            test_inverse = gast.Call(\n                gast.Attribute(\n                    node.test, gast.Name(\'eq\', gast.Load(), None), gast.Load()),\n                [gast.Num(0)], [])\n            else_node = gast.If(any_active(test_inverse), node.orelse, [])\n            node.orelse = []\n            self.add_mask(else_node, test_inverse)\n            nodes.append(else_node)\n        node.test = any_active(node.test)\n        return nodes\n\n    def visit_While(self, node):\n        if len(node.orelse) > 0:\n            raise NotImplementedError(""cannot process while-else"")\n        node = self.generic_visit(node)\n        node = self.add_mask(node, node.test)\n        node.test = any_active(node.test)\n        node = self.synchronize_lcds(node)\n        return node\n\n    def visit_Assign(self, node):\n        if len(node.targets) > 1:\n            raise NotImplementedError(""cannot process multiple assignment"")\n        if not isinstance(node.targets[0], gast.Name):\n            raise NotImplementedError(""cannot process indexed assignment"")\n        # $lhs = $lhs.update_($rhs, matchbox.EXECUTION_MASK) if (lhs in vars()\n        # or lhs in globals()) and isinstance($lhs, (matchbox.MaskedBatch,\n        # matchbox.TENSOR_TYPE)) else $rhs\n        node.value = gast.IfExp(\n            gast.BoolOp(gast.And(),\n                [gast.BoolOp(gast.Or(),\n                    [gast.Compare(gast.Str(node.targets[0].id), [gast.In()],\n                        [gast.Call(gast.Name(\'vars\', gast.Load, None),\n                                   [], [])]),\n                     gast.Compare(gast.Str(node.targets[0].id), [gast.In()],\n                        [gast.Call(gast.Name(\'globals\', gast.Load, None),\n                                   [], [])])]),\n                 # gast.Compare(\n                 #    gast.Attribute(\n                 #      gast.Name(\'matchbox\', gast.Load(), None),\n                 #      gast.Name(\'EXECUTION_MASK\', gast.Load(), None),\n                 #      gast.Load()),\n                 #    [gast.IsNot()],\n                 #    [gast.NameConstant(None)]),\n                 gast.Call(gast.Name(\'isinstance\', gast.Load(), None),\n                           [node.targets[0], gast.Tuple(\n                            [gast.Attribute(\n                                gast.Name(\'matchbox\', gast.Load(), None),\n                                gast.Name(\'MaskedBatch\', gast.Load(), None),\n                                gast.Load()),\n                             gast.Attribute(\n                                gast.Name(\'matchbox\', gast.Load(), None),\n                                gast.Name(\'TENSOR_TYPE\', gast.Load(), None),\n                                gast.Load())], gast.Load())], [])]),\n            gast.Call(\n                gast.Attribute(\n                    gast.Name(node.targets[0].id, gast.Load(), None),\n                    gast.Name(\'_update\', gast.Load(), None),\n                    gast.Load()),\n                [node.value, gast.Attribute(\n                  gast.Name(\'matchbox\', gast.Load(), None),\n                  gast.Name(\'EXECUTION_MASK\', gast.Load(), None),\n                  gast.Load())], []),\n            node.value)\n        return node\n\n    def add_mask(self, node, mask):\n        node.body = [pushmask(mask)] + node.body + [popmask]\n        return node\n\n    def synchronize_lcds(self, node):\n        node = FuseAttributes().visit(node)\n        loads, lcds = defaultdict(list), set()\n        for child in node.body:\n            for n in gast.walk(child):\n                if isinstance(n, gast.Name) and isinstance(n.ctx, gast.Load):\n                    loads[n.id].append(n)\n            if isinstance(child, gast.Assign):\n                name = child.targets[0].id\n                if name in loads:\n                    if name in lcds:\n                        raise NotImplementedError(""cannot process LCD ""\n                                                  ""stored to twice"")\n                    lcds.add(name)\n        node = SplitAttributes().visit(node)\n        synchronizes = []\n        for name in lcds:\n            synchronize = gast.Assign(\n                [gast.Name(name, gast.Store(), None)],\n                gast.Call(\n                    gast.Attribute(\n                        gast.Name(name, gast.Load(), None),\n                        gast.Name(\'_synchronize\', gast.Load(), None),\n                        None),\n                    [], []))\n            synchronizes.append(synchronize)\n        node.body.extend(synchronizes)\n        return node\n\ndef batch(fn):\n    node = code_to_ast(fn)\n    node = ExecutionMasking().visit(node)\n    return compile_function(node, fn.__globals__)\n'"
matchbox/test_utils.py,3,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\nfrom torch.autograd import Variable\nimport matchbox\nfrom matchbox import functional as F\nfrom matchbox import MaskedBatch\n\nimport random\nimport numpy as np\n\ndef mb_rand(*dims):\n    dims = [dim for dim in dims if dim != ()]\n    xs = [Variable(torch.rand(1, *(random.randint(1, size) if b else size\n                  for b, size in dims[1:]))) for i in range(dims[0])]\n    xb = MaskedBatch.fromlist(xs, tuple(b for b, d in dims[1:]))\n    return xs, xb\n\ndef mb_assert_allclose(xs, ybs):\n    if isinstance(ybs, Variable):\n        np.testing.assert_allclose(xs.data.numpy(), ybs.data.numpy(), rtol=1e-3)\n    elif isinstance(ybs, MaskedBatch):\n        mb_assert_allclose(xs, ybs.examples())\n    else:\n        if isinstance(ybs, (list, tuple)):\n            for j, yb in enumerate(ybs):\n                for i, y in enumerate(yb.examples()):\n                    mb_assert_allclose(xs[i][j], y)\n        else:\n            for x, yb in zip(xs, ybs):\n                mb_assert_allclose(x, yb)\n\ndef mb_assert(f, xsarr, xbarr, bs):\n    ys = [f(*(xs[j] if isinstance(xs, list) else xs for xs in xsarr))\n          for j in range(bs)]\n    ybs = f(*xbarr)\n    mb_assert_allclose(ys, ybs)\n\ndef mb_test(f, *dimsarr):\n    xsarr, xbarr = [], []\n    bs = None\n    for dims in dimsarr:\n        if not isinstance(dims, tuple):\n            xsarr.append(xsarr[dims])\n            xbarr.append(xbarr[dims])\n        elif isinstance(dims[-1], tuple):\n            bs = dims[0]\n            xs, xb = mb_rand(*dims)\n            xsarr.append(xs)\n            xbarr.append(xb)\n        else:\n            x = Variable(torch.rand(*dims))\n            xsarr.append(x)\n            xbarr.append(x)\n    mb_assert(f, xsarr, xbarr, bs)\n'"
test/test_control_flow.py,1,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch import nn\nimport matchbox\nfrom matchbox import functional as F\nfrom matchbox import MaskedBatch, batch\nfrom matchbox.test_utils import mb_test, mb_assert\n\nimport random\n\n@batch\ndef while_loop(x):\n    while x > 0:\n        x = x - 1\n    return x\n\ndef test_while():\n    mb_test(while_loop, (4, ()))\n\n@batch\ndef if_else(x):\n    if x > 0:\n        x = x - 1\n    else:\n        pass\n    return x\n\ndef test_if_else():\n    mb_test(if_else, (4, ()))\n\n@batch\ndef if_noelse(x):\n    if x > 0:\n        x = x - 1\n    return x\n\ndef test_if_noelse():\n    mb_test(if_noelse, (4, ()))\n'"
test/test_functions.py,3,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\nfrom torch.autograd import Variable\nimport matchbox\nfrom matchbox import functional as F\nfrom matchbox import MaskedBatch\nfrom matchbox.test_utils import mb_test, mb_assert\n\nimport random\n\ndef test_embedding():\n    xs = [Variable(torch.LongTensor(1, random.randint(1, 3)).random_(5))\n          for i in range(4)]\n    W = Variable(torch.rand(5, 2))\n    xb = MaskedBatch.fromlist(xs, (True,))\n    mb_assert(F.embedding, (xs, W), (xb, W), 4)\n\ndef test_mean():\n    mb_test(lambda x: x.mean(2),\n            (4, (True, 3), (False, 2)))\n\ndef test_std():\n    mb_test(lambda x: x.std(2),\n            (4, (True, 3), (False, 2)))\n\ndef test_matmul():\n    mb_test(lambda a, b: a @ b,\n            (4, (True, 3), (False, 2)), (4, (False, 2), (True, 3)))\n\ndef test_transpose():\n    mb_test(lambda x: x.transpose(1, 2),\n            (4, (True, 3), (False, 2)))\n\ndef test_causal_mask():\n    mb_test(lambda x: x.causal_mask(2, 1).softmax() @ x,\n            (4, (False, 3), (False, 3)))\n    mb_test(lambda x: (x @ x.transpose(1, 2)).causal_mask(2, 1).softmax() @ x,\n            (4, (True, 3), (False, 2)))\n'"
test/test_rnns.py,1,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\nfrom torch.autograd import Variable\nfrom torch import nn\nimport matchbox\nfrom matchbox import functional as F\nfrom matchbox import MaskedBatch, batch\nfrom matchbox.test_utils import mb_test, mb_assert\n\nimport random\n\ndef test_rnn_cell():\n    mb_test(nn.RNNCell(2, 2), (4, (False, 2)), (4, (False, 2)))\n\ndef test_rnn():\n    @batch\n    def simple_rnn(x, h0, cell):\n        h = h0\n        for xt in x.unbind(1):\n            h = cell(xt, h)\n        return h\n    def SimpleRNN(cell):\n        def inner(x, h0):\n            return simple_rnn(x, h0, cell)\n        return inner\n    mb_test(SimpleRNN(nn.RNNCell(2, 2)),\n            (4, (True, 3), (False, 2)), (4, (False, 2)))\n\nclass RNNClass(nn.Module):\n    def __init__(self, cell):\n        super().__init__()\n        self.cell = cell\n    @batch\n    def forward(self, x, h0=None):\n        h = x.new(x.size(0), x.size(-1)).zero_() if h0 is None else h0\n        for xt in x.unbind(1):\n            h = self.cell(xt, h)\n        return h\n\ndef test_rnn_class():\n    mb_test(RNNClass(nn.RNNCell(2, 2)),\n            (4, (True, 3), (False, 2)))\n\nclass LSTMClass(nn.Module):\n    def __init__(self, in_size, out_size):\n        super().__init__()\n        self.cell = nn.LSTMCell(in_size, out_size)\n    @batch\n    def forward(self, x, h0=None, c0=None):\n        h = x.new(x.size(0), x.size(-1)).zero_() if h0 is None else h0\n        c = x.new(x.size(0), x.size(-1)).zero_() if c0 is None else c0\n        for xt in x.unbind(1):\n            state = self.cell(xt, (h, c))\n            h = state[0]\n            c = state[1]\n        return h\n\ndef test_lstm_class():\n    mb_test(LSTMClass(2, 2),\n            (4, (True, 3), (False, 2)))\n\ndef test_bilstm_class():\n    class BiLSTMClass(nn.Module):\n        def __init__(self, in_size, out_size):\n            super().__init__()\n            self.fcell = nn.LSTMCell(in_size, out_size)\n            self.rcell = nn.LSTMCell(in_size, out_size)\n        @batch\n        def forward(self, x, h0=None, c0=None):\n            hf = x.batch_zeros(x.size(-1)) if h0 is None else h0\n            cf = x.batch_zeros(x.size(-1)) if c0 is None else c0\n            for xt in x.unbind(1):\n                state = self.fcell(xt, (hf, cf))\n                hf = state[0]\n                cf = state[1]\n            hr = x.batch_zeros(x.size(-1)) if h0 is None else h0\n            cr = x.batch_zeros(x.size(-1)) if c0 is None else c0\n            for xt in reversed(x.unbind(1)):\n                state = self.rcell(xt, (hr, cr))\n                hr = state[0]\n                cr = state[1]\n            return hf, hr\n    mb_test(BiLSTMClass(2, 2),\n            (4, (True, 3), (False, 2)))\n\nclass AccumRNNClass(nn.Module):\n    def __init__(self, cell, dynamic):\n        super().__init__()\n        self.cell = cell\n        self.dynamic = dynamic\n    @batch\n    def forward(self, x, h0=None):\n        h = x.new(x.size(0), x.size(-1)).zero_() if h0 is None else h0\n        encoding = []\n        for xt in x.unbind(1):\n            h = self.cell(xt, h)\n            encoding.append(h)\n        return F.stack(encoding, 1, self.dynamic)\n\ndef test_accum_rnn_class():\n    mb_test(AccumRNNClass(nn.RNNCell(2, 2), None),\n            (4, (True, 3), (False, 2)))\n    mb_test(AccumRNNClass(nn.RNNCell(2, 2), True),\n            (4, (True, 3), (False, 2)))\n\ndef test_accum_birnn_class():\n    class AccumBiRNNClass(nn.Module):\n        def __init__(self, size):\n            super().__init__()\n            self.fwd = nn.RNNCell(size, size)\n            self.bwd = nn.RNNCell(size, size)\n        @batch\n        def forward(self, x):\n            h0 = x.batch_zeros(x.size(-1))\n            h = h0\n            fwd = []\n            bwd = []\n            for xt in x.unbind(1):\n                h = self.fwd(xt, h)\n                fwd.append(h)\n            fwd = F.stack(fwd, 1)\n            h = h0\n            for xt in reversed(x.unbind(1)):\n                h = self.bwd(xt, h)\n                bwd.append(h)\n            bwd = F.stack(reversed(bwd), 1)\n            return F.cat((fwd, bwd), 2)\n    mb_test(AccumBiRNNClass(1),\n            (4, (True, 3), (False, 1)))\n\ndef test_readme():\n    class RNN(nn.Module):\n        def __init__(self, size):\n            super().__init__()\n            self.cell = nn.RNNCell(size, size)\n        @matchbox.batch\n        def forward(self, x):\n            h = x.new_zeros(x.size(0), x.size(-1))\n            for xt in x.unbind(1):\n                h = self.cell(xt, h)\n            return h\n    mb_test(RNN(1),\n            (4, (True, 3), (False, 1)))\n'"
matchbox/functional/__init__.py,11,"b""# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\n\nfrom .nnet import dropout, linear, embedding, softmax, cross_entropy\nfrom .elementwise import log, sqrt, sin, cos, tan, relu, tanh, sigmoid\nfrom .tensor_math import matmul\nfrom .indexing import getitem\nfrom .tensor_shape import split, chunk, cat, stack, unbind\nfrom .tensor_shape import contiguous, view, transpose, permute\nfrom .tensor_shape import split_dim, join_dims, size_as_tensor, maxsize\nfrom .special import causal_mask\nfrom . import reduction\nfrom . import constructors\n\nimport sys\n\n# monkeypatching\n# the giant hammer approach has problems:\n# torch.nn.functional = sys.modules[__name__]\n# so instead we'll do it piecemeal\n\nimport torch.nn.modules.sparse\nimport torch.nn.modules.linear\nimport torch.nn.modules.dropout\nimport torch.nn._functions.rnn\n\ntorch.nn.modules.sparse.F = sys.modules[__name__]\ntorch.nn.modules.linear.F = sys.modules[__name__]\ntorch.nn.modules.dropout.F = sys.modules[__name__]\ntorch.nn._functions.rnn.F = sys.modules[__name__]\n\nif torch.__version__ < '0.4':\n    def embed_forward(self, input):\n        return embedding(\n            input, self.weight, self.padding_idx, self.max_norm,\n            self.norm_type, self.scale_grad_by_freq, self.sparse)\n    torch.nn.Embedding.forward = embed_forward\n"""
matchbox/functional/constructors.py,0,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\n\nfrom matchbox import MaskedBatch\nfrom matchbox.compat import MAYBE_VARIABLE, TENSOR_TYPE\n\ndef _inject_new(original):\n    def inner(self, *sizes):\n        source = self.data if isinstance(self, MaskedBatch) else self\n        if not any(isinstance(size, MaskedBatch) for size in sizes):\n            return original(source, *(int(size) for size in sizes))\n        if isinstance(sizes[0], MaskedBatch):\n            raise ValueError(""batch size dimension must be static"")\n        dims = tuple(isinstance(size, MaskedBatch) for size in sizes[1:])\n        maxsizes = [size.data.max() if isinstance(size, MaskedBatch)\n                    else int(size) for size in sizes]\n        bs = maxsizes[0]\n        masksizes = [s if b else 1 for s, b in zip(maxsizes[1:], dims)]\n        data = original(source, *maxsizes)\n        mask = source.new_zeros(bs, *masksizes)\n        # TODO this should be\n        # mask[range(bs), *(s - 1 for s in masksizes)] = 1\n        # mask = mask[:, *(slice(None, None, -1) if b\n        #                  else slice(None, None, None) for b in dims)]\n        # for d, b in enumerate(dims):\n        #     if not b: continue\n        #     mask = mask.cumsum(d + 1)\n        # mask = mask[:, *(slice(None, None, -1) if b\n        #                  else slice(None, None, None) for b in dims)]\n        # if faking negative strides is fast enough;\n        # we can also use numpy if it\'s worth it.\n        for i in range(bs):\n            inds = [slice(0, int(size.data[i])) if b else slice(None)\n                    for size, b in zip(sizes[1:], dims)]\n            mask[(slice(i, i + 1), *inds)] = 1\n        return MaskedBatch(data, mask, dims)\n    return inner\n\nMaskedBatch.new_empty = TENSOR_TYPE.new_empty = _inject_new(\n    TENSOR_TYPE.new_empty)\nMaskedBatch.new_zeros = TENSOR_TYPE.new_zeros = _inject_new(\n    TENSOR_TYPE.new_zeros)\nMaskedBatch.new_ones = TENSOR_TYPE.new_ones = _inject_new(\n    TENSOR_TYPE.new_ones)\n\ndef _inject_batch_new(original):\n    def inner(batch, *sizes):\n        if len(sizes) == 0:\n            return original(batch, *batch.size())\n        return original(batch, batch.size(0), *sizes)\n    return inner\n\nMaskedBatch.batch_empty = TENSOR_TYPE.batch_empty = _inject_batch_new(\n    TENSOR_TYPE.new_empty)\nMaskedBatch.batch_zeros = TENSOR_TYPE.batch_zeros = _inject_batch_new(\n    TENSOR_TYPE.new_zeros)\nMaskedBatch.batch_ones = TENSOR_TYPE.batch_ones = _inject_batch_new(\n    TENSOR_TYPE.new_ones)\n'"
matchbox/functional/elementwise.py,1,"b""# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\nfrom torch.nn import functional as F\n\nfrom matchbox import MaskedBatch\nfrom matchbox.compat import MAYBE_VARIABLE, TENSOR_TYPE\n\ndef _elementwise_unary(fn):\n    def inner(batch, *args, **kwargs):\n        if not isinstance(batch, MaskedBatch):\n            return fn(batch, *args, **kwargs)\n        data = fn(batch.data, *args, **kwargs)\n        mask = batch.mask.type_as(data)\n        dims = batch.dims\n        return MaskedBatch(data, mask, dims)\n    return inner\n\nMaskedBatch.float = _elementwise_unary(TENSOR_TYPE.float)\nMaskedBatch.double = _elementwise_unary(TENSOR_TYPE.double)\nMaskedBatch.byte = _elementwise_unary(TENSOR_TYPE.byte)\nMaskedBatch.int = _elementwise_unary(TENSOR_TYPE.int)\nMaskedBatch.long = _elementwise_unary(TENSOR_TYPE.long)\n\nMaskedBatch.floor = _elementwise_unary(TENSOR_TYPE.floor)\nMaskedBatch.ceil = _elementwise_unary(TENSOR_TYPE.ceil)\nMaskedBatch.clamp = _elementwise_unary(TENSOR_TYPE.clamp)\n\nMaskedBatch.log = log = _elementwise_unary(TENSOR_TYPE.log)\nMaskedBatch.sqrt = sqrt = _elementwise_unary(TENSOR_TYPE.sqrt)\nMaskedBatch.sin = sin = _elementwise_unary(TENSOR_TYPE.sin)\nMaskedBatch.cos = cos = _elementwise_unary(TENSOR_TYPE.cos)\nMaskedBatch.tan = tan = _elementwise_unary(TENSOR_TYPE.tan)\n\nMaskedBatch.relu = relu = _elementwise_unary(F.relu)\nMaskedBatch.tanh = tanh = _elementwise_unary(F.tanh)\nMaskedBatch.sigmoid = sigmoid = _elementwise_unary(F.sigmoid)\n\nMaskedBatch.__neg__ = _elementwise_unary(TENSOR_TYPE.__neg__)\n\ndef _elementwise_binary(fn):\n    def inner(batch1, batch2, **kwargs):\n        if not isinstance(batch1, MaskedBatch) and not isinstance(batch2, MaskedBatch):\n            return fn(batch1, batch2, **kwargs)\n        if isinstance(batch2, MaskedBatch):\n            data = fn(batch1.data, batch2.data, **kwargs)\n            mask = batch1.mask * batch2.mask\n            dims = tuple(b1 or b2 for b1, b2 in zip(batch1.dims, batch2.dims))\n        else:\n            data = fn(batch1.data, batch2, **kwargs)\n            mask = batch1.mask.type_as(data)\n            dims = batch1.dims\n        return MaskedBatch(data, mask, dims)\n    return inner\n\nMaskedBatch.__add__ = _elementwise_binary(TENSOR_TYPE.__add__)\nMaskedBatch.__sub__ = _elementwise_binary(TENSOR_TYPE.__sub__)\nMaskedBatch.__mul__ = _elementwise_binary(TENSOR_TYPE.__mul__)\nMaskedBatch.__truediv__ = _elementwise_binary(TENSOR_TYPE.__truediv__)\nMaskedBatch.__radd__ = _elementwise_binary(TENSOR_TYPE.__radd__)\nMaskedBatch.__rsub__ = _elementwise_binary(TENSOR_TYPE.__rsub__)\nMaskedBatch.__rmul__ = _elementwise_binary(TENSOR_TYPE.__rmul__)\nMaskedBatch.__rtruediv__ = _elementwise_binary(TENSOR_TYPE.__rtruediv__)\n\nMaskedBatch.__lt__ = _elementwise_binary(TENSOR_TYPE.__lt__)\nMaskedBatch.__le__ = _elementwise_binary(TENSOR_TYPE.__le__)\nMaskedBatch.__eq__ = _elementwise_binary(TENSOR_TYPE.__eq__)\nMaskedBatch.__ne__ = _elementwise_binary(TENSOR_TYPE.__ne__)\nMaskedBatch.__gt__ = _elementwise_binary(TENSOR_TYPE.__gt__)\nMaskedBatch.__ge__ = _elementwise_binary(TENSOR_TYPE.__ge__)\n\nMaskedBatch.lt = _elementwise_binary(TENSOR_TYPE.lt)\nMaskedBatch.le = _elementwise_binary(TENSOR_TYPE.le)\nMaskedBatch.eq = _elementwise_binary(TENSOR_TYPE.eq)\nMaskedBatch.ne = _elementwise_binary(TENSOR_TYPE.ne)\nMaskedBatch.gt = _elementwise_binary(TENSOR_TYPE.gt)\nMaskedBatch.ge = _elementwise_binary(TENSOR_TYPE.ge)\n\ndef _inject_arith(original, replacement):\n    def inner(self, other):\n        if isinstance(other, MaskedBatch):\n            return replacement(self, other)\n        return original(self, other)\n    return inner\n\nTENSOR_TYPE.__add__ = _inject_arith(TENSOR_TYPE.__add__, lambda a, b: b + a)\nTENSOR_TYPE.__sub__ = _inject_arith(TENSOR_TYPE.__sub__, lambda a, b: -b + a)\nTENSOR_TYPE.__mul__ = _inject_arith(TENSOR_TYPE.__mul__, lambda a, b: b * a)\n# TODO fix __sub__; it's ugly\n# TENSOR_TYPE.__matmul__ = _inject_arith(TENSOR_TYPE.__matmul__, lambda a, b:)\n# TENSOR_TYPE.__truediv__ = _inject_arith(TENSOR_TYPE.__truediv__, lambda a, b:)\n"""
matchbox/functional/indexing.py,0,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\n\nfrom matchbox import MaskedBatch\nfrom matchbox.compat import MAYBE_VARIABLE, TENSOR_TYPE\n\ndef getitem(batch, index):\n    if not isinstance(index, tuple) or index[0] != slice(None):\n        raise ValueError(""first index must be :"")\n    if None in index:\n        raise NotImplementedError(""cannot index with None"")\n    data = batch.data[index]\n    index = list(index)\n    for i, (ind, b) in enumerate(zip(index[1:], batch.dims)):\n        if b:\n            if isinstance(ind, int) and ind < 0:\n                raise NotImplementedError(""cannot index dynamic dim with ""\n                                          ""negative integer"")\n            if isinstance(ind, slice) and ind.stop is not None and ind.stop < 0:\n                if ind.step is not None or ind.start is not None:\n                    raise NotImplementedError(""cannot index dynamic dim with ""\n                                              ""complex slice"")\n                index[i + 1] = slice(-ind.stop, None)\n    index = tuple(index)\n    mask = batch.mask[tuple(i if b else 0 if isinstance(i, int) else slice(None)\n                       for i, b in zip(index, (True,) + batch.dims))]\n    dims = tuple(b for i, b in zip(index[1:] + (slice(None),) * len(batch.dims),\n                                   batch.dims)\n                 if not isinstance(i, int)) # could be faster\n    return MaskedBatch(data, mask, dims)\n\nMaskedBatch.__getitem__ = getitem\n'"
matchbox/functional/nnet.py,2,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\nfrom torch.nn import functional as F\n\nfrom matchbox import MaskedBatch\nfrom matchbox.compat import MAYBE_VARIABLE, TENSOR_TYPE\n\ndef dropout(batch, p=0.5, training=False, inplace=False):\n    if not isinstance(batch, MaskedBatch):\n        return F.dropout(batch, p, training, inplace)\n    data = F.dropout(batch.data, p, training, inplace)\n    return MaskedBatch(data, batch.mask, batch.dims)\n\nMaskedBatch.dropout = dropout\nTENSOR_TYPE.dropout = dropout\n\ndef linear(batch, weight, bias=None):\n    if not isinstance(batch, MaskedBatch):\n        return F.linear(batch, weight, bias)\n    if batch.dims[-1]:\n        raise ValueError(""cannot contract static and dynamic dimensions"")\n    data = F.linear(batch.data, weight, bias)\n    return MaskedBatch(data, batch.mask, batch.dims)\n\ndef embedding(batch, weight, padding_idx=None, max_norm=None, norm_type=2,\n              scale_grad_by_freq=False, sparse=False):\n    def compat_embedding(batch, weight, padding_idx, max_norm, norm_type,\n                         scale_grad_by_freq, sparse):\n        if torch.__version__ >= \'0.4\':\n            return F.embedding(batch, weight, padding_idx, max_norm, norm_type,\n                               scale_grad_by_freq, sparse)\n        if padding_idx is not None:\n            raise ValueError(""F.embedding doesn\'t support padding_idx for torch < 0.4"")\n        return F.embedding(batch, weight, max_norm, norm_type,\n                           scale_grad_by_freq, sparse)\n\n    if not isinstance(batch, MaskedBatch):\n        return compat_embedding(batch, weight, padding_idx, max_norm, norm_type,\n                                scale_grad_by_freq, sparse)\n    #data = batch.data - batch.mask\n    data = batch.data\n    data = compat_embedding(\n        data, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\n    mask = batch.mask.unsqueeze(-1).float()\n    dims = batch.dims + (False,)\n    return MaskedBatch(data, mask, dims)\n\ndef softmax(batch, dim=-1):\n    if not isinstance(batch, MaskedBatch):\n        return F.softmax(batch, dim)\n    if dim == 0:\n        raise ValueError(""cannot softmax over batch dimension"")\n    elif dim < 0:\n        dim += batch.dim()\n    dims = batch.dims\n    if dims[dim - 1]:\n        data = F.softmax(batch.data * batch.mask, dim) * batch.mask\n        data = data / data.sum(dim, keepdim=True)\n        data[data.ne(data).detach()] = 0 # remove NaNs\n        mask = batch.mask.narrow(dim, 0, 1)\n        dims = dims[:dim - 1] + (False,) + dims[dim:]\n    else:\n        data = F.softmax(batch.data, dim)\n        mask = batch.mask\n    return MaskedBatch(data, mask, dims)\n\nMaskedBatch.softmax = softmax\nTENSOR_TYPE.softmax = softmax\n\ndef cross_entropy(input, target, weight=None, size_average=True,\n                  ignore_index=-1, reduce=True):\n    if not isinstance(input, MaskedBatch) and not isinstance(target, MaskedBatch):\n        ret = F.cross_entropy(input.contiguous().view(-1, input.size(-1)),\n                              target.contiguous().view(-1),\n                              weight, size_average, ignore_index, reduce)\n        if reduce: return ret\n        return ret.view(input.size(0), input.size(1))\n    target_data = (target.data + target.mask - 1).view(-1)\n    input_data = input.data.view(target_data.size(0), -1)\n    if ignore_index != -1:\n        raise ValueError(""cannot set ignore_index with MaskedBatch"")\n    data = F.cross_entropy(\n        input_data, target_data, weight, size_average, ignore_index, reduce)\n    if reduce: return data\n    data = data.view(input.maxsize(0), input.maxsize(1))\n    mask = input.mask.squeeze(-1) * target.mask.float()\n    return MaskedBatch(data, mask, target.dims)\n'"
matchbox/functional/reduction.py,3,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\n\nfrom matchbox import MaskedBatch\nfrom matchbox.compat import MAYBE_VARIABLE, TENSOR_TYPE\n\ndef _reduce(fn, zero_preserving=False):\n    def inner(batch, dim=None, keepdim=False):\n        if dim is None:\n            if not zero_preserving and __builtins__[\'any\'](batch.dims):\n                raise NotImplementedError(\n                    ""cannot reduce to scalar with non-zero-preserving kernel ""\n                    ""if dynamic dims present"")\n            mask = batch.mask[(slice(None), *(0 for d in batch.dims))]\n            dims = ()\n        else:\n            if dim < 0:\n                dim += batch.dim()\n            if not zero_preserving and batch.dims[dim - 1]:\n                raise NotImplementedError(""cannot reduce over dynamic dim ""\n                                          ""with non-zero-preserving kernel"")\n            if keepdim:\n                mask = batch.mask[tuple(slice(0, 1) if i == dim else slice(None)\n                                        for i in range(batch.mask.dim()))]\n                dims = tuple(False if i == dim - 1 else d\n                             for i, d in enumerate(batch.dims))\n            else:\n                mask = batch.mask[tuple(0 if i == dim else slice(None)\n                                        for i in range(batch.mask.dim()))]\n                dims = tuple(d for i, d in enumerate(batch.dims)\n                             if i != dim - 1)\n        data = fn(batch.data * batch.mask, dim=dim, keepdim=keepdim)\n        return MaskedBatch(data, mask, dims)\n    return inner\n\nMaskedBatch.sum = _reduce(torch.sum, zero_preserving=True)\nMaskedBatch.mean = _reduce(torch.mean)\nMaskedBatch.std = _reduce(torch.std)\n\ndef any(batch):\n    return (batch.data * batch.mask).any()\n\nMaskedBatch.any = any\n\ndef all(batch):\n    return (batch.data * batch.mask).all()\n\nMaskedBatch.all = all\n'"
matchbox/functional/special.py,2,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\n\nfrom matchbox import MaskedBatch\nfrom matchbox.compat import MAYBE_VARIABLE, TENSOR_TYPE\n\ndef causal_mask(batch, in_dim, out_dim):\n    \'\'\'if in_dim is indexed by i and out_dim by j, masks ret[i,j] where i > j\'\'\'\n    if not isinstance(batch, MaskedBatch):\n        # TODO or we could just promote to MaskedBatch /shrug\n        if in_dim == 1 and out_dim == 2:\n            return batch - batch.new(\n                *batch.size()[1:]).fill_(1e10).tril(-1).unsqueeze(0)\n        elif in_dim == 2 and out_dim == 1:\n            return batch - batch.new(\n                *batch.size()[1:]).fill_(1e10).triu(1).unsqueeze(0)\n        else:\n            raise NotImplementedError(""unsupported arguments for causal_mask"")\n    if in_dim == 1 and out_dim == 2:\n        mask = batch.mask * batch.mask.new(\n            *batch.data.size()[1:]).fill_(1).triu(0).unsqueeze(0)\n    elif in_dim == 2 and out_dim == 1:\n        mask = batch.mask * batch.mask.new(\n            *batch.data.size()[1:]).fill_(1).tril(0).unsqueeze(0)\n    else:\n        raise NotImplementedError(""unsupported arguments for causal_mask"")\n    dims = tuple(True if d + 1 in (in_dim, out_dim) else b\n                 for d, b in enumerate(batch.dims))\n    return MaskedBatch(batch.data, mask, dims)\n\nMaskedBatch.causal_mask = causal_mask\nTENSOR_TYPE.causal_mask = causal_mask\n\ndef _synchronize(batch):\n    if not isinstance(batch, MaskedBatch):\n        return batch\n    if any(batch.dims):\n        raise ValueError(""cannot synchronize batch with dynamic dimensions"")\n    mask = batch.mask + (1 - batch.mask)\n    return MaskedBatch(batch.data, mask, batch.dims)\n\nMaskedBatch._synchronize = _synchronize\nTENSOR_TYPE._synchronize = _synchronize\n\ndef _update(batch, new, update_mask=None):\n    if not isinstance(new, MaskedBatch) and (\n            not isinstance(batch, MaskedBatch) or update_mask is None):\n        return new\n    update_mask = (new.mask.byte() if update_mask is None\n                   else update_mask.data * update_mask.mask)\n    if isinstance(batch, MaskedBatch):\n        data = torch.where(update_mask, new.data, batch.data)\n    else:\n        data = torch.where(update_mask, new.data, batch)\n    return MaskedBatch(data, update_mask.type_as(data), new.dims)\n\nMaskedBatch._update = _update\nTENSOR_TYPE._update = _update\n\n# def _for(closure, iterator):\n#     for i in iterator:\n#         closure(i)\n'"
matchbox/functional/tensor_math.py,0,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\n\nfrom matchbox import MaskedBatch\nfrom matchbox.compat import MAYBE_VARIABLE, TENSOR_TYPE\n\ndef matmul(batch1, batch2):\n    if not isinstance(batch1, MaskedBatch) and not isinstance(batch2, MaskedBatch):\n        return batch1 @ batch2\n    if isinstance(batch1, MaskedBatch) and isinstance(batch2, MaskedBatch):\n        dims1 = len(batch1.dims)\n        dims2 = len(batch2.dims)\n        data1 = batch1.data * batch1.mask\n        data2 = batch2.data * batch2.mask\n        if dims1 == 1:\n            data1 = data1.unsqueeze(-2)\n        if dims2 == 1 and dims1 == 1:\n            data2 = data2.unsqueeze(-1)\n        data = data1 @ data2\n        if dims1 == 1 and dims2 == 1:\n            #if (batch1.dims[0] or batch2.dims[0]) and not batch1.mask.eq(batch2.mask).all():\n            #    raise ValueError(""cannot contract non-matching dimensions"")\n            mask = batch1.mask[:, :1]\n            dims = ()\n        if dims1 == 2 and dims2 == 1:\n            #if (batch1.dims[1] or batch2.dims[0]) and not batch1.mask[:, 0].eq(batch2.mask).all():\n            #    raise ValueError(""cannot contract non-matching dimensions"")\n            mask = batch1.mask[:, :, :1] @ batch2.mask[:, :1]\n            dims = batch1.dims[:1]\n        elif dims1 == 1 and dims2 == 2:\n            #if (batch1.dims[0] or batch2.dims[0]) and not batch1.mask.eq(batch2.mask[:, :, 0]).all():\n            #    raise ValueError(""cannot contract non-matching dimensions"")\n            mask = batch1.mask[:, :1].unsqueeze(-2) @ batch2.mask[:, :1, :]\n            dims = batch2.dims[1:]\n        elif dims1 == 2 and dims2 == 2:\n            #if (batch1.dims[1] or batch2.dims[0]) and not batch1.mask[:, 0].eq(batch2.mask[:, :, 0]).all():\n            #    raise ValueError(""cannot contract non-matching dimensions"")\n            mask = batch1.mask[:, :, :1] @ batch2.mask[:, :1, :]\n            dims = batch1.dims[:1] + batch2.dims[1:]\n        else:\n            raise NotImplementedError(""matmul not implemented with batches of 3+D tensors"")\n    else:\n        raise NotImplementedError(""matmul not implemented between MaskedBatch and tensor"")\n    return MaskedBatch(data, mask, dims)\n\nMaskedBatch.__matmul__ = matmul\n'"
matchbox/functional/tensor_shape.py,17,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nimport torch\n\nfrom matchbox import MaskedBatch\nfrom matchbox.compat import MAYBE_VARIABLE, TENSOR_TYPE\n\ndef split(batch, split_size_or_sections, dim=0):\n    if not isinstance(batch, MaskedBatch):\n        return torch.split(batch, split_size_or_sections, dim)\n    if dim < 0:\n        dim += batch.dim()\n    if dim > 0 and batch.dims[dim - 1]:\n        return tuple(MaskedBatch(data, mask, batch.dims) for data, mask in zip(\n            torch.split(batch.data, split_size_or_sections, dim),\n            torch.split(batch.mask, split_size_or_sections, dim)))\n    return tuple(MaskedBatch(data, batch.mask, batch.dims) for data\n                 in torch.split(batch.data, split_size_or_sections, dim))\n\nMaskedBatch.split = split\n\ndef chunk(batch, chunks, dim=0):\n    if dim < 0:\n        dim += batch.dim()\n    split_size = (batch.maxsize(dim) + chunks - 1) // chunks\n    return split(batch, split_size, dim)\n\nMaskedBatch.chunk = chunk\n\ndef cat(sequence, dim):\n    sequence = list(sequence)\n    if len(sequence) == 0:\n        raise ValueError(""cannot stack empty sequence"")\n    first = sequence[0]\n    if not isinstance(first, MaskedBatch):\n        return torch.cat(sequence, dim)\n    data = torch.cat([batch.data for batch in sequence], dim)\n    if first.dims[dim - 1]:\n        mask = torch.cat([batch.mask for batch in sequence], dim)\n    else:\n        mask = first.mask\n    return MaskedBatch(data, mask, first.dims)\n\ndef stack(sequence, dim, dynamic=None):\n    sequence = list(sequence)\n    if len(sequence) == 0:\n        raise ValueError(""cannot stack empty sequence"")\n    first = sequence[0]\n    if not isinstance(first, MaskedBatch):\n        return torch.stack(sequence, dim)\n    if dim < 0:\n        dim += first.dim() + 1\n    if dynamic is None:\n        dynamic = not first.mask.eq(sequence[-1].mask).all()\n    data = torch.cat([batch.data.unsqueeze(dim) for batch in sequence], dim)\n    if dynamic:\n        mask = torch.cat(\n            [batch.mask.unsqueeze(dim) for batch in sequence], dim)\n    else:\n        mask = first.mask.unsqueeze(dim)\n    dims = first.dims[:dim - 1] + (dynamic,) + first.dims[dim - 1:]\n    return MaskedBatch(data, mask, dims)\n\ndef unbind(batch, dim):\n    if not isinstance(batch, MaskedBatch):\n        return torch.unbind(batch, dim)\n    if dim == 0:\n        raise ValueError(""cannot unbind over batch dimension"")\n    dims = tuple(b for d, b in enumerate(batch.dims) if d != dim - 1)\n    if batch.dims[dim - 1]:\n        return tuple(MaskedBatch(data, mask, dims)\n                     for data, mask in zip(torch.unbind(batch.data, dim),\n                                           torch.unbind(batch.mask, dim)))\n    else:\n        mask = batch.mask.squeeze(dim)\n        return tuple(MaskedBatch(data, mask, dims)\n                     for data in torch.unbind(batch.data, dim))\n\nMaskedBatch.unbind = unbind\nTENSOR_TYPE.unbind = unbind\n\ndef contiguous(batch):\n    return MaskedBatch(\n        batch.data.contiguous(), batch.mask.contiguous(), batch.dims)\n\nMaskedBatch.contiguous = contiguous\n\ndef view(batch, *sizes):\n    bs = batch.data.size(0)\n    if sizes[0] not in (1, -1, bs):\n        raise ValueError(""first dim in view must be 1, -1, or batch size"")\n    sizes = (bs,) + sizes[1:]\n    data = batch.data.view(*sizes) # TODO can throw\n    mask_sizes = (bs,) + tuple(batch.data.size(i) if sizes[i] == -1 else 1\n                               for i in range(1, len(sizes)))\n    mask = batch.mask.view(*mask_sizes) # TODO can this throw if data doesn\'t?\n    dims = tuple(sizes[i] == -1 for i in range(1, len(sizes)))\n    return MaskedBatch(data, mask, dims)\n\nMaskedBatch.view = view\n\ndef transpose(batch, dim1, dim2):\n    if dim1 > batch.dim() or dim2 > batch.dim():\n        if dim1 < 0:\n            dim1 += batch.dim()\n        if dim2 < 0:\n            dim2 += batch.dim()\n        permutation = [dim2 if i == dim1 else dim1 if i == dim2 else i\n                       for i in range(batch.dim() + 1)][:batch.dim()]\n        return batch.permute(*permutation)\n    if not isinstance(batch, MaskedBatch):\n        return torch.transpose(batch, dim1, dim2)\n    data = batch.data.transpose(dim1, dim2)\n    mask = batch.mask.transpose(dim1, dim2)\n    dims = list(batch.dims)\n    dims[dim1 - 1], dims[dim2 - 1] = dims[dim2 - 1], dims[dim1 - 1]\n    dims = tuple(dims)\n    return MaskedBatch(data, mask, dims)\n\nMaskedBatch.transpose = transpose\nTENSOR_TYPE.transpose = transpose\n\ndef permute(batch, *permutation):\n    data = batch.data.permute(*permutation)\n    mask = batch.mask.permute(*permutation)\n    dims = tuple(batch.dims[i - 1] for i in permutation[1:])\n    return MaskedBatch(data, mask, dims)\n\nMaskedBatch.permute = permute\n\ndef split_dim(batch, dim, split_by):\n    if dim < 0:\n        dim += batch.dim()\n    if batch.data.size(dim) % split_by != 0:\n        raise ValueError(""size of dim not divisible by split_by"")\n    sizes = ((s // split_by, split_by) if d == dim else (s,)\n             for d, s in enumerate(batch.data.size()))\n    if not isinstance(batch, MaskedBatch):\n        return batch.contiguous().view(*(n for tup in sizes for n in tup))\n    if dim == 0:\n        msizes = ((s // split_by, split_by) if d == dim else (s,)\n                 for d, s in enumerate(batch.mask.size()))\n        mask = batch.mask.contiguous().view(*(n for tup in msizes for n in tup))\n        mask = mask.narrow(1, 0, 1)\n    else:\n        if batch.dims[dim - 1]:\n            raise ValueError(""cannot split dynamic dimension"")\n        mask = batch.mask.unsqueeze(dim)\n    data = batch.data.contiguous().view(*(n for tup in sizes for n in tup))\n    dims = batch.dims[:dim] + (False,) + batch.dims[dim:]\n    return MaskedBatch(data, mask, dims)\n\nMaskedBatch.split_dim = split_dim\nTENSOR_TYPE.split_dim = split_dim\n\ndef join_dims(batch, dim1, dim2):\n    if dim1 < 0:\n        dim1 += batch.dim()\n    if dim2 < 0:\n        dim2 += batch.dim()\n    if dim2 != dim1 + 1:\n        order = [n for n in range(batch.dim()) if n != dim2]\n        order.insert(dim1 + 1, dim2)\n        batch = batch.permute(*order)\n        if dim2 < dim1:\n            dim1 -= 1\n    if not isinstance(batch, MaskedBatch):\n        sizes = (batch.size(d + 1) * s if d == dim1 else s\n                 for d, s in enumerate(batch.size()) if d != dim1 + 1)\n        return batch.contiguous().view(*sizes)\n    sizes = (batch.data.size(d + 1) * s if d == dim1 else s\n             for d, s in enumerate(batch.data.size()) if d != dim1 + 1)\n    data = batch.data.contiguous().view(*sizes)\n    if dim1 == 0:\n        mask = batch.mask.expand(*(s if d == dim1 + 1 else -1\n                                   for d, s in enumerate(batch.data.size())))\n        sizes = (s * mask.size(d + 1) if d == dim1 else s\n                 for d, s in enumerate(mask.size()) if d != dim1 + 1)\n        mask = mask.contiguous().view(*sizes)\n    else:\n        mask = batch.mask.squeeze(dim1 + 1)\n    dims = batch.dims[:dim1] + batch.dims[dim1 + 1:]\n    return MaskedBatch(data, mask, dims)\n\nMaskedBatch.join_dims = join_dims\nTENSOR_TYPE.join_dims = join_dims\n\ndef size_as_tensor(batch, dim):\n    if not isinstance(batch, MaskedBatch):\n        return MAYBE_VARIABLE(torch.LongTensor([batch.size(dim)]))\n    if dim is None:\n        return tuple(batch.size(d) for d in range(len(batch.dims) + 1))\n    if dim < 0:\n        dim += batch.dim()\n    if dim == 0 or not batch.dims[dim - 1]:\n        return MAYBE_VARIABLE(torch.LongTensor([batch.data.size(dim)]))\n    if any(batch.dims[:dim - 1] + batch.dims[dim:]):\n        raise NotImplementedError(""cannot get size in any of two or ""\n                                  ""more dynamic dimensions"")\n    data = batch.mask.long().sum(dim).view(-1)\n    mask = data.new(batch.mask.size(0)).fill_(1)\n    return MaskedBatch(data, mask, ())\n\nMaskedBatch.size_as_tensor = size_as_tensor\nTENSOR_TYPE.size_as_tensor = size_as_tensor\n\ndef maxsize(batch, dim=None):\n    return batch.data.size() if dim is None else batch.data.size(dim)\n\nMaskedBatch.maxsize = maxsize\nTENSOR_TYPE.maxsize = maxsize\n'"
matchbox/recompile/__init__.py,0,"b'# Copyright (c) 2018, salesforce.com, inc.\n# All rights reserved.\n# Licensed under the BSD 3-Clause license.\n# For full license text, see the LICENSE file in the repo root\n# or https://opensource.org/licenses/BSD-3-Clause\n\nfrom .compile_function import compile_function\nfrom .code_to_ast import code_to_ast\n'"
matchbox/recompile/code_to_ast.py,0,"b'# This code is MODIFIED from the version in Astor (github.com/berkerpeksag/astor)\n# found at https://github.com/berkerpeksag/astor/blob/master/astor/file_util.py\n#\n# Part of the astor library for Python AST manipulation.\n# License: 3-clause BSD\n# Copyright (c) 2012-2015 Patrick Maupin\n# Copyright (c) 2013-2015 Berker Peksag\n\nimport ast\nimport sys\nimport os\n\nimport gast\n\nclass CodeToAst(object):\n    """"""Given a module, or a function that was compiled as part\n    of a module, re-compile the module into an AST and extract\n    the sub-AST for the function.  Allow caching to reduce\n    number of compiles.\n    Also contains static helper utility functions to\n    look for python files, to parse python files, and to extract\n    the file/line information from a code object.\n    """"""\n\n    @staticmethod\n    def parse_file(fname):\n        """"""Parse a python file into an AST.\n        This is a very thin wrapper around ast.parse\n            TODO: Handle encodings other than the default (issue #26)\n        """"""\n        try:\n            with open(fname, \'r\') as f:\n                fstr = f.read()\n        except IOError:\n            if fname != \'stdin\':\n                raise\n            sys.stdout.write(\'\\nReading from stdin:\\n\\n\')\n            fstr = sys.stdin.read()\n        fstr = fstr.replace(\'\\r\\n\', \'\\n\').replace(\'\\r\', \'\\n\')\n        if not fstr.endswith(\'\\n\'):\n            fstr += \'\\n\'\n        return ast.parse(fstr, filename=fname)\n\n    @staticmethod\n    def get_file_info(codeobj):\n        """"""Returns the file and line number of a code object.\n            If the code object has a __file__ attribute (e.g. if\n            it is a module), then the returned line number will\n            be 0\n        """"""\n        fname = getattr(codeobj, \'__file__\', None)\n        linenum = 0\n        if fname is None:\n            func_code = codeobj.__code__\n            fname = func_code.co_filename\n            linenum = func_code.co_firstlineno\n        fname = fname.replace(\'.pyc\', \'.py\')\n        return fname, linenum\n\n    def __init__(self, cache=None):\n        self.cache = cache or {}\n\n    def __call__(self, codeobj):\n        cache = self.cache\n        key = self.get_file_info(codeobj)\n        result = cache.get(key)\n        if result is not None:\n            return result\n        fname = key[0]\n        cache[(fname, 0)] = mod_ast = gast.ast_to_gast(self.parse_file(fname))\n        for obj in gast.walk(mod_ast):\n            if isinstance(obj, gast.FunctionDef):\n                cache[(fname, obj.lineno)] = obj\n        return cache[key]\n\ncode_to_ast = CodeToAst()\n'"
matchbox/recompile/compile_function.py,0,"b'# This code is MODIFIED from the version in Tangent (github.com/google/tangent)\n# found at https://github.com/google/tangent/blob/master/tangent/compile.py\n#\n# Copyright 2017 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n#      Unless required by applicable law or agreed to in writing, software\n#      distributed under the License is distributed on an ""AS IS"" BASIS,\n#      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#      See the License for the specific language governing permissions and\n#      limitations under the License.\n\nfrom __future__ import absolute_import\nimport os\nimport tempfile\nfrom uuid import uuid4\n\nimport astor\nimport gast\nimport six\nif six.PY3:\n    from importlib import util\nelse:\n    import imp\n\ndef compile_file(source, globals_=None):\n    """"""Compile by saving to file and importing that.\n    Compiling the AST/source code this way ensures that the source code is\n    readable by e.g. `pdb` or `inspect`.\n    Args:\n    source: The code to compile, either as a string or as an AST.\n    globals_: A dictionary of variables that should be available as globals in\n        the compiled module. They will be monkey patched after importing the\n        module.\n    Returns:\n    A module object containing the compiled source code.\n    """"""\n    if isinstance(source, gast.AST):\n        source = astor.to_source(gast.gast_to_ast(source))\n\n    # Write source to temporary file\n    tempdir = tempfile.mkdtemp()\n    uuid = str(uuid4().hex[:4])\n    tmpname = os.path.join(tempdir, \'matchbox_%s.py\' % uuid)\n    with open(tmpname, \'w\') as f:\n        f.write(source)\n\n    # Load the temporary file as a module\n    module_name = \'matchbox_%s\' % uuid\n    if six.PY3:\n        spec = util.spec_from_file_location(module_name, tmpname)\n        m = util.module_from_spec(spec)\n        spec.loader.exec_module(m)\n    else:\n        m = imp.load_source(module_name, tmpname)\n\n    # Update the modules namespace\n    if globals_:\n        m.__dict__.update(globals_)\n    return m\n\n\ndef compile_function(node, globals_=None):\n    """"""Convert an AST into a function with inspectable source.\n    This function uses `compile_file` internally, but instead of returning the\n    entire module it will return the function only.\n    Args:\n    node: A `FunctionDef` node or a `Module` node which contains at least one\n        `FunctionDef` node. If a module contains multiple functions, a handle\n        to the first one will be returned.\n    globals_: See `compile_file`\n    Returns:\n    A handle to the compiled function.\n    Raises:\n    TypeError: If the input is not a string or AST.\n    ValueError: If no function can be found.\n    """"""\n    module = compile_file(node, globals_)\n    return getattr(module, node.name)\n'"
