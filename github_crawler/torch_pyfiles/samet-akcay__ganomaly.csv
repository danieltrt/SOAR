file_path,api_count,code
options.py,1,"b'"""""" Options\n\nThis script is largely based on junyanz/pytorch-CycleGAN-and-pix2pix.\n\nReturns:\n    [argparse]: Class containing argparse\n""""""\n\nimport argparse\nimport os\nimport torch\n\n# pylint: disable=C0103,C0301,R0903,W0622\n\nclass Options():\n    """"""Options class\n\n    Returns:\n        [argparse]: argparse containing train and test options\n    """"""\n\n    def __init__(self):\n        ##\n        #\n        self.parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n        ##\n        # Base\n        self.parser.add_argument(\'--dataset\', default=\'cifar10\', help=\'folder | cifar10 | mnist \')\n        self.parser.add_argument(\'--dataroot\', default=\'\', help=\'path to dataset\')\n        self.parser.add_argument(\'--batchsize\', type=int, default=64, help=\'input batch size\')\n        self.parser.add_argument(\'--workers\', type=int, help=\'number of data loading workers\', default=8)\n        self.parser.add_argument(\'--droplast\', action=\'store_true\', default=True, help=\'Drop last batch size.\')\n        self.parser.add_argument(\'--isize\', type=int, default=32, help=\'input image size.\')\n        self.parser.add_argument(\'--nc\', type=int, default=3, help=\'input image channels\')\n        self.parser.add_argument(\'--nz\', type=int, default=100, help=\'size of the latent z vector\')\n        self.parser.add_argument(\'--ngf\', type=int, default=64)\n        self.parser.add_argument(\'--ndf\', type=int, default=64)\n        self.parser.add_argument(\'--extralayers\', type=int, default=0, help=\'Number of extra layers on gen and disc\')\n        self.parser.add_argument(\'--device\', type=str, default=\'gpu\', help=\'Device: gpu | cpu\')\n        self.parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        self.parser.add_argument(\'--ngpu\', type=int, default=1, help=\'number of GPUs to use\')\n        self.parser.add_argument(\'--name\', type=str, default=\'experiment_name\', help=\'name of the experiment\')\n        self.parser.add_argument(\'--model\', type=str, default=\'ganomaly\', help=\'chooses which model to use. ganomaly\')\n        self.parser.add_argument(\'--display_server\', type=str, default=""http://localhost"", help=\'visdom server of the web display\')\n        self.parser.add_argument(\'--display_port\', type=int, default=8097, help=\'visdom port of the web display\')\n        self.parser.add_argument(\'--display_id\', type=int, default=0, help=\'window id of the web display\')\n        self.parser.add_argument(\'--display\', action=\'store_true\', help=\'Use visdom.\')\n        self.parser.add_argument(\'--outf\', default=\'./output\', help=\'folder to output images and model checkpoints\')\n        self.parser.add_argument(\'--manualseed\', default=-1, type=int, help=\'manual seed\')\n        self.parser.add_argument(\'--abnormal_class\', default=\'car\', help=\'Anomaly class idx for mnist and cifar datasets\')\n        self.parser.add_argument(\'--proportion\', type=float, default=0.1, help=\'Proportion of anomalies in test set.\')\n        self.parser.add_argument(\'--metric\', type=str, default=\'roc\', help=\'Evaluation metric.\')\n\n        ##\n        # Train\n        self.parser.add_argument(\'--print_freq\', type=int, default=100, help=\'frequency of showing training results on console\')\n        self.parser.add_argument(\'--save_image_freq\', type=int, default=100, help=\'frequency of saving real and fake images\')\n        self.parser.add_argument(\'--save_test_images\', action=\'store_true\', help=\'Save test images for demo.\')\n        self.parser.add_argument(\'--load_weights\', action=\'store_true\', help=\'Load the pretrained weights\')\n        self.parser.add_argument(\'--resume\', default=\'\', help=""path to checkpoints (to continue training)"")\n        self.parser.add_argument(\'--phase\', type=str, default=\'train\', help=\'train, val, test, etc\')\n        self.parser.add_argument(\'--iter\', type=int, default=0, help=\'Start from iteration i\')\n        self.parser.add_argument(\'--niter\', type=int, default=15, help=\'number of epochs to train for\')\n        self.parser.add_argument(\'--beta1\', type=float, default=0.5, help=\'momentum term of adam\')\n        self.parser.add_argument(\'--lr\', type=float, default=0.0002, help=\'initial learning rate for adam\')\n        self.parser.add_argument(\'--w_adv\', type=float, default=1, help=\'Adversarial loss weight\')\n        self.parser.add_argument(\'--w_con\', type=float, default=50, help=\'Reconstruction loss weight\')\n        self.parser.add_argument(\'--w_enc\', type=float, default=1, help=\'Encoder loss weight.\')\n        self.isTrain = True\n        self.opt = None\n\n    def parse(self):\n        """""" Parse Arguments.\n        """"""\n\n        self.opt = self.parser.parse_args()\n        self.opt.isTrain = self.isTrain   # train or test\n\n        str_ids = self.opt.gpu_ids.split(\',\')\n        self.opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                self.opt.gpu_ids.append(id)\n\n        # set gpu ids\n        if self.opt.device == \'gpu\':\n            torch.cuda.set_device(self.opt.gpu_ids[0])\n\n        args = vars(self.opt)\n\n        # print(\'------------ Options -------------\')\n        # for k, v in sorted(args.items()):\n        #     print(\'%s: %s\' % (str(k), str(v)))\n        # print(\'-------------- End ----------------\')\n\n        # save to the disk\n        if self.opt.name == \'experiment_name\':\n            self.opt.name = ""%s/%s"" % (self.opt.model, self.opt.dataset)\n        expr_dir = os.path.join(self.opt.outf, self.opt.name, \'train\')\n        test_dir = os.path.join(self.opt.outf, self.opt.name, \'test\')\n\n        if not os.path.isdir(expr_dir):\n            os.makedirs(expr_dir)\n        if not os.path.isdir(test_dir):\n            os.makedirs(test_dir)\n\n        file_name = os.path.join(expr_dir, \'opt.txt\')\n        with open(file_name, \'wt\') as opt_file:\n            opt_file.write(\'------------ Options -------------\\n\')\n            for k, v in sorted(args.items()):\n                opt_file.write(\'%s: %s\\n\' % (str(k), str(v)))\n            opt_file.write(\'-------------- End ----------------\\n\')\n        return self.opt\n'"
train.py,0,"b'""""""\nTRAIN GANOMALY\n\n. Example: Run the following command from the terminal.\n    run train.py                             \\\n        --model ganomaly                        \\\n        --dataset UCSD_Anomaly_Dataset/UCSDped1 \\\n        --batchsize 32                          \\\n        --isize 256                         \\\n        --nz 512                                \\\n        --ngf 64                               \\\n        --ndf 64\n""""""\n\n\n##\n# LIBRARIES\nfrom __future__ import print_function\n\nfrom options import Options\nfrom lib.data import load_data\nfrom lib.model import Ganomaly\n\n##\ndef train():\n    """""" Training\n    """"""\n\n    ##\n    # ARGUMENTS\n    opt = Options().parse()\n    ##\n    # LOAD DATA\n    dataloader = load_data(opt)\n    ##\n    # LOAD MODEL\n    model = Ganomaly(opt, dataloader)\n    ##\n    # TRAIN MODEL\n    model.train()\n\nif __name__ == \'__main__\':\n    train()\n'"
lib/__init__.py,0,b''
lib/data.py,23,"b'""""""\nLOAD DATA from file.\n""""""\n\n# pylint: disable=C0301,E1101,W0622,C0103,R0902,R0915\n\n##\nimport os\nimport torch\nimport numpy as np\nimport torchvision.datasets as datasets\nfrom torchvision.datasets import MNIST\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as transforms\n\n##\ndef load_data(opt):\n    """""" Load Data\n\n    Args:\n        opt ([type]): Argument Parser\n\n    Raises:\n        IOError: Cannot Load Dataset\n\n    Returns:\n        [type]: dataloader\n    """"""\n\n    ##\n    # LOAD DATA SET\n    if opt.dataroot == \'\':\n        opt.dataroot = \'./data/{}\'.format(opt.dataset)\n\n    if opt.dataset in [\'cifar10\']:\n        splits = [\'train\', \'test\']\n        drop_last_batch = {\'train\': True, \'test\': False}\n        shuffle = {\'train\': True, \'test\': False}\n\n        transform = transforms.Compose(\n            [\n                transforms.Resize(opt.isize),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ]\n        )\n\n        classes = {\n            \'plane\': 0, \'car\': 1, \'bird\': 2, \'cat\': 3, \'deer\': 4,\n            \'dog\': 5, \'frog\': 6, \'horse\': 7, \'ship\': 8, \'truck\': 9\n        }\n\n        dataset = {}\n        dataset[\'train\'] = CIFAR10(root=\'./data\', train=True, download=True, transform=transform)\n        dataset[\'test\'] = CIFAR10(root=\'./data\', train=False, download=True, transform=transform)\n\n        dataset[\'train\'].data, dataset[\'train\'].targets, \\\n        dataset[\'test\'].data, dataset[\'test\'].targets = get_cifar_anomaly_dataset(\n            trn_img=dataset[\'train\'].data,\n            trn_lbl=dataset[\'train\'].targets,\n            tst_img=dataset[\'test\'].data,\n            tst_lbl=dataset[\'test\'].targets,\n            abn_cls_idx=classes[opt.abnormal_class],\n            manualseed=opt.manualseed\n        )\n\n        dataloader = {x: torch.utils.data.DataLoader(dataset=dataset[x],\n                                                     batch_size=opt.batchsize,\n                                                     shuffle=shuffle[x],\n                                                     num_workers=int(opt.workers),\n                                                     drop_last=drop_last_batch[x],\n                                                     worker_init_fn=(None if opt.manualseed == -1\n                                                     else lambda x: np.random.seed(opt.manualseed)))\n                      for x in splits}\n        return dataloader\n\n    elif opt.dataset in [\'mnist\']:\n        opt.abnormal_class = int(opt.abnormal_class)\n\n        splits = [\'train\', \'test\']\n        drop_last_batch = {\'train\': True, \'test\': False}\n        shuffle = {\'train\': True, \'test\': True}\n\n        transform = transforms.Compose(\n            [\n                transforms.Resize(opt.isize),\n                transforms.ToTensor(),\n                transforms.Normalize((0.1307,), (0.3081,))\n            ]\n        )\n\n        dataset = {}\n        dataset[\'train\'] = MNIST(root=\'./data\', train=True, download=True, transform=transform)\n        dataset[\'test\'] = MNIST(root=\'./data\', train=False, download=True, transform=transform)\n\n        dataset[\'train\'].data, dataset[\'train\'].targets, \\\n        dataset[\'test\'].data, dataset[\'test\'].targets = get_mnist_anomaly_dataset(\n            trn_img=dataset[\'train\'].data,\n            trn_lbl=dataset[\'train\'].targets,\n            tst_img=dataset[\'test\'].data,\n            tst_lbl=dataset[\'test\'].targets,\n            abn_cls_idx=opt.abnormal_class,\n            manualseed=opt.manualseed\n        )\n\n        dataloader = {x: torch.utils.data.DataLoader(dataset=dataset[x],\n                                                     batch_size=opt.batchsize,\n                                                     shuffle=shuffle[x],\n                                                     num_workers=int(opt.workers),\n                                                     drop_last=drop_last_batch[x],\n                                                     worker_init_fn=(None if opt.manualseed == -1\n                                                     else lambda x: np.random.seed(opt.manualseed)))\n                      for x in splits}\n        return dataloader\n\n    elif opt.dataset in [\'mnist2\']:\n        opt.abnormal_class = int(opt.abnormal_class)\n\n        splits = [\'train\', \'test\']\n        drop_last_batch = {\'train\': True, \'test\': False}\n        shuffle = {\'train\': True, \'test\': True}\n\n        transform = transforms.Compose(\n            [\n                transforms.Resize(opt.isize),\n                transforms.ToTensor(),\n                transforms.Normalize((0.1307,), (0.3081,))\n            ]\n        )\n\n        dataset = {}\n        dataset[\'train\'] = MNIST(root=\'./data\', train=True, download=True, transform=transform)\n        dataset[\'test\'] = MNIST(root=\'./data\', train=False, download=True, transform=transform)\n\n        dataset[\'train\'].data, dataset[\'train\'].targets, \\\n        dataset[\'test\'].data, dataset[\'test\'].targets = get_mnist2_anomaly_dataset(\n            trn_img=dataset[\'train\'].data,\n            trn_lbl=dataset[\'train\'].targets,\n            tst_img=dataset[\'test\'].data,\n            tst_lbl=dataset[\'test\'].targets,\n            nrm_cls_idx=opt.abnormal_class,\n            proportion=opt.proportion,\n            manualseed=opt.manualseed\n        )\n\n        dataloader = {x: torch.utils.data.DataLoader(dataset=dataset[x],\n                                                     batch_size=opt.batchsize,\n                                                     shuffle=shuffle[x],\n                                                     num_workers=int(opt.workers),\n                                                     drop_last=drop_last_batch[x],\n                                                     worker_init_fn=(None if opt.manualseed == -1\n                                                     else lambda x: np.random.seed(opt.manualseed)))\n                      for x in splits}\n        return dataloader\n\n    else:\n        splits = [\'train\', \'test\']\n        drop_last_batch = {\'train\': True, \'test\': False}\n        shuffle = {\'train\': True, \'test\': True}\n        transform = transforms.Compose([transforms.Resize(opt.isize),\n                                        transforms.CenterCrop(opt.isize),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ])\n\n        dataset = {x: ImageFolder(os.path.join(opt.dataroot, x), transform) for x in splits}\n        dataloader = {x: torch.utils.data.DataLoader(dataset=dataset[x],\n                                                     batch_size=opt.batchsize,\n                                                     shuffle=shuffle[x],\n                                                     num_workers=int(opt.workers),\n                                                     drop_last=drop_last_batch[x],\n                                                     worker_init_fn=(None if opt.manualseed == -1\n                                                     else lambda x: np.random.seed(opt.manualseed)))\n                      for x in splits}\n        return dataloader\n\n##\ndef get_cifar_anomaly_dataset(trn_img, trn_lbl, tst_img, tst_lbl, abn_cls_idx=0, manualseed=-1):\n    """"""[summary]\n\n    Arguments:\n        trn_img {np.array} -- Training images\n        trn_lbl {np.array} -- Training labels\n        tst_img {np.array} -- Test     images\n        tst_lbl {np.array} -- Test     labels\n\n    Keyword Arguments:\n        abn_cls_idx {int} -- Anomalous class index (default: {0})\n\n    Returns:\n        [np.array] -- New training-test images and labels.\n    """"""\n    # Convert train-test labels into numpy array.\n    trn_lbl = np.array(trn_lbl)\n    tst_lbl = np.array(tst_lbl)\n\n    # --\n    # Find idx, img, lbl for abnormal and normal on org dataset.\n    nrm_trn_idx = np.where(trn_lbl != abn_cls_idx)[0]\n    abn_trn_idx = np.where(trn_lbl == abn_cls_idx)[0]\n    nrm_trn_img = trn_img[nrm_trn_idx]    # Normal training images\n    abn_trn_img = trn_img[abn_trn_idx]    # Abnormal training images\n    nrm_trn_lbl = trn_lbl[nrm_trn_idx]    # Normal training labels\n    abn_trn_lbl = trn_lbl[abn_trn_idx]    # Abnormal training labels.\n\n    nrm_tst_idx = np.where(tst_lbl != abn_cls_idx)[0]\n    abn_tst_idx = np.where(tst_lbl == abn_cls_idx)[0]\n    nrm_tst_img = tst_img[nrm_tst_idx]    # Normal training images\n    abn_tst_img = tst_img[abn_tst_idx]    # Abnormal training images.\n    nrm_tst_lbl = tst_lbl[nrm_tst_idx]    # Normal training labels\n    abn_tst_lbl = tst_lbl[abn_tst_idx]    # Abnormal training labels.\n\n    # --\n    # Assign labels to normal (0) and abnormals (1)\n    nrm_trn_lbl[:] = 0\n    nrm_tst_lbl[:] = 0\n    abn_trn_lbl[:] = 1\n    abn_tst_lbl[:] = 1\n\n    # --\n    if manualseed != -1:\n        # Random seed.\n        # Concatenate the original train and test sets.\n        nrm_img = np.concatenate((nrm_trn_img, nrm_tst_img), axis=0)\n        nrm_lbl = np.concatenate((nrm_trn_lbl, nrm_tst_lbl), axis=0)\n        abn_img = np.concatenate((abn_trn_img, abn_tst_img), axis=0)\n        abn_lbl = np.concatenate((abn_trn_lbl, abn_tst_lbl), axis=0)\n\n        # Split the normal data into the new train and tests.\n        idx = np.arange(len(nrm_lbl))\n        np.random.seed(manualseed)\n        np.random.shuffle(idx)\n\n        nrm_trn_len = int(len(idx) * 0.80)\n        nrm_trn_idx = idx[:nrm_trn_len]\n        nrm_tst_idx = idx[nrm_trn_len:]\n\n        nrm_trn_img = nrm_img[nrm_trn_idx]\n        nrm_trn_lbl = nrm_lbl[nrm_trn_idx]\n        nrm_tst_img = nrm_img[nrm_tst_idx]\n        nrm_tst_lbl = nrm_lbl[nrm_tst_idx]\n\n    # Create new anomaly dataset based on the following data structure:\n    # - anomaly dataset\n    #   . -> train\n    #        . -> normal\n    #   . -> test\n    #        . -> normal\n    #        . -> abnormal\n    new_trn_img = np.copy(nrm_trn_img)\n    new_trn_lbl = np.copy(nrm_trn_lbl)\n    new_tst_img = np.concatenate((nrm_tst_img, abn_trn_img, abn_tst_img), axis=0)\n    new_tst_lbl = np.concatenate((nrm_tst_lbl, abn_trn_lbl, abn_tst_lbl), axis=0)\n\n    return new_trn_img, new_trn_lbl, new_tst_img, new_tst_lbl\n\n##\ndef get_mnist_anomaly_dataset(trn_img, trn_lbl, tst_img, tst_lbl, abn_cls_idx=0, manualseed=-1):\n    """"""[summary]\n\n    Arguments:\n        trn_img {np.array} -- Training images\n        trn_lbl {np.array} -- Training labels\n        tst_img {np.array} -- Test     images\n        tst_lbl {np.array} -- Test     labels\n\n    Keyword Arguments:\n        abn_cls_idx {int} -- Anomalous class index (default: {0})\n\n    Returns:\n        [np.array] -- New training-test images and labels.\n    """"""\n    # --\n    # Find normal abnormal indexes.\n    nrm_trn_idx = torch.from_numpy(np.where(trn_lbl.numpy() != abn_cls_idx)[0])\n    abn_trn_idx = torch.from_numpy(np.where(trn_lbl.numpy() == abn_cls_idx)[0])\n    nrm_tst_idx = torch.from_numpy(np.where(tst_lbl.numpy() != abn_cls_idx)[0])\n    abn_tst_idx = torch.from_numpy(np.where(tst_lbl.numpy() == abn_cls_idx)[0])\n\n    # --\n    # Find normal and abnormal images\n    nrm_trn_img = trn_img[nrm_trn_idx]    # Normal training images\n    abn_trn_img = trn_img[abn_trn_idx]    # Abnormal training images.\n    nrm_tst_img = tst_img[nrm_tst_idx]    # Normal training images\n    abn_tst_img = tst_img[abn_tst_idx]    # Abnormal training images.\n\n    # --\n    # Find normal and abnormal labels.\n    nrm_trn_lbl = trn_lbl[nrm_trn_idx]    # Normal training labels\n    abn_trn_lbl = trn_lbl[abn_trn_idx]    # Abnormal training labels.\n    nrm_tst_lbl = tst_lbl[nrm_tst_idx]    # Normal training labels\n    abn_tst_lbl = tst_lbl[abn_tst_idx]    # Abnormal training labels.\n\n    # --\n    # Assign labels to normal (0) and abnormals (1)\n    nrm_trn_lbl[:] = 0\n    nrm_tst_lbl[:] = 0\n    abn_trn_lbl[:] = 1\n    abn_tst_lbl[:] = 1\n\n    # --\n    if manualseed != -1:\n        # Random seed.\n        # Concatenate the original train and test sets.\n        nrm_img = torch.cat((nrm_trn_img, nrm_tst_img), dim=0)\n        nrm_lbl = torch.cat((nrm_trn_lbl, nrm_tst_lbl), dim=0)\n        abn_img = torch.cat((abn_trn_img, abn_tst_img), dim=0)\n        abn_lbl = torch.cat((abn_trn_lbl, abn_tst_lbl), dim=0)\n\n        # Split the normal data into the new train and tests.\n        idx = np.arange(len(nrm_lbl))\n        np.random.seed(manualseed)\n        np.random.shuffle(idx)\n\n        nrm_trn_len = int(len(idx) * 0.80)\n        nrm_trn_idx = idx[:nrm_trn_len]\n        nrm_tst_idx = idx[nrm_trn_len:]\n\n        nrm_trn_img = nrm_img[nrm_trn_idx]\n        nrm_trn_lbl = nrm_lbl[nrm_trn_idx]\n        nrm_tst_img = nrm_img[nrm_tst_idx]\n        nrm_tst_lbl = nrm_lbl[nrm_tst_idx]\n\n    # Create new anomaly dataset based on the following data structure:\n    new_trn_img = nrm_trn_img.clone()\n    new_trn_lbl = nrm_trn_lbl.clone()\n    new_tst_img = torch.cat((nrm_tst_img, abn_trn_img, abn_tst_img), dim=0)\n    new_tst_lbl = torch.cat((nrm_tst_lbl, abn_trn_lbl, abn_tst_lbl), dim=0)\n\n    return new_trn_img, new_trn_lbl, new_tst_img, new_tst_lbl\n\n##\ndef get_mnist2_anomaly_dataset(trn_img, trn_lbl, tst_img, tst_lbl, nrm_cls_idx=0, proportion=0.5,\n                               manualseed=-1):\n    """""" Create mnist 2 anomaly dataset.\n\n    Arguments:\n        trn_img {np.array} -- Training images\n        trn_lbl {np.array} -- Training labels\n        tst_img {np.array} -- Test     images\n        tst_lbl {np.array} -- Test     labels\n\n    Keyword Arguments:\n        nrm_cls_idx {int} -- Anomalous class index (default: {0})\n\n    Returns:\n        [tensor] -- New training-test images and labels.\n    """"""\n    # Seed for deterministic behavior\n    if manualseed != -1:\n        torch.manual_seed(manualseed)\n\n    # --\n    # Find normal abnormal indexes.\n    # TODO: PyTorch v0.4 has torch.where function\n    nrm_trn_idx = torch.from_numpy(np.where(trn_lbl.numpy() == nrm_cls_idx)[0])\n    abn_trn_idx = torch.from_numpy(np.where(trn_lbl.numpy() != nrm_cls_idx)[0])\n    nrm_tst_idx = torch.from_numpy(np.where(tst_lbl.numpy() == nrm_cls_idx)[0])\n    abn_tst_idx = torch.from_numpy(np.where(tst_lbl.numpy() != nrm_cls_idx)[0])\n\n    # Get n percent of the abnormal samples.\n    abn_tst_idx = abn_tst_idx[torch.randperm(len(abn_tst_idx))]\n    abn_tst_idx = abn_tst_idx[:int(len(abn_tst_idx) * proportion)]\n\n\n    # --\n    # Find normal and abnormal images\n    nrm_trn_img = trn_img[nrm_trn_idx]    # Normal training images\n    abn_trn_img = trn_img[abn_trn_idx]    # Abnormal training images.\n    nrm_tst_img = tst_img[nrm_tst_idx]    # Normal training images\n    abn_tst_img = tst_img[abn_tst_idx]    # Abnormal training images.\n\n    # --\n    # Find normal and abnormal labels.\n    nrm_trn_lbl = trn_lbl[nrm_trn_idx]    # Normal training labels\n    abn_trn_lbl = trn_lbl[abn_trn_idx]    # Abnormal training labels.\n    nrm_tst_lbl = tst_lbl[nrm_tst_idx]    # Normal training labels\n    abn_tst_lbl = tst_lbl[abn_tst_idx]    # Abnormal training labels.\n\n    # --\n    # Assign labels to normal (0) and abnormals (1)\n    nrm_trn_lbl[:] = 0\n    nrm_tst_lbl[:] = 0\n    abn_trn_lbl[:] = 1\n    abn_tst_lbl[:] = 1\n\n    # Create new anomaly dataset based on the following data structure:\n    new_trn_img = nrm_trn_img.clone()\n    new_trn_lbl = nrm_trn_lbl.clone()\n    new_tst_img = torch.cat((nrm_tst_img, abn_tst_img), dim=0)\n    new_tst_lbl = torch.cat((nrm_tst_lbl, abn_tst_lbl), dim=0)\n\n    return new_trn_img, new_trn_lbl, new_tst_img, new_tst_lbl'"
lib/evaluate.py,0,"b'"""""" Evaluate ROC\n\nReturns:\n    auc, eer: Area under the curve, Equal Error Rate\n""""""\n\n# pylint: disable=C0103,C0301\n\n##\n# LIBRARIES\nfrom __future__ import print_function\n\nimport os\nfrom sklearn.metrics import roc_curve, auc, average_precision_score, f1_score\nfrom scipy.optimize import brentq\nfrom scipy.interpolate import interp1d\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nrc(\'font\', **{\'family\': \'serif\', \'serif\': [\'Computer Modern\']})\nrc(\'text\', usetex=True)\n\ndef evaluate(labels, scores, metric=\'roc\'):\n    if metric == \'roc\':\n        return roc(labels, scores)\n    elif metric == \'auprc\':\n        return auprc(labels, scores)\n    elif metric == \'f1_score\':\n        threshold = 0.20\n        scores[scores >= threshold] = 1\n        scores[scores <  threshold] = 0\n        return f1_score(labels, scores)\n    else:\n        raise NotImplementedError(""Check the evaluation metric."")\n\n##\ndef roc(labels, scores, saveto=None):\n    """"""Compute ROC curve and ROC area for each class""""""\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n\n    labels = labels.cpu()\n    scores = scores.cpu()\n\n    # True/False Positive Rates.\n    fpr, tpr, _ = roc_curve(labels, scores)\n    roc_auc = auc(fpr, tpr)\n\n    # Equal Error Rate\n    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n\n    if saveto:\n        plt.figure()\n        lw = 2\n        plt.plot(fpr, tpr, color=\'darkorange\', lw=lw, label=\'(AUC = %0.2f, EER = %0.2f)\' % (roc_auc, eer))\n        plt.plot([eer], [1-eer], marker=\'o\', markersize=5, color=""navy"")\n        plt.plot([0, 1], [1, 0], color=\'navy\', lw=1, linestyle=\':\')\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel(\'False Positive Rate\')\n        plt.ylabel(\'True Positive Rate\')\n        plt.title(\'Receiver operating characteristic\')\n        plt.legend(loc=""lower right"")\n        plt.savefig(os.path.join(saveto, ""ROC.pdf""))\n        plt.close()\n\n    return roc_auc\n\ndef auprc(labels, scores):\n    ap = average_precision_score(labels, scores)\n    return ap\n'"
lib/loss.py,3,"b'""""""\nLosses\n""""""\n# pylint: disable=C0301,C0103,R0902,R0915,W0221,W0622\n\n\n##\n# LIBRARIES\nimport torch\n\n##\ndef l1_loss(input, target):\n    """""" L1 Loss without reduce flag.\n\n    Args:\n        input (FloatTensor): Input tensor\n        target (FloatTensor): Output tensor\n\n    Returns:\n        [FloatTensor]: L1 distance between input and output\n    """"""\n\n    return torch.mean(torch.abs(input - target))\n\n##\ndef l2_loss(input, target, size_average=True):\n    """""" L2 Loss without reduce flag.\n\n    Args:\n        input (FloatTensor): Input tensor\n        target (FloatTensor): Output tensor\n\n    Returns:\n        [FloatTensor]: L2 distance between input and output\n    """"""\n    if size_average:\n        return torch.mean(torch.pow((input-target), 2))\n    else:\n        return torch.pow((input-target), 2)\n'"
lib/model.py,29,"b'""""""GANomaly\n""""""\n# pylint: disable=C0301,E1101,W0622,C0103,R0902,R0915\n\n##\nfrom collections import OrderedDict\nimport os\nimport time\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom torch.autograd import Variable\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.utils.data\nimport torchvision.utils as vutils\n\nfrom lib.networks import NetG, NetD, weights_init\nfrom lib.visualizer import Visualizer\nfrom lib.loss import l2_loss\nfrom lib.evaluate import evaluate\n\n\nclass BaseModel():\n    """""" Base Model for ganomaly\n    """"""\n    def __init__(self, opt, dataloader):\n        ##\n        # Seed for deterministic behavior\n        self.seed(opt.manualseed)\n\n        # Initalize variables.\n        self.opt = opt\n        self.visualizer = Visualizer(opt)\n        self.dataloader = dataloader\n        self.trn_dir = os.path.join(self.opt.outf, self.opt.name, \'train\')\n        self.tst_dir = os.path.join(self.opt.outf, self.opt.name, \'test\')\n        self.device = torch.device(""cuda:0"" if self.opt.device != \'cpu\' else ""cpu"")\n\n    ##\n    def set_input(self, input:torch.Tensor):\n        """""" Set input and ground truth\n\n        Args:\n            input (FloatTensor): Input data for batch i.\n        """"""\n        with torch.no_grad():\n            self.input.resize_(input[0].size()).copy_(input[0])\n            self.gt.resize_(input[1].size()).copy_(input[1])\n            self.label.resize_(input[1].size())\n\n            # Copy the first batch as the fixed input.\n            if self.total_steps == self.opt.batchsize:\n                self.fixed_input.resize_(input[0].size()).copy_(input[0])\n\n    ##\n    def seed(self, seed_value):\n        """""" Seed \n        \n        Arguments:\n            seed_value {int} -- [description]\n        """"""\n        # Check if seed is default value\n        if seed_value == -1:\n            return\n\n        # Otherwise seed all functionality\n        import random\n        random.seed(seed_value)\n        torch.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        np.random.seed(seed_value)\n        torch.backends.cudnn.deterministic = True\n\n    ##\n    def get_errors(self):\n        """""" Get netD and netG errors.\n\n        Returns:\n            [OrderedDict]: Dictionary containing errors.\n        """"""\n\n        errors = OrderedDict([\n            (\'err_d\', self.err_d.item()),\n            (\'err_g\', self.err_g.item()),\n            (\'err_g_adv\', self.err_g_adv.item()),\n            (\'err_g_con\', self.err_g_con.item()),\n            (\'err_g_enc\', self.err_g_enc.item())])\n\n        return errors\n\n    ##\n    def get_current_images(self):\n        """""" Returns current images.\n\n        Returns:\n            [reals, fakes, fixed]\n        """"""\n\n        reals = self.input.data\n        fakes = self.fake.data\n        fixed = self.netg(self.fixed_input)[0].data\n\n        return reals, fakes, fixed\n\n    ##\n    def save_weights(self, epoch):\n        """"""Save netG and netD weights for the current epoch.\n\n        Args:\n            epoch ([int]): Current epoch number.\n        """"""\n\n        weight_dir = os.path.join(self.opt.outf, self.opt.name, \'train\', \'weights\')\n        if not os.path.exists(weight_dir): os.makedirs(weight_dir)\n\n        torch.save({\'epoch\': epoch + 1, \'state_dict\': self.netg.state_dict()},\n                   \'%s/netG.pth\' % (weight_dir))\n        torch.save({\'epoch\': epoch + 1, \'state_dict\': self.netd.state_dict()},\n                   \'%s/netD.pth\' % (weight_dir))\n\n    ##\n    def train_one_epoch(self):\n        """""" Train the model for one epoch.\n        """"""\n\n        self.netg.train()\n        epoch_iter = 0\n        for data in tqdm(self.dataloader[\'train\'], leave=False, total=len(self.dataloader[\'train\'])):\n            self.total_steps += self.opt.batchsize\n            epoch_iter += self.opt.batchsize\n\n            self.set_input(data)\n            # self.optimize()\n            self.optimize_params()\n\n            if self.total_steps % self.opt.print_freq == 0:\n                errors = self.get_errors()\n                if self.opt.display:\n                    counter_ratio = float(epoch_iter) / len(self.dataloader[\'train\'].dataset)\n                    self.visualizer.plot_current_errors(self.epoch, counter_ratio, errors)\n\n            if self.total_steps % self.opt.save_image_freq == 0:\n                reals, fakes, fixed = self.get_current_images()\n                self.visualizer.save_current_images(self.epoch, reals, fakes, fixed)\n                if self.opt.display:\n                    self.visualizer.display_current_images(reals, fakes, fixed)\n\n        print("">> Training model %s. Epoch %d/%d"" % (self.name, self.epoch+1, self.opt.niter))\n        # self.visualizer.print_current_errors(self.epoch, errors)\n\n    ##\n    def train(self):\n        """""" Train the model\n        """"""\n\n        ##\n        # TRAIN\n        self.total_steps = 0\n        best_auc = 0\n\n        # Train for niter epochs.\n        print("">> Training model %s."" % self.name)\n        for self.epoch in range(self.opt.iter, self.opt.niter):\n            # Train for one epoch\n            self.train_one_epoch()\n            res = self.test()\n            if res[\'AUC\'] > best_auc:\n                best_auc = res[\'AUC\']\n                self.save_weights(self.epoch)\n            self.visualizer.print_current_performance(res, best_auc)\n        print("">> Training model %s.[Done]"" % self.name)\n\n    ##\n    def test(self):\n        """""" Test GANomaly model.\n\n        Args:\n            dataloader ([type]): Dataloader for the test set\n\n        Raises:\n            IOError: Model weights not found.\n        """"""\n        with torch.no_grad():\n            # Load the weights of netg and netd.\n            if self.opt.load_weights:\n                path = ""./output/{}/{}/train/weights/netG.pth"".format(self.name.lower(), self.opt.dataset)\n                pretrained_dict = torch.load(path)[\'state_dict\']\n\n                try:\n                    self.netg.load_state_dict(pretrained_dict)\n                except IOError:\n                    raise IOError(""netG weights not found"")\n                print(\'   Loaded weights.\')\n\n            self.opt.phase = \'test\'\n\n            # Create big error tensor for the test set.\n            self.an_scores = torch.zeros(size=(len(self.dataloader[\'test\'].dataset),), dtype=torch.float32, device=self.device)\n            self.gt_labels = torch.zeros(size=(len(self.dataloader[\'test\'].dataset),), dtype=torch.long,    device=self.device)\n            self.latent_i  = torch.zeros(size=(len(self.dataloader[\'test\'].dataset), self.opt.nz), dtype=torch.float32, device=self.device)\n            self.latent_o  = torch.zeros(size=(len(self.dataloader[\'test\'].dataset), self.opt.nz), dtype=torch.float32, device=self.device)\n\n            # print(""   Testing model %s."" % self.name)\n            self.times = []\n            self.total_steps = 0\n            epoch_iter = 0\n            for i, data in enumerate(self.dataloader[\'test\'], 0):\n                self.total_steps += self.opt.batchsize\n                epoch_iter += self.opt.batchsize\n                time_i = time.time()\n                self.set_input(data)\n                self.fake, latent_i, latent_o = self.netg(self.input)\n\n                error = torch.mean(torch.pow((latent_i-latent_o), 2), dim=1)\n                time_o = time.time()\n\n                self.an_scores[i*self.opt.batchsize : i*self.opt.batchsize+error.size(0)] = error.reshape(error.size(0))\n                self.gt_labels[i*self.opt.batchsize : i*self.opt.batchsize+error.size(0)] = self.gt.reshape(error.size(0))\n                self.latent_i [i*self.opt.batchsize : i*self.opt.batchsize+error.size(0), :] = latent_i.reshape(error.size(0), self.opt.nz)\n                self.latent_o [i*self.opt.batchsize : i*self.opt.batchsize+error.size(0), :] = latent_o.reshape(error.size(0), self.opt.nz)\n\n                self.times.append(time_o - time_i)\n\n                # Save test images.\n                if self.opt.save_test_images:\n                    dst = os.path.join(self.opt.outf, self.opt.name, \'test\', \'images\')\n                    if not os.path.isdir(dst):\n                        os.makedirs(dst)\n                    real, fake, _ = self.get_current_images()\n                    vutils.save_image(real, \'%s/real_%03d.eps\' % (dst, i+1), normalize=True)\n                    vutils.save_image(fake, \'%s/fake_%03d.eps\' % (dst, i+1), normalize=True)\n\n            # Measure inference time.\n            self.times = np.array(self.times)\n            self.times = np.mean(self.times[:100] * 1000)\n\n            # Scale error vector between [0, 1]\n            self.an_scores = (self.an_scores - torch.min(self.an_scores)) / (torch.max(self.an_scores) - torch.min(self.an_scores))\n            # auc, eer = roc(self.gt_labels, self.an_scores)\n            auc = evaluate(self.gt_labels, self.an_scores, metric=self.opt.metric)\n            performance = OrderedDict([(\'Avg Run Time (ms/batch)\', self.times), (\'AUC\', auc)])\n\n            if self.opt.display_id > 0 and self.opt.phase == \'test\':\n                counter_ratio = float(epoch_iter) / len(self.dataloader[\'test\'].dataset)\n                self.visualizer.plot_performance(self.epoch, counter_ratio, performance)\n            return performance\n\n##\nclass Ganomaly(BaseModel):\n    """"""GANomaly Class\n    """"""\n\n    @property\n    def name(self): return \'Ganomaly\'\n\n    def __init__(self, opt, dataloader):\n        super(Ganomaly, self).__init__(opt, dataloader)\n\n        # -- Misc attributes\n        self.epoch = 0\n        self.times = []\n        self.total_steps = 0\n\n        ##\n        # Create and initialize networks.\n        self.netg = NetG(self.opt).to(self.device)\n        self.netd = NetD(self.opt).to(self.device)\n        self.netg.apply(weights_init)\n        self.netd.apply(weights_init)\n\n        ##\n        if self.opt.resume != \'\':\n            print(""\\nLoading pre-trained networks."")\n            self.opt.iter = torch.load(os.path.join(self.opt.resume, \'netG.pth\'))[\'epoch\']\n            self.netg.load_state_dict(torch.load(os.path.join(self.opt.resume, \'netG.pth\'))[\'state_dict\'])\n            self.netd.load_state_dict(torch.load(os.path.join(self.opt.resume, \'netD.pth\'))[\'state_dict\'])\n            print(""\\tDone.\\n"")\n\n        self.l_adv = l2_loss\n        self.l_con = nn.L1Loss()\n        self.l_enc = l2_loss\n        self.l_bce = nn.BCELoss()\n\n        ##\n        # Initialize input tensors.\n        self.input = torch.empty(size=(self.opt.batchsize, 3, self.opt.isize, self.opt.isize), dtype=torch.float32, device=self.device)\n        self.label = torch.empty(size=(self.opt.batchsize,), dtype=torch.float32, device=self.device)\n        self.gt    = torch.empty(size=(opt.batchsize,), dtype=torch.long, device=self.device)\n        self.fixed_input = torch.empty(size=(self.opt.batchsize, 3, self.opt.isize, self.opt.isize), dtype=torch.float32, device=self.device)\n        self.real_label = torch.ones (size=(self.opt.batchsize,), dtype=torch.float32, device=self.device)\n        self.fake_label = torch.zeros(size=(self.opt.batchsize,), dtype=torch.float32, device=self.device)\n        ##\n        # Setup optimizer\n        if self.opt.isTrain:\n            self.netg.train()\n            self.netd.train()\n            self.optimizer_d = optim.Adam(self.netd.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999))\n            self.optimizer_g = optim.Adam(self.netg.parameters(), lr=self.opt.lr, betas=(self.opt.beta1, 0.999))\n\n    ##\n    def forward_g(self):\n        """""" Forward propagate through netG\n        """"""\n        self.fake, self.latent_i, self.latent_o = self.netg(self.input)\n\n    ##\n    def forward_d(self):\n        """""" Forward propagate through netD\n        """"""\n        self.pred_real, self.feat_real = self.netd(self.input)\n        self.pred_fake, self.feat_fake = self.netd(self.fake.detach())\n\n    ##\n    def backward_g(self):\n        """""" Backpropagate through netG\n        """"""\n        self.err_g_adv = self.l_adv(self.netd(self.input)[1], self.netd(self.fake)[1])\n        self.err_g_con = self.l_con(self.fake, self.input)\n        self.err_g_enc = self.l_enc(self.latent_o, self.latent_i)\n        self.err_g = self.err_g_adv * self.opt.w_adv + \\\n                     self.err_g_con * self.opt.w_con + \\\n                     self.err_g_enc * self.opt.w_enc\n        self.err_g.backward(retain_graph=True)\n\n    ##\n    def backward_d(self):\n        """""" Backpropagate through netD\n        """"""\n        # Real - Fake Loss\n        self.err_d_real = self.l_bce(self.pred_real, self.real_label)\n        self.err_d_fake = self.l_bce(self.pred_fake, self.fake_label)\n\n        # NetD Loss & Backward-Pass\n        self.err_d = (self.err_d_real + self.err_d_fake) * 0.5\n        self.err_d.backward()\n\n    ##\n    def reinit_d(self):\n        """""" Re-initialize the weights of netD\n        """"""\n        self.netd.apply(weights_init)\n        print(\'   Reloading net d\')\n\n    def optimize_params(self):\n        """""" Forwardpass, Loss Computation and Backwardpass.\n        """"""\n        # Forward-pass\n        self.forward_g()\n        self.forward_d()\n\n        # Backward-pass\n        # netg\n        self.optimizer_g.zero_grad()\n        self.backward_g()\n        self.optimizer_g.step()\n\n        # netd\n        self.optimizer_d.zero_grad()\n        self.backward_d()\n        self.optimizer_d.step()\n        if self.err_d.item() < 1e-5: self.reinit_d()\n'"
lib/networks.py,2,"b'"""""" Network architectures.\n""""""\n# pylint: disable=W0221,W0622,C0103,R0913\n\n##\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\n\n##\ndef weights_init(mod):\n    """"""\n    Custom weights initialization called on netG, netD and netE\n    :param m:\n    :return:\n    """"""\n    classname = mod.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        mod.weight.data.normal_(0.0, 0.02)\n    elif classname.find(\'BatchNorm\') != -1:\n        mod.weight.data.normal_(1.0, 0.02)\n        mod.bias.data.fill_(0)\n\n###\nclass Encoder(nn.Module):\n    """"""\n    DCGAN ENCODER NETWORK\n    """"""\n\n    def __init__(self, isize, nz, nc, ndf, ngpu, n_extra_layers=0, add_final_conv=True):\n        super(Encoder, self).__init__()\n        self.ngpu = ngpu\n        assert isize % 16 == 0, ""isize has to be a multiple of 16""\n\n        main = nn.Sequential()\n        # input is nc x isize x isize\n        main.add_module(\'initial-conv-{0}-{1}\'.format(nc, ndf),\n                        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False))\n        main.add_module(\'initial-relu-{0}\'.format(ndf),\n                        nn.LeakyReLU(0.2, inplace=True))\n        csize, cndf = isize / 2, ndf\n\n        # Extra layers\n        for t in range(n_extra_layers):\n            main.add_module(\'extra-layers-{0}-{1}-conv\'.format(t, cndf),\n                            nn.Conv2d(cndf, cndf, 3, 1, 1, bias=False))\n            main.add_module(\'extra-layers-{0}-{1}-batchnorm\'.format(t, cndf),\n                            nn.BatchNorm2d(cndf))\n            main.add_module(\'extra-layers-{0}-{1}-relu\'.format(t, cndf),\n                            nn.LeakyReLU(0.2, inplace=True))\n\n        while csize > 4:\n            in_feat = cndf\n            out_feat = cndf * 2\n            main.add_module(\'pyramid-{0}-{1}-conv\'.format(in_feat, out_feat),\n                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))\n            main.add_module(\'pyramid-{0}-batchnorm\'.format(out_feat),\n                            nn.BatchNorm2d(out_feat))\n            main.add_module(\'pyramid-{0}-relu\'.format(out_feat),\n                            nn.LeakyReLU(0.2, inplace=True))\n            cndf = cndf * 2\n            csize = csize / 2\n\n        # state size. K x 4 x 4\n        if add_final_conv:\n            main.add_module(\'final-{0}-{1}-conv\'.format(cndf, 1),\n                            nn.Conv2d(cndf, nz, 4, 1, 0, bias=False))\n\n        self.main = main\n\n    def forward(self, input):\n        if self.ngpu > 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n\n        return output\n\n##\nclass Decoder(nn.Module):\n    """"""\n    DCGAN DECODER NETWORK\n    """"""\n    def __init__(self, isize, nz, nc, ngf, ngpu, n_extra_layers=0):\n        super(Decoder, self).__init__()\n        self.ngpu = ngpu\n        assert isize % 16 == 0, ""isize has to be a multiple of 16""\n\n        cngf, tisize = ngf // 2, 4\n        while tisize != isize:\n            cngf = cngf * 2\n            tisize = tisize * 2\n\n        main = nn.Sequential()\n        # input is Z, going into a convolution\n        main.add_module(\'initial-{0}-{1}-convt\'.format(nz, cngf),\n                        nn.ConvTranspose2d(nz, cngf, 4, 1, 0, bias=False))\n        main.add_module(\'initial-{0}-batchnorm\'.format(cngf),\n                        nn.BatchNorm2d(cngf))\n        main.add_module(\'initial-{0}-relu\'.format(cngf),\n                        nn.ReLU(True))\n\n        csize, _ = 4, cngf\n        while csize < isize // 2:\n            main.add_module(\'pyramid-{0}-{1}-convt\'.format(cngf, cngf // 2),\n                            nn.ConvTranspose2d(cngf, cngf // 2, 4, 2, 1, bias=False))\n            main.add_module(\'pyramid-{0}-batchnorm\'.format(cngf // 2),\n                            nn.BatchNorm2d(cngf // 2))\n            main.add_module(\'pyramid-{0}-relu\'.format(cngf // 2),\n                            nn.ReLU(True))\n            cngf = cngf // 2\n            csize = csize * 2\n\n        # Extra layers\n        for t in range(n_extra_layers):\n            main.add_module(\'extra-layers-{0}-{1}-conv\'.format(t, cngf),\n                            nn.Conv2d(cngf, cngf, 3, 1, 1, bias=False))\n            main.add_module(\'extra-layers-{0}-{1}-batchnorm\'.format(t, cngf),\n                            nn.BatchNorm2d(cngf))\n            main.add_module(\'extra-layers-{0}-{1}-relu\'.format(t, cngf),\n                            nn.ReLU(True))\n\n        main.add_module(\'final-{0}-{1}-convt\'.format(cngf, nc),\n                        nn.ConvTranspose2d(cngf, nc, 4, 2, 1, bias=False))\n        main.add_module(\'final-{0}-tanh\'.format(nc),\n                        nn.Tanh())\n        self.main = main\n\n    def forward(self, input):\n        if self.ngpu > 1:\n            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n        else:\n            output = self.main(input)\n        return output\n\n\n##\nclass NetD(nn.Module):\n    """"""\n    DISCRIMINATOR NETWORK\n    """"""\n\n    def __init__(self, opt):\n        super(NetD, self).__init__()\n        model = Encoder(opt.isize, 1, opt.nc, opt.ngf, opt.ngpu, opt.extralayers)\n        layers = list(model.main.children())\n\n        self.features = nn.Sequential(*layers[:-1])\n        self.classifier = nn.Sequential(layers[-1])\n        self.classifier.add_module(\'Sigmoid\', nn.Sigmoid())\n\n    def forward(self, x):\n        features = self.features(x)\n        features = features\n        classifier = self.classifier(features)\n        classifier = classifier.view(-1, 1).squeeze(1)\n\n        return classifier, features\n\n##\nclass NetG(nn.Module):\n    """"""\n    GENERATOR NETWORK\n    """"""\n\n    def __init__(self, opt):\n        super(NetG, self).__init__()\n        self.encoder1 = Encoder(opt.isize, opt.nz, opt.nc, opt.ngf, opt.ngpu, opt.extralayers)\n        self.decoder = Decoder(opt.isize, opt.nz, opt.nc, opt.ngf, opt.ngpu, opt.extralayers)\n        self.encoder2 = Encoder(opt.isize, opt.nz, opt.nc, opt.ngf, opt.ngpu, opt.extralayers)\n\n    def forward(self, x):\n        latent_i = self.encoder1(x)\n        gen_imag = self.decoder(latent_i)\n        latent_o = self.encoder2(gen_imag)\n        return gen_imag, latent_i, latent_o\n'"
lib/visualizer.py,0,"b'"""""" This file contains Visualizer class based on Facebook\'s visdom.\n\nReturns:\n    Visualizer(): Visualizer class to display plots and images\n""""""\n\n##\nimport os\nimport time\nimport numpy as np\nimport torchvision.utils as vutils\n\n##\nclass Visualizer():\n    """""" Visualizer wrapper based on Visdom.\n\n    Returns:\n        Visualizer: Class file.\n    """"""\n    # pylint: disable=too-many-instance-attributes\n    # Reasonable.\n\n    ##\n    def __init__(self, opt):\n        # self.opt = opt\n        self.display_id = opt.display_id\n        self.win_size = 256\n        self.name = opt.name\n        self.opt = opt\n        if self.opt.display:\n            import visdom\n            self.vis = visdom.Visdom(server=opt.display_server, port=opt.display_port)\n\n        # --\n        # Dictionaries for plotting data and results.\n        self.plot_data = None\n        self.plot_res = None\n\n        # --\n        # Path to train and test directories.\n        self.img_dir = os.path.join(opt.outf, opt.name, \'train\', \'images\')\n        self.tst_img_dir = os.path.join(opt.outf, opt.name, \'test\', \'images\')\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n        if not os.path.exists(self.tst_img_dir):\n            os.makedirs(self.tst_img_dir)\n        # --\n        # Log file.\n        self.log_name = os.path.join(opt.outf, opt.name, \'loss_log.txt\')\n        with open(self.log_name, ""a"") as log_file:\n            now = time.strftime(""%c"")\n            log_file.write(\'================ Training Loss (%s) ================\\n\' % now)\n\n    ##\n    @staticmethod\n    def normalize(inp):\n        """"""Normalize the tensor\n\n        Args:\n            inp ([FloatTensor]): Input tensor\n\n        Returns:\n            [FloatTensor]: Normalized tensor.\n        """"""\n        return (inp - inp.min()) / (inp.max() - inp.min() + 1e-5)\n\n    ##\n    def plot_current_errors(self, epoch, counter_ratio, errors):\n        """"""Plot current errros.\n\n        Args:\n            epoch (int): Current epoch\n            counter_ratio (float): Ratio to plot the range between two epoch.\n            errors (OrderedDict): Error for the current epoch.\n        """"""\n\n        if not hasattr(self, \'plot_data\') or self.plot_data is None:\n            self.plot_data = {\'X\': [], \'Y\': [], \'legend\': list(errors.keys())}\n        self.plot_data[\'X\'].append(epoch + counter_ratio)\n        self.plot_data[\'Y\'].append([errors[k] for k in self.plot_data[\'legend\']])\n        self.vis.line(\n            X=np.stack([np.array(self.plot_data[\'X\'])] * len(self.plot_data[\'legend\']), 1),\n            Y=np.array(self.plot_data[\'Y\']),\n            opts={\n                \'title\': self.name + \' loss over time\',\n                \'legend\': self.plot_data[\'legend\'],\n                \'xlabel\': \'Epoch\',\n                \'ylabel\': \'Loss\'\n            },\n            win=4\n        )\n\n    ##\n    def plot_performance(self, epoch, counter_ratio, performance):\n        """""" Plot performance\n\n        Args:\n            epoch (int): Current epoch\n            counter_ratio (float): Ratio to plot the range between two epoch.\n            performance (OrderedDict): Performance for the current epoch.\n        """"""\n        if not hasattr(self, \'plot_res\') or self.plot_res is None:\n            self.plot_res = {\'X\': [], \'Y\': [], \'legend\': list(performance.keys())}\n        self.plot_res[\'X\'].append(epoch + counter_ratio)\n        self.plot_res[\'Y\'].append([performance[k] for k in self.plot_res[\'legend\']])\n        self.vis.line(\n            X=np.stack([np.array(self.plot_res[\'X\'])] * len(self.plot_res[\'legend\']), 1),\n            Y=np.array(self.plot_res[\'Y\']),\n            opts={\n                \'title\': self.name + \'Performance Metrics\',\n                \'legend\': self.plot_res[\'legend\'],\n                \'xlabel\': \'Epoch\',\n                \'ylabel\': \'Stats\'\n            },\n            win=5\n        )\n\n    ##\n    def print_current_errors(self, epoch, errors):\n        """""" Print current errors.\n\n        Args:\n            epoch (int): Current epoch.\n            errors (OrderedDict): Error for the current epoch.\n            batch_i (int): Current batch\n            batch_n (int): Total Number of batches.\n        """"""\n        # message = \'   [%d/%d] \' % (epoch, self.opt.niter)\n        message = \'   Loss: [%d/%d] \' % (epoch, self.opt.niter)\n        for key, val in errors.items():\n            message += \'%s: %.3f \' % (key, val)\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    ##\n    def print_current_performance(self, performance, best):\n        """""" Print current performance results.\n\n        Args:\n            performance ([OrderedDict]): Performance of the model\n            best ([int]): Best performance.\n        """"""\n        message = \'   \'\n        for key, val in performance.items():\n            message += \'%s: %.3f \' % (key, val)\n        message += \'max AUC: %.3f\' % best\n\n        print(message)\n        with open(self.log_name, ""a"") as log_file:\n            log_file.write(\'%s\\n\' % message)\n\n    def display_current_images(self, reals, fakes, fixed):\n        """""" Display current images.\n\n        Args:\n            epoch (int): Current epoch\n            counter_ratio (float): Ratio to plot the range between two epoch.\n            reals ([FloatTensor]): Real Image\n            fakes ([FloatTensor]): Fake Image\n            fixed ([FloatTensor]): Fixed Fake Image\n        """"""\n        reals = self.normalize(reals.cpu().numpy())\n        fakes = self.normalize(fakes.cpu().numpy())\n        fixed = self.normalize(fixed.cpu().numpy())\n\n        self.vis.images(reals, win=1, opts={\'title\': \'Reals\'})\n        self.vis.images(fakes, win=2, opts={\'title\': \'Fakes\'})\n        self.vis.images(fixed, win=3, opts={\'title\': \'Fixed\'})\n\n    def save_current_images(self, epoch, reals, fakes, fixed):\n        """""" Save images for epoch i.\n\n        Args:\n            epoch ([int])        : Current epoch\n            reals ([FloatTensor]): Real Image\n            fakes ([FloatTensor]): Fake Image\n            fixed ([FloatTensor]): Fixed Fake Image\n        """"""\n        vutils.save_image(reals, \'%s/reals.png\' % self.img_dir, normalize=True)\n        vutils.save_image(fakes, \'%s/fakes.png\' % self.img_dir, normalize=True)\n        vutils.save_image(fixed, \'%s/fixed_fakes_%03d.png\' %(self.img_dir, epoch+1), normalize=True)\n'"
