file_path,api_count,code
__init__.py,0,b''
make_trainset_statistics.py,4,"b'import torch\nfrom torch.utils.data import DataLoader\nfrom pase.dataset import PairWavDataset, DictCollater, MetaWavConcatDataset\nfrom torchvision.transforms import Compose\nfrom pase.transforms import *\nimport argparse\nimport pickle\nfrom train import make_transforms\nimport pase\nfrom pase.utils import *\n\ndef build_dataset_providers(opts):\n\n    assert len(opts.data_root) > 0, (\n        ""Expected at least one data_root argument""\n    )\n\n    assert len(opts.data_root) == len(opts.data_cfg), (\n        ""Provide same number of data_root and data_cfg arguments""\n    )\n\n    if len(opts.data_root) == 1 and \\\n        len(opts.dataset) < 1:\n        opts.dataset.append(\'PairWavDataset\')\n\n    assert len(opts.data_root) == len(opts.dataset), (\n        ""Provide same number of data_root and dataset arguments""\n    )\n\n    minions_cfg = worker_parser(opts.net_cfg)\n    trans, batch_keys = make_transforms(opts.chunk_size, minions_cfg,\n                                        opts.hop_size)\n    """"""\n    trans = Compose([\n        ToTensor(),\n        MIChunkWav(opts.chunk_size),\n        #LPS(hop=opts.hop_size, win=opts.win_size),\n        #Gammatone(hop=opts.hop_size),\n        #LPC(hop=opts.hop_size),\n        #FBanks(hop=opts.hop_size),\n        #MFCC(hop=opts.hop_size, win=opts.win_size),\n        #KaldiMFCC(kaldi_root=opts.kaldi_root, hop=opts.hop_size, win=opts.win_size),\n        #KaldiPLP(kaldi_root=opts.kaldi_root, hop=opts.hop_size, win=opts.win_size),\n        #Prosody(hop=opts.hop_size)\n        LPS(hop=opts.LPS_hop,win=opts.LPS_win,der_order=opts.LPS_der_order),\n        Gammatone(hop=opts.gammatone_hop,win=opts.gammatone_win,der_order=opts.gammatone_der_order),\n        #LPC(hop=opts.LPC_hop),\n        FBanks(hop=opts.fbanks_hop,win=opts.fbanks_win,der_order=opts.fbanks_der_order),\n        MFCC(hop=opts.mfccs_hop,win=opts.mfccs_win,order=opts.mfccs_order,der_order=opts.mfccs_der_order),\n        #MFCC_librosa(hop=opts.mfccs_librosa_hop,win=opts.mfccs_librosa_win,order=opts.mfccs_librosa_order,der_order=opts.mfccs_librosa_der_order,n_mels=opts.mfccs_librosa_n_mels,htk=opts.mfccs_librosa_htk),\n        #KaldiMFCC(kaldi_root=opts.kaldi_root, hop=opts.kaldimfccs_hop, win=opts.kaldimfccs_win,num_mel_bins=opts.kaldimfccs_num_mel_bins,num_ceps=opts.kaldimfccs_num_ceps,der_order=opts.kaldimfccs_der_order),\n        #KaldiPLP(kaldi_root=opts.kaldi_root, hop=opts.kaldiplp_hop, win=opts.kaldiplp_win),\n        Prosody(hop=opts.prosody_hop, win=opts.prosody_win, der_order=opts.prosody_der_order)\n    ])\n    """"""\n\n    dsets = []\n    for idx in range(len(opts.data_root)):\n        dataset = getattr(pase.dataset, opts.dataset[idx])\n        dset = dataset(opts.data_root[idx], opts.data_cfg[idx], \'train\',\n                       transform=trans, ihm2sdm=opts.ihm2sdm)\n        #dset = PairWavDataset(opts.data_root[idx], opts.data_cfg[idx], \'train\',\n        #                 transform=trans)\n        dsets.append(dset)\n\n    if len(dsets) > 1:\n        return MetaWavConcatDataset(dsets), batch_keys\n    else:\n        return dsets[0], batch_keys\n\ndef extract_stats(opts):\n    dset = build_dataset_providers(opts)\n    collater_keys = dset[-1]\n    dset = dset[0]\n    collater = DictCollater()\n    collater.batching_keys.extend(collater_keys)\n    dloader = DataLoader(dset, batch_size = 100,\n                         shuffle=True, collate_fn=collater,\n                         num_workers=opts.num_workers)\n    # Compute estimation of bpe. As we sample chunks randomly, we\n    # should say that an epoch happened after seeing at least as many\n    # chunks as total_train_wav_dur // chunk_size\n    bpe = (dset.total_wav_dur // opts.chunk_size) // 500\n    data = {}\n    # run one epoch of training data to extract z-stats of minions\n    for bidx, batch in enumerate(dloader, start=1):\n        print(\'Bidx: {}/{}\'.format(bidx, bpe))\n        for k, v in batch.items():\n            if k in opts.exclude_keys:\n                continue\n            if k not in data:\n                data[k] = []\n            data[k].append(v)\n\n        if bidx >= opts.max_batches:\n            break\n\n    stats = {}\n    data = dict((k, torch.cat(v)) for k, v in data.items())\n    for k, v in data.items():\n        stats[k] = {\'mean\':torch.mean(torch.mean(v, dim=2), dim=0),\n                    \'std\':torch.std(torch.std(v, dim=2), dim=0)}\n    with open(opts.out_file, \'wb\') as stats_f:\n        pickle.dump(stats, stats_f)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_root\', action=\'append\', \n                        default=[])\n    parser.add_argument(\'--data_cfg\', action=\'append\', \n                        default=[])\n    parser.add_argument(\'--dataset\', action=\'append\', \n                        default=[])\n    parser.add_argument(\'--exclude_keys\', type=str, nargs=\'+\', \n                        default=[\'chunk\', \'chunk_rand\', \'chunk_ctxt\'])\n    parser.add_argument(\'--num_workers\', type=int, default=1)\n    parser.add_argument(\'--chunk_size\', type=int, default=16000)\n    parser.add_argument(\'--max_batches\', type=int, default=20)\n    parser.add_argument(\'--out_file\', type=str)\n    parser.add_argument(\'--hop_size\', type=int, default=160)\n    #parser.add_argument(\'--win_size\', type=int, default=400)\n    \n    # setting hop/wlen for each features\n    parser.add_argument(\'--LPS_hop\', type=int, default=160)\n    parser.add_argument(\'--LPS_win\', type=int, default=400)\n    parser.add_argument(\'--LPS_der_order\', type=int, default=0)\n    #parser.add_argument(\'--gammatone_hop\', type=int, default=160)\n    parser.add_argument(\'--gammatone_win\', type=int, default=400)\n    parser.add_argument(\'--gammatone_der_order\', type=int, default=0)\n    #parser.add_argument(\'--LPC_hop\', type=int, default=160)\n    parser.add_argument(\'--LPC_win\', type=int, default=400)\n    #parser.add_argument(\'--fbanks_hop\', type=int, default=160)\n    parser.add_argument(\'--fbanks_win\', type=int, default=400)\n    parser.add_argument(\'--fbanks_der_order\', type=int, default=0)\n    #parser.add_argument(\'--mfccs_hop\', type=int, default=160)\n    parser.add_argument(\'--mfccs_win\', type=int, default=400)\n    parser.add_argument(\'--mfccs_order\', type=int, default=20)\n    parser.add_argument(\'--mfccs_der_order\', type=int, default=0)\n    #parser.add_argument(\'--prosody_hop\', type=int, default=160)\n    parser.add_argument(\'--prosody_win\', type=int, default=400)\n    parser.add_argument(\'--prosody_der_order\', type=int, default=0)\n    #parser.add_argument(\'--kaldimfccs_hop\', type=int, default=160)\n    parser.add_argument(\'--kaldimfccs_win\', type=int, default=400)\n    parser.add_argument(\'--kaldimfccs_der_order\', type=int, default=0)\n    parser.add_argument(\'--kaldimfccs_num_mel_bins\', type=int, default=20)\n    parser.add_argument(\'--kaldimfccs_num_ceps\', type=int, default=20)\n    #parser.add_argument(\'--kaldiplp_hop\', type=int, default=160)\n    parser.add_argument(\'--kaldiplp_win\', type=int, default=400)\n    \n    #parser.add_argument(\'--mfccs_librosa_hop\', type=int, default=160)\n    parser.add_argument(\'--mfccs_librosa_win\', type=int, default=400)\n    parser.add_argument(\'--mfccs_librosa_order\', type=int, default=20)\n    parser.add_argument(\'--mfccs_librosa_der_order\', type=int, default=0)\n    parser.add_argument(\'--mfccs_librosa_n_mels\', type=int, default=40)\n    parser.add_argument(\'--mfccs_librosa_htk\', type=int, default=True)\n    parser.add_argument(\'--net_cfg\', type=str, default=None)\n\n    \n    parser.add_argument(\'--ihm2sdm\', type=str, default=None,\n                        help=\'Relevant only to ami-like dataset providers\')\n    parser.add_argument(\'--kaldi_root\', type=str, default=None,\n                        help=\'Absolute path to kaldi installation. Possibly of use for feature related bits.\')\n    opts = parser.parse_args()\n    extract_stats(opts)\n'"
precompute_aco_data.py,2,"b'from pase.dataset import WavDataset, DictCollater, uttwav_collater\nfrom torchvision.transforms import Compose\nfrom torch.utils.data import DataLoader\nfrom pase.transforms import *\nimport argparse\nfrom pase.utils import pase_parser\nimport tqdm\nimport os\n\n\ndef make_transforms(opts, minions_cfg):\n    trans = [ToTensor()]\n    znorm = False\n    for minion in minions_cfg:\n        name = minion[\'name\']\n        if name == \'mi\' or name == \'cmi\' or name == \'spc\':\n            continue\n        elif name == \'lps\':\n            znorm = True\n            trans.append(LPS(opts.nfft, hop=160, win=400))\n        elif name == \'mfcc\':\n            znorm = True\n            trans.append(MFCC(hop=160))\n        elif name == \'prosody\':\n            znorm = True\n            trans.append(Prosody(hop=160, win=400))\n        elif name == \'chunk\':\n            znorm = True\n        else:\n            raise TypeError(\'Unrecognized module \\""{}\\""\'\n                            \'whilst building transfromations\'.format(name))\n    if znorm:\n        trans.append(ZNorm(opts.stats))\n    trans = Compose(trans)\n    return trans\n\ndef extract_acos(dloader, transform, save_path, split):\n    for bidx, batch in tqdm.tqdm(enumerate(dloader, start=1),\n                                 total=len(dloader)):\n        # transform the wav batch element\n        wav, uttname, _ = batch\n        uttname = os.path.splitext(os.path.basename(uttname[0]))[0]\n        aco = transform(wav.view(-1))\n        for k in aco.keys():\n            if \'uttname\' in k or \'raw\' in k or \'chunk\' in k:\n                continue\n            save_dir = os.path.join(save_path, split, k)\n            if not os.path.exists(save_dir):\n                os.makedirs(save_dir)\n            kname = uttname + \'.{}\'.format(k)\n            torch.save(aco[k], os.path.join(save_dir,\n                                            kname))\n\ndef main(opts):\n    minions_cfg = pase_parser(opts.net_cfg)\n    trans = make_transforms(opts, minions_cfg)\n    # Build Dataset(s) and DataLoader(s)\n    dset = WavDataset(opts.data_root, opts.data_cfg, \'train\',\n                      preload_wav=False,\n                      return_uttname=True)\n    dloader = DataLoader(dset, batch_size=1,\n                         shuffle=True, collate_fn=uttwav_collater,\n                         num_workers=opts.num_workers)\n    va_dset = WavDataset(opts.data_root, opts.data_cfg,\n                         \'valid\', \n                          preload_wav=False,\n                          return_uttname=True)\n    va_dloader = DataLoader(va_dset, batch_size=1,\n                            shuffle=False, collate_fn=uttwav_collater,\n                            num_workers=opts.num_workers)\n    extract_acos(dloader, trans, opts.save_path, \'train\')\n    extract_acos(va_dloader, trans, opts.save_path, \'valid\')\n\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_root\', type=str, \n                        default=\'data/LibriSpeech/Librispeech_spkid_sel\')\n    parser.add_argument(\'--data_cfg\', type=str, \n                        default=\'data/librispeech_data.cfg\')\n    parser.add_argument(\'--stats\', type=str, default=\'data/librispeech_stats.pkl\')\n    parser.add_argument(\'--save_path\', type=str, default=\'data/Librispeech/\')\n    parser.add_argument(\'--net_cfg\', type=str, default=\'cfg/all.cfg\')\n    parser.add_argument(\'--nfft\', type=int, default=2048)\n    parser.add_argument(\'--num_workers\', type=int, default=0)\n     \n    opts = parser.parse_args()\n    if not os.path.exists(opts.save_path):\n        os.makedirs(opts.save_path)\n    main(opts)\n'"
setup.py,0,"b""from setuptools import setup\n\nsetup(\n    name='PASE',\n    version='0.1.1-dev',\n    packages=['pase', 'pase.models', 'pase.models.WorkerScheduler', 'pase.models.Minions'],\n)\n"""
train.py,8,"b'# from pase.models.core import Waveminionet\n\nimport warnings\n# Pawel: this one is for nightly build of pytorch, as it\n# spits out massive number of warnings\nwarnings.filterwarnings(\'ignore\')\n\nimport librosa\nfrom pase.models.modules import VQEMA\nfrom pase.dataset import PairWavDataset, DictCollater, MetaWavConcatDataset\nfrom pase.models.WorkerScheduler.trainer import trainer\n#from torchvision.transforms import Compose\nfrom pase.transforms import *\nfrom pase.losses import *\nfrom pase.utils import pase_parser, worker_parser\nimport pase\nfrom torch.utils.data import DataLoader\nimport torch\nimport pickle\nimport torch.nn as nn\nimport numpy as np\nimport argparse\nimport os\nimport json\nimport random\ntorch.backends.cudnn.benchmark = True\n\n\ndef str2bool(v):\n  return v.lower() in (""yes"", ""true"", ""t"", ""1"")\n\ndef str2None(v):\n    if v.lower() in (\'none\'):\n        return None\n    return v\n\ndef make_transforms(chunk_size, workers_cfg, hop,\n                    random_scale=False,\n                    stats=None, trans_cache=None):\n    trans = [ToTensor()]\n    keys = [\'totensor\']\n    # go through all minions first to check whether\n    # there is MI or not to make chunker\n    mi = False\n    for type, minions_cfg in workers_cfg.items():\n        for minion in minions_cfg:\n            if \'mi\' in minion[\'name\']:\n                mi = True\n    if mi:\n        trans.append(MIChunkWav(chunk_size, random_scale=random_scale))\n    else:\n        trans.append(SingleChunkWav(chunk_size, random_scale=random_scale))\n\n    collater_keys = []\n    znorm = False\n    for type, minions_cfg in workers_cfg.items():\n        for minion in minions_cfg:\n            name = minion[\'name\']\n            if name in collater_keys:\n                raise ValueError(\'Duplicated key {} in minions\'.format(name))\n            collater_keys.append(name)\n            # look for the transform config if available \n            # in this minion\n            tr_cfg=minion.pop(\'transform\', {})\n            tr_cfg[\'hop\'] = hop\n            if name == \'mi\' or name == \'cmi\' or name == \'spc\' or \\\n               name == \'overlap\' or name == \'gap\' or \'regu\' in name:\n                continue\n            elif \'lps\' in name:\n                znorm = True\n                # copy the minion name into the transform name\n                tr_cfg[\'name\'] = name\n                #trans.append(LPS(opts.nfft, hop=opts.LPS_hop, win=opts.LPS_win, der_order=opts.LPS_der_order))\n                trans.append(LPS(**tr_cfg))\n            elif \'gtn\' in name:\n                znorm = True\n                tr_cfg[\'name\'] = name\n                trans.append(Gammatone(**tr_cfg))\n                #trans.append(Gammatone(opts.gtn_fmin, opts.gtn_channels, \n                #                       hop=opts.gammatone_hop, win=opts.gammatone_win,der_order=opts.gammatone_der_order))\n            elif \'lpc\' in name:\n                znorm = True\n                tr_cfg[\'name\'] = name\n                trans.append(LPC(**tr_cfg))\n                #trans.append(LPC(opts.lpc_order, hop=opts.LPC_hop,\n                #                 win=opts.LPC_win))\n            elif \'fbank\' in name:\n                znorm = True\n                tr_cfg[\'name\'] = name\n                trans.append(FBanks(**tr_cfg))\n                #trans.append(FBanks(n_filters=opts.fbank_filters, \n                #                    n_fft=opts.nfft,\n                #                    hop=opts.fbanks_hop,\n                #                    win=opts.fbanks_win,\n                #                    der_order=opts.fbanks_der_order))\n            \n            elif \'mfcc_librosa\' in name:\n                znorm = True\n                tr_cfg[\'name\'] = name\n                trans.append(MFCC_librosa(**tr_cfg))\n                #trans.append(MFCC_librosa(hop=opts.mfccs_librosa_hop, win=opts.mfccs_librosa_win, order=opts.mfccs_librosa_order, der_order=opts.mfccs_librosa_der_order, n_mels=opts.mfccs_librosa_n_mels, htk=opts.mfccs_librosa_htk))\n            elif \'mfcc\' in name:\n                znorm = True\n                tr_cfg[\'name\'] = name\n                trans.append(MFCC(**tr_cfg))\n                #trans.append(MFCC(hop=opts.mfccs_hop, win=opts.mfccs_win, order=opts.mfccs_order, der_order=opts.mfccs_der_order))\n            elif \'prosody\' in name:\n                znorm = True\n                tr_cfg[\'name\'] = name\n                trans.append(Prosody(**tr_cfg))\n                #trans.append(Prosody(hop=opts.prosody_hop, win=opts.prosody_win, der_order=opts.prosody_der_order))\n            elif name == \'chunk\' or name == \'cchunk\':\n                znorm = False\n            elif \'kaldimfcc\' in name:\n                znorm = True\n                tr_cfg[\'name\'] = name\n                trans.append(KaldiMFCC(**tr_cfg))\n                #trans.append(KaldiMFCC(kaldi_root=opts.kaldi_root, hop=opts.kaldimfccs_hop, win=opts.kaldimfccs_win,num_mel_bins=opts.kaldimfccs_num_mel_bins,num_ceps=opts.kaldimfccs_num_ceps,der_order=opts.kaldimfccs_der_order))\n            elif ""kaldiplp"" in name:\n                znorm = True\n                tr_cfg[\'name\'] = name\n                trans.append(KaldiPLP(**tr_cfg))\n                #trans.append(KaldiPLP(kaldi_root=opts.kaldi_root, hop=opts.kaldiplp_hop, win=opts.kaldiplp_win))\n            else:\n                raise TypeError(\'Unrecognized module \\""{}\\""\'\n                                \'whilst building transfromations\'.format(name))\n            keys.append(name)\n    if znorm and stats is not None:\n        trans.append(ZNorm(stats))\n        keys.append(\'znorm\')\n    if trans_cache is None:\n        trans = Compose(trans)\n    else:\n        print (keys, trans)\n        trans = CachedCompose(trans, keys, trans_cache)\n    return trans, collater_keys\n\n\ndef config_zerospeech(noises_dir=None,\n                      noises_snrs=[0, 5, 10]):\n    trans = SimpleAdditive(noises_dir, noises_snrs)\n    return trans\n\ndef build_dataset_providers(opts, minions_cfg):\n\n    dr = len(opts.data_root)\n    dc = len(opts.data_cfg)\n\n    if dr > 1 or dc > 1:\n        assert dr == dc, (\n            ""Specced at least one repeated option for data_root or data_cfg.""\n            ""This assumes multiple datasets, and their resp configs should be matched.""\n            ""Currently got {} data_root and {} data_cfg options"".format(dr, dc)\n        )\n        if opts.dtrans_cfg is not None and len(opts.dtrans_cfg) > 0:\n            assert dr == len(opts.dtrans_cfg), (\n                ""Spec one dtrans_cfg per data_root (can be the same) or None""\n            )\n        #make sure defaults for dataset has been properly set\n        if len(opts.dataset) < dr:\n            print (\'Provided fewer dataset options than data_root. Repeating default.\')\n            for _ in range(len(opts.datasets), dr):\n                opts.dataset.append(\'LibriSpeechSegTupleWavDataset\')\n        if len(opts.zero_speech_p) < dr:\n            print (\'Provided fewer zero_speech_p options than data_roots. Repeating default.\')\n            for _ in range(len(opts.zero_speech_p), dr):\n                opts.zero_speech_p.append(0)\n\n    #this is to set default in proper way, as argparse\n    #uses whatever is set as default in append mode as\n    #initial values (i.e. do not override them)\n    if len(opts.dataset) < 1:\n        opts.dataset.append(\'LibriSpeechSegTupleWavDataset\')\n\n    #TODO: allow for different base transforms for different datasets\n    trans, batch_keys = make_transforms(opts.chunk_size, minions_cfg,\n                                        opts.hop,\n                                        opts.random_scale,\n                                        opts.stats, opts.trans_cache)\n    print(trans)\n\n    dsets, va_dsets = [], []\n    for idx in range(dr):\n        print (\'Preparing dset for {}\'.format(opts.data_root[idx]))\n        if opts.dtrans_cfg is not None and \\\n            len(opts.dtrans_cfg) > 0 and \\\n            str2None(opts.dtrans_cfg[idx]) is not None :\n            with open(opts.dtrans_cfg[idx], \'r\') as dtr_cfg:\n                dtr = json.load(dtr_cfg)\n                #dtr[\'trans_p\'] = opts.distortion_p\n                dist_trans = config_distortions(**dtr)\n                print(dist_trans)\n        else:\n            dist_trans = None\n        if opts.zerospeech_cfg is not None \\\n            and len(opts.zero_speech_p) > 0 \\\n              and opts.zero_speech_p[idx] > 0:\n            with open(opts.zerospeech_cfg[idx], \'r\') as zsp_cfg:\n                ztr = json.load(zsp_cfg)\n                zp_trans = config_zerospeech(**ztr)\n                print(zp_trans)\n        else:\n            zp_trans = None\n        # Build Dataset(s) and DataLoader(s)\n        dataset = getattr(pase.dataset, opts.dataset[idx])\n        print (\'Dataset name {} and opts {}\'.format(dataset, opts.dataset[idx]))\n        dset = dataset(opts.data_root[idx], opts.data_cfg[idx], \'train\',\n                       transform=trans,\n                       noise_folder=opts.noise_folder,\n                       whisper_folder=opts.whisper_folder,\n                       distortion_probability=opts.distortion_p,\n                       distortion_transforms=dist_trans,\n                       zero_speech_p=opts.zero_speech_p[idx],\n                       zero_speech_transform=zp_trans,\n                       preload_wav=opts.preload_wav,\n                       ihm2sdm=opts.ihm2sdm)\n\n        dsets.append(dset)\n\n        if opts.do_eval:\n            va_dset = dataset(opts.data_root[idx], opts.data_cfg[idx],\n                              \'valid\', transform=trans,\n                              noise_folder=opts.noise_folder,\n                              whisper_folder=opts.whisper_folder,\n                              distortion_probability=opts.distortion_p,\n                              distortion_transforms=dist_trans,\n                              zero_speech_p=opts.zero_speech_p[idx],\n                              zero_speech_transform=zp_trans,\n                              preload_wav=opts.preload_wav,\n                              ihm2sdm=opts.ihm2sdm)\n            va_dsets.append(va_dset)\n\n    ret = None\n    if len(dsets) > 1:\n        ret = (MetaWavConcatDataset(dsets), )\n        if opts.do_eval:\n            ret = ret + (MetaWavConcatDataset(va_dsets), )\n    else:\n        ret = (dsets[0], )\n        if opts.do_eval:\n            ret = ret + (va_dsets[0], )\n\n    if opts.do_eval is False or len(va_dsets) == 0:\n        ret = ret + (None, )\n\n    return ret, batch_keys\n\ndef train(opts):\n    CUDA = True if torch.cuda.is_available() and not opts.no_cuda else False\n    device = \'cuda\' if CUDA else \'cpu\'\n    num_devices = 1\n    np.random.seed(opts.seed)\n    random.seed(opts.seed)\n    torch.manual_seed(opts.seed)\n    if CUDA:\n        torch.cuda.manual_seed_all(opts.seed)\n        num_devices = torch.cuda.device_count()\n        print(\'[*] Using CUDA {} devices\'.format(num_devices))\n    else:\n        print(\'[!] Using CPU\')\n    print(\'Seeds initialized to {}\'.format(opts.seed))\n\n    #torch.autograd.set_detect_anomaly(True)\n\n    # --------------------- \n    # Build Model\n\n    minions_cfg = worker_parser(opts.net_cfg)\n    #make_transforms(opts, minions_cfg)\n    opts.random_scale = str2bool(opts.random_scale)\n\n    dsets, collater_keys = build_dataset_providers(opts, minions_cfg)\n    dset, va_dset = dsets\n    # Build collater, appending the keys from the loaded transforms to the\n    # existing default ones\n    collater = DictCollater()\n    collater.batching_keys.extend(collater_keys)\n    dloader = DataLoader(dset, batch_size=opts.batch_size,\n                         shuffle=True, collate_fn=collater,\n                         num_workers=opts.num_workers,drop_last=True,\n                         pin_memory=CUDA)\n    # Compute estimation of bpe. As we sample chunks randomly, we\n    # should say that an epoch happened after seeing at least as many\n    # chunks as total_train_wav_dur // chunk_size\n    bpe = (dset.total_wav_dur // opts.chunk_size) // opts.batch_size\n    print (""Dataset has a total {} hours of training data"".format(dset.total_wav_dur/16000/3600.0))\n    opts.bpe = bpe\n    if opts.do_eval:\n        assert va_dset is not None, (\n            ""Asked to do validation, but failed to build validation set""\n        )\n        va_dloader = DataLoader(va_dset, batch_size=opts.batch_size,\n                                shuffle=True, collate_fn=DictCollater(),\n                                num_workers=opts.num_workers,drop_last=True,\n                                pin_memory=CUDA)\n        va_bpe = (va_dset.total_wav_dur // opts.chunk_size) // opts.batch_size\n        opts.va_bpe = va_bpe\n    else:\n        va_dloader = None\n    # fastet lr to MI\n    #opts.min_lrs = {\'mi\':0.001}\n\n    if opts.fe_cfg is not None:\n        with open(opts.fe_cfg, \'r\') as fe_cfg_f:\n            print(fe_cfg_f)\n            fe_cfg = json.load(fe_cfg_f)\n            print(fe_cfg)\n    else:\n        fe_cfg = None\n\n    # load config file for attention blocks\n    if opts.att_cfg:\n        with open(opts.att_cfg) as f:\n            att_cfg = json.load(f)\n            print(att_cfg)\n    else:\n        att_cfg = None\n\n    print(str2bool(opts.tensorboard))\n    Trainer = trainer(frontend_cfg=fe_cfg,\n                      att_cfg=att_cfg,\n                      minions_cfg=minions_cfg,\n                      cfg=vars(opts),\n                      backprop_mode=opts.backprop_mode,\n                      lr_mode=opts.lr_mode,\n                      tensorboard=str2bool(opts.tensorboard),\n                      device=device)\n    print(Trainer.model)\n    print(\'Frontend params: \', Trainer.model.frontend.describe_params())\n\n    Trainer.model.to(device)\n\n    Trainer.train_(dloader, device=device, valid_dataloader=va_dloader)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_root\', action=\'append\', \n                        default=[])\n    parser.add_argument(\'--data_cfg\', action=\'append\', \n                        default=[])\n    parser.add_argument(\'--dtrans_cfg\', action=\'append\', default=[],\n                        help=\'Distortion transform to apply, note in case of\'\n                              \'mutliple datasets, provide config multiple times\')\n    parser.add_argument(\'--zerospeech_cfg\', action=\'append\', default=None)\n    parser.add_argument(\'--zero_speech_p\', action=\'append\', type=float,\n                        default=[0.0])\n    parser.add_argument(\'--dataset\', action=\'append\',\n                        default=[],\n                        help=\'Dataset to be used: \'\n                             \'(1) PairWavDataset, \'\n                             \'(2) LibriSpeechSegTupleWavDataset, \'\n                             \'(Def: LibriSpeechSegTupleWavDataset.)\'\n                             \'When used multiple times, datasets get\'\n                             \'concatenated with ConcatDataset\')\n    parser.add_argument(\'--stats\', type=str,\n                        default=\'data/librispeech_stats.pkl\',\n                        help=\'Stats file\')\n\n    parser.add_argument(\'--noise_folder\', type=str, default=None)\n    parser.add_argument(\'--whisper_folder\', type=str, default=None)\n    parser.add_argument(\'--distortion_p\', type=float, default=0.4)\n    parser.add_argument(\'--net_ckpt\', type=str, default=None,\n                        help=\'Ckpt to initialize the full network \'\n                             \'(Def: None).\')\n    parser.add_argument(\'--net_cfg\', type=str, help=""Workers configuration file (see cfg/workers/*.cfg)"",\n                        default=None)\n    parser.add_argument(\'--fe_cfg\', help=""Frontend (main) model definition, see cfg/frontend/*.cfg - PASE or PASE+"", type=str, default=None)\n    #parser.add_argument(\'--do_eval\', action=\'store_true\', default=False)\n    parser.add_argument(\'--pretrained_ckpt\', type=str, default=None)\n    parser.add_argument(\'--save_path\', type=str, default=\'ckpt\')\n    parser.add_argument(\'--max_ckpts\', type=int, default=5)\n    parser.add_argument(\'--trans_cache\', type=str,\n                        default=None)\n    parser.add_argument(\'--num_workers\', type=int, default=2)\n    parser.add_argument(\'--seed\', type=int, default=2)\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False)\n    parser.add_argument(\'--random_scale\', type=str, default=\'False\', help=""random scaling of noise"")\n    parser.add_argument(\'--chunk_size\', type=int, default=16000)\n    parser.add_argument(\'--log_freq\', type=int, default=100)\n    parser.add_argument(\'--epoch\', type=int, default=1000)\n    parser.add_argument(\'--nfft\', type=int, default=2048)\n    parser.add_argument(\'--fbank_filters\', type=int, default=40)\n    parser.add_argument(\'--lpc_order\', type=int, default=25)\n    parser.add_argument(\'--gtn_channels\', type=int, default=40)\n    parser.add_argument(\'--gtn_fmin\', type=int, default=100)\n    parser.add_argument(\'--batch_size\', type=int, default=100)\n    parser.add_argument(\'--hidden_size\', type=int, default=256)\n    parser.add_argument(\'--hidden_layers\', type=int, default=2)\n    parser.add_argument(\'--fe_opt\', type=str, default=\'Adam\')\n    parser.add_argument(\'--min_opt\', type=str, default=\'Adam\')\n    parser.add_argument(\'--lrdec_step\', type=int, default=30,\n                        help=\'Number of epochs to scale lr (Def: 30).\')\n    parser.add_argument(\'--lrdecay\', type=float, default=0,\n                        help=\'Learning rate decay factor with \'\n                             \'cross validation. After patience \'\n                             \'epochs, lr decays this amount in \'\n                             \'all optimizers. \' \n                             \'If zero, no decay is applied (Def: 0).\')\n    parser.add_argument(\'--dout\', type=float, default=0.2)\n    parser.add_argument(\'--fe_lr\', type=float, default=0.0001)\n    parser.add_argument(\'--min_lr\', type=float, default=0.0004)\n    parser.add_argument(\'--z_lr\', type=float, default=0.0004)\n    parser.add_argument(\'--rndmin_train\', action=\'store_true\',\n                        default=False)\n    parser.add_argument(\'--adv_loss\', type=str, default=\'BCE\',\n                        help=\'BCE or L2\')\n    parser.add_argument(\'--warmup\', type=int, default=1000000000,\n                        help=\'Epoch to begin applying z adv \'\n                             \'(Def: 1000000000 to not apply it).\')\n    parser.add_argument(\'--zinit_weight\', type=float, default=1)\n    parser.add_argument(\'--zinc\', type=float, default=0.0002)\n    parser.add_argument(\'--vq_K\', type=int, default=50,\n                        help=\'Number of K embeddings in VQ-enc. \'\n                             \'(Def: 50).\')\n    parser.add_argument(\'--log_grad_keys\', type=str, nargs=\'+\',\n                        default=[])\n    parser.add_argument(\'--vq\', action=\'store_true\', default=False,\n                        help=\'Do VQ quantization of enc output (Def: False).\')\n    parser.add_argument(\'--cchunk_prior\', action=\'store_true\', default=False)\n    parser.add_argument(\'--sup_exec\', type=str, default=None)\n    \n    parser.add_argument(\'--sup_freq\', type=int, default=1)\n    parser.add_argument(\'--preload_wav\', action=\'store_true\', default=False,\n                        help=\'Preload wav files in Dataset (Def: False).\')\n    parser.add_argument(\'--cache_on_load\', action=\'store_true\', default=False,\n                        help=\'Argument to activate cache loading on the fly \'\n                             \'for the wav files in datasets (Def: False).\')\n\n    parser.add_argument(\'--no_continue\', type=str, default=""False"",help=""whether continue the training"")\n    parser.add_argument(\'--lr_mode\', type=str, default=\'step\', help=\'learning rate scheduler mode\')\n    parser.add_argument(\'--att_cfg\', type=str, help=\'Path to the config file of attention blocks\')\n    parser.add_argument(\'--avg_factor\', type=float, default=0, help=""running average factor for option running_avg for attention"")\n    parser.add_argument(\'--att_mode\', type=str, help=\'options for attention block\')\n    parser.add_argument(\'--tensorboard\', type=str, default=\'True\', help=\'use tensorboard for logging\')\n    parser.add_argument(\'--backprop_mode\', type=str, default=\'base\',help=\'backprop policy can be choose from: [base, select_one, select_half]\')\n    parser.add_argument(\'--dropout_rate\', type=float, default=0.5, help=""drop out rate for workers"")\n    parser.add_argument(\'--delta\', type=float, help=""delta for hyper volume loss scheduling"")\n    parser.add_argument(\'--temp\', type=float, help=""temp for softmax or adaptive losss"")\n    parser.add_argument(\'--alpha\', type=float, help=""alpha for adaptive loss"")\n    parser.add_argument(\'--att_K\', type=int, help=""top K indices to select for attention"")\n\n    #this one is for AMI/ICSI parallel like datasets, so one can selectively pick sdm chunks \n    parser.add_argument(\'--ihm2sdm\', type=str, default=None,\n                            help=""Pick random of one of these channels.""\n                                 ""Can be empty or None in which case only""\n                                 ""ihm channel gets used for chunk and cchunk"")\n    #some transformations rely on kaldi to extract feats\n    parser.add_argument(\'--kaldi_root\', type=str, default=None,\n                        help=\'Absolute path to kaldi installation. Possibly of use for feature related bits.\')\n    parser.add_argument(\'--hop\', type=int, default=160)\n\n    opts = parser.parse_args()\n    # enforce evaluation for now, no option to disable\n    opts.do_eval = True\n    opts.ckpt_continue = not str2bool(opts.no_continue)\n    if opts.net_cfg is None:\n        raise ValueError(\'Please specify a net_cfg file\')\n\n    if not os.path.exists(opts.save_path):\n        os.makedirs(opts.save_path)\n\n    with open(os.path.join(opts.save_path, \'train.opts\'), \'w\') as opts_f:\n        opts_f.write(json.dumps(vars(opts), indent=2))\n    train(opts)\n'"
unsupervised_data_cfg_librispeech.py,0,"b'import json\n#import librosa\nimport argparse\nimport random\nfrom random import shuffle\nimport numpy as np\nimport torchaudio\nimport os\n\ndef get_file_dur(fname):\n    try:\n        x, rate = torchaudio.load(fname)\n    except RuntimeError:\n        print(f""Error processing {fname}"")\n        return (0)\n\n    return x.shape[1]\n\n\ndef main(opts):\n    random.seed(opts.seed)\n    spk2idx = np.load(opts.libri_dict, allow_pickle=True)\n    spk2idx = dict(spk2idx.any())\n    data_cfg = {\'train\':{\'data\':[],\n                         \'speakers\':[]},\n                \'valid\':{\'data\':[],\n                         \'speakers\':[]},\n                \'test\':{\'data\':[],\n                        \'speakers\':[]},\n                \'speakers\':[]}\n    with open(opts.train_scp, \'r\') as train_f:\n        train_files = [l.rstrip() for l in train_f]\n        shuffle(train_files)\n        if opts.valid_scp is None:\n            N_valid_files = int(len(train_files) * opts.val_ratio)\n            valid_files = train_files[:N_valid_files]\n            train_files = train_files[N_valid_files:]\n        train_dur = 0\n        for ti, train_file in enumerate(train_files, start=1):\n            print(\'Processing train file {:7d}/{:7d}\'.format(ti,\n                                                             len(train_files)),\n                  end=\'\\r\')\n            spk = spk2idx[train_file]\n            if spk not in data_cfg[\'speakers\']:\n                data_cfg[\'speakers\'].append(spk)\n                data_cfg[\'train\'][\'speakers\'].append(spk)\n            data_cfg[\'train\'][\'data\'].append({\'filename\':train_file,\n                                              \'spk\':spk})\n            train_dur += get_file_dur(os.path.join(opts.data_root,\n                                                   train_file))\n        data_cfg[\'train\'][\'total_wav_dur\'] = train_dur\n        print()\n        if opts.valid_scp is None:\n            valid_dur = 0\n            for ti, valid_file in enumerate(valid_files, start=1):\n                print(\'Processing valid file {:7d}/{:7d}\'.format(ti,\n                                                                 len(valid_files)),\n                      end=\'\\r\')\n                spk = spk2idx[valid_file]\n                if spk not in data_cfg[\'speakers\']:\n                    data_cfg[\'speakers\'].append(spk)\n                    data_cfg[\'valid\'][\'speakers\'].append(spk)\n                data_cfg[\'valid\'][\'data\'].append({\'filename\':valid_file,\n                                                  \'spk\':spk})\n                valid_dur += get_file_dur(os.path.join(opts.data_root,\n                                                       valid_file))\n            data_cfg[\'valid\'][\'total_wav_dur\'] = valid_dur\n            print()\n\n    if opts.valid_scp is not None:\n        with open(opts.valid_scp, \'r\') as valid_f:\n            valid_files = [l.rstrip() for l in valid_f]\n            valid_dur = 0\n            for ti, valid_file in enumerate(valid_files, start=1):\n                print(\'Processing valid file {:7d}/{:7d}\'.format(ti,\n                                                                 len(valid_files)),\n                      end=\'\\r\')\n                spk = spk2idx[valid_file]\n                if spk not in data_cfg[\'speakers\']:\n                    data_cfg[\'speakers\'].append(spk)\n                    data_cfg[\'valid\'][\'speakers\'].append(spk)\n                data_cfg[\'valid\'][\'data\'].append({\'filename\':valid_file,\n                                                  \'spk\':spk})\n                valid_dur += get_file_dur(os.path.join(opts.data_root,\n                                                       valid_file))\n            data_cfg[\'valid\'][\'total_wav_dur\'] = valid_dur\n            print()\n\n    with open(opts.test_scp, \'r\') as test_f:\n        test_files = [l.rstrip() for l in test_f]\n        test_dur = 0\n        for ti, test_file in enumerate(test_files, start=1):\n            print(\'Processing test file {:7d}/{:7d}\'.format(ti,\n                                                            len(test_files)),\n                  end=\'\\r\')\n            spk = spk2idx[test_file]\n            if spk not in data_cfg[\'speakers\']:\n                data_cfg[\'speakers\'].append(spk)\n                data_cfg[\'test\'][\'speakers\'].append(spk)\n            data_cfg[\'test\'][\'data\'].append({\'filename\':test_file,\n                                              \'spk\':spk})\n            test_dur += get_file_dur(os.path.join(opts.data_root,\n                                                  test_file))\n        data_cfg[\'test\'][\'total_wav_dur\'] = test_dur\n    print()\n\n    with open(opts.cfg_file, \'w\') as cfg_f:\n        cfg_f.write(json.dumps(data_cfg))\n\n\n            \n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_root\', type=str, \n                        default=\'data/LibriSpeech/Librispeech_spkid_sel\')\n    parser.add_argument(\'--train_scp\', type=str, default=None)\n    parser.add_argument(\'--valid_scp\', type=str, default=None)\n    parser.add_argument(\'--test_scp\', type=str, default=None)\n    parser.add_argument(\'--val_ratio\', type=float, default=0.1,\n                        help=\'Validation ratio to take out of training \'\n                             \'in utterances ratio (Def: 0.1).\')\n    parser.add_argument(\'--cfg_file\', type=str, default=\'data/librispeech_data.cfg\')\n    parser.add_argument(\'--libri_dict\', type=str,\n                        default=\'data/LibriSpeech/libri_dict.npy\')\n    parser.add_argument(\'--seed\', type=int, default=3)\n    \n    opts = parser.parse_args()\n    main(opts)\n\n'"
ASR/data_io.py,0,"b'##########################################################\n# pytorch-kaldi v.0.1                                      \n# Mirco Ravanelli, Titouan Parcollet\n# Mila, University of Montreal\n# October 2018\n##########################################################\n\nimport numpy as np\nimport sys\nfrom utils import compute_cw_max,dict_fea_lab_arch,is_sequential_dict\nimport os\nimport configparser\nimport re, gzip, struct\n\ndef load_dataset(fea_scp,fea_opts,lab_folder,lab_opts,left,right, max_sequence_length, output_folder, fea_only=False):\n    def _input_is_wav_file(fea_scp):\n        with open(fea_scp, \'r\') as f:\n            first_line = f.readline()\n        ark_file = first_line.split(\' \')[1].split(\':\')[0]\n        with open(ark_file, \'rb\') as f:\n            first_ark_line = f.readline()\n        return b\'RIFF\' in first_ark_line\n    def _input_is_feature_file(fea_scp):\n        return not _input_is_wav_file(fea_scp)\n    def _read_features_and_labels_with_kaldi(fea_scp, fea_opts, fea_only, lab_folder, lab_opts, output_folder):\n        fea = dict()\n        lab = dict()\n        if _input_is_feature_file(fea_scp):\n            kaldi_bin=""copy-feats""\n            read_function = read_mat_ark\n        elif _input_is_wav_file(fea_scp):\n            kaldi_bin=""wav-copy""\n            read_function = read_vec_flt_ark\n        fea = { k:m for k,m in read_function(\'ark:\'+kaldi_bin+\' scp:\'+fea_scp+\' ark:- |\'+fea_opts,output_folder) }\n        if not fea_only:\n            lab = { k:v for k,v in read_vec_int_ark(\'gunzip -c \'+lab_folder+\'/ali*.gz | \'+lab_opts+\' \'+lab_folder+\'/final.mdl ark:- ark:-|\',output_folder)  if k in fea} # Note that I\'m copying only the aligments of the loaded fea\n            fea = {k: v for k, v in fea.items() if k in lab} # This way I remove all the features without an aligment (see log file in alidir ""Did not Succeded"")\n        return fea, lab\n    def _chunk_features_and_labels(max_sequence_length, fea, lab, fea_only, input_is_wav):\n        def _append_to_concat_list(fea_chunked, lab_chunked, fea_conc, lab_conc, name):\n            for j in range(0, len(fea_chunked)):\n                fea_conc.append(fea_chunked[j])\n                lab_conc.append(lab_chunked[j])\n                if len(fea_chunked) > 1:\n                    snt_name.append(name+\'_split\'+str(j))\n                else:\n                    snt_name.append(k)\n            return fea_conc, lab_conc\n        def _chunk(max_sequence_length, fea, lab, fea_only):\n            def _chunk_by_input_and_output_chunk_config(chunk_config, fea, lab, fea_only):\n                \'\'\' \n                If the sequence length is above the threshold, we split it with a minimal length max/4\n                If max length = 500, then the split will start at 500 + (500/4) = 625. \n                A seq of length 625 will be splitted in one of 500 and one of 125\n                \'\'\'\n                chunk_size_fea, chunk_step_fea, chunk_size_lab, chunk_step_lab = chunk_config[\'chunk_size_fea\'], chunk_config[\'chunk_step_fea\'], chunk_config[\'chunk_size_lab\'], chunk_config[\'chunk_step_lab\']\n                fea_chunked = list()\n                lab_chunked = list()\n                split_threshold_fea = chunk_size_fea + (chunk_size_fea/4)\n                if(len(fea) > chunk_size_fea) and chunk_size_fea>0:\n                    nr_of_chunks = (len(fea) + chunk_size_fea - 1) // chunk_size_fea\n                    for i in range(nr_of_chunks):\n                        chunk_start_fea = i * chunk_step_fea\n                        if(len(fea[chunk_start_fea:]) > split_threshold_fea):\n                            chunk_end_fea = chunk_start_fea + chunk_size_fea\n                            fea_chunk = fea[chunk_start_fea:chunk_end_fea]\n                            if not fea_only:\n                                chunk_start_lab = i * chunk_step_lab\n                                chunk_end_lab = chunk_start_lab + chunk_size_lab\n                                lab_chunk = lab[chunk_start_lab:chunk_end_lab]\n                            else:\n                                lab_chunk = np.zeros((fea_chunk.shape[0],))\n                            fea_chunked.append(fea_chunk)\n                            lab_chunked.append(lab_chunk)\n                        else:\n                            fea_chunk = fea[chunk_start_fea:]\n                            if not fea_only:\n                                chunk_start_lab = i * chunk_step_lab\n                                lab_chunk = lab[chunk_start_lab:]\n                            else:\n                                lab_chunk = np.zeros((fea_chunk.shape[0],))\n                            lab_chunked.append(lab_chunk)\n                            fea_chunked.append(fea_chunk)\n                            break\n                else:\n                    fea_chunked.append(fea)\n                    if not fea_only:\n                      lab_chunked.append(lab)\n                    else:\n                      lab_chunked.append(np.zeros((fea.shape[0],)))\n                return fea_chunked, lab_chunked\n           \n            chunk_config = dict()\n            if type(max_sequence_length) == dict:\n                chunk_config[\'chunk_size_fea\'] = max_sequence_length[\'chunk_size_fea\']\n                chunk_config[\'chunk_step_fea\'] = max_sequence_length[\'chunk_step_fea\']\n                chunk_config[\'chunk_size_lab\'] = max_sequence_length[\'chunk_size_lab\']\n                chunk_config[\'chunk_step_lab\'] = max_sequence_length[\'chunk_step_lab\']\n            elif type(max_sequence_length) == int:\n                chunk_config[\'chunk_size_fea\'] = max_sequence_length\n                chunk_config[\'chunk_step_fea\'] = max_sequence_length\n                chunk_config[\'chunk_size_lab\'] = max_sequence_length\n                chunk_config[\'chunk_step_lab\'] = max_sequence_length\n            else:\n                raise ValueError(\'Unknown type of max_sequence_length\')\n            return _chunk_by_input_and_output_chunk_config(chunk_config, fea, lab, fea_only)\n\n        snt_name = list()\n        fea_conc = list()\n        lab_conc = list()\n        feature_keys_soted_by_sequence_length = sorted(sorted(fea.keys()), key=lambda k: len(fea[k]))\n        for k in feature_keys_soted_by_sequence_length:\n            fea_el = fea[k]\n            lab_el = None\n            if not fea_only:\n                lab_el = lab[k]\n            fea_chunked, lab_chunked = _chunk(max_sequence_length, fea_el, lab_el, fea_only)\n            fea_conc, lab_conc = _append_to_concat_list(fea_chunked, lab_chunked, fea_conc, lab_conc, k)\n        return fea_conc, lab_conc, snt_name\n    def _concatenate_features_and_labels(fea_conc, lab_conc):\n        def _sort_chunks_by_length(fea_conc, lab_conc):\n            fea_zipped = zip(fea_conc,lab_conc)\n            fea_sorted = sorted(fea_zipped, key=lambda x: x[0].shape[0])\n            fea_conc,lab_conc = zip(*fea_sorted)\n            return fea_conc, lab_conc\n        def _get_end_index_from_list(conc):\n            end_snt=0\n            end_index=list()\n            for entry in conc:\n                end_snt=end_snt+entry.shape[0]\n                end_index.append(end_snt)\n            return end_index\n\n        fea_conc, lab_conc = _sort_chunks_by_length(fea_conc, lab_conc)\n        end_index_fea = _get_end_index_from_list(fea_conc)\n        end_index_lab = _get_end_index_from_list(lab_conc)\n        fea_conc=np.concatenate(fea_conc)\n        lab_conc=np.concatenate(lab_conc)\n        return fea_conc, lab_conc, end_index_fea, end_index_lab\n    def _match_feature_and_label_sequence_lengths(fea, lab, max_sequence_length):\n        ALLOW_FRAME_DIFF_LARGER_ONE = False\n        def _adjust_feature_sequence_length(fea, nr_of_fea_for_lab):\n            nr_of_fea = fea.shape[0]\n            if nr_of_fea > nr_of_fea_for_lab:\n                fea_adj = np.take(fea, range(nr_of_fea_for_lab), axis=0)\n            elif nr_of_fea < nr_of_fea_for_lab:\n                padding = np.zeros(shape=(nr_of_fea_for_lab-nr_of_fea,) + fea.shape[1:])\n                fea_adj = np.concatenate([fea, padding], axis=0)\n            else:\n                fea_adj = fea\n            return fea_adj\n        chunk_size_fea = max_sequence_length[\'chunk_size_fea\']\n        chunk_step_fea = max_sequence_length[\'chunk_step_fea\']\n        chunk_size_lab = max_sequence_length[\'chunk_size_lab\']\n        chunk_step_lab = max_sequence_length[\'chunk_step_lab\']\n        window_shift = max_sequence_length[\'window_shift\']\n        window_size = max_sequence_length[\'window_size\']\n        for k in fea.keys():\n            nr_of_fea = fea[k].shape[0]\n            nr_of_lab = lab[k].shape[0]\n            nr_of_fea_for_lab = (nr_of_lab - 1) * window_shift + window_size\n            if abs(nr_of_fea - nr_of_fea_for_lab) > window_shift and not ALLOW_FRAME_DIFF_LARGER_ONE:\n               raise ValueError(\'Nr. of features: \' + str(nr_of_fea) + \' does not match nr. of labels: \' + str(nr_of_lab) + \' with expected nr. of features: \' + str(nr_of_fea_for_lab))\n            fea[k] = _adjust_feature_sequence_length(fea[k], nr_of_fea_for_lab)\n        return fea, lab\n\n    fea, lab = _read_features_and_labels_with_kaldi(fea_scp, fea_opts, fea_only, lab_folder, lab_opts, output_folder)\n    if _input_is_wav_file(fea_scp) and (not fea_only):\n        fea, lab = _match_feature_and_label_sequence_lengths(fea, lab, max_sequence_length)\n    fea_chunks, lab_chunks, chunk_names = _chunk_features_and_labels(max_sequence_length, fea, lab, fea_only, _input_is_wav_file(fea_scp))\n    fea_conc, lab_conc, end_index_fea, end_index_lab = _concatenate_features_and_labels(fea_chunks, lab_chunks)\n    return [chunk_names,fea_conc,lab_conc,np.asarray(end_index_fea),np.asarray(end_index_lab)] \n\n\ndef context_window_old(fea,left,right):\n \n N_row=fea.shape[0]\n N_fea=fea.shape[1]\n frames = np.empty((N_row-left-right, N_fea*(left+right+1)))\n \n for frame_index in range(left,N_row-right):\n  right_context=fea[frame_index+1:frame_index+right+1].flatten() # right context\n  left_context=fea[frame_index-left:frame_index].flatten() # left context\n  current_frame=np.concatenate([left_context,fea[frame_index],right_context])\n  frames[frame_index-left]=current_frame\n\n return frames\n\ndef context_window(fea,left,right):\n \n    N_elem=fea.shape[0]\n    N_fea=fea.shape[1]\n    \n    fea_conc=np.empty([N_elem,N_fea*(left+right+1)])\n    \n    index_fea=0\n    for lag in range(-left,right+1):\n        fea_conc[:,index_fea:index_fea+fea.shape[1]]=np.roll(fea,-lag,axis=0)\n        index_fea=index_fea+fea.shape[1]\n        \n    fea_conc=fea_conc[left:fea_conc.shape[0]-right]\n    return fea_conc\n\n\ndef load_chunk(fea_scp,fea_opts,lab_folder,lab_opts,left,right,max_sequence_length, output_folder,fea_only=False):\n  \n  # open the file\n  [data_name,data_set,data_lab,end_index_fea,end_index_lab]=load_dataset(fea_scp,fea_opts,lab_folder,lab_opts,left,right, max_sequence_length, output_folder, fea_only)\n\n  # TODO: currently end_index_lab is ignored\n\n  # Context window\n  if left!=0 or right!=0:\n      data_set=context_window(data_set,left,right)\n\n  end_index_fea=end_index_fea-left\n  end_index_fea[-1]=end_index_fea[-1]-right\n\n  # mean and variance normalization\n  data_set=(data_set-np.mean(data_set,axis=0))/np.std(data_set,axis=0)\n\n  # Label processing\n  data_lab=data_lab-data_lab.min()\n  if right>0:\n    data_lab=data_lab[left:-right]\n  else:\n    data_lab=data_lab[left:]   \n  \n  data_set=np.column_stack((data_set, data_lab))\n\n  return [data_name,data_set,end_index_fea]\n\ndef load_counts(class_counts_file):\n    with open(class_counts_file) as f:\n        row = next(f).strip().strip(\'[]\').strip()\n        counts = np.array([ np.float32(v) for v in row.split() ])\n    return counts \n\ndef read_lab_fea_refac01(cfg_file, fea_only, shared_list, output_folder):\n    def _read_chunk_specific_config(cfg_file):\n        if not(os.path.exists(cfg_file)):\n            sys.stderr.write(\'ERROR: The config file %s does not exist!\\n\'%(cfg_file))\n            sys.exit(0)\n        else:\n            config = configparser.ConfigParser()\n            config.read(cfg_file)\n        return config\n    def _read_from_config(config, fea_only):\n        def _get_max_seq_length_from_config_str(config_str):\n            max_seq_length=[int(e) for e in config_str.split(\',\')]\n            if len(max_seq_length) == 1:\n                max_seq_length = max_seq_length[0]\n            else:\n                assert len(max_seq_length) == 6\n                max_seq_length_list = max_seq_length\n                max_seq_length = dict()\n                max_seq_length[\'chunk_size_fea\'] = max_seq_length_list[0]\n                max_seq_length[\'chunk_step_fea\'] = max_seq_length_list[1]\n                max_seq_length[\'chunk_size_lab\'] = max_seq_length_list[2]\n                max_seq_length[\'chunk_step_lab\'] = max_seq_length_list[3]\n                max_seq_length[\'window_shift\'] = max_seq_length_list[4]\n                max_seq_length[\'window_size\'] = max_seq_length_list[5]\n            return max_seq_length\n        \n        to_do=config[\'exp\'][\'to_do\']\n        if to_do==\'train\':\n            max_seq_length=_get_max_seq_length_from_config_str(config[\'batches\'][\'max_seq_length_train\'])\n        if to_do==\'valid\':\n            max_seq_length=_get_max_seq_length_from_config_str(config[\'batches\'][\'max_seq_length_valid\'])\n        if to_do==\'forward\':\n            max_seq_length=-1 # do to break forward sentences\n            fea_only=True\n        fea_dict, lab_dict, arch_dict = dict_fea_lab_arch(config, fea_only)\n        seq_model = is_sequential_dict(config, arch_dict)\n        return to_do, max_seq_length, fea_dict, lab_dict, arch_dict, seq_model\n    def _read_features_and_labels(fea_dict, lab_dict, max_seq_length, fea_only, output_folder):\n        def _get_fea_config_from_dict(fea_dict_entr):\n            fea_scp = fea_dict_entr[1]\n            fea_opts = fea_dict_entr[2]\n            cw_left = int(fea_dict_entr[3])\n            cw_right = int(fea_dict_entr[4])\n            return fea_scp, fea_opts, cw_left, cw_right\n        def _get_lab_config_from_dict(lab_dict_entr, fea_only):\n            if fea_only:\n                lab_folder = None \n                lab_opts = None\n            else:\n                lab_folder = lab_dict_entr[1]\n                lab_opts = lab_dict_entr[2]\n            return lab_folder, lab_opts\n        def _compensate_for_different_context_windows(data_set_fea, data_set_lab, cw_left_max, cw_left, cw_right_max, cw_right, data_end_index_fea, data_end_index_lab):\n            data_set_lab = np.take(data_set_lab, range(cw_left_max-cw_left,data_set_lab.shape[0]-(cw_right_max-cw_right)), axis=0, mode=\'clip\')\n            data_set_fea = np.take(data_set_fea, range(cw_left_max-cw_left,data_set_fea.shape[0]-(cw_right_max-cw_right)), axis=0, mode=\'clip\')\n            data_end_index_fea = data_end_index_fea - (cw_left_max - cw_left)\n            data_end_index_lab = data_end_index_lab - (cw_left_max - cw_left)\n            data_end_index_fea[-1] = data_end_index_fea[-1] - (cw_right_max - cw_right)\n            data_end_index_lab[-1] = data_end_index_lab[-1] - (cw_right_max - cw_right)\n            return data_set_lab, data_set_fea, data_end_index_fea, data_end_index_lab\n        def _update_data(data_set, labs, fea_dict, fea, fea_index, data_set_fea, labs_fea, cnt_fea, cnt_lab):\n            if cnt_fea==0 and cnt_lab==0:\n                data_set=data_set_fea\n                labs=labs_fea\n                fea_dict[fea].append(fea_index)\n                fea_index=fea_index+data_set_fea.shape[1]\n                fea_dict[fea].append(fea_index)\n                fea_dict[fea].append(fea_dict[fea][6]-fea_dict[fea][5])\n            elif cnt_fea==0 and (not cnt_lab==0):\n                labs=np.column_stack((labs,labs_fea))\n            elif (not cnt_fea==0) and cnt_lab==0:\n                data_set=np.column_stack((data_set,data_set_fea))\n                fea_dict[fea].append(fea_index)\n                fea_index=fea_index+data_set_fea.shape[1]\n                fea_dict[fea].append(fea_index)\n                fea_dict[fea].append(fea_dict[fea][6]-fea_dict[fea][5])\n            return data_set, labs, fea_dict, fea_index\n        def _check_consistency(data_name, data_name_fea, data_end_index_fea_ini, data_end_index_fea, data_end_index_lab_ini, data_end_index_lab):\n            if not (data_name == data_name_fea):\n                sys.stderr.write(\'ERROR: different sentence ids are detected for the different features. Plase check again input feature lists""\\n\')\n                sys.exit(0)\n            if not (data_end_index_fea_ini == data_end_index_fea).all():\n                sys.stderr.write(\'ERROR end_index must be the same for all the sentences""\\n\')\n                sys.exit(0)\n            if not (data_end_index_lab_ini == data_end_index_lab).all():\n                sys.stderr.write(\'ERROR end_index must be the same for all the sentences""\\n\')\n                sys.exit(0)\n        def _update_lab_dict(lab_dict, data_set):\n            cnt_lab=0\n            for lab in lab_dict.keys():\n                lab_dict[lab].append(data_set.shape[1]+cnt_lab)\n                cnt_lab=cnt_lab+1\n            return lab_dict\n        def _load_chunk_refac01(fea_scp,fea_opts,lab_folder,lab_opts,left,right,max_sequence_length, output_folder,fea_only=False):\n            [data_name,data_set,data_lab,end_index_fea,end_index_lab]=load_dataset(fea_scp,fea_opts,lab_folder,lab_opts,left,right, max_sequence_length, output_folder, fea_only)\n            # TODO: this function will currently only work well if no context window is given or fea and lab have the same time dimensionality\n            # Context window\n            if left!=0 or right!=0:\n                data_set=context_window(data_set,left,right)\n            end_index_fea = end_index_fea - left\n            end_index_lab = end_index_lab - left\n            end_index_fea[-1] = end_index_fea[-1] - right\n            end_index_lab[-1] = end_index_lab[-1] - right\n            # mean and variance normalization\n            data_set=(data_set-np.mean(data_set,axis=0))/np.std(data_set,axis=0)\n            # Label processing\n            data_lab=data_lab-data_lab.min()\n            if right>0:\n                data_lab=data_lab[left:-right]\n            else:\n                data_lab=data_lab[left:]   \n            if len(data_set.shape) == 1:\n                data_set = np.expand_dims(data_set, -1)\n            return [data_name, data_set, data_lab, end_index_fea, end_index_lab]\n        \n        cw_left_max, cw_right_max = compute_cw_max(fea_dict)\n        fea_index=0\n        cnt_fea=0\n        data_name = None \n        data_end_index_fea_ini = None \n        data_end_index_lab_ini = None \n        data_set = None\n        labs = None\n        for fea in fea_dict.keys():\n            fea_scp, fea_opts, cw_left, cw_right = _get_fea_config_from_dict(fea_dict[fea])\n            cnt_lab=0\n            if fea_only:\n                lab_dict.update({\'lab_name\':\'none\'})\n            for lab in lab_dict.keys():\n                lab_folder, lab_opts = _get_lab_config_from_dict(lab_dict[lab], fea_only)\n                data_name_fea, data_set_fea, data_set_lab, data_end_index_fea, data_end_index_lab = _load_chunk_refac01(fea_scp, fea_opts, lab_folder, lab_opts, cw_left, cw_right, max_seq_length, output_folder, fea_only)\n                if sum([abs(e) for e in [cw_left_max, cw_right_max, cw_left, cw_right]]) != 0: \n                    data_set_lab, data_set_fea, data_end_index_fea, data_end_index_lab = _compensate_for_different_context_windows(data_set_fea, data_set_lab, cw_left_max, cw_left, cw_right_max, cw_right, data_end_index_fea, data_end_index_lab)\n                if cnt_fea == 0 and cnt_lab == 0:\n                    data_end_index_fea_ini = data_end_index_fea\n                    data_end_index_lab_ini = data_end_index_lab\n                    data_name = data_name_fea\n                data_set, labs, fea_dict, fea_index = _update_data(data_set, labs, fea_dict, fea, fea_index, data_set_fea, data_set_lab, cnt_fea, cnt_lab)\n                _check_consistency(data_name, data_name_fea, data_end_index_fea_ini, data_end_index_fea, data_end_index_lab_ini, data_end_index_lab)\n                cnt_lab=cnt_lab+1\n            cnt_fea=cnt_fea+1\n        if not fea_only:\n            lab_dict = _update_lab_dict(lab_dict, data_set)\n        return data_name, data_end_index_fea_ini, data_end_index_lab_ini, fea_dict, lab_dict, data_set, labs\n    def _reorder_data_set(data_set, labs, seq_model, to_do):\n        if not(seq_model) and to_do != \'forward\' and (data_set.shape[0] == labs.shape[0]):\n            data_set_shape = data_set.shape[1]\n            data_set_joint = np.column_stack((data_set,labs))\n            np.random.shuffle(data_set)\n            data_set = data_set_joint[:, :data_set_shape]\n            labs = np.squeeze(data_set_joint[:, data_set_shape:], axis=-1)\n        return data_set, labs\n    def _append_to_shared_list(shared_list, data_name, data_end_index_fea, data_end_index_lab, fea_dict, lab_dict, arch_dict, data_set):\n        shared_list.append(data_name)\n        shared_list.append(data_end_index_fea)\n        shared_list.append(data_end_index_lab)\n        shared_list.append(fea_dict)\n        shared_list.append(lab_dict)\n        shared_list.append(arch_dict)\n        shared_list.append(data_set)\n        return shared_list\n\n    config = _read_chunk_specific_config(cfg_file)\n    to_do, max_seq_length, fea_dict, lab_dict, arch_dict, seq_model = _read_from_config(config, fea_only)\n    data_name, data_end_index_fea, data_end_index_lab, fea_dict, lab_dict, data_set, labs = _read_features_and_labels(fea_dict, lab_dict, max_seq_length, fea_only, output_folder)\n    data_set, labs = _reorder_data_set(data_set, labs, seq_model, to_do)\n    data_set = {\'input\': data_set, \'ref\': labs}\n    shared_list = _append_to_shared_list(shared_list, data_name, data_end_index_fea, data_end_index_lab, fea_dict, lab_dict, arch_dict, data_set)\n\ndef read_lab_fea(cfg_file,fea_only,shared_list,output_folder):\n    \n    # Reading chunk-specific cfg file (first argument-mandatory file) \n    if not(os.path.exists(cfg_file)):\n         sys.stderr.write(\'ERROR: The config file %s does not exist!\\n\'%(cfg_file))\n         sys.exit(0)\n    else:\n        config = configparser.ConfigParser()\n        config.read(cfg_file)\n        \n    \n    # Reading some cfg parameters\n    to_do=config[\'exp\'][\'to_do\']\n    \n    if to_do==\'train\':\n        max_seq_length=int(config[\'batches\'][\'max_seq_length_train\']) #*(int(info_file[-13:-10])+1) # increasing over the epochs\n\n    if to_do==\'valid\':\n        max_seq_length=int(config[\'batches\'][\'max_seq_length_valid\'])\n\n    if to_do==\'forward\':\n        max_seq_length=-1 # do to break forward sentences\n    \n    [fea_dict,lab_dict,arch_dict]=dict_fea_lab_arch(config,fea_only)\n    [cw_left_max,cw_right_max]=compute_cw_max(fea_dict)\n    \n    fea_index=0\n    cnt_fea=0\n    for fea in fea_dict.keys():\n        \n        # reading the features\n        fea_scp=fea_dict[fea][1]\n        fea_opts=fea_dict[fea][2]\n        cw_left=int(fea_dict[fea][3])\n        cw_right=int(fea_dict[fea][4])\n        \n        cnt_lab=0\n\n        # Production case, we don\'t have labels (lab_name = none)\n        if fea_only:\n          lab_dict.update({\'lab_name\':\'none\'})\n        for lab in lab_dict.keys():\n            # Production case, we don\'t have labels (lab_name = none)\n            if fea_only:\n              lab_folder=None \n              lab_opts=None\n            else:\n              lab_folder=lab_dict[lab][1]\n              lab_opts=lab_dict[lab][2]\n    \n            [data_name_fea,data_set_fea,data_end_index_fea]=load_chunk(fea_scp,fea_opts,lab_folder,lab_opts,cw_left,cw_right,max_seq_length, output_folder, fea_only)\n    \n            \n            # making the same dimenion for all the features (compensating for different context windows)\n            labs_fea=data_set_fea[cw_left_max-cw_left:data_set_fea.shape[0]-(cw_right_max-cw_right),-1]\n            data_set_fea=data_set_fea[cw_left_max-cw_left:data_set_fea.shape[0]-(cw_right_max-cw_right),0:-1]\n            data_end_index_fea=data_end_index_fea-(cw_left_max-cw_left)\n            data_end_index_fea[-1]=data_end_index_fea[-1]-(cw_right_max-cw_right)\n    \n            \n            \n            if cnt_fea==0 and cnt_lab==0:\n                data_set=data_set_fea\n                labs=labs_fea\n                data_end_index=data_end_index_fea\n                data_end_index=data_end_index_fea\n                data_name=data_name_fea\n                \n                fea_dict[fea].append(fea_index)\n                fea_index=fea_index+data_set_fea.shape[1]\n                fea_dict[fea].append(fea_index)\n                fea_dict[fea].append(fea_dict[fea][6]-fea_dict[fea][5])\n                \n                \n            else:\n                if cnt_fea==0:\n                    labs=np.column_stack((labs,labs_fea))\n                \n                if cnt_lab==0:\n                    data_set=np.column_stack((data_set,data_set_fea))\n                    fea_dict[fea].append(fea_index)\n                    fea_index=fea_index+data_set_fea.shape[1]\n                    fea_dict[fea].append(fea_index)\n                    fea_dict[fea].append(fea_dict[fea][6]-fea_dict[fea][5])\n                \n                \n                # Checks if lab_names are the same for all the features\n                if not(data_name==data_name_fea):\n                    sys.stderr.write(\'ERROR: different sentence ids are detected for the different features. Plase check again input feature lists""\\n\')\n                    sys.exit(0)\n                \n                # Checks if end indexes are the same for all the features\n                if not(data_end_index==data_end_index_fea).all():\n                    sys.stderr.write(\'ERROR end_index must be the same for all the sentences""\\n\')\n                    sys.exit(0)\n                    \n            cnt_lab=cnt_lab+1\n    \n    \n        cnt_fea=cnt_fea+1\n        \n    cnt_lab=0\n    if not fea_only:   \n      for lab in lab_dict.keys():\n          lab_dict[lab].append(data_set.shape[1]+cnt_lab)\n          cnt_lab=cnt_lab+1\n           \n    data_set=np.column_stack((data_set,labs))\n    \n    # check automatically if the model is sequential\n    seq_model=is_sequential_dict(config,arch_dict)\n    \n    # Randomize if the model is not sequential\n    if not(seq_model) and to_do!=\'forward\':\n        np.random.shuffle(data_set)\n     \n    # Split dataset in many part. If the dataset is too big, we can have issues to copy it into the shared memory (due to pickle limits)\n    #N_split=10\n    #data_set=np.array_split(data_set, N_split)\n    \n    # Adding all the elements in the shared list    \n    shared_list.append(data_name)\n    shared_list.append(data_end_index)\n    shared_list.append(fea_dict)\n    shared_list.append(lab_dict)\n    shared_list.append(arch_dict)\n    shared_list.append(data_set)\n    \n\n\n\n# The following libraries are copied from kaldi-io-for-python project (https://github.com/vesis84/kaldi-io-for-python)\n    \n# Copyright 2014-2016  Brno University of Technology (author: Karel Vesely)\n# Licensed under the Apache License, Version 2.0 (the ""License"")\n    \n#################################################\n# Define all custom exceptions,\nclass UnsupportedDataType(Exception): pass\nclass UnknownVectorHeader(Exception): pass\nclass UnknownMatrixHeader(Exception): pass\n\nclass BadSampleSize(Exception): pass\nclass BadInputFormat(Exception): pass\n\nclass SubprocessFailed(Exception): pass\n\n#################################################\n# Data-type independent helper functions,\n\ndef open_or_fd(file, output_folder,mode=\'rb\'):\n  """""" fd = open_or_fd(file)\n   Open file, gzipped file, pipe, or forward the file-descriptor.\n   Eventually seeks in the \'file\' argument contains \':offset\' suffix.\n  """"""\n  offset = None\n\n  try:\n    # strip \'ark:\' prefix from r{x,w}filename (optional),\n    if re.search(\'^(ark|scp)(,scp|,b|,t|,n?f|,n?p|,b?o|,n?s|,n?cs)*:\', file):\n      (prefix,file) = file.split(\':\',1)\n    # separate offset from filename (optional),\n    if re.search(\':[0-9]+$\', file):\n      (file,offset) = file.rsplit(\':\',1)\n    # input pipe?\n    if file[-1] == \'|\':\n      fd = popen(file[:-1], output_folder,\'rb\') # custom,\n    # output pipe?\n    elif file[0] == \'|\':\n      fd = popen(file[1:], output_folder,\'wb\') # custom,\n    # is it gzipped?\n    elif file.split(\'.\')[-1] == \'gz\':\n      fd = gzip.open(file, mode)\n    # a normal file...\n    else:\n      fd = open(file, mode)\n  except TypeError:\n    # \'file\' is opened file descriptor,\n    fd = file\n  # Eventually seek to offset,\n  if offset != None: fd.seek(int(offset))\n  \n  return fd\n\n# based on \'/usr/local/lib/python3.4/os.py\'\ndef popen(cmd, output_folder,mode=""rb""):\n  if not isinstance(cmd, str):\n    raise TypeError(""invalid cmd type (%s, expected string)"" % type(cmd))\n\n  import subprocess, io, threading\n\n  # cleanup function for subprocesses,\n  def cleanup(proc, cmd):\n    ret = proc.wait()\n    if ret > 0:\n      raise SubprocessFailed(\'cmd %s returned %d !\' % (cmd,ret))\n    return\n\n  # text-mode,\n  if mode == ""r"":\n    err=open(output_folder+\'/log.log\',""a"")\n    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,stderr=err)\n    threading.Thread(target=cleanup,args=(proc,cmd)).start() # clean-up thread,\n    return io.TextIOWrapper(proc.stdout)\n  elif mode == ""w"":\n    err=open(output_folder+\'/log.log\',""a"")\n    proc = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE,stderr=err)\n    threading.Thread(target=cleanup,args=(proc,cmd)).start() # clean-up thread,\n    return io.TextIOWrapper(proc.stdin)\n  # binary,\n  elif mode == ""rb"":\n    err=open(output_folder+\'/log.log\',""a"")\n    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE,stderr=err)\n    threading.Thread(target=cleanup,args=(proc,cmd)).start() # clean-up thread,\n    return proc.stdout\n  elif mode == ""wb"":\n    err=open(output_folder+\'/log.log\',""a"")\n    proc = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE,stderr=err)\n    threading.Thread(target=cleanup,args=(proc,cmd)).start() # clean-up thread,\n    return proc.stdin\n  # sanity,\n  else:\n    raise ValueError(""invalid mode %s"" % mode)\n\n\ndef read_key(fd):\n  """""" [key] = read_key(fd)\n   Read the utterance-key from the opened ark/stream descriptor \'fd\'.\n  """"""\n  key = \'\'\n  while 1:\n    char = fd.read(1).decode(""latin1"")\n    if char == \'\' : break\n    if char == \' \' : break\n    key += char\n  key = key.strip()\n  if key == \'\': return None # end of file,\n  assert(re.match(\'^\\S+$\',key) != None) # check format (no whitespace!)\n  return key\n\n\n#################################################\n# Integer vectors (alignments, ...),\n\ndef read_ali_ark(file_or_fd,output_folder):\n  """""" Alias to \'read_vec_int_ark()\' """"""\n  return read_vec_int_ark(file_or_fd,output_folder)\n\ndef read_vec_int_ark(file_or_fd,output_folder):\n  """""" generator(key,vec) = read_vec_int_ark(file_or_fd)\n   Create generator of (key,vector<int>) tuples, which reads from the ark file/stream.\n   file_or_fd : ark, gzipped ark, pipe or opened file descriptor.\n\n   Read ark to a \'dictionary\':\n   d = { u:d for u,d in kaldi_io.read_vec_int_ark(file) }\n  """"""\n  fd = open_or_fd(file_or_fd,output_folder)\n  try:\n    key = read_key(fd)\n    while key:\n      ali = read_vec_int(fd,output_folder)\n      yield key, ali\n      key = read_key(fd)\n  finally:\n    if fd is not file_or_fd: fd.close()\n\ndef read_vec_int(file_or_fd,output_folder):\n  """""" [int-vec] = read_vec_int(file_or_fd)\n   Read kaldi integer vector, ascii or binary input,\n  """"""\n  fd = open_or_fd(file_or_fd,output_folder)\n  binary = fd.read(2).decode()\n  if binary == \'\\0B\': # binary flag\n    assert(fd.read(1).decode() == \'\\4\'); # int-size\n    vec_size = np.frombuffer(fd.read(4), dtype=\'int32\', count=1)[0] # vector dim\n    if vec_size == 0:\n      return np.array([], dtype=\'int32\')\n    # Elements from int32 vector are sored in tuples: (sizeof(int32), value),\n    vec = np.frombuffer(fd.read(vec_size*5), dtype=[(\'size\',\'int8\'),(\'value\',\'int32\')], count=vec_size)\n    assert(vec[0][\'size\'] == 4) # int32 size,\n    ans = vec[:][\'value\'] # values are in 2nd column,\n  else: # ascii,\n    arr = (binary + fd.readline().decode()).strip().split()\n    try:\n      arr.remove(\'[\'); arr.remove(\']\') # optionally\n    except ValueError:\n      pass\n    ans = np.array(arr, dtype=int)\n  if fd is not file_or_fd : fd.close() # cleanup\n  return ans\n\n# Writing,\ndef write_vec_int(file_or_fd, output_folder, v, key=\'\'):\n  """""" write_vec_int(f, v, key=\'\')\n   Write a binary kaldi integer vector to filename or stream.\n   Arguments:\n   file_or_fd : filename or opened file descriptor for writing,\n   v : the vector to be stored,\n   key (optional) : used for writing ark-file, the utterance-id gets written before the vector.\n\n   Example of writing single vector:\n   kaldi_io.write_vec_int(filename, vec)\n\n   Example of writing arkfile:\n   with open(ark_file,\'w\') as f:\n     for key,vec in dict.iteritems():\n       kaldi_io.write_vec_flt(f, vec, key=key)\n  """"""\n  fd = open_or_fd(file_or_fd, output_folder, mode=\'wb\')\n  if sys.version_info[0] == 3: assert(fd.mode == \'wb\')\n  try:\n    if key != \'\' : fd.write((key+\' \').encode(""latin1"")) # ark-files have keys (utterance-id),\n    fd.write(\'\\0B\'.encode()) # we write binary!\n    # dim,\n    fd.write(\'\\4\'.encode()) # int32 type,\n    fd.write(struct.pack(np.dtype(\'int32\').char, v.shape[0]))\n    # data,\n    for i in range(len(v)):\n      fd.write(\'\\4\'.encode()) # int32 type,\n      fd.write(struct.pack(np.dtype(\'int32\').char, v[i])) # binary,\n  finally:\n    if fd is not file_or_fd : fd.close()\n\n\n#################################################\n# Float vectors (confidences, ivectors, ...),\n\n# Reading,\ndef read_vec_flt_scp(file_or_fd,output_folder):\n  """""" generator(key,mat) = read_vec_flt_scp(file_or_fd)\n   Returns generator of (key,vector) tuples, read according to kaldi scp.\n   file_or_fd : scp, gzipped scp, pipe or opened file descriptor.\n\n   Iterate the scp:\n   for key,vec in kaldi_io.read_vec_flt_scp(file):\n     ...\n\n   Read scp to a \'dictionary\':\n   d = { key:mat for key,mat in kaldi_io.read_mat_scp(file) }\n  """"""\n  fd = open_or_fd(file_or_fd,output_folder)\n  try:\n    for line in fd:\n      (key,rxfile) = line.decode().split(\' \')\n      vec = read_vec_flt(rxfile,output_folder)\n      yield key, vec\n  finally:\n    if fd is not file_or_fd : fd.close()\n\ndef read_vec_flt_ark(file_or_fd,output_folder):\n  """""" generator(key,vec) = read_vec_flt_ark(file_or_fd)\n   Create generator of (key,vector<float>) tuples, reading from an ark file/stream.\n   file_or_fd : ark, gzipped ark, pipe or opened file descriptor.\n\n   Read ark to a \'dictionary\':\n   d = { u:d for u,d in kaldi_io.read_vec_flt_ark(file) }\n  """"""\n  fd = open_or_fd(file_or_fd,output_folder)\n  try:\n    key = read_key(fd)\n    while key:\n      ali = read_vec_flt(fd,output_folder)\n      yield key, ali\n      key = read_key(fd)\n  finally:\n    if fd is not file_or_fd: fd.close()\n\ndef read_vec_flt(file_or_fd,output_folder):\n  """""" [flt-vec] = read_vec_flt(file_or_fd)\n   Read kaldi float vector, ascii or binary input,\n  """"""\n  fd = open_or_fd(file_or_fd,output_folder)\n  binary = fd.read(2).decode()\n  if binary == \'\\0B\': # binary flag\n    return _read_vec_flt_binary(fd)\n  elif binary == \'RI\':\n    return _read_vec_flt_riff(fd)\n  else:  # ascii,\n    arr = (binary + fd.readline().decode()).strip().split()\n    try:\n      arr.remove(\'[\'); arr.remove(\']\') # optionally\n    except ValueError:\n      pass\n    ans = np.array(arr, dtype=float)\n  if fd is not file_or_fd : fd.close() # cleanup\n  return ans\n\ndef _read_vec_flt_riff(fd):\n    RIFF_CHUNK_DESCR_HEADER_SIZE = 12\n    ALREADY_READ_HEADER_BYTES = 2\n    SUB_CHUNK_HEADER_SIZE = 8\n    DATA_CHUNK_HEADER_SIZE = 8\n    def pcm2float(signal, dtype=\'float32\'):\n        signal = np.asarray(signal)\n        dtype = np.dtype(dtype)\n        return signal.astype(dtype) / dtype.type(-np.iinfo(signal.dtype).min)\n\n    import struct\n    header = fd.read(RIFF_CHUNK_DESCR_HEADER_SIZE - ALREADY_READ_HEADER_BYTES)\n    assert header[:2] == b\'FF\'\n    chunk_header = fd.read(SUB_CHUNK_HEADER_SIZE)\n    subchunk_id, subchunk_size = struct.unpack(\'<4sI\', chunk_header)\n    aformat, channels, samplerate, byterate, block_align, bps = struct.unpack(\'HHIIHH\', fd.read(subchunk_size))\n    subchunk2_id, subchunk2_size = struct.unpack(\'<4sI\', fd.read(DATA_CHUNK_HEADER_SIZE))\n    pcm_data = np.frombuffer(fd.read(subchunk2_size), dtype=\'int\' + str(bps))\n    return pcm2float(pcm_data)\n\ndef _read_vec_flt_binary(fd):\n  header = fd.read(3).decode()\n  if header == \'FV \' : sample_size = 4 # floats\n  elif header == \'DV \' : sample_size = 8 # doubles\n  else : raise UnknownVectorHeader(""The header contained \'%s\'"" % header)\n  assert (sample_size > 0)\n  # Dimension,\n  assert (fd.read(1).decode() == \'\\4\'); # int-size\n  vec_size = np.frombuffer(fd.read(4), dtype=\'int32\', count=1)[0] # vector dim\n  if vec_size == 0:\n    return np.array([], dtype=\'float32\')\n  # Read whole vector,\n  buf = fd.read(vec_size * sample_size)\n  if sample_size == 4 : ans = np.frombuffer(buf, dtype=\'float32\')\n  elif sample_size == 8 : ans = np.frombuffer(buf, dtype=\'float64\')\n  else : raise BadSampleSize\n  return ans\n\n# Writing,\ndef write_vec_flt(file_or_fd, output_folder, v, key=\'\'):\n  """""" write_vec_flt(f, v, key=\'\')\n   Write a binary kaldi vector to filename or stream. Supports 32bit and 64bit floats.\n   Arguments:\n   file_or_fd : filename or opened file descriptor for writing,\n   v : the vector to be stored,\n   key (optional) : used for writing ark-file, the utterance-id gets written before the vector.\n\n   Example of writing single vector:\n   kaldi_io.write_vec_flt(filename, vec)\n\n   Example of writing arkfile:\n   with open(ark_file,\'w\') as f:\n     for key,vec in dict.iteritems():\n       kaldi_io.write_vec_flt(f, vec, key=key)\n  """"""\n  fd = open_or_fd(file_or_fd,output_folder, mode=\'wb\')\n  if sys.version_info[0] == 3: assert(fd.mode == \'wb\')\n  try:\n    if key != \'\' : fd.write((key+\' \').encode(""latin1"")) # ark-files have keys (utterance-id),\n    fd.write(\'\\0B\'.encode()) # we write binary!\n    # Data-type,\n    if v.dtype == \'float32\': fd.write(\'FV \'.encode())\n    elif v.dtype == \'float64\': fd.write(\'DV \'.encode())\n    else: raise UnsupportedDataType(""\'%s\', please use \'float32\' or \'float64\'"" % v.dtype)\n    # Dim,\n    fd.write(\'\\04\'.encode())\n    fd.write(struct.pack(np.dtype(\'uint32\').char, v.shape[0])) # dim\n    # Data,\n    fd.write(v.tobytes())\n  finally:\n    if fd is not file_or_fd : fd.close()\n\n\n#################################################\n# Float matrices (features, transformations, ...),\n\n# Reading,\ndef read_mat_scp(file_or_fd,output_folder):\n  """""" generator(key,mat) = read_mat_scp(file_or_fd)\n   Returns generator of (key,matrix) tuples, read according to kaldi scp.\n   file_or_fd : scp, gzipped scp, pipe or opened file descriptor.\n\n   Iterate the scp:\n   for key,mat in kaldi_io.read_mat_scp(file):\n     ...\n\n   Read scp to a \'dictionary\':\n   d = { key:mat for key,mat in kaldi_io.read_mat_scp(file) }\n  """"""\n  fd = open_or_fd(file_or_fd,output_folder)\n  try:\n    for line in fd:\n      (key,rxfile) = line.decode().split(\' \')\n      mat = read_mat(rxfile,output_folder)\n      yield key, mat\n  finally:\n    if fd is not file_or_fd : fd.close()\n\ndef read_mat_ark(file_or_fd,output_folder):\n  """""" generator(key,mat) = read_mat_ark(file_or_fd)\n   Returns generator of (key,matrix) tuples, read from ark file/stream.\n   file_or_fd : scp, gzipped scp, pipe or opened file descriptor.\n\n   Iterate the ark:\n   for key,mat in kaldi_io.read_mat_ark(file):\n     ...\n\n   Read ark to a \'dictionary\':\n   d = { key:mat for key,mat in kaldi_io.read_mat_ark(file) }\n  """"""\n\n  fd = open_or_fd(file_or_fd,output_folder)\n  try:\n    key = read_key(fd)\n    while key:\n      mat = read_mat(fd,output_folder)\n      yield key, mat\n      key = read_key(fd)   \n  finally:\n    if fd is not file_or_fd : fd.close()\n  \n\n\ndef read_mat(file_or_fd,output_folder):\n  """""" [mat] = read_mat(file_or_fd)\n   Reads single kaldi matrix, supports ascii and binary.\n   file_or_fd : file, gzipped file, pipe or opened file descriptor.\n  """"""\n  fd = open_or_fd(file_or_fd,output_folder)\n  try:\n    binary = fd.read(2).decode()\n    if binary == \'\\0B\' :\n      mat = _read_mat_binary(fd)\n    else:\n      assert(binary == \' [\')\n      mat = _read_mat_ascii(fd)\n  finally:\n    if fd is not file_or_fd: fd.close()\n  return mat\n\ndef _read_mat_binary(fd):\n  # Data type\n  header = fd.read(3).decode()\n  # \'CM\', \'CM2\', \'CM3\' are possible values,\n  if header.startswith(\'CM\'): return _read_compressed_mat(fd, header)\n  elif header == \'FM \': sample_size = 4 # floats\n  elif header == \'DM \': sample_size = 8 # doubles\n  else: raise UnknownMatrixHeader(""The header contained \'%s\'"" % header)\n  assert(sample_size > 0)\n  # Dimensions\n  s1, rows, s2, cols = np.frombuffer(fd.read(10), dtype=\'int8,int32,int8,int32\', count=1)[0]\n  # Read whole matrix\n  buf = fd.read(rows * cols * sample_size)\n  if sample_size == 4 : vec = np.frombuffer(buf, dtype=\'float32\')\n  elif sample_size == 8 : vec = np.frombuffer(buf, dtype=\'float64\')\n  else : raise BadSampleSize\n  mat = np.reshape(vec,(rows,cols))\n  return mat\n\ndef _read_mat_ascii(fd):\n  rows = []\n  while 1:\n    line = fd.readline().decode()\n    if (len(line) == 0) : raise BadInputFormat # eof, should not happen!\n    if len(line.strip()) == 0 : continue # skip empty line\n    arr = line.strip().split()\n    if arr[-1] != \']\':\n      rows.append(np.array(arr,dtype=\'float32\')) # not last line\n    else:\n      rows.append(np.array(arr[:-1],dtype=\'float32\')) # last line\n      mat = np.vstack(rows)\n      return mat\n\n\ndef _read_compressed_mat(fd, format):\n  """""" Read a compressed matrix,\n      see: https://github.com/kaldi-asr/kaldi/blob/master/src/matrix/compressed-matrix.h\n      methods: CompressedMatrix::Read(...), CompressedMatrix::CopyToMat(...),\n  """"""\n  assert(format == \'CM \') # The formats CM2, CM3 are not supported...\n\n  # Format of header \'struct\',\n  global_header = np.dtype([(\'minvalue\',\'float32\'),(\'range\',\'float32\'),(\'num_rows\',\'int32\'),(\'num_cols\',\'int32\')]) # member \'.format\' is not written,\n  per_col_header = np.dtype([(\'percentile_0\',\'uint16\'),(\'percentile_25\',\'uint16\'),(\'percentile_75\',\'uint16\'),(\'percentile_100\',\'uint16\')])\n\n  # Read global header,\n  globmin, globrange, rows, cols = np.frombuffer(fd.read(16), dtype=global_header, count=1)[0]\n\n  # The data is structed as [Colheader, ... , Colheader, Data, Data , .... ]\n  #                         {           cols           }{     size         }\n  col_headers = np.frombuffer(fd.read(cols*8), dtype=per_col_header, count=cols)\n  col_headers = np.array([np.array([x for x in y]) * globrange * 1.52590218966964e-05 + globmin for y in col_headers], dtype=np.float32)\n  data = np.reshape(np.frombuffer(fd.read(cols*rows), dtype=\'uint8\', count=cols*rows), newshape=(cols,rows)) # stored as col-major,\n\n  mat = np.zeros((cols,rows), dtype=\'float32\')\n  p0 = col_headers[:, 0].reshape(-1, 1)\n  p25 = col_headers[:, 1].reshape(-1, 1)\n  p75 = col_headers[:, 2].reshape(-1, 1)\n  p100 = col_headers[:, 3].reshape(-1, 1)\n  mask_0_64 = (data <= 64)\n  mask_193_255 = (data > 192)\n  mask_65_192 = (~(mask_0_64 | mask_193_255))\n\n  mat += (p0  + (p25 - p0) / 64. * data) * mask_0_64.astype(np.float32)\n  mat += (p25 + (p75 - p25) / 128. * (data - 64)) * mask_65_192.astype(np.float32)\n  mat += (p75 + (p100 - p75) / 63. * (data - 192)) * mask_193_255.astype(np.float32)\n\n  return mat.T # transpose! col-major -> row-major,\n\n\n# Writing,\ndef write_mat(output_folder,file_or_fd, m, key=\'\'):\n  """""" write_mat(f, m, key=\'\')\n  Write a binary kaldi matrix to filename or stream. Supports 32bit and 64bit floats.\n  Arguments:\n   file_or_fd : filename of opened file descriptor for writing,\n   m : the matrix to be stored,\n   key (optional) : used for writing ark-file, the utterance-id gets written before the matrix.\n\n   Example of writing single matrix:\n   kaldi_io.write_mat(filename, mat)\n\n   Example of writing arkfile:\n   with open(ark_file,\'w\') as f:\n     for key,mat in dict.iteritems():\n       kaldi_io.write_mat(f, mat, key=key)\n  """"""\n  fd = open_or_fd(file_or_fd, output_folder, mode=\'wb\')\n  if sys.version_info[0] == 3: assert(fd.mode == \'wb\')\n  try:\n    if key != \'\' : fd.write((key+\' \').encode(""latin1"")) # ark-files have keys (utterance-id),\n    fd.write(\'\\0B\'.encode()) # we write binary!\n    # Data-type,\n    if m.dtype == \'float32\': fd.write(\'FM \'.encode())\n    elif m.dtype == \'float64\': fd.write(\'DM \'.encode())\n    else: raise UnsupportedDataType(""\'%s\', please use \'float32\' or \'float64\'"" % m.dtype)\n    # Dims,\n    fd.write(\'\\04\'.encode())\n    fd.write(struct.pack(np.dtype(\'uint32\').char, m.shape[0])) # rows\n    fd.write(\'\\04\'.encode())\n    fd.write(struct.pack(np.dtype(\'uint32\').char, m.shape[1])) # cols\n    # Data,\n    fd.write(m.tobytes())\n  finally:\n    if fd is not file_or_fd : fd.close()\n\n\n#################################################\n# \'Posterior\' kaldi type (posteriors, confusion network, nnet1 training targets, ...)\n# Corresponds to: vector<vector<tuple<int,float> > >\n# - outer vector: time axis\n# - inner vector: records at the time\n# - tuple: int = index, float = value\n#\n\ndef read_cnet_ark(file_or_fd,output_folder):\n  """""" Alias of function \'read_post_ark()\', \'cnet\' = confusion network """"""\n  return read_post_ark(file_or_fd,output_folder)\n\ndef read_post_rxspec(file_):\n  """""" adaptor to read both \'ark:...\' and \'scp:...\' inputs of posteriors,\n  """"""\n  if file_.startswith(""ark:""):\n      return read_post_ark(file_)\n  elif file_.startswith(""scp:""):\n      return read_post_scp(file_)\n  else:\n      print(""unsupported intput type: %s"" % file_)\n      print(""it should begint with \'ark:\' or \'scp:\'"")\n      sys.exit(1)\n\ndef read_post_scp(file_or_fd,output_folder):\n  """""" generator(key,post) = read_post_scp(file_or_fd)\n   Returns generator of (key,post) tuples, read according to kaldi scp.\n   file_or_fd : scp, gzipped scp, pipe or opened file descriptor.\n\n   Iterate the scp:\n   for key,post in kaldi_io.read_post_scp(file):\n     ...\n\n   Read scp to a \'dictionary\':\n   d = { key:post for key,post in kaldi_io.read_post_scp(file) }\n  """"""\n  fd = open_or_fd(file_or_fd,output_folder)\n  try:\n    for line in fd:\n      (key,rxfile) = line.decode().split(\' \')\n      post = read_post(rxfile)\n      yield key, post\n  finally:\n    if fd is not file_or_fd : fd.close()\n\ndef read_post_ark(file_or_fd,output_folder):\n  """""" generator(key,vec<vec<int,float>>) = read_post_ark(file)\n   Returns generator of (key,posterior) tuples, read from ark file.\n   file_or_fd : ark, gzipped ark, pipe or opened file descriptor.\n\n   Iterate the ark:\n   for key,post in kaldi_io.read_post_ark(file):\n     ...\n\n   Read ark to a \'dictionary\':\n   d = { key:post for key,post in kaldi_io.read_post_ark(file) }\n  """"""\n  fd = open_or_fd(file_or_fd,output_folder)\n  try:\n    key = read_key(fd)\n    while key:\n      post = read_post(fd)\n      yield key, post\n      key = read_key(fd)\n  finally:\n    if fd is not file_or_fd: fd.close()\n\ndef read_post(file_or_fd,output_folder):\n  """""" [post] = read_post(file_or_fd)\n   Reads single kaldi \'Posterior\' in binary format.\n\n   The \'Posterior\' is C++ type \'vector<vector<tuple<int,float> > >\',\n   the outer-vector is usually time axis, inner-vector are the records\n   at given time,  and the tuple is composed of an \'index\' (integer)\n   and a \'float-value\'. The \'float-value\' can represent a probability\n   or any other numeric value.\n\n   Returns vector of vectors of tuples.\n  """"""\n  fd = open_or_fd(file_or_fd,output_folder)\n  ans=[]\n  binary = fd.read(2).decode(); assert(binary == \'\\0B\'); # binary flag\n  assert(fd.read(1).decode() == \'\\4\'); # int-size\n  outer_vec_size = np.frombuffer(fd.read(4), dtype=\'int32\', count=1)[0] # number of frames (or bins)\n\n  # Loop over \'outer-vector\',\n  for i in range(outer_vec_size):\n    assert(fd.read(1).decode() == \'\\4\'); # int-size\n    inner_vec_size = np.frombuffer(fd.read(4), dtype=\'int32\', count=1)[0] # number of records for frame (or bin)\n    data = np.frombuffer(fd.read(inner_vec_size*10), dtype=[(\'size_idx\',\'int8\'),(\'idx\',\'int32\'),(\'size_post\',\'int8\'),(\'post\',\'float32\')], count=inner_vec_size)\n    assert(data[0][\'size_idx\'] == 4)\n    assert(data[0][\'size_post\'] == 4)\n    ans.append(data[[\'idx\',\'post\']].tolist())\n\n  if fd is not file_or_fd: fd.close()\n  return ans\n\n\n#################################################\n# Kaldi Confusion Network bin begin/end times,\n# (kaldi stores CNs time info separately from the Posterior).\n#\n\ndef read_cntime_ark(file_or_fd,output_folder):\n  """""" generator(key,vec<tuple<float,float>>) = read_cntime_ark(file_or_fd)\n   Returns generator of (key,cntime) tuples, read from ark file.\n   file_or_fd : file, gzipped file, pipe or opened file descriptor.\n\n   Iterate the ark:\n   for key,time in kaldi_io.read_cntime_ark(file):\n     ...\n\n   Read ark to a \'dictionary\':\n   d = { key:time for key,time in kaldi_io.read_post_ark(file) }\n  """"""\n  fd = open_or_fd(file_or_fd,output_folder)\n  try:\n    key = read_key(fd)\n    while key:\n      cntime = read_cntime(fd)\n      yield key, cntime\n      key = read_key(fd)\n  finally:\n    if fd is not file_or_fd : fd.close()\n\ndef read_cntime(file_or_fd,output_folder):\n  """""" [cntime] = read_cntime(file_or_fd)\n   Reads single kaldi \'Confusion Network time info\', in binary format:\n   C++ type: vector<tuple<float,float> >.\n   (begin/end times of bins at the confusion network).\n\n   Binary layout is \'<num-bins> <beg1> <end1> <beg2> <end2> ...\'\n\n   file_or_fd : file, gzipped file, pipe or opened file descriptor.\n\n   Returns vector of tuples.\n  """"""\n  fd = open_or_fd(file_or_fd,output_folder)\n  binary = fd.read(2).decode(); assert(binary == \'\\0B\'); # assuming it\'s binary\n\n  assert(fd.read(1).decode() == \'\\4\'); # int-size\n  vec_size = np.frombuffer(fd.read(4), dtype=\'int32\', count=1)[0] # number of frames (or bins)\n\n  data = np.frombuffer(fd.read(vec_size*10), dtype=[(\'size_beg\',\'int8\'),(\'t_beg\',\'float32\'),(\'size_end\',\'int8\'),(\'t_end\',\'float32\')], count=vec_size)\n  assert(data[0][\'size_beg\'] == 4)\n  assert(data[0][\'size_end\'] == 4)\n  ans = data[[\'t_beg\',\'t_end\']].tolist() # Return vector of tuples (t_beg,t_end),\n\n  if fd is not file_or_fd : fd.close()\n  return ans\n\n\n#################################################\n# Segments related,\n#\n\n# Segments as \'Bool vectors\' can be handy,\n# - for \'superposing\' the segmentations,\n# - for frame-selection in Speaker-ID experiments,\ndef read_segments_as_bool_vec(segments_file):\n  """""" [ bool_vec ] = read_segments_as_bool_vec(segments_file)\n   using kaldi \'segments\' file for 1 wav, format : \'<utt> <rec> <t-beg> <t-end>\'\n   - t-beg, t-end is in seconds,\n   - assumed 100 frames/second,\n  """"""\n  segs = np.loadtxt(segments_file, dtype=\'object,object,f,f\', ndmin=1)\n  # Sanity checks,\n  assert(len(segs) > 0) # empty segmentation is an error,\n  assert(len(np.unique([rec[1] for rec in segs ])) == 1) # segments with only 1 wav-file,\n  # Convert time to frame-indexes,\n  start = np.rint([100 * rec[2] for rec in segs]).astype(int)\n  end = np.rint([100 * rec[3] for rec in segs]).astype(int)\n  # Taken from \'read_lab_to_bool_vec\', htk.py,\n  frms = np.repeat(np.r_[np.tile([False,True], len(end)), False],\n                   np.r_[np.c_[start - np.r_[0, end[:-1]], end-start].flat, 0])\n  assert np.sum(end-start) == np.sum(frms)\n  return frms\n'"
ASR/neural_networks.py,92,"b'##########################################################\n# pytorch-kaldi v.0.1                                      \n# Mirco Ravanelli, Titouan Parcollet\n# Mila, University of Montreal\n# October 2018\n##########################################################\n\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport numpy as np\nfrom distutils.util import strtobool\nimport math\n\n# uncomment below if you want to use SRU\n# and you need to install SRU: pip install sru[cuda].\n# or you can install it from source code: https://github.com/taolei87/sru.\n# import sru\n\ndef context_window(fea,left,right):\n \n    N_elem=fea.shape[0]\n    N_fea=fea.shape[1]\n    \n    fea_conc=np.empty([N_elem,N_fea*(left+right+1)])\n    \n    index_fea=0\n    for lag in range(-left,right+1):\n        fea_conc[:,index_fea:index_fea+fea.shape[1]]=np.roll(fea,lag,axis=0)\n        index_fea=index_fea+fea.shape[1]\n        \n    fea_conc=fea_conc[left:fea_conc.shape[0]-right]\n    \n    return fea_conc\n\nclass LayerNorm(nn.Module):\n\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm,self).__init__()\n        self.gamma = nn.Parameter(torch.ones(features))\n        self.beta = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\n\ndef act_fun(act_type):\n\n if act_type==""relu"":\n    return nn.ReLU()\n            \n if act_type==""tanh"":\n    return nn.Tanh()\n            \n if act_type==""sigmoid"":\n    return nn.Sigmoid()\n           \n if act_type==""leaky_relu"":\n    return nn.LeakyReLU(0.2)\n            \n if act_type==""elu"":\n    return nn.ELU()\n                     \n if act_type==""softmax"":\n    return nn.LogSoftmax(dim=1)\n        \n if act_type==""linear"":\n     return nn.LeakyReLU(1) # initializzed like this, but not used in forward!\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, options,inp_dim):\n        super(MLP, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.dnn_lay=list(map(int, options[\'dnn_lay\'].split(\',\')))\n        self.dnn_drop=list(map(float, options[\'dnn_drop\'].split(\',\'))) \n        self.dnn_use_batchnorm=list(map(strtobool, options[\'dnn_use_batchnorm\'].split(\',\')))\n        self.dnn_use_laynorm=list(map(strtobool, options[\'dnn_use_laynorm\'].split(\',\'))) \n        self.dnn_use_laynorm_inp=strtobool(options[\'dnn_use_laynorm_inp\'])\n        self.dnn_use_batchnorm_inp=strtobool(options[\'dnn_use_batchnorm_inp\'])\n        self.dnn_act=options[\'dnn_act\'].split(\',\')\n        \n       \n        self.wx  = nn.ModuleList([])\n        self.bn  = nn.ModuleList([])\n        self.ln  = nn.ModuleList([])\n        self.act = nn.ModuleList([])\n        self.drop = nn.ModuleList([])\n       \n  \n        # input layer normalization\n        if self.dnn_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # input batch normalization    \n        if self.dnn_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n           \n        self.N_dnn_lay=len(self.dnn_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_dnn_lay):\n            \n             # dropout\n             self.drop.append(nn.Dropout(p=self.dnn_drop[i]))\n             \n             # activation\n             self.act.append(act_fun(self.dnn_act[i]))\n             \n             \n             add_bias=True\n             \n             # layer norm initialization\n             self.ln.append(LayerNorm(self.dnn_lay[i]))\n             self.bn.append(nn.BatchNorm1d(self.dnn_lay[i],momentum=0.05))\n             \n             if self.dnn_use_laynorm[i] or self.dnn_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Linear operations\n             self.wx.append(nn.Linear(current_input, self.dnn_lay[i],bias=add_bias))\n             \n             # weight initialization\n             self.wx[i].weight = torch.nn.Parameter(torch.Tensor(self.dnn_lay[i],current_input).uniform_(-np.sqrt(0.01/(current_input+self.dnn_lay[i])),np.sqrt(0.01/(current_input+self.dnn_lay[i]))))\n             self.wx[i].bias = torch.nn.Parameter(torch.zeros(self.dnn_lay[i]))\n             \n             current_input=self.dnn_lay[i]\n             \n        self.out_dim=current_input\n         \n    def forward(self, x):\n        \n      # Applying Layer/Batch Norm\n      if bool(self.dnn_use_laynorm_inp):\n        x=self.ln0((x))\n        \n      if bool(self.dnn_use_batchnorm_inp):\n\n        x=self.bn0((x))\n        \n      for i in range(self.N_dnn_lay):\n           \n          if self.dnn_use_laynorm[i] and not(self.dnn_use_batchnorm[i]):\n           x = self.drop[i](self.act[i](self.ln[i](self.wx[i](x))))\n          \n          if self.dnn_use_batchnorm[i] and not(self.dnn_use_laynorm[i]):\n           x = self.drop[i](self.act[i](self.bn[i](self.wx[i](x))))\n           \n          if self.dnn_use_batchnorm[i]==True and self.dnn_use_laynorm[i]==True:\n           x = self.drop[i](self.act[i](self.bn[i](self.ln[i](self.wx[i](x)))))\n          \n          if self.dnn_use_batchnorm[i]==False and self.dnn_use_laynorm[i]==False:\n           x = self.drop[i](self.act[i](self.wx[i](x)))\n            \n          \n      return x\n\n\nclass LSTM_cudnn(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(LSTM_cudnn, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.hidden_size=int(options[\'hidden_size\'])\n        self.num_layers=int(options[\'num_layers\'])\n        self.bias=bool(strtobool(options[\'bias\']))\n        self.batch_first=bool(strtobool(options[\'batch_first\']))\n        self.dropout=float(options[\'dropout\'])\n        self.bidirectional=bool(strtobool(options[\'bidirectional\']))\n        \n        self.lstm = nn.ModuleList([nn.LSTM(self.input_dim, self.hidden_size, self.num_layers, \n                            bias=self.bias,dropout=self.dropout,bidirectional=self.bidirectional)])\n         \n        self.out_dim=self.hidden_size+self.bidirectional*self.hidden_size\n               \n        \n    def forward(self, x):\n        \n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n            c0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            c0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            \n        if x.is_cuda:\n            h0=h0.cuda()\n            c0=c0.cuda()\n            \n        output, (hn, cn) = self.lstm[0](x, (h0, c0))\n        \n        \n        return output\n    \n\nclass GRU_cudnn(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(GRU_cudnn, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.hidden_size=int(options[\'hidden_size\'])\n        self.num_layers=int(options[\'num_layers\'])\n        self.bias=bool(strtobool(options[\'bias\']))\n        self.batch_first=bool(strtobool(options[\'batch_first\']))\n        self.dropout=float(options[\'dropout\'])\n        self.bidirectional=bool(strtobool(options[\'bidirectional\']))\n        \n        self.gru = nn.ModuleList([nn.GRU(self.input_dim, self.hidden_size, self.num_layers, \n                            bias=self.bias,dropout=self.dropout,bidirectional=self.bidirectional)])\n         \n        self.out_dim=self.hidden_size+self.bidirectional*self.hidden_size\n               \n        \n    def forward(self, x):\n        \n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            \n        if x.is_cuda:\n            h0=h0.cuda()\n\n        output, hn = self.gru[0](x, h0)\n        \n        \n        return output\n \n    \nclass RNN_cudnn(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(RNN_cudnn, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.hidden_size=int(options[\'hidden_size\'])\n        self.num_layers=int(options[\'num_layers\'])\n        self.nonlinearity=options[\'nonlinearity\']\n        self.bias=bool(strtobool(options[\'bias\']))\n        self.batch_first=bool(strtobool(options[\'batch_first\']))\n        self.dropout=float(options[\'dropout\'])\n        self.bidirectional=bool(strtobool(options[\'bidirectional\']))\n        \n        self.rnn = nn.ModuleList([nn.RNN(self.input_dim, self.hidden_size, self.num_layers, \n                            nonlinearity=self.nonlinearity,bias=self.bias,dropout=self.dropout,bidirectional=self.bidirectional)])\n         \n        self.out_dim=self.hidden_size+self.bidirectional*self.hidden_size\n               \n        \n    def forward(self, x):\n        \n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            \n        if x.is_cuda:\n            h0=h0.cuda()\n\n        output, hn = self.rnn[0](x, h0)\n        \n        \n        return output\n    \n    \nclass LSTM(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(LSTM, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.lstm_lay=list(map(int, options[\'lstm_lay\'].split(\',\')))\n        self.lstm_drop=list(map(float, options[\'lstm_drop\'].split(\',\'))) \n        self.lstm_use_batchnorm=list(map(strtobool, options[\'lstm_use_batchnorm\'].split(\',\')))\n        self.lstm_use_laynorm=list(map(strtobool, options[\'lstm_use_laynorm\'].split(\',\'))) \n        self.lstm_use_laynorm_inp=strtobool(options[\'lstm_use_laynorm_inp\'])\n        self.lstm_use_batchnorm_inp=strtobool(options[\'lstm_use_batchnorm_inp\'])\n        self.lstm_act=options[\'lstm_act\'].split(\',\')\n        self.lstm_orthinit=strtobool(options[\'lstm_orthinit\'])\n\n        self.bidir=strtobool(options[\'lstm_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wfx  = nn.ModuleList([]) # Forget\n        self.ufh  = nn.ModuleList([]) # Forget\n        \n        self.wix  = nn.ModuleList([]) # Input\n        self.uih  = nn.ModuleList([]) # Input  \n        \n        self.wox  = nn.ModuleList([]) # Output\n        self.uoh  = nn.ModuleList([]) # Output  \n        \n        self.wcx  = nn.ModuleList([]) # Cell state\n        self.uch = nn.ModuleList([])  # Cell state\n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wfx  = nn.ModuleList([]) # Batch Norm\n        self.bn_wix  = nn.ModuleList([]) # Batch Norm\n        self.bn_wox  = nn.ModuleList([]) # Batch Norm\n        self.bn_wcx = nn.ModuleList([]) # Batch Norm\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.lstm_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.lstm_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_lstm_lay=len(self.lstm_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_lstm_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.lstm_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.lstm_use_laynorm[i] or self.lstm_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wfx.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n             self.wix.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n             self.wox.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n             self.wcx.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n            \n             # Recurrent connections\n             self.ufh.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             self.uih.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             self.uoh.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             self.uch.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             \n             if self.lstm_orthinit:\n                nn.init.orthogonal_(self.ufh[i].weight)\n                nn.init.orthogonal_(self.uih[i].weight)\n                nn.init.orthogonal_(self.uoh[i].weight)\n                nn.init.orthogonal_(self.uch[i].weight)\n            \n             \n             # batch norm initialization\n             self.bn_wfx.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n             self.bn_wix.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n             self.bn_wox.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n             self.bn_wcx.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n                \n             self.ln.append(LayerNorm(self.lstm_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.lstm_lay[i]\n             else:\n                 current_input=self.lstm_lay[i]\n                 \n        self.out_dim=self.lstm_lay[i]+self.bidir*self.lstm_lay[i]\n            \n             \n        \n    def forward(self, x):\n\n        # Applying Layer/Batch Norm\n        if bool(self.lstm_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.lstm_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_lstm_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.lstm_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.lstm_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.lstm_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.lstm_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wfx_out=self.wfx[i](x)\n            wix_out=self.wix[i](x)\n            wox_out=self.wox[i](x)\n            wcx_out=self.wcx[i](x)\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.lstm_use_batchnorm[i]:\n\n                wfx_out_bn=self.bn_wfx[i](wfx_out.view(wfx_out.shape[0]*wfx_out.shape[1],wfx_out.shape[2]))\n                wfx_out=wfx_out_bn.view(wfx_out.shape[0],wfx_out.shape[1],wfx_out.shape[2])\n         \n                wix_out_bn=self.bn_wix[i](wix_out.view(wix_out.shape[0]*wix_out.shape[1],wix_out.shape[2]))\n                wix_out=wix_out_bn.view(wix_out.shape[0],wix_out.shape[1],wix_out.shape[2])\n   \n                wox_out_bn=self.bn_wox[i](wox_out.view(wox_out.shape[0]*wox_out.shape[1],wox_out.shape[2]))\n                wox_out=wox_out_bn.view(wox_out.shape[0],wox_out.shape[1],wox_out.shape[2])\n\n                wcx_out_bn=self.bn_wcx[i](wcx_out.view(wcx_out.shape[0]*wcx_out.shape[1],wcx_out.shape[2]))\n                wcx_out=wcx_out_bn.view(wcx_out.shape[0],wcx_out.shape[1],wcx_out.shape[2]) \n            \n            \n            # Processing time steps\n            hiddens = []\n            ct=h_init\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # LSTM equations\n                ft=torch.sigmoid(wfx_out[k]+self.ufh[i](ht))\n                it=torch.sigmoid(wix_out[k]+self.uih[i](ht))\n                ot=torch.sigmoid(wox_out[k]+self.uoh[i](ht))\n                ct=it*self.act[i](wcx_out[k]+self.uch[i](ht))*drop_mask+ft*ct\n                ht=ot*self.act[i](ct)\n                \n                if self.lstm_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n    \nclass GRU(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(GRU, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.gru_lay=list(map(int, options[\'gru_lay\'].split(\',\')))\n        self.gru_drop=list(map(float, options[\'gru_drop\'].split(\',\'))) \n        self.gru_use_batchnorm=list(map(strtobool, options[\'gru_use_batchnorm\'].split(\',\')))\n        self.gru_use_laynorm=list(map(strtobool, options[\'gru_use_laynorm\'].split(\',\'))) \n        self.gru_use_laynorm_inp=strtobool(options[\'gru_use_laynorm_inp\'])\n        self.gru_use_batchnorm_inp=strtobool(options[\'gru_use_batchnorm_inp\'])\n        self.gru_orthinit=strtobool(options[\'gru_orthinit\'])\n        self.gru_act=options[\'gru_act\'].split(\',\')\n        self.bidir=strtobool(options[\'gru_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n        \n        self.wz  = nn.ModuleList([]) # Update Gate\n        self.uz  = nn.ModuleList([]) # Update Gate\n        \n        self.wr  = nn.ModuleList([]) # Reset Gate\n        self.ur  = nn.ModuleList([]) # Reset Gate  \n        \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n        self.bn_wz  = nn.ModuleList([]) # Batch Norm\n        self.bn_wr  = nn.ModuleList([]) # Batch Norm\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.gru_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.gru_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_gru_lay=len(self.gru_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_gru_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.gru_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.gru_use_laynorm[i] or self.gru_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.gru_lay[i],bias=add_bias))\n             self.wz.append(nn.Linear(current_input, self.gru_lay[i],bias=add_bias))\n             self.wr.append(nn.Linear(current_input, self.gru_lay[i],bias=add_bias))\n             \n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.gru_lay[i], self.gru_lay[i],bias=False))\n             self.uz.append(nn.Linear(self.gru_lay[i], self.gru_lay[i],bias=False))\n             self.ur.append(nn.Linear(self.gru_lay[i], self.gru_lay[i],bias=False))\n\n             if self.gru_orthinit:\n                nn.init.orthogonal_(self.uh[i].weight)\n                nn.init.orthogonal_(self.uz[i].weight)\n                nn.init.orthogonal_(self.ur[i].weight)\n\n             \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.gru_lay[i],momentum=0.05))\n             self.bn_wz.append(nn.BatchNorm1d(self.gru_lay[i],momentum=0.05))\n             self.bn_wr.append(nn.BatchNorm1d(self.gru_lay[i],momentum=0.05))\n\n                \n             self.ln.append(LayerNorm(self.gru_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.gru_lay[i]\n             else:\n                 current_input=self.gru_lay[i]\n                 \n        self.out_dim=self.gru_lay[i]+self.bidir*self.gru_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.gru_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.gru_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_gru_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.gru_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.gru_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.gru_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.gru_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            wz_out=self.wz[i](x)\n            wr_out=self.wr[i](x)\n\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.gru_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n                wz_out_bn=self.bn_wz[i](wz_out.view(wz_out.shape[0]*wz_out.shape[1],wz_out.shape[2]))\n                wz_out=wz_out_bn.view(wz_out.shape[0],wz_out.shape[1],wz_out.shape[2])\n   \n                wr_out_bn=self.bn_wr[i](wr_out.view(wr_out.shape[0]*wr_out.shape[1],wr_out.shape[2]))\n                wr_out=wr_out_bn.view(wr_out.shape[0],wr_out.shape[1],wr_out.shape[2])\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # gru equation\n                zt=torch.sigmoid(wz_out[k]+self.uz[i](ht))\n                rt=torch.sigmoid(wr_out[k]+self.ur[i](ht))\n                at=wh_out[k]+self.uh[i](rt*ht)\n                hcand=self.act[i](at)*drop_mask\n                ht=(zt*ht+(1-zt)*hcand)\n                \n                \n                if self.gru_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n\n\nclass liGRU(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(liGRU, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.ligru_lay=list(map(int, options[\'ligru_lay\'].split(\',\')))\n        self.ligru_drop=list(map(float, options[\'ligru_drop\'].split(\',\'))) \n        self.ligru_use_batchnorm=list(map(strtobool, options[\'ligru_use_batchnorm\'].split(\',\')))\n        self.ligru_use_laynorm=list(map(strtobool, options[\'ligru_use_laynorm\'].split(\',\'))) \n        self.ligru_use_laynorm_inp=strtobool(options[\'ligru_use_laynorm_inp\'])\n        self.ligru_use_batchnorm_inp=strtobool(options[\'ligru_use_batchnorm_inp\'])\n        self.ligru_orthinit=strtobool(options[\'ligru_orthinit\'])\n        self.ligru_act=options[\'ligru_act\'].split(\',\')\n        self.bidir=strtobool(options[\'ligru_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n        \n        self.wz  = nn.ModuleList([]) # Update Gate\n        self.uz  = nn.ModuleList([]) # Update Gate\n              \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n        self.bn_wz  = nn.ModuleList([]) # Batch Norm\n\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.ligru_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.ligru_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_ligru_lay=len(self.ligru_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_ligru_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.ligru_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.ligru_use_laynorm[i] or self.ligru_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.ligru_lay[i],bias=add_bias))\n             self.wz.append(nn.Linear(current_input, self.ligru_lay[i],bias=add_bias))\n\n             \n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i],bias=False))\n             self.uz.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i],bias=False))\n\n             if self.ligru_orthinit:\n             \tnn.init.orthogonal_(self.uh[i].weight)\n             \tnn.init.orthogonal_(self.uz[i].weight)\n\n\n             \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.ligru_lay[i],momentum=0.05))\n             self.bn_wz.append(nn.BatchNorm1d(self.ligru_lay[i],momentum=0.05))\n\n\n                \n             self.ln.append(LayerNorm(self.ligru_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.ligru_lay[i]\n             else:\n                 current_input=self.ligru_lay[i]\n                 \n        self.out_dim=self.ligru_lay[i]+self.bidir*self.ligru_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.ligru_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.ligru_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_ligru_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.ligru_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.ligru_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.ligru_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.ligru_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            wz_out=self.wz[i](x)\n\n\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.ligru_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n                wz_out_bn=self.bn_wz[i](wz_out.view(wz_out.shape[0]*wz_out.shape[1],wz_out.shape[2]))\n                wz_out=wz_out_bn.view(wz_out.shape[0],wz_out.shape[1],wz_out.shape[2])\n\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # ligru equation\n                zt=torch.sigmoid(wz_out[k]+self.uz[i](ht))\n                at=wh_out[k]+self.uh[i](ht)\n                hcand=self.act[i](at)*drop_mask\n                ht=(zt*ht+(1-zt)*hcand)\n                \n                \n                if self.ligru_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n    \nclass minimalGRU(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(minimalGRU, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.minimalgru_lay=list(map(int, options[\'minimalgru_lay\'].split(\',\')))\n        self.minimalgru_drop=list(map(float, options[\'minimalgru_drop\'].split(\',\'))) \n        self.minimalgru_use_batchnorm=list(map(strtobool, options[\'minimalgru_use_batchnorm\'].split(\',\')))\n        self.minimalgru_use_laynorm=list(map(strtobool, options[\'minimalgru_use_laynorm\'].split(\',\'))) \n        self.minimalgru_use_laynorm_inp=strtobool(options[\'minimalgru_use_laynorm_inp\'])\n        self.minimalgru_use_batchnorm_inp=strtobool(options[\'minimalgru_use_batchnorm_inp\'])\n        self.minimalgru_orthinit=strtobool(options[\'minimalgru_orthinit\'])\n        self.minimalgru_act=options[\'minimalgru_act\'].split(\',\')\n        self.bidir=strtobool(options[\'minimalgru_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n        \n        self.wz  = nn.ModuleList([]) # Update Gate\n        self.uz  = nn.ModuleList([]) # Update Gate\n              \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n        self.bn_wz  = nn.ModuleList([]) # Batch Norm\n\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.minimalgru_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.minimalgru_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_minimalgru_lay=len(self.minimalgru_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_minimalgru_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.minimalgru_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.minimalgru_use_laynorm[i] or self.minimalgru_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.minimalgru_lay[i],bias=add_bias))\n             self.wz.append(nn.Linear(current_input, self.minimalgru_lay[i],bias=add_bias))\n\n             \n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.minimalgru_lay[i], self.minimalgru_lay[i],bias=False))\n             self.uz.append(nn.Linear(self.minimalgru_lay[i], self.minimalgru_lay[i],bias=False))\n\n             if self.minimalgru_orthinit:\n             \tnn.init.orthogonal_(self.uh[i].weight)\n             \tnn.init.orthogonal_(self.uz[i].weight)\n\n\n             \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.minimalgru_lay[i],momentum=0.05))\n             self.bn_wz.append(nn.BatchNorm1d(self.minimalgru_lay[i],momentum=0.05))\n\n\n                \n             self.ln.append(LayerNorm(self.minimalgru_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.minimalgru_lay[i]\n             else:\n                 current_input=self.minimalgru_lay[i]\n                 \n        self.out_dim=self.minimalgru_lay[i]+self.bidir*self.minimalgru_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.minimalgru_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.minimalgru_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_minimalgru_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.minimalgru_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.minimalgru_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.minimalgru_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.minimalgru_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            wz_out=self.wz[i](x)\n\n\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.minimalgru_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n                wz_out_bn=self.bn_wz[i](wz_out.view(wz_out.shape[0]*wz_out.shape[1],wz_out.shape[2]))\n                wz_out=wz_out_bn.view(wz_out.shape[0],wz_out.shape[1],wz_out.shape[2])\n\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # minimalgru equation\n                zt=torch.sigmoid(wz_out[k]+self.uz[i](ht))\n                at=wh_out[k]+self.uh[i](zt*ht)\n                hcand=self.act[i](at)*drop_mask\n                ht=(zt*ht+(1-zt)*hcand)\n                \n                \n                if self.minimalgru_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n\nclass RNN(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(RNN, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.rnn_lay=list(map(int, options[\'rnn_lay\'].split(\',\')))\n        self.rnn_drop=list(map(float, options[\'rnn_drop\'].split(\',\'))) \n        self.rnn_use_batchnorm=list(map(strtobool, options[\'rnn_use_batchnorm\'].split(\',\')))\n        self.rnn_use_laynorm=list(map(strtobool, options[\'rnn_use_laynorm\'].split(\',\'))) \n        self.rnn_use_laynorm_inp=strtobool(options[\'rnn_use_laynorm_inp\'])\n        self.rnn_use_batchnorm_inp=strtobool(options[\'rnn_use_batchnorm_inp\'])\n        self.rnn_orthinit=strtobool(options[\'rnn_orthinit\'])\n        self.rnn_act=options[\'rnn_act\'].split(\',\')\n        self.bidir=strtobool(options[\'rnn_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n                   \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.rnn_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.rnn_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_rnn_lay=len(self.rnn_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_rnn_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.rnn_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.rnn_use_laynorm[i] or self.rnn_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.rnn_lay[i],bias=add_bias))\n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.rnn_lay[i], self.rnn_lay[i],bias=False))\n\n             if self.rnn_orthinit:\n             \tnn.init.orthogonal_(self.uh[i].weight)\n          \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.rnn_lay[i],momentum=0.05))\n\n             self.ln.append(LayerNorm(self.rnn_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.rnn_lay[i]\n             else:\n                 current_input=self.rnn_lay[i]\n                 \n        self.out_dim=self.rnn_lay[i]+self.bidir*self.rnn_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.rnn_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.rnn_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_rnn_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.rnn_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.rnn_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.rnn_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.rnn_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.rnn_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # rnn equation\n                at=wh_out[k]+self.uh[i](ht)\n                ht=self.act[i](at)*drop_mask\n                \n                \n                if self.rnn_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n\nclass CNN(nn.Module):\n    \n    def __init__(self,options,inp_dim):\n       super(CNN,self).__init__()\n    \n       # Reading parameters\n       self.input_dim=inp_dim\n       self.cnn_N_filt=list(map(int, options[\'cnn_N_filt\'].split(\',\')))\n\n       self.cnn_len_filt=list(map(int, options[\'cnn_len_filt\'].split(\',\')))\n       self.cnn_max_pool_len=list(map(int, options[\'cnn_max_pool_len\'].split(\',\')))\n       \n       self.cnn_act=options[\'cnn_act\'].split(\',\')\n       self.cnn_drop=list(map(float, options[\'cnn_drop\'].split(\',\')))\n       \n       self.cnn_use_laynorm=list(map(strtobool, options[\'cnn_use_laynorm\'].split(\',\')))\n       self.cnn_use_batchnorm=list(map(strtobool, options[\'cnn_use_batchnorm\'].split(\',\')))\n       self.cnn_use_laynorm_inp=strtobool(options[\'cnn_use_laynorm_inp\'])\n       self.cnn_use_batchnorm_inp=strtobool(options[\'cnn_use_batchnorm_inp\'])\n       \n       self.N_cnn_lay=len(self.cnn_N_filt)\n       self.conv  = nn.ModuleList([])\n       self.bn  = nn.ModuleList([])\n       self.ln  = nn.ModuleList([])\n       self.act = nn.ModuleList([])\n       self.drop = nn.ModuleList([])\n       \n       \n       if self.cnn_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n           \n       if self.cnn_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n           \n       current_input=self.input_dim \n       \n       for i in range(self.N_cnn_lay):\n\n         N_filt=int(self.cnn_N_filt[i])\n         len_filt=int(self.cnn_len_filt[i])\n         \n         # dropout\n         self.drop.append(nn.Dropout(p=self.cnn_drop[i]))\n         \n         # activation\n         self.act.append(act_fun(self.cnn_act[i]))\n            \n         # layer norm initialization         \n         self.ln.append(LayerNorm([N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])]))\n\n         self.bn.append(nn.BatchNorm1d(N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i]),momentum=0.05))\n\n            \n\n         if i==0:\n          self.conv.append(nn.Conv1d(1, N_filt, len_filt))\n              \n         else:\n          self.conv.append(nn.Conv1d(self.cnn_N_filt[i-1], self.cnn_N_filt[i], self.cnn_len_filt[i]))\n          \n         current_input=int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])\n\n         \n       self.out_dim=current_input*N_filt\n\n\n\n    def forward(self, x):\n        \n       batch=x.shape[0]\n       seq_len=x.shape[1]\n       \n       if bool(self.cnn_use_laynorm_inp):\n        x=self.ln0((x))\n        \n       if bool(self.cnn_use_batchnorm_inp):\n        x=self.bn0((x))\n        \n       x=x.view(batch,1,seq_len)\n       \n       for i in range(self.N_cnn_lay):\n           \n         if self.cnn_use_laynorm[i]:\n          x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))\n          \n         if self.cnn_use_batchnorm[i]:\n          x = self.drop[i](self.act[i](self.bn[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))\n\n         if self.cnn_use_batchnorm[i]==False and self.cnn_use_laynorm[i]==False:\n          x = self.drop[i](self.act[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i])))\n\n       \n       x = x.view(batch,-1)\n\n       return x\n\n\nclass SincNet(nn.Module):\n    \n    def __init__(self,options,inp_dim):\n       super(SincNet,self).__init__()\n    \n       # Reading parameters\n       self.input_dim=inp_dim\n       self.sinc_N_filt=list(map(int, options[\'sinc_N_filt\'].split(\',\')))\n\n       self.sinc_len_filt=list(map(int, options[\'sinc_len_filt\'].split(\',\')))\n       self.sinc_max_pool_len=list(map(int, options[\'sinc_max_pool_len\'].split(\',\')))\n       \n       self.sinc_act=options[\'sinc_act\'].split(\',\')\n       self.sinc_drop=list(map(float, options[\'sinc_drop\'].split(\',\')))\n       \n       self.sinc_use_laynorm=list(map(strtobool, options[\'sinc_use_laynorm\'].split(\',\')))\n       self.sinc_use_batchnorm=list(map(strtobool, options[\'sinc_use_batchnorm\'].split(\',\')))\n       self.sinc_use_laynorm_inp=strtobool(options[\'sinc_use_laynorm_inp\'])\n       self.sinc_use_batchnorm_inp=strtobool(options[\'sinc_use_batchnorm_inp\'])\n       \n       self.N_sinc_lay=len(self.sinc_N_filt)\n       \n       self.sinc_sample_rate=int(options[\'sinc_sample_rate\'])\n       self.sinc_min_low_hz=int(options[\'sinc_min_low_hz\'])\n       self.sinc_min_band_hz=int(options[\'sinc_min_band_hz\'])\n\n       \n       self.conv  = nn.ModuleList([])\n       self.bn  = nn.ModuleList([])\n       self.ln  = nn.ModuleList([])\n       self.act = nn.ModuleList([])\n       self.drop = nn.ModuleList([])\n       \n       \n       if self.sinc_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n           \n       if self.sinc_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n           \n       current_input=self.input_dim \n       \n       for i in range(self.N_sinc_lay):\n\n         N_filt=int(self.sinc_N_filt[i])\n         len_filt=int(self.sinc_len_filt[i])\n         \n         # dropout\n         self.drop.append(nn.Dropout(p=self.sinc_drop[i]))\n         \n         # activation\n         self.act.append(act_fun(self.sinc_act[i]))\n            \n         # layer norm initialization         \n         self.ln.append(LayerNorm([N_filt,int((current_input-self.sinc_len_filt[i]+1)/self.sinc_max_pool_len[i])]))\n\n         self.bn.append(nn.BatchNorm1d(N_filt,int((current_input-self.sinc_len_filt[i]+1)/self.sinc_max_pool_len[i]),momentum=0.05))\n\n            \n\n         if i==0:\n          self.conv.append(SincConv(1, N_filt, len_filt,sample_rate=self.sinc_sample_rate, min_low_hz=self.sinc_min_low_hz, min_band_hz=self.sinc_min_band_hz))\n              \n         else:\n          self.conv.append(nn.Conv1d(self.sinc_N_filt[i-1], self.sinc_N_filt[i], self.sinc_len_filt[i]))\n          \n         current_input=int((current_input-self.sinc_len_filt[i]+1)/self.sinc_max_pool_len[i])\n\n         \n       self.out_dim=current_input*N_filt\n\n\n\n    def forward(self, x):\n        \n       batch=x.shape[0]\n       seq_len=x.shape[1]\n       \n       if bool(self.sinc_use_laynorm_inp):\n        x=self.ln0(x)\n        \n       if bool(self.sinc_use_batchnorm_inp):\n        x=self.bn0(x)\n        \n       x=x.view(batch,1,seq_len)\n       \n       for i in range(self.N_sinc_lay):\n           \n         if self.sinc_use_laynorm[i]:\n          x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i]))))\n          \n         if self.sinc_use_batchnorm[i]:\n          x = self.drop[i](self.act[i](self.bn[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i]))))\n\n         if self.sinc_use_batchnorm[i]==False and self.sinc_use_laynorm[i]==False:\n          x = self.drop[i](self.act[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i])))\n\n       \n       x = x.view(batch,-1)\n\n       return x\n\n\n\nclass SincConv(nn.Module):\n    """"""Sinc-based convolution\n    Parameters\n    ----------\n    in_channels : `int`\n        Number of input channels. Must be 1.\n    out_channels : `int`\n        Number of filters.\n    kernel_size : `int`\n        Filter length.\n    sample_rate : `int`, optional\n        Sample rate. Defaults to 16000.\n    Usage\n    -----\n    See `torch.nn.Conv1d`\n    Reference\n    ---------\n    Mirco Ravanelli, Yoshua Bengio,\n    ""Speaker Recognition from raw waveform with SincNet"".\n    https://arxiv.org/abs/1808.00158\n    """"""\n\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride=1, padding=0, dilation=1, bias=False, groups=1,\n                 sample_rate=16000, min_low_hz=50, min_band_hz=50):\n\n        super(SincConv,self).__init__()\n\n        if in_channels != 1:\n            #msg = (f\'SincConv only support one input channel \'\n            #       f\'(here, in_channels = {in_channels:d}).\')\n            msg = ""SincConv only support one input channel (here, in_channels = {%i})"" % (in_channels)\n            raise ValueError(msg)\n\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        \n        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n        if kernel_size%2==0:\n            self.kernel_size=self.kernel_size+1\n            \n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n\n        if bias:\n            raise ValueError(\'SincConv does not support bias.\')\n        if groups > 1:\n            raise ValueError(\'SincConv does not support groups.\')\n\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # initialize filterbanks such that they are equally spaced in Mel scale\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n\n        mel = np.linspace(self.to_mel(low_hz),\n                          self.to_mel(high_hz),\n                          self.out_channels + 1)\n        hz = self.to_hz(mel) / self.sample_rate\n        \n\n        # filter lower frequency (out_channels, 1)\n        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n\n        # filter frequency band (out_channels, 1)\n        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n\n        # Hamming window\n        #self.window_ = torch.hamming_window(self.kernel_size)\n        n_lin=torch.linspace(0, self.kernel_size, steps=self.kernel_size)\n        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n\n        # (kernel_size, 1)\n        n = (self.kernel_size - 1) / 2\n        self.n_ = torch.arange(-n, n+1).view(1, -1) / self.sample_rate\n\n\n    def sinc(self, x):\n        # Numerically stable definition\n        x_left=x[:,0:int((x.shape[1]-1)/2)]\n        y_left=torch.sin(x_left) / x_left\n        y_right= torch.flip(y_left,dims=[1])\n        \n        sinc=torch.cat([y_left,torch.ones([x.shape[0],1]).to(x.device),y_right],dim=1)\n        \n\n        return sinc\n\n    def forward(self, waveforms):\n        """"""\n        Parameters\n        ----------\n        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n            Batch of waveforms.\n        Returns\n        -------\n        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n            Batch of sinc filters activations.\n        """"""\n\n        self.n_ = self.n_.to(waveforms.device)\n\n        self.window_ = self.window_.to(waveforms.device)\n\n        low = self.min_low_hz / self.sample_rate + torch.abs(self.low_hz_)\n        high = low + self.min_band_hz /self.sample_rate + torch.abs(self.band_hz_)\n\n        f_times_t = torch.matmul(low, self.n_)\n\n        low_pass1 = 2 * low * self.sinc(\n            2 * math.pi * f_times_t * self.sample_rate)\n        \n\n\n        f_times_t = torch.matmul(high, self.n_)\n        low_pass2 = 2 * high * self.sinc(\n            2 * math.pi * f_times_t * self.sample_rate)\n        \n\n        band_pass = low_pass2 - low_pass1\n        max_, _ = torch.max(band_pass, dim=1, keepdim=True)\n        band_pass = band_pass / max_\n\n        self.filters = (band_pass * self.window_).view(\n            self.out_channels, 1, self.kernel_size)\n\n        return F.conv1d(waveforms, self.filters, stride=self.stride,\n                        padding=self.padding, dilation=self.dilation,\n                         bias=None, groups=1) \n        \n        \nclass SincConv_fast(nn.Module):\n    """"""Sinc-based convolution\n    Parameters\n    ----------\n    in_channels : `int`\n        Number of input channels. Must be 1.\n    out_channels : `int`\n        Number of filters.\n    kernel_size : `int`\n        Filter length.\n    sample_rate : `int`, optional\n        Sample rate. Defaults to 16000.\n    Usage\n    -----\n    See `torch.nn.Conv1d`\n    Reference\n    ---------\n    Mirco Ravanelli, Yoshua Bengio,\n    ""Speaker Recognition from raw waveform with SincNet"".\n    https://arxiv.org/abs/1808.00158\n    """"""\n\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride=1, padding=0, dilation=1, bias=False, groups=1,\n                 sample_rate=16000, min_low_hz=50, min_band_hz=50):\n\n        super(SincConv_fast,self).__init__()\n\n        if in_channels != 1:\n            #msg = (f\'SincConv only support one input channel \'\n            #       f\'(here, in_channels = {in_channels:d}).\')\n            msg = ""SincConv only support one input channel (here, in_channels = {%i})"" % (in_channels)\n            raise ValueError(msg)\n\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        \n        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n        if kernel_size%2==0:\n            self.kernel_size=self.kernel_size+1\n            \n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n\n        if bias:\n            raise ValueError(\'SincConv does not support bias.\')\n        if groups > 1:\n            raise ValueError(\'SincConv does not support groups.\')\n\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # initialize filterbanks such that they are equally spaced in Mel scale\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n\n        mel = np.linspace(self.to_mel(low_hz),\n                          self.to_mel(high_hz),\n                          self.out_channels + 1)\n        hz = self.to_hz(mel)\n        \n\n        # filter lower frequency (out_channels, 1)\n        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n\n        # filter frequency band (out_channels, 1)\n        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n\n        # Hamming window\n        #self.window_ = torch.hamming_window(self.kernel_size)\n        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\n        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n\n\n        # (kernel_size, 1)\n        n = (self.kernel_size - 1) / 2.0\n        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\n\n \n\n\n    def forward(self, waveforms):\n        """"""\n        Parameters\n        ----------\n        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n            Batch of waveforms.\n        Returns\n        -------\n        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n            Batch of sinc filters activations.\n        """"""\n\n        self.n_ = self.n_.to(waveforms.device)\n\n        self.window_ = self.window_.to(waveforms.device)\n\n        low = self.min_low_hz  + torch.abs(self.low_hz_)\n        \n        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n        band=(high-low)[:,0]\n        \n        f_times_t_low = torch.matmul(low, self.n_)\n        f_times_t_high = torch.matmul(high, self.n_)\n\n        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations. \n        band_pass_center = 2*band.view(-1,1)\n        band_pass_right= torch.flip(band_pass_left,dims=[1])\n        \n        \n        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n\n        \n        band_pass = band_pass / (2*band[:,None])\n        \n\n        self.filters = (band_pass).view(\n            self.out_channels, 1, self.kernel_size)\n\n        return F.conv1d(waveforms, self.filters, stride=self.stride,\n                        padding=self.padding, dilation=self.dilation,\n                         bias=None, groups=1) \n\n        \ndef flip(x, dim):\n    xsize = x.size()\n    dim = x.dim() + dim if dim < 0 else dim\n    x = x.contiguous()\n    x = x.view(-1, *xsize[dim:])\n    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, -1, -1), (\'cpu\',\'cuda\')[x.is_cuda])().long(), :]\n    return x.view(xsize)\n\nclass SRU(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(SRU, self).__init__()\n        self.input_dim = inp_dim\n        self.hidden_size = int(options[\'sru_hidden_size\'])\n        self.num_layers = int(options[\'sru_num_layers\'])\n        self.dropout = float(options[\'sru_dropout\'])\n        self.rnn_dropout = float(options[\'sru_rnn_dropout\'])\n        self.use_tanh = bool(strtobool(options[\'sru_use_tanh\']))\n        self.use_relu = bool(strtobool(options[\'sru_use_relu\']))\n        self.use_selu = bool(strtobool(options[\'sru_use_selu\']))\n        self.weight_norm = bool(strtobool(options[\'sru_weight_norm\']))\n        self.layer_norm = bool(strtobool(options[\'sru_layer_norm\']))\n        self.bidirectional = bool(strtobool(options[\'sru_bidirectional\']))\n        self.is_input_normalized = bool(strtobool(options[\'sru_is_input_normalized\']))\n        self.has_skip_term = bool(strtobool(options[\'sru_has_skip_term\']))\n        self.rescale = bool(strtobool(options[\'sru_rescale\']))\n        self.highway_bias = float(options[\'sru_highway_bias\'])\n        self.n_proj = int(options[\'sru_n_proj\'])\n        self.sru = sru.SRU(self.input_dim, self.hidden_size,\n                            num_layers=self.num_layers,\n                            dropout=self.dropout,\n                            rnn_dropout=self.rnn_dropout,\n                            bidirectional=self.bidirectional,\n                            n_proj=self.n_proj,\n                            use_tanh=self.use_tanh,\n                            use_selu=self.use_selu,\n                            use_relu=self.use_relu,\n                            weight_norm=self.weight_norm,\n                            layer_norm=self.layer_norm,\n                            has_skip_term=self.has_skip_term,\n                            is_input_normalized=self.is_input_normalized,\n                            highway_bias=self.highway_bias,\n                            rescale=self.rescale)\n        self.out_dim = self.hidden_size+self.bidirectional*self.hidden_size\n\n    def forward(self, x):\n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size*2)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n        if x.is_cuda:\n            h0 = h0.cuda()\n        output, hn = self.sru(x, c0=h0)\n        return output\n\n'"
ASR/run_TIMIT_fast.py,19,"b'# Mirco Ravanelli\n# Mila, June 2019\n\n# This script runs a simple speech recognition experiment on the top of PASE features. \n# The results are reported in terms of Frame Error Rate over phonemes (context-independent). \n# This system is not designed for an extensive evaluation of PASE features, but mainly for quickly monitoring the performance of PASE during the self-supervised training phase.\n# The results are printed in standard output and within the text file specified in the last argument.\n\n# To run it:\n# python run_TIMIT_fast.py ../cfg/PASE.cfg ../PASE.ckpt /home/mirco/Dataset/TIMIT  TIMIT_asr_exp.res\n#\n# To run the experiment with the noisy and reverberated version of TIMIT, just change the data folder with the one containing TIMIT_rev_noise.\n\nimport warnings\nwarnings.filterwarnings(""ignore"")\n\nimport librosa\n\nimport os\nimport sys\nfrom neural_networks import MLP,context_window\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport pickle\nfrom pase.models.frontend import wf_builder\n# from waveminionet.models.frontend import wf_builder #old models\nimport soundfile as sf\nimport os\nimport json\n# import pase.models as models\n# import models.WorkerScheduler\nfrom pase.models.WorkerScheduler.encoder import *\n\n\ndef get_freer_gpu(trials=10):\n    for j in range(trials):\n         os.system(\'nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp\')\n         memory_available = [int(x.split()[2]) for x in open(\'tmp\', \'r\').readlines()]\n         dev_ = torch.device(\'cuda:\'+str(np.argmax(memory_available)))\n         try:\n            a = torch.rand(1).cuda(dev_)\n            return dev_\n         except: \n            pass\n            print(\'NO GPU AVAILABLE!!!\')\n            exit(1)\n\n\npase_cfg=sys.argv[1] # e.g, \'../cfg/PASE.cfg\'\npase_model=sys.argv[2] # e.g, \'../PASE.ckpt\'\ndata_folder=sys.argv[3] # e.g., \'/home/mirco/Dataset/TIMIT\'\noutput_file=sys.argv[4] # e.g., \'TIMIT_asr_exp.res\'\n\n\n# Label files for TIMIT\nlab_file=\'TIMIT_lab_mono.pkl\'\nlab_file_dev=\'TIMIT_lab_mono_dev.pkl\'\n\n# File list for TIMIT\ntr_lst_file=\'timit_tr.lst\'\ndev_lst_file=\'timit_dev.lst\'\n\ntr_lst = [line.rstrip(\'\\n\') for line in open(tr_lst_file)]\ndev_lst = [line.rstrip(\'\\n\') for line in open(dev_lst_file)]\n\n# Training parameters\nN_epochs=24\nseed=1234\nbatch_size=128\nhalving_factor=0.5\nlr=0.0012\nleft=1\nright=1\n\n# Neural network parameters\noptions={}\noptions[\'dnn_lay\']=\'1024,48\'\noptions[\'dnn_drop\']=\'0.15,0.0\'\noptions[\'dnn_use_batchnorm\']=\'False,False\'\noptions[\'dnn_use_laynorm\']=\'True,False\'\noptions[\'dnn_use_laynorm_inp\']=\'True\'\noptions[\'dnn_use_batchnorm_inp\']=\'False\'\noptions[\'dnn_act\']=\'relu,softmax\'\n\n#device=0 #get_freer_gpu()\ndevice=\'cuda\'\n\n\n# folder creation\ntext_file=open(output_file, ""w"")\n\n# Loading pase\npase =wf_builder(pase_cfg)\npase.load_pretrained(pase_model, load_last=True, verbose=False)\npase.to(device)\npase.eval()\n\n# reading the training signals\nprint(""Waveform reading..."")\nfea={}\nfor wav_file in tr_lst:\n    [signal, fs] = sf.read(data_folder+\'/\'+wav_file)\n    signal=signal/np.max(np.abs(signal))\n    signal = signal.astype(np.float32)\n    \n    fea_id=wav_file.split(\'/\')[-2]+\'_\'+wav_file.split(\'/\')[-1].split(\'.\')[0]\n    fea[fea_id]=torch.from_numpy(signal).float().view(1,1,-1)\n\n\n# reading the dev signals\nfea_dev={}\nfor wav_file in dev_lst:\n    [signal, fs] = sf.read(data_folder+\'/\'+wav_file)\n    signal=signal/np.max(np.abs(signal))\n    fea_id=wav_file.split(\'/\')[-2]+\'_\'+wav_file.split(\'/\')[-1].split(\'.\')[0]\n    fea_dev[fea_id]=torch.from_numpy(signal).float().view(1,1,-1)\n\n\n# Computing pase features for training\nprint(\'Computing PASE features...\')\nwith torch.no_grad():\n    fea_pase={}\n    for wi, snt_id in enumerate(fea.keys()):\n        #pase.eval()\n        fea_pase[snt_id]=pase(fea[snt_id].to(device), device=device,mode=\'avg_norm\').to(\'cpu\').detach()\n        fea_pase[snt_id]=fea_pase[snt_id].view(fea_pase[snt_id].shape[1],fea_pase[snt_id].shape[2]).transpose(0,1)\n        print(\'Processed training utterance {}/{} features\'.format(wi + 1,\n                                                                   len(fea.keys())))\n\ninp_dim=fea_pase[snt_id].shape[1]*(left+right+1)\n\nwith torch.no_grad():\n    # Computing pase features for test\n    fea_pase_dev={}\n    for wi, snt_id in enumerate(fea_dev.keys()):\n        fea_pase_dev[snt_id]=pase(fea_dev[snt_id].to(device), device=device, mode=\'avg_norm\').to(\'cpu\').detach()\n        fea_pase_dev[snt_id]=fea_pase_dev[snt_id].view(fea_pase_dev[snt_id].shape[1],fea_pase_dev[snt_id].shape[2]).transpose(0,1)\n        print(\'Processed test utterance {}/{} features\'.format(wi + 1,\n                                                               len(fea_dev.keys())))\n\n  \n# Label file reading\nwith open(lab_file, \'rb\') as handle:\n    lab = pickle.load(handle)\n\nwith open(lab_file_dev, \'rb\') as handle:\n    lab_dev = pickle.load(handle)\n    \n\n# Network initialization\nnnet=MLP(options,inp_dim)\n\nnnet.to(device)\n\ncost=nn.NLLLoss()\n\n# Optimizer initialization\noptimizer = optim.SGD(nnet.parameters(), lr=lr, momentum=0.0)\n\n# Seeds initialization\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# Batch creation (train)\nfea_lst=[]\nlab_lst=[]\n\nprint(""Data Preparation..."")\nfor snt in fea_pase.keys():\n    if fea_pase[snt].shape[0]-lab[snt].shape[0]!=2:\n        if fea_pase[snt].shape[0]-lab[snt].shape[0]==3:\n            fea_lst.append(fea_pase[snt][:-3])\n            lab_lst.append(lab[snt])\n        elif fea_pase[snt].shape[0]-lab[snt].shape[0]==1:\n            fea_lst.append(fea_pase[snt][:-1])\n            lab_lst.append(lab[snt])\n        else:\n            print(\'length error\')\n            sys.exit(0)\n    else:\n        fea_lst.append(fea_pase[snt][:-2])\n        lab_lst.append(lab[snt])\n\n# batch creation (dev)\nfea_lst_dev=[]\nlab_lst_dev=[]\nfor snt in fea_pase_dev.keys():\n    if fea_pase_dev[snt].shape[0]-lab_dev[snt].shape[0]!=2:\n        if fea_pase_dev[snt].shape[0]-lab_dev[snt].shape[0]==3:\n            fea_lst_dev.append(fea_pase_dev[snt][:-3])\n            lab_lst_dev.append(lab_dev[snt])\n        elif fea_pase_dev[snt].shape[0]-lab_dev[snt].shape[0]==1:\n            fea_lst_dev.append(fea_pase_dev[snt][:-1])\n            lab_lst_dev.append(lab_dev[snt])\n        else:\n            print(\'length error\')\n            sys.exit(0)\n    else:\n\n        fea_lst_dev.append(fea_pase_dev[snt][:-2])\n        lab_lst_dev.append(lab_dev[snt])\n    \n    \n\n# feature matrix (training)\nfea_conc=np.concatenate(fea_lst)\nfea_conc=context_window(fea_conc,left,right)\n\n# feature matrix (dev)\nfea_conc_dev=np.concatenate(fea_lst_dev)\nfea_conc_dev=context_window(fea_conc_dev,left,right)\n\n# feature normalization\nfea_conc=(fea_conc-np.mean(fea_conc,axis=0))/np.std(fea_conc,axis=0)\nfea_conc_dev=(fea_conc_dev-np.mean(fea_conc_dev,axis=0))/np.std(fea_conc_dev,axis=0)\n\n\n# lab matrix\nlab_conc=np.concatenate(lab_lst)\nlab_conc_dev=np.concatenate(lab_lst_dev)\n\nif right>0:\n    lab_conc=lab_conc[left:-right]\n    lab_conc_dev=lab_conc_dev[left:-right]\nelse:\n    lab_conc=lab_conc[left:]\n    lab_conc_dev=lab_conc_dev[left:]\n\n# lab normalization\nlab_conc=lab_conc-lab_conc.min()\nlab_conc_dev=lab_conc_dev-lab_conc_dev.min()\n\n\n# dataset composition\ndataset=np.concatenate([fea_conc,lab_conc.reshape(-1,1)],axis=1)\ndataset_dev=np.concatenate([fea_conc_dev,lab_conc_dev.reshape(-1,1)],axis=1)\n\n# shuffling\nnp.random.shuffle(dataset)\n\n# converting to pytorch\n#dataset=torch.from_numpy(dataset).float().to(device)\ndataset=torch.from_numpy(dataset).float()\n#dataset_dev=torch.from_numpy(dataset_dev).float().to(device)\ndataset_dev=torch.from_numpy(dataset_dev).float()\n\n\n# computing N_batches\nN_ex_tr=dataset.shape[0]\nN_batches=int(N_ex_tr/batch_size)\n\nN_ex_dev=dataset_dev.shape[0]\nN_batches_dev=int(N_ex_dev/batch_size)\n\nerr_batch_history=[]\n\n# Training loop\nprint(""Training..."")\nfor ep in range(N_epochs):\n    err_batches=0\n    loss_batches=0\n    \n    beg_batch=0\n    \n    # training modality\n    nnet.train()\n    \n    # random shuffling\n    shuffle_index=torch.randperm(dataset.shape[0])\n    dataset=dataset[shuffle_index]\n    \n    for batch_id in range(N_batches):\n        \n        # Batch selection\n        end_batch=beg_batch+batch_size\n        batch=dataset[beg_batch:end_batch]\n        batch=batch.to(device)\n        \n        fea_batch=batch[:,:-1]\n        lab_batch=batch[:,-1].long()\n        \n        # computing the output probabilities\n        out=nnet(fea_batch)\n           \n        # computing the loss\n        loss=cost(out,lab_batch)\n        \n        # computing the error\n        pred=torch.max(out,dim=1)[1] \n        err = torch.mean((pred!=lab_batch).float())\n\n        # loss/error accumulation        \n        err_batches=err_batches+err.detach()\n        loss_batches=loss_batches+loss.detach()\n    \n        \n        optimizer.zero_grad()\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        beg_batch=end_batch\n        \n\n        \n    # evaluation\n    nnet.eval()\n    beg_batch=0\n    \n    err_batches_dev=0\n    loss_batches_dev=0\n    \n    with torch.no_grad():\n        for batch_id in range(N_batches_dev):\n            \n            end_batch=beg_batch+batch_size\n            \n            batch_dev=dataset_dev[beg_batch:end_batch]\n            batch_dev=batch_dev.to(device)\n            \n            fea_batch_dev=batch_dev[:,:-1]\n            lab_batch_dev=batch_dev[:,-1].long()\n            \n            out=nnet(fea_batch_dev)\n            \n            loss=cost(out,lab_batch_dev)\n            \n            pred=torch.max(out,dim=1)[1] \n            err = torch.mean((pred!=lab_batch_dev).float())\n            \n            err_batches_dev=err_batches_dev+err.detach()\n            loss_batches_dev=loss_batches_dev+loss.detach()\n            \n            beg_batch=end_batch\n        \n    \n    err_batch_history.append(err_batches_dev/N_batches_dev)\n    \n    \n    print(""epoch=%i loss_tr=%f err_tr=%f loss_te=%f err_te=%f lr=%f"" %(ep,loss_batches/N_batches,err_batches/N_batches,loss_batches_dev/N_batches_dev,err_batches_dev/N_batches_dev,lr))\n    text_file.write(""epoch=%i loss_tr=%f err_tr=%f loss_te=%f err_te=%f lr=%f\\n"" %(ep,loss_batches/N_batches,err_batches/N_batches,loss_batches_dev/N_batches_dev,err_batches_dev/N_batches_dev,lr))\n\n    # learning rate annealing\n    if ep>0:\n        if (err_batch_history[-2]-err_batch_history[-1])/err_batch_history[-2]<0.0025:\n            lr=lr*halving_factor\n            optimizer.param_groups[0][\'lr\']=lr\n\n\nprint(\'BEST ERR=%f\' %(min(err_batch_history)))\nprint(\'BEST ACC=%f\' %(1-min(err_batch_history)))\ntext_file.write(\'BEST_ERR=%f\\n\' %(min(err_batch_history)))\ntext_file.write(\'BEST_ACC=%f\\n\' %(1-min(err_batch_history)))\ntext_file.close()\n    \n    \n    \n    \n\n\n\n'"
ASR/run_TIMIT_full_decoding.py,35,"b'#\n# To run a TIMIT experiment, go to the ASR folder and execute the following command:\n#\n# python run_TIMIT_full_decoding.py ../cfg/frontend/PASE+.cfg ../FE_e199.ckpt $SLURM_TMPDIR/TIMIT/ TIMIT_asr_exp cfg/MLP_PASE.cfg cfg/decoder.cfg \n#\n\n\n# Importing libraries\nimport os\nimport sys\nfrom neural_networks import MLP, context_window\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport pickle\nfrom pase.models.frontend import wf_builder\nimport soundfile as sf\nimport json\n\nfrom data_io import write_mat, open_or_fd\nfrom utils import run_shell\n\nimport warnings\nwarnings.filterwarnings(""ignore"")\n\ndef get_freer_gpu(trials=10):\n    for j in range(trials):\n        os.system(\'nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp\')\n        memory_available = [int(x.split()[2])\n                            for x in open(\'tmp\', \'r\').readlines()]\n        dev_ = torch.device(\'cuda:\'+str(np.argmax(memory_available)))\n        try:\n            a = torch.rand(1).cuda(dev_)\n            return dev_\n        except:\n            pass\n            print(\'NO GPU AVAILABLE!!!\')\n            exit(1)\n\n# Reading inputs\npase_cfg = sys.argv[1]  # e.g, \'../cfg/frontend/PASE+.cfg\'\npase_model = sys.argv[2]  # e.g, \'../FE_e199.ckp\' (download the pre-trained PASE+ model as described in the doc)\ndata_folder = sys.argv[3]  # e.g., \'/home/mirco/Dataset/TIMIT\'\noutput_folder = sys.argv[4]  # e.g., \'TIMIT_asr_exp\'\ncfg_file = sys.argv[5]  # e.g, cfg/MLP_pase.cfg\ncfg_dec = sys.argv[6]  # e.g., cfg/decoder.cfg\n\nskip_training = False\n\n# using absolute path for output folder\noutput_folder=os.path.abspath(output_folder)\n\ncount_file = output_folder+\'/count.npy\'\nASR_model_file = output_folder+\'/model.pkl\'  # e.g., TIMIT_matconv_512/model.pkl\n\nif not os.path.exists(output_folder):\n    os.mkdir(output_folder)\n\nif not(skip_training):\n    output_file = output_folder+\'/res.res\'\n    output_model = output_folder+\'/model.pkl\'\n\n    # Label files for TIMIT\n    lab_file = \'TIMIT_lab_cd.pkl\'\n    lab_file_dev = \'TIMIT_lab_cd_dev.pkl\'\n\n    # File list for TIMIT\n    tr_lst_file = \'timit_tr_kaldi.lst\'\n    dev_lst_file = \'timit_dev_kaldi.lst\'\n\n    tr_lst = [line.rstrip(\'\\n\') for line in open(tr_lst_file)]\n    dev_lst = [line.rstrip(\'\\n\') for line in open(dev_lst_file)]\n\n    # Training parameters\n\n    with open(cfg_file, ""r"") as read_file:\n        cfg = json.load(read_file)\n\n    # Parameters\n    N_epochs = int(cfg[\'N_epochs\'])\n    seed = int(cfg[\'seed\'])\n    batch_size = int(cfg[\'batch_size\'])\n    halving_factor = float(cfg[\'halving_factor\'])\n    lr = float(cfg[\'lr\'])\n    left = int(cfg[\'left\']) # Reduce this to minimize memory (but it has an effect on performance too)\n    right = int(cfg[\'right\']) # Reduce this to minimize memory (but it has an effect on performance too)\n    avg_spk = bool(cfg[\'avg_spk\'])\n    dnn_lay = cfg[\'dnn_lay\']\n    dnn_drop = cfg[\'dnn_drop\']\n    dnn_use_batchnorm = cfg[\'dnn_use_batchnorm\']\n    dnn_use_laynorm = cfg[\'dnn_use_laynorm\']\n    dnn_use_laynorm_inp = cfg[\'dnn_use_laynorm_inp\']\n    dnn_use_batchnorm_inp = cfg[\'dnn_use_batchnorm_inp\']\n    dnn_act = cfg[\'dnn_act\']\n\n    options = {}\n    options[\'dnn_lay\'] = dnn_lay\n    options[\'dnn_drop\'] = dnn_drop\n    options[\'dnn_use_batchnorm\'] = dnn_use_batchnorm\n    options[\'dnn_use_laynorm\'] = dnn_use_laynorm\n    options[\'dnn_use_laynorm_inp\'] = dnn_use_laynorm_inp\n    options[\'dnn_use_batchnorm_inp\'] = dnn_use_batchnorm_inp\n    options[\'dnn_act\'] = dnn_act\n\n    device = get_freer_gpu()\n\n    # folder creation\n    text_file = open(output_file, ""w"")\n\n    # Loading pase\n    pase = wf_builder(pase_cfg)\n    pase.load_pretrained(pase_model, load_last=True, verbose=False)\n    pase.to(device)\n    pase.eval()\n\n    # reading the training signals\n    print(""Waveform reading..."")\n    fea = {}\n    for wav_file in tr_lst:\n        [signal, fs] = sf.read(data_folder+\'/\'+wav_file)\n        signal = signal/np.max(np.abs(signal))\n        signal = signal.astype(np.float32)\n\n        fea_id = wav_file.split(\'/\')[-2]+\'_\' + \\\n            wav_file.split(\'/\')[-1].split(\'.\')[0]\n        fea[fea_id] = torch.from_numpy(\n            signal).float().to(device).view(1, 1, -1)\n\n    # reading the dev signals\n    fea_dev = {}\n    for wav_file in dev_lst:\n        [signal, fs] = sf.read(data_folder+\'/\'+wav_file)\n        signal = signal/np.max(np.abs(signal))\n        fea_id = wav_file.split(\'/\')[-2]+\'_\' + \\\n            wav_file.split(\'/\')[-1].split(\'.\')[0]\n        fea_dev[fea_id] = torch.from_numpy(\n            signal).float().to(device).view(1, 1, -1)\n\n    # Computing pase features for training\n    print(\'Computing PASE features...\')\n    fea_pase = {}\n    mean_spk = {}\n    std_spk = {}\n    for snt_id in fea.keys():\n        pase.eval()\n\n        if avg_spk:\n            fea_pase[snt_id] = pase(fea[snt_id], device).to(\'cpu\').detach()\n        else:\n            fea_pase[snt_id] = pase(\n                fea[snt_id], device, mode=\'avg_norm\').to(\'cpu\').detach()\n        fea_pase[snt_id] = fea_pase[snt_id].view(\n            fea_pase[snt_id].shape[1], fea_pase[snt_id].shape[2]).transpose(0, 1)\n        spk_id = snt_id.split(\'_\')[0]\n        if spk_id not in mean_spk:\n            mean_spk[spk_id] = []\n            std_spk[spk_id] = []\n        mean_spk[spk_id].append(torch.mean(fea_pase[snt_id], dim=0))\n        std_spk[spk_id].append(torch.std(fea_pase[snt_id], dim=0))\n\n    # compute per-speaker mean and variance\n    if avg_spk:\n        for spk_id in mean_spk.keys():\n            mean_spk[spk_id] = torch.mean(torch.stack(mean_spk[spk_id]), dim=0)\n            std_spk[spk_id] = torch.mean(torch.stack(std_spk[spk_id]), dim=0)\n\n        # apply speaker normalization\n        for snt_id in fea.keys():\n            spk_id = snt_id.split(\'_\')[0]\n            # /std_spk[spk_id]\n            fea_pase[snt_id] = (fea_pase[snt_id]-mean_spk[spk_id])\n\n    inp_dim = fea_pase[snt_id].shape[1]*(left+right+1)\n\n    # Computing pase features for test\n    fea_pase_dev = {}\n    mean_spk_dev = {}\n    std_spk_dev = {}\n\n    for snt_id in fea_dev.keys():\n\n        if avg_spk:\n            fea_pase_dev[snt_id] = pase(\n                fea_dev[snt_id], device).to(\'cpu\').detach()\n        else:\n            fea_pase_dev[snt_id] = pase(\n                fea_dev[snt_id], device, mode=\'avg_norm\').to(\'cpu\').detach()\n\n        fea_pase_dev[snt_id] = fea_pase_dev[snt_id].view(\n            fea_pase_dev[snt_id].shape[1], fea_pase_dev[snt_id].shape[2]).transpose(0, 1)\n        spk_id = snt_id.split(\'_\')[0]\n        if spk_id not in mean_spk_dev:\n            mean_spk_dev[spk_id] = []\n            std_spk_dev[spk_id] = []\n        mean_spk_dev[spk_id].append(torch.mean(fea_pase_dev[snt_id], dim=0))\n        std_spk_dev[spk_id].append(torch.std(fea_pase_dev[snt_id], dim=0))\n\n    # compute per-speaker mean and variance\n    if avg_spk:\n        for spk_id in mean_spk_dev.keys():\n            mean_spk_dev[spk_id] = torch.mean(\n                torch.stack(mean_spk_dev[spk_id]), dim=0)\n            std_spk_dev[spk_id] = torch.mean(\n                torch.stack(std_spk_dev[spk_id]), dim=0)\n\n        # apply speaker normalization\n        for snt_id in fea_dev.keys():\n            spk_id = snt_id.split(\'_\')[0]\n            # /std_spk_dev[spk_id]\n            fea_pase_dev[snt_id] = (fea_pase_dev[snt_id]-mean_spk_dev[spk_id])\n\n    # Label file reading\n    with open(lab_file, \'rb\') as handle:\n        lab = pickle.load(handle)\n\n    with open(lab_file_dev, \'rb\') as handle:\n        lab_dev = pickle.load(handle)\n\n    # Network initialization\n    nnet = MLP(options, inp_dim)\n\n    nnet.to(device)\n\n    cost = nn.NLLLoss()\n\n    # Optimizer initialization\n    optimizer = optim.SGD(nnet.parameters(), lr=lr, momentum=0.0)\n\n    # Seeds initialization\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    # Batch creation (train)\n    fea_lst = []\n    lab_lst = []\n\n    print(""Data Preparation..."")\n    for snt in fea_pase.keys():\n        if fea_pase[snt].shape[0]-lab[snt].shape[0] != 2:\n            if fea_pase[snt].shape[0]-lab[snt].shape[0] == 3:\n                fea_lst.append(fea_pase[snt][:-3])\n                lab_lst.append(lab[snt])\n            elif fea_pase[snt].shape[0]-lab[snt].shape[0] == 1:\n                fea_lst.append(fea_pase[snt][:-1])\n                lab_lst.append(lab[snt])\n            else:\n                print(\'length error\')\n                sys.exit(0)\n        else:\n            fea_lst.append(fea_pase[snt][:-2])\n            lab_lst.append(lab[snt])\n\n    # batch creation (dev)\n    fea_lst_dev = []\n    lab_lst_dev = []\n    for snt in fea_pase_dev.keys():\n        if fea_pase_dev[snt].shape[0]-lab_dev[snt].shape[0] != 2:\n            if fea_pase_dev[snt].shape[0]-lab_dev[snt].shape[0] == 3:\n                fea_lst_dev.append(fea_pase_dev[snt][:-3])\n                lab_lst_dev.append(lab_dev[snt])\n            elif fea_pase_dev[snt].shape[0]-lab_dev[snt].shape[0] == 1:\n                fea_lst_dev.append(fea_pase_dev[snt][:-1])\n                lab_lst_dev.append(lab_dev[snt])\n            else:\n                print(\'length error\')\n                sys.exit(0)\n        else:\n\n            fea_lst_dev.append(fea_pase_dev[snt][:-2])\n            lab_lst_dev.append(lab_dev[snt])\n\n    # feature matrix (training)\n    fea_conc = np.concatenate(fea_lst)\n    fea_conc = context_window(fea_conc, left, right)\n\n    # feature matrix (dev)\n    fea_conc_dev = np.concatenate(fea_lst_dev)\n    fea_conc_dev = context_window(fea_conc_dev, left, right)\n\n    # lab matrix\n    lab_conc = np.concatenate(lab_lst)\n    lab_conc_dev = np.concatenate(lab_lst_dev)\n\n    if right > 0:\n        lab_conc = lab_conc[left:-right]\n        lab_conc_dev = lab_conc_dev[left:-right]\n    else:\n        lab_conc = lab_conc[left:]\n        lab_conc_dev = lab_conc_dev[left:]\n\n    # lab normalization\n    lab_conc = lab_conc-lab_conc.min()\n\n    # create count file (useful to scale posteriors before decoding)\n    unique, counts = np.unique(lab_conc, return_counts=True)\n    count_file = output_folder+\'/\'+\'count.npy\'\n    np.save(count_file, counts)\n\n    id_file = output_folder+\'/\'+\'ids.npy\'\n    np.save(id_file, unique)\n\n    lab_conc_dev = lab_conc_dev-lab_conc_dev.min()\n\n    # dataset composition\n    dataset = np.concatenate([fea_conc, lab_conc.reshape(-1, 1)], axis=1)\n    dataset_dev = np.concatenate(\n        [fea_conc_dev, lab_conc_dev.reshape(-1, 1)], axis=1)\n\n    dataset_dev = torch.from_numpy(dataset_dev).float()\n\n    # computing N_batches\n    N_ex_tr = dataset.shape[0]\n    N_batches = int(N_ex_tr/batch_size)\n\n    N_ex_dev = dataset_dev.shape[0]\n    N_batches_dev = int(N_ex_dev/batch_size)\n\n    err_batch_history = []\n\n    # Training loop\n    print(""Training..."")\n    for ep in range(N_epochs):\n        err_batches = 0\n        loss_batches = 0\n\n        beg_batch = 0\n\n        # training modality\n        nnet.train()\n\n        # random shuffling\n        np.random.shuffle(dataset)\n\n        for batch_id in range(N_batches):\n\n            # Batch selection\n            end_batch = beg_batch+batch_size\n            batch = torch.from_numpy(dataset[beg_batch:end_batch]).float()\n            batch = batch.to(device)\n\n            fea_batch = batch[:, :-1]\n            lab_batch = batch[:, -1].long()\n\n            # computing the output probabilities\n            out = nnet(fea_batch)\n\n            # computing the loss\n            loss = cost(out, lab_batch)\n\n            # computing the error\n            pred = torch.max(out, dim=1)[1]\n            err = torch.mean((pred != lab_batch).float())\n\n            # loss/error accumulation\n            err_batches = err_batches+err.detach()\n            loss_batches = loss_batches+loss.detach()\n\n            optimizer.zero_grad()\n\n            loss.backward()\n\n            optimizer.step()\n\n            beg_batch = end_batch\n\n        # evaluation\n        nnet.eval()\n        beg_batch = 0\n\n        err_batches_dev = 0\n        loss_batches_dev = 0\n\n        with torch.no_grad():\n            for batch_id in range(N_batches_dev):\n\n                end_batch = beg_batch+batch_size\n\n                batch_dev = dataset_dev[beg_batch:end_batch]\n                batch_dev = batch_dev.to(device)\n\n                fea_batch_dev = batch_dev[:, :-1]\n                lab_batch_dev = batch_dev[:, -1].long()\n\n                out = nnet(fea_batch_dev)\n\n                loss = cost(out, lab_batch_dev)\n\n                pred = torch.max(out, dim=1)[1]\n                err = torch.mean((pred != lab_batch_dev).float())\n\n                err_batches_dev = err_batches_dev+err.detach()\n                loss_batches_dev = loss_batches_dev+loss.detach()\n\n                beg_batch = end_batch\n\n        err_batch_history.append(err_batches_dev/N_batches_dev)\n\n        print(""epoch=%i loss_tr=%f err_tr=%f loss_te=%f err_te=%f lr=%f"" % (ep, loss_batches/N_batches,\n            err_batches/N_batches, loss_batches_dev/N_batches_dev, err_batches_dev/N_batches_dev, lr))\n        \n        text_file = open(output_file, ""a+"")\n        text_file.write(""epoch=%i loss_tr=%f err_tr=%f loss_te=%f err_te=%f lr=%f\\n"" % (\n            ep, loss_batches/N_batches, err_batches/N_batches, loss_batches_dev/N_batches_dev, err_batches_dev/N_batches_dev, lr))\n        text_file.close()\n\n        # learning rate annealing\n        if ep > 0:\n\n            # save the model if it is the best (according to err dev)\n            if min(err_batch_history) == err_batch_history[-1]:\n                torch.save(nnet.state_dict(), output_model)\n\n            if (err_batch_history[-2]-err_batch_history[-1])/err_batch_history[-2] < 0.0025:\n                lr = lr*halving_factor\n                optimizer.param_groups[0][\'lr\'] = lr\n\n    print(\'BEST ERR=%f\' % (min(err_batch_history)))\n    print(\'BEST ACC=%f\' % (1-min(err_batch_history)))\n    text_file = open(output_file, ""a+"")\n    text_file.write(\'BEST_ERR=%f\\n\' % (min(err_batch_history)))\n    text_file.write(\'BEST_ACC=%f\\n\' % (1-min(err_batch_history)))\n    text_file.close()\n\n\nark_file = output_folder+\'/post.ark\'\n\ndevice = \'cuda\'\n\ndecoding_only = False\n\nif not decoding_only:\n    counts = np.load(count_file)\n    log_counts = np.log(counts/np.sum(counts))\n\n    if not os.path.exists(output_folder):\n        os.mkdir(output_folder)\n\n    output_file = output_folder+\'/res.res\'\n\n    dev_lst_file = \'timit_te.lst\'\n    dev_lst = [line.rstrip(\'\\n\') for line in open(dev_lst_file)]\n\n    # Training parameters\n    with open(cfg_file, ""r"") as read_file:\n        cfg = json.load(read_file)\n\n    with open(cfg_dec, ""r"") as read_file:\n        cfg_dec = json.load(read_file)\n\n    with open(output_folder+\'/dec_cfg.ini\', \'w\') as file:\n        file.write(\'[decoding]\\n\')\n        for key in cfg_dec.keys():\n            file.write(\'%s=%s\\n\' % (key, cfg_dec[key]))\n\n    # Parameters\n    N_epochs = int(cfg[\'N_epochs\'])\n    seed = int(cfg[\'seed\'])\n    batch_size = int(cfg[\'batch_size\'])\n    halving_factor = float(cfg[\'halving_factor\'])\n    lr = float(cfg[\'lr\'])\n    left = int(cfg[\'left\'])\n    right = int(cfg[\'right\'])\n    avg_spk = bool(cfg[\'avg_spk\'])\n    dnn_lay = cfg[\'dnn_lay\']\n    dnn_drop = cfg[\'dnn_drop\']\n    dnn_use_batchnorm = cfg[\'dnn_use_batchnorm\']\n    dnn_use_laynorm = cfg[\'dnn_use_laynorm\']\n    dnn_use_laynorm_inp = cfg[\'dnn_use_laynorm_inp\']\n    dnn_use_batchnorm_inp = cfg[\'dnn_use_batchnorm_inp\']\n    dnn_act = cfg[\'dnn_act\']\n\n    options = {}\n    options[\'dnn_lay\'] = dnn_lay\n    options[\'dnn_drop\'] = dnn_drop\n    options[\'dnn_use_batchnorm\'] = dnn_use_batchnorm\n    options[\'dnn_use_laynorm\'] = dnn_use_laynorm\n    options[\'dnn_use_laynorm_inp\'] = dnn_use_laynorm_inp\n    options[\'dnn_use_batchnorm_inp\'] = dnn_use_batchnorm_inp\n    options[\'dnn_act\'] = dnn_act\n\n    # folder creation\n    #text_file = open(output_file, ""w"")\n\n    # Loading pase\n    pase = wf_builder(pase_cfg)\n    pase.load_pretrained(pase_model, load_last=True, verbose=False)\n    pase.to(device)\n    pase.eval()\n\n    # reading the training signals\n    print(""Waveform reading..."")\n\n    # reading the dev signals\n    fea_dev = {}\n    for wav_file in dev_lst:\n        [signal, fs] = sf.read(data_folder+\'/\'+wav_file)\n        signal = signal/np.max(np.abs(signal))\n        fea_id = wav_file.split(\'/\')[-2]+\'_\' + \\\n            wav_file.split(\'/\')[-1].split(\'.\')[0]\n        fea_dev[fea_id] = torch.from_numpy(\n            signal).float().to(device).view(1, 1, -1)\n\n    # Computing pase features for training\n    print(\'Computing PASE features...\')\n\n    # Computing pase features for test\n    fea_pase_dev = {}\n    mean_spk_dev = {}\n    std_spk_dev = {}\n\n    for snt_id in fea_dev.keys():\n\n        if avg_spk:\n            fea_pase_dev[snt_id] = pase(\n                fea_dev[snt_id], device).to(\'cpu\').detach()\n        else:\n            fea_pase_dev[snt_id] = pase(\n                fea_dev[snt_id], device, mode=\'avg_norm\').to(\'cpu\').detach()\n\n        fea_pase_dev[snt_id] = fea_pase_dev[snt_id].view(\n            fea_pase_dev[snt_id].shape[1], fea_pase_dev[snt_id].shape[2]).transpose(0, 1)\n        spk_id = snt_id.split(\'_\')[0]\n        if spk_id not in mean_spk_dev:\n            mean_spk_dev[spk_id] = []\n            std_spk_dev[spk_id] = []\n        mean_spk_dev[spk_id].append(torch.mean(fea_pase_dev[snt_id], dim=0))\n        std_spk_dev[spk_id].append(torch.std(fea_pase_dev[snt_id], dim=0))\n\n    # compute per-speaker mean and variance\n    if avg_spk:\n        for spk_id in mean_spk_dev.keys():\n            mean_spk_dev[spk_id] = torch.mean(\n                torch.stack(mean_spk_dev[spk_id]), dim=0)\n            std_spk_dev[spk_id] = torch.mean(\n                torch.stack(std_spk_dev[spk_id]), dim=0)\n\n        # apply speaker normalization\n        for snt_id in fea_dev.keys():\n            spk_id = snt_id.split(\'_\')[0]\n            # /std_spk_dev[spk_id]\n            fea_pase_dev[snt_id] = (fea_pase_dev[snt_id]-mean_spk_dev[spk_id])\n            fea_pase_dev[snt_id] = context_window(\n                fea_pase_dev[snt_id], left, right)\n\n    # Network initialization\n    inp_dim = fea_pase_dev[snt_id].shape[1]\n    nnet = MLP(options, inp_dim)\n    nnet.to(device)\n\n    nnet.load_state_dict(torch.load(ASR_model_file))\n    nnet.eval()\n\n    post_file = open_or_fd(ark_file, output_folder, \'wb\')\n\n    for snt_id in fea_dev.keys():\n        pout = nnet(torch.from_numpy(fea_pase_dev[snt_id]).to(device).float())\n        pout = pout-torch.tensor(log_counts).float().to(device)\n        write_mat(output_folder, post_file, pout.data.cpu().numpy(), snt_id)\n\n# doing decoding\nprint(\'Decoding...\')\ncmd_decode = cfg_dec[\'decoding_script_folder\'] + \'/\' + cfg_dec[\'decoding_script\'] + \' \' + \\\n    os.path.abspath(output_folder+\'/dec_cfg.ini\')+\' \' + \\\n    output_folder+\'/dec\' + \' \\""\' + ark_file + \'\\""\'\nprint(cmd_decode)\nrun_shell(cmd_decode)\n'"
ASR/run_minichime5_fast.py,17,"b'# Mirco Ravanelli\n# Mila, June 2019\n\n# This script runs a simple speech recognition experiment on the top of PASE features. \n# The results are reported in terms of Frame Error Rate over phonemes (context-independent). \n# This system is not designed for an extensive evaluation of PASE features, but mainly for quickly monitoring the performance of PASE during the self-supervised training phase.\n# The results are printed in standard output and within the text file specified in the last argument.\n\n# The current version uses 5 hours of binaural recordings for training and 1 hour recordings for evaluation. To use it, you have to download the data from:\n\n# You also need to pull the repo and run the following script (change the paths according to your needs):\n# python run_minichime5_fast.py ../cfg/PASE_MAT_sinc_jiany_512.cfg #../exp_pase_aspp_stride8_512/FE_e89.ckpt /scratch/ravanelm/datasets/MiniCHiME5_5h_ct #/scratch/ravanelm/datasets/MiniCHiME5_5h_ct/lists/snt2ali_tr5h.pkl #/scratch/ravanelm/datasets/MiniCHiME5_5h_ct/lists/snt2ali_dev1h.pkl  #/scratch/ravanelm/datasets/MiniCHiME5_5h_ct/lists/list_tr_shuffled_sel5h.txt  #/scratch/ravanelm/datasets/MiniCHiME5_5h_ct/lists/list_dev_shuffled_sel1h.txt res.res\n\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\nimport librosa\n\nimport os\nimport sys\nfrom neural_networks import MLP,context_window\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport pickle\nfrom pase.models.frontend import wf_builder\n# from waveminionet.models.frontend import wf_builder #old models\nimport soundfile as sf\nimport os\nimport json\n# import pase.models as models\n# import models.WorkerScheduler\nfrom pase.models.WorkerScheduler.encoder import *\n\ndef get_freer_gpu(trials=10):\n    for j in range(trials):\n         os.system(\'nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp\')\n         memory_available = [int(x.split()[2]) for x in open(\'tmp\', \'r\').readlines()]\n         dev_ = torch.device(\'cuda:\'+str(np.argmax(memory_available)))\n         try:\n            a = torch.rand(1).cuda(dev_)\n            return dev_\n         except: \n            pass\n            print(\'NO GPU AVAILABLE!!!\')\n            exit(1)\n\n\npase_cfg=sys.argv[1]\npase_model=sys.argv[2]\ndata_folder=sys.argv[3]\n\n# Label files for MiniCHiME5\nlab_file=sys.argv[4]\nlab_file_dev=sys.argv[5]\n\n# File list for MiniCHiME5\ntr_lst_file=dev=sys.argv[6]\ndev_lst_file=sys.argv[7]\n\noutput_file=sys.argv[8]\n\n\n\ntr_lst = [line.rstrip(\'\\n\') for line in open(tr_lst_file)]\ndev_lst = [line.rstrip(\'\\n\') for line in open(dev_lst_file)]\n\n# Training parameters\nN_epochs=24\nseed=1234\nbatch_size=128\nhalving_factor=0.5\nlr=0.0012\nleft=1\nright=1\n\n# Neural network parameters\noptions={}\noptions[\'dnn_lay\']=\'1024,42\'\noptions[\'dnn_drop\']=\'0.15,0.0\'\noptions[\'dnn_use_batchnorm\']=\'False,False\'\noptions[\'dnn_use_laynorm\']=\'True,False\'\noptions[\'dnn_use_laynorm_inp\']=\'True\'\noptions[\'dnn_use_batchnorm_inp\']=\'False\'\noptions[\'dnn_act\']=\'relu,softmax\'\n\ndevice=0 #get_freer_gpu()\n\n\n# folder creation\ntext_file=open(output_file, ""w"")\n\n# Loading pase\npase =wf_builder(pase_cfg)\npase.load_pretrained(pase_model, load_last=True, verbose=False)\npase.to(device)\npase.eval()\n\n# reading the training signals\nprint(""Waveform reading..."")\nfea={}\nfor wav_file in tr_lst:\n    [signal, fs] = sf.read(data_folder+\'/\'+wav_file)\n    signal=signal/np.max(np.abs(signal))\n    signal = signal.astype(np.float32)\n    \n    fea_id=wav_file.split(\'/\')[-1].replace(\'.wav\',\'\')\n    \n    fea[fea_id]=torch.from_numpy(signal).float().to(device).view(1,1,-1)\n\n\n# reading the dev signals\nfea_dev={}\nfor wav_file in dev_lst:\n    [signal, fs] = sf.read(data_folder+\'/\'+wav_file)\n    signal=signal/np.max(np.abs(signal))\n    fea_id=wav_file.split(\'/\')[-1].replace(\'.wav\',\'\')\n    fea_dev[fea_id]=torch.from_numpy(signal).float().to(device).view(1,1,-1)\n\n# Computing pase features for training\nprint(\'Computing PASE features...\')\nfea_pase={}\nfor snt_id in fea.keys():\n    pase.eval()\n    fea_pase[snt_id]=pase(fea[snt_id], device).to(\'cpu\').detach()\n    fea_pase[snt_id]=fea_pase[snt_id].view(fea_pase[snt_id].shape[1],fea_pase[snt_id].shape[2]).transpose(0,1)\n\ninp_dim=fea_pase[snt_id].shape[1]*(left+right+1)\n\n# Computing pase features for test\nfea_pase_dev={}\nfor snt_id in fea_dev.keys():\n    fea_pase_dev[snt_id]=pase(fea_dev[snt_id], device).to(\'cpu\').detach()\n    fea_pase_dev[snt_id]=fea_pase_dev[snt_id].view(fea_pase_dev[snt_id].shape[1],fea_pase_dev[snt_id].shape[2]).transpose(0,1)\n\n  \n# Label file reading\nwith open(lab_file, \'rb\') as handle:\n    lab = pickle.load(handle)\n\nwith open(lab_file_dev, \'rb\') as handle:\n    lab_dev = pickle.load(handle)\n    \n\n# Network initialization\nnnet=MLP(options,inp_dim)\n\nnnet.to(device)\n\ncost=nn.NLLLoss()\n\n# Optimizer initialization\noptimizer = optim.SGD(nnet.parameters(), lr=lr, momentum=0.0)\n\n# Seeds initialization\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# Batch creation (train)\nfea_lst=[]\nlab_lst=[]\n\nprint(""Data Preparation..."")\nfor snt in fea_pase.keys():\n    #print(fea_pase[snt].shape)\n    #print(lab[snt].shape)\n\n    if fea_pase[snt].shape[0]-lab[snt].shape[0]!=2:\n        if fea_pase[snt].shape[0]-lab[snt].shape[0]==3:\n            fea_lst.append(fea_pase[snt][:-3])\n            lab_lst.append(lab[snt])\n        elif fea_pase[snt].shape[0]-lab[snt].shape[0]==1:\n            fea_lst.append(fea_pase[snt][:-1])\n            lab_lst.append(lab[snt])\n\n    else:\n        fea_lst.append(fea_pase[snt][:-2])\n        lab_lst.append(lab[snt])\n\n# batch creation (dev)\nfea_lst_dev=[]\nlab_lst_dev=[]\nfor snt in fea_pase_dev.keys():\n    if fea_pase_dev[snt].shape[0]-lab_dev[snt].shape[0]!=2:\n        if fea_pase_dev[snt].shape[0]-lab_dev[snt].shape[0]==3:\n            fea_lst_dev.append(fea_pase_dev[snt][:-3])\n            lab_lst_dev.append(lab_dev[snt])\n        elif fea_pase_dev[snt].shape[0]-lab_dev[snt].shape[0]==1:\n            fea_lst_dev.append(fea_pase_dev[snt][:-1])\n            lab_lst_dev.append(lab_dev[snt])       \n    else:\n        fea_lst_dev.append(fea_pase_dev[snt][:-2])\n        lab_lst_dev.append(lab_dev[snt])\n    \n\n\n# feature matrix (training)\nfea_conc=np.concatenate(fea_lst)\nfea_conc=context_window(fea_conc,left,right)\n\n# feature matrix (dev)\nfea_conc_dev=np.concatenate(fea_lst_dev)\nfea_conc_dev=context_window(fea_conc_dev,left,right)\n\n# feature normalization\nfea_conc=(fea_conc-np.mean(fea_conc,axis=0))/np.std(fea_conc,axis=0)\nfea_conc_dev=(fea_conc_dev-np.mean(fea_conc_dev,axis=0))/np.std(fea_conc_dev,axis=0)\n\n\n# lab matrix\nlab_conc=np.concatenate(lab_lst)\nlab_conc_dev=np.concatenate(lab_lst_dev)\n\nif right>0:\n    lab_conc=lab_conc[left:-right]\n    lab_conc_dev=lab_conc_dev[left:-right]\nelse:\n    lab_conc=lab_conc[left:]\n    lab_conc_dev=lab_conc_dev[left:]\n\n# lab normalization\nlab_conc=lab_conc-lab_conc.min()\nlab_conc_dev=lab_conc_dev-lab_conc_dev.min()\n\n\n# dataset composition\ndataset=np.concatenate([fea_conc,lab_conc.reshape(-1,1)],axis=1)\ndataset_dev=np.concatenate([fea_conc_dev,lab_conc_dev.reshape(-1,1)],axis=1)\n\n# shuffling\nnp.random.shuffle(dataset)\n\n# converting to pytorch\n#dataset=torch.from_numpy(dataset).float().to(device)\ndataset=torch.from_numpy(dataset).float()\n#dataset_dev=torch.from_numpy(dataset_dev).float().to(device)\ndataset_dev=torch.from_numpy(dataset_dev).float()\n\n\n# computing N_batches\nN_ex_tr=dataset.shape[0]\nN_batches=int(N_ex_tr/batch_size)\n\nN_ex_dev=dataset_dev.shape[0]\nN_batches_dev=int(N_ex_dev/batch_size)\n\nerr_batch_history=[]\n\n# Training loop\nprint(""Training..."")\nfor ep in range(N_epochs):\n    err_batches=0\n    loss_batches=0\n    \n    beg_batch=0\n    \n    # training modality\n    nnet.train()\n    \n    # random shuffling\n    shuffle_index=torch.randperm(dataset.shape[0])\n    dataset=dataset[shuffle_index]\n    \n    for batch_id in range(N_batches):\n        \n        # Batch selection\n        end_batch=beg_batch+batch_size\n        batch=dataset[beg_batch:end_batch]\n        batch=batch.to(device)\n        \n        fea_batch=batch[:,:-1]\n        lab_batch=batch[:,-1].long()\n        \n        # computing the output probabilities\n        out=nnet(fea_batch)\n           \n        # computing the loss\n        loss=cost(out,lab_batch)\n        \n        # computing the error\n        pred=torch.max(out,dim=1)[1] \n        err = torch.mean((pred!=lab_batch).float())\n\n        # loss/error accumulation        \n        err_batches=err_batches+err.detach()\n        loss_batches=loss_batches+loss.detach()\n    \n        \n        optimizer.zero_grad()\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        beg_batch=end_batch\n        \n\n        \n    # evaluation\n    nnet.eval()\n    beg_batch=0\n    \n    err_batches_dev=0\n    loss_batches_dev=0\n    \n    with torch.no_grad():\n        for batch_id in range(N_batches_dev):\n            \n            end_batch=beg_batch+batch_size\n            \n            batch_dev=dataset_dev[beg_batch:end_batch]\n            batch_dev=batch_dev.to(device)\n            \n            fea_batch_dev=batch_dev[:,:-1]\n            lab_batch_dev=batch_dev[:,-1].long()\n            \n            out=nnet(fea_batch_dev)\n            \n            loss=cost(out,lab_batch_dev)\n            \n            pred=torch.max(out,dim=1)[1] \n            err = torch.mean((pred!=lab_batch_dev).float())\n            \n            err_batches_dev=err_batches_dev+err.detach()\n            loss_batches_dev=loss_batches_dev+loss.detach()\n            \n            beg_batch=end_batch\n        \n    \n    err_batch_history.append(err_batches_dev/N_batches_dev)\n    \n    \n    print(""epoch=%i loss_tr=%f err_tr=%f loss_te=%f err_te=%f lr=%f"" %(ep,loss_batches/N_batches,err_batches/N_batches,loss_batches_dev/N_batches_dev,err_batches_dev/N_batches_dev,lr))\n    text_file.write(""epoch=%i loss_tr=%f err_tr=%f loss_te=%f err_te=%f lr=%f\\n"" %(ep,loss_batches/N_batches,err_batches/N_batches,loss_batches_dev/N_batches_dev,err_batches_dev/N_batches_dev,lr))\n\n    # learning rate annealing\n    if ep>0:\n        if (err_batch_history[-2]-err_batch_history[-1])/err_batch_history[-2]<0.0025:\n            lr=lr*halving_factor\n            optimizer.param_groups[0][\'lr\']=lr\n\n\nprint(\'BEST ERR=%f\' %(min(err_batch_history)))\nprint(\'BEST ACC=%f\' %(1-min(err_batch_history)))\ntext_file.write(\'BEST_ERR=%f\\n\' %(min(err_batch_history)))\ntext_file.write(\'BEST_ACC=%f\\n\' %(1-min(err_batch_history)))\ntext_file.close()\n    \n    \n    \n    \n\n\n\n'"
ASR/utils.py,10,"b'##########################################################\n# pytorch-kaldi v.0.1                                      \n# Mirco Ravanelli, Titouan Parcollet\n# Mila, University of Montreal\n# October 2018\n##########################################################\n\nimport configparser\nimport sys\nimport os.path\nimport random\nimport subprocess\nimport numpy as np\nimport re\nimport glob\nfrom distutils.util import strtobool\nimport importlib\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\n\n\n\ndef run_command(cmd):\n    """"""from http://blog.kagesenshi.org/2008/02/teeing-python-subprocesspopen-output.html\n    """"""\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    stdout = []\n    while True:\n        line = p.stdout.readline()\n        stdout.append(line)\n        print(line.decode(""utf-8""))\n        if line == \'\' and p.poll() != None:\n            break\n    return \'\'.join(stdout)\n\ndef run_shell_display(cmd):\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,shell=True)\n    while True:\n        out = p.stdout.read(1).decode(\'utf-8\')\n        if out == \'\' and p.poll() != None:\n            break\n        if out != \'\':\n            sys.stdout.write(out)\n            sys.stdout.flush()\n    return\n      \ndef run_shell(cmd):\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,shell=True)\n    \n    (output, err) = p.communicate()\n    p.wait()\n  \n    return output\n\n\ndef read_args_command_line(args,config):\n    \n    sections=[]\n    fields=[]\n    values=[]\n\n    for i in range(2,len(args)):\n\n        # check if the option is valid for second level\n        r2=re.compile(\'--.*,.*=.*\')\n\n        # check if the option is valid for 4 level\n        r4=re.compile(\'--.*,.*,.*,.*="".*""\')\n        if r2.match(args[i]) is None and r4.match(args[i]) is None:\n            sys.stderr.write(\'ERROR: option \\""%s\\"" from command line is not valid! (the format must be \\""--section,field=value\\"")\\n\' %(args[i]))\n            sys.exit(0)\n        \n        sections.append(re.search(\'--(.*),\', args[i]).group(1))\n        fields.append(re.search(\',(.*)\', args[i].split(\'=\')[0]).group(1))\n        values.append(re.search(\'=(.*)\', args[i]).group(1))\n\n    # parsing command line arguments\n    for i in range(len(sections)):\n\n        # Remove multi level is level >= 2\n        sections[i] = sections[i].split(\',\')[0]\n\n        if sections[i] in config.sections():\n\n            # Case of args level > than 2 like --sec,fields,0,field=""value""\n            if len(fields[i].split(\',\')) >= 2:\n\n                splitted = fields[i].split(\',\')\n\n                #Get the actual fields\n                field  = splitted[0]\n                number = int(splitted[1])\n                f_name = splitted[2]\n                if field in list(config[sections[i]]):\n\n                    # Get the current string of the corresponding field\n                    current_config_field = config[sections[i]][field]\n\n                    # Count the number of occurence of the required field\n                    matching = re.findall(f_name+\'.\', current_config_field)\n                    if number >= len(matching):\n                        sys.stderr.write(\'ERROR: the field number \\""%s\\"" provided from command line is not valid, we found \\""%s\\"" \\""%s\\"" field(s) in section \\""%s\\""!\\n\' %(number, len(matching), f_name, field ))\n                        sys.exit(0)\n                    else:\n                        \n                        # Now replace\n                        str_to_be_replaced         = re.findall(f_name+\'.*\', current_config_field)[number]\n                        new_str                    = str(f_name+\'=\'+values[i])\n                        replaced                   = nth_replace_string(current_config_field, str_to_be_replaced, new_str, number+1)\n                        config[sections[i]][field] = replaced\n\n                else:\n                    sys.stderr.write(\'ERROR: field \\""%s\\"" of section \\""%s\\"" from command line is not valid!"")\\n\' %(field,sections[i]))\n                    sys.exit(0)\n            else:\n                if fields[i] in list(config[sections[i]]):\n                    config[sections[i]][fields[i]]=values[i]\n                else:\n                    sys.stderr.write(\'ERROR: field \\""%s\\"" of section \\""%s\\"" from command line is not valid!"")\\n\' %(fields[i],sections[i])) \n                    sys.exit(0)\n        else:\n            sys.stderr.write(\'ERROR: section \\""%s\\"" from command line is not valid!"")\\n\' %(sections[i]))\n            sys.exit(0)\n        \n    return [sections,fields,values]\n\n\ndef compute_avg_performance(info_lst):\n    \n    losses=[]\n    errors=[]\n    times=[]\n\n    for tr_info_file in info_lst:\n        config_res = configparser.ConfigParser()\n        config_res.read(tr_info_file)\n        losses.append(float(config_res[\'results\'][\'loss\']))\n        errors.append(float(config_res[\'results\'][\'err\']))\n        times.append(float(config_res[\'results\'][\'elapsed_time_chunk\']))\n        \n    loss=np.mean(losses)\n    error=np.mean(errors)\n    time=np.sum(times)\n    \n    return [loss,error,time]\n\n\ndef check_field(inp,type_inp,field):\n\n    valid_field=True\n    \n    if inp==\'\' and field!=\'cmd\':\n       sys.stderr.write(""ERROR: The the field  \\""%s\\"" of the config file is empty! \\n"" % (field))\n       valid_field=False\n       sys.exit(0)\n       \n    if type_inp==\'path\':\n        if not(os.path.isfile(inp)) and not(os.path.isdir(inp)) and inp!=\'none\':\n            sys.stderr.write(""ERROR: The path \\""%s\\"" specified in the field  \\""%s\\"" of the config file does not exists! \\n"" % (inp,field)) \n            valid_field=False\n            sys.exit(0)\n      \n    if \'{\' and \'}\' in type_inp :\n        arg_list=type_inp[1:-1].split(\',\')\n        if inp not in arg_list:\n            sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain %s  arguments \\n"" % (field,arg_list))\n            valid_field=False\n            sys.exit(0)\n       \n    if \'int(\' in type_inp:\n        try: \n            int(inp)\n        except ValueError:\n            sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain an integer (got \\""%s\\"") \\n"" % (field,inp))\n            valid_field=False\n            sys.exit(0)\n            \n        # Check if the value if within the expected range\n        lower_bound=type_inp.split(\',\')[0][4:]\n        upper_bound=type_inp.split(\',\')[1][:-1]\n        \n        if lower_bound!=""-inf"":\n            if int(inp)<int(lower_bound):\n                sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain an integer greater than %s (got \\""%s\\"") \\n"" % (field,lower_bound,inp))\n                valid_field=False\n                sys.exit(0)\n                \n        if upper_bound!=""inf"":\n            if int(inp)>int(upper_bound):\n                sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain an integer smaller than %s (got \\""%s\\"") \\n"" % (field,upper_bound,inp))        \n                valid_field=False\n                sys.exit(0)\n    \n    if \'float(\' in type_inp:\n        try: \n            float(inp)\n        except ValueError:\n            sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain a float (got \\""%s\\"") \\n"" % (field,inp))\n            valid_field=False\n            sys.exit(0)\n\n        # Check if the value if within the expected range\n        lower_bound=type_inp.split(\',\')[0][6:]\n        upper_bound=type_inp.split(\',\')[1][:-1]\n\n        \n        if lower_bound!=""-inf"":\n            if float(inp)<float(lower_bound):\n                sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain a float greater than %s (got \\""%s\\"") \\n"" % (field,lower_bound,inp))\n                valid_field=False\n                sys.exit(0)\n                \n        if upper_bound!=""inf"":\n            if float(inp)>float(upper_bound):\n                sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain a float smaller than %s (got \\""%s\\"") \\n"" % (field,upper_bound,inp))\n                valid_field=False\n                sys.exit(0)\n        \n    if type_inp==\'bool\':\n        lst={\'True\',\'true\',\'1\',\'False\',\'false\',\'0\'}\n        if not(inp in lst):\n            sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain a boolean (got \\""%s\\"") \\n"" % (field,inp))\n            valid_field=False\n            sys.exit(0)\n            \n    if \'int_list(\' in type_inp:\n        lst=inp.split(\',\')\n        try: \n            list(map(int,lst))\n        except ValueError:\n            sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain a list of integer (got \\""%s\\"").  Make also sure there aren\'t white spaces between commas.\\n"" % (field,inp))\n            valid_field=False\n            sys.exit(0)\n\n        # Check if the value if within the expected range\n        lower_bound=type_inp.split(\',\')[0][9:]\n        upper_bound=type_inp.split(\',\')[1][:-1]\n\n        for elem in lst:\n     \n            if lower_bound!=""-inf"":\n                if int(elem)<int(lower_bound):\n                    sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain an integer greater than %s (got \\""%s\\"") \\n"" % (field,lower_bound,elem))\n                    valid_field=False\n                    sys.exit(0)\n            if upper_bound!=""inf"":\n                if int(elem)>int(upper_bound):\n                    sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain an integer smaller than %s (got \\""%s\\"") \\n"" % (field,upper_bound,elem))\n                    valid_field=False\n                    sys.exit(0)            \n            \n    if \'float_list(\' in type_inp:\n        lst=inp.split(\',\')\n        try: \n            list(map(float,lst))\n        except ValueError:\n            sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain a list of floats (got \\""%s\\""). Make also sure there aren\'t white spaces between commas. \\n"" % (field,inp))\n            valid_field=False\n            sys.exit(0)\n        # Check if the value if within the expected range\n        lower_bound=type_inp.split(\',\')[0][11:]\n        upper_bound=type_inp.split(\',\')[1][:-1]\n\n        for elem in lst:\n     \n            if lower_bound!=""-inf"":\n                if float(elem)<float(lower_bound):\n                    sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain a float greater than %s (got \\""%s\\"") \\n"" % (field,lower_bound,elem))\n                    valid_field=False\n                    sys.exit(0)\n                    \n            if upper_bound!=""inf"":\n                if float(elem)>float(upper_bound):\n                    sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain a float smaller than %s (got \\""%s\\"") \\n"" % (field,upper_bound,elem))\n                    valid_field=False\n                    sys.exit(0)                  \n\n    if type_inp==\'bool_list\':\n        lst={\'True\',\'true\',\'1\',\'False\',\'false\',\'0\'}\n        inps=inp.split(\',\')\n        for elem in inps:\n            if not(elem in lst):\n                sys.stderr.write(""ERROR: The field \\""%s\\"" can only contain a list of boolean (got \\""%s\\""). Make also sure there aren\'t white spaces between commas.\\n"" % (field,inp))\n                valid_field=False\n                sys.exit(0)\n                    \n            \n    return  valid_field  \n\n\n\ndef get_all_archs(config):\n    \n    arch_lst=[]\n    for sec in config.sections():\n        if \'architecture\' in sec:\n            arch_lst.append(sec)\n    return arch_lst\n            \n    \ndef expand_section(config_proto,config):\n    \n    \n    # expands config_proto with fields in prototype files  \n    name_data=[]\n    name_arch=[]\n    for sec in config.sections():\n\n        if \'dataset\' in sec:\n\n            config_proto.add_section(sec)\n            config_proto[sec]=config_proto[\'dataset\']\n            name_data.append(config[sec][\'data_name\'])\n            \n        if \'architecture\' in sec:\n            name_arch.append(config[sec][\'arch_name\'])\n            config_proto.add_section(sec)\n            config_proto[sec]=config_proto[\'architecture\']\n            proto_file=config[sec][\'arch_proto\']\n    \n            # Reading proto file (architecture)\n            config_arch = configparser.ConfigParser()  \n            config_arch.read(proto_file)\n            \n            # Reading proto options\n            fields_arch=list(dict(config_arch.items(\'proto\')).keys())\n            fields_arch_type=list(dict(config_arch.items(\'proto\')).values())\n    \n            for i in range(len(fields_arch)):\n                config_proto.set(sec,fields_arch[i],fields_arch_type[i])\n\n\n            # Reading proto file (architecture_optimizer)\n            opt_type=config[sec][\'arch_opt\']\n            if opt_type==\'sgd\':\n                proto_file=\'proto/sgd.proto\'\n                \n            if opt_type==\'rmsprop\':\n                proto_file=\'proto/rmsprop.proto\'\n                \n            if opt_type==\'adam\':\n                proto_file=\'proto/adam.proto\'\n                \n            config_arch = configparser.ConfigParser()  \n            config_arch.read(proto_file)\n            \n            # Reading proto options\n            fields_arch=list(dict(config_arch.items(\'proto\')).keys())\n            fields_arch_type=list(dict(config_arch.items(\'proto\')).values())\n    \n            for i in range(len(fields_arch)):\n                config_proto.set(sec,fields_arch[i],fields_arch_type[i])\n                \n                \n    config_proto.remove_section(\'dataset\')\n    config_proto.remove_section(\'architecture\')\n    \n    return [config_proto,name_data,name_arch]\n    \n    \ndef expand_section_proto(config_proto,config):\n     \n    # Read config proto file\n    config_proto_optim_file=config[\'optimization\'][\'opt_proto\']\n    config_proto_optim = configparser.ConfigParser()\n    config_proto_optim.read(config_proto_optim_file)\n    for optim_par in list(config_proto_optim[\'proto\']):\n        config_proto.set(\'optimization\',optim_par,config_proto_optim[\'proto\'][optim_par])\n\n    \n\n\n    \ndef check_cfg_fields(config_proto,config,cfg_file):\n    \n    # Check mandatory sections and fields            \n    sec_parse=True\n       \n    for sec in config_proto.sections():\n        \n        if any(sec in s for s in config.sections()):\n            \n            # Check fields\n            for field in list(dict(config_proto.items(sec)).keys()):\n    \n                if not(field in config[sec]):\n                  sys.stderr.write(""ERROR: The confg file %s does not contain the field \\""%s=\\"" in section  \\""[%s]\\"" (mandatory)!\\n"" % (cfg_file,field,sec))               \n                  sec_parse=False\n                else:\n                    field_type=config_proto[sec][field]\n                    if not(check_field(config[sec][field],field_type,field)):           \n                        sec_parse=False\n                       \n                       \n    \n        # If a mandatory section doesn\'t exist...\n        else:\n            sys.stderr.write(""ERROR: The confg file %s does not contain \\""[%s]\\"" section (mandatory)!\\n"" % (cfg_file,sec))\n            sec_parse=False\n    \n    if sec_parse==False:\n        sys.stderr.write(""ERROR: Revise the confg file %s \\n"" % (cfg_file))\n        sys.exit(0)\n    return sec_parse\n\n\ndef check_consistency_with_proto(cfg_file,cfg_file_proto):\n\n    sec_parse=True\n    \n    # Check if cfg file exists\n    try:\n      open(cfg_file, \'r\')\n    except IOError:\n       sys.stderr.write(""ERROR: The confg file %s does not exist!\\n"" % (cfg_file))\n       sys.exit(0)\n       \n    # Check if cfg proto  file exists\n    try:\n      open(cfg_file_proto, \'r\')\n    except IOError:\n       sys.stderr.write(""ERROR: The confg file %s does not exist!\\n"" % (cfg_file_proto))\n       sys.exit(0)     \n      \n    # Parser Initialization    \n    config = configparser.ConfigParser()\n    \n    # Reading the cfg file\n    config.read(cfg_file)\n\n    # Reading proto cfg file    \n    config_proto = configparser.ConfigParser()\n    config_proto.read(cfg_file_proto)\n\n    \n    # Adding the multiple entries in data and architecture sections \n    [config_proto,name_data,name_arch]=expand_section(config_proto,config)\n        \n    # Check mandatory sections and fields            \n    sec_parse=check_cfg_fields(config_proto,config,cfg_file)\n    \n    if sec_parse==False:\n        sys.exit(0)\n        \n    return [config_proto,name_data,name_arch]\n    \n       \n    \ndef check_cfg(cfg_file,config,cfg_file_proto):\n \n    # Check consistency between cfg_file and cfg_file_proto    \n    [config_proto,name_data,name_arch]=check_consistency_with_proto(cfg_file,cfg_file_proto)\n\n    # Reload data_name because they might be altered by arguments\n    name_data=[]\n    for sec in config.sections():\n        if \'dataset\' in sec:\n            name_data.append(config[sec][\'data_name\'])\n            \n    # check consistency between [data_use] vs [data*]\n    sec_parse=True\n    data_use_with=[]\n    for data in list(dict(config.items(\'data_use\')).values()):\n        data_use_with.append(data.split(\',\'))\n        \n    data_use_with=sum(data_use_with, [])\n\n    if not(set(data_use_with).issubset(name_data)):\n        sys.stderr.write(""ERROR: in [data_use] you are using a dataset not specified in [dataset*] %s \\n"" % (cfg_file))\n        sec_parse=False\n        sys.exit(0) \n     \n    # Set to false the first layer norm layer if the architecture is sequential (to avoid numerical instabilities)\n    seq_model=False\n    for sec in config.sections():\n     if ""architecture"" in sec:  \n         if strtobool(config[sec][\'arch_seq_model\']):\n             seq_model=True\n             break\n         \n    if seq_model:\n        for item in list(config[\'architecture1\'].items()):\n            if \'use_laynorm\' in item[0] and \'_inp\' not in item[0]:\n                ln_list=item[1].split(\',\')\n                if ln_list[0]==\'True\':\n                    ln_list[0]=\'False\'\n                    config[\'architecture1\'][item[0]]=\',\'.join(ln_list)\n\n                    \n        \n    # Production case (We don\'t have the alignement for the forward_with), by default the prod\n    # Flag is set to False, and the dataset prod number to 1, corresponding to no prod dataset\n    config[\'exp\'][\'production\']=str(\'False\')\n    prod_dataset_number=""dataset1""\n\n    for data in name_data:\n\n        [lab_names,_,_]=parse_lab_field(config[cfg_item2sec(config,\'data_name\',data)][\'lab\'])\n        if ""none"" in lab_names and data == config[\'data_use\'][\'forward_with\']:\n            config[\'exp\'][\'production\']=str(\'True\')\n            prod_data_name = data\n            for sec in config.sections():\n                if \'dataset\' in sec:\n                    if config[sec][\'data_name\'] == data:\n                        prod_dataset_number = sec\n        else:\n            continue\n\n    # If production case is detected, remove all the other datasets except production\n    if config[\'exp\'][\'production\'] == str(\'True\'):\n        name_data = [elem for elem in name_data if elem == prod_data_name]\n\n    # Parse fea and lab  fields in datasets*\n    cnt=0\n    fea_names_lst=[]\n    lab_names_lst=[]\n    for data in name_data:\n\n        [lab_names,_,_]=parse_lab_field(config[cfg_item2sec(config,\'data_name\',data)][\'lab\'])\n        if ""none"" in lab_names:\n            continue\n\n        [fea_names,fea_lsts,fea_opts,cws_left,cws_right]=parse_fea_field(config[cfg_item2sec(config,\'data_name\',data)][\'fea\'])\n        [lab_names,lab_folders,lab_opts]=parse_lab_field(config[cfg_item2sec(config,\'data_name\',data)][\'lab\'])\n\n        fea_names_lst.append(sorted(fea_names))\n        lab_names_lst.append(sorted(lab_names))\n        \n        # Check that fea_name doesn\'t contain special characters\n        for name_features in fea_names_lst[cnt]:\n            if not(re.match(""^[a-zA-Z0-9]*$"", name_features)):\n                    sys.stderr.write(""ERROR: features names (fea_name=) must contain only letters or numbers (no special characters as \\""_,$,..\\"") \\n"" )\n                    sec_parse=False\n                    sys.exit(0) \n            \n        if cnt>0:\n            if fea_names_lst[cnt-1]!=fea_names_lst[cnt]:\n                sys.stderr.write(""ERROR: features name (fea_name) must be the same of all the datasets! \\n"" )\n                sec_parse=False\n                sys.exit(0) \n            if lab_names_lst[cnt-1]!=lab_names_lst[cnt]:\n                sys.stderr.write(""ERROR: labels name (lab_name) must be the same of all the datasets! \\n"" )\n                sec_parse=False\n                sys.exit(0) \n            \n        cnt=cnt+1\n\n    # Create the output folder \n    out_folder=config[\'exp\'][\'out_folder\']\n\n    if not os.path.exists(out_folder) or not(os.path.exists(out_folder+\'/exp_files\')) :\n        os.makedirs(out_folder+\'/exp_files\')\n        \n    # Parsing forward field\n    model=config[\'model\'][\'model\']\n    possible_outs=list(re.findall(\'(.*)=\',model.replace(\' \',\'\')))\n    forward_out_lst=config[\'forward\'][\'forward_out\'].split(\',\')\n    forward_norm_lst=config[\'forward\'][\'normalize_with_counts_from\'].split(\',\')\n    forward_norm_bool_lst=config[\'forward\'][\'normalize_posteriors\'].split(\',\')\n\n    lab_lst=list(re.findall(\'lab_name=(.*)\\n\',config[prod_dataset_number][\'lab\'].replace(\' \',\'\')))\n    lab_folders=list(re.findall(\'lab_folder=(.*)\\n\',config[prod_dataset_number][\'lab\'].replace(\' \',\'\')))\n    N_out_lab=[\'none\'] * len(lab_lst)\n\n    if config[\'exp\'][\'production\'] == str(\'False\'):\n        for i in range(len(lab_opts)):\n            \n            # Compute number of monophones if needed\n            if ""ali-to-phones"" in lab_opts[i]:\n\n                log_file=config[\'exp\'][\'out_folder\']+\'/log.log\'\n                folder_lab_count=lab_folders[i]\n                cmd=""hmm-info ""+folder_lab_count+""/final.mdl | awk \'/phones/{print $4}\'""\n                output=run_shell(cmd,log_file)\n                if output.decode().rstrip()==\'\':\n                    sys.stderr.write(""ERROR: hmm-info command doesn\'t exist. Make sure your .bashrc contains the Kaldi paths and correctly exports it.\\n"")\n                    sys.exit(0)\n        \n                N_out=int(output.decode().rstrip())\n                N_out_lab[i]=N_out\n\n\n        \n    \n        for i in range(len(forward_out_lst)):\n\n            if forward_out_lst[i] not in possible_outs:\n                sys.stderr.write(\'ERROR: the output \\""%s\\"" in the section \\""forward_out\\"" is not defined in section model)\\n\' %(forward_out_lst[i]))\n                sys.exit(0)\n\n            if strtobool(forward_norm_bool_lst[i]):\n\n                if forward_norm_lst[i] not in lab_lst:\n                    if not os.path.exists(forward_norm_lst[i]):\n                        sys.stderr.write(\'ERROR: the count_file \\""%s\\"" in the section \\""forward_out\\"" does not exist)\\n\' %(forward_norm_lst[i]))\n                        sys.exit(0)\n                    else:\n                        # Check if the specified file is in the right format\n                        f = open(forward_norm_lst[i],""r"")\n                        cnts = f.read()\n                        if not(bool(re.match(""(.*)\\[(.*)\\]"", cnts))):\n                            sys.stderr.write(\'ERROR: the count_file \\""%s\\"" in the section \\""forward_out\\"" not in the right format)\\n\' %(forward_norm_lst[i]))\n                            \n                        \n                else:\n                    # Try to automatically retrieve the count file from the config file\n                \n                        \n                    # Compute the number of context-dependent phone states    \n                    if ""ali-to-pdf"" in lab_opts[lab_lst.index(forward_norm_lst[i])]:\n                        log_file=config[\'exp\'][\'out_folder\']+\'/log.log\'\n                        folder_lab_count=lab_folders[lab_lst.index(forward_norm_lst[i])]\n                        cmd=""hmm-info ""+folder_lab_count+""/final.mdl | awk \'/pdfs/{print $4}\'""\n                        output=run_shell(cmd,log_file)\n                        if output.decode().rstrip()==\'\':\n                            sys.stderr.write(""ERROR: hmm-info command doesn\'t exist. Make sure your .bashrc contains the Kaldi paths and correctly exports it.\\n"")\n                            sys.exit(0)\n                        N_out=int(output.decode().rstrip())\n                        N_out_lab[lab_lst.index(forward_norm_lst[i])]=N_out\n                        count_file_path=out_folder+\'/exp_files/forward_\'+forward_out_lst[i]+\'_\'+forward_norm_lst[i]+\'.count\'\n                        cmd=""analyze-counts --print-args=False --verbose=0 --binary=false --counts-dim=""+str(N_out)+"" \\""ark:ali-to-pdf ""+folder_lab_count+""/final.mdl \\\\\\""ark:gunzip -c ""+folder_lab_count+""/ali.*.gz |\\\\\\"" ark:- |\\"" ""+ count_file_path\n                        run_shell(cmd,log_file)\n                        forward_norm_lst[i]=count_file_path\n\n                    else:\n                        sys.stderr.write(\'ERROR: Not able to automatically retrieve count file for the label \\""%s\\"". Please add a valid count file path in \\""normalize_with_counts_from\\"" or set normalize_posteriors=False \\n\' %(forward_norm_lst[i]))\n                        sys.exit(0)\n                    \n    # Update the config file with the count_file paths\n    config[\'forward\'][\'normalize_with_counts_from\']="","".join(forward_norm_lst)\n\n    \n    # When possible replace the pattern ""N_out_lab*"" with the detected number of output\n    for sec in config.sections():\n        for field in list(config[sec]):\n            for i in range(len(lab_lst)):\n                pattern=\'N_out_\'+lab_lst[i]\n\n                if pattern in config[sec][field]:\n                    if N_out_lab[i]!=\'none\':\n                        config[sec][field]=config[sec][field].replace(pattern,str(N_out_lab[i]))\n\n                    else:\n                       sys.stderr.write(\'ERROR: Cannot automatically retrieve the number of output in %s. Please, add manually the number of outputs \\n\' %(pattern))\n                       sys.exit(0)\n                       \n                       \n    # Check the model field\n    parse_model_field(cfg_file)\n\n    \n    # Create block diagram picture of the model\n    create_block_diagram(cfg_file)\n    \n\n\n    if sec_parse==False:\n        sys.exit(0)\n \n        \n    return  [config,name_data,name_arch] \n\n\ndef cfg_item2sec(config,field,value):\n    \n    for sec in config.sections():\n        if field in list(dict(config.items(sec)).keys()):\n            if value in list(dict(config.items(sec)).values()):\n                return sec\n            \n    sys.stderr.write(""ERROR: %s=%s not found in config file \\n"" % (field,value))\n    sys.exit(0)\n    return -1\n        \n        \n        \ndef split_chunks(seq, size):\n        newseq = []\n        splitsize = 1.0/size*len(seq)\n        for i in range(size):\n                newseq.append(seq[int(round(i*splitsize)):int(round((i+1)*splitsize))])\n        return newseq\n\ndef get_chunks_after_which_to_validate(N_ck_tr, nr_of_valid_per_epoch):\n    def _partition_chunks(N_ck_tr, nr_of_valid_per_epoch):\n        chunk_part = list()\n        chunk_size = int(np.ceil(N_ck_tr / float(nr_of_valid_per_epoch)))\n        for i1 in range(nr_of_valid_per_epoch):\n            chunk_part.append(range(0, N_ck_tr)[i1*chunk_size:(i1+1)*chunk_size])\n        return chunk_part\n\n    part_chunk_ids = _partition_chunks(N_ck_tr, nr_of_valid_per_epoch)\n    chunk_ids = list()\n    for l in part_chunk_ids:\n        chunk_ids.append(l[-1])\n    return chunk_ids\n\ndef do_validation_after_chunk(ck, N_ck_tr, config):\n    def _get_nr_of_valid_per_epoch_from_config(config):\n        if not \'nr_of_valid_per_epoch\' in config[\'exp\']:\n            return 1\n        return int(config[\'exp\'][\'nr_of_valid_per_epoch\'])\n\n    nr_of_valid_per_epoch = _get_nr_of_valid_per_epoch_from_config(config) \n    valid_chunks = get_chunks_after_which_to_validate(N_ck_tr, nr_of_valid_per_epoch)\n    if ck in valid_chunks:\n        return True\n    else:\n        return False\n\ndef _get_val_file_name_base(dataset, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val):\n    file_name = \'valid_\' + dataset + \'_ep\' + format(ep, N_ep_str_format) + \'_trCk\' + format(ck, N_ck_str_format)\n    if ck_val is None:\n        file_name += \'*\'\n    else:\n        file_name += \'_ck\' + format(ck_val, N_ck_str_format_val)\n    \n    return file_name\n\ndef get_val_lst_file_path(out_folder, valid_data, ep, ck, ck_val, fea_name, N_ep_str_format, N_ck_str_format, N_ck_str_format_val):\n    def _get_val_lst_file_name(dataset, ep, ck, ck_val, fea_name, N_ep_str_format, N_ck_str_format, N_ck_str_format_val): \n        file_name = _get_val_file_name_base(dataset, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val)\n        file_name += \'_\'\n        if not fea_name is None:\n            file_name += fea_name\n        else:\n            file_name += \'*\'\n        file_name += \'.lst\'\n        return file_name\n    \n    lst_file_name = _get_val_lst_file_name(valid_data, ep, ck, ck_val, fea_name, N_ep_str_format, N_ck_str_format, N_ck_str_format_val)\n    lst_file = out_folder+\'/exp_files/\' + lst_file_name\n    return lst_file\n\ndef get_val_info_file_path(out_folder, valid_data, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val):\n    def _get_val_info_file_name(dataset, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val):\n        file_name = _get_val_file_name_base(dataset, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val)\n        file_name += \'.info\'\n        return file_name\n\n    info_file_name = _get_val_info_file_name(valid_data, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val)\n    info_file = out_folder+\'/exp_files/\' + info_file_name\n    return info_file\n\ndef get_val_cfg_file_path(out_folder, valid_data, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val):\n    def _get_val_cfg_file_name(dataset, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val):\n        file_name = _get_val_file_name_base(dataset, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val)\n        file_name += \'.cfg\'\n        return file_name\n\n    cfg_file_name = _get_val_cfg_file_name(valid_data, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val)\n    config_chunk_file = out_folder + \'/exp_files/\' + cfg_file_name\n    return config_chunk_file\n\ndef create_configs(config):\n    \n    # This function create the chunk-specific config files\n    cfg_file_proto_chunk=config[\'cfg_proto\'][\'cfg_proto_chunk\']\n    N_ep=int(config[\'exp\'][\'N_epochs_tr\'])\n    N_ep_str_format=\'0\'+str(max(math.ceil(np.log10(N_ep)),1))+\'d\'\n    tr_data_lst=config[\'data_use\'][\'train_with\'].split(\',\')\n    valid_data_lst=config[\'data_use\'][\'valid_with\'].split(\',\')\n    max_seq_length_train=config[\'batches\'][\'max_seq_length_train\']\n    forward_data_lst=config[\'data_use\'][\'forward_with\'].split(\',\')\n    is_production = strtobool(config[\'exp\'][\'production\'])\n\n    \n    out_folder=config[\'exp\'][\'out_folder\']\n    cfg_file=out_folder+\'/conf.cfg\'\n    chunk_lst=out_folder+\'/exp_files/list_chunks.txt\'\n    \n    lst_chunk_file = open(chunk_lst, \'w\')\n    \n    # Read the batch size string\n    batch_size_tr_str=config[\'batches\'][\'batch_size_train\']\n    batch_size_tr_arr=expand_str_ep(batch_size_tr_str,\'int\',N_ep,\'|\',\'*\')\n    \n    # Read the max_seq_length_train\n    if len(max_seq_length_train.split(\',\')) == 1:\n        max_seq_length_tr_arr=expand_str_ep(max_seq_length_train,\'int\',N_ep,\'|\',\'*\')\n    else:\n        max_seq_length_tr_arr=[max_seq_length_train] * N_ep\n\n\n    cfg_file_proto=config[\'cfg_proto\'][\'cfg_proto\']\n\n    [config,name_data,name_arch]=check_cfg(cfg_file,config,cfg_file_proto)\n\n    arch_lst=get_all_archs(config)\n    lr={}\n    improvement_threshold={}\n    halving_factor={}\n    pt_files={}\n    drop_rates={}\n    for arch in arch_lst:\n        lr_arr=expand_str_ep(config[arch][\'arch_lr\'],\'float\',N_ep,\'|\',\'*\')\n        lr[arch]=lr_arr\n\n        improvement_threshold[arch]=float(config[arch][\'arch_improvement_threshold\'])\n        halving_factor[arch]=float(config[arch][\'arch_halving_factor\'])\n        pt_files[arch]=config[arch][\'arch_pretrain_file\']\n        \n        # Loop over all the sections and look for a ""_drop"" field (to perform dropout scheduling\n        for (field_key, field_val) in config.items(arch):\n            if ""_drop"" in field_key:\n                drop_lay=field_val.split(\',\')\n                N_lay=len(drop_lay)\n                drop_rates[arch]=[]\n                for lay_id in range(N_lay):\n                    drop_rates[arch].append(expand_str_ep(drop_lay[lay_id],\'float\',N_ep,\'|\',\'*\'))\n                 \n                # Check dropout factors\n                for dropout_factor in drop_rates[arch][0]:\n                    if float(dropout_factor)<0.0 or float(dropout_factor)>1.0:\n                        sys.stderr.write(\'The dropout rate should be between 0 and 1. Got %s in %s.\\n\' %(dropout_factor,field_key))\n                        sys.exit(0)\n\n    # Production case, we don\'t want to train, only forward without labels\n    if is_production:\n        ep           = N_ep-1\n        N_ep         = 0\n        model_files  = {}\n        max_seq_length_train_curr = max_seq_length_train\n\n        for arch in pt_files.keys():\n            model_files[arch] = out_folder+\'/exp_files/final_\'+arch+\'.pkl\'\n        \n    \n    if strtobool(config[\'batches\'][\'increase_seq_length_train\']):\n        max_seq_length_train_curr = config[\'batches\'][\'start_seq_len_train\']\n        if len(max_seq_length_train.split(\',\')) == 1:\n            max_seq_length_train_curr=int(max_seq_length_train_curr)\n        else:\n            # TODO: add support for increasing seq length when fea and lab have different time dimensionality\n            pass\n\n    for ep in range(N_ep):\n        \n        for tr_data in tr_data_lst:\n            \n            # Compute the total number of chunks for each training epoch\n            N_ck_tr=compute_n_chunks(out_folder,tr_data,ep,N_ep_str_format,\'train\')\n            N_ck_str_format=\'0\'+str(max(math.ceil(np.log10(N_ck_tr)),1))+\'d\'\n         \n            # ***Epoch training***\n            for ck in range(N_ck_tr):\n                \n                # path of the list of features for this chunk\n                lst_file=out_folder+\'/exp_files/train_\'+tr_data+\'_ep\'+format(ep, N_ep_str_format)+\'_ck\'+format(ck, N_ck_str_format)+\'_*.lst\'\n                \n                # paths of the output files (info,model,chunk_specific cfg file)\n                info_file=out_folder+\'/exp_files/train_\'+tr_data+\'_ep\'+format(ep,  N_ep_str_format)+\'_ck\'+format(ck, N_ck_str_format)+\'.info\'\n                \n                if ep+ck==0:\n                    model_files_past={}\n                else:\n                    model_files_past=model_files\n                    \n                model_files={}\n                for arch in pt_files.keys():\n                    model_files[arch]=info_file.replace(\'.info\',\'_\'+arch+\'.pkl\')\n                \n                config_chunk_file=out_folder+\'/exp_files/train_\'+tr_data+\'_ep\'+format(ep, N_ep_str_format)+\'_ck\'+format(ck, N_ck_str_format)+\'.cfg\'\n                lst_chunk_file.write(config_chunk_file+\'\\n\')\n                \n                if strtobool(config[\'batches\'][\'increase_seq_length_train\'])==False:\n                    if len(max_seq_length_train.split(\',\')) == 1:\n                        max_seq_length_train_curr=int(max_seq_length_tr_arr[ep])\n                    else:\n                        max_seq_length_train_curr=max_seq_length_tr_arr[ep]\n                    \n                # Write chunk-specific cfg file\n                write_cfg_chunk(cfg_file,config_chunk_file,cfg_file_proto_chunk,pt_files,lst_file,info_file,\'train\',tr_data,lr,max_seq_length_train_curr,name_data,ep,ck,batch_size_tr_arr[ep],drop_rates)\n                \n                # update pt_file (used to initialized the DNN for the next chunk)  \n                for pt_arch in pt_files.keys():\n                    pt_files[pt_arch]=out_folder+\'/exp_files/train_\'+tr_data+\'_ep\'+format(ep, N_ep_str_format)+\'_ck\'+format(ck, N_ck_str_format)+\'_\'+pt_arch+\'.pkl\'\n                if do_validation_after_chunk(ck, N_ck_tr, config):\n                    for valid_data in valid_data_lst:\n                        N_ck_valid = compute_n_chunks(out_folder,valid_data,ep,N_ep_str_format,\'valid\')\n                        N_ck_str_format_val = \'0\'+str(max(math.ceil(np.log10(N_ck_valid)),1))+\'d\'\n                        for ck_val in range(N_ck_valid):\n                            lst_file = get_val_lst_file_path(out_folder, valid_data, ep, ck, ck_val, None, N_ep_str_format, N_ck_str_format, N_ck_str_format_val)\n                            info_file = get_val_info_file_path(out_folder, valid_data, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val)\n                            config_chunk_file = get_val_cfg_file_path(out_folder, valid_data, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val)\n                            lst_chunk_file.write(config_chunk_file+\'\\n\')\n                            write_cfg_chunk(cfg_file, config_chunk_file, cfg_file_proto_chunk, model_files, lst_file, info_file, \'valid\', valid_data, lr, max_seq_length_train_curr, name_data, ep, ck_val, batch_size_tr_arr[ep], drop_rates)\n                    if strtobool(config[\'batches\'][\'increase_seq_length_train\']):\n                        if len(max_seq_length_train.split(\',\')) == 1:\n                            max_seq_length_train_curr=max_seq_length_train_curr*int(config[\'batches\'][\'multply_factor_seq_len_train\'])\n                            if max_seq_length_train_curr>int(max_seq_length_tr_arr[ep]):\n                                max_seq_length_train_curr=int(max_seq_length_tr_arr[ep])\n                        else:\n                            # TODO: add support for increasing seq length when fea and lab have different time dimensionality\n                            pass\n        \n    for forward_data in forward_data_lst:\n               \n             # Compute the number of chunks\n             N_ck_forward=compute_n_chunks(out_folder,forward_data,ep,N_ep_str_format,\'forward\')\n             N_ck_str_format=\'0\'+str(max(math.ceil(np.log10(N_ck_forward)),1))+\'d\'\n             \n             for ck in range(N_ck_forward):\n                        \n                # path of the list of features for this chunk\n                lst_file=out_folder+\'/exp_files/forward_\'+forward_data+\'_ep\'+format(ep, N_ep_str_format)+\'_ck\'+format(ck, N_ck_str_format)+\'_*.lst\'\n                \n                # output file\n                info_file=out_folder+\'/exp_files/forward_\'+forward_data+\'_ep\'+format(ep, N_ep_str_format)+\'_ck\'+format(ck, N_ck_str_format)+\'.info\'\n                config_chunk_file=out_folder+\'/exp_files/forward_\'+forward_data+\'_ep\'+format(ep, N_ep_str_format)+\'_ck\'+format(ck, N_ck_str_format)+\'.cfg\'\n                lst_chunk_file.write(config_chunk_file+\'\\n\')\n                \n                # Write chunk-specific cfg file\n                write_cfg_chunk(cfg_file,config_chunk_file,cfg_file_proto_chunk,model_files,lst_file,info_file,\'forward\',forward_data,lr,max_seq_length_train_curr,name_data,ep,ck,batch_size_tr_arr[ep],drop_rates)\n                    \n    lst_chunk_file.close()\n                    \n                    \ndef create_lists(config):\n    def _get_validation_data_for_chunks(fea_names, list_fea, N_chunks): \n        full_list=[]\n        for i in range(len(fea_names)):\n            full_list.append([line.rstrip(\'\\n\')+\',\' for line in open(list_fea[i])])\n            full_list[i]=sorted(full_list[i])\n        full_list_fea_conc=full_list[0]\n        for i in range(1,len(full_list)):  \n            full_list_fea_conc=list(map(str.__add__,full_list_fea_conc,full_list[i]))\n        random.shuffle(full_list_fea_conc)\n        valid_chunks_fea=list(split_chunks(full_list_fea_conc,N_chunks))\n        return valid_chunks_fea\n    def _shuffle_forward_data(config):\n        if \'shuffle_forwarding_data\' in config[\'forward\']:\n            suffle_on_forwarding = strtobool(config[\'forward\'][\'shuffle_forwarding_data\'])\n            if not suffle_on_forwarding:\n                return False\n        return True\n    \n    # splitting data into chunks (see out_folder/additional_files)\n    out_folder=config[\'exp\'][\'out_folder\']\n    seed=int(config[\'exp\'][\'seed\'])\n    N_ep=int(config[\'exp\'][\'N_epochs_tr\'])    \n    N_ep_str_format=\'0\'+str(max(math.ceil(np.log10(N_ep)),1))+\'d\'\n    \n    # Setting the random seed\n    random.seed(seed)\n    \n    # training chunk lists creation    \n    tr_data_name=config[\'data_use\'][\'train_with\'].split(\',\')\n    \n    # Reading validation feature lists\n    for dataset in tr_data_name:\n        sec_data=cfg_item2sec(config,\'data_name\',dataset)\n        [fea_names,list_fea,fea_opts,cws_left,cws_right]=parse_fea_field(config[cfg_item2sec(config,\'data_name\',dataset)][\'fea\'])\n\n        N_chunks= int(config[sec_data][\'N_chunks\'])\n        N_ck_str_format=\'0\'+str(max(math.ceil(np.log10(N_chunks)),1))+\'d\'\n         \n        full_list=[]\n        \n        for i in range(len(fea_names)):\n            full_list.append([line.rstrip(\'\\n\')+\',\' for line in open(list_fea[i])])\n            full_list[i]=sorted(full_list[i])\n          \n\n        # concatenating all the featues in a single file (useful for shuffling consistently)\n        full_list_fea_conc=full_list[0]\n        for i in range(1,len(full_list)):  \n            full_list_fea_conc=list(map(str.__add__,full_list_fea_conc,full_list[i]))\n         \n        \n        for ep in range(N_ep):\n           #  randomize the list\n           random.shuffle(full_list_fea_conc)\n           tr_chunks_fea=list(split_chunks(full_list_fea_conc,N_chunks))\n           tr_chunks_fea.reverse()\n    \n           for ck in range(N_chunks):\n                for i in range(len(fea_names)):\n                    \n                    tr_chunks_fea_split=[];\n                    for snt in tr_chunks_fea[ck]:\n                        #print(snt.split(\',\')[i])\n                        tr_chunks_fea_split.append(snt.split(\',\')[i])\n                    output_lst_file=out_folder+\'/exp_files/train_\'+dataset+\'_ep\'+format(ep,  N_ep_str_format)+\'_ck\'+format(ck, N_ck_str_format)+\'_\'+fea_names[i]+\'.lst\'\n                    f=open(output_lst_file,\'w\')\n                    tr_chunks_fea_wr=map(lambda x:x+\'\\n\', tr_chunks_fea_split)\n                    f.writelines(tr_chunks_fea_wr)\n                    f.close()\n                if do_validation_after_chunk(ck, N_chunks, config):\n                    valid_data_name=config[\'data_use\'][\'valid_with\'].split(\',\')\n                    for dataset_val in valid_data_name:\n                        sec_data = cfg_item2sec(config,\'data_name\',dataset_val)\n                        fea_names, list_fea, fea_opts, cws_left, cws_right = parse_fea_field(config[cfg_item2sec(config,\'data_name\',dataset_val)][\'fea\'])\n                        N_chunks_val = int(config[sec_data][\'N_chunks\'])\n                        N_ck_str_format_val = \'0\'+str(max(math.ceil(np.log10(N_chunks_val)),1))+\'d\'\n                        valid_chunks_fea = _get_validation_data_for_chunks(fea_names, list_fea, N_chunks_val)\n                        for ck_val in range(N_chunks_val):\n                            for fea_idx in range(len(fea_names)):\n                                valid_chunks_fea_split=[];\n                                for snt in valid_chunks_fea[ck_val]:\n                                    valid_chunks_fea_split.append(snt.split(\',\')[fea_idx])\n                                output_lst_file = get_val_lst_file_path(out_folder, dataset_val, ep, ck, ck_val, fea_names[fea_idx], N_ep_str_format, N_ck_str_format, N_ck_str_format_val)\n                                f=open(output_lst_file,\'w\')\n                                valid_chunks_fea_wr=map(lambda x:x+\'\\n\', valid_chunks_fea_split)\n                                f.writelines(valid_chunks_fea_wr)\n                                f.close()\n                    \n    # forward chunk lists creation    \n    forward_data_name=config[\'data_use\'][\'forward_with\'].split(\',\')\n    \n    # Reading validation feature lists\n    for dataset in forward_data_name:\n        sec_data=cfg_item2sec(config,\'data_name\',dataset)\n        [fea_names,list_fea,fea_opts,cws_left,cws_right]=parse_fea_field(config[cfg_item2sec(config,\'data_name\',dataset)][\'fea\'])\n\n        N_chunks= int(config[sec_data][\'N_chunks\'])\n        N_ck_str_format=\'0\'+str(max(math.ceil(np.log10(N_chunks)),1))+\'d\'\n        \n        full_list=[]\n        \n        for i in range(len(fea_names)):\n            full_list.append([line.rstrip(\'\\n\')+\',\' for line in open(list_fea[i])])\n            full_list[i]=sorted(full_list[i])\n          \n\n        # concatenating all the featues in a single file (useful for shuffling consistently)\n        full_list_fea_conc=full_list[0]\n        for i in range(1,len(full_list)):  \n            full_list_fea_conc=list(map(str.__add__,full_list_fea_conc,full_list[i]))\n            \n         \n        # randomize the list\n        if _shuffle_forward_data(config):\n            random.shuffle(full_list_fea_conc)\n        forward_chunks_fea=list(split_chunks(full_list_fea_conc,N_chunks))\n\n        \n        for ck in range(N_chunks):\n            for i in range(len(fea_names)):\n                \n                forward_chunks_fea_split=[];\n                for snt in forward_chunks_fea[ck]:\n                    #print(snt.split(\',\')[i])\n                    forward_chunks_fea_split.append(snt.split(\',\')[i])\n                    \n                output_lst_file=out_folder+\'/exp_files/forward_\'+dataset+\'_ep\'+format(ep, N_ep_str_format)+\'_ck\'+format(ck,N_ck_str_format)+\'_\'+fea_names[i]+\'.lst\'\n                f=open(output_lst_file,\'w\')\n                forward_chunks_fea_wr=map(lambda x:x+\'\\n\', forward_chunks_fea_split)\n                f.writelines(forward_chunks_fea_wr)\n                f.close()  \n\n    \n    \ndef write_cfg_chunk(cfg_file,config_chunk_file,cfg_file_proto_chunk,pt_files,lst_file,info_file,to_do,data_set_name,lr,max_seq_length_train_curr,name_data,ep,ck,batch_size,drop_rates):\n                    \n    # writing the chunk-specific cfg file\n    config = configparser.ConfigParser()\n    config.read(cfg_file)\n    \n    config_chunk = configparser.ConfigParser()\n    config_chunk.read(cfg_file)\n    \n    # Exp section\n    config_chunk[\'exp\'][\'to_do\']=to_do\n    config_chunk[\'exp\'][\'out_info\']=info_file\n    \n    # change seed for randomness\n    config_chunk[\'exp\'][\'seed\']=str(int(config_chunk[\'exp\'][\'seed\'])+ep+ck)\n    \n    config_chunk[\'batches\'][\'batch_size_train\']=batch_size\n    \n    for arch in pt_files.keys():\n        config_chunk[arch][\'arch_pretrain_file\']=pt_files[arch]\n    \n    # writing the current learning rate\n    for lr_arch in lr.keys():\n        config_chunk[lr_arch][\'arch_lr\']=str(lr[lr_arch][ep])\n        \n        for (field_key, field_val) in config.items(lr_arch):\n            if ""_drop"" in field_key:\n                N_lay=len(drop_rates[lr_arch])\n                drop_arr=[]\n                for lay in range(N_lay):\n                    drop_arr.append(drop_rates[lr_arch][lay][ep])\n                \n                config_chunk[lr_arch][field_key]=str(\',\'.join(drop_arr))\n\n       \n    # Data_chunk section\n    config_chunk.add_section(\'data_chunk\')\n\n    config_chunk[\'data_chunk\']=config[cfg_item2sec(config,\'data_name\',data_set_name)]\n   \n\n    lst_files=sorted(glob.glob(lst_file))\n\n    current_fea=config_chunk[\'data_chunk\'][\'fea\']\n    \n    list_current_fea=re.findall(\'fea_name=(.*)\\nfea_lst=(.*)\\n\', current_fea)\n    \n    for (fea, path) in list_current_fea:\n        for path_cand in lst_files:\n            fea_type_cand=re.findall(\'_(.*).lst\', path_cand)[0].split(\'_\')[-1]\n            if fea_type_cand==fea:\n                config_chunk[\'data_chunk\'][\'fea\']=config_chunk[\'data_chunk\'][\'fea\'].replace(path,path_cand)\n                \n    \n    config_chunk.remove_option(\'data_chunk\',\'data_name\')\n    config_chunk.remove_option(\'data_chunk\',\'N_chunks\')\n    \n    config_chunk.remove_section(\'decoding\')\n    config_chunk.remove_section(\'data_use\')\n    \n    data_to_del = []\n    for sec in config.sections():\n        if \'dataset\' in sec:\n            data_to_del.append(config[sec][\'data_name\'])\n\n    for dataset in data_to_del:\n        config_chunk.remove_section(cfg_item2sec(config_chunk,\'data_name\',dataset))\n    \n    # Create batche section\n    config_chunk.remove_option(\'batches\',\'increase_seq_length_train\')\n    config_chunk.remove_option(\'batches\',\'start_seq_len_train\')\n    config_chunk.remove_option(\'batches\',\'multply_factor_seq_len_train\')\n    \n    config_chunk[\'batches\'][\'max_seq_length_train\']=str(max_seq_length_train_curr)\n    \n    # Write cfg_file_chunk\n    with open(config_chunk_file, \'w\') as configfile:\n        config_chunk.write(configfile)\n                   \n    # Check cfg_file_chunk\n    [config_proto_chunk,name_data_ck,name_arch_ck]=check_consistency_with_proto(config_chunk_file,cfg_file_proto_chunk)\n\n    \ndef parse_fea_field(fea):\n    \n    # Adding the required fields into a list\n    fea_names=[]\n    fea_lsts=[]\n    fea_opts=[]\n    cws_left=[]\n    cws_right=[]\n    \n    for line in fea.split(\'\\n\'):\n        \n        line=re.sub(\' +\',\' \',line)\n    \n        if \'fea_name=\' in line:\n           fea_names.append(line.split(\'=\')[1]) \n           \n        if \'fea_lst=\' in line:\n            fea_lsts.append(line.split(\'=\')[1]) \n            \n        if \'fea_opts=\' in line:\n            fea_opts.append(line.split(\'fea_opts=\')[1]) \n            \n        if \'cw_left=\' in line:\n            cws_left.append(line.split(\'=\')[1])\n            if not(check_field(line.split(\'=\')[1],\'int(0,inf)\',\'cw_left\')):\n                sys.exit(0)\n            \n        if \'cw_right=\' in line:\n            cws_right.append(line.split(\'=\')[1])         \n            if not(check_field(line.split(\'=\')[1],\'int(0,inf)\',\'cw_right\')):\n                sys.exit(0)  \n                \n                \n    # Check features names\n    if not(sorted(fea_names)==sorted(list(set(fea_names)))):\n      sys.stderr.write(\'ERROR fea_names must be different! (got %s)\' %(fea_names)) \n      sys.exit(0)\n    \n    snt_lst=[]\n    cnt=0\n    \n    # Check consistency of feature lists\n    for fea_lst in fea_lsts:\n         if not(os.path.isfile(fea_lst)):\n             sys.stderr.write(""ERROR: The path \\""%s\\"" specified in the field  \\""fea_lst\\"" of the config file does not exists! \\n"" % (fea_lst))\n             sys.exit(0)\n         else:\n             snts = sorted([line.rstrip(\'\\n\').split(\' \')[0] for line in open(fea_lst)])\n             snt_lst.append(snts)\n             # Check if all the sentences are present in all the list files \n             if cnt>0:\n                 if snt_lst[cnt-1]!=snt_lst[cnt]:\n                     sys.stderr.write(""ERROR: the files %s in fea_lst contain a different set of sentences! \\n"" % (fea_lst))\n                     sys.exit(0)\n             cnt=cnt+1\n    return [fea_names,fea_lsts,fea_opts,cws_left,cws_right]\n\n\ndef parse_lab_field(lab):\n    \n    # Adding the required fields into a list\n    lab_names=[]\n    lab_folders=[]\n    lab_opts=[]\n    \n    \n    for line in lab.split(\'\\n\'):\n        \n        line=re.sub(\' +\',\' \',line)\n        \n    \n    \n        if \'lab_name=\' in line:\n           lab_names.append(line.split(\'=\')[1]) \n           \n        if \'lab_folder=\' in line:\n            lab_folders.append(line.split(\'=\')[1]) \n            \n        if \'lab_opts=\' in line:\n            lab_opts.append(line.split(\'lab_opts=\')[1]) \n            \n                \n    \n               \n    # Check features names\n    if not(sorted(lab_names)==sorted(list(set(lab_names)))):\n      sys.stderr.write(\'ERROR lab_names must be different! (got %s)\' %(lab_names)) \n      sys.exit(0)\n    \n    \n    # Check consistency of feature lists\n    for lab_fold in lab_folders:\n         if not(os.path.isdir(lab_fold)):\n             sys.stderr.write(""ERROR: The path \\""%s\\"" specified in the field  \\""lab_folder\\"" of the config file does not exists! \\n"" % (lab_fold))\n             sys.exit(0)\n             \n    return [lab_names,lab_folders,lab_opts]\n\n\ndef compute_n_chunks(out_folder,data_list,ep,N_ep_str_format,step):\n    list_ck=sorted(glob.glob(out_folder+\'/exp_files/\'+step+\'_\'+data_list+\'_ep\'+format(ep, N_ep_str_format)+\'*.lst\'))\n    last_ck=list_ck[-1]\n    N_ck=int(re.findall(\'_ck(.+)_\', last_ck)[-1].split(\'_\')[0])+1\n    return N_ck\n\n\ndef parse_model_field(cfg_file):\n        \n    # Reading the config file\n    config = configparser.ConfigParser()\n    config.read(cfg_file)\n    \n    # reading the proto file\n    model_proto_file=config[\'model\'][\'model_proto\']\n    f = open(model_proto_file,""r"")\n    proto_model = f.read()\n    \n    # readiing the model string\n    model=config[\'model\'][\'model\']\n    \n    # Reading fea,lab arch architectures from the cfg file   \n    fea_lst=list(re.findall(\'fea_name=(.*)\\n\',config[\'dataset1\'][\'fea\'].replace(\' \',\'\')))\n    lab_lst=list(re.findall(\'lab_name=(.*)\\n\',config[\'dataset1\'][\'lab\'].replace(\' \',\'\')))\n    arch_lst=list(re.findall(\'arch_name=(.*)\\n\',open(cfg_file, \'r\').read().replace(\' \',\'\')))\n    possible_operations=re.findall(\'(.*)\\((.*),(.*)\\)\\n\',proto_model)\n\n       \n    \n    possible_inputs=fea_lst\n    model_arch=list(filter(None, model.replace(\' \',\'\').split(\'\\n\')))\n    \n    # Reading the model field line by line\n    for line in model_arch:\n        \n        pattern=\'(.*)=(.*)\\((.*),(.*)\\)\'\n        \n        if not re.match(pattern,line):\n            sys.stderr.write(\'ERROR: all the entries must be of the following type: output=operation(str,str), got (%s)\\n\'%(line))\n            sys.exit(0)\n        else:\n            \n             # Analyze line and chech if it is compliant with proto_model\n             [out_name,operation,inp1,inp2]=list(re.findall(pattern,line)[0])\n             inps=[inp1,inp2]\n             \n             found=False\n             for i in range(len(possible_operations)):\n                 if operation==possible_operations[i][0]:\n                     found=True\n    \n                     for k in range(1,3):\n                         if possible_operations[i][k]==\'architecture\':\n                             if inps[k-1] not in arch_lst:\n                                 sys.stderr.write(\'ERROR: the architecture \\""%s\\"" is not in the architecture lists of the config file (possible architectures are %s)\\n\' %(inps[k-1],arch_lst))\n                                 sys.exit(0)\n                                 \n                         if possible_operations[i][k]==\'label\':\n                             if inps[k-1] not in lab_lst:\n                                 sys.stderr.write(\'ERROR: the label \\""%s\\"" is not in the label lists of the config file (possible labels are %s)\\n\' %(inps[k-1],lab_lst))\n                                 sys.exit(0)\n     \n                         if possible_operations[i][k]==\'input\':\n                             if inps[k-1] not in possible_inputs:\n                                 sys.stderr.write(\'ERROR: the input \\""%s\\"" is not defined before (possible inputs are %s)\\n\' %(inps[k-1],possible_inputs))\n                                 sys.exit(0) \n                                 \n                         if possible_operations[i][k]==\'float\':\n                             \n                             try:\n                                 float(inps[k-1])\n                             except ValueError:\n                                     sys.stderr.write(\'ERROR: the input \\""%s\\"" must be a float, got %s\\n\' %(inps[k-1],line))\n                                     sys.exit(0) \n                        \n                     # Update the list of possible inpus\n                     possible_inputs.append(out_name)\n                     break\n                         \n                         \n                     \n             if found==False:\n                 sys.stderr.write((\'ERROR: operation \\""%s\\"" does not exists (not defined into the model proto file)\\n\'%(operation)))\n                 sys.exit(0)\n                 \n    # Check for the mandatory fiels\n    if \'loss_final\' not in """".join(model_arch):\n        sys.stderr.write(\'ERROR: the variable loss_final should be defined in model\\n\')\n        sys.exit(0)\n    \n    if \'err_final\' not in """".join(model_arch):\n        sys.stderr.write(\'ERROR: the variable err_final should be defined in model\\n\')\n        sys.exit(0)\n        \ndef terminal_node_detection(model_arch,node):\n    \n    terminal=True\n    pattern=\'(.*)=(.*)\\((.*),(.*)\\)\'\n    \n    for line in model_arch:\n        [out_name,operation,inp1,inp2]=list(re.findall(pattern,line)[0])\n        if inp1==node or inp2==node:\n            terminal=False\n            \n    return terminal\n    \n    \ndef create_block_connection(lst_inp,model_arch,diag_lines,cnt_names,arch_dict):\n  \n  if lst_inp==[]:\n      return [[],[],diag_lines]\n  \n  pattern=\'(.*)=(.*)\\((.*),(.*)\\)\'\n\n  arch_current=[]\n  output_conn=[]\n  current_inp=[]\n\n   \n  for input_element in lst_inp:\n           \n      \n      for l in range(len(model_arch)):\n          [out_name,operation,inp1,inp2]=list(re.findall(pattern,model_arch[l])[0])\n          \n          if inp1==input_element or inp2==input_element:\n              if operation==\'compute\':\n                  arch_current.append(inp1)\n                  output_conn.append(out_name)\n                  current_inp.append(inp2)\n                  model_arch[l]=\'processed\'+\'=\'+operation+\'(\'+inp1+\',processed)\'\n              else:\n                  arch_current.append(out_name)\n                  output_conn.append(out_name)\n                  if inp1==input_element:\n                     current_inp.append(inp1)\n                     model_arch[l]=out_name+\'=\'+operation+\'(processed,\'+inp2+\')\'\n                     \n                  if inp2==input_element:\n                     current_inp.append(inp2)\n                     model_arch[l]=out_name+\'=\'+operation+\'(\'+inp1+\',processed)\'\n       \n    \n  for i in range(len(arch_current)):            \n       # Create connections           \n       diag_lines=diag_lines+str(cnt_names.index(arch_dict[current_inp[i]]))+\' -> \'+str(cnt_names.index(arch_current[i]))+\' [label = ""\'+current_inp[i]+\'""]\\n\'\n   \n  #  remove terminal nodes from output list\n  output_conn_pruned=[]\n  for node in output_conn:\n      if not(terminal_node_detection(model_arch,node)):\n          output_conn_pruned.append(node)\n          \n  [arch_current,output_conn,diag_lines]=create_block_connection(output_conn,model_arch,diag_lines,cnt_names,arch_dict)\n            \n  return [arch_current,output_conn_pruned,diag_lines]\n\n        \ndef create_block_diagram(cfg_file):\n    \n    # Reading the config file\n    config = configparser.ConfigParser()\n    config.read(cfg_file)\n    \n    # readiing the model string\n    model=config[\'model\'][\'model\']\n    \n    # Reading fea,lab arch architectures from the cfg file   \n    pattern=\'(.*)=(.*)\\((.*),(.*)\\)\'\n    \n    fea_lst=list(re.findall(\'fea_name=(.*)\\n\',config[\'dataset1\'][\'fea\'].replace(\' \',\'\')))\n    lab_lst=list(re.findall(\'lab_name=(.*)\\n\',config[\'dataset1\'][\'lab\'].replace(\' \',\'\')))\n    arch_lst=list(re.findall(\'arch_name=(.*)\\n\',open(cfg_file, \'r\').read().replace(\' \',\'\')))\n    \n    \n    out_diag_file=config[\'exp\'][\'out_folder\']+\'/model.diag\'\n    \n    model_arch=list(filter(None, model.replace(\' \',\'\').split(\'\\n\')))\n    \n    \n    diag_lines=\'blockdiag {\\n\';\n    \n    cnt=0\n    cnt_names=[]\n    arch_lst=[]\n    fea_lst_used=[]\n    lab_lst_used=[]\n    \n    for line in model_arch:\n        if \'err_final=\' in line:\n            model_arch.remove(line)\n            \n    # Initializations of the blocks\n    for line in model_arch:\n     [out_name,operation,inp1,inp2]=list(re.findall(pattern,line)[0])\n     \n     if operation!=\'compute\':\n                    \n            # node architecture\n            diag_lines=diag_lines+str(cnt)+\' [label=""\'+operation+\'"",shape = roundedbox];\\n\'\n            arch_lst.append(out_name)\n            cnt_names.append(out_name)\n            cnt=cnt+1\n            \n            # labels\n            if inp2 in lab_lst:\n                diag_lines=diag_lines+str(cnt)+\' [label=""\'+inp2+\'"",shape = roundedbox];\\n\'\n                if inp2 not in lab_lst_used:\n                    lab_lst_used.append(inp2)\n                    cnt_names.append(inp2)\n                    cnt=cnt+1\n                    \n            # features\n            if inp1 in fea_lst :\n                diag_lines=diag_lines+str(cnt)+\' [label=""\'+inp1+\'"",shape = circle];\\n\'\n                if inp1 not in fea_lst_used:\n                    fea_lst_used.append(inp1)\n                    cnt_names.append(inp1)\n                    cnt=cnt+1\n                    \n            if inp2 in fea_lst :\n                diag_lines=diag_lines+str(cnt)+\' [label=""\'+inp2+\'"",shape = circle];\\n\'\n                if inp2 not in fea_lst_used:\n                    fea_lst_used.append(inp2)\n                    cnt_names.append(inp2)\n                    cnt=cnt+1\n            \n            \n            \n     else:\n        # architecture\n        diag_lines=diag_lines+str(cnt)+\' [label=""\'+inp1+\'"",shape = box];\\n\'\n        arch_lst.append(inp1)\n        cnt_names.append(inp1)\n        cnt=cnt+1\n        \n        # feature\n        if inp2 in fea_lst:\n            diag_lines=diag_lines+str(cnt)+\' [label=""\'+inp2+\'"",shape = circle];\\n\'\n            if inp2 not in fea_lst_used:\n                fea_lst_used.append(inp2)\n                cnt_names.append(inp2)\n                cnt=cnt+1\n                   \n    \n    # Connections across blocks\n    lst_conc=fea_lst_used+lab_lst_used\n    \n    arch_dict={}\n    \n    for elem in lst_conc:\n        arch_dict[elem]=elem\n        \n    for model_line in model_arch:\n        [out_name,operation,inp1,inp2]=list(re.findall(pattern,model_line)[0])\n        if operation==\'compute\':\n            arch_dict[out_name]=inp1\n        else:\n           arch_dict[out_name]=out_name \n     \n    \n          \n    output_conn=lst_conc\n    [arch_current,output_conn,diag_lines]=create_block_connection(output_conn,model_arch,diag_lines,cnt_names,arch_dict)\n   \n               \n    diag_lines=diag_lines+\'}\'\n\n\n    # Write the diag file describing the model\n    with open(out_diag_file, ""w"") as text_file:\n        text_file.write(""%s"" % diag_lines )\n        \n    # Create image from the diag file\n    log_file=config[\'exp\'][\'out_folder\']+\'/log.log\'\n    cmd=\'blockdiag -Tsvg \'+out_diag_file+\' -o \'+config[\'exp\'][\'out_folder\']+\'/model.svg\'\n    run_shell(cmd,log_file)\n    \n    \ndef list_fea_lab_arch(config): # cancel\n    model=config[\'model\'][\'model\'].split(\'\\n\')\n    fea_lst=list(re.findall(\'fea_name=(.*)\\n\',config[\'data_chunk\'][\'fea\'].replace(\' \',\'\')))\n    lab_lst=list(re.findall(\'lab_name=(.*)\\n\',config[\'data_chunk\'][\'lab\'].replace(\' \',\'\')))\n\n    \n    fea_lst_used=[]\n    lab_lst_used=[]\n    arch_lst_used=[]\n    \n    fea_dict_used={}\n    lab_dict_used={}\n    arch_dict_used={}\n    \n    fea_lst_used_name=[]\n    lab_lst_used_name=[]\n    arch_lst_used_name=[]\n    \n    fea_field=config[\'data_chunk\'][\'fea\']\n    lab_field=config[\'data_chunk\'][\'lab\']\n    \n    pattern=\'(.*)=(.*)\\((.*),(.*)\\)\'\n    \n    for line in model:\n        [out_name,operation,inp1,inp2]=list(re.findall(pattern,line)[0])\n        \n        if inp1 in fea_lst and inp1 not in fea_lst_used_name :\n            pattern_fea=""fea_name=""+inp1+""\\nfea_lst=(.*)\\nfea_opts=(.*)\\ncw_left=(.*)\\ncw_right=(.*)""\n            fea_lst_used.append((inp1+"",""+"","".join(list(re.findall(pattern_fea,fea_field)[0]))).split(\',\'))\n            fea_dict_used[inp1]=(inp1+"",""+"","".join(list(re.findall(pattern_fea,fea_field)[0]))).split(\',\')\n            \n            fea_lst_used_name.append(inp1)\n        if inp2 in fea_lst and inp2 not in fea_lst_used_name:\n            pattern_fea=""fea_name=""+inp2+""\\nfea_lst=(.*)\\nfea_opts=(.*)\\ncw_left=(.*)\\ncw_right=(.*)""\n            fea_lst_used.append((inp2+"",""+"","".join(list(re.findall(pattern_fea,fea_field)[0]))).split(\',\'))\n            fea_dict_used[inp2]=(inp2+"",""+"","".join(list(re.findall(pattern_fea,fea_field)[0]))).split(\',\')\n            \n            fea_lst_used_name.append(inp2)\n        if inp1 in lab_lst and inp1 not in lab_lst_used_name:\n            pattern_lab=""lab_name=""+inp1+""\\nlab_folder=(.*)\\nlab_opts=(.*)""\n            lab_lst_used.append((inp1+"",""+"","".join(list(re.findall(pattern_lab,lab_field)[0]))).split(\',\'))\n            lab_dict_used[inp1]=(inp1+"",""+"","".join(list(re.findall(pattern_lab,lab_field)[0]))).split(\',\')\n            \n            lab_lst_used_name.append(inp1)\n            \n        if inp2 in lab_lst and inp2 not in lab_lst_used_name:\n            pattern_lab=""lab_name=""+inp2+""\\nlab_folder=(.*)\\nlab_opts=(.*)""\n            lab_lst_used.append((inp2+"",""+"","".join(list(re.findall(pattern_lab,lab_field)[0]))).split(\',\'))\n            lab_dict_used[inp2]=(inp2+"",""+"","".join(list(re.findall(pattern_lab,lab_field)[0]))).split(\',\')\n            \n            lab_lst_used_name.append(inp2)\n            \n        if operation==\'compute\' and inp1 not in arch_lst_used_name:\n            arch_id=cfg_item2sec(config,\'arch_name\',inp1)\n            arch_seq_model=strtobool(config[arch_id][\'arch_seq_model\'])\n            arch_lst_used.append([arch_id,inp1,arch_seq_model])\n            arch_dict_used[inp1]=[arch_id,inp1,arch_seq_model]\n            \n            arch_lst_used_name.append(inp1)\n            \n            \n    # convert to unicode (for python 2)\n    for i in range(len(fea_lst_used)):\n        fea_lst_used[i]=list(map(str, fea_lst_used[i]))\n        \n    for i in range(len(lab_lst_used)):\n        lab_lst_used[i]=list(map(str, lab_lst_used[i])) \n        \n    for i in range(len(arch_lst_used)):\n        arch_lst_used[i]=list(map(str, arch_lst_used[i]))\n     \n    return [fea_lst_used,lab_lst_used,arch_lst_used]\n\n\n\ndef dict_fea_lab_arch(config, fea_only):\n    model=config[\'model\'][\'model\'].split(\'\\n\')\n    fea_lst=list(re.findall(\'fea_name=(.*)\\n\',config[\'data_chunk\'][\'fea\'].replace(\' \',\'\')))\n    lab_lst=list(re.findall(\'lab_name=(.*)\\n\',config[\'data_chunk\'][\'lab\'].replace(\' \',\'\')))\n\n    fea_lst_used=[]\n    lab_lst_used=[]\n    arch_lst_used=[]\n    \n    fea_dict_used={}\n    lab_dict_used={}\n    arch_dict_used={}\n    \n    fea_lst_used_name=[]\n    lab_lst_used_name=[]\n    arch_lst_used_name=[]\n    \n    fea_field=config[\'data_chunk\'][\'fea\']\n    lab_field=config[\'data_chunk\'][\'lab\']\n    \n    pattern=\'(.*)=(.*)\\((.*),(.*)\\)\'\n    \n    for line in model:\n        [out_name,operation,inp1,inp2]=list(re.findall(pattern,line)[0])\n        \n        if inp1 in fea_lst and inp1 not in fea_lst_used_name :\n            pattern_fea=""fea_name=""+inp1+""\\nfea_lst=(.*)\\nfea_opts=(.*)\\ncw_left=(.*)\\ncw_right=(.*)""\n            if sys.version_info[0]==2:\n                fea_lst_used.append((inp1+"",""+"","".join(list(re.findall(pattern_fea,fea_field)[0]))).encode(\'utf8\').split(\',\'))\n                fea_dict_used[inp1]=(inp1+"",""+"","".join(list(re.findall(pattern_fea,fea_field)[0]))).encode(\'utf8\').split(\',\')\n            else:\n                fea_lst_used.append((inp1+"",""+"","".join(list(re.findall(pattern_fea,fea_field)[0]))).split(\',\'))\n                fea_dict_used[inp1]=(inp1+"",""+"","".join(list(re.findall(pattern_fea,fea_field)[0]))).split(\',\')\n            \n            fea_lst_used_name.append(inp1)\n            \n            \n        if inp2 in fea_lst and inp2 not in fea_lst_used_name:\n            pattern_fea=""fea_name=""+inp2+""\\nfea_lst=(.*)\\nfea_opts=(.*)\\ncw_left=(.*)\\ncw_right=(.*)""\n            if sys.version_info[0]==2:\n                fea_lst_used.append((inp2+"",""+"","".join(list(re.findall(pattern_fea,fea_field)[0]))).encode(\'utf8\').split(\',\'))\n                fea_dict_used[inp2]=(inp2+"",""+"","".join(list(re.findall(pattern_fea,fea_field)[0]))).encode(\'utf8\').split(\',\')\n            else:\n                fea_lst_used.append((inp2+"",""+"","".join(list(re.findall(pattern_fea,fea_field)[0]))).split(\',\'))\n                fea_dict_used[inp2]=(inp2+"",""+"","".join(list(re.findall(pattern_fea,fea_field)[0]))).split(\',\')\n                \n            \n            fea_lst_used_name.append(inp2)\n        if inp1 in lab_lst and inp1 not in lab_lst_used_name and not fea_only:\n            pattern_lab=""lab_name=""+inp1+""\\nlab_folder=(.*)\\nlab_opts=(.*)""\n            \n            if sys.version_info[0]==2:\n                lab_lst_used.append((inp1+"",""+"","".join(list(re.findall(pattern_lab,lab_field)[0]))).encode(\'utf8\').split(\',\'))\n                lab_dict_used[inp1]=(inp1+"",""+"","".join(list(re.findall(pattern_lab,lab_field)[0]))).encode(\'utf8\').split(\',\')\n            else:\n                lab_lst_used.append((inp1+"",""+"","".join(list(re.findall(pattern_lab,lab_field)[0]))).split(\',\'))\n                lab_dict_used[inp1]=(inp1+"",""+"","".join(list(re.findall(pattern_lab,lab_field)[0]))).split(\',\')\n            \n            lab_lst_used_name.append(inp1)\n        \n        if inp2 in lab_lst and inp2 not in lab_lst_used_name and not fea_only:\n            # Testing production case (no labels)\n            pattern_lab=""lab_name=""+inp2+""\\nlab_folder=(.*)\\nlab_opts=(.*)""\n            if sys.version_info[0]==2:\n                lab_lst_used.append((inp2+"",""+"","".join(list(re.findall(pattern_lab,lab_field)[0]))).encode(\'utf8\').split(\',\'))\n                lab_dict_used[inp2]=(inp2+"",""+"","".join(list(re.findall(pattern_lab,lab_field)[0]))).encode(\'utf8\').split(\',\')\n            else:\n                lab_lst_used.append((inp2+"",""+"","".join(list(re.findall(pattern_lab,lab_field)[0]))).split(\',\'))\n                lab_dict_used[inp2]=(inp2+"",""+"","".join(list(re.findall(pattern_lab,lab_field)[0]))).split(\',\')              \n            \n            lab_lst_used_name.append(inp2)\n            \n        if operation==\'compute\' and inp1 not in arch_lst_used_name:\n            arch_id=cfg_item2sec(config,\'arch_name\',inp1)\n            arch_seq_model=strtobool(config[arch_id][\'arch_seq_model\'])\n            arch_lst_used.append([arch_id,inp1,arch_seq_model])\n            arch_dict_used[inp1]=[arch_id,inp1,arch_seq_model]\n            \n            arch_lst_used_name.append(inp1)\n            \n            \n    # convert to unicode (for python 2)\n    for i in range(len(fea_lst_used)):\n        fea_lst_used[i]=list(map(str, fea_lst_used[i]))\n        \n    for i in range(len(lab_lst_used)):\n        lab_lst_used[i]=list(map(str, lab_lst_used[i])) \n        \n    for i in range(len(arch_lst_used)):\n        arch_lst_used[i]=list(map(str, arch_lst_used[i]))\n     \n    return [fea_dict_used,lab_dict_used,arch_dict_used]\n\n\n\ndef is_sequential(config,arch_lst): # To cancel\n    seq_model=False\n    \n    for [arch_id,arch_name,arch_seq] in arch_lst:\n        if strtobool(config[arch_id][\'arch_seq_model\']):\n            seq_model=True\n            break\n    return seq_model\n\n\ndef is_sequential_dict(config,arch_dict):\n    seq_model=False\n    \n    for arch in arch_dict.keys():\n        arch_id=arch_dict[arch][0]\n        if strtobool(config[arch_id][\'arch_seq_model\']):\n            seq_model=True\n            break\n    return seq_model\n\n\ndef compute_cw_max(fea_dict):\n    cw_left_arr=[]\n    cw_right_arr=[]\n    \n    for fea in fea_dict.keys():\n        cw_left_arr.append(int(fea_dict[fea][3]))\n        cw_right_arr.append(int(fea_dict[fea][4]))\n    \n    cw_left_max=max(cw_left_arr)\n    cw_right_max=max(cw_right_arr)\n    \n    return [cw_left_max,cw_right_max]\n\n\ndef model_init(inp_out_dict,model,config,arch_dict,use_cuda,multi_gpu,to_do):\n    \n    pattern=\'(.*)=(.*)\\((.*),(.*)\\)\'\n     \n    nns={}\n    costs={}\n       \n    for line in model:\n        [out_name,operation,inp1,inp2]=list(re.findall(pattern,line)[0])\n        \n        if operation==\'compute\':\n            \n            # computing input dim\n            inp_dim=inp_out_dict[inp2][-1]\n            \n            # import the class\n            module = importlib.import_module(config[arch_dict[inp1][0]][\'arch_library\'])\n            nn_class=getattr(module, config[arch_dict[inp1][0]][\'arch_class\'])\n            \n            # add use cuda and todo options\n            config.set(arch_dict[inp1][0],\'use_cuda\',config[\'exp\'][\'use_cuda\'])\n            config.set(arch_dict[inp1][0],\'to_do\',config[\'exp\'][\'to_do\'])\n            \n            arch_freeze_flag=strtobool(config[arch_dict[inp1][0]][\'arch_freeze\'])\n\n            \n            # initialize the neural network\n            net=nn_class(config[arch_dict[inp1][0]],inp_dim)\n            \n            if use_cuda:\n                net.cuda()\n\n\n            if to_do==\'train\':\n                if not(arch_freeze_flag):\n                    net.train()\n                else:\n                   # Switch to eval modality if architecture is frozen (mainly for batch_norm/dropout functions)\n                   net.eval() \n            else:\n                net.eval()\n    \n            \n            # addigng nn into the nns dict\n            nns[arch_dict[inp1][1]]=net\n            \n            out_dim=net.out_dim\n                \n            # updating output dim\n            inp_out_dict[out_name]=[out_dim]\n            \n        if operation==\'concatenate\':\n            \n            inp_dim1=inp_out_dict[inp1][-1]\n            inp_dim2=inp_out_dict[inp2][-1]\n            \n            inp_out_dict[out_name]=[inp_dim1+inp_dim2]\n        \n        if operation==\'cost_nll\':\n            costs[out_name] = nn.NLLLoss()\n            inp_out_dict[out_name]=[1]\n            \n            \n        if operation==\'cost_err\':\n            inp_out_dict[out_name]=[1]\n            \n        if operation==\'mult\' or operation==\'sum\' or operation==\'mult_constant\' or operation==\'sum_constant\' or operation==\'avg\' or operation==\'mse\':\n            inp_out_dict[out_name]=inp_out_dict[inp1]    \n\n    return [nns,costs]\n\n\n\ndef optimizer_init(nns,config,arch_dict):\n    \n    # optimizer init\n    optimizers={}\n    for net in nns.keys():\n        \n        lr=float(config[arch_dict[net][0]][\'arch_lr\'])\n        \n        if config[arch_dict[net][0]][\'arch_opt\']==\'sgd\':\n            \n            opt_momentum=float(config[arch_dict[net][0]][\'opt_momentum\'])\n            opt_weight_decay=float(config[arch_dict[net][0]][\'opt_weight_decay\'])\n            opt_dampening=float(config[arch_dict[net][0]][\'opt_dampening\'])\n            opt_nesterov=strtobool(config[arch_dict[net][0]][\'opt_nesterov\'])\n            \n            \n            optimizers[net]=(optim.SGD(nns[net].parameters(),\n                      lr=lr,\n                      momentum=opt_momentum,\n                      weight_decay=opt_weight_decay,\n                      dampening=opt_dampening,\n                      nesterov=opt_nesterov))\n            \n    \n        if config[arch_dict[net][0]][\'arch_opt\']==\'adam\':\n            \n            opt_betas=list(map(float,(config[arch_dict[net][0]][\'opt_betas\'].split(\',\'))))\n            opt_eps=float(config[arch_dict[net][0]][\'opt_eps\'])\n            opt_weight_decay=float(config[arch_dict[net][0]][\'opt_weight_decay\'])\n            opt_amsgrad=strtobool(config[arch_dict[net][0]][\'opt_amsgrad\'])\n            \n            optimizers[net]=(optim.Adam(nns[net].parameters(),\n                      lr=lr,\n                      betas=opt_betas,\n                      eps=opt_eps,\n                      weight_decay=opt_weight_decay,\n                      amsgrad=opt_amsgrad))\n    \n            \n            \n            \n        if config[arch_dict[net][0]][\'arch_opt\']==\'rmsprop\':\n            \n            opt_momentum=float(config[arch_dict[net][0]][\'opt_momentum\'])\n            opt_alpha=float(config[arch_dict[net][0]][\'opt_alpha\'])\n            opt_eps=float(config[arch_dict[net][0]][\'opt_eps\'])\n            opt_centered=strtobool(config[arch_dict[net][0]][\'opt_centered\'])\n            opt_weight_decay=float(config[arch_dict[net][0]][\'opt_weight_decay\'])\n            \n            \n            optimizers[net]=(optim.RMSprop(nns[net].parameters(),\n                      lr=lr,\n                      momentum=opt_momentum,\n                      alpha=opt_alpha,\n                      eps=opt_eps,\n                      centered=opt_centered,\n                      weight_decay=opt_weight_decay))\n            \n    return optimizers\n\n\ndef forward_model_refac01(fea_dict, lab_dict, arch_dict, model, nns, costs, inp, ref, inp_out_dict, max_len_fea, max_len_lab, batch_size, to_do, forward_outs):\n    def _add_input_features_to_outs_dict(fea_dict, outs_dict, inp):\n        for fea in fea_dict.keys():\n            if len(inp.shape) == 3 and len(fea_dict[fea]) > 1:\n                outs_dict[fea] = inp[:,:,fea_dict[fea][5]:fea_dict[fea][6]]\n            if len(inp.shape) == 2 and len(fea_dict[fea]) > 1:\n                outs_dict[fea] = inp[:,fea_dict[fea][5]:fea_dict[fea][6]]\n        return outs_dict\n    def _compute_layer_values(inp_out_dict, inp2, inp, inp1, max_len, batch_size, arch_dict, out_name, nns, outs_dict, to_do):\n        def _is_input_feature(inp_out_dict, inp2):\n            if len(inp_out_dict[inp2]) > 1:\n                return True\n            return False\n        def _extract_respective_feature_from_input(inp, inp_out_dict, inp2, arch_dict, inp1, max_len, batch_size):\n            if len(inp.shape) ==3 :\n                inp_dnn = inp\n                if not(bool(arch_dict[inp1][2])):\n                    inp_dnn = inp_dnn.view(max_len*batch_size,-1)\n            if len(inp.shape) == 2:\n                inp_dnn = inp\n                if bool(arch_dict[inp1][2]):\n                    inp_dnn = inp_dnn.view(max_len,batch_size,-1)\n            return inp_dnn\n        \n        do_break = False\n        if _is_input_feature(inp_out_dict, inp2):\n            inp_dnn = _extract_respective_feature_from_input(inp, inp_out_dict, inp2, arch_dict, inp1, max_len, batch_size)\n            outs_dict[out_name] = nns[inp1](inp_dnn)\n        else:\n            if not(bool(arch_dict[inp1][2])) and len(outs_dict[inp2].shape) == 3:\n                outs_dict[inp2] = outs_dict[inp2].view(outs_dict[inp2].shape[0]*outs_dict[inp2].shape[1],-1)\n            if bool(arch_dict[inp1][2]) and len(outs_dict[inp2].shape) == 2:\n                # TODO: This computation needs to be made independent of max_len in case the network is performing sub sampling in time\n                outs_dict[inp2] = outs_dict[inp2].view(max_len,batch_size,-1)\n            outs_dict[out_name] = nns[inp1](outs_dict[inp2])\n        if to_do == \'forward\' and out_name == forward_outs[-1]:\n            do_break = True\n        return outs_dict, do_break\n    def _get_labels_from_input(ref, inp2, lab_dict):\n        if len(inp.shape)==3:\n            lab_dnn = ref \n        if len(inp.shape)==2:\n            lab_dnn = ref \n        lab_dnn=lab_dnn.view(-1).long()\n        return lab_dnn\n    def _get_network_output(outs_dict, inp1, max_len, batch_size):\n        out=outs_dict[inp1]\n        if len(out.shape) == 3:\n            out = out.view(max_len*batch_size, -1)\n        return out\n\n    outs_dict={}\n    _add_input_features_to_outs_dict(fea_dict, outs_dict, inp)\n    layer_string_pattern = \'(.*)=(.*)\\((.*),(.*)\\)\'\n    for line in model:\n        out_name, operation, inp1, inp2 = list(re.findall(layer_string_pattern,line)[0])\n        if operation == \'compute\':\n            outs_dict, do_break = _compute_layer_values(inp_out_dict, inp2, inp, inp1, max_len_fea, batch_size, arch_dict, out_name, nns, outs_dict, to_do)\n            if do_break:\n                break\n        elif operation == \'cost_nll\':\n            lab_dnn = _get_labels_from_input(ref, inp2, lab_dict)\n            out = _get_network_output(outs_dict, inp1, max_len_lab, batch_size)\n            if to_do != \'forward\':\n                outs_dict[out_name]=costs[out_name](out, lab_dnn)\n        elif operation == \'cost_err\':\n            lab_dnn = _get_labels_from_input(ref, inp2, lab_dict)\n            out = _get_network_output(outs_dict, inp1, max_len_lab, batch_size)\n            if to_do != \'forward\':\n                pred = torch.max(out,dim=1)[1] \n                err = torch.mean((pred!=lab_dnn).float())\n                outs_dict[out_name]=err\n        elif operation == \'concatenate\':\n            dim_conc = len(outs_dict[inp1].shape)-1\n            outs_dict[out_name] = torch.cat((outs_dict[inp1],outs_dict[inp2]),dim_conc) #check concat axis\n            if to_do == \'forward\' and out_name == forward_outs[-1]:\n                break\n        elif operation == \'mult\':\n            outs_dict[out_name] = outs_dict[inp1] * outs_dict[inp2]\n            if to_do == \'forward\' and out_name == forward_outs[-1]:\n                break\n        elif operation == \'sum\':\n            outs_dict[out_name] = outs_dict[inp1] + outs_dict[inp2] \n            if to_do == \'forward\' and out_name == forward_outs[-1]:\n                break\n        elif operation == \'mult_constant\':\n            outs_dict[out_name] = outs_dict[inp1] * float(inp2)\n            if to_do == \'forward\' and out_name == forward_outs[-1]:\n                break\n        elif operation == \'sum_constant\':\n            outs_dict[out_name] = outs_dict[inp1] + float(inp2)\n            if to_do == \'forward\' and out_name == forward_outs[-1]:\n                break\n        elif operation == \'avg\':\n            outs_dict[out_name] = (outs_dict[inp1] + outs_dict[inp2])/2\n            if to_do == \'forward\' and out_name == forward_outs[-1]:\n                break\n        elif operation == \'mse\':\n            outs_dict[out_name] = torch.mean((outs_dict[inp1] - outs_dict[inp2]) ** 2)\n            if to_do == \'forward\' and out_name == forward_outs[-1]:\n                break\n    return  outs_dict\n\n\ndef forward_model(fea_dict,lab_dict,arch_dict,model,nns,costs,inp,inp_out_dict,max_len,batch_size,to_do,forward_outs):\n    \n    # Forward Step\n    outs_dict={}\n    pattern=\'(.*)=(.*)\\((.*),(.*)\\)\'\n    \n    # adding input features to out_dict:\n    for fea in fea_dict.keys():\n        if len(inp.shape)==3 and len(fea_dict[fea])>1:\n            outs_dict[fea]=inp[:,:,fea_dict[fea][5]:fea_dict[fea][6]]\n\n        if len(inp.shape)==2 and len(fea_dict[fea])>1:\n            outs_dict[fea]=inp[:,fea_dict[fea][5]:fea_dict[fea][6]]\n\n    \n    \n    for line in model:\n        [out_name,operation,inp1,inp2]=list(re.findall(pattern,line)[0])\n\n        if operation==\'compute\':\n            \n            if len(inp_out_dict[inp2])>1: # if it is an input feature\n                \n                # Selection of the right feature in the inp tensor\n                if len(inp.shape)==3:\n                    inp_dnn=inp[:,:,inp_out_dict[inp2][-3]:inp_out_dict[inp2][-2]]\n                    if not(bool(arch_dict[inp1][2])):\n                        inp_dnn=inp_dnn.view(max_len*batch_size,-1)\n                                 \n                if len(inp.shape)==2:\n                    inp_dnn=inp[:,inp_out_dict[inp2][-3]:inp_out_dict[inp2][-2]]\n                    if bool(arch_dict[inp1][2]):\n                        inp_dnn=inp_dnn.view(max_len,batch_size,-1)\n                    \n                outs_dict[out_name]=nns[inp1](inp_dnn)\n\n                \n            else:\n                if not(bool(arch_dict[inp1][2])) and len(outs_dict[inp2].shape)==3:\n                    outs_dict[inp2]=outs_dict[inp2].view(max_len*batch_size,-1)\n                    \n                if bool(arch_dict[inp1][2]) and len(outs_dict[inp2].shape)==2:\n                    outs_dict[inp2]=outs_dict[inp2].view(max_len,batch_size,-1)\n                    \n                outs_dict[out_name]=nns[inp1](outs_dict[inp2])\n                \n            if to_do==\'forward\' and out_name==forward_outs[-1]:\n                break\n\n        \n        if operation==\'cost_nll\':\n            \n            # Put labels in the right format\n            if len(inp.shape)==3:\n                lab_dnn=inp[:,:,lab_dict[inp2][3]]\n            if len(inp.shape)==2:\n                lab_dnn=inp[:,lab_dict[inp2][3]]\n            \n            lab_dnn=lab_dnn.view(-1).long()\n            \n            # put output in the right format\n            out=outs_dict[inp1]\n\n            \n            if len(out.shape)==3:\n                out=out.view(max_len*batch_size,-1)\n            \n            if to_do!=\'forward\':\n                outs_dict[out_name]=costs[out_name](out, lab_dnn)\n            \n            \n        if operation==\'cost_err\':\n\n            if len(inp.shape)==3:\n                lab_dnn=inp[:,:,lab_dict[inp2][3]]\n            if len(inp.shape)==2:\n                lab_dnn=inp[:,lab_dict[inp2][3]]\n            \n            lab_dnn=lab_dnn.view(-1).long()\n            \n            # put output in the right format\n            out=outs_dict[inp1]\n            \n            if len(out.shape)==3:\n                out=out.view(max_len*batch_size,-1)\n            \n            if to_do!=\'forward\':\n                pred=torch.max(out,dim=1)[1] \n                err = torch.mean((pred!=lab_dnn).float())\n                outs_dict[out_name]=err\n                #print(err)\n\n        \n        if operation==\'concatenate\':\n           dim_conc=len(outs_dict[inp1].shape)-1\n           outs_dict[out_name]=torch.cat((outs_dict[inp1],outs_dict[inp2]),dim_conc) #check concat axis\n           if to_do==\'forward\' and out_name==forward_outs[-1]:\n                break\n            \n        if operation==\'mult\':\n            outs_dict[out_name]=outs_dict[inp1]*outs_dict[inp2]\n            if to_do==\'forward\' and out_name==forward_outs[-1]:\n                break\n \n        if operation==\'sum\':\n            outs_dict[out_name]=outs_dict[inp1]+outs_dict[inp2] \n            if to_do==\'forward\' and out_name==forward_outs[-1]:\n                break\n            \n        if operation==\'mult_constant\':\n            outs_dict[out_name]=outs_dict[inp1]*float(inp2)\n            if to_do==\'forward\' and out_name==forward_outs[-1]:\n                break\n            \n        if operation==\'sum_constant\':\n            outs_dict[out_name]=outs_dict[inp1]+float(inp2)\n            if to_do==\'forward\' and out_name==forward_outs[-1]:\n                break\n            \n        if operation==\'avg\':\n            outs_dict[out_name]=(outs_dict[inp1]+outs_dict[inp2])/2\n            if to_do==\'forward\' and out_name==forward_outs[-1]:\n                break\n            \n        if operation==\'mse\':\n            outs_dict[out_name]=torch.mean((outs_dict[inp1] - outs_dict[inp2]) ** 2)\n            if to_do==\'forward\' and out_name==forward_outs[-1]:\n                break\n\n\n            \n    return  outs_dict\n\ndef dump_epoch_results(res_file_path, ep, tr_data_lst, tr_loss_tot, tr_error_tot, tot_time, valid_data_lst, valid_peformance_dict, lr, N_ep):\n    \n    #\n    # Default terminal line size is 80 characters, try new dispositions to fit this limit\n    #\n\n    N_ep_str_format=\'0\'+str(max(math.ceil(np.log10(N_ep)),1))+\'d\'\n    res_file = open(res_file_path, ""a"")\n    res_file.write(\'ep=%s tr=%s loss=%s err=%s \' %(format(ep, N_ep_str_format),tr_data_lst,format(tr_loss_tot/len(tr_data_lst), ""0.3f""),format(tr_error_tot/len(tr_data_lst), ""0.3f"")))\n    print(\' \')\n    print(\'----- Summary epoch %s / %s\'%(format(ep, N_ep_str_format),format(N_ep-1, N_ep_str_format)))\n    print(\'Training on %s\' %(tr_data_lst))\n    print(\'Loss = %s | err = %s \'%(format(tr_loss_tot/len(tr_data_lst), ""0.3f""),format(tr_error_tot/len(tr_data_lst), ""0.3f"")))\n    print(\'-----\')\n    for valid_data in valid_data_lst:\n        res_file.write(\'valid=%s loss=%s err=%s \' %(valid_data,format(valid_peformance_dict[valid_data][0], ""0.3f""),format(valid_peformance_dict[valid_data][1], ""0.3f"")))\n        print(\'Validating on %s\' %(valid_data))\n        print(\'Loss = %s | err = %s \'%(format(valid_peformance_dict[valid_data][0], ""0.3f""),format(valid_peformance_dict[valid_data][1], ""0.3f"")))\n\n    print(\'-----\')\n    for lr_arch in lr.keys():\n        res_file.write(\'lr_%s=%s \' %(lr_arch,lr[lr_arch][ep]))\n        print(\'Learning rate on %s = %s \' %(lr_arch,lr[lr_arch][ep]))\n    print(\'-----\')\n    res_file.write(\'time(s)=%i\\n\' %(int(tot_time)))\n    print(\'Elapsed time (s) = %i\\n\' %(int(tot_time)))\n    print(\' \')\n    res_file.close()\n\ndef progress(count, total, status=\'\'):\n  bar_len = 40\n  filled_len = int(round(bar_len * count / float(total)))\n\n  percents = round(100.0 * count / float(total), 1)\n  bar = \'=\' * filled_len + \'-\' * (bar_len - filled_len)\n\n  if count == total-1:\n    sys.stdout.write(\'[%s] %s%s %s\\r\' % (bar, 100, \'%\', status))\n    sys.stdout.write(""\\n"")\n  else:\n    sys.stdout.write(\'[%s] %s%s %s\\r\' % (bar, percents, \'%\', status))\n    \n  sys.stdout.flush()  \n\n\ndef export_loss_acc_to_txt(out_folder, N_ep, val_lst):\n\n    if not os.path.exists(out_folder+\'/generated_outputs\') :\n        os.makedirs(out_folder+\'/generated_outputs\')\n\n    nb_val     = len(val_lst)\n    res        = open(out_folder+\'/res.res\', \'r\').readlines()\n\n    tr_loss   = []\n    tr_acc    = []\n    val_loss  = np.ndarray((nb_val,N_ep))\n    val_acc   = np.ndarray((nb_val,N_ep))\n\n    line_cpt = 0\n    for i in range(N_ep): \n        splitted = res[i].split(\' \')\n\n        # Getting uniq training loss and acc\n        tr_loss.append(float(splitted[2].split(\'=\')[1]))\n        tr_acc.append(1 - float(splitted[3].split(\'=\')[1]))\n\n        # Getting multiple or uniq val loss and acc\n        # +5 to avoird the 6 first columns of the res.res file\n        for i in range(nb_val):\n            val_loss[i][line_cpt] = float(splitted[(i*3)+5].split(\'=\')[1])\n            val_acc[i][line_cpt]  = 1 - float(splitted[(i*3)+6].split(\'=\')[1])\n\n        line_cpt += 1\n\n    # Saving to files\n    np.savetxt(out_folder+\'/generated_outputs/tr_loss.txt\', np.asarray(tr_loss), \'%0.3f\', delimiter=\',\')\n    np.savetxt(out_folder+\'/generated_outputs/tr_acc.txt\', np.asarray(tr_acc), \'%0.3f\', delimiter=\',\')\n\n    for i in range(nb_val):\n        np.savetxt(out_folder+\'/generated_outputs/val_\'+str(i)+\'_loss.txt\', val_loss[i], \'%0.5f\', delimiter=\',\')\n        np.savetxt(out_folder+\'/generated_outputs/val_\'+str(i)+\'_acc.txt\', val_acc[i], \'%0.5f\', delimiter=\',\')\n\n\ndef create_curves(out_folder, N_ep, val_lst):\n\n    try:\n        import matplotlib as mpl\n        mpl.use(\'Agg\')\n        import matplotlib.pyplot as plt\n\n    except ValueError:\n        print(\'WARNING:  matplotlib is not installed. The plots of the training curves have not been created.\')\n        sys.exit(0)\n      \n    print(\' \')\n    print(\'-----\')\n    print(\'Generating output files and plots ... \')\n    export_loss_acc_to_txt(out_folder, N_ep, val_lst)\n\n    if not os.path.exists(out_folder+\'/generated_outputs\') :\n        sys.stdacc.write(\'accOR: No results generated please call export_loss_err_to_txt() before\')\n        sys.exit(0)\n\n    nb_epoch = len(open(out_folder+\'/generated_outputs/tr_loss.txt\', \'r\').readlines())\n    x        = np.arange(nb_epoch)\n    nb_val   = len(val_lst)\n\n    # Loading train Loss and acc\n    tr_loss  = np.loadtxt(out_folder+\'/generated_outputs/tr_loss.txt\')\n    tr_acc   = np.loadtxt(out_folder+\'/generated_outputs/tr_acc.txt\')\n    \n    # Loading val loss and acc\n    val_loss = []\n    val_acc  = []\n    for i in range(nb_val):\n        val_loss.append(np.loadtxt(out_folder+\'/generated_outputs/val_\'+str(i)+\'_loss.txt\'))\n        val_acc.append(np.loadtxt(out_folder+\'/generated_outputs/val_\'+str(i)+\'_acc.txt\'))\n\n    #\n    # LOSS PLOT\n    #\n\n    # Getting maximum values\n    max_loss = np.amax(tr_loss)\n    for i in range(nb_val):\n        if np.amax(val_loss[i]) > max_loss:\n            max_loss = np.amax(val_loss[i])\n\n    # Plot train loss and acc\n    plt.plot(x, tr_loss, label=""train_loss"")\n\n    # Plot val loss and acc\n    for i in range(nb_val):\n        plt.plot(x, val_loss[i], label=\'val_\'+str(i)+\'_loss\')\n\n    plt.ylabel(\'Loss\')\n    plt.xlabel(\'Epoch\')\n    plt.title(\'Evolution of the loss function\')\n    plt.axis([0, nb_epoch-1, 0, max_loss+1])\n    plt.legend()\n    plt.savefig(out_folder+\'/generated_outputs/loss.png\')\n\n    # Clear plot\n    plt.gcf().clear()\n\n    #\n    # ACC PLOT\n    #\n\n    # Plot train loss and acc\n    plt.plot(x, tr_acc, label=""train_acc"")\n\n    # Plot val loss and acc\n    for i in range(nb_val):\n        plt.plot(x, val_acc[i], label=\'val_\'+str(i)+\'_acc\')\n\n    plt.ylabel(\'Accuracy\')\n    plt.xlabel(\'Epoch\')\n    plt.title(\'Evolution of the accuracy\')\n    plt.axis([0, nb_epoch-1, 0, 1])\n    plt.legend()\n    plt.savefig(out_folder+\'/generated_outputs/acc.png\')\n\n    print(\'OK\')\n\n# Replace the nth pattern in a string\ndef nth_replace_string(s, sub, repl, nth):\n    find = s.find(sub)\n\n    # if find is not p1 we have found at least one match for the substring\n    i = find != -1\n    # loop util we find the nth or we find no match\n    while find != -1 and i != nth:\n        # find + 1 means we start at the last match start index + 1\n        find = s.find(sub, find + 1)\n        i += 1\n    # if i  is equal to nth we found nth matches so replace\n    if i == nth:\n        return s[:find]+repl+s[find + len(sub):]\n    return s\n\n\ndef change_lr_cfg(cfg_file,lr,ep):\n    \n    config = configparser.ConfigParser()\n    config.read(cfg_file)\n    field=\'arch_lr\'\n    \n    for lr_arch in lr.keys():\n\n        config.set(lr_arch,field,str(lr[lr_arch][ep]))\n            \n    # Write cfg_file_chunk\n    with open(cfg_file, \'w\') as configfile:\n        config.write(configfile)\n    \ndef shift(arr, num, fill_value=np.nan):\n    if num >= 0:\n        return np.concatenate((np.full(num, fill_value), arr[:-num]))\n    else:\n        return np.concatenate((arr[-num:], np.full(-num, fill_value)))\n\n  \ndef expand_str_ep(str_compact,type_inp,N_ep,split_elem,mult_elem):\n    \n    lst_out=[]\n    \n    str_compact_lst=str_compact.split(split_elem)\n    \n    for elem in str_compact_lst:\n        elements=elem.split(mult_elem)\n\n        if type_inp==\'int\':\n            try: \n                int(elements[0])\n            except ValueError:\n              sys.stderr.write(\'The string ""%s"" must contain integers. Got %s.\\n\' %(str_compact,elements[0])) \n              sys.exit(0)\n              \n        if type_inp==\'float\':\n            try: \n                float(elements[0])\n            except ValueError:\n              sys.stderr.write(\'The string ""%s"" must contain floats. Got %s.\\n\' %(str_compact,elements[0])) \n              sys.exit(0)\n        \n        if len(elements)==2:\n            try: \n                int(elements[1])\n                lst_out.extend([elements[0] for i in range(int(elements[1]))])\n            except ValueError:\n              sys.stderr.write(\'The string ""%s"" must contain integers. Got %s\\n\' %(str_compact,elements[1])) \n              sys.exit(0)\n        \n        if len(elements)==1:\n            lst_out.append(elements[0])\n    \n    if len(str_compact_lst)==1 and len(elements)==1:\n        lst_out.extend([elements[0] for i in range(N_ep-1)])\n        \n      \n    # Final check\n    if len(lst_out)!=N_ep:\n       sys.stderr.write(\'The total number of elements specified in the string ""%s"" is equal to %i not equal to the total number of epochs %s.\\n\' %(str_compact,len(lst_out),N_ep)) \n       sys.exit(0)\n      \n    return lst_out\n    \n    \n'"
downstream_prep/get_voxforge_lid_data.py,0,"b'### from: https://github.com/PiSchool/spoken-language-id/blob/master/voxforge.py\n\n\nimport os\nimport re\nimport csv\nimport tarfile\nimport argparse\nfrom collections import Counter\nfrom collections import OrderedDict\n\nimport wget\nimport requests\n\n\ndef make_args():\n\tparser = argparse.ArgumentParser(description=""VoxForge dataset downloader."")\n\tparser.add_argument(\'--per-user\', default=1, type=int, help=""Limit the number of recordings per user"")\n\tparser.add_argument(\'--per-user-archives\', default=1, type=int, help=""Limit the number of archives per user"")\n\tparser.add_argument(\'-d\', \'--output-dir\', default=\'voxforge_samples\', help=""Directory to output wave files to"")\n\tparser.add_argument(\'-l\', \'--output-log\', default=\'voxforge_samples.csv\', help=""Metadata about downloaded files"")\n\treturn parser.parse_args()\n\n\nif __name__ == \'__main__\':\n\tbase_url = \'http://www.repository.voxforge1.org/downloads/{lang}/Trunk/Audio/Original/48kHz_16bit/{archive}\'\n\tlanguages = {\n\t\t\'Italian\': \'it\',\n\t\t\'French\': \'fr\',\n\t\t\'Portuguese\': \'pt\',\n\t\t\'German\': \'de\',\n\t\t\'English\': \'SpeechCorpus\',\n\t\t\'Spanish\': \'es\',\n\t\t\'Persian\': \'fa\',\n\t\t\'Russian\': \'ru\',\n\t\t\'Turkish\': \'tr\',\n\t\t\'Chinese\': \'zh\',\n\t}\n\n\targs = make_args()\n\tos.makedirs(args.output_dir, exist_ok=True)\n\n\tlog_file = open(args.output_log, \'w\')\n\tlog_csv = csv.writer(log_file, lineterminator=\'\\n\')\n\tfor lang_name, lang_code in languages.items():\n\t\tdownload_url = base_url.format(lang=lang_code, archive=\'\')\n\n\t\t# Download an HTML page with archive names\n\t\tprint(""Downloading archives for {}."".format(lang_name))\n\t\tresp = requests.get(download_url)\n\n\t\t# Generate a list of (archive_name, user_name) pairs\n\t\tarchives = re.findall(r\'((\\w+)-[\\w-]+\\.tgz)\', resp.text)\n\n\t\t# Remove duplicates, retain order\n\t\tarchives = OrderedDict.fromkeys(archives)\n\n\t\t# Pick a single archive from each user\n\t\tuser_archives = Counter()\n\t\tuser_recordings = Counter()\n\t\tfor archive, user in archives:\n\t\t\tif user_archives[user] >= args.per_user_archives:\n\t\t\t\t# We have enough archives of this user\n\t\t\t\tcontinue\n\n\t\t\tuser_archives[user] += 1\n\n\t\t\t# Download an archive\n\t\t\tdownload_url = base_url.format(lang=lang_code, archive=archive)\n\n\t\t\t# Remove the downloaded archive\n\t\t\tarchive_file = \'/tmp/foo.tgz\'\n\t\t\ttry:\n\t\t\t\tos.remove(archive_file)\n\t\t\texcept OSError:\n\t\t\t\tpass\n\n\t\t\t# Download the archive\n\t\t\twget.download(download_url, out=archive_file)\n\t\t\tprint()\n\n\t\t\tper_archive_count = 0\n\t\t\twith tarfile.open(archive_file, errorlevel=2) as tar:\n\t\t\t\tfor member in tar.getmembers():\n\t\t\t\t\t# Find a member that is a wave file, e.g. archive-name/wav/it-0123.wav\n\t\t\t\t\twav_filename = re.match(r\'([\\w-]+)/.+/([\\w-]+\\.wav)\', member.name)\n\t\t\t\t\tif wav_filename is not None:\n\t\t\t\t\t\tif user_recordings[user] >= args.per_user:\n\t\t\t\t\t\t\t# We have enough files from this user\n\t\t\t\t\t\t\tbreak\n\n\t\t\t\t\t\tuser_recordings[user] += 1\n\t\t\t\t\t\tper_archive_count += 1\n\t\t\t\t\t\tarchive_name = wav_filename.group(1)\n\n\t\t\t\t\t\t# Rename conveniently\n\t\t\t\t\t\tmember.name = \'{archive}-{wav}\'.format(archive=archive_name, wav=wav_filename.group(2))\n\n\t\t\t\t\t\t# Skip if exists\n\t\t\t\t\t\tif os.path.isfile(os.path.join(args.output_dir, member.name)):\n\t\t\t\t\t\t\tcontinue\n\n\t\t\t\t\t\ttar.extract(member, path=args.output_dir)\n\t\t\t\t\t\tlog_csv.writerow([member.name, lang_name, user, user_recordings[user]])\n\t\t\t\t\t\tlog_file.flush()\n\t\t\tprint(""Extracted {} files from {}"".format(per_archive_count, archive))\n\n\t\tprint(""Recordings by {} users."".format(len(user_archives)))\n\tlog_file.close()\n'"
downstream_prep/prep_voxceleb.py,0,"b'import argparse\nimport os\nimport glob\nimport numpy as np\nimport librosa\n\ndef prep_rec(input_rec_path, out_rec_path, sr=16000, out_length_seconds=10):\n\n\ttry:\n\n\t\ty, s = librosa.load(input_rec_path, sr=sr)\n\n\t\tn_samples = sr*out_length_seconds\n\n\t\ttry:\n\t\t\tridx = np.random.randint(0, len(y)-n_samples)\n\t\t\tlibrosa.output.write_wav(out_rec_path, y[ridx:(ridx+n_samples)], sr=sr)\n\n\t\t\ty = y[ridx:(ridx+n_samples)]\n\n\t\texcept ValueError:\n\n\t\t\tmul = int(np.ceil(n_samples/len(y)))\n\t\t\ty = np.tile(y, (mul))[:n_samples]\n\n\t\tlibrosa.output.write_wav(out_rec_path, y, sr=sr)\n\n\t\treturn True\n\n\texcept:\n\n\t\treturn False\n\ndef clean_dir(dir_path):\n\n\tfile_list = glob.glob(dir_path+\'/*.*\')\n\n\tif len(file_list)>0:\n\t\tfor file_ in file_list:\n\t\t\tos.remove(file_)\n\ndef dump_list(list_, path_):\n\n\twith open(path_, \'w\') as f:\n\t\tfor el in list_:\n\t\t\titem = el + \'\\n\'\n\t\t\tf.write(""%s"" % item)\n\nif __name__ == \'__main__\':\n\n\tparser = argparse.ArgumentParser(description=\'Prep vox.\')\n\tparser.add_argument(\'--path-to-data\', type=str, default=\'./data/\')\n\tparser.add_argument(\'--out-path\', type=str, default=\'./\')\n\tparser.add_argument(\'--out-sr\', type=int, default=16000)\n\tparser.add_argument(\'--out-length\', type=int, default=10)\n\tparser.add_argument(\'--nspk\', type=int, default=100)\n\tparser.add_argument(\'--ntrials\', type=int, default=10)\n\targs = parser.parse_args()\n\n\tif not os.path.isdir(args.out_path+\'train\'):\n\t\tos.mkdir(args.out_path+\'train\')\n\n\tif not os.path.isdir(args.out_path+\'test\'):\n\t\tos.mkdir(args.out_path+\'test\')\n\n\tspk_list = np.random.choice(os.listdir(args.path_to_data), args.nspk, replace=False)\n\n\ttrain_list, test_list = [], []\n\tutt2spk = {}\n\n\tfor i, spk in enumerate(spk_list):\n\n\t\tprint(\'Speaker: {}\'.format(spk))\n\n\t\tfolder_list = os.listdir(args.path_to_data + spk + \'/\')\n\t\trec_list = []\n\n\t\tfor folder in folder_list:\n\t\t\tfolder_recs = os.listdir(args.path_to_data + spk + \'/\' + folder + \'/\')\n\n\t\t\tfor rec in folder_recs:\n\t\t\t\trec_list.append(args.path_to_data + spk + \'/\' + folder + \'/\' + rec)\n\n\t\ttrain_rec, test_rec = np.random.choice(rec_list, 2, replace=False)\n\n\t\ttrials=0\n\t\tsuccess=False\n\t\twhile not success and trials<args.ntrials:\n\n\t\t\tclean_dir(args.path_to_data+\'train\')\n\t\t\tclean_dir(args.path_to_data+\'test\')\n\n\t\t\ttrain_rec, test_rec = np.random.choice(rec_list, 2, replace=False)\n\n\t\t\ttrain_utt = train_rec.split(\'/\')[-1]\n\t\t\ttrain_folder = train_rec.split(\'/\')[-2]\n\t\t\ttest_utt = test_rec.split(\'/\')[-1]\n\t\t\ttest_folder = test_rec.split(\'/\')[-2]\n\n\t\t\tsuccess = prep_rec(train_rec, args.out_path+\'train/\'+spk+\'_-_\'+train_folder+\'_-_\'+train_utt, sr=args.out_sr, out_length_seconds=args.out_length) and prep_rec(test_rec, args.out_path+\'test/\'+spk+\'_-_\'+test_folder+\'_-_\'+test_utt, sr=args.out_sr, out_length_seconds=args.out_length)\n\n\t\t\ttrials+=1\n\n\t\tif trials>=args.ntrials:\n\t\t\tprint(\'Failed!!\')\n\t\t\texit(1)\n\n\t\ttrain_list.append(spk+\'_-_\'+train_folder+\'_-_\'+train_utt)\n\t\ttest_list.append(spk+\'_-_\'+test_folder+\'_-_\'+test_utt)\n\t\tutt2spk[train_list[-1]] = i\n\t\tutt2spk[test_list[-1]] = i\n\n\tif not os.path.isdir(args.out_path+\'lists\'):\n\t\tos.mkdir(args.out_path+\'lists\')\n\n\tprint(\'Any overlap between train and test lists: {}\'.format(bool(set(train_list) & set(test_list))))\n\n\tdump_list(train_list, args.out_path+\'lists/train_list\')\n\tdump_list(test_list, args.out_path+\'lists/test_list\')\n\tnp.save(args.out_path+\'lists/utt2spk\', utt2spk)\n'"
downstream_prep/prep_voxforge.py,0,"b'### Copied VAD stuff from https://github.com/idnavid/py_vad_tool\n\nimport argparse\nimport os\nimport glob\nimport numpy as np\nimport librosa\nimport pandas as pd\nimport numpy as np\nfrom scipy.io import wavfile\n\ndef add_wgn(s,var=1e-4):\n\tnp.random.seed(0)\n\tnoise = np.random.normal(0,var,len(s))\n\treturn s + noise\n\ndef enframe(x, win_len, hop_len):\n\n\tx = np.squeeze(x)\n\tif x.ndim != 1:\n\t\traise TypeError(""enframe input must be a 1-dimensional array."")\n\tn_frames = 1 + np.int(np.floor((len(x) - win_len) / float(hop_len)))\n\tx_framed = np.zeros((n_frames, win_len))\n\tfor i in range(n_frames):\n\t\tx_framed[i] = x[i * hop_len : i * hop_len + win_len]\n\treturn x_framed\n\ndef deframe(x_framed, win_len, hop_len):\n\tn_frames = len(x_framed)\n\tn_samples = n_frames*hop_len + win_len\n\tx_samples = np.zeros((n_samples,1))\n\tfor i in range(n_frames):\n\t\tx_samples[i*hop_len : i*hop_len + win_len] = x_framed[i]\n\treturn x_samples\n\ndef zero_mean(xframes):\n\tm = np.mean(xframes,axis=1)\n\txframes = xframes - np.tile(m,(xframes.shape[1],1)).T\n\treturn xframes\n\ndef compute_nrg(xframes):\n\tn_frames = xframes.shape[1]\n\treturn np.diagonal(np.dot(xframes,xframes.T))/float(n_frames)\n\ndef compute_log_nrg(xframes):\n\tn_frames = xframes.shape[1]\n\traw_nrgs = np.log(compute_nrg(xframes+1e-5))/float(n_frames)\n\treturn (raw_nrgs - np.mean(raw_nrgs))/(np.sqrt(np.var(raw_nrgs)))\n\ndef power_spectrum(xframes):\n\tX = np.fft.fft(xframes,axis=1)\n\tX = np.abs(X[:,:X.shape[1]/2])**2\n\treturn np.sqrt(X)\n\ndef nrg_vad(xframes,percent_thr,nrg_thr=0.,context=5):\n\txframes = zero_mean(xframes)\n\tn_frames = xframes.shape[1]\n\t\n\t# Compute per frame energies:\n\txnrgs = compute_log_nrg(xframes)\n\txvad = np.zeros((n_frames,1))\n\tfor i in range(n_frames):\n\t\tstart = max(i-context,0)\n\t\tend = min(i+context,n_frames-1)\n\t\tn_above_thr = np.sum(xnrgs[start:end]>nrg_thr)\n\t\tn_total = end-start+1\n\t\txvad[i] = 1.*((float(n_above_thr)/n_total) > percent_thr)\n\treturn xvad\n\ndef prep_rec(input_rec_path, out_rec_path, sr=16000, out_length_seconds=10, vad=False):\n\n\ttry:\n\n\t\ty, s = librosa.load(input_rec_path, sr=sr)\n\t\tassert len(y)>s*2\n\n\texcept:\n\n\t\tprint(\'skipping recording {}\'.format(input_rec_path))\n\t\treturn\n\n\tn_samples = sr*out_length_seconds\n\n\tif vad:\n\t\twin_len = int(s*0.025)\n\t\thop_len = int(s*0.010)\n\t\tsframes = enframe(y,win_len,hop_len)\n\t\tpercent_high_nrg = 0.5\n\t\tvad = nrg_vad(sframes,percent_high_nrg)\n\t\tvad = deframe(vad,win_len,hop_len)[:len(y)].squeeze()\n\t\ty = y[np.where(vad==1)]\n\n\ttry:\n\t\tridx = np.random.randint(0, len(y)-n_samples)\n\t\tlibrosa.output.write_wav(out_rec_path, y[ridx:(ridx+n_samples)], sr=sr)\n\n\t\ty = y[ridx:(ridx+n_samples)]\n\n\texcept ValueError:\n\n\t\ttry:\n\t\t\tmul = int(np.ceil(n_samples/len(y)))\n\t\t\ty = np.tile(y, (mul))[:n_samples]\n\t\texcept ZeroDivisionError:\n\t\t\tprint(\'skipping recording {}\'.format(input_rec_path))\n\t\t\treturn\n\n\tlibrosa.output.write_wav(out_rec_path, y, sr=sr)\n\ndef dump_list(list_, path_):\n\n\twith open(path_, \'w\') as f:\n\t\tfor el in list_:\n\t\t\titem = el + \'\\n\'\n\t\t\tf.write(""%s"" % item)\n\ndef create_list(data_):\n\n\tlang2rec = {}\n\n\tfor line in data_:\n\n\t\ttry:\n\t\t\tlang2rec[line[1]].append(line[0])\n\t\texcept KeyError:\n\t\t\tlang2rec[line[1]]=[line[0]]\n\treturn lang2rec\n\nif __name__ == \'__main__\':\n\n\tparser = argparse.ArgumentParser(description=\'Prep voxforge.\')\n\tparser.add_argument(\'--path-to-data\', type=str, default=\'./data/\')\n\tparser.add_argument(\'--path-to-metadata\', type=str, default=\'./data/voxforge.csv\')\n\tparser.add_argument(\'--out-path\', type=str, default=\'./\')\n\tparser.add_argument(\'--out-sr\', type=int, default=16000)\n\tparser.add_argument(\'--out-length\', type=int, default=10)\n\tparser.add_argument(\'--nrecs\', type=int, default=30)\n\tparser.add_argument(\'--vad\', action=\'store_true\', default=False, help=\'Enables vad\')\n\tparser.add_argument(\'--traintest\', action=\'store_true\', default=False, help=\'Enables train test split\')\n\targs = parser.parse_args()\n\n\tif args.traintest:\n\n\t\tif not os.path.isdir(args.out_path+\'train\'):\n\t\t\tos.mkdir(args.out_path+\'train\')\n\n\t\tif not os.path.isdir(args.out_path+\'test\'):\n\t\t\tos.mkdir(args.out_path+\'test\')\n\n\tmeta_data = pd.read_csv(args.path_to_metadata, sep=\',\' , header=None).values\n\n\tlang2utt = create_list(meta_data)\n\n\ttrain_list, test_list = [], []\n\tutt2lang = {}\n\n\tfor i, lang in enumerate(lang2utt):\n\n\t\tprint(\'Language: {}\'.format(lang))\n\n\t\trec_list = lang2utt[lang]\n\n\t\tassert len(rec_list)>1, ""Not enough recordings for language {}"".format(lang)\n\n\t\tif args.traintest:\n\n\t\t\trec_list = np.random.choice(rec_list, min(args.nrecs, len(rec_list)), replace=False)\n\n\t\t\tmid_idx=len(rec_list)//3\n\t\t\ttrain_rec, test_rec = rec_list[mid_idx:], rec_list[:mid_idx]\n\n\t\t\tfor rec in train_rec:\n\t\t\t\tprep_rec(args.path_to_data+rec, args.out_path+\'train/\'+lang+\'_-_\'+rec, sr=args.out_sr, out_length_seconds=args.out_length, vad=args.vad)\n\t\t\t\ttrain_list.append(lang+\'_-_\'+rec)\n\t\t\t\tutt2lang[lang+\'_-_\'+rec]=i\n\n\t\t\tfor rec in test_rec:\n\t\t\t\tprep_rec(args.path_to_data+rec, args.out_path+\'test/\'+lang+\'_-_\'+rec, sr=args.out_sr, out_length_seconds=args.out_length, vad=args.vad)\n\t\t\t\ttest_list.append(lang+\'_-_\'+rec)\n\t\t\t\tutt2lang[lang+\'_-_\'+rec]=i\n\n\t\telse:\n\n\t\t\tfor rec in rec_list:\n\t\t\t\tprep_rec(args.path_to_data+rec, args.out_path+lang+\'_-_\'+rec, sr=args.out_sr, out_length_seconds=args.out_length, vad=args.vad)\n\n\tif args.traintest:\n\n\t\tif not os.path.isdir(args.out_path+\'lists\'):\n\t\t\tos.mkdir(args.out_path+\'lists\')\n\n\t\tdump_list(train_list, args.out_path+\'lists/train_list\')\n\t\tdump_list(test_list, args.out_path+\'lists/test_list\')\n\t\tnp.save(args.out_path+\'lists/utt2lang\', utt2lang)\n'"
emorec/arff2npy.py,0,"b""import arff\nimport numpy as np\nfrom ahoproc_tools.interpolate import interpolation\nimport tqdm\nimport argparse\nimport pickle\nimport os\n\n\ndef main(opts):\n    for ai, afile in tqdm.tqdm(enumerate(opts.arff_files), total=len(opts.arff_files)):\n        with open(afile) as af:\n            data = arff.load(af)\n            attrs = [at[0] for at in data['attributes']]\n            f0_idx = attrs.index('F0_sma')\n            data = data['data']\n            array = []\n            X = []\n            for dpoint in data:\n                # ignore name, timestamp and class\n                f0_val = dpoint[f0_idx]\n                if f0_val > 0:\n                    dpoint[f0_idx] = np.log(f0_val)\n                else:\n                    dpoint[f0_idx] = -1e10\n                array.append(dpoint[2:-1])\n            array = np.array(array, dtype=np.float32)\n            lf0, _ = interpolation(array[:, -1], -1e10)\n            array[:, -1] = lf0\n            if opts.out_stats is not None:\n                X.append(array)\n            npfile = os.path.splitext(afile)[0]\n            np.save(os.path.join(npfile), array.T)\n    if opts.out_stats is not None:\n        X = np.concatenate(X, axis=0)\n        mn = np.mean(X, axis=0)\n        sd = np.std(X, axis=0)\n        with open(opts.out_stats, 'wb') as out_f:\n            pickle.dump({'mean':mn, 'std':sd}, out_f)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--arff_root', type=str,\n                        default='data/IEMOCAP_ahsn_leave-two-speaker-out_LLD')\n    parser.add_argument('--arff_files', type=str, default=None, nargs='+')\n    parser.add_argument('--out_stats', type=str, default=None)\n\n    opts = parser.parse_args()\n    main(opts)\n\n"""
emorec/neural_networks.py,92,"b'##########################################################\n# pytorch-kaldi v.0.1                                      \n# Mirco Ravanelli, Titouan Parcollet\n# Mila, University of Montreal\n# October 2018\n##########################################################\n\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport numpy as np\nfrom distutils.util import strtobool\nimport math\n\n# uncomment below if you want to use SRU\n# and you need to install SRU: pip install sru[cuda].\n# or you can install it from source code: https://github.com/taolei87/sru.\n# import sru\n\ndef context_window(fea,left,right):\n \n    N_elem=fea.shape[0]\n    N_fea=fea.shape[1]\n    \n    fea_conc=np.empty([N_elem,N_fea*(left+right+1)])\n    \n    index_fea=0\n    for lag in range(-left,right+1):\n        fea_conc[:,index_fea:index_fea+fea.shape[1]]=np.roll(fea,lag,axis=0)\n        index_fea=index_fea+fea.shape[1]\n        \n    fea_conc=fea_conc[left:fea_conc.shape[0]-right]\n    \n    return fea_conc\n\nclass LayerNorm(nn.Module):\n\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm,self).__init__()\n        self.gamma = nn.Parameter(torch.ones(features))\n        self.beta = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\n\ndef act_fun(act_type):\n\n if act_type==""relu"":\n    return nn.ReLU()\n            \n if act_type==""tanh"":\n    return nn.Tanh()\n            \n if act_type==""sigmoid"":\n    return nn.Sigmoid()\n           \n if act_type==""leaky_relu"":\n    return nn.LeakyReLU(0.2)\n            \n if act_type==""elu"":\n    return nn.ELU()\n                     \n if act_type==""softmax"":\n    return nn.LogSoftmax(dim=1)\n        \n if act_type==""linear"":\n     return nn.LeakyReLU(1) # initializzed like this, but not used in forward!\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, options,inp_dim):\n        super(MLP, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.dnn_lay=list(map(int, options[\'dnn_lay\'].split(\',\')))\n        self.dnn_drop=list(map(float, options[\'dnn_drop\'].split(\',\'))) \n        self.dnn_use_batchnorm=list(map(strtobool, options[\'dnn_use_batchnorm\'].split(\',\')))\n        self.dnn_use_laynorm=list(map(strtobool, options[\'dnn_use_laynorm\'].split(\',\'))) \n        self.dnn_use_laynorm_inp=strtobool(options[\'dnn_use_laynorm_inp\'])\n        self.dnn_use_batchnorm_inp=strtobool(options[\'dnn_use_batchnorm_inp\'])\n        self.dnn_act=options[\'dnn_act\'].split(\',\')\n        \n       \n        self.wx  = nn.ModuleList([])\n        self.bn  = nn.ModuleList([])\n        self.ln  = nn.ModuleList([])\n        self.act = nn.ModuleList([])\n        self.drop = nn.ModuleList([])\n       \n  \n        # input layer normalization\n        if self.dnn_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # input batch normalization    \n        if self.dnn_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n           \n        self.N_dnn_lay=len(self.dnn_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_dnn_lay):\n            \n             # dropout\n             self.drop.append(nn.Dropout(p=self.dnn_drop[i]))\n             \n             # activation\n             self.act.append(act_fun(self.dnn_act[i]))\n             \n             \n             add_bias=True\n             \n             # layer norm initialization\n             self.ln.append(LayerNorm(self.dnn_lay[i]))\n             self.bn.append(nn.BatchNorm1d(self.dnn_lay[i],momentum=0.05))\n             \n             if self.dnn_use_laynorm[i] or self.dnn_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Linear operations\n             self.wx.append(nn.Linear(current_input, self.dnn_lay[i],bias=add_bias))\n             \n             # weight initialization\n             self.wx[i].weight = torch.nn.Parameter(torch.Tensor(self.dnn_lay[i],current_input).uniform_(-np.sqrt(0.01/(current_input+self.dnn_lay[i])),np.sqrt(0.01/(current_input+self.dnn_lay[i]))))\n             self.wx[i].bias = torch.nn.Parameter(torch.zeros(self.dnn_lay[i]))\n             \n             current_input=self.dnn_lay[i]\n             \n        self.out_dim=current_input\n         \n    def forward(self, x):\n        \n      # Applying Layer/Batch Norm\n      if bool(self.dnn_use_laynorm_inp):\n        x=self.ln0((x))\n        \n      if bool(self.dnn_use_batchnorm_inp):\n\n        x=self.bn0((x))\n        \n      for i in range(self.N_dnn_lay):\n           \n          if self.dnn_use_laynorm[i] and not(self.dnn_use_batchnorm[i]):\n           x = self.drop[i](self.act[i](self.ln[i](self.wx[i](x))))\n          \n          if self.dnn_use_batchnorm[i] and not(self.dnn_use_laynorm[i]):\n           x = self.drop[i](self.act[i](self.bn[i](self.wx[i](x))))\n           \n          if self.dnn_use_batchnorm[i]==True and self.dnn_use_laynorm[i]==True:\n           x = self.drop[i](self.act[i](self.bn[i](self.ln[i](self.wx[i](x)))))\n          \n          if self.dnn_use_batchnorm[i]==False and self.dnn_use_laynorm[i]==False:\n           x = self.drop[i](self.act[i](self.wx[i](x)))\n            \n          \n      return x\n\n\nclass LSTM_cudnn(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(LSTM_cudnn, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.hidden_size=int(options[\'hidden_size\'])\n        self.num_layers=int(options[\'num_layers\'])\n        self.bias=bool(strtobool(options[\'bias\']))\n        self.batch_first=bool(strtobool(options[\'batch_first\']))\n        self.dropout=float(options[\'dropout\'])\n        self.bidirectional=bool(strtobool(options[\'bidirectional\']))\n        \n        self.lstm = nn.ModuleList([nn.LSTM(self.input_dim, self.hidden_size, self.num_layers, \n                            bias=self.bias,dropout=self.dropout,bidirectional=self.bidirectional)])\n         \n        self.out_dim=self.hidden_size+self.bidirectional*self.hidden_size\n               \n        \n    def forward(self, x):\n        \n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n            c0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            c0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            \n        if x.is_cuda:\n            h0=h0.cuda()\n            c0=c0.cuda()\n            \n        output, (hn, cn) = self.lstm[0](x, (h0, c0))\n        \n        \n        return output\n    \n\nclass GRU_cudnn(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(GRU_cudnn, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.hidden_size=int(options[\'hidden_size\'])\n        self.num_layers=int(options[\'num_layers\'])\n        self.bias=bool(strtobool(options[\'bias\']))\n        self.batch_first=bool(strtobool(options[\'batch_first\']))\n        self.dropout=float(options[\'dropout\'])\n        self.bidirectional=bool(strtobool(options[\'bidirectional\']))\n        \n        self.gru = nn.ModuleList([nn.GRU(self.input_dim, self.hidden_size, self.num_layers, \n                            bias=self.bias,dropout=self.dropout,bidirectional=self.bidirectional)])\n         \n        self.out_dim=self.hidden_size+self.bidirectional*self.hidden_size\n               \n        \n    def forward(self, x):\n        \n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            \n        if x.is_cuda:\n            h0=h0.cuda()\n\n        output, hn = self.gru[0](x, h0)\n        \n        \n        return output\n \n    \nclass RNN_cudnn(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(RNN_cudnn, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.hidden_size=int(options[\'hidden_size\'])\n        self.num_layers=int(options[\'num_layers\'])\n        self.nonlinearity=options[\'nonlinearity\']\n        self.bias=bool(strtobool(options[\'bias\']))\n        self.batch_first=bool(strtobool(options[\'batch_first\']))\n        self.dropout=float(options[\'dropout\'])\n        self.bidirectional=bool(strtobool(options[\'bidirectional\']))\n        \n        self.rnn = nn.ModuleList([nn.RNN(self.input_dim, self.hidden_size, self.num_layers, \n                            nonlinearity=self.nonlinearity,bias=self.bias,dropout=self.dropout,bidirectional=self.bidirectional)])\n         \n        self.out_dim=self.hidden_size+self.bidirectional*self.hidden_size\n               \n        \n    def forward(self, x):\n        \n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            \n        if x.is_cuda:\n            h0=h0.cuda()\n\n        output, hn = self.rnn[0](x, h0)\n        \n        \n        return output\n    \n    \nclass LSTM(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(LSTM, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.lstm_lay=list(map(int, options[\'lstm_lay\'].split(\',\')))\n        self.lstm_drop=list(map(float, options[\'lstm_drop\'].split(\',\'))) \n        self.lstm_use_batchnorm=list(map(strtobool, options[\'lstm_use_batchnorm\'].split(\',\')))\n        self.lstm_use_laynorm=list(map(strtobool, options[\'lstm_use_laynorm\'].split(\',\'))) \n        self.lstm_use_laynorm_inp=strtobool(options[\'lstm_use_laynorm_inp\'])\n        self.lstm_use_batchnorm_inp=strtobool(options[\'lstm_use_batchnorm_inp\'])\n        self.lstm_act=options[\'lstm_act\'].split(\',\')\n        self.lstm_orthinit=strtobool(options[\'lstm_orthinit\'])\n\n        self.bidir=strtobool(options[\'lstm_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wfx  = nn.ModuleList([]) # Forget\n        self.ufh  = nn.ModuleList([]) # Forget\n        \n        self.wix  = nn.ModuleList([]) # Input\n        self.uih  = nn.ModuleList([]) # Input  \n        \n        self.wox  = nn.ModuleList([]) # Output\n        self.uoh  = nn.ModuleList([]) # Output  \n        \n        self.wcx  = nn.ModuleList([]) # Cell state\n        self.uch = nn.ModuleList([])  # Cell state\n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wfx  = nn.ModuleList([]) # Batch Norm\n        self.bn_wix  = nn.ModuleList([]) # Batch Norm\n        self.bn_wox  = nn.ModuleList([]) # Batch Norm\n        self.bn_wcx = nn.ModuleList([]) # Batch Norm\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.lstm_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.lstm_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_lstm_lay=len(self.lstm_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_lstm_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.lstm_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.lstm_use_laynorm[i] or self.lstm_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wfx.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n             self.wix.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n             self.wox.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n             self.wcx.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n            \n             # Recurrent connections\n             self.ufh.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             self.uih.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             self.uoh.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             self.uch.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             \n             if self.lstm_orthinit:\n                nn.init.orthogonal_(self.ufh[i].weight)\n                nn.init.orthogonal_(self.uih[i].weight)\n                nn.init.orthogonal_(self.uoh[i].weight)\n                nn.init.orthogonal_(self.uch[i].weight)\n            \n             \n             # batch norm initialization\n             self.bn_wfx.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n             self.bn_wix.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n             self.bn_wox.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n             self.bn_wcx.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n                \n             self.ln.append(LayerNorm(self.lstm_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.lstm_lay[i]\n             else:\n                 current_input=self.lstm_lay[i]\n                 \n        self.out_dim=self.lstm_lay[i]+self.bidir*self.lstm_lay[i]\n            \n             \n        \n    def forward(self, x):\n\n        # Applying Layer/Batch Norm\n        if bool(self.lstm_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.lstm_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_lstm_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.lstm_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.lstm_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.lstm_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.lstm_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wfx_out=self.wfx[i](x)\n            wix_out=self.wix[i](x)\n            wox_out=self.wox[i](x)\n            wcx_out=self.wcx[i](x)\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.lstm_use_batchnorm[i]:\n\n                wfx_out_bn=self.bn_wfx[i](wfx_out.view(wfx_out.shape[0]*wfx_out.shape[1],wfx_out.shape[2]))\n                wfx_out=wfx_out_bn.view(wfx_out.shape[0],wfx_out.shape[1],wfx_out.shape[2])\n         \n                wix_out_bn=self.bn_wix[i](wix_out.view(wix_out.shape[0]*wix_out.shape[1],wix_out.shape[2]))\n                wix_out=wix_out_bn.view(wix_out.shape[0],wix_out.shape[1],wix_out.shape[2])\n   \n                wox_out_bn=self.bn_wox[i](wox_out.view(wox_out.shape[0]*wox_out.shape[1],wox_out.shape[2]))\n                wox_out=wox_out_bn.view(wox_out.shape[0],wox_out.shape[1],wox_out.shape[2])\n\n                wcx_out_bn=self.bn_wcx[i](wcx_out.view(wcx_out.shape[0]*wcx_out.shape[1],wcx_out.shape[2]))\n                wcx_out=wcx_out_bn.view(wcx_out.shape[0],wcx_out.shape[1],wcx_out.shape[2]) \n            \n            \n            # Processing time steps\n            hiddens = []\n            ct=h_init\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # LSTM equations\n                ft=torch.sigmoid(wfx_out[k]+self.ufh[i](ht))\n                it=torch.sigmoid(wix_out[k]+self.uih[i](ht))\n                ot=torch.sigmoid(wox_out[k]+self.uoh[i](ht))\n                ct=it*self.act[i](wcx_out[k]+self.uch[i](ht))*drop_mask+ft*ct\n                ht=ot*self.act[i](ct)\n                \n                if self.lstm_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n    \nclass GRU(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(GRU, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.gru_lay=list(map(int, options[\'gru_lay\'].split(\',\')))\n        self.gru_drop=list(map(float, options[\'gru_drop\'].split(\',\'))) \n        self.gru_use_batchnorm=list(map(strtobool, options[\'gru_use_batchnorm\'].split(\',\')))\n        self.gru_use_laynorm=list(map(strtobool, options[\'gru_use_laynorm\'].split(\',\'))) \n        self.gru_use_laynorm_inp=strtobool(options[\'gru_use_laynorm_inp\'])\n        self.gru_use_batchnorm_inp=strtobool(options[\'gru_use_batchnorm_inp\'])\n        self.gru_orthinit=strtobool(options[\'gru_orthinit\'])\n        self.gru_act=options[\'gru_act\'].split(\',\')\n        self.bidir=strtobool(options[\'gru_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n        \n        self.wz  = nn.ModuleList([]) # Update Gate\n        self.uz  = nn.ModuleList([]) # Update Gate\n        \n        self.wr  = nn.ModuleList([]) # Reset Gate\n        self.ur  = nn.ModuleList([]) # Reset Gate  \n        \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n        self.bn_wz  = nn.ModuleList([]) # Batch Norm\n        self.bn_wr  = nn.ModuleList([]) # Batch Norm\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.gru_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.gru_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_gru_lay=len(self.gru_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_gru_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.gru_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.gru_use_laynorm[i] or self.gru_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.gru_lay[i],bias=add_bias))\n             self.wz.append(nn.Linear(current_input, self.gru_lay[i],bias=add_bias))\n             self.wr.append(nn.Linear(current_input, self.gru_lay[i],bias=add_bias))\n             \n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.gru_lay[i], self.gru_lay[i],bias=False))\n             self.uz.append(nn.Linear(self.gru_lay[i], self.gru_lay[i],bias=False))\n             self.ur.append(nn.Linear(self.gru_lay[i], self.gru_lay[i],bias=False))\n\n             if self.gru_orthinit:\n                nn.init.orthogonal_(self.uh[i].weight)\n                nn.init.orthogonal_(self.uz[i].weight)\n                nn.init.orthogonal_(self.ur[i].weight)\n\n             \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.gru_lay[i],momentum=0.05))\n             self.bn_wz.append(nn.BatchNorm1d(self.gru_lay[i],momentum=0.05))\n             self.bn_wr.append(nn.BatchNorm1d(self.gru_lay[i],momentum=0.05))\n\n                \n             self.ln.append(LayerNorm(self.gru_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.gru_lay[i]\n             else:\n                 current_input=self.gru_lay[i]\n                 \n        self.out_dim=self.gru_lay[i]+self.bidir*self.gru_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.gru_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.gru_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_gru_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.gru_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.gru_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.gru_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.gru_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            wz_out=self.wz[i](x)\n            wr_out=self.wr[i](x)\n\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.gru_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n                wz_out_bn=self.bn_wz[i](wz_out.view(wz_out.shape[0]*wz_out.shape[1],wz_out.shape[2]))\n                wz_out=wz_out_bn.view(wz_out.shape[0],wz_out.shape[1],wz_out.shape[2])\n   \n                wr_out_bn=self.bn_wr[i](wr_out.view(wr_out.shape[0]*wr_out.shape[1],wr_out.shape[2]))\n                wr_out=wr_out_bn.view(wr_out.shape[0],wr_out.shape[1],wr_out.shape[2])\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # gru equation\n                zt=torch.sigmoid(wz_out[k]+self.uz[i](ht))\n                rt=torch.sigmoid(wr_out[k]+self.ur[i](ht))\n                at=wh_out[k]+self.uh[i](rt*ht)\n                hcand=self.act[i](at)*drop_mask\n                ht=(zt*ht+(1-zt)*hcand)\n                \n                \n                if self.gru_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n\n\nclass liGRU(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(liGRU, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.ligru_lay=list(map(int, options[\'ligru_lay\'].split(\',\')))\n        self.ligru_drop=list(map(float, options[\'ligru_drop\'].split(\',\'))) \n        self.ligru_use_batchnorm=list(map(strtobool, options[\'ligru_use_batchnorm\'].split(\',\')))\n        self.ligru_use_laynorm=list(map(strtobool, options[\'ligru_use_laynorm\'].split(\',\'))) \n        self.ligru_use_laynorm_inp=strtobool(options[\'ligru_use_laynorm_inp\'])\n        self.ligru_use_batchnorm_inp=strtobool(options[\'ligru_use_batchnorm_inp\'])\n        self.ligru_orthinit=strtobool(options[\'ligru_orthinit\'])\n        self.ligru_act=options[\'ligru_act\'].split(\',\')\n        self.bidir=strtobool(options[\'ligru_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n        \n        self.wz  = nn.ModuleList([]) # Update Gate\n        self.uz  = nn.ModuleList([]) # Update Gate\n              \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n        self.bn_wz  = nn.ModuleList([]) # Batch Norm\n\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.ligru_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.ligru_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_ligru_lay=len(self.ligru_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_ligru_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.ligru_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.ligru_use_laynorm[i] or self.ligru_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.ligru_lay[i],bias=add_bias))\n             self.wz.append(nn.Linear(current_input, self.ligru_lay[i],bias=add_bias))\n\n             \n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i],bias=False))\n             self.uz.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i],bias=False))\n\n             if self.ligru_orthinit:\n             \tnn.init.orthogonal_(self.uh[i].weight)\n             \tnn.init.orthogonal_(self.uz[i].weight)\n\n\n             \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.ligru_lay[i],momentum=0.05))\n             self.bn_wz.append(nn.BatchNorm1d(self.ligru_lay[i],momentum=0.05))\n\n\n                \n             self.ln.append(LayerNorm(self.ligru_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.ligru_lay[i]\n             else:\n                 current_input=self.ligru_lay[i]\n                 \n        self.out_dim=self.ligru_lay[i]+self.bidir*self.ligru_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.ligru_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.ligru_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_ligru_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.ligru_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.ligru_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.ligru_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.ligru_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            wz_out=self.wz[i](x)\n\n\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.ligru_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n                wz_out_bn=self.bn_wz[i](wz_out.view(wz_out.shape[0]*wz_out.shape[1],wz_out.shape[2]))\n                wz_out=wz_out_bn.view(wz_out.shape[0],wz_out.shape[1],wz_out.shape[2])\n\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # ligru equation\n                zt=torch.sigmoid(wz_out[k]+self.uz[i](ht))\n                at=wh_out[k]+self.uh[i](ht)\n                hcand=self.act[i](at)*drop_mask\n                ht=(zt*ht+(1-zt)*hcand)\n                \n                \n                if self.ligru_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n    \nclass minimalGRU(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(minimalGRU, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.minimalgru_lay=list(map(int, options[\'minimalgru_lay\'].split(\',\')))\n        self.minimalgru_drop=list(map(float, options[\'minimalgru_drop\'].split(\',\'))) \n        self.minimalgru_use_batchnorm=list(map(strtobool, options[\'minimalgru_use_batchnorm\'].split(\',\')))\n        self.minimalgru_use_laynorm=list(map(strtobool, options[\'minimalgru_use_laynorm\'].split(\',\'))) \n        self.minimalgru_use_laynorm_inp=strtobool(options[\'minimalgru_use_laynorm_inp\'])\n        self.minimalgru_use_batchnorm_inp=strtobool(options[\'minimalgru_use_batchnorm_inp\'])\n        self.minimalgru_orthinit=strtobool(options[\'minimalgru_orthinit\'])\n        self.minimalgru_act=options[\'minimalgru_act\'].split(\',\')\n        self.bidir=strtobool(options[\'minimalgru_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n        \n        self.wz  = nn.ModuleList([]) # Update Gate\n        self.uz  = nn.ModuleList([]) # Update Gate\n              \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n        self.bn_wz  = nn.ModuleList([]) # Batch Norm\n\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.minimalgru_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.minimalgru_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_minimalgru_lay=len(self.minimalgru_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_minimalgru_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.minimalgru_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.minimalgru_use_laynorm[i] or self.minimalgru_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.minimalgru_lay[i],bias=add_bias))\n             self.wz.append(nn.Linear(current_input, self.minimalgru_lay[i],bias=add_bias))\n\n             \n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.minimalgru_lay[i], self.minimalgru_lay[i],bias=False))\n             self.uz.append(nn.Linear(self.minimalgru_lay[i], self.minimalgru_lay[i],bias=False))\n\n             if self.minimalgru_orthinit:\n             \tnn.init.orthogonal_(self.uh[i].weight)\n             \tnn.init.orthogonal_(self.uz[i].weight)\n\n\n             \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.minimalgru_lay[i],momentum=0.05))\n             self.bn_wz.append(nn.BatchNorm1d(self.minimalgru_lay[i],momentum=0.05))\n\n\n                \n             self.ln.append(LayerNorm(self.minimalgru_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.minimalgru_lay[i]\n             else:\n                 current_input=self.minimalgru_lay[i]\n                 \n        self.out_dim=self.minimalgru_lay[i]+self.bidir*self.minimalgru_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.minimalgru_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.minimalgru_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_minimalgru_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.minimalgru_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.minimalgru_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.minimalgru_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.minimalgru_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            wz_out=self.wz[i](x)\n\n\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.minimalgru_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n                wz_out_bn=self.bn_wz[i](wz_out.view(wz_out.shape[0]*wz_out.shape[1],wz_out.shape[2]))\n                wz_out=wz_out_bn.view(wz_out.shape[0],wz_out.shape[1],wz_out.shape[2])\n\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # minimalgru equation\n                zt=torch.sigmoid(wz_out[k]+self.uz[i](ht))\n                at=wh_out[k]+self.uh[i](zt*ht)\n                hcand=self.act[i](at)*drop_mask\n                ht=(zt*ht+(1-zt)*hcand)\n                \n                \n                if self.minimalgru_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n\nclass RNN(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(RNN, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.rnn_lay=list(map(int, options[\'rnn_lay\'].split(\',\')))\n        self.rnn_drop=list(map(float, options[\'rnn_drop\'].split(\',\'))) \n        self.rnn_use_batchnorm=list(map(strtobool, options[\'rnn_use_batchnorm\'].split(\',\')))\n        self.rnn_use_laynorm=list(map(strtobool, options[\'rnn_use_laynorm\'].split(\',\'))) \n        self.rnn_use_laynorm_inp=strtobool(options[\'rnn_use_laynorm_inp\'])\n        self.rnn_use_batchnorm_inp=strtobool(options[\'rnn_use_batchnorm_inp\'])\n        self.rnn_orthinit=strtobool(options[\'rnn_orthinit\'])\n        self.rnn_act=options[\'rnn_act\'].split(\',\')\n        self.bidir=strtobool(options[\'rnn_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n                   \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.rnn_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.rnn_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_rnn_lay=len(self.rnn_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_rnn_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.rnn_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.rnn_use_laynorm[i] or self.rnn_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.rnn_lay[i],bias=add_bias))\n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.rnn_lay[i], self.rnn_lay[i],bias=False))\n\n             if self.rnn_orthinit:\n             \tnn.init.orthogonal_(self.uh[i].weight)\n          \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.rnn_lay[i],momentum=0.05))\n\n             self.ln.append(LayerNorm(self.rnn_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.rnn_lay[i]\n             else:\n                 current_input=self.rnn_lay[i]\n                 \n        self.out_dim=self.rnn_lay[i]+self.bidir*self.rnn_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.rnn_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.rnn_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_rnn_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.rnn_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.rnn_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.rnn_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.rnn_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.rnn_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # rnn equation\n                at=wh_out[k]+self.uh[i](ht)\n                ht=self.act[i](at)*drop_mask\n                \n                \n                if self.rnn_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n\nclass CNN(nn.Module):\n    \n    def __init__(self,options,inp_dim):\n       super(CNN,self).__init__()\n    \n       # Reading parameters\n       self.input_dim=inp_dim\n       self.cnn_N_filt=list(map(int, options[\'cnn_N_filt\'].split(\',\')))\n\n       self.cnn_len_filt=list(map(int, options[\'cnn_len_filt\'].split(\',\')))\n       self.cnn_max_pool_len=list(map(int, options[\'cnn_max_pool_len\'].split(\',\')))\n       \n       self.cnn_act=options[\'cnn_act\'].split(\',\')\n       self.cnn_drop=list(map(float, options[\'cnn_drop\'].split(\',\')))\n       \n       self.cnn_use_laynorm=list(map(strtobool, options[\'cnn_use_laynorm\'].split(\',\')))\n       self.cnn_use_batchnorm=list(map(strtobool, options[\'cnn_use_batchnorm\'].split(\',\')))\n       self.cnn_use_laynorm_inp=strtobool(options[\'cnn_use_laynorm_inp\'])\n       self.cnn_use_batchnorm_inp=strtobool(options[\'cnn_use_batchnorm_inp\'])\n       \n       self.N_cnn_lay=len(self.cnn_N_filt)\n       self.conv  = nn.ModuleList([])\n       self.bn  = nn.ModuleList([])\n       self.ln  = nn.ModuleList([])\n       self.act = nn.ModuleList([])\n       self.drop = nn.ModuleList([])\n       \n       \n       if self.cnn_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n           \n       if self.cnn_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n           \n       current_input=self.input_dim \n       \n       for i in range(self.N_cnn_lay):\n\n         N_filt=int(self.cnn_N_filt[i])\n         len_filt=int(self.cnn_len_filt[i])\n         \n         # dropout\n         self.drop.append(nn.Dropout(p=self.cnn_drop[i]))\n         \n         # activation\n         self.act.append(act_fun(self.cnn_act[i]))\n            \n         # layer norm initialization         \n         self.ln.append(LayerNorm([N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])]))\n\n         self.bn.append(nn.BatchNorm1d(N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i]),momentum=0.05))\n\n            \n\n         if i==0:\n          self.conv.append(nn.Conv1d(1, N_filt, len_filt))\n              \n         else:\n          self.conv.append(nn.Conv1d(self.cnn_N_filt[i-1], self.cnn_N_filt[i], self.cnn_len_filt[i]))\n          \n         current_input=int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])\n\n         \n       self.out_dim=current_input*N_filt\n\n\n\n    def forward(self, x):\n        \n       batch=x.shape[0]\n       seq_len=x.shape[1]\n       \n       if bool(self.cnn_use_laynorm_inp):\n        x=self.ln0((x))\n        \n       if bool(self.cnn_use_batchnorm_inp):\n        x=self.bn0((x))\n        \n       x=x.view(batch,1,seq_len)\n       \n       for i in range(self.N_cnn_lay):\n           \n         if self.cnn_use_laynorm[i]:\n          x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))\n          \n         if self.cnn_use_batchnorm[i]:\n          x = self.drop[i](self.act[i](self.bn[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))\n\n         if self.cnn_use_batchnorm[i]==False and self.cnn_use_laynorm[i]==False:\n          x = self.drop[i](self.act[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i])))\n\n       \n       x = x.view(batch,-1)\n\n       return x\n\n\nclass SincNet(nn.Module):\n    \n    def __init__(self,options,inp_dim):\n       super(SincNet,self).__init__()\n    \n       # Reading parameters\n       self.input_dim=inp_dim\n       self.sinc_N_filt=list(map(int, options[\'sinc_N_filt\'].split(\',\')))\n\n       self.sinc_len_filt=list(map(int, options[\'sinc_len_filt\'].split(\',\')))\n       self.sinc_max_pool_len=list(map(int, options[\'sinc_max_pool_len\'].split(\',\')))\n       \n       self.sinc_act=options[\'sinc_act\'].split(\',\')\n       self.sinc_drop=list(map(float, options[\'sinc_drop\'].split(\',\')))\n       \n       self.sinc_use_laynorm=list(map(strtobool, options[\'sinc_use_laynorm\'].split(\',\')))\n       self.sinc_use_batchnorm=list(map(strtobool, options[\'sinc_use_batchnorm\'].split(\',\')))\n       self.sinc_use_laynorm_inp=strtobool(options[\'sinc_use_laynorm_inp\'])\n       self.sinc_use_batchnorm_inp=strtobool(options[\'sinc_use_batchnorm_inp\'])\n       \n       self.N_sinc_lay=len(self.sinc_N_filt)\n       \n       self.sinc_sample_rate=int(options[\'sinc_sample_rate\'])\n       self.sinc_min_low_hz=int(options[\'sinc_min_low_hz\'])\n       self.sinc_min_band_hz=int(options[\'sinc_min_band_hz\'])\n\n       \n       self.conv  = nn.ModuleList([])\n       self.bn  = nn.ModuleList([])\n       self.ln  = nn.ModuleList([])\n       self.act = nn.ModuleList([])\n       self.drop = nn.ModuleList([])\n       \n       \n       if self.sinc_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n           \n       if self.sinc_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n           \n       current_input=self.input_dim \n       \n       for i in range(self.N_sinc_lay):\n\n         N_filt=int(self.sinc_N_filt[i])\n         len_filt=int(self.sinc_len_filt[i])\n         \n         # dropout\n         self.drop.append(nn.Dropout(p=self.sinc_drop[i]))\n         \n         # activation\n         self.act.append(act_fun(self.sinc_act[i]))\n            \n         # layer norm initialization         \n         self.ln.append(LayerNorm([N_filt,int((current_input-self.sinc_len_filt[i]+1)/self.sinc_max_pool_len[i])]))\n\n         self.bn.append(nn.BatchNorm1d(N_filt,int((current_input-self.sinc_len_filt[i]+1)/self.sinc_max_pool_len[i]),momentum=0.05))\n\n            \n\n         if i==0:\n          self.conv.append(SincConv(1, N_filt, len_filt,sample_rate=self.sinc_sample_rate, min_low_hz=self.sinc_min_low_hz, min_band_hz=self.sinc_min_band_hz))\n              \n         else:\n          self.conv.append(nn.Conv1d(self.sinc_N_filt[i-1], self.sinc_N_filt[i], self.sinc_len_filt[i]))\n          \n         current_input=int((current_input-self.sinc_len_filt[i]+1)/self.sinc_max_pool_len[i])\n\n         \n       self.out_dim=current_input*N_filt\n\n\n\n    def forward(self, x):\n        \n       batch=x.shape[0]\n       seq_len=x.shape[1]\n       \n       if bool(self.sinc_use_laynorm_inp):\n        x=self.ln0(x)\n        \n       if bool(self.sinc_use_batchnorm_inp):\n        x=self.bn0(x)\n        \n       x=x.view(batch,1,seq_len)\n       \n       for i in range(self.N_sinc_lay):\n           \n         if self.sinc_use_laynorm[i]:\n          x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i]))))\n          \n         if self.sinc_use_batchnorm[i]:\n          x = self.drop[i](self.act[i](self.bn[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i]))))\n\n         if self.sinc_use_batchnorm[i]==False and self.sinc_use_laynorm[i]==False:\n          x = self.drop[i](self.act[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i])))\n\n       \n       x = x.view(batch,-1)\n\n       return x\n\n\n\nclass SincConv(nn.Module):\n    """"""Sinc-based convolution\n    Parameters\n    ----------\n    in_channels : `int`\n        Number of input channels. Must be 1.\n    out_channels : `int`\n        Number of filters.\n    kernel_size : `int`\n        Filter length.\n    sample_rate : `int`, optional\n        Sample rate. Defaults to 16000.\n    Usage\n    -----\n    See `torch.nn.Conv1d`\n    Reference\n    ---------\n    Mirco Ravanelli, Yoshua Bengio,\n    ""Speaker Recognition from raw waveform with SincNet"".\n    https://arxiv.org/abs/1808.00158\n    """"""\n\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride=1, padding=0, dilation=1, bias=False, groups=1,\n                 sample_rate=16000, min_low_hz=50, min_band_hz=50):\n\n        super(SincConv,self).__init__()\n\n        if in_channels != 1:\n            #msg = (f\'SincConv only support one input channel \'\n            #       f\'(here, in_channels = {in_channels:d}).\')\n            msg = ""SincConv only support one input channel (here, in_channels = {%i})"" % (in_channels)\n            raise ValueError(msg)\n\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        \n        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n        if kernel_size%2==0:\n            self.kernel_size=self.kernel_size+1\n            \n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n\n        if bias:\n            raise ValueError(\'SincConv does not support bias.\')\n        if groups > 1:\n            raise ValueError(\'SincConv does not support groups.\')\n\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # initialize filterbanks such that they are equally spaced in Mel scale\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n\n        mel = np.linspace(self.to_mel(low_hz),\n                          self.to_mel(high_hz),\n                          self.out_channels + 1)\n        hz = self.to_hz(mel) / self.sample_rate\n        \n\n        # filter lower frequency (out_channels, 1)\n        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n\n        # filter frequency band (out_channels, 1)\n        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n\n        # Hamming window\n        #self.window_ = torch.hamming_window(self.kernel_size)\n        n_lin=torch.linspace(0, self.kernel_size, steps=self.kernel_size)\n        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n\n        # (kernel_size, 1)\n        n = (self.kernel_size - 1) / 2\n        self.n_ = torch.arange(-n, n+1).view(1, -1) / self.sample_rate\n\n\n    def sinc(self, x):\n        # Numerically stable definition\n        x_left=x[:,0:int((x.shape[1]-1)/2)]\n        y_left=torch.sin(x_left) / x_left\n        y_right= torch.flip(y_left,dims=[1])\n        \n        sinc=torch.cat([y_left,torch.ones([x.shape[0],1]).to(x.device),y_right],dim=1)\n        \n\n        return sinc\n\n    def forward(self, waveforms):\n        """"""\n        Parameters\n        ----------\n        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n            Batch of waveforms.\n        Returns\n        -------\n        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n            Batch of sinc filters activations.\n        """"""\n\n        self.n_ = self.n_.to(waveforms.device)\n\n        self.window_ = self.window_.to(waveforms.device)\n\n        low = self.min_low_hz / self.sample_rate + torch.abs(self.low_hz_)\n        high = low + self.min_band_hz /self.sample_rate + torch.abs(self.band_hz_)\n\n        f_times_t = torch.matmul(low, self.n_)\n\n        low_pass1 = 2 * low * self.sinc(\n            2 * math.pi * f_times_t * self.sample_rate)\n        \n\n\n        f_times_t = torch.matmul(high, self.n_)\n        low_pass2 = 2 * high * self.sinc(\n            2 * math.pi * f_times_t * self.sample_rate)\n        \n\n        band_pass = low_pass2 - low_pass1\n        max_, _ = torch.max(band_pass, dim=1, keepdim=True)\n        band_pass = band_pass / max_\n\n        self.filters = (band_pass * self.window_).view(\n            self.out_channels, 1, self.kernel_size)\n\n        return F.conv1d(waveforms, self.filters, stride=self.stride,\n                        padding=self.padding, dilation=self.dilation,\n                         bias=None, groups=1) \n        \n        \nclass SincConv_fast(nn.Module):\n    """"""Sinc-based convolution\n    Parameters\n    ----------\n    in_channels : `int`\n        Number of input channels. Must be 1.\n    out_channels : `int`\n        Number of filters.\n    kernel_size : `int`\n        Filter length.\n    sample_rate : `int`, optional\n        Sample rate. Defaults to 16000.\n    Usage\n    -----\n    See `torch.nn.Conv1d`\n    Reference\n    ---------\n    Mirco Ravanelli, Yoshua Bengio,\n    ""Speaker Recognition from raw waveform with SincNet"".\n    https://arxiv.org/abs/1808.00158\n    """"""\n\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride=1, padding=0, dilation=1, bias=False, groups=1,\n                 sample_rate=16000, min_low_hz=50, min_band_hz=50):\n\n        super(SincConv_fast,self).__init__()\n\n        if in_channels != 1:\n            #msg = (f\'SincConv only support one input channel \'\n            #       f\'(here, in_channels = {in_channels:d}).\')\n            msg = ""SincConv only support one input channel (here, in_channels = {%i})"" % (in_channels)\n            raise ValueError(msg)\n\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        \n        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n        if kernel_size%2==0:\n            self.kernel_size=self.kernel_size+1\n            \n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n\n        if bias:\n            raise ValueError(\'SincConv does not support bias.\')\n        if groups > 1:\n            raise ValueError(\'SincConv does not support groups.\')\n\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # initialize filterbanks such that they are equally spaced in Mel scale\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n\n        mel = np.linspace(self.to_mel(low_hz),\n                          self.to_mel(high_hz),\n                          self.out_channels + 1)\n        hz = self.to_hz(mel)\n        \n\n        # filter lower frequency (out_channels, 1)\n        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n\n        # filter frequency band (out_channels, 1)\n        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n\n        # Hamming window\n        #self.window_ = torch.hamming_window(self.kernel_size)\n        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\n        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n\n\n        # (kernel_size, 1)\n        n = (self.kernel_size - 1) / 2.0\n        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\n\n \n\n\n    def forward(self, waveforms):\n        """"""\n        Parameters\n        ----------\n        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n            Batch of waveforms.\n        Returns\n        -------\n        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n            Batch of sinc filters activations.\n        """"""\n\n        self.n_ = self.n_.to(waveforms.device)\n\n        self.window_ = self.window_.to(waveforms.device)\n\n        low = self.min_low_hz  + torch.abs(self.low_hz_)\n        \n        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n        band=(high-low)[:,0]\n        \n        f_times_t_low = torch.matmul(low, self.n_)\n        f_times_t_high = torch.matmul(high, self.n_)\n\n        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations. \n        band_pass_center = 2*band.view(-1,1)\n        band_pass_right= torch.flip(band_pass_left,dims=[1])\n        \n        \n        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n\n        \n        band_pass = band_pass / (2*band[:,None])\n        \n\n        self.filters = (band_pass).view(\n            self.out_channels, 1, self.kernel_size)\n\n        return F.conv1d(waveforms, self.filters, stride=self.stride,\n                        padding=self.padding, dilation=self.dilation,\n                         bias=None, groups=1) \n\n        \ndef flip(x, dim):\n    xsize = x.size()\n    dim = x.dim() + dim if dim < 0 else dim\n    x = x.contiguous()\n    x = x.view(-1, *xsize[dim:])\n    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, -1, -1), (\'cpu\',\'cuda\')[x.is_cuda])().long(), :]\n    return x.view(xsize)\n\nclass SRU(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(SRU, self).__init__()\n        self.input_dim = inp_dim\n        self.hidden_size = int(options[\'sru_hidden_size\'])\n        self.num_layers = int(options[\'sru_num_layers\'])\n        self.dropout = float(options[\'sru_dropout\'])\n        self.rnn_dropout = float(options[\'sru_rnn_dropout\'])\n        self.use_tanh = bool(strtobool(options[\'sru_use_tanh\']))\n        self.use_relu = bool(strtobool(options[\'sru_use_relu\']))\n        self.use_selu = bool(strtobool(options[\'sru_use_selu\']))\n        self.weight_norm = bool(strtobool(options[\'sru_weight_norm\']))\n        self.layer_norm = bool(strtobool(options[\'sru_layer_norm\']))\n        self.bidirectional = bool(strtobool(options[\'sru_bidirectional\']))\n        self.is_input_normalized = bool(strtobool(options[\'sru_is_input_normalized\']))\n        self.has_skip_term = bool(strtobool(options[\'sru_has_skip_term\']))\n        self.rescale = bool(strtobool(options[\'sru_rescale\']))\n        self.highway_bias = float(options[\'sru_highway_bias\'])\n        self.n_proj = int(options[\'sru_n_proj\'])\n        self.sru = sru.SRU(self.input_dim, self.hidden_size,\n                            num_layers=self.num_layers,\n                            dropout=self.dropout,\n                            rnn_dropout=self.rnn_dropout,\n                            bidirectional=self.bidirectional,\n                            n_proj=self.n_proj,\n                            use_tanh=self.use_tanh,\n                            use_selu=self.use_selu,\n                            use_relu=self.use_relu,\n                            weight_norm=self.weight_norm,\n                            layer_norm=self.layer_norm,\n                            has_skip_term=self.has_skip_term,\n                            is_input_normalized=self.is_input_normalized,\n                            highway_bias=self.highway_bias,\n                            rescale=self.rescale)\n        self.out_dim = self.hidden_size+self.bidirectional*self.hidden_size\n\n    def forward(self, x):\n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size*2)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n        if x.is_cuda:\n            h0 = h0.cuda()\n        output, hn = self.sru(x, c0=h0)\n        return output\n\n'"
emorec/prepare_iemocap.py,0,"b'""""""\nCreated on Tue May 23 17:24:02 2017\n\n@author: eesungkim\n\nSrc repo: https://github.com/eesungkim/Speech_Emotion_Recognition_DNN-ELM\n\nModified on June 2019 by Santi Pascual\n\n""""""\nimport os\nimport re\nimport scipy.io.wavfile\nimport numpy as np\nfrom os import listdir\nfrom os.path import isfile, join\n\ndef makedirs(dirname):\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\ndef load_utterInfo(inputFile):\n    pattern = re.compile(\'[\\[]*[0-9]*[.][0-9]*[ -]*[0-9]*[.][0-9]*[\\]][\\t][a-z0-9_]*[\\t][a-z]{3}[\\t][\\[][0-9]*[.][0-9]*[, ]+[0-9]*[.][0-9]*[, ]+[0-9]*[.][0-9]*[\\]]\',re.IGNORECASE)\n    with open (inputFile, ""r"") as myfile:\n        data=myfile.read().replace(\'\\n\', \' \')\n    result = pattern.findall(data)\n    out = []\n    for i in result:\n        a = i.replace(\'[\',\'\')\n        b = a.replace(\' - \',\'\\t\')\n        c = b.replace(\']\',\'\')\n        x = c.replace(\', \',\'\\t\')\n        out.append(x.split(\'\\t\'))\n    return out\n\ndef make5thWaves(pathSession):\n    txtPattern = re.compile(\'[.]+(txt)$\',re.IGNORECASE)\n    pathEmo = pathSession+\'/dialog/EmoEvaluation/\'\n    pathWav = pathSession+\'/dialog/wav/\'\n    pathWavFolder1 = pathSession+\'/sentences/wav1/\'\n    for emoFile in [f for f in listdir(pathEmo) if isfile(join(pathEmo, f))]:\n        path=pathWav+txtPattern.split(emoFile)[0]+\'.wav\'\n        (sr,signal) = scipy.io.wavfile.read(path,mmap=False)\n        for utterance in (load_utterInfo(pathEmo+emoFile)):\n                t0 = int(np.ceil(float(utterance[0])*sr))\n                tf = int(np.ceil(float(utterance[1])*sr))\n                \n                if(utterance[2][-4]==\'F\'): # Session 1\xeb\xa7\x8c L(0) channel : main, R(0): \xeb\xb3\xb4\xec\xa1\xb0\n                        mono = signal[t0:tf][:,0]\n                else:   \n                        mono = signal[t0:tf][:,1]\n                \n                folderpath=pathWavFolder1+utterance[2][:-5]\n                makedirs(folderpath)\n                fileName=pathWavFolder1+utterance[2][:-5]+\'/\'+utterance[2]+\'.wav\'\n                scipy.io.wavfile.write(fileName,16000, mono)\n                \ndef load_session(pathSession):\n    pathEmo = pathSession+\'/dialog/EmoEvaluation/\'\n    pathWavFolder = pathSession+\'/sentences/wav/\'\n    \n    improvisedUtteranceList = []\n    for emoFile in [f for f in listdir(pathEmo) if isfile(join(pathEmo, f))]:\n        for utterance in (load_utterInfo(pathEmo+emoFile)):\n            # if ((utterance[3] == \'neu\') or (utterance[3] == \'hap\') or (utterance[3] == \'sad\') or (utterance[3] == \'fru\') or (utterance[3] ==\'exc\')):\n            if ((utterance[3] == \'neu\') or (utterance[3] == \'hap\') or (utterance[3] == \'sad\') or (utterance[3] == \'ang\') or (utterance[3] ==\'exc\')):\n                path=pathWavFolder+utterance[2][:-5]+\'/\'+utterance[2]+\'.wav\'\n                (sr,signal) = scipy.io.wavfile.read(path,mmap=False)\n        \n                if(emoFile[7] != \'i\'):\n                    if(utterance[2][7] ==\'s\'):\n                        improvisedUtteranceList.append([signal,utterance[3],utterance[2][18]])\n                    else:\n                        improvisedUtteranceList.append([signal,utterance[3],utterance[2][15]])\n                else:\n                    improvisedUtteranceList.append([signal,utterance[3],utterance[2][15]])\n    return improvisedUtteranceList\n\ndef count_emotion(session):\n    dic={\'neu\':0, \'hap\':0, \'sad\':0, \'ang\':0, \'sur\':0, \'fea\':0, \'dis\':0, \'fru\':0, \'exc\':0, \'xxx\':0}\n    for i in range(len(session)):\n        if(session[i][1] == \'neu\'): dic[\'neu\']+=1\n        elif(session[i][1] == \'hap\'): dic[\'hap\']+=1\n        elif(session[i][1] == \'sad\'): dic[\'sad\']+=1\n        elif(session[i][1] == \'ang\'): dic[\'ang\']+=1\n        elif(session[i][1] == \'sur\'): dic[\'sur\']+=1\n        elif(session[i][1] == \'fea\'): dic[\'fea\']+=1\n        elif(session[i][1] == \'dis\'): dic[\'dis\']+=1\n        elif(session[i][1] == \'fru\'): dic[\'fru\']+=1\n        elif(session[i][1] == \'exc\'): dic[\'exc\']+=1\n        elif(session[i][1] == \'xxx\'): dic[\'xxx\']+=1\n    return dic\n\n# Save classified wave files to manipulate easily\ndef save_wavFile(session, pathName):\n    makedirs(pathName)\n    for idx,utterance in enumerate(session):\n        label = utterance[1]\n        if(label==\'exc\'):\n            # label=\'exc\'\n            label=\'hap\'\n        directory = ""%s/%s"" % (pathName,label)\n        makedirs(directory)\n        filename = ""%s/psn%s%s_%s_s%03d_orgn.wav"" % (directory,pathName[-4],pathName[-2],label,idx)      \n        scipy.io.wavfile.write(filename,16000, utterance[0])\n\n    return 0\n\ndef main05(ROOT_PATH, path_loadSession, path_directory,MODEL_NAME):\n    #make5thWaves(""%s%s""%(path_loadSession,5))\n    for k in range(5):\n        session_=[]\n        session = load_session(""%s%s""%(path_loadSession,k+1))\n        for idx in range(len(session)):\n            session_.append(session[idx])\n\n        dic_ = count_emotion(session_)\n        print(\'=\'*50)\n        print(\'Total Session_%d :\'%(k+1) +"" %d""%sum(dic_.values()))\n        print(dic_)\n        pathName1 = ""%s/session%d/"" %(path_directory,(k+1) )\n        print(\'=\'*50)\n        if save_wavFile(session_,pathName1) == 0 :\n            print(\'Completed to save session_%d Wave files successfully.\' %(k+1))\n    print(\'=\'*50)\n\nif __name__ == \'__main__\':\n    ROOT_PATH = os.getcwd()\n    AUDIO_DIR = \'data/IEMOCAP_full_release/Session\'\n    MOD_AUDIO_DIR = \'data/IEMOCAP_ahsn_leave-two-speaker-out\'\n    LOG_NAME=\'IEMOCAP\'\n    ORIGINAL_DATASET_PATH = os.path.join(ROOT_PATH, AUDIO_DIR)\n    MODIFIED_DATASET_PATH = os.path.join(ROOT_PATH, MOD_AUDIO_DIR)\n    main05(ROOT_PATH,ORIGINAL_DATASET_PATH,MODIFIED_DATASET_PATH,LOG_NAME)\n\n'"
emorec/run_IEMOCAP_fast.py,22,"b'# Mirco Ravanelli\n# Mila, June 2019\n\n# This script runs a simple emotion recognition experiment on the top of PASE features. \n# The results are reported in terms of Frame Error Rate/ Sentence Error Rate over four emotions of the IEMOCAP dataset \n# This system is not designed for an extensive evaluation of PASE features, but mainly for quickly monitoring the performance of PASE during the self-supervised training phase.\n# The results are printed in standard output and within a text file in $output_folder/res.res\n\n# To run it:\n# python run_IEMOCAP_fast.py ../cfg/PASE.cfg ../PASE.ckpt /home/mirco/Dataset/IEMOCAP_processed iemocap_exp.res\n\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\nimport sys\nfrom neural_networks import MLP,context_window\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nfrom pase.models.frontend import wf_builder\n# from waveminionet.models.frontend import wf_builder #old models\nimport soundfile as sf\nimport os\nimport json\nfrom pase.models.WorkerScheduler.encoder import *\n\n\ndef get_freer_gpu(trials=10):\n    for j in range(trials):\n         os.system(\'nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp\')\n         memory_available = [int(x.split()[2]) for x in open(\'tmp\', \'r\').readlines()]\n         dev_ = torch.device(\'cuda:\'+str(np.argmax(memory_available)))\n         try:\n            a = torch.rand(1).cuda(dev_)\n            return dev_\n         except: \n            pass\n            print(\'NO GPU AVAILABLE!!!\')\n            exit(1)\n\n\npase_cfg=sys.argv[1] # e.g, \'../cfg/PASE.cfg\'\npase_model=sys.argv[2] # e.g, \'../PASE.ckpt\'\ndata_folder=sys.argv[3] # eg. \'/home/mirco/Dataset/IEMOCAP_ahsn_leave-two-speaker-out\'\noutput_file=sys.argv[4] # e.g., \'iemocap_exp.res\'\n\n\n# Label dict\nlab={}\nlab[\'ang\']=0\nlab[\'hap\']=1\nlab[\'neu\']=2\nlab[\'sad\']=3\n\n\n\n# File list for IEMOCAP\ntr_lst_file=\'tr_lst.txt\'\ndev_lst_file=\'te_lst.txt\'\n\ntr_lst = [line.rstrip(\'\\n\') for line in open(tr_lst_file)]\ndev_lst = [line.rstrip(\'\\n\') for line in open(dev_lst_file)]\n\n\n# Training parameters\nN_epochs=15\nseed=1234\nbatch_size=128\nhalving_factor=0.8\nlr=0.0001\nleft=0\nright=0\n\n# Neural network parameters\noptions={}\noptions[\'dnn_lay\']=\'256,4\'\noptions[\'dnn_drop\']=\'0.15,0.0\'\noptions[\'dnn_use_batchnorm\']=\'False,False\'\noptions[\'dnn_use_laynorm\']=\'True,False\'\noptions[\'dnn_use_laynorm_inp\']=\'True\'\noptions[\'dnn_use_batchnorm_inp\']=\'False\'\noptions[\'dnn_act\']=\'relu,softmax\'\n\ndevice=0 #get_freer_gpu()\n\ndname=os.path.dirname(output_file)\nif dname == \'\':\n    dname = \'.\'\nif not os.path.exists(dname):\n    os.makedirs(dname)\n\n# output file creation\ntext_file=open(output_file, ""w"")\n\n# Loading pase\npase=wf_builder(pase_cfg)\npase.load_pretrained(pase_model, load_last=True, verbose=False)\npase.to(device)\npase.eval()\n\n# reading the training signals\nprint(""Waveform reading..."")\nfea={}\nfor wav_file in tr_lst:\n    [signal, fs] = sf.read(data_folder+\'/\'+wav_file)\n    #signal=signal/np.max(np.abs(signal))\n    signal = signal.astype(np.float32)\n    fea_id=wav_file.split(\'/\')[-2]+\'_\'+wav_file.split(\'/\')[-1]\n    fea[fea_id]=torch.from_numpy(signal).float().to(device).view(1,1,-1)\n\n\n# reading the dev signals\nfea_dev={}\nfor wav_file in dev_lst:\n    [signal, fs] = sf.read(data_folder+\'/\'+wav_file)\n    #signal=signal/np.max(np.abs(signal))\n    fea_id=wav_file.split(\'/\')[-2]+\'_\'+wav_file.split(\'/\')[-1]\n    fea_dev[fea_id]=torch.from_numpy(signal).float().to(device).view(1,1,-1)\n\n\n\n# Computing pase features for training\nprint(\'Computing PASE features...\')\nfea_pase={}\nfor snt_id in fea.keys():\n    pase.eval()\n    fea_pase[snt_id]=pase(fea[snt_id], device).to(\'cpu\').detach()\n    fea_pase[snt_id]=fea_pase[snt_id].view(fea_pase[snt_id].shape[1],fea_pase[snt_id].shape[2]).transpose(0,1)\n    avg_vect=fea_pase[snt_id].mean(0).repeat(fea_pase[snt_id].shape[0],1)\n    avg_neu=fea_pase[snt_id].mean(1)\n    std_vect=fea_pase[snt_id].std(0).repeat(fea_pase[snt_id].shape[0],1)\n    std_neu=fea_pase[snt_id].std(1)\n    fea_pase[snt_id]=torch.cat([(fea_pase[snt_id]),avg_vect],1)\n\ninp_dim=fea_pase[snt_id].shape[1]*(left+right+1)\n\n# Computing pase features for test\nfea_pase_dev={}\nfor snt_id in fea_dev.keys():\n    fea_pase_dev[snt_id]=pase(fea_dev[snt_id], device).detach()\n    fea_pase_dev[snt_id]=fea_pase_dev[snt_id].view(fea_pase_dev[snt_id].shape[1],fea_pase_dev[snt_id].shape[2]).transpose(0,1)\n    avg_vect=fea_pase_dev[snt_id].mean(0).repeat(fea_pase_dev[snt_id].shape[0],1)\n    avg_neu=fea_pase_dev[snt_id].mean(1)\n    std_vect=fea_pase_dev[snt_id].std(0).repeat(fea_pase_dev[snt_id].shape[0],1)\n    std_neu=fea_pase_dev[snt_id].std(1)\n\n    fea_pase_dev[snt_id]=torch.cat([(fea_pase_dev[snt_id]),avg_vect],1)\n\n  \n\n# Network initialization\nnnet=MLP(options,inp_dim)\n\nnnet.to(device)\n\ncost=nn.NLLLoss()\n\n# Optimizer initialization\noptimizer = optim.SGD(nnet.parameters(), lr=lr, momentum=0.0)\n\n# Seeds initialization\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# Batch creation (train)\nfea_lst=[]\nlab_lst=[]\n\nprint(""Data Preparation..."")\nfor snt in fea_pase.keys():\n        fea_lst.append(fea_pase[snt])\n        lab_lst.append(np.zeros(fea_pase[snt].shape[0])+lab[snt.split(\'_\')[0]])\n\n    \n# feature matrix (training)\nfea_conc=np.concatenate(fea_lst)\nfea_conc=context_window(fea_conc,left,right)\n\n# feature normalization\nmean=np.mean(fea_conc,axis=0)\nstd=np.std(fea_conc,axis=0)\n\n# normalization\nfea_conc=(fea_conc-mean)/std\n\nmean=torch.from_numpy(mean).float().to(device)\nstd=torch.from_numpy(std).float().to(device)\n\n# lab matrix\nlab_conc=np.concatenate(lab_lst)\n\nif right>0:\n    lab_conc=lab_conc[left:-right]\nelse:\n    lab_conc=lab_conc[left:]\n\n\n# dataset composition\ndataset=np.concatenate([fea_conc,lab_conc.reshape(-1,1)],axis=1)\n\n# shuffling\nnp.random.shuffle(dataset)\n\n#dataset=torch.from_numpy(dataset).float().to(device)\ndataset=torch.from_numpy(dataset).float()\n\n# computing N_batches\nN_ex_tr=dataset.shape[0]\nN_batches=int(N_ex_tr/batch_size)\n\n\nerr_dev_fr_history=[]\nerr_dev_snt_history=[]\n\n# Training loop\nprint(""Training..."")\nfor ep in range(N_epochs):\n    err_batches=0\n    loss_batches=0\n    \n    beg_batch=0\n    \n    # training modality\n    nnet.train()\n    \n    # random shuffling\n    shuffle_index=torch.randperm(dataset.shape[0])\n    dataset=dataset[shuffle_index]\n    \n    for batch_id in range(N_batches):\n        \n        # Batch selection\n        end_batch=beg_batch+batch_size\n        batch=dataset[beg_batch:end_batch]\n        batch=batch.to(device)\n        \n        fea_batch=batch[:,:-1]\n        lab_batch=batch[:,-1].long()\n        \n                    \n        # computing the output probabilities\n        out=nnet(fea_batch)\n           \n        # computing the loss\n        loss=cost(out,lab_batch)\n        \n        # computing the error\n        pred=torch.max(out,dim=1)[1] \n        err = torch.mean((pred!=lab_batch).float())\n        \n        # loss/error accumulation        \n        err_batches=err_batches+err.detach()\n        loss_batches=loss_batches+loss.detach()\n    \n        \n        optimizer.zero_grad()\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        beg_batch=end_batch\n        \n\n        \n    # evaluation\n    nnet.eval()\n    \n    \n    with torch.no_grad():\n    \n        err_dev_fr_mean=0\n        err_dev_snt_mean=0\n        loss_dev_mean=0\n        \n        N_dev_snt=len(list(fea_pase_dev.keys()))\n        \n        for dev_snt in fea_pase_dev.keys():\n            \n             fea_dev_norm=(fea_pase_dev[dev_snt]-mean)/std\n             out_dev=nnet(fea_dev_norm)\n             lab_snt=torch.zeros(fea_pase_dev[dev_snt].shape[0])+lab[dev_snt.split(\'_\')[0]]\n             lab_snt=lab_snt.long().to(device)\n             loss_dev=cost(out_dev,lab_snt)\n             \n             # frame level error\n             pred_dev=torch.max(out_dev,dim=1)[1] \n             err_dev = torch.mean((pred_dev!=lab_snt).float())\n             \n             # sentence error level\n             prob_sum=torch.sum(out_dev,dim=0)\n             pred_dev_snt=torch.argmax(prob_sum) \n             err_snt=(pred_dev_snt!=lab_snt[0]).float()\n             \n             err_dev_fr_mean=err_dev_fr_mean+err_dev.detach()\n             loss_dev_mean=loss_dev_mean+loss_dev.detach()\n             err_dev_snt_mean=err_dev_snt_mean+err_snt.detach()\n         \n         \n    err_dev_fr_history.append(err_dev_fr_mean/N_dev_snt)\n    err_dev_snt_history.append(err_dev_snt_mean/N_dev_snt)\n    \n    \n    print(""epoch=%i loss_tr=%f err_tr=%f loss_te=%f err_te_fr=%f err_te_snt=%f lr=%f"" %(ep,loss_batches/N_batches,err_batches/N_batches,loss_dev_mean/N_dev_snt,err_dev_fr_mean/N_dev_snt,err_dev_snt_mean/N_dev_snt,lr))\n    text_file.write(""epoch=%i loss_tr=%f err_tr=%f loss_te=%f err_te_fr=%f err_te_snt=%f lr=%f \\n"" %(ep,loss_batches/N_batches,err_batches/N_batches,loss_dev_mean/N_dev_snt,err_dev_fr_mean/N_dev_snt,err_dev_snt_mean/N_dev_snt,lr))\n    \n    # learning rate annealing\n    if ep>0:\n        if (err_dev_fr_history[-2]-err_dev_fr_history[-1])/err_dev_fr_history[-2]<0.0025:\n            lr=lr*halving_factor\n            optimizer.param_groups[0][\'lr\']=lr\n\n\nprint(\'BEST ERR=%f\' %(min(err_dev_snt_history)))\nprint(\'BEST ACC=%f\' %(1-min(err_dev_snt_history)))\ntext_file.write(\'BEST_ERR=%f\\n\' %(min(err_dev_snt_history)))\ntext_file.write(\'BEST_ACC=%f\\n\' %(1-min(err_dev_snt_history)))\ntext_file.close()\n    \n    \n    \n    \n\n\n\n'"
emorec/train.py,14,"b'import torch\nimport torch.nn as nn\nimport glob\nimport os\nimport tqdm\nimport numpy as np\nimport argparse\nimport json\nimport random\nimport timeit\nfrom tensorboardX import SummaryWriter\nimport pase\nfrom random import shuffle\nfrom pase.dataset import *\nfrom pase.models.frontend import wf_builder\nimport pase.models.classifiers as pmods\nimport pase.dataset as pdsets\nfrom pase.transforms import SingleChunkWav\nfrom pase.utils import kfold_data, split_train_valid\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n#torch.backends.cudnn.benchmark = False\n\n\ndef retrieve_model_and_datasets(encoder_cfg,\n                                model_cfg, data_cfg,\n                                train_list, valid_list,\n                                test_list):\n\n    with open(model_cfg, \'r\') as cfg_f:\n        model_cfg = json.load(cfg_f)\n\n    if encoder_cfg is not None:\n        with open(encoder_cfg, \'r\') as cfg_f:\n            encoder_cfg = json.load(cfg_f)\n\n    with open(data_cfg, \'r\') as cfg_f:\n        data_cfg = json.load(cfg_f)\n\n    # prepare the three datasets; train, valid and test\n    splits = [train_list, valid_list, test_list]\n    cls_name = model_cfg.pop(\'name\')\n    dset_name = data_cfg.pop(\'name\')\n    if \'chunk_cfg\' in data_cfg:\n        chunker = SingleChunkWav(**data_cfg.pop(\'chunk_cfg\'))\n        data_cfg[\'chunker\'] = chunker\n\n    if encoder_cfg is not None:\n        name = encoder_cfg.pop(\'name\')\n\n        if name == \'pase\' or name == \'PASE\':\n            if \'ckpt\' in encoder_cfg:\n                ckpt = encoder_cfg.pop(\'ckpt\')\n            else:\n                ckpt = None\n            encoder = wf_builder(encoder_cfg)\n            if ckpt is not None:\n                encoder.load_pretrained(ckpt,\n                                        load_last=True,\n                                        verbose=True)\n            model_cfg[\'frontend\'] = encoder\n        elif name == \'tdnn\' or name == \'TDNN\':\n            model_cfg[\'xvector\'] = True\n            encoder = TDNN(**encoder_cfg)\n            model_cfg[\'frontend\'] = encoder\n        else:\n            raise ValueError(\'Unrecognized encoder: \', name)\n\n    model = getattr(pmods, cls_name)(**model_cfg)\n    datasets = []\n    for si, split in enumerate(splits, start=1):\n        if split is None:\n            # skip this split (validation for instance)\n            datasets.append(None)\n        else:\n            data_cfg[\'split_list\'] = split\n            if si >= len(splits) - 1 and \'chunker\' in data_cfg:\n                # remove the chunker for test split\n                del data_cfg[\'chunker\']\n            datasets.append(getattr(pdsets, dset_name)(**data_cfg))\n    return model, datasets\n\n\ndef accuracy(Y_, Y):\n    # Get rid of temporal resolution here,\n    # average likelihood in time and then\n    # compute argmax and accuracy\n    Y__avg = torch.mean(Y_, 2)\n    pred = Y__avg.max(1, keepdim=True)[1]\n    acc = pred.eq(Y[:, 0].view_as(pred)).float().mean().item()\n    return acc\n\ndef valid_round(dloader, model, writer, epoch, device=\'cpu\', tag=\'eval\'):\n    model.eval()\n    with torch.no_grad():\n        val_loss = []\n        val_acc = []\n        for bi, batch in enumerate(dloader, start=1):\n            X, Y = batch\n            if len(X.shape) == 2:\n                X = X.unsqueeze(1)\n            X = X.to(device)\n            Y = Y.to(device)\n            y = model(X)\n            loss = F.nll_loss(y, Y)\n            acc = accuracy(y, Y)\n            val_loss.append(loss.item())\n            val_acc.append(acc)\n        mval = np.mean(val_loss)\n        macc = np.mean(val_acc)\n        print(\'{}: Epoch {} mloss: {:.2f}, macc: {:.2f}\'.format(tag, epoch,\n                                                                mval, macc))\n        writer.add_scalar(\'{}/loss\'.format(tag), mval, epoch)\n        writer.add_scalar(\'{}/acc\'.format(tag), macc, epoch)\n        return mval, macc\n\ndef main(opts):\n    CUDA = True if torch.cuda.is_available() and not opts.no_cuda else False\n    device = \'cuda\' if CUDA else \'cpu\'\n    num_devices = 1\n    np.random.seed(opts.seed)\n    random.seed(opts.seed)\n    torch.manual_seed(opts.seed)\n    if CUDA:\n        torch.cuda.manual_seed_all(opts.seed)\n        num_devices = torch.cuda.device_count()\n        print(\'[*] Using CUDA {} devices\'.format(num_devices))\n    else:\n        print(\'[!] Using CPU\')\n    print(\'Seeds initialized to {}\'.format(opts.seed))\n    # check num of classes by loading the utt2class file\n    with open(opts.utt2class, \'r\') as u2c_f:\n        utt2class = json.load(u2c_f)\n        num_classes = len(set(utt2class.values()))\n        data_list = list(utt2class.keys())\n    save_path = opts.save_path\n    if not os.path.exists(save_path):\n        os.makedirs(save_path)\n    train_list = [l.rstrip() for l in open(opts.train_list, \'r\')]\n    test_list = [l.rstrip() for l in open(opts.test_list, \'r\')]\n    if opts.valid_p > 0:\n        train_list, valid_list = split_train_valid(train_list, valid_p=opts.valid_p)\n    else:\n        valid_list = None\n    model, dset = retrieve_model_and_datasets(opts.encoder_cfg,\n                                              opts.model_cfg,\n                                              opts.data_cfg,\n                                              train_list, valid_list,\n                                              test_list)\n    model.describe_params()\n    if opts.ckpt is not None:\n        # load ckpt for the model on previous state\n        model.load_pretrained(opts.ckpt)\n    if num_devices > 1:\n        model_dp = nn.DataParallel(model)\n    else:\n        model_dp = model\n    model_dp.to(device)\n    if not opts.no_train:\n        # Build DataLoaders: train, valid and test\n        dloader = DataLoader(dset[0], opts.batch_size,\n                             shuffle=True, \n                             pin_memory=CUDA)\n        if dset[1] is not None:\n            va_dloader = DataLoader(dset[1], 1, shuffle=False,\n                                    pin_memory=CUDA)\n        else:\n            va_dloader = None\n        VALID = va_dloader is not None\n        te_dloader = DataLoader(dset[2], 1, shuffle=False,\n                                pin_memory=CUDA)\n        writer = SummaryWriter(save_path)\n        # Build optimizer and scheduler\n        if opts.opt == \'adam\':\n            opt = optim.Adam(model.parameters(), opts.lr)\n        else:\n            opt = optim.SGD(model.parameters(), opts.lr)\n        if opts.sched == \'plateau\' and VALID:\n            sched = ReduceLROnPlateau(opt, \'max\', factor=0.5,\n                                      patience=opts.plateau_patience)\n        else:\n            sched = StepLR(opt, step_size=opts.decay_step,\n                           gamma=0.5)\n        best_val_acc = 0\n        estop_pat = opts.early_stop_patience\n        beg_t = timeit.default_timer()\n        for giter in range(1, opts.iters + 1):\n            X, Y = next(dloader.__iter__())\n            if len(X.shape) == 2:\n                X = X.unsqueeze(1)\n            #X = random_slice_X(X)\n            X = X.to(device)\n            Y = Y.to(device)\n            y = model_dp(X)\n            loss = F.nll_loss(y, Y)\n            loss.backward()\n            acc = accuracy(y, Y)\n            opt.step()\n            opt.zero_grad()\n            end_t = timeit.default_timer()\n            if giter % opts.log_freq == 0:\n                print(\'Iter {:5d}/{:5d} loss: {:.2f}, acc: {:.2f} \'\n                      \'btime: {:.1f} s\'.format(giter, opts.iters,\n                                               loss.item(),\n                                               acc,\n                                               end_t - beg_t))\n                writer.add_scalar(\'train/loss\', loss.item(), giter)\n                writer.add_scalar(\'train/acc\', acc, giter)\n            beg_t = timeit.default_timer()\n            if not isinstance(sched, ReduceLROnPlateau):\n                sched.step()\n            if giter % opts.save_freq == 0:\n                best_val = False\n                if VALID:\n                    val, acc = valid_round(va_dloader, model, writer, giter, device)\n                    if best_val_acc < acc:\n                        best_val_acc = acc\n                        estop_pat = opts.early_stop_patience\n                        best_val = True\n                    else:\n                        estop_pat -= 1\n                        if estop_pat <= 0:\n                            print(\'BREAKING TRAINING LOOP AFTER {} VAL STEPS \'\n                                  \'WITHOUT \'\n                                  \'IMPROVEMENT\'.format(opts.early_stop_patience))\n                            break\n                tes, teacc = valid_round(te_dloader, model, writer, giter,\n                                         device, \'test\')\n                model.save(save_path,\n                           giter, best_val=best_val)\n                if isinstance(sched, ReduceLROnPlateau):\n                    sched.step(acc)\n                model.train()\n    # TEST THIS FOLD AND STORE THE VALUE IN A LOG IN ROOT SAVE_PATH\n    te_files = dset[2].split_list\n    data_root = dset[2].data_root\n    # TODO: Load last best ckpt\n    best_ckpt = get_best_ckpt(save_path)\n    if best_ckpt is None:\n        print(\'ERROR in test: skipping this fold for \'\n              \'did not find a ckpt\')\n        raise NotImplementedError\n    print(\'Loading ckpt for test: \', best_ckpt)\n    model.load_pretrained(best_ckpt, load_last=True)\n    with torch.no_grad():\n        model.eval()\n        res = {}\n        for test_file in tqdm.tqdm(te_files, total=len(te_files)):\n            bname = os.path.splitext(test_file)[0]\n            lab = utt2class[bname]\n            if isinstance(dset[2], WavClassDataset):\n                wav, rate = sf.read(os.path.join(data_root, bname + "".wav""))\n                wav = torch.FloatTensor(wav).view(1, -1)\n                wav = dset[2].z_norm(wav)\n            else:\n                wav = np.load(os.path.join(data_root, bname + "".npy""))\n                wav = torch.FloatTensor(wav).view(wav.shape[0], -1)\n                wav = dset[2].z_norm(wav.transpose(0, 1)).transpose(0, 1)\n            wav = wav.unsqueeze(0)\n            wav = wav.to(device)\n            y = model(wav)\n            res[bname] = {\'prediction\':y.max(1, keepdim=True)[1].item(),\n                              \'lab\':lab}\n        with open(os.path.join(save_path, \'predictions.json\'), \'w\') as f:\n            f.write(json.dumps(res, indent=2))\n\ndef get_best_ckpt(load_path):\n    ckpt_track = glob.glob(os.path.join(load_path, \'*-checkpoints\'))\n    if len(ckpt_track) == 0:\n        return None\n    ckpt_track = ckpt_track[0]\n    with open(ckpt_track, \'r\') as f:\n        ckpts = json.load(f)\n        curr_ckpt = \'weights_{}\'.format(ckpts[\'current\'])\n        return os.path.join(load_path, curr_ckpt)\n\ndef random_slice_X(X, lens=[32000, 96000]):\n    sz = random.choice(list(range(lens[0], lens[1])))\n    idxs = list(range(X.shape[-1] - sz))\n    beg_i = random.choice(idxs)\n    X = X[:, :, beg_i:beg_i + sz]\n    return X\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_root\', type=str,\n                        default=\'data/IEMOCAP_ahsn_leave-two-speaker-out\')\n    parser.add_argument(\'--train_list\', type=str,\n                        default=\'data/sess4train.guia\')\n    parser.add_argument(\'--test_list\', type=str, default=\'data/sess4test.guia\')\n    parser.add_argument(\'--ckpt\', type=str, default=None)\n    parser.add_argument(\'--valid_split\', type=float, default=0.1,\n                        help=\'Validation split inside the training fold\' \\\n                             \' (Def: 0.1).\')\n    parser.add_argument(\'--utt2class\', type=str, \n                        default=\'data/utt2class.json\')\n    parser.add_argument(\'--batch_size\', type=int, default=32)\n    parser.add_argument(\'--iters\', type=int, default=200000)\n    parser.add_argument(\'--seed\', type=int, default=1)\n    parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False)\n    parser.add_argument(\'--save_path\', type=str, default=\'IEMOCAP_ckpt\')\n    parser.add_argument(\'--encoder_cfg\', type=str, default=None)\n    parser.add_argument(\'--model_cfg\', type=str, default=None)\n    parser.add_argument(\'--data_cfg\', type=str, default=None)\n    parser.add_argument(\'--plateau_patience\', type=int, default=5)\n    parser.add_argument(\'--log_freq\', type=int, default=200)\n    parser.add_argument(\'--save_freq\', type=int, default=2500)\n    parser.add_argument(\'--early_stop_patience\', type=int, default=20)\n    parser.add_argument(\'--decay_step\', type=int, default=20000,\n                        help=\'Number of iters til LR decay (Def: 20k).\')\n    parser.add_argument(\'--sched\', type=str, default=\'plateau\')\n    parser.add_argument(\'--lr\', type=float, default=0.1)\n    parser.add_argument(\'--valid_p\', type=float, default=0)\n    parser.add_argument(\'--no-train\', action=\'store_true\', default=False)\n    parser.add_argument(\'--opt\', type=str, default=\'adam\')\n    parser.add_argument(\'--iter_sched\', action=\'store_true\', default=False)\n\n    opts = parser.parse_args()\n\n    main(opts)\n'"
pase/__init__.py,0,b''
pase/dataset.py,21,"b'import torch\nimport torch.nn.functional as F\nimport re\nimport glob\nfrom torch.utils.data import Dataset, ConcatDataset\nimport math\nimport torchaudio\nimport json\nimport tqdm\nimport pickle\nimport os\ntry:\n    from .utils import *\nexcept ImportError:\n    from utils import *\nimport random\nimport numpy as np\nfrom collections import defaultdict\n\n\nclass DictCollater(object):\n\n    def __init__(self, batching_keys=[\'cchunk\',\n                                      \'chunk\',\n                                      \'chunk_ctxt\',\n                                      \'chunk_rand\',\n                                      \'overlap\',\n                                      \'lps\',\n                                      \'lpc\',\n                                      \'gtn\',\n                                      \'fbank\',\n                                      \'mfcc\',\n                                      \'mfcc_librosa\',\n                                      \'prosody\',\n                                      \'kaldimfcc\',\n                                      \'kaldiplp\'],\n                 meta_keys=[],\n                 labs=False):\n        self.batching_keys = batching_keys\n        self.labs = labs\n        self.meta_keys = meta_keys\n\n    def __call__(self, batch):\n        batches = {}\n        lab_b = False\n        labs = None\n        lab_batches = []\n        meta = {}\n        for sample in batch:\n            if len(sample) > 1 and self.labs:\n                labs = sample[1:]\n                sample = sample[0]\n                if len(lab_batches) == 0:\n                    for lab in labs:\n                        lab_batches.append([])\n            for k, v in sample.items():\n                if k in self.meta_keys:\n                    if k not in meta:\n                        meta[k] = []\n                    meta[k].append(v)\n                if k not in self.batching_keys:\n                    continue\n                if k not in batches:\n                    batches[k] = []\n                if v.dim() == 1:\n                    v = v.view(1, 1, -1)\n                elif v.dim() == 2:\n                    v = v.unsqueeze(0)\n                else:\n                    raise ValueError(\'Error in collating dimensions for size \'\n                                     \'{}\'.format(v.size()))\n                batches[k].append(v)\n            if labs is not None:\n                for lab_i, lab in enumerate(labs):\n                    lab_batches[lab_i].append(lab)\n        for k in batches.keys():\n            batches[k] = torch.cat(batches[k], dim=0)\n        rets = [batches]\n        if labs is not None:\n            for li in range(len(lab_batches)):\n                lab_batches_T = lab_batches[li]\n                lab_batches_T = torch.tensor(lab_batches_T)\n                rets.append(lab_batches_T)\n        if len(meta) > 0:\n            rets.append(meta)\n        if len(rets) == 1:\n            return rets[0]\n        else:\n            return rets\n\n\ndef uttwav_collater(batch):\n    """""" Simple collater where (wav, utt) pairs are\n    given by the a dataset, and (wavs, utts, lens) are\n    returned\n    """"""\n    max_len = 0\n    for sample in batch:\n        wav, uttname = sample\n        if wav.shape[0] > max_len:\n            max_len = wav.shape[0]\n\n    wavs = []\n    utts = []\n    lens = []\n\n    for sample in batch:\n        wav, uttname = sample\n        T = wav.shape[0]\n        P = max_len - T\n        if P > 0:\n            wav = np.concatenate((wav,\n                                  np.zeros((P,))),\n                                 axis=0)\n        wavs.append(wav)\n        utts.append(uttname)\n        lens.append(T)\n    return torch.FloatTensor(wavs), utts, torch.LongTensor(lens)\n\n\ndef ft2spk_collater(batch):\n    """""" Simple collater where (fbank, spkid) pairs are\n    given by the a dataset, and (fbanks, spkids, lens) are\n    returned\n    """"""\n    max_len = 0\n    for sample in batch:\n        ft, _ = sample\n        if ft.shape[1] > max_len:\n            max_len = ft.shape[1]\n\n    fts = []\n    labs = []\n    lens = []\n\n    for sample in batch:\n        ft, lab = sample\n        seq_len = ft.shape[1]\n        if seq_len < max_len:\n            P = max_len - seq_len\n            # repeat this amount at the beginning\n            rep = int(math.ceil(P / seq_len))\n            if rep > 1:\n                ft = torch.cat((ft.repeat(1, rep), ft), dim=1)\n                ft = ft[:, -max_len:]\n            else:\n                pad = ft[:, :P]\n                ft = torch.cat((pad, ft), dim=1)\n        elif seq_len > max_len:\n            # trim randomly within utterance\n            idxs = list(range(seq_len - max_len))\n            beg_i = random.choice(idxs)\n            ft = ft[:, beg_i:]\n        fts.append(ft.unsqueeze(0))\n        labs.append(lab.unsqueeze(0))\n        lens.append(seq_len)\n    return torch.cat(fts, dim=0), torch.cat(labs, dim=0), lens\n\n\nclass WavDataset(Dataset):\n\n    def __init__(self, data_root, data_cfg_file, split,\n                 transform=None, sr=None,\n                 return_spk=False,\n                 preload_wav=False,\n                 return_uttname=False,\n                 transforms_cache=None,\n                 distortion_transforms=None,\n                 whisper_folder=None,\n                 noise_folder=None,\n                 cache_on_load=False,\n                 distortion_probability=0.4,\n                 zero_speech_p=0,\n                 zero_speech_transform=None,\n                 verbose=True,\n                 *args, **kwargs):\n        # sr: sampling rate, (Def: None, the one in the wav header)\n        self.sr = sr\n        self.data_root = data_root\n        self.cache_on_load = cache_on_load\n        self.data_cfg_file = data_cfg_file\n        if not isinstance(data_cfg_file, str):\n            raise ValueError(\'Please specify a path to a cfg \'\n                             \'file for loading data.\')\n\n        self.return_uttname = return_uttname\n        self.return_spk = return_spk\n        self.split = split\n        self.transform = transform\n        self.transforms_cache = transforms_cache\n        self.distortion_transforms = distortion_transforms\n        self.zero_speech_p = zero_speech_p\n        self.zero_speech_transform = zero_speech_transform\n        self.whisper_folder = whisper_folder\n        self.noise_folder = noise_folder\n        self.preload_wav = preload_wav\n        self.distortion_probability = distortion_probability\n        with open(data_cfg_file, \'r\') as data_cfg_f:\n            self.data_cfg = json.load(data_cfg_f)\n            self.spk_info = self.data_cfg[\'speakers\']\n            if verbose:\n                print(\'Found {} speakers info\'.format(len(self.spk_info)))\n                wavs = self.data_cfg[split][\'data\']\n                print(\'Found {} files in {} split\'.format(len(wavs),\n                                                          split))\n                spks = self.data_cfg[split][\'speakers\']\n                print(\'Found {} speakers in {} split\'.format(len(spks),\n                                                             split))\n                self.total_wav_dur = int(self.data_cfg[split][\'total_wav_dur\'])\n                if \'spk2idx\' in self.data_cfg and return_spk:\n                    self.spk2idx = self.data_cfg[\'spk2idx\']\n                    print(\'Loaded spk2idx with {} \'\n                          \'speakers\'.format(len(self.spk2idx)))\n            self.wavs = wavs\n        self.wav_cache = {}\n        if whisper_folder is not None:\n            self.whisper_cache = {}\n        if noise_folder is not None:\n            self.noise_cache = {}\n        if preload_wav:\n            print(\'Pre-loading wavs to memory\')\n            for wavstruct in tqdm.tqdm(self.wavs, total=len(self.wavs)):\n                uttname = wavstruct[\'filename\']\n                wname = os.path.join(self.data_root, uttname)\n                self.retrieve_cache(wname, self.wav_cache)\n                if hasattr(self, \'whisper_cache\'):\n                    dwname = os.path.join(whisper_folder, uttname)\n                    self.retrieve_cache(dwname, self.whisper_cache)\n                if hasattr(self, \'noise_cache\'):\n                    nwname = os.path.join(noise_folder, uttname)\n                    self.retrieve_cache(nwname, self.noise_cache)\n\n    def __len__(self):\n        return len(self.wavs)\n\n    def retrieve_cache(self, fname, cache):\n        if (self.cache_on_load or self.preload_wav) and fname in cache:\n            return cache[fname]\n        else:\n            wav, rate = torchaudio.load(fname)\n            wav = wav.numpy().squeeze()\n            #fix in case wav is stereo, in which case\n            #pick first channel only\n            if wav.ndim > 1:\n                wav = wav[:,0]\n            wav = wav.astype(np.float32)\n            if self.cache_on_load:\n                cache[fname] = wav\n            return wav\n\n    def __getitem__(self, index):\n        if sample_probable(self.zero_speech_p):\n            wav = zerospeech(int(5 * 16e3))\n            if self.zero_speech_transform is not None:\n                wav = self.zero_speech_transform(wav)\n        else:\n            uttname = self.wavs[index][\'filename\']\n            wname = os.path.join(self.data_root, uttname)\n            wav = self.retrieve_cache(wname, self.wav_cache)\n            if self.transform is not None:\n                wav = self.transform(wav)\n        rets = [wav]\n        if self.return_uttname:\n            rets = rets + [uttname]\n        if self.return_spk:\n            rets = rets + [self.spk2idx[self.wavs[index][\'speaker\']]]\n        if len(rets) == 1:\n            return rets[0]\n        else:\n            return rets\n\n\nclass PairWavDataset(WavDataset):\n    """""" Return paired wavs, one is current wav and the other one is a randomly\n        chosen one.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.rwav_cache = {}\n\n    def __getitem__(self, index):\n        # create candidate indices for random other wavs without current index\n        indices = list(range(len(self.wavs)))\n        indices.remove(index)\n        rindex = random.choice(indices)\n        rwname = os.path.join(self.data_root, self.wavs[rindex][\'filename\'])\n        rwav = self.retrieve_cache(rwname, self.wav_cache)\n        # Load current wav or generate the zero-version\n        if sample_probable(self.zero_speech_p):\n            ZERO_SPEECH = True\n            wav = zerospeech(int(5 * 16e3))\n            uttname = \'zerospeech.wav\'\n        else:\n            ZERO_SPEECH = False\n            uttname = self.wavs[index][\'filename\']\n            # Here we select two wavs, the current one and a randomly chosen one\n            wname = os.path.join(self.data_root, uttname)\n            wav = self.retrieve_cache(wname, self.wav_cache)\n            #print (\'Wav shape for {} is {}\'.format(uttname, wav.shape))\n        pkg = {\'raw\': wav, \'raw_rand\': rwav,\n               \'uttname\': uttname, \'split\': self.split}\n        # Apply the set of \'target\' transforms on the clean data\n        if self.transform is not None:\n            pkg = self.transform(pkg)\n\n        pkg[\'cchunk\'] = pkg[\'chunk\'].squeeze(0)\n        # initialize overlap label\n        if \'dec_resolution\' in pkg:\n            pkg[\'overlap\'] = torch.zeros(len(pkg[\'chunk\']) // pkg[\'dec_resolution\']).float()\n        else:\n            pkg[\'overlap\'] = torch.zeros(len(pkg[\'chunk\'])).float()\n\n        if self.distortion_transforms and not ZERO_SPEECH:\n            pkg = self.distortion_transforms(pkg)\n        \n        if self.zero_speech_transform and ZERO_SPEECH:\n            pkg = self.zero_speech_transform(pkg)\n\n        if self.transform is None:\n            # if no transforms happened do not send a package\n            return pkg[\'chunk\'], pkg[\'raw_rand\']\n        else:\n            # otherwise return the full package\n            return pkg\n\n\nclass GenhancementDataset(Dataset):\n    """""" Return the regular package with current (noisy) wav, \n        random neighbor wav (also noisy), and clean output\n    """"""\n\n    def __init__(self, data_root, data_cfg_file, split,\n                 transform=None, sr=None,\n                 return_spk=False,\n                 preload_wav=False,\n                 return_uttname=False,\n                 transforms_cache=None,\n                 distortion_transforms=None,\n                 whisper_folder=None,\n                 noise_folder=None,\n                 cache_on_load=False,\n                 distortion_probability=0.4,\n                 zero_speech_p=0,\n                 zero_speech_transform=None,\n                 verbose=True,\n                 *args, **kwargs):\n        # TODO: half of these useless arguments should be removed in \n        # this dataset, but need to homogeneize the datasets or something\n        super().__init__()\n        # sr: sampling rate, (Def: None, the one in the wav header)\n        self.sr = sr\n        self.data_root = data_root\n        self.data_cfg_file = data_cfg_file\n        if not isinstance(data_cfg_file, str):\n            raise ValueError(\'Please specify a path to a cfg \'\n                             \'file for loading data.\')\n\n        self.split = split\n        self.transform = transform\n        with open(data_cfg_file, \'r\') as data_cfg_f:\n            self.data_cfg = json.load(data_cfg_f)\n            self.spk_info = self.data_cfg[\'speakers\']\n            if verbose:\n                print(\'Found {} speakers info\'.format(len(self.spk_info)))\n                noisy_wavs = self.data_cfg[split][\'data\']\n                print(\'Found {} files in {} split\'.format(len(noisy_wavs),\n                                                          split))\n                spks = self.data_cfg[split][\'speakers\']\n                print(\'Found {} speakers in {} split\'.format(len(spks),\n                                                             split))\n                self.total_wav_dur = int(self.data_cfg[split][\'total_wav_dur\'])\n                if \'spk2idx\' in self.data_cfg and return_spk:\n                    self.spk2idx = self.data_cfg[\'spk2idx\']\n                    print(\'Loaded spk2idx with {} \'\n                          \'speakers\'.format(len(self.spk2idx)))\n            self.noisy_wavs = noisy_wavs\n\n    def __len__(self):\n        return len(self.noisy_wavs)\n\n    def __getitem__(self, index):\n        # create candidate indices for random other wavs without current index\n        indices = list(range(len(self.noisy_wavs)))\n        indices.remove(index)\n        rindex = random.choice(indices)\n        # sample from the noisies\n        rwname = os.path.join(self.data_root, self.noisy_wavs[rindex][\'filename\'])\n        rwav, rate = sf.read(rwname)\n        rwav = rwav.astype(np.float32)\n        # Load current clean wav \n        uttname = self.noisy_wavs[index][\'filename\']\n        # Santi:\n        # --------------\n        # TODO: Fix this shameful ""path replacement"" assuming there is a\n        # \'noisy\' -> \'clean\' valid redirection from the config paths\n        # Clean chunk has to be forwarded first to compute regression outputs\n        # then we can load and chunk the same window on current noisy piece\n        nwname = os.path.join(self.data_root, uttname)\n        cwname = nwname.replace(\'noisy\', \'clean\')\n        wav, rate = sf.read(cwname)\n        wav = wav.astype(np.float32)\n        pkg = {\'raw\': wav, \'raw_rand\': rwav,\n               \'uttname\': uttname, \'split\': self.split}\n        # Apply the set of \'target\' transforms on the clean data\n        if self.transform is not None:\n            pkg = self.transform(pkg)\n\n        # Load the noisy one and chunk it\n        nwav, rate = sf.read(nwname)\n        nwav = nwav.astype(np.float32)\n        # re-direct the chunk to be cchunk (clean chunk)\n        pkg[\'cchunk\'] = pkg[\'chunk\'].squeeze(0)\n        # make the current noisy chunk\n        chunk_beg = pkg[\'chunk_beg_i\']\n        chunk_end = pkg[\'chunk_end_i\']\n        chunk = nwav[chunk_beg:chunk_end]\n        pkg[\'chunk\'] = torch.FloatTensor(chunk)\n        pkg[\'raw\'] = nwav\n\n        if self.transform is None:\n            # if no transforms happened do not send a package\n            return pkg[\'chunk\'], pkg[\'raw_rand\']\n        else:\n            # otherwise return the full package\n            return pkg\n\nclass LibriSpeechSegTupleWavDataset(PairWavDataset):\n    """""" Return three wavs, one is current wav, another one is\n        the continuation of a pre-chunked utterance following the name\n        pattern <prefix>-<utt_id>.wav. So for example for file\n        1001-134707-0001-2.wav we have to get its paired wav as\n        1001-134707-0001-0.wav for instance, which is a neighbor within\n        utterance level. Finally, another random and different utterance\n        following the filename is returned too as random context.\n    """"""\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.rec = re.compile(r\'(\\d+).wav\')\n        # pre-cache prefixes to load from dictionary quicker\n        self.neighbor_prefixes = {}\n        for wav in self.wavs:\n            fname = wav[\'filename\']\n            prefix = self.rec.sub(\'\', fname)\n            if prefix not in self.neighbor_prefixes:\n                self.neighbor_prefixes[prefix] = []\n            self.neighbor_prefixes[prefix].append(fname)\n        print(\'Found {} prefixes in \'\n              \'utterances\'.format(len(self.neighbor_prefixes)))\n\n    def __getitem__(self, index):\n        # Load current wav or generate the zero-version\n        if sample_probable(self.zero_speech_p):\n            ZERO_SPEECH = True\n            wav = zerospeech(int(5 * 16e3))\n            cwav = wav\n            uttname = \'zerospeech.wav\'\n        else:\n            ZERO_SPEECH = False\n            uttname = self.wavs[index][\'filename\']\n            # Here we select the three wavs.\n            # (1) Current wav selection\n            wname = os.path.join(self.data_root, uttname)\n            wav = self.retrieve_cache(wname, self.wav_cache)\n            # (2) Context wav selection by utterance name pattern. If\n            # no other sub-index is found, the same as current wav is returned\n            prefix = self.rec.sub(\'\', uttname)\n            neighbors = self.neighbor_prefixes[prefix]\n            # print(\'Wname: \', wname)\n            # delete current file\n            # print (\'Uttn {}, Pref {}\'.format(uttname, prefix))\n            # print(\'Found nehg: \', neighbors)\n            neighbors.remove(uttname)\n            # print(\'Found nehg: \', neighbors)\n            # pick random one if possible, otherwise it will be empty\n            if len(neighbors) > 0:\n                cwname = os.path.join(self.data_root, random.choice(neighbors))\n                cwav = self.retrieve_cache(cwname, self.wav_cache)\n            else:\n                cwav = wav\n        # (2) Random wav selection for out of context sample\n        # create candidate indices without current index\n        indices = list(range(len(self.wavs)))\n        indices.remove(index)\n        rindex = random.choice(indices)\n        rwname = os.path.join(self.data_root, self.wavs[rindex][\'filename\'])\n        rwav = self.retrieve_cache(rwname, self.wav_cache)\n        pkg = {\'raw\': wav, \'raw_rand\': rwav, \'raw_ctxt\': cwav,\n               \'uttname\': uttname, \'split\': self.split}\n        # Apply the set of \'target\' transforms on the clean data\n        if self.transform is not None:\n            pkg = self.transform(pkg)\n\n        pkg[\'cchunk\'] = pkg[\'chunk\'].squeeze(0)\n        # initialize overlap label\n        pkg[\'overlap\'] = torch.zeros(pkg[\'chunk\'].shape[-1] // pkg[\'dec_resolution\']).float()\n\n        if self.distortion_transforms and not ZERO_SPEECH:\n            pkg = self.distortion_transforms(pkg)\n        \n        if self.zero_speech_transform and ZERO_SPEECH:\n            pkg = self.zero_speech_transform(pkg)\n\n        # sf.write(\'/tmp/ex_chunk.wav\', pkg[\'chunk\'], 16000)\n        # sf.write(\'/tmp/ex_cchunk.wav\', pkg[\'cchunk\'], 16000)\n        # raise NotImplementedError\n        if self.transform is None:\n            # if no transforms happened do not send a package\n            return pkg[\'chunk\'], pkg[\'raw_rand\']\n        else:\n            # otherwise return the full package\n            return pkg\n\n\nclass AmiSegTupleWavDataset(PairWavDataset):\n    """""" Returns 4 wavs:\n    1st is IHM chunk, 2nd is continuation chunk\n    3rd is a corresponding to ihm distant channel (random one from the mic array)\n    4th is a random (ideally) non-related chunk \n    Note, this can also only work with only ihms (when pair_sdms=None)\n    """"""\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        assert self.zero_speech_p == 0, (\n            ""Zero speech mode is not supported for AMI as of now""\n        )\n        assert \'ihm2sdm\' in kwargs, (\n            ""Need to provide ihm2sdm for AMI dataset""\n        )\n        self.ihm2sdm = None\n        self.do_ihm2sdm = False\n        if kwargs[\'ihm2sdm\'] is not None:\n            self.ihm2sdm = kwargs[\'ihm2sdm\'].split(\',\')\n            assert len(self.ihm2sdm) > 0, (\n                ""Expected at least one sdm channel, got {}"".format(self.ihm2sdm)\n            )\n            self.do_ihm2sdm = True\n        if self.do_ihm2sdm:\n            print (\'Parallel mode enabled, will pair ihm with sdms: {}\'.format(self.ihm2sdm))\n        else:\n            print (\'Single channel mode enabled, will feed only ihm data\')\n        # pre-cache prefixes to load from dictionary quicker\n        self.neighbor_prefixes = {}\n        lost_segs, lost_indices = [], []\n\n        if self.do_ihm2sdm:\n            for index, wav in enumerate(self.wavs):\n                for sdm_idx in self.ihm2sdm:\n                    if sdm_idx not in wav:\n                        lost_segs.append(wav[\'filename\'])\n                        lost_indices.append(index)\n            print (\'In total {} sdm segments were missing and removed\'.format(len(lost_segs)))\n            for index in sorted(lost_indices, reverse=True):\n                del self.wavs[index]\n        \n        self.rec = re.compile(r\'(\\d+).wav\')\n        for idx, wav in enumerate(self.wavs):\n            fname = wav[\'filename\']\n            prefix = self.rec.sub(\'\', fname)\n            if prefix not in self.neighbor_prefixes:\n                self.neighbor_prefixes[prefix] = []\n            self.neighbor_prefixes[prefix].append((idx, fname))\n        print(\'Found {} prefixes in \'\n              \'utterances\'.format(len(self.neighbor_prefixes)))\n\n    def __getitem__(self, index):\n        # Load current wav \n        # Note, this provided works with parlallel like data (i.e\n        # the one where you have two or more versions of the same\n        # signal, typically from multi-mic setups. As PASE relis on\n        # clean and [possibly] distorted signal, the following code\n        # reads both from parallel data. Notice, clean variant is only\n        # used to self-supervised minions, thus is not fpropped through\n        # the networks \n\n        # I. get and load clean (here ihm) variant as in other provider,\n        # we are gonna need it anyways\n        uttname = self.wavs[index][\'filename\']\n        # Here we select the three wavs.\n        # (1) Current wav selection\n        wname = os.path.join(self.data_root, uttname)\n        wav = self.retrieve_cache(wname, self.wav_cache)\n        # (2) Context wav selection by utterance name pattern. If\n        # no other sub-index is found, the same as current wav is returned\n        prefix = self.rec.sub(\'\', uttname)\n        neighbors = self.neighbor_prefixes[prefix]\n        # print(\'Wname: \', wname)\n        # delete current file\n        #print(\'Found nehg: \', neighbors)\n        #print (""Uttn {}, wn {}, pref {}"".format(uttname, wname, prefix))\n        neighbors.remove((index, uttname))\n        # print(\'Found nehg: \', neighbors)\n        # pick random one if possible, otherwise it will be empty\n        # only sample the for now candidate, we will load the wav\n        # depending on whether sdm or ihm is needed\n        choice = None\n        if len(neighbors) > 0:\n            choice = random.choice(neighbors)\n\n        # (2) Random wav selection for out of context sample\n        # create candidate indices without current index\n        indices = list(range(len(self.wavs)))\n        indices.remove(index)\n        rindex = random.choice(indices)\n\n        # II. depending on config, load either sdm or ihm wavs\n        if self.do_ihm2sdm > 0:\n            #pick random distant channel id from which to load stuff\n            idx = random.choice(self.ihm2sdm)\n            #print (\'Utt {} idx is {}.\'.format(uttname, idx))\n            #print (\'Index {} and cfg {}\'.format(index, self.wavs[index]))\n            #print (\'Rindex {} and cfg {}\'.format(self.wavs[rindex]))\n            #if idx not in self.wavs[index]:\n            #    print (\'Opps {} not found in {}\'.format(idx, self.wavs[index]))\n            #if idx not in self.wavs[rindex]:\n            #    print (\'Oops {} not found in {}\'.format(idx, self.wavs[rindex]))\n            #load waveform sdm eqivalent for ihm\n            sdm_fname = os.path.join(self.data_root, self.wavs[index][idx])\n            sdm_wav = self.retrieve_cache(sdm_fname, self.wav_cache)\n            #load waveform sdm random chunk\n            rsdm_fname = os.path.join(self.data_root, self.wavs[rindex][idx])\n            rand_sdm_wav = self.retrieve_cache(rsdm_fname, self.wav_cache)\n            #load context wavform, given choice above\n            if choice is not None:\n                cindex, fname = choice\n                cwname = os.path.join(self.data_root, self.wavs[cindex][idx])\n                cwav = self.retrieve_cache(cwname, self.wav_cache)\n            else:\n                cwav = sdm_wav\n            # Note: this one is quite dirty trick, but anyways for now\n            # since we have parallel versions of data (i.e. corrputed naturally)\n            # we need to extract self-supervision targets for clean, which is\n            # assumed to be in chunk in all transforms. Thus, we keep it like this\n            # and pass ihm wav in raw (so targets get extracted), we also pass\n            # wav_sdm in raw_clean. After the transforms we swap them so sdm\n            # (not ihm) chunk gets fed into the model, and ihm is preserved in cchunk\n            pkg = {\'raw\': wav, \'raw_rand\': rand_sdm_wav, \'raw_ctxt\': cwav,\n               \'uttname\': uttname, \'split\': self.split, \'raw_clean\':sdm_wav}\n        else:\n            if choice is not None:\n                cindex, fname = choice\n                cwname = os.path.join(self.data_root, fname)\n                cwav = self.retrieve_cache(cwname, self.wav_cache)\n            else:\n                cwav = wav\n            rwav_fname = os.path.join(self.data_root, self.wavs[rindex][\'filename\'])\n            rwav = self.retrieve_cache(rwav_fname, self.wav_cache)\n            pkg = {\'raw\': wav, \'raw_rand\': rwav, \'raw_ctxt\': cwav,\n                    \'uttname\': uttname, \'split\': self.split}\n\n        # Apply the set of \'target\' transforms on the clean data\n        if self.transform is not None:\n            pkg = self.transform(pkg)\n\n        if \'cchunk\' in pkg:\n            chunk = pkg[\'cchunk\']\n            #print (""cchunk 1: size {}"".format(chunk.size()))\n            pkg[\'cchunk\'] = pkg[\'chunk\'].squeeze(0)\n            pkg[\'chunk\'] = chunk.squeeze(0)\n            #print (""cchunk 1: size sq {}"".format(pkg[\'cchunk\'].size()))\n        else:\n            #print (""cchunk 2: size {}"".format(pkg[\'chunk\'].size()))\n            pkg[\'cchunk\'] = pkg[\'chunk\'].squeeze(0)\n            #print (""cchunk 2: size sq {}"".format(pkg[\'cchunk\'].size()))\n\n        # initialize overlap label\n        pkg[\'overlap\'] = torch.zeros(len(pkg[\'chunk\']) // pkg[\'dec_resolution\']).float()\n\n        if self.distortion_transforms:\n            pkg = self.distortion_transforms(pkg)\n\n        # sf.write(\'/tmp/ex_chunk.wav\', pkg[\'chunk\'], 16000)\n        # sf.write(\'/tmp/ex_cchunk.wav\', pkg[\'cchunk\'], 16000)\n        # raise NotImplementedError\n        if self.transform is None:\n            # if no transforms happened do not send a package\n            return pkg[\'chunk\'], pkg[\'raw_rand\']\n        else:\n            # otherwise return the full package\n            return pkg\n\nclass MetaWavConcatDataset(ConcatDataset):\n    """"""This dataset class abstracts pool of several different datasets, \n    each having possibly a different sets of transform / distortion stacks. \n    We abstract pytorch\'s ConcatDataset as the code relies on several \n    dataset specific attributes (like tot_wav_dur) that are assumed to exist\n    """"""\n    def __init__(self, datasets=[]):\n        super(MetaWavConcatDataset, self).__init__(datasets)\n    \n        for dset in self.datasets:\n            assert isinstance(dset, WavDataset), (\n                ""{} is expected to work with WavDataset ""\n                ""instances only."".format(__class__)\n            )\n\n    @property\n    def total_wav_dur(self):\n        tot_dur = 0\n        for d in self.datasets:\n            tot_dur += d.total_wav_dur\n        return tot_dur\n\nclass FeatsClassDataset(Dataset):\n    def __init__(self, data_root, utt2class, split_list,\n                 stats=None, verbose=True, ext=\'fb.npy\'):\n        self.data_root = data_root\n        self.ext = ext\n        if not isinstance(utt2class, str):\n            raise ValueError(\'Please specify a path to a utt2class \'\n                             \'file for loading data.\')\n        if not isinstance(split_list, str):\n            raise ValueError(\'Please specify a path to a split_list \'\n                             \'file for loading data.\')\n        utt2class_ext = utt2class.split(\'.\')[1]\n        if utt2class_ext == \'json\':\n            with open(utt2class, \'r\') as u2s_f:\n                self.utt2class = json.load(u2s_f)\n        else:\n            self.utt2class = np.load(utt2class)\n            self.utt2class = dict(self.utt2class.any())\n        print(\'Found {} speakers\'.format(len(set(self.utt2class.values()))))\n        with open(split_list, \'r\') as sl_f:\n            self.split_list = [l.rstrip() for l in sl_f]\n            print(\'Found {} fbank files\'.format(len(self.split_list)))\n        if stats is not None:\n            with open(stats, \'rb\') as stats_f:\n                self.stats = pickle.load(stats_f)\n\n    def __len__(self):\n        return len(self.split_list)\n\n    def z_norm(self, x):\n        assert hasattr(self, \'stats\')\n        stats = self.stats\n        mean = torch.FloatTensor(stats[\'mean\']).view(-1, 1)\n        std = torch.FloatTensor(stats[\'std\']).view(-1, 1)\n        x = (x - mean) / std\n        return x\n\n    def __getitem__(self, index):\n        item = self.split_list[index]\n        bname = os.path.splitext(item)[0]\n        ft_file = os.path.join(self.data_root, bname + \'.\' + self.ext)\n        ft = torch.FloatTensor(np.load(ft_file).T)\n        if hasattr(self, \'stats\'):\n            ft = self.z_norm(ft)\n        seq_len = ft.shape[1]\n        spk_id = self.utt2class[item]\n        return ft, torch.LongTensor([spk_id])\n\n\nclass WavClassDataset(Dataset):\n    """""" Simple Wav -> classID dataset """"""\n\n    def __init__(self, data_root, utt2class, split_list,\n                 chunker=None,\n                 verbose=True):\n        self.data_root = data_root\n        if not isinstance(utt2class, str):\n            raise ValueError(\'Please specify a path to a utt2class \'\n                             \'file for loading data.\')\n        if not isinstance(split_list, str) and not isinstance(split_list, list):\n            raise ValueError(\'Please specify a path to a split_list \'\n                             \'file for loading data or to the list itself.\')\n        utt2class_ext = utt2class.split(\'.\')[1]\n        if utt2class_ext == \'json\':\n            with open(utt2class, \'r\') as u2s_f:\n                self.utt2class = json.load(u2s_f)\n        else:\n            self.utt2class = np.load(utt2class)\n            self.utt2class = dict(self.utt2class.any())\n        print(\'Found {} classes\'.format(len(set(self.utt2class.values()))))\n        self.chunker = chunker\n        if isinstance(split_list, list):\n            self.split_list = split_list\n        else:\n            with open(split_list, \'r\') as sl_f:\n                self.split_list = [l.rstrip() for l in sl_f]\n                print(\'Found {} wav files\'.format(len(self.split_list)))\n\n    def __len__(self):\n        return len(self.split_list)\n\n    def __getitem__(self, index):\n        item = self.split_list[index]\n        bname = os.path.splitext(item)[0]\n        wav_file = os.path.join(self.data_root, bname + \'.wav\')\n        wav, rate = sf.read(wav_file)\n        wav = torch.FloatTensor(wav)\n        if self.chunker is not None:\n            if len(wav) < self.chunker.chunk_size + 1:\n                P = self.chunker.chunk_size + 1 - len(wav)\n                wav = torch.cat((wav,\n                                 torch.zeros(P)),\n                                dim=0)\n            wav = self.chunker(wav)\n            wav = wav[\'chunk\']\n        spk_id = self.utt2class[item]\n        return wav, torch.LongTensor([spk_id])\n'"
pase/log.py,1,"b""from tensorboardX import SummaryWriter\nimport numpy as np\nimport torch\nimport pickle\nimport os\n\n\nclass PklWriter(object):\n\n    def __init__(self, save_path):\n        from datetime import datetime\n        curr_time = datetime.now().strftime('%b%d_%H-%M-%S')\n        fname = 'losses_{}.pkl'.format(curr_time)\n        self.save_path = os.path.join(save_path, fname)\n        self.losses = {}\n\n    def add_scalar(self, tag, scalar_value, global_step=None):\n        if tag not in self.losses:\n            self.losses[tag] = {'global_step':[],\n                                'scalar_value':[]}\n        if torch.is_tensor(scalar_value):\n            scalar_value = scalar_value.item()\n        self.losses[tag]['scalar_value'].append(scalar_value)\n        self.losses[tag]['global_step'].append(global_step)\n        with open(self.save_path, 'wb') as out_f:\n            pickle.dump(self.losses, out_f)\n\n    def add_histogram(self, tag, values, global_step=None, bins='sturges'):\n        # not implemented for the json logger\n        pass\n\nclass LogWriter(object):\n\n    def __init__(self, save_path, log_types=['tensorboard', 'pkl']):\n        self.save_path = save_path\n        if len(log_types) == 0:\n            raise ValueError('Please specify at least one log_type file to '\n                             'write to in the LogWriter!')\n        self.writers = []\n        for log_type in log_types:\n            if 'tensorboard' == log_type:\n                self.writers.append(SummaryWriter(save_path))\n            elif 'pkl' == log_type:\n                self.writers.append(PklWriter(save_path))\n            else:\n                raise TypeError('Unrecognized log_writer type: ', log_writer)\n\n    def add_scalar(self, tag, scalar_value, global_step=None):\n        for writer in self.writers:\n            writer.add_scalar(tag, scalar_value=scalar_value, \n                              global_step=global_step)\n\n    def add_histogram(self, tag, values, global_step=None, bins='sturges'):\n        for writer in self.writers:\n            writer.add_histogram(tag, values=values, global_step=global_step, \n                                 bins=bins)\n\n"""
pase/losses.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ContextualizedLoss(object):\n    """""" With a possible composition of r\n        consecutive frames\n    """"""\n\n    def __init__(self, criterion, r=None):\n        self.criterion = criterion\n        self.r = r\n\n    def contextualize_r(self, tensor):\n        if self.r is None:\n            return tensor\n        assert isinstance(self.r, int), type(self.r)\n        # ensure it is a 3-D tensor\n        assert len(tensor.shape) == 3, tensor.shape\n        # pad tensor in the edges with zeros\n        pad_ = F.pad(tensor, (self.r // 2, self.r // 2))\n        pt = []\n        # Santi:\n        # TODO: improve this with some proper transposition and stuff\n        # rather than looping, at the expense of more memory I guess\n        for t in range(pad_.size(2) - (self.r - 1)):\n            chunk = pad_[:, :, t:t+self.r].contiguous().view(pad_.size(0),\n                                                             -1).unsqueeze(2)\n            pt.append(chunk)\n        pt = torch.cat(pt, dim=2)\n        return pt\n\n    def __call__(self, pred, gtruth):\n        gtruth_r = self.contextualize_r(gtruth)\n        loss = self.criterion(pred, gtruth_r)\n        return loss\n\n\nclass ZAdversarialLoss(object):\n\n    def __init__(self, z_gen=torch.randn,\n                 batch_acum=1,\n                 grad_reverse=False,\n                 loss=\'L2\'):\n        self.z_gen = z_gen\n        self.batch_acum = batch_acum\n        self.grad_reverse = grad_reverse\n        self.loss = loss\n        if loss == \'L2\':\n            self.criterion = nn.MSELoss()\n        elif loss == \'BCE\':\n            self.criterion = nn.BCEWithLogitsLoss()\n        else:\n            raise ValueError(\'Unrecognized loss \', loss)\n\n    def register_DNet(self, Dnet):\n        self.Dnet = Dnet\n\n    def forward_grad_reverse(self, step, fake, optim, real,\n                             true_lab, fake_lab):\n        dreal = self.Dnet(real) \n        dreal_loss = self.criterion(dreal, true_lab)\n\n        dfake = self.Dnet(fake)\n        dfake_loss = self.criterion(dfake, fake_lab)\n        d_loss = dreal_loss + dfake_loss\n        # backprops through whole structure (even G)\n        # reversing grads in the D -> G transition\n        d_loss.backward(retain_graph=True)\n        if step % self.batch_acum == 0:\n            # step D optimizer\n            optim.step()\n            optim.zero_grad()\n        # all fake and all real\n        return {\'afake_loss\':dfake_loss,\n                \'areal_loss\':dreal_loss}\n\n    def forward_alternate(self, step, fake, optim, real,\n                          true_lab, fake_lab, gfake_exists=False):\n        dreal = self.Dnet(real.detach()) \n        dreal_loss = self.criterion(dreal, true_lab)\n\n        dfake = self.Dnet(fake.detach())\n        dfake_loss = self.criterion(dfake, fake_lab)\n        d_loss = dreal_loss + dfake_loss\n        # backprops through D only\n        d_loss.backward()\n        if step % self.batch_acum == 0:\n            # step D optimizer\n            optim.step()\n            optim.zero_grad()\n\n        greal = self.Dnet(fake)\n        greal_loss = self.criterion(greal, true_lab)\n        ret_losses = {\'dfake_loss\':dfake_loss,\n                      \'dreal_loss\':dreal_loss,\n                      \'d_loss\':d_loss,\n                      \'greal_loss\':greal_loss}\n        if gfake_exists:\n            gfake = self.Dnet(real)\n            gfake_loss = self.criterion(gfake, fake_lab)\n            g_loss = greal_loss + gfake_loss\n            ret_losses[\'gfake_loss\'] = gfake_loss\n        else:\n            g_loss = greal_loss\n        ret_losses[\'g_loss\'] = g_loss\n        return ret_losses\n\n\n    def __call__(self, step, fake, optim, z_true=None,\n                 z_true_trainable=False):\n        if not hasattr(self, \'Dnet\'):\n            raise ValueError(\'Please register Dnet first \'\n                             \'prior to using L2Adversarial Loss.\')\n        if z_true is None:\n            real = self.z_gen(fake.size())\n        else:\n            real = z_true\n\n        lab_1 = torch.ones(real.shape[0], 1, real.shape[2])\n        lab_0 = torch.zeros(lab_1.shape)\n        if fake.is_cuda:\n            real = real.to(\'cuda\')\n            lab_1 = lab_1.to(\'cuda\')\n            lab_0 = lab_0.to(\'cuda\')\n\n        if self.grad_reverse:\n            losses = self.forward_grad_reverse(step, fake, optim,\n                                               z_true, lab_1, lab_0)\n        else:\n            losses = self.forward_alternate(step, fake, optim,\n                                            z_true, lab_1, lab_0,\n                                            z_true_trainable)\n        return losses\n\nclass WaveAdversarialLoss(nn.Module):\n\n    def __init__(self, discriminator, d_optimizer, size_average=True,\n                 loss=\'L2\', batch_acum=1, device=\'cpu\'):\n        super().__init__()\n        self.discriminator = discriminator\n        self.d_optimizer = d_optimizer\n        self.batch_acum = batch_acum\n        if loss == \'L2\':\n            self.loss = nn.MSELoss(size_average)\n            self.labels = [1, -1, 0]\n        elif loss == \'BCE\':\n            self.loss = nn.BCEWithLogitsLoss()\n            self.labels = [1, 0, 1]\n        elif loss == \'Hinge\':\n            self.loss = None\n        else:\n            raise ValueError(\'Urecognized loss: {}\'.format(loss))\n        self.device = device\n\n    def retrieve_label(self, y, lab_value, name=\'\'):\n        #if not hasattr(self, name):\n        label = torch.ones(y.size()) * lab_value\n        label = label.to(self.device)\n        return label\n        #    setattr(self, name, label)\n        #return getattr(self, name)\n\n    def forward(self, iteration, x_fake, x_real, \n                c_real=None, c_fake=None, grad=True):\n        if grad:\n            d_real = self.discriminator(x_real, cond=c_real)\n            if self.loss:\n                rl_lab = self.retrieve_label(d_real, self.labels[0], \'rl_lab\')\n                d_real_loss = self.loss(d_real, rl_lab)\n            else:\n                # hinge loss as vanilla GAN with improved objective\n                d_real_loss = F.relu(1.0 - d_real).mean()\n            \n            d_fake = self.discriminator(x_fake.detach(), cond=c_real)\n            if self.loss:\n                fk_lab = self.retrieve_label(d_fake, self.labels[1], \'fk_lab\')\n                d_fake_loss = self.loss(d_fake, fk_lab)\n            else:\n                # hinge loss as vanilla GAN with improved objective\n                d_fake_loss = F.relu(1.0 + d_fake).mean()\n\n            if c_fake is not None:\n                # an additional label is given to do misalignment signaling\n                d_fake_lab = self.discriminator(x_real,\n                                                cond=c_fake)\n                if self.loss:\n                    d_fake_lab_loss = self.loss(d_fake_lab, fk_lab)\n                else:\n                    d_fake_lab_loss = F.relu(1.0 + d_fake_lab).mean()\n\n                d_loss = d_real_loss + d_fake_loss + d_fake_lab_loss\n            else:\n                d_loss = d_real_loss + d_fake_loss\n\n            d_loss.backward(retain_graph=True)\n            if iteration % self.batch_acum == 0:\n                self.d_optimizer.step()\n                self.d_optimizer.zero_grad()\n\n        g_real = self.discriminator(x_fake, cond=c_real)\n        if self.loss:\n            grl_lab = self.retrieve_label(g_real, self.labels[2], \'grl_lab\')\n            g_real_loss = self.loss(g_real, grl_lab)\n        else:\n            g_real_loss = - g_real.mean()\n        if grad:\n            return {\'g_loss\':g_real_loss, \n                    \'d_real_loss\':d_real_loss,\n                    \'d_fake_loss\':d_fake_loss}\n        else:\n            return {\'g_loss\':g_real_loss}\n\nif __name__ == \'__main__\':\n    loss = ContextualizedLoss(nn.MSELoss(), r=3)\n    pred = torch.randn(1, 3, 5)\n    gtruth= torch.randn(1, 1, 5)\n    loss(pred, gtruth)\n'"
pase/sbatch_writer.py,0,"b'import os\n\nclass submission_writer(object):\n\n    def __init__(self, job_name, out_dir, memory, asr_pth, skp_pth, emo_pth, lang_pth):\n\n        self.job_name = job_name\n        self.out_dir = out_dir\n        self.memory = memory\n        self.tasks = {\'ASR\' : asr_pth, \'spk_id\' : skp_pth, \'EMO\' : emo_pth, \'LANG\' : lang_pth}\n\n    def write(self, sbatch_file_name, cmd):\n        out_dir = ""./downstream_submissions/""\n        if not os.path.exists(out_dir):\n            os.mkdir(out_dir)\n\n        write_slurm_submission_file(os.path.join(out_dir, sbatch_file_name),\n                                    self.job_name,\n                                    self.out_dir,\n                                    self.memory,\n                                    cmd)\n\n    def cmd_maker(self, pase_cfg, latest_ckpt, data_root, res_pth):\n        cmds = []\n        for name, run_file in self.tasks.items():\n            cmd = ""python {} {} {} {} {}\\n"".format(run_file, pase_cfg, latest_ckpt, data_root, res_pth + name)\n            cmds.append(cmd)\n        return cmds\n\n    def __call__(self, sbatch_file_name, pase_cfg, latest_ckpt, data_root, res_pth):\n        cmd = self.cmd_maker(pase_cfg, latest_ckpt, data_root, res_pth)\n        self.write(sbatch_file_name, cmd)\n\n\n\ndef write_slurm_submission_file(sbatch_file_name, job_name, out_dir, memory, run_command_lines, **kwargs):\n    """"""Create a Slurm job submission file based on resource requirements and the set of commands that need to be run.\n    :param sbatch_file_name:\n    :param job_name:\n    :param walltime:\n    :param memory:\n    :param run_command_lines:\n    :param processors:\n    :param partition:\n    :return:\n    """"""\n    writer = open(sbatch_file_name, ""w"")\n    writer.write(""#!/bin/bash\\n\\n"")\n    writer.write(""#SBATCH --job-name="" + job_name + ""\\n"")\n    writer.write(""#SBATCH --nodes=1"" + ""\\n"")\n    writer.write(""#SBATCH --cpus-per-task=8"" + ""\\n"")\n    writer.write(""#SBATCH --mem="" + str(memory) + ""\\n"")\n    writer.write(""#SBATCH --output={}\\n"".format(os.path.join(out_dir, ""{}.%j.out"".format(job_name))))\n    writer.write(""#SBATCH -t 5-00:00:00 \\n"")\n\n    if kwargs is not None:\n        for key, arg in kwargs.items():\n            writer.write(""#SBATCH --{0:s}={1:s}\\n"".format(key, arg))\n\n\n    writer.write(""\\n"")\n    writer.writelines(run_command_lines)\n    writer.close()\n\n\ndef read_slurm_submission_file(sbatch_file_name):\n    """"""Read the Slurm scheduler parameters and run commands from a Slurm job submission file.\n    :param sbatch_file_name:\n    :return:\n    """"""\n    reader = open(sbatch_file_name)\n    sbatch_lines = reader.readlines()\n    reader.close()\n    slurm_parameters = {}\n    for line in [l.strip(""\\n"") for l in sbatch_lines if ""SBATCH"" in l]:\n        splitline = line.split(""--"")[1].split(""="")\n        slurm_parameters[splitline[0]] = splitline[1]\n    run_command_lines = [l for l in sbatch_lines if (""#"" not in l and len(l) > 1)]\n    return (slurm_parameters, run_command_lines)\n\n\n\nif __name__ == ""__main__"":\n    swriter = submission_writer(""test"", ""../log"", memory=32,\n                               asr_pth=""/ASR/run_TIMIT_fast.py"",\n                               skp_pth=""/spk_id/run_minivox_fast.py"",\n                               emo_pth=""/emorec/run/iemocap_fast.py"",\n                               lang_pth=""/??""\n                               )\n\n    swriter(""test_writer"", ""cfg"", ""ckpt"", ""data_root"", ""res_path"")\n'"
pase/transforms.py,68,"b'import torch\nimport torch.nn.functional as F\nimport tqdm\nimport gammatone\nimport tempfile\nfrom gammatone.gtgram import gtgram\nimport numpy as np\nimport subprocess\nimport shlex\nimport random\nimport pysptk\nimport os\nfrom python_speech_features import logfbank\nimport librosa\nimport struct\nimport glob\nimport pickle\nimport soundfile as sf\nfrom scipy import interpolate\nfrom scipy import signal\nfrom scipy.signal import decimate\nfrom scipy.io import loadmat\nimport multiprocessing as mp\nfrom scipy.signal import lfilter, resample\nfrom scipy.interpolate import interp1d\nfrom torchvision.transforms import Compose\nfrom ahoproc_tools.interpolate import interpolation\nfrom ahoproc_tools.io import *\nfrom joblib import Parallel, delayed\n\ntry:\n    import kaldi_io as kio\nexcept ImportError:\n    print (\'kaldi_io is optional, but required when extracting feats with kaldi\')\n\n\n# Make a configurator for the distortions\ndef config_distortions(reverb_irfiles=None, \n                       reverb_fmt=\'imp\',\n                       reverb_data_root=\'.\',\n                       reverb_p=0.5,\n                       reverb_cache=False,\n                       overlap_dir=None,\n                       overlap_list=None,\n                       overlap_snrs=[0, 5, 10],\n                       overlap_reverb=False,\n                       overlap_p=0.5,\n                       noises_dir=None,\n                       noises_snrs=[0, 5, 10],\n                       noises_p=0.5,\n                       noises_cache=False,\n                       speed_range=None,\n                       speed_p=0.5,\n                       resample_factors=[],\n                       resample_p=0.5,\n                       bandrop_irfiles=[],\n                       bandrop_fmt=\'npy\',\n                       bandrop_data_root=\'.\',\n                       bandrop_p=0.5,\n                       downsample_irfiles=[],\n                       downsample_fmt=\'npy\',\n                       downsample_data_root=\'.\',\n                       downsample_p=0.5,\n                       clip_factors=[], \n                       clip_p=0.5,\n                       chop_factors=[],\n                       #chop_factors=[(0.05, 0.025), (0.1, 0.05)], \n                       max_chops=5,\n                       chop_p=0.5,\n                       codec2_p=0.3,\n                       codec2_kbps=1600,\n                       codec2_cachedir=None,\n                       codec2_cache=False,\n                       report=False):\n    trans = []\n    probs = []\n    # first of all, in case we have cached codec2 data\n    if codec2_p > 0. and codec2_cachedir is not None:\n        assert codec2_kbps == 1600, codec2_kbps\n        trans.append(Codec2Cached(cache_dir=codec2_cachedir,\n                                  cache=codec2_cache,\n                                  report=report))\n        probs.append(codec2_p)\n    # Reverb can be shared in two different stages of the pipeline\n    reverb = Reverb(reverb_irfiles, ir_fmt=reverb_fmt,\n                    data_root=reverb_data_root,\n                    cache=reverb_cache,\n                    report=report)\n\n    if reverb_p > 0. and reverb_irfiles is not None:\n        trans.append(reverb)\n        probs.append(reverb_p)\n\n    if overlap_p > 0. and overlap_dir is not None:\n        noise_trans = reverb if overlap_reverb else None\n        trans.append(SimpleAdditiveShift(overlap_dir, overlap_snrs,\n                                         noises_list=overlap_list,\n                                         noise_transform=noise_trans,\n                                         report=report))\n        probs.append(overlap_p)\n\n    if noises_p > 0. and noises_dir is not None:\n        trans.append(SimpleAdditive(noises_dir, noises_snrs, \n                                    cache=noises_cache,\n                                    report=report))\n        probs.append(noises_p)\n\n    if speed_p > 0. and speed_range is not None:\n        # speed changer\n        trans.append(SpeedChange(speed_range, report=report))\n        probs.append(speed_p)\n\n    if resample_p > 0. and len(resample_factors) > 0:\n        trans.append(Resample(resample_factors, report=report))\n        probs.append(resample_p)\n\n    if clip_p > 0. and len(clip_factors) > 0:\n        trans.append(Clipping(clip_factors, report=report))\n        probs.append(clip_p)\n\n    if codec2_p > 0. and codec2_cachedir is None:\n        # codec2 from memory (SLOW)\n        trans.append(Codec2Buffer(report=report, kbps=codec2_kbps))\n        probs.append(codec2_p)\n\n    if chop_p > 0. and len(chop_factors) > 0:\n        trans.append(Chopper(max_chops=max_chops,\n                             chop_factors=chop_factors,\n                             report=report))\n        probs.append(chop_p)\n    if bandrop_p > 0. and bandrop_irfiles is not None:\n        trans.append(BandDrop(bandrop_irfiles,filt_fmt=bandrop_fmt,\n                              data_root=bandrop_data_root,\n                              report=report))\n        probs.append(bandrop_p)\n\n    if downsample_p > 0. and len(downsample_irfiles) > 0:\n        trans.append(Downsample(downsample_irfiles,filt_fmt=downsample_fmt,\n                                data_root=downsample_data_root,\n                                report=report))\n        probs.append(downsample_p)\n\n    if len(trans) > 0:\n        return PCompose(trans, probs=probs, report=report)\n    else:\n        return None\n\ndef norm_and_scale(wav):\n    assert isinstance(wav, torch.Tensor), type(wav)\n    wav = wav / torch.max(torch.abs(wav))\n    return wav * torch.rand(1)\n\n\ndef norm_energy(out_signal, in_signal, eps=1e-14):\n    ienergy = np.dot(in_signal, in_signal)\n    oenergy = np.dot(out_signal, out_signal)\n    return np.sqrt(ienergy / (oenergy + eps)) * out_signal\n\ndef format_package(x):\n    if not isinstance(x, dict):\n        return {\'raw\': x}\n    else:\n        if \'chunk\' not in x:\n            x[\'chunk\'] = x[\'raw\']\n    return x\n\n\nclass ToTensor(object):\n\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        for k, v in pkg.items():\n            # convert everything in the package\n            # into tensors\n            if not isinstance(v, torch.Tensor) and not isinstance(v, str):\n                pkg[k] = torch.tensor(v)\n        return pkg\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'()\'\n\n\nclass ZNorm(object):\n\n    def __init__(self, stats):\n        self.stats_name = stats\n        with open(stats, \'rb\') as stats_f:\n            self.stats = pickle.load(stats_f)\n\n    # @profile\n    def __call__(self, pkg, ignore_keys=[]):\n        pkg = format_package(pkg)\n        for k, st in self.stats.items():\n            # assert k in pkg, \'{} != {}\'.format(list(pkg.keys()),\n            #                                   list(self.stats.keys()))\n            if k in ignore_keys:\n                continue\n            if k in pkg:\n                mean = st[\'mean\'].unsqueeze(1)\n                std = st[\'std\'].unsqueeze(1)\n                pkg[k] = (pkg[k] - mean) / std\n        return pkg\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'({})\'.format(self.stats_name)\n\n\nclass PCompose(object):\n\n    def __init__(self, transforms, probs=0.4, report=False):\n        assert isinstance(transforms, list), type(transforms)\n        self.transforms = transforms\n        self.probs = probs\n        self.report = report\n        if isinstance(probs, list):\n            assert len(transforms) == len(probs), \\\n                \'{} != {}\'.format(len(transforms),\n                                  len(probs))\n\n    #@profile\n    def __call__(self, tensor):\n        x = tensor\n        report = {}\n        for ti, transf in enumerate(self.transforms):\n            if isinstance(self.probs, list):\n                prob = self.probs[ti]\n            else:\n                prob = self.probs\n            if random.random() < prob:\n                x = transf(x)\n                if \'report\' in x:\n                    # get the report\n                    report = x[\'report\']\n        if self.report:\n            return x, report\n        else:\n            return x\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \'(\'\n        for ti, t in enumerate(self.transforms):\n            if isinstance(self.probs, list):\n                prob = self.probs[ti]\n            else:\n                prob = self.probs\n            format_string += \'\\n\'\n            format_string += \'    {0}\'.format(t)\n            format_string += \' >> p={}\'.format(prob)\n        format_string += \'\\n)\'\n        return format_string\n\n\nclass CachedCompose(Compose):\n\n    def __init__(self, transforms, keys, cache_path):\n        super().__init__(transforms)\n        self.cache_path = cache_path\n        self.keys = keys\n        assert len(keys) == len(transforms), \'{} != {}\'.format(len(keys),\n                                                               len(transforms))\n        print(\'Keys: \', keys)\n\n    def __call__(self, x):\n        if \'uttname\' not in x:\n            raise ValueError(\'Utterance name not found when \'\n                             \'looking for cached transforms\')\n        if \'split\' not in x:\n            raise ValueError(\'Split name not found when \'\n                             \'looking for cached transforms\')\n\n        znorm_ignore_flags = []\n        # traverse the keys to look for cache sub-folders\n        for key, t in zip(self.keys, self.transforms):\n            if key == \'totensor\' or key == \'chunk\':\n                x = t(x)\n            elif key == \'znorm\':\n                x = t(x, znorm_ignore_flags)\n            else:\n                aco_dir = os.path.join(self.cache_path, x[\'split\'], key)\n                if os.path.exists(aco_dir):\n                    # look for cached file by name\n                    bname = os.path.splitext(os.path.basename(x[\'uttname\']))[0]\n                    acofile = os.path.join(aco_dir, bname + \'.\' + key)\n                    if not os.path.exists(acofile):\n                        acofile = None\n                    else:\n                        znorm_ignore_flags.append(key)\n                    x = t(x, cached_file=acofile)\n        return x\n\n    def __repr__(self):\n        return super().__repr__()\n\n\nclass SingleChunkWav(object):\n\n    def __init__(self, chunk_size, random_scale=True,\n                 pad_mode=\'reflect\'):\n        self.chunk_size = chunk_size\n        self.random_scale = random_scale\n        self.pad_mode = pad_mode\n\n    def assert_format(self, x):\n        # assert it is a waveform and pytorch tensor\n        assert isinstance(x, torch.Tensor), type(x)\n        # assert x.dim() == 1, x.size()\n\n    #@profile\n    def select_chunk(self, wav, ret_bounds=False, reuse_bounds=None):\n        # select random index\n        chksz = self.chunk_size\n        if len(wav) <= chksz:\n            # padding time\n            P = chksz - len(wav)\n            #if P < len(wav):\n            chk = F.pad(wav.view(1, 1, -1), (0, P), \n                        mode=self.pad_mode).view(-1)\n            #else:\n            #    chk = F.pad(wav.view(1, 1, -1), (0, P), mode=\'replicate\').view(-1)\n            idx = 0\n        elif reuse_bounds is not None:\n            idx, end_i = reuse_bounds\n            # padding that follows is a hack for chime, where segmenteations differ\n            # between mics (by several hundred samples at most) and there may \n            # not be 1:1 correspondence between mics\n            # just a fix to see if it works (its quite rara though)\n            if wav.shape[0] < end_i:\n                #print (""Wshape {}, beg {}, end {}"".format(wav.shape[0], idx, end_i))\n                if idx < wav.shape[0]:\n                    chktmp = wav[idx:]\n                    P = chksz - len(chktmp)\n                    #print (\'Len chktmp {}, P {}\'.format(len(chktmp), P))\n                    if P < len(chktmp):\n                        chk = F.pad(chktmp.view(1, 1, -1), (0, P), mode=\'reflect\').view(-1)\n                    else:\n                        chk = F.pad(chktmp.view(1, 1, -1), (0, P), mode=\'replicate\').view(-1)\n                else:\n                    chk = None\n            else:\n                assert idx >= 0 and \\\n                       idx < end_i and \\\n                       wav.shape[0] >= end_i and \\\n                       chksz == end_i - idx, (\n                   ""Cannot reuse_bounds {} for chksz {} and wav of shape {}""\\\n                             .format(reuse_bounds, chksz, wav.shape)\n                )\n                chk = wav[idx:idx + chksz]\n        else:\n            # idxs = list(range(wav.size(0) - chksz))\n            # idx = random.choice(idxs)\n            idx = np.random.randint(0, wav.size(0) - chksz)\n            chk = wav[idx:idx + chksz]\n        if ret_bounds:\n            return chk, idx, idx + chksz\n        else:\n            return chk\n\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        raw = pkg[\'raw\']\n        self.assert_format(raw)\n        chunk, beg_i, end_i = self.select_chunk(raw, ret_bounds=True)\n        pkg[\'chunk\'] = chunk\n        pkg[\'chunk_beg_i\'] = beg_i\n        pkg[\'chunk_end_i\'] = end_i\n        #to make it compatible with parallel multi-chan data\n        #its backward compatible with single chan\n        if \'raw_clean\' in pkg and pkg[\'raw_clean\'] is not None:\n            raw_clean = pkg[\'raw_clean\']\n            pkg[\'cchunk\'] = self.select_chunk(raw_clean,\\\n                                    reuse_bounds=(beg_i, end_i))\n            if pkg[\'cchunk\'] is None:\n                #in chime5 some parallel seg does not exist, swap clean for these\n                pkg[\'cchunk\'] = pkg[\'chunk\']\n        if self.random_scale:\n            pkg[\'chunk\'] = norm_and_scale(pkg[\'chunk\'])\n            if \'cchunk\' in pkg:\n                pkg[\'cchunk\'] = norm_and_scale(pkg[\'cchunk\'])\n        # specify decimated resolution to be 1 (no decimation) so far\n        pkg[\'dec_resolution\'] = 1\n        return pkg\n\n    def __repr__(self):\n        return self.__class__.__name__ + \\\n               \'({})\'.format(self.chunk_size)\n\n\nclass MIChunkWav(SingleChunkWav):\n    """""" Max-Information chunker expects 3 input wavs,\n        and extract 3 chunks: (chunk, chunk_ctxt,\n        and chunk_rand). The first two correspond to same\n        context, the third one is sampled from the second wav\n    """"""\n\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        if \'raw_rand\' not in pkg:\n            raise ValueError(\'Need at least a pair of wavs to do \'\n                             \'MI chunking! Just got single raw wav?\')\n        raw = pkg[\'raw\']\n        raw_rand = pkg[\'raw_rand\']\n        self.assert_format(raw)\n        self.assert_format(raw_rand)\n        chunk, beg_i, end_i = self.select_chunk(raw, ret_bounds=True)\n        pkg[\'chunk\'] = chunk\n        pkg[\'chunk_beg_i\'] = beg_i\n        pkg[\'chunk_end_i\'] = end_i\n        #added for parallel like corpora with close and distant mics\n        #we do not make asserts here for now if raw is \n        # exactly same as raw_clean, as this was up to segmentation\n        # script\n        #print (""Chunk size is {}"".format(chunk.size()))\n        #print (""Squeezed chunk size is {}"".format(chunk.squeeze(0).size()))\n        if \'raw_clean\' in pkg and pkg[\'raw_clean\'] is not None:\n            raw_clean = pkg[\'raw_clean\']\n            pkg[\'cchunk\'] = self.select_chunk(raw_clean, reuse_bounds=(beg_i, end_i))\n            if pkg[\'cchunk\'] is None:\n                pkg[\'cchunk\'] = pkg[\'chunk\']\n        if \'raw_ctxt\' in pkg and pkg[\'raw_ctxt\'] is not None:\n            raw_ctxt = pkg[\'raw_ctxt\']\n        else:\n            # if no additional chunk is given as raw_ctxt\n            # the same as current raw context is taken\n            # and a random window is selected within\n            raw_ctxt = raw[:]\n        pkg[\'chunk_ctxt\'] = self.select_chunk(raw_ctxt)\n        pkg[\'chunk_rand\'] = self.select_chunk(raw_rand)\n        if self.random_scale:\n            pkg[\'chunk\'] = norm_and_scale(pkg[\'chunk\'])\n            pkg[\'chunk_ctxt\'] = norm_and_scale(pkg[\'chunk_ctxt\'])\n            pkg[\'chunk_rand\'] = norm_and_scale(pkg[\'chunk_rand\'])\n            if \'cchunk\' in pkg:\n                pkg[\'cchunk\'] = norm_and_scale(pkg[\'cchunk\'])\n        # specify decimated resolution to be 1 (no decimation) so far\n        pkg[\'dec_resolution\'] = 1\n        return pkg\n\n\nclass LPS(object):\n\n    def __init__(self, n_fft=2048, hop=160,\n                 win=400, der_order=2,\n                 name=\'lps\',\n                 device=\'cpu\'):\n        self.n_fft = n_fft\n        self.hop = hop\n        self.win = win\n        self.name = name\n        self.der_order=der_order\n        self.device = device\n\n    # @profile\n    def __call__(self, pkg, cached_file=None):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        max_frames = wav.size(0) // self.hop\n        if cached_file is not None:\n            # load pre-computed data\n            X = torch.load(cached_file)\n            beg_i = pkg[\'chunk_beg_i\'] // self.hop\n            end_i = pkg[\'chunk_end_i\'] // self.hop\n            X = X[:, beg_i:end_i]\n            pkg[\'lps\'] = X\n        else:\n            #print (\'Chunks wav shape is {}\'.format(wav.shape))\n            wav = wav.to(self.device)\n            X = torch.stft(wav, self.n_fft,\n                           self.hop, self.win)\n            X = torch.norm(X, 2, dim=2).cpu()[:, :max_frames]\n            X = 10 * torch.log10(X ** 2 + 10e-20).cpu()\n            if self.der_order > 0 :\n                deltas=[X]\n                for n in range(1,self.der_order+1):\n                    deltas.append(librosa.feature.delta(X.numpy(),order=n))\n                X=torch.from_numpy(np.concatenate(deltas))\n     \n            pkg[self.name] = X\n        # Overwrite resolution to hop length\n        pkg[\'dec_resolution\'] = self.hop\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(n_fft={}, hop={}, win={}\'.format(self.n_fft,\n                                                   self.hop,\n                                                   self.win)\n        attrs += \', device={})\'.format(self.device)\n        return self.__class__.__name__ + attrs\n\nclass FBanks(object):\n\n    def __init__(self, n_filters=40, n_fft=512, hop=160,\n                 win=400, rate=16000, der_order=2,\n                 name=\'fbank\',\n                 device=\'cpu\'):\n        self.n_fft = n_fft\n        self.n_filters = n_filters\n        self.rate = rate\n        self.hop = hop\n        self.name = name\n        self.win = win\n        self.der_order=der_order\n        self.name = name\n\n    # @profile\n    def __call__(self, pkg, cached_file=None):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        if torch.is_tensor(wav):\n            wav = wav.data.numpy().astype(np.float32)\n        max_frames = wav.shape[0] // self.hop\n        if cached_file is not None:\n            # load pre-computed data\n            X = torch.load(cached_file)\n            beg_i = pkg[\'chunk_beg_i\'] // self.hop\n            end_i = pkg[\'chunk_end_i\'] // self.hop\n            X = X[:, beg_i:end_i]\n            pkg[self.name] = X\n        else:\n            winlen = (float(self.win) / self.rate)\n            winstep = (float(self.hop) / self.rate)\n            X = logfbank(wav, self.rate, winlen, winstep,\n                         self.n_filters, self.n_fft).T\n            expected_frames = len(wav) // self.hop\n\n            if self.der_order > 0 :\n                deltas=[X]\n                for n in range(1,self.der_order+1):\n                    deltas.append(librosa.feature.delta(X,order=n))\n                X=np.concatenate(deltas)\n\n            fbank = torch.FloatTensor(X)\n            if fbank.shape[1] < expected_frames:\n                P = expected_frames - fbank.shape[1]\n                # pad repeating borders\n                fbank = F.pad(fbank.unsqueeze(0), (0, P), mode=\'replicate\')\n                fbank = fbank.squeeze(0)\n            pkg[self.name] = fbank\n        # Overwrite resolution to hop length\n        pkg[\'dec_resolution\'] = self.hop\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(n_fft={}, n_filters={}, \' \\\n                \'hop={}, win={}\'.format(self.n_fft,\n                                        self.n_filters,\n                                        self.hop,\n                                        self.win)\n        return self.__class__.__name__ + attrs\n\nclass Gammatone(object):\n\n    def __init__(self, f_min=500, n_channels=40, hop=160,\n                 win=400,  der_order=2, rate=16000,\n                 name=\'gtn\',\n                 device=\'cpu\'):\n        self.hop = hop\n        self.win = win\n        self.n_channels = n_channels\n        self.rate = rate\n        self.f_min = f_min\n        self.der_order = der_order\n        self.name = name\n\n    # @profile\n    def __call__(self, pkg, cached_file=None):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        if torch.is_tensor(wav):\n            wav = wav.data.numpy().astype(np.float32)\n        max_frames = wav.shape[0] // self.hop\n        if cached_file is not None:\n            # load pre-computed data\n            X = torch.load(cached_file)\n            beg_i = pkg[\'chunk_beg_i\'] // self.hop\n            end_i = pkg[\'chunk_end_i\'] // self.hop\n            X = X[:, beg_i:end_i]\n            pkg[self.name] = X\n        else:\n            windowtime = float(self.win) / self.rate\n            windowhop = float(self.hop) / self.rate\n            gtn = gammatone.gtgram.gtgram(wav, self.rate, \n                                          windowtime, windowhop,\n                                          self.n_channels,\n                                          self.f_min)\n            gtn = np.log(gtn + 1e-10)\n \n            if self.der_order > 0 :\n                deltas=[gtn]\n                for n in range(1,self.der_order+1):\n                    deltas.append(librosa.feature.delta(gtn,order=n))\n                gtn=np.concatenate(deltas)\n\n            expected_frames = len(wav) // self.hop\n            gtn = torch.FloatTensor(gtn)\n            if gtn.shape[1] < expected_frames:\n                P = expected_frames - gtn.shape[1]\n                # pad repeating borders\n                gtn = F.pad(gtn.unsqueeze(0), (0, P), mode=\'replicate\')\n                gtn = gtn.squeeze(0)\n            #pkg[\'gtn\'] = torch.FloatTensor(gtn[:, :total_frames])\n\n            pkg[self.name] = torch.FloatTensor(gtn)\n        # Overwrite resolution to hop length\n        pkg[\'dec_resolution\'] = self.hop\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(f_min={}, n_channels={}, \' \\\n                \'hop={}, win={})\'.format(self.f_min,\n                                        self.n_channels,\n                                        self.hop,\n                                        self.win)\n        return self.__class__.__name__ + attrs\n\nclass LPC(object):\n\n    def __init__(self, order=25, hop=160,\n                 win=320, name=\'lpc\',\n                 device=\'cpu\'):\n        self.order = order\n        self.hop = hop\n        self.win = win\n        self.window = pysptk.hamming(win).astype(np.float32)\n        self.name = name\n\n    def frame_signal(self, signal, window):\n        \n        frames = []\n        for beg_i in range(0, signal.shape[0], self.hop):\n            frame = signal[beg_i:beg_i + self.win]\n            if len(frame) < self.win:\n                # pad right size with zeros\n                P = self.win - len(frame)\n                frame = np.concatenate((frame,\n                                        np.zeros(P,)), axis=0)\n            frame = frame * window\n            frames.append(frame[None, :])\n        frames = np.concatenate(frames, axis=0)\n        return frames\n\n    # @profile\n    def __call__(self, pkg, cached_file=None):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        if torch.is_tensor(wav):\n            wav = wav.data.numpy().astype(np.float32)\n        max_frames = wav.shape[0] // self.hop\n        if cached_file is not None:\n            # load pre-computed data\n            X = torch.load(cached_file)\n            beg_i = pkg[\'chunk_beg_i\'] // self.hop\n            end_i = pkg[\'chunk_end_i\'] // self.hop\n            X = X[:, beg_i:end_i]\n            pkg[self.name] = X\n        else:\n            wav = self.frame_signal(wav, self.window)\n            #print(\'wav shape: \', wav.shape)\n            lpc = pysptk.sptk.lpc(wav, order=self.order)\n            #print(\'lpc: \', lpc.shape)\n            pkg[self.name] = torch.FloatTensor(lpc)\n        # Overwrite resolution to hop length\n        pkg[\'dec_resolution\'] = self.hop\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(order={}, hop={}, win={})\'.format(self.order,\n                                                    self.hop,\n                                                    self.win)\n        return self.__class__.__name__ + attrs\n\nclass MFCC(object):\n\n    def __init__(self, n_fft=2048, hop=160,\n                 order=13, sr=16000, win=400,\n                 der_order=2, name=\'mfcc\'):\n        self.hop = hop\n        # Santi: the librosa mfcc api does not always\n        # accept a window argument, so we enforce n_fft\n        # to be window to ensure the window len restriction\n        #self.win = win\n        self.n_fft = win\n        self.order = order\n        self.sr = 16000\n        self.der_order=der_order\n        self.name = name\n\n    # @profile\n    def __call__(self, pkg, cached_file=None):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        y = wav.data.numpy()\n        max_frames = y.shape[0] // self.hop\n        if cached_file is not None:\n            # load pre-computed data\n            mfcc = torch.load(cached_file)\n            beg_i = pkg[\'chunk_beg_i\'] // self.hop\n            end_i = pkg[\'chunk_end_i\'] // self.hop\n            mfcc = mfcc[:, beg_i:end_i]\n            pkg[self.name] = mfcc\n        else:\n            # print(y.dtype)\n            mfcc = librosa.feature.mfcc(y, sr=self.sr,\n                                        n_mfcc=self.order,\n                                        n_fft=self.n_fft,\n                                        hop_length=self.hop,\n                                        #win_length=self.win,\n                                        )[:, :max_frames]\n            if self.der_order > 0 :\n                deltas=[mfcc]\n                for n in range(1,self.der_order+1):\n                    deltas.append(librosa.feature.delta(mfcc,order=n))\n                mfcc=np.concatenate(deltas)\n    \n            pkg[self.name] = torch.tensor(mfcc.astype(np.float32))\n        # Overwrite resolution to hop length\n        pkg[\'dec_resolution\'] = self.hop\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(order={}, sr={})\'.format(self.order,\n                                           self.sr)\n        return self.__class__.__name__ + attrs\n\nclass MFCC_librosa(object):\n\n    def __init__(self, n_fft=2048, hop=160,\n                 order=13, sr=16000, win=400,der_order=2,n_mels=40,\n                 htk=True, name=\'mfcc_librosa\'):\n        self.hop = hop\n        # Santi: the librosa mfcc api does not always\n        # accept a window argument, so we enforce n_fft\n        # to be window to ensure the window len restriction\n        #self.win = win\n        self.n_fft = win\n        self.order = order\n        self.sr = 16000\n        self.der_order=der_order\n        self.n_mels=n_mels\n        self.htk=True\n        self.name = name\n\n    # @profile\n    def __call__(self, pkg, cached_file=None):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        y = wav.data.numpy()\n        max_frames = y.shape[0] // self.hop\n        if cached_file is not None:\n            # load pre-computed data\n            mfcc = torch.load(cached_file)\n            beg_i = pkg[\'chunk_beg_i\'] // self.hop\n            end_i = pkg[\'chunk_end_i\'] // self.hop\n            mfcc = mfcc[:, beg_i:end_i]\n            pkg[self.name] = mfcc\n        else:\n            # print(y.dtype)\n            mfcc = librosa.feature.mfcc(y, sr=self.sr,\n                                        n_mfcc=self.order,\n                                        n_fft=self.n_fft,\n                                        hop_length=self.hop,\n                                        #win_length=self.win,\n\t\t\t\t\tn_mels=self.n_mels,\n                                        htk=self.htk,\n                                        )[:, :max_frames]\n            if self.der_order > 0 :\n                deltas=[mfcc]\n                for n in range(1,self.der_order+1):\n                    deltas.append(librosa.feature.delta(mfcc,order=n))\n                mfcc=np.concatenate(deltas)\n\n            pkg[self.name] = torch.tensor(mfcc.astype(np.float32))\n        # Overwrite resolution to hop length\n        pkg[\'dec_resolution\'] = self.hop\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(order={}, sr={})\'.format(self.order,\n                                           self.sr)\n        return self.__class__.__name__ + attrs\n\nclass KaldiFeats(object):\n    def __init__(self, kaldi_root, hop=160, win=400, sr=16000):\n\n        if kaldi_root is None and \'KALDI_ROOT\' in os.environ:\n            kaldi_root = os.environ[\'KALDI_ROOT\']\n\n        assert kaldi_root is not None, (\n            ""Set KALDI_ROOT (either pass via cmd line, or set env variable)""\n        )\n\n        self.kaldi_root = kaldi_root\n        self.hop = hop\n        self.win = win\n        self.sr = sr\n\n        self.frame_shift = int(1000./self.sr * self.hop) #in ms\n        self.frame_length = int(1000./self.sr * self.win) #in ms\n\n    def __execute_command__(self, datain, cmd):\n        #try:\n        fin, fout = kio.open_or_fd(cmd, \'wb\')\n        kio.write_wav(fin, datain, self.sr, key=\'utt\')\n        fin.close() #so its clear nothing new arrives\n        feats_ark = kio.read_mat_ark(fout)\n        for _, feats in feats_ark:\n            fout.close()\n            return feats.T #there is only one to read\n        #except Exception as e:\n        #    print (e)\n        #    return None\n\n    def __repr__(self):\n        return self.__class__.__name__\n\nclass KaldiMFCC(KaldiFeats):\n    def __init__(self, kaldi_root, hop=160, win=400, sr=16000,\n                    num_mel_bins=40, num_ceps=13, der_order=2,\n                    name=\'kaldimfcc\'):\n\n        super(KaldiMFCC, self).__init__(kaldi_root=kaldi_root, \n                                        hop=hop, win=win, sr=sr)\n\n        self.num_mel_bins = num_mel_bins\n        self.num_ceps = num_ceps\n        self.der_order=der_order\n\n        cmd = ""ark:| {}/src/featbin/compute-mfcc-feats --print-args=false ""\\\n               ""--use-energy=false --snip-edges=false --num-ceps={} ""\\\n               ""--frame-length={} --frame-shift={} ""\\\n               ""--num-mel-bins={} --sample-frequency={} ark:- ark:- |""\\\n               "" {}/src/featbin/add-deltas --print-args=false ""\\\n               ""--delta-order={} ark:- ark:- |""\n\n        self.cmd = cmd.format(self.kaldi_root, self.num_ceps,\n                              self.frame_length, self.frame_shift,\n                              self.num_mel_bins, self.sr, self.kaldi_root,\n                              self.der_order)\n        self.name = name\n\n    def __call__(self, pkg, cached_file=None):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        y = wav.data.numpy()\n        max_frames = y.shape[0] // self.hop\n        if cached_file is not None:\n            # load pre-computed data\n            mfcc = torch.load(cached_file)\n            beg_i = pkg[\'chunk_beg_i\'] // self.hop\n            end_i = pkg[\'chunk_end_i\'] // self.hop\n            mfcc = mfcc[:, beg_i:end_i]\n            pkg[self.name] = mfcc\n        else:\n            # print(y.dtype)\n            mfccs = self.__execute_command__(y, self.cmd)\n            assert mfccs is not None, (\n                ""Mfccs extraction failed""\n            )\n            pkg[self.name] = torch.tensor(mfccs[:,:max_frames].astype(np.float32))\n\n        # Overwrite resolution to hop length\n        pkg[\'dec_resolution\'] = self.hop\n        return pkg\n\n    def __repr__(self):\n        attrs = ""(bins={}, ceps={}, sr={})""\\\n                  .format(self.num_mel_bins, self.num_ceps, self.sr)\n        return self.__class__.__name__ + attrs\n\nclass KaldiPLP(KaldiFeats):\n    def __init__(self, kaldi_root, hop=160, win=400, sr=16000,\n                 num_mel_bins=20, num_ceps=20, lpc_order=20,\n                 name=\'kaldiplp\'):\n\n        super(KaldiPLP, self).__init__(kaldi_root=kaldi_root, \n                                        hop=hop, win=win, sr=sr)\n\n        self.num_mel_bins = num_mel_bins\n        self.num_ceps = num_ceps\n        self.lpc_order = lpc_order\n\n        cmd = ""ark:| {}/src/featbin/compute-plp-feats ""\\\n               ""--print-args=false --snip-edges=false --use-energy=false ""\\\n               ""--num-ceps={} --lpc-order={} ""\\\n               ""--frame-length={} --frame-shift={} ""\\\n               ""--num-mel-bins={} --sample-frequency={} ""\\\n               ""ark:- ark:- |""\n\n        self.cmd = cmd.format(self.kaldi_root, self.num_ceps, self.lpc_order, \n                              self.frame_length, self.frame_shift, \n                              self.num_mel_bins, self.sr)\n        self.name = name\n\n    def __call__(self, pkg, cached_file=None):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        y = wav.data.numpy()\n        max_frames = y.shape[0] // self.hop\n        if cached_file is not None:\n            # load pre-computed data\n            plp = torch.load(cached_file)\n            beg_i = pkg[\'chunk_beg_i\'] // self.hop\n            end_i = pkg[\'chunk_end_i\'] // self.hop\n            plp = plp[:, beg_i:end_i]\n            pkg[self.name] = plp\n        else:\n            # print(y.dtype)\n            feats = self.__execute_command__(y, self.cmd)\n            pkg[self.name] = torch.tensor(feats[:,:max_frames].astype(np.float32))\n        \n        # Overwrite resolution to hop length\n        pkg[\'dec_resolution\'] = self.hop\n        return pkg\n\n    def __repr__(self):\n        attrs = ""(bins={}, ceps={}, sr={}, lpc={})""\\\n                  .format(self.num_mel_bins, self.num_ceps, self.sr, self.lpc_order)\n        return self.__class__.__name__ + attrs\n\nclass Prosody(object):\n\n    def __init__(self, hop=160, win=320, f0_min=60, f0_max=300,der_order=2,\n                 sr=16000, name=\'prosody\'):\n        self.hop = hop\n        self.win = win\n        self.f0_min = f0_min\n        self.f0_max = f0_max\n        self.sr = sr\n        self.der_order = der_order\n        self.name = name\n\n    # @profile\n    def __call__(self, pkg, cached_file=None):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        wav = wav.data.numpy()\n        max_frames = wav.shape[0] // self.hop\n        if cached_file is not None:\n            # load pre-computed data\n            proso = torch.load(cached_file)\n            beg_i = pkg[\'chunk_beg_i\'] // self.hop\n            end_i = pkg[\'chunk_end_i\'] // self.hop\n            proso = proso[:, beg_i:end_i]\n            pkg[self.name] = proso\n        else:\n            # first compute logF0 and voiced/unvoiced flag\n            # f0 = pysptk.rapt(wav.astype(np.float32),\n            #                 fs=self.sr, hopsize=self.hop,\n            #                 min=self.f0_min, max=self.f0_max,\n            #                 otype=\'f0\')\n            f0 = pysptk.swipe(wav.astype(np.float64),\n                              fs=self.sr, hopsize=self.hop,\n                              min=self.f0_min,\n                              max=self.f0_max,\n                              otype=\'f0\')\n            # sound = pm.Sound(wav.astype(np.float32), self.sr)\n            # f0 = sound.to_pitch(self.hop / 16000).selected_array[\'frequency\']\n            if len(f0) < max_frames:\n                pad = max_frames - len(f0)\n                f0 = np.concatenate((f0, f0[-pad:]), axis=0)\n            lf0 = np.log(f0 + 1e-10)\n            lf0, uv = interpolation(lf0, -1)\n            lf0 = torch.tensor(lf0.astype(np.float32)).unsqueeze(0)[:, :max_frames]\n            uv = torch.tensor(uv.astype(np.float32)).unsqueeze(0)[:, :max_frames]\n            if torch.sum(uv) == 0:\n                # if frame is completely unvoiced, make lf0 min val\n                lf0 = torch.ones(uv.size()) * np.log(self.f0_min)\n            # assert lf0.min() > 0, lf0.data.numpy()\n            # secondly obtain zcr\n            zcr = librosa.feature.zero_crossing_rate(y=wav,\n                                                     frame_length=self.win,\n                                                     hop_length=self.hop)\n            zcr = torch.tensor(zcr.astype(np.float32))\n            zcr = zcr[:, :max_frames]\n            # finally obtain energy\n            egy = librosa.feature.rmse(y=wav, frame_length=self.win,\n                                       hop_length=self.hop,\n                                       pad_mode=\'constant\')\n            egy = torch.tensor(egy.astype(np.float32))\n            egy = egy[:, :max_frames]\n            proso = torch.cat((lf0, uv, egy, zcr), dim=0)\n  \n            if self.der_order > 0 :\n                deltas=[proso]\n                for n in range(1,self.der_order+1):\n                    deltas.append(librosa.feature.delta(proso.numpy(),order=n))\n                proso=torch.from_numpy(np.concatenate(deltas))\n\n            pkg[self.name] = proso\n        # Overwrite resolution to hop length\n        pkg[\'dec_resolution\'] = self.hop\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(hop={}, win={}, f0_min={}, f0_max={}\'.format(self.hop,\n                                                               self.win,\n                                                               self.f0_min,\n                                                               self.f0_max)\n        attrs += \', sr={})\'.format(self.sr)\n        return self.__class__.__name__ + attrs\n\nclass Reverb(object):\n\n    def __init__(self, ir_files, report=False, ir_fmt=\'mat\',\n                 max_reverb_len=24000,\n                 cache=False,\n                 data_root=\'.\'):\n        if len(ir_files) == 0:\n            # list the directory\n            ir_files = [os.path.basename(f) for f in glob.glob(os.path.join(data_root,\n                                              \'*.{}\'.format(ir_fmt)))]\n            print(\'Found {} *.{} ir_files in {}\'.format(len(ir_files),\n                                                        ir_fmt,\n                                                        data_root))\n        self.ir_files = ir_files\n        assert isinstance(ir_files, list), type(ir_files)\n        assert len(ir_files) > 0, len(ir_files)\n        self.ir_idxs = list(range(len(ir_files)))\n        # self.IR, self.p_max = self.load_IR(ir_file, ir_fmt)\n        self.ir_fmt = ir_fmt\n        self.report = report\n        self.data_root = data_root\n        self.max_reverb_len = max_reverb_len\n        if cache:\n            self.cache = {}\n            for ir_file in self.ir_files:\n                self.load_IR(ir_file, ir_fmt)\n\n    def load_IR(self, ir_file, ir_fmt):\n        ir_file = os.path.join(self.data_root, ir_file)\n        # print(\'loading ir_file: \', ir_file)\n        if hasattr(self, \'cache\') and ir_file in self.cache:\n            return self.cache[ir_file]\n        else:\n            if ir_fmt == \'mat\':\n                IR = loadmat(ir_file, squeeze_me=True, struct_as_record=False)\n                IR = IR[\'risp_imp\']\n            elif ir_fmt == \'imp\' or ir_fmt == \'txt\':\n                IR = np.loadtxt(ir_file)\n            elif ir_fmt == \'npy\':\n                IR = np.load(ir_file)\n            elif ir_fmt == \'wav\':\n                IR, _ = sf.read(ir_file)\n            else:\n                raise TypeError(\'Unrecognized IR format: \', ir_fmt)\n            IR = IR[:self.max_reverb_len]\n            if np.max(IR)>0:\n                IR = IR / np.abs(np.max(IR))\n            p_max = np.argmax(np.abs(IR))\n            if hasattr(self, \'cache\'):\n                self.cache[ir_file] = (IR, p_max)\n            return IR, p_max\n\n    def shift(self, xs, n):\n        e = np.empty_like(xs)\n        if n >= 0:\n            e[:n] = 0.0\n            e[n:] = xs[:-n]\n        else:\n            e[n:] = 0.0\n            e[:n] = xs[-n:]\n        return e\n\n    def sample_IR(self):\n        if len(self.ir_files) == 0:\n            return self.ir_files[0]\n        else:\n            idx = random.choice(self.ir_idxs)\n            return self.ir_files[idx]\n\n    ##@profile\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        # sample an ir_file\n        ir_file = self.sample_IR()\n        IR, p_max = self.load_IR(ir_file, self.ir_fmt)\n        IR = IR.astype(np.float32)\n        wav = wav.data.numpy().reshape(-1)\n        Ex = np.dot(wav, wav)\n        wav = wav.astype(np.float32).reshape(-1)\n        # wav = wav / np.max(np.abs(wav))\n        # rev = signal.fftconvolve(wav, IR, mode=\'full\')\n        rev = signal.convolve(wav, IR, mode=\'full\').reshape(-1)\n        Er = np.dot(rev, rev)\n        # rev = rev / np.max(np.abs(rev))\n        # IR delay compensation\n        rev = self.shift(rev, -p_max)\n        if Er > 0:\n            Eratio = np.sqrt(Ex / Er) \n        else:\n            Eratio = 1.0\n            #rev = rev / np.max(np.abs(rev))\n\n        # Trim rev signal to match clean length\n        rev = rev[:wav.shape[0]]\n        rev = Eratio * rev\n        rev = torch.FloatTensor(rev)\n        if self.report:\n            if \'report\' not in pkg:\n                pkg[\'report\'] = {}\n            pkg[\'report\'][\'ir_file\'] = ir_file\n        pkg[\'chunk\'] = rev\n        return pkg\n\n    def __repr__(self):\n        if len(self.ir_files) > 3:\n            attrs = \'(ir_files={} ...)\'.format(self.ir_files[:3])\n        else:\n            attrs = \'(ir_files={})\'.format(self.ir_files)\n        return self.__class__.__name__ + attrs\n\n\nclass Downsample(object):\n\n    def __init__(self, filt_files, report=False, filt_fmt=\'npy\',\n                 data_root=\'.\'):\n        self.filt_files = filt_files\n        assert isinstance(filt_files, list), type(filt_files)\n        assert len(filt_files) > 0, len(filt_files)\n        self.filt_idxs = list(range(len(filt_files)))\n        self.filt_fmt = filt_fmt\n        self.report = report\n        self.data_root = data_root\n\n    def load_filter(self, filt_file, filt_fmt):\n\n        filt_file = os.path.join(self.data_root, filt_file)\n\n        if filt_fmt == \'mat\':\n            filt_coeff = loadmat(filt_file, squeeze_me=True, struct_as_record=False)\n            filt_coeff = filt_coeff[\'filt_coeff\']\n\n        elif filt_fmt == \'imp\' or filt_fmt == \'txt\':\n            filt_coeff = np.loadtxt(filt_file)\n        elif filt_fmt == \'npy\':\n            filt_coeff = np.load(filt_file)\n        else:\n            raise TypeError(\'Unrecognized filter format: \', filt_fmt)\n\n        filt_coeff = filt_coeff / np.abs(np.max(filt_coeff))\n\n        return filt_coeff\n\n    def shift(self, xs, n):\n        e = np.empty_like(xs)\n        if n >= 0:\n            e[:n] = 0.0\n            e[n:] = xs[:-n]\n        else:\n            e[n:] = 0.0\n            e[:n] = xs[-n:]\n        return e\n\n    def sample_filt(self):\n        if len(self.filt_files) == 0:\n            return self.filt_files[0]\n        else:\n            idx = random.choice(self.filt_idxs)\n            return self.filt_files[idx]\n\n    ##@profile\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        # sample a filter\n        filt_file = self.sample_filt()\n        filt_coeff = self.load_filter(filt_file, self.filt_fmt)\n        filt_coeff = filt_coeff.astype(np.float32)\n        wav = wav.data.numpy().reshape(-1)\n        Ex = np.dot(wav, wav)\n        wav = wav.astype(np.float32).reshape(-1)\n\n        sig_filt = signal.convolve(wav, filt_coeff, mode=\'full\').reshape(-1)\n\n        sig_filt = self.shift(sig_filt, -round(filt_coeff.shape[0] / 2))\n\n        sig_filt = sig_filt[:wav.shape[0]]\n\n        # sig_filt=sig_filt/np.max(np.abs(sig_filt))\n\n        Efilt = np.dot(sig_filt, sig_filt)\n        # Ex = np.dot(wav, wav)\n\n        if Efilt > 0:\n            Eratio = np.sqrt(Ex / Efilt)\n        else:\n            Eratio = 1.0\n            sig_filt = wav\n\n        sig_filt = Eratio * sig_filt\n        sig_filt = torch.FloatTensor(sig_filt)\n        if self.report:\n            if \'report\' not in pkg:\n                pkg[\'report\'] = {}\n            pkg[\'report\'][\'filt_file\'] = filt_file\n        pkg[\'chunk\'] = sig_filt\n        return pkg\n\n    def __repr__(self):\n        if len(self.filt_files) > 3:\n            attrs = \'(filt_files={} ...)\'.format(self.filt_files[:3])\n        else:\n            attrs = \'(filt_files={})\'.format(self.filt_files)\n        return self.__class__.__name__ + attrs\n\n\nclass BandDrop(object):\n\n    def __init__(self, filt_files, report=False, filt_fmt=\'npy\',\n                 data_root=\'.\'):\n        if len(filt_files) == 0:\n            # list the directory\n            filt_files = [os.path.basename(f) for f in glob.glob(os.path.join(data_root,\n                                              \'*.{}\'.format(filt_fmt)))]\n            print(\'Found {} *.{} filt_files in {}\'.format(len(filt_files),\n                                                          filt_fmt,\n                                                          data_root))\n        self.filt_files = filt_files\n        assert isinstance(filt_files, list), type(filt_files)\n        assert len(filt_files) > 0, len(filt_files)\n        self.filt_idxs = list(range(len(filt_files)))\n        self.filt_fmt = filt_fmt\n        self.report = report\n        self.data_root = data_root\n\n    def load_filter(self, filt_file, filt_fmt):\n\n        filt_file = os.path.join(self.data_root, filt_file)\n\n        if filt_fmt == \'mat\':\n            filt_coeff = loadmat(filt_file, squeeze_me=True, struct_as_record=False)\n            filt_coeff = filt_coeff[\'filt_coeff\']\n\n        elif filt_fmt == \'imp\' or filt_fmt == \'txt\':\n            filt_coeff = np.loadtxt(filt_file)\n        elif filt_fmt == \'npy\':\n            filt_coeff = np.load(filt_file)\n        else:\n            raise TypeError(\'Unrecognized filter format: \', filt_fmt)\n\n        filt_coeff = filt_coeff / np.abs(np.max(filt_coeff))\n\n        return filt_coeff\n\n    def shift(self, xs, n):\n        e = np.empty_like(xs)\n        if n >= 0:\n            e[:n] = 0.0\n            e[n:] = xs[:-n]\n        else:\n            e[n:] = 0.0\n            e[:n] = xs[-n:]\n        return e\n\n    def sample_filt(self):\n        if len(self.filt_files) == 0:\n            return self.filt_files[0]\n        else:\n            idx = random.choice(self.filt_idxs)\n            return self.filt_files[idx]\n\n    ##@profile\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        # sample a filter\n        filt_file = self.sample_filt()\n        filt_coeff = self.load_filter(filt_file, self.filt_fmt)\n        filt_coeff = filt_coeff.astype(np.float32)\n        wav = wav.data.numpy().reshape(-1)\n        Ex = np.dot(wav, wav)\n        wav = wav.astype(np.float32).reshape(-1)\n\n        sig_filt = signal.convolve(wav, filt_coeff, mode=\'full\').reshape(-1)\n\n        sig_filt = self.shift(sig_filt, -round(filt_coeff.shape[0] / 2))\n\n        sig_filt = sig_filt[:wav.shape[0]]\n\n        # sig_filt=sig_filt/np.max(np.abs(sig_filt))\n\n        Efilt = np.dot(sig_filt, sig_filt)\n        # Ex = np.dot(wav, wav)\n        if Efilt > 0:\n            Eratio = np.sqrt(Ex / Efilt)\n        else:\n            Eratio = 1.0\n            sig_filt = wav\n\n        sig_filt = Eratio * sig_filt\n        sig_filt = torch.FloatTensor(sig_filt)\n        if self.report:\n            if \'report\' not in pkg:\n                pkg[\'report\'] = {}\n            pkg[\'report\'][\'filt_file\'] = filt_file\n        pkg[\'chunk\'] = sig_filt\n        return pkg\n\n    def __repr__(self):\n        if len(self.filt_files) > 3:\n            attrs = \'(filt_files={} ...)\'.format(self.filt_files[:3])\n        else:\n            attrs = \'(filt_files={})\'.format(self.filt_files)\n        return self.__class__.__name__ + attrs\n\n\nclass Scale(object):\n    """"""Scale audio tensor from a 16-bit integer (represented as a FloatTensor)\n    to a floating point number between -1.0 and 1.0.  Note the 16-bit number is\n    called the ""bit depth"" or ""precision"", not to be confused with ""bit rate"".\n    Args:\n        factor (int): maximum value of input tensor. default: 16-bit depth\n    """"""\n\n    def __init__(self, factor=2 ** 31):\n        self.factor = factor\n\n    def __call__(self, tensor):\n        """"""\n        Args:\n            tensor (Tensor): Tensor of audio of size (Samples x Channels)\n        Returns:\n            Tensor: Scaled by the scale factor. (default between -1.0 and 1.0)\n        """"""\n        if isinstance(tensor, (torch.LongTensor, torch.IntTensor)):\n            tensor = tensor.float()\n\n        return tensor / self.factor\n\n\nclass SimpleChopper(object):\n    """""" Do not use VAD to specify speech regions, just\n        cut randomly some number of regions randomly\n    """"""\n\n    def __init__(self, chop_factors=[(0.05, 0.025), (0.1, 0.05)],\n                 max_chops=5, report=False):\n        self.chop_factors = chop_factors\n        self.max_chops = max_chops\n        self.report = report\n\n    def chop_wav(self, wav):\n        # TODO: finish this\n        raise NotImplementedError(\'Need to be finished\')\n        chop_factors = self.chop_factors\n        # get num of chops to make\n        chops = np.random.randint(1, self.max_chops + 1)\n        # build random indexes to randomly pick regions, not ordered\n        if chops == 1:\n            chop_idxs = [0]\n        else:\n            chop_idxs = np.random.choice(list(range(chops)), chops,\n                                         replace=False)\n        chopped_wav = np.copy(wav)\n        return None\n\n    def __call__(self, pkg, srate=16000):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        # unorm to 16-bit scale for VAD in chopper\n        wav = wav.data.numpy().astype(np.float32)\n        # get speech regions for proper chopping\n        chopped = self.chop_wav(wav)\n        chopped = self.normalizer(torch.FloatTensor(chopped))\n        if self.report:\n            if \'report\' not in pkg:\n                pkg[\'report\'] = {}\n            pkg[\'report\'][\'speech_regions\'] = speech_regions\n        pkg[\'chunk\'] = chopped\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(chop_factors={}, max_chops={})\'.format(\n            self.chop_factors,\n            self.max_chops\n        )\n\n\nclass Chopper(object):\n    def __init__(self, chop_factors=[(0.05, 0.025), (0.1, 0.05)],\n                 max_chops=2, force_regions=False, report=False):\n        # chop factors in seconds (mean, std) per possible chop\n        import webrtcvad\n        self.chop_factors = chop_factors\n        self.max_chops = max_chops\n        self.force_regions = force_regions\n        # create VAD to get speech chunks\n        self.vad = webrtcvad.Vad(2)\n        # make scalers to norm/denorm\n        self.denormalizer = Scale(1. / ((2 ** 15) - 1))\n        self.normalizer = Scale((2 ** 15) - 1)\n        self.report = report\n\n    # @profile\n    def vad_wav(self, wav, srate):\n        """""" Detect the voice activity in the 16-bit mono PCM wav and return\n            a list of tuples: (speech_region_i_beg_sample, center_sample,\n            region_duration)\n        """"""\n        if srate != 16000:\n            raise ValueError(\'Sample rate must be 16kHz\')\n        window_size = 160  # samples\n        regions = []\n        curr_region_counter = 0\n        init = None\n        vad = self.vad\n        if self.force_regions:\n            # Divide the signal into even regions depending on number of chops\n            # to put\n            nregions = wav.shape[0] // self.max_chops\n            reg_len = wav.shape[0] // nregions\n            for beg_i in range(0, wav.shape[0], reg_len):\n                end_sample = beg_i + reg_len\n                center_sample = beg_i + (end_sample - beg_i) / 2\n                regions.append((beg_i, center_sample,\n                                reg_len))\n            return regions\n        else:\n            # Use the VAD to determine actual speech regions\n            for beg_i in range(0, wav.shape[0], window_size):\n                frame = wav[beg_i:beg_i + window_size]\n                if frame.shape[0] >= window_size and \\\n                        vad.is_speech(struct.pack(\'{}i\'.format(window_size),\n                                                  *frame), srate):\n                    curr_region_counter += 1\n                    if init is None:\n                        init = beg_i\n                else:\n                    # end of speech region (or never began yet)\n                    if init is not None:\n                        # close the region\n                        end_sample = init + (curr_region_counter * window_size)\n                        center_sample = init + (end_sample - init) / 2\n                        regions.append((init, center_sample,\n                                        curr_region_counter * window_size))\n                    init = None\n                    curr_region_counter = 0\n            return regions\n\n    # @profile\n    def chop_wav(self, wav, srate, speech_regions):\n        if len(speech_regions) == 0:\n            # print(\'Skipping no speech regions\')\n            return wav, []\n        chop_factors = self.chop_factors\n        # get num of chops to make\n        num_chops = list(range(1, self.max_chops + 1))\n        chops = np.asscalar(np.random.choice(num_chops, 1))\n        # trim it to available regions\n        chops = min(chops, len(speech_regions))\n        #print(\'Making {} chops\'.format(chops))\n        # build random indexes to randomly pick regions, not ordered\n        if chops == 1:\n            chop_idxs = [0]\n        else:\n            chop_idxs = np.random.choice(list(range(chops)), chops,\n                                         replace=False)\n        chopped_wav = np.copy(wav)\n        chops_log = []\n        # make a chop per chosen region\n        for chop_i in chop_idxs:\n            region = speech_regions[chop_i]\n            # decompose the region\n            reg_beg, reg_center, reg_dur = region\n            # pick random chop_factor\n            chop_factor_idx = np.random.choice(range(len(chop_factors)), 1)[0]\n            chop_factor = chop_factors[chop_factor_idx]\n            # compute duration from: std * N(0, 1) + mean\n            mean, std = chop_factor\n            chop_dur = mean + np.random.randn(1) * std\n            # convert dur to samples\n            chop_s_dur = int(chop_dur * srate)\n            chop_beg = max(int(reg_center - (chop_s_dur / 2)), reg_beg)\n            chop_end = min(int(reg_center + (chop_s_dur / 2)), reg_beg +\n                           reg_dur)\n            # print(\'chop_beg: \', chop_beg)\n            # print(\'chop_end: \', chop_end)\n            # chop the selected region with computed dur\n            chopped_wav[chop_beg:chop_end] = 0\n            chops_log.append(float(chop_dur))\n        return chopped_wav, chops_log\n\n    #@profile\n    def __call__(self, pkg, srate=16000):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        # unorm to 16-bit scale for VAD in chopper\n        wav = self.denormalizer(wav)\n        wav = wav.data.numpy()\n        wav = wav.astype(np.int16)\n        if wav.ndim > 1:\n            wav = wav.reshape((-1,))\n        # get speech regions for proper chopping\n        speech_regions = self.vad_wav(wav, srate)\n        chopped, chops = self.chop_wav(wav, srate,\n                                       speech_regions)\n        chopped = chopped.astype(np.float32)\n        chopped = self.normalizer(torch.from_numpy(chopped))\n        if self.report:\n            if \'report\' not in pkg:\n                pkg[\'report\'] = {}\n            pkg[\'report\'][\'chops\'] = chops\n        pkg[\'chunk\'] = chopped\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(chop_factors={}, max_chops={})\'.format(\n            self.chop_factors,\n            self.max_chops\n        )\n        return self.__class__.__name__ + attrs\n\n\nclass Clipping(object):\n\n    def __init__(self, clip_factors=[0.3, 0.4, 0.5],\n                 report=False):\n        self.clip_factors = clip_factors\n        self.report = report\n\n    #@profile\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        wav = wav.data.numpy().astype(np.float32)\n        # cf = np.random.choice(self.clip_factors, 1)\n        cf = random.choice(self.clip_factors)\n        clip = np.maximum(wav, cf * np.min(wav))\n        clip = np.minimum(clip, cf * np.max(wav))\n        clipT = torch.FloatTensor(clip)\n        if self.report:\n            if \'report\' not in pkg:\n                pkg[\'report\'] = {}\n            pkg[\'report\'][\'clip_factor\'] = cf\n        pkg[\'chunk\'] = clipT\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(clip_factors={})\'.format(\n            self.clip_factors\n        )\n        return self.__class__.__name__ + attrs\n\n\nclass Resample(object):\n\n    def __init__(self, factors=[4], report=False):\n        self.factors = factors\n        self.report = report\n\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        wav = wav.data.numpy()\n        factor = random.choice(self.factors)\n        x_lr = decimate(wav, factor).copy()\n        x_lr = torch.FloatTensor(x_lr)\n        x_ = F.interpolate(x_lr.view(1, 1, -1),\n                           scale_factor=factor,\n                           align_corners=True,\n                           mode=\'linear\').view(-1)\n        if self.report:\n            if \'report\' not in pkg:\n                pkg[\'report\'] = {}\n            pkg[\'report\'][\'resample_factor\'] = factor\n        pkg[\'chunk\'] = x_\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(factor={})\'.format(\n            self.factors\n        )\n        return self.__class__.__name__ + attrs\n\n\nclass SimpleAdditive(object):\n\n    def __init__(self, noises_dir, snr_levels=[0, 5, 10],\n                 cache=False,\n                 report=False):\n        self.noises_dir = noises_dir\n        self.snr_levels = snr_levels\n        self.report = report\n        # read noises in dir\n        if isinstance(noises_dir, list):\n            self.noises = []\n            for ndir in noises_dir:\n                self.noises += glob.glob(os.path.join(ndir, \'*.wav\'))\n        else:\n            self.noises = glob.glob(os.path.join(noises_dir, \'*.wav\'))\n        self.nidxs = list(range(len(self.noises)))\n        if len(self.noises) == 0:\n            raise ValueError(\'[!] No noises found in {}\'.format(noises_dir))\n        else:\n            print(\'[*] Found {} noise files\'.format(len(self.noises)))\n        self.eps = 1e-22\n        if cache:\n            self.cache = {}\n            for noise in self.noises:\n                self.load_noise(noise)\n\n    def sample_noise(self):\n        if len(self.noises) == 1:\n            return self.noises[0]\n        else:\n            idx = np.random.randint(0, len(self.noises))\n            # idx = random.choice(self.nidxs)\n            return self.noises[idx]\n\n    def load_noise(self, filename):\n        if hasattr(self, \'cache\') and filename in self.cache:\n            return self.cache[filename]\n        else:\n            nwav, rate = sf.read(filename)\n            if hasattr(self, \'cache\'):\n                self.cache[filename] = nwav\n        return nwav\n\n    def compute_SNR_K(self, signal, noise, snr):\n        Ex = np.dot(signal, signal)\n        En = np.dot(noise, noise)\n        if En > 0:\n            K = np.sqrt(Ex / ((10 ** (snr / 10.)) * En))\n        else:\n            K = 1.0\n        return K, Ex, En\n\n    def norm_energy(self, osignal, ienergy, eps=1e-14):\n        oenergy = np.dot(osignal, osignal)\n        return np.sqrt(ienergy / (oenergy + eps)) * osignal\n\n    #@profile\n    def __call__(self, pkg):\n        """""" Add noise to clean wav """"""\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        wav = wav.data.numpy().reshape(-1)\n        if \'chunk_beg_i\' in pkg:\n            beg_i = pkg[\'chunk_beg_i\']\n            end_i = pkg[\'chunk_end_i\']\n        else:\n            beg_i = 0\n            end_i = wav.shape[0]\n        sel_noise = self.load_noise(self.sample_noise())\n        if len(sel_noise) < len(wav):\n            # pad noise\n            P = len(wav) - len(sel_noise)\n            sel_noise = F.pad(torch.tensor(sel_noise).view(1, 1, -1),\n                              (0, P),\n                              ).view(-1).data.numpy()\n                              #mode=\'reflect\').view(-1).data.numpy()\n        T = end_i - beg_i\n        # TODO: not pre-loading noises from files?\n        if len(sel_noise) > T:\n            n_beg_i = np.random.randint(0, len(sel_noise) - T)\n        else:\n            n_beg_i = 0\n        noise = sel_noise[n_beg_i:n_beg_i + T].astype(np.float32)\n        # randomly sample the SNR level\n        snr = random.choice(self.snr_levels)\n        K, Ex, En = self.compute_SNR_K(wav, noise, snr)\n        scaled_noise = K * noise\n        if En > 0:\n            noisy = wav + scaled_noise\n            noisy = self.norm_energy(noisy, Ex)\n        else:\n            noisy = wav\n\n        x_ = torch.FloatTensor(noisy)\n        if self.report:\n            if \'report\' not in pkg:\n                pkg[\'report\'] = {}\n            pkg[\'report\'][\'snr\'] = snr\n        pkg[\'chunk\'] = x_\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(noises_dir={})\'.format(\n            self.noises_dir\n        )\n        return self.__class__.__name__ + attrs\n\n\nclass SimpleAdditiveShift(SimpleAdditive):\n\n    def __init__(self, noises_dir, snr_levels=[5, 10],\n                 noise_transform=None,\n                 noises_list=None,\n                 report=False):\n        if noises_list is None:\n            super().__init__(noises_dir, snr_levels, report)\n        else:\n            if isinstance(noises_dir, list):\n                assert len(noises_dir) == 1, len(noises_dir)\n                noises_dir = noises_dir[0]\n            with open(noises_list, \'r\') as nf:\n                self.noises = []\n                for nel in nf:\n                    nel = nel.rstrip()\n                    self.noises.append(os.path.join(noises_dir, nel))\n        self.noises_dir = noises_dir\n        self.noises_list = noises_list\n        self.snr_levels = snr_levels\n        self.report = report\n        self.nidxs = list(range(len(self.noises)))\n        if len(self.noises) == 0:\n            raise ValueError(\'[!] No noises found in {}\'.format(noises_dir))\n        else:\n            print(\'[*] Found {} noise files\'.format(len(self.noises)))\n        # additional out_transform to include potential distortions\n        self.noise_transform = noise_transform\n\n    #@profile\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        wav = wav.data.numpy().reshape(-1)\n        # compute shifts of signal\n        shift = np.random.randint(0, int(0.75 * len(wav)))\n        sel_noise = self.load_noise(self.sample_noise())\n        T = len(wav) - shift\n        if len(sel_noise) < T:\n            # pad noise\n            P = T - len(sel_noise)\n            sel_noise = F.pad(torch.tensor(sel_noise).view(1, 1, -1),\n                              (0, P),\n                              mode=\'constant\').view(-1).data.numpy()\n            n_beg_i = 0\n        elif len(sel_noise) > T:\n            n_beg_i = np.random.randint(0, len(sel_noise) - T)\n        else:\n            n_beg_i = 0\n        noise = sel_noise[n_beg_i:n_beg_i + T].astype(np.float32)\n        if self.noise_transform is not None:\n            noise = self.noise_transform({\'chunk\': torch.FloatTensor(noise)})[\'chunk\']\n            noise = noise.data.numpy()\n        pad_len = len(wav) - len(noise)\n        if \'overlap\' in pkg:\n            # anotate a mask of overlapped samples\n            dec_res = pkg[\'dec_resolution\'] \n            dec_len = len(wav) // dec_res\n            #assert dec_len == len(pkg[\'overlap\']), dec_len\n            pkg[\'overlap\'] = torch.cat((torch.zeros(pad_len),\n                                       torch.ones(len(noise))),\n                                       dim=0).float()\n            if dec_res > 1:\n                to_dec = pkg[\'overlap\'].view(-1, dec_res)\n                pkg[\'overlap\'] = torch.mean(to_dec, dim=1)\n\n        # apply padding to equal length now\n        noise = F.pad(torch.tensor(noise).view(1, 1, -1),\n                      (pad_len, 0),\n                      mode=\'constant\').view(-1).data.numpy()\n        # randomly sample the SNR level\n        snr = random.choice(self.snr_levels)\n        K, Ex, En = self.compute_SNR_K(wav, noise, snr)\n        scaled_noise = K * noise\n        noisy = wav + scaled_noise\n        noisy = self.norm_energy(noisy, Ex)\n        x_ = torch.FloatTensor(noisy)\n        if self.report:\n            if \'report\' not in pkg:\n                pkg[\'report\'] = {}\n            pkg[\'report\'][\'snr\'] = snr\n        pkg[\'chunk\'] = x_\n        return pkg\n\n    def __repr__(self):\n        if self.noise_transform is None:\n            attrs = \'(noises_dir={})\'.format(\n                self.noises_dir\n            )\n        else:\n            attrs = \'(noises_dir={}, noises_list={}, \' \\\n                    \'noise_transform={})\'.format(\n                self.noises_dir,\n                self.noises_list,\n                self.noise_transform.__repr__()\n            )\n        return self.__class__.__name__ + attrs\n\n\nclass Additive(object):\n\n    def __init__(self, noises_dir, snr_levels=[0, 5, 10], do_IRS=False,\n                 prob=1):\n        self.prob = prob\n        self.noises_dir = noises_dir\n        self.snr_levels = snr_levels\n        self.do_IRS = do_IRS\n        # read noises in dir\n        noises = glob.glob(os.path.join(noises_dir, \'*.wav\'))\n        if len(noises) == 0:\n            raise ValueError(\'[!] No noises found in {}\'.format(noises_dir))\n        else:\n            print(\'[*] Found {} noise files\'.format(len(noises)))\n            self.noises = []\n            for n_i, npath in enumerate(noises, start=1):\n                # nwav = wavfile.read(npath)[1]\n                nwav = librosa.load(npath, sr=None)[0]\n                self.noises.append({\'file\': npath,\n                                    \'data\': nwav.astype(np.float32)})\n                log_noise_load = \'Loaded noise {:3d}/{:3d}: \' \\\n                                 \'{}\'.format(n_i, len(noises),\n                                             npath)\n                print(log_noise_load)\n        self.eps = 1e-22\n\n    def __call__(self, wav, srate=16000, nbits=16):\n        """""" Add noise to clean wav """"""\n        if isinstance(wav, torch.Tensor):\n            wav = wav.numpy()\n        noise_idx = np.random.choice(list(range(len(self.noises))), 1)\n        sel_noise = self.noises[np.asscalar(noise_idx)]\n        noise = sel_noise[\'data\']\n        snr = np.random.choice(self.snr_levels, 1)\n        # print(\'Applying SNR: {} dB\'.format(snr[0]))\n        if wav.ndim > 1:\n            wav = wav.reshape((-1,))\n        noisy, noise_bound = self.addnoise_asl(wav, noise, srate,\n                                               nbits, snr,\n                                               do_IRS=self.do_IRS)\n        # normalize to avoid clipping\n        if np.max(noisy) >= 1 or np.min(noisy) < -1:\n            small = 0.1\n            while np.max(noisy) >= 1 or np.min(noisy) < -1:\n                noisy = noisy / (1. + small)\n                small = small + 0.1\n        return torch.FloatTensor(noisy.astype(np.float32))\n\n    def addnoise_asl(self, clean, noise, srate, nbits, snr, do_IRS=False):\n        if do_IRS:\n            # Apply IRS filter simulating telephone\n            # handset BW [300, 3200] Hz\n            clean = self.apply_IRS(clean, srate, nbits)\n        Px, asl, c0 = self.asl_P56(clean, srate, nbits)\n        # Px is active speech level ms energy\n        # asl is active factor\n        # c0 is active speech level threshold\n        x = clean\n        x_len = x.shape[0]\n\n        noise_len = noise.shape[0]\n        if noise_len <= x_len:\n            print(\'Noise length: \', noise_len)\n            print(\'Speech length: \', x_len)\n            raise ValueError(\'Noise length has to be greater than speech \'\n                             \'length!\')\n        rand_start_limit = int(noise_len - x_len + 1)\n        rand_start = int(np.round((rand_start_limit - 1) * np.random.rand(1) \\\n                                  + 1))\n        noise_segment = noise[rand_start:rand_start + x_len]\n        noise_bounds = (rand_start, rand_start + x_len)\n\n        if do_IRS:\n            noise_segment = self.apply_IRS(noise_segment, srate, nbits)\n\n        Pn = np.dot(noise_segment.T, noise_segment) / x_len\n\n        # we need to scale the noise segment samples to obtain the\n        # desired SNR = 10 * log10( Px / ((sf ** 2) * Pn))\n        sf = np.sqrt(Px / Pn / (10 ** (snr / 10)))\n        noise_segment = noise_segment * sf\n\n        noisy = x + noise_segment\n\n        return noisy, noise_bounds\n\n    def apply_IRS(self, data, srate, nbits):\n        """""" Apply telephone handset BW [300, 3200] Hz """"""\n        raise NotImplementedError(\'Under construction!\')\n        from pyfftw.interfaces import scipy_fftpack as fftw\n        n = data.shape[0]\n        # find next pow of 2 which is greater or eq to n\n        pow_of_2 = 2 ** (np.ceil(np.log2(n)))\n\n        align_filter_dB = np.array([[0, -200], [50, -40], [100, -20],\n                                    [125, -12], [160, -6], [200, 0],\n                                    [250, 4], [300, 6], [350, 8], [400, 10],\n                                    [500, 11], [600, 12], [700, 12], [800, 12],\n                                    [1000, 12], [1300, 12], [1600, 12], [2000, 12],\n                                    [2500, 12], [3000, 12], [3250, 12], [3500, 4],\n                                    [4000, -200], [5000, -200], [6300, -200],\n                                    [8000, -200]])\n        print(\'align filter dB shape: \', align_filter_dB.shape)\n        num_of_points, trivial = align_filter_dB.shape\n        overallGainFilter = interp1d(align_filter_dB[:, 0], align_filter[:, 1],\n                                     1000)\n\n        x = np.zeros((pow_of_2))\n        x[:data.shape[0]] = data\n\n        x_fft = fftw.fft(x, pow_of_2)\n\n        freq_resolution = srate / pow_of_2\n\n        factorDb = interp1d(align_filter_dB[:, 0],\n                            align_filter_dB[:, 1],\n                            list(range(0, (pow_of_2 / 2) + 1) * \\\n                                 freq_resolution)) - \\\n                   overallGainFilter\n        factor = 10 ** (factorDb / 20)\n\n        factor = [factor, np.fliplr(factor[1:(pow_of_2 / 2 + 1)])]\n        x_fft = x_fft * factor\n\n        y = fftw.ifft(x_fft, pow_of_2)\n\n        data_filtered = y[:n]\n        return data_filtered\n\n    def asl_P56(self, x, srate, nbits):\n        """""" ITU P.56 method B. """"""\n        T = 0.03  # time constant of smoothing in seconds\n        H = 0.2  # hangover time in seconds\n        M = 15.9\n\n        # margin in dB of the diff b/w threshold and active speech level\n        thres_no = nbits - 1  # num of thresholds, for 16 bits it\'s 15\n\n        I = np.ceil(srate * H)  # hangover in samples\n        g = np.exp(-1 / (srate * T))  # smoothing factor in envelop detection\n        c = 2. ** (np.array(list(range(-15, (thres_no + 1) - 16))))\n        # array of thresholds from one quantizing level up to half the max\n        # code, at a step of 2. In case of 16bit: from 2^-15 to 0.5\n        a = np.zeros(c.shape[0])  # activity counter for each level thres\n        hang = np.ones(c.shape[0]) * I  # hangover counter for each level thres\n\n        assert x.ndim == 1, x.shape\n        sq = np.dot(x, x)  # long term level square energy of x\n        x_len = x.shape[0]\n\n        # use 2nd order IIR filter to detect envelope q\n        x_abs = np.abs(x)\n        p = lfilter(np.ones(1) - g, np.array([1, -g]), x_abs)\n        q = lfilter(np.ones(1) - g, np.array([1, -g]), p)\n\n        for k in range(x_len):\n            for j in range(thres_no):\n                if q[k] >= c[j]:\n                    a[j] = a[j] + 1\n                    hang[j] = 0\n                elif hang[j] < I:\n                    a[j] = a[j] + 1\n                    hang[j] = hang[j] + 1\n                else:\n                    break\n        asl = 0\n        asl_ms = 0\n        c0 = None\n        if a[0] == 0:\n            return asl_ms, asl, c0\n        else:\n            den = a[0] + self.eps\n            AdB1 = 10 * np.log10(sq / a[0] + self.eps)\n\n        CdB1 = 20 * np.log10(c[0] + self.eps)\n        if AdB1 - CdB1 < M:\n            return asl_ms, asl, c0\n        AdB = np.zeros(c.shape[0])\n        CdB = np.zeros(c.shape[0])\n        Delta = np.zeros(c.shape[0])\n        AdB[0] = AdB1\n        CdB[0] = CdB1\n        Delta[0] = AdB1 - CdB1\n\n        for j in range(1, AdB.shape[0]):\n            AdB[j] = 10 * np.log10(sq / (a[j] + self.eps) + self.eps)\n            CdB[j] = 20 * np.log10(c[j] + self.eps)\n\n        for j in range(1, Delta.shape[0]):\n            if a[j] != 0:\n                Delta[j] = AdB[j] - CdB[j]\n                if Delta[j] <= M:\n                    # interpolate to find the asl\n                    asl_ms_log, cl0 = self.bin_interp(AdB[j],\n                                                      AdB[j - 1],\n                                                      CdB[j],\n                                                      CdB[j - 1],\n                                                      M, 0.5)\n                    asl_ms = 10 ** (asl_ms_log / 10)\n                    asl = (sq / x_len) / asl_ms\n                    c0 = 10 ** (cl0 / 20)\n                    break\n        return asl_ms, asl, c0\n\n    def bin_interp(self, upcount, lwcount, upthr, lwthr, Margin, tol):\n        if tol < 0:\n            tol = -tol\n\n        # check if extreme counts are not already the true active value\n        iterno = 1\n        if np.abs(upcount - upthr - Margin) < tol:\n            asl_ms_log = lwcount\n            cc = lwthr\n            return asl_ms_log, cc\n        if np.abs(lwcount - lwthr - Margin) < tol:\n            asl_ms_log = lwcount\n            cc = lwthr\n            return asl_ms_log, cc\n\n        midcount = (upcount + lwcount) / 2\n        midthr = (upthr + lwthr) / 2\n        # repeats loop until diff falls inside tolerance (-tol <= diff <= tol)\n        while True:\n            diff = midcount - midthr - Margin\n            if np.abs(diff) <= tol:\n                break\n            # if tol is not met up to 20 iters, then relax tol by 10%\n            iterno += 1\n            if iterno > 20:\n                tol *= 1.1\n\n            if diff > tol:\n                midcount = (upcount + midcount) / 2\n                # upper and mid activities\n                midthr = (upthr + midthr) / 2\n                # ... and thresholds\n            elif diff < -tol:\n                # then new bounds are...\n                midcount = (midcount - lwcount) / 2\n                # middle and lower activities\n                midthr = (midthr + lwthr) / 2\n                # ... and thresholds\n        # since tolerance has been satisfied, midcount is selected as\n        # interpolated value with tol [dB] tolerance\n        asl_ms_log = midcount\n        cc = midthr\n        return asl_ms_log, cc\n\n    def __repr__(self):\n        attrs = \'(noises_dir={}\\n, snr_levels={}\\n, do_IRS={})\'.format(\n            self.noises_dir,\n            self.snr_levels,\n            self.do_IRS\n        )\n        return self.__class__.__name__ + attrs\n\nclass Whisperize(object):\n\n    def __init__(self, sr=16000, cache_dir=None, report=False):\n        self.report = report\n        self.sr = 16000\n        self.AHOCODE = \'ahocoder16_64 $infile $f0file $ccfile $fvfile\'\n        self.AHODECODE = \'ahodecoder16_64 $f0file $ccfile $fvfile $outfile\'\n        self.cache_dir = cache_dir\n\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        if \'uttname\' in pkg:\n            # look for the uttname in whisper format first\n            wuttname = os.path.basename(pkg[\'uttname\'])\n        if self.cache_dir is not None and \\\n                os.path.exists(self.cache_dir) and \'uttname\' in pkg:\n            wfpath = os.path.join(self.cache_dir, wuttname)\n            if not os.path.exists(wfpath):\n                raise ValueError(\'Path {} does not exist\'.format(wfpath))\n            # The cached whisper file exists, load it and chunk it\n            # to match pkg boundaries\n            wav, rate = sf.read(wfpath)\n            beg_i = pkg[\'chunk_beg_i\']\n            end_i = pkg[\'chunk_end_i\']\n            L_ = end_i - beg_i\n            if len(wav) < L_:\n                P = L_ - len(wav)\n                wav = np.concatenate((wav, np.zeros((P,))), axis=0)\n            assert end_i - beg_i <= len(wav), len(wav)\n            wav = wav[beg_i:end_i]\n        else:\n            wav = wav.data.numpy().reshape(-1).astype(np.float32)\n            tf = tempfile.NamedTemporaryFile()\n            tfname = tf.name\n            # save wav to file\n            infile = tfname + \'.wav\'\n            ccfile = tfname + \'.cc\'\n            f0file = tfname + \'.lf0\'\n            fvfile = tfname + \'.fv\'\n            # overwrite infile\n            outfile = infile\n            inwav = np.array(wav).astype(np.float32)\n            # save wav\n            sf.write(infile, wav, self.sr)\n            # encode with vocoder\n            ahocode = self.AHOCODE.replace(\'$infile\', infile)\n            ahocode = ahocode.replace(\'$f0file\', f0file)\n            ahocode = ahocode.replace(\'$fvfile\', fvfile)\n            ahocode = ahocode.replace(\'$ccfile\', ccfile)\n            p = subprocess.Popen(shlex.split(ahocode))\n            p.wait()\n            # read vocoder to know the length\n            lf0 = read_aco_file(f0file, (-1,))\n            nsamples = lf0.shape[0]\n            # Unvoice everything generating -1e10 for logF0 and \n            # 1e3 for FV params\n            lf0 = -1e10 * np.ones(nsamples)\n            fv = 1e3 * np.ones(nsamples)\n            # Write the unvoiced frames overwriting voiced ones\n            write_aco_file(fvfile, fv)\n            write_aco_file(f0file, lf0)\n            # decode with vododer\n            ahodecode = self.AHODECODE.replace(\'$f0file\', f0file)\n            ahodecode = ahodecode.replace(\'$ccfile\', ccfile)\n            ahodecode = ahodecode.replace(\'$fvfile\', fvfile)\n            ahodecode = ahodecode.replace(\'$outfile\', outfile)\n            p = subprocess.Popen(shlex.split(ahodecode))\n            p.wait()\n            wav, _ = sf.read(outfile)\n            wav = norm_energy(wav.astype(np.float32), inwav)\n            if len(wav) > len(inwav):\n                wav = wav[:len(inwav)]\n            tf.close()\n            os.unlink(infile)\n            os.unlink(ccfile)\n            os.unlink(f0file)\n            os.unlink(fvfile)\n        if self.report:\n            if \'report\' not in pkg:\n                pkg[\'report\'] = {}\n            pkg[\'report\'][\'whisper\'] = True\n        pkg[\'chunk\'] = torch.FloatTensor(wav)\n        return pkg\n\n\n    def __repr__(self):\n        attrs = \'(cache_dir={})\'.format(self.cache_dir)\n        return self.__class__.__name__ + attrs\n\n\n\nclass Codec2Buffer(object):\n\n    def __init__(self, kbps=1600, sr=16000, report=False):\n        import pycodec2\n        self.report = report\n        self.kbps = kbps\n        self.sr = sr\n        self.c2 = pycodec2.Codec2(kbps)\n        self.INT16_BYTE_SIZE = 2\n        self.FRAME_SIZE = self.c2.samples_per_frame()\n        #self.PACKET_SIZE = self.c2.samples_per_frame() * self.INT16_BYTE_SIZE\n        #self.STRUCT_FORMAT = \'{}h\'.format(self.c2.samples_per_frame())\n\n    #@profile\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        tensor_wav = wav\n        re_factor = self.sr // 8000\n        wav = wav.data.numpy().reshape(-1).astype(np.float32)\n        inwav = wav\n        #wav = resample(wav, len(wav) // re_factor)\n        #wav = librosa.core.resample(wav, self.sr, 8000,\n        #                            res_type=\'kaiser_fast\')\n        wav = decimate(wav, self.sr // 8000)\n        wav = np.array(wav * (2 ** 15), dtype=np.int16)\n        total_frames = int(np.ceil(len(wav) / self.FRAME_SIZE))\n        P_ = total_frames * self.FRAME_SIZE - len(wav)\n        orilen = len(wav)\n        if P_ > 0:\n            wav = np.concatenate((wav, \n                                  np.zeros((P_,), dtype=np.int16)),\n                                 axis=0)\n        owav = []\n        T = len(wav)\n        data = [wav[t:t + self.FRAME_SIZE] for t in range(0, T,\n                                                          self.FRAME_SIZE)]\n        for frame in data:\n            enc = self.c2.encode(frame)\n            dec = self.c2.decode(enc)\n            owav.extend(dec.tolist())\n        owav = np.array(owav, dtype=np.int16)\n        owav = owav[:orilen]\n        #owav = np.array(owav, dtype=np.float32) / (2 ** 15)\n        # resample up to original srate\n        owav = resample(owav, len(owav) * re_factor)\n        #owav = librosa.core.resample(owav, 8000, self.sr, res_type=\'kaiser_fast\')\n        owav = owav.astype(np.float32) / (2 ** 15)\n        #owav = owav / (2 ** 15)\n        owav = norm_energy(owav, inwav)\n        if self.report:\n            if \'report\' not in pkg:\n                pkg[\'report\'] = {}\n            pkg[\'report\'][\'kbps\'] = self.kbps\n        #tensor_wav.data = torch.from_numpy(owav)\n        pkg[\'chunk\'] = torch.from_numpy(owav)\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(kbps={})\'.format(\n            self.kbps\n        )\n        return self.__class__.__name__ + attrs\n\nclass Codec2Cached(object):\n\n    def __init__(self, cache_path, kbps=1600, cache=False):\n        self.kbps = kbps\n        self.cache_path = cache_path\n        self.do_cache = cache\n        if cache:\n            self.cache = {}\n            wavs = glob.glob(os.path.join(self.cache_path, \'*.wav\'))\n            for wav in wavs:\n                self.load_file(os.path.basename(wav))\n            \n\n    def load_file(self, path):\n        if not os.path.exist(uttpath):\n            raise FileNotFoundError(\'Could not find the file {}\'\n                                    \' in the codec2cache path {}\'\n                                    \'\'.format(uttname, self.cache_path))\n        if hasattr(self, \'cache\') and path in cache:\n            return self.cache[path]\n        else:\n            x, rate = sf.read(path)\n            if hasattr(self, \'cache\'):\n                self.cache[path] = x\n            return x\n\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        # take input ref for energy normalization\n        inwav = pkg[\'chunk\']\n        uttname = os.path.basename(pkg[\'uttname\'])\n        uttpath = os.path.join(self.cache_path,\n                               uttname)\n        owav = self.load_file(uttpath)\n        owav = norm_energy(owav, inwav)\n        if self.report:\n            if \'report\' not in pkg:\n                pkg[\'report\'] = {}\n            pkg[\'report\'][\'kbps\'] = self.kbps\n        pkg[\'chunk\'] = torch.from_numpy(owav)\n        self.do_cache = cache\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(kbps={}, cache_dir={}, cache={})\'.format(\n            self.cache_dir,\n            self.do_cache,\n            self.kbps\n        )\n        return self.__class__.__name__ + attrs\n\n\nclass Codec2(object):\n\n    def __init__(self, kbps=1600, sr=16000, report=False):\n        self.kbps = kbps\n        self.report = report\n        self.sr = sr\n        self.SOX_ENCODE = \'sox $infile -r 8k -b 16 -e signed-integer -c 1 $raw_efile\'\n        self.C2_ENCODE = \'c2enc $kbps $raw_efile $c2file\'\n        self.C2_DECODE = \'c2dec $kbps $c2file $raw_dfile\'\n        self.SOX_DECODE = \'sox -r 8k -b 16 -e signed-integer -c 1 $raw_dfile -r $sr -b 16 -e signed-integer -c 1 $outfile\'\n\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        wav = wav.data.numpy().reshape(-1).astype(np.float32)\n        tf = tempfile.NamedTemporaryFile()\n        tfname = tf.name\n        # save wav to file\n        infile = tfname + \'.wav\'\n        raw_efile = tfname + \'.raw\'\n        c2file = tfname + \'.c2\'\n        raw_dfile = tfname + \'_out.raw\'\n        outfile = tfname + \'_out.wav\'\n        inwav = np.array(wav).astype(np.float32)\n        # save wav\n        sf.write(infile, wav, self.sr)\n        # convert to proper codec 2 input format as raw data with SoX\n        sox_encode = self.SOX_ENCODE.replace(\'$infile\', infile)\n        sox_encode = sox_encode.replace(\'$raw_efile\', raw_efile)\n        p = subprocess.Popen(shlex.split(sox_encode),\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n        p.wait()\n        #print(sox_encode)\n        # Encode with Codec 2\n        c2_encode = self.C2_ENCODE.replace(\'$kbps\', str(self.kbps))\n        c2_encode = c2_encode.replace(\'$raw_efile\', raw_efile)\n        c2_encode = c2_encode.replace(\'$c2file\', c2file)\n        #print(c2_encode)\n        p = subprocess.Popen(shlex.split(c2_encode))\n        p.wait()\n        # Decode with Codec 2\n        c2_decode = self.C2_DECODE.replace(\'$kbps\', str(self.kbps))\n        c2_decode = c2_decode.replace(\'$c2file\', c2file)\n        c2_decode = c2_decode.replace(\'$raw_dfile\', raw_dfile)\n        #print(c2_decode)\n        p = subprocess.Popen(shlex.split(c2_decode), \n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n        p.wait()\n        # Convert back to <sr> sampling rate wav\n        sox_decode = self.SOX_DECODE.replace(\'$raw_dfile\', raw_dfile)\n        sox_decode = sox_decode.replace(\'$sr\', str(self.sr))\n        sox_decode = sox_decode.replace(\'$outfile\', outfile)\n        #print(sox_decode)\n        p = subprocess.Popen(shlex.split(sox_decode),\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.PIPE)\n        p.wait()\n        wav, rate = sf.read(outfile)\n        wav = norm_energy(wav.astype(np.float32), inwav)\n        tf.close()\n        os.unlink(infile)\n        os.unlink(raw_efile)\n        os.unlink(c2file)\n        os.unlink(raw_dfile)\n        os.unlink(outfile)\n        if self.report:\n            if \'report\' not in pkg:\n                pkg[\'report\'] = {}\n            pkg[\'report\'][\'kbps\'] = self.kbps\n        pkg[\'chunk\'] = torch.FloatTensor(wav)\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(kbps={})\'.format(\n            self.kbps\n        )\n        return self.__class__.__name__ + attrs\n        \nclass SpeedChange(object):\n\n    def __init__(self, factor_range=(-0.15, 0.15), report=False):\n        self.factor_range = factor_range\n        self.report = report\n\n    # @profile\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        wav = wav.data.numpy().reshape(-1).astype(np.float32)\n        warp_factor = random.random() * (self.factor_range[1] - \\\n                                         self.factor_range[0]) + \\\n                      self.factor_range[0]\n        samp_warp = wav.shape[0] + int(warp_factor * wav.shape[0])\n        rwav = signal.resample(wav, samp_warp)\n        if len(rwav) > len(wav):\n            mid_i = (len(rwav) // 2) - len(wav) // 2\n            rwav = rwav[mid_i:mid_i + len(wav)]\n        if len(rwav) < len(wav):\n            diff = len(wav) - len(rwav)\n            P = (len(wav) - len(rwav)) // 2\n            if diff % 2 == 0:\n                rwav = np.concatenate((np.zeros(P, ),\n                                       wav,\n                                       np.zeros(P, )),\n                                      axis=0)\n            else:\n                rwav = np.concatenate((np.zeros(P, ),\n                                       wav,\n                                       np.zeros(P + 1, )),\n                                      axis=0)\n        if self.report:\n            if \'report\' not in pkg:\n                pkg[\'report\'] = {}\n            pkg[\'report\'][\'warp_factor\'] = warp_factor\n        pkg[\'chunk\'] = torch.FloatTensor(rwav)\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(factor_range={})\'.format(\n            self.factor_range\n        )\n        return self.__class__.__name__ + attrs\n\nif __name__ == \'__main__\':\n    """"""\n    lpc = Gammatone(n_channels=40, f_min=100)\n    wav, size = sf.read(\'test.wav\')\n    wav = wav[:32000]\n    print(wav.shape)\n    gtn = lpc({\'chunk\':wav})[\'gtn\'].data.numpy()\n    lps = LPS()({\'chunk\':torch.FloatTensor(wav)})[\'lps\'].data.numpy()\n    import matplotlib\n    matplotlib.use(\'Agg\')\n    import matplotlib.pyplot as plt\n    plt.subplot(2,1,1)\n    plt.imshow(np.log(gtn))\n    plt.subplot(2,1,2)\n    #plt.imshow(lps)\n    #plt.subplot(3,1,3)\n    plt.plot(wav)\n    plt.tight_layout()\n    plt.savefig(\'gtn.png\', dpi=200)\n    """"""\n    import json\n    dist_path = \'/home/santi/DB/GEnhancement/distortions_SEGANnoises.cfg\'\n    dtr = json.load(open(dist_path, \'r\'))\n    dist = config_distortions(**dtr)\n    codec = Codec2Buffer()\n    wav, size = sf.read(\'test.wav\')\n    for n in range(100):\n        #buffer_c2 = dist({\'chunk\':torch.tensor(wav)})[\'chunk\']\n        buffer_c2 = codec({\'chunk\':torch.tensor(wav)})[\'chunk\']\n    sf.write(\'/tmp/buffer_test.wav\', buffer_c2, 16000)\n'"
pase/utils.py,19,"b'import json\nimport shlex\nimport subprocess\nimport random\nimport torch\nimport torch.nn as nn\ntry:\n    from .losses import *\nexcept ImportError:\n    from losses import *\nimport random\nfrom random import shuffle\nfrom pase.models.discriminator import *\nimport torch.optim as optim\nfrom torch.autograd import Function\n\n\ndef pase_parser(cfg_fname, batch_acum=1, device=\'cpu\', do_losses=True,\n                frontend=None):\n    with open(cfg_fname, \'r\') as cfg_f:\n        cfg_all = json.load(cfg_f)\n        if do_losses:\n            # change loss section\n            for i, cfg in enumerate(cfg_all):\n                loss_name = cfg_all[i][\'loss\']\n                if hasattr(nn, loss_name):\n                    # retrieve potential r frames parameter\n                    r_frames = cfg_all[i].get(\'r\', None)\n                    # loss in nn Modules\n                    cfg_all[i][\'loss\'] = ContextualizedLoss(getattr(nn, loss_name)(),\n                                                            r=r_frames)\n                else:\n                    if loss_name == \'LSGAN\' or loss_name == \'GAN\':\n                        dnet_cfg = {}\n                        if \'DNet_cfg\' in cfg_all[i]:\n                            dnet_cfg = cfg_all[i].pop(\'DNet_cfg\')\n                        dnet_cfg[\'frontend\'] = frontend\n                        # make DNet\n                        DNet =  RNNDiscriminator(**dnet_cfg)\n                        if \'Dopt_cfg\' in cfg_all[i]:\n                            Dopt_cfg = cfg_all[i].pop(\'Dopt_cfg\')\n                            Dopt = optim.Adam(DNet.parameters(),\n                                                 Dopt_cfg[\'lr\'])\n                        else:\n                            Dopt = optim.Adam(DNet.parameters(), 0.0005)\n                    Dloss = \'L2\' if loss_name == \'LSGAN\' else \'BCE\'\n                    cfg_all[i][\'loss\'] = WaveAdversarialLoss(DNet, Dopt,\n                                                             loss=Dloss,\n                                                             batch_acum=batch_acum,\n                                                             device=device)\n        return cfg_all\n\ndef worker_parser(cfg_fname, batch_acum=1, device=\'cpu\', do_losses=True,\n                frontend=None):\n    with open(cfg_fname, \'r\') as cfg_f:\n        cfg_list = json.load(cfg_f)\n        if do_losses:\n            # change loss section\n            for type, cfg_all in cfg_list.items():\n\n                for i, cfg in enumerate(cfg_all):\n                    loss_name = cfg_all[i][\'loss\']\n                    if hasattr(nn, loss_name):\n                        # retrieve potential r frames parameter\n                        r_frames = cfg_all[i].get(\'r\', None)\n                        # loss in nn Modules\n                        cfg_all[i][\'loss\'] = ContextualizedLoss(getattr(nn, loss_name)(),\n                                                                r=r_frames)\n                    else:\n                        if loss_name == \'LSGAN\' or loss_name == \'GAN\':\n                            dnet_cfg = {}\n                            if \'DNet_cfg\' in cfg_all[i]:\n                                dnet_cfg = cfg_all[i].pop(\'DNet_cfg\')\n                            dnet_cfg[\'frontend\'] = frontend\n                            # make DNet\n                            DNet =  RNNDiscriminator(**dnet_cfg)\n                            if \'Dopt_cfg\' in cfg_all[i]:\n                                Dopt_cfg = cfg_all[i].pop(\'Dopt_cfg\')\n                                Dopt = optim.Adam(DNet.parameters(),\n                                                     Dopt_cfg[\'lr\'])\n                            else:\n                                Dopt = optim.Adam(DNet.parameters(), 0.0005)\n                        Dloss = \'L2\' if loss_name == \'LSGAN\' else \'BCE\'\n                        cfg_all[i][\'loss\'] = WaveAdversarialLoss(DNet, Dopt,\n                                                                 loss=Dloss,\n                                                                 batch_acum=batch_acum,\n                                                                 device=device)\n            cfg_list[type] = cfg_all\n        print(cfg_list)\n        return cfg_list\n\n\ndef build_optimizer(opt_cfg, params):\n    if isinstance(opt_cfg, str):\n        with open(opt_cfg, \'r\') as cfg_f:\n            opt_cfg = json.load(cfg_f)\n    opt_name = opt_cfg.pop(\'name\')\n    if \'sched\' in opt_cfg:\n        sched_cfg = opt_cfg.pop(\'sched\')\n    else:\n        sched_cfg = None\n    opt_cfg[\'params\'] = params\n    opt = getattr(optim, opt_name)(**opt_cfg)\n    if sched_cfg is not None:\n        sname = sched_cfg.pop(\'name\')\n        sched_cfg[\'optimizer\'] = opt\n        sched = getattr(optim.lr_scheduler, sname)(**sched_cfg)\n        return opt, sched\n    else:\n        return opt, None\n\ndef chunk_batch_seq(X, seq_range=[90, 1000]):\n    bsz, nfeats, slen = X.size()\n    min_seq = seq_range[0]\n    max_seq = min(slen, seq_range[1])\n    # sample a random chunk size\n    chsz = random.choice(list(range(min_seq, max_seq)))\n    idxs = list(range(slen - chsz))\n    beg_i = random.choice(idxs)\n    return X[:, :, beg_i:beg_i + chsz]\n\ndef kfold_data(data_list, utt2class, folds=10, valid_p=0.1):\n    # returns the K lists of lists, so each k-th component\n    # is composed of 3 sub-lists\n    #idxs = list(range(len(data_list)))\n    # shuffle the idxs first\n    #shuffle(idxs)\n    # group by class first\n    classes = set(utt2class.values())\n    items = dict((k, []) for k in classes)\n    for data_el in data_list:\n        items[utt2class[data_el]].append(data_el)\n    lens = {}\n    test_splits = {}\n    for k in items.keys():\n        shuffle(items[k])\n        lens[k] = len(items[k])\n        TEST_SPLIT_K = int((1. / folds) * lens[k])\n        test_splits[k] = TEST_SPLIT_K\n    lists = []\n    beg_i = dict((k, 0) for k in test_splits.keys())\n    # now slide a window per fold\n    for fi in range(folds):\n        test_split = []\n        train_split = []\n        valid_split = []\n        print(\'-\' * 30)\n        print(\'Fold {} splits:\'.format(fi))\n        for k, data in items.items():\n            te_split = data[beg_i[k]:beg_i[k] + test_splits[k]]\n            test_split += te_split\n            tr_split = data[:beg_i[k]] + data[beg_i[k] + test_splits[k]:]\n            # select train and valid splits\n            tr_split = tr_split[int(valid_p * len(tr_split)):]\n            va_split = tr_split[:int(valid_p * len(tr_split))]\n            train_split += tr_split\n            valid_split += va_split\n            print(\'Split {} train: {}, valid: {}, test: {}\'\n                  \'\'.format(k, len(tr_split), len(va_split), len(te_split)))\n        # build valid split within train_split\n        lists.append([train_split, valid_split, test_split])\n    return lists\n\nclass AuxiliarSuperviser(object):\n\n    def __init__(self, cmd_file, save_path=\'.\'):\n        self.cmd_file = cmd_file\n        with open(cmd_file, \'r\') as cmd_f:\n            self.cmd = [l.rstrip() for l in cmd_f]\n        self.save_path = save_path\n\n    def __call__(self, iteration, ckpt_path, cfg_path):\n        assert isinstance(iteration, int)\n        assert isinstance(ckpt_path, str)\n        assert isinstance(cfg_path, str)\n        for cmd in self.cmd:\n            sub_cmd = cmd.replace(\'$model\', ckpt_path)\n            sub_cmd = sub_cmd.replace(\'$iteration\', str(iteration))\n            sub_cmd = sub_cmd.replace(\'$cfg\', cfg_path)\n            sub_cmd = sub_cmd.replace(\'$save_path\', self.save_path)\n            print(\'Executing async command: \', sub_cmd)\n            #shsub = shlex.split(sub_cmd)\n            #print(shsub)\n            p = subprocess.Popen(sub_cmd,\n                                shell=True)\n\n\ndef get_grad_norms(model, keys=[]):\n    grads = {}\n    for i, (k, param) in enumerate(dict(model.named_parameters()).items()):\n        accept = False\n        for key in keys:\n            # match substring in collection of model keys\n            if key in k:\n                accept = True\n                break\n        if not accept:\n            continue\n        if param.grad is None:\n            print(\'WARNING getting grads: {} param grad is None\'.format(k))\n            continue\n        grads[k] = torch.norm(param.grad).cpu().item()\n    return grads\n\ndef sample_probable(p):\n    return random.random() < p\n\ndef zerospeech(shape, eps=1e-14):\n    S = np.random.randn(shape) * eps\n    return S.astype(np.float32)\n\n\nclass ScaleGrad(Function):\n\n    @staticmethod\n    def forward(ctx, x, alpha):\n        ctx.alpha = alpha\n\n        return x\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        output = grad_output * ctx.alpha\n\n        return output, None\n\ndef log_sum_exp(x):\n    """""" numerically stable log_sum_exp implementation that prevents overflow """"""\n    # TF ordering\n    axis = len(x.size()) - 1\n    m, _ = torch.max(x, dim=axis)\n    m2, _ = torch.max(x, dim=axis, keepdim=True)\n    return m + torch.log(torch.sum(torch.exp(x - m2), dim=axis))\n\n\ndef discretized_mix_logistic_loss(y_hat, y, num_classes=65536,\n                                  log_scale_min=None, reduce=True):\n    """""" https://github.com/fatchord/WaveRNN/blob/master/utils/distribution.py\n    """"""\n    if log_scale_min is None:\n        log_scale_min = float(np.log(1e-14))\n    y_hat = y_hat.permute(0,2,1)\n    assert y_hat.dim() == 3\n    assert y_hat.size(1) % 3 == 0\n    nr_mix = y_hat.size(1) // 3\n\n    # (B x T x C)\n    y_hat = y_hat.transpose(1, 2)\n\n    # unpack parameters. (B, T, num_mixtures) x 3\n    logit_probs = y_hat[:, :, :nr_mix]\n    means = y_hat[:, :, nr_mix:2 * nr_mix]\n    log_scales = torch.clamp(y_hat[:, :, 2 * nr_mix:3 * nr_mix], min=log_scale_min)\n\n    # B x T x 1 -> B x T x num_mixtures\n    y = y.expand_as(means)\n\n    centered_y = y - means\n    inv_stdv = torch.exp(-log_scales)\n    plus_in = inv_stdv * (centered_y + 1. / (num_classes - 1))\n    cdf_plus = torch.sigmoid(plus_in)\n    min_in = inv_stdv * (centered_y - 1. / (num_classes - 1))\n    cdf_min = torch.sigmoid(min_in)\n\n    # log probability for edge case of 0 (before scaling)\n    # equivalent: torch.log(F.sigmoid(plus_in))\n    log_cdf_plus = plus_in - F.softplus(plus_in)\n\n    # log probability for edge case of 255 (before scaling)\n    # equivalent: (1 - F.sigmoid(min_in)).log()\n    log_one_minus_cdf_min = -F.softplus(min_in)\n\n    # probability for all other cases\n    cdf_delta = cdf_plus - cdf_min\n\n    mid_in = inv_stdv * centered_y\n    # log probability in the center of the bin, to be used in extreme cases\n    # (not actually used in our code)\n    log_pdf_mid = mid_in - log_scales - 2. * F.softplus(mid_in)\n\n    # tf equivalent\n    """"""\n    log_probs = tf.where(x < -0.999, log_cdf_plus,\n                         tf.where(x > 0.999, log_one_minus_cdf_min,\n                                  tf.where(cdf_delta > 1e-5,\n                                           tf.log(tf.maximum(cdf_delta, 1e-12)),\n                                           log_pdf_mid - np.log(127.5))))\n    """"""\n    # TODO: cdf_delta <= 1e-5 actually can happen. How can we choose the value\n    # for num_classes=65536 case? 1e-7? not sure..\n    inner_inner_cond = (cdf_delta > 1e-5).float()\n\n    inner_inner_out = inner_inner_cond * \\\n        torch.log(torch.clamp(cdf_delta, min=1e-12)) + \\\n        (1. - inner_inner_cond) * (log_pdf_mid - np.log((num_classes - 1) / 2))\n    inner_cond = (y > 0.999).float()\n    inner_out = inner_cond * log_one_minus_cdf_min + (1. - inner_cond) * inner_inner_out\n    cond = (y < -0.999).float()\n    log_probs = cond * log_cdf_plus + (1. - cond) * inner_out\n\n    log_probs = log_probs + F.log_softmax(logit_probs, -1)\n\n    if reduce:\n        return -torch.mean(log_sum_exp(log_probs))\n    else:\n        return -log_sum_exp(log_probs).unsqueeze(-1)\n\n\ndef sample_from_discretized_mix_logistic(y, log_scale_min=None):\n    """"""\n    https://github.com/fatchord/WaveRNN/blob/master/utils/distribution.py\n    Sample from discretized mixture of logistic distributions\n    Args:\n        y (Tensor): B x C x T\n        log_scale_min (float): Log scale minimum value\n    Returns:\n        Tensor: sample in range of [-1, 1].\n    """"""\n    if log_scale_min is None:\n        log_scale_min = float(np.log(1e-14))\n    assert y.size(1) % 3 == 0\n    nr_mix = y.size(1) // 3\n\n    # B x T x C\n    y = y.transpose(1, 2)\n    logit_probs = y[:, :, :nr_mix]\n\n    # sample mixture indicator from softmax\n    temp = logit_probs.data.new(logit_probs.size()).uniform_(1e-5, 1.0 - 1e-5)\n    temp = logit_probs.data - torch.log(- torch.log(temp))\n    _, argmax = temp.max(dim=-1)\n\n    # (B, T) -> (B, T, nr_mix)\n    one_hot = F.one_hot(argmax, nr_mix).float()\n    # select logistic parameters\n    means = torch.sum(y[:, :, nr_mix:2 * nr_mix] * one_hot, dim=-1)\n    log_scales = torch.clamp(torch.sum(\n        y[:, :, 2 * nr_mix:3 * nr_mix] * one_hot, dim=-1), min=log_scale_min)\n    # sample from logistic & clip to interval\n    # we don\'t actually round to the nearest 8bit value when sampling\n    u = means.data.new(means.size()).uniform_(1e-5, 1.0 - 1e-5)\n    x = means + torch.exp(log_scales) * (torch.log(u) - torch.log(1. - u))\n\n    x = torch.clamp(torch.clamp(x, min=-1.), max=1.)\n\n    return x\n\n'"
spk_id/knn.py,0,"b""import argparse\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import KNeighborsClassifier\nimport json\nimport timeit\nimport glob\nimport os\n\n\ndef load_train_files(root_path, cfg, split):\n    spk2idx = {}\n    npys = cfg[split]['wav_files']\n    labs = cfg[split]['spk_ids']\n    Y = []\n    X = []\n    spk2idx = {}\n    for npy, lab in zip(npys, labs):\n        npy_name = os.path.join(root_path, npy)\n        x = np.load(npy_name)\n        if lab not in spk2idx:\n            spk2idx[lab] = len(spk2idx)\n        X.append(x.T)\n        Y += [spk2idx[lab]] * x.T.shape[0]\n    return np.concatenate(X, axis=0), np.array(Y), spk2idx\n\ndef load_test_files(root_path, cfg):\n    spk2idx = {}\n    npys = cfg['test']['wav_files']\n    labs = cfg['test']['spk_ids']\n    Y = []\n    X = []\n    for npy, lab in zip(npys, labs):\n        npy_name = os.path.join(root_path, npy)\n        x = np.load(npy_name)\n        if lab not in spk2idx:\n            spk2idx[lab] = len(spk2idx)\n        X.append(x.T)\n        Y += [spk2idx[lab]]\n    return X, Y\n\ndef main(opts):\n    # find npy files in data dir\n    with open(opts.data_cfg, 'r') as cfg_f:\n        # contains train and test files\n        cfg = json.load(cfg_f)\n        train_X, train_Y, spk2idx = load_train_files(opts.data_root,\n                                                     cfg, 'train')\n        test_X, test_Y = load_test_files(opts.data_root, cfg)\n        print('Loaded trainX: ', train_X.shape)\n        print('Loaded trainY: ', train_Y.shape)\n        neigh = KNeighborsClassifier(n_neighbors=opts.k, n_jobs=opts.n_jobs)\n        neigh.fit(train_X, train_Y) \n        accs = []\n        timings = []\n        beg_t = timeit.default_timer()\n        for te_idx in range(len(test_X)):\n            test_x = test_X[te_idx]\n            facc = []\n            preds = [0.] * len(spk2idx)\n            Y_ = neigh.predict(test_x)\n            for ii in range(len(Y_)):\n                preds[Y_[ii]] += 1\n            y_ = np.argmax(preds, axis=0)\n            y = test_Y[te_idx]\n            if y_ == y:\n                accs.append(1)\n            else:\n                accs.append(0.)\n            end_t = timeit.default_timer()\n            timings.append(end_t - beg_t)\n            beg_t = timeit.default_timer()\n            print('Processing test utterance {}/{}, muttime: {:.3f} s'\n                  ''.format(te_idx + 1,\n                            len(test_X),\n                            np.mean(timings)))\n        print('Score on {} samples: {}'.format(len(accs),\n                                               np.mean(accs)))\n        with open(opts.out_log, 'w') as out_f:\n            out_f.write('{:.4f}'.format(np.asscalar(np.mean(accs))))\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_root', type=str,\n                        default=None)\n    parser.add_argument('--data_cfg', type=str,\n                        default=None,\n                        help='Dictionary containing paths to train and '\n                             'test data files.')\n    parser.add_argument('--k', type=int, default=5)\n    parser.add_argument('--n_jobs', type=int, default=None)\n    parser.add_argument('--out_log', type=str, default=None)\n\n\n    opts = parser.parse_args()\n    assert opts.data_root is not None\n    assert opts.data_cfg is not None\n    assert opts.out_log is not None\n    main(opts)\n"""
spk_id/make_fefeats_cfg.py,0,"b""import json\nimport glob\nimport os\n\n\nDATA_PATH = '../fefeats/bsz16/epoch0'\nepoch = list(range(0, 13))\nsplits = ['train', 'test', 'valid']\nMAX_WAVS_SPK = {'train':100,\n                'test':10,\n                'valid':10}\nspk2count = {}\ncfg = {}\n\nsplits = ['train', 'test', 'valid']\nspk2split = {}#0\nspk2idx = {}\ndataset = glob.glob('{}/all/*.npy'.format(DATA_PATH))\nfor filename in dataset:\n    fname = os.path.basename(filename)\n    bname = os.path.splitext(fname)[0]\n    spk_id = bname.split('_')[0]\n    if spk_id not in spk2count:\n        spk2count[spk_id] = {'train':0,\n                             'test':0,\n                             'valid':0}\n        spk2split[spk_id] = 0\n        spk2idx[spk_id] = len(spk2idx)\n    curr_split = spk2split[spk_id]\n    curr_samples = spk2count[spk_id][splits[curr_split]]\n    if  curr_samples >= MAX_WAVS_SPK[splits[curr_split]]:\n        if curr_split >= len(splits) - 1:\n            continue\n        spk2split[spk_id] += 1\n    else:\n        if splits[curr_split] not in cfg:\n            cfg[splits[curr_split]] = {'wav_files':[],\n                                       'spk_ids':[]}\n        cfg[splits[curr_split]]['wav_files'].append(fname)\n        cfg[splits[curr_split]]['spk_ids'].append(spk_id)\n        spk2count[spk_id][splits[curr_split]] += 1\ncfg['spk2idx'] = spk2idx\n\nwith open('bsz16_fefeats_data.cfg', 'w') as cfg_f:\n    cfg_f.write(json.dumps(cfg, indent=2))\n\n"""
spk_id/mfcc_baseline.py,10,"b""import numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport pickle\nimport json\nimport glob\nfrom utils import *\nfrom tensorboardX import SummaryWriter\nimport random\nimport timeit\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nimport torch.nn.functional as F\nfrom ahoproc_tools.io import read_aco_file\nimport os\nfrom waveminionet.models.frontend import WaveFe\nfrom waveminionet.models.modules import Model\nimport librosa\nfrom random import shuffle\nimport argparse\n\n\n# Make Linear classifier model\nclass LinearClassifier(Model):\n    \n    def __init__(self, num_inputs=None,\n                 num_spks=None, \n                 z_bnorm=False,\n                 name='CLS'):\n        super().__init__(name=name)\n        if z_bnorm:\n            # apply z-norm to the input\n            self.z_bnorm = nn.BatchNorm1d(frontend.emb_dim, affine=False)\n        if num_spks is None:\n            raise ValueError('Please specify a number of spks.')\n        self.fc = nn.Conv1d(num_inputs, num_spks, 1)\n        self.act = nn.LogSoftmax(dim=1)\n    \n    def forward(self, x):\n        h = x\n        if hasattr(self, 'z_bnorm'):\n            h = self.z_bnorm(h)\n        h = self.fc(x)\n        y = self.act(h)\n        return y\n\nclass MLPClassifier(Model):\n    \n    def __init__(self, num_inputs=None,\n                 num_spks=None, \n                 hidden_size=2048,\n                 z_bnorm=False,\n                 name='MLP'):\n        super().__init__(name=name, max_ckpts=1000)\n        if num_spks is None:\n            raise ValueError('Please specify a number of spks.')\n        if z_bnorm:\n            # apply z-norm to the input\n            self.z_bnorm = nn.BatchNorm1d(frontend.emb_dim, affine=False)\n        self.model = nn.Sequential(\n            nn.Conv1d(num_inputs, hidden_size, 1),\n            nn.LeakyReLU(),\n            nn.BatchNorm1d(hidden_size),\n            nn.Conv1d(hidden_size, num_spks, 1),\n            nn.LogSoftmax(dim=1)\n        )\n    \n    def forward(self, x):\n        h = x\n        if hasattr(self, 'z_bnorm'):\n            h = self.z_bnorm(h)\n        return self.model(h)\n\ndef select_model(opts, num_spks):\n    if opts.model == 'cls':\n        model = LinearClassifier(num_inputs=opts.order, \n                                 num_spks=num_spks)\n    elif opts.model == 'mlp':\n        model = MLPClassifier(num_inputs=opts.order, num_spks=num_spks,\n                              hidden_size=opts.hidden_size)\n    else:\n        raise TypeError('Unrecognized model {}'.format(opts.model))\n    return model\n\ndef main(opts):\n    CUDA = torch.cuda.is_available() and not opts.no_cuda\n    device = 'cuda' if CUDA else 'cpu'\n    torch.manual_seed(opts.seed)\n    random.seed(opts.seed)\n    np.random.seed(opts.seed)\n    if device == 'cuda':\n        torch.cuda.manual_seed_all(opts.seed)\n    spk2idx = load_spk2idx(opts.spk2idx)\n    NSPK=len(set(spk2idx.values()))\n\n    model = select_model(opts, num_spks=NSPK)\n    model.to(device)\n    print(model)\n    if opts.train:\n        print('=' * 20)\n        print('Entering TRAIN mode')\n        print('=' * 20)\n\n        with open(os.path.join(opts.save_path, 'train.opts'), 'w') as cfg_f:\n            cfg_f.write(json.dumps(vars(opts), indent=2))\n\n        # Open up guia and split valid\n        with open(opts.train_guia) as tr_guia_f: \n            tr_files = [l.rstrip() for l in tr_guia_f]\n\n\n        tr_files_, va_files = build_valid_list(tr_files, spk2idx,\n                                               va_split=opts.va_split)\n        # compute total samples dur\n        beg_t = timeit.default_timer()\n        tr_durs = compute_aco_durs(tr_files_, opts.data_root, opts.order, \n                                   ext=opts.ext, np_fmt=opts.np_fmt)\n        va_durs = compute_aco_durs(va_files, opts.data_root, opts.order,\n                                   ext=opts.ext, np_fmt=opts.np_fmt)\n        train_dur = np.sum(tr_durs)\n        valid_dur = np.sum(va_durs)\n        hop = 160\n        sr = 16000\n        end_t = timeit.default_timer()\n        print('Read tr/va {:.1f} s/{:.1f} s in {} s'.format((train_dur * hop) / sr,\n                                                            (valid_dur * hop) / sr,\n                                                            end_t - beg_t))\n        # Build Datasets\n        dset = LibriSpkIDMFCCDataset(opts.data_root,\n                                     tr_files_, spk2idx,\n                                     opts.order,\n                                     opts.stats,\n                                     ext=opts.ext,\n                                     np_fmt=opts.np_fmt)\n        va_dset = LibriSpkIDMFCCDataset(opts.data_root,\n                                        va_files, spk2idx,\n                                        opts.order, \n                                        opts.stats,\n                                        ext=opts.ext,\n                                        np_fmt=opts.np_fmt)\n        cc = Collater(max_len=opts.max_len)\n        dloader = DataLoader(dset, batch_size=opts.batch_size, collate_fn=cc,\n                             shuffle=True)\n        va_dloader = DataLoader(va_dset, batch_size=opts.batch_size, collate_fn=cc,\n                                shuffle=False)\n        tr_bpe = (train_dur // opts.max_len) // opts.batch_size\n        va_bpe = (valid_dur // opts.max_len) // opts.batch_size\n\n        te_dloader = None\n        # Build optimizer and scheduler\n        opt = select_optimizer(opts, model)\n        sched = select_scheduler(opts, opt)\n        # Make writer\n        writer = SummaryWriter(opts.save_path)\n        best_val_acc = 0\n        # flag for saver\n        best_val = False\n        for epoch in range(1, opts.epoch + 1):\n            train_epoch(dloader, model, opt, epoch, opts.log_freq, writer=writer,\n                        device=device, bpe=tr_bpe)\n            eloss, eacc = eval_epoch(va_dloader, model, epoch, opts.log_freq,\n                                     writer=writer, device=device, bpe=va_bpe,\n                                     key='valid')\n            if opts.sched_mode == 'step':\n                sched.step()\n            else:\n                sched.step(eacc)\n            if eacc > best_val_acc:\n                print('*' * 40)\n                print('New best val acc: {:.3f} => {:.3f}.'\n                      ''.format(best_val_acc, eacc))\n                print('*' * 40)\n                best_val_acc = eacc\n                best_val = True\n            model.save(opts.save_path, epoch - 1, best_val=best_val)\n            best_val = False\n    if opts.test:\n        print('=' * 20)\n        print('Entering TEST mode')\n        print('=' * 20)\n\n        model.load_pretrained(opts.test_ckpt, load_last=True, verbose=True)\n        model.to(device)\n        model.eval()\n        with open(opts.test_guia) as te_guia_f: \n            te_files = [l.rstrip() for l in te_guia_f]\n            te_dset = LibriSpkIDMFCCDataset(opts.data_root,\n                                            te_files, spk2idx,\n                                            opts.order,\n                                            opts.stats,\n                                            ext=opts.ext,\n                                            np_fmt=opts.np_fmt)\n            te_dloader = DataLoader(te_dset, batch_size=1,\n                                    shuffle=False)\n            with torch.no_grad():\n                teloss = []\n                teacc = []\n                timings = []\n                beg_t = timeit.default_timer()\n                if opts.test_log_file is not None:\n                    test_log_f = open(opts.test_log_file, 'w')\n                    test_log_f.write('Filename\\tAccuracy [%]\\tError [%]\\n')\n                else:\n                    test_log_f = None\n                for bidx, batch in enumerate(te_dloader, start=1):\n                    #X, Y, slen = batch\n                    X, Y = batch\n                    X = X.transpose(1, 2).to(device)\n                    Y = Y.to(device)\n                    Y_ = model(X)\n                    Y = Y.view(-1, 1).repeat(1, Y_.size(2))\n                    loss = F.nll_loss(Y_, Y)\n                    acc = accuracy(Y_, Y)\n                    if test_log_f:\n                        test_log_f.write('{}\\t{:.2f}\\t{:.2f}\\n' \\\n                                         ''.format(te_files[bidx - 1],\n                                                   acc * 100,\n                                                   100 - (acc * 100)))\n                    teacc.append(accuracy(Y_, Y))\n                    teloss.append(loss)\n                    end_t = timeit.default_timer()\n                    timings.append(end_t - beg_t)\n                    beg_t = timeit.default_timer()\n                    if bidx % 100 == 0 or bidx == 1:\n                        mteloss = np.mean(teloss)\n                        mteacc = np.mean(teacc)\n                        mtimings = np.mean(timings)\n                    print('Processed test file {}/{} mfiletime: {:.2f} s, '\n                          'macc: {:.4f}, mloss: {:.2f}'\n                          ''.format(bidx, len(te_dloader), mtimings,\n                                    mteacc, mteloss),\n                          end='\\r')\n                print() \n                if test_log_f:\n                    test_log_f.write('-' * 30 + '\\n')\n                    test_log_f.write('Test accuracy: ' \\\n                                     '{:.2f}\\n'.format(np.mean(teacc) * 100))\n                    test_log_f.write('Test error: ' \\\n                                     '{:.2f}\\n'.format(100 - (np.mean(teacc) *100)))\n                    test_log_f.write('Test loss: ' \\\n                                     '{:.2f}\\n'.format(np.mean(teloss)))\n                    test_log_f.close()\n                print('Test accuracy: {:.4f}'.format(np.mean(teacc)))\n                print('Test loss: {:.2f}'.format(np.mean(teloss)))\n\n\ndef train_epoch(dloader_, model, opt, epoch, log_freq=1, writer=None,\n                device='cpu', bpe=None):\n    model.train()\n    if bpe is None:\n        # default is just dataloader length\n        bpe = len(dloader_)\n    global_idx = (epoch - 1) * bpe\n    timings = []\n    beg_t = timeit.default_timer()\n    #for bidx, batch in enumerate(dloader_, start=1):\n    iterator = iter(dloader)\n    for bidx in range(1, bpe + 1):\n        try:\n            batch = next(iterator)\n        except StopIteration:\n            iterator = iter(dloader)\n            batch = next(iterator)\n        opt.zero_grad()\n        X, Y, slens = batch\n        X = X.transpose(1, 2)\n        X = X.to(device)\n        Y = Y.to(device)\n        Y_ = model(X)\n        Y = Y.view(-1, 1).repeat(1, Y_.size(2))\n        loss = F.nll_loss(Y_.squeeze(-1), Y)\n        loss.backward()\n        opt.step()\n        end_t = timeit.default_timer()\n        timings.append(end_t - beg_t)\n        beg_t = timeit.default_timer()\n        #if bidx % log_freq == 0 or bidx >= len(dloader_):\n        if bidx % log_freq == 0 or bidx >= bpe:\n            acc = accuracy(Y_, Y)\n            log_str = 'Batch {:5d}/{:5d} (Epoch {:3d}, Gidx {:5d})' \\\n                      ' '.format(bidx, bpe,\n                                 epoch, global_idx)\n            log_str += 'loss: {:.3f} '.format(loss.item())\n            log_str += 'bacc: {:.2f} '.format(acc)\n            log_str += 'mbtime: {:.3f} s'.format(np.mean(timings))\n            print(log_str)\n            if writer is not None:\n                writer.add_scalar('train/loss', loss.item(),\n                                  global_idx)\n                writer.add_scalar('train/bacc', acc, global_idx)\n        global_idx += 1\n\ndef eval_epoch(dloader_, model, epoch, log_freq=1, writer=None, device='cpu',\n               bpe=None, key='eval'):\n    model.eval()\n    with torch.no_grad():\n        if bpe is None:\n            # default is just dataloader length\n            bpe = len(dloader_)\n        eval_losses = []\n        eval_accs = []\n        timings = []\n        beg_t = timeit.default_timer()\n        #for bidx, batch in enumerate(dloader_, start=1):\n        iterator = iter(dloader)\n        for bidx in range(1, bpe + 1):\n            try:\n                batch = next(iterator)\n            except StopIteration:\n                iterator = iter(dloader)\n                batch = next(iterator)\n            X, Y, slens = batch\n            X = X.transpose(1, 2)\n            X = X.to(device)\n            Y = Y.to(device)\n            Y_ = model(X)\n            Y = Y.view(-1, 1).repeat(1, Y_.size(2))\n            loss = F.nll_loss(Y_, Y)\n            eval_losses.append(loss.item())\n            acc = accuracy(Y_, Y)\n            eval_accs.append(acc)\n            end_t = timeit.default_timer()\n            timings.append(end_t - beg_t)\n            beg_t = timeit.default_timer()\n            #if bidx % log_freq == 0 or bidx >= len(dloader_):\n            if bidx % log_freq == 0 or bidx >= bpe:\n                \n                log_str = 'EVAL::{} Batch {:4d}/{:4d} (Epoch {:3d})' \\\n                          ' '.format(key, bidx, bpe,\n                                     epoch)\n                log_str += 'loss: {:.3f} '.format(loss.item())\n                log_str += 'bacc: {:.2f} '.format(acc)\n                log_str += 'mbtime: {:.3f} s'.format(np.mean(timings))\n                print(log_str)\n        mloss = np.mean(eval_losses)\n        macc = np.mean(eval_accs)\n        if writer is not None:\n            writer.add_scalar('{}/loss'.format(key), mloss,\n                              epoch)\n            writer.add_scalar('{}/acc'.format(key), macc, epoch)\n        print('EVAL epoch {:3d} mean loss: {:.3f}, mean acc: {:.2f} '\n             ''.format(epoch, mloss, macc))\n        return mloss, macc\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--save_path', type=str, default='ckpt_mfcc')\n    parser.add_argument('--data_root', type=str, default=None)\n    parser.add_argument('--batch_size', type=int, default=20)\n    parser.add_argument('--train_guia', type=str, default=None)\n    parser.add_argument('--test_guia', type=str, default=None)\n    parser.add_argument('--spk2idx', type=str, default=None)\n    parser.add_argument('--log_freq', type=int, default=50)\n    parser.add_argument('--epoch', type=int, default=1000)\n    parser.add_argument('--patience', type=int, default=10)\n    parser.add_argument('--seed', type=int, default=4)\n    parser.add_argument('--no-cuda', action='store_true', default=False)\n    parser.add_argument('--z_bnorm', action='store_true', default=False,\n                        help='Use z-norm in z, before any model (Default: '\n                             'False).')\n    parser.add_argument('--va_split', type=float, default=0.2)\n    parser.add_argument('--lr', type=float, default=0.001)\n    parser.add_argument('--momentum', type=float, default=0.9)\n    parser.add_argument('--max_len', type=int, default=100)\n    parser.add_argument('--hidden_size', type=int, default=2048)\n    parser.add_argument('--sched_mode', type=str, default='plateau',\n                        help='(1) plateau (validation), (2) '\n                             'step (step decay) (Def: plateau).')\n    parser.add_argument('--sched_step_size', type=int,\n                        default=30, help='Number of epochs to apply '\n                                          'a learning rate decay (Def: 30).')\n    parser.add_argument('--lrdec', type=float, default=0.1,\n                        help='Decay factor of learning rate after '\n                             'patience epochs of valid accuracy not '\n                             'improving (Def: 0.1).')\n    parser.add_argument('--test_ckpt', type=str, default=None)\n    parser.add_argument('--plateau_mode', type=str, default='max',\n                        help='LR Plateau scheduling mode; (1) max, (2) min '\n                             '(Def: max).')\n    parser.add_argument('--order', type=int, default=39)\n    parser.add_argument('--stats', type=str, default=None)\n    parser.add_argument('--opt', type=str, default='adam')\n    parser.add_argument('--model', type=str, default='mlp',\n                        help='(1) cls, (2) mlp (Def: mlp).')\n    parser.add_argument('--train', action='store_true', default=False)\n    parser.add_argument('--test', action='store_true', default=False)\n    parser.add_argument('--ext', type=str, default='mfcc')\n    parser.add_argument('--test_log_file', type=str, default=None,\n                        help='Possible test log file (Def: None).')\n    parser.add_argument('--np-fmt', action='store_true', default=False,\n                        help='Whether or not aco files are in numpy '\n                             'format (Def: False).')\n    \n    opts = parser.parse_args()\n\n    if not os.path.exists(opts.save_path):\n        os.makedirs(opts.save_path)\n\n    main(opts)\n"""
spk_id/neural_networks.py,92,"b'##########################################################\n# pytorch-kaldi v.0.1                                      \n# Mirco Ravanelli, Titouan Parcollet\n# Mila, University of Montreal\n# October 2018\n##########################################################\n\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport numpy as np\nfrom distutils.util import strtobool\nimport math\n\n# uncomment below if you want to use SRU\n# and you need to install SRU: pip install sru[cuda].\n# or you can install it from source code: https://github.com/taolei87/sru.\n# import sru\n\ndef context_window(fea,left,right):\n \n    N_elem=fea.shape[0]\n    N_fea=fea.shape[1]\n    \n    fea_conc=np.empty([N_elem,N_fea*(left+right+1)])\n    \n    index_fea=0\n    for lag in range(-left,right+1):\n        fea_conc[:,index_fea:index_fea+fea.shape[1]]=np.roll(fea,lag,axis=0)\n        index_fea=index_fea+fea.shape[1]\n        \n    fea_conc=fea_conc[left:fea_conc.shape[0]-right]\n    \n    return fea_conc\n\nclass LayerNorm(nn.Module):\n\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm,self).__init__()\n        self.gamma = nn.Parameter(torch.ones(features))\n        self.beta = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\n\ndef act_fun(act_type):\n\n if act_type==""relu"":\n    return nn.ReLU()\n            \n if act_type==""tanh"":\n    return nn.Tanh()\n            \n if act_type==""sigmoid"":\n    return nn.Sigmoid()\n           \n if act_type==""leaky_relu"":\n    return nn.LeakyReLU(0.2)\n            \n if act_type==""elu"":\n    return nn.ELU()\n                     \n if act_type==""softmax"":\n    return nn.LogSoftmax(dim=1)\n        \n if act_type==""linear"":\n     return nn.LeakyReLU(1) # initializzed like this, but not used in forward!\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, options,inp_dim):\n        super(MLP, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.dnn_lay=list(map(int, options[\'dnn_lay\'].split(\',\')))\n        self.dnn_drop=list(map(float, options[\'dnn_drop\'].split(\',\'))) \n        self.dnn_use_batchnorm=list(map(strtobool, options[\'dnn_use_batchnorm\'].split(\',\')))\n        self.dnn_use_laynorm=list(map(strtobool, options[\'dnn_use_laynorm\'].split(\',\'))) \n        self.dnn_use_laynorm_inp=strtobool(options[\'dnn_use_laynorm_inp\'])\n        self.dnn_use_batchnorm_inp=strtobool(options[\'dnn_use_batchnorm_inp\'])\n        self.dnn_act=options[\'dnn_act\'].split(\',\')\n        \n       \n        self.wx  = nn.ModuleList([])\n        self.bn  = nn.ModuleList([])\n        self.ln  = nn.ModuleList([])\n        self.act = nn.ModuleList([])\n        self.drop = nn.ModuleList([])\n       \n  \n        # input layer normalization\n        if self.dnn_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # input batch normalization    \n        if self.dnn_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n           \n        self.N_dnn_lay=len(self.dnn_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_dnn_lay):\n            \n             # dropout\n             self.drop.append(nn.Dropout(p=self.dnn_drop[i]))\n             \n             # activation\n             self.act.append(act_fun(self.dnn_act[i]))\n             \n             \n             add_bias=True\n             \n             # layer norm initialization\n             self.ln.append(LayerNorm(self.dnn_lay[i]))\n             self.bn.append(nn.BatchNorm1d(self.dnn_lay[i],momentum=0.05))\n             \n             if self.dnn_use_laynorm[i] or self.dnn_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Linear operations\n             self.wx.append(nn.Linear(current_input, self.dnn_lay[i],bias=add_bias))\n             \n             # weight initialization\n             self.wx[i].weight = torch.nn.Parameter(torch.Tensor(self.dnn_lay[i],current_input).uniform_(-np.sqrt(0.01/(current_input+self.dnn_lay[i])),np.sqrt(0.01/(current_input+self.dnn_lay[i]))))\n             self.wx[i].bias = torch.nn.Parameter(torch.zeros(self.dnn_lay[i]))\n             \n             current_input=self.dnn_lay[i]\n             \n        self.out_dim=current_input\n         \n    def forward(self, x):\n        \n      # Applying Layer/Batch Norm\n      if bool(self.dnn_use_laynorm_inp):\n        x=self.ln0((x))\n        \n      if bool(self.dnn_use_batchnorm_inp):\n\n        x=self.bn0((x))\n        \n      for i in range(self.N_dnn_lay):\n           \n          if self.dnn_use_laynorm[i] and not(self.dnn_use_batchnorm[i]):\n           x = self.drop[i](self.act[i](self.ln[i](self.wx[i](x))))\n          \n          if self.dnn_use_batchnorm[i] and not(self.dnn_use_laynorm[i]):\n           x = self.drop[i](self.act[i](self.bn[i](self.wx[i](x))))\n           \n          if self.dnn_use_batchnorm[i]==True and self.dnn_use_laynorm[i]==True:\n           x = self.drop[i](self.act[i](self.bn[i](self.ln[i](self.wx[i](x)))))\n          \n          if self.dnn_use_batchnorm[i]==False and self.dnn_use_laynorm[i]==False:\n           x = self.drop[i](self.act[i](self.wx[i](x)))\n            \n          \n      return x\n\n\nclass LSTM_cudnn(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(LSTM_cudnn, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.hidden_size=int(options[\'hidden_size\'])\n        self.num_layers=int(options[\'num_layers\'])\n        self.bias=bool(strtobool(options[\'bias\']))\n        self.batch_first=bool(strtobool(options[\'batch_first\']))\n        self.dropout=float(options[\'dropout\'])\n        self.bidirectional=bool(strtobool(options[\'bidirectional\']))\n        \n        self.lstm = nn.ModuleList([nn.LSTM(self.input_dim, self.hidden_size, self.num_layers, \n                            bias=self.bias,dropout=self.dropout,bidirectional=self.bidirectional)])\n         \n        self.out_dim=self.hidden_size+self.bidirectional*self.hidden_size\n               \n        \n    def forward(self, x):\n        \n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n            c0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            c0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            \n        if x.is_cuda:\n            h0=h0.cuda()\n            c0=c0.cuda()\n            \n        output, (hn, cn) = self.lstm[0](x, (h0, c0))\n        \n        \n        return output\n    \n\nclass GRU_cudnn(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(GRU_cudnn, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.hidden_size=int(options[\'hidden_size\'])\n        self.num_layers=int(options[\'num_layers\'])\n        self.bias=bool(strtobool(options[\'bias\']))\n        self.batch_first=bool(strtobool(options[\'batch_first\']))\n        self.dropout=float(options[\'dropout\'])\n        self.bidirectional=bool(strtobool(options[\'bidirectional\']))\n        \n        self.gru = nn.ModuleList([nn.GRU(self.input_dim, self.hidden_size, self.num_layers, \n                            bias=self.bias,dropout=self.dropout,bidirectional=self.bidirectional)])\n         \n        self.out_dim=self.hidden_size+self.bidirectional*self.hidden_size\n               \n        \n    def forward(self, x):\n        \n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            \n        if x.is_cuda:\n            h0=h0.cuda()\n\n        output, hn = self.gru[0](x, h0)\n        \n        \n        return output\n \n    \nclass RNN_cudnn(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(RNN_cudnn, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.hidden_size=int(options[\'hidden_size\'])\n        self.num_layers=int(options[\'num_layers\'])\n        self.nonlinearity=options[\'nonlinearity\']\n        self.bias=bool(strtobool(options[\'bias\']))\n        self.batch_first=bool(strtobool(options[\'batch_first\']))\n        self.dropout=float(options[\'dropout\'])\n        self.bidirectional=bool(strtobool(options[\'bidirectional\']))\n        \n        self.rnn = nn.ModuleList([nn.RNN(self.input_dim, self.hidden_size, self.num_layers, \n                            nonlinearity=self.nonlinearity,bias=self.bias,dropout=self.dropout,bidirectional=self.bidirectional)])\n         \n        self.out_dim=self.hidden_size+self.bidirectional*self.hidden_size\n               \n        \n    def forward(self, x):\n        \n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            \n        if x.is_cuda:\n            h0=h0.cuda()\n\n        output, hn = self.rnn[0](x, h0)\n        \n        \n        return output\n    \n    \nclass LSTM(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(LSTM, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.lstm_lay=list(map(int, options[\'lstm_lay\'].split(\',\')))\n        self.lstm_drop=list(map(float, options[\'lstm_drop\'].split(\',\'))) \n        self.lstm_use_batchnorm=list(map(strtobool, options[\'lstm_use_batchnorm\'].split(\',\')))\n        self.lstm_use_laynorm=list(map(strtobool, options[\'lstm_use_laynorm\'].split(\',\'))) \n        self.lstm_use_laynorm_inp=strtobool(options[\'lstm_use_laynorm_inp\'])\n        self.lstm_use_batchnorm_inp=strtobool(options[\'lstm_use_batchnorm_inp\'])\n        self.lstm_act=options[\'lstm_act\'].split(\',\')\n        self.lstm_orthinit=strtobool(options[\'lstm_orthinit\'])\n\n        self.bidir=strtobool(options[\'lstm_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wfx  = nn.ModuleList([]) # Forget\n        self.ufh  = nn.ModuleList([]) # Forget\n        \n        self.wix  = nn.ModuleList([]) # Input\n        self.uih  = nn.ModuleList([]) # Input  \n        \n        self.wox  = nn.ModuleList([]) # Output\n        self.uoh  = nn.ModuleList([]) # Output  \n        \n        self.wcx  = nn.ModuleList([]) # Cell state\n        self.uch = nn.ModuleList([])  # Cell state\n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wfx  = nn.ModuleList([]) # Batch Norm\n        self.bn_wix  = nn.ModuleList([]) # Batch Norm\n        self.bn_wox  = nn.ModuleList([]) # Batch Norm\n        self.bn_wcx = nn.ModuleList([]) # Batch Norm\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.lstm_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.lstm_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_lstm_lay=len(self.lstm_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_lstm_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.lstm_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.lstm_use_laynorm[i] or self.lstm_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wfx.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n             self.wix.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n             self.wox.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n             self.wcx.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n            \n             # Recurrent connections\n             self.ufh.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             self.uih.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             self.uoh.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             self.uch.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             \n             if self.lstm_orthinit:\n                nn.init.orthogonal_(self.ufh[i].weight)\n                nn.init.orthogonal_(self.uih[i].weight)\n                nn.init.orthogonal_(self.uoh[i].weight)\n                nn.init.orthogonal_(self.uch[i].weight)\n            \n             \n             # batch norm initialization\n             self.bn_wfx.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n             self.bn_wix.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n             self.bn_wox.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n             self.bn_wcx.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n                \n             self.ln.append(LayerNorm(self.lstm_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.lstm_lay[i]\n             else:\n                 current_input=self.lstm_lay[i]\n                 \n        self.out_dim=self.lstm_lay[i]+self.bidir*self.lstm_lay[i]\n            \n             \n        \n    def forward(self, x):\n\n        # Applying Layer/Batch Norm\n        if bool(self.lstm_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.lstm_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_lstm_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.lstm_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.lstm_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.lstm_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.lstm_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wfx_out=self.wfx[i](x)\n            wix_out=self.wix[i](x)\n            wox_out=self.wox[i](x)\n            wcx_out=self.wcx[i](x)\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.lstm_use_batchnorm[i]:\n\n                wfx_out_bn=self.bn_wfx[i](wfx_out.view(wfx_out.shape[0]*wfx_out.shape[1],wfx_out.shape[2]))\n                wfx_out=wfx_out_bn.view(wfx_out.shape[0],wfx_out.shape[1],wfx_out.shape[2])\n         \n                wix_out_bn=self.bn_wix[i](wix_out.view(wix_out.shape[0]*wix_out.shape[1],wix_out.shape[2]))\n                wix_out=wix_out_bn.view(wix_out.shape[0],wix_out.shape[1],wix_out.shape[2])\n   \n                wox_out_bn=self.bn_wox[i](wox_out.view(wox_out.shape[0]*wox_out.shape[1],wox_out.shape[2]))\n                wox_out=wox_out_bn.view(wox_out.shape[0],wox_out.shape[1],wox_out.shape[2])\n\n                wcx_out_bn=self.bn_wcx[i](wcx_out.view(wcx_out.shape[0]*wcx_out.shape[1],wcx_out.shape[2]))\n                wcx_out=wcx_out_bn.view(wcx_out.shape[0],wcx_out.shape[1],wcx_out.shape[2]) \n            \n            \n            # Processing time steps\n            hiddens = []\n            ct=h_init\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # LSTM equations\n                ft=torch.sigmoid(wfx_out[k]+self.ufh[i](ht))\n                it=torch.sigmoid(wix_out[k]+self.uih[i](ht))\n                ot=torch.sigmoid(wox_out[k]+self.uoh[i](ht))\n                ct=it*self.act[i](wcx_out[k]+self.uch[i](ht))*drop_mask+ft*ct\n                ht=ot*self.act[i](ct)\n                \n                if self.lstm_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n    \nclass GRU(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(GRU, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.gru_lay=list(map(int, options[\'gru_lay\'].split(\',\')))\n        self.gru_drop=list(map(float, options[\'gru_drop\'].split(\',\'))) \n        self.gru_use_batchnorm=list(map(strtobool, options[\'gru_use_batchnorm\'].split(\',\')))\n        self.gru_use_laynorm=list(map(strtobool, options[\'gru_use_laynorm\'].split(\',\'))) \n        self.gru_use_laynorm_inp=strtobool(options[\'gru_use_laynorm_inp\'])\n        self.gru_use_batchnorm_inp=strtobool(options[\'gru_use_batchnorm_inp\'])\n        self.gru_orthinit=strtobool(options[\'gru_orthinit\'])\n        self.gru_act=options[\'gru_act\'].split(\',\')\n        self.bidir=strtobool(options[\'gru_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n        \n        self.wz  = nn.ModuleList([]) # Update Gate\n        self.uz  = nn.ModuleList([]) # Update Gate\n        \n        self.wr  = nn.ModuleList([]) # Reset Gate\n        self.ur  = nn.ModuleList([]) # Reset Gate  \n        \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n        self.bn_wz  = nn.ModuleList([]) # Batch Norm\n        self.bn_wr  = nn.ModuleList([]) # Batch Norm\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.gru_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.gru_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_gru_lay=len(self.gru_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_gru_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.gru_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.gru_use_laynorm[i] or self.gru_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.gru_lay[i],bias=add_bias))\n             self.wz.append(nn.Linear(current_input, self.gru_lay[i],bias=add_bias))\n             self.wr.append(nn.Linear(current_input, self.gru_lay[i],bias=add_bias))\n             \n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.gru_lay[i], self.gru_lay[i],bias=False))\n             self.uz.append(nn.Linear(self.gru_lay[i], self.gru_lay[i],bias=False))\n             self.ur.append(nn.Linear(self.gru_lay[i], self.gru_lay[i],bias=False))\n\n             if self.gru_orthinit:\n                nn.init.orthogonal_(self.uh[i].weight)\n                nn.init.orthogonal_(self.uz[i].weight)\n                nn.init.orthogonal_(self.ur[i].weight)\n\n             \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.gru_lay[i],momentum=0.05))\n             self.bn_wz.append(nn.BatchNorm1d(self.gru_lay[i],momentum=0.05))\n             self.bn_wr.append(nn.BatchNorm1d(self.gru_lay[i],momentum=0.05))\n\n                \n             self.ln.append(LayerNorm(self.gru_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.gru_lay[i]\n             else:\n                 current_input=self.gru_lay[i]\n                 \n        self.out_dim=self.gru_lay[i]+self.bidir*self.gru_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.gru_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.gru_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_gru_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.gru_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.gru_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.gru_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.gru_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            wz_out=self.wz[i](x)\n            wr_out=self.wr[i](x)\n\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.gru_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n                wz_out_bn=self.bn_wz[i](wz_out.view(wz_out.shape[0]*wz_out.shape[1],wz_out.shape[2]))\n                wz_out=wz_out_bn.view(wz_out.shape[0],wz_out.shape[1],wz_out.shape[2])\n   \n                wr_out_bn=self.bn_wr[i](wr_out.view(wr_out.shape[0]*wr_out.shape[1],wr_out.shape[2]))\n                wr_out=wr_out_bn.view(wr_out.shape[0],wr_out.shape[1],wr_out.shape[2])\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # gru equation\n                zt=torch.sigmoid(wz_out[k]+self.uz[i](ht))\n                rt=torch.sigmoid(wr_out[k]+self.ur[i](ht))\n                at=wh_out[k]+self.uh[i](rt*ht)\n                hcand=self.act[i](at)*drop_mask\n                ht=(zt*ht+(1-zt)*hcand)\n                \n                \n                if self.gru_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n\n\nclass liGRU(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(liGRU, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.ligru_lay=list(map(int, options[\'ligru_lay\'].split(\',\')))\n        self.ligru_drop=list(map(float, options[\'ligru_drop\'].split(\',\'))) \n        self.ligru_use_batchnorm=list(map(strtobool, options[\'ligru_use_batchnorm\'].split(\',\')))\n        self.ligru_use_laynorm=list(map(strtobool, options[\'ligru_use_laynorm\'].split(\',\'))) \n        self.ligru_use_laynorm_inp=strtobool(options[\'ligru_use_laynorm_inp\'])\n        self.ligru_use_batchnorm_inp=strtobool(options[\'ligru_use_batchnorm_inp\'])\n        self.ligru_orthinit=strtobool(options[\'ligru_orthinit\'])\n        self.ligru_act=options[\'ligru_act\'].split(\',\')\n        self.bidir=strtobool(options[\'ligru_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n        \n        self.wz  = nn.ModuleList([]) # Update Gate\n        self.uz  = nn.ModuleList([]) # Update Gate\n              \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n        self.bn_wz  = nn.ModuleList([]) # Batch Norm\n\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.ligru_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.ligru_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_ligru_lay=len(self.ligru_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_ligru_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.ligru_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.ligru_use_laynorm[i] or self.ligru_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.ligru_lay[i],bias=add_bias))\n             self.wz.append(nn.Linear(current_input, self.ligru_lay[i],bias=add_bias))\n\n             \n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i],bias=False))\n             self.uz.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i],bias=False))\n\n             if self.ligru_orthinit:\n             \tnn.init.orthogonal_(self.uh[i].weight)\n             \tnn.init.orthogonal_(self.uz[i].weight)\n\n\n             \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.ligru_lay[i],momentum=0.05))\n             self.bn_wz.append(nn.BatchNorm1d(self.ligru_lay[i],momentum=0.05))\n\n\n                \n             self.ln.append(LayerNorm(self.ligru_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.ligru_lay[i]\n             else:\n                 current_input=self.ligru_lay[i]\n                 \n        self.out_dim=self.ligru_lay[i]+self.bidir*self.ligru_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.ligru_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.ligru_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_ligru_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.ligru_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.ligru_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.ligru_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.ligru_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            wz_out=self.wz[i](x)\n\n\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.ligru_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n                wz_out_bn=self.bn_wz[i](wz_out.view(wz_out.shape[0]*wz_out.shape[1],wz_out.shape[2]))\n                wz_out=wz_out_bn.view(wz_out.shape[0],wz_out.shape[1],wz_out.shape[2])\n\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # ligru equation\n                zt=torch.sigmoid(wz_out[k]+self.uz[i](ht))\n                at=wh_out[k]+self.uh[i](ht)\n                hcand=self.act[i](at)*drop_mask\n                ht=(zt*ht+(1-zt)*hcand)\n                \n                \n                if self.ligru_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n    \nclass minimalGRU(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(minimalGRU, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.minimalgru_lay=list(map(int, options[\'minimalgru_lay\'].split(\',\')))\n        self.minimalgru_drop=list(map(float, options[\'minimalgru_drop\'].split(\',\'))) \n        self.minimalgru_use_batchnorm=list(map(strtobool, options[\'minimalgru_use_batchnorm\'].split(\',\')))\n        self.minimalgru_use_laynorm=list(map(strtobool, options[\'minimalgru_use_laynorm\'].split(\',\'))) \n        self.minimalgru_use_laynorm_inp=strtobool(options[\'minimalgru_use_laynorm_inp\'])\n        self.minimalgru_use_batchnorm_inp=strtobool(options[\'minimalgru_use_batchnorm_inp\'])\n        self.minimalgru_orthinit=strtobool(options[\'minimalgru_orthinit\'])\n        self.minimalgru_act=options[\'minimalgru_act\'].split(\',\')\n        self.bidir=strtobool(options[\'minimalgru_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n        \n        self.wz  = nn.ModuleList([]) # Update Gate\n        self.uz  = nn.ModuleList([]) # Update Gate\n              \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n        self.bn_wz  = nn.ModuleList([]) # Batch Norm\n\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.minimalgru_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.minimalgru_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_minimalgru_lay=len(self.minimalgru_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_minimalgru_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.minimalgru_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.minimalgru_use_laynorm[i] or self.minimalgru_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.minimalgru_lay[i],bias=add_bias))\n             self.wz.append(nn.Linear(current_input, self.minimalgru_lay[i],bias=add_bias))\n\n             \n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.minimalgru_lay[i], self.minimalgru_lay[i],bias=False))\n             self.uz.append(nn.Linear(self.minimalgru_lay[i], self.minimalgru_lay[i],bias=False))\n\n             if self.minimalgru_orthinit:\n             \tnn.init.orthogonal_(self.uh[i].weight)\n             \tnn.init.orthogonal_(self.uz[i].weight)\n\n\n             \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.minimalgru_lay[i],momentum=0.05))\n             self.bn_wz.append(nn.BatchNorm1d(self.minimalgru_lay[i],momentum=0.05))\n\n\n                \n             self.ln.append(LayerNorm(self.minimalgru_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.minimalgru_lay[i]\n             else:\n                 current_input=self.minimalgru_lay[i]\n                 \n        self.out_dim=self.minimalgru_lay[i]+self.bidir*self.minimalgru_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.minimalgru_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.minimalgru_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_minimalgru_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.minimalgru_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.minimalgru_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.minimalgru_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.minimalgru_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            wz_out=self.wz[i](x)\n\n\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.minimalgru_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n                wz_out_bn=self.bn_wz[i](wz_out.view(wz_out.shape[0]*wz_out.shape[1],wz_out.shape[2]))\n                wz_out=wz_out_bn.view(wz_out.shape[0],wz_out.shape[1],wz_out.shape[2])\n\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # minimalgru equation\n                zt=torch.sigmoid(wz_out[k]+self.uz[i](ht))\n                at=wh_out[k]+self.uh[i](zt*ht)\n                hcand=self.act[i](at)*drop_mask\n                ht=(zt*ht+(1-zt)*hcand)\n                \n                \n                if self.minimalgru_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n\nclass RNN(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(RNN, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.rnn_lay=list(map(int, options[\'rnn_lay\'].split(\',\')))\n        self.rnn_drop=list(map(float, options[\'rnn_drop\'].split(\',\'))) \n        self.rnn_use_batchnorm=list(map(strtobool, options[\'rnn_use_batchnorm\'].split(\',\')))\n        self.rnn_use_laynorm=list(map(strtobool, options[\'rnn_use_laynorm\'].split(\',\'))) \n        self.rnn_use_laynorm_inp=strtobool(options[\'rnn_use_laynorm_inp\'])\n        self.rnn_use_batchnorm_inp=strtobool(options[\'rnn_use_batchnorm_inp\'])\n        self.rnn_orthinit=strtobool(options[\'rnn_orthinit\'])\n        self.rnn_act=options[\'rnn_act\'].split(\',\')\n        self.bidir=strtobool(options[\'rnn_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n                   \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.rnn_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.rnn_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_rnn_lay=len(self.rnn_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_rnn_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.rnn_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.rnn_use_laynorm[i] or self.rnn_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.rnn_lay[i],bias=add_bias))\n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.rnn_lay[i], self.rnn_lay[i],bias=False))\n\n             if self.rnn_orthinit:\n             \tnn.init.orthogonal_(self.uh[i].weight)\n          \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.rnn_lay[i],momentum=0.05))\n\n             self.ln.append(LayerNorm(self.rnn_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.rnn_lay[i]\n             else:\n                 current_input=self.rnn_lay[i]\n                 \n        self.out_dim=self.rnn_lay[i]+self.bidir*self.rnn_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.rnn_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.rnn_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_rnn_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.rnn_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.rnn_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.rnn_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.rnn_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.rnn_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # rnn equation\n                at=wh_out[k]+self.uh[i](ht)\n                ht=self.act[i](at)*drop_mask\n                \n                \n                if self.rnn_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n\nclass CNN(nn.Module):\n    \n    def __init__(self,options,inp_dim):\n       super(CNN,self).__init__()\n    \n       # Reading parameters\n       self.input_dim=inp_dim\n       self.cnn_N_filt=list(map(int, options[\'cnn_N_filt\'].split(\',\')))\n\n       self.cnn_len_filt=list(map(int, options[\'cnn_len_filt\'].split(\',\')))\n       self.cnn_max_pool_len=list(map(int, options[\'cnn_max_pool_len\'].split(\',\')))\n       \n       self.cnn_act=options[\'cnn_act\'].split(\',\')\n       self.cnn_drop=list(map(float, options[\'cnn_drop\'].split(\',\')))\n       \n       self.cnn_use_laynorm=list(map(strtobool, options[\'cnn_use_laynorm\'].split(\',\')))\n       self.cnn_use_batchnorm=list(map(strtobool, options[\'cnn_use_batchnorm\'].split(\',\')))\n       self.cnn_use_laynorm_inp=strtobool(options[\'cnn_use_laynorm_inp\'])\n       self.cnn_use_batchnorm_inp=strtobool(options[\'cnn_use_batchnorm_inp\'])\n       \n       self.N_cnn_lay=len(self.cnn_N_filt)\n       self.conv  = nn.ModuleList([])\n       self.bn  = nn.ModuleList([])\n       self.ln  = nn.ModuleList([])\n       self.act = nn.ModuleList([])\n       self.drop = nn.ModuleList([])\n       \n       \n       if self.cnn_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n           \n       if self.cnn_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n           \n       current_input=self.input_dim \n       \n       for i in range(self.N_cnn_lay):\n\n         N_filt=int(self.cnn_N_filt[i])\n         len_filt=int(self.cnn_len_filt[i])\n         \n         # dropout\n         self.drop.append(nn.Dropout(p=self.cnn_drop[i]))\n         \n         # activation\n         self.act.append(act_fun(self.cnn_act[i]))\n            \n         # layer norm initialization         \n         self.ln.append(LayerNorm([N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])]))\n\n         self.bn.append(nn.BatchNorm1d(N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i]),momentum=0.05))\n\n            \n\n         if i==0:\n          self.conv.append(nn.Conv1d(1, N_filt, len_filt))\n              \n         else:\n          self.conv.append(nn.Conv1d(self.cnn_N_filt[i-1], self.cnn_N_filt[i], self.cnn_len_filt[i]))\n          \n         current_input=int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])\n\n         \n       self.out_dim=current_input*N_filt\n\n\n\n    def forward(self, x):\n        \n       batch=x.shape[0]\n       seq_len=x.shape[1]\n       \n       if bool(self.cnn_use_laynorm_inp):\n        x=self.ln0((x))\n        \n       if bool(self.cnn_use_batchnorm_inp):\n        x=self.bn0((x))\n        \n       x=x.view(batch,1,seq_len)\n       \n       for i in range(self.N_cnn_lay):\n           \n         if self.cnn_use_laynorm[i]:\n          x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))\n          \n         if self.cnn_use_batchnorm[i]:\n          x = self.drop[i](self.act[i](self.bn[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))\n\n         if self.cnn_use_batchnorm[i]==False and self.cnn_use_laynorm[i]==False:\n          x = self.drop[i](self.act[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i])))\n\n       \n       x = x.view(batch,-1)\n\n       return x\n\n\nclass SincNet(nn.Module):\n    \n    def __init__(self,options,inp_dim):\n       super(SincNet,self).__init__()\n    \n       # Reading parameters\n       self.input_dim=inp_dim\n       self.sinc_N_filt=list(map(int, options[\'sinc_N_filt\'].split(\',\')))\n\n       self.sinc_len_filt=list(map(int, options[\'sinc_len_filt\'].split(\',\')))\n       self.sinc_max_pool_len=list(map(int, options[\'sinc_max_pool_len\'].split(\',\')))\n       \n       self.sinc_act=options[\'sinc_act\'].split(\',\')\n       self.sinc_drop=list(map(float, options[\'sinc_drop\'].split(\',\')))\n       \n       self.sinc_use_laynorm=list(map(strtobool, options[\'sinc_use_laynorm\'].split(\',\')))\n       self.sinc_use_batchnorm=list(map(strtobool, options[\'sinc_use_batchnorm\'].split(\',\')))\n       self.sinc_use_laynorm_inp=strtobool(options[\'sinc_use_laynorm_inp\'])\n       self.sinc_use_batchnorm_inp=strtobool(options[\'sinc_use_batchnorm_inp\'])\n       \n       self.N_sinc_lay=len(self.sinc_N_filt)\n       \n       self.sinc_sample_rate=int(options[\'sinc_sample_rate\'])\n       self.sinc_min_low_hz=int(options[\'sinc_min_low_hz\'])\n       self.sinc_min_band_hz=int(options[\'sinc_min_band_hz\'])\n\n       \n       self.conv  = nn.ModuleList([])\n       self.bn  = nn.ModuleList([])\n       self.ln  = nn.ModuleList([])\n       self.act = nn.ModuleList([])\n       self.drop = nn.ModuleList([])\n       \n       \n       if self.sinc_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n           \n       if self.sinc_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n           \n       current_input=self.input_dim \n       \n       for i in range(self.N_sinc_lay):\n\n         N_filt=int(self.sinc_N_filt[i])\n         len_filt=int(self.sinc_len_filt[i])\n         \n         # dropout\n         self.drop.append(nn.Dropout(p=self.sinc_drop[i]))\n         \n         # activation\n         self.act.append(act_fun(self.sinc_act[i]))\n            \n         # layer norm initialization         \n         self.ln.append(LayerNorm([N_filt,int((current_input-self.sinc_len_filt[i]+1)/self.sinc_max_pool_len[i])]))\n\n         self.bn.append(nn.BatchNorm1d(N_filt,int((current_input-self.sinc_len_filt[i]+1)/self.sinc_max_pool_len[i]),momentum=0.05))\n\n            \n\n         if i==0:\n          self.conv.append(SincConv(1, N_filt, len_filt,sample_rate=self.sinc_sample_rate, min_low_hz=self.sinc_min_low_hz, min_band_hz=self.sinc_min_band_hz))\n              \n         else:\n          self.conv.append(nn.Conv1d(self.sinc_N_filt[i-1], self.sinc_N_filt[i], self.sinc_len_filt[i]))\n          \n         current_input=int((current_input-self.sinc_len_filt[i]+1)/self.sinc_max_pool_len[i])\n\n         \n       self.out_dim=current_input*N_filt\n\n\n\n    def forward(self, x):\n        \n       batch=x.shape[0]\n       seq_len=x.shape[1]\n       \n       if bool(self.sinc_use_laynorm_inp):\n        x=self.ln0(x)\n        \n       if bool(self.sinc_use_batchnorm_inp):\n        x=self.bn0(x)\n        \n       x=x.view(batch,1,seq_len)\n       \n       for i in range(self.N_sinc_lay):\n           \n         if self.sinc_use_laynorm[i]:\n          x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i]))))\n          \n         if self.sinc_use_batchnorm[i]:\n          x = self.drop[i](self.act[i](self.bn[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i]))))\n\n         if self.sinc_use_batchnorm[i]==False and self.sinc_use_laynorm[i]==False:\n          x = self.drop[i](self.act[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i])))\n\n       \n       x = x.view(batch,-1)\n\n       return x\n\n\n\nclass SincConv(nn.Module):\n    """"""Sinc-based convolution\n    Parameters\n    ----------\n    in_channels : `int`\n        Number of input channels. Must be 1.\n    out_channels : `int`\n        Number of filters.\n    kernel_size : `int`\n        Filter length.\n    sample_rate : `int`, optional\n        Sample rate. Defaults to 16000.\n    Usage\n    -----\n    See `torch.nn.Conv1d`\n    Reference\n    ---------\n    Mirco Ravanelli, Yoshua Bengio,\n    ""Speaker Recognition from raw waveform with SincNet"".\n    https://arxiv.org/abs/1808.00158\n    """"""\n\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride=1, padding=0, dilation=1, bias=False, groups=1,\n                 sample_rate=16000, min_low_hz=50, min_band_hz=50):\n\n        super(SincConv,self).__init__()\n\n        if in_channels != 1:\n            #msg = (f\'SincConv only support one input channel \'\n            #       f\'(here, in_channels = {in_channels:d}).\')\n            msg = ""SincConv only support one input channel (here, in_channels = {%i})"" % (in_channels)\n            raise ValueError(msg)\n\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        \n        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n        if kernel_size%2==0:\n            self.kernel_size=self.kernel_size+1\n            \n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n\n        if bias:\n            raise ValueError(\'SincConv does not support bias.\')\n        if groups > 1:\n            raise ValueError(\'SincConv does not support groups.\')\n\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # initialize filterbanks such that they are equally spaced in Mel scale\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n\n        mel = np.linspace(self.to_mel(low_hz),\n                          self.to_mel(high_hz),\n                          self.out_channels + 1)\n        hz = self.to_hz(mel) / self.sample_rate\n        \n\n        # filter lower frequency (out_channels, 1)\n        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n\n        # filter frequency band (out_channels, 1)\n        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n\n        # Hamming window\n        #self.window_ = torch.hamming_window(self.kernel_size)\n        n_lin=torch.linspace(0, self.kernel_size, steps=self.kernel_size)\n        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n\n        # (kernel_size, 1)\n        n = (self.kernel_size - 1) / 2\n        self.n_ = torch.arange(-n, n+1).view(1, -1) / self.sample_rate\n\n\n    def sinc(self, x):\n        # Numerically stable definition\n        x_left=x[:,0:int((x.shape[1]-1)/2)]\n        y_left=torch.sin(x_left) / x_left\n        y_right= torch.flip(y_left,dims=[1])\n        \n        sinc=torch.cat([y_left,torch.ones([x.shape[0],1]).to(x.device),y_right],dim=1)\n        \n\n        return sinc\n\n    def forward(self, waveforms):\n        """"""\n        Parameters\n        ----------\n        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n            Batch of waveforms.\n        Returns\n        -------\n        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n            Batch of sinc filters activations.\n        """"""\n\n        self.n_ = self.n_.to(waveforms.device)\n\n        self.window_ = self.window_.to(waveforms.device)\n\n        low = self.min_low_hz / self.sample_rate + torch.abs(self.low_hz_)\n        high = low + self.min_band_hz /self.sample_rate + torch.abs(self.band_hz_)\n\n        f_times_t = torch.matmul(low, self.n_)\n\n        low_pass1 = 2 * low * self.sinc(\n            2 * math.pi * f_times_t * self.sample_rate)\n        \n\n\n        f_times_t = torch.matmul(high, self.n_)\n        low_pass2 = 2 * high * self.sinc(\n            2 * math.pi * f_times_t * self.sample_rate)\n        \n\n        band_pass = low_pass2 - low_pass1\n        max_, _ = torch.max(band_pass, dim=1, keepdim=True)\n        band_pass = band_pass / max_\n\n        self.filters = (band_pass * self.window_).view(\n            self.out_channels, 1, self.kernel_size)\n\n        return F.conv1d(waveforms, self.filters, stride=self.stride,\n                        padding=self.padding, dilation=self.dilation,\n                         bias=None, groups=1) \n        \n        \nclass SincConv_fast(nn.Module):\n    """"""Sinc-based convolution\n    Parameters\n    ----------\n    in_channels : `int`\n        Number of input channels. Must be 1.\n    out_channels : `int`\n        Number of filters.\n    kernel_size : `int`\n        Filter length.\n    sample_rate : `int`, optional\n        Sample rate. Defaults to 16000.\n    Usage\n    -----\n    See `torch.nn.Conv1d`\n    Reference\n    ---------\n    Mirco Ravanelli, Yoshua Bengio,\n    ""Speaker Recognition from raw waveform with SincNet"".\n    https://arxiv.org/abs/1808.00158\n    """"""\n\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride=1, padding=0, dilation=1, bias=False, groups=1,\n                 sample_rate=16000, min_low_hz=50, min_band_hz=50):\n\n        super(SincConv_fast,self).__init__()\n\n        if in_channels != 1:\n            #msg = (f\'SincConv only support one input channel \'\n            #       f\'(here, in_channels = {in_channels:d}).\')\n            msg = ""SincConv only support one input channel (here, in_channels = {%i})"" % (in_channels)\n            raise ValueError(msg)\n\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        \n        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n        if kernel_size%2==0:\n            self.kernel_size=self.kernel_size+1\n            \n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n\n        if bias:\n            raise ValueError(\'SincConv does not support bias.\')\n        if groups > 1:\n            raise ValueError(\'SincConv does not support groups.\')\n\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # initialize filterbanks such that they are equally spaced in Mel scale\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n\n        mel = np.linspace(self.to_mel(low_hz),\n                          self.to_mel(high_hz),\n                          self.out_channels + 1)\n        hz = self.to_hz(mel)\n        \n\n        # filter lower frequency (out_channels, 1)\n        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n\n        # filter frequency band (out_channels, 1)\n        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n\n        # Hamming window\n        #self.window_ = torch.hamming_window(self.kernel_size)\n        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\n        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n\n\n        # (kernel_size, 1)\n        n = (self.kernel_size - 1) / 2.0\n        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\n\n \n\n\n    def forward(self, waveforms):\n        """"""\n        Parameters\n        ----------\n        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n            Batch of waveforms.\n        Returns\n        -------\n        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n            Batch of sinc filters activations.\n        """"""\n\n        self.n_ = self.n_.to(waveforms.device)\n\n        self.window_ = self.window_.to(waveforms.device)\n\n        low = self.min_low_hz  + torch.abs(self.low_hz_)\n        \n        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n        band=(high-low)[:,0]\n        \n        f_times_t_low = torch.matmul(low, self.n_)\n        f_times_t_high = torch.matmul(high, self.n_)\n\n        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations. \n        band_pass_center = 2*band.view(-1,1)\n        band_pass_right= torch.flip(band_pass_left,dims=[1])\n        \n        \n        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n\n        \n        band_pass = band_pass / (2*band[:,None])\n        \n\n        self.filters = (band_pass).view(\n            self.out_channels, 1, self.kernel_size)\n\n        return F.conv1d(waveforms, self.filters, stride=self.stride,\n                        padding=self.padding, dilation=self.dilation,\n                         bias=None, groups=1) \n\n        \ndef flip(x, dim):\n    xsize = x.size()\n    dim = x.dim() + dim if dim < 0 else dim\n    x = x.contiguous()\n    x = x.view(-1, *xsize[dim:])\n    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, -1, -1), (\'cpu\',\'cuda\')[x.is_cuda])().long(), :]\n    return x.view(xsize)\n\nclass SRU(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(SRU, self).__init__()\n        self.input_dim = inp_dim\n        self.hidden_size = int(options[\'sru_hidden_size\'])\n        self.num_layers = int(options[\'sru_num_layers\'])\n        self.dropout = float(options[\'sru_dropout\'])\n        self.rnn_dropout = float(options[\'sru_rnn_dropout\'])\n        self.use_tanh = bool(strtobool(options[\'sru_use_tanh\']))\n        self.use_relu = bool(strtobool(options[\'sru_use_relu\']))\n        self.use_selu = bool(strtobool(options[\'sru_use_selu\']))\n        self.weight_norm = bool(strtobool(options[\'sru_weight_norm\']))\n        self.layer_norm = bool(strtobool(options[\'sru_layer_norm\']))\n        self.bidirectional = bool(strtobool(options[\'sru_bidirectional\']))\n        self.is_input_normalized = bool(strtobool(options[\'sru_is_input_normalized\']))\n        self.has_skip_term = bool(strtobool(options[\'sru_has_skip_term\']))\n        self.rescale = bool(strtobool(options[\'sru_rescale\']))\n        self.highway_bias = float(options[\'sru_highway_bias\'])\n        self.n_proj = int(options[\'sru_n_proj\'])\n        self.sru = sru.SRU(self.input_dim, self.hidden_size,\n                            num_layers=self.num_layers,\n                            dropout=self.dropout,\n                            rnn_dropout=self.rnn_dropout,\n                            bidirectional=self.bidirectional,\n                            n_proj=self.n_proj,\n                            use_tanh=self.use_tanh,\n                            use_selu=self.use_selu,\n                            use_relu=self.use_relu,\n                            weight_norm=self.weight_norm,\n                            layer_norm=self.layer_norm,\n                            has_skip_term=self.has_skip_term,\n                            is_input_normalized=self.is_input_normalized,\n                            highway_bias=self.highway_bias,\n                            rescale=self.rescale)\n        self.out_dim = self.hidden_size+self.bidirectional*self.hidden_size\n\n    def forward(self, x):\n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size*2)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n        if x.is_cuda:\n            h0 = h0.cuda()\n        output, hn = self.sru(x, c0=h0)\n        return output\n\n'"
spk_id/nnet.py,14,"b""import numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport pickle\nimport json\nimport glob\nfrom tensorboardX import SummaryWriter\nimport random\nimport timeit\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom ahoproc_tools.io import read_aco_file\nfrom pase.models.frontend import wf_builder\nfrom pase.models.modules import Model\nimport librosa\nfrom random import shuffle\nimport argparse\nfrom utils import *\nimport os\n\n\n# Make Linear classifier model\nclass LinearClassifier(Model):\n    \n    def __init__(self, frontend,\n                 num_spks=None, \n                 ft_fe=False,\n                 z_bnorm=False,\n                 name='CLS'):\n        super().__init__(name=name)\n        self.frontend = frontend\n        self.ft_fe = ft_fe\n        if z_bnorm:\n            # apply z-norm to the input\n            self.z_bnorm = nn.BatchNorm1d(frontend.emb_dim, affine=False)\n        if num_spks is None:\n            raise ValueError('Please specify a number of spks.')\n        self.fc = nn.Conv1d(frontend.emb_dim, num_spks, 1)\n        self.act = nn.LogSoftmax(dim=1)\n    \n    def forward(self, x):\n        h = self.frontend(x)\n        if not self.ft_fe:\n            h = h.detach()\n        if hasattr(self, 'z_bnorm'):\n            h = self.z_bnorm(h)\n        h = self.fc(h)\n        y = self.act(h)\n        return y\n\nclass LibriSpkIDDataset(Dataset):\n    \n    def __init__(self, data_root, files_list, spk2idx):\n        super().__init__()\n        self.files_list = files_list\n        self.data_root = data_root\n        self.spk2idx = spk2idx\n    \n    def __getitem__(self, idx):\n        fpath = os.path.join(self.data_root, self.files_list[idx])\n        wav, sr = librosa.load(fpath, sr=None)\n        lab = self.spk2idx[self.files_list[idx]]\n        return wav, lab\n\n    def __len__(self):\n        return len(self.files_list)\n    \n    \nclass WavCollater(object):\n    \n    def __init__(self, max_len=None):\n        self.max_len = max_len\n        \n    def __call__(self, batch):\n        if self.max_len is None:\n            # track max seq len in batch\n            # and apply it padding others seqs\n            max_len = 0\n            for sample in batch:\n                wav, lab = sample\n                clen = len(wav)\n                if clen > max_len:\n                    max_len = clen\n        else:\n            max_len = self.max_len\n        X = []\n        Y = []\n        slens = []\n        for sample in batch:\n            wav, lab = sample\n            clen = len(wav)\n            if clen < max_len:\n                # pad with zeros in the end\n                P = max_len - clen\n                pad = np.zeros((P,))\n                wav = np.concatenate((wav, pad), axis=0)\n            elif clen > max_len:\n                # trim the end (applied if we specify max_len externally)\n                idxs = list(range(clen - max_len))\n                bidx = random.choice(idxs)\n                wav = wav[bidx:bidx + max_len]\n            X.append(wav)\n            Y.append(lab)\n            slens.append(clen)\n        X = torch.FloatTensor(X)\n        Y = torch.LongTensor(Y)\n        slens = torch.LongTensor(slens)\n        return X, Y, slens\n\n\nclass MLPClassifier(Model):\n\n    def __init__(self, frontend,\n                 num_spks=None,\n                 ft_fe=False,\n                 hidden_size=2048,\n                 hidden_layers=1,\n                 z_bnorm=False,\n                 name='MLP'):\n        # 2048 default size raises 5.6M params\n        super().__init__(name=name, max_ckpts=1000)\n        self.frontend = frontend\n        self.ft_fe = ft_fe\n        if ft_fe:\n            print('Training the front-end')\n        if z_bnorm:\n            # apply z-norm to the input\n            self.z_bnorm = nn.BatchNorm1d(frontend.emb_dim, affine=False)\n        if num_spks is None:\n            raise ValueError('Please specify a number of spks.')\n        layers = [nn.Conv1d(frontend.emb_dim, hidden_size, 1),\n                  nn.LeakyReLU(),\n                  nn.BatchNorm1d(hidden_size)]\n        for n in range(1, hidden_layers):\n            layers += [nn.Conv1d(hidden_size, hidden_size, 1),\n                       nn.LeakyReLU(),\n                       nn.BatchNorm1d(hidden_size)]\n        layers += [nn.Conv1d(hidden_size, num_spks, 1),\n                   nn.LogSoftmax(dim=1)]\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        h = self.frontend(x)\n        if not self.ft_fe:\n            h = h.detach()\n        if hasattr(self, 'z_bnorm'):\n            h = self.z_bnorm(h)\n        return self.model(h)\n\nclass RNNClassifier(Model):\n\n    def __init__(self, frontend,\n                 num_spks=None,\n                 ft_fe=False,\n                 hidden_size=1300,\n                 z_bnorm=False,\n                 uni=False,\n                 return_sequence=False,\n                 name='RNN'):\n        # 1300 default size raises 5.25M params\n        super().__init__(name=name, max_ckpts=1000)\n        self.frontend = frontend\n        self.ft_fe = ft_fe\n        if ft_fe:\n            print('Training the front-end')\n        if z_bnorm:\n            # apply z-norm to the input\n            self.z_bnorm = nn.BatchNorm1d(frontend.emb_dim, affine=False)\n        if num_spks is None:\n            raise ValueError('Please specify a number of spks.')\n        if uni:\n            hsize = hidden_size\n        else:\n            hsize = hidden_size // 2\n        self.rnn = nn.GRU(frontend.emb_dim, hsize,\n                          bidirectional=not uni,\n                          batch_first=True)\n        self.model = nn.Sequential(\n            nn.Conv1d(hidden_size, num_spks, 1),\n            nn.LogSoftmax(dim=1)\n        )\n        self.return_sequence = return_sequence\n        self.uni = uni\n\n    def forward(self, x):\n        h = self.frontend(x)\n        if not self.ft_fe:\n            h = h.detach()\n        if hasattr(self, 'z_bnorm'):\n            h = self.z_bnorm(h)\n        ht, state = self.rnn(h.transpose(1, 2))\n        if self.return_sequence:\n            ht = ht.transpose(1, 2)\n        else:\n            if not self.uni:\n                # pick last time-step for each dir\n                # first chunk feat dim\n                bsz, slen, feats = ht.size()\n                ht = torch.chunk(ht.view(bsz, slen, 2, feats // 2), 2, dim=2)\n                # now select fwd\n                ht_fwd = ht[0][:, -1, 0, :].unsqueeze(2)\n                ht_bwd = ht[1][:, 0, 0, :].unsqueeze(2)\n                ht = torch.cat((ht_fwd, ht_bwd), dim=1)\n            else:\n                # just last time-step works\n                ht = ht[:, -1, :].unsqueeze(2)\n        y = self.model(ht)\n        return y\n\ndef select_model(opts, fe, num_spks):\n    if opts.model == 'cls':\n        model = LinearClassifier(fe, num_spks=num_spks, ft_fe=opts.ft_fe,\n                                 z_bnorm=opts.z_bnorm)\n    elif opts.model == 'mlp':\n        model = MLPClassifier(fe, num_spks=num_spks,\n                              hidden_size=opts.hidden_size,\n                              hidden_layers=opts.hidden_layers,\n                              ft_fe=opts.ft_fe,\n                              z_bnorm=opts.z_bnorm)\n    elif opts.model == 'rnn':\n        model = RNNClassifier(fe, num_spks=num_spks,\n                              hidden_size=opts.hidden_size,\n                              ft_fe=opts.ft_fe,\n                              z_bnorm=opts.z_bnorm,\n                              uni=opts.uni,\n                              return_sequence=opts.return_sequence)\n    else:\n        raise TypeError('Unrecognized model {}'.format(opts.model))\n    return model\n\n\ndef main(opts):\n    CUDA = torch.cuda.is_available() and not opts.no_cuda\n    device = 'cuda' if CUDA else 'cpu'\n    torch.manual_seed(opts.seed)\n    random.seed(opts.seed)\n    np.random.seed(opts.seed)\n    if device == 'cuda':\n        torch.cuda.manual_seed_all(opts.seed)\n    spk2idx = load_spk2idx(opts.spk2idx)\n    NSPK=len(set(spk2idx.values()))\n    # Build Model\n    fe = wf_builder(opts.fe_cfg)\n    if opts.train:\n        print('=' * 20)\n        print('Entering TRAIN mode')\n        print('=' * 20)\n        with open(os.path.join(opts.save_path, 'train.opts'), 'w') as cfg_f:\n            cfg_f.write(json.dumps(vars(opts), indent=2))\n        # Open up guia and split valid\n        with open(opts.train_guia) as tr_guia_f: \n            tr_files = [l.rstrip() for l in tr_guia_f]\n        \n        if opts.test_guia is not None:\n            with open(opts.test_guia) as te_guia_f: \n                te_files = [l.rstrip() for l in te_guia_f]\n\n        tr_files_, va_files = build_valid_list(tr_files, spk2idx,\n                                               va_split=opts.va_split)\n        # compute total samples dur\n        beg_t = timeit.default_timer()\n        tr_durs, sr = compute_utterances_durs(tr_files_, opts.data_root)\n        train_dur = np.sum(tr_durs)\n        end_t = timeit.default_timer()\n        if len(va_files) > 0:\n            va_durs, _ = compute_utterances_durs(va_files, opts.data_root)\n            valid_dur = np.sum(va_durs)\n            print('Read tr/va {:.1f} s/{:.1f} s in {} s'.format(train_dur / sr,\n                                                                valid_dur / sr,\n                                                                end_t - beg_t))\n        else:\n            print('Read tr {:.1f} s in {} s'.format(train_dur / sr,\n                                                    end_t - beg_t))\n            opts.sched_mode='step'\n        # Build Datasets\n        dset = LibriSpkIDDataset(opts.data_root,\n                                 tr_files_, spk2idx)\n        if len(va_files) > 0:\n            va_dset = LibriSpkIDDataset(opts.data_root,\n                                        va_files, spk2idx)\n        cc = WavCollater(max_len=opts.max_len)\n        #cc_vate = WavCollater(max_len=None)\n        cc_vate = cc\n        dloader = DataLoader(dset, batch_size=opts.batch_size, collate_fn=cc,\n                             shuffle=True, num_workers=opts.num_workers)\n        if len(va_files) > 0:\n            va_dloader = DataLoader(va_dset, batch_size=opts.batch_size,\n                                    collate_fn=cc_vate,\n                                    shuffle=False)\n            va_bpe = (valid_dur // opts.max_len) // opts.batch_size\n        tr_bpe = (train_dur // opts.max_len) // opts.batch_size\n        if opts.test_guia is not None:\n            te_dset = LibriSpkIDDataset(opts.data_root,\n                                        te_files, spk2idx)\n            te_dloader = DataLoader(te_dset, batch_size=opts.batch_size,\n                                    collate_fn=cc_vate,\n                                    shuffle=False)\n        if opts.fe_ckpt is not None:\n            fe.load_pretrained(opts.fe_ckpt, load_last=True, verbose=True)\n        else:\n            print('*' * 50)\n            print('** WARNING: TRAINING WITHOUT PRETRAIED WEIGHTS FOR THE '\n                  'FRONT-END **')\n            print('*' * 50)\n            # Enforce training the frontend\n            opts.ft_fe = True\n        model = select_model(opts, fe, NSPK)\n        model.to(device)\n        print(model)\n        # Build optimizer and scheduler\n        opt = select_optimizer(opts, model)\n        sched = select_scheduler(opts, opt)\n        # Make writer\n        writer = SummaryWriter(opts.save_path)\n        best_val_acc = 0\n        # flag for saver\n        best_val = False\n        for epoch in range(1, opts.epoch + 1):\n            train_epoch(dloader, model, opt, epoch, opts.log_freq, writer=writer,\n                        device=device, bpe=tr_bpe)\n            if len(va_files) > 0:\n                eloss, eacc = eval_epoch(va_dloader, model, epoch, opts.log_freq,\n                                         writer=writer, device=device, bpe=va_bpe,\n                                         key='valid')\n            if opts.sched_mode == 'step' or len(va_files) == 0:\n                sched.step()\n            else:\n                sched.step(eacc)\n            if len(va_files) > 0:\n                if eacc > best_val_acc:\n                    print('*' * 40)\n                    print('New best val acc: {:.3f} => {:.3f}.'\n                          ''.format(best_val_acc, eacc))\n                    print('*' * 40)\n                    best_val_acc = eacc\n                    best_val = True\n            model.save(opts.save_path, epoch - 1, best_val=best_val)\n            best_val = False\n            if opts.test_guia is not None:\n                # Eval test on the fly whilst training/validating\n                teloss, teacc = eval_epoch(te_dloader, model, epoch, opts.log_freq,\n                                           writer=writer, device=device, key='test')\n    if opts.test:\n        print('=' * 20)\n        print('Entering TEST mode')\n        print('=' * 20)\n\n        #fe = WaveFe(rnn_pool=opts.rnn_pool, emb_dim=opts.emb_dim)\n        model = select_model(opts, fe, NSPK)\n        model.load_pretrained(opts.test_ckpt, load_last=True, verbose=True)\n        model.to(device)\n        model.eval()\n        with open(opts.test_guia) as te_guia_f: \n            te_files = [l.rstrip() for l in te_guia_f]\n            te_dset = LibriSpkIDDataset(opts.data_root,\n                                        te_files, spk2idx)\n            cc = WavCollater(max_len=None)\n            te_dloader = DataLoader(te_dset, batch_size=1,\n                                    #collate_fn=cc,\n                                    shuffle=False)\n            def filter_by_slens(T, slens, sfactor=160):\n                dims = len(T.size())\n                # extract each sequence by its length\n                seqs =[]\n                for bi in range(T.size(0)):\n                    slen = int(np.ceil(slens[bi] / sfactor ))\n                    if dims == 3:\n                        seqs.append(T[bi, :, :slen])\n                    else: \n                        seqs.append(T[bi, :slen])\n                return seqs\n            with torch.no_grad():\n                teloss = []\n                teacc = []\n                timings = []\n                beg_t = timeit.default_timer()\n                if opts.test_log_file is not None:\n                    test_log_f = open(opts.test_log_file, 'w')\n                    test_log_f.write('Filename\\tAccuracy [%]\\tError [%]\\n')\n                else:\n                    test_log_f = None\n                for bidx, batch in enumerate(te_dloader, start=1):\n                    #X, Y, slen = batch\n                    X, Y = batch\n                    X = X.unsqueeze(1)\n                    X = X.to(device)\n                    Y = Y.to(device)\n                    Y_ = model(X)\n                    Y = Y.view(-1, 1).repeat(1, Y_.size(2))\n                    #Y__seqs = filter_by_slens(Y_, slen)\n                    #Y_seqs = filter_by_slens(Y, slen)\n                    #assert len(Y__seqs) == len(Y_seqs)\n                    #for sidx in range(len(Y__seqs)):\n                    #    y_ = Y__seqs[sidx].unsqueeze(0)\n                    #    y = Y_seqs[sidx].unsqueeze(0)\n                    #    loss = F.nll_loss(y_, y)\n                    #    teacc.append(accuracy(y_, y))\n                    #    teloss.append(loss)\n                    loss = F.nll_loss(Y_, Y)\n                    acc = accuracy(Y_, Y)\n                    if test_log_f:\n                        test_log_f.write('{}\\t{:.2f}\\t{:.2f}\\n' \\\n                                         ''.format(te_files[bidx - 1],\n                                                   acc * 100,\n                                                   100 - (acc * 100)))\n                    teacc.append(accuracy(Y_, Y).item())\n                    teloss.append(loss.item())\n                    end_t = timeit.default_timer()\n                    timings.append(end_t - beg_t)\n                    beg_t = timeit.default_timer()\n                    if bidx % 100 == 0 or bidx == 1:\n                        mteloss = np.mean(teloss)\n                        mteacc = np.mean(teacc)\n                        mtimings = np.mean(timings)\n                    print('Processed test file {}/{} mfiletime: {:.2f} s, '\n                          'macc: {:.4f}, mloss: {:.2f}'\n                          ''.format(bidx, len(te_dloader), mtimings,\n                                    mteacc, mteloss),\n                          end='\\r')\n                print() \n                if test_log_f:\n                    test_log_f.write('-' * 30 + '\\n')\n                    test_log_f.write('Test accuracy: ' \\\n                                     '{:.2f}\\n'.format(np.mean(teacc) * 100))\n                    test_log_f.write('Test error: ' \\\n                                     '{:.2f}\\n'.format(100 - (np.mean(teacc) *100)))\n                    test_log_f.write('Test loss: ' \\\n                                     '{:.2f}\\n'.format(np.mean(teloss)))\n                    test_log_f.close()\n                print('Test accuracy: {:.4f}'.format(np.mean(teacc)))\n                print('Test loss: {:.2f}'.format(np.mean(teloss)))\n\n\n\ndef train_epoch(dloader_, model, opt, epoch, log_freq=1, writer=None,\n                device='cpu', bpe=None):\n    model.train()\n    if not model.ft_fe:\n        model.frontend.eval()\n    if bpe is None:\n        # default is just dataloader length\n        bpe = len(dloader_)\n    global_idx = (epoch - 1) * bpe\n    timings = []\n    beg_t = timeit.default_timer()\n    #for bidx, batch in enumerate(dloader_, start=1):\n    iterator = iter(dloader_)\n    for bidx in range(1, bpe + 1):\n        try:\n            batch = next(iterator)\n        except StopIteration:\n            iterator = iter(dloader_)\n            batch = next(iterator)\n        opt.zero_grad()\n        X, Y, slens = batch\n        #X = X.transpose(1, 2)\n        X = X.unsqueeze(1)\n        X = X.to(device)\n        Y = Y.to(device)\n        Y_ = model(X)\n        Y = Y.view(-1, 1).repeat(1, Y_.size(2))\n        loss = F.nll_loss(Y_.squeeze(-1), Y.squeeze(-1))\n        loss.backward()\n        opt.step()\n        end_t = timeit.default_timer()\n        timings.append(end_t - beg_t)\n        beg_t = timeit.default_timer()\n        if bidx % log_freq == 0 or bidx >= bpe:\n            acc = accuracy(Y_, Y)\n            log_str = 'Batch {:5d}/{:5d} (Epoch {:3d}, Gidx {:5d})' \\\n                      ' '.format(bidx, bpe,\n                                 epoch, global_idx)\n            log_str += 'loss: {:.3f} '.format(loss.item())\n            log_str += 'bacc: {:.2f} '.format(acc)\n            log_str += 'mbtime: {:.3f} s'.format(np.mean(timings))\n            print(log_str)\n            if writer is not None:\n                writer.add_scalar('train/loss', loss.item(),\n                                  global_idx)\n                writer.add_scalar('train/bacc', acc, global_idx)\n        global_idx += 1\n\ndef eval_epoch(dloader_, model, epoch, log_freq=1, writer=None, device='cpu',\n               bpe=None, key='eval'):\n    model.eval()\n    with torch.no_grad():\n        if bpe is None:\n            # default is just dataloader length\n            bpe = len(dloader_)\n        eval_losses = []\n        eval_accs = []\n        timings = []\n        beg_t = timeit.default_timer()\n        #for bidx, batch in enumerate(dloader_, start=1):\n        iterator = iter(dloader_)\n        for bidx in range(1, bpe + 1):\n            try:\n                batch = next(iterator)\n            except StopIteration:\n                iterator = iter(dloader_)\n                batch = next(iterator)\n            X, Y, slens = batch\n            #X = X.transpose(1, 2)\n            X = X.unsqueeze(1)\n            X = X.to(device)\n            Y = Y.to(device)\n            Y_ = model(X)\n            Y = Y.view(-1, 1).repeat(1, Y_.size(2))\n            loss = F.nll_loss(Y_.squeeze(-1), Y.squeeze(-1))\n            #loss = F.nll_loss(Y_, Y)\n            eval_losses.append(loss.item())\n            acc = accuracy(Y_, Y)\n            eval_accs.append(acc)\n            end_t = timeit.default_timer()\n            timings.append(end_t - beg_t)\n            beg_t = timeit.default_timer()\n            if bidx % log_freq == 0 or bidx >= bpe:\n                \n                log_str = 'EVAL::{} Batch {:4d}/{:4d} (Epoch {:3d})' \\\n                          ' '.format(key, bidx, bpe,\n                                     epoch)\n                log_str += 'loss: {:.3f} '.format(loss.item())\n                log_str += 'bacc: {:.2f} '.format(acc)\n                log_str += 'mbtime: {:.3f} s'.format(np.mean(timings))\n                print(log_str)\n        mloss = np.mean(eval_losses)\n        macc = np.mean(eval_accs)\n        if writer is not None:\n            writer.add_scalar('{}/loss'.format(key), mloss,\n                              epoch)\n            writer.add_scalar('{}/acc'.format(key), macc, epoch)\n        print('EVAL epoch {:3d} mean loss: {:.3f}, mean acc: {:.2f} '\n             ''.format(epoch, mloss, macc))\n        return mloss, macc\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--fe_cfg', type=str,\n                        default=None)\n    parser.add_argument('--save_path', type=str, default='ckpt_nnet')\n    parser.add_argument('--data_root', type=str, default=None)\n    parser.add_argument('--num_workers', type=int, default=2)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--train_guia', type=str, default=None)\n    parser.add_argument('--test_guia', type=str, default=None)\n    parser.add_argument('--spk2idx', type=str, default=None)\n    parser.add_argument('--log_freq', type=int, default=50)\n    parser.add_argument('--epoch', type=int, default=150)\n    parser.add_argument('--patience', type=int, default=10)\n    parser.add_argument('--seed', type=int, default=1234)\n    parser.add_argument('--no-cuda', action='store_true', default=False)\n    parser.add_argument('--no-rnn', action='store_true', default=False)\n    parser.add_argument('--ft_fe', action='store_true', default=False)\n    parser.add_argument('--z_bnorm', action='store_true', default=False,\n                        help='Use z-norm in z, before any model (Default: '\n                             'False).')\n    parser.add_argument('--va_split', type=float, default=0.2)\n    parser.add_argument('--lr', type=float, default=0.001)\n    parser.add_argument('--momentum', type=float, default=0.9)\n    parser.add_argument('--max_len', type=int, default=16000)\n    parser.add_argument('--hidden_size', type=int, default=2048)\n    parser.add_argument('--hidden_layers', type=int, default=1)\n    parser.add_argument('--emb_dim', type=int, default=100)\n    parser.add_argument('--stats', type=str, default=None)\n    parser.add_argument('--opt', type=str, default='adam')\n    parser.add_argument('--sched_mode', type=str, default='plateau',\n                        help='(1) plateau (validation), (2) '\n                             'step (step decay) (Def: plateau).')\n    parser.add_argument('--sched_step_size', type=int,\n                        default=30, help='Number of epochs to apply '\n                                          'a learning rate decay (Def: 30).')\n    parser.add_argument('--lrdec', type=float, default=0.5,\n                        help='Decay factor of learning rate after '\n                             'patience epochs of valid accuracy not '\n                             'improving (Def: 0.5).')\n    parser.add_argument('--test_ckpt', type=str, default=None)\n    parser.add_argument('--fe_ckpt', type=str, default=None)\n    parser.add_argument('--plateau_mode', type=str, default='max',\n                        help='LR Plateau scheduling mode; (1) max, (2) min '\n                             '(Def: max).')\n    parser.add_argument('--model', type=str, default='mlp',\n                        help='(1) cls, (2) mlp (Def: cls).')\n    parser.add_argument('--train', action='store_true', default=False)\n    parser.add_argument('--test', action='store_true', default=False)\n    parser.add_argument('--test_log_file', type=str, default=None,\n                        help='Possible test log file (Def: None).')\n    parser.add_argument('--inorm_code', action='store_true', default=False)\n    parser.add_argument('--uni', action='store_true', default=False,\n                        help='Make RNN model unidirectional (Def: False).')\n    parser.add_argument('--return_sequence', action='store_true', default=False,\n                        help='Return sequence of hidden vectors for RNN (Def: False).')\n    \n    opts = parser.parse_args()\n    \n\n    opts.rnn_pool = not opts.no_rnn\n\n    if not os.path.exists(opts.save_path):\n        os.makedirs(opts.save_path)\n\n    main(opts)\n"""
spk_id/run_minivox_fast.py,19,"b'# Mirco Ravanelli\n# Mila, June 2019\n\n# This script runs a simple speaker recognition experiment on the top of PASE features. \n# The results are reported in terms of Frame Error Rate /Sentence Error Rates.\n# This system is not designed for an extensive evaluation of PASE features, but mainly for quickly monitoring the performance of PASE during the self-supervised training phase.\n# The results are printed in standard output and within the text file specified in the last argument.\n\n# To run the speaker recognition exp on minivox celeb:\n# python run_minivox_fast.py ../cfg/PASE.cfg ../PASE.ckpt /scratch/ravanelm/datasets/minivoxceleb_40spk/ minivoxceleb_40spk/minivox_tr_list.txt minivoxceleb_40spk/minivox_test_list.txt  minivoxceleb_40spk/utt2spk.npy  minivox_fina.res\n\n# To run the language id experiment on minivoxforge:\n# python run_minivox_fast.py ../cfg/PASE.cfg ../PASE.ckpt /scratch/ravanelm/datasets/mini_voxforge/ minivoxforge_tr_list.txt minivoxforge_test_list.txt  utt2lang.npy  minivoxforge.res\n\nimport warnings\nwarnings.filterwarnings(\'ignore\')\n\nimport sys\nimport os\nimport json\nfrom neural_networks import MLP,context_window\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nfrom pase.models.frontend import wf_builder\n# from waveminionet.models.frontend import wf_builder #old models\nimport soundfile as sf\nfrom pase.models.WorkerScheduler.encoder import *\n\n\ndef get_freer_gpu(trials=10):\n\tfor j in range(trials):\n\t\tos.system(\'nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp\')\n\t\tmemory_available = [int(x.split()[2]) for x in open(\'tmp\', \'r\').readlines()]\n\t\tdev_ = torch.device(\'cuda:\'+str(np.argmax(memory_available)))\n\t\ttry:\n\t\t\ta = torch.rand(1).cuda(dev_)\n\t\t\treturn dev_\n\t\texcept:\n\t\t\tpass\n\n\tprint(\'NO GPU AVAILABLE!!!\')\n\texit(1)\n\ndef get_nspk(utt2spk):\n    lab_list = []\n    for utt, spk in utt2spk.items():\n        lab_list.append(int(spk))\n\n    return max(lab_list)+1\n\n\npase_cfg=sys.argv[1] # e.g, \'../cfg/PASE.cfg\'\npase_model=sys.argv[2] # e.g, \'../PASE.ckpt\'\ndata_folder=sys.argv[3] # eg. \'/home/mirco/Dataset/mini_voxceleb minivox\'\ntr_lst_file=sys.argv[4] # e.g., \'minivox_tr_list.txt\'\ndev_lst_file=sys.argv[5] # e.g., \'minvox_test_list.txt\'\nlab_file=sys.argv[6] # e.g., \'minvox_test_list.txt\'\noutput_file=sys.argv[7] # e.g., \n\n\nlab=np.load(lab_file, allow_pickle=True).item()\n\n# get number of speakers\n\nnspk=get_nspk(lab)\n\n\ntr_lst = [line.rstrip(\'\\n\') for line in open(tr_lst_file)]\ndev_lst = [line.rstrip(\'\\n\') for line in open(dev_lst_file)]\n\n# Training parameters\nN_epochs=24\nseed=1234\nbatch_size=128\nhalving_factor=0.5\nlr=0.001\nleft=0\nright=0\n\n# Neural network parameters\noptions={}\noptions[\'dnn_lay\']=\'256,\'+str(nspk)\noptions[\'dnn_drop\']=\'0.15,0.0\'\noptions[\'dnn_use_batchnorm\']=\'False,False\'\noptions[\'dnn_use_laynorm\']=\'True,False\'\noptions[\'dnn_use_laynorm_inp\']=\'True\'\noptions[\'dnn_use_batchnorm_inp\']=\'False\'\noptions[\'dnn_act\']=\'relu,softmax\'\n\ndevice=0 #get_freer_gpu()\n\n\n# output file creation\ntext_file=open(output_file, ""w"")\n\n# Loading pase\npase = wf_builder(pase_cfg)\npase.load_pretrained(pase_model, load_last=True, verbose=False)\npase.to(device)\npase.eval()\n\n# reading the training signals\nprint(""Waveform reading..."")\nfea={}\nfor wav_file in tr_lst:\n    [signal, fs] = sf.read(data_folder+\'/train/\'+wav_file)\n    signal=signal/np.max(np.abs(signal))\n    signal = signal.astype(np.float32)\n    \n    fea_id=wav_file.split(\'/\')[-1]\n    fea[fea_id]=torch.from_numpy(signal).float().to(device).view(1,1,-1)\n\n\n# reading the dev signals\nfea_dev={}\nfor wav_file in dev_lst:\n    [signal, fs] = sf.read(data_folder+\'/test/\'+wav_file)\n    signal=signal/np.max(np.abs(signal))\n    fea_id=wav_file.split(\'/\')[-1]\n    fea_dev[fea_id]=torch.from_numpy(signal).float().to(device).view(1,1,-1)\n\n\n\n# Computing pase features for training\nprint(\'Computing PASE features...\')\nfea_pase={}\nfor snt_id in fea.keys():\n    pase.eval()\n    fea_pase[snt_id] = pase(fea[snt_id], device, mode=\'avg_concat\').to(\'cpu\').detach()\n    fea_pase[snt_id]=fea_pase[snt_id].view(fea_pase[snt_id].shape[1],fea_pase[snt_id].shape[2]).transpose(0,1)\n\ninp_dim=fea_pase[snt_id].shape[1]*(left+right+1)\n\n# Computing pase features for test\nfea_pase_dev={}\nfor snt_id in fea_dev.keys():\n    fea_pase_dev[snt_id] = pase(fea_dev[snt_id], device,mode=\'avg_concat\').detach()\n    # fea_pase_dev[snt_id]=pase(fea_dev[snt_id]).detach()\n    fea_pase_dev[snt_id]=fea_pase_dev[snt_id].view(fea_pase_dev[snt_id].shape[1],fea_pase_dev[snt_id].shape[2]).transpose(0,1)\n\n\n  \n\n# Network initialization\nnnet=MLP(options,inp_dim)\n\nnnet.to(device)\n\ncost=nn.NLLLoss()\n\n# Optimizer initialization\noptimizer = optim.SGD(nnet.parameters(), lr=lr, momentum=0.0)\n\n# Seeds initialization\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\n# Batch creation (train)\nfea_lst=[]\nlab_lst=[]\n\nprint(""Data Preparation..."")\nfor snt in fea_pase.keys():\n        fea_lst.append(fea_pase[snt])\n        lab_lst.append(np.zeros(fea_pase[snt].shape[0])+lab[snt])\n\n    \n# feature matrix (training)\nfea_conc=np.concatenate(fea_lst)\nfea_conc=context_window(fea_conc,left,right)\n\n# feature normalization\nmean=np.mean(fea_conc,axis=0)\nstd=np.std(fea_conc,axis=0)\n\n# normalization\n\nfea_conc=(fea_conc-mean)/std\n\nmean=torch.from_numpy(mean).float().to(device)\nstd=torch.from_numpy(std).float().to(device)\n\n# lab matrix\nlab_conc=np.concatenate(lab_lst)\n\nif right>0:\n    lab_conc=lab_conc[left:-right]\nelse:\n    lab_conc=lab_conc[left:]\n\n\n# dataset composition\ndataset=np.concatenate([fea_conc,lab_conc.reshape(-1,1)],axis=1)\n\n# shuffling\nnp.random.shuffle(dataset)\n\ndataset=torch.from_numpy(dataset).float().to(device)\n\n# computing N_batches\nN_ex_tr=dataset.shape[0]\nN_batches=int(N_ex_tr/batch_size)\n\n\nerr_dev_fr_history=[]\nerr_dev_snt_history=[]\n\n# Training loop\nprint(""Training..."")\nfor ep in range(N_epochs):\n    err_batches=0\n    loss_batches=0\n    \n    beg_batch=0\n    \n    # training modality\n    nnet.train()\n    \n    # random shuffling\n    shuffle_index=torch.randperm(dataset.shape[0])\n    dataset=dataset[shuffle_index]\n    \n    for batch_id in range(N_batches):\n        \n        # Batch selection\n        end_batch=beg_batch+batch_size\n        batch=dataset[beg_batch:end_batch]\n        \n        fea_batch=batch[:,:-1]\n        lab_batch=batch[:,-1].long()\n        \n        # computing the output probabilities\n        out=nnet(fea_batch)\n           \n        # computing the loss\n        loss=cost(out,lab_batch)\n        \n        # computing the error\n        pred=torch.max(out,dim=1)[1] \n        err = torch.mean((pred!=lab_batch).float())\n        \n        # loss/error accumulation        \n        err_batches=err_batches+err.detach()\n        loss_batches=loss_batches+loss.detach()\n    \n        \n        optimizer.zero_grad()\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        beg_batch=end_batch\n        \n\n        \n    # evaluation\n    nnet.eval()\n    \n    \n    with torch.no_grad():\n    \n        err_dev_fr_mean=0\n        err_dev_snt_mean=0\n        loss_dev_mean=0\n        \n        N_dev_snt=len(list(fea_pase_dev.keys()))\n        \n        for dev_snt in fea_pase_dev.keys():\n\n             fea_dev_norm=(fea_pase_dev[dev_snt]-mean)/std\n             out_dev=nnet(fea_dev_norm)\n             lab_snt=torch.zeros(fea_pase_dev[dev_snt].shape[0])+lab[dev_snt]\n             lab_snt=lab_snt.long().to(device)\n             loss_dev=cost(out_dev,lab_snt)\n             \n             # frame level error\n             pred_dev=torch.max(out_dev,dim=1)[1] \n             err_dev = torch.mean((pred_dev!=lab_snt).float())\n             \n             # sentence error level\n             prob_sum=torch.sum(out_dev,dim=0)\n             pred_dev_snt=torch.argmax(prob_sum) \n             err_snt=(pred_dev_snt!=lab_snt[0]).float()\n             \n             err_dev_fr_mean=err_dev_fr_mean+err_dev.detach()\n             loss_dev_mean=loss_dev_mean+loss_dev.detach()\n             err_dev_snt_mean=err_dev_snt_mean+err_snt.detach()\n         \n         \n    err_dev_fr_history.append(err_dev_fr_mean/N_dev_snt)\n    err_dev_snt_history.append(err_dev_snt_mean/N_dev_snt)\n    \n    \n    print(""epoch=%i loss_tr=%f err_tr=%f loss_te=%f err_te_fr=%f err_te_snt=%f lr=%f"" %(ep,loss_batches/N_batches,err_batches/N_batches,loss_dev_mean/N_dev_snt,err_dev_fr_mean/N_dev_snt,err_dev_snt_mean/N_dev_snt,lr))\n    text_file.write(""epoch=%i loss_tr=%f err_tr=%f loss_te=%f err_te_fr=%f err_te_snt=%f lr=%f \\n"" %(ep,loss_batches/N_batches,err_batches/N_batches,loss_dev_mean/N_dev_snt,err_dev_fr_mean/N_dev_snt,err_dev_snt_mean/N_dev_snt,lr))\n    \n    # learning rate annealing\n    if ep>0:\n        if (err_dev_fr_history[-2]-err_dev_fr_history[-1])/err_dev_fr_history[-2]<0.0025:\n            lr=lr*halving_factor\n            optimizer.param_groups[0][\'lr\']=lr\n\n\nprint(\'BEST ERR=%f\' %(min(err_dev_fr_history)))\nprint(\'BEST ACC=%f\' %(1-min(err_dev_fr_history)))\ntext_file.write(\'BEST_ERR=%f\\n\' %(min(err_dev_fr_history)))\ntext_file.write(\'BEST_ACC=%f\\n\' %(1-min(err_dev_fr_history)))\ntext_file.close()\n    \n    \n    \n    \n\n\n\n'"
spk_id/select_supervised_ckpt.py,0,"b""import os\nimport sys\n\n#weights_MLP-MLP-11.ckpt\n#weights_MLP-best_MLP-11.ckpt\nif len(sys.argv) < 3:\n    raise ValueError('Not enough input arguments!')\n\n# CKPT PATH\nCKPT_PATH=sys.argv[1]\n# CKPT EPOCH\nCKPT_EPOCH=sys.argv[2]\n\nH1 = os.path.join(CKPT_PATH, 'weights_MLP-MLP-{}.ckpt'.format(CKPT_EPOCH))\nH2 = os.path.join(CKPT_PATH, 'weights_MLP-best_MLP-{}.ckpt'.format(CKPT_EPOCH))\nif os.path.exists(H1):\n    print(H1)\nelif os.path.exists(H2):\n    print(H2)\nelse:\n    # Raise Error code\n    print(1)\n\n"""
spk_id/utils.py,8,"b""import numpy as np\nimport random\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom ahoproc_tools.io import *\nimport os\nimport pickle\nimport torch.nn as nn\nimport torch.optim as optim\nfrom random import shuffle\nimport librosa\n\n\ndef build_valid_list(tr_list, spk2idx, va_split=0.2):\n    if va_split > 0:\n        # apply a split of x% to each speaker in the tr_list\n        spk2utts = {}\n        for tr_file in tr_list:\n            spk = spk2idx[tr_file]\n            if spk not in spk2utts:\n                spk2utts[spk] = []\n            spk2utts[spk].append(tr_file)\n        va_files = []\n        tr_files = []\n        # Now select a split amount per speaker and rebuild train and valid lists\n        for spk, files in spk2utts.items():\n            spk_N = len(files)\n            shuffle(files)\n            spk_vaN = int(np.floor(spk_N * va_split))\n            va_files += files[:spk_vaN]\n            tr_files += files[spk_vaN:]\n        return tr_files, va_files\n    else:\n        return tr_files, []\n\ndef compute_utterances_durs(files, data_root):\n    durs = []\n    for file_ in files:\n        wav, rate = librosa.load(os.path.join(data_root,\n                                              file_),\n                                 sr=None)\n        durs.append(wav.shape[0])\n    return durs, rate\n\ndef compute_aco_durs(files, data_root, order=39,\n                     ext='mfcc', np_fmt=False):\n    durs = []\n    for file_ in files:\n        bname = os.path.splitext(file_)[0]\n        if np_fmt:\n            data = np.load(os.path.join(data_root, \n                                        bname + '.' + ext))\n        else:\n            data = read_aco_file(os.path.join(data_root, \n                                              bname + '.' + ext), \n                                 (-1, order))\n        durs.append(data.shape[0])\n    return durs\n\nclass LibriSpkIDMFCCDataset(Dataset):\n    \n    def __init__(self, data_root, files_list, spk2idx, order,\n                 stats_f=None, ext='mfcc', np_fmt=False):\n        super().__init__()\n        self.files_list = files_list\n        self.data_root = data_root\n        self.spk2idx = spk2idx\n        self.order = order\n        self.ext = ext\n        self.np_fmt = np_fmt\n        with open(stats_f, 'rb') as f:\n            self.stats = pickle.load(f)\n\n    def __getitem__(self, idx):\n        fpath = os.path.join(self.data_root, self.files_list[idx])\n        bname = os.path.splitext(fpath)[0]\n        #data = np.load(bname + '.npy')\n        if self.np_fmt:\n            data = np.load(bname + '.' + self.ext)\n        else:\n            data = read_aco_file(bname + '.' + self.ext, \n                                 (-1, self.order))\n        data = data - np.array(self.stats['mean'])\n        data = data / np.array(self.stats['std'])\n        lab = self.spk2idx[self.files_list[idx]]\n        return data, lab\n\n    def __len__(self):\n        return len(self.files_list)\n\nclass Collater(object):\n    \n    def __init__(self, max_len=None):\n        self.max_len = max_len\n        \n    def __call__(self, batch):\n        if self.max_len is None:\n            # track max seq len in batch\n            # and apply it padding others seqs\n            max_len = 0\n            for sample in batch:\n                mfcc, lab = sample\n                clen = mfcc.shape[0]\n                if clen > max_len:\n                    max_len = clen\n        else:\n            max_len = self.max_len\n        X = []\n        Y = []\n        slens = []\n        for sample in batch:\n            mfcc, lab = sample\n            clen = mfcc.shape[0]\n            if clen < max_len:\n                # pad with zeros in the end\n                P = max_len - clen\n                pad = np.zeros((P, mfcc.shape[1]))\n                mfcc = np.concatenate((mfcc, pad), axis=0)\n            elif clen > max_len:\n                # trim the end (applied if we specify max_len externally)\n                idxs = list(range(mfcc.shape[0] - max_len))\n                bidx = random.choice(idxs)\n                mfcc = mfcc[bidx:bidx + max_len]\n            X.append(mfcc)\n            Y.append(lab)\n            slens.append(clen)\n        X = torch.FloatTensor(X)\n        Y = torch.LongTensor(Y)\n        slens = torch.LongTensor(slens)\n        return X, Y, slens\n\ndef load_spk2idx(spk2idx_file):\n    spk2idx = np.load(spk2idx_file)\n    spk2idx = dict(spk2idx.any())\n    return spk2idx\n\ndef accuracy(Y_, Y):\n    # Get rid of temporal resolution here,\n    # average likelihood in time and then\n    # compute argmax and accuracy\n    Y__avg = torch.mean(Y_, 2)\n    pred = Y__avg.max(1, keepdim=True)[1]\n    acc = pred.eq(Y[:, 0].view_as(pred)).float().mean()\n    return acc\n\ndef select_optimizer(opts, model):\n    if opts.opt == 'adam':\n        return optim.Adam(model.parameters(),\n                          opts.lr)\n    elif opts.opt == 'sgd':\n        return optim.SGD(model.parameters(),\n                         opts.lr, momentum=opts.momentum)\n    elif opts.opt == 'rmsprop':\n        return optim.RMSprop(model.parameters(),\n                             opts.lr, alpha=0.95)\n    else:\n        raise TypeError('Unrecognized optimizer {}'.format(opts.opt))\n\ndef select_scheduler(opts, opt):\n    if opts.sched_mode == 'plateau':\n        sched = lr_scheduler.ReduceLROnPlateau(opt,\n                                               mode=opts.plateau_mode,\n                                               factor=opts.lrdec,\n                                               patience=opts.patience,\n                                               verbose=True)\n    elif opts.sched_mode == 'step':\n        sched = lr_scheduler.StepLR(opt, \n                                    step_size=opts.sched_step_size,\n                                    gamma=opts.lrdec)\n    else:\n        raise TypeError('Unrecognized optimizer LR scheduler'\n                        ' {}'.format(opts.sched_mode))\n    return sched\n"""
util_scripts/clusterize_frontend.py,5,"b""from sklearn.cluster import KMeans\nfrom pase.models.frontend import wf_builder\nfrom pase.dataset import PairWavDataset, DictCollater\nfrom torchvision.transforms import Compose\nfrom pase.transforms import *\nfrom torch.utils.data import DataLoader\nimport numpy as np\nimport argparse\nimport timeit\nimport pickle\nimport os\nimport json\n\n\ndef cluster(opts):\n    CUDA = True if torch.cuda.is_available() else False\n    device = 'cuda' if CUDA else 'cpu'\n    num_devices = 1\n    np.random.seed(opts.seed)\n    random.seed(opts.seed)\n    torch.manual_seed(opts.seed)\n    if CUDA:\n        torch.cuda.manual_seed_all(opts.seed)\n        num_devices = torch.cuda.device_count()\n        print('[*] Using CUDA {} devices'.format(num_devices))\n    else:\n        print('[!] Using CPU')\n    fe = wf_builder(opts.fe_cfg)\n    if opts.fe_ckpt is not None:\n        fe.load_pretrained(opts.fe_ckpt, load_last=True, verbose=True)\n    else:\n        print('WARNING: No pretrained ckpt loaded for FE! Random clustering?')\n    fe.to(device)\n    fe.eval()\n    trans = Compose([ToTensor(),\n                     SingleChunkWav(opts.chunk_size, random_scale=False)])\n    # Build Dataset(s) and DataLoader(s)\n    dset = PairWavDataset(opts.data_root, opts.data_cfg, 'train',\n                         transform=trans)\n    dloader = DataLoader(dset, batch_size=opts.batch_size,\n                         shuffle=True, collate_fn=DictCollater(),\n                         num_workers=opts.num_workers)\n    # acumulate train chunks and do clustering on them,\n    # with each chunk containing several frames\n    X = []\n    timings = []\n    N = opts.num_samples // opts.batch_size\n    beg_t = timeit.default_timer()\n    for bidx in range(1, N + 1, 1):\n        batch = next(dloader.__iter__())\n        chunk = batch['chunk']\n        y = fe(chunk.to(device)).mean(dim=2)\n        X.append(y.view(-1, y.size(-1)).cpu().data.numpy())\n        end_t = timeit.default_timer()\n        timings.append(end_t - beg_t)\n        beg_t = timeit.default_timer()\n        if bidx % opts.log_freq == 0 or bidx >= N:\n            print('Forwarded batch {:4d}/{:4d}, btime: {:.2f} s, '\n                  'mbtime: {:.2f} s'.format(bidx, N, timings[-1],\n                                            np.mean(timings)),\n                  end='\\r')\n    print()\n    X = np.concatenate(X, axis=0)\n    print('Total X shape: ', X.shape)\n    print('Running KMeans...')\n    beg_t = timeit.default_timer()\n    kmeans = KMeans(n_clusters=opts.k_clusters, n_jobs=opts.n_jobs,\n                    verbose=0).fit(X)\n    end_t = timeit.default_timer()\n    print('Clusterized in {:.2f} s'.format(end_t - beg_t))\n    print('Saving KMeans...')\n    with open(os.path.join(opts.save_path, 'kmeans.pkl'), 'wb') as f:\n        pickle.dump(kmeans, f)\n    print('Finished program')\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_cfg', type=str, \n                        default='data/librispeech_data.cfg')\n    parser.add_argument('--data_root', type=str, \n                        default='data/LibriSpeech/Librispeech_spkid_sel')\n    parser.add_argument('--fe_cfg', type=str, default=None)\n    parser.add_argument('--fe_ckpt', type=str, default=None)\n    parser.add_argument('--chunk_size', type=int, default=16000)\n    parser.add_argument('--num_samples', type=int, default=100000)\n    parser.add_argument('--num_workers', type=int, default=1)\n    parser.add_argument('--n_jobs', type=int, default=-1)\n    parser.add_argument('--seed', type=int, default=1)\n    parser.add_argument('--batch_size', type=int, default=200)\n    parser.add_argument('--log_freq', type=int, default=15)\n    parser.add_argument('--k_clusters', type=int, default=128,\n                        help='Number of clusters (Def: 128).')\n    parser.add_argument('--save_path', type=str, default='kmeans_FE')\n    opts = parser.parse_args()\n    if not os.path.exists(opts.save_path):\n        os.makedirs(opts.save_path)\n    with open(os.path.join(opts.save_path, 'cluster.opts'), 'w') as opts_f:\n        opts_f.write(json.dumps(vars(opts), indent=2))\n    cluster(opts)\n\n"""
util_scripts/encode_codec2.py,1,"b""import glob\nimport os\nimport multiprocessing as mp\nfrom pase.transforms import *\nimport tqdm\nimport argparse\n\ndef process_codec(args):\n    c2 = Codec2Buffer()\n    infile, outdir = args\n    bname = os.path.basename(infile)\n    outpath = os.path.join(outdir, bname)\n    x, rate = sf.read(infile)\n    y = c2({'chunk':torch.tensor(x)})['chunk']\n    sf.write(outpath, y, rate)\n\ndef main(opts):\n    assert opts.num_workers > 0, opts.num_workers\n    pool = mp.Pool(opts.num_workers)\n    wavs = glob.glob(os.path.join(opts.input_dir, '*.wav'))\n    args = [(wav, opts.output_dir) for wav in wavs]\n    if not os.path.exists(opts.output_dir):\n        os.makedirs(opts.output_dir)\n    for _ in tqdm.tqdm(pool.imap_unordered(process_codec, args),\n                       total=len(args)):\n        pass\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('input_dir', type=str, default=None)\n    parser.add_argument('output_dir', type=str, default=None)\n    parser.add_argument('--num_workers', type=int, default=8)\n\n    opts = parser.parse_args()\n\n\n    main(opts)\n\n"""
util_scripts/eval_ckpts.py,5,"b""from pase.models.core import Waveminionet\nfrom pase.dataset import PairWavDataset, DictCollater\nfrom torchvision.transforms import Compose\nfrom pase.transforms import *\nfrom pase.losses import *\nfrom pase.utils import pase_parser\nfrom tensorboardX import SummaryWriter\nfrom torch.utils.data import DataLoader\nimport torch\nimport pickle\nimport torch.nn as nn\nimport numpy as np\nimport argparse\nimport os\nimport json\nimport random\n\n\ndef eval(opts):\n    CUDA = True if torch.cuda.is_available() and not opts.no_cuda else False\n    device = 'cuda' if CUDA else 'cpu'\n    np.random.seed(opts.seed)\n    random.seed(opts.seed)\n    torch.manual_seed(opts.seed)\n    if CUDA:\n        torch.cuda.manual_seed_all(opts.seed)\n    print('Seeds initialized to {}'.format(opts.seed))\n    # ---------------------\n    # Transforms\n    trans = Compose([\n        ToTensor(),\n        MIChunkWav(opts.chunk_size, random_scale=opts.random_scale),\n        LPS(opts.nfft, hop=opts.stride, win=400),\n        MFCC(hop=opts.stride),\n        Prosody(hop=opts.stride, win=400),\n        ZNorm(opts.stats)\n    ])\n    print(trans)\n\n    # ---------------------\n    # Build Dataset(s) and DataLoader(s)\n    dset = PairWavDataset(opts.data_root, opts.data_cfg, 'valid',\n                         transform=trans)\n    dloader = DataLoader(dset, batch_size=opts.batch_size,\n                         shuffle=False, collate_fn=DictCollater(),\n                         num_workers=opts.num_workers)\n    # Compute estimation of bpe. As we sample chunks randomly, we\n    # should say that an epoch happened after seeing at least as many\n    # chunks as total_train_wav_dur // chunk_size\n    bpe = (dset.total_wav_dur // opts.chunk_size) // opts.batch_size\n\n    # ---------------------\n    # Build Model\n    if opts.fe_cfg is not None:\n        with open(opts.fe_cfg, 'r') as fe_cfg_f:\n            fe_cfg = json.load(fe_cfg_f)\n            print(fe_cfg)\n    else:\n        fe_cfg = None\n    model = Waveminionet(minions_cfg=pase_parser(opts.net_cfg),\n                         adv_loss=opts.adv_loss,\n                         pretrained_ckpt=opts.pretrained_ckpt,\n                         frontend_cfg=fe_cfg\n                        )\n\n    print(model)\n    model.to(device)\n    writer = SummaryWriter(opts.save_path)\n    if opts.max_epoch is not None:\n        # just make a sequential search til max epoch ckpts\n        ckpts = ['fullmodel_e{}.ckpt'.format(e) for e in range(opts.max_epoch)]\n    else:\n        ckpts = opts.ckpts\n    for model_ckpt in ckpts:\n        # name format is fullmodel_e{}.ckpt\n        epoch = int(model_ckpt.split('_')[-1].split('.')[0][1:])\n        model_ckpt = os.path.join(opts.ckpt_root,\n                                  model_ckpt)\n        print('Loading ckpt ', model_ckpt)\n        model.load_pretrained(model_ckpt, load_last=True, verbose=False)\n        model.eval_(dloader, opts.batch_size, \n                    bpe, log_freq=opts.log_freq,\n                    epoch_idx=epoch, writer=writer,\n                    device=device)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_root', type=str, \n                        default='data/VCTK')\n    parser.add_argument('--data_cfg', type=str, \n                        default='data/vctk_data.cfg')\n    parser.add_argument('--net_cfg', type=str,\n                        default=None)\n    parser.add_argument('--fe_cfg', type=str, default=None)\n    parser.add_argument('--ckpt_root', type=str, default='.')\n    parser.add_argument('--max_epoch', type=int, default=None)\n    parser.add_argument('--ckpts', type=str, nargs='+', default=None)\n    parser.add_argument('--stats', type=str, default='data/vctk_stats.pkl')\n    parser.add_argument('--pretrained_ckpt', type=str, default=None)\n    parser.add_argument('--save_path', type=str, default='ckpt')\n    parser.add_argument('--num_workers', type=int, default=2)\n    parser.add_argument('--seed', type=int, default=2)\n    parser.add_argument('--no-cuda', action='store_true', default=False)\n    parser.add_argument('--random_scale', action='store_true', default=False)\n    parser.add_argument('--chunk_size', type=int, default=16000)\n    parser.add_argument('--log_freq', type=int, default=100)\n    parser.add_argument('--epoch', type=int, default=1000)\n    parser.add_argument('--nfft', type=int, default=2048)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--hidden_size', type=int, default=256)\n    parser.add_argument('--hidden_layers', type=int, default=2)\n    parser.add_argument('--stride', type=int, default=160)\n    parser.add_argument('--fe_opt', type=str, default='Adam')\n    parser.add_argument('--min_opt', type=str, default='Adam')\n    parser.add_argument('--dout', type=float, default=0.2)\n    parser.add_argument('--fe_lr', type=float, default=0.0001)\n    parser.add_argument('--min_lr', type=float, default=0.0004)\n    parser.add_argument('--z_lr', type=float, default=0.0004)\n    parser.add_argument('--rndmin_train', action='store_true',\n                        default=False)\n    parser.add_argument('--adv_loss', type=str, default='BCE',\n                        help='BCE or L2')\n    parser.add_argument('--warmup', type=int, default=1,\n                        help='Epoch to begin applying z adv '\n                             '(Def: 2).')\n    parser.add_argument('--zinit_weight', type=float, default=1)\n    parser.add_argument('--zinc', type=float, default=0.0002)\n\n    opts = parser.parse_args()\n    if opts.net_cfg is None:\n        raise ValueError('Please specify a net_cfg file')\n\n    eval(opts)\n"""
util_scripts/forward_chunk.py,6,"b'from pase.models.core import Waveminionet\nfrom pase.models.frontend import wf_builder\nfrom pase.dataset import PairWavDataset, DictCollater\nfrom torchvision.transforms import Compose\nfrom pase.transforms import *\nfrom pase.losses import *\nfrom pase.utils import pase_parser\nfrom torch.utils.data import DataLoader\nimport tqdm\nimport torch\nimport pickle\nimport torch.nn as nn\nimport soundfile as sf\nimport numpy as np\nimport argparse\nimport os\nimport sys\nimport json\nimport random\n\n\ndef remove_Dcfg(minions_cfg):\n    # remove unnecessary Discriminator config if found\n    # in any of the minion fields\n    for m_i, mcfg in enumerate(minions_cfg):\n        if \'DNet_cfg\' in mcfg:\n            print(\'Removing DNet_cfg\')\n            del mcfg[\'DNet_cfg\']\n        if \'Dopt_cfg\' in mcfg:\n            print(\'Removing Dopt_cfg\')\n            del mcfg[\'Dopt_cfg\']\n\ndef main(opts):\n    device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n    # build network\n    minions_cfg = pase_parser(opts.minions_cfg, do_losses=False)\n    remove_Dcfg(minions_cfg)\n    pase = wf_builder(opts.cfg)\n    model = Waveminionet(minions_cfg=minions_cfg,\n                         num_devices=0,\n                         pretrained_ckpts=opts.ckpt,\n                         z_minion=False,\n                         frontend=pase)\n    model.eval()\n    model.to(device)\n    transf = Reverb([\'data/omologo_revs/IRs_2/IR_223108.imp\'], ir_fmt=\'imp\')\n    minion = model.minions[0]\n    minion.loss = None\n    pase = model.frontend\n    #print(opts.in_files)\n    in_files = [os.path.join(opts.files_root, inf) for inf in opts.in_files]\n    wavs = []\n    wfiles =  []\n    max_len = 0\n    print(\'Total batches: \', len(in_files) // opts.batch_size)\n    with torch.no_grad():\n        for wi, wfile in tqdm.tqdm(enumerate(in_files, start=1),\n                                   total=len(in_files)):\n            wfiles.append(wfile)\n            wav, rate = sf.read(wfile)\n            wavs.append(wav)\n            if len(wav) > max_len:\n                max_len = len(wav)\n            if wi % opts.batch_size == 0 or wi >= len(in_files):\n                lens = []\n                batch = []\n                for bi in range(len(wavs)):\n                    P_ = max_len - len(wavs[bi])\n                    lens.append(len(wavs[bi]))\n                    if P_ > 0:\n                        pad = np.zeros((P_))\n                        wav_ = np.concatenate((wavs[bi], pad), axis=0)\n                    else:\n                        wav_ = wavs[bi]\n                    wav = torch.FloatTensor(wav_)\n                    wav_r = transf({\'chunk\':wav})\n                    batch.append(wav_r[\'chunk\'].view(1, 1, -1))\n                batch = torch.cat(batch, dim=0)\n                x = batch.to(device)\n                h = pase(x)\n                #print(\'frontend size: \', h.size())\n                y = minion(h).cpu()\n                for bi in range(len(wavs)):\n                    bname = os.path.basename(wfiles[bi])\n                    y_ = y[bi].squeeze().data.numpy()\n                    y_ = y_[:lens[bi]]\n                    sf.write(os.path.join(opts.out_path, \n                                          \'{}\'.format(bname)),\n                             y_, 16000)\n                    x_ = x[bi].squeeze().data.numpy()\n                    x_ = x_[:lens[bi]]\n                    sf.write(os.path.join(opts.out_path, \n                                          \'input_{}\'.format(bname)),\n                             x_, 16000)\n                max_len = 0\n                wavs = []\n                wfiles =  []\n                batch = None\n\n    """"""\n    with open(\'data/librispeech_stats.pkl\', \'rb\') as stats_f:\n        stats = pickle.load(stats_f)\n        st = stats[\'chunk\']\n        y_ = y * st[\'std\'] + st[\'mean\']\n        y_ = y_.squeeze().data.numpy()\n        sf.write(\'reconchunk_{}\'.format(bname), y_, 16000)\n    """"""\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--minions_cfg\', type=str,\n                        default=\'cfg/all_RF6250_for_distorted_adversarial.cfg\')\n    parser.add_argument(\'--cfg\', type=str, default=\'cfg/PASE_RF6250.cfg\')\n    parser.add_argument(\'--ckpt\', type=str, default=None)\n    parser.add_argument(\'--batch_size\', type=int, default=10)\n    parser.add_argument(\'--out_path\', type=str, default=\'chunk_generations\')\n    parser.add_argument(\'--in_files\', type=str, default=None, nargs=\'+\')\n    parser.add_argument(\'--files_root\', type=str, default=\'.\')\n    parser.add_argument(\'--stats\', type=str,\n                        default=\'data/librispeech_stats_nochunks.pkl\')\n\n    opts = parser.parse_args()\n    if not os.path.exists(opts.out_path):\n        os.makedirs(opts.out_path)\n    main(opts)\n'"
util_scripts/make_contaminated_trainset.py,3,"b""from pase.dataset import *\nfrom torchvision.transforms import Compose\nimport json\nfrom pase.transforms import *\nimport tqdm\nfrom torch.utils.data import DataLoader\nimport soundfile as sf\nfrom train import config_distortions\nimport random\nimport numpy as np\nimport os\nimport torch\n\nrandom.seed(1)\nnp.random.seed(1)\ntorch.manual_seed(1)\n\nOUT_PATH = 'data/distorted_trainset'\nif not os.path.exists(OUT_PATH):\n    os.makedirs(OUT_PATH)\nNUM_BATCHES = 10000\n\nwith open('cfg/distortions/all_x26.cfg', 'r') as dtr_cfg:\n    dtr = json.load(dtr_cfg)\n    #dtr['trans_p'] = opts.distortion_p\n    dist_trans = config_distortions(**dtr)\n    print(dist_trans)\n    dataset = LibriSpeechSegTupleWavDataset\n    dset = dataset('data/LibriSpeech_50h/all/', \n                   'data/librispeech_data_50h.cfg',\n                   'train',\n                   transform=Compose([ToTensor(),\n                                      SingleChunkWav(32000,\n                                                     random_scale=True)]),\n                   distortion_transforms=dist_trans)\n    dloader = DataLoader(dset, batch_size=100,\n                         shuffle=True,\n                         collate_fn=DictCollater(),\n                         num_workers=4)\n    iterator = iter(dloader)\n    for bidx in tqdm.tqdm(range(1, NUM_BATCHES + 1), total=NUM_BATCHES):\n        try:\n            batch = next(iterator)\n        except StopIteration:\n            iterator = iter(dloader)\n            batch = next(iterator)\n        chunk = batch['chunk']\n        for sidx in range(chunk.shape[0]):\n            # scale it to properly fit in soundfile requirement\n            peak = torch.abs(chunk[sidx]).max().item()\n            sample = chunk[sidx].data.numpy()\n            if peak > 1:\n                sample = sample / peak\n            sf.write(os.path.join(OUT_PATH, 'utt_{}_{}.' \\\n                                  'wav'.format(bidx, sidx + 1)),\n                     sample[0],\n                     16000)\n"""
util_scripts/make_fbanks.py,0,"b""import argparse\nfrom python_speech_features import fbank\nimport multiprocessing as mp\nimport numpy as np\nimport os\nimport tqdm\nimport soundfile as sf\nimport glob\n\n\ndef wav2fbank(args):\n    wavname, out_dir, nfilt, log = args\n    x, rate = sf.read(wavname)\n    fb, egy = fbank(x, rate, nfilt=nfilt)\n    if log:\n        fb = np.log(fb)\n    bname = os.path.splitext(os.path.basename(wavname))[0]\n    outfile = os.path.join(out_dir, bname + '.fb')\n    np.save(outfile, fb)\n\ndef main(opts):\n    tasks = [(w, opts.out_dir, opts.nfilt, opts.log) for w in \\\n             glob.glob(os.path.join(opts.wav_dir, '*.wav'))]\n    pool = mp.Pool(opts.num_workers)\n    for _ in tqdm.tqdm(pool.imap_unordered(wav2fbank, tasks),\n                       total=len(tasks)):\n        pass\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--wav_dir', type=str, default=None)\n    parser.add_argument('--out_dir', type=str, default=None)\n    parser.add_argument('--log', action='store_true', default=False)\n    parser.add_argument('--num_workers', type=int, default=20)\n    parser.add_argument('--nfilt', type=int, default=40)\n    \n    opts = parser.parse_args()\n    if not os.path.exists(opts.out_dir):\n        os.makedirs(opts.out_dir)\n    main(opts)\n"""
util_scripts/project_features.py,1,"b""import numpy as np\nfrom tensorboardX import SummaryWriter\nimport json\nimport random\nrandom.seed(1)\nfrom random import shuffle\nimport torch\nimport tqdm\nimport os\nimport glob\n\n\nSAVE_PATH= 'vctk_projection_paseQRNN_age'\n#SAVE_PATH= 'vctk_projection_paseQRNN_id'\nif not os.path.exists(SAVE_PATH):\n    os.makedirs(SAVE_PATH)\nFEATS_DIR='vc/data/vctk/all_trimmed_paseQRNNpase-eval_vctk_raw/mel/'\nspk_info = json.load(open('vc/data/vctk/speaker-info.json', 'r'))\nprint('Found {} speakers'.format(len(spk_info)))\nfiles = glob.glob(os.path.join(FEATS_DIR, '*.npy'))\nprint('Found {} files'.format(len(files)))\nMAX_SPK_SAMPLES = 10000000\n#MAX_SPK_SAMPLES = 50\n#MAX_SPKS = 30\nMAX_SPKS = None\n#LABEL = 'ACCENTS'\nLABEL = 'AGE'\n#LABEL = 'GENDER'\n#LABEL = None\nshuffle(files)\n\nspk2count = {}\nspk2idx = {}\nlabels = []\nfeats = []\n\nfor fi, filename in tqdm.tqdm(enumerate(files, start=1), total=len(files)):\n    bname = os.path.basename(filename)\n    spkname = bname.split('_')[0]\n    if MAX_SPKS is not None and len(spk2count) >= MAX_SPKS:\n        if spkname not in spk2idx:\n            continue\n    if spkname not in spk2idx:\n        spk2count[spkname] = 0\n        spk2idx[spkname] = len(spk2idx)\n    data = np.load(filename)\n    avg = np.mean(data, axis=1)\n    spk2count[spkname] += 1\n    if spk2count[spkname] < MAX_SPK_SAMPLES:\n        feats.append(avg[None, :])\n        try:\n            lab = spk_info[spkname][LABEL]\n        except KeyError:\n            lab = spk2idx[spkname]\n        labels.append([lab])\n    if fi >= 1807:\n        break\n\nfeats = torch.FloatTensor(np.concatenate(feats, axis=0))\nlabels = np.concatenate(labels, axis=0)\nprint('feats shape: ', feats.shape)\nprint('labels shape: ', labels.shape)\nwriter = SummaryWriter(SAVE_PATH)\nwriter.add_embedding(feats, metadata=labels)\n\n\n\n"""
util_scripts/prosodic_eval.py,10,"b""from pase.models.core import Waveminionet\nfrom pase.dataset import PairWavDataset, DictCollater\nfrom torchvision.transforms import Compose\nfrom pase.transforms import *\nfrom pase.losses import *\nfrom pase.utils import pase_parser\nfrom torch.utils.data import DataLoader\nimport torch\nimport pickle\nimport timeit\nimport torch.nn as nn\nimport numpy as np\nimport argparse\nimport os\nimport json\nimport random\nfrom ahoproc_tools.error_metrics import RMSE, AFPR\n\n\ndef z_denorm(x, stats, device):\n    mean = torch.FloatTensor(stats['mean']).to(device)\n    std = torch.FloatTensor(stats['std']).to(device)\n    return x * std + mean\n\ndef forward_dloader(dloader, bpe, fe, pmodel, stats, tags, device):\n    proso_res = dict((k, []) for k in tags)\n    timings = []\n    beg_t = timeit.default_timer()\n    for bidx in range(bpe):\n        batch = next(dloader.__iter__())\n        with torch.no_grad():\n            Y = batch['prosody'].to(device)\n            fe_h = fe(batch['chunk'].to(device))\n            Y_ = pmodel(fe_h)\n        nfeats = Y.size(1)\n        Y = Y.transpose(1, 2).contiguous().view(-1, nfeats)\n        Y_ = Y_.transpose(1, 2).contiguous().view(-1, nfeats)\n        Y_ = z_denorm(Y_, stats['prosody'], device)\n        loss = pmodel.loss(Y_, Y)\n        # F0\n        pf0 = torch.exp(Y_[:, 0]).data.cpu().numpy()\n        gf0 = torch.exp(Y[:, 0]).data.cpu().numpy()\n        f0_rmse = RMSE(pf0, gf0)\n        proso_res['lf0'] = np.asscalar(f0_rmse)\n        # ZCR\n        pzcr = Y_[:, 2].data.cpu().numpy()\n        gzcr = Y[:, 2].data.cpu().numpy()\n        zcr_rmse = RMSE(pzcr, gzcr)\n        proso_res['zcr'] = np.asscalar(zcr_rmse)\n        # ENERGY\n        pegy = Y_[:, 3].data.cpu().numpy()\n        gegy = Y[:, 3].data.cpu().numpy()\n        egy_rmse = RMSE(pegy, gegy)\n        proso_res['egy'] = np.asscalar(egy_rmse)\n        # U/V FLAG\n        puv = np.round(Y_[:, 1].data.cpu().numpy())\n        guv = Y[:, 1].data.cpu().numpy()\n        uv_afpr = AFPR(puv, guv)[0]\n        proso_res['uv'] = np.asscalar(uv_afpr)\n        end_t = timeit.default_timer()\n        timings.append(end_t - beg_t)\n        beg_t = timeit.default_timer()\n        if (bidx + 1) % 10 == 0 or (bidx + 1) >= len(dloader):\n            print('Done batches {:5d}/{:5d}, mtime: {:.2f} s'\n                  ''.format(bidx + 1, len(dloader), np.mean(timings)),\n                 end='\\r')\n    print('')\n\n    return proso_res\n\ndef eval(opts):\n    CUDA = True if torch.cuda.is_available() and not opts.no_cuda else False\n    device = 'cuda' if CUDA else 'cpu'\n    np.random.seed(opts.seed)\n    random.seed(opts.seed)\n    torch.manual_seed(opts.seed)\n    if CUDA:\n        torch.cuda.manual_seed_all(opts.seed)\n    print('Seeds initialized to {}'.format(opts.seed))\n    # ---------------------\n    # Transforms\n    trans = Compose([\n        ToTensor(),\n        MIChunkWav(opts.chunk_size, random_scale=opts.random_scale),\n        Prosody(hop=opts.stride, win=400)\n    ])\n\n    with open(opts.stats, 'rb') as stats_f:\n        stats = pickle.load(stats_f)\n\n    # ---------------------\n    # Build Dataset(s) and DataLoader(s)\n    dset = PairWavDataset(opts.data_root, opts.data_cfg, 'test',\n                         transform=trans)\n    dloader = DataLoader(dset, batch_size=opts.batch_size,\n                         shuffle=False, collate_fn=DictCollater(),\n                         num_workers=opts.num_workers)\n    # Compute estimation of bpe. As we sample chunks randomly, we\n    # should say that an epoch happened after seeing at least as many\n    # chunks as total_train_wav_dur // chunk_size\n    bpe = (dset.total_wav_dur // opts.chunk_size) // opts.batch_size\n\n    # ---------------------\n    # Build Model\n    if opts.fe_cfg is not None:\n        with open(opts.fe_cfg, 'r') as fe_cfg_f:\n            fe_cfg = json.load(fe_cfg_f)\n            print(fe_cfg)\n    else:\n        fe_cfg = None\n    model = Waveminionet(minions_cfg=pase_parser(opts.net_cfg),\n                         frontend_cfg=fe_cfg\n                        )\n\n    print(model)\n    model.to(device)\n\n    ckpts = opts.ckpts\n    use_epid = False\n    if opts.ckpt_epochs is not None:\n        use_epid = True\n        ckpts = opts.ckpt_epochs\n    if ckpts is None:\n        raise ValueError('Please specify either ckpts or ckpt_epochs')\n\n    if opts.ckpt_root is None:\n        raise ValueError('Please specify ckpt_root!')\n\n    ckpts_res = []\n\n    for ckpt in ckpts:\n        if use_epid:\n            ckpt_name = 'fullmodel_e{}.ckpt'.format(ckpt)\n        else:\n            ckpt_name = ckpt\n        ckpt_path = os.path.join(opts.ckpt_root, ckpt_name)\n        print('Loading ckpt: ', ckpt_path)\n        model.load_pretrained(ckpt_path, load_last=True, verbose=True)\n\n        # select prosodic minion\n        pmodel = None\n        for minion in model.minions:\n            if 'prosody' in minion.name:\n                pmodel = minion\n\n        # select frontend\n        fe = model.frontend\n\n        ckpts_res.append(forward_dloader(dloader, bpe, fe, pmodel, \n                                         stats, opts.tags, device))\n        print('Results for ckpt {}'.format(ckpt_name))\n        print('-' * 25)\n        for k, v in ckpts_res[-1].items():\n            print('{}: {}'.format(k, np.mean(v)))\n        print('=' * 25)\n\n    with open(opts.out_file, 'w') as out_f:\n        out_f.write(json.dumps(ckpts_res, indent=2))\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--out_file', type=str, \n                        default='res_prosodic_eval.json')\n    parser.add_argument('--data_root', type=str, \n                        default='data/VCTK')\n    parser.add_argument('--data_cfg', type=str, \n                        default='data/vctk_data.cfg')\n    parser.add_argument('--net_cfg', type=str,\n                        default=None)\n    parser.add_argument('--fe_cfg', type=str, default=None)\n    parser.add_argument('--ckpt_root', type=str, default='.')\n    parser.add_argument('--ckpts', type=str, nargs='+', \n                        default=None)\n    parser.add_argument('--ckpt_epochs', type=str, nargs='+', \n                        default=None)\n    parser.add_argument('--stats', type=str, default='data/vctk_stats.pkl')\n    parser.add_argument('--save_path', type=str, default='ckpt')\n    parser.add_argument('--num_workers', type=int, default=2)\n    parser.add_argument('--seed', type=int, default=2)\n    parser.add_argument('--no-cuda', action='store_true', default=False)\n    parser.add_argument('--random_scale', action='store_true', default=False)\n    parser.add_argument('--chunk_size', type=int, default=16000)\n    parser.add_argument('--log_freq', type=int, default=100)\n    parser.add_argument('--epoch', type=int, default=1000)\n    parser.add_argument('--nfft', type=int, default=2048)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--stride', type=int, default=160)\n    parser.add_argument('--tags', type=str, nargs='+',\n                        default=['lf0', 'uv', 'zcr', 'egy'])\n\n    opts = parser.parse_args()\n    if opts.net_cfg is None:\n        raise ValueError('Please specify a net_cfg file')\n\n    eval(opts)\n"""
util_scripts/vadproc.py,0,"b'from scipy.io import wavfile\nimport soundfile as sf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport webrtcvad\nimport struct\nimport argparse\n\n\ndef main(opts):\n    rate, wav = wavfile.read(opts.in_file)\n    vad = webrtcvad.Vad()\n    vad.set_mode(opts.vad_mode)\n    win_len = int(opts.win_len * rate + .5)\n    # y is to plot (wav time resolution), vads contains one point per frame\n    y = []\n    vads = []\n    frames = []\n    for beg_i in np.arange(0, len(wav), win_len):\n        end_i = min(beg_i + win_len, len(wav))\n        frame = wav[beg_i:end_i]\n        frames.append(frame)\n        if len(frame) < win_len:\n            P = win_len - len(frame)\n            frame = np.concatenate((frame,\n                                    np.zeros((P,), dtype=frame.dtype)),\n                                    axis=0)\n        xb = struct.pack(""%dh"" % len(frame), *frame)\n        is_speech = vad.is_speech(xb,\n                                  sample_rate = rate)\n        y_i = 1 if is_speech else 0\n        vads.append(y_i)\n        y += [y_i] * win_len\n\n    if opts.show:\n        plt.subplot(2, 1, 1)\n        plt.plot(wav)\n        plt.subplot(2, 1, 2)\n        plt.plot(y)\n        plt.show()\n\n    if opts.trim_sil > 0:\n        assert opts.out_file is not None \n        # post-process to find silence regions over\n        # this value, and delete those pieces of speech\n        count0 = 0\n        fcandidates = []\n        out_samples = []\n        max_sil = int(np.ceil(((opts.trim_sil / 1000) * 16000) / win_len))\n        frame_len = len(frames[0])\n        for idx, (y_i, frame) in enumerate(zip(vads, frames), start=1):\n            if y_i == 0:\n                count0 += 1\n                fcandidates.extend(frame.tolist())\n            if y_i == 1 or idx >= len(frames):\n                # change detected, process all previous counts\n                # and frame candidates, to know whether to \n                # discard all but max allowed or to leave them alone\n                if count0 >= max_sil:\n                    # discard all candidates but last trim_sil ones\n                    K = max_sil * frame_len\n                    fcandidates = fcandidates[-K:]\n                if len(fcandidates) > 0:\n                    out_samples += fcandidates[:]\n                    fcandidates = []\n                frame = frame.tolist()\n                out_samples += frame[:]\n                count0 = 0\n        out_samples = np.array(out_samples, dtype=np.int16)\n        R = 100 - ((len(out_samples) / len(wav)) * 100.)\n        if opts.verbose:\n            print(\'Input num samples: \', len(wav))\n            print(\'Output num samples: \', len(out_samples))\n            print(\'Discarded {} % of samples\'.format(R))\n        if opts.show:\n            plt.subplot(3,1,1)\n            plt.plot(wav)\n            plt.subplot(3,1,2)\n            plt.plot(y)\n            plt.subplot(3,1,3)\n            plt.plot(out_samples)\n            plt.show()\n        sf.write(opts.out_file, out_samples, rate, \'PCM_16\')\n        if opts.out_log is not None:\n            with open(opts.out_log, \'w\') as out_log_f:\n                out_log_f.write(\'isamples\\tosamples\\treduction[%]\\n{}\\t{}\\t{:.2f}\\n\'\n                                \'\'.format(len(wav), len(out_samples), R))\n\n\n\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'in_file\', type=str, default=None,\n                        help=\'Path to wav file to be processed (Def: None).\')\n    parser.add_argument(\'--trim_sil\', type=float,\n                        default=0,\n                        help=\'Silence regions over this value \'\n                             \'will be trimmed. The value 0 has no effect. \'\n                             \'Specify in milisec, please (Def: 0).\')\n    parser.add_argument(\'--verbose\', action=\'store_true\', default=False)\n    parser.add_argument(\'--out_file\', type=str, default=None)\n    parser.add_argument(\'--out_log\', type=str, default=None,\n                        help=\'Will write log in this file if specified\')\n    parser.add_argument(\'--show\', action=\'store_true\', default=False)\n    parser.add_argument(\'--win_len\', type=float, default=0.02, \n                        help=\'In seconds (Def: 0.02).\')\n    parser.add_argument(\'--vad_mode\', type=int, default=3)\n\n\n    opts = parser.parse_args()\n    main(opts)\n'"
ASR/waveminionet/__init__.py,0,b''
ASR/waveminionet/dataset.py,4,"b'import torch\nfrom torch.utils.data import Dataset\nimport soundfile as sf\nimport json\nimport librosa\nimport os\nimport random\nimport numpy as np\nfrom collections import defaultdict\n\n\nclass DictCollater(object):\n\n    def __init__(self, batching_keys=[\'chunk\',\n                                      \'chunk_ctxt\',\n                                      \'chunk_rand\',\n                                      \'lps\',\n                                      \'mfcc\',\n                                      \'prosody\'],\n                labs=False):\n        self.batching_keys = batching_keys\n        self.labs = labs\n\n    def __call__(self, batch):\n        batches = {}\n        lab_b = False\n        labs = None\n        lab_batches = []\n        for sample in batch:\n            if len(sample) > 1 and self.labs:\n                labs = sample[1:]\n                sample = sample[0]\n                if len(lab_batches) == 0:\n                    for lab in labs:\n                        lab_batches.append([])\n            for k, v in sample.items():\n                if k not in self.batching_keys:\n                    continue\n                if k not in batches:\n                    batches[k] = []\n                if v.dim() == 1:\n                    v = v.view(1, 1, -1)\n                elif v.dim() == 2:\n                    v = v.unsqueeze(0)\n                else:\n                    raise ValueError(\'Error in collating dimensions for size \'\n                                     \'{}\'.format(v.size()))\n                batches[k].append(v)\n            if labs is not None:\n                for lab_i, lab in enumerate(labs):\n                    lab_batches[lab_i].append(lab)\n        for k in batches.keys():\n            batches[k] = torch.cat(batches[k], dim=0)\n        if labs is not None:\n            rets = [batches]\n            for li in range(len(lab_batches)):\n                rets.append(torch.tensor(lab_batches[li]))\n        else:\n            rets = batches\n        return rets\n\ndef uttwav_collater(batch):\n    """""" Simple collater where (wav, utt) pairs are\n    given by the a dataset, and (wavs, utts, lens) are\n    returned\n    """"""\n    max_len = 0\n    for sample in batch:\n        wav, uttname = sample\n        if wav.shape[0] > max_len:\n            max_len = wav.shape[0]\n\n    wavs = []\n    utts = []\n    lens = []\n\n    for sample in batch:\n        wav, uttname = sample\n        T = wav.shape[0]\n        P = max_len - T\n        if P > 0:\n            wav = np.concatenate((wav,\n                                  np.zeros((P,))),\n                                 axis=0)\n        wavs.append(wav)\n        utts.append(uttname)\n        lens.append(T)\n    return torch.FloatTensor(wavs), utts, torch.LongTensor(lens)\n\nclass WavDataset(Dataset):\n\n    def __init__(self, data_root, data_cfg_file, split, \n                 transform=None, sr=None, return_uttname=False,\n                 return_spk=False,\n                 verbose=True):\n        # sr: sampling rate, (Def: None, the one in the wav header)\n        self.sr = sr\n        self.data_root = data_root\n        self.data_cfg_file = data_cfg_file\n        if not isinstance(data_cfg_file, str):\n            raise ValueError(\'Please specify a path to a cfg \'\n                             \'file for loading data.\')\n\n        self.return_uttname = return_uttname\n        self.return_spk = return_spk\n        self.split = split\n        self.transform = transform\n        with open(data_cfg_file, \'r\') as data_cfg_f:\n            self.data_cfg = json.load(data_cfg_f)\n            self.spk_info = self.data_cfg[\'speakers\']\n            if verbose:\n                print(\'Found {} speakers info\'.format(len(self.spk_info)))\n                wavs = self.data_cfg[split][\'data\']\n                print(\'Found {} files in {} split\'.format(len(wavs),\n                                                          split))\n                spks = self.data_cfg[split][\'speakers\']\n                print(\'Found {} speakers in {} split\'.format(len(spks),\n                                                             split))\n                self.total_wav_dur = self.data_cfg[split][\'total_wav_dur\']\n                if \'spk2idx\' in self.data_cfg and return_spk:\n                    self.spk2idx = self.data_cfg[\'spk2idx\']\n                    print(\'Loaded spk2idx with {} \'\n                          \'speakers\'.format(len(self.spk2idx)))\n            self.wavs = wavs\n        self.wav_cache = {}\n\n    def __len__(self):\n        return len(self.wavs)\n\n    def retrieve_cache(self, fname, cache):\n        if fname in cache:\n            return cache[fname]\n        else:\n            wav, rate = librosa.load(fname, sr=self.sr)\n            cache[fname] = wav\n            return wav\n\n    def __getitem__(self, index):\n        uttname = self.wavs[index][\'filename\']\n        wname = os.path.join(self.data_root, uttname)\n        wav = self.retrieve_cache(wname, self.wav_cache)\n        if self.transform is not None:\n            wav = self.transform(wav)\n        rets = [wav]\n        if self.return_uttname:\n            rets = rets + [uttname]\n        if self.return_spk:\n            rets = rets + [self.spk2idx[self.wavs[index][\'speaker\']]]\n        if len(rets) == 1: \n            return rets[0]\n        else: \n            return rets\n\nclass PairWavDataset(WavDataset):\n    """""" Return paired wavs, one is current wav and the other one is a randomly\n        chosen one.\n    """"""\n    def __init__(self, data_root, data_cfg_file, split, \n                 transform=None, sr=None, verbose=True):\n        super().__init__(data_root, data_cfg_file, split, transform=transform, \n                         sr=sr,\n                         verbose=verbose)\n        self.rwav_cache = {}\n\n    def __getitem__(self, index):\n        # Here we select two wavs, the current one and a randomly chosen one\n        wname = os.path.join(self.data_root, self.wavs[index][\'filename\'])\n        wav = self.retrieve_cache(wname, self.wav_cache)\n        # create candidate indices without current index\n        indices = list(range(len(self.wavs)))\n        indices.remove(index)\n        rindex = random.choice(indices)\n        rwname = os.path.join(self.data_root, self.wavs[rindex][\'filename\'])\n        rwav = self.retrieve_cache(rwname, self.rwav_cache)\n        if self.transform is not None:\n            ret = self.transform({\'raw\': wav, \'raw_rand\': rwav})\n            return ret\n        else:\n            return wav, rwav\n\n        \n\nif __name__ == \'__main__\':\n    print(\'WavDataset\')\n    print(\'-\' * 30)\n    dset = WavDataset(\'/veu/spascual/DB/VCTK\', \'../data/vctk_data.cfg\', \'train\')\n    print(dset[0])\n    print(dset[0].shape)\n    print(\'=\' * 30)\n\n    dset = PairWavDataset(\'/veu/spascual/DB/VCTK\', \'../data/vctk_data.cfg\', \'train\')\n    print(\'PairWavDataset\')\n    print(\'-\' * 30)\n    wav, rwav = dset[0]\n    print(\'({}, {})\'.format(wav.shape, rwav.shape))\n    print(\'=\' * 30)\n'"
ASR/waveminionet/losses.py,4,"b""import torch\nimport torch.nn as nn\n\n\nclass RegressionLoss(object):\n\n    def __call__(self, pred, gtruth):\n        loss = self.criterion(pred, gtruth)\n        return loss\n\nclass AdversarialLoss(object):\n\n    def __init__(self, z_gen=torch.randn,\n                 loss='L2'):\n        self.z_gen = z_gen\n        self.loss = loss\n        if loss == 'L2':\n            self.criterion = nn.MSELoss()\n        elif loss == 'BCE':\n            self.criterion = nn.BCEWithLogitsLoss()\n        else:\n            raise ValueError('Unrecognized loss ', loss)\n\n    def register_DNet(self, Dnet):\n        self.Dnet = Dnet\n\n    def __call__(self, fake, optim):\n        if not hasattr(self, 'Dnet'):\n            raise ValueError('Please register Dnet first '\n                             'prior to using L2Adversarial Loss.')\n        optim.zero_grad()\n        real = self.z_gen(fake.size())\n        if fake.is_cuda:\n            real = real.to('cuda')\n        dreal = self.Dnet(real)\n        lab_1 = torch.ones(dreal.size())\n        if fake.is_cuda:\n            lab_1 = lab_1.to('cuda')\n        dreal_loss = self.criterion(dreal, lab_1)\n\n        dfake = self.Dnet(fake.detach())\n        lab_0 = torch.zeros(dfake.size())\n        if fake.is_cuda:\n            lab_0 = lab_0.to('cuda')\n        dfake_loss = self.criterion(dfake, lab_0)\n        d_loss = dreal_loss + dfake_loss\n        d_loss.backward()\n        optim.step()\n\n        greal = self.Dnet(fake)\n        greal_loss = self.criterion(greal, lab_1)\n        return dreal_loss, dfake_loss, greal_loss\n\n"""
ASR/waveminionet/transforms.py,17,"b'import torch\nimport numpy as np\nimport random\nimport pysptk\nimport librosa\nimport pickle\nfrom ahoproc_tools.interpolate import interpolation\n\n\ndef norm_and_scale(wav):\n    assert isinstance(wav, torch.Tensor), type(wav)\n    wav = wav / torch.max(torch.abs(wav))\n    return wav * torch.rand(1)\n\ndef format_package(x):\n    if not isinstance(x, dict):\n        return {\'raw\':x}\n    return x\n\nclass ToTensor(object):\n\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        for k, v in pkg.items():\n            # convert everything in the package\n            # into tensors\n            if not isinstance(v, torch.Tensor):\n                pkg[k] = torch.tensor(v)\n        return pkg\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'()\'\n\nclass ZNorm(object):\n\n    def __init__(self, stats):\n        self.stats_name = stats\n        with open(stats, \'rb\') as stats_f:\n            self.stats = pickle.load(stats_f)\n\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        for k, st in self.stats.items():\n            #assert k in pkg, \'{} != {}\'.format(list(pkg.keys()),\n            #                                   list(self.stats.keys()))\n            if k in pkg:\n                mean = st[\'mean\'].unsqueeze(1)\n                std = st[\'std\'].unsqueeze(1)\n                pkg[k] = (pkg[k] - mean) / std\n        return pkg\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'({})\'.format(self.stats_name)\n\nclass SingleChunkWav(object):\n\n    def __init__(self, chunk_size, random_scale=True):\n        self.chunk_size = chunk_size\n        self.random_scale = random_scale\n\n    def assert_format(self, x):\n        # assert it is a waveform and pytorch tensor\n        assert isinstance(x, torch.Tensor), type(x)\n        assert x.dim() == 1, x.size()\n\n    def select_chunk(self, wav):\n        # select random index\n        chksz = self.chunk_size\n        idxs = list(range(wav.size(0) - chksz))\n        idx = random.choice(idxs)\n        chk = wav[idx:idx + chksz]\n        return chk\n\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        raw = pkg[\'raw\']\n        self.assert_format(raw)\n        pkg[\'chunk\'] = self.select_chunk(raw)\n        if self.random_scale:\n            pkg[\'chunk\'] = norm_and_scale(pkg[\'chunk\'])\n        return pkg\n\n    def __repr__(self):\n        return self.__class__.__name__ + \\\n                \'({})\'.format(self.chunk_size)\n\nclass MIChunkWav(SingleChunkWav):\n\n    """""" Max-Information chunker expects 2 input wavs,\n        and extract 3 chunks: (chunk, chunk_ctxt,\n        and chunk_rand). The first two correspond to same\n        waveform, the third one is sampled from the second wav\n    """"""\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        if \'raw_rand\' not in pkg:\n            raise ValueError(\'Need an input pair of wavs to do \'\n                             \'MI chunking! Just got single raw wav?\')\n        raw = pkg[\'raw\']\n        raw_rand = pkg[\'raw_rand\']\n        self.assert_format(raw)\n        self.assert_format(raw_rand)\n        pkg[\'chunk\'] = self.select_chunk(raw)\n        pkg[\'chunk_ctxt\'] = self.select_chunk(raw)\n        pkg[\'chunk_rand\'] = self.select_chunk(raw_rand)\n        if self.random_scale:\n            pkg[\'chunk\'] = norm_and_scale(pkg[\'chunk\'])\n            pkg[\'chunk_ctxt\'] = norm_and_scale(pkg[\'chunk_ctxt\'])\n            pkg[\'chunk_rand\'] = norm_and_scale(pkg[\'chunk_rand\'])\n        return pkg\n\nclass LPS(object):\n\n    def __init__(self, n_fft=2048, hop=80,\n                 win=320, \n                 device=\'cpu\'):\n        self.n_fft = n_fft\n        self.hop = hop\n        self.win = win\n        self.device = device\n\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        max_frames = wav.size(0) // self.hop\n        wav = wav.to(self.device)\n        X = torch.stft(wav, self.n_fft,\n                       self.hop, self.win)\n        X = torch.norm(X, 2, dim=2).cpu()[:, :max_frames]\n        pkg[\'lps\'] = 10 * torch.log10(X ** 2 + 10e-20).cpu()\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(n_fft={}, hop={}, win={}\'.format(self.n_fft,\n                                                   self.hop,\n                                                   self.win)\n        attrs += \', device={})\'.format(self.device)\n        return self.__class__.__name__ + attrs\n\nclass MFCC(object):\n\n    def __init__(self, n_fft=2048, hop=80, \n                 order=20, sr=16000):\n        self.n_fft = n_fft\n        self.hop = hop\n        self.order = order\n        self.sr = 16000\n\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        y = wav.data.numpy()\n        max_frames = y.shape[0] // self.hop\n        mfcc = librosa.feature.mfcc(y, sr=self.sr,\n                                    n_mfcc=self.order,\n                                    n_fft=self.n_fft,\n                                    hop_length=self.hop\n                                   )[:, :max_frames]\n        pkg[\'mfcc\'] = torch.tensor(mfcc.astype(np.float32))\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(order={}, sr={})\'.format(self.order,\n                                           self.sr)\n        return self.__class__.__name__ + attrs\n\nclass Prosody(object):\n\n    def __init__(self, hop=80, win=320, f0_min=60, f0_max=300,\n                 sr=16000):\n        self.hop = hop\n        self.win = win\n        self.f0_min = f0_min\n        self.f0_max = f0_max\n        self.sr = sr\n\n    def __call__(self, pkg):\n        pkg = format_package(pkg)\n        wav = pkg[\'chunk\']\n        wav = wav.data.numpy()\n        max_frames = wav.shape[0] // self.hop\n        # first compute logF0 and voiced/unvoiced flag\n        f0 = pysptk.swipe(wav.astype(np.float64),\n                          fs=self.sr, hopsize=self.hop,\n                          min=self.f0_min,\n                          max=self.f0_max,\n                          otype=\'f0\')\n        lf0 = np.log(f0 + 1e-10)\n        lf0, uv = interpolation(lf0, -1)\n        lf0 = torch.tensor(lf0.astype(np.float32)).unsqueeze(0)[:, :max_frames]\n        uv = torch.tensor(uv.astype(np.float32)).unsqueeze(0)[:, :max_frames]\n        if torch.sum(uv) == 0:\n            # if frame is completely unvoiced, make lf0 min val\n            lf0 = torch.ones(uv.size()) * np.log(self.f0_min)\n        assert lf0.min() > 0, lf0.data.numpy()\n        # secondly obtain zcr\n        zcr = librosa.feature.zero_crossing_rate(y=wav,\n                                                 frame_length=self.win,\n                                                 hop_length=self.hop)\n        zcr = torch.tensor(zcr.astype(np.float32))\n        zcr = zcr[:, :max_frames]\n        # finally obtain energy\n        egy = librosa.feature.rmse(y=wav, frame_length=self.win,\n                                   hop_length=self.hop,\n                                   pad_mode=\'constant\')\n        egy = torch.tensor(egy.astype(np.float32))\n        egy = egy[:, :max_frames]\n        proso = torch.cat((lf0, uv, egy, zcr), dim=0)\n        pkg[\'prosody\'] = proso\n        return pkg\n\n    def __repr__(self):\n        attrs = \'(hop={}, win={}, f0_min={}, f0_max={}\'.format(self.hop,\n                                                               self.win,\n                                                               self.f0_min,\n                                                               self.f0_max)\n        attrs += \', sr={})\'.format(self.sr)\n        return self.__class__.__name__ + attrs\n                \n\nif __name__ == \'__main__\':\n    import librosa\n    from torchvision.transforms import Compose\n    wav, rate = librosa.load(\'test.wav\')\n    trans = Compose([\n        ToTensor(),\n        MIChunkWav(16000),\n        LPS(),\n        MFCC(),\n        Prosody()\n    ])\n    print(trans)\n    x = trans({\'raw\':wav, \'raw_rand\':wav})\n    print(list(x.keys()))\n\n\n\n'"
ASR/waveminionet/utils.py,1,"b""import json\nimport torch\nimport torch.nn as nn\n\n\ndef waveminionet_parser(cfg_fname):\n    with open(cfg_fname, 'r') as cfg_f:\n        cfg_all = json.load(cfg_f)\n        # change loss section to select those\n        # from nn package\n        for i, cfg in enumerate(cfg_all):\n            cfg_all[i]['loss'] = getattr(nn, \n                                         cfg_all[i]['loss'])()\n        return cfg_all\n"""
data/prep/chime5_utils.py,0,"b'\r\nimport errno\r\nimport sys\r\nimport argparse\r\nimport logging\r\nimport os\r\nimport re\r\nimport random\r\nimport numpy\r\nimport soundfile as sf\r\nimport multiprocessing\r\nimport tqdm\r\nimport json\r\nfrom kaldi_data_dir import KaldiDataDir\r\n\r\n#these are global functions, as we use them in Pool workers later so\r\n#must be pickable at any level\r\ndef get_wav_and_chan(path):\r\n    #check if raw file, or smth like, in which case we need path and chan\r\n    #sox /export/corpora/CHiME5/audio/train/S03_P09.wav -t wav - remix 1 |\r\n    if re.match(r\'.*\\|\', path):\r\n        #we play with pipe, extract wav and channel\r\n        r = re.search(r\'.*\\s(.*\\.wav)\\s.*remix\\s([1-9]).*\', path)\r\n        if r:\r\n            return r.group(1), int(r.group(2))-1\r\n        return None, None\r\n    return path, None\r\n\r\ndef process_segment(filein, fileout, beg, end, sigin=None, fs=None):\r\n    #print (""Processing {} to {} for seg ({},{})"".format(filein, fileout, beg, end))\r\n    #return True\r\n    if sigin is not None:\r\n        assert fs is not None, (\r\n            ""Passed signal as array, but not specified sampling rate (fs)""\r\n        )\r\n    path, chan = get_wav_and_chan(filein)\r\n    if path is None:\r\n        print (""File {} cannot be parsed"".format(path))\r\n        return False\r\n\r\n    if not os.path.exists(path):\r\n        print (""File {} not found"".format(path))\r\n        return False\r\n\r\n    if sigin is None:\r\n        sigin, fs = sf.read(path)\r\n    \r\n    beg_i, end_i = int(beg*fs), int(end*fs)\r\n    if beg_i >= end_i or end_i > sigin.shape[0]:\r\n        print (""Cant extract segment {} - {}, as wav {} is {} ""\\\r\n            .format(beg_i, end_i, filein, sigin.shape[0]))\r\n        return False\r\n\r\n    if chan is not None:\r\n        if sigin.ndim > 1 and sigin.shape[1] <= chan:\r\n            print (""File {} has not {} chan present"".format(filein, chan))\r\n            return False\r\n        sigout = sigin[beg_i:end_i, chan]\r\n    else:\r\n        sigout = sigin[beg_i:end_i]\r\n    sf.write(fileout, sigout, fs)\r\n    return True\r\n\r\ndef pool_segment_worker(sess):\r\n    sessid, utts = sess\r\n    tot_success = 0\r\n    for utt in utts:\r\n        uttid, e = utt\r\n        r = process_segment(filein=e[\'file_in\'],\r\n                             fileout=e[\'file_out\'],\r\n                             beg=e[\'seg_beg\'], \r\n                             end=e[\'seg_end\'])\r\n        if r: tot_success += 1\r\n        #print (""Processed {} from {} success: {}"".format(uttid, sessid, r))\r\n    return tot_success\r\n\r\ndef pool_recording_worker(sess):\r\n    sessid, utts = sess\r\n    tot_success = 0\r\n    sigin, fs, sessfile = None, None, None\r\n    #for each utterance in sessid file (one large file)\r\n    for utt in utts:\r\n        uttid, e = utt\r\n        filein = e[\'file_in\'] \r\n        fileout = e[\'file_out\']\r\n        beg = e[\'seg_beg\']\r\n        end = e[\'seg_end\']\r\n\r\n        #preload the recording once at first request\r\n        if sigin is None:\r\n            sessfile, _ = get_wav_and_chan(filein)\r\n            sigin, fs = sf.read(sessfile)\r\n        else:\r\n            path, _ = get_wav_and_chan(filein)\r\n            assert path == sessfile, (\r\n                ""Expected segments share same source wav file {} in session {},""\\\r\n                  ""but got {} for utt {}"".format(sessfile, sessid, path, uttid)\r\n            ) \r\n        r = process_segment(filein=filein, fileout=fileout, \r\n                       beg=beg, end=end, \r\n                       sigin=sigin, fs=fs)\r\n        #r = True\r\n        if r:\r\n            tot_success += 1\r\n            #print (""Proc utt {} (sess {}). From {} to {} into {}"".format(uttid, sessid, beg, end, fileout))\r\n        else:\r\n            print (""Failed to process utt {} (sess {})"".format(utt, sessid))\r\n    return tot_success\r\n\r\nclass PasePrep4Chime5(object):\r\n    def __init__(self, out_dir, ihm_dir, sdm_dir=None, num_workers=5):\r\n        assert os.path.exists(out_dir), (\r\n            ""Out dir {} expected to exists"".format(out_dir)\r\n        )\r\n        self.out_dir = out_dir\r\n        self.name = ihm_dir\r\n        self.ihm = KaldiDataDir(ihm_dir)\r\n        self.sdm = None\r\n        if sdm_dir is not None:\r\n            self.sdm = KaldiDataDir(sdm_dir)\r\n        self.num_workers = num_workers\r\n        self.fs = 16000\r\n\r\n    def show_stats(self):\r\n        print (""Stats for {}"".format(self.name))\r\n        print (""\\t #spkeakers: {}"".format(self.ihm.num_spk))\r\n        print (""\\t #utterances: {}"".format(self.ihm.num_utt))\r\n        print (""\\t Tot dur: {} hours"".format(self.ihm.total_duarion/3600))\r\n\r\n        if self.sdm is not None:\r\n            print (""Stats for {}"".format(self.name))\r\n            print (""\\t #spkeakers: {}"".format(self.sdm.num_spk))\r\n            print (""\\t #utterances: {}"".format(self.sdm.num_utt))\r\n            print (""\\t Tot dur: {} hours"".format(self.sdm.total_duarion/3600))\r\n\r\n    def get_segments_per_spk(self):\r\n        """"""Returns \r\n        """"""\r\n        for i, spk in enumerate(self.ihm.spk2utt_.keys()):\r\n            utts = self.ihm.spk2utt_[spk]\r\n            print (""Spk {} has {} segments"".format(spk, len(utts)))\r\n            for utt in utts:\r\n                seg = self.ihm.utt2segments_[utt].split("" "")\r\n                rec = seg[0]\r\n                wav = self.ihm.utt2wav_[rec]\r\n                print (""Utt {}, rec {}, wav {} "".format(utt, rec, wav))\r\n            if i>10:\r\n                break\r\n\r\n    def get_worn_uall_overlap(self):\r\n\r\n        def dist2gen(u):\r\n            x = re.sub(r\'\\.CH[0-9]\', \'\', u)\r\n            x = re.sub(r\'\\_U0[0-9]\\_\', \'_\', x)\r\n            return x\r\n        \r\n        def worn2gen(u):\r\n            x = re.sub(r\'\\.[L|R]\', \'\', u)\r\n            return x\r\n\r\n        utts = self.ihm.utt2spk_.keys()\r\n        utts_worn = list(filter(lambda u: re.search(r\'\\.[L|R]\\-\', u), utts))\r\n        utts = self.sdm.utt2spk_.keys()\r\n        utts_dist = list(filter(lambda u: re.search(r\'\\.CH[0-9]\\-\', u), utts))\r\n        print (""Found {} worn and {} dist files"".format(len(utts_worn), len(utts_dist)))\r\n\r\n        gen_utts_worn = dict(list(map(worn2gen, utts_worn)))\r\n        gen_utts_dist = dict(list(map(dist2gen, utts_dist)))\r\n        worn_set = set(gen_utts_worn.keys())\r\n        dist_set = set(gen_utts_dist.keys())\r\n\r\n        #gen_utts_worn = list(map(worn2gen, utts_worn))\r\n        #gen_utts_dist = list(map(dist2gen, utts_dist))\r\n        #worn_set = set(gen_utts_worn)\r\n        #dist_set = set(gen_utts_dist)\r\n\r\n        isect = worn_set & dist_set\r\n\r\n        print (isect)\r\n        print (""Orig lists are {} and {} long"".format(len(gen_utts_worn), len(gen_utts_dist)))\r\n        print (""Intersection is {}, while worn and dist are {}, {}"".format(len(isect), len(worn_set), len(dist_set)))\r\n        #print (isect)\r\n\r\n    def get_Us_for_worn_text(self, min_words_per_seg=2):\r\n        """"""\r\n        segmentations differ a bit for each of devices vs. worn, thus\r\n        we pair them up based on text hash.\r\n        """"""\r\n\r\n        def mk_txt_id(text, utt):\r\n            ps = utt.split(""_"")\r\n            return ""{}_{} {}"".format(ps[0], ps[1], text)\r\n\r\n        ihm_utts = list(self.ihm.utt2text_.keys())\r\n        sdm_utts = list(self.sdm.utt2text_.keys())\r\n\r\n        random.shuffle(ihm_utts)\r\n        random.shuffle(sdm_utts)\r\n\r\n        text2utt_ihm = {}\r\n        skipped_length_ihms = 0\r\n        skipped_doubles = 0\r\n        for utt in ihm_utts:\r\n            txt = self.ihm.utt2text_[utt]\r\n            if len(txt.split("" "")) < min_words_per_seg:\r\n                skipped_length_ihms += 1\r\n                continue\r\n            newid = mk_txt_id(txt, utt)\r\n            if newid in text2utt_ihm:\r\n                #print (""Skipping {} in {} -> {}"".format(newid, utt, text2utt_ihm[newid]))\r\n                skipped_doubles += 1\r\n                continue\r\n            text2utt_ihm[newid] = utt\r\n\r\n        print (""For IHM, skipped {} too short and {} doubled segs (out of {}). Left {}""\\\r\n                   .format(skipped_length_ihms, skipped_doubles, len(ihm_utts), len(text2utt_ihm)))\r\n\r\n        text2utt_sdm = {}\r\n        skipped_length_sdms = 0\r\n        skipped_doubles = 0\r\n        for utt in sdm_utts:\r\n            txt = self.sdm.utt2text_[utt]\r\n            if len(txt.split("" "")) < min_words_per_seg:\r\n                skipped_length_sdms += 1\r\n                continue\r\n            newid = mk_txt_id(txt, utt)\r\n            if newid in text2utt_sdm:\r\n                skipped_doubles += 1\r\n                continue\r\n            text2utt_sdm[newid] = utt\r\n\r\n        print (""For SDM, skipped {} too short and {} segs (out of {}). Left {}""\\\r\n                   .format(skipped_length_sdms, skipped_doubles, len(sdm_utts), len(text2utt_sdm)))\r\n\r\n        u1, u2 = set(text2utt_ihm.keys()), set(text2utt_sdm.keys())\r\n        utts_joint = u1 & u2\r\n        print (""Overlap is {}"".format(len(utts_joint)))\r\n\r\n        spks = self.ihm.spk2utt_.keys()\r\n        spk2chunks = {spk: {\'ihm\':[], \'sdm\':[]} for spk in spks}\r\n        tot = 0\r\n        for idx, utt_joint in enumerate(utts_joint):\r\n            try:\r\n                org_utt_ihm = text2utt_ihm[utt_joint]\r\n                org_utt_sdm = text2utt_sdm[utt_joint]\r\n                #print (""Trying to get {} {}"".format(org_utt_ihm, org_utt_sdm))\r\n                spk_ihm = self.ihm.utt2spk_[org_utt_ihm]\r\n                spk_sdm = self.sdm.utt2spk_[org_utt_sdm]\r\n                assert spk_ihm == spk_sdm, (\r\n                    ""{} {} not the same"".format(spk_ihm, spk_sdm)\r\n                )\r\n                #print (""IHM / SDM are {} {}"".format(org_utt_ihm, org_utt_sdm))\r\n                #reco_ihm, beg_ihm, end_ihm = self.ihm.utt2segments_[org_utt_ihm]\r\n                #reco_sdm, beg_sdm, end_sdm = self.sdm.utt2segments_[org_utt_sdm]\r\n                #path_ihm = self.ihm.utt2wav_[reco_ihm]\r\n                #path_sdm = self.sdm.utt2wav_[reco_sdm]\r\n                spk2chunks[spk_ihm][\'ihm\'].append(org_utt_ihm)\r\n                spk2chunks[spk_ihm][\'sdm\'].append(org_utt_sdm)\r\n            except KeyError as e:\r\n                print (""Keys {}, {} {} {} {}"".format(e, utt_joint, org_utt_ihm, org_utt_sdm, spk_ihm))\r\n                continue\r\n\r\n\r\n            #print (""Loaded {} {}. {} to {} and {}"".format(spk, utt, start, stop, text))\r\n            #print (""Ihm path is {}"".format(path))\r\n\r\n        return spk2chunks\r\n\r\n    def to_data_cfg(self, spk2chunks):\r\n\r\n        data_cfg = {\'train\':{\'data\':[],\r\n                         \'speakers\':[],\r\n                         \'total_wav_dur\':0},\r\n                    \'valid\':{\'data\':[],\r\n                         \'speakers\':[],\r\n                         \'total_wav_dur\':0},\r\n                    \'test\':{\'data\':[],\r\n                        \'speakers\':[],\r\n                        \'total_wav_dur\':0},\r\n                    \'speakers\':[]}\r\n\r\n        audio_info = {\'ihm\':{}, \'sdm\':{}}\r\n\r\n        valid = \'P42\'\r\n        test = \'P41\'\r\n\r\n        tot_files = 0\r\n        for spk in sorted(spk2chunks.keys()):\r\n            ihm_utts = spk2chunks[spk][\'ihm\']\r\n            sdm_utts = spk2chunks[spk][\'sdm\']\r\n            print (""Processing spk {} with {} segs"".format(spk, len(ihm_utts)))\r\n            for idx, paths in enumerate(zip(ihm_utts, sdm_utts)):\r\n                #print (""Processing idx {} and paths {}"".format(idx, paths))\r\n                org_utt_ihm = paths[0]\r\n                org_utt_sdm = paths[1]\r\n\r\n                reco_ihm, beg_ihm, end_ihm = self.ihm.utt2segments_[org_utt_ihm]\r\n                reco_sdm, beg_sdm, end_sdm = self.sdm.utt2segments_[org_utt_sdm]\r\n\r\n                path_ihm = self.ihm.utt2wav_[reco_ihm]\r\n                path_sdm = self.sdm.utt2wav_[reco_sdm]\r\n\r\n                #out_ihm_file = ""{}_{}-{}.wav"".format(spk, reco_ihm, idx)\r\n                out_ihm_file = ""{}-{}.wav"".format(spk, idx)\r\n                out_ihm_path = os.path.join(self.out_dir, out_ihm_file)\r\n                out_sdm_file = ""{}_{}-{}.wav"".format(spk, reco_sdm, idx)\r\n                #out_sdm_file = ""{}_rdm-{}.wav"".format(spk, idx)\r\n                out_sdm_path = os.path.join(self.out_dir, out_sdm_file)\r\n\r\n                #print (""Proc {} paths {} out_path {}, reco {}"".format(idx, paths, out_ihm_file, reco_ihm))\r\n\r\n                audio_entry_ihm = {\'file_in\':path_ihm,\r\n                                   \'file_out\': out_ihm_path,\r\n                                   \'seg_beg\': beg_ihm,\r\n                                   \'seg_end\': end_ihm}\r\n                audio_entry_sdm = {\'file_in\': path_sdm,\r\n                                   \'file_out\': out_sdm_path,\r\n                                   \'seg_beg\': beg_sdm,\r\n                                   \'seg_end\': end_sdm}\r\n\r\n                if reco_ihm not in audio_info[\'ihm\']:\r\n                    audio_info[\'ihm\'][reco_ihm] = []\r\n                if reco_sdm not in audio_info[\'sdm\']:\r\n                    audio_info[\'sdm\'][reco_sdm] = []\r\n\r\n                audio_info[\'ihm\'][reco_ihm].append((org_utt_ihm, audio_entry_ihm))\r\n                audio_info[\'sdm\'][reco_sdm].append((org_utt_sdm, audio_entry_sdm))\r\n\r\n                dset = \'train\'\r\n                if spk in valid:\r\n                    dset = \'valid\'\r\n                elif spk in test:\r\n                    dset = \'test\'\r\n\r\n                entry={\'filename\': out_ihm_file,\r\n                       \'1\': out_sdm_file,\r\n                       \'spk\': spk}\r\n                data_cfg[dset][\'data\'].append(entry)\r\n                if spk not in data_cfg[dset][\'speakers\']:\r\n                    data_cfg[dset][\'speakers\'].append(spk)\r\n\r\n                data_cfg[dset][\'total_wav_dur\'] += int((end_ihm-beg_ihm)*self.fs)\r\n\r\n                tot_files += 1\r\n\r\n        print (""Data cfg contains {} files in total"".format(tot_files))\r\n        print (""Tot training length is {} hours"".format(data_cfg[\'train\'][\'total_wav_dur\']/self.fs/3600))\r\n\r\n        return data_cfg, audio_info\r\n\r\n    def segment_audio(self, audio_info):\r\n\r\n        #we want to order by recording, so each one gets loaded only once\r\n        pool = multiprocessing.Pool(processes=self.num_workers)\r\n        sessions = [(k,v) for k,v in audio_info[\'ihm\'].items()]\r\n        print (""Processing IHM sessions...."")\r\n        tot_success=0\r\n        for f in tqdm.tqdm(pool.imap(pool_recording_worker, sessions), \r\n                                         total=len(sessions)):\r\n            tot_success += f\r\n            #print (\'F \', f)\r\n        print (""Processed succesfully {} IHM files"".format(tot_success))\r\n\r\n        sessions = [(k,v) for k,v in audio_info[\'sdm\'].items()]\r\n        print (""Processing SDM sessions...."")\r\n        tot_success=0\r\n        for f in tqdm.tqdm(pool.imap(pool_recording_worker, sessions),\r\n                                total=len(sessions)):\r\n            tot_success += f\r\n            #print (\'F \', f)\r\n        print (""Processed succesfully {} SDM files"".format(tot_success))\r\n\r\nif __name__ == ""__main__"":\r\n    #train_worn_u100k=\'/mnt/c/work/repos/pase/data_splits/train_worn_u100k\'\r\n    #d = PasePrep4Chime5(train_worn_u100k)\r\n    #d.show_stats()\r\n    #d.get_segments_per_spk()\r\n    #d.get_worn_u100k_overlap()\r\n\r\n    out_dir=\'/tmp-corpora/chime5segmented\'\r\n    #train_worn=\'/mnt/c/work/repos/pase/data_splits/train_worn_stereo\'\r\n    #train_dist=\'/mnt/c/work/repos/pase/data_splits/train_uall\'\r\n    train_worn=\'/disks/data1/pawel/repos/kaldi/egs/chime5/s5/data/train_worn_stereo\'\r\n    train_dist=\'/disks/data1/pawel/repos/kaldi/egs/chime5/s5/data/train_uall\'\r\n    d = PasePrep4Chime5(out_dir, train_worn, train_dist, num_workers=5)\r\n    d.show_stats()\r\n    #d.get_segments_per_spk()\r\n    spk2chunks = d.get_Us_for_worn_text()\r\n    #if not os.path.exists(\'spk2chunks.npy\'):\r\n    #    spk2chunks = d.get_Us_for_worn_text()\r\n    #    numpy.save(\'spk2chunks.npy\', spk2chunks)\r\n    #else:\r\n    #    spk2chunks = numpy.load(\'spk2chunks.npy\', allow_pickle=True)\r\n\r\n    data_cfg, audio_info  = d.to_data_cfg(spk2chunks)\r\n\r\n    #sess = audio_info[\'sdm\'].items()\r\n    #for s in sess:\r\n    #    sessid, utts = s\r\n    #    for utt in utts:\r\n    #        uttid, e = utt\r\n    #        filein = e[\'file_in\']\r\n    #        fileout = e[\'file_out\']\r\n    #        print (""Uttid {}, sess {}, fileout {}"".format(uttid, sessid, fileout))\r\n\r\n    with open(""chime5_seg_mathched.cfg"", \'w\') as cfg_f:\r\n        cfg_f.write(json.dumps(data_cfg))\r\n\r\n    d.segment_audio(audio_info)\r\n\r\n    #train_u100k=\'/mnt/c/work/repos/pase/data_splits/train_u100k\'\r\n    #d = PasePrep4Chime5(train_u100k)\r\n    #d.show_stats()\r\n\r\n    #train_worn=\'/mnt/c/work/repos/pase/data_splits/train_worn_stereo\'\r\n    #d = PasePrep4Chime5(train_worn)\r\n    #d.show_stats()\r\n'"
data/prep/kaldi_data_dir.py,0,"b'""""""\r\n    2016 Pawel Swietojanski\r\n""""""\r\nimport errno\r\nimport sys\r\nimport argparse\r\nimport logging\r\nimport os\r\nimport re\r\nimport random\r\nimport numpy\r\nimport soundfile as sf\r\nimport multiprocessing\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\n\r\ndef make_directory(directory):\r\n    """"""Creates a directory on disk, first checks whether it exists""""""\r\n    if not os.path.exists(directory):\r\n        try:\r\n            os.makedirs(directory)\r\n        except OSError as error:\r\n            if error.errno != errno.EEXIST:\r\n                raise IOError(\'Could not create a direcotry\' + directory)\r\n\r\ndef execute_shell_cmd():\r\n    pass\r\n\r\n\r\nclass KaldiDataDir(object):\r\n    """"""Reads/writes Kaldi data directories""""""\r\n\r\n    def __init__(self, directory, preload=True):\r\n\r\n        self.directory = directory\r\n\r\n        self.uttids_ = {}\r\n        self.spkids_ = {}\r\n\r\n        self.utt2spk_ = {}\r\n        self.spk2utt_ = {}\r\n        self.utt2gender_ = {}\r\n        self.utt2dur_ = {}\r\n        self.utt2text_ = {}\r\n        self.utt2segments_ = {}\r\n        self.utt2text_ = {}\r\n        self.utt2wav_ = {}\r\n        self.utt2native_ = {}\r\n        self.reco2file_and_channel_ = {}\r\n\r\n        self.utt2reco_ = {}\r\n        # below two fields are not-typical for Kaldi data organisation, but it\r\n        # using our metadata to generate scoring labels for different\r\n        # conditions data was collected in (room, noise-on/noise-off, etc.)\r\n        # utt2cond_ maps an utterance to stm compatible condition ids\r\n        # (space separated)\r\n        self.utt2cond_ = {}\r\n        # cond2desc_ contains details on conditions, for example:\r\n        # ""<ID>"" ""<COL_HD>"" ""<DESC>""\r\n        # ""M"" ""Male"" ""Male Talkers"" (see also to_stm method)\r\n        # ""N1"" ""Noise"" ""Noise on""\r\n        # ""N0"" ""Noise"" ""Noise off""\r\n        self.cond2desc_ = {}\r\n\r\n        if preload:\r\n            self.read_datadir()\r\n\r\n    @property\r\n    def num_spk(self):\r\n        return len(self.spk2utt_)\r\n    \r\n    @property\r\n    def num_utt(self):\r\n        return len(self.utt2spk_)\r\n    \r\n    @property\r\n    def total_duarion(self):\r\n        tot_dur = 0\r\n        for d in self.utt2dur_.values():\r\n            tot_dur += d\r\n        return tot_dur\r\n\r\n    def load(self):\r\n        """"""This function should implement exact backend mapper (CSV, SQL, etc.)\r\n        """"""\r\n        raise NotImplementedError()\r\n\r\n    def ids_constructor(self, row):\r\n        """"""\r\n        Shall implement by specific recipes on how to compose required utt ids\r\n        given some unstructured data (for example, when fetching raw data from\r\n        database/s3)\r\n        """"""\r\n        raise NotImplementedError()\r\n\r\n    def write_datadir(self):\r\n        """"""Writes internal state to Kaldi compatible directory""""""\r\n        try:\r\n            self.__write_utt2spk()\r\n            self.__write_spk2utt()\r\n            self.__write_utt2gender()\r\n            self.__write_utt2cond()\r\n            self.__write_cond2desc()\r\n            self.__write_utt2dur()\r\n            self.__write_segments()\r\n            self.__write_text()\r\n            self.__write_wavscp()\r\n            self.__write_reco2file_and_channel()\r\n        except:\r\n            raise Exception(\'Duming files failed\')\r\n\r\n    def read_datadir(self):\r\n        """"""Writes internal state to Kaldi compatible directory""""""\r\n        \r\n        a = self.__read_utt2spk()\r\n        b = self.__read_wavscp()\r\n        c = self.__read_text()\r\n        d = self.__read_segments()\r\n\r\n        l = [a,b,c,d]\r\n        assert any(l) is not False, (\r\n            ""Expected files utt2spk, wavscp and text exists {}"".format(l)\r\n        )\r\n\r\n        if not self.__read_spk2utt():\r\n            print (""Read utt 2 spk skipped"")\r\n        if not self.__read_utt2gender():\r\n            print (""Read utt 2 gender skipped"")\r\n        if not self.__read_utt2cond():\r\n            print (""Read utt 2 cond skipped"")\r\n        if not self.__read_cond2desc():\r\n            print (""Read cond 2 desc skipped"")\r\n        if not self.__read_utt2dur():\r\n            print (""Read utt2dur skipped"")\r\n        if not self.__read_reco2file_and_channel():\r\n            print (""Reading rec2file_and_channel skipped"")\r\n\r\n    def to_stm(self):\r\n        """"""\r\n        Maps internal representation into stm that shall be used for scoring.\r\n        This will allows us to group different types of data by metadata\r\n        (and thus get a separate WER result for each, on top of aggregate one)\r\n\r\n        See for details:\r\n            http://www1.icsi.berkeley.edu/Speech/docs/sctk-1.2/infmts.htm\r\n\r\n        The format follows like this (excerpt of above desc):\r\n\r\n        STM :== <F> <C> <S> <BT> <ET> [ <LABEL> ] transcript . . .\r\n\r\n        For example:\r\n        ;; LABEL ""<ID>"" ""<COL_HD>"" ""<DESC>""\r\n        ;; LABEL ""M"" ""Male"" ""Male Talkers""\r\n        ;; LABEL ""F"" ""Female"" ""Female Talkers""\r\n        ;; LABEL ""01"" ""Story 1"" ""Business news""\r\n        ;; LABEL ""00"" ""Not in Story"" ""Words or Phrases not contained\r\n                       in a story""\r\n        940328 1 A 4.00 18.10 <O,F,00> FROM LOS ANGELES\r\n        940328 1 B 18.10 25.55 <O,M,01> MEXICO IN TURMOIL\r\n        """"""\r\n        with open(self.directory + ""/stm"", \'w\') as stm:\r\n            stm.write("";; LABEL \\""M\\"" \\""Male\\"" \\""Male Talkers\\""\\n"")\r\n            stm.write("";; LABEL \\""F\\"" \\""Female\\"" \\""Female Talkers\\""\\n"")\r\n            stm.write("";; LABEL \\""L1\\"" \\""Natives\\"" \\""Native speakers\\""\\n"")\r\n            stm.write("";; LABEL \\""L0\\"" \\""Non-natives\\"" ""\r\n                      ""\\""Non native speakers\\""\\n"")\r\n            for key in sorted(self.cond2desc_.keys()):\r\n                stm.write("";; LABEL "" + self.cond2desc_[key] + ""\\n"")\r\n\r\n            for key in sorted(self.uttids_.keys()):\r\n                wav = os.path.basename(self.utt2wav_[key])\r\n                stm.write(wav + "" A "" + self.utt2spk_[key] + "" 0 30.0 <O,"" +\r\n                          self.utt2gender_[key] + "","" +\r\n                          self.utt2native_[key] + "","" +\r\n                          self.utt2cond_[key] + ""> "" +\r\n                          self.utt2text_[key] + ""\\n"")\r\n\r\n    def __check_consistency(self):\r\n        pass\r\n\r\n    def __write_dict(self, fname, wdict):\r\n        if wdict is None or len(wdict.keys()) < 1:\r\n            return\r\n        with open(os.path.join(self.directory, fname), \'w\') as fh:\r\n            for key, val in sorted(wdict.items()):\r\n                fh.write(key + "" "" + val + ""\\n"")\r\n\r\n    def __read_dict(self, fname, wdict):\r\n        try:\r\n            with open(os.path.join(self.directory, fname), \'r\') as fh:\r\n                for idx, line in enumerate(fh):\r\n                    line = line.strip()\r\n                    try:\r\n                        key, val = re.split(\' \', line, maxsplit=1)\r\n                        if key in wdict:\r\n                            logger.warning(""Warning, "" + key + "" existsted and was overriden"")\r\n                        wdict[key] = val.strip()\r\n                    except ValueError as ve:\r\n                        print (""Incorrect line no. {} of {} ({})."".format(idx, fname, line))\r\n                        print ("" Error is \\""{}\\"""".format(ve))\r\n            return True\r\n        except:\r\n            return False\r\n\r\n    def __write_utt2spk(self):\r\n        self.__write_dict(""utt2spk"", self.utt2spk_)\r\n\r\n    def __read_utt2spk(self):\r\n        return self.__read_dict(""utt2spk"", self.utt2spk_)\r\n\r\n    def __write_spk2utt(self):\r\n        self.__write_dict(""spk2utt"", self.spk2utt_)\r\n\r\n    def __read_spk2utt(self):\r\n        spk2utt = {}\r\n        r = self.__read_dict(""spk2utt"", spk2utt)\r\n        if r:\r\n            self.spk2utt_ = {k: v.split("" "") for k,v in spk2utt.items()}\r\n        return r\r\n\r\n    def __write_utt2gender(self):\r\n        self.__write_dict(""utt2gender"", self.utt2gender_)\r\n\r\n    def __read_utt2gender(self):\r\n        return self.__read_dict(""utt2gender"", self.utt2gender_)\r\n\r\n    def __write_utt2cond(self):\r\n        self.__write_dict(""utt2cond"", self.utt2cond_)\r\n\r\n    def __read_utt2cond(self):\r\n        return self.__read_dict(""utt2cond"", self.utt2cond_)\r\n\r\n    def __write_cond2desc(self):\r\n        self.__write_dict(""cond2desc"", self.cond2desc_)\r\n\r\n    def __read_cond2desc(self):\r\n        return self.__read_dict(""cond2desc"", self.cond2desc_)\r\n\r\n    def __write_utt2dur(self):\r\n        self.__write_dict(""utt2dur"", self.utt2dur_)\r\n\r\n    def __read_utt2dur(self):\r\n        utt2dur = {}\r\n        r = self.__read_dict(""utt2dur"", utt2dur)\r\n        if r:\r\n            self.utt2dur_ = {k: float(v) for k,v in utt2dur.items()}\r\n        return r\r\n\r\n    def __write_segments(self):\r\n        self.__write_dict(""segments"", self.utt2segments_)\r\n\r\n    def __read_segments(self):\r\n        def seg2list(s):\r\n            l = s.split("" "")\r\n            assert len(l) == 3, (\r\n                ""Incorrect seg format""\r\n            )\r\n            l[1] = float(l[1])\r\n            l[2] = float(l[2])\r\n            return l\r\n\r\n        utt2seg = {}\r\n        r = self.__read_dict(""segments"", utt2seg)\r\n        if r:\r\n            self.utt2segments_ = {k: seg2list(v) for k,v in utt2seg.items()}\r\n        return r\r\n\r\n    def __write_text(self):\r\n        self.__write_dict(""text"", self.utt2text_)\r\n\r\n    def __read_text(self):\r\n        return self.__read_dict(""text"", self.utt2text_)\r\n\r\n    def __write_utt2native(self):\r\n        self.__write_dict(""utt2native"", self.utt2native_)\r\n\r\n    def __read_utt2native(self):\r\n        return self.__read_dict(""utt2native"", self.utt2native_)\r\n\r\n    def __write_wavscp(self):\r\n        self.__write_dict(""wav.scp"", self.utt2wav_)\r\n\r\n    def __read_wavscp(self):\r\n       return  self.__read_dict(""wav.scp"", self.utt2wav_)\r\n\r\n    def __write_reco2file_and_channel(self):\r\n        self.__write_dict(""reco2file_and_channel"", self.reco2file_and_channel_)\r\n\r\n    def __read_reco2file_and_channel(self):\r\n        return self.__read_dict(""rec2file_and_channel"", self.reco2file_and_channel_)\r\n\r\n\r\ndef kaldi_env(kaldi_root):\r\n    kaldi_root = kaldi_root.strip()\r\n    os.environ[\'KALDI_ROOT\'] = kaldi_root\r\n    os.environ[\'PATH\'] = os.popen(\r\n        \'echo $KALDI_ROOT/src/bin:\'\r\n        \'$KALDI_ROOT/tools/openfst/bin:\'\r\n        \'$KALDI_ROOT/src/fstbin/:\'\r\n        \'$KALDI_ROOT/src/gmmbin/:\'\r\n        \'$KALDI_ROOT/src/featbin/:\'\r\n        \'$KALDI_ROOT/src/lm/:\'\r\n        \'$KALDI_ROOT/src/sgmmbin/:\'\r\n        \'$KALDI_ROOT/src/sgmm2bin/:\'\r\n        \'$KALDI_ROOT/src/fgmmbin/:\'\r\n        \'$KALDI_ROOT/src/latbin/:\'\r\n        \'$KALDI_ROOT/src/nnetbin:\'\r\n        \'$KALDI_ROOT/src/nnet2bin:\'\r\n        \'$KALDI_ROOT/src/nnet3bin:\'\r\n        \'$KALDI_ROOT/src/online2bin/:\'\r\n        \'$KALDI_ROOT/src/ivectorbin/:\'\r\n        \'$KALDI_ROOT/src/lmbin/\').readline().strip() + \':\' + os.environ[\'PATH\']\r\n\r\n\r\n\r\n         \r\n\r\n\r\n\r\n'"
data/prep/prepare_openslr_rirs_cfg.py,0,"b'import json\nimport argparse\nimport random\nfrom random import shuffle\nimport numpy as np\nimport os\nimport re\n\ndef load_filenames(opts):\n    rooms = {\'smallroom\':[], \'mediumroom\':[], \'largeroom\':[]}\n    for room in rooms.keys():\n        rir_list_fn = os.path.join(opts.data_root, room, \'rir_list\')\n        with open(rir_list_fn, \'r\') as fn:\n            for line in fn:\n                rooms[room].append(line.split(\' \')[4].strip())\n    return rooms\n\ndef main(opts):\n\n    if opts.existing_cfg is not None:\n        with open(opts.existing_cfg, \'r\') as ex_f:\n            out = json.load(ex_f)\n            out[\'reverb_data_root\'] = opts.data_root\n            out[\'reverb_fmt\'] = \'wav\'\n            out[\'reverb_irfiles\'] = []\n    else:\n        out = {""reverb_data_root"":opts.data_root, \n               ""reverb_fmt"":""wav"",\n               ""reverb_irfiles"":[]}\n\n    rooms = load_filenames(opts)\n    final_rirs = []\n\n    rirs = rooms[\'smallroom\']\n    if opts.small_room_ratio < 1.0:\n        sel = int(len(rirs)*opts.small_room_ratio)\n        print (\'Found {} in small room. Selecting random {} out of them.\'\\\n                .format(len(rirs), sel))\n        shuffle(rirs)\n        final_rirs.extend(rirs[:sel])\n    else:\n        final_rirs.extend(rirs)\n\n    rirs = rooms[\'mediumroom\']\n    if opts.medium_room_ratio < 1.0:\n        sel = int(len(rirs)*opts.medium_room_ratio)\n        print (\'Found {} in medium room. Selecting random {} out of them.\'\\\n                .format(len(rirs), sel))\n        shuffle(rirs)\n        final_rirs.extend(rirs[:sel])\n    else:\n        final_rirs.extend(rirs)\n\n    rirs = rooms[\'largeroom\']\n    if opts.large_room_ratio < 1.0:\n        sel = int(len(rirs)*opts.large_room_ratio)\n        print (\'Found {} in large room. Selecting random {} out of them.\'\\\n                .format(len(rirs), sel))\n        shuffle(rirs)\n        final_rirs.extend(rirs[:sel])\n    else:\n        final_rirs.extend(rirs)\n\n    print (\'Found total {} rir paths\'.format(len(final_rirs)))\n    out[""reverb_irfiles""].extend(sorted(final_rirs))\n\n    with open(opts.out_file, \'w\') as f:\n        f.write(json.dumps(out, indent=2))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--data_root\', type=str, required=True)\n    parser.add_argument(\'--tot_num_rirs\', type=int, default=-1)\n    parser.add_argument(\'--small_room_ratio\', type=int, default=1.0)\n    parser.add_argument(\'--medium_room_ratio\', type=float, default=1.0)\n    parser.add_argument(\'--large_room_ratio\', type=float, default=1.0)\n    parser.add_argument(\'--convert2npy\', action=\'store_true\', default=False)\n    parser.add_argument(\'--existing_cfg\', type=str, default=None)\n    parser.add_argument(\'--out_file\', type=str, required=True)\n\n    \n    opts = parser.parse_args()\n    main(opts)\n\n'"
data/prep/prepare_segmented_dataset_ami.py,0,"b'\nimport soundfile as sf\nimport sys\nimport tqdm\nimport shutil\nimport os\nimport multiprocessing as mp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\nfrom timeit import default_timer as timer\n\ndef copy_folder(in_folder, out_folder):\n    if not os.path.isdir(out_folder):\n        print(\'Replicating dataset structure...\')\n        beg_t = timer()\n        shutil.copytree(in_folder, out_folder, ignore=ig_f)\n        end_t = timer()\n        print(\'Replicated structure in {:.1f} s\'.format(end_t - beg_t))\n\ndef ig_f(dir, files):\n    return [f for f in files if os.path.isfile(os.path.join(dir, f))]\n\ndef handle_multichannel_wav(wav, channel):\n    suffixes = {0:\'A\', 1:\'B\', 2:\'C\', 3:\'D\'}\n    if channel > 0:\n        assert wav.ndim > 1 and channel < wav.shape[1], (\n            ""Asked to extract {} channel but file has only sinlge channel"".format(channel)\n        )\n    #wav = wav[:, channel]\n    return wav, suffixes[channel]\n\ndef segment_signal(args):\n    data_root, meetpath, wav_file = args\n    wlen = 3200\n    wshift = 80\n    en_th = 0.3\n    smooth_window = 40\n    smooth_th_low = 0.25\n    smooth_th_high = 0.6\n    avoid_sentences_less_that = 24000\n    wav_path = os.path.join(data_root, meetpath, wav_file)\n    #signal, fs = sf.read(data_folder+wav_file)\n    signal, fs = sf.read(wav_path)\n\n    signal, side = handle_multichannel_wav(signal, 0)\n\n    signal = signal / np.max(np.abs(signal))\n    \n    beg_fr=[0]\n    end_fr=[wlen]\n    count_fr=0\n    en_fr=[]\n    \n    while end_fr[count_fr] < signal.shape[0]:\n        #print(beg_fr[count_fr])\n        #print(end_fr[count_fr])\n        signal_seg = signal[beg_fr[count_fr]:end_fr[count_fr]]\n        en_fr.append(np.mean(np.abs(signal_seg) ** 1))\n        beg_fr.append(beg_fr[count_fr]+wshift)\n        end_fr.append(beg_fr[count_fr]+wlen+wshift)\n        count_fr = count_fr + 1\n    \n    en_arr=np.asarray(en_fr)\n    mean_en=np.mean(en_arr)\n    en_bin=(en_arr > mean_en * en_th).astype(int)\n    en_bin_smooth=np.zeros(en_bin.shape)\n    \n    # smooting the window\n    for i in range(count_fr):\n        if i + smooth_window > count_fr - 1:\n            wlen_smooth = count_fr\n        else:\n            wlen_smooth = i + smooth_window\n            \n        en_bin_smooth[i] = np.mean(en_bin[i:wlen_smooth])\n      \n    en_bin_smooth_new = np.zeros(en_bin.shape)\n    \n    vad = False\n    beg_arr_vad=[]\n    end_arr_vad=[]\n    \n    for i in range(count_fr):\n        if vad==False:\n            \n            if en_bin_smooth[i]>smooth_th_high:\n                if i<count_fr-1:\n                    vad=True\n                    en_bin_smooth_new[i]=1\n                    beg_arr_vad.append((beg_fr[i])+wlen)\n            else:\n                en_bin_smooth_new[i]=0                    \n\n        else:\n            if i==count_fr-1:\n                end_arr_vad.append(end_fr[i]) \n                break\n            if en_bin_smooth[i]<smooth_th_low:\n                vad=False\n                en_bin_smooth_new[i]=0\n                end_arr_vad.append((beg_fr[i])+wlen)\n            else:\n                en_bin_smooth_new[i]=1 \n       \n    if len(beg_arr_vad) != len(end_arr_vad):\n        print(\'error\')\n        sys.exit(0)\n\n    # Writing on buffer\n    out_buffer = []\n    count_seg=0\n    for i in range(len(beg_arr_vad)):\n        #count_seg_tot=count_seg_tot+1\n        if end_arr_vad[i] - beg_arr_vad[i] > avoid_sentences_less_that:\n            seg_str = wav_file+ \' \' + str(beg_arr_vad[i]) + \' \' + \\\n                    str(end_arr_vad[i]) + \' \' + str(count_seg) + \'\\n\'\n            out_buffer.append(seg_str)\n            count_seg = count_seg + 1\n        #else:\n        #    count_short = count_short + 1\n    return out_buffer\n\ndef mk_mic_path(meetid, chan, cond=\'ihm\'):\n    assert cond in [\'ihm\', \'sdm\'], (\n        ""For AMI, cond shoud be in ihm or sdm, got {}"".format(cond)\n    )\n    meetpath = ""{}/audio"".format(meetid)\n    if cond == \'ihm\':\n        return meetpath, ""{}.Headset-{}.wav"".format(meetid, chan)\n    return meetpath, ""{}.Array1-0{}.wav"".format(meetid, chan)\n\ndef main(opts):\n    # copy folder structure\n    copy_folder(opts.data_root, opts.out_root)\n\n    headsets = [0, 1, 2, 3] #there is one extra headset in one meeting, but ignore it\n    meetings = []\n    with open(opts.ami_meeting_ids, \'r\') as f:\n        for meetid in f:\n            meetings.append(meetid.strip()) \n\n    assert len(meetings) > 0, (\n        ""Looks like meeting list is empty""\n    )\n\n    sdms = []\n    if len(opts.map_ihm2sdm) > 0:\n        sdms = opts.map_ihm2sdm.split("","")\n        for sdm in sdms:\n            assert sdm in [\'0\', \'1\', \'2\', \'3\', \'4\', \'5\', \'6\', \'7\'], (\n                ""There are only 8 distant mics in AMI (0...7)""\n                ""Pick one of them instead {}"".format(sdm)\n            )\n\n    print (""Preparing AMI for {} meetings,""\n            "" headset plus {} sdms channels"".format(len(meetings), len(sdms)))\n\n    file2spkidx = {}\n\n    for meeting in meetings:\n        print (""Processing meeting {}"".format(meeting))\n        file_out = ""{}/{}.Headset.vad"".format(opts.out_root, meeting)\n        if not os.path.exists(file_out):\n            print(\'VADing signals to build {} list...\'.format(file_out))\n            with open(file_out, \'w\') as f:\n                # Paramters for Voice Activity Detection\n                wav_lst = []\n                for headset in headsets:\n                    meetpath, headset_file = mk_mic_path(meeting, headset, \'ihm\')\n                    wav_lst.append((opts.data_root, meetpath, headset_file))\n                \n\t\t#commented as it does not work well with aws nfs jsalt stuff\n                #pool = mp.Pool(opts.num_workers)\n                #for annotations in tqdm.tqdm(pool.imap(segment_signal, wav_lst), \n                #                             total=len(wav_lst)):\n\t\t# \n                #    for annotation in annotations:\n                #        f.write(annotation)\n                for wav_entry in wav_lst:\n                   annotations = segment_signal(wav_entry)\n                   for annotation in annotations:\n                       f.write(annotation)\n        else:\n            print(\'[!] Found existing {} file, proceeding with it\'.format(file_out))\n    \n        # now read the list back to create the output chunks\n        with open(file_out, \'r\') as f:\n            fnames = [l.rstrip() for l in f]\n            print(\'Producing segments out of VAD list for ihms...\')\n            beg_t = timer()\n            \n            for headset in headsets:\n                meetpath, headset_file = mk_mic_path(meeting, headset, \'ihm\')\n                print (\'Working on {}\'.format(headset_file))\n                signal, fs = sf.read(os.path.join(opts.data_root, meetpath, headset_file))\n                signal, side = handle_multichannel_wav(signal, opts.channel)\n                signal = signal / np.max(np.abs(signal))\n                for li, line in tqdm.tqdm(enumerate(fnames, start=1), total=len(fnames)):\n                    wav_file, beg_samp, end_samp, seg_id = line.split(\' \')\n                    if wav_file != headset_file:\n                        # we have joint vad file for all headsets, so its handier for sdms\n                        continue\n                    segment = signal[int(float(beg_samp)):int(float(end_samp))]\n                    out_wav = wav_file.replace(\'.wav\',  \'-\' + str(seg_id) + \'.wav\')\n                    path_out = os.path.join(opts.out_root, meetpath, out_wav)\n                    #print (\'\\tExporting IHM segment {}\'.format(path_out))\n                    sf.write(path_out, segment, fs)\n                    file2spkidx[out_wav] = wav_file.replace(\'.wav\', \'\')\n\n            if len(sdms) > 0:\n                print(\'Producing segments out of VAD list for sdms...\')\n                for sdm in sdms:\n                    meetpath, sdm_file = mk_mic_path(meeting, sdm, \'sdm\')\n                    path_in = os.path.join(opts.data_root, meetpath, sdm_file)\n\n                    if not os.path.exists(path_in):\n                        print (\'File {} not found. Skipping.\'.format(path_in))\n                        continue\n\n                    signal, fs = sf.read(path_in)\n                    signal = signal / np.max(np.abs(signal))\n\n                    for li, line in tqdm.tqdm(enumerate(fnames, start=1), total=len(fnames)):\n                        wav_file, beg_samp, end_samp, seg_id = line.split(\' \')\n                        segment = signal[int(float(beg_samp)):int(float(end_samp))]\n                        wav_file_basename = wav_file.replace(\'.wav\',\'\')\n                        wav_out = ""{}-{}.Arr1-0{}.wav"".format(wav_file_basename, seg_id, sdm)\n                        path_out = os.path.join(opts.out_root, meetpath, wav_out)\n                        #print (\'\\tExporting SDM segment {}\'.format(path_out))\n                        sf.write(path_out, segment, fs)\n                        file2spkidx[wav_out] = wav_file_basename\n            end_t = timer()\n\n            print(\'Finalized segments production for meeting : \'\n                \'{}\'.format(meeting))\n            print(\'Production time: {:.1f} s\'.format(end_t - beg_t))\n\n    np.save(os.path.join(opts.out_root, opts.utt2spk_dict),\n            file2spkidx, allow_pickle=True)\n\n    print (\'Finished all stuff\')\n           \nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--ami_meeting_ids\', type=str, default=\'ami_split_train.list\')\n    parser.add_argument(\'--data_root\', type=str, default=None)\n    parser.add_argument(\'--num_workers\', type=int, default=4)\n    parser.add_argument(\'--out_root\', type=str, default=None,\n                        help=\'Directory where files will be stored \'\n                             \'(Def: None).\')\n    parser.add_argument(\'--map_ihm2sdm\', type=str, default=""1,3,5,7"",\n                        help=\'Extract VAD segments for these distant channels, on top of close-talk one\')\n    parser.add_argument(\'--utt2spk_dict\', type=str, default=\'utt2spk.npy\')\n    parser.add_argument(\'--channel\', type=int, default=0,\n                        help=""In case of multi channel file, pick this channel"")\n    opts = parser.parse_args()\n    assert opts.num_workers > 0, opts.num_workers\n    if opts.data_root is None:\n        raise ValueError(\'Please specify an input data root (e.g. \'\n                         \'data/LibriSpeech)\')\n    if opts.out_root is None:\n        raise ValueError(\'Please specify an output data root (e.g. \'\n                         \'data/LibriSpeech_seg)\')\n    main(opts)\n\n'"
data/prep/prepare_segmented_dataset_libri.py,0,"b""import soundfile as sf\nimport sys\nimport tqdm\nimport shutil\nimport os\nimport multiprocessing as mp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\nfrom timeit import default_timer as timer\n\ndef copy_folder(in_folder, out_folder):\n    if not os.path.isdir(out_folder):\n        print('Replicating dataset structure...')\n        beg_t = timer()\n        shutil.copytree(in_folder, out_folder, ignore=ig_f)\n        end_t = timer()\n        print('Replicated structure in {:.1f} s'.format(end_t - beg_t))\n  \ndef ig_f(dir, files):\n    return [f for f in files if os.path.isfile(os.path.join(dir, f))]\n\ndef segment_signal(args):\n    data_root, wav_file = args\n    wlen = 3200\n    wshift = 80\n    en_th = 0.3\n    smooth_window = 40\n    smooth_th_low = 0.25\n    smooth_th_high = 0.6\n    avoid_sentences_less_that = 24000\n    wav_path = os.path.join(data_root, wav_file)\n    #signal, fs = sf.read(data_folder+wav_file)\n    signal, fs = sf.read(wav_path)\n    signal = signal / np.max(np.abs(signal))\n    \n    beg_fr=[0]\n    end_fr=[wlen]\n    count_fr=0\n    en_fr=[]\n    \n    while end_fr[count_fr] < signal.shape[0]:\n        #print(beg_fr[count_fr])\n        #print(end_fr[count_fr])\n        signal_seg = signal[beg_fr[count_fr]:end_fr[count_fr]]\n        en_fr.append(np.mean(np.abs(signal_seg) ** 1))\n        beg_fr.append(beg_fr[count_fr]+wshift)\n        end_fr.append(beg_fr[count_fr]+wlen+wshift)\n        count_fr = count_fr + 1\n    \n    en_arr=np.asarray(en_fr)\n    mean_en=np.mean(en_arr)\n    en_bin=(en_arr > mean_en * en_th).astype(int)\n    en_bin_smooth=np.zeros(en_bin.shape)\n    \n    # smooting the window\n    for i in range(count_fr):\n        if i + smooth_window > count_fr - 1:\n            wlen_smooth = count_fr\n        else:\n            wlen_smooth = i + smooth_window\n            \n        en_bin_smooth[i] = np.mean(en_bin[i:wlen_smooth])\n      \n    en_bin_smooth_new = np.zeros(en_bin.shape)\n    \n    vad = False\n    beg_arr_vad=[]\n    end_arr_vad=[]\n    \n    for i in range(count_fr):\n        if vad==False:\n            \n            if en_bin_smooth[i]>smooth_th_high:\n                if i<count_fr-1:\n                    vad=True\n                    en_bin_smooth_new[i]=1\n                    beg_arr_vad.append((beg_fr[i])+wlen)\n            else:\n                en_bin_smooth_new[i]=0                    \n\n        else:\n            if i==count_fr-1:\n                end_arr_vad.append(end_fr[i]) \n                break\n            if en_bin_smooth[i]<smooth_th_low:\n                vad=False\n                en_bin_smooth_new[i]=0\n                end_arr_vad.append((beg_fr[i])+wlen)\n            else:\n                en_bin_smooth_new[i]=1 \n       \n    if len(beg_arr_vad) != len(end_arr_vad):\n        print('error')\n        sys.exit(0)\n\n    # Writing on buffer\n    out_buffer = []\n    count_seg=0\n    for i in range(len(beg_arr_vad)):\n        #count_seg_tot=count_seg_tot+1\n        if end_arr_vad[i] - beg_arr_vad[i] > avoid_sentences_less_that:\n            seg_str = wav_file + ' ' + str(beg_arr_vad[i]) + ' ' + \\\n                    str(end_arr_vad[i]) + ' ' + str(count_seg) + '\\n'\n            out_buffer.append(seg_str)\n            count_seg = count_seg + 1\n        #else:\n        #    count_short = count_short + 1\n    return out_buffer\n\ndef main(opts):\n    data_folder = opts.data_root\n    file_lst = opts.file_list\n    file_out = opts.file_out\n    save_path = opts.out_root\n\n    # copy folder structure\n    copy_folder(opts.data_root, opts.out_root)\n    \n    if not os.path.exists(file_out):\n        print('VADing signals to build {} list...'.format(file_out))\n        pool = mp.Pool(opts.num_workers)\n\n        with open(file_out, 'w') as f:\n            # Paramters for Voice Activity Detection\n\n            # Readline all the files\n            with open(file_lst, 'r') as lst_f:\n                wav_lst = [(data_folder, line.rstrip()) for line in lst_f]\n\n                count=1\n                count_seg_tot=0\n                count_short=0\n\n                wi = 1\n                for annotations in tqdm.tqdm(pool.imap(segment_signal, wav_lst), \n                                             total=len(wav_lst)):\n                    for annotation in annotations:\n                        f.write(annotation)\n    else:\n        print('[!] Found existing {} file, proceeding with it'.format(file_out))\n            \n    # now read the list back to create the output chunks\n    with open(file_out, 'r') as f:\n        fnames = [l.rstrip() for l in f]\n        print('Producing segments out of VAD list...')\n        beg_t = timer()\n        for li, line in tqdm.tqdm(enumerate(fnames, start=1), total=len(fnames)):\n            wav_file, beg_samp, end_samp, seg_id = line.split(' ')\n            signal, fs = sf.read(os.path.join(opts.data_root, wav_file))\n            signal = signal / np.max(np.abs(signal))\n            signal = signal[int(float(beg_samp)):int(float(end_samp))]\n            path_out = os.path.join(opts.out_root, wav_file)\n            path_out = path_out.replace('.flac', '-' + str(seg_id) + '.wav')\n            sf.write(path_out, signal, fs)\n        end_t = timer()\n        print('Finalized segments production to output path: '\n              '{}'.format(opts.out_root))\n        print('Production time: {:.1f} s'.format(end_t - beg_t))\n           \nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_root', type=str, default=None)\n    parser.add_argument('--file_list', type=str, default='data/libri_all_tr.lst')\n    parser.add_argument('--file_out', type=str, default='data/libri_snt_vad.lst')\n    parser.add_argument('--num_workers', type=int, default=5)\n    parser.add_argument('--out_root', type=str, default=None,\n                        help='Directory where files will be stored '\n                             '(Def: None).')\n    opts = parser.parse_args()\n    assert opts.num_workers > 0, opts.num_workers\n    if opts.data_root is None:\n        raise ValueError('Please specify an input data root (e.g. '\n                         'data/LibriSpeech)')\n    if opts.out_root is None:\n        raise ValueError('Please specify an output data root (e.g. '\n                         'data/LibriSpeech_seg)')\n    main(opts)\n\n"""
data/prep/prepare_segmented_dataset_swbd.py,0,"b'import soundfile as sf\nimport sys\nimport tqdm\nimport shutil\nimport os\nimport multiprocessing as mp\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport argparse\nfrom timeit import default_timer as timer\n\ndef copy_folder(in_folder, out_folder):\n    if not os.path.isdir(out_folder):\n        print(\'Replicating dataset structure...\')\n        beg_t = timer()\n        shutil.copytree(in_folder, out_folder, ignore=ig_f)\n        end_t = timer()\n        print(\'Replicated structure in {:.1f} s\'.format(end_t - beg_t))\n  \ndef ig_f(dir, files):\n    return [f for f in files if os.path.isfile(os.path.join(dir, f))]\n\ndef handle_multichannel_wav(wav, channel):\n    suffixes = {0:\'A\', 1:\'B\', 2:\'C\', 3:\'D\'}\n    if channel > 0:\n        assert wav.ndim > 1 and channel < wav.shape[1], (\n            ""Asked to extract {} channel but file has only sinlge channel"".format(channel)\n        )\n    wav = wav[:, channel]\n    return wav, suffixes[channel]\n\ndef segment_signal(args):\n    data_root, wav_file, channel = args\n    wlen = 3200\n    wshift = 80\n    en_th = 0.3\n    smooth_window = 40\n    smooth_th_low = 0.25\n    smooth_th_high = 0.6\n    avoid_sentences_less_that = 24000\n    wav_path = os.path.join(data_root, wav_file)\n    #signal, fs = sf.read(data_folder+wav_file)\n    signal, fs = sf.read(wav_path)\n\n    signal, side = handle_multichannel_wav(signal, channel)\n\n    signal = signal / np.max(np.abs(signal))\n    \n    beg_fr=[0]\n    end_fr=[wlen]\n    count_fr=0\n    en_fr=[]\n    \n    while end_fr[count_fr] < signal.shape[0]:\n        #print(beg_fr[count_fr])\n        #print(end_fr[count_fr])\n        signal_seg = signal[beg_fr[count_fr]:end_fr[count_fr]]\n        en_fr.append(np.mean(np.abs(signal_seg) ** 1))\n        beg_fr.append(beg_fr[count_fr]+wshift)\n        end_fr.append(beg_fr[count_fr]+wlen+wshift)\n        count_fr = count_fr + 1\n    \n    en_arr=np.asarray(en_fr)\n    mean_en=np.mean(en_arr)\n    en_bin=(en_arr > mean_en * en_th).astype(int)\n    en_bin_smooth=np.zeros(en_bin.shape)\n    \n    # smooting the window\n    for i in range(count_fr):\n        if i + smooth_window > count_fr - 1:\n            wlen_smooth = count_fr\n        else:\n            wlen_smooth = i + smooth_window\n            \n        en_bin_smooth[i] = np.mean(en_bin[i:wlen_smooth])\n      \n    en_bin_smooth_new = np.zeros(en_bin.shape)\n    \n    vad = False\n    beg_arr_vad=[]\n    end_arr_vad=[]\n    \n    for i in range(count_fr):\n        if vad==False:\n            \n            if en_bin_smooth[i]>smooth_th_high:\n                if i<count_fr-1:\n                    vad=True\n                    en_bin_smooth_new[i]=1\n                    beg_arr_vad.append((beg_fr[i])+wlen)\n            else:\n                en_bin_smooth_new[i]=0                    \n\n        else:\n            if i==count_fr-1:\n                end_arr_vad.append(end_fr[i]) \n                break\n            if en_bin_smooth[i]<smooth_th_low:\n                vad=False\n                en_bin_smooth_new[i]=0\n                end_arr_vad.append((beg_fr[i])+wlen)\n            else:\n                en_bin_smooth_new[i]=1 \n       \n    if len(beg_arr_vad) != len(end_arr_vad):\n        print(\'error\')\n        sys.exit(0)\n\n    # Writing on buffer\n    out_buffer = []\n    count_seg=0\n    for i in range(len(beg_arr_vad)):\n        #count_seg_tot=count_seg_tot+1\n        if end_arr_vad[i] - beg_arr_vad[i] > avoid_sentences_less_that:\n            seg_str = wav_file+ \' \' + str(beg_arr_vad[i]) + \' \' + \\\n                    str(end_arr_vad[i]) + \' \' + str(count_seg) + \'\\n\'\n            out_buffer.append(seg_str)\n            count_seg = count_seg + 1\n        #else:\n        #    count_short = count_short + 1\n    return out_buffer\n\ndef main(opts):\n    data_folder = opts.data_root\n    file_lst = opts.file_list\n    file_out = opts.file_out\n    save_path = opts.out_root\n\n    # copy folder structure\n    copy_folder(opts.data_root, opts.out_root)\n    \n    if not os.path.exists(file_out):\n        print(\'VADing signals to build {} list...\'.format(file_out))\n        pool = mp.Pool(opts.num_workers)\n\n        with open(file_out, \'w\') as f:\n            # Paramters for Voice Activity Detection\n\n            # Readline all the files\n            with open(file_lst, \'r\') as lst_f:\n                wav_lst = [(data_folder, line.rstrip(), opts.channel) for line in lst_f]\n\n                count=1\n                count_seg_tot=0\n                count_short=0\n\n                wi = 1\n                for annotations in tqdm.tqdm(pool.imap(segment_signal, wav_lst), \n                                             total=len(wav_lst)):\n                    for annotation in annotations:\n                        f.write(annotation)\n    else:\n        print(\'[!] Found existing {} file, proceeding with it\'.format(file_out))\n    \n    # now read the list back to create the output chunks\n    with open(file_out, \'r\') as f:\n        fnames = [l.rstrip() for l in f]\n        print(\'Producing segments out of VAD list...\')\n        beg_t = timer()\n        for li, line in tqdm.tqdm(enumerate(fnames, start=1), total=len(fnames)):\n            wav_file, beg_samp, end_samp, seg_id = line.split(\' \')\n            signal, fs = sf.read(os.path.join(opts.data_root, wav_file))\n            signal, side = handle_multichannel_wav(signal, opts.channel)\n            signal = signal / np.max(np.abs(signal))\n            signal = signal[int(float(beg_samp)):int(float(end_samp))]\n            path_out = os.path.join(opts.out_root, wav_file)\n            path_out = path_out.replace(\'.sph\', \'-\' + str(side) + \'-\' + str(seg_id) + \'.wav\')\n            sf.write(path_out, signal, fs)\n        end_t = timer()\n        print(\'Finalized segments production to output path: \'\n              \'{}\'.format(opts.out_root))\n        print(\'Production time: {:.1f} s\'.format(end_t - beg_t))\n           \nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_root\', type=str, default=None)\n    parser.add_argument(\'--file_list\', type=str, default=\'data/libri_all_tr.lst\')\n    parser.add_argument(\'--file_out\', type=str, default=\'data/libri_snt_vad.lst\')\n    parser.add_argument(\'--num_workers\', type=int, default=5)\n    parser.add_argument(\'--out_root\', type=str, default=None,\n                        help=\'Directory where files will be stored \'\n                             \'(Def: None).\')\n    parser.add_argument(\'--resample_to\', type=int, default=None,\n                        help=\'If original waveform is different than this, up/down sample to this value\')\n    parser.add_argument(\'--channel\', type=int, default=0,\n                        help=""In case of multi channel file, pick this channel"")\n    opts = parser.parse_args()\n    assert opts.num_workers > 0, opts.num_workers\n    if opts.data_root is None:\n        raise ValueError(\'Please specify an input data root (e.g. \'\n                         \'data/LibriSpeech)\')\n    if opts.out_root is None:\n        raise ValueError(\'Please specify an output data root (e.g. \'\n                         \'data/LibriSpeech_seg)\')\n    main(opts)\n\n'"
data/prep/unsupervised_data_cfg_ami.py,0,"b'import json\nimport librosa\nimport argparse\nimport random\nfrom random import shuffle\nimport numpy as np\nimport os\nimport re\n\ndef get_file_dur(fname):\n    x, rate = librosa.load(fname, sr=None)\n    return len(x)\n\ndef parse_list(file_in, opts):\n\n    def utt2spk_fun(path):\n        bsn = os.path.basename(path)\n        match = re.match(r\'(.*Headset\\-\\d).*\', bsn)\n        spk = None\n        if match:\n            spk = match.group(1)\n        return bsn, spk\n\n    def sdm2ihm_and_chan_fun(path):\n        bsn = os.path.basename(path)\n        match = re.match(r\'(.*Headset\\-\\d\\-[\\d)]*)(\\.Arr1-0)(\\d).*\', bsn)\n        ihm, chan, sdm = None, None, None\n        if match:\n            ihm = match.group(1) + \'.wav\'\n            chan = match.group(3)\n            sdm = match.group(1)+match.group(2)+match.group(3) + \'.wav\'\n            return ihm, sdm, chan\n        else:\n            return None     \n         \n    entries = []\n    with open(file_in) as scps:\n        entries = [scp.strip() for scp in scps]\n\n        #first, get headsets only, those will be used for spkids\n        ihms = list(filter(lambda x: re.search(r\'.*Headset\\-\\d\\-(\\d)*\\.wav\', x), entries))\n        ihm_utt2spk = dict(list(map(utt2spk_fun, ihms)))\n        ihm2sdms = {k:{} for k in ihm_utt2spk.keys()}\n    \n        if len(opts.map_ihm2sdm):\n            chans = opts.map_ihm2sdm.split("","")\n            sdms = list(filter(lambda x: re.search(r\'.*Arr.*\', x), entries))\n            for sdm_path in sdms:\n                ihm, sdm, chan = sdm2ihm_and_chan_fun(sdm_path)\n                if chan not in chans:\n                    print (\'Chan {} not in expected chans {}. Skipping\'.format(chan, chans))\n                    continue\n                # pick only distant segments with the corresponding entry in ihm\n                if ihm in ihm2sdms: \n                    #print (\'Adding {} to {},{}, as {}\'.format(sdm, ihm, chan, ihm in ihm2sdms))\n                    ihm2sdms[ihm][chan] = sdm\n                else:\n                    print (\'Ihm {} extracted from sdm {} not found in the ihm list\'.format(ihm, sdm))\n            \n            if len(ihm2sdms[ihm].keys()) != len(chans):\n                print (\'Removed {} utt as the corresponding sdms channels not found\'.format(ihm))\n                print (\'There are two AMI meetings that are missing distant channels\')\n                ihm2sdms.pop(ihm, None)\n                ihm_utt2spk.pop(ihm, None)\n\n        return ihm_utt2spk, ihm2sdms\n    return None, None\n\ndef mk_ami_path(utt):\n    bsn = os.path.basename(utt)\n    match = re.match(r\'(.*)\\.Headset.*\', bsn)\n    meetid = None\n    if match is not None:\n        meetid = match.group(1)\n    assert (meetid is not None), (\n        ""Cant extract meeting id from {}. Is this AMI corpus?"".format(utt)\n    )\n    return ""{}/audio/{}"".format(meetid, bsn)\n\ndef main(opts):\n    random.seed(opts.seed)\n\n    utt2spk, ihm2sdms = parse_list(opts.train_scp, opts)\n    utt2spk_test, ihm2sdms_test = parse_list(opts.test_scp, opts)\n\n    assert utt2spk is not None and ihm2sdms is not None, (\n        ""Looks like parsing of {} did not suceed"".format(opts.train_scp)\n    )\n\n    assert utt2spk_test is not None and ihm2sdms_test is not None, (\n        ""Looks like parsing of {} did not suceed"".format(opts.test_scp)\n    )\n\n    data_cfg = {\'train\':{\'data\':[],\n                         \'speakers\':[]},\n                \'valid\':{\'data\':[],\n                         \'speakers\':[]},\n                \'test\':{\'data\':[],\n                        \'speakers\':[]},\n                \'speakers\':[]}\n\n    # get train / valid keys split\n    keys = list(utt2spk.keys())\n    shuffle(keys)\n    N_valid_files = int(len(keys) * opts.val_ratio)\n    valid_keys = keys[:N_valid_files]\n    train_keys = keys[N_valid_files:]\n\n    train_dur = 0\n    for idx, ihm_utt in enumerate(train_keys, start=1):\n\n        print(\'Processing train file {:7d}/{:7d}\'.format(idx, len(train_keys)),\n               end=\'\\r\')\n        \n        spk = utt2spk[ihm_utt]\n        if spk not in data_cfg[\'speakers\']:\n            data_cfg[\'speakers\'].append(spk)\n            data_cfg[\'train\'][\'speakers\'].append(spk)\n\n        sdm_utts = ihm2sdms[ihm_utt]\n        entry = {\'filename\':mk_ami_path(ihm_utt), \'spk\':spk}\n        for chan, sdm_utt in sdm_utts.items():\n            entry[chan] = mk_ami_path(sdm_utt)\n        data_cfg[\'train\'][\'data\'].append(entry)\n\n        train_dur += get_file_dur(os.path.join(opts.data_root,\n                                                   mk_ami_path(ihm_utt)))\n    data_cfg[\'train\'][\'total_wav_dur\'] = train_dur\n    print()\n\n    valid_dur = 0\n    for idx, ihm_utt in enumerate(valid_keys, start=1):\n\n        print(\'Processing valid file {:7d}/{:7d}\'.format(idx, len(valid_keys)),\n               end=\'\\r\')\n        \n        spk = utt2spk[ihm_utt]\n        if spk not in data_cfg[\'speakers\']:\n            data_cfg[\'speakers\'].append(spk)\n            data_cfg[\'valid\'][\'speakers\'].append(spk)\n\n        sdm_utts = ihm2sdms[ihm_utt]\n        entry = {\'filename\':mk_ami_path(ihm_utt), \'spk\':spk}\n        for chan, sdm_utt in sdm_utts.items():\n            entry[chan] = mk_ami_path(sdm_utt)\n        data_cfg[\'valid\'][\'data\'].append(entry)\n        \n        valid_dur += get_file_dur(os.path.join(opts.data_root,\n                                                   mk_ami_path(ihm_utt)))\n    data_cfg[\'valid\'][\'total_wav_dur\'] = valid_dur\n    print()\n\n    test_dur = 0\n    test_keys = utt2spk_test.keys()\n    for idx, ihm_utt in enumerate(test_keys, start=1):\n\n        print(\'Processing test file {:7d}/{:7d}\'.format(idx, len(test_keys)),\n               end=\'\\r\')\n        \n        spk = utt2spk_test[ihm_utt]\n        if spk not in data_cfg[\'speakers\']:\n            data_cfg[\'speakers\'].append(spk)\n            data_cfg[\'test\'][\'speakers\'].append(spk)\n\n        sdm_utts = ihm2sdms_test[ihm_utt]\n        entry = {\'filename\':mk_ami_path(ihm_utt), \'spk\':spk}\n        for chan, sdm_utt in sdm_utts.items():\n            entry[chan] = mk_ami_path(sdm_utt)\n        data_cfg[\'test\'][\'data\'].append(entry)\n        \n        test_dur += get_file_dur(os.path.join(opts.data_root,\n                                                   mk_ami_path(ihm_utt)))\n    data_cfg[\'test\'][\'total_wav_dur\'] = test_dur\n    print()\n\n    with open(opts.cfg_file, \'w\') as cfg_f:\n        cfg_f.write(json.dumps(data_cfg))\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--map_ihm2sdm\', type=str, default=""1,3,5,7"",\n                        help=\'Extract VAD segments for these distant channels, on top of close-talk one\')\n    parser.add_argument(\'--data_root\', type=str, \n                        default=\'data/LibriSpeech/Librispeech_spkid_sel\')\n    parser.add_argument(\'--train_scp\', type=str, default=None)\n    parser.add_argument(\'--test_scp\', type=str, default=None)\n    parser.add_argument(\'--val_ratio\', type=float, default=0.1,\n                        help=\'Validation ratio to take out of training \'\n                             \'in utterances ratio (Def: 0.1).\')\n    parser.add_argument(\'--cfg_file\', type=str, default=\'data/librispeech_data.cfg\')\n    parser.add_argument(\'--seed\', type=int, default=3)\n    \n    opts = parser.parse_args()\n    main(opts)\n\n'"
data/prep/unsupervised_data_cfg_librispeech.py,0,"b""import json\nimport librosa\nimport argparse\nimport random\nfrom random import shuffle\nimport numpy as np\nimport os\n\ndef get_file_dur(fname):\n    x, rate = librosa.load(fname, sr=None)\n    return len(x)\n\ndef main(opts):\n    random.seed(opts.seed)\n    spk2idx = np.load(opts.libri_dict, allow_pickle=True)\n    spk2idx = dict(spk2idx.any())\n    data_cfg = {'train':{'data':[],\n                         'speakers':[]},\n                'valid':{'data':[],\n                         'speakers':[]},\n                'test':{'data':[],\n                        'speakers':[]},\n                'speakers':[]}\n    with open(opts.train_scp, 'r') as train_f:\n        train_files = [l.rstrip() for l in train_f]\n        shuffle(train_files)\n        N_valid_files = int(len(train_files) * opts.val_ratio)\n        valid_files = train_files[:N_valid_files]\n        train_files = train_files[N_valid_files:]\n        train_dur = 0\n        for ti, train_file in enumerate(train_files, start=1):\n            print('Processing train file {:7d}/{:7d}'.format(ti,\n                                                             len(train_files)),\n                  end='\\r')\n            spk = spk2idx[train_file]\n            if spk not in data_cfg['speakers']:\n                data_cfg['speakers'].append(spk)\n                data_cfg['train']['speakers'].append(spk)\n            data_cfg['train']['data'].append({'filename':train_file,\n                                              'spk':spk})\n            train_dur += get_file_dur(os.path.join(opts.data_root,\n                                                   train_file))\n        data_cfg['train']['total_wav_dur'] = train_dur\n        print()\n\n        valid_dur = 0\n        for ti, valid_file in enumerate(valid_files, start=1):\n            print('Processing valid file {:7d}/{:7d}'.format(ti,\n                                                             len(valid_files)),\n                  end='\\r')\n            spk = spk2idx[valid_file]\n            if spk not in data_cfg['speakers']:\n                data_cfg['speakers'].append(spk)\n                data_cfg['valid']['speakers'].append(spk)\n            data_cfg['valid']['data'].append({'filename':valid_file,\n                                              'spk':spk})\n            valid_dur += get_file_dur(os.path.join(opts.data_root,\n                                                   valid_file))\n        data_cfg['valid']['total_wav_dur'] = valid_dur\n        print()\n\n    with open(opts.test_scp, 'r') as test_f:\n        test_files = [l.rstrip() for l in test_f]\n        test_dur = 0\n        for ti, test_file in enumerate(test_files, start=1):\n            print('Processing test file {:7d}/{:7d}'.format(ti,\n                                                            len(test_files)),\n                  end='\\r')\n            spk = spk2idx[test_file]\n            if spk not in data_cfg['speakers']:\n                data_cfg['speakers'].append(spk)\n                data_cfg['test']['speakers'].append(spk)\n            data_cfg['test']['data'].append({'filename':test_file,\n                                              'spk':spk})\n            test_dur += get_file_dur(os.path.join(opts.data_root,\n                                                  test_file))\n        data_cfg['test']['total_wav_dur'] = test_dur\n    print()\n\n    with open(opts.cfg_file, 'w') as cfg_f:\n        cfg_f.write(json.dumps(data_cfg))\n\n\n            \n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_root', type=str, \n                        default='data/LibriSpeech/Librispeech_spkid_sel')\n    parser.add_argument('--train_scp', type=str, default=None)\n    parser.add_argument('--test_scp', type=str, default=None)\n    parser.add_argument('--val_ratio', type=float, default=0.1,\n                        help='Validation ratio to take out of training '\n                             'in utterances ratio (Def: 0.1).')\n    parser.add_argument('--cfg_file', type=str, default='data/librispeech_data.cfg')\n    parser.add_argument('--libri_dict', type=str,\n                        default='data/LibriSpeech/libri_dict.npy')\n    parser.add_argument('--seed', type=int, default=3)\n    \n    opts = parser.parse_args()\n    main(opts)\n\n"""
data/prep/unsupervised_data_cfg_vctk.py,0,"b""import json\nimport glob\nimport os\nimport argparse\nimport re\nimport numpy as np\nimport librosa\nimport timeit\n\n\ndef main(opts):\n    data_root = opts.data_root\n    if data_root is None:\n        raise ValueError('Please specify a data_root directory where '\n                         'VCTK is located, containing its wav/wav16 '\n                         'sub-dir and speaker-info.txt')\n    cfg_file = opts.cfg_file\n    prog = re.compile('\\s+')\n    spk_info_fname = os.path.join(data_root, 'speaker-info.txt')\n    header = []\n    idx2head = {}\n    # store spks info dictionary\n    spks = {}\n    with open(spk_info_fname, 'r') as spk_info_f:\n        for li, line in enumerate(spk_info_f, start=1):\n            content = prog.split(line.rstrip())\n            if li == 1:\n                header = [h for h in content]\n            else:\n                if len(content) > len(header):\n                    # merge last elements for they are\n                    # many-word regions\n                    content = content[:len(header) - 1] + \\\n                        ['_'.join(content[len(header) - 1:])]\n                elif len(content) < len(header):\n                    content += ['UNK']\n                assert len(content) == len(header), print(content)\n                spks[content[0]] = dict((k, v) for k, v in zip(header[1:],\n                                                               content[1:]))\n    # We have the speakers cfg section, now let's build the data split by spks\n    spk_ids = list(spks.keys())\n    N = len(spk_ids)\n    train_N = int(np.floor(opts.train_split * N))\n    valid_N = int(np.floor(opts.valid_split * N))\n    test_split = 1. - (opts.train_split + opts.valid_split)\n    test_N = int(np.ceil(test_split * N))\n    print('Speakers splits')\n    print('-' * 30)\n    print('train_N: {}, valid_N: {}, test_N: {}'.format(train_N, valid_N,\n                                                        test_N))\n    data_cfg = {'train':{'data':[],\n                         'speakers':[]},\n                'valid':{'data':[],\n                         'speakers':[]},\n                'test':{'data':[],\n                        'speakers':[]},\n                'speakers':spks}\n\n    if os.path.exists(os.path.join(data_root, 'wav16')):\n        WAV_DIR = 'wav16'\n    else:\n        # By default point to 48KHz wavs\n        print('WARNING: Using 48KHz wavs as no \\'wav16\\' dir was found!')\n        WAV_DIR = 'wav48'\n\n    splits = ['train', 'valid', 'test']\n    splits_N = [train_N, valid_N, test_N]\n\n    spk_utts = dict((k, {}) for k in splits)\n    # if command line specification is zero, it is equivalent to infinite\n    spk_max_utts = {'train':np.inf if opts.max_train_utts_spk == 0 else opts.max_train_utts_spk,\n                    'valid':np.inf if opts.max_valid_utts_spk == 0 else opts.max_valid_utts_spk,\n                    'test':np.inf if opts.max_test_utts_spk == 0 else opts.max_test_utts_spk\n                   }\n\n    # 1) Train split, 2) Valid split, 3) Test split\n    split_pointer = 0\n    for si, (split, split_N) in enumerate(zip(splits, splits_N), start=1):\n        TRAIN = (split == 'train')\n        split_spks = spk_ids[split_pointer:split_pointer + split_N]\n        total_wav_dur = 0\n\n        timings = []\n        beg_t = timeit.default_timer()\n        for spk_i, spk_ in enumerate(split_spks, start=1):\n            wavs = glob.glob(os.path.join(data_root, WAV_DIR, \n                                          'p' + spk_, '*.wav'))\n            # Start utterance counter per spk\n            if spk_ not in spk_utts[split]:\n                spk_utts[split][spk_] = 0\n            for wi, wav in enumerate(wavs):\n                if spk_utts[split][spk_] >= spk_max_utts[split]:\n                    # Skip utterance if this speaker already has maxed out\n                    #print('Speaker {} already maxed out (MAX {}): '\n                    #      '{}'.format(spk_, spk_max_utts, spk_utts[spk_]))\n                    continue\n                # Just count a new utterance in\n                spk_utts[split][spk_] += 1\n                x, rate = librosa.load(wav, sr=None)\n                if x.shape[0] < opts.min_len:\n                    # Ignore  too short seqs\n                    print('Ignoring wav {} for len is {} < {}'.format(wav,\n                                                                      x.shape[0],\n                                                                      opts.min_len))\n                    continue\n                total_wav_dur += x.shape[0]\n                bname = os.path.basename(wav)\n                data_cfg[split]['data'].append(\n                    {'filename':os.path.join(WAV_DIR,\n                                             'p' + spk_,\n                                             bname),\n                     'spk':spk_}\n                )\n                if spk_ not in data_cfg[split]['speakers']:\n                    data_cfg[split]['speakers'].append(spk_)\n            end_t = timeit.default_timer()\n            timings.append(end_t - beg_t)\n            beg_t = timeit.default_timer()\n            print('{}/{} processed spks for split {}/{} ({}) '\n                  'mbtime: {:.3f} s'.format(spk_i, len(split_spks),\n                                            si, len(splits),\n                                            split, np.mean(timings)),\n                 end='\\r')\n        print('')\n        # write total wav dur\n        data_cfg[split]['total_wav_dur'] = total_wav_dur\n        split_pointer += split_N\n\n\n    # Write final config file onto specified output path\n    with open(cfg_file, 'w') as cfg_f:\n        cfg_f.write(json.dumps(data_cfg))\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('data_root', type=str, default=None)\n    parser.add_argument('--cfg_file', type=str, default='vctk_data.cfg')\n    parser.add_argument('--train_split', type=float, default=0.88)\n    parser.add_argument('--valid_split', type=float, default=0.06)\n    parser.add_argument('--min_len', type=int, default=16000)\n    parser.add_argument('--max_train_utts_spk', type=int, default=0,\n                        help='Maximum training utterances per spk. '\n                             'This allows to make smaller trainset '\n                             'to experiment with N utts per spk. '\n                             'If it is zero, it takes all utts '\n                             '(Def: 0).')\n    parser.add_argument('--max_valid_utts_spk', type=int, default=0,\n                        help='Maximum validation utterances per spk. '\n                             'This allows to make smaller valset '\n                             'to experiment with N utts per spk. '\n                             'If it is zero, it takes all utts '\n                             '(Def: 0).')\n\n    parser.add_argument('--max_test_utts_spk', type=int, default=0,\n                        help='Maximum test utterances per spk. '\n                             'This allows to make smaller testset '\n                             'to experiment with N utts per spk. '\n                             'If it is zero, it takes all utts '\n                             '(Def: 0).')\n\n    opts = parser.parse_args()\n\n    main(opts)\n"""
pase/models/__init__.py,0,b''
pase/models/aspp.py,8,"b'import math\nimport torch\nimport torch.nn as nn\nfrom .modules import *\nimport torch.nn.functional as F\n\n\nclass _ASPPModule(Model):\n    def __init__(self, inplanes, planes, kernel_size, padding, dilation):\n        super(_ASPPModule, self).__init__()\n        self.atrous_conv = nn.Conv1d(inplanes, planes, kernel_size=kernel_size,\n                                            stride=1, padding=padding, dilation=dilation, bias=False)\n        self.bn = nn.BatchNorm1d(planes)\n        self.relu = nn.ReLU()\n\n        self._init_weight()\n\n    def forward(self, x):\n        x = self.atrous_conv(x)\n        x = self.bn(x)\n\n        return self.relu(x)\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm1d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\nclass _ASPPModule2d(Model):\n    def __init__(self, inplanes, planes, kernel_size, padding, dilation):\n        super(_ASPPModule2d, self).__init__()\n        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n                                            stride=1, padding=padding, dilation=dilation, bias=False)\n        self.bn = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU()\n\n        self._init_weight()\n\n    def forward(self, x):\n        x = self.atrous_conv(x)\n        x = self.bn(x)\n\n        return self.relu(x)\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\nclass ASPP(Model):\n    def __init__(self, inplanes, emb_dim, dilations=[1, 6, 12, 18], fmaps=48, dense=False):\n        super(ASPP, self).__init__()\n\n        if not dense:\n\n\n\n            self.aspp1 = _ASPPModule(inplanes, fmaps, 1, padding=0, dilation=dilations[0])\n            self.aspp2 = _ASPPModule(inplanes, fmaps, 3, padding=dilations[1], dilation=dilations[1])\n            self.aspp3 = _ASPPModule(inplanes, fmaps, 3, padding=dilations[2], dilation=dilations[2])\n            self.aspp4 = _ASPPModule(inplanes, fmaps, 3, padding=dilations[3], dilation=dilations[3])\n\n            self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool1d((1)),\n                                                     nn.Conv1d(inplanes, fmaps, 1, stride=1, bias=False),\n                                                     nn.BatchNorm1d(fmaps),\n                                                     nn.ReLU())\n\n        else:\n\n            self.aspp1 = _ASPPModule(inplanes, fmaps, dilations[0], padding=0, dilation=1)\n            self.aspp2 = _ASPPModule(inplanes, fmaps, dilations[1], padding=dilations[1]//2, dilation=1)\n            self.aspp3 = _ASPPModule(inplanes, fmaps, dilations[2], padding=dilations[2]//2, dilation=1)\n            self.aspp4 = _ASPPModule(inplanes, fmaps, dilations[3], padding=dilations[3]//2, dilation=1)\n\n            self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool1d((1)),\n                                                 nn.Conv1d(inplanes, fmaps, 1, stride=1, bias=False),\n                                                 nn.BatchNorm1d(fmaps),\n                                                 nn.ReLU())\n\n        self.conv1 = nn.Conv1d(fmaps * 5, emb_dim, 1, bias=False)\n        self.bn1 = nn.BatchNorm1d(emb_dim)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self._init_weight()\n\n    def forward(self, x):\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.global_avg_pool(x)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode=\'linear\', align_corners=True)\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        return self.dropout(x)\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm1d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\nclass ASPP2d(Model):\n    def __init__(self, inplanes, emb_dim, dilations=[1, 6, 12, 18], fmaps=48, dense=False):\n        super(ASPP2d, self).__init__()\n\n        if not dense:\n\n\n\n            self.aspp1 = _ASPPModule2d(inplanes, fmaps, 1, padding=0, dilation=dilations[0])\n            self.aspp2 = _ASPPModule2d(inplanes, fmaps, 3, padding=dilations[1], dilation=dilations[1])\n            self.aspp3 = _ASPPModule2d(inplanes, fmaps, 3, padding=dilations[2], dilation=dilations[2])\n            self.aspp4 = _ASPPModule2d(inplanes, fmaps, 3, padding=dilations[3], dilation=dilations[3])\n\n            self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n                                                     nn.Conv2d(inplanes, fmaps, 1, stride=1, bias=False),\n                                                     nn.BatchNorm2d(fmaps),\n                                                     nn.ReLU())\n\n\n\n        self.conv1 = nn.Conv2d(fmaps * 5, 1, 1, bias=False)\n        self.bn1 = nn.BatchNorm2d(1)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self._init_weight()\n\n    def forward(self, x):\n\n        x = x.unsqueeze(1)\n\n        x1 = self.aspp1(x)\n        x2 = self.aspp2(x)\n        x3 = self.aspp3(x)\n        x4 = self.aspp4(x)\n        x5 = self.global_avg_pool(x)\n        x5 = F.interpolate(x5, size=x4.size()[2:], mode=\'bilinear\', align_corners=True)\n        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.dropout(x).squeeze(1)\n        return x\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                # m.weight.data.normal_(0, math.sqrt(2. / n))\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\nclass aspp_resblock(Model):\n\n    def __init__(self, in_channel, out_channel, kernel_size, stride, dilations, fmaps, pool2d=False, dense=False):\n\n        super().__init__(name=""aspp_resblock"")\n\n        padding = kernel_size // 2\n\n        if pool2d:\n            self.block1 = nn.Sequential(ASPP2d(1, out_channel, dilations, fmaps, dense),\n                                        nn.Conv1d(in_channel, out_channel, kernel_size=kernel_size, stride=stride,\n                                                  padding=padding, bias=False),\n                                        nn.BatchNorm1d(out_channel),\n                                        nn.ReLU(out_channel))\n\n            self.block2 = nn.Sequential(ASPP2d(1, out_channel, dilations, fmaps, dense),\n                                        nn.Conv1d(out_channel, out_channel, kernel_size=kernel_size, stride=1,\n                                                  padding=padding, bias=False),\n                                        nn.BatchNorm1d(out_channel),\n                                        nn.ReLU(out_channel))\n\n        else:\n            self.block1 = nn.Sequential(ASPP(in_channel, out_channel, dilations, fmaps, dense),\n                                        nn.Conv1d(out_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=padding, bias=False),\n                                        nn.BatchNorm1d(out_channel),\n                                        nn.ReLU(out_channel))\n\n            self.block2 = nn.Sequential(ASPP(out_channel, out_channel, dilations, fmaps, dense),\n                                        nn.Conv1d(out_channel, out_channel, kernel_size=kernel_size, stride=1, padding=padding, bias=False),\n                                        nn.BatchNorm1d(out_channel),\n                                        nn.ReLU(out_channel))\n\n        self._init_weight()\n\n    def forward(self, x):\n\n        out_1 = self.block1(x)\n        out_2 = self.block2(out_1)\n\n        y = out_1 + out_2\n\n        return y\n\n\n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d):\n                n = m.kernel_size[0] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm1d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n\n\n\n\n'"
pase/models/attention_block.py,5,"b'from .modules import *\nfrom .neural_networks import MLP\nimport torch\nimport torch.nn.functional as F\n\nclass attention_block(Model):\n\n    def __init__(self, emb_dim, name, options, K, strides, chunksize, avg_factor=0, mode=""concat""):\n        super().__init__(name=name)\n        \n        self.name = name\n        self.mode = mode\n        self.emb_dim = emb_dim\n        self.avg_factor = avg_factor\n        nn_input = self.cal_nn_input_dim(strides, chunksize)\n\n        self.mlp = MLP(options=options, inp_dim= nn_input)\n        self.K = K\n        self.avg_init=True\n    \n\n    def forward(self, hidden, device):\n        batch_size = hidden.shape[0]\n        feature_length = hidden.shape[2]\n        hidden = hidden.contiguous()\n    \n        if self.mode == ""concat"":\n            hidden_att = hidden.view(hidden.shape[0], self.emb_dim * feature_length)\n        if self.mode == ""avg_time"":\n            hidden_att = hidden.mean(-1)\n        if self.mode == ""avg_time_batch"":\n            hidden_att = hidden.mean(-1).mean(0).unsqueeze(0)\n\n        distribution = self.mlp(hidden_att)\n\n        if self.avg_init:\n            self.running_dist = self.init_running_avg(batch_size).to(device).detach()\n            self.avg_init = False\n\n        self.running_dist = self.running_dist.detach() * self.avg_factor + distribution * (1 - self.avg_factor)\n        \n\n        distribution = self.running_dist\n        \n        # distribution = torch.sum(distribution, dim=1)\n        _, indices = torch.topk(distribution, dim=1, k=self.K, largest=True, sorted=False)\n        \n        # select according to the index\n        mask = torch.zeros(distribution.size(), requires_grad=False).to(device).detach()\n        mask = mask.scatter(1,indices,1).unsqueeze(-1).repeat(1,1,feature_length)\n\n        selection = mask * hidden\n\n        return selection, mask\n\n    def cal_nn_input_dim(self, strides, chunk_size):\n\n        if self.mode == ""concat"":\n\n            compress_factor = 1\n            for s in strides:\n                compress_factor = compress_factor * s\n\n            if chunk_size % compress_factor != 0:\n                raise ValueError(\'chunk_size should be divisible by the product of the strides factors!\')\n\n            nn_input = int(chunk_size // compress_factor) * self.emb_dim\n            print(""input_dim of the attention blocks: {}"".format(nn_input))\n            return nn_input\n\n        if self.mode == ""avg_time"" or self.mode == ""avg_time_batch"":\n\n            return self.emb_dim\n\n\n    def init_running_avg(self, batch_size):\n        dist = torch.randn(self.emb_dim).float()\n        dist = dist.unsqueeze(0).repeat(batch_size,1)\n        dist = F.softmax(dist,dim=1)\n        return dist\n\n\n\n\n\n\n'"
pase/models/classifiers.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ntry:\n    from .modules import *\nexcept ImportError:\n    from modules import *\n\n\n\nclass EmoDRNLSTM(Model):\n    """""" Based on https://ieeexplore.ieee.org/document/8682154 \n        (Li et al. 2019), without MHA\n    """"""\n    def __init__(self, num_inputs, num_outputs, max_ckpts=5, \n                 frontend=None, ft_fe=False, dropout=0,\n                 rnn_dropout=0, att=False, att_heads=4,\n                 att_dropout=0,\n                 name=\'EmoDRNMHA\'):\n        super().__init__(max_ckpts=max_ckpts, name=name)\n        self.num_inputs = num_inputs\n        self.num_outputs = num_outputs\n        self.frontend = frontend\n        self.ft_fe = ft_fe\n        self.drn = nn.Sequential(\n            # first conv block (10, 32), \n            nn.Conv1d(num_inputs, 32, 10),\n            # decimating x2\n            nn.Conv1d(32, 64, 2, stride=2),\n            # first residual blocks (2 resblocks)\n            ResBasicBlock1D(64, 64, kwidth=5, att=att,\n                            att_heads=att_heads,\n                            att_dropout=att_dropout), \n            ResBasicBlock1D(64, 64, kwidth=5, att=att,\n                            att_heads=att_heads,\n                            att_dropout=att_dropout), \n            # dropout feature maps\n            nn.Dropout2d(dropout),\n            # decimating x2\n            nn.Conv1d(64, 128, 2, stride=2),\n            # second residual blocks (2 resblocks)\n            ResBasicBlock1D(128, 128, kwidth=5, att=att,\n                            att_heads=att_heads,\n                            att_dropout=att_dropout), \n            ResBasicBlock1D(128, 128, kwidth=5, att=att,\n                            att_heads=att_heads,\n                            att_dropout=att_dropout), \n            # dropout feature maps\n            nn.Dropout2d(dropout),\n            nn.Conv1d(128, 256, 1, stride=1),\n            # third residual blocks (2 dilated resblocks)\n            ResBasicBlock1D(256, 256, kwidth=3, dilation=2,\n                            att=att,\n                            att_heads=att_heads,\n                            att_dropout=att_dropout), \n            ResBasicBlock1D(256, 256, kwidth=3, dilation=2,\n                            att=att,\n                            att_heads=att_heads,\n                            att_dropout=att_dropout), \n            # dropout feature maps\n            nn.Dropout2d(dropout),\n            nn.Conv1d(256, 512, 1, stride=1),\n            # fourth residual blocks (2 dilated resblocks)\n            ResBasicBlock1D(512, 512, kwidth=3, dilation=4,\n                            att=att,\n                            att_heads=att_heads,\n                            att_dropout=att_dropout), \n            ResBasicBlock1D(512, 512, kwidth=3, dilation=4,\n                            att=att,\n                            att_heads=att_heads,\n                            att_dropout=att_dropout), \n            # dropout feature maps\n            nn.Dropout2d(dropout)\n        )\n        # recurrent pooling with 2 LSTM layers\n        self.rnn = nn.LSTM(512, 512, num_layers=2, batch_first=True,\n                           dropout=rnn_dropout)\n        # mlp on top (https://ieeexplore.ieee.org/abstract/document/7366551)\n        self.mlp = nn.Sequential(\n            nn.Conv1d(512, 200, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(200, 200, 1),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(200, num_outputs, 1),\n            nn.LogSoftmax(dim=1)\n        )\n\n    def forward(self, x):\n        # input x with shape [B, F, T]\n        # FORWARD THROUGH DRN\n        # ----------------------------\n        if self.frontend is not None:\n            x = self.frontend(x)\n            if not self.ft_fe:\n                x = x.detach()\n        x = F.pad(x, (4, 5))\n        x = self.drn(x)\n        # FORWARD THROUGH RNN\n        # ----------------------------\n        x = x.transpose(1, 2)\n        x, _ = self.rnn(x)\n        xt = torch.chunk(x, x.shape[1], dim=1)\n        x = xt[-1].transpose(1, 2)\n        # FORWARD THROUGH DNn\n        # ----------------------------\n        x = self.mlp(x)\n        return x\n\nclass MLPClassifier(Model):\n\n    def __init__(self, num_inputs,\n                 frontend=None,\n                 num_spks=None,\n                 ft_fe=False,\n                 hidden_size=2048,\n                 hidden_layers=1,\n                 z_bnorm=False,\n                 max_ckpts=5,\n                 time_pool=False,\n                 name=\'MLP\'):\n        # 2048 default size raises 5.6M params\n        super().__init__(name=name, max_ckpts=max_ckpts)\n        self.num_inputs = num_inputs\n        self.frontend = frontend\n        self.ft_fe = ft_fe\n        if ft_fe:\n            print(\'Training the front-end\')\n        if z_bnorm:\n            # apply z-norm to the input\n            self.z_bnorm = nn.BatchNorm1d(num_inputs, affine=False)\n        if num_spks is None:\n            raise ValueError(\'Please specify a number of spks.\')\n        layers = [nn.Conv1d(num_inputs, hidden_size, 1),\n                  nn.LeakyReLU(),\n                  nn.BatchNorm1d(hidden_size)]\n        for n in range(1, hidden_layers):\n            layers += [nn.Conv1d(hidden_size, hidden_size, 1),\n                       nn.LeakyReLU(),\n                       nn.BatchNorm1d(hidden_size)]\n        layers += [nn.Conv1d(hidden_size, num_spks, 1),\n                   nn.LogSoftmax(dim=1)]\n        self.model = nn.Sequential(*layers)\n        self.time_pool = time_pool\n\n    def forward(self, x):\n        if self.frontend is not None:\n            x = self.frontend(x)\n        h = x\n        if self.time_pool:\n            h = h.mean(dim=2, keepdim=True)\n        if not self.ft_fe:\n            h = h.detach()\n        if hasattr(self, \'z_bnorm\'):\n            h = self.z_bnorm(h)\n        return self.model(h)\n\nclass RNNClassifier(Model):\n\n    def __init__(self, num_inputs,\n                 frontend=None,\n                 num_spks=None,\n                 ft_fe=False,\n                 hidden_size=1300,\n                 hidden_layers=1,\n                 z_bnorm=False,\n                 uni=False,\n                 return_sequence=False,\n                 name=\'RNN\'):\n        # 1300 default size raises 5.25M params\n        super().__init__(name=name, max_ckpts=1000)\n        self.num_inputs = num_inputs\n        self.frontend = frontend\n        self.ft_fe = ft_fe\n        if ft_fe:\n            print(\'Training the front-end\')\n        if z_bnorm:\n            # apply z-norm to the input\n            self.z_bnorm = nn.BatchNorm1d(num_inputs, affine=False)\n        if num_spks is None:\n            raise ValueError(\'Please specify a number of spks.\')\n        if uni:\n            hsize = hidden_size\n        else:\n            hsize = hidden_size // 2\n        self.rnn = nn.GRU(num_inputs, hsize,\n                          num_layers=hidden_layers,\n                          bidirectional=not uni,\n                          batch_first=True)\n        self.model = nn.Sequential(\n            nn.Conv1d(hidden_size, num_spks, 1),\n            nn.LogSoftmax(dim=1)\n        )\n        self.return_sequence = return_sequence\n        self.uni = uni\n\n    def forward(self, x):\n        if self.frontend is not None:\n            x = self.frontend(x)\n        h = x\n        if not self.ft_fe:\n            h = h.detach()\n        if hasattr(self, \'z_bnorm\'):\n            h = self.z_bnorm(h)\n        ht, state = self.rnn(h.transpose(1, 2))\n        if self.return_sequence:\n            ht = ht.transpose(1, 2)\n        else:\n            if not self.uni:\n                # pick last time-step for each dir\n                # first chunk feat dim\n                bsz, slen, feats = ht.size()\n                ht = torch.chunk(ht.view(bsz, slen, 2, feats // 2), 2, dim=2)\n                # now select fwd\n                ht_fwd = ht[0][:, -1, 0, :].unsqueeze(2)\n                ht_bwd = ht[1][:, 0, 0, :].unsqueeze(2)\n                ht = torch.cat((ht_fwd, ht_bwd), dim=1)\n            else:\n                # just last time-step works\n                ht = ht[:, -1, :].unsqueeze(2)\n        y = self.model(ht)\n        return y\n\nif __name__ == \'__main__\':\n    drn = EmoDRNLSTM(1, 4)\n    print(drn)\n    x = torch.randn(1, 1, 100)\n    y = drn(x)\n    print(\'y size: \', y.size())\n'"
pase/models/core.py,29,"b'from .minions import *\nfrom ..losses import *\nfrom ..utils import AuxiliarSuperviser, get_grad_norms\nfrom ..log import *\n#from tensorboardX import SummaryWriter\nimport soundfile as sf\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nimport numpy as np\nimport random\nimport timeit\nimport os\n\n\nclass Waveminionet(Model):\n\n    def __init__(self, frontend=None, frontend_cfg=None,\n                 minions_cfg=None, z_minion=True,\n                 z_cfg=None, adv_loss=\'BCE\',\n                 num_devices=1, pretrained_ckpts=None,\n                 name=\'Waveminionet\'):\n        super().__init__(name=name)\n        # augmented wav processing net\n        # it trains simultaneously with many tasks\n        # forcing a hierarchy of abstraction to distill # the contents within waveforms \n        if minions_cfg is None or len(minions_cfg) < 1:\n            raise ValueError(\'Please specify a stack of minions\'\n                             \' config with at least 1 minion. \'\n                             \'GIMME SOMETHING TO DO.\')\n        if frontend is not None:\n            self.frontend = frontend\n        else:\n            if frontend_cfg is None:\n                # default params\n                self.frontend = WaveFe()\n            else:\n                self.frontend = WaveFe(**frontend_cfg)\n        if self.frontend.quantizer is not None:\n            self.vq = True\n        else:\n            self.vq = False\n        # -------- MINION STACK --------\n        self.minions = nn.ModuleList()\n        self.mi_fwd = False\n        ninp = self.frontend.emb_dim\n        self.min2idx = {}\n        for minion_cfg in minions_cfg:\n            if \'mi\' in minion_cfg[\'name\'] and not self.mi_fwd:\n                # add additional code for pair (either CMI or MI)\n                # (just once, thus use mi_fwd flag)\n                ninp += self.frontend.emb_dim\n            minion_cfg[\'num_inputs\'] = ninp\n            minion = minion_maker(minion_cfg)\n            self.minions.append(minion)\n            self.min2idx[minion.name] = len(self.min2idx) \n            if hasattr(minion, \'skip\') and minion.skip:\n                nouts = minion.hidden_size\n                # acumulate num of inputs (concat skip connection)\n                ninp += nouts\n            if \'mi\' in minion.name:\n                # if MI minion is present, multi chunk forward\n                # is needed (3 chunks are fwd)\n                self.mi_fwd = True\n        if z_minion:\n            # Make the minion enforcing the shape of the latent space\n            # to be like some prior z_gen enforced in the loss\n            # This minion is disconnected from others, just enforcing\n            # frontend\'s output to follow Z, but no skip,\n            # and it always backprops even in random backprop selection\n            # as it acts as a regularizer\n            if z_cfg is None:\n                z_cfg = {\n                    \'num_inputs\':self.frontend.emb_dim,\n                    \'num_outputs\':1,\n                    \'hidden_layers\':3,\n                    \'hidden_size\':1024,\n                    \'norm_type\':\'bnorm\',\n                    \'dropout\':0.,\n                    \'kwidths\':[31,11,5],\n                    \'name\':\'z\',\n                    \'grad_reverse\':False,\n                    \'skip\':False\n                }\n            self.z_cfg = z_cfg\n            self.z_adv_loss = adv_loss\n            #self.z_minion = minion_maker(z_cfg)\n            #self.z_minion.loss.register_DNet(self.z_minion)\n        if pretrained_ckpts is not None:\n            self.load_checkpoints(pretrained_ckpts)\n        if num_devices > 1:\n            self.frontend_dp = nn.DataParallel(self.frontend)\n            self.minions_dp = nn.ModuleList([nn.DataParallel(m) for m in \\\n                                             self.minions])\n\n    def build_z_minion(self, cfg):\n        print(\'Built regularizer Z minion\')\n        device = \'cuda\' if next(self.parameters()).is_cuda else \'cpu\'\n        self.z_cfg[\'loss\'] = ZAdversarialLoss(loss=self.z_adv_loss,\n                                              batch_acum=cfg[\'batch_acum\'])\n        # Build the regularizer Z minion\n        self.z_minion = minion_maker(self.z_cfg)\n        self.z_minion.loss.register_DNet(self.z_minion)\n        self.z_minion.to(device)\n\n    def forward(self, x):\n        raise NotImplementedError\n        fe_h = self.frontend(x)\n        #print(\'front-end inference: \', fe_h.size())\n        h = fe_h\n        outs = {}\n        for mi, minion in enumerate(self.minions, start=1):\n            y, h_ = minion(h)\n            if minion.skip:\n                h_c = torch.cat((h, h_), dim=1)\n                h = h_c\n            else:\n                h = h\n            outs[minion.name] = y\n        return outs, h\n\n    def join_skip(self, x, skip):\n        if skip is None:\n            return x\n        else:\n            return torch.cat((x, skip), dim=1)\n\n    def load_checkpoints(self, load_path):\n        # create each savers first for all net components\n        savers = [Saver(self.frontend, load_path, \n                        prefix=\'PASE-\')]\n        if hasattr(self, \'z_minion\'):\n            savers.append(Saver(self.z_minion, load_path,\n                           prefix=\'Zminion-\'))\n        for mi, minion in enumerate(self.minions, start=1):\n            savers.append(Saver(minion, load_path, \n                                prefix=\'M-{}-\'.format(minion.name)))\n        # now load each ckpt found\n        giters = 0\n        for saver in savers:\n            # try loading all savers last state if not forbidden is active\n            try:\n                state = saver.read_latest_checkpoint()\n                giter_ = saver.load_ckpt_step(state)\n                print(\'giter_ found: \', giter_)\n                # assert all ckpts happened at last same step\n                if giters == 0:\n                    giters = giter_\n                else:\n                    assert giters == giter_, giter_\n                saver.load_pretrained_ckpt(os.path.join(load_path,\n                                                        \'weights_\' + state), \n                                           load_last=True)\n            except TypeError:\n                break\n\n    def forward_chunk(self, frontend, batch, chunk_name, device):\n        if self.vq:\n            vq_loss, fe_Q, \\\n            vq_pp, vq_idx = frontend(batch[chunk_name].to(device))\n            return fe_Q\n        else:\n            return frontend(batch[chunk_name].to(device))\n\n    def train_(self, dloader, cfg, device=\'cpu\', va_dloader=None):\n        epoch = cfg[\'epoch\']\n        bsize = cfg[\'batch_size\']\n        batch_acum = cfg[\'batch_acum\']\n        save_path = cfg[\'save_path\']\n        log_freq = cfg[\'log_freq\']\n        sup_freq = cfg[\'sup_freq\']\n        grad_keys = cfg[\'log_grad_keys\']\n        if cfg[\'sup_exec\'] is not None:\n            aux_save_path = os.path.join(cfg[\'save_path\'],\n                                         \'sup_aux\')\n            if not os.path.exists(aux_save_path):\n                os.makedirs(aux_save_path)\n            self.aux_sup = AuxiliarSuperviser(cfg[\'sup_exec\'], aux_save_path)\n        # Adversarial auto-encoder hyperparams\n        warmup_epoch = cfg[\'warmup\']\n        zinit_weight = cfg[\'zinit_weight\']\n        zinc = cfg[\'zinc\']\n        zweight = 0\n        if hasattr(self, \'frontend_dp\'):\n            frontend = self.frontend_dp\n        else:\n            frontend = self.frontend\n        # Build the regularizer minion\n        self.build_z_minion(cfg)\n        # Make the log writer(s)\n        writer = LogWriter(save_path, log_types=cfg[\'log_types\'])\n        bpe = cfg[\'bpe\'] if \'bpe\' in cfg else len(dloader)\n        print(\'=\' * 50)\n        print(\'Beginning training...\')\n        print(\'Batches per epoch: \', bpe)\n        # rndmin_train flag means we donly backprop one minion path        \n        # per batch update, selecting the minion randomly\n        rndmin_train = cfg[\'rndmin_train\']\n        print(\'Randomized minion training: \', rndmin_train)\n        feopt = getattr(optim, cfg[\'fe_opt\'])(self.frontend.parameters(), \n                                              lr=cfg[\'fe_lr\'])\n        # Make the saver array. Each one will refer to one model. Init\n        # with frontend model and optimizer\n        savers = [Saver(self.frontend, save_path, \n                        max_ckpts=cfg[\'max_ckpts\'],\n                        optimizer=feopt, prefix=\'PASE-\')]\n        lrdecay = cfg[\'lrdecay\']\n        if lrdecay > 0:\n            fesched = optim.lr_scheduler.StepLR(feopt,\n                                                step_size=cfg[\'lrdec_step\'],\n                                                gamma=cfg[\'lrdecay\'])\n            #fesched = optim.lr_scheduler.ReduceLROnPlateau(feopt,\n            #                                               mode=\'min\',\n            #                                               factor=lrdecay,\n            #                                               verbose=True)\n        if hasattr(self, \'z_minion\'):\n            z_lr = cfg[\'z_lr\']\n            zopt = getattr(optim, cfg[\'min_opt\'])(self.z_minion.parameters(), \n                                                  lr=z_lr)\n            if lrdecay > 0:\n                #zsched = optim.lr_scheduler.ReduceLROnPlateau(zopt,\n                #                                              mode=\'min\',\n                #                                              factor=lrdecay,\n                #                                              verbose=True)\n                zsched = optim.lr_scheduler.StepLR(zopt,\n                                                   step_size=cfg[\'lrdec_step\'],\n                                                   gamma=cfg[\'lrdecay\'])\n            savers.append(Saver(self.z_minion,\n                                save_path, max_ckpts=cfg[\'max_ckpts\'],\n                                optimizer=zopt, prefix=\'Zminion-\'))\n\n        # print model components\n        print(self)\n        if \'min_lrs\' in cfg:\n            min_lrs = cfg[\'min_lrs\']\n        else:\n            min_lrs = None\n        minopts = {}\n        minscheds = {}\n        for mi, minion in enumerate(self.minions, start=1):\n            min_opt = cfg[\'min_opt\']\n            min_lr = cfg[\'min_lr\']\n            if min_lrs is not None and minion.name in min_lrs:\n                min_lr = min_lrs[minion.name]\n                print(\'Applying lr {:.5f} to minion {}\'.format(min_lr,\n                                                               minion.name))\n            minopts[minion.name] = getattr(optim, min_opt)(minion.parameters(),\n                                                           lr=min_lr)\n            if lrdecay > 0:\n                #minsched = lr_scheduler.ReduceLROnPlateau(minopts[minion.name],\n                #                                          mode=\'min\',\n                #                                          factor=lrdecay,\n                #                                          verbose=True)\n                minsched = lr_scheduler.StepLR(minopts[minion.name],\n                                               step_size=cfg[\'lrdec_step\'],\n                                               gamma=cfg[\'lrdecay\'])\n                minscheds[minion.name] = minsched\n            savers.append(Saver(minion, save_path, max_ckpts=cfg[\'max_ckpts\'],\n                                optimizer=minopts[minion.name],\n                                prefix=\'M-{}-\'.format(minion.name)))\n\n        minions_run = self.minions\n        if hasattr(self, \'minions_dp\'):\n            minions_run = self.minions_dp\n\n        min_global_steps = {}\n        if cfg[\'ckpt_continue\']:\n            giters = 0\n            for saver in savers:\n                # try loading all savers last state if not forbidden is active\n                try:\n                    state = saver.read_latest_checkpoint()\n                    giter_ = saver.load_ckpt_step(state)\n                    print(\'giter_ found: \', giter_)\n                    # assert all ckpts happened at last same step\n                    if giters == 0:\n                        giters = giter_\n                    else:\n                        assert giters == giter_, giter_\n                    saver.load_pretrained_ckpt(os.path.join(save_path,\n                                                            \'weights_\' + state), \n                                               load_last=True)\n                except TypeError:\n                    break\n\n            global_step = giters\n            # redefine num epochs depending on where we left it\n            epoch_beg = int(global_step / bpe)\n            epoch = epoch - epoch_beg\n        else:\n            epoch_beg = 0\n            global_step = 0\n\n        z_losses = None\n        print(\'Beginning step of training: \', global_step)\n        print(\'Looping for {} epochs: \'.format(epoch))\n\n        for epoch_ in range(epoch_beg, epoch_beg + epoch):\n            self.train()\n            timings = []\n            beg_t = timeit.default_timer()\n            min_loss = {}\n            if epoch_ + 1 == warmup_epoch and hasattr(self, \'z_minion\'):\n                zweight = zinit_weight\n\n            iterator = iter(dloader)\n            for bidx in range(1, bpe + 1):\n                try:\n                    batch = next(iterator)\n                except StopIteration:\n                    iterator = iter(dloader)\n                    batch = next(iterator)\n                #feopt.zero_grad()\n                fe_h = {}\n                # accumulate all forwards of FE and concat in same batch\n                fe_forwards = [batch[\'chunk\']]\n                # forward chunk (alone) through frontend\n                if self.mi_fwd:\n                    fe_forwards.extend([batch[\'chunk_ctxt\'],\n                                       batch[\'chunk_rand\']])\n                fe_forwards.append(batch[\'cchunk\'])\n                # build triplet batch and forward it too\n                fe_forwards_b = torch.cat(fe_forwards, dim=0)\n                if self.vq:\n                    vq_loss, fe_Q, \\\n                    vq_pp, vq_idx = frontend(fe_forwards_b.to(device))\n                    fe_h[\'all\'] = fe_Q\n                else:\n                    fe_h[\'all\'] = frontend(fe_forwards_b.to(device))\n                # slice the tensor back in batch dimension\n                all_feh = torch.chunk(fe_h[\'all\'], len(fe_forwards), dim=0)\n                fe_h[\'chunk\'] = all_feh[0]\n                fe_h[\'cchunk\'] = all_feh[-1]\n                min_h = {}\n                h = fe_h[\'chunk\']\n                skip_acum = None\n                for mi, minion in enumerate(minions_run, start=1):\n                    min_name = self.minions[mi - 1].name\n                    if \'mi\' in min_name:\n                        triplet_P = self.join_skip(torch.cat((all_feh[0],\n                                                              all_feh[1]),\n                                                             dim=1), skip_acum)\n                        triplet_N = self.join_skip(torch.cat((all_feh[0],\n                                                              all_feh[2]),\n                                                             dim=1), skip_acum)\n                        triplet_all = torch.cat((triplet_P, triplet_N), dim=0)\n                        if min_name == \'cmi\':\n                            # average through time dimension for ChunkMI\n                            triplet_all = torch.mean(triplet_all, dim=2,\n                                                     keepdim=True)\n                        y = minion(triplet_all)\n                        bsz = y.size(0)//2\n                        slen = y.size(2)\n                        batch[min_name] = torch.cat((torch.ones(bsz, 1, slen),\n                                                     torch.zeros(bsz, 1, slen)),\n                                                    dim=0)\n\n                    else:\n                        if self.minions[mi - 1].skip:\n                            y, h_ = minion(self.join_skip(h, skip_acum))\n                            if skip_acum is None:\n                                skip_acum = h_\n                            else:\n                                skip_acum = torch.cat((skip_acum, h_), dim=1)\n                        else:\n                            y = minion(self.join_skip(h, skip_acum))\n                        if min_name == \'spc\':\n                            # we must create the spc labels, composed of \n                            # B ones and B zeros (future and past). It\n                            # internally creates 2B samples\n                            bsz = y.size(0) // 2\n                            slen = y.size(2)\n                            batch[\'spc\'] = torch.cat((torch.ones(bsz, 1, slen),\n                                                      torch.zeros(bsz, 1,\n                                                                  slen)),\n                                                     dim=0)\n                    min_h[min_name] = y\n\n                if epoch_ + 1 >= warmup_epoch and hasattr(self, \'z_minion\'):\n                    # First shape the hidden space as Z if needed\n                    # Adversarial learning to map Fe(wav) to Z ~ prior\n                    if cfg[\'cchunk_prior\']:\n                        # clean chunk inference is the \'real\' reference\n                        z_real = fe_h[\'cchunk\']\n                        z_true_trainable = True\n                    else:\n                        z_real = None\n                        z_true_trainable = False\n                    z_losses = self.z_minion.loss(global_step,\n                                                  fe_h[\'chunk\'],\n                                                  zopt,\n                                                  z_true=z_real,\n                                                  z_true_trainable=z_true_trainable)\n                    # TODO: weight new adversarial minion?\n                    #g_loss = zweight * g_loss\n                   \n                    # update weight incrementally if needed still\n                    zweight = min(1, zweight + zinc)\n\n                global_step += 1\n                t_loss = torch.zeros(1).to(device)\n                if z_losses is not None and \'g_loss\' in z_losses:\n                    # aggregate adversarial loss for PASE trunk\n                    t_loss += z_losses[\'g_loss\']\n                if self.vq:\n                    # aggregate VQ loss\n                    t_loss += vq_loss\n                # backprop time\n                if rndmin_train:\n                    if rnd_min not in min_global_steps:\n                        min_global_steps[rnd_min] = 0\n                    min_names = list(min_h.keys())\n                    rnd_min = random.choice(min_names)\n                    #minopts[rnd_min].zero_grad()\n                    y_ = min_h[rnd_min]\n                    minion = minions_run[self.min2idx[rnd_min]]\n                    y_lab = batch[rnd_min].to(device)\n                    lweight = minion.loss_weight\n                    if isinstance(minion.loss, WaveAdversarialLoss):\n                        loss = minion.loss(min_global_steps[rnd_min],\n                                           y_, y_lab, c_real=fe_h[\'chunk\'])\n                        d_real_loss = loss[\'d_real_loss\']\n                        d_fake_loss = loss[\'d_fake_loss\']\n                        if not \'{}_Dreal\'.format(rnd_min) in min_loss:\n                            min_loss[\'{}_Dreal\'.format(rnd_min)] = []\n                            min_loss[\'{}_Dfake\'.format(rnd_min)] = []\n                        if not \'{}_Dreal\'.format(rnd_min) in min_global_steps:\n                            min_global_steps[\'{}_Dreal\'.format(rnd_min)] = 0\n                            min_global_steps[\'{}_Dfake\'.format(rnd_min)] = 0\n                        min_loss[\'{}_Dreal\'.format(rnd_min)] = d_real_loss.item()\n                        min_loss[\'{}_Dfake\'.format(rnd_min)] = d_fake_loss.item()\n                        loss = loss[\'g_loss\']\n                        loss = lweight * loss\n                        loss.backward()\n                    else:\n                        loss = minion.loss(y_, y_lab)\n                        loss = lweight * loss\n                        loss.backward()\n                    if rnd_min not in min_loss:\n                        min_loss[rnd_min] = []\n                    min_loss[rnd_min].append(loss.item())\n                    min_global_steps[rnd_min] += 1\n                    if \'{}_Dreal\'.format(rnd_min) in min_global_steps:\n                        min_global_steps[\'{}_Dreal\'.format(rnd_min)] += 1\n                        min_global_steps[\'{}_Dfake\'.format(rnd_min)] += 1\n                    #minopts[rnd_min].step()\n                else:\n                    #if hasattr(self, \'minions_dp\'):\n                    #    raise NotImplementedError(\'DataParallel to be included\')\n                    # Compute all minion losses\n                    for min_name, y_ in min_h.items():\n                        if min_name not in min_global_steps:\n                            min_global_steps[min_name] = 0\n                        minion = minions_run[self.min2idx[min_name]]\n                        minopts[min_name].zero_grad()\n                        y_lab = batch[min_name].to(device)\n                        lweight = minion.loss_weight\n                        if isinstance(minion.loss, WaveAdversarialLoss):\n                            loss = minion.loss(min_global_steps[min_name],\n                                               y_, y_lab, c_real=fe_h[\'chunk\'])\n                            d_real_loss = loss[\'d_real_loss\']\n                            d_fake_loss = loss[\'d_fake_loss\']\n                            if not \'{}_Dreal\'.format(min_name) in min_loss:\n                                min_loss[\'{}_Dreal\'.format(min_name)] = []\n                                min_loss[\'{}_Dfake\'.format(min_name)] = []\n                            if not \'{}_Dreal\'.format(min_name) in min_global_steps:\n                                min_global_steps[\'{}_Dreal\'.format(min_name)] = 0\n                                min_global_steps[\'{}_Dfake\'.format(min_name)] = 0\n                            min_loss[\'{}_Dreal\'.format(min_name)].append(d_real_loss.item())\n                            min_loss[\'{}_Dfake\'.format(min_name)].append(d_fake_loss.item())\n                            loss = loss[\'g_loss\']\n                            loss = lweight * loss\n                        else:\n                            loss = minion.loss(y_, y_lab)\n                            loss = lweight * loss\n                        t_loss += loss\n                        #loss.backward(retain_graph=True)\n                        if min_name not in min_loss:\n                            min_loss[min_name] = []\n                        min_loss[min_name].append(loss.item())\n                        min_global_steps[min_name] += 1\n                        if \'{}_Dreal\'.format(min_name) in min_global_steps:\n                            min_global_steps[\'{}_Dreal\'.format(min_name)] += 1\n                            min_global_steps[\'{}_Dfake\'.format(min_name)] += 1\n                        #minopts[min_name].step()\n                    t_loss.backward()\n                if bidx % batch_acum == 0 or bidx >= bpe:\n                    # first store the gradients\n                    grads = get_grad_norms(self, grad_keys)\n                    # update minions\n                    for min_name, y_ in min_h.items():\n                        minopts[min_name].step()\n                        minopts[min_name].zero_grad()\n                    # update frontend\n                    feopt.step()\n                    feopt.zero_grad()\n                    if epoch_ + 1 >= warmup_epoch and hasattr(self, \'z_minion\'):\n                        zopt.step()\n                        zopt.zero_grad()\n                end_t = timeit.default_timer()\n                timings.append(end_t - beg_t)\n                beg_t = timeit.default_timer()\n                if bidx % log_freq == 0 or bidx >= bpe:\n                    print(\'-\' * 50)\n                    print(\'Batch {}/{} (Epoch {}):\'.format(bidx, bpe, epoch_))\n                    for min_name, losses in min_loss.items():\n                        print(\'Minion {} loss: {:.3f} gidx: \'\n                              \'{:5d} \'.format(min_name, losses[-1], \n                                              min_global_steps[min_name]))\n                        writer.add_scalar(\'train/{}_loss\'.format(min_name),\n                                          losses[-1], min_global_steps[min_name])\n                        if min_name in min_h:\n                            # Adversarial are not included\n                            writer.add_histogram(\'train/{}\'.format(min_name),\n                                                 min_h[min_name].data,\n                                                 bins=\'sturges\',\n                                                 global_step=min_global_steps[min_name])\n                        if min_name in min_h:\n                            # Adversarial are not included\n                            writer.add_histogram(\'train/gtruth_{}\'.format(min_name),\n                                                 batch[min_name].data,\n                                                 bins=\'sturges\',\n                                                 global_step=min_global_steps[min_name])\n                    if z_losses is not None:\n                        z_log = \'ZMinion \'\n                        if \'dfake_loss\' in z_losses:\n                            dfake_loss = z_losses[\'dfake_loss\'].item()\n                            z_log += \'dfake_loss: {:.3f},\'.format(dfake_loss)\n                            writer.add_scalar(\'train/dfake_loss\',\n                                              dfake_loss,\n                                              global_step)\n                        if \'dreal_loss\' in z_losses:\n                            dreal_loss = z_losses[\'dreal_loss\'].item()\n                            writer.add_scalar(\'train/dreal_loss\',\n                                              dreal_loss,\n                                              global_step)\n                            z_log += \' dreal_loss: {:.3f},\'.format(dreal_loss)\n                        if \'greal_loss\' in z_losses:\n                            greal_loss = z_losses[\'greal_loss\'].item()\n                            z_log += \', greal_loss: {:.3f},\'.format(greal_loss)\n                            writer.add_scalar(\'train/greal_loss\',\n                                              greal_loss,\n                                              global_step)\n                        if \'gfake_loss\' in z_losses:\n                            gfake_loss = z_losses[\'gfake_loss\'].item()\n                            z_log += \', gfake_loss: {:.3f},\'.format(gfake_loss)\n                            writer.add_scalar(\'train/gfake_loss\',\n                                              gfake_loss,\n                                              global_step)\n                        print(z_log)\n                        #writer.add_scalar(\'train/zweight\',\n                        #                  zweight,\n                        #                  global_step)\n                        if z_true_trainable:\n                            writer.add_histogram(\'train/z_real\',\n                                                 fe_h[\'cchunk\'],\n                                                 bins=\'sturges\',\n                                                 global_step=global_step)\n                        writer.add_histogram(\'train/z_fake\',\n                                             fe_h[\'chunk\'],\n                                             bins=\'sturges\',\n                                             global_step=global_step)\n                    if self.vq:\n                        print(\'VQLoss: {:.3f}, VQPP: \'\n                              \'{:.3f}\'.format(vq_loss.item(), vq_pp.item()))\n                        writer.add_scalar(\'train/vq_loss\', vq_loss.item(),\n                                          global_step=global_step)\n                        writer.add_scalar(\'train/vq_pp\', vq_pp.item(),\n                                          global_step=global_step)\n                    # --- Get gradient norms for sanity check ----\n                    for kgrad, vgrad in grads.items():\n                        writer.add_scalar(\'train/GRAD/{}\'.format(kgrad),\n                                          vgrad, global_step)\n                    print(\'Total summed loss: {:.3f}\'.format(t_loss.item()))\n\n\n                    print(\'Mean batch time: {:.3f} s\'.format(np.mean(timings)))\n            # epoch end\n            if va_dloader is not None:\n                va_bpe = cfg[\'va_bpe\']\n                eloss = self.eval_(va_dloader, bsize, va_bpe, log_freq=log_freq,\n                                   epoch_idx=epoch_,\n                                   writer=writer, device=device)\n\n                """"""\n                if lrdecay > 0:\n                    # update frontend lr\n                    fesched.step(eloss)\n                    # update Z minion lr\n                    if hasattr(self, \'z_minion\'):\n                        zsched.step(eloss)\n                    # update each minion lr\n                    for mi, minion in enumerate(self.minions, start=1):\n                        minscheds[minion.name].step(eloss)\n                """"""\n\n            if lrdecay > 0:\n                # update frontend lr\n                fesched.step()\n                # update Z minion lr\n                if hasattr(self, \'z_minion\'):\n                    zsched.step()\n                # update each minion lr\n                for mi, minion in enumerate(self.minions, start=1):\n                    minscheds[minion.name].step()\n\n            # Save plain frontend weights \n            fe_path = os.path.join(save_path, \n                                   \'FE_e{}.ckpt\'.format(epoch_))\n            torch.save(self.frontend.state_dict(), fe_path)\n            # Run through each saver to save model and optimizer\n            for saver in savers:\n                saver.save(saver.prefix[:-1], global_step)\n            #torch.save(self.state_dict(),\n            #           os.path.join(save_path,\n            #                        \'fullmodel_e{}.ckpt\'.format(epoch_)))\n            # TODO: sup. aux losses\n            if (epoch_ + 1 ) % sup_freq == 0 or \\\n               (epoch_ + 1) >= (epoch_beg + epoch):\n                if hasattr(self, \'aux_sup\'):\n                    self.aux_sup(epoch_, fe_path, cfg[\'fe_cfg\'])\n\n    def eval_(self, dloader, batch_size, bpe, log_freq,\n              epoch_idx=0, writer=None, device=\'cpu\'):\n        self.eval()\n        with torch.no_grad():\n            bsize = batch_size\n            frontend = self.frontend\n            minions_run = self.minions\n            print(\'=\' * 50)\n            print(\'Beginning evaluation...\')\n            timings = []\n            beg_t = timeit.default_timer()\n            min_loss = {}\n\n            iterator = iter(dloader)\n            for bidx in range(1, bpe + 1):\n                try:\n                    batch = next(iterator)\n                except StopIteration:\n                    iterator = iter(dloader)\n                    batch = next(iterator)\n                # Build chunk keys to know what to encode\n                chunk_keys = [\'chunk\']\n                if self.mi_fwd:\n                    chunk_keys += [\'chunk_ctxt\', \'chunk_rand\']\n                fe_h = {}\n                # Forward chunk(s) through frontend\n                for k in chunk_keys:\n                    fe_h[k] = frontend(batch[k].to(device))\n                min_h = {}\n                h = fe_h[\'chunk\']\n                skip_acum = None\n                for mi, minion in enumerate(minions_run, start=1):\n                    min_name = self.minions[mi - 1].name\n                    if \'mi\' in min_name:\n                        triplet_P = self.join_skip(torch.cat((fe_h[\'chunk\'],\n                                                              fe_h[\'chunk_ctxt\']),\n                                                             dim=1), skip_acum)\n                        triplet_N = self.join_skip(torch.cat((fe_h[\'chunk\'],\n                                                              fe_h[\'chunk_rand\']),\n                                                             dim=1), skip_acum)\n                        triplet_all = torch.cat((triplet_P, triplet_N), dim=0)\n                        if min_name == \'cmi\':\n                            # average through time dimension for ChunkMI\n                            triplet_all = torch.mean(triplet_all, dim=2,\n                                                     keepdim=True)\n                        y = minion(triplet_all)\n                        bsz = y.size(0)//2\n                        slen = y.size(2)\n                        batch[min_name] = torch.cat((torch.ones(bsz, 1, slen),\n                                                     torch.zeros(bsz, 1, slen)),\n                                                    dim=0)\n                    else:\n                        if self.minions[mi - 1].skip:\n                            y, h_ = minion(self.join_skip(h, skip_acum))\n                            if skip_acum is None:\n                                skip_acum = h_\n                            else:\n                                skip_acum = torch.cat((skip_acum, h_), dim=1)\n                        else:\n                            y = minion(self.join_skip(h, skip_acum))\n                        if min_name == \'spc\':\n                            # we must create the spc labels, composed of \n                            # B ones and B zeros (future and past). It\n                            # internally creates 2B samples\n                            bsz = y.size(0) // 2\n                            slen = y.size(2)\n                            batch[\'spc\'] = torch.cat((torch.ones(bsz, 1, slen),\n                                                      torch.zeros(bsz, 1,\n                                                                  slen)),\n                                                     dim=0)\n                    min_h[min_name] = y\n\n                # Compute all minion losses\n                for min_name, y_ in min_h.items():\n                    y_lab = batch[min_name].to(device)\n                    minion = self.minions[self.min2idx[min_name]]\n                    lweight = minion.loss_weight\n                    if isinstance(minion.loss, WaveAdversarialLoss):\n                        loss = minion.loss(bidx,\n                                           y_, y_lab, c_real=fe_h[\'chunk\'],\n                                           grad=False)\n                        loss = loss[\'g_loss\']\n                    else:\n                        loss = minion.loss(y_, y_lab)\n                    loss = lweight * loss\n                    #loss = lweight * self.minions[self.min2idx[min_name]].loss(y_, y_lab)\n                    if min_name not in min_loss:\n                        min_loss[min_name] = []\n                    min_loss[min_name].append(loss.item())\n                end_t = timeit.default_timer()\n                timings.append(end_t - beg_t)\n                beg_t = timeit.default_timer()\n                \n                if bidx % log_freq == 0 or bidx >= bpe:\n                    print(\'-\' * 50)\n                    print(\'EVAL Batch {}/{} (Epoch {}):\'.format(bidx, \n                                                                bpe,\n                                                                epoch_idx))\n                    for min_name, losses in min_loss.items():\n                        print(\'Minion {} loss: {:.3f}\'\n                              \'\'.format(min_name, losses[-1]))\n                    print(\'Mean batch time: {:.3f} s\'.format(np.mean(timings)))\n\n            # --------------------------------------------------------------\n            # After all eval data, write mean values of epoch per minion\n            aggregate = 0\n            for min_name, losses in min_loss.items():\n                mlosses = np.mean(losses)\n                writer.add_scalar(\'eval/{}_loss\'.format(min_name),\n                                  mlosses, epoch_idx)\n                aggregate += mlosses\n            # aggregate eval loss\n            writer.add_scalar(\'eval/total_loss\', aggregate,\n                              epoch_idx)\n            return aggregate\n\n\n    def state_dict(self):\n        sdict = {}\n        for k, v in super().state_dict().items():\n            if \'_dp.\' in k:\n                # skip any DataParallel wrapped thing\n                continue\n            sdict[k] = v\n        return sdict\n\n\nif __name__ == \'__main__\':\n    wmodel = Waveminionet(\n        minions_cfg=[\n            {\'num_outputs\':1,\n             \'dropout\':0.2,\n             \'name\':\'chunk\',\n             \'type\':\'decoder\',\n             },\n            {\'num_outputs\':257,\n             \'dropout\':0.2,\n             \'name\':\'lps\',\n             },\n            {\'num_outputs\':40,\n             \'dropout\':0.2,\n             \'name\':\'mfcc\'\n             },\n            {\'num_outputs\':4,\n             \'dropout\':0.2,\n             \'name\':\'prosody\'\n             },\n            #{\'num_outputs\':1,\n            # \'dropout\':0.2,\n            # \'name\':\'mi\',\n            # \'keys\':[\'chunk\',\n            #         \'chunk_ctxt\',\n            #         \'chunk_rand\']\n            #},\n        ]\n                         )\n    print(wmodel)\n    x = torch.randn(1, 1, 8000)\n    outs, y = wmodel(x)\n    for k, v in outs.items():\n        print(\'{} : {}\'.format(k, v.size()))\n'"
pase/models/decoders.py,6,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom .frontend import *\n#from .Minions.minions import *\nimport random\n\nclass SpectrumLM(nn.Module):\n    """""" RNN lang model for spectrum frame preds """"""\n    def __init__(self, rnn_size, rnn_layers, out_dim,\n                 dropout,\n                 cuda, rnn_type=\'LSTM\',\n                 bidirectional=False):\n        super().__init__()\n\n        self.do_cuda = cuda\n        self.rnn_size = rnn_size\n        self.rnn_layers = rnn_layers\n        self.rnn_type = rnn_type\n        self.out_dim = out_dim\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n        if bidirectional:\n            self.dirs = 2\n        else:\n            self.dirs = 1\n        assert rnn_type == \'LSTM\' or rnn_type == \'GRU\', rnn_type\n        self.rnn = getattr(nn, rnn_type)(self.out_dim, self.rnn_size,\n                                        self.rnn_layers,\n                                        batch_first=True,\n                                        dropout=self.dropout,\n                                        bidirectional=bidirectional)\n        self.out_fc = nn.Linear(self.rnn_size, self.out_dim)\n\n    def forward(self, x, dec_steps, state=None, \n                dec_cps={}):\n        # x is just a time-step input [B, F]\n        assert len(x.size()) == 2, x.size()\n        if state is None:\n            state = self.init_hidden(x.size(0))\n        assert isinstance(dec_cps, dict), type(dec_cps)\n        x = x.unsqueeze(1)\n        ht = x\n        frames = []\n        # forward through RNN\n        for t in range(dec_steps):\n            if t in dec_cps:\n                #print(\'Using cp at t:{}\'.format(t))\n                ht = dec_cps[t]\n                if len(ht.size()) == 2:\n                    # add time axis\n                    ht = ht.unsqueeze(1)\n            #print(\'Forwarding ht: \', ht.size())\n            ht, state = self.rnn(ht, state)\n            ht = self.out_fc(ht)\n            frames.append(ht)\n        frames = torch.cat(frames, 1)\n        return frames, state\n\n    def init_hidden(self, bsz):\n        h0 = Variable(torch.randn(self.dirs * self.rnn_layers,\n                                  bsz, self.rnn_size))\n        if self.do_cuda:\n            h0 = h0.cuda()\n        if self.rnn_type == \'LSTM\':\n            c0 = h0.clone()\n            return (h0, c0)\n        else:\n            return h0\n\nclass SpectrogramDecoder(Model):\n\n    def __init__(self, num_inputs, nfft=1024, \n                 strides=[1, 1, 1],\n                 kwidths=[3, 3, 3],\n                 fmaps=[256, 256, 256],\n                 norm_type=None,\n                 name=\'SpectrogramDecoder\'):\n        super().__init__(name=name)\n        ninp = num_inputs\n        self.dec = nn.ModuleList()\n        for di, (kwidth, stride, fmap) in enumerate(zip(kwidths, strides,\n                                                        fmaps), \n                                                    start=1):\n            if stride > 1:\n                self.dec.append(GDeconv1DBlock(ninp, fmap, kwidth, stride,\n                                               norm_type=norm_type))\n            else:\n                self.dec.append(GConv1DBlock(ninp, fmap, kwidth, 1,\n                                             norm_type=norm_type))\n            ninp = fmap\n        self.dec.append(nn.Conv1d(ninp, nfft // 2 + 1, 1))\n\n    def forward(self, x):\n        for dec in self.dec: x = dec(x)\n        return x\n\n\nif __name__ == \'__main__\':\n    spec_dec = SpectrogramDecoder(1024, 1024)\n    x = torch.randn(1, 512, 100)\n    y = spec_dec(x)\n    print(x.shape)\n    print(y.shape)\n'"
pase/models/discriminator.py,8,"b""import torch\nimport torch.nn as nn\nimport math\nimport torch.nn.functional as F\nfrom torch.nn.utils.spectral_norm import spectral_norm\nimport numpy as np\nimport json\nimport os\ntry:\n    from modules import *\nexcept ImportError:\n    from .modules import *\n\n\nclass WaveDiscriminator(nn.Module):\n\n    def __init__(self, ninputs=1,\n                 fmaps=[128, 128, 256, 256, 512, 100],\n                 strides=[10, 4, 4, 1, 1, 1],\n                 kwidths=[30, 30, 30, 3, 3, 3],\n                 norm_type='snorm'\n                ):\n        super().__init__()\n        self.aco_decimator = nn.ModuleList()\n        ninp = ninputs\n        for fmap, kwidth, stride in zip(fmaps, kwidths, strides):\n            self.aco_decimator.append(GConv1DBlock(ninp, fmap, kwidth,\n                                                   stride, norm_type=norm_type))\n                                                   \n            ninp = fmap\n        self.out_fc = nn.Conv1d(fmaps[-1], 1, 1)\n        if norm_type == 'snorm':\n            nn.utils.spectral_norm(self.out_fc)\n        self.norm_type = norm_type\n\n    def build_conditionW(self, cond):\n        if cond is not None:\n            cond_dim = cond.size(1)\n            if not hasattr(self, 'proj_W'):\n                self.proj_W = nn.Linear(cond_dim, cond_dim, bias=False)\n                if self.norm_type == 'snorm':\n                    nn.utils.spectral_norm(self.proj_W)\n                if cond.is_cuda:\n                    self.proj_W.cuda()\n        \n    def forward(self, x, cond=None):\n        self.build_conditionW(cond)\n        h = x\n        for di in range(len(self.aco_decimator)):\n            dec_layer = self.aco_decimator[di]\n            h = dec_layer(h)\n        bsz, nfeats, slen = h.size()\n        if cond is not None:\n            cond = torch.mean(cond, dim=2)\n            # project conditioner with bilinear W\n            cond = self.proj_W(cond)\n            h = torch.mean(h, dim=2)\n            h = h.view(-1, nfeats)\n            cond = cond.view(-1, nfeats)\n            cls = torch.bmm(h.unsqueeze(1),\n                            cond.unsqueeze(2)).squeeze(2)\n            cls = cls.view(bsz, 1)\n        y = self.out_fc(h.unsqueeze(2)).squeeze(2)\n        y = y + cls\n        return y.squeeze(1)\n\nif __name__ == '__main__':\n    waveD = WaveDiscriminator()\n    x = torch.randn(1, 1, 16000)\n    h = torch.randn(1, 100, 100)\n    y = waveD(x, h)\n    print('y size: ', y.size())\n"""
pase/models/encoders.py,1,"b'import torch\nimport torch.nn as nn\nfrom .core import LayerNorm\n\n\nclass AhoCNNEncoder(nn.Module):\n\n    def __init__(self, input_dim, kwidth=3, dropout=0.5, layer_norm=False):\n        super().__init__()\n        pad = (kwidth - 1) // 2\n\n        if layer_norm:\n            norm_layer = LayerNorm\n        else:\n            norm_layer = nn.BatchNorm1d\n\n        self.enc = nn.Sequential(\n            nn.Conv1d(input_dim, 256, kwidth, stride=1, padding=pad),\n            norm_layer(256),\n            nn.PReLU(256),\n            nn.Conv1d(256, 256, kwidth, stride=1, padding=pad),\n            norm_layer(256),\n            nn.PReLU(256),\n            nn.MaxPool1d(2),\n            nn.Dropout(0.2),\n            nn.Conv1d(256, 512, kwidth, stride=1, padding=pad),\n            norm_layer(512),\n            nn.PReLU(512),\n            nn.Conv1d(512, 512, kwidth, stride=1, padding=pad),\n            norm_layer(512),\n            nn.PReLU(512),\n            nn.MaxPool1d(2),\n            nn.Dropout(0.2),\n            nn.Conv1d(512, 1024, kwidth, stride=1, padding=pad),\n            norm_layer(1024),\n            nn.PReLU(1024),\n            nn.Conv1d(1024, 1024, kwidth, stride=1, padding=pad),\n            norm_layer(1024),\n            nn.PReLU(1024),\n            nn.MaxPool1d(2),\n            nn.Dropout(0.2),\n            nn.Conv1d(1024, 1024, kwidth, stride=1, padding=pad),\n        )\n\n    def forward(self, x):\n        return self.enc(x)\n\n\nclass AhoCNNHourGlassEncoder(nn.Module):\n\n    def __init__(self, input_dim, kwidth=3, dropout=0.5, layer_norm=False):\n        super().__init__()\n        pad = (kwidth - 1) // 2\n\n        if layer_norm:\n            norm_layer = LayerNorm\n        else:\n            norm_layer = nn.BatchNorm1d\n\n        self.enc = nn.Sequential(\n            nn.Conv1d(input_dim, 64, kwidth, stride=1, padding=pad),\n            norm_layer(64),\n            nn.PReLU(64),\n            nn.Conv1d(64, 128, kwidth, stride=1, padding=pad),\n            norm_layer(128),\n            nn.PReLU(128),\n            nn.MaxPool1d(2),\n            nn.Dropout(dropout),\n            nn.Conv1d(128, 256, kwidth, stride=1, padding=pad),\n            norm_layer(256),\n            nn.PReLU(256),\n            nn.Conv1d(256, 512, kwidth, stride=1, padding=pad),\n            norm_layer(512),\n            nn.PReLU(512),\n            nn.MaxPool1d(2),\n            nn.Dropout(dropout),\n            nn.Conv1d(512, 256, kwidth, stride=1, padding=pad),\n            norm_layer(256),\n            nn.PReLU(256),\n            nn.Conv1d(256, 128, kwidth, stride=1, padding=pad),\n            norm_layer(128),\n            nn.PReLU(128),\n            nn.MaxPool1d(2),\n            nn.Dropout(dropout),\n            nn.Conv1d(128, 64, kwidth, stride=1, padding=pad),\n            norm_layer(64),\n            nn.PReLU(64),\n        )\n\n    def forward(self, x):\n        return self.enc(x)\n'"
pase/models/frontend.py,7,"b'import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport json\nfrom pase.models.WorkerScheduler.encoder import encoder\nimport torchvision.models as models\ntry:\n    from modules import *\n    from aspp import aspp_resblock\n    from tdnn import TDNN\nexcept ImportError:\n    from .modules import *\n    from .aspp import aspp_resblock\n    from .tdnn import TDNN\n\n\n\ndef wf_builder(cfg_path):\n    if cfg_path is not None:\n        if isinstance(cfg_path, str):\n            with open(cfg_path, \'r\') as cfg_f:\n                cfg = json.load(cfg_f)\n                return wf_builder(cfg)\n        elif isinstance(cfg_path, dict):\n            if ""name"" in cfg_path.keys():\n                model_name = cfg_path[\'name\']\n                if cfg_path[\'name\'] == ""asppRes"":\n                    return aspp_res_encoder(**cfg_path)\n                elif model_name == ""Resnet50"":\n                    return Resnet50_encoder(**cfg_path)\n                elif model_name == ""tdnn"":\n                    return TDNNFe(**cfg_path)\n                else:\n                    raise TypeError(\'Unrecognized frontend type: \', model_name)\n            else:\n                return WaveFe(**cfg_path)\n        else:\n            TypeError(\'Unexpected config for WaveFe\')\n    else:\n        raise ValueError(""cfg cannot be None!"")\n\n\nclass TDNNFe(Model):\n    """""" Time-Delayed Neural Network front-end\n    """"""\n    def __init__(self, num_inputs=1,\n                 sincnet=True,\n                 kwidth=641, stride=160,\n                 fmaps=128, norm_type=\'bnorm\',\n                 pad_mode=\'reflect\',\n                 sr=16000, emb_dim=256,\n                 activation=None,\n                 rnn_pool=False,\n                 rnn_layers=1,\n                 rnn_dropout=0,\n                 rnn_type=\'qrnn\',\n                 name=\'TDNNFe\'):\n        super().__init__(name=name) \n        # apply sincnet at first layer\n        self.sincnet = sincnet\n        self.emb_dim = emb_dim\n        ninp = num_inputs\n        if self.sincnet:\n            self.feblock = FeBlock(ninp, fmaps, kwidth, stride,\n                                   1, act=activation,\n                                   pad_mode=pad_mode,\n                                   norm_type=norm_type,\n                                   sincnet=True,\n                                   sr=sr)\n            ninp = fmaps\n        # 2 is just a random number because it is not used\n        # with unpooled method\n        self.tdnn = TDNN(ninp, 2, method=\'unpooled\')\n        fmap = self.tdnn.emb_dim\n        # last projection\n        if rnn_pool:\n            self.rnn = build_rnn_block(fmap, emb_dim // 2,\n                                       rnn_layers=rnn_layers,\n                                       rnn_type=rnn_type,\n                                       bidirectional=True,\n                                       dropout=rnn_dropout)\n            self.W = nn.Conv1d(emb_dim, emb_dim, 1)\n        else:\n            self.W = nn.Conv1d(fmap, emb_dim, 1)\n        self.rnn_pool = rnn_pool\n\n    def forward(self, batch, device=None, mode=None):\n\n        # batch possible chunk and contexts, or just forward non-dict tensor\n        x, data_fmt = format_frontend_chunk(batch, device)\n        if hasattr(self, \'feblock\'): \n            h = self.feblock(x)\n        \n        h = self.tdnn(h)\n\n        if self.rnn_pool:\n            h = h.transpose(1, 2).transpose(0, 1)\n            h, _ = self.rnn(h)\n            h = h.transpose(0, 1).transpose(1, 2)\n\n        y = self.W(h)\n\n        return format_frontend_output(y, data_fmt, mode)\n        """"""\n        if self.training:\n            if batched:\n                embedding = torch.chunk(y, 3, dim=0)\n                chunk = embedding[0]\n            else:\n                chunk = y\n            return embedding, chunk\n        else:\n            return select_output(h, mode=mode)\n        """"""\n\nclass WaveFe(Model):\n    """""" Convolutional front-end to process waveforms\n        into a decimated intermediate representation \n    """"""\n    def __init__(self, num_inputs=1, \n                 sincnet=True,\n                 kwidths=[251, 10, 5, 5, 5, 5, 5, 5], \n                 strides=[1, 10, 2, 1, 2, 1, 2, 2], \n                 dilations=[1, 1, 1, 1, 1, 1, 1, 1],\n                 fmaps=[64, 64, 128, 128, 256, 256, 512, 512],\n                 norm_type=\'bnorm\',\n                 pad_mode=\'reflect\', sr=16000,\n                 emb_dim=256,\n                 rnn_dim=None,\n                 activation=None,\n                 rnn_pool=False,\n                 rnn_layers=1,\n                 rnn_dropout=0,\n                 rnn_type=\'qrnn\',\n                 vq_K=None,\n                 vq_beta=0.25,\n                 vq_gamma=0.99,\n                 norm_out=False,\n                 tanh_out=False,\n                 resblocks=False,\n                 denseskips=False,\n                 densemerge=\'sum\',\n                 name=\'WaveFe\'):\n        super().__init__(name=name) \n        # apply sincnet at first layer\n        self.sincnet = sincnet\n        self.kwidths = kwidths\n        self.strides = strides\n        self.fmaps = fmaps\n        self.densemerge = densemerge\n        if denseskips:\n            self.denseskips = nn.ModuleList()\n        self.blocks = nn.ModuleList()\n        assert len(kwidths) == len(strides)\n        assert len(strides) == len(fmaps)\n        concat_emb_dim = emb_dim\n        ninp = num_inputs\n        for n, (kwidth, stride, dilation, fmap) in enumerate(zip(kwidths, \n                                                                 strides,\n                                                                 dilations,\n                                                                 fmaps), \n                                                             start=1):\n            if n > 1:\n                # make sure sincnet is deactivated after first layer\n                sincnet = False\n            if resblocks and not sincnet:\n                feblock = FeResBlock(ninp, fmap, kwidth, \n                                     downsample=stride,\n                                     act=activation,\n                                     pad_mode=pad_mode, norm_type=norm_type)\n            else:\n                feblock = FeBlock(ninp, fmap, kwidth, stride,\n                                  dilation,\n                                  act=activation,\n                                  pad_mode=pad_mode,\n                                  norm_type=norm_type,\n                                  sincnet=sincnet,\n                                  sr=sr)\n            self.blocks.append(feblock)\n            if denseskips and n < len(kwidths):\n                # add projection adapter \n                self.denseskips.append(nn.Conv1d(fmap, emb_dim, 1, bias=False))\n                if densemerge == \'concat\':\n                    concat_emb_dim += emb_dim\n            ninp = fmap\n        # last projection\n        if rnn_pool:\n            if rnn_dim is None:\n                rnn_dim = emb_dim\n            self.rnn = build_rnn_block(fmap, rnn_dim // 2,\n                                       rnn_layers=rnn_layers,\n                                       rnn_type=rnn_type,\n                                       bidirectional=True,\n                                       dropout=rnn_dropout)\n            self.W = nn.Conv1d(rnn_dim, emb_dim, 1)\n        else:\n            self.W = nn.Conv1d(fmap, emb_dim, 1)\n        self.emb_dim = concat_emb_dim\n        self.rnn_pool = rnn_pool\n        if vq_K is not None and vq_K > 0:\n            self.quantizer = VQEMA(vq_K, self.emb_dim,\n                                   vq_beta, vq_gamma)\n        else:\n            self.quantizer = None\n        # ouptut vectors are normalized to norm^2 1\n        if norm_out:\n            if norm_type == \'bnorm\':\n                self.norm_out = nn.BatchNorm1d(self.emb_dim, affine=False)\n            else:\n                self.norm_out = nn.InstanceNorm1d(self.emb_dim)\n        self.tanh_out = tanh_out\n\n    def fuse_skip(self, input_, skip):\n        #print(\'input_ shape: \', input_.shape)\n        #print(\'skip shape: \', skip.shape)\n        dfactor = skip.shape[2] // input_.shape[2]\n        if dfactor > 1:\n            #print(\'dfactor: \', dfactor)\n            # downsample skips\n            # [B, F, T]\n            maxlen = input_.shape[2] * dfactor\n            skip = skip[:, :, :maxlen]\n            bsz, feats, slen = skip.shape\n            skip_re = skip.view(bsz, feats, slen // dfactor, dfactor)\n            skip = torch.mean(skip_re, dim=3)\n            #skip = F.adaptive_avg_pool1d(skip, input_.shape[2])\n        if self.densemerge == \'concat\':\n            return torch.cat((input_, skip), dim=1)\n        elif self.densemerge == \'sum\':\n            return input_ + skip\n        else:\n            raise TypeError(\'Unknown densemerge: \', self.densemerge)\n        \n    def forward(self, batch, device=None, mode=None):\n        # batch possible chunk and contexts, or just forward non-dict tensor\n        x, data_fmt = format_frontend_chunk(batch, device)\n        h = x\n        denseskips = hasattr(self, \'denseskips\')\n        if denseskips:\n            dskips = None\n            dskips = []\n        for n, block in enumerate(self.blocks):\n            h = block(h)\n            if denseskips and (n + 1) < len(self.blocks):\n                # denseskips happen til the last but one layer\n                # til the embedding one\n                proj = self.denseskips[n]\n                dskips.append(proj(h))\n                """"""\n                if dskips is None:\n                    dskips = proj(h)\n                else:\n                    h_proj = proj(h)\n                    dskips = self.fuse_skip(h_proj, dskips)\n                """"""\n        if self.rnn_pool:\n            h = h.transpose(1, 2).transpose(0, 1)\n            h, _ = self.rnn(h)\n            h = h.transpose(0, 1).transpose(1, 2)\n            #y = self.W(h) \n        #else:\n        y = self.W(h)\n        if denseskips:\n            for dskip in dskips:\n                # sum all dskips contributions in the embedding\n                y = self.fuse_skip(y, dskip)\n        if hasattr(self, \'norm_out\'):\n            y = self.norm_out(y)\n        if self.tanh_out:\n            y = torch.tanh(y)\n\n        if self.quantizer is not None:\n            qloss, y, pp, enc = self.quantizer(y)\n            if self.training:\n                return qloss, y, pp, enc\n            else:\n                return y\n\n        return format_frontend_output(y, data_fmt, mode)\n\n\nclass aspp_res_encoder(Model):\n\n    def __init__(self, sinc_out, hidden_dim, kernel_sizes=[11, 11, 11, 11], sinc_kernel=251,sinc_stride=1,strides=[10, 4, 2, 2], dilations=[1, 6, 12, 18], fmaps=48, name=\'aspp_encoder\', pool2d=False, rnn_pool=False, rnn_add=False, concat=[False, False, False, True], dense=False):\n        super().__init__(name=name)\n        self.sinc = SincConv_fast(1, sinc_out, sinc_kernel,\n                                  sample_rate=16000,\n                                  padding=\'SAME\',\n                                  stride=sinc_stride,\n                                  pad_mode=\'reflect\'\n                                  )\n\n\n        self.ASPP_blocks = nn.ModuleList()\n\n        for i in range(len(kernel_sizes)):\n            if i == 0:\n                self.ASPP_blocks.append(aspp_resblock(sinc_out, hidden_dim, kernel_sizes[i], strides[i], dilations, fmaps[i], pool2d[i], dense))\n            else:\n                self.ASPP_blocks.append(aspp_resblock(hidden_dim, hidden_dim, kernel_sizes[i], strides[i], dilations, fmaps[i], pool2d[i], dense))\n\n\n        self.rnn_pool = rnn_pool\n        self.rnn_add = rnn_add\n        self.concat = concat\n        assert ((self.rnn_pool and self.rnn_add) or not self.rnn_pool) or self.rnn_pool\n\n        if rnn_pool:\n            self.rnn = build_rnn_block(hidden_dim, hidden_dim // 2,\n                                       rnn_layers=1,\n                                       rnn_type=\'qrnn\',\n                                       bidirectional=True,\n                                       dropout=0)\n            self.W = nn.Conv1d(hidden_dim, hidden_dim, 1)\n\n\n        self.emb_dim = hidden_dim\n\n\n\n    def forward(self, batch, device=None, mode=None):\n\n        # batch possible chunk and contexts, or just forward non-dict tensor\n        x, data_fmt = format_frontend_chunk(batch, device)\n\n        sinc_out = self.sinc(x)\n\n        out = []\n        input = sinc_out\n        for i, block in enumerate(self.ASPP_blocks, 0):\n            input = block(input)\n            if self.concat[i]:\n                out.append(input)\n\n        if len(out) > 1:\n            out = self.fuse(out)\n            out = torch.cat(out, dim=1)\n        else:\n            out = out[0]\n\n\n\n        if self.rnn_pool:\n            rnn_out = out.transpose(1, 2).transpose(0, 1)\n            rnn_out, _ = self.rnn(rnn_out)\n            rnn_out = rnn_out.transpose(0, 1).transpose(1, 2)\n\n        if self.rnn_pool and self.rnn_add:\n            h = out + rnn_out\n        elif self.rnn_pool and not self.rnn_add:\n            h = rnn_out\n        else:\n            h = out\n\n        return format_frontend_output(h, data_fmt, mode)\n\n\n    def fuse(self, out):\n        last_feature = out[-1]\n        for i in range(len(out) - 1):\n            out[i] = F.adaptive_avg_pool1d(out[i], last_feature.shape[-1])\n        return out\n\nclass Resnet50_encoder(Model):\n\n    def __init__(self, sinc_out, hidden_dim, sinc_kernel=251, sinc_stride=1, conv_stride=5, kernel_size=21, pretrained=True,name=""Resnet50""):\n        super().__init__(name=name)\n        self.sinc = SincConv_fast(1, sinc_out, sinc_kernel,\n                                  sample_rate=16000,\n                                  padding=\'SAME\',\n                                  stride=sinc_stride,\n                                  pad_mode=\'reflect\'\n                                  )\n\n        self.conv1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=kernel_size, stride=conv_stride, padding= kernel_size // 2, bias=False),\n                                   nn.BatchNorm2d(64),\n                                   nn.ReLU(64))\n\n        resnet = models.resnet34(pretrained=pretrained)\n        self.resnet = nn.Sequential(resnet.layer1,\n                                    resnet.layer2,\n                                    resnet.layer3,\n                                    resnet.layer4\n                                    )\n\n        self.conv2 = nn.Sequential(nn.Conv2d(512, 256, kernel_size=[2, 1], stride=1, bias=False))\n\n        self.emb_dim = hidden_dim\n\n\n    def forward(self, batch, device=None, mode=None):\n\n        # batch possible chunk and contexts, or just forward non-dict tensor\n        x, data_fmt = format_frontend_chunk(batch, device)\n\n        sinc_out = self.sinc(x).unsqueeze(1)\n\n        # print(sinc_out.shape)\n\n        conv_out = self.conv1(sinc_out)\n\n        # print(conv_out.shape)\n\n        res_out = self.resnet(conv_out)\n\n        # print(res_out.shape)\n\n        h =self.conv2(res_out).squeeze(2)\n\n        # print(h.shape)\n\n        return format_frontend_output(h, data_fmt, mode)\n\n'"
pase/models/modules.py,75,"b'import torch\nimport torch.nn as nn\nimport math\nimport torch.nn.functional as F\nfrom torch.distributions import Binomial\nfrom torch.nn.utils.spectral_norm import spectral_norm\nfrom torch.nn.utils.weight_norm import weight_norm\nimport numpy as np\nimport json\nimport os\ntry:\n    from torchqrnn import QRNN\nexcept ImportError:\n    QRNN = None\n\ndef format_frontend_chunk(batch, device=\'cpu\'):\n    if type(batch) == dict:\n        if \'chunk_ctxt\' and \'chunk_rand\' in batch:\n            keys = [\'chunk\', \'chunk_ctxt\', \'chunk_rand\', \'cchunk\']\n            # cluster all \'chunk\'s, including possible \'cchunk\'\n            batches = [batch[k] for k in keys if k in batch]\n            x = torch.cat(batches, dim=0).to(device)\n            # store the number of batches condensed as format\n            data_fmt = len(batches)\n        else:\n            x = batch[\'chunk\'].to(device)\n            data_fmt = 1\n    else:\n        x = batch\n        data_fmt = 0\n    return x, data_fmt\n\ndef format_frontend_output(y, data_fmt, mode):\n    #assert data_fmt >= 0\n    if data_fmt > 1:\n        embedding = torch.chunk(y, data_fmt, dim=0)\n        chunk = embedding[0]\n        return embedding, chunk\n    elif data_fmt == 1:\n        chunk = embedding = y\n        return embedding, chunk\n    else:\n        return select_output(y, mode=mode)\n\ndef build_rnn_block(in_size, rnn_size, rnn_layers, rnn_type,\n                    bidirectional=True,\n                    dropout=0, use_cuda=True):\n    if (rnn_type.lower() == \'qrnn\') and QRNN is not None:\n        if bidirectional:\n            print(\'WARNING: QRNN ignores bidirectional flag\')\n            rnn_size = 2 * rnn_size\n        rnn = QRNN(in_size, rnn_size, rnn_layers, dropout=dropout, window=2,\n                   use_cuda=use_cuda)\n    elif rnn_type.lower() == \'lstm\' or rnn_type.lower() == \'gru\':\n        rnn = getattr(nn, rnn_type.upper())(in_size, rnn_size, rnn_layers,\n                                            dropout=dropout,\n                                            bidirectional=bidirectional)\n    else:\n        raise TypeError(\'Unrecognized rnn type: \', rnn_type)\n    return rnn\n        \ndef select_output(h, mode=None):\n    if mode == ""avg_norm"":\n        return h - torch.mean(h, dim=2, keepdim=True)\n    elif mode == ""avg_concat"":\n        global_avg = torch.mean(h, dim=2, keepdim=True).repeat(1, 1, h.shape[-1])\n        return torch.cat((h, global_avg), dim=1)\n    elif mode == ""avg_norm_concat"":\n        global_avg = torch.mean(h, dim=2, keepdim=True)\n        h = h - global_avg\n        global_feature = global_avg.repeat(1, 1, h.shape[-1])\n        return torch.cat((h, global_feature), dim=1)\n    else:\n        return h\n\n\ndef build_norm_layer(norm_type, param=None, num_feats=None):\n    if norm_type == \'bnorm\':\n        return nn.BatchNorm1d(num_feats)\n    elif norm_type == \'snorm\':\n        spectral_norm(param)\n        return None\n    elif norm_type == \'bsnorm\':\n        spectral_norm(param)\n        return nn.BatchNorm1d(num_feats)\n    elif norm_type == \'lnorm\':\n        return nn.LayerNorm(num_feats)\n    elif norm_type == \'wnorm\':\n        weight_norm(param)\n        return None\n    elif norm_type == \'inorm\':\n        return nn.InstanceNorm1d(num_feats, affine=False)\n    elif norm_type == \'affinorm\':\n        return nn.InstanceNorm1d(num_feats, affine=True)\n    elif norm_type is None:\n        return None\n    else:\n        raise TypeError(\'Unrecognized norm type: \', norm_type)\n\ndef forward_norm(x, norm_layer):\n    if norm_layer is not None:\n        if isinstance(norm_layer, nn.LayerNorm):\n            x = x.transpose(1, 2)\n        x = norm_layer(x)\n        if isinstance(norm_layer, nn.LayerNorm):\n            x = x.transpose(1, 2)\n        return x\n    else:\n        return x\n\ndef build_activation(activation, params, init=0):\n    if activation == \'prelu\' or activation is None:\n        return nn.PReLU(params, init=init)\n    if isinstance(activation, str):\n        return getattr(nn, activation)()\n    else:\n        return activation\n\ndef forward_activation(activation, tensor):\n    if activation == \'glu\':\n        # split tensor in two in channels dim\n        z, g = torch.chunk(tensor, 2, dim=1)\n        y = z * torch.sigmoid(g)\n        return y\n    else:\n        return activation(tensor)\n\ndef get_padding(kwidth, dilation):\n    return (kwidth // 2) * dilation\n\nclass NeuralBlock(nn.Module):\n\n    def __init__(self, name=\'NeuralBlock\'):\n        super().__init__()\n        self.name = name\n\n\t# https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/5\n    def describe_params(self):\n        pp = 0\n        for p in list(self.parameters()):\n            nn = 1\n            for s in list(p.size()):\n                nn = nn * s\n            pp += nn\n        print(\'-\' * 10)\n        print(self)\n        print(\'Num params: \', pp)\n        print(\'-\' * 10)\n        return pp\n\nclass Saver(object):\n\n    def __init__(self, model, save_path, max_ckpts=5, optimizer=None, prefix=\'\'):\n        self.model = model\n        self.save_path = save_path\n        self.ckpt_path = os.path.join(save_path, \'{}checkpoints\'.format(prefix)) \n        self.max_ckpts = max_ckpts\n        self.optimizer = optimizer\n        self.prefix = prefix\n\n    def save(self, model_name, step, best_val=False):\n        save_path = self.save_path\n        if not os.path.exists(save_path):\n            os.makedirs(save_path)\n\n        ckpt_path = self.ckpt_path\n        if os.path.exists(ckpt_path):\n            with open(ckpt_path, \'r\') as ckpt_f:\n                # read latest checkpoints\n                ckpts = json.load(ckpt_f)\n        else:\n            ckpts = {\'latest\':[], \'current\':[]}\n\n        model_path = \'{}-{}.ckpt\'.format(model_name, step)\n        if best_val: \n            model_path = \'best_\' + model_path\n        model_path = \'{}{}\'.format(self.prefix, model_path)\n        \n        # get rid of oldest ckpt, with is the frst one in list\n        latest = ckpts[\'latest\']\n        if len(latest) > 0:\n            todel = latest[0]\n            if self.max_ckpts is not None:\n                if len(latest) > self.max_ckpts:\n                    try:\n                        print(\'Removing old ckpt {}\'.format(os.path.join(save_path, \n                                                            \'weights_\' + todel)))\n                        os.remove(os.path.join(save_path, \'weights_\' + todel))\n                        latest = latest[1:] \n                    except FileNotFoundError:\n                        print(\'ERROR: ckpt is not there?\')\n\n        latest += [model_path]\n\n        ckpts[\'latest\'] = latest\n        ckpts[\'current\'] = model_path\n\n        with open(ckpt_path, \'w\') as ckpt_f:\n            ckpt_f.write(json.dumps(ckpts, indent=2))\n\n        st_dict = {\'step\':step,\n                   \'state_dict\':self.model.state_dict()}\n\n        if self.optimizer is not None: \n            st_dict[\'optimizer\'] = self.optimizer.state_dict()\n        # now actually save the model and its weights\n        #torch.save(self.model, os.path.join(save_path, model_path))\n        torch.save(st_dict, os.path.join(save_path, \n                                          \'weights_\' + \\\n                                           model_path))\n\n    def read_latest_checkpoint(self):\n        ckpt_path = self.ckpt_path\n        print(\'Reading latest checkpoint from {}...\'.format(ckpt_path))\n        if not os.path.exists(ckpt_path):\n            print(\'[!] No checkpoint found in {}\'.format(self.save_path))\n            return None\n        else:\n            with open(ckpt_path, \'r\') as ckpt_f:\n                ckpts = json.load(ckpt_f)\n            curr_ckpt = ckpts[\'current\'] \n            return curr_ckpt\n\n    #def load(self):\n    #    save_path = self.save_path\n    #    ckpt_path = self.ckpt_path\n    #    print(\'Reading latest checkpoint from {}...\'.format(ckpt_path))\n    #    if not os.path.exists(ckpt_path):\n    #        raise FileNotFoundError(\'[!] Could not load model. Ckpt \'\n    #                                \'{} does not exist!\'.format(ckpt_path))\n    #    with open(ckpt_path, \'r\') as ckpt_f:\n    #        ckpts = json.load(ckpt_f)\n    #    curr_ckpt = ckpts[\'curent\'] \n    #    st_dict = torch.load(os.path.join(save_path, curr_ckpt))\n    #    return \n\n    def load_weights(self):\n        save_path = self.save_path\n        curr_ckpt = self.read_latest_checkpoint()\n        if curr_ckpt is None:\n            print(\'[!] No weights to be loaded\')\n            return False\n        else:\n            st_dict = torch.load(os.path.join(save_path,\n                                              \'weights_\' + \\\n                                              curr_ckpt))\n            if \'state_dict\' in st_dict:\n                # new saving mode\n                model_state = st_dict[\'state_dict\']\n                self.model.load_state_dict(model_state)\n                if self.optimizer is not None and \'optimizer\' in st_dict:\n                    self.optimizer.load_state_dict(st_dict[\'optimizer\'])\n            else:\n                # legacy mode, only model was saved\n                self.model.load_state_dict(st_dict)\n            print(\'[*] Loaded weights\')\n            return True\n\n    def load_ckpt_step(self, curr_ckpt):\n        ckpt = torch.load(os.path.join(self.save_path,\n                                       \'weights_\' + \\\n                                       curr_ckpt),\n                          map_location=\'cpu\')\n        step = ckpt[\'step\']\n        return step\n\n    def load_pretrained_ckpt(self, ckpt_file, load_last=False, load_opt=True,\n                             verbose=True):\n        model_dict = self.model.state_dict() \n        st_dict = torch.load(ckpt_file, \n                             map_location=lambda storage, loc: storage)\n        if \'state_dict\' in st_dict:\n            pt_dict = st_dict[\'state_dict\']\n        else:\n            # legacy mode\n            pt_dict = st_dict\n        all_pt_keys = list(pt_dict.keys())\n        if not load_last:\n            # Get rid of last layer params (fc output in D)\n            allowed_keys = all_pt_keys[:-2]\n        else:\n            allowed_keys = all_pt_keys[:]\n        # Filter unnecessary keys from loaded ones and those not existing\n        pt_dict = {k: v for k, v in pt_dict.items() if k in model_dict and \\\n                   k in allowed_keys and v.size() == model_dict[k].size()}\n        if verbose:\n            print(\'Current Model keys: \', len(list(model_dict.keys())))\n            print(\'Current Pt keys: \', len(list(pt_dict.keys())))\n            print(\'Loading matching keys: \', list(pt_dict.keys()))\n        if len(pt_dict.keys()) != len(model_dict.keys()):\n            raise ValueError(\'WARNING: LOADING DIFFERENT NUM OF KEYS\')\n            print(\'WARNING: LOADING DIFFERENT NUM OF KEYS\')\n        # overwrite entries in existing dict\n        model_dict.update(pt_dict)\n        # load the new state dict\n        self.model.load_state_dict(model_dict)\n        for k in model_dict.keys():\n            if k not in allowed_keys:\n                print(\'WARNING: {} weights not loaded from pt ckpt\'.format(k))\n        if self.optimizer is not None and \'optimizer\' in st_dict and load_opt:\n            self.optimizer.load_state_dict(st_dict[\'optimizer\'])\n\n\nclass Model(NeuralBlock):\n\n    def __init__(self, max_ckpts=5, name=\'BaseModel\'):\n        super().__init__()\n        self.name = name\n        self.optim = None\n        self.max_ckpts = max_ckpts\n\n    def save(self, save_path, step, best_val=False, saver=None):\n        model_name = self.name\n\n        if not hasattr(self, \'saver\') and saver is None:\n            self.saver = Saver(self, save_path,\n                               optimizer=self.optim,\n                               prefix=model_name + \'-\',\n                               max_ckpts=self.max_ckpts)\n\n        if saver is None:\n            self.saver.save(model_name, step, best_val=best_val)\n        else:\n            # save with specific saver\n            saver.save(model_name, step, best_val=best_val)\n\n    def load(self, save_path):\n        if os.path.isdir(save_path):\n            if not hasattr(self, \'saver\'):\n                self.saver = Saver(self, save_path, \n                                   optimizer=self.optim,\n                                   prefix=self.name + \'-\',\n                                   max_ckpts=self.max_ckpts)\n            self.saver.load_weights()\n        else:\n            print(\'Loading ckpt from ckpt: \', save_path)\n            # consider it as ckpt to load per-se\n            self.load_pretrained(save_path)\n\n    def load_pretrained(self, ckpt_path, load_last=False, verbose=True):\n        # tmp saver\n        saver = Saver(self, \'.\', optimizer=self.optim)\n        saver.load_pretrained_ckpt(ckpt_path, load_last, verbose=verbose)\n\n\n    def activation(self, name):\n        return getattr(nn, name)()\n\n    def parameters(self):\n        return filter(lambda p: p.requires_grad, super().parameters())\n\n    def get_total_params(self):\n        pp = 0\n        for p in list(self.parameters()):\n            nn = 1\n            for s in list(p.size()):\n                nn = nn * s\n            pp += nn\n        return pp\n\n    def describe_params(self):\n        pp = 0\n        if hasattr(self, \'blocks\'):\n            for b in self.blocks:\n                p = b.describe_params()\n                pp += p\n        else:\n            print(\'Warning: did not find a list of blocks...\')\n            print(\'Just printing all params calculation.\')\n        total_params = self.get_total_params()\n        print(\'{} total params: {}\'.format(self.name,\n                                           total_params))\n        return total_params\n\n\nclass GConv1DBlock(NeuralBlock):\n\n    def __init__(self, ninp, fmaps,\n                 kwidth, stride=1, norm_type=None,\n                 act=\'prelu\',\n                 name=\'GConv1DBlock\'):\n        super().__init__(name=name)\n        if act is not None and act == \'glu\':\n            Wfmaps = 2 * fmaps\n        else:\n            Wfmaps = fmaps\n        self.conv = nn.Conv1d(ninp, Wfmaps, kwidth, stride=stride)\n        self.norm = build_norm_layer(norm_type, self.conv, fmaps)\n        self.act = build_activation(act, fmaps)\n        self.kwidth = kwidth\n        self.stride = stride\n\n    def forward(self, x):\n        if self.stride > 1 or self.kwidth % 2 == 0:\n            P = (self.kwidth // 2 - 1,\n                 self.kwidth // 2)\n        else:\n            P = (self.kwidth // 2,\n                 self.kwidth // 2)\n        x_p = F.pad(x, P, mode=\'reflect\')\n        h = self.conv(x_p)\n        h = forward_activation(self.act, h)\n        h = forward_norm(h, self.norm)\n        #h = self.act(h)\n        return h\n\nclass PatternedDropout(nn.Module):\n    def __init__(self, emb_size, p=0.5, \n                 dropout_mode=[\'fixed_rand\'],\n                 ratio_fixed=None,\n                 range_fixed=None,\n                 drop_whole_channels=False):\n        """"""Applies a fixed pattern of dropout for the whole training\n        session (i.e applies different only among pre-specified dimensions)\n        """"""\n        super(PatternedDropout, self).__init__()\n\n        if p < 0 or p > 1:\n            raise ValueError(""dropout probability has to be between 0 and 1, ""\n                             ""but got {}"".format(p))\n        self.p = p\n\n        if self.p > 0:\n\n            d_modes = [\'std\', \'fixed_rand\', \'fixed_given\']\n            assert dropout_mode in d_modes, (\n                ""Expected dropout mode in {}, got {}"".format(d_modes, dropout_mode)\n            )\n            self.drop_whole_channels = drop_whole_channels\n            self.dropout_fixed = False\n\n            if dropout_mode == \'fixed_rand\':\n\n                self.dropout_fixed = True\n                assert ratio_fixed is not None, (\n                    ""{} needs \'ratio_fixed\' arg set."".format(dropout_mode)\n                )\n                if ratio_fixed <= 0  or ratio_fixed > 1:\n                    raise ValueError(""{} mode needs \'ratio_fixed\' to be""\\\n                                    "" set and in (0, 1) range, got {}""\\\n                                    .format(dropout_mode, ratio_fixed))\n                self.ratio_fixed = ratio_fixed\n                self.dropped_dimsize = int(emb_size - emb_size*ratio_fixed)\n                tot_idx = np.arange(emb_size)\n                sel_idx = np.sort(np.random.choice(tot_idx, \n                                size=self.dropped_dimsize, replace=False))\n\n            elif dropout_mode == \'fixed_given\':\n\n                self.dropout_fixed = True\n                if range_fixed is None or \\\n                    not isinstance(range_fixed, str) or \\\n                    len(range_fixed.split("":"")) < 2:\n                    raise ValueError(""{} mode needs \'range_dropped\' to be""\\\n                                    "" set (i.e. 10:20)"".format(dropout_mode))\n                rng = range_fixed.split("":"")\n                beg = int(rng[0])\n                end = int(rng[1])\n                assert beg < end and end <= emb_size, (\n                    ""Incorrect range {}"".format(range_fixed)\n                )\n                self.dropped_dimsize = int(emb_size - (end -beg))\n                tot_idx = np.arange(emb_size)\n                fixed_idx = np.arange(beg, end, 1)\n                sel_idx = np.setdiff1d(tot_idx, fixed_idx, assume_unique=True)\n\n            if self.dropout_fixed:\n                assert len(sel_idx) > 0, (\n                    ""Asked for fixed dropout, but sel_idx {}"".format(sel_idx)\n                )\n                print (""Enabled dropout mode: {}. p={}, drop channels {}. Selected indices to apply dropout are: {}""\\\n                        .format(dropout_mode, self.p, drop_whole_channels, sel_idx))\n                self.dindexes = torch.LongTensor(sel_idx)\n                self.p = p\n                self.p_scale = 1. / (1. - self.p)\n            else:\n                # notice, it is up to the user to make sure experiments between\n                # fixed dropout and regular one are comparabe w.r.t p (i.e. for\n                # fixed mode we only keep droping a subset of units (i.e. 50%),\n                # thus p is effectively lower when compared to regular dropout)\n                self.p =  p\n                print (""Using std dropout with p={}"".format(self.p))\n        else:\n            print (\'Dropout at the inputs disabled, as p={}\'.format(self.p))\n\n    def forward(self, x):\n\n        if self.p == 0 or not self.training:\n            return x\n\n        if self.dropout_fixed and self.training:\n            self.dindexes = self.dindexes.to(x.device)\n            assert len(x.size()) == 3, (\n                ""Expected to get 3 dimensional tensor, got {}""\\\n                   .format(len(x.size()))\n            )\n            bsize, emb_size, tsize = x.size()\n            #print (bsize, esize, tsize)\n            if self.drop_whole_channels:\n                batch_mask = torch.full(size=(bsize, emb_size), fill_value=1.0, device=x.device)\n                probs = torch.full(size=(bsize, self.dropped_dimsize),\n                                  fill_value=1.-self.p)\n                b = Binomial(total_count=1, probs=probs)\n                mask = b.sample()\n                mask = mask.to(x.device)\n                batch_mask[:,self.dindexes] *= (mask * self.p_scale)\n                #print (\'mask dc\', mask)\n                #print (\'maks dcv\', mask.view(bsize, self.dropped_dimsize, -1))\n                #x[:,self.dindexes,:] = x[:,self.dindexes,:].clone() * self.p_scale\\\n                #                         * mask.view(bsize, self.dropped_dimsize, -1)\n                x = x * batch_mask.view(bsize, emb_size, -1)\n            else:\n                batch_mask = torch.ones_like(x, device=x.device)\n                probs = torch.full(size=(bsize, self.dropped_dimsize, tsize), \n                                 fill_value=1.-self.p)\n                b = Binomial(total_count=1, probs=probs)\n                mask = b.sample()\n                mask = mask.to(x.device)\n                batch_mask[:,self.dindexes,:] *= (mask * self.p_scale)\n                x = x * batch_mask\n                #xx = x.data.clone()\n                #x[:,self.dindexes,:] = x[:,self.dindexes,:].clone() * mask * self.p_scale\n            return x\n        else:\n            return F.dropout(x, p=self.p, training=self.training)\n\nclass MLPBlock(NeuralBlock):\n\n    def __init__(self, ninp, fmaps, din=0, dout=0, context=1, \n                 tie_context_weights=False, name=\'MLPBlock\',\n                 ratio_fixed=None, range_fixed=None, \n                 dropin_mode=\'std\', drop_channels=False, emb_size=100):\n        super().__init__(name=name)\n        self.ninp = ninp\n        self.fmaps = fmaps\n        self.tie_context_weights = tie_context_weights\n        assert context % 2 != 0, context\n        if tie_context_weights:\n            self.W = nn.Conv1d(ninp, fmaps, 1)\n            self.pool = nn.AvgPool1d(kernel_size=context, stride=1,\n                                      padding=context//2, count_include_pad=False)\n        else:\n            self.W = nn.Conv1d(ninp, fmaps, context, padding=context//2)\n\n        self.din = PatternedDropout(emb_size=emb_size, p=din, \n                                    dropout_mode=dropin_mode,\n                                    range_fixed=range_fixed,\n                                    ratio_fixed=ratio_fixed,\n                                    drop_whole_channels=drop_channels)\n        self.act = nn.PReLU(fmaps)\n        self.dout = nn.Dropout(dout)\n\n    def forward(self, x, device=None):\n        if self.tie_context_weights:\n            return self.dout(self.act(self.pool(self.W(self.din(x)))))\n        return self.dout(self.act(self.W(self.din(x))))\n\nclass GDeconv1DBlock(NeuralBlock):\n\n    def __init__(self, ninp, fmaps,\n                 kwidth, stride=4, norm_type=None,\n                 act=None,\n                 bias=True,\n                 name=\'GDeconv1DBlock\'):\n        super().__init__(name=name)\n        if act is not None and act == \'glu\':\n            Wfmaps = 2 * fmaps\n        else:\n            Wfmaps = fmaps\n        pad = max(0, (stride - kwidth)//-2)\n        self.deconv = nn.ConvTranspose1d(ninp, Wfmaps,\n                                         kwidth, \n                                         stride=stride,\n                                         padding=pad, \n                                         bias=bias)\n        self.norm = build_norm_layer(norm_type, self.deconv,\n                                     Wfmaps)\n        self.act = build_activation(act, fmaps)\n        self.kwidth = kwidth\n        self.stride = stride\n\n    def forward(self, x):\n        h = self.deconv(x)\n        if (self.stride % 2 != 0 and self.kwidth % 2 == 0) or \\\n           (self.stride % 2 == 0 and self.kwidth % 2 != 0): # and self.stride > self.kwidth:\n            h = h[:, :, :-1]\n        h = forward_norm(h, self.norm)\n        h = forward_activation(self.act, h)\n        return h\n\nclass ResBasicBlock1D(NeuralBlock):\n    """""" Adapted from\n        https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n    """"""\n    expansion = 1\n\n    def __init__(self, inplanes, planes, kwidth=3, \n                 dilation=1, norm_layer=None, name=\'ResBasicBlock1D\'):\n        super().__init__(name=name)\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm1d\n\n        # compute padding given dilation factor\n        P  = (kwidth // 2) * dilation\n        self.conv1 = nn.Conv1d(inplanes, planes, kwidth,\n                               stride=1, padding=P,\n                               bias=False, dilation=dilation)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv1d(planes, planes, kwidth,\n                               padding=P, dilation=dilation,\n                               bias=False)\n        self.bn2 = norm_layer(planes)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out += identity\n        out = self.relu(out)\n        return out\n\nclass ResDilatedModule(NeuralBlock):\n\n    def __init__(self, ninp, fmaps,\n                 res_fmaps,\n                 kwidth, dilation,\n                 norm_type=None,\n                 act=None,\n                 causal=False,\n                 name=\'ResDilatedModule\'):\n        super().__init__(name=name)\n        assert kwidth % 2 != 0\n        self.causal = causal\n        self.dil_conv = nn.Conv1d(ninp, fmaps,\n                                  kwidth, dilation=dilation)\n        if act is not None:\n            self.act = getattr(nn, act)()\n        else:\n            self.act = nn.PReLU(fmaps, init=0)\n        self.dil_norm = build_norm_layer(norm_type, self.dil_conv,\n                                         fmaps)\n        self.kwidth = kwidth\n        self.dilation = dilation\n        # skip 1x1 convolution\n        self.conv_1x1_skip = nn.Conv1d(fmaps, ninp, 1)\n        self.conv_1x1_skip_norm = build_norm_layer(norm_type, \n                                                   self.conv_1x1_skip,\n                                                   ninp)\n        # residual 1x1 convolution\n        self.conv_1x1_res = nn.Conv1d(fmaps, res_fmaps, 1)\n        self.conv_1x1_res_norm = build_norm_layer(norm_type, \n                                                  self.conv_1x1_res,\n                                                  res_fmaps)\n\n    def forward(self, x):\n        if self.causal:\n            kw__1 = self.kwidth - 1\n            P = kw__1 + kw__1 * (self.dilation - 1)\n            # causal padding\n            x_p = F.pad(x, (P, 0))\n        else:\n            kw_2 = self.kwidth // 2\n            P = kw_2 * self.dilation\n            x_p = F.pad(x, (P, P))\n        # dilated conv\n        h = self.dil_conv(x_p)\n        # normalization if applies\n        h = forward_norm(h, self.dil_norm)\n        # activation\n        h = self.act(h)\n        a = h\n        # conv 1x1 to make residual connection\n        h = self.conv_1x1_skip(h)\n        # normalization if applies\n        h = forward_norm(h, self.conv_1x1_skip_norm)\n        # return with skip connection\n        y = x + h\n        # also return res connection (going to further net point directly)\n        sh = self.conv_1x1_res(a)\n        sh = forward_norm(sh, self.conv_1x1_res_norm)\n        return y, sh\n\n# SincNet conv layer\ndef flip(x, dim):\n    xsize = x.size()\n    dim = x.dim() + dim if dim < 0 else dim\n    x = x.contiguous()\n    x = x.view(-1, *xsize[dim:])\n    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, \n                      -1, -1), (\'cpu\',\'cuda\')[x.is_cuda])().long(), :]\n    return x.view(xsize)\n\n\ndef sinc(band,t_right, cuda=False):\n    y_right= torch.sin(2*math.pi*band*t_right)/(2*math.pi*band*t_right)\n    y_left= flip(y_right,0)\n\n    ones = torch.ones(1)\n    if cuda:\n        ones = ones.to(\'cuda\')\n    y=torch.cat([y_left, ones, y_right])\n\n    return y\n    \n    \n# Modified from https://github.com/mravanelli/SincNet\nclass SincConv(nn.Module):\n\n    def __init__(self, N_filt, Filt_dim, fs, stride=1,\n                 padding=\'VALID\', pad_mode=\'reflect\'):\n        super(SincConv, self).__init__()\n\n        # Mel Initialization of the filterbanks\n        low_freq_mel = 80\n        high_freq_mel = (2595 * np.log10(1 + (fs / 2) \\\n                                         / 700))  # Convert Hz to Mel\n        mel_points = np.linspace(low_freq_mel, high_freq_mel, \n                                 N_filt)  # Equally spaced in Mel scale\n        f_cos = (700 * (10 ** (mel_points / 2595) - 1)) # Convert Mel to Hz\n        b1 = np.roll(f_cos, 1)\n        b2 = np.roll(f_cos, -1)\n        b1[0] = 30\n        b2[-1] = (fs / 2) - 100\n                \n        self.freq_scale=fs * 1.0\n        self.filt_b1 = nn.Parameter(torch.from_numpy(b1/self.freq_scale))\n        self.filt_band = nn.Parameter(torch.from_numpy((b2-b1)/self.freq_scale))\n\n        self.N_filt = N_filt\n        self.Filt_dim = Filt_dim\n        self.fs = fs\n        self.padding = padding\n        self.stride =stride\n        self.pad_mode = pad_mode\n        \n    def forward(self, x):\n        cuda = x.is_cuda\n        filters=torch.zeros((self.N_filt, self.Filt_dim))\n        N=self.Filt_dim\n        t_right=torch.linspace(1, (N - 1) / 2, \n                               steps=int((N - 1) / 2)) / self.fs\n        if cuda:\n            filters = filters.to(\'cuda\')\n            t_right = t_right.to(\'cuda\')\n        \n        min_freq=50.0;\n        min_band=50.0;\n        filt_beg_freq = torch.abs(self.filt_b1) + min_freq / self.freq_scale\n        filt_end_freq = filt_beg_freq + (torch.abs(self.filt_band) + \\\n                                         min_band / self.freq_scale)\n        n = torch.linspace(0, N, steps = N)\n        # Filter window (hamming)\n        window=(0.54 - 0.46 * torch.cos(2 * math.pi * n / N)).float()\n        if cuda:\n            window = window.to(\'cuda\')\n        for i in range(self.N_filt):\n            low_pass1 = 2 * filt_beg_freq[i].float()* \\\n                    sinc(filt_beg_freq[i].float() * self.freq_scale, \n                         t_right, cuda)\n            low_pass2 = 2 * filt_end_freq[i].float()* \\\n                    sinc(filt_end_freq[i].float() * self.freq_scale, \n                         t_right, cuda)\n            band_pass=(low_pass2 - low_pass1)\n            band_pass=band_pass/torch.max(band_pass)\n            if cuda:\n                band_pass = band_pass.to(\'cuda\')\n\n            filters[i,:]=band_pass * window\n        if self.padding == \'SAME\':\n            if self.stride > 1:\n                x_p = F.pad(x, (self.Filt_dim // 2 - 1,\n                                self.Filt_dim // 2), mode=self.pad_mode)\n            else:\n                x_p = F.pad(x, (self.Filt_dim // 2,\n                                self.Filt_dim // 2), mode=self.pad_mode)\n        else:\n            x_p = x\n        out = F.conv1d(x_p, filters.view(self.N_filt, 1, self.Filt_dim),\n                       stride=self.stride)\n        return out\n\nclass SincConv_fast(nn.Module):\n    """"""Sinc-based convolution\n    Parameters\n    ----------\n    in_channels : `int`\n        Number of input channels. Must be 1.\n    out_channels : `int`\n        Number of filters.\n    kernel_size : `int`\n        Filter length.\n    sample_rate : `int`, optional\n        Sample rate. Defaults to 16000.\n    Usage\n    -----\n    See `torch.nn.Conv1d`\n    Reference\n    ---------\n    Mirco Ravanelli, Yoshua Bengio,\n    ""Speaker Recognition from raw waveform with SincNet"".\n    https://arxiv.org/abs/1808.00158\n    """"""\n\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride=1, padding=\'VALID\', pad_mode=\'reflect\',\n                 dilation=1, bias=False, groups=1,\n                 sample_rate=16000, min_low_hz=50, min_band_hz=50):\n\n        super(SincConv_fast,self).__init__()\n\n        if in_channels != 1:\n            #msg = (f\'SincConv only support one input channel \'\n            #       f\'(here, in_channels = {in_channels:d}).\')\n            msg = ""SincConv only support one input channel (here, in_channels = {%i})"" % (in_channels)\n            raise ValueError(msg)\n\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        \n        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n        if kernel_size%2==0:\n            self.kernel_size=self.kernel_size+1\n            \n        self.stride = stride\n        self.padding = padding\n        self.pad_mode = pad_mode\n        self.dilation = dilation\n\n        if bias:\n            raise ValueError(\'SincConv does not support bias.\')\n        if groups > 1:\n            raise ValueError(\'SincConv does not support groups.\')\n\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # initialize filterbanks such that they are equally spaced in Mel scale\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n\n        mel = np.linspace(self.to_mel(low_hz),\n                          self.to_mel(high_hz),\n                          self.out_channels + 1)\n        hz = self.to_hz(mel)\n        \n\n        # filter lower frequency (out_channels, 1)\n        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n\n        # filter frequency band (out_channels, 1)\n        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n\n        # Hamming window\n        #self.window_ = torch.hamming_window(self.kernel_size)\n        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\n        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n\n\n        # (kernel_size, 1)\n        n = (self.kernel_size - 1) / 2.0\n        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\n\n \n\n\n    def forward(self, waveforms):\n        """"""\n        Parameters\n        ----------\n        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n            Batch of waveforms.\n        Returns\n        -------\n        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n            Batch of sinc filters activations.\n        """"""\n\n        self.n_ = self.n_.to(waveforms.device)\n\n        self.window_ = self.window_.to(waveforms.device)\n\n        low = self.min_low_hz  + torch.abs(self.low_hz_)\n        \n        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n        band=(high-low)[:,0]\n        \n        f_times_t_low = torch.matmul(low, self.n_)\n        f_times_t_high = torch.matmul(high, self.n_)\n\t\t# Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). \n        # I just have expanded the sinc and simplified the terms. This way I avoid several useless computations.\n        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_  \n        band_pass_center = 2*band.view(-1,1)\n        band_pass_right= torch.flip(band_pass_left,dims=[1])\n        \n        \n        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n\n        \n        band_pass = band_pass / (2*band[:,None])\n        \n\n        self.filters = (band_pass).view(\n            self.out_channels, 1, self.kernel_size)\n\n        x = waveforms \n\n        if self.padding == \'SAME\':\n            if self.stride > 1:\n                x_p = F.pad(x, (self.kernel_size // 2 - 1,\n                                self.kernel_size // 2), mode=self.pad_mode)\n            else:\n                x_p = F.pad(x, (self.kernel_size // 2,\n                                self.kernel_size // 2), mode=self.pad_mode)\n        else:\n            x_p = x\n\n        return F.conv1d(x_p, self.filters, stride=self.stride,\n                        padding=0, dilation=self.dilation,\n                        bias=None, groups=1) \n\nclass FeResBlock(NeuralBlock):\n\n    def __init__(self, num_inputs,\n                 fmaps, kwidth, \n                 dilations=[1, 2],\n                 downsample=1,\n                 pad_mode=\'constant\',\n                 act=None,\n                 norm_type=None,\n                 name=\'FeResBlock\'):\n        super().__init__(name=name)\n        if act is not None and act == \'glu\':\n            Wfmaps = 2 * fmaps\n        else:\n            Wfmaps = fmaps\n        self.num_inputs = num_inputs\n        self.fmaps = fmaps\n        self.kwidth = kwidth\n        downscale = 1. / downsample\n        self.downscale = downscale\n        # stride is ignored for no downsampling is\n        # possible in FeResBlock\n        self.stride = 1\n        #self.dilation = dilation\n        dilation = dilations[0]\n        self.conv1 = nn.Conv1d(num_inputs,\n                               Wfmaps, kwidth,\n                               dilation=dilation,\n                               padding=get_padding(kwidth, dilation))\n        self.norm1 = build_norm_layer(norm_type,\n                                      self.conv1,\n                                      fmaps)\n        self.act1 = build_activation(act, fmaps)\n        dilation = dilations[1]\n        self.conv2 = nn.Conv1d(fmaps, Wfmaps,\n                               kwidth,\n                               dilation=dilation,\n                               padding=get_padding(kwidth, dilation))\n        #assert self.norm2 is not None\n        self.norm2 = build_norm_layer(norm_type,\n                                      self.conv2,\n                                      fmaps)\n        self.act2 = build_activation(act, fmaps)\n        if self.num_inputs != self.fmaps:\n            # build projection layer\n            self.resproj = nn.Conv1d(self.num_inputs,\n                                     self.fmaps, 1)\n\n    def forward(self, x):\n        """"""\n        # compute pad factor\n        if self.kwidth % 2 == 0:\n            if self.dilation > 1:\n                raise ValueError(\'Not supported dilation with even kwdith\')\n            P = (self.kwidth // 2 - 1,\n                 self.kwidth // 2)\n        else:\n            pad = (self.kwidth // 2) * (self.dilation - 1) + \\\n                    (self.kwidth // 2)\n            P = (pad, pad)\n        """"""\n        identity = x\n        #x = F.pad(x, P, mode=self.pad_mode)\n        if self.downscale < 1:\n            x = F.interpolate(x, scale_factor=self.downscale)\n        x = self.conv1(x)\n        x = forward_norm(x, self.norm1)\n        x = forward_activation(self.act1, x)\n        x = self.conv2(x)\n        x = forward_activation(self.act2, x)\n        if hasattr(self, \'resproj\'):\n            identity = self.resproj(identity)\n        if self.downscale < 1:\n            identity = F.interpolate(identity, scale_factor=self.downscale)\n        x = x + identity\n        x = forward_norm(x, self.norm2)\n        return x\n\nclass FeBlock(NeuralBlock):\n\n    def __init__(self, num_inputs,\n                 fmaps, kwidth, stride,\n                 dilation,\n                 pad_mode=\'reflect\',\n                 act=None,\n                 norm_type=None,\n                 sincnet=False,\n                 sr=16000,\n                 name=\'FeBlock\'):\n        super().__init__(name=name)\n        if act is not None and act == \'glu\':\n            Wfmaps = 2 * fmaps\n        else:\n            Wfmaps = fmaps\n        self.num_inputs = num_inputs\n        self.fmaps = fmaps\n        self.kwidth = kwidth\n        self.stride = stride\n        self.dilation = dilation\n        self.pad_mode = pad_mode\n        self.sincnet = sincnet\n        if sincnet:\n            # only one-channel signal can be analyzed\n            assert num_inputs == 1, num_inputs\n            self.conv = SincConv_fast(1, Wfmaps,\n                                      kwidth, \n                                      sample_rate=sr,\n                                      padding=\'SAME\',\n                                      stride=stride,\n                                      pad_mode=pad_mode)\n        else:\n            self.conv = nn.Conv1d(num_inputs,\n                                  Wfmaps,\n                                  kwidth,\n                                  stride,\n                                  dilation=dilation)\n        if not (norm_type == \'snorm\' and sincnet):\n            self.norm = build_norm_layer(norm_type,\n                                         self.conv,\n                                         Wfmaps)\n        self.act = build_activation(act, fmaps)\n\n    def forward(self, x):\n        if self.kwidth > 1 and not self.sincnet:\n            # compute pad factor\n            if self.stride > 1 or self.kwidth % 2 == 0:\n                if self.dilation > 1:\n                    raise ValueError(\'Cannot make dilated convolution with \'\n                                     \'stride > 1\')\n                P = (self.kwidth // 2 - 1,\n                     self.kwidth // 2)\n            else:\n                pad = (self.kwidth // 2) * (self.dilation - 1) + \\\n                        (self.kwidth // 2)\n                P = (pad, pad)\n            x = F.pad(x, P, mode=self.pad_mode)\n        h = self.conv(x)\n        if hasattr(self, \'norm\'):\n            h = forward_norm(h, self.norm)\n        h = forward_activation(self.act, h)\n        #h = self.act(h)\n        return h\n\n\nclass VQEMA(nn.Module):\n    """""" VQ w/ Exp. Moving Averages,\n        as in (https://arxiv.org/pdf/1711.00937.pdf A.1).\n        Partly based on\n        https://github.com/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb\n    """"""\n    def __init__(self, emb_K, emb_dim, beta,\n                 gamma, eps=1e-5):\n        super().__init__()\n        self.emb_K = emb_K\n        self.emb_dim = emb_dim\n        self.emb = nn.Embedding(self.emb_K,\n                                self.emb_dim)\n        self.emb.weight.data.normal_()\n        self.beta = beta\n        self.gamma = gamma\n        self.register_buffer(\'ema_cluster_size\', torch.zeros(emb_K))\n        self.ema_w = nn.Parameter(torch.Tensor(emb_K, emb_dim))\n        self.ema_w.data.normal_()\n\n        self.gamma = gamma\n        self.eps = eps\n\n    def forward(self, inputs):\n        # convert inputs [B, F, T] -> [BxT, F]\n        inputs = inputs.permute(0, 2, 1).contiguous()\n        input_shape = inputs.shape\n        flat_input = inputs.view(-1, self.emb_dim)\n        device = \'cuda\' if inputs.is_cuda else \'cpu\'\n\n        # TODO: UNDERSTAND THIS COMPUTATION\n        # compute distances\n        dist = (torch.sum(flat_input ** 2, dim=1, keepdim=True) + \\\n                torch.sum(self.emb.weight ** 2, dim=1) - \\\n                2 * torch.matmul(flat_input, self.emb.weight.t()))\n\n        # Encoding\n        enc_indices = torch.argmin(dist, dim=1).unsqueeze(1)\n        enc = torch.zeros(enc_indices.shape[0], self.emb_K).to(device)\n        enc.scatter_(1, enc_indices, 1)\n        \n        # Use EMA to update emb vectors\n        if self.training:\n            self.ema_cluster_size = self.ema_cluster_size * self.gamma + \\\n                    (1 - self.gamma) * torch.sum(enc, 0)\n            n = torch.sum(self.ema_cluster_size.data)\n            self.ema_cluster_size = (\n                (self.ema_cluster_size + self.eps) / \\\n                (n + self.emb_K * self.eps) * n\n            )\n            dw = torch.matmul(enc.t(), flat_input)\n            self.ema_w = nn.Parameter(self.ema_w * self.gamma + \\\n                                      (1 - self.gamma) * dw)\n            self.emb.weight = nn.Parameter(self.ema_w / \\\n                                           self.ema_cluster_size.unsqueeze(1))\n\n        # Quantize and reshape\n        Q = torch.matmul(enc, self.emb.weight).view(input_shape)\n\n        # Loss \n        e_latent_loss = torch.mean((Q.detach() - inputs) ** 2)\n        loss = self.beta * e_latent_loss\n\n        Q = inputs + (Q - inputs).detach()\n        avg_probs = torch.mean(enc, dim=0)\n        # perplexity\n        PP = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n        return loss, Q.permute(0, 2, 1).contiguous(), PP, enc\n\nclass SimpleResBlock1D(nn.Module):\n    """""" Based on WaveRNN a publicly available WaveRNN implementation:\n        https://github.com/fatchord/WaveRNN/blob/master/models/fatchord_version.py\n    """"""\n\n    def __init__(self, dims):\n        super().__init__()\n        self.conv1 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n        self.batch_norm1 = nn.BatchNorm1d(dims)\n        self.batch_norm2 = nn.BatchNorm1d(dims)\n\n    def forward(self, x):\n        residual = x\n        x = self.conv1(x)\n        x = self.batch_norm1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = self.batch_norm2(x)\n        return x + residual\n\n\nclass MelResNet(nn.Module):\n    """""" Based on WaveRNN a publicly available WaveRNN implementation:\n        https://github.com/fatchord/WaveRNN/blob/master/models/fatchord_version.py\n    """"""\n\n    def __init__(self, res_blocks, in_dims, compute_dims, res_out_dims, pad):\n        super().__init__()\n        k_size = pad * 2 + 1\n        self.conv_in = nn.Conv1d(in_dims, compute_dims, kernel_size=k_size, bias=False)\n        self.batch_norm = nn.BatchNorm1d(compute_dims)\n        self.layers = nn.ModuleList()\n        for i in range(res_blocks):\n            self.layers.append(SimpleResBlock1D(compute_dims))\n        self.conv_out = nn.Conv1d(compute_dims, res_out_dims, kernel_size=1)\n\n    def forward(self, x):\n        x = self.conv_in(x)\n        x = self.batch_norm(x)\n        x = F.relu(x)\n        for f in self.layers: x = f(x)\n        x = self.conv_out(x)\n        return x\n\nclass Stretch2d(nn.Module):\n    """""" Based on WaveRNN a publicly available WaveRNN implementation:\n        https://github.com/fatchord/WaveRNN/blob/master/models/fatchord_version.py\n    """"""\n\n    def __init__(self, x_scale, y_scale):\n        super().__init__()\n        self.x_scale = x_scale\n        self.y_scale = y_scale\n\n    def forward(self, x):\n        b, c, h, w = x.size()\n        x = x.unsqueeze(-1).unsqueeze(3)\n        # y_scale is feat dim, x_scale is time\n        x = x.repeat(1, 1, 1, self.y_scale, 1, self.x_scale)\n        return x.view(b, c, h * self.y_scale, w * self.x_scale)\n\nclass UpsampleNetwork(nn.Module):\n    """""" Based on WaveRNN a publicly available WaveRNN implementation:\n        https://github.com/fatchord/WaveRNN/blob/master/models/fatchord_version.py\n    """"""\n\n    def __init__(self, feat_dims, upsample_scales=[4, 4, 10], compute_dims=128,\n                 res_blocks=10, res_out_dims=128, pad=2):\n        super().__init__()\n        self.num_outputs = res_out_dims\n        total_scale = np.cumproduct(upsample_scales)[-1]\n        self.indent = pad * total_scale\n        self.resnet = MelResNet(res_blocks, feat_dims, compute_dims, res_out_dims, pad)\n        self.resnet_stretch = Stretch2d(total_scale, 1)\n        self.up_layers = nn.ModuleList()\n        for scale in upsample_scales:\n            k_size = (1, scale * 2 + 1)\n            padding = (0, scale)\n            stretch = Stretch2d(scale, 1)\n            conv = nn.Conv2d(1, 1, kernel_size=k_size, padding=padding, bias=False)\n            conv.weight.data.fill_(1. / k_size[1])\n            self.up_layers.append(stretch)\n            self.up_layers.append(conv)\n\n    def forward(self, m):\n        aux = self.resnet(m).unsqueeze(1)\n        aux = self.resnet_stretch(aux)\n        aux = aux.squeeze(1)\n        m = m.unsqueeze(1)\n        for f in self.up_layers: m = f(m)\n        m = m.squeeze(1)[:, :, self.indent:-self.indent]\n        return m.transpose(1, 2), aux.transpose(1, 2)\n\n\n\nif __name__ == \'__main__\':\n    """"""\n    import matplotlib\n    import matplotlib.pyplot as plt\n    import numpy as np\n    # 800 samples @ 16kHz is 50ms\n    T = 800\n    # n = 20 z time-samples per frame\n    n = 20\n    zgen = ZGen(n, T // n, \n                z_amp=0.5)\n    all_z = None\n    for t in range(0, 200, 5):\n        time_idx = torch.LongTensor([t])\n        z_ten = zgen(time_idx)\n        print(z_ten.size())\n        z_ten = z_ten.squeeze()\n        if all_z is None:\n            all_z = z_ten\n        else:\n            all_z = np.concatenate((all_z, z_ten), axis=1)\n    N = 20\n    for k in range(N):\n        plt.subplot(N, 1, k + 1)\n        plt.plot(all_z[k, :], label=k)\n        plt.ylabel(k)\n    plt.show()\n\n    # ResBlock\n    resblock = ResBlock1D(40, 100, 5, dilation=8)\n    print(resblock)\n    z = z_ten.unsqueeze(0)\n    print(\'Z size: \', z.size())\n    y = resblock(z)\n    print(\'Y size: \', y.size())\n\n    x = torch.randn(1, 1, 16) \n    deconv = GDeconv1DBlock(1, 1, 31)\n    y = deconv(x)\n    print(\'x: {} -> y: {} deconv\'.format(x.size(),\n                                         y.size()))\n    conv = GConv1DBlock(1, 1, 31, stride=4)\n    z = conv(y)\n    print(\'y: {} -> z: {} conv\'.format(y.size(),\n                                       z.size()))\n    """"""\n    #x = torch.randn(1, 1, 16384)\n    #sincnet = SincConv(1024, 251, 16000, padding=\'SAME\')\n    #feblock = FeBlock(1, 100, 251, 1)\n    #y = feblock(x)\n    #print(\'y size: \', y.size())\n    #vq = VQEMA(50, 100, 0.25, 0.99)\n    #x = torch.randn(10, 100, 160)\n    #_, Q, PP , _ = vq(x)\n#    conv = SincConv_fast(1, 10,\n#                         251, \n#                         sample_rate=16000,\n#                         padding=\'SAME\',\n#                         stride=160)\n    #conv = GConv1DBlock(1, 10, 21, 1)\n    #conv = FeResBlock(1, 10, 3, 1, 1)\n    #conv = ResDilatedModule(1, 50, 256, 3, 1024)\n    #x = torch.randn(1, 1, 16000)\n    #print(conv)\n    #y, res = conv(x)\n    #print(y.size())\n    upnet = UpsampleNetwork(100, [4, 4, 10], 256, 10, 100, 2)\n    print(upnet)\n    x = torch.randn(1, 100, 200)\n    z, y = upnet(x)\n    print(x.shape, z.shape, y.shape)\n\n\n\n\n'"
pase/models/neural_networks.py,92,"b'##########################################################\n# pytorch-kaldi v.0.1                                      \n# Mirco Ravanelli, Titouan Parcollet\n# Mila, University of Montreal\n# October 2018\n##########################################################\n\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport numpy as np\nfrom distutils.util import strtobool\nimport math\n\n# uncomment below if you want to use SRU\n# and you need to install SRU: pip install sru[cuda].\n# or you can install it from source code: https://github.com/taolei87/sru.\n# import sru\n\ndef context_window(fea,left,right):\n \n    N_elem=fea.shape[0]\n    N_fea=fea.shape[1]\n    \n    fea_conc=np.empty([N_elem,N_fea*(left+right+1)])\n    \n    index_fea=0\n    for lag in range(-left,right+1):\n        fea_conc[:,index_fea:index_fea+fea.shape[1]]=np.roll(fea,lag,axis=0)\n        index_fea=index_fea+fea.shape[1]\n        \n    fea_conc=fea_conc[left:fea_conc.shape[0]-right]\n    \n    return fea_conc\n\nclass LayerNorm(nn.Module):\n\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm,self).__init__()\n        self.gamma = nn.Parameter(torch.ones(features))\n        self.beta = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\n\ndef act_fun(act_type):\n\n if act_type==""relu"":\n    return nn.ReLU()\n            \n if act_type==""tanh"":\n    return nn.Tanh()\n            \n if act_type==""sigmoid"":\n    return nn.Sigmoid()\n           \n if act_type==""leaky_relu"":\n    return nn.LeakyReLU(0.2)\n            \n if act_type==""elu"":\n    return nn.ELU()\n                     \n if act_type==""softmax"":\n    return nn.LogSoftmax(dim=1)\n        \n if act_type==""linear"":\n     return nn.LeakyReLU(1) # initializzed like this, but not used in forward!\n\n\n\nclass MLP(nn.Module):\n    def __init__(self, options,inp_dim):\n        super(MLP, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.dnn_lay=list(map(int, options[\'dnn_lay\'].split(\',\')))\n        self.dnn_drop=list(map(float, options[\'dnn_drop\'].split(\',\'))) \n        self.dnn_use_batchnorm=list(map(strtobool, options[\'dnn_use_batchnorm\'].split(\',\')))\n        self.dnn_use_laynorm=list(map(strtobool, options[\'dnn_use_laynorm\'].split(\',\'))) \n        self.dnn_use_laynorm_inp=strtobool(options[\'dnn_use_laynorm_inp\'])\n        self.dnn_use_batchnorm_inp=strtobool(options[\'dnn_use_batchnorm_inp\'])\n        self.dnn_act=options[\'dnn_act\'].split(\',\')\n        \n       \n        self.wx  = nn.ModuleList([])\n        self.bn  = nn.ModuleList([])\n        self.ln  = nn.ModuleList([])\n        self.act = nn.ModuleList([])\n        self.drop = nn.ModuleList([])\n       \n  \n        # input layer normalization\n        if self.dnn_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # input batch normalization    \n        if self.dnn_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n           \n        self.N_dnn_lay=len(self.dnn_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_dnn_lay):\n            \n             # dropout\n             self.drop.append(nn.Dropout(p=self.dnn_drop[i]))\n             \n             # activation\n             self.act.append(act_fun(self.dnn_act[i]))\n             \n             \n             add_bias=True\n             \n             # layer norm initialization\n             self.ln.append(LayerNorm(self.dnn_lay[i]))\n             self.bn.append(nn.BatchNorm1d(self.dnn_lay[i],momentum=0.05))\n             \n             if self.dnn_use_laynorm[i] or self.dnn_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Linear operations\n             self.wx.append(nn.Linear(current_input, self.dnn_lay[i],bias=add_bias))\n             \n             # weight initialization\n             self.wx[i].weight = torch.nn.Parameter(torch.Tensor(self.dnn_lay[i],current_input).uniform_(-np.sqrt(0.01/(current_input+self.dnn_lay[i])),np.sqrt(0.01/(current_input+self.dnn_lay[i]))))\n             self.wx[i].bias = torch.nn.Parameter(torch.zeros(self.dnn_lay[i]))\n             \n             current_input=self.dnn_lay[i]\n             \n        self.out_dim=current_input\n         \n    def forward(self, x):\n        \n      # Applying Layer/Batch Norm\n      if bool(self.dnn_use_laynorm_inp):\n        x=self.ln0((x))\n        \n      if bool(self.dnn_use_batchnorm_inp):\n\n        x=self.bn0((x))\n        \n      for i in range(self.N_dnn_lay):\n           \n          if self.dnn_use_laynorm[i] and not(self.dnn_use_batchnorm[i]):\n           x = self.drop[i](self.act[i](self.ln[i](self.wx[i](x))))\n          \n          if self.dnn_use_batchnorm[i] and not(self.dnn_use_laynorm[i]):\n           x = self.drop[i](self.act[i](self.bn[i](self.wx[i](x))))\n           \n          if self.dnn_use_batchnorm[i]==True and self.dnn_use_laynorm[i]==True:\n           x = self.drop[i](self.act[i](self.bn[i](self.ln[i](self.wx[i](x)))))\n          \n          if self.dnn_use_batchnorm[i]==False and self.dnn_use_laynorm[i]==False:\n           x = self.drop[i](self.act[i](self.wx[i](x)))\n            \n          \n      return x\n\n\nclass LSTM_cudnn(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(LSTM_cudnn, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.hidden_size=int(options[\'hidden_size\'])\n        self.num_layers=int(options[\'num_layers\'])\n        self.bias=bool(strtobool(options[\'bias\']))\n        self.batch_first=bool(strtobool(options[\'batch_first\']))\n        self.dropout=float(options[\'dropout\'])\n        self.bidirectional=bool(strtobool(options[\'bidirectional\']))\n        \n        self.lstm = nn.ModuleList([nn.LSTM(self.input_dim, self.hidden_size, self.num_layers, \n                            bias=self.bias,dropout=self.dropout,bidirectional=self.bidirectional)])\n         \n        self.out_dim=self.hidden_size+self.bidirectional*self.hidden_size\n               \n        \n    def forward(self, x):\n        \n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n            c0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            c0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            \n        if x.is_cuda:\n            h0=h0.cuda()\n            c0=c0.cuda()\n            \n        output, (hn, cn) = self.lstm[0](x, (h0, c0))\n        \n        \n        return output\n    \n\nclass GRU_cudnn(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(GRU_cudnn, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.hidden_size=int(options[\'hidden_size\'])\n        self.num_layers=int(options[\'num_layers\'])\n        self.bias=bool(strtobool(options[\'bias\']))\n        self.batch_first=bool(strtobool(options[\'batch_first\']))\n        self.dropout=float(options[\'dropout\'])\n        self.bidirectional=bool(strtobool(options[\'bidirectional\']))\n        \n        self.gru = nn.ModuleList([nn.GRU(self.input_dim, self.hidden_size, self.num_layers, \n                            bias=self.bias,dropout=self.dropout,bidirectional=self.bidirectional)])\n         \n        self.out_dim=self.hidden_size+self.bidirectional*self.hidden_size\n               \n        \n    def forward(self, x):\n        \n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            \n        if x.is_cuda:\n            h0=h0.cuda()\n\n        output, hn = self.gru[0](x, h0)\n        \n        \n        return output\n \n    \nclass RNN_cudnn(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(RNN_cudnn, self).__init__()\n        \n        self.input_dim=inp_dim\n        self.hidden_size=int(options[\'hidden_size\'])\n        self.num_layers=int(options[\'num_layers\'])\n        self.nonlinearity=options[\'nonlinearity\']\n        self.bias=bool(strtobool(options[\'bias\']))\n        self.batch_first=bool(strtobool(options[\'batch_first\']))\n        self.dropout=float(options[\'dropout\'])\n        self.bidirectional=bool(strtobool(options[\'bidirectional\']))\n        \n        self.rnn = nn.ModuleList([nn.RNN(self.input_dim, self.hidden_size, self.num_layers, \n                            nonlinearity=self.nonlinearity,bias=self.bias,dropout=self.dropout,bidirectional=self.bidirectional)])\n         \n        self.out_dim=self.hidden_size+self.bidirectional*self.hidden_size\n               \n        \n    def forward(self, x):\n        \n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers*2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            \n        if x.is_cuda:\n            h0=h0.cuda()\n\n        output, hn = self.rnn[0](x, h0)\n        \n        \n        return output\n    \n    \nclass LSTM(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(LSTM, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.lstm_lay=list(map(int, options[\'lstm_lay\'].split(\',\')))\n        self.lstm_drop=list(map(float, options[\'lstm_drop\'].split(\',\'))) \n        self.lstm_use_batchnorm=list(map(strtobool, options[\'lstm_use_batchnorm\'].split(\',\')))\n        self.lstm_use_laynorm=list(map(strtobool, options[\'lstm_use_laynorm\'].split(\',\'))) \n        self.lstm_use_laynorm_inp=strtobool(options[\'lstm_use_laynorm_inp\'])\n        self.lstm_use_batchnorm_inp=strtobool(options[\'lstm_use_batchnorm_inp\'])\n        self.lstm_act=options[\'lstm_act\'].split(\',\')\n        self.lstm_orthinit=strtobool(options[\'lstm_orthinit\'])\n\n        self.bidir=strtobool(options[\'lstm_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wfx  = nn.ModuleList([]) # Forget\n        self.ufh  = nn.ModuleList([]) # Forget\n        \n        self.wix  = nn.ModuleList([]) # Input\n        self.uih  = nn.ModuleList([]) # Input  \n        \n        self.wox  = nn.ModuleList([]) # Output\n        self.uoh  = nn.ModuleList([]) # Output  \n        \n        self.wcx  = nn.ModuleList([]) # Cell state\n        self.uch = nn.ModuleList([])  # Cell state\n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wfx  = nn.ModuleList([]) # Batch Norm\n        self.bn_wix  = nn.ModuleList([]) # Batch Norm\n        self.bn_wox  = nn.ModuleList([]) # Batch Norm\n        self.bn_wcx = nn.ModuleList([]) # Batch Norm\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.lstm_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.lstm_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_lstm_lay=len(self.lstm_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_lstm_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.lstm_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.lstm_use_laynorm[i] or self.lstm_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wfx.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n             self.wix.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n             self.wox.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n             self.wcx.append(nn.Linear(current_input, self.lstm_lay[i],bias=add_bias))\n            \n             # Recurrent connections\n             self.ufh.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             self.uih.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             self.uoh.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             self.uch.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             \n             if self.lstm_orthinit:\n                nn.init.orthogonal_(self.ufh[i].weight)\n                nn.init.orthogonal_(self.uih[i].weight)\n                nn.init.orthogonal_(self.uoh[i].weight)\n                nn.init.orthogonal_(self.uch[i].weight)\n            \n             \n             # batch norm initialization\n             self.bn_wfx.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n             self.bn_wix.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n             self.bn_wox.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n             self.bn_wcx.append(nn.BatchNorm1d(self.lstm_lay[i],momentum=0.05))\n                \n             self.ln.append(LayerNorm(self.lstm_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.lstm_lay[i]\n             else:\n                 current_input=self.lstm_lay[i]\n                 \n        self.out_dim=self.lstm_lay[i]+self.bidir*self.lstm_lay[i]\n            \n             \n        \n    def forward(self, x):\n\n        # Applying Layer/Batch Norm\n        if bool(self.lstm_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.lstm_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_lstm_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.lstm_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.lstm_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.lstm_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.lstm_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wfx_out=self.wfx[i](x)\n            wix_out=self.wix[i](x)\n            wox_out=self.wox[i](x)\n            wcx_out=self.wcx[i](x)\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.lstm_use_batchnorm[i]:\n\n                wfx_out_bn=self.bn_wfx[i](wfx_out.view(wfx_out.shape[0]*wfx_out.shape[1],wfx_out.shape[2]))\n                wfx_out=wfx_out_bn.view(wfx_out.shape[0],wfx_out.shape[1],wfx_out.shape[2])\n         \n                wix_out_bn=self.bn_wix[i](wix_out.view(wix_out.shape[0]*wix_out.shape[1],wix_out.shape[2]))\n                wix_out=wix_out_bn.view(wix_out.shape[0],wix_out.shape[1],wix_out.shape[2])\n   \n                wox_out_bn=self.bn_wox[i](wox_out.view(wox_out.shape[0]*wox_out.shape[1],wox_out.shape[2]))\n                wox_out=wox_out_bn.view(wox_out.shape[0],wox_out.shape[1],wox_out.shape[2])\n\n                wcx_out_bn=self.bn_wcx[i](wcx_out.view(wcx_out.shape[0]*wcx_out.shape[1],wcx_out.shape[2]))\n                wcx_out=wcx_out_bn.view(wcx_out.shape[0],wcx_out.shape[1],wcx_out.shape[2]) \n            \n            \n            # Processing time steps\n            hiddens = []\n            ct=h_init\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # LSTM equations\n                ft=torch.sigmoid(wfx_out[k]+self.ufh[i](ht))\n                it=torch.sigmoid(wix_out[k]+self.uih[i](ht))\n                ot=torch.sigmoid(wox_out[k]+self.uoh[i](ht))\n                ct=it*self.act[i](wcx_out[k]+self.uch[i](ht))*drop_mask+ft*ct\n                ht=ot*self.act[i](ct)\n                \n                if self.lstm_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n    \nclass GRU(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(GRU, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.gru_lay=list(map(int, options[\'gru_lay\'].split(\',\')))\n        self.gru_drop=list(map(float, options[\'gru_drop\'].split(\',\'))) \n        self.gru_use_batchnorm=list(map(strtobool, options[\'gru_use_batchnorm\'].split(\',\')))\n        self.gru_use_laynorm=list(map(strtobool, options[\'gru_use_laynorm\'].split(\',\'))) \n        self.gru_use_laynorm_inp=strtobool(options[\'gru_use_laynorm_inp\'])\n        self.gru_use_batchnorm_inp=strtobool(options[\'gru_use_batchnorm_inp\'])\n        self.gru_orthinit=strtobool(options[\'gru_orthinit\'])\n        self.gru_act=options[\'gru_act\'].split(\',\')\n        self.bidir=strtobool(options[\'gru_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n        \n        self.wz  = nn.ModuleList([]) # Update Gate\n        self.uz  = nn.ModuleList([]) # Update Gate\n        \n        self.wr  = nn.ModuleList([]) # Reset Gate\n        self.ur  = nn.ModuleList([]) # Reset Gate  \n        \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n        self.bn_wz  = nn.ModuleList([]) # Batch Norm\n        self.bn_wr  = nn.ModuleList([]) # Batch Norm\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.gru_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.gru_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_gru_lay=len(self.gru_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_gru_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.gru_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.gru_use_laynorm[i] or self.gru_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.gru_lay[i],bias=add_bias))\n             self.wz.append(nn.Linear(current_input, self.gru_lay[i],bias=add_bias))\n             self.wr.append(nn.Linear(current_input, self.gru_lay[i],bias=add_bias))\n             \n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.gru_lay[i], self.gru_lay[i],bias=False))\n             self.uz.append(nn.Linear(self.gru_lay[i], self.gru_lay[i],bias=False))\n             self.ur.append(nn.Linear(self.gru_lay[i], self.gru_lay[i],bias=False))\n\n             if self.gru_orthinit:\n                nn.init.orthogonal_(self.uh[i].weight)\n                nn.init.orthogonal_(self.uz[i].weight)\n                nn.init.orthogonal_(self.ur[i].weight)\n\n             \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.gru_lay[i],momentum=0.05))\n             self.bn_wz.append(nn.BatchNorm1d(self.gru_lay[i],momentum=0.05))\n             self.bn_wr.append(nn.BatchNorm1d(self.gru_lay[i],momentum=0.05))\n\n                \n             self.ln.append(LayerNorm(self.gru_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.gru_lay[i]\n             else:\n                 current_input=self.gru_lay[i]\n                 \n        self.out_dim=self.gru_lay[i]+self.bidir*self.gru_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.gru_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.gru_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_gru_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.gru_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.gru_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.gru_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.gru_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            wz_out=self.wz[i](x)\n            wr_out=self.wr[i](x)\n\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.gru_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n                wz_out_bn=self.bn_wz[i](wz_out.view(wz_out.shape[0]*wz_out.shape[1],wz_out.shape[2]))\n                wz_out=wz_out_bn.view(wz_out.shape[0],wz_out.shape[1],wz_out.shape[2])\n   \n                wr_out_bn=self.bn_wr[i](wr_out.view(wr_out.shape[0]*wr_out.shape[1],wr_out.shape[2]))\n                wr_out=wr_out_bn.view(wr_out.shape[0],wr_out.shape[1],wr_out.shape[2])\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # gru equation\n                zt=torch.sigmoid(wz_out[k]+self.uz[i](ht))\n                rt=torch.sigmoid(wr_out[k]+self.ur[i](ht))\n                at=wh_out[k]+self.uh[i](rt*ht)\n                hcand=self.act[i](at)*drop_mask\n                ht=(zt*ht+(1-zt)*hcand)\n                \n                \n                if self.gru_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n\n\nclass liGRU(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(liGRU, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.ligru_lay=list(map(int, options[\'ligru_lay\'].split(\',\')))\n        self.ligru_drop=list(map(float, options[\'ligru_drop\'].split(\',\'))) \n        self.ligru_use_batchnorm=list(map(strtobool, options[\'ligru_use_batchnorm\'].split(\',\')))\n        self.ligru_use_laynorm=list(map(strtobool, options[\'ligru_use_laynorm\'].split(\',\'))) \n        self.ligru_use_laynorm_inp=strtobool(options[\'ligru_use_laynorm_inp\'])\n        self.ligru_use_batchnorm_inp=strtobool(options[\'ligru_use_batchnorm_inp\'])\n        self.ligru_orthinit=strtobool(options[\'ligru_orthinit\'])\n        self.ligru_act=options[\'ligru_act\'].split(\',\')\n        self.bidir=strtobool(options[\'ligru_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n        \n        self.wz  = nn.ModuleList([]) # Update Gate\n        self.uz  = nn.ModuleList([]) # Update Gate\n              \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n        self.bn_wz  = nn.ModuleList([]) # Batch Norm\n\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.ligru_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.ligru_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_ligru_lay=len(self.ligru_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_ligru_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.ligru_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.ligru_use_laynorm[i] or self.ligru_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.ligru_lay[i],bias=add_bias))\n             self.wz.append(nn.Linear(current_input, self.ligru_lay[i],bias=add_bias))\n\n             \n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i],bias=False))\n             self.uz.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i],bias=False))\n\n             if self.ligru_orthinit:\n             \tnn.init.orthogonal_(self.uh[i].weight)\n             \tnn.init.orthogonal_(self.uz[i].weight)\n\n\n             \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.ligru_lay[i],momentum=0.05))\n             self.bn_wz.append(nn.BatchNorm1d(self.ligru_lay[i],momentum=0.05))\n\n\n                \n             self.ln.append(LayerNorm(self.ligru_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.ligru_lay[i]\n             else:\n                 current_input=self.ligru_lay[i]\n                 \n        self.out_dim=self.ligru_lay[i]+self.bidir*self.ligru_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.ligru_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.ligru_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_ligru_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.ligru_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.ligru_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.ligru_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.ligru_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            wz_out=self.wz[i](x)\n\n\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.ligru_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n                wz_out_bn=self.bn_wz[i](wz_out.view(wz_out.shape[0]*wz_out.shape[1],wz_out.shape[2]))\n                wz_out=wz_out_bn.view(wz_out.shape[0],wz_out.shape[1],wz_out.shape[2])\n\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # ligru equation\n                zt=torch.sigmoid(wz_out[k]+self.uz[i](ht))\n                at=wh_out[k]+self.uh[i](ht)\n                hcand=self.act[i](at)*drop_mask\n                ht=(zt*ht+(1-zt)*hcand)\n                \n                \n                if self.ligru_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n    \nclass minimalGRU(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(minimalGRU, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.minimalgru_lay=list(map(int, options[\'minimalgru_lay\'].split(\',\')))\n        self.minimalgru_drop=list(map(float, options[\'minimalgru_drop\'].split(\',\'))) \n        self.minimalgru_use_batchnorm=list(map(strtobool, options[\'minimalgru_use_batchnorm\'].split(\',\')))\n        self.minimalgru_use_laynorm=list(map(strtobool, options[\'minimalgru_use_laynorm\'].split(\',\'))) \n        self.minimalgru_use_laynorm_inp=strtobool(options[\'minimalgru_use_laynorm_inp\'])\n        self.minimalgru_use_batchnorm_inp=strtobool(options[\'minimalgru_use_batchnorm_inp\'])\n        self.minimalgru_orthinit=strtobool(options[\'minimalgru_orthinit\'])\n        self.minimalgru_act=options[\'minimalgru_act\'].split(\',\')\n        self.bidir=strtobool(options[\'minimalgru_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n        \n        self.wz  = nn.ModuleList([]) # Update Gate\n        self.uz  = nn.ModuleList([]) # Update Gate\n              \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n        self.bn_wz  = nn.ModuleList([]) # Batch Norm\n\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.minimalgru_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.minimalgru_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_minimalgru_lay=len(self.minimalgru_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_minimalgru_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.minimalgru_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.minimalgru_use_laynorm[i] or self.minimalgru_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.minimalgru_lay[i],bias=add_bias))\n             self.wz.append(nn.Linear(current_input, self.minimalgru_lay[i],bias=add_bias))\n\n             \n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.minimalgru_lay[i], self.minimalgru_lay[i],bias=False))\n             self.uz.append(nn.Linear(self.minimalgru_lay[i], self.minimalgru_lay[i],bias=False))\n\n             if self.minimalgru_orthinit:\n             \tnn.init.orthogonal_(self.uh[i].weight)\n             \tnn.init.orthogonal_(self.uz[i].weight)\n\n\n             \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.minimalgru_lay[i],momentum=0.05))\n             self.bn_wz.append(nn.BatchNorm1d(self.minimalgru_lay[i],momentum=0.05))\n\n\n                \n             self.ln.append(LayerNorm(self.minimalgru_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.minimalgru_lay[i]\n             else:\n                 current_input=self.minimalgru_lay[i]\n                 \n        self.out_dim=self.minimalgru_lay[i]+self.bidir*self.minimalgru_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.minimalgru_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.minimalgru_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_minimalgru_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.minimalgru_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.minimalgru_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.minimalgru_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.minimalgru_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            wz_out=self.wz[i](x)\n\n\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.minimalgru_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n                wz_out_bn=self.bn_wz[i](wz_out.view(wz_out.shape[0]*wz_out.shape[1],wz_out.shape[2]))\n                wz_out=wz_out_bn.view(wz_out.shape[0],wz_out.shape[1],wz_out.shape[2])\n\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # minimalgru equation\n                zt=torch.sigmoid(wz_out[k]+self.uz[i](ht))\n                at=wh_out[k]+self.uh[i](zt*ht)\n                hcand=self.act[i](at)*drop_mask\n                ht=(zt*ht+(1-zt)*hcand)\n                \n                \n                if self.minimalgru_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n\nclass RNN(nn.Module):\n    \n    def __init__(self, options,inp_dim):\n        super(RNN, self).__init__()\n        \n        # Reading parameters\n        self.input_dim=inp_dim\n        self.rnn_lay=list(map(int, options[\'rnn_lay\'].split(\',\')))\n        self.rnn_drop=list(map(float, options[\'rnn_drop\'].split(\',\'))) \n        self.rnn_use_batchnorm=list(map(strtobool, options[\'rnn_use_batchnorm\'].split(\',\')))\n        self.rnn_use_laynorm=list(map(strtobool, options[\'rnn_use_laynorm\'].split(\',\'))) \n        self.rnn_use_laynorm_inp=strtobool(options[\'rnn_use_laynorm_inp\'])\n        self.rnn_use_batchnorm_inp=strtobool(options[\'rnn_use_batchnorm_inp\'])\n        self.rnn_orthinit=strtobool(options[\'rnn_orthinit\'])\n        self.rnn_act=options[\'rnn_act\'].split(\',\')\n        self.bidir=strtobool(options[\'rnn_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.to_do=options[\'to_do\']\n        \n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n        \n        \n        # List initialization\n        self.wh  = nn.ModuleList([])\n        self.uh  = nn.ModuleList([])\n                   \n        \n        self.ln  = nn.ModuleList([]) # Layer Norm\n        self.bn_wh  = nn.ModuleList([]) # Batch Norm\n\n        \n        self.act  = nn.ModuleList([]) # Activations\n       \n  \n        # Input layer normalization\n        if self.rnn_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n          \n        # Input batch normalization    \n        if self.rnn_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d(self.input_dim,momentum=0.05)\n           \n        self.N_rnn_lay=len(self.rnn_lay)\n             \n        current_input=self.input_dim\n        \n        # Initialization of hidden layers\n        \n        for i in range(self.N_rnn_lay):\n             \n             # Activations\n             self.act.append(act_fun(self.rnn_act[i]))\n            \n             add_bias=True\n             \n             \n             if self.rnn_use_laynorm[i] or self.rnn_use_batchnorm[i]:\n                 add_bias=False\n             \n                  \n             # Feed-forward connections\n             self.wh.append(nn.Linear(current_input, self.rnn_lay[i],bias=add_bias))\n            \n             # Recurrent connections\n             self.uh.append(nn.Linear(self.rnn_lay[i], self.rnn_lay[i],bias=False))\n\n             if self.rnn_orthinit:\n             \tnn.init.orthogonal_(self.uh[i].weight)\n          \n             # batch norm initialization\n             self.bn_wh.append(nn.BatchNorm1d(self.rnn_lay[i],momentum=0.05))\n\n             self.ln.append(LayerNorm(self.rnn_lay[i]))\n                \n             if self.bidir:\n                 current_input=2*self.rnn_lay[i]\n             else:\n                 current_input=self.rnn_lay[i]\n                 \n        self.out_dim=self.rnn_lay[i]+self.bidir*self.rnn_lay[i]\n            \n             \n        \n    def forward(self, x):\n        \n        # Applying Layer/Batch Norm\n        if bool(self.rnn_use_laynorm_inp):\n            x=self.ln0((x))\n        \n        if bool(self.rnn_use_batchnorm_inp):\n            x_bn=self.bn0(x.view(x.shape[0]*x.shape[1],x.shape[2]))\n            x=x_bn.view(x.shape[0],x.shape[1],x.shape[2])\n\n          \n        for i in range(self.N_rnn_lay):\n            \n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.rnn_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.rnn_lay[i])\n        \n               \n            # Drop mask initilization (same mask for all time steps)            \n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.rnn_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.rnn_drop[i]])\n                \n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n               \n                 \n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out=self.wh[i](x)\n            \n            # Apply batch norm if needed (all steos in parallel)\n            if self.rnn_use_batchnorm[i]:\n\n                wh_out_bn=self.bn_wh[i](wh_out.view(wh_out.shape[0]*wh_out.shape[1],wh_out.shape[2]))\n                wh_out=wh_out_bn.view(wh_out.shape[0],wh_out.shape[1],wh_out.shape[2])\n         \n\n\n            \n            # Processing time steps\n            hiddens = []\n            ht=h_init\n            \n            for k in range(x.shape[0]):\n                \n                # rnn equation\n                at=wh_out[k]+self.uh[i](ht)\n                ht=self.act[i](at)*drop_mask\n                \n                \n                if self.rnn_use_laynorm[i]:\n                    ht=self.ln[i](ht)\n                    \n                hiddens.append(ht)\n                \n            # Stacking hidden states\n            h=torch.stack(hiddens)\n            \n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n                \n            # Setup x for the next hidden layer\n            x=h\n\n              \n        return x\n\nclass CNN(nn.Module):\n    \n    def __init__(self,options,inp_dim):\n       super(CNN,self).__init__()\n    \n       # Reading parameters\n       self.input_dim=inp_dim\n       self.cnn_N_filt=list(map(int, options[\'cnn_N_filt\'].split(\',\')))\n\n       self.cnn_len_filt=list(map(int, options[\'cnn_len_filt\'].split(\',\')))\n       self.cnn_max_pool_len=list(map(int, options[\'cnn_max_pool_len\'].split(\',\')))\n       \n       self.cnn_act=options[\'cnn_act\'].split(\',\')\n       self.cnn_drop=list(map(float, options[\'cnn_drop\'].split(\',\')))\n       \n       self.cnn_use_laynorm=list(map(strtobool, options[\'cnn_use_laynorm\'].split(\',\')))\n       self.cnn_use_batchnorm=list(map(strtobool, options[\'cnn_use_batchnorm\'].split(\',\')))\n       self.cnn_use_laynorm_inp=strtobool(options[\'cnn_use_laynorm_inp\'])\n       self.cnn_use_batchnorm_inp=strtobool(options[\'cnn_use_batchnorm_inp\'])\n       \n       self.N_cnn_lay=len(self.cnn_N_filt)\n       self.conv  = nn.ModuleList([])\n       self.bn  = nn.ModuleList([])\n       self.ln  = nn.ModuleList([])\n       self.act = nn.ModuleList([])\n       self.drop = nn.ModuleList([])\n       \n       \n       if self.cnn_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n           \n       if self.cnn_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n           \n       current_input=self.input_dim \n       \n       for i in range(self.N_cnn_lay):\n\n         N_filt=int(self.cnn_N_filt[i])\n         len_filt=int(self.cnn_len_filt[i])\n         \n         # dropout\n         self.drop.append(nn.Dropout(p=self.cnn_drop[i]))\n         \n         # activation\n         self.act.append(act_fun(self.cnn_act[i]))\n            \n         # layer norm initialization         \n         self.ln.append(LayerNorm([N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])]))\n\n         self.bn.append(nn.BatchNorm1d(N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i]),momentum=0.05))\n\n            \n\n         if i==0:\n          self.conv.append(nn.Conv1d(1, N_filt, len_filt))\n              \n         else:\n          self.conv.append(nn.Conv1d(self.cnn_N_filt[i-1], self.cnn_N_filt[i], self.cnn_len_filt[i]))\n          \n         current_input=int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])\n\n         \n       self.out_dim=current_input*N_filt\n\n\n\n    def forward(self, x):\n        \n       batch=x.shape[0]\n       seq_len=x.shape[1]\n       \n       if bool(self.cnn_use_laynorm_inp):\n        x=self.ln0((x))\n        \n       if bool(self.cnn_use_batchnorm_inp):\n        x=self.bn0((x))\n        \n       x=x.view(batch,1,seq_len)\n       \n       for i in range(self.N_cnn_lay):\n           \n         if self.cnn_use_laynorm[i]:\n          x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))\n          \n         if self.cnn_use_batchnorm[i]:\n          x = self.drop[i](self.act[i](self.bn[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))\n\n         if self.cnn_use_batchnorm[i]==False and self.cnn_use_laynorm[i]==False:\n          x = self.drop[i](self.act[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i])))\n\n       \n       x = x.view(batch,-1)\n\n       return x\n\n\nclass SincNet(nn.Module):\n    \n    def __init__(self,options,inp_dim):\n       super(SincNet,self).__init__()\n    \n       # Reading parameters\n       self.input_dim=inp_dim\n       self.sinc_N_filt=list(map(int, options[\'sinc_N_filt\'].split(\',\')))\n\n       self.sinc_len_filt=list(map(int, options[\'sinc_len_filt\'].split(\',\')))\n       self.sinc_max_pool_len=list(map(int, options[\'sinc_max_pool_len\'].split(\',\')))\n       \n       self.sinc_act=options[\'sinc_act\'].split(\',\')\n       self.sinc_drop=list(map(float, options[\'sinc_drop\'].split(\',\')))\n       \n       self.sinc_use_laynorm=list(map(strtobool, options[\'sinc_use_laynorm\'].split(\',\')))\n       self.sinc_use_batchnorm=list(map(strtobool, options[\'sinc_use_batchnorm\'].split(\',\')))\n       self.sinc_use_laynorm_inp=strtobool(options[\'sinc_use_laynorm_inp\'])\n       self.sinc_use_batchnorm_inp=strtobool(options[\'sinc_use_batchnorm_inp\'])\n       \n       self.N_sinc_lay=len(self.sinc_N_filt)\n       \n       self.sinc_sample_rate=int(options[\'sinc_sample_rate\'])\n       self.sinc_min_low_hz=int(options[\'sinc_min_low_hz\'])\n       self.sinc_min_band_hz=int(options[\'sinc_min_band_hz\'])\n\n       \n       self.conv  = nn.ModuleList([])\n       self.bn  = nn.ModuleList([])\n       self.ln  = nn.ModuleList([])\n       self.act = nn.ModuleList([])\n       self.drop = nn.ModuleList([])\n       \n       \n       if self.sinc_use_laynorm_inp:\n           self.ln0=LayerNorm(self.input_dim)\n           \n       if self.sinc_use_batchnorm_inp:\n           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n           \n       current_input=self.input_dim \n       \n       for i in range(self.N_sinc_lay):\n\n         N_filt=int(self.sinc_N_filt[i])\n         len_filt=int(self.sinc_len_filt[i])\n         \n         # dropout\n         self.drop.append(nn.Dropout(p=self.sinc_drop[i]))\n         \n         # activation\n         self.act.append(act_fun(self.sinc_act[i]))\n            \n         # layer norm initialization         \n         self.ln.append(LayerNorm([N_filt,int((current_input-self.sinc_len_filt[i]+1)/self.sinc_max_pool_len[i])]))\n\n         self.bn.append(nn.BatchNorm1d(N_filt,int((current_input-self.sinc_len_filt[i]+1)/self.sinc_max_pool_len[i]),momentum=0.05))\n\n            \n\n         if i==0:\n          self.conv.append(SincConv(1, N_filt, len_filt,sample_rate=self.sinc_sample_rate, min_low_hz=self.sinc_min_low_hz, min_band_hz=self.sinc_min_band_hz))\n              \n         else:\n          self.conv.append(nn.Conv1d(self.sinc_N_filt[i-1], self.sinc_N_filt[i], self.sinc_len_filt[i]))\n          \n         current_input=int((current_input-self.sinc_len_filt[i]+1)/self.sinc_max_pool_len[i])\n\n         \n       self.out_dim=current_input*N_filt\n\n\n\n    def forward(self, x):\n        \n       batch=x.shape[0]\n       seq_len=x.shape[1]\n       \n       if bool(self.sinc_use_laynorm_inp):\n        x=self.ln0(x)\n        \n       if bool(self.sinc_use_batchnorm_inp):\n        x=self.bn0(x)\n        \n       x=x.view(batch,1,seq_len)\n       \n       for i in range(self.N_sinc_lay):\n           \n         if self.sinc_use_laynorm[i]:\n          x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i]))))\n          \n         if self.sinc_use_batchnorm[i]:\n          x = self.drop[i](self.act[i](self.bn[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i]))))\n\n         if self.sinc_use_batchnorm[i]==False and self.sinc_use_laynorm[i]==False:\n          x = self.drop[i](self.act[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i])))\n\n       \n       x = x.view(batch,-1)\n\n       return x\n\n\n\nclass SincConv(nn.Module):\n    """"""Sinc-based convolution\n    Parameters\n    ----------\n    in_channels : `int`\n        Number of input channels. Must be 1.\n    out_channels : `int`\n        Number of filters.\n    kernel_size : `int`\n        Filter length.\n    sample_rate : `int`, optional\n        Sample rate. Defaults to 16000.\n    Usage\n    -----\n    See `torch.nn.Conv1d`\n    Reference\n    ---------\n    Mirco Ravanelli, Yoshua Bengio,\n    ""Speaker Recognition from raw waveform with SincNet"".\n    https://arxiv.org/abs/1808.00158\n    """"""\n\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride=1, padding=0, dilation=1, bias=False, groups=1,\n                 sample_rate=16000, min_low_hz=50, min_band_hz=50):\n\n        super(SincConv,self).__init__()\n\n        if in_channels != 1:\n            #msg = (f\'SincConv only support one input channel \'\n            #       f\'(here, in_channels = {in_channels:d}).\')\n            msg = ""SincConv only support one input channel (here, in_channels = {%i})"" % (in_channels)\n            raise ValueError(msg)\n\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        \n        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n        if kernel_size%2==0:\n            self.kernel_size=self.kernel_size+1\n            \n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n\n        if bias:\n            raise ValueError(\'SincConv does not support bias.\')\n        if groups > 1:\n            raise ValueError(\'SincConv does not support groups.\')\n\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # initialize filterbanks such that they are equally spaced in Mel scale\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n\n        mel = np.linspace(self.to_mel(low_hz),\n                          self.to_mel(high_hz),\n                          self.out_channels + 1)\n        hz = self.to_hz(mel) / self.sample_rate\n        \n\n        # filter lower frequency (out_channels, 1)\n        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n\n        # filter frequency band (out_channels, 1)\n        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n\n        # Hamming window\n        #self.window_ = torch.hamming_window(self.kernel_size)\n        n_lin=torch.linspace(0, self.kernel_size, steps=self.kernel_size)\n        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n\n        # (kernel_size, 1)\n        n = (self.kernel_size - 1) / 2\n        self.n_ = torch.arange(-n, n+1).view(1, -1) / self.sample_rate\n\n\n    def sinc(self, x):\n        # Numerically stable definition\n        x_left=x[:,0:int((x.shape[1]-1)/2)]\n        y_left=torch.sin(x_left) / x_left\n        y_right= torch.flip(y_left,dims=[1])\n        \n        sinc=torch.cat([y_left,torch.ones([x.shape[0],1]).to(x.device),y_right],dim=1)\n        \n\n        return sinc\n\n    def forward(self, waveforms):\n        """"""\n        Parameters\n        ----------\n        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n            Batch of waveforms.\n        Returns\n        -------\n        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n            Batch of sinc filters activations.\n        """"""\n\n        self.n_ = self.n_.to(waveforms.device)\n\n        self.window_ = self.window_.to(waveforms.device)\n\n        low = self.min_low_hz / self.sample_rate + torch.abs(self.low_hz_)\n        high = low + self.min_band_hz /self.sample_rate + torch.abs(self.band_hz_)\n\n        f_times_t = torch.matmul(low, self.n_)\n\n        low_pass1 = 2 * low * self.sinc(\n            2 * math.pi * f_times_t * self.sample_rate)\n        \n\n\n        f_times_t = torch.matmul(high, self.n_)\n        low_pass2 = 2 * high * self.sinc(\n            2 * math.pi * f_times_t * self.sample_rate)\n        \n\n        band_pass = low_pass2 - low_pass1\n        max_, _ = torch.max(band_pass, dim=1, keepdim=True)\n        band_pass = band_pass / max_\n\n        self.filters = (band_pass * self.window_).view(\n            self.out_channels, 1, self.kernel_size)\n\n        return F.conv1d(waveforms, self.filters, stride=self.stride,\n                        padding=self.padding, dilation=self.dilation,\n                         bias=None, groups=1) \n        \n        \nclass SincConv_fast(nn.Module):\n    """"""Sinc-based convolution\n    Parameters\n    ----------\n    in_channels : `int`\n        Number of input channels. Must be 1.\n    out_channels : `int`\n        Number of filters.\n    kernel_size : `int`\n        Filter length.\n    sample_rate : `int`, optional\n        Sample rate. Defaults to 16000.\n    Usage\n    -----\n    See `torch.nn.Conv1d`\n    Reference\n    ---------\n    Mirco Ravanelli, Yoshua Bengio,\n    ""Speaker Recognition from raw waveform with SincNet"".\n    https://arxiv.org/abs/1808.00158\n    """"""\n\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride=1, padding=0, dilation=1, bias=False, groups=1,\n                 sample_rate=16000, min_low_hz=50, min_band_hz=50):\n\n        super(SincConv_fast,self).__init__()\n\n        if in_channels != 1:\n            #msg = (f\'SincConv only support one input channel \'\n            #       f\'(here, in_channels = {in_channels:d}).\')\n            msg = ""SincConv only support one input channel (here, in_channels = {%i})"" % (in_channels)\n            raise ValueError(msg)\n\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        \n        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n        if kernel_size%2==0:\n            self.kernel_size=self.kernel_size+1\n            \n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n\n        if bias:\n            raise ValueError(\'SincConv does not support bias.\')\n        if groups > 1:\n            raise ValueError(\'SincConv does not support groups.\')\n\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # initialize filterbanks such that they are equally spaced in Mel scale\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n\n        mel = np.linspace(self.to_mel(low_hz),\n                          self.to_mel(high_hz),\n                          self.out_channels + 1)\n        hz = self.to_hz(mel)\n        \n\n        # filter lower frequency (out_channels, 1)\n        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n\n        # filter frequency band (out_channels, 1)\n        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n\n        # Hamming window\n        #self.window_ = torch.hamming_window(self.kernel_size)\n        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\n        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n\n\n        # (kernel_size, 1)\n        n = (self.kernel_size - 1) / 2.0\n        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\n\n \n\n\n    def forward(self, waveforms):\n        """"""\n        Parameters\n        ----------\n        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n            Batch of waveforms.\n        Returns\n        -------\n        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n            Batch of sinc filters activations.\n        """"""\n\n        self.n_ = self.n_.to(waveforms.device)\n\n        self.window_ = self.window_.to(waveforms.device)\n\n        low = self.min_low_hz  + torch.abs(self.low_hz_)\n        \n        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n        band=(high-low)[:,0]\n        \n        f_times_t_low = torch.matmul(low, self.n_)\n        f_times_t_high = torch.matmul(high, self.n_)\n\n        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations. \n        band_pass_center = 2*band.view(-1,1)\n        band_pass_right= torch.flip(band_pass_left,dims=[1])\n        \n        \n        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n\n        \n        band_pass = band_pass / (2*band[:,None])\n        \n\n        self.filters = (band_pass).view(\n            self.out_channels, 1, self.kernel_size)\n\n        return F.conv1d(waveforms, self.filters, stride=self.stride,\n                        padding=self.padding, dilation=self.dilation,\n                         bias=None, groups=1) \n\n        \ndef flip(x, dim):\n    xsize = x.size()\n    dim = x.dim() + dim if dim < 0 else dim\n    x = x.contiguous()\n    x = x.view(-1, *xsize[dim:])\n    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, -1, -1), (\'cpu\',\'cuda\')[x.is_cuda])().long(), :]\n    return x.view(xsize)\n\nclass SRU(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(SRU, self).__init__()\n        self.input_dim = inp_dim\n        self.hidden_size = int(options[\'sru_hidden_size\'])\n        self.num_layers = int(options[\'sru_num_layers\'])\n        self.dropout = float(options[\'sru_dropout\'])\n        self.rnn_dropout = float(options[\'sru_rnn_dropout\'])\n        self.use_tanh = bool(strtobool(options[\'sru_use_tanh\']))\n        self.use_relu = bool(strtobool(options[\'sru_use_relu\']))\n        self.use_selu = bool(strtobool(options[\'sru_use_selu\']))\n        self.weight_norm = bool(strtobool(options[\'sru_weight_norm\']))\n        self.layer_norm = bool(strtobool(options[\'sru_layer_norm\']))\n        self.bidirectional = bool(strtobool(options[\'sru_bidirectional\']))\n        self.is_input_normalized = bool(strtobool(options[\'sru_is_input_normalized\']))\n        self.has_skip_term = bool(strtobool(options[\'sru_has_skip_term\']))\n        self.rescale = bool(strtobool(options[\'sru_rescale\']))\n        self.highway_bias = float(options[\'sru_highway_bias\'])\n        self.n_proj = int(options[\'sru_n_proj\'])\n        self.sru = sru.SRU(self.input_dim, self.hidden_size,\n                            num_layers=self.num_layers,\n                            dropout=self.dropout,\n                            rnn_dropout=self.rnn_dropout,\n                            bidirectional=self.bidirectional,\n                            n_proj=self.n_proj,\n                            use_tanh=self.use_tanh,\n                            use_selu=self.use_selu,\n                            use_relu=self.use_relu,\n                            weight_norm=self.weight_norm,\n                            layer_norm=self.layer_norm,\n                            has_skip_term=self.has_skip_term,\n                            is_input_normalized=self.is_input_normalized,\n                            highway_bias=self.highway_bias,\n                            rescale=self.rescale)\n        self.out_dim = self.hidden_size+self.bidirectional*self.hidden_size\n\n    def forward(self, x):\n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size*2)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n        if x.is_cuda:\n            h0 = h0.cuda()\n        output, hn = self.sru(x, c0=h0)\n        return output\n\n'"
pase/models/pase.py,1,"b'try:\n    from .Minions.minions import *\n    from .Minions.cls_minions import *\n    from .attention_block import attention_block\n    from .frontend import wf_builder\n    from .WorkerScheduler.encoder import *\nexcept ImportError:\n    from Minions.minions import *\n    from Minions.cls_minions import *\n    from attention_block import attention_block\n    from frontend import wf_builder\n    from WorkerScheduler.encoder import *\nimport numpy as np\nimport torch\nimport soundfile as sf\n\nclass pase_attention(Model):\n\n    def __init__(self,\n                 frontend=None,\n                 frontend_cfg=None,\n                 att_cfg=None,\n                 minions_cfg=None,\n                 cls_lst=[""mi"", ""cmi"", ""spc""],\n                 regr_lst=[""chunk"", ""lps"", ""mfcc"", ""prosody""],\n                 adv_lst=[],\n                 K=40,\n                 att_mode=""concat"",\n                 avg_factor=0,\n                 chunk_size=16000,\n                 pretrained_ckpt=None,\n                 name=""adversarial""):\n\n        super().__init__(name=name)\n        if minions_cfg is None or len(minions_cfg) < 1:\n            raise ValueError(\'Please specify a stack of minions\'\n                             \' config with at least 1 minion. \'\n                             \'GIMME SOMETHING TO DO.\')\n\n        # init frontend\n        print(frontend_cfg)\n        self.frontend = wf_builder(frontend_cfg)\n\n        # init all workers\n        # putting them into 3 lists\n        self.cls_lst = cls_lst\n        self.reg_lst = regr_lst\n        self.adv_lst = adv_lst\n\n        ninp = self.frontend.emb_dim\n        self.regression_workers = nn.ModuleList()\n        self.classification_workers = nn.ModuleList()\n        self.adversarial_workers = nn.ModuleList()\n        self.attention_blocks = nn.ModuleList()\n\n        # nn_input = self.cal_nn_input_dim(frontend_cfg[\'strides\'], chunk_size)\n\n        # auto infer the output dim of first nn layer\n        att_cfg[\'dnn_lay\'] += "","" + str(ninp)\n\n        for type, cfg_lst in minions_cfg.items():\n            for cfg in cfg_lst:\n\n                if type == \'cls\':\n                    cfg[\'num_inputs\'] = ninp\n                    self.classification_workers.append(cls_worker_maker(cfg, ninp))\n                    self.attention_blocks.append(attention_block(ninp, cfg[\'name\'], att_cfg, K, frontend_cfg[\'strides\'], chunk_size, avg_factor,att_mode))\n\n                elif type == \'regr\':\n                    cfg[\'num_inputs\'] = ninp\n                    minion = minion_maker(cfg)\n                    self.regression_workers.append(minion)\n                    self.attention_blocks.append(attention_block(ninp, cfg[\'name\'], att_cfg, K, frontend_cfg[\'strides\'], chunk_size, avg_factor,att_mode))\n\n                elif type == \'adv\':\n                    cfg[\'num_inputs\'] = ninp\n                    minion = minion_maker(cfg)\n                    self.adversarial_workers.append(minion)\n                    self.attention_blocks.append(attention_block(ninp, cfg[\'name\'], att_cfg, K, frontend_cfg[\'strides\'], chunk_size, avg_factor,att_mode))\n\n                else:\n                    raise TypeError(\'Unrecognized worker type: \', type)\n\n        if pretrained_ckpt is not None:\n            self.load_pretrained(pretrained_ckpt, load_last=True)\n\n    def forward(self, x, alpha=1, device=None):\n\n        # forward the encoder\n        # x[chunk, context, rand] => y[chunk, context, rand], chunk\n\n        h, chunk = self.frontend(x, device)\n\n        # forward all attention blocks\n        # chunk => new_chunk, indices\n        new_hidden = {}\n        for att_block in self.attention_blocks:\n            hidden, indices = att_block(chunk, device)\n            new_hidden[att_block.name] = (hidden, indices)\n\n        # forward all classification workers\n        # h => chunk\n\n        preds = {}\n        labels = {}\n        for worker in self.regression_workers:\n            hidden, _ = new_hidden[worker.name]\n            y = worker(hidden, alpha)\n            preds[worker.name] = y\n            labels[worker.name] = x[worker.name].to(device).detach()\n            if worker.name == \'chunk\':\n                labels[worker.name] = x[\'cchunk\'].to(device).detach()\n\n        # forward all regression workers\n        # h => y, label\n\n        for worker in self.classification_workers:\n            hidden, mask = new_hidden[worker.name]\n            h = [hidden, h[1] * mask, h[2] * mask]\n            if worker.name == ""spc"":\n                y, label = worker(hidden, alpha, device)\n            elif worker.name == ""overlap"":\n                y = worker(hidden, alpha)\n                label = x[worker.name].to(device).detach()\n            else:\n                y, label = worker(h, alpha, device=device)\n            preds[worker.name] = y\n            labels[worker.name] = label\n\n        return h, chunk, preds, labels\n\n\nclass pase_chunking(Model):\n\n    def __init__(self,\n                 frontend=None,\n                 frontend_cfg=None,\n                 minions_cfg=None,\n                 cls_lst=[""mi"", ""cmi"", ""spc""],\n                 regr_lst=[""chunk"", ""lps"", ""mfcc"", ""prosody""],\n                 chunk_size=None,\n                 batch_size=None,\n                 pretrained_ckpt=None,\n                 name=""adversarial""):\n\n        super().__init__(name=name)\n        if minions_cfg is None or len(minions_cfg) < 1:\n            raise ValueError(\'Please specify a stack of minions\'\n                             \' config with at least 1 minion. \'\n                             \'GIMME SOMETHING TO DO.\')\n\n        # init frontend\n        if \'aspp\' in frontend_cfg.keys():\n            self.frontend = aspp_encoder(sinc_out=frontend_cfg[\'sinc_out\'], hidden_dim=frontend_cfg[\'hidden_dim\'])\n        elif \'aspp_res\' in frontend_cfg.keys():\n            self.frontend = aspp_res_encoder(sinc_out=frontend_cfg[\'sinc_out\'],\n                                             hidden_dim=frontend_cfg[\'hidden_dim\'],\n                                             stride=frontend_cfg[\'strides\'],\n                                             rnn_pool=frontend_cfg[\'rnn_pool\'])\n        else:\n            self.frontend = encoder(WaveFe(**frontend_cfg))\n\n        # init all workers\n        # putting them into two lists\n        self.cls_lst = cls_lst\n        self.reg_lst = regr_lst\n\n        self.ninp = self.frontend.emb_dim\n        self.regression_workers = nn.ModuleList()\n        self.classification_workers = nn.ModuleList()\n\n        self.K = chunk_size\n        self.chunk_masks = None\n        for cfg in minions_cfg:\n\n            if cfg[""name""] in self.cls_lst:\n                self.classification_workers.append(cls_worker_maker(cfg, ninp))\n\n            elif cfg[""name""] in self.reg_lst:\n                cfg[\'num_inputs\'] = self.ninp\n                minion = minion_maker(cfg)\n                self.regression_workers.append(minion)\n\n        if pretrained_ckpt is not None:\n            self.load_pretrained(pretrained_ckpt, load_last=True)\n\n    def forward(self, x, device):\n\n        if self.chunk_masks is None:\n            for worker in self.regression_workers:\n                self.chunk_masks[worker.name] = self.generate_mask(worker.name, x).to(device)\n            for worker in self.classification_workers:\n                self.chunk_masks[worker.name] = self.generate_mask(worker.name, x).to(device)\n\n        # forward the encoder\n        # x[chunk, context, rand] => y[chunk, context, rand], chunk\n\n        h, chunk = self.frontend(x, device)\n\n        # forward all classification workers\n        # h => chunk\n\n        preds = {}\n        labels = {}\n        for worker in self.regression_workers:\n            chunk = chunk * self.chunk_masks[worker.name]\n            y = worker(chunk)\n            preds[worker.name] = y\n            labels[worker.name] = x[worker.name].to(device).detach()\n            if worker.name == \'chunk\':\n                labels[worker.name] =  x[\'cchunk\'].to(device).detach()\n\n        # forward all regression workers\n        # h => y, label\n\n        for worker in self.classification_workers:\n            h = [h[0] * self.chunk_masks[worker.name], h[1] * self.chunk_masks[worker.name], h[2] * self.chunk_masks[worker.name]]\n            chunk = h[0]\n            if worker.name == ""spc"":\n                y, label = worker(chunk, device)\n            else:\n                y, label = worker(h, device)\n            preds[worker.name] = y\n            labels[worker.name] = label\n\n        return h, chunk, preds, labels\n\n    def generate_mask(self, name, x):\n        selection_mask = np.zeros(self.ninp)\n        selection_mask[:self.K] = 1\n        selection_mask = np.random.shuffle(selection_mask)\n        mask = torch.zeros(x.size())\n        for i in range(self.K):\n            mask[:, selection_mask[i], :] = 1\n        print(""generated masks for {}: {}"".format(name, selection_mask))\n        return mask\n\n\n\n\nclass pase(Model):\n\n    def __init__(self,\n                 frontend=None,\n                 frontend_cfg=None,\n                 minions_cfg=None,\n                 cls_lst=[""mi"", ""cmi"", ""spc""],\n                 regr_lst=[""chunk"", ""lps"", ""mfcc"", ""prosody""],\n                 pretrained_ckpt=None,\n                 name=""adversarial""):\n        super().__init__(name=name)\n        if minions_cfg is None or len(minions_cfg) < 1:\n            raise ValueError(\'Please specify a stack of minions\'\n                             \' config with at least 1 minion. \'\n                             \'GIMME SOMETHING TO DO.\')\n\n        # init frontend\n        print(""pase config ==>"", frontend_cfg)\n        self.frontend = wf_builder(frontend_cfg)\n\n\n        # init all workers\n        # putting them into two lists\n        self.cls_lst = cls_lst\n        self.reg_lst = regr_lst\n\n        ninp = self.frontend.emb_dim\n        self.regression_workers = nn.ModuleList()\n        self.classification_workers = nn.ModuleList()\n        # these are unparameterized\n        self.regularizer_workers = []\n        self.fwd_cchunk = False\n\n        count_cat = 0\n        if ""concat"" in frontend_cfg.keys():\n            for cat in frontend_cfg[\'concat\']:\n                if cat:\n                    count_cat += 1\n        if count_cat == 0:\n            count_cat = 1\n\n        ninp *= count_cat\n\n        print(""==>concat features from {} levels"".format(count_cat))\n        print(""==>input size for workers: {}"".format(ninp))\n\n        for type, cfg_lst in minions_cfg.items():\n\n            for cfg in cfg_lst:\n\n                if type == \'cls\':\n                    cfg[\'num_inputs\'] = ninp\n                    self.classification_workers.append(cls_worker_maker(cfg, ninp))\n\n                elif type == \'regr\':\n                    cfg[\'num_inputs\'] = ninp\n                    minion = minion_maker(cfg)\n                    self.regression_workers.append(minion)\n                \n                elif type == \'regu\':\n                    if \'cchunk\' in cfg[\'name\']:\n                        # cchunk will be necessary\n                        self.fwd_cchunk = True\n                    minion = minion_maker(cfg)\n                    self.regularizer_workers.append(minion)\n\n        if pretrained_ckpt is not None:\n            self.load_pretrained(pretrained_ckpt, load_last=True)\n\n    def forward(self, x, alpha=1, device=None):\n\n        # forward the encoder\n        # x[chunk, context, rand, cchunk] => y[chunk, context, rand, cchunk], chunk\n        x_ = dict((k, v) for k, v in x.items())\n        if not self.fwd_cchunk:\n            # remove key if it exists\n            x_.pop(\'cchunk\', None)\n        h = self.frontend(x_, device)\n        if len(h) > 1:\n            assert len(h) == 2, len(h)\n            h, chunk = h\n\n        # forward all classification workers\n        # h => chunk\n        preds = {}\n        labels = {}\n\n        for worker in self.regularizer_workers:\n            preds[worker.name] = chunk\n            # select forwarded data from the PASE frontend according to \n            # from last position which must be \'cchunk\'\n            # This way PASE(chunk) is enforced to fall over PASE(cchunk)\n            labels[worker.name] = h[-1].to(device).detach()\n\n        for worker in self.regression_workers:\n            y = worker(chunk, alpha)\n            preds[worker.name] = y\n            labels[worker.name] = x[worker.name].to(device).detach()\n            if worker.name == \'chunk\':\n                labels[worker.name] = x[worker.name].to(device).detach()\n\n        # forward all regression workers\n        # h => y, label\n\n        for worker in self.classification_workers:\n            if worker.name == ""spc"" or worker.name == ""gap"":\n                y, label = worker(chunk, alpha, device=device)\n            elif worker.name == ""overlap"":\n                y = worker(chunk, alpha)\n                label = x[worker.name].to(device).detach()\n            else:\n                y, label = worker(h, alpha, device=device)\n            preds[worker.name] = y\n            labels[worker.name] = label\n\n        return h, chunk, preds, labels\n'"
pase/models/tdnn.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ntry:\n    from .modules import *\nexcept ImportError:\n    from modules import *\n\n\nclass StatisticalPooling(nn.Module):\n\n    def forward(self, x):\n        # x is 3-D with axis [B, feats, T]\n        mu = x.mean(dim=2, keepdim=True)\n        std = x.std(dim=2, keepdim=True)\n        return torch.cat((mu, std), dim=1)\n\nclass TDNN(Model):\n    # Architecture taken from x-vectors extractor\n    # https://www.danielpovey.com/files/2018_icassp_xvectors.pdf\n    def __init__(self, num_inputs, num_outputs, \n                 method=\'cls\',\n                 name=\'TDNN\'):\n        super().__init__(name=name)\n        self.method = method\n        self.model = nn.Sequential(\n            nn.Conv1d(num_inputs, 512, 5, padding=2),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(512, 512, 3, dilation=2, padding=2),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(512, 512, 3, dilation=3, padding=3),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(512, 512, 1),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(512, 1500, 1),\n            nn.BatchNorm1d(1500),\n            nn.ReLU(inplace=True),\n            StatisticalPooling(),\n            nn.Conv1d(3000, 512, 1),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(512, 512, 1),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Conv1d(512, num_outputs, 1),\n            nn.LogSoftmax(dim=1)\n        )\n        if method == \'cls\':\n            print(\'Using cls TDNN method\')\n        elif method == \'xvector\':\n            # get output features at affine after stats pooling\n            self.model = nn.Sequential(*list(self.model.children())[:-5])\n            print(\'Using xvector TDNN method\')\n        elif method == \'unpooled\':\n            # get output features right before the pooling\n            self.model = nn.Sequential(*list(self.model.children())[:-9])\n            print(\'Using unpooled TDNN method\')\n        else:\n            raise TypeError(\'Unrecognized TDNN method: \', method)\n        self.emb_dim = 1500\n\n    def forward(self, x):\n        return self.model(x)\n\n    def load_pretrained(self, ckpt_path, verbose=True):\n        if self.method != \'cls\':\n            # remove last layers from dict first\n            ckpt = torch.load(ckpt_path, \n                              map_location=lambda storage, loc: storage)\n            sdict = ckpt[\'state_dict\']\n            curr_keys = list(dict(self.named_parameters()).keys())\n            del_keys = [k for k in sdict.keys() if k not in curr_keys]\n            # delete other keys from ckpt\n            for k in del_keys:\n                del sdict[k]\n            # now load the weights remaining as feat extractor\n            self.load_state_dict(sdict)\n        else:\n            # load everything\n            super().load_pretrained(ckpt_path, load_last=True,\n                                    verbose=verbose)\n\nif __name__ == \'__main__\':\n    """"""\n    sp = StatisticalPooling()\n    x = torch.randn(1, 100, 1000)\n    y = sp(x)\n    print(\'y size: \', y.size())\n    tdnn = TDNN(24, 1200, xvector=True)\n    x = torch.randn(1, 24, 27000)\n    y = tdnn(x)\n    print(\'y size: \', y.size())\n    tdnn.load_pretrained(\'/tmp/xvector.ckpt\')\n    """"""\n    x = torch.randn(2, 24, 1000)\n    tdnn = TDNN(24, 2, method=\'unpooled\')\n    print(tdnn(x).shape)\n'"
pase/test/dataset.py,1,"b'from pase.dataset import LibriSpeechSegTupleWavDataset\nfrom pase.transforms import *\n\nfrom argparse import ArgumentParser\n\nfrom torch.utils.data import DataLoader\n\nif __name__ == ""__main__"":\n    parser = ArgumentParser()\n    parser.add_argument(""--data_root"", type=str, required=True)\n    parser.add_argument(""--data_cfg_file"", type=str, required=True)\n    parser.add_argument(""--split"", type=str, default=""train"", help=""train or test?"", required=False)\n    args = parser.parse_args()\n\n    transforms = [ToTensor(), SingleChunkWav(chunk_size=16000), LPS()]\n    trans = Compose(transforms)\n\n    dataset = LibriSpeechSegTupleWavDataset(args.data_root, args.data_cfg_file, args.split, trans)\n\n    dl = DataLoader(dataset)\n    it = iter(dl)\n\n    try:\n        a = next(it)\n        while(a != None):\n            a = next(it)\n            print(a[\'raw\'].shape)\n    except StopIteration:\n        print(""Done"")\n        exit(0)'"
ASR/kaldi_decoding_scripts/utils/filt.py,0,"b'#!/usr/bin/env python\n\n# Apache 2.0\n\nimport sys\n\nvocab=set()\nwith open(sys.argv[1]) as vocabfile:\n    for line in vocabfile:\n        vocab.add(line.strip())\n\nwith open(sys.argv[2]) as textfile:\n    for line in textfile:\n        print "" "".join(map(lambda word: word if word in vocab else \'<UNK>\', line.strip().split()))\n'"
ASR/kaldi_decoding_scripts/utils/reverse_arpa.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Copyright 2012 Mirko Hannemann BUT, mirko.hannemann@gmail.com\n\nimport sys\nimport codecs # for UTF-8/unicode\n\nif len(sys.argv) != 2:\n    print \'usage: reverse_arpa arpa.in\'\n    sys.exit()\narpaname = sys.argv[1]\n\n#\\data\\\n#ngram 1=4\n#ngram 2=2\n#ngram 3=2\n#\n#\\1-grams:\n#-5.234679\ta -3.3\n#-3.456783\tb\n#0.0000000\t<s> -2.5\n#-4.333333\t</s>\n#\n#\\2-grams:\n#-1.45678\ta b -3.23\n#-1.30490\t<s> a -4.2\n#\n#\\3-grams:\n#-0.34958\t<s> a b\n#-0.23940\ta b </s>\n#\\end\\\n\n# read language model in ARPA format\ntry:\n  file = codecs.open(arpaname, ""r"", ""utf-8"")\nexcept IOError:\n  print \'file not found: \' + arpaname\n  sys.exit()\n\ntext=file.readline()\nwhile (text and text[:6] != ""\\\\data\\\\""): text=file.readline()\nif not text:\n  print ""invalid ARPA file""\n  sys.exit()\n#print text,\nwhile (text and text[:5] != ""ngram""): text=file.readline()\n\n# get ngram counts\ncngrams=[]\nn=0\nwhile (text and text[:5] == ""ngram""):\n  ind = text.split(""="")\n  counts = int(ind[1].strip())\n  r = ind[0].split()\n  read_n = int(r[1].strip())\n  if read_n != n+1:\n    print ""invalid ARPA file:"", text\n    sys.exit()\n  n = read_n\n  cngrams.append(counts)\n  #print text,\n  text=file.readline()\n\n# read all n-grams order by order\nsentprob = 0.0 # sentence begin unigram\nngrams=[]\ninf=float(""inf"")\nfor n in range(1,len(cngrams)+1): # unigrams, bigrams, trigrams\n  while (text and ""-grams:"" not in text): text=file.readline()\n  if n != int(text[1]):\n    print ""invalid ARPA file:"", text\n    sys.exit()\n  #print text,cngrams[n-1]\n  this_ngrams={} # stores all read ngrams\n  for ng in range(cngrams[n-1]):\n    while (text and len(text.split())<2):\n      text=file.readline()\n      if (not text) or ((len(text.split())==1) and ((""-grams:"" in text) or (text[:5] == ""\\\\end\\\\""))): break\n    if (not text) or ((len(text.split())==1) and ((""-grams:"" in text) or (text[:5] == ""\\\\end\\\\""))):\n      break # to deal with incorrect ARPA files\n    entry = text.split()\n    prob = float(entry[0])\n    if len(entry)>n+1:\n      back = float(entry[-1])\n      words = entry[1:n+1]\n    else:\n      back = 0.0\n      words = entry[1:]\n    ngram = "" "".join(words)\n    if (n==1) and words[0]==""<s>"":\n      sentprob = prob\n      prob = 0.0\n    this_ngrams[ngram] = (prob,back)\n    #print prob,ngram.encode(""utf-8""),back\n\n    for x in range(n-1,0,-1):\n      # add all missing backoff ngrams for reversed lm\n      l_ngram = "" "".join(words[:x]) # shortened ngram\n      r_ngram = "" "".join(words[1:1+x]) # shortened ngram with offset one\n      if l_ngram not in ngrams[x-1]: # create missing ngram\n        ngrams[x-1][l_ngram] = (0.0,inf)\n        #print ngram, ""create 0.0"", l_ngram, ""inf""\n      if r_ngram not in ngrams[x-1]: # create missing ngram\n        ngrams[x-1][r_ngram] = (0.0,inf)\n        #print ngram, ""create 0.0"", r_ngram, ""inf"",x,n,h_ngram\n\n      # add all missing backoff ngrams for forward lm\n      h_ngram = "" "".join(words[n-x:]) # shortened history\n      if h_ngram not in ngrams[x-1]: # create missing ngram\n        ngrams[x-1][h_ngram] = (0.0,inf)\n        #print ""create inf"", h_ngram, ""0.0""\n    text=file.readline()\n    if (not text) or ((len(text.split())==1) and ((""-grams:"" in text) or (text[:5] == ""\\\\end\\\\""))): break\n  ngrams.append(this_ngrams)\n\nwhile (text and text[:5] != ""\\\\end\\\\""): text=file.readline()\nif not text:\n  print ""invalid ARPA file""\n  sys.exit()\nfile.close()\n#print text,\n\n#fourgram ""maxent"" model (b(ABCD)=0):\n#p(A)+b(A) A 0\n#p(AB)+b(AB)-b(A)-p(B) AB 0\n#p(ABC)+b(ABC)-b(AB)-p(BC) ABC 0\n#p(ABCD)+b(ABCD)-b(ABC)-p(BCD) ABCD 0\n\n#fourgram reverse ARPA model (b(ABCD)=0):\n#p(A)+b(A) A 0\n#p(AB)+b(AB)-p(B)+p(A) BA 0\n#p(ABC)+b(ABC)-p(BC)+p(AB)-p(B)+p(A) CBA 0\n#p(ABCD)+b(ABCD)-p(BCD)+p(ABC)-p(BC)+p(AB)-p(B)+p(A) DCBA 0\n\n# compute new reversed ARPA model\nprint ""\\\\data\\\\""\nfor n in range(1,len(cngrams)+1): # unigrams, bigrams, trigrams\n  print ""ngram ""+str(n)+""=""+str(len(ngrams[n-1].keys()))\noffset = 0.0\nfor n in range(1,len(cngrams)+1): # unigrams, bigrams, trigrams\n  print ""\\\\""+str(n)+""-grams:""\n  keys = ngrams[n-1].keys()\n  keys.sort()\n  for ngram in keys:\n    prob = ngrams[n-1][ngram]\n    # reverse word order\n    words = ngram.split()\n    rstr = "" "".join(reversed(words))\n    # swap <s> and </s>\n    rev_ngram = rstr.replace(""<s>"",""<temp>"").replace(""</s>"",""<s>"").replace(""<temp>"",""</s>"")\n\n    revprob = prob[0]\n    if (prob[1] != inf): # only backoff weights from not newly created ngrams\n      revprob = revprob + prob[1]\n    #print prob[0],prob[1]\n    # sum all missing terms in decreasing ngram order\n    for x in range(n-1,0,-1): \n      l_ngram = "" "".join(words[:x]) # shortened ngram\n      if l_ngram not in ngrams[x-1]:\n        sys.stderr.write(rev_ngram+"": not found ""+l_ngram+""\\n"")\n      p_l = ngrams[x-1][l_ngram][0]\n      #print p_l,l_ngram\n      revprob = revprob + p_l\n\n      r_ngram = "" "".join(words[1:1+x]) # shortened ngram with offset one\n      if r_ngram not in ngrams[x-1]:\n        sys.stderr.write(rev_ngram+"": not found ""+r_ngram+""\\n"")\n      p_r = ngrams[x-1][r_ngram][0]\n      #print -p_r,r_ngram\n      revprob = revprob - p_r\n\n    if n != len(cngrams): #not highest order\n      back = 0.0\n      if rev_ngram[:3] == ""<s>"": # special handling since arpa2fst ignores <s> weight\n        if n == 1:\n          offset = revprob # remember <s> weight\n          revprob = sentprob # apply <s> weight from forward model\n          back = offset\n        elif n == 2:\n          revprob = revprob + offset # add <s> weight to bigrams starting with <s>\n      if (prob[1] != inf): # only backoff weights from not newly created ngrams\n        print revprob,rev_ngram.encode(""utf-8""),back\n      else:\n        print revprob,rev_ngram.encode(""utf-8""),""-100000.0""\n    else: # highest order - no backoff weights\n      if (n==2) and (rev_ngram[:3] == ""<s>""): revprob = revprob + offset\n      print revprob,rev_ngram.encode(""utf-8"")\nprint ""\\\\end\\\\""\n'"
ASR/waveminionet/models/__init__.py,0,b''
ASR/waveminionet/models/core.py,31,"b'from .modules import *\nfrom .frontend import *\nfrom .minions import *\nfrom ..losses import *\nfrom tensorboardX import SummaryWriter\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nimport numpy as np\nimport random\nimport json\nimport timeit\nimport os\n\n\nclass Waveminionet(Model):\n\n    def __init__(self, frontend=None, frontend_cfg=None,\n                 minions_cfg=None, z_minion=True,\n                 z_cfg=None, adv_loss=\'BCE\',\n                 num_devices=1, pretrained_ckpt=None,\n                 name=\'Waveminionet\'):\n        super().__init__(name=name)\n        # augmented wav processing net\n        # it trains simultaneously with many tasks\n        # forcing a hierarchy of abstraction to distill # the contents within waveforms \n        if minions_cfg is None or len(minions_cfg) < 1:\n            raise ValueError(\'Please specify a stack of minions\'\n                             \' config with at least 1 minion. \'\n                             \'GIMME SOMETHING TO DO.\')\n        if frontend is not None:\n            self.frontend = frontend\n        else:\n            if frontend_cfg is None:\n                # default params\n                self.frontend = WaveFe()\n            else:\n                self.frontend = WaveFe(**frontend_cfg)\n        if self.frontend.quantizer is not None:\n            self.vq = True\n        else:\n            self.vq = False\n        # -------- MINION STACK --------\n        self.minions = nn.ModuleList()\n        self.mi_fwd = False\n        ninp = self.frontend.emb_dim\n        self.min2idx = {}\n        for minion_cfg in minions_cfg:\n            if \'mi\' in minion_cfg[\'name\'] and not self.mi_fwd:\n                # add additional code for pair (either CMI or MI)\n                # (just once, thus use mi_fwd flag)\n                ninp += self.frontend.emb_dim\n            minion_cfg[\'num_inputs\'] = ninp\n            minion = minion_maker(minion_cfg)\n            self.minions.append(minion)\n            self.min2idx[minion.name] = len(self.min2idx) \n            if hasattr(minion, \'skip\') and minion.skip:\n                nouts = minion.hidden_size\n                # acumulate num of inputs (concat skip connection)\n                ninp += nouts\n            if \'mi\' in minion.name:\n                # if MI minion is present, multi chunk forward\n                # is needed (3 chunks are fwd)\n                self.mi_fwd = True\n        if z_minion:\n            # Make the minion enforcing the shape of the latent space\n            # to be like some prior z_gen enforced in the loss\n            # This minion is disconnected from others, just enforcing\n            # frontend\'s output to follow Z, but no skip,\n            # and it always backprops even in random backprop selection\n            # as it acts as a regularizer\n            if z_cfg is None:\n                z_cfg = {\n                    \'num_inputs\':self.frontend.emb_dim,\n                    \'num_outputs\':1,\n                    \'dropout\':0.,\n                    \'name\':\'z\',\n                    \'skip\':False,\n                    \'loss\':AdversarialLoss(loss=adv_loss)\n                }\n            self.z_minion = minion_maker(z_cfg)\n            self.z_minion.loss.register_DNet(self.z_minion)\n        if pretrained_ckpt is not None:\n            self.load_pretrained(pretrained_ckpt, load_last=True)\n        if num_devices > 1:\n            self.frontend_dp = nn.DataParallel(self.frontend)\n            self.minions_dp = nn.ModuleList([nn.DataParallel(m) for m in \\\n                                             self.minions])\n\n    def forward(self, x):\n        raise NotImplementedError\n        fe_h = self.frontend(x)\n        #print(\'front-end inference: \', fe_h.size())\n        h = fe_h\n        outs = {}\n        for mi, minion in enumerate(self.minions, start=1):\n            y, h_ = minion(h)\n            if minion.skip:\n                h_c = torch.cat((h, h_), dim=1)\n                h = h_c\n            else:\n                h = h\n            outs[minion.name] = y\n        return outs, h\n\n    def join_skip(self, x, skip):\n        if skip is None:\n            return x\n        else:\n            return torch.cat((x, skip), dim=1)\n\n    def train_(self, dloader, cfg, device=\'cpu\', va_dloader=None):\n        epoch = cfg[\'epoch\']\n        bsize = cfg[\'batch_size\']\n        save_path = cfg[\'save_path\']\n        log_freq = cfg[\'log_freq\']\n        warmup_epoch = cfg[\'warmup\']\n        zinit_weight = cfg[\'zinit_weight\']\n        zinc = cfg[\'zinc\']\n        zweight = 0\n        if hasattr(self, \'frontend_dp\'):\n            frontend = self.frontend_dp\n        else:\n            frontend = self.frontend\n        writer = SummaryWriter(save_path)\n        bpe = cfg[\'bpe\'] if \'bpe\' in cfg else len(dloader)\n        print(\'=\' * 50)\n        print(\'Beginning training...\')\n        print(\'Batches per epoch: \', bpe)\n        # rndmin_train flag means we donly backprop one minion path        \n        # per batch update, selecting the minion randomly\n        rndmin_train = cfg[\'rndmin_train\']\n        print(\'Randomized minion training: \', rndmin_train)\n        feopt = getattr(optim, cfg[\'fe_opt\'])(self.frontend.parameters(), \n                                              lr=cfg[\'fe_lr\'])\n        lrdecay = cfg[\'lrdecay\']\n        if lrdecay > 0:\n            fesched = optim.lr_scheduler.StepLR(feopt,\n                                                step_size=cfg[\'lrdec_step\'],\n                                                gamma=cfg[\'lrdecay\'])\n            #fesched = optim.lr_scheduler.ReduceLROnPlateau(feopt,\n            #                                               mode=\'min\',\n            #                                               factor=lrdecay,\n            #                                               verbose=True)\n        if hasattr(self, \'z_minion\'):\n            z_lr = cfg[\'z_lr\']\n            zopt = getattr(optim, cfg[\'min_opt\'])(self.z_minion.parameters(), \n                                                  lr=z_lr)\n            if lrdecay > 0:\n                zsched = optim.lr_scheduler.ReduceLROnPlateau(zopt,\n                                                              mode=\'min\',\n                                                              factor=lrdecay,\n                                                              verbose=True)\n                zsched = optim.lr_scheduler.StepLR(zopt,\n                                                   step_size=cfg[\'lrdec_step\'],\n                                                   gamma=cfg[\'lrdecay\'])\n\n        if \'min_lrs\' in cfg:\n            min_lrs = cfg[\'min_lrs\']\n        else:\n            min_lrs = None\n        minopts = {}\n        minscheds = {}\n        for mi, minion in enumerate(self.minions, start=1):\n            min_opt = cfg[\'min_opt\']\n            min_lr = cfg[\'min_lr\']\n            if min_lrs is not None and minion.name in min_lrs:\n                min_lr = min_lrs[minion.name]\n                print(\'Applying lr {:.5f} to minion {}\'.format(min_lr,\n                                                               minion.name))\n            minopts[minion.name] = getattr(optim, min_opt)(minion.parameters(),\n                                                           lr=min_lr)\n            if lrdecay > 0:\n                #minsched = lr_scheduler.ReduceLROnPlateau(minopts[minion.name],\n                #                                          mode=\'min\',\n                #                                          factor=lrdecay,\n                #                                          verbose=True)\n                minsched = lr_scheduler.StepLR(minopts[minion.name],\n                                               step_size=cfg[\'lrdec_step\'],\n                                               gamma=cfg[\'lrdecay\'])\n                minscheds[minion.name] = minsched\n\n\n        minions_run = self.minions\n        if hasattr(self, \'minions_dp\'):\n            minions_run = self.minions_dp\n\n        min_global_steps = {}\n        global_step = 0\n        for epoch_ in range(epoch):\n            self.train()\n            timings = []\n            beg_t = timeit.default_timer()\n            min_loss = {}\n            if epoch_ + 1 == warmup_epoch and hasattr(self, \'z_minion\'):\n                zweight = zinit_weight\n\n            for bidx in range(1, bpe + 1):\n                batch = next(dloader.__iter__())\n                feopt.zero_grad()\n                fe_h = {}\n                # forward chunk (alone) through frontend\n                if self.mi_fwd:\n                    # build triplet batch and forward it too\n                    triplet = torch.cat((batch[\'chunk\'],\n                                         batch[\'chunk_ctxt\'],\n                                         batch[\'chunk_rand\']),\n                                        dim=0)\n                    if self.vq:\n                        vq_loss, fe_Q, \\\n                        vq_pp, vq_idx = frontend(triplet.to(device))\n                        fe_h[\'triplet\'] = fe_Q\n                    else:\n                        fe_h[\'triplet\'] = frontend(triplet.to(device))\n                    triplets = torch.chunk(fe_h[\'triplet\'], 3,\n                                           dim=0)\n                    fe_h[\'chunk\'] = triplets[0]\n                else:\n                    if self.vq:\n                        vq_loss, fe_Q, \\\n                        vq_pp, vq_idx = frontend(batch[\'chunk\'].to(device))\n                        fe_h[\'chunk\'] = fe_Q\n                    else:\n                        fe_h[\'chunk\'] = frontend(batch[\'chunk\'].to(device))\n\n                min_h = {}\n                h = fe_h[\'chunk\']\n                skip_acum = None\n                for mi, minion in enumerate(minions_run, start=1):\n                    min_name = self.minions[mi - 1].name\n                    if \'mi\' in min_name:\n                        triplet_P = self.join_skip(torch.cat((triplets[0],\n                                                              triplets[1]),\n                                                             dim=1), skip_acum)\n                        triplet_N = self.join_skip(torch.cat((triplets[0],\n                                                              triplets[2]),\n                                                             dim=1), skip_acum)\n                        triplet_all = torch.cat((triplet_P, triplet_N), dim=0)\n                        if min_name == \'cmi\':\n                            # average through time dimension for ChunkMI\n                            triplet_all = torch.mean(triplet_all, dim=2,\n                                                     keepdim=True)\n                        y = minion(triplet_all)\n                        bsz = y.size(0)//2\n                        slen = y.size(2)\n                        batch[min_name] = torch.cat((torch.ones(bsz, 1, slen),\n                                                     torch.zeros(bsz, 1, slen)),\n                                                    dim=0)\n\n                    else:\n                        if self.minions[mi - 1].skip:\n                            y, h_ = minion(self.join_skip(h, skip_acum))\n                            if skip_acum is None:\n                                skip_acum = h_\n                            else:\n                                skip_acum = torch.cat((skip_acum, h_), dim=1)\n                        else:\n                            y = minion(self.join_skip(h, skip_acum))\n                        if min_name == \'spc\':\n                            # we must create the spc labels, composed of \n                            # B ones and B zeros (future and past). It\n                            # internally creates 2B samples\n                            bsz = y.size(0) // 2\n                            slen = y.size(2)\n                            batch[\'spc\'] = torch.cat((torch.ones(bsz, 1, slen),\n                                                      torch.zeros(bsz, 1,\n                                                                  slen)),\n                                                     dim=0)\n                    min_h[min_name] = y\n\n                if epoch_ + 1 >= warmup_epoch and hasattr(self, \'z_minion\'):\n                    # First shape the hidden space as Z if needed\n                    zopt.zero_grad()\n                    # Adversarial learning to map Fe(wav) to Z ~ prior\n                    dreal_loss, dfake_loss, \\\n                            greal_loss = self.z_minion.loss(fe_h[\'chunk\'],\n                                                            zopt)\n                    d_loss = dreal_loss + dfake_loss\n\n                    greal_loss = zweight * greal_loss\n                   \n                    greal_loss.backward(retain_graph=True)\n                    # update weight incrementally if needed still\n                    zweight = min(1, zweight + zinc)\n                else:\n                    dreal_loss = torch.zeros(1)\n                    dfake_loss = torch.zeros(1)\n                    greal_loss = torch.zeros(1)\n                global_step += 1\n\n                # backprop time\n                if rndmin_train:\n                    min_names = list(min_h.keys())\n                    rnd_min = random.choice(min_names)\n                    minopts[rnd_min].zero_grad()\n                    y_ = min_h[rnd_min]\n                    minion = minions_run[self.min2idx[rnd_min]]\n                    y_lab = batch[rnd_min].to(device)\n                    loss = self.minions[self.min2idx[rnd_min]].loss(y_, y_lab)\n                    loss.backward()\n                    if rnd_min not in min_loss:\n                        min_loss[rnd_min] = []\n                    if rnd_min not in min_global_steps:\n                        min_global_steps[rnd_min] = 0\n                    min_loss[rnd_min].append(loss.item())\n                    min_global_steps[rnd_min] += 1\n                    minopts[rnd_min].step()\n                else:\n                    if hasattr(self, \'minions_dp\'):\n                        raise NotImplementedError(\'DataParallel to be included\')\n                    # Compute all minion losses\n                    for min_name, y_ in min_h.items():\n                        minopts[min_name].zero_grad()\n                        y_lab = batch[min_name].to(device)\n                        loss = self.minions[self.min2idx[min_name]].loss(y_, y_lab)\n                        loss.backward(retain_graph=True)\n                        if min_name not in min_loss:\n                            min_loss[min_name] = []\n                        if min_name not in min_global_steps:\n                            min_global_steps[min_name] = 0\n                        min_loss[min_name].append(loss.item())\n                        min_global_steps[min_name] += 1\n                        minopts[min_name].step()\n                end_t = timeit.default_timer()\n                timings.append(end_t - beg_t)\n                beg_t = timeit.default_timer()\n                if self.vq:\n                    # Backprop VQ related stuff\n                    vq_loss.backward()\n                feopt.step()\n                if bidx % log_freq == 0 or bidx >= bpe:\n                    print(\'-\' * 50)\n                    print(\'Batch {}/{} (Epoch {}):\'.format(bidx, bpe, epoch_))\n                    for min_name, losses in min_loss.items():\n                        print(\'Minion {} loss: {:.3f} gidx: \'\n                              \'{:5d} \'.format(min_name, losses[-1], \n                                              min_global_steps[min_name]))\n                        writer.add_scalar(\'train/{}_loss\'.format(min_name),\n                                          losses[-1], min_global_steps[min_name])\n                        writer.add_histogram(\'train/{}\'.format(min_name),\n                                             min_h[min_name].data,\n                                             bins=\'sturges\',\n                                             global_step=min_global_steps[min_name])\n                        writer.add_histogram(\'train/gtruth_{}\'.format(min_name),\n                                             batch[min_name].data,\n                                             bins=\'sturges\',\n                                             global_step=min_global_steps[min_name])\n                    if hasattr(self, \'z_minion\'):\n                        print(\'ZMinion dfake_loss: {:.3f}, dreal_loss: {:.3f}, \'\n                              \'gloss: {:.3f}\'.format(dfake_loss.item(),\n                                                     dreal_loss.item(),\n                                                     greal_loss.item()))\n                        writer.add_scalar(\'train/dfake_loss\',\n                                          dfake_loss.item(),\n                                          global_step)\n                        writer.add_scalar(\'train/dreal_loss\',\n                                          dreal_loss.item(),\n                                          global_step)\n                        writer.add_scalar(\'train/g_loss\',\n                                          greal_loss.item(),\n                                          global_step)\n                        writer.add_scalar(\'train/zweight\',\n                                          zweight,\n                                          global_step)\n                        writer.add_histogram(\'train/z\',\n                                             fe_h[\'chunk\'],\n                                             bins=\'sturges\',\n                                             global_step=global_step)\n                    if self.vq:\n                        print(\'VQLoss: {:.2f}, VQPP: \'\n                              \'{:.2f}\'.format(vq_loss.item(), vq_pp.item()))\n                        writer.add_scalar(\'train/vq_loss\', vq_loss.item(),\n                                          global_step=global_step)\n                        writer.add_scalar(\'train/vq_pp\', vq_pp.item(),\n                                          global_step=global_step)\n\n\n                    print(\'Mean batch time: {:.3f} s\'.format(np.mean(timings)))\n            # epoch end\n            if va_dloader is not None:\n                va_bpe = cfg[\'va_bpe\']\n                eloss = self.eval_(va_dloader, bsize, va_bpe, log_freq=log_freq,\n                                   epoch_idx=epoch_,\n                                   writer=writer, device=device)\n                """"""\n                if lrdecay > 0:\n                    # update frontend lr\n                    fesched.step(eloss)\n                    # update Z minion lr\n                    if hasattr(self, \'z_minion\'):\n                        zsched.step(eloss)\n                    # update each minion lr\n                    for mi, minion in enumerate(self.minions, start=1):\n                        minscheds[minion.name].step(eloss)\n                """"""\n\n            if lrdecay > 0:\n                # update frontend lr\n                fesched.step()\n                # update Z minion lr\n                if hasattr(self, \'z_minion\'):\n                    zsched.step()\n                # update each minion lr\n                for mi, minion in enumerate(self.minions, start=1):\n                    minscheds[minion.name].step()\n\n            torch.save(self.frontend.state_dict(),\n                       os.path.join(save_path,\n                                    \'FE_e{}.ckpt\'.format(epoch_)))\n            torch.save(self.state_dict(),\n                       os.path.join(save_path,\n                                    \'fullmodel_e{}.ckpt\'.format(epoch_)))\n\n\n    def eval_(self, dloader, batch_size, bpe, log_freq,\n             epoch_idx=0, writer=None, device=\'cpu\'):\n        self.eval()\n        with torch.no_grad():\n            bsize = batch_size\n            frontend = self.frontend\n            minions_run = self.minions\n            print(\'=\' * 50)\n            print(\'Beginning evaluation...\')\n            timings = []\n            beg_t = timeit.default_timer()\n            min_loss = {}\n            for bidx in range(1, bpe + 1):\n                batch = next(dloader.__iter__())\n                # Build chunk keys to know what to encode\n                chunk_keys = [\'chunk\']\n                if self.mi_fwd:\n                    chunk_keys += [\'chunk_ctxt\', \'chunk_rand\']\n                fe_h = {}\n                # Forward chunk(s) through frontend\n                for k in chunk_keys:\n                    fe_h[k] = frontend(batch[k].to(device))\n                min_h = {}\n                h = fe_h[\'chunk\']\n                skip_acum = None\n                for mi, minion in enumerate(minions_run, start=1):\n                    min_name = self.minions[mi - 1].name\n                    if \'mi\' in min_name:\n                        triplet_P = self.join_skip(torch.cat((fe_h[\'chunk\'],\n                                                              fe_h[\'chunk_ctxt\']),\n                                                             dim=1), skip_acum)\n                        triplet_N = self.join_skip(torch.cat((fe_h[\'chunk\'],\n                                                              fe_h[\'chunk_rand\']),\n                                                             dim=1), skip_acum)\n                        triplet_all = torch.cat((triplet_P, triplet_N), dim=0)\n                        if min_name == \'cmi\':\n                            # average through time dimension for ChunkMI\n                            triplet_all = torch.mean(triplet_all, dim=2,\n                                                     keepdim=True)\n                        y = minion(triplet_all)\n                        bsz = y.size(0)//2\n                        slen = y.size(2)\n                        batch[min_name] = torch.cat((torch.ones(bsz, 1, slen),\n                                                     torch.zeros(bsz, 1, slen)),\n                                                    dim=0)\n                    else:\n                        if self.minions[mi - 1].skip:\n                            y, h_ = minion(self.join_skip(h, skip_acum))\n                            if skip_acum is None:\n                                skip_acum = h_\n                            else:\n                                skip_acum = torch.cat((skip_acum, h_), dim=1)\n                        else:\n                            y = minion(self.join_skip(h, skip_acum))\n                        if min_name == \'spc\':\n                            # we must create the spc labels, composed of \n                            # B ones and B zeros (future and past). It\n                            # internally creates 2B samples\n                            bsz = y.size(0) // 2\n                            slen = y.size(2)\n                            batch[\'spc\'] = torch.cat((torch.ones(bsz, 1, slen),\n                                                      torch.zeros(bsz, 1,\n                                                                  slen)),\n                                                     dim=0)\n                    min_h[min_name] = y\n\n                # Compute all minion losses\n                for min_name, y_ in min_h.items():\n                    y_lab = batch[min_name].to(device)\n                    loss = self.minions[self.min2idx[min_name]].loss(y_, y_lab)\n                    if min_name not in min_loss:\n                        min_loss[min_name] = []\n                    min_loss[min_name].append(loss.item())\n                end_t = timeit.default_timer()\n                timings.append(end_t - beg_t)\n                beg_t = timeit.default_timer()\n                \n                if bidx % log_freq == 0 or bidx >= bpe:\n                    print(\'-\' * 50)\n                    print(\'EVAL Batch {}/{} (Epoch {}):\'.format(bidx, \n                                                                bpe,\n                                                                epoch_idx))\n                    for min_name, losses in min_loss.items():\n                        print(\'Minion {} loss: {:.3f}\'\n                              \'\'.format(min_name, losses[-1]))\n                    print(\'Mean batch time: {:.3f} s\'.format(np.mean(timings)))\n\n            # --------------------------------------------------------------\n            # After all eval data, write mean values of epoch per minion\n            aggregate = 0\n            for min_name, losses in min_loss.items():\n                mlosses = np.mean(losses)\n                writer.add_scalar(\'eval/{}_loss\'.format(min_name),\n                                  mlosses, epoch_idx)\n                aggregate += mlosses\n            # aggregate eval loss\n            writer.add_scalar(\'eval/total_loss\', aggregate,\n                              epoch_idx)\n            return aggregate\n\n\n    def state_dict(self):\n        sdict = {}\n        for k, v in super().state_dict().items():\n            if \'_dp.\' in k:\n                # skip any DataParallel wrapped thing\n                continue\n            sdict[k] = v\n        return sdict\n\nif __name__ == \'__main__\':\n    import json\n    wmodel = Waveminionet(\n                          minions_cfg=[\n                              {\'num_outputs\':1,\n                               \'dropout\':0.2,\n                               \'name\':\'chunk\',\n                               \'type\':\'decoder\',\n                              },\n                              {\'num_outputs\':257,\n                               \'dropout\':0.2,\n                               \'name\':\'lps\',\n                              },\n                              {\'num_outputs\':40,\n                               \'dropout\':0.2,\n                               \'name\':\'mfcc\'\n                              },\n                              {\'num_outputs\':4,\n                               \'dropout\':0.2,\n                               \'name\':\'prosody\'\n                              },\n                              #{\'num_outputs\':1,\n                              # \'dropout\':0.2,\n                              # \'name\':\'mi\',\n                              # \'keys\':[\'chunk\',\n                              #         \'chunk_ctxt\',\n                              #         \'chunk_rand\']\n                              #},\n                          ]\n                         )\n    print(wmodel)\n    x = torch.randn(1, 1, 8000)\n    outs, y = wmodel(x)\n    for k, v in outs.items():\n        print(\'{} : {}\'.format(k, v.size()))\n'"
ASR/waveminionet/models/decoders.py,5,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nfrom .frontend import *\nfrom .minions import *\nimport random\n\nclass SpectrumLM(nn.Module):\n    """""" RNN lang model for spectrum frame preds """"""\n    def __init__(self, rnn_size, rnn_layers, out_dim,\n                 dropout,\n                 cuda, rnn_type=\'LSTM\',\n                 bidirectional=False):\n        super().__init__()\n\n        self.do_cuda = cuda\n        self.rnn_size = rnn_size\n        self.rnn_layers = rnn_layers\n        self.rnn_type = rnn_type\n        self.out_dim = out_dim\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n        if bidirectional:\n            self.dirs = 2\n        else:\n            self.dirs = 1\n        assert rnn_type == \'LSTM\' or rnn_type == \'GRU\', rnn_type\n        self.rnn = getattr(nn, rnn_type)(self.out_dim, self.rnn_size,\n                                        self.rnn_layers,\n                                        batch_first=True,\n                                        dropout=self.dropout,\n                                        bidirectional=bidirectional)\n        self.out_fc = nn.Linear(self.rnn_size, self.out_dim)\n\n    def forward(self, x, dec_steps, state=None, \n                dec_cps={}):\n        # x is just a time-step input [B, F]\n        assert len(x.size()) == 2, x.size()\n        if state is None:\n            state = self.init_hidden(x.size(0))\n        assert isinstance(dec_cps, dict), type(dec_cps)\n        x = x.unsqueeze(1)\n        ht = x\n        frames = []\n        # forward through RNN\n        for t in range(dec_steps):\n            if t in dec_cps:\n                #print(\'Using cp at t:{}\'.format(t))\n                ht = dec_cps[t]\n                if len(ht.size()) == 2:\n                    # add time axis\n                    ht = ht.unsqueeze(1)\n            #print(\'Forwarding ht: \', ht.size())\n            ht, state = self.rnn(ht, state)\n            ht = self.out_fc(ht)\n            frames.append(ht)\n        frames = torch.cat(frames, 1)\n        return frames, state\n\n    def init_hidden(self, bsz):\n        h0 = Variable(torch.randn(self.dirs * self.rnn_layers,\n                                  bsz, self.rnn_size))\n        if self.do_cuda:\n            h0 = h0.cuda()\n        if self.rnn_type == \'LSTM\':\n            c0 = h0.clone()\n            return (h0, c0)\n        else:\n            return h0\n'"
ASR/waveminionet/models/encoders.py,1,"b'import torch\nimport torch.nn as nn\nfrom .core import LayerNorm\n\n\nclass AhoCNNEncoder(nn.Module):\n\n    def __init__(self, input_dim, kwidth=3, dropout=0.5, layer_norm=False):\n        super().__init__()\n        pad = (kwidth - 1) // 2\n\n        if layer_norm:\n            norm_layer = LayerNorm\n        else:\n            norm_layer = nn.BatchNorm1d\n\n        self.enc = nn.Sequential(\n            nn.Conv1d(input_dim, 256, kwidth, stride=1, padding=pad),\n            norm_layer(256),\n            nn.PReLU(256),\n            nn.Conv1d(256, 256, kwidth, stride=1, padding=pad),\n            norm_layer(256),\n            nn.PReLU(256),\n            nn.MaxPool1d(2),\n            nn.Dropout(0.2),\n            nn.Conv1d(256, 512, kwidth, stride=1, padding=pad),\n            norm_layer(512),\n            nn.PReLU(512),\n            nn.Conv1d(512, 512, kwidth, stride=1, padding=pad),\n            norm_layer(512),\n            nn.PReLU(512),\n            nn.MaxPool1d(2),\n            nn.Dropout(0.2),\n            nn.Conv1d(512, 1024, kwidth, stride=1, padding=pad),\n            norm_layer(1024),\n            nn.PReLU(1024),\n            nn.Conv1d(1024, 1024, kwidth, stride=1, padding=pad),\n            norm_layer(1024),\n            nn.PReLU(1024),\n            nn.MaxPool1d(2),\n            nn.Dropout(0.2),\n            nn.Conv1d(1024, 1024, kwidth, stride=1, padding=pad),\n        )\n\n    def forward(self, x):\n        return self.enc(x)\n\n\nclass AhoCNNHourGlassEncoder(nn.Module):\n\n    def __init__(self, input_dim, kwidth=3, dropout=0.5, layer_norm=False):\n        super().__init__()\n        pad = (kwidth - 1) // 2\n\n        if layer_norm:\n            norm_layer = LayerNorm\n        else:\n            norm_layer = nn.BatchNorm1d\n\n        self.enc = nn.Sequential(\n            nn.Conv1d(input_dim, 64, kwidth, stride=1, padding=pad),\n            norm_layer(64),\n            nn.PReLU(64),\n            nn.Conv1d(64, 128, kwidth, stride=1, padding=pad),\n            norm_layer(128),\n            nn.PReLU(128),\n            nn.MaxPool1d(2),\n            nn.Dropout(dropout),\n            nn.Conv1d(128, 256, kwidth, stride=1, padding=pad),\n            norm_layer(256),\n            nn.PReLU(256),\n            nn.Conv1d(256, 512, kwidth, stride=1, padding=pad),\n            norm_layer(512),\n            nn.PReLU(512),\n            nn.MaxPool1d(2),\n            nn.Dropout(dropout),\n            nn.Conv1d(512, 256, kwidth, stride=1, padding=pad),\n            norm_layer(256),\n            nn.PReLU(256),\n            nn.Conv1d(256, 128, kwidth, stride=1, padding=pad),\n            norm_layer(128),\n            nn.PReLU(128),\n            nn.MaxPool1d(2),\n            nn.Dropout(dropout),\n            nn.Conv1d(128, 64, kwidth, stride=1, padding=pad),\n            norm_layer(64),\n            nn.PReLU(64),\n        )\n\n    def forward(self, x):\n        return self.enc(x)\n'"
ASR/waveminionet/models/frontend.py,3,"b'import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport json\nif  __name__ == \'__main__\':\n    from modules import *\nelse:\n    from .modules import *\n\n\ndef wf_builder(cfg_path):\n    with open(cfg_path, \'r\') as cfg_f:\n        cfg = json.load(cfg_f)\n        return WaveFe(**cfg)\n\nclass WaveFe(Model):\n    """""" Convolutional front-end to process waveforms\n        into a decimated intermediate representation \n    """"""\n    def __init__(self, num_inputs=1, \n                 sincnet=True,\n                 kwidths=[251, 10, 5, 5, 5, 5, 5, 5], \n                 strides=[1, 10, 2, 1, 2, 1, 2, 2], \n                 fmaps=[64, 64, 128, 128, 256, 256, 512, 512],\n                 norm_type=\'bnorm\',\n                 pad_mode=\'reflect\', sr=16000,\n                 emb_dim=256,\n                 rnn_pool=False,\n                 vq_K=None,\n                 vq_beta=0.25,\n                 vq_gamma=0.99,\n                 norm_out=False,\n                 name=\'WaveFe\'):\n        super().__init__(name=name) \n        # apply sincnet at first layer\n        self.sincnet = sincnet\n        self.kwidths = kwidths\n        self.strides = strides\n        self.fmaps = fmaps\n        self.blocks = nn.ModuleList()\n        assert len(kwidths) == len(strides)\n        assert len(strides) == len(fmaps)\n        ninp = num_inputs\n        for n, (kwidth, stride, fmap) in enumerate(zip(kwidths, strides,\n                                                       fmaps), start=1):\n            if n > 1:\n                # make sure sincnet is deactivated after first layer\n                sincnet = False\n            self.blocks.append(FeBlock(ninp, fmap, kwidth, stride,\n                                       pad_mode=pad_mode,\n                                       norm_type=norm_type,\n                                       sincnet=sincnet,\n                                       sr=sr))\n            ninp = fmap\n        # last projection\n        if rnn_pool:\n            self.rnn = nn.GRU(fmap, emb_dim // 2, bidirectional=True, \n                              batch_first=True)\n            self.W = nn.Linear(emb_dim, emb_dim)\n        else:\n            self.W = nn.Conv1d(fmap, emb_dim, 1)\n        self.emb_dim = emb_dim\n        self.rnn_pool = rnn_pool\n        if vq_K is not None and vq_K > 0:\n            self.quantizer = VQEMA(vq_K, self.emb_dim,\n                                   vq_beta, vq_gamma)\n        else:\n            self.quantizer = None\n        # ouptut vectors are normalized to norm^2 1\n        if norm_out:\n            self.norm_out = nn.BatchNorm1d(self.emb_dim, affine=False)\n\n        \n    def forward(self, x):\n        h = x\n        for n, block in enumerate(self.blocks):\n            h = block(h)\n        if self.rnn_pool:\n            ht, _ = self.rnn(h.transpose(1, 2))\n            y = self.W(ht) \n            y = y.transpose(1, 2)\n        else:\n            y = self.W(h)\n        if hasattr(self, \'norm_out\'):\n            y = self.norm_out(y)\n\n        if self.quantizer is not None:\n            qloss, y, pp, enc = self.quantizer(y)\n            if self.training:\n                return qloss, y, pp, enc\n            else:\n                return y\n        return y\n\nif __name__ == \'__main__\':\n    from modules import *\n    wavefe = WaveFe(norm_type=\'bnorm\')\n    print(wavefe)\n    wavefe.describe_params()\n    x = torch.randn(1, 1, 16000)\n    y = wavefe(x)\n    print(y.size())\n    vq = VQEMA(50, 20, 0.25, 0.99)\n    _, yq, _ , _ = vq(y)\n    print(yq.size())\n    qwavefe = WaveFe(norm_type=\'bnorm\', emb_dim=20)\n    qwavefe.eval()\n    yq2 = qwavefe(x)\n    print(yq2.size())\n    # try builder\n    wfb = wf_builder(\'../../cfg/frontend_RF160ms_emb100.cfg\')\n    print(wfb)\n'"
ASR/waveminionet/models/minions.py,6,"b'import torch\nimport torch.nn as nn\nfrom .frontend import WaveFe\nfrom .modules import *\nimport torch.nn.functional as F\nimport json\nimport random\n\n\ndef minion_maker(cfg):\n    mtype = cfg.pop(\'type\', \'mlp\')\n    if mtype == \'mlp\':\n        minion = MLPMinion(**cfg)\n    elif mtype == \'decoder\':\n        minion = DecoderMinion(**cfg)\n    elif mtype == \'spc\':\n        minion = SPCMinion(**cfg)\n    elif mtype == \'gru\':\n        minion = GRUMinion(**cfg)\n    else:\n        raise TypeError(\'Unrecognized minion type {}\'.format(mtype))\n    return minion\n\nclass MLPBlock(NeuralBlock):\n\n    def __init__(self, ninp, fmaps, dout=0, name=\'MLPBlock\'):\n        super().__init__(name=name)\n        self.ninp = ninp\n        self.fmaps = fmaps\n        self.W = nn.Conv1d(ninp, fmaps, 1)\n        self.act = nn.PReLU(fmaps)\n        self.dout = nn.Dropout(dout)\n    \n    def forward(self, x):\n        return self.dout(self.act(self.W(x)))\n\nclass DecoderMinion(Model):\n\n    def __init__(self, num_inputs,\n                 num_outputs,\n                 dropout, hidden_size=256,\n                 hidden_layers=2,\n                 fmaps=[256, 256, 128, 128, 128, 64, 64],\n                 strides=[2, 2, 2, 2, 2, 5],\n                 kwidths=[2, 2, 2, 2, 2, 5],\n                 norm_type=None,\n                 skip=False,\n                 loss=None,\n                 keys=None,\n                 name=\'DecoderMinion\'):\n        super().__init__(name=name)\n        self.num_inputs = num_inputs\n        self.num_outputs = num_outputs\n        self.dropout = dropout\n        self.skip = skip\n        self.hidden_size = hidden_size\n        self.hidden_layers = hidden_layers\n        self.fmaps = fmaps\n        self.strides = strides\n        self.kwidths = kwidths\n        self.norm_type = norm_type\n        self.loss = loss\n        self.keys = keys\n        if keys is None:\n            keys = [name]\n        self.blocks = nn.ModuleList()\n        ninp = num_inputs\n        # First go through deconvolving structure\n        for (fmap, kw, stride) in zip(fmaps, kwidths, strides):\n            block = GDeconv1DBlock(ninp, fmap, kw, stride,\n                                   norm_type=norm_type)\n            self.blocks.append(block)\n            ninp = fmap\n\n        for _ in range(hidden_layers):\n            self.blocks.append(MLPBlock(ninp,\n                                        hidden_size, dropout))\n            ninp = hidden_size\n        self.W = nn.Conv1d(hidden_size, num_outputs, 1)\n        \n    def forward(self, x):\n        h = x\n        for bi, block in enumerate(self.blocks, start=1):\n            h_ = h\n            h = block(h)\n        y = self.W(h)\n        if self.skip:\n            return y, h\n        else:\n            return y\n                 \nclass MLPMinion(Model):\n\n    def __init__(self, num_inputs, \n                 num_outputs,\n                 dropout, hidden_size=256,\n                 hidden_layers=2,\n                 skip=True,\n                 loss=None,\n                 keys=None,\n                 name=\'MLPMinion\'):\n        super().__init__(name=name)\n        # Implemented with Conv1d layers to not \n        # transpose anything in time, such that\n        # frontend and minions are attached very simply\n        self.num_inputs = num_inputs\n        self.num_outputs = num_outputs\n        self.dropout = dropout\n        self.skip = skip\n        self.hidden_size = hidden_size\n        self.hidden_layers = hidden_layers\n        self.loss = loss\n        self.keys = keys\n        if keys is None:\n            keys = [name]\n        self.blocks = nn.ModuleList()\n        ninp = num_inputs\n        for _ in range(hidden_layers):\n            self.blocks.append(MLPBlock(ninp,\n                                        hidden_size,\n                                        dropout))\n            ninp = hidden_size\n        self.W = nn.Conv1d(hidden_size, num_outputs, 1)\n        \n    def forward(self, x):\n        h = x\n        for bi, block in enumerate(self.blocks, start=1):\n            h = block(h)\n        y = self.W(h)\n        if self.skip:\n            return y, h\n        else:\n            return y\n\nclass GRUMinion(Model):\n\n    def __init__(self, num_inputs, \n                 num_outputs,\n                 dropout, hidden_size=256,\n                 hidden_layers=2,\n                 skip=True,\n                 loss=None,\n                 keys=None,\n                 name=\'GRUMinion\'):\n        super().__init__(name=name)\n        self.num_inputs = num_inputs\n        self.num_outputs = num_outputs\n        self.dropout = dropout\n        self.skip = skip\n        self.hidden_size = hidden_size\n        self.hidden_layers = hidden_layers\n        self.loss = loss\n        self.keys = keys\n        if keys is None:\n            keys = [name]\n        self.blocks = nn.ModuleList()\n        ninp = num_inputs\n        self.rnn = nn.GRU(ninp,\n                          hidden_size,\n                          num_layers=hidden_layers,\n                          batch_first=True,\n                          dropout=dropout)\n        self.W = nn.Conv1d(hidden_size, num_outputs, 1)\n        \n    def forward(self, x):\n        h, _ = self.rnn(x.transpose(1, 2))\n        h = h.transpose(1, 2)\n        y = self.W(h)\n        if self.skip:\n            return y, h\n        else:\n            return y\n\nclass SPCMinion(MLPMinion):\n\n    def __init__(self, num_inputs,\n                 num_outputs,\n                 dropout, hidden_size=256,\n                 hidden_layers=2,\n                 ctxt_frames=5,\n                 seq_pad=16,\n                 skip=True,\n                 loss=None,\n                 keys=None,\n                 name=\'SPCMinion\'):\n        # num_inputs is code dimension in each time-step,\n        # so the MLP has [num_inputs x ctxt_frames] inputs\n        # as we unroll time dimension to fixed-sized windows\n        print(\'num_inputs: \', num_inputs)\n        print(\'ctxt_frames: \', ctxt_frames)\n        num_inputs = (ctxt_frames + 1) * num_inputs\n        print(\'num_inputs: \', num_inputs)\n        super().__init__(num_inputs=num_inputs,\n                         num_outputs=num_outputs,\n                         dropout=dropout,\n                         hidden_size=hidden_size,\n                         hidden_layers=hidden_layers,\n                         skip=skip,\n                         loss=loss,\n                         keys=keys,\n                         name=name)\n        self.ctxt_frames = ctxt_frames\n        self.seq_pad = seq_pad\n\n    def forward(self, x):\n        # x is a batch of sequences\n        # of dims [B, channels, time]\n        # first select a ""central"" time-step\n        # with enough seq_pad an ctxt_frames\n        # margin M = seq_pad + ctxt_frames on both sides\n        seq_pad = self.seq_pad\n        N = self.ctxt_frames\n        M = seq_pad + N\n        idxs_t = list(range(M+1, x.size(2) - M))\n        t = random.choice(idxs_t)\n\n        bsz = x.size(0)\n\n        # now select future_t (to begin future seq)\n        idxs_ft = list(range(t + seq_pad, x.size(2) - N))\n        future_t = random.choice(idxs_ft)\n        idxs_pt = list(range(N, t - seq_pad))\n        past_t = random.choice(idxs_pt)\n\n        # chunk input sequences and current frame\n        future = x[:, :, future_t:future_t + N].contiguous().view(bsz, -1)\n        past = x[:, :, past_t- N:past_t].contiguous().view(bsz, -1)\n        current = x[:, :, t].contiguous()\n        \n        # positive batch (future data)\n        pos = torch.cat((current, future), dim=1)\n        # negative batch (past data)\n        neg = torch.cat((current, past), dim=1)\n\n        # forward both jointly\n        x_full = torch.cat((pos, neg), dim=0).unsqueeze(2)\n        h = x_full\n        for bi, block in enumerate(self.blocks, start=1):\n            h = block(h)\n        y = self.W(h)\n        if self.skip:\n            return y, h\n        else:\n            return y\n\nif __name__ == \'__main__\':\n    #minion = MLPMinion(256, 128, 0)\n    minion = DecoderMinion(256, 128, 0)\n    x = torch.randn(1, 256, 200)\n    print(x)\n    minion.describe_params()\n    y, h = minion(x)\n    print(h.size())\n    print(y.size())\n'"
ASR/waveminionet/models/modules.py,56,"b'import torch\nimport torch.nn as nn\nimport math\nimport torch.nn.functional as F\nfrom torch.nn.utils.spectral_norm import spectral_norm\nimport numpy as np\nimport json\nimport os\n\n\ndef build_norm_layer(norm_type, param=None, num_feats=None):\n    if norm_type == \'bnorm\':\n        return nn.BatchNorm1d(num_feats)\n    elif norm_type == \'snorm\':\n        spectral_norm(param)\n        return None\n    elif norm_type is None:\n        return None\n    else:\n        raise TypeError(\'Unrecognized norm type: \', norm_type)\n\ndef forward_norm(x, norm_layer):\n    if norm_layer is not None:\n        return norm_layer(x)\n    else:\n        return x\n\nclass NeuralBlock(nn.Module):\n\n    def __init__(self, name=\'NeuralBlock\'):\n        super().__init__()\n        self.name = name\n\n\t# https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/5\n    def describe_params(self):\n        pp = 0\n        for p in list(self.parameters()):\n            nn = 1\n            for s in list(p.size()):\n                nn = nn * s\n            pp += nn\n        print(\'-\' * 10)\n        print(self)\n        print(\'Num params: \', pp)\n        print(\'-\' * 10)\n        return pp\n\nclass Saver(object):\n\n    def __init__(self, model, save_path, max_ckpts=5, optimizer=None, prefix=\'\'):\n        self.model = model\n        self.save_path = save_path\n        self.ckpt_path = os.path.join(save_path, \'{}checkpoints\'.format(prefix)) \n        self.max_ckpts = max_ckpts\n        self.optimizer = optimizer\n        self.prefix = prefix\n\n    def save(self, model_name, step, best_val=False):\n        save_path = self.save_path\n        if not os.path.exists(save_path):\n            os.makedirs(save_path)\n\n        ckpt_path = self.ckpt_path\n        if os.path.exists(ckpt_path):\n            with open(ckpt_path, \'r\') as ckpt_f:\n                # read latest checkpoints\n                ckpts = json.load(ckpt_f)\n        else:\n            ckpts = {\'latest\':[], \'current\':[]}\n\n        model_path = \'{}-{}.ckpt\'.format(model_name, step)\n        if best_val: \n            model_path = \'best_\' + model_path\n        model_path = \'{}{}\'.format(self.prefix, model_path)\n        \n        # get rid of oldest ckpt, with is the frst one in list\n        latest = ckpts[\'latest\']\n        if len(latest) > 0:\n            todel = latest[0]\n            if self.max_ckpts is not None:\n                if len(latest) > self.max_ckpts:\n                    try:\n                        print(\'Removing old ckpt {}\'.format(os.path.join(save_path, \n                                                            \'weights_\' + todel)))\n                        os.remove(os.path.join(save_path, \'weights_\' + todel))\n                        latest = latest[1:] \n                    except FileNotFoundError:\n                        print(\'ERROR: ckpt is not there?\')\n\n        latest += [model_path]\n\n        ckpts[\'latest\'] = latest\n        ckpts[\'current\'] = model_path\n\n        with open(ckpt_path, \'w\') as ckpt_f:\n            ckpt_f.write(json.dumps(ckpts, indent=2))\n\n        st_dict = {\'step\':step,\n                   \'state_dict\':self.model.state_dict()}\n\n        if self.optimizer is not None: \n            st_dict[\'optimizer\'] = self.optimizer.state_dict()\n        # now actually save the model and its weights\n        #torch.save(self.model, os.path.join(save_path, model_path))\n        torch.save(st_dict, os.path.join(save_path, \n                                          \'weights_\' + \\\n                                           model_path))\n\n    def read_latest_checkpoint(self):\n        ckpt_path = self.ckpt_path\n        print(\'Reading latest checkpoint from {}...\'.format(ckpt_path))\n        if not os.path.exists(ckpt_path):\n            print(\'[!] No checkpoint found in {}\'.format(self.save_path))\n            return False\n        else:\n            with open(ckpt_path, \'r\') as ckpt_f:\n                ckpts = json.load(ckpt_f)\n            curr_ckpt = ckpts[\'current\'] \n            return curr_ckpt\n\n    #def load(self):\n    #    save_path = self.save_path\n    #    ckpt_path = self.ckpt_path\n    #    print(\'Reading latest checkpoint from {}...\'.format(ckpt_path))\n    #    if not os.path.exists(ckpt_path):\n    #        raise FileNotFoundError(\'[!] Could not load model. Ckpt \'\n    #                                \'{} does not exist!\'.format(ckpt_path))\n    #    with open(ckpt_path, \'r\') as ckpt_f:\n    #        ckpts = json.load(ckpt_f)\n    #    curr_ckpt = ckpts[\'curent\'] \n    #    st_dict = torch.load(os.path.join(save_path, curr_ckpt))\n    #    return \n\n    def load_weights(self):\n        save_path = self.save_path\n        curr_ckpt = self.read_latest_checkpoint()\n        if curr_ckpt is False:\n            if not os.path.exists(ckpt_path):\n                print(\'[!] No weights to be loaded\')\n                return False\n        else:\n            st_dict = torch.load(os.path.join(save_path,\n                                              \'weights_\' + \\\n                                              curr_ckpt))\n            if \'state_dict\' in st_dict:\n                # new saving mode\n                model_state = st_dict[\'state_dict\']\n                self.model.load_state_dict(model_state)\n                if self.optimizer is not None and \'optimizer\' in st_dict:\n                    self.optimizer.load_state_dict(st_dict[\'optimizer\'])\n            else:\n                # legacy mode, only model was saved\n                self.model.load_state_dict(st_dict)\n            print(\'[*] Loaded weights\')\n            return True\n\n    def load_pretrained_ckpt(self, ckpt_file, load_last=False, load_opt=True,\n                             verbose=True):\n        model_dict = self.model.state_dict() \n        st_dict = torch.load(ckpt_file, \n                             map_location=lambda storage, loc: storage)\n        if \'state_dict\' in st_dict:\n            pt_dict = st_dict[\'state_dict\']\n        else:\n            # legacy mode\n            pt_dict = st_dict\n        all_pt_keys = list(pt_dict.keys())\n        if not load_last:\n            # Get rid of last layer params (fc output in D)\n            allowed_keys = all_pt_keys[:-2]\n        else:\n            allowed_keys = all_pt_keys[:]\n        # Filter unnecessary keys from loaded ones and those not existing\n        pt_dict = {k: v for k, v in pt_dict.items() if k in model_dict and \\\n                   k in allowed_keys and v.size() == model_dict[k].size()}\n        if verbose:\n            print(\'Current Model keys: \', len(list(model_dict.keys())))\n            print(\'Loading Pt Model keys: \', len(list(pt_dict.keys())))\n            print(\'Loading matching keys: \', list(pt_dict.keys()))\n        if len(pt_dict.keys()) != len(model_dict.keys()):\n            print(\'WARNING: LOADING DIFFERENT NUM OF KEYS\')\n        # overwrite entries in existing dict\n        model_dict.update(pt_dict)\n        # load the new state dict\n        self.model.load_state_dict(model_dict)\n        for k in model_dict.keys():\n            if k not in allowed_keys:\n                print(\'WARNING: {} weights not loaded from pt ckpt\'.format(k))\n        if self.optimizer is not None and \'optimizer\' in st_dict and load_opt:\n            self.optimizer.load_state_dict(st_dict[\'optimizer\'])\n\n\nclass Model(NeuralBlock):\n\n    def __init__(self, max_ckpts=5, name=\'BaseModel\'):\n        super().__init__()\n        self.name = name\n        self.optim = None\n        self.max_ckpts = max_ckpts\n\n    def save(self, save_path, step, best_val=False, saver=None):\n        model_name = self.name\n\n        if not hasattr(self, \'saver\') and saver is None:\n            self.saver = Saver(self, save_path,\n                               optimizer=self.optim,\n                               prefix=model_name + \'-\',\n                               max_ckpts=self.max_ckpts)\n\n        if saver is None:\n            self.saver.save(model_name, step, best_val=best_val)\n        else:\n            # save with specific saver\n            saver.save(model_name, step, best_val=best_val)\n\n    def load(self, save_path):\n        if os.path.isdir(save_path):\n            if not hasattr(self, \'saver\'):\n                self.saver = Saver(self, save_path, \n                                   optimizer=self.optim,\n                                   prefix=model_name + \'-\',\n                                   max_ckpts=self.max_ckpts)\n            self.saver.load_weights()\n        else:\n            print(\'Loading ckpt from ckpt: \', save_path)\n            # consider it as ckpt to load per-se\n            self.load_pretrained(save_path)\n\n    def load_pretrained(self, ckpt_path, load_last=False, verbose=True):\n        # tmp saver\n        saver = Saver(self, \'.\', optimizer=self.optim)\n        saver.load_pretrained_ckpt(ckpt_path, load_last, verbose=verbose)\n\n\n    def activation(self, name):\n        return getattr(nn, name)()\n\n    def parameters(self):\n        return filter(lambda p: p.requires_grad, super().parameters())\n\n    def get_total_params(self):\n        pp = 0\n        for p in list(self.parameters()):\n            nn = 1\n            for s in list(p.size()):\n                nn = nn * s\n            pp += nn\n        return pp\n\n    def describe_params(self):\n        pp = 0\n        if hasattr(self, \'blocks\'):\n            for b in self.blocks:\n                p = b.describe_params()\n                pp += p\n        else:\n            print(\'Warning: did not find a list of blocks...\')\n            print(\'Just printing all params calculation.\')\n        total_params = self.get_total_params()\n        print(\'{} total params: {}\'.format(self.name,\n                                           total_params))\n        return total_params\n\n\nclass GConv1DBlock(NeuralBlock):\n\n    def __init__(self, ninp, fmaps,\n                 kwidth, stride=1, norm_type=None,\n                 name=\'GConv1DBlock\'):\n        super().__init__(name=name)\n        self.conv = nn.Conv1d(ninp, fmaps, kwidth, stride=stride)\n        self.norm = build_norm_layer(norm_type, self.conv, fmaps)\n        self.act = nn.PReLU(fmaps, init=0)\n        self.kwidth = kwidth\n        self.stride = stride\n\n\n    def forward(self, x):\n        if self.stride > 1:\n            P = (self.kwidth // 2 - 1,\n                 self.kwidth // 2)\n        else:\n            P = (self.kwidth // 2,\n                 self.kwidth // 2)\n        x_p = F.pad(x, P, mode=\'reflect\')\n        h = self.conv(x_p)\n        h = forward_norm(h, self.norm)\n        h = self.act(h)\n        return h\n\nclass GDeconv1DBlock(NeuralBlock):\n\n    def __init__(self, ninp, fmaps,\n                 kwidth, stride=4, norm_type=None,\n                 act=None,\n                 name=\'GDeconv1DBlock\'):\n        super().__init__(name=name)\n        pad = max(0, (stride - kwidth)//-2)\n        self.deconv = nn.ConvTranspose1d(ninp, fmaps,\n                                         kwidth, \n                                         stride=stride,\n                                         padding=pad)\n        self.norm = build_norm_layer(norm_type, self.deconv,\n                                     fmaps)\n        if act is not None:\n            self.act = getattr(nn, act)()\n        else:\n            self.act = nn.PReLU(fmaps, init=0)\n        self.kwidth = kwidth\n        self.stride = stride\n\n    def forward(self, x):\n        h = self.deconv(x)\n        if self.kwidth % 2 != 0 and self.stride < self.kwidth:\n            h = h[:, :, :-1]\n        h = forward_norm(h, self.norm)\n        h = self.act(h)\n        return h\n\nclass ResARModule(NeuralBlock):\n\n    def __init__(self, ninp, fmaps,\n                 res_fmaps,\n                 kwidth, dilation,\n                 norm_type=None,\n                 act=None,\n                 name=\'ResARModule\'):\n        super().__init__(name=name)\n        self.dil_conv = nn.Conv1d(ninp, fmaps,\n                                  kwidth, dilation=dilation)\n        if act is not None:\n            self.act = getattr(nn, act)()\n        else:\n            self.act = nn.PReLU(fmaps, init=0)\n        self.dil_norm = build_norm_layer(norm_type, self.dil_conv,\n                                         fmaps)\n        self.kwidth = kwidth\n        self.dilation = dilation\n        # skip 1x1 convolution\n        self.conv_1x1_skip = nn.Conv1d(fmaps, ninp, 1)\n        self.conv_1x1_skip_norm = build_norm_layer(norm_type, \n                                                   self.conv_1x1_skip,\n                                                   ninp)\n        # residual 1x1 convolution\n        self.conv_1x1_res = nn.Conv1d(fmaps, res_fmaps, 1)\n        self.conv_1x1_res_norm = build_norm_layer(norm_type, \n                                                  self.conv_1x1_res,\n                                                  res_fmaps)\n\n    def forward(self, x):\n        kw__1 = self.kwidth - 1\n        P = kw__1 + kw__1 * (self.dilation - 1)\n        # causal padding\n        x_p = F.pad(x, (P, 0))\n        # dilated conv\n        h = self.dil_conv(x_p)\n        # normalization if applies\n        h = forward_norm(h, self.dil_norm)\n        # activation\n        h = self.act(h)\n        a = h\n        # conv 1x1 to make residual connection\n        h = self.conv_1x1_skip(h)\n        # normalization if applies\n        h = forward_norm(h, self.conv_1x1_skip_norm)\n        # return with skip connection\n        y = x + h\n        # also return res connection (going to further net point directly)\n        sh = self.conv_1x1_res(a)\n        sh = forward_norm(sh, self.conv_1x1_res_norm)\n        return y, sh\n\n# SincNet conv layer\ndef flip(x, dim):\n    xsize = x.size()\n    dim = x.dim() + dim if dim < 0 else dim\n    x = x.contiguous()\n    x = x.view(-1, *xsize[dim:])\n    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, \n                      -1, -1), (\'cpu\',\'cuda\')[x.is_cuda])().long(), :]\n    return x.view(xsize)\n\n\ndef sinc(band,t_right, cuda=False):\n    y_right= torch.sin(2*math.pi*band*t_right)/(2*math.pi*band*t_right)\n    y_left= flip(y_right,0)\n\n    ones = torch.ones(1)\n    if cuda:\n        ones = ones.to(\'cuda\')\n    y=torch.cat([y_left, ones, y_right])\n\n    return y\n    \n    \n# Modified from https://github.com/mravanelli/SincNet\nclass SincConv(nn.Module):\n\n    def __init__(self, N_filt, Filt_dim, fs, stride=1,\n                 padding=\'VALID\', pad_mode=\'reflect\'):\n        super(SincConv, self).__init__()\n\n        # Mel Initialization of the filterbanks\n        low_freq_mel = 80\n        high_freq_mel = (2595 * np.log10(1 + (fs / 2) \\\n                                         / 700))  # Convert Hz to Mel\n        mel_points = np.linspace(low_freq_mel, high_freq_mel, \n                                 N_filt)  # Equally spaced in Mel scale\n        f_cos = (700 * (10 ** (mel_points / 2595) - 1)) # Convert Mel to Hz\n        b1 = np.roll(f_cos, 1)\n        b2 = np.roll(f_cos, -1)\n        b1[0] = 30\n        b2[-1] = (fs / 2) - 100\n                \n        self.freq_scale=fs * 1.0\n        self.filt_b1 = nn.Parameter(torch.from_numpy(b1/self.freq_scale))\n        self.filt_band = nn.Parameter(torch.from_numpy((b2-b1)/self.freq_scale))\n\n        self.N_filt = N_filt\n        self.Filt_dim = Filt_dim\n        self.fs = fs\n        self.padding = padding\n        self.stride =stride\n        self.pad_mode = pad_mode\n        \n    def forward(self, x):\n        cuda = x.is_cuda\n        filters=torch.zeros((self.N_filt, self.Filt_dim))\n        N=self.Filt_dim\n        t_right=torch.linspace(1, (N - 1) / 2, \n                               steps=int((N - 1) / 2)) / self.fs\n        if cuda:\n            filters = filters.to(\'cuda\')\n            t_right = t_right.to(\'cuda\')\n        \n        min_freq=50.0;\n        min_band=50.0;\n        filt_beg_freq = torch.abs(self.filt_b1) + min_freq / self.freq_scale\n        filt_end_freq = filt_beg_freq + (torch.abs(self.filt_band) + \\\n                                         min_band / self.freq_scale)\n        n = torch.linspace(0, N, steps = N)\n        # Filter window (hamming)\n        window=(0.54 - 0.46 * torch.cos(2 * math.pi * n / N)).float()\n        if cuda:\n            window = window.to(\'cuda\')\n        for i in range(self.N_filt):\n            low_pass1 = 2 * filt_beg_freq[i].float()* \\\n                    sinc(filt_beg_freq[i].float() * self.freq_scale, \n                         t_right, cuda)\n            low_pass2 = 2 * filt_end_freq[i].float()* \\\n                    sinc(filt_end_freq[i].float() * self.freq_scale, \n                         t_right, cuda)\n            band_pass=(low_pass2 - low_pass1)\n            band_pass=band_pass/torch.max(band_pass)\n            if cuda:\n                band_pass = band_pass.to(\'cuda\')\n\n            filters[i,:]=band_pass * window\n        if self.padding == \'SAME\':\n            if self.stride > 1:\n                x_p = F.pad(x, (self.Filt_dim // 2 - 1,\n                                self.Filt_dim // 2), mode=self.pad_mode)\n            else:\n                x_p = F.pad(x, (self.Filt_dim // 2,\n                                self.Filt_dim // 2), mode=self.pad_mode)\n        else:\n            x_p = x\n        out = F.conv1d(x_p, filters.view(self.N_filt, 1, self.Filt_dim),\n                       stride=self.stride)\n        return out\n\nclass SincConv_fast(nn.Module):\n    """"""Sinc-based convolution\n    Parameters\n    ----------\n    in_channels : `int`\n        Number of input channels. Must be 1.\n    out_channels : `int`\n        Number of filters.\n    kernel_size : `int`\n        Filter length.\n    sample_rate : `int`, optional\n        Sample rate. Defaults to 16000.\n    Usage\n    -----\n    See `torch.nn.Conv1d`\n    Reference\n    ---------\n    Mirco Ravanelli, Yoshua Bengio,\n    ""Speaker Recognition from raw waveform with SincNet"".\n    https://arxiv.org/abs/1808.00158\n    """"""\n\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 stride=1, padding=\'VALID\', pad_mode=\'reflect\',\n                 dilation=1, bias=False, groups=1,\n                 sample_rate=16000, min_low_hz=50, min_band_hz=50):\n\n        super(SincConv_fast,self).__init__()\n\n        if in_channels != 1:\n            #msg = (f\'SincConv only support one input channel \'\n            #       f\'(here, in_channels = {in_channels:d}).\')\n            msg = ""SincConv only support one input channel (here, in_channels = {%i})"" % (in_channels)\n            raise ValueError(msg)\n\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        \n        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n        if kernel_size%2==0:\n            self.kernel_size=self.kernel_size+1\n            \n        self.stride = stride\n        self.padding = padding\n        self.pad_mode = pad_mode\n        self.dilation = dilation\n\n        if bias:\n            raise ValueError(\'SincConv does not support bias.\')\n        if groups > 1:\n            raise ValueError(\'SincConv does not support groups.\')\n\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # initialize filterbanks such that they are equally spaced in Mel scale\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n\n        mel = np.linspace(self.to_mel(low_hz),\n                          self.to_mel(high_hz),\n                          self.out_channels + 1)\n        hz = self.to_hz(mel)\n        \n\n        # filter lower frequency (out_channels, 1)\n        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n\n        # filter frequency band (out_channels, 1)\n        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n\n        # Hamming window\n        #self.window_ = torch.hamming_window(self.kernel_size)\n        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\n        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n\n\n        # (kernel_size, 1)\n        n = (self.kernel_size - 1) / 2.0\n        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\n\n \n\n\n    def forward(self, waveforms):\n        """"""\n        Parameters\n        ----------\n        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n            Batch of waveforms.\n        Returns\n        -------\n        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n            Batch of sinc filters activations.\n        """"""\n\n        self.n_ = self.n_.to(waveforms.device)\n\n        self.window_ = self.window_.to(waveforms.device)\n\n        low = self.min_low_hz  + torch.abs(self.low_hz_)\n        \n        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n        band=(high-low)[:,0]\n        \n        f_times_t_low = torch.matmul(low, self.n_)\n        f_times_t_high = torch.matmul(high, self.n_)\n\t\t# Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). \n        # I just have expanded the sinc and simplified the terms. This way I avoid several useless computations.\n        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_  \n        band_pass_center = 2*band.view(-1,1)\n        band_pass_right= torch.flip(band_pass_left,dims=[1])\n        \n        \n        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n\n        \n        band_pass = band_pass / (2*band[:,None])\n        \n\n        self.filters = (band_pass).view(\n            self.out_channels, 1, self.kernel_size)\n\n        x = waveforms \n\n        if self.padding == \'SAME\':\n            if self.stride > 1:\n                x_p = F.pad(x, (self.kernel_size // 2 - 1,\n                                self.kernel_size // 2), mode=self.pad_mode)\n            else:\n                x_p = F.pad(x, (self.kernel_size // 2,\n                                self.kernel_size // 2), mode=self.pad_mode)\n        else:\n            x_p = x\n\n        return F.conv1d(x_p, self.filters, stride=self.stride,\n                        padding=0, dilation=self.dilation,\n                        bias=None, groups=1) \n\n\nclass FeBlock(NeuralBlock):\n\n    def __init__(self, num_inputs,\n                 fmaps, kwidth, stride,\n                 pad_mode=\'reflect\',\n                 norm_type=None,\n                 sincnet=False,\n                 sr=16000,\n                 name=\'FeBlock\'):\n        super().__init__(name=name)\n        self.num_inputs = num_inputs\n        self.fmaps = fmaps\n        self.kwidth = kwidth\n        self.stride = stride\n        self.pad_mode = pad_mode\n        self.sincnet = sincnet\n        if sincnet:\n            # only one-channel signal can be analyzed\n            assert num_inputs == 1, num_inputs\n            self.conv = SincConv_fast(1, fmaps,\n                                      kwidth, \n                                      sample_rate=sr,\n                                      padding=\'SAME\',\n                                      stride=stride,\n                                      pad_mode=pad_mode)\n        else:\n            self.conv = nn.Conv1d(num_inputs,\n                                  fmaps,\n                                  kwidth,\n                                  stride)\n        self.norm = build_norm_layer(norm_type,\n                                     self.conv,\n                                     fmaps)\n        self.act = nn.PReLU(fmaps)\n\n\n    def forward(self, x):\n        if self.kwidth > 1 and not self.sincnet:\n            # compute pad factor\n            if self.stride > 1:\n                P = (self.kwidth // 2 - 1,\n                     self.kwidth // 2)\n            else:\n                P = (self.kwidth // 2,\n                     self.kwidth // 2)\n            x = F.pad(x, P, mode=self.pad_mode)\n        h = self.conv(x)\n        h = forward_norm(h, self.norm)\n        h = self.act(h)\n        return h\n\n\nclass VQEMA(nn.Module):\n    """""" VQ w/ Exp. Moving Averages,\n        as in (https://arxiv.org/pdf/1711.00937.pdf A.1).\n        Partly based on\n        https://github.com/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb\n    """"""\n    def __init__(self, emb_K, emb_dim, beta,\n                 gamma, eps=1e-5):\n        super().__init__()\n        self.emb_K = emb_K\n        self.emb_dim = emb_dim\n        self.emb = nn.Embedding(self.emb_K,\n                                self.emb_dim)\n        self.emb.weight.data.normal_()\n        self.beta = beta\n        self.gamma = gamma\n        self.register_buffer(\'ema_cluster_size\', torch.zeros(emb_K))\n        self.ema_w = nn.Parameter(torch.Tensor(emb_K, emb_dim))\n        self.ema_w.data.normal_()\n\n        self.gamma = gamma\n        self.eps = eps\n\n    def forward(self, inputs):\n        # convert inputs [B, F, T] -> [BxT, F]\n        inputs = inputs.permute(0, 2, 1).contiguous()\n        input_shape = inputs.shape\n        flat_input = inputs.view(-1, self.emb_dim)\n        device = \'cuda\' if inputs.is_cuda else \'cpu\'\n\n        # TODO: UNDERSTAND THIS COMPUTATION\n        # compute distances\n        dist = (torch.sum(flat_input ** 2, dim=1, keepdim=True) + \\\n                torch.sum(self.emb.weight ** 2, dim=1) - \\\n                2 * torch.matmul(flat_input, self.emb.weight.t()))\n\n        # Encoding\n        enc_indices = torch.argmin(dist, dim=1).unsqueeze(1)\n        enc = torch.zeros(enc_indices.shape[0], self.emb_K).to(device)\n        enc.scatter_(1, enc_indices, 1)\n        \n        # Use EMA to update emb vectors\n        if self.training:\n            self.ema_cluster_size = self.ema_cluster_size * self.gamma + \\\n                    (1 - self.gamma) * torch.sum(enc, 0)\n            n = torch.sum(self.ema_cluster_size.data)\n            self.ema_cluster_size = (\n                (self.ema_cluster_size + self.eps) / \\\n                (n + self.emb_K * self.eps) * n\n            )\n            dw = torch.matmul(enc.t(), flat_input)\n            self.ema_w = nn.Parameter(self.ema_w * self.gamma + \\\n                                      (1 - self.gamma) * dw)\n            self.emb.weight = nn.Parameter(self.ema_w / \\\n                                           self.ema_cluster_size.unsqueeze(1))\n\n        # Quantize and reshape\n        Q = torch.matmul(enc, self.emb.weight).view(input_shape)\n\n        # Loss \n        e_latent_loss = torch.mean((Q.detach() - inputs) ** 2)\n        loss = self.beta * e_latent_loss\n\n        Q = inputs + (Q - inputs).detach()\n        avg_probs = torch.mean(enc, dim=0)\n        # perplexity\n        PP = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n        return loss, Q.permute(0, 2, 1).contiguous(), PP, enc\n\n\n\nif __name__ == \'__main__\':\n    """"""\n    import matplotlib\n    import matplotlib.pyplot as plt\n    import numpy as np\n    # 800 samples @ 16kHz is 50ms\n    T = 800\n    # n = 20 z time-samples per frame\n    n = 20\n    zgen = ZGen(n, T // n, \n                z_amp=0.5)\n    all_z = None\n    for t in range(0, 200, 5):\n        time_idx = torch.LongTensor([t])\n        z_ten = zgen(time_idx)\n        print(z_ten.size())\n        z_ten = z_ten.squeeze()\n        if all_z is None:\n            all_z = z_ten\n        else:\n            all_z = np.concatenate((all_z, z_ten), axis=1)\n    N = 20\n    for k in range(N):\n        plt.subplot(N, 1, k + 1)\n        plt.plot(all_z[k, :], label=k)\n        plt.ylabel(k)\n    plt.show()\n\n    # ResBlock\n    resblock = ResBlock1D(40, 100, 5, dilation=8)\n    print(resblock)\n    z = z_ten.unsqueeze(0)\n    print(\'Z size: \', z.size())\n    y = resblock(z)\n    print(\'Y size: \', y.size())\n\n    x = torch.randn(1, 1, 16) \n    deconv = GDeconv1DBlock(1, 1, 31)\n    y = deconv(x)\n    print(\'x: {} -> y: {} deconv\'.format(x.size(),\n                                         y.size()))\n    conv = GConv1DBlock(1, 1, 31, stride=4)\n    z = conv(y)\n    print(\'y: {} -> z: {} conv\'.format(y.size(),\n                                       z.size()))\n    """"""\n    #x = torch.randn(1, 1, 16384)\n    #sincnet = SincConv(1024, 251, 16000, padding=\'SAME\')\n    #feblock = FeBlock(1, 100, 251, 1)\n    #y = feblock(x)\n    #print(\'y size: \', y.size())\n    vq = VQEMA(50, 100, 0.25, 0.99)\n    x = torch.randn(10, 100, 160)\n    _, Q, PP , _ = vq(x)\n\n\n\n\n'"
pase/models/Minions/__init__.py,0,b'from .cls_minions import *\nfrom .minions import *\n'
pase/models/Minions/cls_minions.py,12,"b'import torch\nimport torch.nn as nn\nfrom ..frontend import WaveFe\nfrom ..modules import *\nfrom .minions import *\nimport torch.nn.functional as F\nimport json\nimport random\n\ndef cls_worker_maker(cfg, emb_dim):\n    print(""="" * 50)\n    print(""name"", cfg[""name""])\n    print(""="" * 50)\n    if cfg[""name""] == ""mi"":\n        return LIM(cfg, emb_dim)\n\n    elif cfg[""name""] == ""cmi"":\n        return GIM(cfg, emb_dim)\n\n    elif cfg[""name""] == ""spc"":\n        return SPC(cfg, emb_dim)\n\n    elif cfg[""name""] == ""gap"":\n        return Gap(cfg, emb_dim)\n\n    else:\n        return minion_maker(cfg)\n\ndef make_samples(x,augment):\n\n        x_pos = torch.cat((x[0], x[1]), dim=1)\n        x_neg = torch.cat((x[0], x[2]), dim=1)\n\n        if augment:\n            x_pos2 = torch.cat((x[1], x[0]), dim=1)\n            x_neg2 = torch.cat((x[1], x[2]), dim=1)\n\n            x_pos=torch.cat((x_pos,x_pos2),dim=0)\n            x_neg=torch.cat((x_neg,x_neg2),dim=0)\n\n\n\n        return  x_pos, x_neg\n\n\n\ndef make_labels(y):\n    bsz = y.size(0) // 2\n    slen = y.size(2)\n    label = torch.cat((torch.ones(bsz, 1, slen, requires_grad=False), torch.zeros(bsz, 1, slen, requires_grad=False)), dim=0)\n    return label\n\nclass LIM(Model):\n\n    def __init__(self, cfg, emb_dim):\n        super().__init__(name=cfg[\'name\'])\n\n        cfg[\'num_inputs\'] = 2 * emb_dim\n\n        if \'augment\' in cfg.keys():\n            self.augment=cfg[\'augment\']\n        else:\n            self.augment=False\n\n        self.minion = minion_maker(cfg)\n        self.loss = self.minion.loss\n        self.loss_weight = self.minion.loss_weight\n\n    def forward(self, x, alpha=1, device=None):\n        x_pos, x_neg = make_samples(x,self.augment)\n        x = torch.cat((x_pos, x_neg), dim=0).to(device)\n        y = self.minion(x, alpha)\n        label = make_labels(y).to(device)\n        return y, label\n\nclass GIM(Model):\n\n    def __init__(self, cfg, emb_dim):\n        super().__init__(name=cfg[\'name\'])\n\n        cfg[\'num_inputs\'] = 2 * emb_dim\n\n        if \'augment\' in cfg.keys():\n            self.augment=cfg[\'augment\']\n        else:\n            self.augment=False\n\n\n        self.minion = minion_maker(cfg)\n        self.loss = self.minion.loss\n        self.loss_weight = self.minion.loss_weight\n\n    def forward(self, x, alpha=1, device=None):\n        x_pos, x_neg = make_samples(x,self.augment)\n        x = torch.cat((x_pos, x_neg), dim=0).to(device)\n        x = torch.mean(x, dim=2, keepdim=True)\n        y = self.minion(x, alpha)\n        label = make_labels(y).to(device)\n        return y, label\n\nclass SPC(Model):\n\n    def __init__(self, cfg, emb_dim):\n        super().__init__(name=cfg[\'name\'])\n        cfg[\'num_inputs\'] = emb_dim\n\n        self.minion = minion_maker(cfg)\n\n        self.loss = self.minion.loss\n        self.loss_weight = self.minion.loss_weight\n\n    def forward(self, x, alpha=1, device=None):\n        y = self.minion(x, alpha)\n        label = make_labels(y).to(device)\n        return  y, label\n\nclass Gap(Model):\n\n    def __init__(self, cfg, emb_dim):\n        super().__init__(name=cfg[\'name\'])\n\n        cfg[\'num_inputs\'] = 2 * emb_dim\n\n        self.minion = minion_maker(cfg)\n        self.loss = self.minion.loss\n        self.loss_weight = self.minion.loss_weight\n\n    def forward(self, x, alpha=1, device=None):\n        y, label = self.minion(x, alpha)\n        label = label.float().to(device)\n        return y, label\n\nclass AdversarialChunk(Model):\n\n    def __init__(self, cfg, emb_dim):\n        super().__init__(name=cfg[\'name\'])\n\n        self.minion = minion_maker(cfg)\n        self.loss = self.minion.loss\n        self.loss_weight = self.minion.loss_weight\n\n    def forward(self, x, alpha=1, device=None):\n        y, label = self.minion(x, alpha)\n        label = label.float().to(device)\n        return y, label\n'"
pase/models/Minions/minions.py,38,"b'import torch\nimport torch.nn as nn\nfrom ..frontend import WaveFe\nfrom ..modules import *\nimport torch.nn.functional as F\nimport json\nimport random\nfrom pase.utils import *\nimport sys\n\ndef minion_maker(cfg):\n    if isinstance(cfg, str):\n        with open(cfg, ""r"") as f:\n            cfg = json.load(f)\n    print(""="" * 50)\n    print(""name"", cfg[""name""])\n    print(""="" * 50)\n    mtype = cfg.pop(\'type\', \'mlp\')\n    if mtype == \'mlp\':\n        minion = MLPMinion(**cfg)\n    elif mtype == \'decoder\':\n        minion = DecoderMinion(**cfg)\n    elif mtype == \'wavernn\':\n        minion = WaveRNNMinion(**cfg)\n    elif mtype == \'spc\':\n        minion = SPCMinion(**cfg)\n    elif mtype == \'gap\':\n        minion = GapMinion(**cfg)\n    elif mtype == \'gru\':\n        minion = GRUMinion(**cfg)\n    elif mtype == \'regularizer\':\n        minion = RegularizerMinion(**cfg)\n    else:\n        raise TypeError(\'Unrecognized minion type {}\'.format(mtype))\n    return minion\n\nclass RegularizerMinion(object):\n\n\n    def __init__(self, num_inputs=None,\n                 loss=\'MSELoss\',\n                 loss_weight=1.,\n                 name=\'\'):\n        if isinstance(loss, str):\n            self.loss = getattr(nn, loss)()\n        else:\n            self.loss = loss\n        self.loss_weight = loss_weight\n        self.name = name\n\n    def __call__(self, x, alpha=1, device=None):\n        return self.forward(x, alpha=alpha, device=device)\n\n    def forward(self, x, alpha=1, device=None):\n        # identity function\n        return x\n\nclass WaveRNNMinion(Model):\n\n    """""" Based on WaveRNN a publicly available WaveRNN implementation:\n        https://github.com/fatchord/WaveRNN/blob/master/models/fatchord_version.py\n    """"""\n\n    def __init__(self, num_inputs,\n                 rnn_dims=512,\n                 fc_dims=512,\n                 bits=9,\n                 sample_rate=16000,\n                 hop_length=160,\n                 mode=\'RAW\',\n                 pad=2,\n                 upsample_cfg={},\n                 loss=None,\n                 loss_weight=1.,\n                 keys=None,\n                 name=\'WaveRNNMinion\'):\n        super().__init__(name=name)\n        feat_dims = num_inputs\n        self.num_inputs = num_inputs\n        self.loss = loss\n        self.loss_weight = loss_weight\n        self.keys = keys\n        self.mode = mode\n        self.pad = pad\n        if self.mode == \'RAW\':\n            self.n_classes = 2 ** bits\n        elif self.mode == \'MOL\':\n            self.n_classes = 30\n        else:\n            RuntimeError(""Unknown model mode value - "", self.mode)\n\n        upsample_cfg[\'feat_dims\'] = num_inputs\n        upsample_cfg[\'pad\'] = pad\n        self.upsample = UpsampleNetwork(**upsample_cfg)\n\n        self.rnn_dims = rnn_dims\n        self.aux_dims = self.upsample.num_outputs // 4\n        self.hop_length = hop_length\n        self.sample_rate = sample_rate\n\n        self.I = nn.Linear(feat_dims + self.aux_dims + 1, rnn_dims)\n        self.rnn1 = nn.GRU(rnn_dims, rnn_dims, batch_first=True)\n        self.rnn2 = nn.GRU(rnn_dims + self.aux_dims, rnn_dims, batch_first=True)\n        self.fc1 = nn.Linear(rnn_dims + self.aux_dims, fc_dims)\n        self.fc2 = nn.Linear(fc_dims + self.aux_dims, fc_dims)\n        self.fc3 = nn.Linear(fc_dims, self.n_classes)\n\n        self.step = nn.Parameter(torch.zeros(1).long(), requires_grad=False)\n\n        if keys is None:\n            keys = [name]\n        self.sg = ScaleGrad()\n\n    def forward(self, x, mels, alpha=1, device=None):\n        self.sg.apply(x, alpha)\n        device = next(self.parameters()).device  # use same device as parameters\n        \n        self.step += 1\n        bsize = x.size(0)\n        h1 = torch.zeros(1, bsize, self.rnn_dims, device=device)\n        h2 = torch.zeros(1, bsize, self.rnn_dims, device=device)\n        mels, aux = self.upsample(mels)\n\n        aux_idx = [self.aux_dims * i for i in range(5)]\n        a1 = aux[:, :, aux_idx[0]:aux_idx[1]]\n        a2 = aux[:, :, aux_idx[1]:aux_idx[2]]\n        a3 = aux[:, :, aux_idx[2]:aux_idx[3]]\n        a4 = aux[:, :, aux_idx[3]:aux_idx[4]]\n\n        x = torch.cat([x.unsqueeze(-1), mels, a1], dim=2)\n        x = self.I(x)\n        res = x\n        x, _ = self.rnn1(x, h1)\n\n        x = x + res\n        res = x\n        x = torch.cat([x, a2], dim=2)\n        x, _ = self.rnn2(x, h2)\n\n        x = x + res\n        x = torch.cat([x, a3], dim=2)\n        x = F.relu(self.fc1(x))\n\n        x = torch.cat([x, a4], dim=2)\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\n    def generate(self, mels, save_path, batched, target, overlap, mu_law):\n        device = next(self.parameters()).device  # use same device as parameters\n\n        mu_law = mu_law if self.mode == \'RAW\' else False\n\n        self.eval()\n        output = []\n        start = time.time()\n        rnn1 = self.get_gru_cell(self.rnn1)\n        rnn2 = self.get_gru_cell(self.rnn2)\n\n        with torch.no_grad():\n\n            mels = torch.as_tensor(mels, device=device)\n            wave_len = (mels.size(-1) - 1) * self.hop_length\n            mels = self.pad_tensor(mels.transpose(1, 2), pad=self.pad, side=\'both\')\n            mels, aux = self.upsample(mels.transpose(1, 2))\n\n            if batched:\n                mels = self.fold_with_overlap(mels, target, overlap)\n                aux = self.fold_with_overlap(aux, target, overlap)\n\n            b_size, seq_len, _ = mels.size()\n\n            h1 = torch.zeros(b_size, self.rnn_dims, device=device)\n            h2 = torch.zeros(b_size, self.rnn_dims, device=device)\n            x = torch.zeros(b_size, 1, device=device)\n\n            d = self.aux_dims\n            aux_split = [aux[:, :, d * i:d * (i + 1)] for i in range(4)]\n\n            for i in range(seq_len):\n\n                m_t = mels[:, i, :]\n\n                a1_t, a2_t, a3_t, a4_t = \\\n                    (a[:, i, :] for a in aux_split)\n\n                x = torch.cat([x, m_t, a1_t], dim=1)\n                x = self.I(x)\n                h1 = rnn1(x, h1)\n\n                x = x + h1\n                inp = torch.cat([x, a2_t], dim=1)\n                h2 = rnn2(inp, h2)\n\n                x = x + h2\n                x = torch.cat([x, a3_t], dim=1)\n                x = F.relu(self.fc1(x))\n\n                x = torch.cat([x, a4_t], dim=1)\n                x = F.relu(self.fc2(x))\n\n                logits = self.fc3(x)\n\n                if self.mode == \'MOL\':\n                    sample = sample_from_discretized_mix_logistic(logits.unsqueeze(0).transpose(1, 2))\n                    output.append(sample.view(-1))\n                    # x = torch.FloatTensor([[sample]]).cuda()\n                    x = sample.transpose(0, 1)\n\n                elif self.mode == \'RAW\':\n                    posterior = F.softmax(logits, dim=1)\n                    distrib = torch.distributions.Categorical(posterior)\n\n                    sample = 2 * distrib.sample().float() / (self.n_classes - 1.) - 1.\n                    output.append(sample)\n                    x = sample.unsqueeze(-1)\n                else:\n                    raise RuntimeError(""Unknown model mode value - "", self.mode)\n\n                #if i % 100 == 0: self.gen_display(i, seq_len, b_size, start)\n\n        output = torch.stack(output).transpose(0, 1)\n        output = output.cpu().numpy()\n        output = output.astype(np.float64)\n\n        if batched:\n            output = self.xfade_and_unfold(output, target, overlap)\n        else:\n            output = output[0]\n\n        if mu_law:\n            output = decode_mu_law(output, self.n_classes, False)\n\n        # Fade-out at the end to avoid signal cutting out suddenly\n        fade_out = np.linspace(1, 0, 20 * self.hop_length)\n        output = output[:wave_len]\n        output[-20 * self.hop_length:] *= fade_out\n\n        save_wav(output, save_path)\n\n        self.train()\n\n        return output\n\n\n    def get_gru_cell(self, gru):\n        gru_cell = nn.GRUCell(gru.input_size, gru.hidden_size)\n        gru_cell.weight_hh.data = gru.weight_hh_l0.data\n        gru_cell.weight_ih.data = gru.weight_ih_l0.data\n        gru_cell.bias_hh.data = gru.bias_hh_l0.data\n        gru_cell.bias_ih.data = gru.bias_ih_l0.data\n        return gru_cell\n\n    def pad_tensor(self, x, pad, side=\'both\'):\n        # NB - this is just a quick method i need right now\n        # i.e., it won\'t generalise to other shapes/dims\n        b, t, c = x.size()\n        total = t + 2 * pad if side == \'both\' else t + pad\n        padded = torch.zeros(b, total, c, device=x.device)\n        if side == \'before\' or side == \'both\':\n            padded[:, pad:pad + t, :] = x\n        elif side == \'after\':\n            padded[:, :t, :] = x\n        return padded\n\n    def fold_with_overlap(self, x, target, overlap):\n\n        \'\'\' Fold the tensor with overlap for quick batched inference.\n            Overlap will be used for crossfading in xfade_and_unfold()\n        Args:\n            x (tensor)    : Upsampled conditioning features.\n                            shape=(1, timesteps, features)\n            target (int)  : Target timesteps for each index of batch\n            overlap (int) : Timesteps for both xfade and rnn warmup\n        Return:\n            (tensor) : shape=(num_folds, target + 2 * overlap, features)\n        Details:\n            x = [[h1, h2, ... hn]]\n            Where each h is a vector of conditioning features\n            Eg: target=2, overlap=1 with x.size(1)=10\n            folded = [[h1, h2, h3, h4],\n                      [h4, h5, h6, h7],\n                      [h7, h8, h9, h10]]\n        \'\'\'\n\n        _, total_len, features = x.size()\n\n        # Calculate variables needed\n        num_folds = (total_len - overlap) // (target + overlap)\n        extended_len = num_folds * (overlap + target) + overlap\n        remaining = total_len - extended_len\n\n        # Pad if some time steps poking out\n        if remaining != 0:\n            num_folds += 1\n            padding = target + 2 * overlap - remaining\n            x = self.pad_tensor(x, padding, side=\'after\')\n\n        folded = torch.zeros(num_folds, target + 2 * overlap, features, device=x.device)\n\n        # Get the values for the folded tensor\n        for i in range(num_folds):\n            start = i * (target + overlap)\n            end = start + target + 2 * overlap\n            folded[i] = x[:, start:end, :]\n\n        return folded\n\n    def xfade_and_unfold(self, y, target, overlap):\n\n        \'\'\' Applies a crossfade and unfolds into a 1d array.\n        Args:\n            y (ndarry)    : Batched sequences of audio samples\n                            shape=(num_folds, target + 2 * overlap)\n                            dtype=np.float64\n            overlap (int) : Timesteps for both xfade and rnn warmup\n        Return:\n            (ndarry) : audio samples in a 1d array\n                       shape=(total_len)\n                       dtype=np.float64\n        Details:\n            y = [[seq1],\n                 [seq2],\n                 [seq3]]\n            Apply a gain envelope at both ends of the sequences\n            y = [[seq1_in, seq1_target, seq1_out],\n                 [seq2_in, seq2_target, seq2_out],\n                 [seq3_in, seq3_target, seq3_out]]\n            Stagger and add up the groups of samples:\n            [seq1_in, seq1_target, (seq1_out + seq2_in), seq2_target, ...]\n        \'\'\'\n\n        num_folds, length = y.shape\n        target = length - 2 * overlap\n        total_len = num_folds * (target + overlap) + overlap\n\n        # Need some silence for the rnn warmup\n        silence_len = overlap // 2\n        fade_len = overlap - silence_len\n        silence = np.zeros((silence_len), dtype=np.float64)\n\n        # Equal power crossfade\n        t = np.linspace(-1, 1, fade_len, dtype=np.float64)\n        fade_in = np.sqrt(0.5 * (1 + t))\n        fade_out = np.sqrt(0.5 * (1 - t))\n\n        # Concat the silence to the fades\n        fade_in = np.concatenate([silence, fade_in])\n        fade_out = np.concatenate([fade_out, silence])\n\n        # Apply the gain to the overlap samples\n        y[:, :overlap] *= fade_in\n        y[:, -overlap:] *= fade_out\n\n        unfolded = np.zeros((total_len), dtype=np.float64)\n\n        # Loop to add up all the samples\n        for i in range(num_folds):\n            start = i * (target + overlap)\n            end = start + target + 2 * overlap\n            unfolded[start:end] += y[i]\n\n        return unfolded\n\n\nclass DecoderMinion(Model):\n\n    def __init__(self, num_inputs,\n                 num_outputs,\n                 dropout, \n                 dropout_time=0.0,\n                 shuffle = False,\n                 shuffle_depth = 7,\n                 hidden_size=256,\n                 hidden_layers=2,\n                 fmaps=[256, 256, 128, 128, 128, 64, 64],\n                 strides=[2, 2, 2, 2, 2, 5],\n                 kwidths=[2, 2, 2, 2, 2, 5],\n                 norm_type=None,\n                 skip=False,\n                 loss=None,\n                 loss_weight=1.,\n                 keys=None,\n                 name=\'DecoderMinion\'):\n        super().__init__(name=name)\n        self.num_inputs = num_inputs\n        self.num_outputs = num_outputs\n        self.dropout = dropout\n        self.dropout_time = dropout_time\n        self.shuffle = shuffle\n        self.shuffle_depth = shuffle_depth\n        self.skip = skip\n        self.hidden_size = hidden_size\n        self.hidden_layers = hidden_layers\n        self.fmaps = fmaps\n        self.strides = strides\n        self.kwidths = kwidths\n        self.norm_type = norm_type\n        self.loss = loss\n        self.loss_weight = loss_weight\n        self.keys = keys\n\n        if keys is None:\n            keys = [name]\n        self.blocks = nn.ModuleList()\n        ninp = num_inputs\n        # First go through deconvolving structure\n        for (fmap, kw, stride) in zip(fmaps, kwidths, strides):\n            block = GDeconv1DBlock(ninp, fmap, kw, stride,\n                                   norm_type=norm_type)\n            self.blocks.append(block)\n            ninp = fmap\n\n        for _ in range(hidden_layers):\n            self.blocks.append(MLPBlock(ninp,\n                                        hidden_size, dropout))\n            ninp = hidden_size\n        self.W = nn.Conv1d(hidden_size, num_outputs, 1)\n        self.sg = ScaleGrad()\n\n    def forward(self, x, alpha=1, device=None):\n        \n        self.sg.apply(x, alpha)\n        \n        # The following part of the code drops out some time steps, but the worker should reconstruct all of them (i.e, the original signal)\n        # This way we encourage learning features with a larger contextual information\n        if self.dropout_time > 0:\n            mask=(torch.FloatTensor(x.shape[0],x.shape[2]).to(\'cuda\').uniform_() > self.dropout_time).float().unsqueeze(1)\n            x=x*mask\n\n        # The following function (when active) shuffles the time order of the input PASE features. Note that the shuffle has a certain depth (shuffle_depth). \n        # This allows shuffling features that are reasonably close, hopefully encouraging PASE to learn a longer context.\n        if self.shuffle:\n            x = torch.split(x, self.shuffle_depth, dim=2)\n            shuffled_x=[]\n            for elem in x:\n                    r=torch.randperm(elem.shape[2])\n                    shuffled_x.append(elem[:,:,r])\n\n            x=torch.cat(shuffled_x,dim=2)\n\n        h = x\n        for bi, block in enumerate(self.blocks, start=1):\n            h_ = h\n            h = block(h)\n        y = self.W(h)\n        if self.skip:\n            return y, h\n        else:\n            return y\n\n\nclass MLPMinion(Model):\n\n    def __init__(self, num_inputs,\n                 num_outputs,\n                 dropout, dropout_time=0.0,hidden_size=256,\n                 dropin=0.0,\n                 hidden_layers=2,\n                 context=1,\n                 tie_context_weights=False,\n                 skip=True,\n                 loss=None,\n                 loss_weight=1.,\n                 keys=None,\n                 augment=False,\n                 r=1, \n                 name=\'MLPMinion\',\n                 ratio_fixed=None, range_fixed=None, \n                 dropin_mode=\'std\', drop_channels=False, emb_size=100):\n        super().__init__(name=name)\n        # Implemented with Conv1d layers to not\n        # transpose anything in time, such that\n        # frontend and minions are attached very simply\n        self.num_inputs = num_inputs\n        assert context % 2 != 0, context\n        self.context = context\n        self.tie_context_weights = tie_context_weights\n        self.dropout = dropout\n        self.dropout_time = dropout_time\n        self.skip = skip\n        self.hidden_size = hidden_size\n        self.hidden_layers = hidden_layers\n        self.loss = loss\n        self.loss_weight = loss_weight\n        self.keys = keys\n        if keys is None:\n            keys = [name]\n        # r frames predicted at once in the output\n        self.r = r\n        # multiplies number of output dims\n        self.num_outputs = num_outputs * r\n        self.blocks = nn.ModuleList()\n        ninp = num_inputs\n        for hi in range(hidden_layers):\n            self.blocks.append(MLPBlock(ninp,\n                                        hidden_size,\n                                        din=dropin,\n                                        dout=dropout,\n                                        context=context,\n                                        tie_context_weights=tie_context_weights,\n                                        emb_size=emb_size, \n                                        dropin_mode=dropin_mode,\n                                        range_fixed=range_fixed,\n                                        ratio_fixed=ratio_fixed,\n                                        drop_channels=drop_channels))\n            ninp = hidden_size\n            # in case context has been assigned,\n            # it is overwritten to 1\n            context = 1\n        self.W = nn.Conv1d(ninp, self.num_outputs, context,\n                           padding=context//2)\n        self.sg = ScaleGrad()\n\n    def forward(self, x, alpha=1, device=None):\n        self.sg.apply(x, alpha)\n        \n        if self.dropout_time > 0 and self.context > 1:\n            mask=(torch.FloatTensor(x.shape[0],x.shape[2]).to(\'cuda\').uniform_() > self.dropout_time).float().unsqueeze(1)\n            x=x*mask\n\n        h = x\n        for bi, block in enumerate(self.blocks, start=1):\n            h = block(h)\n        y = self.W(h)\n        if self.skip:\n            return y, h\n        else:\n            return y\n\n\nclass GRUMinion(Model):\n\n    def __init__(self, num_inputs,\n                 num_outputs,\n                 dropout, hidden_size=256,\n                 hidden_layers=2,\n                 skip=True,\n                 loss=None,\n                 loss_weight=1.,\n                 keys=None,\n                 name=\'GRUMinion\'):\n        super().__init__(name=name)\n        self.num_inputs = num_inputs\n        self.num_outputs = num_outputs\n        self.dropout = dropout\n        self.skip = skip\n        self.hidden_size = hidden_size\n        self.hidden_layers = hidden_layers\n        self.loss = loss\n        self.loss_weight = loss_weight\n        self.keys = keys\n        if keys is None:\n            keys = [name]\n        self.blocks = nn.ModuleList()\n        ninp = num_inputs\n        self.rnn = nn.GRU(ninp,\n                          hidden_size,\n                          num_layers=hidden_layers,\n                          batch_first=True,\n                          dropout=dropout)\n        self.W = nn.Conv1d(hidden_size, num_outputs, 1)\n        self.sg = ScaleGrad()\n\n    def forward(self, x, alpha=1, device=None):\n        self.sg.apply(x, alpha)\n        h, _ = self.rnn(x.transpose(1, 2))\n        h = h.transpose(1, 2)\n        y = self.W(h)\n        if self.skip:\n            return y, h\n        else:\n            return y\n\n\nclass SPCMinion(MLPMinion):\n\n    def __init__(self, num_inputs,\n                 num_outputs,\n                 dropout, hidden_size=256,\n                 hidden_layers=2,\n                 ctxt_frames=5,\n                 seq_pad=16,\n                 skip=True,\n                 loss=None,\n                 loss_weight=1.,\n                 keys=None,\n                 name=\'SPCMinion\'):\n        # num_inputs is code dimension in each time-step,\n        # so the MLP has [num_inputs x ctxt_frames] inputs\n        # as we unroll time dimension to fixed-sized windows\n        print(\'num_inputs: \', num_inputs)\n        print(\'ctxt_frames: \', ctxt_frames)\n        num_inputs = (ctxt_frames + 1) * num_inputs\n        print(\'num_inputs: \', num_inputs)\n        super().__init__(num_inputs=num_inputs,\n                         num_outputs=num_outputs,\n                         dropout=dropout,\n                         hidden_size=hidden_size,\n                         hidden_layers=hidden_layers,\n                         skip=skip,\n                         loss=loss,\n                         loss_weight=loss_weight,\n                         keys=keys,\n                         name=name)\n        self.ctxt_frames = ctxt_frames\n        self.seq_pad = seq_pad\n        self.sg = ScaleGrad()\n\n    def forward(self, x, alpha=1, device=None):\n        # x is a batch of sequences\n        # of dims [B, channels, time]\n        # first select a ""central"" time-step\n        # with enough seq_pad an ctxt_frames\n        # margin M = seq_pad + ctxt_frames on both sides\n        self.sg.apply(x, alpha)\n        seq_pad = self.seq_pad\n        N = self.ctxt_frames\n        M = seq_pad + N\n        idxs_t = list(range(M + 1, x.size(2) - M))\n        t = random.choice(idxs_t)\n\n        bsz = x.size(0)\n\n        # now select future_t (to begin future seq)\n        idxs_ft = list(range(t + seq_pad, x.size(2) - N))\n        future_t = random.choice(idxs_ft)\n        idxs_pt = list(range(N, t - seq_pad))\n        past_t = random.choice(idxs_pt)\n\n        # chunk input sequences and current frame\n        future = x[:, :, future_t:future_t + N].contiguous().view(bsz, -1)\n        past = x[:, :, past_t - N:past_t].contiguous().view(bsz, -1)\n        current = x[:, :, t].contiguous()\n\n        # positive batch (future data)\n        pos = torch.cat((current, future), dim=1)\n        # negative batch (past data)\n        neg = torch.cat((current, past), dim=1)\n\n        # forward both jointly\n        x_full = torch.cat((pos, neg), dim=0).unsqueeze(2)\n        h = x_full\n        for bi, block in enumerate(self.blocks, start=1):\n            h = block(h)\n        y = self.W(h)\n        if self.skip:\n            return y, h\n        else:\n            return y\n\nclass GapMinion(MLPMinion):\n\n    def __init__(self, num_inputs,\n                 num_outputs,\n                 dropout, hidden_size=256,\n                 hidden_layers=2,\n                 skip=True,\n                 loss=None,\n                 loss_weight=1.,\n                 keys=None,\n                 name=\'GapMinion\'):\n        super().__init__(num_inputs=num_inputs,\n                         num_outputs=num_outputs,\n                         dropout=dropout,\n                         hidden_size=hidden_size,\n                         hidden_layers=hidden_layers,\n                         skip=skip,\n                         loss=loss,\n                         loss_weight=loss_weight,\n                         keys=keys,\n                         name=name)\n        self.sg = ScaleGrad()\n\n    def forward(self, x, alpha=1, device=None):\n        # x is a batch of sequences\n        # of dims [B, channels, time]\n        # Select randomly two chunks out of T possible\n        self.sg.apply(x, alpha)\n        T = x.shape[2]\n        aidx = torch.LongTensor(np.random.randint(0, T, size=x.shape[0]))\n        bidx = torch.LongTensor(np.random.randint(0, T, size=x.shape[0]))\n        x_a = []\n        x_b = []\n        dists = []\n        for i_, (aidx_, bidx_) in enumerate(zip(aidx, bidx)):\n            x_a.append(x[i_, :, aidx_].unsqueeze(0))\n            x_b.append(x[i_, :, bidx_].unsqueeze(0))\n            dist = torch.abs(aidx_ - bidx_) / (T - 1)\n            dists.append(dist)\n        x_a = torch.cat(x_a, dim=0)\n        x_b = torch.cat(x_b, dim=0)\n        x_full = torch.cat((x_a, x_b), dim=1).unsqueeze(2)\n        dists = torch.LongTensor(dists)\n        dists = dists.view(-1, 1, 1)\n        \n        h = x_full\n        for bi, block in enumerate(self.blocks, start=1):\n            h = block(h)\n        y = self.W(h)\n        # concat groundtruth to preds\n        if self.skip:\n            return y, h, dists\n        else:\n            return y, dists\n\n'"
pase/models/WorkerScheduler/__init__.py,0,b''
pase/models/WorkerScheduler/encoder.py,6,"b""import torch.nn as nn\nfrom ..modules import *\nfrom ..aspp import ASPP, aspp_resblock\nimport torch.nn.functional as F\nimport json\nimport random\n\nclass encoder(Model):\n\n    def __init__(self, frontend, name='encoder'):\n        super().__init__(name)\n        self.frontend = frontend\n        self.emb_dim = self.frontend.emb_dim\n\n    def forward(self, batch, device):\n\n        if type(batch) == dict:\n            x = torch.cat((batch['chunk'],\n                                 batch['chunk_ctxt'],\n                                 batch['chunk_rand']),\n                                dim=0).to(device)\n        else:\n            x = batch\n\n        y = self.frontend(x)\n\n        if type(batch) == dict:\n            embedding = torch.chunk(y, 3, dim=0)\n\n            chunk = embedding[0]\n\n            return embedding, chunk\n        else:\n            return y\n\n\nclass aspp_res_encoder(Model):\n\n    def __init__(self, sinc_out, hidden_dim, kernel_sizes=[11, 11, 11, 11], strides=[10, 4, 2, 2], dilations=[1, 6, 12, 18], fmaps=48, name='aspp_encoder', pool2d=False, rnn_pool=False, dense=False):\n        super().__init__(name=name)\n        self.sinc = SincConv_fast(1, sinc_out, 251,\n                                  sample_rate=16000,\n                                  padding='SAME',\n                                  stride=1,\n                                  pad_mode='reflect'\n                                  )\n\n\n        self.ASPP_blocks = nn.ModuleList()\n\n        for i in range(len(kernel_sizes)):\n            if i == 0:\n                self.ASPP_blocks.append(aspp_resblock(sinc_out, hidden_dim, kernel_sizes[i], strides[i], dilations, fmaps[i], pool2d[i], dense))\n            else:\n                self.ASPP_blocks.append(aspp_resblock(hidden_dim, hidden_dim, kernel_sizes[i], strides[i], dilations, fmaps[i], pool2d[i], dense))\n\n\n        self.rnn_pool = rnn_pool\n\n        if rnn_pool:\n            self.rnn = build_rnn_block(hidden_dim, hidden_dim // 2,\n                                       rnn_layers=1,\n                                       rnn_type='qrnn',\n                                       bidirectional=True,\n                                       dropout=0)\n            self.W = nn.Conv1d(hidden_dim, hidden_dim, 1)\n\n\n        self.emb_dim = hidden_dim\n\n\n\n    def forward(self, batch, device):\n\n        if type(batch) == dict:\n            x = torch.cat((batch['chunk'],\n                           batch['chunk_ctxt'],\n                           batch['chunk_rand']),\n                          dim=0).to(device)\n        else:\n            x = batch\n\n        sinc_out = self.sinc(x)\n\n        out = sinc_out\n        for block in self.ASPP_blocks:\n            out = block(out)\n\n        h = out\n\n        if self.rnn_pool:\n            h = h.transpose(1, 2).transpose(0, 1)\n            h, _ = self.rnn(h)\n            h = h.transpose(0, 1).transpose(1, 2)\n\n\n        if type(batch) == dict:\n            embedding = torch.chunk(h, 3, dim=0)\n\n            chunk = embedding[0]\n            return embedding, chunk\n        else:\n            return h\n\n"""
pase/models/WorkerScheduler/lr_scheduler.py,0,"b'import math\n\nclass LR_Scheduler(object):\n    """"""Learning Rate Scheduler\n    Step mode: ``lr = baselr * 0.1 ^ {floor(epoch-1 / lr_step)}``\n    Cosine mode: ``lr = baselr * 0.5 * (1 + cos(iter/maxiter))``\n    Poly mode: ``lr = baselr * (1 - iter/maxiter) ^ 0.9``\n    Args:\n        args:\n          :attr:`args.lr_scheduler` lr scheduler mode (`cos`, `poly`),\n          :attr:`args.lr` base learning rate, :attr:`args.epochs` number of epochs,\n          :attr:`args.lr_step`\n        iters_per_epoch: number of iterations per epoch\n    """"""\n    def __init__(self, mode, optim_name, base_lr, num_epochs, iters_per_epoch=0,\n                 lr_step=30, warmup_epochs=0):\n        self.mode = mode\n        self.name = optim_name\n        print(\'Using {} LR Scheduler for {}!\'.format(self.mode, optim_name))\n        self.lr = base_lr\n        if mode == \'step\':\n            assert lr_step\n        self.lr_step = lr_step\n        self.iters_per_epoch = iters_per_epoch\n        self.N = num_epochs * iters_per_epoch\n        self.epoch = -1\n        self.warmup_iters = warmup_epochs * iters_per_epoch\n\n    def __call__(self, optimizer, i, epoch, loss):\n        T = epoch * self.iters_per_epoch + i\n        if self.mode == \'cos\':\n            lr = 0.5 * self.lr * (1 + math.cos(1.0 * T / self.N * math.pi))\n        elif self.mode == \'poly\':\n            lr = self.lr * pow((1 - 1.0 * T / self.N), 0.9)\n        elif self.mode == \'step\':\n            lr = self.lr * (0.1 ** (epoch // self.lr_step))\n        else:\n            raise NotImplemented\n        # warm up lr schedule\n        if self.warmup_iters > 0 and T < self.warmup_iters:\n            lr = lr * 1.0 * T / self.warmup_iters\n        # if epoch > self.epoch:\n\n        # if self.name == ""frontend"":\n        #     print(\'%s, learning rate = %.8f, loss = %.4f\' % (""total"", lr, loss))\n        # else:\n        #     print(\'%s, learning rate = %.8f, loss = %.4f\' % (self.name, lr, loss))\n\n        self.epoch = epoch\n        assert lr >= 0\n        self._adjust_learning_rate(optimizer, lr)\n        return lr\n\n    def _adjust_learning_rate(self, optimizer, lr):\n        if len(optimizer.param_groups) == 1:\n            optimizer.param_groups[0][\'lr\'] = lr\n        else:\n            # enlarge the lr at the head\n            optimizer.param_groups[0][\'lr\'] = lr\n            for i in range(1, len(optimizer.param_groups)):\n                optimizer.param_groups[i][\'lr\'] = lr * 10'"
pase/models/WorkerScheduler/min_norm_solvers.py,3,"b'import numpy as np\nimport torch\n\n# https://github.com/intel-isl/MultiObjectiveOptimization/blob/master/multi_task/min_norm_solvers.py\nclass MinNormSolver:\n    MAX_ITER = 250\n    STOP_CRIT = 1e-5\n\n    def _min_norm_element_from2(v1v1, v1v2, v2v2):\n        """"""\n        Analytical solution for min_{c} |cx_1 + (1-c)x_2|_2^2\n        d is the distance (objective) optimzed\n        v1v1 = <x1,x1>\n        v1v2 = <x1,x2>\n        v2v2 = <x2,x2>\n        """"""\n        if v1v2 >= v1v1:\n            # Case: Fig 1, third column\n            gamma = 0.999\n            cost = v1v1\n            return gamma, cost\n        if v1v2 >= v2v2:\n            # Case: Fig 1, first column\n            gamma = 0.001\n            cost = v2v2\n            return gamma, cost\n        # Case: Fig 1, second column\n        gamma = -1.0 * ((v1v2 - v2v2) / (v1v1 + v2v2 - 2 * v1v2))\n        cost = v2v2 + gamma * (v1v2 - v2v2)\n        return gamma, cost\n\n    def _min_norm_2d(vecs, dps):\n        """"""\n        Find the minimum norm solution as combination of two points\n        This is correct only in 2D\n        ie. min_c |\\sum c_i x_i|_2^2 st. \\sum c_i = 1 , 1 >= c_1 >= 0 for all i, c_i + c_j = 1.0 for some i, j\n        """"""\n        dmin = 1e8\n        for i in range(len(vecs)):\n            for j in range(i + 1, len(vecs)):\n                if (i, j) not in dps:\n                    dps[(i, j)] = 0.0\n                    for k in range(len(vecs[i])):\n                        dps[(i, j)] += torch.dot(vecs[i][k], vecs[j][k]).item()\n                    dps[(j, i)] = dps[(i, j)]\n                if (i, i) not in dps:\n                    dps[(i, i)] = 0.0\n                    for k in range(len(vecs[i])):\n                        dps[(i, i)] += torch.dot(vecs[i][k], vecs[i][k]).item()\n                if (j, j) not in dps:\n                    dps[(j, j)] = 0.0\n                    for k in range(len(vecs[i])):\n                        dps[(j, j)] += torch.dot(vecs[j][k], vecs[j][k]).item()\n                c, d = MinNormSolver._min_norm_element_from2(dps[(i, i)], dps[(i, j)], dps[(j, j)])\n                if d < dmin:\n                    dmin = d\n                    sol = [(i, j), c, d]\n        return sol, dps\n\n    def _projection2simplex(y):\n        """"""\n        Given y, it solves argmin_z |y-z|_2 st \\sum z = 1 , 1 >= z_i >= 0 for all i\n        """"""\n        m = len(y)\n        sorted_y = np.flip(np.sort(y), axis=0)\n        tmpsum = 0.0\n        tmax_f = (np.sum(y) - 1.0) / m\n        for i in range(m - 1):\n            tmpsum += sorted_y[i]\n            tmax = (tmpsum - 1) / (i + 1.0)\n            if tmax > sorted_y[i + 1]:\n                tmax_f = tmax\n                break\n        return np.maximum(y - tmax_f, np.zeros(y.shape))\n\n    def _next_point(cur_val, grad, n):\n        proj_grad = grad - (np.sum(grad) / n)\n        tm1 = -1.0 * cur_val[proj_grad < 0] / proj_grad[proj_grad < 0]\n        tm2 = (1.0 - cur_val[proj_grad > 0]) / (proj_grad[proj_grad > 0])\n\n        skippers = np.sum(tm1 < 1e-7) + np.sum(tm2 < 1e-7)\n        t = 1\n        if len(tm1[tm1 > 1e-7]) > 0:\n            t = np.min(tm1[tm1 > 1e-7])\n        if len(tm2[tm2 > 1e-7]) > 0:\n            t = min(t, np.min(tm2[tm2 > 1e-7]))\n\n        next_point = proj_grad * t + cur_val\n        next_point = MinNormSolver._projection2simplex(next_point)\n        return next_point\n\n    def find_min_norm_element(vecs):\n        """"""\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the projected gradient descent until convergence\n        """"""\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            grad_dir = -1.0 * np.dot(grad_mat, sol_vec)\n            new_point = MinNormSolver._next_point(sol_vec, grad_dir, n)\n            # Re-compute the inner products for line search\n            v1v1 = 0.0\n            v1v2 = 0.0\n            v2v2 = 0.0\n            for i in range(n):\n                for j in range(n):\n                    v1v1 += sol_vec[i] * sol_vec[j] * dps[(i, j)]\n                    v1v2 += sol_vec[i] * new_point[j] * dps[(i, j)]\n                    v2v2 += new_point[i] * new_point[j] * dps[(i, j)]\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec + (1 - nc) * new_point\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec\n\n    def find_min_norm_element_FW(vecs):\n        """"""\n        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n        Hence, we find the best 2-task solution, and then run the Frank Wolfe until convergence\n        """"""\n        # Solution lying at the combination of two points\n        dps = {}\n        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\n        n = len(vecs)\n        sol_vec = np.zeros(n)\n        sol_vec[init_sol[0][0]] = init_sol[1]\n        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\n        if n < 3:\n            # This is optimal for n=2, so return the solution\n            return sol_vec, init_sol[2]\n\n        iter_count = 0\n\n        grad_mat = np.zeros((n, n))\n        for i in range(n):\n            for j in range(n):\n                grad_mat[i, j] = dps[(i, j)]\n\n        while iter_count < MinNormSolver.MAX_ITER:\n            t_iter = np.argmin(np.dot(grad_mat, sol_vec))\n\n            v1v1 = np.dot(sol_vec, np.dot(grad_mat, sol_vec))\n            v1v2 = np.dot(sol_vec, grad_mat[:, t_iter])\n            v2v2 = grad_mat[t_iter, t_iter]\n\n            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n            new_sol_vec = nc * sol_vec\n            new_sol_vec[t_iter] += 1 - nc\n\n            change = new_sol_vec - sol_vec\n            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n                return sol_vec, nd\n            sol_vec = new_sol_vec\n\n\ndef gradient_normalizers(grads, losses, normalization_type):\n    gn = {}\n    if normalization_type == \'l2\':\n        for t in grads:\n            gn[t] = np.sqrt(np.sum([gr.pow(2).sum().item() for gr in grads[t]]))\n    elif normalization_type == \'loss\':\n        for t in grads:\n            gn[t] = losses[t]\n    elif normalization_type == \'loss+\':\n        for t in grads:\n            gn[t] = losses[t] * np.sqrt(np.sum([gr.pow(2).sum().item() for gr in grads[t]]))\n    elif normalization_type == \'none\':\n        for t in grads:\n            gn[t] = 1.0\n    else:\n        print(\'ERROR: Invalid Normalization Type\')\n    return gn'"
pase/models/WorkerScheduler/radam.py,7,"b""import math\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n        self.buffer = [[None, None, None] for ind in range(10)]\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = self.buffer[int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    else:\n                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:            \n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n\nclass PlainRAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n\n        super(PlainRAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(PlainRAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                beta2_t = beta2 ** state['step']\n                N_sma_max = 2 / (1 - beta2) - 1\n                N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:                    \n                    step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n                else:\n                    step_size = group['lr'] / (1 - beta1 ** state['step'])\n                    p_data_fp32.add_(-step_size, exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n\n\nclass AdamW(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, warmup = 0):\n        defaults = dict(lr=lr, betas=betas, eps=eps,\n                        weight_decay=weight_decay, warmup = warmup)\n        super(AdamW, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(AdamW, self).__setstate__(state)\n\n    def step(self, closure=None):\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                state['step'] += 1\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                denom = exp_avg_sq.sqrt().add_(group['eps'])\n                bias_correction1 = 1 - beta1 ** state['step']\n                bias_correction2 = 1 - beta2 ** state['step']\n                \n                if group['warmup'] > state['step']:\n                    scheduled_lr = 1e-8 + state['step'] * group['lr'] / group['warmup']\n                else:\n                    scheduled_lr = group['lr']\n\n                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n                \n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * scheduled_lr, p_data_fp32)\n\n                p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n\n                p.data.copy_(p_data_fp32)\n\n        return loss\n"""
pase/models/WorkerScheduler/trainer.py,6,"b'from ..Minions.minions import *\nfrom ..Minions.cls_minions import *\nfrom .encoder import encoder\nfrom .lr_scheduler import LR_Scheduler\nfrom ..pase import pase, pase_attention, pase_chunking\nfrom .worker_scheduler import backprop_scheduler\nfrom ...utils import AuxiliarSuperviser, get_grad_norms\nfrom .radam import *\nimport torch\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nimport torch.nn as nn\nimport numpy as np\nimport random\nimport os\nimport pickle\nfrom tqdm import tqdm, trange\ntry:\n    from tensorboardX import SummaryWriter\n    use_tb = True\nexcept ImportError:\n    print(\'cannot import Tensorboard, use pickle for logging\')\n    use_tb = False\n\n\nclass trainer(object):\n    def __init__(self,\n                 frontend=None,\n                 frontend_cfg=None,\n                 att_cfg=None,\n                 minions_cfg=None,\n                 cfg=None,\n                 cls_lst=[],\n                 regr_lst=[],\n                 pretrained_ckpt=None,\n                 tensorboard=None,\n                 backprop_mode=None,\n                 lr_mode = \'step\',\n                 name=\'Pase_base\',\n                 device=None):\n\n        # init the pase from cfg file\n        if len(cls_lst) == 0 and \'cls\' in minions_cfg:\n            cls_lst = [worker[\'name\'] for worker in minions_cfg[\'cls\']]\n        if len(regr_lst) == 0 and \'regr\' in minions_cfg:\n            regr_lst = [worker[\'name\'] for worker in minions_cfg[\'regr\']]\n\n        print(\'Cls: \', cls_lst)\n        print(\'Regr: \', regr_lst)\n\n        if att_cfg:\n            print(""training pase with attention!"")\n            self.model = pase_attention(frontend=frontend,\n                            frontend_cfg=frontend_cfg,\n                            minions_cfg=minions_cfg,\n                            att_cfg=att_cfg,\n                            cls_lst=cls_lst, regr_lst=regr_lst,\n                            K=cfg[\'att_K\'],\n                            avg_factor=cfg[\'avg_factor\'],\n                            att_mode=cfg[\'att_mode\'],\n                            chunk_size=cfg[\'chunk_size\'],\n                            pretrained_ckpt=pretrained_ckpt,\n                            name=name)\n        else:\n            print(""training pase..."")\n            self.model = pase(frontend=frontend,\n                              frontend_cfg=frontend_cfg,\n                              minions_cfg=minions_cfg,\n                              cls_lst=cls_lst, regr_lst=regr_lst,\n                              pretrained_ckpt=pretrained_ckpt,\n                              name=name)\n\n        # init param\n        self.epoch = cfg[\'epoch\']\n        self.bsize = cfg[\'batch_size\']\n        self.save_path = cfg[\'save_path\']\n        self.log_freq = cfg[\'log_freq\']\n        self.bpe = cfg[\'bpe\']\n        self.va_bpe = cfg[\'va_bpe\']\n        self.savers = []\n        self.fronted_cfg = frontend_cfg\n        self.cfg = cfg\n\n\n\n        if cfg[\'fe_opt\'].lower() == \'radam\':\n            self.frontend_optim = RAdam(self.model.frontend.parameters(),\n                                        lr=cfg[\'fe_lr\'])\n        else:\n            # init front end optim\n            self.frontend_optim = getattr(optim, cfg[\'fe_opt\'])(self.model.frontend.parameters(),\n                                                  lr=cfg[\'fe_lr\'])\n        self.fe_scheduler = LR_Scheduler(lr_mode, lr_step=cfg[\'lrdec_step\'], optim_name=""frontend"", base_lr=cfg[\'fe_lr\'],\n                                    num_epochs=self.epoch,\n                                    iters_per_epoch=self.bpe)\n\n        self.savers.append(Saver(self.model.frontend, self.save_path,\n                        max_ckpts=cfg[\'max_ckpts\'],\n                        optimizer=self.frontend_optim, prefix=\'PASE-\'))\n\n        # init workers optim\n        self.cls_optim = {}\n        self.cls_scheduler = {}\n        for worker in self.model.classification_workers:\n            min_opt = cfg[\'min_opt\']\n            min_lr = cfg[\'min_lr\']\n            if min_opt.lower() == \'radam\':\n                self.cls_optim[worker.name] = RAdam(worker.parameters(),\n                                                    lr=min_lr)\n            else:\n                self.cls_optim[worker.name] = getattr(optim, min_opt)(worker.parameters(),\n                                                                      lr=min_lr)\n\n            worker_scheduler = LR_Scheduler(lr_mode, lr_step=cfg[\'lrdec_step\'],optim_name=worker.name, base_lr=min_lr,\n                                            num_epochs=self.epoch,\n                                            iters_per_epoch=self.bpe)\n            self.cls_scheduler[worker.name] = worker_scheduler\n            \n            self.savers.append(Saver(worker, self.save_path, max_ckpts=cfg[\'max_ckpts\'],\n                                     optimizer=self.cls_optim[worker.name],\n                                     prefix=\'M-{}-\'.format(worker.name)))\n\n\n        self.regr_optim = {}\n        self.regr_scheduler = {}\n        for worker in self.model.regression_workers:\n            min_opt = cfg[\'min_opt\']\n            min_lr = cfg[\'min_lr\']\n            # could be a regularizer minion\n            if min_opt.lower() == \'radam\':\n                self.regr_optim[worker.name] = RAdam(worker.parameters(),\n                                                     lr=min_lr)\n            else:\n                self.regr_optim[worker.name] = getattr(optim, min_opt)(worker.parameters(),\n                                                                       lr=min_lr)\n            worker_scheduler = LR_Scheduler(lr_mode, lr_step=cfg[\'lrdec_step\'], optim_name=worker.name, base_lr=min_lr,\n                                            num_epochs=self.epoch,\n                                            iters_per_epoch=self.bpe)\n            self.regr_scheduler[worker.name] = worker_scheduler\n\n            self.savers.append(Saver(worker, self.save_path, max_ckpts=cfg[\'max_ckpts\'],\n                                     optimizer=self.regr_optim[worker.name],\n                                     prefix=\'M-{}-\'.format(worker.name)))\n\n        self.epoch_beg = 0\n\n\n        # init tensorboard writer\n        print(""Use tenoserboard: {}"".format(tensorboard))\n        self.tensorboard = tensorboard and use_tb\n        if tensorboard and use_tb:\n            self.writer = SummaryWriter(self.save_path)\n        else:\n            self.writer = None\n            self.train_losses = {}\n            self.valid_losses = {}\n\n        # init backprop scheduler\n        assert backprop_mode is not None\n        self.backprop = backprop_scheduler(self.model, mode=backprop_mode)\n        self.alphaSG = 1\n\n        if backprop_mode == ""dropout"":\n            print(backprop_mode)\n            print(""droping workers with rate: {}"".format(cfg[\'dropout_rate\']))\n            self.worker_drop_rate = cfg[\'dropout_rate\']\n        else:\n            self.worker_drop_rate = None\n\n        if backprop_mode == ""hyper_volume"":\n            print(""using hyper volume with delta: {}"".format(cfg[\'delta\']))\n            self.delta = cfg[\'delta\']\n        else:\n            self.delta = None\n\n        if backprop_mode == ""softmax"":\n            print(""using softmax with temp: {}"".format(cfg[\'temp\']))\n            self.temp = cfg[\'temp\']\n        else:\n            self.temp = None\n\n        if backprop_mode == ""adaptive"":\n            print(""using adaptive with temp: {}, alpha: {}"".format(cfg[\'temp\'], cfg[\'alpha\']))\n            self.temp = cfg[\'temp\']\n            self.alpha = cfg[\'alpha\']\n        else:\n            # self.temp = None\n            self.alpha = None\n\n        # auto supervise task evaluation\n        if cfg[\'sup_exec\'] is not None:\n            aux_save_path = os.path.join(cfg[\'save_path\'],\n                                             \'sup_aux\')\n            if not os.path.exists(aux_save_path):\n                os.makedirs(aux_save_path)\n            self.aux_sup = AuxiliarSuperviser(cfg[\'sup_exec\'], aux_save_path)\n        self.sup_freq = cfg[\'sup_freq\']\n\n    #@profile\n    def train_(self, dataloader, valid_dataloader, device):\n\n        print(\'=\' * 50)\n        print(\'Beginning training...\')\n        print(\'Batches per epoch: \', self.bpe)\n        print(\'Loss schedule policy: {}\'.format(self.backprop.mode))\n\n        if self.cfg[""ckpt_continue""]:\n            self.resume_training(device)\n\n        else:\n            self.epoch_beg = 0\n\n        for e in range(self.epoch_beg, self.epoch):\n\n            self.model.train()\n\n            iterator = iter(dataloader)\n            \n            with trange(1, self.bpe + 1) as pbar:\n                for bidx in pbar:\n                    pbar.set_description(""Epoch {}/{}"".format(e, self.epoch))\n                    try:\n                        batch = next(iterator)\n                    except StopIteration:\n                        iterator = iter(dataloader)\n                        batch = next(iterator)\n\n                    # inference\n                    h, chunk, preds, labels = self.model.forward(batch, self.alphaSG, device)\n\n                    # backprop using scheduler\n                    losses, self.alphaSG = self.backprop(preds,\n                                                         labels,\n                                                         self.cls_optim,\n                                                         self.regr_optim,\n                                                         self.frontend_optim,\n                                                         device=device,\n                                                         dropout_rate=self.worker_drop_rate,\n                                                         delta=self.delta,\n                                                         temperture=self.temp,\n                                                         alpha=self.alpha,\n                                                         batch = batch)\n\n\n                    if bidx % self.log_freq == 0 or bidx >= self.bpe:\n                        # decrease learning rate\n                        lrs = {}\n                        lrs[""frontend""] = self.fe_scheduler(self.frontend_optim, bidx, e, losses[""total""].item())\n\n                        for name, scheduler in self.cls_scheduler.items():\n                            lrs[name] = scheduler(self.cls_optim[name], bidx, e, losses[name].item())\n\n                        for name, scheduler in self.regr_scheduler.items():\n                            lrs[name] = scheduler(self.regr_optim[name], bidx, e, losses[name].item())\n\n                        for k in losses.keys():\n                            if k not in lrs:\n                                lrs[k] = 0\n\n                        # print out info\n                        self.train_logger(preds, labels, losses, e, bidx, lrs, pbar)\n\n            self._eval(valid_dataloader,\n                       epoch=e,\n                       device=device)\n\n            fe_path = os.path.join(self.save_path,\n                                   \'FE_e{}.ckpt\'.format(e))\n            torch.save(self.model.frontend.state_dict(), fe_path)\n\n            for saver in self.savers:\n                saver.save(saver.prefix[:-1], e * self.bpe + bidx)\n\n            # TODO: sup. aux losses\n            if (e + 1) % self.sup_freq == 0 or \\\n                    (e + 1) >= self.epoch:\n                if hasattr(self, \'aux_sup\'):\n                    self.aux_sup(e, fe_path, self.cfg[\'fe_cfg\'])\n\n\n\n    def _eval(self, dataloader, epoch=0, device=\'cpu\'):\n\n        self.model.eval()\n        with torch.no_grad():\n            print(\'=\' * 50)\n            print(\'Beginning evaluation...\')\n            running_loss = {}\n            iterator = iter(dataloader)\n\n            with trange(1, self.va_bpe + 1) as pbar:\n                for bidx in pbar:\n                    pbar.set_description(""Eval: {}/{}"".format(bidx, self.va_bpe+1))\n                    try:\n                        batch = next(iterator)\n                    except StopIteration:\n                        iterator = iter(dataloader)\n                        batch = next(iterator)\n\n                    # inference\n                    h, chunk, preds, labels = self.model.forward(batch, device=device)\n\n                    # calculate losses\n                    tot_loss = torch.tensor([0.]).to(device)\n                    losses = {}\n                    for worker in self.model.classification_workers:\n                        loss = worker.loss(preds[worker.name], labels[worker.name])\n                        losses[worker.name] = loss\n                        tot_loss += loss\n                        if worker.name not in running_loss:\n                            running_loss[worker.name] = [loss.item()]\n                        else:\n                            running_loss[worker.name].append(loss.item())\n\n                    for worker in self.model.regression_workers:\n                        loss = worker.loss(preds[worker.name], labels[worker.name])\n                        losses[worker.name] = loss\n                        tot_loss += loss\n                        if worker.name not in running_loss:\n                            running_loss[worker.name] = [loss.item()]\n                        else:\n                            running_loss[worker.name].append(loss.item())\n                    if \'total\' not in running_loss:\n                        running_loss[""total""] = [tot_loss.item()]\n                    else:\n                        running_loss[""total""].append(tot_loss.item())\n\n                    if bidx % self.log_freq == 0 or bidx >= self.bpe:\n                        pbar.write(\'-\' * 50)\n                        pbar.write(\'EVAL Batch {}/{} (Epoch {}):\'.format(bidx,\n                                                                    self.va_bpe,\n                                                                    epoch))\n                        for name, loss in losses.items():\n                            pbar.write(\'{} loss: {:.3f}\'\n                                  \'\'.format(name, loss.item()))\n\n            self.eval_logger(running_loss, epoch, pbar)\n\n    def resume_training(self, device):\n        giters = 0\n        for saver in self.savers:\n            # try loading all savers last state if not forbidden is active\n            try:\n                state = saver.read_latest_checkpoint()\n                giter_ = saver.load_ckpt_step(state)\n                print(\'giter_ found: \', giter_)\n                # assert all ckpts happened at last same step\n                if giters == 0:\n                    giters = giter_\n                else:\n                    assert giters == giter_, giter_\n                saver.load_pretrained_ckpt(os.path.join(self.save_path,\n                                                        \'weights_\' + state),\n                                           load_last=True)\n            except TypeError:\n                break\n\n            global_step = giters\n            # redefine num epochs depending on where we left it\n            self.epoch_beg = int(global_step / self.bpe)\n\n        # self.load_checkpoints(self.save_path)\n        self.model.to(device)\n\n    def load_checkpoints(self, load_path):\n\n        # now load each ckpt found\n        giters = 0\n        for saver in self.savers:\n            # try loading all savers last state if not forbidden is active\n            try:\n                state = saver.read_latest_checkpoint()\n                giter_ = saver.load_ckpt_step(state)\n                print(\'giter_ found: \', giter_)\n                # assert all ckpts happened at last same step\n                if giters == 0:\n                    giters = giter_\n                else:\n                    assert giters == giter_, giter_\n                saver.load_pretrained_ckpt(os.path.join(load_path,\n                                                        \'weights_\' + state), \n                                           load_last=True)\n            except TypeError:\n                break\n\n\n    def train_logger(self, preds, labels, losses, epoch, bidx, lrs, pbar):\n        step = epoch * self.bpe + bidx\n        pbar.write(""="" * 50)\n        pbar.write(\'Batch {}/{} (Epoch {}) step: {}:\'.format(bidx, self.bpe, epoch, step))\n\n        for name, loss in losses.items():\n            if name == ""total"":\n                pbar.write(\'%s, learning rate = %.8f, loss = %.4f\' % (""total"", lrs[\'frontend\'], loss))\n            else:\n                pbar.write(\'%s, learning rate = %.8f, loss = %.4f\' % (name, lrs[name], loss))\n\n            if self.writer:\n\n                self.writer.add_scalar(\'train/{}_loss\'.format(name),\n                                  loss.item(),\n                                  global_step=step)\n\n                if name != ""total"":\n                    self.writer.add_histogram(\'train/{}\'.format(name),\n                                         preds[name].data,\n                                         bins=\'sturges\',\n                                         global_step=step)\n\n                    self.writer.add_histogram(\'train/gtruth_{}\'.format(name),\n                                         labels[name].data,\n                                         bins=\'sturges\',\n                                         global_step=step)\n\n        grads = get_grad_norms(self.model)\n        for kgrad, vgrad in grads.items():\n            writer.add_scalar(\'train/GRAD/{}\'.format(kgrad),\n                              vgrad, global_step)\n\n        if not self.tensorboard:\n\n            for name, _ in preds.items():\n                    preds[name] = preds[name].data\n                    labels[name] = labels[name].data\n\n            self.train_losses[\'itr\'] = step\n            self.train_losses[\'losses\'] = losses\n            self.train_losses[\'dist\'] = preds\n            self.train_losses[\'dist_gt\'] = labels\n\n            with open(os.path.join(self.save_path, \'train_losses.pkl\'), ""wb"") as f:\n                pbar.write(""saved log to {}"".format(os.path.join(self.save_path, \'train_losses.pkl\')))\n                pickle.dump(self.train_losses, f, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def eval_logger(self, running_loss, epoch, pbar):\n        pbar.write(""="" * 50)\n        if self.writer:\n            for name, loss in running_loss.items():\n                loss = np.mean(loss)\n                pbar.write(""avg loss {}: {}"".format(name, loss))\n\n                self.writer.add_scalar(\'eval/{}_loss\'.format(name),\n                                        loss,\n                                        global_step=epoch)\n        else:\n            self.valid_losses[\'epoch\'] = epoch\n            self.valid_losses[\'losses\'] = running_loss\n\n            with open(os.path.join(self.save_path, \'valid_losses.pkl\'), ""wb"") as f:\n                pbar.write(""saved log to {}"".format(os.path.join(self.save_path, \'valid_losses.pkl\')))\n                pickle.dump(self.valid_losses, f, protocol=pickle.HIGHEST_PROTOCOL)\n'"
pase/models/WorkerScheduler/worker_scheduler.py,19,"b'import torch\nimport random\nimport numpy as np\nimport torch.nn.functional as F\nfrom .min_norm_solvers import MinNormSolver, gradient_normalizers\nfrom torch.autograd import Variable\n\n\nclass backprop_scheduler(object):\n\n    def __init__(self, model, mode=None):\n        self.model = model\n        self.mode = mode\n        self.num_worker = len(self.model.regression_workers) + len(self.model.classification_workers)\n        self.Q = torch.zeros(self.num_worker).detach()\n        self.last_loss = torch.zeros(self.num_worker).detach()\n        self.pi = torch.ones(self.num_worker).detach()\n\n\n    def __call__(self, preds, label, cls_optim, regr_optim, frontend_optim, device, h=None, dropout_rate=None, delta=None, temperture=None, alpha=None, batch=None):\n\n        if self.mode == ""base"":\n            return self._base_scheduler(preds, label, cls_optim, regr_optim, frontend_optim, device)\n        elif self.mode == ""adversarial"":\n            return self._adversarial(preds, label, cls_optim, regr_optim, frontend_optim, device)\n        elif self.mode == ""select_one"":\n            return self._select_one(preds, label, cls_optim, regr_optim, frontend_optim, device)\n        elif self.mode == ""select_half"":\n            return self._select_half(preds, label, cls_optim, regr_optim, frontend_optim, device)\n        elif self.mode == ""dropout"":\n            return self._drop_out(preds, label, cls_optim, regr_optim, frontend_optim, device=device, dropout_rate=dropout_rate)\n        elif self.mode == ""hyper_volume"":\n            return self._hyper_volume(preds, label, cls_optim, regr_optim, frontend_optim, device=device, delta=delta)\n        elif self.mode == ""softmax"":\n            return self._softmax(preds, label, cls_optim, regr_optim, frontend_optim, temperture=temperture, device=device)\n        elif self.mode == ""adaptive"":\n            return self._online_adaptive(preds, label, cls_optim, regr_optim, frontend_optim, temperture=temperture, alpha=alpha, device=device)\n        elif self.mode == ""MGD"":\n            return self._MGDA(preds, label, cls_optim, regr_optim, frontend_optim, batch=batch, device=device)\n        else:\n            raise NotImplementedError\n\n    def _base_scheduler(self, preds, label, cls_optim, regr_optim, frontend_optim, device):\n        frontend_optim.zero_grad()\n        tot_loss = 0\n        losses = {}\n        for worker in self.model.classification_workers:\n            cls_optim[worker.name].zero_grad()\n            loss = worker.loss_weight * worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            tot_loss += loss\n\n        for worker in self.model.regression_workers:\n            regr_optim[worker.name].zero_grad()\n            loss = worker.loss_weight * worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            tot_loss += loss\n\n        for worker in self.model.regularizer_workers:\n            loss = worker.loss_weight * worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            tot_loss += loss\n\n        tot_loss.backward()\n\n        for _, optim in cls_optim.items():\n            optim.step()\n\n        for _, optim in regr_optim.items():\n            optim.step()\n\n        frontend_optim.step()\n        losses[""total""] = tot_loss\n\n        return losses, 1\n\n\n    def _select_one(self, preds, label, cls_optim, regr_optim, frontend_optim, device):\n        self.count += 1\n        loss_lst = []\n        num_worker = len(self.model.regression_workers) + len(self.model.classification_workers)\n\n        frontend_optim.zero_grad()\n        losses = {}\n\n        selected = self.count % num_worker\n\n        # select one\n        if selected > 3:\n            worker = self.model.classification_workers[selected - 4]\n            loss = worker.loss(preds[worker.name], label[worker.name])\n        else:\n            worker = self.model.classification_workers[selected]\n            loss = worker.loss(preds[worker.name], label[worker.name])\n\n        tot_loss = loss\n\n        tot_loss.backward()\n\n        for _, optim in cls_optim.items():\n            optim.step()\n\n        for _, optim in regr_optim.items():\n            optim.step()\n\n        frontend_optim.step()\n        losses[""total""] = tot_loss\n\n        return losses, 1\n\n    def _select_half(self, preds, label, cls_optim, regr_optim, frontend_optim, device):\n        num_worker = len(self.model.regression_workers) + len(self.model.classification_workers)\n        loss_tmp = torch.zeros(num_worker).to(device)\n        idx = 0\n\n        frontend_optim.zero_grad()\n        losses = {}\n        for worker in self.model.classification_workers:\n            cls_optim[worker.name].zero_grad()\n            loss = worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            loss_tmp[idx] = loss\n            idx += 1\n\n        for worker in self.model.regression_workers:\n            regr_optim[worker.name].zero_grad()\n            loss = worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            loss_tmp[idx] = loss\n            idx += 1\n\n        # generate mask\n        mask = np.random.randint(2, size=num_worker)\n        while np.sum(mask) > 4 or np.sum(mask) < 3:\n            mask = np.random.randint(2, size=num_worker)\n        mask = torch.from_numpy(mask).type(torch.FloatTensor).to(device)\n\n        #sum up losses\n        tot_loss = torch.sum(mask * loss_tmp, dim=0)\n\n        tot_loss.backward()\n\n        for _, optim in cls_optim.items():\n            optim.step()\n\n        for _, optim in regr_optim.items():\n            optim.step()\n\n        frontend_optim.step()\n        losses[""total""] = tot_loss\n\n        return losses, 1\n\n    def _drop_out(self, preds, label, cls_optim, regr_optim, frontend_optim, dropout_rate, device):\n        loss_tmp = torch.zeros(7, requires_grad=True).to(device)\n        idx = 0\n\n        assert dropout_rate is not None\n        re_mask = np.random.binomial(1, dropout_rate, size=len(self.model.regression_workers))\n        cls_mask = np.random.binomial(1, dropout_rate, size=len(self.model.classification_workers))\n\n\n        frontend_optim.zero_grad()\n        losses = {}\n        for i, worker in enumerate(self.model.classification_workers):\n            cls_optim[worker.name].zero_grad()\n\n            if cls_mask[i] == 1:\n                loss = worker.loss(preds[worker.name], label[worker.name])\n            else:\n                loss = 0\n\n            losses[worker.name] = loss\n            loss_tmp[idx] = loss\n            idx += 1\n\n        for worker in self.model.regression_workers:\n            regr_optim[worker.name].zero_grad()\n\n            if re_mask[i] == 1:\n                loss = worker.loss(preds[worker.name], label[worker.name])\n            else:\n                loss = 0\n            losses[worker.name] = loss\n            loss_tmp[idx] = loss\n            idx += 1\n\n\n        #sum up losses\n        tot_loss = torch.sum(loss_tmp, dim=0)\n\n        tot_loss.backward()\n\n        for _, optim in cls_optim.items():\n            optim.step()\n\n        for _, optim in regr_optim.items():\n            optim.step()\n\n        frontend_optim.step()\n        losses[""total""] = tot_loss\n\n        return losses, 1\n\n    def _hyper_volume(self, preds, label, cls_optim, regr_optim, frontend_optim, delta ,device):\n        assert delta > 1\n\n        num_worker = len(self.model.regression_workers) + len(self.model.classification_workers)\n        loss_tmp = torch.zeros(num_worker).to(device)\n        idx = 0\n\n        frontend_optim.zero_grad()\n        losses = {}\n        for worker in self.model.classification_workers:\n            cls_optim[worker.name].zero_grad()\n            loss = worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            loss_tmp[idx] = loss\n            idx += 1\n\n        for worker in self.model.regression_workers:\n            regr_optim[worker.name].zero_grad()\n            loss = worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            loss_tmp[idx] = loss\n            idx += 1\n\n\n        #sum up losses\n        eta = delta * torch.max(loss_tmp.detach()).item()\n        hyper_votolume = torch.sum(loss_tmp)\n\n        alpha = 1 / (eta - loss_tmp + 1e-6)\n\n        hyper_votolume.backward()\n\n        for _, optim in cls_optim.items():\n            optim.step()\n\n        for _, optim in regr_optim.items():\n            optim.step()\n\n        frontend_optim.step()\n        losses[""total""] = hyper_votolume\n\n        return losses, alpha\n\n    def _softmax(self, preds, label, cls_optim, regr_optim, frontend_optim, temperture, device):\n        assert temperture > 0\n\n        num_worker = len(self.model.regression_workers) + len(self.model.classification_workers)\n        loss_tmp = []\n        idx = 0\n\n        frontend_optim.zero_grad()\n        losses = {}\n        for worker in self.model.classification_workers:\n            cls_optim[worker.name].zero_grad()\n            loss = worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            loss_tmp.append(loss.item() * temperture)\n            # idx += 1\n\n        for worker in self.model.regression_workers:\n            regr_optim[worker.name].zero_grad()\n            loss = worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            loss_tmp.append(loss.item() * temperture)\n            # idx += 1\n\n\n        alpha = self._stable_softmax(loss_tmp)\n\n        tot_loss = 0\n        for worker in self.model.classification_workers:\n            # tot_loss += alpha[idx] * losses[worker.name]\n            tot_loss += losses[worker.name]\n            idx += 1\n        for worker in self.model.regression_workers:\n            # tot_loss += alpha[idx] * losses[worker.name]\n            tot_loss += losses[worker.name]\n            idx += 1\n\n        # tot_loss = torch.sum(alpha.detach() * loss_vec)\n        tot_loss.backward()\n\n\n        for _, optim in cls_optim.items():\n            optim.step()\n\n        for _, optim in regr_optim.items():\n            optim.step()\n\n        frontend_optim.step()\n        losses[""total""] = tot_loss\n\n        return losses, alpha\n\n    def _online_adaptive(self, preds, label, cls_optim, regr_optim, frontend_optim, temperture, alpha, device):\n\n        assert temperture > 0 and alpha > 0\n        # device = preds[\'chunk\'].device\n        num_worker = len(self.model.regression_workers) + len(self.model.classification_workers)\n        loss_tmp = torch.zeros(num_worker).to(device)\n        idx = 0\n\n        frontend_optim.zero_grad()\n        losses = {}\n        for worker in self.model.classification_workers:\n            cls_optim[worker.name].zero_grad()\n            loss = worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            loss_tmp[idx] = loss\n            idx += 1\n\n        for worker in self.model.regression_workers:\n            regr_optim[worker.name].zero_grad()\n            loss = worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            loss_tmp[idx] = loss\n            idx += 1\n\n        R_t = self.last_loss.to(device) - loss_tmp\n\n        with torch.no_grad():\n            Q_t = alpha * R_t.detach() + (1 - alpha) * self.Q.to(device)\n\n            self.pi = F.softmax(temperture * Q_t, dim=0)\n\n\n        tot_loss = torch.sum(loss_tmp)\n        tot_loss.backward()\n\n        self.last_loss = loss_tmp.detach()\n        self.Q = Q_t.detach()\n\n        for _, optim in cls_optim.items():\n            optim.step()\n\n        for _, optim in regr_optim.items():\n            optim.step()\n\n        frontend_optim.step()\n        losses[""total""] = tot_loss\n\n        return losses, self.pi\n\n    def _MGDA(self, preds, label, cls_optim, regr_optim, frontend_optim, batch, device):\n        frontend_optim.zero_grad()\n        losses = {}\n\n        grads = {}\n        for worker in self.model.classification_workers:\n            self.model.zero_grad()\n            h, chunk, preds, labels = self.model.forward(batch, 1, device)\n            # print(worker.name)\n            loss = worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            grads[worker.name] = self._get_gen_grads(loss)\n\n        for worker in self.model.regression_workers:\n            self.model.zero_grad()\n            h, chunk, preds, labels = self.model.forward(batch, 1, device)\n            # print(worker.name)\n            loss = worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            grads[worker.name] = self._get_gen_grads(loss)\n\n\n\n        sol, min_norm = MinNormSolver.find_min_norm_element([grads[worker].unsqueeze(0) for worker, _ in grads.items()])\n        alpha = sol\n\n        tot_loss = 0\n        # idx = 0\n\n        self.model.zero_grad()\n        h, chunk, preds, labels = self.model.forward(batch, 1, device)\n        for worker in self.model.classification_workers:\n            loss = worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            tot_loss += loss\n            # tot_loss += sol[idx] * loss\n\n\n        for worker in self.model.regression_workers:\n            loss = worker.loss(preds[worker.name], label[worker.name])\n            losses[worker.name] = loss\n            tot_loss += loss\n            # tot_loss += sol[idx] * loss\n\n\n\n\n        tot_loss.backward()\n\n\n        for _, optim in cls_optim.items():\n            optim.step()\n\n        for _, optim in regr_optim.items():\n            optim.step()\n\n        frontend_optim.step()\n        losses[""total""] = tot_loss\n\n        return losses, alpha\n\n    def _get_gen_grads(self, loss_):\n        # grads = torch.autograd.grad(outputs=loss_, inputs=self.model.frontend.parameters())\n        self.model.frontend.zero_grad()\n        loss_.backward()\n        # grads = self.model.frontend.grad()\n        for params in self.model.frontend.parameters():\n\n            try:\n                grads_ = torch.cat([grads_, params.grad.view(-1)], 0)\n            except:\n                grads_ = params.grad.view(-1)\n\n        return grads_ / grads_.norm()\n\n    def _stable_softmax(self, x):\n        z = np.asarray(x, np.float) - np.max(x)\n        numerator = np.exp(z)\n        denominator = np.sum(numerator)\n        softmax = numerator / denominator\n\n        return softmax\n\n\n\n\n\n'"
ASR/kaldi_decoding_scripts/utils/nnet/gen_dct_mat.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2012  Brno University of Technology (author: Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# ./gen_dct_mat.py\n# script generates matrix with DCT transform, which is sparse \n# and takes into account that data-layout is along frequency axis, \n# while DCT is done along temporal axis.\n\nfrom math import *\nimport sys\n\n\nfrom optparse import OptionParser\n\nparser = OptionParser()\nparser.add_option(\'--fea-dim\', dest=\'dim\', help=\'feature dimension\')\nparser.add_option(\'--splice\', dest=\'splice\', help=\'applied splice value\')\nparser.add_option(\'--dct-basis\', dest=\'dct_basis\', help=\'number of DCT basis\')\n(options, args) = parser.parse_args()\n\nif(options.dim == None):\n    parser.print_help()\n    sys.exit(1)\n\ndim=int(options.dim)\nsplice=int(options.splice)\ndct_basis=int(options.dct_basis)\n\ntimeContext=2*splice+1\n\n\n#generate the DCT matrix\nM_PI = 3.1415926535897932384626433832795\nM_SQRT2 = 1.4142135623730950488016887\n\n\n#generate sparse DCT matrix\nprint \'[\'\nfor k in range(dct_basis):\n    for m in range(dim):\n        for n in range(timeContext):\n          if(n==0): \n              print m*\'0 \',\n          else: \n              print (dim-1)*\'0 \',\n          print str(sqrt(2.0/timeContext)*cos(M_PI/timeContext*k*(n+0.5))),\n          if(n==timeContext-1):\n              print (dim-m-1)*\'0 \',\n        print\n    print \n\nprint \']\'\n\n'"
ASR/kaldi_decoding_scripts/utils/nnet/gen_hamm_mat.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2012  Brno University of Technology (author: Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# ./gen_hamm_mat.py\n# script generates diagonal matrix with hamming window values\n\nfrom math import *\nimport sys\n\n\nfrom optparse import OptionParser\n\nparser = OptionParser()\nparser.add_option(\'--fea-dim\', dest=\'dim\', help=\'feature dimension\')\nparser.add_option(\'--splice\', dest=\'splice\', help=\'applied splice value\')\n(options, args) = parser.parse_args()\n\nif(options.dim == None):\n    parser.print_help()\n    sys.exit(1)\n\ndim=int(options.dim)\nsplice=int(options.splice)\n\n\n#generate the diagonal matrix with hammings\nM_2PI = 6.283185307179586476925286766559005\n\ndim_mat=(2*splice+1)*dim\ntimeContext=2*splice+1\nprint \'[\'\nfor row in range(dim_mat):\n    for col in range(dim_mat):\n        if col!=row:\n            print \'0\',\n        else:\n            i=int(row/dim)\n            print str(0.54 - 0.46*cos((M_2PI * i) / (timeContext-1))),\n    print\n\nprint \']\'\n\n\n'"
ASR/kaldi_decoding_scripts/utils/nnet/gen_splice.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2012  Brno University of Technology (author: Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# ./gen_splice.py\n# generates <splice> Component\n\nfrom math import *\nimport sys\n\n\nfrom optparse import OptionParser\n\nparser = OptionParser()\nparser.add_option(\'--fea-dim\', dest=\'dim_in\', help=\'feature dimension\')\nparser.add_option(\'--splice\', dest=\'splice\', help=\'number of frames to concatenate with the central frame\')\nparser.add_option(\'--splice-step\', dest=\'splice_step\', help=\'splicing step (frames dont need to be consecutive, --splice 3 --splice-step 2 will select offsets: -6 -4 -2 0 2 4 6)\', default=\'1\' )\n(options, args) = parser.parse_args()\n\nif(options.dim_in == None):\n    parser.print_help()\n    sys.exit(1)\n\ndim_in=int(options.dim_in)\nsplice=int(options.splice)\nsplice_step=int(options.splice_step)\n\ndim_out=(2*splice+1)*dim_in\n\nprint \'<splice>\', dim_out, dim_in\nprint \'[\',\n\nsplice_vec = range(-splice*splice_step, splice*splice_step+1, splice_step)\nfor idx in range(len(splice_vec)):\n    print splice_vec[idx],\n\nprint \']\'\n\n'"
ASR/kaldi_decoding_scripts/utils/nnet/make_blstm_proto.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2015  Brno University of Technology (author: Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# Generated Nnet prototype, to be initialized by \'nnet-initialize\'.\n\nimport sys\n\n###\n### Parse options\n###\nfrom optparse import OptionParser\nusage=""%prog [options] <feat-dim> <num-leaves> >nnet-proto-file""\nparser = OptionParser(usage)\n#\nparser.add_option(\'--num-cells\', dest=\'num_cells\', type=\'int\', default=800, \n                   help=\'Number of LSTM cells [default: %default]\');\nparser.add_option(\'--num-recurrent\', dest=\'num_recurrent\', type=\'int\', default=512, \n                   help=\'Number of LSTM recurrent units [default: %default]\');\nparser.add_option(\'--num-layers\', dest=\'num_layers\', type=\'int\', default=2, \n                   help=\'Number of LSTM layers [default: %default]\');\nparser.add_option(\'--lstm-stddev-factor\', dest=\'lstm_stddev_factor\', type=\'float\', default=0.01, \n                   help=\'Standard deviation of initialization [default: %default]\');\nparser.add_option(\'--param-stddev-factor\', dest=\'param_stddev_factor\', type=\'float\', default=0.04, \n                   help=\'Standard deviation in output layer [default: %default]\');\nparser.add_option(\'--clip-gradient\', dest=\'clip_gradient\', type=\'float\', default=5.0, \n                   help=\'Clipping constant applied to gradients [default: %default]\');\n#\n(o,args) = parser.parse_args()\nif len(args) != 2 : \n  parser.print_help()\n  sys.exit(1)\n\n(feat_dim, num_leaves) = map(int,args);\n\n# Original prototype from Jiayu,\n#<NnetProto>\n#<Transmit> <InputDim> 40 <OutputDim> 40\n#<LstmProjectedStreams> <InputDim> 40 <OutputDim> 512 <CellDim> 800 <ParamScale> 0.01 <NumStream> 4\n#<AffineTransform> <InputDim> 512 <OutputDim> 8000 <BiasMean> 0.000000 <BiasRange> 0.000000 <ParamStddev> 0.04\n#<Softmax> <InputDim> 8000 <OutputDim> 8000\n#</NnetProto>\n\nprint ""<NnetProto>""\n# normally we won\'t use more than 2 layers of LSTM\nif o.num_layers == 1:\n    print ""<BLstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f"" % \\\n        (feat_dim, 2*o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)\nelif o.num_layers == 2:\n    print ""<BLstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f"" % \\\n        (feat_dim, 2*o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)\n    print ""<BLstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f"" % \\\n        (2*o.num_recurrent, 2*o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)\nelse:\n    sys.stderr.write(""make_lstm_proto.py ERROR: more than 2 layers of LSTM, not supported yet.\\n"")\n    sys.exit(1)\nprint ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> 0.0 <BiasRange> 0.0 <ParamStddev> %f"" % \\\n    (2*o.num_recurrent, num_leaves, o.param_stddev_factor)\nprint ""<Softmax> <InputDim> %d <OutputDim> %d"" % \\\n    (num_leaves, num_leaves)\nprint ""</NnetProto>""\n\n\n'"
ASR/kaldi_decoding_scripts/utils/nnet/make_cnn2d_proto.py,0,"b'#!/usr/bin/python\n\n# Copyright 2014  Brno University of Technology (author: Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# Generated Nnet prototype, to be initialized by \'nnet-initialize\'.\n\nimport math, random, sys, warnings\nfrom optparse import OptionParser\n\n###\n### Parse options\n###\nusage=""%prog [options] <feat-dim> <num-leaves> <num-hidden-layers> <num-hidden-neurons>  >nnet-proto-file""\nparser = OptionParser(usage)\n\nparser.add_option(\'--activation-type\', dest=\'activation_type\', \n                   help=\'Select type of activation function : (<Sigmoid>|<Tanh>) [default: %default]\', \n                   default=\'<Sigmoid>\', type=\'string\');\n\nparser.add_option(\'--cnn1-num-filters\', dest=\'cnn1_num_filters\',\n\t\t   help=\'Number of filters in first convolutional layer [default: %default]\',\n\t\t   default=128, type=\'int\')\n# this is given by splice\n# parser.add_option(\'--cnn1-fmap-x-len\', dest=\'cnn1_fmap_x_len\',\n# \t  \t   help=\'Size of cnn1-fmap-x-len [default: %default]\',\n# \t\t   default=11, type=\'int\')\n\n# this should be equal to feat_raw_dim\n# parser.add_option(\'--cnn1-fmap-y-len\', dest=\'cnn1_fmap_y_len\',\n# \t  \t   help=\'Size of cnn1-fmap-y-len [default: %default]\',\n# \t\t   default=32, type=\'int\')\n\nparser.add_option(\'--cnn1-filt-x-len\', dest=\'cnn1_filt_x_len\',\n\t  \t   help=\'Size of cnn1-filt-x-len [default: %default]\',\n\t\t   default=9, type=\'int\')\nparser.add_option(\'--cnn1-filt-y-len\', dest=\'cnn1_filt_y_len\',\n\t  \t   help=\'Size of cnn1-filt-y-len [default: %default]\',\n\t\t   default=9, type=\'int\')\n\nparser.add_option(\'--cnn1-filt-x-step\', dest=\'cnn1_filt_x_step\',\n\t  \t   help=\'Size of cnn1-filt-x-step [default: %default]\',\n\t\t   default=1, type=\'int\')\nparser.add_option(\'--cnn1-filt-y-step\', dest=\'cnn1_filt_y_step\',\n\t  \t   help=\'Size of cnn1-filt-y-step [default: %default]\',\n\t\t   default=1, type=\'int\')\nparser.add_option(\'--cnn1-connect-fmap\', dest=\'cnn1_connect_fmap\',\n\t  \t   help=\'Size of cnn1-connect-fmap [default: %default]\',\n\t\t   default=0, type=\'int\')\n\nparser.add_option(\'--pool1-x-len\', dest=\'pool1_x_len\',\n\t  \t   help=\'Size of pool1-filt-x-len [default: %default]\',\n\t\t   default=1, type=\'int\')\nparser.add_option(\'--pool1-x-step\', dest=\'pool1_x_step\',\n\t  \t   help=\'Size of pool1-x-step [default: %default]\',\n\t\t   default=1, type=\'int\')\n\n\n# \nparser.add_option(\'--pool1-y-len\', dest=\'pool1_y_len\',\n\t  \t   help=\'Size of pool1-y-len [default: %default]\',\n\t\t   default=3, type=\'int\')\nparser.add_option(\'--pool1-y-step\', dest=\'pool1_y_step\',\n\t  \t   help=\'Size of pool1-y-step [default: %default]\',\n\t\t   default=3, type=\'int\')\n\nparser.add_option(\'--pool1-type\', dest=\'pool1_type\',\n\t\t  help=\'Type of pooling (Max || Average) [default: %default]\',\n\t\t  default=\'Max\', type=\'string\')\n\nparser.add_option(\'--cnn2-num-filters\', dest=\'cnn2_num_filters\',\n\t\t   help=\'Number of filters in first convolutional layer [default: %default]\',\n\t\t   default=256, type=\'int\')\nparser.add_option(\'--cnn2-filt-x-len\', dest=\'cnn2_filt_x_len\',\n\t  \t   help=\'Size of cnn2-filt-x-len [default: %default]\',\n\t\t   default=3, type=\'int\')\nparser.add_option(\'--cnn2-filt-y-len\', dest=\'cnn2_filt_y_len\',\n\t  \t   help=\'Size of cnn2-filt-y-len [default: %default]\',\n\t\t   default=4, type=\'int\')\nparser.add_option(\'--cnn2-filt-x-step\', dest=\'cnn2_filt_x_step\',\n\t  \t   help=\'Size of cnn2-filt-x-step [default: %default]\',\n\t\t   default=1, type=\'int\')\nparser.add_option(\'--cnn2-filt-y-step\', dest=\'cnn2_filt_y_step\',\n\t  \t   help=\'Size of cnn2-filt-y-step [default: %default]\',\n\t\t   default=1, type=\'int\')\nparser.add_option(\'--cnn2-connect-fmap\', dest=\'cnn2_connect_fmap\',\n\t  \t   help=\'Size of cnn2-connect-fmap [default: %default]\',\n\t\t   default=1, type=\'int\')\n\nparser.add_option(\'--pitch-dim\', dest=\'pitch_dim\',\n\t\t  help=\'Number of features representing pitch [default: %default]\',\n\t\t  default=0, type=\'int\')\nparser.add_option(\'--delta-order\', dest=\'delta_order\',\n\t\t  help=\'Order of delta features [default: %default]\',\n\t\t  default=2, type=\'int\')\nparser.add_option(\'--splice\', dest=\'splice\',\n\t\t  help=\'Length of splice [default: %default]\',\n\t\t  default=5,type=\'int\')\nparser.add_option(\'--dir\', dest=\'dirct\',\n\t\t  help=\'Directory, where network prototypes will be saved [default: %default]\',\n\t\t  default=\'.\', type=\'string\')\nparser.add_option(\'--num-pitch-neurons\', dest=\'num_pitch_neurons\',\n\t\t  help=\'Number of neurons in layers processing pitch features [default: %default]\',\n\t\t  default=\'200\', type=\'int\')\n\n\n(o,args) = parser.parse_args()\nif len(args) != 1 : \n  parser.print_help()\n  sys.exit(1)\n  \nfeat_dim=int(args[0])\n### End parse options \n\nfeat_raw_dim = feat_dim / (o.delta_order+1) / (o.splice*2+1) - o.pitch_dim # we need number of feats without deltas and splice and pitch\no.cnn1_fmap_y_len = feat_raw_dim\no.cnn1_fmap_x_len = o.splice*2+1\n\n# Check\nassert(feat_dim > 0)\nassert(o.pool1_type == \'Max\' or o.pool1_type == \'Average\')\n\n## Extra checks if dimensions are matching, if not match them by \n## producing a warning\n# cnn1\nassert( (o.cnn1_fmap_y_len - o.cnn1_filt_y_len) % o.cnn1_filt_y_step == 0 )\nassert( (o.cnn1_fmap_x_len - o.cnn1_filt_x_len) % o.cnn1_filt_x_step == 0 )\n\n# subsample1\ncnn1_out_fmap_y_len=((1 + (o.cnn1_fmap_y_len - o.cnn1_filt_y_len) / o.cnn1_filt_y_step))\ncnn1_out_fmap_x_len=((1 + (o.cnn1_fmap_x_len - o.cnn1_filt_x_len) / o.cnn1_filt_x_step))\n\n# fix filt_len and filt_step\ndef fix_filt_step(inp_len, filt_len, filt_step):\n  \n  if ((inp_len - filt_len) % filt_step == 0):\n    return filt_step\n  else:\n    # filt_step <= filt_len\n    for filt_step in xrange(filt_len, 0, -1):\n      if ((inp_len - filt_len) % filt_step == 0):\n        return filt_step\n    \no.pool1_y_step = fix_filt_step(cnn1_out_fmap_y_len, o.pool1_y_len, o.pool1_y_step)\nif o.pool1_y_step == 1 and o.pool1_y_len != 1:\n  warnings.warn(\'WARNING: Choose different pool1_y_len as subsampling is not happening\');\n  \no.pool1_x_step = fix_filt_step(cnn1_out_fmap_x_len, o.pool1_x_len, o.pool1_x_step)\nif o.pool1_x_step == 1 and o.pool1_x_len != 1:\n  warnings.warn(\'WARNING: Choose different pool1_x_len as subsampling is not happening\');\n\n\n###\n### Print prototype of the network\n###\n\n# Begin the prototype\nprint ""<NnetProto>""\n\n# Convolutional part of network\n\'\'\'1st CNN layer\'\'\'\ncnn1_input_dim=feat_raw_dim * (o.delta_order+1) * (o.splice*2+1)\ncnn1_out_fmap_x_len=((1 + (o.cnn1_fmap_x_len - o.cnn1_filt_x_len) / o.cnn1_filt_x_step))\ncnn1_out_fmap_y_len=((1 + (o.cnn1_fmap_y_len - o.cnn1_filt_y_len) / o.cnn1_filt_y_step))\ncnn1_output_dim=o.cnn1_num_filters * cnn1_out_fmap_x_len * cnn1_out_fmap_y_len\n\n\'\'\'1st Pooling layer\'\'\'\npool1_input_dim=cnn1_output_dim\npool1_fmap_x_len=cnn1_out_fmap_x_len\npool1_out_fmap_x_len=((1 + (pool1_fmap_x_len - o.pool1_x_len) / o.pool1_x_step))\npool1_fmap_y_len=cnn1_out_fmap_y_len\npool1_out_fmap_y_len=((1 + (pool1_fmap_y_len - o.pool1_y_len) / o.pool1_y_step))\npool1_output_dim=o.cnn1_num_filters*pool1_out_fmap_x_len*pool1_out_fmap_y_len\n\n\'\'\'2nd CNN layer\'\'\'\ncnn2_input_dim=pool1_output_dim\ncnn2_fmap_x_len=pool1_out_fmap_x_len\ncnn2_out_fmap_x_len=((1 + (cnn2_fmap_x_len - o.cnn2_filt_x_len) / o.cnn2_filt_x_step))\ncnn2_fmap_y_len=pool1_out_fmap_y_len\ncnn2_out_fmap_y_len=((1 + (cnn2_fmap_y_len - o.cnn2_filt_y_len) / o.cnn2_filt_y_step))\ncnn2_output_dim=o.cnn2_num_filters * cnn2_out_fmap_x_len * cnn2_out_fmap_y_len\n\n\nconvolution_proto = \'\'\n\nconvolution_proto += ""<Convolutional2DComponent> <InputDim> %d <OutputDim> %d <FmapXLen> %d <FmapYLen> %d <FiltXLen> %d <FiltYLen> %d <FiltXStep> %d <FiltYStep> %d <ConnectFmap> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f\\n"" % \\\n    ( cnn1_input_dim, cnn1_output_dim, o.cnn1_fmap_x_len, o.cnn1_fmap_y_len, o.cnn1_filt_x_len, o.cnn1_filt_y_len, o.cnn1_filt_x_step, o.cnn1_filt_y_step, o.cnn1_connect_fmap, 0.0, 0.0, 0.01 )\nconvolution_proto += ""<%sPooling2DComponent> <InputDim> %d <OutputDim> %d <FmapXLen> %d <FmapYLen> %d <PoolXLen> %d <PoolYLen> %d <PoolXStep> %d <PoolYStep> %d\\n"" % \\\n    ( o.pool1_type, pool1_input_dim, pool1_output_dim, pool1_fmap_x_len, pool1_fmap_y_len, o.pool1_x_len, o.pool1_y_len, o.pool1_x_step, o.pool1_y_step )\nconvolution_proto += ""<Rescale> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % \\\n    ( pool1_output_dim, pool1_output_dim, 1.0 )\nconvolution_proto += ""<AddShift> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % \\\n    ( pool1_output_dim, pool1_output_dim, 0.0 )\nconvolution_proto += ""%s <InputDim> %d <OutputDim> %d\\n"" % \\\n    ( o.activation_type, pool1_output_dim, pool1_output_dim )\nconvolution_proto += ""<Convolutional2DComponent> <InputDim> %d <OutputDim> %d <FmapXLen> %d <FmapYLen> %d <FiltXLen> %d <FiltYLen> %d <FiltXStep> %d <FiltYStep> %d <ConnectFmap> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f\\n"" % \\\n    ( cnn2_input_dim, cnn2_output_dim, cnn2_fmap_x_len, cnn2_fmap_y_len, o.cnn2_filt_x_len, o.cnn2_filt_y_len, o.cnn2_filt_x_step, o.cnn2_filt_y_step, o.cnn2_connect_fmap, -2.0, 4.0, 0.1 )\nconvolution_proto += ""<Rescale> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % \\\n    ( cnn2_output_dim, cnn2_output_dim, 1.0)\nconvolution_proto += ""<AddShift> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % \\\n    ( cnn2_output_dim, cnn2_output_dim, 0.0)\nconvolution_proto += ""%s <InputDim> %d <OutputDim> %d\\n"" % \\\n    ( o.activation_type, cnn2_output_dim, cnn2_output_dim)\n\nif (o.pitch_dim > 0):\n  # convolutional part\n  f_conv = open(\'%s/nnet.proto.convolution\' % o.dirct, \'w\')\n  f_conv.write(\'<NnetProto>\\n\')\n  f_conv.write(convolution_proto)\n  f_conv.write(\'</NnetProto>\\n\')\n  f_conv.close()\n  \n  # pitch part\n  f_pitch = open(\'%s/nnet.proto.pitch\' % o.dirct, \'w\')\n  f_pitch.write(\'<NnetProto>\\n\')\n  f_pitch.write(\'<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f\\n\' % \\\n\t\t((o.pitch_dim * (o.delta_order+1) * (o.splice*2+1)), o.num_pitch_neurons, -2.0, 4.0, 0.109375))\n  f_pitch.write(\'%s <InputDim> %d <OutputDim> %d\\n\' % \\\n\t\t(o.activation_type, o.num_pitch_neurons, o.num_pitch_neurons))\n  f_pitch.write(\'<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f\\n\' % \\\n\t\t(o.num_pitch_neurons, o.num_pitch_neurons, -2.0, 4.0, 0.109375))\n  f_pitch.write(\'%s <InputDim> %d <OutputDim> %d\\n\' % \\\n\t\t(o.activation_type, o.num_pitch_neurons, o.num_pitch_neurons))\n  f_pitch.write(\'</NnetProto>\\n\')\n  f_pitch.close()\n\n  # paralell part\n  vector = \'\'\n  for i in range(1, (feat_raw_dim + o.pitch_dim) * (o.delta_order+1) * (o.splice*2+1), feat_raw_dim + o.pitch_dim):\n    vector += \'%d:1:%d \' % (i, i + feat_raw_dim - 1)\n  for i in range(feat_raw_dim+1, (feat_raw_dim + o.pitch_dim) * (o.delta_order+1) * (o.splice*2+1), feat_raw_dim + o.pitch_dim):\n    vector += \'%d:1:%d \' % (i, i + o.pitch_dim - 1)\n  print \'<Copy> <InputDim> %d <OutputDim> %d <BuildVector>  %s </BuildVector> \' % \\\n\t((feat_raw_dim + o.pitch_dim) * (o.delta_order+1) * (o.splice*2+1), (feat_raw_dim + o.pitch_dim) * (o.delta_order+1) * (o.splice*2+1), vector)\n  print \'<ParallelComponent> <InputDim> %d <OutputDim> %d <NestedNnetProto> %s %s </NestedNnetProto>\' % \\\n\t((feat_raw_dim + o.pitch_dim) * (o.delta_order+1) * (o.splice*2+1), o.num_pitch_neurons + cnn2_output_dim, \'%s/nnet.proto.convolution\' % o.dirct, \'%s/nnet.proto.pitch\' % o.dirct)\n\n  num_convolution_output = o.num_pitch_neurons + cnn2_output_dim\nelse: # no pitch\n  print convolution_proto\n\n# We are done!\nsys.exit(0)\n\n\n'"
ASR/kaldi_decoding_scripts/utils/nnet/make_cnn_proto.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2014  Brno University of Technology (author: Katerina Zmolikova, Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# Generated Nnet prototype, to be initialized by \'nnet-initialize\'.\n\nimport math, random, sys\nfrom optparse import OptionParser\n\n###\n### Parse options\n###\nusage=""%prog [options] <feat-dim> <num-leaves> <num-hidden-layers> <num-hidden-neurons>  >nnet-proto-file""\nparser = OptionParser(usage)\n\nparser.add_option(\'--activation-type\', dest=\'activation_type\', \n                   help=\'Select type of activation function : (<Sigmoid>|<Tanh>) [default: %default]\', \n                   default=\'<Sigmoid>\', type=\'string\');\nparser.add_option(\'--num-filters1\', dest=\'num_filters1\',\n\t\t   help=\'Number of filters in first convolutional layer [default: %default]\',\n\t\t   default=128, type=\'int\')\nparser.add_option(\'--num-filters2\', dest=\'num_filters2\',\n\t\t   help=\'Number of filters in second convolutional layer [default: %default]\',\n\t\t   default=256, type=\'int\')\nparser.add_option(\'--pool-size\', dest=\'pool_size\',\n\t  \t   help=\'Size of pooling [default: %default]\',\n\t\t   default=3, type=\'int\')\nparser.add_option(\'--pool-step\', dest=\'pool_step\',\n\t\t  help=\'Step of pooling [default: %default]\',\n\t\t  default=3, type=\'int\')\nparser.add_option(\'--pool-type\', dest=\'pool_type\',\n\t\t  help=\'Type of pooling (Max || Average) [default: %default]\',\n\t\t  default=\'Max\', type=\'string\')\nparser.add_option(\'--pitch-dim\', dest=\'pitch_dim\',\n\t\t  help=\'Number of features representing pitch [default: %default]\',\n\t\t  default=0, type=\'int\')\nparser.add_option(\'--delta-order\', dest=\'delta_order\',\n\t\t  help=\'Order of delta features [default: %default]\',\n\t\t  default=2, type=\'int\')\nparser.add_option(\'--splice\', dest=\'splice\',\n\t\t  help=\'Length of splice [default: %default]\',\n\t\t  default=5,type=\'int\')\nparser.add_option(\'--patch-step1\', dest=\'patch_step1\',\n\t\t  help=\'Patch step of first convolutional layer [default: %default]\',\n\t\t  default=1, type=\'int\')\nparser.add_option(\'--patch-dim1\', dest=\'patch_dim1\',\n\t\t  help=\'Dim of convolutional kernel in 1st layer (freq. axis) [default: %default]\',\n  \t\t  default=8, type=\'int\')\nparser.add_option(\'--patch-dim2\', dest=\'patch_dim2\',\n\t\t  help=\'Dim of convolutional kernel in 2nd layer (freq. axis) [default: %default]\',\n  \t\t  default=4, type=\'int\')\nparser.add_option(\'--dir\', dest=\'protodir\',\n\t\t  help=\'Directory, where network prototypes will be saved [default: %default]\',\n\t\t  default=\'.\', type=\'string\')\nparser.add_option(\'--num-pitch-neurons\', dest=\'num_pitch_neurons\',\n\t\t  help=\'Number of neurons in layers processing pitch features [default: %default]\',\n\t\t  default=\'200\', type=\'int\')\n\n(o,args) = parser.parse_args()\nif len(args) != 1 : \n  parser.print_help()\n  sys.exit(1)\n \nfeat_dim = int(args[0]);\n### End parse options \n\nfeat_raw_dim = feat_dim / (o.delta_order+1) / (o.splice*2+1) - o.pitch_dim # we need number of feats without deltas and splice and pitch\n\n# Check\nassert(feat_dim > 0)\nassert(o.pool_type == \'Max\' or o.pool_type == \'Average\')\n\n###\n### Print prototype of the network\n###\n\n# Begin the prototype\nprint ""<NnetProto>""\n\n# Convolutional part of network\nnum_patch1 = 1 + (feat_raw_dim - o.patch_dim1) / o.patch_step1\nnum_pool = 1 + (num_patch1 - o.pool_size) / o.pool_step\npatch_dim2 = o.patch_dim2 * o.num_filters1\npatch_step2 = o.num_filters1\npatch_stride2 = num_pool * o.num_filters1 # same as layer1 outputs \nnum_patch2 = 1 + (num_pool * o.num_filters1 - patch_dim2) / patch_step2\n\nconvolution_proto = \'\'  \n\nconvolution_proto += ""<ConvolutionalComponent> <InputDim> %d <OutputDim> %d <PatchDim> %d <PatchStep> %d <PatchStride> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <MaxNorm> %f\\n"" % \\\n\t\t\t(feat_raw_dim * (o.delta_order+1) * (o.splice*2+1), o.num_filters1 * num_patch1, o.patch_dim1, o.patch_step1, feat_raw_dim, -1.0, 2.0, 0.02, 30) #~8x11x3 = 264 inputs\nconvolution_proto += ""<%sPoolingComponent> <InputDim> %d <OutputDim> %d <PoolSize> %d <PoolStep> %d <PoolStride> %d\\n"" % \\\n\t\t\t(o.pool_type, o.num_filters1*num_patch1, o.num_filters1*num_pool, o.pool_size, o.pool_step, o.num_filters1)\nconvolution_proto += ""<Rescale> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % \\\n\t\t\t(o.num_filters1*num_pool, o.num_filters1*num_pool, 1)\nconvolution_proto += ""<AddShift> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % \\\n\t\t\t(o.num_filters1*num_pool, o.num_filters1*num_pool, 0)\nconvolution_proto += ""%s <InputDim> %d <OutputDim> %d\\n"" % \\\n\t\t\t(o.activation_type, o.num_filters1*num_pool, o.num_filters1*num_pool)\nconvolution_proto += ""<ConvolutionalComponent> <InputDim> %d <OutputDim> %d <PatchDim> %d <PatchStep> %d <PatchStride> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <MaxNorm> %f\\n"" % \\\n\t\t\t(o.num_filters1*num_pool, o.num_filters2*num_patch2, patch_dim2, patch_step2, patch_stride2, -2.0, 4.0, 0.1, 50) #~4x128 = 512 inputs\nconvolution_proto += ""<Rescale> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % \\\n\t\t\t(o.num_filters2 * num_patch2, o.num_filters2*num_patch2, 1)\nconvolution_proto += ""<AddShift> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % \\\n\t\t\t(o.num_filters2*num_patch2, o.num_filters2*num_patch2, 0)\nconvolution_proto += ""%s <InputDim> %d <OutputDim> %d\\n"" % \\\n\t\t\t(o.activation_type, o.num_filters2*num_patch2, o.num_filters2*num_patch2)\n\nif (o.pitch_dim > 0):\n  # convolutional part\n  f_conv = open(\'%s/nnet.proto.convolution\' % o.protodir, \'w\')\n  f_conv.write(\'<NnetProto>\\n\')\n  f_conv.write(convolution_proto)\n  f_conv.write(\'</NnetProto>\\n\')\n  f_conv.close()\n  \n  # pitch part\n  f_pitch = open(\'%s/nnet.proto.pitch\' % o.protodir, \'w\')\n  f_pitch.write(\'<NnetProto>\\n\')\n  f_pitch.write(\'<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f\\n\' % \\\n\t\t((o.pitch_dim * (o.delta_order+1) * (o.splice*2+1)), o.num_pitch_neurons, -2, 4, 0.02))\n  f_pitch.write(\'%s <InputDim> %d <OutputDim> %d\\n\' % \\\n\t\t(o.activation_type, o.num_pitch_neurons, o.num_pitch_neurons))\n  f_pitch.write(\'<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f\\n\' % \\\n\t\t(o.num_pitch_neurons, o.num_pitch_neurons, -2, 4, 0.1))\n  f_pitch.write(\'%s <InputDim> %d <OutputDim> %d\\n\' % \\\n\t\t(o.activation_type, o.num_pitch_neurons, o.num_pitch_neurons))\n  f_pitch.write(\'</NnetProto>\\n\')\n  f_pitch.close()\n\n  # paralell part\n  vector = \'\'\n  for i in range(1, (feat_raw_dim + o.pitch_dim) * (o.delta_order+1) * (o.splice*2+1), feat_raw_dim + o.pitch_dim):\n    vector += \'%d:1:%d \' % (i, i + feat_raw_dim - 1)\n  for i in range(feat_raw_dim+1, (feat_raw_dim + o.pitch_dim) * (o.delta_order+1) * (o.splice*2+1), feat_raw_dim + o.pitch_dim):\n    vector += \'%d:1:%d \' % (i, i + o.pitch_dim - 1)\n  print \'<Copy> <InputDim> %d <OutputDim> %d <BuildVector> %s </BuildVector>\' % \\\n\t((feat_raw_dim + o.pitch_dim) * (o.delta_order+1) * (o.splice*2+1), (feat_raw_dim + o.pitch_dim) * (o.delta_order+1) * (o.splice*2+1), vector)\n  print \'<ParallelComponent> <InputDim> %d <OutputDim> %d <NestedNnetProto> %s %s </NestedNnetProto>\' % \\\n\t((feat_raw_dim + o.pitch_dim) * (o.delta_order+1) * (o.splice*2+1), o.num_pitch_neurons + o.num_filters2*num_patch2, \'%s/nnet.proto.convolution\' % o.protodir, \'%s/nnet.proto.pitch\' % o.protodir)\n\nelse: # no pitch\n  print convolution_proto\n\n# We are done!\nsys.exit(0)\n\n'"
ASR/kaldi_decoding_scripts/utils/nnet/make_lstm_proto.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2015  Brno University of Technology (author: Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# Generated Nnet prototype, to be initialized by \'nnet-initialize\'.\n\nimport sys\n\n###\n### Parse options\n###\nfrom optparse import OptionParser\nusage=""%prog [options] <feat-dim> <num-leaves> >nnet-proto-file""\nparser = OptionParser(usage)\n#\nparser.add_option(\'--num-cells\', dest=\'num_cells\', type=\'int\', default=800, \n                   help=\'Number of LSTM cells [default: %default]\');\nparser.add_option(\'--num-recurrent\', dest=\'num_recurrent\', type=\'int\', default=512, \n                   help=\'Number of LSTM recurrent units [default: %default]\');\nparser.add_option(\'--num-layers\', dest=\'num_layers\', type=\'int\', default=2, \n                   help=\'Number of LSTM layers [default: %default]\');\nparser.add_option(\'--lstm-stddev-factor\', dest=\'lstm_stddev_factor\', type=\'float\', default=0.01, \n                   help=\'Standard deviation of initialization [default: %default]\');\nparser.add_option(\'--param-stddev-factor\', dest=\'param_stddev_factor\', type=\'float\', default=0.04, \n                   help=\'Standard deviation in output layer [default: %default]\');\nparser.add_option(\'--clip-gradient\', dest=\'clip_gradient\', type=\'float\', default=5.0, \n                   help=\'Clipping constant applied to gradients [default: %default]\');\n#\n(o,args) = parser.parse_args()\nif len(args) != 2 : \n  parser.print_help()\n  sys.exit(1)\n\n(feat_dim, num_leaves) = map(int,args);\n\n# Original prototype from Jiayu,\n#<NnetProto>\n#<Transmit> <InputDim> 40 <OutputDim> 40\n#<LstmProjectedStreams> <InputDim> 40 <OutputDim> 512 <CellDim> 800 <ParamScale> 0.01 <NumStream> 4\n#<AffineTransform> <InputDim> 512 <OutputDim> 8000 <BiasMean> 0.000000 <BiasRange> 0.000000 <ParamStddev> 0.04\n#<Softmax> <InputDim> 8000 <OutputDim> 8000\n#</NnetProto>\n\nprint ""<NnetProto>""\n# normally we won\'t use more than 2 layers of LSTM\nif o.num_layers == 1:\n    print ""<LstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f"" % \\\n        (feat_dim, o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)\nelif o.num_layers == 2:\n    print ""<LstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f"" % \\\n        (feat_dim, o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)\n    print ""<LstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f"" % \\\n        (o.num_recurrent, o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)\nelse:\n    sys.stderr.write(""make_lstm_proto.py ERROR: more than 2 layers of LSTM, not supported yet.\\n"")\n    sys.exit(1)\nprint ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> 0.0 <BiasRange> 0.0 <ParamStddev> %f"" % \\\n    (o.num_recurrent, num_leaves, o.param_stddev_factor)\nprint ""<Softmax> <InputDim> %d <OutputDim> %d"" % \\\n    (num_leaves, num_leaves)\nprint ""</NnetProto>""\n\n\n'"
ASR/kaldi_decoding_scripts/utils/nnet/make_nnet_proto.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2014  Brno University of Technology (author: Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# Generated Nnet prototype, to be initialized by \'nnet-initialize\'.\n\nimport math, random, sys, re\n\n###\n### Parse options\n###\nfrom optparse import OptionParser\nusage=""%prog [options] <feat-dim> <num-leaves> <num-hid-layers> <num-hid-neurons> >nnet-proto-file""\nparser = OptionParser(usage)\n\nparser.add_option(\'--no-proto-head\', dest=\'with_proto_head\', \n                   help=\'Do not put <NnetProto> head-tag in the prototype [default: %default]\', \n                   default=True, action=\'store_false\');\nparser.add_option(\'--no-softmax\', dest=\'with_softmax\', \n                   help=\'Do not put <SoftMax> in the prototype [default: %default]\', \n                   default=True, action=\'store_false\');\nparser.add_option(\'--block-softmax-dims\', dest=\'block_softmax_dims\', \n                   help=\'Generate <BlockSoftmax> with dims D1:D2:D3 [default: %default]\', \n                   default="""", type=\'string\');\nparser.add_option(\'--activation-type\', dest=\'activation_type\', \n                   help=\'Select type of activation function : (<Sigmoid>|<Tanh>) [default: %default]\', \n                   default=\'<Sigmoid>\', type=\'string\');\nparser.add_option(\'--hid-bias-mean\', dest=\'hid_bias_mean\', \n                   help=\'Set bias for hidden activations [default: %default]\', \n                   default=-2.0, type=\'float\');\nparser.add_option(\'--hid-bias-range\', dest=\'hid_bias_range\', \n                   help=\'Set bias range for hidden activations (+/- 1/2 range around mean) [default: %default]\', \n                   default=4.0, type=\'float\');\nparser.add_option(\'--param-stddev-factor\', dest=\'param_stddev_factor\', \n                   help=\'Factor to rescale Normal distriburtion for initalizing weight matrices [default: %default]\', \n                   default=0.1, type=\'float\');\nparser.add_option(\'--bottleneck-dim\', dest=\'bottleneck_dim\', \n                   help=\'Make bottleneck network with desired bn-dim (0 = no bottleneck) [default: %default]\',\n                   default=0, type=\'int\');\nparser.add_option(\'--no-glorot-scaled-stddev\', dest=\'with_glorot\', \n                   help=\'Generate normalized weights according to X.Glorot paper, but mapping U->N with same variance (factor sqrt(x/(dim_in+dim_out)))\', \n                   action=\'store_false\', default=True);\nparser.add_option(\'--no-smaller-input-weights\', dest=\'smaller_input_weights\', \n                   help=\'Disable 1/12 reduction of stddef in input layer [default: %default]\', \n                   action=\'store_false\', default=True);\nparser.add_option(\'--no-bottleneck-trick\', dest=\'bottleneck_trick\',\n                   help=\'Disable smaller initial weights and learning rate around bottleneck\',\n                   action=\'store_false\', default=True);\nparser.add_option(\'--max-norm\', dest=\'max_norm\', \n                   help=\'Max radius of neuron-weights in L2 space (if longer weights get shrinked, not applied to last layer, 0.0 = disable) [default: %default]\', \n                   default=0.0, type=\'float\');\n\n\n(o,args) = parser.parse_args()\nif len(args) != 4 : \n  parser.print_help()\n  sys.exit(1)\n  \n(feat_dim, num_leaves, num_hid_layers, num_hid_neurons) = map(int,args);\n### End parse options \n\n\n# Check\nassert(feat_dim > 0)\nassert(num_leaves > 0)\nassert(num_hid_layers >= 0)\nassert(num_hid_neurons > 0)\nif o.block_softmax_dims:\n  assert(sum(map(int, re.split(""[,:]"", o.block_softmax_dims))) == num_leaves) # posible separators : \',\' \':\'\n\n# Optionaly scale\ndef Glorot(dim1, dim2):\n  if o.with_glorot:\n    # 35.0 = magic number, gives ~1.0 in inner layers for hid-dim 1024dim,\n    return 35.0 * math.sqrt(2.0/(dim1+dim2)); \n  else:\n    return 1.0\n\n\n###\n### Print prototype of the network\n###\n\n# NO HIDDEN LAYER, ADDING BOTTLENECK!\n# No hidden layer while adding bottleneck means:\n# - add bottleneck layer + hidden layer + output layer\nif num_hid_layers == 0 and o.bottleneck_dim != 0:\n  assert(o.bottleneck_dim > 0)\n  assert(num_hid_layers == 0)\n  if o.with_proto_head : print ""<NnetProto>""\n  if o.bottleneck_trick:\n    # 25% smaller stddev -> small bottleneck range, 10x smaller learning rate\n    print ""<LinearTransform> <InputDim> %d <OutputDim> %d <ParamStddev> %f <LearnRateCoef> %f"" % \\\n     (feat_dim, o.bottleneck_dim, \\\n      (o.param_stddev_factor * Glorot(feat_dim, o.bottleneck_dim) * 0.75 ), 0.1)\n    # 25% smaller stddev -> smaller gradient in prev. layer, 10x smaller learning rate for weigts & biases\n    print ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <LearnRateCoef> %f <BiasLearnRateCoef> %f <MaxNorm> %f"" % \\\n     (o.bottleneck_dim, num_hid_neurons, o.hid_bias_mean, o.hid_bias_range, \\\n      (o.param_stddev_factor * Glorot(o.bottleneck_dim, num_hid_neurons) * 0.75 ), 0.1, 0.1, o.max_norm)\n  else:\n    print ""<LinearTransform> <InputDim> %d <OutputDim> %d <ParamStddev> %f"" % \\\n     (feat_dim, o.bottleneck_dim, \\\n      (o.param_stddev_factor * Glorot(feat_dim, o.bottleneck_dim)))\n    print ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <MaxNorm> %f"" % \\\n     (o.bottleneck_dim, num_hid_neurons, o.hid_bias_mean, o.hid_bias_range, \\\n      (o.param_stddev_factor * Glorot(o.bottleneck_dim, num_hid_neurons)), o.max_norm)\n  print ""%s <InputDim> %d <OutputDim> %d"" % (o.activation_type, num_hid_neurons, num_hid_neurons) # Non-linearity\n  # Last AffineTransform (10x smaller learning rate on bias)\n  print ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <LearnRateCoef> %f <BiasLearnRateCoef> %f"" % \\\n   (num_hid_neurons, num_leaves, 0.0, 0.0, \\\n    (o.param_stddev_factor * Glorot(num_hid_neurons, num_leaves)), 1.0, 0.1)\n  # Optionaly append softmax\n  if o.with_softmax:\n    if o.block_softmax_dims == """":\n      print ""<Softmax> <InputDim> %d <OutputDim> %d"" % (num_leaves, num_leaves)\n    else:\n      print ""<BlockSoftmax> <InputDim> %d <OutputDim> %d <BlockDims> %s"" % (num_leaves, num_leaves, o.block_softmax_dims)\n  print ""</NnetProto>""\n  # We are done!\n  sys.exit(0)\n\n# NO HIDDEN LAYERS!\n# Add only last layer (logistic regression)\nif num_hid_layers == 0:\n  if o.with_proto_head : print ""<NnetProto>"" \n  print ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f"" % \\\n        (feat_dim, num_leaves, 0.0, 0.0, (o.param_stddev_factor * Glorot(feat_dim, num_leaves)))\n  if o.with_softmax:\n    if o.block_softmax_dims == """":\n      print ""<Softmax> <InputDim> %d <OutputDim> %d"" % (num_leaves, num_leaves)\n    else:\n      print ""<BlockSoftmax> <InputDim> %d <OutputDim> %d <BlockDims> %s"" % (num_leaves, num_leaves, o.block_softmax_dims)\n  print ""</NnetProto>""\n  # We are done!\n  sys.exit(0)\n\n\n# THE USUAL DNN PROTOTYPE STARTS HERE!\n# Assuming we have >0 hidden layers,\nassert(num_hid_layers > 0)\n\n# Begin the prototype,\nif o.with_proto_head : print ""<NnetProto>"" \n\n# First AffineTranform,\nprint ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <MaxNorm> %f"" % \\\n      (feat_dim, num_hid_neurons, o.hid_bias_mean, o.hid_bias_range, \\\n       (o.param_stddev_factor * Glorot(feat_dim, num_hid_neurons) * \\\n        (math.sqrt(1.0/12.0) if o.smaller_input_weights else 1.0)), o.max_norm) \n      # Note.: compensating dynamic range mismatch between input features and Sigmoid-hidden layers,\n      # i.e. mapping the std-dev of N(0,1) (input features) to std-dev of U[0,1] (sigmoid-outputs).\n      # This is done by multiplying with stddev(U[0,1]) = sqrt(1/12).\n      # The stddev of weights is consequently reduced by 0.29x.\nprint ""%s <InputDim> %d <OutputDim> %d"" % (o.activation_type, num_hid_neurons, num_hid_neurons)\n\n# Internal AffineTransforms,\nfor i in range(num_hid_layers-1):\n  print ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <MaxNorm> %f"" % \\\n        (num_hid_neurons, num_hid_neurons, o.hid_bias_mean, o.hid_bias_range, \\\n         (o.param_stddev_factor * Glorot(num_hid_neurons, num_hid_neurons)), o.max_norm)\n  print ""%s <InputDim> %d <OutputDim> %d"" % (o.activation_type, num_hid_neurons, num_hid_neurons)\n\n# Optionaly add bottleneck,\nif o.bottleneck_dim != 0:\n  assert(o.bottleneck_dim > 0)\n  if o.bottleneck_trick:\n    # 25% smaller stddev -> small bottleneck range, 10x smaller learning rate\n    print ""<LinearTransform> <InputDim> %d <OutputDim> %d <ParamStddev> %f <LearnRateCoef> %f"" % \\\n     (num_hid_neurons, o.bottleneck_dim, \\\n      (o.param_stddev_factor * Glorot(num_hid_neurons, o.bottleneck_dim) * 0.75 ), 0.1)\n    # 25% smaller stddev -> smaller gradient in prev. layer, 10x smaller learning rate for weigts & biases\n    print ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <LearnRateCoef> %f <BiasLearnRateCoef> %f <MaxNorm> %f"" % \\\n     (o.bottleneck_dim, num_hid_neurons, o.hid_bias_mean, o.hid_bias_range, \\\n      (o.param_stddev_factor * Glorot(o.bottleneck_dim, num_hid_neurons) * 0.75 ), 0.1, 0.1, o.max_norm)\n  else:\n    # Same learninig-rate and stddev-formula everywhere,\n    print ""<LinearTransform> <InputDim> %d <OutputDim> %d <ParamStddev> %f"" % \\\n     (num_hid_neurons, o.bottleneck_dim, \\\n      (o.param_stddev_factor * Glorot(num_hid_neurons, o.bottleneck_dim)))\n    print ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <MaxNorm> %f"" % \\\n     (o.bottleneck_dim, num_hid_neurons, o.hid_bias_mean, o.hid_bias_range, \\\n      (o.param_stddev_factor * Glorot(o.bottleneck_dim, num_hid_neurons)), o.max_norm)\n  print ""%s <InputDim> %d <OutputDim> %d"" % (o.activation_type, num_hid_neurons, num_hid_neurons)\n\n# Last AffineTransform (10x smaller learning rate on bias)\nprint ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <LearnRateCoef> %f <BiasLearnRateCoef> %f"" % \\\n      (num_hid_neurons, num_leaves, 0.0, 0.0, \\\n       (o.param_stddev_factor * Glorot(num_hid_neurons, num_leaves)), 1.0, 0.1)\n\n# Optionaly append softmax\nif o.with_softmax:\n  if o.block_softmax_dims == """":\n    print ""<Softmax> <InputDim> %d <OutputDim> %d"" % (num_leaves, num_leaves)\n  else:\n    print ""<BlockSoftmax> <InputDim> %d <OutputDim> %d <BlockDims> %s"" % (num_leaves, num_leaves, o.block_softmax_dims)\n\n# End the prototype\nprint ""</NnetProto>""\n\n# We are done!\nsys.exit(0)\n\n'"
