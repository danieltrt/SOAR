file_path,api_count,code
censor.py,0,"b'import sys\nimport warnings\nfrom modules.data import bert_data_clf\nfrom modules.models.classifiers import BERTBiLSTMAttnClassifier\nfrom modules.train.train_clf import NerLearner\n\n\nwarnings.filterwarnings(""ignore"")\nsys.path.append(""../"")\n\n\ndef main():\n    train_df_path = ""/home/ubuntu/censor/train2.csv""\n    valid_df_path = ""/home/ubuntu/censor/dev2.csv""\n    test_df_path = ""/home/ubuntu/censor/test.csv""\n    num_epochs = 100\n\n\n    data = bert_data_clf.LearnDataClass.create(\n        train_df_path=train_df_path,\n        valid_df_path=valid_df_path,\n        idx2cls_path=""/home/ubuntu/censor/idx2cls.txt"",\n        clear_cache=False,\n        batch_size=64\n    )\n\n    model = BERTBiLSTMAttnClassifier.create(len(data.train_ds.cls2idx), hidden_dim=768)\n    learner = NerLearner(\n        model, data, ""/home/ubuntu/censor/cls.cpt4"", t_total=num_epochs * len(data.train_dl))\n    learner.fit(epochs=num_epochs)\n    \n\nif __name__ == ""__main__"":\n    main()\n'"
modules/__init__.py,0,"b'from .utils import get_tqdm\n\n\ntqdm = get_tqdm()\n\n\n__all__ = [""tqdm""]\n'"
modules/utils.py,0,"b'import os\nimport json\nimport numpy\nimport bson\nimport sys\n\n\ndef ipython_info():\n    ip = False\n    if \'ipykernel\' in sys.modules:\n        ip = \'notebook\'\n    elif \'IPython\' in sys.modules:\n        ip = \'terminal\'\n    return ip\n\n\ndef get_tqdm():\n    ip = ipython_info()\n    if ip == ""terminal"" or not ip:\n        from tqdm import tqdm\n        return tqdm\n    else:\n        try:\n            from tqdm import tqdm_notebook\n            return tqdm_notebook\n        except:\n            from tqdm import tqdm\n            return tqdm\n\n\nclass JsonEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, numpy.integer):\n            return int(obj)\n        elif isinstance(obj, numpy.floating):\n            return float(obj)\n        elif isinstance(obj, numpy.ndarray):\n            return obj.tolist()\n        elif isinstance(obj, bson.ObjectId):\n            return str(obj)\n        else:\n            return super(JsonEncoder, self).default(obj)\n\n\ndef jsonify(data):\n    return json.dumps(data, cls=JsonEncoder)\n\n\ndef read_config(config):\n    if isinstance(config, str):\n        with open(config, ""r"", encoding=""utf-8"") as f:\n            config = json.load(f)\n    return config\n\n\ndef save_config(config, path):\n    with open(path, ""w"") as file:\n        json.dump(config, file, cls=JsonEncoder)\n\n\ndef if_none(origin, other):\n    return other if origin is None else origin\n\n\ndef get_files_path_from_dir(path):\n    f = []\n    for dir_path, dir_names, filenames in os.walk(path):\n        for f_name in filenames:\n            f.append(dir_path + ""/"" + f_name)\n    return f\n'"
modules/analyze_utils/__init__.py,0,"b'from .utils import *\n\n\n__all__ = [""read_json"", ""save_json""]\n'"
modules/analyze_utils/main_metrics.py,0,"b'# This code is reused from https://github.com/deepmipt/DeepPavlov/blob/master/deeppavlov/metrics/fmeasure.py\nimport itertools\nfrom collections import OrderedDict\n\n\ndef chunk_finder(current_token, previous_token, tag):\n    current_tag = current_token.split(\'_\', 1)[-1]\n    previous_tag = previous_token.split(\'_\', 1)[-1]\n    if previous_tag != tag:\n        previous_tag = \'O\'\n    if current_tag != tag:\n        current_tag = \'O\'\n    if (previous_tag == \'O\' and current_token == \'B_\' + tag) or \\\n            (previous_token == \'I_\' + tag and current_token == \'B_\' + tag) or \\\n            (previous_token == \'B_\' + tag and current_token == \'B_\' + tag) or \\\n            (previous_tag == \'O\' and current_token == \'I_\' + tag):\n        create_chunk = True\n    else:\n        create_chunk = False\n\n    if (previous_token == \'I_\' + tag and current_token == \'B_\' + tag) or \\\n            (previous_token == \'B_\' + tag and current_token == \'B_\' + tag) or \\\n            (current_tag == \'O\' and previous_token == \'I_\' + tag) or \\\n            (current_tag == \'O\' and previous_token == \'B_\' + tag):\n        pop_out = True\n    else:\n        pop_out = False\n    return create_chunk, pop_out\n\n\ndef _global_stats_f1(results):\n    total_true_entities = 0\n    total_predicted_entities = 0\n    total_precision = 0\n    total_recall = 0\n    total_f1 = 0\n    total_correct = 0\n    for tag in results:\n        if tag == \'__total__\':\n            continue\n\n        n_pred = results[tag][\'n_pred\']\n        n_true = results[tag][\'n_true\']\n        total_correct += results[tag][\'tp\']\n        total_true_entities += n_true\n        total_predicted_entities += n_pred\n        total_precision += results[tag][\'precision\'] * n_pred\n        total_recall += results[tag][\'recall\'] * n_true\n        total_f1 += results[tag][\'f1\'] * n_true\n    if total_true_entities > 0:\n        accuracy = total_correct / total_true_entities * 100\n        total_recall = total_recall / total_true_entities\n    else:\n        accuracy = 0\n        total_recall = 0\n    if total_predicted_entities > 0:\n        total_precision = total_precision / total_predicted_entities\n    else:\n        total_precision = 0\n\n    if total_precision + total_recall > 0:\n        total_f1 = 2 * total_precision * total_recall / (total_precision + total_recall)\n    else:\n        total_f1 = 0\n\n    total_res = {\'n_predicted_entities\': total_predicted_entities,\n                 \'n_true_entities\': total_true_entities,\n                 \'precision\': total_precision,\n                 \'recall\': total_recall,\n                 \'f1\': total_f1}\n    return total_res, accuracy, total_true_entities, total_predicted_entities, total_correct\n\n\ndef precision_recall_f1(y_true, y_pred, print_results=True, short_report=False, entity_of_interest=None):\n    y_true = list(itertools.chain(*y_true))\n    y_pred = list(itertools.chain(*y_pred))\n    # Find all tags\n    tags = set()\n    for tag in itertools.chain(y_true, y_pred):\n        if tag not in [""O"", ""I_O"", ""B_O""]:\n            current_tag = tag[2:]\n            tags.add(current_tag)\n    tags = sorted(list(tags))\n\n    results = OrderedDict()\n    for tag in tags:\n        results[tag] = OrderedDict()\n    results[\'__total__\'] = OrderedDict()\n    n_tokens = len(y_true)\n    # Firstly we find all chunks in the ground truth and prediction\n    # For each chunk we write starting and ending indices\n\n    for tag in tags:\n        count = 0\n        true_chunk = []\n        pred_chunk = []\n        y_true = [str(y) for y in y_true]\n        y_pred = [str(y) for y in y_pred]\n        prev_tag_true = \'O\'\n        prev_tag_pred = \'O\'\n        while count < n_tokens:\n            yt = y_true[count]\n            yp = y_pred[count]\n\n            create_chunk_true, pop_out_true = chunk_finder(yt, prev_tag_true, tag)\n            if pop_out_true:\n                true_chunk[-1] = (true_chunk[-1], count - 1)\n            if create_chunk_true:\n                true_chunk.append(count)\n\n            create_chunk_pred, pop_out_pred = chunk_finder(yp, prev_tag_pred, tag)\n            if pop_out_pred:\n                pred_chunk[-1] = (pred_chunk[-1], count - 1)\n            if create_chunk_pred:\n                pred_chunk.append(count)\n            prev_tag_true = yt\n            prev_tag_pred = yp\n            count += 1\n\n        if len(true_chunk) > 0 and not isinstance(true_chunk[-1], tuple):\n            true_chunk[-1] = (true_chunk[-1], count - 1)\n        if len(pred_chunk) > 0 and not isinstance(pred_chunk[-1], tuple):\n            pred_chunk[-1] = (pred_chunk[-1], count - 1)\n\n        # Then we find all correctly classified intervals\n        # True positive results\n        tp = len(set(pred_chunk).intersection(set(true_chunk)))\n        # And then just calculate errors of the first and second kind\n        # False negative\n        fn = len(true_chunk) - tp\n        # False positive\n        fp = len(pred_chunk) - tp\n        if tp + fp > 0:\n            precision = tp / (tp + fp) * 100\n        else:\n            precision = 0\n        if tp + fn > 0:\n            recall = tp / (tp + fn) * 100\n        else:\n            recall = 0\n        if precision + recall > 0:\n            f1 = 2 * precision * recall / (precision + recall)\n        else:\n            f1 = 0\n        results[tag][\'precision\'] = precision\n        results[tag][\'recall\'] = recall\n        results[tag][\'f1\'] = f1\n        results[tag][\'n_pred\'] = len(pred_chunk)\n        results[tag][\'n_true\'] = len(true_chunk)\n        results[tag][\'tp\'] = tp\n        results[tag][\'fn\'] = fn\n        results[tag][\'fp\'] = fp\n\n    results[\'__total__\'], accuracy, total_true_entities, total_predicted_entities, total_correct = _global_stats_f1(results)\n    results[\'__total__\'][\'n_pred\'] = total_predicted_entities\n    results[\'__total__\'][\'n_true\'] = total_true_entities\n    results[\'__total__\'][""n_tokens""] = n_tokens\n    if print_results:\n        _print_conll_report(results, short_report, entity_of_interest)\n    return results\n\n\ndef _print_conll_report(results, short_report=False, entity_of_interest=None):\n    _, accuracy, total_true_entities, total_predicted_entities, total_correct = _global_stats_f1(results)\n    n_tokens = results[\'__total__\'][""n_tokens""]\n    tags = list(results.keys())\n\n    s = \'processed {len} tokens \' \\\n        \'with {tot_true} phrases; \' \\\n        \'found: {tot_pred} phrases;\' \\\n        \' correct: {tot_cor}.\\n\\n\'.format(len=n_tokens,\n                                          tot_true=total_true_entities,\n                                          tot_pred=total_predicted_entities,\n                                          tot_cor=total_correct)\n\n    s += \'precision:  {tot_prec:.2f}%; \' \\\n         \'recall:  {tot_recall:.2f}%; \' \\\n         \'FB1:  {tot_f1:.2f}\\n\\n\'.format(acc=accuracy,\n                                         tot_prec=results[\'__total__\'][\'precision\'],\n                                         tot_recall=results[\'__total__\'][\'recall\'],\n                                         tot_f1=results[\'__total__\'][\'f1\'])\n\n    if not short_report:\n        for tag in tags:\n            if entity_of_interest is not None:\n                if entity_of_interest in tag:\n                    s += \'\\t\' + tag + \': precision:  {tot_prec:.2f}%; \' \\\n                                      \'recall:  {tot_recall:.2f}%; \' \\\n                                      \'F1:  {tot_f1:.2f} \' \\\n                                      \'{tot_predicted}\\n\\n\'.format(tot_prec=results[tag][\'precision\'],\n                                                                   tot_recall=results[tag][\'recall\'],\n                                                                   tot_f1=results[tag][\'f1\'],\n                                                                   tot_predicted=results[tag][\'n_pred\'])\n            elif tag != \'__total__\':\n                s += \'\\t\' + tag + \': precision:  {tot_prec:.2f}%; \' \\\n                                  \'recall:  {tot_recall:.2f}%; \' \\\n                                  \'F1:  {tot_f1:.2f} \' \\\n                                  \'{tot_predicted}\\n\\n\'.format(tot_prec=results[tag][\'precision\'],\n                                                               tot_recall=results[tag][\'recall\'],\n                                                               tot_f1=results[tag][\'f1\'],\n                                                               tot_predicted=results[tag][\'n_pred\'])\n    elif entity_of_interest is not None:\n        s += \'\\t\' + entity_of_interest + \': precision:  {tot_prec:.2f}%; \' \\\n                          \'recall:  {tot_recall:.2f}%; \' \\\n                          \'F1:  {tot_f1:.2f} \' \\\n                          \'{tot_predicted}\\n\\n\'.format(tot_prec=results[entity_of_interest][\'precision\'],\n                                                       tot_recall=results[entity_of_interest][\'recall\'],\n                                                       tot_f1=results[entity_of_interest][\'f1\'],\n                                                       tot_predicted=results[entity_of_interest][\'n_pred\'])\n    print(s)\n'"
modules/analyze_utils/plot_metrics.py,0,"b'import numpy as np\nfrom collections import defaultdict\nfrom matplotlib import pyplot as plt\nfrom .utils import tokens2spans, bert_labels2tokens, voting_choicer, first_choicer\nfrom sklearn_crfsuite.metrics import flat_classification_report\nfrom sklearn.metrics import f1_score\n\n\ndef plot_by_class_curve(history, metric_, sup_labels):\n    by_class = get_by_class_metric(history, metric_, sup_labels)\n    vals = list(by_class.values())\n    x = np.arange(len(vals[0]))\n    args = []\n    for val in vals:\n        args.append(x)\n        args.append(val)\n    plt.figure(figsize=(15, 10))\n    plt.grid(True)\n    plt.plot(*args)\n    plt.legend(list(by_class.keys()))\n    _, _ = plt.yticks(np.arange(0, 1, step=0.1))\n    plt.show()\n\n\ndef get_metrics_by_class(text_res, sup_labels):\n    # text_res = flat_classification_report(y_true, y_pred, labels=labels, digits=3)\n    res = {}\n    for line in text_res.split(""\\n""):\n        line = line.split()\n        if len(line) and line[0] in sup_labels:\n            res[line[0]] = {key: val for key, val in zip([""prec"", ""rec"", ""f1""], line[1:-1])}\n    return res\n\n\ndef get_by_class_metric(history, metric_, sup_labels):\n    res = defaultdict(list)\n    for h in history:\n        h = get_metrics_by_class(h, sup_labels)\n        for class_, metrics_ in h.items():\n            res[class_].append(float(metrics_[metric_]))\n    return res\n\n\ndef get_max_metric(history, metric_, sup_labels, return_idx=False):\n    by_class = get_by_class_metric(history, metric_, sup_labels)\n    by_class_arr = np.array(list(by_class.values()))\n    idx = np.array(by_class_arr.sum(0)).argmax()\n    if return_idx:\n        return list(zip(by_class.keys(), by_class_arr[:, idx])), idx\n    return list(zip(by_class.keys(), by_class_arr[:, idx]))\n\n\ndef get_mean_max_metric(history, metric_=""f1"", return_idx=False):\n    m_idx = 0\n    if metric_ == ""f1"":\n        m_idx = 2\n    elif m_idx == ""rec"":\n        m_idx = 1\n    metrics = [float(h.split(""\\n"")[-3].split()[2 + m_idx]) for h in history]\n    idx = np.argmax(metrics)\n    res = metrics[idx]\n    if return_idx:\n        return idx, res\n    return res\n\n\ndef get_bert_span_report(dl, preds, labels=None, fn=voting_choicer):\n    pred_tokens, pred_labels = bert_labels2tokens(dl, preds)\n    true_tokens, true_labels = bert_labels2tokens(dl, [x.bert_labels for x in dl.dataset])\n    spans_pred = tokens2spans(pred_tokens, pred_labels)\n    spans_true = tokens2spans(true_tokens, true_labels)\n    res_t = []\n    res_p = []\n    for pred_span, true_span in zip(spans_pred, spans_true):\n        text2span = {t: l for t, l in pred_span}\n        for (pt, pl), (tt, tl) in zip(pred_span, true_span):\n            res_t.append(tl)\n            if tt in text2span:\n                res_p.append(pl)\n            else:\n                res_p.append(""O"")\n    return flat_classification_report([res_t], [res_p], labels=labels, digits=4)\n\n\ndef analyze_bert_errors(dl, labels, fn=voting_choicer):\n    errors = []\n    res_tokens = []\n    res_labels = []\n    r_labels = [x.labels for x in dl.dataset]\n    for f, l_, rl in zip(dl.dataset, labels, r_labels):\n        label = fn(f.tok_map, l_)\n        label_r = fn(f.tok_map, rl)\n        prev_idx = 0\n        errors_ = []\n        # if len(label_r) > 1:\n        # assert len(label_r) == len(f.tokens) - 1\n        for idx, (lbl, rl, t) in enumerate(zip(label, label_r, f.tokens)):\n            if lbl != rl:\n                errors_.append(\n                    {""token: "": t,\n                     ""real_label"": rl,\n                     ""pred_label"": lbl,\n                     ""bert_token"": f.bert_tokens[prev_idx:f.tok_map[idx]],\n                     ""real_bert_label"": f.labels[prev_idx:f.tok_map[idx]],\n                     ""pred_bert_label"": l_[prev_idx:f.tok_map[idx]],\n                     ""text_example"": "" "".join(f.tokens[1:-1]),\n                     ""labels"": "" "".join(label_r[1:])})\n            prev_idx = f.tok_map[idx]\n        errors.append(errors_)\n        res_tokens.append(f.tokens[1:-1])\n        res_labels.append(label[1:])\n    return res_tokens, res_labels, errors\n\n\ndef get_f1_score(y_true, y_pred, labels):\n    res_t = []\n    res_p = []\n    for yts, yps in zip(y_true, y_pred):\n        for yt, yp in zip(yts, yps):\n                res_t.append(yt)\n                res_p.append(yp)\n    return f1_score(res_t, res_p, average=""macro"", labels=labels)\n'"
modules/analyze_utils/utils.py,0,"b'from collections import Counter\nimport numpy as np\nimport json\nimport numpy\n\n\ndef voting_choicer(tok_map, labels):\n    label = []\n    prev_idx = 0\n    for origin_idx in tok_map:\n        votes = []\n        for l in labels[prev_idx:origin_idx]:\n            if l != ""X"":\n                votes.append(l)\n        vote_labels = Counter(votes)\n        if not len(vote_labels):\n            vote_labels = {""B_O"": 1}\n        # vote_labels = Counter(c)\n        lb = sorted(list(vote_labels), key=lambda x: vote_labels[x])\n        if len(lb):\n            label.append(lb[-1])\n        prev_idx = origin_idx\n        if origin_idx < 0:\n            break\n\n    return label\n\n\ndef first_choicer(tok_map, labels):\n    label = []\n    prev_idx = 0\n    for origin_idx in tok_map:\n        l = labels[prev_idx]\n        if l in [""X""]:\n            l = ""B_O""\n        if l == ""B_O"":\n            for ll in labels[prev_idx + 1:origin_idx]:\n                if ll not in [""B_O"", ""I_O"", ""X""]:\n                    l = ll\n                    break\n        label.append(l)\n        prev_idx = origin_idx\n        if origin_idx < 0:\n            break\n    # assert ""[SEP]"" not in label\n    return label\n\n\ndef bert_labels2tokens(dl, labels, fn=voting_choicer):\n    res_tokens = []\n    res_labels = []\n    for f, l in zip(dl.dataset, labels):\n        label = fn(f.tok_map, l[1:])\n\n        res_tokens.append(f.tokens[1:-1])\n        res_labels.append(label[1:])\n    return res_tokens, res_labels\n\n\ndef tokens2spans_(tokens_, labels_):\n    res = []\n    idx_ = 0\n    while idx_ < len(labels_):\n        label = labels_[idx_]\n        if label in [""I_O"", ""B_O"", ""O""]:\n            res.append((tokens_[idx_], ""O""))\n            idx_ += 1\n        elif label == ""<eos>"":\n            break\n        elif label == ""[CLS]"" or label == ""<bos>"":\n            res.append((tokens_[idx_], label))\n            idx_ += 1\n        else:\n            span = [tokens_[idx_]]\n            try:\n                span_label = labels_[idx_].split(""_"")[1]\n            except IndexError:\n                print(label, labels_[idx_].split(""_""))\n                span_label = None\n            idx_ += 1\n            while idx_ < len(labels_) and labels_[idx_] not in [""I_O"", ""B_O"", ""O""] \\\n                    and labels_[idx_].split(""_"")[0] == ""I"":\n                if span_label == labels_[idx_].split(""_"")[1]:\n                    span.append(tokens_[idx_])\n                    idx_ += 1\n                else:\n                    break\n            res.append(("" "".join(span), span_label))\n    return res\n\n\ndef tokens2spans(tokens, labels):\n    assert len(tokens) == len(labels)\n\n    return list(map(lambda x: tokens2spans_(*x), zip(tokens, labels)))\n\n\ndef encode_position(pos, emb_dim=10):\n    """"""The sinusoid position encoding""""""\n\n    # keep dim 0 for padding token position encoding zero vector\n    if pos == 0:\n        return np.zeros(emb_dim)\n    position_enc = np.array(\n        [pos / np.power(10000, 2 * (j // 2) / emb_dim) for j in range(emb_dim)])\n\n    # apply sin on 0th,2nd,4th...emb_dim\n    position_enc[0::2] = np.sin(position_enc[0::2])\n    # apply cos on 1st,3rd,5th...emb_dim\n    position_enc[1::2] = np.cos(position_enc[1::2])\n    return list(position_enc.reshape(-1))\n\n\nclass JsonEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, numpy.integer):\n            return int(obj)\n        elif isinstance(obj, numpy.floating):\n            return float(obj)\n        elif isinstance(obj, numpy.ndarray):\n            return obj.tolist()\n        else:\n            return super(JsonEncoder, self).default(obj)\n\n\ndef jsonify(data):\n    return json.dumps(data, cls=JsonEncoder)\n\n\ndef read_json(config):\n    if isinstance(config, str):\n        with open(config, ""r"") as f:\n            config = json.load(f)\n    return config\n\n\ndef save_json(config, path):\n    with open(path, ""w"") as file:\n        json.dump(config, file, cls=JsonEncoder)\n'"
modules/data/__init__.py,0,b''
modules/data/bert_data.py,2,"b'from torch.utils.data import DataLoader\nimport torch\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom modules.utils import read_config, if_none\nfrom modules import tqdm\nimport pandas as pd\nfrom copy import deepcopy\n\n\nclass InputFeature(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(\n            self,\n            # Bert data\n            bert_tokens, input_ids, input_mask, input_type_ids,\n            # Ner data\n            bert_labels, labels_ids, labels,\n            # Origin data\n            tokens, tok_map,\n            # Cls data\n            cls=None, id_cls=None):\n        """"""\n        Data has the following structure.\n        data[0]: list, tokens ids\n        data[1]: list, tokens mask\n        data[2]: list, tokens type ids (for bert)\n        data[3]: list, bert labels ids\n        """"""\n        self.data = []\n        # Bert data\n        self.bert_tokens = bert_tokens\n        self.input_ids = input_ids\n        self.data.append(input_ids)\n        self.input_mask = input_mask\n        self.data.append(input_mask)\n        self.input_type_ids = input_type_ids\n        self.data.append(input_type_ids)\n        # Ner data\n        self.bert_labels = bert_labels\n        self.labels_ids = labels_ids\n        self.data.append(labels_ids)\n        # Classification data\n        self.cls = cls\n        self.id_cls = id_cls\n        if id_cls is not None:\n            self.data.append(id_cls)\n        # Origin data\n        self.tokens = tokens\n        self.tok_map = tok_map\n        self.labels = labels\n\n    def __iter__(self):\n        return iter(self.data)\n\n\nclass TextDataLoader(DataLoader):\n    def __init__(self, data_set, shuffle=False, device=""cuda"", batch_size=16):\n        super(TextDataLoader, self).__init__(\n            dataset=data_set,\n            collate_fn=self.collate_fn,\n            shuffle=shuffle,\n            batch_size=batch_size\n        )\n        self.device = device\n\n    def collate_fn(self, data):\n        res = []\n        token_ml = max(map(lambda x_: sum(x_.data[1]), data))\n        for sample in data:\n            example = []\n            for x in sample:\n                if isinstance(x, list):\n                    x = x[:token_ml]\n                example.append(x)\n            res.append(example)\n        res_ = []\n        for x in zip(*res):\n            res_.append(torch.LongTensor(x))\n        return [t.to(self.device) for t in res_]\n\n\nclass TextDataSet(object):\n\n    @classmethod\n    def from_config(cls, config, clear_cache=False, df=None):\n        return cls.create(**read_config(config), clear_cache=clear_cache, df=df)\n\n    @classmethod\n    def create(cls,\n               idx2labels_path,\n               df_path=None,\n               idx2labels=None,\n               idx2cls=None,\n               idx2cls_path=None,\n               min_char_len=1,\n               model_name=""bert-base-multilingual-cased"",\n               max_sequence_length=424,\n               pad_idx=0,\n               clear_cache=False,\n               is_cls=False,\n               markup=""IO"",\n               df=None, tokenizer=None):\n        if tokenizer is None:\n            tokenizer = BertTokenizer.from_pretrained(model_name)\n        config = {\n            ""min_char_len"": min_char_len,\n            ""model_name"": model_name,\n            ""max_sequence_length"": max_sequence_length,\n            ""clear_cache"": clear_cache,\n            ""df_path"": df_path,\n            ""pad_idx"": pad_idx,\n            ""is_cls"": is_cls,\n            ""idx2labels_path"": idx2labels_path,\n            ""idx2cls_path"": idx2cls_path,\n            ""markup"": markup\n        }\n        if df is None and df_path is not None:\n            df = pd.read_csv(df_path, sep=\'\\t\')\n        elif df is None:\n            if is_cls:\n                df = pd.DataFrame(columns=[""labels"", ""text"", ""clf""])\n            else:\n                df = pd.DataFrame(columns=[""labels"", ""text""])\n        if clear_cache:\n            _ = cls.create_vocabs(\n                df, tokenizer, idx2labels_path, markup, idx2cls_path, pad_idx, is_cls, idx2labels, idx2cls)\n        self = cls(tokenizer, df=df, config=config, is_cls=is_cls)\n        self.load(df=df)\n        return self\n\n    @staticmethod\n    def create_vocabs(\n            df, tokenizer, idx2labels_path, markup=""IO"",\n            idx2cls_path=None, pad_idx=0, is_cls=False, idx2labels=None, idx2cls=None):\n        if idx2labels is None:\n            label2idx = {""[PAD]"": pad_idx, \'[CLS]\': 1, \'[SEP]\': 2, ""X"": 3}\n            idx2label = [""[PAD]"", \'[CLS]\', \'[SEP]\', ""X""]\n        else:\n            label2idx = {label: idx for idx, label in enumerate(idx2labels)}\n            idx2label = idx2labels\n        idx2cls = idx2cls\n        cls2idx = None\n        if is_cls:\n            idx2cls = []\n            cls2idx = {label: idx for idx, label in enumerate(idx2cls)}\n        for _, row in tqdm(df.iterrows(), total=len(df), leave=False, desc=""Creating labels vocabs""):\n            labels = row.labels.split()\n            origin_tokens = row.text.split()\n            if is_cls and row.cls not in cls2idx:\n                cls2idx[row.cls] = len(cls2idx)\n                idx2cls.append(row.cls)\n            prev_label = """"\n            for origin_token, label in zip(origin_tokens, labels):\n                if markup == ""BIO"":\n                    prefix = ""B_""\n                else:\n                    prefix = ""I_""\n                if label != ""O"":\n                    label = label.split(""_"")[1]\n                    if label == prev_label:\n                        prefix = ""I_""\n                    prev_label = label\n                else:\n                    prev_label = label\n                cur_tokens = tokenizer.tokenize(origin_token)\n                bert_label = [prefix + label] + [""X""] * (len(cur_tokens) - 1)\n                for label_ in bert_label:\n                    if label_ not in label2idx:\n                        label2idx[label_] = len(label2idx)\n                        idx2label.append(label_)\n        with open(idx2labels_path, ""w"", encoding=""utf-8"") as f:\n            for label in idx2label:\n                f.write(""{}\\n"".format(label))\n\n        if is_cls:\n            with open(idx2cls_path, ""w"", encoding=""utf-8"") as f:\n                for label in idx2cls:\n                    f.write(""{}\\n"".format(label))\n\n        return label2idx, idx2label, cls2idx, idx2cls\n\n    def load(self, df_path=None, df=None):\n        df_path = if_none(df_path, self.config[""df_path""])\n        if df is None:\n            self.df = pd.read_csv(df_path, sep=\'\\t\')\n        self.label2idx = {}\n        self.idx2label = []\n        with open(self.config[""idx2labels_path""], ""r"", encoding=""utf-8"") as f:\n            for idx, label in enumerate(f.readlines()):\n                label = label.strip()\n                self.label2idx[label] = idx\n                self.idx2label.append(label)\n\n        if self.config[""is_cls""]:\n            self.idx2cls = []\n            self.cls2idx = {}\n            with open(self.config[""idx2cls_path""], ""r"", encoding=""utf-8"") as f:\n                for idx, label in enumerate(f.readlines()):\n                    label = label.strip()\n                    self.cls2idx[label] = idx\n                    self.idx2cls.append(label)\n\n    def create_feature(self, row):\n        bert_tokens = []\n        bert_labels = []\n        orig_tokens = row.text.split()\n        origin_labels = row.labels.split()\n        tok_map = []\n        prev_label = """"\n        for orig_token, label in zip(orig_tokens, origin_labels):\n            cur_tokens = self.tokenizer.tokenize(orig_token)\n            if self.config[""max_sequence_length""] - 2 < len(bert_tokens) + len(cur_tokens):\n                break\n            if self.config[""markup""] == ""BIO"":\n                prefix = ""B_""\n            else:\n                prefix = ""I_""\n            if label != ""O"":\n                label = label.split(""_"")[1]\n                if label == prev_label:\n                    prefix = ""I_""\n                prev_label = label\n            else:\n                prev_label = label\n            cur_tokens = self.tokenizer.tokenize(orig_token)\n            bert_label = [prefix + label] + [""X""] * (len(cur_tokens) - 1)\n            tok_map.append(len(bert_tokens))\n            bert_tokens.extend(cur_tokens)\n            bert_labels.extend(bert_label)\n\n        orig_tokens = [""[CLS]""] + orig_tokens + [""[SEP]""]\n        bert_labels = [""[CLS]""] + bert_labels + [""[SEP]""]\n        if self.config[""markup""] == ""BIO"":\n            O_label = self.label2idx.get(""B_O"")\n        else:\n            O_label = self.label2idx.get(""I_O"")\n        input_ids = self.tokenizer.convert_tokens_to_ids([\'[CLS]\'] + bert_tokens + [\'[SEP]\'])\n        labels_ids = [self.label2idx.get(l, O_label) for l in bert_labels]\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1] * len(input_ids)\n        # Zero-pad up to the sequence length.\n        while len(input_ids) < self.config[""max_sequence_length""]:\n            input_ids.append(self.config[""pad_idx""])\n            labels_ids.append(self.config[""pad_idx""])\n            input_mask.append(0)\n            tok_map.append(-1)\n        input_type_ids = [0] * len(input_ids)\n        cls = None\n        id_cls = None\n        if self.is_cls:\n            cls = row.cls\n            try:\n                id_cls = self.cls2idx[cls]\n            except KeyError:\n                id_cls = self.cls2idx[str(cls)]\n        return InputFeature(\n            # Bert data\n            bert_tokens=bert_tokens,\n            input_ids=input_ids,\n            input_mask=input_mask,\n            input_type_ids=input_type_ids,\n            bert_labels=bert_labels, labels_ids=labels_ids, labels=origin_labels,\n            # Origin data\n            tokens=orig_tokens,\n            tok_map=tok_map,\n            # Cls\n            cls=cls, id_cls=id_cls\n        )\n\n    def __getitem__(self, item):\n        if self.config[""df_path""] is None and self.df is None:\n            raise ValueError(""Should setup df_path or df."")\n        if self.df is None:\n            self.load()\n\n        return self.create_feature(self.df.iloc[item])\n\n    def __len__(self):\n        return len(self.df) if self.df is not None else 0\n\n    def save(self, df_path=None):\n        df_path = if_none(df_path, self.config[""df_path""])\n        self.df.to_csv(df_path, sep=\'\\t\', index=False)\n\n    def __init__(\n            self, tokenizer,\n            df=None,\n            config=None,\n            idx2label=None,\n            idx2cls=None,\n            is_cls=False):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.config = config\n        self.idx2label = idx2label\n        self.label2idx = None\n        if idx2label is not None:\n            self.label2idx = {label: idx for idx, label in enumerate(idx2label)}\n\n        self.idx2cls = idx2cls\n\n        if idx2cls is not None:\n            self.cls2idx = {label: idx for idx, label in enumerate(idx2cls)}\n        self.is_cls = is_cls\n\n\nclass LearnData(object):\n    def __init__(self, train_ds=None, train_dl=None, valid_ds=None, valid_dl=None):\n        self.train_ds = train_ds\n        self.train_dl = train_dl\n        self.valid_ds = valid_ds\n        self.valid_dl = valid_dl\n\n    @classmethod\n    def create(cls,\n               # DataSet params\n               train_df_path,\n               valid_df_path,\n               idx2labels_path,\n               idx2labels=None,\n               idx2cls=None,\n               idx2cls_path=None,\n               min_char_len=1,\n               model_name=""bert-base-multilingual-cased"",\n               max_sequence_length=424,\n               pad_idx=0,\n               clear_cache=False,\n               is_cls=False,\n               markup=""IO"",\n               train_df=None,\n               valid_df=None,\n               # DataLoader params\n               device=""cuda"", batch_size=16):\n        train_ds = None\n        train_dl = None\n        valid_ds = None\n        valid_dl = None\n        if idx2labels_path is not None:\n            train_ds = TextDataSet.create(\n                idx2labels_path,\n                train_df_path,\n                idx2labels=idx2labels,\n                idx2cls=idx2cls,\n                idx2cls_path=idx2cls_path,\n                min_char_len=min_char_len,\n                model_name=model_name,\n                max_sequence_length=max_sequence_length,\n                pad_idx=pad_idx,\n                clear_cache=clear_cache,\n                is_cls=is_cls,\n                markup=markup,\n                df=train_df)\n            if len(train_ds):\n                train_dl = TextDataLoader(train_ds, device=device, shuffle=True, batch_size=batch_size)\n        if valid_df_path is not None:\n            valid_ds = TextDataSet.create(\n                idx2labels_path,\n                valid_df_path,\n                idx2labels=train_ds.idx2label,\n                idx2cls=train_ds.idx2cls,\n                idx2cls_path=idx2cls_path,\n                min_char_len=min_char_len,\n                model_name=model_name,\n                max_sequence_length=max_sequence_length,\n                pad_idx=pad_idx,\n                clear_cache=False,\n                is_cls=is_cls,\n                markup=markup,\n                df=valid_df, tokenizer=train_ds.tokenizer)\n            valid_dl = TextDataLoader(valid_ds, device=device, batch_size=batch_size)\n\n        self = cls(train_ds, train_dl, valid_ds, valid_dl)\n        self.device = device\n        self.batch_size = batch_size\n        return self\n\n    def load(self):\n        if self.train_ds is not None:\n            self.train_ds.load()\n        if self.valid_ds is not None:\n            self.valid_ds.load()\n\n    def save(self):\n        if self.train_ds is not None:\n            self.train_ds.save()\n        if self.valid_ds is not None:\n            self.valid_ds.save()\n\n\ndef get_data_loader_for_predict(data, df_path=None, df=None):\n    config = deepcopy(data.train_ds.config)\n    config[""df_path""] = df_path\n    config[""clear_cache""] = False\n    ds = TextDataSet.create(\n        idx2labels=data.train_ds.idx2label,\n        idx2cls=data.train_ds.idx2cls,\n        df=df, tokenizer=data.train_ds.tokenizer, **config)\n    return TextDataLoader(\n        ds, device=data.device, batch_size=data.batch_size, shuffle=False)\n'"
modules/data/bert_data_clf.py,0,"b'from .bert_data import TextDataLoader\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom modules.utils import read_config, if_none\nfrom modules import tqdm\nimport pandas as pd\nfrom copy import deepcopy\n\n\nclass InputFeature(object):\n    """"""A single set of features of data.""""""\n\n    def __init__(\n            self,\n            # Bert data\n            bert_tokens, input_ids, input_mask, input_type_ids,\n            # Origin data\n            tokens, tok_map,\n            # Cls data\n            cls=None, id_cls=None):\n        """"""\n        Data has the following structure.\n        data[0]: list, tokens ids\n        data[1]: list, tokens mask\n        data[2]: list, tokens type ids (for bert)\n        """"""\n        self.data = []\n        # Bert data\n        self.bert_tokens = bert_tokens\n        self.input_ids = input_ids\n        self.data.append(input_ids)\n        self.input_mask = input_mask\n        self.data.append(input_mask)\n        self.input_type_ids = input_type_ids\n        self.data.append(input_type_ids)\n        # Classification data\n        self.cls = cls\n        self.id_cls = id_cls\n        if cls is not None:\n            self.data.append(id_cls)\n        # Origin data\n        self.tokens = tokens\n        self.tok_map = tok_map\n\n    def __iter__(self):\n        return iter(self.data)\n\n\nclass TextDataSet(object):\n\n    @classmethod\n    def from_config(cls, config, clear_cache=False, df=None):\n        return cls.create(**read_config(config), clear_cache=clear_cache, df=df)\n\n    @classmethod\n    def create(cls,\n               df_path=None,\n               idx2cls=None,\n               idx2cls_path=None,\n               min_char_len=1,\n               model_name=""bert-base-multilingual-cased"",\n               max_sequence_length=424,\n               pad_idx=0,\n               clear_cache=False,\n               df=None, tokenizer=None):\n        if tokenizer is None:\n            tokenizer = BertTokenizer.from_pretrained(model_name)\n        config = {\n            ""min_char_len"": min_char_len,\n            ""model_name"": model_name,\n            ""max_sequence_length"": max_sequence_length,\n            ""clear_cache"": clear_cache,\n            ""df_path"": df_path,\n            ""pad_idx"": pad_idx,\n            ""idx2cls_path"": idx2cls_path\n        }\n        if df is None and df_path is not None:\n            df = pd.read_csv(df_path, sep=\'\\t\', engine=\'python\')\n        elif df is None:\n            df = pd.DataFrame(columns=[""text"", ""clf""])\n        if clear_cache:\n            _, idx2cls = cls.create_vocabs(df, idx2cls_path, idx2cls)\n        self = cls(tokenizer, df=df, config=config, idx2cls=idx2cls)\n        self.load(df=df)\n        return self\n\n    @staticmethod\n    def create_vocabs(\n            df, idx2cls_path, idx2cls=None):\n        idx2cls = idx2cls\n        cls2idx = {}\n        if idx2cls is not None:\n            cls2idx = {label: idx for idx, label in enumerate(idx2cls)}\n        else:\n            idx2cls = []\n        for _, row in tqdm(df.iterrows(), total=len(df), leave=False, desc=""Creating labels vocabs""):\n            if row.cls not in cls2idx:\n                cls2idx[row.cls] = len(cls2idx)\n                idx2cls.append(row.cls)\n\n        with open(idx2cls_path, ""w"", encoding=""utf-8"") as f:\n            for label in idx2cls:\n                f.write(""{}\\n"".format(label))\n\n        return cls2idx, idx2cls\n\n    def load(self, df_path=None, df=None):\n        df_path = if_none(df_path, self.config[""df_path""])\n        if df is None:\n            self.df = pd.read_csv(df_path, sep=\'\\t\')\n\n        self.idx2cls = []\n        self.cls2idx = {}\n        with open(self.config[""idx2cls_path""], ""r"", encoding=""utf-8"") as f:\n            for idx, label in enumerate(f.readlines()):\n                label = label.strip()\n                self.cls2idx[label] = idx\n                self.idx2cls.append(label)\n\n    def create_feature(self, row):\n        bert_tokens = []\n        orig_tokens = row.text.split()\n        tok_map = []\n        for orig_token in orig_tokens:\n            cur_tokens = self.tokenizer.tokenize(orig_token)\n            if self.config[""max_sequence_length""] - 2 < len(bert_tokens) + len(cur_tokens):\n                break\n            cur_tokens = self.tokenizer.tokenize(orig_token)\n            tok_map.append(len(bert_tokens))\n            bert_tokens.extend(cur_tokens)\n\n        orig_tokens = [""[CLS]""] + orig_tokens + [""[SEP]""]\n\n        input_ids = self.tokenizer.convert_tokens_to_ids([\'[CLS]\'] + bert_tokens + [\'[SEP]\'])\n        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n        # tokens are attended to.\n        input_mask = [1] * len(input_ids)\n        # Zero-pad up to the sequence length.\n        while len(input_ids) < self.config[""max_sequence_length""]:\n            input_ids.append(self.config[""pad_idx""])\n            input_mask.append(0)\n            tok_map.append(-1)\n        input_type_ids = [0] * len(input_ids)\n        cls = str(row.cls)\n        id_cls = self.cls2idx[cls]\n        return InputFeature(\n            # Bert data\n            bert_tokens=bert_tokens,\n            input_ids=input_ids,\n            input_mask=input_mask,\n            input_type_ids=input_type_ids,\n            # Origin data\n            tokens=orig_tokens,\n            tok_map=tok_map,\n            # Cls\n            cls=cls, id_cls=id_cls\n        )\n\n    def __getitem__(self, item):\n        if self.config[""df_path""] is None and self.df is None:\n            raise ValueError(""Should setup df_path or df."")\n        if self.df is None:\n            self.load()\n\n        return self.create_feature(self.df.iloc[item])\n\n    def __len__(self):\n        return len(self.df) if self.df is not None else 0\n\n    def save(self, df_path=None):\n        df_path = if_none(df_path, self.config[""df_path""])\n        self.df.to_csv(df_path, sep=\'\\t\', index=False)\n\n    def __init__(\n            self, tokenizer,\n            df=None,\n            config=None,\n            idx2cls=None):\n        self.df = df\n        self.tokenizer = tokenizer\n        self.config = config\n        self.label2idx = None\n\n        self.idx2cls = idx2cls\n        if idx2cls is not None:\n            self.cls2idx = {label: idx for idx, label in enumerate(idx2cls)}\n\n\nclass LearnDataClass(object):\n    def __init__(self, train_ds=None, train_dl=None, valid_ds=None, valid_dl=None):\n        self.train_ds = train_ds\n        self.train_dl = train_dl\n        self.valid_ds = valid_ds\n        self.valid_dl = valid_dl\n\n    @classmethod\n    def create(cls,\n               # DataSet params\n               train_df_path,\n               valid_df_path,\n               idx2cls=None,\n               idx2cls_path=None,\n               min_char_len=1,\n               model_name=""bert-base-multilingual-cased"",\n               max_sequence_length=424,\n               pad_idx=0,\n               clear_cache=False,\n               train_df=None,\n               valid_df=None,\n               # DataLoader params\n               device=""cuda"", batch_size=16):\n        train_ds = None\n        train_dl = None\n        valid_ds = None\n        valid_dl = None\n        if idx2cls_path is not None:\n            train_ds = TextDataSet.create(\n                train_df_path,\n                idx2cls=idx2cls,\n                idx2cls_path=idx2cls_path,\n                min_char_len=min_char_len,\n                model_name=model_name,\n                max_sequence_length=max_sequence_length,\n                pad_idx=pad_idx,\n                clear_cache=clear_cache,\n                df=train_df)\n            if len(train_ds):\n                train_dl = TextDataLoader(train_ds, device=device, shuffle=True, batch_size=batch_size)\n        if valid_df_path is not None:\n            valid_ds = TextDataSet.create(\n                valid_df_path,\n                idx2cls=train_ds.idx2cls,\n                idx2cls_path=idx2cls_path,\n                min_char_len=min_char_len,\n                model_name=model_name,\n                max_sequence_length=max_sequence_length,\n                pad_idx=pad_idx,\n                clear_cache=False,\n                df=valid_df, tokenizer=train_ds.tokenizer)\n            valid_dl = TextDataLoader(valid_ds, device=device, batch_size=batch_size)\n\n        self = cls(train_ds, train_dl, valid_ds, valid_dl)\n        self.device = device\n        self.batch_size = batch_size\n        return self\n\n    def load(self):\n        if self.train_ds is not None:\n            self.train_ds.load()\n        if self.valid_ds is not None:\n            self.valid_ds.load()\n\n    def save(self):\n        if self.train_ds is not None:\n            self.train_ds.save()\n        if self.valid_ds is not None:\n            self.valid_ds.save()\n\n\ndef get_data_loader_for_predict(data, df_path=None, df=None):\n    config = deepcopy(data.train_ds.config)\n    config[""df_path""] = df_path\n    config[""clear_cache""] = False\n    ds = TextDataSet.create(\n        idx2cls=data.train_ds.idx2cls,\n        df=df, tokenizer=data.train_ds.tokenizer, **config)\n    return TextDataLoader(\n        ds, device=data.device, batch_size=data.batch_size, shuffle=False), ds\n'"
modules/data/download_data.py,0,"b'import urllib\nimport sys\nimport os\n\n\ntasks_urls = {\n    ""conll2003"": [\n        [""eng.testa"", ""https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testa""],\n        [""eng.testb"", ""https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testb""],\n        [""eng.train"", ""https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.train""]\n    ]}\n\n\ndef download_data(task_name, data_dir):\n    req = urllib\n    if sys.version_info >= (3, 0):\n        req = urllib.request\n    for data_file, url in tasks_urls[task_name]:\n        if not os.path.exists(data_dir):\n            os.mkdir(data_dir)\n        _ = req.urlretrieve(url, os.path.join(data_dir, data_file))\n'"
modules/layers/__init__.py,0,b''
modules/layers/crf.py,11,"b'import torch\nfrom torch import nn\n\n\n# TODO: move to utils\ndef log_sum_exp(tensor, dim=0):\n    """"""LogSumExp operation.""""""\n    m, _ = torch.max(tensor, dim)\n    m_exp = m.unsqueeze(-1).expand_as(tensor)\n    return m + torch.log(torch.sum(torch.exp(tensor - m_exp), dim))\n\n\ndef sequence_mask(lens, max_len=None):\n    batch_size = lens.size(0)\n\n    if max_len is None:\n        max_len = lens.max().item()\n\n    ranges = torch.arange(0, max_len).long()\n    ranges = ranges.unsqueeze(0).expand(batch_size, max_len)\n\n    if lens.data.is_cuda:\n        ranges = ranges.cuda()\n\n    lens_exp = lens.unsqueeze(1).expand_as(ranges)\n    mask = ranges < lens_exp\n\n    return mask\n\n\nclass CRF(nn.Module):\n    def forward(self, *input_):\n        return self.viterbi_decode(*input_)\n\n    def __init__(self, label_size):\n        super(CRF, self).__init__()\n\n        self.label_size = label_size\n        self.start = self.label_size - 2\n        self.end = self.label_size - 1\n        transition = torch.randn(self.label_size, self.label_size)\n        self.transition = nn.Parameter(transition)\n        self.initialize()\n\n    def initialize(self):\n        self.transition.data[:, self.end] = -100.0\n        self.transition.data[self.start, :] = -100.0\n\n    @staticmethod\n    def pad_logits(logits):\n        # lens = lens.data\n        batch_size, seq_len, label_num = logits.size()\n        # pads = Variable(logits.data.new(batch_size, seq_len, 2).fill_(-1000.0),\n        #                 requires_grad=False)\n        pads = logits.new_full((batch_size, seq_len, 2), -1000.0,\n                               requires_grad=False)\n        logits = torch.cat([logits, pads], dim=2)\n        return logits\n\n    def calc_binary_score(self, labels, lens):\n        batch_size, seq_len = labels.size()\n\n        # labels_ext = Variable(labels.data.new(batch_size, seq_len + 2))\n        labels_ext = labels.new_empty((batch_size, seq_len + 2))\n        labels_ext[:, 0] = self.start\n        labels_ext[:, 1:-1] = labels\n        mask = sequence_mask(lens + 1, max_len=(seq_len + 2)).long()\n        # pad_stop = Variable(labels.data.new(1).fill_(self.end))\n        pad_stop = labels.new_full((1,), self.end, requires_grad=False)\n        pad_stop = pad_stop.unsqueeze(-1).expand(batch_size, seq_len + 2)\n        labels_ext = (1 - mask) * pad_stop + mask * labels_ext\n        labels = labels_ext\n\n        trn = self.transition\n        trn_exp = trn.unsqueeze(0).expand(batch_size, *trn.size())\n        lbl_r = labels[:, 1:]\n        lbl_rexp = lbl_r.unsqueeze(-1).expand(*lbl_r.size(), trn.size(0))\n        trn_row = torch.gather(trn_exp, 1, lbl_rexp)\n\n        lbl_lexp = labels[:, :-1].unsqueeze(-1)\n        trn_scr = torch.gather(trn_row, 2, lbl_lexp)\n        trn_scr = trn_scr.squeeze(-1)\n\n        mask = sequence_mask(lens + 1).float()\n        trn_scr = trn_scr * mask\n        score = trn_scr\n\n        return score\n\n    @staticmethod\n    def calc_unary_score(logits, labels, lens):\n        labels_exp = labels.unsqueeze(-1)\n        scores = torch.gather(logits, 2, labels_exp).squeeze(-1)\n        mask = sequence_mask(lens).float()\n        scores = scores * mask\n        return scores\n\n    def calc_gold_score(self, logits, labels, lens):\n        unary_score = self.calc_unary_score(logits, labels, lens).sum(\n            1).squeeze(-1)\n        binary_score = self.calc_binary_score(labels, lens).sum(1).squeeze(-1)\n        return unary_score + binary_score\n\n    def calc_norm_score(self, logits, lens):\n        batch_size, seq_len, feat_dim = logits.size()\n        # alpha = logits.data.new(batch_size, self.label_size).fill_(-10000.0)\n        alpha = logits.new_full((batch_size, self.label_size), -100.0)\n        alpha[:, self.start] = 0\n        # alpha = Variable(alpha)\n        lens_ = lens.clone()\n\n        logits_t = logits.transpose(1, 0)\n        for logit in logits_t:\n            logit_exp = logit.unsqueeze(-1).expand(batch_size,\n                                                   *self.transition.size())\n            alpha_exp = alpha.unsqueeze(1).expand(batch_size,\n                                                  *self.transition.size())\n            trans_exp = self.transition.unsqueeze(0).expand_as(alpha_exp)\n            mat = logit_exp + alpha_exp + trans_exp\n            alpha_nxt = log_sum_exp(mat, 2).squeeze(-1)\n\n            mask = (lens_ > 0).float().unsqueeze(-1).expand_as(alpha)\n            alpha = mask * alpha_nxt + (1 - mask) * alpha\n            lens_ = lens_ - 1\n\n        alpha = alpha + self.transition[self.end].unsqueeze(0).expand_as(alpha)\n        norm = log_sum_exp(alpha, 1).squeeze(-1)\n\n        return norm\n\n    def viterbi_decode(self, logits, lens):\n        """"""Borrowed from pytorch tutorial\n        Arguments:\n            logits: [batch_size, seq_len, n_labels] FloatTensor\n            lens: [batch_size] LongTensor\n        """"""\n        batch_size, seq_len, n_labels = logits.size()\n        # vit = logits.data.new(batch_size, self.label_size).fill_(-10000)\n        vit = logits.new_full((batch_size, self.label_size), -100.0)\n        vit[:, self.start] = 0\n        # vit = Variable(vit)\n        c_lens = lens.clone()\n\n        logits_t = logits.transpose(1, 0)\n        pointers = []\n        for logit in logits_t:\n            vit_exp = vit.unsqueeze(1).expand(batch_size, n_labels, n_labels)\n            trn_exp = self.transition.unsqueeze(0).expand_as(vit_exp)\n            vit_trn_sum = vit_exp + trn_exp\n            vt_max, vt_argmax = vit_trn_sum.max(2)\n\n            vt_max = vt_max.squeeze(-1)\n            vit_nxt = vt_max + logit\n            pointers.append(vt_argmax.squeeze(-1).unsqueeze(0))\n\n            mask = (c_lens > 0).float().unsqueeze(-1).expand_as(vit_nxt)\n            vit = mask * vit_nxt + (1 - mask) * vit\n\n            mask = (c_lens == 1).float().unsqueeze(-1).expand_as(vit_nxt)\n            vit += mask * self.transition[self.end].unsqueeze(\n                0).expand_as(vit_nxt)\n\n            c_lens = c_lens - 1\n\n        pointers = torch.cat(pointers)\n        scores, idx = vit.max(1)\n        # idx = idx.squeeze(-1)\n        paths = [idx.unsqueeze(1)]\n        for argmax in reversed(pointers):\n            idx_exp = idx.unsqueeze(-1)\n            idx = torch.gather(argmax, 1, idx_exp)\n            idx = idx.squeeze(-1)\n\n            paths.insert(0, idx.unsqueeze(1))\n\n        paths = torch.cat(paths[1:], 1)\n        scores = scores.squeeze(-1)\n\n        return scores, paths\n'"
modules/layers/decoders.py,16,"b'import torch\nfrom torch.nn import functional\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom .layers import Linears\nfrom .crf import CRF\nfrom .ncrf import NCRF\n\n\nclass CRFDecoder(nn.Module):\n    def __init__(self, crf, label_size, input_dim, input_dropout=0.5):\n        super(CRFDecoder, self).__init__()\n        self.input_dim = input_dim\n        self.input_dropout = nn.Dropout(p=input_dropout)\n        self.linear = Linears(in_features=input_dim,\n                              out_features=label_size,\n                              hiddens=[input_dim // 2])\n        self.crf = crf\n        self.label_size = label_size\n\n    def forward_model(self, inputs):\n        batch_size, seq_len, input_dim = inputs.size()\n        output = inputs.contiguous().view(-1, self.input_dim)\n        output = self.input_dropout(output)\n        # Fully-connected layer\n        output = self.linear.forward(output)\n        output = output.view(batch_size, seq_len, self.label_size)\n        return output\n\n    def forward(self, inputs, labels_mask):\n        lens = labels_mask.sum(-1)\n        logits = self.forward_model(inputs)\n        logits = self.crf.pad_logits(logits)\n        scores, preds = self.crf.viterbi_decode(logits, lens)\n        return preds\n\n    def score(self, inputs, labels_mask, labels):\n        lens = labels_mask.sum(-1)\n        logits = self.forward_model(inputs)\n        logits = self.crf.pad_logits(logits)\n        norm_score = self.crf.calc_norm_score(logits, lens)\n        gold_score = self.crf.calc_gold_score(logits, labels, lens)\n        loglik = gold_score - norm_score\n        return -loglik.mean()\n\n    @classmethod\n    def create(cls, label_size, input_dim, input_dropout=0.5):\n        return cls(CRF(label_size + 2), label_size, input_dim, input_dropout)\n\n\nclass NMTDecoder(nn.Module):\n    def __init__(self,\n                 label_size,\n                 embedding_dim=64, hidden_dim=256, rnn_layers=1,\n                 dropout_p=0.1, pad_idx=0):\n        super(NMTDecoder, self).__init__()\n        self.slot_size = label_size\n        self.pad_idx = pad_idx\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.rnn_layers = rnn_layers\n        self.dropout_p = dropout_p\n        self.embedding = nn.Embedding(self.slot_size, self.embedding_dim)\n        self.lstm = nn.LSTM(self.embedding_dim + self.hidden_dim * 2,\n                            self.hidden_dim, self.rnn_layers,\n                            batch_first=True)\n        self.attn = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.slot_out = nn.Linear(self.hidden_dim * 2, self.slot_size)\n\n        self.loss = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\n        self.init_weights()\n\n    def init_weights(self):\n        nn.init.xavier_normal(self.embedding.weight)\n        nn.init.xavier_normal(self.attn.weight)\n        nn.init.xavier_normal(self.slot_out.weight)\n\n    def attention(self, hidden, encoder_outputs, input_mask):\n        """"""\n        hidden : 1,B,D\n        encoder_outputs : B,T,D\n        input_mask : B,T # ByteTensor\n        """"""\n        input_mask = input_mask == 0\n        hidden = hidden.squeeze(0).unsqueeze(2)\n\n        # B\n        batch_size = encoder_outputs.size(0)\n        # T\n        max_len = encoder_outputs.size(1)\n        # B*T,D -> B*T,D\n        energies = self.attn(encoder_outputs.contiguous().view(batch_size * max_len, -1))\n        energies = energies.view(batch_size, max_len, -1)\n        # B,T,D * B,D,1 --> B,1,T\n        attn_energies = energies.bmm(hidden).transpose(1, 2)\n        # PAD masking\n        attn_energies = attn_energies.squeeze(1).masked_fill(input_mask, -1e12)\n\n        # B,T\n        alpha = functional.softmax(attn_energies)\n        # B,1,T\n        alpha = alpha.unsqueeze(1)\n        # B,1,T * B,T,D => B,1,D\n        context = alpha.bmm(encoder_outputs)\n        # B,1,D\n        return context\n\n    def forward_model(self, encoder_outputs, input_mask):\n        real_context = []\n\n        for idx, o in enumerate(encoder_outputs):\n            real_length = input_mask[idx].sum().cpu().data.tolist()\n            real_context.append(o[real_length - 1])\n        context = torch.cat(real_context).view(encoder_outputs.size(0), -1).unsqueeze(1)\n\n        batch_size = encoder_outputs.size(0)\n\n        input_mask = input_mask == 0\n        # Get the embedding of the current input word\n\n        embedded = Variable(torch.zeros(batch_size, self.embedding_dim))\n        if self.use_cuda:\n            embedded = embedded.cuda()\n        embedded = embedded.unsqueeze(1)\n        decode = []\n        aligns = encoder_outputs.transpose(0, 1)\n        length = encoder_outputs.size(1)\n        for i in range(length):\n            # B,1,D\n            aligned = aligns[i].unsqueeze(1)\n            # input, context, aligned encoder hidden, hidden\n            _, hidden = self.lstm(torch.cat((embedded, context, aligned), 2))\n\n            # print(hidden[0].shape, context.transpose(0, 1).shape)\n\n            concated = torch.cat((hidden[0], context.transpose(0, 1)), 2)\n            score = self.slot_out(concated.squeeze(0))\n            softmaxed = functional.log_softmax(score)\n            decode.append(softmaxed)\n            _, input = torch.max(softmaxed, 1)\n            embedded = self.embedding(input.unsqueeze(1))\n\n            context = self.attention(hidden[0], encoder_outputs, input_mask)\n        slot_scores = torch.cat(decode, 1)\n\n        return slot_scores.view(batch_size, length, -1)\n\n    def forward(self, encoder_outputs, input_mask):\n        scores = self.forward_model(encoder_outputs, input_mask)\n        return scores.argmax(-1)\n\n    def score(self, encoder_outputs, input_mask, labels_ids):\n        scores = self.forward_model(encoder_outputs, input_mask)\n        batch_size = encoder_outputs.shape[0]\n        len_ = encoder_outputs.shape[1]\n        return self.loss(scores.view(batch_size * len_, -1), labels_ids.view(-1))\n\n    @classmethod\n    def create(cls, label_size,\n               embedding_dim=64, hidden_dim=256, rnn_layers=1, dropout_p=0.1, pad_idx=0):\n        return cls(label_size=label_size,\n                   embedding_dim=embedding_dim, hidden_dim=hidden_dim,\n                   rnn_layers=rnn_layers, dropout_p=dropout_p, pad_idx=pad_idx)\n\n\nclass PoolingLinearClassifier(nn.Module):\n    """"""Create a linear classifier with pooling.""""""\n\n    def __init__(self, input_dim, intent_size, input_dropout=0.5):\n        super(PoolingLinearClassifier, self).__init__()\n        self.input_dim = input_dim\n        self.intent_size = intent_size\n        self.input_dropout = input_dropout\n        self.dropout = nn.Dropout(p=input_dropout)\n        self.linear = Linears(input_dim * 3, intent_size, [input_dim // 2], activation=""relu"")\n\n    @staticmethod\n    def pool(x, bs, is_max):\n        """"""Pool the tensor along the seq_len dimension.""""""\n        f = functional.adaptive_max_pool1d if is_max else functional.adaptive_avg_pool1d\n        return f(x.permute(1, 2, 0), (1,)).view(bs, -1)\n\n    def forward(self, output):\n        output = self.dropout(output).transpose(0, 1)\n        sl, bs, _ = output.size()\n        avgpool = self.pool(output, bs, False)\n        mxpool = self.pool(output, bs, True)\n        x = torch.cat([output[-1], mxpool, avgpool], 1)\n        return self.linear(x)\n\n\nclass NMTJointDecoder(nn.Module):\n    def __init__(self,\n                 label_size, intent_size,\n                 embedding_dim=64, hidden_dim=256, rnn_layers=1,\n                 dropout_p=0.1, pad_idx=0):\n        super(NMTJointDecoder, self).__init__()\n        self.slot_size = label_size\n        self.intent_size = intent_size\n        self.pad_idx = pad_idx\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.rnn_layers = rnn_layers\n        self.dropout_p = dropout_p\n        self.embedding = nn.Embedding(self.slot_size, self.embedding_dim)\n        self.lstm = nn.LSTM(self.embedding_dim + self.hidden_dim * 2,\n                            self.hidden_dim, self.rnn_layers,\n                            batch_first=True)\n        self.attn = nn.Linear(self.hidden_dim, self.hidden_dim)\n        self.slot_out = nn.Linear(self.hidden_dim * 2, self.slot_size)\n\n        self.loss = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\n        self.intent_loss = nn.CrossEntropyLoss()\n        self.intent_out = Linears(\n            in_features=self.hidden_dim * 2,\n            out_features=self.intent_size,\n            hiddens=[hidden_dim // 2],\n            activation=""relu"")\n\n        self.init_weights()\n\n    def init_weights(self):\n        nn.init.xavier_normal(self.embedding.weight)\n        nn.init.xavier_normal(self.attn.weight)\n        nn.init.xavier_normal(self.slot_out.weight)\n\n    def attention(self, hidden, encoder_outputs, input_mask):\n        """"""\n        hidden : 1,B,D\n        encoder_outputs : B,T,D\n        input_mask : B,T # ByteTensor\n        """"""\n        input_mask = input_mask == 0\n        hidden = hidden.squeeze(0).unsqueeze(2)\n\n        # B\n        batch_size = encoder_outputs.size(0)\n        # T\n        max_len = encoder_outputs.size(1)\n        # B*T,D -> B*T,D\n        energies = self.attn(encoder_outputs.contiguous().view(batch_size * max_len, -1))\n        energies = energies.view(batch_size, max_len, -1)\n        # B,T,D * B,D,1 --> B,1,T\n        attn_energies = energies.bmm(hidden).transpose(1, 2)\n        # PAD masking\n        attn_energies = attn_energies.squeeze(1).masked_fill(input_mask, -1e12)\n\n        # B,T\n        alpha = functional.softmax(attn_energies)\n        # B,1,T\n        alpha = alpha.unsqueeze(1)\n        # B,1,T * B,T,D => B,1,D\n        context = alpha.bmm(encoder_outputs)\n        # B,1,D\n        return context\n\n    def forward_model(self, encoder_outputs, input_mask):\n        real_context = []\n\n        for idx, o in enumerate(encoder_outputs):\n            real_length = input_mask[idx].sum().cpu().data.tolist()\n            real_context.append(o[real_length - 1])\n        context = torch.cat(real_context).view(encoder_outputs.size(0), -1).unsqueeze(1)\n\n        batch_size = encoder_outputs.size(0)\n\n        input_mask = input_mask == 0\n        # Get the embedding of the current input word\n\n        embedded = Variable(torch.zeros(batch_size, self.embedding_dim))\n        if self.use_cuda:\n            embedded = embedded.cuda()\n        embedded = embedded.unsqueeze(1)\n        decode = []\n        aligns = encoder_outputs.transpose(0, 1)\n        length = encoder_outputs.size(1)\n        intent_score = None\n        for i in range(length):\n            # B,1,D\n            aligned = aligns[i].unsqueeze(1)\n            # input, context, aligned encoder hidden, hidden\n            _, hidden = self.lstm(torch.cat((embedded, context, aligned), 2))\n\n            # for Intent Detection\n            if i == 0:\n                intent_hidden = hidden[0].clone()\n                # 1,B,D\n                intent_context = self.attention(intent_hidden, encoder_outputs, input_mask)\n                concated = torch.cat((intent_hidden, intent_context.transpose(0, 1)), 2)\n                # B,D\n                intent_score = self.intent_out(concated.squeeze(0))\n\n            concated = torch.cat((hidden[0], context.transpose(0, 1)), 2)\n            score = self.slot_out(concated.squeeze(0))\n            softmaxed = functional.log_softmax(score)\n            decode.append(softmaxed)\n            _, input = torch.max(softmaxed, 1)\n            embedded = self.embedding(input.unsqueeze(1))\n\n            context = self.attention(hidden[0], encoder_outputs, input_mask)\n        slot_scores = torch.cat(decode, 1)\n\n        return slot_scores.view(batch_size, length, -1), intent_score\n\n    def forward(self, encoder_outputs, input_mask):\n        scores, intent_score = self.forward_model(encoder_outputs, input_mask)\n        return scores.argmax(-1), intent_score.argmax(-1)\n\n    def score(self, encoder_outputs, input_mask, labels_ids, cls_ids):\n        scores, intent_score = self.forward_model(encoder_outputs, input_mask)\n        batch_size = encoder_outputs.shape[0]\n        len_ = encoder_outputs.shape[1]\n        return self.loss(scores.view(batch_size * len_, -1), labels_ids.view(-1)) + self.intent_loss(\n            intent_score, cls_ids)\n\n    @classmethod\n    def create(cls, label_size, intent_size,\n               embedding_dim=64, hidden_dim=256, rnn_layers=1, dropout_p=0.1, pad_idx=0):\n        return cls(label_size=label_size, intent_size=intent_size,\n                   embedding_dim=embedding_dim, hidden_dim=hidden_dim,\n                   rnn_layers=rnn_layers, dropout_p=dropout_p, pad_idx=pad_idx)\n\n\nclass NCRFDecoder(nn.Module):\n\n    def __init__(self,\n                 crf, label_size, input_dim, input_dropout=0.5, nbest=8):\n        super(NCRFDecoder, self).__init__()\n        self.input_dim = input_dim\n        self.dropout = nn.Dropout(input_dropout)\n        self.linear = Linears(in_features=input_dim,\n                              out_features=label_size,\n                              hiddens=[input_dim // 2])\n        self.nbest = nbest\n        self.crf = crf\n        self.label_size = label_size\n\n    def forward_model(self, inputs):\n        batch_size, seq_len, input_dim = inputs.size()\n        inputs = self.dropout(inputs)\n\n        output = inputs.contiguous().view(-1, self.input_dim)\n        # Fully-connected layer\n        output = self.linear.forward(output)\n        output = output.view(batch_size, seq_len, self.label_size)\n        return output\n\n    def forward(self, inputs, labels_mask):\n        logits = self.forward_model(inputs)\n        _, preds = self.crf._viterbi_decode_nbest(logits, labels_mask, self.nbest)\n        preds = preds[:, :, 0]\n        return preds\n\n    def score(self, inputs, labels_mask, labels):\n        logits = self.forward_model(inputs)\n        crf_score = self.crf.neg_log_likelihood_loss(logits, labels_mask, labels) / logits.size(0)\n        return crf_score\n\n    @classmethod\n    def from_config(cls, config):\n        return cls.create(**config)\n\n    @classmethod\n    def create(cls, label_size, input_dim, input_dropout=0.5, nbest=8, device=""cuda""):\n        return cls(NCRF(label_size, device), label_size + 2, input_dim, input_dropout, nbest)\n\n\nclass ClassDecoder(nn.Module):\n\n    def __init__(self, intent_size, input_dim, input_dropout=0.3):\n        super(ClassDecoder, self).__init__()\n        self.intent_loss = nn.CrossEntropyLoss()\n        self.intent_size = intent_size\n        self.input_dropout = input_dropout\n        self.input_dim = input_dim\n        self.intent_out = PoolingLinearClassifier(input_dim, intent_size, input_dropout)\n\n    def forward(self, inputs):\n        return self.intent_out(inputs).argmax(-1)\n\n    def score(self, inputs, cls_ids):\n        return self.intent_loss(self.intent_out(inputs), cls_ids)\n'"
modules/layers/embedders.py,7,"b'from pytorch_pretrained_bert import BertModel\nimport torch\n\n\nclass BERTEmbedder(torch.nn.Module):\n    def __init__(self, model, config):\n        super(BERTEmbedder, self).__init__()\n        self.config = config\n        self.model = model\n        if self.config[""mode""] == ""weighted"":\n            self.bert_weights = torch.nn.Parameter(torch.FloatTensor(12, 1))\n            self.bert_gamma = torch.nn.Parameter(torch.FloatTensor(1, 1))\n        self.init_weights()\n\n    def init_weights(self):\n        if self.config[""mode""] == ""weighted"":\n            torch.nn.init.xavier_normal(self.bert_gamma)\n            torch.nn.init.xavier_normal(self.bert_weights)\n\n    @classmethod\n    def create(\n            cls, model_name=\'bert-base-multilingual-cased\',\n            device=""cuda"", mode=""weighted"",\n            is_freeze=True):\n        config = {\n            ""model_name"": model_name,\n            ""device"": device,\n            ""mode"": mode,\n            ""is_freeze"": is_freeze\n        }\n        model = BertModel.from_pretrained(model_name)\n        model.to(device)\n        model.train()\n        self = cls(model, config)\n        if is_freeze:\n            self.freeze()\n        return self\n\n    @classmethod\n    def from_config(cls, config):\n        return cls.create(**config)\n\n    def forward(self, batch):\n        """"""\n        batch has the following structure:\n            data[0]: list, tokens ids\n            data[1]: list, tokens mask\n            data[2]: list, tokens type ids (for bert)\n            data[3]: list, bert labels ids\n        """"""\n        encoded_layers, _ = self.model(\n            input_ids=batch[0],\n            token_type_ids=batch[2],\n            attention_mask=batch[1],\n            output_all_encoded_layers=self.config[""mode""] == ""weighted"")\n        if self.config[""mode""] == ""weighted"":\n            encoded_layers = torch.stack([a * b for a, b in zip(encoded_layers, self.bert_weights)])\n            return self.bert_gamma * torch.sum(encoded_layers, dim=0)\n        return encoded_layers\n\n    def freeze(self):\n        for param in self.model.parameters():\n            param.requires_grad = False\n'"
modules/layers/layers.py,20,"b'from torch.nn import functional\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import init\nfrom torch.nn.utils import rnn as rnn_utils\nimport math\n\n\nclass BiLSTM(nn.Module):\n\n    def __init__(self, embedding_size=768, hidden_dim=512, rnn_layers=1, dropout=0.5):\n        super(BiLSTM, self).__init__()\n        self.embedding_size = embedding_size\n        self.hidden_dim = hidden_dim\n        self.rnn_layers = rnn_layers\n        self.dropout = nn.Dropout(dropout)\n        self.lstm = nn.LSTM(\n            embedding_size,\n            hidden_dim // 2,\n            rnn_layers, batch_first=True, bidirectional=True)\n\n    def forward(self, input_, input_mask):\n        length = input_mask.sum(-1)\n        sorted_lengths, sorted_idx = torch.sort(length, descending=True)\n        input_ = input_[sorted_idx]\n        packed_input = rnn_utils.pack_padded_sequence(input_, sorted_lengths.data.tolist(), batch_first=True)\n        output, (hidden, _) = self.lstm(packed_input)\n        padded_outputs = rnn_utils.pad_packed_sequence(output, batch_first=True)[0]\n        _, reversed_idx = torch.sort(sorted_idx)\n        return padded_outputs[reversed_idx], hidden[:, reversed_idx]\n\n    @classmethod\n    def create(cls, *args, **kwargs):\n        return cls(*args, **kwargs)\n\n\nclass Linear(nn.Linear):\n    def __init__(self,\n                 in_features: int,\n                 out_features: int,\n                 bias: bool = True):\n        super(Linear, self).__init__(in_features, out_features, bias=bias)\n        init.orthogonal_(self.weight)\n\n\nclass Linears(nn.Module):\n    def __init__(self,\n                 in_features,\n                 out_features,\n                 hiddens,\n                 bias=True,\n                 activation=\'tanh\'):\n        super(Linears, self).__init__()\n        assert len(hiddens) > 0\n\n        self.in_features = in_features\n        self.out_features = self.output_size = out_features\n\n        in_dims = [in_features] + hiddens[:-1]\n        self.linears = nn.ModuleList([Linear(in_dim, out_dim, bias=bias)\n                                      for in_dim, out_dim\n                                      in zip(in_dims, hiddens)])\n        self.output_linear = Linear(hiddens[-1], out_features, bias=bias)\n        self.activation = getattr(functional, activation)\n\n    def forward(self, inputs):\n        linear_outputs = inputs\n        for linear in self.linears:\n            linear_outputs = linear.forward(linear_outputs)\n            linear_outputs = self.activation(linear_outputs)\n        return self.output_linear.forward(linear_outputs)\n\n\n# Reused from https://github.com/JayParks/transformer/\nclass ScaledDotProductAttention(nn.Module):\n    def __init__(self, d_k, dropout=.1):\n        super(ScaledDotProductAttention, self).__init__()\n        self.scale_factor = np.sqrt(d_k)\n        self.softmax = nn.Softmax(dim=-1)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, q, k, v, attn_mask=None):\n        # q: [b_size x len_q x d_k]\n        # k: [b_size x len_k x d_k]\n        # v: [b_size x len_v x d_v] note: (len_k == len_v)\n        attn = torch.bmm(q, k.transpose(1, 2)) / self.scale_factor  # attn: [b_size x len_q x len_k]\n        if attn_mask is not None:\n            print(attn_mask.size(), attn.size())\n            assert attn_mask.size() == attn.size()\n            attn.data.masked_fill_(attn_mask, -float(\'inf\'))\n\n        attn = self.softmax(attn)\n        attn = self.dropout(attn)\n        outputs = torch.bmm(attn, v) # outputs: [b_size x len_q x d_v]\n\n        return outputs, attn\n\n\nclass LayerNormalization(nn.Module):\n    def __init__(self, d_hid, eps=1e-3):\n        super(LayerNormalization, self).__init__()\n        self.gamma = nn.Parameter(torch.ones(d_hid), requires_grad=True)\n        self.beta = nn.Parameter(torch.zeros(d_hid), requires_grad=True)\n        self.eps = eps\n\n    def forward(self, z):\n        mean = z.mean(dim=-1, keepdim=True,)\n        std = z.std(dim=-1, keepdim=True,)\n        ln_out = (z - mean.expand_as(z)) / (std.expand_as(z) + self.eps)\n        ln_out = self.gamma.expand_as(ln_out) * ln_out + self.beta.expand_as(ln_out)\n\n        return ln_out\n\n\nclass _MultiHeadAttention(nn.Module):\n    def __init__(self, d_k, d_v, d_model, n_heads, dropout):\n        super(_MultiHeadAttention, self).__init__()\n        self.d_k = d_k\n        self.d_v = d_v\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.w_q = nn.Parameter(torch.FloatTensor(n_heads, d_model, d_k))\n        self.w_k = nn.Parameter(torch.FloatTensor(n_heads, d_model, d_k))\n        self.w_v = nn.Parameter(torch.FloatTensor(n_heads, d_model, d_v))\n\n        self.attention = ScaledDotProductAttention(d_k, dropout)\n\n        init.xavier_normal(self.w_q)\n        init.xavier_normal(self.w_k)\n        init.xavier_normal(self.w_v)\n\n    def forward(self, q, k, v, attn_mask=None):\n        (d_k, d_v, d_model, n_heads) = (self.d_k, self.d_v, self.d_model, self.n_heads)\n        b_size = k.size(0)\n\n        q_s = q.repeat(n_heads, 1, 1).view(n_heads, -1, d_model)  # [n_heads x b_size * len_q x d_model]\n        k_s = k.repeat(n_heads, 1, 1).view(n_heads, -1, d_model)  # [n_heads x b_size * len_k x d_model]\n        v_s = v.repeat(n_heads, 1, 1).view(n_heads, -1, d_model)  # [n_heads x b_size * len_v x d_model]\n\n        q_s = torch.bmm(q_s, self.w_q).view(b_size * n_heads, -1, d_k)  # [b_size * n_heads x len_q x d_k]\n        k_s = torch.bmm(k_s, self.w_k).view(b_size * n_heads, -1, d_k)  # [b_size * n_heads x len_k x d_k]\n        v_s = torch.bmm(v_s, self.w_v).view(b_size * n_heads, -1, d_v)  # [b_size * n_heads x len_v x d_v]\n\n        # perform attention, result_size = [b_size * n_heads x len_q x d_v]\n        if attn_mask is not None:\n            attn_mask = attn_mask.repeat(n_heads, 1, 1)\n        outputs, attn = self.attention(q_s, k_s, v_s, attn_mask=attn_mask)\n\n        # return a list of tensors of shape [b_size x len_q x d_v] (length: n_heads)\n        return torch.split(outputs, b_size, dim=0), attn\n\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_k, d_v, d_model, n_heads, dropout):\n        super(MultiHeadAttention, self).__init__()\n        self.attention = _MultiHeadAttention(d_k, d_v, d_model, n_heads, dropout)\n        self.proj = Linear(n_heads * d_v, d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = LayerNormalization(d_model)\n\n    def forward(self, q, k, v, attn_mask):\n        # q: [b_size x len_q x d_model]\n        # k: [b_size x len_k x d_model]\n        # v: [b_size x len_v x d_model] note (len_k == len_v)\n        residual = q\n        # outputs: a list of tensors of shape [b_size x len_q x d_v] (length: n_heads)\n        outputs, attn = self.attention(q, k, v, attn_mask=attn_mask)\n        # concatenate \'n_heads\' multi-head attentions\n        outputs = torch.cat(outputs, dim=-1)\n        # project back to residual size, result_size = [b_size x len_q x d_model]\n        outputs = self.proj(outputs)\n        outputs = self.dropout(outputs)\n\n        return self.layer_norm(residual + outputs), attn\n\n\nclass _BahdanauAttention(nn.Module):\n    def __init__(self, method, hidden_size):\n        super(_BahdanauAttention, self).__init__()\n        self.method = method\n        self.hidden_size = hidden_size\n        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n        self.v = nn.Parameter(torch.rand(hidden_size))\n        stdv = 1. / math.sqrt(self.v.size(0))\n        self.v.data.normal_(mean=0, std=stdv)\n\n    def forward(self, hidden, encoder_outputs, mask=None):\n        """"""\n        :param hidden:\n            previous hidden state of the decoder, in shape (layers*directions,B,H)\n        :param encoder_outputs:\n            encoder outputs from Encoder, in shape (T,B,H)\n        :param mask:\n            used for masking. NoneType or tensor in shape (B) indicating sequence length\n        :return\n            attention energies in shape (B,T)\n        """"""\n        max_len = encoder_outputs.size(0)\n        # this_batch_size = encoder_outputs.size(1)\n        H = hidden.repeat(max_len, 1, 1).transpose(0, 1)\n        # [B*T*H]\n        encoder_outputs = encoder_outputs.transpose(0, 1)\n        # compute attention score\n        attn_energies = self.score(H, encoder_outputs)\n        if mask is not None:\n            attn_energies = attn_energies.masked_fill(mask, -1e18)\n        # normalize with softmax\n        return functional.softmax(attn_energies).unsqueeze(1)\n\n    def score(self, hidden, encoder_outputs):\n        # [B*T*2H]->[B*T*H]\n        energy = functional.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n        # [B*H*T]\n        energy = energy.transpose(2, 1)\n        # [B*1*H]\n        v = self.v.repeat(encoder_outputs.data.shape[0], 1).unsqueeze(1)\n        # [B*1*T]\n        energy = torch.bmm(v, energy)\n        # [B*T]\n        return energy.squeeze(1)\n\n\nclass BahdanauAttention(nn.Module):\n    """"""Reused from https://github.com/chrisbangun/pytorch-seq2seq_with_attention/""""""\n\n    def __init__(self, hidden_dim=128, query_dim=128, memory_dim=128):\n        super(BahdanauAttention, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.query_dim = query_dim\n        self.memory_dim = memory_dim\n        self.sofmax = nn.Softmax()\n\n        self.query_layer = nn.Linear(query_dim, hidden_dim, bias=False)\n        self.memory_layer = nn.Linear(memory_dim, hidden_dim, bias=False)\n        self.alignment_layer = nn.Linear(hidden_dim, 1, bias=False)\n\n    def alignment_score(self, query, keys):\n        query = self.query_layer(query)\n        keys = self.memory_layer(keys)\n\n        extendded_query = query.unsqueeze(1)\n        alignment = self.alignment_layer(functional.tanh(extendded_query + keys))\n        return alignment.squeeze(2)\n\n    def forward(self, query, keys):\n        alignment_score = self.alignment_score(query, keys)\n        weight = functional.softmax(alignment_score)\n        context = weight.unsqueeze(2) * keys\n        total_context = context.sum(1)\n        return total_context, alignment_score\n'"
modules/layers/ncrf.py,34,"b'# -*- coding: utf-8 -*-\n# @Author: Jie Yang\n# @Date:   2017-12-04 23:19:38\n# @Last Modified by:   Jie Yang,     Contact: jieynlp@gmail.com\n# @Last Modified time: 2018-05-16 16:57:39\n# https://github.com/Das-Boot/NCRFpp\n# @inproceedings{yang2018ncrf,  \n#  title={NCRF++: An Open-source Neural Sequence Labeling Toolkit},  \n#  author={Yang, Jie and Zhang, Yue},  \n#  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics},\n#  Url = {https://arxiv.org/pdf/1806.05626.pdf},\n#  year={2018}  \n# }\nfrom __future__ import print_function\nimport torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nSTART_TAG = -2\nSTOP_TAG = -1\n\n\n# Compute log sum exp in a numerically stable way for the forward algorithm\ndef log_sum_exp(vec, m_size):\n    """"""\n    calculate log of exp sum\n    args:\n        vec (batch_size, vanishing_dim, hidden_dim) : input tensor\n        m_size : hidden_dim\n    return:\n        batch_size, hidden_dim\n    """"""\n    _, idx = torch.max(vec, 1)  # B * 1 * M\n    max_score = torch.gather(vec, 1, idx.view(-1, 1, m_size)).view(-1, 1, m_size)\n    # B * M\n    return max_score.view(-1, m_size) + torch.log(torch.sum(torch.exp(vec - max_score.expand_as(vec)), 1)).view(-1,\n                                                                                                                m_size)\n\n\nclass NCRF(nn.Module):\n\n    def __init__(self, tagset_size, device):\n        super(NCRF, self).__init__()\n        print(""build CRF..."")\n        self.device = device\n        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n        self.tagset_size = tagset_size\n        # # We add 2 here, because of START_TAG and STOP_TAG\n        # # transitions (f_tag_size, t_tag_size), transition value from f_tag to t_tag\n        init_transitions = torch.zeros(self.tagset_size + 2, self.tagset_size + 2)\n        init_transitions[:, START_TAG] = -10000.0\n        init_transitions[STOP_TAG, :] = -10000.0\n        init_transitions[:, 0] = -10000.0\n        init_transitions[0, :] = -10000.0\n        if self.device:\n            init_transitions = init_transitions.to(device)\n        self.transitions = nn.Parameter(init_transitions)\n\n        # self.transitions = nn.Parameter(torch.Tensor(self.tagset_size+2, self.tagset_size+2))\n        # self.transitions.data.zero_()\n\n    def _calculate_PZ(self, feats, mask):\n        """"""\n            input:\n                feats: (batch, seq_len, self.tag_size+2)\n                masks: (batch, seq_len)\n        """"""\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n        # print feats.view(seq_len, tag_size)\n        assert (tag_size == self.tagset_size + 2)\n        mask = mask.transpose(1, 0).contiguous()\n        ins_num = seq_len * batch_size\n        # be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n        feats = feats.transpose(1, 0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n        # need to consider start\n        scores = feats + self.transitions.view(1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n        # build iter\n        seq_iter = enumerate(scores)\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        # only need start from start_tag\n        partition = inivalues[:, START_TAG, :].clone().view(batch_size, tag_size, 1)  # bat_size * to_target_size\n\n        # add start score (from start to all tag, duplicate to batch_size)\n        # partition = partition + self.transitions[START_TAG,:].view(1, tag_size, 1).expand(batch_size, tag_size, 1)\n        # iter over last scores\n        for idx, cur_values in seq_iter:\n            # previous to_target is current from_target\n            # partition: previous results log(exp(from_target)), #(batch_size * from_target)\n            # cur_values: bat_size * from_target * to_target\n\n            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size,\n                                                                                                  tag_size)\n            cur_partition = log_sum_exp(cur_values, tag_size)\n            # print cur_partition.data\n\n            # (bat_size * from_target * to_target) -> (bat_size * to_target)\n            mask_idx = mask[idx, :].view(batch_size, 1).expand(batch_size, tag_size)\n\n            # effective updated partition part, only keep the partition value of mask value = 1\n            mask_idx = mask_idx.byte()\n            masked_cur_partition = cur_partition.masked_select(mask_idx)\n            # let mask_idx broadcastable, to disable warning\n            mask_idx = mask_idx.contiguous().view(batch_size, tag_size, 1)\n\n            # replace the partition where the maskvalue=1, other partition value keeps the same\n            partition.masked_scatter_(mask_idx, masked_cur_partition)\n        # until the last state, add transition score\n        # for all partition (and do log_sum_exp) then select the value in STOP_TAG\n        cur_values = self.transitions.view(1, tag_size, tag_size).expand(batch_size, tag_size,\n                                                                         tag_size) + partition.contiguous().view(\n            batch_size, tag_size, 1).expand(batch_size, tag_size, tag_size)\n        cur_partition = log_sum_exp(cur_values, tag_size)\n        final_partition = cur_partition[:, STOP_TAG]\n        return final_partition.sum(), scores\n\n    def _viterbi_decode(self, feats, mask):\n        """"""\n            input:\n                feats: (batch, seq_len, self.tag_size+2)\n                mask: (batch, seq_len)\n            output:\n                decode_idx: (batch, seq_len) decoded sequence\n                path_score: (batch, 1) corresponding score for each sequence (to be implementated)\n        """"""\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n        assert (tag_size == self.tagset_size + 2)\n        # calculate sentence length for each sentence\n        length_mask = torch.sum(mask.long(), dim=1).view(batch_size, 1).long()\n        # mask to (seq_len, batch_size)\n        mask = mask.transpose(1, 0).contiguous()\n        ins_num = seq_len * batch_size\n        # be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n        feats = feats.transpose(1, 0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n        # need to consider start\n        scores = feats + self.transitions.view(1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n\n        # build iter\n        seq_iter = enumerate(scores)\n        # record the position of best score\n        back_points = list()\n        partition_history = list()\n        #  reverse mask (bug for mask = 1- mask, use this as alternative choice)\n        # mask = 1 + (-1)*mask\n        mask = (1 - mask.long()).byte()\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        # only need start from start_tag\n        partition = inivalues[:, START_TAG, :].clone().view(batch_size, tag_size)  # bat_size * to_target_size\n        # print ""init part:"",partition.size()\n        partition_history.append(partition)\n        # iter over last scores\n        for idx, cur_values in seq_iter:\n            # previous to_target is current from_target\n            # partition: previous results log(exp(from_target)), #(batch_size * from_target)\n            # cur_values: batch_size * from_target * to_target\n            cur_values = cur_values + partition.contiguous().view(batch_size, tag_size, 1).expand(batch_size, tag_size,\n                                                                                                  tag_size)\n            # forscores, cur_bp = torch.max(cur_values[:,:-2,:], 1) # do not consider START_TAG/STOP_TAG\n            # print ""cur value:"", cur_values.size()\n            partition, cur_bp = torch.max(cur_values, 1)\n            # print ""partsize:"",partition.size()\n            # exit(0)\n            # print partition\n            # print cur_bp\n            # print ""one best, "",idx\n            partition_history.append(partition)\n            # cur_bp: (batch_size, tag_size) max source score position in current tag\n            # set padded label as 0, which will be filtered in post processing\n            cur_bp.masked_fill_(mask[idx].view(batch_size, 1).expand(batch_size, tag_size), 0)\n            back_points.append(cur_bp)\n        # exit(0)\n        # add score to final STOP_TAG\n        partition_history = torch.cat(partition_history, 0).view(seq_len, batch_size, -1).transpose(1,\n                                                                                                    0).contiguous()\n        # (batch_size, seq_len. tag_size)\n        # get the last position for each setences, and select the last partitions using gather()\n        last_position = length_mask.view(batch_size, 1, 1).expand(batch_size, 1, tag_size) - 1\n        last_partition = torch.gather(partition_history, 1, last_position).view(batch_size, tag_size, 1)\n        # calculate the score from last partition to end state (and then select the STOP_TAG from it)\n        last_values = last_partition.expand(batch_size, tag_size, tag_size) + self.transitions.view(1, tag_size,\n                                                                                                    tag_size).expand(\n            batch_size, tag_size, tag_size)\n        _, last_bp = torch.max(last_values, 1)\n        pad_zero = autograd.Variable(torch.zeros(batch_size, tag_size)).long()\n        if self.device:\n            pad_zero = pad_zero.to(self.device)\n        back_points.append(pad_zero)\n        back_points = torch.cat(back_points).view(seq_len, batch_size, tag_size)\n\n        # select end ids in STOP_TAG\n        pointer = last_bp[:, STOP_TAG]\n        insert_last = pointer.contiguous().view(batch_size, 1, 1).expand(batch_size, 1, tag_size)\n        back_points = back_points.transpose(1, 0).contiguous()\n        # move the end ids(expand to tag_size) to the corresponding position of back_points to replace the 0 values\n        # print ""lp:"",last_position\n        # print ""il:"",insert_last\n        back_points.scatter_(1, last_position, insert_last)\n        # print ""bp:"",back_points\n        # exit(0)\n        back_points = back_points.transpose(1, 0).contiguous()\n        # decode from the end, padded position ids are 0, which will be filtered if following evaluation\n        decode_idx = autograd.Variable(torch.LongTensor(seq_len, batch_size))\n        if self.device:\n            decode_idx = decode_idx.to(self.device)\n        decode_idx[-1] = pointer.data\n        for idx in range(len(back_points) - 2, -1, -1):\n            pointer = torch.gather(back_points[idx], 1, pointer.contiguous().view(batch_size, 1))\n            decode_idx[idx] = pointer.data\n        path_score = None\n        decode_idx = decode_idx.transpose(1, 0)\n        return path_score, decode_idx\n\n    def forward(self, feats):\n        path_score, best_path = self._viterbi_decode(feats)\n        return path_score, best_path\n\n    def _score_sentence(self, scores, mask, tags):\n        """"""\n            input:\n                scores: variable (seq_len, batch, tag_size, tag_size)\n                mask: (batch, seq_len)\n                tags: tensor  (batch, seq_len)\n            output:\n                score: sum of score for gold sequences within whole batch\n        """"""\n        # Gives the score of a provided tag sequence\n        batch_size = scores.size(1)\n        seq_len = scores.size(0)\n        tag_size = scores.size(2)\n        # convert tag value into a new format, recorded label bigram information to index\n        new_tags = autograd.Variable(torch.LongTensor(batch_size, seq_len))\n        if self.device:\n            new_tags = new_tags.to(self.device)\n        for idx in range(seq_len):\n            if idx == 0:\n                # start -> first score\n                new_tags[:, 0] = (tag_size - 2) * tag_size + tags[:, 0]\n\n            else:\n                new_tags[:, idx] = tags[:, idx - 1] * tag_size + tags[:, idx]\n\n        # transition for label to STOP_TAG\n        end_transition = self.transitions[:, STOP_TAG].contiguous().view(1, tag_size).expand(batch_size, tag_size)\n        # length for batch,  last word position = length - 1\n        length_mask = torch.sum(mask.long(), dim=1).view(batch_size, 1).long()\n        # index the label id of last word\n        end_ids = torch.gather(tags, 1, length_mask - 1)\n\n        # index the transition score for end_id to STOP_TAG\n        end_energy = torch.gather(end_transition, 1, end_ids)\n\n        # convert tag as (seq_len, batch_size, 1)\n        new_tags = new_tags.transpose(1, 0).contiguous().view(seq_len, batch_size, 1)\n        # need convert tags id to search from 400 positions of scores\n        tg_energy = torch.gather(scores.view(seq_len, batch_size, -1), 2, new_tags).view(seq_len,\n                                                                                         batch_size)\n        # seq_len * bat_size\n        # mask transpose to (seq_len, batch_size)\n        mask = mask.byte()\n        tg_energy = tg_energy.masked_select(mask.transpose(1, 0))\n\n        # # calculate the score from START_TAG to first label\n        # start_transition = self.transitions[START_TAG,:].view(1, tag_size).expand(batch_size, tag_size)\n        # start_energy = torch.gather(start_transition, 1, tags[0,:])\n\n        # add all score together\n        # gold_score = start_energy.sum() + tg_energy.sum() + end_energy.sum()\n        gold_score = tg_energy.sum() + end_energy.sum()\n        return gold_score\n\n    def neg_log_likelihood_loss(self, feats, mask, tags):\n        # nonegative log likelihood\n        forward_score, scores = self._calculate_PZ(feats, mask)\n        gold_score = self._score_sentence(scores, mask, tags)\n        return forward_score - gold_score\n\n    def _viterbi_decode_nbest(self, feats, mask, nbest):\n        """"""\n            input:\n                feats: (batch, seq_len, self.tag_size+2)\n                mask: (batch, seq_len)\n            output:\n                decode_idx: (batch, nbest, seq_len) decoded sequence\n                path_score: (batch, nbest) corresponding score for each sequence (to be implementated)\n                nbest decode for sentence with one token is not well supported, to be optimized\n        """"""\n        batch_size = feats.size(0)\n        seq_len = feats.size(1)\n        tag_size = feats.size(2)\n        assert (tag_size == self.tagset_size + 2)\n        # calculate sentence length for each sentence\n        length_mask = torch.sum(mask.long(), dim=1).view(batch_size, 1).long()\n        # mask to (seq_len, batch_size)\n        mask = mask.transpose(1, 0).contiguous()\n        ins_num = seq_len * batch_size\n        # be careful the view shape, it is .view(ins_num, 1, tag_size) but not .view(ins_num, tag_size, 1)\n        feats = feats.transpose(1, 0).contiguous().view(ins_num, 1, tag_size).expand(ins_num, tag_size, tag_size)\n        # need to consider start\n        scores = feats + self.transitions.view(1, tag_size, tag_size).expand(ins_num, tag_size, tag_size)\n        scores = scores.view(seq_len, batch_size, tag_size, tag_size)\n\n        # build iter\n        seq_iter = enumerate(scores)\n        # record the position of best score\n        back_points = list()\n        partition_history = list()\n        #  reverse mask (bug for mask = 1- mask, use this as alternative choice)\n        # mask = 1 + (-1)*mask\n        mask = (1 - mask.long()).byte()\n        _, inivalues = next(seq_iter)  # bat_size * from_target_size * to_target_size\n        # only need start from start_tag\n        partition = inivalues[:, START_TAG, :].clone()  # bat_size * to_target_size\n        # initial partition [batch_size, tag_size]\n        partition_history.append(partition.view(batch_size, tag_size, 1).expand(batch_size, tag_size, nbest))\n        # iter over last scores\n        for idx, cur_values in seq_iter:\n            if idx == 1:\n                cur_values = cur_values.view(batch_size, tag_size, tag_size) + partition.contiguous().view(batch_size,\n                                                                                                           tag_size,\n                                                                                                           1).expand(\n                    batch_size, tag_size, tag_size)\n            else:\n                # previous to_target is current from_target\n                # partition: previous results log(exp(from_target)), #(batch_size * nbest * from_target)\n                # cur_values: batch_size * from_target * to_target\n                cur_values = cur_values.view(batch_size, tag_size, 1, tag_size).expand(\n                    batch_size, tag_size, nbest, tag_size) + partition.contiguous().view(\n                    batch_size, tag_size, nbest, 1).expand(batch_size, tag_size, nbest, tag_size)\n                # compare all nbest and all from target\n                cur_values = cur_values.view(batch_size, tag_size * nbest, tag_size)\n                # print ""cur size:"",cur_values.size()\n            partition, cur_bp = torch.topk(cur_values, nbest, 1)\n            # cur_bp/partition: [batch_size, nbest, tag_size],\n            # id should be normize through nbest in following backtrace step\n            # print partition[:,0,:]\n            # print cur_bp[:,0,:]\n            # print ""nbest, "",idx\n            if idx == 1:\n                cur_bp = cur_bp * nbest\n            partition = partition.transpose(2, 1)\n            cur_bp = cur_bp.transpose(2, 1)\n\n            # print partition\n            # exit(0)\n            # partition: (batch_size * to_target * nbest)\n            # cur_bp: (batch_size * to_target * nbest)\n            # Notice the cur_bp number is the whole position of tag_size*nbest, need to convert when decode\n            partition_history.append(partition)\n            # cur_bp: (batch_size,nbest, tag_size) topn source score position in current tag\n            # set padded label as 0, which will be filtered in post processing\n            # mask[idx] ? mask[idx-1]\n            cur_bp.masked_fill_(mask[idx].view(batch_size, 1, 1).expand(batch_size, tag_size, nbest), 0)\n            # print cur_bp[0]\n            back_points.append(cur_bp)\n        # add score to final STOP_TAG\n        partition_history = torch.cat(partition_history, 0).view(\n            seq_len, batch_size, tag_size, nbest).transpose(1, 0).contiguous()\n        # (batch_size, seq_len, nbest, tag_size)\n        # get the last position for each setences, and select the last partitions using gather()\n        last_position = length_mask.view(batch_size, 1, 1, 1).expand(batch_size, 1, tag_size, nbest) - 1\n        last_partition = torch.gather(partition_history, 1, last_position).view(batch_size, tag_size, nbest, 1)\n        # calculate the score from last partition to end state (and then select the STOP_TAG from it)\n        last_values = last_partition.expand(batch_size, tag_size, nbest, tag_size) + self.transitions.view(\n            1, tag_size, 1, tag_size).expand(batch_size, tag_size, nbest, tag_size)\n        last_values = last_values.view(batch_size, tag_size * nbest, tag_size)\n        end_partition, end_bp = torch.topk(last_values, nbest, 1)\n        # end_partition: (batch, nbest, tag_size)\n        end_bp = end_bp.transpose(2, 1)\n        # end_bp: (batch, tag_size, nbest)\n        pad_zero = autograd.Variable(torch.zeros(batch_size, tag_size, nbest)).long()\n        if self.device:\n            pad_zero = pad_zero.to(self.device)\n        back_points.append(pad_zero)\n        back_points = torch.cat(back_points).view(seq_len, batch_size, tag_size, nbest)\n\n        # select end ids in STOP_TAG\n        pointer = end_bp[:, STOP_TAG, :]  # (batch_size, nbest)\n        insert_last = pointer.contiguous().view(batch_size, 1, 1, nbest).expand(batch_size, 1, tag_size, nbest)\n        back_points = back_points.transpose(1, 0).contiguous()\n        # move the end ids(expand to tag_size) to the corresponding position of back_points to replace the 0 values\n        # print ""lp:"",last_position\n        # print ""il:"",insert_last[0]\n        # exit(0)\n        # copy the ids of last position:insert_last to back_points, though the last_position index\n        # last_position includes the length of batch sentences\n        # print ""old:"", back_points[9,0,:,:]\n        back_points.scatter_(1, last_position, insert_last)\n        # back_points: [batch_size, seq_length, tag_size, nbest]\n        # print ""new:"", back_points[9,0,:,:]\n        # exit(0)\n        # print pointer[2]\n        \'\'\'\n        back_points: in simple demonstratration\n        x,x,x,x,x,x,x,x,x,7\n        x,x,x,x,x,4,0,0,0,0\n        x,x,6,0,0,0,0,0,0,0\n        \'\'\'\n\n        back_points = back_points.transpose(1, 0).contiguous()\n        # print back_points[0]\n        # back_points: (seq_len, batch, tag_size, nbest)\n        # decode from the end, padded position ids are 0, which will be filtered in following evaluation\n        decode_idx = autograd.Variable(torch.LongTensor(seq_len, batch_size, nbest))\n        if self.device:\n            decode_idx = decode_idx.to(self.device)\n        decode_idx[-1] = pointer.data / nbest\n        # print ""pointer-1:"",pointer[2]\n        # exit(0)\n        # use old mask, let 0 means has token\n        for idx in range(len(back_points) - 2, -1, -1):\n            # print ""pointer: "",idx,  pointer[3]\n            # print ""back:"",back_points[idx][3]\n            # print ""mask:"",mask[idx+1,3]\n            new_pointer = torch.gather(back_points[idx].view(batch_size, tag_size * nbest), 1,\n                                       pointer.contiguous().view(batch_size, nbest))\n            decode_idx[idx] = new_pointer.data / nbest\n            # # use new pointer to remember the last end nbest ids for non longest\n            pointer = new_pointer + pointer.contiguous().view(batch_size, nbest) * mask[idx].view(batch_size, 1).expand(\n                batch_size, nbest).long()\n\n        decode_idx = decode_idx.transpose(1, 0)\n\n        # calculate probability for each sequence\n        scores = end_partition[:, :, STOP_TAG]\n        # scores: [batch_size, nbest]\n        max_scores, _ = torch.max(scores, 1)\n        minus_scores = scores - max_scores.view(batch_size, 1).expand(batch_size, nbest)\n        path_score = F.softmax(minus_scores, 1)\n        # path_score: [batch_size, nbest]\n        # exit(0)\n        return path_score, decode_idx\n'"
modules/models/__init__.py,0,b''
modules/models/bert_models.py,0,"b'from modules.layers.decoders import *\nfrom modules.layers.embedders import *\nfrom modules.layers.layers import BiLSTM, MultiHeadAttention\nimport abc\n\n\nclass BERTNerModel(nn.Module, metaclass=abc.ABCMeta):\n    """"""Base class for all BERT Models""""""\n\n    @abc.abstractmethod\n    def forward(self, batch):\n        raise NotImplementedError(""abstract method forward must be implemented"")\n\n    @abc.abstractmethod\n    def score(self, batch):\n        raise NotImplementedError(""abstract method score must be implemented"")\n\n    @abc.abstractmethod\n    def create(self, *args, **kwargs):\n        raise NotImplementedError(""abstract method create must be implemented"")\n\n    def get_n_trainable_params(self):\n        pp = 0\n        for p in list(self.parameters()):\n            if p.requires_grad:\n                num = 1\n                for s in list(p.size()):\n                    num = num * s\n                pp += num\n        return pp\n\n\nclass BERTBiLSTMCRF(BERTNerModel):\n\n    def __init__(self, embeddings, lstm, crf, device=""cuda""):\n        super(BERTBiLSTMCRF, self).__init__()\n        self.embeddings = embeddings\n        self.lstm = lstm\n        self.crf = crf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        return self.crf.forward(output, labels_mask)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, labels = batch\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        return self.crf.score(output, labels_mask, labels)\n\n    @classmethod\n    def create(cls,\n               label_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # BiLSTM params\n               embedding_size=768, hidden_dim=512, rnn_layers=1, lstm_dropout=0.3,\n               # CRFDecoder params\n               crf_dropout=0.5,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        lstm = BiLSTM.create(\n                embedding_size=embedding_size, hidden_dim=hidden_dim, rnn_layers=rnn_layers, dropout=lstm_dropout)\n        crf = CRFDecoder.create(label_size, hidden_dim, crf_dropout)\n        return cls(embeddings, lstm, crf, device)\n\n\nclass BERTBiLSTMNCRF(BERTNerModel):\n\n    def __init__(self, embeddings, lstm, crf, device=""cuda""):\n        super(BERTBiLSTMNCRF, self).__init__()\n        self.embeddings = embeddings\n        self.lstm = lstm\n        self.crf = crf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        return self.crf.forward(output, labels_mask)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, labels = batch\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        return self.crf.score(output, labels_mask, labels)\n\n    @classmethod\n    def create(cls,\n               label_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # BiLSTM params\n               embedding_size=768, hidden_dim=512, rnn_layers=1, lstm_dropout=0.3,\n               # NCRFDecoder params\n               crf_dropout=0.5, nbest=1,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        lstm = BiLSTM.create(\n                embedding_size=embedding_size, hidden_dim=hidden_dim, rnn_layers=rnn_layers, dropout=lstm_dropout)\n        crf = NCRFDecoder.create(\n            label_size, hidden_dim, crf_dropout, nbest, device=device)\n        return cls(embeddings, lstm, crf, device)\n\n\nclass BERTAttnCRF(BERTNerModel):\n\n    def __init__(self, embeddings, attn, crf, device=""cuda""):\n        super(BERTAttnCRF, self).__init__()\n        self.embeddings = embeddings\n        self.attn = attn\n        self.crf = crf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.attn(input_embeddings, input_embeddings, input_embeddings, None)\n        return self.crf.forward(output, labels_mask)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, labels = batch\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.attn(input_embeddings, input_embeddings, input_embeddings, None)\n        return self.crf.score(output, labels_mask, labels)\n\n    @classmethod\n    def create(cls,\n               label_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # Attn params\n               embedding_size=768, key_dim=64, val_dim=64, num_heads=3, attn_dropout=0.3,\n               # CRFDecoder params\n               crf_dropout=0.5,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        attn = MultiHeadAttention(key_dim, val_dim, embedding_size, num_heads, attn_dropout)\n        crf = CRFDecoder.create(\n            label_size, embedding_size, crf_dropout)\n        return cls(embeddings, attn, crf, device)\n\n\nclass BERTAttnNCRF(BERTNerModel):\n\n    def __init__(self, embeddings, attn, crf, device=""cuda""):\n        super(BERTAttnNCRF, self).__init__()\n        self.embeddings = embeddings\n        self.attn = attn\n        self.crf = crf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.attn(input_embeddings, input_embeddings, input_embeddings, None)\n        return self.crf.forward(output, labels_mask)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, labels = batch\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.attn(input_embeddings, input_embeddings, input_embeddings, None)\n        return self.crf.score(output, labels_mask, labels)\n\n    @classmethod\n    def create(cls,\n               label_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # Attn params\n               embedding_size=768, key_dim=64, val_dim=64, num_heads=3, attn_dropout=0.3,\n               # NCRFDecoder params\n               crf_dropout=0.5, nbest=1,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        attn = MultiHeadAttention(key_dim, val_dim, embedding_size, num_heads, attn_dropout)\n        crf = NCRFDecoder.create(\n            label_size, embedding_size, crf_dropout, nbest=nbest, device=device)\n        return cls(embeddings, attn, crf, device)\n\n\nclass BERTBiLSTMAttnCRF(BERTNerModel):\n\n    def __init__(self, embeddings, lstm, attn, crf, device=""cuda""):\n        super(BERTBiLSTMAttnCRF, self).__init__()\n        self.embeddings = embeddings\n        self.lstm = lstm\n        self.attn = attn\n        self.crf = crf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        output, _ = self.attn(output, output, output, None)\n        return self.crf.forward(output, labels_mask)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, labels = batch\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        output, _ = self.attn(output, output, output, None)\n        return self.crf.score(output, labels_mask, labels)\n\n    @classmethod\n    def create(cls,\n               label_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # BiLSTM\n               hidden_dim=512, rnn_layers=1, lstm_dropout=0.3,\n               # Attn params\n               embedding_size=768, key_dim=64, val_dim=64, num_heads=3, attn_dropout=0.3,\n               # CRFDecoder params\n               crf_dropout=0.5,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        lstm = BiLSTM.create(\n            embedding_size=embedding_size, hidden_dim=hidden_dim, rnn_layers=rnn_layers, dropout=lstm_dropout)\n        attn = MultiHeadAttention(key_dim, val_dim, hidden_dim, num_heads, attn_dropout)\n        crf = CRFDecoder.create(\n            label_size, hidden_dim, crf_dropout)\n        return cls(embeddings, lstm, attn, crf, device)\n\n\nclass BERTBiLSTMAttnNCRF(BERTNerModel):\n\n    def __init__(self, embeddings, lstm, attn, crf, device=""cuda""):\n        super(BERTBiLSTMAttnNCRF, self).__init__()\n        self.embeddings = embeddings\n        self.lstm = lstm\n        self.attn = attn\n        self.crf = crf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        output, _ = self.attn(output, output, output, None)\n        return self.crf.forward(output, labels_mask)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, labels = batch\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        output, _ = self.attn(output, output, output, None)\n        return self.crf.score(output, labels_mask, labels)\n\n    @classmethod\n    def create(cls,\n               label_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # BiLSTM\n               hidden_dim=512, rnn_layers=1, lstm_dropout=0.3,\n               # Attn params\n               embedding_size=768, key_dim=64, val_dim=64, num_heads=3, attn_dropout=0.3,\n               # NCRFDecoder params\n               crf_dropout=0.5, nbest=1,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        lstm = BiLSTM.create(\n            embedding_size=embedding_size, hidden_dim=hidden_dim, rnn_layers=rnn_layers, dropout=lstm_dropout)\n        attn = MultiHeadAttention(key_dim, val_dim, hidden_dim, num_heads, attn_dropout)\n        crf = NCRFDecoder.create(\n            label_size, hidden_dim, crf_dropout, nbest=nbest, device=device)\n        return cls(embeddings, lstm, attn, crf, device)\n\n\nclass BERTBiLSTMAttnNCRFJoint(BERTNerModel):\n\n    def __init__(self, embeddings, lstm, attn, crf, clf, device=""cuda""):\n        super(BERTBiLSTMAttnNCRFJoint, self).__init__()\n        self.embeddings = embeddings\n        self.lstm = lstm\n        self.attn = attn\n        self.crf = crf\n        self.clf = clf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        output, _ = self.attn(output, output, output, None)\n        return self.crf.forward(output, labels_mask), self.clf(output)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, labels, cls_ids = batch\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        output, _ = self.attn(output, output, output, None)\n        return self.crf.score(output, labels_mask, labels) + self.clf.score(output, cls_ids)\n\n    @classmethod\n    def create(cls,\n               label_size, intent_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # BiLSTM\n               hidden_dim=512, rnn_layers=1, lstm_dropout=0.3,\n               # Attn params\n               embedding_size=768, key_dim=64, val_dim=64, num_heads=3, attn_dropout=0.3,\n               # NCRFDecoder params\n               crf_dropout=0.5, nbest=1,\n               # Clf params\n               clf_dropout=0.3,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        lstm = BiLSTM.create(\n            embedding_size=embedding_size, hidden_dim=hidden_dim, rnn_layers=rnn_layers, dropout=lstm_dropout)\n        attn = MultiHeadAttention(key_dim, val_dim, hidden_dim, num_heads, attn_dropout)\n        crf = NCRFDecoder.create(\n            label_size, hidden_dim, crf_dropout, nbest=nbest, device=device)\n        clf = ClassDecoder(intent_size, hidden_dim, clf_dropout)\n        return cls(embeddings, lstm, attn, crf, clf, device)\n\n\nclass BERTBiLSTMAttnCRFJoint(BERTNerModel):\n\n    def __init__(self, embeddings, lstm, attn, crf, clf, device=""cuda""):\n        super(BERTBiLSTMAttnCRFJoint, self).__init__()\n        self.embeddings = embeddings\n        self.lstm = lstm\n        self.attn = attn\n        self.crf = crf\n        self.clf = clf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        output, _ = self.attn(output, output, output, None)\n        return self.crf.forward(output, labels_mask), self.clf(output)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, labels, cls_ids = batch\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        output, _ = self.attn(output, output, output, None)\n        return self.crf.score(output, labels_mask, labels) + self.clf.score(output, cls_ids)\n\n    @classmethod\n    def create(cls,\n               label_size, intent_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # BiLSTM\n               hidden_dim=512, rnn_layers=1, lstm_dropout=0.3,\n               # Attn params\n               embedding_size=768, key_dim=64, val_dim=64, num_heads=3, attn_dropout=0.3,\n               # CRFDecoder params\n               crf_dropout=0.5,\n               # Clf params\n               clf_dropout=0.3,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        lstm = BiLSTM.create(\n            embedding_size=embedding_size, hidden_dim=hidden_dim, rnn_layers=rnn_layers, dropout=lstm_dropout)\n        attn = MultiHeadAttention(key_dim, val_dim, hidden_dim, num_heads, attn_dropout)\n        crf = CRFDecoder.create(\n            label_size, hidden_dim, crf_dropout)\n        clf = ClassDecoder(intent_size, hidden_dim, clf_dropout)\n        return cls(embeddings, lstm, attn, crf, clf, device)\n\n\nclass BERTBiLSTMCRFJoint(BERTNerModel):\n\n    def __init__(self, embeddings, lstm, crf, clf, device=""cuda""):\n        super(BERTBiLSTMCRFJoint, self).__init__()\n        self.embeddings = embeddings\n        self.lstm = lstm\n        self.crf = crf\n        self.clf = clf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        return self.crf.forward(output, labels_mask), self.clf(output)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, labels, cls_ids = batch\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        return self.crf.score(output, labels_mask, labels) + self.clf.score(output, cls_ids)\n\n    @classmethod\n    def create(cls,\n               label_size, intent_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # BiLSTM params\n               embedding_size=768, hidden_dim=512, rnn_layers=1, lstm_dropout=0.3,\n               # CRFDecoder params\n               crf_dropout=0.5,\n               # Clf params\n               clf_dropout=0.3,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        lstm = BiLSTM.create(\n                embedding_size=embedding_size, hidden_dim=hidden_dim, rnn_layers=rnn_layers, dropout=lstm_dropout)\n        crf = CRFDecoder.create(label_size, hidden_dim, crf_dropout)\n        clf = ClassDecoder(intent_size, hidden_dim, clf_dropout)\n        return cls(embeddings, lstm, crf, clf, device)\n\n\nclass BERTBiLSTMNCRFJoint(BERTNerModel):\n\n    def __init__(self, embeddings, lstm, crf, clf, device=""cuda""):\n        super(BERTBiLSTMNCRFJoint, self).__init__()\n        self.embeddings = embeddings\n        self.lstm = lstm\n        self.crf = crf\n        self.clf = clf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        return self.crf.forward(output, labels_mask), self.clf(output)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, labels, cls_ids = batch\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        return self.crf.score(output, labels_mask, labels) + self.clf.score(output, cls_ids)\n\n    @classmethod\n    def create(cls,\n               label_size, intent_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # BiLSTM params\n               embedding_size=768, hidden_dim=512, rnn_layers=1, lstm_dropout=0.3,\n               # CRFDecoder params\n               crf_dropout=0.5, nbest=1,\n               # Clf params\n               clf_dropout=0.3,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        lstm = BiLSTM.create(\n                embedding_size=embedding_size, hidden_dim=hidden_dim, rnn_layers=rnn_layers, dropout=lstm_dropout)\n        crf = NCRFDecoder.create(label_size, hidden_dim, crf_dropout, nbest=nbest, device=device)\n        clf = ClassDecoder(intent_size, hidden_dim, clf_dropout)\n        return cls(embeddings, lstm, crf, clf, device)\n\n\nclass BERTAttnCRFJoint(BERTNerModel):\n\n    def __init__(self, embeddings, attn, crf, clf, device=""cuda""):\n        super(BERTAttnCRFJoint, self).__init__()\n        self.embeddings = embeddings\n        self.attn = attn\n        self.crf = crf\n        self.clf = clf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.attn(input_embeddings, input_embeddings, input_embeddings, None)\n        return self.crf.forward(output, labels_mask), self.clf(output)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, labels, cls_ids = batch\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.attn(input_embeddings, input_embeddings, input_embeddings, None)\n        return self.crf.score(output, labels_mask, labels) + self.clf.score(output, cls_ids)\n\n    @classmethod\n    def create(cls,\n               label_size, intent_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # BiLSTM\n               hidden_dim=512, rnn_layers=1, lstm_dropout=0.3,\n               # Attn params\n               embedding_size=768, key_dim=64, val_dim=64, num_heads=3, attn_dropout=0.3,\n               # CRFDecoder params\n               crf_dropout=0.5,\n               # Clf params\n               clf_dropout=0.3,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        attn = MultiHeadAttention(key_dim, val_dim, hidden_dim, num_heads, attn_dropout)\n        crf = CRFDecoder.create(\n            label_size, hidden_dim, crf_dropout)\n        clf = ClassDecoder(intent_size, hidden_dim, clf_dropout)\n        return cls(embeddings, attn, crf, clf, device)\n\n\nclass BERTAttnNCRFJoint(BERTNerModel):\n\n    def __init__(self, embeddings, attn, crf, clf, device=""cuda""):\n        super(BERTAttnNCRFJoint, self).__init__()\n        self.embeddings = embeddings\n        self.attn = attn\n        self.crf = crf\n        self.clf = clf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.attn(input_embeddings, input_embeddings, input_embeddings, None)\n        return self.crf.forward(output, labels_mask), self.clf(output)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, labels, cls_ids = batch\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.attn(input_embeddings, input_embeddings, input_embeddings, None)\n        return self.crf.score(output, labels_mask, labels) + self.clf.score(output, cls_ids)\n\n    @classmethod\n    def create(cls,\n               label_size, intent_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # BiLSTM\n               hidden_dim=512, rnn_layers=1, lstm_dropout=0.3,\n               # Attn params\n               embedding_size=768, key_dim=64, val_dim=64, num_heads=3, attn_dropout=0.3,\n               # NCRFDecoder params\n               crf_dropout=0.5, nbest=1,\n               # Clf params\n               clf_dropout=0.3,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        attn = MultiHeadAttention(key_dim, val_dim, hidden_dim, num_heads, attn_dropout)\n        crf = NCRFDecoder.create(\n            label_size, hidden_dim, crf_dropout, nbest=nbest, device=device)\n        clf = ClassDecoder(intent_size, hidden_dim, clf_dropout)\n        return cls(embeddings, attn, crf, clf, device)\n\n\nclass BERTNCRF(BERTNerModel):\n\n    def __init__(self, embeddings, crf, device=""cuda""):\n        super(BERTNCRF, self).__init__()\n        self.embeddings = embeddings\n        self.crf = crf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        return self.crf.forward(input_embeddings, labels_mask)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, labels = batch\n        input_embeddings = self.embeddings(batch)\n        return self.crf.score(input_embeddings, labels_mask, labels)\n\n    @classmethod\n    def create(cls,\n               label_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               embedding_size=768,\n               # NCRFDecoder params\n               crf_dropout=0.5, nbest=1,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        crf = NCRFDecoder.create(\n            label_size, embedding_size, crf_dropout, nbest=nbest, device=device)\n        return cls(embeddings, crf, device)\n\n\nclass BERTCRF(BERTNerModel):\n\n    def __init__(self, embeddings, crf, device=""cuda""):\n        super(BERTCRF, self).__init__()\n        self.embeddings = embeddings\n        self.crf = crf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        return self.crf.forward(input_embeddings, labels_mask)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, labels = batch\n        input_embeddings = self.embeddings(batch)\n        return self.crf.score(input_embeddings, labels_mask, labels)\n\n    @classmethod\n    def create(cls,\n               label_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               embedding_size=768,\n               # NCRFDecoder params\n               crf_dropout=0.5,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        crf = CRFDecoder.create(\n            label_size, embedding_size, crf_dropout)\n        return cls(embeddings, crf, device)\n'"
modules/models/classifiers.py,0,"b'from .bert_models import BERTNerModel\nfrom modules.layers.decoders import *\nfrom modules.layers.embedders import *\nfrom modules.layers.layers import *\n\n\nclass BERTLinearsClassifier(BERTNerModel):\n\n    def __init__(self, embeddings, linear, dropout, activation, device=""cuda""):\n        super(BERTLinearsClassifier, self).__init__()\n        self.embeddings = embeddings\n        self.linear = linear\n        self.dropout = dropout\n        self.activation = activation\n        self.intent_loss = nn.CrossEntropyLoss()\n        self.to(device)\n\n    @staticmethod\n    def pool(x, bs, is_max):\n        """"""Pool the tensor along the seq_len dimension.""""""\n        f = functional.adaptive_max_pool1d if is_max else functional.adaptive_avg_pool1d\n        return f(x.permute(1, 2, 0), (1,)).view(bs, -1)\n\n    def forward(self, batch):\n        input_embeddings = self.embeddings(batch)\n        output = self.dropout(input_embeddings).transpose(0, 1)\n        sl, bs, _ = output.size()\n        output = self.pool(output, bs, True)\n        output = self.linear(output)\n        return self.activation(output).argmax(-1)\n\n    def score(self, batch):\n        input_embeddings = self.embeddings(batch)\n        output = self.dropout(input_embeddings).transpose(0, 1)\n        sl, bs, _ = output.size()\n        output = self.pool(output, bs, True)\n        output = self.linear(output)\n        return self.intent_loss(self.activation(output), batch[-1])\n\n    @classmethod\n    def create(cls,\n               intent_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # Decoder params\n               embedding_size=768, clf_dropout=0.3, num_hiddens=2,\n               activation=""tanh"",\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        linear = Linears(embedding_size, intent_size, [embedding_size // 2**idx for idx in range(num_hiddens)])\n        dropout = nn.Dropout(clf_dropout)\n        activation = getattr(functional, activation)\n        return cls(embeddings, linear, dropout, activation, device)\n\n\nclass BERTLinearClassifier(BERTNerModel):\n\n    def __init__(self, embeddings, linear, dropout, activation, device=""cuda""):\n        super(BERTLinearClassifier, self).__init__()\n        self.embeddings = embeddings\n        self.linear = linear\n        self.dropout = dropout\n        self.activation = activation\n        self.intent_loss = nn.CrossEntropyLoss()\n        self.to(device)\n\n    @staticmethod\n    def pool(x, bs, is_max):\n        """"""Pool the tensor along the seq_len dimension.""""""\n        f = functional.adaptive_max_pool1d if is_max else functional.adaptive_avg_pool1d\n        return f(x.permute(1, 2, 0), (1,)).view(bs, -1)\n\n    def forward(self, batch):\n        input_embeddings = self.embeddings(batch)\n        output = self.dropout(input_embeddings).transpose(0, 1)\n        sl, bs, _ = output.size()\n        output = self.pool(output, bs, True)\n        output = self.linear(output)\n        return self.activation(output).argmax(-1)\n\n    def score(self, batch):\n        input_embeddings = self.embeddings(batch)\n        output = self.dropout(input_embeddings).transpose(0, 1)\n        sl, bs, _ = output.size()\n        output = self.pool(output, bs, True)\n        output = self.linear(output)\n        return self.intent_loss(self.activation(output), batch[-1])\n\n    @classmethod\n    def create(cls,\n               intent_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # Decoder params\n               embedding_size=768, clf_dropout=0.3,\n               activation=""sigmoid"",\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        linear = Linear(embedding_size, intent_size)\n        dropout = nn.Dropout(clf_dropout)\n        activation = getattr(functional, activation)\n        return cls(embeddings, linear, dropout, activation, device)\n\n\nclass BERTBaseClassifier(BERTNerModel):\n\n    def __init__(self, embeddings, clf, device=""cuda""):\n        super(BERTBaseClassifier, self).__init__()\n        self.embeddings = embeddings\n        self.clf = clf\n        self.to(device)\n\n    def forward(self, batch):\n        input_embeddings = self.embeddings(batch)\n        return self.clf(input_embeddings)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids, cls_ids = batch\n        input_embeddings = self.embeddings(batch)\n        return self.clf.score(input_embeddings, cls_ids)\n\n    @classmethod\n    def create(cls,\n               intent_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # Decoder params\n               embedding_size=768, clf_dropout=0.3,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        clf = ClassDecoder(intent_size, embedding_size, clf_dropout)\n        return cls(embeddings, clf, device)\n\n\nclass BERTBiLSTMAttnClassifier(BERTNerModel):\n\n    def __init__(self, embeddings, lstm, attn, clf, device=""cuda""):\n        super(BERTBiLSTMAttnClassifier, self).__init__()\n        self.embeddings = embeddings\n        self.lstm = lstm\n        self.attn = attn\n        self.clf = clf\n        self.to(device)\n\n    def forward(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        output, _ = self.attn(output, output, output, None)\n        return self.clf(output)\n\n    def score(self, batch):\n        input_, labels_mask, input_type_ids = batch[:3]\n        input_embeddings = self.embeddings(batch)\n        output, _ = self.lstm.forward(input_embeddings, labels_mask)\n        output, _ = self.attn(output, output, output, None)\n        return self.clf.score(output, batch[-1])\n\n    @classmethod\n    def create(cls,\n               intent_size,\n               # BertEmbedder params\n               model_name=\'bert-base-multilingual-cased\', mode=""weighted"", is_freeze=True,\n               # Decoder params\n               clf_dropout=0.3,\n               # BiLSTM\n               hidden_dim=512, rnn_layers=1, lstm_dropout=0.3,\n               # Attn params\n               embedding_size=768, key_dim=64, val_dim=64, num_heads=3, attn_dropout=0.3,\n               # Global params\n               device=""cuda""):\n        embeddings = BERTEmbedder.create(model_name=model_name, device=device, mode=mode, is_freeze=is_freeze)\n        lstm = BiLSTM.create(\n            embedding_size=embedding_size, hidden_dim=hidden_dim, rnn_layers=rnn_layers, dropout=lstm_dropout)\n        attn = MultiHeadAttention(key_dim, val_dim, hidden_dim, num_heads, attn_dropout)\n        clf = ClassDecoder(intent_size, hidden_dim, clf_dropout)\n        return cls(embeddings, lstm, attn, clf, device)\n'"
modules/train/__init__.py,0,b''
modules/train/optimization.py,6,"b'# coding=utf-8\n# https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/optimization.py\n# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n""""""PyTorch optimization for BERT model.""""""\n\nimport math\nimport torch\nfrom torch.optim import Optimizer\nfrom torch.optim.optimizer import required\nfrom torch.nn.utils import clip_grad_norm_\n\n\ndef warmup_cosine(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 0.5 * (1.0 + torch.cos(math.pi * x))\n\n\ndef warmup_constant(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 1.0\n\n\ndef warmup_linear(x, warmup=0.002):\n    if x < warmup:\n        return x/warmup\n    return 1.0 - x\n\n\nSCHEDULES = {\n    \'warmup_cosine\': warmup_cosine,\n    \'warmup_constant\': warmup_constant,\n    \'warmup_linear\': warmup_linear,\n}\n\n\nclass BertAdam(Optimizer):\n    """"""Implements BERT version of Adam algorithm with weight decay fix.\n    Params:\n        lr: learning rate\n        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n        t_total: total number of training steps for the learning\n            rate schedule, -1  means constant learning rate. Default: -1\n        schedule: schedule to use for the warmup (see above). Default: \'warmup_linear\'\n        b1: Adams b1. Default: 0.9\n        b2: Adams b2. Default: 0.999\n        e: Adams epsilon. Default: 1e-6\n        weight_decay: Weight decay. Default: 0.01\n        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n    """"""\n    def __init__(self, model, lr=required, warmup=0.1, t_total=-1, schedule=\'warmup_linear\',\n                 b1=0.8, b2=0.999, e=1e-6, weight_decay=0.01,\n                 max_grad_norm=1.0):\n        if lr is not required and lr < 0.0:\n            raise ValueError(""Invalid learning rate: {} - should be >= 0.0"".format(lr))\n        if schedule not in SCHEDULES:\n            raise ValueError(""Invalid schedule parameter: {}"".format(schedule))\n        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n            raise ValueError(""Invalid warmup: {} - should be in [0.0, 1.0[ or -1"".format(warmup))\n        if not 0.0 <= b1 < 1.0:\n            raise ValueError(""Invalid b1 parameter: {} - should be in [0.0, 1.0["".format(b1))\n        if not 0.0 <= b2 < 1.0:\n            raise ValueError(""Invalid b2 parameter: {} - should be in [0.0, 1.0["".format(b2))\n        if not e >= 0.0:\n            raise ValueError(""Invalid epsilon value: {} - should be >= 0.0"".format(e))\n        defaults = dict(lr=lr, schedule=schedule, warmup=warmup, t_total=t_total,\n                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n                        max_grad_norm=max_grad_norm)\n        # Prepare optimizer\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\'bias\', \'LayerNorm.bias\', \'LayerNorm.weight\']\n        optimizer_grouped_parameters = [\n            {\'params\': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \'weight_decay\': 0.01},\n            {\'params\': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \'weight_decay\': 0.0}\n            ]\n        super(BertAdam, self).__init__(optimizer_grouped_parameters, defaults)\n        self.global_step = 1\n        self.t_total = t_total\n\n    def get_lr(self):\n        lr = []\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                state = self.state[p]\n                if len(state) == 0:\n                    return [0]\n                if group[\'t_total\'] != -1:\n                    schedule_fct = SCHEDULES[group[\'schedule\']]\n                    lr_scheduled = group[\'lr\'] * schedule_fct(state[\'step\'] / group[\'t_total\'], group[\'warmup\'])\n                else:\n                    lr_scheduled = group[\'lr\']\n                lr.append(lr_scheduled)\n        return lr\n\n    def update_lr(self):\n        if 0 < self.t_total:\n            lr_this_step = self.defaults[""lr""] * warmup_linear(self.global_step / self.t_total, self.defaults[""warmup""])\n            for param_group in self.param_groups:\n                param_group[\'lr\'] = lr_this_step\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        self.update_lr()\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\'Adam does not support sparse gradients, please consider SparseAdam instead\')\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'next_m\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'next_v\'] = torch.zeros_like(p.data)\n\n                next_m, next_v = state[\'next_m\'], state[\'next_v\']\n                beta1, beta2 = group[\'b1\'], group[\'b2\']\n\n                # Add grad clipping\n                if group[\'max_grad_norm\'] > 0:\n                    clip_grad_norm_(p, group[\'max_grad_norm\'])\n\n                # Decay the first and second moment running average coefficient\n                # In-place operations to update the averages at the same time\n                next_m.mul_(beta1).add_(1 - beta1, grad)\n                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                update = next_m / (next_v.sqrt() + group[\'e\'])\n\n                # Just adding the square of the weights to the loss function is *not*\n                # the correct way of using L2 regularization/weight decay with Adam,\n                # since that will interact with the m and v parameters in strange ways.\n                #\n                # Instead we want to decay the weights in a manner that doesn\'t interact\n                # with the m/v parameters. This is equivalent to adding the square\n                # of the weights to the loss with plain (non-momentum) SGD.\n                if group[\'weight_decay\'] > 0.0:\n                    update += group[\'weight_decay\'] * p.data\n\n                if group[\'t_total\'] != -1:\n                    schedule_fct = SCHEDULES[group[\'schedule\']]\n                    lr_scheduled = group[\'lr\'] * schedule_fct(state[\'step\']/group[\'t_total\'], group[\'warmup\'])\n                else:\n                    lr_scheduled = group[\'lr\']\n\n                update_with_lr = lr_scheduled * update\n                p.data.add_(-update_with_lr)\n\n                state[\'step\'] += 1\n\n                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n                # No bias correction\n                # bias_correction1 = 1 - beta1 ** state[\'step\']\n                # bias_correction2 = 1 - beta2 ** state[\'step\']\n        self.global_step += 1\n        return loss\n'"
modules/train/train.py,3,"b'from modules import tqdm\nfrom sklearn_crfsuite.metrics import flat_classification_report\nimport logging\nimport torch\nfrom .optimization import BertAdam\nfrom modules.analyze_utils.plot_metrics import get_mean_max_metric\nfrom modules.data.bert_data import get_data_loader_for_predict\n\n\ndef train_step(dl, model, optimizer, num_epoch=1):\n    model.train()\n    epoch_loss = 0\n    idx = 0\n    pr = tqdm(dl, total=len(dl), leave=False)\n    for batch in pr:\n        idx += 1\n        model.zero_grad()\n        loss = model.score(batch)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        loss = loss.data.cpu().tolist()\n        epoch_loss += loss\n        pr.set_description(""train loss: {}"".format(epoch_loss / idx))\n        torch.cuda.empty_cache()\n    logging.info(""\\nepoch {}, average train epoch loss={:.5}\\n"".format(\n        num_epoch, epoch_loss / idx))\n\n\ndef transformed_result(preds, mask, id2label, target_all=None, pad_idx=0):\n    preds_cpu = []\n    targets_cpu = []\n    lc = len(id2label)\n    if target_all is not None:\n        for batch_p, batch_t, batch_m in zip(preds, target_all, mask):\n            for pred, true_, bm in zip(batch_p, batch_t, batch_m):\n                sent = []\n                sent_t = []\n                bm = bm.sum().cpu().data.tolist()\n                for p, t in zip(pred[:bm], true_[:bm]):\n                    p = p.cpu().data.tolist()\n                    p = p if p < lc else pad_idx\n                    sent.append(p)\n                    sent_t.append(t.cpu().data.tolist())\n                preds_cpu.append([id2label[w] for w in sent])\n                targets_cpu.append([id2label[w] for w in sent_t])\n    else:\n        for batch_p, batch_m in zip(preds, mask):\n            \n            for pred, bm in zip(batch_p, batch_m):\n                assert len(pred) == len(bm)\n                bm = bm.sum().cpu().data.tolist()\n                sent = pred[:bm].cpu().data.tolist()\n                preds_cpu.append([id2label[w] for w in sent])\n    if target_all is not None:\n        return preds_cpu, targets_cpu\n    else:\n        return preds_cpu\n\n    \ndef transformed_result_cls(preds, target_all, cls2label, return_target=True):\n    preds_cpu = []\n    targets_cpu = []\n    for batch_p, batch_t in zip(preds, target_all):\n        for pred, true_ in zip(batch_p, batch_t):\n            preds_cpu.append(cls2label[pred.cpu().data.tolist()])\n            if return_target:\n                targets_cpu.append(cls2label[true_.cpu().data.tolist()])\n    if return_target:\n        return preds_cpu, targets_cpu\n    return preds_cpu\n\n\ndef validate_step(dl, model, id2label, sup_labels, id2cls=None):\n    model.eval()\n    idx = 0\n    preds_cpu, targets_cpu = [], []\n    preds_cpu_cls, targets_cpu_cls = [], []\n    for batch in tqdm(dl, total=len(dl), leave=False):\n        idx += 1\n        labels_mask, labels_ids = batch[1], batch[3]\n        preds = model.forward(batch)\n        if id2cls is not None:\n            preds, preds_cls = preds\n            preds_cpu_, targets_cpu_ = transformed_result_cls([preds_cls], [batch[-1]], id2cls)\n            preds_cpu_cls.extend(preds_cpu_)\n            targets_cpu_cls.extend(targets_cpu_)\n        preds_cpu_, targets_cpu_ = transformed_result([preds], [labels_mask], id2label, [labels_ids])\n        preds_cpu.extend(preds_cpu_)\n        targets_cpu.extend(targets_cpu_)\n    clf_report = flat_classification_report(targets_cpu, preds_cpu, labels=sup_labels, digits=3)\n    if id2cls is not None:\n        clf_report_cls = flat_classification_report([targets_cpu_cls], [preds_cpu_cls], digits=3)\n        return clf_report, clf_report_cls\n    return clf_report\n\n\ndef predict(dl, model, id2label, id2cls=None):\n    model.eval()\n    idx = 0\n    preds_cpu = []\n    preds_cpu_cls = []\n    for batch in tqdm(dl, total=len(dl), leave=False, desc=""Predicting""):\n        idx += 1\n        labels_mask, labels_ids = batch[1], batch[3]\n        preds = model.forward(batch)\n        if id2cls is not None:\n            preds, preds_cls = preds\n            preds_cpu_ = transformed_result_cls([preds_cls], [preds_cls], id2cls, False)\n            preds_cpu_cls.extend(preds_cpu_)\n\n        preds_cpu_ = transformed_result([preds], [labels_mask], id2label)\n        preds_cpu.extend(preds_cpu_)\n    if id2cls is not None:\n        return preds_cpu, preds_cpu_cls\n    return preds_cpu\n\n\nclass NerLearner(object):\n\n    def __init__(self, model, data, best_model_path, lr=0.001, betas=[0.8, 0.9], clip=1.0,\n                 verbose=True, sup_labels=None, t_total=-1, warmup=0.1, weight_decay=0.01,\n                 validate_every=1, schedule=""warmup_linear"", e=1e-6):\n        logging.basicConfig(level=logging.INFO)\n        self.model = model\n        self.optimizer = BertAdam(model, lr, t_total=t_total, b1=betas[0], b2=betas[1], max_grad_norm=clip)\n        self.optimizer_defaults = dict(\n            model=model, lr=lr, warmup=warmup, t_total=t_total, schedule=schedule,\n            b1=betas[0], b2=betas[1], e=e, weight_decay=weight_decay,\n            max_grad_norm=clip)\n\n        self.lr = lr\n        self.betas = betas\n        self.clip = clip\n        self.sup_labels = sup_labels\n        self.t_total = t_total\n        self.warmup = warmup\n        self.weight_decay = weight_decay\n        self.validate_every = validate_every\n        self.schedule = schedule\n        self.data = data\n        self.e = e\n        if sup_labels is None:\n            sup_labels = data.train_ds.idx2label[4:]\n        self.sup_labels = sup_labels\n        self.best_model_path = best_model_path\n        self.verbose = verbose\n        self.history = []\n        self.cls_history = []\n        self.epoch = 0\n        self.best_target_metric = 0.\n\n    def fit(self, epochs=100, resume_history=True, target_metric=""f1""):\n        if not resume_history:\n            self.optimizer_defaults[""t_total""] = epochs * len(self.data.train_dl)\n            self.optimizer = BertAdam(**self.optimizer_defaults)\n            self.history = []\n            self.cls_history = []\n            self.epoch = 0\n            self.best_target_metric = 0.\n        elif self.verbose:\n            logging.info(""Resuming train... Current epoch {}."".format(self.epoch))\n        try:\n            for _ in range(epochs):\n                self.epoch += 1\n                self.fit_one_cycle(self.epoch, target_metric)\n        except KeyboardInterrupt:\n            pass\n\n    def fit_one_cycle(self, epoch, target_metric=""f1""):\n        train_step(self.data.train_dl, self.model, self.optimizer, epoch)\n        if epoch % self.validate_every == 0:\n            if self.data.train_ds.is_cls:\n                rep, rep_cls = validate_step(\n                    self.data.valid_dl, self.model, self.data.train_ds.idx2label, self.sup_labels,\n                    self.data.train_ds.idx2cls)\n                self.cls_history.append(rep_cls)\n            else:\n                rep = validate_step(\n                    self.data.valid_dl, self.model, self.data.train_ds.idx2label, self.sup_labels)\n            self.history.append(rep)\n        idx, metric = get_mean_max_metric(self.history, target_metric, True)\n        if self.verbose:\n            logging.info(""on epoch {} by max_{}: {}"".format(idx, target_metric, metric))\n            print(self.history[-1])\n            if self.data.train_ds.is_cls:\n                logging.info(""on epoch {} classification report:"")\n                print(self.cls_history[-1])\n        # Store best model\n        if self.best_target_metric < metric:\n            self.best_target_metric = metric\n            if self.verbose:\n                logging.info(""Saving new best model..."")\n            self.save_model()\n\n    def predict(self, dl=None, df_path=None, df=None):\n        if dl is None:\n            dl = get_data_loader_for_predict(self.data, df_path, df)\n        if self.data.train_ds.is_cls:\n            return predict(dl, self.model, self.data.train_ds.idx2label, self.data.train_ds.idx2cls)\n        return predict(dl, self.model, self.data.train_ds.idx2label)\n    \n    def save_model(self, path=None):\n        path = path if path else self.best_model_path\n        torch.save(self.model.state_dict(), path)\n    \n    def load_model(self, path=None):\n        path = path if path else self.best_model_path\n        self.model.load_state_dict(torch.load(path))\n'"
modules/train/train_clf.py,3,"b'from modules import tqdm\nfrom sklearn_crfsuite.metrics import flat_classification_report\nimport logging\nimport torch\nfrom .optimization import BertAdam\nfrom modules.analyze_utils.plot_metrics import get_mean_max_metric\nfrom modules.data.bert_data_clf import get_data_loader_for_predict\n\n\ndef train_step(dl, model, optimizer, num_epoch=1):\n    model.train()\n    epoch_loss = 0\n    idx = 0\n    pr = tqdm(dl, total=len(dl), leave=False)\n    for batch in pr:\n        idx += 1\n        model.zero_grad()\n        loss = model.score(batch)\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        loss = loss.data.cpu().tolist()\n        epoch_loss += loss\n        pr.set_description(""train loss: {}"".format(epoch_loss / idx))\n        torch.cuda.empty_cache()\n    logging.info(""\\nepoch {}, average train epoch loss={:.5}\\n"".format(\n        num_epoch, epoch_loss / idx))\n\n\ndef transformed_result_cls(preds, target_all, cls2label, return_target=True):\n    preds_cpu = []\n    targets_cpu = []\n    for batch_p, batch_t in zip(preds, target_all):\n        for pred, true_ in zip(batch_p, batch_t):\n            preds_cpu.append(cls2label[pred.cpu().data.tolist()])\n            if return_target:\n                targets_cpu.append(cls2label[true_.cpu().data.tolist()])\n    if return_target:\n        return preds_cpu, targets_cpu\n    return preds_cpu\n\n\ndef validate_step(dl, model, id2cls):\n    model.eval()\n    idx = 0\n    preds_cpu_cls, targets_cpu_cls = [], []\n    for batch in tqdm(dl, total=len(dl), leave=False, desc=""Validation""):\n        idx += 1\n        preds_cls = model.forward(batch)\n        preds_cpu_, targets_cpu_ = transformed_result_cls([preds_cls], [batch[-1]], id2cls)\n        preds_cpu_cls.extend(preds_cpu_)\n        targets_cpu_cls.extend(targets_cpu_)\n    clf_report_cls = flat_classification_report([targets_cpu_cls], [preds_cpu_cls], digits=4)\n    return clf_report_cls\n\n\ndef predict(dl, model, id2cls):\n    model.eval()\n    idx = 0\n    preds_cpu_cls = []\n    for batch in tqdm(dl, total=len(dl), leave=False, desc=""Predicting""):\n        idx += 1\n        preds_cls = model.forward(batch)\n        preds_cpu_ = transformed_result_cls([preds_cls], [preds_cls], id2cls, False)\n        preds_cpu_cls.extend(preds_cpu_)\n\n    return preds_cpu_cls\n\n\nclass NerLearner(object):\n\n    def __init__(self, model, data, best_model_path, lr=0.001, betas=[0.8, 0.9], clip=1.0,\n                 verbose=True, t_total=-1, warmup=0.1, weight_decay=0.01,\n                 validate_every=1, schedule=""warmup_linear"", e=1e-6):\n        logging.basicConfig(level=logging.INFO)\n        self.model = model\n        self.optimizer = BertAdam(model, lr, t_total=t_total, b1=betas[0], b2=betas[1], max_grad_norm=clip)\n        self.optimizer_defaults = dict(\n            model=model, lr=lr, warmup=warmup, t_total=t_total, schedule=schedule,\n            b1=betas[0], b2=betas[1], e=e, weight_decay=weight_decay,\n            max_grad_norm=clip)\n\n        self.lr = lr\n        self.betas = betas\n        self.clip = clip\n        self.t_total = t_total\n        self.warmup = warmup\n        self.weight_decay = weight_decay\n        self.validate_every = validate_every\n        self.schedule = schedule\n        self.data = data\n        self.e = e\n        self.best_model_path = best_model_path\n        self.verbose = verbose\n        self.cls_history = []\n        self.epoch = 0\n        self.best_target_metric = 0.\n\n    def fit(self, epochs=100, resume_history=True, target_metric=""f1""):\n        if not resume_history:\n            self.optimizer_defaults[""t_total""] = epochs * len(self.data.train_dl)\n            self.optimizer = BertAdam(**self.optimizer_defaults)\n            self.cls_history = []\n            self.epoch = 0\n            self.best_target_metric = 0.\n        elif self.verbose:\n            logging.info(""Resuming train... Current epoch {}."".format(self.epoch))\n        try:\n            for _ in range(epochs):\n                self.epoch += 1\n                self.fit_one_cycle(self.epoch, target_metric)\n        except KeyboardInterrupt:\n            pass\n\n    def fit_one_cycle(self, epoch, target_metric=""f1""):\n        train_step(self.data.train_dl, self.model, self.optimizer, epoch)\n        if epoch % self.validate_every == 0:\n            rep_cls = validate_step(self.data.valid_dl, self.model, self.data.train_ds.idx2cls)\n            self.cls_history.append(rep_cls)\n        idx, metric = get_mean_max_metric(self.cls_history, target_metric, True)\n        if self.verbose:\n            logging.info(""on epoch {} by max_{}: {}"".format(idx, target_metric, metric))\n            print(self.cls_history[-1])\n\n        # Store best model\n        if self.best_target_metric < metric:\n            self.best_target_metric = metric\n            if self.verbose:\n                logging.info(""Saving new best model..."")\n            self.save_model()\n\n    def predict(self, dl=None, df_path=None, df=None):\n        if dl is None:\n            dl, ds = get_data_loader_for_predict(self.data, df_path, df)\n        return predict(dl, self.model, self.data.train_ds.idx2cls)\n    \n    def save_model(self, path=None):\n        path = path if path else self.best_model_path\n        torch.save(self.model.state_dict(), path)\n    \n    def load_model(self, path=None):\n        path = path if path else self.best_model_path\n        self.model.load_state_dict(torch.load(path))\n'"
modules/data/conll2003/__init__.py,0,"b'from .prc import conll2003_preprocess\n\n\n__all__ = [""conll2003_preprocess""]\n'"
modules/data/conll2003/prc.py,0,"b'import pandas as pd\nfrom modules import tqdm\nimport argparse\nimport codecs\nimport os\n\n\ndef conll2003_preprocess(\n        data_dir, train_name=""eng.train"", dev_name=""eng.testa"", test_name=""eng.testb""):\n    train_f = read_data(os.path.join(data_dir, train_name))\n    dev_f = read_data(os.path.join(data_dir, dev_name))\n    test_f = read_data(os.path.join(data_dir, test_name))\n\n    train = pd.DataFrame({""labels"": [x[0] for x in train_f], ""text"": [x[1] for x in train_f]})\n    train[""cls""] = train[""labels""].apply(lambda x: all([y.split(""_"")[0] == ""O"" for y in x.split()]))\n    train.to_csv(os.path.join(data_dir, ""{}.train.csv"".format(train_name)), index=False, sep=""\\t"")\n\n    dev = pd.DataFrame({""labels"": [x[0] for x in dev_f], ""text"": [x[1] for x in dev_f]})\n    dev[""cls""] = dev[""labels""].apply(lambda x: all([y.split(""_"")[0] == ""O"" for y in x.split()]))\n    dev.to_csv(os.path.join(data_dir, ""{}.dev.csv"".format(dev_name)), index=False, sep=""\\t"")\n\n    test_ = pd.DataFrame({""labels"": [x[0] for x in test_f], ""text"": [x[1] for x in test_f]})\n    test_[""cls""] = test_[""labels""].apply(lambda x: all([y.split(""_"")[0] == ""O"" for y in x.split()]))\n    test_.to_csv(os.path.join(data_dir, ""{}.dev.csv"".format(test_name)), index=False, sep=""\\t"")\n\n\ndef read_data(input_file):\n    """"""Reads a BIO data.""""""\n    with codecs.open(input_file, ""r"", encoding=""utf-8"") as f:\n        lines = []\n        words = []\n        labels = []\n        f_lines = f.readlines()\n        for line in tqdm(f_lines, total=len(f_lines), desc=""Process {}"".format(input_file)):\n            contends = line.strip()\n            word = line.strip().split(\' \')[0]\n            label = line.strip().split(\' \')[-1]\n            if contends.startswith(""-DOCSTART-""):\n                words.append(\'\')\n                continue\n\n            if len(contends) == 0 and not len(words):\n                words.append("""")\n\n            if len(contends) == 0 and words[-1] == \'.\':\n                lbl = \' \'.join([label for label in labels if len(label) > 0])\n                w = \' \'.join([word for word in words if len(word) > 0])\n                lines.append([lbl, w])\n                words = []\n                labels = []\n                continue\n            words.append(word)\n            labels.append(label.replace(""-"", ""_""))\n        return lines\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_dir\', type=str)\n    parser.add_argument(\'--train_name\', type=str, default=""eng.train"")\n    parser.add_argument(\'--dev_name\', type=str, default=""eng.testa"")\n    parser.add_argument(\'--test_name\', type=str, default=""eng.testb"")\n    return vars(parser.parse_args())\n\n\nif __name__ == ""__main__"":\n    conll2003_preprocess(**parse_args())\n'"
modules/data/fre/__init__.py,0,"b'# -*- coding: utf-8 -*-\nfrom .reader import Reader as FREReader\nfrom .prc import fact_ru_eval_preprocess\n\n__all__ = [""FREReader"", ""fact_ru_eval_preprocess""]\n'"
modules/data/fre/prc.py,0,"b'from modules.data.fre.reader import Reader\nimport pandas as pd\nfrom modules import tqdm\nimport argparse\n\n\ndef fact_ru_eval_preprocess(dev_dir, test_dir, dev_df_path, test_df_path):\n    dev_reader = Reader(dev_dir)\n    dev_reader.read_dir()\n    dev_texts, dev_tags = dev_reader.split()\n    res_tags = []\n    res_tokens = []\n    for tag, tokens in tqdm(zip(dev_tags, dev_texts), total=len(dev_tags), desc=""Process FactRuEval2016 dev set.""):\n        if len(tag):\n            res_tags.append(tag)\n            res_tokens.append(tokens)\n    dev = pd.DataFrame({""labels"": list(map("" "".join, res_tags)), ""text"": list(map("" "".join, res_tokens))})\n    dev[""clf""] = dev[""labels""].apply(lambda x: all([y.split(""_"")[0] == ""O"" for y in x.split()]))\n    dev.to_csv(dev_df_path, index=False, sep=""\\t"")\n\n    test_reader = Reader(test_dir)\n    test_reader.read_dir()\n    test_texts, test_tags = test_reader.split()\n    res_tags = []\n    res_tokens = []\n    for tag, tokens in tqdm(zip(test_tags, test_texts), total=len(test_tags), desc=""Process FactRuEval2016 test set.""):\n        if len(tag):\n            res_tags.append(tag)\n            res_tokens.append(tokens)\n    valid = pd.DataFrame({""labels"": list(map("" "".join, res_tags)), ""text"": list(map("" "".join, res_tokens))})\n    valid[""clf""] = valid[""labels""].apply(lambda x: all([y.split(""_"")[0] == ""O"" for y in x.split()]))\n    valid.to_csv(test_df_path, index=False, sep=""\\t"")\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-dd\', \'--dev_dir\', type=str)\n    parser.add_argument(\'-td\', \'--test_dir\', type=str)\n    parser.add_argument(\'-ddp\', \'--dev_df_path\', type=str)\n    parser.add_argument(\'-tdp\', \'--test_df_path\', type=str)\n    return vars(parser.parse_args())\n\n\nif __name__ == ""__main__"":\n    fact_ru_eval_preprocess(**parse_args())\n'"
modules/data/fre/reader.py,0,"b'# -*- coding: utf-8 -*-\nimport pandas as pd\nfrom .utils import get_file_names\nfrom .entity.document import Document\n\n\nclass Reader(object):\n\n    def __init__(self,\n                 dir_path,\n                 document_creator=Document,\n                 get_file_names_=get_file_names,\n                 tagged=True):\n        self.path = dir_path\n        self.tagged = tagged\n        self.documents = []\n        self.document_creator = document_creator\n        self.get_file_names = get_file_names_\n\n    def split(self, use_morph=False):\n        res_texts = []\n        res_tags = []\n        for doc in self.documents:\n            sent_tokens = []\n            sent_tags = []\n            for token in doc.tagged_tokens:\n                if token.get_tag() == ""O"" and token.text == ""."":\n                    res_texts.append(tuple(sent_tokens))\n                    res_tags.append(tuple(sent_tags))\n                    sent_tokens = []\n                    sent_tags = []\n                else:\n                    text = token.text\n                    sent_tokens.append(text)\n                    sent_tags.append(token.get_tag())\n        if use_morph:\n            return res_texts, res_tags\n        return res_texts, res_tags\n\n    def to_data_frame(self, split=False):\n        if split:\n            docs = self.split()\n        else:\n            docs = []\n            for doc in self.documents:\n                docs.append([(token.text, token.get_tag()) for token in doc.tagged_tokens])\n\n        texts = []\n        tags = []\n        for sent in docs:\n            sample_text = []\n            sample_tag = []\n            for text, tag in sent:\n                sample_text.append(text)\n                sample_tag.append(tag)\n            texts.append("" "".join(sample_text))\n            tags.append("" "".join(sample_tag))\n        return pd.DataFrame({""texts"": texts, ""tags"": tags}, columns=[""texts"", ""tags""])\n\n    def read_dir(self):\n        for path in self.get_file_names(self.path):\n            self.documents.append(self.document_creator(path, self.tagged))\n\n    def get_text_tokens(self):\n        return [doc.to_text_tokens() for doc in self.documents]\n\n    def get_text_tags(self):\n        return [doc.get_tags() for doc in self.documents]\n'"
modules/data/fre/utils.py,0,"b""import os\n\n\ndef get_file_names(path):\n    res = []\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            if file.endswith('.tokens'):\n                res.append(os.path.join(root, os.path.splitext(file)[0]))\n    return res\n"""
modules/data/fre/bilou/__init__.py,0,b'\n'
modules/data/fre/bilou/from_bilou.py,0,"b'# -*- coding: utf-8 -*-\n\n\ndef untag(list_of_tags, list_of_tokens):\n    """"""\n    :param list_of_tags:\n    :param list_of_tokens:\n    :return:\n    """"""\n    if len(list_of_tags) == len(list_of_tokens):\n        dict_of_final_ne = {}\n        ne_words = []\n        ne_tag = None\n\n        for index in range(len(list_of_tokens)):\n            if not ((ne_tag is not None) ^ (ne_words != [])):\n                current_tag = list_of_tags[index]\n                current_token = list_of_tokens[index]\n\n                if current_tag.startswith(\'B\') or current_tag.startswith(\'I\'):\n                    dict_of_final_ne, ne_words, ne_tag = __check_bi(\n                        dict_of_final_ne, ne_words, ne_tag, current_tag, current_token)\n                elif current_tag.startswith(\'L\'):\n                    dict_of_final_ne, ne_words, ne_tag = __check_l(\n                        dict_of_final_ne, ne_words, ne_tag, current_tag, current_token)\n                elif current_tag.startswith(\'O\'):\n                    dict_of_final_ne, ne_words, ne_tag = __finish_ne_if_required(dict_of_final_ne, ne_words, ne_tag)\n\n                elif current_tag.startswith(\'U\'):\n                    dict_of_final_ne, ne_words, ne_tag = __check_u(dict_of_final_ne, ne_words, ne_tag, current_tag,\n                                                                   current_token)\n                else:\n                    raise ValueError(""tag contains no BILOU tags"")\n            else:\n                if ne_tag is None:\n                    raise Exception(\'Somehow ne_tag is None and ne_words is not None\')\n                else:\n                    raise Exception(\'Somehow ne_words is None and ne_tag is not None\')\n\n        dict_of_final_ne, ne_words, ne_tag = __finish_ne_if_required(dict_of_final_ne, ne_words, ne_tag)\n        return __to_output_format(dict_of_final_ne)\n    else:\n        raise ValueError(\'lengths are not equal\')\n\n\ndef __check_bi(dict_of_final_ne, ne_words, ne_tag, current_tag, current_token):\n    if ne_tag is None and ne_words == []:\n        ne_tag = current_tag[1:]\n        ne_words = [current_token]\n    else:\n        if current_tag.startswith(\'I\') and ne_tag == current_tag[1:]:\n            ne_words.append(current_token)\n        else:\n            dict_of_final_ne, ne_words, ne_tag = __replace_by_new(dict_of_final_ne, ne_words, ne_tag, current_tag,\n                                                                  current_token)\n    return dict_of_final_ne, ne_words, ne_tag\n\n\ndef __check_l(dict_of_final_ne, ne_words, ne_tag, current_tag, current_token):\n    if ne_tag == current_tag[1:]:\n        dict_of_final_ne, ne_words, ne_tag = __finish_ne_if_required(dict_of_final_ne, ne_words+[current_token], ne_tag)\n    else:\n        dict_of_final_ne, ne_words, ne_tag = __finish_ne_if_required(dict_of_final_ne, ne_words, ne_tag)\n        dict_of_final_ne, ne_words, ne_tag = __finish_ne_if_required(dict_of_final_ne, [current_token], current_tag[1:])\n    return dict_of_final_ne, ne_words, ne_tag\n\n\ndef __check_u(dict_of_final_ne, ne_words, ne_tag, current_tag, current_token):\n    dict_of_final_ne, ne_words, ne_tag = __finish_ne_if_required(dict_of_final_ne, ne_words, ne_tag)\n    return __finish_ne_if_required(dict_of_final_ne, [current_token], current_tag[1:])\n\n\ndef __replace_by_new(dict_of_final_ne, ne_words, ne_tag, current_tag, current_token):\n    dict_of_final_ne, ne_words, ne_tag = __finish_ne_if_required(dict_of_final_ne, ne_words, ne_tag)\n    ne_tag = current_tag[1:]\n    ne_words = [current_token]\n    return dict_of_final_ne, ne_words, ne_tag\n\n\ndef __finish_ne_if_required(dict_of_final_ne, ne_words, ne_tag):\n    if ne_tag is not None and ne_words != []:\n        dict_of_final_ne[tuple(ne_words)] = ne_tag\n        ne_tag = None\n        ne_words = []\n    return dict_of_final_ne, ne_words, ne_tag\n\n\ndef __to_output_format(dict_nes):\n    """"""\n    :param dict_nes:\n    :return:\n    """"""\n    list_of_results_for_output = []\n\n    for tokens_tuple, tag in dict_nes.items():\n        position = int(tokens_tuple[0].get_position())\n        length = int(tokens_tuple[-1].get_position()) + int(tokens_tuple[-1].get_length()) - position\n        list_of_results_for_output.append([tag, position, length])\n\n    return list_of_results_for_output\n'"
modules/data/fre/bilou/to_bilou.py,0,"b'# -*- coding: utf-8 -*-\nfrom ..entity.taggedtoken import TaggedToken\n\n\ndef get_tagged_tokens_from(dict_of_nes, token_list):\n    list_of_tagged_tokens = [TaggedToken(\'O\', token_list[i]) for i in range(len(token_list))]\n    dict_of_tokens_with_indexes = {token_list[i].id: i for i in range(len(token_list))}\n\n    for ne in dict_of_nes.values():\n        for tokenid in ne[\'tokens_list\']:\n            try:\n                tag = format_tag(tokenid, ne)\n            except ValueError:\n                tag = ""O""\n            id_in_token_tuple = dict_of_tokens_with_indexes[tokenid]\n            token = token_list[id_in_token_tuple]\n            list_of_tagged_tokens[id_in_token_tuple] = TaggedToken(tag, token)\n    return list_of_tagged_tokens\n\n\ndef format_tag(tokenid, ne):\n    bilou = __choose_bilou_tag_for(tokenid, ne[\'tokens_list\'])\n    formatted_tag = __tag_to_fact_ru_eval_format(ne[\'tag\'])\n    return ""{}_{}"".format(bilou, formatted_tag)\n\n\ndef __choose_bilou_tag_for(token_id, token_list):\n    if len(token_list) == 1:\n        return \'B\'\n    elif len(token_list) > 1:\n        if token_list.index(token_id) == 0:\n            return \'B\'\n        else:\n            return \'I\'\n\n\ndef __tag_to_fact_ru_eval_format(tag):\n    if tag == \'Person\':\n        return \'PER\'\n    elif tag == \'Org\':\n        return \'ORG\'\n    elif tag == \'Location\':\n        return \'LOC\'\n    elif tag == \'LocOrg\':\n        return \'LOC\'\n    elif tag == \'Project\':\n        return \'ORG\'\n    else:\n        raise ValueError(\'tag \' + tag + "" is not the right tag"")\n'"
modules/data/fre/entity/__init__.py,0,b'# -*- coding: utf-8 -*-\n'
modules/data/fre/entity/document.py,0,"b'import codecs\nfrom .token import Token\nfrom .taggedtoken import TaggedToken\nfrom collections import defaultdict\nfrom ..bilou import to_bilou\n\n\nclass Document(object):\n    def __init__(self, path, tagged=True, encoding=""utf-8""):\n        self.path = path\n        self.tagged = tagged\n        self.encoding = encoding\n        self.tokens = []\n        self.tagged_tokens = []\n        self.load()\n\n    def to_text_tokens(self):\n        return [token.text for token in self.tokens]\n\n    def get_tags(self):\n        return [token.get_tag() for token in self.tagged_tokens]\n\n    def load(self):\n        self.tokens = self.__get_tokens_from_file()\n        if self.tagged:\n            self.tagged_tokens = self.__get_tagged_tokens_from()\n        else:\n            self.tagged_tokens = [TaggedToken(None, token) for token in self.tokens]\n        return self\n\n    def parse_file(self, path):\n        with codecs.open(path, \'r\', encoding=self.encoding, errors=""ignore"") as file:\n            rows = file.read().split(\'\\n\')\n        return [row.split(\' # \')[0].split() for row in rows if len(row) != 0]\n\n    def __get_tokens_from_file(self):\n        rows = self.parse_file(self.path + \'.tokens\')\n        tokens = []\n        for token_str in rows:\n            tokens.append(Token().from_sting(token_str))\n        return tokens\n\n    def __get_tagged_tokens_from(self):\n        span_dict = self.__span_id2token_ids(self.path + \'.spans\', [token.id for token in self.tokens])\n        object_dict = self.__to_dict_of_objects(self.path + \'.objects\')\n        dict_of_nes = self.__merge(object_dict, span_dict, self.tokens)\n        return to_bilou.get_tagged_tokens_from(dict_of_nes, self.tokens)\n\n    def __span_id2token_ids(self, span_file, token_ids):\n        span_list = self.parse_file(span_file)\n        dict_of_spans = {}\n        for span in span_list:\n            span_id = span[0]\n            span_start = span[4]\n            span_length_in_tokens = int(span[5])\n            list_of_token_of_spans = self.__find_tokens_for(span_start, span_length_in_tokens, token_ids)\n            dict_of_spans[span_id] = list_of_token_of_spans\n        return dict_of_spans\n\n    @staticmethod\n    def __find_tokens_for(start, length, token_ids):\n        list_of_tokens = []\n        index = token_ids.index(start)\n        for i in range(length):\n            list_of_tokens.append(token_ids[index + i])\n        return list_of_tokens\n\n    def __to_dict_of_objects(self, object_file):\n        object_list = self.parse_file(object_file)\n        dict_of_objects = {}\n        for obj in object_list:\n            object_id = obj[0]\n            object_tag = obj[1]\n            object_spans = obj[2:]\n            dict_of_objects[object_id] = {\'tag\': object_tag, \'spans\': object_spans}\n        return dict_of_objects\n\n    def __merge(self, object_dict, span_dict, tokens):\n        ne_dict = self.__get_dict_of_nes(object_dict, span_dict)\n        return self.__clean(ne_dict, tokens)\n\n    @staticmethod\n    def __get_dict_of_nes(object_dict, span_dict):\n        ne_dict = defaultdict(set)\n        for obj_id, obj_values in object_dict.items():\n            for span in obj_values[\'spans\']:\n                ne_dict[(obj_id, obj_values[\'tag\'])].update(span_dict[span])\n        for ne in ne_dict:\n            ne_dict[ne] = sorted(list(set([int(i) for i in ne_dict[ne]])))\n        return ne_dict\n\n    def __clean(self, ne_dict, tokens):\n        sorted_nes = sorted(ne_dict.items(), key=self.__sort_by_tokens)\n        dict_of_tokens_by_id = {}\n        for i in range(len(tokens)):\n            dict_of_tokens_by_id[tokens[i].id] = i\n        result_nes = {}\n        if len(sorted_nes) != 0:\n            start_ne = sorted_nes[0]\n            for ne in sorted_nes:\n                if self.__not_intersect(start_ne[1], ne[1]):\n                    result_nes[start_ne[0][0]] = {\n                        \'tokens_list\': self.__check_order(start_ne[1], dict_of_tokens_by_id, tokens),\n                        \'tag\': start_ne[0][1]}\n                    start_ne = ne\n                else:\n                    result_tokens_list = self.__check_normal_form(start_ne[1], ne[1])\n                    start_ne = (start_ne[0], result_tokens_list)\n            result_nes[start_ne[0][0]] = {\n                \'tokens_list\': self.__check_order(start_ne[1], dict_of_tokens_by_id, tokens),\n                \'tag\': start_ne[0][1]}\n        return result_nes\n\n    @staticmethod\n    def __sort_by_tokens(tokens):\n        ids_as_int = [int(token_id) for token_id in tokens[1]]\n        return min(ids_as_int), -max(ids_as_int)\n\n    @staticmethod\n    def __not_intersect(start_ne, current_ne):\n        intersection = set.intersection(set(start_ne), set(current_ne))\n        return intersection == set()\n\n    def __check_normal_form(self, start_ne, ne):\n        all_tokens = set.union(set(start_ne), set(ne))\n        return self.__find_all_range_of_tokens(all_tokens)\n\n    @staticmethod\n    def __find_all_range_of_tokens(tokens):\n        tokens = sorted(tokens)\n        if (tokens[-1] - tokens[0] - len(tokens)) < 5:\n            return list(range(tokens[0], tokens[-1] + 1))\n        else:\n            return tokens\n\n    def __check_order(self, list_of_tokens, dict_of_tokens_by_id, tokens):\n        list_of_tokens = [str(i) for i in self.__find_all_range_of_tokens(list_of_tokens)]\n        result = []\n        for token in list_of_tokens:\n            if token in dict_of_tokens_by_id:\n                result.append((token, dict_of_tokens_by_id[token]))\n        result = sorted(result, key=self.__sort_by_position)\n        result = self.__add_quotation_marks(result, tokens)\n        return [r[0] for r in result]\n\n    @staticmethod\n    def __sort_by_position(result_tuple):\n        return result_tuple[1]\n\n    @staticmethod\n    def __add_quotation_marks(result, tokens):\n        result_tokens_texts = [tokens[token[1]].text for token in result]\n        prev_pos = result[0][1] - 1\n        next_pos = result[-1][1] + 1\n\n        if prev_pos >= 0 and tokens[prev_pos].text == \'\xc2\xab\' \\\n                and \'\xc2\xbb\' in result_tokens_texts and \'\xc2\xab\' not in result_tokens_texts:\n            result = [(tokens[prev_pos].id, prev_pos)] + result\n\n        if next_pos < len(tokens) and tokens[next_pos].text == \'\xc2\xbb\' \\\n                and \'\xc2\xab\' in result_tokens_texts and \'\xc2\xbb\' not in result_tokens_texts:\n            result = result + [(tokens[next_pos].id, next_pos)]\n\n        return result\n'"
modules/data/fre/entity/taggedtoken.py,0,"b'# -*- coding: utf-8 -*-\n\n\nclass TaggedToken(object):\n\n    @property\n    def text(self):\n        return self.__token.text\n\n    def __init__(self, tag, token):\n        self.__tag = tag\n        self.__token = token\n\n    def get_token(self):\n        return self.__token\n\n    def get_tag(self):\n        return self.__tag\n\n    def __repr__(self):\n        if self.__tag:\n            return ""<"" + self.__tag + ""_"" + str(self.__token) + "">""\n        else:\n            return ""<None_"" + str(self.__token) + "">""\n'"
modules/data/fre/entity/token.py,0,"b'# -*- coding: utf-8 -*-\n\n\nclass Token(object):\n    __token_id__ = 0\n\n    @property\n    def length(self):\n        return self.__length\n\n    @property\n    def position(self):\n        return self.__position\n\n    @property\n    def id(self):\n        return self.__id\n\n    @property\n    def text(self):\n        return self.__text\n\n    @property\n    def all(self):\n        return self.__id, self.__position, self.__length, self.__text\n\n    @property\n    def tag(self):\n        return self.tag\n\n    def __init__(self, token_id=None, position=None, length=None, text=None):\n        self.__id = token_id\n        if token_id is None:\n            self.__id = Token.__token_id__\n            Token.__token_id__ += 1\n        self.__position = position\n        self.__length = length\n        self.__text = text\n        self.__tag = None\n\n    def from_sting(self, string):\n        self.__id, self.__position, self.__length, self.__text = string\n        return self\n\n    def __len__(self):\n        return self.__length\n\n    def __str__(self):\n        return self.__text\n\n    def __repr__(self):\n        return ""<<"" + self.__id + ""_"" + self.__text + "">>""\n'"
