file_path,api_count,code
beamdecode.py,3,"b'import _init_path\nimport torch\nimport feature\nfrom models.conv import GatedConv\nimport torch.nn.functional as F\nfrom ctcdecode import CTCBeamDecoder\n\nalpha = 0.8\nbeta = 0.3\nlm_path = ""lm/zh_giga.no_cna_cmn.prune01244.klm""\ncutoff_top_n = 40\ncutoff_prob = 1.0\nbeam_width = 32\nnum_processes = 4\nblank_index = 0\n\nmodel = GatedConv.load(""pretrained/gated-conv.pth"")\nmodel.eval()\n\ndecoder = CTCBeamDecoder(\n    model.vocabulary,\n    lm_path,\n    alpha,\n    beta,\n    cutoff_top_n,\n    cutoff_prob,\n    beam_width,\n    num_processes,\n    blank_index,\n)\n\n\ndef translate(vocab, out, out_len):\n    return """".join([vocab[x] for x in out[0:out_len]])\n\n\ndef predict(f):\n    wav = feature.load_audio(f)\n    spec = feature.spectrogram(wav)\n    spec.unsqueeze_(0)\n    with torch.no_grad():\n        y = model.cnn(spec)\n        y = F.softmax(y, 1)\n    y_len = torch.tensor([y.size(-1)])\n    y = y.permute(0, 2, 1)  # B * T * V\n    print(""decoding"")\n    out, score, offset, out_len = decoder.decode(y, y_len)\n    return translate(model.vocabulary, out[0][0], out_len[0][0])\n'"
data.py,7,"b'import torch\nimport librosa\nimport wave\nimport numpy as np\nimport scipy\nimport json\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\n\nsample_rate = 16000\nwindow_size = 0.02\nwindow_stride = 0.01\nn_fft = int(sample_rate * window_size)\nwin_length = n_fft\nhop_length = int(sample_rate * window_stride)\nwindow = ""hamming""\n\n\ndef load_audio(wav_path, normalize=True):  # -> numpy array\n    with wave.open(wav_path) as wav:\n        wav = np.frombuffer(wav.readframes(wav.getnframes()), dtype=""int16"")\n        wav = wav.astype(""float"")\n    if normalize:\n        return (wav - wav.mean()) / wav.std()\n    else:\n        return wav\n\n\ndef spectrogram(wav, normalize=True):\n    D = librosa.stft(\n        wav, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window\n    )\n\n    spec, phase = librosa.magphase(D)\n    spec = np.log1p(spec)\n    spec = torch.FloatTensor(spec)\n\n    if normalize:\n        spec = (spec - spec.mean()) / spec.std()\n\n    return spec\n\n\nclass MASRDataset(Dataset):\n    def __init__(self, index_path, labels_path):\n        with open(index_path) as f:\n            idx = f.readlines()\n        idx = [x.strip().split("","", 1) for x in idx]\n        self.idx = idx\n        with open(labels_path) as f:\n            labels = json.load(f)\n        self.labels = dict([(labels[i], i) for i in range(len(labels))])\n        self.labels_str = labels\n\n    def __getitem__(self, index):\n        wav, transcript = self.idx[index]\n        wav = load_audio(wav)\n        spect = spectrogram(wav)\n        transcript = list(filter(None, [self.labels.get(x) for x in transcript]))\n\n        return spect, transcript\n\n    def __len__(self):\n        return len(self.idx)\n\n\ndef _collate_fn(batch):\n    def func(p):\n        return p[0].size(1)\n\n    batch = sorted(batch, key=lambda sample: sample[0].size(1), reverse=True)\n    longest_sample = max(batch, key=func)[0]\n    freq_size = longest_sample.size(0)\n    minibatch_size = len(batch)\n    max_seqlength = longest_sample.size(1)\n    inputs = torch.zeros(minibatch_size, freq_size, max_seqlength)\n    input_lens = torch.IntTensor(minibatch_size)\n    target_lens = torch.IntTensor(minibatch_size)\n    targets = []\n    for x in range(minibatch_size):\n        sample = batch[x]\n        tensor = sample[0]\n        target = sample[1]\n        seq_length = tensor.size(1)\n        inputs[x].narrow(1, 0, seq_length).copy_(tensor)\n        input_lens[x] = seq_length\n        target_lens[x] = len(target)\n        targets.extend(target)\n    targets = torch.IntTensor(targets)\n    return inputs, targets, input_lens, target_lens\n\n\nclass MASRDataLoader(DataLoader):\n    def __init__(self, *args, **kwargs):\n        super(MASRDataLoader, self).__init__(*args, **kwargs)\n        self.collate_fn = _collate_fn\n\n'"
decoder.py,2,"b'import Levenshtein as Lev\nimport torch\nfrom six.moves import xrange\n\n\nclass Decoder(object):\n    """"""\n    Basic decoder class from which all other decoders inherit. Implements several\n    helper functions. Subclasses should implement the decode() method.\n\n    Arguments:\n        labels (string): mapping from integers to characters.\n        blank_index (int, optional): index for the blank \'_\' character. Defaults to 0.\n        space_index (int, optional): index for the space \' \' character. Defaults to 28.\n    """"""\n\n    def __init__(self, labels, blank_index=0):\n        # e.g. labels = ""_\'ABCDEFGHIJKLMNOPQRSTUVWXYZ#""\n        self.labels = labels\n        self.int_to_char = dict([(i, c) for (i, c) in enumerate(labels)])\n        self.blank_index = blank_index\n        """"""\n        space_index = len(labels)  # To prevent errors in decode, we add an out of bounds index for the space\n        if \' \' in labels:\n            space_index = labels.index(\' \')\n        self.space_index = space_index\n        """"""\n\n    def wer(self, s1, s2):\n        """"""\n        Computes the Word Error Rate, defined as the edit distance between the\n        two provided sentences after tokenizing to words.\n        Arguments:\n            s1 (string): space-separated sentence\n            s2 (string): space-separated sentence\n        """"""\n\n        # build mapping of words to integers\n        b = set(s1.split() + s2.split())\n        word2char = dict(zip(b, range(len(b))))\n\n        # map the words to a char array (Levenshtein packages only accepts\n        # strings)\n        w1 = [chr(word2char[w]) for w in s1.split()]\n        w2 = [chr(word2char[w]) for w in s2.split()]\n\n        return Lev.distance("""".join(w1), """".join(w2))\n\n    def cer(self, s1, s2):\n        """"""\n        Computes the Character Error Rate, defined as the edit distance.\n\n        Arguments:\n            s1 (string): space-separated sentence\n            s2 (string): space-separated sentence\n        """"""\n        s1, s2, = s1.replace("" "", """"), s2.replace("" "", """")\n        return Lev.distance(s1, s2)\n\n    def decode(self, probs, sizes=None):\n        """"""\n        Given a matrix of character probabilities, returns the decoder\'s\n        best guess of the transcription\n\n        Arguments:\n            probs: Tensor of character probabilities, where probs[c,t]\n                            is the probability of character c at time t\n            sizes(optional): Size of each sequence in the mini-batch\n        Returns:\n            string: sequence of the model\'s best guess for the transcription\n        """"""\n        raise NotImplementedError\n\n\nclass GreedyDecoder(Decoder):\n    def __init__(self, labels, blank_index=0):\n        super(GreedyDecoder, self).__init__(labels, blank_index)\n\n    def convert_to_strings(\n        self, sequences, sizes=None, remove_repetitions=False, return_offsets=False\n    ):\n        """"""Given a list of numeric sequences, returns the corresponding strings""""""\n        strings = []\n        offsets = [] if return_offsets else None\n        for x in xrange(len(sequences)):\n            seq_len = sizes[x] if sizes is not None else len(sequences[x])\n            string, string_offsets = self.process_string(\n                sequences[x], seq_len, remove_repetitions\n            )\n            strings.append([string])  # We only return one path\n            if return_offsets:\n                offsets.append([string_offsets])\n        if return_offsets:\n            return strings, offsets\n        else:\n            return strings\n\n    def process_string(self, sequence, size, remove_repetitions=False):\n        string = """"\n        offsets = []\n        for i in range(size):\n            char = self.int_to_char[sequence[i].item()]\n            if char != self.int_to_char[self.blank_index]:\n                # if this char is a repetition and remove_repetitions=true, then skip\n                if (\n                    remove_repetitions\n                    and i != 0\n                    and char == self.int_to_char[sequence[i - 1].item()]\n                ):\n                    pass\n                else:\n                    string = string + char\n                    offsets.append(i)\n        return string, torch.tensor(offsets, dtype=torch.int)\n\n    def decode(self, probs, sizes=None):\n        """"""\n        Returns the argmax decoding given the probability matrix. Removes\n        repeated elements in the sequence, as well as blanks.\n\n        Arguments:\n            probs: Tensor of character probabilities from the network. Expected shape of batch x seq_length x output_dim\n            sizes(optional): Size of each sequence in the mini-batch\n        Returns:\n            strings: sequences of the model\'s best guess for the transcription on inputs\n            offsets: time step per character predicted\n        """"""\n        _, max_probs = torch.max(probs, 2)\n        strings, offsets = self.convert_to_strings(\n            max_probs.view(max_probs.size(0), max_probs.size(1)),\n            sizes,\n            remove_repetitions=True,\n            return_offsets=True,\n        )\n        return strings, offsets\n'"
feature.py,1,"b'import librosa\nimport wave\nimport numpy as np\nimport torch\n\nsample_rate = 16000\nwindow_size = 0.02\nwindow_stride = 0.01\nn_fft = int(sample_rate * window_size)\nwin_length = n_fft\nhop_length = int(sample_rate * window_stride)\nwindow = ""hamming""\n\n\ndef load_audio(wav_path, normalize=True):  # -> numpy array\n    with wave.open(wav_path) as wav:\n        wav = np.frombuffer(wav.readframes(wav.getnframes()), dtype=""int16"")\n        wav = wav.astype(""float"")\n    if normalize:\n        wav = (wav - wav.mean()) / wav.std()\n    return wav\n\n\ndef spectrogram(wav, normalize=True):\n    D = librosa.stft(\n        wav, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=window\n    )\n\n    spec, phase = librosa.magphase(D)\n    spec = np.log1p(spec)\n    spec = torch.FloatTensor(spec)\n\n    if normalize:\n        spec = (spec - spec.mean()) / spec.std()\n\n    return spec\n'"
train.py,6,"b'import torch\nimport torch.nn as nn\nimport data\nfrom models.conv import GatedConv\nfrom tqdm import tqdm\nfrom decoder import GreedyDecoder\nfrom warpctc_pytorch import CTCLoss\nimport tensorboardX as tensorboard\nimport torch.nn.functional as F\nimport json\n\n\ndef train(\n    model,\n    epochs=1000,\n    batch_size=64,\n    train_index_path=""data_aishell/train-sort.manifest"",\n    dev_index_path=""data_aishell/dev.manifest"",\n    labels_path=""data_aishell/labels.json"",\n    learning_rate=0.6,\n    momentum=0.8,\n    max_grad_norm=0.2,\n    weight_decay=0,\n):\n    train_dataset = data.MASRDataset(train_index_path, labels_path)\n    batchs = (len(train_dataset) + batch_size - 1) // batch_size\n    dev_dataset = data.MASRDataset(dev_index_path, labels_path)\n    train_dataloader = data.MASRDataLoader(\n        train_dataset, batch_size=batch_size, num_workers=8\n    )\n    train_dataloader_shuffle = data.MASRDataLoader(\n        train_dataset, batch_size=batch_size, num_workers=8, shuffle=True\n    )\n    dev_dataloader = data.MASRDataLoader(\n        dev_dataset, batch_size=batch_size, num_workers=8\n    )\n    parameters = model.parameters()\n    optimizer = torch.optim.SGD(\n        parameters,\n        lr=learning_rate,\n        momentum=momentum,\n        nesterov=True,\n        weight_decay=weight_decay,\n    )\n    ctcloss = CTCLoss(size_average=True)\n    # lr_sched = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.985)\n    writer = tensorboard.SummaryWriter()\n    gstep = 0\n    for epoch in range(epochs):\n        epoch_loss = 0\n        if epoch > 0:\n            train_dataloader = train_dataloader_shuffle\n        # lr_sched.step()\n        lr = get_lr(optimizer)\n        writer.add_scalar(""lr/epoch"", lr, epoch)\n        for i, (x, y, x_lens, y_lens) in enumerate(train_dataloader):\n            x = x.to(""cuda"")\n            out, out_lens = model(x, x_lens)\n            out = out.transpose(0, 1).transpose(0, 2)\n            loss = ctcloss(out, y, out_lens, y_lens)\n            optimizer.zero_grad()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n            optimizer.step()\n            epoch_loss += loss.item()\n            writer.add_scalar(""loss/step"", loss.item(), gstep)\n            gstep += 1\n            print(\n                ""[{}/{}][{}/{}]\\tLoss = {}"".format(\n                    epoch + 1, epochs, i, int(batchs), loss.item()\n                )\n            )\n        epoch_loss = epoch_loss / batchs\n        cer = eval(model, dev_dataloader)\n        writer.add_scalar(""loss/epoch"", epoch_loss, epoch)\n        writer.add_scalar(""cer/epoch"", cer, epoch)\n        print(""Epoch {}: Loss= {}, CER = {}"".format(epoch, epoch_loss, cer))\n        torch.save(model, ""pretrained/model_{}.pth"".format(epoch))\n\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group[""lr""]\n\n\ndef eval(model, dataloader):\n    model.eval()\n    decoder = GreedyDecoder(dataloader.dataset.labels_str)\n    cer = 0\n    print(""decoding"")\n    with torch.no_grad():\n        for i, (x, y, x_lens, y_lens) in tqdm(enumerate(dataloader)):\n            x = x.to(""cuda"")\n            outs, out_lens = model(x, x_lens)\n            outs = F.softmax(outs, 1)\n            outs = outs.transpose(1, 2)\n            ys = []\n            offset = 0\n            for y_len in y_lens:\n                ys.append(y[offset : offset + y_len])\n                offset += y_len\n            out_strings, out_offsets = decoder.decode(outs, out_lens)\n            y_strings = decoder.convert_to_strings(ys)\n            for pred, truth in zip(out_strings, y_strings):\n                trans, ref = pred[0], truth[0]\n                cer += decoder.cer(trans, ref) / float(len(ref))\n        cer /= len(dataloader.dataset)\n    model.train()\n    return cer\n\n\nif __name__ == ""__main__"":\n    with open(""data_aishell/labels.json"") as f:\n        vocabulary = json.load(f)\n        vocabulary = """".join(vocabulary)\n    model = GatedConv(vocabulary)\n    model.to(""cuda"")\n    train(model)\n'"
examples/_init_path.py,0,"b'import os.path\nimport sys\n\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\n\nthis_dir = os.path.dirname(__file__)\n\n# Add project path to PYTHONPATH\nproj_path = os.path.join(this_dir, "".."")\nadd_path(proj_path)\n'"
examples/demo-client.py,0,"b'import requests\nimport _init_path\nimport feature\nfrom record import record\n\nserver = ""http://localhost:5000/recognize""\n\nrecord(""record.wav"", time=5)  # modify time to how long you want\n\nf = open(""record.wav"", ""rb"")\n\nfiles = {""file"": f}\n\nr = requests.post(server, files=files)\n\nprint("""")\nprint(""\xe8\xaf\x86\xe5\x88\xab\xe7\xbb\x93\xe6\x9e\x9c:"")\nprint(r.text)\n'"
examples/demo-recognize.py,0,"b'import _init_path\nfrom models.conv import GatedConv\n\nmodel = GatedConv.load(""pretrained/gated-conv.pth"")\n\ntext = model.predict(""test.wav"")\n\nprint("""")\nprint(""\xe8\xaf\x86\xe5\x88\xab\xe7\xbb\x93\xe6\x9e\x9c:"")\nprint(text)\n'"
examples/demo-record-recognize.py,0,"b'import _init_path\nfrom models.conv import GatedConv\nfrom record import record\n\nmodel = GatedConv.load(""pretrained/gated-conv.pth"")\n\nrecord(""record.wav"", time=5)  # modify time to how long you want\n\ntext = model.predict(""record.wav"")\n\nprint("""")\nprint(""\xe8\xaf\x86\xe5\x88\xab\xe7\xbb\x93\xe6\x9e\x9c:"")\nprint(text)\n'"
examples/demo-server.py,0,"b'from flask import Flask, request\nimport _init_path\nfrom models.conv import GatedConv\nimport sys\nimport json\n\nprint(""Loading model..."")\n\nimport beamdecode\n\nprint(""Model loaded"")\n\napp = Flask(__name__)\n\n\n@app.route(""/recognize"", methods=[""POST""])\ndef recognize():\n    f = request.files[""file""]\n    f.save(""test.wav"")\n    return beamdecode.predict(""test.wav"")\n\n\napp.run(""0.0.0.0"", debug=True)\n'"
examples/embedding.py,2,"b'import _init_path\nfrom models.conv import GatedConv\nimport numpy as np\nimport torch\nimport heapq\nfrom torch.nn.utils import remove_weight_norm\n\ntorch.set_grad_enabled(False)\n\nmodel = GatedConv.load(""pretrained/gated-conv.pth"")\nmodel.eval()\n\nconv = model.cnn[10]\nremove_weight_norm(conv)\n\n\nw = conv.weight.squeeze().detach()\nb = conv.bias.unsqueeze(1).detach()\n\nembed = w\n\nvocab = model.vocabulary\nv = dict((vocab[i], i) for i in range(len(vocab)))\n\n\ndef cos(c1, c2):\n    e1, e2 = embed[v[c1]], embed[v[c2]]\n    return (e1 * e2).sum() / (e1.norm() * e2.norm())\n\n\ndef nearest(c, n=5):\n    def gen():\n        for c_ in v:\n            if c_ == c:\n                continue\n            yield cos(c, c_), c_\n\n    return heapq.nlargest(n, gen())\n\n\ndef main():\n    while True:\n        c = input(""\xe8\xaf\xb7\xe8\xbe\x93\xe5\x85\xa5\xe4\xb8\x80\xe4\xb8\xaa\xe6\xb1\x89\xe5\xad\x97\xef\xbc\x9a"")\n        if c not in v:\n            print(f""\xe8\xaf\x8d\xe6\xb1\x87\xe8\xa1\xa8\xe9\x87\x8c\xe6\xb2\xa1\xe6\x9c\x89\xe3\x80\x8c{c}\xe3\x80\x8d"")\n            continue\n        print(""\xe4\xbb\xa5\xe4\xb8\x8b\xe6\x98\xafcos\xe7\x9b\xb8\xe4\xbc\xbc\xe5\xba\xa6\xe6\x9c\x80\xe9\xab\x98\xe7\x9a\x84"")\n        for p, c in nearest(c):\n            print(c, end="", "")\n        print("""")\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/record.py,0,"b'import pyaudio\nimport wave\n\nframerate = 16000\nNUM_SAMPLES = 2000\nchannels = 1\nsampwidth = 2\nTIME = 10\n\n\ndef save_wave_file(filename, data):\n    wf = wave.open(filename, ""wb"")\n    wf.setnchannels(channels)\n    wf.setsampwidth(sampwidth)\n    wf.setframerate(framerate)\n    wf.writeframes(b"""".join(data))\n    wf.close()\n\n\ndef record(f, time=5):\n    p = pyaudio.PyAudio()\n    stream = p.open(\n        format=pyaudio.paInt16,\n        channels=1,\n        rate=framerate,\n        input=True,\n        frames_per_buffer=NUM_SAMPLES,\n    )\n    my_buf = []\n    count = 0\n    print(""\xe5\xbd\x95\xe9\x9f\xb3\xe4\xb8\xad(5s)"")\n    while count < TIME * time:\n        string_audio_data = stream.read(NUM_SAMPLES)\n        my_buf.append(string_audio_data)\n        count += 1\n        print(""."", end="""", flush=True)\n\n    save_wave_file(f, my_buf)\n    stream.close()\n'"
examples/train.py,0,"b'import _init_path\nfrom models.conv import GatedConv\n\nmodel = GatedConv.load(""pretrained/gated-conv.pth"")\n\nmodel.to_train()\n\nmodel.fit(""train.manifest"", ""train.manifest"")\n'"
models/__init__.py,0,b''
models/base.py,2,"b'import torch\nimport torch.nn as nn\n\n\nclass MASRModel(nn.Module):\n    def __init__(self, **config):\n        super().__init__()\n        self.config = config\n\n    @classmethod\n    def load(cls, path):\n        package = torch.load(path)\n        state_dict = package[""state_dict""]\n        config = package[""config""]\n        m = cls(**config)\n        m.load_state_dict(state_dict)\n        return m\n\n    def to_train(self):\n        from .trainable import TrainableModel\n\n        self.__class__.__bases__ = (TrainableModel,)\n        return self\n\n    def predict(self, *args):\n        raise NotImplementedError()\n\n    # -> texts: list, len(list) = B\n    def _default_decode(self, yp, yp_lens):\n        idxs = yp.argmax(1)\n        texts = []\n        for idx, out_len in zip(idxs, yp_lens):\n            idx = idx[:out_len]\n            text = """"\n            last = None\n            for i in idx:\n                if i.item() not in (last, self.blank):\n                    text += self.vocabulary[i.item()]\n                last = i\n            texts.append(text)\n        return texts\n\n    def decode(self, *outputs):  # texts -> list of size B\n        return self._default_decode(*outputs)\n'"
models/conv.py,3,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.utils import weight_norm\nfrom .base import MASRModel\nimport feature\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, conv, p):\n        super().__init__()\n        self.conv = conv\n        nn.init.kaiming_normal_(self.conv.weight)\n        self.conv = weight_norm(self.conv)\n        self.act = nn.GLU(1)\n        self.dropout = nn.Dropout(p, inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.act(x)\n        x = self.dropout(x)\n        return x\n\n\nclass GatedConv(MASRModel):\n    """""" This is a model between Wav2letter and Gated Convnets.\n        The core block of this model is Gated Convolutional Network""""""\n\n    def __init__(self, vocabulary, blank=0, name=""masr""):\n        """""" vocabulary : str : string of all labels such that vocaulary[0] == ctc_blank  """"""\n        super().__init__(vocabulary=vocabulary, name=name, blank=blank)\n        self.blank = blank\n        self.vocabulary = vocabulary\n        self.name = name\n        output_units = len(vocabulary)\n        modules = []\n        modules.append(ConvBlock(nn.Conv1d(161, 500, 48, 2, 97), 0.2))\n\n        for i in range(7):\n            modules.append(ConvBlock(nn.Conv1d(250, 500, 7, 1), 0.3))\n\n        modules.append(ConvBlock(nn.Conv1d(250, 2000, 32, 1), 0.5))\n\n        modules.append(ConvBlock(nn.Conv1d(1000, 2000, 1, 1), 0.5))\n\n        modules.append(weight_norm(nn.Conv1d(1000, output_units, 1, 1)))\n\n        self.cnn = nn.Sequential(*modules)\n\n    def forward(self, x, lens):  # -> B * V * T\n        x = self.cnn(x)\n        for module in self.modules():\n            if type(module) == nn.modules.Conv1d:\n                lens = (\n                    lens - module.kernel_size[0] + 2 * module.padding[0]\n                ) // module.stride[0] + 1\n        return x, lens\n\n    def predict(self, path):\n        self.eval()\n        wav = feature.load_audio(path)\n        spec = feature.spectrogram(wav)\n        spec.unsqueeze_(0)\n        x_lens = spec.size(-1)\n        out = self.cnn(spec)\n        out_len = torch.tensor([out.size(-1)])\n        text = self.decode(out, out_len)\n        self.train()\n        return text[0]\n'"
models/trainable.py,3,"b'from .base import MASRModel\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport data\nfrom tensorboardX import SummaryWriter\nfrom warpctc_pytorch import CTCLoss\nfrom tqdm import tqdm\nfrom Levenshtein import distance\n\n\nclass TrainableModel(MASRModel):\n    def __init__(self, **config):\n        super().__init__(**config)\n\n    def save(self, path):\n        state_dict = self.state_dict()\n        config = self.config\n        package = {""state_dict"": state_dict, ""config"": config}\n        torch.save(package, path)\n\n    def loss(self, *pred_targets):  # -> loss: scalar tensor\n        preds, targets = pred_targets\n        return self._default_loss(*preds, *targets)\n\n    def cer(self, texts, *targets):  # -> cer: float\n        return self._default_cer(texts, *targets)\n\n    def _default_loss(self, yp, yp_lens, y, y_lens):  # -> ctc_loss: scalar tensor\n        criterion = CTCLoss(size_average=True)\n        yp = yp.permute(2, 0, 1)  # B * V * T -> T * B * V\n        loss = criterion(yp, y, yp_lens, y_lens)\n        return loss\n\n    def _default_cer(self, texts, y, y_lens):  # -> cer: float\n        index = 0\n        cer = 0\n        for text, y_len in zip(texts, y_lens):\n            target = y[index : (index + y_len)]\n            target = """".join(self.vocabulary[i] for i in target)\n            print(text, target)\n            cer += distance(text, target) / len(target)\n            index += y_len\n        cer /= len(y_lens)\n        return cer\n\n    def test(self, test_index, batch_size=64):  # -> cer: float\n        self.eval()\n        test_dataset = data.MASRDataset(test_index, self.vocabulary)\n        test_loader = data.MASRDataLoader(\n            test_dataset, batch_size, shuffle=False, num_workers=16\n        )\n        test_steps = len(test_loader)\n        cer = 0\n        for inputs, targets in tqdm(test_loader, total=test_steps):\n            x, x_lens = inputs\n            x = x.to(""cuda"")\n            outputs = self.forward(x, x_lens)\n            texts = self.decode(*outputs)\n            cer += self.cer(texts, *targets)\n        cer /= test_steps\n        self.train()\n        return cer\n\n    def fit(\n        self,\n        train_index,\n        dev_index,\n        epochs=100,\n        train_batch_size=64,\n        lr=0.6,\n        momentum=0.8,\n        grad_clip=0.2,\n        dev_batch_size=64,\n        sorta_grad=True,\n        tensorboard=True,\n        quiet=False,\n    ):\n        self.to(""cuda"")\n        self.train()\n        if tensorboard:\n            writer = SummaryWriter()\n        optimizer = optim.SGD(self.parameters(), lr, momentum, nesterov=True)\n        train_dataset = data.MASRDataset(train_index, self.vocabulary)\n        train_loader_shuffle = data.MASRDataLoader(\n            train_dataset, train_batch_size, shuffle=True, num_workers=16\n        )\n        if sorta_grad:\n            train_loader_sort = data.MASRDataLoader(\n                train_dataset, train_batch_size, shuffle=False, num_workers=16\n            )\n        train_steps = len(train_loader_shuffle)\n        gstep = 0\n        for epoch in range(epochs):\n            avg_loss = 0\n            if epoch == 0 and sorta_grad:\n                train_loader = train_loader_sort\n            else:\n                train_loader = train_loader_shuffle\n            for step, (inputs, targets) in enumerate(train_loader):\n                x, x_lens = inputs\n                x = x.to(""cuda"")\n                gstep += 1\n                outputs = self.forward(x, x_lens)\n                loss = self.loss(outputs, targets)\n                optimizer.zero_grad()\n                loss.backward()\n                nn.utils.clip_grad_norm_(self.parameters(), grad_clip)\n                optimizer.step()\n                avg_loss += loss.item()\n                if not quiet:\n                    print(\n                        ""[{}/{}][{}/{}]\\tLoss = {}"".format(\n                            epoch + 1, epochs, step + 1, train_steps, loss.item()\n                        )\n                    )\n                if tensorboard:\n                    writer.add_scalar(""loss/step"", loss.item(), gstep)\n            cer = self.test(dev_index, dev_batch_size)\n            avg_loss /= train_steps\n            if not quiet:\n                print(""Epoch {}\\t CER = {}\\t"".format(epoch + 1, cer))\n            if tensorboard:\n                writer.add_scalar(""cer/epoch"", cer, epoch + 1)\n                writer.add_scalar(""loss/epoch"", loss, epoch + 1)\n            self.save(""pretrained/{}_epoch_{}.pth"".format(self.name, epoch + 1))\n\n'"
