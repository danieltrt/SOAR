file_path,api_count,code
REINFORCE.py,6,"b'import gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n#Hyperparameters\nlearning_rate = 0.0002\ngamma         = 0.98\n\nclass Policy(nn.Module):\n    def __init__(self):\n        super(Policy, self).__init__()\n        self.data = []\n        \n        self.fc1 = nn.Linear(4, 128)\n        self.fc2 = nn.Linear(128, 2)\n        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.softmax(self.fc2(x), dim=0)\n        return x\n      \n    def put_data(self, item):\n        self.data.append(item)\n        \n    def train_net(self):\n        R = 0\n        self.optimizer.zero_grad()\n        for r, prob in self.data[::-1]:\n            R = r + gamma * R\n            loss = -torch.log(prob) * R\n            loss.backward()\n        self.optimizer.step()\n        self.data = []\n\ndef main():\n    env = gym.make(\'CartPole-v1\')\n    pi = Policy()\n    score = 0.0\n    print_interval = 20\n    \n    \n    for n_epi in range(10000):\n        s = env.reset()\n        done = False\n        \n        while not done: # CartPole-v1 forced to terminates at 500 step.\n            prob = pi(torch.from_numpy(s).float())\n            m = Categorical(prob)\n            a = m.sample()\n            s_prime, r, done, info = env.step(a.item())\n            pi.put_data((r,prob[a]))\n            s = s_prime\n            score += r\n            \n        pi.train_net()\n        \n        if n_epi%print_interval==0 and n_epi!=0:\n            print(""# of episode :{}, avg score : {}"".format(n_epi, score/print_interval))\n            score = 0.0\n    env.close()\n    \nif __name__ == \'__main__\':\n    main()'"
a2c.py,12,"b'import gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\nimport torch.multiprocessing as mp\nimport time\nimport numpy as np\n\n# Hyperparameters\nn_train_processes = 3\nlearning_rate = 0.0002\nupdate_interval = 5\ngamma = 0.98\nmax_train_steps = 60000\nPRINT_INTERVAL = update_interval * 100\n\nclass ActorCritic(nn.Module):\n    def __init__(self):\n        super(ActorCritic, self).__init__()\n        self.fc1 = nn.Linear(4, 256)\n        self.fc_pi = nn.Linear(256, 2)\n        self.fc_v = nn.Linear(256, 1)\n\n    def pi(self, x, softmax_dim=1):\n        x = F.relu(self.fc1(x))\n        x = self.fc_pi(x)\n        prob = F.softmax(x, dim=softmax_dim)\n        return prob\n\n    def v(self, x):\n        x = F.relu(self.fc1(x))\n        v = self.fc_v(x)\n        return v\n\ndef worker(worker_id, master_end, worker_end):\n    master_end.close()  # Forbid worker to use the master end for messaging\n    env = gym.make(\'CartPole-v1\')\n    env.seed(worker_id)\n\n    while True:\n        cmd, data = worker_end.recv()\n        if cmd == \'step\':\n            ob, reward, done, info = env.step(data)\n            if done:\n                ob = env.reset()\n            worker_end.send((ob, reward, done, info))\n        elif cmd == \'reset\':\n            ob = env.reset()\n            worker_end.send(ob)\n        elif cmd == \'reset_task\':\n            ob = env.reset_task()\n            worker_end.send(ob)\n        elif cmd == \'close\':\n            worker_end.close()\n            break\n        elif cmd == \'get_spaces\':\n            worker_end.send((env.observation_space, env.action_space))\n        else:\n            raise NotImplementedError\n\nclass ParallelEnv:\n    def __init__(self, n_train_processes):\n        self.nenvs = n_train_processes\n        self.waiting = False\n        self.closed = False\n        self.workers = list()\n\n        master_ends, worker_ends = zip(*[mp.Pipe() for _ in range(self.nenvs)])\n        self.master_ends, self.worker_ends = master_ends, worker_ends\n\n        for worker_id, (master_end, worker_end) in enumerate(zip(master_ends, worker_ends)):\n            p = mp.Process(target=worker,\n                           args=(worker_id, master_end, worker_end))\n            p.daemon = True\n            p.start()\n            self.workers.append(p)\n\n        # Forbid master to use the worker end for messaging\n        for worker_end in worker_ends:\n            worker_end.close()\n\n    def step_async(self, actions):\n        for master_end, action in zip(self.master_ends, actions):\n            master_end.send((\'step\', action))\n        self.waiting = True\n\n    def step_wait(self):\n        results = [master_end.recv() for master_end in self.master_ends]\n        self.waiting = False\n        obs, rews, dones, infos = zip(*results)\n        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n\n    def reset(self):\n        for master_end in self.master_ends:\n            master_end.send((\'reset\', None))\n        return np.stack([master_end.recv() for master_end in self.master_ends])\n\n    def step(self, actions):\n        self.step_async(actions)\n        return self.step_wait()\n\n    def close(self):  # For clean up resources\n        if self.closed:\n            return\n        if self.waiting:\n            [master_end.recv() for master_end in self.master_ends]\n        for master_end in self.master_ends:\n            master_end.send((\'close\', None))\n        for worker in self.workers:\n            worker.join()\n            self.closed = True\n\ndef test(step_idx, model):\n    env = gym.make(\'CartPole-v1\')\n    score = 0.0\n    done = False\n    num_test = 10\n\n    for _ in range(num_test):\n        s = env.reset()\n        while not done:\n            prob = model.pi(torch.from_numpy(s).float(), softmax_dim=0)\n            a = Categorical(prob).sample().numpy()\n            s_prime, r, done, info = env.step(a)\n            s = s_prime\n            score += r\n        done = False\n    print(f""Step # :{step_idx}, avg score : {score/num_test:.1f}"")\n\n    env.close()\n\ndef compute_target(v_final, r_lst, mask_lst):\n    G = v_final.reshape(-1)\n    td_target = list()\n\n    for r, mask in zip(r_lst[::-1], mask_lst[::-1]):\n        G = r + gamma * G * mask\n        td_target.append(G)\n\n    return torch.tensor(td_target[::-1]).float()\n\nif __name__ == \'__main__\':\n    envs = ParallelEnv(n_train_processes)\n\n    model = ActorCritic()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    step_idx = 0\n    s = envs.reset()\n    while step_idx < max_train_steps:\n        s_lst, a_lst, r_lst, mask_lst = list(), list(), list(), list()\n        for _ in range(update_interval):\n            prob = model.pi(torch.from_numpy(s).float())\n            a = Categorical(prob).sample().numpy()\n            s_prime, r, done, info = envs.step(a)\n\n            s_lst.append(s)\n            a_lst.append(a)\n            r_lst.append(r/100.0)\n            mask_lst.append(1 - done)\n\n            s = s_prime\n            step_idx += 1\n\n        s_final = torch.from_numpy(s_prime).float()\n        v_final = model.v(s_final).detach().clone().numpy()\n        td_target = compute_target(v_final, r_lst, mask_lst)\n\n        td_target_vec = td_target.reshape(-1)\n        s_vec = torch.tensor(s_lst).float().reshape(-1, 4)  # 4 == Dimension of state\n        a_vec = torch.tensor(a_lst).reshape(-1).unsqueeze(1)\n        advantage = td_target_vec - model.v(s_vec).reshape(-1)\n\n        pi = model.pi(s_vec, softmax_dim=1)\n        pi_a = pi.gather(1, a_vec).reshape(-1)\n        loss = -(torch.log(pi_a) * advantage.detach()).mean() +\\\n            F.smooth_l1_loss(model.v(s_vec).reshape(-1), td_target_vec)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if step_idx % PRINT_INTERVAL == 0:\n            test(step_idx, model)\n\n    envs.close()'"
a3c.py,11,"b'import gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\nimport torch.multiprocessing as mp\nimport time\n\n# Hyperparameters\nn_train_processes = 3\nlearning_rate = 0.0002\nupdate_interval = 5\ngamma = 0.98\nmax_train_ep = 300\nmax_test_ep = 400\n\n\nclass ActorCritic(nn.Module):\n    def __init__(self):\n        super(ActorCritic, self).__init__()\n        self.fc1 = nn.Linear(4, 256)\n        self.fc_pi = nn.Linear(256, 2)\n        self.fc_v = nn.Linear(256, 1)\n\n    def pi(self, x, softmax_dim=0):\n        x = F.relu(self.fc1(x))\n        x = self.fc_pi(x)\n        prob = F.softmax(x, dim=softmax_dim)\n        return prob\n\n    def v(self, x):\n        x = F.relu(self.fc1(x))\n        v = self.fc_v(x)\n        return v\n\n\ndef train(global_model, rank):\n    local_model = ActorCritic()\n    local_model.load_state_dict(global_model.state_dict())\n\n    optimizer = optim.Adam(global_model.parameters(), lr=learning_rate)\n\n    env = gym.make(\'CartPole-v1\')\n\n    for n_epi in range(max_train_ep):\n        done = False\n        s = env.reset()\n        while not done:\n            s_lst, a_lst, r_lst = [], [], []\n            for t in range(update_interval):\n                prob = local_model.pi(torch.from_numpy(s).float())\n                m = Categorical(prob)\n                a = m.sample().item()\n                s_prime, r, done, info = env.step(a)\n\n                s_lst.append(s)\n                a_lst.append([a])\n                r_lst.append(r/100.0)\n\n                s = s_prime\n                if done:\n                    break\n\n            s_final = torch.tensor(s_prime, dtype=torch.float)\n            R = 0.0 if done else local_model.v(s_final).item()\n            td_target_lst = []\n            for reward in r_lst[::-1]:\n                R = gamma * R + reward\n                td_target_lst.append([R])\n            td_target_lst.reverse()\n\n            s_batch, a_batch, td_target = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n                torch.tensor(td_target_lst)\n            advantage = td_target - local_model.v(s_batch)\n\n            pi = local_model.pi(s_batch, softmax_dim=1)\n            pi_a = pi.gather(1, a_batch)\n            loss = -torch.log(pi_a) * advantage.detach() + \\\n                F.smooth_l1_loss(local_model.v(s_batch), td_target.detach())\n\n            optimizer.zero_grad()\n            loss.mean().backward()\n            for global_param, local_param in zip(global_model.parameters(), local_model.parameters()):\n                global_param._grad = local_param.grad\n            optimizer.step()\n            local_model.load_state_dict(global_model.state_dict())\n\n    env.close()\n    print(""Training process {} reached maximum episode."".format(rank))\n\n\ndef test(global_model):\n    env = gym.make(\'CartPole-v1\')\n    score = 0.0\n    print_interval = 20\n\n    for n_epi in range(max_test_ep):\n        done = False\n        s = env.reset()\n        while not done:\n            prob = global_model.pi(torch.from_numpy(s).float())\n            a = Categorical(prob).sample().item()\n            s_prime, r, done, info = env.step(a)\n            s = s_prime\n            score += r\n\n        if n_epi % print_interval == 0 and n_epi != 0:\n            print(""# of episode :{}, avg score : {:.1f}"".format(\n                n_epi, score/print_interval))\n            score = 0.0\n            time.sleep(1)\n    env.close()\n\n\nif __name__ == \'__main__\':\n    global_model = ActorCritic()\n    global_model.share_memory()\n\n    processes = []\n    for rank in range(n_train_processes + 1):  # + 1 for test process\n        if rank == 0:\n            p = mp.Process(target=test, args=(global_model,))\n        else:\n            p = mp.Process(target=train, args=(global_model, rank,))\n        p.start()\n        processes.append(p)\n    for p in processes:\n        p.join()'"
acer.py,10,"b'import gym\nimport random\nimport collections\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n# Characteristics\n# 1. Discrete action space, single thread version.\n# 2. Does not support trust-region updates.\n\n#Hyperparameters\nlearning_rate = 0.0002\ngamma         = 0.98\nbuffer_limit  = 6000  \nrollout_len   = 10   \nbatch_size    = 4     # Indicates 4 sequences per mini-batch (4*rollout_len = 40 samples total)\nc             = 1.0   # For truncating importance sampling ratio\n\nclass ReplayBuffer():\n    def __init__(self):\n        self.buffer = collections.deque(maxlen=buffer_limit)\n\n    def put(self, seq_data):\n        self.buffer.append(seq_data)\n    \n    def sample(self, on_policy=False):\n        if on_policy:\n            mini_batch = [self.buffer[-1]]\n        else:\n            mini_batch = random.sample(self.buffer, batch_size)\n\n        s_lst, a_lst, r_lst, prob_lst, done_lst, is_first_lst = [], [], [], [], [], []\n        for seq in mini_batch:\n            is_first = True  # Flag for indicating whether the transition is the first item from a sequence\n            for transition in seq:\n                s, a, r, prob, done = transition\n\n                s_lst.append(s)\n                a_lst.append([a])\n                r_lst.append(r)\n                prob_lst.append(prob)\n                done_mask = 0.0 if done else 1.0\n                done_lst.append(done_mask)\n                is_first_lst.append(is_first)\n                is_first = False\n\n        s,a,r,prob,done_mask,is_first = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n                                        r_lst, torch.tensor(prob_lst, dtype=torch.float), done_lst, \\\n                                        is_first_lst\n        return s,a,r,prob,done_mask,is_first\n    \n    def size(self):\n        return len(self.buffer)\n      \nclass ActorCritic(nn.Module):\n    def __init__(self):\n        super(ActorCritic, self).__init__()\n        self.fc1 = nn.Linear(4,256)\n        self.fc_pi = nn.Linear(256,2)\n        self.fc_q = nn.Linear(256,2)\n        \n    def pi(self, x, softmax_dim = 0):\n        x = F.relu(self.fc1(x))\n        x = self.fc_pi(x)\n        pi = F.softmax(x, dim=softmax_dim)\n        return pi\n    \n    def q(self, x):\n        x = F.relu(self.fc1(x))\n        q = self.fc_q(x)\n        return q\n      \ndef train(model, optimizer, memory, on_policy=False):\n    s,a,r,prob,done_mask,is_first = memory.sample(on_policy)\n    \n    q = model.q(s)\n    q_a = q.gather(1,a)\n    pi = model.pi(s, softmax_dim = 1)\n    pi_a = pi.gather(1,a)\n    v = (q * pi).sum(1).unsqueeze(1).detach()\n    \n    rho = pi.detach()/prob\n    rho_a = rho.gather(1,a)\n    rho_bar = rho_a.clamp(max=c)\n    correction_coeff = (1-c/rho).clamp(min=0)\n\n    q_ret = v[-1] * done_mask[-1]\n    q_ret_lst = []\n    for i in reversed(range(len(r))):\n        q_ret = r[i] + gamma * q_ret\n        q_ret_lst.append(q_ret.item())\n        q_ret = rho_bar[i] * (q_ret - q_a[i]) + v[i]\n        \n        if is_first[i] and i!=0:\n            q_ret = v[i-1] * done_mask[i-1] # When a new sequence begins, q_ret is initialized  \n            \n    q_ret_lst.reverse()\n    q_ret = torch.tensor(q_ret_lst, dtype=torch.float).unsqueeze(1)\n    \n    loss1 = -rho_bar * torch.log(pi_a) * (q_ret - v) \n    loss2 = -correction_coeff * pi.detach() * torch.log(pi) * (q.detach()-v) # bias correction term\n    loss = loss1 + loss2.sum(1) + F.smooth_l1_loss(q_a, q_ret)\n    \n    optimizer.zero_grad()\n    loss.mean().backward()\n    optimizer.step()\n        \ndef main():\n    env = gym.make(\'CartPole-v1\')\n    memory = ReplayBuffer()\n    model = ActorCritic()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    score = 0.0\n    print_interval = 20    \n\n    for n_epi in range(10000):\n        s = env.reset()\n        done = False\n        \n        while not done:\n            seq_data = []\n            for t in range(rollout_len): \n                prob = model.pi(torch.from_numpy(s).float())\n                a = Categorical(prob).sample().item()\n                s_prime, r, done, info = env.step(a)\n                seq_data.append((s, a, r/100.0, prob.detach().numpy(), done))\n\n                score +=r\n                s = s_prime\n                if done:\n                    break\n                    \n            memory.put(seq_data)\n            if memory.size()>500:\n                train(model, optimizer, memory, on_policy=True)\n                train(model, optimizer, memory)\n        \n        if n_epi%print_interval==0 and n_epi!=0:\n            print(""# of episode :{}, avg score : {:.1f}, buffer size : {}"".format(n_epi, score/print_interval, memory.size()))\n            score = 0.0\n\n    env.close()\n\nif __name__ == \'__main__\':\n    main()'"
actor_critic.py,9,"b'import gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n#Hyperparameters\nlearning_rate = 0.0002\ngamma         = 0.98\nn_rollout     = 10\n\nclass ActorCritic(nn.Module):\n    def __init__(self):\n        super(ActorCritic, self).__init__()\n        self.data = []\n        \n        self.fc1 = nn.Linear(4,256)\n        self.fc_pi = nn.Linear(256,2)\n        self.fc_v = nn.Linear(256,1)\n        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n        \n    def pi(self, x, softmax_dim = 0):\n        x = F.relu(self.fc1(x))\n        x = self.fc_pi(x)\n        prob = F.softmax(x, dim=softmax_dim)\n        return prob\n    \n    def v(self, x):\n        x = F.relu(self.fc1(x))\n        v = self.fc_v(x)\n        return v\n    \n    def put_data(self, transition):\n        self.data.append(transition)\n        \n    def make_batch(self):\n        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n        for transition in self.data:\n            s,a,r,s_prime,done = transition\n            s_lst.append(s)\n            a_lst.append([a])\n            r_lst.append([r/100.0])\n            s_prime_lst.append(s_prime)\n            done_mask = 0.0 if done else 1.0\n            done_lst.append([done_mask])\n        \n        s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n                                                               torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \\\n                                                               torch.tensor(done_lst, dtype=torch.float)\n        self.data = []\n        return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n  \n    def train_net(self):\n        s, a, r, s_prime, done = self.make_batch()\n        td_target = r + gamma * self.v(s_prime) * done\n        delta = td_target - self.v(s)\n        \n        pi = self.pi(s, softmax_dim=1)\n        pi_a = pi.gather(1,a)\n        loss = -torch.log(pi_a) * delta.detach() + F.smooth_l1_loss(self.v(s), td_target.detach())\n\n        self.optimizer.zero_grad()\n        loss.mean().backward()\n        self.optimizer.step()         \n      \ndef main():  \n    env = gym.make(\'CartPole-v1\')\n    model = ActorCritic()    \n    print_interval = 20\n    score = 0.0\n\n    for n_epi in range(10000):\n        done = False\n        s = env.reset()\n        while not done:\n            for t in range(n_rollout):\n                prob = model.pi(torch.from_numpy(s).float())\n                m = Categorical(prob)\n                a = m.sample().item()\n                s_prime, r, done, info = env.step(a)\n                model.put_data((s,a,r,s_prime,done))\n                \n                s = s_prime\n                score += r\n                \n                if done:\n                    break                     \n            \n            model.train_net()\n            \n        if n_epi%print_interval==0 and n_epi!=0:\n            print(""# of episode :{}, avg score : {:.1f}"".format(n_epi, score/print_interval))\n            score = 0.0\n    env.close()\n\nif __name__ == \'__main__\':\n    main()'"
ddpg.py,9,"b'import gym\nimport random\nimport collections\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n#Hyperparameters\nlr_mu        = 0.0005\nlr_q         = 0.001\ngamma        = 0.99\nbatch_size   = 32\nbuffer_limit = 50000\ntau          = 0.005 # for target network soft update\n\nclass ReplayBuffer():\n    def __init__(self):\n        self.buffer = collections.deque(maxlen=buffer_limit)\n\n    def put(self, transition):\n        self.buffer.append(transition)\n    \n    def sample(self, n):\n        mini_batch = random.sample(self.buffer, n)\n        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n\n        for transition in mini_batch:\n            s, a, r, s_prime, done_mask = transition\n            s_lst.append(s)\n            a_lst.append([a])\n            r_lst.append([r])\n            s_prime_lst.append(s_prime)\n            done_mask_lst.append([done_mask])\n        \n        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n               torch.tensor(done_mask_lst)\n    \n    def size(self):\n        return len(self.buffer)\n\nclass MuNet(nn.Module):\n    def __init__(self):\n        super(MuNet, self).__init__()\n        self.fc1 = nn.Linear(3, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc_mu = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        mu = torch.tanh(self.fc_mu(x))*2 # Multipled by 2 because the action space of the Pendulum-v0 is [-2,2]\n        return mu\n\nclass QNet(nn.Module):\n    def __init__(self):\n        super(QNet, self).__init__()\n        \n        self.fc_s = nn.Linear(3, 64)\n        self.fc_a = nn.Linear(1,64)\n        self.fc_q = nn.Linear(128, 32)\n        self.fc_3 = nn.Linear(32,1)\n\n    def forward(self, x, a):\n        h1 = F.relu(self.fc_s(x))\n        h2 = F.relu(self.fc_a(a))\n        cat = torch.cat([h1,h2], dim=1)\n        q = F.relu(self.fc_q(cat))\n        q = self.fc_3(q)\n        return q\n\nclass OrnsteinUhlenbeckNoise:\n    def __init__(self, mu):\n        self.theta, self.dt, self.sigma = 0.1, 0.01, 0.1\n        self.mu = mu\n        self.x_prev = np.zeros_like(self.mu)\n\n    def __call__(self):\n        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n        self.x_prev = x\n        return x\n      \ndef train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer):\n    s,a,r,s_prime,done_mask  = memory.sample(batch_size)\n    \n    target = r + gamma * q_target(s_prime, mu_target(s_prime))\n    q_loss = F.smooth_l1_loss(q(s,a), target.detach())\n    q_optimizer.zero_grad()\n    q_loss.backward()\n    q_optimizer.step()\n    \n    mu_loss = -q(s,mu(s)).mean() # That\'s all for the policy loss.\n    mu_optimizer.zero_grad()\n    mu_loss.backward()\n    mu_optimizer.step()\n    \ndef soft_update(net, net_target):\n    for param_target, param in zip(net_target.parameters(), net.parameters()):\n        param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)\n    \ndef main():\n    env = gym.make(\'Pendulum-v0\')\n    memory = ReplayBuffer()\n\n    q, q_target = QNet(), QNet()\n    q_target.load_state_dict(q.state_dict())\n    mu, mu_target = MuNet(), MuNet()\n    mu_target.load_state_dict(mu.state_dict())\n\n    score = 0.0\n    print_interval = 20\n\n    mu_optimizer = optim.Adam(mu.parameters(), lr=lr_mu)\n    q_optimizer  = optim.Adam(q.parameters(), lr=lr_q)\n    ou_noise = OrnsteinUhlenbeckNoise(mu=np.zeros(1))\n\n    for n_epi in range(10000):\n        s = env.reset()\n        \n        for t in range(300): # maximum length of episode is 200 for Pendulum-v0\n            a = mu(torch.from_numpy(s).float()) \n            a = a.item() + ou_noise()[0]\n            s_prime, r, done, info = env.step([a])\n            memory.put((s,a,r/100.0,s_prime,done))\n            score +=r\n            s = s_prime\n\n            if done:\n                break              \n                \n        if memory.size()>2000:\n            for i in range(10):\n                train(mu, mu_target, q, q_target, memory, q_optimizer, mu_optimizer)\n                soft_update(mu, mu_target)\n                soft_update(q,  q_target)\n        \n        if n_epi%print_interval==0 and n_epi!=0:\n            print(""# of episode :{}, avg score : {:.1f}"".format(n_epi, score/print_interval))\n            score = 0.0\n\n    env.close()\n\nif __name__ == \'__main__\':\n    main()'"
dqn.py,7,"b'import gym\nimport collections\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n#Hyperparameters\nlearning_rate = 0.0005\ngamma         = 0.98\nbuffer_limit  = 50000\nbatch_size    = 32\n\nclass ReplayBuffer():\n    def __init__(self):\n        self.buffer = collections.deque(maxlen=buffer_limit)\n    \n    def put(self, transition):\n        self.buffer.append(transition)\n    \n    def sample(self, n):\n        mini_batch = random.sample(self.buffer, n)\n        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n        \n        for transition in mini_batch:\n            s, a, r, s_prime, done_mask = transition\n            s_lst.append(s)\n            a_lst.append([a])\n            r_lst.append([r])\n            s_prime_lst.append(s_prime)\n            done_mask_lst.append([done_mask])\n\n        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n               torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n               torch.tensor(done_mask_lst)\n    \n    def size(self):\n        return len(self.buffer)\n\nclass Qnet(nn.Module):\n    def __init__(self):\n        super(Qnet, self).__init__()\n        self.fc1 = nn.Linear(4, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, 2)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n      \n    def sample_action(self, obs, epsilon):\n        out = self.forward(obs)\n        coin = random.random()\n        if coin < epsilon:\n            return random.randint(0,1)\n        else : \n            return out.argmax().item()\n            \ndef train(q, q_target, memory, optimizer):\n    for i in range(10):\n        s,a,r,s_prime,done_mask = memory.sample(batch_size)\n\n        q_out = q(s)\n        q_a = q_out.gather(1,a)\n        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n        target = r + gamma * max_q_prime * done_mask\n        loss = F.smooth_l1_loss(q_a, target)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\ndef main():\n    env = gym.make(\'CartPole-v1\')\n    q = Qnet()\n    q_target = Qnet()\n    q_target.load_state_dict(q.state_dict())\n    memory = ReplayBuffer()\n\n    print_interval = 20\n    score = 0.0  \n    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n\n    for n_epi in range(10000):\n        epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) #Linear annealing from 8% to 1%\n        s = env.reset()\n        done = False\n\n        while not done:\n            a = q.sample_action(torch.from_numpy(s).float(), epsilon)      \n            s_prime, r, done, info = env.step(a)\n            done_mask = 0.0 if done else 1.0\n            memory.put((s,a,r/100.0,s_prime, done_mask))\n            s = s_prime\n\n            score += r\n            if done:\n                break\n            \n        if memory.size()>2000:\n            train(q, q_target, memory, optimizer)\n\n        if n_epi%print_interval==0 and n_epi!=0:\n            q_target.load_state_dict(q.state_dict())\n            print(""n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%"".format(\n                                                            n_epi, score/print_interval, memory.size(), epsilon*100))\n            score = 0.0\n    env.close()\n\nif __name__ == \'__main__\':\n    main()'"
ppo-lstm.py,13,"b'#PPO-LSTM\nimport gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\nimport time\nimport numpy as np\n\n#Hyperparameters\nlearning_rate = 0.0005\ngamma         = 0.98\nlmbda         = 0.95\neps_clip      = 0.1\nK_epoch       = 2\nT_horizon     = 20\n\nclass PPO(nn.Module):\n    def __init__(self):\n        super(PPO, self).__init__()\n        self.data = []\n        \n        self.fc1   = nn.Linear(4,64)\n        self.lstm  = nn.LSTM(64,32)\n        self.fc_pi = nn.Linear(32,2)\n        self.fc_v  = nn.Linear(32,1)\n        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n\n    def pi(self, x, hidden):\n        x = F.relu(self.fc1(x))\n        x = x.view(-1, 1, 64)\n        x, lstm_hidden = self.lstm(x, hidden)\n        x = self.fc_pi(x)\n        prob = F.softmax(x, dim=2)\n        return prob, lstm_hidden\n    \n    def v(self, x, hidden):\n        x = F.relu(self.fc1(x))\n        x = x.view(-1, 1, 64)\n        x, lstm_hidden = self.lstm(x, hidden)\n        v = self.fc_v(x)\n        return v\n      \n    def put_data(self, transition):\n        self.data.append(transition)\n        \n    def make_batch(self):\n        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, h_in_lst, h_out_lst, done_lst = [], [], [], [], [], [], [], []\n        for transition in self.data:\n            s, a, r, s_prime, prob_a, h_in, h_out, done = transition\n            \n            s_lst.append(s)\n            a_lst.append([a])\n            r_lst.append([r])\n            s_prime_lst.append(s_prime)\n            prob_a_lst.append([prob_a])\n            h_in_lst.append(h_in)\n            h_out_lst.append(h_out)\n            done_mask = 0 if done else 1\n            done_lst.append([done_mask])\n            \n        s,a,r,s_prime,done_mask,prob_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n                                         torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n                                         torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst)\n        self.data = []\n        return s,a,r,s_prime, done_mask, prob_a, h_in_lst[0], h_out_lst[0]\n        \n    def train_net(self):\n        s,a,r,s_prime,done_mask, prob_a, (h1_in, h2_in), (h1_out, h2_out) = self.make_batch()\n        first_hidden  = (h1_in.detach(), h2_in.detach())\n        second_hidden = (h1_out.detach(), h2_out.detach())\n\n        for i in range(K_epoch):\n            v_prime = self.v(s_prime, second_hidden).squeeze(1)\n            td_target = r + gamma * v_prime * done_mask\n            v_s = self.v(s, first_hidden).squeeze(1)\n            delta = td_target - v_s\n            delta = delta.detach().numpy()\n            \n            advantage_lst = []\n            advantage = 0.0\n            for item in delta[::-1]:\n                advantage = gamma * lmbda * advantage + item[0]\n                advantage_lst.append([advantage])\n            advantage_lst.reverse()\n            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n\n            pi, _ = self.pi(s, first_hidden)\n            pi_a = pi.squeeze(1).gather(1,a)\n            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == log(exp(a)-exp(b))\n\n            surr1 = ratio * advantage\n            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(v_s, td_target.detach())\n\n            self.optimizer.zero_grad()\n            loss.mean().backward(retain_graph=True)\n            self.optimizer.step()\n        \ndef main():\n    env = gym.make(\'CartPole-v1\')\n    model = PPO()\n    score = 0.0\n    print_interval = 20\n    \n    for n_epi in range(10000):\n        h_out = (torch.zeros([1, 1, 32], dtype=torch.float), torch.zeros([1, 1, 32], dtype=torch.float))\n        s = env.reset()\n        done = False\n        \n        while not done:\n            for t in range(T_horizon):\n                h_in = h_out\n                prob, h_out = model.pi(torch.from_numpy(s).float(), h_in)\n                prob = prob.view(-1)\n                m = Categorical(prob)\n                a = m.sample().item()\n                s_prime, r, done, info = env.step(a)\n\n                model.put_data((s, a, r/100.0, s_prime, prob[a].item(), h_in, h_out, done))\n                s = s_prime\n\n                score += r\n                if done:\n                    break\n                    \n            model.train_net()\n\n        if n_epi%print_interval==0 and n_epi!=0:\n            print(""# of episode :{}, avg score : {:.1f}"".format(n_epi, score/print_interval))\n            score = 0.0\n\n    env.close()\n\nif __name__ == \'__main__\':\n    main()'"
ppo.py,12,"b'import gym\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical\n\n#Hyperparameters\nlearning_rate = 0.0005\ngamma         = 0.98\nlmbda         = 0.95\neps_clip      = 0.1\nK_epoch       = 3\nT_horizon     = 20\n\nclass PPO(nn.Module):\n    def __init__(self):\n        super(PPO, self).__init__()\n        self.data = []\n        \n        self.fc1   = nn.Linear(4,256)\n        self.fc_pi = nn.Linear(256,2)\n        self.fc_v  = nn.Linear(256,1)\n        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n\n    def pi(self, x, softmax_dim = 0):\n        x = F.relu(self.fc1(x))\n        x = self.fc_pi(x)\n        prob = F.softmax(x, dim=softmax_dim)\n        return prob\n    \n    def v(self, x):\n        x = F.relu(self.fc1(x))\n        v = self.fc_v(x)\n        return v\n      \n    def put_data(self, transition):\n        self.data.append(transition)\n        \n    def make_batch(self):\n        s_lst, a_lst, r_lst, s_prime_lst, prob_a_lst, done_lst = [], [], [], [], [], []\n        for transition in self.data:\n            s, a, r, s_prime, prob_a, done = transition\n            \n            s_lst.append(s)\n            a_lst.append([a])\n            r_lst.append([r])\n            s_prime_lst.append(s_prime)\n            prob_a_lst.append([prob_a])\n            done_mask = 0 if done else 1\n            done_lst.append([done_mask])\n            \n        s,a,r,s_prime,done_mask, prob_a = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n                                          torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n                                          torch.tensor(done_lst, dtype=torch.float), torch.tensor(prob_a_lst)\n        self.data = []\n        return s, a, r, s_prime, done_mask, prob_a\n        \n    def train_net(self):\n        s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n\n        for i in range(K_epoch):\n            td_target = r + gamma * self.v(s_prime) * done_mask\n            delta = td_target - self.v(s)\n            delta = delta.detach().numpy()\n\n            advantage_lst = []\n            advantage = 0.0\n            for delta_t in delta[::-1]:\n                advantage = gamma * lmbda * advantage + delta_t[0]\n                advantage_lst.append([advantage])\n            advantage_lst.reverse()\n            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n\n            pi = self.pi(s, softmax_dim=1)\n            pi_a = pi.gather(1,a)\n            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n\n            surr1 = ratio * advantage\n            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target.detach())\n\n            self.optimizer.zero_grad()\n            loss.mean().backward()\n            self.optimizer.step()\n        \ndef main():\n    env = gym.make(\'CartPole-v1\')\n    model = PPO()\n    score = 0.0\n    print_interval = 20\n\n    for n_epi in range(10000):\n        s = env.reset()\n        done = False\n        while not done:\n            for t in range(T_horizon):\n                prob = model.pi(torch.from_numpy(s).float())\n                m = Categorical(prob)\n                a = m.sample().item()\n                s_prime, r, done, info = env.step(a)\n\n                model.put_data((s, a, r/100.0, s_prime, prob[a].item(), done))\n                s = s_prime\n\n                score += r\n                if done:\n                    break\n\n            model.train_net()\n\n        if n_epi%print_interval==0 and n_epi!=0:\n            print(""# of episode :{}, avg score : {:.1f}"".format(n_epi, score/print_interval))\n            score = 0.0\n\n    env.close()\n\nif __name__ == \'__main__\':\n    main()'"
