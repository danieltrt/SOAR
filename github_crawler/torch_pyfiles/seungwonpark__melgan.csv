file_path,api_count,code
hubconf.py,5,"b""dependencies = ['torch']\nimport torch\nfrom model.generator import Generator\n\nmodel_params = {\n    'nvidia_tacotron2_LJ11_epoch6400': {\n        'mel_channel': 80,\n        'model_url': 'https://github.com/seungwonpark/melgan/releases/download/v0.3-alpha/nvidia_tacotron2_LJ11_epoch6400.pt',\n    },\n}\n\n\ndef melgan(model_name='nvidia_tacotron2_LJ11_epoch6400', pretrained=True, progress=True):\n    params = model_params[model_name]\n    model = Generator(params['mel_channel'])\n\n    if pretrained:\n        state_dict = torch.hub.load_state_dict_from_url(params['model_url'],\n                                                        progress=progress)\n        model.load_state_dict(state_dict['model_g'])\n\n    model.eval(inference=True)\n\n    return model\n\n\nif __name__ == '__main__':\n    vocoder = torch.hub.load('seungwonpark/melgan', 'melgan')\n    mel = torch.randn(1, 80, 234) # use your own mel-spectrogram here\n\n    print('Input mel-spectrogram shape: {}'.format(mel.shape))\n\n    if torch.cuda.is_available():\n        print('Moving data & model to GPU')\n        vocoder = vocoder.cuda()\n        mel = mel.cuda()\n\n    with torch.no_grad():\n        audio = vocoder.inference(mel)\n\n    print('Output audio shape: {}'.format(audio.shape))\n"""
inference.py,3,"b'import os\nimport glob\nimport tqdm\nimport torch\nimport argparse\nfrom scipy.io.wavfile import write\n\nfrom model.generator import Generator\nfrom utils.hparams import HParam, load_hparam_str\n\nMAX_WAV_VALUE = 32768.0\n\n\ndef main(args):\n    checkpoint = torch.load(args.checkpoint_path)\n    if args.config is not None:\n        hp = HParam(args.config)\n    else:\n        hp = load_hparam_str(checkpoint[\'hp_str\'])\n\n    model = Generator(hp.audio.n_mel_channels).cuda()\n    model.load_state_dict(checkpoint[\'model_g\'])\n    model.eval(inference=False)\n\n    with torch.no_grad():\n        for melpath in tqdm.tqdm(glob.glob(os.path.join(args.input_folder, \'*.mel\'))):\n            mel = torch.load(melpath)\n            if len(mel.shape) == 2:\n                mel = mel.unsqueeze(0)\n            mel = mel.cuda()\n\n            audio = model.inference(mel)\n            audio = audio.cpu().detach().numpy()\n\n            out_path = melpath.replace(\'.mel\', \'_reconstructed_epoch%04d.wav\' % checkpoint[\'epoch\'])\n            write(out_path, hp.audio.sampling_rate, audio)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', type=str, default=None,\n                        help=""yaml file for config. will use hp_str from checkpoint if not given."")\n    parser.add_argument(\'-p\', \'--checkpoint_path\', type=str, required=True,\n                        help=""path of checkpoint pt file for evaluation"")\n    parser.add_argument(\'-i\', \'--input_folder\', type=str, required=True,\n                        help=""directory of mel-spectrograms to invert into raw audio. "")\n    args = parser.parse_args()\n\n    main(args)\n'"
preprocess.py,2,"b'import os\nimport glob\nimport tqdm\nimport torch\nimport argparse\nimport numpy as np\n\nfrom utils.stft import TacotronSTFT\nfrom utils.hparams import HParam\nfrom utils.utils import read_wav_np\n\n\ndef main(hp, args):\n    stft = TacotronSTFT(filter_length=hp.audio.filter_length,\n                        hop_length=hp.audio.hop_length,\n                        win_length=hp.audio.win_length,\n                        n_mel_channels=hp.audio.n_mel_channels,\n                        sampling_rate=hp.audio.sampling_rate,\n                        mel_fmin=hp.audio.mel_fmin,\n                        mel_fmax=hp.audio.mel_fmax)\n\n    wav_files = glob.glob(os.path.join(args.data_path, \'**\', \'*.wav\'), recursive=True)\n\n    for wavpath in tqdm.tqdm(wav_files, desc=\'preprocess wav to mel\'):\n        sr, wav = read_wav_np(wavpath)\n        assert sr == hp.audio.sampling_rate, \\\n            ""sample rate mismatch. expected %d, got %d at %s"" % \\\n            (hp.audio.sampling_rate, sr, wavpath)\n        \n        if len(wav) < hp.audio.segment_length + hp.audio.pad_short:\n            wav = np.pad(wav, (0, hp.audio.segment_length + hp.audio.pad_short - len(wav)), \\\n                    mode=\'constant\', constant_values=0.0)\n\n        wav = torch.from_numpy(wav).unsqueeze(0)\n        mel = stft.mel_spectrogram(wav)\n\n        melpath = wavpath.replace(\'.wav\', \'.mel\')\n        torch.save(mel, melpath)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', type=str, required=True,\n                        help=""yaml file for config."")\n    parser.add_argument(\'-d\', \'--data_path\', type=str, required=True,\n                        help=""root directory of wav files"")\n    args = parser.parse_args()\n    hp = HParam(args.config)\n\n    main(hp, args)\n'"
trainer.py,0,"b'import os\nimport time\nimport logging\nimport argparse\n\nfrom utils.train import train\nfrom utils.hparams import HParam\nfrom utils.writer import MyWriter\nfrom datasets.dataloader import create_dataloader\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', type=str, required=True,\n                        help=""yaml file for configuration"")\n    parser.add_argument(\'-p\', \'--checkpoint_path\', type=str, default=None,\n                        help=""path of checkpoint pt file to resume training"")\n    parser.add_argument(\'-n\', \'--name\', type=str, required=True,\n                        help=""name of the model for logging, saving checkpoint"")\n    args = parser.parse_args()\n\n    hp = HParam(args.config)\n    with open(args.config, \'r\') as f:\n        hp_str = \'\'.join(f.readlines())\n\n    pt_dir = os.path.join(hp.log.chkpt_dir, args.name)\n    log_dir = os.path.join(hp.log.log_dir, args.name)\n    os.makedirs(pt_dir, exist_ok=True)\n    os.makedirs(log_dir, exist_ok=True)\n\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\'%(asctime)s - %(levelname)s - %(message)s\',\n        handlers=[\n            logging.FileHandler(os.path.join(log_dir,\n                \'%s-%d.log\' % (args.name, time.time()))),\n            logging.StreamHandler()\n        ]\n    )\n    logger = logging.getLogger()\n\n    writer = MyWriter(hp, log_dir)\n\n    assert hp.audio.hop_length == 256, \\\n        \'hp.audio.hop_length must be equal to 256, got %d\' % hp.audio.hop_length\n    assert hp.data.train != \'\' and hp.data.validation != \'\', \\\n        \'hp.data.train and hp.data.validation can\\\'t be empty: please fix %s\' % args.config\n\n    trainloader = create_dataloader(hp, args, True)\n    valloader = create_dataloader(hp, args, False)\n\n    train(args, pt_dir, args.checkpoint_path, trainloader, valloader, writer, logger, hp, hp_str)\n'"
datasets/dataloader.py,4,"b""import os\nimport glob\nimport torch\nimport random\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom utils.utils import read_wav_np\n\n\ndef create_dataloader(hp, args, train):\n    dataset = MelFromDisk(hp, args, train)\n\n    if train:\n        return DataLoader(dataset=dataset, batch_size=hp.train.batch_size, shuffle=True,\n            num_workers=hp.train.num_workers, pin_memory=True, drop_last=True)\n    else:\n        return DataLoader(dataset=dataset, batch_size=1, shuffle=False,\n            num_workers=hp.train.num_workers, pin_memory=True, drop_last=False)\n\n\nclass MelFromDisk(Dataset):\n    def __init__(self, hp, args, train):\n        self.hp = hp\n        self.args = args\n        self.train = train\n        self.path = hp.data.train if train else hp.data.validation\n        self.wav_list = glob.glob(os.path.join(self.path, '**', '*.wav'), recursive=True)\n        self.mel_segment_length = hp.audio.segment_length // hp.audio.hop_length + 2\n        self.mapping = [i for i in range(len(self.wav_list))]\n\n    def __len__(self):\n        return len(self.wav_list)\n\n    def __getitem__(self, idx):\n        if self.train:\n            idx1 = idx\n            idx2 = self.mapping[idx1]\n            return self.my_getitem(idx1), self.my_getitem(idx2)\n        else:\n            return self.my_getitem(idx)\n\n    def shuffle_mapping(self):\n        random.shuffle(self.mapping)\n\n    def my_getitem(self, idx):\n        wavpath = self.wav_list[idx]\n        melpath = wavpath.replace('.wav', '.mel')\n        sr, audio = read_wav_np(wavpath)\n        if len(audio) < self.hp.audio.segment_length + self.hp.audio.pad_short:\n            audio = np.pad(audio, (0, self.hp.audio.segment_length + self.hp.audio.pad_short - len(audio)), \\\n                    mode='constant', constant_values=0.0)\n\n        audio = torch.from_numpy(audio).unsqueeze(0)\n        mel = torch.load(melpath).squeeze(0)\n\n        if self.train:\n            max_mel_start = mel.size(1) - self.mel_segment_length\n            mel_start = random.randint(0, max_mel_start)\n            mel_end = mel_start + self.mel_segment_length\n            mel = mel[:, mel_start:mel_end]\n\n            audio_start = mel_start * self.hp.audio.hop_length\n            audio = audio[:, audio_start:audio_start+self.hp.audio.segment_length]\n\n        audio = audio + (1/32768) * torch.randn_like(audio)\n        return mel, audio\n"""
model/discriminator.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n\n        self.discriminator = nn.ModuleList([\n            nn.Sequential(\n                nn.ReflectionPad1d(7),\n                nn.utils.weight_norm(nn.Conv1d(1, 16, kernel_size=15, stride=1)),\n                nn.LeakyReLU(0.2, inplace=True),\n            ),\n            nn.Sequential(\n                nn.utils.weight_norm(nn.Conv1d(16, 64, kernel_size=41, stride=4, padding=20, groups=4)),\n                nn.LeakyReLU(0.2, inplace=True),\n            ),\n            nn.Sequential(\n                nn.utils.weight_norm(nn.Conv1d(64, 256, kernel_size=41, stride=4, padding=20, groups=16)),\n                nn.LeakyReLU(0.2, inplace=True),\n            ),\n            nn.Sequential(\n                nn.utils.weight_norm(nn.Conv1d(256, 1024, kernel_size=41, stride=4, padding=20, groups=64)),\n                nn.LeakyReLU(0.2, inplace=True),\n            ),\n            nn.Sequential(\n                nn.utils.weight_norm(nn.Conv1d(1024, 1024, kernel_size=41, stride=4, padding=20, groups=256)),\n                nn.LeakyReLU(0.2, inplace=True),\n            ),\n            nn.Sequential(\n                nn.utils.weight_norm(nn.Conv1d(1024, 1024, kernel_size=5, stride=1, padding=2)),\n                nn.LeakyReLU(0.2, inplace=True),\n            ),\n            nn.utils.weight_norm(nn.Conv1d(1024, 1, kernel_size=3, stride=1, padding=1)),\n        ])\n\n    def forward(self, x):\n        '''\n            returns: (list of 6 features, discriminator score)\n            we directly predict score without last sigmoid function\n            since we're using Least Squares GAN (https://arxiv.org/abs/1611.04076)\n        '''\n        features = list()\n        for module in self.discriminator:\n            x = module(x)\n            features.append(x)\n        return features[:-1], features[-1]\n\n\nif __name__ == '__main__':\n    model = Discriminator()\n\n    x = torch.randn(3, 1, 22050)\n    print(x.shape)\n\n    features, score = model(x)\n    for feat in features:\n        print(feat.shape)\n    print(score.shape)\n\n    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(pytorch_total_params)"""
model/generator.py,6,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .res_stack import ResStack\n# from res_stack import ResStack\n\nMAX_WAV_VALUE = 32768.0\n\n\nclass Generator(nn.Module):\n    def __init__(self, mel_channel):\n        super(Generator, self).__init__()\n        self.mel_channel = mel_channel\n\n        self.generator = nn.Sequential(\n            nn.ReflectionPad1d(3),\n            nn.utils.weight_norm(nn.Conv1d(mel_channel, 512, kernel_size=7, stride=1)),\n\n            nn.LeakyReLU(0.2),\n            nn.utils.weight_norm(nn.ConvTranspose1d(512, 256, kernel_size=16, stride=8, padding=4)),\n\n            ResStack(256),\n\n            nn.LeakyReLU(0.2),\n            nn.utils.weight_norm(nn.ConvTranspose1d(256, 128, kernel_size=16, stride=8, padding=4)),\n\n            ResStack(128),\n\n            nn.LeakyReLU(0.2),\n            nn.utils.weight_norm(nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1)),\n\n            ResStack(64),\n\n            nn.LeakyReLU(0.2),\n            nn.utils.weight_norm(nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1)),\n\n            ResStack(32),\n\n            nn.LeakyReLU(0.2),\n            nn.ReflectionPad1d(3),\n            nn.utils.weight_norm(nn.Conv1d(32, 1, kernel_size=7, stride=1)),\n            nn.Tanh(),\n        )\n\n    def forward(self, mel):\n        mel = (mel + 5.0) / 5.0 # roughly normalize spectrogram\n        return self.generator(mel)\n\n    def eval(self, inference=False):\n        super(Generator, self).eval()\n\n        # don't remove weight norm while validation in training loop\n        if inference:\n            self.remove_weight_norm()\n\n    def remove_weight_norm(self):\n        for idx, layer in enumerate(self.generator):\n            if len(layer.state_dict()) != 0:\n                try:\n                    nn.utils.remove_weight_norm(layer)\n                except:\n                    layer.remove_weight_norm()\n\n    def inference(self, mel):\n        hop_length = 256\n        # pad input mel with zeros to cut artifact\n        # see https://github.com/seungwonpark/melgan/issues/8\n        zero = torch.full((1, self.mel_channel, 10), -11.5129).to(mel.device)\n        mel = torch.cat((mel, zero), dim=2)\n\n        audio = self.forward(mel)\n        audio = audio.squeeze() # collapse all dimension except time axis\n        audio = audio[:-(hop_length*10)]\n        audio = MAX_WAV_VALUE * audio\n        audio = audio.clamp(min=-MAX_WAV_VALUE, max=MAX_WAV_VALUE-1)\n        audio = audio.short()\n\n        return audio\n\n\n'''\n    to run this, fix \n    from . import ResStack\n    into\n    from res_stack import ResStack\n'''\nif __name__ == '__main__':\n    model = Generator(80)\n\n    x = torch.randn(3, 80, 10)\n    print(x.shape)\n\n    y = model(x)\n    print(y.shape)\n    assert y.shape == torch.Size([3, 1, 2560])\n\n    pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(pytorch_total_params)"""
model/identity.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def forward(self, x):\n        return x\n\n'"
model/multiscale.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .discriminator import Discriminator\nfrom .identity import Identity\n\n\nclass MultiScaleDiscriminator(nn.Module):\n    def __init__(self):\n        super(MultiScaleDiscriminator, self).__init__()\n\n        self.discriminators = nn.ModuleList(\n            [Discriminator() for _ in range(3)]\n        )\n        \n        self.pooling = nn.ModuleList(\n            [Identity()] +\n            [nn.AvgPool1d(kernel_size=4, stride=2, padding=1, count_include_pad=False) for _ in range(1, 3)]\n        )\n\n    def forward(self, x):\n        ret = list()\n\n        for pool, disc in zip(self.pooling, self.discriminators):\n            x = pool(x)\n            ret.append(disc(x))\n\n        return ret # [(feat, score), (feat, score), (feat, score)]\n'"
model/res_stack.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\nclass ResStack(nn.Module):\n    def __init__(self, channel):\n        super(ResStack, self).__init__()\n\n        self.blocks = nn.ModuleList([\n            nn.Sequential(\n                nn.LeakyReLU(0.2),\n                nn.ReflectionPad1d(3**i),\n                nn.utils.weight_norm(nn.Conv1d(channel, channel, kernel_size=3, dilation=3**i)),\n                nn.LeakyReLU(0.2),\n                nn.utils.weight_norm(nn.Conv1d(channel, channel, kernel_size=1)),\n            )\n            for i in range(3)\n        ])\n\n        self.shortcuts = nn.ModuleList([\n            nn.utils.weight_norm(nn.Conv1d(channel, channel, kernel_size=1))\n            for i in range(3)\n        ])\n\n    def forward(self, x):\n        for block, shortcut in zip(self.blocks, self.shortcuts):\n            x = shortcut(x) + block(x)\n        return x\n\n    def remove_weight_norm(self):\n        for block, shortcut in zip(self.blocks, self.shortcuts):\n            nn.utils.remove_weight_norm(block[2])\n            nn.utils.remove_weight_norm(block[4])\n            nn.utils.remove_weight_norm(shortcut)\n'"
utils/audio_processing.py,3,"b'import torch\nimport numpy as np\nfrom scipy.signal import get_window\nimport librosa.util as librosa_util\n\n\ndef window_sumsquare(window, n_frames, hop_length=200, win_length=800,\n                     n_fft=800, dtype=np.float32, norm=None):\n    """"""\n    # from librosa 0.6\n    Compute the sum-square envelope of a window function at a given hop length.\n\n    This is used to estimate modulation effects induced by windowing\n    observations in short-time fourier transforms.\n\n    Parameters\n    ----------\n    window : string, tuple, number, callable, or list-like\n        Window specification, as in `get_window`\n\n    n_frames : int > 0\n        The number of analysis frames\n\n    hop_length : int > 0\n        The number of samples to advance between frames\n\n    win_length : [optional]\n        The length of the window function.  By default, this matches `n_fft`.\n\n    n_fft : int > 0\n        The length of each analysis frame.\n\n    dtype : np.dtype\n        The data type of the output\n\n    Returns\n    -------\n    wss : np.ndarray, shape=`(n_fft + hop_length * (n_frames - 1))`\n        The sum-squared envelope of the window function\n    """"""\n    if win_length is None:\n        win_length = n_fft\n\n    n = n_fft + hop_length * (n_frames - 1)\n    x = np.zeros(n, dtype=dtype)\n\n    # Compute the squared window at the desired length\n    win_sq = get_window(window, win_length, fftbins=True)\n    win_sq = librosa_util.normalize(win_sq, norm=norm)**2\n    win_sq = librosa_util.pad_center(win_sq, n_fft)\n\n    # Fill the envelope\n    for i in range(n_frames):\n        sample = i * hop_length\n        x[sample:min(n, sample + n_fft)] += win_sq[:max(0, min(n_fft, n - sample))]\n    return x\n\n\ndef griffin_lim(magnitudes, stft_fn, n_iters=30):\n    """"""\n    PARAMS\n    ------\n    magnitudes: spectrogram magnitudes\n    stft_fn: STFT class with transform (STFT) and inverse (ISTFT) methods\n    """"""\n\n    angles = np.angle(np.exp(2j * np.pi * np.random.rand(*magnitudes.size())))\n    angles = angles.astype(np.float32)\n    angles = torch.autograd.Variable(torch.from_numpy(angles))\n    signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n\n    for i in range(n_iters):\n        _, angles = stft_fn.transform(signal)\n        signal = stft_fn.inverse(magnitudes, angles).squeeze(1)\n    return signal\n\n\ndef dynamic_range_compression(x, C=1, clip_val=1e-5):\n    """"""\n    PARAMS\n    ------\n    C: compression factor\n    """"""\n    return torch.log(torch.clamp(x, min=clip_val) * C)\n\n\ndef dynamic_range_decompression(x, C=1):\n    """"""\n    PARAMS\n    ------\n    C: compression factor used to compress\n    """"""\n    return torch.exp(x) / C\n'"
utils/hparams.py,0,"b'# modified from https://github.com/HarryVolek/PyTorch_Speaker_Verification\n\nimport os\nimport yaml\n\n\ndef load_hparam_str(hp_str):\n    path = \'temp-restore.yaml\'\n    with open(path, \'w\') as f:\n        f.write(hp_str)\n    ret = HParam(path)\n    os.remove(path)\n    return ret\n\n\ndef load_hparam(filename):\n    stream = open(filename, \'r\')\n    docs = yaml.load_all(stream, Loader=yaml.Loader)\n    hparam_dict = dict()\n    for doc in docs:\n        for k, v in doc.items():\n            hparam_dict[k] = v\n    return hparam_dict\n\n\ndef merge_dict(user, default):\n    if isinstance(user, dict) and isinstance(default, dict):\n        for k, v in default.items():\n            if k not in user:\n                user[k] = v\n            else:\n                user[k] = merge_dict(user[k], v)\n    return user\n\n\nclass Dotdict(dict):\n    """"""\n    a dictionary that supports dot notation \n    as well as dictionary access notation \n    usage: d = DotDict() or d = DotDict({\'val1\':\'first\'})\n    set attributes: d.val2 = \'second\' or d[\'val2\'] = \'second\'\n    get attributes: d.val2 or d[\'val2\']\n    """"""\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n    def __init__(self, dct=None):\n        dct = dict() if not dct else dct\n        for key, value in dct.items():\n            if hasattr(value, \'keys\'):\n                value = Dotdict(value)\n            self[key] = value\n\n\nclass HParam(Dotdict):\n\n    def __init__(self, file):\n        super(Dotdict, self).__init__()\n        hp_dict = load_hparam(file)\n        hp_dotdict = Dotdict(hp_dict)\n        for k, v in hp_dotdict.items():\n            setattr(self, k, v)\n            \n    __getattr__ = Dotdict.__getitem__\n    __setattr__ = Dotdict.__setitem__\n    __delattr__ = Dotdict.__delitem__\n'"
utils/plotting.py,0,"b'import matplotlib\nmatplotlib.use(""Agg"")\nimport matplotlib.pylab as plt\nimport numpy as np\n\n\ndef save_figure_to_numpy(fig):\n    # save it to a numpy array.\n    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\'\')\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    data = np.transpose(data, (2, 0, 1))\n    return data\n\n\ndef plot_waveform_to_numpy(waveform):\n    fig, ax = plt.subplots(figsize=(12, 3))\n    ax.plot()\n    ax.plot(range(len(waveform)), waveform,\n            linewidth=0.1, alpha=0.7, color=\'blue\')\n\n    plt.xlabel(""Samples"")\n    plt.ylabel(""Amplitude"")\n    plt.ylim(-1, 1)\n    plt.tight_layout()\n\n    fig.canvas.draw()\n    data = save_figure_to_numpy(fig)\n    plt.close()\n    return data\n'"
utils/stft.py,21,"b'""""""\nBSD 3-Clause License\n\nCopyright (c) 2017, Prem Seetharaman\nAll rights reserved.\n\n* Redistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice,\n  this list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the\n  documentation and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from this\n  software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n""""""\n\nimport torch\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom scipy.signal import get_window\nfrom librosa.util import pad_center, tiny\nfrom .audio_processing import window_sumsquare, dynamic_range_compression, dynamic_range_decompression\nfrom librosa.filters import mel as librosa_mel_fn\n\n\nclass STFT(torch.nn.Module):\n    """"""adapted from Prem Seetharaman\'s https://github.com/pseeth/pytorch-stft""""""\n    def __init__(self, filter_length=800, hop_length=200, win_length=800,\n                 window=\'hann\'):\n        super(STFT, self).__init__()\n        self.filter_length = filter_length\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.window = window\n        self.forward_transform = None\n        scale = self.filter_length / self.hop_length\n        fourier_basis = np.fft.fft(np.eye(self.filter_length))\n\n        cutoff = int((self.filter_length / 2 + 1))\n        fourier_basis = np.vstack([np.real(fourier_basis[:cutoff, :]),\n                                   np.imag(fourier_basis[:cutoff, :])])\n\n        forward_basis = torch.FloatTensor(fourier_basis[:, None, :])\n        inverse_basis = torch.FloatTensor(\n            np.linalg.pinv(scale * fourier_basis).T[:, None, :])\n\n        if window is not None:\n            assert(filter_length >= win_length)\n            # get window and zero center pad it to filter_length\n            fft_window = get_window(window, win_length, fftbins=True)\n            fft_window = pad_center(fft_window, filter_length)\n            fft_window = torch.from_numpy(fft_window).float()\n\n            # window the bases\n            forward_basis *= fft_window\n            inverse_basis *= fft_window\n\n        self.register_buffer(\'forward_basis\', forward_basis.float())\n        self.register_buffer(\'inverse_basis\', inverse_basis.float())\n\n    def transform(self, input_data):\n        num_batches = input_data.size(0)\n        num_samples = input_data.size(1)\n\n        self.num_samples = num_samples\n\n        # similar to librosa, reflect-pad the input\n        input_data = input_data.view(num_batches, 1, num_samples)\n        input_data = F.pad(\n            input_data.unsqueeze(1),\n            (int(self.filter_length / 2), int(self.filter_length / 2), 0, 0),\n            mode=\'reflect\')\n        input_data = input_data.squeeze(1)\n\n        # https://github.com/NVIDIA/tacotron2/issues/125\n        forward_transform = F.conv1d(\n            input_data.cuda(),\n            Variable(self.forward_basis, requires_grad=False).cuda(),\n            stride=self.hop_length,\n            padding=0).cpu()\n\n        cutoff = int((self.filter_length / 2) + 1)\n        real_part = forward_transform[:, :cutoff, :]\n        imag_part = forward_transform[:, cutoff:, :]\n\n        magnitude = torch.sqrt(real_part**2 + imag_part**2)\n        phase = torch.autograd.Variable(\n            torch.atan2(imag_part.data, real_part.data))\n\n        return magnitude, phase\n\n    def inverse(self, magnitude, phase):\n        recombine_magnitude_phase = torch.cat(\n            [magnitude*torch.cos(phase), magnitude*torch.sin(phase)], dim=1)\n\n        inverse_transform = F.conv_transpose1d(\n            recombine_magnitude_phase,\n            Variable(self.inverse_basis, requires_grad=False),\n            stride=self.hop_length,\n            padding=0)\n\n        if self.window is not None:\n            window_sum = window_sumsquare(\n                self.window, magnitude.size(-1), hop_length=self.hop_length,\n                win_length=self.win_length, n_fft=self.filter_length,\n                dtype=np.float32)\n            # remove modulation effects\n            approx_nonzero_indices = torch.from_numpy(\n                np.where(window_sum > tiny(window_sum))[0])\n            window_sum = torch.autograd.Variable(\n                torch.from_numpy(window_sum), requires_grad=False)\n            window_sum = window_sum.cuda() if magnitude.is_cuda else window_sum\n            inverse_transform[:, :, approx_nonzero_indices] /= window_sum[approx_nonzero_indices]\n\n            # scale by hop ratio\n            inverse_transform *= float(self.filter_length) / self.hop_length\n\n        inverse_transform = inverse_transform[:, :, int(self.filter_length/2):]\n        inverse_transform = inverse_transform[:, :, :-int(self.filter_length/2):]\n\n        return inverse_transform\n\n    def forward(self, input_data):\n        self.magnitude, self.phase = self.transform(input_data)\n        reconstruction = self.inverse(self.magnitude, self.phase)\n        return reconstruction\n\n\nclass TacotronSTFT(torch.nn.Module):\n    def __init__(self, filter_length=1024, hop_length=256, win_length=1024,\n                 n_mel_channels=80, sampling_rate=22050, mel_fmin=0.0,\n                 mel_fmax=None):\n        super(TacotronSTFT, self).__init__()\n        self.n_mel_channels = n_mel_channels\n        self.sampling_rate = sampling_rate\n        self.stft_fn = STFT(filter_length, hop_length, win_length)\n        mel_basis = librosa_mel_fn(\n            sampling_rate, filter_length, n_mel_channels, mel_fmin, mel_fmax)\n        mel_basis = torch.from_numpy(mel_basis).float()\n        self.register_buffer(\'mel_basis\', mel_basis)\n\n    def spectral_normalize(self, magnitudes):\n        output = dynamic_range_compression(magnitudes)\n        return output\n\n    def spectral_de_normalize(self, magnitudes):\n        output = dynamic_range_decompression(magnitudes)\n        return output\n\n    def mel_spectrogram(self, y):\n        """"""Computes mel-spectrograms from a batch of waves\n        PARAMS\n        ------\n        y: Variable(torch.FloatTensor) with shape (B, T) in range [-1, 1]\n\n        RETURNS\n        -------\n        mel_output: torch.FloatTensor of shape (B, n_mel_channels, T)\n        """"""\n        assert(torch.min(y.data) >= -1)\n        assert(torch.max(y.data) <= 1)\n\n        magnitudes, phases = self.stft_fn.transform(y)\n        magnitudes = magnitudes.data\n        mel_output = torch.matmul(self.mel_basis, magnitudes)\n        mel_output = self.spectral_normalize(mel_output)\n        return mel_output\n'"
utils/train.py,12,"b'import os\nimport math\nimport tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport itertools\nimport traceback\n\nfrom model.generator import Generator\nfrom model.multiscale import MultiScaleDiscriminator\nfrom .utils import get_commit_hash\nfrom .validation import validate\n\n\ndef train(args, pt_dir, chkpt_path, trainloader, valloader, writer, logger, hp, hp_str):\n    model_g = Generator(hp.audio.n_mel_channels).cuda()\n    model_d = MultiScaleDiscriminator().cuda()\n\n    optim_g = torch.optim.Adam(model_g.parameters(),\n        lr=hp.train.adam.lr, betas=(hp.train.adam.beta1, hp.train.adam.beta2))\n    optim_d = torch.optim.Adam(model_d.parameters(),\n        lr=hp.train.adam.lr, betas=(hp.train.adam.beta1, hp.train.adam.beta2))\n\n    githash = get_commit_hash()\n\n    init_epoch = -1\n    step = 0\n\n    if chkpt_path is not None:\n        logger.info(""Resuming from checkpoint: %s"" % chkpt_path)\n        checkpoint = torch.load(chkpt_path)\n        model_g.load_state_dict(checkpoint[\'model_g\'])\n        model_d.load_state_dict(checkpoint[\'model_d\'])\n        optim_g.load_state_dict(checkpoint[\'optim_g\'])\n        optim_d.load_state_dict(checkpoint[\'optim_d\'])\n        step = checkpoint[\'step\']\n        init_epoch = checkpoint[\'epoch\']\n\n        if hp_str != checkpoint[\'hp_str\']:\n            logger.warning(""New hparams is different from checkpoint. Will use new."")\n\n        if githash != checkpoint[\'githash\']:\n            logger.warning(""Code might be different: git hash is different."")\n            logger.warning(""%s -> %s"" % (checkpoint[\'githash\'], githash))\n\n    else:\n        logger.info(""Starting new training run."")\n\n    # this accelerates training when the size of minibatch is always consistent.\n    # if not consistent, it\'ll horribly slow down.\n    torch.backends.cudnn.benchmark = True\n\n    try:\n        model_g.train()\n        model_d.train()\n        for epoch in itertools.count(init_epoch+1):\n            if epoch % hp.log.validation_interval == 0:\n                with torch.no_grad():\n                    validate(hp, args, model_g, model_d, valloader, writer, step)\n\n            trainloader.dataset.shuffle_mapping()\n            loader = tqdm.tqdm(trainloader, desc=\'Loading train data\')\n            for (melG, audioG), (melD, audioD) in loader:\n                melG = melG.cuda()\n                audioG = audioG.cuda()\n                melD = melD.cuda()\n                audioD = audioD.cuda()\n\n                # generator\n                optim_g.zero_grad()\n                fake_audio = model_g(melG)[:, :, :hp.audio.segment_length]\n                disc_fake = model_d(fake_audio)\n                disc_real = model_d(audioG)\n                loss_g = 0.0\n                for (feats_fake, score_fake), (feats_real, _) in zip(disc_fake, disc_real):\n                    loss_g += torch.mean(torch.sum(torch.pow(score_fake - 1.0, 2), dim=[1, 2]))\n                    for feat_f, feat_r in zip(feats_fake, feats_real):\n                        loss_g += hp.model.feat_match * torch.mean(torch.abs(feat_f - feat_r))\n\n                loss_g.backward()\n                optim_g.step()\n\n                # discriminator\n                fake_audio = model_g(melD)[:, :, :hp.audio.segment_length]\n                fake_audio = fake_audio.detach()\n                loss_d_sum = 0.0\n                for _ in range(hp.train.rep_discriminator):\n                    optim_d.zero_grad()\n                    disc_fake = model_d(fake_audio)\n                    disc_real = model_d(audioD)\n                    loss_d = 0.0\n                    for (_, score_fake), (_, score_real) in zip(disc_fake, disc_real):\n                        loss_d += torch.mean(torch.sum(torch.pow(score_real - 1.0, 2), dim=[1, 2]))\n                        loss_d += torch.mean(torch.sum(torch.pow(score_fake, 2), dim=[1, 2]))\n\n                    loss_d.backward()\n                    optim_d.step()\n                    loss_d_sum += loss_d\n\n                step += 1\n                # logging\n                loss_g = loss_g.item()\n                loss_d_avg = loss_d_sum / hp.train.rep_discriminator\n                loss_d_avg = loss_d_avg.item()\n                if any([loss_g > 1e8, math.isnan(loss_g), loss_d_avg > 1e8, math.isnan(loss_d_avg)]):\n                    logger.error(""loss_g %.01f loss_d_avg %.01f at step %d!"" % (loss_g, loss_d_avg, step))\n                    raise Exception(""Loss exploded"")\n\n                if step % hp.log.summary_interval == 0:\n                    writer.log_training(loss_g, loss_d_avg, step)\n                    loader.set_description(""g %.04f d %.04f | step %d"" % (loss_g, loss_d_avg, step))\n\n            if epoch % hp.log.save_interval == 0:\n                save_path = os.path.join(pt_dir, \'%s_%s_%04d.pt\'\n                    % (args.name, githash, epoch))\n                torch.save({\n                    \'model_g\': model_g.state_dict(),\n                    \'model_d\': model_d.state_dict(),\n                    \'optim_g\': optim_g.state_dict(),\n                    \'optim_d\': optim_d.state_dict(),\n                    \'step\': step,\n                    \'epoch\': epoch,\n                    \'hp_str\': hp_str,\n                    \'githash\': githash,\n                }, save_path)\n                logger.info(""Saved checkpoint to: %s"" % save_path)\n\n    except Exception as e:\n        logger.info(""Exiting due to exception: %s"" % e)\n        traceback.print_exc()\n'"
utils/utils.py,0,"b'import random\nimport subprocess\nimport numpy as np\nfrom scipy.io.wavfile import read\n\n\ndef get_commit_hash():\n    message = subprocess.check_output([""git"", ""rev-parse"", ""--short"", ""HEAD""])\n    return message.strip().decode(\'utf-8\')\n\ndef read_wav_np(path):\n    sr, wav = read(path)\n\n    if len(wav.shape) == 2:\n        wav = wav[:, 0]\n\n    if wav.dtype == np.int16:\n        wav = wav / 32768.0\n    elif wav.dtype == np.int32:\n        wav = wav / 2147483648.0\n    elif wav.dtype == np.uint8:\n        wav = (wav - 128) / 128.0\n\n    wav = wav.astype(np.float32)\n\n    return sr, wav\n'"
utils/validation.py,6,"b""import tqdm\nimport torch\n\n\ndef validate(hp, args, generator, discriminator, valloader, writer, step):\n    generator.eval()\n    discriminator.eval()\n    torch.backends.cudnn.benchmark = False\n\n    loader = tqdm.tqdm(valloader, desc='Validation loop')\n    loss_g_sum = 0.0\n    loss_d_sum = 0.0\n    for mel, audio in loader:\n        mel = mel.cuda()\n        audio = audio.cuda()\n\n        # generator\n        fake_audio = generator(mel)\n        disc_fake = discriminator(fake_audio[:, :, :audio.size(2)])\n        disc_real = discriminator(audio)\n        loss_g = 0.0\n        loss_d = 0.0\n        for (feats_fake, score_fake), (feats_real, score_real) in zip(disc_fake, disc_real):\n            loss_g += torch.mean(torch.sum(torch.pow(score_fake - 1.0, 2), dim=[1, 2]))\n            for feat_f, feat_r in zip(feats_fake, feats_real):\n                loss_g += hp.model.feat_match * torch.mean(torch.abs(feat_f - feat_r))\n            loss_d += torch.mean(torch.sum(torch.pow(score_real - 1.0, 2), dim=[1, 2]))\n            loss_d += torch.mean(torch.sum(torch.pow(score_fake, 2), dim=[1, 2]))\n\n        loss_g_sum += loss_g.item()\n        loss_d_sum += loss_d.item()\n\n    loss_g_avg = loss_g_sum / len(valloader.dataset)\n    loss_d_avg = loss_d_sum / len(valloader.dataset)\n\n    audio = audio[0][0].cpu().detach().numpy()\n    fake_audio = fake_audio[0][0].cpu().detach().numpy()\n\n    writer.log_validation(loss_g_avg, loss_d_avg, generator, discriminator, audio, fake_audio, step)\n\n    torch.backends.cudnn.benchmark = True\n"""
utils/writer.py,0,"b""from tensorboardX import SummaryWriter\n\nfrom .plotting import plot_waveform_to_numpy\n\n\nclass MyWriter(SummaryWriter):\n    def __init__(self, hp, logdir):\n        super(MyWriter, self).__init__(logdir)\n        self.sample_rate = hp.audio.sampling_rate\n        self.is_first = True\n\n    def log_training(self, g_loss, d_loss, step):\n        self.add_scalar('train.g_loss', g_loss, step)\n        self.add_scalar('train.d_loss', d_loss, step)\n\n    def log_validation(self, g_loss, d_loss, generator, discriminator, target, prediction, step):\n        self.add_scalar('validation.g_loss', g_loss, step)\n        self.add_scalar('validation.d_loss', d_loss, step)\n\n        self.add_audio('raw_audio_predicted', prediction, step, self.sample_rate)\n        self.add_image('waveform_predicted', plot_waveform_to_numpy(prediction), step)\n\n        self.log_histogram(generator, step)\n        self.log_histogram(discriminator, step)\n\n        if self.is_first:\n            self.add_audio('raw_audio_target', target, step, self.sample_rate)\n            self.add_image('waveform_target', plot_waveform_to_numpy(target), step)\n            self.is_first = False\n\n    def log_histogram(self, model, step):\n        for tag, value in model.named_parameters():\n            self.add_histogram(tag.replace('.', '/'), value.cpu().detach().numpy(), step)\n"""
