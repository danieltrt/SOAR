file_path,api_count,code
caption.py,11,"b'import torch\nimport torch.nn.functional as F\nimport numpy as np\nimport json\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport skimage.transform\nimport argparse\nfrom scipy.misc import imread, imresize\nfrom PIL import Image\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n\ndef caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3):\n    """"""\n    Reads an image and captions it with beam search.\n\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param image_path: path to image\n    :param word_map: word map\n    :param beam_size: number of sequences to consider at each decode-step\n    :return: caption, weights for visualization\n    """"""\n\n    k = beam_size\n    vocab_size = len(word_map)\n\n    # Read image and process\n    img = imread(image_path)\n    if len(img.shape) == 2:\n        img = img[:, :, np.newaxis]\n        img = np.concatenate([img, img, img], axis=2)\n    img = imresize(img, (256, 256))\n    img = img.transpose(2, 0, 1)\n    img = img / 255.\n    img = torch.FloatTensor(img).to(device)\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    transform = transforms.Compose([normalize])\n    image = transform(img)  # (3, 256, 256)\n\n    # Encode\n    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n    enc_image_size = encoder_out.size(1)\n    encoder_dim = encoder_out.size(3)\n\n    # Flatten encoding\n    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n    num_pixels = encoder_out.size(1)\n\n    # We\'ll treat the problem as having a batch size of k\n    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n\n    # Tensor to store top k previous words at each step; now they\'re just <start>\n    k_prev_words = torch.LongTensor([[word_map[\'<start>\']]] * k).to(device)  # (k, 1)\n\n    # Tensor to store top k sequences; now they\'re just <start>\n    seqs = k_prev_words  # (k, 1)\n\n    # Tensor to store top k sequences\' scores; now they\'re just 0\n    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n\n    # Tensor to store top k sequences\' alphas; now they\'re just 1s\n    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n\n    # Lists to store completed sequences, their alphas and scores\n    complete_seqs = list()\n    complete_seqs_alpha = list()\n    complete_seqs_scores = list()\n\n    # Start decoding\n    step = 1\n    h, c = decoder.init_hidden_state(encoder_out)\n\n    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n    while True:\n\n        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n\n        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n\n        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n\n        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n        awe = gate * awe\n\n        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n\n        scores = decoder.fc(h)  # (s, vocab_size)\n        scores = F.log_softmax(scores, dim=1)\n\n        # Add\n        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n\n        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n        if step == 1:\n            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n        else:\n            # Unroll and find top scores, and their unrolled indices\n            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n\n        # Convert unrolled indices to actual indices of scores\n        prev_word_inds = top_k_words / vocab_size  # (s)\n        next_word_inds = top_k_words % vocab_size  # (s)\n\n        # Add new words to sequences, alphas\n        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n\n        # Which sequences are incomplete (didn\'t reach <end>)?\n        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n                           next_word != word_map[\'<end>\']]\n        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n\n        # Set aside complete sequences\n        if len(complete_inds) > 0:\n            complete_seqs.extend(seqs[complete_inds].tolist())\n            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n            complete_seqs_scores.extend(top_k_scores[complete_inds])\n        k -= len(complete_inds)  # reduce beam length accordingly\n\n        # Proceed with incomplete sequences\n        if k == 0:\n            break\n        seqs = seqs[incomplete_inds]\n        seqs_alpha = seqs_alpha[incomplete_inds]\n        h = h[prev_word_inds[incomplete_inds]]\n        c = c[prev_word_inds[incomplete_inds]]\n        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n\n        # Break if things have been going on too long\n        if step > 50:\n            break\n        step += 1\n\n    i = complete_seqs_scores.index(max(complete_seqs_scores))\n    seq = complete_seqs[i]\n    alphas = complete_seqs_alpha[i]\n\n    return seq, alphas\n\n\ndef visualize_att(image_path, seq, alphas, rev_word_map, smooth=True):\n    """"""\n    Visualizes caption with weights at every word.\n\n    Adapted from paper authors\' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n\n    :param image_path: path to image that has been captioned\n    :param seq: caption\n    :param alphas: weights\n    :param rev_word_map: reverse word mapping, i.e. ix2word\n    :param smooth: smooth weights?\n    """"""\n    image = Image.open(image_path)\n    image = image.resize([14 * 24, 14 * 24], Image.LANCZOS)\n\n    words = [rev_word_map[ind] for ind in seq]\n\n    for t in range(len(words)):\n        if t > 50:\n            break\n        plt.subplot(np.ceil(len(words) / 5.), 5, t + 1)\n\n        plt.text(0, 1, \'%s\' % (words[t]), color=\'black\', backgroundcolor=\'white\', fontsize=12)\n        plt.imshow(image)\n        current_alpha = alphas[t, :]\n        if smooth:\n            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=24, sigma=8)\n        else:\n            alpha = skimage.transform.resize(current_alpha.numpy(), [14 * 24, 14 * 24])\n        if t == 0:\n            plt.imshow(alpha, alpha=0)\n        else:\n            plt.imshow(alpha, alpha=0.8)\n        plt.set_cmap(cm.Greys_r)\n        plt.axis(\'off\')\n    plt.show()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Show, Attend, and Tell - Tutorial - Generate Caption\')\n\n    parser.add_argument(\'--img\', \'-i\', help=\'path to image\')\n    parser.add_argument(\'--model\', \'-m\', help=\'path to model\')\n    parser.add_argument(\'--word_map\', \'-wm\', help=\'path to word map JSON\')\n    parser.add_argument(\'--beam_size\', \'-b\', default=5, type=int, help=\'beam size for beam search\')\n    parser.add_argument(\'--dont_smooth\', dest=\'smooth\', action=\'store_false\', help=\'do not smooth alpha overlay\')\n\n    args = parser.parse_args()\n\n    # Load model\n    checkpoint = torch.load(args.model, map_location=str(device))\n    decoder = checkpoint[\'decoder\']\n    decoder = decoder.to(device)\n    decoder.eval()\n    encoder = checkpoint[\'encoder\']\n    encoder = encoder.to(device)\n    encoder.eval()\n\n    # Load word map (word2ix)\n    with open(args.word_map, \'r\') as j:\n        word_map = json.load(j)\n    rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n\n    # Encode, decode with attention and beam search\n    seq, alphas = caption_image_beam_search(encoder, decoder, args.img, word_map, args.beam_size)\n    alphas = torch.FloatTensor(alphas)\n\n    # Visualize caption and attention of best sequence\n    visualize_att(args.img, seq, alphas, rev_word_map, args.smooth)\n'"
create_input_files.py,0,"b""from utils import create_input_files\n\nif __name__ == '__main__':\n    # Create input files (along with word map)\n    create_input_files(dataset='coco',\n                       karpathy_json_path='../caption data/dataset_coco.json',\n                       image_folder='/media/ssd/caption data/',\n                       captions_per_image=5,\n                       min_word_freq=5,\n                       output_folder='/media/ssd/caption data/',\n                       max_len=50)\n"""
datasets.py,5,"b'import torch\nfrom torch.utils.data import Dataset\nimport h5py\nimport json\nimport os\n\n\nclass CaptionDataset(Dataset):\n    """"""\n    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n    """"""\n\n    def __init__(self, data_folder, data_name, split, transform=None):\n        """"""\n        :param data_folder: folder where data files are stored\n        :param data_name: base name of processed datasets\n        :param split: split, one of \'TRAIN\', \'VAL\', or \'TEST\'\n        :param transform: image transform pipeline\n        """"""\n        self.split = split\n        assert self.split in {\'TRAIN\', \'VAL\', \'TEST\'}\n\n        # Open hdf5 file where images are stored\n        self.h = h5py.File(os.path.join(data_folder, self.split + \'_IMAGES_\' + data_name + \'.hdf5\'), \'r\')\n        self.imgs = self.h[\'images\']\n\n        # Captions per image\n        self.cpi = self.h.attrs[\'captions_per_image\']\n\n        # Load encoded captions (completely into memory)\n        with open(os.path.join(data_folder, self.split + \'_CAPTIONS_\' + data_name + \'.json\'), \'r\') as j:\n            self.captions = json.load(j)\n\n        # Load caption lengths (completely into memory)\n        with open(os.path.join(data_folder, self.split + \'_CAPLENS_\' + data_name + \'.json\'), \'r\') as j:\n            self.caplens = json.load(j)\n\n        # PyTorch transformation pipeline for the image (normalizing, etc.)\n        self.transform = transform\n\n        # Total number of datapoints\n        self.dataset_size = len(self.captions)\n\n    def __getitem__(self, i):\n        # Remember, the Nth caption corresponds to the (N // captions_per_image)th image\n        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n        if self.transform is not None:\n            img = self.transform(img)\n\n        caption = torch.LongTensor(self.captions[i])\n\n        caplen = torch.LongTensor([self.caplens[i]])\n\n        if self.split is \'TRAIN\':\n            return img, caption, caplen\n        else:\n            # For validation of testing, also return all \'captions_per_image\' captions to find BLEU-4 score\n            all_captions = torch.LongTensor(\n                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n            return img, caption, caplen, all_captions\n\n    def __len__(self):\n        return self.dataset_size\n'"
eval.py,11,"b'import torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nfrom datasets import *\nfrom utils import *\nfrom nltk.translate.bleu_score import corpus_bleu\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\n# Parameters\ndata_folder = \'/media/ssd/caption data\'  # folder with data files saved by create_input_files.py\ndata_name = \'coco_5_cap_per_img_5_min_word_freq\'  # base name shared by data files\ncheckpoint = \'../BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar\'  # model checkpoint\nword_map_file = \'/media/ssd/caption data/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json\'  # word map, ensure it\'s the same the data was encoded with and the model was trained with\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")  # sets device for model and PyTorch tensors\ncudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n\n# Load model\ncheckpoint = torch.load(checkpoint)\ndecoder = checkpoint[\'decoder\']\ndecoder = decoder.to(device)\ndecoder.eval()\nencoder = checkpoint[\'encoder\']\nencoder = encoder.to(device)\nencoder.eval()\n\n# Load word map (word2ix)\nwith open(word_map_file, \'r\') as j:\n    word_map = json.load(j)\nrev_word_map = {v: k for k, v in word_map.items()}\nvocab_size = len(word_map)\n\n# Normalization transform\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n\ndef evaluate(beam_size):\n    """"""\n    Evaluation\n\n    :param beam_size: beam size at which to generate captions for evaluation\n    :return: BLEU-4 score\n    """"""\n    # DataLoader\n    loader = torch.utils.data.DataLoader(\n        CaptionDataset(data_folder, data_name, \'TEST\', transform=transforms.Compose([normalize])),\n        batch_size=1, shuffle=True, num_workers=1, pin_memory=True)\n\n    # TODO: Batched Beam Search\n    # Therefore, do not use a batch_size greater than 1 - IMPORTANT!\n\n    # Lists to store references (true captions), and hypothesis (prediction) for each image\n    # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n    # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n    references = list()\n    hypotheses = list()\n\n    # For each image\n    for i, (image, caps, caplens, allcaps) in enumerate(\n            tqdm(loader, desc=""EVALUATING AT BEAM SIZE "" + str(beam_size))):\n\n        k = beam_size\n\n        # Move to GPU device, if available\n        image = image.to(device)  # (1, 3, 256, 256)\n\n        # Encode\n        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n        enc_image_size = encoder_out.size(1)\n        encoder_dim = encoder_out.size(3)\n\n        # Flatten encoding\n        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        # We\'ll treat the problem as having a batch size of k\n        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n\n        # Tensor to store top k previous words at each step; now they\'re just <start>\n        k_prev_words = torch.LongTensor([[word_map[\'<start>\']]] * k).to(device)  # (k, 1)\n\n        # Tensor to store top k sequences; now they\'re just <start>\n        seqs = k_prev_words  # (k, 1)\n\n        # Tensor to store top k sequences\' scores; now they\'re just 0\n        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n\n        # Lists to store completed sequences and scores\n        complete_seqs = list()\n        complete_seqs_scores = list()\n\n        # Start decoding\n        step = 1\n        h, c = decoder.init_hidden_state(encoder_out)\n\n        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n        while True:\n\n            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n\n            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n\n            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n            awe = gate * awe\n\n            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n\n            scores = decoder.fc(h)  # (s, vocab_size)\n            scores = F.log_softmax(scores, dim=1)\n\n            # Add\n            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n\n            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n            if step == 1:\n                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n            else:\n                # Unroll and find top scores, and their unrolled indices\n                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n\n            # Convert unrolled indices to actual indices of scores\n            prev_word_inds = top_k_words / vocab_size  # (s)\n            next_word_inds = top_k_words % vocab_size  # (s)\n\n            # Add new words to sequences\n            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n\n            # Which sequences are incomplete (didn\'t reach <end>)?\n            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n                               next_word != word_map[\'<end>\']]\n            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n\n            # Set aside complete sequences\n            if len(complete_inds) > 0:\n                complete_seqs.extend(seqs[complete_inds].tolist())\n                complete_seqs_scores.extend(top_k_scores[complete_inds])\n            k -= len(complete_inds)  # reduce beam length accordingly\n\n            # Proceed with incomplete sequences\n            if k == 0:\n                break\n            seqs = seqs[incomplete_inds]\n            h = h[prev_word_inds[incomplete_inds]]\n            c = c[prev_word_inds[incomplete_inds]]\n            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n\n            # Break if things have been going on too long\n            if step > 50:\n                break\n            step += 1\n\n        i = complete_seqs_scores.index(max(complete_seqs_scores))\n        seq = complete_seqs[i]\n\n        # References\n        img_caps = allcaps[0].tolist()\n        img_captions = list(\n            map(lambda c: [w for w in c if w not in {word_map[\'<start>\'], word_map[\'<end>\'], word_map[\'<pad>\']}],\n                img_caps))  # remove <start> and pads\n        references.append(img_captions)\n\n        # Hypotheses\n        hypotheses.append([w for w in seq if w not in {word_map[\'<start>\'], word_map[\'<end>\'], word_map[\'<pad>\']}])\n\n        assert len(references) == len(hypotheses)\n\n    # Calculate BLEU-4 scores\n    bleu4 = corpus_bleu(references, hypotheses)\n\n    return bleu4\n\n\nif __name__ == \'__main__\':\n    beam_size = 1\n    print(""\\nBLEU-4 score @ beam size of %d is %.4f."" % (beam_size, evaluate(beam_size)))\n'"
models.py,4,"b'import torch\nfrom torch import nn\nimport torchvision\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n\nclass Encoder(nn.Module):\n    """"""\n    Encoder.\n    """"""\n\n    def __init__(self, encoded_image_size=14):\n        super(Encoder, self).__init__()\n        self.enc_image_size = encoded_image_size\n\n        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n\n        # Remove linear and pool layers (since we\'re not doing classification)\n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n\n        # Resize image to fixed size to allow input images of variable size\n        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n\n        self.fine_tune()\n\n    def forward(self, images):\n        """"""\n        Forward propagation.\n\n        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n        :return: encoded images\n        """"""\n        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n        return out\n\n    def fine_tune(self, fine_tune=True):\n        """"""\n        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n\n        :param fine_tune: Allow?\n        """"""\n        for p in self.resnet.parameters():\n            p.requires_grad = False\n        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n        for c in list(self.resnet.children())[5:]:\n            for p in c.parameters():\n                p.requires_grad = fine_tune\n\n\nclass Attention(nn.Module):\n    """"""\n    Attention Network.\n    """"""\n\n    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n        """"""\n        :param encoder_dim: feature size of encoded images\n        :param decoder_dim: size of decoder\'s RNN\n        :param attention_dim: size of the attention network\n        """"""\n        super(Attention, self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder\'s output\n        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n\n    def forward(self, encoder_out, decoder_hidden):\n        """"""\n        Forward propagation.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n        :return: attention weighted encoding, weights\n        """"""\n        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n        alpha = self.softmax(att)  # (batch_size, num_pixels)\n        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n\n        return attention_weighted_encoding, alpha\n\n\nclass DecoderWithAttention(nn.Module):\n    """"""\n    Decoder.\n    """"""\n\n    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n        """"""\n        :param attention_dim: size of attention network\n        :param embed_dim: embedding size\n        :param decoder_dim: size of decoder\'s RNN\n        :param vocab_size: size of vocabulary\n        :param encoder_dim: feature size of encoded images\n        :param dropout: dropout\n        """"""\n        super(DecoderWithAttention, self).__init__()\n\n        self.encoder_dim = encoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.decoder_dim = decoder_dim\n        self.vocab_size = vocab_size\n        self.dropout = dropout\n\n        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n        self.dropout = nn.Dropout(p=self.dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n        self.init_weights()  # initialize some layers with the uniform distribution\n\n    def init_weights(self):\n        """"""\n        Initializes some parameters with values from the uniform distribution, for easier convergence.\n        """"""\n        self.embedding.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n\n    def load_pretrained_embeddings(self, embeddings):\n        """"""\n        Loads embedding layer with pre-trained embeddings.\n\n        :param embeddings: pre-trained embeddings\n        """"""\n        self.embedding.weight = nn.Parameter(embeddings)\n\n    def fine_tune_embeddings(self, fine_tune=True):\n        """"""\n        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n\n        :param fine_tune: Allow?\n        """"""\n        for p in self.embedding.parameters():\n            p.requires_grad = fine_tune\n\n    def init_hidden_state(self, encoder_out):\n        """"""\n        Creates the initial hidden and cell states for the decoder\'s LSTM based on the encoded images.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :return: hidden state, cell state\n        """"""\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n\n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        """"""\n        Forward propagation.\n\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        """"""\n\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n\n        # Flatten image\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        # Sort input data by decreasing lengths; why? apparent below\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n\n        # Initialize LSTM state\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n\n        # We won\'t decode at the <end> position, since we\'ve finished generating as soon as we generate <end>\n        # So, decoding lengths are actual lengths - 1\n        decode_lengths = (caption_lengths - 1).tolist()\n\n        # Create tensors to hold word predicion scores and alphas\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n\n        # At each time-step, decode by\n        # attention-weighing the encoder\'s output based on the decoder\'s previous hidden state output\n        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n                                                                h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n'"
train.py,13,"b'import time\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torchvision.transforms as transforms\nfrom torch import nn\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom models import Encoder, DecoderWithAttention\nfrom datasets import *\nfrom utils import *\nfrom nltk.translate.bleu_score import corpus_bleu\n\n# Data parameters\ndata_folder = \'/media/ssd/caption data\'  # folder with data files saved by create_input_files.py\ndata_name = \'coco_5_cap_per_img_5_min_word_freq\'  # base name shared by data files\n\n# Model parameters\nemb_dim = 512  # dimension of word embeddings\nattention_dim = 512  # dimension of attention linear layers\ndecoder_dim = 512  # dimension of decoder RNN\ndropout = 0.5\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")  # sets device for model and PyTorch tensors\ncudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n\n# Training parameters\nstart_epoch = 0\nepochs = 120  # number of epochs to train for (if early stopping is not triggered)\nepochs_since_improvement = 0  # keeps track of number of epochs since there\'s been an improvement in validation BLEU\nbatch_size = 32\nworkers = 1  # for data-loading; right now, only 1 works with h5py\nencoder_lr = 1e-4  # learning rate for encoder if fine-tuning\ndecoder_lr = 4e-4  # learning rate for decoder\ngrad_clip = 5.  # clip gradients at an absolute value of\nalpha_c = 1.  # regularization parameter for \'doubly stochastic attention\', as in the paper\nbest_bleu4 = 0.  # BLEU-4 score right now\nprint_freq = 100  # print training/validation stats every __ batches\nfine_tune_encoder = False  # fine-tune encoder?\ncheckpoint = None  # path to checkpoint, None if none\n\n\ndef main():\n    """"""\n    Training and validation.\n    """"""\n\n    global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder, data_name, word_map\n\n    # Read word map\n    word_map_file = os.path.join(data_folder, \'WORDMAP_\' + data_name + \'.json\')\n    with open(word_map_file, \'r\') as j:\n        word_map = json.load(j)\n\n    # Initialize / load checkpoint\n    if checkpoint is None:\n        decoder = DecoderWithAttention(attention_dim=attention_dim,\n                                       embed_dim=emb_dim,\n                                       decoder_dim=decoder_dim,\n                                       vocab_size=len(word_map),\n                                       dropout=dropout)\n        decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n                                             lr=decoder_lr)\n        encoder = Encoder()\n        encoder.fine_tune(fine_tune_encoder)\n        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n                                             lr=encoder_lr) if fine_tune_encoder else None\n\n    else:\n        checkpoint = torch.load(checkpoint)\n        start_epoch = checkpoint[\'epoch\'] + 1\n        epochs_since_improvement = checkpoint[\'epochs_since_improvement\']\n        best_bleu4 = checkpoint[\'bleu-4\']\n        decoder = checkpoint[\'decoder\']\n        decoder_optimizer = checkpoint[\'decoder_optimizer\']\n        encoder = checkpoint[\'encoder\']\n        encoder_optimizer = checkpoint[\'encoder_optimizer\']\n        if fine_tune_encoder is True and encoder_optimizer is None:\n            encoder.fine_tune(fine_tune_encoder)\n            encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n                                                 lr=encoder_lr)\n\n    # Move to GPU, if available\n    decoder = decoder.to(device)\n    encoder = encoder.to(device)\n\n    # Loss function\n    criterion = nn.CrossEntropyLoss().to(device)\n\n    # Custom dataloaders\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\n    train_loader = torch.utils.data.DataLoader(\n        CaptionDataset(data_folder, data_name, \'TRAIN\', transform=transforms.Compose([normalize])),\n        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n    val_loader = torch.utils.data.DataLoader(\n        CaptionDataset(data_folder, data_name, \'VAL\', transform=transforms.Compose([normalize])),\n        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n\n    # Epochs\n    for epoch in range(start_epoch, epochs):\n\n        # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n        if epochs_since_improvement == 20:\n            break\n        if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n            adjust_learning_rate(decoder_optimizer, 0.8)\n            if fine_tune_encoder:\n                adjust_learning_rate(encoder_optimizer, 0.8)\n\n        # One epoch\'s training\n        train(train_loader=train_loader,\n              encoder=encoder,\n              decoder=decoder,\n              criterion=criterion,\n              encoder_optimizer=encoder_optimizer,\n              decoder_optimizer=decoder_optimizer,\n              epoch=epoch)\n\n        # One epoch\'s validation\n        recent_bleu4 = validate(val_loader=val_loader,\n                                encoder=encoder,\n                                decoder=decoder,\n                                criterion=criterion)\n\n        # Check if there was an improvement\n        is_best = recent_bleu4 > best_bleu4\n        best_bleu4 = max(recent_bleu4, best_bleu4)\n        if not is_best:\n            epochs_since_improvement += 1\n            print(""\\nEpochs since last improvement: %d\\n"" % (epochs_since_improvement,))\n        else:\n            epochs_since_improvement = 0\n\n        # Save checkpoint\n        save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n                        decoder_optimizer, recent_bleu4, is_best)\n\n\ndef train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n    """"""\n    Performs one epoch\'s training.\n\n    :param train_loader: DataLoader for training data\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param criterion: loss layer\n    :param encoder_optimizer: optimizer to update encoder\'s weights (if fine-tuning)\n    :param decoder_optimizer: optimizer to update decoder\'s weights\n    :param epoch: epoch number\n    """"""\n\n    decoder.train()  # train mode (dropout and batchnorm is used)\n    encoder.train()\n\n    batch_time = AverageMeter()  # forward prop. + back prop. time\n    data_time = AverageMeter()  # data loading time\n    losses = AverageMeter()  # loss (per word decoded)\n    top5accs = AverageMeter()  # top5 accuracy\n\n    start = time.time()\n\n    # Batches\n    for i, (imgs, caps, caplens) in enumerate(train_loader):\n        data_time.update(time.time() - start)\n\n        # Move to GPU, if available\n        imgs = imgs.to(device)\n        caps = caps.to(device)\n        caplens = caplens.to(device)\n\n        # Forward prop.\n        imgs = encoder(imgs)\n        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n\n        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n        targets = caps_sorted[:, 1:]\n\n        # Remove timesteps that we didn\'t decode at, or are pads\n        # pack_padded_sequence is an easy trick to do this\n        scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n        targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n\n        # Calculate loss\n        loss = criterion(scores, targets)\n\n        # Add doubly stochastic attention regularization\n        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n        # Back prop.\n        decoder_optimizer.zero_grad()\n        if encoder_optimizer is not None:\n            encoder_optimizer.zero_grad()\n        loss.backward()\n\n        # Clip gradients\n        if grad_clip is not None:\n            clip_gradient(decoder_optimizer, grad_clip)\n            if encoder_optimizer is not None:\n                clip_gradient(encoder_optimizer, grad_clip)\n\n        # Update weights\n        decoder_optimizer.step()\n        if encoder_optimizer is not None:\n            encoder_optimizer.step()\n\n        # Keep track of metrics\n        top5 = accuracy(scores, targets, 5)\n        losses.update(loss.item(), sum(decode_lengths))\n        top5accs.update(top5, sum(decode_lengths))\n        batch_time.update(time.time() - start)\n\n        start = time.time()\n\n        # Print status\n        if i % print_freq == 0:\n            print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                  \'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\'.format(epoch, i, len(train_loader),\n                                                                          batch_time=batch_time,\n                                                                          data_time=data_time, loss=losses,\n                                                                          top5=top5accs))\n\n\ndef validate(val_loader, encoder, decoder, criterion):\n    """"""\n    Performs one epoch\'s validation.\n\n    :param val_loader: DataLoader for validation data.\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param criterion: loss layer\n    :return: BLEU-4 score\n    """"""\n    decoder.eval()  # eval mode (no dropout or batchnorm)\n    if encoder is not None:\n        encoder.eval()\n\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top5accs = AverageMeter()\n\n    start = time.time()\n\n    references = list()  # references (true captions) for calculating BLEU-4 score\n    hypotheses = list()  # hypotheses (predictions)\n\n    # explicitly disable gradient calculation to avoid CUDA memory error\n    # solves the issue #57\n    with torch.no_grad():\n        # Batches\n        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n\n            # Move to device, if available\n            imgs = imgs.to(device)\n            caps = caps.to(device)\n            caplens = caplens.to(device)\n\n            # Forward prop.\n            if encoder is not None:\n                imgs = encoder(imgs)\n            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n\n            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n            targets = caps_sorted[:, 1:]\n\n            # Remove timesteps that we didn\'t decode at, or are pads\n            # pack_padded_sequence is an easy trick to do this\n            scores_copy = scores.clone()\n            scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n            targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n\n            # Calculate loss\n            loss = criterion(scores, targets)\n\n            # Add doubly stochastic attention regularization\n            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n            # Keep track of metrics\n            losses.update(loss.item(), sum(decode_lengths))\n            top5 = accuracy(scores, targets, 5)\n            top5accs.update(top5, sum(decode_lengths))\n            batch_time.update(time.time() - start)\n\n            start = time.time()\n\n            if i % print_freq == 0:\n                print(\'Validation: [{0}/{1}]\\t\'\n                      \'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                      \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                      \'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t\'.format(i, len(val_loader), batch_time=batch_time,\n                                                                                loss=losses, top5=top5accs))\n\n            # Store references (true captions), and hypothesis (prediction) for each image\n            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n\n            # References\n            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n            for j in range(allcaps.shape[0]):\n                img_caps = allcaps[j].tolist()\n                img_captions = list(\n                    map(lambda c: [w for w in c if w not in {word_map[\'<start>\'], word_map[\'<pad>\']}],\n                        img_caps))  # remove <start> and pads\n                references.append(img_captions)\n\n            # Hypotheses\n            _, preds = torch.max(scores_copy, dim=2)\n            preds = preds.tolist()\n            temp_preds = list()\n            for j, p in enumerate(preds):\n                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n            preds = temp_preds\n            hypotheses.extend(preds)\n\n            assert len(references) == len(hypotheses)\n\n        # Calculate BLEU-4 scores\n        bleu4 = corpus_bleu(references, hypotheses)\n\n        print(\n            \'\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n\'.format(\n                loss=losses,\n                top5=top5accs,\n                bleu=bleu4))\n\n    return bleu4\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils.py,5,"b'import os\nimport numpy as np\nimport h5py\nimport json\nimport torch\nfrom scipy.misc import imread, imresize\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom random import seed, choice, sample\n\n\ndef create_input_files(dataset, karpathy_json_path, image_folder, captions_per_image, min_word_freq, output_folder,\n                       max_len=100):\n    """"""\n    Creates input files for training, validation, and test data.\n\n    :param dataset: name of dataset, one of \'coco\', \'flickr8k\', \'flickr30k\'\n    :param karpathy_json_path: path of Karpathy JSON file with splits and captions\n    :param image_folder: folder with downloaded images\n    :param captions_per_image: number of captions to sample per image\n    :param min_word_freq: words occuring less frequently than this threshold are binned as <unk>s\n    :param output_folder: folder to save files\n    :param max_len: don\'t sample captions longer than this length\n    """"""\n\n    assert dataset in {\'coco\', \'flickr8k\', \'flickr30k\'}\n\n    # Read Karpathy JSON\n    with open(karpathy_json_path, \'r\') as j:\n        data = json.load(j)\n\n    # Read image paths and captions for each image\n    train_image_paths = []\n    train_image_captions = []\n    val_image_paths = []\n    val_image_captions = []\n    test_image_paths = []\n    test_image_captions = []\n    word_freq = Counter()\n\n    for img in data[\'images\']:\n        captions = []\n        for c in img[\'sentences\']:\n            # Update word frequency\n            word_freq.update(c[\'tokens\'])\n            if len(c[\'tokens\']) <= max_len:\n                captions.append(c[\'tokens\'])\n\n        if len(captions) == 0:\n            continue\n\n        path = os.path.join(image_folder, img[\'filepath\'], img[\'filename\']) if dataset == \'coco\' else os.path.join(\n            image_folder, img[\'filename\'])\n\n        if img[\'split\'] in {\'train\', \'restval\'}:\n            train_image_paths.append(path)\n            train_image_captions.append(captions)\n        elif img[\'split\'] in {\'val\'}:\n            val_image_paths.append(path)\n            val_image_captions.append(captions)\n        elif img[\'split\'] in {\'test\'}:\n            test_image_paths.append(path)\n            test_image_captions.append(captions)\n\n    # Sanity check\n    assert len(train_image_paths) == len(train_image_captions)\n    assert len(val_image_paths) == len(val_image_captions)\n    assert len(test_image_paths) == len(test_image_captions)\n\n    # Create word map\n    words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n    word_map = {k: v + 1 for v, k in enumerate(words)}\n    word_map[\'<unk>\'] = len(word_map) + 1\n    word_map[\'<start>\'] = len(word_map) + 1\n    word_map[\'<end>\'] = len(word_map) + 1\n    word_map[\'<pad>\'] = 0\n\n    # Create a base/root name for all output files\n    base_filename = dataset + \'_\' + str(captions_per_image) + \'_cap_per_img_\' + str(min_word_freq) + \'_min_word_freq\'\n\n    # Save word map to a JSON\n    with open(os.path.join(output_folder, \'WORDMAP_\' + base_filename + \'.json\'), \'w\') as j:\n        json.dump(word_map, j)\n\n    # Sample captions for each image, save images to HDF5 file, and captions and their lengths to JSON files\n    seed(123)\n    for impaths, imcaps, split in [(train_image_paths, train_image_captions, \'TRAIN\'),\n                                   (val_image_paths, val_image_captions, \'VAL\'),\n                                   (test_image_paths, test_image_captions, \'TEST\')]:\n\n        with h5py.File(os.path.join(output_folder, split + \'_IMAGES_\' + base_filename + \'.hdf5\'), \'a\') as h:\n            # Make a note of the number of captions we are sampling per image\n            h.attrs[\'captions_per_image\'] = captions_per_image\n\n            # Create dataset inside HDF5 file to store images\n            images = h.create_dataset(\'images\', (len(impaths), 3, 256, 256), dtype=\'uint8\')\n\n            print(""\\nReading %s images and captions, storing to file...\\n"" % split)\n\n            enc_captions = []\n            caplens = []\n\n            for i, path in enumerate(tqdm(impaths)):\n\n                # Sample captions\n                if len(imcaps[i]) < captions_per_image:\n                    captions = imcaps[i] + [choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]\n                else:\n                    captions = sample(imcaps[i], k=captions_per_image)\n\n                # Sanity check\n                assert len(captions) == captions_per_image\n\n                # Read images\n                img = imread(impaths[i])\n                if len(img.shape) == 2:\n                    img = img[:, :, np.newaxis]\n                    img = np.concatenate([img, img, img], axis=2)\n                img = imresize(img, (256, 256))\n                img = img.transpose(2, 0, 1)\n                assert img.shape == (3, 256, 256)\n                assert np.max(img) <= 255\n\n                # Save image to HDF5 file\n                images[i] = img\n\n                for j, c in enumerate(captions):\n                    # Encode captions\n                    enc_c = [word_map[\'<start>\']] + [word_map.get(word, word_map[\'<unk>\']) for word in c] + [\n                        word_map[\'<end>\']] + [word_map[\'<pad>\']] * (max_len - len(c))\n\n                    # Find caption lengths\n                    c_len = len(c) + 2\n\n                    enc_captions.append(enc_c)\n                    caplens.append(c_len)\n\n            # Sanity check\n            assert images.shape[0] * captions_per_image == len(enc_captions) == len(caplens)\n\n            # Save encoded captions and their lengths to JSON files\n            with open(os.path.join(output_folder, split + \'_CAPTIONS_\' + base_filename + \'.json\'), \'w\') as j:\n                json.dump(enc_captions, j)\n\n            with open(os.path.join(output_folder, split + \'_CAPLENS_\' + base_filename + \'.json\'), \'w\') as j:\n                json.dump(caplens, j)\n\n\ndef init_embedding(embeddings):\n    """"""\n    Fills embedding tensor with values from the uniform distribution.\n\n    :param embeddings: embedding tensor\n    """"""\n    bias = np.sqrt(3.0 / embeddings.size(1))\n    torch.nn.init.uniform_(embeddings, -bias, bias)\n\n\ndef load_embeddings(emb_file, word_map):\n    """"""\n    Creates an embedding tensor for the specified word map, for loading into the model.\n\n    :param emb_file: file containing embeddings (stored in GloVe format)\n    :param word_map: word map\n    :return: embeddings in the same order as the words in the word map, dimension of embeddings\n    """"""\n\n    # Find embedding dimension\n    with open(emb_file, \'r\') as f:\n        emb_dim = len(f.readline().split(\' \')) - 1\n\n    vocab = set(word_map.keys())\n\n    # Create tensor to hold embeddings, initialize\n    embeddings = torch.FloatTensor(len(vocab), emb_dim)\n    init_embedding(embeddings)\n\n    # Read embedding file\n    print(""\\nLoading embeddings..."")\n    for line in open(emb_file, \'r\'):\n        line = line.split(\' \')\n\n        emb_word = line[0]\n        embedding = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\n\n        # Ignore word if not in train_vocab\n        if emb_word not in vocab:\n            continue\n\n        embeddings[word_map[emb_word]] = torch.FloatTensor(embedding)\n\n    return embeddings, emb_dim\n\n\ndef clip_gradient(optimizer, grad_clip):\n    """"""\n    Clips gradients computed during backpropagation to avoid explosion of gradients.\n\n    :param optimizer: optimizer with the gradients to be clipped\n    :param grad_clip: clip value\n    """"""\n    for group in optimizer.param_groups:\n        for param in group[\'params\']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)\n\n\ndef save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n                    bleu4, is_best):\n    """"""\n    Saves model checkpoint.\n\n    :param data_name: base name of processed dataset\n    :param epoch: epoch number\n    :param epochs_since_improvement: number of epochs since last improvement in BLEU-4 score\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param encoder_optimizer: optimizer to update encoder\'s weights, if fine-tuning\n    :param decoder_optimizer: optimizer to update decoder\'s weights\n    :param bleu4: validation BLEU-4 score for this epoch\n    :param is_best: is this checkpoint the best so far?\n    """"""\n    state = {\'epoch\': epoch,\n             \'epochs_since_improvement\': epochs_since_improvement,\n             \'bleu-4\': bleu4,\n             \'encoder\': encoder,\n             \'decoder\': decoder,\n             \'encoder_optimizer\': encoder_optimizer,\n             \'decoder_optimizer\': decoder_optimizer}\n    filename = \'checkpoint_\' + data_name + \'.pth.tar\'\n    torch.save(state, filename)\n    # If this checkpoint is the best so far, store a copy so it doesn\'t get overwritten by a worse checkpoint\n    if is_best:\n        torch.save(state, \'BEST_\' + filename)\n\n\nclass AverageMeter(object):\n    """"""\n    Keeps track of most recent, average, sum, and count of a metric.\n    """"""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, shrink_factor):\n    """"""\n    Shrinks learning rate by a specified factor.\n\n    :param optimizer: optimizer whose learning rate must be shrunk.\n    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n    """"""\n\n    print(""\\nDECAYING learning rate."")\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = param_group[\'lr\'] * shrink_factor\n    print(""The new learning rate is %f\\n"" % (optimizer.param_groups[0][\'lr\'],))\n\n\ndef accuracy(scores, targets, k):\n    """"""\n    Computes top-k accuracy, from predicted and true labels.\n\n    :param scores: scores from the model\n    :param targets: true labels\n    :param k: k in top-k accuracy\n    :return: top-k accuracy\n    """"""\n\n    batch_size = targets.size(0)\n    _, ind = scores.topk(k, 1, True, True)\n    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n    correct_total = correct.view(-1).float().sum()  # 0D tensor\n    return correct_total.item() * (100.0 / batch_size)\n'"
