file_path,api_count,code
datasets.py,1,"b'from torch.utils.data import Dataset\nimport torch\n\n\nclass WCDataset(Dataset):\n    """"""\n    PyTorch Dataset for the LM-LSTM-CRF model. To be used by a PyTorch DataLoader to feed batches to the model.\n    """"""\n\n    def __init__(self, wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths):\n        """"""\n        :param wmaps: padded encoded word sequences\n        :param cmaps_f: padded encoded forward character sequences\n        :param cmaps_b: padded encoded backward character sequences\n        :param cmarkers_f: padded forward character markers\n        :param cmarkers_b: padded backward character markers\n        :param tmaps: padded encoded tag sequences (indices in unrolled CRF scores)\n        :param wmap_lengths: word sequence lengths\n        :param cmap_lengths: character sequence lengths\n        """"""\n        self.wmaps = wmaps\n        self.cmaps_f = cmaps_f\n        self.cmaps_b = cmaps_b\n        self.cmarkers_f = cmarkers_f\n        self.cmarkers_b = cmarkers_b\n        self.tmaps = tmaps\n        self.wmap_lengths = wmap_lengths\n        self.cmap_lengths = cmap_lengths\n\n        self.data_size = self.wmaps.size(0)\n\n    def __getitem__(self, i):\n        return self.wmaps[i], self.cmaps_f[i], self.cmaps_b[i], self.cmarkers_f[i], self.cmarkers_b[i], self.tmaps[i], \\\n               self.wmap_lengths[i], self.cmap_lengths[i]\n\n    def __len__(self):\n        return self.data_size\n'"
dynamic_rnn.py,3,"b'import torch\nfrom torch import nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n# Create a tensor with variable length sequences and pads (25)\nseqs = torch.LongTensor([[0, 1, 2, 3, 25, 25, 25],\n                         [4, 5, 25, 25, 25, 25, 25],\n                         [6, 7, 8, 9, 10, 11, 25]])\n\n# Store lengths of the actual sequences, ignoring padding\n# These are the points up to which we want the RNN to process the sequence\nseq_lens = torch.LongTensor([4, 2, 6])\n\n# Sort by decreasing lengths\nseq_lens, sort_ind = seq_lens.sort(dim=0, descending=True)\nseqs = seqs[sort_ind]\n\n# Create an embedding layer, with 0 vectors for the pads\nembeds = nn.Embedding(26, 10, padding_idx=25)\n\n# Create an LSTM layer\nlstm = nn.LSTM(10, 50, bidirectional=False, batch_first=True)\n\n# WITHOUT DYNAMIC BATCHING\n\nembeddings = embeds(seqs)\nout_static, _ = lstm(embeddings)\n\n# The number of timesteps in the output will be the same as the total padded timesteps in the input,\n# since the LSTM computed over the pads\nassert out_static.size(1) == embeddings.size(1)\n\n# Look at the output at a timestep that we know is a pad\nprint(out_static[1, -1])\n\n# WITH DYNAMIC BATCHING\n\n# Pack the sequence\npacked_seqs = pack_padded_sequence(embeddings, seq_lens.tolist(), batch_first=True)\n\n# To execute the LSTM over only the valid timesteps\nout_dynamic, _ = lstm(packed_seqs)\n\n# Use the inverse function to re-pad it\nout_dynamic, lens = pad_packed_sequence(out_dynamic, batch_first=True)\n\n# Note that since we re-padded it, the total padded timesteps will be the length of the longest sequence (6)\nassert out_dynamic.size(1) != embeddings.size(1)\nprint(out_dynamic.shape)\n\n# Look at the output at a timestep that we know is a pad\nprint(out_dynamic[1, -1])\n\n# It\'s all zeros!\n\n#########################################################\n\n# So, what does pack_padded_sequence do?\n# It removes pads, flattens by timestep, and keeps track of effective batch_size at each timestep\n\n# The RNN computes only on the effective batch size ""b_t"" at each timestep\n# This is why we sort - so the top ""b_t"" rows at timestep ""t"" are aligned with the top ""b_t"" outputs from timestep ""t-1""\n\n# Consider the original encoded sequences (sorted)\nprint(seqs)\n\n# Let\'s pack it\npacked_seqs = pack_padded_sequence(seqs, seq_lens, batch_first=True)\n\n# The result of pack_padded_sequence() is a tuple containing the flattened tensor and the effective batch size at each timestep\n# Here\'s the flattened tensor with pads removed\nprint(packed_seqs[0])\n# You can see it\'s flattened timestep-wise\n# Since pads are removed, the total datapoints are equal to the number of valid timsteps\nassert packed_seqs[0].size(0) == sum(seq_lens.tolist())\n\n# Here\'s the effective batch size at each timestep\nprint(packed_seqs[1])\n# If you look at the original encoded sequences, you can see this is true\nprint(seqs)\n\n'"
inference.py,11,"b'import torch\n\n\nclass ViterbiDecoder():\n    """"""\n    Viterbi Decoder.\n    """"""\n\n    def __init__(self, tag_map):\n        """"""\n        :param tag_map: tag map\n        """"""\n        self.tagset_size = len(tag_map)\n        self.start_tag = tag_map[\'<start>\']\n        self.end_tag = tag_map[\'<end>\']\n\n    def decode(self, scores, lengths):\n        """"""\n        :param scores: CRF scores\n        :param lengths: word sequence lengths\n        :return: decoded sequences\n        """"""\n        batch_size = scores.size(0)\n        word_pad_len = scores.size(1)\n\n        # Create a tensor to hold accumulated sequence scores at each current tag\n        scores_upto_t = torch.zeros(batch_size, self.tagset_size)\n\n        # Create a tensor to hold back-pointers\n        # i.e., indices of the previous_tag that corresponds to maximum accumulated score at current tag\n        # Let pads be the <end> tag index, since that was the last tag in the decoded sequence\n        backpointers = torch.ones((batch_size, max(lengths), self.tagset_size), dtype=torch.long) * self.end_tag\n\n        for t in range(max(lengths)):\n            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n            if t == 0:\n                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n                backpointers[:batch_size_t, t, :] = torch.ones((batch_size_t, self.tagset_size),\n                                                               dtype=torch.long) * self.start_tag\n            else:\n                # We add scores at current timestep to scores accumulated up to previous timestep, and\n                # choose the previous timestep that corresponds to the max. accumulated score for each current timestep\n                scores_upto_t[:batch_size_t], backpointers[:batch_size_t, t, :] = torch.max(\n                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n                    dim=1)  # (batch_size, tagset_size)\n\n        # Decode/trace best path backwards\n        decoded = torch.zeros((batch_size, backpointers.size(1)), dtype=torch.long)\n        pointer = torch.ones((batch_size, 1),\n                             dtype=torch.long) * self.end_tag  # the pointers at the ends are all <end> tags\n\n        for t in list(reversed(range(backpointers.size(1)))):\n            decoded[:, t] = torch.gather(backpointers[:, t, :], 1, pointer).squeeze(1)\n            pointer = decoded[:, t].unsqueeze(1)  # (batch_size, 1)\n\n        # Sanity check\n        assert torch.equal(decoded[:, 0], torch.ones((batch_size), dtype=torch.long) * self.start_tag)\n\n        # Remove the <starts> at the beginning, and append with <ends> (to compare to targets, if any)\n        decoded = torch.cat([decoded[:, 1:], torch.ones((batch_size, 1), dtype=torch.long) * self.start_tag],\n                            dim=1)\n\n        return decoded\n'"
models.py,10,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom utils import *\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n\nclass Highway(nn.Module):\n    """"""\n    Highway Network.\n    """"""\n\n    def __init__(self, size, num_layers=1, dropout=0.5):\n        """"""\n        :param size: size of linear layer (matches input size)\n        :param num_layers: number of transform and gate layers\n        :param dropout: dropout\n        """"""\n        super(Highway, self).__init__()\n        self.size = size\n        self.num_layers = num_layers\n        self.transform = nn.ModuleList()  # list of transform layers\n        self.gate = nn.ModuleList()  # list of gate layers\n        self.dropout = nn.Dropout(p=dropout)\n\n        for i in range(num_layers):\n            transform = nn.Linear(size, size)\n            gate = nn.Linear(size, size)\n            self.transform.append(transform)\n            self.gate.append(gate)\n\n    def forward(self, x):\n        """"""\n        Forward propagation.\n\n        :param x: input tensor\n        :return: output tensor, with same dimensions as input tensor\n        """"""\n        transformed = nn.functional.relu(self.transform[0](x))  # transform input\n        g = nn.functional.sigmoid(self.gate[0](x))  # calculate how much of the transformed input to keep\n\n        out = g * transformed + (1 - g) * x  # combine input and transformed input in this ratio\n\n        # If there are additional layers\n        for i in range(1, self.num_layers):\n            out = self.dropout(out)\n            transformed = nn.functional.relu(self.transform[i](out))\n            g = nn.functional.sigmoid(self.gate[i](out))\n\n            out = g * transformed + (1 - g) * out\n\n        return out\n\n\nclass CRF(nn.Module):\n    """"""\n    Conditional Random Field.\n    """"""\n\n    def __init__(self, hidden_dim, tagset_size):\n        """"""\n        :param hidden_dim: size of word RNN/BLSTM\'s output\n        :param tagset_size: number of tags\n        """"""\n        super(CRF, self).__init__()\n        self.tagset_size = tagset_size\n        self.emission = nn.Linear(hidden_dim, self.tagset_size)\n        self.transition = nn.Parameter(torch.Tensor(self.tagset_size, self.tagset_size))\n        self.transition.data.zero_()\n\n    def forward(self, feats):\n        """"""\n        Forward propagation.\n\n        :param feats: output of word RNN/BLSTM, a tensor of dimensions (batch_size, timesteps, hidden_dim)\n        :return: CRF scores, a tensor of dimensions (batch_size, timesteps, tagset_size, tagset_size)\n        """"""\n        self.batch_size = feats.size(0)\n        self.timesteps = feats.size(1)\n\n        emission_scores = self.emission(feats)  # (batch_size, timesteps, tagset_size)\n        emission_scores = emission_scores.unsqueeze(2).expand(self.batch_size, self.timesteps, self.tagset_size,\n                                                              self.tagset_size)  # (batch_size, timesteps, tagset_size, tagset_size)\n\n        crf_scores = emission_scores + self.transition.unsqueeze(0).unsqueeze(\n            0)  # (batch_size, timesteps, tagset_size, tagset_size)\n        return crf_scores\n\n\nclass LM_LSTM_CRF(nn.Module):\n    """"""\n    The encompassing LM-LSTM-CRF model.\n    """"""\n\n    def __init__(self, tagset_size, charset_size, char_emb_dim, char_rnn_dim, char_rnn_layers, vocab_size,\n                 lm_vocab_size, word_emb_dim, word_rnn_dim, word_rnn_layers, dropout, highway_layers=1):\n        """"""\n        :param tagset_size: number of tags\n        :param charset_size: size of character vocabulary\n        :param char_emb_dim: size of character embeddings\n        :param char_rnn_dim: size of character RNNs/LSTMs\n        :param char_rnn_layers: number of layers in character RNNs/LSTMs\n        :param vocab_size: input vocabulary size\n        :param lm_vocab_size: vocabulary size of language models (in-corpus words subject to word frequency threshold)\n        :param word_emb_dim: size of word embeddings\n        :param word_rnn_dim: size of word RNN/BLSTM\n        :param word_rnn_layers:  number of layers in word RNNs/LSTMs\n        :param dropout: dropout\n        :param highway_layers: number of transform and gate layers\n        """"""\n\n        super(LM_LSTM_CRF, self).__init__()\n\n        self.tagset_size = tagset_size  # this is the size of the output vocab of the tagging model\n\n        self.charset_size = charset_size\n        self.char_emb_dim = char_emb_dim\n        self.char_rnn_dim = char_rnn_dim\n        self.char_rnn_layers = char_rnn_layers\n\n        self.wordset_size = vocab_size  # this is the size of the input vocab (embedding layer) of the tagging model\n        self.lm_vocab_size = lm_vocab_size  # this is the size of the output vocab of the language model\n        self.word_emb_dim = word_emb_dim\n        self.word_rnn_dim = word_rnn_dim\n        self.word_rnn_layers = word_rnn_layers\n\n        self.highway_layers = highway_layers\n\n        self.dropout = nn.Dropout(p=dropout)\n\n        self.char_embeds = nn.Embedding(self.charset_size, self.char_emb_dim)  # character embedding layer\n        self.forw_char_lstm = nn.LSTM(self.char_emb_dim, self.char_rnn_dim, num_layers=self.char_rnn_layers,\n                                      bidirectional=False, dropout=dropout)  # forward character LSTM\n        self.back_char_lstm = nn.LSTM(self.char_emb_dim, self.char_rnn_dim, num_layers=self.char_rnn_layers,\n                                      bidirectional=False, dropout=dropout)  # backward character LSTM\n\n        self.word_embeds = nn.Embedding(self.wordset_size, self.word_emb_dim)  # word embedding layer\n        self.word_blstm = nn.LSTM(self.word_emb_dim + self.char_rnn_dim * 2, self.word_rnn_dim // 2,\n                                  num_layers=self.word_rnn_layers, bidirectional=True, dropout=dropout)  # word BLSTM\n\n        self.crf = CRF((self.word_rnn_dim // 2) * 2, self.tagset_size)  # conditional random field\n\n        self.forw_lm_hw = Highway(self.char_rnn_dim, num_layers=self.highway_layers,\n                                  dropout=dropout)  # highway to transform forward char LSTM output for the forward language model\n        self.back_lm_hw = Highway(self.char_rnn_dim, num_layers=self.highway_layers,\n                                  dropout=dropout)  # highway to transform backward char LSTM output for the backward language model\n        self.subword_hw = Highway(2 * self.char_rnn_dim, num_layers=self.highway_layers,\n                                  dropout=dropout)  # highway to transform combined forward and backward char LSTM outputs for use in the word BLSTM\n\n        self.forw_lm_out = nn.Linear(self.char_rnn_dim,\n                                     self.lm_vocab_size)  # linear layer to find vocabulary scores for the forward language model\n        self.back_lm_out = nn.Linear(self.char_rnn_dim,\n                                     self.lm_vocab_size)  # linear layer to find vocabulary scores for the backward language model\n\n    def init_word_embeddings(self, embeddings):\n        """"""\n        Initialize embeddings with pre-trained embeddings.\n\n        :param embeddings: pre-trained embeddings\n        """"""\n        self.word_embeds.weight = nn.Parameter(embeddings)\n\n    def fine_tune_word_embeddings(self, fine_tune=False):\n        """"""\n        Fine-tune embedding layer? (Not fine-tuning only makes sense if using pre-trained embeddings).\n\n        :param fine_tune: Fine-tune?\n        """"""\n        for p in self.word_embeds.parameters():\n            p.requires_grad = fine_tune\n\n    def forward(self, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, wmaps, tmaps, wmap_lengths, cmap_lengths):\n        """"""\n        Forward propagation.\n\n        :param cmaps_f: padded encoded forward character sequences, a tensor of dimensions (batch_size, char_pad_len)\n        :param cmaps_b: padded encoded backward character sequences, a tensor of dimensions (batch_size, char_pad_len)\n        :param cmarkers_f: padded forward character markers, a tensor of dimensions (batch_size, word_pad_len)\n        :param cmarkers_b: padded backward character markers, a tensor of dimensions (batch_size, word_pad_len)\n        :param wmaps: padded encoded word sequences, a tensor of dimensions (batch_size, word_pad_len)\n        :param tmaps: padded tag sequences, a tensor of dimensions (batch_size, word_pad_len)\n        :param wmap_lengths: word sequence lengths, a tensor of dimensions (batch_size)\n        :param cmap_lengths: character sequence lengths, a tensor of dimensions (batch_size, word_pad_len)\n        """"""\n        self.batch_size = cmaps_f.size(0)\n        self.word_pad_len = wmaps.size(1)\n\n        # Sort by decreasing true char. sequence length\n        cmap_lengths, char_sort_ind = cmap_lengths.sort(dim=0, descending=True)\n        cmaps_f = cmaps_f[char_sort_ind]\n        cmaps_b = cmaps_b[char_sort_ind]\n        cmarkers_f = cmarkers_f[char_sort_ind]\n        cmarkers_b = cmarkers_b[char_sort_ind]\n        wmaps = wmaps[char_sort_ind]\n        tmaps = tmaps[char_sort_ind]\n        wmap_lengths = wmap_lengths[char_sort_ind]\n\n        # Embedding look-up for characters\n        cf = self.char_embeds(cmaps_f)  # (batch_size, char_pad_len, char_emb_dim)\n        cb = self.char_embeds(cmaps_b)\n\n        # Dropout\n        cf = self.dropout(cf)  # (batch_size, char_pad_len, char_emb_dim)\n        cb = self.dropout(cb)\n\n        # Pack padded sequence\n        cf = pack_padded_sequence(cf, cmap_lengths.tolist(),\n                                  batch_first=True)  # packed sequence of char_emb_dim, with real sequence lengths\n        cb = pack_padded_sequence(cb, cmap_lengths.tolist(), batch_first=True)\n\n        # LSTM\n        cf, _ = self.forw_char_lstm(cf)  # packed sequence of char_rnn_dim, with real sequence lengths\n        cb, _ = self.back_char_lstm(cb)\n\n        # Unpack packed sequence\n        cf, _ = pad_packed_sequence(cf, batch_first=True)  # (batch_size, max_char_len_in_batch, char_rnn_dim)\n        cb, _ = pad_packed_sequence(cb, batch_first=True)\n\n        # Sanity check\n        assert cf.size(1) == max(cmap_lengths.tolist()) == list(cmap_lengths)[0]\n\n        # Select RNN outputs only at marker points (spaces in the character sequence)\n        cmarkers_f = cmarkers_f.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n        cmarkers_b = cmarkers_b.unsqueeze(2).expand(self.batch_size, self.word_pad_len, self.char_rnn_dim)\n        cf_selected = torch.gather(cf, 1, cmarkers_f)  # (batch_size, word_pad_len, char_rnn_dim)\n        cb_selected = torch.gather(cb, 1, cmarkers_b)\n\n        # Only for co-training, not useful for tagging after model is trained\n        if self.training:\n            lm_f = self.forw_lm_hw(self.dropout(cf_selected))  # (batch_size, word_pad_len, char_rnn_dim)\n            lm_b = self.back_lm_hw(self.dropout(cb_selected))\n            lm_f_scores = self.forw_lm_out(self.dropout(lm_f))  # (batch_size, word_pad_len, lm_vocab_size)\n            lm_b_scores = self.back_lm_out(self.dropout(lm_b))\n\n        # Sort by decreasing true word sequence length\n        wmap_lengths, word_sort_ind = wmap_lengths.sort(dim=0, descending=True)\n        wmaps = wmaps[word_sort_ind]\n        tmaps = tmaps[word_sort_ind]\n        cf_selected = cf_selected[word_sort_ind]  # for language model\n        cb_selected = cb_selected[word_sort_ind]\n        if self.training:\n            lm_f_scores = lm_f_scores[word_sort_ind]\n            lm_b_scores = lm_b_scores[word_sort_ind]\n\n        # Embedding look-up for words\n        w = self.word_embeds(wmaps)  # (batch_size, word_pad_len, word_emb_dim)\n        w = self.dropout(w)\n\n        # Sub-word information at each word\n        subword = self.subword_hw(self.dropout(\n            torch.cat((cf_selected, cb_selected), dim=2)))  # (batch_size, word_pad_len, 2 * char_rnn_dim)\n        subword = self.dropout(subword)\n\n        # Concatenate word embeddings and sub-word features\n        w = torch.cat((w, subword), dim=2)  # (batch_size, word_pad_len, word_emb_dim + 2 * char_rnn_dim)\n\n        # Pack padded sequence\n        w = pack_padded_sequence(w, list(wmap_lengths),\n                                 batch_first=True)  # packed sequence of word_emb_dim + 2 * char_rnn_dim, with real sequence lengths\n\n        # LSTM\n        w, _ = self.word_blstm(w)  # packed sequence of word_rnn_dim, with real sequence lengths\n\n        # Unpack packed sequence\n        w, _ = pad_packed_sequence(w, batch_first=True)  # (batch_size, max_word_len_in_batch, word_rnn_dim)\n        w = self.dropout(w)\n\n        crf_scores = self.crf(w)  # (batch_size, max_word_len_in_batch, tagset_size, tagset_size)\n\n        if self.training:\n            return crf_scores, lm_f_scores, lm_b_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind\n        else:\n            return crf_scores, wmaps, tmaps, wmap_lengths, word_sort_ind, char_sort_ind  # sort inds to reorder, if req.\n\n\nclass ViterbiLoss(nn.Module):\n    """"""\n    Viterbi Loss.\n    """"""\n\n    def __init__(self, tag_map):\n        """"""\n        :param tag_map: tag map\n        """"""\n        super(ViterbiLoss, self).__init__()\n        self.tagset_size = len(tag_map)\n        self.start_tag = tag_map[\'<start>\']\n        self.end_tag = tag_map[\'<end>\']\n\n    def forward(self, scores, targets, lengths):\n        """"""\n        Forward propagation.\n\n        :param scores: CRF scores\n        :param targets: true tags indices in unrolled CRF scores\n        :param lengths: word sequence lengths\n        :return: viterbi loss\n        """"""\n\n        batch_size = scores.size(0)\n        word_pad_len = scores.size(1)\n\n        # Gold score\n\n        targets = targets.unsqueeze(2)\n        scores_at_targets = torch.gather(scores.view(batch_size, word_pad_len, -1), 2, targets).squeeze(\n            2)  # (batch_size, word_pad_len)\n\n        # Everything is already sorted by lengths\n        scores_at_targets, _ = pack_padded_sequence(scores_at_targets, lengths, batch_first=True)\n        gold_score = scores_at_targets.sum()\n\n        # All paths\' scores\n\n        # Create a tensor to hold accumulated sequence scores at each current tag\n        scores_upto_t = torch.zeros(batch_size, self.tagset_size).to(device)\n\n        for t in range(max(lengths)):\n            batch_size_t = sum([l > t for l in lengths])  # effective batch size (sans pads) at this timestep\n            if t == 0:\n                scores_upto_t[:batch_size_t] = scores[:batch_size_t, t, self.start_tag, :]  # (batch_size, tagset_size)\n            else:\n                # We add scores at current timestep to scores accumulated up to previous timestep, and log-sum-exp\n                # Remember, the cur_tag of the previous timestep is the prev_tag of this timestep\n                # So, broadcast prev. timestep\'s cur_tag scores along cur. timestep\'s cur_tag dimension\n                scores_upto_t[:batch_size_t] = log_sum_exp(\n                    scores[:batch_size_t, t, :, :] + scores_upto_t[:batch_size_t].unsqueeze(2),\n                    dim=1)  # (batch_size, tagset_size)\n\n        # We only need the final accumulated scores at the <end> tag\n        all_paths_scores = scores_upto_t[:, self.end_tag].sum()\n\n        viterbi_loss = all_paths_scores - gold_score\n        viterbi_loss = viterbi_loss / batch_size\n\n        return viterbi_loss\n'"
train.py,8,"b'import time\nimport torch\nimport torch.optim as optim\nimport os\nimport sys\nfrom models import LM_LSTM_CRF, ViterbiLoss\nfrom utils import *\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom datasets import WCDataset\nfrom inference import ViterbiDecoder\nfrom sklearn.metrics import f1_score\n\n# Data parameters\ntask = \'ner\'  # tagging task, to choose column in CoNLL 2003 dataset\ntrain_file = \'./datasets/eng.train\'  # path to training data\nval_file = \'./datasets/eng.testa\'  # path to validation data\ntest_file = \'./datasets/eng.testb\'  # path to test data\nemb_file = \'./embeddings/glove.6B.100d.txt\'  # path to pre-trained word embeddings\nmin_word_freq = 5  # threshold for word frequency\nmin_char_freq = 1  # threshold for character frequency\ncaseless = True  # lowercase everything?\nexpand_vocab = True  # expand model\'s input vocabulary to the pre-trained embeddings\' vocabulary?\n\n# Model parameters\nchar_emb_dim = 30  # character embedding size\nwith open(emb_file, \'r\') as f:\n    word_emb_dim = len(f.readline().split(\' \')) - 1  # word embedding size\nword_rnn_dim = 300  # word RNN size\nchar_rnn_dim = 300  # character RNN size\nchar_rnn_layers = 1  # number of layers in character RNN\nword_rnn_layers = 1  # number of layers in word RNN\nhighway_layers = 1  # number of layers in highway network\ndropout = 0.5  # dropout\nfine_tune_word_embeddings = False  # fine-tune pre-trained word embeddings?\n\n# Training parameters\nstart_epoch = 0  # start at this epoch\nbatch_size = 10  # batch size\nlr = 0.015  # learning rate\nlr_decay = 0.05  # decay learning rate by this amount\nmomentum = 0.9  # momentum\nworkers = 1  # number of workers for loading data in the DataLoader\nepochs = 200  # number of epochs to run without early-stopping\ngrad_clip = 5.  # clip gradients at this value\nprint_freq = 100  # print training or validation status every __ batches\nbest_f1 = 0.  # F1 score to start with\ncheckpoint = None  # path to model checkpoint, None if none\n\ntag_ind = 1 if task == \'pos\' else 3  # choose column in CoNLL 2003 dataset\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n\ndef main():\n    """"""\n    Training and validation.\n    """"""\n    global best_f1, epochs_since_improvement, checkpoint, start_epoch, word_map, char_map, tag_map\n\n    # Read training and validation data\n    train_words, train_tags = read_words_tags(train_file, tag_ind, caseless)\n    val_words, val_tags = read_words_tags(val_file, tag_ind, caseless)\n\n    # Initialize model or load checkpoint\n    if checkpoint is not None:\n        checkpoint = torch.load(checkpoint)\n        model = checkpoint[\'model\']\n        optimizer = checkpoint[\'optimizer\']\n        word_map = checkpoint[\'word_map\']\n        lm_vocab_size = checkpoint[\'lm_vocab_size\']\n        tag_map = checkpoint[\'tag_map\']\n        char_map = checkpoint[\'char_map\']\n        start_epoch = checkpoint[\'epoch\'] + 1\n        best_f1 = checkpoint[\'f1\']\n    else:\n        word_map, char_map, tag_map = create_maps(train_words + val_words, train_tags + val_tags, min_word_freq,\n                                                  min_char_freq)  # create word, char, tag maps\n        embeddings, word_map, lm_vocab_size = load_embeddings(emb_file, word_map,\n                                                              expand_vocab)  # load pre-trained embeddings\n\n        model = LM_LSTM_CRF(tagset_size=len(tag_map),\n                            charset_size=len(char_map),\n                            char_emb_dim=char_emb_dim,\n                            char_rnn_dim=char_rnn_dim,\n                            char_rnn_layers=char_rnn_layers,\n                            vocab_size=len(word_map),\n                            lm_vocab_size=lm_vocab_size,\n                            word_emb_dim=word_emb_dim,\n                            word_rnn_dim=word_rnn_dim,\n                            word_rnn_layers=word_rnn_layers,\n                            dropout=dropout,\n                            highway_layers=highway_layers).to(device)\n        model.init_word_embeddings(embeddings.to(device))  # initialize embedding layer with pre-trained embeddings\n        model.fine_tune_word_embeddings(fine_tune_word_embeddings)  # fine-tune\n        optimizer = optim.SGD(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=momentum)\n\n    # Loss functions\n    lm_criterion = nn.CrossEntropyLoss().to(device)\n    crf_criterion = ViterbiLoss(tag_map).to(device)\n\n    # Since the language model\'s vocab is restricted to in-corpus indices, encode training/val with only these!\n    # word_map might have been expanded, and in-corpus words eliminated due to low frequency might still be added because\n    # they were in the pre-trained embeddings\n    temp_word_map = {k: v for k, v in word_map.items() if v <= word_map[\'<unk>\']}\n    train_inputs = create_input_tensors(train_words, train_tags, temp_word_map, char_map,\n                                        tag_map)\n    val_inputs = create_input_tensors(val_words, val_tags, temp_word_map, char_map, tag_map)\n\n    # DataLoaders\n    train_loader = torch.utils.data.DataLoader(WCDataset(*train_inputs), batch_size=batch_size, shuffle=True,\n                                               num_workers=workers, pin_memory=False)\n    val_loader = torch.utils.data.DataLoader(WCDataset(*val_inputs), batch_size=batch_size, shuffle=True,\n                                             num_workers=workers, pin_memory=False)\n\n    # Viterbi decoder (to find accuracy during validation)\n    vb_decoder = ViterbiDecoder(tag_map)\n\n    # Epochs\n    for epoch in range(start_epoch, epochs):\n\n        # One epoch\'s training\n        train(train_loader=train_loader,\n              model=model,\n              lm_criterion=lm_criterion,\n              crf_criterion=crf_criterion,\n              optimizer=optimizer,\n              epoch=epoch,\n              vb_decoder=vb_decoder)\n\n        # One epoch\'s validation\n        val_f1 = validate(val_loader=val_loader,\n                          model=model,\n                          crf_criterion=crf_criterion,\n                          vb_decoder=vb_decoder)\n\n        # Did validation F1 score improve?\n        is_best = val_f1 > best_f1\n        best_f1 = max(val_f1, best_f1)\n        if not is_best:\n            epochs_since_improvement += 1\n            print(""\\nEpochs since improvement: %d\\n"" % (epochs_since_improvement,))\n        else:\n            epochs_since_improvement = 0\n\n        # Save checkpoint\n        save_checkpoint(epoch, model, optimizer, val_f1, word_map, char_map, tag_map, lm_vocab_size, is_best)\n\n        # Decay learning rate every epoch\n        adjust_learning_rate(optimizer, lr / (1 + (epoch + 1) * lr_decay))\n\n\ndef train(train_loader, model, lm_criterion, crf_criterion, optimizer, epoch, vb_decoder):\n    """"""\n    Performs one epoch\'s training.\n\n    :param train_loader: DataLoader for training data\n    :param model: model\n    :param lm_criterion: cross entropy loss layer\n    :param crf_criterion: viterbi loss layer\n    :param optimizer: optimizer\n    :param epoch: epoch number\n    :param vb_decoder: viterbi decoder (to decode and find F1 score)\n    """"""\n\n    model.train()  # training mode enables dropout\n\n    batch_time = AverageMeter()  # forward prop. + back prop. time per batch\n    data_time = AverageMeter()  # data loading time per batch\n    ce_losses = AverageMeter()  # cross entropy loss\n    vb_losses = AverageMeter()  # viterbi loss\n    f1s = AverageMeter()  # f1 score\n\n    start = time.time()\n\n    # Batches\n    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(\n            train_loader):\n\n        data_time.update(time.time() - start)\n\n        max_word_len = max(wmap_lengths.tolist())\n        max_char_len = max(cmap_lengths.tolist())\n\n        # Reduce batch\'s padded length to maximum in-batch sequence\n        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don\'t compute over the pads)\n        wmaps = wmaps[:, :max_word_len].to(device)\n        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n        tmaps = tmaps[:, :max_word_len].to(device)\n        wmap_lengths = wmap_lengths.to(device)\n        cmap_lengths = cmap_lengths.to(device)\n\n        # Forward prop.\n        crf_scores, lm_f_scores, lm_b_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n                                                                                                             cmaps_b,\n                                                                                                             cmarkers_f,\n                                                                                                             cmarkers_b,\n                                                                                                             wmaps,\n                                                                                                             tmaps,\n                                                                                                             wmap_lengths,\n                                                                                                             cmap_lengths)\n\n        # LM loss\n\n        # We don\'t predict the next word at the pads or <end> tokens\n        # We will only predict at [dunston, checks, in] among [dunston, checks, in, <end>, <pad>, <pad>, ...]\n        # So, prediction lengths are word sequence lengths - 1\n        lm_lengths = wmap_lengths_sorted - 1\n        lm_lengths = lm_lengths.tolist()\n\n        # Remove scores at timesteps we won\'t predict at\n        # pack_padded_sequence is a good trick to do this (see dynamic_rnn.py, where we explore this)\n        lm_f_scores, _ = pack_padded_sequence(lm_f_scores, lm_lengths, batch_first=True)\n        lm_b_scores, _ = pack_padded_sequence(lm_b_scores, lm_lengths, batch_first=True)\n\n        # For the forward sequence, targets are from the second word onwards, up to <end>\n        # (timestep -> target) ...dunston -> checks, ...checks -> in, ...in -> <end>\n        lm_f_targets = wmaps_sorted[:, 1:]\n        lm_f_targets, _ = pack_padded_sequence(lm_f_targets, lm_lengths, batch_first=True)\n\n        # For the backward sequence, targets are <end> followed by all words except the last word\n        # ...notsnud -> <end>, ...skcehc -> dunston, ...ni -> checks\n        lm_b_targets = torch.cat(\n            [torch.LongTensor([word_map[\'<end>\']] * wmaps_sorted.size(0)).unsqueeze(1).to(device), wmaps_sorted], dim=1)\n        lm_b_targets, _ = pack_padded_sequence(lm_b_targets, lm_lengths, batch_first=True)\n\n        # Calculate loss\n        ce_loss = lm_criterion(lm_f_scores, lm_f_targets) + lm_criterion(lm_b_scores, lm_b_targets)\n        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n        loss = ce_loss + vb_loss\n\n        # Back prop.\n        optimizer.zero_grad()\n        loss.backward()\n\n        if grad_clip is not None:\n            clip_gradient(optimizer, grad_clip)\n\n        optimizer.step()\n\n        # Viterbi decode to find accuracy / f1\n        decoded = vb_decoder.decode(crf_scores.to(""cpu""), wmap_lengths_sorted.to(""cpu""))\n\n        # Remove timesteps we won\'t predict at, and also <end> tags, because to predict them would be cheating\n        decoded, _ = pack_padded_sequence(decoded, lm_lengths, batch_first=True)\n        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n        tmaps_sorted, _ = pack_padded_sequence(tmaps_sorted, lm_lengths, batch_first=True)\n\n        # F1\n        f1 = f1_score(tmaps_sorted.to(""cpu"").numpy(), decoded.numpy(), average=\'macro\')\n\n        # Keep track of metrics\n        ce_losses.update(ce_loss.item(), sum(lm_lengths))\n        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n        batch_time.update(time.time() - start)\n        f1s.update(f1, sum(lm_lengths))\n\n        start = time.time()\n\n        # Print training status\n        if i % print_freq == 0:\n            print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                  \'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                  \'CE Loss {ce_loss.val:.4f} ({ce_loss.avg:.4f})\\t\'\n                  \'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t\'\n                  \'F1 {f1.val:.3f} ({f1.avg:.3f})\'.format(epoch, i, len(train_loader),\n                                                          batch_time=batch_time,\n                                                          data_time=data_time, ce_loss=ce_losses,\n                                                          vb_loss=vb_losses, f1=f1s))\n\n\ndef validate(val_loader, model, crf_criterion, vb_decoder):\n    """"""\n    Performs one epoch\'s validation.\n\n    :param val_loader: DataLoader for validation data\n    :param model: model\n    :param crf_criterion: viterbi loss layer\n    :param vb_decoder: viterbi decoder\n    :return: validation F1 score\n    """"""\n    model.eval()\n\n    batch_time = AverageMeter()\n    vb_losses = AverageMeter()\n    f1s = AverageMeter()\n\n    start = time.time()\n\n    for i, (wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps, wmap_lengths, cmap_lengths) in enumerate(\n            val_loader):\n\n        max_word_len = max(wmap_lengths.tolist())\n        max_char_len = max(cmap_lengths.tolist())\n\n        # Reduce batch\'s padded length to maximum in-batch sequence\n        # This saves some compute on nn.Linear layers (RNNs are unaffected, since they don\'t compute over the pads)\n        wmaps = wmaps[:, :max_word_len].to(device)\n        cmaps_f = cmaps_f[:, :max_char_len].to(device)\n        cmaps_b = cmaps_b[:, :max_char_len].to(device)\n        cmarkers_f = cmarkers_f[:, :max_word_len].to(device)\n        cmarkers_b = cmarkers_b[:, :max_word_len].to(device)\n        tmaps = tmaps[:, :max_word_len].to(device)\n        wmap_lengths = wmap_lengths.to(device)\n        cmap_lengths = cmap_lengths.to(device)\n\n        # Forward prop.\n        crf_scores, wmaps_sorted, tmaps_sorted, wmap_lengths_sorted, _, __ = model(cmaps_f,\n                                                                                   cmaps_b,\n                                                                                   cmarkers_f,\n                                                                                   cmarkers_b,\n                                                                                   wmaps,\n                                                                                   tmaps,\n                                                                                   wmap_lengths,\n                                                                                   cmap_lengths)\n\n        # Viterbi / CRF layer loss\n        vb_loss = crf_criterion(crf_scores, tmaps_sorted, wmap_lengths_sorted)\n\n        # Viterbi decode to find accuracy / f1\n        decoded = vb_decoder.decode(crf_scores.to(""cpu""), wmap_lengths_sorted.to(""cpu""))\n\n        # Remove timesteps we won\'t predict at, and also <end> tags, because to predict them would be cheating\n        decoded, _ = pack_padded_sequence(decoded, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n        tmaps_sorted = tmaps_sorted % vb_decoder.tagset_size  # actual target indices (see create_input_tensors())\n        tmaps_sorted, _ = pack_padded_sequence(tmaps_sorted, (wmap_lengths_sorted - 1).tolist(), batch_first=True)\n\n        # f1\n        f1 = f1_score(tmaps_sorted.to(""cpu"").numpy(), decoded.numpy(), average=\'macro\')\n\n        # Keep track of metrics\n        vb_losses.update(vb_loss.item(), crf_scores.size(0))\n        f1s.update(f1, sum((wmap_lengths_sorted - 1).tolist()))\n        batch_time.update(time.time() - start)\n\n        start = time.time()\n\n        if i % print_freq == 0:\n            print(\'Validation: [{0}/{1}]\\t\'\n                  \'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'VB Loss {vb_loss.val:.4f} ({vb_loss.avg:.4f})\\t\'\n                  \'F1 Score {f1.val:.3f} ({f1.avg:.3f})\\t\'.format(i, len(val_loader), batch_time=batch_time,\n                                                                  vb_loss=vb_losses, f1=f1s))\n\n    print(\n        \'\\n * LOSS - {vb_loss.avg:.3f}, F1 SCORE - {f1.avg:.3f}\\n\'.format(vb_loss=vb_losses,\n                                                                          f1=f1s))\n\n    return f1s.avg\n\n\nif __name__ == \'__main__\':\n    main()\n'"
utils.py,20,"b'from collections import Counter\nimport codecs\nimport itertools\nfrom functools import reduce\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\n\ndef read_words_tags(file, tag_ind, caseless=True):\n    """"""\n    Reads raw data in the CoNLL 2003 format and returns word and tag sequences.\n\n    :param file: file with raw data in the CoNLL 2003 format\n    :param tag_ind: column index of tag\n    :param caseless: lowercase words?\n    :return: word, tag sequences\n    """"""\n    with codecs.open(file, \'r\', \'utf-8\') as f:\n        lines = f.readlines()\n    words = []\n    tags = []\n    temp_w = []\n    temp_t = []\n    for line in lines:\n        if not (line.isspace() or (len(line) > 10 and line[0:10] == \'-DOCSTART-\')):\n            feats = line.rstrip(\'\\n\').split()\n            temp_w.append(feats[0].lower() if caseless else feats[0])\n            temp_t.append(feats[tag_ind])\n        elif len(temp_w) > 0:\n            assert len(temp_w) == len(temp_t)\n            words.append(temp_w)\n            tags.append(temp_t)\n            temp_w = []\n            temp_t = []\n    # last sentence\n    if len(temp_w) > 0:\n        assert len(temp_w) == len(temp_t)\n        words.append(temp_w)\n        tags.append(temp_t)\n\n    # Sanity check\n    assert len(words) == len(tags)\n\n    return words, tags\n\n\ndef create_maps(words, tags, min_word_freq=5, min_char_freq=1):\n    """"""\n    Creates word, char, tag maps.\n\n    :param words: word sequences\n    :param tags: tag sequences\n    :param min_word_freq: words that occur fewer times than this threshold are binned as <unk>s\n    :param min_char_freq: characters that occur fewer times than this threshold are binned as <unk>s\n    :return: word, char, tag maps\n    """"""\n    word_freq = Counter()\n    char_freq = Counter()\n    tag_map = set()\n    for w, t in zip(words, tags):\n        word_freq.update(w)\n        char_freq.update(list(reduce(lambda x, y: list(x) + [\' \'] + list(y), w)))\n        tag_map.update(t)\n\n    word_map = {k: v + 1 for v, k in enumerate([w for w in word_freq.keys() if word_freq[w] > min_word_freq])}\n    char_map = {k: v + 1 for v, k in enumerate([c for c in char_freq.keys() if char_freq[c] > min_char_freq])}\n    tag_map = {k: v + 1 for v, k in enumerate(tag_map)}\n\n    word_map[\'<pad>\'] = 0\n    word_map[\'<end>\'] = len(word_map)\n    word_map[\'<unk>\'] = len(word_map)\n    char_map[\'<pad>\'] = 0\n    char_map[\'<end>\'] = len(char_map)\n    char_map[\'<unk>\'] = len(char_map)\n    tag_map[\'<pad>\'] = 0\n    tag_map[\'<start>\'] = len(tag_map)\n    tag_map[\'<end>\'] = len(tag_map)\n\n    return word_map, char_map, tag_map\n\n\ndef create_input_tensors(words, tags, word_map, char_map, tag_map):\n    """"""\n    Creates input tensors that will be used to create a PyTorch Dataset.\n\n    :param words: word sequences\n    :param tags: tag sequences\n    :param word_map: word map\n    :param char_map: character map\n    :param tag_map: tag map\n    :return: padded encoded words, padded encoded forward chars, padded encoded backward chars,\n            padded forward character markers, padded backward character markers, padded encoded tags,\n            word sequence lengths, char sequence lengths\n    """"""\n    # Encode sentences into word maps with <end> at the end\n    # [[\'dunston\', \'checks\', \'in\', \'<end>\']] -> [[4670, 4670, 185, 4669]]\n    wmaps = list(map(lambda s: list(map(lambda w: word_map.get(w, word_map[\'<unk>\']), s)) + [word_map[\'<end>\']], words))\n\n    # Forward and backward character streams\n    # [[\'d\', \'u\', \'n\', \'s\', \'t\', \'o\', \'n\', \' \', \'c\', \'h\', \'e\', \'c\', \'k\', \'s\', \' \', \'i\', \'n\', \' \']]\n    chars_f = list(map(lambda s: list(reduce(lambda x, y: list(x) + [\' \'] + list(y), s)) + [\' \'], words))\n    # [[\'n\', \'i\', \' \', \'s\', \'k\', \'c\', \'e\', \'h\', \'c\', \' \', \'n\', \'o\', \'t\', \'s\', \'n\', \'u\', \'d\', \' \']]\n    chars_b = list(\n        map(lambda s: list(reversed([\' \'] + list(reduce(lambda x, y: list(x) + [\' \'] + list(y), s)))), words))\n\n    # Encode streams into forward and backward character maps with <end> at the end\n    # [[29, 2, 12, 8, 7, 14, 12, 3, 6, 18, 1, 6, 21, 8, 3, 17, 12, 3, 60]]\n    cmaps_f = list(\n        map(lambda s: list(map(lambda c: char_map.get(c, char_map[\'<unk>\']), s)) + [char_map[\'<end>\']], chars_f))\n    # [[12, 17, 3, 8, 21, 6, 1, 18, 6, 3, 12, 14, 7, 8, 12, 2, 29, 3, 60]]\n    cmaps_b = list(\n        map(lambda s: list(map(lambda c: char_map.get(c, char_map[\'<unk>\']), s)) + [char_map[\'<end>\']], chars_b))\n\n    # Positions of spaces and <end> character\n    # Words are predicted or encoded at these places in the language and tagging models respectively\n    # [[7, 14, 17, 18]] are points after \'...dunston\', \'...checks\', \'...in\', \'...<end>\' respectively\n    cmarkers_f = list(map(lambda s: [ind for ind in range(len(s)) if s[ind] == char_map[\' \']] + [len(s) - 1], cmaps_f))\n    # Reverse the markers for the backward stream before adding <end>, so the words of the f and b markers coincide\n    # i.e., [[17, 9, 2, 18]] are points after \'...notsnud\', \'...skcehc\', \'...ni\', \'...<end>\' respectively\n    cmarkers_b = list(\n        map(lambda s: list(reversed([ind for ind in range(len(s)) if s[ind] == char_map[\' \']])) + [len(s) - 1],\n            cmaps_b))\n\n    # Encode tags into tag maps with <end> at the end\n    tmaps = list(map(lambda s: list(map(lambda t: tag_map[t], s)) + [tag_map[\'<end>\']], tags))\n    # Since we\'re using CRF scores of size (prev_tags, cur_tags), find indices of target sequence in the unrolled scores\n    # This will be row_index (i.e. prev_tag) * n_columns (i.e. tagset_size) + column_index (i.e. cur_tag)\n    tmaps = list(map(lambda s: [tag_map[\'<start>\'] * len(tag_map) + s[0]] + [s[i - 1] * len(tag_map) + s[i] for i in\n                                                                             range(1, len(s))], tmaps))\n    # Note - the actual tag indices can be recovered with tmaps % len(tag_map)\n\n    # Pad, because need fixed length to be passed around by DataLoaders and other layers\n    word_pad_len = max(list(map(lambda s: len(s), wmaps)))\n    char_pad_len = max(list(map(lambda s: len(s), cmaps_f)))\n\n    # Sanity check\n    assert word_pad_len == max(list(map(lambda s: len(s), tmaps)))\n\n    padded_wmaps = []\n    padded_cmaps_f = []\n    padded_cmaps_b = []\n    padded_cmarkers_f = []\n    padded_cmarkers_b = []\n    padded_tmaps = []\n    wmap_lengths = []\n    cmap_lengths = []\n\n    for w, cf, cb, cmf, cmb, t in zip(wmaps, cmaps_f, cmaps_b, cmarkers_f, cmarkers_b, tmaps):\n        # Sanity  checks\n        assert len(w) == len(cmf) == len(cmb) == len(t)\n        assert len(cmaps_f) == len(cmaps_b)\n\n        # Pad\n        # A note -  it doesn\'t really matter what we pad with, as long as it\'s a valid index\n        # i.e., we\'ll extract output at those pad points (to extract equal lengths), but never use them\n\n        padded_wmaps.append(w + [word_map[\'<pad>\']] * (word_pad_len - len(w)))\n        padded_cmaps_f.append(cf + [char_map[\'<pad>\']] * (char_pad_len - len(cf)))\n        padded_cmaps_b.append(cb + [char_map[\'<pad>\']] * (char_pad_len - len(cb)))\n\n        # 0 is always a valid index to pad markers with (-1 is too but torch.gather has some issues with it)\n        padded_cmarkers_f.append(cmf + [0] * (word_pad_len - len(w)))\n        padded_cmarkers_b.append(cmb + [0] * (word_pad_len - len(w)))\n\n        padded_tmaps.append(t + [tag_map[\'<pad>\']] * (word_pad_len - len(t)))\n\n        wmap_lengths.append(len(w))\n        cmap_lengths.append(len(cf))\n\n        # Sanity check\n        assert len(padded_wmaps[-1]) == len(padded_tmaps[-1]) == len(padded_cmarkers_f[-1]) == len(\n            padded_cmarkers_b[-1]) == word_pad_len\n        assert len(padded_cmaps_f[-1]) == len(padded_cmaps_b[-1]) == char_pad_len\n\n    padded_wmaps = torch.LongTensor(padded_wmaps)\n    padded_cmaps_f = torch.LongTensor(padded_cmaps_f)\n    padded_cmaps_b = torch.LongTensor(padded_cmaps_b)\n    padded_cmarkers_f = torch.LongTensor(padded_cmarkers_f)\n    padded_cmarkers_b = torch.LongTensor(padded_cmarkers_b)\n    padded_tmaps = torch.LongTensor(padded_tmaps)\n    wmap_lengths = torch.LongTensor(wmap_lengths)\n    cmap_lengths = torch.LongTensor(cmap_lengths)\n\n    return padded_wmaps, padded_cmaps_f, padded_cmaps_b, padded_cmarkers_f, padded_cmarkers_b, padded_tmaps, \\\n           wmap_lengths, cmap_lengths\n\n\ndef init_embedding(input_embedding):\n    """"""\n    Initialize embedding tensor with values from the uniform distribution.\n\n    :param input_embedding: embedding tensor\n    :return:\n    """"""\n    bias = np.sqrt(3.0 / input_embedding.size(1))\n    nn.init.uniform_(input_embedding, -bias, bias)\n\n\ndef load_embeddings(emb_file, word_map, expand_vocab=True):\n    """"""\n    Load pre-trained embeddings for words in the word map.\n\n    :param emb_file: file with pre-trained embeddings (in the GloVe format)\n    :param word_map: word map\n    :param expand_vocab: expand vocabulary of word map to vocabulary of pre-trained embeddings?\n    :return: embeddings for words in word map, (possibly expanded) word map,\n            number of words in word map that are in-corpus (subject to word frequency threshold)\n    """"""\n    with open(emb_file, \'r\') as f:\n        emb_len = len(f.readline().split(\' \')) - 1\n\n    print(""Embedding length is %d."" % emb_len)\n\n    # Create tensor to hold embeddings for words that are in-corpus\n    ic_embs = torch.FloatTensor(len(word_map), emb_len)\n    init_embedding(ic_embs)\n\n    if expand_vocab:\n        print(""You have elected to include embeddings that are out-of-corpus."")\n        ooc_words = []\n        ooc_embs = []\n    else:\n        print(""You have elected NOT to include embeddings that are out-of-corpus."")\n\n    # Read embedding file\n    print(""\\nLoading embeddings..."")\n    for line in open(emb_file, \'r\'):\n        line = line.split(\' \')\n\n        emb_word = line[0]\n        embedding = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\n\n        if not expand_vocab and emb_word not in word_map:\n            continue\n\n        # If word is in train_vocab, store at the correct index (as in the word_map)\n        if emb_word in word_map:\n            ic_embs[word_map[emb_word]] = torch.FloatTensor(embedding)\n\n        # If word is in dev or test vocab, store it and its embedding into lists\n        elif expand_vocab:\n            ooc_words.append(emb_word)\n            ooc_embs.append(embedding)\n\n    lm_vocab_size = len(word_map)  # keep track of lang. model\'s output vocab size (no out-of-corpus words)\n\n    if expand_vocab:\n        print(""\'word_map\' is being updated accordingly."")\n        for word in ooc_words:\n            word_map[word] = len(word_map)\n        ooc_embs = torch.FloatTensor(np.asarray(ooc_embs))\n        embeddings = torch.cat([ic_embs, ooc_embs], 0)\n\n    else:\n        embeddings = ic_embs\n\n    # Sanity check\n    assert embeddings.size(0) == len(word_map)\n\n    print(""\\nDone.\\n Embedding vocabulary: %d\\n Language Model vocabulary: %d.\\n"" % (len(word_map), lm_vocab_size))\n\n    return embeddings, word_map, lm_vocab_size\n\n\ndef clip_gradient(optimizer, grad_clip):\n    """"""\n    Clip gradients computed during backpropagation to prevent gradient explosion.\n\n    :param optimizer: optimized with the gradients to be clipped\n    :param grad_clip: gradient clip value\n    """"""\n    for group in optimizer.param_groups:\n        for param in group[\'params\']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)\n\n\ndef save_checkpoint(epoch, model, optimizer, val_f1, word_map, char_map, tag_map, lm_vocab_size, is_best):\n    """"""\n    Save model checkpoint.\n\n    :param epoch: epoch number\n    :param model: model\n    :param optimizer: optimized\n    :param val_f1: validation F1 score\n    :param word_map: word map\n    :param char_map: char map\n    :param tag_map: tag map\n    :param lm_vocab_size: number of words in-corpus, i.e. size of output vocabulary of linear model\n    :param is_best: is this checkpoint the best so far?\n    :return:\n    """"""\n    state = {\'epoch\': epoch,\n             \'f1\': val_f1,\n             \'model\': model,\n             \'optimizer\': optimizer,\n             \'word_map\': word_map,\n             \'tag_map\': tag_map,\n             \'char_map\': char_map,\n             \'lm_vocab_size\': lm_vocab_size}\n    filename = \'checkpoint_lm_lstm_crf.pth.tar\'\n    torch.save(state, filename)\n    # If checkpoint is the best so far, create a copy to avoid being overwritten by a subsequent worse checkpoint\n    if is_best:\n        torch.save(state, \'BEST_\' + filename)\n\n\nclass AverageMeter(object):\n    """"""\n    Keeps track of most recent, average, sum, and count of a metric.\n    """"""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, new_lr):\n    """"""\n    Shrinks learning rate by a specified factor.\n\n    :param optimizer: optimizer whose learning rates must be decayed\n    :param new_lr: new learning rate\n    """"""\n\n    print(""\\nDECAYING learning rate."")\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = new_lr\n    print(""The new learning rate is %f\\n"" % (optimizer.param_groups[0][\'lr\'],))\n\n\ndef log_sum_exp(tensor, dim):\n    """"""\n    Calculates the log-sum-exponent of a tensor\'s dimension in a numerically stable way.\n\n    :param tensor: tensor\n    :param dim: dimension to calculate log-sum-exp of\n    :return: log-sum-exp\n    """"""\n    m, _ = torch.max(tensor, dim)\n    m_expanded = m.unsqueeze(dim).expand_as(tensor)\n    return m + torch.log(torch.sum(torch.exp(tensor - m_expanded), dim))\n'"
