file_path,api_count,code
classify.py,5,"b'import torch\nfrom torch import nn\nfrom utils import preprocess, rev_label_map\nimport json\nimport os\nfrom nltk.tokenize import PunktSentenceTokenizer, TreebankWordTokenizer\nfrom PIL import Image, ImageDraw, ImageFont\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n# Load model\ncheckpoint = \'BEST_checkpoint_han.pth.tar\'\ncheckpoint = torch.load(checkpoint)\nmodel = checkpoint[\'model\']\nmodel = model.to(device)\nmodel.eval()\n\n# Pad limits, can use any high-enough value since our model does not compute over the pads\nsentence_limit = 15\nword_limit = 20\n\n# Word map to encode with\ndata_folder = \'/media/ssd/han data\'\nwith open(os.path.join(data_folder, \'word_map.json\'), \'r\') as j:\n    word_map = json.load(j)\n\n# Tokenizers\nsent_tokenizer = PunktSentenceTokenizer()\nword_tokenizer = TreebankWordTokenizer()\n\n\ndef classify(document):\n    """"""\n    Classify a document with the Hierarchial Attention Network (HAN).\n\n    :param document: a document in text form\n    :return: pre-processed tokenized document, class scores, attention weights for words, attention weights for sentences, sentence lengths\n    """"""\n    # A list to store the document tokenized into words\n    doc = list()\n\n    # Tokenize document into sentences\n    sentences = list()\n    for paragraph in preprocess(document).splitlines():\n        sentences.extend([s for s in sent_tokenizer.tokenize(paragraph)])\n\n    # Tokenize sentences into words\n    for s in sentences[:sentence_limit]:\n        w = word_tokenizer.tokenize(s)[:word_limit]\n        if len(w) == 0:\n            continue\n        doc.append(w)\n\n    # Number of sentences in the document\n    sentences_in_doc = len(doc)\n    sentences_in_doc = torch.LongTensor([sentences_in_doc]).to(device)  # (1)\n\n    # Number of words in each sentence\n    words_in_each_sentence = list(map(lambda s: len(s), doc))\n    words_in_each_sentence = torch.LongTensor(words_in_each_sentence).unsqueeze(0).to(device)  # (1, n_sentences)\n\n    # Encode document with indices from the word map\n    encoded_doc = list(\n        map(lambda s: list(map(lambda w: word_map.get(w, word_map[\'<unk>\']), s)) + [0] * (word_limit - len(s)),\n            doc)) + [[0] * word_limit] * (sentence_limit - len(doc))\n    encoded_doc = torch.LongTensor(encoded_doc).unsqueeze(0).to(device)\n\n    # Apply the HAN model\n    scores, word_alphas, sentence_alphas = model(encoded_doc, sentences_in_doc,\n                                                 words_in_each_sentence)  # (1, n_classes), (1, n_sentences, max_sent_len_in_document), (1, n_sentences)\n    scores = scores.squeeze(0)  # (n_classes)\n    scores = nn.functional.softmax(scores, dim=0)  # (n_classes)\n    word_alphas = word_alphas.squeeze(0)  # (n_sentences, max_sent_len_in_document)\n    sentence_alphas = sentence_alphas.squeeze(0)  # (n_sentences)\n    words_in_each_sentence = words_in_each_sentence.squeeze(0)  # (n_sentences)\n\n    return doc, scores, word_alphas, sentence_alphas, words_in_each_sentence\n\n\ndef visualize_attention(doc, scores, word_alphas, sentence_alphas, words_in_each_sentence):\n    """"""\n    Visualize important sentences and words, as seen by the HAN model.\n\n    :param doc: pre-processed tokenized document\n    :param scores: class scores, a tensor of size (n_classes)\n    :param word_alphas: attention weights of words, a tensor of size (n_sentences, max_sent_len_in_document)\n    :param sentence_alphas: attention weights of sentences, a tensor of size (n_sentences)\n    :param words_in_each_sentence: sentence lengths, a tensor of size (n_sentences)\n    """"""\n    # Find best prediction\n    score, prediction = scores.max(dim=0)\n    prediction = \'{category} ({score:.2f}%)\'.format(category=rev_label_map[prediction.item()], score=score.item() * 100)\n\n    # For each word, find it\'s effective importance (sentence alpha * word alpha)\n    alphas = (sentence_alphas.unsqueeze(1) * word_alphas * words_in_each_sentence.unsqueeze(\n        1).float() / words_in_each_sentence.max().float())\n    # alphas = word_alphas * words_in_each_sentence.unsqueeze(1).float() / words_in_each_sentence.max().float()\n    alphas = alphas.to(\'cpu\')\n\n    # Determine size of the image, visualization properties for each word, and each sentence\n    min_font_size = 15  # minimum size possible for a word, because size is scaled by normalized word*sentence alphas\n    max_font_size = 55  # maximum size possible for a word, because size is scaled by normalized word*sentence alphas\n    space_size = ImageFont.truetype(""./calibril.ttf"", max_font_size).getsize(\' \')  # use spaces of maximum font size\n    line_spacing = 15  # spacing between sentences\n    left_buffer = 100  # initial empty space on the left where sentence-rectangles will be drawn\n    top_buffer = 2 * min_font_size + 3 * line_spacing  # initial empty space on the top where the detected category will be displayed\n    image_width = left_buffer  # width of the entire image so far\n    image_height = top_buffer + line_spacing  # height of the entire image so far\n    word_loc = [image_width, image_height]  # top-left coordinates of the next word that will be printed\n    rectangle_height = 0.75 * max_font_size  # height of the rectangles that will represent sentence alphas\n    max_rectangle_width = 0.8 * left_buffer  # maximum width of the rectangles that will represent sentence alphas, scaled by sentence alpha\n    rectangle_loc = [0.9 * left_buffer,\n                     image_height + rectangle_height]  # bottom-right coordinates of next rectangle that will be printed\n    word_viz_properties = list()\n    sentence_viz_properties = list()\n    for s, sentence in enumerate(doc):\n        # Find visualization properties for each sentence, represented by rectangles\n        # Factor to scale by\n        sentence_factor = sentence_alphas[s].item() / sentence_alphas.max().item()\n\n        # Color of rectangle\n        rectangle_saturation = str(int(sentence_factor * 100))\n        rectangle_lightness = str(25 + 50 - int(sentence_factor * 50))\n        rectangle_color = \'hsl(0,\' + rectangle_saturation + \'%,\' + rectangle_lightness + \'%)\'\n\n        # Bounds of rectangle\n        rectangle_bounds = [rectangle_loc[0] - sentence_factor * max_rectangle_width,\n                            rectangle_loc[1] - rectangle_height] + rectangle_loc\n\n        # Save sentence\'s rectangle\'s properties\n        sentence_viz_properties.append({\'bounds\': rectangle_bounds.copy(),\n                                        \'color\': rectangle_color})\n\n        for w, word in enumerate(sentence):\n            # Find visualization properties for each word\n            # Factor to scale by\n            word_factor = alphas[s, w].item() / alphas.max().item()\n\n            # Color of word\n            word_saturation = str(int(word_factor * 100))\n            word_lightness = str(25 + 50 - int(word_factor * 50))\n            word_color = \'hsl(0,\' + word_saturation + \'%,\' + word_lightness + \'%)\'\n\n            # Size of word\n            word_font_size = int(min_font_size + word_factor * (max_font_size - min_font_size))\n            word_font = ImageFont.truetype(""./calibril.ttf"", word_font_size)\n\n            # Save word\'s properties\n            word_viz_properties.append({\'loc\': word_loc.copy(),\n                                        \'word\': word,\n                                        \'font\': word_font,\n                                        \'color\': word_color})\n\n            # Update word and sentence locations for next word, height, width values\n            word_size = word_font.getsize(word)\n            word_loc[0] += word_size[0] + space_size[0]\n            image_width = max(image_width, word_loc[0])\n        word_loc[0] = left_buffer\n        word_loc[1] += max_font_size + line_spacing\n        image_height = max(image_height, word_loc[1])\n        rectangle_loc[1] += max_font_size + line_spacing\n\n    # Create blank image\n    img = Image.new(\'RGB\', (image_width, image_height), (255, 255, 255))\n\n    # Draw\n    draw = ImageDraw.Draw(img)\n    # Words\n    for viz in word_viz_properties:\n        draw.text(xy=viz[\'loc\'], text=viz[\'word\'], fill=viz[\'color\'], font=viz[\'font\'])\n    # Rectangles that represent sentences\n    for viz in sentence_viz_properties:\n        draw.rectangle(xy=viz[\'bounds\'], fill=viz[\'color\'])\n    # Detected category/topic\n    category_font = ImageFont.truetype(""./calibril.ttf"", min_font_size)\n    draw.text(xy=[line_spacing, line_spacing], text=\'Detected Category:\', fill=\'grey\', font=category_font)\n    draw.text(xy=[line_spacing, line_spacing + category_font.getsize(\'Detected Category:\')[1] + line_spacing],\n              text=prediction.upper(), fill=\'black\',\n              font=category_font)\n    del draw\n\n    # Display\n    img.show()\n\n\n\nif __name__ == \'__main__\':\n    document = \'How do computers work? I have a CPU I want to use. But my keyboard and motherboard do not help.\\n\\n You can just google how computers work. Honestly, its easy.\'\n    document = \'But think about it! It\\\'s so cool. Physics is really all about math. what feynman said, hehe\'\n    document = ""I think I\'m falling sick. There was some indigestion at first. But now a fever is beginning to take hold.""\n    document = ""I want to tell you something important. Get into the stock market and investment funds. Make some money so you can buy yourself some yogurt.""\n    document = ""You know what\'s wrong with this country? republicans and democrats. always at each other\'s throats\\n There\'s no respect, no bipartisanship.""\n    visualize_attention(*classify(document))\n'"
create_input_files.py,0,"b""from utils import create_input_files, train_word2vec_model\n\nif __name__ == '__main__':\n    create_input_files(csv_folder='./yahoo_answers_csv',\n                       output_folder='/media/ssd/han data',\n                       sentence_limit=15,\n                       word_limit=20,\n                       min_word_count=5)\n\n    train_word2vec_model(data_folder='/media/ssd/han data',\n                         algorithm='skipgram')\n"""
datasets.py,6,"b'from torch.utils.data import Dataset\nimport torch\nimport os\n\n\nclass HANDataset(Dataset):\n    """"""\n    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n    """"""\n    def __init__(self, data_folder, split):\n        """"""\n        :param data_folder: folder where data files are stored\n        :param split: split, one of \'TRAIN\' or \'TEST\'\n        """"""\n        split = split.upper()\n        assert split in {\'TRAIN\', \'TEST\'}\n        self.split = split\n\n        # Load data\n        self.data = torch.load(os.path.join(data_folder, split + \'_data.pth.tar\'))\n\n    def __getitem__(self, i):\n        return torch.LongTensor(self.data[\'docs\'][i]), \\\n               torch.LongTensor([self.data[\'sentences_per_document\'][i]]), \\\n               torch.LongTensor(self.data[\'words_per_sentence\'][i]), \\\n               torch.LongTensor([self.data[\'labels\'][i]])\n\n    def __len__(self):\n        return len(self.data[\'labels\'])\n'"
eval.py,4,"b'import time\nfrom utils import *\nfrom datasets import HANDataset\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n# Data parameters\ndata_folder = \'/media/ssd/han data\'\n\n# Evaluation parameters\nbatch_size = 64  # batch size\nworkers = 4  # number of workers for loading data in the DataLoader\nprint_freq = 2000  # print training or validation status every __ batches\ncheckpoint = \'checkpoint_han.pth.tar\'\n\n# Load model\ncheckpoint = torch.load(checkpoint)\nmodel = checkpoint[\'model\']\nmodel = model.to(device)\nmodel.eval()\n\n# Load test data\ntest_loader = torch.utils.data.DataLoader(HANDataset(data_folder, \'test\'), batch_size=batch_size, shuffle=False,\n                                          num_workers=workers, pin_memory=True)\n\n# Track metrics\naccs = AverageMeter()  # accuracies\n\n# Evaluate in batches\nfor i, (documents, sentences_per_document, words_per_sentence, labels) in enumerate(\n        tqdm(test_loader, desc=\'Evaluating\')):\n\n    documents = documents.to(device)  # (batch_size, sentence_limit, word_limit)\n    sentences_per_document = sentences_per_document.squeeze(1).to(device)  # (batch_size)\n    words_per_sentence = words_per_sentence.to(device)  # (batch_size, sentence_limit)\n    labels = labels.squeeze(1).to(device)  # (batch_size)\n\n    # Forward prop.\n    scores, word_alphas, sentence_alphas = model(documents, sentences_per_document,\n                                                 words_per_sentence)  # (n_documents, n_classes), (n_documents, max_doc_len_in_batch, max_sent_len_in_batch), (n_documents, max_doc_len_in_batch)\n\n    # Find accuracy\n    _, predictions = scores.max(dim=1)  # (n_documents)\n    correct_predictions = torch.eq(predictions, labels).sum().item()\n    accuracy = correct_predictions / labels.size(0)\n\n    # Keep track of metrics\n    accs.update(accuracy, labels.size(0))\n\n    start = time.time()\n\n# Print final result\nprint(\'\\n * TEST ACCURACY - %.1f per cent\\n\' % (accs.avg * 100))\n'"
model.py,10,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\n\n\nclass HierarchialAttentionNetwork(nn.Module):\n    """"""\n    The overarching Hierarchial Attention Network (HAN).\n    """"""\n\n    def __init__(self, n_classes, vocab_size, emb_size, word_rnn_size, sentence_rnn_size, word_rnn_layers,\n                 sentence_rnn_layers, word_att_size, sentence_att_size, dropout=0.5):\n        """"""\n        :param n_classes: number of classes\n        :param vocab_size: number of words in the vocabulary of the model\n        :param emb_size: size of word embeddings\n        :param word_rnn_size: size of (bidirectional) word-level RNN\n        :param sentence_rnn_size: size of (bidirectional) sentence-level RNN\n        :param word_rnn_layers: number of layers in word-level RNN\n        :param sentence_rnn_layers: number of layers in sentence-level RNN\n        :param word_att_size: size of word-level attention layer\n        :param sentence_att_size: size of sentence-level attention layer\n        :param dropout: dropout\n        """"""\n        super(HierarchialAttentionNetwork, self).__init__()\n\n        # Sentence-level attention module (which will, in-turn, contain the word-level attention module)\n        self.sentence_attention = SentenceAttention(vocab_size, emb_size, word_rnn_size, sentence_rnn_size,\n                                                    word_rnn_layers, sentence_rnn_layers, word_att_size,\n                                                    sentence_att_size, dropout)\n\n        # Classifier\n        self.fc = nn.Linear(2 * sentence_rnn_size, n_classes)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, documents, sentences_per_document, words_per_sentence):\n        """"""\n        Forward propagation.\n\n        :param documents: encoded document-level data, a tensor of dimensions (n_documents, sent_pad_len, word_pad_len)\n        :param sentences_per_document: document lengths, a tensor of dimensions (n_documents)\n        :param words_per_sentence: sentence lengths, a tensor of dimensions (n_documents, sent_pad_len)\n        :return: class scores, attention weights of words, attention weights of sentences\n        """"""\n        # Apply sentence-level attention module (and in turn, word-level attention module) to get document embeddings\n        document_embeddings, word_alphas, sentence_alphas = self.sentence_attention(documents, sentences_per_document,\n                                                                                    words_per_sentence)  # (n_documents, 2 * sentence_rnn_size), (n_documents, max(sentences_per_document), max(words_per_sentence)), (n_documents, max(sentences_per_document))\n\n        # Classify\n        scores = self.fc(self.dropout(document_embeddings))  # (n_documents, n_classes)\n\n        return scores, word_alphas, sentence_alphas\n\n\nclass SentenceAttention(nn.Module):\n    """"""\n    The sentence-level attention module.\n    """"""\n\n    def __init__(self, vocab_size, emb_size, word_rnn_size, sentence_rnn_size, word_rnn_layers, sentence_rnn_layers,\n                 word_att_size, sentence_att_size, dropout):\n        """"""\n        :param vocab_size: number of words in the vocabulary of the model\n        :param emb_size: size of word embeddings\n        :param word_rnn_size: size of (bidirectional) word-level RNN\n        :param sentence_rnn_size: size of (bidirectional) sentence-level RNN\n        :param word_rnn_layers: number of layers in word-level RNN\n        :param sentence_rnn_layers: number of layers in sentence-level RNN\n        :param word_att_size: size of word-level attention layer\n        :param sentence_att_size: size of sentence-level attention layer\n        :param dropout: dropout\n        """"""\n        super(SentenceAttention, self).__init__()\n\n        # Word-level attention module\n        self.word_attention = WordAttention(vocab_size, emb_size, word_rnn_size, word_rnn_layers, word_att_size,\n                                            dropout)\n\n        # Bidirectional sentence-level RNN\n        self.sentence_rnn = nn.GRU(2 * word_rnn_size, sentence_rnn_size, num_layers=sentence_rnn_layers,\n                                   bidirectional=True, dropout=dropout, batch_first=True)\n\n        # Sentence-level attention network\n        self.sentence_attention = nn.Linear(2 * sentence_rnn_size, sentence_att_size)\n\n        # Sentence context vector to take dot-product with\n        self.sentence_context_vector = nn.Linear(sentence_att_size, 1,\n                                                 bias=False)  # this performs a dot product with the linear layer\'s 1D parameter vector, which is the sentence context vector\n        # You could also do this with:\n        # self.sentence_context_vector = nn.Parameter(torch.FloatTensor(1, sentence_att_size))\n        # self.sentence_context_vector.data.uniform_(-0.1, 0.1)\n        # And then take the dot-product\n\n        # Dropout\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, documents, sentences_per_document, words_per_sentence):\n        """"""\n        Forward propagation.\n\n        :param documents: encoded document-level data, a tensor of dimensions (n_documents, sent_pad_len, word_pad_len)\n        :param sentences_per_document: document lengths, a tensor of dimensions (n_documents)\n        :param words_per_sentence: sentence lengths, a tensor of dimensions (n_documents, sent_pad_len)\n        :return: document embeddings, attention weights of words, attention weights of sentences\n        """"""\n\n        # Re-arrange as sentences by removing sentence-pads (DOCUMENTS -> SENTENCES)\n        packed_sentences = pack_padded_sequence(documents,\n                                                lengths=sentences_per_document.tolist(),\n                                                batch_first=True,\n                                                enforce_sorted=False)  # a PackedSequence object, where \'data\' is the flattened sentences (n_sentences, word_pad_len)\n\n        # Re-arrange sentence lengths in the same way (DOCUMENTS -> SENTENCES)\n        packed_words_per_sentence = pack_padded_sequence(words_per_sentence,\n                                                         lengths=sentences_per_document.tolist(),\n                                                         batch_first=True,\n                                                         enforce_sorted=False)  # a PackedSequence object, where \'data\' is the flattened sentence lengths (n_sentences)\n\n        # Find sentence embeddings by applying the word-level attention module\n        sentences, word_alphas = self.word_attention(packed_sentences.data,\n                                                     packed_words_per_sentence.data)  # (n_sentences, 2 * word_rnn_size), (n_sentences, max(words_per_sentence))\n        sentences = self.dropout(sentences)\n\n        # Apply the sentence-level RNN over the sentence embeddings (PyTorch automatically applies it on the PackedSequence)\n        packed_sentences, _ = self.sentence_rnn(PackedSequence(data=sentences,\n                                                               batch_sizes=packed_sentences.batch_sizes,\n                                                               sorted_indices=packed_sentences.sorted_indices,\n                                                               unsorted_indices=packed_sentences.unsorted_indices))  # a PackedSequence object, where \'data\' is the output of the RNN (n_sentences, 2 * sentence_rnn_size)\n\n        # Find attention vectors by applying the attention linear layer on the output of the RNN\n        att_s = self.sentence_attention(packed_sentences.data)  # (n_sentences, att_size)\n        att_s = torch.tanh(att_s)  # (n_sentences, att_size)\n        # Take the dot-product of the attention vectors with the context vector (i.e. parameter of linear layer)\n        att_s = self.sentence_context_vector(att_s).squeeze(1)  # (n_sentences)\n\n        # Compute softmax over the dot-product manually\n        # Manually because they have to be computed only over sentences in the same document\n\n        # First, take the exponent\n        max_value = att_s.max()  # scalar, for numerical stability during exponent calculation\n        att_s = torch.exp(att_s - max_value)  # (n_sentences)\n\n        # Re-arrange as documents by re-padding with 0s (SENTENCES -> DOCUMENTS)\n        att_s, _ = pad_packed_sequence(PackedSequence(data=att_s,\n                                                      batch_sizes=packed_sentences.batch_sizes,\n                                                      sorted_indices=packed_sentences.sorted_indices,\n                                                      unsorted_indices=packed_sentences.unsorted_indices),\n                                       batch_first=True)  # (n_documents, max(sentences_per_document))\n\n        # Calculate softmax values as now sentences are arranged in their respective documents\n        sentence_alphas = att_s / torch.sum(att_s, dim=1, keepdim=True)  # (n_documents, max(sentences_per_document))\n\n        # Similarly re-arrange sentence-level RNN outputs as documents by re-padding with 0s (SENTENCES -> DOCUMENTS)\n        documents, _ = pad_packed_sequence(packed_sentences,\n                                           batch_first=True)  # (n_documents, max(sentences_per_document), 2 * sentence_rnn_size)\n\n        # Find document embeddings\n        documents = documents * sentence_alphas.unsqueeze(\n            2)  # (n_documents, max(sentences_per_document), 2 * sentence_rnn_size)\n        documents = documents.sum(dim=1)  # (n_documents, 2 * sentence_rnn_size)\n\n        # Also re-arrange word_alphas (SENTENCES -> DOCUMENTS)\n        word_alphas, _ = pad_packed_sequence(PackedSequence(data=word_alphas,\n                                                            batch_sizes=packed_sentences.batch_sizes,\n                                                            sorted_indices=packed_sentences.sorted_indices,\n                                                            unsorted_indices=packed_sentences.unsorted_indices),\n                                             batch_first=True)  # (n_documents, max(sentences_per_document), max(words_per_sentence))\n\n        return documents, word_alphas, sentence_alphas\n\n\nclass WordAttention(nn.Module):\n    """"""\n    The word-level attention module.\n    """"""\n\n    def __init__(self, vocab_size, emb_size, word_rnn_size, word_rnn_layers, word_att_size, dropout):\n        """"""\n        :param vocab_size: number of words in the vocabulary of the model\n        :param emb_size: size of word embeddings\n        :param word_rnn_size: size of (bidirectional) word-level RNN\n        :param word_rnn_layers: number of layers in word-level RNN\n        :param word_att_size: size of word-level attention layer\n        :param dropout: dropout\n        """"""\n        super(WordAttention, self).__init__()\n\n        # Embeddings (look-up) layer\n        self.embeddings = nn.Embedding(vocab_size, emb_size)\n\n        # Bidirectional word-level RNN\n        self.word_rnn = nn.GRU(emb_size, word_rnn_size, num_layers=word_rnn_layers, bidirectional=True,\n                               dropout=dropout, batch_first=True)\n\n        # Word-level attention network\n        self.word_attention = nn.Linear(2 * word_rnn_size, word_att_size)\n\n        # Word context vector to take dot-product with\n        self.word_context_vector = nn.Linear(word_att_size, 1, bias=False)\n        # You could also do this with:\n        # self.word_context_vector = nn.Parameter(torch.FloatTensor(1, word_att_size))\n        # self.word_context_vector.data.uniform_(-0.1, 0.1)\n        # And then take the dot-product\n\n        self.dropout = nn.Dropout(dropout)\n\n    def init_embeddings(self, embeddings):\n        """"""\n        Initialized embedding layer with pre-computed embeddings.\n\n        :param embeddings: pre-computed embeddings\n        """"""\n        self.embeddings.weight = nn.Parameter(embeddings)\n\n    def fine_tune_embeddings(self, fine_tune=False):\n        """"""\n        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n\n        :param fine_tune: allow?\n        """"""\n        for p in self.embeddings.parameters():\n            p.requires_grad = fine_tune\n\n    def forward(self, sentences, words_per_sentence):\n        """"""\n        Forward propagation.\n\n        :param sentences: encoded sentence-level data, a tensor of dimension (n_sentences, word_pad_len, emb_size)\n        :param words_per_sentence: sentence lengths, a tensor of dimension (n_sentences)\n        :return: sentence embeddings, attention weights of words\n        """"""\n\n        # Get word embeddings, apply dropout\n        sentences = self.dropout(self.embeddings(sentences))  # (n_sentences, word_pad_len, emb_size)\n\n        # Re-arrange as words by removing word-pads (SENTENCES -> WORDS)\n        packed_words = pack_padded_sequence(sentences,\n                                            lengths=words_per_sentence.tolist(),\n                                            batch_first=True,\n                                            enforce_sorted=False)  # a PackedSequence object, where \'data\' is the flattened words (n_words, word_emb)\n\n        # Apply the word-level RNN over the word embeddings (PyTorch automatically applies it on the PackedSequence)\n        packed_words, _ = self.word_rnn(\n            packed_words)  # a PackedSequence object, where \'data\' is the output of the RNN (n_words, 2 * word_rnn_size)\n\n        # Find attention vectors by applying the attention linear layer on the output of the RNN\n        att_w = self.word_attention(packed_words.data)  # (n_words, att_size)\n        att_w = torch.tanh(att_w)  # (n_words, att_size)\n        # Take the dot-product of the attention vectors with the context vector (i.e. parameter of linear layer)\n        att_w = self.word_context_vector(att_w).squeeze(1)  # (n_words)\n\n        # Compute softmax over the dot-product manually\n        # Manually because they have to be computed only over words in the same sentence\n\n        # First, take the exponent\n        max_value = att_w.max()  # scalar, for numerical stability during exponent calculation\n        att_w = torch.exp(att_w - max_value)  # (n_words)\n\n        # Re-arrange as sentences by re-padding with 0s (WORDS -> SENTENCES)\n        att_w, _ = pad_packed_sequence(PackedSequence(data=att_w,\n                                                      batch_sizes=packed_words.batch_sizes,\n                                                      sorted_indices=packed_words.sorted_indices,\n                                                      unsorted_indices=packed_words.unsorted_indices),\n                                       batch_first=True)  # (n_sentences, max(words_per_sentence))\n\n        # Calculate softmax values as now words are arranged in their respective sentences\n        word_alphas = att_w / torch.sum(att_w, dim=1, keepdim=True)  # (n_sentences, max(words_per_sentence))\n\n        # Similarly re-arrange word-level RNN outputs as sentences by re-padding with 0s (WORDS -> SENTENCES)\n        sentences, _ = pad_packed_sequence(packed_words,\n                                           batch_first=True)  # (n_sentences, max(words_per_sentence), 2 * word_rnn_size)\n\n        # Find sentence embeddings\n        sentences = sentences * word_alphas.unsqueeze(2)  # (n_sentences, max(words_per_sentence), 2 * word_rnn_size)\n        sentences = sentences.sum(dim=1)  # (n_sentences, 2 * word_rnn_size)\n\n        return sentences, word_alphas\n'"
train.py,6,"b'import time\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nfrom model import HierarchialAttentionNetwork\nfrom utils import *\nfrom datasets import HANDataset\n\n# Data parameters\ndata_folder = \'/media/ssd/han data\'\nword2vec_file = os.path.join(data_folder, \'word2vec_model\')  # path to pre-trained word2vec embeddings\nwith open(os.path.join(data_folder, \'word_map.json\'), \'r\') as j:\n    word_map = json.load(j)\n\n# Model parameters\nn_classes = len(label_map)\nword_rnn_size = 50  # word RNN size\nsentence_rnn_size = 50  # character RNN size\nword_rnn_layers = 1  # number of layers in character RNN\nsentence_rnn_layers = 1  # number of layers in word RNN\nword_att_size = 100  # size of the word-level attention layer (also the size of the word context vector)\nsentence_att_size = 100  # size of the sentence-level attention layer (also the size of the sentence context vector)\ndropout = 0.3  # dropout\nfine_tune_word_embeddings = True  # fine-tune word embeddings?\n\n# Training parameters\nstart_epoch = 0  # start at this epoch\nbatch_size = 64  # batch size\nlr = 1e-3  # learning rate\nmomentum = 0.9  # momentum\nworkers = 4  # number of workers for loading data in the DataLoader\nepochs = 2  # number of epochs to run\ngrad_clip = None  # clip gradients at this value\nprint_freq = 2000  # print training or validation status every __ batches\ncheckpoint = None  # path to model checkpoint, None if none\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\ncudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n\n\ndef main():\n    """"""\n    Training and validation.\n    """"""\n    global checkpoint, start_epoch, word_map\n\n    # Initialize model or load checkpoint\n    if checkpoint is not None:\n        checkpoint = torch.load(checkpoint)\n        model = checkpoint[\'model\']\n        optimizer = checkpoint[\'optimizer\']\n        word_map = checkpoint[\'word_map\']\n        start_epoch = checkpoint[\'epoch\'] + 1\n        print(\n            \'\\nLoaded checkpoint from epoch %d.\\n\' % (start_epoch - 1))\n    else:\n        embeddings, emb_size = load_word2vec_embeddings(word2vec_file, word_map)  # load pre-trained word2vec embeddings\n\n        model = HierarchialAttentionNetwork(n_classes=n_classes,\n                                            vocab_size=len(word_map),\n                                            emb_size=emb_size,\n                                            word_rnn_size=word_rnn_size,\n                                            sentence_rnn_size=sentence_rnn_size,\n                                            word_rnn_layers=word_rnn_layers,\n                                            sentence_rnn_layers=sentence_rnn_layers,\n                                            word_att_size=word_att_size,\n                                            sentence_att_size=sentence_att_size,\n                                            dropout=dropout)\n        model.sentence_attention.word_attention.init_embeddings(\n            embeddings)  # initialize embedding layer with pre-trained embeddings\n        model.sentence_attention.word_attention.fine_tune_embeddings(fine_tune_word_embeddings)  # fine-tune\n        optimizer = optim.Adam(params=filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n\n    # Loss functions\n    criterion = nn.CrossEntropyLoss()\n\n    # Move to device\n    model = model.to(device)\n    criterion = criterion.to(device)\n\n    # DataLoaders\n    train_loader = torch.utils.data.DataLoader(HANDataset(data_folder, \'train\'), batch_size=batch_size, shuffle=True,\n                                               num_workers=workers, pin_memory=True)\n\n    # Epochs\n    for epoch in range(start_epoch, epochs):\n        # One epoch\'s training\n        train(train_loader=train_loader,\n              model=model,\n              criterion=criterion,\n              optimizer=optimizer,\n              epoch=epoch)\n\n        # Decay learning rate every epoch\n        adjust_learning_rate(optimizer, 0.1)\n\n        # Save checkpoint\n        save_checkpoint(epoch, model, optimizer, word_map)\n\n\ndef train(train_loader, model, criterion, optimizer, epoch):\n    """"""\n    Performs one epoch\'s training.\n\n    :param train_loader: DataLoader for training data\n    :param model: model\n    :param criterion: cross entropy loss layer\n    :param optimizer: optimizer\n    :param epoch: epoch number\n    """"""\n\n    model.train()  # training mode enables dropout\n\n    batch_time = AverageMeter()  # forward prop. + back prop. time per batch\n    data_time = AverageMeter()  # data loading time per batch\n    losses = AverageMeter()  # cross entropy loss\n    accs = AverageMeter()  # accuracies\n\n    start = time.time()\n\n    # Batches\n    for i, (documents, sentences_per_document, words_per_sentence, labels) in enumerate(train_loader):\n\n        data_time.update(time.time() - start)\n\n        documents = documents.to(device)  # (batch_size, sentence_limit, word_limit)\n        sentences_per_document = sentences_per_document.squeeze(1).to(device)  # (batch_size)\n        words_per_sentence = words_per_sentence.to(device)  # (batch_size, sentence_limit)\n        labels = labels.squeeze(1).to(device)  # (batch_size)\n\n        # Forward prop.\n        scores, word_alphas, sentence_alphas = model(documents, sentences_per_document,\n                                                     words_per_sentence)  # (n_documents, n_classes), (n_documents, max_doc_len_in_batch, max_sent_len_in_batch), (n_documents, max_doc_len_in_batch)\n\n        # Loss\n        loss = criterion(scores, labels)  # scalar\n\n        # Back prop.\n        optimizer.zero_grad()\n        loss.backward()\n\n        # Clip gradients\n        if grad_clip is not None:\n            clip_gradient(optimizer, grad_clip)\n\n        # Update\n        optimizer.step()\n\n        # Find accuracy\n        _, predictions = scores.max(dim=1)  # (n_documents)\n        correct_predictions = torch.eq(predictions, labels).sum().item()\n        accuracy = correct_predictions / labels.size(0)\n\n        # Keep track of metrics\n        losses.update(loss.item(), labels.size(0))\n        batch_time.update(time.time() - start)\n        accs.update(accuracy, labels.size(0))\n\n        start = time.time()\n\n        # Print training status\n        if i % print_freq == 0:\n            print(\'Epoch: [{0}][{1}/{2}]\\t\'\n                  \'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\n                  \'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t\'\n                  \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\n                  \'Accuracy {acc.val:.3f} ({acc.avg:.3f})\'.format(epoch, i, len(train_loader),\n                                                                  batch_time=batch_time,\n                                                                  data_time=data_time, loss=losses,\n                                                                  acc=accs))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
understanding_packed_sequences.py,5,"b""import torch\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, PackedSequence\nfrom torch import nn\n\n# Let's create a batch of sequences of varying length\nsequences = torch.FloatTensor([[1, 2, 0, 0, 0, 0],  # length 2\n                               [3, 4, 5, 0, 0, 0],  # length 3\n                               [5, 6, 0, 0, 0, 0],  # length 2\n                               [8, 9, 10, 11, 12, 0]\n                               ])  # length 5\nseq_lengths = torch.LongTensor([2, 3, 2, 5])\n# Since they're of various lengths, they are padded with 0s to a fixed length \xe2\x80\x94 this is the only way they can be stored a tensor!\n\n# Apply an RNN over these sequences\nrnn = nn.RNN(1, 1, batch_first=True)\nrnn_output, _ = rnn(sequences.unsqueeze(2))\nrnn_output = rnn_output.squeeze(2)\nprint(rnn_output)\n# As you can see, the RNN computed over the pads, which is wasteful\n# Also, you've to manually disregard the RNN outputs at the padded positions for further processing, or loss calculation, or whatever\n# Furthermore, if the RNN was bidirectional, the output would be wrong because it will start with the pads in the backward direction\n\n# It's not just with an RNN \xe2\x80\x94 any other operation would also compute over the pads\nfc = nn.Linear(1, 1)\nfc_output = fc(sequences.unsqueeze(2)).squeeze(2)\nprint(fc_output)\n\n######################################## Packed Sequences ###############################################\n\n# How do we avoid this?\n# With PyTorch PackedSequence objects!\npacked_sequences = pack_padded_sequence(sequences.unsqueeze(2),\n                                        lengths=seq_lengths,\n                                        batch_first=True,\n                                        enforce_sorted=False)  # The .unsqueeze(2) is simply to add a third dimension which an RNN expects\n# This created a PackedSequence object WITHOUT PADS from the padded sequences, upon which a PyTorch RNN can operate directly\nrnn_output, _ = rnn(packed_sequences)\n# The output of the RNN is a PackedSequence object as well\n# So convert it from a PackedSequence back to its padded form\nrnn_output, __ = pad_packed_sequence(rnn_output, batch_first=True)\nrnn_output = rnn_output.squeeze(2)\nprint(rnn_output)\n# There was no computation at the pads!\n# Note that, when it was re-padded, it was re-padded only up to the maximum sequence length (5), and not the original padded length (6)\n\n# What's inside a PackedSequence object?\n# The 'sorted_indices' attribute contains the sorting order to reorder the sequences by decreasing lengths \xe2\x80\x94 more on the reason for sorting below\nprint(packed_sequences.sorted_indices)\n# The 'data' attribute contains a flattened form of the sorted sequences without the pads\nprint(packed_sequences.data)\n# The 'batch_sizes' attribute notes the effective (non-pad) batch-size at each timestep\nprint(packed_sequences.batch_sizes)\n# The 'unsorted_indices' attribute contains the unsorting order to restore the original order of the sequences\nprint(packed_sequences.unsorted_indices)\n\n# All of these attributes can also be accessed as a tuple\ndata, batch_sizes, sorted_indices, unsorted_indices = packed_sequences\n\n# So, RNNs can operate on PackedSequences, and it all magically works without computing over the pads\n# What about other operations?\n# Since the 'data' attribute is a flattened form without the pads, you can use it for other operations\nfc_output = fc(packed_sequences.data)  # and any other operations you want to do\n# After everything you need is done, re-pad it into its original form (if required)\nfc_output, _ = pad_packed_sequence(PackedSequence(data=fc_output,\n                                                  batch_sizes=packed_sequences.batch_sizes,\n                                                  sorted_indices=packed_sequences.sorted_indices,\n                                                  unsorted_indices=packed_sequences.unsorted_indices),\n                                   batch_first=True)\nfc_output = fc_output.squeeze(2)\nprint(fc_output)\n\n######################################## What's really happening here? ###############################################\n\n# A PackedSequence essentially flattens the padded tensor by timestep, keeping only the non-pad units at each timestep\n\n# 1. The sequences are sorted by decreasing sequence lengths, which is the equivalent of:\nsorted_lengths, sort_indices = torch.sort(seq_lengths, descending=True)\nsorted_sequences = sequences[sort_indices]\n# The reason for the sorting is that the non-pads must be concentrated at the top\n# This prevents alignment problems when the pads are eliminated\nprint(sort_indices)\nprint(packed_sequences.sorted_indices)\nprint(sorted_sequences)\nprint(sorted_lengths)\n\n# 2. At each timestep, the effective batch size (excluding the pads) is noted, which is the equivalent of:\neffective_batch_sizes = [(i < sorted_lengths).sum().item() for i in range(sorted_sequences.size(1))]\nprint(effective_batch_sizes)\nprint(packed_sequences.batch_sizes)\n\n# 3. The sequences are flattened by timestep (excluding the pads), which is the equivalent of:\nflattened_sequences = torch.cat(\n    [sorted_sequences[:, i][:effective_batch_sizes[i]] for i in range(sorted_sequences.size(1))], dim=0)\nprint(flattened_sequences)\nprint(packed_sequences.data.squeeze(1))\n\n# RNNs operate on the sorted sequences only up to the effective batch size (b) at each timestep\n# For the next timestep, it takes only the 'b' top outputs from the previous timestep, and so on...\n# Please see the tutorial for a visual explanation\n\n# Any other operation, such as a linear layer, can operate directly upon the flattened sequence ('data' attribute) since it doesn't contain any pads\n\n# For something like loss computation over the non-pads, it's really convenient to just do it over the 'data' attribute of a PackedSequence since it will eliminate the pads for you\n# I do this in my Image Captioning and Sequence Labeling tutorials, in train.py\n\n# For custom sequential operations, using the effective batch size at each timestep to avoid computation over the pads is very useful\n# I do this in my Image Captioning and Sequence Labeling tutorials, search for 'batch_size_t'\n"""
utils.py,7,"b'import torch\nfrom torch import nn\nimport numpy as np\nfrom collections import Counter\nfrom nltk.tokenize import PunktSentenceTokenizer, TreebankWordTokenizer\nfrom tqdm import tqdm\nimport pandas as pd\nimport itertools\nimport os\nimport json\nimport gensim\nimport logging\n\nclasses = [\'Society & Culture\',\n           \'Science & Mathematics\',\n           \'Health\',\n           \'Education & Reference\',\n           \'Computers & Internet\',\n           \'Sports\',\n           \'Business & Finance\',\n           \'Entertainment & Music\',\n           \'Family & Relationships\',\n           \'Politics & Government\']\nlabel_map = {k: v for v, k in enumerate(classes)}\nrev_label_map = {v: k for k, v in label_map.items()}\n\n# Tokenizers\nsent_tokenizer = PunktSentenceTokenizer()\nword_tokenizer = TreebankWordTokenizer()\n\n\ndef preprocess(text):\n    """"""\n    Pre-process text for use in the model. This includes lower-casing, standardizing newlines, removing junk.\n\n    :param text: a string\n    :return: cleaner string\n    """"""\n    if isinstance(text, float):\n        return \'\'\n\n    return text.lower().replace(\'<br />\', \'\\n\').replace(\'<br>\', \'\\n\').replace(\'\\\\n\', \'\\n\').replace(\'&#xd;\', \'\\n\')\n\n\ndef read_csv(csv_folder, split, sentence_limit, word_limit):\n    """"""\n    Read CSVs containing raw training data, clean documents and labels, and do a word-count.\n\n    :param csv_folder: folder containing the CSV\n    :param split: train or test CSV?\n    :param sentence_limit: truncate long documents to these many sentences\n    :param word_limit: truncate long sentences to these many words\n    :return: documents, labels, a word-count\n    """"""\n    assert split in {\'train\', \'test\'}\n\n    docs = []\n    labels = []\n    word_counter = Counter()\n    data = pd.read_csv(os.path.join(csv_folder, split + \'.csv\'), header=None)\n    for i in tqdm(range(data.shape[0])):\n        row = list(data.loc[i, :])\n\n        sentences = list()\n\n        for text in row[1:]:\n            for paragraph in preprocess(text).splitlines():\n                sentences.extend([s for s in sent_tokenizer.tokenize(paragraph)])\n\n        words = list()\n        for s in sentences[:sentence_limit]:\n            w = word_tokenizer.tokenize(s)[:word_limit]\n            # If sentence is empty (due to removing punctuation, digits, etc.)\n            if len(w) == 0:\n                continue\n            words.append(w)\n            word_counter.update(w)\n        # If all sentences were empty\n        if len(words) == 0:\n            continue\n\n        labels.append(int(row[0]) - 1)  # since labels are 1-indexed in the CSV\n        docs.append(words)\n\n    return docs, labels, word_counter\n\n\ndef create_input_files(csv_folder, output_folder, sentence_limit, word_limit, min_word_count=5,\n                       save_word2vec_data=True):\n    """"""\n    Create data files to be used for training the model.\n\n    :param csv_folder: folder where the CSVs with the raw data are located\n    :param output_folder: folder where files must be created\n    :param sentence_limit: truncate long documents to these many sentences\n    :param word_limit: truncate long sentences to these many words\n    :param min_word_count: discard rare words which occur fewer times than this number\n    :param save_word2vec_data: whether to save the data required for training word2vec embeddings\n    """"""\n    # Read training data\n    print(\'\\nReading and preprocessing training data...\\n\')\n    train_docs, train_labels, word_counter = read_csv(csv_folder, \'train\', sentence_limit, word_limit)\n\n    # Save text data for word2vec\n    if save_word2vec_data:\n        torch.save(train_docs, os.path.join(output_folder, \'word2vec_data.pth.tar\'))\n        print(\'\\nText data for word2vec saved to %s.\\n\' % os.path.abspath(output_folder))\n\n    # Create word map\n    word_map = dict()\n    word_map[\'<pad>\'] = 0\n    for word, count in word_counter.items():\n        if count >= min_word_count:\n            word_map[word] = len(word_map)\n    word_map[\'<unk>\'] = len(word_map)\n    print(\'\\nDiscarding words with counts less than %d, the size of the vocabulary is %d.\\n\' % (\n        min_word_count, len(word_map)))\n\n    with open(os.path.join(output_folder, \'word_map.json\'), \'w\') as j:\n        json.dump(word_map, j)\n    print(\'Word map saved to %s.\\n\' % os.path.abspath(output_folder))\n\n    # Encode and pad\n    print(\'Encoding and padding training data...\\n\')\n    encoded_train_docs = list(map(lambda doc: list(\n        map(lambda s: list(map(lambda w: word_map.get(w, word_map[\'<unk>\']), s)) + [0] * (word_limit - len(s)),\n            doc)) + [[0] * word_limit] * (sentence_limit - len(doc)), train_docs))\n    sentences_per_train_document = list(map(lambda doc: len(doc), train_docs))\n    words_per_train_sentence = list(\n        map(lambda doc: list(map(lambda s: len(s), doc)) + [0] * (sentence_limit - len(doc)), train_docs))\n\n    # Save\n    print(\'Saving...\\n\')\n    assert len(encoded_train_docs) == len(train_labels) == len(sentences_per_train_document) == len(\n        words_per_train_sentence)\n    # Because of the large data, saving as a JSON can be very slow\n    torch.save({\'docs\': encoded_train_docs,\n                \'labels\': train_labels,\n                \'sentences_per_document\': sentences_per_train_document,\n                \'words_per_sentence\': words_per_train_sentence},\n               os.path.join(output_folder, \'TRAIN_data.pth.tar\'))\n    print(\'Encoded, padded training data saved to %s.\\n\' % os.path.abspath(output_folder))\n\n    # Free some memory\n    del train_docs, encoded_train_docs, train_labels, sentences_per_train_document, words_per_train_sentence\n\n    # Read test data\n    print(\'Reading and preprocessing test data...\\n\')\n    test_docs, test_labels, _ = read_csv(csv_folder, \'test\', sentence_limit, word_limit)\n\n    # Encode and pad\n    print(\'\\nEncoding and padding test data...\\n\')\n    encoded_test_docs = list(map(lambda doc: list(\n        map(lambda s: list(map(lambda w: word_map.get(w, word_map[\'<unk>\']), s)) + [0] * (word_limit - len(s)),\n            doc)) + [[0] * word_limit] * (sentence_limit - len(doc)), test_docs))\n    sentences_per_test_document = list(map(lambda doc: len(doc), test_docs))\n    words_per_test_sentence = list(\n        map(lambda doc: list(map(lambda s: len(s), doc)) + [0] * (sentence_limit - len(doc)), test_docs))\n\n    # Save\n    print(\'Saving...\\n\')\n    assert len(encoded_test_docs) == len(test_labels) == len(sentences_per_test_document) == len(\n        words_per_test_sentence)\n    torch.save({\'docs\': encoded_test_docs,\n                \'labels\': test_labels,\n                \'sentences_per_document\': sentences_per_test_document,\n                \'words_per_sentence\': words_per_test_sentence},\n               os.path.join(output_folder, \'TEST_data.pth.tar\'))\n    print(\'Encoded, padded test data saved to %s.\\n\' % os.path.abspath(output_folder))\n\n    print(\'All done!\\n\')\n\n\ndef train_word2vec_model(data_folder, algorithm=\'skipgram\'):\n    """"""\n    Train a word2vec model for word embeddings.\n\n    See the paper by Mikolov et. al. for details - https://arxiv.org/pdf/1310.4546.pdf\n\n    :param data_folder: folder with the word2vec training data\n    :param algorithm: use the Skip-gram or Continous Bag Of Words (CBOW) algorithm?\n    """"""\n    assert algorithm in [\'skipgram\', \'cbow\']\n    sg = 1 if algorithm is \'skipgram\' else 0\n\n    # Read data\n    sentences = torch.load(os.path.join(data_folder, \'word2vec_data.pth.tar\'))\n    sentences = list(itertools.chain.from_iterable(sentences))\n\n    # Activate logging for verbose training\n    logging.basicConfig(format=\'%(asctime)s : %(levelname)s : %(message)s\', level=logging.INFO)\n\n    # Initialize and train the model (this will take some time)\n    model = gensim.models.word2vec.Word2Vec(sentences=sentences, size=200, workers=8, window=10, min_count=5,\n                                            sg=sg)\n\n    # Normalize vectors and save model\n    model.init_sims(True)\n    model.wv.save(os.path.join(data_folder, \'word2vec_model\'))\n\n\ndef init_embedding(input_embedding):\n    """"""\n    Initialize embedding tensor with values from the uniform distribution.\n\n    :param input_embedding: embedding tensor\n    """"""\n    bias = np.sqrt(3.0 / input_embedding.size(1))\n    nn.init.uniform_(input_embedding, -bias, bias)\n\n\ndef load_word2vec_embeddings(word2vec_file, word_map):\n    """"""\n    Load pre-trained embeddings for words in the word map.\n\n    :param word2vec_file: location of the trained word2vec model\n    :param word_map: word map\n    :return: embeddings for words in the word map, embedding size\n    """"""\n    # Load word2vec model into memory\n    w2v = gensim.models.KeyedVectors.load(word2vec_file, mmap=\'r\')\n\n    print(""\\nEmbedding length is %d.\\n"" % w2v.vector_size)\n\n    # Create tensor to hold embeddings for words that are in-corpus\n    embeddings = torch.FloatTensor(len(word_map), w2v.vector_size)\n    init_embedding(embeddings)\n\n    # Read embedding file\n    print(""Loading embeddings..."")\n    for word in word_map:\n        if word in w2v.vocab:\n            embeddings[word_map[word]] = torch.FloatTensor(w2v[word])\n\n    print(""Done.\\n Embedding vocabulary: %d.\\n"" % len(word_map))\n\n    return embeddings, w2v.vector_size\n\n\ndef clip_gradient(optimizer, grad_clip):\n    """"""\n    Clip gradients computed during backpropagation to prevent gradient explosion.\n\n    :param optimizer: optimized with the gradients to be clipped\n    :param grad_clip: gradient clip value\n    """"""\n    for group in optimizer.param_groups:\n        for param in group[\'params\']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip, grad_clip)\n\n\ndef save_checkpoint(epoch, model, optimizer, word_map):\n    """"""\n    Save model checkpoint.\n\n    :param epoch: epoch number\n    :param model: model\n    :param optimizer: optimizer\n    :param best_acc: best accuracy achieved so far (not necessarily in this checkpoint)\n    :param word_map: word map\n    :param epochs_since_improvement: number of epochs since last improvement\n    :param is_best: is this checkpoint the best so far?\n    """"""\n    state = {\'epoch\': epoch,\n             \'model\': model,\n             \'optimizer\': optimizer,\n             \'word_map\': word_map}\n    filename = \'checkpoint_han.pth.tar\'\n    torch.save(state, filename)\n\n\nclass AverageMeter(object):\n    """"""\n    Keeps track of most recent, average, sum, and count of a metric.\n    """"""\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef adjust_learning_rate(optimizer, scale_factor):\n    """"""\n    Shrinks learning rate by a specified factor.\n\n    :param optimizer: optimizer whose learning rates must be decayed\n    :param scale_factor: factor to scale by\n    """"""\n\n    print(""\\nDECAYING learning rate."")\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = param_group[\'lr\'] * scale_factor\n    print(""The new learning rate is %f\\n"" % (optimizer.param_groups[0][\'lr\'],))\n'"
