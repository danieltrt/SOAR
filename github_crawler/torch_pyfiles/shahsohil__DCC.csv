file_path,api_count,code
pytorch/DCC.py,17,"b'from __future__ import print_function\nimport os\nimport random\nimport math\nimport numpy as np\nimport scipy.io as sio\nimport argparse\n\nfrom config import cfg, get_data_dir, get_output_dir, AverageMeter, remove_files_in_dir\n\nimport data_params as dp\nimport matplotlib.pyplot as plt\nimport io\nimport PIL.Image\nfrom torchvision.transforms import ToTensor\n\nimport torch\nimport torch.optim as optim\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\n\nfrom custom_data import DCCPT_data, DCCFT_data, DCCSampler\nfrom DCCLoss import DCCWeightedELoss, DCCLoss\nfrom DCCComputation import makeDCCinp, computeHyperParams, computeObj\n\n# used for logging to TensorBoard\nfrom tensorboard_logger import Logger\n\n# Parse all the input argument\nparser = argparse.ArgumentParser(description=\'PyTorch DCC Finetuning\')\nparser.add_argument(\'--data\', dest=\'db\', type=str, default=\'mnist\',\n                    help=\'Name of the dataset. The name should match with the output folder name.\')\nparser.add_argument(\'--batchsize\', type=int, default=cfg.PAIRS_PER_BATCH, help=\'batch size used for Finetuning\')\nparser.add_argument(\'--nepoch\', type=int, default=500, help=\'maximum number of iterations used for Finetuning\')\n# By default M = 20 is used. For convolutional SDAE M=10 was used.\n# Similarly, for different NW architecture different value for M may be required.\nparser.add_argument(\'--M\', type=int, default=20, help=\'inner number of epochs at which to change lambda\')\nparser.add_argument(\'--lr\', default=0.001, type=float, help=\'initial learning rate\')\nparser.add_argument(\'--manualSeed\', default=cfg.RNG_SEED, type=int, help=\'manual seed\')\nparser.add_argument(\'--net\', dest=\'torchmodel\', help=\'path to the pretrained weights file\', default=None, type=str)\nparser.add_argument(\'--resume\', \'-r\', action=\'store_true\', help=\'resume from checkpoint\')\nparser.add_argument(\'--level\', default=0, type=int, help=\'epoch to resume from\')\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'number of GPUs to use\')\nparser.add_argument(\'--deviceID\', type=int, help=\'deviceID\', default=0)\nparser.add_argument(\'--h5\', dest=\'h5\', action=\'store_true\', help=\'to store as h5py file\')\nparser.add_argument(\'--dim\', type=int, help=\'dimension of embedding space\', default=10)\nparser.add_argument(\'--tensorboard\', help=\'Log progress to TensorBoard\', action=\'store_true\')\nparser.add_argument(\'--id\', type=int, help=\'identifying number for storing tensorboard logs\')\nparser.add_argument(\'--clean_log\', action=\'store_true\', help=\'remove previous tensorboard logs under this ID\')\n\n\ndef main(args, net=None):\n    global oldassignment\n\n    datadir = get_data_dir(args.db)\n    outputdir = get_output_dir(args.db)\n\n    logger = None\n    if args.tensorboard:\n        # One should create folder for storing logs\n        loggin_dir = os.path.join(outputdir, \'runs\', \'DCC\')\n        if not os.path.exists(loggin_dir):\n            os.makedirs(loggin_dir)\n        loggin_dir = os.path.join(loggin_dir, \'%s\' % (args.id))\n        if args.clean_log:\n            remove_files_in_dir(loggin_dir)\n        logger = Logger(loggin_dir)\n\n    use_cuda = torch.cuda.is_available()\n\n    # Set the seed for reproducing the results\n    random.seed(args.manualSeed)\n    np.random.seed(args.manualSeed)\n    torch.manual_seed(args.manualSeed)\n    if use_cuda:\n        torch.cuda.manual_seed_all(args.manualSeed)\n        torch.backends.cudnn.enabled = True\n        cudnn.benchmark = True\n\n\n    startepoch = 0\n    kwargs = {\'num_workers\': 5, \'pin_memory\': True} if use_cuda else {}\n\n    # setting up dataset specific objects\n    trainset = DCCPT_data(root=datadir, train=True, h5=args.h5)\n    testset = DCCPT_data(root=datadir, train=False, h5=args.h5)\n    numeval = len(trainset) + len(testset)\n\n    # extracting training data from the pretrained.mat file\n    data, labels, pairs, Z, sampweight = makeDCCinp(args)\n\n    # For simplicity, I have created placeholder for each datasets and model\n    load_pretraining = True if net is None else False\n    if net is None:\n        net = dp.load_predefined_extract_net(args)\n\n    # reshaping data for some datasets\n    if args.db == \'cmnist\':\n        data = data.reshape((-1, 1, 28, 28))\n    elif args.db == \'ccoil100\':\n        data = data.reshape((-1, 3, 128, 128))\n    elif args.db == \'cytf\':\n        data = data.reshape((-1, 3, 55, 55))\n    elif args.db == \'cyale\':\n        data = data.reshape((-1, 1, 168, 192))\n\n    totalset = torch.utils.data.ConcatDataset([trainset, testset])\n\n    # computing and initializing the hyperparams\n    _sigma1, _sigma2, _lambda, _delta, _delta1, _delta2, lmdb, lmdb_data = computeHyperParams(pairs, Z)\n    oldassignment = np.zeros(len(pairs))\n    stopping_threshold = int(math.ceil(cfg.STOPPING_CRITERION * float(len(pairs))))\n\n    # Create dataset and random batch sampler for Finetuning stage\n    trainset = DCCFT_data(pairs, data, sampweight)\n    batch_sampler = DCCSampler(trainset, shuffle=True, batch_size=args.batchsize)\n\n    # copying model params from Pretrained (SDAE) weights file\n    if load_pretraining:\n        load_weights(args, outputdir, net)\n\n\n    # creating objects for loss functions, U\'s are initialized to Z here\n    # Criterion1 corresponds to reconstruction loss\n    criterion1 = DCCWeightedELoss(size_average=True)\n    # Criterion2 corresponds to sum of pairwise and data loss terms\n    criterion2 = DCCLoss(Z.shape[0], Z.shape[1], Z, size_average=True)\n\n    if use_cuda:\n        net.cuda()\n        criterion1 = criterion1.cuda()\n        criterion2 = criterion2.cuda()\n\n    # setting up data loader for training and testing phase\n    trainloader = torch.utils.data.DataLoader(trainset, batch_sampler=batch_sampler, **kwargs)\n    testloader = torch.utils.data.DataLoader(totalset, batch_size=args.batchsize, shuffle=False, **kwargs)\n\n    # setting up optimizer - the bias params should have twice the learning rate w.r.t. weights params\n    bias_params = filter(lambda x: (\'bias\' in x[0]), net.named_parameters())\n    bias_params = list(map(lambda x: x[1], bias_params))\n    nonbias_params = filter(lambda x: (\'bias\' not in x[0]), net.named_parameters())\n    nonbias_params = list(map(lambda x: x[1], nonbias_params))\n\n    optimizer = optim.Adam([{\'params\': bias_params, \'lr\': 2*args.lr},\n                            {\'params\': nonbias_params},\n                            {\'params\': criterion2.parameters(), \'lr\': args.lr},\n                            ], lr=args.lr, betas=(0.99, 0.999))\n\n    # this is needed for WARM START\n    if args.resume:\n        filename = outputdir+\'/FTcheckpoint_%d.pth.tar\' % args.level\n        if os.path.isfile(filename):\n            print(""==> loading checkpoint \'{}\'"".format(filename))\n            checkpoint = torch.load(filename)\n            net.load_state_dict(checkpoint[\'state_dict\'])\n            criterion2.load_state_dict(checkpoint[\'criterion_state_dict\'])\n            startepoch = checkpoint[\'epoch\']\n            optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            _sigma1 = checkpoint[\'sigma1\']\n            _sigma2 = checkpoint[\'sigma2\']\n            _lambda = checkpoint[\'lambda\']\n            _delta = checkpoint[\'delta\']\n            _delta1 = checkpoint[\'delta1\']\n            _delta2 = checkpoint[\'delta2\']\n        else:\n            print(""==> no checkpoint found at \'{}\'"".format(filename))\n            raise ValueError\n\n    # This is the actual Algorithm\n    flag = 0\n    for epoch in range(startepoch, args.nepoch):\n        if logger:\n            logger.log_value(\'sigma1\', _sigma1, epoch)\n            logger.log_value(\'sigma2\', _sigma2, epoch)\n            logger.log_value(\'lambda\', _lambda, epoch)\n\n        train(trainloader, net, optimizer, criterion1, criterion2, epoch, use_cuda, _sigma1, _sigma2, _lambda, logger)\n        Z, U, change_in_assign, assignment = test(testloader, net, criterion2, epoch, use_cuda, _delta, pairs, numeval, flag, logger)\n\n        if flag:\n            # As long as the change in label assignment < threshold, DCC continues to run.\n            # Note: This condition is always met in the very first epoch after the flag is set.\n            # This false criterion is overwritten by checking for the condition twice.\n            if change_in_assign > stopping_threshold:\n                flag += 1\n            if flag == 4:\n                break\n\n        if((epoch+1) % args.M == 0):\n            _sigma1 = max(_delta1, _sigma1 / 2)\n            _sigma2 = max(_delta2, _sigma2 / 2)\n            if _sigma2 == _delta2 and flag == 0:\n                # Start checking for stopping criterion\n                flag = 1\n\n        # Save checkpoint\n        index = (epoch // args.M) * args.M\n        save_checkpoint({\'epoch\': epoch+1,\n                         \'state_dict\': net.state_dict(),\n                         \'criterion_state_dict\': criterion2.state_dict(),\n                         \'optimizer\': optimizer.state_dict(),\n                         \'sigma1\': _sigma1,\n                         \'sigma2\': _sigma2,\n                         \'lambda\': _lambda,\n                         \'delta\': _delta,\n                         \'delta1\': _delta1,\n                         \'delta2\': _delta2,\n                         }, index, filename=outputdir)\n\n    output = {\'Z\': Z, \'U\': U, \'gtlabels\': labels, \'w\': pairs, \'cluster\':assignment}\n    sio.savemat(os.path.join(outputdir, \'features\'), output)\n\ndef load_weights(args, outputdir, net):\n    filename = os.path.join(outputdir, args.torchmodel)\n    if os.path.isfile(filename):\n        print(""==> loading params from checkpoint \'{}\'"".format(filename))\n        checkpoint = torch.load(filename)\n        net.load_state_dict(checkpoint[\'state_dict\'])\n    else:\n        print(""==> no checkpoint found at \'{}\'"".format(filename))\n        raise ValueError\n\n# Training\ndef train(trainloader, net, optimizer, criterion1, criterion2, epoch, use_cuda, _sigma1, _sigma2, _lambda, logger):\n    losses = AverageMeter()\n    losses1 = AverageMeter()\n    losses2 = AverageMeter()\n\n    print(\'\\n Epoch: %d\' % epoch)\n\n    net.train()\n\n    for batch_idx, (inputs, pairweights, sampweights, pairs, index) in enumerate(trainloader):\n        inputs = torch.squeeze(inputs,0)\n        pairweights = torch.squeeze(pairweights)\n        sampweights = torch.squeeze(sampweights)\n        index = torch.squeeze(index)\n        pairs = pairs.view(-1, 2)\n\n        if use_cuda:\n            inputs = inputs.cuda()\n            pairweights = pairweights.cuda()\n            sampweights = sampweights.cuda()\n            index = index.cuda()\n            pairs = pairs.cuda()\n\n        optimizer.zero_grad()\n        inputs_Var, sampweights, pairweights = Variable(inputs), Variable(sampweights, requires_grad=False), \\\n                                               Variable(pairweights, requires_grad=False)\n\n        enc, dec = net(inputs_Var)\n        loss1 = criterion1(inputs_Var, dec, sampweights)\n        loss2 = criterion2(enc, sampweights, pairweights, pairs, index, _sigma1, _sigma2, _lambda)\n        loss = loss1 + loss2\n\n        # record loss\n        losses1.update(loss1.item(), inputs.size(0))\n        losses2.update(loss2.item(), inputs.size(0))\n        losses.update(loss.item(), inputs.size(0))\n\n        loss.backward()\n        optimizer.step()\n\n    # log to TensorBoard\n    if logger:\n        logger.log_value(\'total_loss\', losses.avg, epoch)\n        logger.log_value(\'reconstruction_loss\', losses1.avg, epoch)\n        logger.log_value(\'dcc_loss\', losses2.avg, epoch)\n\n\n# Testing\ndef test(testloader, net, criterion, epoch, use_cuda, _delta, pairs, numeval, flag, logger):\n    net.eval()\n\n    original = []\n    features = []\n    labels = []\n\n    for batch_idx, (inputs, targets) in enumerate(testloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        inputs_Var = Variable(inputs, volatile=True)\n        enc, dec = net(inputs_Var)\n        features += list(enc.data.cpu().numpy())\n        labels += list(targets)\n        original += list(inputs.cpu().numpy())\n\n    original, features, labels = np.asarray(original).astype(np.float32), np.asarray(features).astype(np.float32), \\\n                                  np.asarray(labels)\n\n    U = criterion.U.data.cpu().numpy()\n\n    change_in_assign = 0\n    assignment = -np.ones(len(labels))\n\n    if logger and epoch % 3 == 0:\n        logger.log_images(\'representatives\', plot_to_image(U, \'representatives\'), epoch)\n\n    # logs clustering measures only if sigma2 has reached the minimum (delta2)\n    if flag:\n        index, ari, ami, nmi, acc, n_components, assignment = computeObj(U, pairs, _delta, labels, numeval)\n\n        # log to TensorBoard\n        change_in_assign = np.abs(oldassignment - index).sum()\n        if logger:\n            logger.log_value(\'ARI\', ari, epoch)\n            logger.log_value(\'AMI\', ami, epoch)\n            logger.log_value(\'NMI\', nmi, epoch)\n            logger.log_value(\'ACC\', acc, epoch)\n            logger.log_value(\'Numcomponents\', n_components, epoch)\n            logger.log_value(\'labeldiff\', change_in_assign, epoch)\n\n        oldassignment[...] = index\n\n    return features, U, change_in_assign, assignment\n\ndef plot_to_image(U, title):\n    plt.clf()\n    plt.scatter(U[:,0], U[:,1])\n    plt.title(title)\n    buf = io.BytesIO()\n    plt.savefig(buf, format=\'png\')\n    buf.seek(0)\n    image = PIL.Image.open(buf)\n    image = ToTensor()(image).unsqueeze(0)\n    return image\n\n# Saving checkpoint\ndef save_checkpoint(state, index, filename):\n    newfilename = os.path.join(filename, \'FTcheckpoint_%d.pth.tar\' % index)\n    torch.save(state, newfilename)\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    main(args)\n'"
pytorch/DCCComputation.py,0,"b'import os\nimport numpy as np\n\nimport scipy.io as sio\nfrom scipy.optimize import linear_sum_assignment\nfrom scipy.sparse import csr_matrix, diags\nimport scipy.sparse as sparse\nfrom scipy.sparse.csgraph import connected_components\nfrom sklearn import metrics\n\nfrom config import cfg, get_data_dir\n\ndef makeDCCinp(args):\n    # pretrained.mat or pretrained.h5 must be placed under the ../data/""db""/ directory. ""db"" stands for dataset\n    datadir = get_data_dir(args.db)\n    datafile = \'pretrained\'\n\n    if args.h5:\n        datafile = os.path.join(datadir, datafile+\'.h5\')\n    else:\n        datafile = os.path.join(datadir, datafile+\'.mat\')\n    assert os.path.exists(datafile), \'Training data not found at `{:s}`\'.format(datafile)\n\n    if args.h5:\n        import h5py\n        raw_data = h5py.File(datafile, \'r\')\n    else:\n        raw_data = sio.loadmat(datafile, mat_dtype=True)\n\n    data = raw_data[\'X\'][:].astype(np.float32)\n    Z = raw_data[\'Z\'][:].astype(np.float32)\n    # correct special case where Z is N x 1 and it gets loaded as 1 x N\n    if Z.shape[0] == 1:\n        Z = np.transpose(Z)\n\n    labels = np.squeeze(raw_data[\'gtlabels\'][:])\n    pairs = raw_data[\'w\'][:, :2].astype(int)\n\n    if args.h5:\n        raw_data.close()\n\n    print(\'\\n Loaded `{:s}` dataset for finetuning\'.format(args.db))\n\n    numpairs = pairs.shape[0]\n    numsamples = data.shape[0]\n\n    # Creating pairwise weights and individual sample sample for reconstruction loss term\n    R = csr_matrix((np.ones(numpairs, dtype=np.float32), (pairs[:, 0], pairs[:, 1])), shape=(numsamples, numsamples))\n    R = R + R.transpose()\n    nconn = np.squeeze(np.array(np.sum(R, 1)))\n    weights = np.average(nconn) / np.sqrt(nconn[pairs[:, 0]] * nconn[pairs[:, 1]])\n    pairs = np.hstack((pairs, np.atleast_2d(weights).transpose()))\n\n    return data, labels, pairs, Z, nconn\n\n\ndef computeHyperParams(pairs, Z):\n    numpairs = len(pairs)\n    numsamples = len(Z)\n    epsilon = np.linalg.norm(Z[pairs[:, 0].astype(int)] - Z[pairs[:, 1].astype(int)], axis=1)\n    epsilon = np.sort(epsilon)\n    # if largest is noise then just consider as noise\n    if epsilon[-1] / np.sqrt(cfg.DIM) < cfg.RCC.NOISE_THRESHOLD:\n        epsilon = np.asarray([cfg.RCC.NOISE_THRESHOLD])\n    else:\n        epsilon = epsilon[np.where(epsilon / np.sqrt(cfg.DIM) > cfg.RCC.NOISE_THRESHOLD)]\n\n    # threshold for finding connected components\n    robsamp = int(numpairs * cfg.RCC.MIN_RATIO_SAMPLES_DELTA)\n    _delta = np.average(epsilon[:robsamp])\n\n    robsamp = min(cfg.RCC.MAX_NUM_SAMPLES_DELTA, robsamp)\n    _delta2 = float(np.average(epsilon[:robsamp]) / 2)\n    _sigma2 = float(3 * (epsilon[-1] ** 2))\n\n    _delta1 = float(np.average(np.linalg.norm(Z - np.average(Z, axis=0)[np.newaxis, :], axis=1) ** 2))\n    _sigma1 = float(max(cfg.RCC.GNC_DATA_START_POINT, 16 * _delta1))\n\n    print(\'The endpoints are Delta1: {:.3f}, Delta2: {:.3f}\'.format(_delta1, _delta2))\n\n    lmdb = np.ones(numpairs, dtype=np.float32)\n    lmdb_data = np.ones(numsamples, dtype=np.float32)\n    _lambda = compute_lambda(pairs, Z, lmdb, lmdb_data)\n\n    return _sigma1, _sigma2, _lambda, _delta, _delta1, _delta2, lmdb, lmdb_data\n\n\ndef compute_lambda(pairs, Z, lmdb, lmdb_data):\n    numsamples = len(Z)\n\n    R = csr_matrix((lmdb * pairs[:,2], (pairs[:,0].astype(int), pairs[:,1].astype(int))), shape=(numsamples, numsamples))\n    R = R + R.transpose()\n\n    D = diags(np.squeeze(np.array(np.sum(R,1))), 0)\n    I = diags(lmdb_data, 0)\n\n    spndata = np.linalg.norm(I * Z, ord=2)\n    eiglmdbdata,_ = sparse.linalg.eigsh(I, k=1)\n    eigM,_ = sparse.linalg.eigsh(D - R, k=1)\n\n    _lambda = float(spndata / (eiglmdbdata + eigM))\n\n    return _lambda\n\n\ndef computeObj(U, pairs, _delta, gtlabels, numeval):\n    """""" This is similar to computeObj function in Matlab """"""\n    numsamples = len(U)\n    diff = np.linalg.norm(U[pairs[:, 0].astype(int)] - U[pairs[:, 1].astype(int)], axis=1)**2\n\n    # computing clustering measures\n    index1 = np.sqrt(diff) < _delta\n    index = np.where(index1)\n    adjacency = csr_matrix((np.ones(len(index[0])), (pairs[index[0], 0].astype(int), pairs[index[0], 1].astype(int))),\n                           shape=(numsamples, numsamples))\n    adjacency = adjacency + adjacency.transpose()\n    n_components, labels = connected_components(adjacency, directed=False)\n\n    index2 = labels[pairs[:, 0].astype(int)] == labels[pairs[:, 1].astype(int)]\n\n    ari, ami, nmi, acc = benchmarking(gtlabels[:numeval], labels[:numeval])\n\n    return index2, ari, ami, nmi, acc, n_components, labels\n\n\ndef benchmarking(gtlabels, labels):\n    # TODO: Please note that the AMI definition used in the paper differs from that in the sklearn python package.\n    # TODO: Please modify it accordingly.\n    numeval = len(gtlabels)\n    ari = metrics.adjusted_rand_score(gtlabels[:numeval], labels[:numeval])\n    ami = metrics.adjusted_mutual_info_score(gtlabels[:numeval], labels[:numeval])\n    nmi = metrics.normalized_mutual_info_score(gtlabels[:numeval], labels[:numeval])\n    acc = clustering_accuracy(gtlabels[:numeval], labels[:numeval])\n\n    return ari, ami, nmi, acc\n\n\ndef clustering_accuracy(gtlabels, labels):\n    cost_matrix = []\n    categories = np.unique(gtlabels)\n    nr = np.amax(labels) + 1\n    for i in np.arange(len(categories)):\n        cost_matrix.append(np.bincount(labels[gtlabels == categories[i]], minlength=nr))\n    cost_matrix = np.asarray(cost_matrix).T\n    row_ind, col_ind = linear_sum_assignment(np.max(cost_matrix) - cost_matrix)\n\n    return float(cost_matrix[row_ind, col_ind].sum()) / len(gtlabels)\n'"
pytorch/DCCLoss.py,8,"b""import torch\nimport torch.nn as nn\nimport numpy as np\n\nclass DCCWeightedELoss(nn.Module):\n    def __init__(self, size_average=True):\n        super(DCCWeightedELoss, self).__init__()\n        self.size_average = size_average\n\n    def forward(self, inputs, outputs, weights):\n        out = (inputs - outputs).view(len(inputs), -1)\n        out = torch.sum(weights * torch.norm(out, p=2, dim=1)**2)\n\n        assert np.isfinite(out.data.cpu().numpy()).all(), 'Nan found in data'\n\n        if self.size_average:\n            out = out / inputs.nelement()\n\n        return out\n\nclass DCCLoss(nn.Module):\n    def __init__(self, nsamples, ndim, initU, size_average=True):\n        super(DCCLoss, self).__init__()\n        self.dim = ndim\n        self.nsamples = nsamples\n        self.size_average = size_average\n        self.U = nn.Parameter(torch.Tensor(self.nsamples, self.dim))\n        self.reset_parameters(initU+1e-6*np.random.randn(*initU.shape).astype(np.float32))\n\n    def reset_parameters(self, initU):\n        assert np.isfinite(initU).all(), 'Nan found in initialization'\n        self.U.data = torch.from_numpy(initU)\n\n    def forward(self, enc_out, sampweights, pairweights, pairs, index, _sigma1, _sigma2, _lambda):\n        centroids = self.U[index]\n\n        # note that sigmas here are labelled mu in the paper\n        # data loss\n        # enc_out is Y, the original embedding without shift\n        out1 = torch.norm((enc_out - centroids).view(len(enc_out), -1), p=2, dim=1) ** 2\n        out11 = torch.sum(_sigma1 * sampweights * out1 / (_sigma1 + out1))\n\n        # pairwise loss\n        out2 = torch.norm((centroids[pairs[:, 0]] - centroids[pairs[:, 1]]).view(len(pairs), -1), p=2, dim=1) ** 2\n\n        out21 = _lambda * torch.sum(_sigma2 * pairweights * out2 / (_sigma2 + out2))\n\n        out = out11 + out21\n\n        if self.size_average:\n            out = out / enc_out.nelement()\n\n        return out"""
pytorch/SDAE.py,3,"b""import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\n\n# The model definition for Stacked Denoising AE.\n# This model is used during the pretraining stage.\nclass SDAE(nn.Module):\n    def __init__(self, dim, dropout=0.2, slope=0.0):\n        super(SDAE, self).__init__()\n        self.in_dim = dim[0]\n        self.nlayers = len(dim)-1\n        self.reluslope = slope\n        self.enc, self.dec = [], []\n        for i in range(self.nlayers):\n            self.enc.append(nn.Linear(dim[i], dim[i+1]))\n            setattr(self, 'enc_{}'.format(i), self.enc[-1])\n            self.dec.append(nn.Linear(dim[i+1], dim[i]))\n            setattr(self, 'dec_{}'.format(i), self.dec[-1])\n        self.base = []\n        for i in range(self.nlayers):\n            self.base.append(nn.Sequential(*self.enc[:i]))\n        self.dropmodule1 = nn.Dropout(p=dropout)\n        self.dropmodule2 = nn.Dropout(p=dropout)\n        self.loss = nn.MSELoss(size_average=True)\n\n        # initialization\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                init.normal(m.weight, std=1e-2)\n                if m.bias.data is not None:\n                    init.constant(m.bias, 0)\n\n    def forward(self,x,index):\n        inp = x.view(-1, self.in_dim)\n        encoded = inp\n        for i, encoder in enumerate(self.enc):\n            if i < index:\n                encoded = encoder(encoded)\n                if i < self.nlayers-1:\n                    encoded = F.leaky_relu(encoded, negative_slope=self.reluslope)\n            if i == index:\n                inp = encoded\n                out = encoded\n                if index:\n                    out = self.dropmodule1(out)\n                out = encoder(out)\n        if index < self.nlayers-1:\n            out = F.leaky_relu(out, negative_slope=self.reluslope)\n            out = self.dropmodule2(out)\n        if index >= self.nlayers:\n            out = encoded\n        for i, decoder in reversed(list(enumerate(self.dec))):\n            if index >= self.nlayers:\n                out = decoder(out)\n                if i:\n                    out = F.leaky_relu(out, negative_slope=self.reluslope)\n            if i == index:\n                out = decoder(out)\n                if index:\n                    out = F.leaky_relu(out, negative_slope=self.reluslope)\n        out = self.loss(out, inp)\n        return out\n"""
pytorch/__init__.py,0,b''
pytorch/config.py,0,"b'from __future__ import print_function\nimport os.path as osp\nimport os\nfrom easydict import EasyDict as edict\n\n__C = edict()\n\ncfg = __C\n\n#######\n# OPTIONS FROM RCC CODE\n#######\n__C.RCC = edict()\n\n__C.RCC.NOISE_THRESHOLD = 0.01\n\n__C.RCC.MAX_NUM_SAMPLES_DELTA = 250\n\n__C.RCC.MIN_RATIO_SAMPLES_DELTA = 0.01\n\n__C.RCC.GNC_DATA_START_POINT = 132*16\n\n#######\n# MISC OPTIONS\n#######\n\n# For reproducibility\n__C.RNG_SEED = 50\n\n# Root directory\n__C.ROOT_DIR = osp.abspath(osp.join(osp.dirname(__file__),\'..\'))\n\n# size of the dataset\n__C.SAMPLES = 70000\n\n# embedding dimension\n__C.DIM = 10\n\n# Number of pairs per batch\n__C.PAIRS_PER_BATCH = 128\n\n# Fraction of ""change in label assignment of pairs"" to be considered for stopping criterion - 1% of pairs\n__C.STOPPING_CRITERION = 0.001\n\n\ndef get_data_dir(db):\n    """"""\n    :param db:\n    :return: path to data directory\n    """"""\n    path = osp.abspath(osp.join(__C.ROOT_DIR, \'data\', db))\n    return path\n\ndef get_output_dir(db):\n    """"""\n    :param db:\n    :return: path to data directory\n    """"""\n    path = osp.abspath(osp.join(__C.ROOT_DIR, \'data\', db, \'results\'))\n    return path\n\ndef remove_files_in_dir(dir):\n    if not osp.isdir(dir):\n        return\n    for file in os.listdir(dir):\n        file_path = os.path.join(dir, file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n        except Exception as e:\n            print(e)\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count'"
pytorch/convSDAE.py,3,"b""import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\n\n# The model definition for training convolutional Stacked Denoising AE.\n# This model is used during the pretraining stage.\nclass convSDAE(nn.Module):\n    def __init__(self, dim, output_padding, numpen, dropout=0.2, slope=0.0):\n        super(convSDAE, self).__init__()\n        self.in_dim = dim[0]\n        self.nlayers = len(dim)-1\n        self.reluslope = slope\n        self.numpen = numpen\n        self.enc, self.dec = [], []\n        self.benc, self.bdec = [], []\n        for i in range(self.nlayers):\n            if i == self.nlayers - 1:\n                self.enc.append(nn.Linear(dim[i]*numpen*numpen, dim[i+1]))\n                self.benc.append(nn.BatchNorm2d(dim[i + 1]))\n                self.dec.append(nn.ConvTranspose2d(dim[i + 1], dim[i], kernel_size=numpen, stride=1))\n                self.bdec.append(nn.BatchNorm2d(dim[i]))\n            elif i == 0:\n                self.enc.append(nn.Conv2d(dim[i], dim[i + 1], kernel_size=4, stride=2, padding=1))\n                self.benc.append(nn.BatchNorm2d(dim[i + 1]))\n                self.dec.append(nn.ConvTranspose2d(dim[i+1], dim[i], kernel_size=4, stride=2, padding=1,\n                                                   output_padding=output_padding[i]))\n                self.bdec.append(nn.BatchNorm2d(dim[i]))\n            else:\n                self.enc.append(nn.Conv2d(dim[i], dim[i + 1], kernel_size=5, stride=2, padding=2))\n                self.benc.append(nn.BatchNorm2d(dim[i+1]))\n                self.dec.append(nn.ConvTranspose2d(dim[i+1], dim[i], kernel_size=5, stride=2, padding=2,\n                                                   output_padding=output_padding[i]))\n                self.bdec.append(nn.BatchNorm2d(dim[i]))\n            setattr(self, 'enc_{}'.format(i), self.enc[-1])\n            setattr(self, 'benc_{}'.format(i), self.benc[-1])\n            setattr(self, 'dec_{}'.format(i), self.dec[-1])\n            setattr(self, 'bdec_{}'.format(i), self.bdec[-1])\n        self.base = []\n        self.bbase = []\n        for i in range(self.nlayers):\n            self.base.append(nn.Sequential(*self.enc[:i]))\n            self.bbase.append(nn.Sequential(*self.benc[:i]))\n        self.dropmodule1 = nn.Dropout(p=dropout)\n        self.dropmodule2 = nn.Dropout(p=dropout)\n        self.loss = nn.MSELoss(size_average=True)\n\n        # initialization\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                init.normal(m.weight, std=1e-2)\n                if m.bias.data is not None:\n                    init.constant(m.bias, 0)\n            elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                init.kaiming_normal(m.weight, mode='fan_out')\n                if m.bias.data is not None:\n                    init.constant(m.bias, 0)\n\n    def forward(self,x,index):\n        inp = x\n        encoded = x\n        for i, (encoder,bencoder) in enumerate(zip(self.enc, self.benc)):\n            if i == self.nlayers-1:\n                encoded = encoded.view(encoded.size(0), -1)\n            if i < index:\n                encoded = encoder(encoded)\n                # this is to avoid batch norm during last layer of encoder\n                if i < self.nlayers-1:\n                    encoded = bencoder(encoded)\n                    encoded = F.leaky_relu(encoded, negative_slope=self.reluslope)\n            if i == index:\n                inp = encoded\n                out = encoded\n                if index:\n                    out = self.dropmodule1(out)\n                out = encoder(out)\n                if i < self.nlayers-1:\n                    out = bencoder(out)\n        if index < self.nlayers-1:\n            out = F.leaky_relu(out, negative_slope=self.reluslope)\n            out = self.dropmodule2(out)\n        if index >= self.nlayers:\n            out = encoded\n        for i, (decoder, bdecoder) in reversed(list(enumerate(zip(self.dec, self.bdec)))):\n            if index >= self.nlayers-1 and i == self.nlayers-1:\n                out = out.view(out.size(0),-1,1,1)\n            if index >= self.nlayers:\n                out = decoder(out)\n                if i:\n                    out = bdecoder(out)\n                    out = F.leaky_relu(out, negative_slope=self.reluslope)\n            if i == index:\n                out = decoder(out)\n                if index:\n                    out = bdecoder(out)\n                    out = F.leaky_relu(out, negative_slope=self.reluslope)\n        out = self.loss(out, inp)\n        return out\n\n"""
pytorch/copyGraph.py,0,"b""from __future__ import print_function\nimport os\nimport h5py\nimport numpy as np\nimport argparse\nimport scipy.io as sio\nfrom config import get_data_dir\n\n# python 3 compatibility\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\n\n# Note that just like in RCC & RCC-DR, the graph is built on original data.\n# Once the features are extracted from the pretrained SDAE,\n# they are merged along with the mkNN graph data into a single file using this module.\nparser = argparse.ArgumentParser(\n    description='This module is used to merge graph and extracted features into single file')\nparser.add_argument('--data', dest='db', type=str, default='mnist', help='name of the dataset')\nparser.add_argument('--graph', dest='g', help='path to the graph file', default=None, type=str)\nparser.add_argument('--features', dest='feat', help='path to the feature file', default=None, type=str)\nparser.add_argument('--out', dest='out', help='path to the output file', default=None, type=str)\nparser.add_argument('--h5', dest='h5', action='store_true', help='to store as h5py file')\n\n\ndef main(args):\n    datadir = get_data_dir(args.db)\n\n    featurefile = os.path.join(datadir, args.feat)\n    graphfile = os.path.join(datadir, args.g)\n    outputfile = os.path.join(datadir, args.out)\n    if os.path.isfile(featurefile) and os.path.isfile(graphfile):\n\n        if args.h5:\n            data0 = h5py.File(featurefile, 'r')\n            data1 = h5py.File(graphfile, 'r')\n            data2 = h5py.File(outputfile + '.h5', 'w')\n        else:\n            fo = open(featurefile, 'rb')\n            data0 = pickle.load(fo)\n            data1 = sio.loadmat(graphfile)\n            fo.close()\n\n        x0 = data0['data'][:].astype(np.float32).reshape((len(data0['labels'][:]), -1))\n        x1 = data1['X'][:].astype(np.float32).reshape((len(data1['gtlabels'].T), -1))\n\n        a, b = np.where(x0 - x1)\n        assert not a.size\n\n        joined_data = {'gtlabels': data0['labels'][:], 'X': data0['data'][:].astype(np.float32),\n                       'Z': data0['Z'][:].astype(np.float32),\n                       'w': data1['w'][:].astype(np.float32)}\n\n        if args.h5:\n            data2.create_dataset('gtlabels', data=data0['labels'][:])\n            data2.create_dataset('X', data=data0['data'][:].astype(np.float32))\n            data2.create_dataset('Z', data=data0['Z'][:].astype(np.float32))\n            data2.create_dataset('w', data=data1['w'][:].astype(np.float32))\n            data0.close()\n            data1.close()\n            data2.close()\n        else:\n            sio.savemat(outputfile + '.mat', joined_data)\n        return joined_data\n    else:\n        print('one or both the files not found')\n        raise FileNotFoundError\n\n\nif __name__ == '__main__':\n    args = parser.parse_args()\n    main(args)\n"""
pytorch/custom_data.py,2,"b'import os.path as osp\nimport torch.utils.data as data\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nimport scipy.io as sio\nimport numpy as np\nimport h5py\n\nclass DCCPT_data(data.Dataset):\n    """"""Custom dataset loader for Pretraining SDAE""""""\n\n    def __init__(self, root, train=True, h5=False, labeled=False):\n        self.root_dir = root\n        self.train = train\n\n        if self.train:\n            if h5:\n                data = h5py.File(osp.join(root, \'traindata.h5\'), \'r\')\n            else:\n                data = sio.loadmat(osp.join(root, \'traindata.mat\'), mat_dtype=True)\n            self.train_data = data[\'X\'][:].astype(np.float32)\n            self.train_labels = np.squeeze(data[\'Y\'][:])\n        else:\n            if labeled:\n                filename = \'labeldata\'\n            else:\n                filename = \'testdata\'\n            if h5:\n                data = h5py.File(osp.join(root, filename+\'.h5\'), \'r\')\n            else:\n                data = sio.loadmat(osp.join(root, filename+\'.mat\'), mat_dtype=True)\n            self.test_data = data[\'X\'][:].astype(np.float32)\n            self.test_labels = np.squeeze(data[\'Y\'][:])\n\n    def __len__(self):\n        if self.train:\n            return len(self.train_labels)\n        else:\n            return len(self.test_labels)\n\n    def __getitem__(self, item):\n        if self.train:\n            data, target = self.train_data[item], self.train_labels[item]\n        else:\n            data, target = self.test_data[item], self.test_labels[item]\n\n        return data, target\n\nclass DCCFT_data(data.Dataset):\n    """""" Custom dataset loader for the finetuning stage of DCC""""""\n\n    def __init__(self, pairs, data, samp_weights):\n        self._pairs = pairs\n        self._data = data\n        self._sampweight = samp_weights\n\n    def __len__(self):\n        return len(self._pairs)\n\n    def __getitem__(self, inds):\n        current_pairs = self._pairs[inds].copy()\n        data_ind, mapping = np.unique(current_pairs[:, :2].astype(int), return_inverse=True)\n        current_pairs[:, :2] = mapping.reshape((len(inds), 2))\n\n        # preparing weights for pairs as well as sample weight for weighted reconstruction error\n        weights_blob = current_pairs[:, 2].astype(np.float32)\n\n        sampweight_blob = np.zeros(len(data_ind), dtype=np.float32)\n        sampweight_blob[...] = np.bincount(mapping) / self._sampweight[data_ind]\n\n        # preparing data\n        if len(self._data.shape) > 2:\n            data_blob = np.zeros((len(data_ind), self._data.shape[1], self._data.shape[2], self._data.shape[3]), dtype=np.float32)\n        else:\n            data_blob = np.zeros((len(data_ind), self._data.shape[1]), dtype=np.float32)\n        data_blob[...] = self._data[data_ind]\n\n        assert np.isfinite(data_blob).all(), \'Nan found in data\'\n        assert np.isfinite(weights_blob).all(), \'Nan found in weights\'\n        assert np.isfinite(sampweight_blob).all(), \'Nan found in sample weights\'\n\n        return data_blob, weights_blob, sampweight_blob, current_pairs[:, :2].astype(int), data_ind\n\n\nclass DCCSampler(object):\n    """"""Custom Sampler is required. This sampler prepares batch by passing list of\n    data indices instead of running over individual index as in pytorch sampler""""""\n    def __init__(self, pairs, shuffle=False, batch_size=1, drop_last=False):\n        if shuffle:\n            self.sampler = RandomSampler(pairs)\n        else:\n            self.sampler = SequentialSampler(pairs)\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n\n    def __iter__(self):\n        batch = []\n        for idx in self.sampler:\n            batch.append(idx)\n            if len(batch) == self.batch_size:\n                batch = [batch]\n                yield batch\n                batch = []\n        if len(batch) > 0 and not self.drop_last:\n            batch = [batch]\n            yield batch\n\n    def __len__(self):\n        if self.drop_last:\n            return len(self.sampler) // self.batch_size\n        else:\n            return (len(self.sampler) + self.batch_size - 1) // self.batch_size'"
pytorch/data_params.py,0,"b'from easydict import EasyDict as edict\nfrom extractSDAE import extractSDAE\nfrom extractconvSDAE import extractconvSDAE\nfrom SDAE import SDAE\nfrom convSDAE import convSDAE\n\neasy = edict()\neasy.name = \'easy\'\neasy.dim = [2]\n\n\n# TODO port other dataset\'s parameters to here\ndef extract_convsdae_mnist(slope=0.0):\n    return extractconvSDAE(dim=[1, 50, 50, 50, 10], output_padding=[0, 1, 0], numpen=4, slope=slope)\n\n\ndef extract_convsdae_coil100(slope=0.0):\n    return extractconvSDAE(dim=[3, 50, 50, 50, 50, 50, 10], output_padding=[0, 1, 1, 1, 1], numpen=4, slope=slope)\n\n\ndef extract_convsdae_ytf(slope=0.0):\n    return extractconvSDAE(dim=[3, 50, 50, 50, 50, 10], output_padding=[1, 0, 1, 0], numpen=4, slope=slope)\n\n\ndef extract_convsdae_yale(slope=0.0):\n    return extractconvSDAE(dim=[1, 50, 50, 50, 50, 50, 10], output_padding=[(0, 0), (1, 1), (1, 1), (0, 1), (0, 1)],\n                           numpen=6,\n                           slope=slope)\n\n\ndef extract_sdae_mnist(slope=0.0, dim=10):\n    return extractSDAE(dim=[784, 500, 500, 2000, dim], slope=slope)\n\n\ndef extract_sdae_reuters(slope=0.0, dim=10):\n    return extractSDAE(dim=[2000, 500, 500, 2000, dim], slope=slope)\n\n\ndef extract_sdae_ytf(slope=0.0, dim=10):\n    return extractSDAE(dim=[9075, 500, 500, 2000, dim], slope=slope)\n\n\ndef extract_sdae_coil100(slope=0.0, dim=10):\n    return extractSDAE(dim=[49152, 500, 500, 2000, dim], slope=slope)\n\n\ndef extract_sdae_yale(slope=0.0, dim=10):\n    return extractSDAE(dim=[32256, 500, 500, 2000, dim], slope=slope)\n\n\ndef extract_sdae_easy(slope=0.0, dim=1):\n    return extractSDAE(dim=easy.dim + [dim], slope=slope)\n\n\ndef sdae_mnist(dropout=0.2, slope=0.0, dim=10):\n    return SDAE(dim=[784, 500, 500, 2000, dim], dropout=dropout, slope=slope)\n\n\ndef sdae_reuters(dropout=0.2, slope=0.0, dim=10):\n    return SDAE(dim=[2000, 500, 500, 2000, dim], dropout=dropout, slope=slope)\n\n\ndef sdae_ytf(dropout=0.2, slope=0.0, dim=10):\n    return SDAE(dim=[9075, 500, 500, 2000, dim], dropout=dropout, slope=slope)\n\n\ndef sdae_coil100(dropout=0.2, slope=0.0, dim=10):\n    return SDAE(dim=[49152, 500, 500, 2000, dim], dropout=dropout, slope=slope)\n\n\ndef sdae_yale(dropout=0.2, slope=0.0, dim=10):\n    return SDAE(dim=[32256, 500, 500, 2000, dim], dropout=dropout, slope=slope)\n\n\ndef sdae_easy(dropout=0.2, slope=0.0, dim=1):\n    return SDAE(dim=easy.dim + [dim], dropout=dropout, slope=slope)\n\n\ndef convsdae_mnist(dropout=0.2, slope=0.0):\n    return convSDAE(dim=[1, 50, 50, 50, 10], output_padding=[0, 1, 0], numpen=4, dropout=dropout, slope=slope)\n\n\ndef convsdae_coil100(dropout=0.2, slope=0.0):\n    return convSDAE(dim=[3, 50, 50, 50, 50, 50, 10], output_padding=[0, 1, 1, 1, 1], numpen=4, dropout=dropout,\n                    slope=slope)\n\n\ndef convsdae_ytf(dropout=0.2, slope=0.0):\n    return convSDAE(dim=[3, 50, 50, 50, 50, 10], output_padding=[1, 0, 1, 0], numpen=4, dropout=dropout, slope=slope)\n\n\ndef convsdae_yale(dropout=0.2, slope=0.0):\n    return convSDAE(dim=[1, 50, 50, 50, 50, 50, 10], output_padding=[(0, 0), (1, 1), (1, 1), (0, 1), (0, 1)], numpen=6,\n                    dropout=dropout, slope=slope)\n\n\ndef load_predefined_net(args, params):\n    if args.db == \'mnist\':\n        net = sdae_mnist(dropout=params[\'dropout\'], slope=params[\'reluslope\'], dim=args.dim)\n    elif args.db == \'reuters\' or args.db == \'rcv1\':\n        net = sdae_reuters(dropout=params[\'dropout\'], slope=params[\'reluslope\'], dim=args.dim)\n    elif args.db == \'ytf\':\n        net = sdae_ytf(dropout=params[\'dropout\'], slope=params[\'reluslope\'], dim=args.dim)\n    elif args.db == \'coil100\':\n        net = sdae_coil100(dropout=params[\'dropout\'], slope=params[\'reluslope\'], dim=args.dim)\n    elif args.db == \'yale\':\n        net = sdae_yale(dropout=params[\'dropout\'], slope=params[\'reluslope\'], dim=args.dim)\n    elif args.db == \'cmnist\':\n        net = convsdae_mnist(dropout=params[\'dropout\'], slope=params[\'reluslope\'])\n    elif args.db == \'ccoil100\':\n        net = convsdae_coil100(dropout=params[\'dropout\'], slope=params[\'reluslope\'])\n    elif args.db == \'cytf\':\n        net = convsdae_ytf(dropout=params[\'dropout\'], slope=params[\'reluslope\'])\n    elif args.db == \'cyale\':\n        net = convsdae_yale(dropout=params[\'dropout\'], slope=params[\'reluslope\'])\n    elif args.db == \'easy\':\n        net = sdae_easy(dropout=params[\'dropout\'], slope=params[\'reluslope\'], dim=args.dim)\n    else:\n        raise ValueError(""Unexpected database %s"" % args.db)\n\n    return net\n\n\ndef load_predefined_extract_net(args):\n    reluslope = 0.0\n\n    if args.db == \'mnist\':\n        net = extract_sdae_mnist(slope=reluslope, dim=args.dim)\n    elif args.db == \'reuters\' or args.db == \'rcv1\':\n        net = extract_sdae_reuters(slope=reluslope, dim=args.dim)\n    elif args.db == \'ytf\':\n        net = extract_sdae_ytf(slope=reluslope, dim=args.dim)\n    elif args.db == \'coil100\':\n        net = extract_sdae_coil100(slope=reluslope, dim=args.dim)\n    elif args.db == \'yale\':\n        net = extract_sdae_yale(slope=reluslope, dim=args.dim)\n    elif args.db == \'cmnist\':\n        net = extract_convsdae_mnist(slope=reluslope)\n    elif args.db == \'ccoil100\':\n        net = extract_convsdae_coil100(slope=reluslope)\n    elif args.db == \'cytf\':\n        net = extract_convsdae_ytf(slope=reluslope)\n    elif args.db == \'cyale\':\n        net = extract_convsdae_yale(slope=reluslope)\n    elif args.db == easy.name:\n        net = extract_sdae_easy(slope=reluslope, dim=args.dim)\n    else:\n        raise ValueError(""Unexpected database %s"" % args.db)\n\n    return net\n'"
pytorch/easy_example.py,1,"b'""""""Example for doing all steps in code only (other examples require calling different files separately)""""""\nimport torch.nn as nn\n\nfrom config import cfg, get_data_dir\nfrom easydict import EasyDict as edict\nfrom edgeConstruction import compressed_data\nimport matplotlib.pyplot as plt\nimport data_params as dp\nimport make_data\nimport pretraining\nimport extract_feature\nimport copyGraph\nimport DCC\n\n\nclass IdentityNet(nn.Module):\n    """"""Substitute for the autoencoder for visualization and debugging just the clustering part""""""\n    def __init__(self):\n        super(IdentityNet, self).__init__()\n\n    def forward(self, x):\n        # internal encoding is x and output is also just x\n        return x, x\n\n\ndatadir = get_data_dir(dp.easy.name)\nN = 600\n\n# first create the data\nX, labels = make_data.make_easy_visual_data(datadir, N)\n\n# visualize data\n# we know there are 3 classes\nfor c in range(3):\n    x = X[labels == c, :]\n    plt.scatter(x[:, 0], x[:, 1], label=str(c))\nplt.legend()\nplt.show()\n\n# then construct mkNN graph\nk = 50\ncompressed_data(dp.easy.name, N, k, preprocess=\'none\', algo=\'knn\', isPCA=None, format=\'mat\')\n\n# then pretrain to get features\nargs = edict()\nargs.db = dp.easy.name\nargs.niter = 500\nargs.step = 300\nargs.lr = 0.001\n\n# if we need to resume for faster debugging/results\nargs.resume = False\nargs.level = None\n\nargs.batchsize = 300\nargs.ngpu = 1\nargs.deviceID = 0\nargs.tensorboard = True\nargs.h5 = False\nargs.id = 2\nargs.dim = 2\nargs.manualSeed = cfg.RNG_SEED\nargs.clean_log = True\n\n# if we comment out the next pretraining step and the identity network, use the latest checkpoint\nindex = len(dp.easy.dim) - 1\nnet = None\n# if we comment out the next pretraining step we use the identity network\nnet = IdentityNet()\nindex, net = pretraining.main(args)\n\n# extract pretrained features\nargs.feat = \'pretrained\'\nargs.torchmodel = \'checkpoint_{}.pth.tar\'.format(index)\nextract_feature.main(args, net=net)\n\n# merge the features and mkNN graph\nargs.g = \'pretrained.mat\'\nargs.out = \'pretrained\'\nargs.feat = \'pretrained.pkl\'\ncopyGraph.main(args)\n\n# actually do DCC\nargs.batchsize = cfg.PAIRS_PER_BATCH\nargs.nepoch = 500\nargs.M = 20\nargs.lr = 0.001\nout = DCC.main(args, net=net)\n'"
pytorch/edgeConstruction.py,0,"b'from time import time\nimport os\nimport numpy as np\nimport scipy.io as sio\nimport argparse\nimport random\n\nfrom config import cfg, get_data_dir, get_output_dir\n\nfrom sklearn.preprocessing import scale as skscale\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import normalize\nfrom sklearn.decomposition import PCA\n\n\ndef load_data(filename, n_samples):\n    import cPickle\n    fo = open(filename, \'rb\')\n    data = cPickle.load(fo)\n    fo.close()\n    labels = data[\'labels\'][0:n_samples]\n    labels = np.squeeze(labels)\n    features = data[\'data\'][0:n_samples]\n    features = features.astype(np.float32, copy=False)\n    features = features.reshape((n_samples, -1))\n    return labels, features\n\n\ndef load_matdata(filename, n_samples):\n    # TODO switch other loading to also use new X,Y convention instead of labels,data?\n    data = sio.loadmat(filename)\n    labels = data[\'Y\'][0:n_samples]\n    labels = np.squeeze(labels)\n    features = data[\'X\'][0:n_samples]\n    features = features.astype(np.float32, copy=False)\n    # TODO figure out why we need to reshape this...\n    # features = features.reshape((n_samples, -1))\n    return labels, features\n\n\ndef load_data_h5py(filename, n_samples):\n    import h5py\n    data = h5py.File(filename, \'r\')\n    labels = data[\'labels\'][0:n_samples]\n    labels = np.squeeze(labels)\n    features = data[\'data\'][0:n_samples]\n    features = features.astype(np.float32, copy=False)\n    features = features.reshape((n_samples, -1))\n    data.close()\n    return labels, features\n\n\ndef load_train_and_validation(loader, datadir, n_samples):\n    td = os.path.join(datadir, \'traindata.mat\')\n    # TODO n_samples don\'t really make sense as a single parameter anymore since data set is split in 2\n    lt, ft = loader(td, n_samples)\n\n    tv = os.path.join(datadir, \'testdata.mat\')\n    lv, fv = loader(tv, n_samples)\n\n    return np.concatenate((lt, lv)), np.concatenate((ft, fv))\n\n\ndef feature_transformation(features, preprocessing=\'normalization\'):\n    n_samples, n_features = features.shape\n    if preprocessing == \'scale\':\n        features = skscale(features, copy=False)\n    elif preprocessing == \'minmax\':\n        minmax_scale = MinMaxScaler().fit(features)\n        features = minmax_scale.transform(features)\n    elif preprocessing == \'normalization\':\n        features = np.sqrt(n_features) * normalize(features, copy=False)\n    else:\n        print(\'No preprocessing is applied\')\n    return features\n\n\ndef kNN(X, k, measure=\'euclidean\'):\n    """"""\n    Construct pairwise weights by finding the k nearest neighbors to each point\n    and assigning a Gaussian-based distance.\n\n    Parameters\n    ----------\n    X : [n_samples, n_dim] array\n    k : int\n        number of neighbors for each sample in X\n    """"""\n    from scipy.spatial import distance\n\n    weights = []\n    w = distance.cdist(X, X, measure)\n    y = np.argsort(w, axis=1)\n\n    for i, x in enumerate(X):\n        distances, indices = w[i, y[i, 1:k + 1]], y[i, 1:k + 1]\n        for (d, j) in zip(distances, indices):\n            if i < j:\n                weights.append((i, j, d * d))\n            else:\n                weights.append((j, i, d * d))\n    weights = sorted(weights, key=lambda r: (r[0], r[1]))\n    return np.unique(np.asarray(weights), axis=0)\n\n\ndef mkNN(X, k, measure=\'euclidean\'):\n    """"""\n    Construct mutual_kNN for large scale dataset\n\n    If j is one of i\'s closest neighbors and i is also one of j\'s closest members,\n    the edge will appear once with (i,j) where i < j.\n\n    Parameters\n    ----------\n    X : [n_samples, n_dim] array\n    k : int\n      number of neighbors for each sample in X\n    """"""\n    from scipy.spatial import distance\n    from scipy.sparse import csr_matrix, triu, find\n    from scipy.sparse.csgraph import minimum_spanning_tree\n\n    samples = X.shape[0]\n    batchsize = 10000\n    b = np.arange(k + 1)\n    b = tuple(b[1:].ravel())\n\n    z = np.zeros((samples, k))\n    weigh = np.zeros_like(z)\n\n    # This loop speeds up the computation by operating in batches\n    # This can be parallelized to further utilize CPU/GPU resource\n    for x in np.arange(0, samples, batchsize):\n        start = x\n        end = min(x + batchsize, samples)\n\n        w = distance.cdist(X[start:end], X, measure)\n\n        y = np.argpartition(w, b, axis=1)\n\n        z[start:end, :] = y[:, 1:k + 1]\n        weigh[start:end, :] = np.reshape(w[tuple(np.repeat(np.arange(end - start), k)), tuple(y[:, 1:k + 1].ravel())],\n                                         (end - start, k))\n        del (w)\n\n    ind = np.repeat(np.arange(samples), k)\n\n    P = csr_matrix((np.ones((samples * k)), (ind.ravel(), z.ravel())), shape=(samples, samples))\n    Q = csr_matrix((weigh.ravel(), (ind.ravel(), z.ravel())), shape=(samples, samples))\n\n    Tcsr = minimum_spanning_tree(Q)\n    P = P.minimum(P.transpose()) + Tcsr.maximum(Tcsr.transpose())\n    P = triu(P, k=1)\n\n    return np.asarray(find(P)).T\n\n\ndef compressed_data(dataset, n_samples, k, preprocess=None, algo=\'mknn\', isPCA=None, format=\'mat\'):\n    datadir = get_data_dir(dataset)\n    if format == \'pkl\':\n        labels, features = load_train_and_validation(load_data, datadir, n_samples)\n    elif format == \'h5\':\n        labels, features = load_train_and_validation(load_data_h5py, datadir, n_samples)\n    else:\n        labels, features = load_train_and_validation(load_matdata, datadir, n_samples)\n\n    features = feature_transformation(features, preprocessing=preprocess)\n\n    # PCA is computed for Text dataset. Please refer RCC paper for exact details.\n    features1 = features.copy()\n    if isPCA is not None:\n        pca = PCA(n_components=isPCA, svd_solver=\'full\').fit(features)\n        features1 = pca.transform(features)\n\n    t0 = time()\n\n    if algo == \'knn\':\n        weights = kNN(features1, k=k, measure=\'euclidean\')\n    else:\n        weights = mkNN(features1, k=k, measure=\'cosine\')\n\n    print(\'The time taken for edge set computation is {}\'.format(time() - t0))\n\n    filepath = os.path.join(datadir, \'pretrained\')\n    if format == \'h5\':\n        import h5py\n        fo = h5py.File(filepath + \'.h5\', \'w\')\n        fo.create_dataset(\'X\', data=features)\n        fo.create_dataset(\'w\', data=weights[:, :2])\n        fo.create_dataset(\'gtlabels\', data=labels)\n        fo.close()\n    else:\n        sio.savemat(filepath + \'.mat\', mdict={\'X\': features, \'w\': weights[:, :2], \'gtlabels\': labels})\n\n\ndef parse_args():\n    """""" Parse input arguments """"""\n    parser = argparse.ArgumentParser(description=\'Feature extraction for RCC algorithm\')\n\n    parser.add_argument(\'--dataset\', default=None, type=str,\n                        help=\'The entered dataset file must be in the Data folder\')\n    parser.add_argument(\'--prep\', dest=\'prep\', default=\'none\', type=str,\n                        help=\'preprocessing of data: scale,minmax,normalization,none\')\n    parser.add_argument(\'--algo\', dest=\'algo\', default=\'mknn\', type=str,\n                        help=\'Algorithm to use: knn,mknn\')\n    parser.add_argument(\'--k\', dest=\'k\', default=10, type=int,\n                        help=\'Number of nearest neighbor to consider\')\n    parser.add_argument(\'--pca\', dest=\'pca\', default=None, type=int,\n                        help=\'Dimension of PCA processing before kNN graph construction\')\n    parser.add_argument(\'--samples\', dest=\'nsamples\', default=0, type=int,\n                        help=\'total samples to consider\')\n    parser.add_argument(\'--format\', choices=[\'mat\', \'pkl\', \'h5\'], default=\'mat\', help=\'Dataset format\')\n\n    args = parser.parse_args()\n    return args\n\n\nif __name__ == \'__main__\':\n    """"""\n   -----------------------------\n   Dataset\t|samples| dimension\n   -----------------------------\n   Mnist\t|70000\t| [28,28,1]\n   YaleB\t|2414\t| [168,192,1]\n   Coil100\t|7200\t| [128,128,3]\n   YTF  \t|10056\t| [55,55,3]\n   Reuters\t|9082\t| 2000\n   RCV1\t\t|10000\t| 2000 \n   -----------------------------   \n   """"""\n\n    random.seed(50)\n\n    args = parse_args()\n    print(\'Called with args:\')\n    print(args)\n\n    # storing compressed data\n    compressed_data(args.dataset, args.nsamples, args.k, preprocess=args.prep, algo=args.algo, isPCA=args.pca,\n                    format=args.format)\n'"
pytorch/extractSDAE.py,3,"b""import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\n\n# This class is similar to SDAE code.\n# This model is initiated when SDAE is needed for training without Dropout modules i.e., during DCC.\nclass extractSDAE(nn.Module):\n    def __init__(self, dim, slope=0.0):\n        super(extractSDAE, self).__init__()\n        self.in_dim = dim[0]\n        self.nlayers = len(dim)-1\n        self.reluslope = slope\n        self.enc, self.dec = [], []\n        for i in range(self.nlayers):\n            self.enc.append(nn.Linear(dim[i], dim[i+1]))\n            setattr(self, 'enc_{}'.format(i), self.enc[-1])\n            self.dec.append(nn.Linear(dim[i+1], dim[i]))\n            setattr(self, 'dec_{}'.format(i), self.dec[-1])\n        self.base = []\n        for i in range(self.nlayers):\n            self.base.append(nn.Sequential(*self.enc[:i]))\n\n        # initialization\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                init.normal(m.weight, std=1e-2)\n                if m.bias.data is not None:\n                    init.constant(m.bias, 0)\n\n    def forward(self,x):\n        inp = x.view(-1, self.in_dim)\n        encoded = inp\n        for i, encoder in enumerate(self.enc):\n            encoded = encoder(encoded)\n            if i < self.nlayers-1:\n                encoded = F.leaky_relu(encoded, negative_slope=self.reluslope)\n        out = encoded\n        for i, decoder in reversed(list(enumerate(self.dec))):\n            out = decoder(out)\n            if i:\n                out = F.leaky_relu(out, negative_slope=self.reluslope)\n        return encoded, out\n\n"""
pytorch/extract_feature.py,9,"b'from __future__ import print_function\nimport os\nimport random\nimport numpy as np\nimport argparse\nfrom config import cfg, get_data_dir, get_output_dir\nimport data_params as dp\n\n# python 3 compatibility\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\n\nfrom custom_data import DCCPT_data\n\n# Parse all the input argument\nparser = argparse.ArgumentParser(description=\'Module for extracting features from pretrained SDAE\')\nparser.add_argument(\'--manualSeed\', default=cfg.RNG_SEED, type=int, help=\'manual seed\')\nparser.add_argument(\'--data\', dest=\'db\', type=str, default=\'mnist\', help=\'name of the dataset\')\nparser.add_argument(\'--net\', dest=\'torchmodel\', help=\'path to the weights file\', default=None, type=str)\nparser.add_argument(\'--features\', dest=\'feat\', help=\'path to the feature file\', default=None, type=str)\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'number of GPUs to use\')\nparser.add_argument(\'--deviceID\', type=int, help=\'deviceID\', default=0)\nparser.add_argument(\'--dim\', type=int, help=\'dimension\', default=10)\nparser.add_argument(\'--h5\', dest=\'h5\', help=\'to store as h5py file\', default=False, type=bool)\n\ndef main(args, net=None):\n    datadir = get_data_dir(args.db)\n    outputdir = get_output_dir(args.db)\n\n    use_cuda = torch.cuda.is_available()\n\n    # Set the seed for reproducing the results\n    random.seed(args.manualSeed)\n    np.random.seed(args.manualSeed)\n    torch.manual_seed(args.manualSeed)\n    if use_cuda:\n        torch.cuda.manual_seed_all(args.manualSeed)\n        torch.backends.cudnn.enabled = True\n        cudnn.benchmark = True\n\n    kwargs = {\'num_workers\': 0, \'pin_memory\': True} if use_cuda else {}\n    trainset = DCCPT_data(root=datadir, train=True, h5=args.h5)\n    testset = DCCPT_data(root=datadir, train=False, h5=args.h5)\n\n    # load from checkpoint if we\'re not given an external net\n    load_checkpoint = True if net is None else False\n    if net is None:\n        net = dp.load_predefined_extract_net(args)\n\n    totalset = torch.utils.data.ConcatDataset([trainset, testset])\n    dataloader = torch.utils.data.DataLoader(totalset, batch_size=100, shuffle=False, **kwargs)\n\n    # copying model params from checkpoint\n    if load_checkpoint:\n        filename = os.path.join(outputdir, args.torchmodel)\n        if os.path.isfile(filename):\n            print(""==> loading params from checkpoint \'{}\'"".format(filename))\n            checkpoint = torch.load(filename)\n            net.load_state_dict(checkpoint[\'state_dict\'])\n        else:\n            print(""==> no checkpoint found at \'{}\'"".format(filename))\n            raise ValueError\n\n    if use_cuda:\n        net.cuda()\n\n    print(\'Extracting features ...\')\n    features, features_dr, labels = extract(dataloader, net, use_cuda)\n    print(\'Done.\\n\')\n\n    feat_path = os.path.join(datadir, args.feat)\n    if args.h5:\n        import h5py\n        fo = h5py.File(feat_path + \'.h5\', \'w\')\n        fo.create_dataset(\'labels\', data=labels)\n        fo.create_dataset(\'Z\', data=np.squeeze(features_dr))\n        fo.create_dataset(\'data\', data=np.squeeze(features))\n        fo.close()\n    else:\n        fo = open(feat_path + \'.pkl\', \'wb\')\n        pickle.dump({\'labels\': labels, \'Z\': np.squeeze(features_dr), \'data\': np.squeeze(features)}, fo, protocol=2)\n        fo.close()\n    return features, features_dr, labels\n\ndef extract(dataloader, net, use_cuda):\n    net.eval()\n\n    original = []\n    features = []\n    labels = []\n\n    for batch_idx, (inputs, targets) in enumerate(dataloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        inputs_Var = Variable(inputs, volatile=True)\n        enc, dec = net(inputs_Var)\n        features += list(enc.data.cpu().numpy())\n        labels += list(targets)\n        original += list(inputs.cpu().numpy())\n\n    original = np.asarray(original).astype(np.float32)\n    if len(original.shape) != len(inputs.shape):\n        original = original[:,np.newaxis,:,:]\n\n    return original, np.asarray(features).astype(np.float32), np.asarray(labels)\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    main(args)'"
pytorch/extractconvSDAE.py,3,"b""import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\n\nclass extractconvSDAE(nn.Module):\n    def __init__(self, dim, output_padding, numpen, slope=0.0):\n        super(extractconvSDAE, self).__init__()\n        self.in_dim = dim[0]\n        self.nlayers = len(dim)-1\n        self.reluslope = slope\n        self.numpen = numpen\n        self.enc, self.dec = [], []\n        self.benc, self.bdec = [], []\n        for i in range(self.nlayers):\n            if i == self.nlayers - 1:\n                self.enc.append(nn.Linear(dim[i]*numpen*numpen, dim[i+1]))\n                self.benc.append(nn.BatchNorm2d(dim[i + 1]))\n                self.dec.append(nn.ConvTranspose2d(dim[i + 1], dim[i], kernel_size=numpen, stride=1))\n                self.bdec.append(nn.BatchNorm2d(dim[i]))\n            elif i == 0:\n                self.enc.append(nn.Conv2d(dim[i], dim[i + 1], kernel_size=4, stride=2, padding=1))\n                self.benc.append(nn.BatchNorm2d(dim[i + 1]))\n                self.dec.append(nn.ConvTranspose2d(dim[i+1], dim[i], kernel_size=4, stride=2, padding=1,\n                                                   output_padding=output_padding[i]))\n                self.bdec.append(nn.BatchNorm2d(dim[i]))\n            else:\n                self.enc.append(nn.Conv2d(dim[i], dim[i + 1], kernel_size=5, stride=2, padding=2))\n                self.benc.append(nn.BatchNorm2d(dim[i + 1]))\n                self.dec.append(nn.ConvTranspose2d(dim[i+1], dim[i], kernel_size=5, stride=2, padding=2,\n                                                   output_padding=output_padding[i]))\n                self.bdec.append(nn.BatchNorm2d(dim[i]))\n            setattr(self, 'enc_{}'.format(i), self.enc[-1])\n            setattr(self, 'benc_{}'.format(i), self.benc[-1])\n            setattr(self, 'dec_{}'.format(i), self.dec[-1])\n            setattr(self, 'bdec_{}'.format(i), self.bdec[-1])\n        self.base = []\n        self.bbase = []\n        for i in range(self.nlayers):\n            self.base.append(nn.Sequential(*self.enc[:i]))\n            self.bbase.append(nn.Sequential(*self.benc[:i]))\n\n        # initialization\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                init.normal(m.weight, std=1e-2)\n                if m.bias.data is not None:\n                    init.constant(m.bias, 0)\n            elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n                init.kaiming_normal(m.weight, mode='fan_out')\n                if m.bias.data is not None:\n                    init.constant(m.bias, 0)\n\n    def forward(self,x):\n        encoded = x\n        for i, (encoder,bencoder) in enumerate(zip(self.enc,self.benc)):\n            if i == self.nlayers-1:\n                encoded = encoded.view(encoded.size(0), -1)\n            encoded = encoder(encoded)\n            if i < self.nlayers-1:\n                encoded = bencoder(encoded)\n                encoded = F.leaky_relu(encoded, negative_slope=self.reluslope)\n        out = encoded\n        for i, (decoder,bdecoder) in reversed(list(enumerate(zip(self.dec,self.bdec)))):\n            if i == self.nlayers-1:\n                out = out.view(out.size(0), -1, 1, 1)\n            out = decoder(out)\n            if i:\n                out = bdecoder(out)\n                out = F.leaky_relu(out, negative_slope=self.reluslope)\n        return encoded, out\n\n"""
pytorch/make_data.py,0,"b'import os\nimport os.path as osp\nfrom config import cfg, get_data_dir\n\nimport random\nimport argparse\nimport numpy as np\nimport scipy.io as sio\nimport h5py\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.datasets.samples_generator import make_blobs\n\n\ndef make_reuters_data(path, N):\n    did_to_cat = {}\n    cat_list = [\'CCAT\', \'GCAT\', \'MCAT\', \'ECAT\']\n    with open(osp.join(path, \'rcv1-v2.topics.qrels\')) as fin:\n        for line in fin.readlines():\n            line = line.strip().split(\' \')\n            cat = line[0]\n            did = int(line[1])\n            if cat in cat_list:\n                did_to_cat[did] = did_to_cat.get(did, []) + [cat]\n        for did in did_to_cat.keys():\n            if len(did_to_cat[did]) > 1:\n                del did_to_cat[did]\n\n    dat_list = [\'lyrl2004_tokens_test_pt0.dat\',\n                \'lyrl2004_tokens_test_pt1.dat\',\n                \'lyrl2004_tokens_test_pt2.dat\',\n                \'lyrl2004_tokens_test_pt3.dat\',\n                \'lyrl2004_tokens_train.dat\']\n    data = []\n    target = []\n    cat_to_cid = {\'CCAT\': 0, \'GCAT\': 1, \'MCAT\': 2, \'ECAT\': 3}\n    del did\n    for dat in dat_list:\n        with open(osp.join(path, dat)) as fin:\n            for line in fin.readlines():\n                if line.startswith(\'.I\'):\n                    if \'did\' in locals():\n                        assert doc != \'\'\n                        if did_to_cat.has_key(did):\n                            data.append(doc)\n                            target.append(cat_to_cid[did_to_cat[did][0]])\n                    did = int(line.strip().split(\' \')[1])\n                    doc = \'\'\n                elif line.startswith(\'.W\'):\n                    assert doc == \'\'\n                else:\n                    doc += line\n\n    assert len(data) == len(did_to_cat)\n\n    X = CountVectorizer(dtype=np.float64, max_features=2000, max_df=0.90).fit_transform(data)\n    Y = np.asarray(target)\n\n    X = TfidfTransformer(norm=\'l2\', sublinear_tf=True).fit_transform(X)\n    X = np.asarray(X.todense())\n\n    minmaxscale = MinMaxScaler().fit(X)\n    X = minmaxscale.transform(X)\n\n    p = np.random.permutation(X.shape[0])\n    X = X[p]\n    Y = Y[p]\n\n    fo = h5py.File(osp.join(path, \'traindata.h5\'), \'w\')\n    fo.create_dataset(\'X\', data=X[:N * 6 / 7])\n    fo.create_dataset(\'Y\', data=Y[:N * 6 / 7])\n    fo.close()\n\n    fo = h5py.File(osp.join(path, \'testdata.h5\'), \'w\')\n    fo.create_dataset(\'X\', data=X[N * 6 / 7:N])\n    fo.create_dataset(\'Y\', data=Y[N * 6 / 7:N])\n    fo.close()\n\n\ndef load_mnist(root, training):\n    if training:\n        data = \'train-images-idx3-ubyte\'\n        label = \'train-labels-idx1-ubyte\'\n        N = 60000\n    else:\n        data = \'t10k-images-idx3-ubyte\'\n        label = \'t10k-labels-idx1-ubyte\'\n        N = 10000\n    with open(osp.join(root, data), \'rb\') as fin:\n        fin.seek(16, os.SEEK_SET)\n        X = np.fromfile(fin, dtype=np.uint8).reshape((N, 28 * 28))\n    with open(osp.join(root, label), \'rb\') as fin:\n        fin.seek(8, os.SEEK_SET)\n        Y = np.fromfile(fin, dtype=np.uint8)\n    return X, Y\n\n\ndef make_mnist_data(path, isconv=False):\n    X, Y = load_mnist(path, True)\n    X = X.astype(np.float64)\n    X2, Y2 = load_mnist(path, False)\n    X2 = X2.astype(np.float64)\n    X3 = np.concatenate((X, X2), axis=0)\n\n    minmaxscale = MinMaxScaler().fit(X3)\n\n    X = minmaxscale.transform(X)\n    if isconv:\n        X = X.reshape((-1, 1, 28, 28))\n\n    sio.savemat(osp.join(path, \'traindata.mat\'), {\'X\': X, \'Y\': Y})\n\n    X2 = minmaxscale.transform(X2)\n    if isconv:\n        X2 = X2.reshape((-1, 1, 28, 28))\n\n    sio.savemat(osp.join(path, \'testdata.mat\'), {\'X\': X2, \'Y\': Y2})\n\n\ndef make_misc_data(path, filename, dim, isconv=False):\n    import cPickle\n    fo = open(osp.join(path, filename), \'r\')\n    data = cPickle.load(fo)\n    fo.close()\n    X = data[\'data\'].astype(np.float64)\n    Y = data[\'labels\']\n\n    minmaxscale = MinMaxScaler().fit(X)\n    X = minmaxscale.transform(X)\n\n    p = np.random.permutation(X.shape[0])\n    X = X[p]\n    Y = Y[p]\n\n    N = X.shape[0]\n\n    if isconv:\n        X = X.reshape((-1, dim[2], dim[0], dim[1]))\n    save_misc_data(path, X, Y, N)\n\n\ndef make_easy_visual_data(path, N=600):\n    """"""Make 3 clusters of 2D data where the cluster centers lie along a line.\n    The latent variable would be just their x or y value since that uniquely defines their projection onto the line.\n    """"""\n\n    line = (1.5, 1)\n    centers = [(m, m * line[0] + line[1]) for m in (-4, 0, 6)]\n    cluster_std = [1, 1, 1.5]\n    X, labels = make_blobs(n_samples=N, cluster_std=cluster_std, centers=centers, n_features=len(centers[0]))\n\n    # scale data\n    minmaxscale = MinMaxScaler().fit(X)\n    X = minmaxscale.transform(X)\n\n    save_misc_data(path, X, labels, N)\n    return X, labels\n\n\ndef save_misc_data(path, X, Y, N):\n    threshold_index = int(N * 4/5)\n    sio.savemat(osp.join(path, \'traindata.mat\'), {\'X\': X[:threshold_index], \'Y\': Y[:threshold_index]})\n    sio.savemat(osp.join(path, \'testdata.mat\'), {\'X\': X[threshold_index:], \'Y\': Y[threshold_index:]})\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--data\', dest=\'db\', type=str, default=\'mnist\', help=\'name of the dataset\')\n\n    args = parser.parse_args()\n    np.random.seed(cfg.RNG_SEED)\n    random.seed(cfg.RNG_SEED)\n\n    datadir = get_data_dir(args.db)\n    strpath = osp.join(datadir, \'traindata.mat\')\n\n    if not os.path.exists(strpath):\n        if args.db == \'mnist\':\n            make_mnist_data(datadir)\n        elif args.db == \'reuters\':\n            make_reuters_data(datadir, 10000)\n        elif args.db == \'ytf\':\n            make_misc_data(datadir, \'YTFrgb.pkl\', [55, 55, 3])\n        elif args.db == \'coil100\':\n            make_misc_data(datadir, \'coil100rgb.pkl\', [128, 128, 3])\n        elif args.db == \'yale\':\n            make_misc_data(datadir, \'yale_DoG.pkl\', [168, 192, 1])\n        elif args.db == \'rcv1\':\n            make_misc_data(datadir, \'reuters.pkl\', [1, 1, 2000])\n        elif args.db == \'cmnist\':\n            make_mnist_data(datadir, isconv=True)\n        elif args.db == \'cytf\':\n            make_misc_data(datadir, \'YTFrgb.pkl\', [55, 55, 3], isconv=True)\n        elif args.db == \'ccoil100\':\n            make_misc_data(datadir, \'coil100rgb.pkl\', [128, 128, 3], isconv=True)\n        elif args.db == \'cyale\':\n            make_misc_data(datadir, \'yale_DoG.pkl\', [168, 192, 1], isconv=True)\n        elif args.db == \'easy\':\n            make_easy_visual_data(datadir)\n'"
pytorch/pretraining.py,13,"b'from __future__ import print_function\nimport os\nimport random\nimport numpy as np\nimport argparse\nfrom config import cfg, get_data_dir, get_output_dir, AverageMeter, remove_files_in_dir\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.autograd import Variable\nimport torch.backends.cudnn as cudnn\nimport data_params as dp\n\nfrom custom_data import DCCPT_data\n\n# used for logging to TensorBoard\nfrom tensorboard_logger import Logger\n\n# Parse all the input argument\nparser = argparse.ArgumentParser(description=\'PyTorch SDAE Training\')\nparser.add_argument(\'--batchsize\', type=int, default=256, help=\'batch size used for pretraining\')\n# mnist=50000, ytf=6700, coil100=5000, reuters10k=50000, yale=10000, rcv1=6100. This amounts to ~200 epochs.\nparser.add_argument(\'--niter\', type=int, default=50000, help=\'number of iterations used for pretraining\')\n# mnist=20000, ytf=2700, coil100=2000, reuters10k=20000, yale=4000, rcv1=2450. This amounts to ~80 epochs.\nparser.add_argument(\'--step\', type=int, default=20000,\n                    help=\'stepsize in terms of number of iterations for pretraining. lr is decreased by 10 after every stepsize.\')\n# Note: The learning rate of pretraining stage differs for each dataset.\n# As noted in the paper, it depends on the original dimension of the data samples.\n# This is purely selected such that the SDAE\'s are trained with maximum possible learning rate for each dataset.\n# We set mnist,reuters,rcv1=10, ytf=1, coil100,yaleb=0.1\n# For convolutional SDAE lr if fixed to 0.1\nparser.add_argument(\'--lr\', default=10, type=float, help=\'initial learning rate for pretraining\')\nparser.add_argument(\'--manualSeed\', default=cfg.RNG_SEED, type=int, help=\'manual seed\')\nparser.add_argument(\'--resume\', \'-r\', action=\'store_true\', help=\'resume from checkpoint\')\nparser.add_argument(\'--level\', default=0, type=int, help=\'index of the module to resume from\')\nparser.add_argument(\'--data\', dest=\'db\', type=str, default=\'mnist\', help=\'name of the dataset\')\nparser.add_argument(\'--ngpu\', type=int, default=1, help=\'number of GPUs to use\')\nparser.add_argument(\'--dim\', type=int, help=\'dimension of embedding space\', default=10)\nparser.add_argument(\'--deviceID\', type=int, help=\'deviceID\', default=0)\nparser.add_argument(\'--h5\', dest=\'h5\', help=\'to store as h5py file\', default=False, type=bool)\nparser.add_argument(\'--tensorboard\', help=\'Log progress to TensorBoard\', action=\'store_true\')\nparser.add_argument(\'--id\', type=int, help=\'identifying number for storing tensorboard logs\')\n\ndef main(args):\n    datadir = get_data_dir(args.db)\n    outputdir = get_output_dir(args.db)\n\n    logger = None\n    if args.tensorboard:\n        # One should create folder for storing logs\n        loggin_dir = os.path.join(outputdir, \'runs\', \'pretraining\')\n        if not os.path.exists(loggin_dir):\n            os.makedirs(loggin_dir)\n        loggin_dir = os.path.join(loggin_dir, \'%s\' % (args.id))\n        if args.clean_log:\n            remove_files_in_dir(loggin_dir)\n        logger = Logger(loggin_dir)\n\n    use_cuda = torch.cuda.is_available()\n\n    # Set the seed for reproducing the results\n    random.seed(args.manualSeed)\n    np.random.seed(args.manualSeed)\n    torch.manual_seed(args.manualSeed)\n    if use_cuda:\n        torch.cuda.manual_seed_all(args.manualSeed)\n        torch.backends.cudnn.enabled = True\n        cudnn.benchmark = True\n\n    kwargs = {\'num_workers\': 0, \'pin_memory\': True} if use_cuda else {}\n    trainset = DCCPT_data(root=datadir, train=True, h5=args.h5)\n    testset = DCCPT_data(root=datadir, train=False, h5=args.h5)\n\n    nepoch = int(np.ceil(np.array(args.niter * args.batchsize, dtype=float) / len(trainset)))\n    step = int(np.ceil(np.array(args.step * args.batchsize, dtype=float) / len(trainset)))\n\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batchsize, shuffle=True, **kwargs)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=True, **kwargs)\n\n    return pretrain(args, outputdir, {\'nlayers\':4, \'dropout\':0.2, \'reluslope\':0.0,\n                       \'nepoch\':nepoch, \'lrate\':[args.lr], \'wdecay\':[0.0], \'step\':step}, use_cuda, trainloader, testloader, logger)\n\ndef pretrain(args, outputdir, params, use_cuda, trainloader, testloader, logger):\n    numlayers = params[\'nlayers\']\n    lr = params[\'lrate\'][0]\n    maxepoch = params[\'nepoch\']\n    stepsize = params[\'step\']\n    startlayer = 0\n\n    net = dp.load_predefined_net(args, params)\n\n    # correct for the number of layers\n    if args.db == \'ccoil100\':\n        numlayers = 6\n    elif args.db == \'cytf\':\n        numlayers = 5\n    elif args.db == \'cyale\':\n        numlayers = 6\n    elif args.db == \'easy\':\n        numlayers = len(dp.easy.dim)\n\n    # For the final FT stage of SDAE pretraining, the total epoch is twice that of previous stages.\n    maxepoch = [maxepoch]*numlayers + [maxepoch*2]\n    stepsize = [stepsize]*(numlayers+1)\n\n    if args.resume:\n        filename = outputdir+\'/checkpoint_%d.pth.tar\' % args.level\n        if os.path.isfile(filename):\n            print(""==> loading checkpoint \'{}\'"".format(filename))\n            checkpoint = torch.load(filename)\n            net.load_state_dict(checkpoint[\'state_dict\'])\n            startlayer = args.level+1\n        else:\n            print(""==> no checkpoint found at \'{}\'"".format(filename))\n            raise ValueError\n\n    if use_cuda:\n        net.cuda()\n\n    for index in range(startlayer, numlayers+1):\n        # Freezing previous layer weights\n        if index < numlayers:\n            for par in net.base[index].parameters():\n                par.requires_grad = False\n            if args.db == \'cmnist\' or args.db == \'ccoil100\' or args.db == \'cytf\' or args.db == \'cyale\':\n                for par in net.bbase[index].parameters():\n                    par.requires_grad = False\n                for m in net.bbase[index].modules():\n                    if isinstance(m, nn.BatchNorm2d):\n                        m.training = False\n        else:\n            for par in net.base[numlayers-1].parameters():\n                par.requires_grad = True\n            if args.db == \'cmnist\' or args.db == \'ccoil100\' or args.db == \'cytf\' or args.db == \'cyale\':\n                for par in net.bbase[numlayers-1].parameters():\n                    par.requires_grad = True\n                for m in net.bbase[numlayers-1].modules():\n                    if isinstance(m, nn.BatchNorm2d):\n                        m.training = True\n\n        # setting up optimizer - the bias params should have twice the learning rate w.r.t. weights params\n        bias_params = filter(lambda x: (\'bias\' in x[0]) and (x[1].requires_grad), net.named_parameters())\n        bias_params = list(map(lambda x: x[1], bias_params))\n        nonbias_params = filter(lambda x: (\'bias\' not in x[0]) and (x[1].requires_grad), net.named_parameters())\n        nonbias_params = list(map(lambda x: x[1], nonbias_params))\n\n        optimizer = optim.SGD([{\'params\': bias_params, \'lr\': 2*lr}, {\'params\': nonbias_params}],\n                              lr=lr, momentum=0.9, weight_decay=params[\'wdecay\'][0], nesterov=True)\n\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=stepsize[index], gamma=0.1)\n\n        print(\'\\nIndex: %d \\t Maxepoch: %d\'%(index, maxepoch[index]))\n\n        for epoch in range(maxepoch[index]):\n            scheduler.step()\n            train(trainloader, net, index, optimizer, epoch, use_cuda, logger)\n            test(testloader, net, index, epoch, use_cuda, logger)\n            # Save checkpoint\n            save_checkpoint({\'epoch\': epoch+1, \'state_dict\': net.state_dict(), \'optimizer\': optimizer.state_dict()},\n                            index, filename=outputdir)\n\n    outnet = dp.load_predefined_extract_net(args)\n    outnet.load_state_dict(net.state_dict())\n    return index, outnet\n\n\n# Training\ndef train(trainloader, net, index, optimizer, epoch, use_cuda, logger):\n    losses = AverageMeter()\n\n    print(\'\\nIndex: %d \\t Epoch: %d\' %(index,epoch))\n\n    net.train()\n\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        optimizer.zero_grad()\n        inputs_Var = Variable(inputs)\n        outputs = net(inputs_Var, index)\n\n        # record loss\n        losses.update(outputs.item(), inputs.size(0))\n\n        outputs.backward()\n        optimizer.step()\n\n    # log to TensorBoard\n    if logger:\n        logger.log_value(\'train_loss_{}\'.format(index), losses.avg, epoch)\n\n\n# Testing\ndef test(testloader, net, index, epoch, use_cuda, logger):\n    losses = AverageMeter()\n\n    net.eval()\n\n    for batch_idx, (inputs, targets) in enumerate(testloader):\n        if use_cuda:\n            inputs = inputs.cuda()\n        inputs_Var = Variable(inputs, volatile=True)\n        outputs = net(inputs_Var, index)\n\n        # measure accuracy and record loss\n        losses.update(outputs.item(), inputs.size(0))\n\n    # log to TensorBoard\n    if logger:\n        logger.log_value(\'val_loss_{}\'.format(index), losses.avg, epoch)\n\n\n# Saving checkpoint\ndef save_checkpoint(state, index, filename):\n    torch.save(state, filename+\'/checkpoint_%d.pth.tar\' % index)\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    main(args)'"
