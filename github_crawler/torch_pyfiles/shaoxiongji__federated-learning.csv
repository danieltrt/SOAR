file_path,api_count,code
main_fed.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Python version: 3.6\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport copy\nimport numpy as np\nfrom torchvision import datasets, transforms\nimport torch\n\nfrom utils.sampling import mnist_iid, mnist_noniid, cifar_iid\nfrom utils.options import args_parser\nfrom models.Update import LocalUpdate\nfrom models.Nets import MLP, CNNMnist, CNNCifar\nfrom models.Fed import FedAvg\nfrom models.test import test_img\n\n\nif __name__ == \'__main__\':\n    # parse args\n    args = args_parser()\n    args.device = torch.device(\'cuda:{}\'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else \'cpu\')\n\n    # load dataset and split users\n    if args.dataset == \'mnist\':\n        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n        dataset_train = datasets.MNIST(\'../data/mnist/\', train=True, download=True, transform=trans_mnist)\n        dataset_test = datasets.MNIST(\'../data/mnist/\', train=False, download=True, transform=trans_mnist)\n        # sample users\n        if args.iid:\n            dict_users = mnist_iid(dataset_train, args.num_users)\n        else:\n            dict_users = mnist_noniid(dataset_train, args.num_users)\n    elif args.dataset == \'cifar\':\n        trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        dataset_train = datasets.CIFAR10(\'../data/cifar\', train=True, download=True, transform=trans_cifar)\n        dataset_test = datasets.CIFAR10(\'../data/cifar\', train=False, download=True, transform=trans_cifar)\n        if args.iid:\n            dict_users = cifar_iid(dataset_train, args.num_users)\n        else:\n            exit(\'Error: only consider IID setting in CIFAR10\')\n    else:\n        exit(\'Error: unrecognized dataset\')\n    img_size = dataset_train[0][0].shape\n\n    # build model\n    if args.model == \'cnn\' and args.dataset == \'cifar\':\n        net_glob = CNNCifar(args=args).to(args.device)\n    elif args.model == \'cnn\' and args.dataset == \'mnist\':\n        net_glob = CNNMnist(args=args).to(args.device)\n    elif args.model == \'mlp\':\n        len_in = 1\n        for x in img_size:\n            len_in *= x\n        net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n    else:\n        exit(\'Error: unrecognized model\')\n    print(net_glob)\n    net_glob.train()\n\n    # copy weights\n    w_glob = net_glob.state_dict()\n\n    # training\n    loss_train = []\n    cv_loss, cv_acc = [], []\n    val_loss_pre, counter = 0, 0\n    net_best = None\n    best_loss = None\n    val_acc_list, net_list = [], []\n\n    for iter in range(args.epochs):\n        w_locals, loss_locals = [], []\n        m = max(int(args.frac * args.num_users), 1)\n        idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n        for idx in idxs_users:\n            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n            w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n            w_locals.append(copy.deepcopy(w))\n            loss_locals.append(copy.deepcopy(loss))\n        # update global weights\n        w_glob = FedAvg(w_locals)\n\n        # copy weight to net_glob\n        net_glob.load_state_dict(w_glob)\n\n        # print loss\n        loss_avg = sum(loss_locals) / len(loss_locals)\n        print(\'Round {:3d}, Average loss {:.3f}\'.format(iter, loss_avg))\n        loss_train.append(loss_avg)\n\n    # plot loss curve\n    plt.figure()\n    plt.plot(range(len(loss_train)), loss_train)\n    plt.ylabel(\'train_loss\')\n    plt.savefig(\'./save/fed_{}_{}_{}_C{}_iid{}.png\'.format(args.dataset, args.model, args.epochs, args.frac, args.iid))\n\n    # testing\n    net_glob.eval()\n    acc_train, loss_train = test_img(net_glob, dataset_train, args)\n    acc_test, loss_test = test_img(net_glob, dataset_test, args)\n    print(""Training accuracy: {:.2f}"".format(acc_train))\n    print(""Testing accuracy: {:.2f}"".format(acc_test))\n\n'"
main_nn.py,5,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Python version: 3.6\n\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\nfrom utils.options import args_parser\nfrom models.Nets import MLP, CNNMnist, CNNCifar\n\n\ndef test(net_g, data_loader):\n    # testing\n    net_g.eval()\n    test_loss = 0\n    correct = 0\n    l = len(data_loader)\n    for idx, (data, target) in enumerate(data_loader):\n        data, target = data.to(args.device), target.to(args.device)\n        log_probs = net_g(data)\n        test_loss += F.cross_entropy(log_probs, target).item()\n        y_pred = log_probs.data.max(1, keepdim=True)[1]\n        correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n\n    test_loss /= len(data_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(data_loader.dataset),\n        100. * correct / len(data_loader.dataset)))\n\n    return correct, test_loss\n\n\nif __name__ == '__main__':\n    # parse args\n    args = args_parser()\n    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n\n    torch.manual_seed(args.seed)\n\n    # load dataset and split users\n    if args.dataset == 'mnist':\n        dataset_train = datasets.MNIST('./data/mnist/', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ]))\n        img_size = dataset_train[0][0].shape\n    elif args.dataset == 'cifar':\n        transform = transforms.Compose(\n            [transforms.ToTensor(),\n             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        dataset_train = datasets.CIFAR10('./data/cifar', train=True, transform=transform, target_transform=None, download=True)\n        img_size = dataset_train[0][0].shape\n    else:\n        exit('Error: unrecognized dataset')\n\n    # build model\n    if args.model == 'cnn' and args.dataset == 'cifar':\n        net_glob = CNNCifar(args=args).to(args.device)\n    elif args.model == 'cnn' and args.dataset == 'mnist':\n        net_glob = CNNMnist(args=args).to(args.device)\n    elif args.model == 'mlp':\n        len_in = 1\n        for x in img_size:\n            len_in *= x\n        net_glob = MLP(dim_in=len_in, dim_hidden=64, dim_out=args.num_classes).to(args.device)\n    else:\n        exit('Error: unrecognized model')\n    print(net_glob)\n\n    # training\n    optimizer = optim.SGD(net_glob.parameters(), lr=args.lr, momentum=args.momentum)\n    train_loader = DataLoader(dataset_train, batch_size=64, shuffle=True)\n\n    list_loss = []\n    net_glob.train()\n    for epoch in range(args.epochs):\n        batch_loss = []\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(args.device), target.to(args.device)\n            optimizer.zero_grad()\n            output = net_glob(data)\n            loss = F.cross_entropy(output, target)\n            loss.backward()\n            optimizer.step()\n            if batch_idx % 50 == 0:\n                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                    epoch, batch_idx * len(data), len(train_loader.dataset),\n                           100. * batch_idx / len(train_loader), loss.item()))\n            batch_loss.append(loss.item())\n        loss_avg = sum(batch_loss)/len(batch_loss)\n        print('\\nTrain loss:', loss_avg)\n        list_loss.append(loss_avg)\n\n    # plot loss\n    plt.figure()\n    plt.plot(range(len(list_loss)), list_loss)\n    plt.xlabel('epochs')\n    plt.ylabel('train loss')\n    plt.savefig('./log/nn_{}_{}_{}.png'.format(args.dataset, args.model, args.epochs))\n\n    # testing\n    if args.dataset == 'mnist':\n        dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ]))\n        test_loader = DataLoader(dataset_test, batch_size=1000, shuffle=False)\n    elif args.dataset == 'cifar':\n        transform = transforms.Compose(\n            [transforms.ToTensor(),\n             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        dataset_test = datasets.CIFAR10('./data/cifar', train=False, transform=transform, target_transform=None, download=True)\n        test_loader = DataLoader(dataset_test, batch_size=1000, shuffle=False)\n    else:\n        exit('Error: unrecognized dataset')\n\n    print('test on', len(dataset_test), 'samples')\n    test_acc, test_loss = test(net_glob, test_loader)\n"""
data/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @python: 3.6\n\n'
models/Fed.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Python version: 3.6\n\nimport copy\nimport torch\nfrom torch import nn\n\n\ndef FedAvg(w):\n    w_avg = copy.deepcopy(w[0])\n    for k in w_avg.keys():\n        for i in range(1, len(w)):\n            w_avg[k] += w[i][k]\n        w_avg[k] = torch.div(w_avg[k], len(w))\n    return w_avg\n'"
models/Nets.py,1,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Python version: 3.6\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim_in, dim_hidden, dim_out):\n        super(MLP, self).__init__()\n        self.layer_input = nn.Linear(dim_in, dim_hidden)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout()\n        self.layer_hidden = nn.Linear(dim_hidden, dim_out)\n\n    def forward(self, x):\n        x = x.view(-1, x.shape[1]*x.shape[-2]*x.shape[-1])\n        x = self.layer_input(x)\n        x = self.dropout(x)\n        x = self.relu(x)\n        x = self.layer_hidden(x)\n        return x\n\n\nclass CNNMnist(nn.Module):\n    def __init__(self, args):\n        super(CNNMnist, self).__init__()\n        self.conv1 = nn.Conv2d(args.num_channels, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, args.num_classes)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return x\n\n\nclass CNNCifar(nn.Module):\n    def __init__(self, args):\n        super(CNNCifar, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, args.num_classes)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n'"
models/Update.py,2,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Python version: 3.6\n\nimport torch\nfrom torch import nn, autograd\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\nimport random\nfrom sklearn import metrics\n\n\nclass DatasetSplit(Dataset):\n    def __init__(self, dataset, idxs):\n        self.dataset = dataset\n        self.idxs = list(idxs)\n\n    def __len__(self):\n        return len(self.idxs)\n\n    def __getitem__(self, item):\n        image, label = self.dataset[self.idxs[item]]\n        return image, label\n\n\nclass LocalUpdate(object):\n    def __init__(self, args, dataset=None, idxs=None):\n        self.args = args\n        self.loss_func = nn.CrossEntropyLoss()\n        self.selected_clients = []\n        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=self.args.local_bs, shuffle=True)\n\n    def train(self, net):\n        net.train()\n        # train and update\n        optimizer = torch.optim.SGD(net.parameters(), lr=self.args.lr, momentum=0.5)\n\n        epoch_loss = []\n        for iter in range(self.args.local_ep):\n            batch_loss = []\n            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n                images, labels = images.to(self.args.device), labels.to(self.args.device)\n                net.zero_grad()\n                log_probs = net(images)\n                loss = self.loss_func(log_probs, labels)\n                loss.backward()\n                optimizer.step()\n                if self.args.verbose and batch_idx % 10 == 0:\n                    print('Update Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                        iter, batch_idx * len(images), len(self.ldr_train.dataset),\n                               100. * batch_idx / len(self.ldr_train), loss.item()))\n                batch_loss.append(loss.item())\n            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n        return net.state_dict(), sum(epoch_loss) / len(epoch_loss)\n\n"""
models/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @python: 3.6\n\n'
models/test.py,2,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @python: 3.6\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\n\ndef test_img(net_g, datatest, args):\n    net_g.eval()\n    # testing\n    test_loss = 0\n    correct = 0\n    data_loader = DataLoader(datatest, batch_size=args.bs)\n    l = len(data_loader)\n    for idx, (data, target) in enumerate(data_loader):\n        if args.gpu != -1:\n            data, target = data.cuda(), target.cuda()\n        log_probs = net_g(data)\n        # sum up batch loss\n        test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n        # get the index of the max log-probability\n        y_pred = log_probs.data.max(1, keepdim=True)[1]\n        correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n\n    test_loss /= len(data_loader.dataset)\n    accuracy = 100.00 * correct / len(data_loader.dataset)\n    if args.verbose:\n        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n            test_loss, correct, len(data_loader.dataset), accuracy))\n    return accuracy, test_loss\n\n"""
utils/__init__.py,0,b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# @python: 3.6\n\n'
utils/options.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Python version: 3.6\n\nimport argparse\n\ndef args_parser():\n    parser = argparse.ArgumentParser()\n    # federated arguments\n    parser.add_argument(\'--epochs\', type=int, default=10, help=""rounds of training"")\n    parser.add_argument(\'--num_users\', type=int, default=100, help=""number of users: K"")\n    parser.add_argument(\'--frac\', type=float, default=0.1, help=""the fraction of clients: C"")\n    parser.add_argument(\'--local_ep\', type=int, default=5, help=""the number of local epochs: E"")\n    parser.add_argument(\'--local_bs\', type=int, default=10, help=""local batch size: B"")\n    parser.add_argument(\'--bs\', type=int, default=128, help=""test batch size"")\n    parser.add_argument(\'--lr\', type=float, default=0.01, help=""learning rate"")\n    parser.add_argument(\'--momentum\', type=float, default=0.5, help=""SGD momentum (default: 0.5)"")\n    parser.add_argument(\'--split\', type=str, default=\'user\', help=""train-test split type, user or sample"")\n\n    # model arguments\n    parser.add_argument(\'--model\', type=str, default=\'mlp\', help=\'model name\')\n    parser.add_argument(\'--kernel_num\', type=int, default=9, help=\'number of each kind of kernel\')\n    parser.add_argument(\'--kernel_sizes\', type=str, default=\'3,4,5\',\n                        help=\'comma-separated kernel size to use for convolution\')\n    parser.add_argument(\'--norm\', type=str, default=\'batch_norm\', help=""batch_norm, layer_norm, or None"")\n    parser.add_argument(\'--num_filters\', type=int, default=32, help=""number of filters for conv nets"")\n    parser.add_argument(\'--max_pool\', type=str, default=\'True\',\n                        help=""Whether use max pooling rather than strided convolutions"")\n\n    # other arguments\n    parser.add_argument(\'--dataset\', type=str, default=\'mnist\', help=""name of dataset"")\n    parser.add_argument(\'--iid\', action=\'store_true\', help=\'whether i.i.d or not\')\n    parser.add_argument(\'--num_classes\', type=int, default=10, help=""number of classes"")\n    parser.add_argument(\'--num_channels\', type=int, default=3, help=""number of channels of imges"")\n    parser.add_argument(\'--gpu\', type=int, default=0, help=""GPU ID, -1 for CPU"")\n    parser.add_argument(\'--stopping_rounds\', type=int, default=10, help=\'rounds of early stopping\')\n    parser.add_argument(\'--verbose\', action=\'store_true\', help=\'verbose print\')\n    parser.add_argument(\'--seed\', type=int, default=1, help=\'random seed (default: 1)\')\n    args = parser.parse_args()\n    return args\n'"
utils/sampling.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Python version: 3.6\n\n\nimport numpy as np\nfrom torchvision import datasets, transforms\n\ndef mnist_iid(dataset, num_users):\n    """"""\n    Sample I.I.D. client data from MNIST dataset\n    :param dataset:\n    :param num_users:\n    :return: dict of image index\n    """"""\n    num_items = int(len(dataset)/num_users)\n    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n    for i in range(num_users):\n        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n        all_idxs = list(set(all_idxs) - dict_users[i])\n    return dict_users\n\n\ndef mnist_noniid(dataset, num_users):\n    """"""\n    Sample non-I.I.D client data from MNIST dataset\n    :param dataset:\n    :param num_users:\n    :return:\n    """"""\n    num_shards, num_imgs = 200, 300\n    idx_shard = [i for i in range(num_shards)]\n    dict_users = {i: np.array([], dtype=\'int64\') for i in range(num_users)}\n    idxs = np.arange(num_shards*num_imgs)\n    labels = dataset.train_labels.numpy()\n\n    # sort labels\n    idxs_labels = np.vstack((idxs, labels))\n    idxs_labels = idxs_labels[:,idxs_labels[1,:].argsort()]\n    idxs = idxs_labels[0,:]\n\n    # divide and assign\n    for i in range(num_users):\n        rand_set = set(np.random.choice(idx_shard, 2, replace=False))\n        idx_shard = list(set(idx_shard) - rand_set)\n        for rand in rand_set:\n            dict_users[i] = np.concatenate((dict_users[i], idxs[rand*num_imgs:(rand+1)*num_imgs]), axis=0)\n    return dict_users\n\n\ndef cifar_iid(dataset, num_users):\n    """"""\n    Sample I.I.D. client data from CIFAR10 dataset\n    :param dataset:\n    :param num_users:\n    :return: dict of image index\n    """"""\n    num_items = int(len(dataset)/num_users)\n    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n    for i in range(num_users):\n        dict_users[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n        all_idxs = list(set(all_idxs) - dict_users[i])\n    return dict_users\n\n\nif __name__ == \'__main__\':\n    dataset_train = datasets.MNIST(\'../data/mnist/\', train=True, download=True,\n                                   transform=transforms.Compose([\n                                       transforms.ToTensor(),\n                                       transforms.Normalize((0.1307,), (0.3081,))\n                                   ]))\n    num = 100\n    d = mnist_noniid(dataset_train, num)\n'"
