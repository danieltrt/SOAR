file_path,api_count,code
src/ae.py,8,"b'"""""" (AE)\nStandard Autoencoder\n\nAutoencoders take an input representation, encode it into a reduced\ndimensionality space using an \'encoder network.\' They then decode this\nencoded representation by using a \'decoder network,\' which transforms it\nback to its original representation.\n""""""\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.utils import make_grid\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy\n\nfrom tqdm import tqdm\nfrom itertools import product\n\nfrom utils import *\n\n\nclass Encoder(nn.Module):\n    """""" Feedforward network encoder. Input is an image, output is encoded\n    vector representation of that image.\n    """"""\n    def __init__(self, image_size, hidden_dim):\n        super().__init__()\n\n        self.linear = nn.Linear(image_size, hidden_dim)\n\n    def forward(self, x):\n        return F.relu(self.linear(x))\n\n\nclass Decoder(nn.Module):\n    """""" Feedforward network decoder. Input is an encoded vector representation,\n    output is reconstructed image.\n    """"""\n    def __init__(self, hidden_dim, image_size):\n        super().__init__()\n\n        self.linear = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, encoder_output):\n        return torch.sigmoid(self.linear(encoder_output))\n\n\nclass Autoencoder(nn.Module):\n    """""" Autoencoder super class to encode then decode an image\n    """"""\n    def __init__(self, image_size=784, hidden_dim=32):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.encoder = Encoder(image_size=image_size, hidden_dim=hidden_dim)\n        self.decoder = Decoder(hidden_dim=hidden_dim, image_size=image_size)\n\n    def forward(self, x):\n        return self.decoder(self.encoder(x))\n\n\nclass AutoencoderTrainer:\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False):\n        """""" Object to hold data iterators, train the model """"""\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.best_val_loss = 1e10\n        self.debugging_image, _ = next(iter(test_iter))\n        self.viz = viz\n\n        self.recon_loss = []\n        self.num_epochs = 0\n\n    def train(self, num_epochs, lr=1e-3, weight_decay=1e-5):\n        """""" Train a Variational Autoencoder\n            Logs progress using total loss, reconstruction loss, kl_divergence,\n            and validation loss\n\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            lr: float, learning rate for Adam optimizer (default 1e-3)\n            weight_decay: float, weight decay for Adam optimizer (default 1e-5)\n        """"""\n\n        # Adam optimizer, sigmoid cross entropy for reconstructing binary MNIST\n        optimizer = optim.Adam(params=[p for p in self.model.parameters()\n                                            if p.requires_grad],\n                                 lr=lr,\n                                 weight_decay=weight_decay)\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs+1)):\n\n            self.model.train()\n            epoch_loss = []\n\n            for batch in self.train_iter:\n\n                # Zero out gradients\n                optimizer.zero_grad()\n\n                # Compute reconstruction loss (binary cross entropy)\n                batch_loss = self.compute_batch(batch)\n\n                # Update parameters\n                batch_loss.backward()\n                optimizer.step()\n\n                # Log progress metrics\n                epoch_loss.append(batch_loss.item())\n\n            # Save progress\n            self.recon_loss.extend(epoch_loss)\n\n            # Evaluate the model on the validation set\n            self.model.eval()\n            val_loss = self.evaluate(self.val_iter)\n\n            # Early stopping based on validation loss\n            if val_loss < self.best_val_loss:\n                self.best_model = deepcopy(self.model)\n                self.best_val_loss = val_loss\n\n            # Print progress\n            print (""Epoch[%d/%d], Train Loss: %.4f, Val Loss: %.4f""\n                   %(epoch, num_epochs, np.mean(epoch_loss), val_loss))\n            self.num_epochs += 1\n\n            # Debugging and visualization purposes\n            if self.viz:\n                self.reconstruct_images(self.debugging_image, epoch)\n                plt.show()\n\n    def compute_batch(self, batch):\n        """""" Compute loss for a batch of examples """"""\n\n        # Process a batch, reshape from an image to a 2D tensor and send to GPU\n        images, _ = batch\n        images = to_cuda(images.view(images.shape[0], -1))\n\n        # Encode the image and then decode the representation\n        outputs = self.model(images)\n\n        # L2 (mean squared error) loss.\n        recon_loss = torch.sum((images - outputs) ** 2)\n\n        return recon_loss\n\n    def evaluate(self, iterator):\n        """""" Evaluate on a given dataset """"""\n        return np.mean([self.compute_batch(batch).item() for batch in iterator])\n\n    def reconstruct_images(self, images, epoch, save=True):\n        """""" Reconstruct a fixed input at each epoch for progress viz """"""\n        # Reshape images, pass through model, reshape reconstructed output\n        batch = to_cuda(images.view(images.shape[0], -1))\n        reconst_images = self.model(batch)\n        reconst_images = reconst_images.view(images.shape).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size, k = int(reconst_images.shape[0]**0.5), 0\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].imshow(reconst_images[k].data.numpy(), cmap=\'gray\')\n            k += 1\n\n        # Save\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(self.debugging_image.data,\n                                         outname + \'real.png\',\n                                         nrow=grid_size)\n            torchvision.utils.save_image(reconst_images.unsqueeze(1).data,\n                                         outname + \'reconst_%d.png\' %(epoch),\n                                         nrow=grid_size)\n\n    def viz_loss(self):\n        """""" Visualize reconstruction loss """"""\n        # Set style, figure size\n        plt.style.use(\'ggplot\')\n        plt.rcParams[""figure.figsize""] = (8,6)\n\n        # Plot reconstruction loss in red\n        plt.plot(np.linspace(1, self.num_epochs, len(self.recon_loss)),\n                 self.recon_loss,\n                 \'r\')\n\n        # Add legend, title\n        plt.legend([\'Reconstruction loss\'])\n        plt.title(self.name)\n        plt.show()\n\n    def save_model(self, savepath):\n        """""" Save model state dictionary """"""\n        torch.save(self.model.state_dict(), savepath)\n\n    def load_model(self, loadpath):\n        """""" Load state dictionary into model """"""\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)\n\n\nif __name__ == \'__main__\':\n\n    # Load in binzarized MNIST data, separate into data loaders\n    train_iter, val_iter, test_iter = get_data()\n\n    # Init model\n    model = Autoencoder(image_size=784,\n                        hidden_dim=32)\n\n    # Init trainer\n    trainer = AutoencoderTrainer(model=model,\n                                 train_iter=train_iter,\n                                 val_iter=val_iter,\n                                 test_iter=test_iter,\n                                 viz=False)\n\n    # Train\n    trainer.train(num_epochs=5,\n                  lr=1e-3,\n                  weight_decay=1e-5)\n'"
src/bayes_gan.py,0,"b'# TODO\n"""""" (BayesGAN) https://arxiv.org/abs/1705.09558\nBayesian GAN\n\nFrom the authors:\n\n""Bayesian GAN (Saatchi and Wilson, 2017) is a Bayesian formulation of Generative\nAdversarial Networks (Goodfellow, 2014) where we learn the distributions of the\ngenerator parameters $\\theta_g$ and the discriminator parameters $\\theta_d$\ninstead of optimizing for point estimates. The benefits of the Bayesian approach\ninclude the flexibility to model multimodality in the parameter space, as well\nas the ability to prevent mode collapse in the maximum likelihood (non-Bayesian)\ncase.\n\nWe learn Bayesian GAN via an approximate inference algorithm called Stochastic\nGradient Hamiltonian Monte Carlo (SGHMC) which is a gradient-based MCMC methods\nwhose samples approximate the true posterior distributions of $\\theta_g$ and\n$\\theta_d$. The Bayesian GAN training process starts from sampling noise $z$\nfrom a fixed  distribution(typically standard d-dim normal). The noise is fed\nto the generator where the parameters  $\\theta_g$ are sampled from the posterior\ndistribution $p(\\theta_g | D)$. The generated  image given the parameters\n$\\theta_g$ ($G(z|\\theta_g)$) as well as the real data are presented to the\ndiscriminator, whose parameters are sample from its posterior\ndistribution $p(\\theta_d|D)$. We update the posteriors using the gradients\n$\\frac{\\partial \\log p(\\theta_g|D) }{\\partial \\theta_g }$ and\n$\\frac{\\partial \\log p(\\theta_d|D) }{\\partial \\theta_d }$ with\nStochastic Gradient Hamiltonian Monte Carlo (SGHMC).""\n\nSGHMC is fancy for using point estimates (as in most GANs) to infer the\nposteriors.\n""""""\n'"
src/be_gan.py,13,"b'"""""" (BEGAN) https://arxiv.org/abs/1703.10717\nBoundary Equilibrium GAN\n\nBEGAN uses an autoencoder as a discriminator and optimizes a lower bound of the\nWasserstein distance between auto-encoder loss distributions on real and fake\ndata (as opposed to the sample distributions of the generator and real data).\n\nDuring training:\n\n    1) D (here, an autoencoder) reconstructs real images and is optimized to\n    minimize this reconstruction loss.\n    2) As a byproduct of D\'s optimization, the reconstruction loss of generated\n    images is increased. We optimize G to minimize the reconstruction loss of\n    the generated images.\n\nThis setup trains D and G simultaneously while preserving the adversarial setup.\n\nThe authors introduce an additional hyperparameter \xce\xb3 \xe2\x88\x88 [0,1] to maintain the\nequilibrium between the D and G. Equilibrium between D and G occurs when\nE[loss(D(x))] == E[loss(D(G(z)))]. This \xce\xb3 is useful because an equilibrium is\nnecessary for successful training of the BEGAN, and ""the discriminator has two\ncompeting goals: auto-encode real images and discriminate real from generated\nimages. The \xce\xb3 term lets us balance these. Lower values of \xce\xb3 lead to lower image\ndiversity because the discriminator focuses more heavily on auto-encoding real\nimages."" We define \xce\xb3 = E[loss(D(G(z)))] / E[loss(D(x))]. Then\nE[loss(D(G(z)))] == \xce\xb3E[loss(D(x))]. To keep this balance, the authors introduce\na variable kt \xe2\x88\x88 [0,1] to control how much emphasis to put on loss(D(G(z)))\nduring gradient descent.\n""""""\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom itertools import product\nfrom tqdm import tqdm\n\nfrom utils import *\n\n\nclass Generator(nn.Module):\n    """""" Generator. Input is noise, output is a generated image.\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim):\n        super().__init__()\n\n        self.linear = nn.Linear(z_dim, hidden_dim)\n        self.generate = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        generation = torch.sigmoid(self.generate(activated))\n        return generation\n\n\nclass Discriminator(nn.Module):\n    """""" Autoencoder. Input is an image (real, generated), output is the\n    reconstructed image.\n    """"""\n    def __init__(self, image_size, hidden_dim):\n        super().__init__()\n\n        self.encoder = nn.Linear(image_size, hidden_dim)\n        self.decoder = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, x):\n        encoded = F.relu(self.encoder(x))\n        decoded = self.decoder(encoded)\n        return decoded\n\n\nclass BEGAN(nn.Module):\n    """""" Super class to contain both Discriminator (D) and Generator (G)\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.G = Generator(image_size, hidden_dim, z_dim)\n        self.D = Discriminator(image_size, hidden_dim)\n\n        self.shape = int(image_size ** 0.5)\n\n\nclass BEGANTrainer:\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False):\n        """""" Object to hold data iterators, train a GAN variant """"""\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.Glosses = []\n        self.Dlosses = []\n\n        self.viz = viz\n        self.num_epochs = 0\n\n    def train(self, num_epochs, G_lr=1e-4, D_lr=1e-4, D_steps=1,\n                    GAMMA=0.50, LAMBDA=1e-3, K=0.00):\n        """""" Train a Bounded Equilibrium GAN\n            Logs progress using G loss, D loss, convergence metric,\n            visualizations of Generator output.\n\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            G_lr: float, learning rate for generator\'s optimizer\n            D_lr: float, learning rate for discriminator\'s optimizer\n            D_steps: int, ratio for how often to train D compared to G\n            GAMMA: float, balance equilibrium between G and D objectives\n            LAMBDA: float, weight D loss for updating K\n            K: float, how much to emphasize loss(D(G(z))) in initial D loss\n        """"""\n\n        # Adam optimizers\n        G_optimizer = optim.Adam(params=[p for p in self.model.G.parameters()\n                                        if p.requires_grad], lr=G_lr)\n        D_optimizer = optim.Adam(params=[p for p in self.model.D.parameters()\n                                        if p.requires_grad], lr=D_lr)\n\n        # Reduce learning rate by factor of 2 if convergence_metric stops\n        # decreasing by a threshold for last five epochs\n        G_scheduler = ReduceLROnPlateau(G_optimizer, factor=0.50, threshold=0.01,\n                                        patience=5*len(self.train_iter))\n        D_scheduler = ReduceLROnPlateau(D_optimizer, factor=0.50, threshold=0.01,\n                                        patience=5*len(self.train_iter))\n\n        # Approximate steps/epoch given D_steps per epoch\n        # --> roughly train in the same way as if D_step (1) == G_step (1)\n        epoch_steps = int(np.ceil(len(self.train_iter) / (D_steps)))\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs + 1)):\n\n            self.model.train()\n            G_losses, D_losses = [], []\n\n            for _ in range(epoch_steps):\n\n                D_step_loss = []\n\n                # TRAINING D: Train D for D_steps\n                for _ in range(D_steps):\n\n                    # Retrieve batch\n                    images = self.process_batch(self.train_iter)\n\n                    # Zero out gradients for D\n                    D_optimizer.zero_grad()\n\n                    # Train the discriminator using BEGAN loss\n                    D_loss, DX_loss, DG_loss = self.train_D(images, K)\n\n                    # Update parameters\n                    D_loss.backward()\n                    D_optimizer.step()\n\n                    # Save relevant output for progress logging\n                    D_step_loss.append(D_loss.item())\n\n                # So that G_loss and D_loss have the same number of entries\n                D_losses.append(np.mean(D_step_loss))\n\n                # TRAINING G: Zero out gradients for G.\n                G_optimizer.zero_grad()\n\n                # Train the generator using BEGAN loss\n                G_loss = self.train_G(images)\n\n                # Update parameters\n                G_loss.backward()\n                G_optimizer.step()\n\n                # Save relevant output for progress logging\n                G_losses.append(G_loss.item())\n\n                # PROPORTIONAL CONTROL THEORY: Dynamically update K,\n                # log convergence measure\n                convergence = (DX_loss+torch.abs(GAMMA*DX_loss-DG_loss)).item()\n                K_update = (K + LAMBDA*(GAMMA*DX_loss - DG_loss)).item()\n                K = min(max(0, K_update), 1)\n\n                # Learning rate scheduler\n                D_scheduler.step(convergence)\n                G_scheduler.step(convergence)\n\n            # Save losses\n            self.Glosses.extend(G_losses)\n            self.Dlosses.extend(D_losses)\n\n            # Progress logging\n            print (""Epoch[%d/%d], G Loss: %.4f, D Loss: %.4f, K: %.4f, Convergence Measure: %.4f""\n                   %(epoch, num_epochs, np.mean(G_losses),\n                     np.mean(D_losses), K, convergence))\n            self.num_epochs += 1\n\n            # Visualize generator progress\n            if self.viz:\n                self.generate_images(epoch)\n                plt.show()\n\n    def train_D(self, images, K):\n        """""" Run 1 step of training for discriminator\n\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n            K: how much to emphasize loss(D(G(z))) in total D loss\n        Output:\n            D_loss: BEGAN loss for discriminator,\n            E[||x-AE(x)||1] - K*E[G(z) - AE(G(z))]\n        """"""\n\n        # Reconstruct the images using D (autoencoder), get reconstruction loss\n        DX_reconst = self.model.D(images)\n        DX_loss = torch.mean(torch.sum(torch.abs(DX_reconst - images), dim=1))\n\n        # Sample outputs from the generator\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Reconstruct the generation using D (autoencoder)\n        DG_reconst = self.model.D(G_output)\n        DG_loss = torch.mean(torch.sum(torch.abs(DG_reconst - G_output), dim=1))\n\n        # Put it all together\n        D_loss = DX_loss - (K * DG_loss)\n\n        return D_loss, DX_loss, DG_loss\n\n    def train_G(self, images):\n        """""" Run 1 step of training for generator\n\n        Input:\n            images: batch of images reshaped to [batch_size, -1]\n        Output:\n            G_loss: BEGAN loss for G, E[||G(z) - AE(G(Z))||1]\n        """"""\n\n        # Get noise, classify it using G, then reconstruct the output of G\n        # using D (autoencoder).\n        noise = self.compute_noise(images.shape[0], self.model.z_dim) # z\n        G_output = self.model.G(noise) # G(z)\n        DG_reconst = self.model.D(G_output) # D(G(z))\n\n        # Reconstruct the generation using D\n        G_loss = torch.mean(torch.sum(torch.abs(DG_reconst - G_output), dim=1))\n\n        return G_loss\n\n    def compute_noise(self, batch_size, z_dim):\n        """""" Compute random noise for input into Generator G """"""\n        return to_cuda(torch.randn(batch_size, z_dim))\n\n    def process_batch(self, iterator):\n        """""" Generate a process batch to be input into the discriminator D """"""\n        images, _ = next(iter(iterator))\n        images = to_cuda(images.view(images.shape[0], -1))\n        return images\n\n    def generate_images(self, epoch, num_outputs=36, save=True):\n        """""" Visualize progress of generator learning """"""\n\n        # Turn off any regularization\n        self.model.eval()\n\n        # Sample noise vector\n        noise = self.compute_noise(num_outputs, self.model.z_dim)\n\n        # Transform noise to image\n        images = self.model.G(noise)\n\n        # Reshape to proper image size\n        images = images.view(images.shape[0],\n                             self.model.shape,\n                             self.model.shape,\n                             -1).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size = int(num_outputs**0.5)\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].cla()\n            ax[i,j].imshow(images[i+j].data.numpy(), cmap=\'gray\')\n\n        # Save images if desired\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(images.unsqueeze(1).data,\n                                         outname + \'reconst_%d.png\'\n                                         %(epoch), nrow=grid_size)\n\n    def viz_loss(self):\n        """""" Visualize loss for the generator, discriminator """"""\n\n        # Set style, figure size\n        plt.style.use(\'ggplot\')\n        plt.rcParams[""figure.figsize""] = (8,6)\n\n        # Plot Discriminator loss in red\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Dlosses,\n                 \'r\')\n\n        # Plot Generator loss in green\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Glosses,\n                 \'g\')\n\n        # Add legend, title\n        plt.legend([\'Discriminator\', \'Generator\'])\n        plt.title(self.name)\n        plt.show()\n\n    def save_model(self, savepath):\n        """""" Save model state dictionary """"""\n        torch.save(self.model.state_dict(), savepath)\n\n    def load_model(self, loadpath):\n        """""" Load state dictionary into model """"""\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)\n\n\nif __name__ == \'__main__\':\n\n    # Load in binarized MNIST data, separate into data loaders\n    train_iter, val_iter, test_iter = get_data()\n\n    # Init model\n    model = BEGAN(image_size=784,\n                  hidden_dim=400,\n                  z_dim=20)\n\n    # Init trainer\n    trainer = BEGANTrainer(model=model,\n                           train_iter=train_iter,\n                           val_iter=val_iter,\n                           test_iter=test_iter,\n                           viz=False)\n\n    # Train\n    trainer.train(num_epochs=25,\n                  G_lr=1e-4,\n                  D_lr=1e-4,\n                  D_steps=1)\n'"
src/bir_vae.py,16,"b'"""""" (BIR-VAE) https://arxiv.org/abs/1807.07306\nBounded Information Rate Variational Autoencoder\n\nThis VAE variant makes a slight change to the original formulation\nin an effort to enforce mutual information between our inputs x and the\nlatent space z. The change is setting the variance of q(z|x) instead of\nlearning it, which allows us to control the information rate across the\nchannel (Eqn. 7). It also implicity maximizes mutual information between\nx and z without direct computation subject to the constraint q(z)=N(0,I).\nThis happens when the Maximum Mean Discrepancy between q(z) and p(z) is\n0, and causes the mutual information term to reduce to a constant because\nthe differential entropy between h_q(z)[z] and h_q(z|x)[z] are both fixed\n(Eqn. 10/11). The output of the decode is the mean of the isotropic\nGaussian with variance 1, so the log likelihood reduced to the negative\nmean square error (i.e. we use MSELoss instead of NLLLoss).\n""""""\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.utils import make_grid\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy\n\nfrom tqdm import tqdm\nfrom itertools import product\n\nfrom utils import *\n\n\nclass Encoder(nn.Module):\n    """""" MLP encoder for VAE. Input is an image, outputs are the mean and std of\n    the latent representation z pre-reparametrization\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim):\n        super().__init__()\n\n        self.linear = nn.Linear(image_size, hidden_dim)\n        self.mu = nn.Linear(hidden_dim, z_dim)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        mu = self.mu(activated)\n        return mu\n\n\nclass Decoder(nn.Module):\n    """""" MLP decoder for VAE. Input is a reparametrized latent representation,\n    output is reconstructed image\n    """"""\n    def __init__(self, z_dim, hidden_dim, image_size):\n        super().__init__()\n\n        self.linear = nn.Linear(z_dim, hidden_dim)\n        self.recon = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, z):\n        activated = F.relu(self.linear(z))\n        reconstructed = torch.sigmoid(self.recon(activated))\n        return reconstructed\n\n\nclass BIRVAE(nn.Module):\n    """""" VAE super class to reconstruct an image. Contains reparametrization\n    method. Parameter I indicates how many \'bits\' should be let through.\n    """"""\n    def __init__(self, image_size=784, hidden_dim=400, z_dim=20, I=13.3):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.encoder = Encoder(image_size=image_size, hidden_dim=hidden_dim, z_dim=z_dim)\n        self.decoder = Decoder(z_dim=z_dim, hidden_dim=hidden_dim, image_size=image_size)\n\n        self.shape = int(image_size ** 0.5)\n        self.set_var = 1/(4**(I/z_dim))\n\n    def forward(self, x):\n        mu = self.encoder(x)\n        z = self.reparameterize(mu)\n        out_img = self.decoder(z)\n        return out_img, z\n\n    def reparameterize(self, mu):\n        """""""" Reparametrization trick: z = mean + epsilon, where epsilon ~ N(0, set_var).""""""\n        eps = to_cuda(torch.from_numpy(np.random.normal(loc=0.0,\n                                                        scale=self.set_var,\n                                                        size=mu.shape)).float())\n        z = mu + eps # Algorithm 1\n        return z\n\n\nclass BIRVAETrainer:\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False):\n        """""" Object to hold data iterators, train the model """"""\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.best_val_loss = 1e10\n        self.debugging_image, _ = next(iter(test_iter))\n        self.viz = viz\n\n        self.mmd_loss = []\n        self.recon_loss = []\n        self.num_epochs = 0\n\n    def train(self, num_epochs, lr=1e-3, weight_decay=1e-5):\n        """""" Train a Variational Autoencoder\n\n            Logs progress using total loss, reconstruction loss, maximum mean\n            discrepancy (MMD), and validation loss\n\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            lr: float, learning rate for Adam optimizer\n            weight_decay: float, weight decay for Adam optimizer\n        """"""\n        # Adam optimizer, sigmoid cross entropy for reconstructing binary MNIST\n        optimizer = optim.Adam(params=[p for p in self.model.parameters()\n                                       if p.requires_grad],\n                               lr=lr,\n                               weight_decay=weight_decay)\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs+1)):\n\n            self.model.train()\n            epoch_loss, epoch_recon, epoch_mmd = [], [], []\n\n            for batch in self.train_iter:\n\n                # Zero out gradients\n                optimizer.zero_grad()\n\n                # Compute mean squared error loss, mean maximum discrepancy loss\n                mse_loss, mmd_loss = self.compute_batch(batch)\n                batch_loss = mse_loss + mmd_loss\n\n                # Update parameters\n                batch_loss.backward()\n                optimizer.step()\n\n                # Log metrics\n                epoch_loss.append(batch_loss.item())\n                epoch_recon.append(mse_loss.item())\n                epoch_mmd.append(mmd_loss.item())\n\n            # Save progress\n            self.mmd_loss.extend(epoch_mmd)\n            self.recon_loss.extend(epoch_recon)\n\n            # Test the model on the validation set\n            self.model.eval()\n            val_loss = self.evaluate(self.val_iter)\n\n            # Early stopping\n            if val_loss < self.best_val_loss:\n                self.best_model = deepcopy(self.model)\n                self.best_val_loss = val_loss\n\n            # Progress logging\n            print (""Epoch[%d/%d], Total Loss: %.4f, MSE Loss: %.4f, MMD Loss: %.4f, Val Loss: %.4f""\n                   %(epoch, num_epochs, np.mean(epoch_loss), np.mean(epoch_recon), np.mean(epoch_mmd), val_loss))\n\n            # Debugging and visualization purposes\n            if self.viz:\n                self.sample_images(epoch)\n                plt.show()\n\n    def compute_batch(self, batch, LAMBDA=1000.):\n        """""" Compute loss for a batch of examples\n\n        LAMBDA: (float) a weighting factor for MMD loss vs. MSE loss\n        """"""\n\n        # Reshape images\n        images, _ = batch\n        images = to_cuda(images.view(images.shape[0], -1))\n\n        # Get output images, mean, std of encoded space\n        outputs, z = self.model(images)\n\n        # Mean squared error loss\n        mse_loss = torch.sum((images - outputs) ** 2)\n\n        # Maximum mean discrepancy\n        mmd_loss = LAMBDA * self.maximum_mean_discrepancy(z)\n\n        return mse_loss, mmd_loss\n\n    def maximum_mean_discrepancy(self, z):\n        """""" Maximum mean discrepancy of a Gaussian kernel """"""\n        x = torch.randn(z.shape)\n        x_kernel = self.compute_kernel(x, x)\n        y_kernel = self.compute_kernel(z, z)\n        xy_kernel = self.compute_kernel(x, z)\n        mmd_loss = x_kernel.sum() + y_kernel.sum() - 2*xy_kernel.sum()\n        return mmd_loss\n\n    def compute_kernel(self, x, y):\n        """""" Compute Gaussian kernel for MMD (Eqn. 13) """"""\n        # Get sizes, dimensions\n        x_size, y_size, dim = x.size(0), y.size(0), x.size(1)\n\n        # Unsqueeze and expand so we can compute element-wise operations\n        x, y = x.unsqueeze(1), y.unsqueeze(0)\n        tiled_x, tiled_y = x.expand(x_size, y_size, dim), y.expand(x_size, y_size, dim)\n\n        # Compute Gaussian Kernel (Eqn. 13)\n        kernel_input = torch.div(torch.mean(torch.pow(tiled_x-tiled_y, 2), dim=2), dim)\n        return torch.exp(-kernel_input)\n\n    def evaluate(self, iterator):\n        """""" Evaluate on a given dataset """"""\n        loss = []\n        for batch in iterator:\n            mse_loss, mmd_loss = self.compute_batch(batch)\n            batch_loss = mse_loss + mmd_loss\n            loss.append(batch_loss.item())\n\n        loss = np.mean(loss)\n        return loss\n\n    def reconstruct_images(self, images, epoch, save=True):\n        """""" Reconstruct a fixed input at each epoch for progress\n        visualization\n        """"""\n        # Reshape images, pass through model, reshape reconstructed output\n        batch = to_cuda(images.view(images.shape[0], -1))\n        reconst_images, _ = self.model(batch)\n        reconst_images = reconst_images.view(images.shape).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size, k = int(reconst_images.shape[0]**0.5), 0\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].imshow(reconst_images[k].data.numpy(), cmap=\'gray\')\n            k += 1\n\n        # Save\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(images.data,\n                                         outname + \'real.png\',\n                                         nrow=grid_size)\n            torchvision.utils.save_image(reconst_images.unsqueeze(1).data,\n                                         outname + \'reconst_%d.png\' %(epoch),\n                                         nrow=grid_size)\n\n    def sample_images(self, epoch=-100, num_images=36, save=True):\n        """""" Viz method 1: generate images by sampling z ~ p(z), x ~ p(x|z,\xce\xb8) """"""\n\n        # Sample z\n        z = to_cuda(torch.randn(num_images, self.model.z_dim))\n\n        # Pass into decoder\n        sample = self.model.decoder(z)\n\n        # Plot\n        to_img = ToPILImage()\n        img = to_img(make_grid(sample.data.view(num_images,\n                                                -1,\n                                                self.model.shape,\n                                                self.model.shape),\n                               nrow=int(num_images**0.5)))\n        display(img)\n\n        # Save\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            img.save(outname + \'sample_%d.png\' %(epoch))\n\n    def sample_interpolated_images(self):\n        """""" Viz method 2: sample two random latent vectors from p(z),\n        then sample from their interpolated values """"""\n\n        # Sample latent vectors\n        z1 = torch.normal(torch.zeros(self.model.z_dim), 1)\n        z2 = torch.normal(torch.zeros(self.model.z_dim), 1)\n        to_img = ToPILImage()\n\n        # Interpolate within latent vectors\n        for alpha in np.linspace(0, 1, self.model.z_dim):\n            z = to_cuda(alpha*z1 + (1-alpha)*z2)\n            sample = self.model.decoder(z)\n            display(to_img(make_grid(sample.data.view(-1,\n                                                      self.model.shape,\n                                                      self.model.shape))))\n\n    def explore_latent_space(self, num_epochs=3):\n        """""" Viz method 3: train a VAE with 2 latent variables,\n        compare variational means\n        """"""\n        # Initialize and train a VAE with size two dimension latent space\n        train_iter, val_iter, test_iter = get_data()\n        latent_model = BIRVAE(image_size=784, hidden_dim=400, z_dim=2, I=13.3)\n        latent_space = BIRVAETrainer(latent_model, train_iter, val_iter, test_iter)\n        latent_space.train(num_epochs)\n        latent_model = latent_space.best_model\n\n        # Across batches in train iter, collect variationa means\n        data = []\n        for batch in train_iter:\n            images, labels = batch\n            images = to_cuda(images.view(images.shape[0], -1))\n            mu = latent_model.encoder(images)\n\n            for label, (m1, m2) in zip(labels, mu):\n                data.append((label.item(), m1.item(), m2.item()))\n\n        # Plot\n        labels, m1s, m2s = zip(*data)\n        plt.figure(figsize=(10,10))\n        plt.scatter(m1s, m2s, c=labels)\n        plt.legend([str(i) for i in set(labels)])\n\n        # Evenly sample across latent space, visualize the outputs\n        mu = torch.stack([torch.FloatTensor([m1, m2])\n                          for m1 in np.linspace(-2, 2, 10)\n                          for m2 in np.linspace(-2, 2, 10)])\n        samples = latent_model.decoder(to_cuda(mu))\n        to_img = ToPILImage()\n        display(to_img(make_grid(samples.data.view(mu.shape[0],\n                                                   -1,\n                                                   latent_model.shape,\n                                                   latent_model.shape),\n                                 nrow=10)))\n\n        return latent_model\n\n    def make_all(self):\n        """""" Execute all viz methods outlined in this class """"""\n\n        print(\'Sampling images from latent space...\')\n        self.sample_images(save=False)\n\n        print(\'Interpolating between two randomly sampled...\')\n        self.sample_interpolated_images()\n\n        print(\'Exploring latent representations...\')\n        _ = self.explore_latent_space()\n\n    def viz_loss(self):\n        """""" Visualize reconstruction loss """"""\n        # Set style, figure size\n        plt.style.use(\'ggplot\')\n        plt.rcParams[""figure.figsize""] = (8,6)\n\n        # Plot reconstruction loss in red\n        plt.plot(np.linspace(1, self.num_epochs, len(self.recon_loss)),\n                 self.recon_loss,\n                 \'r\')\n\n        # Plot KL-divergence in green\n        plt.plot(np.linspace(1, self.num_epochs, len(self.mmd_loss)),\n                 self.mmd_loss,\n                 \'g\')\n\n        # Add legend, title\n        plt.legend([\'Reconstruction\', \'Maximum Mean Discrepancy\'])\n        plt.title(self.name)\n        plt.show()\n\n    def save_model(self, savepath):\n        """""" Save model state dictionary """"""\n        torch.save(self.model.state_dict(), savepath)\n\n    def load_model(self, loadpath):\n        """""" Load state dictionary into model """"""\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)\n\n\nif __name__ == ""__main__"":\n\n    # Load in binzarized MNIST data, separate into data loaders\n    train_iter, val_iter, test_iter = get_data()\n\n    # Init model\n    model = BIRVAE(image_size=784,\n                   hidden_dim=400,\n                   z_dim=20,\n                   I=13.3)\n\n    # Init trainer\n    trainer = BIRVAETrainer(model=model,\n                            train_iter=train_iter,\n                            val_iter=val_iter,\n                            test_iter=test_iter,\n                            viz=False)\n\n    # Train\n    trainer.train(num_epochs=10,\n                  lr=1e-3,\n                  weight_decay=1e-5)\n'"
src/dra_gan.py,17,"b'"""""" (DRAGAN) https://arxiv.org/abs/1705.07215\nDeep Regret Analytic GAN\n\nThe output of DRAGAN\'s D can be interpretted as a probability, similarly to\nMMGAN and NSGAN. DRAGAN is similar to WGANGP, but seems less stable.\n\nProposes to study GANs from a regret minimization perspective. This\nmodel is very similar to WGAN GP, in that it is applying a gradient penalty to\ntry and get at an improved training objective based on how D and G would\noptimally perform. They apply the gradient penalty only close to the real data\nmanifold (whereas WGAN GP picks the gradient location on a random line between\na real and randomly generated fake sample). For further details, see\nSection 2.5 of the paper.\n""""""\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom itertools import product\nfrom tqdm import tqdm\n\nfrom utils import *\n\n\nclass Generator(nn.Module):\n    """""" Generator. Input is noise, output is a generated image.\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim):\n        super().__init__()\n\n        self.linear = nn.Linear(z_dim, hidden_dim)\n        self.generate = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        generation = torch.sigmoid(self.generate(activated))\n        return generation\n\n\nclass Discriminator(nn.Module):\n    """""" Critic (not trained to classify). Input is an image (real or generated),\n    output is the approximate Wasserstein Distance between z~P(G(z)) and real.\n    """"""\n    def __init__(self, image_size, hidden_dim, output_dim):\n        super().__init__()\n\n        self.linear = nn.Linear(image_size, hidden_dim)\n        self.discriminate = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        discrimination = torch.sigmoid(self.discriminate(activated))\n        return discrimination\n\n\nclass DRAGAN(nn.Module):\n    """""" Super class to contain both Discriminator (D) and Generator (G)\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim, output_dim=1):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.G = Generator(image_size, hidden_dim, z_dim)\n        self.D = Discriminator(image_size, hidden_dim, output_dim)\n\n        self.shape = int(image_size ** 0.5)\n\n\nclass DRAGANTrainer:\n    """""" Object to hold data iterators, train a GAN variant\n    """"""\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False):\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.Glosses = []\n        self.Dlosses = []\n\n        self.viz = viz\n        self.num_epochs = 0\n\n    def train(self, num_epochs, G_lr=1e-4, D_lr=1e-4, D_steps=5):\n        """""" Train a Deep Regret Analytic GAN\n\n            Logs progress using G loss, D loss, G(x), D(G(x)),\n            visualizations of Generator output.\n\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            G_lr: float, learning rate for generator\'s Adam optimizer\n            D_lr: float, learning rate for discriminator\'s Adam optimizer\n            D_steps: int, ratio for how often to train D compared to G\n        """"""\n        # Initialize optimizers\n        G_optimizer = optim.Adam(params=[p for p in self.model.G.parameters()\n                                        if p.requires_grad], lr=G_lr)\n        D_optimizer = optim.Adam(params=[p for p in self.model.D.parameters()\n                                        if p.requires_grad], lr=D_lr)\n\n        # Approximate steps/epoch given D_steps per epoch\n        # --> roughly train in the same way as if D_step (1) == G_step (1)\n        epoch_steps = int(np.ceil(len(self.train_iter) / (D_steps)))\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs+1)):\n\n            self.model.train()\n            G_losses, D_losses = [], []\n\n            for _ in range(epoch_steps):\n\n                D_step_loss = []\n\n                for _ in range(D_steps):\n\n                    # Reshape images\n                    images = self.process_batch(self.train_iter)\n\n                    # TRAINING D: Zero out gradients for D\n                    D_optimizer.zero_grad()\n\n                    # Train the discriminator to approximate the Wasserstein\n                    # distance between real, generated distributions\n                    D_loss = self.train_D(images)\n\n                    # Update parameters\n                    D_loss.backward()\n                    D_optimizer.step()\n\n                    # Log results, backpropagate the discriminator network\n                    D_step_loss.append(D_loss.item())\n\n                # We report D_loss in this way so that G_loss and D_loss\n                # have the same number of entries.\n                D_losses.append(np.mean(D_step_loss))\n\n                # TRAINING G: Zero out gradients for G\n                G_optimizer.zero_grad()\n\n                # Train the generator\n                G_loss = self.train_G(images)\n\n                # Log results, update parameters\n                G_losses.append(G_loss.item())\n                G_loss.backward()\n                G_optimizer.step()\n\n            # Save progress\n            self.Glosses.extend(G_losses)\n            self.Dlosses.extend(D_losses)\n\n            # Progress logging\n            print (""Epoch[%d/%d], G Loss: %.4f, D Loss: %.4f""\n                   %(epoch, num_epochs, np.mean(G_losses), np.mean(D_losses)))\n            self.num_epochs += 1\n\n            # Visualize generator progress\n            if self.viz:\n                self.generate_images(epoch)\n                plt.show()\n\n    def train_D(self, images, LAMBDA=10, K=1, C=1):\n        """""" Run 1 step of training for discriminator\n\n        Input:\n            model: model instantiation\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            D_loss: DRAGAN loss for discriminator,\n            -E[log(D(x))] - E[log(1 - D(G(z)))] + \xce\xbbE[(||\xe2\x88\x87 D(G(z))|| - 1)^2]\n        """"""\n        # Classify the real batch images, get the loss for these\n        DX_score = self.model.D(images)\n\n        # Sample noise z, generate output G(z)\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Classify generated batch images\n        DG_score = self.model.D(G_output)\n\n        # Compute vanilla (original paper) D loss\n        D_loss = -torch.mean(torch.log(DX_score + 1e-8) \\\n                    + torch.log(1 - DG_score + 1e-8))\n\n        # GRADIENT PENALTY STEPS:\n        # Uniformly sample along one straight line per each batch entry.\n        delta = to_cuda(torch.rand(images.shape[0], 1).expand(images.size()))\n\n        # Generate images from the noise, ensure unit\n        G_interpolation = to_var(delta*images.data + (1-delta) *\n                                 (images.data + C*images.data.std() \\\n                                 * to_cuda(torch.rand(images.size()))))\n\n        # Discriminate generator interpolation\n        D_interpolation = self.model.D(G_interpolation)\n        interp_shape = D_interpolation.shape\n\n        # Compute the gradients of D with respect to the noise generated input\n        gradients = torch.autograd.grad(outputs=D_interpolation,\n                                        inputs=G_interpolation,\n                                        grad_outputs=to_cuda(torch.ones(interp_shape)),\n                                        only_inputs=True,\n                                        create_graph=True,\n                                        retain_graph=True)[0]\n\n        # Full gradient penalty\n        grad_penalty = LAMBDA*torch.mean((gradients.norm(2, dim=1)-K)**2)\n\n        # Compute DRAGAN loss for D\n        D_loss = D_loss + grad_penalty\n\n        return D_loss\n\n    def train_G(self, images):\n        """""" Run 1 step of training for generator\n\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            G_loss: DRAGAN (non-saturating) loss for G, -E[log(D(G(z)))]\n        """"""\n        # Get noise (denoted z), classify it using G, then classify the\n        # output of G using D.\n        noise = self.compute_noise(images.shape[0], self.model.z_dim) # z\n        G_output = self.model.G(noise) # G(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute the non-saturating loss for how D did versus the generations\n        # of G using sigmoid cross entropy\n        G_loss = -torch.mean(torch.log(DG_score + 1e-8))\n\n        return G_loss\n\n    def compute_noise(self, batch_size, z_dim):\n        """""" Compute random noise for input into Generator G """"""\n        return to_cuda(torch.randn(batch_size, z_dim))\n\n    def process_batch(self, iterator):\n        """""" Generate a process batch to be input into the Discriminator D """"""\n        images, _ = next(iter(iterator))\n        images = to_cuda(images.view(images.shape[0], -1))\n        return images\n\n    def generate_images(self, epoch, num_outputs=36, save=True):\n        """""" Visualize progress of generator learning """"""\n        # Turn off any regularization\n        self.model.eval()\n\n        # Sample noise vector\n        noise = self.compute_noise(num_outputs, self.model.z_dim)\n\n        # Transform noise to image\n        images = self.model.G(noise)\n\n        # Reshape to proper image size\n        images = images.view(images.shape[0],\n                             self.model.shape,\n                             self.model.shape,\n                             -1).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size, k = int(num_outputs**0.5), 0\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].imshow(images[k].data.numpy(), cmap=\'gray\')\n            k += 1\n\n        # Save images if desired\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(images.unsqueeze(1).data,\n                                         outname + \'reconst_%d.png\'\n                                         %(epoch), nrow=grid_size)\n\n    def viz_loss(self):\n        """""" Visualize loss for the generator, discriminator """"""\n        # Set style, figure size\n        plt.style.use(\'ggplot\')\n        plt.rcParams[""figure.figsize""] = (8,6)\n\n        # Plot Discriminator loss in red\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Dlosses,\n                 \'r\')\n\n        # Plot Generator loss in green\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Glosses,\n                 \'g\')\n\n        # Add legend, title\n        plt.legend([\'Discriminator\', \'Generator\'])\n        plt.title(self.name)\n        plt.show()\n\n    def save_model(self, savepath):\n        """""" Save model state dictionary """"""\n        torch.save(self.model.state_dict(), savepath)\n\n    def load_model(self, loadpath):\n        """""" Load state dictionary into model """"""\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)\n\n\nif __name__ == ""__main__"":\n    # Load in binarized MNIST data, separate into data loaders\n    train_iter, val_iter, test_iter = get_data()\n\n    # Init model\n    model = DRAGAN(image_size=784,\n                   hidden_dim=400,\n                   z_dim=20)\n\n    # Init trainer\n    trainer = DRAGANTrainer(model=model,\n                            train_iter=train_iter,\n                            val_iter=val_iter,\n                            test_iter=test_iter,\n                            viz=False)\n\n    # Train\n    trainer.train(num_epochs=65,\n                  G_lr=1e-4,\n                  D_lr=1e-4,\n                  D_steps=1)\n'"
src/f_gan.py,24,"b'"""""" (f-GAN) https://arxiv.org/abs/1606.00709\nf-Divergence GANs\n\nThe authors empirically demonstrate that when the generative model is\nmisspecified and does not contain the true distribution, the divergence\nfunction used for estimation has a strong influence on which model is\nlearned. To address this issue, they theoretically show that the\ngenerative-adversarial approach is a special case of an existing, more\ngeneral variational divergence estimation approach and that any\nf-divergence can be used for training generative neural samplers (which\nare defined as models that take a random input vector and produce a sample\nfrom a probability distribution defined by the network weights). They\nthen empirically show the effect of using different training\ndivergences on a trained model\'s average log likelihood of sampled data.\n\nThey test (forward) Kullback-Leibler, reverse Kullback-Leibler, Pearson\nchi-squared, Neyman chi-squared, squared Hellinger, Jensen-Shannon,\nand Jeffrey divergences.\n\nWe exclude Neyman and Jeffrey due to poor performance and nontrivial\nimplementations to yield \'convergence\' (see scipy.special.lambertw\nfor how to implement Jeffrey, and Table 6 of Appendix C of the paper\nfor how to implement Neyman)\n""""""\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom itertools import product\nfrom tqdm import tqdm\n\nfrom utils import *\n\n\nclass Generator(nn.Module):\n    """""" Generator. Input is noise, output is a generated image.\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim):\n        super().__init__()\n        self.linear = nn.Linear(z_dim, hidden_dim)\n        self.generate = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        generation = torch.sigmoid(self.generate(activated))\n        return generation\n\n\nclass Discriminator(nn.Module):\n    """""" Discriminator. Input is an image (real or generated),\n    output is P(generated).\n    """"""\n    def __init__(self, image_size, hidden_dim, output_dim):\n        super().__init__()\n        self.linear = nn.Linear(image_size, hidden_dim)\n        self.discriminate = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        discrimination = torch.sigmoid(self.discriminate(activated))\n        return discrimination\n\n\nclass fGAN(nn.Module):\n    """""" Super class to contain both Discriminator (D) and Generator (G)\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim, output_dim=1):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.G = Generator(image_size, hidden_dim, z_dim)\n        self.D = Discriminator(image_size, hidden_dim, output_dim)\n\n        self.shape = int(image_size ** 0.5)\n\n\nclass Divergence:\n    """""" Compute G and D loss using an f-divergence metric.\n    Implementations based on Table 6 (Appendix C) of the arxiv paper.\n    """"""\n    def __init__(self, method):\n        self.method = method.lower().strip()\n        assert self.method in [\'total_variation\',\n                               \'forward_kl\',\n                               \'reverse_kl\',\n                               \'pearson\',\n                               \'hellinger\',\n                               \'jensen_shannon\'], \\\n            \'Invalid divergence.\'\n\n    def D_loss(self, DX_score, DG_score):\n        """""" Compute batch loss for discriminator using f-divergence metric """"""\n\n        if self.method == \'total_variation\':\n            return -(torch.mean(0.5*torch.tanh(DX_score)) \\\n                        - torch.mean(0.5*torch.tanh(DG_score)))\n\n        elif self.method == \'forward_kl\':\n            return -(torch.mean(DX_score) - torch.mean(torch.exp(DG_score-1)))\n\n        elif self.method == \'reverse_kl\':\n            return -(torch.mean(-torch.exp(DX_score)) - torch.mean(-1-DG_score))\n\n        elif self.method == \'pearson\':\n            return -(torch.mean(DX_score) - torch.mean(0.25*DG_score**2 + DG_score))\n\n        elif self.method == \'hellinger\':\n            return -(torch.mean(1-torch.exp(DX_score)) \\\n                        - torch.mean((1-torch.exp(DG_score))/(torch.exp(DG_score))))\n\n        elif self.method == \'jensen_shannon\':\n            return -(torch.mean(torch.tensor(2.)-(1+torch.exp(-DX_score))) \\\n                        - torch.mean(-(torch.tensor(2.)-torch.exp(DG_score))))\n\n    def G_loss(self, DG_score):\n        """""" Compute batch loss for generator using f-divergence metric """"""\n\n        if self.method == \'total_variation\':\n            return -torch.mean(0.5*torch.tanh(DG_score))\n\n        elif self.method == \'forward_kl\':\n            return -torch.mean(torch.exp(DG_score-1))\n\n        elif self.method == \'reverse_kl\':\n            return -torch.mean(-1-DG_score)\n\n        elif self.method == \'pearson\':\n            return -torch.mean(0.25*DG_score**2 + DG_score)\n\n        elif self.method == \'hellinger\':\n            return -torch.mean((1-torch.exp(DG_score))/(torch.exp(DG_score)))\n\n        elif self.method == \'jensen_shannon\':\n            return -torch.mean(-(torch.tensor(2.)-torch.exp(DG_score)))\n\n\nclass fGANTrainer:\n    """""" Object to hold data iterators, train a GAN variant\n    """"""\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False):\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.Glosses = []\n        self.Dlosses = []\n\n        self.viz = viz\n        self.num_epochs = 0\n\n    def train(self, num_epochs, method, G_lr=1e-4, D_lr=1e-4, D_steps=1):\n        """""" Train a standard vanilla GAN architecture using f-divergence as loss\n\n            Logs progress using G loss, D loss, G(x), D(G(x)), visualizations\n            of Generator output.\n\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            method: str, divergence metric to optimize\n            G_lr: float, learning rate for generator\'s Adam optimizer\n            D_lr: float, learning rate for discriminsator\'s Adam optimizer\n            D_steps: int, ratio for how often to train D compared to G\n        """"""\n        # Initialize loss, indicate which GAN it is\n        self.loss_fnc = Divergence(method)\n\n        # Initialize optimizers\n        G_optimizer = optim.Adam(params=[p for p in self.model.G.parameters()\n                                        if p.requires_grad], lr=G_lr)\n        D_optimizer = optim.Adam(params=[p for p in self.model.D.parameters()\n                                        if p.requires_grad], lr=D_lr)\n\n        # Approximate steps/epoch given D_steps per epoch\n        # --> roughly train in the same way as if D_step (1) == G_step (1)\n        epoch_steps = int(np.ceil(len(self.train_iter) / (D_steps)))\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs+1)):\n\n            self.model.train()\n            G_losses, D_losses = [], []\n\n            for _ in range(epoch_steps):\n\n                D_step_loss = []\n\n                for _ in range(D_steps):\n\n                    # Reshape images\n                    images = self.process_batch(self.train_iter)\n\n                    # TRAINING D: Zero out gradients for D\n                    D_optimizer.zero_grad()\n\n                    # Train D to discriminate between real and generated images\n                    D_loss = self.train_D(images)\n\n                    # Update parameters\n                    D_loss.backward()\n                    D_optimizer.step()\n\n                    # Log results, backpropagate the discriminator network\n                    D_step_loss.append(D_loss.item())\n\n                # So that G_loss and D_loss have the same number of entries.\n                D_losses.append(np.mean(D_step_loss))\n\n                # TRAINING G: Zero out gradients for G\n                G_optimizer.zero_grad()\n\n                # Train G to generate images that fool the discriminator\n                G_loss = self.train_G(images)\n\n                # Log results, update parameters\n                G_losses.append(G_loss.item())\n                G_loss.backward()\n                G_optimizer.step()\n\n            # Save progress\n            self.Glosses.extend(G_losses)\n            self.Dlosses.extend(D_losses)\n\n            # Progress logging\n            print (""Epoch[%d/%d], G Loss: %.4f, D Loss: %.4f""\n                   %(epoch, num_epochs, np.mean(G_losses), np.mean(D_losses)))\n            self.num_epochs += 1\n\n            # Visualize generator progress\n            if self.viz:\n                self.generate_images(epoch)\n                plt.show()\n\n    def train_D(self, images):\n        """""" Run 1 step of training for discriminator\n\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            D_loss: f-divergence between generated, true distributions\n        """"""\n        # Classify the real batch images, get the loss for these\n        DX_score = self.model.D(images)\n\n        # Sample noise z, generate output G(z)\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Classify the fake batch images, get the loss for these using sigmoid cross entropy\n        DG_score = self.model.D(G_output)\n\n        # Compute f-divergence loss\n        D_loss = self.loss_fnc.D_loss(DX_score, DG_score)\n\n        return D_loss\n\n    def train_G(self, images):\n        """""" Run 1 step of training for generator\n\n        Input:\n            images: batch of images reshaped to [batch_size, -1]\n        Output:\n            G_loss: f-divergence for difference between generated, true distributiones\n        """"""\n        # Get noise (denoted z), classify it using G, then classify the output\n        # of G using D.\n        noise = self.compute_noise(images.shape[0], self.model.z_dim) # z\n        G_output = self.model.G(noise) # G(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute f-divergence loss\n        G_loss = self.loss_fnc.G_loss(DG_score)\n\n        return G_loss\n\n    def compute_noise(self, batch_size, z_dim):\n        """""" Compute random noise for input into Generator G """"""\n        return to_cuda(torch.randn(batch_size, z_dim))\n\n    def process_batch(self, iterator):\n        """""" Generate a process batch to be input into the Discriminator D """"""\n        images, _ = next(iter(iterator))\n        images = to_cuda(images.view(images.shape[0], -1))\n        return images\n\n    def generate_images(self, epoch, num_outputs=36, save=True):\n        """""" Visualize progress of generator learning """"""\n        # Turn off any regularization\n        self.model.eval()\n\n        # Sample noise vector\n        noise = self.compute_noise(num_outputs, self.model.z_dim)\n\n        # Transform noise to image\n        images = self.model.G(noise)\n\n        # Reshape to proper image size\n        images = images.view(images.shape[0],\n                             self.model.shape,\n                             self.model.shape,\n                             -1).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size, k = int(num_outputs**0.5), 0\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].imshow(images[k].data.numpy(), cmap=\'gray\')\n            k += 1\n\n        # Save images if desired\n        if save:\n            outname = \'../viz/\' + self.name + \'/\' + self.loss_fnc.method + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(images.unsqueeze(1).data,\n                                         outname + \'reconst_%d.png\'\n                                         %(epoch), nrow=grid_size)\n\n    def viz_loss(self):\n        """""" Visualize loss for the generator, discriminator """"""\n        # Set style, figure size\n        plt.style.use(\'ggplot\')\n        plt.rcParams[""figure.figsize""] = (8,6)\n\n        # Plot Discriminator loss in red\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Dlosses,\n                 \'r\')\n\n        # Plot Generator loss in green\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Glosses,\n                 \'g\')\n\n        # Add legend, title\n        plt.legend([\'Discriminator\', \'Generator\'])\n        plt.title(self.name + \' : \' + self.loss_fnc.method)\n        plt.show()\n\n    def save_model(self, savepath):\n        """""" Save model state dictionary """"""\n        torch.save(self.model.state_dict(), savepath)\n\n    def load_model(self, loadpath):\n        """""" Load state dictionary into model """"""\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)\n\n\nif __name__ == \'__main__\':\n\n    # Load in binarized MNIST data, separate into data loaders\n    train_iter, val_iter, test_iter = get_data()\n\n    # Init model\n    model = fGAN(image_size=784,\n                 hidden_dim=400,\n                 z_dim=20)\n\n    # Init trainer\n    trainer = fGANTrainer(model=model,\n                          train_iter=train_iter,\n                          val_iter=val_iter,\n                          test_iter=test_iter,\n                          viz=False)\n    # Train\n    trainer.train(num_epochs=25,\n                  method=\'jensen_shannon\',\n                  G_lr=1e-4,\n                  D_lr=1e-4,\n                  D_steps=1)\n'"
src/fisher_gan.py,13,"b'"""""" (FisherGAN) https://arxiv.org/abs/1606.07536\nFisher GAN\n\nFrom the abstract:\n""In this paper we introduce Fisher GAN which fits within the\nIntegral Probability Metrics (IPM) framework for training GANs.\nFisher GAN defines a critic with a data dependent constraint on\nits second order moments. We show in this paper that Fisher GAN\nallows for stable and time efficient training that does not\ncompromise the capacity of the critic, and does not need data\nindependent constraints such as weight clipping.""\n\nIntegral Probability Metrics (IPM) framework simply means that\nthe outputs of the discriminator can be interpretted\nprobabilistically. This is similar to WGAN/WGAN-GP. Whereas\nWGAN-GP uses a penalty on the gradients of the critic, FisherGAN\nimposes a constraint on the second order moments of the critic.\nAlso, the Fisher IPM corresponds to the Chi-squared distance\nbetween distributions.\n\nThe main empirical claims are that FisherGAN yields better\ninception scores and has less computational overhead than WGAN.\n""""""\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom itertools import product\nfrom tqdm import tqdm\n\nfrom utils import *\n\n\nclass Generator(nn.Module):\n    """""" Generator. Input is noise, output is a generated image.\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim):\n        super().__init__()\n        self.linear = nn.Linear(z_dim, hidden_dim)\n        self.generate = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        generation = torch.sigmoid(self.generate(activated))\n        return generation\n\n\nclass Discriminator(nn.Module):\n    """""" Discriminator. Input is an image (real or generated),\n    output is P(generated).\n    """"""\n    def __init__(self, image_size, hidden_dim, output_dim):\n        super().__init__()\n        self.linear = nn.Linear(image_size, hidden_dim)\n        self.discriminate = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        discrimination = torch.sigmoid(self.discriminate(activated))\n        return discrimination\n\n\nclass FisherGAN(nn.Module):\n    """""" Super class to contain both Discriminator (D) and Generator (G)\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim, output_dim=1):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.G = Generator(image_size, hidden_dim, z_dim)\n        self.D = Discriminator(image_size, hidden_dim, output_dim)\n\n        self.shape = int(image_size ** 0.5)\n\n\nclass FisherGANTrainer:\n    """""" Object to hold data iterators, train a GAN variant\n    """"""\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False):\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.Glosses = []\n        self.Dlosses = []\n\n        self.viz = viz\n        self.num_epochs = 0\n\n    def train(self, num_epochs, G_lr=1e-4, D_lr=1e-4, D_steps=1, RHO=1e-6):\n        """""" Train FisherGAN using IPM framework\n\n            Logs progress using G loss, D loss, G(x), D(G(x)),\n            IPM ratio (want close to 0.50), Lambda (want close to 0),\n            and visualizations of Generator output.\n\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            G_lr: float, learning rate for generator\'s Adam optimizer\n            D_lr: float, learning rate for discriminator\'s Adam optimizer\n            D_steps: int, ratio for how often to train D compared to G\n            LAMBDA: float, initial weight on constraint term\n            RHO: float, quadratic penalty weight\n        """"""\n        # Initialize alpha\n        self.LAMBDA = to_var(torch.zeros(1))\n        self.RHO = to_var(torch.tensor(RHO))\n\n        # Initialize optimizers\n        G_optimizer = optim.Adam(params=[p for p in self.model.G.parameters()\n                                        if p.requires_grad], lr=G_lr)\n        D_optimizer = optim.Adam(params=[p for p in self.model.D.parameters()\n                                        if p.requires_grad], lr=D_lr)\n\n        # Approximate steps/epoch given D_steps per epoch\n        # --> roughly train in the same way as if D_step (1) == G_step (1)\n        epoch_steps = int(np.ceil(len(self.train_iter) / (D_steps)))\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs+1)):\n\n            self.model.train()\n            G_losses, D_losses = [], []\n\n            for _ in range(epoch_steps):\n\n                D_step_loss = []\n\n                for _ in range(D_steps):\n\n                    # Reshape images\n                    images = self.process_batch(self.train_iter)\n\n                    # TRAINING D: Zero out gradients for D\n                    D_optimizer.zero_grad()\n\n                    # Train D to discriminate between real and generated images\n                    D_loss, IPM_ratio = self.train_D(images)\n\n                    # Update parameters\n                    D_loss.backward()\n\n                    # Minimize lambda for \'artisinal SGD\'\n                    self.LAMBDA = self.LAMBDA + self.RHO*self.LAMBDA.grad\n                    self.LAMBDA = to_var(self.LAMBDA.detach())\n\n                    # Now step optimizer\n                    D_optimizer.step()\n\n                    # Log results, backpropagate the discriminator network\n                    D_step_loss.append(D_loss.item())\n\n                # So that G_loss and D_loss have the same number of entries.\n                D_losses.append(np.mean(D_step_loss))\n\n                # TRAINING G: Zero out gradients for G\n                G_optimizer.zero_grad()\n\n                # Train the Generator to fool the discriminator\n                G_loss = self.train_G(images)\n\n                # Log results, update parameters\n                G_losses.append(G_loss.item())\n                G_loss.backward()\n                G_optimizer.step()\n\n            # Save progress\n            self.Glosses.extend(G_losses)\n            self.Dlosses.extend(D_losses)\n\n            # Progress logging\n            print (""Epoch[%d/%d], G Loss: %.4f, D Loss: %.4f, IPM ratio: %.4f, Lambda: %.4f""\n                   %(epoch, num_epochs, np.mean(G_losses), np.mean(D_losses),\n                     IPM_ratio, self.LAMBDA))\n            self.num_epochs += 1\n\n            # Visualize generator progress\n            if self.viz:\n                self.generate_images(epoch)\n                plt.show()\n\n    def train_D(self, images):\n        """""" Run 1 step of training for discriminator\n\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            D_loss: FisherGAN IPM loss (Equation 9 of paper)\n        """"""\n        # Generate labels (ones indicate real images, zeros indicate generated)\n        X_labels = to_cuda(torch.ones(images.shape[0], 1))\n        G_labels = to_cuda(torch.zeros(images.shape[0], 1))\n\n        # Classify the real batch images, get the loss for these\n        DX_score = self.model.D(images)\n\n        # Sample noise z, generate output G(z), discriminate D(G(z))\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n        DG_score = self.model.D(G_output)\n\n        # First and second order central moments (Gaussian assumed)\n        DX_moment_1, DG_moment_1  = DX_score.mean(), DG_score.mean()\n        DX_moment_2, DG_moment_2 = (DX_score**2).mean(), (DG_score**2).mean()\n\n        # Compute constraint on second order moments\n        OMEGA = 1 - (0.5*DX_moment_2 + 0.5*DG_moment_2)\n\n        # Compute loss (Eqn. 9)\n        D_loss = -((DX_moment_1-DG_moment_1) \\\n                    + self.LAMBDA*OMEGA \\\n                    - (self.RHO/2)*(OMEGA**2))\n\n        # For progress logging\n        IPM_ratio = DX_moment_1.item() - DG_moment_1.item() \\\n                    / 0.5*(DX_moment_2.item() - DG_moment_2.item())**0.5\n\n        return D_loss, IPM_ratio\n\n    def train_G(self, images):\n        """""" Run 1 step of training for generator\n\n        Input:\n            images: batch of images reshaped to [batch_size, -1]\n        Output:\n            G_loss: FisherGAN IPM loss (Equation 9 of paper)\n        """"""\n        # Get noise (denoted z), classify it using G, then classify the output\n        # of G using D.\n        noise = self.compute_noise(images.shape[0], self.model.z_dim) # z\n        G_output = self.model.G(noise) # G(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute loss by minimizing mean difference\n        G_loss = -DG_score.mean()\n\n        return G_loss\n\n    def compute_noise(self, batch_size, z_dim):\n        """""" Compute random noise for input into the Generator G """"""\n        return to_cuda(torch.randn(batch_size, z_dim))\n\n    def process_batch(self, iterator):\n        """""" Generate a process batch to be input into the Discriminator D """"""\n        images, _ = next(iter(iterator))\n        images = to_cuda(images.view(images.shape[0], -1))\n        return images\n\n    def generate_images(self, epoch, num_outputs=36, save=True):\n        """""" Visualize progress of generator learning """"""\n        # Turn off any regularization\n        self.model.eval()\n\n        # Sample noise vector\n        noise = self.compute_noise(num_outputs, self.model.z_dim)\n\n        # Transform noise to image\n        images = self.model.G(noise)\n\n        # Reshape to proper image size\n        images = images.view(images.shape[0],\n                             self.model.shape,\n                             self.model.shape,\n                             -1).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size, k = int(num_outputs**0.5), 0\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].imshow(images[k].data.numpy(), cmap=\'gray\')\n            k += 1\n\n        # Save images if desired\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(images.unsqueeze(1).data,\n                                         outname + \'reconst_%d.png\'\n                                         %(epoch), nrow=grid_size)\n\n    def viz_loss(self):\n        """""" Visualize loss for the generator, discriminator """"""\n        # Set style, figure size\n        plt.style.use(\'ggplot\')\n        plt.rcParams[""figure.figsize""] = (8,6)\n\n        # Plot Discriminator loss in red\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Dlosses,\n                 \'r\')\n\n        # Plot Generator loss in green\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Glosses,\n                 \'g\')\n\n        # Add legend, title\n        plt.legend([\'Discriminator\', \'Generator\'])\n        plt.title(self.name)\n        plt.show()\n\n    def save_model(self, savepath):\n        """""" Save model state dictionary """"""\n        torch.save(self.model.state_dict(), savepath)\n\n    def load_model(self, loadpath):\n        """""" Load state dictionary into model """"""\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)\n\n\nif __name__ == \'__main__\':\n    # Load in binarized MNIST data, separate into data loaders\n    train_iter, val_iter, test_iter = get_data()\n\n    # Init model\n    model = FisherGAN(image_size=784,\n                      hidden_dim=400,\n                      z_dim=20)\n    # Init trainer\n    trainer = FisherGANTrainer(model=model,\n                               train_iter=train_iter,\n                               val_iter=val_iter,\n                               test_iter=test_iter,\n                               viz=False)\n\n    # Train\n    trainer.train(num_epochs=25,\n                  G_lr=1e-4,\n                  D_lr=1e-4,\n                  D_steps=1)\n'"
src/info_gan.py,19,"b'"""""" (InfoGAN) https://arxiv.org/abs/1606.03657\nInformation GAN\n\nFrom the paper:\n\n""In this paper, we present a simple modification to the generative adversarial\nnetwork objective that encourages it to learn interpretable and meaningful\nrepresentations. We do so by maximizing the mutual information between a fixed\nsmall subset of the GAN\xe2\x80\x99s noise variables and the observations, which turns out\nto be relatively straightforward. Despite its simplicity, we found our method\nto be surprisingly effective: it was able to discover highly semantic and\nmeaningful hidden representations on a number of image datasets: digits (MNIST),\nfaces (CelebA), and house numbers (SVHN).""\n\nThe Generator input is split into two parts: a traditional ""noise"" vector (z)\nand a latent ""code\xe2\x80\x9d vector (c) that targets the salient structured semantic\nfeatures of the data distribution. These vectors are made meaningful by\nmaximizing the mutual information lower bound between c and the G(c, z).\nSince mutual information is inefficient to compute directly, we estimate it\nusing an auxiliary network Q.\n\nThe auxiliary network Q(c|x) approximates P(c|x), the true posterior. We use\nthis to compute the mutual information by sampling c from our assumed prior\nP(c), sampling a noise vector z, using them both to sample  x ~ G(c, z), and\nthen passing x to Q(c|x). We then use Q(c|x) to maximize the mutual information\nbetween c and G(z, c) and backpropagate its estimate back to both G and Q.\n""""""\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom itertools import product\nfrom tqdm import tqdm\n\nfrom utils import *\n\n\nclass Generator(nn.Module):\n    """""" Generator. Input is noise and latent variables, output is a generated\n    image.\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim, disc_dim, cont_dim):\n        super().__init__()\n        self.linear = nn.Linear(z_dim + disc_dim + cont_dim, hidden_dim)\n        self.generate = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        generation = torch.sigmoid(self.generate(activated))\n        return generation\n\n\nclass Discriminator(nn.Module):\n    """""" Discriminator. Input is an image (real or generated), output is\n    P(generated), continuous latent variables, discrete latent variables.\n    """"""\n    def __init__(self, image_size, hidden_dim, output_dim):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.linear = nn.Linear(image_size, hidden_dim)\n        self.discriminator = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        discrimination = torch.sigmoid(self.discriminator(activated))\n        return discrimination\n\n\nclass Q(nn.Module):\n    """""" Auxiliary network Q(c|x) that approximates P(c|x), the true posterior.\n    Input is an image, output are latent variables.\n    """"""\n    def __init__(self, image_size, hidden_dim, disc_dim, cont_dim):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.linear = nn.Linear(image_size, hidden_dim)\n        self.inference = nn.Linear(hidden_dim, disc_dim+cont_dim)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        inferred = self.inference(activated)\n        discrete, continuous = inferred[:, :self.disc_dim], inferred[:, self.disc_dim:]\n        return discrete, continuous\n\n\nclass InfoGAN(nn.Module):\n    """""" Super class to contain both Discriminator (D) and Generator (G)\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim, disc_dim, cont_dim, output_dim=1):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.G = Generator(image_size, hidden_dim, z_dim, disc_dim, cont_dim)\n        self.D = Discriminator(image_size, hidden_dim, output_dim)\n        self.Q = Q(image_size, hidden_dim, disc_dim, cont_dim)\n\n        self.shape = int(image_size ** 0.5)\n\n\nclass InfoGANTrainer:\n    """""" Object to hold data iterators, train a GAN variant\n    """"""\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False):\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.Glosses = []\n        self.Dlosses = []\n        self.MIlosses = []\n\n        self.viz = viz\n        self.num_epochs = 0\n\n    def train(self, num_epochs, G_lr=2e-4, D_lr=2e-4, D_steps=1):\n        """""" Train InfoGAN using the non-saturating setup from vanilla GAN.\n            Logs progress using G loss, D loss, G(x), D(G(x)),\n            visualizations of Generator output.\n\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            G_lr: float, learning rate for generator\'s Adam optimizer (default 2e-4)\n            D_lr: float, learning rate for discriminator\'s Adam optimizer (default 2e-4)\n            D_steps: int, training step ratio for how often to train D compared to G (default 1)\n        """"""\n        # Initialize optimizers\n        parameters = {\'D\': [p for p in self.model.D.parameters() if p.requires_grad],\n                      \'G\': [p for p in self.model.G.parameters() if p.requires_grad],\n                      \'Q\': [p for p in self.model.Q.parameters() if p.requires_grad]}\n\n        D_optimizer = optim.Adam(params=parameters[\'D\'], lr=D_lr)\n        G_optimizer = optim.Adam(params=parameters[\'G\'], lr=G_lr)\n        MI_optimizer = optim.Adam(params=(parameters[\'G\']+parameters[\'Q\']), lr=G_lr)\n\n        # Approximate steps/epoch given D_steps per epoch\n        # --> roughly train in the same way as if D_step (1) == G_step (1)\n        epoch_steps = int(np.ceil(len(self.train_iter) / (D_steps)))\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs+1)):\n\n            self.model.train()\n            G_losses, D_losses, MI_losses = [], [], []\n\n            for _ in range(epoch_steps):\n\n                D_step_loss = []\n\n                for _ in range(D_steps):\n\n                    # Reshape images\n                    images = self.process_batch(self.train_iter)\n\n                    # TRAINING D: Zero out gradients for D\n                    D_optimizer.zero_grad()\n\n                    # Learn to discriminate between real and generated images\n                    D_loss = self.train_D(images)\n\n                    # Update parameters\n                    D_loss.backward()\n                    D_optimizer.step()\n\n                    # Log results, backpropagate the discriminator network\n                    D_step_loss.append(D_loss.item())\n\n                # So that G_loss and D_loss have the same number of entries.\n                D_losses.append(np.mean(D_step_loss))\n\n                # TRAINING G: Zero out gradients for G\n                G_optimizer.zero_grad()\n\n                # Learn to generate images that fool the discriminator\n                G_loss = self.train_G(images)\n\n                # Log results, update parameters\n                G_losses.append(G_loss.item())\n                G_loss.backward()\n                G_optimizer.step()\n\n                # TRAINING Q: Zero out gradients for Q\n                MI_optimizer.zero_grad()\n\n                # Learn to estimate Mutual Information\n                MI_loss = self.train_Q(images)\n\n                # Update parameters\n                MI_losses.append(MI_loss.item())\n                MI_loss.backward()\n                MI_optimizer.step()\n\n            # Save progress\n            self.Glosses.extend(G_losses)\n            self.Dlosses.extend(D_losses)\n            self.MIlosses.extend(MI_losses)\n\n            # Progress logging\n            print (""Epoch[%d/%d], G Loss: %.4f, D Loss: %.4f, MI Loss: %.4f""\n                   %(epoch, num_epochs, np.mean(G_losses),\n                     np.mean(D_losses), np.mean(MI_losses)))\n            self.num_epochs += 1\n\n            # Visualize generator progress\n            if self.viz:\n                self.generate_images(epoch)\n                plt.show()\n\n    def train_D(self, images):\n        """""" Run 1 step of training for discriminator\n\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            D_loss: InfoGAN non-saturating loss for discriminator\n            -E[log(D(x))] - E[log(1 - D(G(c, z)))]\n        """"""\n        # Classify the real batch images, get the loss for these\n        DX_score = self.model.D(images)\n\n        # Sample noise z, generate output G(c, z)\n        noise = self.compute_noise(images.shape[0], self.model.z_dim,\n                                   self.model.disc_dim, self.model.cont_dim)\n        G_output = self.model.G(noise)\n\n        # Classify the fake batch images, get the loss for these using sigmoid cross entropy\n        DG_score = self.model.D(G_output)\n\n        # Compute vanilla (original paper) D loss\n        D_loss = -torch.mean(torch.log(DX_score + 1e-8) + torch.log(1 - DG_score + 1e-8))\n\n        return torch.sum(D_loss)\n\n    def train_G(self, images):\n        """""" Run 1 step of training for generator\n\n        Input:\n            images: batch of images reshaped to [batch_size, -1]\n        Output:\n            G_loss: InfoGAN non-saturating loss for generator\n            -E[log(D(G(c, z)))]\n        """"""\n\n        # Get noise (denoted z), classify it using G, then classify the output of G using D.\n        noise = self.compute_noise(images.shape[0], self.model.z_dim,\n                                   self.model.disc_dim, self.model.cont_dim, c=None) # c=[c1, c2], z\n        G_output = self.model.G(noise) # G(c, z)\n        DG_score = self.model.D(G_output) # D(G(c, z))\n\n        # Compute the non-saturating loss for how D did versus the generations of G using sigmoid cross entropy\n        G_loss = -torch.mean(torch.log(DG_score + 1e-8))\n\n        return G_loss\n\n    def train_Q(self, images, LAMBDA=1):\n        """""" Run 1 step of training for auxiliary approximator network\n\n        Input:\n            images: batch of images reshaped to [batch_size, -1]\n        Output:\n            MI_loss: Approximation of mutual information\n            \xce\xbbI(c, G(c, z)) where I is the approximated mutual information\n            between our prior and the GAN distribution\n        """"""\n        # Generate labels for the generator batch images (all 0, since they are fake)\n        G_labels = to_cuda(torch.zeros(images.shape[0], 1))\n\n        # Sample noise z, generate output G(c, z)\n        noise = self.compute_noise(images.shape[0], self.model.z_dim,\n                                   self.model.disc_dim, self.model.cont_dim)\n\n        # Transform noise using G\n        G_output = self.model.G(noise)\n\n        # Approximate true posterior for categorical, Gaussian latent variables\n        Q_discrete, Q_continuous = self.model.Q(G_output)\n\n        # Compute mutual information loss\n        # Discrete component\n        discrete_target = noise[:, self.model.z_dim:self.model.z_dim+self.model.disc_dim]\n        disc_loss = F.cross_entropy(Q_discrete, torch.max(discrete_target, 1)[1])\n\n        # Continuous component\n        continuous_target = noise[:, self.model.z_dim+self.model.disc_dim:]\n        cont_loss = F.mse_loss(Q_continuous, continuous_target)\n\n        # Sum it up\n        MI_loss = LAMBDA * (disc_loss + cont_loss)\n\n        return MI_loss\n\n    def compute_noise(self, batch_size, z_dim, disc_dim, cont_dim, c=None):\n        """""" Compute random noise for the generator to learn to make images\n        OPTIONAL: set c to explore latent dimension space.\n        """"""\n\n        # Noise vector (z)\n        z = torch.randn(batch_size, z_dim)\n\n        # Uniformly distributed categorical latent variables (c1)\n        disc_c = torch.zeros((batch_size, disc_dim))\n        if c is not None:\n            categorical = int(c)*torch.ones((batch_size,), dtype=torch.long)\n        else:\n            categorical = torch.randint(0, disc_dim, (batch_size,), dtype=torch.long)\n        disc_c[range(batch_size), categorical] = 1\n\n        # Gaussian continuous latent variables (c2)\n        cont_c = torch.randn(batch_size, cont_dim)\n\n        return to_cuda(torch.cat((z, disc_c, cont_c), dim=1))\n\n    def process_batch(self, iterator):\n        """""" Generate a process batch to be input into the discriminator D """"""\n        images, _ = next(iter(iterator))\n        images = to_cuda(images.view(images.shape[0], -1))\n        return images\n\n    def generate_images(self, epoch, num_outputs=36, save=True, c=None):\n        """""" Visualize progress of generator learning """"""\n        # Turn off any regularization\n        self.model.eval()\n\n        # Sample noise vector\n        noise = self.compute_noise(num_outputs, self.model.z_dim,\n                                   self.model.disc_dim, self.model.cont_dim, c=c)\n\n        # Transform noise to image\n        images = self.model.G(noise)\n\n        # Reshape to proper image size\n        images = images.view(images.shape[0], self.model.shape, self.model.shape, -1).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size, k = int(num_outputs**0.5), 0\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].imshow(images[k].data.numpy(), cmap=\'gray\')\n            k += 1\n\n        # Save images if desired\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(images.unsqueeze(1).data,\n                                         outname + \'reconst_%d.png\'\n                                         %(epoch), nrow=grid_size)\n\n    def viz_loss(self):\n        """""" Visualize loss for the generator, discriminator """"""\n        # Set style, figure size\n        plt.style.use(\'ggplot\')\n        plt.rcParams[""figure.figsize""] = (8,6)\n\n        # Plot Discriminator loss in red\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Dlosses,\n                 \'r\')\n\n        # Plot Generator loss in green\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Glosses,\n                 \'g\')\n\n        # Add legend, title\n        plt.legend([\'Discriminator\', \'Generator\'])\n        plt.title(self.name)\n        plt.show()\n\n    def save_model(self, savepath):\n        """""" Save model state dictionary """"""\n        torch.save(self.model.state_dict(), savepath)\n\n    def load_model(self, loadpath):\n        """""" Load state dictionary into model """"""\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)\n\nif __name__ == \'__main__\':\n\n    # Load in binarized MNIST data, separate into data loaders\n    train_iter, val_iter, test_iter = get_data()\n\n    # Init model\n    model = InfoGAN(image_size=784,\n                    hidden_dim=400,\n                    z_dim=20,\n                    disc_dim=10,\n                    cont_dim=10)\n\n    # Init trainer\n    trainer = InfoGANTrainer(model=model,\n                             train_iter=train_iter,\n                             val_iter=val_iter,\n                             test_iter=test_iter,\n                             viz=False)\n\n    # Train\n    trainer.train(num_epochs=25,\n                  G_lr=2e-4,\n                  D_lr=2e-4,\n                  D_steps=1)\n'"
src/ls_gan.py,12,"b'"""""" (LS GAN) https://arxiv.org/abs/1611.04076\nLeast Squares GAN\n\nThe output of LSGAN\'s D is unbounded unless passed through an activation\nfunction. In this implementation, we include a sigmoid activation function as\nthis empirically improves visualizations for binary MNIST.\n\nTackles the vanishing gradients problem associated with GANs by swapping out\nthe cross entropy loss function with the least squares (L2) loss function.\nThe authors show that minimizing this objective is equivalent to minimizing the\nPearson chi-squared divergence. They claim that using the L2 loss function\npenalizes samples that appear to be real to the discriminator, but lie far away\nfrom the decision boundary. In this way, the generated images are made to appear\ncloser to real data. It also stabilizes training.\n""""""\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom itertools import product\nfrom tqdm import tqdm\n\nfrom utils import *\n\n\nclass Generator(nn.Module):\n    """""" Generator. Input is noise, output is a generated image.\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim):\n        super().__init__()\n\n        self.linear = nn.Linear(z_dim, hidden_dim)\n        self.generate = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        generation = torch.sigmoid(self.generate(activated))\n        return generation\n\n\nclass Discriminator(nn.Module):\n    """""" Critic (not trained to classify). Input is an image (real or generated),\n    output is approximate least-squares divergence.\n    """"""\n    def __init__(self, image_size, hidden_dim, output_dim):\n        super().__init__()\n\n        self.linear = nn.Linear(image_size, hidden_dim)\n        self.discriminate = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        discrimination = torch.sigmoid(self.discriminate(activated))\n        return discrimination\n\n\nclass LSGAN(nn.Module):\n    """""" Super class to contain both Discriminator (D) and Generator (G)\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim, output_dim=1):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.G = Generator(image_size, hidden_dim, z_dim)\n        self.D = Discriminator(image_size, hidden_dim, output_dim)\n\n        self.shape = int(image_size ** 0.5)\n\n\nclass LSGANTrainer:\n    """""" Object to hold data iterators, train a GAN variant\n    """"""\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False):\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.Glosses = []\n        self.Dlosses = []\n\n        self.viz = viz\n        self.num_epochs = 0\n\n    def train(self, num_epochs, G_lr=1e-4, D_lr=1e-4, D_steps=1):\n        """""" Train a least-squares GAN with Gradient Penalty\n\n            Logs progress using G loss, D loss, G(x), D(G(x)), visualizations\n            of Generator output.\n\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            G_lr: float, learning rate for generator\'s Adam optimize\n            D_lr: float, learning rate for discriminator\'s Adam optimizer\n            D_steps: int, ratio for how often to train D compared to G\n        """"""\n        # Initialize optimizers\n        G_optimizer = optim.Adam(params=[p for p in self.model.G.parameters()\n                                        if p.requires_grad], lr=G_lr)\n        D_optimizer = optim.Adam(params=[p for p in self.model.D.parameters()\n                                        if p.requires_grad], lr=D_lr)\n\n        # Approximate steps/epoch given D_steps per epoch -->\n        #  roughly train in the same way as if D_step (1) == G_step (1)\n        epoch_steps = int(np.ceil(len(self.train_iter) / (D_steps)))\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs+1)):\n            self.model.train()\n            G_losses, D_losses = [], []\n\n            for _ in range(epoch_steps):\n\n                D_step_loss = []\n\n                for _ in range(D_steps):\n\n                    # Reshape images\n                    images = self.process_batch(self.train_iter)\n\n                    # TRAINING D: Zero out gradients for D\n                    D_optimizer.zero_grad()\n\n                    # Train D to approximate the distance between real, generated\n                    D_loss = self.train_D(images)\n\n                    # Update parameters\n                    D_loss.backward()\n                    D_optimizer.step()\n\n                    # Log results, backpropagate the discriminator network\n                    D_step_loss.append(D_loss.item())\n\n                # So that G_loss and D_loss have the same number of entries.\n                D_losses.append(np.mean(D_step_loss))\n\n                # TRAINING G: Zero out gradients for G\n                G_optimizer.zero_grad()\n\n                # Train the generator to (roughly) minimize the approximated\n                # least-squares distance\n                G_loss = self.train_G(images)\n\n                # Log results, update parameters\n                G_losses.append(G_loss.item())\n                G_loss.backward()\n                G_optimizer.step()\n\n            # Save progress\n            self.Glosses.extend(G_losses)\n            self.Dlosses.extend(D_losses)\n\n            # Progress logging\n            print (""Epoch[%d/%d], G Loss: %.4f, D Loss: %.4f""\n                   %(epoch, num_epochs, np.mean(G_losses), np.mean(D_losses)))\n            self.num_epochs += 1\n\n            # Visualize generator progress\n            if self.viz:\n                self.generate_images(epoch)\n                plt.show()\n\n    def train_D(self, images, a=0, b=1):\n        """""" Run 1 step of training for discriminator\n\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            D_loss: L2 loss for discriminator,\n            0.50 * E[(D(x) - a)^2] + 0.50 * E[(D(G(z)) - b)^2],\n            where a and b are labels for generated (0) and real (1) data\n        """"""\n        # Sample noise, an output from the generator\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Use the discriminator to sample real, generated images\n        DX_score = self.model.D(images) # D(x)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute L2 loss for D\n        D_loss = (0.50 * torch.mean((DX_score - b)**2)) \\\n                    + (0.50 * torch.mean((DG_score - a)**2))\n\n        return D_loss\n\n    def train_G(self, images, c=1):\n        """""" Run 1 step of training for generator\n\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            G_loss: L2 loss for G,\n            0.50 * E[(D(G(z)) - c)^2],\n            where c is the label that G wants D to believe for fake data (1)\n        """"""\n        # Get noise, classify it using G, then classify the output of G using D.\n        noise = self.compute_noise(images.shape[0], self.model.z_dim) # z\n        G_output = self.model.G(noise) # G(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute L2 loss for G\n        G_loss = 0.50 * torch.mean((DG_score - c)**2)\n\n        return G_loss\n\n    def compute_noise(self, batch_size, z_dim):\n        """""" Compute random noise for input into the Generator G """"""\n        return to_cuda(torch.randn(batch_size, z_dim))\n\n    def process_batch(self, iterator):\n        """""" Generate a process batch to be input into the Discriminator D """"""\n        images, _ = next(iter(iterator))\n        images = to_cuda(images.view(images.shape[0], -1))\n        return images\n\n    def generate_images(self, epoch, num_outputs=36, save=True):\n        """""" Visualize progress of generator learning """"""\n        # Turn off any regularization\n        self.model.eval()\n\n        # Sample noise vector\n        noise = self.compute_noise(num_outputs, self.model.z_dim)\n\n        # Transform noise to image\n        images = self.model.G(noise)\n\n        # Reshape to proper image size\n        images = images.view(images.shape[0],\n                             self.model.shape,\n                             self.model.shape,\n                             -1).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size, k = int(num_outputs**0.5), 0\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].imshow(images[k].data.numpy(), cmap=\'gray\')\n            k += 1\n\n        # Save images if desired\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(images.unsqueeze(1).data,\n                                         outname + \'reconst_%d.png\'\n                                         %(epoch), nrow=grid_size)\n\n    def viz_loss(self):\n        """""" Visualize loss for the generator, discriminator """"""\n        # Set style, figure size\n        plt.style.use(\'ggplot\')\n        plt.rcParams[""figure.figsize""] = (8,6)\n\n        # Plot Discriminator loss in red\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Dlosses,\n                 \'r\')\n\n        # Plot Generator loss in green\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Glosses,\n                 \'g\')\n\n        # Add legend, title\n        plt.legend([\'Discriminator\', \'Generator\'])\n        plt.title(self.name)\n        plt.show()\n\n    def save_model(self, savepath):\n        """""" Save model state dictionary """"""\n        torch.save(self.model.state_dict(), savepath)\n\n    def load_model(self, loadpath):\n        """""" Load state dictionary into model """"""\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)\n\nif __name__ == ""__main__"":\n\n    # Load in binarized MNIST data, separate into data loaders\n    train_iter, val_iter, test_iter = get_data()\n\n    # Init model\n    model = LSGAN(image_size=784,\n                  hidden_dim=400,\n                  z_dim=20)\n\n    # Init trainer\n    trainer = LSGANTrainer(model=model,\n                           train_iter=train_iter,\n                           val_iter=val_iter,\n                           test_iter=test_iter,\n                           viz=False)\n\n    # Train\n    trainer.train(num_epochs=25,\n                  G_lr=1e-4,\n                  D_lr=1e-4,\n                  D_steps=1)\n'"
src/mm_gan.py,12,"b'"""""" (MM GAN) https://arxiv.org/abs/1406.2661\nMini-max GAN\n\nFrom the abstract: \'We propose a new framework for estimating generative models\nvia an adversarial process, in which we simultaneously train two models: a\ngenerative model G that captures the data distribution, and a discriminative\nmodel D that estimates the probability that a sample came from the training data\nrather than G. The training procedure for G is to maximize the probability of D\nmaking a mistake.\'\n\nCompared to MM GAN, the only change is the generator\'s loss.\n\nNS GAN: L(G) = -E[log(D(G(z)))]\nMM GAN: L(G) =  E[log(1-D(G(z)))]\n\nIn both NS GAN and MM GAN, the output of G can be interpretted as a probability.\n""""""\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom itertools import product\nfrom tqdm import tqdm\n\nfrom utils import *\n\n\nclass Generator(nn.Module):\n    """""" Generator. Input is noise, output is a generated image.\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim):\n        super().__init__()\n\n        self.linear = nn.Linear(z_dim, hidden_dim)\n        self.generate = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        generation = torch.sigmoid(self.generate(activated))\n        return generation\n\n\nclass Discriminator(nn.Module):\n    """""" Discriminator. Input is an image (real or generated), output is\n    P(generated).\n    """"""\n    def __init__(self, image_size, hidden_dim, output_dim):\n        super().__init__()\n\n        self.linear = nn.Linear(image_size, hidden_dim)\n        self.discriminate = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        discrimination = torch.sigmoid(self.discriminate(activated))\n        return discrimination\n\n\nclass MMGAN(nn.Module):\n    """""" Super class to contain both Discriminator (D) and Generator (G)\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim, output_dim=1):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.G = Generator(image_size, hidden_dim, z_dim)\n        self.D = Discriminator(image_size, hidden_dim, output_dim)\n\n        self.shape = int(image_size ** 0.5)\n\n\nclass MMGANTrainer:\n    """""" Object to hold data iterators, train a GAN variant\n    """"""\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False):\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.Glosses = []\n        self.Dlosses = []\n\n        self.viz = viz\n        self.num_epochs = 0\n\n    def train(self, num_epochs, G_lr=2e-4, D_lr=2e-4, D_steps=1, G_init=5):\n        """""" Train a vanilla GAN using minimax gradients loss for the generator.\n\n            Logs progress using G loss, D loss, G(x), D(G(x)), visualizations\n            of Generator output.\n\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            G_lr: float, learning rate for generator\'s Adam optimizer\n            D_lr: float, learning rate for discriminator\'s Adam optimizer\n            D_steps: int, ratio for how often to train D compared to G\n            G_init: int, number of training steps to pre-train G for\n        """"""\n        # Initialize optimizers\n        G_optimizer = optim.Adam(params=[p for p in self.model.G.parameters()\n                                        if p.requires_grad], lr=G_lr)\n        D_optimizer = optim.Adam(params=[p for p in self.model.D.parameters() if p.requires_grad], lr=D_lr)\n\n        # Approximate steps/epoch given D_steps per epoch\n        # --> roughly train in the same way as if D_step (1) == G_step (1)\n        epoch_steps = int(np.ceil(len(self.train_iter) / (D_steps)))\n\n        # Let G train for a few steps before beginning to jointly train G\n        # and D because MM GANs have trouble learning very early on in training\n        if G_init > 0:\n            for _ in range(G_init):\n                # Process a batch of images\n                images = self.process_batch(self.train_iter)\n\n                # Zero out gradients for G\n                G_optimizer.zero_grad()\n\n                # Pre-train G\n                G_loss = self.train_G(images)\n\n                # Backpropagate the generator network\n                G_loss.backward()\n                G_optimizer.step()\n\n            print(\'G pre-trained for {0} training steps.\'.format(G_init))\n        else:\n            print(\'G not pre-trained -- GAN unlikely to converge.\')\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs+1)):\n            self.model.train()\n            G_losses, D_losses = [], []\n\n            for _ in range(epoch_steps):\n\n                D_step_loss = []\n\n                for _ in range(D_steps):\n\n                    # Reshape images\n                    images = self.process_batch(self.train_iter)\n\n                    # TRAINING D: Zero out gradients for D\n                    D_optimizer.zero_grad()\n\n                    # Train D to learn to discriminate between real and generated images\n                    D_loss = self.train_D(images)\n\n                    # Update parameters\n                    D_loss.backward()\n                    D_optimizer.step()\n\n                    # Log results, backpropagate the discriminator network\n                    D_step_loss.append(D_loss.item())\n\n                # We report D_loss in this way so that G_loss and D_loss have the same number of entries.\n                D_losses.append(np.mean(D_step_loss))\n\n                # TRAINING G: Zero out gradients for G\n                G_optimizer.zero_grad()\n\n                # Train G to generate images that fool the discriminator\n                G_loss = self.train_G(images)\n\n                # Log results, update parameters\n                G_losses.append(G_loss.item())\n                G_loss.backward()\n                G_optimizer.step()\n\n            # Save progress\n            self.Glosses.extend(G_losses)\n            self.Dlosses.extend(D_losses)\n\n            # Progress logging\n            print (""Epoch[%d/%d], G Loss: %.4f, D Loss: %.4f""\n                   %(epoch, num_epochs, np.mean(G_losses), np.mean(D_losses)))\n            self.num_epochs += 1\n\n            # Visualize generator progress\n            if self.viz:\n                self.generate_images(epoch)\n                plt.show()\n\n    def train_D(self, images):\n        """""" Run 1 step of training for discriminator\n\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            D_loss: non-saturing loss for discriminator,\n            -E[log(D(x))] - E[log(1 - D(G(z)))]\n        """"""\n        # Classify the real batch images, get the loss for these\n        DX_score = self.model.D(images)\n\n        # Sample noise z, generate output G(z)\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Classify the fake batch images, get the loss for these using sigmoid cross entropy\n        DG_score = self.model.D(G_output)\n\n        # Compute vanilla (original paper) D loss\n        D_loss = torch.sum(-torch.mean(torch.log(DX_score + 1e-8)\n                            + torch.log(1 - DG_score + 1e-8)))\n\n        return D_loss\n\n    def train_G(self, images):\n        """""" Run 1 step of training for generator\n\n        Input:\n            images: batch of images reshaped to [batch_size, -1]\n        Output:\n            G_loss: minimax loss for how well G(z) fools D,\n            -E[log(D(G(z)))]\n        """"""\n        # Get noise (denoted z), classify it using G, then classify the output of G using D.\n        noise = self.compute_noise(images.shape[0], self.model.z_dim) # z\n        G_output = self.model.G(noise) # G(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute the minimax loss for how D did versus the generations of G using sigmoid cross entropy\n        G_loss = torch.mean(torch.log((1-DG_score) + 1e-8))\n\n        return G_loss\n\n    def compute_noise(self, batch_size, z_dim):\n        """""" Compute random noise for the generator to learn to make images from """"""\n        return to_cuda(torch.randn(batch_size, z_dim))\n\n    def process_batch(self, iterator):\n        """""" Generate a process batch to be input into the discriminator D """"""\n        images, _ = next(iter(iterator))\n        images = to_cuda(images.view(images.shape[0], -1))\n        return images\n\n    def generate_images(self, epoch, num_outputs=36, save=True):\n        """""" Visualize progress of generator learning """"""\n        # Turn off any regularization\n        self.model.eval()\n\n        # Sample noise vector\n        noise = self.compute_noise(num_outputs, self.model.z_dim)\n\n        # Transform noise to image\n        images = self.model.G(noise)\n\n        # Reshape to proper image size\n        images = images.view(images.shape[0], self.model.shape, self.model.shape, -1).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size, k = int(num_outputs**0.5), 0\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].imshow(images[k].data.numpy(), cmap=\'gray\')\n            k += 1\n\n        # Save images if desired\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(images.unsqueeze(1).data,\n                                         outname + \'reconst_%d.png\'\n                                         %(epoch), nrow=grid_size)\n\n    def viz_loss(self):\n        """""" Visualize loss for the generator, discriminator """"""\n        # Set style, figure size\n        plt.style.use(\'ggplot\')\n        plt.rcParams[""figure.figsize""] = (8,6)\n\n        # Plot Discriminator loss in red\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Dlosses,\n                 \'r\')\n\n        # Plot Generator loss in green\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Glosses,\n                 \'g\')\n\n        # Add legend, title\n        plt.legend([\'Discriminator\', \'Generator\'])\n        plt.title(self.name)\n        plt.show()\n\n    def save_model(self, savepath):\n        """""" Save model state dictionary """"""\n        torch.save(self.model.state_dict(), savepath)\n\n    def load_model(self, loadpath):\n        """""" Load state dictionary into model """"""\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)\n\n\nif __name__ == ""__main__"":\n\n    # Load in binarized MNIST data, separate into data loaders\n    train_iter, val_iter, test_iter = get_data()\n\n    # Init model\n    model = MMGAN(image_size=784,\n                  hidden_dim=400,\n                  z_dim=20)\n\n    # Init trainer\n    trainer = MMGANTrainer(model=model,\n                           train_iter=train_iter,\n                           val_iter=val_iter,\n                           test_iter=test_iter,\n                           viz=False)\n\n    # Train\n    trainer.train(num_epochs=25,\n                  G_lr=2e-4,\n                  D_lr=2e-4,\n                  D_steps=1,\n                  G_init=5)\n'"
src/ns_gan.py,12,"b'"""""" (NS GAN) https://arxiv.org/abs/1406.2661\nNon-saturating GAN.\n\nFrom the abstract: \'We propose a new framework for estimating generative models\nvia an adversarial process, in which we simultaneously train two models: a\ngenerative model G that captures the data distribution, and a discriminative\nmodel D that estimates the probability that a sample came from the training data\nrather than G. The training procedure for G is to maximize the probability of D\nmaking a mistake.\'\n\nCompared to MM GAN, the only change is the generator\'s loss.\n\nNS GAN: L(G) = -E[log(D(G(z)))]\nMM GAN: L(G) =  E[log(1-D(G(z)))]\n\nIn both NS GAN and MM GAN, the output of G can be interpretted as a probability.\n""""""\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom itertools import product\nfrom tqdm import tqdm\n\nfrom utils import *\n\n\nclass Generator(nn.Module):\n    """""" Generator. Input is noise, output is a generated image.\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim):\n        super().__init__()\n        self.linear = nn.Linear(z_dim, hidden_dim)\n        self.generate = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        generation = torch.sigmoid(self.generate(activated))\n        return generation\n\n\nclass Discriminator(nn.Module):\n    """""" Discriminator. Input is an image (real or generated), output is P(generated).\n    """"""\n    def __init__(self, image_size, hidden_dim, output_dim):\n        super().__init__()\n        self.linear = nn.Linear(image_size, hidden_dim)\n        self.discriminate = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        discrimination = torch.sigmoid(self.discriminate(activated))\n        return discrimination\n\n\nclass NSGAN(nn.Module):\n    """""" Super class to contain both Discriminator (D) and Generator (G)\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim, output_dim=1):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.G = Generator(image_size, hidden_dim, z_dim)\n        self.D = Discriminator(image_size, hidden_dim, output_dim)\n\n        self.shape = int(image_size ** 0.5)\n\n\nclass NSGANTrainer:\n    """""" Object to hold data iterators, train a GAN variant\n    """"""\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False):\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.Glosses = []\n        self.Dlosses = []\n\n        self.viz = viz\n        self.num_epochs = 0\n\n    def train(self, num_epochs, G_lr=2e-4, D_lr=2e-4, D_steps=1):\n        """""" Train a vanilla GAN using non-saturating gradients loss.\n\n            Logs progress using G loss, D loss, G(x), D(G(x)), visualizations\n            of Generator output.\n\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            G_lr: float, learning rate for generator\'s Adam optimizer\n            D_lr: float, learning rate for discriminator\'s Adam optimizer\n            D_steps: int, ratio for how often to train D compared to G\n        """"""\n        # Initialize optimizers\n        G_optimizer = optim.Adam(params=[p for p in self.model.G.parameters()\n                                        if p.requires_grad], lr=G_lr)\n        D_optimizer = optim.Adam(params=[p for p in self.model.D.parameters()\n                                        if p.requires_grad], lr=D_lr)\n\n        # Approximate steps/epoch given D_steps per epoch\n        # --> roughly train in the same way as if D_step (1) == G_step (1)\n        epoch_steps = int(np.ceil(len(self.train_iter) / (D_steps)))\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs+1)):\n\n            self.model.train()\n            G_losses, D_losses = [], []\n\n            for _ in range(epoch_steps):\n\n                D_step_loss = []\n\n                for _ in range(D_steps):\n\n                    # Reshape images\n                    images = self.process_batch(self.train_iter)\n\n                    # TRAINING D: Zero out gradients for D\n                    D_optimizer.zero_grad()\n\n                    # Learn to discriminate between real and generated images\n                    D_loss = self.train_D(images)\n\n                    # Update parameters\n                    D_loss.backward()\n                    D_optimizer.step()\n\n                    # Log results, backpropagate the discriminator network\n                    D_step_loss.append(D_loss.item())\n\n                # So that G_loss and D_loss have the same number of entries.\n                D_losses.append(np.mean(D_step_loss))\n\n                # TRAINING G: Zero out gradients for G\n                G_optimizer.zero_grad()\n\n                # Learn to generate images that fool the discriminator\n                G_loss = self.train_G(images)\n\n                # Log results, update parameters\n                G_losses.append(G_loss.item())\n                G_loss.backward()\n                G_optimizer.step()\n\n            # Save progress\n            self.Glosses.extend(G_losses)\n            self.Dlosses.extend(D_losses)\n\n            # Progress logging\n            print (""Epoch[%d/%d], G Loss: %.4f, D Loss: %.4f""\n                   %(epoch, num_epochs, np.mean(G_losses), np.mean(D_losses)))\n            self.num_epochs += 1\n\n            # Visualize generator progress\n            if self.viz:\n                self.generate_images(epoch)\n                plt.show()\n\n    def train_D(self, images):\n        """""" Run 1 step of training for discriminator\n\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            D_loss: non-saturing loss for discriminator,\n            -E[log(D(x))] - E[log(1 - D(G(z)))]\n        """"""\n\n        # Sample noise z, generate output G(z)\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Classify the generated and real batch images\n        DX_score = self.model.D(images) # D(x)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute vanilla (original paper) D loss\n        D_loss = torch.sum(-torch.mean(torch.log(DX_score + 1e-8)\n                            + torch.log(1 - DG_score + 1e-8)))\n\n        return D_loss\n\n    def train_G(self, images):\n        """""" Run 1 step of training for generator\n\n        Input:\n            images: batch of images reshaped to [batch_size, -1]\n        Output:\n            G_loss: non-saturating loss for how well G(z) fools D,\n            -E[log(D(G(z)))]\n        """"""\n\n        # Get noise (denoted z), classify it using G, then classify the output\n        # of G using D.\n        noise = self.compute_noise(images.shape[0], self.model.z_dim) # (z)\n        G_output = self.model.G(noise) # G(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute the non-saturating loss for how D did versus the generations\n        # of G using sigmoid cross entropy\n        G_loss = -torch.mean(torch.log(DG_score + 1e-8))\n\n        return G_loss\n\n    def compute_noise(self, batch_size, z_dim):\n        """""" Compute random noise for the generator to learn to make images from """"""\n        return to_cuda(torch.randn(batch_size, z_dim))\n\n    def process_batch(self, iterator):\n        """""" Generate a process batch to be input into the discriminator D """"""\n        images, _ = next(iter(iterator))\n        images = to_cuda(images.view(images.shape[0], -1))\n        return images\n\n    def generate_images(self, epoch, num_outputs=36, save=True):\n        """""" Visualize progress of generator learning """"""\n        # Turn off any regularization\n        self.model.eval()\n\n        # Sample noise vector\n        noise = self.compute_noise(num_outputs, self.model.z_dim)\n\n        # Transform noise to image\n        images = self.model.G(noise)\n\n        # Reshape to square image size\n        images = images.view(images.shape[0],\n                             self.model.shape,\n                             self.model.shape,\n                             -1).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size, k = int(num_outputs**0.5), 0\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].imshow(images[k].data.numpy(), cmap=\'gray\')\n            k += 1\n\n        # Save images if desired\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(images.unsqueeze(1).data,\n                                         outname + \'reconst_%d.png\'\n                                         %(epoch), nrow=grid_size)\n\n    def viz_loss(self):\n        """""" Visualize loss for the generator, discriminator """"""\n        # Set style, figure size\n        plt.style.use(\'ggplot\')\n        plt.rcParams[""figure.figsize""] = (8,6)\n\n        # Plot Discriminator loss in red, Generator loss in green\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Dlosses,\n                 \'r\')\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Glosses,\n                 \'g\')\n\n        # Add legend, title\n        plt.legend([\'Discriminator\', \'Generator\'])\n        plt.title(self.name)\n        plt.show()\n\n    def save_model(self, savepath):\n        """""" Save model state dictionary """"""\n        torch.save(self.model.state_dict(), savepath)\n\n    def load_model(self, loadpath):\n        """""" Load state dictionary into model """"""\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)\n\n\nif __name__ == ""__main__"":\n\n    # Load in binarized MNIST data, separate into data loaders\n    train_iter, val_iter, test_iter = get_data()\n\n    # Init model\n    model = NSGAN(image_size=784,\n                  hidden_dim=400,\n                  z_dim=20)\n\n    # Init trainer\n    trainer = NSGANTrainer(model=model,\n                           train_iter=train_iter,\n                           val_iter=val_iter,\n                           test_iter=test_iter,\n                           viz=False)\n\n    # Train\n    trainer.train(num_epochs=25,\n                  G_lr=2e-4,\n                  D_lr=2e-4,\n                  D_steps=1)\n'"
src/ra_gan.py,12,"b'"""""" (RaGAN) https://arxiv.org/abs/1807.00734\nRelativistic GAN\n\nRelativistic GANs argue that the GAN generator should decrease the\ndiscriminator\'s output probability that real data is real in addition to\nincreasing its output probability that fake data is real. By doing this, GANs\nare claimed to be more stable and generate higher quality images.\n\nDiscriminator loss is changed such that the discriminator estimates the\nprobability that the given real data is more realistic than a randomly sampled\nfake data. Generator loss is change such that real data is less likely to be\nclassified as real and fake data is more likely to be classified as real.\n\nFor computational efficiency, the discriminator estimates the probability that\nthe given real data is more realistic than fake data, on average. Otherwise,\nthe network would need to consider all combinations of real and fake data in the\nminibatch. This would require O(m^2) instead of O(m), where m is batch size.\n\nL(D) = -E[log( sigmoid(D(x) - E[D(G(z))]) )]\n        - E[log(1 - sigmoid(D(G(z)) - E[D(x)]))]\n\nL(G) = -E[log( sigmoid(D(G(z)) - E[D(x)]) )]\n        - E[log(1 - sigmoid(D(x) - E[D(G(z))]))]\n\nThis implementation uses non-saturating (NS) GAN as a case study. The actual\nmodification proposed herein can be applied to any GAN in which the output of\nthe discriminator can be interpretted as a probability.\n""""""\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom itertools import product\nfrom tqdm import tqdm\n\nfrom utils import *\n\n\nclass Generator(nn.Module):\n    """""" Generator. Input is noise, output is a generated image.\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim):\n        super().__init__()\n        self.linear = nn.Linear(z_dim, hidden_dim)\n        self.generate = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        generation = torch.sigmoid(self.generate(activated))\n        return generation\n\n\nclass Discriminator(nn.Module):\n    """""" Discriminator. Input is an image (real or generated),\n    output is P(generated).\n    """"""\n    def __init__(self, image_size, hidden_dim, output_dim):\n        super().__init__()\n        self.linear = nn.Linear(image_size, hidden_dim)\n        self.discriminate = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        discrimination = torch.sigmoid(self.discriminate(activated))\n        return discrimination\n\n\nclass RaNSGAN(nn.Module):\n    """""" Super class to contain both Discriminator (D) and Generator (G)\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim, output_dim=1):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.G = Generator(image_size, hidden_dim, z_dim)\n        self.D = Discriminator(image_size, hidden_dim, output_dim)\n\n        self.shape = int(image_size ** 0.5)\n\n\nclass RaNSGANTrainer:\n    """""" Object to hold data iterators, train a GAN variant\n    """"""\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False):\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.Glosses = []\n        self.Dlosses = []\n\n        self.viz = viz\n        self.num_epochs = 0\n\n    def train(self, num_epochs, G_lr=2e-4, D_lr=2e-4, D_steps=1):\n        """""" Train a relativistic non-saturating GAN\n\n            Logs progress using G loss, D loss, G(x), D(G(x)), visualizations\n            of Generator output.\n\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            G_lr: float, learning rate for generator\'s Adam optimizer\n            D_lr: float, learning rate for discriminator\'s Adam optimizer\n            D_steps: int, ratio for how often to train D compared to G\n        """"""\n        # Initialize optimizers\n        G_optimizer = optim.Adam(params=[p for p in self.model.G.parameters()\n                                        if p.requires_grad], lr=G_lr)\n        D_optimizer = optim.Adam(params=[p for p in self.model.D.parameters()\n                                        if p.requires_grad], lr=D_lr)\n\n        # Approximate steps/epoch given D_steps per epoch\n        # --> roughly train in the same way as if D_step (1) == G_step (1)\n        epoch_steps = int(np.ceil(len(self.train_iter) / (D_steps)))\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs+1)):\n            self.model.train()\n            G_losses, D_losses = [], []\n\n            for _ in range(epoch_steps):\n\n                D_step_loss = []\n\n                for _ in range(D_steps):\n\n                    # Reshape images\n                    images = self.process_batch(self.train_iter)\n\n                    # TRAINING D: Zero out gradients for D\n                    D_optimizer.zero_grad()\n\n                    # Learn to discriminate between real and generated images\n                    D_loss = self.train_D(images)\n\n                    # Update parameters\n                    D_loss.backward()\n                    D_optimizer.step()\n\n                    # Log results, backpropagate the discriminator network\n                    D_step_loss.append(D_loss.item())\n\n                # So that G_loss and D_loss have the same number of entries.\n                D_losses.append(np.mean(D_step_loss))\n\n                # TRAINING G: Zero out gradients for G\n                G_optimizer.zero_grad()\n\n                # Learn to generate images that fool the discriminator\n                G_loss = self.train_G(images)\n\n                # Log results, update parameters\n                G_losses.append(G_loss.item())\n                G_loss.backward()\n                G_optimizer.step()\n\n            # Save progress\n            self.Glosses.extend(G_losses)\n            self.Dlosses.extend(D_losses)\n\n            # Progress logging\n            print (""Epoch[%d/%d], G Loss: %.4f, D Loss: %.4f""\n                   %(epoch, num_epochs, np.mean(G_losses), np.mean(D_losses)))\n            self.num_epochs += 1\n\n            # Visualize generator progress\n            if self.viz:\n                self.generate_images(epoch)\n                plt.show()\n\n    def train_D(self, images):\n        """""" Run 1 step of training for discriminator\n\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            D_loss: non-saturing loss for discriminator,\n            -E[log( sigmoid(D(x) - E[D(G(z))]) )]\n              - E[log(1 - sigmoid(D(G(z)) - E[D(x)]))]\n        """"""\n        # Classify the real batch images, get the loss for these\n        DX_score = self.model.D(images)\n\n        # Sample noise z, generate output G(z)\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Classify the generated batch images\n        DG_score = self.model.D(G_output)\n\n        # Compute D loss\n        D_loss = -torch.mean(torch.log(torch.sigmoid(DX_score-DG_score.mean())+1e-8) \\\n                    + torch.log(torch.sigmoid(1 - DG_score) + 1e-8)) / 2\n\n        return D_loss\n\n    def train_G(self, images):\n        """""" Run 1 step of training for generator\n\n        Input:\n            images: batch of images reshaped to [batch_size, -1]\n        Output:\n            G_loss: non-saturating loss for how well G(z) fools D,\n            -E[log(sigmoid(D(G(z))-E[D(x)]))]\n                -E[log(1-sigmoid(D(x)-E[D(G(z))]))]\n        """"""\n        # Get noise (denoted z), classify it using G, then classify the output\n        # of G using D.\n        noise = self.compute_noise(images.shape[0], self.model.z_dim) # z\n        G_output = self.model.G(noise) # G(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute the non-saturating loss for how D did versus the generations\n        # of G\n        G_loss = -torch.mean(torch.log(DG_score + 1e-8))\n\n        return G_loss\n\n    def compute_noise(self, batch_size, z_dim):\n        """""" Compute random noise for the input to Generator G """"""\n        return to_cuda(torch.randn(batch_size, z_dim))\n\n    def process_batch(self, iterator):\n        """""" Generate a process batch to be input into the discriminator D """"""\n        images, _ = next(iter(iterator))\n        images = to_cuda(images.view(images.shape[0], -1))\n        return images\n\n    def generate_images(self, epoch, num_outputs=36, save=True):\n        """""" Visualize progress of generator learning """"""\n        # Turn off any regularization\n        self.model.eval()\n\n        # Sample noise vector\n        noise = self.compute_noise(num_outputs, self.model.z_dim)\n\n        # Transform noise to image\n        images = self.model.G(noise)\n\n        # Reshape to square image size\n        images = images.view(images.shape[0],\n                             self.model.shape,\n                             self.model.shape,\n                             -1).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size, k = int(num_outputs**0.5), 0\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].imshow(images[k].data.numpy(), cmap=\'gray\')\n            k += 1\n\n        # Save images if desired\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(images.unsqueeze(1).data,\n                                         outname + \'reconst_%d.png\'\n                                         %(epoch), nrow=grid_size)\n\n    def viz_loss(self):\n        """""" Visualize loss for the generator, discriminator """"""\n        # Set style, figure size\n        plt.style.use(\'ggplot\')\n        plt.rcParams[""figure.figsize""] = (8,6)\n\n        # Plot Discriminator loss in red\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Dlosses,\n                 \'r\')\n\n        # Plot Generator loss in green\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Glosses,\n                 \'g\')\n\n        # Add legend, title\n        plt.legend([\'Discriminator\', \'Generator\'])\n        plt.title(self.name)\n        plt.show()\n\n    def save_model(self, savepath):\n        """""" Save model state dictionary """"""\n        torch.save(self.model.state_dict(), savepath)\n\n    def load_model(self, loadpath):\n        """""" Load state dictionary into model """"""\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)\n\n\nif __name__ == \'__main__\':\n    # Load in binarized MNIST data, separate into data loaders\n    train_iter, val_iter, test_iter = get_data()\n\n    # Initialize model\n    model = RaNSGAN(image_size=784,\n                    hidden_dim=400,\n                    z_dim=20)\n\n    # Initialize trainer\n    trainer = RaNSGANTrainer(model=model,\n                            train_iter=train_iter,\n                            val_iter=val_iter,\n                            test_iter=test_iter,\n                            viz=False)\n\n    # Train\n    trainer.train(num_epochs=25,\n                  G_lr=2e-4,\n                  D_lr=2e-4,\n                  D_steps=1)\n'"
src/utils.py,12,"b'import torch\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\n\ndef to_var(x):\n    """""" Make a tensor cuda-erized and requires gradient """"""\n    return to_cuda(x).requires_grad_()\n\ndef to_cuda(x):\n    """""" Cuda-erize a tensor """"""\n    if torch.cuda.is_available():\n        x = x.cuda()\n    return x\n\ndef get_data(BATCH_SIZE=100):\n    """""" Load data for binared MNIST """"""\n    torch.manual_seed(3435)\n\n    # Download our data\n    train_dataset = datasets.MNIST(root=\'./data/\',\n                                    train=True,\n                                    transform=transforms.ToTensor(),\n                                    download=True)\n\n    test_dataset = datasets.MNIST(root=\'./data/\',\n                                   train=False,\n                                   transform=transforms.ToTensor())\n\n    # Use greyscale values as sampling probabilities to get back to [0,1]\n    train_img = torch.stack([torch.bernoulli(d[0]) for d in train_dataset])\n    train_label = torch.LongTensor([d[1] for d in train_dataset])\n\n    test_img = torch.stack([torch.bernoulli(d[0]) for d in test_dataset])\n    test_label = torch.LongTensor([d[1] for d in test_dataset])\n\n    # MNIST has no official train dataset so use last 10000 as validation\n    val_img = train_img[-10000:].clone()\n    val_label = train_label[-10000:].clone()\n\n    train_img = train_img[:-10000]\n    train_label = train_label[:-10000]\n\n    # Create data loaders\n    train = torch.utils.data.TensorDataset(train_img, train_label)\n    val = torch.utils.data.TensorDataset(val_img, val_label)\n    test = torch.utils.data.TensorDataset(test_img, test_label)\n\n    train_iter = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n    val_iter = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE, shuffle=True)\n    test_iter = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=True)\n\n    return train_iter, val_iter, test_iter\n'"
src/vae.py,15,"b'"""""" (VAE) https://arxiv.org/abs/1312.6114\nVariational Autoencoder\n\nFrom the abstract:\n\n""We introduce a stochastic variational inference and learning algorithm that\nscales to large datasets and, under some mild differentiability conditions,\neven works in the intractable case. Our contributions is two-fold. First, we\nshow that a reparameterization of the variational lower bound yields a lower\nbound estimator that can be straightforwardly optimized using standard\nstochastic gradient methods. Second, we show that for i.i.d. datasets with\ncontinuous latent variables per datapoint, posterior inference can be made\nespecially efficient by fitting an approximate inference model (also called a\nrecognition model) to the intractable posterior using the proposed lower bound\nestimator.""\n\nBasically VAEs encode an input into a given dimension z, reparametrize that z\nusing it\'s mean and std, and then reconstruct the image from reparametrized z.\nThis lets us tractably model latent representations that we may not be\nexplicitly aware of that are in the data. For a simple example of what this may\nlook like, read up on ""Karl Pearson\'s Crabs."" The basic idea was that a\nscientist collected data on a population of crabs, noticed that the distribution\nwas non-normal, and Pearson postulated it was because there were likely more\nthan one population of crabs studied. This would\'ve been a latent variable,\nsince the data colllector did not initially know or perhaps even suspect this.\n""""""\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.utils import make_grid\n\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom copy import deepcopy\n\nfrom tqdm import tqdm\nfrom itertools import product\n\nfrom utils import *\n\n\nclass Encoder(nn.Module):\n    """""" MLP encoder for VAE. Input is an image,\n    outputs are the mean, std of the latent variable z pre-reparametrization\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim):\n        super().__init__()\n\n        self.linear = nn.Linear(image_size, hidden_dim)\n        self.mu = nn.Linear(hidden_dim, z_dim)\n        self.log_var = nn.Linear(hidden_dim, z_dim)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        mu, log_var = self.mu(activated), self.log_var(activated)\n        return mu, log_var\n\n\nclass Decoder(nn.Module):\n    """""" MLP decoder for VAE. Input is a reparametrized latent representation,\n    output is reconstructed image\n    """"""\n    def __init__(self, z_dim, hidden_dim, image_size):\n        super().__init__()\n\n        self.linear = nn.Linear(z_dim, hidden_dim)\n        self.recon = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, z):\n        activated = F.relu(self.linear(z))\n        reconstructed = torch.sigmoid(self.recon(activated))\n        return reconstructed\n\n\nclass VAE(nn.Module):\n    """""" VAE super class to reconstruct an image. Contains reparametrization\n    method for latent variable z\n    """"""\n    def __init__(self, image_size=784, hidden_dim=400, z_dim=20):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.encoder = Encoder(image_size=image_size, hidden_dim=hidden_dim, z_dim=z_dim)\n        self.decoder = Decoder(z_dim=z_dim, hidden_dim=hidden_dim, image_size=image_size)\n\n        self.shape = int(image_size ** 0.5)\n\n    def forward(self, x):\n        mu, log_var = self.encoder(x)\n        z = self.reparameterize(mu, log_var)\n        out_img = self.decoder(z)\n        return out_img, mu, log_var\n\n    def reparameterize(self, mu, log_var):\n        """""""" Reparametrization trick: z = mean + std*epsilon,\n        where epsilon ~ N(0, 1).\n        """"""\n        epsilon = to_cuda(torch.randn(mu.shape))\n        z = mu + epsilon * torch.exp(log_var/2) # 2 for convert var to std\n        return z\n\n\nclass VAETrainer:\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False):\n        """""" Object to hold data iterators, train the model """"""\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.best_val_loss = 1e10\n        self.debugging_image, _ = next(iter(test_iter))\n        self.viz = viz\n\n        self.kl_loss = []\n        self.recon_loss = []\n        self.num_epochs = 0\n\n    def train(self, num_epochs, lr=1e-3, weight_decay=1e-5):\n        """""" Train a Variational Autoencoder\n\n            Logs progress using total loss, reconstruction loss, kl_divergence,\n            and validation loss\n\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            lr: float, learning rate for Adam optimizer\n            weight_decay: float, weight decay for Adam optimizer\n        """"""\n        # Adam optimizer, sigmoid cross entropy for reconstructing binary MNIST\n        optimizer = optim.Adam(params=[p for p in self.model.parameters()\n                                      if p.requires_grad],\n                               lr=lr,\n                               weight_decay=weight_decay)\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs+1)):\n\n            self.model.train()\n            epoch_loss, epoch_recon, epoch_kl = [], [], []\n\n            for batch in self.train_iter:\n\n                # Zero out gradients\n                optimizer.zero_grad()\n\n                # Compute reconstruction loss, Kullback-Leibler divergence\n                # for a batch for the variational lower bound (ELBO)\n                recon_loss, kl_diverge = self.compute_batch(batch)\n                batch_loss = recon_loss + kl_diverge\n\n                # Update parameters\n                batch_loss.backward()\n                optimizer.step()\n\n                # Log metrics\n                epoch_loss.append(batch_loss.item())\n                epoch_recon.append(recon_loss.item())\n                epoch_kl.append(kl_diverge.item())\n\n            # Save progress\n            self.kl_loss.extend(epoch_kl)\n            self.recon_loss.extend(epoch_recon)\n\n            # Test the model on the validation set\n            self.model.eval()\n            val_loss = self.evaluate(self.val_iter)\n\n            # Early stopping\n            if val_loss < self.best_val_loss:\n                self.best_model = deepcopy(self.model)\n                self.best_val_loss = val_loss\n\n            # Progress logging\n            print (""Epoch[%d/%d], Total Loss: %.4f, Reconst Loss: %.4f, KL Div: %.7f, Val Loss: %.4f""\n                   %(epoch, num_epochs, np.mean(epoch_loss),\n                   np.mean(epoch_recon), np.mean(epoch_kl), val_loss))\n            self.num_epochs += 1\n\n            # Debugging and visualization purposes\n            if self.viz:\n                self.sample_images(epoch)\n                plt.show()\n\n    def compute_batch(self, batch):\n        """""" Compute loss for a batch of examples """"""\n        # Reshape images\n        images, _ = batch\n        images = to_cuda(images.view(images.shape[0], -1))\n\n        # Get output images, mean, std of encoded space\n        outputs, mu, log_var = self.model(images)\n\n        # L2 (mean squared error) loss\n        recon_loss = torch.sum((images - outputs) ** 2)\n\n        # Kullback-Leibler divergence between encoded space, Gaussian\n        kl_diverge = self.kl_divergence(mu, log_var)\n\n        return recon_loss, kl_diverge\n\n    def kl_divergence(self, mu, log_var):\n        """""" Compute Kullback-Leibler divergence """"""\n        return torch.sum(0.5 * (mu**2 + torch.exp(log_var) - log_var - 1))\n\n    def evaluate(self, iterator):\n        """""" Evaluate on a given dataset """"""\n        loss = []\n        for batch in iterator:\n            recon_loss, kl_diverge = self.compute_batch(batch)\n            batch_loss = recon_loss + kl_diverge\n            loss.append(batch_loss.item())\n\n        loss = np.mean(loss)\n        return loss\n\n    def reconstruct_images(self, images, epoch, save=True):\n        """""" Sample images from latent space at each epoch """"""\n        # Reshape images, pass through model, reshape reconstructed output\n        batch = to_cuda(images.view(images.shape[0], -1))\n        reconst_images, _, _ = self.model(batch)\n        reconst_images = reconst_images.view(images.shape).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size, k = int(reconst_images.shape[0]**0.5), 0\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].imshow(reconst_images[k].data.numpy(), cmap=\'gray\')\n            k += 1\n\n        # Save\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(images.data,\n                                         outname + \'real.png\',\n                                         nrow=grid_size)\n            torchvision.utils.save_image(reconst_images.unsqueeze(1).data,\n                                         outname + \'reconst_%d.png\' %(epoch),\n                                         nrow=grid_size)\n\n    def sample_images(self, epoch=-100, num_images=36, save=True):\n        """""" Viz method 1: generate images by sampling z ~ p(z), x ~ p(x|z,\xce\xb8) """"""\n        # Sample z\n        z = to_cuda(torch.randn(num_images, self.model.z_dim))\n\n        # Pass into decoder\n        sample = self.model.decoder(z)\n\n        # Plot\n        to_img = ToPILImage()\n        img = to_img(make_grid(sample.data.view(num_images, \n                                                -1, \n                                                self.model.shape, \n                                                self.model.shape),\n                     nrow=int(num_images**0.5)))\n        display(img)\n\n        # Save\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            img.save(outname + \'sample_%d.png\' %(epoch))\n\n    def sample_interpolated_images(self):\n        """""" Viz method 2: sample two random latent vectors from p(z),\n        then sample from their interpolated values\n        """"""\n        # Sample latent vectors\n        z1 = torch.normal(torch.zeros(self.model.z_dim), 1)\n        z2 = torch.normal(torch.zeros(self.model.z_dim), 1)\n        to_img = ToPILImage()\n\n        # Interpolate within latent vectors\n        for alpha in np.linspace(0, 1, self.model.z_dim):\n            z = to_cuda(alpha*z1 + (1-alpha)*z2)\n            sample = self.model.decoder(z)\n            display(to_img(make_grid(sample.data.view(-1,\n                                                      self.model.shape,\n                                                      self.model.shape))))\n\n    def explore_latent_space(self, num_epochs=3):\n        """""" Viz method 3: train a VAE with 2 latent variables,\n        compare variational means\n        """"""\n        # Initialize and train a VAE with size two dimension latent space\n        train_iter, val_iter, test_iter = get_data()\n        latent_model = VAE(image_size=784, hidden_dim=400, z_dim=2)\n        latent_space = VAETrainer(latent_model, train_iter, val_iter, test_iter)\n        latent_space.train(num_epochs)\n        latent_model = latent_space.best_model\n\n        # Across batches in train iter, collect variationa means\n        data = []\n        for batch in train_iter:\n            images, labels = batch\n            images = to_cuda(images.view(images.shape[0], -1))\n            mu, log_var = latent_model.encoder(images)\n\n            for label, (m1, m2) in zip(labels, mu):\n                data.append((label.item(), m1.item(), m2.item()))\n\n        # Plot\n        labels, m1s, m2s = zip(*data)\n        plt.figure(figsize=(10,10))\n        plt.scatter(m1s, m2s, c=labels)\n        plt.legend([str(i) for i in set(labels)])\n\n        # Evenly sample across latent space, visualize the outputs\n        mu = torch.stack([torch.FloatTensor([m1, m2])\n                          for m1 in np.linspace(-2, 2, 10)\n                          for m2 in np.linspace(-2, 2, 10)])\n        samples = latent_model.decoder(to_cuda(mu))\n        to_img = ToPILImage()\n        display(to_img(make_grid(samples.data.view(mu.shape[0],\n                                                   -1,\n                                                   latent_model.shape,\n                                                   latent_model.shape),\n                                 nrow=10)))\n\n        return latent_model\n\n    def make_all(self):\n        """""" Execute all latent space viz methods outlined in this class """"""\n\n        print(\'Sampled images from latent space:\')\n        self.sample_images(save=False)\n\n        print(\'Interpolating between two randomly sampled\')\n        self.sample_interpolated_images()\n\n        print(\'Exploring latent representations\')\n        _ = self.explore_latent_space()\n\n    def viz_loss(self):\n        """""" Visualize reconstruction loss """"""\n        # Set style, figure size\n        plt.style.use(\'ggplot\')\n        plt.rcParams[""figure.figsize""] = (8,6)\n\n        # Plot reconstruction loss in red, KL divergence in green\n        plt.plot(np.linspace(1, self.num_epochs, len(self.recon_loss)),\n                 self.recon_loss,\n                 \'r\')\n        plt.plot(np.linspace(1, self.num_epochs, len(self.kl_loss)),\n                 self.kl_loss,\n                 \'g\')\n\n        # Add legend, title\n        plt.legend([\'Reconstruction\', \'Kullback-Leibler\'])\n        plt.title(self.name)\n        plt.show()\n\n    def save_model(self, savepath):\n        """""" Save model state dictionary """"""\n        torch.save(self.model.state_dict(), savepath)\n\n    def load_model(self, loadpath):\n        """""" Load state dictionary into model """"""\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)\n\nif __name__ == ""__main__"":\n\n    # Load in binzarized MNIST data, separate into data loaders\n    train_iter, val_iter, test_iter = get_data()\n\n    # Init model\n    model = VAE(image_size=784,\n                hidden_dim=400,\n                z_dim=20)\n\n    # Init trainer\n    trainer = VAETrainer(model=model,\n                      train_iter=train_iter,\n                      val_iter=val_iter,\n                      test_iter=test_iter,\n                      viz=False)\n\n    # Train\n    trainer.train(num_epochs=5,\n                  lr=1e-3,\n                  weight_decay=1e-5)\n'"
src/w_gan.py,11,"b'"""""" (WGAN) https://arxiv.org/abs/1701.07875\nWasserstein GAN\n\nThe output of WGAN\'s D is unbounded unless passed through an activation\nfunction. In this implementation, we include a sigmoid activation function\nas this empirically improves visualizations for binary MNIST.\n\nWGAN utilizes the Wasserstein distance to produce a value function which has\nbetter theoretical properties than the vanilla GAN. In particular, the authors\nprove that there exist distributions for which Jenson-Shannon, Kullback-Leibler,\nReverse Kullback Leibler, and Total Variaton distance metrics where Wasserstein\ndoes. Furthermore, the Wasserstein distance has guarantees of continuity and\ndifferentiability in neural network settings where the previously mentioned\ndistributions may not. Lastly, they show that that every distribution that\nconverges under KL, reverse-KL, TV, and JS divergences also converges under the\nWasserstein divergence and that a small Wasserstein distance corresponds to a\nsmall difference in distributions. The downside is that Wasserstein distance\ncannot be tractably computed directly. But if we make sure the discriminator\n(aka Critic because it is not actually classifying) lies in the space of\n1-Lipschitz functions, we can use that to approximate it instead. We crudely\nenforce this via a weight clamping parameter C.\n\nNote that this implementation uses RMSprop optimizer instead of Adam, as per\nthe original paper.\n""""""\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom itertools import product\nfrom tqdm import tqdm\n\nfrom src import utils\n\n\nclass Generator(nn.Module):\n    """""" Generator. Input is noise, output is a generated image.\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim):\n        super().__init__()\n\n        self.linear = nn.Linear(z_dim, hidden_dim)\n        self.generate = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        generation = torch.sigmoid(self.generate(activated))\n        return generation\n\n\nclass Discriminator(nn.Module):\n    """""" Critic (not trained to classify). Input is an image (real or generated),\n    output is the approximate Wasserstein Distance between z~P(G(z)) and real.\n    """"""\n    def __init__(self, image_size, hidden_dim, output_dim):\n        super().__init__()\n\n        self.linear = nn.Linear(image_size, hidden_dim)\n        self.discriminate = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        discrimination = torch.sigmoid(self.discriminate(activated))\n        return discrimination\n\n\nclass WGAN(nn.Module):\n    """""" Super class to contain both Discriminator (D) and Generator (G)\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim, output_dim=1):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.G = Generator(image_size, hidden_dim, z_dim)\n        self.D = Discriminator(image_size, hidden_dim, output_dim)\n\n        self.shape = int(image_size ** 0.5)\n\n\nclass WGANTrainer:\n    """""" Object to hold data iterators, train a GAN variant\n    """"""\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False):\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.Glosses = []\n        self.Dlosses = []\n\n        self.viz = viz\n        self.num_epochs = 0\n\n    def train(self, num_epochs, G_lr=5e-5, D_lr=5e-5, D_steps=5, clip=0.01):\n        """""" Train a Wasserstein GAN\n\n            Logs progress using G loss, D loss, G(x), D(G(x)), visualizations\n            of Generator output.\n\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            G_lr: float, learning rate for generator\'s RMProp optimizer\n            D_lr: float, learning rate for discriminator\'s RMSProp optimizer\n            D_steps: int, ratio for how often to train D compared to G\n            clip: float, bound for parameters [-c, c] to enforce K-Lipschitz\n        """"""\n        # Initialize optimizers\n        G_optimizer = optim.Adam(params=[p for p in self.model.G.parameters()\n                                        if p.requires_grad], lr=G_lr)\n        D_optimizer = optim.Adam(params=[p for p in self.model.D.parameters()\n                                        if p.requires_grad], lr=D_lr)\n\n        # Approximate steps/epoch given D_steps per epoch\n        # --> roughly train in the same way as if D_step (1) == G_step (1)\n        epoch_steps = int(np.ceil(len(self.train_iter) / (D_steps)))\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs+1)):\n\n            self.model.train()\n            G_losses, D_losses = [], []\n\n            for _ in range(epoch_steps):\n\n                D_step_loss = []\n\n                for _ in range(D_steps):\n\n                    # Reshape images\n                    images = self.process_batch(self.train_iter)\n\n                    # TRAINING D: Zero out gradients for D\n                    D_optimizer.zero_grad()\n\n                    # Train the discriminator to approximate the Wasserstein\n                    # distance between real, generated distributions\n                    D_loss = self.train_D(images)\n\n                    # Update parameters\n                    D_loss.backward()\n                    D_optimizer.step()\n\n                    # Log results, backpropagate the discriminator network\n                    D_step_loss.append(D_loss.item())\n\n                    # Clamp weights (crudely enforces K-Lipschitz)\n                    self.clip_D_weights(clip)\n\n                # We report D_loss in this way so that G_loss and D_loss have\n                # the same number of entries.\n                D_losses.append(np.mean(D_step_loss))\n\n                # TRAINING G: Zero out gradients for G\n                G_optimizer.zero_grad()\n\n                # Train the generator to (roughly) minimize the approximated\n                # Wasserstein distance\n                G_loss = self.train_G(images)\n\n                # Log results, update parameters\n                G_losses.append(G_loss.item())\n                G_loss.backward()\n                G_optimizer.step()\n\n            # Save progress\n            self.Glosses.extend(G_losses)\n            self.Dlosses.extend(D_losses)\n\n            # Progress logging\n            print (""Epoch[%d/%d], G Loss: %.4f, D Loss: %.4f""\n                   %(epoch, num_epochs, np.mean(G_losses), np.mean(D_losses)))\n            self.num_epochs += 1\n\n            # Visualize generator progress\n            if self.viz:\n                self.generate_images(epoch)\n                plt.show()\n\n    def train_D(self, images):\n        """""" Run 1 step of training for discriminator\n\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            D_loss: wasserstein loss for discriminator,\n            -E[D(x)] + E[D(G(z))]\n        """"""\n        # Sample from the generator\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Score real, generated images\n        DX_score = self.model.D(images) # D(x), ""real""\n        DG_score = self.model.D(G_output) # D(G(x\')), ""fake""\n\n        # Compute WGAN loss for D\n        D_loss = -1 * (torch.mean(DX_score)) + torch.mean(DG_score)\n\n        return D_loss\n\n    def train_G(self, images):\n        """""" Run 1 step of training for generator\n\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            G_loss: wasserstein loss for generator,\n            -E[D(G(z))]\n        """"""\n        # Get noise, classify it using G, then classify the output of G using D.\n        noise = self.compute_noise(images.shape[0], self.model.z_dim) # z\n        G_output = self.model.G(noise) # G(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute WGAN loss for G\n        G_loss = -1 * (torch.mean(DG_score))\n\n        return G_loss\n\n    def compute_noise(self, batch_size, z_dim):\n        """""" Compute random noise for input into the Generator G """"""\n        return to_cuda(torch.randn(batch_size, z_dim))\n\n    def process_batch(self, iterator):\n        """""" Generate a process batch to be input into the Discriminator D """"""\n        images, _ = next(iter(iterator))\n        images = to_cuda(images.view(images.shape[0], -1))\n        return images\n\n    def clip_D_weights(self, clip):\n        for parameter in self.model.D.parameters():\n            parameter.data.clamp_(-clip, clip)\n\n    def generate_images(self, epoch, num_outputs=36, save=True):\n        """""" Visualize progress of generator learning """"""\n        # Turn off any regularization\n        self.model.eval()\n\n        # Sample noise vector\n        noise = self.compute_noise(num_outputs, self.model.z_dim)\n\n        # Transform noise to image\n        images = self.model.G(noise)\n\n        # Reshape to square image size\n        images = images.view(images.shape[0],\n                             self.model.shape,\n                             self.model.shape,\n                             -1).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size, k = int(num_outputs**0.5), 0\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].imshow(images[k].data.numpy(), cmap=\'gray\')\n            k += 1\n\n        # Save images if desired\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(images.unsqueeze(1).data,\n                                         outname + \'reconst_%d.png\'\n                                         %(epoch), nrow=grid_size)\n\n    def viz_loss(self):\n        """""" Visualize loss for the generator, discriminator """"""\n        # Set style, figure size\n        plt.style.use(\'ggplot\')\n        plt.rcParams[""figure.figsize""] = (8,6)\n\n        # Plot Discriminator loss in red\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Dlosses,\n                 \'r\')\n\n        # Plot Generator loss in green\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Glosses,\n                 \'g\')\n\n        # Add legend, title\n        plt.legend([\'Discriminator\', \'Generator\'])\n        plt.title(self.name)\n        plt.show()\n\n    def save_model(self, savepath):\n        """""" Save model state dictionary """"""\n        torch.save(self.model.state_dict(), savepath)\n\n    def load_model(self, loadpath):\n        """""" Load state dictionary into model """"""\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)\n\n\nif __name__ == ""__main__"":\n\n    # Load in binarized MNIST data, separate into data loaders\n    train_iter, val_iter, test_iter = get_data()\n\n    # Init model\n    model = WGAN(image_size=784,\n                  hidden_dim=400,\n                  z_dim=20)\n\n    # Init trainer\n    trainer = WGANTrainer(model=model,\n                          train_iter=train_iter,\n                          val_iter=val_iter,\n                          test_iter=test_iter,\n                          viz=False)\n\n    # Train\n    trainer.train(num_epochs=100,\n                  G_lr=5e-5,\n                  D_lr=5e-5,\n                  D_steps=5,\n                  clip=0.01)\n'"
src/w_gp_gan.py,14,"b'"""""" (WGPGAN) https://arxiv.org/abs/1701.07875\nWasserstein GAN with Gradient Penalties (\'Improved Training of Wasserstein GANs\')\n\nThe output of WGPGAN\'s D is unbounded unless passed through an activation\nfunction. In this implementation, we use a ReLU activation function\nas this empirically improves visualizations for binary MNIST.\n\nWGAN GP roposes a gradient penalty to add to the WGAN discriminator loss as an\nalternative method for enforcing the Lipschitz constraint (previously done via\nweight clipping). This penalty does not suffer from the biasing of the\ndiscriminator toward simple funtions due to weight clipping. Additionally, the\nreformulation of the discriminator by adding a gradient penaltyterm makes batch\nnormalization not necessary. This is notable because batch normalization\nimplicitly changes the discriminator\'s problem from mapping one-to-one to\nmany-to-many.\n""""""\n\nimport torch, torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom itertools import product\nfrom tqdm import tqdm\n\nfrom utils import *\n\n\nclass Generator(nn.Module):\n    """""" Generator. Input is noise, output is a generated image.\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim):\n        super().__init__()\n\n        self.linear = nn.Linear(z_dim, hidden_dim)\n        self.generate = nn.Linear(hidden_dim, image_size)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        generation = torch.sigmoid(self.generate(activated))\n        return generation\n\n\nclass Discriminator(nn.Module):\n    """""" Critic (not trained to classify). Input is an image (real or generated),\n    output is the approximate least-squares distance between z~P(G(z)) and real.\n    """"""\n    def __init__(self, image_size, hidden_dim, output_dim):\n        super().__init__()\n\n        self.linear = nn.Linear(image_size, hidden_dim)\n        self.discriminate = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        activated = F.relu(self.linear(x))\n        discrimination = F.relu(self.discriminate(activated))\n        return discrimination\n\n\nclass WGPGAN(nn.Module):\n    """""" Super class to contain both Discriminator (D) and Generator (G)\n    """"""\n    def __init__(self, image_size, hidden_dim, z_dim, output_dim=1):\n        super().__init__()\n\n        self.__dict__.update(locals())\n\n        self.G = Generator(image_size, hidden_dim, z_dim)\n        self.D = Discriminator(image_size, hidden_dim, output_dim)\n\n        self.shape = int(image_size ** 0.5)\n\n\nclass WGPGANTrainer:\n    """""" Object to hold data iterators, train a GAN variant\n    """"""\n    def __init__(self, model, train_iter, val_iter, test_iter, viz=False):\n        self.model = to_cuda(model)\n        self.name = model.__class__.__name__\n\n        self.train_iter = train_iter\n        self.val_iter = val_iter\n        self.test_iter = test_iter\n\n        self.Glosses = []\n        self.Dlosses = []\n\n        self.viz = viz\n        self.num_epochs = 0\n\n    def train(self, num_epochs, G_lr=1e-4, D_lr=1e-4, D_steps=5):\n        """""" Train a WGAN GP\n\n            Logs progress using G loss, D loss, G(x), D(G(x)), visualizations\n            of Generator output.\n\n        Inputs:\n            num_epochs: int, number of epochs to train for\n            G_lr: float, learning rate for generator\'s Adam optimizer\n            D_lr: float, learning rate for discriminator\'s Adam optimizer\n            D_steps: int, ratio for how often to train D compared to G\n        """"""\n        # Initialize optimizers\n        G_optimizer = optim.Adam(params=[p for p in self.model.G.parameters()\n                                        if p.requires_grad], lr=G_lr)\n        D_optimizer = optim.Adam(params=[p for p in self.model.D.parameters()\n                                        if p.requires_grad], lr=D_lr)\n\n        # Approximate steps/epoch given D_steps per epoch\n        # --> roughly train in the same way as if D_step (1) == G_step (1)\n        epoch_steps = int(np.ceil(len(self.train_iter) / (D_steps)))\n\n        # Begin training\n        for epoch in tqdm(range(1, num_epochs+1)):\n\n            self.model.train()\n            G_losses, D_losses = [], []\n\n            for _ in range(epoch_steps):\n\n                D_step_loss = []\n\n                for _ in range(D_steps):\n\n                    # Reshape images\n                    images = self.process_batch(self.train_iter)\n\n                    # TRAINING D: Zero out gradients for D\n                    D_optimizer.zero_grad()\n\n                    # Train the discriminator to approximate the Wasserstein\n                    # distance between real, generated distributions\n                    D_loss = self.train_D(images)\n\n                    # Update parameters\n                    D_loss.backward()\n                    D_optimizer.step()\n\n                    # Log results, backpropagate the discriminator network\n                    D_step_loss.append(D_loss.item())\n\n                # We report D_loss in this way so that G_loss and D_loss have\n                # the same number of entries.\n                D_losses.append(np.mean(D_step_loss))\n\n                # TRAINING G: Zero out gradients for G\n                G_optimizer.zero_grad()\n\n                # Train the generator to (roughly) minimize the approximated\n                # Wasserstein distance\n                G_loss = self.train_G(images)\n\n                # Log results, update parameters\n                G_losses.append(G_loss.item())\n                G_loss.backward()\n                G_optimizer.step()\n\n            # Save progress\n            self.Glosses.extend(G_losses)\n            self.Dlosses.extend(D_losses)\n\n            # Progress logging\n            print (""Epoch[%d/%d], G Loss: %.4f, D Loss: %.4f""\n                   %(epoch, num_epochs, np.mean(G_losses), np.mean(D_losses)))\n            self.num_epochs += 1\n\n            # Visualize generator progress\n            if self.viz:\n                self.generate_images(epoch)\n                plt.show()\n\n    def train_D(self, images, LAMBDA=10):\n        """""" Run 1 step of training for discriminator\n\n        Input:\n            images: batch of images (reshaped to [batch_size, -1])\n        Output:\n            D_loss: Wasserstein loss for discriminator,\n            -E[D(x)] + E[D(G(z))] + \xce\xbbE[(||\xe2\x88\x87 D(\xce\xb5x + (1 \xe2\x88\x92 \xce\xb5G(z)))|| - 1)^2]\n        """"""\n        # ORIGINAL CRITIC STEPS:\n        # Sample noise, an output from the generator\n        noise = self.compute_noise(images.shape[0], self.model.z_dim)\n        G_output = self.model.G(noise)\n\n        # Use the discriminator to sample real, generated images\n        DX_score = self.model.D(images) # D(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # GRADIENT PENALTY:\n        # Uniformly sample along one straight line per each batch entry.\n        epsilon = to_var(torch.rand(images.shape[0], 1).expand(images.size()))\n\n        # Generate images from the noise, ensure unit gradient norm 1\n        # See Section 4 and Algorithm 1 of original paper for full explanation.\n        G_interpolation = epsilon*images + (1-epsilon)*G_output\n        D_interpolation = self.model.D(G_interpolation)\n\n        # Compute the gradients of D with respect to the noise generated input\n        weight = to_cuda(torch.ones(D_interpolation.size()))\n\n        gradients = torch.autograd.grad(outputs=D_interpolation,\n                                        inputs=G_interpolation,\n                                        grad_outputs=weight,\n                                        only_inputs=True,\n                                        create_graph=True,\n                                        retain_graph=True)[0]\n\n        # Full gradient penalty\n        grad_penalty = LAMBDA * torch.mean((gradients.norm(2, dim=1) - 1)**2)\n\n        # Compute WGAN-GP loss for D\n        D_loss = torch.mean(DG_score) - torch.mean(DX_score) + grad_penalty\n\n        return D_loss\n\n    def train_G(self, images):\n        """""" Run 1 step of training for generator\n\n        Input:\n            images: batch of images reshaped to [batch_size, -1]\n        Output:\n            G_loss: wasserstein loss for generator,\n            -E[D(G(z))]\n        """"""\n        # Get noise, classify it using G, then classify the output of G using D.\n        noise = self.compute_noise(images.shape[0], self.model.z_dim) # z\n        G_output = self.model.G(noise) # G(z)\n        DG_score = self.model.D(G_output) # D(G(z))\n\n        # Compute WGAN-GP loss for G (same loss as WGAN)\n        G_loss = -1 * (torch.mean(DG_score))\n\n        return G_loss\n\n    def compute_noise(self, batch_size, z_dim):\n        """""" Compute random noise for input to the Generator G """"""\n        return to_cuda(torch.randn(batch_size, z_dim))\n\n    def process_batch(self, iterator):\n        """""" Generate a process batch to be input into the discriminator D """"""\n        images, _ = next(iter(iterator))\n        images = to_cuda(images.view(images.shape[0], -1))\n        return images\n\n    def generate_images(self, epoch, num_outputs=36, save=True):\n        """""" Visualize progress of generator learning """"""\n        # Turn off any regularization\n        self.model.eval()\n\n        # Sample noise vector\n        noise = self.compute_noise(num_outputs, self.model.z_dim)\n\n        # Transform noise to image\n        images = self.model.G(noise)\n\n        # Reshape to square image size\n        images = images.view(images.shape[0],\n                             self.model.shape,\n                             self.model.shape,\n                             -1).squeeze()\n\n        # Plot\n        plt.close()\n        grid_size, k = int(num_outputs**0.5), 0\n        fig, ax = plt.subplots(grid_size, grid_size, figsize=(5, 5))\n        for i, j in product(range(grid_size), range(grid_size)):\n            ax[i,j].get_xaxis().set_visible(False)\n            ax[i,j].get_yaxis().set_visible(False)\n            ax[i,j].imshow(images[k].data.numpy(), cmap=\'gray\')\n            k += 1\n\n        # Save images if desired\n        if save:\n            outname = \'../viz/\' + self.name + \'/\'\n            if not os.path.exists(outname):\n                os.makedirs(outname)\n            torchvision.utils.save_image(images.unsqueeze(1).data,\n                                         outname + \'reconst_%d.png\'\n                                         %(epoch), nrow=grid_size)\n\n    def viz_loss(self):\n        """""" Visualize loss for the generator, discriminator """"""\n        # Set style, figure size\n        plt.style.use(\'ggplot\')\n        plt.rcParams[""figure.figsize""] = (8,6)\n\n        # Plot Discriminator loss in red\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Dlosses,\n                 \'r\')\n\n        # Plot Generator loss in green\n        plt.plot(np.linspace(1, self.num_epochs, len(self.Dlosses)),\n                 self.Glosses,\n                 \'g\')\n\n        # Add legend, title\n        plt.legend([\'Discriminator\', \'Generator\'])\n        plt.title(self.name)\n        plt.show()\n\n    def save_model(self, savepath):\n        """""" Save model state dictionary """"""\n        torch.save(self.model.state_dict(), savepath)\n\n    def load_model(self, loadpath):\n        """""" Load state dictionary into model """"""\n        state = torch.load(loadpath)\n        self.model.load_state_dict(state)\n\nif __name__ == ""__main__"":\n\n    # Load in binarized MNIST data, separate into data loaders\n    train_iter, val_iter, test_iter = get_data()\n\n    # Init model\n    model = WGPGAN(image_size=784,\n                   hidden_dim=400,\n                   z_dim=20)\n\n    # Init trainer\n    trainer = WGPGANTrainer(model=model,\n                            train_iter=train_iter,\n                            val_iter=val_iter,\n                            test_iter=test_iter,\n                            viz=False)\n\n    # Train\n    trainer.train(num_epochs=25,\n                  G_lr=1e-4,\n                  D_lr=1e-4,\n                  D_steps=1)\n'"
