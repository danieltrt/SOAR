file_path,api_count,code
pytorch/test.py,0,"b'import numpy as np\nimport cv2\nimport os\nimport subprocess\nimport glob\nfrom options.test_options import TestOptions\nfrom model.net import InpaintingModel_GMCNN\nfrom util.utils import generate_rect_mask, generate_stroke_mask, getLatest\n\nos.environ[\'CUDA_VISIBLE_DEVICES\']=str(np.argmax([int(x.split()[2]) for x in subprocess.Popen(\n        ""nvidia-smi -q -d Memory | grep -A4 GPU | grep Free"", shell=True, stdout=subprocess.PIPE).stdout.readlines()]\n        ))\n\nconfig = TestOptions().parse()\n\nif os.path.isfile(config.dataset_path):\n    pathfile = open(config.dataset_path, \'rt\').read().splitlines()\nelif os.path.isdir(config.dataset_path):\n    pathfile = glob.glob(os.path.join(config.dataset_path, \'*.png\'))\nelse:\n    print(\'Invalid testing data file/folder path.\')\n    exit(1)\ntotal_number = len(pathfile)\ntest_num = total_number if config.test_num == -1 else min(total_number, config.test_num)\nprint(\'The total number of testing images is {}, and we take {} for test.\'.format(total_number, test_num))\n\nprint(\'configuring model..\')\nourModel = InpaintingModel_GMCNN(in_channels=4, opt=config)\nourModel.print_networks()\nif config.load_model_dir != \'\':\n    print(\'Loading pretrained model from {}\'.format(config.load_model_dir))\n    ourModel.load_networks(getLatest(os.path.join(config.load_model_dir, \'*.pth\')))\n    print(\'Loading done.\')\n\nif config.random_mask:\n    np.random.seed(config.seed)\n\nfor i in range(test_num):\n    if config.mask_type == \'rect\':\n        mask, _ = generate_rect_mask(config.img_shapes, config.mask_shapes, config.random_mask)\n    else:\n        mask = generate_stroke_mask(im_size=(config.img_shapes[0], config.img_shapes[1]),\n                                    parts=8, maxBrushWidth=20, maxLength=100, maxVertex=20)\n    image = cv2.imread(pathfile[i])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    h, w = image.shape[:2]\n\n    if h >= config.img_shapes[0] and w >= config.img_shapes[1]:\n        h_start = (h-config.img_shapes[0]) // 2\n        w_start = (w-config.img_shapes[1]) // 2\n        image = image[h_start: h_start+config.img_shapes[0], w_start: w_start+config.img_shapes[1], :]\n    else:\n        t = min(h, w)\n        image = image[(h-t)//2:(h-t)//2+t, (w-t)//2:(w-t)//2+t, :]\n        image = cv2.resize(image, (config.img_shapes[1], config.img_shapes[0]))\n\n    image = np.transpose(image, [2, 0, 1])\n    image = np.expand_dims(image, axis=0)\n    image_vis = image * (1-mask) + 255 * mask\n    image_vis = np.transpose(image_vis[0][::-1,:,:], [1, 2, 0])\n    cv2.imwrite(os.path.join(config.saving_path, \'input_{:03d}.png\'.format(i)), image_vis.astype(np.uint8))\n\n    h, w = image.shape[2:]\n    grid = 4\n    image = image[:, :, :h // grid * grid, :w // grid * grid]\n    mask = mask[:, :, :h // grid * grid, :w // grid * grid]\n    result = ourModel.evaluate(image, mask)\n    result = np.transpose(result[0][::-1,:,:], [1, 2, 0])\n    cv2.imwrite(os.path.join(config.saving_path, \'{:03d}.png\'.format(i)), result)\n    print(\' > {} / {}\'.format(i+1, test_num))\nprint(\'done.\')\n'"
pytorch/train.py,2,"b""import os\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport torchvision.utils as vutils\nfrom tensorboardX import SummaryWriter\nfrom data.data import InpaintingDataset, ToTensor\nfrom model.net import InpaintingModel_GMCNN\nfrom options.train_options import TrainOptions\nfrom util.utils import getLatest\n\nconfig = TrainOptions().parse()\n\nprint('loading data..')\ndataset = InpaintingDataset(config.dataset_path, '', transform=transforms.Compose([\n    ToTensor()\n]))\ndataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, num_workers=4, drop_last=True)\nprint('data loaded..')\n\nprint('configuring model..')\nourModel = InpaintingModel_GMCNN(in_channels=4, opt=config)\nourModel.print_networks()\nif config.load_model_dir != '':\n    print('Loading pretrained model from {}'.format(config.load_model_dir))\n    ourModel.load_networks(getLatest(os.path.join(config.load_model_dir, '*.pth')))\n    print('Loading done.')\n# ourModel = torch.nn.DataParallel(ourModel).cuda()\nprint('model setting up..')\nprint('training initializing..')\nwriter = SummaryWriter(log_dir=config.model_folder)\ncnt = 0\nfor epoch in range(config.epochs):\n\n    for i, data in enumerate(dataloader):\n        gt = data['gt'].cuda()\n        # normalize to values between -1 and 1\n        gt = gt / 127.5 - 1\n\n        data_in = {'gt': gt}\n        ourModel.setInput(data_in)\n        ourModel.optimize_parameters()\n\n        if (i+1) % config.viz_steps == 0:\n            ret_loss = ourModel.get_current_losses()\n            if config.pretrain_network is False:\n                print(\n                    '[%d, %5d] G_loss: %.4f (rec: %.4f, ae: %.4f, adv: %.4f, mrf: %.4f), D_loss: %.4f'\n                    % (epoch + 1, i + 1, ret_loss['G_loss'], ret_loss['G_loss_rec'], ret_loss['G_loss_ae'],\n                       ret_loss['G_loss_adv'], ret_loss['G_loss_mrf'], ret_loss['D_loss']))\n                writer.add_scalar('adv_loss', ret_loss['G_loss_adv'], cnt)\n                writer.add_scalar('D_loss', ret_loss['D_loss'], cnt)\n                writer.add_scalar('G_mrf_loss', ret_loss['G_loss_mrf'], cnt)\n            else:\n                print('[%d, %5d] G_loss: %.4f (rec: %.4f, ae: %.4f)'\n                      % (epoch + 1, i + 1, ret_loss['G_loss'], ret_loss['G_loss_rec'], ret_loss['G_loss_ae']))\n\n            writer.add_scalar('G_loss', ret_loss['G_loss'], cnt)\n            writer.add_scalar('reconstruction_loss', ret_loss['G_loss_rec'], cnt)\n            writer.add_scalar('autoencoder_loss', ret_loss['G_loss_ae'], cnt)\n\n            images = ourModel.get_current_visuals_tensor()\n            im_completed = vutils.make_grid(images['completed'], normalize=True, scale_each=True)\n            im_input = vutils.make_grid(images['input'], normalize=True, scale_each=True)\n            im_gt = vutils.make_grid(images['gt'], normalize=True, scale_each=True)\n            writer.add_image('gt', im_gt, cnt)\n            writer.add_image('input', im_input, cnt)\n            writer.add_image('completed', im_completed, cnt)\n            if (i+1) % config.train_spe == 0:\n                print('saving model ..')\n                ourModel.save_networks(epoch+1)\n        cnt += 1\n    ourModel.save_networks(epoch+1)\n\nwriter.export_scalars_to_json(os.path.join(config.model_folder, 'GMCNN_scalars.json'))\nwriter.close()\n"""
tensorflow/painter_gmcnn.py,0,"b'from tkinter import *\nfrom PIL import Image, ImageTk, ImageDraw\nimport tkinter.filedialog as tkFileDialog\nimport numpy as np\nimport cv2\nimport os\nimport subprocess\nimport argparse\nimport tensorflow as tf\nfrom options.test_options import TestOptions\n\nfrom net.network import GMCNNModel\n\n# os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(np.argmax([int(x.split()[2]) for x in subprocess.Popen(\n#     ""nvidia-smi -q -d Memory | grep -A4 GPU | grep Free"", shell=True, stdout=subprocess.PIPE).stdout.readlines()]))\nos.environ[\'CUDA_VISIBLE_DEVICES\'] = \'0\'\n\n# model_savepath = {\'places2ful\': \'model_places2ful512/\', \'places2\': \'model_places2ful/\', \'celebAHQ\': \'model_celebAHQ/\',\n#                   \'paris_streetview\': \'model_paris/\', \'celebA\': \'model_celebA/\'}\n\nclass Paint(object):\n    MARKER_COLOR = \'white\'\n\n    def __init__(self, config):\n        self.config = config\n\n        self.root = Tk()\n\n        self.rect_button = Button(self.root, text=\'rectangle\', command=self.use_rect, width=12, height=3)\n        self.rect_button.grid(row=0, column=2)\n\n        self.poly_button = Button(self.root, text=\'stroke\', command=self.use_poly, width=12, height=3)\n        self.poly_button.grid(row=1, column=2)\n\n        self.revoke_button = Button(self.root, text=\'revoke\', command=self.revoke, width=12, height=3)\n        self.revoke_button.grid(row=2, column=2)\n\n        self.clear_button = Button(self.root, text=\'clear\', command=self.clear, width=12, height=3)\n        self.clear_button.grid(row=3, column=2)\n\n        self.c = Canvas(self.root, bg=\'white\', width=config.img_shapes[1]+8, height=config.img_shapes[0])\n        self.c.grid(row=0, column=0, rowspan=8)\n\n        self.out = Canvas(self.root, bg=\'white\', width=config.img_shapes[1]+8, height=config.img_shapes[0])\n        self.out.grid(row=0, column=1, rowspan=8)\n\n        self.save_button = Button(self.root, text=""save"", command=self.save, width=12, height=3)\n        self.save_button.grid(row=6, column=2)\n\n        self.load_button = Button(self.root, text=\'load\', command=self.load, width=12, height=3)\n        self.load_button.grid(row=5, column=2)\n\n        self.fill_button = Button(self.root, text=\'fill\', command=self.fill, width=12, height=3)\n        self.fill_button.grid(row=7, column=2)\n        self.filename = None\n\n        self.setup()\n        self.root.mainloop()\n\n    def setup(self):\n        self.old_x = None\n        self.old_y = None\n        self.start_x = None\n        self.start_y = None\n        self.end_x = None\n        self.end_y = None\n        self.eraser_on = False\n        self.active_button = self.rect_button\n        self.isPainting = False\n        self.c.bind(\'<B1-Motion>\', self.paint)\n        self.c.bind(\'<ButtonRelease-1>\', self.reset)\n        self.c.bind(\'<Button-1>\', self.beginPaint)\n        self.c.bind(\'<Enter>\', self.icon2pen)\n        self.c.bind(\'<Leave>\', self.icon2mice)\n        self.mode = \'rect\'\n        self.rect_buf = None\n        self.line_buf = None\n        assert self.mode in [\'rect\', \'poly\']\n        self.paint_color = self.MARKER_COLOR\n        self.mask_candidate = []\n        self.rect_candidate = []\n        self.im_h = None\n        self.im_w = None\n        self.mask = None\n        self.result = None\n        self.blank = None\n        self.line_width = 24\n\n        self.model = GMCNNModel()\n        self.reuse = False\n        sess_config = tf.ConfigProto()\n        sess_config.gpu_options.allow_growth = False\n        self.sess = tf.Session(config=sess_config)\n\n        self.input_image_tf = tf.placeholder(dtype=tf.float32,\n                                             shape=[1, self.config.img_shapes[0], self.config.img_shapes[1], 3])\n        self.input_mask_tf = tf.placeholder(dtype=tf.float32,\n                                            shape=[1, self.config.img_shapes[0], self.config.img_shapes[1], 1])\n\n        output = self.model.evaluate(self.input_image_tf, self.input_mask_tf, config=self.config, reuse=self.reuse)\n        output = (output + 1) * 127.5\n        output = tf.minimum(tf.maximum(output[:, :, :, ::-1], 0), 255)\n        self.output = tf.cast(output, tf.uint8)\n\n        # load pretrained model\n        vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n        assign_ops = list(map(lambda x: tf.assign(x, tf.contrib.framework.load_variable(config.load_model_dir, x.name)),\n                              vars_list))\n        self.sess.run(assign_ops)\n        print(\'Model loaded.\')\n\n    def checkResp(self):\n        assert len(self.mask_candidate) == len(self.rect_candidate)\n\n    def load(self):\n        self.filename = tkFileDialog.askopenfilename(initialdir=\'./imgs\',\n                                                     title=""Select file"",\n                                                     filetypes=((""png files"", ""*.png""), (""jpg files"", ""*.jpg""),\n                                                                (""all files"", ""*.*"")))\n        self.filename_ = self.filename.split(\'/\')[-1][:-4]\n        self.filepath = \'/\'.join(self.filename.split(\'/\')[:-1])\n        print(self.filename_, self.filepath)\n        try:\n            photo = Image.open(self.filename)\n            self.image = cv2.imread(self.filename)\n        except:\n            print(\'do not load image\')\n        else:\n            self.im_w, self.im_h = photo.size\n            self.mask = np.zeros((self.im_h, self.im_w, 1)).astype(np.uint8)\n            print(photo.size)\n            self.displayPhoto = photo\n            self.displayPhoto = self.displayPhoto.resize((self.im_w, self.im_h))\n            self.draw = ImageDraw.Draw(self.displayPhoto)\n            self.photo_tk = ImageTk.PhotoImage(image=self.displayPhoto)\n            self.c.create_image(0, 0, image=self.photo_tk, anchor=NW)\n            self.rect_candidate.clear()\n            self.mask_candidate.clear()\n            if self.blank is None:\n                self.blank = Image.open(\'imgs/blank.png\')\n            self.blank = self.blank.resize((self.im_w, self.im_h))\n            self.blank_tk = ImageTk.PhotoImage(image=self.blank)\n            self.out.create_image(0, 0, image=self.blank_tk, anchor=NW)\n\n    def save(self):\n        img = np.array(self.displayPhoto)\n        cv2.imwrite(os.path.join(self.filepath, \'tmp.png\'), img)\n\n        if self.mode == \'rect\':\n            self.mask[:,:,:] = 0\n            for rect in self.mask_candidate:\n                self.mask[rect[1]:rect[3], rect[0]:rect[2], :] = 1\n        cv2.imwrite(os.path.join(self.filepath, self.filename_+\'_mask.png\'), self.mask*255)\n        cv2.imwrite(os.path.join(self.filepath, self.filename_ + \'_gm_result.png\'), self.result[0][:, :, ::-1])\n\n    def fill(self):\n        if self.mode == \'rect\':\n            for rect in self.mask_candidate:\n                self.mask[rect[1]:rect[3], rect[0]:rect[2], :] = 1\n        image = np.expand_dims(self.image, 0)\n        mask = np.expand_dims(self.mask, 0)\n        print(image.shape)\n        print(mask.shape)\n\n        self.result = self.sess.run(self.output, feed_dict={self.input_image_tf: image * 1.0,\n                                                            self.input_mask_tf: mask * 1.0})\n        cv2.imwrite(\'./imgs/tmp.png\', self.result[0][:, :, ::-1])\n\n        photo = Image.open(\'./imgs/tmp.png\')\n        self.displayPhotoResult = photo\n        self.displayPhotoResult = self.displayPhotoResult.resize((self.im_w, self.im_h))\n        self.photo_tk_result = ImageTk.PhotoImage(image=self.displayPhotoResult)\n        self.out.create_image(0, 0, image=self.photo_tk_result, anchor=NW)\n        return\n\n    def use_rect(self):\n        self.activate_button(self.rect_button)\n        self.mode = \'rect\'\n\n    def use_poly(self):\n        self.activate_button(self.poly_button)\n        self.mode = \'poly\'\n\n    def revoke(self):\n        if len(self.rect_candidate) > 0:\n            self.c.delete(self.rect_candidate[-1])\n            self.rect_candidate.remove(self.rect_candidate[-1])\n            self.mask_candidate.remove(self.mask_candidate[-1])\n        self.checkResp()\n\n    def clear(self):\n        self.mask = np.zeros((self.im_h, self.im_w, 1)).astype(np.uint8)\n        if self.mode == \'poly\':\n            photo = Image.open(self.filename)\n            self.image = cv2.imread(self.filename)\n            self.displayPhoto = photo\n            self.displayPhoto = self.displayPhoto.resize((self.im_w, self.im_h))\n            self.draw = ImageDraw.Draw(self.displayPhoto)\n            self.photo_tk = ImageTk.PhotoImage(image=self.displayPhoto)\n            self.c.create_image(0, 0, image=self.photo_tk, anchor=NW)\n        else:\n            if self.rect_candidate is None or len(self.rect_candidate) == 0:\n                return\n            for item in self.rect_candidate:\n                self.c.delete(item)\n            self.rect_candidate.clear()\n            self.mask_candidate.clear()\n            self.checkResp()\n\n    #TODO: reset canvas\n    #TODO: undo and redo\n    #TODO: draw triangle, rectangle, oval, text\n\n    def activate_button(self, some_button, eraser_mode=False):\n        self.active_button.config(relief=RAISED)\n        some_button.config(relief=SUNKEN)\n        self.active_button = some_button\n        self.eraser_on = eraser_mode\n\n    def beginPaint(self, event):\n        self.start_x = event.x\n        self.start_y = event.y\n        self.isPainting = True\n\n    def paint(self, event):\n        if self.start_x and self.start_y and self.mode == \'rect\':\n            self.end_x = max(min(event.x, self.im_w), 0)\n            self.end_y = max(min(event.y, self.im_h), 0)\n            rect = self.c.create_rectangle(self.start_x, self.start_y, self.end_x, self.end_y, fill=self.paint_color)\n            if self.rect_buf is not None:\n                self.c.delete(self.rect_buf)\n            self.rect_buf = rect\n        elif self.old_x and self.old_y and self.mode == \'poly\':\n            line = self.c.create_line(self.old_x, self.old_y, event.x, event.y,\n                                      width=self.line_width, fill=self.paint_color, capstyle=ROUND,\n                                      smooth=True, splinesteps=36)\n            cv2.line(self.mask, (self.old_x, self.old_y), (event.x, event.y), (1), self.line_width)\n        self.old_x = event.x\n        self.old_y = event.y\n\n    def reset(self, event):\n        self.old_x, self.old_y = None, None\n        if self.mode == \'rect\':\n            self.isPainting = False\n            rect = self.c.create_rectangle(self.start_x, self.start_y, self.end_x, self.end_y,\n                                           fill=self.paint_color)\n            if self.rect_buf is not None:\n                self.c.delete(self.rect_buf)\n            self.rect_buf = None\n            self.rect_candidate.append(rect)\n\n            x1, y1, x2, y2 = min(self.start_x, self.end_x), min(self.start_y, self.end_y),\\\n                             max(self.start_x, self.end_x), max(self.start_y, self.end_y)\n            # up left corner, low right corner\n            self.mask_candidate.append((x1, y1, x2, y2))\n            print(self.mask_candidate[-1])\n\n    def icon2pen(self, event):\n        return\n\n    def icon2mice(self, event):\n        return\n\n\nif __name__ == \'__main__\':\n    config = TestOptions().parse()\n    config.mode = \'silent\'\n    ge = Paint(config)\n'"
tensorflow/test.py,0,"b'import numpy as np\nimport cv2\nimport os\nimport subprocess\nimport glob\nimport tensorflow as tf\nfrom options.test_options import TestOptions\nfrom util.util import generate_mask_rect, generate_mask_stroke\nfrom net.network import GMCNNModel\n\nos.environ[\'CUDA_VISIBLE_DEVICES\']=str(np.argmax([int(x.split()[2]) for x in subprocess.Popen(\n        ""nvidia-smi -q -d Memory | grep -A4 GPU | grep Free"", shell=True, stdout=subprocess.PIPE).stdout.readlines()]\n        ))\n\nconfig = TestOptions().parse()\n\nif os.path.isfile(config.dataset_path):\n    pathfile = open(config.dataset_path, \'rt\').read().splitlines()\nelif os.path.isdir(config.dataset_path):\n    pathfile = glob.glob(os.path.join(config.dataset_path, \'*.png\'))\nelse:\n    print(\'Invalid testing data file/folder path.\')\n    exit(1)\ntotal_number = len(pathfile)\ntest_num = total_number if config.test_num == -1 else min(total_number, config.test_num)\nprint(\'The total number of testing images is {}, and we take {} for test.\'.format(total_number, test_num))\n\nmodel = GMCNNModel()\n\nreuse = False\nsess_config = tf.ConfigProto()\nsess_config.gpu_options.allow_growth = False\nwith tf.Session(config=sess_config) as sess:\n    input_image_tf = tf.placeholder(dtype=tf.float32, shape=[1, config.img_shapes[0], config.img_shapes[1], 3])\n    input_mask_tf = tf.placeholder(dtype=tf.float32, shape=[1, config.img_shapes[0], config.img_shapes[1], 1])\n\n    output = model.evaluate(input_image_tf, input_mask_tf, config=config, reuse=reuse)\n    output = (output + 1) * 127.5\n    output = tf.minimum(tf.maximum(output[:, :, :, ::-1], 0), 255)\n    output = tf.cast(output, tf.uint8)\n\n    # load pretrained model\n    vars_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n    assign_ops = list(map(lambda x: tf.assign(x, tf.contrib.framework.load_variable(config.load_model_dir, x.name)),\n                          vars_list))\n    sess.run(assign_ops)\n    print(\'Model loaded.\')\n    total_time = 0\n\n    if config.random_mask:\n        np.random.seed(config.seed)\n\n    for i in range(test_num):\n        if config.mask_type == \'rect\':\n            mask = generate_mask_rect(config.img_shapes, config.mask_shapes, config.random_mask)\n        else:\n            mask = generate_mask_stroke(im_size=(config.img_shapes[0], config.img_shapes[1]),\n                                        parts=8, maxBrushWidth=24, maxLength=100, maxVertex=20)\n        image = cv2.imread(pathfile[i])\n\n        h, w = image.shape[:2]\n\n        if h >= config.img_shapes[0] and w >= config.img_shapes[1]:\n            h_start = (h-config.img_shapes[0]) // 2\n            w_start = (w-config.img_shapes[1]) // 2\n            image = image[h_start: h_start+config.img_shapes[0], w_start: w_start+config.img_shapes[1], :]\n        else:\n            t = min(h, w)\n            image = image[(h-t)//2:(h-t)//2+t, (w-t)//2:(w-t)//2+t, :]\n            image = cv2.resize(image, (config.img_shapes[1], config.img_shapes[0]))\n\n        # cv2.imwrite(os.path.join(config.saving_path, \'gt_{:03d}.png\'.format(i)), image.astype(np.uint8))\n        image = image * (1-mask) + 255 * mask\n        cv2.imwrite(os.path.join(config.saving_path, \'input_{:03d}.png\'.format(i)), image.astype(np.uint8))\n\n        assert image.shape[:2] == mask.shape[:2]\n\n        h, w = image.shape[:2]\n        grid = 4\n        image = image[:h // grid * grid, :w // grid * grid, :]\n        mask = mask[:h // grid * grid, :w // grid * grid, :]\n\n        image = np.expand_dims(image, 0)\n        mask = np.expand_dims(mask, 0)\n\n        result = sess.run(output, feed_dict={input_image_tf: image, input_mask_tf: mask})\n        cv2.imwrite(os.path.join(config.saving_path, \'{:03d}.png\'.format(i)), result[0][:, :, ::-1])\n        print(\' > {} / {}\'.format(i+1, test_num))\nprint(\'done.\')\n'"
tensorflow/train.py,0,"b'import os\nimport tensorflow as tf\nfrom net.network import GMCNNModel\nfrom data.data import DataLoader\nfrom options.train_options import TrainOptions\n\nconfig = TrainOptions().parse()\n\nmodel = GMCNNModel()\n\n# training data\n# print(config.img_shapes)\ndataLoader = DataLoader(filename=config.dataset_path, batch_size=config.batch_size,\n                        im_size=config.img_shapes)\nimages = dataLoader.next()[:, :, :, ::-1] # input BRG images\ng_vars, d_vars, losses = model.build_net(images, config=config)\n\nlr = tf.get_variable(\n    \'lr\', shape=[], trainable=False,\n    initializer=tf.constant_initializer(config.lr))\n\ng_optimizer = tf.train.AdamOptimizer(lr, beta1=0.5, beta2=0.9)\nd_optimizer = g_optimizer\n\ng_train_op = g_optimizer.minimize(losses[\'g_loss\'], var_list=g_vars)\nd_train_op = d_optimizer.minimize(losses[\'d_loss\'], var_list=d_vars)\n\nsaver = tf.train.Saver(max_to_keep=20, keep_checkpoint_every_n_hours=1)\n\nsummary_op = tf.summary.merge_all()\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    if config.load_model_dir != \'\':\n        print(\'[-] Loading the pretrained model from: {}\'.format(config.load_model_dir))\n        ckpt = tf.train.get_checkpoint_state(config.load_model_dir)\n        if ckpt:\n            # saver.restore(sess, tf.train.latest_checkpoint(config.load_model_dir))\n            assign_ops = list(\n                map(lambda x: tf.assign(x, tf.contrib.framework.load_variable(config.load_model_dir, x.name)),\n                    g_vars))\n            sess.run(assign_ops)\n            print(""[*] Loading SUCCESS."")\n        else:\n            print(""[x] Loading ERROR."")\n\n    summary_writer = tf.summary.FileWriter(config.model_folder, sess.graph, flush_secs=30)\n\n    coord = tf.train.Coordinator()\n    thread = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n    for step in range(1, config.max_iters+1):\n\n        if config.pretrain_network is False:\n            for _ in range(5):\n                _, d_loss = sess.run([d_train_op, losses[\'d_loss\']])\n\n        _, g_loss = sess.run([g_train_op, losses[\'g_loss\']])\n\n        if step % config.viz_steps == 0:\n            print(\'[{:04d}, {:04d}] G_loss > {}\'.format(step // config.train_spe, step % config.train_spe, g_loss))\n            summary_writer.add_summary(sess.run(summary_op), global_step=step)\n\n        if step % config.train_spe == 0:\n            saver.save(sess, os.path.join(config.model_folder, config.model_prefix), step)\n\n    coord.request_stop()\n    coord.join(thread)\n'"
pytorch/data/__init__.py,0,b''
pytorch/data/data.py,3,"b""import torch\nimport numpy as np\nimport cv2\nimport os\nfrom torch.utils.data import Dataset\n\nclass ToTensor(object):\n    def __call__(self, sample):\n        entry = {}\n        for k in sample:\n            if k == 'rect':\n                entry[k] = torch.IntTensor(sample[k])\n            else:\n                entry[k] = torch.FloatTensor(sample[k])\n        return entry\n\n\nclass InpaintingDataset(Dataset):\n    def __init__(self, info_list, root_dir='', im_size=(256, 256), transform=None):\n        self.filenames = open(info_list, 'rt').read().splitlines()\n        self.root_dir = root_dir\n        self.transform = transform\n        self.im_size = im_size\n        np.random.seed(2018)\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def read_image(self, filepath):\n        image = cv2.imread(filepath)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        h, w, c = image.shape\n        if h != self.im_size[0] or w != self.im_size[1]:\n            ratio = max(1.0*self.im_size[0]/h, 1.0*self.im_size[1]/w)\n            im_scaled = cv2.resize(image, None, fx=ratio, fy=ratio)\n            h, w, _ = im_scaled.shape\n            h_idx = (h-self.im_size[0]) // 2\n            w_idx = (w-self.im_size[1]) // 2\n            im_scaled = im_scaled[h_idx:h_idx+self.im_size[0], w_idx:w_idx+self.im_size[1],:]\n            im_scaled = np.transpose(im_scaled, [2, 0, 1])\n        else:\n            im_scaled = np.transpose(image, [2, 0, 1])\n        return im_scaled\n\n    def __getitem__(self, idx):\n        image = self.read_image(os.path.join(self.root_dir, self.filenames[idx]))\n        sample = {'gt': image}\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n"""
pytorch/model/__init__.py,0,b''
pytorch/model/basemodel.py,8,"b""import os\nimport torch\nimport torch.nn as nn\n\n# a complex model consisted of several nets, and each net will be explicitly defined in other py class files\nclass BaseModel(nn.Module):\n    def __init__(self):\n        super(BaseModel,self).__init__()\n\n    def init(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.save_dir = opt.model_folder\n        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n        self.model_names = []\n\n    def setInput(self, inputData):\n        self.input = inputData\n\n    def forward(self):\n        pass\n\n    def optimize_parameters(self):\n        pass\n\n    def get_current_visuals(self):\n        pass\n\n    def get_current_losses(self):\n        pass\n\n    def update_learning_rate(self):\n        pass\n\n    def test(self):\n        with torch.no_grad():\n            self.forward()\n\n    # save models to the disk\n    def save_networks(self, which_epoch):\n        for name in self.model_names:\n            if isinstance(name, str):\n                save_filename = '%s_net_%s.pth' % (which_epoch, name)\n                save_path = os.path.join(self.save_dir, save_filename)\n                net = getattr(self, 'net' + name)\n\n                if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n                    torch.save(net.state_dict(), save_path)\n                    # net.cuda(self.gpu_ids[0])\n                else:\n                    torch.save(net.state_dict(), save_path)\n\n    def __patch_instance_norm_state_dict(self, state_dict, module, keys, i=0):\n        key = keys[i]\n        if i + 1 == len(keys):  # at the end, pointing to a parameter/buffer\n            if module.__class__.__name__.startswith('InstanceNorm') and \\\n                    (key == 'running_mean' or key == 'running_var'):\n                if getattr(module, key) is None:\n                    state_dict.pop('.'.join(keys))\n        else:\n            self.__patch_instance_norm_state_dict(state_dict, getattr(module, key), keys, i + 1)\n\n    # load models from the disk\n    def load_networks(self, load_path):\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                if isinstance(net, torch.nn.DataParallel):\n                    net = net.module\n                print('loading the model from %s' % load_path)\n                # if you are using PyTorch newer than 0.4 (e.g., built from\n                # GitHub source), you can remove str() on self.device\n                state_dict = torch.load(load_path)\n                # patch InstanceNorm checkpoints prior to 0.4\n                for key in list(state_dict.keys()):  # need to copy keys here because we mutate in loop\n                    self.__patch_instance_norm_state_dict(state_dict, net, key.split('.'))\n                net.load_state_dict(state_dict)\n\n    # print network information\n    def print_networks(self, verbose=True):\n        print('---------- Networks initialized -------------')\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, 'net' + name)\n                num_params = 0\n                for param in net.parameters():\n                    num_params += param.numel()\n                if verbose:\n                    print(net)\n                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))\n        print('-----------------------------------------------')\n\n    # set requies_grad=Fasle to avoid computation\n    def set_requires_grad(self, nets, requires_grad=False):\n        if not isinstance(nets, list):\n            nets = [nets]\n        for net in nets:\n            if net is not None:\n                for param in net.parameters():\n                    param.requires_grad = requires_grad\n"""
pytorch/model/basenet.py,6,"b""import os\nimport torch\nimport torch.nn as nn\n\nclass BaseNet(nn.Module):\n    def __init__(self):\n        super(BaseNet, self).__init__()\n\n    def init(self, opt):\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.save_dir = opt.checkpoint_dir\n        self.device = torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n\n    def forward(self, *input):\n        return super(BaseNet, self).forward(*input)\n\n    def test(self, *input):\n        with torch.no_grad():\n            self.forward(*input)\n\n    def save_network(self, network_label, epoch_label):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        torch.save(self.cpu().state_dict(), save_path)\n\n    def load_network(self, network_label, epoch_label):\n        save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n        save_path = os.path.join(self.save_dir, save_filename)\n        if not os.path.isfile(save_path):\n            print('%s not exists yet!' % save_path)\n        else:\n            try:\n                self.load_state_dict(torch.load(save_path))\n            except:\n                pretrained_dict = torch.load(save_path)\n                model_dict = self.state_dict()\n                try:\n                    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n                    self.load_state_dict(pretrained_dict)\n                    print('Pretrained network %s has excessive layers; Only loading layers that are used' % network_label)\n                except:\n                    print('Pretrained network %s has fewer layers; The following are not initialized: ' % network_label)\n                    for k, v in pretrained_dict.items():\n                        if v.size() == model_dict[k].size():\n                            model_dict[k] = v\n\n                    for k, v in model_dict.items():\n                        if k not in pretrained_dict or v.size() != pretrained_dict[k].size():\n                            print(k.split('.')[0])\n                    self.load_state_dict(model_dict)\n"""
pytorch/model/layer.py,9,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom util.utils import gauss_kernel\nimport torchvision.models as models\nimport numpy as np\n\n\nclass Conv2d_BN(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n        super(Conv2d_BN, self).__init__()\n        self.model = nn.Sequential([\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias),\n            nn.BatchNorm2d(out_channels)\n        ])\n\n    def forward(self, *input):\n        return self.model(*input)\n\n\nclass upsampling(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, scale=2):\n        super(upsampling, self).__init__()\n        assert isinstance(scale, int)\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding,\n                     dilation=dilation, groups=groups, bias=bias)\n        self.scale = scale\n\n    def forward(self, x):\n        h, w = x.size(2) * self.scale, x.size(3) * self.scale\n        xout = self.conv(F.interpolate(input=x, size=(h, w), mode='nearest', align_corners=True))\n        return xout\n\n\nclass PureUpsampling(nn.Module):\n    def __init__(self, scale=2, mode='bilinear'):\n        super(PureUpsampling, self).__init__()\n        assert isinstance(scale, int)\n        self.scale = scale\n        self.mode = mode\n\n    def forward(self, x):\n        h, w = x.size(2) * self.scale, x.size(3) * self.scale\n        if self.mode == 'nearest':\n            xout = F.interpolate(input=x, size=(h, w), mode=self.mode)\n        else:\n            xout = F.interpolate(input=x, size=(h, w), mode=self.mode, align_corners=True)\n        return xout\n\n\nclass GaussianBlurLayer(nn.Module):\n    def __init__(self, size, sigma, in_channels=1, stride=1, pad=1):\n        super(GaussianBlurLayer, self).__init__()\n        self.size = size\n        self.sigma = sigma\n        self.ch = in_channels\n        self.stride = stride\n        self.pad = nn.ReflectionPad2d(pad)\n\n    def forward(self, x):\n        kernel = gauss_kernel(self.size, self.sigma, self.ch, self.ch)\n        kernel_tensor = torch.from_numpy(kernel)\n        kernel_tensor = kernel_tensor.cuda()\n        x = self.pad(x)\n        blurred = F.conv2d(x, kernel_tensor, stride=self.stride)\n        return blurred\n\n\nclass ConfidenceDrivenMaskLayer(nn.Module):\n    def __init__(self, size=65, sigma=1.0/40, iters=7):\n        super(ConfidenceDrivenMaskLayer, self).__init__()\n        self.size = size\n        self.sigma = sigma\n        self.iters = iters\n        self.propagationLayer = GaussianBlurLayer(size, sigma, pad=32)\n\n    def forward(self, mask):\n        # here mask 1 indicates missing pixels and 0 indicates the valid pixels\n        init = 1 - mask\n        mask_confidence = None\n        for i in range(self.iters):\n            mask_confidence = self.propagationLayer(init)\n            mask_confidence = mask_confidence * mask\n            init = mask_confidence + (1 - mask)\n        return mask_confidence\n\n\nclass VGG19(nn.Module):\n    def __init__(self, pool='max'):\n        super(VGG19, self).__init__()\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        if pool == 'max':\n            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n        elif pool == 'avg':\n            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x):\n        out = {}\n        out['r11'] = F.relu(self.conv1_1(x))\n        out['r12'] = F.relu(self.conv1_2(out['r11']))\n        out['p1'] = self.pool1(out['r12'])\n        out['r21'] = F.relu(self.conv2_1(out['p1']))\n        out['r22'] = F.relu(self.conv2_2(out['r21']))\n        out['p2'] = self.pool2(out['r22'])\n        out['r31'] = F.relu(self.conv3_1(out['p2']))\n        out['r32'] = F.relu(self.conv3_2(out['r31']))\n        out['r33'] = F.relu(self.conv3_3(out['r32']))\n        out['r34'] = F.relu(self.conv3_4(out['r33']))\n        out['p3'] = self.pool3(out['r34'])\n        out['r41'] = F.relu(self.conv4_1(out['p3']))\n        out['r42'] = F.relu(self.conv4_2(out['r41']))\n        out['r43'] = F.relu(self.conv4_3(out['r42']))\n        out['r44'] = F.relu(self.conv4_4(out['r43']))\n        out['p4'] = self.pool4(out['r44'])\n        out['r51'] = F.relu(self.conv5_1(out['p4']))\n        out['r52'] = F.relu(self.conv5_2(out['r51']))\n        out['r53'] = F.relu(self.conv5_3(out['r52']))\n        out['r54'] = F.relu(self.conv5_4(out['r53']))\n        out['p5'] = self.pool5(out['r54'])\n        return out\n\n\nclass VGG19FeatLayer(nn.Module):\n    def __init__(self):\n        super(VGG19FeatLayer, self).__init__()\n        self.vgg19 = models.vgg19(pretrained=True).features.eval().cuda()\n        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1).cuda()\n\n    def forward(self, x):\n        out = {}\n        x = x - self.mean\n        ci = 1\n        ri = 0\n        for layer in self.vgg19.children():\n            if isinstance(layer, nn.Conv2d):\n                ri += 1\n                name = 'conv{}_{}'.format(ci, ri)\n            elif isinstance(layer, nn.ReLU):\n                ri += 1\n                name = 'relu{}_{}'.format(ci, ri)\n                layer = nn.ReLU(inplace=False)\n            elif isinstance(layer, nn.MaxPool2d):\n                ri = 0\n                name = 'pool_{}'.format(ci)\n                ci += 1\n            elif isinstance(layer, nn.BatchNorm2d):\n                name = 'bn_{}'.format(ci)\n            else:\n                raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n            x = layer(x)\n            out[name] = x\n        # print([x for x in out])\n        return out\n\n\ndef init_weights(net, init_type='normal', gain=0.02):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n            if init_type == 'normal':\n                nn.init.normal_(m.weight.data, 0.0, gain)\n            elif init_type == 'xavier':\n                nn.init.xavier_normal_(m.weight.data, gain=gain)\n            elif init_type == 'kaiming':\n                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n            elif init_type == 'orthogonal':\n                nn.init.orthogonal_(m.weight.data, gain=gain)\n            else:\n                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n            if hasattr(m, 'bias') and m.bias is not None:\n                nn.init.constant_(m.bias.data, 0.0)\n        elif classname.find('BatchNorm2d') != -1:\n            nn.init.normal_(m.weight.data, 1.0, gain)\n            nn.init.constant_(m.bias.data, 0.0)\n\n    print('initialize network with %s' % init_type)\n    net.apply(init_func)\n\n\ndef init_net(net, init_type='normal', gpu_ids=[]):\n    if len(gpu_ids) > 0:\n        assert(torch.cuda.is_available())\n        net.to(gpu_ids[0])\n        net = torch.nn.DataParallel(net, gpu_ids)\n    init_weights(net, init_type)\n    return net\n\n\ndef l2normalize(v, eps=1e-12):\n    return v / (v.norm()+eps)\n\n\nclass SpectralNorm(nn.Module):\n    def __init__(self, module, name='weight', power_iteration=1):\n        super(SpectralNorm, self).__init__()\n        self.module = module\n        self.name = name\n        self.power_iteration = power_iteration\n        if not self._made_params():\n            self._make_params()\n\n    def _update_u_v(self):\n        u = getattr(self.module, self.name + '_u')\n        v = getattr(self.module, self.name + '_v')\n        w = getattr(self.module, self.name + '_bar')\n\n        height = w.data.shape[0]\n        for _ in range(self.power_iteration):\n            v.data = l2normalize(torch.mv(torch.t(w.view(height, -1).data), u.data))\n            u.data = l2normalize(torch.mv(w.view(height, -1).data, v.data))\n\n        sigma = u.dot(w.view(height, -1).mv(v))\n        setattr(self.module, self.name, w / sigma.expand_as(w))\n\n    def _made_params(self):\n        try:\n            u = getattr(self.module, self.name + '_u')\n            v = getattr(self.module, self.name + '_v')\n            w = getattr(self.module, self.name + '_bar')\n            return True\n        except AttributeError:\n            return False\n\n    def _make_params(self):\n        w = getattr(self.module, self.name)\n\n        height = w.data.shape[0]\n        width = w.view(height, -1).data.shape[1]\n\n        u = nn.Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n        v = nn.Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n        u.data = l2normalize(u.data)\n        v.data = l2normalize(v.data)\n        w_bar = nn.Parameter(w.data)\n\n        del self.module._parameters[self.name]\n\n        self.module.register_parameter(self.name+'_u', u)\n        self.module.register_parameter(self.name+'_v', v)\n        self.module.register_parameter(self.name+'_bar', w_bar)\n\n    def forward(self, *input):\n        self._update_u_v()\n        return self.module.forward(*input)\n\n\nclass PartialConv(nn.Module):\n    def __init__(self, in_channels=3, out_channels=32, ksize=3, stride=1):\n        super(PartialConv, self).__init__()\n        self.ksize = ksize\n        self.stride = stride\n        self.fnum = 32\n        self.padSize = self.ksize // 2\n        self.pad = nn.ReflectionPad2d(self.padSize)\n        self.eplison = 1e-5\n        self.conv = nn.Conv2d(in_channels, out_channels, stride=stride, kernel_size=ksize)\n\n    def forward(self, x, mask):\n\n        mask_ch = mask.size(1)\n        sum_kernel_np = np.ones((mask_ch, mask_ch, self.ksize, self.ksize), dtype=np.float32)\n        sum_kernel = torch.from_numpy(sum_kernel_np).cuda()\n\n        x = x * mask / (F.conv2d(mask, sum_kernel, stride=1, padding=self.padSize)+self.eplison)\n        x = self.pad(x)\n        x = self.conv(x)\n        mask = F.max_pool2d(mask, self.ksize, stride=self.stride, padding=self.padSize)\n        return x, mask\n\n\nclass GatedConv(nn.Module):\n    def __init__(self, in_channels=3, out_channels=32, ksize=3, stride=1, act=F.elu):\n        super(GatedConv, self).__init__()\n        self.ksize = ksize\n        self.stride = stride\n        self.act = act\n        self.padSize = self.ksize // 2\n        self.pad = nn.ReflectionPad2d(self.padSize)\n        self.convf = nn.Conv2d(in_channels, out_channels, stride=stride, kernel_size=ksize)\n        self.convm = nn.Conv2d(in_channels, out_channels, stride=stride, kernel_size=ksize,\n                               padding=self.padSize)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.convf(x)\n        x = self.act(x)\n        m = self.convm(x)\n        m = F.sigmoid(m)\n        x = x * m\n        return x\n\n\nclass GatedDilatedConv(nn.Module):\n    def __init__(self, in_channels, out_channels, ksize=3, stride=1, pad=1, dilation=2, act=F.elu):\n        super(GatedDilatedConv, self).__init__()\n        self.ksize = ksize\n        self.stride = stride\n        self.act = act\n        self.padSize = pad\n        self.pad = nn.ReflectionPad2d(self.padSize)\n        self.convf = nn.Conv2d(in_channels, out_channels, stride=stride, kernel_size=ksize, dilation=dilation)\n        self.convm = nn.Conv2d(in_channels, out_channels, stride=stride, kernel_size=ksize, dilation=dilation,\n                               padding=self.padSize)\n\n    def forward(self, x):\n        x = self.pad(x)\n        x = self.convf(x)\n        x = self.act(x)\n        m = self.convm(x)\n        m = F.sigmoid(m)\n        x = x * m\n        return x\n"""
pytorch/model/loss.py,22,"b""import torch\nimport torch.nn as nn\nimport torch.autograd as autograd\nimport torch.nn.functional as F\nfrom model.layer import VGG19FeatLayer\nfrom functools import reduce\n\nclass WGANLoss(nn.Module):\n    def __init__(self):\n        super(WGANLoss, self).__init__()\n\n    def __call__(self, input, target):\n        d_loss = (input - target).mean()\n        g_loss = -input.mean()\n        return {'g_loss': g_loss, 'd_loss': d_loss}\n\n\ndef gradient_penalty(xin, yout, mask=None):\n    gradients = autograd.grad(yout, xin, create_graph=True,\n                              grad_outputs=torch.ones(yout.size()).cuda(), retain_graph=True, only_inputs=True)[0]\n    if mask is not None:\n        gradients = gradients * mask\n    gradients = gradients.view(gradients.size(0), -1)\n    gp = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gp\n\n\ndef random_interpolate(gt, pred):\n    batch_size = gt.size(0)\n    alpha = torch.rand(batch_size, 1, 1, 1).cuda()\n    # alpha = alpha.expand(gt.size()).cuda()\n    interpolated = gt * alpha + pred * (1 - alpha)\n    return interpolated\n\n\nclass IDMRFLoss(nn.Module):\n    def __init__(self, featlayer=VGG19FeatLayer):\n        super(IDMRFLoss, self).__init__()\n        self.featlayer = featlayer()\n        self.feat_style_layers = {'relu3_2': 1.0, 'relu4_2': 1.0}\n        self.feat_content_layers = {'relu4_2': 1.0}\n        self.bias = 1.0\n        self.nn_stretch_sigma = 0.5\n        self.lambda_style = 1.0\n        self.lambda_content = 1.0\n\n    def sum_normalize(self, featmaps):\n        reduce_sum = torch.sum(featmaps, dim=1, keepdim=True)\n        return featmaps / reduce_sum\n\n    def patch_extraction(self, featmaps):\n        patch_size = 1\n        patch_stride = 1\n        patches_as_depth_vectors = featmaps.unfold(2, patch_size, patch_stride).unfold(3, patch_size, patch_stride)\n        self.patches_OIHW = patches_as_depth_vectors.permute(0, 2, 3, 1, 4, 5)\n        dims = self.patches_OIHW.size()\n        self.patches_OIHW = self.patches_OIHW.view(-1, dims[3], dims[4], dims[5])\n        return self.patches_OIHW\n\n    def compute_relative_distances(self, cdist):\n        epsilon = 1e-5\n        div = torch.min(cdist, dim=1, keepdim=True)[0]\n        relative_dist = cdist / (div + epsilon)\n        return relative_dist\n\n    def exp_norm_relative_dist(self, relative_dist):\n        scaled_dist = relative_dist\n        dist_before_norm = torch.exp((self.bias - scaled_dist)/self.nn_stretch_sigma)\n        self.cs_NCHW = self.sum_normalize(dist_before_norm)\n        return self.cs_NCHW\n\n    def mrf_loss(self, gen, tar):\n        meanT = torch.mean(tar, 1, keepdim=True)\n        gen_feats, tar_feats = gen - meanT, tar - meanT\n\n        gen_feats_norm = torch.norm(gen_feats, p=2, dim=1, keepdim=True)\n        tar_feats_norm = torch.norm(tar_feats, p=2, dim=1, keepdim=True)\n\n        gen_normalized = gen_feats / gen_feats_norm\n        tar_normalized = tar_feats / tar_feats_norm\n\n        cosine_dist_l = []\n        BatchSize = tar.size(0)\n\n        for i in range(BatchSize):\n            tar_feat_i = tar_normalized[i:i+1, :, :, :]\n            gen_feat_i = gen_normalized[i:i+1, :, :, :]\n            patches_OIHW = self.patch_extraction(tar_feat_i)\n\n            cosine_dist_i = F.conv2d(gen_feat_i, patches_OIHW)\n            cosine_dist_l.append(cosine_dist_i)\n        cosine_dist = torch.cat(cosine_dist_l, dim=0)\n        cosine_dist_zero_2_one = - (cosine_dist - 1) / 2\n        relative_dist = self.compute_relative_distances(cosine_dist_zero_2_one)\n        rela_dist = self.exp_norm_relative_dist(relative_dist)\n        dims_div_mrf = rela_dist.size()\n        k_max_nc = torch.max(rela_dist.view(dims_div_mrf[0], dims_div_mrf[1], -1), dim=2)[0]\n        div_mrf = torch.mean(k_max_nc, dim=1)\n        div_mrf_sum = -torch.log(div_mrf)\n        div_mrf_sum = torch.sum(div_mrf_sum)\n        return div_mrf_sum\n\n    def forward(self, gen, tar):\n        gen_vgg_feats = self.featlayer(gen)\n        tar_vgg_feats = self.featlayer(tar)\n\n        style_loss_list = [self.feat_style_layers[layer] * self.mrf_loss(gen_vgg_feats[layer], tar_vgg_feats[layer]) for layer in self.feat_style_layers]\n        self.style_loss = reduce(lambda x, y: x+y, style_loss_list) * self.lambda_style\n\n        content_loss_list = [self.feat_content_layers[layer] * self.mrf_loss(gen_vgg_feats[layer], tar_vgg_feats[layer]) for layer in self.feat_content_layers]\n        self.content_loss = reduce(lambda x, y: x+y, content_loss_list) * self.lambda_content\n\n        return self.style_loss + self.content_loss\n\n\nclass StyleLoss(nn.Module):\n    def __init__(self, featlayer=VGG19FeatLayer, style_layers=None):\n        super(StyleLoss, self).__init__()\n        self.featlayer = featlayer()\n        if style_layers is not None:\n            self.feat_style_layers = style_layers\n        else:\n            self.feat_style_layers = {'relu2_2': 1.0, 'relu3_2': 1.0, 'relu4_2': 1.0}\n\n    def gram_matrix(self, x):\n        b, c, h, w = x.size()\n        feats = x.view(b * c, h * w)\n        g = torch.mm(feats, feats.t())\n        return g.div(b * c * h * w)\n\n    def _l1loss(self, gen, tar):\n        return torch.abs(gen-tar).mean()\n\n    def forward(self, gen, tar):\n        gen_vgg_feats = self.featlayer(gen)\n        tar_vgg_feats = self.featlayer(tar)\n        style_loss_list = [self.feat_style_layers[layer] * self._l1loss(self.gram_matrix(gen_vgg_feats[layer]), self.gram_matrix(tar_vgg_feats[layer])) for\n                           layer in self.feat_style_layers]\n        style_loss = reduce(lambda x, y: x + y, style_loss_list)\n        return style_loss\n\n\nclass ContentLoss(nn.Module):\n    def __init__(self, featlayer=VGG19FeatLayer, content_layers=None):\n        super(ContentLoss, self).__init__()\n        self.featlayer = featlayer()\n        if content_layers is not None:\n            self.feat_content_layers = content_layers\n        else:\n            self.feat_content_layers = {'relu4_2': 1.0}\n\n    def _l1loss(self, gen, tar):\n        return torch.abs(gen-tar).mean()\n\n    def forward(self, gen, tar):\n        gen_vgg_feats = self.featlayer(gen)\n        tar_vgg_feats = self.featlayer(tar)\n        content_loss_list = [self.feat_content_layers[layer] * self._l1loss(gen_vgg_feats[layer], tar_vgg_feats[layer]) for\n                             layer in self.feat_content_layers]\n        content_loss = reduce(lambda x, y: x + y, content_loss_list)\n        return content_loss\n\n\nclass TVLoss(nn.Module):\n    def __init__(self):\n        super(TVLoss, self).__init__()\n\n    def forward(self, x):\n        h_x, w_x = x.size()[2:]\n        h_tv = torch.abs(x[:, :, 1:, :] - x[:, :, :h_x-1, :])\n        w_tv = torch.abs(x[:, :, :, 1:] - x[:, :, :, :w_x-1])\n        loss = torch.sum(h_tv) + torch.sum(w_tv)\n        return loss\n"""
pytorch/model/net.py,13,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model.basemodel import BaseModel\nfrom model.basenet import BaseNet\nfrom model.loss import WGANLoss, IDMRFLoss\nfrom model.layer import init_weights, PureUpsampling, ConfidenceDrivenMaskLayer, SpectralNorm\nimport numpy as np\n\n# generative multi-column convolutional neural net\nclass GMCNN(BaseNet):\n    def __init__(self, in_channels, out_channels, cnum=32, act=F.elu, norm=F.instance_norm, using_norm=False):\n        super(GMCNN, self).__init__()\n        self.act = act\n        self.using_norm = using_norm\n        if using_norm is True:\n            self.norm = norm\n        else:\n            self.norm = None\n        ch = cnum\n\n        # network structure\n        self.EB1 = []\n        self.EB2 = []\n        self.EB3 = []\n        self.decoding_layers = []\n\n        self.EB1_pad_rec = []\n        self.EB2_pad_rec = []\n        self.EB3_pad_rec = []\n\n        self.EB1.append(nn.Conv2d(in_channels, ch, kernel_size=7, stride=1))\n\n        self.EB1.append(nn.Conv2d(ch, ch * 2, kernel_size=7, stride=2))\n        self.EB1.append(nn.Conv2d(ch * 2, ch * 2, kernel_size=7, stride=1))\n\n        self.EB1.append(nn.Conv2d(ch * 2, ch * 4, kernel_size=7, stride=2))\n        self.EB1.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=7, stride=1))\n        self.EB1.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=7, stride=1))\n\n        self.EB1.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=7, stride=1, dilation=2))\n        self.EB1.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=7, stride=1, dilation=4))\n        self.EB1.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=7, stride=1, dilation=8))\n        self.EB1.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=7, stride=1, dilation=16))\n\n        self.EB1.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=7, stride=1))\n        self.EB1.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=7, stride=1))\n\n        self.EB1.append(PureUpsampling(scale=4))\n\n        self.EB1_pad_rec = [3, 3, 3, 3, 3, 3, 6, 12, 24, 48, 3, 3, 0]\n\n        self.EB2.append(nn.Conv2d(in_channels, ch, kernel_size=5, stride=1))\n\n        self.EB2.append(nn.Conv2d(ch, ch * 2, kernel_size=5, stride=2))\n        self.EB2.append(nn.Conv2d(ch * 2, ch * 2, kernel_size=5, stride=1))\n\n        self.EB2.append(nn.Conv2d(ch * 2, ch * 4, kernel_size=5, stride=2))\n        self.EB2.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=5, stride=1))\n        self.EB2.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=5, stride=1))\n\n        self.EB2.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=5, stride=1, dilation=2))\n        self.EB2.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=5, stride=1, dilation=4))\n        self.EB2.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=5, stride=1, dilation=8))\n        self.EB2.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=5, stride=1, dilation=16))\n\n        self.EB2.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=5, stride=1))\n        self.EB2.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=5, stride=1))\n\n        self.EB2.append(PureUpsampling(scale=2, mode='nearest'))\n        self.EB2.append(nn.Conv2d(ch * 4, ch * 2, kernel_size=5, stride=1))\n        self.EB2.append(nn.Conv2d(ch * 2, ch * 2, kernel_size=5, stride=1))\n        self.EB2.append(PureUpsampling(scale=2))\n        self.EB2_pad_rec = [2, 2, 2, 2, 2, 2, 4, 8, 16, 32, 2, 2, 0, 2, 2, 0]\n\n        self.EB3.append(nn.Conv2d(in_channels, ch, kernel_size=3, stride=1))\n\n        self.EB3.append(nn.Conv2d(ch, ch * 2, kernel_size=3, stride=2))\n        self.EB3.append(nn.Conv2d(ch * 2, ch * 2, kernel_size=3, stride=1))\n\n        self.EB3.append(nn.Conv2d(ch * 2, ch * 4, kernel_size=3, stride=2))\n        self.EB3.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=3, stride=1))\n        self.EB3.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=3, stride=1))\n\n        self.EB3.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=3, stride=1, dilation=2))\n        self.EB3.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=3, stride=1, dilation=4))\n        self.EB3.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=3, stride=1, dilation=8))\n        self.EB3.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=3, stride=1, dilation=16))\n\n        self.EB3.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=3, stride=1))\n        self.EB3.append(nn.Conv2d(ch * 4, ch * 4, kernel_size=3, stride=1))\n\n        self.EB3.append(PureUpsampling(scale=2, mode='nearest'))\n        self.EB3.append(nn.Conv2d(ch * 4, ch * 2, kernel_size=3, stride=1))\n        self.EB3.append(nn.Conv2d(ch * 2, ch * 2, kernel_size=3, stride=1))\n        self.EB3.append(PureUpsampling(scale=2, mode='nearest'))\n        self.EB3.append(nn.Conv2d(ch * 2, ch, kernel_size=3, stride=1))\n        self.EB3.append(nn.Conv2d(ch, ch, kernel_size=3, stride=1))\n\n        self.EB3_pad_rec = [1, 1, 1, 1, 1, 1, 2, 4, 8, 16, 1, 1, 0, 1, 1, 0, 1, 1]\n\n        self.decoding_layers.append(nn.Conv2d(ch * 7, ch // 2, kernel_size=3, stride=1))\n        self.decoding_layers.append(nn.Conv2d(ch // 2, out_channels, kernel_size=3, stride=1))\n\n        self.decoding_pad_rec = [1, 1]\n\n        self.EB1 = nn.ModuleList(self.EB1)\n        self.EB2 = nn.ModuleList(self.EB2)\n        self.EB3 = nn.ModuleList(self.EB3)\n        self.decoding_layers = nn.ModuleList(self.decoding_layers)\n\n        # padding operations\n        padlen = 49\n        self.pads = [0] * padlen\n        for i in range(padlen):\n            self.pads[i] = nn.ReflectionPad2d(i)\n        self.pads = nn.ModuleList(self.pads)\n\n    def forward(self, x):\n        x1, x2, x3 = x, x, x\n        for i, layer in enumerate(self.EB1):\n            pad_idx = self.EB1_pad_rec[i]\n            x1 = layer(self.pads[pad_idx](x1))\n            if self.using_norm:\n                x1 = self.norm(x1)\n            if pad_idx != 0:\n                x1 = self.act(x1)\n\n        for i, layer in enumerate(self.EB2):\n            pad_idx = self.EB2_pad_rec[i]\n            x2 = layer(self.pads[pad_idx](x2))\n            if self.using_norm:\n                x2 = self.norm(x2)\n            if pad_idx != 0:\n                x2 = self.act(x2)\n\n        for i, layer in enumerate(self.EB3):\n            pad_idx = self.EB3_pad_rec[i]\n            x3 = layer(self.pads[pad_idx](x3))\n            if self.using_norm:\n                x3 = self.norm(x3)\n            if pad_idx != 0:\n                x3 = self.act(x3)\n\n        x_d = torch.cat((x1, x2, x3), 1)\n        x_d = self.act(self.decoding_layers[0](self.pads[self.decoding_pad_rec[0]](x_d)))\n        x_d = self.decoding_layers[1](self.pads[self.decoding_pad_rec[1]](x_d))\n        x_out = torch.clamp(x_d, -1, 1)\n        return x_out\n\n\n# return one dimensional output indicating the probability of realness or fakeness\nclass Discriminator(BaseNet):\n    def __init__(self, in_channels, cnum=32, fc_channels=8*8*32*4, act=F.elu, norm=None, spectral_norm=True):\n        super(Discriminator, self).__init__()\n        self.act = act\n        self.norm = norm\n        self.embedding = None\n        self.logit = None\n\n        ch = cnum\n        self.layers = []\n        if spectral_norm:\n            self.layers.append(SpectralNorm(nn.Conv2d(in_channels, ch, kernel_size=5, padding=2, stride=2)))\n            self.layers.append(SpectralNorm(nn.Conv2d(ch, ch * 2, kernel_size=5, padding=2, stride=2)))\n            self.layers.append(SpectralNorm(nn.Conv2d(ch * 2, ch * 4, kernel_size=5, padding=2, stride=2)))\n            self.layers.append(SpectralNorm(nn.Conv2d(ch * 4, ch * 4, kernel_size=5, padding=2, stride=2)))\n            self.layers.append(SpectralNorm(nn.Linear(fc_channels, 1)))\n        else:\n            self.layers.append(nn.Conv2d(in_channels, ch, kernel_size=5, padding=2, stride=2))\n            self.layers.append(nn.Conv2d(ch, ch * 2, kernel_size=5, padding=2, stride=2))\n            self.layers.append(nn.Conv2d(ch*2, ch*4, kernel_size=5, padding=2, stride=2))\n            self.layers.append(nn.Conv2d(ch*4, ch*4, kernel_size=5, padding=2, stride=2))\n            self.layers.append(nn.Linear(fc_channels, 1))\n        self.layers = nn.ModuleList(self.layers)\n\n    def forward(self, x):\n        for layer in self.layers[:-1]:\n            x = layer(x)\n            if self.norm is not None:\n                x = self.norm(x)\n            x = self.act(x)\n        self.embedding = x.view(x.size(0), -1)\n        self.logit = self.layers[-1](self.embedding)\n        return self.logit\n\n\nclass GlobalLocalDiscriminator(BaseNet):\n    def __init__(self, in_channels, cnum=32, g_fc_channels=16*16*32*4, l_fc_channels=8*8*32*4, act=F.elu, norm=None,\n                 spectral_norm=True):\n        super(GlobalLocalDiscriminator, self).__init__()\n        self.act = act\n        self.norm = norm\n\n        self.global_discriminator = Discriminator(in_channels=in_channels, fc_channels=g_fc_channels, cnum=cnum,\n                                                  act=act, norm=norm, spectral_norm=spectral_norm)\n        self.local_discriminator = Discriminator(in_channels=in_channels, fc_channels=l_fc_channels, cnum=cnum,\n                                                 act=act, norm=norm, spectral_norm=spectral_norm)\n\n    def forward(self, x_g, x_l):\n        x_global = self.global_discriminator(x_g)\n        x_local = self.local_discriminator(x_l)\n        return x_global, x_local\n\n\nfrom util.utils import generate_mask\n\n\nclass InpaintingModel_GMCNN(BaseModel):\n    def __init__(self, in_channels, act=F.elu, norm=None, opt=None):\n        super(InpaintingModel_GMCNN, self).__init__()\n        self.opt = opt\n        self.init(opt)\n\n        self.confidence_mask_layer = ConfidenceDrivenMaskLayer()\n\n        self.netGM = GMCNN(in_channels, out_channels=3, cnum=opt.g_cnum, act=act, norm=norm).cuda()\n        init_weights(self.netGM)\n        self.model_names = ['GM']\n        if self.opt.phase == 'test':\n            return\n\n        self.netD = None\n\n        self.optimizer_G = torch.optim.Adam(self.netGM.parameters(), lr=opt.lr, betas=(0.5, 0.9))\n        self.optimizer_D = None\n\n        self.wganloss = None\n        self.recloss = nn.L1Loss()\n        self.aeloss = nn.L1Loss()\n        self.mrfloss = None\n        self.lambda_adv = opt.lambda_adv\n        self.lambda_rec = opt.lambda_rec\n        self.lambda_ae = opt.lambda_ae\n        self.lambda_gp = opt.lambda_gp\n        self.lambda_mrf = opt.lambda_mrf\n        self.G_loss = None\n        self.G_loss_reconstruction = None\n        self.G_loss_mrf = None\n        self.G_loss_adv, self.G_loss_adv_local = None, None\n        self.G_loss_ae = None\n        self.D_loss, self.D_loss_local = None, None\n        self.GAN_loss = None\n\n        self.gt, self.gt_local = None, None\n        self.mask, self.mask_01 = None, None\n        self.rect = None\n        self.im_in, self.gin = None, None\n\n        self.completed, self.completed_local = None, None\n        self.completed_logit, self.completed_local_logit = None, None\n        self.gt_logit, self.gt_local_logit = None, None\n\n        self.pred = None\n\n        if self.opt.pretrain_network is False:\n            if self.opt.mask_type == 'rect':\n                self.netD = GlobalLocalDiscriminator(3, cnum=opt.d_cnum, act=act,\n                                                     g_fc_channels=opt.img_shapes[0]//16*opt.img_shapes[1]//16*opt.d_cnum*4,\n                                                     l_fc_channels=opt.mask_shapes[0]//16*opt.mask_shapes[1]//16*opt.d_cnum*4,\n                                                     spectral_norm=self.opt.spectral_norm).cuda()\n            else:\n                self.netD = GlobalLocalDiscriminator(3, cnum=opt.d_cnum, act=act,\n                                                     spectral_norm=self.opt.spectral_norm,\n                                                     g_fc_channels=opt.img_shapes[0]//16*opt.img_shapes[1]//16*opt.d_cnum*4,\n                                                     l_fc_channels=opt.img_shapes[0]//16*opt.img_shapes[1]//16*opt.d_cnum*4).cuda()\n            init_weights(self.netD)\n            self.optimizer_D = torch.optim.Adam(filter(lambda x: x.requires_grad, self.netD.parameters()), lr=opt.lr,\n                                                betas=(0.5, 0.9))\n            self.wganloss = WGANLoss()\n            self.mrfloss = IDMRFLoss()\n\n    def initVariables(self):\n        self.gt = self.input['gt']\n        mask, rect = generate_mask(self.opt.mask_type, self.opt.img_shapes, self.opt.mask_shapes)\n        self.mask_01 = torch.from_numpy(mask).cuda().repeat([self.opt.batch_size, 1, 1, 1])\n        self.mask = self.confidence_mask_layer(self.mask_01)\n        if self.opt.mask_type == 'rect':\n            self.rect = [rect[0, 0], rect[0, 1], rect[0, 2], rect[0, 3]]\n            self.gt_local = self.gt[:, :, self.rect[0]:self.rect[0] + self.rect[1],\n                            self.rect[2]:self.rect[2] + self.rect[3]]\n        else:\n            self.gt_local = self.gt\n        self.im_in = self.gt * (1 - self.mask_01)\n        self.gin = torch.cat((self.im_in, self.mask_01), 1)\n\n    def forward_G(self):\n        self.G_loss_reconstruction = self.recloss(self.completed * self.mask, self.gt.detach() * self.mask)\n        self.G_loss_reconstruction = self.G_loss_reconstruction / torch.mean(self.mask_01)\n        self.G_loss_ae = self.aeloss(self.pred * (1 - self.mask_01), self.gt.detach() * (1 - self.mask_01))\n        self.G_loss_ae = self.G_loss_ae / torch.mean(1 - self.mask_01)\n        self.G_loss = self.lambda_rec * self.G_loss_reconstruction + self.lambda_ae * self.G_loss_ae\n        if self.opt.pretrain_network is False:\n            # discriminator\n            self.completed_logit, self.completed_local_logit = self.netD(self.completed, self.completed_local)\n            self.G_loss_mrf = self.mrfloss((self.completed_local+1)/2.0, (self.gt_local.detach()+1)/2.0)\n            self.G_loss = self.G_loss + self.lambda_mrf * self.G_loss_mrf\n\n            self.G_loss_adv = -self.completed_logit.mean()\n            self.G_loss_adv_local = -self.completed_local_logit.mean()\n            self.G_loss = self.G_loss + self.lambda_adv * (self.G_loss_adv + self.G_loss_adv_local)\n\n    def forward_D(self):\n        self.completed_logit, self.completed_local_logit = self.netD(self.completed.detach(), self.completed_local.detach())\n        self.gt_logit, self.gt_local_logit = self.netD(self.gt, self.gt_local)\n        # hinge loss\n        self.D_loss_local = nn.ReLU()(1.0 - self.gt_local_logit).mean() + nn.ReLU()(1.0 + self.completed_local_logit).mean()\n        self.D_loss = nn.ReLU()(1.0 - self.gt_logit).mean() + nn.ReLU()(1.0 + self.completed_logit).mean()\n        self.D_loss = self.D_loss + self.D_loss_local\n\n    def backward_G(self):\n        self.G_loss.backward()\n\n    def backward_D(self):\n        self.D_loss.backward(retain_graph=True)\n\n    def optimize_parameters(self):\n        self.initVariables()\n\n        self.pred = self.netGM(self.gin)\n        self.completed = self.pred * self.mask_01 + self.gt * (1 - self.mask_01)\n        if self.opt.mask_type == 'rect':\n            self.completed_local = self.completed[:, :, self.rect[0]:self.rect[0] + self.rect[1],\n                                   self.rect[2]:self.rect[2] + self.rect[3]]\n        else:\n            self.completed_local = self.completed\n\n        if self.opt.pretrain_network is False:\n            for i in range(self.opt.D_max_iters):\n                self.optimizer_D.zero_grad()\n                self.optimizer_G.zero_grad()\n                self.forward_D()\n                self.backward_D()\n                self.optimizer_D.step()\n\n        self.optimizer_G.zero_grad()\n        self.forward_G()\n        self.backward_G()\n        self.optimizer_G.step()\n\n    def get_current_losses(self):\n        l = {'G_loss': self.G_loss.item(), 'G_loss_rec': self.G_loss_reconstruction.item(),\n             'G_loss_ae': self.G_loss_ae.item()}\n        if self.opt.pretrain_network is False:\n            l.update({'G_loss_adv': self.G_loss_adv.item(),\n                      'G_loss_adv_local': self.G_loss_adv_local.item(),\n                      'D_loss': self.D_loss.item(),\n                      'G_loss_mrf': self.G_loss_mrf.item()})\n        return l\n\n    def get_current_visuals(self):\n        return {'input': self.im_in.cpu().detach().numpy(), 'gt': self.gt.cpu().detach().numpy(),\n                'completed': self.completed.cpu().detach().numpy()}\n\n    def get_current_visuals_tensor(self):\n        return {'input': self.im_in.cpu().detach(), 'gt': self.gt.cpu().detach(),\n                'completed': self.completed.cpu().detach()}\n\n    def evaluate(self, im_in, mask):\n        im_in = torch.from_numpy(im_in).type(torch.FloatTensor).cuda() / 127.5 - 1\n        mask = torch.from_numpy(mask).type(torch.FloatTensor).cuda()\n        im_in = im_in * (1-mask)\n        xin = torch.cat((im_in, mask), 1)\n        ret = self.netGM(xin) * mask + im_in * (1-mask)\n        ret = (ret.cpu().detach().numpy() + 1) * 127.5\n        return ret.astype(np.uint8)\n"""
pytorch/options/__init__.py,0,b''
pytorch/options/test_options.py,0,"b""import argparse\nimport os\nimport time\n\nclass TestOptions:\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.initialized = False\n\n    def initialize(self):\n        self.parser.add_argument('--dataset', type=str, default='paris_streetview',\n                                 help='The dataset of the experiment.')\n        self.parser.add_argument('--data_file', type=str, default='./imgs/paris-streetview_256x256', help='the file storing testing file paths')\n        self.parser.add_argument('--test_dir', type=str, default='./test_results', help='models are saved here')\n        self.parser.add_argument('--load_model_dir', type=str, default='./checkpoints', help='pretrained models are given here')\n        self.parser.add_argument('--seed', type=int, default=1, help='random seed')\n        self.parser.add_argument('--gpu_ids', type=str, default='0')\n\n        self.parser.add_argument('--model', type=str, default='gmcnn')\n        self.parser.add_argument('--random_mask', type=int, default=0,\n                                 help='using random mask')\n\n        self.parser.add_argument('--img_shapes', type=str, default='256,256,3',\n                                 help='given shape parameters: h,w,c or h,w')\n        self.parser.add_argument('--mask_shapes', type=str, default='128,128',\n                                 help='given mask parameters: h,w')\n        self.parser.add_argument('--mask_type', type=str, default='rect')\n        self.parser.add_argument('--test_num', type=int, default=-1)\n        self.parser.add_argument('--mode', type=str, default='save')\n        self.parser.add_argument('--phase', type=str, default='test')\n\n        # for generator\n        self.parser.add_argument('--g_cnum', type=int, default=32,\n                                 help='# of generator filters in first conv layer')\n        self.parser.add_argument('--d_cnum', type=int, default=32,\n                                 help='# of discriminator filters in first conv layer')\n\n    def parse(self):\n        if not self.initialized:\n            self.initialize()\n        self.opt = self.parser.parse_args()\n\n        if self.opt.data_file != '':\n            self.opt.dataset_path = self.opt.data_file\n\n        if os.path.exists(self.opt.test_dir) is False:\n            os.mkdir(self.opt.test_dir)\n\n        assert self.opt.random_mask in [0, 1]\n        self.opt.random_mask = True if self.opt.random_mask == 1 else False\n\n        assert self.opt.mask_type in ['rect', 'stroke']\n\n        str_img_shapes = self.opt.img_shapes.split(',')\n        self.opt.img_shapes = [int(x) for x in str_img_shapes]\n\n        str_mask_shapes = self.opt.mask_shapes.split(',')\n        self.opt.mask_shapes = [int(x) for x in str_mask_shapes]\n\n        # model name and date\n        self.opt.date_str = 'test_'+time.strftime('%Y%m%d-%H%M%S')\n        self.opt.model_folder = self.opt.date_str + '_' + self.opt.dataset + '_' + self.opt.model\n        self.opt.model_folder += '_s' + str(self.opt.img_shapes[0]) + 'x' + str(self.opt.img_shapes[1])\n        self.opt.model_folder += '_gc' + str(self.opt.g_cnum)\n        self.opt.model_folder += '_randmask-' + self.opt.mask_type if self.opt.random_mask else ''\n        if self.opt.random_mask:\n            self.opt.model_folder += '_seed-' + str(self.opt.seed)\n        self.opt.saving_path = os.path.join(self.opt.test_dir, self.opt.model_folder)\n\n        if os.path.exists(self.opt.saving_path) is False and self.opt.mode == 'save':\n            os.mkdir(self.opt.saving_path)\n\n        args = vars(self.opt)\n\n        print('------------ Options -------------')\n        for k, v in sorted(args.items()):\n            print('%s: %s' % (str(k), str(v)))\n        print('-------------- End ----------------')\n\n        return self.opt\n"""
pytorch/options/train_options.py,0,"b""import argparse\nimport os\nimport time\n\nclass TrainOptions:\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.initialized = False\n\n    def initialize(self):\n        # experiment specifics\n        self.parser.add_argument('--dataset', type=str, default='paris_streetview',\n                                 help='dataset of the experiment.')\n        self.parser.add_argument('--data_file', type=str, default='', help='the file storing training image paths')\n        self.parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2')\n        self.parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints', help='models are saved here')\n        self.parser.add_argument('--load_model_dir', type=str, default='', help='pretrained models are given here')\n        self.parser.add_argument('--phase', type=str, default='train')\n\n        # input/output sizes\n        self.parser.add_argument('--batch_size', type=int, default=16, help='input batch size')\n\n        # for setting inputs\n        self.parser.add_argument('--random_crop', type=int, default=1,\n                                 help='using random crop to process input image when '\n                                      'the required size is smaller than the given size')\n        self.parser.add_argument('--random_mask', type=int, default=1)\n        self.parser.add_argument('--mask_type', type=str, default='rect')\n        self.parser.add_argument('--pretrain_network', type=int, default=0)\n        self.parser.add_argument('--lambda_adv', type=float, default=1e-3)\n        self.parser.add_argument('--lambda_rec', type=float, default=1.4)\n        self.parser.add_argument('--lambda_ae', type=float, default=1.2)\n        self.parser.add_argument('--lambda_mrf', type=float, default=0.05)\n        self.parser.add_argument('--lambda_gp', type=float, default=10)\n        self.parser.add_argument('--random_seed', type=bool, default=False)\n        self.parser.add_argument('--padding', type=str, default='SAME')\n        self.parser.add_argument('--D_max_iters', type=int, default=5)\n        self.parser.add_argument('--lr', type=float, default=1e-5, help='learning rate for training')\n\n        self.parser.add_argument('--train_spe', type=int, default=1000)\n        self.parser.add_argument('--epochs', type=int, default=40)\n        self.parser.add_argument('--viz_steps', type=int, default=5)\n        self.parser.add_argument('--spectral_norm', type=int, default=1)\n\n        self.parser.add_argument('--img_shapes', type=str, default='256,256,3',\n                                 help='given shape parameters: h,w,c or h,w')\n        self.parser.add_argument('--mask_shapes', type=str, default='128,128',\n                                 help='given mask parameters: h,w')\n        self.parser.add_argument('--max_delta_shapes', type=str, default='32,32')\n        self.parser.add_argument('--margins', type=str, default='0,0')\n\n\n        # for generator\n        self.parser.add_argument('--g_cnum', type=int, default=32,\n                                 help='# of generator filters in first conv layer')\n        self.parser.add_argument('--d_cnum', type=int, default=64,\n                                 help='# of discriminator filters in first conv layer')\n\n        # for id-mrf computation\n        self.parser.add_argument('--vgg19_path', type=str, default='vgg19_weights/imagenet-vgg-verydeep-19.mat')\n        # for instance-wise features\n        self.initialized = True\n\n    def parse(self):\n        if not self.initialized:\n            self.initialize()\n        self.opt = self.parser.parse_args()\n\n        self.opt.dataset_path = self.opt.data_file\n\n        str_ids = self.opt.gpu_ids.split(',')\n        self.opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                self.opt.gpu_ids.append(str(id))\n\n        assert self.opt.random_crop in [0, 1]\n        self.opt.random_crop = True if self.opt.random_crop == 1 else False\n\n        assert self.opt.random_mask in [0, 1]\n        self.opt.random_mask = True if self.opt.random_mask == 1 else False\n\n        assert self.opt.pretrain_network in [0, 1]\n        self.opt.pretrain_network = True if self.opt.pretrain_network == 1 else False\n\n        assert self.opt.spectral_norm in [0, 1]\n        self.opt.spectral_norm = True if self.opt.spectral_norm == 1 else False\n\n        assert self.opt.padding in ['SAME', 'MIRROR']\n\n        assert self.opt.mask_type in ['rect', 'stroke']\n\n        str_img_shapes = self.opt.img_shapes.split(',')\n        self.opt.img_shapes = [int(x) for x in str_img_shapes]\n\n        str_mask_shapes = self.opt.mask_shapes.split(',')\n        self.opt.mask_shapes = [int(x) for x in str_mask_shapes]\n\n        str_max_delta_shapes = self.opt.max_delta_shapes.split(',')\n        self.opt.max_delta_shapes = [int(x) for x in str_max_delta_shapes]\n\n        str_margins = self.opt.margins.split(',')\n        self.opt.margins = [int(x) for x in str_margins]\n\n        # model name and date\n        self.opt.date_str = time.strftime('%Y%m%d-%H%M%S')\n        self.opt.model_name = 'GMCNN'\n        self.opt.model_folder = self.opt.date_str + '_' + self.opt.model_name\n        self.opt.model_folder += '_' + self.opt.dataset\n        self.opt.model_folder += '_b' + str(self.opt.batch_size)\n        self.opt.model_folder += '_s' + str(self.opt.img_shapes[0]) + 'x' + str(self.opt.img_shapes[1])\n        self.opt.model_folder += '_gc' + str(self.opt.g_cnum)\n        self.opt.model_folder += '_dc' + str(self.opt.d_cnum)\n\n        self.opt.model_folder += '_randmask-' + self.opt.mask_type if self.opt.random_mask else ''\n        self.opt.model_folder += '_pretrain' if self.opt.pretrain_network else ''\n\n        if os.path.isdir(self.opt.checkpoint_dir) is False:\n            os.mkdir(self.opt.checkpoint_dir)\n\n        self.opt.model_folder = os.path.join(self.opt.checkpoint_dir, self.opt.model_folder)\n        if os.path.isdir(self.opt.model_folder) is False:\n            os.mkdir(self.opt.model_folder)\n\n        # set gpu ids\n        if len(self.opt.gpu_ids) > 0:\n            os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(self.opt.gpu_ids)\n\n        args = vars(self.opt)\n\n        print('------------ Options -------------')\n        for k, v in sorted(args.items()):\n            print('%s: %s' % (str(k), str(v)))\n        print('-------------- End ----------------')\n\n        return self.opt\n"""
pytorch/util/__init__.py,0,b''
pytorch/util/utils.py,0,"b""import numpy as np\nimport scipy.stats as st\nimport cv2\nimport time\nimport os\nimport glob\n\ndef gauss_kernel(size=21, sigma=3, inchannels=3, outchannels=3):\n    interval = (2 * sigma + 1.0) / size\n    x = np.linspace(-sigma-interval/2,sigma+interval/2,size+1)\n    ker1d = np.diff(st.norm.cdf(x))\n    kernel_raw = np.sqrt(np.outer(ker1d, ker1d))\n    kernel = kernel_raw / kernel_raw.sum()\n    out_filter = np.array(kernel, dtype=np.float32)\n    out_filter = out_filter.reshape((1, 1, size, size))\n    out_filter = np.tile(out_filter, [outchannels, inchannels, 1, 1])\n    return out_filter\n\n\ndef np_free_form_mask(maxVertex, maxLength, maxBrushWidth, maxAngle, h, w):\n    mask = np.zeros((h, w, 1), np.float32)\n    numVertex = np.random.randint(maxVertex + 1)\n    startY = np.random.randint(h)\n    startX = np.random.randint(w)\n    brushWidth = 0\n    for i in range(numVertex):\n        angle = np.random.randint(maxAngle + 1)\n        angle = angle / 360.0 * 2 * np.pi\n        if i % 2 == 0:\n            angle = 2 * np.pi - angle\n        length = np.random.randint(maxLength + 1)\n        brushWidth = np.random.randint(10, maxBrushWidth + 1) // 2 * 2\n        nextY = startY + length * np.cos(angle)\n        nextX = startX + length * np.sin(angle)\n\n        nextY = np.maximum(np.minimum(nextY, h - 1), 0).astype(np.int)\n        nextX = np.maximum(np.minimum(nextX, w - 1), 0).astype(np.int)\n\n        cv2.line(mask, (startY, startX), (nextY, nextX), 1, brushWidth)\n        cv2.circle(mask, (startY, startX), brushWidth // 2, 2)\n\n        startY, startX = nextY, nextX\n    cv2.circle(mask, (startY, startX), brushWidth // 2, 2)\n    return mask\n\n\ndef generate_rect_mask(im_size, mask_size, margin=8, rand_mask=True):\n    mask = np.zeros((im_size[0], im_size[1])).astype(np.float32)\n    if rand_mask:\n        sz0, sz1 = mask_size[0], mask_size[1]\n        of0 = np.random.randint(margin, im_size[0] - sz0 - margin)\n        of1 = np.random.randint(margin, im_size[1] - sz1 - margin)\n    else:\n        sz0, sz1 = mask_size[0], mask_size[1]\n        of0 = (im_size[0] - sz0) // 2\n        of1 = (im_size[1] - sz1) // 2\n    mask[of0:of0+sz0, of1:of1+sz1] = 1\n    mask = np.expand_dims(mask, axis=0)\n    mask = np.expand_dims(mask, axis=0)\n    rect = np.array([[of0, sz0, of1, sz1]], dtype=int)\n    return mask, rect\n\n\ndef generate_stroke_mask(im_size, parts=10, maxVertex=20, maxLength=100, maxBrushWidth=24, maxAngle=360):\n    mask = np.zeros((im_size[0], im_size[1], 1), dtype=np.float32)\n    for i in range(parts):\n        mask = mask + np_free_form_mask(maxVertex, maxLength, maxBrushWidth, maxAngle, im_size[0], im_size[1])\n    mask = np.minimum(mask, 1.0)\n    mask = np.transpose(mask, [2, 0, 1])\n    mask = np.expand_dims(mask, 0)\n    return mask\n\n\ndef generate_mask(type, im_size, mask_size):\n    if type == 'rect':\n        return generate_rect_mask(im_size, mask_size)\n    else:\n        return generate_stroke_mask(im_size), None\n\n\ndef getLatest(folder_path):\n    files = glob.glob(folder_path)\n    file_times = list(map(lambda x: time.ctime(os.path.getctime(x)), files))\n    return files[sorted(range(len(file_times)), key=lambda x: file_times[x])[-1]]\n"""
tensorflow/data/__init__.py,0,b''
tensorflow/data/data.py,0,"b""import tensorflow as tf\n\nclass DataLoader:\n    def __init__(self, filename, im_size, batch_size):\n        self.filelist = open(filename, 'rt').read().splitlines()\n        \n        if not self.filelist:\n            exit('\\nError: file list is empty\\n')\n        \n        self.im_size = im_size\n        self.batch_size = batch_size\n        self.data_queue = None\n\n    def next(self):\n        with tf.variable_scope('feed'):\n            filelist_tensor = tf.convert_to_tensor(self.filelist, dtype=tf.string)\n            self.data_queue = tf.train.slice_input_producer([filelist_tensor])\n\n            im_gt = tf.image.decode_image(tf.read_file(self.data_queue[0]), channels=3)\n            # im_gt = tf.cast(im_gt, tf.float32) / 127.5 - 1\n            im_gt = tf.cast(im_gt, tf.float32)\n            im_gt = tf.image.resize_image_with_crop_or_pad(im_gt, self.im_size[0], self.im_size[1])\n            im_gt.set_shape([self.im_size[0], self.im_size[1], 3])\n            batch_gt = tf.train.batch([im_gt], batch_size=self.batch_size, num_threads=4)\n        return batch_gt\n"""
tensorflow/net/__init__.py,0,b''
tensorflow/net/network.py,0,"b""import tensorflow as tf\nfrom net.ops import random_bbox, bbox2mask, local_patch\nfrom net.ops import priority_loss_mask\nfrom net.ops import id_mrf_reg\nfrom net.ops import gan_wgan_loss, gradients_penalty, random_interpolates\nfrom net.ops import free_form_mask_tf\nfrom util.util import f2uint\nfrom functools import partial\n\nclass GMCNNModel:\n    def __init__(self):\n        self.config = None\n\n        # shortcut ops\n        self.conv7 = partial(tf.layers.conv2d, kernel_size=7, activation=tf.nn.elu, padding='SAME')\n        self.conv5 = partial(tf.layers.conv2d, kernel_size=5, activation=tf.nn.elu, padding='SAME')\n        self.conv3 = partial(tf.layers.conv2d, kernel_size=3, activation=tf.nn.elu, padding='SAME')\n        self.conv5_ds = partial(tf.layers.conv2d, kernel_size=5, strides=2, activation=tf.nn.leaky_relu, padding='SAME')\n\n    def build_generator(self, x, mask, reuse=False, name='inpaint_net'):\n        xshape = x.get_shape().as_list()\n        xh, xw = xshape[1], xshape[2]\n        ones_x = tf.ones_like(x)[:, :, :, 0:1]\n        x_w_mask = tf.concat([x, ones_x, ones_x * mask], axis=3)\n\n        # network with three branches\n        cnum = self.config.g_cnum\n        b_names = ['b1', 'b2', 'b3', 'merge']\n\n        conv_7 = self.conv7\n        conv_5 = self.conv5\n        conv_3 = self.conv3\n        with tf.variable_scope(name, reuse=reuse):\n            # branch 1\n            x = conv_7(inputs=x_w_mask, filters=cnum, strides=1, name=b_names[0] + 'conv1')\n            x = conv_7(inputs=x, filters=2*cnum, strides=2, name=b_names[0] + 'conv2_downsample')\n            x = conv_7(inputs=x, filters=2*cnum, strides=1, name=b_names[0] + 'conv3')\n            x = conv_7(inputs=x, filters=4*cnum, strides=2, name=b_names[0] + 'conv4_downsample')\n            x = conv_7(inputs=x, filters=4*cnum, strides=1, name=b_names[0] + 'conv5')\n            x = conv_7(inputs=x, filters=4*cnum, strides=1, name=b_names[0] + 'conv6')\n            x = conv_7(inputs=x, filters=4*cnum, strides=1, dilation_rate=2, name=b_names[0] + 'conv7_atrous')\n            x = conv_7(inputs=x, filters=4*cnum, strides=1, dilation_rate=4, name=b_names[0] + 'conv8_atrous')\n            x = conv_7(inputs=x, filters=4*cnum, strides=1, dilation_rate=8, name=b_names[0] + 'conv9_atrous')\n            x = conv_7(inputs=x, filters=4*cnum, strides=1, dilation_rate=16, name=b_names[0] + 'conv10_atrous')\n            if cnum > 32:\n                x = conv_7(inputs=x, filters=4 * cnum, strides=1, dilation_rate=32, name=b_names[0] + 'conv11_atrous')\n            x = conv_7(inputs=x, filters=4*cnum, strides=1, name=b_names[0] + 'conv11')\n            x = conv_7(inputs=x, filters=4*cnum, strides=1, name=b_names[0] + 'conv12')\n            x_b1 = tf.image.resize_bilinear(x, [xh, xw], align_corners=True)\n\n            # branch 2\n            x = conv_5(inputs=x_w_mask, filters=cnum, strides=1, name=b_names[1] + 'conv1')\n            x = conv_5(inputs=x, filters=2 * cnum, strides=2, name=b_names[1] + 'conv2_downsample')\n            x = conv_5(inputs=x, filters=2 * cnum, strides=1, name=b_names[1] + 'conv3')\n            x = conv_5(inputs=x, filters=4 * cnum, strides=2, name=b_names[1] + 'conv4_downsample')\n            x = conv_5(inputs=x, filters=4 * cnum, strides=1, name=b_names[1] + 'conv5')\n            x = conv_5(inputs=x, filters=4 * cnum, strides=1, name=b_names[1] + 'conv6')\n            x = conv_5(inputs=x, filters=4 * cnum, strides=1, dilation_rate=2, name=b_names[1] + 'conv7_atrous')\n            x = conv_5(inputs=x, filters=4 * cnum, strides=1, dilation_rate=4, name=b_names[1] + 'conv8_atrous')\n            x = conv_5(inputs=x, filters=4 * cnum, strides=1, dilation_rate=8, name=b_names[1] + 'conv9_atrous')\n            x = conv_5(inputs=x, filters=4 * cnum, strides=1, dilation_rate=16, name=b_names[1] + 'conv10_atrous')\n            if cnum > 32:\n                x = conv_5(inputs=x, filters=4 * cnum, strides=1, dilation_rate=32, name=b_names[1] + 'conv11_atrous')\n            x = conv_5(inputs=x, filters=4 * cnum, strides=1, name=b_names[1] + 'conv11')\n            x = conv_5(inputs=x, filters=4 * cnum, strides=1, name=b_names[1] + 'conv12')\n            x = tf.image.resize_nearest_neighbor(x, [xh//2, xw//2], align_corners=True)\n            with tf.variable_scope(b_names[1] + 'conv13_upsample'):\n                x = conv_3(inputs=x, filters=2 * cnum, strides=1, name=b_names[1] + 'conv13_upsample_conv')\n            x = conv_5(inputs=x, filters=2 * cnum, strides=1, name=b_names[1] + 'conv14')\n            x_b2 = tf.image.resize_bilinear(x, [xh, xw], align_corners=True)\n\n            # branch 3\n            x = conv_5(inputs=x_w_mask, filters=cnum, strides=1, name=b_names[2] + 'conv1')\n            x = conv_3(inputs=x, filters=2 * cnum, strides=2, name=b_names[2] + 'conv2_downsample')\n            x = conv_3(inputs=x, filters=2 * cnum, strides=1, name=b_names[2] + 'conv3')\n            x = conv_3(inputs=x, filters=4 * cnum, strides=2, name=b_names[2] + 'conv4_downsample')\n            x = conv_3(inputs=x, filters=4 * cnum, strides=1, name=b_names[2] + 'conv5')\n            x = conv_3(inputs=x, filters=4 * cnum, strides=1, name=b_names[2] + 'conv6')\n            x = conv_3(inputs=x, filters=4 * cnum, strides=1, dilation_rate=2, name=b_names[2] + 'conv7_atrous')\n            x = conv_3(inputs=x, filters=4 * cnum, strides=1, dilation_rate=4, name=b_names[2] + 'conv8_atrous')\n            x = conv_3(inputs=x, filters=4 * cnum, strides=1, dilation_rate=8, name=b_names[2] + 'conv9_atrous')\n            x = conv_3(inputs=x, filters=4 * cnum, strides=1, dilation_rate=16, name=b_names[2] + 'conv10_atrous')\n            if cnum > 32:\n                x = conv_3(inputs=x, filters=4 * cnum, strides=1, dilation_rate=32, name=b_names[2] + 'conv11_atrous')\n            x = conv_3(inputs=x, filters=4 * cnum, strides=1, name=b_names[2] + 'conv11')\n            x = conv_3(inputs=x, filters=4 * cnum, strides=1, name=b_names[2] + 'conv12')\n            x = tf.image.resize_nearest_neighbor(x, [xh // 2, xw // 2], align_corners=True)\n            with tf.variable_scope(b_names[2] + 'conv13_upsample'):\n                x = conv_3(inputs=x, filters=2 * cnum, strides=1, name=b_names[2] + 'conv13_upsample_conv')\n            x = conv_3(inputs=x, filters=2 * cnum, strides=1, name=b_names[2] + 'conv14')\n            x = tf.image.resize_nearest_neighbor(x, [xh, xw], align_corners=True)\n            with tf.variable_scope(b_names[2] + 'conv15_upsample'):\n                x = conv_3(inputs=x, filters=cnum, strides=1, name=b_names[2] + 'conv15_upsample_conv')\n            x_b3 = conv_3(inputs=x, filters=cnum//2, strides=1, name=b_names[2] + 'conv16')\n\n            x_merge = tf.concat([x_b1, x_b2, x_b3], axis=3)\n\n            x = conv_3(inputs=x_merge, filters=cnum // 2, strides=1, name=b_names[3] + 'conv17')\n            x = tf.layers.conv2d(inputs=x, kernel_size=3, filters=3, strides=1, activation=None, padding='SAME',\n                                 name=b_names[3] + 'conv18')\n            x = tf.clip_by_value(x, -1., 1.)\n        return x\n\n\n    def wgan_patch_discriminator(self, x, mask, d_cnum, reuse=False):\n        cnum = d_cnum\n        with tf.variable_scope('discriminator_local', reuse=reuse):\n            h, w = mask.get_shape().as_list()[1:3]\n            x = self.conv5_ds(x, filters=cnum, name='conv1')\n            x = self.conv5_ds(x, filters=cnum*2, name='conv2')\n            x = self.conv5_ds(x, filters=cnum*4, name='conv3')\n            x = self.conv5_ds(x, filters=cnum*8, name='conv4')\n            x = tf.layers.conv2d(x, kernel_size=5, strides=2, filters=1, activation=None, name='conv5', padding='SAME')\n\n            mask = tf.contrib.layers.max_pool2d(mask, 2, padding='SAME')\n            mask = tf.contrib.layers.max_pool2d(mask, 2, padding='SAME')\n            mask = tf.contrib.layers.max_pool2d(mask, 2, padding='SAME')\n            mask = tf.contrib.layers.max_pool2d(mask, 2, padding='SAME')\n            mask = tf.contrib.layers.max_pool2d(mask, 2, padding='SAME')\n\n            x = x * mask\n            x = tf.reduce_sum(x, axis=[1, 2, 3]) / tf.reduce_sum(mask, axis=[1, 2, 3])\n            mask_local = tf.image.resize_nearest_neighbor(mask, [h, w], align_corners=True)\n            return x, mask_local\n\n\n    def wgan_local_discriminator(self, x, d_cnum, reuse=False):\n        cnum = d_cnum\n        with tf.variable_scope('disc_local', reuse=reuse):\n            x = self.conv5_ds(x, filters=cnum, name='conv1')\n            x = self.conv5_ds(x, filters=cnum * 2, name='conv2')\n            x = self.conv5_ds(x, filters=cnum * 4, name='conv3')\n            x = self.conv5_ds(x, filters=cnum * 8, name='conv4')\n            x = self.conv5_ds(x, filters=cnum * 4, name='conv5')\n            x = self.conv5_ds(x, filters=cnum * 2, name='conv6')\n\n            x = tf.layers.flatten(x, name='flatten')\n            return x\n\n    def wgan_global_discriminator(self, x, d_cnum, reuse=False):\n        cnum = d_cnum\n        with tf.variable_scope('disc_global', reuse=reuse):\n            x = self.conv5_ds(x, filters=cnum, name='conv1')\n            x = self.conv5_ds(x, filters=cnum * 2, name='conv2')\n            x = self.conv5_ds(x, filters=cnum * 4, name='conv3')\n            x = self.conv5_ds(x, filters=cnum * 8, name='conv4')\n            x = self.conv5_ds(x, filters=cnum * 4, name='conv5')\n            x = self.conv5_ds(x, filters=cnum * 2, name='conv6')\n            x = tf.layers.flatten(x, name='flatten')\n            return x\n\n    def wgan_discriminator(self, batch_local, batch_global, d_cnum, reuse=False):\n        with tf.variable_scope('discriminator', reuse=reuse):\n            dlocal = self.wgan_local_discriminator(batch_local, d_cnum, reuse=reuse)\n            dglobal = self.wgan_global_discriminator(batch_global, d_cnum, reuse=reuse)\n            dout_local = tf.layers.dense(dlocal, 1, name='dout_local_fc')\n            dout_global = tf.layers.dense(dglobal, 1, name='dout_global_fc')\n            return dout_local, dout_global\n\n    def wgan_mask_discriminator(self, batch_global, mask, d_cnum, reuse=False):\n        with tf.variable_scope('discriminator', reuse=reuse):\n            dglobal = self.wgan_global_discriminator(batch_global, d_cnum, reuse=reuse)\n            dout_global = tf.layers.dense(dglobal, 1, name='dout_global_fc')\n            dout_local, mask_local = self.wgan_patch_discriminator(batch_global, mask, d_cnum, reuse=reuse)\n        return dout_local, dout_global, mask_local\n\n    def build_net(self, batch_data, config, summary=True, reuse=False):\n        self.config = config\n        batch_pos = batch_data / 127.5 - 1.\n        # generate mask, 1 represents masked point\n        if config.mask_type == 'rect':\n            bbox = random_bbox(config)\n            mask = bbox2mask(bbox, config, name='mask_c')\n        else:\n            mask = free_form_mask_tf(parts=8, im_size=(config.img_shapes[0], config.img_shapes[1]),\n                                     maxBrushWidth=20, maxLength=80, maxVertex=16)\n        batch_incomplete = batch_pos * (1. - mask)\n        mask_priority = priority_loss_mask(mask)\n        batch_predicted = self.build_generator(batch_incomplete, mask, reuse=reuse)\n\n        losses = {}\n        # apply mask and complete image\n        batch_complete = batch_predicted * mask + batch_incomplete * (1. - mask)\n        if config.mask_type == 'rect':\n            # local patches\n            local_patch_batch_pos = local_patch(batch_pos, bbox)\n            local_patch_batch_complete = local_patch(batch_complete, bbox)\n            local_patch_mask = local_patch(mask, bbox)\n            local_patch_batch_pred = local_patch(batch_predicted, bbox)\n            mask_priority = local_patch(mask_priority, bbox)\n        else:\n            local_patch_batch_pos = batch_pos\n            local_patch_batch_complete = batch_complete\n            local_patch_batch_pred = batch_predicted\n\n        if config.pretrain_network:\n            print('Pretrain the whole net with only reconstruction loss.')\n\n        if not config.pretrain_network:\n            config.feat_style_layers = {'conv3_2': 1.0, 'conv4_2': 1.0}\n            config.feat_content_layers = {'conv4_2': 1.0}\n\n            config.mrf_style_w = 1.0\n            config.mrf_content_w = 1.0\n\n            ID_MRF_loss = id_mrf_reg(local_patch_batch_pred, local_patch_batch_pos, config)\n            # ID_MRF_loss = id_mrf_reg(batch_predicted, batch_pos, config)\n\n            losses['ID_MRF_loss'] = ID_MRF_loss\n            tf.summary.scalar('losses/ID_MRF_loss', losses['ID_MRF_loss'])\n\n        pretrain_l1_alpha = config.pretrain_l1_alpha\n        losses['l1_loss'] = \\\n            pretrain_l1_alpha * tf.reduce_mean(tf.abs(local_patch_batch_pos - local_patch_batch_pred) * mask_priority)\n        if not config.pretrain_network:\n            losses['l1_loss'] += tf.reduce_mean(ID_MRF_loss * config.mrf_alpha)\n        losses['ae_loss'] = pretrain_l1_alpha * tf.reduce_mean(tf.abs(batch_pos - batch_predicted) * (1. - mask))\n        if not config.pretrain_network:\n            losses['ae_loss'] += pretrain_l1_alpha * tf.reduce_mean(tf.abs(batch_pos - batch_predicted) * (1. - mask))\n        losses['ae_loss'] /= tf.reduce_mean(1. - mask)\n\n        if summary:\n            viz_img = tf.concat([batch_pos, batch_incomplete, batch_predicted, batch_complete], axis=2)[:, :, :, ::-1]\n            tf.summary.image('gt__degraded__predicted__completed', f2uint(viz_img))\n            tf.summary.scalar('losses/l1_loss', losses['l1_loss'])\n            tf.summary.scalar('losses/ae_loss', losses['ae_loss'])\n\n        # gan\n        batch_pos_neg = tf.concat([batch_pos, batch_complete], axis=0)\n\n        if config.mask_type == 'rect':\n            # local deterministic patch\n            local_patch_batch_pos_neg = tf.concat([local_patch_batch_pos, local_patch_batch_complete], 0)\n            # wgan with gradient penalty\n            pos_neg_local, pos_neg_global = self.wgan_discriminator(local_patch_batch_pos_neg,\n                                                                    batch_pos_neg, config.d_cnum, reuse=reuse)\n        else:\n            pos_neg_local, pos_neg_global, mask_local = self.wgan_mask_discriminator(batch_pos_neg,\n                                                                                     mask, config.d_cnum, reuse=reuse)\n        pos_local, neg_local = tf.split(pos_neg_local, 2)\n        pos_global, neg_global = tf.split(pos_neg_global, 2)\n        # wgan loss\n        global_wgan_loss_alpha = 1.0\n        g_loss_local, d_loss_local = gan_wgan_loss(pos_local, neg_local, name='gan/local_gan')\n        g_loss_global, d_loss_global = gan_wgan_loss(pos_global, neg_global, name='gan/global_gan')\n        losses['g_loss'] = global_wgan_loss_alpha * g_loss_global + g_loss_local\n        losses['d_loss'] = d_loss_global + d_loss_local\n        # gp\n        interpolates_global = random_interpolates(batch_pos, batch_complete)\n        if config.mask_type == 'rect':\n            interpolates_local = random_interpolates(local_patch_batch_pos, local_patch_batch_complete)\n            dout_local, dout_global = self.wgan_discriminator(\n                interpolates_local, interpolates_global, config.d_cnum, reuse=True)\n        else:\n            interpolates_local = interpolates_global\n            dout_local, dout_global, _ = self.wgan_mask_discriminator(interpolates_global, mask, config.d_cnum, reuse=True)\n\n        # apply penalty\n        if config.mask_type == 'rect':\n            penalty_local = gradients_penalty(interpolates_local, dout_local, mask=local_patch_mask)\n        else:\n            penalty_local = gradients_penalty(interpolates_local, dout_local, mask=mask)\n        penalty_global = gradients_penalty(interpolates_global, dout_global, mask=mask)\n        losses['gp_loss'] = config.wgan_gp_lambda * (penalty_local + penalty_global)\n        losses['d_loss'] = losses['d_loss'] + losses['gp_loss']\n        if summary and not config.pretrain_network:\n            tf.summary.scalar('convergence/d_loss', losses['d_loss'])\n            tf.summary.scalar('convergence/local_d_loss', d_loss_local)\n            tf.summary.scalar('convergence/global_d_loss', d_loss_global)\n            tf.summary.scalar('gan_wgan_loss/gp_loss', losses['gp_loss'])\n            tf.summary.scalar('gan_wgan_loss/gp_penalty_local', penalty_local)\n            tf.summary.scalar('gan_wgan_loss/gp_penalty_global', penalty_global)\n\n        if config.pretrain_network:\n            losses['g_loss'] = 0\n        else:\n            losses['g_loss'] = config.gan_loss_alpha * losses['g_loss']\n        losses['g_loss'] += config.l1_loss_alpha * losses['l1_loss']\n        ##\n\n        print('Set L1_LOSS_ALPHA to %f' % config.l1_loss_alpha)\n        print('Set GAN_LOSS_ALPHA to %f' % config.gan_loss_alpha)\n\n        losses['g_loss'] += config.ae_loss_alpha * losses['ae_loss']\n        print('Set AE_LOSS_ALPHA to %f' % config.ae_loss_alpha)\n        g_vars = tf.get_collection(\n            tf.GraphKeys.TRAINABLE_VARIABLES, 'inpaint_net')\n        d_vars = tf.get_collection(\n            tf.GraphKeys.TRAINABLE_VARIABLES, 'discriminator')\n        return g_vars, d_vars, losses\n\n    def evaluate(self, im, mask, config, reuse=False):\n        # generate mask, 1 represents masked point\n        self.config = config\n        im = im / 127.5 - 1\n        im = im * (1 - mask)\n        # inpaint\n        batch_predict = self.build_generator(im, mask, reuse=reuse)\n        # apply mask and reconstruct\n        batch_complete = batch_predict * mask + im * (1 - mask)\n        return batch_complete\n"""
tensorflow/net/ops.py,0,"b'import cv2\nimport numpy as np\nimport tensorflow as tf\nfrom logging import exception\nimport math\nimport scipy.stats as st\nimport os\nimport urllib\nimport scipy\nfrom scipy import io\n\nnp.random.seed(2018)\n\n""""""\nhttps://arxiv.org/abs/1806.03589\n""""""\ndef np_free_form_mask(maxVertex, maxLength, maxBrushWidth, maxAngle, h, w):\n    mask = np.zeros((h, w, 1), np.float32)\n    numVertex = np.random.randint(maxVertex + 1)\n    startY = np.random.randint(h)\n    startX = np.random.randint(w)\n    brushWidth = 0\n    for i in range(numVertex):\n        angle = np.random.randint(maxAngle + 1)\n        angle = angle / 360.0 * 2 * np.pi\n        if i % 2 == 0:\n            angle = 2 * np.pi - angle\n        length = np.random.randint(maxLength + 1)\n        brushWidth = np.random.randint(10, maxBrushWidth + 1) // 2 * 2\n        nextY = startY + length * np.cos(angle)\n        nextX = startX + length * np.sin(angle)\n\n        nextY = np.maximum(np.minimum(nextY, h - 1), 0).astype(np.int)\n        nextX = np.maximum(np.minimum(nextX, w - 1), 0).astype(np.int)\n\n        cv2.line(mask, (startY, startX), (nextY, nextX), 1, brushWidth)\n        cv2.circle(mask, (startY, startX), brushWidth // 2, 2)\n\n        startY, startX = nextY, nextX\n    cv2.circle(mask, (startY, startX), brushWidth // 2, 2)\n    return mask\n\n\ndef free_form_mask_tf(parts, maxVertex=16, maxLength=60, maxBrushWidth=14, maxAngle=360, im_size=(256, 256), name=\'fmask\'):\n    # mask = np.zeros((im_size[0], im_size[1], 1), dtype=np.float32)\n    with tf.variable_scope(name):\n        mask = tf.Variable(tf.zeros([1, im_size[0], im_size[1], 1]), name=\'free_mask\')\n        maxVertex = tf.constant(maxVertex, dtype=tf.int32)\n        maxLength = tf.constant(maxLength, dtype=tf.int32)\n        maxBrushWidth = tf.constant(maxBrushWidth, dtype=tf.int32)\n        maxAngle = tf.constant(maxAngle, dtype=tf.int32)\n        h = tf.constant(im_size[0], dtype=tf.int32)\n        w = tf.constant(im_size[1], dtype=tf.int32)\n        for i in range(parts):\n            p = tf.py_func(np_free_form_mask, [maxVertex, maxLength, maxBrushWidth, maxAngle, h, w], tf.float32)\n            p = tf.reshape(p, [1, im_size[0], im_size[1], 1])\n            mask = mask + p\n        mask = tf.minimum(mask, 1.0)\n    return mask\n\n\ndef gauss_kernel(size=21, sigma=3):\n    interval = (2 * sigma + 1.0) / size\n    x = np.linspace(-sigma-interval/2, sigma+interval/2, size+1)\n    ker1d = np.diff(st.norm.cdf(x))\n    kernel_raw = np.sqrt(np.outer(ker1d, ker1d))\n    kernel = kernel_raw / kernel_raw.sum()\n    out_filter = np.array(kernel, dtype=np.float32)\n    out_filter = out_filter.reshape((size, size, 1, 1))\n    return out_filter\n\n\ndef tf_make_guass_var(size, sigma):\n    kernel = gauss_kernel(size, sigma)\n    var = tf.Variable(tf.convert_to_tensor(kernel))\n    return var\n\n\ndef priority_loss_mask(mask, hsize=64, sigma=1/40, iters=7):\n    kernel = tf_make_guass_var(hsize, sigma)\n    init = 1-mask\n    mask_priority = None\n    for i in range(iters):\n        mask_priority = tf.nn.conv2d(init, kernel, strides=[1,1,1,1], padding=\'SAME\')\n        mask_priority = mask_priority * mask\n        init = mask_priority + (1-mask)\n    return mask_priority\n\n\ndef random_interpolates(x, y, alpha=None):\n    """"""\n    x: first dimension as batch_size\n    y: first dimension as batch_size\n    alpha: [BATCH_SIZE, 1]\n    """"""\n    shape = x.get_shape().as_list()\n    x = tf.reshape(x, [shape[0], -1])\n    y = tf.reshape(y, [shape[0], -1])\n    if alpha is None:\n        alpha = tf.random_uniform(shape=[shape[0], 1])\n    interpolates = x + alpha*(y - x)\n    return tf.reshape(interpolates, shape)\n\n\ndef random_bbox(config):\n    """"""Generate a random tlhw with configuration.\n\n    Args:\n        config: Config should have configuration including IMG_SHAPES,\n            VERTICAL_MARGIN, HEIGHT, HORIZONTAL_MARGIN, WIDTH.\n\n    Returns:\n        tuple: (top, left, height, width)\n\n    """"""\n    img_shape = config.img_shapes\n    img_height = img_shape[0]\n    img_width = img_shape[1]\n    if config.random_mask is True:\n        maxt = img_height - config.margins[0] - config.mask_shapes[0]\n        maxl = img_width - config.margins[1] - config.mask_shapes[1]\n        t = tf.random_uniform(\n            [], minval=config.margins[0], maxval=maxt, dtype=tf.int32)\n        l = tf.random_uniform(\n            [], minval=config.margins[1], maxval=maxl, dtype=tf.int32)\n    else:\n        t = config.mask_shapes[0]//2\n        l = config.mask_shapes[1]//2\n    h = tf.constant(config.mask_shapes[0])\n    w = tf.constant(config.mask_shapes[1])\n    return (t, l, h, w)\n\n\ndef bbox2mask(bbox, config, name=\'mask\'):\n    """"""Generate mask tensor from bbox.\n\n    Args:\n        bbox: configuration tuple, (top, left, height, width)\n        config: Config should have configuration including IMG_SHAPES,\n            MAX_DELTA_HEIGHT, MAX_DELTA_WIDTH.\n\n    Returns:\n        tf.Tensor: output with shape [1, H, W, 1]\n\n    """"""\n    def npmask(bbox, height, width, delta_h, delta_w):\n        mask = np.zeros((1, height, width, 1), np.float32)\n        h = np.random.randint(delta_h//2+1)\n        w = np.random.randint(delta_w//2+1)\n        mask[:, bbox[0]+h:bbox[0]+bbox[2]-h,\n             bbox[1]+w:bbox[1]+bbox[3]-w, :] = 1.\n        return mask\n    with tf.variable_scope(name), tf.device(\'/cpu:0\'):\n        img_shape = config.img_shapes\n        height = img_shape[0]\n        width = img_shape[1]\n        mask = tf.py_func(\n            npmask,\n            [bbox, height, width,\n             config.max_delta_shapes[0], config.max_delta_shapes[1]],\n            tf.float32, stateful=False)\n        mask.set_shape([1] + [height, width] + [1])\n    return mask\n\n\ndef local_patch(x, bbox):\n    """"""Crop local patch according to bbox.\n\n    Args:\n        x: input\n        bbox: (top, left, height, width)\n\n    Returns:\n        tf.Tensor: local patch\n\n    """"""\n    x = tf.image.crop_to_bounding_box(x, bbox[0], bbox[1], bbox[2], bbox[3])\n    return x\n\n\ndef gradients_penalty(x, y, mask=None, norm=1.):\n    """"""Improved Training of Wasserstein GANs\n\n    - https://arxiv.org/abs/1704.00028\n    """"""\n    gradients = tf.gradients(y, x)[0]\n    if mask is None:\n        mask = tf.ones_like(gradients)\n    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients) * mask, axis=[1, 2, 3]))\n    return tf.reduce_mean(tf.square(slopes - norm))\n\n\ndef gan_wgan_loss(pos, neg, name=\'gan_wgan_loss\'):\n    """"""\n    wgan loss function for GANs.\n\n    - Wasserstein GAN: https://arxiv.org/abs/1701.07875\n    """"""\n    with tf.variable_scope(name):\n        d_loss = tf.reduce_mean(neg-pos)\n        g_loss = -tf.reduce_mean(neg)\n        tf.summary.scalar(\'d_loss\', d_loss)\n        tf.summary.scalar(\'g_loss\', g_loss)\n        tf.summary.scalar(\'pos_value_avg\', tf.reduce_mean(pos))\n        tf.summary.scalar(\'neg_value_avg\', tf.reduce_mean(neg))\n    return g_loss, d_loss\n\n\ndef average_gradients(tower_grads):\n    """""" Calculate the average gradient for each shared variable across\n    all towers.\n\n    **Note** that this function provides a synchronization point\n    across all towers.\n\n    Args:\n        tower_grads: List of lists of (gradient, variable) tuples.\n            The outer list is over individual gradients. The inner list is\n            over the gradient calculation for each tower.\n\n    Returns:\n        List of pairs of (gradient, variable) where the gradient\n            has been averaged across all towers.\n\n    """"""\n    average_grads = []\n    for grad_and_vars in zip(*tower_grads):\n        v = grad_and_vars[0][1]\n        # sum\n        grad = tf.add_n([x[0] for x in grad_and_vars])\n        # average\n        grad = grad / float(len(tower_grads))\n        grad_and_var = (grad, v)\n        average_grads.append(grad_and_var)\n    return average_grads\n\n\n""""""\nid-mrf\n""""""\nfrom enum import Enum\n\nclass Distance(Enum):\n    L2 = 0\n    DotProduct = 1\n\nclass CSFlow:\n    def __init__(self, sigma=float(0.1), b=float(1.0)):\n        self.b = b\n        self.sigma = sigma\n\n    def __calculate_CS(self, scaled_distances, axis_for_normalization=3):\n        self.scaled_distances = scaled_distances\n        self.cs_weights_before_normalization = tf.exp((self.b - scaled_distances) / self.sigma, name=\'weights_before_normalization\')\n        self.cs_NHWC = CSFlow.sum_normalize(self.cs_weights_before_normalization, axis_for_normalization)\n\n    def reversed_direction_CS(self):\n        cs_flow_opposite = CSFlow(self.sigma, self.b)\n        cs_flow_opposite.raw_distances = self.raw_distances\n        work_axis = [1, 2]\n        relative_dist = cs_flow_opposite.calc_relative_distances(axis=work_axis)\n        cs_flow_opposite.__calculate_CS(relative_dist, work_axis)\n        return cs_flow_opposite\n\n    # --\n    @staticmethod\n    def create_using_L2(I_features, T_features, sigma=float(0.1), b=float(1.0)):\n        cs_flow = CSFlow(sigma, b)\n        with tf.name_scope(\'CS\'):\n            sT = T_features.shape.as_list()\n            sI = I_features.shape.as_list()\n\n            Ivecs = tf.reshape(I_features, (sI[0], -1, sI[3]))\n            Tvecs = tf.reshape(T_features, (sI[0], -1, sT[3]))\n            r_Ts = tf.reduce_sum(Tvecs * Tvecs, 2)\n            r_Is = tf.reduce_sum(Ivecs * Ivecs, 2)\n            raw_distances_list = []\n            for i in range(sT[TensorAxis.N]):\n                Ivec, Tvec, r_T, r_I = Ivecs[i], Tvecs[i], r_Ts[i], r_Is[i]\n                A = tf.matmul(Tvec,tf.transpose(Ivec))\n                cs_flow.A = A\n                # A = tf.matmul(Tvec, tf.transpose(Ivec))\n                r_T = tf.reshape(r_T, [-1, 1])  # turn to column vector\n                dist = r_T - 2 * A + r_I\n                cs_shape = sI[:3] + [dist.shape[0].value]\n                cs_shape[0] = 1\n                dist = tf.reshape(tf.transpose(dist), cs_shape)\n                # protecting against numerical problems, dist should be positive\n                dist = tf.maximum(float(0.0), dist)\n                # dist = tf.sqrt(dist)\n                raw_distances_list += [dist]\n\n            cs_flow.raw_distances = tf.convert_to_tensor([tf.squeeze(raw_dist, axis=0) for raw_dist in raw_distances_list])\n\n            relative_dist = cs_flow.calc_relative_distances()\n            cs_flow.__calculate_CS(relative_dist)\n            return cs_flow\n\n    #--\n    @staticmethod\n    def create_using_dotP(I_features, T_features, sigma=float(1.0), b=float(1.0)):\n        cs_flow = CSFlow(sigma, b)\n        with tf.name_scope(\'CS\'):\n            # prepare feature before calculating cosine distance\n            T_features, I_features = cs_flow.center_by_T(T_features, I_features)\n            with tf.name_scope(\'TFeatures\'):\n                T_features = CSFlow.l2_normalize_channelwise(T_features)\n            with tf.name_scope(\'IFeatures\'):\n                I_features = CSFlow.l2_normalize_channelwise(I_features)\n                # work seperatly for each example in dim 1\n                cosine_dist_l = []\n                N, _, _, _ = T_features.shape.as_list()\n                for i in range(N):\n                    T_features_i = tf.expand_dims(T_features[i, :, :, :], 0)\n                    I_features_i = tf.expand_dims(I_features[i, :, :, :], 0)\n                    patches_i = cs_flow.patch_decomposition(T_features_i)\n                    cosine_dist_i = tf.nn.conv2d(I_features_i, patches_i, strides=[1, 1, 1, 1],\n                                                        padding=\'VALID\', use_cudnn_on_gpu=True, name=\'cosine_dist\')\n                    cosine_dist_l.append(cosine_dist_i)\n\n                cs_flow.cosine_dist = tf.concat(cosine_dist_l, axis = 0)\n\n                cosine_dist_zero_to_one = -(cs_flow.cosine_dist - 1) / 2\n                cs_flow.raw_distances = cosine_dist_zero_to_one\n\n                relative_dist = cs_flow.calc_relative_distances()\n                cs_flow.__calculate_CS(relative_dist)\n                return cs_flow\n\n    def calc_relative_distances(self, axis=3):\n        epsilon = 1e-5\n        div = tf.reduce_min(self.raw_distances, axis=axis, keep_dims=True)\n        # div = tf.reduce_mean(self.raw_distances, axis=axis, keep_dims=True)\n        relative_dist = self.raw_distances / (div + epsilon)\n        return relative_dist\n\n    def weighted_average_dist(self, axis=3):\n        if not hasattr(self, \'raw_distances\'):\n            raise exception(\'raw_distances property does not exists. cant calculate weighted average l2\')\n\n        multiply = self.raw_distances * self.cs_NHWC\n        return tf.reduce_sum(multiply, axis=axis, name=\'weightedDistPerPatch\')\n\n    # --\n    @staticmethod\n    def create(I_features, T_features, distance : Distance, nnsigma=float(1.0), b=float(1.0)):\n        if distance.value == Distance.DotProduct.value:\n            cs_flow = CSFlow.create_using_dotP(I_features, T_features, nnsigma, b)\n        elif distance.value == Distance.L2.value:\n            cs_flow = CSFlow.create_using_L2(I_features, T_features, nnsigma, b)\n        else:\n            raise ""not supported distance "" + distance.__str__()\n        return cs_flow\n\n    @staticmethod\n    def sum_normalize(cs, axis=3):\n        reduce_sum = tf.reduce_sum(cs, axis, keep_dims=True, name=\'sum\')\n        return tf.divide(cs, reduce_sum, name=\'sumNormalized\')\n\n    def center_by_T(self, T_features, I_features):\n        # assuming both input are of the same size\n\n        # calculate stas over [batch, height, width], expecting 1x1xDepth tensor\n        axes = [0, 1, 2]\n        self.meanT, self.varT = tf.nn.moments(\n            T_features, axes, name=\'TFeatures/moments\')\n        # we do not divide by std since its causing the histogram\n        # for the final cs to be very thin, so the NN weights\n        # are not distinctive, giving similar values for all patches.\n        # stdT = tf.sqrt(varT, ""stdT"")\n        # correct places with std zero\n        # stdT[tf.less(stdT, tf.constant(0.001))] = tf.constant(1)\n        with tf.name_scope(\'TFeatures/centering\'):\n            self.T_features_centered = T_features - self.meanT\n        with tf.name_scope(\'IFeatures/centering\'):\n            self.I_features_centered = I_features - self.meanT\n\n        return self.T_features_centered, self.I_features_centered\n\n    @staticmethod\n    def l2_normalize_channelwise(features):\n        norms = tf.norm(features, ord=\'euclidean\', axis=3, name=\'norm\')\n        # expanding the norms tensor to support broadcast division\n        norms_expanded = tf.expand_dims(norms, 3)\n        features = tf.divide(features, norms_expanded, name=\'normalized\')\n        return features\n\n    def patch_decomposition(self, T_features):\n        # patch decomposition\n        patch_size = 1\n        patches_as_depth_vectors = tf.extract_image_patches(\n            images=T_features, ksizes=[1, patch_size, patch_size, 1],\n            strides=[1, 1, 1, 1], rates=[1, 1, 1, 1], padding=\'VALID\',\n            name=\'patches_as_depth_vectors\')\n\n        self.patches_NHWC = tf.reshape(\n            patches_as_depth_vectors,\n            shape=[-1, patch_size, patch_size, patches_as_depth_vectors.shape[3].value],\n            name=\'patches_PHWC\')\n\n        self.patches_HWCN = tf.transpose(\n            self.patches_NHWC,\n            perm=[1, 2, 3, 0],\n            name=\'patches_HWCP\')  # tf.conv2 ready format\n\n        return self.patches_HWCN\n\n\ndef mrf_loss(T_features, I_features, distance=Distance.DotProduct, nnsigma=float(1.0)):\n    T_features = tf.convert_to_tensor(T_features, dtype=tf.float32)\n    I_features = tf.convert_to_tensor(I_features, dtype=tf.float32)\n\n    with tf.name_scope(\'cx\'):\n        cs_flow = CSFlow.create(I_features, T_features, distance, nnsigma)\n        # sum_normalize:\n        height_width_axis = [1, 2]\n        # To:\n        cs = cs_flow.cs_NHWC\n        k_max_NC = tf.reduce_max(cs, axis=height_width_axis)\n        CS = tf.reduce_mean(k_max_NC, axis=[1])\n        CS_as_loss = 1 - CS\n        CS_loss = -tf.log(1 - CS_as_loss)\n        CS_loss = tf.reduce_mean(CS_loss)\n        return CS_loss\n\n\ndef random_sampling(tensor_in, n, indices=None):\n    N, H, W, C = tf.convert_to_tensor(tensor_in).shape.as_list()\n    S = H * W\n    tensor_NSC = tf.reshape(tensor_in, [N, S, C])\n    all_indices = list(range(S))\n    shuffled_indices = tf.random_shuffle(all_indices)\n    indices = tf.gather(shuffled_indices, list(range(n)), axis=0) if indices is None else indices\n    res = tf.gather(tensor_NSC, indices, axis=1)\n    return res, indices\n\n\ndef random_pooling(feats, output_1d_size=100):\n    is_input_tensor = type(feats) is tf.Tensor\n\n    if is_input_tensor:\n        feats = [feats]\n\n    # convert all inputs to tensors\n    feats = [tf.convert_to_tensor(feats_i) for feats_i in feats]\n\n    N, H, W, C = feats[0].shape.as_list()\n    feats_sampled_0, indices = random_sampling(feats[0], output_1d_size ** 2)\n    res = [feats_sampled_0]\n    for i in range(1, len(feats)):\n        feats_sampled_i, _ = random_sampling(feats[i], -1, indices)\n        res.append(feats_sampled_i)\n\n    res = [tf.reshape(feats_sampled_i, [N, output_1d_size, output_1d_size, C]) for feats_sampled_i in res]\n    if is_input_tensor:\n        return res[0]\n    return res\n\n\ndef crop_quarters(feature_tensor):\n    N, fH, fW, fC = feature_tensor.shape.as_list()\n    quarters_list = []\n    quarter_size = [N, round(fH / 2), round(fW / 2), fC]\n    quarters_list.append(tf.slice(feature_tensor, [0, 0, 0, 0], quarter_size))\n    quarters_list.append(tf.slice(feature_tensor, [0, round(fH / 2), 0, 0], quarter_size))\n    quarters_list.append(tf.slice(feature_tensor, [0, 0, round(fW / 2), 0], quarter_size))\n    quarters_list.append(tf.slice(feature_tensor, [0, round(fH / 2), round(fW / 2), 0], quarter_size))\n    feature_tensor = tf.concat(quarters_list, axis=0)\n    return feature_tensor\n\n\ndef id_mrf_reg_feat(feat_A, feat_B, config):\n    if config.crop_quarters is True:\n        feat_A = crop_quarters(feat_A)\n        feat_B = crop_quarters(feat_B)\n\n    N, fH, fW, fC = feat_A.shape.as_list()\n    if fH * fW <= config.max_sampling_1d_size ** 2:\n        print(\' #### Skipping pooling ....\')\n    else:\n        print(\' #### pooling %d**2 out of %dx%d\' % (config.max_sampling_1d_size, fH, fW))\n        feat_A, feat_B = random_pooling([feat_A, feat_B], output_1d_size=config.max_sampling_1d_size)\n\n    return mrf_loss(feat_A, feat_B, distance=config.Dist, nnsigma=config.nn_stretch_sigma)\n\n\nfrom easydict import EasyDict as edict\n# scale of im_src and im_dst: [-1, 1]\ndef id_mrf_reg(im_src, im_dst, config):\n    vgg = Vgg19(filepath=config.vgg19_path)\n\n    src_vgg = vgg.build_vgg19((im_src + 1) * 127.5)\n    dst_vgg = vgg.build_vgg19((im_dst + 1) * 127.5, reuse=True)\n\n    feat_style_layers = config.feat_style_layers\n    feat_content_layers = config.feat_content_layers\n\n    mrf_style_w = config.mrf_style_w\n    mrf_content_w = config.mrf_content_w\n\n    mrf_config = edict()\n    mrf_config.crop_quarters = False\n    mrf_config.max_sampling_1d_size = 65\n    mrf_config.Dist = Distance.DotProduct\n    mrf_config.nn_stretch_sigma = 0.5  # 0.1\n\n    mrf_style_loss = [w * id_mrf_reg_feat(src_vgg[layer], dst_vgg[layer], mrf_config)\n                      for layer, w in feat_style_layers.items()]\n    mrf_style_loss = tf.reduce_sum(mrf_style_loss)\n\n    mrf_content_loss = [w * id_mrf_reg_feat(src_vgg[layer], dst_vgg[layer], mrf_config)\n                        for layer, w in feat_content_layers.items()]\n    mrf_content_loss = tf.reduce_sum(mrf_content_loss)\n\n    id_mrf_loss = mrf_style_loss * mrf_style_w + mrf_content_loss * mrf_content_w\n    return id_mrf_loss\n\n\nclass Vgg19(object):\n    def __init__(self, filepath=None):\n        self.mean = np.array([123.6800, 116.7790, 103.9390]).reshape((1, 1, 1, 3))\n        self.vgg_weights = filepath if filepath is not None else os.path.join(\'vgg19_weights\', \'imagenet-vgg-verydeep-19.mat\')\n        if os.path.exists(self.vgg_weights) is False:\n            self.vgg_weights = os.path.join(\'vgg19_weights\', \'imagenet-vgg-verydeep-19.mat\')\n            if os.path.isdir(\'vgg19_weights\') is False:\n                os.mkdir(\'vgg19_weights\')\n            url = \'http://www.vlfeat.org/matconvnet/models/imagenet-vgg-verydeep-19.mat\'\n            print(\'Downloading vgg19..\')\n            urllib.request.urlretrieve(url, self.vgg_weights)\n            print(\'vgg19 weights have been downloaded and stored in {}\'.format(self.vgg_weights))\n\n    def build_net(self, ntype, nin, nwb=None, name=None):\n        if ntype == \'conv\':\n            return tf.nn.relu(tf.nn.conv2d(nin, nwb[0], strides=[1, 1, 1, 1], padding=\'SAME\', name=name) + nwb[1])\n        elif ntype == \'pool\':\n            return tf.nn.avg_pool(nin, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\'SAME\')\n\n    def get_weight_bias(self, vgg_layers, i):\n        weights = vgg_layers[i][0][0][2][0][0]\n        weights = tf.constant(weights)\n        bias = vgg_layers[i][0][0][2][0][1]\n        bias = tf.constant(np.reshape(bias, (bias.size)))\n        return weights, bias\n\n    def build_vgg19(self, input, reuse=False):\n        with tf.variable_scope(\'vgg19\', reuse=reuse):\n            net = {}\n            # vgg_rawnet = scipy.io.loadmat(self.vgg_weights)\n            vgg_rawnet = io.loadmat(self.vgg_weights)\n            vgg_layers = vgg_rawnet[\'layers\'][0]\n            net[\'input\'] = input - self.mean\n            net[\'conv1_1\'] = self.build_net(\'conv\', net[\'input\'], self.get_weight_bias(vgg_layers, 0),\n                                            name=\'vgg_conv1_1\')\n            net[\'conv1_2\'] = self.build_net(\'conv\', net[\'conv1_1\'], self.get_weight_bias(vgg_layers, 2),\n                                            name=\'vgg_conv1_2\')\n            net[\'pool1\'] = self.build_net(\'pool\', net[\'conv1_2\'])\n            net[\'conv2_1\'] = self.build_net(\'conv\', net[\'pool1\'], self.get_weight_bias(vgg_layers, 5),\n                                            name=\'vgg_conv2_1\')\n            net[\'conv2_2\'] = self.build_net(\'conv\', net[\'conv2_1\'], self.get_weight_bias(vgg_layers, 7),\n                                            name=\'vgg_conv2_2\')\n            net[\'pool2\'] = self.build_net(\'pool\', net[\'conv2_2\'])\n            net[\'conv3_1\'] = self.build_net(\'conv\', net[\'pool2\'], self.get_weight_bias(vgg_layers, 10),\n                                            name=\'vgg_conv3_1\')\n            net[\'conv3_2\'] = self.build_net(\'conv\', net[\'conv3_1\'], self.get_weight_bias(vgg_layers, 12),\n                                            name=\'vgg_conv3_2\')\n            net[\'conv3_3\'] = self.build_net(\'conv\', net[\'conv3_2\'], self.get_weight_bias(vgg_layers, 14),\n                                            name=\'vgg_conv3_3\')\n            net[\'conv3_4\'] = self.build_net(\'conv\', net[\'conv3_3\'], self.get_weight_bias(vgg_layers, 16),\n                                            name=\'vgg_conv3_4\')\n            net[\'pool3\'] = self.build_net(\'pool\', net[\'conv3_4\'])\n            net[\'conv4_1\'] = self.build_net(\'conv\', net[\'pool3\'], self.get_weight_bias(vgg_layers, 19),\n                                            name=\'vgg_conv4_1\')\n            net[\'conv4_2\'] = self.build_net(\'conv\', net[\'conv4_1\'], self.get_weight_bias(vgg_layers, 21),\n                                            name=\'vgg_conv4_2\')\n            net[\'conv4_3\'] = self.build_net(\'conv\', net[\'conv4_2\'], self.get_weight_bias(vgg_layers, 23),\n                                            name=\'vgg_conv4_3\')\n            net[\'conv4_4\'] = self.build_net(\'conv\', net[\'conv4_3\'], self.get_weight_bias(vgg_layers, 25),\n                                            name=\'vgg_conv4_4\')\n            net[\'pool4\'] = self.build_net(\'pool\', net[\'conv4_4\'])\n            net[\'conv5_1\'] = self.build_net(\'conv\', net[\'pool4\'], self.get_weight_bias(vgg_layers, 28),\n                                            name=\'vgg_conv5_1\')\n            net[\'conv5_2\'] = self.build_net(\'conv\', net[\'conv5_1\'], self.get_weight_bias(vgg_layers, 30),\n                                            name=\'vgg_conv5_2\')\n            net[\'conv5_3\'] = self.build_net(\'conv\', net[\'conv5_2\'], self.get_weight_bias(vgg_layers, 32),\n                                            name=\'vgg_conv5_3\')\n            net[\'conv5_4\'] = self.build_net(\'conv\', net[\'conv5_3\'], self.get_weight_bias(vgg_layers, 34),\n                                            name=\'vgg_conv5_4\')\n            net[\'pool5\'] = self.build_net(\'pool\', net[\'conv5_4\'])\n        return net\n'"
tensorflow/options/__init__.py,0,b''
tensorflow/options/test_options.py,0,"b""import argparse\nimport os\nimport time\n\nclass TestOptions:\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.initialized = False\n\n    def initialize(self):\n        self.parser.add_argument('--dataset', type=str, default='paris_streetview',\n                                 help='The dataset of the experiment.')\n        self.parser.add_argument('--data_file', type=str, default='./imgs/paris-streetview_256x256', help='the file storing testing file paths')\n        self.parser.add_argument('--test_dir', type=str, default='./test_results', help='models are saved here')\n        self.parser.add_argument('--load_model_dir', type=str, default='./checkpoints', help='pretrained models are given here')\n        self.parser.add_argument('--model_prefix', type=str, default='snap')\n        self.parser.add_argument('--seed', type=int, default=1, help='random seed')\n\n        self.parser.add_argument('--model', type=str, default='gmcnn')\n        self.parser.add_argument('--random_mask', type=int, default=0,\n                                 help='using random mask')\n\n        self.parser.add_argument('--img_shapes', type=str, default='256,256,3',\n                                 help='given shape parameters: h,w,c or h,w')\n        self.parser.add_argument('--mask_shapes', type=str, default='128,128',\n                                 help='given mask parameters: h,w')\n        self.parser.add_argument('--mask_type', type=str, default='rect')\n        self.parser.add_argument('--test_num', type=int, default=-1)\n        self.parser.add_argument('--mode', type=str, default='save')\n\n        # for generator\n        self.parser.add_argument('--g_cnum', type=int, default=32,\n                                 help='# of generator filters in first conv layer')\n        self.parser.add_argument('--d_cnum', type=int, default=64,\n                                 help='# of discriminator filters in first conv layer')\n\n    def parse(self):\n        if not self.initialized:\n            self.initialize()\n        self.opt = self.parser.parse_args()\n\n        if self.opt.data_file != '':\n            self.opt.dataset_path = self.opt.data_file\n\n        if os.path.exists(self.opt.test_dir) is False:\n            os.mkdir(self.opt.test_dir)\n\n        assert self.opt.random_mask in [0, 1]\n        self.opt.random_mask = True if self.opt.random_mask == 1 else False\n\n        assert self.opt.mask_type in ['rect', 'stroke']\n\n        str_img_shapes = self.opt.img_shapes.split(',')\n        self.opt.img_shapes = [int(x) for x in str_img_shapes]\n\n        str_mask_shapes = self.opt.mask_shapes.split(',')\n        self.opt.mask_shapes = [int(x) for x in str_mask_shapes]\n\n        # model name and date\n        self.opt.date_str = 'test_'+time.strftime('%Y%m%d-%H%M%S')\n        self.opt.model_folder = self.opt.date_str + '_' + self.opt.dataset + '_' + self.opt.model\n        self.opt.model_folder += '_s' + str(self.opt.img_shapes[0]) + 'x' + str(self.opt.img_shapes[1])\n        self.opt.model_folder += '_gc' + str(self.opt.g_cnum)\n        self.opt.model_folder += '_randmask-' + self.opt.mask_type if self.opt.random_mask else ''\n        if self.opt.random_mask:\n            self.opt.model_folder += '_seed-' + str(self.opt.seed)\n        self.opt.saving_path = os.path.join(self.opt.test_dir, self.opt.model_folder)\n\n        if os.path.exists(self.opt.saving_path) is False and self.opt.mode == 'save':\n            os.mkdir(self.opt.saving_path)\n\n        args = vars(self.opt)\n\n        print('------------ Options -------------')\n        for k, v in sorted(args.items()):\n            print('%s: %s' % (str(k), str(v)))\n        print('-------------- End ----------------')\n\n        return self.opt\n"""
tensorflow/options/train_options.py,0,"b""import argparse\nimport os\nimport time\n\nclass TrainOptions:\n    def __init__(self):\n        self.parser = argparse.ArgumentParser()\n        self.initialized = False\n        self.dataset_training_paths = {\n            'celebahq': '',\n            'celeba': '',\n            'places2': '',\n            'imagenet': '',\n            'paris_streetview': '',\n            'places2full': '',\n            'adobe_5k': '',\n            'celeba_wild': ''\n        }\n\n    def initialize(self):\n        self.parser.add_argument('--dataset', type=str, default='paris_streetview', help='The dataset of the experiment.')\n        self.parser.add_argument('--data_file', type=str, default='', help='the file storing training file paths')\n        self.parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0')\n        self.parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n        self.parser.add_argument('--load_model_dir', type=str, default='', help='pretrained models are given here')\n        self.parser.add_argument('--model_prefix', type=str, default='snap', help='models are saved here')\n\n        self.parser.add_argument('--batch_size', type=int, default=16, help='input batch size')\n\n        self.parser.add_argument('--random_mask', type=int, default=1)\n        self.parser.add_argument('--mask_type', type=str, default='rect')\n        self.parser.add_argument('--pretrain_network', type=int, default=0)\n        self.parser.add_argument('--gan_loss_alpha', type=float, default=1e-3)\n        self.parser.add_argument('--wgan_gp_lambda', type=float, default=10)\n        self.parser.add_argument('--pretrain_l1_alpha', type=float, default=1.2)\n        self.parser.add_argument('--l1_loss_alpha', type=float, default=1.4)\n        self.parser.add_argument('--ae_loss_alpha', type=float, default=1.2)\n        self.parser.add_argument('--mrf_alpha', type=float, default=0.05)\n        self.parser.add_argument('--random_seed', type=bool, default=False)\n        self.parser.add_argument('--lr', type=float, default=1e-5, help='learning rate for training')\n\n        self.parser.add_argument('--train_spe', type=int, default=1000)\n        self.parser.add_argument('--max_iters', type=int, default=40000)\n        self.parser.add_argument('--viz_steps', type=int, default=5)\n\n        self.parser.add_argument('--img_shapes', type=str, default='256,256,3',\n                                 help='given shape parameters: h,w,c or h,w')\n        self.parser.add_argument('--mask_shapes', type=str, default='128,128',\n                                 help='given mask parameters: h,w')\n        self.parser.add_argument('--max_delta_shapes', type=str, default='32,32')\n        self.parser.add_argument('--margins', type=str, default='0,0')\n        # for generator\n        self.parser.add_argument('--g_cnum', type=int, default=32,\n                                 help='# of generator filters in first conv layer')\n        self.parser.add_argument('--d_cnum', type=int, default=64,\n                                 help='# of discriminator filters in first conv layer')\n\n        self.parser.add_argument('--vgg19_path', type=str, default='vgg19_weights/imagenet-vgg-verydeep-19.mat')\n        self.initialized = True\n\n    def parse(self):\n        if not self.initialized:\n            self.initialize()\n        self.opt = self.parser.parse_args()\n\n        assert self.opt.dataset in self.dataset_training_paths.keys()\n        self.opt.dataset_path = \\\n            self.dataset_training_paths[self.opt.dataset] if self.opt.data_file == '' else self.opt.data_file\n\n        str_ids = self.opt.gpu_ids.split(',')\n        self.opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                self.opt.gpu_ids.append(str(id))\n\n        assert self.opt.random_mask in [0, 1]\n        self.opt.random_mask = True if self.opt.random_mask == 1 else False\n\n        assert self.opt.pretrain_network in [0, 1]\n        self.opt.pretrain_network = True if self.opt.pretrain_network == 1 else False\n\n        assert self.opt.mask_type in ['rect', 'stroke']\n\n        str_img_shapes = self.opt.img_shapes.split(',')\n        self.opt.img_shapes = [int(x) for x in str_img_shapes]\n\n        str_mask_shapes = self.opt.mask_shapes.split(',')\n        self.opt.mask_shapes = [int(x) for x in str_mask_shapes]\n\n        str_max_delta_shapes = self.opt.max_delta_shapes.split(',')\n        self.opt.max_delta_shapes = [int(x) for x in str_max_delta_shapes]\n\n        str_margins = self.opt.margins.split(',')\n        self.opt.margins = [int(x) for x in str_margins]\n\n        # model name and date\n        self.opt.date_str = time.strftime('%Y%m%d-%H%M%S')\n        self.opt.model_name = 'GMCNN'\n        self.opt.model_folder = self.opt.date_str + '_' + self.opt.model_name\n        self.opt.model_folder += '_' + self.opt.dataset\n        self.opt.model_folder += '_b' + str(self.opt.batch_size)\n        self.opt.model_folder += '_s' + str(self.opt.img_shapes[0]) + 'x' + str(self.opt.img_shapes[1])\n        self.opt.model_folder += '_gc' + str(self.opt.g_cnum)\n        self.opt.model_folder += '_dc' + str(self.opt.d_cnum)\n\n        self.opt.model_folder += '_randmask-' + self.opt.mask_type if self.opt.random_mask else ''\n        self.opt.model_folder += '_pretrain' if self.opt.pretrain_network else ''\n\n        if os.path.isdir(self.opt.checkpoints_dir) is False:\n            os.mkdir(self.opt.checkpoints_dir)\n\n        self.opt.model_folder = os.path.join(self.opt.checkpoints_dir, self.opt.model_folder)\n        if os.path.isdir(self.opt.model_folder) is False:\n            os.mkdir(self.opt.model_folder)\n\n        # set gpu ids\n        if len(self.opt.gpu_ids) > 0:\n            os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(self.opt.gpu_ids)\n\n        args = vars(self.opt)\n\n        print('------------ Options -------------')\n        for k, v in sorted(args.items()):\n            print('%s: %s' % (str(k), str(v)))\n        print('-------------- End ----------------')\n\n        return self.opt\n"""
tensorflow/util/__init__.py,0,b''
tensorflow/util/util.py,0,"b'import numpy as np\nimport cv2\nimport math\nimport time\nimport shutil\nimport os, re\nimport tensorflow as tf\nfrom net.ops import np_free_form_mask\n\ndef f2uint(x):\n    if x.__class__ == tf.Tensor:\n        return tf.cast(tf.clip_by_value((x+1)*127.5, 0, 255), tf.uint8)\n    else:\n        return np.clip((x+1)*127.5, 0, 255).astype(np.uint8)\n\n\ndef generate_mask_rect(im_shapes, mask_shapes, rand=True):\n    mask = np.zeros((im_shapes[0], im_shapes[1])).astype(np.float32)\n    if rand:\n        of0 = np.random.randint(0, im_shapes[0]-mask_shapes[0])\n        of1 = np.random.randint(0, im_shapes[1]-mask_shapes[1])\n    else:\n        of0 = (im_shapes[0]-mask_shapes[0])//2\n        of1 = (im_shapes[1]-mask_shapes[1])//2\n    mask[of0:of0+mask_shapes[0], of1:of1+mask_shapes[1]] = 1\n    mask = np.expand_dims(mask, axis=2)\n    return mask\n\n\ndef generate_mask_stroke(im_size, parts=16, maxVertex=24, maxLength=100, maxBrushWidth=24, maxAngle=360):\n    h, w = im_size[:2]\n    mask = np.zeros((h, w, 1), dtype=np.float32)\n    for i in range(parts):\n        mask = mask + np_free_form_mask(maxVertex, maxLength, maxBrushWidth, maxAngle, h, w)\n    mask = np.minimum(mask, 1.0)\n    return mask\n\n'"
