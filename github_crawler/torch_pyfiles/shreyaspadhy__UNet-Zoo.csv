file_path,api_count,code
CLSTM.py,20,"b""import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n# Batch x NumChannels x Height x Width\n# UNET --> BatchSize x 1 (3?) x 240 x 240\n# BDCLSTM --> BatchSize x 64 x 240 x240\n\n''' Class CLSTMCell.\n    This represents a single node in a CLSTM series.\n    It produces just one time (spatial) step output.\n'''\n\n\nclass CLSTMCell(nn.Module):\n\n    # Constructor\n    def __init__(self, input_channels, hidden_channels,\n                 kernel_size, bias=True):\n        super(CLSTMCell, self).__init__()\n\n        assert hidden_channels % 2 == 0\n\n        self.input_channels = input_channels\n        self.hidden_channels = hidden_channels\n        self.bias = bias\n        self.kernel_size = kernel_size\n        self.num_features = 4\n\n        self.padding = (kernel_size - 1) // 2\n        self.conv = nn.Conv2d(self.input_channels + self.hidden_channels,\n                              self.num_features * self.hidden_channels,\n                              self.kernel_size,\n                              1,\n                              self.padding)\n\n    # Forward propogation formulation\n    def forward(self, x, h, c):\n        # print('x: ', x.type)\n        # print('h: ', h.type)\n        combined = torch.cat((x, h), dim=1)\n        A = self.conv(combined)\n\n        # NOTE: A? = xz * Wx? + hz-1 * Wh? + b? where * is convolution\n        (Ai, Af, Ao, Ag) = torch.split(A,\n                                       A.size()[1] // self.num_features,\n                                       dim=1)\n\n        i = torch.sigmoid(Ai)     # input gate\n        f = torch.sigmoid(Af)     # forget gate\n        o = torch.sigmoid(Ao)     # output gate\n        g = torch.tanh(Ag)\n\n        c = c * f + i * g           # cell activation state\n        h = o * torch.tanh(c)     # cell hidden state\n\n        return h, c\n\n    @staticmethod\n    def init_hidden(batch_size, hidden_c, shape):\n        try:\n            return(Variable(torch.zeros(batch_size,\n                                    hidden_c,\n                                    shape[0],\n                                    shape[1])).cuda(),\n               Variable(torch.zeros(batch_size,\n                                    hidden_c,\n                                    shape[0],\n                                    shape[1])).cuda())\n        except:\n            return(Variable(torch.zeros(batch_size,\n                                    hidden_c,\n                                    shape[0],\n                                    shape[1])),\n                    Variable(torch.zeros(batch_size,\n                                    hidden_c,\n                                    shape[0],\n                                    shape[1])))\n\n\n''' Class CLSTM.\n    This represents a series of CLSTM nodes (one direction)\n'''\n\n\nclass CLSTM(nn.Module):\n    # Constructor\n    def __init__(self, input_channels=64, hidden_channels=[64],\n                 kernel_size=5, bias=True):\n        super(CLSTM, self).__init__()\n\n        # store stuff\n        self.input_channels = [input_channels] + hidden_channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.num_layers = len(hidden_channels)\n\n        self.bias = bias\n        self.all_layers = []\n\n        # create a node for each layer in the CLSTM\n        for layer in range(self.num_layers):\n            name = 'cell{}'.format(layer)\n            cell = CLSTMCell(self.input_channels[layer],\n                             self.hidden_channels[layer],\n                             self.kernel_size,\n                             self.bias)\n            setattr(self, name, cell)\n            self.all_layers.append(cell)\n\n    # Forward propogation\n    # x --> BatchSize x NumSteps x NumChannels x Height x Width\n    #       BatchSize x 2 x 64 x 240 x 240\n    def forward(self, x):\n        bsize, steps, _, height, width = x.size()\n        internal_state = []\n        outputs = []\n        for step in range(steps):\n            input = torch.squeeze(x[:, step, :, :, :], dim=1)\n            for layer in range(self.num_layers):\n                # populate hidden states for all layers\n                if step == 0:\n                    (h, c) = CLSTMCell.init_hidden(bsize,\n                                                   self.hidden_channels[layer],\n                                                   (height, width))\n                    internal_state.append((h, c))\n\n                # do forward\n                name = 'cell{}'.format(layer)\n                (h, c) = internal_state[layer]\n\n                input, c = getattr(self, name)(\n                    input, h, c)  # forward propogation call\n                internal_state[layer] = (input, c)\n\n            outputs.append(input)\n\n        #for i in range(len(outputs)):\n        #    print(outputs[i].size())\n        return outputs\n\n\nclass BDCLSTM(nn.Module):\n    # Constructor\n    def __init__(self, input_channels=64, hidden_channels=[64],\n                 kernel_size=5, bias=True, num_classes=2):\n\n        super(BDCLSTM, self).__init__()\n        self.forward_net = CLSTM(\n            input_channels, hidden_channels, kernel_size, bias)\n        self.reverse_net = CLSTM(\n            input_channels, hidden_channels, kernel_size, bias)\n        self.conv = nn.Conv2d(\n            2 * hidden_channels[-1], num_classes, kernel_size=1)\n        self.soft = nn.Softmax2d()\n\n    # Forward propogation\n    # x --> BatchSize x NumChannels x Height x Width\n    #       BatchSize x 64 x 240 x 240\n    def forward(self, x1, x2, x3):\n        x1 = torch.unsqueeze(x1, dim=1)\n        x2 = torch.unsqueeze(x2, dim=1)\n        x3 = torch.unsqueeze(x3, dim=1)\n\n        xforward = torch.cat((x1, x2), dim=1)\n        xreverse = torch.cat((x3, x2), dim=1)\n\n        yforward = self.forward_net(xforward)\n        yreverse = self.reverse_net(xreverse)\n\n        # assumes y is BatchSize x NumClasses x 240 x 240\n        # print(yforward[-1].type)\n        ycat = torch.cat((yforward[-1], yreverse[-1]), dim=1)\n        # print(ycat.size())\n        y = self.conv(ycat)\n        # print(y.type)\n        y = self.soft(y)\n        # print(y.type)\n        return y\n"""
data.py,1,"b'import re\nimport torch\nfrom torch.utils.data.dataset import Dataset\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nimport scipy.io as sio\nfrom dataParser import getMaskFileName, getImg\nimport torchvision.transforms as tr\n\n\nclass BraTSDatasetUnet(Dataset):\n    __file = []\n    __im = []\n    __mask = []\n    im_ht = 0\n    im_wd = 0\n    dataset_size = 0\n\n    def __init__(self, dataset_folder, train=True, keywords=[""P1"", ""1"", ""flair""], im_size=[128, 128], transform=None):\n\n        self.__file = []\n        self.__im = []\n        self.__mask = []\n        self.im_ht = im_size[0]\n        self.im_wd = im_size[1]\n        self.transform = transform\n\n        folder = dataset_folder\n        # # Open and load text file including the whole training data\n        if train:\n            folder = dataset_folder + ""Train/""\n        else:\n            folder = dataset_folder + ""Test/""\n\n        for file in os.listdir(folder):\n            if file.endswith("".png""):\n                filename = os.path.splitext(file)[0]\n                filename_fragments = filename.split(""_"")\n                samekeywords = list(set(filename_fragments) & set(keywords))\n                if len(samekeywords) == len(keywords):\n                    # 1. read file name\n                    self.__file.append(filename)\n                    # 2. read raw image\n                    # TODO: I think we should open image only in getitem,\n                    # otherwise memory explodes\n\n                    # rawImage = getImg(folder + file)\n                    self.__im.append(folder + file)\n                    # 3. read mask image\n                    mask_file = getMaskFileName(file)\n                    # maskImage = getImg(folder + mask_file)\n                    self.__mask.append(folder + mask_file)\n        # self.dataset_size = len(self.__file)\n\n        # print(""lengths : "", len(self.__im), len(self.__mask))\n        self.dataset_size = len(self.__file)\n\n        if not train:\n            sio.savemat(\'filelist2.mat\', {\'data\': self.__im})\n\n    def __getitem__(self, index):\n\n        img = getImg(self.__im[index])\n        mask = getImg(self.__mask[index])\n\n        img = img.resize((self.im_ht, self.im_wd))\n        mask = mask.resize((self.im_ht, self.im_wd))\n        # mask.show()\n\n        if self.transform is not None:\n            # TODO: Not sure why not take full image\n            img_tr = self.transform(img)\n            mask_tr = self.transform(mask)\n            # img_tr = self.transform(img[None, :, :])\n            # mask_tr = self.transform(mask[None, :, :])\n\n        return img_tr, mask_tr\n        # return img.float(), mask.float()\n\n    def __len__(self):\n\n        return len(self.__im)\n\n\nclass BraTSDatasetLSTM(Dataset):\n    __im = []\n    __mask = []\n    __im1 = []\n    __im3 = []\n    im_ht = 0\n    im_wd = 0\n    dataset_size = 0\n\n    def __init__(self, dataset_folder, train=True, keywords=[""P1"", ""1"", ""flair""], im_size=[128, 128], transform=None):\n\n        self.__file = []\n        self.__im = []\n        self.__mask = []\n        self.im_ht = im_size[0]\n        self.im_wd = im_size[1]\n        self.transform = transform\n\n        folder = dataset_folder\n        # # Open and load text file including the whole training data\n        if train:\n            folder = dataset_folder + ""Train/""\n        else:\n            folder = dataset_folder + ""Test/""\n\n        # print(""files : "", os.listdir(folder))\n        # print(""Folder : "", folder)\n        max_file = 0\n        min_file = 10000000\n        for file in os.listdir(folder):\n            if file.endswith("".png""):\n                m = re.search(\'(P[0-9]*[_])([0-9]*)\', file)\n                pic_num = int(m.group(2))\n                if pic_num > max_file:\n                    max_file = pic_num\n                if pic_num < min_file:\n                    min_file = pic_num\n\n        # print(\'min file number: \', min_file)\n        # print(\'max file number: \', max_file)\n\n        for file in os.listdir(folder):\n            if file.endswith("".png""):\n                filename = os.path.splitext(file)[0]\n                filename_fragments = filename.split(""_"")\n                samekeywords = list(set(filename_fragments) & set(keywords))\n                if len(samekeywords) == len(keywords):\n                    # 1. read file name\n                    # 2. read raw image\n                    # TODO: I think we should open image only in getitem,\n                    # otherwise memory explodes\n\n                    # rawImage = getImg(folder + file)\n\n                    if (filename_fragments[2] != str(min_file)) and (filename_fragments[2] != str(max_file)):\n                        # print(""TEST : "", filename_fragments[2])\n                        self.__im.append(folder + file)\n\n                        file1 = filename_fragments[0] + \'_\' + filename_fragments[1] + \'_\' + str(\n                            int(filename_fragments[2]) - 1) + \'_\' + filename_fragments[3] + \'.png\'\n\n                        self.__im1.append(folder + file1)\n\n                        file3 = filename_fragments[0] + \'_\' + filename_fragments[1] + \'_\' + str(\n                            int(filename_fragments[2]) + 1) + \'_\' + filename_fragments[3] + \'.png\'\n\n                        self.__im3.append(folder + file3)\n                        # 3. read mask image\n                        mask_file = getMaskFileName(file)\n                        # maskImage = getImg(folder + mask_file)\n                        self.__mask.append(folder + mask_file)\n        # self.dataset_size = len(self.__file)\n\n        # print(""lengths : "", len(self.__im), len(self.__mask))\n        self.dataset_size = len(self.__file)\n\n    def __getitem__(self, index):\n\n        img1 = getImg(self.__im1[index])\n        img = getImg(self.__im[index])\n        img3 = getImg(self.__im3[index])\n        mask = getImg(self.__mask[index])\n\n        # img.show()\n        # mask.show()\n\n        if self.transform is not None:\n            # TODO: Not sure why not take full image\n            img_tr1 = self.transform(img1)\n            img_tr = self.transform(img)\n            img_tr3 = self.transform(img3)\n            mask_tr = self.transform(mask)\n            # img_tr = self.transform(img[None, :, :])\n            # mask_tr = self.transform(mask[None, :, :])\n\n        return img_tr1, img_tr, img_tr3, mask_tr\n        # return img.float(), mask.float()\n\n    def __len__(self):\n\n        return len(self.__im)\n'"
dataParser.py,0,"b'\nimport os\nfrom PIL import Image\nimport numpy as np\nfrom skimage import color\nfrom skimage import io\n\n\ndef getMaskFileName(file):\n\n    mask_file = file.replace(""flair.png"", ""seg.png"")\n    mask_file = mask_file.replace(""t1.png"", ""seg.png"")\n    mask_file = mask_file.replace(""t2.png"", ""seg.png"")\n    mask_file = mask_file.replace(""t1ce.png"", ""seg.png"")\n\n    return mask_file\n\n\ndef getImg(imgpathway):\n    # image_file = Image.open(imgpathway)  # open colour image\n    # img = image_file.convert(\'L\')\n    # IMG = np.asarray(img.getdata())\n    # img = io.imread(imgpathway, as_grey=True)\n    img = Image.open(imgpathway)\n    # img = np.asarray(img)\n    # img *= 65536.0 / np.max(img)\n    # IMG.astype(np.uint16)\n    # plt.imshow(IMG, cmap=\'gray\')\n    # plt.show()\n    return img\n\n\ndef File2Image(self, index):\n    file = self.__file[index]\n    filename_fragments = file.split(""_"")\n    if filename_fragments[1] == \'0\' or filename_fragments[1] == \'154\':\n        # Not sure what to do here\n        return 0, 0\n\n    filename1 = filename_fragments[0] + filename_fragments[1] + \'_\' + \\\n        str(int(filename_fragments[2]) - 1) + \'_\' + filename_fragments[3]\n    filename3 = filename_fragments[0] + filename_fragments[1] + \'_\' + \\\n        str(int(filename_fragments[2]) + 1) + \'_\' + filename_fragments[3]\n\n    idx1 = self.__file.index(filename1)\n    idx3 = self.__file.index(filename3)\n    img1 = self.__im[idx1]\n    img3 = self.__im[idx3]\n\n    return img1, img3\n'"
losses.py,20,"b'# -*- coding: utf-8 -*-\nimport torch\nimport torch.nn as nn\nimport torch.functional as f\nimport numpy as np\n\n\nclass DICELossMultiClass(nn.Module):\n\n    def __init__(self):\n        super(DICELossMultiClass, self).__init__()\n\n    def forward(self, output, mask):\n        num_classes = output.size(1)\n        dice_eso = 0\n        for i in range(num_classes):\n            probs = torch.squeeze(output[:, i, :, :], 1)\n            mask = torch.squeeze(mask[:, i, :, :], 1)\n\n            num = probs * mask\n            num = torch.sum(num, 2)\n            num = torch.sum(num, 1)\n\n            # print( num )\n\n            den1 = probs * probs\n            # print(den1.size())\n            den1 = torch.sum(den1, 2)\n            den1 = torch.sum(den1, 1)\n\n            # print(den1.size())\n\n            den2 = mask * mask\n            # print(den2.size())\n            den2 = torch.sum(den2, 2)\n            den2 = torch.sum(den2, 1)\n\n            # print(den2.size())\n            eps = 0.0000001\n            dice = 2 * ((num + eps) / (den1 + den2 + eps))\n            # dice_eso = dice[:, 1:]\n            dice_eso += dice\n\n        loss = 1 - torch.sum(dice_eso) / dice_eso.size(0)\n        return loss\n\n\nclass DICELoss(nn.Module):\n\n    def __init__(self):\n        super(DICELoss, self).__init__()\n\n    def forward(self, output, mask):\n\n        probs = torch.squeeze(output, 1)\n        mask = torch.squeeze(mask, 1)\n\n        intersection = probs * mask\n        intersection = torch.sum(intersection, 2)\n        intersection = torch.sum(intersection, 1)\n\n        den1 = probs * probs\n        den1 = torch.sum(den1, 2)\n        den1 = torch.sum(den1, 1)\n\n        den2 = mask * mask\n        den2 = torch.sum(den2, 2)\n        den2 = torch.sum(den2, 1)\n\n        eps = 1e-8\n        dice = 2 * ((intersection + eps) / (den1 + den2 + eps))\n        # dice_eso = dice[:, 1:]\n        dice_eso = dice\n\n        loss = 1 - torch.sum(dice_eso) / dice_eso.size(0)\n        return loss\n'"
main.py,8,"b'# %% -*- coding: utf-8 -*-\n\'\'\'\nAuthor: Shreyas Padhy\nDriver file for Standard UNet Implementation\n\'\'\'\nfrom __future__ import print_function\n\nimport argparse\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport scipy.io as sio\nimport torchvision.transforms as tr\n\nfrom data import BraTSDatasetUnet, BraTSDatasetLSTM\nfrom losses import DICELossMultiClass\nfrom models import UNet\nfrom tqdm import tqdm\nimport numpy as np\n\n# %% import transforms\n\n# %% Training settings\nparser = argparse.ArgumentParser(\n    description=\'UNet + BDCLSTM for BraTS Dataset\')\nparser.add_argument(\'--batch-size\', type=int, default=4, metavar=\'N\',\n                    help=\'input batch size for training (default: 64)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1000, metavar=\'N\',\n                    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\'--train\', action=\'store_true\', default=False,\n                    help=\'Argument to train model (default: False)\')\nparser.add_argument(\'--epochs\', type=int, default=1, metavar=\'N\',\n                    help=\'number of epochs to train (default: 10)\')\nparser.add_argument(\'--lr\', type=float, default=0.001, metavar=\'LR\',\n                    help=\'learning rate (default: 0.01)\')\nparser.add_argument(\'--cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training (default: False)\')\nparser.add_argument(\'--log-interval\', type=int, default=1, metavar=\'N\',\n                    help=\'batches to wait before logging training status\')\nparser.add_argument(\'--size\', type=int, default=128, metavar=\'N\',\n                    help=\'imsize\')\nparser.add_argument(\'--load\', type=str, default=None, metavar=\'str\',\n                    help=\'weight file to load (default: None)\')\nparser.add_argument(\'--data-folder\', type=str, default=\'./Data/\', metavar=\'str\',\n                    help=\'folder that contains data (default: test dataset)\')\nparser.add_argument(\'--save\', type=str, default=\'OutMasks\', metavar=\'str\',\n                    help=\'Identifier to save npy arrays with\')\nparser.add_argument(\'--modality\', type=str, default=\'flair\', metavar=\'str\',\n                    help=\'Modality to use for training (default: flair)\')\nparser.add_argument(\'--optimizer\', type=str, default=\'SGD\', metavar=\'str\',\n                    help=\'Optimizer (default: SGD)\')\n\nargs = parser.parse_args()\nargs.cuda = args.cuda and torch.cuda.is_available()\n\nDATA_FOLDER = args.data_folder\n\n# %% Loading in the Dataset\ndset_train = BraTSDatasetUnet(DATA_FOLDER, train=True,\n                              keywords=[args.modality],\n                              im_size=[args.size, args.size],\n                              transform=tr.ToTensor())\n\ntrain_loader = DataLoader(dset_train,\n                          batch_size=args.batch_size,\n                          shuffle=True, num_workers=1)\n\ndset_test = BraTSDatasetUnet(DATA_FOLDER, train=False,\n                             keywords=[args.modality],\n                             im_size=[args.size, args.size],\n                             transform=tr.ToTensor())\n\ntest_loader = DataLoader(dset_test,\n                         batch_size=args.test_batch_size,\n                         shuffle=False, num_workers=1)\n\n\nprint(""Training Data : "", len(train_loader.dataset))\nprint(""Test Data :"", len(test_loader.dataset))\n\n# %% Loading in the model\nmodel = UNet()\n\nif args.cuda:\n    model.cuda()\n\nif args.optimizer == \'SGD\':\n    optimizer = optim.SGD(model.parameters(), lr=args.lr,\n                          momentum=0.99)\nif args.optimizer == \'ADAM\':\n    optimizer = optim.Adam(model.parameters(), lr=args.lr,\n                           betas=(args.beta1, args.beta2))\n\n\n# Defining Loss Function\ncriterion = DICELossMultiClass()\n\n\ndef train(epoch, loss_lsit):\n    model.train()\n    for batch_idx, (image, mask) in enumerate(train_loader):\n        if args.cuda:\n            image, mask = image.cuda(), mask.cuda()\n\n        image, mask = Variable(image), Variable(mask)\n\n        optimizer.zero_grad()\n\n        output = model(image)\n\n        loss = criterion(output, mask)\n        loss_list.append(loss.data[0])\n\n        loss.backward()\n        optimizer.step()\n\n        if batch_idx % args.log_interval == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tAverage DICE Loss: {:.6f}\'.format(\n                epoch, batch_idx * len(image), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data[0]))\n\n\ndef test(train_accuracy=False, save_output=False):\n    test_loss = 0\n\n    if train_accuracy:\n        loader = train_loader\n    else:\n        loader = test_loader\n\n    for batch_idx, (image, mask) in tqdm(enumerate(loader)):\n        if args.cuda:\n            image, mask = image.cuda(), mask.cuda()\n\n        image, mask = Variable(image, volatile=True), Variable(\n            mask, volatile=True)\n\n        output = model(image)\n\n        # test_loss += criterion(output, mask).data[0]\n        maxes, out = torch.max(output, 1, keepdim=True)\n\n        if save_output and (not train_accuracy):\n            np.save(\'./npy-files/out-files/{}-batch-{}-outs.npy\'.format(args.save,\n                                                                        batch_idx),\n                    out.data.byte().cpu().numpy())\n            np.save(\'./npy-files/out-files/{}-batch-{}-masks.npy\'.format(args.save,\n                                                                         batch_idx),\n                    mask.data.byte().cpu().numpy())\n            np.save(\'./npy-files/out-files/{}-batch-{}-images.npy\'.format(args.save,\n                                                                          batch_idx),\n                    image.data.float().cpu().numpy())\n\n        if save_output and train_accuracy:\n            np.save(\'./npy-files/out-files/{}-train-batch-{}-outs.npy\'.format(args.save,\n                                                                              batch_idx),\n                    out.data.byte().cpu().numpy())\n            np.save(\'./npy-files/out-files/{}-train-batch-{}-masks.npy\'.format(args.save,\n                                                                               batch_idx),\n                    mask.data.byte().cpu().numpy())\n            np.save(\'./npy-files/out-files/{}-train-batch-{}-images.npy\'.format(args.save,\n                                                                                batch_idx),\n                    image.data.float().cpu().numpy())\n\n        test_loss += criterion(output, mask).data[0]\n\n    # Average Dice Coefficient\n    test_loss /= len(loader)\n    if train_accuracy:\n        print(\'\\nTraining Set: Average DICE Coefficient: {:.4f})\\n\'.format(\n            test_loss))\n    else:\n        print(\'\\nTest Set: Average DICE Coefficient: {:.4f})\\n\'.format(\n            test_loss))\n\n\nif args.train:\n    loss_list = []\n    for i in tqdm(range(args.epochs)):\n        train(i, loss_list)\n        test()\n\n    plt.plot(loss_list)\n    plt.title(""UNet bs={}, ep={}, lr={}"".format(args.batch_size,\n                                                args.epochs, args.lr))\n    plt.xlabel(""Number of iterations"")\n    plt.ylabel(""Average DICE loss per batch"")\n    plt.savefig(""./plots/{}-UNet_Loss_bs={}_ep={}_lr={}.png"".format(args.save,\n                                                                    args.batch_size,\n                                                                    args.epochs,\n                                                                    args.lr))\n\n    np.save(\'./npy-files/loss-files/{}-UNet_Loss_bs={}_ep={}_lr={}.npy\'.format(args.save,\n                                                                               args.batch_size,\n                                                                               args.epochs,\n                                                                               args.lr),\n            np.asarray(loss_list))\n\n    torch.save(model.state_dict(), \'unet-final-{}-{}-{}\'.format(args.batch_size,\n                                                                args.epochs,\n                                                                args.lr))\nelif args.load is not None:\n    model.load_state_dict(torch.load(args.load))\n    test(save_output=True)\n    test(train_accuracy=True)\n'"
main_bdclstm.py,7,"b'import argparse\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.optim as optim\nfrom losses import DICELossMultiClass\n\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as tr\n\nfrom data import BraTSDatasetLSTM\nfrom CLSTM import BDCLSTM\nfrom models import *\n\n# %% import transforms\n\nUNET_MODEL_FILE = \'unetsmall-100-10-0.001\'\nMODALITY = [""flair""]\n\n# %% Training settings\nparser = argparse.ArgumentParser(description=\'UNet+BDCLSTM for BraTS Dataset\')\nparser.add_argument(\'--batch-size\', type=int, default=4, metavar=\'N\',\n                    help=\'input batch size for training (default: 64)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1000, metavar=\'N\',\n                    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\'--train\', action=\'store_true\', default=False,\n                    help=\'Argument to train model (default: False)\')\nparser.add_argument(\'--epochs\', type=int, default=1, metavar=\'N\',\n                    help=\'number of epochs to train (default: 10)\')\nparser.add_argument(\'--lr\', type=float, default=0.001, metavar=\'LR\',\n                    help=\'learning rate (default: 0.01)\')\nparser.add_argument(\'--mom\', type=float, default=0.99, metavar=\'MOM\',\n                    help=\'SGD momentum (default=0.99)\')\nparser.add_argument(\'--cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training (default: False)\')\nparser.add_argument(\'--log-interval\', type=int, default=1, metavar=\'N\',\n                    help=\'batches to wait before logging training status\')\nparser.add_argument(\'--test-dataset\', action=\'store_true\', default=False,\n                    help=\'test on smaller dataset (default: False)\')\nparser.add_argument(\'--size\', type=int, default=128, metavar=\'N\',\n                    help=\'imsize\')\nparser.add_argument(\'--drop\', action=\'store_true\', default=False,\n                    help=\'enables drop\')\nparser.add_argument(\'--data-folder\', type=str, default=\'./Data-Nonzero/\', metavar=\'str\',\n                    help=\'folder that contains data (default: test dataset)\')\n\n\nargs = parser.parse_args()\nargs.cuda = args.cuda and torch.cuda.is_available()\nif args.cuda:\n    print(""We are on the GPU!"")\n\nDATA_FOLDER = args.data_folder\n\n# %% Loading in the Dataset\ndset_test = BraTSDatasetLSTM(\n    DATA_FOLDER, keywords=MODALITY, transform=tr.ToTensor())\ntest_loader = DataLoader(\n    dset_test, batch_size=args.test_batch_size, shuffle=False, num_workers=1)\n\ndset_train = BraTSDatasetLSTM(\n    DATA_FOLDER, keywords=MODALITY, transform=tr.ToTensor())\ntrain_loader = DataLoader(\n    dset_train, batch_size=args.batch_size, shuffle=True, num_workers=1)\n\n\n# %% Loading in the models\nunet = UNetSmall()\nunet.load_state_dict(torch.load(UNET_MODEL_FILE))\nmodel = BDCLSTM(input_channels=32, hidden_channels=[32])\n\nif args.cuda:\n    unet.cuda()\n    model.cuda()\n\n# Setting Optimizer\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.mom)\ncriterion = DICELossMultiClass()\n\n# Define Training Loop\n\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (image1, image2, image3, mask) in enumerate(train_loader):\n        if args.cuda:\n            image1, image2, image3, mask = image1.cuda(), \\\n                image2.cuda(), \\\n                image3.cuda(), \\\n                mask.cuda()\n\n        image1, image2, image3, mask = Variable(image1), \\\n            Variable(image2), \\\n            Variable(image3), \\\n            Variable(mask)\n\n        optimizer.zero_grad()\n\n        map1 = unet(image1, return_features=True)\n        map2 = unet(image2, return_features=True)\n        map3 = unet(image3, return_features=True)\n\n        output = model(map1, map2, map3)\n        loss = criterion(output, mask)\n\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args.log_interval == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(image1), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data[0]))\n\n\ndef test(train_accuracy=False):\n    test_loss = 0\n\n    if train_accuracy == True:\n        loader = train_loader\n    else:\n        loader = test_loader\n\n    for (image1, image2, image3, mask) in loader:\n        if args.cuda:\n            image1, image2, image3, mask = image1.cuda(), \\\n                image2.cuda(), \\\n                image3.cuda(), \\\n                mask.cuda()\n\n        image1, image2, image3, mask = Variable(image1, volatile=True), \\\n            Variable(image2, volatile=True), \\\n            Variable(image3, volatile=True), \\\n            Variable(mask, volatile=True)\n        map1 = unet(image1, return_features=True)\n        map2 = unet(image2, return_features=True)\n        map3 = unet(image3, return_features=True)\n\n        # print(image1.type)\n        # print(map1.type)\n\n        output = model(map1, map2, map3)\n        test_loss += criterion(output, mask).data[0]\n\n    test_loss /= len(loader)\n    if train_accuracy:\n        print(\n            \'\\nTraining Set: Average Dice Coefficient: {:.4f}\\n\'.format(test_loss))\n    else:\n        print(\n            \'\\nTest Set: Average Dice Coefficient: {:.4f}\\n\'.format(test_loss))\n\n\nif args.train:\n    for i in range(args.epochs):\n        train(i)\n        test()\n\n    torch.save(model.state_dict(),\n               \'bdclstm-{}-{}-{}\'.format(args.batch_size, args.epochs, args.lr))\nelse:\n    model.load_state_dict(torch.load(\'bdclstm-{}-{}-{}\'.format(args.batch_size,\n                                                               args.epochs,\n                                                               args.lr)))\n    test()\n    test(train_accuracy=True)\n'"
main_small.py,7,"b'# %% -*- coding: utf-8 -*-\n\'\'\'\nAuthor: Shreyas Padhy\nDriver file for Unet and BDC-LSTM Implementation\n\'\'\'\n\nfrom __future__ import print_function\n\nimport argparse\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as tr\n\nfrom data import BraTSDatasetUnet, BraTSDatasetLSTM\nfrom losses import DICELoss\nfrom models import UNetSmall\nfrom tqdm import tqdm\nimport scipy.io as sio\nimport numpy as np\n\n# %% import transforms\n\n# %% Training settings\nparser = argparse.ArgumentParser(description=\'UNet+BDCLSTM for BraTS Dataset\')\nparser.add_argument(\'--batch-size\', type=int, default=4, metavar=\'N\',\n                    help=\'input batch size for training (default: 64)\')\nparser.add_argument(\'--test-batch-size\', type=int, default=1000, metavar=\'N\',\n                    help=\'input batch size for testing (default: 1000)\')\nparser.add_argument(\'--train\', action=\'store_true\', default=False,\n                    help=\'Argument to train model (default: False)\')\nparser.add_argument(\'--epochs\', type=int, default=1, metavar=\'N\',\n                    help=\'number of epochs to train (default: 10)\')\nparser.add_argument(\'--lr\', type=float, default=0.001, metavar=\'LR\',\n                    help=\'learning rate (default: 0.01)\')\nparser.add_argument(\'--cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training (default: False)\')\nparser.add_argument(\'--log-interval\', type=int, default=1, metavar=\'N\',\n                    help=\'batches to wait before logging training status\')\nparser.add_argument(\'--size\', type=int, default=128, metavar=\'N\',\n                    help=\'imsize\')\nparser.add_argument(\'--load\', type=str, default=None, metavar=\'str\',\n                    help=\'weight file to load (default: None)\')\nparser.add_argument(\'--data-folder\', type=str, default=\'./Data/\', metavar=\'str\',\n                    help=\'folder that contains data (default: test dataset)\')\nparser.add_argument(\'--save\', type=str, default=\'OutMasks\', metavar=\'str\',\n                    help=\'Identifier to save npy arrays with\')\nparser.add_argument(\'--modality\', type=str, default=\'flair\', metavar=\'str\',\n                    help=\'Modality to use for training (default: flair)\')\nparser.add_argument(\'--optimizer\', type=str, default=\'ADAM\', metavar=\'str\',\n                    help=\'Optimizer (default: SGD)\')\nparser.add_argument(\'--clip\', action=\'store_true\', default=False,\n                    help=\'enables gradnorm clip of 1.0 (default: False)\')\nargs = parser.parse_args()\nargs.cuda = args.cuda and torch.cuda.is_available()\n\nDATA_FOLDER = args.data_folder\n\n# %% Loading in the Dataset\ndset_train = BraTSDatasetUnet(DATA_FOLDER, train=True,\n                              keywords=[args.modality],\n                              im_size=[args.size, args.size], transform=tr.ToTensor())\n\ntrain_loader = DataLoader(dset_train,\n                          batch_size=args.batch_size,\n                          shuffle=True, num_workers=1)\n\ndset_test = BraTSDatasetUnet(DATA_FOLDER, train=False,\n                             keywords=[args.modality],\n                             im_size=[args.size, args.size], transform=tr.ToTensor())\n\ntest_loader = DataLoader(dset_test,\n                         batch_size=args.test_batch_size,\n                         shuffle=False, num_workers=1)\n\n\nprint(""Training Data : "", len(train_loader.dataset))\nprint(""Testing Data : "", len(test_loader.dataset))\n\n# %% Loading in the model\nmodel = UNetSmall()\n\nif args.cuda:\n    model.cuda()\n\nif args.optimizer == \'SGD\':\n    optimizer = optim.SGD(model.parameters(), lr=args.lr,\n                          momentum=0.99)\nif args.optimizer == \'ADAM\':\n    optimizer = optim.Adam(model.parameters(), lr=args.lr,\n                           betas=(0.9, 0.999))\n\n\n# Defining Loss Function\ncriterion = DICELoss()\n# Define Training Loop\n\n\ndef train(epoch, loss_list):\n    model.train()\n    for batch_idx, (image, mask) in enumerate(train_loader):\n        if args.cuda:\n            image, mask = image.cuda(), mask.cuda()\n\n        image, mask = Variable(image), Variable(mask)\n\n        optimizer.zero_grad()\n\n        output = model(image)\n\n        loss = criterion(output, mask)\n        loss_list.append(loss.data[0])\n\n        loss.backward()\n        optimizer.step()\n\n        if args.clip:\n            nn.utils.clip_grad_norm(model.parameters(), max_norm=1)\n\n        if batch_idx % args.log_interval == 0:\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(image), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.data[0]))\n\n\ndef test(train_accuracy=False, save_output=False):\n    test_loss = 0\n    correct = 0\n\n    if train_accuracy:\n        loader = train_loader\n    else:\n        loader = test_loader\n\n    for batch_idx, (image, mask) in tqdm(enumerate(loader)):\n        if args.cuda:\n            image, mask = image.cuda(), mask.cuda()\n\n        image, mask = Variable(image, volatile=True), Variable(\n            mask, volatile=True)\n\n        output = model(image)\n\n        test_loss += criterion(output, mask).data[0]\n\n        output.data.round_()\n\n        if save_output and (not train_accuracy):\n            np.save(\'./npy-files/out-files/{}-unetsmall-batch-{}-outs.npy\'.format(args.save,\n                                                                                  batch_idx),\n                    output.data.byte().cpu().numpy())\n            np.save(\'./npy-files/out-files/{}-unetsmall--batch-{}-masks.npy\'.format(args.save,\n                                                                                    batch_idx),\n                    mask.data.byte().cpu().numpy())\n            np.save(\'./npy-files/out-files/{}-unetsmall--batch-{}-images.npy\'.format(args.save,\n                                                                                     batch_idx),\n                    image.data.float().cpu().numpy())\n\n        if save_output and train_accuracy:\n            np.save(\'./npy-files/out-files/{}-unetsmall-train-batch-{}-outs.npy\'.format(args.save,\n                                                                                        batch_idx),\n                    output.data.byte().cpu().numpy())\n            np.save(\'./npy-files/out-files/{}-unetsmall-train-batch-{}-masks.npy\'.format(args.save,\n                                                                                         batch_idx),\n                    mask.data.byte().cpu().numpy())\n            np.save(\'./npy-files/out-files/{}-unetsmall-train-batch-{}-images.npy\'.format(args.save,\n                                                                                          batch_idx),\n                    image.data.float().cpu().numpy())\n\n    # Average Dice Coefficient\n    test_loss /= len(loader)\n    if train_accuracy:\n        print(\'\\nTraining Set: Average DICE Coefficient: {:.4f})\\n\'.format(\n            test_loss))\n    else:\n        print(\'\\nTest Set: Average DICE Coefficient: {:.4f})\\n\'.format(\n            test_loss))\n\n\nif args.train:\n    loss_list = []\n    for i in tqdm(range(args.epochs)):\n        train(i, loss_list)\n        test(train_accuracy=False, save_output=False)\n        test(train_accuracy=True, save_output=False)\n\n    plt.plot(loss_list)\n    plt.title(""UNetSmall bs={}, ep={}, lr={}"".format(args.batch_size,\n                                                     args.epochs, args.lr))\n    plt.xlabel(""Number of iterations"")\n    plt.ylabel(""Average DICE loss per batch"")\n    plt.savefig(""./plots/{}-UNetSmall_Loss_bs={}_ep={}_lr={}.png"".format(args.save,\n                                                                         args.batch_size,\n                                                                         args.epochs,\n                                                                         args.lr))\n\n    np.save(\'./npy-files/loss-files/{}-UNetSmall_Loss_bs={}_ep={}_lr={}.npy\'.format(args.save,\n                                                                                    args.batch_size,\n                                                                                    args.epochs,\n                                                                                    args.lr),\n            np.asarray(loss_list))\n\n    torch.save(model.state_dict(), \'unetsmall-final-{}-{}-{}\'.format(args.batch_size,\n                                                                     args.epochs,\n                                                                     args.lr))\nelse:\n    model.load_state_dict(torch.load(args.load))\n    test(save_output=True)\n    test(train_accuracy=True)\n'"
models.py,3,"b""# -*- coding: utf-8 -*-\nimport torch.nn as nn\nimport torch\n\n\nclass UNet(nn.Module):\n    def __init__(self, num_channels=1, num_classes=2):\n        super(UNet, self).__init__()\n        num_feat = [64, 128, 256, 512, 1024]\n\n        self.down1 = nn.Sequential(Conv3x3(num_channels, num_feat[0]))\n\n        self.down2 = nn.Sequential(nn.MaxPool2d(kernel_size=2),\n                                   Conv3x3(num_feat[0], num_feat[1]))\n\n        self.down3 = nn.Sequential(nn.MaxPool2d(kernel_size=2),\n                                   Conv3x3(num_feat[1], num_feat[2]))\n\n        self.down4 = nn.Sequential(nn.MaxPool2d(kernel_size=2),\n                                   Conv3x3(num_feat[2], num_feat[3]))\n\n        self.bottom = nn.Sequential(nn.MaxPool2d(kernel_size=2),\n                                    Conv3x3(num_feat[3], num_feat[4]))\n\n        self.up1 = UpConcat(num_feat[4], num_feat[3])\n        self.upconv1 = Conv3x3(num_feat[4], num_feat[3])\n\n        self.up2 = UpConcat(num_feat[3], num_feat[2])\n        self.upconv2 = Conv3x3(num_feat[3], num_feat[2])\n\n        self.up3 = UpConcat(num_feat[2], num_feat[1])\n        self.upconv3 = Conv3x3(num_feat[2], num_feat[1])\n\n        self.up4 = UpConcat(num_feat[1], num_feat[0])\n        self.upconv4 = Conv3x3(num_feat[1], num_feat[0])\n\n        self.final = nn.Sequential(nn.Conv2d(num_feat[0],\n                                             num_classes,\n                                             kernel_size=1),\n                                   nn.Softmax2d())\n\n    def forward(self, inputs, return_features=False):\n        # print(inputs.data.size())\n        down1_feat = self.down1(inputs)\n        # print(down1_feat.size())\n        down2_feat = self.down2(down1_feat)\n        # print(down2_feat.size())\n        down3_feat = self.down3(down2_feat)\n        # print(down3_feat.size())\n        down4_feat = self.down4(down3_feat)\n        # print(down4_feat.size())\n        bottom_feat = self.bottom(down4_feat)\n\n        # print(bottom_feat.size())\n        up1_feat = self.up1(bottom_feat, down4_feat)\n        # print(up1_feat.size())\n        up1_feat = self.upconv1(up1_feat)\n        # print(up1_feat.size())\n        up2_feat = self.up2(up1_feat, down3_feat)\n        # print(up2_feat.size())\n        up2_feat = self.upconv2(up2_feat)\n        # print(up2_feat.size())\n        up3_feat = self.up3(up2_feat, down2_feat)\n        # print(up3_feat.size())\n        up3_feat = self.upconv3(up3_feat)\n        # print(up3_feat.size())\n        up4_feat = self.up4(up3_feat, down1_feat)\n        # print(up4_feat.size())\n        up4_feat = self.upconv4(up4_feat)\n        # print(up4_feat.size())\n\n        if return_features:\n            outputs = up4_feat\n        else:\n            outputs = self.final(up4_feat)\n\n        return outputs\n\n\nclass UNetSmall(nn.Module):\n    def __init__(self, num_channels=1, num_classes=2):\n        super(UNetSmall, self).__init__()\n        num_feat = [32, 64, 128, 256]\n\n        self.down1 = nn.Sequential(Conv3x3Small(num_channels, num_feat[0]))\n\n        self.down2 = nn.Sequential(nn.MaxPool2d(kernel_size=2),\n                                   nn.BatchNorm2d(num_feat[0]),\n                                   Conv3x3Small(num_feat[0], num_feat[1]))\n\n        self.down3 = nn.Sequential(nn.MaxPool2d(kernel_size=2),\n                                   nn.BatchNorm2d(num_feat[1]),\n                                   Conv3x3Small(num_feat[1], num_feat[2]))\n\n        self.bottom = nn.Sequential(nn.MaxPool2d(kernel_size=2),\n                                    nn.BatchNorm2d(num_feat[2]),\n                                    Conv3x3Small(num_feat[2], num_feat[3]),\n                                    nn.BatchNorm2d(num_feat[3]))\n\n        self.up1 = UpSample(num_feat[3], num_feat[2])\n        self.upconv1 = nn.Sequential(Conv3x3Small(num_feat[3] + num_feat[2], num_feat[2]),\n                                     nn.BatchNorm2d(num_feat[2]))\n\n        self.up2 = UpSample(num_feat[2], num_feat[1])\n        self.upconv2 = nn.Sequential(Conv3x3Small(num_feat[2] + num_feat[1], num_feat[1]),\n                                     nn.BatchNorm2d(num_feat[1]))\n\n        self.up3 = UpSample(num_feat[1], num_feat[0])\n        self.upconv3 = nn.Sequential(Conv3x3Small(num_feat[1] + num_feat[0], num_feat[0]),\n                                     nn.BatchNorm2d(num_feat[0]))\n\n        self.final = nn.Sequential(nn.Conv2d(num_feat[0],\n                                             1,\n                                             kernel_size=1),\n                                   nn.Sigmoid())\n\n    def forward(self, inputs, return_features=False):\n        # print(inputs.data.size())\n        down1_feat = self.down1(inputs)\n        # print(down1_feat.size())\n        down2_feat = self.down2(down1_feat)\n        # print(down2_feat.size())\n        down3_feat = self.down3(down2_feat)\n        # print(down3_feat.size())\n        bottom_feat = self.bottom(down3_feat)\n\n        # print(bottom_feat.size())\n        up1_feat = self.up1(bottom_feat, down3_feat)\n        # print(up1_feat.size())\n        up1_feat = self.upconv1(up1_feat)\n        # print(up1_feat.size())\n        up2_feat = self.up2(up1_feat, down2_feat)\n        # print(up2_feat.size())\n        up2_feat = self.upconv2(up2_feat)\n        # print(up2_feat.size())\n        up3_feat = self.up3(up2_feat, down1_feat)\n        # print(up3_feat.size())\n        up3_feat = self.upconv3(up3_feat)\n        # print(up3_feat.size())\n\n        if return_features:\n            outputs = up3_feat\n        else:\n            outputs = self.final(up3_feat)\n\n        return outputs\n\n\nclass Conv3x3(nn.Module):\n    def __init__(self, in_feat, out_feat):\n        super(Conv3x3, self).__init__()\n\n        self.conv1 = nn.Sequential(nn.Conv2d(in_feat, out_feat,\n                                             kernel_size=3,\n                                             stride=1,\n                                             padding=1),\n                                   nn.BatchNorm2d(out_feat),\n                                   nn.ReLU())\n\n        self.conv2 = nn.Sequential(nn.Conv2d(out_feat, out_feat,\n                                             kernel_size=3,\n                                             stride=1,\n                                             padding=1),\n                                   nn.BatchNorm2d(out_feat),\n                                   nn.ReLU())\n\n    def forward(self, inputs):\n        outputs = self.conv1(inputs)\n        outputs = self.conv2(outputs)\n        return outputs\n\n\nclass Conv3x3Drop(nn.Module):\n    def __init__(self, in_feat, out_feat):\n        super(Conv3x3Drop, self).__init__()\n\n        self.conv1 = nn.Sequential(nn.Conv2d(in_feat, out_feat,\n                                             kernel_size=3,\n                                             stride=1,\n                                             padding=1),\n                                   nn.Dropout(p=0.2),\n                                   nn.ReLU())\n\n        self.conv2 = nn.Sequential(nn.Conv2d(out_feat, out_feat,\n                                             kernel_size=3,\n                                             stride=1,\n                                             padding=1),\n                                   nn.BatchNorm2d(out_feat),\n                                   nn.ReLU())\n\n    def forward(self, inputs):\n        outputs = self.conv1(inputs)\n        outputs = self.conv2(outputs)\n        return outputs\n\n\nclass Conv3x3Small(nn.Module):\n    def __init__(self, in_feat, out_feat):\n        super(Conv3x3Small, self).__init__()\n\n        self.conv1 = nn.Sequential(nn.Conv2d(in_feat, out_feat,\n                                             kernel_size=3,\n                                             stride=1,\n                                             padding=1),\n                                   nn.ELU(),\n                                   nn.Dropout(p=0.2))\n\n        self.conv2 = nn.Sequential(nn.Conv2d(out_feat, out_feat,\n                                             kernel_size=3,\n                                             stride=1,\n                                             padding=1),\n                                   nn.ELU())\n\n    def forward(self, inputs):\n        outputs = self.conv1(inputs)\n        outputs = self.conv2(outputs)\n        return outputs\n\n\nclass UpConcat(nn.Module):\n    def __init__(self, in_feat, out_feat):\n        super(UpConcat, self).__init__()\n\n        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n\n        # self.deconv = nn.ConvTranspose2d(in_feat, out_feat,\n        #                                  kernel_size=3,\n        #                                  stride=1,\n        #                                  dilation=1)\n\n        self.deconv = nn.ConvTranspose2d(in_feat,\n                                         out_feat,\n                                         kernel_size=2,\n                                         stride=2)\n\n    def forward(self, inputs, down_outputs):\n        # TODO: Upsampling required after deconv?\n        # outputs = self.up(inputs)\n        outputs = self.deconv(inputs)\n        out = torch.cat([down_outputs, outputs], 1)\n        return out\n\n\nclass UpSample(nn.Module):\n    def __init__(self, in_feat, out_feat):\n        super(UpSample, self).__init__()\n\n        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n\n        self.deconv = nn.ConvTranspose2d(in_feat,\n                                         out_feat,\n                                         kernel_size=2,\n                                         stride=2)\n\n    def forward(self, inputs, down_outputs):\n        # TODO: Upsampling required after deconv?\n        outputs = self.up(inputs)\n        # outputs = self.deconv(inputs)\n        out = torch.cat([outputs, down_outputs], 1)\n        return out\n"""
plot_ims.py,0,"b'import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.io as sio\n# outs = np.load(\'Numpy-batch-7-outs.npy\')\n# masks = np.load(\'Numpy-batch-7-masks.npy\')\n\nfinal_outs = []\nfinal_masks = []\nfinal_images = []\n\nloss = np.load(\'./OutMasks-UNet_Loss_bs=4_ep=1_lr=0.001.npy\')\n\nfor i in range(38):\n    outs = np.load(\n        \'./npy-files/out-files/UNetSmall-NonZero-unetsmall-batch-{}-outs.npy\'.format(i))\n    masks = np.load(\n        \'./npy-files/out-files/UNetSmall-NonZero-unetsmall--batch-{}-masks.npy\'.format(i))\n    images = np.load(\n        \'./npy-files/out-files/UNetSmall-NonZero-unetsmall--batch-{}-images.npy\'.format(i))\n\n    final_outs.append(outs)\n    final_masks.append(masks)\n    final_images.append(images)\nfinal_outs = np.asarray(final_outs)\nfinal_masks = np.asarray(final_masks)\nfinal_images = np.asarray(final_images)\n\nprint(final_images[0].shape)\nfor i in range(38):\n    print(final_images[i].shape)\n    plt.imshow(np.squeeze(final_images[i][49, :, :]), cmap=\'gray\')\n    plt.show()\n\n\n# print(final_outs.shape)\n# print(final_masks.shape)\n\n# sio.savemat(\'./mat-files/final_outputs.mat\', {\'data\': final_outs})\n# sio.savemat(\'./mat-files/final_masks.mat\', {\'data\': final_masks})\n# sio.savemat(\'./mat-files/final_images.mat\', {\'data\': final_images})\n# for i in range(len(outs)):\n#     plt1 = 255 * np.squeeze(outs[i, :, :, :]).astype(\'uint8\')\n#     plt2 = 255 * np.squeeze(masks[i, :, :, :]).astype(\'uint8\')\n#     print(plt1, plt2)\n#     plt.subplot(1, 2, 1)\n#     plt.imshow(plt1, cmap=\'gray\')\n#     plt.title(""UNet Out"")\n#     plt.subplot(1, 2, 2)\n#     plt.imshow(plt2, cmap=\'gray\')\n#     plt.title(""Mask"")\n'"
