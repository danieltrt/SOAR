file_path,api_count,code
apps/__init__.py,0,b''
apps/crop_img.py,0,"b'import os\nimport cv2\nimport numpy as np \n\nfrom pathlib import Path\nimport argparse\n\ndef get_bbox(msk):\n    rows = np.any(msk, axis=1)\n    cols = np.any(msk, axis=0)\n    rmin, rmax = np.where(rows)[0][[0,-1]]\n    cmin, cmax = np.where(cols)[0][[0,-1]]\n\n    return rmin, rmax, cmin, cmax\n\ndef process_img(img, msk, bbox=None):\n    if bbox is None:\n        bbox = get_bbox(msk > 100)\n    cx = (bbox[3] + bbox[2])//2\n    cy = (bbox[1] + bbox[0])//2\n\n    w = img.shape[1]\n    h = img.shape[0]\n    height = int(1.138*(bbox[1] - bbox[0]))\n    hh = height//2\n\n    # crop\n    dw = min(cx, w-cx, hh)\n    if cy-hh < 0:\n        img = cv2.copyMakeBorder(img,hh-cy,0,0,0,cv2.BORDER_CONSTANT,value=[0,0,0])    \n        msk = cv2.copyMakeBorder(msk,hh-cy,0,0,0,cv2.BORDER_CONSTANT,value=0) \n        cy = hh\n    if cy+hh > h:\n        img = cv2.copyMakeBorder(img,0,cy+hh-h,0,0,cv2.BORDER_CONSTANT,value=[0,0,0])    \n        msk = cv2.copyMakeBorder(msk,0,cy+hh-h,0,0,cv2.BORDER_CONSTANT,value=0)    \n    img = img[cy-hh:(cy+hh),cx-dw:cx+dw,:]\n    msk = msk[cy-hh:(cy+hh),cx-dw:cx+dw]\n    dw = img.shape[0] - img.shape[1]\n    if dw != 0:\n        img = cv2.copyMakeBorder(img,0,0,dw//2,dw//2,cv2.BORDER_CONSTANT,value=[0,0,0])    \n        msk = cv2.copyMakeBorder(msk,0,0,dw//2,dw//2,cv2.BORDER_CONSTANT,value=0)    \n    img = cv2.resize(img, (512, 512))\n    msk = cv2.resize(msk, (512, 512))\n\n    kernel = np.ones((3,3),np.uint8)\n    msk = cv2.erode((255*(msk > 100)).astype(np.uint8), kernel, iterations = 1)\n\n    return img, msk\n\ndef main():\n    \'\'\'\n    given foreground mask, this script crops and resizes an input image and mask for processing.\n    \'\'\'\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-i\', \'--input_image\', type=str, help=\'if the image has alpha channel, it will be used as mask\')\n    parser.add_argument(\'-m\', \'--input_mask\', type=str)\n    parser.add_argument(\'-o\', \'--out_path\', type=str, default=\'./sample_images\')\n    args = parser.parse_args()\n\n    img = cv2.imread(args.input_image, cv2.IMREAD_UNCHANGED)\n    if img.shape[2] == 4:\n        msk = img[:,:,3:]\n        img = img[:,:,:3]\n    else:\n        msk = cv2.imread(args.input_mask, cv2.IMREAD_GRAYSCALE)\n\n    img_new, msk_new = process_img(img, msk)\n\n    img_name = Path(args.input_image).stem\n\n    cv2.imwrite(os.path.join(args.out_path, img_name + \'.png\'), img_new)\n    cv2.imwrite(os.path.join(args.out_path, img_name + \'_mask.png\'), msk_new)\n\nif __name__ == ""__main__"":\n    main()'"
apps/eval.py,6,"b'import sys\nimport os\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), \'..\')))\nROOT_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\nimport time\nimport json\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom lib.options import BaseOptions\nfrom lib.mesh_util import *\nfrom lib.sample_util import *\nfrom lib.train_util import *\nfrom lib.model import *\n\nfrom PIL import Image\nimport torchvision.transforms as transforms\nimport glob\nimport tqdm\n\n# get options\nopt = BaseOptions().parse()\n\nclass Evaluator:\n    def __init__(self, opt, projection_mode=\'orthogonal\'):\n        self.opt = opt\n        self.load_size = self.opt.loadSize\n        self.to_tensor = transforms.Compose([\n            transforms.Resize(self.load_size),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        # set cuda\n        cuda = torch.device(\'cuda:%d\' % opt.gpu_id) if torch.cuda.is_available() else torch.device(\'cpu\')\n\n        # create net\n        netG = HGPIFuNet(opt, projection_mode).to(device=cuda)\n        print(\'Using Network: \', netG.name)\n\n        if opt.load_netG_checkpoint_path:\n            netG.load_state_dict(torch.load(opt.load_netG_checkpoint_path, map_location=cuda))\n\n        if opt.load_netC_checkpoint_path is not None:\n            print(\'loading for net C ...\', opt.load_netC_checkpoint_path)\n            netC = ResBlkPIFuNet(opt).to(device=cuda)\n            netC.load_state_dict(torch.load(opt.load_netC_checkpoint_path, map_location=cuda))\n        else:\n            netC = None\n\n        os.makedirs(opt.results_path, exist_ok=True)\n        os.makedirs(\'%s/%s\' % (opt.results_path, opt.name), exist_ok=True)\n\n        opt_log = os.path.join(opt.results_path, opt.name, \'opt.txt\')\n        with open(opt_log, \'w\') as outfile:\n            outfile.write(json.dumps(vars(opt), indent=2))\n\n        self.cuda = cuda\n        self.netG = netG\n        self.netC = netC\n\n    def load_image(self, image_path, mask_path):\n        # Name\n        img_name = os.path.splitext(os.path.basename(image_path))[0]\n        # Calib\n        B_MIN = np.array([-1, -1, -1])\n        B_MAX = np.array([1, 1, 1])\n        projection_matrix = np.identity(4)\n        projection_matrix[1, 1] = -1\n        calib = torch.Tensor(projection_matrix).float()\n        # Mask\n        mask = Image.open(mask_path).convert(\'L\')\n        mask = transforms.Resize(self.load_size)(mask)\n        mask = transforms.ToTensor()(mask).float()\n        # image\n        image = Image.open(image_path).convert(\'RGB\')\n        image = self.to_tensor(image)\n        image = mask.expand_as(image) * image\n        return {\n            \'name\': img_name,\n            \'img\': image.unsqueeze(0),\n            \'calib\': calib.unsqueeze(0),\n            \'mask\': mask.unsqueeze(0),\n            \'b_min\': B_MIN,\n            \'b_max\': B_MAX,\n        }\n\n    def eval(self, data, use_octree=False):\n        \'\'\'\n        Evaluate a data point\n        :param data: a dict containing at least [\'name\'], [\'image\'], [\'calib\'], [\'b_min\'] and [\'b_max\'] tensors.\n        :return:\n        \'\'\'\n        opt = self.opt\n        with torch.no_grad():\n            self.netG.eval()\n            if self.netC:\n                self.netC.eval()\n            save_path = \'%s/%s/result_%s.obj\' % (opt.results_path, opt.name, data[\'name\'])\n            if self.netC:\n                gen_mesh_color(opt, self.netG, self.netC, self.cuda, data, save_path, use_octree=use_octree)\n            else:\n                gen_mesh(opt, self.netG, self.cuda, data, save_path, use_octree=use_octree)\n\n\nif __name__ == \'__main__\':\n    evaluator = Evaluator(opt)\n\n    test_images = glob.glob(os.path.join(opt.test_folder_path, \'*\'))\n    test_images = [f for f in test_images if (\'png\' in f or \'jpg\' in f) and (not \'mask\' in f)]\n    test_masks = [f[:-4]+\'_mask.png\' for f in test_images]\n\n    print(""num; "", len(test_masks))\n\n    for image_path, mask_path in tqdm.tqdm(zip(test_images, test_masks)):\n        try:\n            print(image_path, mask_path)\n            data = evaluator.load_image(image_path, mask_path)\n            evaluator.eval(data, True)\n        except Exception as e:\n           print(""error:"", e.args)\n'"
apps/prt_util.py,0,"b""import os\nimport trimesh\nimport numpy as np\nimport math\nfrom scipy.special import sph_harm\nimport argparse\nfrom tqdm import tqdm\n\ndef factratio(N, D):\n    if N >= D:\n        prod = 1.0\n        for i in range(D+1, N+1):\n            prod *= i\n        return prod\n    else:\n        prod = 1.0\n        for i in range(N+1, D+1):\n            prod *= i\n        return 1.0 / prod\n\ndef KVal(M, L):\n    return math.sqrt(((2 * L + 1) / (4 * math.pi)) * (factratio(L - M, L + M)))\n\ndef AssociatedLegendre(M, L, x):\n    if M < 0 or M > L or np.max(np.abs(x)) > 1.0:\n        return np.zeros_like(x)\n    \n    pmm = np.ones_like(x)\n    if M > 0:\n        somx2 = np.sqrt((1.0 + x) * (1.0 - x))\n        fact = 1.0\n        for i in range(1, M+1):\n            pmm = -pmm * fact * somx2\n            fact = fact + 2\n    \n    if L == M:\n        return pmm\n    else:\n        pmmp1 = x * (2 * M + 1) * pmm\n        if L == M+1:\n            return pmmp1\n        else:\n            pll = np.zeros_like(x)\n            for i in range(M+2, L+1):\n                pll = (x * (2 * i - 1) * pmmp1 - (i + M - 1) * pmm) / (i - M)\n                pmm = pmmp1\n                pmmp1 = pll\n            return pll\n\ndef SphericalHarmonic(M, L, theta, phi):\n    if M > 0:\n        return math.sqrt(2.0) * KVal(M, L) * np.cos(M * phi) * AssociatedLegendre(M, L, np.cos(theta))\n    elif M < 0:\n        return math.sqrt(2.0) * KVal(-M, L) * np.sin(-M * phi) * AssociatedLegendre(-M, L, np.cos(theta))\n    else:\n        return KVal(0, L) * AssociatedLegendre(0, L, np.cos(theta))\n\ndef save_obj(mesh_path, verts):\n    file = open(mesh_path, 'w')    \n    for v in verts:\n        file.write('v %.4f %.4f %.4f\\n' % (v[0], v[1], v[2]))\n    file.close()\n\ndef sampleSphericalDirections(n):\n    xv = np.random.rand(n,n)\n    yv = np.random.rand(n,n)\n    theta = np.arccos(1-2 * xv)\n    phi = 2.0 * math.pi * yv\n\n    phi = phi.reshape(-1)\n    theta = theta.reshape(-1)\n\n    vx = -np.sin(theta) * np.cos(phi)\n    vy = -np.sin(theta) * np.sin(phi)\n    vz = np.cos(theta)\n    return np.stack([vx, vy, vz], 1), phi, theta\n\ndef getSHCoeffs(order, phi, theta):\n    shs = []\n    for n in range(0, order+1):\n        for m in range(-n,n+1):\n            s = SphericalHarmonic(m, n, theta, phi)\n            shs.append(s)\n    \n    return np.stack(shs, 1)\n\ndef computePRT(mesh_path, n, order):\n    mesh = trimesh.load(mesh_path, process=False)\n    vectors, phi, theta = sampleSphericalDirections(n)\n    SH = getSHCoeffs(order, phi, theta)\n\n    w = 4.0 * math.pi / (n*n)\n\n    origins = mesh.vertices\n    normals = mesh.vertex_normals\n    n_v = origins.shape[0]\n\n    origins = np.repeat(origins[:,None], n, axis=1).reshape(-1,3)\n    normals = np.repeat(normals[:,None], n, axis=1).reshape(-1,3)\n    PRT_all = None\n    for i in tqdm(range(n)):\n        SH = np.repeat(SH[None,(i*n):((i+1)*n)], n_v, axis=0).reshape(-1,SH.shape[1])\n        vectors = np.repeat(vectors[None,(i*n):((i+1)*n)], n_v, axis=0).reshape(-1,3)\n\n        dots = (vectors * normals).sum(1)\n        front = (dots > 0.0)\n\n        delta = 1e-3*min(mesh.bounding_box.extents)\n        hits = mesh.ray.intersects_any(origins + delta * normals, vectors)\n        nohits = np.logical_and(front, np.logical_not(hits))\n\n        PRT = (nohits.astype(np.float) * dots)[:,None] * SH\n        \n        if PRT_all is not None:\n            PRT_all += (PRT.reshape(-1, n, SH.shape[1]).sum(1))\n        else:\n            PRT_all = (PRT.reshape(-1, n, SH.shape[1]).sum(1))\n\n    PRT = w * PRT_all\n\n    # NOTE: trimesh sometimes break the original vertex order, but topology will not change.\n    # when loading PRT in other program, use the triangle list from trimesh.\n    return PRT, mesh.faces\n\ndef testPRT(dir_path, n=40):\n    if dir_path[-1] == '/':\n        dir_path = dir_path[:-1]\n    sub_name = dir_path.split('/')[-1][:-4]\n    obj_path = os.path.join(dir_path, sub_name + '_100k.obj')\n    os.makedirs(os.path.join(dir_path, 'bounce'), exist_ok=True)\n\n    PRT, F = computePRT(obj_path, n, 2)\n    np.savetxt(os.path.join(dir_path, 'bounce', 'bounce0.txt'), PRT, fmt='%.8f')\n    np.save(os.path.join(dir_path, 'bounce', 'face.npy'), F)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-i', '--input', type=str, default='/home/shunsuke/Downloads/rp_dennis_posed_004_OBJ')\n    parser.add_argument('-n', '--n_sample', type=int, default=40, help='squared root of number of sampling. the higher, the more accurate, but slower')\n    args = parser.parse_args()\n\n    testPRT(args.input)"""
apps/render_data.py,0,"b""#from data.config import raw_dataset, render_dataset, archive_dataset, model_list, zip_path\n\nfrom lib.renderer.camera import Camera\nimport numpy as np\nfrom lib.renderer.mesh import load_obj_mesh, compute_tangent, compute_normal, load_obj_mesh_mtl\nfrom lib.renderer.camera import Camera\nimport os\nimport cv2\nimport time\nimport math\nimport random\nimport pyexr\nimport argparse\nfrom tqdm import tqdm\n\n\ndef make_rotate(rx, ry, rz):\n    sinX = np.sin(rx)\n    sinY = np.sin(ry)\n    sinZ = np.sin(rz)\n\n    cosX = np.cos(rx)\n    cosY = np.cos(ry)\n    cosZ = np.cos(rz)\n\n    Rx = np.zeros((3,3))\n    Rx[0, 0] = 1.0\n    Rx[1, 1] = cosX\n    Rx[1, 2] = -sinX\n    Rx[2, 1] = sinX\n    Rx[2, 2] = cosX\n\n    Ry = np.zeros((3,3))\n    Ry[0, 0] = cosY\n    Ry[0, 2] = sinY\n    Ry[1, 1] = 1.0\n    Ry[2, 0] = -sinY\n    Ry[2, 2] = cosY\n\n    Rz = np.zeros((3,3))\n    Rz[0, 0] = cosZ\n    Rz[0, 1] = -sinZ\n    Rz[1, 0] = sinZ\n    Rz[1, 1] = cosZ\n    Rz[2, 2] = 1.0\n\n    R = np.matmul(np.matmul(Rz,Ry),Rx)\n    return R\n\ndef rotateSH(SH, R):\n    SHn = SH\n    \n    # 1st order\n    SHn[1] = R[1,1]*SH[1] - R[1,2]*SH[2] + R[1,0]*SH[3]\n    SHn[2] = -R[2,1]*SH[1] + R[2,2]*SH[2] - R[2,0]*SH[3]\n    SHn[3] = R[0,1]*SH[1] - R[0,2]*SH[2] + R[0,0]*SH[3]\n\n    # 2nd order\n    SHn[4:,0] = rotateBand2(SH[4:,0],R)\n    SHn[4:,1] = rotateBand2(SH[4:,1],R)\n    SHn[4:,2] = rotateBand2(SH[4:,2],R)\n\n    return SHn\n\ndef rotateBand2(x, R):\n    s_c3 = 0.94617469575\n    s_c4 = -0.31539156525\n    s_c5 = 0.54627421529\n\n    s_c_scale = 1.0/0.91529123286551084\n    s_c_scale_inv = 0.91529123286551084\n\n    s_rc2 = 1.5853309190550713*s_c_scale\n    s_c4_div_c3 = s_c4/s_c3\n    s_c4_div_c3_x2 = (s_c4/s_c3)*2.0\n\n    s_scale_dst2 = s_c3 * s_c_scale_inv\n    s_scale_dst4 = s_c5 * s_c_scale_inv\n\n    sh0 =  x[3] + x[4] + x[4] - x[1]\n    sh1 =  x[0] + s_rc2*x[2] +  x[3] + x[4]\n    sh2 =  x[0]\n    sh3 = -x[3]\n    sh4 = -x[1]\n    \n    r2x = R[0][0] + R[0][1]\n    r2y = R[1][0] + R[1][1]\n    r2z = R[2][0] + R[2][1]\n    \n    r3x = R[0][0] + R[0][2]\n    r3y = R[1][0] + R[1][2]\n    r3z = R[2][0] + R[2][2]\n    \n    r4x = R[0][1] + R[0][2]\n    r4y = R[1][1] + R[1][2]\n    r4z = R[2][1] + R[2][2]\n    \n    sh0_x = sh0 * R[0][0]\n    sh0_y = sh0 * R[1][0]\n    d0 = sh0_x * R[1][0]\n    d1 = sh0_y * R[2][0]\n    d2 = sh0 * (R[2][0] * R[2][0] + s_c4_div_c3)\n    d3 = sh0_x * R[2][0]\n    d4 = sh0_x * R[0][0] - sh0_y * R[1][0]\n    \n    sh1_x = sh1 * R[0][2]\n    sh1_y = sh1 * R[1][2]\n    d0 += sh1_x * R[1][2]\n    d1 += sh1_y * R[2][2]\n    d2 += sh1 * (R[2][2] * R[2][2] + s_c4_div_c3)\n    d3 += sh1_x * R[2][2]\n    d4 += sh1_x * R[0][2] - sh1_y * R[1][2]\n    \n    sh2_x = sh2 * r2x\n    sh2_y = sh2 * r2y\n    d0 += sh2_x * r2y\n    d1 += sh2_y * r2z\n    d2 += sh2 * (r2z * r2z + s_c4_div_c3_x2)\n    d3 += sh2_x * r2z\n    d4 += sh2_x * r2x - sh2_y * r2y\n    \n    sh3_x = sh3 * r3x\n    sh3_y = sh3 * r3y\n    d0 += sh3_x * r3y\n    d1 += sh3_y * r3z\n    d2 += sh3 * (r3z * r3z + s_c4_div_c3_x2)\n    d3 += sh3_x * r3z\n    d4 += sh3_x * r3x - sh3_y * r3y\n    \n    sh4_x = sh4 * r4x\n    sh4_y = sh4 * r4y\n    d0 += sh4_x * r4y\n    d1 += sh4_y * r4z\n    d2 += sh4 * (r4z * r4z + s_c4_div_c3_x2)\n    d3 += sh4_x * r4z\n    d4 += sh4_x * r4x - sh4_y * r4y\n\n    dst = x\n    dst[0] = d0\n    dst[1] = -d1\n    dst[2] = d2 * s_scale_dst2\n    dst[3] = -d3\n    dst[4] = d4 * s_scale_dst4\n\n    return dst\n\ndef render_prt_ortho(out_path, folder_name, subject_name, shs, rndr, rndr_uv, im_size, angl_step=4, n_light=1, pitch=[0]):\n    cam = Camera(width=im_size, height=im_size)\n    cam.ortho_ratio = 0.4 * (512 / im_size)\n    cam.near = -100\n    cam.far = 100\n    cam.sanity_check()\n\n    # set path for obj, prt\n    mesh_file = os.path.join(folder_name, subject_name + '_100k.obj')\n    if not os.path.exists(mesh_file):\n        print('ERROR: obj file does not exist!!', mesh_file)\n        return \n    prt_file = os.path.join(folder_name, 'bounce', 'bounce0.txt')\n    if not os.path.exists(prt_file):\n        print('ERROR: prt file does not exist!!!', prt_file)\n        return\n    face_prt_file = os.path.join(folder_name, 'bounce', 'face.npy')\n    if not os.path.exists(face_prt_file):\n        print('ERROR: face prt file does not exist!!!', prt_file)\n        return\n    text_file = os.path.join(folder_name, 'tex', subject_name + '_dif_2k.jpg')\n    if not os.path.exists(text_file):\n        print('ERROR: dif file does not exist!!', text_file)\n        return             \n\n    texture_image = cv2.imread(text_file)\n    texture_image = cv2.cvtColor(texture_image, cv2.COLOR_BGR2RGB)\n\n    vertices, faces, normals, faces_normals, textures, face_textures = load_obj_mesh(mesh_file, with_normal=True, with_texture=True)\n    vmin = vertices.min(0)\n    vmax = vertices.max(0)\n    up_axis = 1 if (vmax-vmin).argmax() == 1 else 2\n    \n    vmed = np.median(vertices, 0)\n    vmed[up_axis] = 0.5*(vmax[up_axis]+vmin[up_axis])\n    y_scale = 180/(vmax[up_axis] - vmin[up_axis])\n\n    rndr.set_norm_mat(y_scale, vmed)\n    rndr_uv.set_norm_mat(y_scale, vmed)\n\n    tan, bitan = compute_tangent(vertices, faces, normals, textures, face_textures)\n    prt = np.loadtxt(prt_file)\n    face_prt = np.load(face_prt_file)\n    rndr.set_mesh(vertices, faces, normals, faces_normals, textures, face_textures, prt, face_prt, tan, bitan)    \n    rndr.set_albedo(texture_image)\n\n    rndr_uv.set_mesh(vertices, faces, normals, faces_normals, textures, face_textures, prt, face_prt, tan, bitan)   \n    rndr_uv.set_albedo(texture_image)\n\n    os.makedirs(os.path.join(out_path, 'GEO', 'OBJ', subject_name),exist_ok=True)\n    os.makedirs(os.path.join(out_path, 'PARAM', subject_name),exist_ok=True)\n    os.makedirs(os.path.join(out_path, 'RENDER', subject_name),exist_ok=True)\n    os.makedirs(os.path.join(out_path, 'MASK', subject_name),exist_ok=True)\n    os.makedirs(os.path.join(out_path, 'UV_RENDER', subject_name),exist_ok=True)\n    os.makedirs(os.path.join(out_path, 'UV_MASK', subject_name),exist_ok=True)\n    os.makedirs(os.path.join(out_path, 'UV_POS', subject_name),exist_ok=True)\n    os.makedirs(os.path.join(out_path, 'UV_NORMAL', subject_name),exist_ok=True)\n\n    if not os.path.exists(os.path.join(out_path, 'val.txt')):\n        f = open(os.path.join(out_path, 'val.txt'), 'w')\n        f.close()\n\n    # copy obj file\n    cmd = 'cp %s %s' % (mesh_file, os.path.join(out_path, 'GEO', 'OBJ', subject_name))\n    print(cmd)\n    os.system(cmd)\n\n    for p in pitch:\n        for y in tqdm(range(0, 360, angl_step)):\n            R = np.matmul(make_rotate(math.radians(p), 0, 0), make_rotate(0, math.radians(y), 0))\n            if up_axis == 2:\n                R = np.matmul(R, make_rotate(math.radians(90),0,0))\n\n            rndr.rot_matrix = R\n            rndr_uv.rot_matrix = R\n            rndr.set_camera(cam)\n            rndr_uv.set_camera(cam)\n\n            for j in range(n_light):\n                sh_id = random.randint(0,shs.shape[0]-1)\n                sh = shs[sh_id]\n                sh_angle = 0.2*np.pi*(random.random()-0.5)\n                sh = rotateSH(sh, make_rotate(0, sh_angle, 0).T)\n\n                dic = {'sh': sh, 'ortho_ratio': cam.ortho_ratio, 'scale': y_scale, 'center': vmed, 'R': R}\n                \n                rndr.set_sh(sh)        \n                rndr.analytic = False\n                rndr.use_inverse_depth = False\n                rndr.display()\n\n                out_all_f = rndr.get_color(0)\n                out_mask = out_all_f[:,:,3]\n                out_all_f = cv2.cvtColor(out_all_f, cv2.COLOR_RGBA2BGR)\n\n                np.save(os.path.join(out_path, 'PARAM', subject_name, '%d_%d_%02d.npy'%(y,p,j)),dic)\n                cv2.imwrite(os.path.join(out_path, 'RENDER', subject_name, '%d_%d_%02d.jpg'%(y,p,j)),255.0*out_all_f)\n                cv2.imwrite(os.path.join(out_path, 'MASK', subject_name, '%d_%d_%02d.png'%(y,p,j)),255.0*out_mask)\n\n                rndr_uv.set_sh(sh)\n                rndr_uv.analytic = False\n                rndr_uv.use_inverse_depth = False\n                rndr_uv.display()\n\n                uv_color = rndr_uv.get_color(0)\n                uv_color = cv2.cvtColor(uv_color, cv2.COLOR_RGBA2BGR)\n                cv2.imwrite(os.path.join(out_path, 'UV_RENDER', subject_name, '%d_%d_%02d.jpg'%(y,p,j)),255.0*uv_color)\n\n                if y == 0 and j == 0 and p == pitch[0]:\n                    uv_pos = rndr_uv.get_color(1)\n                    uv_mask = uv_pos[:,:,3]\n                    cv2.imwrite(os.path.join(out_path, 'UV_MASK', subject_name, '00.png'),255.0*uv_mask)\n\n                    data = {'default': uv_pos[:,:,:3]} # default is a reserved name\n                    pyexr.write(os.path.join(out_path, 'UV_POS', subject_name, '00.exr'), data) \n\n                    uv_nml = rndr_uv.get_color(2)\n                    uv_nml = cv2.cvtColor(uv_nml, cv2.COLOR_RGBA2BGR)\n                    cv2.imwrite(os.path.join(out_path, 'UV_NORMAL', subject_name, '00.png'),255.0*uv_nml)\n\n\nif __name__ == '__main__':\n    shs = np.load('./env_sh.npy')\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-i', '--input', type=str, default='/home/shunsuke/Downloads/rp_dennis_posed_004_OBJ')\n    parser.add_argument('-o', '--out_dir', type=str, default='/home/shunsuke/Documents/hf_human')\n    parser.add_argument('-m', '--ms_rate', type=int, default=1, help='higher ms rate results in less aliased output. MESA renderer only supports ms_rate=1.')\n    parser.add_argument('-e', '--egl',  action='store_true', help='egl rendering option. use this when rendering with headless server with NVIDIA GPU')\n    parser.add_argument('-s', '--size',  type=int, default=512, help='rendering image size')\n    args = parser.parse_args()\n\n    # NOTE: GL context has to be created before any other OpenGL function loads.\n    from lib.renderer.gl.init_gl import initialize_GL_context\n    initialize_GL_context(width=args.size, height=args.size, egl=args.egl)\n\n    from lib.renderer.gl.prt_render import PRTRender\n    rndr = PRTRender(width=args.size, height=args.size, ms_rate=args.ms_rate, egl=args.egl)\n    rndr_uv = PRTRender(width=args.size, height=args.size, uv_mode=True, egl=args.egl)\n\n    if args.input[-1] == '/':\n        args.input = args.input[:-1]\n    subject_name = args.input.split('/')[-1][:-4]\n    render_prt_ortho(args.out_dir, args.input, subject_name, shs, rndr, rndr_uv, args.size, 1, 1, pitch=[0])"""
apps/train_color.py,12,"b""import sys\nimport os\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\nROOT_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\nimport time\nimport json\nimport numpy as np\nimport cv2\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom lib.options import BaseOptions\nfrom lib.mesh_util import *\nfrom lib.sample_util import *\nfrom lib.train_util import *\nfrom lib.data import *\nfrom lib.model import *\nfrom lib.geometry import index\n\n# get options\nopt = BaseOptions().parse()\n\ndef train_color(opt):\n    # set cuda\n    cuda = torch.device('cuda:%d' % opt.gpu_id)\n\n    train_dataset = TrainDataset(opt, phase='train')\n    test_dataset = TrainDataset(opt, phase='test')\n\n    projection_mode = train_dataset.projection_mode\n\n    # create data loader\n    train_data_loader = DataLoader(train_dataset,\n                                   batch_size=opt.batch_size, shuffle=not opt.serial_batches,\n                                   num_workers=opt.num_threads, pin_memory=opt.pin_memory)\n\n    print('train data size: ', len(train_data_loader))\n\n    # NOTE: batch size should be 1 and use all the points for evaluation\n    test_data_loader = DataLoader(test_dataset,\n                                  batch_size=1, shuffle=False,\n                                  num_workers=opt.num_threads, pin_memory=opt.pin_memory)\n    print('test data size: ', len(test_data_loader))\n\n    # create net\n    netG = HGPIFuNet(opt, projection_mode).to(device=cuda)\n\n    lr = opt.learning_rate\n\n    # Always use resnet for color regression\n    netC = ResBlkPIFuNet(opt).to(device=cuda)\n    optimizerC = torch.optim.Adam(netC.parameters(), lr=opt.learning_rate)\n\n    def set_train():\n        netG.eval()\n        netC.train()\n\n    def set_eval():\n        netG.eval()\n        netC.eval()\n\n    print('Using NetworkG: ', netG.name, 'networkC: ', netC.name)\n\n    # load checkpoints\n    if opt.load_netG_checkpoint_path is not None:\n        print('loading for net G ...', opt.load_netG_checkpoint_path)\n        netG.load_state_dict(torch.load(opt.load_netG_checkpoint_path, map_location=cuda))\n    else:\n        model_path_G = '%s/%s/netG_latest' % (opt.checkpoints_path, opt.name)\n        print('loading for net G ...', model_path_G)\n        netG.load_state_dict(torch.load(model_path_G, map_location=cuda))\n\n    if opt.load_netC_checkpoint_path is not None:\n        print('loading for net C ...', opt.load_netC_checkpoint_path)\n        netC.load_state_dict(torch.load(opt.load_netC_checkpoint_path, map_location=cuda))\n\n    if opt.continue_train:\n        if opt.resume_epoch < 0:\n            model_path_C = '%s/%s/netC_latest' % (opt.checkpoints_path, opt.name)\n        else:\n            model_path_C = '%s/%s/netC_epoch_%d' % (opt.checkpoints_path, opt.name, opt.resume_epoch)\n\n        print('Resuming from ', model_path_C)\n        netC.load_state_dict(torch.load(model_path_C, map_location=cuda))\n\n    os.makedirs(opt.checkpoints_path, exist_ok=True)\n    os.makedirs(opt.results_path, exist_ok=True)\n    os.makedirs('%s/%s' % (opt.checkpoints_path, opt.name), exist_ok=True)\n    os.makedirs('%s/%s' % (opt.results_path, opt.name), exist_ok=True)\n\n    opt_log = os.path.join(opt.results_path, opt.name, 'opt.txt')\n    with open(opt_log, 'w') as outfile:\n        outfile.write(json.dumps(vars(opt), indent=2))\n\n    # training\n    start_epoch = 0 if not opt.continue_train else max(opt.resume_epoch,0)\n    for epoch in range(start_epoch, opt.num_epoch):\n        epoch_start_time = time.time()\n\n        set_train()\n        iter_data_time = time.time()\n        for train_idx, train_data in enumerate(train_data_loader):\n            iter_start_time = time.time()\n            # retrieve the data\n            image_tensor = train_data['img'].to(device=cuda)\n            calib_tensor = train_data['calib'].to(device=cuda)\n            color_sample_tensor = train_data['color_samples'].to(device=cuda)\n\n            image_tensor, calib_tensor = reshape_multiview_tensors(image_tensor, calib_tensor)\n\n            if opt.num_views > 1:\n                color_sample_tensor = reshape_sample_tensor(color_sample_tensor, opt.num_views)\n\n            rgb_tensor = train_data['rgbs'].to(device=cuda)\n\n            with torch.no_grad():\n                netG.filter(image_tensor)\n            resC, error = netC.forward(image_tensor, netG.get_im_feat(), color_sample_tensor, calib_tensor, labels=rgb_tensor)\n\n            optimizerC.zero_grad()\n            error.backward()\n            optimizerC.step()\n\n            iter_net_time = time.time()\n            eta = ((iter_net_time - epoch_start_time) / (train_idx + 1)) * len(train_data_loader) - (\n                    iter_net_time - epoch_start_time)\n\n            if train_idx % opt.freq_plot == 0:\n                print(\n                    'Name: {0} | Epoch: {1} | {2}/{3} | Err: {4:.06f} | LR: {5:.06f} | dataT: {6:.05f} | netT: {7:.05f} | ETA: {8:02d}:{9:02d}'.format(\n                        opt.name, epoch, train_idx, len(train_data_loader),\n                        error.item(),\n                        lr,\n                        iter_start_time - iter_data_time,\n                        iter_net_time - iter_start_time, int(eta // 60),\n                        int(eta - 60 * (eta // 60))))\n\n            if train_idx % opt.freq_save == 0 and train_idx != 0:\n                torch.save(netC.state_dict(), '%s/%s/netC_latest' % (opt.checkpoints_path, opt.name))\n                torch.save(netC.state_dict(), '%s/%s/netC_epoch_%d' % (opt.checkpoints_path, opt.name, epoch))\n\n            if train_idx % opt.freq_save_ply == 0:\n                save_path = '%s/%s/pred_col.ply' % (opt.results_path, opt.name)\n                rgb = resC[0].transpose(0, 1).cpu() * 0.5 + 0.5\n                points = color_sample_tensor[0].transpose(0, 1).cpu()\n                save_samples_rgb(save_path, points.detach().numpy(), rgb.detach().numpy())\n\n            iter_data_time = time.time()\n\n        #### test\n        with torch.no_grad():\n            set_eval()\n\n            if not opt.no_num_eval:\n                test_losses = {}\n                print('calc error (test) ...')\n                test_color_error = calc_error_color(opt, netG, netC, cuda, test_dataset, 100)\n                print('eval test | color error:', test_color_error)\n                test_losses['test_color'] = test_color_error\n\n                print('calc error (train) ...')\n                train_dataset.is_train = False\n                train_color_error = calc_error_color(opt, netG, netC, cuda, train_dataset, 100)\n                train_dataset.is_train = True\n                print('eval train | color error:', train_color_error)\n                test_losses['train_color'] = train_color_error\n\n            if not opt.no_gen_mesh:\n                print('generate mesh (test) ...')\n                for gen_idx in tqdm(range(opt.num_gen_mesh_test)):\n                    test_data = random.choice(test_dataset)\n                    save_path = '%s/%s/test_eval_epoch%d_%s.obj' % (\n                        opt.results_path, opt.name, epoch, test_data['name'])\n                    gen_mesh_color(opt, netG, netC, cuda, test_data, save_path)\n\n                print('generate mesh (train) ...')\n                train_dataset.is_train = False\n                for gen_idx in tqdm(range(opt.num_gen_mesh_test)):\n                    train_data = random.choice(train_dataset)\n                    save_path = '%s/%s/train_eval_epoch%d_%s.obj' % (\n                        opt.results_path, opt.name, epoch, train_data['name'])\n                    gen_mesh_color(opt, netG, netC, cuda, train_data, save_path)\n                train_dataset.is_train = True\n\nif __name__ == '__main__':\n    train_color(opt)"""
apps/train_shape.py,8,"b""import sys\nimport os\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\nROOT_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\nimport time\nimport json\nimport numpy as np\nimport cv2\nimport random\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nfrom lib.options import BaseOptions\nfrom lib.mesh_util import *\nfrom lib.sample_util import *\nfrom lib.train_util import *\nfrom lib.data import *\nfrom lib.model import *\nfrom lib.geometry import index\n\n# get options\nopt = BaseOptions().parse()\n\ndef train(opt):\n    # set cuda\n    cuda = torch.device('cuda:%d' % opt.gpu_id)\n\n    train_dataset = TrainDataset(opt, phase='train')\n    test_dataset = TrainDataset(opt, phase='test')\n\n    projection_mode = train_dataset.projection_mode\n\n    # create data loader\n    train_data_loader = DataLoader(train_dataset,\n                                   batch_size=opt.batch_size, shuffle=not opt.serial_batches,\n                                   num_workers=opt.num_threads, pin_memory=opt.pin_memory)\n\n    print('train data size: ', len(train_data_loader))\n\n    # NOTE: batch size should be 1 and use all the points for evaluation\n    test_data_loader = DataLoader(test_dataset,\n                                  batch_size=1, shuffle=False,\n                                  num_workers=opt.num_threads, pin_memory=opt.pin_memory)\n    print('test data size: ', len(test_data_loader))\n\n    # create net\n    netG = HGPIFuNet(opt, projection_mode).to(device=cuda)\n    optimizerG = torch.optim.RMSprop(netG.parameters(), lr=opt.learning_rate, momentum=0, weight_decay=0)\n    lr = opt.learning_rate\n    print('Using Network: ', netG.name)\n    \n    def set_train():\n        netG.train()\n\n    def set_eval():\n        netG.eval()\n\n    # load checkpoints\n    if opt.load_netG_checkpoint_path is not None:\n        print('loading for net G ...', opt.load_netG_checkpoint_path)\n        netG.load_state_dict(torch.load(opt.load_netG_checkpoint_path, map_location=cuda))\n\n    if opt.continue_train:\n        if opt.resume_epoch < 0:\n            model_path = '%s/%s/netG_latest' % (opt.checkpoints_path, opt.name)\n        else:\n            model_path = '%s/%s/netG_epoch_%d' % (opt.checkpoints_path, opt.name, opt.resume_epoch)\n        print('Resuming from ', model_path)\n        netG.load_state_dict(torch.load(model_path, map_location=cuda))\n\n    os.makedirs(opt.checkpoints_path, exist_ok=True)\n    os.makedirs(opt.results_path, exist_ok=True)\n    os.makedirs('%s/%s' % (opt.checkpoints_path, opt.name), exist_ok=True)\n    os.makedirs('%s/%s' % (opt.results_path, opt.name), exist_ok=True)\n\n    opt_log = os.path.join(opt.results_path, opt.name, 'opt.txt')\n    with open(opt_log, 'w') as outfile:\n        outfile.write(json.dumps(vars(opt), indent=2))\n\n    # training\n    start_epoch = 0 if not opt.continue_train else max(opt.resume_epoch,0)\n    for epoch in range(start_epoch, opt.num_epoch):\n        epoch_start_time = time.time()\n\n        set_train()\n        iter_data_time = time.time()\n        for train_idx, train_data in enumerate(train_data_loader):\n            iter_start_time = time.time()\n\n            # retrieve the data\n            image_tensor = train_data['img'].to(device=cuda)\n            calib_tensor = train_data['calib'].to(device=cuda)\n            sample_tensor = train_data['samples'].to(device=cuda)\n\n            image_tensor, calib_tensor = reshape_multiview_tensors(image_tensor, calib_tensor)\n\n            if opt.num_views > 1:\n                sample_tensor = reshape_sample_tensor(sample_tensor, opt.num_views)\n\n            label_tensor = train_data['labels'].to(device=cuda)\n\n            res, error = netG.forward(image_tensor, sample_tensor, calib_tensor, labels=label_tensor)\n\n            optimizerG.zero_grad()\n            error.backward()\n            optimizerG.step()\n\n            iter_net_time = time.time()\n            eta = ((iter_net_time - epoch_start_time) / (train_idx + 1)) * len(train_data_loader) - (\n                    iter_net_time - epoch_start_time)\n\n            if train_idx % opt.freq_plot == 0:\n                print(\n                    'Name: {0} | Epoch: {1} | {2}/{3} | Err: {4:.06f} | LR: {5:.06f} | Sigma: {6:.02f} | dataT: {7:.05f} | netT: {8:.05f} | ETA: {9:02d}:{10:02d}'.format(\n                        opt.name, epoch, train_idx, len(train_data_loader), error.item(), lr, opt.sigma,\n                                                                            iter_start_time - iter_data_time,\n                                                                            iter_net_time - iter_start_time, int(eta // 60),\n                        int(eta - 60 * (eta // 60))))\n\n            if train_idx % opt.freq_save == 0 and train_idx != 0:\n                torch.save(netG.state_dict(), '%s/%s/netG_latest' % (opt.checkpoints_path, opt.name))\n                torch.save(netG.state_dict(), '%s/%s/netG_epoch_%d' % (opt.checkpoints_path, opt.name, epoch))\n\n            if train_idx % opt.freq_save_ply == 0:\n                save_path = '%s/%s/pred.ply' % (opt.results_path, opt.name)\n                r = res[0].cpu()\n                points = sample_tensor[0].transpose(0, 1).cpu()\n                save_samples_truncted_prob(save_path, points.detach().numpy(), r.detach().numpy())\n\n            iter_data_time = time.time()\n\n        # update learning rate\n        lr = adjust_learning_rate(optimizerG, epoch, lr, opt.schedule, opt.gamma)\n\n        #### test\n        with torch.no_grad():\n            set_eval()\n\n            if not opt.no_num_eval:\n                test_losses = {}\n                print('calc error (test) ...')\n                test_errors = calc_error(opt, netG, cuda, test_dataset, 100)\n                print('eval test MSE: {0:06f} IOU: {1:06f} prec: {2:06f} recall: {3:06f}'.format(*test_errors))\n                MSE, IOU, prec, recall = test_errors\n                test_losses['MSE(test)'] = MSE\n                test_losses['IOU(test)'] = IOU\n                test_losses['prec(test)'] = prec\n                test_losses['recall(test)'] = recall\n\n                print('calc error (train) ...')\n                train_dataset.is_train = False\n                train_errors = calc_error(opt, netG, cuda, train_dataset, 100)\n                train_dataset.is_train = True\n                print('eval train MSE: {0:06f} IOU: {1:06f} prec: {2:06f} recall: {3:06f}'.format(*train_errors))\n                MSE, IOU, prec, recall = train_errors\n                test_losses['MSE(train)'] = MSE\n                test_losses['IOU(train)'] = IOU\n                test_losses['prec(train)'] = prec\n                test_losses['recall(train)'] = recall\n\n            if not opt.no_gen_mesh:\n                print('generate mesh (test) ...')\n                for gen_idx in tqdm(range(opt.num_gen_mesh_test)):\n                    test_data = random.choice(test_dataset)\n                    save_path = '%s/%s/test_eval_epoch%d_%s.obj' % (\n                        opt.results_path, opt.name, epoch, test_data['name'])\n                    gen_mesh(opt, netG, cuda, test_data, save_path)\n\n                print('generate mesh (train) ...')\n                train_dataset.is_train = False\n                for gen_idx in tqdm(range(opt.num_gen_mesh_test)):\n                    train_data = random.choice(train_dataset)\n                    save_path = '%s/%s/train_eval_epoch%d_%s.obj' % (\n                        opt.results_path, opt.name, epoch, train_data['name'])\n                    gen_mesh(opt, netG, cuda, train_data, save_path)\n                train_dataset.is_train = True\n\n\nif __name__ == '__main__':\n    train(opt)"""
lib/__init__.py,0,b''
lib/colab_util.py,6,"b'import io\nimport os\nimport torch\nfrom skimage.io import imread\nimport numpy as np\nimport cv2\nfrom tqdm import tqdm_notebook as tqdm\nimport base64\nfrom IPython.display import HTML\n\n# Util function for loading meshes\nfrom pytorch3d.io import load_objs_as_meshes\n\nfrom IPython.display import HTML\nfrom base64 import b64encode\n\n# Data structures and functions for rendering\nfrom pytorch3d.structures import Meshes, Textures\nfrom pytorch3d.renderer import (\n    look_at_view_transform,\n    OpenGLOrthographicCameras, \n    PointLights, \n    DirectionalLights, \n    Materials, \n    RasterizationSettings, \n    MeshRenderer, \n    MeshRasterizer,  \n    TexturedSoftPhongShader,\n    HardPhongShader\n)\n\ndef set_renderer():\n    # Setup\n    device = torch.device(""cuda:0"")\n    torch.cuda.set_device(device)\n\n    # Initialize an OpenGL perspective camera.\n    R, T = look_at_view_transform(2.0, 0, 180) \n    cameras = OpenGLOrthographicCameras(device=device, R=R, T=T)\n\n    raster_settings = RasterizationSettings(\n        image_size=512, \n        blur_radius=0.0, \n        faces_per_pixel=1, \n        bin_size = None, \n        max_faces_per_bin = None\n    )\n\n    lights = PointLights(device=device, location=((2.0, 2.0, 2.0),))\n\n    renderer = MeshRenderer(\n        rasterizer=MeshRasterizer(\n            cameras=cameras, \n            raster_settings=raster_settings\n        ),\n        shader=HardPhongShader(\n            device=device, \n            cameras=cameras,\n            lights=lights\n        )\n    )\n    return renderer\n\ndef get_verts_rgb_colors(obj_path):\n  rgb_colors = []\n\n  f = open(obj_path)\n  lines = f.readlines()\n  for line in lines:\n    ls = line.split(\' \')\n    if len(ls) == 7:\n      rgb_colors.append(ls[-3:])\n\n  return np.array(rgb_colors, dtype=\'float32\')[None, :, :]\n\ndef generate_video_from_obj(obj_path, video_path, renderer):\n    # Setup\n    device = torch.device(""cuda:0"")\n    torch.cuda.set_device(device)\n\n    # Load obj file\n    verts_rgb_colors = get_verts_rgb_colors(obj_path)\n    verts_rgb_colors = torch.from_numpy(verts_rgb_colors).to(device)\n    textures = Textures(verts_rgb=verts_rgb_colors)\n    wo_textures = Textures(verts_rgb=torch.ones_like(verts_rgb_colors)*0.75)\n\n    # Load obj\n    mesh = load_objs_as_meshes([obj_path], device=device)\n\n    # Set mesh\n    vers = mesh._verts_list\n    faces = mesh._faces_list\n    mesh_w_tex = Meshes(vers, faces, textures)\n    mesh_wo_tex = Meshes(vers, faces, wo_textures)\n\n    # create VideoWriter\n    fourcc = cv2. VideoWriter_fourcc(*\'MP4V\')\n    out = cv2.VideoWriter(video_path, fourcc, 20.0, (1024,512))\n\n    for i in tqdm(range(90)):\n        R, T = look_at_view_transform(1.8, 0, i*4, device=device)\n        images_w_tex = renderer(mesh_w_tex, R=R, T=T)\n        images_w_tex = np.clip(images_w_tex[0, ..., :3].cpu().numpy(), 0.0, 1.0)[:, :, ::-1] * 255\n        images_wo_tex = renderer(mesh_wo_tex, R=R, T=T)\n        images_wo_tex = np.clip(images_wo_tex[0, ..., :3].cpu().numpy(), 0.0, 1.0)[:, :, ::-1] * 255\n        image = np.concatenate([images_w_tex, images_wo_tex], axis=1)\n        out.write(image.astype(\'uint8\'))\n    out.release()\n\ndef video(path):\n    mp4 = open(path,\'rb\').read()\n    data_url = ""data:video/mp4;base64,"" + b64encode(mp4).decode()\n    return HTML(\'<video width=500 controls loop> <source src=""%s"" type=""video/mp4""></video>\' % data_url)\n'"
lib/ext_transform.py,3,"b'import random\n\nimport numpy as np\nfrom skimage.filters import gaussian\nimport torch\nfrom PIL import Image, ImageFilter\n\n\nclass RandomVerticalFlip(object):\n    def __call__(self, img):\n        if random.random() < 0.5:\n            return img.transpose(Image.FLIP_TOP_BOTTOM)\n        return img\n\n\nclass DeNormalize(object):\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n        return tensor\n\n\nclass MaskToTensor(object):\n    def __call__(self, img):\n        return torch.from_numpy(np.array(img, dtype=np.int32)).long()\n\n\nclass FreeScale(object):\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = tuple(reversed(size))  # size: (h, w)\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        return img.resize(self.size, self.interpolation)\n\n\nclass FlipChannels(object):\n    def __call__(self, img):\n        img = np.array(img)[:, :, ::-1]\n        return Image.fromarray(img.astype(np.uint8))\n\n\nclass RandomGaussianBlur(object):\n    def __call__(self, img):\n        sigma = 0.15 + random.random() * 1.15\n        blurred_img = gaussian(np.array(img), sigma=sigma, multichannel=True)\n        blurred_img *= 255\n        return Image.fromarray(blurred_img.astype(np.uint8))\n\n# Lighting data augmentation take from here - https://github.com/eladhoffer/convNet.pytorch/blob/master/preprocess.py\n\n\nclass Lighting(object):\n    """"""Lighting noise(AlexNet - style PCA - based noise)""""""\n\n    def __init__(self, alphastd, \n                 eigval=(0.2175, 0.0188, 0.0045), \n                 eigvec=((-0.5675, 0.7192, 0.4009),\n                         (-0.5808, -0.0045, -0.8140),\n                         (-0.5836, -0.6948, 0.4203))):\n        self.alphastd = alphastd\n        self.eigval = torch.Tensor(eigval)\n        self.eigvec = torch.Tensor(eigvec)\n\n    def __call__(self, img):\n        if self.alphastd == 0:\n            return img\n\n        alpha = img.new().resize_(3).normal_(0, self.alphastd)\n        rgb = self.eigvec.type_as(img).clone()\\\n            .mul(alpha.view(1, 3).expand(3, 3))\\\n            .mul(self.eigval.view(1, 3).expand(3, 3))\\\n            .sum(1).squeeze()\n        return img.add(rgb.view(3, 1, 1).expand_as(img))\n'"
lib/geometry.py,6,"b""import torch\n\n\ndef index(feat, uv):\n    '''\n\n    :param feat: [B, C, H, W] image features\n    :param uv: [B, 2, N] uv coordinates in the image plane, range [0, 1]\n    :return: [B, C, N] image features at the uv coordinates\n    '''\n    uv = uv.transpose(1, 2)  # [B, N, 2]\n    uv = uv.unsqueeze(2)  # [B, N, 1, 2]\n    # NOTE: for newer PyTorch, it seems that training results are degraded due to implementation diff in F.grid_sample\n    # for old versions, simply remove the aligned_corners argument.\n    samples = torch.nn.functional.grid_sample(feat, uv, align_corners=True)  # [B, C, N, 1]\n    return samples[:, :, :, 0]  # [B, C, N]\n\n\ndef orthogonal(points, calibrations, transforms=None):\n    '''\n    Compute the orthogonal projections of 3D points into the image plane by given projection matrix\n    :param points: [B, 3, N] Tensor of 3D points\n    :param calibrations: [B, 3, 4] Tensor of projection matrix\n    :param transforms: [B, 2, 3] Tensor of image transform matrix\n    :return: xyz: [B, 3, N] Tensor of xyz coordinates in the image plane\n    '''\n    rot = calibrations[:, :3, :3]\n    trans = calibrations[:, :3, 3:4]\n    pts = torch.baddbmm(trans, rot, points)  # [B, 3, N]\n    if transforms is not None:\n        scale = transforms[:2, :2]\n        shift = transforms[:2, 2:3]\n        pts[:, :2, :] = torch.baddbmm(shift, scale, pts[:, :2, :])\n    return pts\n\n\ndef perspective(points, calibrations, transforms=None):\n    '''\n    Compute the perspective projections of 3D points into the image plane by given projection matrix\n    :param points: [Bx3xN] Tensor of 3D points\n    :param calibrations: [Bx3x4] Tensor of projection matrix\n    :param transforms: [Bx2x3] Tensor of image transform matrix\n    :return: xy: [Bx2xN] Tensor of xy coordinates in the image plane\n    '''\n    rot = calibrations[:, :3, :3]\n    trans = calibrations[:, :3, 3:4]\n    homo = torch.baddbmm(trans, rot, points)  # [B, 3, N]\n    xy = homo[:, :2, :] / homo[:, 2:3, :]\n    if transforms is not None:\n        scale = transforms[:2, :2]\n        shift = transforms[:2, 2:3]\n        xy = torch.baddbmm(shift, scale, xy)\n\n    xyz = torch.cat([xy, homo[:, 2:3, :]], 1)\n    return xyz\n"""
lib/mesh_util.py,1,"b""from skimage import measure\nimport numpy as np\nimport torch\nfrom .sdf import create_grid, eval_grid_octree, eval_grid\nfrom skimage import measure\n\n\ndef reconstruction(net, cuda, calib_tensor,\n                   resolution, b_min, b_max,\n                   use_octree=False, num_samples=10000, transform=None):\n    '''\n    Reconstruct meshes from sdf predicted by the network.\n    :param net: a BasePixImpNet object. call image filter beforehead.\n    :param cuda: cuda device\n    :param calib_tensor: calibration tensor\n    :param resolution: resolution of the grid cell\n    :param b_min: bounding box corner [x_min, y_min, z_min]\n    :param b_max: bounding box corner [x_max, y_max, z_max]\n    :param use_octree: whether to use octree acceleration\n    :param num_samples: how many points to query each gpu iteration\n    :return: marching cubes results.\n    '''\n    # First we create a grid by resolution\n    # and transforming matrix for grid coordinates to real world xyz\n    coords, mat = create_grid(resolution, resolution, resolution,\n                              b_min, b_max, transform=transform)\n\n    # Then we define the lambda function for cell evaluation\n    def eval_func(points):\n        points = np.expand_dims(points, axis=0)\n        points = np.repeat(points, net.num_views, axis=0)\n        samples = torch.from_numpy(points).to(device=cuda).float()\n        net.query(samples, calib_tensor)\n        pred = net.get_preds()[0][0]\n        return pred.detach().cpu().numpy()\n\n    # Then we evaluate the grid\n    if use_octree:\n        sdf = eval_grid_octree(coords, eval_func, num_samples=num_samples)\n    else:\n        sdf = eval_grid(coords, eval_func, num_samples=num_samples)\n\n    # Finally we do marching cubes\n    try:\n        verts, faces, normals, values = measure.marching_cubes_lewiner(sdf, 0.5)\n        # transform verts into world coordinate system\n        verts = np.matmul(mat[:3, :3], verts.T) + mat[:3, 3:4]\n        verts = verts.T\n        return verts, faces, normals, values\n    except:\n        print('error cannot marching cubes')\n        return -1\n\n\ndef save_obj_mesh(mesh_path, verts, faces):\n    file = open(mesh_path, 'w')\n\n    for v in verts:\n        file.write('v %.4f %.4f %.4f\\n' % (v[0], v[1], v[2]))\n    for f in faces:\n        f_plus = f + 1\n        file.write('f %d %d %d\\n' % (f_plus[0], f_plus[2], f_plus[1]))\n    file.close()\n\n\ndef save_obj_mesh_with_color(mesh_path, verts, faces, colors):\n    file = open(mesh_path, 'w')\n\n    for idx, v in enumerate(verts):\n        c = colors[idx]\n        file.write('v %.4f %.4f %.4f %.4f %.4f %.4f\\n' % (v[0], v[1], v[2], c[0], c[1], c[2]))\n    for f in faces:\n        f_plus = f + 1\n        file.write('f %d %d %d\\n' % (f_plus[0], f_plus[2], f_plus[1]))\n    file.close()\n\n\ndef save_obj_mesh_with_uv(mesh_path, verts, faces, uvs):\n    file = open(mesh_path, 'w')\n\n    for idx, v in enumerate(verts):\n        vt = uvs[idx]\n        file.write('v %.4f %.4f %.4f\\n' % (v[0], v[1], v[2]))\n        file.write('vt %.4f %.4f\\n' % (vt[0], vt[1]))\n\n    for f in faces:\n        f_plus = f + 1\n        file.write('f %d/%d %d/%d %d/%d\\n' % (f_plus[0], f_plus[0],\n                                              f_plus[2], f_plus[2],\n                                              f_plus[1], f_plus[1]))\n    file.close()\n"""
lib/net_util.py,15,"b'import torch\nfrom torch.nn import init\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport functools\n\nimport numpy as np\nfrom .mesh_util import *\nfrom .sample_util import *\nfrom .geometry import index\nimport cv2\nfrom PIL import Image\nfrom tqdm import tqdm\n\n\ndef reshape_multiview_tensors(image_tensor, calib_tensor):\n    # Careful here! Because we put single view and multiview together,\n    # the returned tensor.shape is 5-dim: [B, num_views, C, W, H]\n    # So we need to convert it back to 4-dim [B*num_views, C, W, H]\n    # Don\'t worry classifier will handle multi-view cases\n    image_tensor = image_tensor.view(\n        image_tensor.shape[0] * image_tensor.shape[1],\n        image_tensor.shape[2],\n        image_tensor.shape[3],\n        image_tensor.shape[4]\n    )\n    calib_tensor = calib_tensor.view(\n        calib_tensor.shape[0] * calib_tensor.shape[1],\n        calib_tensor.shape[2],\n        calib_tensor.shape[3]\n    )\n\n    return image_tensor, calib_tensor\n\n\ndef reshape_sample_tensor(sample_tensor, num_views):\n    if num_views == 1:\n        return sample_tensor\n    # Need to repeat sample_tensor along the batch dim num_views times\n    sample_tensor = sample_tensor.unsqueeze(dim=1)\n    sample_tensor = sample_tensor.repeat(1, num_views, 1, 1)\n    sample_tensor = sample_tensor.view(\n        sample_tensor.shape[0] * sample_tensor.shape[1],\n        sample_tensor.shape[2],\n        sample_tensor.shape[3]\n    )\n    return sample_tensor\n\n\ndef gen_mesh(opt, net, cuda, data, save_path, use_octree=True):\n    image_tensor = data[\'img\'].to(device=cuda)\n    calib_tensor = data[\'calib\'].to(device=cuda)\n\n    net.filter(image_tensor)\n\n    b_min = data[\'b_min\']\n    b_max = data[\'b_max\']\n    try:\n        save_img_path = save_path[:-4] + \'.png\'\n        save_img_list = []\n        for v in range(image_tensor.shape[0]):\n            save_img = (np.transpose(image_tensor[v].detach().cpu().numpy(), (1, 2, 0)) * 0.5 + 0.5)[:, :, ::-1] * 255.0\n            save_img_list.append(save_img)\n        save_img = np.concatenate(save_img_list, axis=1)\n        Image.fromarray(np.uint8(save_img[:,:,::-1])).save(save_img_path)\n\n        verts, faces, _, _ = reconstruction(\n            net, cuda, calib_tensor, opt.resolution, b_min, b_max, use_octree=use_octree)\n        verts_tensor = torch.from_numpy(verts.T).unsqueeze(0).to(device=cuda).float()\n        xyz_tensor = net.projection(verts_tensor, calib_tensor[:1])\n        uv = xyz_tensor[:, :2, :]\n        color = index(image_tensor[:1], uv).detach().cpu().numpy()[0].T\n        color = color * 0.5 + 0.5\n        save_obj_mesh_with_color(save_path, verts, faces, color)\n    except Exception as e:\n        print(e)\n        print(\'Can not create marching cubes at this time.\')\n\ndef gen_mesh_color(opt, netG, netC, cuda, data, save_path, use_octree=True):\n    image_tensor = data[\'img\'].to(device=cuda)\n    calib_tensor = data[\'calib\'].to(device=cuda)\n\n    netG.filter(image_tensor)\n    netC.filter(image_tensor)\n    netC.attach(netG.get_im_feat())\n\n    b_min = data[\'b_min\']\n    b_max = data[\'b_max\']\n    try:\n        save_img_path = save_path[:-4] + \'.png\'\n        save_img_list = []\n        for v in range(image_tensor.shape[0]):\n            save_img = (np.transpose(image_tensor[v].detach().cpu().numpy(), (1, 2, 0)) * 0.5 + 0.5)[:, :, ::-1] * 255.0\n            save_img_list.append(save_img)\n        save_img = np.concatenate(save_img_list, axis=1)\n        Image.fromarray(np.uint8(save_img[:,:,::-1])).save(save_img_path)\n\n        verts, faces, _, _ = reconstruction(\n            netG, cuda, calib_tensor, opt.resolution, b_min, b_max, use_octree=use_octree)\n\n        # Now Getting colors\n        verts_tensor = torch.from_numpy(verts.T).unsqueeze(0).to(device=cuda).float()\n        verts_tensor = reshape_sample_tensor(verts_tensor, opt.num_views)\n\n        color = np.zeros(verts.shape)\n        interval = opt.num_sample_color\n        for i in range(len(color) // interval):\n            left = i * interval\n            right = i * interval + interval\n            if i == len(color) // interval - 1:\n                right = -1\n            netC.query(verts_tensor[:, :, left:right], calib_tensor)\n            rgb = netC.get_preds()[0].detach().cpu().numpy() * 0.5 + 0.5\n            color[left:right] = rgb.T\n\n        save_obj_mesh_with_color(save_path, verts, faces, color)\n    except Exception as e:\n        print(e)\n        print(\'Can not create marching cubes at this time.\')\n\ndef adjust_learning_rate(optimizer, epoch, lr, schedule, gamma):\n    """"""Sets the learning rate to the initial LR decayed by schedule""""""\n    if epoch in schedule:\n        lr *= gamma\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lr\n    return lr\n\n\ndef compute_acc(pred, gt, thresh=0.5):\n    \'\'\'\n    return:\n        IOU, precision, and recall\n    \'\'\'\n    with torch.no_grad():\n        vol_pred = pred > thresh\n        vol_gt = gt > thresh\n\n        union = vol_pred | vol_gt\n        inter = vol_pred & vol_gt\n\n        true_pos = inter.sum().float()\n\n        union = union.sum().float()\n        if union == 0:\n            union = 1\n        vol_pred = vol_pred.sum().float()\n        if vol_pred == 0:\n            vol_pred = 1\n        vol_gt = vol_gt.sum().float()\n        if vol_gt == 0:\n            vol_gt = 1\n        return true_pos / union, true_pos / vol_pred, true_pos / vol_gt\n\n\ndef calc_error(opt, net, cuda, dataset, num_tests):\n    if num_tests > len(dataset):\n        num_tests = len(dataset)\n    with torch.no_grad():\n        erorr_arr, IOU_arr, prec_arr, recall_arr = [], [], [], []\n        for idx in tqdm(range(num_tests)):\n            data = dataset[idx * len(dataset) // num_tests]\n            # retrieve the data\n            image_tensor = data[\'img\'].to(device=cuda)\n            calib_tensor = data[\'calib\'].to(device=cuda)\n            sample_tensor = data[\'samples\'].to(device=cuda).unsqueeze(0)\n            if opt.num_views > 1:\n                sample_tensor = reshape_sample_tensor(sample_tensor, opt.num_views)\n            label_tensor = data[\'labels\'].to(device=cuda).unsqueeze(0)\n\n            res, error = net.forward(image_tensor, sample_tensor, calib_tensor, labels=label_tensor)\n\n            IOU, prec, recall = compute_acc(res, label_tensor)\n\n            # print(\n            #     \'{0}/{1} | Error: {2:06f} IOU: {3:06f} prec: {4:06f} recall: {5:06f}\'\n            #         .format(idx, num_tests, error.item(), IOU.item(), prec.item(), recall.item()))\n            erorr_arr.append(error.item())\n            IOU_arr.append(IOU.item())\n            prec_arr.append(prec.item())\n            recall_arr.append(recall.item())\n\n    return np.average(erorr_arr), np.average(IOU_arr), np.average(prec_arr), np.average(recall_arr)\n\ndef calc_error_color(opt, netG, netC, cuda, dataset, num_tests):\n    if num_tests > len(dataset):\n        num_tests = len(dataset)\n    with torch.no_grad():\n        error_color_arr = []\n\n        for idx in tqdm(range(num_tests)):\n            data = dataset[idx * len(dataset) // num_tests]\n            # retrieve the data\n            image_tensor = data[\'img\'].to(device=cuda)\n            calib_tensor = data[\'calib\'].to(device=cuda)\n            color_sample_tensor = data[\'color_samples\'].to(device=cuda).unsqueeze(0)\n\n            if opt.num_views > 1:\n                color_sample_tensor = reshape_sample_tensor(color_sample_tensor, opt.num_views)\n\n            rgb_tensor = data[\'rgbs\'].to(device=cuda).unsqueeze(0)\n\n            netG.filter(image_tensor)\n            _, errorC = netC.forward(image_tensor, netG.get_im_feat(), color_sample_tensor, calib_tensor, labels=rgb_tensor)\n\n            # print(\'{0}/{1} | Error inout: {2:06f} | Error color: {3:06f}\'\n            #       .format(idx, num_tests, errorG.item(), errorC.item()))\n            error_color_arr.append(errorC.item())\n\n    return np.average(error_color_arr)\n\n\ndef conv3x3(in_planes, out_planes, strd=1, padding=1, bias=False):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3,\n                     stride=strd, padding=padding, bias=bias)\n\ndef init_weights(net, init_type=\'normal\', init_gain=0.02):\n    """"""Initialize network weights.\n\n    Parameters:\n        net (network)   -- network to be initialized\n        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n\n    We use \'normal\' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n    work better for some applications. Feel free to try yourself.\n    """"""\n\n    def init_func(m):  # define the initialization function\n        classname = m.__class__.__name__\n        if hasattr(m, \'weight\') and (classname.find(\'Conv\') != -1 or classname.find(\'Linear\') != -1):\n            if init_type == \'normal\':\n                init.normal_(m.weight.data, 0.0, init_gain)\n            elif init_type == \'xavier\':\n                init.xavier_normal_(m.weight.data, gain=init_gain)\n            elif init_type == \'kaiming\':\n                init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')\n            elif init_type == \'orthogonal\':\n                init.orthogonal_(m.weight.data, gain=init_gain)\n            else:\n                raise NotImplementedError(\'initialization method [%s] is not implemented\' % init_type)\n            if hasattr(m, \'bias\') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find(\n                \'BatchNorm2d\') != -1:  # BatchNorm Layer\'s weight is not a matrix; only normal distribution applies.\n            init.normal_(m.weight.data, 1.0, init_gain)\n            init.constant_(m.bias.data, 0.0)\n\n    print(\'initialize network with %s\' % init_type)\n    net.apply(init_func)  # apply the initialization function <init_func>\n\n\ndef init_net(net, init_type=\'normal\', init_gain=0.02, gpu_ids=[]):\n    """"""Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights\n    Parameters:\n        net (network)      -- the network to be initialized\n        init_type (str)    -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n        gain (float)       -- scaling factor for normal, xavier and orthogonal.\n        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n\n    Return an initialized network.\n    """"""\n    if len(gpu_ids) > 0:\n        assert (torch.cuda.is_available())\n        net.to(gpu_ids[0])\n        net = torch.nn.DataParallel(net, gpu_ids)  # multi-GPUs\n    init_weights(net, init_type, init_gain=init_gain)\n    return net\n\n\ndef imageSpaceRotation(xy, rot):\n    \'\'\'\n    args:\n        xy: (B, 2, N) input\n        rot: (B, 2) x,y axis rotation angles\n\n    rotation center will be always image center (other rotation center can be represented by additional z translation)\n    \'\'\'\n    disp = rot.unsqueeze(2).sin().expand_as(xy)\n    return (disp * xy).sum(dim=1)\n\n\ndef cal_gradient_penalty(netD, real_data, fake_data, device, type=\'mixed\', constant=1.0, lambda_gp=10.0):\n    """"""Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\n\n    Arguments:\n        netD (network)              -- discriminator network\n        real_data (tensor array)    -- real images\n        fake_data (tensor array)    -- generated images from the generator\n        device (str)                -- GPU / CPU: from torch.device(\'cuda:{}\'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device(\'cpu\')\n        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].\n        constant (float)            -- the constant used in formula ( | |gradient||_2 - constant)^2\n        lambda_gp (float)           -- weight for this loss\n\n    Returns the gradient penalty loss\n    """"""\n    if lambda_gp > 0.0:\n        if type == \'real\':  # either use real images, fake images, or a linear interpolation of two.\n            interpolatesv = real_data\n        elif type == \'fake\':\n            interpolatesv = fake_data\n        elif type == \'mixed\':\n            alpha = torch.rand(real_data.shape[0], 1)\n            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]).contiguous().view(\n                *real_data.shape)\n            alpha = alpha.to(device)\n            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n        else:\n            raise NotImplementedError(\'{} not implemented\'.format(type))\n        interpolatesv.requires_grad_(True)\n        disc_interpolates = netD(interpolatesv)\n        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,\n                                        grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n                                        create_graph=True, retain_graph=True, only_inputs=True)\n        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp  # added eps\n        return gradient_penalty, gradients\n    else:\n        return 0.0, None\n\ndef get_norm_layer(norm_type=\'instance\'):\n    """"""Return a normalization layer\n    Parameters:\n        norm_type (str) -- the name of the normalization layer: batch | instance | none\n    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n    """"""\n    if norm_type == \'batch\':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n    elif norm_type == \'instance\':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n    elif norm_type == \'group\':\n        norm_layer = functools.partial(nn.GroupNorm, 32)\n    elif norm_type == \'none\':\n        norm_layer = None\n    else:\n        raise NotImplementedError(\'normalization layer [%s] is not found\' % norm_type)\n    return norm_layer\n\nclass Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_planes, out_planes, norm=\'batch\'):\n        super(ConvBlock, self).__init__()\n        self.conv1 = conv3x3(in_planes, int(out_planes / 2))\n        self.conv2 = conv3x3(int(out_planes / 2), int(out_planes / 4))\n        self.conv3 = conv3x3(int(out_planes / 4), int(out_planes / 4))\n\n        if norm == \'batch\':\n            self.bn1 = nn.BatchNorm2d(in_planes)\n            self.bn2 = nn.BatchNorm2d(int(out_planes / 2))\n            self.bn3 = nn.BatchNorm2d(int(out_planes / 4))\n            self.bn4 = nn.BatchNorm2d(in_planes)\n        elif norm == \'group\':\n            self.bn1 = nn.GroupNorm(32, in_planes)\n            self.bn2 = nn.GroupNorm(32, int(out_planes / 2))\n            self.bn3 = nn.GroupNorm(32, int(out_planes / 4))\n            self.bn4 = nn.GroupNorm(32, in_planes)\n        \n        if in_planes != out_planes:\n            self.downsample = nn.Sequential(\n                self.bn4,\n                nn.ReLU(True),\n                nn.Conv2d(in_planes, out_planes,\n                          kernel_size=1, stride=1, bias=False),\n            )\n        else:\n            self.downsample = None\n\n    def forward(self, x):\n        residual = x\n\n        out1 = self.bn1(x)\n        out1 = F.relu(out1, True)\n        out1 = self.conv1(out1)\n\n        out2 = self.bn2(out1)\n        out2 = F.relu(out2, True)\n        out2 = self.conv2(out2)\n\n        out3 = self.bn3(out2)\n        out3 = F.relu(out3, True)\n        out3 = self.conv3(out3)\n\n        out3 = torch.cat((out1, out2, out3), 1)\n\n        if self.downsample is not None:\n            residual = self.downsample(residual)\n\n        out3 += residual\n\n        return out3\n  '"
lib/options.py,0,"b""import argparse\nimport os\n\n\nclass BaseOptions():\n    def __init__(self):\n        self.initialized = False\n\n    def initialize(self, parser):\n        # Datasets related\n        g_data = parser.add_argument_group('Data')\n        g_data.add_argument('--dataroot', type=str, default='./data',\n                            help='path to images (data folder)')\n\n        g_data.add_argument('--loadSize', type=int, default=512, help='load size of input image')\n\n        # Experiment related\n        g_exp = parser.add_argument_group('Experiment')\n        g_exp.add_argument('--name', type=str, default='example',\n                           help='name of the experiment. It decides where to store samples and models')\n        g_exp.add_argument('--debug', action='store_true', help='debug mode or not')\n\n        g_exp.add_argument('--num_views', type=int, default=1, help='How many views to use for multiview network.')\n        g_exp.add_argument('--random_multiview', action='store_true', help='Select random multiview combination.')\n\n        # Training related\n        g_train = parser.add_argument_group('Training')\n        g_train.add_argument('--gpu_id', type=int, default=0, help='gpu id for cuda')\n        g_train.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2, -1 for CPU mode')\n\n        g_train.add_argument('--num_threads', default=1, type=int, help='# sthreads for loading data')\n        g_train.add_argument('--serial_batches', action='store_true',\n                             help='if true, takes images in order to make batches, otherwise takes them randomly')\n        g_train.add_argument('--pin_memory', action='store_true', help='pin_memory')\n        \n        g_train.add_argument('--batch_size', type=int, default=2, help='input batch size')\n        g_train.add_argument('--learning_rate', type=float, default=1e-3, help='adam learning rate')\n        g_train.add_argument('--learning_rateC', type=float, default=1e-3, help='adam learning rate')\n        g_train.add_argument('--num_epoch', type=int, default=100, help='num epoch to train')\n\n        g_train.add_argument('--freq_plot', type=int, default=10, help='freqency of the error plot')\n        g_train.add_argument('--freq_save', type=int, default=50, help='freqency of the save_checkpoints')\n        g_train.add_argument('--freq_save_ply', type=int, default=100, help='freqency of the save ply')\n       \n        g_train.add_argument('--no_gen_mesh', action='store_true')\n        g_train.add_argument('--no_num_eval', action='store_true')\n        \n        g_train.add_argument('--resume_epoch', type=int, default=-1, help='epoch resuming the training')\n        g_train.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n\n        # Testing related\n        g_test = parser.add_argument_group('Testing')\n        g_test.add_argument('--resolution', type=int, default=256, help='# of grid in mesh reconstruction')\n        g_test.add_argument('--test_folder_path', type=str, default=None, help='the folder of test image')\n\n        # Sampling related\n        g_sample = parser.add_argument_group('Sampling')\n        g_sample.add_argument('--sigma', type=float, default=5.0, help='perturbation standard deviation for positions')\n\n        g_sample.add_argument('--num_sample_inout', type=int, default=5000, help='# of sampling points')\n        g_sample.add_argument('--num_sample_color', type=int, default=0, help='# of sampling points')\n\n        g_sample.add_argument('--z_size', type=float, default=200.0, help='z normalization factor')\n\n        # Model related\n        g_model = parser.add_argument_group('Model')\n        # General\n        g_model.add_argument('--norm', type=str, default='group',\n                             help='instance normalization or batch normalization or group normalization')\n        g_model.add_argument('--norm_color', type=str, default='instance',\n                             help='instance normalization or batch normalization or group normalization')\n\n        # hg filter specify\n        g_model.add_argument('--num_stack', type=int, default=4, help='# of hourglass')\n        g_model.add_argument('--num_hourglass', type=int, default=2, help='# of stacked layer of hourglass')\n        g_model.add_argument('--skip_hourglass', action='store_true', help='skip connection in hourglass')\n        g_model.add_argument('--hg_down', type=str, default='ave_pool', help='ave pool || conv64 || conv128')\n        g_model.add_argument('--hourglass_dim', type=int, default='256', help='256 | 512')\n\n        # Classification General\n        g_model.add_argument('--mlp_dim', nargs='+', default=[257, 1024, 512, 256, 128, 1], type=int,\n                             help='# of dimensions of mlp')\n        g_model.add_argument('--mlp_dim_color', nargs='+', default=[513, 1024, 512, 256, 128, 3],\n                             type=int, help='# of dimensions of color mlp')\n\n        g_model.add_argument('--use_tanh', action='store_true',\n                             help='using tanh after last conv of image_filter network')\n\n        # for train\n        parser.add_argument('--random_flip', action='store_true', help='if random flip')\n        parser.add_argument('--random_trans', action='store_true', help='if random flip')\n        parser.add_argument('--random_scale', action='store_true', help='if random flip')\n        parser.add_argument('--no_residual', action='store_true', help='no skip connection in mlp')\n        parser.add_argument('--schedule', type=int, nargs='+', default=[60, 80],\n                            help='Decrease learning rate at these epochs.')\n        parser.add_argument('--gamma', type=float, default=0.1, help='LR is multiplied by gamma on schedule.')\n        parser.add_argument('--color_loss_type', type=str, default='l1', help='mse | l1')\n\n        # for eval\n        parser.add_argument('--val_test_error', action='store_true', help='validate errors of test data')\n        parser.add_argument('--val_train_error', action='store_true', help='validate errors of train data')\n        parser.add_argument('--gen_test_mesh', action='store_true', help='generate test mesh')\n        parser.add_argument('--gen_train_mesh', action='store_true', help='generate train mesh')\n        parser.add_argument('--all_mesh', action='store_true', help='generate meshs from all hourglass output')\n        parser.add_argument('--num_gen_mesh_test', type=int, default=1,\n                            help='how many meshes to generate during testing')\n\n        # path\n        parser.add_argument('--checkpoints_path', type=str, default='./checkpoints', help='path to save checkpoints')\n        parser.add_argument('--load_netG_checkpoint_path', type=str, default=None, help='path to save checkpoints')\n        parser.add_argument('--load_netC_checkpoint_path', type=str, default=None, help='path to save checkpoints')\n        parser.add_argument('--results_path', type=str, default='./results', help='path to save results ply')\n        parser.add_argument('--load_checkpoint_path', type=str, help='path to save results ply')\n        parser.add_argument('--single', type=str, default='', help='single data for training')\n        # for single image reconstruction\n        parser.add_argument('--mask_path', type=str, help='path for input mask')\n        parser.add_argument('--img_path', type=str, help='path for input image')\n\n        # aug\n        group_aug = parser.add_argument_group('aug')\n        group_aug.add_argument('--aug_alstd', type=float, default=0.0, help='augmentation pca lighting alpha std')\n        group_aug.add_argument('--aug_bri', type=float, default=0.0, help='augmentation brightness')\n        group_aug.add_argument('--aug_con', type=float, default=0.0, help='augmentation contrast')\n        group_aug.add_argument('--aug_sat', type=float, default=0.0, help='augmentation saturation')\n        group_aug.add_argument('--aug_hue', type=float, default=0.0, help='augmentation hue')\n        group_aug.add_argument('--aug_blur', type=float, default=0.0, help='augmentation blur')\n\n        # special tasks\n        self.initialized = True\n        return parser\n\n    def gather_options(self):\n        # initialize parser with basic options\n        if not self.initialized:\n            parser = argparse.ArgumentParser(\n                formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n            parser = self.initialize(parser)\n\n        self.parser = parser\n\n        return parser.parse_args()\n\n    def print_options(self, opt):\n        message = ''\n        message += '----------------- Options ---------------\\n'\n        for k, v in sorted(vars(opt).items()):\n            comment = ''\n            default = self.parser.get_default(k)\n            if v != default:\n                comment = '\\t[default: %s]' % str(default)\n            message += '{:>25}: {:<30}{}\\n'.format(str(k), str(v), comment)\n        message += '----------------- End -------------------'\n        print(message)\n\n    def parse(self):\n        opt = self.gather_options()\n        return opt\n"""
lib/sample_util.py,0,"b""import numpy as np\n\n\ndef save_samples_truncted_prob(fname, points, prob):\n    '''\n    Save the visualization of sampling to a ply file.\n    Red points represent positive predictions.\n    Green points represent negative predictions.\n    :param fname: File name to save\n    :param points: [N, 3] array of points\n    :param prob: [N, 1] array of predictions in the range [0~1]\n    :return:\n    '''\n    r = (prob > 0.5).reshape([-1, 1]) * 255\n    g = (prob < 0.5).reshape([-1, 1]) * 255\n    b = np.zeros(r.shape)\n\n    to_save = np.concatenate([points, r, g, b], axis=-1)\n    return np.savetxt(fname,\n                      to_save,\n                      fmt='%.6f %.6f %.6f %d %d %d',\n                      comments='',\n                      header=(\n                          'ply\\nformat ascii 1.0\\nelement vertex {:d}\\nproperty float x\\nproperty float y\\nproperty float z\\nproperty uchar red\\nproperty uchar green\\nproperty uchar blue\\nend_header').format(\n                          points.shape[0])\n                      )\n\n\ndef save_samples_rgb(fname, points, rgb):\n    '''\n    Save the visualization of sampling to a ply file.\n    Red points represent positive predictions.\n    Green points represent negative predictions.\n    :param fname: File name to save\n    :param points: [N, 3] array of points\n    :param rgb: [N, 3] array of rgb values in the range [0~1]\n    :return:\n    '''\n    to_save = np.concatenate([points, rgb * 255], axis=-1)\n    return np.savetxt(fname,\n                      to_save,\n                      fmt='%.6f %.6f %.6f %d %d %d',\n                      comments='',\n                      header=(\n                          'ply\\nformat ascii 1.0\\nelement vertex {:d}\\nproperty float x\\nproperty float y\\nproperty float z\\nproperty uchar red\\nproperty uchar green\\nproperty uchar blue\\nend_header').format(\n                          points.shape[0])\n                      )\n"""
lib/sdf.py,0,"b""import numpy as np\n\n\ndef create_grid(resX, resY, resZ, b_min=np.array([0, 0, 0]), b_max=np.array([1, 1, 1]), transform=None):\n    '''\n    Create a dense grid of given resolution and bounding box\n    :param resX: resolution along X axis\n    :param resY: resolution along Y axis\n    :param resZ: resolution along Z axis\n    :param b_min: vec3 (x_min, y_min, z_min) bounding box corner\n    :param b_max: vec3 (x_max, y_max, z_max) bounding box corner\n    :return: [3, resX, resY, resZ] coordinates of the grid, and transform matrix from mesh index\n    '''\n    coords = np.mgrid[:resX, :resY, :resZ]\n    coords = coords.reshape(3, -1)\n    coords_matrix = np.eye(4)\n    length = b_max - b_min\n    coords_matrix[0, 0] = length[0] / resX\n    coords_matrix[1, 1] = length[1] / resY\n    coords_matrix[2, 2] = length[2] / resZ\n    coords_matrix[0:3, 3] = b_min\n    coords = np.matmul(coords_matrix[:3, :3], coords) + coords_matrix[:3, 3:4]\n    if transform is not None:\n        coords = np.matmul(transform[:3, :3], coords) + transform[:3, 3:4]\n        coords_matrix = np.matmul(transform, coords_matrix)\n    coords = coords.reshape(3, resX, resY, resZ)\n    return coords, coords_matrix\n\n\ndef batch_eval(points, eval_func, num_samples=512 * 512 * 512):\n    num_pts = points.shape[1]\n    sdf = np.zeros(num_pts)\n\n    num_batches = num_pts // num_samples\n    for i in range(num_batches):\n        sdf[i * num_samples:i * num_samples + num_samples] = eval_func(\n            points[:, i * num_samples:i * num_samples + num_samples])\n    if num_pts % num_samples:\n        sdf[num_batches * num_samples:] = eval_func(points[:, num_batches * num_samples:])\n\n    return sdf\n\n\ndef eval_grid(coords, eval_func, num_samples=512 * 512 * 512):\n    resolution = coords.shape[1:4]\n    coords = coords.reshape([3, -1])\n    sdf = batch_eval(coords, eval_func, num_samples=num_samples)\n    return sdf.reshape(resolution)\n\n\ndef eval_grid_octree(coords, eval_func,\n                     init_resolution=64, threshold=0.01,\n                     num_samples=512 * 512 * 512):\n    resolution = coords.shape[1:4]\n\n    sdf = np.zeros(resolution)\n\n    dirty = np.ones(resolution, dtype=np.bool)\n    grid_mask = np.zeros(resolution, dtype=np.bool)\n\n    reso = resolution[0] // init_resolution\n\n    while reso > 0:\n        # subdivide the grid\n        grid_mask[0:resolution[0]:reso, 0:resolution[1]:reso, 0:resolution[2]:reso] = True\n        # test samples in this iteration\n        test_mask = np.logical_and(grid_mask, dirty)\n        #print('step size:', reso, 'test sample size:', test_mask.sum())\n        points = coords[:, test_mask]\n\n        sdf[test_mask] = batch_eval(points, eval_func, num_samples=num_samples)\n        dirty[test_mask] = False\n\n        # do interpolation\n        if reso <= 1:\n            break\n        for x in range(0, resolution[0] - reso, reso):\n            for y in range(0, resolution[1] - reso, reso):\n                for z in range(0, resolution[2] - reso, reso):\n                    # if center marked, return\n                    if not dirty[x + reso // 2, y + reso // 2, z + reso // 2]:\n                        continue\n                    v0 = sdf[x, y, z]\n                    v1 = sdf[x, y, z + reso]\n                    v2 = sdf[x, y + reso, z]\n                    v3 = sdf[x, y + reso, z + reso]\n                    v4 = sdf[x + reso, y, z]\n                    v5 = sdf[x + reso, y, z + reso]\n                    v6 = sdf[x + reso, y + reso, z]\n                    v7 = sdf[x + reso, y + reso, z + reso]\n                    v = np.array([v0, v1, v2, v3, v4, v5, v6, v7])\n                    v_min = v.min()\n                    v_max = v.max()\n                    # this cell is all the same\n                    if (v_max - v_min) < threshold:\n                        sdf[x:x + reso, y:y + reso, z:z + reso] = (v_max + v_min) / 2\n                        dirty[x:x + reso, y:y + reso, z:z + reso] = False\n        reso //= 2\n\n    return sdf.reshape(resolution)\n"""
lib/train_util.py,5,"b'import torch\nimport numpy as np\nfrom .mesh_util import *\nfrom .sample_util import *\nfrom .geometry import *\nimport cv2\nfrom PIL import Image\nfrom tqdm import tqdm\n\ndef reshape_multiview_tensors(image_tensor, calib_tensor):\n    # Careful here! Because we put single view and multiview together,\n    # the returned tensor.shape is 5-dim: [B, num_views, C, W, H]\n    # So we need to convert it back to 4-dim [B*num_views, C, W, H]\n    # Don\'t worry classifier will handle multi-view cases\n    image_tensor = image_tensor.view(\n        image_tensor.shape[0] * image_tensor.shape[1],\n        image_tensor.shape[2],\n        image_tensor.shape[3],\n        image_tensor.shape[4]\n    )\n    calib_tensor = calib_tensor.view(\n        calib_tensor.shape[0] * calib_tensor.shape[1],\n        calib_tensor.shape[2],\n        calib_tensor.shape[3]\n    )\n\n    return image_tensor, calib_tensor\n\n\ndef reshape_sample_tensor(sample_tensor, num_views):\n    if num_views == 1:\n        return sample_tensor\n    # Need to repeat sample_tensor along the batch dim num_views times\n    sample_tensor = sample_tensor.unsqueeze(dim=1)\n    sample_tensor = sample_tensor.repeat(1, num_views, 1, 1)\n    sample_tensor = sample_tensor.view(\n        sample_tensor.shape[0] * sample_tensor.shape[1],\n        sample_tensor.shape[2],\n        sample_tensor.shape[3]\n    )\n    return sample_tensor\n\n\ndef gen_mesh(opt, net, cuda, data, save_path, use_octree=True):\n    image_tensor = data[\'img\'].to(device=cuda)\n    calib_tensor = data[\'calib\'].to(device=cuda)\n\n    net.filter(image_tensor)\n\n    b_min = data[\'b_min\']\n    b_max = data[\'b_max\']\n    try:\n        save_img_path = save_path[:-4] + \'.png\'\n        save_img_list = []\n        for v in range(image_tensor.shape[0]):\n            save_img = (np.transpose(image_tensor[v].detach().cpu().numpy(), (1, 2, 0)) * 0.5 + 0.5)[:, :, ::-1] * 255.0\n            save_img_list.append(save_img)\n        save_img = np.concatenate(save_img_list, axis=1)\n        Image.fromarray(np.uint8(save_img[:,:,::-1])).save(save_img_path)\n\n        verts, faces, _, _ = reconstruction(\n            net, cuda, calib_tensor, opt.resolution, b_min, b_max, use_octree=use_octree)\n        verts_tensor = torch.from_numpy(verts.T).unsqueeze(0).to(device=cuda).float()\n        xyz_tensor = net.projection(verts_tensor, calib_tensor[:1])\n        uv = xyz_tensor[:, :2, :]\n        color = index(image_tensor[:1], uv).detach().cpu().numpy()[0].T\n        color = color * 0.5 + 0.5\n        save_obj_mesh_with_color(save_path, verts, faces, color)\n    except Exception as e:\n        print(e)\n        print(\'Can not create marching cubes at this time.\')\n\ndef gen_mesh_color(opt, netG, netC, cuda, data, save_path, use_octree=True):\n    image_tensor = data[\'img\'].to(device=cuda)\n    calib_tensor = data[\'calib\'].to(device=cuda)\n\n    netG.filter(image_tensor)\n    netC.filter(image_tensor)\n    netC.attach(netG.get_im_feat())\n\n    b_min = data[\'b_min\']\n    b_max = data[\'b_max\']\n    try:\n        save_img_path = save_path[:-4] + \'.png\'\n        save_img_list = []\n        for v in range(image_tensor.shape[0]):\n            save_img = (np.transpose(image_tensor[v].detach().cpu().numpy(), (1, 2, 0)) * 0.5 + 0.5)[:, :, ::-1] * 255.0\n            save_img_list.append(save_img)\n        save_img = np.concatenate(save_img_list, axis=1)\n        Image.fromarray(np.uint8(save_img[:,:,::-1])).save(save_img_path)\n\n        verts, faces, _, _ = reconstruction(\n            netG, cuda, calib_tensor, opt.resolution, b_min, b_max, use_octree=use_octree)\n\n        # Now Getting colors\n        verts_tensor = torch.from_numpy(verts.T).unsqueeze(0).to(device=cuda).float()\n        verts_tensor = reshape_sample_tensor(verts_tensor, opt.num_views)\n        color = np.zeros(verts.shape)\n        interval = 10000\n        for i in range(len(color) // interval):\n            left = i * interval\n            right = i * interval + interval\n            if i == len(color) // interval - 1:\n                right = -1\n            netC.query(verts_tensor[:, :, left:right], calib_tensor)\n            rgb = netC.get_preds()[0].detach().cpu().numpy() * 0.5 + 0.5\n            color[left:right] = rgb.T\n\n        save_obj_mesh_with_color(save_path, verts, faces, color)\n    except Exception as e:\n        print(e)\n        print(\'Can not create marching cubes at this time.\')\n\ndef adjust_learning_rate(optimizer, epoch, lr, schedule, gamma):\n    """"""Sets the learning rate to the initial LR decayed by schedule""""""\n    if epoch in schedule:\n        lr *= gamma\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = lr\n    return lr\n\n\ndef compute_acc(pred, gt, thresh=0.5):\n    \'\'\'\n    return:\n        IOU, precision, and recall\n    \'\'\'\n    with torch.no_grad():\n        vol_pred = pred > thresh\n        vol_gt = gt > thresh\n\n        union = vol_pred | vol_gt\n        inter = vol_pred & vol_gt\n\n        true_pos = inter.sum().float()\n\n        union = union.sum().float()\n        if union == 0:\n            union = 1\n        vol_pred = vol_pred.sum().float()\n        if vol_pred == 0:\n            vol_pred = 1\n        vol_gt = vol_gt.sum().float()\n        if vol_gt == 0:\n            vol_gt = 1\n        return true_pos / union, true_pos / vol_pred, true_pos / vol_gt\n\n\ndef calc_error(opt, net, cuda, dataset, num_tests):\n    if num_tests > len(dataset):\n        num_tests = len(dataset)\n    with torch.no_grad():\n        erorr_arr, IOU_arr, prec_arr, recall_arr = [], [], [], []\n        for idx in tqdm(range(num_tests)):\n            data = dataset[idx * len(dataset) // num_tests]\n            # retrieve the data\n            image_tensor = data[\'img\'].to(device=cuda)\n            calib_tensor = data[\'calib\'].to(device=cuda)\n            sample_tensor = data[\'samples\'].to(device=cuda).unsqueeze(0)\n            if opt.num_views > 1:\n                sample_tensor = reshape_sample_tensor(sample_tensor, opt.num_views)\n            label_tensor = data[\'labels\'].to(device=cuda).unsqueeze(0)\n\n            res, error = net.forward(image_tensor, sample_tensor, calib_tensor, labels=label_tensor)\n\n            IOU, prec, recall = compute_acc(res, label_tensor)\n\n            # print(\n            #     \'{0}/{1} | Error: {2:06f} IOU: {3:06f} prec: {4:06f} recall: {5:06f}\'\n            #         .format(idx, num_tests, error.item(), IOU.item(), prec.item(), recall.item()))\n            erorr_arr.append(error.item())\n            IOU_arr.append(IOU.item())\n            prec_arr.append(prec.item())\n            recall_arr.append(recall.item())\n\n    return np.average(erorr_arr), np.average(IOU_arr), np.average(prec_arr), np.average(recall_arr)\n\ndef calc_error_color(opt, netG, netC, cuda, dataset, num_tests):\n    if num_tests > len(dataset):\n        num_tests = len(dataset)\n    with torch.no_grad():\n        error_color_arr = []\n\n        for idx in tqdm(range(num_tests)):\n            data = dataset[idx * len(dataset) // num_tests]\n            # retrieve the data\n            image_tensor = data[\'img\'].to(device=cuda)\n            calib_tensor = data[\'calib\'].to(device=cuda)\n            color_sample_tensor = data[\'color_samples\'].to(device=cuda).unsqueeze(0)\n\n            if opt.num_views > 1:\n                color_sample_tensor = reshape_sample_tensor(color_sample_tensor, opt.num_views)\n\n            rgb_tensor = data[\'rgbs\'].to(device=cuda).unsqueeze(0)\n\n            netG.filter(image_tensor)\n            _, errorC = netC.forward(image_tensor, netG.get_im_feat(), color_sample_tensor, calib_tensor, labels=rgb_tensor)\n\n            # print(\'{0}/{1} | Error inout: {2:06f} | Error color: {3:06f}\'\n            #       .format(idx, num_tests, errorG.item(), errorC.item()))\n            error_color_arr.append(errorC.item())\n\n    return np.average(error_color_arr)\n\n'"
lib/data/BaseDataset.py,1,"b'from torch.utils.data import Dataset\nimport random\n\n\nclass BaseDataset(Dataset):\n    \'\'\'\n    This is the Base Datasets.\n    Itself does nothing and is not runnable.\n    Check self.get_item function to see what it should return.\n    \'\'\'\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def __init__(self, opt, phase=\'train\'):\n        self.opt = opt\n        self.is_train = self.phase == \'train\'\n        self.projection_mode = \'orthogonal\'  # Declare projection mode here\n\n    def __len__(self):\n        return 0\n\n    def get_item(self, index):\n        # In case of a missing file or IO error, switch to a random sample instead\n        try:\n            res = {\n                \'name\': None,  # name of this subject\n                \'b_min\': None,  # Bounding box (x_min, y_min, z_min) of target space\n                \'b_max\': None,  # Bounding box (x_max, y_max, z_max) of target space\n\n                \'samples\': None,  # [3, N] samples\n                \'labels\': None,  # [1, N] labels\n\n                \'img\': None,  # [num_views, C, H, W] input images\n                \'calib\': None,  # [num_views, 4, 4] calibration matrix\n                \'extrinsic\': None,  # [num_views, 4, 4] extrinsic matrix\n                \'mask\': None,  # [num_views, 1, H, W] segmentation masks\n            }\n            return res\n        except:\n            print(""Requested index %s has missing files. Using a random sample instead."" % index)\n            return self.get_item(index=random.randint(0, self.__len__() - 1))\n\n    def __getitem__(self, index):\n        return self.get_item(index)\n'"
lib/data/EvalDataset.py,7,"b""from torch.utils.data import Dataset\nimport numpy as np\nimport os\nimport random\nimport torchvision.transforms as transforms\nfrom PIL import Image, ImageOps\nimport cv2\nimport torch\nfrom PIL.ImageFilter import GaussianBlur\nimport trimesh\nimport cv2\n\n\nclass EvalDataset(Dataset):\n    @staticmethod\n    def modify_commandline_options(parser):\n        return parser\n\n    def __init__(self, opt, root=None):\n        self.opt = opt\n        self.projection_mode = 'orthogonal'\n\n        # Path setup\n        self.root = self.opt.dataroot\n        if root is not None:\n            self.root = root\n        self.RENDER = os.path.join(self.root, 'RENDER')\n        self.MASK = os.path.join(self.root, 'MASK')\n        self.PARAM = os.path.join(self.root, 'PARAM')\n        self.OBJ = os.path.join(self.root, 'GEO', 'OBJ')\n\n        self.phase = 'val'\n        self.load_size = self.opt.loadSize\n\n        self.num_views = self.opt.num_views\n\n        self.max_view_angle = 360\n        self.interval = 1\n        self.subjects = self.get_subjects()\n\n        # PIL to tensor\n        self.to_tensor = transforms.Compose([\n            transforms.Resize(self.load_size),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n\n    def get_subjects(self):\n        var_file = os.path.join(self.root, 'val.txt')\n        if os.path.exists(var_file):\n            var_subjects = np.loadtxt(var_file, dtype=str)\n            return sorted(list(var_subjects))\n        all_subjects = os.listdir(self.RENDER)\n        return sorted(list(all_subjects))\n\n    def __len__(self):\n        return len(self.subjects) * self.max_view_angle // self.interval\n\n    def get_render(self, subject, num_views, view_id=None, random_sample=False):\n        '''\n        Return the render data\n        :param subject: subject name\n        :param num_views: how many views to return\n        :param view_id: the first view_id. If None, select a random one.\n        :return:\n            'img': [num_views, C, W, H] images\n            'calib': [num_views, 4, 4] calibration matrix\n            'extrinsic': [num_views, 4, 4] extrinsic matrix\n            'mask': [num_views, 1, W, H] masks\n        '''\n        # For now we only have pitch = 00. Hard code it here\n        pitch = 0\n        # Select a random view_id from self.max_view_angle if not given\n        if view_id is None:\n            view_id = np.random.randint(self.max_view_angle)\n        # The ids are an even distribution of num_views around view_id\n        view_ids = [(view_id + self.max_view_angle // num_views * offset) % self.max_view_angle\n                    for offset in range(num_views)]\n        if random_sample:\n            view_ids = np.random.choice(self.max_view_angle, num_views, replace=False)\n\n        calib_list = []\n        render_list = []\n        mask_list = []\n        extrinsic_list = []\n\n        for vid in view_ids:\n            param_path = os.path.join(self.PARAM, subject, '%d_%02d.npy' % (vid, pitch))\n            render_path = os.path.join(self.RENDER, subject, '%d_%02d.jpg' % (vid, pitch))\n            mask_path = os.path.join(self.MASK, subject, '%d_%02d.png' % (vid, pitch))\n\n            # loading calibration data\n            param = np.load(param_path)\n            # pixel unit / world unit\n            ortho_ratio = param.item().get('ortho_ratio')\n            # world unit / model unit\n            scale = param.item().get('scale')\n            # camera center world coordinate\n            center = param.item().get('center')\n            # model rotation\n            R = param.item().get('R')\n\n            translate = -np.matmul(R, center).reshape(3, 1)\n            extrinsic = np.concatenate([R, translate], axis=1)\n            extrinsic = np.concatenate([extrinsic, np.array([0, 0, 0, 1]).reshape(1, 4)], 0)\n            # Match camera space to image pixel space\n            scale_intrinsic = np.identity(4)\n            scale_intrinsic[0, 0] = scale / ortho_ratio\n            scale_intrinsic[1, 1] = -scale / ortho_ratio\n            scale_intrinsic[2, 2] = -scale / ortho_ratio\n            # Match image pixel space to image uv space\n            uv_intrinsic = np.identity(4)\n            uv_intrinsic[0, 0] = 1.0 / float(self.opt.loadSize // 2)\n            uv_intrinsic[1, 1] = 1.0 / float(self.opt.loadSize // 2)\n            uv_intrinsic[2, 2] = 1.0 / float(self.opt.loadSize // 2)\n            # Transform under image pixel space\n            trans_intrinsic = np.identity(4)\n\n            mask = Image.open(mask_path).convert('L')\n            render = Image.open(render_path).convert('RGB')\n\n            intrinsic = np.matmul(trans_intrinsic, np.matmul(uv_intrinsic, scale_intrinsic))\n            calib = torch.Tensor(np.matmul(intrinsic, extrinsic)).float()\n            extrinsic = torch.Tensor(extrinsic).float()\n\n            mask = transforms.Resize(self.load_size)(mask)\n            mask = transforms.ToTensor()(mask).float()\n            mask_list.append(mask)\n\n            render = self.to_tensor(render)\n            render = mask.expand_as(render) * render\n\n            render_list.append(render)\n            calib_list.append(calib)\n            extrinsic_list.append(extrinsic)\n\n        return {\n            'img': torch.stack(render_list, dim=0),\n            'calib': torch.stack(calib_list, dim=0),\n            'extrinsic': torch.stack(extrinsic_list, dim=0),\n            'mask': torch.stack(mask_list, dim=0)\n        }\n\n    def get_item(self, index):\n        # In case of a missing file or IO error, switch to a random sample instead\n        try:\n            sid = index % len(self.subjects)\n            vid = (index // len(self.subjects)) * self.interval\n            # name of the subject 'rp_xxxx_xxx'\n            subject = self.subjects[sid]\n            res = {\n                'name': subject,\n                'mesh_path': os.path.join(self.OBJ, subject + '.obj'),\n                'sid': sid,\n                'vid': vid,\n            }\n            render_data = self.get_render(subject, num_views=self.num_views, view_id=vid,\n                                          random_sample=self.opt.random_multiview)\n            res.update(render_data)\n            return res\n        except Exception as e:\n            print(e)\n            return self.get_item(index=random.randint(0, self.__len__() - 1))\n\n    def __getitem__(self, index):\n        return self.get_item(index)\n"""
lib/data/TrainDataset.py,15,"b""from torch.utils.data import Dataset\nimport numpy as np\nimport os\nimport random\nimport torchvision.transforms as transforms\nfrom PIL import Image, ImageOps\nimport cv2\nimport torch\nfrom PIL.ImageFilter import GaussianBlur\nimport trimesh\nimport logging\n\nlog = logging.getLogger('trimesh')\nlog.setLevel(40)\n\ndef load_trimesh(root_dir):\n    folders = os.listdir(root_dir)\n    meshs = {}\n    for i, f in enumerate(folders):\n        sub_name = f\n        meshs[sub_name] = trimesh.load(os.path.join(root_dir, f, '%s_100k.obj' % sub_name))\n\n    return meshs\n\ndef save_samples_truncted_prob(fname, points, prob):\n    '''\n    Save the visualization of sampling to a ply file.\n    Red points represent positive predictions.\n    Green points represent negative predictions.\n    :param fname: File name to save\n    :param points: [N, 3] array of points\n    :param prob: [N, 1] array of predictions in the range [0~1]\n    :return:\n    '''\n    r = (prob > 0.5).reshape([-1, 1]) * 255\n    g = (prob < 0.5).reshape([-1, 1]) * 255\n    b = np.zeros(r.shape)\n\n    to_save = np.concatenate([points, r, g, b], axis=-1)\n    return np.savetxt(fname,\n                      to_save,\n                      fmt='%.6f %.6f %.6f %d %d %d',\n                      comments='',\n                      header=(\n                          'ply\\nformat ascii 1.0\\nelement vertex {:d}\\nproperty float x\\nproperty float y\\nproperty float z\\nproperty uchar red\\nproperty uchar green\\nproperty uchar blue\\nend_header').format(\n                          points.shape[0])\n                      )\n\n\nclass TrainDataset(Dataset):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n    def __init__(self, opt, phase='train'):\n        self.opt = opt\n        self.projection_mode = 'orthogonal'\n\n        # Path setup\n        self.root = self.opt.dataroot\n        self.RENDER = os.path.join(self.root, 'RENDER')\n        self.MASK = os.path.join(self.root, 'MASK')\n        self.PARAM = os.path.join(self.root, 'PARAM')\n        self.UV_MASK = os.path.join(self.root, 'UV_MASK')\n        self.UV_NORMAL = os.path.join(self.root, 'UV_NORMAL')\n        self.UV_RENDER = os.path.join(self.root, 'UV_RENDER')\n        self.UV_POS = os.path.join(self.root, 'UV_POS')\n        self.OBJ = os.path.join(self.root, 'GEO', 'OBJ')\n\n        self.B_MIN = np.array([-128, -28, -128])\n        self.B_MAX = np.array([128, 228, 128])\n\n        self.is_train = (phase == 'train')\n        self.load_size = self.opt.loadSize\n\n        self.num_views = self.opt.num_views\n\n        self.num_sample_inout = self.opt.num_sample_inout\n        self.num_sample_color = self.opt.num_sample_color\n\n        self.yaw_list = list(range(0,360,1))\n        self.pitch_list = [0]\n        self.subjects = self.get_subjects()\n\n        # PIL to tensor\n        self.to_tensor = transforms.Compose([\n            transforms.Resize(self.load_size),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n\n        # augmentation\n        self.aug_trans = transforms.Compose([\n            transforms.ColorJitter(brightness=opt.aug_bri, contrast=opt.aug_con, saturation=opt.aug_sat,\n                                   hue=opt.aug_hue)\n        ])\n\n        self.mesh_dic = load_trimesh(self.OBJ)\n\n    def get_subjects(self):\n        all_subjects = os.listdir(self.RENDER)\n        var_subjects = np.loadtxt(os.path.join(self.root, 'val.txt'), dtype=str)\n        if len(var_subjects) == 0:\n            return all_subjects\n\n        if self.is_train:\n            return sorted(list(set(all_subjects) - set(var_subjects)))\n        else:\n            return sorted(list(var_subjects))\n\n    def __len__(self):\n        return len(self.subjects) * len(self.yaw_list) * len(self.pitch_list)\n\n    def get_render(self, subject, num_views, yid=0, pid=0, random_sample=False):\n        '''\n        Return the render data\n        :param subject: subject name\n        :param num_views: how many views to return\n        :param view_id: the first view_id. If None, select a random one.\n        :return:\n            'img': [num_views, C, W, H] images\n            'calib': [num_views, 4, 4] calibration matrix\n            'extrinsic': [num_views, 4, 4] extrinsic matrix\n            'mask': [num_views, 1, W, H] masks\n        '''\n        pitch = self.pitch_list[pid]\n\n        # The ids are an even distribution of num_views around view_id\n        view_ids = [self.yaw_list[(yid + len(self.yaw_list) // num_views * offset) % len(self.yaw_list)]\n                    for offset in range(num_views)]\n        if random_sample:\n            view_ids = np.random.choice(self.yaw_list, num_views, replace=False)\n\n        calib_list = []\n        render_list = []\n        mask_list = []\n        extrinsic_list = []\n\n        for vid in view_ids:\n            param_path = os.path.join(self.PARAM, subject, '%d_%d_%02d.npy' % (vid, pitch, 0))\n            render_path = os.path.join(self.RENDER, subject, '%d_%d_%02d.jpg' % (vid, pitch, 0))\n            mask_path = os.path.join(self.MASK, subject, '%d_%d_%02d.png' % (vid, pitch, 0))\n\n            # loading calibration data\n            param = np.load(param_path, allow_pickle=True)\n            # pixel unit / world unit\n            ortho_ratio = param.item().get('ortho_ratio')\n            # world unit / model unit\n            scale = param.item().get('scale')\n            # camera center world coordinate\n            center = param.item().get('center')\n            # model rotation\n            R = param.item().get('R')\n\n            translate = -np.matmul(R, center).reshape(3, 1)\n            extrinsic = np.concatenate([R, translate], axis=1)\n            extrinsic = np.concatenate([extrinsic, np.array([0, 0, 0, 1]).reshape(1, 4)], 0)\n            # Match camera space to image pixel space\n            scale_intrinsic = np.identity(4)\n            scale_intrinsic[0, 0] = scale / ortho_ratio\n            scale_intrinsic[1, 1] = -scale / ortho_ratio\n            scale_intrinsic[2, 2] = scale / ortho_ratio\n            # Match image pixel space to image uv space\n            uv_intrinsic = np.identity(4)\n            uv_intrinsic[0, 0] = 1.0 / float(self.opt.loadSize // 2)\n            uv_intrinsic[1, 1] = 1.0 / float(self.opt.loadSize // 2)\n            uv_intrinsic[2, 2] = 1.0 / float(self.opt.loadSize // 2)\n            # Transform under image pixel space\n            trans_intrinsic = np.identity(4)\n\n            mask = Image.open(mask_path).convert('L')\n            render = Image.open(render_path).convert('RGB')\n\n            if self.is_train:\n                # Pad images\n                pad_size = int(0.1 * self.load_size)\n                render = ImageOps.expand(render, pad_size, fill=0)\n                mask = ImageOps.expand(mask, pad_size, fill=0)\n\n                w, h = render.size\n                th, tw = self.load_size, self.load_size\n\n                # random flip\n                if self.opt.random_flip and np.random.rand() > 0.5:\n                    scale_intrinsic[0, 0] *= -1\n                    render = transforms.RandomHorizontalFlip(p=1.0)(render)\n                    mask = transforms.RandomHorizontalFlip(p=1.0)(mask)\n\n                # random scale\n                if self.opt.random_scale:\n                    rand_scale = random.uniform(0.9, 1.1)\n                    w = int(rand_scale * w)\n                    h = int(rand_scale * h)\n                    render = render.resize((w, h), Image.BILINEAR)\n                    mask = mask.resize((w, h), Image.NEAREST)\n                    scale_intrinsic *= rand_scale\n                    scale_intrinsic[3, 3] = 1\n\n                # random translate in the pixel space\n                if self.opt.random_trans:\n                    dx = random.randint(-int(round((w - tw) / 10.)),\n                                        int(round((w - tw) / 10.)))\n                    dy = random.randint(-int(round((h - th) / 10.)),\n                                        int(round((h - th) / 10.)))\n                else:\n                    dx = 0\n                    dy = 0\n\n                trans_intrinsic[0, 3] = -dx / float(self.opt.loadSize // 2)\n                trans_intrinsic[1, 3] = -dy / float(self.opt.loadSize // 2)\n\n                x1 = int(round((w - tw) / 2.)) + dx\n                y1 = int(round((h - th) / 2.)) + dy\n\n                render = render.crop((x1, y1, x1 + tw, y1 + th))\n                mask = mask.crop((x1, y1, x1 + tw, y1 + th))\n\n                render = self.aug_trans(render)\n\n                # random blur\n                if self.opt.aug_blur > 0.00001:\n                    blur = GaussianBlur(np.random.uniform(0, self.opt.aug_blur))\n                    render = render.filter(blur)\n\n            intrinsic = np.matmul(trans_intrinsic, np.matmul(uv_intrinsic, scale_intrinsic))\n            calib = torch.Tensor(np.matmul(intrinsic, extrinsic)).float()\n            extrinsic = torch.Tensor(extrinsic).float()\n\n            mask = transforms.Resize(self.load_size)(mask)\n            mask = transforms.ToTensor()(mask).float()\n            mask_list.append(mask)\n\n            render = self.to_tensor(render)\n            render = mask.expand_as(render) * render\n\n            render_list.append(render)\n            calib_list.append(calib)\n            extrinsic_list.append(extrinsic)\n\n        return {\n            'img': torch.stack(render_list, dim=0),\n            'calib': torch.stack(calib_list, dim=0),\n            'extrinsic': torch.stack(extrinsic_list, dim=0),\n            'mask': torch.stack(mask_list, dim=0)\n        }\n\n    def select_sampling_method(self, subject):\n        if not self.is_train:\n            random.seed(1991)\n            np.random.seed(1991)\n            torch.manual_seed(1991)\n        mesh = self.mesh_dic[subject]\n        surface_points, _ = trimesh.sample.sample_surface(mesh, 4 * self.num_sample_inout)\n        sample_points = surface_points + np.random.normal(scale=self.opt.sigma, size=surface_points.shape)\n\n        # add random points within image space\n        length = self.B_MAX - self.B_MIN\n        random_points = np.random.rand(self.num_sample_inout // 4, 3) * length + self.B_MIN\n        sample_points = np.concatenate([sample_points, random_points], 0)\n        np.random.shuffle(sample_points)\n\n        inside = mesh.contains(sample_points)\n        inside_points = sample_points[inside]\n        outside_points = sample_points[np.logical_not(inside)]\n\n        nin = inside_points.shape[0]\n        inside_points = inside_points[\n                        :self.num_sample_inout // 2] if nin > self.num_sample_inout // 2 else inside_points\n        outside_points = outside_points[\n                         :self.num_sample_inout // 2] if nin > self.num_sample_inout // 2 else outside_points[\n                                                                                               :(self.num_sample_inout - nin)]\n\n        samples = np.concatenate([inside_points, outside_points], 0).T\n        labels = np.concatenate([np.ones((1, inside_points.shape[0])), np.zeros((1, outside_points.shape[0]))], 1)\n\n        # save_samples_truncted_prob('out.ply', samples.T, labels.T)\n        # exit()\n\n        samples = torch.Tensor(samples).float()\n        labels = torch.Tensor(labels).float()\n        \n        del mesh\n\n        return {\n            'samples': samples,\n            'labels': labels\n        }\n\n\n    def get_color_sampling(self, subject, yid, pid=0):\n        yaw = self.yaw_list[yid]\n        pitch = self.pitch_list[pid]\n        uv_render_path = os.path.join(self.UV_RENDER, subject, '%d_%d_%02d.jpg' % (yaw, pitch, 0))\n        uv_mask_path = os.path.join(self.UV_MASK, subject, '%02d.png' % (0))\n        uv_pos_path = os.path.join(self.UV_POS, subject, '%02d.exr' % (0))\n        uv_normal_path = os.path.join(self.UV_NORMAL, subject, '%02d.png' % (0))\n\n        # Segmentation mask for the uv render.\n        # [H, W] bool\n        uv_mask = cv2.imread(uv_mask_path)\n        uv_mask = uv_mask[:, :, 0] != 0\n        # UV render. each pixel is the color of the point.\n        # [H, W, 3] 0 ~ 1 float\n        uv_render = cv2.imread(uv_render_path)\n        uv_render = cv2.cvtColor(uv_render, cv2.COLOR_BGR2RGB) / 255.0\n\n        # Normal render. each pixel is the surface normal of the point.\n        # [H, W, 3] -1 ~ 1 float\n        uv_normal = cv2.imread(uv_normal_path)\n        uv_normal = cv2.cvtColor(uv_normal, cv2.COLOR_BGR2RGB) / 255.0\n        uv_normal = 2.0 * uv_normal - 1.0\n        # Position render. each pixel is the xyz coordinates of the point\n        uv_pos = cv2.imread(uv_pos_path, 2 | 4)[:, :, ::-1]\n\n        ### In these few lines we flattern the masks, positions, and normals\n        uv_mask = uv_mask.reshape((-1))\n        uv_pos = uv_pos.reshape((-1, 3))\n        uv_render = uv_render.reshape((-1, 3))\n        uv_normal = uv_normal.reshape((-1, 3))\n\n        surface_points = uv_pos[uv_mask]\n        surface_colors = uv_render[uv_mask]\n        surface_normal = uv_normal[uv_mask]\n\n        if self.num_sample_color:\n            sample_list = random.sample(range(0, surface_points.shape[0] - 1), self.num_sample_color)\n            surface_points = surface_points[sample_list].T\n            surface_colors = surface_colors[sample_list].T\n            surface_normal = surface_normal[sample_list].T\n\n        # Samples are around the true surface with an offset\n        normal = torch.Tensor(surface_normal).float()\n        samples = torch.Tensor(surface_points).float() \\\n                  + torch.normal(mean=torch.zeros((1, normal.size(1))), std=self.opt.sigma).expand_as(normal) * normal\n\n        # Normalized to [-1, 1]\n        rgbs_color = 2.0 * torch.Tensor(surface_colors).float() - 1.0\n\n        return {\n            'color_samples': samples,\n            'rgbs': rgbs_color\n        }\n\n    def get_item(self, index):\n        # In case of a missing file or IO error, switch to a random sample instead\n        # try:\n        sid = index % len(self.subjects)\n        tmp = index // len(self.subjects)\n        yid = tmp % len(self.yaw_list)\n        pid = tmp // len(self.yaw_list)\n\n        # name of the subject 'rp_xxxx_xxx'\n        subject = self.subjects[sid]\n        res = {\n            'name': subject,\n            'mesh_path': os.path.join(self.OBJ, subject + '.obj'),\n            'sid': sid,\n            'yid': yid,\n            'pid': pid,\n            'b_min': self.B_MIN,\n            'b_max': self.B_MAX,\n        }\n        render_data = self.get_render(subject, num_views=self.num_views, yid=yid, pid=pid,\n                                        random_sample=self.opt.random_multiview)\n        res.update(render_data)\n\n        if self.opt.num_sample_inout:\n            sample_data = self.select_sampling_method(subject)\n            res.update(sample_data)\n        \n        # img = np.uint8((np.transpose(render_data['img'][0].numpy(), (1, 2, 0)) * 0.5 + 0.5)[:, :, ::-1] * 255.0)\n        # rot = render_data['calib'][0,:3, :3]\n        # trans = render_data['calib'][0,:3, 3:4]\n        # pts = torch.addmm(trans, rot, sample_data['samples'][:, sample_data['labels'][0] > 0.5])  # [3, N]\n        # pts = 0.5 * (pts.numpy().T + 1.0) * render_data['img'].size(2)\n        # for p in pts:\n        #     img = cv2.circle(img, (p[0], p[1]), 2, (0,255,0), -1)\n        # cv2.imshow('test', img)\n        # cv2.waitKey(1)\n\n        if self.num_sample_color:\n            color_data = self.get_color_sampling(subject, yid=yid, pid=pid)\n            res.update(color_data)\n        return res\n        # except Exception as e:\n        #     print(e)\n        #     return self.get_item(index=random.randint(0, self.__len__() - 1))\n\n    def __getitem__(self, index):\n        return self.get_item(index)"""
lib/data/__init__.py,0,b'from .EvalDataset import EvalDataset\nfrom .TrainDataset import TrainDataset'
lib/model/BasePIFuNet.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..geometry import index, orthogonal, perspective\n\nclass BasePIFuNet(nn.Module):\n    def __init__(self,\n                 projection_mode=\'orthogonal\',\n                 error_term=nn.MSELoss(),\n                 ):\n        """"""\n        :param projection_mode:\n        Either orthogonal or perspective.\n        It will call the corresponding function for projection.\n        :param error_term:\n        nn Loss between the predicted [B, Res, N] and the label [B, Res, N]\n        """"""\n        super(BasePIFuNet, self).__init__()\n        self.name = \'base\'\n\n        self.error_term = error_term\n\n        self.index = index\n        self.projection = orthogonal if projection_mode == \'orthogonal\' else perspective\n\n        self.preds = None\n        self.labels = None\n\n    def forward(self, points, images, calibs, transforms=None):\n        \'\'\'\n        :param points: [B, 3, N] world space coordinates of points\n        :param images: [B, C, H, W] input images\n        :param calibs: [B, 3, 4] calibration matrices for each image\n        :param transforms: Optional [B, 2, 3] image space coordinate transforms\n        :return: [B, Res, N] predictions for each point\n        \'\'\'\n        self.filter(images)\n        self.query(points, calibs, transforms)\n        return self.get_preds()\n\n    def filter(self, images):\n        \'\'\'\n        Filter the input images\n        store all intermediate features.\n        :param images: [B, C, H, W] input images\n        \'\'\'\n        None\n\n    def query(self, points, calibs, transforms=None, labels=None):\n        \'\'\'\n        Given 3D points, query the network predictions for each point.\n        Image features should be pre-computed before this call.\n        store all intermediate features.\n        query() function may behave differently during training/testing.\n        :param points: [B, 3, N] world space coordinates of points\n        :param calibs: [B, 3, 4] calibration matrices for each image\n        :param transforms: Optional [B, 2, 3] image space coordinate transforms\n        :param labels: Optional [B, Res, N] gt labeling\n        :return: [B, Res, N] predictions for each point\n        \'\'\'\n        None\n\n    def get_preds(self):\n        \'\'\'\n        Get the predictions from the last query\n        :return: [B, Res, N] network prediction for the last query\n        \'\'\'\n        return self.preds\n\n    def get_error(self):\n        \'\'\'\n        Get the network loss from the last query\n        :return: loss term\n        \'\'\'\n        return self.error_term(self.preds, self.labels)\n'"
lib/model/ConvFilters.py,8,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models.resnet as resnet\nimport torchvision.models.vgg as vgg\n\n\nclass MultiConv(nn.Module):\n    def __init__(self, filter_channels):\n        super(MultiConv, self).__init__()\n        self.filters = []\n\n        for l in range(0, len(filter_channels) - 1):\n            self.filters.append(\n                nn.Conv2d(filter_channels[l], filter_channels[l + 1], kernel_size=4, stride=2))\n            self.add_module(""conv%d"" % l, self.filters[l])\n\n    def forward(self, image):\n        \'\'\'\n        :param image: [BxC_inxHxW] tensor of input image\n        :return: list of [BxC_outxHxW] tensors of output features\n        \'\'\'\n        y = image\n        # y = F.relu(self.bn0(self.conv0(y)), True)\n        feat_pyramid = [y]\n        for i, f in enumerate(self.filters):\n            y = f(y)\n            if i != len(self.filters) - 1:\n                y = F.leaky_relu(y)\n            # y = F.max_pool2d(y, kernel_size=2, stride=2)\n            feat_pyramid.append(y)\n        return feat_pyramid\n\n\nclass Vgg16(torch.nn.Module):\n    def __init__(self):\n        super(Vgg16, self).__init__()\n        vgg_pretrained_features = vgg.vgg16(pretrained=True).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n\n        for x in range(4):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(4, 9):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(9, 16):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(16, 23):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(23, 30):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n\n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu1_2 = h\n        h = self.slice2(h)\n        h_relu2_2 = h\n        h = self.slice3(h)\n        h_relu3_3 = h\n        h = self.slice4(h)\n        h_relu4_3 = h\n        h = self.slice5(h)\n        h_relu5_3 = h\n\n        return [h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3]\n\n\nclass ResNet(nn.Module):\n    def __init__(self, model=\'resnet18\'):\n        super(ResNet, self).__init__()\n\n        if model == \'resnet18\':\n            net = resnet.resnet18(pretrained=True)\n        elif model == \'resnet34\':\n            net = resnet.resnet34(pretrained=True)\n        elif model == \'resnet50\':\n            net = resnet.resnet50(pretrained=True)\n        else:\n            raise NameError(\'Unknown Fan Filter setting!\')\n\n        self.conv1 = net.conv1\n\n        self.pool = net.maxpool\n        self.layer0 = nn.Sequential(net.conv1, net.bn1, net.relu)\n        self.layer1 = net.layer1\n        self.layer2 = net.layer2\n        self.layer3 = net.layer3\n        self.layer4 = net.layer4\n\n    def forward(self, image):\n        \'\'\'\n        :param image: [BxC_inxHxW] tensor of input image\n        :return: list of [BxC_outxHxW] tensors of output features\n        \'\'\'\n\n        y = image\n        feat_pyramid = []\n        y = self.layer0(y)\n        feat_pyramid.append(y)\n        y = self.layer1(self.pool(y))\n        feat_pyramid.append(y)\n        y = self.layer2(y)\n        feat_pyramid.append(y)\n        y = self.layer3(y)\n        feat_pyramid.append(y)\n        y = self.layer4(y)\n        feat_pyramid.append(y)\n\n        return feat_pyramid\n'"
lib/model/ConvPIFuNet.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .BasePIFuNet import BasePIFuNet\nfrom .SurfaceClassifier import SurfaceClassifier\nfrom .DepthNormalizer import DepthNormalizer\nfrom .ConvFilters import *\nfrom ..net_util import init_net\n\nclass ConvPIFuNet(BasePIFuNet):\n    '''\n    Conv Piximp network is the standard 3-phase network that we will use.\n    The image filter is a pure multi-layer convolutional network,\n    while during feature extraction phase all features in the pyramid at the projected location\n    will be aggregated.\n    It does the following:\n        1. Compute image feature pyramids and store it in self.im_feat_list\n        2. Calculate calibration and indexing on each of the feat, and append them together\n        3. Classification.\n    '''\n\n    def __init__(self,\n                 opt,\n                 projection_mode='orthogonal',\n                 error_term=nn.MSELoss(),\n                 ):\n        super(ConvPIFuNet, self).__init__(\n            projection_mode=projection_mode,\n            error_term=error_term)\n\n        self.name = 'convpifu'\n\n        self.opt = opt\n        self.num_views = self.opt.num_views\n\n        self.image_filter = self.define_imagefilter(opt)\n\n        self.surface_classifier = SurfaceClassifier(\n            filter_channels=self.opt.mlp_dim,\n            num_views=self.opt.num_views,\n            no_residual=self.opt.no_residual,\n            last_op=nn.Sigmoid())\n\n        self.normalizer = DepthNormalizer(opt)\n\n        # This is a list of [B x Feat_i x H x W] features\n        self.im_feat_list = []\n\n        init_net(self)\n\n    def define_imagefilter(self, opt):\n        net = None\n        if opt.netIMF == 'multiconv':\n            net = MultiConv(opt.enc_dim)\n        elif 'resnet' in opt.netIMF:\n            net = ResNet(model=opt.netIMF)\n        elif opt.netIMF == 'vgg16':\n            net = Vgg16()\n        else:\n            raise NotImplementedError('model name [%s] is not recognized' % opt.imf_type)\n\n        return net\n\n    def filter(self, images):\n        '''\n        Filter the input images\n        store all intermediate features.\n        :param images: [B, C, H, W] input images\n        '''\n        self.im_feat_list = self.image_filter(images)\n\n    def query(self, points, calibs, transforms=None, labels=None):\n        '''\n        Given 3D points, query the network predictions for each point.\n        Image features should be pre-computed before this call.\n        store all intermediate features.\n        query() function may behave differently during training/testing.\n        :param points: [B, 3, N] world space coordinates of points\n        :param calibs: [B, 3, 4] calibration matrices for each image\n        :param transforms: Optional [B, 2, 3] image space coordinate transforms\n        :param labels: Optional [B, Res, N] gt labeling\n        :return: [B, Res, N] predictions for each point\n        '''\n        if labels is not None:\n            self.labels = labels\n\n        xyz = self.projection(points, calibs, transforms)\n        xy = xyz[:, :2, :]\n        z = xyz[:, 2:3, :]\n\n        z_feat = self.normalizer(z)\n\n        # This is a list of [B, Feat_i, N] features\n        point_local_feat_list = [self.index(im_feat, xy) for im_feat in self.im_feat_list]\n        point_local_feat_list.append(z_feat)\n        # [B, Feat_all, N]\n        point_local_feat = torch.cat(point_local_feat_list, 1)\n\n        self.preds = self.surface_classifier(point_local_feat)\n"""
lib/model/DepthNormalizer.py,2,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass DepthNormalizer(nn.Module):\n    def __init__(self, opt):\n        super(DepthNormalizer, self).__init__()\n        self.opt = opt\n\n    def forward(self, z, calibs=None, index_feat=None):\n        '''\n        Normalize z_feature\n        :param z_feat: [B, 1, N] depth value for z in the image coordinate system\n        :return:\n        '''\n        z_feat = z * (self.opt.loadSize // 2) / self.opt.z_size\n        return z_feat\n"""
lib/model/HGFilters.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom ..net_util import *\n\n\nclass HourGlass(nn.Module):\n    def __init__(self, num_modules, depth, num_features, norm=\'batch\'):\n        super(HourGlass, self).__init__()\n        self.num_modules = num_modules\n        self.depth = depth\n        self.features = num_features\n        self.norm = norm\n\n        self._generate_network(self.depth)\n\n    def _generate_network(self, level):\n        self.add_module(\'b1_\' + str(level), ConvBlock(self.features, self.features, norm=self.norm))\n\n        self.add_module(\'b2_\' + str(level), ConvBlock(self.features, self.features, norm=self.norm))\n\n        if level > 1:\n            self._generate_network(level - 1)\n        else:\n            self.add_module(\'b2_plus_\' + str(level), ConvBlock(self.features, self.features, norm=self.norm))\n\n        self.add_module(\'b3_\' + str(level), ConvBlock(self.features, self.features, norm=self.norm))\n\n    def _forward(self, level, inp):\n        # Upper branch\n        up1 = inp\n        up1 = self._modules[\'b1_\' + str(level)](up1)\n\n        # Lower branch\n        low1 = F.avg_pool2d(inp, 2, stride=2)\n        low1 = self._modules[\'b2_\' + str(level)](low1)\n\n        if level > 1:\n            low2 = self._forward(level - 1, low1)\n        else:\n            low2 = low1\n            low2 = self._modules[\'b2_plus_\' + str(level)](low2)\n\n        low3 = low2\n        low3 = self._modules[\'b3_\' + str(level)](low3)\n\n        # NOTE: for newer PyTorch (1.3~), it seems that training results are degraded due to implementation diff in F.grid_sample\n        # if the pretrained model behaves weirdly, switch with the commented line.\n        # NOTE: I also found that ""bicubic"" works better.\n        up2 = F.interpolate(low3, scale_factor=2, mode=\'bicubic\', align_corners=True)\n        # up2 = F.interpolate(low3, scale_factor=2, mode=\'nearest)\n\n        return up1 + up2\n\n    def forward(self, x):\n        return self._forward(self.depth, x)\n\n\nclass HGFilter(nn.Module):\n    def __init__(self, opt):\n        super(HGFilter, self).__init__()\n        self.num_modules = opt.num_stack\n\n        self.opt = opt\n\n        # Base part\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n\n        if self.opt.norm == \'batch\':\n            self.bn1 = nn.BatchNorm2d(64)\n        elif self.opt.norm == \'group\':\n            self.bn1 = nn.GroupNorm(32, 64)\n\n        if self.opt.hg_down == \'conv64\':\n            self.conv2 = ConvBlock(64, 64, self.opt.norm)\n            self.down_conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n        elif self.opt.hg_down == \'conv128\':\n            self.conv2 = ConvBlock(64, 128, self.opt.norm)\n            self.down_conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)\n        elif self.opt.hg_down == \'ave_pool\':\n            self.conv2 = ConvBlock(64, 128, self.opt.norm)\n        else:\n            raise NameError(\'Unknown Fan Filter setting!\')\n\n        self.conv3 = ConvBlock(128, 128, self.opt.norm)\n        self.conv4 = ConvBlock(128, 256, self.opt.norm)\n\n        # Stacking part\n        for hg_module in range(self.num_modules):\n            self.add_module(\'m\' + str(hg_module), HourGlass(1, opt.num_hourglass, 256, self.opt.norm))\n\n            self.add_module(\'top_m_\' + str(hg_module), ConvBlock(256, 256, self.opt.norm))\n            self.add_module(\'conv_last\' + str(hg_module),\n                            nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\n            if self.opt.norm == \'batch\':\n                self.add_module(\'bn_end\' + str(hg_module), nn.BatchNorm2d(256))\n            elif self.opt.norm == \'group\':\n                self.add_module(\'bn_end\' + str(hg_module), nn.GroupNorm(32, 256))\n                \n            self.add_module(\'l\' + str(hg_module), nn.Conv2d(256,\n                                                            opt.hourglass_dim, kernel_size=1, stride=1, padding=0))\n\n            if hg_module < self.num_modules - 1:\n                self.add_module(\n                    \'bl\' + str(hg_module), nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\n                self.add_module(\'al\' + str(hg_module), nn.Conv2d(opt.hourglass_dim,\n                                                                 256, kernel_size=1, stride=1, padding=0))\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)), True)\n        tmpx = x\n        if self.opt.hg_down == \'ave_pool\':\n            x = F.avg_pool2d(self.conv2(x), 2, stride=2)\n        elif self.opt.hg_down in [\'conv64\', \'conv128\']:\n            x = self.conv2(x)\n            x = self.down_conv2(x)\n        else:\n            raise NameError(\'Unknown Fan Filter setting!\')\n\n        normx = x\n\n        x = self.conv3(x)\n        x = self.conv4(x)\n\n        previous = x\n\n        outputs = []\n        for i in range(self.num_modules):\n            hg = self._modules[\'m\' + str(i)](previous)\n\n            ll = hg\n            ll = self._modules[\'top_m_\' + str(i)](ll)\n\n            ll = F.relu(self._modules[\'bn_end\' + str(i)]\n                        (self._modules[\'conv_last\' + str(i)](ll)), True)\n\n            # Predict heatmaps\n            tmp_out = self._modules[\'l\' + str(i)](ll)\n            outputs.append(tmp_out)\n\n            if i < self.num_modules - 1:\n                ll = self._modules[\'bl\' + str(i)](ll)\n                tmp_out_ = self._modules[\'al\' + str(i)](tmp_out)\n                previous = previous + ll + tmp_out_\n\n        return outputs, tmpx.detach(), normx\n'"
lib/model/HGPIFuNet.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .BasePIFuNet import BasePIFuNet\nfrom .SurfaceClassifier import SurfaceClassifier\nfrom .DepthNormalizer import DepthNormalizer\nfrom .HGFilters import *\nfrom ..net_util import init_net\n\n\nclass HGPIFuNet(BasePIFuNet):\n    '''\n    HG PIFu network uses Hourglass stacks as the image filter.\n    It does the following:\n        1. Compute image feature stacks and store it in self.im_feat_list\n            self.im_feat_list[-1] is the last stack (output stack)\n        2. Calculate calibration\n        3. If training, it index on every intermediate stacks,\n            If testing, it index on the last stack.\n        4. Classification.\n        5. During training, error is calculated on all stacks.\n    '''\n\n    def __init__(self,\n                 opt,\n                 projection_mode='orthogonal',\n                 error_term=nn.MSELoss(),\n                 ):\n        super(HGPIFuNet, self).__init__(\n            projection_mode=projection_mode,\n            error_term=error_term)\n\n        self.name = 'hgpifu'\n\n        self.opt = opt\n        self.num_views = self.opt.num_views\n\n        self.image_filter = HGFilter(opt)\n\n        self.surface_classifier = SurfaceClassifier(\n            filter_channels=self.opt.mlp_dim,\n            num_views=self.opt.num_views,\n            no_residual=self.opt.no_residual,\n            last_op=nn.Sigmoid())\n\n        self.normalizer = DepthNormalizer(opt)\n\n        # This is a list of [B x Feat_i x H x W] features\n        self.im_feat_list = []\n        self.tmpx = None\n        self.normx = None\n\n        self.intermediate_preds_list = []\n\n        init_net(self)\n\n    def filter(self, images):\n        '''\n        Filter the input images\n        store all intermediate features.\n        :param images: [B, C, H, W] input images\n        '''\n        self.im_feat_list, self.tmpx, self.normx = self.image_filter(images)\n        # If it is not in training, only produce the last im_feat\n        if not self.training:\n            self.im_feat_list = [self.im_feat_list[-1]]\n\n    def query(self, points, calibs, transforms=None, labels=None):\n        '''\n        Given 3D points, query the network predictions for each point.\n        Image features should be pre-computed before this call.\n        store all intermediate features.\n        query() function may behave differently during training/testing.\n        :param points: [B, 3, N] world space coordinates of points\n        :param calibs: [B, 3, 4] calibration matrices for each image\n        :param transforms: Optional [B, 2, 3] image space coordinate transforms\n        :param labels: Optional [B, Res, N] gt labeling\n        :return: [B, Res, N] predictions for each point\n        '''\n        if labels is not None:\n            self.labels = labels\n\n        xyz = self.projection(points, calibs, transforms)\n        xy = xyz[:, :2, :]\n        z = xyz[:, 2:3, :]\n\n        in_img = (xy[:, 0] >= -1.0) & (xy[:, 0] <= 1.0) & (xy[:, 1] >= -1.0) & (xy[:, 1] <= 1.0)\n\n        z_feat = self.normalizer(z, calibs=calibs)\n\n        if self.opt.skip_hourglass:\n            tmpx_local_feature = self.index(self.tmpx, xy)\n\n        self.intermediate_preds_list = []\n\n        for im_feat in self.im_feat_list:\n            # [B, Feat_i + z, N]\n            point_local_feat_list = [self.index(im_feat, xy), z_feat]\n\n            if self.opt.skip_hourglass:\n                point_local_feat_list.append(tmpx_local_feature)\n\n            point_local_feat = torch.cat(point_local_feat_list, 1)\n\n            # out of image plane is always set to 0\n            pred = in_img[:,None].float() * self.surface_classifier(point_local_feat)\n            self.intermediate_preds_list.append(pred)\n\n        self.preds = self.intermediate_preds_list[-1]\n\n    def get_im_feat(self):\n        '''\n        Get the image filter\n        :return: [B, C_feat, H, W] image feature after filtering\n        '''\n        return self.im_feat_list[-1]\n\n    def get_error(self):\n        '''\n        Hourglass has its own intermediate supervision scheme\n        '''\n        error = 0\n        for preds in self.intermediate_preds_list:\n            error += self.error_term(preds, self.labels)\n        error /= len(self.intermediate_preds_list)\n        \n        return error\n\n    def forward(self, images, points, calibs, transforms=None, labels=None):\n        # Get image feature\n        self.filter(images)\n\n        # Phase 2: point query\n        self.query(points=points, calibs=calibs, transforms=transforms, labels=labels)\n\n        # get the prediction\n        res = self.get_preds()\n        \n        # get the error\n        error = self.get_error()\n\n        return res, error"""
lib/model/ResBlkPIFuNet.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .BasePIFuNet import BasePIFuNet\nimport functools\nfrom .SurfaceClassifier import SurfaceClassifier\nfrom .DepthNormalizer import DepthNormalizer\nfrom ..net_util import *\n\n\nclass ResBlkPIFuNet(BasePIFuNet):\n    def __init__(self, opt,\n                 projection_mode=\'orthogonal\'):\n        if opt.color_loss_type == \'l1\':\n            error_term = nn.L1Loss()\n        elif opt.color_loss_type == \'mse\':\n            error_term = nn.MSELoss()\n\n        super(ResBlkPIFuNet, self).__init__(\n            projection_mode=projection_mode,\n            error_term=error_term)\n\n        self.name = \'respifu\'\n        self.opt = opt\n\n        norm_type = get_norm_layer(norm_type=opt.norm_color)\n        self.image_filter = ResnetFilter(opt, norm_layer=norm_type)\n\n        self.surface_classifier = SurfaceClassifier(\n            filter_channels=self.opt.mlp_dim_color,\n            num_views=self.opt.num_views,\n            no_residual=self.opt.no_residual,\n            last_op=nn.Tanh())\n\n        self.normalizer = DepthNormalizer(opt)\n\n        init_net(self)\n\n    def filter(self, images):\n        \'\'\'\n        Filter the input images\n        store all intermediate features.\n        :param images: [B, C, H, W] input images\n        \'\'\'\n        self.im_feat = self.image_filter(images)\n\n    def attach(self, im_feat):\n        self.im_feat = torch.cat([im_feat, self.im_feat], 1)\n\n    def query(self, points, calibs, transforms=None, labels=None):\n        \'\'\'\n        Given 3D points, query the network predictions for each point.\n        Image features should be pre-computed before this call.\n        store all intermediate features.\n        query() function may behave differently during training/testing.\n        :param points: [B, 3, N] world space coordinates of points\n        :param calibs: [B, 3, 4] calibration matrices for each image\n        :param transforms: Optional [B, 2, 3] image space coordinate transforms\n        :param labels: Optional [B, Res, N] gt labeling\n        :return: [B, Res, N] predictions for each point\n        \'\'\'\n        if labels is not None:\n            self.labels = labels\n\n        xyz = self.projection(points, calibs, transforms)\n        xy = xyz[:, :2, :]\n        z = xyz[:, 2:3, :]\n\n        z_feat = self.normalizer(z)\n\n        # This is a list of [B, Feat_i, N] features\n        point_local_feat_list = [self.index(self.im_feat, xy), z_feat]\n        # [B, Feat_all, N]\n        point_local_feat = torch.cat(point_local_feat_list, 1)\n\n        self.preds = self.surface_classifier(point_local_feat)\n\n    def forward(self, images, im_feat, points, calibs, transforms=None, labels=None):\n        self.filter(images)\n\n        self.attach(im_feat)\n\n        self.query(points, calibs, transforms, labels)\n\n        res = self.get_preds()\n        error = self.get_error()\n\n        return res, error\n\nclass ResnetBlock(nn.Module):\n    """"""Define a Resnet block""""""\n\n    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias, last=False):\n        """"""Initialize the Resnet block\n        A resnet block is a conv block with skip connections\n        We construct a conv block with build_conv_block function,\n        and implement skip connections in <forward> function.\n        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n        """"""\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias, last)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias, last=False):\n        """"""Construct a convolutional block.\n        Parameters:\n            dim (int)           -- the number of channels in the conv layer.\n            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers.\n            use_bias (bool)     -- if the conv layer uses bias or not\n        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n        """"""\n        conv_block = []\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n        if use_dropout:\n            conv_block += [nn.Dropout(0.5)]\n\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n        if last:\n            conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias)]\n        else:\n            conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        """"""Forward function (with skip connections)""""""\n        out = x + self.conv_block(x)  # add skip connections\n        return out\n\n\nclass ResnetFilter(nn.Module):\n    """"""Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n    We adapt Torch code and idea from Justin Johnson\'s neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n    """"""\n\n    def __init__(self, opt, input_nc=3, output_nc=256, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False,\n                 n_blocks=6, padding_type=\'reflect\'):\n        """"""Construct a Resnet-based generator\n        Parameters:\n            input_nc (int)      -- the number of channels in input images\n            output_nc (int)     -- the number of channels in output images\n            ngf (int)           -- the number of filters in the last conv layer\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers\n            n_blocks (int)      -- the number of ResNet blocks\n            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n        """"""\n        assert (n_blocks >= 0)\n        super(ResnetFilter, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):  # add downsampling layers\n            mult = 2 ** i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2 ** n_downsampling\n        for i in range(n_blocks):  # add ResNet blocks\n            if i == n_blocks - 1:\n                model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer,\n                                      use_dropout=use_dropout, use_bias=use_bias, last=True)]\n            else:\n                model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer,\n                                      use_dropout=use_dropout, use_bias=use_bias)]\n\n        if opt.use_tanh:\n            model += [nn.Tanh()]\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        """"""Standard forward""""""\n        return self.model(input)\n'"
lib/model/SurfaceClassifier.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SurfaceClassifier(nn.Module):\n    def __init__(self, filter_channels, num_views=1, no_residual=True, last_op=None):\n        super(SurfaceClassifier, self).__init__()\n\n        self.filters = []\n        self.num_views = num_views\n        self.no_residual = no_residual\n        filter_channels = filter_channels\n        self.last_op = last_op\n\n        if self.no_residual:\n            for l in range(0, len(filter_channels) - 1):\n                self.filters.append(nn.Conv1d(\n                    filter_channels[l],\n                    filter_channels[l + 1],\n                    1))\n                self.add_module(""conv%d"" % l, self.filters[l])\n        else:\n            for l in range(0, len(filter_channels) - 1):\n                if 0 != l:\n                    self.filters.append(\n                        nn.Conv1d(\n                            filter_channels[l] + filter_channels[0],\n                            filter_channels[l + 1],\n                            1))\n                else:\n                    self.filters.append(nn.Conv1d(\n                        filter_channels[l],\n                        filter_channels[l + 1],\n                        1))\n\n                self.add_module(""conv%d"" % l, self.filters[l])\n\n    def forward(self, feature):\n        \'\'\'\n\n        :param feature: list of [BxC_inxHxW] tensors of image features\n        :param xy: [Bx3xN] tensor of (x,y) coodinates in the image plane\n        :return: [BxC_outxN] tensor of features extracted at the coordinates\n        \'\'\'\n\n        y = feature\n        tmpy = feature\n        for i, f in enumerate(self.filters):\n            if self.no_residual:\n                y = self._modules[\'conv\' + str(i)](y)\n            else:\n                y = self._modules[\'conv\' + str(i)](\n                    y if i == 0\n                    else torch.cat([y, tmpy], 1)\n                )\n            if i != len(self.filters) - 1:\n                y = F.leaky_relu(y)\n\n            if self.num_views > 1 and i == len(self.filters) // 2:\n                y = y.view(\n                    -1, self.num_views, y.shape[1], y.shape[2]\n                ).mean(dim=1)\n                tmpy = feature.view(\n                    -1, self.num_views, feature.shape[1], feature.shape[2]\n                ).mean(dim=1)\n\n        if self.last_op:\n            y = self.last_op(y)\n\n        return y\n'"
lib/model/VhullPIFuNet.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom .BasePIFuNet import BasePIFuNet\n\n\nclass VhullPIFuNet(BasePIFuNet):\n    '''\n    Vhull Piximp network is a minimal network demonstrating how the template works\n    also, it helps debugging the training/test schemes\n    It does the following:\n        1. Compute the masks of images and stores under self.im_feats\n        2. Calculate calibration and indexing\n        3. Return if the points fall into the intersection of all masks\n    '''\n\n    def __init__(self,\n                 num_views,\n                 projection_mode='orthogonal',\n                 error_term=nn.MSELoss(),\n                 ):\n        super(VhullPIFuNet, self).__init__(\n            projection_mode=projection_mode,\n            error_term=error_term)\n        self.name = 'vhull'\n\n        self.num_views = num_views\n\n        self.im_feat = None\n\n    def filter(self, images):\n        '''\n        Filter the input images\n        store all intermediate features.\n        :param images: [B, C, H, W] input images\n        '''\n        # If the image has alpha channel, use the alpha channel\n        if images.shape[1] > 3:\n            self.im_feat = images[:, 3:4, :, :]\n        # Else, tell if it's not white\n        else:\n            self.im_feat = images[:, 0:1, :, :]\n\n    def query(self, points, calibs, transforms=None, labels=None):\n        '''\n        Given 3D points, query the network predictions for each point.\n        Image features should be pre-computed before this call.\n        store all intermediate features.\n        query() function may behave differently during training/testing.\n        :param points: [B, 3, N] world space coordinates of points\n        :param calibs: [B, 3, 4] calibration matrices for each image\n        :param transforms: Optional [B, 2, 3] image space coordinate transforms\n        :param labels: Optional [B, Res, N] gt labeling\n        :return: [B, Res, N] predictions for each point\n        '''\n        if labels is not None:\n            self.labels = labels\n\n        xyz = self.projection(points, calibs, transforms)\n        xy = xyz[:, :2, :]\n\n        point_local_feat = self.index(self.im_feat, xy)\n        local_shape = point_local_feat.shape\n        point_feat = point_local_feat.view(\n            local_shape[0] // self.num_views,\n            local_shape[1] * self.num_views,\n            -1)\n        pred = torch.prod(point_feat, dim=1)\n\n        self.preds = pred.unsqueeze(1)\n"""
lib/model/__init__.py,0,b'from .BasePIFuNet import BasePIFuNet\nfrom .VhullPIFuNet import VhullPIFuNet\nfrom .ConvPIFuNet import ConvPIFuNet\nfrom .HGPIFuNet import HGPIFuNet\nfrom .ResBlkPIFuNet import ResBlkPIFuNet\n'
lib/renderer/__init__.py,0,b''
lib/renderer/camera.py,0,"b""import cv2\r\nimport numpy as np\r\n\r\nfrom .glm import ortho\r\n\r\n\r\nclass Camera:\r\n    def __init__(self, width=1600, height=1200):\r\n        # Focal Length\r\n        # equivalent 50mm\r\n        focal = np.sqrt(width * width + height * height)\r\n        self.focal_x = focal\r\n        self.focal_y = focal\r\n        # Principal Point Offset\r\n        self.principal_x = width / 2\r\n        self.principal_y = height / 2\r\n        # Axis Skew\r\n        self.skew = 0\r\n        # Image Size\r\n        self.width = width\r\n        self.height = height\r\n\r\n        self.near = 1\r\n        self.far = 10\r\n\r\n        # Camera Center\r\n        self.center = np.array([0, 0, 1.6])\r\n        self.direction = np.array([0, 0, -1])\r\n        self.right = np.array([1, 0, 0])\r\n        self.up = np.array([0, 1, 0])\r\n\r\n        self.ortho_ratio = None\r\n\r\n    def sanity_check(self):\r\n        self.center = self.center.reshape([-1])\r\n        self.direction = self.direction.reshape([-1])\r\n        self.right = self.right.reshape([-1])\r\n        self.up = self.up.reshape([-1])\r\n\r\n        assert len(self.center) == 3\r\n        assert len(self.direction) == 3\r\n        assert len(self.right) == 3\r\n        assert len(self.up) == 3\r\n\r\n    @staticmethod\r\n    def normalize_vector(v):\r\n        v_norm = np.linalg.norm(v)\r\n        return v if v_norm == 0 else v / v_norm\r\n\r\n    def get_real_z_value(self, z):\r\n        z_near = self.near\r\n        z_far = self.far\r\n        z_n = 2.0 * z - 1.0\r\n        z_e = 2.0 * z_near * z_far / (z_far + z_near - z_n * (z_far - z_near))\r\n        return z_e\r\n\r\n    def get_rotation_matrix(self):\r\n        rot_mat = np.eye(3)\r\n        s = self.right\r\n        s = self.normalize_vector(s)\r\n        rot_mat[0, :] = s\r\n        u = self.up\r\n        u = self.normalize_vector(u)\r\n        rot_mat[1, :] = -u\r\n        rot_mat[2, :] = self.normalize_vector(self.direction)\r\n\r\n        return rot_mat\r\n\r\n    def get_translation_vector(self):\r\n        rot_mat = self.get_rotation_matrix()\r\n        trans = -np.dot(rot_mat, self.center)\r\n        return trans\r\n\r\n    def get_intrinsic_matrix(self):\r\n        int_mat = np.eye(3)\r\n\r\n        int_mat[0, 0] = self.focal_x\r\n        int_mat[1, 1] = self.focal_y\r\n        int_mat[0, 1] = self.skew\r\n        int_mat[0, 2] = self.principal_x\r\n        int_mat[1, 2] = self.principal_y\r\n\r\n        return int_mat\r\n\r\n    def get_projection_matrix(self):\r\n        ext_mat = self.get_extrinsic_matrix()\r\n        int_mat = self.get_intrinsic_matrix()\r\n\r\n        return np.matmul(int_mat, ext_mat)\r\n\r\n    def get_extrinsic_matrix(self):\r\n        rot_mat = self.get_rotation_matrix()\r\n        int_mat = self.get_intrinsic_matrix()\r\n        trans = self.get_translation_vector()\r\n\r\n        extrinsic = np.eye(4)\r\n        extrinsic[:3, :3] = rot_mat\r\n        extrinsic[:3, 3] = trans\r\n\r\n        return extrinsic[:3, :]\r\n\r\n    def set_rotation_matrix(self, rot_mat):\r\n        self.direction = rot_mat[2, :]\r\n        self.up = -rot_mat[1, :]\r\n        self.right = rot_mat[0, :]\r\n\r\n    def set_intrinsic_matrix(self, int_mat):\r\n        self.focal_x = int_mat[0, 0]\r\n        self.focal_y = int_mat[1, 1]\r\n        self.skew = int_mat[0, 1]\r\n        self.principal_x = int_mat[0, 2]\r\n        self.principal_y = int_mat[1, 2]\r\n\r\n    def set_projection_matrix(self, proj_mat):\r\n        res = cv2.decomposeProjectionMatrix(proj_mat)\r\n        int_mat, rot_mat, camera_center_homo = res[0], res[1], res[2]\r\n        camera_center = camera_center_homo[0:3] / camera_center_homo[3]\r\n        camera_center = camera_center.reshape(-1)\r\n        int_mat = int_mat / int_mat[2][2]\r\n\r\n        self.set_intrinsic_matrix(int_mat)\r\n        self.set_rotation_matrix(rot_mat)\r\n        self.center = camera_center\r\n\r\n        self.sanity_check()\r\n\r\n    def get_gl_matrix(self):\r\n        z_near = self.near\r\n        z_far = self.far\r\n        rot_mat = self.get_rotation_matrix()\r\n        int_mat = self.get_intrinsic_matrix()\r\n        trans = self.get_translation_vector()\r\n\r\n        extrinsic = np.eye(4)\r\n        extrinsic[:3, :3] = rot_mat\r\n        extrinsic[:3, 3] = trans\r\n        axis_adj = np.eye(4)\r\n        axis_adj[2, 2] = -1\r\n        axis_adj[1, 1] = -1\r\n        model_view = np.matmul(axis_adj, extrinsic)\r\n\r\n        projective = np.zeros([4, 4])\r\n        projective[:2, :2] = int_mat[:2, :2]\r\n        projective[:2, 2:3] = -int_mat[:2, 2:3]\r\n        projective[3, 2] = -1\r\n        projective[2, 2] = (z_near + z_far)\r\n        projective[2, 3] = (z_near * z_far)\r\n\r\n        if self.ortho_ratio is None:\r\n            ndc = ortho(0, self.width, 0, self.height, z_near, z_far)\r\n            perspective = np.matmul(ndc, projective)\r\n        else:\r\n            perspective = ortho(-self.width * self.ortho_ratio / 2, self.width * self.ortho_ratio / 2,\r\n                                -self.height * self.ortho_ratio / 2, self.height * self.ortho_ratio / 2,\r\n                                z_near, z_far)\r\n\r\n        return perspective, model_view\r\n\r\n\r\ndef KRT_from_P(proj_mat, normalize_K=True):\r\n    res = cv2.decomposeProjectionMatrix(proj_mat)\r\n    K, Rot, camera_center_homog = res[0], res[1], res[2]\r\n    camera_center = camera_center_homog[0:3] / camera_center_homog[3]\r\n    trans = -Rot.dot(camera_center)\r\n    if normalize_K:\r\n        K = K / K[2][2]\r\n    return K, Rot, trans\r\n\r\n\r\ndef MVP_from_P(proj_mat, width, height, near=0.1, far=10000):\r\n    '''\r\n    Convert OpenCV camera calibration matrix to OpenGL projection and model view matrix\r\n    :param proj_mat: OpenCV camera projeciton matrix\r\n    :param width: Image width\r\n    :param height: Image height\r\n    :param near: Z near value\r\n    :param far: Z far value\r\n    :return: OpenGL projection matrix and model view matrix\r\n    '''\r\n    res = cv2.decomposeProjectionMatrix(proj_mat)\r\n    K, Rot, camera_center_homog = res[0], res[1], res[2]\r\n    camera_center = camera_center_homog[0:3] / camera_center_homog[3]\r\n    trans = -Rot.dot(camera_center)\r\n    K = K / K[2][2]\r\n\r\n    extrinsic = np.eye(4)\r\n    extrinsic[:3, :3] = Rot\r\n    extrinsic[:3, 3:4] = trans\r\n    axis_adj = np.eye(4)\r\n    axis_adj[2, 2] = -1\r\n    axis_adj[1, 1] = -1\r\n    model_view = np.matmul(axis_adj, extrinsic)\r\n\r\n    zFar = far\r\n    zNear = near\r\n    projective = np.zeros([4, 4])\r\n    projective[:2, :2] = K[:2, :2]\r\n    projective[:2, 2:3] = -K[:2, 2:3]\r\n    projective[3, 2] = -1\r\n    projective[2, 2] = (zNear + zFar)\r\n    projective[2, 3] = (zNear * zFar)\r\n\r\n    ndc = ortho(0, width, 0, height, zNear, zFar)\r\n\r\n    perspective = np.matmul(ndc, projective)\r\n\r\n    return perspective, model_view\r\n"""
lib/renderer/glm.py,0,"b'import numpy as np\n\n\ndef vec3(x, y, z):\n    return np.array([x, y, z], dtype=np.float32)\n\n\ndef radians(v):\n    return np.radians(v)\n\n\ndef identity():\n    return np.identity(4, dtype=np.float32)\n\n\ndef empty():\n    return np.zeros([4, 4], dtype=np.float32)\n\n\ndef magnitude(v):\n    return np.linalg.norm(v)\n\n\ndef normalize(v):\n    m = magnitude(v)\n    return v if m == 0 else v / m\n\n\ndef dot(u, v):\n    return np.sum(u * v)\n\n\ndef cross(u, v):\n    res = vec3(0, 0, 0)\n    res[0] = u[1] * v[2] - u[2] * v[1]\n    res[1] = u[2] * v[0] - u[0] * v[2]\n    res[2] = u[0] * v[1] - u[1] * v[0]\n    return res\n\n\n# below functions can be optimized\n\ndef translate(m, v):\n    res = np.copy(m)\n    res[:, 3] = m[:, 0] * v[0] + m[:, 1] * v[1] + m[:, 2] * v[2] + m[:, 3]\n    return res\n\n\ndef rotate(m, angle, v):\n    a = angle\n    c = np.cos(a)\n    s = np.sin(a)\n\n    axis = normalize(v)\n    temp = (1 - c) * axis\n\n    rot = empty()\n    rot[0][0] = c + temp[0] * axis[0]\n    rot[0][1] = temp[0] * axis[1] + s * axis[2]\n    rot[0][2] = temp[0] * axis[2] - s * axis[1]\n\n    rot[1][0] = temp[1] * axis[0] - s * axis[2]\n    rot[1][1] = c + temp[1] * axis[1]\n    rot[1][2] = temp[1] * axis[2] + s * axis[0]\n\n    rot[2][0] = temp[2] * axis[0] + s * axis[1]\n    rot[2][1] = temp[2] * axis[1] - s * axis[0]\n    rot[2][2] = c + temp[2] * axis[2]\n\n    res = empty()\n    res[:, 0] = m[:, 0] * rot[0][0] + m[:, 1] * rot[0][1] + m[:, 2] * rot[0][2]\n    res[:, 1] = m[:, 0] * rot[1][0] + m[:, 1] * rot[1][1] + m[:, 2] * rot[1][2]\n    res[:, 2] = m[:, 0] * rot[2][0] + m[:, 1] * rot[2][1] + m[:, 2] * rot[2][2]\n    res[:, 3] = m[:, 3]\n    return res\n\n\ndef perspective(fovy, aspect, zNear, zFar):\n    tanHalfFovy = np.tan(fovy / 2)\n\n    res = empty()\n    res[0][0] = 1 / (aspect * tanHalfFovy)\n    res[1][1] = 1 / (tanHalfFovy)\n    res[2][3] = -1\n    res[2][2] = - (zFar + zNear) / (zFar - zNear)\n    res[3][2] = -(2 * zFar * zNear) / (zFar - zNear)\n\n    return res.T\n\n\ndef ortho(left, right, bottom, top, zNear, zFar):\n    # res = np.ones([4, 4], dtype=np.float32)\n    res = identity()\n    res[0][0] = 2 / (right - left)\n    res[1][1] = 2 / (top - bottom)\n    res[2][2] = - 2 / (zFar - zNear)\n    res[3][0] = - (right + left) / (right - left)\n    res[3][1] = - (top + bottom) / (top - bottom)\n    res[3][2] = - (zFar + zNear) / (zFar - zNear)\n    return res.T\n\n\ndef lookat(eye, center, up):\n    f = normalize(center - eye)\n    s = normalize(cross(f, up))\n    u = cross(s, f)\n\n    res = identity()\n    res[0][0] = s[0]\n    res[1][0] = s[1]\n    res[2][0] = s[2]\n    res[0][1] = u[0]\n    res[1][1] = u[1]\n    res[2][1] = u[2]\n    res[0][2] = -f[0]\n    res[1][2] = -f[1]\n    res[2][2] = -f[2]\n    res[3][0] = -dot(s, eye)\n    res[3][1] = -dot(u, eye)\n    res[3][2] = -dot(f, eye)\n    return res.T\n\n\ndef transform(d, m):\n    return np.dot(m, d.T).T\n'"
lib/renderer/mesh.py,0,"b'import numpy as np\n\n\ndef save_obj_mesh(mesh_path, verts, faces):\n    file = open(mesh_path, \'w\')\n    for v in verts:\n        file.write(\'v %.4f %.4f %.4f\\n\' % (v[0], v[1], v[2]))\n    for f in faces:\n        f_plus = f + 1\n        file.write(\'f %d %d %d\\n\' % (f_plus[0], f_plus[1], f_plus[2]))\n    file.close()\n\n# https://github.com/ratcave/wavefront_reader\ndef read_mtlfile(fname):\n    materials = {}\n    with open(fname) as f:\n        lines = f.read().splitlines()\n\n    for line in lines:\n        if line:\n            split_line = line.strip().split(\' \', 1)\n            if len(split_line) < 2:\n                continue\n\n            prefix, data = split_line[0], split_line[1]\n            if \'newmtl\' in prefix:\n                material = {}\n                materials[data] = material\n            elif materials:\n                if data:\n                    split_data = data.strip().split(\' \')\n\n                    # assume texture maps are in the same level\n                    # WARNING: do not include space in your filename!!                    \n                    if \'map\' in prefix:\n                        material[prefix] = split_data[-1].split(\'\\\\\')[-1]\n                    elif len(split_data) > 1:\n                        material[prefix] = tuple(float(d) for d in split_data)\n                    else:\n                        try:\n                            material[prefix] = int(data)\n                        except ValueError:\n                            material[prefix] = float(data)\n\n    return materials\n\n\ndef load_obj_mesh_mtl(mesh_file):\n    vertex_data = []\n    norm_data = []\n    uv_data = []\n\n    face_data = []\n    face_norm_data = []\n    face_uv_data = []\n\n    # face per material\n    face_data_mat = {}\n    face_norm_data_mat = {}\n    face_uv_data_mat = {}\n\n    # current material name\n    mtl_data = None\n    cur_mat = None\n\n    if isinstance(mesh_file, str):\n        f = open(mesh_file, ""r"")\n    else:\n        f = mesh_file\n    for line in f:\n        if isinstance(line, bytes):\n            line = line.decode(""utf-8"")\n        if line.startswith(\'#\'):\n            continue\n        values = line.split()\n        if not values:\n            continue\n\n        if values[0] == \'v\':\n            v = list(map(float, values[1:4]))\n            vertex_data.append(v)\n        elif values[0] == \'vn\':\n            vn = list(map(float, values[1:4]))\n            norm_data.append(vn)\n        elif values[0] == \'vt\':\n            vt = list(map(float, values[1:3]))\n            uv_data.append(vt)\n        elif values[0] == \'mtllib\':\n            mtl_data = read_mtlfile(mesh_file.replace(mesh_file.split(\'/\')[-1],values[1]))\n        elif values[0] == \'usemtl\':\n            cur_mat = values[1]\n        elif values[0] == \'f\':\n            # local triangle data\n            l_face_data = []\n            l_face_uv_data = []\n            l_face_norm_data = []\n\n            # quad mesh\n            if len(values) > 4:\n                f = list(map(lambda x: int(x.split(\'/\')[0]) if int(x.split(\'/\')[0]) < 0 else int(x.split(\'/\')[0])-1, values[1:4]))\n                l_face_data.append(f)\n                f = list(map(lambda x: int(x.split(\'/\')[0]) if int(x.split(\'/\')[0]) < 0 else int(x.split(\'/\')[0])-1, [values[3], values[4], values[1]]))\n                l_face_data.append(f)\n            # tri mesh\n            else:\n                f = list(map(lambda x: int(x.split(\'/\')[0]) if int(x.split(\'/\')[0]) < 0 else int(x.split(\'/\')[0])-1, values[1:4]))\n                l_face_data.append(f)\n            # deal with texture\n            if len(values[1].split(\'/\')) >= 2:\n                # quad mesh\n                if len(values) > 4:\n                    f = list(map(lambda x: int(x.split(\'/\')[1]) if int(x.split(\'/\')[1]) < 0 else int(x.split(\'/\')[1])-1, values[1:4]))\n                    l_face_uv_data.append(f)\n                    f = list(map(lambda x: int(x.split(\'/\')[1]) if int(x.split(\'/\')[1]) < 0 else int(x.split(\'/\')[1])-1, [values[3], values[4], values[1]]))\n                    l_face_uv_data.append(f)\n                # tri mesh\n                elif len(values[1].split(\'/\')[1]) != 0:\n                    f = list(map(lambda x: int(x.split(\'/\')[1]) if int(x.split(\'/\')[1]) < 0 else int(x.split(\'/\')[1])-1, values[1:4]))\n                    l_face_uv_data.append(f)\n            # deal with normal\n            if len(values[1].split(\'/\')) == 3:\n                # quad mesh\n                if len(values) > 4:\n                    f = list(map(lambda x: int(x.split(\'/\')[2]) if int(x.split(\'/\')[2]) < 0 else int(x.split(\'/\')[2])-1, values[1:4]))\n                    l_face_norm_data.append(f)\n                    f = list(map(lambda x: int(x.split(\'/\')[2]) if int(x.split(\'/\')[2]) < 0 else int(x.split(\'/\')[2])-1, [values[3], values[4], values[1]]))\n                    l_face_norm_data.append(f)\n                # tri mesh\n                elif len(values[1].split(\'/\')[2]) != 0:\n                    f = list(map(lambda x: int(x.split(\'/\')[2]) if int(x.split(\'/\')[2]) < 0 else int(x.split(\'/\')[2])-1, values[1:4]))\n                    l_face_norm_data.append(f)\n            \n            face_data += l_face_data\n            face_uv_data += l_face_uv_data\n            face_norm_data += l_face_norm_data\n\n            if cur_mat is not None:\n                if cur_mat not in face_data_mat.keys():\n                    face_data_mat[cur_mat] = []\n                if cur_mat not in face_uv_data_mat.keys():\n                    face_uv_data_mat[cur_mat] = []\n                if cur_mat not in face_norm_data_mat.keys():\n                    face_norm_data_mat[cur_mat] = []\n                face_data_mat[cur_mat] += l_face_data\n                face_uv_data_mat[cur_mat] += l_face_uv_data\n                face_norm_data_mat[cur_mat] += l_face_norm_data\n\n    vertices = np.array(vertex_data)\n    faces = np.array(face_data)\n\n    norms = np.array(norm_data)\n    norms = normalize_v3(norms)\n    face_normals = np.array(face_norm_data)\n\n    uvs = np.array(uv_data)\n    face_uvs = np.array(face_uv_data)\n\n    out_tuple = (vertices, faces, norms, face_normals, uvs, face_uvs)\n\n    if cur_mat is not None and mtl_data is not None:\n        for key in face_data_mat:\n            face_data_mat[key] = np.array(face_data_mat[key])\n            face_uv_data_mat[key] = np.array(face_uv_data_mat[key])\n            face_norm_data_mat[key] = np.array(face_norm_data_mat[key])\n        \n        out_tuple += (face_data_mat, face_norm_data_mat, face_uv_data_mat, mtl_data)\n\n    return out_tuple\n    \n\ndef load_obj_mesh(mesh_file, with_normal=False, with_texture=False):\n    vertex_data = []\n    norm_data = []\n    uv_data = []\n\n    face_data = []\n    face_norm_data = []\n    face_uv_data = []\n\n    if isinstance(mesh_file, str):\n        f = open(mesh_file, ""r"")\n    else:\n        f = mesh_file\n    for line in f:\n        if isinstance(line, bytes):\n            line = line.decode(""utf-8"")\n        if line.startswith(\'#\'):\n            continue\n        values = line.split()\n        if not values:\n            continue\n\n        if values[0] == \'v\':\n            v = list(map(float, values[1:4]))\n            vertex_data.append(v)\n        elif values[0] == \'vn\':\n            vn = list(map(float, values[1:4]))\n            norm_data.append(vn)\n        elif values[0] == \'vt\':\n            vt = list(map(float, values[1:3]))\n            uv_data.append(vt)\n\n        elif values[0] == \'f\':\n            # quad mesh\n            if len(values) > 4:\n                f = list(map(lambda x: int(x.split(\'/\')[0]), values[1:4]))\n                face_data.append(f)\n                f = list(map(lambda x: int(x.split(\'/\')[0]), [values[3], values[4], values[1]]))\n                face_data.append(f)\n            # tri mesh\n            else:\n                f = list(map(lambda x: int(x.split(\'/\')[0]), values[1:4]))\n                face_data.append(f)\n            \n            # deal with texture\n            if len(values[1].split(\'/\')) >= 2:\n                # quad mesh\n                if len(values) > 4:\n                    f = list(map(lambda x: int(x.split(\'/\')[1]), values[1:4]))\n                    face_uv_data.append(f)\n                    f = list(map(lambda x: int(x.split(\'/\')[1]), [values[3], values[4], values[1]]))\n                    face_uv_data.append(f)\n                # tri mesh\n                elif len(values[1].split(\'/\')[1]) != 0:\n                    f = list(map(lambda x: int(x.split(\'/\')[1]), values[1:4]))\n                    face_uv_data.append(f)\n            # deal with normal\n            if len(values[1].split(\'/\')) == 3:\n                # quad mesh\n                if len(values) > 4:\n                    f = list(map(lambda x: int(x.split(\'/\')[2]), values[1:4]))\n                    face_norm_data.append(f)\n                    f = list(map(lambda x: int(x.split(\'/\')[2]), [values[3], values[4], values[1]]))\n                    face_norm_data.append(f)\n                # tri mesh\n                elif len(values[1].split(\'/\')[2]) != 0:\n                    f = list(map(lambda x: int(x.split(\'/\')[2]), values[1:4]))\n                    face_norm_data.append(f)\n\n    vertices = np.array(vertex_data)\n    faces = np.array(face_data) - 1\n\n    if with_texture and with_normal:\n        uvs = np.array(uv_data)\n        face_uvs = np.array(face_uv_data) - 1\n        norms = np.array(norm_data)\n        if norms.shape[0] == 0:\n            norms = compute_normal(vertices, faces)\n            face_normals = faces\n        else:\n            norms = normalize_v3(norms)\n            face_normals = np.array(face_norm_data) - 1\n        return vertices, faces, norms, face_normals, uvs, face_uvs\n\n    if with_texture:\n        uvs = np.array(uv_data)\n        face_uvs = np.array(face_uv_data) - 1\n        return vertices, faces, uvs, face_uvs\n\n    if with_normal:\n        norms = np.array(norm_data)\n        norms = normalize_v3(norms)\n        face_normals = np.array(face_norm_data) - 1\n        return vertices, faces, norms, face_normals\n\n    return vertices, faces\n\n\ndef normalize_v3(arr):\n    \'\'\' Normalize a numpy array of 3 component vectors shape=(n,3) \'\'\'\n    lens = np.sqrt(arr[:, 0] ** 2 + arr[:, 1] ** 2 + arr[:, 2] ** 2)\n    eps = 0.00000001\n    lens[lens < eps] = eps\n    arr[:, 0] /= lens\n    arr[:, 1] /= lens\n    arr[:, 2] /= lens\n    return arr\n\n\ndef compute_normal(vertices, faces):\n    # Create a zeroed array with the same type and shape as our vertices i.e., per vertex normal\n    norm = np.zeros(vertices.shape, dtype=vertices.dtype)\n    # Create an indexed view into the vertex array using the array of three indices for triangles\n    tris = vertices[faces]\n    # Calculate the normal for all the triangles, by taking the cross product of the vectors v1-v0, and v2-v0 in each triangle\n    n = np.cross(tris[::, 1] - tris[::, 0], tris[::, 2] - tris[::, 0])\n    # n is now an array of normals per triangle. The length of each normal is dependent the vertices,\n    # we need to normalize these, so that our next step weights each normal equally.\n    normalize_v3(n)\n    # now we have a normalized array of normals, one per triangle, i.e., per triangle normals.\n    # But instead of one per triangle (i.e., flat shading), we add to each vertex in that triangle,\n    # the triangles\' normal. Multiple triangles would then contribute to every vertex, so we need to normalize again afterwards.\n    # The cool part, we can actually add the normals through an indexed view of our (zeroed) per vertex normal array\n    norm[faces[:, 0]] += n\n    norm[faces[:, 1]] += n\n    norm[faces[:, 2]] += n\n    normalize_v3(norm)\n\n    return norm\n\n# compute tangent and bitangent\ndef compute_tangent(vertices, faces, normals, uvs, faceuvs):    \n    # NOTE: this could be numerically unstable around [0,0,1]\n    # but other current solutions are pretty freaky somehow\n    c1 = np.cross(normals, np.array([0,1,0.0]))\n    tan = c1\n    normalize_v3(tan)\n    btan = np.cross(normals, tan)\n\n    # NOTE: traditional version is below\n\n    # pts_tris = vertices[faces]\n    # uv_tris = uvs[faceuvs]\n\n    # W = np.stack([pts_tris[::, 1] - pts_tris[::, 0], pts_tris[::, 2] - pts_tris[::, 0]],2)\n    # UV = np.stack([uv_tris[::, 1] - uv_tris[::, 0], uv_tris[::, 2] - uv_tris[::, 0]], 1)\n    \n    # for i in range(W.shape[0]):\n    #     W[i,::] = W[i,::].dot(np.linalg.inv(UV[i,::]))\n\n    # tan = np.zeros(vertices.shape, dtype=vertices.dtype)\n    # tan[faces[:,0]] += W[:,:,0]\n    # tan[faces[:,1]] += W[:,:,0]\n    # tan[faces[:,2]] += W[:,:,0]\n\n    # btan = np.zeros(vertices.shape, dtype=vertices.dtype)\n    # btan[faces[:,0]] += W[:,:,1]\n    # btan[faces[:,1]] += W[:,:,1]    \n    # btan[faces[:,2]] += W[:,:,1]\n\n    # normalize_v3(tan)\n    \n    # ndott = np.sum(normals*tan, 1, keepdims=True)\n    # tan = tan - ndott * normals\n\n    # normalize_v3(btan)\n    # normalize_v3(tan)\n\n    # tan[np.sum(np.cross(normals, tan) * btan, 1) < 0,:] *= -1.0\n\n    return tan, btan\n\nif __name__ == \'__main__\':\n    pts, tri, nml, trin, uvs, triuv = load_obj_mesh(\'/home/ICT2000/ssaito/Documents/Body/tmp/Baseball_Pitching/0012.obj\', True, True)\n    compute_tangent(pts, tri, uvs, triuv)'"
lib/renderer/gl/__init__.py,0,b''
lib/renderer/gl/cam_render.py,0,"b""from .render import Render\n\nGLUT = None\n\nclass CamRender(Render):\n    def __init__(self, width=1600, height=1200, name='Cam Renderer',\n                 program_files=['simple.fs', 'simple.vs'], color_size=1, ms_rate=1, egl=False):\n        Render.__init__(self, width, height, name, program_files, color_size, ms_rate=ms_rate, egl=egl)\n        self.camera = None\n\n        if not egl:\n            global GLUT\n            import OpenGL.GLUT as GLUT\n            GLUT.glutDisplayFunc(self.display)\n            GLUT.glutKeyboardFunc(self.keyboard)\n\n    def set_camera(self, camera):\n        self.camera = camera\n        self.projection_matrix, self.model_view_matrix = camera.get_gl_matrix()\n\n    def keyboard(self, key, x, y):\n        # up\n        eps = 1\n        # print(key)\n        if key == b'w':\n            self.camera.center += eps * self.camera.direction\n        elif key == b's':\n            self.camera.center -= eps * self.camera.direction\n        if key == b'a':\n            self.camera.center -= eps * self.camera.right\n        elif key == b'd':\n            self.camera.center += eps * self.camera.right\n        if key == b' ':\n            self.camera.center += eps * self.camera.up\n        elif key == b'x':\n            self.camera.center -= eps * self.camera.up\n        elif key == b'i':\n            self.camera.near += 0.1 * eps\n            self.camera.far += 0.1 * eps\n        elif key == b'o':\n            self.camera.near -= 0.1 * eps\n            self.camera.far -= 0.1 * eps\n\n        self.projection_matrix, self.model_view_matrix = self.camera.get_gl_matrix()\n\n    def show(self):\n        if GLUT is not None:\n            GLUT.glutMainLoop()\n"""
lib/renderer/gl/framework.py,0,"b'# Mario Rosasco, 2016\n# adapted from framework.cpp, Copyright (C) 2010-2012 by Jason L. McKesson\n# This file is licensed under the MIT License.\n#\n# NB: Unlike in the framework.cpp organization, the main loop is contained\n# in the tutorial files, not in this framework file. Additionally, a copy of\n# this module file must exist in the same directory as the tutorial files\n# to be imported properly.\n\nimport os\nfrom OpenGL.GL import *\n\n# Function that creates and compiles shaders according to the given type (a GL enum value) and\n# shader program (a file containing a GLSL program).\ndef loadShader(shaderType, shaderFile):\n    # check if file exists, get full path name\n    strFilename = findFileOrThrow(shaderFile)\n    shaderData = None\n    with open(strFilename, \'r\') as f:\n        shaderData = f.read()\n\n    shader = glCreateShader(shaderType)\n    glShaderSource(shader, shaderData)  # note that this is a simpler function call than in C\n\n    # This shader compilation is more explicit than the one used in\n    # framework.cpp, which relies on a glutil wrapper function.\n    # This is made explicit here mainly to decrease dependence on pyOpenGL\n    # utilities and wrappers, which docs caution may change in future versions.\n    glCompileShader(shader)\n\n    status = glGetShaderiv(shader, GL_COMPILE_STATUS)\n    if status == GL_FALSE:\n        # Note that getting the error log is much simpler in Python than in C/C++\n        # and does not require explicit handling of the string buffer\n        strInfoLog = glGetShaderInfoLog(shader)\n        strShaderType = """"\n        if shaderType is GL_VERTEX_SHADER:\n            strShaderType = ""vertex""\n        elif shaderType is GL_GEOMETRY_SHADER:\n            strShaderType = ""geometry""\n        elif shaderType is GL_FRAGMENT_SHADER:\n            strShaderType = ""fragment""\n\n        print(""Compilation failure for "" + strShaderType + "" shader:\\n"" + str(strInfoLog))\n\n    return shader\n\n\n# Function that accepts a list of shaders, compiles them, and returns a handle to the compiled program\ndef createProgram(shaderList):\n    program = glCreateProgram()\n\n    for shader in shaderList:\n        glAttachShader(program, shader)\n\n    glLinkProgram(program)\n\n    status = glGetProgramiv(program, GL_LINK_STATUS)\n    if status == GL_FALSE:\n        # Note that getting the error log is much simpler in Python than in C/C++\n        # and does not require explicit handling of the string buffer\n        strInfoLog = glGetProgramInfoLog(program)\n        print(""Linker failure: \\n"" + str(strInfoLog))\n\n    for shader in shaderList:\n        glDetachShader(program, shader)\n\n    return program\n\n\n# Helper function to locate and open the target file (passed in as a string).\n# Returns the full path to the file as a string.\ndef findFileOrThrow(strBasename):\n    # Keep constant names in C-style convention, for readability\n    # when comparing to C(/C++) code.\n    if os.path.isfile(strBasename):\n        return strBasename\n\n    LOCAL_FILE_DIR = ""data"" + os.sep\n    GLOBAL_FILE_DIR = os.path.dirname(os.path.abspath(__file__)) + os.sep + ""data"" + os.sep\n\n    strFilename = LOCAL_FILE_DIR + strBasename\n    if os.path.isfile(strFilename):\n        return strFilename\n\n    strFilename = GLOBAL_FILE_DIR + strBasename\n    if os.path.isfile(strFilename):\n        return strFilename\n\n    raise IOError(\'Could not find target file \' + strBasename)'"
lib/renderer/gl/glcontext.py,0,"b'""""""Headless GPU-accelerated OpenGL context creation on Google Colaboratory.\r\n\r\nTypical usage:\r\n\r\n    # Optional PyOpenGL configuratiopn can be done here.\r\n    # import OpenGL\r\n    # OpenGL.ERROR_CHECKING = True\r\n\r\n    # \'glcontext\' must be imported before any OpenGL.* API.\r\n    from lucid.misc.gl.glcontext import create_opengl_context\r\n\r\n    # Now it\'s safe to import OpenGL and EGL functions\r\n    import OpenGL.GL as gl\r\n\r\n    # create_opengl_context() creates a GL context that is attached to an\r\n    # offscreen surface of the specified size. Note that rendering to buffers\r\n    # of other sizes and formats is still possible with OpenGL Framebuffers.\r\n    #\r\n    # Users are expected to directly use the EGL API in case more advanced\r\n    # context management is required.\r\n    width, height = 640, 480\r\n    create_opengl_context((width, height))\r\n\r\n    # OpenGL context is available here.\r\n\r\n""""""\r\n\r\nfrom __future__ import print_function\r\n\r\n# pylint: disable=unused-import,g-import-not-at-top,g-statement-before-imports\r\n\r\ntry:\r\n  import OpenGL\r\nexcept:\r\n  print(\'This module depends on PyOpenGL.\')\r\n  print(\'Please run ""\\033[1m!pip install -q pyopengl\\033[0m"" \'\r\n        \'prior importing this module.\')\r\n  raise\r\n\r\nimport ctypes\r\nfrom ctypes import pointer, util\r\nimport os\r\n\r\nos.environ[\'PYOPENGL_PLATFORM\'] = \'egl\'\r\n\r\n# OpenGL loading workaround.\r\n#\r\n# * PyOpenGL tries to load libGL, but we need libOpenGL, see [1,2].\r\n#   This could have been solved by a symlink libGL->libOpenGL, but:\r\n#\r\n# * Python 2.7 can\'t find libGL and linEGL due to a bug (see [3])\r\n#   in ctypes.util, that was only wixed in Python 3.6.\r\n#\r\n# So, the only solution I\'ve found is to monkeypatch ctypes.util\r\n# [1] https://devblogs.nvidia.com/egl-eye-opengl-visualization-without-x-server/\r\n# [2] https://devblogs.nvidia.com/linking-opengl-server-side-rendering/\r\n# [3] https://bugs.python.org/issue9998\r\n_find_library_old = ctypes.util.find_library\r\ntry:\r\n\r\n  def _find_library_new(name):\r\n    return {\r\n        \'GL\': \'libOpenGL.so\',\r\n        \'EGL\': \'libEGL.so\',\r\n    }.get(name, _find_library_old(name))\r\n  util.find_library = _find_library_new\r\n  import OpenGL.GL as gl\r\n  import OpenGL.EGL as egl\r\nexcept:\r\n  print(\'Unable to load OpenGL libraries. \'\r\n        \'Make sure you use GPU-enabled backend.\')\r\n  print(\'Press ""Runtime->Change runtime type"" and set \'\r\n        \'""Hardware accelerator"" to GPU.\')\r\n  raise\r\nfinally:\r\n  util.find_library = _find_library_old\r\n\r\n\r\ndef create_opengl_context(surface_size=(640, 480)):\r\n  """"""Create offscreen OpenGL context and make it current.\r\n\r\n  Users are expected to directly use EGL API in case more advanced\r\n  context management is required.\r\n\r\n  Args:\r\n    surface_size: (width, height), size of the offscreen rendering surface.\r\n  """"""\r\n  egl_display = egl.eglGetDisplay(egl.EGL_DEFAULT_DISPLAY)\r\n\r\n  major, minor = egl.EGLint(), egl.EGLint()\r\n  egl.eglInitialize(egl_display, pointer(major), pointer(minor))\r\n\r\n  config_attribs = [\r\n      egl.EGL_SURFACE_TYPE, egl.EGL_PBUFFER_BIT, egl.EGL_BLUE_SIZE, 8,\r\n      egl.EGL_GREEN_SIZE, 8, egl.EGL_RED_SIZE, 8, egl.EGL_DEPTH_SIZE, 24,\r\n      egl.EGL_RENDERABLE_TYPE, egl.EGL_OPENGL_BIT, egl.EGL_NONE\r\n  ]\r\n  config_attribs = (egl.EGLint * len(config_attribs))(*config_attribs)\r\n\r\n  num_configs = egl.EGLint()\r\n  egl_cfg = egl.EGLConfig()\r\n  egl.eglChooseConfig(egl_display, config_attribs, pointer(egl_cfg), 1,\r\n                      pointer(num_configs))\r\n\r\n  width, height = surface_size\r\n  pbuffer_attribs = [\r\n      egl.EGL_WIDTH,\r\n      width,\r\n      egl.EGL_HEIGHT,\r\n      height,\r\n      egl.EGL_NONE,\r\n  ]\r\n  pbuffer_attribs = (egl.EGLint * len(pbuffer_attribs))(*pbuffer_attribs)\r\n  egl_surf = egl.eglCreatePbufferSurface(egl_display, egl_cfg, pbuffer_attribs)\r\n\r\n  egl.eglBindAPI(egl.EGL_OPENGL_API)\r\n\r\n  egl_context = egl.eglCreateContext(egl_display, egl_cfg, egl.EGL_NO_CONTEXT,\r\n                                     None)\r\n  egl.eglMakeCurrent(egl_display, egl_surf, egl_surf, egl_context)'"
lib/renderer/gl/init_gl.py,0,"b'_glut_window = None\n_context_inited = None\n\ndef initialize_GL_context(width=512, height=512, egl=False):\n    \'\'\'\n    default context uses GLUT\n    \'\'\'\n    if not egl:\n        import OpenGL.GLUT as GLUT      \n        display_mode = GLUT.GLUT_DOUBLE | GLUT.GLUT_RGB | GLUT.GLUT_DEPTH\n        global _glut_window\n        if _glut_window is None:\n            GLUT.glutInit()\n            GLUT.glutInitDisplayMode(display_mode)\n            GLUT.glutInitWindowSize(width, height)\n            GLUT.glutInitWindowPosition(0, 0)\n            _glut_window = GLUT.glutCreateWindow(""My Render."")\n    else:\n        from .glcontext import create_opengl_context\n        global _context_inited\n        if _context_inited is None:\n            create_opengl_context((width, height))\n            _context_inited = True\n\n'"
lib/renderer/gl/prt_render.py,0,"b""import numpy as np\nimport random\n\nfrom .framework import *\nfrom .cam_render import CamRender\n\nclass PRTRender(CamRender):\n    def __init__(self, width=1600, height=1200, name='PRT Renderer', uv_mode=False, ms_rate=1, egl=False):\n        program_files = ['prt.vs', 'prt.fs'] if not uv_mode else ['prt_uv.vs', 'prt_uv.fs']\n        CamRender.__init__(self, width, height, name, program_files=program_files, color_size=8, ms_rate=ms_rate, egl=egl)\n\n        # WARNING: this differs from vertex_buffer and vertex_data in Render\n        self.vert_buffer = {}\n        self.vert_data = {}\n\n        self.norm_buffer = {}\n        self.norm_data = {}\n\n        self.tan_buffer = {}\n        self.tan_data = {}\n\n        self.btan_buffer = {}\n        self.btan_data = {}\n\n        self.prt1_buffer = {}\n        self.prt1_data = {}\n        self.prt2_buffer = {}\n        self.prt2_data = {}        \n        self.prt3_buffer = {}\n        self.prt3_data = {}\n\n        self.uv_buffer = {}\n        self.uv_data = {}\n\n        self.render_texture_mat = {}\n\n        self.vertex_dim = {}\n        self.n_vertices = {}\n\n        self.norm_mat_unif = glGetUniformLocation(self.program, 'NormMat')\n        self.normalize_matrix = np.eye(4)\n\n        self.shcoeff_unif = glGetUniformLocation(self.program, 'SHCoeffs')\n        self.shcoeffs = np.zeros((9,3))\n        self.shcoeffs[0,:] = 1.0\n        #self.shcoeffs[1:,:] = np.random.rand(8,3)\n\n        self.hasAlbedoUnif = glGetUniformLocation(self.program, 'hasAlbedoMap')\n        self.hasNormalUnif = glGetUniformLocation(self.program, 'hasNormalMap')\n\n        self.analyticUnif = glGetUniformLocation(self.program, 'analytic')\n        self.analytic = False\n\n        self.rot_mat_unif = glGetUniformLocation(self.program, 'RotMat')\n        self.rot_matrix = np.eye(3)\n\n    def set_texture(self, mat_name, smplr_name, texture):\n        # texture_image: H x W x 3\n        width = texture.shape[1]\n        height = texture.shape[0]\n        texture = np.flip(texture, 0)\n        img_data = np.fromstring(texture.tostring(), np.uint8)\n\n        if mat_name not in self.render_texture_mat:\n            self.render_texture_mat[mat_name] = {} \n        if smplr_name in self.render_texture_mat[mat_name].keys():\n            glDeleteTextures([self.render_texture_mat[mat_name][smplr_name]])\n            del self.render_texture_mat[mat_name][smplr_name]\n        self.render_texture_mat[mat_name][smplr_name] = glGenTextures(1)\n        glActiveTexture(GL_TEXTURE0)\n        \n        glPixelStorei(GL_UNPACK_ALIGNMENT, 1)\n        glBindTexture(GL_TEXTURE_2D, self.render_texture_mat[mat_name][smplr_name])\n\n        glTexImage2D(GL_TEXTURE_2D, 0, GL_RGB, width, height, 0, GL_RGB, GL_UNSIGNED_BYTE, img_data)\n        \n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAX_LEVEL, 3)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR)\n        glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR_MIPMAP_LINEAR)\n\n        glGenerateMipmap(GL_TEXTURE_2D)\n        \n    def set_albedo(self, texture_image, mat_name='all'):\n        self.set_texture(mat_name, 'AlbedoMap', texture_image)\n\n    def set_normal_map(self, texture_image, mat_name='all'):\n        self.set_texture(mat_name, 'NormalMap', texture_image)\n\n    def set_mesh(self, vertices, faces, norms, faces_nml, uvs, faces_uvs, prt, faces_prt, tans, bitans, mat_name='all'):\n        self.vert_data[mat_name] = vertices[faces.reshape([-1])]\n        self.n_vertices[mat_name] = self.vert_data[mat_name].shape[0]\n        self.vertex_dim[mat_name] = self.vert_data[mat_name].shape[1]\n\n        if mat_name not in self.vert_buffer.keys():\n            self.vert_buffer[mat_name] = glGenBuffers(1)\n        glBindBuffer(GL_ARRAY_BUFFER, self.vert_buffer[mat_name])\n        glBufferData(GL_ARRAY_BUFFER, self.vert_data[mat_name], GL_STATIC_DRAW)\n\n        self.uv_data[mat_name] = uvs[faces_uvs.reshape([-1])]\n        if mat_name not in self.uv_buffer.keys():\n            self.uv_buffer[mat_name] = glGenBuffers(1)\n        glBindBuffer(GL_ARRAY_BUFFER, self.uv_buffer[mat_name])\n        glBufferData(GL_ARRAY_BUFFER, self.uv_data[mat_name], GL_STATIC_DRAW)\n\n        self.norm_data[mat_name] = norms[faces_nml.reshape([-1])]\n        if mat_name not in self.norm_buffer.keys():\n            self.norm_buffer[mat_name] = glGenBuffers(1)\n        glBindBuffer(GL_ARRAY_BUFFER, self.norm_buffer[mat_name])\n        glBufferData(GL_ARRAY_BUFFER, self.norm_data[mat_name], GL_STATIC_DRAW)\n\n        self.tan_data[mat_name] = tans[faces_nml.reshape([-1])]\n        if mat_name not in self.tan_buffer.keys():\n            self.tan_buffer[mat_name] = glGenBuffers(1)\n        glBindBuffer(GL_ARRAY_BUFFER, self.tan_buffer[mat_name])\n        glBufferData(GL_ARRAY_BUFFER, self.tan_data[mat_name], GL_STATIC_DRAW)\n\n        self.btan_data[mat_name] = bitans[faces_nml.reshape([-1])]\n        if mat_name not in self.btan_buffer.keys():\n            self.btan_buffer[mat_name] = glGenBuffers(1)\n        glBindBuffer(GL_ARRAY_BUFFER, self.btan_buffer[mat_name])\n        glBufferData(GL_ARRAY_BUFFER, self.btan_data[mat_name], GL_STATIC_DRAW)\n\n        self.prt1_data[mat_name] = prt[faces_prt.reshape([-1])][:,:3]\n        self.prt2_data[mat_name] = prt[faces_prt.reshape([-1])][:,3:6]\n        self.prt3_data[mat_name] = prt[faces_prt.reshape([-1])][:,6:]\n\n        if mat_name not in self.prt1_buffer.keys():\n            self.prt1_buffer[mat_name] = glGenBuffers(1)\n        if mat_name not in self.prt2_buffer.keys():\n            self.prt2_buffer[mat_name] = glGenBuffers(1)\n        if mat_name not in self.prt3_buffer.keys():\n            self.prt3_buffer[mat_name] = glGenBuffers(1)\n        glBindBuffer(GL_ARRAY_BUFFER, self.prt1_buffer[mat_name])\n        glBufferData(GL_ARRAY_BUFFER, self.prt1_data[mat_name], GL_STATIC_DRAW)\n        glBindBuffer(GL_ARRAY_BUFFER, self.prt2_buffer[mat_name])\n        glBufferData(GL_ARRAY_BUFFER, self.prt2_data[mat_name], GL_STATIC_DRAW)\n        glBindBuffer(GL_ARRAY_BUFFER, self.prt3_buffer[mat_name])\n        glBufferData(GL_ARRAY_BUFFER, self.prt3_data[mat_name], GL_STATIC_DRAW)\n        \n        glBindBuffer(GL_ARRAY_BUFFER, 0)\n\n    def set_mesh_mtl(self, vertices, faces, norms, faces_nml, uvs, faces_uvs, tans, bitans, prt):\n        for key in faces:\n            self.vert_data[key] = vertices[faces[key].reshape([-1])]\n            self.n_vertices[key] = self.vert_data[key].shape[0]\n            self.vertex_dim[key] = self.vert_data[key].shape[1]\n\n            if key not in self.vert_buffer.keys():\n                self.vert_buffer[key] = glGenBuffers(1)\n            glBindBuffer(GL_ARRAY_BUFFER, self.vert_buffer[key])\n            glBufferData(GL_ARRAY_BUFFER, self.vert_data[key], GL_STATIC_DRAW)\n\n            self.uv_data[key] = uvs[faces_uvs[key].reshape([-1])]\n            if key not in self.uv_buffer.keys():\n                self.uv_buffer[key] = glGenBuffers(1)\n            glBindBuffer(GL_ARRAY_BUFFER, self.uv_buffer[key])\n            glBufferData(GL_ARRAY_BUFFER, self.uv_data[key], GL_STATIC_DRAW)\n\n            self.norm_data[key] = norms[faces_nml[key].reshape([-1])]\n            if key not in self.norm_buffer.keys():\n                self.norm_buffer[key] = glGenBuffers(1)\n            glBindBuffer(GL_ARRAY_BUFFER, self.norm_buffer[key])\n            glBufferData(GL_ARRAY_BUFFER, self.norm_data[key], GL_STATIC_DRAW)\n\n            self.tan_data[key] = tans[faces_nml[key].reshape([-1])]\n            if key not in self.tan_buffer.keys():\n                self.tan_buffer[key] = glGenBuffers(1)\n            glBindBuffer(GL_ARRAY_BUFFER, self.tan_buffer[key])\n            glBufferData(GL_ARRAY_BUFFER, self.tan_data[key], GL_STATIC_DRAW)\n\n            self.btan_data[key] = bitans[faces_nml[key].reshape([-1])]\n            if key not in self.btan_buffer.keys():\n                self.btan_buffer[key] = glGenBuffers(1)\n            glBindBuffer(GL_ARRAY_BUFFER, self.btan_buffer[key])\n            glBufferData(GL_ARRAY_BUFFER, self.btan_data[key], GL_STATIC_DRAW)\n\n            self.prt1_data[key] = prt[faces[key].reshape([-1])][:,:3]\n            self.prt2_data[key] = prt[faces[key].reshape([-1])][:,3:6]\n            self.prt3_data[key] = prt[faces[key].reshape([-1])][:,6:]\n\n            if key not in self.prt1_buffer.keys():\n                self.prt1_buffer[key] = glGenBuffers(1)\n            if key not in self.prt2_buffer.keys():\n                self.prt2_buffer[key] = glGenBuffers(1)\n            if key not in self.prt3_buffer.keys():\n                self.prt3_buffer[key] = glGenBuffers(1)\n            glBindBuffer(GL_ARRAY_BUFFER, self.prt1_buffer[key])\n            glBufferData(GL_ARRAY_BUFFER, self.prt1_data[key], GL_STATIC_DRAW)\n            glBindBuffer(GL_ARRAY_BUFFER, self.prt2_buffer[key])\n            glBufferData(GL_ARRAY_BUFFER, self.prt2_data[key], GL_STATIC_DRAW)\n            glBindBuffer(GL_ARRAY_BUFFER, self.prt3_buffer[key])\n            glBufferData(GL_ARRAY_BUFFER, self.prt3_data[key], GL_STATIC_DRAW)\n\n        glBindBuffer(GL_ARRAY_BUFFER, 0)\n\n    def cleanup(self):\n        \n        glBindBuffer(GL_ARRAY_BUFFER, 0)\n        for key in self.vert_data:\n            glDeleteBuffers(1, [self.vert_buffer[key]])\n            glDeleteBuffers(1, [self.norm_buffer[key]])\n            glDeleteBuffers(1, [self.uv_buffer[key]])\n\n            glDeleteBuffers(1, [self.tan_buffer[key]])\n            glDeleteBuffers(1, [self.btan_buffer[key]])\n            glDeleteBuffers(1, [self.prt1_buffer[key]])\n            glDeleteBuffers(1, [self.prt2_buffer[key]])\n            glDeleteBuffers(1, [self.prt3_buffer[key]])\n\n            glDeleteBuffers(1, [])\n\n            for smplr in self.render_texture_mat[key]:\n                glDeleteTextures([self.render_texture_mat[key][smplr]])\n\n        self.vert_buffer = {}\n        self.vert_data = {}\n\n        self.norm_buffer = {}\n        self.norm_data = {}\n\n        self.tan_buffer = {}\n        self.tan_data = {}\n\n        self.btan_buffer = {}\n        self.btan_data = {}\n\n        self.prt1_buffer = {}\n        self.prt1_data = {}\n\n        self.prt2_buffer = {}\n        self.prt2_data = {}\n\n        self.prt3_buffer = {}\n        self.prt3_data = {}\n\n        self.uv_buffer = {}\n        self.uv_data = {}\n\n        self.render_texture_mat = {}\n\n        self.vertex_dim = {}\n        self.n_vertices = {}\n    \n    def randomize_sh(self):\n        self.shcoeffs[0,:] = 0.8\n        self.shcoeffs[1:,:] = 1.0*np.random.rand(8,3)\n\n    def set_sh(self, sh):\n        self.shcoeffs = sh\n\n    def set_norm_mat(self, scale, center):\n        N = np.eye(4)\n        N[:3, :3] = scale*np.eye(3)\n        N[:3, 3] = -scale*center\n\n        self.normalize_matrix = N\n\n    def draw(self):\n        self.draw_init()\n\n        glDisable(GL_BLEND)\n        #glBlendFunc(GL_SRC_ALPHA, GL_ONE_MINUS_SRC_ALPHA)\n        glEnable(GL_MULTISAMPLE)\n\n        glUseProgram(self.program)\n        glUniformMatrix4fv(self.norm_mat_unif, 1, GL_FALSE, self.normalize_matrix.transpose())\n        glUniformMatrix4fv(self.model_mat_unif, 1, GL_FALSE, self.model_view_matrix.transpose())\n        glUniformMatrix4fv(self.persp_mat_unif, 1, GL_FALSE, self.projection_matrix.transpose())\n\n        if 'AlbedoMap' in self.render_texture_mat['all']:\n            glUniform1ui(self.hasAlbedoUnif, GLuint(1))\n        else:\n            glUniform1ui(self.hasAlbedoUnif, GLuint(0))\n\n        if 'NormalMap' in self.render_texture_mat['all']:\n            glUniform1ui(self.hasNormalUnif, GLuint(1))\n        else:\n            glUniform1ui(self.hasNormalUnif, GLuint(0))\n\n        glUniform1ui(self.analyticUnif, GLuint(1) if self.analytic else GLuint(0))\n\n        glUniform3fv(self.shcoeff_unif, 9, self.shcoeffs)\n\n        glUniformMatrix3fv(self.rot_mat_unif, 1, GL_FALSE, self.rot_matrix.transpose())\n\n        for mat in self.vert_buffer:\n            # Handle vertex buffer\n            glBindBuffer(GL_ARRAY_BUFFER, self.vert_buffer[mat])\n            glEnableVertexAttribArray(0)\n            glVertexAttribPointer(0, self.vertex_dim[mat], GL_DOUBLE, GL_FALSE, 0, None)\n\n            # Handle normal buffer\n            glBindBuffer(GL_ARRAY_BUFFER, self.norm_buffer[mat])\n            glEnableVertexAttribArray(1)\n            glVertexAttribPointer(1, 3, GL_DOUBLE, GL_FALSE, 0, None)\n\n            # Handle uv buffer\n            glBindBuffer(GL_ARRAY_BUFFER, self.uv_buffer[mat])\n            glEnableVertexAttribArray(2)\n            glVertexAttribPointer(2, 2, GL_DOUBLE, GL_FALSE, 0, None)\n\n            # Handle tan buffer\n            glBindBuffer(GL_ARRAY_BUFFER, self.tan_buffer[mat])\n            glEnableVertexAttribArray(3)\n            glVertexAttribPointer(3, 3, GL_DOUBLE, GL_FALSE, 0, None)\n\n            # Handle btan buffer\n            glBindBuffer(GL_ARRAY_BUFFER, self.btan_buffer[mat])\n            glEnableVertexAttribArray(4)\n            glVertexAttribPointer(4, 3, GL_DOUBLE, GL_FALSE, 0, None)\n\n            # Handle PTR buffer\n            glBindBuffer(GL_ARRAY_BUFFER, self.prt1_buffer[mat])\n            glEnableVertexAttribArray(5)\n            glVertexAttribPointer(5, 3, GL_DOUBLE, GL_FALSE, 0, None)\n\n            glBindBuffer(GL_ARRAY_BUFFER, self.prt2_buffer[mat])\n            glEnableVertexAttribArray(6)\n            glVertexAttribPointer(6, 3, GL_DOUBLE, GL_FALSE, 0, None)\n\n            glBindBuffer(GL_ARRAY_BUFFER, self.prt3_buffer[mat])\n            glEnableVertexAttribArray(7)\n            glVertexAttribPointer(7, 3, GL_DOUBLE, GL_FALSE, 0, None)\n\n            for i, smplr in enumerate(self.render_texture_mat[mat]):\n                glActiveTexture(GL_TEXTURE0 + i)\n                glBindTexture(GL_TEXTURE_2D, self.render_texture_mat[mat][smplr])\n                glUniform1i(glGetUniformLocation(self.program, smplr), i)\n\n            glDrawArrays(GL_TRIANGLES, 0, self.n_vertices[mat])\n\n            glDisableVertexAttribArray(7)\n            glDisableVertexAttribArray(6)\n            glDisableVertexAttribArray(5)\n            glDisableVertexAttribArray(4)\n            glDisableVertexAttribArray(3)\n            glDisableVertexAttribArray(2)\n            glDisableVertexAttribArray(1)\n            glDisableVertexAttribArray(0)\n\n        glBindBuffer(GL_ARRAY_BUFFER, 0)\n\n        glUseProgram(0)\n\n        glDisable(GL_BLEND)\n        glDisable(GL_MULTISAMPLE)\n\n        self.draw_end()\n"""
lib/renderer/gl/render.py,0,"b'from ctypes import *\n\nimport numpy as np\nfrom .framework import *\n\nGLUT = None\n\n# NOTE: Render class assumes GL context is created already.\nclass Render:\n    def __init__(self, width=1600, height=1200, name=\'GL Renderer\',\n                 program_files=[\'simple.fs\', \'simple.vs\'], color_size=1, ms_rate=1, egl=False):\n        self.width = width\n        self.height = height\n        self.name = name\n        self.use_inverse_depth = False\n        self.egl = egl\n        \n        glEnable(GL_DEPTH_TEST)\n\n        glClampColor(GL_CLAMP_READ_COLOR, GL_FALSE)\n        glClampColor(GL_CLAMP_FRAGMENT_COLOR, GL_FALSE)\n        glClampColor(GL_CLAMP_VERTEX_COLOR, GL_FALSE)\n\n        # init program\n        shader_list = []\n\n        for program_file in program_files:\n            _, ext = os.path.splitext(program_file)\n            if ext == \'.vs\':\n                shader_list.append(loadShader(GL_VERTEX_SHADER, program_file))\n            elif ext == \'.fs\':\n                shader_list.append(loadShader(GL_FRAGMENT_SHADER, program_file))\n            elif ext == \'.gs\':\n                shader_list.append(loadShader(GL_GEOMETRY_SHADER, program_file))\n\n        self.program = createProgram(shader_list)\n\n        for shader in shader_list:\n            glDeleteShader(shader)\n\n        # Init uniform variables\n        self.model_mat_unif = glGetUniformLocation(self.program, \'ModelMat\')\n        self.persp_mat_unif = glGetUniformLocation(self.program, \'PerspMat\')\n\n        self.vertex_buffer = glGenBuffers(1)\n\n        # Init screen quad program and buffer\n        self.quad_program, self.quad_buffer = self.init_quad_program()\n\n        # Configure frame buffer\n        self.frame_buffer = glGenFramebuffers(1)\n        glBindFramebuffer(GL_FRAMEBUFFER, self.frame_buffer)\n\n        self.intermediate_fbo = None\n        if ms_rate > 1:\n            # Configure texture buffer to render to\n            self.color_buffer = []\n            for i in range(color_size):\n                color_buffer = glGenTextures(1)\n                multi_sample_rate = ms_rate\n                glBindTexture(GL_TEXTURE_2D_MULTISAMPLE, color_buffer)\n                glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE)\n                glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE)\n                glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR)\n                glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR)\n                glTexImage2DMultisample(GL_TEXTURE_2D_MULTISAMPLE, multi_sample_rate, GL_RGBA32F, self.width, self.height, GL_TRUE)\n                glBindTexture(GL_TEXTURE_2D_MULTISAMPLE, 0)\n                glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0 + i, GL_TEXTURE_2D_MULTISAMPLE, color_buffer, 0)\n                self.color_buffer.append(color_buffer)\n\n            self.render_buffer = glGenRenderbuffers(1)\n            glBindRenderbuffer(GL_RENDERBUFFER, self.render_buffer)\n            glRenderbufferStorageMultisample(GL_RENDERBUFFER, multi_sample_rate, GL_DEPTH24_STENCIL8, self.width, self.height)\n            glBindRenderbuffer(GL_RENDERBUFFER, 0)\n            glFramebufferRenderbuffer(GL_FRAMEBUFFER, GL_DEPTH_STENCIL_ATTACHMENT, GL_RENDERBUFFER, self.render_buffer)\n\n            attachments = []\n            for i in range(color_size):\n                attachments.append(GL_COLOR_ATTACHMENT0 + i)\n            glDrawBuffers(color_size, attachments)\n            glBindFramebuffer(GL_FRAMEBUFFER, 0)\n\n            self.intermediate_fbo = glGenFramebuffers(1)\n            glBindFramebuffer(GL_FRAMEBUFFER, self.intermediate_fbo)\n\n            self.screen_texture = []\n            for i in range(color_size):\n                screen_texture = glGenTextures(1)\n                glBindTexture(GL_TEXTURE_2D, screen_texture)\n                glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA32F, self.width, self.height, 0, GL_RGBA, GL_FLOAT, None)\n                glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR)\n                glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR)\n                glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0 + i, GL_TEXTURE_2D, screen_texture, 0)\n                self.screen_texture.append(screen_texture)\n\n            glDrawBuffers(color_size, attachments)\n            glBindFramebuffer(GL_FRAMEBUFFER, 0)\n        else:\n            self.color_buffer = []\n            for i in range(color_size):\n                color_buffer = glGenTextures(1)\n                glBindTexture(GL_TEXTURE_2D, color_buffer)\n                glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE)\n                glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE)\n                glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST)\n                glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST)\n                glTexImage2D(GL_TEXTURE_2D, 0, GL_RGBA32F, self.width, self.height, 0, GL_RGBA, GL_FLOAT, None)\n                glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0 + i, GL_TEXTURE_2D, color_buffer, 0)\n                self.color_buffer.append(color_buffer)\n \n            # Configure depth texture map to render to\n            self.depth_buffer = glGenTextures(1)\n            glBindTexture(GL_TEXTURE_2D, self.depth_buffer)\n            glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT)\n            glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT)\n            glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST)\n            glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST)\n            glTexParameteri(GL_TEXTURE_2D, GL_DEPTH_TEXTURE_MODE, GL_INTENSITY)\n            glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_COMPARE_MODE, GL_COMPARE_R_TO_TEXTURE)\n            glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_COMPARE_FUNC, GL_LEQUAL)\n            glTexImage2D(GL_TEXTURE_2D, 0, GL_DEPTH_COMPONENT, self.width, self.height, 0, GL_DEPTH_COMPONENT, GL_FLOAT, None)\n            glFramebufferTexture2D(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_TEXTURE_2D, self.depth_buffer, 0)\n\n            attachments = []\n            for i in range(color_size):\n                attachments.append(GL_COLOR_ATTACHMENT0 + i)\n            glDrawBuffers(color_size, attachments)\n            self.screen_texture = self.color_buffer\n\n            glBindFramebuffer(GL_FRAMEBUFFER, 0)\n\n        \n        # Configure texture buffer if needed\n        self.render_texture = None\n\n        # NOTE: original render_texture only support one input\n        # this is tentative member of this issue\n        self.render_texture_v2 = {}\n\n        # Inner storage for buffer data\n        self.vertex_data = None\n        self.vertex_dim = None\n        self.n_vertices = None\n\n        self.model_view_matrix = None\n        self.projection_matrix = None\n\n        if not egl:\n            global GLUT\n            import OpenGL.GLUT as GLUT\n            GLUT.glutDisplayFunc(self.display)\n\n\n    def init_quad_program(self):\n        shader_list = []\n\n        shader_list.append(loadShader(GL_VERTEX_SHADER, ""quad.vs""))\n        shader_list.append(loadShader(GL_FRAGMENT_SHADER, ""quad.fs""))\n\n        the_program = createProgram(shader_list)\n\n        for shader in shader_list:\n            glDeleteShader(shader)\n\n        # vertex attributes for a quad that fills the entire screen in Normalized Device Coordinates.\n        # positions # texCoords\n        quad_vertices = np.array(\n            [-1.0, 1.0, 0.0, 1.0,\n             -1.0, -1.0, 0.0, 0.0,\n             1.0, -1.0, 1.0, 0.0,\n\n             -1.0, 1.0, 0.0, 1.0,\n             1.0, -1.0, 1.0, 0.0,\n             1.0, 1.0, 1.0, 1.0]\n        )\n\n        quad_buffer = glGenBuffers(1)\n        glBindBuffer(GL_ARRAY_BUFFER, quad_buffer)\n        glBufferData(GL_ARRAY_BUFFER, quad_vertices, GL_STATIC_DRAW)\n\n        glBindBuffer(GL_ARRAY_BUFFER, 0)\n\n        return the_program, quad_buffer\n\n    def set_mesh(self, vertices, faces):\n        self.vertex_data = vertices[faces.reshape([-1])]\n        self.vertex_dim = self.vertex_data.shape[1]\n        self.n_vertices = self.vertex_data.shape[0]\n\n        glBindBuffer(GL_ARRAY_BUFFER, self.vertex_buffer)\n        glBufferData(GL_ARRAY_BUFFER, self.vertex_data, GL_STATIC_DRAW)\n\n        glBindBuffer(GL_ARRAY_BUFFER, 0)\n\n    def set_viewpoint(self, projection, model_view):\n        self.projection_matrix = projection\n        self.model_view_matrix = model_view\n\n    def draw_init(self):\n        glBindFramebuffer(GL_FRAMEBUFFER, self.frame_buffer)\n        glEnable(GL_DEPTH_TEST)\n\n        glClearColor(0.0, 0.0, 0.0, 0.0)\n        if self.use_inverse_depth:\n            glDepthFunc(GL_GREATER)\n            glClearDepth(0.0)\n        else:\n            glDepthFunc(GL_LESS)\n            glClearDepth(1.0)\n        glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT)\n\n    def draw_end(self):\n        if self.intermediate_fbo is not None:\n            for i in range(len(self.color_buffer)):\n                glBindFramebuffer(GL_READ_FRAMEBUFFER, self.frame_buffer)\n                glReadBuffer(GL_COLOR_ATTACHMENT0 + i)\n                glBindFramebuffer(GL_DRAW_FRAMEBUFFER, self.intermediate_fbo)\n                glDrawBuffer(GL_COLOR_ATTACHMENT0 + i)\n                glBlitFramebuffer(0, 0, self.width, self.height, 0, 0, self.width, self.height, GL_COLOR_BUFFER_BIT, GL_NEAREST)\n\n        glBindFramebuffer(GL_FRAMEBUFFER, 0)\n        glDepthFunc(GL_LESS)\n        glClearDepth(1.0)\n\n    def draw(self):\n        self.draw_init()\n\n        glUseProgram(self.program)\n        glUniformMatrix4fv(self.model_mat_unif, 1, GL_FALSE, self.model_view_matrix.transpose())\n        glUniformMatrix4fv(self.persp_mat_unif, 1, GL_FALSE, self.projection_matrix.transpose())\n\n        glBindBuffer(GL_ARRAY_BUFFER, self.vertex_buffer)\n\n        glEnableVertexAttribArray(0)\n        glVertexAttribPointer(0, self.vertex_dim, GL_DOUBLE, GL_FALSE, 0, None)\n\n        glDrawArrays(GL_TRIANGLES, 0, self.n_vertices)\n\n        glDisableVertexAttribArray(0)\n\n        glBindBuffer(GL_ARRAY_BUFFER, 0)\n\n        glUseProgram(0)\n\n        self.draw_end()\n\n    def get_color(self, color_id=0):\n        glBindFramebuffer(GL_FRAMEBUFFER, self.intermediate_fbo if self.intermediate_fbo is not None else self.frame_buffer)\n        glReadBuffer(GL_COLOR_ATTACHMENT0 + color_id)\n        data = glReadPixels(0, 0, self.width, self.height, GL_RGBA, GL_FLOAT, outputType=None)\n        glBindFramebuffer(GL_FRAMEBUFFER, 0)\n        rgb = data.reshape(self.height, self.width, -1)\n        rgb = np.flip(rgb, 0)\n        return rgb\n\n    def get_z_value(self):\n        glBindFramebuffer(GL_FRAMEBUFFER, self.frame_buffer)\n        data = glReadPixels(0, 0, self.width, self.height, GL_DEPTH_COMPONENT, GL_FLOAT, outputType=None)\n        glBindFramebuffer(GL_FRAMEBUFFER, 0)\n        z = data.reshape(self.height, self.width)\n        z = np.flip(z, 0)\n        return z\n\n    def display(self):\n        self.draw()\n\n        if not self.egl:\n            # First we draw a scene.\n            # Notice the result is stored in the texture buffer.\n\n            # Then we return to the default frame buffer since we will display on the screen.\n            glBindFramebuffer(GL_FRAMEBUFFER, 0)\n\n            # Do the clean-up.\n            glClearColor(0.0, 0.0, 0.0, 0.0)\n            glClear(GL_COLOR_BUFFER_BIT)\n\n            # We draw a rectangle which covers the whole screen.\n            glUseProgram(self.quad_program)\n            glBindBuffer(GL_ARRAY_BUFFER, self.quad_buffer)\n\n            size_of_double = 8\n            glEnableVertexAttribArray(0)\n            glVertexAttribPointer(0, 2, GL_DOUBLE, GL_FALSE, 4 * size_of_double, None)\n            glEnableVertexAttribArray(1)\n            glVertexAttribPointer(1, 2, GL_DOUBLE, GL_FALSE, 4 * size_of_double, c_void_p(2 * size_of_double))\n\n            glDisable(GL_DEPTH_TEST)\n\n            # The stored texture is then mapped to this rectangle.\n            # properly assing color buffer texture\n            glActiveTexture(GL_TEXTURE0)\n            glBindTexture(GL_TEXTURE_2D, self.screen_texture[0])\n            glUniform1i(glGetUniformLocation(self.quad_program, \'screenTexture\'), 0)\n\n            glDrawArrays(GL_TRIANGLES, 0, 6)\n\n            glDisableVertexAttribArray(1)\n            glDisableVertexAttribArray(0)\n\n            glEnable(GL_DEPTH_TEST)\n            glBindBuffer(GL_ARRAY_BUFFER, 0)\n            glUseProgram(0)\n\n            GLUT.glutSwapBuffers()\n            GLUT.glutPostRedisplay()\n\n    def show(self):\n        if not self.egl:\n            GLUT.glutMainLoop()\n'"
