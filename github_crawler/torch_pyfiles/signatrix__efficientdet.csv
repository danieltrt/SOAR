file_path,api_count,code
mAP_evaluation.py,2,"b'from src.dataset import CocoDataset, Resizer, Normalizer\nfrom torchvision import transforms\nfrom pycocotools.cocoeval import COCOeval\nimport json\nimport torch\n\n\ndef evaluate_coco(dataset, model, threshold=0.05):\n    model.eval()\n    with torch.no_grad():\n        results = []\n        image_ids = []\n\n        for index in range(len(dataset)):\n            data = dataset[index]\n            scale = data[\'scale\']\n            scores, labels, boxes = model(data[\'img\'].cuda().permute(2, 0, 1).float().unsqueeze(dim=0))\n            boxes /= scale\n\n            if boxes.shape[0] > 0:\n\n                boxes[:, 2] -= boxes[:, 0]\n                boxes[:, 3] -= boxes[:, 1]\n\n                for box_id in range(boxes.shape[0]):\n                    score = float(scores[box_id])\n                    label = int(labels[box_id])\n                    box = boxes[box_id, :]\n\n                    if score < threshold:\n                        break\n\n                    image_result = {\n                        \'image_id\': dataset.image_ids[index],\n                        \'category_id\': dataset.label_to_coco_label(label),\n                        \'score\': float(score),\n                        \'bbox\': box.tolist(),\n                    }\n\n                    results.append(image_result)\n\n            # append image to list of processed images\n            image_ids.append(dataset.image_ids[index])\n\n            # print progress\n            print(\'{}/{}\'.format(index, len(dataset)), end=\'\\r\')\n\n        if not len(results):\n            return\n\n        # write output\n        json.dump(results, open(\'{}_bbox_results.json\'.format(dataset.set_name), \'w\'), indent=4)\n\n        # load results in COCO evaluation tool\n        coco_true = dataset.coco\n        coco_pred = coco_true.loadRes(\'{}_bbox_results.json\'.format(dataset.set_name))\n\n        # run COCO evaluation\n        coco_eval = COCOeval(coco_true, coco_pred, \'bbox\')\n        coco_eval.params.imgIds = image_ids\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        coco_eval.summarize()\n\n\nif __name__ == \'__main__\':\n    efficientdet = torch.load(""trained_models/signatrix_efficientdet_coco.pth"").module\n    efficientdet.cuda()\n    dataset_val = CocoDataset(""data/COCO"", set=\'val2017\',\n                              transform=transforms.Compose([Normalizer(), Resizer()]))\n    evaluate_coco(dataset_val, efficientdet)\n'"
test_dataset.py,2,"b'import os\nimport argparse\nimport torch\nfrom torchvision import transforms\nfrom src.dataset import CocoDataset, Resizer, Normalizer\nfrom src.config import COCO_CLASSES, colors\nimport cv2\nimport shutil\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\n        ""EfficientDet: Scalable and Efficient Object Detection implementation by Signatrix GmbH"")\n    parser.add_argument(""--image_size"", type=int, default=512, help=""The common width and height for all images"")\n    parser.add_argument(""--data_path"", type=str, default=""data/COCO"", help=""the root folder of dataset"")\n    parser.add_argument(""--cls_threshold"", type=float, default=0.5)\n    parser.add_argument(""--nms_threshold"", type=float, default=0.5)\n    parser.add_argument(""--pretrained_model"", type=str, default=""trained_models/signatrix_efficientdet_coco.pth"")\n    parser.add_argument(""--output"", type=str, default=""predictions"")\n    args = parser.parse_args()\n    return args\n\n\n\ndef test(opt):\n    model = torch.load(opt.pretrained_model).module\n    model.cuda()\n    dataset = CocoDataset(opt.data_path, set=\'val2017\', transform=transforms.Compose([Normalizer(), Resizer()]))\n\n    if os.path.isdir(opt.output):\n        shutil.rmtree(opt.output)\n    os.makedirs(opt.output)\n\n    for index in range(len(dataset)):\n        data = dataset[index]\n        scale = data[\'scale\']\n        with torch.no_grad():\n            scores, labels, boxes = model(data[\'img\'].cuda().permute(2, 0, 1).float().unsqueeze(dim=0))\n            boxes /= scale\n\n        if boxes.shape[0] > 0:\n            image_info = dataset.coco.loadImgs(dataset.image_ids[index])[0]\n            path = os.path.join(dataset.root_dir, \'images\', dataset.set_name, image_info[\'file_name\'])\n            output_image = cv2.imread(path)\n\n            for box_id in range(boxes.shape[0]):\n                pred_prob = float(scores[box_id])\n                if pred_prob < opt.cls_threshold:\n                    break\n                pred_label = int(labels[box_id])\n                xmin, ymin, xmax, ymax = boxes[box_id, :]\n                color = colors[pred_label]\n                cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)\n                text_size = cv2.getTextSize(COCO_CLASSES[pred_label] + \' : %.2f\' % pred_prob, cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n\n                cv2.rectangle(output_image, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4), color, -1)\n                cv2.putText(\n                    output_image, COCO_CLASSES[pred_label] + \' : %.2f\' % pred_prob,\n                    (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,\n                    (255, 255, 255), 1)\n\n            cv2.imwrite(""{}/{}_prediction.jpg"".format(opt.output, image_info[""file_name""][:-4]), output_image)\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    test(opt)\n'"
test_video.py,5,"b'import argparse\nimport torch\nfrom src.config import COCO_CLASSES, colors\nimport cv2\nimport numpy as np\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\n        ""EfficientDet: Scalable and Efficient Object Detection implementation by Signatrix GmbH"")\n    parser.add_argument(""--image_size"", type=int, default=512, help=""The common width and height for all images"")\n    parser.add_argument(""--cls_threshold"", type=float, default=0.5)\n    parser.add_argument(""--nms_threshold"", type=float, default=0.5)\n    parser.add_argument(""--pretrained_model"", type=str, default=""trained_models/signatrix_efficientdet_coco.pth"")\n    parser.add_argument(""--input"", type=str, default=""test_videos/input.mp4"")\n    parser.add_argument(""--output"", type=str, default=""test_videos/output.mp4"")\n\n    args = parser.parse_args()\n    return args\n\n\ndef test(opt):\n    model = torch.load(opt.pretrained_model).module\n    if torch.cuda.is_available():\n        model.cuda()\n\n    cap = cv2.VideoCapture(opt.input)\n    out = cv2.VideoWriter(opt.output,  cv2.VideoWriter_fourcc(*""MJPG""), int(cap.get(cv2.CAP_PROP_FPS)),\n                          (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n    while cap.isOpened():\n        flag, image = cap.read()\n        output_image = np.copy(image)\n        if flag:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        else:\n            break\n        height, width = image.shape[:2]\n        image = image.astype(np.float32) / 255\n        image[:, :, 0] = (image[:, :, 0] - 0.485) / 0.229\n        image[:, :, 1] = (image[:, :, 1] - 0.456) / 0.224\n        image[:, :, 2] = (image[:, :, 2] - 0.406) / 0.225\n        if height > width:\n            scale = opt.image_size / height\n            resized_height = opt.image_size\n            resized_width = int(width * scale)\n        else:\n            scale = opt.image_size / width\n            resized_height = int(height * scale)\n            resized_width = opt.image_size\n\n        image = cv2.resize(image, (resized_width, resized_height))\n\n        new_image = np.zeros((opt.image_size, opt.image_size, 3))\n        new_image[0:resized_height, 0:resized_width] = image\n        new_image = np.transpose(new_image, (2, 0, 1))\n        new_image = new_image[None, :, :, :]\n        new_image = torch.Tensor(new_image)\n        if torch.cuda.is_available():\n            new_image = new_image.cuda()\n        with torch.no_grad():\n            scores, labels, boxes = model(new_image)\n            boxes /= scale\n        if boxes.shape[0] == 0:\n            continue\n\n        for box_id in range(boxes.shape[0]):\n            pred_prob = float(scores[box_id])\n            if pred_prob < opt.cls_threshold:\n                break\n            pred_label = int(labels[box_id])\n            xmin, ymin, xmax, ymax = boxes[box_id, :]\n            color = colors[pred_label]\n            cv2.rectangle(output_image, (xmin, ymin), (xmax, ymax), color, 2)\n            text_size = cv2.getTextSize(COCO_CLASSES[pred_label] + \' : %.2f\' % pred_prob, cv2.FONT_HERSHEY_PLAIN, 1, 1)[0]\n            cv2.rectangle(output_image, (xmin, ymin), (xmin + text_size[0] + 3, ymin + text_size[1] + 4), color, -1)\n            cv2.putText(\n                output_image, COCO_CLASSES[pred_label] + \' : %.2f\' % pred_prob,\n                (xmin, ymin + text_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1,\n                (255, 255, 255), 1)\n        out.write(output_image)\n\n    cap.release()\n    out.release()\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    test(opt)\n'"
train.py,19,"b'import os\nimport argparse\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom src.dataset import CocoDataset, Resizer, Normalizer, Augmenter, collater\nfrom src.model import EfficientDet\nfrom tensorboardX import SummaryWriter\nimport shutil\nimport numpy as np\nfrom tqdm.autonotebook import tqdm\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(\n        ""EfficientDet: Scalable and Efficient Object Detection implementation by Signatrix GmbH"")\n    parser.add_argument(""--image_size"", type=int, default=512, help=""The common width and height for all images"")\n    parser.add_argument(""--batch_size"", type=int, default=8, help=""The number of images per batch"")\n    parser.add_argument(""--lr"", type=float, default=1e-4)\n    parser.add_argument(\'--alpha\', type=float, default=0.25)\n    parser.add_argument(\'--gamma\', type=float, default=1.5)\n    parser.add_argument(""--num_epochs"", type=int, default=500)\n    parser.add_argument(""--test_interval"", type=int, default=1, help=""Number of epoches between testing phases"")\n    parser.add_argument(""--es_min_delta"", type=float, default=0.0,\n                        help=""Early stopping\'s parameter: minimum change loss to qualify as an improvement"")\n    parser.add_argument(""--es_patience"", type=int, default=0,\n                        help=""Early stopping\'s parameter: number of epochs with no improvement after which training will be stopped. Set to 0 to disable this technique."")\n    parser.add_argument(""--data_path"", type=str, default=""data/COCO"", help=""the root folder of dataset"")\n    parser.add_argument(""--log_path"", type=str, default=""tensorboard/signatrix_efficientdet_coco"")\n    parser.add_argument(""--saved_path"", type=str, default=""trained_models"")\n\n    args = parser.parse_args()\n    return args\n\n\ndef train(opt):\n    num_gpus = 1\n    if torch.cuda.is_available():\n        num_gpus = torch.cuda.device_count()\n        torch.cuda.manual_seed(123)\n    else:\n        torch.manual_seed(123)\n\n    training_params = {""batch_size"": opt.batch_size * num_gpus,\n                       ""shuffle"": True,\n                       ""drop_last"": True,\n                       ""collate_fn"": collater,\n                       ""num_workers"": 12}\n\n    test_params = {""batch_size"": opt.batch_size,\n                   ""shuffle"": False,\n                   ""drop_last"": False,\n                   ""collate_fn"": collater,\n                   ""num_workers"": 12}\n\n    training_set = CocoDataset(root_dir=opt.data_path, set=""train2017"",\n                               transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]))\n    training_generator = DataLoader(training_set, **training_params)\n\n    test_set = CocoDataset(root_dir=opt.data_path, set=""val2017"",\n                           transform=transforms.Compose([Normalizer(), Resizer()]))\n    test_generator = DataLoader(test_set, **test_params)\n\n    model = EfficientDet(num_classes=training_set.num_classes())\n\n\n    if os.path.isdir(opt.log_path):\n        shutil.rmtree(opt.log_path)\n    os.makedirs(opt.log_path)\n\n    if not os.path.isdir(opt.saved_path):\n        os.makedirs(opt.saved_path)\n\n    writer = SummaryWriter(opt.log_path)\n    if torch.cuda.is_available():\n        model = model.cuda()\n        model = nn.DataParallel(model)\n\n    optimizer = torch.optim.Adam(model.parameters(), opt.lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n\n    best_loss = 1e5\n    best_epoch = 0\n    model.train()\n\n    num_iter_per_epoch = len(training_generator)\n    for epoch in range(opt.num_epochs):\n        model.train()\n        # if torch.cuda.is_available():\n        #     model.module.freeze_bn()\n        # else:\n        #     model.freeze_bn()\n        epoch_loss = []\n        progress_bar = tqdm(training_generator)\n        for iter, data in enumerate(progress_bar):\n            try:\n                optimizer.zero_grad()\n                if torch.cuda.is_available():\n                    cls_loss, reg_loss = model([data[\'img\'].cuda().float(), data[\'annot\'].cuda()])\n                else:\n                    cls_loss, reg_loss = model([data[\'img\'].float(), data[\'annot\']])\n\n                cls_loss = cls_loss.mean()\n                reg_loss = reg_loss.mean()\n                loss = cls_loss + reg_loss\n                if loss == 0:\n                    continue\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n                optimizer.step()\n                epoch_loss.append(float(loss))\n                total_loss = np.mean(epoch_loss)\n\n                progress_bar.set_description(\n                    \'Epoch: {}/{}. Iteration: {}/{}. Cls loss: {:.5f}. Reg loss: {:.5f}. Batch loss: {:.5f} Total loss: {:.5f}\'.format(\n                        epoch + 1, opt.num_epochs, iter + 1, num_iter_per_epoch, cls_loss, reg_loss, loss,\n                        total_loss))\n                writer.add_scalar(\'Train/Total_loss\', total_loss, epoch * num_iter_per_epoch + iter)\n                writer.add_scalar(\'Train/Regression_loss\', reg_loss, epoch * num_iter_per_epoch + iter)\n                writer.add_scalar(\'Train/Classfication_loss (focal loss)\', cls_loss, epoch * num_iter_per_epoch + iter)\n\n            except Exception as e:\n                print(e)\n                continue\n        scheduler.step(np.mean(epoch_loss))\n\n        if epoch % opt.test_interval == 0:\n            model.eval()\n            loss_regression_ls = []\n            loss_classification_ls = []\n            for iter, data in enumerate(test_generator):\n                with torch.no_grad():\n                    if torch.cuda.is_available():\n                        cls_loss, reg_loss = model([data[\'img\'].cuda().float(), data[\'annot\'].cuda()])\n                    else:\n                        cls_loss, reg_loss = model([data[\'img\'].float(), data[\'annot\']])\n\n                    cls_loss = cls_loss.mean()\n                    reg_loss = reg_loss.mean()\n\n                    loss_classification_ls.append(float(cls_loss))\n                    loss_regression_ls.append(float(reg_loss))\n\n            cls_loss = np.mean(loss_classification_ls)\n            reg_loss = np.mean(loss_regression_ls)\n            loss = cls_loss + reg_loss\n\n            print(\n                \'Epoch: {}/{}. Classification loss: {:1.5f}. Regression loss: {:1.5f}. Total loss: {:1.5f}\'.format(\n                    epoch + 1, opt.num_epochs, cls_loss, reg_loss,\n                    np.mean(loss)))\n            writer.add_scalar(\'Test/Total_loss\', loss, epoch)\n            writer.add_scalar(\'Test/Regression_loss\', reg_loss, epoch)\n            writer.add_scalar(\'Test/Classfication_loss (focal loss)\', cls_loss, epoch)\n\n            if loss + opt.es_min_delta < best_loss:\n                best_loss = loss\n                best_epoch = epoch\n                torch.save(model, os.path.join(opt.saved_path, ""signatrix_efficientdet_coco.pth""))\n\n                dummy_input = torch.rand(opt.batch_size, 3, 512, 512)\n                if torch.cuda.is_available():\n                    dummy_input = dummy_input.cuda()\n                if isinstance(model, nn.DataParallel):\n                    model.module.backbone_net.model.set_swish(memory_efficient=False)\n\n                    torch.onnx.export(model.module, dummy_input,\n                                      os.path.join(opt.saved_path, ""signatrix_efficientdet_coco.onnx""),\n                                      verbose=False)\n                    model.module.backbone_net.model.set_swish(memory_efficient=True)\n                else:\n                    model.backbone_net.model.set_swish(memory_efficient=False)\n\n                    torch.onnx.export(model, dummy_input,\n                                      os.path.join(opt.saved_path, ""signatrix_efficientdet_coco.onnx""),\n                                      verbose=False)\n                    model.backbone_net.model.set_swish(memory_efficient=True)\n\n            # Early stopping\n            if epoch - best_epoch > opt.es_patience > 0:\n                print(""Stop training at epoch {}. The lowest loss achieved is {}"".format(epoch, loss))\n                break\n    writer.close()\n\n\nif __name__ == ""__main__"":\n    opt = get_args()\n    train(opt)\n'"
src/config.py,0,"b'COCO_CLASSES = [""person"", ""bicycle"", ""car"", ""motorcycle"", ""airplane"", ""bus"", ""train"", ""truck"", ""boat"",\n                ""traffic light"", ""fire hydrant"", ""stop sign"", ""parking meter"", ""bench"", ""bird"", ""cat"", ""dog"",\n                ""horse"", ""sheep"", ""cow"", ""elephant"", ""bear"", ""zebra"", ""giraffe"", ""backpack"", ""umbrella"",\n                ""handbag"", ""tie"", ""suitcase"", ""frisbee"", ""skis"", ""snowboard"", ""sports ball"", ""kite"",\n                ""baseball bat"", ""baseball glove"", ""skateboard"", ""surfboard"", ""tennis racket"", ""bottle"",\n                ""wine glass"", ""cup"", ""fork"", ""knife"", ""spoon"", ""bowl"", ""banana"", ""apple"", ""sandwich"", ""orange"",\n                ""broccoli"", ""carrot"", ""hot dog"", ""pizza"", ""donut"", ""cake"", ""chair"", ""couch"", ""potted plant"",\n                ""bed"", ""dining table"", ""toilet"", ""tv"", ""laptop"", ""mouse"", ""remote"", ""keyboard"", ""cell phone"",\n                ""microwave"", ""oven"", ""toaster"", ""sink"", ""refrigerator"", ""book"", ""clock"", ""vase"", ""scissors"",\n                ""teddy bear"", ""hair drier"", ""toothbrush""]\n\ncolors = [(39, 129, 113), (164, 80, 133), (83, 122, 114), (99, 81, 172), (95, 56, 104), (37, 84, 86), (14, 89, 122),\n          (80, 7, 65), (10, 102, 25), (90, 185, 109), (106, 110, 132), (169, 158, 85), (188, 185, 26), (103, 1, 17),\n          (82, 144, 81), (92, 7, 184), (49, 81, 155), (179, 177, 69), (93, 187, 158), (13, 39, 73), (12, 50, 60),\n          (16, 179, 33), (112, 69, 165), (15, 139, 63), (33, 191, 159), (182, 173, 32), (34, 113, 133), (90, 135, 34),\n          (53, 34, 86), (141, 35, 190), (6, 171, 8), (118, 76, 112), (89, 60, 55), (15, 54, 88), (112, 75, 181),\n          (42, 147, 38), (138, 52, 63), (128, 65, 149), (106, 103, 24), (168, 33, 45), (28, 136, 135), (86, 91, 108),\n          (52, 11, 76), (142, 6, 189), (57, 81, 168), (55, 19, 148), (182, 101, 89), (44, 65, 179), (1, 33, 26),\n          (122, 164, 26), (70, 63, 134), (137, 106, 82), (120, 118, 52), (129, 74, 42), (182, 147, 112), (22, 157, 50),\n          (56, 50, 20), (2, 22, 177), (156, 100, 106), (21, 35, 42), (13, 8, 121), (142, 92, 28), (45, 118, 33),\n          (105, 118, 30), (7, 185, 124), (46, 34, 146), (105, 184, 169), (22, 18, 5), (147, 71, 73), (181, 64, 91),\n          (31, 39, 184), (164, 179, 33), (96, 50, 18), (95, 15, 106), (113, 68, 54), (136, 116, 112), (119, 139, 130),\n          (31, 139, 34), (66, 6, 127), (62, 39, 2), (49, 99, 180), (49, 119, 155), (153, 50, 183), (125, 38, 3),\n          (129, 87, 143), (49, 87, 40), (128, 62, 120), (73, 85, 148), (28, 144, 118), (29, 9, 24), (175, 45, 108),\n          (81, 175, 64), (178, 19, 157), (74, 188, 190), (18, 114, 2), (62, 128, 96), (21, 3, 150), (0, 6, 95),\n          (2, 20, 184), (122, 37, 185)]\n'"
src/dataset.py,5,"b'import os\nimport torch\nimport numpy as np\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom pycocotools.coco import COCO\nimport cv2\n\n\nclass CocoDataset(Dataset):\n    def __init__(self, root_dir, set=\'train2017\', transform=None):\n\n        self.root_dir = root_dir\n        self.set_name = set\n        self.transform = transform\n\n        self.coco = COCO(os.path.join(self.root_dir, \'annotations\', \'instances_\' + self.set_name + \'.json\'))\n        self.image_ids = self.coco.getImgIds()\n\n        self.load_classes()\n\n    def load_classes(self):\n\n        # load class names (name -> label)\n        categories = self.coco.loadCats(self.coco.getCatIds())\n        categories.sort(key=lambda x: x[\'id\'])\n\n        self.classes = {}\n        self.coco_labels = {}\n        self.coco_labels_inverse = {}\n        for c in categories:\n            self.coco_labels[len(self.classes)] = c[\'id\']\n            self.coco_labels_inverse[c[\'id\']] = len(self.classes)\n            self.classes[c[\'name\']] = len(self.classes)\n\n        # also load the reverse (label -> name)\n        self.labels = {}\n        for key, value in self.classes.items():\n            self.labels[value] = key\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n\n        img = self.load_image(idx)\n        annot = self.load_annotations(idx)\n        sample = {\'img\': img, \'annot\': annot}\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n\n    def load_image(self, image_index):\n        image_info = self.coco.loadImgs(self.image_ids[image_index])[0]\n        path = os.path.join(self.root_dir, \'images\', self.set_name, image_info[\'file_name\'])\n        img = cv2.imread(path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # if len(img.shape) == 2:\n        #     img = skimage.color.gray2rgb(img)\n\n        return img.astype(np.float32) / 255.\n\n    def load_annotations(self, image_index):\n        # get ground truth annotations\n        annotations_ids = self.coco.getAnnIds(imgIds=self.image_ids[image_index], iscrowd=False)\n        annotations = np.zeros((0, 5))\n\n        # some images appear to miss annotations\n        if len(annotations_ids) == 0:\n            return annotations\n\n        # parse annotations\n        coco_annotations = self.coco.loadAnns(annotations_ids)\n        for idx, a in enumerate(coco_annotations):\n\n            # some annotations have basically no width / height, skip them\n            if a[\'bbox\'][2] < 1 or a[\'bbox\'][3] < 1:\n                continue\n\n            annotation = np.zeros((1, 5))\n            annotation[0, :4] = a[\'bbox\']\n            annotation[0, 4] = self.coco_label_to_label(a[\'category_id\'])\n            annotations = np.append(annotations, annotation, axis=0)\n\n        # transform from [x, y, w, h] to [x1, y1, x2, y2]\n        annotations[:, 2] = annotations[:, 0] + annotations[:, 2]\n        annotations[:, 3] = annotations[:, 1] + annotations[:, 3]\n\n        return annotations\n\n    def coco_label_to_label(self, coco_label):\n        return self.coco_labels_inverse[coco_label]\n\n    def label_to_coco_label(self, label):\n        return self.coco_labels[label]\n\n    def num_classes(self):\n        return 80\n\n\ndef collater(data):\n    imgs = [s[\'img\'] for s in data]\n    annots = [s[\'annot\'] for s in data]\n    scales = [s[\'scale\'] for s in data]\n\n    imgs = torch.from_numpy(np.stack(imgs, axis=0))\n\n    max_num_annots = max(annot.shape[0] for annot in annots)\n\n    if max_num_annots > 0:\n\n        annot_padded = torch.ones((len(annots), max_num_annots, 5)) * -1\n\n        if max_num_annots > 0:\n            for idx, annot in enumerate(annots):\n                if annot.shape[0] > 0:\n                    annot_padded[idx, :annot.shape[0], :] = annot\n    else:\n        annot_padded = torch.ones((len(annots), 1, 5)) * -1\n\n    imgs = imgs.permute(0, 3, 1, 2)\n\n    return {\'img\': imgs, \'annot\': annot_padded, \'scale\': scales}\n\n\nclass Resizer(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample, common_size=512):\n        image, annots = sample[\'img\'], sample[\'annot\']\n        height, width, _ = image.shape\n        if height > width:\n            scale = common_size / height\n            resized_height = common_size\n            resized_width = int(width * scale)\n        else:\n            scale = common_size / width\n            resized_height = int(height * scale)\n            resized_width = common_size\n\n        image = cv2.resize(image, (resized_width, resized_height))\n\n        new_image = np.zeros((common_size, common_size, 3))\n        new_image[0:resized_height, 0:resized_width] = image\n\n        annots[:, :4] *= scale\n\n        return {\'img\': torch.from_numpy(new_image), \'annot\': torch.from_numpy(annots), \'scale\': scale}\n\n\nclass Augmenter(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample, flip_x=0.5):\n        if np.random.rand() < flip_x:\n            image, annots = sample[\'img\'], sample[\'annot\']\n            image = image[:, ::-1, :]\n\n            rows, cols, channels = image.shape\n\n            x1 = annots[:, 0].copy()\n            x2 = annots[:, 2].copy()\n\n            x_tmp = x1.copy()\n\n            annots[:, 0] = cols - x2\n            annots[:, 2] = cols - x_tmp\n\n            sample = {\'img\': image, \'annot\': annots}\n\n        return sample\n\n\nclass Normalizer(object):\n\n    def __init__(self):\n        self.mean = np.array([[[0.485, 0.456, 0.406]]])\n        self.std = np.array([[[0.229, 0.224, 0.225]]])\n\n    def __call__(self, sample):\n        image, annots = sample[\'img\'], sample[\'annot\']\n\n        return {\'img\': ((image.astype(np.float32) - self.mean) / self.std), \'annot\': annots}\n'"
src/loss.py,43,"b'import torch\nimport torch.nn as nn\n\n\ndef calc_iou(a, b):\n\n    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n    iw = torch.min(torch.unsqueeze(a[:, 2], dim=1), b[:, 2]) - torch.max(torch.unsqueeze(a[:, 0], 1), b[:, 0])\n    ih = torch.min(torch.unsqueeze(a[:, 3], dim=1), b[:, 3]) - torch.max(torch.unsqueeze(a[:, 1], 1), b[:, 1])\n    iw = torch.clamp(iw, min=0)\n    ih = torch.clamp(ih, min=0)\n    ua = torch.unsqueeze((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), dim=1) + area - iw * ih\n    ua = torch.clamp(ua, min=1e-8)\n    intersection = iw * ih\n    IoU = intersection / ua\n\n    return IoU\n\n\nclass FocalLoss(nn.Module):\n    def __init__(self):\n        super(FocalLoss, self).__init__()\n\n    def forward(self, classifications, regressions, anchors, annotations):\n        alpha = 0.25\n        gamma = 2.0\n        batch_size = classifications.shape[0]\n        classification_losses = []\n        regression_losses = []\n\n        anchor = anchors[0, :, :]\n\n        anchor_widths = anchor[:, 2] - anchor[:, 0]\n        anchor_heights = anchor[:, 3] - anchor[:, 1]\n        anchor_ctr_x = anchor[:, 0] + 0.5 * anchor_widths\n        anchor_ctr_y = anchor[:, 1] + 0.5 * anchor_heights\n\n        for j in range(batch_size):\n\n            classification = classifications[j, :, :]\n            regression = regressions[j, :, :]\n\n            bbox_annotation = annotations[j, :, :]\n            bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1]\n\n            if bbox_annotation.shape[0] == 0:\n                if torch.cuda.is_available():\n                    regression_losses.append(torch.tensor(0).float().cuda())\n                    classification_losses.append(torch.tensor(0).float().cuda())\n                else:\n                    regression_losses.append(torch.tensor(0).float())\n                    classification_losses.append(torch.tensor(0).float())\n\n                continue\n\n            classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4)\n\n            IoU = calc_iou(anchors[0, :, :], bbox_annotation[:, :4])\n\n            IoU_max, IoU_argmax = torch.max(IoU, dim=1)\n\n            # compute the loss for classification\n            targets = torch.ones(classification.shape) * -1\n            if torch.cuda.is_available():\n                targets = targets.cuda()\n\n            targets[torch.lt(IoU_max, 0.4), :] = 0\n\n            positive_indices = torch.ge(IoU_max, 0.5)\n\n            num_positive_anchors = positive_indices.sum()\n\n            assigned_annotations = bbox_annotation[IoU_argmax, :]\n\n            targets[positive_indices, :] = 0\n            targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1\n\n            alpha_factor = torch.ones(targets.shape) * alpha\n            if torch.cuda.is_available():\n                alpha_factor = alpha_factor.cuda()\n\n            alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor)\n            focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification)\n            focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n\n            bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification))\n\n            cls_loss = focal_weight * bce\n\n            zeros = torch.zeros(cls_loss.shape)\n            if torch.cuda.is_available():\n                zeros = zeros.cuda()\n            cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, zeros)\n\n            classification_losses.append(cls_loss.sum() / torch.clamp(num_positive_anchors.float(), min=1.0))\n\n\n            if positive_indices.sum() > 0:\n                assigned_annotations = assigned_annotations[positive_indices, :]\n\n                anchor_widths_pi = anchor_widths[positive_indices]\n                anchor_heights_pi = anchor_heights[positive_indices]\n                anchor_ctr_x_pi = anchor_ctr_x[positive_indices]\n                anchor_ctr_y_pi = anchor_ctr_y[positive_indices]\n\n                gt_widths = assigned_annotations[:, 2] - assigned_annotations[:, 0]\n                gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1]\n                gt_ctr_x = assigned_annotations[:, 0] + 0.5 * gt_widths\n                gt_ctr_y = assigned_annotations[:, 1] + 0.5 * gt_heights\n\n                gt_widths = torch.clamp(gt_widths, min=1)\n                gt_heights = torch.clamp(gt_heights, min=1)\n\n                targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi\n                targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi\n                targets_dw = torch.log(gt_widths / anchor_widths_pi)\n                targets_dh = torch.log(gt_heights / anchor_heights_pi)\n\n                targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh))\n                targets = targets.t()\n\n                norm = torch.Tensor([[0.1, 0.1, 0.2, 0.2]])\n                if torch.cuda.is_available():\n                    norm = norm.cuda()\n                targets = targets / norm\n\n                regression_diff = torch.abs(targets - regression[positive_indices, :])\n\n                regression_loss = torch.where(\n                    torch.le(regression_diff, 1.0 / 9.0),\n                    0.5 * 9.0 * torch.pow(regression_diff, 2),\n                    regression_diff - 0.5 / 9.0\n                )\n                regression_losses.append(regression_loss.mean())\n            else:\n                if torch.cuda.is_available():\n                    regression_losses.append(torch.tensor(0).float().cuda())\n                else:\n                    regression_losses.append(torch.tensor(0).float())\n\n        return torch.stack(classification_losses).mean(dim=0, keepdim=True), torch.stack(regression_losses).mean(dim=0,\n                                                                                                                 keepdim=True)\n'"
src/model.py,22,"b'import torch.nn as nn\nimport torch\nimport math\nfrom efficientnet_pytorch import EfficientNet as EffNet\nfrom src.utils import BBoxTransform, ClipBoxes, Anchors\nfrom src.loss import FocalLoss\nfrom torchvision.ops.boxes import nms as nms_torch\n\n\ndef nms(dets, thresh):\n    return nms_torch(dets[:, :4], dets[:, 4], thresh)\n\n\nclass ConvBlock(nn.Module):\n    def __init__(self, num_channels):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(num_channels, num_channels, kernel_size=3, stride=1, padding=1, groups=num_channels),\n            nn.Conv2d(num_channels, num_channels, kernel_size=1, stride=1, padding=0),\n            nn.BatchNorm2d(num_features=num_channels, momentum=0.9997, eps=4e-5), nn.ReLU())\n\n    def forward(self, input):\n        return self.conv(input)\n\n\nclass BiFPN(nn.Module):\n    def __init__(self, num_channels, epsilon=1e-4):\n        super(BiFPN, self).__init__()\n        self.epsilon = epsilon\n        # Conv layers\n        self.conv6_up = ConvBlock(num_channels)\n        self.conv5_up = ConvBlock(num_channels)\n        self.conv4_up = ConvBlock(num_channels)\n        self.conv3_up = ConvBlock(num_channels)\n        self.conv4_down = ConvBlock(num_channels)\n        self.conv5_down = ConvBlock(num_channels)\n        self.conv6_down = ConvBlock(num_channels)\n        self.conv7_down = ConvBlock(num_channels)\n\n        # Feature scaling layers\n        self.p6_upsample = nn.Upsample(scale_factor=2, mode=\'nearest\')\n        self.p5_upsample = nn.Upsample(scale_factor=2, mode=\'nearest\')\n        self.p4_upsample = nn.Upsample(scale_factor=2, mode=\'nearest\')\n        self.p3_upsample = nn.Upsample(scale_factor=2, mode=\'nearest\')\n\n        self.p4_downsample = nn.MaxPool2d(kernel_size=2)\n        self.p5_downsample = nn.MaxPool2d(kernel_size=2)\n        self.p6_downsample = nn.MaxPool2d(kernel_size=2)\n        self.p7_downsample = nn.MaxPool2d(kernel_size=2)\n\n        # Weight\n        self.p6_w1 = nn.Parameter(torch.ones(2))\n        self.p6_w1_relu = nn.ReLU()\n        self.p5_w1 = nn.Parameter(torch.ones(2))\n        self.p5_w1_relu = nn.ReLU()\n        self.p4_w1 = nn.Parameter(torch.ones(2))\n        self.p4_w1_relu = nn.ReLU()\n        self.p3_w1 = nn.Parameter(torch.ones(2))\n        self.p3_w1_relu = nn.ReLU()\n\n        self.p4_w2 = nn.Parameter(torch.ones(3))\n        self.p4_w2_relu = nn.ReLU()\n        self.p5_w2 = nn.Parameter(torch.ones(3))\n        self.p5_w2_relu = nn.ReLU()\n        self.p6_w2 = nn.Parameter(torch.ones(3))\n        self.p6_w2_relu = nn.ReLU()\n        self.p7_w2 = nn.Parameter(torch.ones(2))\n        self.p7_w2_relu = nn.ReLU()\n\n    def forward(self, inputs):\n        """"""\n            P7_0 -------------------------- P7_2 -------->\n\n            P6_0 ---------- P6_1 ---------- P6_2 -------->\n\n            P5_0 ---------- P5_1 ---------- P5_2 -------->\n\n            P4_0 ---------- P4_1 ---------- P4_2 -------->\n\n            P3_0 -------------------------- P3_2 -------->\n        """"""\n\n        # P3_0, P4_0, P5_0, P6_0 and P7_0\n        p3_in, p4_in, p5_in, p6_in, p7_in = inputs\n        # P7_0 to P7_2\n        # Weights for P6_0 and P7_0 to P6_1\n        p6_w1 = self.p6_w1_relu(self.p6_w1)\n        weight = p6_w1 / (torch.sum(p6_w1, dim=0) + self.epsilon)\n        # Connections for P6_0 and P7_0 to P6_1 respectively\n        p6_up = self.conv6_up(weight[0] * p6_in + weight[1] * self.p6_upsample(p7_in))\n        # Weights for P5_0 and P6_0 to P5_1\n        p5_w1 = self.p5_w1_relu(self.p5_w1)\n        weight = p5_w1 / (torch.sum(p5_w1, dim=0) + self.epsilon)\n        # Connections for P5_0 and P6_0 to P5_1 respectively\n        p5_up = self.conv5_up(weight[0] * p5_in + weight[1] * self.p5_upsample(p6_up))\n        # Weights for P4_0 and P5_0 to P4_1\n        p4_w1 = self.p4_w1_relu(self.p4_w1)\n        weight = p4_w1 / (torch.sum(p4_w1, dim=0) + self.epsilon)\n        # Connections for P4_0 and P5_0 to P4_1 respectively\n        p4_up = self.conv4_up(weight[0] * p4_in + weight[1] * self.p4_upsample(p5_up))\n\n        # Weights for P3_0 and P4_1 to P3_2\n        p3_w1 = self.p3_w1_relu(self.p3_w1)\n        weight = p3_w1 / (torch.sum(p3_w1, dim=0) + self.epsilon)\n        # Connections for P3_0 and P4_1 to P3_2 respectively\n        p3_out = self.conv3_up(weight[0] * p3_in + weight[1] * self.p3_upsample(p4_up))\n\n        # Weights for P4_0, P4_1 and P3_2 to P4_2\n        p4_w2 = self.p4_w2_relu(self.p4_w2)\n        weight = p4_w2 / (torch.sum(p4_w2, dim=0) + self.epsilon)\n        # Connections for P4_0, P4_1 and P3_2 to P4_2 respectively\n        p4_out = self.conv4_down(\n            weight[0] * p4_in + weight[1] * p4_up + weight[2] * self.p4_downsample(p3_out))\n        # Weights for P5_0, P5_1 and P4_2 to P5_2\n        p5_w2 = self.p5_w2_relu(self.p5_w2)\n        weight = p5_w2 / (torch.sum(p5_w2, dim=0) + self.epsilon)\n        # Connections for P5_0, P5_1 and P4_2 to P5_2 respectively\n        p5_out = self.conv5_down(\n            weight[0] * p5_in + weight[1] * p5_up + weight[2] * self.p5_downsample(p4_out))\n        # Weights for P6_0, P6_1 and P5_2 to P6_2\n        p6_w2 = self.p6_w2_relu(self.p6_w2)\n        weight = p6_w2 / (torch.sum(p6_w2, dim=0) + self.epsilon)\n        # Connections for P6_0, P6_1 and P5_2 to P6_2 respectively\n        p6_out = self.conv6_down(\n            weight[0] * p6_in + weight[1] * p6_up + weight[2] * self.p6_downsample(p5_out))\n        # Weights for P7_0 and P6_2 to P7_2\n        p7_w2 = self.p7_w2_relu(self.p7_w2)\n        weight = p7_w2 / (torch.sum(p7_w2, dim=0) + self.epsilon)\n        # Connections for P7_0 and P6_2 to P7_2\n        p7_out = self.conv7_down(weight[0] * p7_in + weight[1] * self.p7_downsample(p6_out))\n\n        return p3_out, p4_out, p5_out, p6_out, p7_out\n\n\nclass Regressor(nn.Module):\n    def __init__(self, in_channels, num_anchors, num_layers):\n        super(Regressor, self).__init__()\n        layers = []\n        for _ in range(num_layers):\n            layers.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1))\n            layers.append(nn.ReLU(True))\n        self.layers = nn.Sequential(*layers)\n        self.header = nn.Conv2d(in_channels, num_anchors * 4, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, inputs):\n        inputs = self.layers(inputs)\n        inputs = self.header(inputs)\n        output = inputs.permute(0, 2, 3, 1)\n        return output.contiguous().view(output.shape[0], -1, 4)\n\n\nclass Classifier(nn.Module):\n    def __init__(self, in_channels, num_anchors, num_classes, num_layers):\n        super(Classifier, self).__init__()\n        self.num_anchors = num_anchors\n        self.num_classes = num_classes\n        layers = []\n        for _ in range(num_layers):\n            layers.append(nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1))\n            layers.append(nn.ReLU(True))\n        self.layers = nn.Sequential(*layers)\n        self.header = nn.Conv2d(in_channels, num_anchors * num_classes, kernel_size=3, stride=1, padding=1)\n        self.act = nn.Sigmoid()\n\n    def forward(self, inputs):\n        inputs = self.layers(inputs)\n        inputs = self.header(inputs)\n        inputs = self.act(inputs)\n        inputs = inputs.permute(0, 2, 3, 1)\n        output = inputs.contiguous().view(inputs.shape[0], inputs.shape[1], inputs.shape[2], self.num_anchors,\n                                          self.num_classes)\n        return output.contiguous().view(output.shape[0], -1, self.num_classes)\n\n\nclass EfficientNet(nn.Module):\n    def __init__(self, ):\n        super(EfficientNet, self).__init__()\n        model = EffNet.from_pretrained(\'efficientnet-b0\')\n        del model._conv_head\n        del model._bn1\n        del model._avg_pooling\n        del model._dropout\n        del model._fc\n        self.model = model\n\n    def forward(self, x):\n        x = self.model._swish(self.model._bn0(self.model._conv_stem(x)))\n        feature_maps = []\n        for idx, block in enumerate(self.model._blocks):\n            drop_connect_rate = self.model._global_params.drop_connect_rate\n            if drop_connect_rate:\n                drop_connect_rate *= float(idx) / len(self.model._blocks)\n            x = block(x, drop_connect_rate=drop_connect_rate)\n            if block._depthwise_conv.stride == [2, 2]:\n                feature_maps.append(x)\n\n        return feature_maps[1:]\n\n\nclass EfficientDet(nn.Module):\n    def __init__(self, num_anchors=9, num_classes=20, compound_coef=0):\n        super(EfficientDet, self).__init__()\n        self.compound_coef = compound_coef\n\n        self.num_channels = [64, 88, 112, 160, 224, 288, 384, 384][self.compound_coef]\n\n        self.conv3 = nn.Conv2d(40, self.num_channels, kernel_size=1, stride=1, padding=0)\n        self.conv4 = nn.Conv2d(80, self.num_channels, kernel_size=1, stride=1, padding=0)\n        self.conv5 = nn.Conv2d(192, self.num_channels, kernel_size=1, stride=1, padding=0)\n        self.conv6 = nn.Conv2d(192, self.num_channels, kernel_size=3, stride=2, padding=1)\n        self.conv7 = nn.Sequential(nn.ReLU(),\n                                   nn.Conv2d(self.num_channels, self.num_channels, kernel_size=3, stride=2, padding=1))\n\n        self.bifpn = nn.Sequential(*[BiFPN(self.num_channels) for _ in range(min(2 + self.compound_coef, 8))])\n\n        self.num_classes = num_classes\n        self.regressor = Regressor(in_channels=self.num_channels, num_anchors=num_anchors,\n                                   num_layers=3 + self.compound_coef // 3)\n        self.classifier = Classifier(in_channels=self.num_channels, num_anchors=num_anchors, num_classes=num_classes,\n                                     num_layers=3 + self.compound_coef // 3)\n\n        self.anchors = Anchors()\n        self.regressBoxes = BBoxTransform()\n        self.clipBoxes = ClipBoxes()\n        self.focalLoss = FocalLoss()\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n        prior = 0.01\n\n        self.classifier.header.weight.data.fill_(0)\n        self.classifier.header.bias.data.fill_(-math.log((1.0 - prior) / prior))\n\n        self.regressor.header.weight.data.fill_(0)\n        self.regressor.header.bias.data.fill_(0)\n\n        self.backbone_net = EfficientNet()\n\n    def freeze_bn(self):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.eval()\n\n    def forward(self, inputs):\n        if len(inputs) == 2:\n            is_training = True\n            img_batch, annotations = inputs\n        else:\n            is_training = False\n            img_batch = inputs\n\n        c3, c4, c5 = self.backbone_net(img_batch)\n        p3 = self.conv3(c3)\n        p4 = self.conv4(c4)\n        p5 = self.conv5(c5)\n        p6 = self.conv6(c5)\n        p7 = self.conv7(p6)\n\n        features = [p3, p4, p5, p6, p7]\n        features = self.bifpn(features)\n\n        regression = torch.cat([self.regressor(feature) for feature in features], dim=1)\n        classification = torch.cat([self.classifier(feature) for feature in features], dim=1)\n        anchors = self.anchors(img_batch)\n\n        if is_training:\n            return self.focalLoss(classification, regression, anchors, annotations)\n        else:\n            transformed_anchors = self.regressBoxes(anchors, regression)\n            transformed_anchors = self.clipBoxes(transformed_anchors, img_batch)\n\n            scores = torch.max(classification, dim=2, keepdim=True)[0]\n\n            scores_over_thresh = (scores > 0.05)[0, :, 0]\n\n            if scores_over_thresh.sum() == 0:\n                return [torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)]\n\n            classification = classification[:, scores_over_thresh, :]\n            transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n            scores = scores[:, scores_over_thresh, :]\n\n            anchors_nms_idx = nms(torch.cat([transformed_anchors, scores], dim=2)[0, :, :], 0.5)\n\n            nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(dim=1)\n\n            return [nms_scores, nms_class, transformed_anchors[0, anchors_nms_idx, :]]\n\n\nif __name__ == \'__main__\':\n    from tensorboardX import SummaryWriter\n    def count_parameters(model):\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    model = EfficientDet(num_classes=80)\n    print (count_parameters(model))\n'"
src/utils.py,13,"b'import torch\nimport torch.nn as nn\nimport numpy as np\n\n\nclass BBoxTransform(nn.Module):\n\n    def __init__(self, mean=None, std=None):\n        super(BBoxTransform, self).__init__()\n        if mean is None:\n            self.mean = torch.from_numpy(np.array([0, 0, 0, 0]).astype(np.float32))\n        else:\n            self.mean = mean\n        if std is None:\n            self.std = torch.from_numpy(np.array([0.1, 0.1, 0.2, 0.2]).astype(np.float32))\n        else:\n            self.std = std\n        if torch.cuda.is_available():\n            self.mean = self.mean.cuda()\n            self.std = self.std.cuda()\n\n    def forward(self, boxes, deltas):\n\n        widths = boxes[:, :, 2] - boxes[:, :, 0]\n        heights = boxes[:, :, 3] - boxes[:, :, 1]\n        ctr_x = boxes[:, :, 0] + 0.5 * widths\n        ctr_y = boxes[:, :, 1] + 0.5 * heights\n\n        dx = deltas[:, :, 0] * self.std[0] + self.mean[0]\n        dy = deltas[:, :, 1] * self.std[1] + self.mean[1]\n        dw = deltas[:, :, 2] * self.std[2] + self.mean[2]\n        dh = deltas[:, :, 3] * self.std[3] + self.mean[3]\n\n        pred_ctr_x = ctr_x + dx * widths\n        pred_ctr_y = ctr_y + dy * heights\n        pred_w = torch.exp(dw) * widths\n        pred_h = torch.exp(dh) * heights\n\n        pred_boxes_x1 = pred_ctr_x - 0.5 * pred_w\n        pred_boxes_y1 = pred_ctr_y - 0.5 * pred_h\n        pred_boxes_x2 = pred_ctr_x + 0.5 * pred_w\n        pred_boxes_y2 = pred_ctr_y + 0.5 * pred_h\n\n        pred_boxes = torch.stack([pred_boxes_x1, pred_boxes_y1, pred_boxes_x2, pred_boxes_y2], dim=2)\n\n        return pred_boxes\n\n\nclass ClipBoxes(nn.Module):\n\n    def __init__(self):\n        super(ClipBoxes, self).__init__()\n\n    def forward(self, boxes, img):\n        batch_size, num_channels, height, width = img.shape\n\n        boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)\n        boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)\n\n        boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width)\n        boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height)\n\n        return boxes\n\n\nclass Anchors(nn.Module):\n    def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None):\n        super(Anchors, self).__init__()\n\n        if pyramid_levels is None:\n            self.pyramid_levels = [3, 4, 5, 6, 7]\n        if strides is None:\n            self.strides = [2 ** x for x in self.pyramid_levels]\n        if sizes is None:\n            self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]\n        if ratios is None:\n            self.ratios = np.array([0.5, 1, 2])\n        if scales is None:\n            self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n\n    def forward(self, image):\n\n        image_shape = image.shape[2:]\n        image_shape = np.array(image_shape)\n        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n\n        all_anchors = np.zeros((0, 4)).astype(np.float32)\n\n        for idx, p in enumerate(self.pyramid_levels):\n            anchors = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)\n            shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors)\n            all_anchors = np.append(all_anchors, shifted_anchors, axis=0)\n\n        all_anchors = np.expand_dims(all_anchors, axis=0)\n\n        anchors = torch.from_numpy(all_anchors.astype(np.float32))\n        if torch.cuda.is_available():\n            anchors = anchors.cuda()\n        return anchors\n\n\ndef generate_anchors(base_size=16, ratios=None, scales=None):\n    if ratios is None:\n        ratios = np.array([0.5, 1, 2])\n\n    if scales is None:\n        scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n\n    num_anchors = len(ratios) * len(scales)\n    anchors = np.zeros((num_anchors, 4))\n    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n    areas = anchors[:, 2] * anchors[:, 3]\n    anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n\n    return anchors\n\n\ndef compute_shape(image_shape, pyramid_levels):\n    image_shape = np.array(image_shape[:2])\n    image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels]\n    return image_shapes\n\n\ndef shift(shape, stride, anchors):\n    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n    shifts = np.vstack((\n        shift_x.ravel(), shift_y.ravel(),\n        shift_x.ravel(), shift_y.ravel()\n    )).transpose()\n\n    A = anchors.shape[0]\n    K = shifts.shape[0]\n    all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n    all_anchors = all_anchors.reshape((K * A, 4))\n\n    return all_anchors\n'"
