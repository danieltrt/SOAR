file_path,api_count,code
src/config.py,0,"b""# coding: utf-8\n\n\ntraining_root = './CT'\ntesting_root = './CT'\nresnext_101_32_path = 'resnext_101_32x4d.pth'\n"""
src/main.py,5,"b'import os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torch.optim import Adam\n\nimport dill\nimport argparse\nfrom data import medicalDataLoader\nfrom models.my_stacked_danet import DAF_stack\nfrom common.progressBar import printProgressBar\nfrom common.utils import (evaluate3D,\n                          reconstruct3D,\n                          saveImages_for3D,\n                          getOneHotSegmentation,\n                          predToSegmentation,\n                          getTargetSegmentation,\n                          computeDiceOneHot,\n                          DicesToDice,\n                          inference,\n                          to_var\n                          )\n    \ndef weights_init(m):\n    if type(m) == nn.Conv2d or type(m) == nn.ConvTranspose2d:\n        nn.init.xavier_normal(m.weight.data)\n    elif type(m) == nn.BatchNorm2d:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\ndef runTraining(args):\n    print(\'-\' * 40)\n    print(\'~~~~~~~~  Starting the training... ~~~~~~\')\n    print(\'-\' * 40)\n\n    batch_size = args.batch_size\n    batch_size_val = 1\n    batch_size_val_save = 1\n    lr = args.lr\n\n    epoch = args.epochs\n    root_dir = args.root\n    model_dir = \'model\'\n    \n    print(\' Dataset: {} \'.format(root_dir))\n\n    transform = transforms.Compose([\n        transforms.ToTensor()\n    ])\n\n    mask_transform = transforms.Compose([\n        transforms.ToTensor()\n    ])\n\n    train_set = medicalDataLoader.MedicalImageDataset(\'train\',\n                                                      root_dir,\n                                                      transform=transform,\n                                                      mask_transform=mask_transform,\n                                                      augment=True,\n                                                      equalize=False)\n\n    train_loader = DataLoader(train_set,\n                              batch_size=batch_size,\n                              num_workers=args.num_workers,\n                              shuffle=True)\n\n    val_set = medicalDataLoader.MedicalImageDataset(\'val\',\n                                                    root_dir,\n                                                    transform=transform,\n                                                    mask_transform=mask_transform,\n                                                    equalize=False)\n\n    val_loader = DataLoader(val_set,\n                            batch_size=batch_size_val,\n                            num_workers=args.num_workers,\n                            shuffle=False)\n                                                   \n    val_loader_save_images = DataLoader(val_set,\n                                        batch_size=batch_size_val_save,\n                                        num_workers= args.num_workers,\n                                        shuffle=False)\n\n                                                                    \n    # Initialize\n    print(""~~~~~~~~~~~ Creating the DAF Stacked model ~~~~~~~~~~"")\n    net = DAF_stack()\n    print("" Model Name: {}"".format(args.modelName))\n    print("" Model ot create: DAF_Stacked"")\n\n    net.apply(weights_init)\n\n    softMax = nn.Softmax()\n    CE_loss = nn.CrossEntropyLoss()\n    Dice_loss = computeDiceOneHot()\n    mseLoss = nn.MSELoss()\n\n    if torch.cuda.is_available():\n        net.cuda()\n        softMax.cuda()\n        CE_loss.cuda()\n        Dice_loss.cuda()\n\n    optimizer = Adam(net.parameters(), lr=lr, betas=(0.9, 0.99), amsgrad=False)\n\n    BestDice, BestEpoch = 0, 0\n    BestDice3D = [0,0,0,0]\n\n    d1Val = []\n    d2Val = []\n    d3Val = []\n    d4Val = []\n\n    d1Val_3D = []\n    d2Val_3D = []\n    d3Val_3D = []\n    d4Val_3D = []\n\n    d1Val_3D_std = []\n    d2Val_3D_std = []\n    d3Val_3D_std = []\n    d4Val_3D_std = []\n\n    Losses = []\n\n    print(""~~~~~~~~~~~ Starting the training ~~~~~~~~~~"")\n    for i in range(epoch):\n        net.train()\n        lossVal = []\n\n        totalImages = len(train_loader)\n       \n        for j, data in enumerate(train_loader):\n            image, labels, img_names = data\n\n            # prevent batchnorm error for batch of size 1\n            if image.size(0) != batch_size:\n                continue\n\n            optimizer.zero_grad()\n            MRI = to_var(image)\n            Segmentation = to_var(labels)\n\n            ################### Train ###################\n            net.zero_grad()\n\n            # Network outputs\n            semVector_1_1, \\\n            semVector_2_1, \\\n            semVector_1_2, \\\n            semVector_2_2, \\\n            semVector_1_3, \\\n            semVector_2_3, \\\n            semVector_1_4, \\\n            semVector_2_4, \\\n            inp_enc0, \\\n            inp_enc1, \\\n            inp_enc2, \\\n            inp_enc3, \\\n            inp_enc4, \\\n            inp_enc5, \\\n            inp_enc6, \\\n            inp_enc7, \\\n            out_enc0, \\\n            out_enc1, \\\n            out_enc2, \\\n            out_enc3, \\\n            out_enc4, \\\n            out_enc5, \\\n            out_enc6, \\\n            out_enc7, \\\n            outputs0, \\\n            outputs1, \\\n            outputs2, \\\n            outputs3, \\\n            outputs0_2, \\\n            outputs1_2, \\\n            outputs2_2, \\\n            outputs3_2 = net(MRI)\n\n            segmentation_prediction = (\n                outputs0 + outputs1 + outputs2 + outputs3 +\\\n                outputs0_2 + outputs1_2 + outputs2_2 + outputs3_2\n                ) / 8\n            predClass_y = softMax(segmentation_prediction)\n\n            Segmentation_planes = getOneHotSegmentation(Segmentation)\n\n            segmentation_prediction_ones = predToSegmentation(predClass_y)\n\n            # It needs the logits, not the softmax\n            Segmentation_class = getTargetSegmentation(Segmentation)\n\n            # Cross-entropy loss\n            loss0 = CE_loss(outputs0, Segmentation_class)\n            loss1 = CE_loss(outputs1, Segmentation_class)\n            loss2 = CE_loss(outputs2, Segmentation_class)\n            loss3 = CE_loss(outputs3, Segmentation_class)\n            loss0_2 = CE_loss(outputs0_2, Segmentation_class)\n            loss1_2 = CE_loss(outputs1_2, Segmentation_class)\n            loss2_2 = CE_loss(outputs2_2, Segmentation_class)\n            loss3_2 = CE_loss(outputs3_2, Segmentation_class)\n\n            lossSemantic1 = mseLoss(semVector_1_1, semVector_2_1)\n            lossSemantic2 = mseLoss(semVector_1_2, semVector_2_2)\n            lossSemantic3 = mseLoss(semVector_1_3, semVector_2_3)\n            lossSemantic4 = mseLoss(semVector_1_4, semVector_2_4)\n\n            lossRec0 = mseLoss(inp_enc0, out_enc0)\n            lossRec1 = mseLoss(inp_enc1, out_enc1)\n            lossRec2 = mseLoss(inp_enc2, out_enc2)\n            lossRec3 = mseLoss(inp_enc3, out_enc3)\n            lossRec4 = mseLoss(inp_enc4, out_enc4)\n            lossRec5 = mseLoss(inp_enc5, out_enc5)\n            lossRec6 = mseLoss(inp_enc6, out_enc6)\n            lossRec7 = mseLoss(inp_enc7, out_enc7)\n\n            lossG = (loss0 + loss1 + loss2 + loss3 + loss0_2 + loss1_2 + loss2_2 + loss3_2)\\\n                + 0.25 * (lossSemantic1 + lossSemantic2 + lossSemantic3 + lossSemantic4) \\\n                + 0.1 * (lossRec0 + lossRec1 + lossRec2 + lossRec3 + lossRec4 + lossRec5 + lossRec6 + lossRec7)  # CE_lossG\n\n            # Compute the DSC\n            DicesN, DicesB, DicesW, DicesT, DicesZ = Dice_loss(segmentation_prediction_ones, Segmentation_planes)\n\n            DiceB = DicesToDice(DicesB)\n            DiceW = DicesToDice(DicesW)\n            DiceT = DicesToDice(DicesT)\n            DiceZ = DicesToDice(DicesZ)\n\n            Dice_score = (DiceB + DiceW + DiceT+ DiceZ) / 4\n\n            lossG.backward()\n            optimizer.step()\n            \n            lossVal.append(lossG.cpu().data.numpy())\n\n            printProgressBar(j + 1, totalImages,\n                             prefix=""[Training] Epoch: {} "".format(i),\n                             length=15,\n                             suffix="" Mean Dice: {:.4f}, Dice1: {:.4f} , Dice2: {:.4f}, , Dice3: {:.4f}, Dice4: {:.4f} "".format(\n                                 Dice_score.cpu().data.numpy(),\n                                 DiceB.data.cpu().data.numpy(),\n                                 DiceW.data.cpu().data.numpy(),\n                                 DiceT.data.cpu().data.numpy(),\n                                 DiceZ.data.cpu().data.numpy(),))\n\n      \n        printProgressBar(totalImages, totalImages,\n                             done=""[Training] Epoch: {}, LossG: {:.4f}"".format(i,np.mean(lossVal)))\n       \n        # Save statistics\n        modelName = args.modelName\n        directory = args.save_dir + modelName\n        \n        Losses.append(np.mean(lossVal))\n        \n        d1,d2,d3,d4 = inference(net, val_loader)\n        \n        d1Val.append(d1)\n        d2Val.append(d2)\n        d3Val.append(d3)\n        d4Val.append(d4)\n\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        np.save(os.path.join(directory, \'Losses.npy\'), Losses)\n        np.save(os.path.join(directory, \'d1Val.npy\'), d1Val)\n        np.save(os.path.join(directory, \'d2Val.npy\'), d2Val)\n        np.save(os.path.join(directory, \'d3Val.npy\'), d3Val)\n\n        currentDice = (d1+d2+d3+d4)/4\n\n        print(""[val] DSC: (1): {:.4f} (2): {:.4f}  (3): {:.4f} (4): {:.4f}"".format(d1,d2,d3,d4)) # MRI\n\n        currentDice = currentDice.data.numpy()\n\n        # Evaluate on 3D\n        saveImages_for3D(net, val_loader_save_images, batch_size_val_save, 1000, modelName, False, False)\n        reconstruct3D(modelName, 1000, isBest=False)\n        DSC_3D = evaluate3D(modelName)\n\n        mean_DSC3D = np.mean(DSC_3D, 0)\n        std_DSC3D = np.std(DSC_3D,0)\n\n        d1Val_3D.append(mean_DSC3D[0])\n        d2Val_3D.append(mean_DSC3D[1])\n        d3Val_3D.append(mean_DSC3D[2])\n        d4Val_3D.append(mean_DSC3D[3])\n        d1Val_3D_std.append(std_DSC3D[0])\n        d2Val_3D_std.append(std_DSC3D[1])\n        d3Val_3D_std.append(std_DSC3D[2])\n        d4Val_3D_std.append(std_DSC3D[3])\n\n        np.save(os.path.join(directory, \'d0Val_3D.npy\'), d1Val_3D)\n        np.save(os.path.join(directory, \'d1Val_3D.npy\'), d2Val_3D)\n        np.save(os.path.join(directory, \'d2Val_3D.npy\'), d3Val_3D)\n        np.save(os.path.join(directory, \'d3Val_3D.npy\'), d4Val_3D)\n        \n        np.save(os.path.join(directory, \'d0Val_3D_std.npy\'), d1Val_3D_std)\n        np.save(os.path.join(directory, \'d1Val_3D_std.npy\'), d2Val_3D_std)\n        np.save(os.path.join(directory, \'d2Val_3D_std.npy\'), d3Val_3D_std)\n        np.save(os.path.join(directory, \'d3Val_3D_std.npy\'), d4Val_3D_std)\n\n\n        if currentDice > BestDice:\n            BestDice = currentDice\n\n            BestEpoch = i\n            \n            if currentDice > 0.40:\n\n                if np.mean(mean_DSC3D)>np.mean(BestDice3D):\n                    BestDice3D = mean_DSC3D\n\n                print(""###    In 3D -----> MEAN: {}, Dice(1): {:.4f} Dice(2): {:.4f} Dice(3): {:.4f} Dice(4): {:.4f}   ###"".format(np.mean(mean_DSC3D),mean_DSC3D[0], mean_DSC3D[1], mean_DSC3D[2], mean_DSC3D[3]))\n\n                print(""~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Saving best model..... ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"")\n                if not os.path.exists(model_dir):\n                    os.makedirs(model_dir)\n                torch.save(net.state_dict(), os.path.join(model_dir, ""Best_"" + modelName + "".pth""),pickle_module=dill)\n                reconstruct3D(modelName, 1000, isBest=True)\n\n        print(""###                                                       ###"")\n        print(""###    Best Dice: {:.4f} at epoch {} with Dice(1): {:.4f} Dice(2): {:.4f} Dice(3): {:.4f} Dice(4): {:.4f}   ###"".format(BestDice, BestEpoch, d1,d2,d3,d4))\n        print(""###    Best Dice in 3D: {:.4f} with Dice(1): {:.4f} Dice(2): {:.4f} Dice(3): {:.4f} Dice(4): {:.4f} ###"".format(np.mean(BestDice3D),BestDice3D[0], BestDice3D[1], BestDice3D[2], BestDice3D[3] ))\n        print(""###                                                       ###"")\n\n        if i % (BestEpoch + 50) == 0:\n            for param_group in optimizer.param_groups:\n                lr = lr*0.5\n                param_group[\'lr\'] = lr\n                print(\' ----------  New learning Rate: {}\'.format(lr))\n\n\nif __name__ == \'__main__\':\n    parser=argparse.ArgumentParser()\n    parser.add_argument(\'--modelName\',default = \'MS-Dual-Guided\',type=str)\n    parser.add_argument(\'--root\', default = \'../DataSet/\', type = str)\n    parser.add_argument(\'--num_workers\', default = 4, type = int)\n    parser.add_argument(\'--save_dir\', default = \'Results/Statistics/\', type =str)\n    parser.add_argument(\'--batch_size\',default = 8,type = int)\n    parser.add_argument(\'--epochs\',default = 500,type = int)\n    parser.add_argument(\'--lr\',default = 0.001,type = float)\n    args=parser.parse_args()\n    runTraining(args)\n'"
src/common/normalizeImages.py,0,"b""import sys\nimport pdb\nfrom os.path import isfile, join\nimport os\nimport numpy as np\nimport nibabel as nib\nimport scipy.io as sio\n\nfrom scipy import misc\nimport re\n\nfrom scipy import ndimage\nfrom scipy import misc\n\ndef getImageImageList(imagesFolder):\n    if os.path.exists(imagesFolder):\n       imageNames = [f for f in os.listdir(imagesFolder) if isfile(join(imagesFolder, f))]\n\n    imageNames.sort()\n\n    return imageNames\n    \ndef normalizeImages(argv):\n    \n    imagesFolder = argv[0]\n\n    imageNames = getImageImageList(imagesFolder)\n    numImages = len(imageNames)\n   \n    for s_i in range(numImages):\n        \n        image = ndimage.imread(imagesFolder+imageNames[s_i])\n        \n        if image.shape[0] > 256:\n            centerX = int(image.shape[0]/2)\n            centerY = int(image.shape[1]/2)\n            newImage = image[centerX-128:centerX+128,centerY-128:centerY+128]\n            image = newImage\n            \n        normalized = (image-np.min(image))/(np.max(image)-np.min(image))\n        pdb.set_trace()\n        normalized = normalized*255\n        \n        image = normalized.astype('uint8')\n        \n        print('Min value: {}  and Max value: {}....Total values: {}'.format(np.min(image),np.max(image),len(np.unique(image))))\n        \n        gt = np.zeros((image.shape))\n        \n        if s_i < 10:\n            misc.imsave('./Demo_Corstem/val/Img/Img_0' + str(s_i) + '.png',image)\n            misc.imsave('./Demo_Corstem/val/GT/Img_0' + str(s_i) + '.png',gt)\n        else:\n            misc.imsave('./Demo_Corstem/val/Img/Img_' + str(s_i) + '.png',image)\n            misc.imsave('./Demo_Corstem/val/GT/Img_0' + str(s_i) + '.png',gt)\n        \nif __name__ == '__main__':\n    normalizeImages(sys.argv[1:])\n"""
src/common/progressBar.py,0,"b'# -*- coding: utf-8 -*-\nimport sys\nimport os\nimport pdb\n\ndef printProgressBar(iteration, total, prefix=\'\', suffix=\'\', decimals=1, length=100,\n                     fill=\'=\', empty=\' \', tip=\'>\', begin=\'[\', end=\']\', done=""[DONE]"", clear=True):\n    """"""\n    Print iterations progress.\n    Call in a loop to create terminal progress bar\n    @params:\n        iteration   - Required : current iteration                          [int]\n        total       - Required : total iterations                           [int]\n        prefix      - Optional : prefix string                              [str]\n        suffix      - Optional : suffix string                              [str]\n        decimals    - Optional : positive number of decimals in percent     [int]\n        length      - Optional : character length of bar                    [int]\n        fill        - Optional : bar fill character                         [str] (ex: \'\xc3\xa2\xe2\x80\x93 \', \'\xc3\xa2\xe2\x80\x93\xcb\x86\', \'#\', \'=\')\n        empty       - Optional : not filled bar character                   [str] (ex: \'-\', \' \', \'\xc3\xa2\xe2\x82\xac\xc2\xa2\')\n        tip         - Optional : character at the end of the fill bar       [str] (ex: \'>\', \'\')\n        begin       - Optional : starting bar character                     [str] (ex: \'|\', \'\xc3\xa2\xe2\x80\x93\xe2\x80\xa2\', \'[\')\n        end         - Optional : ending bar character                       [str] (ex: \'|\', \'\xc3\xa2\xe2\x80\x93\xc2\x8f\', \']\')\n        done        - Optional : display message when 100% is reached       [str] (ex: ""[DONE]"")\n        clear       - Optional : display completion message or leave as is  [str]\n    """"""\n    #pdb.set_trace()\n    percent = (""{0:."" + str(decimals) + ""f}"").format(100 * (iteration / float(total)))\n    filledLength = int(length * iteration // total)\n    bar = fill * filledLength\n    if iteration != total:\n         bar = bar + tip\n    bar = bar + empty * (length - filledLength - len(tip))\n    display = \'\\r{prefix}{begin}{bar}{end} {percent}%{suffix}\' \\\n              .format(prefix=prefix, begin=begin, bar=bar, end=end, percent=percent, suffix=suffix)\n    print(display, end=\'\'),   # comma after print() required for python 2\n    if iteration == total:      # print with newline on complete\n        if clear:               # display given complete message with spaces to \'erase\' previous progress bar\n            finish = \'\\r{prefix}{done}\'.format(prefix=prefix, done=done)\n            if hasattr(str, \'decode\'):   # handle python 2 non-unicode strings for proper length measure\n                finish = finish.decode(\'utf-8\')\n                display = display.decode(\'utf-8\')\n            clear = \' \' * max(len(display) - len(finish), 0)\n            print(finish + clear)\n        else:\n            print(\'\')\n\n\ndef verbose(verboseLevel, requiredLevel, printFunc=print, *printArgs, **kwPrintArgs):\n    """"""\n    Calls `printFunc` passing it `printArgs` and `kwPrintArgs`\n    only if `verboseLevel` meets the `requiredLevel` of verbosity.\n\n    Following forms are supported:\n\n        > verbose(1, 0, ""message"")\n\n            >> message\n\n        > verbose(1, 0, ""message1"", ""message2"")\n\n            >> message1 message2\n\n        > verbose(1, 2, ""message"")\n\n            >>          <nothing since verbosity level not high enough>\n\n        > verbose(1, 1, lambda x: print(\'MSG: \' + x), \'message\')\n\n            >> MSG: message\n\n        > def myprint(x, y=""msg_y"", z=True): print(\'MSG_Y: \' + y) if z else print(\'MSG_X: \' + x)\n        > verbose(1, 1, myprint, ""msg_x"", ""msg_y"")\n\n            >> MSG_Y: msg_y\n\n        > verbose(1, 1, myprint, ""msg_x"", ""msg_Y!"", z=True)\n\n            >> MSG_Y: msg_Y!\n\n        > verbose(1, 1, myprint, ""msg_x"", z=False)\n\n            >> MSG_X: msg_x\n\n        > verbose(1, 1, myprint, ""msg_x"", z=True)\n\n            >> MSG_Y: msg_y\n    """"""\n    if verboseLevel >= requiredLevel:\n        # handle cases when no additional arguments are provided (default print nothing)\n        printArgs = printArgs if printArgs is not None else tuple([\'\'])\n        # handle cases when verbose is called directly with the object (ex: str) to print\n        if not hasattr(printFunc, \'__call__\'):\n            printArgs = tuple([printFunc]) + printArgs\n            printFunc = print\n        printFunc(*printArgs, **kwPrintArgs)\n\n\ndef print_flush(txt=\'\'):\n    print(txt)\n    sys.stdout.flush()\n\n\nif os.name == \'nt\':\n    import msvcrt\n    import ctypes\n\n    class _CursorInfo(ctypes.Structure):\n        _fields_ = [(""size"", ctypes.c_int),\n                    (""visible"", ctypes.c_byte)]\n\n\ndef hide_cursor():\n    if os.name == \'nt\':\n        ci = _CursorInfo()\n        handle = ctypes.windll.kernel32.GetStdHandle(-11)\n        ctypes.windll.kernel32.GetConsoleCursorInfo(handle, ctypes.byref(ci))\n        ci.visible = False\n        ctypes.windll.kernel32.SetConsoleCursorInfo(handle, ctypes.byref(ci))\n    elif os.name == \'posix\':\n        sys.stdout.write(""\\033[?25l"")\n        sys.stdout.flush()\n\n\ndef show_cursor():\n    if os.name == \'nt\':\n        ci = _CursorInfo()\n        handle = ctypes.windll.kernel32.GetStdHandle(-11)\n        ctypes.windll.kernel32.GetConsoleCursorInfo(handle, ctypes.byref(ci))\n        ci.visible = True\n        ctypes.windll.kernel32.SetConsoleCursorInfo(handle, ctypes.byref(ci))\n    elif os.name == \'posix\':\n        sys.stdout.write(""\\033[?25h"")\n        sys.stdout.flush()\n'"
src/common/utils.py,17,"b'import os\nimport numpy as np\nimport scipy.io as sio\nimport pdb\nimport time\nfrom os.path import isfile, join\n\nimport nibabel as nib\nfrom PIL import Image\nfrom medpy.metric.binary import dc,hd\nimport skimage.transform as skiTransf\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision\n\nfrom .progressBar import printProgressBar\n\ndef load_nii(imageFileName, printFileNames):\n    if printFileNames == True:\n        print ("" ... Loading file: {}"".format(imageFileName))\n\n    img_proxy = nib.load(imageFileName)\n    imageData = img_proxy.get_data()\n\n    return (imageData, img_proxy)\n\ndef evaluate3D(modelName):\n\n    # Path where the ground truth (as nifti file) should be for comparisons\n    path_GT = \'./DataSet_Challenge/GT_Nifti/Val_1\'\n\n    # Path where the predictions are saved\n    path_Pred = \'Results/Images/\' + modelName + \'/Nifti\'\n\n    if not os.path.exists(\'Results/Images/\' + modelName + \'/Nifti/\'):\n        os.makedirs(\'Results/Images/\' + modelName + \'/Nifti/\',exist_ok=True)\n    GT_names = getImageImageList(path_GT)\n    Pred_names = getImageImageList(path_Pred)\n\n    GT_names.sort()\n    Pred_names.sort()\n\n    numClasses = 4\n    DSC = np.zeros((len(Pred_names), numClasses))\n\n    for s_i in range(len(Pred_names)):\n        path_Subj_GT = path_GT +\'/\'+GT_names[s_i]\n        path_Subj_pred = path_Pred +\'/\'+Pred_names[s_i]\n\n        [imageDataGT, img_proxy] = load_nii(path_Subj_GT, printFileNames=False)\n        [imageDataCNN, img_proxy] = load_nii(path_Subj_pred, printFileNames=False)\n\n        for c_i in range(numClasses):\n            label_GT = np.zeros(imageDataGT.shape, dtype=np.int8)\n            label_CNN = np.zeros(imageDataCNN.shape, dtype=np.int8)\n            idx_GT = np.where(imageDataGT == c_i+1)\n            label_GT[idx_GT] = 1\n            idx_CNN = np.where(imageDataCNN == c_i+1)\n            label_CNN[idx_CNN] = 1\n\n            DSC[s_i,c_i] = dc(label_GT,label_CNN)\n\n    return DSC\n\n\ndef getImageImageList(imagesFolder):\n    if os.path.exists(imagesFolder):\n       imageNames = [f for f in os.listdir(imagesFolder) if isfile(join(imagesFolder, f))]\n\n    imageNames.sort()\n\n    return imageNames\n\ndef to_var(x):\n    if torch.cuda.is_available():\n        x = x.cuda()\n    return Variable(x)\n\n\ndef reconstruct3D(modelName,epoch, isBest=False):\n\n    path = \'Results/Images/\' + modelName + \'/\' + str(epoch)\n    subjNames = os.listdir(path)\n\n\n    for s_i in range(len(subjNames)):\n         path_Subj = path +\'/\'+subjNames[s_i]\n         imgNames = getImageImageList(path_Subj)\n\n         numImages = len(imgNames)\n\n         xSize = 256\n         ySize = 256\n         vol_numpy = np.zeros((xSize, ySize, numImages))\n\n         for t_i in range(numImages-1):\n\n             imagePIL = Image.open(path_Subj + \'/\'+str(t_i+1)+\'.png\').convert(\'LA\')\n             imageNP = np.array(imagePIL)\n             vol_numpy[:, :, t_i] = imageNP[:,:,0]/63 # To have labels in the range [0,1,2]\n\n         xform = np.eye(4) * 2\n         imgNifti = nib.nifti1.Nifti1Image(vol_numpy, xform)\n         if not os.path.exists(\'Results/Images/\' + modelName + \'/Nifti/\'):\n             os.makedirs(\'Results/Images/\' + modelName + \'/Nifti/\',exist_ok=True)\n         niftiName = \'Results/Images/\' + modelName + \'/Nifti/\' + subjNames[s_i]\n         nib.save(imgNifti, niftiName)\n\n         if (isBest):\n             if not os.path.exists(\'Results/Images/\' + modelName + \'/Nifti_Best/\'):\n                 os.makedirs(\'Results/Images/\' + modelName + \'/Nifti_Best/\', exist_ok=True)\n             niftiName = \'Results/Images/\' + modelName + \'/Nifti_Best/\' + subjNames[s_i]\n             nib.save(imgNifti, niftiName)\n\n\n        \nclass computeDiceOneHot(nn.Module):\n    def __init__(self):\n        super(computeDiceOneHot, self).__init__()\n\n    def dice(self, input, target):\n        inter = (input * target).float().sum()\n        sum = input.sum() + target.sum()\n        if (sum == 0).all():\n            return (2 * inter + 1e-8) / (sum + 1e-8)\n\n        return 2 * (input * target).float().sum() / (input.sum() + target.sum())\n\n    def inter(self, input, target):\n        return (input * target).float().sum()\n\n    def sum(self, input, target):\n        return input.sum() + target.sum()\n\n    def forward(self, pred, GT):\n        # GT is 4x320x320 of 0 and 1\n        # pred is converted to 0 and 1\n        batchsize = GT.size(0)\n        DiceN = to_var(torch.zeros(batchsize, 2))\n        DiceB = to_var(torch.zeros(batchsize, 2))\n        DiceW = to_var(torch.zeros(batchsize, 2))\n        DiceT = to_var(torch.zeros(batchsize, 2))\n        DiceZ = to_var(torch.zeros(batchsize, 2))\n\n        for i in range(batchsize):\n            DiceN[i, 0] = self.inter(pred[i, 0], GT[i, 0])\n            DiceB[i, 0] = self.inter(pred[i, 1], GT[i, 1])\n            DiceW[i, 0] = self.inter(pred[i, 2], GT[i, 2])\n            DiceT[i, 0] = self.inter(pred[i, 3], GT[i, 3])\n            DiceZ[i, 0] = self.inter(pred[i, 4], GT[i, 4])\n\n            DiceN[i, 1] = self.sum(pred[i, 0], GT[i, 0])\n            DiceB[i, 1] = self.sum(pred[i, 1], GT[i, 1])\n            DiceW[i, 1] = self.sum(pred[i, 2], GT[i, 2])\n            DiceT[i, 1] = self.sum(pred[i, 3], GT[i, 3])\n            DiceZ[i, 1] = self.sum(pred[i, 4], GT[i, 4])\n\n        return DiceN, DiceB , DiceW, DiceT, DiceZ\n\n\ndef DicesToDice(Dices):\n    sums = Dices.sum(dim=0)\n    return (2 * sums[0] + 1e-8) / (sums[1] + 1e-8)\n\n    \ndef getSingleImage(pred):\n    # input is a 4-channels image corresponding to the predictions of the net\n    # output is a gray level image (1 channel) of the segmentation with ""discrete"" values\n    num_classes = 5\n    Val = to_var(torch.zeros(num_classes))\n\n    # Chaos MRI\n    Val[1] = 0.24705882\n    Val[2] = 0.49411765\n    Val[3] = 0.7411765\n    Val[4] = 0.9882353\n    \n    x = predToSegmentation(pred)\n   \n    out = x * Val.view(1, 5, 1, 1)\n\n    return out.sum(dim=1, keepdim=True)\n\n\ndef predToSegmentation(pred):\n    Max = pred.max(dim=1, keepdim=True)[0]\n    x = pred / Max\n    return (x == 1).float()\n\n\ndef getOneHotSegmentation(batch):\n    backgroundVal = 0\n\n    # Chaos MRI (These values are to set label values as 0,1,2,3 and 4)\n    label1 = 0.24705882\n    label2 = 0.49411765\n    label3 = 0.7411765\n    label4 = 0.9882353\n    \n    oneHotLabels = torch.cat((batch == backgroundVal, batch == label1, batch == label2, batch == label3, batch == label4),\n                             dim=1)\n    \n    return oneHotLabels.float()\n\n\ndef getTargetSegmentation(batch):\n    # input is 1-channel of values between 0 and 1\n    # values are as follows : 0, 0.3137255, 0.627451 and 0.94117647\n    # output is 1 channel of discrete values : 0, 1, 2 and 3\n    \n    denom = 0.24705882 # for Chaos MRI  Dataset this value\n\n    return (batch / denom).round().long().squeeze()\n\n\ndef saveImages_for3D(net, img_batch, batch_size, epoch, modelName, deepSupervision=False, isBest= False):\n    # print("" Saving images....."")\n    path = \'Results/Images/\' + modelName + \'/\' + str(epoch)\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\n    total = len(img_batch)\n    net.eval()\n    softMax = nn.Softmax()\n    for i, data in enumerate(img_batch):\n        image, labels, img_names = data\n\n        MRI = to_var(image)\n        Segmentation = to_var(labels)\n\n        segmentation_prediction = net(MRI)\n\n        pred_y = softMax(segmentation_prediction)\n        \n        segmentation = getSingleImage(pred_y)\n\n        out = torch.cat((MRI, segmentation, Segmentation))\n\n        str_1 = img_names[0].split(\'/Img/\')\n        str_subj = str_1[1].split(\'slice\')\n\n        path_Subj = path + \'/\' + str_subj[0]\n        if not os.path.exists(path_Subj):\n            os.makedirs(path_Subj)\n\n        str_subj = str_subj[1].split(\'_\')\n        torchvision.utils.save_image(segmentation.data, os.path.join(path_Subj, str_subj[1]))\n    printProgressBar(total, total, done=""Images saved !"")\n\n\ndef inference(net, img_batch):\n    total = len(img_batch)\n\n    Dice1 = torch.zeros(total, 2)\n    Dice2 = torch.zeros(total, 2)\n    Dice3 = torch.zeros(total, 2)\n    Dice4 = torch.zeros(total, 2)\n    \n    net.eval()\n    img_names_ALL = []\n\n    dice = computeDiceOneHot().cuda()\n    softMax = nn.Softmax().cuda()\n    for i, data in enumerate(img_batch):\n        \n        printProgressBar(i, total, prefix=""[Inference] Getting segmentations..."", length=30)\n        image, labels, img_names = data\n        img_names_ALL.append(img_names[0].split(\'/\')[-1].split(\'.\')[0])\n\n        MRI = to_var(image)\n        Segmentation = to_var(labels)\n\n        segmentation_prediction = net(MRI)\n\n        pred_y = softMax(segmentation_prediction)\n        Segmentation_planes = getOneHotSegmentation(Segmentation)\n\n        segmentation_prediction_ones = predToSegmentation(pred_y)\n        DicesN, Dices1, Dices2, Dices3, Dices4 = dice(segmentation_prediction_ones, Segmentation_planes)\n\n        Dice1[i] = Dices1.data\n        Dice2[i] = Dices2.data\n        Dice3[i] = Dices3.data\n        Dice4[i] = Dices4.data\n\n    printProgressBar(total, total, done=""[Inference] Segmentation Done !"")\n\n    ValDice1 = DicesToDice(Dice1)\n    ValDice2 = DicesToDice(Dice2)\n    ValDice3 = DicesToDice(Dice3)\n    ValDice4 = DicesToDice(Dice4)\n   \n    return [ValDice1,ValDice2,ValDice3,ValDice4]\n\n\n\nclass MaskToTensor(object):\n    def __call__(self, img):\n        return torch.from_numpy(np.array(img, dtype=np.int32)).float()\n\n'"
src/data/medicalDataLoader.py,1,"b'from __future__ import print_function, division\n\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom PIL import Image, ImageOps\nfrom random import random, randint\n\n# Ignore warnings\nimport warnings\nimport pdb\n\nwarnings.filterwarnings(""ignore"")\n\ndef make_dataset(root, mode):\n    assert mode in [\'train\', \'val\', \'test\']\n    items = []\n    if mode == \'train\':\n        train_img_path = os.path.join(root, \'train\', \'Img\')\n        train_mask_path = os.path.join(root, \'train\', \'GT\')\n\n        images = os.listdir(train_img_path)\n        labels = os.listdir(train_mask_path)\n\n        images.sort()\n        labels.sort()\n        \n        for it_im, it_gt in zip(images, labels):\n            item = (os.path.join(train_img_path, it_im), os.path.join(train_mask_path, it_gt))\n            items.append(item)\n    elif mode == \'val\':\n        train_img_path = os.path.join(root, \'val\', \'Img\')\n        train_mask_path = os.path.join(root, \'val\', \'GT\')\n\n        images = os.listdir(train_img_path)\n        labels = os.listdir(train_mask_path)\n\n        images.sort()\n        labels.sort()\n\n        for it_im, it_gt in zip(images, labels):\n            item = (os.path.join(train_img_path, it_im), os.path.join(train_mask_path, it_gt))\n            items.append(item)\n    else:\n        train_img_path = os.path.join(root, \'test\', \'Img\')\n        train_mask_path = os.path.join(root, \'test\', \'GT\')\n\n        images = os.listdir(train_img_path)\n        labels = os.listdir(train_mask_path)\n\n        images.sort()\n        labels.sort()\n\n        for it_im, it_gt in zip(images, labels):\n            item = (os.path.join(train_img_path, it_im), os.path.join(train_mask_path, it_gt))\n            items.append(item)\n\n    return items\n\n\nclass MedicalImageDataset(Dataset):\n    """"""CHAOS dataset.""""""\n\n    def __init__(self, mode, root_dir, transform=None, mask_transform=None, augment=False, equalize=False):\n        """"""\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        """"""\n        self.root_dir = root_dir\n        self.transform = transform\n        self.mask_transform = mask_transform\n        self.imgs = make_dataset(root_dir, mode)\n        self.augmentation = augment\n        self.equalize = equalize\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def augment(self, img, mask):\n        if random() > 0.5:\n            img = ImageOps.flip(img)\n            mask = ImageOps.flip(mask)\n        if random() > 0.5:\n            img = ImageOps.mirror(img)\n            mask = ImageOps.mirror(mask)\n        if random() > 0.5:\n            angle = random() * 60 - 30\n            img = img.rotate(angle)\n            mask = mask.rotate(angle)\n        return img, mask\n\n    def __getitem__(self, index):\n        img_path, mask_path = self.imgs[index]\n        # print(""{} and {}"".format(img_path,mask_path))\n        #img = Image.open(img_path)  # .convert(\'RGB\')\n        #mask = Image.open(mask_path)  # .convert(\'RGB\')\n        img = Image.open(img_path).convert(\'L\')\n        mask = Image.open(mask_path).convert(\'L\')\n        \n        #print(\'{} and {}\'.format(img_path,mask_path))\n        if self.equalize:\n            img = ImageOps.equalize(img)\n\n        if self.augmentation:\n            img, mask = self.augment(img, mask)\n\n        if self.transform:\n            img = self.transform(img)\n            mask = self.mask_transform(mask)\n\n        return [img, mask, img_path]\n'"
src/models/attention.py,11,"b'import math\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\nimport pdb\n\n#torch_ver = torch.__version__[:3]\n\n__all__ = [\'PAM_Module\', \'CAM_Module\', \'semanticModule\']\n\n\n\nclass _EncoderBlock(nn.Module):\n    """"""\n    Encoder block for Semantic Attention Module\n    """"""\n    def __init__(self, in_channels, out_channels, dropout=False):\n        super(_EncoderBlock, self).__init__()\n        layers = [\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        ]\n        if dropout:\n            layers.append(nn.Dropout())\n        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n        self.encode = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.encode(x)\n\n\nclass _DecoderBlock(nn.Module):\n    """"""\n    Decoder Block for Semantic Attention Module\n    """"""\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super(_DecoderBlock, self).__init__()\n        self.decode = nn.Sequential(\n            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(middle_channels, middle_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=2, stride=2),\n        )\n\n    def forward(self, x):\n        return self.decode(x)\n\n\nclass semanticModule(nn.Module):\n    """"""\n    Semantic attention module\n    """"""\n    def __init__(self, in_dim):\n        super(semanticModule, self).__init__()\n        self.chanel_in = in_dim\n\n        self.enc1 = _EncoderBlock(in_dim, in_dim*2)\n        self.enc2 = _EncoderBlock(in_dim*2, in_dim*4)\n        self.dec2 = _DecoderBlock(in_dim * 4, in_dim * 2, in_dim * 2)\n        self.dec1 = _DecoderBlock(in_dim * 2, in_dim, in_dim )\n\n    def forward(self,x):\n\n        enc1 = self.enc1(x)\n        enc2 = self.enc2(enc1)\n\n        dec2 = self.dec2( enc2)\n        dec1 = self.dec1( F.upsample(dec2, enc1.size()[2:], mode=\'bilinear\'))\n\n        return enc2.view(-1), dec1\n\nclass PAM_Module(nn.Module):\n    """""" Position attention module""""""\n    #Ref from SAGAN\n    def __init__(self, in_dim):\n        super(PAM_Module, self).__init__()\n        self.chanel_in = in_dim\n\n        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n        self.softmax = nn.Softmax(dim=-1)\n    def forward(self, x):\n        """"""\n        Parameters:\n        ----------\n            inputs :\n                x : input feature maps( B X C X H X W)\n            returns :\n                out : attention value + input feature\n                attention: B X (HxW) X (HxW)\n        """"""\n        m_batchsize, C, height, width = x.size()\n        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n\n        energy = torch.bmm(proj_query, proj_key)\n        attention = self.softmax(energy)\n        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n        out = out.view(m_batchsize, C, height, width)\n\n        out = self.gamma * out + x\n        return out\n\n\nclass CAM_Module(nn.Module):\n    """""" Channel attention module""""""\n    def __init__(self, in_dim):\n        super(CAM_Module, self).__init__()\n        self.chanel_in = in_dim\n        \n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.softmax  = nn.Softmax(dim=-1)\n    def forward(self,x):\n        """"""\n        Parameters:\n        ----------\n            inputs :\n                x : input feature maps( B X C X H X W)\n            returns :\n                out : attention value + input feature\n                attention: B X C X C\n        """"""\n        m_batchsize, C, height, width = x.size()\n        proj_query = x.view(m_batchsize, C, -1)\n        proj_key = x.view(m_batchsize, C, -1).permute(0, 2, 1)\n       \n        energy = torch.bmm(proj_query, proj_key)\n        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n        attention = self.softmax(energy_new)\n        proj_value = x.view(m_batchsize, C, -1)\n\n        out = torch.bmm(attention, proj_value)\n        out = out.view(m_batchsize, C, height, width)\n\n        out = self.gamma * out + x\n        return out\n\nclass PAM_CAM_Layer(nn.Module):\n    """"""\n    Helper Function for PAM and CAM attention\n    \n    Parameters:\n    ----------\n    input:\n        in_ch : input channels\n        use_pam : Boolean value whether to use PAM_Module or CAM_Module\n    output:\n        returns the attention map\n    """"""\n    def __init__(self, in_ch, use_pam = True):\n        super(PAM_CAM_Layer, self).__init__()\n        \n        self.attn = nn.Sequential(\n            nn.Conv2d(in_ch * 2, in_ch, kernel_size=3, padding=1),\n            nn.BatchNorm2d(in_ch),\n            nn.PReLU(),\n            PAM_Module(in_ch) if use_pam else CAM_Module(in_ch),\n\t\t\tnn.Conv2d(in_ch, in_ch, kernel_size=3, padding=1),\n            nn.BatchNorm2d(in_ch),\n            nn.PReLU()\n        )\n    \n    def forward(self, x):\n        return self.attn(x)\n    \nclass MultiConv(nn.Module):\n    """"""\n    Helper function for Multiple Convolutions for refining.\n    \n    Parameters:\n    ----------\n    inputs:\n        in_ch : input channels\n        out_ch : output channels\n        attn : Boolean value whether to use Softmax or PReLU\n    outputs:\n        returns the refined convolution tensor\n    """"""\n    def __init__(self, in_ch, out_ch, attn = True):\n        super(MultiConv, self).__init__()\n        \n        self.fuse_attn = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64), \n            nn.PReLU(),\n            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1), \n            nn.BatchNorm2d(64), \n            nn.PReLU(),\n            nn.Conv2d(out_ch, out_ch, kernel_size=1), \n            nn.BatchNorm2d(64), \n            nn.Softmax2d() if attn else nn.PReLU()\n        )\n    \n    def forward(self, x):\n        return self.fuse_attn(x)\n'"
src/models/my_stacked_danet.py,36,"b""from functools import reduce\n\nimport pdb\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom .attention import (\n    PAM_Module,\n    CAM_Module,\n    semanticModule,\n    PAM_CAM_Layer,\n    MultiConv\n)\n\nfrom .resnext101_regular import ResNeXt101\n\nclass DAF_stack(nn.Module):\n    def __init__(self):\n        super(DAF_stack, self).__init__()\n        self.resnext = ResNeXt101()\n\n        self.down4 = nn.Sequential(\n            nn.Conv2d(2048, 64, kernel_size=1), nn.BatchNorm2d(64), nn.PReLU()\n        )\n        self.down3 = nn.Sequential(\n            nn.Conv2d(1024, 64, kernel_size=1), nn.BatchNorm2d(64), nn.PReLU()\n        )\n        self.down2 = nn.Sequential(\n            nn.Conv2d(512, 64, kernel_size=1), nn.BatchNorm2d(64), nn.PReLU()\n        )\n        self.down1 = nn.Sequential(\n            nn.Conv2d(256, 64, kernel_size=1), nn.BatchNorm2d(64), nn.PReLU()\n        )\n\n        inter_channels = 64\n        out_channels=64\n\n        self.conv6_1 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))\n        self.conv6_2 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))\n        self.conv6_3 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))\n        self.conv6_4 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))\n\n        self.conv7_1 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))\n        self.conv7_2 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))\n        self.conv7_3 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))\n        self.conv7_4 = nn.Sequential(nn.Dropout2d(0.1, False), nn.Conv2d(64, out_channels, 1))\n\n        self.conv8_1=nn.Conv2d(64,64,1)\n        self.conv8_2=nn.Conv2d(64,64,1)\n        self.conv8_3=nn.Conv2d(64,64,1)\n        self.conv8_4=nn.Conv2d(64,64,1)\n        self.conv8_11=nn.Conv2d(64,64,1)\n        self.conv8_12=nn.Conv2d(64,64,1)\n        self.conv8_13=nn.Conv2d(64,64,1)\n        self.conv8_14=nn.Conv2d(64,64,1)\n\n        self.softmax_1 = nn.Softmax(dim=-1)\n\n        self.pam_attention_1_1= PAM_CAM_Layer(64, True)\n        self.cam_attention_1_1= PAM_CAM_Layer(64, False)\n        self.semanticModule_1_1 = semanticModule(128)\n        \n        self.conv_sem_1_1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        self.conv_sem_1_2 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        self.conv_sem_1_3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        self.conv_sem_1_4 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n    \n    #Dual Attention mechanism\n        self.pam_attention_1_2 = PAM_CAM_Layer(64)\n        self.cam_attention_1_2 = PAM_CAM_Layer(64, False)\n        self.pam_attention_1_3 = PAM_CAM_Layer(64)\n        self.cam_attention_1_3 = PAM_CAM_Layer(64, False)\n        self.pam_attention_1_4 = PAM_CAM_Layer(64)\n        self.cam_attention_1_4 = PAM_CAM_Layer(64, False)\n        \n        self.pam_attention_2_1 = PAM_CAM_Layer(64)\n        self.cam_attention_2_1 = PAM_CAM_Layer(64, False)\n        self.semanticModule_2_1 = semanticModule(128)\n        \n        self.conv_sem_2_1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        self.conv_sem_2_2 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        self.conv_sem_2_3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n        self.conv_sem_2_4 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n\n        self.pam_attention_2_2 = PAM_CAM_Layer(64)\n        self.cam_attention_2_2 = PAM_CAM_Layer(64, False)\n        self.pam_attention_2_3 = PAM_CAM_Layer(64)\n        self.cam_attention_2_3 = PAM_CAM_Layer(64, False)\n        self.pam_attention_2_4 = PAM_CAM_Layer(64)\n        self.cam_attention_2_4 = PAM_CAM_Layer(64, False)\n        \n        self.fuse1 = MultiConv(256, 64, False)\n\n        self.attention4 = MultiConv(128, 64)\n        self.attention3 = MultiConv(128, 64)\n        self.attention2 = MultiConv(128, 64)\n        self.attention1 = MultiConv(128, 64)\n\n        self.refine4 = MultiConv(128, 64, False)\n        self.refine3 = MultiConv(128, 64, False)\n        self.refine2 = MultiConv(128, 64, False)\n        self.refine1 = MultiConv(128, 64, False)\n\n        self.predict4 = nn.Conv2d(64, 5, kernel_size=1)\n        self.predict3 = nn.Conv2d(64, 5, kernel_size=1)\n        self.predict2 = nn.Conv2d(64, 5, kernel_size=1)\n        self.predict1 = nn.Conv2d(64, 5, kernel_size=1)\n\n        self.predict4_2 = nn.Conv2d(64, 5, kernel_size=1)\n        self.predict3_2 = nn.Conv2d(64, 5, kernel_size=1)\n        self.predict2_2 = nn.Conv2d(64, 5, kernel_size=1)\n        self.predict1_2 = nn.Conv2d(64, 5, kernel_size=1)\n\n    def forward(self, x):\n        layer0 = self.resnext.layer0(x)\n        layer1 = self.resnext.layer1(layer0)\n        layer2 = self.resnext.layer2(layer1)\n        layer3 = self.resnext.layer3(layer2)\n        layer4 = self.resnext.layer4(layer3)\n\n        down4 = F.upsample(self.down4(layer4), size=layer1.size()[2:], mode='bilinear')\n        down3 = F.upsample(self.down3(layer3), size=layer1.size()[2:], mode='bilinear')\n        down2 = F.upsample(self.down2(layer2), size=layer1.size()[2:], mode='bilinear')\n        down1 = self.down1(layer1)\n\n        predict4 = self.predict4(down4)\n        predict3 = self.predict3(down3)\n        predict2 = self.predict2(down2)\n        predict1 = self.predict1(down1)\n\n        fuse1 = self.fuse1(torch.cat((down4, down3, down2, down1), 1))\n\n        semVector_1_1,semanticModule_1_1 = self.semanticModule_1_1(torch.cat((down4, fuse1),1))\n\n\n        attn_pam4 = self.pam_attention_1_4(torch.cat((down4, fuse1), 1))\n        attn_cam4 = self.cam_attention_1_4(torch.cat((down4, fuse1), 1))\n\n        attention1_4=self.conv8_1((attn_cam4+attn_pam4)*self.conv_sem_1_1(semanticModule_1_1))\n\n        semVector_1_2, semanticModule_1_2 = self.semanticModule_1_1(torch.cat((down3, fuse1), 1))\n        attn_pam3 = self.pam_attention_1_3(torch.cat((down3, fuse1), 1))\n        attn_cam3 = self.cam_attention_1_3(torch.cat((down3, fuse1), 1))\n        attention1_3=self.conv8_2((attn_cam3+attn_pam3)*self.conv_sem_1_2(semanticModule_1_2))\n\n        semVector_1_3, semanticModule_1_3 = self.semanticModule_1_1(torch.cat((down2, fuse1), 1))\n        attn_pam2 = self.pam_attention_1_2(torch.cat((down2, fuse1), 1))\n        attn_cam2 = self.cam_attention_1_2(torch.cat((down2, fuse1), 1))\n        attention1_2=self.conv8_3((attn_cam2+attn_pam2)*self.conv_sem_1_3(semanticModule_1_3))\n\n        semVector_1_4, semanticModule_1_4 = self.semanticModule_1_1(torch.cat((down1, fuse1), 1))\n        attn_pam1 = self.pam_attention_1_1(torch.cat((down1, fuse1), 1))\n        attn_cam1 = self.cam_attention_1_1(torch.cat((down1, fuse1), 1))\n        attention1_1 = self.conv8_4((attn_cam1+attn_pam1) * self.conv_sem_1_4(semanticModule_1_4))\n        \n        ##new design with stacked attention\n\n        semVector_2_1, semanticModule_2_1 = self.semanticModule_2_1(torch.cat((down4, attention1_4 * fuse1), 1))\n\n        refine4_1 = self.pam_attention_2_4(torch.cat((down4,attention1_4*fuse1),1))\n        refine4_2 = self.cam_attention_2_4(torch.cat((down4,attention1_4*fuse1),1))\n        refine4 = self.conv8_11((refine4_1+refine4_2) * self.conv_sem_2_1(semanticModule_2_1))\n\n        semVector_2_2, semanticModule_2_2 = self.semanticModule_2_1(torch.cat((down3, attention1_3 * fuse1), 1))\n        refine3_1 = self.pam_attention_2_3(torch.cat((down3,attention1_3*fuse1),1))\n        refine3_2 = self.cam_attention_2_3(torch.cat((down3,attention1_3*fuse1),1))\n        refine3 = self.conv8_12((refine3_1+refine3_2) * self.conv_sem_2_2(semanticModule_2_2))\n\n        semVector_2_3, semanticModule_2_3 = self.semanticModule_2_1(torch.cat((down2, attention1_2 * fuse1), 1))\n        refine2_1 = self.pam_attention_2_2(torch.cat((down2,attention1_2*fuse1),1))\n        refine2_2 = self.cam_attention_2_2(torch.cat((down2,attention1_2*fuse1),1))\n        refine2 = self.conv8_13((refine2_1+refine2_2)*self.conv_sem_2_3(semanticModule_2_3))\n\n        semVector_2_4, semanticModule_2_4 = self.semanticModule_2_1(torch.cat((down1, attention1_1 * fuse1), 1))\n        refine1_1 = self.pam_attention_2_1(torch.cat((down1,attention1_1 * fuse1),1))\n        refine1_2 = self.cam_attention_2_1(torch.cat((down1,attention1_1 * fuse1),1))\n\n        refine1=self.conv8_14((refine1_1+refine1_2) * self.conv_sem_2_4(semanticModule_2_4))\n        \n        predict4_2 = self.predict4_2(refine4)\n        predict3_2 = self.predict3_2(refine3)\n        predict2_2 = self.predict2_2(refine2)\n        predict1_2 = self.predict1_2(refine1)\n\n        predict1 = F.upsample(predict1, size=x.size()[2:], mode='bilinear')\n        predict2 = F.upsample(predict2, size=x.size()[2:], mode='bilinear')\n        predict3 = F.upsample(predict3, size=x.size()[2:], mode='bilinear')\n        predict4 = F.upsample(predict4, size=x.size()[2:], mode='bilinear')\n\n        predict1_2 = F.upsample(predict1_2, size=x.size()[2:], mode='bilinear')\n        predict2_2 = F.upsample(predict2_2, size=x.size()[2:], mode='bilinear')\n        predict3_2 = F.upsample(predict3_2, size=x.size()[2:], mode='bilinear')\n        predict4_2 = F.upsample(predict4_2, size=x.size()[2:], mode='bilinear')\n        \n        if self.training:\n            return semVector_1_1,\\\n                   semVector_2_1, \\\n                   semVector_1_2, \\\n                   semVector_2_2, \\\n                   semVector_1_3, \\\n                   semVector_2_3, \\\n                   semVector_1_4, \\\n                   semVector_2_4, \\\n                   torch.cat((down1, fuse1), 1),\\\n                   torch.cat((down2, fuse1), 1),\\\n                   torch.cat((down3, fuse1), 1),\\\n                   torch.cat((down4, fuse1), 1), \\\n                   torch.cat((down1, attention1_1 * fuse1), 1), \\\n                   torch.cat((down2, attention1_2 * fuse1), 1), \\\n                   torch.cat((down3, attention1_3 * fuse1), 1), \\\n                   torch.cat((down4, attention1_4 * fuse1), 1), \\\n                   semanticModule_1_4, \\\n                   semanticModule_1_3, \\\n                   semanticModule_1_2, \\\n                   semanticModule_1_1, \\\n                   semanticModule_2_4, \\\n                   semanticModule_2_3, \\\n                   semanticModule_2_2, \\\n                   semanticModule_2_1, \\\n                   predict1, \\\n                   predict2, \\\n                   predict3, \\\n                   predict4, \\\n                   predict1_2, \\\n                   predict2_2, \\\n                   predict3_2, \\\n                   predict4_2\n        else:\n            return ((predict1_2 + predict2_2 + predict3_2 + predict4_2) / 4)\n        \n"""
src/models/resnext.py,1,"b'from functools import reduce\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import resnext50_32x4d, resnext101_32x8d\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def forward(self, input):\n        return list(map(self.lambda_func, self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def forward(self, input):\n        return reduce(self.lambda_func, self.forward_prepare(input))\n\ndef resnext50():\n    model  = resnext50_32x4d()\n    model.conv1 = nn.Conv2d(1, 64, (7, 7), (2, 2), (3, 3), 1, 1, bias = False)\n    model.avgpool = nn.AvgPool2d((7,7), (1, 1))\n    model.fc = nn.Sequential(\n        Lambda(lambda  x: x.view(x.size(0), -1)),\n        Lambda(lambda  x: x.view(1, -1) if 1 == len(x.size()) else x),\n        nn.Linear(2048, 1000)\n    )\n    return model \n\ndef resnext101():\n    model  = resnext101_32x8d()\n    model.conv1 = nn.Conv2d(1, 64, (7, 7), (2, 2), (3, 3), 1, 1, bias = False)\n    model.avgpool = nn.AvgPool2d((7,7), (1, 1))\n    model.fc = nn.Sequential(\n        Lambda(lambda  x: x.view(x.size(0), -1)),\n        Lambda(lambda  x: x.view(1, -1) if 1 == len(x.size()) else x),\n        nn.Linear(2048, 1000)\n    )\n    return model\n'"
src/models/resnext101_regular.py,0,"b'import torch\nfrom torch import nn\n\nfrom .resnext import resnext50, resnext101\n\nclass ResNeXt101(nn.Module):\n    def __init__(self):\n        super(ResNeXt101, self).__init__()\n        net = resnext101()\n        \n        net = list(net.children())\n        self.layer0 = nn.Sequential(*net[:3])\n        self.layer1 = nn.Sequential(*net[3: 5])\n        self.layer2 = net[5]\n        self.layer3 = net[6]\n        self.layer4 = net[7]\n\n    def forward(self, x):\n        layer0 = self.layer0(x)\n        layer1 = self.layer1(layer0)\n        layer2 = self.layer2(layer1)\n        layer3 = self.layer3(layer2)\n        layer4 = self.layer4(layer3)\n        return layer4\n'"
