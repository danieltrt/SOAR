file_path,api_count,code
tests/test_a2c.py,30,"b'\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport time\nimport math\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--discountFactor\', dest=\'discountFactor\', type=float, default=0.99)\nparser.add_argument(\'--traceLength\', dest=\'traceLength\', type=int, default=5)\nparser.add_argument(\'--totalStepCount\', dest=\'totalStepCount\', type=int, default=1000)\nparser.add_argument(\'--batchSize\', dest=\'batchSize\', type=int, default=128)\nparser.add_argument(\'--hiddenCount\', dest=\'hiddenCount\', type=int, default=16)\nparser.add_argument(\'--logSteps\', dest=\'logSteps\', type=int, default=100)\nparser.add_argument(\'--cuda\', dest=\'cuda\', type=bool, default=False)\nargs = parser.parse_args()\n\nif args.cuda:\n  device = \'cuda\'\nelse:\n  device = \'cpu\'\n\ndef normalized_columns_init(self, std = 1.0):\n  result = torch.randn_like(self)\n  result.mul_(std / torch.sqrt((result * result).sum([0], keepdim = True)))\n  self.data.copy_(result)\n\nclass Model():\n  def __init__(result, traceLength, observationCount, actionCount, hiddenSize):\n    result.hiddenSize = hiddenSize\n\n    result.gru = nn.GRUCell(observationCount, hiddenSize).to(device=device)\n    result.policyNet = nn.Linear(hiddenSize, actionCount).to(device=device)\n    result.valueNFunctionNet = nn.Linear(hiddenSize, 1).to(device=device)\n\n    normalized_columns_init(result.policyNet.weight.data)\n    normalized_columns_init(result.valueNFunctionNet.weight.data)\n\n    result.optimizer = optim.Adam(list(result.gru.parameters()) + list(result.policyNet.parameters()) + list(result.valueNFunctionNet.parameters()))\n\n  def policy(self, observation, initialState, mask = None):\n\n    while observation.ndimension() < 3:\n      observation = observation.unsqueeze(0)\n\n    #featureCount = np.prod(observationSpace)\n    batchSize = observation.size(1)\n    traceLength = observation.size(0)\n\n    self.state_h = initialState\n    if initialState is None:\n      self.state_h = torch.zeros([batchSize, self.hiddenSize], device=device)\n\n    logits = []\n    values = []\n\n    for i in range(0, traceLength):\n      self.state_h = self.state_h.detach()\n      self.state_h = self.gru(observation[i], self.state_h)\n\n      if not (mask is None):\n        # Mask off invalid steps. This will also zero the gadient\n        self.state_h = self.state_h * mask[i].type(torch.FloatTensor).unsqueeze(-1)\n\n      logits.append(self.policyNet(self.state_h))\n      values.append(self.valueNFunctionNet(self.state_h))\n\n    return (torch.stack(logits), torch.stack(values).squeeze())\n\n  def explorationStep(self, observation, initialState):\n    with torch.no_grad():\n      \n      logits, valueFunction = self.policy(observation, initialState)\n      probs = logits.softmax(2)\n\n      action = probs.squeeze(0).multinomial(1).squeeze()\n      return (action, valueFunction, self.state_h)\n\n  def inferenceStep(self, observation, initialState):\n    with torch.no_grad():\n      logits, valueFunction = self.policy(observation, initialState)\n      action = logits.argmax(2).type(torch.FloatTensor).squeeze()\n      return (action, self.state_h)\n\n  def estimateValue(self, observation, initialState): \n    with torch.no_grad():\n      logits, valueFunction = self.policy(observation, initialState)\n      return valueFunction\n\n  def train(self, observation, action, done, targetValue, value, initialState):\n\n    logits, valueFunction = self.policy(observation, initialState, done)\n\n    probs = logits.softmax(2)\n    logProbs = logits.log_softmax(2) # Robust against zero-grads, unline `log(softmax())`\n\n    actionCount = logits.size(1)\n    oneHotActions = torch.zeros_like(logits).scatter_(2, action.unsqueeze(-1).type(torch.LongTensor), 1.0) # one_hot(action, actionCount)\n\n    advantage = targetValue - value\n    valueDiff = targetValue - valueFunction\n\n    policyEntropy = -torch.mean(torch.sum(probs * logProbs, [2]))\n    policyGradientLoss = -torch.mean(torch.sum(logProbs * oneHotActions, [2]) * advantage) # tf.nn.sparse_softmax_cross_entropy_with_logits(logits = policy.pi, labels = action)\n    valueFunctionLoss = 0.5 * torch.mean(valueDiff * valueDiff)\n    loss = policyGradientLoss + 0.5 * valueFunctionLoss - 0.01 * policyEntropy\n\n    loss.backward()\n    self.optimizer.step()\n    self.optimizer.zero_grad()\n\n    return (loss, policyGradientLoss, valueFunctionLoss, policyEntropy)\n\nclass Environment():\n  def reset(self, done = None):\n    if done == None:\n      self.state.uniform_(-0.05, 0.05)\n    else:\n      self.state.masked_scatter_(done.unsqueeze(1), zeros([self.state.numel], device=device).uniform_(-0.05, 0.05))\n    \n    self.done = torch.zeros_like(self.done)\n\n    return self.state\n\n  def __init__(result, batchSize):\n    result.batchSize = batchSize\n\n    result.gravity = 9.8\n    result.masscart = 1.0\n    result.masspole = 0.1\n    result.total_mass = (result.masspole + result.masscart)\n    result.length = 0.5 # actually half the pole\'s length\n    result.polemass_length = (result.masspole * result.length)\n    result.force_mag = 10.0\n    result.tau = 0.02  # seconds between state updates\n    result.euler_integrator = True\n\n    # Angle at which to fail the episode\n    result.theta_threshold_radians = 12 * 2 * math.pi / 360\n    result.x_threshold = 2.4\n\n    # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n    # let angleLimit = [\n    #   self.x_threshold * 2, float.high,\n    #   self.theta_threshold_radians * 2, float.high\n    # ]\n\n    result.action_space = 2 #spaces.Discrete(2)\n    result.observation_space = 4 #spaces.Box(-high, high, dtype=np.float32)\n\n    #result.seed()\n    result.state = torch.zeros([batchSize, 4], device=device)\n    result.done = torch.zeros([batchSize], device=device).type(torch.ByteTensor)\n\n  def step(self, action):\n    #assert self.action_space.contains(action), ""%r (%s) invalid""%(action, type(action))\n    x, x_dot, theta, theta_dot = (self.state[:, 0], self.state[:, 1], self.state[:, 2], self.state[:, 3])\n\n    force = (action * 2.0 - 1.0) * self.force_mag\n    costheta = torch.cos(theta)\n    sintheta = torch.sin(theta)\n    temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n\n    thetaacc = (self.gravity * sintheta - costheta * temp) / (self.length * (4.0 / 3.0 - self.masspole * costheta * costheta / self.total_mass))\n    xacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n\n    if self.euler_integrator:\n      x = x + self.tau * x_dot\n      x_dot = x_dot + self.tau * xacc\n      theta = theta + self.tau * theta_dot\n      theta_dot = theta_dot + self.tau * thetaacc\n\n    else: # semi-implicit euler\n      x_dot = x_dot + self.tau * xacc\n      x = x + self.tau * x_dot\n      theta_dot = theta_dot + self.tau * thetaacc\n      theta = theta + self.tau * theta_dot\n\n    self.state[:, 0], self.state[:, 1], self.state[:, 2], self.state[:, 3] = (x, x_dot, theta, theta_dot)\n\n    reward = torch.zeros([self.batchSize], device=device)\n    reward.masked_fill_(self.done ^ 1, 1.0) # not done\n\n    self.done = \\\n      self.done | \\\n      (x < -self.x_threshold) | \\\n      (x > self.x_threshold) | \\\n      (theta < -self.theta_threshold_radians) | \\\n      (theta > self.theta_threshold_radians)\n    return (self.state, reward, self.done)\n\nenvironment = Environment(args.batchSize)\ntestEnvironment = Environment(10)\nmodel = Model(args.traceLength, environment.observation_space, environment.action_space, args.hiddenCount)\n\nobservation = environment.reset()\nstate = None\n\ntraceTime = 0.0\ntrainingTime = 0.0\ntrainingCount = 0\n\nfor batchIndex in range(0, args.totalStepCount):\n\n  currentTime = time.perf_counter()\n\n  initialState = state\n  observationBatch = torch.zeros([args.traceLength, args.batchSize, environment.observation_space], device=device)\n  actionBatch = torch.zeros([args.traceLength, args.batchSize], device=device)\n  valueBatch = torch.zeros([args.traceLength, args.batchSize], device=device)\n  rewardBatch = torch.zeros([args.traceLength, args.batchSize], device=device)\n  doneBatch = torch.zeros([args.traceLength, args.batchSize], device=device).type(torch.ByteTensor)\n\n  # Do N steps through the environment, randomly sampling actions from the policy\n  for i in range(0, args.traceLength):\n    observationBatch[i] = observation\n    actionBatch[i], valueBatch[i], state = model.explorationStep(observation, state)\n    observation, rewardBatch[i], doneBatch[i] = environment.step(actionBatch[i])\n\n    if i > 0:\n      observation.masked_fill_(doneBatch[i - 1].unsqueeze(-1), 0.0)\n\n    #echo fmt""{actionBatch[i, 0].float32} --> {observation[0, 0].float32}, {observation[0, 1].float32}, {observation[0, 2].float32 / PI * 180}, {observation[0, 3].float32} --> {rewardBatch[i, 0].float32} ({doneBatch[i, 0].type(torch.FloatTensor).float32})""\n\n  #observationBatch.masked_fill_(doneBatch.unsqueeze(-1), 0.0)\n\n  # Estimate the value of the state just AFTER the trace\n  # If the last step (or any before) was the end of the episode, the estimate is 0.\n  valueEstimate = model.estimateValue(observation, state) * (doneBatch[args.traceLength - 1] ^ 1).type(torch.FloatTensor)\n\n  # Propagate the value of each step back through the trace,\n  # summing up immediate returns and discounted future returns\n  discountedReturn = torch.zeros([args.traceLength, args.batchSize], device=device)\n  for i in reversed(range(args.traceLength)):\n    valueEstimate = valueEstimate * args.discountFactor + rewardBatch[i].squeeze()\n    discountedReturn[i] = valueEstimate\n\n  traceTime += time.perf_counter() - currentTime\n  currentTime = time.perf_counter()\n\n  loss, policyLoss, valueLoss, policyEntropy = model.train(observationBatch, actionBatch, doneBatch, discountedReturn, valueBatch, initialState)\n\n  trainingTime += time.perf_counter() - currentTime\n  trainingCount += 1\n\n  if batchIndex % args.logSteps == args.logSteps - 1:\n    print(f""Training: trace collection time: {traceTime / trainingCount}s, training time: {trainingTime / trainingCount}s"")\n    print(f""Training: loss = {loss}, policyLoss = {policyLoss}, valueLoss = {valueLoss}, policyEntropy = {policyEntropy}"")\n\n    currentTime = time.perf_counter()\n\n    eval_state = None\n    eval_observation = testEnvironment.reset()\n    totalReward = 0.0\n\n    stepCount = 0\n    while True:\n      eval_action, eval_state = model.inferenceStep(eval_observation, eval_state)\n      eval_observation, eval_reward, eval_done = testEnvironment.step(eval_action)\n\n      eval_rewards = eval_reward.masked_select(eval_done ^ 1)\n      totalReward += eval_rewards.sum() / eval_observation.size(0)\n\n      # print(f""{action[0].float32} --> {observation[0, 0].float32}, {observation[0, 1].float32}, {observation[0, 2].float32 / PI * 180}, {observation[0, 3].float32} --> {reward[0].float32} ({done[0].type(torch.FloatTensor).float32})"")\n\n      stepCount += 1\n\n      if stepCount == 200:\n        break\n\n      if eval_rewards.numel() == 0:\n        break\n\n    print(f""Testing: total reward = {totalReward}"")\n\n  if doneBatch[args.traceLength - 1].all():\n    observation = environment.reset()\n    state = None'"
tests/test_autograd.py,6,"b'import torch\n\nx = torch.tensor([[0.1, 0.3], [-0.4, 0.2]])\ny = torch.tensor([[0.7, -0.5], [0.1, 0.1]])\nz = torch.tensor([[0.2, -0.4], [-0.5, -0.2]])\n\nx.requires_grad = True\n\nr = ((x + y) * y).sin() + (z - x).tanh()\nr.backward(torch.ones_like(r))\nprint(x.grad)\n\n\n\n\nx = torch.tensor([0.9, 0.8, 0.7])\nx.requires_grad = True\n\na, b, c = x.chunk(3, 0)\nr = (c + (a + a))\nr.backward(torch.ones_like(r))\nprint(x.grad)'"
tests/test_xor.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ninputs = torch.tensor([\n  [0.0, 0.0],\n  [0.0, 1.0],\n  [1.0, 0.0],\n  [1.0, 1.0],\n])\n\ntargets = torch.tensor([\n  [0.0],\n  [1.0],\n  [1.0],\n  [0.0],\n])\n\nfc1 = nn.Linear(2, 4)\nfc2 = nn.Linear(4, 1)\nloss_fn = nn.MSELoss()\noptimizer = optim.SGD(list(fc1.parameters()) + list(fc2.parameters()), lr = 0.01, momentum = 0.1)\n\ntorch.set_num_threads(1)\n\nfor i in range(0, 50000):\n  optimizer.zero_grad()\n\n  predictions = fc1(inputs).relu()\n  predictions = fc2(predictions).sigmoid()\n\n  loss = loss_fn(predictions, targets)\n  loss.backward()\n  optimizer.step()\n\n  if i % 5000 == 0:\n    print(loss)\n'"
