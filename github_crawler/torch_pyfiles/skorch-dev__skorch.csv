file_path,api_count,code
setup.py,0,"b'import os\n\nfrom setuptools import setup, find_packages\n\n\nwith open(\'VERSION\', \'r\') as f:\n    version = f.read().rstrip()\n\nwith open(\'requirements.txt\') as f:\n    install_requires = [l.strip() for l in f]\n\ntests_require = [\n    \'pytest\',\n    \'pytest-cov\',\n]\n\ndocs_require = [\n    \'Sphinx\',\n    \'sphinx_rtd_theme\',\n    \'numpydoc\',\n]\n\nhere = os.path.abspath(os.path.dirname(__file__))\ntry:\n    README = open(os.path.join(here, \'README.rst\')).read()\nexcept IOError:\n    README = \'\'\n\ntry:\n    CHANGES = open(os.path.join(here, \'CHANGES.txt\')).read()\nexcept IOError:\n    CHANGES = \'\'\n\nsetup(\n    name=\'skorch\',\n    version=version,\n    description=\'scikit-learn compatible neural network library for pytorch\',\n    long_description=README,\n    license=\'new BSD 3-Clause\',\n    packages=find_packages(),\n    include_package_data=True,\n    url=""https://github.com/skorch-dev/skorch"",\n    zip_safe=False,\n    install_requires=install_requires,\n    extras_require={\n        \'testing\': tests_require,\n        \'docs\': docs_require,\n    },\n)\n'"
docs/conf.py,1,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n#\n# skorch documentation build configuration file, created by\n# sphinx-quickstart on Fri Oct 13 11:29:12 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(\'..\'))\n\n\n#  on_rtd is whether we are on readthedocs.org, this line of code grabbed\n#  from docs.readthedocs.org\non_rtd = os.environ.get(\'READTHEDOCS\', None) == \'True\'\n\n\n# -- General configuration ------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'numpydoc\',\n    \'sphinx.ext.linkcode\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.autosectionlabel\',\n    \'sphinx.ext.intersphinx\',\n]\n\nnumpydoc_class_members_toctree = False\n\nintersphinx_mapping = {\n    \'pytorch\': (\'https://pytorch.org/docs/stable/\', None),\n    \'sklearn\': (\'http://scikit-learn.org/stable/\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n    \'python\': (\'https://docs.python.org/3\', None),\n}\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\nsource_suffix = \'.rst\'\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# General information about the project.\nproject = \'skorch\'\ncopyright = \'2017, Marian Tietz, Daniel Nouri, Benjamin Bossan\'\nauthor = \'Marian Tietz, Daniel Nouri, Benjamin Bossan\'\n\n# The version info for the project you\'re documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\n#\n# The short X.Y version.\n# version = \'0.1\'\n# The full version, including alpha/beta/rc tags.\n# release = \'0.1\'\nwith open(\'../VERSION\', \'r\') as f:\n    release = f.read().strip()\n    version = release.rsplit(\'.\', 1)[0]\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\', \'**tests**\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \'sphinx\'\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output ----------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'default\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\ndef setup(app):\n    app.add_stylesheet(\'css/my_theme.css\')\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nif not on_rtd:  # only import and set the theme if we\'re building docs locally\n    import sphinx_rtd_theme\n    html_theme = \'sphinx_rtd_theme\'\n    html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n\n\n# -- Options for HTMLHelp output ------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'skorchdoc\'\n\n\n# -- Options for LaTeX output ---------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'skorch.tex\', \'skorch Documentation\',\n     \'Marian Tietz, Daniel Nouri, Benjamin Bossan\', \'manual\'),\n]\n\n\n# -- Options for manual page output ---------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'skorch\', \'skorch Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output -------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'skorch\', \'skorch Documentation\',\n     author, \'skorch\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n# -- GitHub source code link ----------------------------------------------\n\n# Functionality to build github source URI, taken from sklearn.\n\nfrom operator import attrgetter\nimport inspect\nimport subprocess\nfrom functools import partial\n\nREVISION_CMD = \'git rev-parse --short HEAD\'\n\ndef _get_git_revision():\n    try:\n        revision = subprocess.check_output(REVISION_CMD.split()).strip()\n    except (subprocess.CalledProcessError, OSError):\n        print(\'Failed to execute git to get revision\')\n        return None\n    return revision.decode(\'utf-8\')\n\n\ndef _linkcode_resolve(domain, info, package, url_fmt, revision):\n    """"""Determine a link to online source for a class/method/function\n\n    This is called by sphinx.ext.linkcode\n\n    An example with a long-untouched module that everyone has\n    >>> _linkcode_resolve(\'py\', {\'module\': \'tty\',\n    ...                          \'fullname\': \'setraw\'},\n    ...                   package=\'tty\',\n    ...                   url_fmt=\'http://hg.python.org/cpython/file/\'\n    ...                           \'{revision}/Lib/{package}/{path}#L{lineno}\',\n    ...                   revision=\'xxxx\')\n    \'http://hg.python.org/cpython/file/xxxx/Lib/tty/tty.py#L18\'\n    """"""\n\n    if revision is None:\n        return\n    if domain not in (\'py\', \'pyx\'):\n        return\n    if not info.get(\'module\') or not info.get(\'fullname\'):\n        return\n\n    class_name = info[\'fullname\'].split(\'.\')[0]\n    if type(class_name) != str:\n        # Python 2 only\n        class_name = class_name.encode(\'utf-8\')\n    module = __import__(info[\'module\'], fromlist=[class_name])\n    obj = attrgetter(info[\'fullname\'])(module)\n\n    try:\n        fn = inspect.getsourcefile(obj)\n    except Exception:\n        fn = None\n    if not fn:\n        try:\n            fn = inspect.getsourcefile(sys.modules[obj.__module__])\n        except Exception:\n            fn = None\n    if not fn:\n        return\n\n    fn = os.path.relpath(fn,\n                         start=os.path.dirname(__import__(package).__file__))\n    try:\n        lineno = inspect.getsourcelines(obj)[1]\n    except Exception:\n        lineno = \'\'\n    return url_fmt.format(revision=revision, package=package,\n                          path=fn, lineno=lineno)\n\ndef project_linkcode_resolve(domain, info):\n    global _linkcode_git_revision\n    return _linkcode_resolve(domain, info,\n            package=\'skorch\',\n            revision=_linkcode_git_revision,\n            url_fmt=\'https://github.com/skorch-dev/skorch/\'\n                    \'blob/{revision}/\'\n                    \'{package}/{path}#L{lineno}\')\n\n_linkcode_git_revision = _get_git_revision()\n\n# The following is used by sphinx.ext.linkcode to provide links to github\nlinkcode_resolve = project_linkcode_resolve\n'"
skorch/__init__.py,2,"b'""""""skorch base imports""""""\n\nimport pkg_resources\nfrom pkg_resources import parse_version\n\nfrom .history import History\nfrom .net import NeuralNet\nfrom .classifier import NeuralNetClassifier\nfrom .classifier import NeuralNetBinaryClassifier\nfrom .regressor import NeuralNetRegressor\nfrom . import callbacks\n\n\nMIN_TORCH_VERSION = \'1.1.0\'\n\ntry:\n    # pylint: disable=wrong-import-position\n    import torch\nexcept ModuleNotFoundError:\n    raise ModuleNotFoundError(\n        ""No module named \'torch\', and skorch depends on PyTorch ""\n        ""(aka \'torch\'). ""\n        ""Visit https://pytorch.org/ for installation instructions."")\n\ntorch_version = pkg_resources.get_distribution(\'torch\').version\nif parse_version(torch_version) < parse_version(MIN_TORCH_VERSION):\n    msg = (\'skorch depends on a newer version of PyTorch (at least {req}, not \'\n           \'{installed}). Visit https://pytorch.org for installation details\')\n    raise ImportWarning(msg.format(req=MIN_TORCH_VERSION, installed=torch_version))\n\n\n__all__ = [\n    \'History\',\n    \'NeuralNet\',\n    \'NeuralNetClassifier\',\n    \'NeuralNetBinaryClassifier\',\n    \'NeuralNetRegressor\',\n    \'callbacks\',\n]\n\n\ntry:\n    __version__ = pkg_resources.get_distribution(\'skorch\').version\nexcept:  # pylint: disable=bare-except\n    __version__ = \'n/a\'\n'"
skorch/classifier.py,10,"b'""""""NeuralNet subclasses for classification tasks.""""""\n\nimport re\n\nimport numpy as np\nfrom sklearn.base import ClassifierMixin\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom skorch import NeuralNet\nfrom skorch.callbacks import EpochTimer\nfrom skorch.callbacks import PrintLog\nfrom skorch.callbacks import EpochScoring\nfrom skorch.callbacks import PassthroughScoring\nfrom skorch.dataset import CVSplit\nfrom skorch.utils import get_dim\nfrom skorch.utils import is_dataset\nfrom skorch.utils import to_numpy\n\n\nneural_net_clf_doc_start = """"""NeuralNet for classification tasks\n\n    Use this specifically if you have a standard classification task,\n    with input data X and target y.\n\n""""""\n\nneural_net_clf_additional_text = """"""\n\n    criterion : torch criterion (class, default=torch.nn.NLLLoss)\n      Negative log likelihood loss. Note that the module should return\n      probabilities, the log is applied during ``get_loss``.\n\n    classes : None or list (default=None)\n      If None, the ``classes_`` attribute will be inferred from the\n      ``y`` data passed to ``fit``. If a non-empty list is passed,\n      that list will be returned as ``classes_``. If the initial\n      skorch behavior should be restored, i.e. raising an\n      ``AttributeError``, pass an empty list.""""""\n\nneural_net_clf_additional_attribute = """"""classes_ : array, shape (n_classes, )\n      A list of class labels known to the classifier.\n\n""""""\n\n\ndef get_neural_net_clf_doc(doc):\n    doc = neural_net_clf_doc_start + "" "" + doc.split(""\\n "", 4)[-1]\n    pattern = re.compile(r\'(\\n\\s+)(criterion .*\\n)(\\s.+){1,99}\')\n    start, end = pattern.search(doc).span()\n    doc = doc[:start] + neural_net_clf_additional_text + doc[end:]\n    doc = doc + neural_net_clf_additional_attribute\n    return doc\n\n\n# pylint: disable=missing-docstring\nclass NeuralNetClassifier(NeuralNet, ClassifierMixin):\n    __doc__ = get_neural_net_clf_doc(NeuralNet.__doc__)\n\n    def __init__(\n            self,\n            module,\n            *args,\n            criterion=torch.nn.NLLLoss,\n            train_split=CVSplit(5, stratified=True),\n            classes=None,\n            **kwargs\n    ):\n        super(NeuralNetClassifier, self).__init__(\n            module,\n            *args,\n            criterion=criterion,\n            train_split=train_split,\n            **kwargs\n        )\n        self.classes = classes\n\n    @property\n    def _default_callbacks(self):\n        return [\n            (\'epoch_timer\', EpochTimer()),\n            (\'train_loss\', PassthroughScoring(\n                name=\'train_loss\',\n                on_train=True,\n            )),\n            (\'valid_loss\', PassthroughScoring(\n                name=\'valid_loss\',\n            )),\n            (\'valid_acc\', EpochScoring(\n                \'accuracy\',\n                name=\'valid_acc\',\n                lower_is_better=False,\n            )),\n            (\'print_log\', PrintLog()),\n        ]\n\n    @property\n    def classes_(self):\n        if self.classes is not None:\n            if not len(self.classes):\n                raise AttributeError(""{} has no attribute \'classes_\'"".format(\n                    self.__class__.__name__))\n            return self.classes\n        return self.classes_inferred_\n\n    # pylint: disable=signature-differs\n    def check_data(self, X, y):\n        if (\n                (y is None) and\n                (not is_dataset(X)) and\n                (self.iterator_train is DataLoader)\n        ):\n            msg = (""No y-values are given (y=None). You must either supply a ""\n                   ""Dataset as X or implement your own DataLoader for ""\n                   ""training (and your validation) and supply it using the ""\n                   ""``iterator_train`` and ``iterator_valid`` parameters ""\n                   ""respectively."")\n            raise ValueError(msg)\n        if y is not None:\n            self.classes_inferred_ = np.unique(y)\n\n    # pylint: disable=arguments-differ\n    def get_loss(self, y_pred, y_true, *args, **kwargs):\n        if isinstance(self.criterion_, torch.nn.NLLLoss):\n            eps = torch.finfo(y_pred.dtype).eps\n            y_pred = torch.log(y_pred + eps)\n        return super().get_loss(y_pred, y_true, *args, **kwargs)\n\n    # pylint: disable=signature-differs\n    def fit(self, X, y, **fit_params):\n        """"""See ``NeuralNet.fit``.\n\n        In contrast to ``NeuralNet.fit``, ``y`` is non-optional to\n        avoid mistakenly forgetting about ``y``. However, ``y`` can be\n        set to ``None`` in case it is derived dynamically from\n        ``X``.\n\n        """"""\n        # pylint: disable=useless-super-delegation\n        # this is actually a pylint bug:\n        # https://github.com/PyCQA/pylint/issues/1085\n        return super(NeuralNetClassifier, self).fit(X, y, **fit_params)\n\n    def predict_proba(self, X):\n        """"""Where applicable, return probability estimates for\n        samples.\n\n        If the module\'s forward method returns multiple outputs as a\n        tuple, it is assumed that the first output contains the\n        relevant information and the other values are ignored. If all\n        values are relevant, consider using\n        :func:`~skorch.NeuralNet.forward` instead.\n\n        Parameters\n        ----------\n        X : input data, compatible with skorch.dataset.Dataset\n          By default, you should be able to pass:\n\n            * numpy arrays\n            * torch tensors\n            * pandas DataFrame or Series\n            * scipy sparse CSR matrices\n            * a dictionary of the former three\n            * a list/tuple of the former three\n            * a Dataset\n\n          If this doesn\'t work with your data, you have to pass a\n          ``Dataset`` that can deal with the data.\n\n        Returns\n        -------\n        y_proba : numpy ndarray\n\n        """"""\n        # Only the docstring changed from parent.\n        # pylint: disable=useless-super-delegation\n        return super().predict_proba(X)\n\n    def predict(self, X):\n        """"""Where applicable, return class labels for samples in X.\n\n        If the module\'s forward method returns multiple outputs as a\n        tuple, it is assumed that the first output contains the\n        relevant information and the other values are ignored. If all\n        values are relevant, consider using\n        :func:`~skorch.NeuralNet.forward` instead.\n\n        Parameters\n        ----------\n        X : input data, compatible with skorch.dataset.Dataset\n          By default, you should be able to pass:\n\n            * numpy arrays\n            * torch tensors\n            * pandas DataFrame or Series\n            * scipy sparse CSR matrices\n            * a dictionary of the former three\n            * a list/tuple of the former three\n            * a Dataset\n\n          If this doesn\'t work with your data, you have to pass a\n          ``Dataset`` that can deal with the data.\n\n        Returns\n        -------\n        y_pred : numpy ndarray\n\n        """"""\n        y_preds = []\n        for yp in self.forward_iter(X, training=False):\n            yp = yp[0] if isinstance(yp, tuple) else yp\n            y_preds.append(to_numpy(yp.max(-1)[-1]))\n        y_pred = np.concatenate(y_preds, 0)\n        return y_pred\n\n\nneural_net_binary_clf_doc_start = """"""NeuralNet for binary classification tasks\n\n    Use this specifically if you have a binary classification task,\n    with input data X and target y. y must be 1d.\n\n""""""\n\nneural_net_binary_clf_criterion_text = """"""\n\n    criterion : torch criterion (class, default=torch.nn.BCEWithLogitsLoss)\n      Binary cross entropy loss with logits. Note that the module should return\n      the logit of probabilities with shape (batch_size, ).\n\n    threshold : float (default=0.5)\n      Probabilities above this threshold is classified as 1. ``threshold``\n      is used by ``predict`` and ``predict_proba`` for classification.""""""\n\n\ndef get_neural_net_binary_clf_doc(doc):\n    doc = neural_net_binary_clf_doc_start + "" "" + doc.split(""\\n "", 4)[-1]\n    pattern = re.compile(r\'(\\n\\s+)(criterion .*\\n)(\\s.+){1,99}\')\n    start, end = pattern.search(doc).span()\n    doc = doc[:start] + neural_net_binary_clf_criterion_text + doc[end:]\n    return doc\n\n\nclass NeuralNetBinaryClassifier(NeuralNet, ClassifierMixin):\n    # pylint: disable=missing-docstring\n    __doc__ = get_neural_net_binary_clf_doc(NeuralNet.__doc__)\n\n    def __init__(\n            self,\n            module,\n            *args,\n            criterion=torch.nn.BCEWithLogitsLoss,\n            train_split=CVSplit(5, stratified=True),\n            threshold=0.5,\n            **kwargs\n    ):\n        super().__init__(\n            module,\n            criterion=criterion,\n            train_split=train_split,\n            *args,\n            **kwargs\n        )\n        self.threshold = threshold\n\n    @property\n    def _default_callbacks(self):\n        return [\n            (\'epoch_timer\', EpochTimer()),\n            (\'train_loss\', PassthroughScoring(\n                name=\'train_loss\',\n                on_train=True,\n            )),\n            (\'valid_loss\', PassthroughScoring(\n                name=\'valid_loss\',\n            )),\n            (\'valid_acc\', EpochScoring(\n                \'accuracy\',\n                name=\'valid_acc\',\n                lower_is_better=False,\n            )),\n            (\'print_log\', PrintLog()),\n        ]\n\n    @property\n    def classes_(self):\n        return [0, 1]\n\n    # pylint: disable=signature-differs\n    def check_data(self, X, y):\n        super().check_data(X, y)\n        if get_dim(y) != 1:\n            raise ValueError(""The target data should be 1-dimensional."")\n\n    def infer(self, x, **fit_params):\n        """"""Perform an inference step\n\n        The first output of the ``module`` must be a single array that\n        has either shape (n,) or shape (n, 1). In the latter case, the\n        output will be reshaped to become 1-dim.\n\n        """"""\n        y_infer = super().infer(x, **fit_params)\n        rest = None\n        if isinstance(y_infer, tuple):\n            y_infer, *rest = y_infer\n\n        if (y_infer.dim() > 2) or ((y_infer.dim() == 2) and (y_infer.shape[1] != 1)):\n            raise ValueError(\n                ""Expected module output to have shape (n,) or ""\n                ""(n, 1), got {} instead"".format(tuple(y_infer.shape)))\n\n        y_infer = y_infer.reshape(-1)\n        if rest is None:\n            return y_infer\n        return (y_infer,) + tuple(rest)\n\n    # pylint: disable=signature-differs\n    def fit(self, X, y, **fit_params):\n        """"""See ``NeuralNet.fit``.\n\n        In contrast to ``NeuralNet.fit``, ``y`` is non-optional to\n        avoid mistakenly forgetting about ``y``. However, ``y`` can be\n        set to ``None`` in case it is derived dynamically from\n        ``X``.\n\n        """"""\n        # pylint: disable=useless-super-delegation\n        # this is actually a pylint bug:\n        # https://github.com/PyCQA/pylint/issues/1085\n        return super().fit(X, y, **fit_params)\n\n    def predict(self, X):\n        """"""Where applicable, return class labels for samples in X.\n\n        If the module\'s forward method returns multiple outputs as a\n        tuple, it is assumed that the first output contains the\n        relevant information and the other values are ignored. If all\n        values are relevant, consider using\n        :func:`~skorch.NeuralNet.forward` instead.\n\n        Parameters\n        ----------\n        X : input data, compatible with skorch.dataset.Dataset\n          By default, you should be able to pass:\n\n            * numpy arrays\n            * torch tensors\n            * pandas DataFrame or Series\n            * scipy sparse CSR matrices\n            * a dictionary of the former three\n            * a list/tuple of the former three\n            * a Dataset\n\n          If this doesn\'t work with your data, you have to pass a\n          ``Dataset`` that can deal with the data.\n\n        Returns\n        -------\n        y_pred : numpy ndarray\n\n        """"""\n        y_proba = self.predict_proba(X)\n        return (y_proba[:, 1] > self.threshold).astype(\'uint8\')\n\n    # pylint: disable=missing-docstring\n    def predict_proba(self, X):\n        """"""Where applicable, return probability estimates for\n        samples.\n\n        If the module\'s forward method returns multiple outputs as a\n        tuple, it is assumed that the first output contains the\n        relevant information and the other values are ignored. If all\n        values are relevant, consider using\n        :func:`~skorch.NeuralNet.forward` instead.\n\n        Parameters\n        ----------\n        X : input data, compatible with skorch.dataset.Dataset\n          By default, you should be able to pass:\n\n            * numpy arrays\n            * torch tensors\n            * pandas DataFrame or Series\n            * scipy sparse CSR matrices\n            * a dictionary of the former three\n            * a list/tuple of the former three\n            * a Dataset\n\n          If this doesn\'t work with your data, you have to pass a\n          ``Dataset`` that can deal with the data.\n\n        Returns\n        -------\n        y_proba : numpy ndarray\n\n        """"""\n        y_probas = []\n        self.check_is_fitted(attributes=[\'criterion_\'])\n        bce_logits_loss = isinstance(\n            self.criterion_, torch.nn.BCEWithLogitsLoss)\n\n        for yp in self.forward_iter(X, training=False):\n            yp = yp[0] if isinstance(yp, tuple) else yp\n            if bce_logits_loss:\n                yp = torch.sigmoid(yp)\n            y_probas.append(to_numpy(yp))\n        y_proba = np.concatenate(y_probas, 0)\n        y_proba = np.stack((1 - y_proba, y_proba), axis=1)\n        return y_proba\n'"
skorch/cli.py,2,"b'""""""Helper functions for quick command line interfaces with skorch and\nfire.\n\n""""""\n\nfrom functools import partial\nfrom importlib import import_module\nfrom itertools import chain\nimport re\nimport shlex\nimport sys\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\n\n\n__all__ = [\'parse_args\']\n\n\n# matches: bar(), foo.bar(), foo.bar(baz)\nP_PARAMS = re.compile(r""(?P<name>^[a-zA-Z][a-zA-Z0-9_\\.]*)(?P<params>\\(.*\\)$)"")\n\nP_DEFAULTS = re.compile(\n    # standard, matches: int (default=123)\n    r""(.+\\s\\(default\\s?\\=\\s?(?P<default>.+)\\)$)|""\n    # no parens, matches: int, default=123\n    r""(.+\\sdefault\\s?\\=\\s?(?P<default_np>.+)$)|""\n    # no equal, matches: int, default 123\n    r""(.+default\\s(?P<default_ne>.+))|""\n    # \'by-default\', matches: str (l2 by default)\n    r""[^\\(]+\\((?P<default_bd>[^\\""\\\']+)(\\sby\\sdefault\\)?)|""\n    # \'by-default-double-tick\', matches: ""l1"" or ""l2"" (""l2"" by default)\n    r""[^\\(]+\\(\\""(?P<default_bd_dt>.+)\\""\\sby\\sdefault\\)?|""\n    # \'by-default-single-tick\', matches: \'l1\' or \'l2\' (\'l2\' by default)\n    r""[^\\(]+\\(\\\'(?P<default_bd_st>.+)\\\'\\sby\\sdefault\\)?""\n)\n\n\ndef _param_split(params):\n    return (p.strip(\' ,\') for p in shlex.split(params))\n\n\ndef _get_span(s, pattern):\n    """"""Return the span of the first group that matches the pattern.""""""\n    i, j = -1, -1\n\n    match = pattern.match(s)\n    if not match:\n        return i, j\n\n    for group_name in pattern.groupindex:\n        i, j = match.span(group_name)\n        if (i, j) != (-1, -1):\n            return i, j\n\n    return i, j\n\n\ndef _substitute_default(s, new_value):\n    """"""Replaces the default value in a parameter docstring by a new value.\n\n    The docstring must conform to the numpydoc style and have the form\n    ""something (keyname=<value-to-replace>)""\n\n    If no matching pattern is found or ``new_value`` is None, return\n    the input untouched.\n\n    Examples\n    --------\n    >>> _replace_default(\'int (default=128)\', 256)\n    \'int (default=256)\'\n    >>> _replace_default(\'nonlin (default = ReLU())\', nn.Hardtanh(1, 2))\n    \'nonlin (default = Hardtanh(min_val=1, max_val=2))\'\n\n    """"""\n    if new_value is None:\n        return s\n\n    # BB: ideally, I would like to replace the \'default*\' group\n    # directly but I haven\'t found a way to do this\n    i, j = _get_span(s, pattern=P_DEFAULTS)\n    if (i, j) == (-1, -1):\n        return s\n    return \'{}{}{}\'.format(s[:i], new_value, s[j:])\n\n\ndef _parse_args_kwargs(params):\n    from fire.parser import DefaultParseValue\n\n    args = ()\n    kwargs = {}\n    for param in _param_split(params):\n        if \'=\' not in param:\n            args += (DefaultParseValue(param),)\n        else:\n            k, v = param.split(\'=\')\n            kwargs[k.strip()] = DefaultParseValue(v)\n    return args, kwargs\n\n\ndef _resolve_dotted_name(dotted_name):\n    """"""Returns objects from strings\n\n    Deals e.g. with \'torch.nn.Softmax(dim=-1)\'.\n\n    Modified from palladium:\n\n    https://github.com/ottogroup/palladium/blob/8a066a9a7690557d9b1b6ed54b7d1a1502ba59e3/palladium/util.py\n\n    with added support for instantiated objects.\n\n    """"""\n    if not isinstance(dotted_name, str):\n        return dotted_name\n\n    if \'.\' not in dotted_name:\n        return dotted_name\n\n    args = None\n    params = None\n    match = P_PARAMS.match(dotted_name)\n    if match:\n        dotted_name = match.group(\'name\')\n        params = match.group(\'params\')\n\n    module, name = dotted_name.rsplit(\'.\', 1)\n    attr = import_module(module)\n    attr = getattr(attr, name)\n\n    if params:\n        args, kwargs = _parse_args_kwargs(params[1:-1])\n        attr = attr(*args, **kwargs)\n\n    return attr\n\n\ndef parse_net_kwargs(kwargs):\n    """"""Parse arguments for the estimator.\n\n    Resolves dotted names and instantiated classes.\n\n    Examples\n    --------\n    >>> kwargs = {\'lr\': 0.1, \'module__nonlin\': \'torch.nn.Hardtanh(-2, max_val=3)\'}\n    >>> parse_net_kwargs(kwargs)\n    {\'lr\': 0.1, \'module__nonlin\': Hardtanh(min_val=-2, max_val=3)}\n\n    """"""\n    if not kwargs:\n        return kwargs\n\n    resolved = {}\n    for k, v in kwargs.items():\n        resolved[k] = _resolve_dotted_name(v)\n\n    return resolved\n\n\ndef _yield_preproc_steps(model):\n    if not isinstance(model, Pipeline):\n        return\n\n    for key, val in model.get_params().items():\n        if isinstance(val, BaseEstimator):\n            if not isinstance(val, (Pipeline, FeatureUnion)):\n                yield key, val\n\n\ndef _yield_estimators(model):\n    """"""Yield estimator and its prefix from the model.\n\n    First, pipeline preprocessing steps are yielded (if there are\n    any). Next the neural net is yielded. Finally, the module is\n    yielded.\n\n    """"""\n    yield from _yield_preproc_steps(model)\n\n    net_prefixes = []\n    module_prefixes = []\n\n    if isinstance(model, Pipeline):\n        name = model.steps[-1][0]\n        net_prefixes.append(name)\n        module_prefixes.append(name)\n        net = model.steps[-1][1]\n    else:\n        net = model\n\n    yield \'__\'.join(net_prefixes), net\n\n    module = net.module\n    module_prefixes.append(\'module\')\n    yield \'__\'.join(module_prefixes), module\n\n\ndef _extract_estimator_cls(estimator):\n    if isinstance(estimator, partial):\n        # is partialled\n        return estimator.func\n    if not isinstance(estimator, type):\n        # is instance\n        return estimator.__class__\n    return estimator\n\n\ndef _yield_printable_params(param, prefix, defaults):\n    name, default, descr = param\n    name = name if not prefix else \'__\'.join((prefix, name))\n    default = _substitute_default(default, defaults.get(name))\n\n    printable = \'--{} : {}\'.format(name, default)\n    yield printable\n\n    for line in descr:\n        yield line\n\n\ndef _get_help_for_params(params, prefix=\'--\', defaults=None, indent=2):\n    defaults = defaults or {}\n    for param in params:\n        first, *rest = tuple(_yield_printable_params(\n            param, prefix=prefix, defaults=defaults))\n        yield "" "" * indent + first\n        for line in rest:\n            yield "" "" * 2 * indent + line\n\n\ndef _get_help_for_estimator(prefix, estimator, defaults=None):\n    """"""Yield help lines for the given estimator and prefix.""""""\n    from numpydoc.docscrape import ClassDoc\n\n    defaults = defaults or {}\n    estimator = _extract_estimator_cls(estimator)\n    yield ""<{}> options:"".format(estimator.__name__)\n\n    doc = ClassDoc(estimator)\n    yield from _get_help_for_params(\n        doc[\'Parameters\'],\n        prefix=prefix,\n        defaults=defaults,\n    )\n    yield \'\'  # add a newline line between estimators\n\n\ndef print_help(model, defaults=None):\n    """"""Print help for the command line arguments of the given model.\n\n    Parameters\n    ----------\n    model : sklearn.base.BaseEstimator\n      The basic model, e.g. a ``NeuralNet`` or sklearn ``Pipeline``.\n\n    defautls : dict or None (default=None)\n      Optionally, change the default values to use custom\n      defaults. Commandline arguments have precedence over defaults.\n\n    """"""\n    defaults = defaults or {}\n\n    print(""This is the help for the model-specific parameters."")\n    print(""To invoke help for the remaining options, run:"")\n    print(""python {} -- --help"".format(sys.argv[0]))\n    print()\n\n    lines = (_get_help_for_estimator(prefix, estimator, defaults=defaults) for\n             prefix, estimator in _yield_estimators(model))\n    print(\'\\n\'.join(chain(*lines)))\n\n\ndef parse_args(kwargs, defaults=None):\n    """"""Apply command line arguments or show help.\n\n    Use this in conjunction with the fire library to quickly build\n    command line interfaces for your scripts.\n\n    This function returns another function that must be called with\n    the estimator (e.g. ``NeuralNet``) to apply the parsed command\n    line arguments. If the --help option is found, show the\n    estimator-specific help instead.\n\n    Examples\n    --------\n    Content of my_script.py:\n\n    >>> def main(**kwargs):\n    >>>     X, y = get_data()\n    >>>     my_model = get_model()\n    >>>     parsed = parse_args(kwargs)\n    >>>     my_model = parsed(my_model)\n    >>>     my_model.fit(X, y)\n    >>>\n    >>> if __name__ == \'__main__\':\n    >>>     fire.Fire(main)\n\n    Parameters\n    ----------\n    kwargs : dict\n      The arguments as parsed by fire.\n\n    defautls : dict or None (default=None)\n      Optionally, change the default values to use custom\n      defaults. Commandline arguments have precedence over defaults.\n\n    Returns\n    -------\n    print_help_and_exit : callable\n      If --help is in the arguments, print help and exit.\n\n    set_params : callable\n      If --help is not in the options, apply command line arguments to\n      the estimator and return it.\n\n    """"""\n    try:\n        import fire  # pylint: disable=unused-import\n    except ImportError:\n        raise ImportError(""Using skorch cli helpers requires the fire library,""\n                          "" you can install it with pip: pip install fire."")\n    try:\n        import numpydoc.docscrape  # pylint: disable=unused-import\n    except ImportError:\n        raise ImportError(""Using skorch cli helpers requires the numpydoc library,""\n                          "" you can install it with pip: pip install numpydoc."")\n\n    defaults = defaults or {}\n\n    def print_help_and_exit(estimator):\n        print_help(estimator, defaults=defaults)\n        sys.exit()\n\n    def set_params(estimator):\n        estimator.set_params(**defaults)\n        return estimator.set_params(**parse_net_kwargs(kwargs))\n\n    if kwargs.get(\'help\'):\n        return print_help_and_exit\n    return set_params\n'"
skorch/dataset.py,12,"b'""""""Contains custom skorch Dataset and CVSplit.""""""\nimport warnings\nfrom functools import partial\nfrom numbers import Number\n\nimport numpy as np\nfrom scipy import sparse\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import check_cv\nimport torch\nimport torch.utils.data\n\nfrom skorch.utils import flatten\nfrom skorch.utils import is_pandas_ndframe\nfrom skorch.utils import check_indexing\nfrom skorch.utils import multi_indexing\nfrom skorch.utils import to_numpy\n\n\nERROR_MSG_1_ITEM = (\n    ""You are using a non-skorch dataset that returns 1 value. ""\n    ""Remember that for skorch, Dataset.__getitem__ must return exactly ""\n    ""2 values, X and y (more info: ""\n    ""https://skorch.readthedocs.io/en/stable/user/dataset.html)."")\n\n\nERROR_MSG_MORE_THAN_2_ITEMS = (\n    ""You are using a non-skorch dataset that returns {} values. ""\n    ""Remember that for skorch, Dataset.__getitem__ must return exactly ""\n    ""2 values, X and y (more info: ""\n    ""https://skorch.readthedocs.io/en/stable/user/dataset.html)."")\n\n\ndef _apply_to_data(data, func, unpack_dict=False):\n    """"""Apply a function to data, trying to unpack different data\n    types.\n\n    """"""\n    apply_ = partial(_apply_to_data, func=func, unpack_dict=unpack_dict)\n\n    if isinstance(data, dict):\n        if unpack_dict:\n            return [apply_(v) for v in data.values()]\n        return {k: apply_(v) for k, v in data.items()}\n\n    if isinstance(data, (list, tuple)):\n        try:\n            # e.g.list/tuple of arrays\n            return [apply_(x) for x in data]\n        except TypeError:\n            return func(data)\n\n    return func(data)\n\n\ndef _is_sparse(x):\n    try:\n        return sparse.issparse(x) or x.is_sparse\n    except AttributeError:\n        return False\n\n\ndef _len(x):\n    if _is_sparse(x):\n        return x.shape[0]\n    return len(x)\n\n\ndef get_len(data):\n    lens = [_apply_to_data(data, _len, unpack_dict=True)]\n    lens = list(flatten(lens))\n    len_set = set(lens)\n    if len(len_set) != 1:\n        raise ValueError(""Dataset does not have consistent lengths."")\n    return list(len_set)[0]\n\n\ndef uses_placeholder_y(ds):\n    """"""If ``ds`` is a ``skorch.dataset.Dataset`` or a\n    ``skorch.dataset.Dataset`` nested inside a\n    ``torch.utils.data.Subset`` and uses\n    y as a placeholder, return ``True``.""""""\n\n    if isinstance(ds, torch.utils.data.Subset):\n        return uses_placeholder_y(ds.dataset)\n    return isinstance(ds, Dataset) and hasattr(ds, ""y"") and ds.y is None\n\n\ndef unpack_data(data):\n    """"""Unpack data returned by the net\'s iterator into a 2-tuple.\n\n    If the wrong number of items is returned, raise a helpful error\n    message.\n\n    """"""\n    # Note: This function cannot detect it when a user only returns 1\n    # item that is exactly of length 2 (e.g. because the batch size is\n    # 2). In that case, the item will be erroneously split into X and\n    # y.\n    try:\n        X, y = data\n        return X, y\n    except ValueError:\n        # if a 1-tuple/list or something else like a torch tensor\n        if not isinstance(data, (tuple, list)) or len(data) < 2:\n            raise ValueError(ERROR_MSG_1_ITEM)\n        raise ValueError(ERROR_MSG_MORE_THAN_2_ITEMS.format(len(data)))\n\n\nclass Dataset(torch.utils.data.Dataset):\n    # pylint: disable=anomalous-backslash-in-string\n    """"""General dataset wrapper that can be used in conjunction with\n    PyTorch :class:`~torch.utils.data.DataLoader`.\n\n    The dataset will always yield a tuple of two values, the first\n    from the data (``X``) and the second from the target (``y``).\n    However, the target is allowed to be ``None``. In that case,\n    :class:`.Dataset` will currently return a dummy tensor, since\n    :class:`~torch.utils.data.DataLoader` does not work with\n    ``None``\\s.\n\n    :class:`.Dataset` currently works with the following data types:\n\n    * numpy ``array``\\s\n    * PyTorch :class:`~torch.Tensor`\\s\n    * scipy sparse CSR matrices\n    * pandas NDFrame\n    * a dictionary of the former three\n    * a list/tuple of the former three\n\n    Parameters\n    ----------\n    X : see above\n      Everything pertaining to the input data.\n\n    y : see above or None (default=None)\n      Everything pertaining to the target, if there is anything.\n\n    length : int or None (default=None)\n      If not ``None``, determines the length (``len``) of the data.\n      Should usually be left at ``None``, in which case the length is\n      determined by the data itself.\n\n    """"""\n    def __init__(\n            self,\n            X,\n            y=None,\n            length=None,\n    ):\n        self.X = X\n        self.y = y\n\n        self.X_indexing = check_indexing(X)\n        self.y_indexing = check_indexing(y)\n        self.X_is_ndframe = is_pandas_ndframe(X)\n\n        if length is not None:\n            self._len = length\n            return\n\n        # pylint: disable=invalid-name\n        len_X = get_len(X)\n        if y is not None:\n            len_y = get_len(y)\n            if len_y != len_X:\n                raise ValueError(""X and y have inconsistent lengths."")\n        self._len = len_X\n\n    def __len__(self):\n        return self._len\n\n    def transform(self, X, y):\n        # pylint: disable=anomalous-backslash-in-string\n        """"""Additional transformations on ``X`` and ``y``.\n\n        By default, they are cast to PyTorch :class:`~torch.Tensor`\\s.\n        Override this if you want a different behavior.\n\n        Note: If you use this in conjuction with PyTorch\n        :class:`~torch.utils.data.DataLoader`, the latter will call\n        the dataset for each row separately, which means that the\n        incoming ``X`` and ``y`` each are single rows.\n\n        """"""\n        # pytorch DataLoader cannot deal with None so we use 0 as a\n        # placeholder value. We only return a Tensor with one value\n        # (as opposed to ``batchsz`` values) since the pytorch\n        # DataLoader calls __getitem__ for each row in the batch\n        # anyway, which results in a dummy ``y`` value for each row in\n        # the batch.\n        y = torch.Tensor([0]) if y is None else y\n\n        # pytorch cannot convert sparse matrices, for now just make it\n        # dense; squeeze because X[i].shape is (1, n) for csr matrices\n        if sparse.issparse(X):\n            X = X.toarray().squeeze(0)\n        return X, y\n\n    def __getitem__(self, i):\n        X, y = self.X, self.y\n        if self.X_is_ndframe:\n            X = {k: X[k].values.reshape(-1, 1) for k in X}\n\n        Xi = multi_indexing(X, i, self.X_indexing)\n        yi = multi_indexing(y, i, self.y_indexing)\n        return self.transform(Xi, yi)\n\n\nclass CVSplit:\n    """"""Class that performs the internal train/valid split on a dataset.\n\n    The ``cv`` argument here works similarly to the regular sklearn ``cv``\n    parameter in, e.g., ``GridSearchCV``. However, instead of cycling\n    through all splits, only one fixed split (the first one) is\n    used. To get a full cycle through the splits, don\'t use\n    ``NeuralNet``\'s internal validation but instead the corresponding\n    sklearn functions (e.g. ``cross_val_score``).\n\n    We additionally support a float, similar to sklearn\'s\n    ``train_test_split``.\n\n    Parameters\n    ----------\n    cv : int, float, cross-validation generator or an iterable, optional\n      (Refer sklearn\'s User Guide for cross_validation for the various\n      cross-validation strategies that can be used here.)\n\n      Determines the cross-validation splitting strategy.\n      Possible inputs for cv are:\n\n        - None, to use the default 3-fold cross validation,\n        - integer, to specify the number of folds in a ``(Stratified)KFold``,\n        - float, to represent the proportion of the dataset to include\n          in the validation split.\n        - An object to be used as a cross-validation generator.\n        - An iterable yielding train, validation splits.\n\n    stratified : bool (default=False)\n      Whether the split should be stratified. Only works if ``y`` is\n      either binary or multiclass classification.\n\n    random_state : int, RandomState instance, or None (default=None)\n      Control the random state in case that ``(Stratified)ShuffleSplit``\n      is used (which is when a float is passed to ``cv``). For more\n      information, look at the sklearn documentation of\n      ``(Stratified)ShuffleSplit``.\n\n    """"""\n    def __init__(\n            self,\n            cv=5,\n            stratified=False,\n            random_state=None,\n    ):\n        self.stratified = stratified\n        self.random_state = random_state\n\n        if isinstance(cv, Number) and (cv <= 0):\n            raise ValueError(""Numbers less than 0 are not allowed for cv ""\n                             ""but CVSplit got {}"".format(cv))\n\n        if not self._is_float(cv) and random_state is not None:\n            # TODO: raise a ValueError instead of a warning\n            warnings.warn(\n                ""Setting a random_state has no effect since cv is not a float. ""\n                ""This will raise an error in a future. You should leave ""\n                ""random_state to its default (None), or set cv to a float value."",\n                FutureWarning\n            )\n\n        self.cv = cv\n\n    def _is_stratified(self, cv):\n        return isinstance(cv, (StratifiedKFold, StratifiedShuffleSplit))\n\n    def _is_float(self, x):\n        if not isinstance(x, Number):\n            return False\n        return not float(x).is_integer()\n\n    def _check_cv_float(self):\n        cv_cls = StratifiedShuffleSplit if self.stratified else ShuffleSplit\n        return cv_cls(test_size=self.cv, random_state=self.random_state)\n\n    def _check_cv_non_float(self, y):\n        return check_cv(\n            self.cv,\n            y=y,\n            classifier=self.stratified,\n        )\n\n    def check_cv(self, y):\n        """"""Resolve which cross validation strategy is used.""""""\n        y_arr = None\n        if self.stratified:\n            # Try to convert y to numpy for sklearn\'s check_cv; if conversion\n            # doesn\'t work, still try.\n            try:\n                y_arr = to_numpy(y)\n            except (AttributeError, TypeError):\n                y_arr = y\n\n        if self._is_float(self.cv):\n            return self._check_cv_float()\n        return self._check_cv_non_float(y_arr)\n\n    def _is_regular(self, x):\n        return (x is None) or isinstance(x, np.ndarray) or is_pandas_ndframe(x)\n\n    def __call__(self, dataset, y=None, groups=None):\n        bad_y_error = ValueError(\n            ""Stratified CV requires explicitly passing a suitable y."")\n        if (y is None) and self.stratified:\n            raise bad_y_error\n\n        cv = self.check_cv(y)\n        if self.stratified and not self._is_stratified(cv):\n            raise bad_y_error\n\n        # pylint: disable=invalid-name\n        len_dataset = get_len(dataset)\n        if y is not None:\n            len_y = get_len(y)\n            if len_dataset != len_y:\n                raise ValueError(""Cannot perform a CV split if dataset and y ""\n                                 ""have different lengths."")\n\n        args = (np.arange(len_dataset),)\n        if self._is_stratified(cv):\n            args = args + (to_numpy(y),)\n\n        idx_train, idx_valid = next(iter(cv.split(*args, groups=groups)))\n        dataset_train = torch.utils.data.Subset(dataset, idx_train)\n        dataset_valid = torch.utils.data.Subset(dataset, idx_valid)\n        return dataset_train, dataset_valid\n\n    def __repr__(self):\n        # pylint: disable=useless-super-delegation\n        return super(CVSplit, self).__repr__()\n'"
skorch/exceptions.py,0,"b'""""""Contains skorch-specific exceptions and warnings.""""""\n\n\nclass SkorchException(BaseException):\n    """"""Base skorch exception.""""""\n\n\nclass NotInitializedError(SkorchException):\n    """"""Module is not initialized, please call the ``.initialize``\n    method or train the model by calling ``.fit(...)``.\n\n    """"""\n\n\nclass SkorchWarning(UserWarning):\n    """"""Base skorch warning.""""""\n\n\nclass DeviceWarning(SkorchWarning):\n    """"""A problem with a device (e.g. CUDA) was detected.""""""\n'"
skorch/helper.py,6,"b'""""""Helper functions and classes for users.\n\nThey should not be used in skorch directly.\n\n""""""\nfrom collections import Sequence\nfrom collections import namedtuple\nfrom functools import partial\n\nimport numpy as np\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\nimport torch\n\nfrom skorch.cli import parse_args\nfrom skorch.utils import _make_split\nfrom skorch.utils import is_torch_data_type\nfrom skorch.utils import to_tensor\n\n\nclass SliceDict(dict):\n    """"""Wrapper for Python dict that makes it sliceable across values.\n\n    Use this if your input data is a dictionary and you have problems\n    with sklearn not being able to slice it. Wrap your dict with\n    SliceDict and it should usually work.\n\n    Note:\n\n    * SliceDict cannot be indexed by integers, if you want one row,\n      say row 3, use `[3:4]`.\n    * SliceDict accepts numpy arrays and torch tensors as values.\n\n    Examples\n    --------\n    >>> X = {\'key0\': val0, \'key1\': val1}\n    >>> search = GridSearchCV(net, params, ...)\n    >>> search.fit(X, y)  # raises error\n    >>> Xs = SliceDict(key0=val0, key1=val1)  # or Xs = SliceDict(**X)\n    >>> search.fit(Xs, y)  # works\n\n    """"""\n    def __init__(self, **kwargs):\n        lengths = [value.shape[0] for value in kwargs.values()]\n        lengths_set = set(lengths)\n        if lengths_set and (len(lengths_set) != 1):\n            raise ValueError(\n                ""Initialized with items of different lengths: {}""\n                """".format(\', \'.join(map(str, sorted(lengths_set)))))\n\n        if not lengths:\n            self._len = 0\n        else:\n            self._len = lengths[0]\n\n        super(SliceDict, self).__init__(**kwargs)\n\n    def __len__(self):\n        return self._len\n\n    def __getitem__(self, sl):\n        if isinstance(sl, int):\n            # Indexing with integers is not well-defined because that\n            # recudes the dimension of arrays by one, messing up\n            # lengths and shapes.\n            raise ValueError(""SliceDict cannot be indexed by integers."")\n        if isinstance(sl, str):\n            return super(SliceDict, self).__getitem__(sl)\n        return SliceDict(**{k: v[sl] for k, v in self.items()})\n\n    def __setitem__(self, key, value):\n        if not isinstance(key, str):\n            raise TypeError(""Key must be str, not {}."".format(type(key)))\n\n        length = value.shape[0]\n        if not self.keys():\n            self._len = length\n\n        if self._len != length:\n            raise ValueError(\n                ""Cannot set array with shape[0] != {}""\n                """".format(self._len))\n\n        super(SliceDict, self).__setitem__(key, value)\n\n    def update(self, kwargs):\n        for key, value in kwargs.items():\n            self.__setitem__(key, value)\n\n    def __repr__(self):\n        out = super(SliceDict, self).__repr__()\n        return ""SliceDict(**{})"".format(out)\n\n    @property\n    def shape(self):\n        return (self._len,)\n\n    def copy(self):\n        return type(self)(**self)\n\n    def fromkeys(self, *args, **kwargs):\n        """"""fromkeys method makes no sense with SliceDict and is thus not\n        supported.""""""\n        raise TypeError(""SliceDict does not support fromkeys."")\n\n    def __eq__(self, other):\n        if self.keys() != other.keys():\n            return False\n\n        for key, val in self.items():\n            val_other = other[key]\n\n            # torch tensors\n            if is_torch_data_type(val):\n                if not is_torch_data_type(val_other):\n                    return False\n                if not (val == val_other).all():\n                    return False\n                continue\n\n            # numpy arrays\n            if isinstance(val, np.ndarray):\n                if not isinstance(val_other, np.ndarray):\n                    return False\n                if not (val == val_other).all():\n                    return False\n                continue\n\n            # rest\n            if val != val_other:\n                return False\n\n        return True\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n\n# This class must be an instance of Sequence and have an ndim\n# attribute because sklearn will test this.\nclass SliceDataset(Sequence):\n    # pylint: disable=anomalous-backslash-in-string\n    """"""Helper class that wraps a torch dataset to make it work with\n    sklearn.\n\n    Sometimes, sklearn will touch the input data, e.g. when splitting\n    the data for a grid search. This will fail when the input data is\n    a torch dataset. To prevent this, use this wrapper class for your\n    dataset.\n\n    Note: This class will only return the X value by default (i.e. the\n    first value returned by indexing the original dataset). Sklearn,\n    and hence skorch, always require 2 values, X and y. Therefore, you\n    still need to provide the y data separately.\n\n    Note: This class behaves similarly to a PyTorch\n    :class:`~torch.utils.data.Subset` when it is indexed by a slice or\n    numpy array: It will return another ``SliceDataset`` that\n    references the subset instead of the actual values. Only when it\n    is indexed by an int does it return the actual values. The reason\n    for this is to avoid loading all data into memory when sklearn,\n    for instance, creates a train/validation split on the\n    dataset. Data will only be loaded in batches during the fit loop.\n\n    Examples\n    --------\n    >>> X = MyCustomDataset()\n    >>> search = GridSearchCV(net, params, ...)\n    >>> search.fit(X, y)  # raises error\n    >>> ds = SliceDataset(X)\n    >>> search.fit(ds, y)  # works\n\n    Parameters\n    ----------\n    dataset : torch.utils.data.Dataset\n      A valid torch dataset.\n\n    idx : int (default=0)\n      Indicates which element of the dataset should be\n      returned. Typically, the dataset returns both X and y\n      values. SliceDataset can only return 1 value. If you want to\n      get X, choose idx=0 (default), if you want y, choose idx=1.\n\n    indices : list, np.ndarray, or None (default=None)\n      If you only want to return a subset of the dataset, indicate\n      which subset that is by passing this argument. Typically, this\n      can be left to be None, which returns all the data. See also\n      :class:`~torch.utils.data.Subset`.\n\n    """"""\n    def __init__(self, dataset, idx=0, indices=None):\n        self.dataset = dataset\n        self.idx = idx\n        self.indices = indices\n\n        self.indices_ = (self.indices if self.indices is not None\n                         else np.arange(len(self.dataset)))\n        self.ndim = 1\n\n    def __len__(self):\n        return len(self.indices_)\n\n    @property\n    def shape(self):\n        return (len(self),)\n\n    def transform(self, data):\n        """"""Additional transformations on ``data``.\n\n        Note: If you use this in conjuction with PyTorch\n        :class:`~torch.utils.data.DataLoader`, the latter will call\n        the dataset for each row separately, which means that the\n        incoming ``data`` is a single rows.\n\n        """"""\n        return data\n\n    def _select_item(self, Xn):\n        # Raise a custom error message when accessing out of\n        # bounds. However, this will only trigger as soon as this is\n        # indexed by an integer.\n        try:\n            return Xn[self.idx]\n        except IndexError:\n            name = self.__class__.__name__\n            msg = (""{} is trying to access element {} but there are only ""\n                   ""{} elements."".format(name, self.idx, len(Xn)))\n            raise IndexError(msg)\n\n    def __getitem__(self, i):\n        if isinstance(i, (int, np.integer)):\n            Xn = self.dataset[self.indices_[i]]\n            Xi = self._select_item(Xn)\n            return self.transform(Xi)\n\n        if isinstance(i, slice):\n            return SliceDataset(self.dataset, idx=self.idx, indices=self.indices_[i])\n\n        if isinstance(i, np.ndarray):\n            if i.ndim != 1:\n                raise IndexError(""SliceDataset only supports slicing with 1 ""\n                                 ""dimensional arrays, got {} dimensions instead.""\n                                 """".format(i.ndim))\n            if i.dtype == np.bool:\n                i = np.flatnonzero(i)\n\n        return SliceDataset(self.dataset, idx=self.idx, indices=self.indices_[i])\n\n\ndef predefined_split(dataset):\n    """"""Uses ``dataset`` for validiation in :class:`.NeuralNet`.\n\n    Examples\n    --------\n    >>> valid_ds = skorch.Dataset(X, y)\n    >>> net = NeuralNet(..., train_split=predefined_split(valid_ds))\n\n    Parameters\n    ----------\n    dataset: torch Dataset\n       Validiation dataset\n\n    """"""\n    return partial(_make_split, valid_ds=dataset)\n\n\nclass DataFrameTransformer(BaseEstimator, TransformerMixin):\n    """"""Transform a DataFrame into a dict useful for working with skorch.\n\n    Transforms cardinal data to floats and categorical data to vectors\n    of ints so that they can be embedded.\n\n    Although skorch can deal with pandas DataFrames, the default\n    behavior is often not very useful. Use this transformer to\n    transform the DataFrame into a dict with all float columns\n    concatenated using the key ""X"" and all categorical values encoded\n    as integers, using their respective column names as keys.\n\n    Your module must have a matching signature for this to work. It\n    must accept an argument ``X`` for all cardinal\n    values. Additionally, for all categorical values, it must accept\n    an argument with the same name as the corresponding column (see\n    example below). If you need help with the required signature, use\n    the ``describe_signature`` method of this class and pass it your\n    data.\n\n    You can choose whether you want to treat int columns the same as\n    float columns (default) or as categorical values.\n\n    To one-hot encode categorical features, initialize their\n    corresponding embedding layers using the identity matrix.\n\n    Examples\n    --------\n    >>> df = pd.DataFrame({\n    ...     \'col_floats\': np.linspace(0, 1, 12),\n    ...     \'col_ints\': [11, 11, 10] * 4,\n    ...     \'col_cats\': [\'a\', \'b\', \'a\'] * 4,\n    ... })\n    >>> # cast to category dtype to later learn embeddings\n    >>> df[\'col_cats\'] = df[\'col_cats\'].astype(\'category\')\n    >>> y = np.asarray([0, 1, 0] * 4)\n\n    >>> class MyModule(nn.Module):\n    ...     def __init__(self):\n    ...         super().__init__()\n    ...         self.reset_params()\n\n    >>>     def reset_params(self):\n    ...         self.embedding = nn.Embedding(2, 10)\n    ...         self.linear = nn.Linear(2, 10)\n    ...         self.out = nn.Linear(20, 2)\n    ...         self.nonlin = nn.Softmax(dim=-1)\n\n    >>>     def forward(self, X, col_cats):\n    ...         # ""X"" contains the values from col_floats and col_ints\n    ...         # ""col_cats"" contains the values from ""col_cats""\n    ...         X_lin = self.linear(X)\n    ...         X_cat = self.embedding(col_cats)\n    ...         X_concat = torch.cat((X_lin, X_cat), dim=1)\n    ...         return self.nonlin(self.out(X_concat))\n\n    >>> net = NeuralNetClassifier(MyModule)\n    >>> pipe = Pipeline([\n    ...     (\'transform\', DataFrameTransformer()),\n    ...     (\'net\', net),\n    ... ])\n    >>> pipe.fit(df, y)\n\n    Parameters\n    ----------\n    treat_int_as_categorical : bool (default=False)\n      Whether to treat integers as categorical values or as cardinal\n      values, i.e. the same as floats.\n\n    float_dtype : numpy dtype or None (default=np.float32)\n      The dtype to cast the cardinal values to. If None, don\'t change\n      them.\n\n    int_dtype : numpy dtype or None (default=np.int64)\n      The dtype to cast the categorical values to. If None, don\'t\n      change them. If you do this, it can happen that the categorical\n      values will have different dtypes, reflecting the number of\n      unique categories.\n\n    Notes\n    -----\n    The value of X will always be 2-dimensional, even if it only\n    contains 1 column.\n\n    """"""\n    import pandas as pd\n\n    def __init__(\n            self,\n            treat_int_as_categorical=False,\n            float_dtype=np.float32,\n            int_dtype=np.int64,\n    ):\n        self.treat_int_as_categorical = treat_int_as_categorical\n        self.float_dtype = float_dtype\n        self.int_dtype = int_dtype\n\n    def _check_dtypes(self, df):\n        """"""Perform a check on the DataFrame to detect wrong dtypes or keys.\n\n        Makes sure that there are no conflicts in key names.\n\n        If dtypes are found that cannot be dealt with, raises a\n        TypeError with a message indicating which ones caused trouble.\n\n        Raises\n        ------\n        ValueError\n          If there already is a column named \'X\'.\n\n        TypeError\n          If a wrong dtype is found.\n\n        """"""\n        if \'X\' in df:\n            raise ValueError(\n                ""DataFrame contains a column named \'X\', which clashes ""\n                ""with the name chosen for cardinal features; consider ""\n                ""renaming that column."")\n\n        wrong_dtypes = []\n\n        for col, dtype in zip(df, df.dtypes):\n            if isinstance(dtype, self.pd.api.types.CategoricalDtype):\n                continue\n            if np.issubdtype(dtype, np.integer):\n                continue\n            if np.issubdtype(dtype, np.floating):\n                continue\n            wrong_dtypes.append((col, dtype))\n\n        if not wrong_dtypes:\n            return\n\n        wrong_dtypes = sorted(wrong_dtypes, key=lambda tup: tup[0])\n        msg_dtypes = "", "".join(\n            ""{} ({})"".format(col, dtype) for col, dtype in wrong_dtypes)\n        msg = (""The following columns have dtypes that cannot be ""\n               ""interpreted as numerical dtypes: {}"".format(msg_dtypes))\n        raise TypeError(msg)\n\n    # pylint: disable=unused-argument\n    def fit(self, df, y=None, **fit_params):\n        self._check_dtypes(df)\n        return self\n\n    def transform(self, df):\n        """"""Transform DataFrame to become a dict that works well with skorch.\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n          Incoming DataFrame.\n\n        Returns\n        -------\n        X_dict: dict\n          Dictionary with all floats concatenated using the key ""X""\n          and all categorical values encoded as integers, using their\n          respective column names as keys.\n\n        """"""\n        self._check_dtypes(df)\n\n        X_dict = {}\n        Xf = []  # floats\n\n        for col, dtype in zip(df, df.dtypes):\n            X_col = df[col]\n\n            if isinstance(dtype, self.pd.api.types.CategoricalDtype):\n                x = X_col.cat.codes.values\n                if self.int_dtype is not None:\n                    x = x.astype(self.int_dtype)\n                X_dict[col] = x\n                continue\n\n            if (\n                    np.issubdtype(dtype, np.integer)\n                    and self.treat_int_as_categorical\n            ):\n                x = X_col.astype(\'category\').cat.codes.values\n                if self.int_dtype is not None:\n                    x = x.astype(self.int_dtype)\n                X_dict[col] = x\n                continue\n\n            Xf.append(X_col.values)\n\n        if not Xf:\n            return X_dict\n\n        X = np.stack(Xf, axis=1)\n        if self.float_dtype is not None:\n            X = X.astype(self.float_dtype)\n        X_dict[\'X\'] = X\n        return X_dict\n\n    def describe_signature(self, df):\n        """"""Describe the signature required for the given data.\n\n        Pass the DataFrame to receive a description of the signature\n        required for the module\'s forward method. The description\n        consists of three parts:\n\n        1. The names of the arguments that the forward method\n        needs.\n        2. The dtypes of the torch tensors passed to forward.\n        3. The number of input units that are required for the\n        corresponding argument. For the float parameter, this is just\n        the number of dimensions of the tensor. For categorical\n        parameters, it is the number of unique elements.\n\n        Returns\n        -------\n        signature : dict\n          Returns a dict with each key corresponding to one key\n          required for the forward method. The values are dictionaries\n          of two elements. The key ""dtype"" describes the torch dtype\n          of the resulting tensor, the key ""input_units"" describes the\n          required number of input units.\n\n        """"""\n        X_dict = self.fit_transform(df)\n        signature = {}\n\n        X = X_dict.get(\'X\')\n        if X is not None:\n            signature[\'X\'] = dict(\n                dtype=to_tensor(X, device=\'cpu\').dtype,\n                input_units=X.shape[1],\n            )\n\n        for key, val in X_dict.items():\n            if key == \'X\':\n                continue\n\n            tensor = to_tensor(val, device=\'cpu\')\n            nunique = len(torch.unique(tensor))\n            signature[key] = dict(\n                dtype=tensor.dtype,\n                input_units=nunique,\n            )\n\n        return signature\n'"
skorch/history.py,0,"b'""""""Contains history class and helper functions.""""""\n\nimport json\n\nfrom skorch.utils import open_file_like\n\n\n# pylint: disable=invalid-name\nclass _none:\n    """"""Special placeholder since ``None`` is a valid value.""""""\n\n\ndef _not_none(items):\n    """"""Whether the item is a placeholder or contains a placeholder.""""""\n    if not isinstance(items, (tuple, list)):\n        items = (items,)\n    return all(item is not _none for item in items)\n\n\ndef _filter_none(items):\n    """"""Filter special placeholder value, preserves sequence type.""""""\n    type_ = list if isinstance(items, list) else tuple\n    return type_(filter(_not_none, items))\n\n\ndef _getitem(item, i):\n    """"""Extract value or values from dicts.\n\n    Covers the case of a single key or multiple keys. If not found,\n    return placeholders instead.\n\n    """"""\n    if not isinstance(i, (tuple, list)):\n        return item.get(i, _none)\n    type_ = list if isinstance(item, list) else tuple\n    return type_(item.get(j, _none) for j in i)\n\n\ndef _unpack_index(i):\n    """"""Unpack index and return exactly four elements.\n\n    If index is more shallow than 4, return None for trailing\n    dimensions. If index is deeper than 4, raise a KeyError.\n\n    """"""\n    if len(i) > 4:\n        raise KeyError(\n            ""Tried to index history with {} indices but only ""\n            ""4 indices are possible."".format(len(i)))\n\n    # fill trailing indices with None\n    i_e, k_e, i_b, k_b = i + tuple([None] * (4 - len(i)))\n\n    return i_e, k_e, i_b, k_b\n\n\nclass History(list):\n    """"""History contains the information about the training history of\n    a :class:`.NeuralNet`, facilitating some of the more common tasks\n    that are occur during training.\n\n    When you want to log certain information during training (say, a\n    particular score or the norm of the gradients), you should write\n    them to the net\'s history object.\n\n    It is basically a list of dicts for each epoch, that, again,\n    contains a list of dicts for each batch. For convenience, it has\n    enhanced slicing notation and some methods to write new items.\n\n    To access items from history, you may pass a tuple of up to four\n    items:\n\n      1. Slices along the epochs.\n      2. Selects columns from history epochs, may be a single one or a\n         tuple of column names.\n      3. Slices along the batches.\n      4. Selects columns from history batchs, may be a single one or a\n         tuple of column names.\n\n    You may use a combination of the four items.\n\n    If you select columns that are not present in all epochs/batches,\n    only those epochs/batches are chosen that contain said columns. If\n    this set is empty, a ``KeyError`` is raised.\n\n    Examples\n    --------\n    >>> # ACCESSING ITEMS\n    >>> # history of a fitted neural net\n    >>> history = net.history\n    >>> # get current epoch, a dict\n    >>> history[-1]\n    >>> # get train losses from all epochs, a list of floats\n    >>> history[:, \'train_loss\']\n    >>> # get train and valid losses from all epochs, a list of tuples\n    >>> history[:, (\'train_loss\', \'valid_loss\')]\n    >>> # get current batches, a list of dicts\n    >>> history[-1, \'batches\']\n    >>> # get latest batch, a dict\n    >>> history[-1, \'batches\', -1]\n    >>> # get train losses from current batch, a list of floats\n    >>> history[-1, \'batches\', :, \'train_loss\']\n    >>> # get train and valid losses from current batch, a list of tuples\n    >>> history[-1, \'batches\', :, (\'train_loss\', \'valid_loss\')]\n\n    >>> # WRITING ITEMS\n    >>> # add new epoch row\n    >>> history.new_epoch()\n    >>> # add an entry to current epoch\n    >>> history.record(\'my-score\', 123)\n    >>> # add a batch row to the current epoch\n    >>> history.new_batch()\n    >>> # add an entry to the current batch\n    >>> history.record_batch(\'my-batch-score\', 456)\n    >>> # overwrite entry of current batch\n    >>> history.record_batch(\'my-batch-score\', 789)\n\n    """"""\n\n    def new_epoch(self):\n        """"""Register a new epoch row.""""""\n        self.append({\'batches\': []})\n\n    def new_batch(self):\n        """"""Register a new batch row for the current epoch.""""""\n        # pylint: disable=invalid-sequence-index\n        self[-1][\'batches\'].append({})\n\n    def record(self, attr, value):\n        """"""Add a new value to the given column for the current\n        epoch.\n\n        """"""\n        msg = ""Call new_epoch before recording for the first time.""\n        if not self:\n            raise ValueError(msg)\n        self[-1][attr] = value\n\n    def record_batch(self, attr, value):\n        """"""Add a new value to the given column for the current\n        batch.\n\n        """"""\n        # pylint: disable=invalid-sequence-index\n        self[-1][\'batches\'][-1][attr] = value\n\n    def to_list(self):\n        """"""Return history object as a list.""""""\n        return list(self)\n\n    @classmethod\n    def from_file(cls, f):\n        """"""Load the history of a ``NeuralNet`` from a json file.\n\n        Parameters\n        ----------\n        f : file-like object or str\n\n        """"""\n\n        with open_file_like(f, \'r\') as fp:\n            return cls(json.load(fp))\n\n    def to_file(self, f):\n        """"""Saves the history as a json file. In order to use this feature,\n        the history must only contain JSON encodable Python data structures.\n        Numpy and PyTorch types should not be in the history.\n\n        Parameters\n        ----------\n        f : file-like object or str\n\n        """"""\n        with open_file_like(f, \'w\') as fp:\n            json.dump(self.to_list(), fp)\n\n    def __getitem__(self, i):\n        # This implementation resolves indexing backwards,\n        # i.e. starting from the batches, then progressing to the\n        # epochs.\n        if isinstance(i, (int, slice)):\n            i = (i,)\n\n        # i_e: index epoch, k_e: key epoch\n        # i_b: index batch, k_b: key batch\n        i_e, k_e, i_b, k_b = _unpack_index(i)\n        keyerror_msg = ""Key \'{}\' was not found in history.""\n\n        if i_b is not None and k_e != \'batches\':\n            raise KeyError(""History indexing beyond the 2nd level is ""\n                           ""only possible if key \'batches\' is used, ""\n                           ""found key \'{}\'."".format(k_e))\n\n        items = self.to_list()\n\n        # extract indices of batches\n        # handles: history[..., k_e, i_b]\n        if i_b is not None:\n            items = [row[k_e][i_b] for row in items]\n\n        # extract keys of batches\n        # handles: history[..., k_e, i_b][k_b]\n        if k_b is not None:\n            items = [\n                _filter_none([_getitem(b, k_b) for b in batches])\n                if isinstance(batches, (list, tuple))\n                else _getitem(batches, k_b)\n                for batches in items\n            ]\n            # get rid of empty batches\n            items = [b for b in items if b not in (_none, [], ())]\n            if not _filter_none(items):\n                # all rows contained _none or were empty\n                raise KeyError(keyerror_msg.format(k_b))\n\n        # extract epoch-level values, but only if not already done\n        # handles: history[..., k_e]\n        if (k_e is not None) and (i_b is None):\n            items = [_getitem(batches, k_e)\n                     for batches in items]\n            if not _filter_none(items):\n                raise KeyError(keyerror_msg.format(k_e))\n\n        # extract the epochs\n        # handles: history[i_b, ..., ..., ...]\n        if i_e is not None:\n            items = items[i_e]\n            if isinstance(i_e, slice):\n                items = _filter_none(items)\n            if items is _none:\n                raise KeyError(keyerror_msg.format(k_e))\n\n        return items\n'"
skorch/net.py,26,"b'""""""Neural net classes.""""""\n\nimport fnmatch\nfrom itertools import chain\nfrom collections import OrderedDict\nimport tempfile\nimport warnings\n\nimport numpy as np\nfrom sklearn.base import BaseEstimator\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom skorch.callbacks import EpochTimer\nfrom skorch.callbacks import PrintLog\nfrom skorch.callbacks import PassthroughScoring\nfrom skorch.dataset import Dataset\nfrom skorch.dataset import CVSplit\nfrom skorch.dataset import get_len\nfrom skorch.dataset import unpack_data\nfrom skorch.dataset import uses_placeholder_y\nfrom skorch.exceptions import DeviceWarning\nfrom skorch.history import History\nfrom skorch.setter import optimizer_setter\nfrom skorch.utils import FirstStepAccumulator\nfrom skorch.utils import TeeGenerator\nfrom skorch.utils import check_is_fitted\nfrom skorch.utils import duplicate_items\nfrom skorch.utils import get_map_location\nfrom skorch.utils import is_dataset\nfrom skorch.utils import params_for\nfrom skorch.utils import to_device\nfrom skorch.utils import to_numpy\nfrom skorch.utils import to_tensor\n\n\n_PYTORCH_COMPONENTS = {\'criterion\', \'module\', \'optimizer\'}\n""""""Special names that mark pytorch components.\n\nThese special names are used to recognize whether an attribute that is\nbeing set in the net should be added to prefixes_ and\ncuda_dependent_attributes_\n\n""""""\n\n\n# pylint: disable=too-many-instance-attributes\nclass NeuralNet:\n    # pylint: disable=anomalous-backslash-in-string\n    """"""NeuralNet base class.\n\n    The base class covers more generic cases. Depending on your use\n    case, you might want to use :class:`.NeuralNetClassifier` or\n    :class:`.NeuralNetRegressor`.\n\n    In addition to the parameters listed below, there are parameters\n    with specific prefixes that are handled separately. To illustrate\n    this, here is an example:\n\n    >>> net = NeuralNet(\n    ...    ...,\n    ...    optimizer=torch.optimizer.SGD,\n    ...    optimizer__momentum=0.95,\n    ...)\n\n    This way, when ``optimizer`` is initialized, :class:`.NeuralNet`\n    will take care of setting the ``momentum`` parameter to 0.95.\n\n    (Note that the double underscore notation in\n    ``optimizer__momentum`` means that the parameter ``momentum``\n    should be set on the object ``optimizer``. This is the same\n    semantic as used by sklearn.)\n\n    Furthermore, this allows to change those parameters later:\n\n    ``net.set_params(optimizer__momentum=0.99)``\n\n    This can be useful when you want to change certain parameters\n    using a callback, when using the net in an sklearn grid search,\n    etc.\n\n    By default an :class:`.EpochTimer`, :class:`.BatchScoring` (for\n    both training and validation datasets), and :class:`.PrintLog`\n    callbacks are installed for the user\'s convenience.\n\n    Parameters\n    ----------\n    module : torch module (class or instance)\n      A PyTorch :class:`~torch.nn.Module`. In general, the\n      uninstantiated class should be passed, although instantiated\n      modules will also work.\n\n    criterion : torch criterion (class)\n      The uninitialized criterion (loss) used to optimize the\n      module.\n\n    optimizer : torch optim (class, default=torch.optim.SGD)\n      The uninitialized optimizer (update rule) used to optimize the\n      module\n\n    lr : float (default=0.01)\n      Learning rate passed to the optimizer. You may use ``lr`` instead\n      of using ``optimizer__lr``, which would result in the same outcome.\n\n    max_epochs : int (default=10)\n      The number of epochs to train for each ``fit`` call. Note that you\n      may keyboard-interrupt training at any time.\n\n    batch_size : int (default=128)\n      Mini-batch size. Use this instead of setting\n      ``iterator_train__batch_size`` and ``iterator_test__batch_size``,\n      which would result in the same outcome. If ``batch_size`` is -1,\n      a single batch with all the data will be used during training\n      and validation.\n\n    iterator_train : torch DataLoader\n      The default PyTorch :class:`~torch.utils.data.DataLoader` used for\n      training data.\n\n    iterator_valid : torch DataLoader\n      The default PyTorch :class:`~torch.utils.data.DataLoader` used for\n      validation and test data, i.e. during inference.\n\n    dataset : torch Dataset (default=skorch.dataset.Dataset)\n      The dataset is necessary for the incoming data to work with\n      pytorch\'s ``DataLoader``. It has to implement the ``__len__`` and\n      ``__getitem__`` methods. The provided dataset should be capable of\n      dealing with a lot of data types out of the box, so only change\n      this if your data is not supported. You should generally pass the\n      uninitialized ``Dataset`` class and define additional arguments to\n      X and y by prefixing them with ``dataset__``. It is also possible\n      to pass an initialzed ``Dataset``, in which case no additional\n      arguments may be passed.\n\n    train_split : None or callable (default=skorch.dataset.CVSplit(5))\n      If None, there is no train/validation split. Else, train_split\n      should be a function or callable that is called with X and y\n      data and should return the tuple ``dataset_train, dataset_valid``.\n      The validation data may be None.\n\n    callbacks : None or list of Callback instances (default=None)\n      More callbacks, in addition to those returned by\n      ``get_default_callbacks``. Each callback should inherit from\n      :class:`.Callback`. If not ``None``, a list of callbacks is\n      expected where the callback names are inferred from the class\n      name. Name conflicts are resolved by appending a count suffix\n      starting with 1, e.g. ``EpochScoring_1``. Alternatively,\n      a tuple ``(name, callback)`` can be passed, where ``name``\n      should be unique. Callbacks may or may not be instantiated.\n      The callback name can be used to set parameters on specific\n      callbacks (e.g., for the callback with name ``\'print_log\'``, use\n      ``net.set_params(callbacks__print_log__keys_ignored=[\'epoch\',\n      \'train_loss\'])``).\n\n    warm_start : bool (default=False)\n      Whether each fit call should lead to a re-initialization of the\n      module (cold start) or whether the module should be trained\n      further (warm start).\n\n    verbose : int (default=1)\n      Control the verbosity level.\n\n    device : str, torch.device (default=\'cpu\')\n      The compute device to be used. If set to \'cuda\', data in torch\n      tensors will be pushed to cuda tensors before being sent to the\n      module. If set to None, then all compute devices will be left\n      unmodified.\n\n    Attributes\n    ----------\n    prefixes_ : list of str\n      Contains the prefixes to special parameters. E.g., since there\n      is the ``\'module\'`` prefix, it is possible to set parameters like\n      so: ``NeuralNet(..., optimizer__momentum=0.95)``.\n\n    cuda_dependent_attributes_ : list of str\n      Contains a list of all attribute prefixes whose values depend on a\n      CUDA device. If a ``NeuralNet`` trained with a CUDA-enabled device\n      is unpickled on a machine without CUDA or with CUDA disabled, the\n      listed attributes are mapped to CPU.  Expand this list if you\n      want to add other cuda-dependent attributes.\n\n    initialized_ : bool\n      Whether the :class:`.NeuralNet` was initialized.\n\n    module_ : torch module (instance)\n      The instantiated module.\n\n    criterion_ : torch criterion (instance)\n      The instantiated criterion.\n\n    callbacks_ : list of tuples\n      The complete (i.e. default and other), initialized callbacks, in\n      a tuple with unique names.\n\n    """"""\n    prefixes_ = [\'module\', \'iterator_train\', \'iterator_valid\', \'optimizer\',\n                 \'criterion\', \'callbacks\', \'dataset\']\n\n    cuda_dependent_attributes_ = [\'module_\', \'optimizer_\', \'criterion_\']\n\n    # pylint: disable=too-many-arguments\n    def __init__(\n            self,\n            module,\n            criterion,\n            optimizer=torch.optim.SGD,\n            lr=0.01,\n            max_epochs=10,\n            batch_size=128,\n            iterator_train=DataLoader,\n            iterator_valid=DataLoader,\n            dataset=Dataset,\n            train_split=CVSplit(5),\n            callbacks=None,\n            warm_start=False,\n            verbose=1,\n            device=\'cpu\',\n            **kwargs\n    ):\n        self.module = module\n        self.criterion = criterion\n        self.optimizer = optimizer\n        self.lr = lr\n        self.max_epochs = max_epochs\n        self.batch_size = batch_size\n        self.iterator_train = iterator_train\n        self.iterator_valid = iterator_valid\n        self.dataset = dataset\n        self.train_split = train_split\n        self.callbacks = callbacks\n        self.warm_start = warm_start\n        self.verbose = verbose\n        self.device = device\n\n        self._check_deprecated_params(**kwargs)\n        history = kwargs.pop(\'history\', None)\n        initialized = kwargs.pop(\'initialized_\', False)\n        virtual_params = kwargs.pop(\'virtual_params_\', dict())\n\n        kwargs = self._check_kwargs(kwargs)\n        vars(self).update(kwargs)\n\n        self.history_ = history\n        self.initialized_ = initialized\n        self.virtual_params_ = virtual_params\n\n    @property\n    def history(self):\n        return self.history_\n\n    @history.setter\n    def history(self, value):\n        self.history_ = value\n\n    @property\n    def _default_callbacks(self):\n        return [\n            (\'epoch_timer\', EpochTimer()),\n            (\'train_loss\', PassthroughScoring(\n                name=\'train_loss\',\n                on_train=True,\n            )),\n            (\'valid_loss\', PassthroughScoring(\n                name=\'valid_loss\',\n            )),\n            (\'print_log\', PrintLog()),\n        ]\n\n    def get_default_callbacks(self):\n        return self._default_callbacks\n\n    def notify(self, method_name, **cb_kwargs):\n        """"""Call the callback method specified in ``method_name`` with\n        parameters specified in ``cb_kwargs``.\n\n        Method names can be one of:\n        * on_train_begin\n        * on_train_end\n        * on_epoch_begin\n        * on_epoch_end\n        * on_batch_begin\n        * on_batch_end\n\n        """"""\n        getattr(self, method_name)(self, **cb_kwargs)\n        for _, cb in self.callbacks_:\n            getattr(cb, method_name)(self, **cb_kwargs)\n\n    # pylint: disable=unused-argument\n    def on_train_begin(self, net,\n                       X=None, y=None, **kwargs):\n        pass\n\n    # pylint: disable=unused-argument\n    def on_train_end(self, net,\n                     X=None, y=None, **kwargs):\n        pass\n\n    # pylint: disable=unused-argument\n    def on_epoch_begin(self, net,\n                       dataset_train=None, dataset_valid=None, **kwargs):\n        self.history.new_epoch()\n        self.history.record(\'epoch\', len(self.history))\n\n    # pylint: disable=unused-argument\n    def on_epoch_end(self, net,\n                     dataset_train=None, dataset_valid=None, **kwargs):\n        pass\n\n    # pylint: disable=unused-argument\n    def on_batch_begin(self, net,\n                       Xi=None, yi=None, training=False, **kwargs):\n        self.history.new_batch()\n\n    def on_batch_end(self, net,\n                     Xi=None, yi=None, training=False, **kwargs):\n        pass\n\n    def on_grad_computed(self, net, named_parameters,\n                         Xi=None, yi=None,\n                         training=False, **kwargs):\n        pass\n\n    def _yield_callbacks(self):\n        """"""Yield all callbacks set on this instance including\n        a set whether its name was set by the user.\n\n        Handles these cases:\n          * default and user callbacks\n          * callbacks with and without name\n          * initialized and uninitialized callbacks\n          * puts PrintLog(s) last\n\n        """"""\n        print_logs = []\n        for item in self.get_default_callbacks() + (self.callbacks or []):\n            if isinstance(item, (tuple, list)):\n                named_by_user = True\n                name, cb = item\n            else:\n                named_by_user = False\n                cb = item\n                if isinstance(cb, type):  # uninitialized:\n                    name = cb.__name__\n                else:\n                    name = cb.__class__.__name__\n            if isinstance(cb, PrintLog) or (cb == PrintLog):\n                print_logs.append((name, cb, named_by_user))\n            else:\n                yield name, cb, named_by_user\n        yield from print_logs\n\n    def _callbacks_grouped_by_name(self):\n        """"""Group callbacks by name and collect names set by the user.""""""\n        callbacks, names_set_by_user = OrderedDict(), set()\n        for name, cb, named_by_user in self._yield_callbacks():\n            if named_by_user:\n                names_set_by_user.add(name)\n            callbacks[name] = callbacks.get(name, []) + [cb]\n        return callbacks, names_set_by_user\n\n    def _uniquely_named_callbacks(self):\n        """"""Make sure that the returned dict of named callbacks is unique\n        w.r.t. to the callback name. User-defined names will not be\n        renamed on conflict, instead an exception will be raised. The\n        same goes for the event where renaming leads to a conflict.\n        """"""\n        grouped_cbs, names_set_by_user = self._callbacks_grouped_by_name()\n        for name, cbs in grouped_cbs.items():\n            if len(cbs) > 1 and name in names_set_by_user:\n                raise ValueError(""Found duplicate user-set callback name ""\n                                 ""\'{}\'. Use unique names to correct this.""\n                                 .format(name))\n\n            for i, cb in enumerate(cbs):\n                if len(cbs) > 1:\n                    unique_name = \'{}_{}\'.format(name, i+1)\n                    if unique_name in grouped_cbs:\n                        raise ValueError(""Assigning new callback name failed ""\n                                         ""since new name \'{}\' exists already.""\n                                         .format(unique_name))\n                else:\n                    unique_name = name\n                yield unique_name, cb\n\n    def initialize_callbacks(self):\n        """"""Initializes all callbacks and save the result in the\n        ``callbacks_`` attribute.\n\n        Both ``default_callbacks`` and ``callbacks`` are used (in that\n        order). Callbacks may either be initialized or not, and if\n        they don\'t have a name, the name is inferred from the class\n        name. The ``initialize`` method is called on all callbacks.\n\n        The final result will be a list of tuples, where each tuple\n        consists of a name and an initialized callback. If names are\n        not unique, a ValueError is raised.\n\n        """"""\n        callbacks_ = []\n\n        class Dummy:\n            # We cannot use None as dummy value since None is a\n            # legitimate value to be set.\n            pass\n\n        for name, cb in self._uniquely_named_callbacks():\n            # check if callback itself is changed\n            param_callback = getattr(self, \'callbacks__\' + name, Dummy)\n            if param_callback is not Dummy:  # callback itself was set\n                cb = param_callback\n\n            # below: check for callback params\n            # don\'t set a parameter for non-existing callback\n            params = self.get_params_for(\'callbacks__{}\'.format(name))\n            if (cb is None) and params:\n                raise ValueError(""Trying to set a parameter for callback {} ""\n                                 ""which does not exist."".format(name))\n            if cb is None:\n                continue\n\n            if isinstance(cb, type):  # uninitialized:\n                cb = cb(**params)\n            else:\n                cb.set_params(**params)\n            cb.initialize()\n            callbacks_.append((name, cb))\n\n        self.callbacks_ = callbacks_\n        return self\n\n    def initialize_criterion(self):\n        """"""Initializes the criterion.""""""\n        criterion_params = self.get_params_for(\'criterion\')\n        self.criterion_ = self.criterion(**criterion_params)\n        if isinstance(self.criterion_, torch.nn.Module):\n            self.criterion_ = to_device(self.criterion_, self.device)\n        return self\n\n    def _format_reinit_msg(self, name, kwargs=None, triggered_directly=True):\n        """"""Returns a message that informs about re-initializing a compoment.\n\n        Sometimes, the module or optimizer need to be\n        re-initialized. Not only should the user receive a message\n        about this but also should they be informed about what\n        parameters, if any, caused it.\n\n        """"""\n        msg = ""Re-initializing {}"".format(name)\n        if triggered_directly and kwargs:\n            msg += ("" because the following parameters were re-set: {}.""\n                    .format(\', \'.join(sorted(kwargs))))\n        else:\n            msg += "".""\n        return msg\n\n    def initialize_module(self):\n        """"""Initializes the module.\n\n        Note that if the module has learned parameters, those will be\n        reset.\n\n        """"""\n        kwargs = self.get_params_for(\'module\')\n        module = self.module\n        is_initialized = isinstance(module, torch.nn.Module)\n\n        if kwargs or not is_initialized:\n            if is_initialized:\n                module = type(module)\n\n            if (is_initialized or self.initialized_) and self.verbose:\n                msg = self._format_reinit_msg(""module"", kwargs)\n                print(msg)\n\n            module = module(**kwargs)\n\n        self.module_ = to_device(module, self.device)\n        return self\n\n    def _is_virtual_param(self, key):\n        return any(fnmatch.fnmatch(key, pat) for pat in self.virtual_params_)\n\n    def _virtual_setattr(self, param, val):\n        setattr(self, param, val)\n\n    def _register_virtual_param(self, param_patterns, fn=_virtual_setattr):\n        if not isinstance(param_patterns, list):\n            param_patterns = [param_patterns]\n        for pattern in param_patterns:\n            self.virtual_params_[pattern] = fn\n\n    def _apply_virtual_params(self, virtual_kwargs):\n        for pattern, fn in self.virtual_params_.items():\n            for key, val in virtual_kwargs.items():\n                if not fnmatch.fnmatch(key, pattern):\n                    continue\n                fn(self, key, val)\n\n    def initialize_virtual_params(self):\n        self.virtual_params_ = {}\n\n    def initialize_optimizer(self, triggered_directly=True):\n        """"""Initialize the model optimizer. If ``self.optimizer__lr``\n        is not set, use ``self.lr`` instead.\n\n        Parameters\n        ----------\n        triggered_directly : bool (default=True)\n          Only relevant when optimizer is re-initialized.\n          Initialization of the optimizer can be triggered directly\n          (e.g. when lr was changed) or indirectly (e.g. when the\n          module was re-initialized). If and only if the former\n          happens, the user should receive a message informing them\n          about the parameters that caused the re-initialization.\n\n        """"""\n        args, kwargs = self.get_params_for_optimizer(\n            \'optimizer\', self.module_.named_parameters())\n\n        if self.initialized_ and self.verbose:\n            msg = self._format_reinit_msg(\n                ""optimizer"", kwargs, triggered_directly=triggered_directly)\n            print(msg)\n\n        if \'lr\' not in kwargs:\n            kwargs[\'lr\'] = self.lr\n\n        self.optimizer_ = self.optimizer(*args, **kwargs)\n\n        self._register_virtual_param(\n            [\'optimizer__param_groups__*__*\', \'optimizer__*\', \'lr\'],\n            optimizer_setter,\n        )\n\n    def initialize_history(self):\n        """"""Initializes the history.""""""\n        self.history_ = History()\n\n    def initialize(self):\n        """"""Initializes all components of the :class:`.NeuralNet` and\n        returns self.\n\n        """"""\n        self.initialize_virtual_params()\n        self.initialize_callbacks()\n        self.initialize_criterion()\n        self.initialize_module()\n        self.initialize_optimizer()\n        self.initialize_history()\n\n        self.initialized_ = True\n        return self\n\n    def check_data(self, X, y=None):\n        pass\n\n    def validation_step(self, Xi, yi, **fit_params):\n        """"""Perform a forward step using batched data and return the\n        resulting loss.\n\n        The module is set to be in evaluation mode (e.g. dropout is\n        not applied).\n\n        Parameters\n        ----------\n        Xi : input data\n          A batch of the input data.\n\n        yi : target data\n          A batch of the target data.\n\n        **fit_params : dict\n          Additional parameters passed to the ``forward`` method of\n          the module and to the ``self.train_split`` call.\n\n        """"""\n        self.module_.eval()\n        with torch.no_grad():\n            y_pred = self.infer(Xi, **fit_params)\n            loss = self.get_loss(y_pred, yi, X=Xi, training=False)\n        return {\n            \'loss\': loss,\n            \'y_pred\': y_pred,\n            }\n\n    def train_step_single(self, Xi, yi, **fit_params):\n        """"""Compute y_pred, loss value, and update net\'s gradients.\n\n        The module is set to be in train mode (e.g. dropout is\n        applied).\n\n        Parameters\n        ----------\n        Xi : input data\n          A batch of the input data.\n\n        yi : target data\n          A batch of the target data.\n\n        **fit_params : dict\n          Additional parameters passed to the ``forward`` method of\n          the module and to the ``self.train_split`` call.\n\n        """"""\n        self.module_.train()\n        y_pred = self.infer(Xi, **fit_params)\n        loss = self.get_loss(y_pred, yi, X=Xi, training=True)\n        loss.backward()\n\n        self.notify(\n            \'on_grad_computed\',\n            named_parameters=TeeGenerator(self.module_.named_parameters()),\n            X=Xi,\n            y=yi\n        )\n\n        return {\n            \'loss\': loss,\n            \'y_pred\': y_pred,\n            }\n\n    def get_train_step_accumulator(self):\n        """"""Return the train step accumulator.\n\n        By default, the accumulator stores and retrieves the first\n        value from the optimizer call. Most optimizers make only one\n        call, so first value is at the same time the only value.\n\n        In case of some optimizers, e.g. LBFGS,\n        ``train_step_calc_gradient`` is called multiple times, as the\n        loss function is evaluated multiple times per optimizer\n        call. If you don\'t want to return the first value in that\n        case, override this method to return your custom accumulator.\n\n        """"""\n        return FirstStepAccumulator()\n\n    def train_step(self, Xi, yi, **fit_params):\n        """"""Prepares a loss function callable and pass it to the optimizer,\n        hence performing one optimization step.\n\n        Loss function callable as required by some optimizers (and accepted by\n        all of them):\n        https://pytorch.org/docs/master/optim.html#optimizer-step-closure\n\n        The module is set to be in train mode (e.g. dropout is\n        applied).\n\n        Parameters\n        ----------\n        Xi : input data\n          A batch of the input data.\n\n        yi : target data\n          A batch of the target data.\n\n        **fit_params : dict\n          Additional parameters passed to the ``forward`` method of\n          the module and to the train_split call.\n\n        """"""\n        step_accumulator = self.get_train_step_accumulator()\n\n        def step_fn():\n            self.optimizer_.zero_grad()\n            step = self.train_step_single(Xi, yi, **fit_params)\n            step_accumulator.store_step(step)\n            return step[\'loss\']\n\n        self.optimizer_.step(step_fn)\n        return step_accumulator.get_step()\n\n    def evaluation_step(self, Xi, training=False):\n        """"""Perform a forward step to produce the output used for\n        prediction and scoring.\n\n        Therefore the module is set to evaluation mode by default\n        beforehand which can be overridden to re-enable features\n        like dropout by setting ``training=True``.\n\n        """"""\n        self.check_is_fitted()\n        with torch.set_grad_enabled(training):\n            self.module_.train(training)\n            return self.infer(Xi)\n\n    def fit_loop(self, X, y=None, epochs=None, **fit_params):\n        """"""The proper fit loop.\n\n        Contains the logic of what actually happens during the fit\n        loop.\n\n        Parameters\n        ----------\n        X : input data, compatible with skorch.dataset.Dataset\n          By default, you should be able to pass:\n\n            * numpy arrays\n            * torch tensors\n            * pandas DataFrame or Series\n            * scipy sparse CSR matrices\n            * a dictionary of the former three\n            * a list/tuple of the former three\n            * a Dataset\n\n          If this doesn\'t work with your data, you have to pass a\n          ``Dataset`` that can deal with the data.\n\n        y : target data, compatible with skorch.dataset.Dataset\n          The same data types as for ``X`` are supported. If your X is\n          a Dataset that contains the target, ``y`` may be set to\n          None.\n\n        epochs : int or None (default=None)\n          If int, train for this number of epochs; if None, use\n          ``self.max_epochs``.\n\n        **fit_params : dict\n          Additional parameters passed to the ``forward`` method of\n          the module and to the ``self.train_split`` call.\n\n        """"""\n        self.check_data(X, y)\n        epochs = epochs if epochs is not None else self.max_epochs\n\n        dataset_train, dataset_valid = self.get_split_datasets(\n            X, y, **fit_params)\n        on_epoch_kwargs = {\n            \'dataset_train\': dataset_train,\n            \'dataset_valid\': dataset_valid,\n        }\n\n        for _ in range(epochs):\n            self.notify(\'on_epoch_begin\', **on_epoch_kwargs)\n\n            self.run_single_epoch(dataset_train, training=True, prefix=""train"",\n                                  step_fn=self.train_step, **fit_params)\n\n            if dataset_valid is not None:\n                self.run_single_epoch(dataset_valid, training=False, prefix=""valid"",\n                                      step_fn=self.validation_step, **fit_params)\n\n            self.notify(""on_epoch_end"", **on_epoch_kwargs)\n        return self\n\n    def run_single_epoch(self, dataset, training, prefix, step_fn, **fit_params):\n        """"""Compute a single epoch of train or validation.\n\n        Parameters\n        ----------\n        dataset : torch Dataset\n            The initialized dataset to loop over.\n\n        training : bool\n            Whether to set the module to train mode or not.\n\n        prefix : str\n            Prefix to use when saving to the history.\n\n        step_fn : callable\n            Function to call for each batch.\n\n        **fit_params : dict\n            Additional parameters passed to the ``step_fn``.\n        """"""\n        is_placeholder_y = uses_placeholder_y(dataset)\n\n        batch_count = 0\n        for data in self.get_iterator(dataset, training=training):\n            Xi, yi = unpack_data(data)\n            yi_res = yi if not is_placeholder_y else None\n            self.notify(""on_batch_begin"", X=Xi, y=yi_res, training=training)\n            step = step_fn(Xi, yi, **fit_params)\n            self.history.record_batch(prefix + ""_loss"", step[""loss""].item())\n            self.history.record_batch(prefix + ""_batch_size"", get_len(Xi))\n            self.notify(""on_batch_end"", X=Xi, y=yi_res, training=training, **step)\n            batch_count += 1\n\n        self.history.record(prefix + ""_batch_count"", batch_count)\n\n    # pylint: disable=unused-argument\n    def partial_fit(self, X, y=None, classes=None, **fit_params):\n        """"""Fit the module.\n\n        If the module is initialized, it is not re-initialized, which\n        means that this method should be used if you want to continue\n        training a model (warm start).\n\n        Parameters\n        ----------\n        X : input data, compatible with skorch.dataset.Dataset\n          By default, you should be able to pass:\n\n            * numpy arrays\n            * torch tensors\n            * pandas DataFrame or Series\n            * scipy sparse CSR matrices\n            * a dictionary of the former three\n            * a list/tuple of the former three\n            * a Dataset\n\n          If this doesn\'t work with your data, you have to pass a\n          ``Dataset`` that can deal with the data.\n\n        y : target data, compatible with skorch.dataset.Dataset\n          The same data types as for ``X`` are supported. If your X is\n          a Dataset that contains the target, ``y`` may be set to\n          None.\n\n        classes : array, sahpe (n_classes,)\n          Solely for sklearn compatibility, currently unused.\n\n        **fit_params : dict\n          Additional parameters passed to the ``forward`` method of\n          the module and to the ``self.train_split`` call.\n\n        """"""\n        if not self.initialized_:\n            self.initialize()\n\n        self.notify(\'on_train_begin\', X=X, y=y)\n        try:\n            self.fit_loop(X, y, **fit_params)\n        except KeyboardInterrupt:\n            pass\n        self.notify(\'on_train_end\', X=X, y=y)\n        return self\n\n    def fit(self, X, y=None, **fit_params):\n        """"""Initialize and fit the module.\n\n        If the module was already initialized, by calling fit, the\n        module will be re-initialized (unless ``warm_start`` is True).\n\n        Parameters\n        ----------\n        X : input data, compatible with skorch.dataset.Dataset\n          By default, you should be able to pass:\n\n            * numpy arrays\n            * torch tensors\n            * pandas DataFrame or Series\n            * scipy sparse CSR matrices\n            * a dictionary of the former three\n            * a list/tuple of the former three\n            * a Dataset\n\n          If this doesn\'t work with your data, you have to pass a\n          ``Dataset`` that can deal with the data.\n\n        y : target data, compatible with skorch.dataset.Dataset\n          The same data types as for ``X`` are supported. If your X is\n          a Dataset that contains the target, ``y`` may be set to\n          None.\n\n        **fit_params : dict\n          Additional parameters passed to the ``forward`` method of\n          the module and to the ``self.train_split`` call.\n\n        """"""\n        if not self.warm_start or not self.initialized_:\n            self.initialize()\n\n        self.partial_fit(X, y, **fit_params)\n        return self\n\n    def check_is_fitted(self, attributes=None, *args, **kwargs):\n        """"""Checks whether the net is initialized\n\n        Parameters\n        ----------\n        attributes : iterable of str or None (default=None)\n          All the attributes that are strictly required of a fitted\n          net. By default, this is the `module_` attribute.\n\n        Other arguments as in\n        ``sklearn.utils.validation.check_is_fitted``.\n\n        Raises\n        ------\n        skorch.exceptions.NotInitializedError\n          When the given attributes are not present.\n\n        """"""\n        attributes = attributes or [\'module_\']\n        check_is_fitted(self, attributes, *args, **kwargs)\n\n    def forward_iter(self, X, training=False, device=\'cpu\'):\n        """"""Yield outputs of module forward calls on each batch of data.\n        The storage device of the yielded tensors is determined\n        by the ``device`` parameter.\n\n        Parameters\n        ----------\n        X : input data, compatible with skorch.dataset.Dataset\n          By default, you should be able to pass:\n\n            * numpy arrays\n            * torch tensors\n            * pandas DataFrame or Series\n            * scipy sparse CSR matrices\n            * a dictionary of the former three\n            * a list/tuple of the former three\n            * a Dataset\n\n          If this doesn\'t work with your data, you have to pass a\n          ``Dataset`` that can deal with the data.\n\n        training : bool (default=False)\n          Whether to set the module to train mode or not.\n\n        device : string (default=\'cpu\')\n          The device to store each inference result on.\n          This defaults to CPU memory since there is genereally\n          more memory available there. For performance reasons\n          this might be changed to a specific CUDA device,\n          e.g. \'cuda:0\'.\n\n        Yields\n        ------\n        yp : torch tensor\n          Result from a forward call on an individual batch.\n\n        """"""\n        dataset = self.get_dataset(X)\n        iterator = self.get_iterator(dataset, training=training)\n        for data in iterator:\n            Xi = unpack_data(data)[0]\n            yp = self.evaluation_step(Xi, training=training)\n            yield to_device(yp, device=device)\n\n    def forward(self, X, training=False, device=\'cpu\'):\n        """"""Gather and concatenate the output from forward call with\n        input data.\n\n        The outputs from ``self.module_.forward`` are gathered on the\n        compute device specified by ``device`` and then concatenated\n        using PyTorch :func:`~torch.cat`. If multiple outputs are\n        returned by ``self.module_.forward``, each one of them must be\n        able to be concatenated this way.\n\n        Parameters\n        ----------\n        X : input data, compatible with skorch.dataset.Dataset\n          By default, you should be able to pass:\n\n            * numpy arrays\n            * torch tensors\n            * pandas DataFrame or Series\n            * scipy sparse CSR matrices\n            * a dictionary of the former three\n            * a list/tuple of the former three\n            * a Dataset\n\n          If this doesn\'t work with your data, you have to pass a\n          ``Dataset`` that can deal with the data.\n\n        training : bool (default=False)\n          Whether to set the module to train mode or not.\n\n        device : string (default=\'cpu\')\n          The device to store each inference result on.\n          This defaults to CPU memory since there is genereally\n          more memory available there. For performance reasons\n          this might be changed to a specific CUDA device,\n          e.g. \'cuda:0\'.\n\n        Returns\n        -------\n        y_infer : torch tensor\n          The result from the forward step.\n\n        """"""\n        y_infer = list(self.forward_iter(X, training=training, device=device))\n\n        is_multioutput = len(y_infer) > 0 and isinstance(y_infer[0], tuple)\n        if is_multioutput:\n            return tuple(map(torch.cat, zip(*y_infer)))\n        return torch.cat(y_infer)\n\n    def _merge_x_and_fit_params(self, x, fit_params):\n        duplicates = duplicate_items(x, fit_params)\n        if duplicates:\n            msg = ""X and fit_params contain duplicate keys: ""\n            msg += \', \'.join(duplicates)\n            raise ValueError(msg)\n\n        x_dict = dict(x)  # shallow copy\n        x_dict.update(fit_params)\n        return x_dict\n\n    def infer(self, x, **fit_params):\n        """"""Perform a single inference step on a batch of data.\n\n        Parameters\n        ----------\n        x : input data\n          A batch of the input data.\n\n        **fit_params : dict\n          Additional parameters passed to the ``forward`` method of\n          the module and to the ``self.train_split`` call.\n\n        """"""\n        x = to_tensor(x, device=self.device)\n        if isinstance(x, dict):\n            x_dict = self._merge_x_and_fit_params(x, fit_params)\n            return self.module_(**x_dict)\n        return self.module_(x, **fit_params)\n\n    def predict_proba(self, X):\n        """"""Return the output of the module\'s forward method as a numpy\n        array.\n\n        If the module\'s forward method returns multiple outputs as a\n        tuple, it is assumed that the first output contains the\n        relevant information and the other values are ignored. If all\n        values are relevant, consider using\n        :func:`~skorch.NeuralNet.forward` instead.\n\n        Parameters\n        ----------\n        X : input data, compatible with skorch.dataset.Dataset\n          By default, you should be able to pass:\n\n            * numpy arrays\n            * torch tensors\n            * pandas DataFrame or Series\n            * scipy sparse CSR matrices\n            * a dictionary of the former three\n            * a list/tuple of the former three\n            * a Dataset\n\n          If this doesn\'t work with your data, you have to pass a\n          ``Dataset`` that can deal with the data.\n\n        Returns\n        -------\n        y_proba : numpy ndarray\n\n        """"""\n        y_probas = []\n        for yp in self.forward_iter(X, training=False):\n            yp = yp[0] if isinstance(yp, tuple) else yp\n            y_probas.append(to_numpy(yp))\n        y_proba = np.concatenate(y_probas, 0)\n        return y_proba\n\n    def predict(self, X):\n        """"""Where applicable, return class labels for samples in X.\n\n        If the module\'s forward method returns multiple outputs as a\n        tuple, it is assumed that the first output contains the\n        relevant information and the other values are ignored. If all\n        values are relevant, consider using\n        :func:`~skorch.NeuralNet.forward` instead.\n\n        Parameters\n        ----------\n        X : input data, compatible with skorch.dataset.Dataset\n          By default, you should be able to pass:\n\n            * numpy arrays\n            * torch tensors\n            * pandas DataFrame or Series\n            * scipy sparse CSR matrices\n            * a dictionary of the former three\n            * a list/tuple of the former three\n            * a Dataset\n\n          If this doesn\'t work with your data, you have to pass a\n          ``Dataset`` that can deal with the data.\n\n        Returns\n        -------\n        y_pred : numpy ndarray\n\n        """"""\n        return self.predict_proba(X)\n\n    # pylint: disable=unused-argument\n    def get_loss(self, y_pred, y_true, X=None, training=False):\n        """"""Return the loss for this batch.\n\n        Parameters\n        ----------\n        y_pred : torch tensor\n          Predicted target values\n\n        y_true : torch tensor\n          True target values.\n\n        X : input data, compatible with skorch.dataset.Dataset\n          By default, you should be able to pass:\n\n            * numpy arrays\n            * torch tensors\n            * pandas DataFrame or Series\n            * scipy sparse CSR matrices\n            * a dictionary of the former three\n            * a list/tuple of the former three\n            * a Dataset\n\n          If this doesn\'t work with your data, you have to pass a\n          ``Dataset`` that can deal with the data.\n\n        training : bool (default=False)\n          Whether train mode should be used or not.\n\n        """"""\n        y_true = to_tensor(y_true, device=self.device)\n\n        if isinstance(self.criterion_, torch.nn.Module):\n            self.criterion_.train(training)\n\n        return self.criterion_(y_pred, y_true)\n\n    def get_dataset(self, X, y=None):\n        """"""Get a dataset that contains the input data and is passed to\n        the iterator.\n\n        Override this if you want to initialize your dataset\n        differently.\n\n        Parameters\n        ----------\n        X : input data, compatible with skorch.dataset.Dataset\n          By default, you should be able to pass:\n\n            * numpy arrays\n            * torch tensors\n            * pandas DataFrame or Series\n            * scipy sparse CSR matrices\n            * a dictionary of the former three\n            * a list/tuple of the former three\n            * a Dataset\n\n          If this doesn\'t work with your data, you have to pass a\n          ``Dataset`` that can deal with the data.\n\n        y : target data, compatible with skorch.dataset.Dataset\n          The same data types as for ``X`` are supported. If your X is\n          a Dataset that contains the target, ``y`` may be set to\n          None.\n\n        Returns\n        -------\n        dataset\n          The initialized dataset.\n\n        """"""\n        if is_dataset(X):\n            return X\n\n        dataset = self.dataset\n        is_initialized = not callable(dataset)\n\n        kwargs = self.get_params_for(\'dataset\')\n        if kwargs and is_initialized:\n            raise TypeError(""Trying to pass an initialized Dataset while ""\n                            ""passing Dataset arguments ({}) is not ""\n                            ""allowed."".format(kwargs))\n\n        if is_initialized:\n            return dataset\n\n        return dataset(X, y, **kwargs)\n\n    def get_split_datasets(self, X, y=None, **fit_params):\n        """"""Get internal train and validation datasets.\n\n        The validation dataset can be None if ``self.train_split`` is\n        set to None; then internal validation will be skipped.\n\n        Override this if you want to change how the net splits\n        incoming data into train and validation part.\n\n        Parameters\n        ----------\n        X : input data, compatible with skorch.dataset.Dataset\n          By default, you should be able to pass:\n\n            * numpy arrays\n            * torch tensors\n            * pandas DataFrame or Series\n            * scipy sparse CSR matrices\n            * a dictionary of the former three\n            * a list/tuple of the former three\n            * a Dataset\n\n          If this doesn\'t work with your data, you have to pass a\n          ``Dataset`` that can deal with the data.\n\n        y : target data, compatible with skorch.dataset.Dataset\n          The same data types as for ``X`` are supported. If your X is\n          a Dataset that contains the target, ``y`` may be set to\n          None.\n\n        **fit_params : dict\n          Additional parameters passed to the ``self.train_split``\n          call.\n\n        Returns\n        -------\n        dataset_train\n          The initialized training dataset.\n\n        dataset_valid\n          The initialized validation dataset or None\n\n        """"""\n        dataset = self.get_dataset(X, y)\n        if self.train_split:\n            dataset_train, dataset_valid = self.train_split(\n                dataset, y, **fit_params)\n        else:\n            dataset_train, dataset_valid = dataset, None\n        return dataset_train, dataset_valid\n\n    def get_iterator(self, dataset, training=False):\n        """"""Get an iterator that allows to loop over the batches of the\n        given data.\n\n        If ``self.iterator_train__batch_size`` and/or\n        ``self.iterator_test__batch_size`` are not set, use\n        ``self.batch_size`` instead.\n\n        Parameters\n        ----------\n        dataset : torch Dataset (default=skorch.dataset.Dataset)\n          Usually, ``self.dataset``, initialized with the corresponding\n          data, is passed to ``get_iterator``.\n\n        training : bool (default=False)\n          Whether to use ``iterator_train`` or ``iterator_test``.\n\n        Returns\n        -------\n        iterator\n          An instantiated iterator that allows to loop over the\n          mini-batches.\n\n        """"""\n        if training:\n            kwargs = self.get_params_for(\'iterator_train\')\n            iterator = self.iterator_train\n        else:\n            kwargs = self.get_params_for(\'iterator_valid\')\n            iterator = self.iterator_valid\n\n        if \'batch_size\' not in kwargs:\n            kwargs[\'batch_size\'] = self.batch_size\n\n        if kwargs[\'batch_size\'] == -1:\n            kwargs[\'batch_size\'] = len(dataset)\n\n        return iterator(dataset, **kwargs)\n\n    def _get_params_for(self, prefix):\n        return params_for(prefix, self.__dict__)\n\n    def get_params_for(self, prefix):\n        """"""Collect and return init parameters for an attribute.\n\n        Attributes could be, for instance, pytorch modules, criteria,\n        or data loaders (for optimizers, use\n        :meth:`.get_params_for_optimizer` instead). Use the returned\n        arguments to initialize the given attribute like this:\n\n        .. code:: python\n\n            # inside initialize_module method\n            kwargs = self.get_params_for(\'module\')\n            self.module_ = self.module(**kwargs)\n\n        Proceed analogously for the criterion etc.\n\n        The reason to use this method is so that it\'s possible to\n        change the init parameters with :meth:`.set_params`, which\n        in turn makes grid search and other similar things work.\n\n        Note that in general, as a user, you never have to deal with\n        this method because :meth:`.initialize_module` etc. are\n        already taking care of this. You only need to deal with this\n        if you override :meth:`.initialize_module` (or similar\n        methods) because you have some custom code that requires it.\n\n        Parameters\n        ----------\n        prefix : str\n          The name of the attribute whose arguments should be\n          returned. E.g. for the module, it should be ``\'module\'``.\n\n        Returns\n        -------\n        kwargs : dict\n          Keyword arguments to be used as init parameters.\n\n        """"""\n        return self._get_params_for(prefix)\n\n    def _get_params_for_optimizer(self, prefix, named_parameters):\n        kwargs = self.get_params_for(prefix)\n        params = list(named_parameters)\n        pgroups = []\n\n        for pattern, group in kwargs.pop(\'param_groups\', []):\n            matches = [i for i, (name, _) in enumerate(params) if\n                       fnmatch.fnmatch(name, pattern)]\n            if matches:\n                p = [params.pop(i)[1] for i in reversed(matches)]\n                pgroups.append({\'params\': p, **group})\n\n        if params:\n            pgroups.append({\'params\': [p for _, p in params]})\n\n        args = (pgroups,)\n        return args, kwargs\n\n    def get_params_for_optimizer(self, prefix, named_parameters):\n        """"""Collect and return init parameters for an optimizer.\n\n        Parse kwargs configuration for the optimizer identified by\n        the given prefix. Supports param group assignment using wildcards:\n\n        .. code:: python\n\n            optimizer__lr=0.05,\n            optimizer__param_groups=[\n                (\'rnn*.period\', {\'lr\': 0.3, \'momentum\': 0}),\n                (\'rnn0\', {\'lr\': 0.1}),\n            ]\n\n        Generally, use this method like this:\n\n        .. code:: python\n\n            # inside initialize_optimizer method\n            named_params = self.module_.named_parameters()\n            pgroups, kwargs = self.get_params_for_optimizer(\'optimizer\', named_params)\n            if \'lr\' not in kwargs:\n                kwargs[\'lr\'] = self.lr\n            self.optimizer_ = self.optimizer(*pgroups, **kwargs)\n\n        The reason to use this method is so that it\'s possible to\n        change the init parameters with :meth:`.set_params`, which\n        in turn makes grid search and other similar things work.\n\n        Note that in general, as a user, you never have to deal with\n        this method because :meth:`.initialize_optimizer` is already\n        taking care of this. You only need to deal with this if you\n        override :meth:`.initialize_optimizer` because you have some\n        custom code that requires it.\n\n        Parameters\n        ----------\n        prefix : str\n          The name of the optimizer whose arguments should be\n          returned. Typically, this should just be\n          ``\'optimizer\'``. There can be exceptions, however, e.g. if\n          you want to use more than one optimizer.\n\n        named_parameters : iterator\n          Iterator over the parameters of the module that is intended\n          to be optimized. It\'s the return value of\n          ``my_module.named_parameters()``.\n\n        Returns\n        -------\n        args : tuple\n          All positional arguments for this optimizer (right now only\n          one, the parameter groups).\n\n        kwargs : dict\n          All other parameters for this optimizer, e.g. the learning\n          rate.\n\n        """"""\n        args, kwargs = self._get_params_for_optimizer(prefix, named_parameters)\n        return args, kwargs\n\n    def _get_param_names(self):\n        return (k for k in self.__dict__ if not k.endswith(\'_\'))\n\n    def _get_params_callbacks(self, deep=True):\n        """"""sklearn\'s .get_params checks for `hasattr(value,\n        \'get_params\')`. This returns False for a list. But our\n        callbacks reside within a list. Hence their parameters have to\n        be retrieved separately.\n        """"""\n        params = {}\n        if not deep:\n            return params\n\n        callbacks_ = getattr(self, \'callbacks_\', [])\n        for key, val in chain(callbacks_, self._default_callbacks):\n            name = \'callbacks__\' + key\n            params[name] = val\n            if val is None:  # callback deactivated\n                continue\n            for subkey, subval in val.get_params().items():\n                subname = name + \'__\' + subkey\n                params[subname] = subval\n        return params\n\n    def get_params(self, deep=True, **kwargs):\n        params = BaseEstimator.get_params(self, deep=deep, **kwargs)\n        # Callback parameters are not returned by .get_params, needs\n        # special treatment.\n        params_cb = self._get_params_callbacks(deep=deep)\n        params.update(params_cb)\n        return params\n\n    def _check_kwargs(self, kwargs):\n        """"""Check argument names passed at initialization.\n\n        Raises\n        ------\n        TypeError\n          Raises a TypeError if one or more arguments don\'t seem to\n          match or are malformed.\n\n        Returns\n        -------\n        kwargs: dict\n          Return the passed keyword arguments.\n\n        Example\n        -------\n        >>> net = NeuralNetClassifier(MyModule, iterator_train_shuffle=True)\n        TypeError: Got an unexpected argument iterator_train_shuffle,\n        did you mean iterator_train__shuffle?\n\n        """"""\n        unexpected_kwargs = []\n        missing_dunder_kwargs = []\n        for key in kwargs:\n            if key.endswith(\'_\'):\n                continue\n\n            # see https://github.com/skorch-dev/skorch/pull/590 for\n            # why this must be sorted\n            for prefix in sorted(self.prefixes_, key=lambda s: (-len(s), s)):\n                if key.startswith(prefix):\n                    if not key.startswith(prefix + \'__\'):\n                        missing_dunder_kwargs.append((prefix, key))\n                    break\n            else:  # no break means key didn\'t match a prefix\n                unexpected_kwargs.append(key)\n\n        msgs = []\n        if unexpected_kwargs:\n            tmpl = (""__init__() got unexpected argument(s) {}. ""\n                    ""Either you made a typo, or you added new arguments ""\n                    ""in a subclass; if that is the case, the subclass ""\n                    ""should deal with the new arguments explicitly."")\n            msg = tmpl.format(\', \'.join(sorted(unexpected_kwargs)))\n            msgs.append(msg)\n\n        for prefix, key in sorted(missing_dunder_kwargs, key=lambda tup: tup[1]):\n            tmpl = ""Got an unexpected argument {}, did you mean {}?""\n            suffix = key[len(prefix):].lstrip(\'_\')\n            suggestion = prefix + \'__\' + suffix\n            msgs.append(tmpl.format(key, suggestion))\n\n        if msgs:\n            full_msg = \'\\n\'.join(msgs)\n            raise TypeError(full_msg)\n\n        return kwargs\n\n    def _check_deprecated_params(self, **kwargs):\n        pass\n\n    def set_params(self, **kwargs):\n        """"""Set the parameters of this class.\n\n        Valid parameter keys can be listed with ``get_params()``.\n\n        Returns\n        -------\n        self\n\n        """"""\n        self._check_deprecated_params(**kwargs)\n        normal_params, cb_params, special_params = {}, {}, {}\n        virtual_params = {}\n\n        for key, val in kwargs.items():\n            if self._is_virtual_param(key):\n                virtual_params[key] = val\n            elif key.startswith(\'callbacks\'):\n                cb_params[key] = val\n            elif any(key.startswith(prefix) for prefix in self.prefixes_):\n                special_params[key] = val\n            else:\n                normal_params[key] = val\n\n        self._apply_virtual_params(virtual_params)\n        BaseEstimator.set_params(self, **normal_params)\n\n        for key, val in special_params.items():\n            if key.endswith(\'_\'):\n                raise ValueError(\n                    ""Something went wrong here. Please open an issue on ""\n                    ""https://github.com/skorch-dev/skorch/issues detailing what ""\n                    ""caused this error."")\n            setattr(self, key, val)\n\n        # Below: Re-initialize parts of the net if necessary.\n\n        if cb_params:\n            # callbacks need special treatmeant since they are list of tuples\n            self.initialize_callbacks()\n            self._set_params_callback(**cb_params)\n\n        if any(\'criterion\' in key.split(\'__\', 1)[0] for key in special_params):\n            self.initialize_criterion()\n\n        module_triggers_optimizer_reinit = False\n        if any(\'module\' in key.split(\'__\', 1)[0] for key in special_params):\n            self.initialize_module()\n            module_triggers_optimizer_reinit = True\n\n        optimizer_changed = (\n            any(\'optimizer\' in key.split(\'__\', 1)[0] for key in special_params)\n            or \'lr\' in normal_params\n        )\n        if module_triggers_optimizer_reinit or optimizer_changed:\n            # Model selectors such as GridSearchCV will set the\n            # parameters before .initialize() is called, therefore we\n            # need to make sure that we have an initialized model here\n            # as the optimizer depends on it.\n            if not hasattr(self, \'module_\'):\n                self.initialize_module()\n\n            # If we reached this point but the optimizer was not\n            # changed, it means that optimizer initialization was\n            # triggered indirectly.\n            self.initialize_optimizer(triggered_directly=optimizer_changed)\n\n        vars(self).update(kwargs)\n\n        return self\n\n    def _set_params_callback(self, **params):\n        """"""Special handling for setting params on callbacks.""""""\n        # model after sklearn.utils._BaseCompostion._set_params\n        # 1. All steps\n        if \'callbacks\' in params:\n            setattr(self, \'callbacks\', params.pop(\'callbacks\'))\n\n        # 2. Step replacement\n        names, _ = zip(*getattr(self, \'callbacks_\'))\n        for key in params.copy():\n            name = key[11:]  # drop \'callbacks__\'\n            if \'__\' not in name and name in names:\n                self._replace_callback(name, params.pop(key))\n\n        # 3. Step parameters and other initilisation arguments\n        for key in params.copy():\n            name = key[11:]\n            part0, part1 = name.split(\'__\')\n            kwarg = {part1: params.pop(key)}\n            callback = dict(self.callbacks_).get(part0)\n            if callback is not None:\n                callback.set_params(**kwarg)\n            else:\n                raise ValueError(\n                    ""Trying to set a parameter for callback {} ""\n                    ""which does not exist."".format(part0))\n\n        return self\n\n    def _replace_callback(self, name, new_val):\n        # assumes `name` is a valid callback name\n        callbacks_new = self.callbacks_[:]\n        for i, (cb_name, _) in enumerate(callbacks_new):\n            if cb_name == name:\n                callbacks_new[i] = (name, new_val)\n                break\n        setattr(self, \'callbacks_\', callbacks_new)\n\n    def __getstate__(self):\n        state = self.__dict__.copy()\n        cuda_attrs = {}\n        for prefix in self.cuda_dependent_attributes_:\n            for key in state:\n                if isinstance(key, str) and key.startswith(prefix):\n                    cuda_attrs[key] = state[key]\n\n        for k in cuda_attrs:\n            state.pop(k)\n\n        with tempfile.SpooledTemporaryFile() as f:\n            torch.save(cuda_attrs, f)\n            f.seek(0)\n            state[\'__cuda_dependent_attributes__\'] = f.read()\n\n        return state\n\n    def __setstate__(self, state):\n        # get_map_location will automatically choose the\n        # right device in cases where CUDA is not available.\n        map_location = get_map_location(state[\'device\'])\n        load_kwargs = {\'map_location\': map_location}\n        state[\'device\'] = self._check_device(state[\'device\'], map_location)\n\n        with tempfile.SpooledTemporaryFile() as f:\n            f.write(state[\'__cuda_dependent_attributes__\'])\n            f.seek(0)\n            cuda_attrs = torch.load(f, **load_kwargs)\n\n        state.update(cuda_attrs)\n        state.pop(\'__cuda_dependent_attributes__\')\n\n        self.__dict__.update(state)\n\n    def _register_attribute(\n            self,\n            name,\n            prefixes=True,\n            cuda_dependent_attributes=True,\n    ):\n        """"""Add attribute name to prefixes_ and\n        cuda_dependent_attributes_.\n\n        The first is to take care that the attribute works correctly\n        with set_params, e.g. when it comes to re-initialization.\n\n        The second is to make sure that nets trained with CUDA can be\n        loaded without CUDA.\n\n        This method takes care of not mutating the lists.\n\n        Parameters\n        ----------\n        prefixes : bool (default=True)\n          Whether to add to prefixes_.\n\n        cuda_dependent_attributes : bool (default=True)\n          Whether to add to cuda_dependent_attributes_.\n\n        """"""\n        # copy the lists to avoid mutation\n        if prefixes:\n            self.prefixes_ = self.prefixes_[:] + [name]\n\n        if cuda_dependent_attributes:\n            self.cuda_dependent_attributes_ = (\n                self.cuda_dependent_attributes_[:] + [name + \'_\'])\n\n    def _unregister_attribute(\n            self,\n            name,\n            prefixes=True,\n            cuda_dependent_attributes=True,\n    ):\n        """"""Remove attribute name from prefixes_ and\n        cuda_dependent_attributes_.\n\n        Use this to remove PyTorch components that are not needed\n        anymore. This is mostly a clean up job, so as to not leave\n        unnecessary prefixes or cuda-dependent attributes.\n\n        This method takes care of not mutating the lists.\n\n        Parameters\n        ----------\n        prefixes : bool (default=True)\n          Whether to remove from prefixes_.\n\n        cuda_dependent_attributes : bool (default=True)\n          Whether to remove from cuda_dependent_attributes_.\n\n        """"""\n        # copy the lists to avoid mutation\n        if prefixes:\n            self.prefixes_ = [p for p in self.prefixes_ if p != name]\n\n        if cuda_dependent_attributes:\n            self.cuda_dependent_attributes_ = [\n                a for a in self.cuda_dependent_attributes_ if a != name + \'_\']\n\n    def __setattr__(self, name, attr):\n        """"""Set an attribute on the net\n\n        When a custom net with additional torch modules or optimizers\n        is created, those attributes are added to ``prefixes_`` and\n        ``cuda_dependent_attributes_`` automatically.\n\n        """"""\n        # If it\'s a\n        # 1. known attribute or\n        # 2. special param like module__num_units or\n        # 3. not a torch module/optimizer instance or class\n        # just setattr as usual.\n        # For a discussion why we chose this implementation, see here:\n        # https://github.com/skorch-dev/skorch/pull/597\n        is_known = name.endswith(\'_\') or (name in self.prefixes_)\n        is_special_param = \'__\' in name\n        is_torch_component = any(c in name for c in _PYTORCH_COMPONENTS)\n\n        if not (is_known or is_special_param) and is_torch_component:\n            self._register_attribute(name)\n        super().__setattr__(name, attr)\n\n    def __delattr__(self, name):\n        # take extra precautions to undo the changes made in __setattr__\n        self._unregister_attribute(name)\n        super().__delattr__(name)\n\n    def save_params(\n            self, f_params=None, f_optimizer=None, f_history=None):\n        """"""Saves the module\'s parameters, history, and optimizer,\n        not the whole object.\n\n        To save the whole object, use pickle. This is necessary when\n        you need additional learned attributes on the net, e.g. the\n        ``classes_`` attribute on\n        :class:`skorch.classifier.NeuralNetClassifier`.\n\n        ``f_params`` and ``f_optimizer`` uses PyTorchs\'\n        :func:`~torch.save`.\n\n        Parameters\n        ----------\n        f_params : file-like object, str, None (default=None)\n          Path of module parameters. Pass ``None`` to not save\n\n        f_optimizer : file-like object, str, None (default=None)\n          Path of optimizer. Pass ``None`` to not save\n\n        f_history : file-like object, str, None (default=None)\n          Path to history. Pass ``None`` to not save\n\n        Examples\n        --------\n        >>> before = NeuralNetClassifier(mymodule)\n        >>> before.save_params(f_params=\'model.pkl\',\n        >>>                    f_optimizer=\'optimizer.pkl\',\n        >>>                    f_history=\'history.json\')\n        >>> after = NeuralNetClassifier(mymodule).initialize()\n        >>> after.load_params(f_params=\'model.pkl\',\n        >>>                   f_optimizer=\'optimizer.pkl\',\n        >>>                   f_history=\'history.json\')\n\n        """"""\n        if f_params is not None:\n            msg = (\n                ""Cannot save parameters of an un-initialized model. ""\n                ""Please initialize first by calling .initialize() ""\n                ""or by fitting the model with .fit(...)."")\n            self.check_is_fitted(msg=msg)\n            torch.save(self.module_.state_dict(), f_params)\n\n        if f_optimizer is not None:\n            msg = (\n                ""Cannot save state of an un-initialized optimizer. ""\n                ""Please initialize first by calling .initialize() ""\n                ""or by fitting the model with .fit(...)."")\n            self.check_is_fitted(attributes=[\'optimizer_\'], msg=msg)\n            torch.save(self.optimizer_.state_dict(), f_optimizer)\n\n        if f_history is not None:\n            self.history.to_file(f_history)\n\n    def _check_device(self, requested_device, map_device):\n        """"""Compare the requested device with the map device and\n        return the map device if it differs from the requested device\n        along with a warning.\n        """"""\n        type_1 = torch.device(requested_device)\n        type_2 = torch.device(map_device)\n        if type_1 != type_2:\n            warnings.warn(\n                \'Setting self.device = {} since the requested device ({}) \'\n                \'is not available.\'.format(map_device, requested_device),\n                DeviceWarning)\n            return map_device\n        # return requested_device instead of map_device even though we\n        # checked for *type* equality as we might have \'cuda:0\' vs. \'cuda:1\'.\n        return requested_device\n\n    def load_params(\n            self, f_params=None, f_optimizer=None, f_history=None,\n            checkpoint=None):\n        """"""Loads the the module\'s parameters, history, and optimizer,\n        not the whole object.\n\n        To save and load the whole object, use pickle.\n\n        ``f_params`` and ``f_optimizer`` uses PyTorchs\'\n        :func:`~torch.save`.\n\n        Parameters\n        ----------\n        f_params : file-like object, str, None (default=None)\n          Path of module parameters. Pass ``None`` to not load.\n\n        f_optimizer : file-like object, str, None (default=None)\n          Path of optimizer. Pass ``None`` to not load.\n\n        f_history : file-like object, str, None (default=None)\n          Path to history. Pass ``None`` to not load.\n\n        checkpoint : :class:`.Checkpoint`, None (default=None)\n          Checkpoint to load params from. If a checkpoint and a ``f_*``\n          path is passed in, the ``f_*`` will be loaded. Pass\n          ``None`` to not load.\n\n        Examples\n        --------\n        >>> before = NeuralNetClassifier(mymodule)\n        >>> before.save_params(f_params=\'model.pkl\',\n        >>>                    f_optimizer=\'optimizer.pkl\',\n        >>>                    f_history=\'history.json\')\n        >>> after = NeuralNetClassifier(mymodule).initialize()\n        >>> after.load_params(f_params=\'model.pkl\',\n        >>>                   f_optimizer=\'optimizer.pkl\',\n        >>>                   f_history=\'history.json\')\n\n        """"""\n        def _get_state_dict(f):\n            map_location = get_map_location(self.device)\n            self.device = self._check_device(self.device, map_location)\n            return torch.load(f, map_location=map_location)\n\n        if f_history is not None:\n            self.history = History.from_file(f_history)\n\n        if checkpoint is not None:\n            if not self.initialized_:\n                self.initialize()\n            if f_history is None and checkpoint.f_history is not None:\n                self.history = History.from_file(checkpoint.f_history_)\n            formatted_files = checkpoint.get_formatted_files(self)\n            f_params = f_params or formatted_files[\'f_params\']\n            f_optimizer = f_optimizer or formatted_files[\'f_optimizer\']\n\n        if f_params is not None:\n            msg = (\n                ""Cannot load parameters of an un-initialized model. ""\n                ""Please initialize first by calling .initialize() ""\n                ""or by fitting the model with .fit(...)."")\n            self.check_is_fitted(msg=msg)\n            state_dict = _get_state_dict(f_params)\n            self.module_.load_state_dict(state_dict)\n\n        if f_optimizer is not None:\n            msg = (\n                ""Cannot load state of an un-initialized optimizer. ""\n                ""Please initialize first by calling .initialize() ""\n                ""or by fitting the model with .fit(...)."")\n            self.check_is_fitted(attributes=[\'optimizer_\'], msg=msg)\n            state_dict = _get_state_dict(f_optimizer)\n            self.optimizer_.load_state_dict(state_dict)\n\n    def __repr__(self):\n        to_include = [\'module\']\n        to_exclude = []\n        parts = [str(self.__class__) + \'[uninitialized](\']\n        if self.initialized_:\n            parts = [str(self.__class__) + \'[initialized](\']\n            to_include = [\'module_\']\n            to_exclude = [\'module__\']\n\n        for key, val in sorted(self.__dict__.items()):\n            if not any(key.startswith(prefix) for prefix in to_include):\n                continue\n            if any(key.startswith(prefix) for prefix in to_exclude):\n                continue\n\n            val = str(val)\n            if \'\\n\' in val:\n                val = \'\\n  \'.join(val.split(\'\\n\'))\n            parts.append(\'  {}={},\'.format(key, val))\n\n        parts.append(\')\')\n        return \'\\n\'.join(parts)\n'"
skorch/regressor.py,3,"b'""""""NeuralNet subclasses for regression tasks.""""""\n\nimport re\n\nfrom sklearn.base import RegressorMixin\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom skorch import NeuralNet\nfrom skorch.utils import get_dim\nfrom skorch.utils import is_dataset\n\n\nneural_net_reg_doc_start = """"""NeuralNet for regression tasks\n\n    Use this specifically if you have a standard regression task,\n    with input data X and target y. y must be 2d.\n\n""""""\n\nneural_net_reg_criterion_text = """"""\n\n    criterion : torch criterion (class, default=torch.nn.MSELoss)\n      Mean squared error loss.""""""\n\n\ndef get_neural_net_reg_doc(doc):\n    doc = neural_net_reg_doc_start + "" "" + doc.split(""\\n "", 4)[-1]\n    pattern = re.compile(r\'(\\n\\s+)(criterion .*\\n)(\\s.+){1,99}\')\n    start, end = pattern.search(doc).span()\n    doc = doc[:start] + neural_net_reg_criterion_text + doc[end:]\n    return doc\n\n\n# pylint: disable=missing-docstring\nclass NeuralNetRegressor(NeuralNet, RegressorMixin):\n    __doc__ = get_neural_net_reg_doc(NeuralNet.__doc__)\n\n    def __init__(\n            self,\n            module,\n            *args,\n            criterion=torch.nn.MSELoss,\n            **kwargs\n    ):\n        super(NeuralNetRegressor, self).__init__(\n            module,\n            *args,\n            criterion=criterion,\n            **kwargs\n        )\n\n    # pylint: disable=signature-differs\n    def check_data(self, X, y):\n        if (\n                (y is None) and\n                (not is_dataset(X)) and\n                (self.iterator_train is DataLoader)\n        ):\n            raise ValueError(""No y-values are given (y=None). You must ""\n                             ""implement your own DataLoader for training ""\n                             ""(and your validation) and supply it using the ""\n                             ""``iterator_train`` and ``iterator_valid`` ""\n                             ""parameters respectively."")\n        if y is None:\n            # The user implements its own mechanism for generating y.\n            return\n\n        if get_dim(y) == 1:\n            msg = (\n                ""The target data shouldn\'t be 1-dimensional but instead have ""\n                ""2 dimensions, with the second dimension having the same size ""\n                ""as the number of regression targets (usually 1). Please ""\n                ""reshape your target data to be 2-dimensional ""\n                ""(e.g. y = y.reshape(-1, 1)."")\n            raise ValueError(msg)\n\n    # pylint: disable=signature-differs\n    def fit(self, X, y, **fit_params):\n        """"""See ``NeuralNet.fit``.\n\n        In contrast to ``NeuralNet.fit``, ``y`` is non-optional to\n        avoid mistakenly forgetting about ``y``. However, ``y`` can be\n        set to ``None`` in case it is derived dynamically from\n        ``X``.\n\n        """"""\n        # pylint: disable=useless-super-delegation\n        # this is actually a pylint bug:\n        # https://github.com/PyCQA/pylint/issues/1085\n        return super(NeuralNetRegressor, self).fit(X, y, **fit_params)\n'"
skorch/setter.py,0,"b'""""""Setter functions for virtual params such as ``optimizer__lr``.""""""\nimport re\n\n\ndef _extract_optimizer_param_name_and_group(optimizer_name, param):\n    """"""Extract param group and param name from the given parameter name.\n    Raises an error if the param name doesn\'t match one of\n    - ``optimizer__param_groups__<group>__<name>``\n    - ``optimizer__<name>``\n    In the second case group defaults to \'all\'.\n    The second case explicitly forbids ``optimizer__foo__bar``\n    since we do not know how to deal with unknown sub-params.\n    """"""\n    pat_1 = \'__param_groups__(?P<group>[0-9])__(?P<name>.+)\'\n    pat_2 = \'__(?!.*__.*)(?P<name>.+)\'\n    pat_1 = optimizer_name + pat_1\n    pat_2 = optimizer_name + pat_2\n\n    match_1 = re.compile(pat_1).fullmatch(param)\n    match_2 = re.compile(pat_2).fullmatch(param)\n    match = match_1 or match_2\n\n    if not match:\n        raise AttributeError(\'Invalid parameter ""{}"" for optimizer ""{}""\'.format(\n            param,\n            optimizer_name,\n        ))\n\n    groups = match.groupdict()\n    param_group = groups.get(\'group\', \'all\')\n    param_name = groups[\'name\']\n    return param_group, param_name\n\n\ndef _set_optimizer_param(optimizer, param_group, param_name, value):\n    """"""Set a parameter on an all or a specific parameter group of an\n    optimizer instance. To select all param groups, use ``param_group=\'all\'``.\n    """"""\n    if param_group == \'all\':\n        groups = optimizer.param_groups\n    else:\n        groups = [optimizer.param_groups[int(param_group)]]\n\n    for group in groups:\n        group[param_name] = value\n\n\ndef optimizer_setter(\n        net, param, value, optimizer_attr=\'optimizer_\', optimizer_name=\'optimizer\'\n    ):\n    """"""Handle setting of optimizer parameters such as learning rate and\n    parameter group specific parameters such as momentum.\n\n    The parameters ``optimizer_attr`` and ``optimizer_name`` can be specified\n    if there exists more than one optimizer (e.g., in seq2seq models).\n    """"""\n    if param == \'lr\':\n        param_group = \'all\'\n        param_name = \'lr\'\n        net.lr = value\n    else:\n        param_group, param_name = _extract_optimizer_param_name_and_group(\n            optimizer_name, param)\n\n    _set_optimizer_param(\n        optimizer=getattr(net, optimizer_attr),\n        param_group=param_group,\n        param_name=param_name,\n        value=value\n    )\n'"
skorch/toy.py,5,"b'""""""Contains toy functions and classes for quick prototyping and\ntesting.\n\n""""""\n\nfrom functools import partial\n\nfrom torch import nn\n\n\nclass MLPModule(nn.Module):\n    """"""A simple multi-layer perceptron module.\n\n    This can be adapted for usage in different contexts, e.g. binary\n    and multi-class classification, regression, etc.\n\n    Parameters\n    ----------\n    input_units : int (default=20)\n      Number of input units.\n\n    output_units : int (default=2)\n      Number of output units.\n\n    hidden_units : int (default=10)\n      Number of units in hidden layers.\n\n    num_hidden : int (default=1)\n      Number of hidden layers.\n\n    nonlin : torch.nn.Module instance (default=torch.nn.ReLU())\n      Non-linearity to apply after hidden layers.\n\n    output_nonlin : torch.nn.Module instance or None (default=None)\n      Non-linearity to apply after last layer, if any.\n\n    dropout : float (default=0)\n      Dropout rate. Dropout is applied between layers.\n\n    squeeze_output : bool (default=False)\n      Whether to squeeze output. Squeezing can be helpful if you wish\n      your output to be 1-dimensional (e.g. for\n      NeuralNetBinaryClassifier).\n\n    """"""\n    def __init__(\n            self,\n            input_units=20,\n            output_units=2,\n            hidden_units=10,\n            num_hidden=1,\n            nonlin=nn.ReLU(),\n            output_nonlin=None,\n            dropout=0,\n            squeeze_output=False,\n    ):\n        super().__init__()\n        self.input_units = input_units\n        self.output_units = output_units\n        self.hidden_units = hidden_units\n        self.num_hidden = num_hidden\n        self.nonlin = nonlin\n        self.output_nonlin = output_nonlin\n        self.dropout = dropout\n        self.squeeze_output = squeeze_output\n\n        self.reset_params()\n\n    def reset_params(self):\n        """"""(Re)set all parameters.""""""\n        units = [self.input_units]\n        units += [self.hidden_units] * self.num_hidden\n        units += [self.output_units]\n\n        sequence = []\n        for u0, u1 in zip(units, units[1:]):\n            sequence.append(nn.Linear(u0, u1))\n            sequence.append(self.nonlin)\n            sequence.append(nn.Dropout(self.dropout))\n\n        sequence = sequence[:-2]\n        if self.output_nonlin:\n            sequence.append(self.output_nonlin)\n\n        self.sequential = nn.Sequential(*sequence)\n\n    def forward(self, X):  # pylint: disable=arguments-differ\n        X = self.sequential(X)\n        if self.squeeze_output:\n            X = X.squeeze(-1)\n        return X\n\n\ndef make_classifier(output_nonlin=nn.Softmax(dim=-1), **kwargs):\n    """"""Return a multi-layer perceptron to be used with\n    NeuralNetClassifier.\n\n    Parameters\n    ----------\n    input_units : int (default=20)\n      Number of input units.\n\n    output_units : int (default=2)\n      Number of output units.\n\n    hidden_units : int (default=10)\n      Number of units in hidden layers.\n\n    num_hidden : int (default=1)\n      Number of hidden layers.\n\n    nonlin : torch.nn.Module instance (default=torch.nn.ReLU())\n      Non-linearity to apply after hidden layers.\n\n    dropout : float (default=0)\n      Dropout rate. Dropout is applied between layers.\n\n    """"""\n    return partial(MLPModule, output_nonlin=output_nonlin, **kwargs)\n\n\ndef make_binary_classifier(squeeze_output=True, **kwargs):\n    """"""Return a multi-layer perceptron to be used with\n    NeuralNetBinaryClassifier.\n\n    Parameters\n    ----------\n    input_units : int (default=20)\n      Number of input units.\n\n    output_units : int (default=2)\n      Number of output units.\n\n    hidden_units : int (default=10)\n      Number of units in hidden layers.\n\n    num_hidden : int (default=1)\n      Number of hidden layers.\n\n    nonlin : torch.nn.Module instance (default=torch.nn.ReLU())\n      Non-linearity to apply after hidden layers.\n\n    dropout : float (default=0)\n      Dropout rate. Dropout is applied between layers.\n\n    """"""\n    return partial(MLPModule, squeeze_output=squeeze_output, **kwargs)\n\n\ndef make_regressor(output_units=1, **kwargs):\n    """"""Return a multi-layer perceptron to be used with\n    NeuralNetRegressor.\n\n    Parameters\n    ----------\n    input_units : int (default=20)\n      Number of input units.\n\n    output_units : int (default=1)\n      Number of output units.\n\n    hidden_units : int (default=10)\n      Number of units in hidden layers.\n\n    num_hidden : int (default=1)\n      Number of hidden layers.\n\n    nonlin : torch.nn.Module instance (default=torch.nn.ReLU())\n      Non-linearity to apply after hidden layers.\n\n    dropout : float (default=0)\n      Dropout rate. Dropout is applied between layers.\n\n    """"""\n    return partial(MLPModule, output_units=output_units, **kwargs)\n'"
skorch/utils.py,18,"b'""""""skorch utilities.\n\nShould not have any dependency on other skorch packages.\n\n""""""\n\nfrom collections.abc import Sequence\nfrom contextlib import contextmanager\nfrom enum import Enum\nfrom functools import partial\nfrom itertools import tee\nfrom distutils.version import LooseVersion\nimport pathlib\nimport warnings\n\nimport numpy as np\nfrom scipy import sparse\nimport sklearn\nimport torch\nfrom torch.nn.utils.rnn import PackedSequence\nfrom torch.utils.data.dataset import Subset\n\nfrom skorch.exceptions import DeviceWarning\nfrom skorch.exceptions import NotInitializedError\n\nif LooseVersion(sklearn.__version__) >= \'0.22.0\':\n    from sklearn.utils import _safe_indexing as safe_indexing\nelse:\n    from sklearn.utils import safe_indexing\n\n\nclass Ansi(Enum):\n    BLUE = \'\\033[94m\'\n    CYAN = \'\\033[36m\'\n    GREEN = \'\\033[32m\'\n    MAGENTA = \'\\033[35m\'\n    RED = \'\\033[31m\'\n    ENDC = \'\\033[0m\'\n\n\ndef is_torch_data_type(x):\n    # pylint: disable=protected-access\n    return isinstance(x, (torch.Tensor, PackedSequence))\n\n\ndef is_dataset(x):\n    return isinstance(x, torch.utils.data.Dataset)\n\n\n# pylint: disable=not-callable\ndef to_tensor(X, device, accept_sparse=False):\n    """"""Turn input data to torch tensor.\n\n    Parameters\n    ----------\n    X : input data\n      Handles the cases:\n        * PackedSequence\n        * numpy array\n        * torch Tensor\n        * scipy sparse CSR matrix\n        * list or tuple of one of the former\n        * dict with values of one of the former\n\n    device : str, torch.device\n      The compute device to be used. If set to \'cuda\', data in torch\n      tensors will be pushed to cuda tensors before being sent to the\n      module.\n\n    accept_sparse : bool (default=False)\n      Whether to accept scipy sparse matrices as input. If False,\n      passing a sparse matrix raises an error. If True, it is\n      converted to a torch COO tensor.\n\n    Returns\n    -------\n    output : torch Tensor\n\n    """"""\n    to_tensor_ = partial(to_tensor, device=device)\n\n    if is_torch_data_type(X):\n        return to_device(X, device)\n    if isinstance(X, dict):\n        return {key: to_tensor_(val) for key, val in X.items()}\n    if isinstance(X, (list, tuple)):\n        return [to_tensor_(x) for x in X]\n    if np.isscalar(X):\n        return torch.as_tensor(X, device=device)\n    if isinstance(X, Sequence):\n        return torch.as_tensor(np.array(X), device=device)\n    if isinstance(X, np.ndarray):\n        return torch.as_tensor(X, device=device)\n    if sparse.issparse(X):\n        if accept_sparse:\n            return torch.sparse_coo_tensor(\n                X.nonzero(), X.data, size=X.shape).to(device)\n        raise TypeError(""Sparse matrices are not supported. Set ""\n                        ""accept_sparse=True to allow sparse matrices."")\n\n    raise TypeError(""Cannot convert this data type to a torch tensor."")\n\n\ndef to_numpy(X):\n    """"""Generic function to convert a pytorch tensor to numpy.\n\n    Returns X when it already is a numpy array.\n\n    """"""\n    if isinstance(X, np.ndarray):\n        return X\n\n    if is_pandas_ndframe(X):\n        return X.values\n\n    if not is_torch_data_type(X):\n        raise TypeError(""Cannot convert this data type to a numpy array."")\n\n    if X.is_cuda:\n        X = X.cpu()\n\n    if X.requires_grad:\n        X = X.detach()\n\n    return X.numpy()\n\n\ndef to_device(X, device):\n    """"""Generic function to modify the device type of the tensor(s) or module.\n\n    Parameters\n    ----------\n    X : input data\n        Deals with X being a:\n\n         * torch tensor\n         * tuple of torch tensors\n         * PackSequence instance\n         * torch.nn.Module\n\n    device : str, torch.device\n        The compute device to be used. If device=None, return the input\n        unmodified\n\n    """"""\n    if device is None:\n        return X\n\n    # PackedSequence class inherits from a namedtuple\n    if isinstance(X, tuple) and (type(X) != PackedSequence):\n        return tuple(x.to(device) for x in X)\n    return X.to(device)\n\n\ndef get_dim(y):\n    """"""Return the number of dimensions of a torch tensor or numpy\n    array-like object.\n\n    """"""\n    try:\n        return y.ndim\n    except AttributeError:\n        return y.dim()\n\n\ndef is_pandas_ndframe(x):\n    # the sklearn way of determining this\n    return hasattr(x, \'iloc\')\n\n\ndef flatten(arr):\n    for item in arr:\n        if isinstance(item, (tuple, list, dict)):\n            yield from flatten(item)\n        else:\n            yield item\n\n\n# pylint: disable=unused-argument\ndef _indexing_none(data, i):\n    return None\n\n\ndef _indexing_dict(data, i):\n    return {k: v[i] for k, v in data.items()}\n\n\ndef _indexing_list_tuple_of_data(data, i, indexings=None):\n    """"""Data is a list/tuple of data structures (e.g. list of numpy arrays).\n\n    ``indexings`` are the indexing functions for each element of\n    ``data``. If ``indexings`` are not given, the indexing functions\n    for the individual structures have to be determined ad hoc, which\n    is slower.\n\n    """"""\n    if not indexings:\n        return [multi_indexing(x, i) for x in data]\n    return [multi_indexing(x, i, indexing)\n            for x, indexing in zip(data, indexings)]\n\n\ndef _indexing_ndframe(data, i):\n    # During fit, DataFrames are converted to dict, which is why we\n    # might need _indexing_dict.\n    if hasattr(data, \'iloc\'):\n        return data.iloc[i]\n    return _indexing_dict(data, i)\n\n\ndef _indexing_other(data, i):\n    # sklearn\'s safe_indexing doesn\'t work with tuples since 0.22\n    if isinstance(i, (int, np.integer, slice, tuple)):\n        return data[i]\n    return safe_indexing(data, i)\n\n\ndef check_indexing(data):\n    """"""Perform a check how incoming data should be indexed and return an\n    appropriate indexing function with signature f(data, index).\n\n    This is useful for determining upfront how data should be indexed\n    instead of doing it repeatedly for each batch, thus saving some\n    time.\n\n    """"""\n    if data is None:\n        return _indexing_none\n\n    if isinstance(data, dict):\n        # dictionary of containers\n        return _indexing_dict\n\n    if isinstance(data, (list, tuple)):\n        try:\n            # list or tuple of containers\n            # TODO: Is there a better way than just to try to index? This\n            # is error prone (e.g. if one day list of strings are\n            # possible).\n            multi_indexing(data[0], 0)\n            indexings = [check_indexing(x) for x in data]\n            return partial(_indexing_list_tuple_of_data, indexings=indexings)\n        except TypeError:\n            # list or tuple of values\n            return _indexing_other\n\n    if is_pandas_ndframe(data):\n        # pandas NDFrame, will be transformed to dict\n        return _indexing_ndframe\n\n    # torch tensor, numpy ndarray, list\n    return _indexing_other\n\n\ndef _normalize_numpy_indices(i):\n    """"""Normalize the index in case it is a numpy integer or boolean\n    array.""""""\n    if isinstance(i, np.ndarray):\n        if i.dtype == bool:\n            i = tuple(j.tolist() for j in i.nonzero())\n        elif i.dtype == int:\n            i = i.tolist()\n    return i\n\n\ndef multi_indexing(data, i, indexing=None):\n    """"""Perform indexing on multiple data structures.\n\n    Currently supported data types:\n\n    * numpy arrays\n    * torch tensors\n    * pandas NDFrame\n    * a dictionary of the former three\n    * a list/tuple of the former three\n\n    ``i`` can be an integer or a slice.\n\n    Examples\n    --------\n    >>> multi_indexing(np.asarray([1, 2, 3]), 0)\n    1\n\n    >>> multi_indexing(np.asarray([1, 2, 3]), np.s_[:2])\n    array([1, 2])\n\n    >>> multi_indexing(torch.arange(0, 4), np.s_[1:3])\n    tensor([ 1.,  2.])\n\n    >>> multi_indexing([[1, 2, 3], [4, 5, 6]], np.s_[:2])\n    [[1, 2], [4, 5]]\n\n    >>> multi_indexing({\'a\': [1, 2, 3], \'b\': [4, 5, 6]}, np.s_[-2:])\n    {\'a\': [2, 3], \'b\': [5, 6]}\n\n    >>> multi_indexing(pd.DataFrame({\'a\': [1, 2, 3], \'b\': [4, 5, 6]}), [1, 2])\n       a  b\n    1  2  5\n    2  3  6\n\n    Parameters\n    ----------\n    data\n      Data of a type mentioned above.\n\n    i : int or slice\n      Slicing index.\n\n    indexing : function/callable or None (default=None)\n      If not None, use this function for indexing into the data. If\n      None, try to automatically determine how to index data.\n\n    """"""\n    # in case of i being a numpy array\n    i = _normalize_numpy_indices(i)\n\n    # If we already know how to index, use that knowledge\n    if indexing is not None:\n        return indexing(data, i)\n\n    # If we don\'t know how to index, find out and apply\n    return check_indexing(data)(data, i)\n\n\ndef duplicate_items(*collections):\n    """"""Search for duplicate items in all collections.\n\n    Examples\n    --------\n    >>> duplicate_items([1, 2], [3])\n    set()\n    >>> duplicate_items({1: \'a\', 2: \'a\'})\n    set()\n    >>> duplicate_items([\'a\', \'b\', \'a\'])\n    {\'a\'}\n    >>> duplicate_items([1, 2], {3: \'hi\', 4: \'ha\'}, (2, 3))\n    {2, 3}\n\n    """"""\n    duplicates = set()\n    seen = set()\n    for item in flatten(collections):\n        if item in seen:\n            duplicates.add(item)\n        else:\n            seen.add(item)\n    return duplicates\n\n\ndef params_for(prefix, kwargs):\n    """"""Extract parameters that belong to a given sklearn module prefix from\n    ``kwargs``. This is useful to obtain parameters that belong to a\n    submodule.\n\n    Examples\n    --------\n    >>> kwargs = {\'encoder__a\': 3, \'encoder__b\': 4, \'decoder__a\': 5}\n    >>> params_for(\'encoder\', kwargs)\n    {\'a\': 3, \'b\': 4}\n\n    """"""\n    if not prefix.endswith(\'__\'):\n        prefix += \'__\'\n    return {key[len(prefix):]: val for key, val in kwargs.items()\n            if key.startswith(prefix)}\n\n\n# pylint: disable=invalid-name\nclass _none:\n    pass\n\n\ndef data_from_dataset(dataset, X_indexing=None, y_indexing=None):\n    """"""Try to access X and y attribute from dataset.\n\n    Also works when dataset is a subset.\n\n    Parameters\n    ----------\n    dataset : skorch.dataset.Dataset or torch.utils.data.Subset\n      The incoming dataset should be a ``skorch.dataset.Dataset`` or a\n      ``torch.utils.data.Subset`` of a\n      ``skorch.dataset.Dataset``.\n\n    X_indexing : function/callable or None (default=None)\n      If not None, use this function for indexing into the X data. If\n      None, try to automatically determine how to index data.\n\n    y_indexing : function/callable or None (default=None)\n      If not None, use this function for indexing into the y data. If\n      None, try to automatically determine how to index data.\n\n    """"""\n    X, y = _none, _none\n\n    if isinstance(dataset, Subset):\n        X, y = data_from_dataset(\n            dataset.dataset, X_indexing=X_indexing, y_indexing=y_indexing)\n        X = multi_indexing(X, dataset.indices, indexing=X_indexing)\n        y = multi_indexing(y, dataset.indices, indexing=y_indexing)\n    elif hasattr(dataset, \'X\') and hasattr(dataset, \'y\'):\n        X, y = dataset.X, dataset.y\n\n    if (X is _none) or (y is _none):\n        raise AttributeError(""Could not access X and y from dataset."")\n    return X, y\n\n\ndef is_skorch_dataset(ds):\n    """"""Checks if the supplied dataset is an instance of\n    ``skorch.dataset.Dataset`` even when it is nested inside\n    ``torch.util.data.Subset``.""""""\n    from skorch.dataset import Dataset\n    if isinstance(ds, Subset):\n        return is_skorch_dataset(ds.dataset)\n    return isinstance(ds, Dataset)\n\n\n# pylint: disable=unused-argument\ndef noop(*args, **kwargs):\n    """"""No-op function that does nothing and returns ``None``.\n\n    This is useful for defining scoring callbacks that do not need a\n    target extractor.\n    """"""\n\n\n@contextmanager\ndef open_file_like(f, mode):\n    """"""Wrapper for opening a file""""""\n    new_fd = isinstance(f, (str, pathlib.Path))\n    if new_fd:\n        f = open(f, mode)\n    try:\n        yield f\n    finally:\n        if new_fd:\n            f.close()\n\n\n# pylint: disable=unused-argument\ndef train_loss_score(net, X=None, y=None):\n    return net.history[-1, \'batches\', -1, \'train_loss\']\n\n\n# pylint: disable=unused-argument\ndef valid_loss_score(net, X=None, y=None):\n    return net.history[-1, \'batches\', -1, \'valid_loss\']\n\n\nclass FirstStepAccumulator:\n    """"""Store and retrieve the train step data.\n\n    This class simply stores the first step value and returns it.\n\n    For most uses, ``skorch.utils.FirstStepAccumulator`` is what you\n    want, since the optimizer calls the train step exactly\n    once. However, some optimizerss such as LBFGSs make more than one\n    call. If in that case, you don\'t want the first value to be\n    returned (but instead, say, the last value), implement your own\n    accumulator and make sure it is returned by\n    ``NeuralNet.get_train_step_accumulator`` method.\n\n    """"""\n    def __init__(self):\n        self.step = None\n\n    def store_step(self, step):\n        """"""Store the first step.""""""\n        if self.step is None:\n            self.step = step\n\n    def get_step(self):\n        """"""Return the stored step.""""""\n        return self.step\n\n\ndef _make_split(X, y, valid_ds, **kwargs):\n    """"""Used by ``predefined_split`` to allow for pickling""""""\n    return X, valid_ds\n\n\ndef freeze_parameter(param):\n    """"""Convenience function to freeze a passed torch parameter.\n    Used by ``skorch.callbacks.Freezer``\n    """"""\n    param.requires_grad = False\n\n\ndef unfreeze_parameter(param):\n    """"""Convenience function to unfreeze a passed torch parameter.\n    Used by ``skorch.callbacks.Unfreezer``\n    """"""\n    param.requires_grad = True\n\n\ndef get_map_location(target_device, fallback_device=\'cpu\'):\n    """"""Determine the location to map loaded data (e.g., weights)\n    for a given target device (e.g. \'cuda\').\n    """"""\n    if target_device is None:\n        target_device = fallback_device\n\n    map_location = torch.device(target_device)\n\n    # The user wants to use CUDA but there is no CUDA device\n    # available, thus fall back to CPU.\n    if map_location.type == \'cuda\' and not torch.cuda.is_available():\n        warnings.warn(\n            \'Requested to load data to CUDA but no CUDA devices \'\n            \'are available. Loading on device ""{}"" instead.\'.format(\n                fallback_device,\n            ), DeviceWarning)\n        map_location = torch.device(fallback_device)\n    return map_location\n\n\ndef check_is_fitted(estimator, attributes, msg=None, all_or_any=all):\n    """"""Checks whether the net is initialized.\n\n    Note: This calls ``sklearn.utils.validation.check_is_fitted``\n    under the hood, using exactly the same arguments and logic. The\n    only difference is that this function has an adapted error message\n    and raises a ``skorch.exception.NotInitializedError`` instead of\n    an ``sklearn.exceptions.NotFittedError``.\n\n    """"""\n    if msg is None:\n        msg = (""This %(name)s instance is not initialized yet. Call ""\n               ""\'initialize\' or \'fit\' with appropriate arguments ""\n               ""before using this method."")\n\n\n    if not isinstance(attributes, (list, tuple)):\n        attributes = [attributes]\n\n    if not all_or_any([hasattr(estimator, attr) for attr in attributes]):\n        raise NotInitializedError(msg % {\'name\': type(estimator).__name__})\n\n\nclass TeeGenerator:\n    """"""Stores a generator and calls ``tee`` on it to create new generators\n    when ``TeeGenerator`` is iterated over to let you iterate over the given\n    generator more than once.\n\n    """"""\n    def __init__(self, gen):\n        self.gen = gen\n\n    def __iter__(self):\n        self.gen, it = tee(self.gen)\n        yield from it\n'"
examples/benchmarks/freezing.py,5,"b'""""""Benchmark to test runtime and memory performance of\ndifferent freezing approaches.\n\nTest A is done by setting `requires_grad=False` manually\nwhile filtering these parameters from the optimizer using\n``skorch.helper.filtered_optimizer``.\n\nTest B uses the ``Freezer`` via ``ParamMapper`` without\nexplicitly removing the parameters from the optimizer.\n\nIn theory there should be no difference in memory\nconsumption and runtime.\n""""""\nfrom functools import partial\nimport resource\nfrom multiprocessing import Process, Queue\n\nimport torch\nimport skorch\nimport skorch.helper\nfrom skorch.toy import make_classifier\nimport sklearn.datasets\nimport numpy as np\n\n\nX, y = sklearn.datasets.make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_classes=2,\n    random_state=0)\nX = X.astype(\'float32\')\ny = y.astype(\'int64\')\n\nN_LAYERS = 2\n\nmake_module_cls = partial(\n    make_classifier,\n    num_hidden=N_LAYERS,\n    input_units=2,\n    hidden_units=100,\n    output_units=2,\n)\n\nlinear_idcs = list(range(0, (N_LAYERS+1)*3, 3))\n\nnp.random.seed(0)\ntorch.manual_seed(0)\n\n\ndef test_a():\n    # -- first by stripping parameters explicitly\n    np.random.seed(0)\n    torch.manual_seed(0)\n\n    print(\'1\', end=\'\')\n    mod = make_module_cls()()\n\n    # freeze all params but last layer\n    for i in linear_idcs[:-1]:\n        skorch.utils.freeze_parameter(mod.sequential[i].weight)\n        skorch.utils.freeze_parameter(mod.sequential[i].bias)\n\n    opt = skorch.helper.filtered_optimizer(\n        torch.optim.SGD,\n        skorch.helper.filter_requires_grad)\n\n    net = skorch.NeuralNetClassifier(\n        mod,\n        verbose=0,\n        optimizer=opt,\n        warm_start=True)\n\n    for i in linear_idcs[:-1]:\n        assert not mod.sequential[i].weight.requires_grad\n\n    net.fit(X, y)\n\n    for i in linear_idcs[:-1]:\n        assert not mod.sequential[i].weight.requires_grad\n        assert not net.module_.sequential[i].weight.requires_grad\n\n    rss = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n    return rss, net.history[-1, \'valid_loss\'], np.mean(net.history[:, \'dur\'])\n\n\ndef test_b():\n    # -- second by simply freezing them\n    np.random.seed(0)\n    torch.manual_seed(0)\n\n    print(\'2\', end=\'\')\n    mod = make_module_cls()()\n\n    opt = torch.optim.SGD\n    cb = skorch.callbacks.Freezer(\n        [\'sequential.{}.weight\'.format(i) for i in linear_idcs[:-1]] +\n        [\'sequential.{}.bias\'.format(i) for i in linear_idcs[:-1]]\n    )\n\n    net = skorch.NeuralNetClassifier(\n        mod,\n        verbose=0,\n        optimizer=opt,\n        callbacks=[cb])\n    net.fit(X, y)\n\n    for i in linear_idcs[:-1]:\n        assert not mod.sequential[i].weight.requires_grad\n        assert not net.module_.sequential[i].weight.requires_grad\n\n    rss = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n    return rss, net.history[-1, \'valid_loss\'], np.mean(net.history[:, \'dur\'])\n\ndef test_runner(q, fn, n_runs):\n    q.put(np.mean([fn() for _ in range(n_runs)], axis=0))\n\ndef test_forker(test_fn, n_runs):\n    q = Queue()\n    p = Process(target=test_runner, args=(q, test_fn, n_runs))\n    p.start()\n    res = q.get()\n    p.join()\n    return res\n\nif __name__ == \'__main__\':\n    n_runs = 10\n    print(f\'running tests for {n_runs} runs each.\')\n\n    # We fork the tests so that each has its own process and thus its\n    # own memory allocation. Therefore tests don\'t influence each other.\n    dur_a = test_forker(test_a, n_runs)\n    print()\n\n    dur_b = test_forker(test_b, n_runs)\n    print()\n\n    print(f\'test_a: \xc2\xb5_rss = {dur_a[0]}, \xc2\xb5_valid_loss = {dur_a[1]}, \xc2\xb5_dur={dur_a[2]}\')\n    print(f\'test_a: \xc2\xb5_rss = {dur_b[0]}, \xc2\xb5_valid_loss = {dur_b[1]}, \xc2\xb5_dur={dur_b[2]}\')\n\n    # valid losses must be identical\n    assert np.allclose(dur_a[1], dur_b[1])\n\n    # memory usage should be nearly identical (within 4MiB)\n    assert np.allclose(dur_a[0], dur_b[0], atol=4*1024**2)\n\n    # duration should be nearly identical\n    assert np.allclose(dur_a[2], dur_b[2], atol=0.5)\n'"
examples/benchmarks/history.py,2,"b'""""""Benchmark to test time and memory performance of History.\n\nBefore #312, the timing would be roughly 5 sec and memory usage would\ntriple. After #312, the timing would be roughly 2 sec and memory usage\nroughly constant.\n\nFor the reasons, see #306.\n\n""""""\n\nfrom pprint import pprint\nimport time\n\nimport numpy as np\nfrom sklearn.datasets import make_classification\nimport torch\n\nfrom skorch import NeuralNetClassifier\nfrom skorch.callbacks import Callback\nfrom skorch.toy import make_classifier\n\n\nside_effects = []\n\n\nclass TriggerKeyError(Callback):\n    def on_batch_end(self, net, **kwargs):\n        try:\n            net.history[-1, \'batches\', -1, \'foobar\']\n        except Exception as e:\n            pass\n\n\nclass PrintMemory(Callback):\n    def on_batch_end(self, net, **kwargs):\n        side_effects.append((\n            torch.cuda.memory_allocated() / 1e6,\n            torch.cuda.memory_cached() / 1e6\n        ))\n\n\ndef train():\n    X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n    X = X.astype(np.float32)\n    y = y.astype(np.int64)\n\n    module = make_classifier(input_units=20)\n\n    net = NeuralNetClassifier(\n        module,\n        max_epochs=10,\n        lr=0.1,\n        callbacks=[TriggerKeyError(), PrintMemory()],\n        device=\'cuda\',\n    )\n\n    return net.fit(X, y)\n\n\ndef safe_slice(history, keys):\n    # catch errors\n    for key in keys:\n        try:\n            history[key]\n        except (KeyError, IndexError):\n            pass\n\n\ndef performance_history(history):\n    # SUCCESSFUL\n    # level 0\n    for i in range(len(history)):\n        history[i]\n\n    # level 1\n    keys = tuple(history[0].keys())\n    history[0, keys]\n    history[:, keys]\n    for key in keys:\n        history[0, key]\n        history[:, key]\n\n    # level 2\n    for i in range(len(history[0, \'batches\'])):\n        history[0, \'batches\', i]\n        history[:, \'batches\', i]\n    history[:, \'batches\', :]\n\n    # level 3\n    keys = tuple(history[0, \'batches\', 0].keys())\n    history[0, \'batches\', 0, keys]\n    history[:, \'batches\', 0, keys]\n    history[0, \'batches\', :, keys]\n    history[:, \'batches\', :, keys]\n    for key in history[0, \'batches\', 0]:\n        history[0, \'batches\', 0, key]\n        history[:, \'batches\', 0, key]\n        history[0, \'batches\', :, key]\n        history[:, \'batches\', :, key]\n\n    # KEY ERRORS\n    # level 0\n    safe_slice(history, [100000])\n\n    # level 1\n    safe_slice(history, [np.s_[0, \'foo\'], np.s_[:, \'foo\']])\n\n    # level 2\n    safe_slice(history, [\n        np.s_[0, \'batches\', 0],\n        np.s_[:, \'batches\', 0],\n        np.s_[0, \'batches\', :],\n        np.s_[:, \'batches\', :],\n    ])\n\n    # level 3\n    safe_slice(history, [\n        np.s_[0, \'batches\', 0, \'foo\'],\n        np.s_[:, \'batches\', 0, \'foo\'],\n        np.s_[0, \'batches\', :, \'foo\'],\n        np.s_[:, \'batches\', :, \'foo\'],\n        np.s_[0, \'batches\', 0, (\'foo\', \'bar\')],\n        np.s_[:, \'batches\', 0, (\'foo\', \'bar\')],\n        np.s_[0, \'batches\', :, (\'foo\', \'bar\')],\n        np.s_[:, \'batches\', :, (\'foo\', \'bar\')],\n    ])\n\nif __name__ == \'__main__\':\n    net = train()\n    tic = time.time()\n    for _ in range(1000):\n        performance_history(net.history)\n    toc = time.time()\n    print(""Time for performing 1000 runs: {:.5f} sec."".format(toc - tic))\n    assert toc - tic < 10, ""accessing history is too slow""\n\n    print(""Allocated / cached memory"")\n    pprint(side_effects)\n\n    mem_start = side_effects[0][0]\n    mem_end = side_effects[-1][0]\n\n    print(""Memory epoch 1: {:.4f}, last epoch: {:.4f}"".format(\n        mem_start, mem_end))\n    assert np.isclose(mem_start, mem_end, rtol=1/3), ""memory use should be similar""\n'"
examples/benchmarks/mnist.py,21,"b'""""""This script trains a NeuralNetClassifier on MNIST data, once with\nskorch, once with pure PyTorch.\n\nApart from that change, both approaches are as close to eachother as\npossible (e.g. performing the same validation steps).\n\nAt the end, we assert that the test accuracies are very close (1%\ndifference) and that skorch is not too much slower (at worst 30%\nslower).\n\nCall like this:\n\n```\npython examples/benchmarks/mnist.py\npython examples/benchmarks/mnist.py --device cpu --num_samples 5000\n```\n\nWhen called the first time, this will download MNIST data to\nexamples/datasets/mldata (53M).\n\nThis script uses similar parameters as this one:\nhttps://github.com/keras-team/keras/blob/0de2adf04b37aa972c955e69caf6917372b70a5b/examples/mnist_cnn.py\n\n\n""""""\n\nimport argparse\nimport os\nimport time\n\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.utils import shuffle\nfrom skorch.utils import to_device\nfrom skorch import NeuralNetClassifier\nfrom skorch.callbacks import EpochScoring\nimport torch\nfrom torch import nn\n\n\nBATCH_SIZE = 128\nLEARNING_RATE = 0.1\nMAX_EPOCHS = 12\n\n\ndef get_data(num_samples):\n    mnist = fetch_openml(\'mnist_784\')\n    torch.manual_seed(0)\n    X = mnist.data.astype(\'float32\').reshape(-1, 1, 28, 28)\n    y = mnist.target.astype(\'int64\')\n    X, y = shuffle(X, y)\n    X, y = X[:num_samples], y[:num_samples]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n    X_train /= 255\n    X_test /= 255\n    return X_train, X_test, y_train, y_test\n\n\nclass ClassifierModule(nn.Module):\n    def __init__(self):\n        super(ClassifierModule, self).__init__()\n\n        self.cnn = nn.Sequential(\n            nn.Conv2d(1, 32, (3, 3)),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, (3, 3)),\n            nn.ReLU(),\n            nn.MaxPool2d((2, 2)),\n            nn.Dropout(0.25),\n        )\n        self.out = nn.Sequential(\n            nn.Linear(64 * 12 * 12, 128),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(128, 10),\n            nn.Softmax(dim=-1),\n        )\n\n    def forward(self, X, **kwargs):\n        X = self.cnn(X)\n        X = X.reshape(-1, 64 * 12 * 12)\n        X = self.out(X)\n        return X\n\n\ndef performance_skorch(\n        X_train,\n        X_test,\n        y_train,\n        y_test,\n        batch_size,\n        device,\n        lr,\n        max_epochs,\n):\n    torch.manual_seed(0)\n    net = NeuralNetClassifier(\n        ClassifierModule,\n        batch_size=batch_size,\n        optimizer=torch.optim.Adadelta,\n        lr=lr,\n        device=device,\n        max_epochs=max_epochs,\n        callbacks=[\n            (\'tr_acc\', EpochScoring(\n                \'accuracy\',\n                lower_is_better=False,\n                on_train=True,\n                name=\'train_acc\',\n            )),\n        ],\n    )\n    net.fit(X_train, y_train)\n    y_pred = net.predict(X_test)\n    score = accuracy_score(y_test, y_pred)\n    return score\n\n\ndef report(losses, batch_sizes, y, y_proba, epoch, time, training=True):\n    template = ""{} | epoch {:>2} | ""\n\n    loss = np.average(losses, weights=batch_sizes)\n    y_pred = np.argmax(y_proba, axis=1)\n    acc = accuracy_score(y, y_pred)\n\n    template += ""acc: {:.4f} | loss: {:.4f} | time: {:.2f}""\n    print(template.format(\n        \'train\' if training else \'valid\', epoch + 1, acc, loss, time))\n\n\ndef train_torch(\n        model,\n        X,\n        X_test,\n        y,\n        y_test,\n        batch_size,\n        device,\n        lr,\n        max_epochs,\n):\n    model = to_device(model, device)\n\n    idx_train, idx_valid = next(iter(StratifiedKFold(\n        5, random_state=0).split(np.arange(len(X)), y)))\n    X_train, X_valid, y_train, y_valid = (\n        X[idx_train], X[idx_valid], y[idx_train], y[idx_valid])\n    dataset_train = torch.utils.data.TensorDataset(\n        torch.tensor(X_train),\n        torch.tensor(y_train),\n    )\n    dataset_valid = torch.utils.data.TensorDataset(\n        torch.tensor(X_valid),\n        torch.tensor(y_valid),\n    )\n\n    optimizer = torch.optim.Adadelta(model.parameters(), lr=lr)\n    criterion = nn.NLLLoss()\n\n    for epoch in range(max_epochs):\n        train_out = train_step(\n            model,\n            dataset_train,\n            batch_size=batch_size,\n            device=device,\n            criterion=criterion,\n            optimizer=optimizer,\n        )\n        report(y=y_train, epoch=epoch, training=True, **train_out)\n\n        valid_out = valid_step(\n            model,\n            dataset_valid,\n            batch_size=batch_size,\n            device=device,\n            criterion=criterion,\n        )\n        report(y=y_valid, epoch=epoch, training=False, **valid_out)\n\n        print(\'-\' * 50)\n\n    return model\n\n\ndef train_step(model, dataset, device, criterion, batch_size, optimizer):\n    model.train()\n    y_preds = []\n    losses = []\n    batch_sizes = []\n    tic = time.time()\n    for Xi, yi in torch.utils.data.DataLoader(dataset, batch_size=batch_size):\n        Xi, yi = to_device(Xi, device), to_device(yi, device)\n        optimizer.zero_grad()\n        y_pred = model(Xi)\n        y_pred = torch.log(y_pred)\n        loss = criterion(y_pred, yi)\n        loss.backward()\n        optimizer.step()\n\n        y_preds.append(y_pred)\n        losses.append(loss.item())\n        batch_sizes.append(len(Xi))\n    toc = time.time()\n    return {\n        \'losses\': losses,\n        \'batch_sizes\': batch_sizes,\n        \'y_proba\': torch.cat(y_preds).cpu().detach().numpy(),\n        \'time\': toc - tic,\n    }\n\n\ndef valid_step(model, dataset, device, criterion, batch_size):\n    model.eval()\n    y_preds = []\n    losses = []\n    batch_sizes = []\n    tic = time.time()\n    with torch.no_grad():\n        for Xi, yi in torch.utils.data.DataLoader(\n                dataset, batch_size=batch_size,\n        ):\n            Xi, yi = to_device(Xi, device), to_device(yi, device)\n            y_pred = model(Xi)\n            y_pred = torch.log(y_pred)\n            loss = criterion(y_pred, yi)\n\n            y_preds.append(y_pred)\n            loss = loss.item()\n            losses.append(loss)\n            batch_sizes.append(len(Xi))\n    toc = time.time()\n    return {\n        \'losses\': losses,\n        \'batch_sizes\': batch_sizes,\n        \'y_proba\': torch.cat(y_preds).cpu().detach().numpy(),\n        \'time\': toc - tic,\n    }\n\n\ndef performance_torch(\n        X_train,\n        X_test,\n        y_train,\n        y_test,\n        batch_size,\n        device,\n        lr,\n        max_epochs,\n):\n    torch.manual_seed(0)\n    model = ClassifierModule()\n    model = train_torch(\n        model,\n        X_train,\n        X_test,\n        y_train,\n        y_test,\n        batch_size=batch_size,\n        device=device,\n        max_epochs=max_epochs,\n        lr=0.1,\n    )\n\n    X_test = torch.tensor(X_test).to(device)\n    with torch.no_grad():\n        y_pred = model(X_test).cpu().numpy().argmax(1)\n    return accuracy_score(y_test, y_pred)\n\n\ndef main(device, num_samples):\n    data = get_data(num_samples)\n    # trigger potential cuda call overhead\n    torch.zeros(1).to(device)\n\n    if True:\n        print(""\\nTesting skorch performance"")\n        tic = time.time()\n        score_skorch = performance_skorch(\n            *data,\n            batch_size=BATCH_SIZE,\n            max_epochs=MAX_EPOCHS,\n            lr=LEARNING_RATE,\n            device=device,\n        )\n        time_skorch = time.time() - tic\n\n    if True:\n        print(""\\nTesting pure torch performance"")\n        tic = time.time()\n        score_torch = performance_torch(\n            *data,\n            batch_size=BATCH_SIZE,\n            max_epochs=MAX_EPOCHS,\n            lr=LEARNING_RATE,\n            device=device,\n        )\n        time_torch = time.time() - tic\n\n    print(""time skorch: {:.4f}, time torch: {:.4f}"".format(\n        time_skorch, time_torch))\n    print(""score skorch: {:.4f}, score torch: {:.4f}"".format(\n        score_skorch, score_torch))\n\n    assert np.isclose(score_skorch, score_torch, rtol=0.01), ""Scores are not close enough.""\n    assert np.isclose(time_skorch, time_torch, rtol=0.3), ""Times are not close enough.""\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=""skorch MNIST benchmark"")\n    parser.add_argument(\'--device\', type=str, default=\'cuda\',\n                        help=\'device (e.g. ""cuda"", ""cpu"")\')\n    parser.add_argument(\'--num_samples\', type=int, default=20000,\n                        help=\'total number of samples to use\')\n    args = parser.parse_args()\n    main(device=args.device, num_samples=args.num_samples)\n'"
examples/cli/train.py,3,"b'""""""Simple training script for a MLP classifier.\n\nSee accompanying README.md for more details.\n\n""""""\n\nimport pickle\n\nimport fire\nimport numpy as np\nfrom sklearn.datasets import make_classification\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import Normalizer\nfrom skorch import NeuralNetClassifier\nimport torch\nfrom torch import nn\n\nfrom skorch.helper import parse_args\n\n\nnp.random.seed(0)\ntorch.manual_seed(0)\ntorch.cuda.manual_seed(0)\n\n\n# number of input features\nN_FEATURES = 20\n\n# number of classes\nN_CLASSES = 2\n\n# custom defaults for net\nDEFAULTS_NET = {\n    \'batch_size\': 256,\n    \'module__hidden_units\': 30,\n}\n\n# custom defaults for pipeline\nDEFAULTS_PIPE = {\n    \'scale__minmax__feature_range\': (-1, 1),\n    \'net__batch_size\': 256,\n    \'net__module__hidden_units\': 30,\n}\n\n\nclass MLPClassifier(nn.Module):\n    """"""A simple multi-layer perceptron module.\n\n    This can be adapted for usage in different contexts, e.g. binary\n    and multi-class classification, regression, etc.\n\n    Note: This docstring is used to create the help for the CLI.\n\n    Parameters\n    ----------\n    hidden_units : int (default=10)\n      Number of units in hidden layers.\n\n    num_hidden : int (default=1)\n      Number of hidden layers.\n\n    nonlin : torch.nn.Module instance (default=torch.nn.ReLU())\n      Non-linearity to apply after hidden layers.\n\n    dropout : float (default=0)\n      Dropout rate. Dropout is applied between layers.\n\n    """"""\n    def __init__(\n            self,\n            hidden_units=10,\n            num_hidden=1,\n            nonlin=nn.ReLU(),\n            dropout=0,\n    ):\n        super().__init__()\n        self.hidden_units = hidden_units\n        self.num_hidden = num_hidden\n        self.nonlin = nonlin\n        self.dropout = dropout\n\n        self.reset_params()\n\n    def reset_params(self):\n        """"""(Re)set all parameters.""""""\n        units = [N_FEATURES]\n        units += [self.hidden_units] * self.num_hidden\n        units += [N_CLASSES]\n\n        sequence = []\n        for u0, u1 in zip(units, units[1:]):\n            sequence.append(nn.Linear(u0, u1))\n            sequence.append(self.nonlin)\n            sequence.append(nn.Dropout(self.dropout))\n\n        sequence = sequence[:-2]\n        self.sequential = nn.Sequential(*sequence)\n\n    def forward(self, X):\n        return nn.Softmax(dim=-1)(self.sequential(X))\n\n\ndef get_data(n_samples=100):\n    """"""Get synthetic classification data with n_samples samples.""""""\n    X, y = make_classification(\n        n_samples=n_samples,\n        n_features=N_FEATURES,\n        n_classes=N_CLASSES,\n        random_state=0,\n    )\n    X = X.astype(np.float32)\n    return X, y\n\n\ndef get_model(with_pipeline=False):\n    """"""Get a multi-layer perceptron model.\n\n    Optionally, put it in a pipeline that scales the data.\n\n    """"""\n    model = NeuralNetClassifier(MLPClassifier)\n    if with_pipeline:\n        model = Pipeline([\n            (\'scale\', FeatureUnion([\n                (\'minmax\', MinMaxScaler()),\n                (\'normalize\', Normalizer()),\n            ])),\n            (\'select\', SelectKBest(k=N_FEATURES)),  # keep input size constant\n            (\'net\', model),\n        ])\n    return model\n\n\ndef save_model(model, output_file):\n    """"""Save model to output_file, if given""""""\n    if not output_file:\n        return\n\n    with open(output_file, \'wb\') as f:\n        pickle.dump(model, f)\n    print(""Saved model to file \'{}\'."".format(output_file))\n\n\ndef net(n_samples=100, output_file=None, **kwargs):\n    """"""Train an MLP classifier on synthetic data.\n\n    n_samples : int (default=100)\n      Number of training samples\n\n    output_file : str (default=None)\n      If not None, file name used to save the model.\n\n    kwargs : dict\n      Additional model parameters.\n\n    """"""\n\n    model = get_model(with_pipeline=False)\n    # important: wrap the model with the parsed arguments\n    parsed = parse_args(kwargs, defaults=DEFAULTS_NET)\n    model = parsed(model)\n\n    X, y = get_data(n_samples=n_samples)\n    print(""Training MLP classifier"")\n    model.fit(X, y)\n\n    save_model(model, output_file)\n\n\ndef pipeline(n_samples=100, output_file=None, **kwargs):\n    """"""Train an MLP classifier in a pipeline on synthetic data.\n\n    The pipeline scales the input data before passing it to the net.\n\n    Note: This docstring is used to create the help for the CLI.\n\n    Parameters\n    ----------\n    n_samples : int (default=100)\n      Number of training samples\n\n    output_file : str (default=None)\n      If not None, file name used to save the model.\n\n    kwargs : dict\n      Additional model parameters.\n\n    """"""\n\n    model = get_model(with_pipeline=True)\n    # important: wrap the model with the parsed arguments\n    parsed = parse_args(kwargs, defaults=DEFAULTS_PIPE)\n    model = parsed(model)\n\n    X, y = get_data(n_samples=n_samples)\n    print(""Training MLP classifier in a pipeline"")\n    model.fit(X, y)\n\n    save_model(model, output_file)\n\n\nif __name__ == \'__main__\':\n    # register 2 functions, ""net"" and ""pipeline""\n    fire.Fire({\'net\': net, \'pipeline\': pipeline})\n'"
examples/nuclei_image_segmentation/dataset.py,2,"b'""""""Dataset for cells""""""\nfrom itertools import zip_longest, product, chain\nimport random\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms.functional import (pad, to_tensor, normalize,\n                                               hflip, vflip, crop)\nfrom PIL import Image\n\n\ndef calcuate_bboxes(im_shape, patch_size):\n    """"""Calculate bound boxes based on image shape and size of the bounding box\n    given by `patch_size`""""""\n    h, w = im_shape\n    ph, pw = patch_size\n\n    steps_h = chain(range(0, h - ph, ph), [h - ph])\n    steps_w = chain(range(0, w - pw, pw), [w - pw])\n\n    return product(steps_h, steps_w)\n\n\nclass PatchedDataset(Dataset):\n    """"""Creates patches of cells.\n\n    Parameters\n    ----------\n    base_dataset: CellsDataset\n        Dataset of cells\n    patch_size: tuple of ints (default=(256, 256))\n        The size of each patch\n    random_flips: bool (default=False)\n        If true, patches and masks will be randomly flipped horizontally and\n        vertically.\n    padding: int (default=16)\n        Amount of paddding around each image and mask\n    """"""\n    def __init__(self,\n                 base_dataset,\n                 patch_size=(256, 256),\n                 random_flips=False,\n                 padding=16):\n        super().__init__()\n        self.base_dataset = base_dataset\n        self.patch_size = patch_size\n        self.patch_size_expanded = (patch_size[0] + 2 * padding,\n                                    patch_size[1] + 2 * padding)\n        self.padding = padding\n        self.random_flips = random_flips\n\n        coords = []\n        for idx, (_, mask) in enumerate(self.base_dataset):\n            w, h = mask.size\n            bboxes = calcuate_bboxes((h, w), self.patch_size)\n            idx_bboxes = list(zip_longest([], bboxes, fillvalue=idx))\n            coords.extend(idx_bboxes)\n        self.coords = coords\n\n    def __len__(self):\n        return len(self.coords)\n\n    def __getitem__(self, idx):\n        img_idx, (i, j) = self.coords[idx]\n        cell, mask = self.base_dataset[img_idx]\n        h, w = self.patch_size_expanded\n\n        cell = pad(cell, self.padding, padding_mode=\'reflect\')\n        mask = pad(mask, self.padding, padding_mode=\'reflect\')\n\n        cell = crop(cell, i, j, h, w)\n        mask = crop(mask, i, j, h, w)\n\n        if self.random_flips:\n            if random.random() < 0.5:\n                cell = hflip(cell)\n                mask = hflip(mask)\n\n            if random.random() < 0.5:\n                cell = vflip(cell)\n                mask = vflip(mask)\n\n        cell = to_tensor(cell)\n        mask = torch.as_tensor((np.array(mask) == 255).astype(\'float32\'))\n\n        # mean and std of imagenet\n        cell = normalize(cell, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n        return cell, mask\n\n\nclass CellsDataset(Dataset):\n    """"""Constructs cell dataset""""""\n    def __init__(self, sample_dirs):\n        super().__init__()\n        self.sample_dirs = sample_dirs\n\n    def __len__(self):\n        return len(self.sample_dirs)\n\n    def __getitem__(self, idx):\n        sample_dir = self.sample_dirs[idx]\n        cell_fn = (sample_dir / \'images\' / sample_dir.name).with_suffix(\'.png\')\n        mask_fn = sample_dir / \'mask.png\'\n\n        cell, mask = Image.open(cell_fn).convert(\'RGB\'), Image.open(mask_fn)\n        assert cell.size == mask.size\n        return cell, mask\n'"
examples/nuclei_image_segmentation/model.py,6,"b'import torch\nimport torch.nn as nn\nfrom torchvision import models\n\n\ndef make_decoder_block(in_channels, middle_channels, out_channels):\n    return nn.Sequential(\n        nn.Conv2d(in_channels, middle_channels, 3, padding=1),\n        nn.ReLU(inplace=True),\n        nn.ConvTranspose2d(\n            middle_channels, out_channels, kernel_size=4, stride=2, padding=1),\n        nn.ReLU(inplace=True))\n\n\nclass UNet(nn.Module):\n    """"""UNet Model inspired by the the original UNet paper\n\n    Parameters\n    ----------\n    pretrained: bool (default=True)\n        Option to use pretrained vgg16_bn based on ImageNet\n\n    References\n    ----------\n\n    .. [1] Olaf Ronneberger, Philipp Fischer, Thomas Brox, 2015,\n        ""U-Net: Convolutional Networks for Biomedical Image Segmentation,"".\n        ""MICCAI"" `<https://arxiv.org/abs/1505.04597>`_\n    """"""\n    def __init__(self, pretrained=False):\n        super().__init__()\n        encoder = models.vgg16_bn(pretrained=pretrained).features\n\n        self.conv1 = encoder[:6]\n        self.conv2 = encoder[6:13]\n        self.conv3 = encoder[13:23]\n        self.conv4 = encoder[23:33]\n        self.conv5 = encoder[33:43]\n\n        self.center = nn.Sequential(\n            encoder[43],  # MaxPool\n            make_decoder_block(512, 512, 256))\n\n        self.dec5 = make_decoder_block(256 + 512, 512, 256)\n        self.dec4 = make_decoder_block(256 + 512, 512, 256)\n        self.dec3 = make_decoder_block(256 + 256, 256, 64)\n        self.dec2 = make_decoder_block(64 + 128, 128, 32)\n        self.dec1 = nn.Sequential(\n            nn.Conv2d(32 + 64, 32, 3, padding=1), nn.ReLU(inplace=True))\n        self.final = nn.Conv2d(32, 1, kernel_size=1)\n\n    def forward(self, x):\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(conv1)\n        conv3 = self.conv3(conv2)\n        conv4 = self.conv4(conv3)\n        conv5 = self.conv5(conv4)\n\n        center = self.center(conv5)\n\n        dec5 = self.dec5(torch.cat([center, conv5], 1))\n        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n        dec1 = self.dec1(torch.cat([dec2, conv1], 1))\n\n        return self.final(dec1)\n'"
examples/nuclei_image_segmentation/prepare_dataset.py,0,"b'#!/usr/bin/env python\n""""""Each sample contains multiple masks. This script combines theese masks into\none image, thus creating one mask for each sample.\n""""""\nfrom multiprocessing import Pool\nfrom pathlib import Path\nfrom contextlib import ExitStack\n\nfrom tqdm import tqdm\nfrom PIL import Image\n\n\ndef combine_masks(mask_root_dir):\n    mask_output_fn = mask_root_dir / \'mask.png\'\n    if mask_output_fn.exists():\n        return\n    mask_fn_iter = mask_root_dir.glob(\'masks/*.png\')\n    img = Image.open(next(mask_fn_iter))\n    for fn in mask_fn_iter:\n        mask = Image.open(fn)\n        img.paste(mask, (0, 0), mask)\n    img.save(mask_output_fn)\n\n\n# Combine masks into one\nsamples_dirs = list(d for d in Path(\'data/cells\').iterdir() if d.is_dir())\nwith ExitStack() as stack:\n    pool = stack.enter_context(Pool())\n    pbar = stack.enter_context(tqdm(total=len(samples_dirs)))\n    for _ in tqdm(pool.imap_unordered(combine_masks, samples_dirs)):\n        pbar.update()\n'"
examples/nuclei_image_segmentation/utils.py,4,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.functional import binary_cross_entropy_with_logits\nfrom torchvision.transforms.functional import to_pil_image\nimport matplotlib.pyplot as plt\n\n\ndef convert_cell_to_img(t, padding=16):\n    """"""Converts pytorch tensor into a Pillow Image. The padding will be removed\n    from the resulting image""""""\n    std = torch.Tensor([0.229, 0.224, 0.225]).reshape(-1, 1, 1)\n    mu = torch.Tensor([0.485, 0.456, 0.406]).reshape(-1, 1, 1)\n    output = t.mul(std)\n    output.add_(mu)\n    img = to_pil_image(output)\n    w, h = img.size\n    return img.crop((padding, padding, w - padding, h - padding))\n\n\ndef plot_mask_cells(mask_cells, padding=16):\n    """"""Plots cells with their true mask, predicted mask.\n\n    Parameters\n    ----------\n    mask_cells: list of tuples (`true_mask`, `predicted_mask`, `cell`)\n    padding: int (default=16)\n        Padding around mask to remove.\n    """"""\n    fig, axes = plt.subplots(len(mask_cells), 3, figsize=(12, 10))\n    for idx, (axes, mask_cell) in enumerate(zip(axes, mask_cells), 1):\n        ax1, ax2, ax3 = axes\n        true_mask, predicted_mask, cell = mask_cell\n        plot_mask_cell(\n                true_mask, predicted_mask, cell,\n                \'Type {}\'.format(idx),\n                ax1, ax2, ax3,\n                padding=padding)\n    fig.tight_layout()\n    return fig, axes\n\n\ndef plot_mask_cell(true_mask,\n                   predicted_mask,\n                   cell,\n                   suffix,\n                   ax1,\n                   ax2,\n                   ax3,\n                   padding=16):\n    """"""Plots a single cell with a its true mask and predicuted mask""""""\n    for ax in [ax1, ax2, ax3]:\n        ax.grid(False)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    ax1.imshow(true_mask[padding:-padding, padding:-padding], cmap=\'viridis\')\n    ax1.set_title(\'True Mask - {}\'.format(suffix))\n    ax2.imshow(\n        predicted_mask[padding:-padding, padding:-padding], cmap=\'viridis\')\n    ax2.set_title(\'Predicted Mask - {}\'.format(suffix))\n    ax3.imshow(convert_cell_to_img(cell, padding=padding))\n    ax3.set_title(\'Image - {}\'.format(suffix))\n    return ax1, ax2, ax3\n\n\ndef plot_masks(mask_1, mask_2, mask_3):\n    """"""Plots three masks""""""\n    fig, ((ax1, ax2, ax3)) = plt.subplots(1, 3, figsize=(12, 5))\n    for ax in [ax1, ax2, ax3]:\n        ax.grid(False)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    ax1.set_title(""Type 1"")\n    ax1.imshow(mask_1, cmap=\'viridis\')\n    ax2.set_title(""Type 2"")\n    ax2.imshow(mask_2, cmap=\'viridis\')\n    ax3.set_title(""Type 3"")\n    ax3.imshow(mask_3, cmap=\'viridis\')\n    return ax1, ax2, ax3\n\n\ndef plot_cells(cell_1, cell_2, cell_3):\n    """"""Plots three cells""""""\n    fig, ((ax1, ax2, ax3)) = plt.subplots(1, 3, figsize=(12, 5))\n    for ax in [ax1, ax2, ax3]:\n        ax.grid(False)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    ax1.set_title(""Type 1"")\n    ax1.imshow(cell_1)\n    ax2.set_title(""Type 2"")\n    ax2.imshow(cell_2)\n    ax3.set_title(""Type 3"")\n    ax3.imshow(cell_3)\n    return ax1, ax2, ax3\n'"
examples/rnn_classifer/dask-config.py,0,"b""{\n    'grid_search': {\n        '__factory__': 'palladium.fit.with_parallel_backend',\n        'estimator': {\n            '__factory__': 'sklearn.model_selection.GridSearchCV',\n            'estimator': {'__copy__': 'model'},\n            'param_grid': {'__copy__': 'grid_search.param_grid'},\n            'scoring': {'__copy__': 'scoring'},\n        },\n        'backend': 'dask',\n    },\n\n    '_init_client': {\n        '__factory__': 'dask.distributed.Client',\n        'address': '127.0.0.1:8786',\n    },\n}\n"""
examples/rnn_classifer/model.py,1,"b""import os\nfrom urllib.request import urlretrieve\nimport tarfile\n\nfrom dstoolbox.transformers import Padder2d\nfrom dstoolbox.transformers import TextFeaturizer\nimport numpy as np\nfrom sklearn.datasets import load_files\nfrom sklearn.pipeline import Pipeline\nfrom skorch import NeuralNetClassifier\nimport torch\nfrom torch import nn\nfrom palladium.interfaces import DatasetLoader as IDatasetLoader\n\nF = nn.functional\n\nDATA_URL = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\nDATA_FN = DATA_URL.rsplit('/', 1)[1]\n\nnp.random.seed(0)\n\n\ndef download():\n    if not os.path.exists('aclImdb'):\n        # unzip data if it does not exist\n        if not os.path.exists(DATA_FN):\n            urlretrieve(DATA_URL, DATA_FN)\n        with tarfile.open(DATA_FN, 'r:gz') as f:\n            f.extractall()\n\n\nclass DatasetLoader(IDatasetLoader):\n    def __init__(self, path='aclImdb/train/'):\n        self.path = path\n\n    def __call__(self):\n        download()\n        dataset = load_files(self.path, categories=['pos', 'neg'])\n        X, y = dataset['data'], dataset['target']\n        X = np.asarray([x.decode() for x in X])  # decode from bytes\n        return X, y\n\n\nclass RNNClassifier(nn.Module):\n    def __init__(\n        self,\n        embedding_dim=128,\n        rec_layer_type='lstm',\n        num_units=128,\n        num_layers=2,\n        dropout=0,\n        vocab_size=1000,\n    ):\n        super().__init__()\n        self.embedding_dim = embedding_dim\n        self.rec_layer_type = rec_layer_type.lower()\n        self.num_units = num_units\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        self.emb = nn.Embedding(\n            vocab_size + 1, embedding_dim=self.embedding_dim)\n\n        rec_layer = {'lstm': nn.LSTM, 'gru': nn.GRU}[self.rec_layer_type]\n        # We have to make sure that the recurrent layer is batch_first,\n        # since sklearn assumes the batch dimension to be the first\n        self.rec = rec_layer(\n            self.embedding_dim, self.num_units,\n            num_layers=num_layers, batch_first=True,\n            )\n\n        self.output = nn.Linear(self.num_units, 2)\n\n    def forward(self, X):\n        embeddings = self.emb(X)\n        # from the recurrent layer, only take the activities from the\n        # last sequence step\n        if self.rec_layer_type == 'gru':\n            _, rec_out = self.rec(embeddings)\n        else:\n            _, (rec_out, _) = self.rec(embeddings)\n        rec_out = rec_out[-1]  # take output of last RNN layer\n        drop = F.dropout(rec_out, p=self.dropout)\n        # Remember that the final non-linearity should be softmax, so\n        # that our predict_proba method outputs actual probabilities!\n        out = F.softmax(self.output(drop), dim=-1)\n        return out\n\n\ndef create_pipeline(\n    vocab_size=1000,\n    max_len=50,\n    use_cuda=False,\n    **kwargs\n):\n    return Pipeline([\n        ('to_idx', TextFeaturizer(max_features=vocab_size)),\n        ('pad', Padder2d(max_len=max_len, pad_value=vocab_size, dtype=int)),\n        ('net', NeuralNetClassifier(\n            RNNClassifier,\n            device=('cuda' if use_cuda else 'cpu'),\n            max_epochs=5,\n            lr=0.01,\n            optimizer=torch.optim.RMSprop,\n            module__vocab_size=vocab_size,\n            **kwargs,\n        ))\n    ])\n"""
examples/rnn_classifer/palladium-config.py,0,"b""{\n    'dataset_loader_train': {\n        '__factory__': 'model.DatasetLoader',\n        'path': 'aclImdb/train/',\n    },\n\n    'dataset_loader_test': {\n        '__factory__': 'model.DatasetLoader',\n        'path': 'aclImdb/test/',\n    },\n\n    'model': {\n        '__factory__': 'model.create_pipeline',\n        'use_cuda': True,\n    },\n\n    'model_persister': {\n        '__factory__': 'palladium.persistence.File',\n        'path': 'rnn-model-{version}',\n    },\n\n    'grid_search': {\n        'param_grid': {\n            'to_idx__stop_words': ['english', None],\n            'to_idx__lowercase': [False, True],\n            'to_idx__ngram_range': [(1, 1), (2, 2)],\n            'net__module__embedding_dim': [32, 64, 128, 256],\n            'net__module__rec_layer_type': ['gru', 'lstm'],\n            'net__module__num_units': [32, 64, 128, 256],\n            'net__module__num_layers': [1, 2, 3],\n            'net__module__dropout': [0, 0.25, 0.5, 0.75],\n            'net__lr': [0.003, 0.01, 0.03],\n            'net__max_epochs': [5, 10],\n        },\n    },\n\n    'scoring': 'accuracy',\n\n    'predict_service': {\n        '__factory__': 'palladium.server.PredictService',\n        'mapping': [\n            ('text', 'str'),\n        ],\n        'predict_proba': True,\n        'unwrap_sample': True,\n    },\n\n}\n"""
examples/translation/data.py,0,"b'import unicodedata\nimport string\nimport re\n\n\nSOS_token = 0\nEOS_token = 1\n\n\nclass Lang:\n    def __init__(self, name):\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: ""SOS"", 1: ""EOS""}\n        self.n_words = 2  # Count SOS and EOS\n\n    def addSentence(self, sentence):\n        for word in sentence.split(\' \'):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1\n\n\n######################################################################\n# The files are all in Unicode, to simplify we will turn Unicode\n# characters to ASCII, make everything lowercase, and trim most\n# punctuation.\n#\n\n# Turn a Unicode string to plain ASCII, thanks to\n# http://stackoverflow.com/a/518232/2809427\ndef unicodeToAscii(s):\n    return \'\'.join(\n        c for c in unicodedata.normalize(\'NFD\', s)\n        if unicodedata.category(c) != \'Mn\'\n    )\n\n# Lowercase, trim, and remove non-letter characters\n\n\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r""([.!?])"", r"" \\1"", s)\n    s = re.sub(r""[^a-zA-Z.!?]+"", r"" "", s)\n    return s\n\n\n######################################################################\n# To read the data file we will split the file into lines, and then split\n# lines into pairs. The files are all English \xe2\x86\x92 Other Language, so if we\n# want to translate from Other Language \xe2\x86\x92 English I added the ``reverse``\n# flag to reverse the pairs.\n#\n\ndef readLangs(lang1, lang2, reverse=False):\n    print(""Reading lines..."")\n\n    # Read the file and split into lines\n    lines = open(\'data/%s-%s.txt\' % (lang1, lang2), encoding=\'utf-8\').\\\n        read().strip().split(\'\\n\')\n\n    # Split every line into pairs and normalize\n    pairs = [[normalizeString(s) for s in l.split(\'\\t\')] for l in lines]\n\n    # Reverse pairs, make Lang instances\n    if reverse:\n        pairs = [list(reversed(p)) for p in pairs]\n        input_lang = Lang(lang2)\n        output_lang = Lang(lang1)\n    else:\n        input_lang = Lang(lang1)\n        output_lang = Lang(lang2)\n\n    return input_lang, output_lang, pairs\n\n\n######################################################################\n# Since there are a *lot* of example sentences and we want to train\n# something quickly, we\'ll trim the data set to only relatively short and\n# simple sentences. Here the maximum length is 10 words (that includes\n# ending punctuation) and we\'re filtering to sentences that translate to\n# the form ""I am"" or ""He is"" etc. (accounting for apostrophes replaced\n# earlier).\n#\n\nMAX_LENGTH = 10\n\neng_prefixes = (\n    ""i am "", ""i m "",\n    ""he is"", ""he s "",\n    ""she is"", ""she s"",\n    ""you are"", ""you re "",\n    ""we are"", ""we re "",\n    ""they are"", ""they re ""\n)\n\n\ndef filterPair(p):\n    return len(p[0].split(\' \')) < MAX_LENGTH and \\\n        len(p[1].split(\' \')) < MAX_LENGTH and \\\n        p[1].startswith(eng_prefixes)\n\n\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]\n\n\n######################################################################\n# The full process for preparing the data is:\n#\n# -  Read text file and split into lines, split lines into pairs\n# -  Normalize text, filter by length and content\n# -  Make word lists from sentences in pairs\n#\n\ndef prepareData(lang1, lang2, reverse=False):\n    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n    print(""Read %s sentence pairs"" % len(pairs))\n    pairs = filterPairs(pairs)\n    print(""Trimmed to %s sentence pairs"" % len(pairs))\n    print(""Counting words..."")\n    for pair in pairs:\n        input_lang.addSentence(pair[0])\n        output_lang.addSentence(pair[1])\n    print(""Counted words:"")\n    print(input_lang.name, input_lang.n_words)\n    print(output_lang.name, output_lang.n_words)\n    return input_lang, output_lang, pairs\n\n'"
examples/word_language_model/data.py,3,"b'import os\n\nfrom skorch.utils import to_device\n\nimport torch\nfrom torch.autograd import Variable\n\n\nclass Dictionary:\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\nclass Corpus:\n    def __init__(self, path):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(os.path.join(path, \'train.txt\'))\n        self.valid = self.tokenize(os.path.join(path, \'valid.txt\'))\n        self.test = self.tokenize(os.path.join(path, \'test.txt\'))\n\n    def tokenize(self, path):\n        """"""Tokenizes a text file.""""""\n        assert os.path.exists(path)\n        # Add words to the dictionary\n        with open(path, \'r\') as f:\n            tokens = 0\n            for line in f:\n                words = line.split() + [\'<eos>\']\n                tokens += len(words)\n                for word in words:\n                    self.dictionary.add_word(word)\n\n        # Tokenize file content\n        with open(path, \'r\') as f:\n            ids = torch.LongTensor(tokens)\n            token = 0\n            for line in f:\n                words = line.split() + [\'<eos>\']\n                for word in words:\n                    ids[token] = self.dictionary.word2idx[word]\n                    token += 1\n\n        return ids\n\n\nclass Loader:\n    def __init__(self, source, device=\'cpu\', bptt=10, batch_size=20, evaluation=False):\n        self.evaluation = evaluation\n        self.bptt = bptt\n        self.batch_size = batch_size\n        self.device = device\n        if isinstance(source.X, Variable):\n            data = source.X.data.long()\n        else:\n            data = torch.LongTensor(source.X)\n        self.batches = self.batchify(data, batch_size)\n\n    def batchify(self, data, bsz):\n        # Work out how cleanly we can divide the dataset into bsz parts.\n        nbatch = data.size(0) // bsz\n        # Trim off any extra elements that wouldn\'t cleanly fit (remainders).\n        data = data.narrow(0, 0, nbatch * bsz)\n        # Evenly divide the data across the bsz batches.\n        data = data.view(bsz, -1).t().contiguous()\n        return to_device(data, self.device)\n\n    def get_batch(self, i):\n        seq_len = min(self.bptt, len(self.batches) - 1 - i)\n        data = Variable(self.batches[i:i+seq_len], volatile=self.evaluation)\n        target = Variable(self.batches[i+1:i+1+seq_len].view(-1))\n        return data, target\n\n    def __iter__(self):\n        for i in range(0, self.batches.size(0) - 1, self.bptt):\n            yield self.get_batch(i)\n'"
examples/word_language_model/generate.py,4,"b""import argparse\n\nimport skorch\nimport torch\nfrom torch.autograd import Variable\n\nimport data\nimport model\nimport net\n\nparser = argparse.ArgumentParser(description='PyTorch PennTreeBank RNN/LSTM Language Model')\nparser.add_argument('--data', type=str, default='./data/penn',\n                    help='location of the data corpus')\nparser.add_argument('--bptt', type=int, default=35,\n                    help='sequence length')\nparser.add_argument('--seed', type=int, default=1111,\n                    help='random seed')\nparser.add_argument('--no-cuda', dest='cuda', action='store_false',\n                    help='use CUDA')\nparser.add_argument('--checkpoint', type=str, default='./model.pt',\n                    help='model checkpoint to use')\nparser.add_argument('--outf', type=str, default='generated.txt',\n                    help='output file for generated text')\nparser.add_argument('--temperature', type=float, default=1.0,\n                    help='temperature - higher will increase diversity')\nparser.add_argument('--words', type=int, default='1000',\n                    help='number of words to generate')\nparser.add_argument('--log-interval', type=int, default=100,\n                    help='reporting interval')\n\nargs = parser.parse_args()\n\ntorch.manual_seed(args.seed)\n\ncorpus = data.Corpus(args.data)\nntokens = len(corpus.dictionary)\ndevice = 'cuda' if args.cuda else 'cpu'\n\nnet = net.Net(\n    module=model.RNNModel,\n    batch_size=1,\n    device=device,\n    module__rnn_type='LSTM',\n    module__ntoken=ntokens,\n    module__ninp=200,\n    module__nhid=200,\n    module__nlayers=2)\nnet.initialize()\nnet.load_params(args.checkpoint)\n\nhidden = None\ninput = skorch.utils.to_tensor(torch.rand(1, 1).mul(ntokens).long(),\n                               device=device)\n\nwith open(args.outf, 'w') as outf:\n    for i in range(args.words):\n        word_idx, hidden = net.sample(\n                input=input,\n                temperature=args.temperature,\n                hidden=hidden)\n        input = skorch.utils.to_tensor(\n                torch.LongTensor([[word_idx]]),\n                device=device)\n\n        word = corpus.dictionary.idx2word[word_idx]\n        outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n\n        if i % args.log_interval == 0:\n            print('| Generated {}/{} words'.format(i, args.words))\n"""
examples/word_language_model/model.py,2,"b'import torch.nn as nn\nfrom torch.autograd import Variable\n\nclass RNNModel(nn.Module):\n    """"""Container module with an encoder, a recurrent module, and a decoder.""""""\n\n    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n        super(RNNModel, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        if rnn_type in [\'LSTM\', \'GRU\']:\n            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n        else:\n            try:\n                nonlinearity = {\'RNN_TANH\': \'tanh\', \'RNN_RELU\': \'relu\'}[rnn_type]\n            except KeyError:\n                raise ValueError( """"""An invalid option for `--model` was supplied,\n                                 options are [\'LSTM\', \'GRU\', \'RNN_TANH\' or \'RNN_RELU\']"""""")\n            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n        self.decoder = nn.Linear(nhid, ntoken)\n\n        # Optionally tie weights as in:\n        # ""Using the Output Embedding to Improve Language Models"" (Press & Wolf 2016)\n        # https://arxiv.org/abs/1608.05859\n        # and\n        # ""Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling"" (Inan et al. 2016)\n        # https://arxiv.org/abs/1611.01462\n        if tie_weights:\n            if nhid != ninp:\n                raise ValueError(\'When using the tied flag, nhid must be equal to emsize\')\n            self.decoder.weight = self.encoder.weight\n\n        self.init_weights()\n\n        self.rnn_type = rnn_type\n        self.nhid = nhid\n        self.nlayers = nlayers\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.fill_(0)\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, input, hidden):\n        emb = self.drop(self.encoder(input))\n        output, hidden = self.rnn(emb, hidden)\n        output = self.drop(output)\n        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters()).data\n        if self.rnn_type == \'LSTM\':\n            return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))\n        else:\n            return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())\n'"
examples/word_language_model/net.py,6,"b'import skorch\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom sklearn.metrics import f1_score\n\n\nclass Net(skorch.NeuralNet):\n\n    def __init__(\n            self,\n            criterion=torch.nn.CrossEntropyLoss,\n            clip=0.25,\n            lr=20,\n            ntokens=10000,\n            *args,\n            **kwargs\n    ):\n        self.clip = clip\n        self.ntokens = ntokens\n        super(Net, self).__init__(criterion=criterion, lr=lr, *args, **kwargs)\n\n    def repackage_hidden(self, h):\n        """"""Wraps hidden states in new Variables, to detach them from their history.""""""\n        if isinstance(h, Variable):\n            return torch.tensor(h.data, device=h.device)\n        else:\n            return tuple(self.repackage_hidden(v) for v in h)\n\n    def on_epoch_begin(self, *args, **kwargs):\n        super().on_epoch_begin(*args, **kwargs)\n\n        # As an optimization to save tensor allocation for each\n        # batch we initialize the hidden state only once per epoch.\n        # This optimization was taken from the original example.\n        self.hidden = self.module_.init_hidden(self.batch_size)\n\n    def train_step(self, X, y):\n        self.module_.train()\n\n        # Repackage shared hidden state so that the previous batch\n        # does not influence the current one.\n        self.hidden = self.repackage_hidden(self.hidden)\n        self.module_.zero_grad()\n\n        output, self.hidden = self.module_(X, self.hidden)\n        y_pred = output.view(-1, self.ntokens)\n\n        loss = self.get_loss(y_pred, y)\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(self.module_.parameters(), self.clip)\n        for p in self.module_.parameters():\n            p.data.add_(-self.lr, p.grad.data)\n        return {\'loss\': loss, \'y_pred\': y_pred}\n\n    def validation_step(self, X, y):\n        self.module_.eval()\n\n        hidden = self.module_.init_hidden(self.batch_size)\n        output, _ = self.module_(X, hidden)\n        output_flat = output.view(-1, self.ntokens)\n\n        return {\'loss\': self.get_loss(output_flat, y), \'y_pred\': output_flat}\n\n    def evaluation_step(self, X, **kwargs):\n        self.module_.eval()\n\n        X = skorch.utils.to_tensor(X, device=self.device)\n        hidden = self.module_.init_hidden(self.batch_size)\n        output, _ = self.module_(X, hidden)\n\n        return output.view(-1, self.ntokens)\n\n    def predict(self, X):\n        return np.argmax(super().predict(X), -1)\n\n    def sample(self, input, temperature=1., hidden=None):\n        hidden = self.module_.init_hidden(1) if hidden is None else hidden\n        output, hidden = self.module_(input, hidden)\n        probas = output.squeeze().data.div(temperature).exp()\n        sample = torch.multinomial(probas, 1)[-1]\n        if probas.dim() > 1:\n            sample = sample[0]\n        return sample, self.repackage_hidden(hidden)\n\n    def sample_n(self, num_words, input, temperature=1., hidden=None):\n        preds = [None] * num_words\n        for i in range(num_words):\n            preds[i], hidden = self.sample(input, hidden=hidden)\n            input = skorch.utils.to_tensor(torch.LongTensor([[preds[i]]]),\n                                           device=self.device)\n        return preds, hidden\n\n    def score(self, X, y=None):\n        ds = self.get_dataset(X)\n        target_iterator = self.get_iterator(ds, training=False)\n\n        y_true = np.concatenate([skorch.utils.to_numpy(y) for _, y in target_iterator])\n        y_pred = self.predict(X)\n\n        return f1_score(y_true, y_pred, average=\'micro\')\n'"
examples/word_language_model/train.py,2,"b'import argparse\n\nimport skorch\nimport torch\nfrom sklearn.model_selection import GridSearchCV\n\nimport data\nfrom model import RNNModel\nfrom net import Net\n\nparser = argparse.ArgumentParser(description=\'PyTorch PennTreeBank RNN/LSTM Language Model\')\nparser.add_argument(\'--data\', type=str, default=\'./data/penn\',\n                    help=\'location of the data corpus\')\nparser.add_argument(\'--bptt\', type=int, default=35,\n                    help=\'sequence length\')\nparser.add_argument(\'--batch_size\', type=int, default=20, metavar=\'N\',\n                    help=\'batch size\')\nparser.add_argument(\'--epochs\', type=int, default=10, metavar=\'N\',\n                    help=\'number of epochs\')\nparser.add_argument(\'--data-limit\', type=int, default=-1,\n                    help=\'Limit the input data to length N.\')\nparser.add_argument(\'--seed\', type=int, default=1111,\n                    help=\'random seed\')\nparser.add_argument(\'--no-cuda\', dest=\'cuda\', action=\'store_false\',\n                    help=\'use CUDA\')\nparser.add_argument(\'--save\', type=str,  default=\'model.pt\',\n                    help=\'path to save the final model\')\nargs = parser.parse_args()\n\ntorch.manual_seed(args.seed)\n\ncorpus = data.Corpus(args.data)\nntokens = len(corpus.dictionary)\ndevice = \'cuda\' if args.cuda else \'cpu\'\n\nclass LRAnnealing(skorch.callbacks.Callback):\n    def on_epoch_end(self, net, **kwargs):\n        if not net.history[-1][\'valid_loss_best\']:\n            net.lr /= 4.0\n\nclass ExamplePrinter(skorch.callbacks.Callback):\n    def on_epoch_end(self, net, **kwargs):\n        seed_sentence = ""the meaning of""\n        indices = [corpus.dictionary.word2idx[n] for n in seed_sentence.split()]\n        indices = skorch.utils.to_tensor(\n            torch.LongTensor([indices]).t(), device=device)\n        sentence, _ = net.sample_n(num_words=10, input=indices)\n        print(seed_sentence,\n              "" "".join([corpus.dictionary.idx2word[n] for n in sentence]))\n\n\ndef my_train_split(ds, y):\n    # Return (corpus.train, corpus.valid) in case the network\n    # is fitted using net.fit(corpus.train).\n    return ds, skorch.dataset.Dataset(corpus.valid[:200], y=None)\n\nnet = Net(\n    module=RNNModel,\n    max_epochs=args.epochs,\n    batch_size=args.batch_size,\n    device=device,\n    callbacks=[\n        skorch.callbacks.Checkpoint(),\n        skorch.callbacks.ProgressBar(),\n        LRAnnealing(),\n        ExamplePrinter()\n    ],\n    module__rnn_type=\'LSTM\',\n    module__ntoken=ntokens,\n    module__ninp=200,\n    module__nhid=200,\n    module__nlayers=2,\n\n    # Use (corpus.train, corpus.valid) as validation split.\n    # Even though we are doing a grid search, we use an internal\n    # validation set to determine when to save (Checkpoint callback)\n    # and when to decrease the learning rate (LRAnnealing callback).\n    train_split=my_train_split,\n\n    # To demonstrate that skorch is able to use already available\n    # data loaders as well, we use the data loader from the word\n    # language model.\n    iterator_train=data.Loader,\n    iterator_train__device=device,\n    iterator_train__bptt=args.bptt,\n    iterator_valid=data.Loader,\n    iterator_valid__device=device,\n    iterator_valid__bptt=args.bptt)\n\n\n# Demonstrate the use of grid search by testing different learning\n# rates while saving the best model at the end.\n\nparams = [\n    {\n        \'lr\': [10,20,30],\n    },\n]\n\npl = GridSearchCV(net, params)\n\npl.fit(corpus.train[:args.data_limit].numpy())\n\nprint(""Results of grid search:"")\nprint(""Best parameter configuration:"", pl.best_params_)\nprint(""Achieved F1 score:"", pl.best_score_)\n\nprint(""Saving best model to \'{}\'."".format(args.save))\npl.best_estimator_.save_params(f_params=args.save)\n\n'"
skorch/callbacks/__init__.py,0,"b'""""""This module serves to elevate callbacks in submodules to the\nskorch.callback namespace. Remember to define `__all__` in each\nsubmodule.\n\n""""""\n\n# pylint: disable=wildcard-import\n\nfrom .base import *\nfrom .logging import *\nfrom .regularization import *\nfrom .scoring import *\nfrom .training import *\nfrom .lr_scheduler import *\n\n\n__all__ = [\n    \'BatchScoring\',\n    \'Callback\',\n    \'Checkpoint\',\n    \'EarlyStopping\',\n    \'EpochScoring\',\n    \'EpochTimer\',\n    \'Freezer\',\n    \'GradientNormClipping\',\n    \'Initializer\',\n    \'LRScheduler\',\n    \'LoadInitState\',\n    \'NeptuneLogger\',\n    \'ParamMapper\',\n    \'PassthroughScoring\',\n    \'PrintLog\',\n    \'ProgressBar\',\n    \'TrainEndCheckpoint\',\n    \'TensorBoard\',\n    \'Unfreezer\',\n    \'WandbLogger\',\n    \'WarmRestartLR\',\n]\n'"
skorch/callbacks/base.py,0,"b'"""""" Basic callback definition. """"""\n\nfrom sklearn.base import BaseEstimator\n\n\n__all__ = [\'Callback\']\n\n\nclass Callback:\n    """"""Base class for callbacks.\n\n    All custom callbacks should inherit from this class. The subclass\n    may override any of the ``on_...`` methods. It is, however, not\n    necessary to override all of them, since it\'s okay if they don\'t\n    have any effect.\n\n    Classes that inherit from this also gain the ``get_params`` and\n    ``set_params`` method.\n\n    """"""\n    def initialize(self):\n        """"""(Re-)Set the initial state of the callback. Use this\n        e.g. if the callback tracks some state that should be reset\n        when the model is re-initialized.\n\n        This method should return self.\n\n        """"""\n        return self\n\n    def on_train_begin(self, net,\n                       X=None, y=None, **kwargs):\n        """"""Called at the beginning of training.""""""\n\n    def on_train_end(self, net,\n                     X=None, y=None, **kwargs):\n        """"""Called at the end of training.""""""\n\n    def on_epoch_begin(self, net,\n                       dataset_train=None, dataset_valid=None, **kwargs):\n        """"""Called at the beginning of each epoch.""""""\n\n    def on_epoch_end(self, net,\n                     dataset_train=None, dataset_valid=None, **kwargs):\n        """"""Called at the end of each epoch.""""""\n\n    def on_batch_begin(self, net,\n                       X=None, y=None, training=None, **kwargs):\n        """"""Called at the beginning of each batch.""""""\n\n    def on_batch_end(self, net,\n                     X=None, y=None, training=None, **kwargs):\n        """"""Called at the end of each batch.""""""\n\n    def on_grad_computed(self, net, named_parameters,\n                         X=None, y=None, training=None, **kwargs):\n        """"""Called once per batch after gradients have been computed but before\n        an update step was performed.\n        """"""\n\n    def _get_param_names(self):\n        return (key for key in self.__dict__ if not key.endswith(\'_\'))\n\n    def get_params(self, deep=True):\n        return BaseEstimator.get_params(self, deep=deep)\n\n    def set_params(self, **params):\n        BaseEstimator.set_params(self, **params)\n'"
skorch/callbacks/logging.py,1,"b'"""""" Callbacks for printing, logging and log information.""""""\n\nimport sys\nimport time\nfrom contextlib import suppress\nfrom numbers import Number\nfrom itertools import cycle\nfrom pathlib import Path\n\nimport numpy as np\nimport tqdm\nfrom tabulate import tabulate\n\nfrom skorch.utils import Ansi\nfrom skorch.dataset import get_len\nfrom skorch.callbacks import Callback\n\n__all__ = [\'EpochTimer\', \'NeptuneLogger\', \'WandbLogger\', \'PrintLog\', \'ProgressBar\',\n           \'TensorBoard\']\n\n\ndef filter_log_keys(keys, keys_ignored=None):\n    """"""Filter out keys that are generally to be ignored.\n\n    This is used by several callbacks to filter out keys from history\n    that should not be logged.\n\n    Parameters\n    ----------\n    keys : iterable of str\n      All keys.\n\n    keys_ignored : iterable of str or None (default=None)\n      If not None, collection of extra keys to be ignored.\n\n    """"""\n    keys_ignored = keys_ignored or ()\n    for key in keys:\n        if not (\n                key == \'epoch\' or\n                (key in keys_ignored) or\n                key.endswith(\'_best\') or\n                key.endswith(\'_batch_count\') or\n                key.startswith(\'event_\')\n        ):\n            yield key\n\n\nclass EpochTimer(Callback):\n    """"""Measures the duration of each epoch and writes it to the\n    history with the name ``dur``.\n\n    """"""\n    def __init__(self, **kwargs):\n        super(EpochTimer, self).__init__(**kwargs)\n\n        self.epoch_start_time_ = None\n\n    def on_epoch_begin(self, net, **kwargs):\n        self.epoch_start_time_ = time.time()\n\n    def on_epoch_end(self, net, **kwargs):\n        net.history.record(\'dur\', time.time() - self.epoch_start_time_)\n\n\nclass NeptuneLogger(Callback):\n    """"""Logs results from history to Neptune\n\n    Neptune is a lightweight experiment tracking tool.\n    You can read more about it here: https://neptune.ai\n\n    Use this callback to automatically log all interesting values from\n    your net\'s history to Neptune.\n\n    The best way to log additional information is to log directly to the\n    experiment object or subclass the ``on_*`` methods.\n\n    To monitor resource consumption install psutil\n\n    >>> pip install psutil\n\n    You can view example experiment logs here:\n    https://ui.neptune.ai/o/shared/org/skorch-integration/e/SKOR-13/charts\n\n    Examples\n    --------\n    >>> # Install neptune\n    >>> pip install neptune-client\n    >>> # Create a neptune experiment object\n    >>> import neptune\n    ...\n    ... # We are using api token for an anonymous user.\n    ... # For your projects use the token associated with your neptune.ai account\n    >>> neptune.init(api_token=\'ANONYMOUS\',\n    ...              project_qualified_name=\'shared/skorch-integration\')\n    ...\n    ... experiment = neptune.create_experiment(\n    ...                        name=\'skorch-basic-example\',\n    ...                        params={\'max_epochs\': 20,\n    ...                                \'lr\': 0.01},\n    ...                        upload_source_files=[\'skorch_example.py\'])\n\n    >>> # Create a neptune_logger callback\n    >>> neptune_logger = NeptuneLogger(experiment, close_after_train=False)\n\n    >>> # Pass a logger to net callbacks argument\n    >>> net = NeuralNetClassifier(\n    ...           ClassifierModule,\n    ...           max_epochs=20,\n    ...           lr=0.01,\n    ...           callbacks=[neptune_logger])\n\n    >>> # Log additional metrics after training has finished\n    >>> from sklearn.metrics import roc_auc_score\n    ... y_pred = net.predict_proba(X)\n    ... auc = roc_auc_score(y, y_pred[:, 1])\n    ...\n    ... neptune_logger.experiment.log_metric(\'roc_auc_score\', auc)\n\n    >>> # log charts like ROC curve\n    ... from scikitplot.metrics import plot_roc\n    ... import matplotlib.pyplot as plt\n    ...\n    ... fig, ax = plt.subplots(figsize=(16, 12))\n    ... plot_roc(y, y_pred, ax=ax)\n    ... neptune_logger.experiment.log_image(\'roc_curve\', fig)\n\n    >>> # log net object after training\n    ... net.save_params(f_params=\'basic_model.pkl\')\n    ... neptune_logger.experiment.log_artifact(\'basic_model.pkl\')\n\n    >>> # close experiment\n    ... neptune_logger.experiment.stop()\n\n    Parameters\n    ----------\n    experiment : neptune.experiments.Experiment\n      Instantiated ``Experiment`` class.\n\n    log_on_batch_end : bool (default=False)\n      Whether to log loss and other metrics on batch level.\n\n    close_after_train : bool (default=True)\n      Whether to close the ``Experiment`` object once training\n      finishes. Set this parameter to False if you want to continue\n      logging to the same Experiment or if you use it as a context\n      manager.\n\n    keys_ignored : str or list of str (default=None)\n      Key or list of keys that should not be logged to\n      Neptune. Note that in addition to the keys provided by the\n      user, keys such as those starting with \'event_\' or ending on\n      \'_best\' are ignored by default.\n\n    Attributes\n    ----------\n    first_batch_ : bool\n        Helper attribute that is set to True at initialization and changes\n        to False on first batch end. Can be used when we want to log things\n        exactly once.\n\n    .. _Neptune: https://www.neptune.ai\n\n    """"""\n\n    def __init__(\n            self,\n            experiment,\n            log_on_batch_end=False,\n            close_after_train=True,\n            keys_ignored=None,\n    ):\n        self.experiment = experiment\n        self.log_on_batch_end = log_on_batch_end\n        self.close_after_train = close_after_train\n        self.keys_ignored = keys_ignored\n\n    def initialize(self):\n        self.first_batch_ = True\n\n        keys_ignored = self.keys_ignored\n        if isinstance(keys_ignored, str):\n            keys_ignored = [keys_ignored]\n        self.keys_ignored_ = set(keys_ignored or [])\n        self.keys_ignored_.add(\'batches\')\n        return self\n\n    def on_batch_end(self, net, **kwargs):\n        if self.log_on_batch_end:\n            batch_logs = net.history[-1][\'batches\'][-1]\n\n            for key in filter_log_keys(batch_logs.keys(), self.keys_ignored_):\n                self.experiment.log_metric(key, batch_logs[key])\n\n        self.first_batch_ = False\n\n    def on_epoch_end(self, net, **kwargs):\n        """"""Automatically log values from the last history step.""""""\n        history = net.history\n        epoch_logs = history[-1]\n        epoch = epoch_logs[\'epoch\']\n\n        for key in filter_log_keys(epoch_logs.keys(), self.keys_ignored_):\n            self.experiment.log_metric(key, x=epoch, y=epoch_logs[key])\n\n    def on_train_end(self, net, **kwargs):\n        if self.close_after_train:\n            self.experiment.stop()\n\nclass WandbLogger(Callback):\n    """"""Logs best model and metrics to `Weights & Biases <https://docs.wandb.com/>`_\n\n    Use this callback to automatically log best trained model, all metrics from\n    your net\'s history, model topology and computer resources to Weights & Biases\n    after each epoch.\n\n    Every file saved in `wandb_run.dir` is automatically logged to W&B servers.\n\n    See `example run\n    <https://app.wandb.ai/borisd13/skorch/runs/s20or4ct/overview?workspace=user-borisd13>`_\n\n    Examples\n    --------\n    >>> # Install wandb\n    ... pip install wandb\n\n    >>> import wandb\n    >>> from skorch.callbacks import WandbLogger\n\n    >>> # Create a wandb Run\n    ... wandb_run = wandb.init()\n    >>> # Alternative: Create a wandb Run without having a W&B account\n    ... wandb_run = wandb.init(anonymous=""allow)\n\n    >>> # Log hyper-parameters (optional)\n    ... wandb_run.config.update({""learning rate"": 1e-3, ""batch size"": 32})\n\n    >>> net = NeuralNet(..., callbacks=[WandbLogger(wandb_run)])\n    >>> net.fit(X, y)\n\n    Parameters\n    ----------\n    wandb_run : wandb.wandb_run.Run\n      wandb Run used to log data.\n\n    save_model : bool (default=True)\n      Whether to save a checkpoint of the best model and upload it\n      to your Run on W&B servers.\n\n    keys_ignored : str or list of str (default=None)\n      Key or list of keys that should not be logged to\n      tensorboard. Note that in addition to the keys provided by the\n      user, keys such as those starting with \'event_\' or ending on\n      \'_best\' are ignored by default.\n    """"""\n\n    def __init__(\n            self,\n            wandb_run,\n            save_model=True,\n            keys_ignored=None,\n    ):\n        self.wandb_run = wandb_run\n        self.save_model = save_model\n        self.keys_ignored = keys_ignored\n\n    def initialize(self):\n        keys_ignored = self.keys_ignored\n        if isinstance(keys_ignored, str):\n            keys_ignored = [keys_ignored]\n        self.keys_ignored_ = set(keys_ignored or [])\n        self.keys_ignored_.add(\'batches\')\n        return self\n\n    def on_train_begin(self, net, **kwargs):\n        """"""Log model topology and add a hook for gradients""""""\n        self.wandb_run.watch(net.module_)\n\n    def on_epoch_end(self, net, **kwargs):\n        """"""Log values from the last history step and save best model""""""\n        hist = net.history[-1]\n        keys_kept = filter_log_keys(hist, keys_ignored=self.keys_ignored_)\n        logged_vals = {k: hist[k] for k in keys_kept}\n        self.wandb_run.log(logged_vals)\n\n        # save best model\n        if self.save_model and hist[\'valid_loss_best\']:\n            model_path = Path(self.wandb_run.dir) / \'best_model.pth\'\n            with model_path.open(\'wb\') as model_file:\n                net.save_params(f_params=model_file)\n\n\nclass PrintLog(Callback):\n    """"""Print useful information from the model\'s history as a table.\n\n    By default, ``PrintLog`` prints everything from the history except\n    for ``\'batches\'``.\n\n    To determine the best loss, ``PrintLog`` looks for keys that end on\n    ``\'_best\'`` and associates them with the corresponding loss. E.g.,\n    ``\'train_loss_best\'`` will be matched with ``\'train_loss\'``. The\n    :class:`skorch.callbacks.EpochScoring` callback takes care of\n    creating those entries, which is why ``PrintLog`` works best in\n    conjunction with that callback.\n\n    ``PrintLog`` treats keys with the ``\'event_\'`` prefix in a special\n    way. They are assumed to contain information about occasionally\n    occuring events. The ``False`` or ``None`` entries (indicating\n    that an event did not occur) are not printed, resulting in empty\n    cells in the table, and ``True`` entries are printed with ``+``\n    symbol. ``PrintLog`` groups all event columns together and pushes\n    them to the right, just before the ``\'dur\'`` column.\n\n    *Note*: ``PrintLog`` will not result in good outputs if the number\n    of columns varies between epochs, e.g. if the valid loss is only\n    present on every other epoch.\n\n    Parameters\n    ----------\n    keys_ignored : str or list of str (default=None)\n      Key or list of keys that should not be part of the printed\n      table. Note that in addition to the keys provided by the user,\n      keys such as those starting with \'event_\' or ending on \'_best\'\n      are ignored by default.\n\n    sink : callable (default=print)\n      The target that the output string is sent to. By default, the\n      output is printed to stdout, but the sink could also be a\n      logger, etc.\n\n    tablefmt : str (default=\'simple\')\n      The format of the table. See the documentation of the ``tabulate``\n      package for more detail. Can be \'plain\', \'grid\', \'pipe\', \'html\',\n      \'latex\', among others.\n\n    floatfmt : str (default=\'.4f\')\n      The number formatting. See the documentation of the ``tabulate``\n      package for more details.\n\n    stralign : str (default=\'right\')\n      The alignment of columns with strings. Can be \'left\', \'center\',\n      \'right\', or ``None`` (disable alignment). Default is \'right\' (to\n      be consistent with numerical columns).\n\n    """"""\n    def __init__(\n            self,\n            keys_ignored=None,\n            sink=print,\n            tablefmt=\'simple\',\n            floatfmt=\'.4f\',\n            stralign=\'right\',\n    ):\n        self.keys_ignored = keys_ignored\n        self.sink = sink\n        self.tablefmt = tablefmt\n        self.floatfmt = floatfmt\n        self.stralign = stralign\n\n    def initialize(self):\n        self.first_iteration_ = True\n\n        keys_ignored = self.keys_ignored\n        if isinstance(keys_ignored, str):\n            keys_ignored = [keys_ignored]\n        self.keys_ignored_ = set(keys_ignored or [])\n        self.keys_ignored_.add(\'batches\')\n        return self\n\n    def format_row(self, row, key, color):\n        """"""For a given row from the table, format it (i.e. floating\n        points and color if applicable).\n\n        """"""\n        value = row[key]\n\n        if isinstance(value, bool) or value is None:\n            return \'+\' if value else \'\'\n\n        if not isinstance(value, Number):\n            return value\n\n        # determine if integer value\n        is_integer = float(value).is_integer()\n        template = \'{}\' if is_integer else \'{:\' + self.floatfmt + \'}\'\n\n        # if numeric, there could be a \'best\' key\n        key_best = key + \'_best\'\n        if (key_best in row) and row[key_best]:\n            template = color + template + Ansi.ENDC.value\n        return template.format(value)\n\n    def _sorted_keys(self, keys):\n        """"""Sort keys, dropping the ones that should be ignored.\n\n        The keys that are in ``self.ignored_keys`` or that end on\n        \'_best\' are dropped. Among the remaining keys:\n          * \'epoch\' is put first;\n          * \'dur\' is put last;\n          * keys that start with \'event_\' are put just before \'dur\';\n          * all remaining keys are sorted alphabetically.\n        """"""\n        sorted_keys = []\n\n        # make sure \'epoch\' comes first\n        if (\'epoch\' in keys) and (\'epoch\' not in self.keys_ignored_):\n            sorted_keys.append(\'epoch\')\n\n        # ignore keys like *_best or event_*\n        for key in filter_log_keys(sorted(keys), keys_ignored=self.keys_ignored_):\n            if key != \'dur\':\n                sorted_keys.append(key)\n\n        # add event_* keys\n        for key in sorted(keys):\n            if key.startswith(\'event_\') and (key not in self.keys_ignored_):\n                sorted_keys.append(key)\n\n        # make sure \'dur\' comes last\n        if (\'dur\' in keys) and (\'dur\' not in self.keys_ignored_):\n            sorted_keys.append(\'dur\')\n\n        return sorted_keys\n\n    def _yield_keys_formatted(self, row):\n        colors = cycle([color.value for color in Ansi if color != color.ENDC])\n        for key, color in zip(self._sorted_keys(row.keys()), colors):\n            formatted = self.format_row(row, key, color=color)\n            if key.startswith(\'event_\'):\n                key = key[6:]\n            yield key, formatted\n\n    def table(self, row):\n        headers = []\n        formatted = []\n        for key, formatted_row in self._yield_keys_formatted(row):\n            headers.append(key)\n            formatted.append(formatted_row)\n\n        return tabulate(\n            [formatted],\n            headers=headers,\n            tablefmt=self.tablefmt,\n            floatfmt=self.floatfmt,\n            stralign=self.stralign,\n        )\n\n    def _sink(self, text, verbose):\n        if (self.sink is not print) or verbose:\n            self.sink(text)\n\n    # pylint: disable=unused-argument\n    def on_epoch_end(self, net, **kwargs):\n        data = net.history[-1]\n        verbose = net.verbose\n        tabulated = self.table(data)\n\n        if self.first_iteration_:\n            header, lines = tabulated.split(\'\\n\', 2)[:2]\n            self._sink(header, verbose)\n            self._sink(lines, verbose)\n            self.first_iteration_ = False\n\n        self._sink(tabulated.rsplit(\'\\n\', 1)[-1], verbose)\n        if self.sink is print:\n            sys.stdout.flush()\n\n\nclass ProgressBar(Callback):\n    """"""Display a progress bar for each epoch.\n\n    The progress bar includes elapsed and estimated remaining time for\n    the current epoch, the number of batches processed, and other\n    user-defined metrics. The progress bar is erased once the epoch is\n    completed.\n\n    ``ProgressBar`` needs to know the total number of batches per\n    epoch in order to display a meaningful progress bar. By default,\n    this number is determined automatically using the dataset length\n    and the batch size. If this heuristic does not work for some\n    reason, you may either specify the number of batches explicitly\n    or let the ``ProgressBar`` count the actual number of batches in\n    the previous epoch.\n\n    For jupyter notebooks a non-ASCII progress bar can be printed\n    instead. To use this feature, you need to have `ipywidgets\n    <https://ipywidgets.readthedocs.io/en/stable/user_install.html>`_\n    installed.\n\n    Parameters\n    ----------\n\n    batches_per_epoch : int, str (default=\'auto\')\n      Either a concrete number or a string specifying the method used\n      to determine the number of batches per epoch automatically.\n      ``\'auto\'`` means that the number is computed from the length of\n      the dataset and the batch size. ``\'count\'`` means that the\n      number is determined by counting the batches in the previous\n      epoch. Note that this will leave you without a progress bar at\n      the first epoch.\n\n    detect_notebook : bool (default=True)\n      If enabled, the progress bar determines if its current environment\n      is a jupyter notebook and switches to a non-ASCII progress bar.\n\n    postfix_keys : list of str (default=[\'train_loss\', \'valid_loss\'])\n      You can use this list to specify additional info displayed in the\n      progress bar such as metrics and losses. A prerequisite to this is\n      that these values are residing in the history on batch level already,\n      i.e. they must be accessible via\n\n      >>> net.history[-1, \'batches\', -1, key]\n    """"""\n    def __init__(\n            self,\n            batches_per_epoch=\'auto\',\n            detect_notebook=True,\n            postfix_keys=None\n    ):\n        self.batches_per_epoch = batches_per_epoch\n        self.detect_notebook = detect_notebook\n        self.postfix_keys = postfix_keys or [\'train_loss\', \'valid_loss\']\n\n    def in_ipynb(self):\n        try:\n            return get_ipython().__class__.__name__ == \'ZMQInteractiveShell\'\n        except NameError:\n            return False\n\n    def _use_notebook(self):\n        return self.in_ipynb() if self.detect_notebook else False\n\n    def _get_batch_size(self, net, training):\n        name = \'iterator_train\' if training else \'iterator_valid\'\n        net_params = net.get_params()\n        return net_params.get(name + \'__batch_size\', net_params[\'batch_size\'])\n\n    def _get_batches_per_epoch_phase(self, net, dataset, training):\n        if dataset is None:\n            return 0\n        batch_size = self._get_batch_size(net, training)\n        return int(np.ceil(get_len(dataset) / batch_size))\n\n    def _get_batches_per_epoch(self, net, dataset_train, dataset_valid):\n        return (self._get_batches_per_epoch_phase(net, dataset_train, True) +\n                self._get_batches_per_epoch_phase(net, dataset_valid, False))\n\n    def _get_postfix_dict(self, net):\n        postfix = {}\n        for key in self.postfix_keys:\n            try:\n                postfix[key] = net.history[-1, \'batches\', -1, key]\n            except KeyError:\n                pass\n        return postfix\n\n    # pylint: disable=attribute-defined-outside-init\n    def on_batch_end(self, net, **kwargs):\n        self.pbar.set_postfix(self._get_postfix_dict(net), refresh=False)\n        self.pbar.update()\n\n    # pylint: disable=attribute-defined-outside-init, arguments-differ\n    def on_epoch_begin(self, net, dataset_train=None, dataset_valid=None, **kwargs):\n        # Assume it is a number until proven otherwise.\n        batches_per_epoch = self.batches_per_epoch\n\n        if self.batches_per_epoch == \'auto\':\n            batches_per_epoch = self._get_batches_per_epoch(\n                net, dataset_train, dataset_valid\n            )\n        elif self.batches_per_epoch == \'count\':\n            if len(net.history) <= 1:\n                # No limit is known until the end of the first epoch.\n                batches_per_epoch = None\n            else:\n                batches_per_epoch = len(net.history[-2, \'batches\'])\n\n        if self._use_notebook():\n            self.pbar = tqdm.tqdm_notebook(total=batches_per_epoch, leave=False)\n        else:\n            self.pbar = tqdm.tqdm(total=batches_per_epoch, leave=False)\n\n    def on_epoch_end(self, net, **kwargs):\n        self.pbar.close()\n\n\ndef rename_tensorboard_key(key):\n    """"""Rename keys from history to keys in TensorBoard\n\n    Specifically, prefixes all names with ""Loss/"" if they seem to be\n    losses.\n\n    """"""\n    if key.startswith(\'train\') or key.startswith(\'valid\'):\n        key = \'Loss/\' + key\n    return key\n\n\nclass TensorBoard(Callback):\n    """"""Logs results from history to TensorBoard\n\n    ""TensorBoard provides the visualization and tooling needed for\n    machine learning experimentation"" (tensorboard_)\n\n    Use this callback to automatically log all interesting values from\n    your net\'s history to tensorboard after each epoch.\n\n    The best way to log additional information is to subclass this\n    callback and add your code to one of the ``on_*`` methods.\n\n    Examples\n    --------\n    >>> # Example to log the bias parameter as a histogram\n    >>> def extract_bias(module):\n    ...     return module.hidden.bias\n\n    >>> class MyTensorBoard(TensorBoard):\n    ...     def on_epoch_end(self, net, **kwargs):\n    ...         bias = extract_bias(net.module_)\n    ...         epoch = net.history[-1, \'epoch\']\n    ...         self.writer.add_histogram(\'bias\', bias, global_step=epoch)\n    ...         super().on_epoch_end(net, **kwargs)  # call super last\n\n    Parameters\n    ----------\n    writer : torch.utils.tensorboard.writer.SummaryWriter\n      Instantiated ``SummaryWriter`` class.\n\n    close_after_train : bool (default=True)\n      Whether to close the ``SummaryWriter`` object once training\n      finishes. Set this parameter to False if you want to continue\n      logging with the same writer or if you use it as a context\n      manager.\n\n    keys_ignored : str or list of str (default=None)\n      Key or list of keys that should not be logged to\n      tensorboard. Note that in addition to the keys provided by the\n      user, keys such as those starting with \'event_\' or ending on\n      \'_best\' are ignored by default.\n\n    key_mapper : callable or function (default=rename_tensorboard_key)\n      This function maps a key name from the history to a tag in\n      tensorboard. This is useful because tensorboard can\n      automatically group similar tags if their names start with the\n      same prefix, followed by a forward slash. By default, this\n      callback will prefix all keys that start with ""train"" or ""valid""\n      with the ""Loss/"" prefix.\n\n    .. _tensorboard: https://www.tensorflow.org/tensorboard/\n\n    """"""\n    def __init__(\n            self,\n            writer,\n            close_after_train=True,\n            keys_ignored=None,\n            key_mapper=rename_tensorboard_key,\n    ):\n        self.writer = writer\n        self.close_after_train = close_after_train\n        self.keys_ignored = keys_ignored\n        self.key_mapper = key_mapper\n\n    def initialize(self):\n        self.first_batch_ = True\n\n        keys_ignored = self.keys_ignored\n        if isinstance(keys_ignored, str):\n            keys_ignored = [keys_ignored]\n        self.keys_ignored_ = set(keys_ignored or [])\n        self.keys_ignored_.add(\'batches\')\n        return self\n\n    def on_batch_end(self, net, **kwargs):\n        self.first_batch_ = False\n\n    def add_scalar_maybe(self, history, key, tag, global_step=None):\n        """"""Add a scalar value from the history to TensorBoard\n\n        Will catch errors like missing keys or wrong value types.\n\n        Parameters\n        ----------\n        history : skorch.History\n          History object saved as attribute on the neural net.\n\n        key : str\n          Key of the desired value in the history.\n\n        tag : str\n          Name of the tag used in TensorBoard.\n\n        global_step : int or None\n          Global step value to record.\n\n        """"""\n        hist = history[-1]\n        val = hist.get(key)\n        if val is None:\n            return\n\n        global_step = global_step if global_step is not None else hist[\'epoch\']\n        with suppress(NotImplementedError):\n            # pytorch raises NotImplementedError on wrong types\n            self.writer.add_scalar(\n                tag=tag,\n                scalar_value=val,\n                global_step=global_step,\n            )\n\n    def on_epoch_end(self, net, **kwargs):\n        """"""Automatically log values from the last history step.""""""\n        history = net.history\n        hist = history[-1]\n        epoch = hist[\'epoch\']\n\n        for key in filter_log_keys(hist, keys_ignored=self.keys_ignored_):\n            tag = self.key_mapper(key)\n            self.add_scalar_maybe(history, key=key, tag=tag, global_step=epoch)\n\n    def on_train_end(self, net, **kwargs):\n        if self.close_after_train:\n            self.writer.close()\n'"
skorch/callbacks/lr_scheduler.py,12,"b'""""""Contains learning rate scheduler callbacks""""""\n\nimport sys\n\n# pylint: disable=unused-import\nimport warnings\n\nimport numpy as np\nimport torch\nfrom torch.optim.lr_scheduler import _LRScheduler\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.optim.lr_scheduler import StepLR\n\ntry:\n    from torch.optim.lr_scheduler import CyclicLR as TorchCyclicLR\nexcept ImportError:\n    # Backward compatibility with torch >= 1.0 && < 1.1\n    TorchCyclicLR = None\nfrom torch.optim.optimizer import Optimizer\nfrom skorch.callbacks import Callback\n\n__all__ = [\'LRScheduler\', \'WarmRestartLR\']\n\n\ndef _check_lr(name, optimizer, lr):\n    """"""Return one learning rate for each param group.""""""\n    n = len(optimizer.param_groups)\n    if not isinstance(lr, (list, tuple)):\n        return lr * np.ones(n)\n\n    if len(lr) != n:\n        raise ValueError(""{} lr values were passed for {} but there are ""\n                         ""{} param groups."".format(n, name, len(lr)))\n    return np.array(lr)\n\n\nclass LRScheduler(Callback):\n    """"""Callback that sets the learning rate of each\n    parameter group according to some policy.\n\n    Parameters\n    ----------\n\n    policy : str or _LRScheduler class (default=\'WarmRestartLR\')\n      Learning rate policy name or scheduler to be used.\n\n    monitor : str or callable (default=None)\n      Value of the history to monitor or function/callable. In\n      the latter case, the callable receives the net instance as\n      argument and is expected to return the score (float) used to\n      determine the learning rate adjustment.\n\n    event_name: str, (default=\'event_lr\')\n      Name of event to be placed in history when the scheduler takes a step.\n      Pass ``None`` to disable placing events in history.\n      **Note:** This feature works only for pytorch version >=1.4\n\n    step_every: str, (default=\'epoch\')\n      Value for when to apply the learning scheduler step. Can be either \'batch\'\n       or \'epoch\'.\n\n    kwargs\n      Additional arguments passed to the lr scheduler.\n\n    """"""\n\n    def __init__(self,\n                 policy=\'WarmRestartLR\',\n                 monitor=\'train_loss\',\n                 event_name=""event_lr"",\n                 step_every=\'epoch\',\n                 **kwargs):\n        self.policy = policy\n        self.monitor = monitor\n        self.event_name = event_name\n        self.step_every = step_every\n        vars(self).update(kwargs)\n\n    def simulate(self, steps, initial_lr):\n        """"""\n        Simulates the learning rate scheduler.\n\n        Parameters\n        ----------\n        steps: int\n          Number of steps to simulate\n\n        initial_lr: float\n          Initial learning rate\n\n        Returns\n        -------\n        lrs: numpy ndarray\n          Simulated learning rates\n\n        """"""\n        test = torch.ones(1, requires_grad=True)\n        opt = torch.optim.SGD([{\'params\': test, \'lr\': initial_lr}])\n        policy_cls = self._get_policy_cls()\n        sch = policy_cls(opt, **self.kwargs)\n\n        lrs = []\n        for _ in range(steps):\n            opt.step()  # suppress warning about .step call order\n            lrs.append(opt.param_groups[0][\'lr\'])\n            sch.step()\n\n        return np.array(lrs)\n\n    def initialize(self):\n        self.policy_ = self._get_policy_cls()\n        self.lr_scheduler_ = None\n        self.batch_idx_ = 0\n        # TODO: Remove this warning on 0.10 release\n        if (self.policy_ == TorchCyclicLR or self.policy_ == ""TorchCyclicLR""\n                and self.step_every == \'epoch\'):\n            warnings.warn(\n                ""The LRScheduler now makes a step every epoch by default. ""\n                ""To have the cyclic lr scheduler update ""\n                ""every batch set step_every=\'batch\'"",\n                FutureWarning\n            )\n\n        return self\n\n    def _get_policy_cls(self):\n        if isinstance(self.policy, str):\n            return getattr(sys.modules[__name__], self.policy)\n        return self.policy\n\n    @property\n    def kwargs(self):\n        # These are the parameters that are passed to the\n        # scheduler. Parameters that don\'t belong there must be\n        # excluded.\n        excluded = (\'policy\', \'monitor\', \'event_name\', \'step_every\')\n        kwargs = {key: val for key, val in vars(self).items()\n                  if not (key in excluded or key.endswith(\'_\'))}\n        return kwargs\n\n    def on_train_begin(self, net, **kwargs):\n        if net.history:\n            try:\n                self.batch_idx_ = sum(net.history[:, \'train_batch_count\'])\n            except KeyError:\n                self.batch_idx_ = sum(len(b) for b in net.history[:, \'batches\'])\n        self.lr_scheduler_ = self._get_scheduler(\n            net, self.policy_, **self.kwargs\n        )\n\n    def on_epoch_end(self, net, **kwargs):\n        if not self.step_every == \'epoch\':\n            return\n        epoch = len(net.history)\n        if isinstance(self.lr_scheduler_, ReduceLROnPlateau):\n            if callable(self.monitor):\n                score = self.monitor(net)\n            else:\n                if self.lr_scheduler_.mode == \'max\':\n                    score = -np.inf\n                elif self.lr_scheduler_.mode == \'min\':\n                    score = np.inf\n                else:\n                    score = net.history[-1, self.monitor]\n\n            self.lr_scheduler_.step(score, epoch)\n            # ReduceLROnPlateau does not expose the current lr so it can\'t be recorded\n        else:\n            if self.event_name is not None and hasattr(\n                    self.lr_scheduler_, ""get_last_lr""):\n                net.history.record(self.event_name,\n                                   self.lr_scheduler_.get_last_lr()[0])\n            self.lr_scheduler_.step(epoch)\n\n    def on_batch_end(self, net, training, **kwargs):\n        if not training or not self.step_every == \'batch\':\n            return\n        if self.event_name is not None and hasattr(\n                self.lr_scheduler_, ""get_last_lr""):\n            net.history.record_batch(self.event_name,\n                                     self.lr_scheduler_.get_last_lr()[0])\n        self.lr_scheduler_.step()\n        self.batch_idx_ += 1\n\n    def _get_scheduler(self, net, policy, **scheduler_kwargs):\n        """"""Return scheduler, based on indicated policy, with appropriate\n        parameters.\n        """"""\n        if policy not in [ReduceLROnPlateau] and \\\n                \'last_epoch\' not in scheduler_kwargs:\n            last_epoch = len(net.history) - 1\n            scheduler_kwargs[\'last_epoch\'] = last_epoch\n\n        return policy(net.optimizer_, **scheduler_kwargs)\n\n\nclass WarmRestartLR(_LRScheduler):\n    """"""Stochastic Gradient Descent with Warm Restarts (SGDR) scheduler.\n\n    This scheduler sets the learning rate of each parameter group\n    according to stochastic gradient descent with warm restarts (SGDR)\n    policy. This policy simulates periodic warm restarts of SGD, where\n    in each restart the learning rate is initialize to some value and is\n    scheduled to decrease.\n\n    Parameters\n    ----------\n    optimizer : torch.optimizer.Optimizer instance.\n      Optimizer algorithm.\n\n    min_lr : float or list of float (default=1e-6)\n      Minimum allowed learning rate during each period for all\n      param groups (float) or each group (list).\n\n    max_lr : float or list of float (default=0.05)\n      Maximum allowed learning rate during each period for all\n      param groups (float) or each group (list).\n\n    base_period : int (default=10)\n      Initial restart period to be multiplied at each restart.\n\n    period_mult : int (default=2)\n      Multiplicative factor to increase the period between restarts.\n\n    last_epoch : int (default=-1)\n      The index of the last valid epoch.\n\n    References\n    ----------\n    .. [1] Ilya Loshchilov and Frank Hutter, 2017, ""Stochastic Gradient\n        Descent with Warm Restarts,"". ""ICLR""\n        `<https://arxiv.org/pdf/1608.03983.pdf>`_\n\n    """"""\n\n    def __init__(\n            self, optimizer,\n            min_lr=1e-6,\n            max_lr=0.05,\n            base_period=10,\n            period_mult=2,\n            last_epoch=-1\n    ):\n        self.min_lr = _check_lr(\'min_lr\', optimizer, min_lr)\n        self.max_lr = _check_lr(\'max_lr\', optimizer, max_lr)\n        self.base_period = base_period\n        self.period_mult = period_mult\n        super(WarmRestartLR, self).__init__(optimizer, last_epoch)\n\n    def _get_current_lr(self, min_lr, max_lr, period, epoch):\n        return min_lr + 0.5 * (max_lr - min_lr) * (\n                1 + np.cos(epoch * np.pi / period))\n\n    def get_lr(self):\n        epoch_idx = float(self.last_epoch)\n        current_period = float(self.base_period)\n        while epoch_idx / current_period > 1.0:\n            epoch_idx -= current_period + 1\n            current_period *= self.period_mult\n\n        current_lrs = self._get_current_lr(\n            self.min_lr,\n            self.max_lr,\n            current_period,\n            epoch_idx\n        )\n        return current_lrs.tolist()\n'"
skorch/callbacks/regularization.py,2,"b'"""""" Post-process regularization steps such as gradient normalizing. """"""\n\nfrom torch.nn.utils import clip_grad_norm_\n\nfrom skorch.callbacks import Callback\n\n\n__all__ = [\'GradientNormClipping\']\n\n\nclass GradientNormClipping(Callback):\n    """"""Clips gradient norm of a module\'s parameters.\n\n    The norm is computed over all gradients together, as if they were\n    concatenated into a single vector. Gradients are modified\n    in-place.\n\n    See :func:`torch.nn.utils.clip_grad_norm_` for more information.\n\n    Parameters\n    ----------\n    gradient_clip_value : float (default=None)\n      If not None, clip the norm of all model parameter gradients to this\n      value. The type of the norm is determined by the\n      ``gradient_clip_norm_type`` parameter and defaults to L2.\n\n    gradient_clip_norm_type : float (default=2)\n      Norm to use when gradient clipping is active. The default is\n      to use L2-norm. Can be \'inf\' for infinity norm.\n\n    """"""\n    def __init__(\n            self,\n            gradient_clip_value=None,\n            gradient_clip_norm_type=2,\n    ):\n        self.gradient_clip_value = gradient_clip_value\n        self.gradient_clip_norm_type = gradient_clip_norm_type\n\n    def on_grad_computed(self, _, named_parameters, **kwargs):\n        if self.gradient_clip_value is None:\n            return\n\n        clip_grad_norm_(\n            (p for _, p in named_parameters),\n            max_norm=self.gradient_clip_value,\n            norm_type=self.gradient_clip_norm_type,\n        )\n'"
skorch/callbacks/scoring.py,0,"b'"""""" Callbacks for calculating scores.""""""\n\nfrom contextlib import contextmanager\nfrom contextlib import suppress\nfrom distutils.version import LooseVersion\nfrom functools import partial\nimport warnings\n\nimport numpy as np\nimport sklearn\nfrom sklearn.metrics import make_scorer, check_scoring\n\nif LooseVersion(sklearn.__version__) >= \'0.22\':\n    from sklearn.metrics._scorer import _BaseScorer\nelse:\n    from sklearn.metrics.scorer import _BaseScorer\n\nfrom skorch.utils import data_from_dataset\nfrom skorch.utils import is_skorch_dataset\nfrom skorch.utils import to_numpy\nfrom skorch.callbacks import Callback\nfrom skorch.utils import check_indexing\nfrom skorch.utils import to_device\n\n\n__all__ = [\'BatchScoring\', \'EpochScoring\', \'PassthroughScoring\']\n\n\n@contextmanager\ndef cache_net_infer(net, use_caching, y_preds):\n    """"""Caching context for ``skorch.NeuralNet`` instance.\n\n    Returns a modified version of the net whose ``infer`` method will\n    subsequently return cached predictions. Leaving the context will\n    undo the overwrite of the ``infer`` method.\n\n    Deprecated.\n\n    """"""\n\n    warnings.warn(\n        ""cache_net_infer is no longer uesd to provide caching for ""\n        ""the scoring callbacks and will hence be removed in a ""\n        ""future release."",\n        DeprecationWarning,\n    )\n\n    if not use_caching:\n        yield net\n        return\n    y_preds = iter(y_preds)\n    net.infer = lambda *a, **kw: next(y_preds)\n\n    try:\n        yield net\n    finally:\n        # By setting net.infer we define an attribute `infer`\n        # that precedes the bound method `infer`. By deleting\n        # the entry from the attribute dict we undo this.\n        del net.__dict__[\'infer\']\n\n\n@contextmanager\ndef _cache_net_forward_iter(net, use_caching, y_preds):\n    """"""Caching context for ``skorch.NeuralNet`` instance.\n\n    Returns a modified version of the net whose ``forward_iter``\n    method will subsequently return cached predictions. Leaving the\n    context will undo the overwrite of the ``forward_iter`` method.\n\n    """"""\n    if not use_caching:\n        yield net\n        return\n    y_preds = iter(y_preds)\n\n    # pylint: disable=unused-argument\n    def cached_forward_iter(*args, device=net.device, **kwargs):\n        for yp in y_preds:\n            yield to_device(yp, device=device)\n\n    net.forward_iter = cached_forward_iter\n    try:\n        yield net\n    finally:\n        # By setting net.forward_iter we define an attribute\n        # `forward_iter` that precedes the bound method\n        # `forward_iter`. By deleting the entry from the attribute\n        # dict we undo this.\n        del net.__dict__[\'forward_iter\']\n\n\ndef convert_sklearn_metric_function(scoring):\n    """"""If ``scoring`` is a sklearn metric function, convert it to a\n    sklearn scorer and return it. Otherwise, return ``scoring`` unchanged.""""""\n    if callable(scoring):\n        module = getattr(scoring, \'__module__\', None)\n\n        # those are scoring objects returned by make_scorer starting\n        # from sklearn 0.22\n        scorer_names = (\'_PredictScorer\', \'_ProbaScorer\', \'_ThresholdScorer\')\n        if (\n                hasattr(module, \'startswith\') and\n                module.startswith(\'sklearn.metrics.\') and\n                not module.startswith(\'sklearn.metrics.scorer\') and\n                not module.startswith(\'sklearn.metrics.tests.\') and\n                not scoring.__class__.__name__ in scorer_names\n        ):\n            return make_scorer(scoring)\n    return scoring\n\n\nclass ScoringBase(Callback):\n    """"""Base class for scoring.\n\n    Subclass and implement an ``on_*`` method before using.\n    """"""\n    def __init__(\n            self,\n            scoring,\n            lower_is_better=True,\n            on_train=False,\n            name=None,\n            target_extractor=to_numpy,\n            use_caching=True,\n    ):\n        self.scoring = scoring\n        self.lower_is_better = lower_is_better\n        self.on_train = on_train\n        self.name = name\n        self.target_extractor = target_extractor\n        self.use_caching = use_caching\n\n    # pylint: disable=protected-access\n    def _get_name(self):\n        """"""Find name of scoring function.""""""\n        if self.name is not None:\n            return self.name\n        if self.scoring_ is None:\n            return \'score\'\n        if isinstance(self.scoring_, str):\n            return self.scoring_\n        if isinstance(self.scoring_, partial):\n            return self.scoring_.func.__name__\n        if isinstance(self.scoring_, _BaseScorer):\n            if hasattr(self.scoring_._score_func, \'__name__\'):\n                # sklearn < 0.22\n                return self.scoring_._score_func.__name__\n            else:\n                # sklearn >= 0.22\n                return self.scoring_._score_func._score_func.__name__\n        if isinstance(self.scoring_, dict):\n            raise ValueError(""Dict not supported as scorer for multi-metric scoring.""\n                             "" Register multiple scoring callbacks instead."")\n        return self.scoring_.__name__\n\n    def initialize(self):\n        self.best_score_ = np.inf if self.lower_is_better else -np.inf\n        self.scoring_ = convert_sklearn_metric_function(self.scoring)\n        self.name_ = self._get_name()\n        return self\n\n    # pylint: disable=attribute-defined-outside-init,arguments-differ\n    def on_train_begin(self, net, X, y, **kwargs):\n        self.X_indexing_ = check_indexing(X)\n        self.y_indexing_ = check_indexing(y)\n\n        # Looks for the right most index where `*_best` is True\n        # That index is used to get the best score in `net.history`\n        with suppress(ValueError, IndexError, KeyError):\n            best_name_history = net.history[:, \'{}_best\'.format(self.name_)]\n            idx_best_reverse = best_name_history[::-1].index(True)\n            idx_best = len(best_name_history) - idx_best_reverse - 1\n            self.best_score_ = net.history[idx_best, self.name_]\n\n    def _scoring(self, net, X_test, y_test):\n        """"""Resolve scoring and apply it to data. Use cached prediction\n        instead of running inference again, if available.""""""\n        scorer = check_scoring(net, self.scoring_)\n        return scorer(net, X_test, y_test)\n\n    def _is_best_score(self, current_score):\n        if self.lower_is_better is None:\n            return None\n        if self.lower_is_better:\n            return current_score < self.best_score_\n        return current_score > self.best_score_\n\n\nclass BatchScoring(ScoringBase):\n    """"""Callback that performs generic scoring on batches.\n\n    This callback determines the score after each batch and stores it\n    in the net\'s history in the column given by ``name``. At the end\n    of the epoch, the average of the scores are determined and also\n    stored in the history. Furthermore, it is determined whether this\n    average score is the best score yet and that information is also\n    stored in the history.\n\n    In contrast to :class:`.EpochScoring`, this callback determines\n    the score for each batch and then averages the score at the end of\n    the epoch. This can be disadvantageous for some scores if the\n    batch size is small -- e.g. area under the ROC will return\n    incorrect scores in this case. Therefore, it is recommnded to use\n    :class:`.EpochScoring` unless you really need the scores for each\n    batch.\n\n    If ``y`` is None, the ``scoring`` function with signature (model, X, y)\n    must be able to handle ``X`` as a ``Tensor`` and ``y=None``.\n\n    Parameters\n    ----------\n    scoring : None, str, or callable\n      If None, use the ``score`` method of the model. If str, it should\n      be a valid sklearn metric (e.g. ""f1_score"", ""accuracy_score""). If\n      a callable, it should have the signature (model, X, y), and it\n      should return a scalar. This works analogously to the ``scoring``\n      parameter in sklearn\'s ``GridSearchCV`` et al.\n\n    lower_is_better : bool (default=True)\n      Whether lower (e.g. log loss) or higher (e.g. accuracy) scores\n      are better.\n\n    on_train : bool (default=False)\n      Whether this should be called during train or validation.\n\n    name : str or None (default=None)\n      If not an explicit string, tries to infer the name from the\n      ``scoring`` argument.\n\n    target_extractor : callable (default=to_numpy)\n      This is called on y before it is passed to scoring.\n\n    use_caching : bool (default=True)\n      Re-use the model\'s prediction for computing the loss to calculate\n      the score. Turning this off will result in an additional inference\n      step for each batch.\n\n    """"""\n    # pylint: disable=unused-argument,arguments-differ\n\n    def on_batch_end(self, net, X, y, training, **kwargs):\n        if training != self.on_train:\n            return\n\n        y_preds = [kwargs[\'y_pred\']]\n        with _cache_net_forward_iter(net, self.use_caching, y_preds) as cached_net:\n            # In case of y=None we will not have gathered any samples.\n            # We expect the scoring function to deal with y=None.\n            y = None if y is None else self.target_extractor(y)\n            try:\n                score = self._scoring(cached_net, X, y)\n                cached_net.history.record_batch(self.name_, score)\n            except KeyError:\n                pass\n\n    def get_avg_score(self, history):\n        if self.on_train:\n            bs_key = \'train_batch_size\'\n        else:\n            bs_key = \'valid_batch_size\'\n\n        weights, scores = list(zip(\n            *history[-1, \'batches\', :, [bs_key, self.name_]]))\n        score_avg = np.average(scores, weights=weights)\n        return score_avg\n\n    # pylint: disable=unused-argument\n    def on_epoch_end(self, net, **kwargs):\n        history = net.history\n        try:  # don\'t raise if there is no valid data\n            history[-1, \'batches\', :, self.name_]\n        except KeyError:\n            return\n\n        score_avg = self.get_avg_score(history)\n        is_best = self._is_best_score(score_avg)\n        if is_best:\n            self.best_score_ = score_avg\n\n        history.record(self.name_, score_avg)\n        if is_best is not None:\n            history.record(self.name_ + \'_best\', bool(is_best))\n\n\nclass EpochScoring(ScoringBase):\n    """"""Callback that performs generic scoring on predictions.\n\n    At the end of each epoch, this callback makes a prediction on\n    train or validation data, determines the score for that prediction\n    and whether it is the best yet, and stores the result in the net\'s\n    history.\n\n    In case you already computed a score value for each batch you\n    can omit the score computation step by return the value from\n    the history. For example:\n\n        >>> def my_score(net, X=None, y=None):\n        ...     losses = net.history[-1, \'batches\', :, \'my_score\']\n        ...     batch_sizes = net.history[-1, \'batches\', :, \'valid_batch_size\']\n        ...     return np.average(losses, weights=batch_sizes)\n        >>> net = MyNet(callbacks=[\n        ...     (\'my_score\', Scoring(my_score, name=\'my_score\'))\n\n    If you fit with a custom dataset, this callback should work as\n    expected as long as ``use_caching=True`` which enables the\n    collection of ``y`` values from the dataset. If you decide to\n    disable the caching of predictions and ``y`` values, you need\n    to write your own scoring function that is able to deal with the\n    dataset and returns a scalar, for example:\n\n        >>> def ds_accuracy(net, ds, y=None):\n        ...     # assume ds yields (X, y), e.g. torchvision.datasets.MNIST\n        ...     y_true = [y for _, y in ds]\n        ...     y_pred = net.predict(ds)\n        ...     return sklearn.metrics.accuracy_score(y_true, y_pred)\n        >>> net = MyNet(callbacks=[\n        ...     EpochScoring(ds_accuracy, use_caching=False)])\n        >>> ds = torchvision.datasets.MNIST(root=mnist_path)\n        >>> net.fit(ds)\n\n    Parameters\n    ----------\n    scoring : None, str, or callable (default=None)\n      If None, use the ``score`` method of the model. If str, it\n      should be a valid sklearn scorer (e.g. ""f1"", ""accuracy""). If a\n      callable, it should have the signature (model, X, y), and it\n      should return a scalar. This works analogously to the\n      ``scoring`` parameter in sklearn\'s ``GridSearchCV`` et al.\n\n    lower_is_better : bool (default=True)\n      Whether lower scores should be considered better or worse.\n\n    on_train : bool (default=False)\n      Whether this should be called during train or validation data.\n\n    name : str or None (default=None)\n      If not an explicit string, tries to infer the name from the\n      ``scoring`` argument.\n\n    target_extractor : callable (default=to_numpy)\n      This is called on y before it is passed to scoring.\n\n    use_caching : bool (default=True)\n      Collect labels and predictions (``y_true`` and ``y_pred``)\n      over the course of one epoch and use the cached values for\n      computing the score. The cached values are shared between\n      all ``EpochScoring`` instances. Disabling this will result\n      in an additional inference step for each epoch and an\n      inability to use arbitrary datasets as input (since we\n      don\'t know how to extract ``y_true`` from an arbitrary\n      dataset).\n\n    """"""\n    def _initialize_cache(self):\n        self.y_trues_ = []\n        self.y_preds_ = []\n\n    def initialize(self):\n        super().initialize()\n        self._initialize_cache()\n        return self\n\n    # pylint: disable=arguments-differ,unused-argument\n    def on_epoch_begin(self, net, dataset_train, dataset_valid, **kwargs):\n        self._initialize_cache()\n\n    # pylint: disable=arguments-differ\n    def on_batch_end(\n            self, net, y, y_pred, training, **kwargs):\n        if not self.use_caching or training != self.on_train:\n            return\n\n        # We collect references to the prediction and target data\n        # emitted by the training process. Since we don\'t copy the\n        # data, all *Scoring callback instances use the same\n        # underlying data. This is also the reason why we don\'t run\n        # self.target_extractor(y) here but on epoch end, so that\n        # there are no copies of parts of y hanging around during\n        # training.\n        if y is not None:\n            self.y_trues_.append(y)\n        self.y_preds_.append(y_pred)\n\n    def get_test_data(self, dataset_train, dataset_valid):\n        """"""Return data needed to perform scoring.\n\n        This is a convenience method that handles picking of\n        train/valid, different types of input data, use of cache,\n        etc. for you.\n\n        Parameters\n        ----------\n        dataset_train\n          Incoming training data or dataset.\n\n        dataset_valid\n          Incoming validation data or dataset.\n\n        Returns\n        -------\n        X_test\n          Input data used for making the prediction.\n\n        y_test\n          Target ground truth. If caching was enabled, return cached\n          y_test.\n\n        y_pred : list\n          The predicted targets. If caching was disabled, the list is\n          empty. If caching was enabled, the list contains the batches\n          of the predictions. It may thus be necessary to concatenate\n          the output before working with it:\n          ``y_pred = np.concatenate(y_pred)``\n\n        """"""\n        dataset = dataset_train if self.on_train else dataset_valid\n\n        if self.use_caching:\n            X_test = dataset\n            y_pred = self.y_preds_\n            y_test = [self.target_extractor(y) for y in self.y_trues_]\n            # In case of y=None we will not have gathered any samples.\n            # We expect the scoring function to deal with y_test=None.\n            y_test = np.concatenate(y_test) if y_test else None\n            return X_test, y_test, y_pred\n\n        if is_skorch_dataset(dataset):\n            X_test, y_test = data_from_dataset(\n                dataset,\n                X_indexing=self.X_indexing_,\n                y_indexing=self.y_indexing_,\n            )\n        else:\n            X_test, y_test = dataset, None\n\n        if y_test is not None:\n            # We allow y_test to be None but the scoring function has\n            # to be able to deal with it (i.e. called without y_test).\n            y_test = self.target_extractor(y_test)\n        return X_test, y_test, []\n\n    def _record_score(self, history, current_score):\n        """"""Record the current store and, if applicable, if it\'s the best score\n        yet.\n\n        """"""\n        history.record(self.name_, current_score)\n\n        is_best = self._is_best_score(current_score)\n        if is_best is None:\n            return\n\n        history.record(self.name_ + \'_best\', bool(is_best))\n        if is_best:\n            self.best_score_ = current_score\n\n    # pylint: disable=unused-argument,arguments-differ\n    def on_epoch_end(\n            self,\n            net,\n            dataset_train,\n            dataset_valid,\n            **kwargs):\n        X_test, y_test, y_pred = self.get_test_data(dataset_train, dataset_valid)\n        if X_test is None:\n            return\n\n        with _cache_net_forward_iter(net, self.use_caching, y_pred) as cached_net:\n            current_score = self._scoring(cached_net, X_test, y_test)\n\n        self._record_score(net.history, current_score)\n\n    def on_train_end(self, *args, **kwargs):\n        self._initialize_cache()\n\n\nclass PassthroughScoring(Callback):\n    """"""Creates scores on epoch level based on batch level scores\n\n    This callback doesn\'t calculate any new scores but instead passes\n    through a score that was created on the batch level. Based on that\n    score, an average across the batch is created (honoring the batch\n    size) and recorded in the history for the given epoch.\n\n    Use this callback when there already is a score calculated on the\n    batch level. If that score has yet to be calculated, use\n    :class:`.BatchScoring` instead.\n\n    Parameters\n    ----------\n    name : str\n      Name of the score recorded on a batch level in the history.\n\n    lower_is_better : bool (default=True)\n      Whether lower (e.g. log loss) or higher (e.g. accuracy) scores\n      are better.\n\n    on_train : bool (default=False)\n      Whether this should be called during train or validation.\n\n    """"""\n    def __init__(\n            self,\n            name,\n            lower_is_better=True,\n            on_train=False,\n    ):\n        self.name = name\n        self.lower_is_better = lower_is_better\n        self.on_train = on_train\n\n    def initialize(self):\n        self.best_score_ = np.inf if self.lower_is_better else -np.inf\n        return self\n\n    def _is_best_score(self, current_score):\n        if self.lower_is_better is None:\n            return None\n        if self.lower_is_better:\n            return current_score < self.best_score_\n        return current_score > self.best_score_\n\n    def get_avg_score(self, history):\n        if self.on_train:\n            bs_key = \'train_batch_size\'\n        else:\n            bs_key = \'valid_batch_size\'\n\n        weights, scores = list(zip(\n            *history[-1, \'batches\', :, [bs_key, self.name]]))\n        score_avg = np.average(scores, weights=weights)\n        return score_avg\n\n    # pylint: disable=unused-argument,arguments-differ\n    def on_epoch_end(self, net, **kwargs):\n        history = net.history\n        try:  # don\'t raise if there is no valid data\n            history[-1, \'batches\', :, self.name]\n        except KeyError:\n            return\n\n        score_avg = self.get_avg_score(history)\n        is_best = self._is_best_score(score_avg)\n        if is_best:\n            self.best_score_ = score_avg\n\n        history.record(self.name, score_avg)\n        if is_best is not None:\n            history.record(self.name + \'_best\', bool(is_best))\n'"
skorch/callbacks/training.py,3,"b'"""""" Callbacks related to training progress. """"""\n\nimport os\nimport pickle\nimport warnings\nfrom contextlib import suppress\nfrom fnmatch import fnmatch\nfrom functools import partial\nfrom itertools import product\n\nimport numpy as np\nfrom skorch.callbacks import Callback\nfrom skorch.exceptions import SkorchException\nfrom skorch.utils import noop\nfrom skorch.utils import open_file_like\nfrom skorch.utils import freeze_parameter\nfrom skorch.utils import unfreeze_parameter\n\n\n__all__ = [\'Checkpoint\', \'EarlyStopping\', \'ParamMapper\', \'Freezer\',\n           \'Unfreezer\', \'Initializer\', \'LoadInitState\', \'TrainEndCheckpoint\']\n\n\nclass Checkpoint(Callback):\n    """"""Save the model during training if the given metric improved.\n\n    This callback works by default in conjunction with the validation\n    scoring callback since it creates a ``valid_loss_best`` value\n    in the history which the callback uses to determine if this\n    epoch is save-worthy.\n\n    You can also specify your own metric to monitor or supply a\n    callback that dynamically evaluates whether the model should\n    be saved in this epoch.\n\n    Some or all of the following can be saved:\n\n      - model parameters (see ``f_params`` parameter);\n      - optimizer state (see ``f_optimizer`` parameter);\n      - training history (see ``f_history`` parameter);\n      - entire model object (see ``f_pickle`` parameter).\n\n    You can implement your own save protocol by subclassing\n    ``Checkpoint`` and overriding :func:`~Checkpoint.save_model`.\n\n    This callback writes a bool flag to the history column\n    ``event_cp`` indicating whether a checkpoint was created or not.\n\n    Example:\n\n    >>> net = MyNet(callbacks=[Checkpoint()])\n    >>> net.fit(X, y)\n\n    Example using a custom monitor where models are saved only in\n    epochs where the validation *and* the train losses are best:\n\n    >>> monitor = lambda net: all(net.history[-1, (\n    ...     \'train_loss_best\', \'valid_loss_best\')])\n    >>> net = MyNet(callbacks=[Checkpoint(monitor=monitor)])\n    >>> net.fit(X, y)\n\n    Parameters\n    ----------\n    monitor : str, function, None\n      Value of the history to monitor or callback that determines\n      whether this epoch should lead to a checkpoint. The callback\n      takes the network instance as parameter.\n\n      In case ``monitor`` is set to ``None``, the callback will save\n      the network at every epoch.\n\n      **Note:** If you supply a lambda expression as monitor, you cannot\n      pickle the wrapper anymore as lambdas cannot be pickled. You can\n      mitigate this problem by using importable functions instead.\n\n    f_params : file-like object, str, None (default=\'params.pt\')\n      File path to the file or file-like object where the model\n      parameters should be saved. Pass ``None`` to disable saving\n      model parameters.\n\n      If the value is a string you can also use format specifiers\n      to, for example, indicate the current epoch. Accessible format\n      values are ``net``, ``last_epoch`` and ``last_batch``.\n      Example to include last epoch number in file name:\n\n      >>> cb = Checkpoint(f_params=""params_{last_epoch[epoch]}.pt"")\n\n    f_optimizer : file-like object, str, None (default=\'optimizer.pt\')\n      File path to the file or file-like object where the optimizer\n      state should be saved. Pass ``None`` to disable saving\n      model parameters.\n\n      Supports the same format specifiers as ``f_params``.\n\n    f_history : file-like object, str, None (default=\'history.json\')\n      File path to the file or file-like object where the model\n      training history should be saved. Pass ``None`` to disable\n      saving history.\n\n    f_pickle : file-like object, str, None (default=None)\n      File path to the file or file-like object where the entire\n      model object should be pickled. Pass ``None`` to disable\n      pickling.\n\n      Supports the same format specifiers as ``f_params``.\n\n    fn_prefix: str (default=\'\')\n      Prefix for filenames. If ``f_params``, ``f_optimizer``, ``f_history``,\n      or ``f_pickle`` are strings, they will be prefixed by ``fn_prefix``.\n\n    dirname: str (default=\'\')\n      Directory where files are stored.\n\n    event_name: str, (default=\'event_cp\')\n      Name of event to be placed in history when checkpoint is triggered.\n      Pass ``None`` to disable placing events in history.\n\n    sink : callable (default=noop)\n      The target that the information about created checkpoints is\n      sent to. This can be a logger or ``print`` function (to send to\n      stdout). By default the output is discarded.\n\n    """"""\n    def __init__(\n            self,\n            monitor=\'valid_loss_best\',\n            f_params=\'params.pt\',\n            f_optimizer=\'optimizer.pt\',\n            f_history=\'history.json\',\n            f_pickle=None,\n            fn_prefix=\'\',\n            dirname=\'\',\n            event_name=\'event_cp\',\n            sink=noop,\n    ):\n        self.monitor = monitor\n        self.f_params = f_params\n        self.f_optimizer = f_optimizer\n        self.f_history = f_history\n        self.f_pickle = f_pickle\n        self.fn_prefix = fn_prefix\n        self.dirname = dirname\n        self.event_name = event_name\n        self.sink = sink\n        self._validate_filenames()\n\n    def initialize(self):\n        self._validate_filenames()\n        if self.dirname and not os.path.exists(self.dirname):\n            os.makedirs(self.dirname, exist_ok=True)\n        return self\n\n    def on_epoch_end(self, net, **kwargs):\n        if ""{}_best"".format(self.monitor) in net.history[-1]:\n            warnings.warn(\n                ""Checkpoint monitor parameter is set to \'{0}\' and the history ""\n                ""contains \'{0}_best\'. Perhaps you meant to set the parameter ""\n                ""to \'{0}_best\'"".format(self.monitor), UserWarning)\n\n        if self.monitor is None:\n            do_checkpoint = True\n        elif callable(self.monitor):\n            do_checkpoint = self.monitor(net)\n        else:\n            try:\n                do_checkpoint = net.history[-1, self.monitor]\n            except KeyError as e:\n                raise SkorchException(\n                    ""Monitor value \'{}\' cannot be found in history. ""\n                    ""Make sure you have validation data if you use ""\n                    ""validation scores for checkpointing."".format(e.args[0]))\n\n        if self.event_name is not None:\n            net.history.record(self.event_name, bool(do_checkpoint))\n\n        if do_checkpoint:\n            self.save_model(net)\n            self._sink(""A checkpoint was triggered in epoch {}."".format(\n                len(net.history) + 1\n            ), net.verbose)\n\n    def save_model(self, net):\n        """"""Save the model.\n\n        This function saves some or all of the following:\n\n          - model parameters;\n          - optimizer state;\n          - training history;\n          - entire model object.\n        """"""\n        if self.f_params is not None:\n            f = self._format_target(net, self.f_params, -1)\n            self._save_params(f, net, ""f_params"", ""model parameters"")\n\n        if self.f_optimizer is not None:\n            f = self._format_target(net, self.f_optimizer, -1)\n            self._save_params(f, net, ""f_optimizer"", ""optimizer state"")\n\n        if self.f_history is not None:\n            f = self.f_history_\n            self._save_params(f, net, ""f_history"", ""history"")\n\n        if self.f_pickle:\n            f_pickle = self._format_target(net, self.f_pickle, -1)\n            with open_file_like(f_pickle, \'wb\') as f:\n                pickle.dump(net, f)\n\n    @property\n    def f_history_(self):\n        # This is a property and not in initialize to allow ``NeuralNet``\n        # to call ``load_params`` without needing the checkpoint to\n        # by initialized.\n        if self.f_history is None:\n            return None\n        return os.path.join(\n            self.dirname, self.fn_prefix + self.f_history)\n\n    def get_formatted_files(self, net):\n        """"""Returns a dictionary of formatted filenames""""""\n        idx = -1\n        if (\n                self.event_name is not None and\n                net.history\n        ):\n            for i, v in enumerate(net.history[:, self.event_name]):\n                if v:\n                    idx = i\n        return {\n            ""f_params"": self._format_target(net, self.f_params, idx),\n            ""f_optimizer"": self._format_target(net, self.f_optimizer, idx),\n            ""f_history"": self.f_history_,\n            ""f_pickle"": self._format_target(net, self.f_pickle, idx)\n        }\n\n    def _save_params(self, f, net, f_name, log_name):\n        try:\n            net.save_params(**{f_name: f})\n        except Exception as e:  # pylint: disable=broad-except\n            self._sink(\n                ""Unable to save {} to {}, {}: {}"".format(\n                    log_name, f, type(e).__name__, e), net.verbose)\n\n    def _format_target(self, net, f, idx):\n        """"""Apply formatting to the target filename template.""""""\n        if f is None:\n            return None\n        if isinstance(f, str):\n            f = self.fn_prefix + f.format(\n                net=net,\n                last_epoch=net.history[idx],\n                last_batch=net.history[idx, \'batches\', -1],\n            )\n            return os.path.join(self.dirname, f)\n        return f\n\n    def _validate_filenames(self):\n        """"""Checks if passed filenames are valid.\n\n        Specifically, f_* parameter should not be passed in\n        conjunction with dirname.\n\n        """"""\n        if not self.dirname:\n            return\n\n        def _is_truthy_and_not_str(f):\n            return f and not isinstance(f, str)\n\n        if (\n                _is_truthy_and_not_str(self.f_optimizer) or\n                _is_truthy_and_not_str(self.f_params) or\n                _is_truthy_and_not_str(self.f_history) or\n                _is_truthy_and_not_str(self.f_pickle)\n        ):\n            raise SkorchException(\n                \'dirname can only be used when f_* are strings\')\n\n    def _sink(self, text, verbose):\n        #  We do not want to be affected by verbosity if sink is not print\n        if (self.sink is not print) or verbose:\n            self.sink(text)\n\n\nclass EarlyStopping(Callback):\n    """"""Callback for stopping training when scores don\'t improve.\n\n    Stop training early if a specified `monitor` metric did not\n    improve in `patience` number of epochs by at least `threshold`.\n\n    Parameters\n    ----------\n    monitor : str (default=\'valid_loss\')\n      Value of the history to monitor to decide whether to stop\n      training or not.  The value is expected to be double and is\n      commonly provided by scoring callbacks such as\n      :class:`skorch.callbacks.EpochScoring`.\n\n    lower_is_better : bool (default=True)\n      Whether lower scores should be considered better or worse.\n\n    patience : int (default=5)\n      Number of epochs to wait for improvement of the monitor value\n      until the training process is stopped.\n\n    threshold : int (default=1e-4)\n      Ignore score improvements smaller than `threshold`.\n\n    threshold_mode : str (default=\'rel\')\n        One of `rel`, `abs`. Decides whether the `threshold` value is\n        interpreted in absolute terms or as a fraction of the best\n        score so far (relative)\n\n    sink : callable (default=print)\n      The target that the information about early stopping is\n      sent to. By default, the output is printed to stdout, but the\n      sink could also be a logger or :func:`~skorch.utils.noop`.\n\n    """"""\n    def __init__(\n            self,\n            monitor=\'valid_loss\',\n            patience=5,\n            threshold=1e-4,\n            threshold_mode=\'rel\',\n            lower_is_better=True,\n            sink=print,\n    ):\n        self.monitor = monitor\n        self.lower_is_better = lower_is_better\n        self.patience = patience\n        self.threshold = threshold\n        self.threshold_mode = threshold_mode\n        self.misses_ = 0\n        self.dynamic_threshold_ = None\n        self.sink = sink\n\n    # pylint: disable=arguments-differ\n    def on_train_begin(self, net, **kwargs):\n        if self.threshold_mode not in [\'rel\', \'abs\']:\n            raise ValueError(""Invalid threshold mode: \'{}\'""\n                             .format(self.threshold_mode))\n        self.misses_ = 0\n        self.dynamic_threshold_ = np.inf if self.lower_is_better else -np.inf\n\n    def on_epoch_end(self, net, **kwargs):\n        current_score = net.history[-1, self.monitor]\n        if not self._is_score_improved(current_score):\n            self.misses_ += 1\n        else:\n            self.misses_ = 0\n            self.dynamic_threshold_ = self._calc_new_threshold(current_score)\n        if self.misses_ == self.patience:\n            if net.verbose:\n                self._sink(""Stopping since {} has not improved in the last ""\n                           ""{} epochs."".format(self.monitor, self.patience),\n                           verbose=net.verbose)\n            raise KeyboardInterrupt\n\n    def _is_score_improved(self, score):\n        if self.lower_is_better:\n            return score < self.dynamic_threshold_\n        return score > self.dynamic_threshold_\n\n    def _calc_new_threshold(self, score):\n        """"""Determine threshold based on score.""""""\n        if self.threshold_mode == \'rel\':\n            abs_threshold_change = self.threshold * score\n        else:\n            abs_threshold_change = self.threshold\n\n        if self.lower_is_better:\n            new_threshold = score - abs_threshold_change\n        else:\n            new_threshold = score + abs_threshold_change\n        return new_threshold\n\n    def _sink(self, text, verbose):\n        #  We do not want to be affected by verbosity if sink is not print\n        if (self.sink is not print) or verbose:\n            self.sink(text)\n\n\nclass ParamMapper(Callback):\n    """"""Map arbitrary functions over module parameters filtered by pattern\n    matching.\n\n    In the simplest case the function is only applied once at\n    the beginning of a given epoch (at ``on_epoch_begin``) but more complex\n    execution schemes (e.g. periodic application) are possible using\n    ``at`` and ``scheduler``.\n\n    Notes\n    -----\n    When starting the training process after saving and loading a model,\n    ``ParamMapper`` might re-initialize parts of your model when the\n    history is not saved along with the model. To avoid this, in case\n    you use ``ParamMapper`` (or subclasses, e.g. :class:`.Initializer`)\n    and want to save your model make sure to either (a) use pickle,\n    (b) save and load the history or (c) remove the parameter mapper\n    callbacks before continuing training.\n\n    Examples\n    --------\n    Initialize a layer on first epoch before the first training step:\n\n    >>> init = partial(torch.nn.init.uniform_, a=0, b=1)\n    >>> cb = ParamMapper(\'linear*.weight\', at=1, fn=init)\n    >>> net = Net(myModule, callbacks=[cb])\n\n    Reset layer initialization if train loss reaches a certain value\n    (e.g. re-initialize on overfit):\n\n    >>> at = lambda net: net.history[-1, \'train_loss\'] < 0.1\n    >>> init = partial(torch.nn.init.uniform_, a=0, b=1)\n    >>> cb = ParamMapper(\'linear0.weight\', at=at, fn=init)\n    >>> net = Net(myModule, callbacks=[cb])\n\n    Periodically freeze and unfreeze all embedding layers:\n\n    >>> def my_sched(net):\n    ...    if len(net.history) % 2 == 0:\n    ...        return skorch.utils.freeze_parameter\n    ...    else:\n    ...        return skorch.utils.unfreeze_parameter\n    >>> cb = ParamMapper(\'embedding*.weight\', schedule=my_sched)\n    >>> net = Net(myModule, callbacks=[cb])\n\n    Parameters\n    ----------\n    patterns : str or callable or list\n      The pattern(s) to match parameter names against. Patterns are\n      UNIX globbing patterns as understood by :func:`~fnmatch.fnmatch`.\n      Patterns can also be callables which will get called with the\n      parameter name and are regarded as a match when the callable\n      returns a truthy value.\n\n      This parameter also supports lists of str or callables so that\n      one ``ParamMapper`` can match a group of parameters.\n\n      Example: ``\'linear*.weight\'`` or ``[\'linear0.*\', \'linear1.bias\']``\n      or ``lambda name: name.startswith(\'linear\')``.\n\n    fn : function\n      The function to apply to each parameter separately.\n\n    at : int or callable\n      In case you specify an integer it represents the epoch number the\n      function ``fn`` is applied to the parameters, in case ``at`` is\n      a function it will receive ``net`` as parameter and the function\n      is applied to the parameter once ``at`` returns ``True``.\n\n    schedule : callable or None\n      If specified this callable supersedes the static ``at``/``fn``\n      combination by dynamically returning the function that is applied\n      on the matched parameters. This way you can, for example, create a\n      schedule that periodically freezes and unfreezes layers.\n\n      The callable\'s signature is ``schedule(net: NeuralNet) -> callable``.\n\n    """"""\n    def __init__(self, patterns, fn=noop, at=1, schedule=None):\n        self.at = at\n        self.fn = fn\n        self.schedule = schedule\n        self.patterns = patterns\n\n    def initialize(self):\n        if not self.schedule:\n            self.schedule = self._default_schedule\n\n        if not isinstance(self.patterns, (list, tuple)):\n            self.patterns = [self.patterns]\n\n        if isinstance(self.at, int):\n            if self.at <= 0:\n                raise ValueError(\n                    \'Invalid value for `at` (at={}). The first possible \'\n                    \'epoch number is 1.\'.format(self.at))\n            self.at = partial(self._epoch_at, epoch=self.at)\n\n        return self\n\n    def named_parameters(self, net):\n        return net.module_.named_parameters()\n\n    def filter_parameters(self, patterns, params):\n        pattern_fns = (\n            pattern if callable(pattern) else partial(fnmatch, pat=pattern)\n            for pattern in patterns\n        )\n        for pattern_fn, (name, param) in product(pattern_fns, params):\n            if pattern_fn(name):\n                yield name, param\n\n    def _default_schedule(self, net):\n        if self.at(net):\n            return self.fn\n        return noop\n\n    def _epoch_at(self, net, epoch=1):\n        return len(net.history) == epoch\n\n    def on_epoch_begin(self, net, **kwargs):\n        params = self.named_parameters(net)\n        params = self.filter_parameters(self.patterns, params)\n        map_fn = self.schedule(net)\n\n        for _, p in params:\n            map_fn(p)\n\n\n\nclass Freezer(ParamMapper):\n    """"""Freeze matching parameters at the start of the first epoch. You may\n    specify a specific point in time (either by epoch number or using a\n    callable) when the parameters are frozen using the ``at`` parameter.\n\n    See :class:`.ParamMapper` for details.\n    """"""\n    def __init__(self, *args, **kwargs):\n        kwargs[\'at\'] = kwargs.get(\'at\', 1)\n        kwargs[\'fn\'] = kwargs.get(\'fn\', freeze_parameter)\n        super().__init__(*args, **kwargs)\n\n\nclass Unfreezer(ParamMapper):\n    """"""Inverse operation of :class:`.Freezer`.""""""\n    def __init__(self, *args, **kwargs):\n        kwargs[\'at\'] = kwargs.get(\'at\', 1)\n        kwargs[\'fn\'] = kwargs.get(\'fn\', unfreeze_parameter)\n        super().__init__(*args, **kwargs)\n\n\nclass Initializer(ParamMapper):\n    """"""Apply any function on matching parameters in the first epoch.\n\n    Examples\n    --------\n\n    Use ``Initializer`` to initialize all dense layer weights with\n    values sampled from an uniform distribution on the beginning of\n    the first epoch:\n\n    >>> init_fn = partial(torch.nn.init.uniform_, a=-1e-3, b=1e-3)\n    >>> cb = Initializer(\'dense*.weight\', fn=init_fn)\n    >>> net = Net(myModule, callbacks=[cb])\n    """"""\n    def __init__(self, *args, **kwargs):\n        kwargs[\'at\'] = kwargs.get(\'at\', 1)\n        super().__init__(*args, **kwargs)\n\n\nclass LoadInitState(Callback):\n    """"""Loads the model, optimizer, and history from a checkpoint into a\n    :class:`.NeuralNet` when training begins.\n\n    Examples\n    --------\n\n    Consider running the following example multiple times:\n\n    >>> cp = Checkpoint(monitor=\'valid_loss_best\')\n    >>> load_state = LoadInitState(cp)\n    >>> net = NeuralNet(..., callbacks=[cp, load_state])\n    >>> net.fit(X, y)\n\n    On the first run, the :class:`.Checkpoint` saves the model, optimizer, and\n    history when the validation loss is minimized. During the first run,\n    there are no files on disk, thus :class:`.LoadInitState` will\n    not load anything. When running the example a second time,\n    :class:`LoadInitState` will load the best model from the first run and\n    continue training from there.\n\n    Parameters\n    ----------\n    checkpoint: :class:`.Checkpoint`\n      Checkpoint to get filenames from.\n\n    """"""\n    def __init__(self, checkpoint):\n        self.checkpoint = checkpoint\n\n    def initialize(self):\n        self.did_load_ = False\n        return self\n\n    def on_train_begin(self, net,\n                       X=None, y=None, **kwargs):\n        if not self.did_load_:\n            self.did_load_ = True\n            with suppress(Exception):\n                net.load_params(checkpoint=self.checkpoint)\n\n\nclass TrainEndCheckpoint(Callback):\n    """"""Saves the model parameters, optimizer state, and history at the end of\n    training. The default ``fn_prefix`` is \'train_end_\'.\n\n    Examples\n    --------\n\n    Consider running the following example multiple times:\n\n    >>> train_end_cp = TrainEndCheckpoint(dirname=\'exp1\')\n    >>> load_state = LoadInitState(train_end_cp)\n    >>> net = NeuralNet(..., callbacks=[train_end_cp, load_state])\n    >>> net.fit(X, y)\n\n    After the first run, model parameters, optimizer state, and history are\n    saved into a directory named `exp1`. On the next run, `LoadInitState` will\n    load the state from the first run and continue training.\n\n    Parameters\n    ----------\n\n    f_params : file-like object, str, None (default=\'params.pt\')\n      File path to the file or file-like object where the model\n      parameters should be saved. Pass ``None`` to disable saving\n      model parameters.\n\n      If the value is a string you can also use format specifiers\n      to, for example, indicate the current epoch. Accessible format\n      values are ``net``, ``last_epoch`` and ``last_batch``.\n      Example to include last epoch number in file name:\n\n      >>> cb = Checkpoint(f_params=""params_{last_epoch[epoch]}.pt"")\n\n    f_optimizer : file-like object, str, None (default=\'optimizer.pt\')\n      File path to the file or file-like object where the optimizer\n      state should be saved. Pass ``None`` to disable saving\n      model parameters.\n\n      Supports the same format specifiers as ``f_params``.\n\n    f_history : file-like object, str, None (default=\'history.json\')\n      File path to the file or file-like object where the model\n      training history should be saved. Pass ``None`` to disable\n      saving history.\n\n    f_pickle : file-like object, str, None (default=None)\n      File path to the file or file-like object where the entire\n      model object should be pickled. Pass ``None`` to disable\n      pickling.\n\n      Supports the same format specifiers as ``f_params``.\n\n    fn_prefix: str (default=\'train_end_\')\n      Prefix for filenames. If ``f_params``, ``f_optimizer``, ``f_history``,\n      or ``f_pickle`` are strings, they will be prefixed by ``fn_prefix``.\n\n    dirname: str (default=\'\')\n      Directory where files are stored.\n\n    sink : callable (default=noop)\n      The target that the information about created checkpoints is\n      sent to. This can be a logger or ``print`` function (to send to\n      stdout). By default the output is discarded.\n    """"""\n    def __init__(\n            self,\n            f_params=\'params.pt\',\n            f_optimizer=\'optimizer.pt\',\n            f_history=\'history.json\',\n            f_pickle=None,\n            fn_prefix=\'train_end_\',\n            dirname=\'\',\n            sink=noop,\n    ):\n        self.f_params = f_params\n        self.f_optimizer = f_optimizer\n        self.f_history = f_history\n        self.f_pickle = f_pickle\n        self.fn_prefix = fn_prefix\n        self.dirname = dirname\n        self.sink = sink\n\n    def initialize(self):\n        self.checkpoint_ = Checkpoint(\n            monitor=None,\n            f_params=self.f_params,\n            f_optimizer=self.f_optimizer,\n            f_history=self.f_history,\n            f_pickle=self.f_pickle,\n            fn_prefix=self.fn_prefix,\n            dirname=self.dirname,\n            event_name=None,\n            sink=self.sink)\n        self.checkpoint_.initialize()\n\n    def on_train_end(self, net, **kwargs):\n        self.checkpoint_.save_model(net)\n        self.checkpoint_._sink(""Final checkpoint triggered"", net.verbose)\n\n    def __getattr__(self, attr):\n        return getattr(self.checkpoint_, attr)\n'"
skorch/tests/__init__.py,0,b''
skorch/tests/conftest.py,2,"b'""""""Contains shared fixtures, hooks, etc.""""""\n\nfrom unittest.mock import Mock\n\nimport numpy as np\nimport pytest\nfrom sklearn.datasets import make_classification\nfrom sklearn.datasets import make_regression\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom torch import nn\n\nF = nn.functional\n\nINFERENCE_METHODS = [\'predict\', \'predict_proba\', \'forward\', \'forward_iter\']\n\n\n###################\n# shared fixtures #\n###################\n\n\n@pytest.fixture(autouse=True)\ndef seeds_fixed():\n    torch.manual_seed(0)\n    torch.cuda.manual_seed(0)\n    np.random.seed(0)\n\n\n@pytest.fixture\ndef module_cls():\n    """"""Simple mock module for triggering scoring.\n\n    This module returns the input without modifying it.\n\n    """"""\n\n    class MyModule(nn.Module):\n        def __init__(self):\n            super(MyModule, self).__init__()\n            self.dense = nn.Linear(1, 1)\n\n        # pylint: disable=arguments-differ\n        def forward(self, X):\n            X = X + 0.0 * self.dense(X)\n            return X\n\n    return MyModule\n\n\n@pytest.fixture(scope=\'module\')\ndef classifier_module():\n    """"""Return a simple classifier module class.""""""\n    from skorch.toy import make_classifier\n    return make_classifier(\n        input_units=20,\n        hidden_units=10,\n        num_hidden=2,\n        dropout=0.5,\n    )\n\n\n@pytest.fixture(scope=\'module\')\ndef multiouput_module():\n    """"""Return a simple classifier module class.""""""\n\n    class MultiOutput(nn.Module):\n        """"""Simple classification module.""""""\n\n        def __init__(self, input_units=20):\n            super(MultiOutput, self).__init__()\n            self.output = nn.Linear(input_units, 2)\n\n        # pylint: disable=arguments-differ\n        def forward(self, X):\n            X = F.softmax(self.output(X), dim=-1)\n            return X, X[:, 0], X[::2]\n\n    return MultiOutput\n\n\n@pytest.fixture(scope=\'module\')\ndef classifier_data():\n    X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n    return X.astype(np.float32), y\n\n\n@pytest.fixture(scope=\'module\')\ndef regression_data():\n    X, y = make_regression(\n        1000, 20, n_informative=10, bias=0, random_state=0)\n    X, y = X.astype(np.float32), y.astype(np.float32).reshape(-1, 1)\n    Xt = StandardScaler().fit_transform(X)\n    yt = StandardScaler().fit_transform(y)\n    return Xt, yt\n\n\n@pytest.fixture(scope=\'module\')\ndef multioutput_regression_data():\n    X, y = make_regression(\n        1000, 20, n_targets=3, n_informative=10, bias=0, random_state=0)\n    X, y = X.astype(np.float32), y.astype(np.float32)\n    Xt = StandardScaler().fit_transform(X)\n    yt = StandardScaler().fit_transform(y)\n    return Xt, yt\n\n\n@pytest.fixture\ndef score55():\n    """"""Simple scoring function.""""""\n\n    # pylint: disable=unused-argument\n    def func(est, X, y, foo=123):\n        return 55\n\n    func.__name__ = \'score55\'\n    return func\n\n\n@pytest.fixture\ndef train_split():\n    # pylint: disable=unused-argument\n    def func(dataset, y):\n        ds_train = type(dataset)(dataset.X[:2], dataset.y[:2])\n        ds_valid = type(dataset)(dataset.X[2:], dataset.y[2:])\n        return ds_train, ds_valid\n\n    return func\n\n\n@pytest.fixture\ndef net_cls():\n    from skorch import NeuralNetRegressor\n    return NeuralNetRegressor\n\n\n@pytest.fixture\ndef data():\n    X = np.array([0, 2, 3, 0]).astype(np.float32).reshape(-1, 1)\n    y = np.array([-1, 0, 5, 4]).astype(np.float32).reshape(-1, 1)\n    return X, y\n\n\nneptune_installed = False\ntry:\n    # pylint: disable=unused-import\n    import neptune\n\n    neptune_installed = True\nexcept ImportError:\n    pass\n\nwandb_installed = False\ntry:\n    # pylint: disable=unused-import\n    import wandb\n\n    wandb_installed = True\nexcept ImportError:\n    pass\n\npandas_installed = False\ntry:\n    # pylint: disable=unused-import\n    import pandas\n\n    pandas_installed = True\nexcept ImportError:\n    pass\n\ntensorboard_installed = False\ntry:\n    # pylint: disable=unused-import\n    import tensorboard\n\n    tensorboard_installed = True\nexcept ImportError:\n    pass\n'"
skorch/tests/test_classifier.py,6,"b'""""""Tests for classifier.py\n\nOnly contains tests that are specific for classifier subclasses.\n\n""""""\n\nfrom unittest.mock import Mock\n\nfrom flaky import flaky\nimport numpy as np\nimport pytest\nimport torch\nfrom sklearn.base import clone\nfrom torch import nn\n\nfrom skorch.tests.conftest import INFERENCE_METHODS\n\n\nclass TestNeuralNet:\n    @pytest.fixture(scope=\'module\')\n    def data(self, classifier_data):\n        return classifier_data\n\n    @pytest.fixture(scope=\'module\')\n    def dummy_callback(self):\n        from skorch.callbacks import Callback\n        cb = Mock(spec=Callback)\n        # make dummy behave like an estimator\n        cb.get_params.return_value = {}\n        cb.set_params = lambda **kwargs: cb\n        return cb\n\n    @pytest.fixture(scope=\'module\')\n    def net_cls(self):\n        from skorch import NeuralNetClassifier\n        return NeuralNetClassifier\n\n    @pytest.fixture(scope=\'module\')\n    def module_cls(self, classifier_module):\n        return classifier_module\n\n    @pytest.fixture(scope=\'module\')\n    def net(self, net_cls, module_cls, dummy_callback):\n        return net_cls(\n            module_cls,\n            callbacks=[(\'dummy\', dummy_callback)],\n            max_epochs=10,\n            lr=0.1,\n        )\n\n    @pytest.fixture(scope=\'module\')\n    def net_fit(self, net, data):\n        # Careful, don\'t call additional fits on this, since that would have\n        # side effects on other tests.\n        X, y = data\n        return net.fit(X, y)\n\n    def test_clone(self, net_fit):\n        clone(net_fit)\n\n    def test_predict_and_predict_proba(self, net_fit, data):\n        X = data[0]\n\n        y_proba = net_fit.predict_proba(X)\n        assert np.allclose(y_proba.sum(1), 1, rtol=1e-5)\n\n        y_pred = net_fit.predict(X)\n        assert np.allclose(np.argmax(y_proba, 1), y_pred, rtol=1e-5)\n\n    def test_score(self, net_fit, data):\n        X, y = data\n        accuracy = net_fit.score(X, y)\n        assert 0. <= accuracy <= 1.\n\n    # classifier-specific test\n    def test_takes_log_with_nllloss(self, net_cls, module_cls, data):\n        net = net_cls(module_cls, criterion=nn.NLLLoss, max_epochs=1)\n        net.initialize()\n\n        mock_loss = Mock(side_effect=nn.NLLLoss())\n        net.criterion_.forward = mock_loss\n        net.partial_fit(*data)  # call partial_fit to avoid re-initialization\n\n        # check that loss was called with log-probabilities\n        for (y_log, _), _ in mock_loss.call_args_list:\n            assert (y_log < 0).all()\n            y_proba = torch.exp(y_log)\n            assert torch.isclose(torch.ones(len(y_proba)), y_proba.sum(1)).all()\n\n    # classifier-specific test\n    def test_takes_no_log_without_nllloss(self, net_cls, module_cls, data):\n        net = net_cls(module_cls, criterion=nn.BCELoss, max_epochs=1)\n        net.initialize()\n\n        mock_loss = Mock(side_effect=nn.NLLLoss())\n        net.criterion_.forward = mock_loss\n        net.partial_fit(*data)  # call partial_fit to avoid re-initialization\n\n        # check that loss was called with raw probabilities\n        for (y_out, _), _ in mock_loss.call_args_list:\n            assert not (y_out < 0).all()\n            assert torch.isclose(torch.ones(len(y_out)), y_out.sum(1)).all()\n\n    # classifier-specific test\n    def test_high_learning_rate(self, net_cls, module_cls, data):\n        # regression test for nan loss with high learning rates issue #481\n        net = net_cls(module_cls, max_epochs=2, lr=2, optimizer=torch.optim.Adam)\n        net.fit(*data)\n        assert np.any(~np.isnan(net.history[:, \'train_loss\']))\n\n    def test_binary_classes_set_by_default(self, net_cls, module_cls, data):\n        net = net_cls(module_cls).fit(*data)\n        assert (net.classes_ == [0, 1]).all()\n\n    def test_non_binary_classes_set_by_default(self, net_cls, module_cls, data):\n        X = data[0]\n        y = np.arange(len(X)) % 10\n        net = net_cls(module_cls, max_epochs=0).fit(X, y)\n        assert (net.classes_ == [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).all()\n\n    def test_classes_data_torch_tensor(self, net_cls, module_cls, data):\n        X = torch.as_tensor(data[0])\n        y = torch.as_tensor(np.arange(len(X)) % 10)\n\n        net = net_cls(module_cls, max_epochs=0).fit(X, y)\n        assert (net.classes_ == [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).all()\n\n    def test_classes_with_gaps(self, net_cls, module_cls, data):\n        X = data[0]\n        y = np.arange(len(X)) % 10\n        y[(y == 0) | (y == 5)] = 4  # remove classes 0 and 5\n        net = net_cls(module_cls, max_epochs=0).fit(X, y)\n        assert (net.classes_ == [1, 2, 3, 4, 6, 7, 8, 9]).all()\n\n    def test_pass_classes_explicitly_overrides(self, net_cls, module_cls, data):\n        net = net_cls(module_cls, max_epochs=0, classes=[\'foo\', \'bar\']).fit(*data)\n        assert net.classes_ == [\'foo\', \'bar\']\n\n    @pytest.mark.parametrize(\'classes\', [[], np.array([])])\n    def test_pass_empty_classes_raises(\n            self, net_cls, module_cls, data, classes):\n        net = net_cls(\n            module_cls, max_epochs=0, classes=classes).fit(*data).fit(*data)\n        with pytest.raises(AttributeError) as exc:\n            net.classes_\n\n        msg = exc.value.args[0]\n        expected = ""NeuralNetClassifier has no attribute \'classes_\'""\n        assert msg == expected\n\n    def test_with_calibrated_classifier_cv(self, net_fit, data):\n        from sklearn.calibration import CalibratedClassifierCV\n        cccv = CalibratedClassifierCV(net_fit, cv=2)\n        cccv.fit(*data)\n\n\nclass TestNeuralNetBinaryClassifier:\n    @pytest.fixture(scope=\'module\')\n    def data(self, classifier_data):\n        X, y = classifier_data\n        return X, y.astype(\'float32\')\n\n    @pytest.fixture(scope=\'module\')\n    def module_cls(self):\n        from skorch.toy import make_binary_classifier\n        return make_binary_classifier(\n            input_units=20,\n            hidden_units=10,\n            output_units=1,\n            num_hidden=1,\n            dropout=0,\n        )\n\n    @pytest.fixture(scope=\'module\')\n    def net_cls(self):\n        from skorch.classifier import NeuralNetBinaryClassifier\n        return NeuralNetBinaryClassifier\n\n    @pytest.fixture(scope=\'module\')\n    def net(self, net_cls, module_cls):\n        return net_cls(\n            module_cls,\n            max_epochs=1,\n            lr=1,\n        )\n\n    @pytest.fixture(scope=\'module\')\n    def net_fit(self, net, data):\n        # Careful, don\'t call additional fits on this, since that would have\n        # side effects on other tests.\n        net.set_params(max_epochs=10)\n        X, y = data\n        net.fit(X, y)\n        net.set_params(max_epochs=1)\n        return net\n\n    def test_fit(self, net_fit):\n        # fitting does not raise anything\n        pass\n\n    def test_clone(self, net_fit):\n        clone(net_fit)\n\n    @pytest.mark.parametrize(\'method\', INFERENCE_METHODS)\n    def test_not_fitted_raises(self, net_cls, module_cls, data, method):\n        from skorch.exceptions import NotInitializedError\n        net = net_cls(module_cls)\n        X = data[0]\n        with pytest.raises(NotInitializedError) as exc:\n            # we call `list` because `forward_iter` is lazy\n            list(getattr(net, method)(X))\n\n        msg = (""This NeuralNetBinaryClassifier instance is not initialized ""\n               ""yet. Call \'initialize\' or \'fit\' with appropriate arguments ""\n               ""before using this method."")\n        assert exc.value.args[0] == msg\n\n    @flaky(max_runs=3)\n    def test_net_learns(self, net_cls, module_cls, data):\n        X, y = data\n        net = net_cls(\n            module_cls,\n            max_epochs=10,\n            lr=1,\n            batch_size=64,\n        )\n        net.fit(X, y)\n\n        train_losses = net.history[:, \'train_loss\']\n        assert train_losses[0] > 1.3 * train_losses[-1]\n\n        valid_acc = net.history[-1, \'valid_acc\']\n        assert valid_acc > 0.65\n\n    def test_batch_size_one(self, net_cls, module_cls, data):\n        X, y = data\n        net = net_cls(\n            module_cls,\n            max_epochs=1,\n            batch_size=1,\n        )\n        net.fit(X, y)\n\n    def test_history_default_keys(self, net_fit):\n        expected_keys = {\n            \'train_loss\', \'valid_loss\', \'epoch\', \'dur\', \'batches\', \'valid_acc\'\n        }\n        for row in net_fit.history:\n            assert expected_keys.issubset(row)\n\n    @pytest.mark.parametrize(\'threshold\', [0, 0.25, 0.5, 0.75, 1])\n    def test_predict_predict_proba(self, net, data, threshold):\n        X, y = data\n        net.threshold = threshold\n        net.fit(X, y)\n\n        y_pred_proba = net.predict_proba(X)\n        assert y_pred_proba.shape == (X.shape[0], 2)\n\n        y_pred_exp = (y_pred_proba[:, 1] > threshold).astype(\'uint8\')\n\n        y_pred_actual = net.predict(X)\n        assert np.allclose(y_pred_exp, y_pred_actual)\n\n    def test_score(self, net_fit, data):\n        X, y = data\n        accuracy = net_fit.score(X, y)\n        assert 0. <= accuracy <= 1.\n\n    def test_target_2d_raises(self, net, data):\n        X, y = data\n        with pytest.raises(ValueError) as exc:\n            net.fit(X, y[:, None])\n\n        assert exc.value.args[0] == (\n            ""The target data should be 1-dimensional."")\n\n    def test_custom_loss_does_not_call_sigmoid(\n            self, net_cls, data, module_cls, monkeypatch):\n        mock = Mock(side_effect=lambda x: x)\n        monkeypatch.setattr(torch, ""sigmoid"", mock)\n\n        net = net_cls(module_cls, max_epochs=1, lr=0.1, criterion=nn.MSELoss)\n        X, y = data\n        net.fit(X, y)\n\n        net.predict_proba(X)\n        assert mock.call_count == 0\n\n    def test_default_loss_does_call_sigmoid(\n            self, net_cls, data, module_cls, monkeypatch):\n        mock = Mock(side_effect=lambda x: x)\n        monkeypatch.setattr(torch, ""sigmoid"", mock)\n\n        net = net_cls(module_cls, max_epochs=1, lr=0.1)\n        X, y = data\n        net.fit(X, y)\n\n        net.predict_proba(X)\n        assert mock.call_count > 0\n\n    def test_with_calibrated_classifier_cv(self, net_fit, data):\n        from sklearn.calibration import CalibratedClassifierCV\n        cccv = CalibratedClassifierCV(net_fit, cv=2)\n        cccv.fit(*data)\n\n    def test_grid_search_with_roc_auc(self, net_fit, data):\n        from sklearn.model_selection import GridSearchCV\n        search = GridSearchCV(\n            net_fit,\n            {\'max_epochs\': [1, 2]},\n            refit=False,\n            cv=3,\n            scoring=\'roc_auc\',\n        )\n        search.fit(*data)\n\n    def test_module_output_not_1d(self, net_cls, data):\n        from skorch.toy import make_classifier\n        module = make_classifier(\n            input_units=20,\n            output_units=1,\n        )  # the output will not be squeezed\n        net = net_cls(module, max_epochs=1)\n        net.fit(*data)  # does not raise\n\n    def test_module_output_2d_raises(self, net_cls, data):\n        from skorch.toy import make_classifier\n        module = make_classifier(\n            input_units=20,\n            output_units=2,\n        )\n        net = net_cls(module, max_epochs=1)\n        with pytest.raises(ValueError) as exc:\n            net.fit(*data)\n\n        msg = exc.value.args[0]\n        expected = (""Expected module output to have shape (n,) or ""\n                    ""(n, 1), got (128, 2) instead"")\n        assert msg == expected\n'"
skorch/tests/test_cli.py,5,"b'""""""Test for cli.py""""""\n\nfrom math import cos\nfrom unittest.mock import Mock\nfrom unittest.mock import patch\n\nimport numpy as np\nimport pytest\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom torch import nn\nfrom torch.nn import RReLU\n\n\nfire_installed = True\ntry:\n    import fire  # pylint: disable=unused-import\nexcept ImportError:\n    fire_installed = False\n\n\n@pytest.mark.skipif(not fire_installed, reason=\'fire libarary not installed\')\nclass TestCli:\n    @pytest.fixture\n    def resolve_dotted_name(self):\n        from skorch.cli import _resolve_dotted_name\n        return _resolve_dotted_name\n\n    @pytest.mark.parametrize(\'name, expected\', [\n        (0, 0),\n        (1.23, 1.23),\n        (\'foo\', \'foo\'),\n        (\'math.cos\', cos),\n        (\'torch.nn\', nn),\n        (\'torch.nn.ReLU\', nn.ReLU),\n    ])\n    def test_resolve_dotted_name(self, resolve_dotted_name, name, expected):\n        result = resolve_dotted_name(name)\n        assert result == expected\n\n    def test_resolve_dotted_name_instantiated(self, resolve_dotted_name):\n        result = resolve_dotted_name(\'torch.nn.RReLU(0.123, upper=0.456)\')\n        assert isinstance(result, RReLU)\n        assert np.isclose(result.lower, 0.123)\n        assert np.isclose(result.upper, 0.456)\n\n    @pytest.fixture\n    def parse_net_kwargs(self):\n        from skorch.cli import parse_net_kwargs\n        return parse_net_kwargs\n\n    def test_parse_net_kwargs(self, parse_net_kwargs):\n        kwargs = {\n            \'lr\': 0.05,\n            \'max_epochs\': 5,\n            \'module__num_units\': 10,\n            \'module__nonlin\': \'torch.nn.RReLU(0.123, upper=0.456)\',\n        }\n        parsed_kwargs = parse_net_kwargs(kwargs)\n\n        assert len(parsed_kwargs) == 4\n        assert np.isclose(parsed_kwargs[\'lr\'], 0.05)\n        assert parsed_kwargs[\'max_epochs\'] == 5\n        assert parsed_kwargs[\'module__num_units\'] == 10\n        assert isinstance(parsed_kwargs[\'module__nonlin\'], RReLU)\n        assert np.isclose(parsed_kwargs[\'module__nonlin\'].lower, 0.123)\n        assert np.isclose(parsed_kwargs[\'module__nonlin\'].upper, 0.456)\n\n    @pytest.fixture\n    def net_cls(self):\n        from skorch import NeuralNetClassifier\n        return NeuralNetClassifier\n\n    @pytest.fixture\n    def net(self, net_cls, classifier_module):\n        return net_cls(classifier_module)\n\n    @pytest.fixture\n    def pipe(self, net):\n        return Pipeline([\n            (\'features\', FeatureUnion([\n                (\'scale\', MinMaxScaler()),\n            ])),\n            (\'net\', net),\n        ])\n\n    @pytest.fixture\n    def yield_estimators(self):\n        from skorch.cli import _yield_estimators\n        return _yield_estimators\n\n    def test_yield_estimators_net(self, yield_estimators, net):\n        result = list(yield_estimators(net))\n\n        assert result[0][0] == \'\'\n        assert result[0][1] is net\n        assert result[1][0] == \'module\'\n        assert result[1][1] is net.module\n\n    def test_yield_estimators_pipe(self, yield_estimators, pipe):\n        result = list(yield_estimators(pipe))\n        scaler = pipe.named_steps[\'features\'].transformer_list[0][1]\n        net = pipe.named_steps[\'net\']\n        module = net.module\n\n        assert result[0][0] == \'features__scale\'\n        assert result[0][1] is scaler\n        assert result[1][0] == \'net\'\n        assert result[1][1] is net\n        assert result[2][0] == \'net__module\'\n        assert result[2][1] is module\n\n    @pytest.fixture\n    def substitute_default(self):\n        from skorch.cli import _substitute_default\n        return _substitute_default\n\n    @pytest.mark.parametrize(\'s, new_value, expected\', [\n        (\'\', \'\', \'\'),\n        (\'\', \'foo\', \'\'),\n        (\'bar\', \'foo\', \'bar\'),\n        (\'int (default=128)\', \'\', \'int (default=)\'),\n        (\'int (default=128)\', None, \'int (default=128)\'),\n        (\'int (default=128)\', \'""""\', \'int (default="""")\'),\n        (\'int (default=128)\', \'128\', \'int (default=128)\'),\n        (\'int (default=128)\', \'256\', \'int (default=256)\'),\n        (\'int (default=128)\', 256, \'int (default=256)\'),\n        (\'with_parens (default=(1, 2))\', (3, 4), \'with_parens (default=(3, 4))\'),\n        (\'int (default =128)\', \'256\', \'int (default =256)\'),\n        (\'int (default= 128)\', \'256\', \'int (default= 256)\'),\n        (\'int (default = 128)\', \'256\', \'int (default = 256)\'),\n        (\n            \'nonlin (default = ReLU())\',\n            nn.Hardtanh(1, 2),\n            \'nonlin (default = {})\'.format(nn.Hardtanh(1, 2))\n        ),\n        (\n            # from sklearn MinMaxScaler\n            \'tuple (min, max), default=(0, 1)\',\n            (-1, 1),\n            \'tuple (min, max), default=(-1, 1)\'\n        ),\n        (\n            # from sklearn MinMaxScaler\n            \'boolean, optional, default True\',\n            False,\n            \'boolean, optional, default False\'\n        ),\n        (\n            # from sklearn Normalizer\n            ""\'l1\', \'l2\', or \'max\', optional (\'l2\' by default)"",\n            \'l1\',\n            ""\'l1\', \'l2\', or \'max\', optional (\'l1\' by default)""\n        ),\n        (\n            # same but double ticks\n            \'""l1"", ""l2"", or ""max"", optional (""l2"" by default)\',\n            \'l1\',\n            \'""l1"", ""l2"", or ""max"", optional (""l1"" by default)\'\n        ),\n        (\n            # same but no ticks\n            ""l1, l2, or max, optional (l2 by default)"",\n            \'l1\',\n            ""l1, l2, or max, optional (l1 by default)""\n        ),\n        (\n            ""tuple, optional ((1, 1) by default)"",\n            (2, 2),\n            ""tuple, optional ((2, 2) by default)""\n        ),\n        (\n            ""nonlin (ReLU() by default)"",\n            nn.Tanh(),\n            ""nonlin (Tanh() by default)""\n        ),\n    ])\n    def test_replace_default(self, substitute_default, s, new_value, expected):\n        result = substitute_default(s, new_value)\n        assert result == expected\n\n    @pytest.fixture\n    def print_help(self):\n        from skorch.cli import print_help\n        return print_help\n\n    def test_print_help_net(self, print_help, net, capsys):\n        print_help(net)\n        out = capsys.readouterr()[0]\n\n        expected_snippets = [\n            \'-- --help\',\n            \'<NeuralNetClassifier> options\',\n            \'--module : torch module (class or instance)\',\n            \'--batch_size : int (default=128)\',\n            \'<MLPModule> options\',\n            \'--module__hidden_units : int (default=10)\'\n        ]\n        for snippet in expected_snippets:\n            assert snippet in out\n\n    def test_print_help_net_custom_defaults(self, print_help, net, capsys):\n        defaults = {\'batch_size\': 256, \'module__hidden_units\': 55}\n        print_help(net, defaults)\n        out = capsys.readouterr()[0]\n\n        expected_snippets = [\n            \'-- --help\',\n            \'<NeuralNetClassifier> options\',\n            \'--module : torch module (class or instance)\',\n            \'--batch_size : int (default=256)\',\n            \'<MLPModule> options\',\n            \'--module__hidden_units : int (default=55)\'\n        ]\n        for snippet in expected_snippets:\n            assert snippet in out\n\n    def test_print_help_pipeline(self, print_help, pipe, capsys):\n        print_help(pipe)\n        out = capsys.readouterr()[0]\n\n        expected_snippets = [\n            \'-- --help\',\n            \'<MinMaxScaler> options\',\n            \'--features__scale__feature_range\',\n            \'<NeuralNetClassifier> options\',\n            \'--net__module : torch module (class or instance)\',\n            \'--net__batch_size : int (default=128)\',\n            \'<MLPModule> options\',\n            \'--net__module__hidden_units : int (default=10)\'\n        ]\n        for snippet in expected_snippets:\n            assert snippet in out\n\n    def test_print_help_pipeline_custom_defaults(\n            self, print_help, pipe, capsys):\n        defaults = {\'net__batch_size\': 256, \'net__module__hidden_units\': 55}\n        print_help(pipe, defaults=defaults)\n        out = capsys.readouterr()[0]\n\n        expected_snippets = [\n            \'-- --help\',\n            \'<MinMaxScaler> options\',\n            \'--features__scale__feature_range\',\n            \'<NeuralNetClassifier> options\',\n            \'--net__module : torch module (class or instance)\',\n            \'--net__batch_size : int (default=256)\',\n            \'<MLPModule> options\',\n            \'--net__module__hidden_units : int (default=55)\'\n        ]\n        for snippet in expected_snippets:\n            assert snippet in out\n\n    @pytest.fixture\n    def parse_args(self):\n        from skorch.cli import parse_args\n        return parse_args\n\n    @pytest.fixture\n    def estimator(self, net_cls):\n        mock = Mock(net_cls)\n        return mock\n\n    def test_parse_args_help(self, parse_args, estimator):\n        with patch(\'skorch.cli.sys.exit\') as exit_:\n            with patch(\'skorch.cli.print_help\') as help_:\n                parsed = parse_args({\'help\': True, \'foo\': \'bar\'})\n                parsed(estimator)\n\n        assert estimator.set_params.call_count == 0  # kwargs and defaults\n        assert help_.call_count == 1\n        assert exit_.call_count == 1\n\n    def test_parse_args_run(self, parse_args, estimator):\n        kwargs = {\'foo\': \'bar\', \'baz\': \'math.cos\'}\n        with patch(\'skorch.cli.sys.exit\') as exit_:\n            with patch(\'skorch.cli.print_help\') as help_:\n                parsed = parse_args(kwargs)\n                parsed(estimator)\n\n        assert estimator.set_params.call_count == 2  # defaults and kwargs\n\n        defaults_set_params = estimator.set_params.call_args_list[0][1]\n        assert not defaults_set_params  # no defaults specified\n\n        kwargs_set_params = estimator.set_params.call_args_list[1][1]\n        assert kwargs_set_params[\'foo\'] == \'bar\'\n        assert kwargs_set_params[\'baz\'] == cos\n\n        assert help_.call_count == 0\n        assert exit_.call_count == 0\n\n    def test_parse_args_net_custom_defaults(self, parse_args, net):\n        defaults = {\'batch_size\': 256, \'module__hidden_units\': 55}\n        kwargs = {\'batch_size\': 123, \'module__nonlin\': nn.Hardtanh(1, 2)}\n        parsed = parse_args(kwargs, defaults)\n        net = parsed(net)\n\n        # cmd line args have precedence over defaults\n        assert net.batch_size == 123\n        assert net.module_.hidden_units == 55\n        assert isinstance(net.module_.nonlin, nn.Hardtanh)\n        assert net.module_.nonlin.min_val == 1\n        assert net.module_.nonlin.max_val == 2\n\n    def test_parse_args_pipe_custom_defaults(self, parse_args, pipe):\n        defaults = {\'net__batch_size\': 256, \'net__module__hidden_units\': 55}\n        kwargs = {\'net__batch_size\': 123, \'net__module__nonlin\': nn.Hardtanh(1, 2)}\n        parsed = parse_args(kwargs, defaults)\n        pipe = parsed(pipe)\n        net = pipe.steps[-1][1]\n\n        # cmd line args have precedence over defaults\n        assert net.batch_size == 123\n        assert net.module_.hidden_units == 55\n        assert isinstance(net.module_.nonlin, nn.Hardtanh)\n        assert net.module_.nonlin.min_val == 1\n        assert net.module_.nonlin.max_val == 2\n'"
skorch/tests/test_dataset.py,26,"b'""""""Tests for dataset.py.""""""\n\nfrom unittest.mock import Mock\n\nimport numpy as np\nimport pytest\nfrom sklearn.datasets import make_classification\nimport torch\nimport torch.utils.data\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom scipy import sparse\nfrom skorch.utils import data_from_dataset\nfrom skorch.utils import is_torch_data_type\nfrom skorch.utils import to_tensor\nfrom skorch.tests.conftest import pandas_installed\n\n\nclass TestGetLen:\n    @pytest.fixture\n    def get_len(self):\n        from skorch.dataset import get_len\n        return get_len\n\n    @pytest.mark.parametrize(\'data, expected\', [\n        (np.zeros(5), 5),\n        (np.zeros((3, 4, 5)), 3),\n        ([np.zeros(5), np.zeros((5, 4))], 5),\n        ((np.zeros((5, 4)), np.zeros(5)), 5),\n        ({\'0\': np.zeros(3), \'1\': np.zeros((3, 4))}, 3),\n        (torch.zeros(5), 5),\n        (torch.zeros((3, 4, 5)), 3),\n        ([torch.zeros(5), torch.zeros((5, 4))], 5),\n        ((torch.zeros((5, 4)), torch.zeros(5)), 5),\n        (sparse.csr_matrix(np.zeros((5, 3))), 5),\n        ({\'0\': torch.zeros(3), \'1\': torch.zeros((3, 4))}, 3),\n        ([0, 1, 2], 3),\n        ([[0, 1, 2], [3, 4, 5]], 3),\n        ({\'0\': [0, 1, 2], \'1\': (3, 4, 5)}, 3),\n        ((\n            [0, 1, 2],\n            np.zeros(3),\n            torch.zeros(3),\n            sparse.csr_matrix(np.zeros((3, 5))),\n            {\'0\': (1, 2, 3)}),\n         3),\n    ])\n    def test_valid_lengths(self, get_len, data, expected):\n        length = get_len(data)\n        assert length == expected\n\n    @pytest.mark.parametrize(\'data\', [\n        [np.zeros(5), np.zeros((4, 5))],\n        {\'0\': np.zeros(3), \'1\': np.zeros((4, 3))},\n        [torch.zeros(5), torch.zeros((4, 5))],\n        {\'0\': torch.zeros(3), \'1\': torch.zeros((4, 3))},\n        [[0, 1, 2], [3, 4]],\n        ([0, 1, 2], [3, 4]),\n        {\'0\': [0, 1, 2], \'1\': (3, 4)},\n        ([0, 1, 2], np.zeros(3), torch.zeros(2), {\'0\': (1, 2, 3)}),\n    ])\n    def test_inconsistent_lengths(self, get_len, data):\n        with pytest.raises(ValueError):\n            get_len(data)\n\n\nclass TestUsesPlaceholderY:\n\n    @pytest.fixture\n    def uses_placeholder_y(self):\n        from skorch.dataset import uses_placeholder_y\n        return uses_placeholder_y\n\n    @pytest.fixture\n    def dataset_cls(self):\n        from skorch.dataset import Dataset\n        return Dataset\n\n    @pytest.fixture\n    def custom_dataset_cls(self):\n        from skorch.dataset import Dataset\n        class CustomDataset(Dataset):\n            # pylint: disable=super-init-not-called\n            def __init__(self):\n                pass\n        return CustomDataset\n\n    @pytest.fixture\n    def cv_split_cls(self):\n        from skorch.dataset import CVSplit\n        return CVSplit\n\n    def test_dataset_uses_y_placeholder(\n            self, dataset_cls, data, uses_placeholder_y):\n        X, _ = data\n        ds = dataset_cls(X, y=None)\n        assert uses_placeholder_y(ds)\n\n    def test_dataset_uses_non_y_placeholder(\n            self, dataset_cls, data, uses_placeholder_y):\n        X, y = data\n        ds = dataset_cls(X, y)\n        assert not uses_placeholder_y(ds)\n\n    def test_custom_dataset_uses_non_y_placeholder(\n            self, custom_dataset_cls, uses_placeholder_y):\n        ds = custom_dataset_cls()\n        assert not uses_placeholder_y(ds)\n\n    def test_subset_uses_placeholder_y(\n            self, dataset_cls, data, uses_placeholder_y,\n            cv_split_cls):\n        X, _ = data\n        ds = dataset_cls(X, y=None)\n        ds_train, ds_valid = cv_split_cls(cv=2)(ds)\n        assert uses_placeholder_y(ds_train)\n        assert uses_placeholder_y(ds_valid)\n\n    def test_subset_dataset_uses_non_y_placeholder(\n            self, dataset_cls, data, uses_placeholder_y,\n            cv_split_cls):\n        X, y = data\n        ds = dataset_cls(X, y)\n        ds_train, ds_valid = cv_split_cls(cv=2)(ds)\n        assert not uses_placeholder_y(ds_train)\n        assert not uses_placeholder_y(ds_valid)\n\n    def test_subset_of_subset_uses_placeholder_y(\n            self, dataset_cls, data, uses_placeholder_y,\n            cv_split_cls):\n        X, _ = data\n        ds = dataset_cls(X, y=None)\n        ds_split, _ = cv_split_cls(cv=4)(ds)\n        ds_train, ds_valid = cv_split_cls(cv=3)(ds_split)\n        assert uses_placeholder_y(ds_train)\n        assert uses_placeholder_y(ds_valid)\n\n    def test_subset_of_subset_uses_non_placeholder_y(\n            self, dataset_cls, data, uses_placeholder_y,\n            cv_split_cls):\n        X, y = data\n        ds = dataset_cls(X, y)\n        ds_split, _ = cv_split_cls(cv=4)(ds)\n        ds_train, ds_valid = cv_split_cls(cv=3)(ds_split)\n        assert not uses_placeholder_y(ds_train)\n        assert not uses_placeholder_y(ds_valid)\n\n\nclass TestNetWithoutY:\n\n    net_fixture_params = [\n        {\'classification\': True, \'batch_size\': 1},\n        {\'classification\': False, \'batch_size\': 1},\n        {\'classification\': True, \'batch_size\': 2},\n        {\'classification\': False, \'batch_size\': 2},\n    ]\n\n    @pytest.fixture\n    def net_cls_1d(self):\n        from skorch.toy import make_regressor\n        return make_regressor(\n            input_units=1,\n            num_hidden=0,\n            output_units=1,\n        )\n\n    @pytest.fixture\n    def net_cls_2d(self):\n        from skorch.toy import make_regressor\n        return make_regressor(\n            input_units=2,\n            num_hidden=0,\n            output_units=1,\n        )\n\n    @pytest.fixture\n    def loader_clf(self):\n        class Loader(torch.utils.data.DataLoader):\n            def __iter__(self):\n                z = super().__iter__()\n                return ((x, torch.zeros(x.size(0)).long()) for x, _ in z)\n        return Loader\n\n    @pytest.fixture\n    def loader_reg(self):\n        class Loader(torch.utils.data.DataLoader):\n            def __iter__(self):\n                z = super().__iter__()\n                return ((x, torch.zeros(x.size(0), 1).float()) for x, _ in z)\n        return Loader\n\n    @pytest.fixture\n    def train_split(self):\n        from skorch.dataset import CVSplit\n        return CVSplit(0.2, stratified=False)\n\n    @pytest.fixture(params=net_fixture_params)\n    def net_1d(self, request, net_cls_1d, train_split):\n        if request.param[\'classification\']:\n            from skorch import NeuralNetClassifier\n            wrap_cls = NeuralNetClassifier\n        else:\n            from skorch import NeuralNetRegressor\n            wrap_cls = NeuralNetRegressor\n\n        return wrap_cls(\n            net_cls_1d,\n            max_epochs=2,\n            train_split=train_split,\n            batch_size=request.param[\'batch_size\']\n        )\n\n    @pytest.fixture(params=net_fixture_params)\n    def net_2d(self, request, net_cls_2d, train_split):\n        if request.param[\'classification\']:\n            from skorch import NeuralNetClassifier\n            wrap_cls = NeuralNetClassifier\n        else:\n            from skorch import NeuralNetRegressor\n            wrap_cls = NeuralNetRegressor\n\n        return wrap_cls(\n            net_cls_2d,\n            max_epochs=2,\n            train_split=train_split,\n            batch_size=request.param[\'batch_size\']\n        )\n\n    @pytest.fixture(params=net_fixture_params)\n    def net_1d_custom_loader(self, request, net_cls_1d,\n                             loader_clf, loader_reg, train_split):\n        """"""Parametrized fixture returning a NeuralNet\n        classifier/regressor, for different batch sizes, working on 1d\n        data.\n\n        """"""\n        if request.param[\'classification\']:\n            from skorch import NeuralNetClassifier\n            wrap_cls = NeuralNetClassifier\n            loader = loader_clf\n        else:\n            from skorch import NeuralNetRegressor\n            wrap_cls = NeuralNetRegressor\n            loader = loader_reg\n\n        return wrap_cls(\n            net_cls_1d,\n            iterator_train=loader,\n            iterator_valid=loader,\n            max_epochs=2,\n            train_split=train_split,\n            batch_size=request.param[\'batch_size\']\n        )\n\n    @pytest.fixture(params=net_fixture_params)\n    def net_2d_custom_loader(self, request, net_cls_2d,\n                             loader_clf, loader_reg, train_split):\n        """"""Parametrized fixture returning a NeuralNet\n        classifier/regressor, for different batch sizes, working on 2d\n        data.\n\n        """"""\n        if request.param[\'classification\']:\n            from skorch import NeuralNetClassifier\n            wrap_cls = NeuralNetClassifier\n            loader = loader_clf\n        else:\n            from skorch import NeuralNetRegressor\n            wrap_cls = NeuralNetRegressor\n            loader = loader_reg\n\n        return wrap_cls(\n            net_cls_2d,\n            iterator_train=loader,\n            iterator_valid=loader,\n            max_epochs=2,\n            train_split=train_split,\n            batch_size=request.param[\'batch_size\']\n        )\n\n    def test_net_1d_tensor_raises_error(self, net_1d):\n        X = torch.arange(0, 8).view(-1, 1).long()\n        # We expect check_data to throw an exception\n        # because we did not specify a custom data loader.\n        with pytest.raises(ValueError):\n            net_1d.fit(X, None)\n\n    def test_net_2d_tensor_raises_error(self, net_2d):\n        X = torch.arange(0, 8).view(4, 2).long()\n        # We expect check_data to throw an exception\n        # because we did not specify a custom data loader.\n        with pytest.raises(ValueError):\n            net_2d.fit(X, None)\n\n    def test_net_1d_custom_loader(self, net_1d_custom_loader):\n        X = torch.arange(0, 8).view(-1, 1).float()\n        # throw away all callbacks since those may raise unrelated errors\n        net_1d_custom_loader.initialize()\n        net_1d_custom_loader.callbacks_ = []\n        # Should not raise an exception.\n        net_1d_custom_loader.partial_fit(X, None)\n\n    def test_net_2d_custom_loader(self, net_2d_custom_loader):\n        X = torch.arange(0, 8).view(4, 2).float()\n        # throw away all callbacks since those may raise unrelated errors\n        net_2d_custom_loader.initialize()\n        net_2d_custom_loader.callbacks_ = []\n        # Should not raise an exception.\n        net_2d_custom_loader.partial_fit(X, None)\n\n\nclass TestNetWithDict:\n    @pytest.fixture(scope=\'module\')\n    def module_cls(self):\n        """"""Return a simple module that concatenates its 2 inputs in\n        forward step.\n\n        """"""\n        class MyModule(nn.Module):\n            def __init__(self):\n                super(MyModule, self).__init__()\n                self.dense = nn.Linear(20, 2)\n\n            # pylint: disable=arguments-differ\n            def forward(self, X0, X1):\n                X = torch.cat((X0, X1), 1)\n                X = F.softmax(self.dense(X), dim=-1)\n                return X\n\n        return MyModule\n\n    @pytest.fixture(scope=\'module\')\n    def data(self):\n        X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n        X = X.astype(np.float32)\n        return X[:, :10], X[:, 10:], y\n\n    @pytest.fixture(scope=\'module\')\n    def net_cls(self):\n        from skorch import NeuralNetClassifier\n        return NeuralNetClassifier\n\n    @pytest.fixture(scope=\'module\')\n    def net(self, net_cls, module_cls):\n        return net_cls(\n            module_cls,\n            max_epochs=2,\n            lr=0.1,\n        )\n\n    def test_fit_predict_proba(self, net, data):\n        X = {\'X0\': data[0], \'X1\': data[1]}\n        y = data[2]\n        net.fit(X, y)\n        y_proba = net.predict_proba(X)\n        assert np.allclose(y_proba.sum(1), 1)\n\n        # Issue #142: check that all batch sizes are consistent with\n        # `net.batch_size`, even when the input type is a dictionary.\n        # Note that we allow for different batch sizes as the total\n        # number of samples may not be divisible by the batch size.\n        batch_sizes = lambda n: set(sum(net.history[:, \'batches\', :, n], []))\n        train_batch_sizes = batch_sizes(\'train_batch_size\')\n        valid_batch_sizes = batch_sizes(\'valid_batch_size\')\n        assert net.batch_size in train_batch_sizes\n        assert net.batch_size in valid_batch_sizes\n\n\nclass TestNetWithList:\n    @pytest.fixture(scope=\'module\')\n    def module_cls(self):\n        """"""Return a simple module that concatenates the input.""""""\n        class MyModule(nn.Module):\n            def __init__(self):\n                super(MyModule, self).__init__()\n                self.dense = nn.Linear(20, 2)\n\n            # pylint: disable=arguments-differ\n            def forward(self, X):\n                X = torch.cat(X, 1)\n                X = F.softmax(self.dense(X), dim=-1)\n                return X\n\n        return MyModule\n\n    @pytest.fixture(scope=\'module\')\n    def data(self):\n        X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n        X = X.astype(np.float32)\n        return [X[:, :10], X[:, 10:]], y\n\n    @pytest.fixture(scope=\'module\')\n    def net_cls(self):\n        from skorch import NeuralNetClassifier\n        return NeuralNetClassifier\n\n    @pytest.fixture(scope=\'module\')\n    def net(self, net_cls, module_cls):\n        return net_cls(\n            module_cls,\n            max_epochs=2,\n            lr=0.1,\n        )\n\n    def test_fit_predict_proba(self, net, data):\n        X, y = data\n        net.fit(X, y)\n        y_proba = net.predict_proba(X)\n        assert np.allclose(y_proba.sum(1), 1)\n\n\n@pytest.mark.skipif(not pandas_installed, reason=\'pandas is not installed\')\nclass TestNetWithPandas:\n    @pytest.fixture(scope=\'module\')\n    def module_cls(self):\n        """"""Return a simple module that concatenates all input values\n        in forward step.\n\n        """"""\n        class MyModule(nn.Module):\n            def __init__(self):\n                super(MyModule, self).__init__()\n                self.dense = nn.Linear(20, 2)\n\n            # pylint: disable=arguments-differ\n            def forward(self, **X):\n                X = torch.cat(list(X.values()), 1)\n                X = F.softmax(self.dense(X), dim=-1)\n                return X\n\n        return MyModule\n\n    @pytest.fixture(scope=\'module\')\n    def pd(self):\n        import pandas as pd\n        return pd\n\n    @pytest.fixture(scope=\'module\')\n    def data(self, pd):\n        X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n        X = X.astype(np.float32)\n        df = pd.DataFrame(X, columns=map(str, range(X.shape[1])))\n        return df, y\n\n    @pytest.fixture(scope=\'module\')\n    def net_cls(self):\n        from skorch import NeuralNetClassifier\n        return NeuralNetClassifier\n\n    @pytest.fixture(scope=\'module\')\n    def net(self, net_cls, module_cls):\n        return net_cls(\n            module_cls,\n            max_epochs=2,\n            lr=0.1,\n        )\n\n    def test_fit_predict_proba(self, net, data):\n        X, y = data\n        net.fit(X, y)\n        y_proba = net.predict_proba(X)\n        assert np.allclose(y_proba.sum(1), 1)\n\n\nclass TestDataset:\n    """"""Note: we don\'t need to test multi_indexing here, since that is\n    already covered.\n\n    """"""\n    @pytest.fixture\n    def dataset_cls(self):\n        from skorch.dataset import Dataset\n        return Dataset\n\n    def test_len_correct(self, dataset_cls):\n        pass\n\n    def test_user_defined_len(self, dataset_cls):\n        pass\n\n    def test_inconsistent_lengths_raises(self, dataset_cls):\n        pass\n\n    def test_with_numpy_array(self, dataset_cls):\n        pass\n\n    def test_with_torch_tensor(self, dataset_cls):\n        pass\n\n    @pytest.mark.skipif(not pandas_installed, reason=\'pandas is not installed\')\n    def test_with_pandas_df(self, dataset_cls):\n        pass\n\n    @pytest.mark.skipif(not pandas_installed, reason=\'pandas is not installed\')\n    def test_with_pandas_series(self, dataset_cls):\n        pass\n\n    def test_with_dict(self, dataset_cls):\n        pass\n\n    def test_with_list_of_numpy_arrays(self, dataset_cls):\n        pass\n\n    @pytest.fixture\n    def dataset_sparse_csr(self, dataset_cls):\n        Xs = sparse.csr_matrix(np.zeros((10, 5)))\n        return dataset_cls(Xs)\n\n    @pytest.mark.parametrize(\'batch_size\', [1, 3, 10, 17])\n    def test_dataloader_with_sparse_csr(self, dataset_sparse_csr, batch_size):\n        loader = DataLoader(dataset_sparse_csr, batch_size=batch_size)\n        for Xb, _ in loader:\n            assert is_torch_data_type(Xb)\n\n\nclass TestTrainSplitIsUsed:\n    @pytest.fixture\n    def iterator(self):\n        """"""Return a simple iterator that yields the input data.""""""\n        class Iterator:\n            """"""An iterator that just yield the input data.""""""\n            # pylint: disable=unused-argument\n            def __init__(self, dataset, *args, **kwargs):\n                self.dataset = dataset\n\n            def __iter__(self):\n                yield self.dataset.X, self.dataset.y\n\n        return Iterator\n\n    @pytest.fixture\n    def data(self):\n        X = torch.arange(0, 12, dtype=torch.float32).view(4, 3)\n        y = torch.LongTensor([0, 1, 1, 0])\n        return X, y\n\n    @pytest.fixture\n    def data_split(self, data):\n        from skorch.dataset import Dataset\n\n        X, y = data\n        dataset_train = Dataset(X[:2], y[:2])\n        dataset_valid = Dataset(X[2:], y[2:])\n        return dataset_train, dataset_valid\n\n    @pytest.fixture\n    def module(self, classifier_module):\n        return classifier_module\n\n    @pytest.fixture\n    def train_split(self, data_split):\n        return Mock(side_effect=[data_split])\n\n    @pytest.fixture\n    def net_and_mock(self, module, data, train_split, iterator):\n        """"""Return a NeuralNetClassifier with mocked train and\n        validation step which save the args and kwargs the methods are\n        calld with.\n\n        """"""\n        from skorch import NeuralNetClassifier\n\n        X, y = data\n        net = NeuralNetClassifier(\n            module,\n            module__input_units=3,\n            max_epochs=1,\n            iterator_train=iterator,\n            iterator_valid=iterator,\n            train_split=train_split\n        )\n        net.initialize()\n        net.callbacks_ = []\n\n        mock = Mock()\n\n        def decorator(func):\n            def wrapper(*args, **kwargs):\n                mock(*args, **kwargs)\n                func.__dict__[\'mock_\'] = mock\n                return func(*args[1:], **kwargs)\n            return wrapper\n\n        import types\n        net.get_iterator = types.MethodType(decorator(net.get_iterator), net)\n        return net.partial_fit(X, y), mock\n\n    def test_steps_called_with_split_data(self, net_and_mock, data_split):\n        mock = net_and_mock[1]\n        assert mock.call_count == 2  # once for train, once for valid\n        assert mock.call_args_list[0][0][1] == data_split[0]\n        assert mock.call_args_list[0][1][\'training\'] is True\n        assert mock.call_args_list[1][0][1] == data_split[1]\n        assert mock.call_args_list[1][1][\'training\'] is False\n\n\nclass TestCVSplit:\n    num_samples = 100\n\n    @staticmethod\n    def assert_datasets_equal(ds0, ds1):\n        """"""Generic function to test equality of dataset values.""""""\n        assert len(ds0) == len(ds1)\n        # pylint: disable=consider-using-enumerate\n        for i in range(len(ds0)):\n            x0, y0 = ds0[i]\n            x1, y1 = ds1[i]\n            try:\n                assert x0 == x1\n            except (RuntimeError, ValueError):\n                assert (x0 == x1).all()\n            try:\n                assert y0 == y1\n            except (RuntimeError, ValueError):\n                assert (y0 == y1).all()\n\n    @pytest.fixture\n    def dataset_cls(self):\n        from skorch.dataset import Dataset\n        return Dataset\n\n    @pytest.fixture\n    def data(self, dataset_cls):\n        X = np.random.random((self.num_samples, 10))\n        assert self.num_samples % 4 == 0\n        y = np.repeat([0, 1, 2, 3], self.num_samples // 4)\n        return dataset_cls(X, y)\n\n    @pytest.fixture\n    def cv_split_cls(self):\n        from skorch.dataset import CVSplit\n        return CVSplit\n\n    def test_reproducible(self, cv_split_cls, data):\n        dataset_train0, dataset_valid0 = cv_split_cls(5)(data)\n        dataset_train1, dataset_valid1 = cv_split_cls(5)(data)\n        self.assert_datasets_equal(dataset_train0, dataset_train1)\n        self.assert_datasets_equal(dataset_valid0, dataset_valid1)\n\n    @pytest.mark.parametrize(\'cv\', [2, 4, 5, 10])\n    def test_different_kfolds(self, cv_split_cls, cv, data):\n        if self.num_samples % cv != 0:\n            raise ValueError(""Num samples not divisible by {}"".format(cv))\n\n        dataset_train, dataset_valid = cv_split_cls(cv)(data)\n        assert len(dataset_train) + len(dataset_valid) == self.num_samples\n        assert len(dataset_valid) == self.num_samples // cv\n\n    @pytest.mark.parametrize(\'cv\', [5, 0.2])\n    def test_stratified(self, cv_split_cls, data, cv):\n        num_expected = self.num_samples // 4\n        y = np.hstack([np.repeat([0, 0, 0], num_expected),\n                       np.repeat([1], num_expected)])\n        data.y = y\n\n        dataset_train, dataset_valid = cv_split_cls(\n            cv, stratified=True)(data, y)\n        y_train = data_from_dataset(dataset_train)[1]\n        y_valid = data_from_dataset(dataset_valid)[1]\n\n        assert y_train.sum() == 0.8 * num_expected\n        assert y_valid.sum() == 0.2 * num_expected\n\n    @pytest.mark.parametrize(\'cv\', [0.1, 0.2, 0.5, 0.75])\n    def test_different_fractions(self, cv_split_cls, cv, data):\n        if not (self.num_samples * cv).is_integer() != 0:\n            raise ValueError(""Num samples cannot be evenly distributed for ""\n                             ""fraction {}"".format(cv))\n\n        dataset_train, dataset_valid = cv_split_cls(cv)(data)\n        assert len(dataset_train) + len(dataset_valid) == self.num_samples\n        assert len(dataset_valid) == self.num_samples * cv\n\n    @pytest.mark.parametrize(\'cv\', [0.1, 0.2, 0.5, 0.75])\n    def test_fraction_no_y(self, cv_split_cls, data, cv):\n        if not (self.num_samples * cv).is_integer() != 0:\n            raise ValueError(""Num samples cannot be evenly distributed for ""\n                             ""fraction {}"".format(cv))\n\n        m = int(cv * self.num_samples)\n        n = int((1 - cv) * self.num_samples)\n        dataset_train, dataset_valid = cv_split_cls(\n            cv, stratified=False)(data, None)\n        assert len(dataset_valid) == m\n        assert len(dataset_train) == n\n\n    def test_fraction_no_classifier(self, cv_split_cls, data):\n        y = np.random.random(self.num_samples)\n        data.y = y\n\n        cv = 0.2\n        m = int(cv * self.num_samples)\n        n = int((1 - cv) * self.num_samples)\n        dataset_train, dataset_valid = cv_split_cls(\n            cv, stratified=False)(data, y)\n\n        assert len(dataset_valid) == m\n        assert len(dataset_train) == n\n\n    @pytest.mark.parametrize(\'cv\', [0, -0.001, -0.2, -3])\n    def test_bad_values_raise(self, cv_split_cls, cv):\n        with pytest.raises(ValueError) as exc:\n            cv_split_cls(cv)\n\n        expected = (""Numbers less than 0 are not allowed for cv ""\n                    ""but CVSplit got {}"".format(cv))\n        assert exc.value.args[0] == expected\n\n    @pytest.mark.parametrize(\'cv\', [5, 0.2])\n    def test_not_stratified(self, cv_split_cls, data, cv):\n        num_expected = self.num_samples // 4\n        y = np.hstack([np.repeat([0, 0, 0], num_expected),\n                       np.repeat([1], num_expected)])\n        data.y = y\n\n        dataset_train, dataset_valid = cv_split_cls(\n            cv, stratified=False)(data, y)\n        y_train = data_from_dataset(dataset_train)[1]\n        y_valid = data_from_dataset(dataset_valid)[1]\n\n        # when not stratified, we cannot know the distribution of targets\n        assert y_train.sum() + y_valid.sum() == num_expected\n\n    def test_predefined_split(self, cv_split_cls, data):\n        from sklearn.model_selection import PredefinedSplit\n        indices = (data.y > 0).astype(int)\n        split = PredefinedSplit(indices)\n\n        dataset_train, dataset_valid = cv_split_cls(split)(data)\n        y_train = data_from_dataset(dataset_train)[1]\n        y_valid = data_from_dataset(dataset_valid)[1]\n\n        assert (y_train > 0).all()\n        assert (y_valid == 0).all()\n\n    def test_with_y_none(self, cv_split_cls, data):\n        data.y = None\n        m = self.num_samples // 5\n        n = self.num_samples - m\n        dataset_train, dataset_valid = cv_split_cls(5)(data)\n\n        assert len(dataset_train) == n\n        assert len(dataset_valid) == m\n\n        y_train = data_from_dataset(dataset_train)[1]\n        y_valid = data_from_dataset(dataset_valid)[1]\n\n        assert y_train is None\n        assert y_valid is None\n\n    def test_with_torch_tensors(self, cv_split_cls, data):\n        data.X = to_tensor(data.X, device=\'cpu\')\n        data.y = to_tensor(data.y, device=\'cpu\')\n        m = self.num_samples // 5\n        n = self.num_samples - m\n        dataset_train, dataset_valid = cv_split_cls(5)(data)\n\n        assert len(dataset_valid) == m\n        assert len(dataset_train) == n\n\n    def test_with_torch_tensors_and_stratified(self, cv_split_cls, data):\n        num_expected = self.num_samples // 4\n        data.X = to_tensor(data.X, device=\'cpu\')\n        y = np.hstack([np.repeat([0, 0, 0], num_expected),\n                       np.repeat([1], num_expected)])\n        data.y = to_tensor(y, device=\'cpu\')\n\n        dataset_train, dataset_valid = cv_split_cls(5, stratified=True)(data, y)\n        y_train = data_from_dataset(dataset_train)[1]\n        y_valid = data_from_dataset(dataset_valid)[1]\n\n        assert y_train.sum() == 0.8 * num_expected\n        assert y_valid.sum() == 0.2 * num_expected\n\n    def test_with_list_of_arrays(self, cv_split_cls, data):\n        data.X = [data.X, data.X]\n        m = self.num_samples // 5\n        n = self.num_samples - m\n\n        dataset_train, dataset_valid = cv_split_cls(5)(data)\n        X_train, y_train = data_from_dataset(dataset_train)\n        X_valid, y_valid = data_from_dataset(dataset_valid)\n\n        assert len(X_train[0]) == len(X_train[1]) == len(y_train) == n\n        assert len(X_valid[0]) == len(X_valid[1]) == len(y_valid) == m\n\n    def test_with_dict(self, cv_split_cls, data):\n        data.X = {\'1\': data.X, \'2\': data.X}\n        dataset_train, dataset_valid = cv_split_cls(5)(data)\n\n        m = self.num_samples // 5\n        n = self.num_samples - m\n\n        X_train, y_train = data_from_dataset(dataset_train)\n        X_valid, y_valid = data_from_dataset(dataset_valid)\n\n        assert len(X_train[\'1\']) == len(X_train[\'2\']) == len(y_train) == n\n        assert len(X_valid[\'1\']) == len(X_valid[\'2\']) == len(y_valid) == m\n\n    @pytest.mark.skipif(not pandas_installed, reason=\'pandas is not installed\')\n    def test_with_pandas(self, cv_split_cls, data):\n        import pandas as pd\n\n        data.X = pd.DataFrame(\n            data.X,\n            columns=[str(i) for i in range(data.X.shape[1])],\n        )\n        dataset_train, dataset_valid = cv_split_cls(5)(data)\n\n        m = self.num_samples // 5\n        X_train, y_train = data_from_dataset(dataset_train)\n        X_valid, y_valid = data_from_dataset(dataset_valid)\n\n        assert len(X_train) + len(X_valid) == self.num_samples\n        assert len(y_train) + len(y_valid) == self.num_samples\n        assert len(X_valid) == len(y_valid) == m\n\n    def test_y_str_val_stratified(self, cv_split_cls, data):\n        y = np.array([\'a\', \'a\', \'a\', \'b\'] * (self.num_samples // 4))\n        if len(data.X) != len(y):\n            raise ValueError\n        data.y = y\n\n        dataset_train, dataset_valid = cv_split_cls(\n            5, stratified=True)(data, y)\n        y_train = data_from_dataset(dataset_train)[1]\n        y_valid = data_from_dataset(dataset_valid)[1]\n\n        assert np.isclose(np.mean(y_train == \'b\'), 0.25)\n        assert np.isclose(np.mean(y_valid == \'b\'), 0.25)\n\n    def test_y_list_of_arr_does_not_raise(self, cv_split_cls, data):\n        y = [np.zeros(self.num_samples), np.ones(self.num_samples)]\n        data.y = y\n        cv_split_cls(5, stratified=False)(data)\n\n    def test_y_list_of_arr_stratified(self, cv_split_cls, data):\n        y = [np.zeros(self.num_samples), np.ones(self.num_samples)]\n        data.y = y\n        with pytest.raises(ValueError) as exc:\n            cv_split_cls(5, stratified=True)(data, y)\n\n        expected = ""Stratified CV requires explicitly passing a suitable y.""\n        assert exc.value.args[0] == expected\n\n    def test_y_dict_does_not_raise(self, cv_split_cls, data):\n        y = {\'a\': np.zeros(self.num_samples), \'b\': np.ones(self.num_samples)}\n        data.y = y\n\n        cv_split_cls(5, stratified=False)(data)\n\n    def test_y_dict_stratified_raises(self, cv_split_cls, data):\n        X = data[0]\n        y = {\'a\': np.zeros(len(X)), \'b\': np.ones(len(X))}\n\n        with pytest.raises(ValueError):\n            # an sklearn error is raised\n            cv_split_cls(5, stratified=True)(X, y)\n\n    @pytest.mark.parametrize(\'cv\', [5, 0.2])\n    @pytest.mark.parametrize(\'X\', [np.zeros((100, 10)), torch.zeros((100, 10))])\n    def test_y_none_stratified(self, cv_split_cls, data, cv, X):\n        data.X = X\n        with pytest.raises(ValueError) as exc:\n            cv_split_cls(cv, stratified=True)(data, None)\n\n        expected = ""Stratified CV requires explicitly passing a suitable y.""\n        assert exc.value.args[0] == expected\n\n    def test_shuffle_split_reproducible_with_random_state(\n            self, cv_split_cls, dataset_cls):\n        n = self.num_samples\n        X, y = np.random.random((n, 10)), np.random.randint(0, 10, size=n)\n        cv = cv_split_cls(0.2, stratified=False)\n\n        dst0, dsv0 = cv(dataset_cls(X, y))\n        dst1, dsv1 = cv(dataset_cls(X, y))\n\n        Xt0, yt0 = data_from_dataset(dst0)\n        Xv0, yv0 = data_from_dataset(dsv0)\n        Xt1, yt1 = data_from_dataset(dst1)\n        Xv1, yv1 = data_from_dataset(dsv1)\n\n        assert not np.allclose(Xt0, Xt1)\n        assert not np.allclose(Xv0, Xv1)\n        assert not np.allclose(yt0, yt1)\n        assert not np.allclose(yv0, yv1)\n\n    def test_group_kfold(self, cv_split_cls, data):\n        from sklearn.model_selection import GroupKFold\n\n        X, y = data.X, data.y\n        n = self.num_samples // 2\n        groups = np.asarray(\n            [0 for _ in range(n)] + [1 for _ in range(self.num_samples - n)])\n\n        dataset_train, dataset_valid = cv_split_cls(\n            GroupKFold(n_splits=2))(data, groups=groups)\n        X_train, y_train = data_from_dataset(dataset_train)\n        X_valid, y_valid = data_from_dataset(dataset_valid)\n\n        assert np.allclose(X[:n], X_train)\n        assert np.allclose(y[:n], y_train)\n        assert np.allclose(X[n:], X_valid)\n        assert np.allclose(y[n:], y_valid)\n\n    @pytest.mark.parametrize(\n        \'args, kwargs, expect_warning\',\n        [\n            ([], {}, False),\n            ([], {""random_state"": 0}, True),\n            ([10], {""random_state"": 0}, True),\n            ([0.7], {""random_state"": 0}, False),\n            ([[]], {}, False),\n            ([[]], {""random_state"": 0}, True),\n        ])\n    def test_random_state_not_used_warning(\n            self, cv_split_cls, args, kwargs, expect_warning):\n        with pytest.warns(None) as record:\n            cv_split_cls(*args, **kwargs)\n\n        if expect_warning:\n            assert len(record) == 1\n            warning = record[0].message\n            assert isinstance(warning, FutureWarning)\n            assert warning.args[0] == (\n                ""Setting a random_state has no effect since cv is not a float. ""\n                ""This will raise an error in a future. You should leave ""\n                ""random_state to its default (None), or set cv to a float value.""\n            )\n        else:\n            assert not record\n'"
skorch/tests/test_helper.py,15,"b'""""""Test for helper.py""""""\nimport pickle\n\nimport numpy as np\nimport pytest\nfrom sklearn.datasets import make_classification\nfrom sklearn.pipeline import Pipeline\nimport torch\nfrom torch import nn\n\nfrom skorch.tests.conftest import pandas_installed\n\n\ndef assert_dicts_equal(d0, d1):\n    assert d0.keys() == d1.keys()\n    for key in d0.keys():\n        val0, val1 = d0[key], d1[key]\n\n        np.testing.assert_allclose(val0, val1)\n        assert val0.dtype == val1.dtype\n\n\nclass TestSliceDict:\n    @pytest.fixture\n    def data(self):\n        X, y = make_classification(100, 20, n_informative=10, random_state=0)\n        return X.astype(np.float32), y\n\n    @pytest.fixture(scope=\'session\')\n    def sldict_cls(self):\n        from skorch.helper import SliceDict\n        return SliceDict\n\n    @pytest.fixture\n    def sldict(self, sldict_cls):\n        return sldict_cls(\n            f0=np.arange(4),\n            f1=np.arange(12).reshape(4, 3),\n        )\n\n    def test_init_inconsistent_shapes(self, sldict_cls):\n        with pytest.raises(ValueError) as exc:\n            sldict_cls(f0=np.ones((10, 5)), f1=np.ones((11, 5)))\n        assert str(exc.value) == (\n            ""Initialized with items of different lengths: 10, 11"")\n\n    @pytest.mark.parametrize(\'item\', [\n        np.ones(4),\n        np.ones((4, 1)),\n        np.ones((4, 4)),\n        np.ones((4, 10, 7)),\n        np.ones((4, 1, 28, 28)),\n    ])\n    def test_set_item_correct_shape(self, sldict, item):\n        # does not raise\n        sldict[\'f2\'] = item\n\n    @pytest.mark.parametrize(\'item\', [\n        np.ones(3),\n        np.ones((1, 100)),\n        np.ones((5, 1000)),\n        np.ones((1, 100, 10)),\n        np.ones((28, 28, 1, 100)),\n    ])\n    def test_set_item_incorrect_shape_raises(self, sldict, item):\n        with pytest.raises(ValueError) as exc:\n            sldict[\'f2\'] = item\n        assert str(exc.value) == (\n            ""Cannot set array with shape[0] != 4"")\n\n    @pytest.mark.parametrize(\'key\', [1, 1.2, (1, 2), [3]])\n    def test_set_item_incorrect_key_type(self, sldict, key):\n        with pytest.raises(TypeError) as exc:\n            sldict[key] = np.ones((100, 5))\n        assert str(exc.value).startswith(""Key must be str, not <"")\n\n    @pytest.mark.parametrize(\'item\', [\n        np.ones(3),\n        np.ones((1, 100)),\n        np.ones((5, 1000)),\n        np.ones((1, 100, 10)),\n        np.ones((28, 28, 1, 100)),\n    ])\n    def test_update_incorrect_shape_raises(self, sldict, item):\n        with pytest.raises(ValueError) as exc:\n            sldict.update({\'f2\': item})\n        assert str(exc.value) == (\n            ""Cannot set array with shape[0] != 4"")\n\n    @pytest.mark.parametrize(\'item\', [123, \'hi\', [1, 2, 3]])\n    def test_set_first_item_no_shape_raises(self, sldict_cls, item):\n        with pytest.raises(AttributeError):\n            sldict_cls(f0=item)\n\n    @pytest.mark.parametrize(\'kwargs, expected\', [\n        ({}, 0),\n        (dict(a=np.zeros(12)), 12),\n        (dict(a=np.zeros(12), b=np.ones((12, 5))), 12),\n        (dict(a=np.ones((10, 1, 1)), b=np.ones((10, 10)), c=np.ones(10)), 10),\n    ])\n    def test_len_and_shape(self, sldict_cls, kwargs, expected):\n        sldict = sldict_cls(**kwargs)\n        assert len(sldict) == expected\n        assert sldict.shape == (expected,)\n\n    def test_get_item_str_key(self, sldict_cls):\n        sldict = sldict_cls(a=np.ones(5), b=np.zeros(5))\n        assert (sldict[\'a\'] == np.ones(5)).all()\n        assert (sldict[\'b\'] == np.zeros(5)).all()\n\n    @pytest.mark.parametrize(\'sl, expected\', [\n        (slice(0, 1), {\'f0\': np.array([0]), \'f1\': np.array([[0, 1, 2]])}),\n        (slice(1, 2), {\'f0\': np.array([1]), \'f1\': np.array([[3, 4, 5]])}),\n        (slice(0, 2), {\'f0\': np.array([0, 1]),\n                       \'f1\': np.array([[0, 1, 2], [3, 4, 5]])}),\n        (slice(0, None), dict(f0=np.arange(4),\n                              f1=np.arange(12).reshape(4, 3))),\n        (slice(-1, None), {\'f0\': np.array([3]),\n                           \'f1\': np.array([[9, 10, 11]])}),\n        (slice(None, None, -1), dict(f0=np.arange(4)[::-1],\n                                     f1=np.arange(12).reshape(4, 3)[::-1])),\n    ])\n    def test_get_item_slice(self, sldict_cls, sldict, sl, expected):\n        sliced = sldict[sl]\n        assert_dicts_equal(sliced, sldict_cls(**expected))\n\n    def test_slice_list(self, sldict, sldict_cls):\n        result = sldict[[0, 2]]\n        expected = sldict_cls(\n            f0=np.array([0, 2]),\n            f1=np.array([[0, 1, 2], [6, 7, 8]]))\n        assert_dicts_equal(result, expected)\n\n    def test_slice_mask(self, sldict, sldict_cls):\n        result = sldict[np.array([1, 0, 1, 0]).astype(bool)]\n        expected = sldict_cls(\n            f0=np.array([0, 2]),\n            f1=np.array([[0, 1, 2], [6, 7, 8]]))\n        assert_dicts_equal(result, expected)\n\n    def test_slice_int(self, sldict):\n        with pytest.raises(ValueError) as exc:\n            # pylint: disable=pointless-statement\n            sldict[0]\n        assert str(exc.value) == \'SliceDict cannot be indexed by integers.\'\n\n    def test_len_sliced(self, sldict):\n        assert len(sldict) == 4\n        for i in range(1, 4):\n            assert len(sldict[:i]) == i\n\n    def test_str_repr(self, sldict, sldict_cls):\n        loc = locals().copy()\n        loc.update({\'array\': np.array, \'SliceDict\': sldict_cls})\n        # pylint: disable=eval-used\n        result = eval(str(sldict), globals(), loc)\n        assert_dicts_equal(result, sldict)\n\n    def test_iter_over_keys(self, sldict):\n        found_keys = {key for key in sldict}\n        expected_keys = {\'f0\', \'f1\'}\n        assert found_keys == expected_keys\n\n    def test_grid_search_with_dict_works(\n            self, sldict_cls, data, classifier_module):\n        from sklearn.model_selection import GridSearchCV\n        from skorch import NeuralNetClassifier\n\n        net = NeuralNetClassifier(classifier_module)\n        X, y = data\n        X = sldict_cls(X=X)\n        params = {\n            \'lr\': [0.01, 0.02],\n            \'max_epochs\': [10, 20],\n        }\n        gs = GridSearchCV(net, params, refit=True, cv=3, scoring=\'accuracy\',\n                          iid=True)\n        gs.fit(X, y)\n        print(gs.best_score_, gs.best_params_)\n\n    def test_copy(self, sldict, sldict_cls):\n        copied = sldict.copy()\n        assert copied.shape == sldict.shape\n        assert isinstance(copied, sldict_cls)\n\n    def test_fromkeys_raises(self, sldict_cls):\n        with pytest.raises(TypeError) as exc:\n            sldict_cls.fromkeys([\'f0\', \'f1\'])\n\n        msg = ""SliceDict does not support fromkeys.""\n        assert exc.value.args[0] == msg\n\n    def test_update(self, sldict, sldict_cls):\n        copied = sldict.copy()\n        copied[\'f0\'] = -copied[\'f0\']\n\n        sldict.update(copied)\n        assert (sldict[\'f0\'] == copied[\'f0\']).all()\n        assert isinstance(sldict, sldict_cls)\n\n    def test_equals_arrays(self, sldict):\n        copied = sldict.copy()\n        copied[\'f0\'] = -copied[\'f0\']\n\n        # pylint: disable=comparison-with-itself\n        assert copied == copied\n        assert not copied == sldict\n        assert copied != sldict\n\n    def test_equals_arrays_deep(self, sldict):\n        copied = sldict.copy()\n        copied[\'f0\'] = np.array(copied[\'f0\'].copy())\n\n        # pylint: disable=comparison-with-itself\n        assert copied == copied\n        assert copied == sldict\n\n    def test_equals_tensors(self, sldict_cls):\n        sldict = sldict_cls(\n            f0=torch.arange(4),\n            f1=torch.arange(12).reshape(4, 3),\n        )\n        copied = sldict.copy()\n        copied[\'f0\'] = -copied[\'f0\']\n\n        # pylint: disable=comparison-with-itself\n        assert copied == copied\n        assert not copied == sldict\n        assert copied != sldict\n\n    def test_equals_tensors_deep(self, sldict_cls):\n        sldict = sldict_cls(\n            f0=torch.arange(4),\n            f1=torch.arange(12).reshape(4, 3),\n        )\n        copied = sldict.copy()\n        copied[\'f0\'] = copied[\'f0\'].clone()\n\n        # pylint: disable=comparison-with-itself\n        assert copied == copied\n        assert copied == sldict\n\n    def test_equals_arrays_tensors_mixed(self, sldict_cls):\n        sldict0 = sldict_cls(\n            f0=np.arange(4),\n            f1=torch.arange(12).reshape(4, 3),\n        )\n        sldict1 = sldict_cls(\n            f0=np.arange(4),\n            f1=torch.arange(12).reshape(4, 3),\n        )\n\n        assert sldict0 == sldict1\n\n        sldict1[\'f0\'] = torch.arange(4)\n        assert sldict0 != sldict1\n\n    def test_equals_different_keys(self, sldict_cls):\n        sldict0 = sldict_cls(\n            a=np.arange(3),\n        )\n        sldict1 = sldict_cls(\n            a=np.arange(3),\n            b=np.arange(3, 6),\n        )\n        assert sldict0 != sldict1\n\n\nclass TestSliceDataset:\n    @pytest.fixture(scope=\'class\')\n    def data(self):\n        X, y = make_classification(100, 20, n_informative=10, random_state=0)\n        return X.astype(np.float32), y\n\n    @pytest.fixture\n    def X(self, data):\n        return data[0]\n\n    @pytest.fixture\n    def y(self, data):\n        return data[1]\n\n    @pytest.fixture\n    def custom_ds(self, data):\n        """"""Return a custom dataset instance""""""\n        from skorch.dataset import Dataset\n        class MyDataset(Dataset):\n            """"""Simple pytorch dataset that returns 2 values""""""\n            def __len__(self):\n                return len(self.X)\n\n            def __getitem__(self, i):\n                Xi = self.X[i]\n                yi = self.y[i]\n                return self.transform(Xi, yi)\n\n        return MyDataset(*data)\n\n    @pytest.fixture(scope=\'session\')\n    def slds_cls(self):\n        from skorch.helper import SliceDataset\n        return SliceDataset\n\n    @pytest.fixture\n    def slds(self, slds_cls, custom_ds):\n        return slds_cls(custom_ds)\n\n    @pytest.fixture\n    def slds_y(self, slds_cls, custom_ds):\n        return slds_cls(custom_ds, idx=1)\n\n    def test_len_and_shape(self, slds, y):\n        assert len(slds) == len(y)\n        assert slds.shape == (len(y),)\n\n    @pytest.mark.parametrize(\'sl\', [\n        slice(0, 1),\n        slice(1, 2),\n        slice(0, 2),\n        slice(0, None),\n        slice(-1, None),\n        slice(None, None, -1),\n        [0],\n        [55],\n        [-3],\n        [0, 10, 3, -8, 3],\n        np.ones(100, dtype=np.bool),\n        # boolean mask array of length 100\n        np.array([0, 0, 1, 0] * 25, dtype=np.bool),\n    ])\n    def test_len_and_shape_sliced(self, slds, y, sl):\n        assert len(slds[sl]) == len(y[sl])\n        assert slds[sl].shape == (len(y[sl]),)\n\n    @pytest.mark.parametrize(\'n\', [0, 1])\n    def test_slice_non_int_is_slicedataset(self, slds_cls, custom_ds, n):\n        slds = slds_cls(custom_ds, idx=n)\n        sl = np.arange(7, 55, 3)\n        sliced = slds[sl]\n        assert isinstance(sliced, slds_cls)\n        assert np.allclose(sliced.indices_, sl)\n\n    @pytest.mark.parametrize(\'n\', [0, 1])\n    @pytest.mark.parametrize(\'sl\', [0, 55, -3])\n    def test_slice(self, slds_cls, custom_ds, X, y, sl, n):\n        data = y if n else X\n        slds = slds_cls(custom_ds, idx=n)\n        sliced = slds[sl]\n        x = data[sl]\n        assert np.allclose(sliced, x)\n\n    @pytest.mark.parametrize(\'n\', [0, 1])\n    @pytest.mark.parametrize(\'sl0, sl1\', [\n        ([55], 0),\n        (slice(0, 1), 0),\n        (slice(-1, None), 0),\n        ([55], -1),\n        ([0, 10, 3, -8, 3], 1),\n        (np.ones(100, dtype=np.bool), 5),\n        # boolean mask array of length 100\n        (np.array([0, 0, 1, 0] * 25, dtype=np.bool), 6),\n    ])\n    def test_slice_twice(self, slds_cls, custom_ds, X, y, sl0, sl1, n):\n        data = X if n == 0 else y\n        slds = slds_cls(custom_ds, idx=n)\n        sliced = slds[sl0][sl1]\n        x = data[sl0][sl1]\n        assert np.allclose(sliced, x)\n\n    @pytest.mark.parametrize(\'n\', [0, 1])\n    @pytest.mark.parametrize(\'sl0, sl1, sl2\', [\n        (slice(0, 50), slice(10, 20), 5),\n        ([0, 10, 3, -8, 3], [1, 2, 3], 2),\n        (np.ones(100, dtype=np.bool), np.arange(10, 40), 29),\n    ])\n    def test_slice_three_times(self, slds_cls, custom_ds, X, y, sl0, sl1, sl2, n):\n        data = y if n else X\n        slds = slds_cls(custom_ds, idx=n)\n        sliced = slds[sl0][sl1][sl2]\n        x = data[sl0][sl1][sl2]\n        assert np.allclose(sliced, x)\n\n    def test_explicitly_pass_indices_at_init(self, slds_cls, custom_ds, X):\n        # test passing indices directy to __init__\n        slds = slds_cls(custom_ds, indices=np.arange(10))\n        sliced0 = slds[5:]\n        assert np.allclose(sliced0, X[5:10])\n\n        sliced1 = sliced0[2]\n        assert np.allclose(sliced1, X[7])\n\n    def test_access_element_out_of_bounds(self, slds_cls, custom_ds):\n        slds = slds_cls(custom_ds, idx=2)\n        with pytest.raises(IndexError) as exc:\n            # pylint: disable=pointless-statement\n            slds[0]\n\n        msg = (""SliceDataset is trying to access element 2 but there are only ""\n               ""2 elements."")\n        assert exc.value.args[0] == msg\n\n    def test_fit_with_slds_works(self, slds, y, classifier_module):\n        from skorch import NeuralNetClassifier\n        net = NeuralNetClassifier(classifier_module)\n        net.fit(slds, y)  # does not raise\n\n    def test_fit_with_slds_without_valid_works(self, slds, y, classifier_module):\n        from skorch import NeuralNetClassifier\n        net = NeuralNetClassifier(classifier_module, train_split=False)\n        net.fit(slds, y)  # does not raise\n\n    def test_grid_search_with_slds_works(\n            self, slds, y, classifier_module):\n        from sklearn.model_selection import GridSearchCV\n        from skorch import NeuralNetClassifier\n\n        net = NeuralNetClassifier(\n            classifier_module,\n            train_split=False,\n            verbose=0,\n        )\n        params = {\n            \'lr\': [0.01, 0.02],\n            \'max_epochs\': [10, 20],\n        }\n        gs = GridSearchCV(net, params, refit=False, cv=3, scoring=\'accuracy\', iid=True)\n        gs.fit(slds, y)  # does not raise\n\n    def test_grid_search_with_slds_and_internal_split_works(\n            self, slds, y, classifier_module):\n        from sklearn.model_selection import GridSearchCV\n        from skorch import NeuralNetClassifier\n\n        net = NeuralNetClassifier(classifier_module)\n        params = {\n            \'lr\': [0.01, 0.02],\n            \'max_epochs\': [10, 20],\n        }\n        gs = GridSearchCV(net, params, refit=True, cv=3, scoring=\'accuracy\', iid=True)\n        gs.fit(slds, y)  # does not raise\n\n    def test_grid_search_with_slds_X_and_slds_y(\n            self, slds, slds_y, classifier_module):\n        from sklearn.model_selection import GridSearchCV\n        from skorch import NeuralNetClassifier\n\n        net = NeuralNetClassifier(\n            classifier_module,\n            train_split=False,\n            verbose=0,\n        )\n        params = {\n            \'lr\': [0.01, 0.02],\n            \'max_epochs\': [10, 20],\n        }\n        gs = GridSearchCV(net, params, refit=False, cv=3, scoring=\'accuracy\', iid=True)\n        gs.fit(slds, slds_y)  # does not raise\n\n    def test_index_with_2d_array_raises(self, slds):\n        i = np.arange(4).reshape(2, 2)\n        with pytest.raises(IndexError) as exc:\n            # pylint: disable=pointless-statement\n            slds[i]\n\n        msg = (""SliceDataset only supports slicing with 1 ""\n               ""dimensional arrays, got 2 dimensions instead."")\n        assert exc.value.args[0] == msg\n\n\nclass TestPredefinedSplit():\n\n    @pytest.fixture\n    def predefined_split(self):\n        from skorch.helper import predefined_split\n        return predefined_split\n\n    def test_pickle(self, predefined_split, data):\n        from skorch.dataset import Dataset\n\n        valid_dataset = Dataset(*data)\n        train_split = predefined_split(valid_dataset)\n\n        # does not raise\n        pickle.dumps(train_split)\n\n\nclass TestDataFrameTransformer:\n    @pytest.fixture\n    def transformer_cls(self):\n        from skorch.helper import DataFrameTransformer\n        return DataFrameTransformer\n\n    @pytest.mark.skipif(not pandas_installed, reason=\'pandas is not installed\')\n    @pytest.fixture\n    def df(self):\n        """"""DataFrame containing float, int, category types""""""\n        import pandas as pd\n\n        df = pd.DataFrame({\n            \'col_floats\': [0.1, 0.2, 0.3],\n            \'col_ints\': [11, 11, 10],\n            \'col_cats\': [\'a\', \'b\', \'a\'],\n        })\n        df[\'col_cats\'] = df[\'col_cats\'].astype(\'category\')\n        return df\n\n    def test_fit_transform_defaults(self, transformer_cls, df):\n        expected = {\n            \'X\': np.asarray([\n                [0.1, 11.0],\n                [0.2, 11.0],\n                [0.3, 10.0],\n            ]).astype(np.float32),\n            \'col_cats\': np.asarray([0, 1, 0]),\n        }\n        Xt = transformer_cls().fit_transform(df)\n        assert_dicts_equal(Xt, expected)\n\n    def test_fit_and_transform_defaults(self, transformer_cls, df):\n        expected = {\n            \'X\': np.asarray([\n                [0.1, 11.0],\n                [0.2, 11.0],\n                [0.3, 10.0],\n            ]).astype(np.float32),\n            \'col_cats\': np.asarray([0, 1, 0]),\n        }\n        Xt = transformer_cls().fit(df).transform(df)\n        assert_dicts_equal(Xt, expected)\n\n    def test_fit_transform_defaults_two_categoricals(\n            self, transformer_cls, df):\n        expected = {\n            \'X\': np.asarray([\n                [0.1, 11.0],\n                [0.2, 11.0],\n                [0.3, 10.0],\n            ]).astype(np.float32),\n            \'col_cats\': np.asarray([0, 1, 0]),\n            \'col_foo\': np.asarray([1, 1, 0]),\n        }\n        df = df.assign(col_foo=df[\'col_ints\'].astype(\'category\'))\n        Xt = transformer_cls().fit_transform(df)\n        assert_dicts_equal(Xt, expected)\n\n    def test_fit_transform_int_as_categorical(self, transformer_cls, df):\n        expected = {\n            \'X\': np.asarray([0.1, 0.2, 0.3]).astype(np.float32).reshape(-1, 1),\n            \'col_ints\': np.asarray([1, 1, 0]),\n            \'col_cats\': np.asarray([0, 1, 0]),\n        }\n        Xt = transformer_cls(treat_int_as_categorical=True).fit_transform(df)\n        assert_dicts_equal(Xt, expected)\n\n    def test_fit_transform_no_X(self, transformer_cls, df):\n        df = df[[\'col_ints\', \'col_cats\']]  # no float type present\n        expected = {\n            \'col_ints\': np.asarray([1, 1, 0]),\n            \'col_cats\': np.asarray([0, 1, 0]),\n        }\n        Xt = transformer_cls(treat_int_as_categorical=True).fit_transform(df)\n        assert_dicts_equal(Xt, expected)\n\n    @pytest.mark.parametrize(\'data\', [\n        np.array([object, object, object]),\n        np.array([\'foo\', \'bar\', \'baz\']),\n    ])\n    def test_invalid_dtype_raises(self, transformer_cls, df, data):\n        df = df.assign(invalid=data)\n        with pytest.raises(TypeError) as exc:\n            transformer_cls().fit_transform(df)\n\n        msg = exc.value.args[0]\n        expected = (""The following columns have dtypes that cannot be ""\n                    ""interpreted as numerical dtypes: invalid (object)"")\n        assert msg == expected\n\n    def test_two_invalid_dtypes_raises(self, transformer_cls, df):\n        df = df.assign(\n            invalid0=np.array([object, object, object]),\n            invalid1=np.array([\'foo\', \'bar\', \'baz\']),\n        )\n        with pytest.raises(TypeError) as exc:\n            transformer_cls().fit_transform(df)\n\n        msg = exc.value.args[0]\n        expected = (""The following columns have dtypes that cannot be ""\n                    ""interpreted as numerical dtypes: invalid0 (object), ""\n                    ""invalid1 (object)"")\n        assert msg == expected\n\n    @pytest.mark.parametrize(\'dtype\', [np.float16, np.float32, np.float64])\n    def test_set_float_dtype(self, transformer_cls, df, dtype):\n        Xt = transformer_cls(float_dtype=dtype).fit_transform(df)\n        assert Xt[\'X\'].dtype == dtype\n\n    @pytest.mark.parametrize(\'dtype\', [np.int16, np.int32, np.int64])\n    def test_set_int_dtype(self, transformer_cls, df, dtype):\n        Xt = transformer_cls(\n            treat_int_as_categorical=True, int_dtype=dtype).fit_transform(df)\n        assert Xt[\'col_cats\'].dtype == dtype\n        assert Xt[\'col_ints\'].dtype == dtype\n\n    def test_leave_float_dtype_as_in_df(self, transformer_cls, df):\n        # None -> don\'t cast\n        Xt = transformer_cls(float_dtype=None).fit_transform(df)\n        assert Xt[\'X\'].dtype == np.float64\n\n    def test_leave_int_dtype_as_in_df(self, transformer_cls, df):\n        # None -> don\'t cast\n        # pandas will use the lowest precision int that is capable to\n        # encode the categories; since we only have 2 values, that is\n        # int8 here\n        Xt = transformer_cls(int_dtype=None).fit_transform(df)\n        assert Xt[\'col_cats\'].dtype == np.int8\n\n    def test_column_named_X_present(self, transformer_cls, df):\n        df = df.assign(X=df[\'col_cats\'])\n        with pytest.raises(ValueError) as exc:\n            transformer_cls().fit(df)\n\n        msg = exc.value.args[0]\n        expected = (""DataFrame contains a column named \'X\', which clashes ""\n                    ""with the name chosen for cardinal features; consider ""\n                    ""renaming that column."")\n        assert msg == expected\n\n    @pytest.fixture\n    def module_cls(self):\n        """"""Simple module with embedding and linear layers""""""\n        # pylint: disable=missing-docstring\n        class MyModule(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.reset_params()\n\n            def reset_params(self):\n                self.embedding = nn.Embedding(2, 10)\n                self.linear = nn.Linear(2, 10)\n                self.out = nn.Linear(20, 2)\n                self.nonlin = nn.Softmax(dim=-1)\n\n            # pylint: disable=arguments-differ\n            def forward(self, X, col_cats):\n                X_lin = self.linear(X)\n                X_cat = self.embedding(col_cats)\n                X_concat = torch.cat((X_lin, X_cat), dim=1)\n                return self.nonlin(self.out(X_concat))\n\n        return MyModule\n\n    @pytest.fixture\n    def net(self, module_cls):\n        from skorch import NeuralNetClassifier\n        net = NeuralNetClassifier(\n            module_cls,\n            train_split=None,\n            max_epochs=3,\n        )\n        return net\n\n    @pytest.fixture\n    def pipe(self, transformer_cls, net):\n        pipe = Pipeline([\n            (\'transform\', transformer_cls()),\n            (\'net\', net),\n        ])\n        return pipe\n\n    def test_fit_and_predict_with_pipeline(self, pipe, df):\n        y = np.asarray([0, 0, 1])\n        pipe.fit(df, y)\n\n        y_proba = pipe.predict_proba(df)\n        assert y_proba.shape == (len(df), 2)\n\n        y_pred = pipe.predict(df)\n        assert y_pred.shape == (len(df),)\n\n    def test_describe_signature_default_df(self, transformer_cls, df):\n        result = transformer_cls().describe_signature(df)\n        expected = {\n            \'X\': {""dtype"": torch.float32, ""input_units"": 2},\n            \'col_cats\': {""dtype"": torch.int64, ""input_units"": 2},\n        }\n        assert result == expected\n\n    def test_describe_signature_non_default_df(self, transformer_cls, df):\n        # replace float column with integer having 3 unique units\n        df = df.assign(col_floats=[1, 2, 0])\n\n        result = transformer_cls(\n            treat_int_as_categorical=True).describe_signature(df)\n        expected = {\n            \'col_floats\': {""dtype"": torch.int64, ""input_units"": 3},\n            \'col_ints\': {""dtype"": torch.int64, ""input_units"": 2},\n            \'col_cats\': {""dtype"": torch.int64, ""input_units"": 2},\n        }\n        assert result == expected\n\n    def test_describe_signature_other_dtypes(self, transformer_cls, df):\n        transformer = transformer_cls(\n            float_dtype=np.float16,\n            int_dtype=np.int32,\n        )\n        result = transformer.describe_signature(df)\n        expected = {\n            \'X\': {""dtype"": torch.float16, ""input_units"": 2},\n            \'col_cats\': {""dtype"": torch.int32, ""input_units"": 2},\n        }\n        assert result == expected\n'"
skorch/tests/test_history.py,0,"b'""""""Tests for history.py.""""""\n\nimport pytest\n\nfrom skorch.history import History\n\n\nclass TestHistory:\n\n    test_epochs = 3\n    test_batches = 4\n\n    @pytest.fixture\n    def history(self):\n        """"""Return a history filled with epoch and batch data.""""""\n        h = History()\n        for num_epoch in range(self.test_epochs):\n            h.new_epoch()\n            h.record(\'duration\', 1)\n            h.record(\'total_loss\', num_epoch + self.test_batches)\n            if num_epoch == 2:\n                h.record(\'extra\', 42)\n            for num_batch in range(self.test_batches):\n                h.new_batch()\n                h.record_batch(\'loss\', num_epoch + num_batch)\n                if num_batch % 2 == 0 and (num_epoch + 1) != self.test_epochs:\n                    h.record_batch(\'extra_batch\', 23)\n        return h\n\n    @pytest.fixture\n    def ref(self, history):\n        return history.to_list()\n\n    def test_list_initialization(self):\n        h = History([1, 2, 3])\n        assert len(h) == 3\n\n    def test_history_length(self, history):\n        assert len(history) == self.test_epochs\n        # we expect to have the extracted batches for each epoch\n        assert len(history[:, \'batches\']) == self.test_epochs\n\n    def test_history_epoch_column(self, history, ref):\n        total_losses = history[:, \'total_loss\']\n        total_losses_ref = [n[\'total_loss\'] for n in ref]\n        assert total_losses == total_losses_ref\n\n    def test_history_epoch_two_columns(self, history, ref):\n        duration_with_losses = history[:, (\'total_loss\', \'duration\')]\n\n        total_losses_ref = [n[\'total_loss\'] for n in ref]\n        durations_ref = [n[\'duration\'] for n in ref]\n        expected = list(zip(total_losses_ref, durations_ref))\n\n        assert duration_with_losses == expected\n\n    def test_history_epoch_two_columns_different_order(self, history, ref):\n        duration_with_losses = history[:, (\'duration\', \'total_loss\')]\n\n        total_losses_ref = [n[\'total_loss\'] for n in ref]\n        durations_ref = [n[\'duration\'] for n in ref]\n        expected = list(zip(durations_ref, total_losses_ref))\n\n        assert duration_with_losses == expected\n\n    def test_history_partial_index(self, history, ref):\n        extra = history[:, \'extra\']\n        assert len(extra) == 1\n        # we retrieve \'extra\' from a slice, therefore we expect a list as result\n        assert extra == [ref[2][\'extra\']]\n\n    def test_history_partial_and_full_index(self, history, ref):\n        total_loss_with_extra = history[:, (\'total_loss\', \'extra\')]\n\n        assert len(total_loss_with_extra) == 1\n        assert total_loss_with_extra[0][0] == ref[2][\'total_loss\']\n        assert total_loss_with_extra[0][1] == ref[2][\'extra\']\n\n    def test_history_partial_join_list(self, history, ref):\n        total = history[:, [\'total_loss\', \'extra\', \'batches\']]\n\n        # there\'s only one epoch with the \'extra\' key.\n        assert len(total) == 1\n        assert total[0][0] == ref[2][\'total_loss\']\n        assert total[0][1] == ref[2][\'extra\']\n        assert total[0][2] == ref[2][\'batches\']\n\n    def test_history_retrieve_single_value(self, history, ref):\n        total_loss_0 = history[0, \'total_loss\']\n        assert total_loss_0 == ref[0][\'total_loss\']\n\n    def test_history_retrieve_multiple_values(self, history, ref):\n        total_loss_0_to_1 = history[0:1, \'total_loss\']\n        assert total_loss_0_to_1 == [n[\'total_loss\'] for n in ref[0:1]]\n\n    def test_history_non_existing_values(self, history):\n        with pytest.raises(KeyError):\n            # pylint: disable=pointless-statement\n            history[:, \'non-existing\']\n        with pytest.raises(KeyError):\n            # pylint: disable=pointless-statement\n            history[0, \'extra\']\n\n    def test_history_non_existing_values_batch(self, history):\n        with pytest.raises(KeyError):\n            # pylint: disable=pointless-statement\n            history[:, \'batches\', :, \'non-existing\']\n        with pytest.raises(KeyError):\n            # pylint: disable=pointless-statement\n            history[:, \'batches\', 1, \'extra_batch\']\n\n    def test_history_mixed_slicing(self, history, ref):\n        losses = history[:, \'batches\', 0, \'loss\']\n\n        assert len(losses) == self.test_epochs\n        assert losses == [epoch[\'batches\'][0][\'loss\'] for epoch in ref]\n\n        losses = history[0, \'batches\', :, \'loss\']\n        assert losses == [batch[\'loss\'] for batch in ref[0][\'batches\']]\n\n    def test_history_partial_and_full_index_batches(self, history, ref):\n        loss_with_extra = history[:, \'batches\', :, (\'loss\', \'extra_batch\')]\n\n        expected_e0 = [(b[\'loss\'], b[\'extra_batch\']) for b in ref[0][\'batches\']\n                       if \'extra_batch\' in b]\n        expected_e1 = [(b[\'loss\'], b[\'extra_batch\']) for b in ref[1][\'batches\']\n                       if \'extra_batch\' in b]\n\n        assert len(loss_with_extra) == self.test_epochs - 1\n        assert loss_with_extra[0] == expected_e0\n        assert loss_with_extra[1] == expected_e1\n\n    def test_history_partial_batches_batch_key_3rd(self, history, ref):\n        extra_batches = history[:, \'batches\', :, \'extra_batch\']\n\n        expected_e0 = [b[\'extra_batch\'] for b in ref[0][\'batches\']\n                       if \'extra_batch\' in b]\n        expected_e1 = [b[\'extra_batch\'] for b in ref[1][\'batches\']\n                       if \'extra_batch\' in b]\n\n        # In every epoch there are 2 batches with the \'extra_batch\'\n        # key except for the last epoch. We therefore two results\n        # of which one of them is an empty list.\n        assert len(extra_batches) == self.test_epochs - 1\n        assert extra_batches[0] == expected_e0\n        assert extra_batches[1] == expected_e1\n\n    def test_history_partial_batches_batch_key_4th(self, history, ref):\n        extra_batches = history[:, \'batches\', :, \'extra_batch\']\n\n        expected_e0 = [b[\'extra_batch\'] for b in ref[0][\'batches\']\n                       if \'extra_batch\' in b]\n        expected_e1 = [b[\'extra_batch\'] for b in ref[1][\'batches\']\n                       if \'extra_batch\' in b]\n\n        # In every epoch there are 2 batches with the \'extra_batch\'\n        # key except for the last epoch. We therefore two results\n        # of which one of them is an empty list.\n        assert len(extra_batches) == self.test_epochs - 1\n        assert extra_batches[0] == expected_e0\n        assert extra_batches[1] == expected_e1\n\n    def test_history_partial_singular_values(self, history):\n        values = history[-1, (\'duration\', \'total_loss\')]\n        expected = (history[-1][\'duration\'], history[-1][\'total_loss\'])\n\n        # pylint: disable=unidiomatic-typecheck\n        assert type(values) == tuple\n        assert values == expected\n\n    def test_history_slice_beyond_batches_but_key_not_batches(self, history):\n        with pytest.raises(KeyError) as exc:\n            # pylint: disable=pointless-statement\n            history[:, \'not-batches\', 0]\n\n        msg = exc.value.args[0]\n        expected = (""History indexing beyond the 2nd level is ""\n                    ""only possible if key \'batches\' is used, ""\n                    ""found key \'not-batches\'."")\n        assert msg == expected\n\n    def test_history_with_invalid_epoch_key(self, history):\n        key = slice(None), \'not-batches\'\n        with pytest.raises(KeyError) as exc:\n            # pylint: disable=pointless-statement\n            history[key]\n\n        msg = exc.value.args[0]\n        expected = ""Key \'not-batches\' was not found in history.""\n        assert msg == expected\n\n\n    def test_history_too_many_indices(self, history):\n        with pytest.raises(KeyError) as exc:\n            # pylint: disable=pointless-statement\n            history[:, \'batches\', :, \'train_loss\', :]\n\n        msg = exc.value.args[0]\n        expected = (""Tried to index history with 5 indices but only ""\n                    ""4 indices are possible."")\n        assert msg == expected\n\n    def test_history_save_load_cycle_file_obj(self, history, tmpdir):\n        history_f = tmpdir.mkdir(\'skorch\').join(\'history.json\')\n\n        with open(str(history_f), \'w\') as f:\n            history.to_file(f)\n\n        with open(str(history_f), \'r\') as f:\n            new_history = History.from_file(f)\n\n        assert history == new_history\n\n    def test_history_save_load_cycle_file_path(self, history, tmpdir):\n        history_f = tmpdir.mkdir(\'skorch\').join(\'history.json\')\n\n        history.to_file(str(history_f))\n        new_history = History.from_file(str(history_f))\n\n        assert history == new_history\n'"
skorch/tests/test_net.py,54,"b'""""""Tests for net.py\n\nAlthough NeuralNetClassifier is used in tests, test only functionality\nthat is general to NeuralNet class.\n\n""""""\n\nimport copy\nfrom functools import partial\nimport os\nfrom pathlib import Path\nimport pickle\nfrom unittest.mock import Mock\nfrom unittest.mock import patch\nimport sys\nfrom contextlib import ExitStack\n\nimport numpy as np\nfrom distutils.version import LooseVersion\nimport pytest\nfrom sklearn.base import clone\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport torch\nfrom torch import nn\nfrom flaky import flaky\n\nfrom skorch.tests.conftest import INFERENCE_METHODS\nfrom skorch.utils import flatten\nfrom skorch.utils import to_numpy\nfrom skorch.utils import is_torch_data_type\n\n\nACCURACY_EXPECTED = 0.65\n\n\n# pylint: disable=too-many-public-methods\nclass TestNeuralNet:\n    @pytest.fixture(scope=\'module\')\n    def data(self, classifier_data):\n        return classifier_data\n\n    @pytest.fixture(scope=\'module\')\n    def dummy_callback(self):\n        from skorch.callbacks import Callback\n        cb = Mock(spec=Callback)\n        # make dummy behave like an estimator\n        cb.get_params.return_value = {}\n        cb.set_params = lambda **kwargs: cb\n        return cb\n\n    @pytest.fixture(scope=\'module\')\n    def module_cls(self, classifier_module):\n        return classifier_module\n\n    @pytest.fixture(scope=\'module\')\n    def net_cls(self):\n        from skorch import NeuralNetClassifier\n        return NeuralNetClassifier\n\n    @pytest.fixture\n    def dataset_cls(self):\n        from skorch.dataset import Dataset\n        return Dataset\n\n    @pytest.fixture\n    def checkpoint_cls(self):\n        from skorch.callbacks import Checkpoint\n        return Checkpoint\n\n    @pytest.fixture(scope=\'module\')\n    def net(self, net_cls, module_cls, dummy_callback):\n        return net_cls(\n            module_cls,\n            callbacks=[(\'dummy\', dummy_callback)],\n            max_epochs=10,\n            lr=0.1,\n        )\n\n    @pytest.fixture(scope=\'module\')\n    def pipe(self, net):\n        return Pipeline([\n            (\'scale\', StandardScaler()),\n            (\'net\', net),\n        ])\n\n    @pytest.fixture(scope=\'module\')\n    def net_fit(self, net_cls, module_cls, dummy_callback, data):\n        # Careful, don\'t call additional fits or set_params on this,\n        # since that would have side effects on other tests.\n        X, y = data\n\n        # We need a new instance of the net and cannot reuse the net\n        # fixture, because otherwise fixture net and net_fit refer to\n        # the same object; also, we cannot clone(net) because this\n        # will result in the dummy_callback not being the mock anymore\n        net = net_cls(\n            module_cls,\n            callbacks=[(\'dummy\', dummy_callback)],\n            max_epochs=10,\n            lr=0.1,\n        )\n        return net.fit(X, y)\n\n    @pytest.fixture\n    def net_pickleable(self, net_fit):\n        """"""NeuralNet instance that removes callbacks that are not\n        pickleable.\n\n        """"""\n        # callback fixture not pickleable, remove it\n        callbacks = net_fit.callbacks\n        net_fit.callbacks = []\n        callbacks_ = net_fit.callbacks_\n        # remove mock callback\n        net_fit.callbacks_ = [(n, cb) for n, cb in net_fit.callbacks_\n                              if not isinstance(cb, Mock)]\n        net_clone = copy.deepcopy(net_fit)\n        net_fit.callbacks = callbacks\n        net_fit.callbacks_ = callbacks_\n        return net_clone\n\n    @pytest.mark.parametrize(""copy_method"", [""pickle"", ""copy.deepcopy""])\n    def test_train_net_after_copy(self, net_cls, module_cls, data,\n                                  copy_method):\n        # This test comes from [issue #317], and makes sure that models\n        # can be trained after copying (which is really pickling).\n        #\n        # [issue #317]:https://github.com/skorch-dev/skorch/issues/317\n        X, y = data\n        n1 = net_cls(module_cls)\n        n1.partial_fit(X, y, epochs=1)\n        if copy_method == ""copy.deepcopy"":\n            n2 = copy.deepcopy(n1)\n        elif copy_method == ""pickle"":\n            n2 = pickle.loads(pickle.dumps(n1))\n        else:\n            raise ValueError\n\n        # Test to make sure the parameters got copied correctly\n        close = [torch.allclose(p1, p2)\n                 for p1, p2 in zip(n1.module_.parameters(),\n                                   n2.module_.parameters())]\n        assert all(close)\n\n        # make sure the parameters change\n        # at least two epochs to make sure `train_loss` updates after copy\n        # (this is a check for the bug in #317, where `train_loss` didn\'t\n        # update at all after copy. This covers that case).\n        n2.partial_fit(X, y, epochs=2)\n        far = [not torch.allclose(p1, p2)\n               for p1, p2 in zip(n1.module_.parameters(),\n                                 n2.module_.parameters())]\n        assert all(far)\n\n        # Make sure the model is being trained, and the loss actually changes\n        # (and hopefully decreases, but no test for that)\n        # If copied incorrectly, the optimizer can\'t see the gradients\n        # calculated by loss.backward(), so the loss stays *exactly* the same\n        assert n2.history[-1][\'train_loss\'] != n2.history[-2][\'train_loss\']\n\n        # Make sure the optimizer params and module params point to the same\n        # memory\n        for opt_param, param in zip(\n                n2.module_.parameters(),\n                n2.optimizer_.param_groups[0][\'params\']):\n            assert param is opt_param\n\n    def test_net_init_one_unknown_argument(self, net_cls, module_cls):\n        with pytest.raises(TypeError) as e:\n            net_cls(module_cls, unknown_arg=123)\n\n        expected = (""__init__() got unexpected argument(s) unknown_arg. ""\n                    ""Either you made a typo, or you added new arguments ""\n                    ""in a subclass; if that is the case, the subclass ""\n                    ""should deal with the new arguments explicitly."")\n        assert e.value.args[0] == expected\n\n    def test_net_init_two_unknown_arguments(self, net_cls, module_cls):\n        with pytest.raises(TypeError) as e:\n            net_cls(module_cls, lr=0.1, mxa_epochs=5,\n                    warm_start=False, bathc_size=20)\n\n        expected = (""__init__() got unexpected argument(s) ""\n                    ""bathc_size, mxa_epochs. ""\n                    ""Either you made a typo, or you added new arguments ""\n                    ""in a subclass; if that is the case, the subclass ""\n                    ""should deal with the new arguments explicitly."")\n        assert e.value.args[0] == expected\n\n    @pytest.mark.parametrize(\'name, suggestion\', [\n        (\'iterator_train_shuffle\', \'iterator_train__shuffle\'),\n        (\'optimizer_momentum\', \'optimizer__momentum\'),\n        (\'modulenum_units\', \'module__num_units\'),\n        (\'criterionreduce\', \'criterion__reduce\'),\n        (\'callbacks_mycb__foo\', \'callbacks__mycb__foo\'),\n    ])\n    def test_net_init_missing_dunder_in_prefix_argument(\n            self, net_cls, module_cls, name, suggestion):\n        # forgot to use double-underscore notation\n        with pytest.raises(TypeError) as e:\n            net_cls(module_cls, **{name: 123})\n\n        tmpl = ""Got an unexpected argument {}, did you mean {}?""\n        expected = tmpl.format(name, suggestion)\n        assert e.value.args[0] == expected\n\n    def test_net_init_missing_dunder_in_2_prefix_arguments(\n            self, net_cls, module_cls):\n        # forgot to use double-underscore notation in 2 arguments\n        with pytest.raises(TypeError) as e:\n            net_cls(\n                module_cls,\n                max_epochs=7,  # correct\n                iterator_train_shuffle=True,  # uses _ instead of __\n                optimizerlr=0.5,  # missing __\n            )\n        expected = (""Got an unexpected argument iterator_train_shuffle, ""\n                    ""did you mean iterator_train__shuffle?\\n""\n                    ""Got an unexpected argument optimizerlr, ""\n                    ""did you mean optimizer__lr?"")\n        assert e.value.args[0] == expected\n\n    def test_net_init_missing_dunder_and_unknown(\n            self, net_cls, module_cls):\n        # unknown argument and forgot to use double-underscore notation\n        with pytest.raises(TypeError) as e:\n            net_cls(\n                module_cls,\n                foobar=123,\n                iterator_train_shuffle=True,\n            )\n        expected = (""__init__() got unexpected argument(s) foobar. ""\n                    ""Either you made a typo, or you added new arguments ""\n                    ""in a subclass; if that is the case, the subclass ""\n                    ""should deal with the new arguments explicitly.\\n""\n                    ""Got an unexpected argument iterator_train_shuffle, ""\n                    ""did you mean iterator_train__shuffle?"")\n        assert e.value.args[0] == expected\n\n    def test_net_with_new_attribute_with_name_clash(\n            self, net_cls, module_cls):\n        # This covers a bug that existed when a new ""settable""\n        # argument was added whose name starts the same as the name\n        # for an existing argument\n        class MyNet(net_cls):\n            # add ""optimizer_2"" as a valid prefix so that it works\n            # with set_params\n            prefixes_ = net_cls.prefixes_[:] + [\'optimizer_2\']\n\n            def __init__(self, *args, **kwargs):\n                super().__init__(*args, **kwargs)\n                self.optimizer_2 = torch.optim.SGD\n\n        # the following line used to raise this error: ""TypeError: Got\n        # an unexpected argument optimizer_2__lr, did you mean\n        # optimizer__2__lr?"" because it was erronously assumed that\n        # ""optimizer_2__lr"" should be dispatched to ""optimizer"", not\n        # ""optimizer_2"".\n        MyNet(module_cls, optimizer_2__lr=0.123)  # should not raise\n\n    def test_fit(self, net_fit):\n        # fitting does not raise anything\n        pass\n\n    @pytest.mark.parametrize(\'method\', INFERENCE_METHODS)\n    def test_not_fitted_raises(self, net_cls, module_cls, data, method):\n        from skorch.exceptions import NotInitializedError\n        net = net_cls(module_cls)\n        X = data[0]\n        with pytest.raises(NotInitializedError) as exc:\n            # we call `list` because `forward_iter` is lazy\n            list(getattr(net, method)(X))\n\n        msg = (""This NeuralNetClassifier instance is not initialized yet. ""\n               ""Call \'initialize\' or \'fit\' with appropriate arguments ""\n               ""before using this method."")\n        assert exc.value.args[0] == msg\n\n    def test_not_fitted_other_attributes(self, module_cls):\n        # pass attributes to check for explicitly\n        with patch(\'skorch.net.check_is_fitted\') as check:\n            from skorch import NeuralNetClassifier\n\n            net = NeuralNetClassifier(module_cls)\n            attributes = [\'foo\', \'bar_\']\n\n            net.check_is_fitted(attributes=attributes)\n            args = check.call_args_list[0][0][1]\n            assert args == attributes\n\n    @flaky(max_runs=3)\n    def test_net_learns(self, net_cls, module_cls, data):\n        X, y = data\n        net = net_cls(\n            module_cls,\n            max_epochs=10,\n            lr=0.1,\n        )\n        net.fit(X, y)\n        y_pred = net.predict(X)\n        assert accuracy_score(y, y_pred) > ACCURACY_EXPECTED\n\n    def test_forward(self, net_fit, data):\n        X = data[0]\n        n = len(X)\n        y_forward = net_fit.forward(X)\n\n        assert is_torch_data_type(y_forward)\n        # Expecting (number of samples, number of output units)\n        assert y_forward.shape == (n, 2)\n\n        y_proba = net_fit.predict_proba(X)\n        assert np.allclose(to_numpy(y_forward), y_proba)\n\n    def test_forward_device_cpu(self, net_fit, data):\n        X = data[0]\n\n        # CPU by default\n        y_forward = net_fit.forward(X)\n        assert isinstance(X, np.ndarray)\n        assert not y_forward.is_cuda\n\n        y_forward = net_fit.forward(X, device=\'cpu\')\n        assert isinstance(X, np.ndarray)\n        assert not y_forward.is_cuda\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=""no cuda device"")\n    def test_forward_device_gpu(self, net_fit, data):\n        X = data[0]\n        y_forward = net_fit.forward(X, device=\'cuda:0\')\n        assert isinstance(X, np.ndarray)\n        assert y_forward.is_cuda\n\n    def test_dropout(self, net_fit, data):\n        # Note: does not test that dropout is really active during\n        # training.\n        X = data[0]\n\n        # check that dropout not active by default\n        y_proba = to_numpy(net_fit.forward(X))\n        y_proba2 = to_numpy(net_fit.forward(X))\n        assert np.allclose(y_proba, y_proba2, rtol=1e-7)\n\n        # check that dropout can be activated\n        y_proba = to_numpy(net_fit.forward(X, training=True))\n        y_proba2 = to_numpy(net_fit.forward(X, training=True))\n        assert not np.allclose(y_proba, y_proba2, rtol=1e-7)\n\n    def test_pickle_save_load(self, net_pickleable, data, tmpdir):\n        X, y = data\n        score_before = accuracy_score(y, net_pickleable.predict(X))\n\n        p = tmpdir.mkdir(\'skorch\').join(\'testmodel.pkl\')\n        with open(str(p), \'wb\') as f:\n            pickle.dump(net_pickleable, f)\n        del net_pickleable\n        with open(str(p), \'rb\') as f:\n            net_new = pickle.load(f)\n\n        score_after = accuracy_score(y, net_new.predict(X))\n        assert np.isclose(score_after, score_before)\n\n    def train_picklable_cuda_net(self, net_pickleable, data):\n        X, y = data\n        w = torch.FloatTensor([1.] * int(y.max() + 1)).to(\'cuda\')\n\n        # Use stateful optimizer (CUDA variables in state) and\n        # a CUDA parametrized criterion along with a CUDA net.\n        net_pickleable.set_params(\n            device=\'cuda\',\n            criterion__weight=w,\n            optimizer=torch.optim.Adam,\n        )\n        net_pickleable.fit(X, y)\n\n        return net_pickleable\n\n    @pytest.fixture\n    def pickled_cuda_net_path(self, net_pickleable, data):\n        path = os.path.join(\'skorch\', \'tests\', \'net_cuda.pkl\')\n\n        # Assume that a previous run on a CUDA-capable device\n        # created `net_cuda.pkl`.\n        if not torch.cuda.is_available():\n            assert os.path.exists(path)\n            return path\n\n        net_pickleable = self.train_picklable_cuda_net(net_pickleable, data)\n\n        with open(path, \'wb\') as f:\n            pickle.dump(net_pickleable, f)\n        return path\n\n    @pytest.mark.parametrize(\'cuda_available\', {False, torch.cuda.is_available()})\n    def test_pickle_load(self, cuda_available, pickled_cuda_net_path):\n        with patch(\'torch.cuda.is_available\', lambda *_: cuda_available):\n            with open(pickled_cuda_net_path, \'rb\') as f:\n                pickle.load(f)\n\n    @pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\n    def test_device_torch_device(self, net_cls, module_cls, device):\n        # Check if native torch.device works as well.\n        if device.startswith(\'cuda\') and not torch.cuda.is_available():\n            pytest.skip()\n        net = net_cls(module=module_cls, device=torch.device(device))\n        net = net.initialize()\n        assert net.module_.sequential[0].weight.device.type.startswith(device)\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=""no cuda device"")\n    @pytest.mark.parametrize(\n        \'save_dev, cuda_available, load_dev, expect_warning\',\n        [\n            (\'cuda\', False, \'cpu\', True),\n            (\'cuda\', True, \'cuda\', False),\n            (\'cpu\', True, \'cpu\', False),\n            (\'cpu\', False, \'cpu\', False),\n        ])\n    def test_pickle_save_and_load_mixed_devices(\n            self,\n            net_cls,\n            module_cls,\n            tmpdir,\n            save_dev,\n            cuda_available,\n            load_dev,\n            expect_warning,\n    ):\n        from skorch.exceptions import DeviceWarning\n        net = net_cls(module=module_cls, device=save_dev).initialize()\n\n        p = tmpdir.mkdir(\'skorch\').join(\'testmodel.pkl\')\n        with open(str(p), \'wb\') as f:\n            pickle.dump(net, f)\n        del net\n\n        with patch(\'torch.cuda.is_available\', lambda *_: cuda_available):\n            with open(str(p), \'rb\') as f:\n                expected_warning = DeviceWarning if expect_warning else None\n                with pytest.warns(expected_warning) as w:\n                    m = pickle.load(f)\n\n        assert torch.device(m.device) == torch.device(load_dev)\n\n        if expect_warning:\n            # We should have captured two warnings:\n            # 1. one for the failed load\n            # 2. for switching devices on the net instance\n            assert len(w.list) == 2\n            assert w.list[0].message.args[0] == (\n                \'Requested to load data to CUDA but no CUDA devices \'\n                \'are available. Loading on device ""cpu"" instead.\')\n            assert w.list[1].message.args[0] == (\n                \'Setting self.device = {} since the requested device ({}) \'\n                \'is not available.\'.format(load_dev, save_dev))\n\n    def test_pickle_save_and_load_uninitialized(\n            self, net_cls, module_cls, tmpdir):\n        net = net_cls(module_cls)\n        p = tmpdir.mkdir(\'skorch\').join(\'testmodel.pkl\')\n        with open(str(p), \'wb\') as f:\n            # does not raise\n            pickle.dump(net, f)\n        with open(str(p), \'rb\') as f:\n            pickle.load(f)\n\n    def test_save_load_state_dict_file(\n            self, net_cls, module_cls, net_fit, data, tmpdir):\n        net = net_cls(module_cls).initialize()\n        X, y = data\n\n        score_before = accuracy_score(y, net_fit.predict(X))\n        score_untrained = accuracy_score(y, net.predict(X))\n        assert not np.isclose(score_before, score_untrained)\n\n        p = tmpdir.mkdir(\'skorch\').join(\'testmodel.pkl\')\n        with open(str(p), \'wb\') as f:\n            net_fit.save_params(f_params=f)\n        del net_fit\n        with open(str(p), \'rb\') as f:\n            net.load_params(f_params=f)\n\n        score_after = accuracy_score(y, net.predict(X))\n        assert np.isclose(score_after, score_before)\n\n    def test_save_load_state_dict_str(\n            self, net_cls, module_cls, net_fit, data, tmpdir):\n        net = net_cls(module_cls).initialize()\n        X, y = data\n\n        score_before = accuracy_score(y, net_fit.predict(X))\n        score_untrained = accuracy_score(y, net.predict(X))\n        assert not np.isclose(score_before, score_untrained)\n\n        p = tmpdir.mkdir(\'skorch\').join(\'testmodel.pkl\')\n        net_fit.save_params(f_params=str(p))\n        del net_fit\n        net.load_params(f_params=str(p))\n\n        score_after = accuracy_score(y, net.predict(X))\n        assert np.isclose(score_after, score_before)\n\n    @pytest.fixture(scope=\'module\')\n    def net_fit_adam(self, net_cls, module_cls, data):\n        net = net_cls(\n            module_cls, max_epochs=2, lr=0.1,\n            optimizer=torch.optim.Adam)\n        net.fit(*data)\n        return net\n\n    def test_save_load_state_dict_file_with_history_optimizer(\n            self, net_cls, module_cls, net_fit_adam, tmpdir):\n\n        skorch_tmpdir = tmpdir.mkdir(\'skorch\')\n        p = skorch_tmpdir.join(\'testmodel.pkl\')\n        o = skorch_tmpdir.join(\'optimizer.pkl\')\n        h = skorch_tmpdir.join(\'history.json\')\n\n        with ExitStack() as stack:\n            p_fp = stack.enter_context(open(str(p), \'wb\'))\n            o_fp = stack.enter_context(open(str(o), \'wb\'))\n            h_fp = stack.enter_context(open(str(h), \'w\'))\n            net_fit_adam.save_params(\n                f_params=p_fp, f_optimizer=o_fp, f_history=h_fp)\n\n            # \'step\' is state from the Adam optimizer\n            orig_steps = [v[\'step\'] for v in\n                          net_fit_adam.optimizer_.state_dict()[\'state\'].values()]\n            orig_loss = np.array(net_fit_adam.history[:, \'train_loss\'])\n            del net_fit_adam\n\n        with ExitStack() as stack:\n            p_fp = stack.enter_context(open(str(p), \'rb\'))\n            o_fp = stack.enter_context(open(str(o), \'rb\'))\n            h_fp = stack.enter_context(open(str(h), \'r\'))\n            new_net = net_cls(\n                module_cls, optimizer=torch.optim.Adam).initialize()\n            new_net.load_params(\n                f_params=p_fp, f_optimizer=o_fp, f_history=h_fp)\n\n            new_steps = [v[\'step\'] for v in\n                         new_net.optimizer_.state_dict()[\'state\'].values()]\n            new_loss = np.array(new_net.history[:, \'train_loss\'])\n\n            assert np.allclose(orig_loss, new_loss)\n            assert orig_steps == new_steps\n\n    def test_save_load_state_dict_str_with_history_optimizer(\n            self, net_cls, module_cls, net_fit_adam, tmpdir):\n\n        skorch_tmpdir = tmpdir.mkdir(\'skorch\')\n        p = str(skorch_tmpdir.join(\'testmodel.pkl\'))\n        o = str(skorch_tmpdir.join(\'optimizer.pkl\'))\n        h = str(skorch_tmpdir.join(\'history.json\'))\n\n        net_fit_adam.save_params(f_params=p, f_optimizer=o, f_history=h)\n\n        # \'step\' is state from the Adam optimizer\n        orig_steps = [v[\'step\'] for v in\n                      net_fit_adam.optimizer_.state_dict()[\'state\'].values()]\n        orig_loss = np.array(net_fit_adam.history[:, \'train_loss\'])\n        del net_fit_adam\n\n        new_net = net_cls(\n            module_cls, optimizer=torch.optim.Adam).initialize()\n        new_net.load_params(f_params=p, f_optimizer=o, f_history=h)\n\n        new_steps = [v[\'step\'] for v in\n                     new_net.optimizer_.state_dict()[\'state\'].values()]\n        new_loss = np.array(new_net.history[:, \'train_loss\'])\n\n        assert np.allclose(orig_loss, new_loss)\n        assert orig_steps == new_steps\n\n    @pytest.mark.parametrize(""explicit_init"", [True, False])\n    def test_save_and_load_from_checkpoint(\n            self, net_cls, module_cls, data, checkpoint_cls, tmpdir,\n            explicit_init):\n\n        skorch_dir = tmpdir.mkdir(\'skorch\')\n        f_params = skorch_dir.join(\'params.pt\')\n        f_optimizer = skorch_dir.join(\'optimizer.pt\')\n        f_history = skorch_dir.join(\'history.json\')\n\n        cp = checkpoint_cls(\n            monitor=None,\n            f_params=str(f_params),\n            f_optimizer=str(f_optimizer),\n            f_history=str(f_history))\n        net = net_cls(\n            module_cls, max_epochs=4, lr=0.1,\n            optimizer=torch.optim.Adam, callbacks=[cp])\n        net.fit(*data)\n        del net\n\n        assert f_params.exists()\n        assert f_optimizer.exists()\n        assert f_history.exists()\n\n        new_net = net_cls(\n            module_cls, max_epochs=4, lr=0.1,\n            optimizer=torch.optim.Adam, callbacks=[cp])\n        if explicit_init:\n            new_net.initialize()\n        new_net.load_params(checkpoint=cp)\n\n        assert len(new_net.history) == 4\n\n        new_net.partial_fit(*data)\n\n        # fit ran twice for a total of 8 epochs\n        assert len(new_net.history) == 8\n\n    def test_checkpoint_with_prefix_and_dirname(\n            self, net_cls, module_cls, data, checkpoint_cls, tmpdir):\n        exp_dir = tmpdir.mkdir(\'skorch\')\n        exp_basedir = exp_dir.join(\'exp1\')\n\n        cp = checkpoint_cls(\n            monitor=None, fn_prefix=\'unet_\', dirname=str(exp_basedir))\n        net = net_cls(\n            module_cls, max_epochs=4, lr=0.1,\n            optimizer=torch.optim.Adam, callbacks=[cp])\n        net.fit(*data)\n\n        assert exp_basedir.join(\'unet_params.pt\').exists()\n        assert exp_basedir.join(\'unet_optimizer.pt\').exists()\n        assert exp_basedir.join(\'unet_history.json\').exists()\n\n    def test_save_and_load_from_checkpoint_formatting(\n            self, net_cls, module_cls, data, checkpoint_cls, tmpdir):\n\n        def epoch_3_scorer(net, *_):\n            return 1 if net.history[-1, \'epoch\'] == 3 else 0\n\n        from skorch.callbacks import EpochScoring\n        scoring = EpochScoring(\n            scoring=epoch_3_scorer, on_train=True)\n\n        skorch_dir = tmpdir.mkdir(\'skorch\')\n        f_params = skorch_dir.join(\n            \'model_epoch_{last_epoch[epoch]}.pt\')\n        f_optimizer = skorch_dir.join(\n            \'optimizer_epoch_{last_epoch[epoch]}.pt\')\n        f_history = skorch_dir.join(\n            \'history.json\')\n\n        cp = checkpoint_cls(\n            monitor=\'epoch_3_scorer\',\n            f_params=str(f_params),\n            f_optimizer=str(f_optimizer),\n            f_history=str(f_history))\n\n        net = net_cls(\n            module_cls, max_epochs=5, lr=0.1,\n            optimizer=torch.optim.Adam, callbacks=[\n                (\'my_score\', scoring), cp\n            ])\n        net.fit(*data)\n        del net\n\n        assert skorch_dir.join(\'model_epoch_3.pt\').exists()\n        assert skorch_dir.join(\'optimizer_epoch_3.pt\').exists()\n        assert skorch_dir.join(\'history.json\').exists()\n\n        new_net = net_cls(\n            module_cls, max_epochs=5, lr=0.1,\n            optimizer=torch.optim.Adam, callbacks=[\n                (\'my_score\', scoring), cp\n            ])\n        new_net.load_params(checkpoint=cp)\n\n        # original run saved checkpoint at epoch 3\n        assert len(new_net.history) == 3\n\n        new_net.partial_fit(*data)\n\n        # training continued from the best epoch of the first run,\n        # the best epoch in the first run happened at epoch 3,\n        # the second ran for 5 epochs, so the final history of the new\n        # net is 3+5 = 7\n        assert len(new_net.history) == 8\n        assert new_net.history[:, \'event_cp\'] == [\n            False, False, True, False, False, False, False, False]\n\n    def test_save_params_not_init_optimizer(\n            self, net_cls, module_cls, tmpdir):\n        from skorch.exceptions import NotInitializedError\n\n        net = net_cls(module_cls).initialize_module()\n        skorch_tmpdir = tmpdir.mkdir(\'skorch\')\n        p = skorch_tmpdir.join(\'testmodel.pkl\')\n        o = skorch_tmpdir.join(\'optimizer.pkl\')\n\n        with pytest.raises(NotInitializedError) as exc:\n            net.save_params(f_params=str(p), f_optimizer=o)\n        expected = (""Cannot save state of an un-initialized optimizer. ""\n                    ""Please initialize first by calling .initialize() ""\n                    ""or by fitting the model with .fit(...)."")\n        assert exc.value.args[0] == expected\n\n    def test_load_params_not_init_optimizer(\n            self, net_cls, module_cls, tmpdir):\n        from skorch.exceptions import NotInitializedError\n\n        net = net_cls(module_cls).initialize_module()\n        skorch_tmpdir = tmpdir.mkdir(\'skorch\')\n        p = skorch_tmpdir.join(\'testmodel.pkl\')\n        o = skorch_tmpdir.join(\'optimizer.pkl\')\n\n        net.save_params(f_params=str(p))\n\n        with pytest.raises(NotInitializedError) as exc:\n            net.load_params(f_params=str(p), f_optimizer=o)\n        expected = (""Cannot load state of an un-initialized optimizer. ""\n                    ""Please initialize first by calling .initialize() ""\n                    ""or by fitting the model with .fit(...)."")\n        assert exc.value.args[0] == expected\n\n    def test_save_state_dict_not_init(\n            self, net_cls, module_cls, tmpdir):\n        from skorch.exceptions import NotInitializedError\n\n        net = net_cls(module_cls)\n        p = tmpdir.mkdir(\'skorch\').join(\'testmodel.pkl\')\n\n        with pytest.raises(NotInitializedError) as exc:\n            net.save_params(f_params=str(p))\n        expected = (""Cannot save parameters of an un-initialized model. ""\n                    ""Please initialize first by calling .initialize() ""\n                    ""or by fitting the model with .fit(...)."")\n        assert exc.value.args[0] == expected\n\n    def test_load_state_dict_not_init(\n            self, net_cls, module_cls, tmpdir):\n        from skorch.exceptions import NotInitializedError\n\n        net = net_cls(module_cls)\n        p = tmpdir.mkdir(\'skorch\').join(\'testmodel.pkl\')\n\n        with pytest.raises(NotInitializedError) as exc:\n            net.load_params(f_params=str(p))\n        expected = (""Cannot load parameters of an un-initialized model. ""\n                    ""Please initialize first by calling .initialize() ""\n                    ""or by fitting the model with .fit(...)."")\n        assert exc.value.args[0] == expected\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=""no cuda device"")\n    def test_save_load_state_cuda_intercompatibility(\n            self, net_cls, module_cls, tmpdir):\n        from skorch.exceptions import DeviceWarning\n        net = net_cls(module_cls, device=\'cuda\').initialize()\n\n        p = tmpdir.mkdir(\'skorch\').join(\'testmodel.pkl\')\n        net.save_params(f_params=str(p))\n\n        with patch(\'torch.cuda.is_available\', lambda *_: False):\n            with pytest.warns(DeviceWarning) as w:\n                net.load_params(f_params=str(p))\n\n        assert w.list[0].message.args[0] == (\n            \'Requested to load data to CUDA but no CUDA devices \'\n            \'are available. Loading on device ""cpu"" instead.\')\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=""no cuda device"")\n    def test_save_params_cuda_load_params_cpu_when_cuda_available(\n            self, net_cls, module_cls, data, tmpdir):\n        # Test that if we have a cuda device, we can save cuda\n        # parameters and then load them to cpu\n        X, y = data\n        net = net_cls(module_cls, device=\'cuda\', max_epochs=1).fit(X, y)\n        p = tmpdir.mkdir(\'skorch\').join(\'testmodel.pkl\')\n        net.save_params(f_params=str(p))\n\n        net2 = net_cls(module_cls, device=\'cpu\').initialize()\n        net2.load_params(f_params=str(p))\n        net2.predict(X)  # does not raise\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=""no cuda device"")\n    @pytest.mark.parametrize(\'parameter,name\', [\n        (\'f_params\', \'net_cuda.pt\'),\n        (\'f_optimizer\', \'optimizer_cuda.pt\'),\n    ])\n    def test_load_cuda_params_to_cuda(\n            self, parameter, name, net_cls, module_cls, data):\n        net = net_cls(module_cls, device=\'cuda\').initialize()\n        # object was trained with CUDA\n        kwargs = {parameter: os.path.join(\'skorch\', \'tests\', name)}\n        net.load_params(**kwargs)\n        net.predict(data[0])  # does not raise\n\n    @pytest.mark.parametrize(\'parameter,name\', [\n        (\'f_params\', \'net_cuda.pt\'),\n        (\'f_optimizer\', \'optimizer_cuda.pt\'),\n    ])\n    def test_load_cuda_params_to_cpu(\n            self, parameter, name, net_cls, module_cls, data):\n        # Note: This test will pass trivially when CUDA is available\n        # but triggered a bug when CUDA is not available.\n        net = net_cls(module_cls).initialize()\n        # object was trained with CUDA\n        kwargs = {parameter: os.path.join(\'skorch\', \'tests\', name)}\n        net.load_params(**kwargs)\n        net.predict(data[0])  # does not raise\n\n    def test_save_params_with_history_file_obj(\n            self, net_cls, module_cls, net_fit, tmpdir):\n        net = net_cls(module_cls).initialize()\n\n        history_before = net_fit.history\n\n        p = tmpdir.mkdir(\'skorch\').join(\'history.json\')\n        with open(str(p), \'w\') as f:\n            net_fit.save_params(f_history=f)\n        del net_fit\n        with open(str(p), \'r\') as f:\n            net.load_params(f_history=f)\n\n        assert net.history == history_before\n\n    @pytest.mark.parametrize(\'converter\', [str, Path])\n    def test_save_params_with_history_file_path(\n            self, net_cls, module_cls, net_fit, tmpdir, converter):\n        # Test loading/saving with different kinds of path representations.\n\n        if converter is Path and sys.version < \'3.6\':\n            # `PosixPath` cannot be `open`ed in Python < 3.6\n            pytest.skip()\n\n        net = net_cls(module_cls).initialize()\n\n        history_before = net_fit.history\n\n        p = tmpdir.mkdir(\'skorch\').join(\'history.json\')\n        net_fit.save_params(f_history=converter(p))\n        del net_fit\n        net.load_params(f_history=converter(p))\n\n        assert net.history == history_before\n\n    @pytest.mark.parametrize(\'method, call_count\', [\n        (\'on_train_begin\', 1),\n        (\'on_train_end\', 1),\n        (\'on_epoch_begin\', 10),\n        (\'on_epoch_end\', 10),\n        # by default: 80/20 train/valid split\n        (\'on_batch_begin\', (800 // 128 + 1) * 10 + (200 // 128 + 1) * 10),\n        (\'on_batch_end\', (800 // 128 + 1) * 10 + (200 // 128 + 1) * 10),\n    ])\n    def test_callback_is_called(self, net_fit, method, call_count):\n        # callback -2 is the mocked callback\n        method = getattr(net_fit.callbacks_[-2][1], method)\n        assert method.call_count == call_count\n        assert method.call_args_list[0][0][0] is net_fit\n\n    def test_history_correct_shape(self, net_fit):\n        assert len(net_fit.history) == net_fit.max_epochs\n\n    def test_history_default_keys(self, net_fit):\n        expected_keys = {\n            \'train_loss\', \'valid_loss\', \'epoch\', \'dur\', \'batches\', \'valid_acc\'}\n        for row in net_fit.history:\n            assert expected_keys.issubset(row)\n\n    def test_history_is_filled(self, net_fit):\n        assert len(net_fit.history) == net_fit.max_epochs\n\n    def test_set_params_works(self, net, data):\n        X, y = data\n        net.fit(X, y)\n\n        assert net.module_.sequential[0].out_features == 10\n        assert isinstance(net.module_.sequential[1], nn.ReLU)\n        assert net.module_.sequential[3].in_features == 10\n        assert np.isclose(net.lr, 0.1)\n\n        net.set_params(\n            module__hidden_units=20,\n            module__nonlin=nn.Tanh(),\n            lr=0.2,\n        )\n        net.fit(X, y)\n\n        assert net.module_.sequential[0].out_features == 20\n        assert isinstance(net.module_.sequential[1], nn.Tanh)\n        assert net.module_.sequential[3].in_features == 20\n        assert np.isclose(net.lr, 0.2)\n\n    def test_set_params_then_initialize_remembers_param(\n            self, net_cls, module_cls):\n        net = net_cls(module_cls)\n\n        # net does not \'forget\' that params were set\n        assert net.verbose != 123\n        net.set_params(verbose=123)\n        assert net.verbose == 123\n        net.initialize()\n        assert net.verbose == 123\n\n    def test_set_params_on_callback_then_initialize_remembers_param(\n            self, net_cls, module_cls):\n        net = net_cls(module_cls).initialize()\n\n        # net does not \'forget\' that params were set\n        assert dict(net.callbacks_)[\'print_log\'].sink is print\n        net.set_params(callbacks__print_log__sink=123)\n        assert dict(net.callbacks_)[\'print_log\'].sink == 123\n        net.initialize()\n        assert dict(net.callbacks_)[\'print_log\'].sink == 123\n\n    def test_changing_model_reinitializes_optimizer(self, net, data):\n        # The idea is that we change the model using `set_params` to\n        # add parameters. Since the optimizer depends on the model\n        # parameters it needs to be reinitialized.\n        X, y = data\n\n        net.set_params(module__nonlin=nn.ReLU())\n        net.fit(X, y)\n\n        net.set_params(module__nonlin=nn.PReLU())\n        assert isinstance(net.module_.nonlin, nn.PReLU)\n        d1 = net.module_.nonlin.weight.data.clone().cpu().numpy()\n\n        # make sure that we do not initialize again by making sure that\n        # the network is initialized and by using partial_fit.\n        assert net.initialized_\n        net.partial_fit(X, y)\n        d2 = net.module_.nonlin.weight.data.clone().cpu().numpy()\n\n        # all newly introduced parameters should have been trained (changed)\n        # by the optimizer after 10 epochs.\n        assert (abs(d2 - d1) > 1e-05).all()\n\n    def test_setting_optimizer_needs_model(self, net_cls, module_cls):\n        net = net_cls(module_cls)\n        assert not hasattr(net, \'module_\')\n        # should not break\n        net.set_params(optimizer=torch.optim.SGD)\n\n    def test_setting_lr_after_init_reflected_in_optimizer(\n            self, net_cls, module_cls):\n        # Fixes a bug that occurred when using set_params(lr=new_lr)\n        # after initialization: The new lr was not reflected in the\n        # optimizer.\n        net = net_cls(module_cls).initialize()\n        net.set_params(lr=10)\n        assert net.lr == 10\n\n        pg_lrs = [pg[\'lr\'] for pg in net.optimizer_.param_groups]\n        for pg_lr in pg_lrs:\n            assert pg_lr == 10\n\n    @pytest.mark.parametrize(\'kwargs,expected\', [\n        ({}, """"),\n        (\n            # virtual params should prevent re-initialization\n            {\'optimizer__lr\': 0.12, \'optimizer__momentum\': 0.34},\n            ("""")\n        ),\n        (\n            {\'module__input_units\': 12, \'module__hidden_units\': 34},\n            (""Re-initializing module because the following ""\n             ""parameters were re-set: hidden_units, input_units.\\n""\n             ""Re-initializing optimizer."")\n        ),\n        (\n            {\'module__input_units\': 12, \'module__hidden_units\': 34,\n             \'optimizer__momentum\': 0.56},\n            (""Re-initializing module because the following ""\n             ""parameters were re-set: hidden_units, input_units.\\n""\n             ""Re-initializing optimizer."")\n        ),\n    ])\n    def test_reinitializing_module_optimizer_message(\n            self, net_cls, module_cls, kwargs, expected, capsys):\n        # When net is initialized, if module or optimizer need to be\n        # re-initialized, alert the user to the fact what parameters\n        # were responsible for re-initialization. Note that when the\n        # module parameters but not optimizer parameters were changed,\n        # the optimizer is re-initialized but not because the\n        # optimizer parameters changed.\n        net = net_cls(module_cls).initialize()\n        net.set_params(**kwargs)\n        msg = capsys.readouterr()[0].strip()\n        assert msg == expected\n\n    @pytest.mark.parametrize(\'kwargs\', [\n        {},\n        {\'module__input_units\': 12, \'module__hidden_units\': 34},\n        {\'lr\': 0.12},\n        {\'optimizer__lr\': 0.12},\n        {\'module__input_units\': 12, \'lr\': 0.56},\n    ])\n    def test_reinitializing_module_optimizer_no_message(\n            self, net_cls, module_cls, kwargs, capsys):\n        # When net is *not* initialized, set_params on module or\n        # optimizer should not trigger a message.\n        net = net_cls(module_cls)\n        net.set_params(**kwargs)\n        msg = capsys.readouterr()[0].strip()\n        assert msg == """"\n\n    def test_optimizer_param_groups(self, net_cls, module_cls):\n        net = net_cls(\n            module_cls,\n            optimizer__param_groups=[\n                (\'sequential.0.*\', {\'lr\': 0.1}),\n                (\'sequential.3.*\', {\'lr\': 0.5}),\n            ],\n        )\n        net.initialize()\n\n        # two custom (1st linear, 2nd linear), one default with the\n        # rest of the parameters (output).\n        assert len(net.optimizer_.param_groups) == 3\n        assert net.optimizer_.param_groups[0][\'lr\'] == 0.1\n        assert net.optimizer_.param_groups[1][\'lr\'] == 0.5\n        assert net.optimizer_.param_groups[2][\'lr\'] == net.lr\n\n    def test_module_params_in_init(self, net_cls, module_cls, data):\n        X, y = data\n\n        net = net_cls(\n            module=module_cls,\n            module__hidden_units=20,\n            module__nonlin=nn.Tanh(),\n        )\n        net.fit(X, y)\n\n        assert net.module_.sequential[0].out_features == 20\n        assert net.module_.sequential[3].in_features == 20\n        assert isinstance(net.module_.sequential[1], nn.Tanh)\n\n    def test_module_initialized_with_partial_module(self, net_cls, module_cls):\n        net = net_cls(partial(module_cls, hidden_units=123))\n        net.initialize()\n        assert net.module_.sequential[0].out_features == 123\n\n    def test_criterion_init_with_params(self, net_cls, module_cls):\n        mock = Mock()\n        net = net_cls(module_cls, criterion=mock, criterion__spam=\'eggs\')\n        net.initialize()\n        assert mock.call_count == 1\n        assert mock.call_args_list[0][1][\'spam\'] == \'eggs\'\n\n    def test_criterion_set_params(self, net_cls, module_cls):\n        mock = Mock()\n        net = net_cls(module_cls, criterion=mock)\n        net.initialize()\n        net.set_params(criterion__spam=\'eggs\')\n        assert mock.call_count == 2\n        assert mock.call_args_list[1][1][\'spam\'] == \'eggs\'\n\n    def test_criterion_non_module(self, net_cls, module_cls, data):\n        # test non-nn.Module classes passed as criterion\n        class SimpleCriterion:\n            def __call__(self, y_pred, y_true):\n                return y_pred.mean()\n\n        net = net_cls(module_cls, criterion=SimpleCriterion)\n        net.initialize()\n        net.fit(*data)\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=""no cuda device"")\n    @pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\n    def test_criterion_params_on_device(self, net_cls, module_cls, device):\n        # attributes like criterion.weight should be automatically moved\n        # to the Net\'s device.\n        criterion = torch.nn.NLLLoss\n        weight = torch.ones(2)\n        net = net_cls(\n            module_cls,\n            criterion=criterion,\n            criterion__weight=weight,\n            device=device,\n        )\n\n        assert weight.device.type == \'cpu\'\n        net.initialize()\n        assert net.criterion_.weight.device.type == device\n\n    def test_callback_with_name_init_with_params(self, net_cls, module_cls):\n        mock = Mock()\n        net = net_cls(\n            module_cls,\n            criterion=Mock(),\n            callbacks=[(\'cb0\', mock)],\n            callbacks__cb0__spam=\'eggs\',\n        )\n        net.initialize()\n        assert mock.initialize.call_count == 1\n        assert mock.set_params.call_args_list[0][1][\'spam\'] == \'eggs\'\n\n    def test_callback_set_params(self, net_cls, module_cls):\n        mock = Mock()\n        net = net_cls(\n            module_cls,\n            criterion=Mock(),\n            callbacks=[(\'cb0\', mock)],\n        )\n        net.initialize()\n        net.set_params(callbacks__cb0__spam=\'eggs\')\n        assert mock.initialize.call_count == 2  # callbacks are re-initialized\n        assert mock.set_params.call_args_list[-1][1][\'spam\'] == \'eggs\'\n\n    def test_callback_name_collides_with_default(self, net_cls, module_cls):\n        net = net_cls(module_cls, callbacks=[(\'train_loss\', Mock())])\n        with pytest.raises(ValueError) as exc:\n            net.initialize()\n        expected = (""Found duplicate user-set callback name \'train_loss\'. ""\n                    ""Use unique names to correct this."")\n        assert str(exc.value) == expected\n\n    def test_callback_same_inferred_name_twice(self, net_cls, module_cls):\n        cb0 = Mock()\n        cb1 = Mock()\n        cb0.__class__.__name__ = \'some-name\'\n        cb1.__class__.__name__ = \'some-name\'\n        net = net_cls(module_cls, callbacks=[cb0, cb1])\n\n        net.initialize()\n\n        cbs = dict(net.callbacks_)\n        assert \'some-name_1\' in cbs\n        assert \'some-name_2\' in cbs\n        assert cbs[\'some-name_1\'] is cb0\n        assert cbs[\'some-name_2\'] is cb1\n\n    def test_callback_keeps_order(self, net_cls, module_cls):\n        cb0 = Mock()\n        cb1 = Mock()\n        cb0.__class__.__name__ = \'B-some-name\'\n        cb1.__class__.__name__ = \'A-some-name\'\n        net = net_cls(module_cls, callbacks=[cb0, cb1])\n\n        net.initialize()\n\n        cbs_names = [name for name, _ in net.callbacks_]\n        expected_names = [\'epoch_timer\', \'train_loss\', \'valid_loss\',\n                          \'valid_acc\', \'B-some-name\', \'A-some-name\',\n                          \'print_log\']\n        assert expected_names == cbs_names\n\n    def test_callback_custom_name_is_untouched(self, net_cls, module_cls):\n        callbacks = [(\'cb0\', Mock()),\n                     (\'cb0\', Mock())]\n        net = net_cls(module_cls, callbacks=callbacks)\n\n        with pytest.raises(ValueError) as exc:\n            net.initialize()\n        expected = (""Found duplicate user-set callback name \'cb0\'. ""\n                    ""Use unique names to correct this."")\n        assert str(exc.value) == expected\n\n    def test_callback_unique_naming_avoids_conflicts(\n            self, net_cls, module_cls):\n        # pylint: disable=invalid-name\n        from skorch.callbacks import Callback\n\n        class cb0(Callback):\n            pass\n\n        class cb0_1(Callback):\n            pass\n\n        callbacks = [cb0(), cb0(), cb0_1()]\n        net = net_cls(module_cls, callbacks=callbacks)\n        with pytest.raises(ValueError) as exc:\n            net.initialize()\n        expected = (""Assigning new callback name failed ""\n                    ""since new name \'cb0_1\' exists already."")\n\n        assert str(exc.value) == expected\n\n    def test_in_sklearn_pipeline(self, pipe, data):\n        X, y = data\n        pipe.fit(X, y)\n        pipe.predict(X)\n        pipe.predict_proba(X)\n        pipe.set_params(net__module__hidden_units=20)\n\n    def test_grid_search_works(self, net_cls, module_cls, data):\n        net = net_cls(module_cls)\n        X, y = data\n        params = {\n            \'lr\': [0.01, 0.02],\n            \'max_epochs\': [10, 20],\n            \'module__hidden_units\': [10, 20],\n        }\n        gs = GridSearchCV(net, params, refit=True, cv=3, scoring=\'accuracy\',\n                          iid=True)\n        gs.fit(X[:100], y[:100])  # for speed\n        print(gs.best_score_, gs.best_params_)\n\n    def test_change_get_loss(self, net_cls, module_cls, data):\n        from skorch.utils import to_tensor\n\n        class MyNet(net_cls):\n            # pylint: disable=unused-argument\n            def get_loss(self, y_pred, y_true, X=None, training=False):\n                y_true = to_tensor(y_true, device=\'cpu\')\n                loss_a = torch.abs(y_true.float() - y_pred[:, 1]).mean()\n                loss_b = ((y_true.float() - y_pred[:, 1]) ** 2).mean()\n                if training:\n                    self.history.record_batch(\'loss_a\', to_numpy(loss_a))\n                    self.history.record_batch(\'loss_b\', to_numpy(loss_b))\n                return loss_a + loss_b\n\n        X, y = data\n        net = MyNet(module_cls, max_epochs=1)\n        net.fit(X, y)\n\n        diffs = []\n        all_losses = net.history[\n            -1, \'batches\', :, (\'train_loss\', \'loss_a\', \'loss_b\')]\n        diffs = [total - a - b for total, a, b in all_losses]\n        assert np.allclose(diffs, 0, atol=1e-7)\n\n    def test_net_no_valid(self, net_cls, module_cls, data):\n        net = net_cls(\n            module_cls,\n            max_epochs=10,\n            lr=0.1,\n            train_split=None,\n        )\n        X, y = data\n        net.fit(X, y)\n        assert net.history[:, \'train_loss\']\n        with pytest.raises(KeyError):\n            # pylint: disable=pointless-statement\n            net.history[:, \'valid_loss\']\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=""no cuda device"")\n    def test_use_cuda_on_model(self, net_cls, module_cls):\n        net_cuda = net_cls(module_cls, device=\'cuda\')\n        net_cuda.initialize()\n        net_cpu = net_cls(module_cls, device=\'cpu\')\n        net_cpu.initialize()\n\n        cpu_tensor = net_cpu.module_.sequential[0].weight.data\n        assert isinstance(cpu_tensor, torch.FloatTensor)\n\n        gpu_tensor = net_cuda.module_.sequential[0].weight.data\n        assert isinstance(gpu_tensor, torch.cuda.FloatTensor)\n\n    def test_get_params_works(self, net_cls, module_cls):\n        from skorch.callbacks import EpochScoring\n\n        net = net_cls(\n            module_cls, callbacks=[(\'myscore\', EpochScoring(\'myscore\'))])\n\n        params = net.get_params(deep=True)\n        # test a couple of expected parameters\n        assert \'verbose\' in params\n        assert \'module\' in params\n        assert \'callbacks\' in params\n        assert \'callbacks__print_log__sink\' in params\n        # not yet initialized\n        assert \'callbacks__myscore__scoring\' not in params\n\n        net.initialize()\n        params = net.get_params(deep=True)\n        # now initialized\n        assert \'callbacks__myscore__scoring\' in params\n\n    def test_get_params_with_uninit_callbacks(self, net_cls, module_cls):\n        from skorch.callbacks import EpochTimer\n\n        net = net_cls(\n            module_cls,\n            callbacks=[EpochTimer, (\'other_timer\', EpochTimer)],\n        )\n        # none of this raises an exception\n        net = clone(net)\n        net.get_params()\n        net.initialize()\n        net.get_params()\n\n    def test_get_params_no_learned_params(self, net_fit):\n        params = net_fit.get_params()\n        params_learned = set(filter(lambda x: x.endswith(\'_\'), params))\n        assert not params_learned\n\n    def test_clone_results_in_uninitialized_net(\n            self, net_fit, data):\n        X, y = data\n        accuracy = accuracy_score(net_fit.predict(X), y)\n        assert accuracy > ACCURACY_EXPECTED  # make sure net has learned\n\n        net_cloned = clone(net_fit).set_params(max_epochs=0)\n        net_cloned.callbacks_ = []\n        net_cloned.partial_fit(X, y)\n        accuracy_cloned = accuracy_score(net_cloned.predict(X), y)\n        assert accuracy_cloned < ACCURACY_EXPECTED\n\n        assert not net_cloned.history\n\n    def test_clone_copies_parameters(self, net_cls, module_cls):\n        kwargs = dict(\n            module__hidden_units=20,\n            lr=0.2,\n            iterator_train__batch_size=123,\n        )\n        net = net_cls(module_cls, **kwargs)\n        net_cloned = clone(net)\n        params = net_cloned.get_params()\n        for key, val in kwargs.items():\n            assert params[key] == val\n\n    def test_with_initialized_module(self, net_cls, module_cls, data):\n        X, y = data\n        net = net_cls(module_cls(), max_epochs=1)\n        net.fit(X, y)\n\n    def test_with_initialized_module_other_params(self, net_cls, module_cls, data):\n        X, y = data\n        net = net_cls(module_cls(), max_epochs=1, module__hidden_units=123)\n        net.fit(X, y)\n        weight = net.module_.sequential[0].weight.data\n        assert weight.shape[0] == 123\n\n    def test_with_initialized_module_non_default(\n            self, net_cls, module_cls, data, capsys):\n        X, y = data\n        net = net_cls(module_cls(hidden_units=123), max_epochs=1)\n        net.fit(X, y)\n        weight = net.module_.sequential[0].weight.data\n        assert weight.shape[0] == 123\n\n        stdout = capsys.readouterr()[0]\n        assert ""Re-initializing module!"" not in stdout\n\n    def test_message_fit_with_initialized_net(\n            self, net_cls, module_cls, data, capsys):\n        net = net_cls(module_cls).initialize()\n        net.fit(*data)\n        stdout = capsys.readouterr()[0]\n\n        msg_module = ""Re-initializing module""\n        assert msg_module in stdout\n\n        msg_optimizer = ""Re-initializing optimizer""\n        assert msg_optimizer in stdout\n\n        # bug: https://github.com/skorch-dev/skorch/issues/436\n        not_expected = \'because the following parameters were re-set\'\n        assert not_expected not in stdout\n\n    def test_with_initialized_module_partial_fit(\n            self, net_cls, module_cls, data, capsys):\n        X, y = data\n        module = module_cls(hidden_units=123)\n        net = net_cls(module, max_epochs=0)\n        net.partial_fit(X, y)\n\n        for p0, p1 in zip(module.parameters(), net.module_.parameters()):\n            assert p0.data.shape == p1.data.shape\n            assert (p0 == p1).data.all()\n\n        stdout = capsys.readouterr()[0]\n        assert ""Re-initializing module!"" not in stdout\n\n    def test_with_initialized_module_warm_start(\n            self, net_cls, module_cls, data, capsys):\n        X, y = data\n        module = module_cls(hidden_units=123)\n        net = net_cls(module, max_epochs=0, warm_start=True)\n        net.partial_fit(X, y)\n\n        for p0, p1 in zip(module.parameters(), net.module_.parameters()):\n            assert p0.data.shape == p1.data.shape\n            assert (p0 == p1).data.all()\n\n        stdout = capsys.readouterr()[0]\n        assert ""Re-initializing module!"" not in stdout\n\n    def test_with_initialized_sequential(self, net_cls, data, capsys):\n        X, y = data\n        module = nn.Sequential(\n            nn.Linear(X.shape[1], 10),\n            nn.ReLU(),\n            nn.Linear(10, 2),\n            nn.Softmax(dim=-1),\n        )\n        net = net_cls(module, max_epochs=1)\n        net.fit(X, y)\n\n        stdout = capsys.readouterr()[0]\n        assert ""Re-initializing module!"" not in stdout\n\n    def test_call_fit_twice_retrains(self, net_cls, module_cls, data):\n        # test that after second fit call, even without entering the\n        # fit loop, parameters have changed (because the module was\n        # re-initialized)\n        X, y = data[0][:100], data[1][:100]\n        net = net_cls(module_cls, warm_start=False).fit(X, y)\n        params_before = net.module_.parameters()\n\n        net.max_epochs = 0\n        net.fit(X, y)\n        params_after = net.module_.parameters()\n\n        assert not net.history\n        for p0, p1 in zip(params_before, params_after):\n            assert (p0 != p1).data.any()\n\n    def test_call_fit_twice_warmstart(self, net_cls, module_cls, data):\n        X, y = data[0][:100], data[1][:100]\n        net = net_cls(module_cls, warm_start=True).fit(X, y)\n        params_before = net.module_.parameters()\n\n        net.max_epochs = 0\n        net.fit(X, y)\n        params_after = net.module_.parameters()\n\n        assert len(net.history) == 10\n        for p0, p1 in zip(params_before, params_after):\n            assert (p0 == p1).data.all()\n\n    def test_partial_fit_first_call(self, net_cls, module_cls, data):\n        # It should be possible to partial_fit without calling fit first.\n        X, y = data[0][:100], data[1][:100]\n        # does not raise\n        net_cls(module_cls, warm_start=True).partial_fit(X, y)\n\n    def test_call_partial_fit_after_fit(self, net_cls, module_cls, data):\n        X, y = data[0][:100], data[1][:100]\n        net = net_cls(module_cls, warm_start=False).fit(X, y)\n        params_before = net.module_.parameters()\n\n        net.max_epochs = 0\n        net.partial_fit(X, y)\n        params_after = net.module_.parameters()\n\n        assert len(net.history) == 10\n        for p0, p1 in zip(params_before, params_after):\n            assert (p0 == p1).data.all()\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=""no cuda device"")\n    def test_binary_classification_with_cuda(self, net_cls, module_cls, data):\n        X, y = data\n        assert y.ndim == 1\n        assert set(y) == {0, 1}\n\n        net = net_cls(module_cls, max_epochs=1, device=\'cuda\')\n        # does not raise\n        net.fit(X, y)\n\n    def test_net_initialized_with_custom_dataset_args(\n            self, net_cls, module_cls, data, dataset_cls):\n        side_effect = []\n\n        class MyDataset(dataset_cls):\n            def __init__(self, *args, foo, **kwargs):\n                super().__init__(*args, **kwargs)\n                side_effect.append(foo)\n\n        net = net_cls(\n            module_cls,\n            dataset=MyDataset,\n            dataset__foo=123,\n            max_epochs=1,\n        )\n        net.fit(*data)\n        assert side_effect == [123]\n\n    @pytest.mark.xfail(raises=ValueError)\n    def test_net_initialized_with_initalized_dataset(\n            self, net_cls, module_cls, data, dataset_cls):\n        net = net_cls(\n            module_cls,\n            dataset=dataset_cls(*data),\n            max_epochs=1,\n\n            # Disable caching to highlight the issue with this\n            # test case (mismatching size between y values)\n            callbacks__valid_acc__use_caching=False,\n        )\n        # FIXME: When dataset is initialized, X and y do not matter\n        # anymore\n        net.fit(*data)  # should not raise\n\n    def test_net_initialized_with_partialed_dataset(\n            self, net_cls, module_cls, data, dataset_cls):\n        X, y = data\n        net = net_cls(\n            module_cls,\n            dataset=partial(dataset_cls, length=len(y)),\n            train_split=None,\n            max_epochs=1,\n        )\n        net.fit(X, y)  # does not raise\n\n    def test_net_initialized_with_initalized_dataset_and_kwargs_raises(\n            self, net_cls, module_cls, data, dataset_cls):\n        net = net_cls(\n            module_cls,\n            dataset=dataset_cls(*data),\n            dataset__foo=123,\n            max_epochs=1,\n        )\n        with pytest.raises(TypeError) as exc:\n            net.fit(*data)\n\n        expected = (""Trying to pass an initialized Dataset while passing ""\n                    ""Dataset arguments ({\'foo\': 123}) is not allowed."")\n        assert exc.value.args[0] == expected\n\n    def test_repr_uninitialized_works(self, net_cls, module_cls):\n        net = net_cls(\n            module_cls,\n            module__hidden_units=55,\n        )\n        result = net.__repr__()\n        expected = """"""<class \'skorch.classifier.NeuralNetClassifier\'>[uninitialized](\n  module={},\n  module__hidden_units=55,\n)"""""".format(module_cls)\n        assert result == expected\n\n    def test_repr_initialized_works(self, net_cls, module_cls):\n        net = net_cls(\n            module_cls,\n            module__hidden_units=42,\n        )\n        net.initialize()\n        result = net.__repr__()\n        expected = """"""<class \'skorch.classifier.NeuralNetClassifier\'>[initialized](\n  module_=MLPModule(\n    (nonlin): ReLU()\n    (output_nonlin): Softmax()\n    (sequential): Sequential(\n      (0): Linear(in_features=20, out_features=42, bias=True)\n      (1): ReLU()\n      (2): Dropout(p=0.5)\n      (3): Linear(in_features=42, out_features=42, bias=True)\n      (4): ReLU()\n      (5): Dropout(p=0.5)\n      (6): Linear(in_features=42, out_features=2, bias=True)\n      (7): Softmax()\n    )\n  ),\n)""""""\n        if LooseVersion(torch.__version__) >= \'1.2\':\n            expected = expected.replace(""Softmax()"", ""Softmax(dim=-1)"")\n            expected = expected.replace(""Dropout(p=0.5)"",\n                                        ""Dropout(p=0.5, inplace=False)"")\n        assert result == expected\n\n    def test_repr_fitted_works(self, net_cls, module_cls, data):\n        X, y = data\n        net = net_cls(\n            module_cls,\n            module__hidden_units=11,\n            module__nonlin=nn.PReLU(),\n        )\n        net.fit(X[:50], y[:50])\n        result = net.__repr__()\n        expected = """"""<class \'skorch.classifier.NeuralNetClassifier\'>[initialized](\n  module_=MLPModule(\n    (nonlin): PReLU(num_parameters=1)\n    (output_nonlin): Softmax()\n    (sequential): Sequential(\n      (0): Linear(in_features=20, out_features=11, bias=True)\n      (1): PReLU(num_parameters=1)\n      (2): Dropout(p=0.5)\n      (3): Linear(in_features=11, out_features=11, bias=True)\n      (4): PReLU(num_parameters=1)\n      (5): Dropout(p=0.5)\n      (6): Linear(in_features=11, out_features=2, bias=True)\n      (7): Softmax()\n    )\n  ),\n)""""""\n        if LooseVersion(torch.__version__) >= \'1.2\':\n            expected = expected.replace(""Softmax()"", ""Softmax(dim=-1)"")\n            expected = expected.replace(""Dropout(p=0.5)"",\n                                        ""Dropout(p=0.5, inplace=False)"")\n        assert result == expected\n\n    def test_fit_params_passed_to_module(self, net_cls, data):\n        from skorch.toy import MLPModule\n\n        X, y = data\n        side_effect = []\n\n        class FPModule(MLPModule):\n            # pylint: disable=arguments-differ\n            def forward(self, X, **fit_params):\n                side_effect.append(fit_params)\n                return super().forward(X)\n\n        net = net_cls(FPModule, max_epochs=1, batch_size=50, train_split=None)\n        # remove callbacks to have better control over side_effect\n        net.initialize()\n        net.callbacks_ = []\n        net.fit(X[:100], y[:100], foo=1, bar=2)\n        net.fit(X[:100], y[:100], bar=3, baz=4)\n\n        assert len(side_effect) == 4  # 2 epochs \xc3\xa0 2 batches\n        assert side_effect[0] == dict(foo=1, bar=2)\n        assert side_effect[1] == dict(foo=1, bar=2)\n        assert side_effect[2] == dict(bar=3, baz=4)\n        assert side_effect[3] == dict(bar=3, baz=4)\n\n    def test_fit_params_passed_to_module_in_pipeline(self, net_cls, data):\n        from skorch.toy import MLPModule\n\n        X, y = data\n        side_effect = []\n\n        class FPModule(MLPModule):\n            # pylint: disable=arguments-differ\n            def forward(self, X, **fit_params):\n                side_effect.append(fit_params)\n                return super().forward(X)\n\n        net = net_cls(FPModule, max_epochs=1, batch_size=50, train_split=None)\n        net.initialize()\n        net.callbacks_ = []\n        pipe = Pipeline([\n            (\'net\', net),\n        ])\n        pipe.fit(X[:100], y[:100], net__foo=1, net__bar=2)\n        pipe.fit(X[:100], y[:100], net__bar=3, net__baz=4)\n\n        assert len(side_effect) == 4  # 2 epochs \xc3\xa0 2 batches\n        assert side_effect[0] == dict(foo=1, bar=2)\n        assert side_effect[1] == dict(foo=1, bar=2)\n        assert side_effect[2] == dict(bar=3, baz=4)\n        assert side_effect[3] == dict(bar=3, baz=4)\n\n    def test_fit_params_passed_to_train_split(self, net_cls, data):\n        from skorch.toy import MLPModule\n\n        X, y = data\n        side_effect = []\n\n        # pylint: disable=unused-argument\n        def fp_train_split(dataset, y=None, **fit_params):\n            side_effect.append(fit_params)\n            return dataset, dataset\n\n        class FPModule(MLPModule):\n            # pylint: disable=unused-argument,arguments-differ\n            def forward(self, X, **fit_params):\n                return super().forward(X)\n\n        net = net_cls(\n            FPModule,\n            max_epochs=1,\n            batch_size=50,\n            train_split=fp_train_split,\n        )\n        net.initialize()\n        net.callbacks_ = []\n        net.fit(X[:100], y[:100], foo=1, bar=2)\n        net.fit(X[:100], y[:100], bar=3, baz=4)\n\n        assert len(side_effect) == 2  # 2 epochs\n        assert side_effect[0] == dict(foo=1, bar=2)\n        assert side_effect[1] == dict(bar=3, baz=4)\n\n    def test_data_dict_and_fit_params(self, net_cls, data):\n        from skorch.toy import MLPModule\n\n        X, y = data\n\n        class FPModule(MLPModule):\n            # pylint: disable=unused-argument,arguments-differ\n            def forward(self, X0, X1, **fit_params):\n                assert fit_params.get(\'foo\') == 3\n                return super().forward(X0)\n\n        net = net_cls(FPModule, max_epochs=1, batch_size=50, train_split=None)\n        # does not raise\n        net.fit({\'X0\': X, \'X1\': X}, y, foo=3)\n\n    def test_data_dict_and_fit_params_conflicting_names_raises(\n            self, net_cls, data):\n        from skorch.toy import MLPModule\n\n        X, y = data\n\n        class FPModule(MLPModule):\n            # pylint: disable=unused-argument,arguments-differ\n            def forward(self, X0, X1, **fit_params):\n                return super().forward(X0)\n\n        net = net_cls(FPModule, max_epochs=1, batch_size=50, train_split=None)\n\n        with pytest.raises(ValueError) as exc:\n            net.fit({\'X0\': X, \'X1\': X}, y, X1=3)\n\n        expected = ""X and fit_params contain duplicate keys: X1""\n        assert exc.value.args[0] == expected\n\n    def test_fit_with_dataset(self, net_cls, module_cls, data, dataset_cls):\n        ds = dataset_cls(*data)\n        net = net_cls(module_cls, max_epochs=1)\n        net.fit(ds, data[1])\n        for key in (\'train_loss\', \'valid_loss\', \'valid_acc\'):\n            assert key in net.history[-1]\n\n    def test_predict_with_dataset(self, net_cls, module_cls, data, dataset_cls):\n        ds = dataset_cls(*data)\n        net = net_cls(module_cls).initialize()\n        y_pred = net.predict(ds)\n        y_proba = net.predict_proba(ds)\n\n        assert y_pred.shape[0] == len(ds)\n        assert y_proba.shape[0] == len(ds)\n\n    def test_fit_with_dataset_X_y_inaccessible_does_not_raise(\n            self, net_cls, module_cls, data):\n        class MyDataset(torch.utils.data.Dataset):\n            """"""Dataset with inaccessible X and y""""""\n            def __init__(self, X, y):\n                self.xx = X  # incorrect attribute name\n                self.yy = y  # incorrect attribute name\n\n            def __len__(self):\n                return len(self.xx)\n\n            def __getitem__(self, i):\n                return self.xx[i], self.yy[i]\n\n        ds = MyDataset(*data)\n        net = net_cls(module_cls, max_epochs=1)\n        net.fit(ds, data[1])  # does not raise\n\n    def test_fit_with_dataset_without_explicit_y(\n            self, net_cls, module_cls, dataset_cls, data):\n        from skorch.dataset import CVSplit\n\n        net = net_cls(\n            module_cls,\n            max_epochs=1,\n            train_split=CVSplit(stratified=False),\n        )\n        ds = dataset_cls(*data)\n        net.fit(ds, None)  # does not raise\n        for key in (\'train_loss\', \'valid_loss\', \'valid_acc\'):\n            assert key in net.history[-1]\n\n    def test_fit_with_dataset_stratified_without_explicit_y_raises(\n            self, net_cls, module_cls, dataset_cls, data):\n        from skorch.dataset import CVSplit\n\n        net = net_cls(\n            module_cls,\n            train_split=CVSplit(stratified=True),\n        )\n        ds = dataset_cls(*data)\n        with pytest.raises(ValueError) as exc:\n            net.fit(ds, None)\n\n        msg = ""Stratified CV requires explicitly passing a suitable y.""\n        assert exc.value.args[0] == msg\n\n    @pytest.fixture\n    def dataset_1_item(self):\n        class Dataset(torch.utils.data.Dataset):\n            def __len__(self):\n                return 100\n\n            def __getitem__(self, i):\n                return 0.0\n        return Dataset\n\n    def test_fit_with_dataset_one_item_error(\n            self, net_cls, module_cls, dataset_1_item):\n        net = net_cls(module_cls, train_split=None)\n        with pytest.raises(ValueError) as exc:\n            net.fit(dataset_1_item(), None)\n\n        msg = (""You are using a non-skorch dataset that returns 1 value. ""\n               ""Remember that for skorch, Dataset.__getitem__ must return ""\n               ""exactly 2 values, X and y (more info: ""\n               ""https://skorch.readthedocs.io/en/stable/user/dataset.html)."")\n        assert exc.value.args[0] == msg\n\n    def test_predict_with_dataset_one_item_error(\n            self, net_cls, module_cls, dataset_1_item):\n        net = net_cls(module_cls, train_split=None).initialize()\n        with pytest.raises(ValueError) as exc:\n            net.predict(dataset_1_item())\n\n        msg = (""You are using a non-skorch dataset that returns 1 value. ""\n               ""Remember that for skorch, Dataset.__getitem__ must return ""\n               ""exactly 2 values, X and y (more info: ""\n               ""https://skorch.readthedocs.io/en/stable/user/dataset.html)."")\n        assert exc.value.args[0] == msg\n\n    @pytest.fixture\n    def dataset_3_items(self):\n        class Dataset(torch.utils.data.Dataset):\n            def __len__(self):\n                return 100\n\n            def __getitem__(self, i):\n                return 0.0, 0.0, 0.0\n        return Dataset\n\n    def test_fit_with_dataset_three_items_error(\n            self, net_cls, module_cls, dataset_3_items):\n        net = net_cls(module_cls, train_split=None)\n        with pytest.raises(ValueError) as exc:\n            net.fit(dataset_3_items(), None)\n\n        msg = (""You are using a non-skorch dataset that returns 3 values. ""\n               ""Remember that for skorch, Dataset.__getitem__ must return ""\n               ""exactly 2 values, X and y (more info: ""\n               ""https://skorch.readthedocs.io/en/stable/user/dataset.html)."")\n        assert exc.value.args[0] == msg\n\n    def test_predict_with_dataset_three_items_error(\n            self, net_cls, module_cls, dataset_3_items):\n        net = net_cls(module_cls, train_split=None).initialize()\n        with pytest.raises(ValueError) as exc:\n            net.predict(dataset_3_items())\n\n        msg = (""You are using a non-skorch dataset that returns 3 values. ""\n               ""Remember that for skorch, Dataset.__getitem__ must return ""\n               ""exactly 2 values, X and y (more info: ""\n               ""https://skorch.readthedocs.io/en/stable/user/dataset.html)."")\n        assert exc.value.args[0] == msg\n\n    @pytest.fixture\n    def multiouput_net(self, net_cls, multiouput_module):\n        return net_cls(multiouput_module).initialize()\n\n    def test_multioutput_forward_iter(self, multiouput_net, data):\n        X = data[0]\n        y_infer = next(multiouput_net.forward_iter(X))\n\n        assert isinstance(y_infer, tuple)\n        assert len(y_infer) == 3\n        assert y_infer[0].shape[0] == min(len(X), multiouput_net.batch_size)\n\n    def test_multioutput_forward(self, multiouput_net, data):\n        X = data[0]\n        n = len(X)\n        y_infer = multiouput_net.forward(X)\n\n        assert isinstance(y_infer, tuple)\n        assert len(y_infer) == 3\n        for arr in y_infer:\n            assert is_torch_data_type(arr)\n\n        # Expecting full output: (number of samples, number of output units)\n        assert y_infer[0].shape == (n, 2)\n        # Expecting only column 0: (number of samples,)\n        assert y_infer[1].shape == (n,)\n        # Expecting only every other row: (number of samples/2, number\n        # of output units)\n        assert y_infer[2].shape == (n // 2, 2)\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=""no cuda device"")\n    def test_multioutput_forward_device_gpu(self, multiouput_net, data):\n        X = data[0]\n        y_infer = multiouput_net.forward(X, device=\'cuda:0\')\n\n        assert isinstance(y_infer, tuple)\n        assert len(y_infer) == 3\n        for arr in y_infer:\n            assert arr.is_cuda\n\n    def test_multioutput_predict(self, multiouput_net, data):\n        X = data[0]\n        n = len(X)\n\n        # does not raise\n        y_pred = multiouput_net.predict(X)\n\n        # Expecting only 1 column containing predict class:\n        # (number of samples,)\n        assert y_pred.shape == (n,)\n        assert set(y_pred) == {0, 1}\n\n    def test_multiouput_predict_proba(self, multiouput_net, data):\n        X = data[0]\n        n = len(X)\n\n        # does not raise\n        y_proba = multiouput_net.predict_proba(X)\n\n        # Expecting full output: (number of samples, number of output units)\n        assert y_proba.shape == (n, 2)\n        # Probabilities, hence these limits\n        assert y_proba.min() >= 0\n        assert y_proba.max() <= 1\n\n    def test_setting_callback_possible(self, net_cls, module_cls):\n        from skorch.callbacks import EpochTimer, PrintLog\n\n        net = net_cls(module_cls, callbacks=[(\'mycb\', PrintLog())])\n        net.initialize()\n\n        assert isinstance(dict(net.callbacks_)[\'mycb\'], PrintLog)\n\n        net.set_params(callbacks__mycb=EpochTimer())\n        assert isinstance(dict(net.callbacks_)[\'mycb\'], EpochTimer)\n\n    def test_setting_callback_default_possible(self, net_cls, module_cls):\n        from skorch.callbacks import EpochTimer, PrintLog\n\n        net = net_cls(module_cls)\n        net.initialize()\n\n        assert isinstance(dict(net.callbacks_)[\'print_log\'], PrintLog)\n\n        net.set_params(callbacks__print_log=EpochTimer())\n        assert isinstance(dict(net.callbacks_)[\'print_log\'], EpochTimer)\n\n    def test_setting_callback_to_none_possible(self, net_cls, module_cls, data):\n        from skorch.callbacks import Callback\n\n        X, y = data[0][:30], data[1][:30]  # accelerate test\n        side_effects = []\n\n        class DummyCallback(Callback):\n            def __init__(self, i):\n                self.i = i\n\n            # pylint: disable=unused-argument, arguments-differ\n            def on_epoch_end(self, *args, **kwargs):\n                side_effects.append(self.i)\n\n        net = net_cls(\n            module_cls,\n            max_epochs=2,\n            callbacks=[\n                (\'cb0\', DummyCallback(0)),\n                (\'cb1\', DummyCallback(1)),\n                (\'cb2\', DummyCallback(2)),\n            ],\n        )\n        net.fit(X, y)\n\n        # all 3 callbacks write to output twice\n        assert side_effects == [0, 1, 2, 0, 1, 2]\n\n        # deactivate cb1\n        side_effects.clear()\n        net.set_params(callbacks__cb1=None)\n        net.fit(X, y)\n\n        assert side_effects == [0, 2, 0, 2]\n\n    def test_setting_callback_to_none_and_more_params_during_init_raises(\n            self, net_cls, module_cls):\n        # if a callback is set to None, setting more params for it\n        # should not work\n        net = net_cls(\n            module_cls, callbacks__print_log=None, callbacks__print_log__sink=1)\n\n        with pytest.raises(ValueError) as exc:\n            net.initialize()\n\n        msg = (""Trying to set a parameter for callback print_log ""\n               ""which does not exist."")\n        assert exc.value.args[0] == msg\n\n    def test_setting_callback_to_none_and_more_params_later_raises(\n            self, net_cls, module_cls):\n        # this should work\n        net = net_cls(module_cls)\n        net.set_params(callbacks__print_log__sink=123)\n        net.set_params(callbacks__print_log=None)\n\n        net = net_cls(module_cls)\n        net.set_params(callbacks__print_log=None)\n        with pytest.raises(ValueError) as exc:\n            net.set_params(callbacks__print_log__sink=123)\n\n        msg = (""Trying to set a parameter for callback print_log ""\n               ""which does not exist."")\n        assert exc.value.args[0] == msg\n\n    def test_set_params_with_unknown_key_raises(self, net):\n        with pytest.raises(ValueError) as exc:\n            net.set_params(foo=123)\n\n        # TODO: check error message more precisely, depending on what\n        # the intended message should be from sklearn side\n        assert exc.value.args[0].startswith(\'Invalid parameter foo for\')\n\n    @pytest.fixture()\n    def sequence_module_cls(self):\n        """"""Simple sequence model with variable size dim 1.""""""\n        class Mod(torch.nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.l = torch.nn.Linear(1, 1)\n            # pylint: disable=arguments-differ\n            def forward(self, x):\n                n = np.random.randint(1, 4)\n                y = self.l(x.float())\n                return torch.randn(1, n, 2) + 0 * y\n        return Mod\n\n    def test_net_variable_prediction_lengths(\n            self, net_cls, sequence_module_cls):\n        # neural net should work fine with fixed y_true but varying y_pred\n        # sequences.\n        X = np.array([1, 5, 3, 6, 2])\n        y = np.array([[0, 0, 1], [1, 0, 1], [0, 1, 1], [1, 1, 0], [0, 1, 0]])\n        X, y = X[:, np.newaxis], y[:, :, np.newaxis]\n        X, y = X.astype(\'float32\'), y.astype(\'float32\')\n\n        net = net_cls(\n            sequence_module_cls,\n            batch_size=1,\n            max_epochs=2,\n            train_split=None,\n        )\n\n        # Mock loss function\n        # pylint: disable=unused-argument\n        def loss_fn(y_pred, y_true, **kwargs):\n            return y_pred[:, 0, 0]\n        net.get_loss = loss_fn\n\n        net.fit(X, y)\n\n    def test_net_variable_label_lengths(self, net_cls, sequence_module_cls):\n        # neural net should work fine with varying y_true sequences.\n        X = np.array([1, 5, 3, 6, 2])\n        y = np.array([[1], [1, 0, 1], [1, 1], [1, 1, 0], [1, 0]])\n        X = X[:, np.newaxis].astype(\'float32\')\n        y = np.array([np.array(n, dtype=\'float32\')[:, np.newaxis] for n in y])\n\n        net = net_cls(\n            sequence_module_cls,\n            batch_size=1,\n            max_epochs=2,\n            train_split=None,\n        )\n\n        # Mock loss function\n        # pylint: disable=unused-argument\n        def loss_fn(y_pred, y_true, **kwargs):\n            return y_pred[:, 0, 0]\n        net.get_loss = loss_fn\n\n        # check_data complains about y.shape = (n,) but\n        # we know that it is actually (n, m) with m in [1;3].\n        net.check_data = lambda *_, **kw: None\n        net.fit(X, y)\n\n    def test_no_grad_during_validation(self, net_cls, module_cls, data):\n        """"""Test that gradient is only calculated during training step,\n        not validation step.""""""\n\n        # pylint: disable=unused-argument\n        def check_grad(*args, loss, training, **kwargs):\n            if training:\n                assert loss.requires_grad\n            else:\n                assert not loss.requires_grad\n\n        mock_cb = Mock(on_batch_end=check_grad)\n        net = net_cls(module_cls, max_epochs=1, callbacks=[mock_cb])\n        net.fit(*data)\n\n    def test_callback_on_grad_computed(self, net_cls, module_cls, data):\n\n        module = module_cls()\n        expected_names = set(name for name, _ in module.named_parameters())\n\n        def on_grad_computed(*args, named_parameters, **kwargs):\n            names = set(name for name, _ in named_parameters)\n            assert expected_names == names\n\n        mock_cb = Mock(on_grad_computed=on_grad_computed)\n        net = net_cls(module, max_epochs=1, callbacks=[mock_cb])\n        net.fit(*data)\n\n    @pytest.mark.parametrize(\'training\', [True, False])\n    def test_no_grad_during_evaluation_unless_training(\n            self, net_cls, module_cls, data, training):\n        """"""Test that gradient is only calculated in training mode\n        during evaluation step.""""""\n        from skorch.utils import to_tensor\n\n        net = net_cls(module_cls).initialize()\n        Xi = to_tensor(data[0][:3], device=\'cpu\')\n        y_eval = net.evaluation_step(Xi, training=training)\n\n        assert y_eval.requires_grad is training\n\n    @pytest.mark.parametrize(\n        \'net_kwargs,expected_train_batch_size,expected_valid_batch_size\',\n        [\n            ({\'batch_size\': -1}, 800, 200),\n            ({\'iterator_train__batch_size\': -1}, 800, 128),\n            ({\'iterator_valid__batch_size\': -1}, 128, 200),\n        ]\n    )\n    def test_batch_size_neg_1_uses_whole_dataset(\n            self, net_cls, module_cls, data, net_kwargs,\n            expected_train_batch_size, expected_valid_batch_size):\n\n        train_loader_mock = Mock(side_effect=torch.utils.data.DataLoader)\n        valid_loader_mock = Mock(side_effect=torch.utils.data.DataLoader)\n\n        net = net_cls(module_cls, max_epochs=1,\n                      iterator_train=train_loader_mock,\n                      iterator_valid=valid_loader_mock,\n                      **net_kwargs)\n        net.fit(*data)\n\n        train_batch_size = net.history[:, \'batches\', :, \'train_batch_size\'][0][0]\n        valid_batch_size = net.history[:, \'batches\', :, \'valid_batch_size\'][0][0]\n\n        assert train_batch_size == expected_train_batch_size\n        assert valid_batch_size == expected_valid_batch_size\n\n        train_kwargs = train_loader_mock.call_args[1]\n        valid_kwargs = valid_loader_mock.call_args[1]\n        assert train_kwargs[\'batch_size\'] == expected_train_batch_size\n        assert valid_kwargs[\'batch_size\'] == expected_valid_batch_size\n\n    @pytest.mark.parametrize(\'batch_size\', [40, 100])\n    def test_batch_count(self, net_cls, module_cls, data, batch_size):\n\n        net = net_cls(module_cls, max_epochs=1, batch_size=batch_size)\n        X, y = data\n        net.fit(X, y)\n\n        train_batch_count = int(0.8 * len(X)) / batch_size\n        valid_batch_count = int(0.2 * len(X)) / batch_size\n\n        assert net.history[:, ""train_batch_count""] == [train_batch_count]\n        assert net.history[:, ""valid_batch_count""] == [valid_batch_count]\n\n    @flaky(max_runs=3)\n    def test_fit_lbfgs_optimizer(self, net_cls, module_cls, data):\n        X, y = data\n        net = net_cls(\n            module_cls,\n            optimizer=torch.optim.LBFGS,\n            lr=1.0,\n            batch_size=-1,\n        )\n        net.fit(X, y)\n\n        last_epoch = net.history[-1]\n        assert last_epoch[\'train_loss\'] < 1.0\n        assert last_epoch[\'valid_loss\'] < 1.0\n        assert last_epoch[\'valid_acc\'] > 0.75\n\n    def test_accumulator_that_returns_last_value(\n            self, net_cls, module_cls, data):\n        # We define an optimizer that calls the step function 3 times\n        # and an accumulator that returns the last of those calls. We\n        # then test that the correct values were stored.\n        from skorch.utils import FirstStepAccumulator\n\n        side_effect = []\n\n        class SGD3Calls(torch.optim.SGD):\n            def step(self, closure=None):\n                for _ in range(3):\n                    loss = super().step(closure)\n                    side_effect.append(float(loss))\n\n        class MyAccumulator(FirstStepAccumulator):\n            """"""Accumulate all steps and return the last.""""""\n            def store_step(self, step):\n                if self.step is None:\n                    self.step = [step]\n                else:\n                    self.step.append(step)\n\n            def get_step(self):\n                # Losses should only ever be retrieved after storing 3\n                # times.\n                assert len(self.step) == 3\n                return self.step[-1]\n\n        X, y = data\n        max_epochs = 2\n        batch_size = 100\n        net = net_cls(\n            module_cls,\n            optimizer=SGD3Calls,\n            max_epochs=max_epochs,\n            batch_size=batch_size,\n            train_split=None,\n        )\n        net.get_train_step_accumulator = MyAccumulator\n        net.fit(X, y)\n\n        # Number of loss calculations is total number of batches x 3.\n        num_batches_per_epoch = int(np.ceil(len(y) / batch_size))\n        expected_calls = 3 * num_batches_per_epoch * max_epochs\n        assert len(side_effect) == expected_calls\n\n        # Every 3rd loss calculation (i.e. the last per call) should\n        # be stored in the history.\n        expected_losses = list(\n            flatten(net.history[:, \'batches\', :, \'train_loss\']))\n        assert np.allclose(side_effect[2::3], expected_losses)\n\n    def test_predefined_split(self, net_cls, module_cls, data):\n        from skorch.dataset import Dataset\n        from skorch.helper import predefined_split\n\n        train_loader_mock = Mock(side_effect=torch.utils.data.DataLoader)\n        valid_loader_mock = Mock(side_effect=torch.utils.data.DataLoader)\n\n        train_ds = Dataset(*data)\n        valid_ds = Dataset(*data)\n\n        net = net_cls(\n            module_cls, max_epochs=1,\n            iterator_train=train_loader_mock,\n            iterator_valid=valid_loader_mock,\n            train_split=predefined_split(valid_ds)\n        )\n\n        net.fit(train_ds, None)\n\n        train_loader_ds = train_loader_mock.call_args[0][0]\n        valid_loader_ds = valid_loader_mock.call_args[0][0]\n\n        assert train_loader_ds == train_ds\n        assert valid_loader_ds == valid_ds\n\n    def test_set_lr_at_runtime_doesnt_reinitialize(self, net_fit):\n        with patch(\'skorch.NeuralNet.initialize_optimizer\') as f:\n            net_fit.set_params(lr=0.9)\n        assert not f.called\n\n    def test_set_lr_at_runtime_sets_lr(self, net_fit):\n        new_lr = net_fit.lr + 1\n        net_fit.set_params(lr=new_lr)\n\n        assert net_fit.lr == new_lr\n        assert net_fit.optimizer_.param_groups[0][\'lr\'] == new_lr\n\n    def test_set_lr_at_runtime_sets_lr_via_pgroup_0(self, net_fit):\n        new_lr = net_fit.lr + 1\n        net_fit.set_params(optimizer__param_groups__0__lr=new_lr)\n\n        # note that setting group does not set global lr\n        assert net_fit.lr != new_lr\n        assert net_fit.optimizer_.param_groups[0][\'lr\'] == new_lr\n\n    def test_set_lr_at_runtime_sets_lr_pgroups(self, net_cls, module_cls, data):\n        lr_pgroup_0 = 0.1\n        lr_pgroup_1 = 0.2\n        lr_pgroup_0_new = 0.3\n        lr_pgroup_1_new = 0.4\n\n        net = net_cls(\n            module_cls,\n            lr=lr_pgroup_1,\n            max_epochs=1,\n            optimizer__param_groups=[\n                (\'sequential.0.*\', {\'lr\': lr_pgroup_0}),\n            ])\n        net.fit(*data)\n\n        # optimizer__param_groups=[g1] will create\n        # - param group 0 matching the definition of g1\n        # - param group 1 matching all other parameters\n        assert net.optimizer_.param_groups[0][\'lr\'] == lr_pgroup_0\n        assert net.optimizer_.param_groups[1][\'lr\'] == lr_pgroup_1\n\n        net.set_params(optimizer__param_groups__0__lr=lr_pgroup_0_new)\n        net.set_params(optimizer__param_groups__1__lr=lr_pgroup_1_new)\n\n        assert net.optimizer_.param_groups[0][\'lr\'] == lr_pgroup_0_new\n        assert net.optimizer_.param_groups[1][\'lr\'] == lr_pgroup_1_new\n\n    def test_criterion_training_set_correctly(self, net_cls, module_cls, data):\n        # check that criterion\'s training attribute is set correctly\n\n        X, y = data[0][:50], data[1][:50]  # don\'t need all the data\n        side_effect = []\n\n        class MyCriterion(nn.NLLLoss):\n            """"""Criterion that records its training attribute""""""\n            def forward(self, *args, **kwargs):\n                side_effect.append(self.training)\n                return super().forward(*args, **kwargs)\n\n        net = net_cls(module_cls, criterion=MyCriterion, max_epochs=1)\n        net.fit(X, y)\n\n        # called once with training=True for train step, once with\n        # training=False for validation step\n        assert side_effect == [True, False]\n\n        net.partial_fit(X, y)\n        # same logic as before\n        assert side_effect == [True, False, True, False]\n\n    def test_criterion_is_not_a_torch_module(self, net_cls, module_cls, data):\n        X, y = data[0][:50], data[1][:50]  # don\'t need all the data\n\n        def my_criterion():\n            return torch.nn.functional.nll_loss\n\n        net = net_cls(module_cls, criterion=my_criterion, max_epochs=1)\n        net.fit(X, y)  # does not raise\n\n    @pytest.mark.parametrize(\'acc_steps\', [1, 2, 3, 5, 10])\n    def test_gradient_accumulation(self, net_cls, module_cls, data, acc_steps):\n        # Test if gradient accumulation technique is possible,\n        # i.e. performing a weight update only every couple of\n        # batches.\n        mock_optimizer = Mock()\n\n        class GradAccNet(net_cls):\n            """"""Net that accumulates gradients""""""\n            def __init__(self, *args, acc_steps=acc_steps, **kwargs):\n                super().__init__(*args, **kwargs)\n                self.acc_steps = acc_steps\n\n            def initialize(self):\n                # This is not necessary for gradient accumulation but\n                # only for testing purposes\n                super().initialize()\n                self.true_optimizer_ = self.optimizer_\n                mock_optimizer.step.side_effect = self.true_optimizer_.step\n                mock_optimizer.zero_grad.side_effect = self.true_optimizer_.zero_grad\n                self.optimizer_ = mock_optimizer\n\n            def get_loss(self, *args, **kwargs):\n                loss = super().get_loss(*args, **kwargs)\n                # because only every nth step is optimized\n                return loss / self.acc_steps\n\n            def train_step(self, Xi, yi, **fit_params):\n                """"""Perform gradient accumulation\n\n                Only optimize every 2nd batch.\n\n                """"""\n                # note that n_train_batches starts at 1 for each epoch\n                n_train_batches = len(self.history[-1, \'batches\'])\n                step = self.train_step_single(Xi, yi, **fit_params)\n\n                if n_train_batches % self.acc_steps == 0:\n                    self.optimizer_.step()\n                    self.optimizer_.zero_grad()\n                return step\n\n        max_epochs = 5\n        net = GradAccNet(module_cls, max_epochs=max_epochs)\n        X, y = data\n        net.fit(X, y)\n\n        n = len(X) * 0.8  # number of training samples\n        b = np.ceil(n / net.batch_size)  # batches per epoch\n        s = b // acc_steps  # number of acc steps per epoch\n        calls_total = s * max_epochs\n        calls_step = mock_optimizer.step.call_count\n        calls_zero_grad = mock_optimizer.zero_grad.call_count\n        assert calls_total == calls_step == calls_zero_grad\n\n    def test_setattr_module(self, net_cls, module_cls):\n        net = net_cls(module_cls)\n        assert \'mymodule\' not in net.prefixes_\n        assert \'mymodule\' not in net.cuda_dependent_attributes_\n\n        class MyNet(net_cls):\n            def __init__(self, *args, **kwargs):\n                super().__init__(*args, **kwargs)\n                self.mymodule = module_cls\n\n        net = MyNet(module_cls)\n        assert \'mymodule\' in net.prefixes_\n        assert \'mymodule_\' in net.cuda_dependent_attributes_\n\n        del net.mymodule\n        assert \'mymodule\' not in net.prefixes_\n        assert \'mymodule_\' not in net.cuda_dependent_attributes_\n\n    def test_setattr_module_instance(self, net_cls, module_cls):\n        net = net_cls(module_cls)\n        assert \'mymodule\' not in net.prefixes_\n        assert \'mymodule\' not in net.cuda_dependent_attributes_\n\n        class MyNet(net_cls):\n            def __init__(self, *args, **kwargs):\n                super().__init__(*args, **kwargs)\n                self.mymodule = module_cls()\n\n        net = MyNet(module_cls)\n        assert \'mymodule\' in net.prefixes_\n        assert \'mymodule_\' in net.cuda_dependent_attributes_\n\n        del net.mymodule\n        assert \'mymodule\' not in net.prefixes_\n        assert \'mymodule_\' not in net.cuda_dependent_attributes_\n\n    def test_setattr_optimizer(self, net_cls, module_cls):\n        net = net_cls(module_cls)\n        assert \'myoptimizer\' not in net.prefixes_\n        assert \'myoptimizer\' not in net.cuda_dependent_attributes_\n\n        class MyNet(net_cls):\n            def __init__(self, *args, **kwargs):\n                super().__init__(*args, **kwargs)\n                self.myoptimizer = torch.optim.SGD\n\n        net = MyNet(module_cls)\n        assert \'myoptimizer\' in net.prefixes_\n        assert \'myoptimizer_\' in net.cuda_dependent_attributes_\n\n        del net.myoptimizer\n        assert \'myoptimizer\' not in net.prefixes_\n        assert \'myoptimizer_\' not in net.cuda_dependent_attributes_\n\n    def test_setattr_ending_in_underscore(self, net_cls, module_cls):\n        # attributes whose name ends in underscore should not be\n        # registered\n        class MyNet(net_cls):\n            def __init__(self, *args, **kwargs):\n                super().__init__(*args, **kwargs)\n                self.mymodule_ = module_cls\n\n        net = MyNet(module_cls)\n        assert \'mymodule\' not in net.prefixes_\n        assert \'mymodule_\' not in net.prefixes_\n        assert \'mymodule_\' not in net.cuda_dependent_attributes_\n\n    def test_setattr_no_duplicates(self, net_cls, module_cls):\n        # the \'module\' attribute is set twice but that shouldn\'t lead\n        # to duplicates in prefixes_ or cuda_dependent_attributes_\n        class MyNet(net_cls):\n            def __init__(self, *args, **kwargs):\n                super().__init__(*args, **kwargs)\n                self.module = module_cls\n\n        net = MyNet(module_cls)\n        assert net.prefixes_.count(\'module\') == 1\n        assert net.cuda_dependent_attributes_.count(\'module_\') == 1\n\n    def test_setattr_non_torch_attribute(self, net_cls, module_cls):\n        # attributes that are not torch modules or optimizers should\n        # not be registered\n        class MyNet(net_cls):\n            def __init__(self, *args, **kwargs):\n                super().__init__(*args, **kwargs)\n                self.num = 123\n\n        net = MyNet(module_cls)\n        assert \'num\' not in net.prefixes_\n        assert \'num_\' not in net.cuda_dependent_attributes_\n\n    def test_setattr_does_not_modify_class_attribute(self, net_cls, module_cls):\n        net = net_cls(module_cls)\n        assert \'mymodule\' not in net.prefixes_\n        assert \'mymodule\' not in net.cuda_dependent_attributes_\n\n        class MyNet(net_cls):\n            def __init__(self, *args, **kwargs):\n                super().__init__(*args, **kwargs)\n                self.mymodule = module_cls\n\n        net = MyNet(module_cls)\n        assert \'mymodule\' in net.prefixes_\n        assert \'mymodule_\' in net.cuda_dependent_attributes_\n\n        assert \'mymodule\' not in net_cls.prefixes_\n        assert \'mymodule_\' not in net_cls.cuda_dependent_attributes_\n\n    def test_set_params_on_custom_module(self, net_cls, module_cls):\n        # set_params requires the prefixes_ attribute to be correctly\n        # set, which is what is tested here\n        class MyNet(net_cls):\n            def __init__(self, *args, mymodule=module_cls, **kwargs):\n                self.mymodule = mymodule\n                super().__init__(*args, **kwargs)\n\n            def initialize_module(self, *args, **kwargs):\n                super().initialize_module(*args, **kwargs)\n\n                params = self.get_params_for(\'mymodule\')\n                self.mymodule_ = self.mymodule(**params)\n\n                return self\n\n        net = MyNet(module_cls, mymodule__hidden_units=77).initialize()\n        hidden_units = net.mymodule_.state_dict()[\'sequential.3.weight\'].shape[1]\n        assert hidden_units == 77\n\n        net.set_params(mymodule__hidden_units=99)\n        hidden_units = net.mymodule_.state_dict()[\'sequential.3.weight\'].shape[1]\n        assert hidden_units == 99\n\n\nclass TestNetSparseInput:\n    @pytest.fixture(scope=\'module\')\n    def net_cls(self):\n        from skorch import NeuralNetClassifier\n        return NeuralNetClassifier\n\n    @pytest.fixture(scope=\'module\')\n    def module_cls(self, classifier_module):\n        return classifier_module\n\n    @pytest.fixture\n    def net(self, net_cls, module_cls):\n        return net_cls(module_cls, lr=0.02, max_epochs=20)\n\n    @pytest.fixture\n    def model(self, net):\n        return Pipeline([\n            # TfidfVectorizer returns a scipy sparse CSR matrix\n            (\'tfidf\', TfidfVectorizer(max_features=20, dtype=np.float32)),\n            (\'net\', net),\n        ])\n\n    @pytest.fixture(scope=\'module\')\n    def X(self):\n        with open(__file__, \'r\') as f:\n            lines = f.readlines()\n        return np.asarray(lines)\n\n    @pytest.fixture(scope=\'module\')\n    def y(self, X):\n        return np.array(\n            [1 if (\' def \' in x) or (\' assert \' in x) else 0 for x in X])\n\n    @flaky(max_runs=3)\n    def test_fit_sparse_csr_learns(self, model, X, y):\n        model.fit(X, y)\n        net = model.steps[-1][1]\n        score_start = net.history[0][\'train_loss\']\n        score_end = net.history[-1][\'train_loss\']\n\n        assert score_start > 1.25 * score_end\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=""no cuda device"")\n    @flaky(max_runs=3)\n    def test_fit_sparse_csr_learns_cuda(self, model, X, y):\n        model.set_params(net__device=\'cuda\')\n        model.fit(X, y)\n        net = model.steps[-1][1]\n        score_start = net.history[0][\'train_loss\']\n        score_end = net.history[-1][\'train_loss\']\n\n        assert score_start > 1.25 * score_end\n'"
skorch/tests/test_regressor.py,0,"b'""""""Tests for regressor.py\n\nOnly contains tests that are specific for regressor subclasses.\n\n""""""\n\nfrom flaky import flaky\nimport numpy as np\nimport pytest\nfrom sklearn.base import clone\nimport torch\n\nfrom skorch.tests.conftest import INFERENCE_METHODS\n\n\nclass TestNeuralNetRegressor:\n    @pytest.fixture(scope=\'module\')\n    def data(self, regression_data):\n        return regression_data\n\n    @pytest.fixture(scope=\'module\')\n    def module_cls(self):\n        from skorch.toy import make_regressor\n        return make_regressor(dropout=0.5)\n\n    @pytest.fixture(scope=\'module\')\n    def net_cls(self):\n        from skorch import NeuralNetRegressor\n        return NeuralNetRegressor\n\n    @pytest.fixture(scope=\'module\')\n    def net(self, net_cls, module_cls):\n        return net_cls(\n            module_cls,\n            max_epochs=20,\n            lr=0.1,\n        )\n\n    @pytest.fixture(scope=\'module\')\n    def multioutput_module_cls(self):\n        from skorch.toy import make_regressor\n        return make_regressor(output_units=3, dropout=0.5)\n\n    @pytest.fixture(scope=\'module\')\n    def multioutput_net(self, net_cls, multioutput_module_cls):\n        return net_cls(\n            multioutput_module_cls,\n            max_epochs=1,\n            lr=0.1,\n        )\n\n    @pytest.fixture(scope=\'module\')\n    def net_fit(self, net, data):\n        # Careful, don\'t call additional fits on this, since that would have\n        # side effects on other tests.\n        X, y = data\n        return net.fit(X, y)\n\n    def test_clone(self, net_fit):\n        clone(net_fit)\n\n    def test_fit(self, net_fit):\n        # fitting does not raise anything\n        pass\n\n    @pytest.mark.parametrize(\'method\', INFERENCE_METHODS)\n    def test_not_fitted_raises(self, net_cls, module_cls, data, method):\n        from skorch.exceptions import NotInitializedError\n        net = net_cls(module_cls)\n        X = data[0]\n        with pytest.raises(NotInitializedError) as exc:\n            # we call `list` because `forward_iter` is lazy\n            list(getattr(net, method)(X))\n\n        msg = (""This NeuralNetRegressor instance is not initialized ""\n               ""yet. Call \'initialize\' or \'fit\' with appropriate arguments ""\n               ""before using this method."")\n        assert exc.value.args[0] == msg\n\n    @flaky(max_runs=3)\n    def test_net_learns(self, net, net_cls, data, module_cls):\n        X, y = data\n        net = net_cls(\n            module_cls,\n            max_epochs=10,\n            lr=0.1,\n        )\n        net.fit(X, y)\n        train_losses = net.history[:, \'train_loss\']\n        assert train_losses[0] > 2 * train_losses[-1]\n\n    def test_history_default_keys(self, net_fit):\n        expected_keys = {\'train_loss\', \'valid_loss\', \'epoch\', \'dur\', \'batches\'}\n        for row in net_fit.history:\n            assert expected_keys.issubset(row)\n\n    def test_target_1d_raises(self, net, data):\n        X, y = data\n        with pytest.raises(ValueError) as exc:\n            net.fit(X, y.flatten())\n        assert exc.value.args[0] == (\n            ""The target data shouldn\'t be 1-dimensional but instead have ""\n            ""2 dimensions, with the second dimension having the same size ""\n            ""as the number of regression targets (usually 1). Please ""\n            ""reshape your target data to be 2-dimensional ""\n            ""(e.g. y = y.reshape(-1, 1)."")\n\n    def test_predict_predict_proba(self, net_fit, data):\n        X = data[0]\n        y_pred = net_fit.predict(X)\n\n        # predictions should not be all zeros\n        assert not np.allclose(y_pred, 0)\n\n        y_proba = net_fit.predict_proba(X)\n        # predict and predict_proba should be identical for regression\n        assert np.allclose(y_pred, y_proba, atol=1e-6)\n\n    def test_score(self, net_fit, data):\n        X, y = data\n        r2_score = net_fit.score(X, y)\n        assert r2_score <= 1.\n\n    def test_multioutput_score(self, multioutput_net, multioutput_regression_data):\n        X, y = multioutput_regression_data\n        multioutput_net.fit(X, y)\n        r2_score = multioutput_net.score(X, y)\n        assert r2_score <= 1.\n'"
skorch/tests/test_setter.py,1,"b'""""""Tests for virtual parameter setters""""""\nfrom unittest.mock import Mock\n\nimport pytest\n\n\nclass TestOptimizerSetter:\n\n    @pytest.fixture\n    def net_dummy(self):\n        from skorch import NeuralNet\n        net = Mock(spec=NeuralNet)\n        net.lr = 0.01\n        return net\n\n    @pytest.fixture\n    def optimizer_dummy(self):\n        from torch.optim import Optimizer\n        optim = Mock(spec=Optimizer)\n        optim.param_groups = [\n            {\'lr\': 0.01, \'momentum\': 0.9},\n            {\'lr\': 0.02, \'momentum\': 0.9}\n        ]\n        return optim\n\n    @pytest.fixture(scope=\'function\')\n    def net_optim_dummy(self, net_dummy, optimizer_dummy):\n        net_dummy.optimizer_ = optimizer_dummy\n        return net_dummy\n\n    @pytest.fixture\n    def setter(self):\n        from skorch.setter import optimizer_setter\n        return optimizer_setter\n\n    def test_lr_attribute_is_updated(self, setter, net_optim_dummy):\n        new_lr = net_optim_dummy.lr + 1\n        setter(net_optim_dummy, \'lr\', new_lr)\n\n        assert net_optim_dummy.lr == new_lr\n\n    def test_wrong_name_raises(self, setter, net_optim_dummy):\n        # should be \'param_groups\' instead\n        param = \'optimizer__param_group__0__lr\'\n        value = 0.1\n        with pytest.raises(AttributeError) as e:\n            setter(net_optim_dummy, param, value)\n\n        assert e.value.args[0] == (\n            \'Invalid parameter ""{param}"" for optimizer ""optimizer""\'\n            .format(param=param)\n        )\n\n    @pytest.mark.parametrize(\'group\', [0, 1])\n    @pytest.mark.parametrize(\'sub_param, value\', [\n        (\'momentum\', 0.1),\n        (\'lr\', 0.3),\n    ])\n    def test_only_specific_param_group_updated(self, setter, net_optim_dummy,\n                                               group, sub_param, value):\n        pgroups = net_optim_dummy.optimizer_.param_groups\n        param = \'optimizer__param_groups__{}__{}\'.format(group, sub_param)\n\n        updated_group_pre = [g for i, g in enumerate(pgroups) if i == group]\n        static_groups_pre = [g for i, g, in enumerate(pgroups) if i != group]\n        assert len(updated_group_pre) == 1\n\n        setter(net_optim_dummy, param, value)\n\n        updated_group_new = [g for i, g in enumerate(pgroups) if i == group]\n        static_groups_new = [g for i, g, in enumerate(pgroups) if i != group]\n\n        assert updated_group_new[0][sub_param] == value\n        assert all(old[sub_param] == new[sub_param] for old, new in zip(\n            static_groups_pre, static_groups_new))\n'"
skorch/tests/test_toy.py,1,"b'""""""Tests for toy.py.""""""\n\nimport numpy as np\nimport pytest\nimport torch\nfrom torch import nn\n\n\nclass TestMLPModule:\n    @pytest.fixture\n    def module_cls(self):\n        from skorch.toy import MLPModule\n        return MLPModule\n\n    def test_one_hidden(self, module_cls):\n        module = module_cls()\n        parameters = list(module.named_parameters())\n\n        # 2 linear * (weight + bias)\n        assert len(parameters) == 4\n\n        # 2 linear, 1 relu, 1 dropout\n        assert len(module.sequential) == 4\n        assert isinstance(module.sequential[0], nn.Linear)\n        assert isinstance(module.sequential[1], nn.ReLU)\n        assert isinstance(module.sequential[2], nn.Dropout)\n        assert isinstance(module.sequential[3], nn.Linear)\n\n    def test_two_hidden(self, module_cls):\n        module = module_cls(num_hidden=2)\n        parameters = list(module.named_parameters())\n\n        # 3 linear * (weight + bias)\n        assert len(parameters) == 6\n\n        # 3 linear, 2 relu, 2 dropout\n        assert len(module.sequential) == 7\n        assert isinstance(module.sequential[0], nn.Linear)\n        assert isinstance(module.sequential[1], nn.ReLU)\n        assert isinstance(module.sequential[2], nn.Dropout)\n        assert isinstance(module.sequential[3], nn.Linear)\n        assert isinstance(module.sequential[4], nn.ReLU)\n        assert isinstance(module.sequential[5], nn.Dropout)\n        assert isinstance(module.sequential[6], nn.Linear)\n\n    @pytest.mark.parametrize(\'num_hidden\', [0, 1, 2, 5, 10])\n    def test_many_hidden(self, module_cls, num_hidden):\n        module = module_cls(num_hidden=num_hidden)\n        parameters = list(module.named_parameters())\n\n        assert len(parameters) == 2 * (num_hidden + 1)\n        assert len(module.sequential) == (3 * num_hidden) + 1\n\n    def test_output_nonlin(self, module_cls):\n        module = module_cls(output_nonlin=nn.Sigmoid())\n\n        # 2 linear, 1 relu, 1 dropout, 1 sigmoid\n        assert len(module.sequential) == 5\n        assert isinstance(module.sequential[0], nn.Linear)\n        assert isinstance(module.sequential[1], nn.ReLU)\n        assert isinstance(module.sequential[2], nn.Dropout)\n        assert isinstance(module.sequential[3], nn.Linear)\n        assert isinstance(module.sequential[4], nn.Sigmoid)\n\n    def test_output_squeezed(self, module_cls):\n        X = torch.zeros((5, 20)).float()\n\n        module = module_cls(output_units=1)\n        y = module(X)\n        assert y.dim() == 2\n\n        module = module_cls(squeeze_output=True, output_units=1)\n        y = module(X)\n        assert y.dim() == 1\n\n    def test_dropout(self, module_cls):\n        module = module_cls(dropout=0.567)\n        assert np.isclose(module.sequential[2].p, 0.567)\n\n    def test_make_classifier(self):\n        from skorch.toy import make_classifier\n        module = make_classifier()()\n        assert isinstance(module.sequential[-1], nn.Softmax)\n\n    def test_make_binary_classifier(self):\n        from skorch.toy import make_binary_classifier\n        module = make_binary_classifier()()\n        assert isinstance(module.sequential[-1], nn.Linear)\n        assert module.squeeze_output is True\n\n    def test_make_regressor(self):\n        from skorch.toy import make_regressor\n        module = make_regressor()()\n        assert module.sequential[-1].out_features == 1\n'"
skorch/tests/test_utils.py,39,"b'""""""Test for utils.py""""""\n\nimport numpy as np\nimport pytest\nfrom scipy import sparse\nimport torch\nfrom torch.nn.utils.rnn import PackedSequence\nfrom torch.nn.utils.rnn import pack_padded_sequence\n\nfrom skorch.tests.conftest import pandas_installed\n\n\nclass TestToTensor:\n    @pytest.fixture\n    def to_tensor(self):\n        from skorch.utils import to_tensor\n        return to_tensor\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=""no cuda device"")\n    def test_device_setting_cuda(self, to_tensor):\n        x = np.ones((2, 3, 4))\n        t = to_tensor(x, device=\'cpu\')\n        assert t.device.type == \'cpu\'\n\n        t = to_tensor(x, device=\'cuda\')\n        assert t.device.type.startswith(\'cuda\')\n\n        t = to_tensor(t, device=\'cuda\')\n        assert t.device.type.startswith(\'cuda\')\n\n        t = to_tensor(t, device=\'cpu\')\n        assert t.device.type == \'cpu\'\n\n    def tensors_equal(self, x, y):\n        """"""""Test that tensors in diverse containers are equal.""""""\n        if isinstance(x, PackedSequence):\n            return self.tensors_equal(x[0], y[0]) and self.tensors_equal(x[1], y[1])\n\n        if isinstance(x, dict):\n            return (\n                (x.keys() == y.keys()) and\n                all(self.tensors_equal(x[k], y[k]) for k in x)\n            )\n\n        if isinstance(x, (list, tuple)):\n            return all(self.tensors_equal(xi, yi) for xi, yi in zip(x, y))\n\n        if x.is_sparse is not y.is_sparse:\n            return False\n\n        if x.is_sparse:\n            x, y = x.to_dense(), y.to_dense()\n\n        return (x == y).all()\n\n    # pylint: disable=no-method-argument\n    def parameters():\n        """"""Yields data, expected value, and device for tensor conversion\n        test.\n\n        Stops earlier when no cuda device is available.\n\n        """"""\n        device = \'cpu\'\n        x = torch.zeros((5, 3)).float()\n        y = torch.as_tensor([2, 2, 1])\n        z = np.arange(15).reshape(5, 3)\n        for X, expected in [\n                (x, x),\n                (y, y),\n                ([x, y], [x, y]),\n                ((x, y), (x, y)),\n                (z, torch.as_tensor(z)),\n                (\n                    {\'a\': x, \'b\': y, \'c\': z},\n                    {\'a\': x, \'b\': y, \'c\': torch.as_tensor(z)}\n                ),\n                (torch.as_tensor(55), torch.as_tensor(55)),\n                (pack_padded_sequence(x, y), pack_padded_sequence(x, y)),\n        ]:\n            yield X, expected, device\n\n        if not torch.cuda.is_available():\n            return\n\n        device = \'cuda\'\n        x = x.to(\'cuda\')\n        y = y.to(\'cuda\')\n        for X, expected in [\n                (x, x),\n                (y, y),\n                ([x, y], [x, y]),\n                ((x, y), (x, y)),\n                (z, torch.as_tensor(z).to(\'cuda\')),\n                (\n                    {\'a\': x, \'b\': y, \'c\': z},\n                    {\'a\': x, \'b\': y, \'c\': torch.as_tensor(z).to(\'cuda\')}\n                ),\n                (torch.as_tensor(55), torch.as_tensor(55).to(\'cuda\')),\n                (\n                    pack_padded_sequence(x, y),\n                    pack_padded_sequence(x, y).to(\'cuda\')\n                ),\n        ]:\n            yield X, expected, device\n\n    @pytest.mark.parametrize(\'X, expected, device\', parameters())\n    def test_tensor_conversion_cuda(self, to_tensor, X, expected, device):\n        result = to_tensor(X, device)\n        assert self.tensors_equal(result, expected)\n        assert self.tensors_equal(expected, result)\n\n    @pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\n    def test_sparse_tensor(self, to_tensor, device):\n        if device == \'cuda\' and not torch.cuda.is_available():\n            pytest.skip()\n\n        inp = sparse.csr_matrix(np.zeros((5, 3)).astype(np.float32))\n        expected = torch.sparse_coo_tensor(size=(5, 3)).to(device)\n\n        result = to_tensor(inp, device=device, accept_sparse=True)\n        assert self.tensors_equal(result, expected)\n\n    @pytest.mark.parametrize(\'device\', [\'cpu\', \'cuda\'])\n    def test_sparse_tensor_not_accepted_raises(self, to_tensor, device):\n        if device == \'cuda\' and not torch.cuda.is_available():\n            pytest.skip()\n\n        inp = sparse.csr_matrix(np.zeros((5, 3)).astype(np.float32))\n        with pytest.raises(TypeError) as exc:\n            to_tensor(inp, device=device)\n\n        msg = (""Sparse matrices are not supported. Set ""\n               ""accept_sparse=True to allow sparse matrices."")\n        assert exc.value.args[0] == msg\n\n\nclass TestToDevice:\n    @pytest.fixture\n    def to_device(self):\n        from skorch.utils import to_device\n        return to_device\n\n    @pytest.fixture\n    def x(self):\n        return torch.zeros(3)\n\n    @pytest.fixture\n    def x_tup(self):\n        return torch.zeros(3), torch.ones((4, 5))\n\n    @pytest.fixture\n    def x_pad_seq(self):\n        value = torch.zeros((5, 3)).float()\n        length = torch.as_tensor([2, 2, 1])\n        return pack_padded_sequence(value, length)\n\n    def check_device_type(self, tensor, device_input, prev_device):\n        """"""assert expected device type conditioned on the input argument for `to_device`""""""\n        if None is device_input:\n            assert tensor.device.type == prev_device\n\n        else:\n            assert tensor.device.type == device_input\n\n    @pytest.mark.parametrize(\'device_from, device_to\', [\n        (\'cpu\', \'cpu\'),\n        (\'cpu\', \'cuda\'),\n        (\'cuda\', \'cpu\'),\n        (\'cuda\', \'cuda\'),\n        (None, None),\n    ])\n    def test_check_device_torch_tensor(self, to_device, x, device_from, device_to):\n        if \'cuda\' in (device_from, device_to) and not torch.cuda.is_available():\n            pytest.skip()\n\n        prev_device = None\n        if None in (device_from, device_to):\n            prev_device = x.device.type\n\n        x = to_device(x, device=device_from)\n        self.check_device_type(x, device_from, prev_device)\n\n        x = to_device(x, device=device_to)\n        self.check_device_type(x, device_to, prev_device)\n\n    @pytest.mark.parametrize(\'device_from, device_to\', [\n        (\'cpu\', \'cpu\'),\n        (\'cpu\', \'cuda\'),\n        (\'cuda\', \'cpu\'),\n        (\'cuda\', \'cuda\'),\n        (None, None),\n    ])\n    def test_check_device_tuple_torch_tensor(\n            self, to_device, x_tup, device_from, device_to):\n        if \'cuda\' in (device_from, device_to) and not torch.cuda.is_available():\n            pytest.skip()\n\n        prev_devices = [None for _ in range(len(x_tup))]\n        if None in (device_from, device_to):\n            prev_devices = [x.device.type for x in x_tup]\n\n        x_tup = to_device(x_tup, device=device_from)\n        for xi, prev_d in zip(x_tup, prev_devices):\n            self.check_device_type(xi, device_from, prev_d)\n\n        x_tup = to_device(x_tup, device=device_to)\n        for xi, prev_d in zip(x_tup, prev_devices):\n            self.check_device_type(xi, device_to, prev_d)\n\n    @pytest.mark.parametrize(\'device_from, device_to\', [\n        (\'cpu\', \'cpu\'),\n        (\'cpu\', \'cuda\'),\n        (\'cuda\', \'cpu\'),\n        (\'cuda\', \'cuda\'),\n        (None, None),\n    ])\n    def test_check_device_packed_padded_sequence(\n            self, to_device, x_pad_seq, device_from, device_to):\n        if \'cuda\' in (device_from, device_to) and not torch.cuda.is_available():\n            pytest.skip()\n\n        prev_device = None\n        if None in (device_from, device_to):\n            prev_device = x_pad_seq.data.device.type\n\n        x_pad_seq = to_device(x_pad_seq, device=device_from)\n        self.check_device_type(x_pad_seq.data, device_from, prev_device)\n\n        x_pad_seq = to_device(x_pad_seq, device=device_to)\n        self.check_device_type(x_pad_seq.data, device_to, prev_device)\n\n\nclass TestDuplicateItems:\n    @pytest.fixture\n    def duplicate_items(self):\n        from skorch.utils import duplicate_items\n        return duplicate_items\n\n    @pytest.mark.parametrize(\'collections\', [\n        ([],),\n        ([], []),\n        ([], [], []),\n        ([1, 2]),\n        ([1, 2], [3]),\n        ([1, 2], [3, \'1\']),\n        ([1], [2], [3], [4]),\n        ({\'1\': 1}, [2]),\n        ({\'1\': 1}, {\'2\': 1}, (\'3\', \'4\')),\n    ])\n    def test_no_duplicates(self, duplicate_items, collections):\n        assert duplicate_items(*collections) == set()\n\n    @pytest.mark.parametrize(\'collections, expected\', [\n        ([1, 1], {1}),\n        ([\'1\', \'1\'], {\'1\'}),\n        ([[1], [1]], {1}),\n        ([[1, 2, 1], [1]], {1}),\n        ([[1, 1], [2, 2]], {1, 2}),\n        ([[1], {1: \'2\', 2: \'2\'}], {1}),\n        ([[1, 2], [3, 4], [2], [3]], {2, 3}),\n        ([{\'1\': 1}, {\'1\': 1}, (\'3\', \'4\')], {\'1\'}),\n    ])\n    def test_duplicates(self, duplicate_items, collections, expected):\n        assert duplicate_items(*collections) == expected\n\n\nclass TestParamsFor:\n    @pytest.fixture\n    def params_for(self):\n        from skorch.utils import params_for\n        return params_for\n\n    @pytest.mark.parametrize(\'prefix, kwargs, expected\', [\n        (\'p1\', {\'p1__a\': 1, \'p1__b\': 2}, {\'a\': 1, \'b\': 2}),\n        (\'p2\', {\'p1__a\': 1, \'p1__b\': 2}, {}),\n        (\'p1\', {\'p1__a\': 1, \'p1__b\': 2, \'p2__a\': 3}, {\'a\': 1, \'b\': 2}),\n        (\'p2\', {\'p1__a\': 1, \'p1__b\': 2, \'p2__a\': 3}, {\'a\': 3}),\n    ])\n    def test_params_for(self, params_for, prefix, kwargs, expected):\n        assert params_for(prefix, kwargs) == expected\n\n\nclass TestDataFromDataset:\n    @pytest.fixture\n    def data_from_dataset(self):\n        from skorch.utils import data_from_dataset\n        return data_from_dataset\n\n    @pytest.fixture\n    def data(self):\n        X = np.arange(8).reshape(4, 2)\n        y = np.array([1, 3, 0, 2])\n        return X, y\n\n    @pytest.fixture\n    def skorch_ds(self, data):\n        from skorch.dataset import Dataset\n        return Dataset(*data)\n\n    @pytest.fixture\n    def subset(self, skorch_ds):\n        from torch.utils.data.dataset import Subset\n        return Subset(skorch_ds, [1, 3])\n\n    @pytest.fixture\n    def subset_subset(self, subset):\n        from torch.utils.data.dataset import Subset\n        return Subset(subset, [0])\n\n    # pylint: disable=missing-docstring\n    @pytest.fixture\n    def other_ds(self, data):\n        class MyDataset:\n            """"""Non-compliant dataset""""""\n            def __init__(self, data):\n                self.data = data\n\n            def __getitem__(self, idx):\n                return self.data[0][idx], self.data[1][idx]\n\n            def __len__(self):\n                return len(self.data[0])\n        return MyDataset(data)\n\n    def test_with_skorch_ds(self, data_from_dataset, data, skorch_ds):\n        X, y = data_from_dataset(skorch_ds)\n        assert (X == data[0]).all()\n        assert (y == data[1]).all()\n\n    def test_with_subset(self, data_from_dataset, data, subset):\n        X, y = data_from_dataset(subset)\n        assert (X == data[0][[1, 3]]).all()\n        assert (y == data[1][[1, 3]]).all()\n\n    def test_with_subset_subset(self, data_from_dataset, data, subset_subset):\n        X, y = data_from_dataset(subset_subset)\n        assert (X == data[0][1]).all()\n        assert (y == data[1][1]).all()\n\n    def test_with_other_ds(self, data_from_dataset, other_ds):\n        with pytest.raises(AttributeError):\n            data_from_dataset(other_ds)\n\n    def test_with_dict_data(self, data_from_dataset, data, subset):\n        subset.dataset.X = {\'X\': subset.dataset.X}\n        X, y = data_from_dataset(subset)\n        assert (X[\'X\'] == data[0][[1, 3]]).all()\n        assert (y == data[1][[1, 3]]).all()\n\n    def test_subset_with_y_none(self, data_from_dataset, data, subset):\n        subset.dataset.y = None\n        X, y = data_from_dataset(subset)\n        assert (X == data[0][[1, 3]]).all()\n        assert y is None\n\n\nclass TestMultiIndexing:\n    @pytest.fixture\n    def multi_indexing(self):\n        from skorch.dataset import multi_indexing\n        return multi_indexing\n\n    @pytest.mark.parametrize(\'data, i, expected\', [\n        (\n            np.arange(12).reshape(4, 3),\n            slice(None),\n            np.arange(12).reshape(4, 3),\n        ),\n        (\n            np.arange(12).reshape(4, 3),\n            np.s_[2],\n            np.array([6, 7, 8]),\n        ),\n        (\n            np.arange(12).reshape(4, 3),\n            np.s_[-2:],\n            np.array([[6, 7, 8], [9, 10, 11]]),\n        ),\n    ])\n    def test_ndarray(self, multi_indexing, data, i, expected):\n        result = multi_indexing(data, i)\n        assert np.allclose(result, expected)\n\n    @pytest.mark.parametrize(\'data, i, expected\', [\n        (\n            torch.arange(0, 12).view(4, 3),\n            slice(None),\n            np.arange(12).reshape(4, 3),\n        ),\n        (\n            torch.arange(0, 12).view(4, 3),\n            np.s_[2],\n            np.array([6, 7, 8]),\n        ),\n        (\n            torch.arange(0, 12).view(4, 3),\n            np.int64(2),\n            np.array([6, 7, 8]),\n        ),\n        (\n            torch.arange(0, 12).view(4, 3),\n            np.s_[-2:],\n            np.array([[6, 7, 8], [9, 10, 11]]),\n        ),\n    ])\n    def test_torch_tensor(self, multi_indexing, data, i, expected):\n        result = multi_indexing(data, i).long().numpy()\n        assert np.allclose(result, expected)\n\n    @pytest.mark.parametrize(\'data, i, expected\', [\n        ([1, 2, 3, 4], slice(None), [1, 2, 3, 4]),\n        ([1, 2, 3, 4], slice(None, 2), [1, 2]),\n        ([1, 2, 3, 4], 2, 3),\n        ([1, 2, 3, 4], -2, 3),\n    ])\n    def test_list(self, multi_indexing, data, i, expected):\n        result = multi_indexing(data, i)\n        assert np.allclose(result, expected)\n\n    @pytest.mark.parametrize(\'data, i, expected\', [\n        ({\'a\': [0, 1, 2], \'b\': [3, 4, 5]}, 0, {\'a\': 0, \'b\': 3}),\n        (\n            {\'a\': [0, 1, 2], \'b\': [3, 4, 5]},\n            np.s_[:2],\n            {\'a\': [0, 1], \'b\': [3, 4]},\n        )\n    ])\n    def test_dict_of_lists(self, multi_indexing, data, i, expected):\n        result = multi_indexing(data, i)\n        assert result == expected\n\n    @pytest.mark.parametrize(\'data, i, expected\', [\n        (\n            {\'a\': np.arange(3), \'b\': np.arange(3, 6)},\n            0,\n            {\'a\': 0, \'b\': 3}\n        ),\n        (\n            {\'a\': np.arange(3), \'b\': np.arange(3, 6)},\n            np.s_[:2],\n            {\'a\': np.arange(2), \'b\': np.arange(3, 5)}\n        ),\n    ])\n    def test_dict_of_arrays(self, multi_indexing, data, i, expected):\n        result = multi_indexing(data, i)\n        assert result.keys() == expected.keys()\n        for k in result:\n            assert np.allclose(result[k], expected[k])\n\n    @pytest.mark.parametrize(\'data, i, expected\', [\n        (\n            {\'a\': torch.arange(0, 3), \'b\': torch.arange(3, 6)},\n            0,\n            {\'a\': 0, \'b\': 3}\n        ),\n        (\n            {\'a\': torch.arange(0, 3), \'b\': torch.arange(3, 6)},\n            np.s_[:2],\n            {\'a\': np.arange(2), \'b\': np.arange(3, 5)}\n        ),\n    ])\n    def test_dict_of_torch_tensors(self, multi_indexing, data, i, expected):\n        result = multi_indexing(data, i)\n        assert result.keys() == expected.keys()\n        for k in result:\n            try:\n                val = result[k].long().numpy()\n            except AttributeError:\n                val = result[k]\n            assert np.allclose(val, expected[k])\n\n    def test_mixed_data(self, multi_indexing):\n        data = [\n            [1, 2, 3],\n            np.arange(3),\n            torch.arange(3, 6),\n            {\'a\': [4, 5, 6], \'b\': [7, 8, 9]},\n        ]\n        result = multi_indexing(data, 0)\n        expected = [1, 0, 3, {\'a\': 4, \'b\': 7}]\n        assert result == expected\n\n    def test_mixed_data_slice(self, multi_indexing):\n        data = [\n            [1, 2, 3],\n            np.arange(3),\n            torch.arange(3, 6),\n            {\'a\': [4, 5, 6], \'b\': [7, 8, 9]},\n        ]\n        result = multi_indexing(data, np.s_[:2])\n        assert result[0] == [1, 2]\n        assert np.allclose(result[1], np.arange(2))\n        assert np.allclose(result[2].long().numpy(), np.arange(3, 5))\n        assert result[3] == {\'a\': [4, 5], \'b\': [7, 8]}\n\n    @pytest.fixture\n    def pd(self):\n        if not pandas_installed:\n            pytest.skip()\n        import pandas as pd\n        return pd\n\n    def test_pandas_dataframe(self, multi_indexing, pd):\n        df = pd.DataFrame({\'a\': [0, 1, 2], \'b\': [3, 4, 5]}, index=[2, 1, 0])\n        result = multi_indexing(df, 0)\n        # Note: taking one row of a DataFrame returns a Series\n        expected = pd.Series(data=[0, 3], index=[\'a\', \'b\'], name=2)\n        assert result.equals(expected)\n\n    def test_pandas_dataframe_slice(self, multi_indexing, pd):\n        import pandas as pd\n        df = pd.DataFrame({\'a\': [0, 1, 2], \'b\': [3, 4, 5]}, index=[2, 1, 0])\n        result = multi_indexing(df, np.s_[:2])\n        expected = pd.DataFrame({\'a\': [0, 1], \'b\': [3, 4]}, index=[2, 1])\n        assert result.equals(expected)\n\n    def test_pandas_series(self, multi_indexing, pd):\n        series = pd.Series(data=[0, 1, 2], index=[2, 1, 0])\n        result = multi_indexing(series, 0)\n        assert result == 0\n\n    def test_pandas_series_slice(self, multi_indexing, pd):\n        series = pd.Series(data=[0, 1, 2], index=[2, 1, 0])\n        result = multi_indexing(series, np.s_[:2])\n        expected = pd.Series(data=[0, 1], index=[2, 1])\n        assert result.equals(expected)\n\n    def test_list_of_dataframe_and_series(self, multi_indexing, pd):\n        data = [\n            pd.DataFrame({\'a\': [0, 1, 2], \'b\': [3, 4, 5]}, index=[2, 1, 0]),\n            pd.Series(data=[0, 1, 2], index=[2, 1, 0]),\n        ]\n        result = multi_indexing(data, 0)\n        assert result[0].equals(\n            pd.Series(data=[0, 3], index=[\'a\', \'b\'], name=2))\n        assert result[1] == 0\n\n    def test_list_of_dataframe_and_series_slice(self, multi_indexing, pd):\n        data = [\n            pd.DataFrame({\'a\': [0, 1, 2], \'b\': [3, 4, 5]}, index=[2, 1, 0]),\n            pd.Series(data=[0, 1, 2], index=[2, 1, 0]),\n        ]\n        result = multi_indexing(data, np.s_[:2])\n        assert result[0].equals(\n            pd.DataFrame({\'a\': [0, 1], \'b\': [3, 4]}, index=[2, 1]))\n        assert result[1].equals(pd.Series(data=[0, 1], index=[2, 1]))\n\n    def test_index_torch_tensor_with_numpy_int_array(self, multi_indexing):\n        X = torch.zeros((1000, 10))\n        i = np.arange(100)\n        result = multi_indexing(X, i)\n        assert (result == X[:100]).all()\n\n    def test_index_torch_tensor_with_numpy_bool_array(self, multi_indexing):\n        X = torch.zeros((1000, 10))\n        i = np.asarray([True] * 100 + [False] * 900)\n        result = multi_indexing(X, i)\n        assert (result == X[:100]).all()\n\n    def test_index_with_float_array_raises(self, multi_indexing):\n        # sklearn < 0.22 raises IndexError with msg0\n        # sklearn >= 0.22 raises ValueError with msg1\n        X = np.zeros(10)\n        i = np.arange(3, 0.5)\n\n        with pytest.raises((IndexError, ValueError)) as exc:\n            multi_indexing(X, i)\n\n        msg0 = ""arrays used as indices must be of integer (or boolean) type""\n        msg1 = (""No valid specification of the columns. Only a scalar, list or ""\n                ""slice of all integers or all strings, or boolean mask is allowed"")\n\n        result = exc.value.args[0]\n        assert result in (msg0, msg1)\n\n    def test_boolean_index_2d(self, multi_indexing):\n        X = np.arange(9).reshape(3, 3)\n        i = np.eye(3).astype(bool)\n        result = multi_indexing(X, i)\n        expected = np.asarray([0, 4, 8])\n        assert np.allclose(result, expected)\n\n    def test_boolean_index_2d_with_torch_tensor(self, multi_indexing):\n        X = torch.LongTensor(np.arange(9).reshape(3, 3))\n        i = np.eye(3).astype(bool)\n\n        res = multi_indexing(X, i)\n        expected = torch.LongTensor([0, 4, 8])\n        assert all(res == expected)\n\n    @pytest.mark.parametrize(\'data, i, expected\', [\n        (\n            np.arange(12).reshape(4, 3),\n            slice(None),\n            np.arange(12).reshape(4, 3),\n        ),\n        (\n            np.arange(12).reshape(4, 3),\n            np.s_[2],\n            np.array([6, 7, 8]),\n        ),\n        (\n            np.arange(12).reshape(4, 3),\n            np.s_[-2:],\n            np.array([[6, 7, 8], [9, 10, 11]]),\n        ),\n    ])\n    def test_sparse_csr_matrix(self, multi_indexing, data, i, expected):\n        data = sparse.csr_matrix(data)\n        result = multi_indexing(data, i).toarray()\n        assert np.allclose(result, expected)\n\n\nclass TestIsSkorchDataset:\n\n    @pytest.fixture\n    def is_skorch_dataset(self):\n        from skorch.utils import is_skorch_dataset\n        return is_skorch_dataset\n\n    # pylint: disable=no-method-argument\n    def type_truth_table():\n        """"""Return a table of (type, bool) tuples that describe what\n        is_skorch_dataset should return when called with that type.\n        """"""\n        from skorch.dataset import Dataset\n        from torch.utils.data.dataset import Subset\n\n        numpy_data = np.array([1, 2, 3])\n        tensor_data = torch.from_numpy(numpy_data)\n        torch_dataset = torch.utils.data.TensorDataset(\n            tensor_data, tensor_data)\n        torch_subset = Subset(torch_dataset, [1, 2])\n        skorch_dataset = Dataset(numpy_data)\n        skorch_subset = Subset(skorch_dataset, [1, 2])\n\n        return [\n            (numpy_data, False),\n            (torch_dataset, False),\n            (torch_subset, False),\n            (skorch_dataset, True),\n            (skorch_subset, True),\n        ]\n\n    @pytest.mark.parametrize(\n        \'input_data,expected\',\n        type_truth_table())\n    def test_data_types(self, is_skorch_dataset, input_data, expected):\n        assert is_skorch_dataset(input_data) == expected\n\n\nclass TestTeeGenerator:\n\n    @pytest.fixture\n    def lazy_generator_cls(self):\n        from skorch.utils import TeeGenerator\n        return TeeGenerator\n\n    def test_returns_copies_of_generator(self, lazy_generator_cls):\n        expected_list = [1, 2, 3]\n\n        def list_gen():\n            yield from expected_list\n        lazy_gen = lazy_generator_cls(list_gen())\n\n        first_return = list(lazy_gen)\n        second_return = [item for item in lazy_gen]\n\n        assert first_return == expected_list\n        assert second_return == expected_list\n'"
skorch/tests/callbacks/test_all.py,0,"b'import itertools\n\nimport pytest\n\n\nclass TestAllCallbacks:\n    @pytest.fixture\n    def callbacks(self):\n        """"""Return all callbacks""""""\n        import skorch.callbacks\n\n        callbacks = []\n        for name in dir(skorch.callbacks):\n            attr = getattr(skorch.callbacks, name)\n            # pylint: disable=unidiomatic-typecheck\n            if not type(attr) is type:\n                continue\n            if issubclass(attr, skorch.callbacks.Callback):\n                callbacks.append(attr)\n        return callbacks\n\n    @pytest.fixture\n    def base_cls(self):\n        from skorch.callbacks import Callback\n        return Callback\n\n    @pytest.fixture\n    def on_x_methods(self):\n        return [\n            \'on_train_begin\',\n            \'on_train_end\',\n            \'on_epoch_begin\',\n            \'on_epoch_end\',\n            \'on_batch_begin\',\n            \'on_batch_end\',\n            \'on_grad_computed\',\n        ]\n\n    def test_on_x_methods_have_kwargs(self, callbacks, on_x_methods):\n        import inspect\n        for callback, method_name in itertools.product(\n                callbacks, on_x_methods):\n            method = getattr(callback, method_name)\n            assert ""kwargs"" in inspect.signature(method).parameters\n\n    def test_set_params_with_unknown_key_raises(self, base_cls):\n        with pytest.raises(ValueError) as exc:\n            base_cls().set_params(foo=123)\n\n        # TODO: check error message more precisely, depending on what\n        # the intended message shouldb e from sklearn side\n        assert exc.value.args[0].startswith(\'Invalid parameter foo for\')\n'"
skorch/tests/callbacks/test_logging.py,2,"b'""""""Tests for callbacks/logging.py""""""\n\nfrom functools import partial\nimport os\nfrom unittest.mock import Mock\nfrom unittest.mock import call, patch\n\nimport numpy as np\nimport pytest\nimport torch\nfrom torch import nn\n\nfrom skorch.tests.conftest import neptune_installed\nfrom skorch.tests.conftest import wandb_installed\nfrom skorch.tests.conftest import tensorboard_installed\n\n\n@pytest.mark.skipif(\n    not neptune_installed, reason=\'neptune is not installed\')\nclass TestNeptune:\n    @pytest.fixture\n    def net_cls(self):\n        from skorch import NeuralNetClassifier\n        return NeuralNetClassifier\n\n    @pytest.fixture\n    def data(self, classifier_data):\n        X, y = classifier_data\n        # accelerate training since we don\'t care for the loss\n        X, y = X[:40], y[:40]\n        return X, y\n\n    @pytest.fixture\n    def neptune_logger_cls(self):\n        from skorch.callbacks import NeptuneLogger\n        return NeptuneLogger\n\n    @pytest.fixture\n    def neptune_experiment_cls(self):\n        import neptune\n        neptune.init(project_qualified_name=""tests/dry-run"",\n                     backend=neptune.OfflineBackend())\n        return neptune.create_experiment\n\n    @pytest.fixture\n    def mock_experiment(self, neptune_experiment_cls):\n        mock = Mock(spec=neptune_experiment_cls)\n        mock.log_metric = Mock()\n        mock.stop = Mock()\n        return mock\n\n    @pytest.fixture\n    def net_fitted(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            neptune_logger_cls,\n            mock_experiment,\n    ):\n        return net_cls(\n            classifier_module,\n            callbacks=[neptune_logger_cls(mock_experiment)],\n            max_epochs=3,\n        ).fit(*data)\n\n    def test_experiment_closed_automatically(self, net_fitted, mock_experiment):\n        assert mock_experiment.stop.call_count == 1\n\n    def test_experiment_not_closed(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            neptune_logger_cls,\n            mock_experiment,\n    ):\n        net_cls(\n            classifier_module,\n            callbacks=[\n                neptune_logger_cls(mock_experiment, close_after_train=False)],\n            max_epochs=2,\n        ).fit(*data)\n        assert mock_experiment.stop.call_count == 0\n\n    def test_ignore_keys(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            neptune_logger_cls,\n            mock_experiment,\n    ):\n        # ignore \'dur\' and \'valid_loss\', \'unknown\' doesn\'t exist but\n        # this should not cause a problem\n        npt = neptune_logger_cls(\n            mock_experiment, keys_ignored=[\'dur\', \'valid_loss\', \'unknown\'])\n        net_cls(\n            classifier_module,\n            callbacks=[npt],\n            max_epochs=3,\n        ).fit(*data)\n\n        # 3 epochs x 2 epoch metrics = 6 calls\n        assert mock_experiment.log_metric.call_count == 6\n        call_args = [args[0][0] for args in mock_experiment.log_metric.call_args_list]\n        assert \'valid_loss\' not in call_args\n\n    def test_keys_ignored_is_string(self, neptune_logger_cls, mock_experiment):\n        npt = neptune_logger_cls(\n            mock_experiment, keys_ignored=\'a-key\').initialize()\n        expected = {\'a-key\', \'batches\'}\n        assert npt.keys_ignored_ == expected\n\n    def test_fit_with_real_experiment(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            neptune_logger_cls,\n            neptune_experiment_cls,\n    ):\n        net = net_cls(\n            classifier_module,\n            callbacks=[neptune_logger_cls(neptune_experiment_cls())],\n            max_epochs=5,\n        )\n        net.fit(*data)\n\n    def test_log_on_batch_level_on(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            neptune_logger_cls,\n            mock_experiment,\n    ):\n        net = net_cls(\n            classifier_module,\n            callbacks=[neptune_logger_cls(mock_experiment, log_on_batch_end=True)],\n            max_epochs=5,\n            batch_size=4,\n            train_split=False\n        )\n        net.fit(*data)\n\n        # 5 epochs x (40/4 batches x 2 batch metrics + 2 epoch metrics) = 110 calls\n        assert mock_experiment.log_metric.call_count == 110\n        mock_experiment.log_metric.assert_any_call(\'train_batch_size\', 4)\n\n    def test_log_on_batch_level_off(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            neptune_logger_cls,\n            mock_experiment,\n    ):\n        net = net_cls(\n            classifier_module,\n            callbacks=[neptune_logger_cls(mock_experiment, log_on_batch_end=False)],\n            max_epochs=5,\n            batch_size=4,\n            train_split=False\n        )\n        net.fit(*data)\n\n        # 5 epochs x 2 epoch metrics = 10 calls\n        assert mock_experiment.log_metric.call_count == 10\n        call_args_list = mock_experiment.log_metric.call_args_list\n        assert call(\'train_batch_size\', 4) not in call_args_list\n\n    def test_first_batch_flag(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            neptune_logger_cls,\n            neptune_experiment_cls,\n    ):\n        npt = neptune_logger_cls(neptune_experiment_cls())\n        npt.initialize()\n        assert npt.first_batch_ is True\n\n        net = net_cls(\n            classifier_module,\n            callbacks=[npt],\n            max_epochs=1,\n        )\n\n        npt.on_batch_end(net)\n        assert npt.first_batch_ is False\n\n@pytest.mark.skipif(\n    not wandb_installed, reason=\'wandb is not installed\')\nclass TestWandb:\n    @pytest.fixture\n    def net_cls(self):\n        from skorch import NeuralNetClassifier\n        return NeuralNetClassifier\n\n    @pytest.fixture\n    def data(self, classifier_data):\n        X, y = classifier_data\n        # accelerate training since we don\'t care for the loss\n        X, y = X[:40], y[:40]\n        return X, y\n\n    @pytest.fixture\n    def wandb_logger_cls(self):\n        from skorch.callbacks import WandbLogger\n        return WandbLogger\n\n    @pytest.fixture\n    def wandb_run_cls(self):\n        import wandb\n        os.environ[\'WANDB_MODE\'] = \'dryrun\' # run offline\n        with wandb.init(anonymous=""allow"") as run:\n            return run\n\n    @pytest.fixture\n    def mock_run(self):\n        mock = Mock()\n        mock.log = Mock()\n        mock.watch = Mock()\n        mock.dir = \'.\'\n        return mock\n\n    def test_ignore_keys(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            wandb_logger_cls,\n            mock_run,\n    ):\n        # ignore \'dur\' and \'valid_loss\', \'unknown\' doesn\'t exist but\n        # this should not cause a problem\n        wandb_cb = wandb_logger_cls(\n            mock_run, keys_ignored=[\'dur\', \'valid_loss\', \'unknown\'])\n        net_cls(\n            classifier_module,\n            callbacks=[wandb_cb],\n            max_epochs=3,\n        ).fit(*data)\n\n        # 3 epochs = 3 calls\n        assert mock_run.log.call_count == 3\n        assert mock_run.watch.call_count == 1\n        call_args = [args[0][0] for args in mock_run.log.call_args_list]\n        assert \'valid_loss\' not in call_args\n\n    def test_keys_ignored_is_string(self, wandb_logger_cls, mock_run):\n        wandb_cb = wandb_logger_cls(\n            mock_run, keys_ignored=\'a-key\').initialize()\n        expected = {\'a-key\', \'batches\'}\n        assert wandb_cb.keys_ignored_ == expected\n\n    def test_fit_with_real_experiment(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            wandb_logger_cls,\n            wandb_run_cls,\n    ):\n        net = net_cls(\n            classifier_module,\n            callbacks=[wandb_logger_cls(wandb_run_cls)],\n            max_epochs=5,\n        )\n        net.fit(*data)\n\nclass TestPrintLog:\n    @pytest.fixture\n    def print_log_cls(self):\n        from skorch.callbacks import PrintLog\n        keys_ignored = [\'dur\', \'event_odd\']\n        return partial(PrintLog, sink=Mock(), keys_ignored=keys_ignored)\n\n    @pytest.fixture\n    def print_log(self, print_log_cls):\n        return print_log_cls().initialize()\n\n    @pytest.fixture\n    def scoring_cls(self):\n        from skorch.callbacks import EpochScoring\n        return EpochScoring\n\n    @pytest.fixture\n    def mse_scoring(self, scoring_cls):\n        return scoring_cls(\n            \'neg_mean_squared_error\',\n            name=\'nmse\',\n        ).initialize()\n\n    @pytest.fixture\n    def odd_epoch_callback(self):\n        from skorch.callbacks import Callback\n        class OddEpochCallback(Callback):\n            def on_epoch_end(self, net, **kwargs):\n                net.history[-1][\'event_odd\'] = bool(len(net.history) % 2)\n\n        return OddEpochCallback().initialize()\n\n    @pytest.fixture\n    def net(self, net_cls, module_cls, train_split, mse_scoring,\n            odd_epoch_callback, print_log, data):\n        net = net_cls(\n            module_cls, batch_size=1, train_split=train_split,\n            callbacks=[mse_scoring, odd_epoch_callback], max_epochs=2)\n        net.initialize()\n        # replace default PrintLog with test PrintLog\n        net.callbacks_[-1] = (\'print_log\', print_log)\n        return net.partial_fit(*data)\n\n    @pytest.fixture\n    def history(self, net):\n        return net.history\n\n    # pylint: disable=unused-argument\n    @pytest.fixture\n    def sink(self, history, print_log):\n        # note: the history fixture is required even if not used because it\n        # triggers the calls on print_log\n        return print_log.sink\n\n    @pytest.fixture\n    def ansi(self):\n        from skorch.utils import Ansi\n        return Ansi\n\n    def test_call_count(self, sink):\n        # header + lines + 2 epochs\n        assert sink.call_count == 4\n\n    def test_header(self, sink):\n        header = sink.call_args_list[0][0][0]\n        columns = header.split()\n        expected = [\'epoch\', \'nmse\', \'train_loss\', \'valid_loss\']\n        assert columns == expected\n\n    def test_lines(self, sink):\n        lines = sink.call_args_list[1][0][0].split()\n        # Lines have length 2 + length of column, or 8 if the column\n        # name is short and the values are floats.\n        expected = [\n            \'-\' * (len(\'epoch\') + 2),\n            \'-\' * 8,\n            \'-\' * (len(\'train_loss\') + 2),\n            \'-\' * (len(\'valid_loss\') + 2),\n        ]\n        assert lines\n        assert lines == expected\n\n    @pytest.mark.parametrize(\'epoch\', [0, 1])\n    def test_first_row(self, sink, ansi, epoch, history):\n        row = sink.call_args_list[epoch + 2][0][0]\n        items = row.split()\n\n        # epoch, nmse, valid, train\n        assert len(items) == 4\n\n        # epoch, starts at 1\n        assert items[0] == str(epoch + 1)\n\n        # is best\n        are_best = [\n            history[epoch, \'nmse_best\'],\n            history[epoch, \'train_loss_best\'],\n            history[epoch, \'valid_loss_best\'],\n        ]\n\n        # test that cycled colors are used if best\n        for item, color, is_best in zip(items[1:], list(ansi)[1:], are_best):\n            if is_best:\n                # if best, text colored\n                assert item.startswith(color.value)\n                assert item.endswith(ansi.ENDC.value)\n            else:\n                # if not best, text is only float, so converting possible\n                float(item)\n\n    def test_args_passed_to_tabulate(self, history):\n        with patch(\'skorch.callbacks.logging.tabulate\') as tab:\n            from skorch.callbacks import PrintLog\n            print_log = PrintLog(\n                tablefmt=\'latex\',\n                floatfmt=\'.9f\',\n            ).initialize()\n            print_log.table(history[-1])\n\n            assert tab.call_count == 1\n            assert tab.call_args_list[0][1][\'tablefmt\'] == \'latex\'\n            assert tab.call_args_list[0][1][\'floatfmt\'] == \'.9f\'\n\n    def test_with_additional_key(self, history, print_log_cls):\n        keys_ignored = [\'event_odd\']  # \'dur\' no longer ignored\n        print_log = print_log_cls(\n            sink=Mock(), keys_ignored=keys_ignored).initialize()\n        # does not raise\n        print_log.on_epoch_end(Mock(history=history))\n\n        header = print_log.sink.call_args_list[0][0][0]\n        columns = header.split()\n        expected = [\'epoch\', \'nmse\', \'train_loss\', \'valid_loss\', \'dur\']\n        assert columns == expected\n\n    def test_keys_ignored_as_str(self, print_log_cls):\n        print_log = print_log_cls(keys_ignored=\'a-key\').initialize()\n        assert print_log.keys_ignored_ == {\'a-key\', \'batches\'}\n\n        print_log.initialize()\n        assert print_log.keys_ignored_ == set([\'a-key\', \'batches\'])\n\n    def test_keys_ignored_is_None(self, print_log_cls):\n        print_log = print_log_cls(keys_ignored=None)\n        assert print_log.keys_ignored is None\n\n        print_log.initialize()\n        assert print_log.keys_ignored_ == set([\'batches\'])\n\n    def test_with_event_key(self, history, print_log_cls):\n        print_log = print_log_cls(sink=Mock(), keys_ignored=None).initialize()\n        # history has two epochs, write them one by one\n        print_log.on_epoch_end(Mock(history=history[:-1]))\n        print_log.on_epoch_end(Mock(history=history))\n\n        header = print_log.sink.call_args_list[0][0][0]\n        columns = header.split()\n        expected = [\'epoch\', \'nmse\', \'train_loss\', \'valid_loss\', \'odd\', \'dur\']\n        assert columns == expected\n\n        odd_row = print_log.sink.call_args_list[2][0][0].split()\n        even_row = print_log.sink.call_args_list[3][0][0].split()\n        assert len(odd_row) == 6  # odd row has entries in every column\n        assert odd_row[4] == \'+\'  # including \'+\' sign for the \'event_odd\'\n        assert len(even_row) == 5  # even row does not have \'event_odd\' entry\n\n    def test_witout_valid_data(\n            self, net_cls, module_cls, mse_scoring, print_log, data):\n        net = net_cls(\n            module_cls, batch_size=1, train_split=None,\n            callbacks=[mse_scoring], max_epochs=2)\n        net.initialize()\n        # replace default PrintLog with test PrintLog\n        net.callbacks_[-1] = (\'print_log\', print_log)\n        net.partial_fit(*data)\n\n        sink = print_log.sink\n        row = sink.call_args_list[2][0][0]\n        items = row.split()\n\n        assert len(items) == 2  # no valid, only epoch and train\n\n    def test_print_not_skipped_if_verbose(self, capsys):\n        from skorch.callbacks import PrintLog\n\n        print_log = PrintLog().initialize()\n        net = Mock(history=[{\'loss\': 123}], verbose=1)\n\n        print_log.on_epoch_end(net)\n\n        stdout = capsys.readouterr()[0]\n        result = [x.strip() for x in stdout.split()]\n        expected = [\'loss\', \'------\', \'123\']\n        assert result == expected\n\n    def test_print_skipped_if_not_verbose(self, capsys):\n        from skorch.callbacks import PrintLog\n\n        print_log = PrintLog().initialize()\n        net = Mock(history=[{\'loss\': 123}], verbose=0)\n\n        print_log.on_epoch_end(net)\n\n        stdout = capsys.readouterr()[0]\n        assert not stdout\n\n\nclass TestProgressBar:\n    @pytest.fixture\n    def progressbar_cls(self):\n        from skorch.callbacks import ProgressBar\n        return ProgressBar\n\n    @pytest.fixture\n    def net_cls(self):\n        """"""very simple network that trains for 2 epochs""""""\n        from skorch import NeuralNetRegressor\n        from skorch.toy import make_regressor\n\n        module_cls = make_regressor(\n            input_units=1,\n            num_hidden=0,\n            output_units=1,\n        )\n\n        return partial(\n            NeuralNetRegressor,\n            module=module_cls,\n            train_split=None,\n            max_epochs=2,\n            batch_size=10)\n\n    @pytest.fixture(scope=\'module\')\n    def data(self):\n        X = np.zeros((20, 1), dtype=\'float32\')\n        y = np.zeros((20, 1), dtype=\'float32\')\n        return X, y\n\n    @pytest.mark.parametrize(\'postfix\', [\n        [],\n        [\'train_loss\'],\n        [\'train_loss\', \'valid_loss\'],\n        [\'doesnotexist\'],\n        [\'train_loss\', \'doesnotexist\'],\n    ])\n    def test_invalid_postfix(self, postfix, net_cls, progressbar_cls, data):\n        net = net_cls(callbacks=[\n            progressbar_cls(postfix_keys=postfix),\n        ])\n        net.fit(*data)\n\n    @patch(\'tqdm.tqdm\')\n    @pytest.mark.parametrize(\'scheme,expected_total\', [\n        (\'auto\', [2, 2]),\n        (\'count\', [None, 2]),\n        (None, [None, None]),\n        (2, [2, 2]),  # correct number of batches_per_epoch (20 // 10)\n        (3, [3, 3]),  # offset by +1, should still work\n        (1, [1, 1]),  # offset by -1, should still work\n    ])\n    def test_different_count_schemes(\n            self, tqdm_mock, scheme, expected_total, net_cls, progressbar_cls, data):\n        net = net_cls(callbacks=[\n            progressbar_cls(batches_per_epoch=scheme),\n        ])\n        net.fit(*data)\n        assert tqdm_mock.call_count == 2\n        for i, total in enumerate(expected_total):\n            assert tqdm_mock.call_args_list[i][1][\'total\'] == total\n\n\n@pytest.mark.skipif(\n    not tensorboard_installed, reason=\'tensorboard is not installed\')\nclass TestTensorBoard:\n    @pytest.fixture\n    def net_cls(self):\n        from skorch import NeuralNetClassifier\n        return NeuralNetClassifier\n\n    @pytest.fixture\n    def data(self, classifier_data):\n        X, y = classifier_data\n        # accelerate training since we don\'t care for the loss\n        X, y = X[:40], y[:40]\n        return X, y\n\n    @pytest.fixture\n    def tensorboard_cls(self):\n        from skorch.callbacks import TensorBoard\n        return TensorBoard\n\n    @pytest.fixture\n    def summary_writer_cls(self):\n        from torch.utils.tensorboard import SummaryWriter\n        return SummaryWriter\n\n    @pytest.fixture\n    def mock_writer(self, summary_writer_cls):\n        mock = Mock(spec=summary_writer_cls)\n        return mock\n\n    @pytest.fixture\n    def net_fitted(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            tensorboard_cls,\n            mock_writer,\n    ):\n        return net_cls(\n            classifier_module,\n            callbacks=[tensorboard_cls(mock_writer)],\n            max_epochs=3,\n        ).fit(*data)\n\n    @pytest.mark.skipif(\n        True, reason=""Waiting for proper implementation of graph tracing"")\n    def test_graph_added_once(self, net_fitted, mock_writer):\n        # graph should just be added once\n        assert mock_writer.add_graph.call_count == 1\n\n    @pytest.mark.skipif(\n        True, reason=""Waiting for proper implementation of graph tracing"")\n    def test_include_graph_false(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            tensorboard_cls,\n            mock_writer,\n    ):\n        net_cls(\n            classifier_module,\n            callbacks=[tensorboard_cls(mock_writer, include_graph=False)],\n            max_epochs=2,\n        ).fit(*data)\n        assert mock_writer.add_graph.call_count == 0\n\n    def test_writer_closed_automatically(self, net_fitted, mock_writer):\n        assert mock_writer.close.call_count == 1\n\n    def test_writer_not_closed(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            tensorboard_cls,\n            mock_writer,\n    ):\n        net_cls(\n            classifier_module,\n            callbacks=[tensorboard_cls(mock_writer, close_after_train=False)],\n            max_epochs=2,\n        ).fit(*data)\n        assert mock_writer.close.call_count == 0\n\n    def test_keys_from_history_logged(self, net_fitted, mock_writer):\n        add_scalar = mock_writer.add_scalar\n\n        # 3 epochs with 4 keys\n        assert add_scalar.call_count == 3 * 4\n        keys = {call_args[1][\'tag\'] for call_args in add_scalar.call_args_list}\n        expected = {\'dur\', \'Loss/train_loss\', \'Loss/valid_loss\', \'Loss/valid_acc\'}\n        assert keys == expected\n\n    def test_ignore_keys(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            tensorboard_cls,\n            mock_writer,\n    ):\n        # ignore \'dur\' and \'valid_loss\', \'unknown\' doesn\'t exist but\n        # this should not cause a problem\n        tb = tensorboard_cls(\n            mock_writer, keys_ignored=[\'dur\', \'valid_loss\', \'unknown\'])\n        net_cls(\n            classifier_module,\n            callbacks=[tb],\n            max_epochs=3,\n        ).fit(*data)\n        add_scalar = mock_writer.add_scalar\n\n        keys = {call_args[1][\'tag\'] for call_args in add_scalar.call_args_list}\n        expected = {\'Loss/train_loss\', \'Loss/valid_acc\'}\n        assert keys == expected\n\n    def test_keys_ignored_is_string(self, tensorboard_cls, mock_writer):\n        tb = tensorboard_cls(mock_writer, keys_ignored=\'a-key\').initialize()\n        expected = {\'a-key\', \'batches\'}\n        assert tb.keys_ignored_ == expected\n\n    def test_other_key_mapper(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            tensorboard_cls,\n            mock_writer,\n    ):\n        # just map all keys to uppercase\n        tb = tensorboard_cls(mock_writer, key_mapper=lambda s: s.upper())\n        net_cls(\n            classifier_module,\n            callbacks=[tb],\n            max_epochs=3,\n        ).fit(*data)\n        add_scalar = mock_writer.add_scalar\n\n        keys = {call_args[1][\'tag\'] for call_args in add_scalar.call_args_list}\n        expected = {\'DUR\', \'TRAIN_LOSS\', \'VALID_LOSS\', \'VALID_ACC\'}\n        assert keys == expected\n\n    @pytest.fixture\n    def add_scalar_maybe(self, tensorboard_cls, mock_writer):\n        tb = tensorboard_cls(mock_writer)\n        return tb.add_scalar_maybe\n\n    @pytest.fixture\n    def history(self):\n        return [\n            {\'loss\': 0.1, \'epoch\': 1, \'foo\': [\'invalid\', \'type\']},\n            {\'loss\': 0.2, \'epoch\': 2, \'foo\': [\'invalid\', \'type\']},\n        ]\n\n    def test_add_scalar_maybe_uses_last_epoch_values(\n            self, add_scalar_maybe, mock_writer, history):\n        add_scalar_maybe(history, key=\'loss\', tag=\'myloss\', global_step=2)\n        call_kwargs = mock_writer.add_scalar.call_args_list[0][1]\n        assert call_kwargs[\'tag\'] == \'myloss\'\n        assert call_kwargs[\'scalar_value\'] == 0.2\n        assert call_kwargs[\'global_step\'] == 2\n\n    def test_add_scalar_maybe_infers_epoch(\n            self, add_scalar_maybe, mock_writer, history):\n        # don\'t indicate \'global_step\' value\n        add_scalar_maybe(history, key=\'loss\', tag=\'myloss\')\n        call_kwargs = mock_writer.add_scalar.call_args_list[0][1]\n        assert call_kwargs[\'global_step\'] == 2\n\n    def test_add_scalar_maybe_unknown_key_does_not_raise(\n            self, tensorboard_cls, summary_writer_cls, history):\n        tb = tensorboard_cls(summary_writer_cls())\n        # does not raise:\n        tb.add_scalar_maybe(history, key=\'unknown\', tag=\'bar\')\n\n    def test_add_scalar_maybe_wrong_type_does_not_raise(\n            self, tensorboard_cls, summary_writer_cls, history):\n        tb = tensorboard_cls(summary_writer_cls())\n        # value of \'foo\' is a list but that does not raise:\n        tb.add_scalar_maybe(history, key=\'foo\', tag=\'bar\')\n\n    def test_fit_with_real_summary_writer(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            tensorboard_cls,\n            summary_writer_cls,\n            tmp_path,\n    ):\n        path = str(tmp_path)\n\n        net = net_cls(\n            classifier_module,\n            callbacks=[tensorboard_cls(summary_writer_cls(path))],\n            max_epochs=5,\n        )\n        net.fit(*data)\n\n        # is not empty\n        assert os.listdir(path)\n\n    def test_fit_with_dict_input(\n            self,\n            net_cls,\n            classifier_module,\n            data,\n            tensorboard_cls,\n            summary_writer_cls,\n            tmp_path,\n    ):\n        from skorch.toy import MLPModule\n        path = str(tmp_path)\n        X, y = data\n\n        # create a dictionary with unordered keys\n        X_dict = {k: X[:, i:i + 4] for k, i in zip(\'cebad\', range(0, X.shape[1], 4))}\n\n        class MyModule(MLPModule):\n            # use different order for args here\n            def forward(self, b, e, c, d, a, **kwargs):\n                X = torch.cat((b, e, c, d, a), 1)\n                return super().forward(X, **kwargs)\n\n        net = net_cls(\n            MyModule(output_nonlin=nn.Softmax(dim=-1)),\n            callbacks=[tensorboard_cls(summary_writer_cls(path))],\n            max_epochs=5,\n        )\n        net.fit(X_dict, y)\n\n        # is not empty\n        assert os.listdir(path)'"
skorch/tests/callbacks/test_lr_scheduler.py,10,"b'""""""Tests for lr_scheduler.py""""""\nfrom distutils.version import LooseVersion\nfrom unittest.mock import Mock\n\nimport numpy as np\nimport pytest\nimport torch\nfrom sklearn.base import clone\nfrom torch.optim import SGD\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.optim.lr_scheduler import CyclicLR as TorchCyclicLR\n\nfrom skorch import NeuralNetClassifier\nfrom skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler\n\n\n@pytest.mark.filterwarnings(""ignore::DeprecationWarning"")\nclass TestLRCallbacks:\n\n    @pytest.mark.parametrize(\'policy\', [StepLR, \'StepLR\'])\n    def test_simulate_lrs_epoch_step(self, policy):\n        lr_sch = LRScheduler(policy, step_size=2)\n        lrs = lr_sch.simulate(6, 1)\n        expected = np.array([1.0, 1.0, 0.1, 0.1, 0.01, 0.01])\n        assert np.allclose(expected, lrs)\n\n    @pytest.mark.parametrize(\'policy\', [TorchCyclicLR])\n    def test_simulate_lrs_batch_step(self, policy):\n        lr_sch = LRScheduler(\n            policy, base_lr=1, max_lr=5, step_size_up=4, step_every=\'batch\')\n        lrs = lr_sch.simulate(11, 1)\n        expected = np.array([1, 2, 3, 4, 5, 4, 3, 2, 1, 2, 3])\n        assert np.allclose(expected, lrs)\n\n    @pytest.mark.parametrize(\'policy, instance, kwargs\', [\n        (\'LambdaLR\', LambdaLR, {\'lr_lambda\': (lambda x: 1e-1)}),\n        (\'StepLR\', StepLR, {\'step_size\': 30}),\n        (\'MultiStepLR\', MultiStepLR, {\'milestones\': [30, 90]}),\n        (\'ExponentialLR\', ExponentialLR, {\'gamma\': 0.1}),\n        (\'ReduceLROnPlateau\', ReduceLROnPlateau, {}),\n        (\'WarmRestartLR\', WarmRestartLR, {}),\n        (\'CosineAnnealingLR\', CosineAnnealingLR, {\'T_max\': 5, \'eta_min\': 1e-3}),\n        (WarmRestartLR, WarmRestartLR, {}),\n    ])\n    def test_lr_callback_init_policies(\n            self,\n            classifier_module,\n            classifier_data,\n            policy,\n            instance,\n            kwargs,\n    ):\n        X, y = classifier_data\n        lr_policy = LRScheduler(policy, **kwargs)\n        net = NeuralNetClassifier(\n            classifier_module, max_epochs=2, callbacks=[lr_policy]\n        )\n        net.fit(X, y)\n        assert any(list(map(\n            lambda x: isinstance(\n                getattr(x[1], \'lr_scheduler_\', None), instance),\n            net.callbacks_\n        )))\n\n    @pytest.mark.parametrize(\'policy, kwargs\', [\n        (\'LambdaLR\', {\'lr_lambda\': (lambda x: 1e-1)}),\n        (\'StepLR\', {\'step_size\': 30}),\n        (\'MultiStepLR\', {\'milestones\': [30, 90]}),\n        (\'ExponentialLR\', {\'gamma\': 0.1}),\n        (\'ReduceLROnPlateau\', {}),\n        (\'WarmRestartLR\', {}),\n        (\'CosineAnnealingLR\', {\'T_max\': 3}),\n    ])\n    def test_lr_callback_steps_correctly(\n            self,\n            classifier_module,\n            classifier_data,\n            policy,\n            kwargs,\n    ):\n        max_epochs = 2\n        X, y = classifier_data\n        lr_policy = LRScheduler(policy, **kwargs)\n        net = NeuralNetClassifier(\n            classifier_module(),\n            max_epochs=max_epochs,\n            batch_size=16,\n            callbacks=[lr_policy],\n        )\n        net.fit(X, y)\n        # pylint: disable=protected-access\n        assert lr_policy.lr_scheduler_.last_epoch == max_epochs\n\n    @pytest.mark.parametrize(\'policy, kwargs\', [\n        (TorchCyclicLR, {\'base_lr\': 1e-3, \'max_lr\': 6e-3, \'step_every\': \'batch\'}),\n    ])\n    def test_lr_callback_batch_steps_correctly(\n            self,\n            classifier_module,\n            classifier_data,\n            policy,\n            kwargs,\n    ):\n        batch_size = 100\n        max_epochs = 2\n\n        X, y = classifier_data\n        num_examples = len(X)\n\n        lr_policy = LRScheduler(policy, **kwargs)\n        net = NeuralNetClassifier(classifier_module(), max_epochs=max_epochs,\n                                  batch_size=batch_size, callbacks=[lr_policy])\n        net.fit(X, y)\n\n        total_iterations_per_epoch = num_examples / batch_size\n        # 80% of sample used for training by default\n        total_training_iterations_per_epoch = 0.8 * total_iterations_per_epoch\n\n        expected = int(total_training_iterations_per_epoch * max_epochs)\n        # pylint: disable=protected-access\n        assert lr_policy.batch_idx_ == expected\n\n    @pytest.mark.parametrize(\'policy, kwargs\', [\n        (TorchCyclicLR, {\'base_lr\': 1e-3, \'max_lr\': 6e-3, \'step_every\': \'batch\'}),\n    ])\n    def test_lr_callback_batch_steps_correctly_fallback(\n            self,\n            classifier_module,\n            classifier_data,\n            policy,\n            kwargs,\n    ):\n        batch_size = 100\n        max_epochs = 2\n\n        X, y = classifier_data\n        num_examples = len(X)\n\n        lr_policy = LRScheduler(policy, **kwargs)\n        net = NeuralNetClassifier(classifier_module(), max_epochs=max_epochs,\n                                  batch_size=batch_size, callbacks=[lr_policy])\n        net.fit(X, y)\n\n        # Removes batch count information in the last two epochs\n        for i in range(max_epochs):\n            del net.history[i][""train_batch_count""]\n            del net.history[i][""valid_batch_count""]\n        net.partial_fit(X, y)\n\n        total_iterations_per_epoch = num_examples / batch_size\n\n        # batch_counts were removed thus the total iterations of the last\n        # epoch is used\n        total_iterations_fit_run = total_iterations_per_epoch * max_epochs\n\n        # 80% of sample used for training by default\n        total_iterations_partial_fit_run = (\n                0.8 * total_iterations_per_epoch * max_epochs)\n\n        # called fit AND partial_fit\n        total_iterations = (total_iterations_fit_run +\n                            total_iterations_partial_fit_run)\n        # Failback to using both valid and training batches counts on\n        # second run\n        expected = int(total_iterations)\n        # pylint: disable=protected-access\n        assert lr_policy.batch_idx_ == expected\n\n    def test_lr_scheduler_cloneable(self):\n        # reproduces bug #271\n        scheduler = LRScheduler(WarmRestartLR, base_lr=123)\n        clone(scheduler)  # does not raise\n\n    def test_lr_scheduler_set_params(self, classifier_module, classifier_data):\n        scheduler = LRScheduler(\n            TorchCyclicLR, base_lr=123, max_lr=999, step_every=\'batch\')\n        net = NeuralNetClassifier(\n            classifier_module,\n            max_epochs=0,\n            callbacks=[(\'scheduler\', scheduler)],\n        )\n        net.set_params(callbacks__scheduler__base_lr=456)\n        net.fit(*classifier_data)  # we need to trigger on_train_begin\n        assert net.callbacks[0][1].lr_scheduler_.base_lrs[0] == 456\n\n    @pytest.mark.parametrize(\'policy,kwargs\', [\n        (StepLR, {\'gamma\': 0.9, \'step_size\': 1})\n    ])\n    @pytest.mark.skipif(\n        LooseVersion(torch.__version__) < \'1.4\',\n        reason=""Feature isn\'t supported with this torch version.""\n    )\n    def test_lr_scheduler_record_epoch_step(self,\n                                            classifier_module,\n                                            classifier_data,\n                                            policy,\n                                            kwargs):\n        epochs = 3\n        scheduler = LRScheduler(policy, **kwargs)\n        lrs = scheduler.simulate(epochs, initial_lr=123.)\n        net = NeuralNetClassifier(\n            classifier_module,\n            max_epochs=epochs,\n            lr=123.,\n            callbacks=[(\'scheduler\', scheduler)]\n        )\n        net.fit(*classifier_data)\n        assert np.all(net.history[:, \'event_lr\'] == lrs)\n\n    @pytest.mark.skipif(\n        LooseVersion(torch.__version__) < \'1.4\',\n        reason=""Feature isn\'t supported with this torch version.""\n    )\n    def test_lr_scheduler_record_batch_step(self, classifier_module, classifier_data):\n        X, y = classifier_data\n        batch_size = 128\n\n        scheduler = LRScheduler(\n            TorchCyclicLR,\n            base_lr=1,\n            max_lr=5,\n            step_size_up=4,\n            step_every=\'batch\'\n        )\n        net = NeuralNetClassifier(\n            classifier_module,\n            max_epochs=1,\n            lr=123.,\n            batch_size=batch_size,\n            callbacks=[(\'scheduler\', scheduler)]\n        )\n        net.fit(X, y)\n        new_lrs = scheduler.simulate(\n            net.history[-1, \'train_batch_count\'],\n            initial_lr=123.,\n        )\n        assert np.all(net.history[-1, \'batches\', :, \'event_lr\'] == new_lrs)\n\n    def test_cyclic_lr_with_epoch_step_warning(self,\n                                               classifier_module,\n                                               classifier_data):\n        msg = (""The LRScheduler now makes a step every epoch by default. ""\n               ""To have the cyclic lr scheduler update ""\n               ""every batch set step_every=\'batch\'"")\n        with pytest.warns(FutureWarning, match=msg) as record:\n            scheduler = LRScheduler(\n                TorchCyclicLR, base_lr=123, max_lr=999)\n            net = NeuralNetClassifier(\n                classifier_module,\n                max_epochs=0,\n                callbacks=[(\'scheduler\', scheduler)],\n            )\n            net.initialize()\n        assert len(record) == 1\n\n\nclass TestReduceLROnPlateau:\n\n    def get_net_with_mock(\n            self, classifier_data, classifier_module, monitor=\'train_loss\'):\n        """"""Returns a net with a mocked lr policy that allows to check what\n        it\'s step method was called with.\n\n        """"""\n        X, y = classifier_data\n        net = NeuralNetClassifier(\n            classifier_module,\n            callbacks=[\n                (\'scheduler\', LRScheduler(ReduceLROnPlateau, monitor=monitor)),\n            ],\n            max_epochs=1,\n        ).fit(X, y)\n\n        # mock the policy\n        policy = dict(net.callbacks_)[\'scheduler\'].lr_scheduler_\n        mock_step = Mock(side_effect=policy.step)\n        policy.step = mock_step\n\n        # make sure that mocked policy is set\n        scheduler = dict(net.callbacks_)[\'scheduler\']\n        # pylint: disable=protected-access\n        scheduler._get_scheduler = lambda *args, **kwargs: policy\n\n        net.partial_fit(X, y)\n        return net, mock_step\n\n    @pytest.mark.parametrize(\'monitor\', [\'train_loss\', \'valid_loss\', \'epoch\'])\n    def test_reduce_lr_monitor_with_string(\n            self, monitor, classifier_data, classifier_module):\n        # step should be called with the 2nd to last value from that\n        # history entry\n        net, mock_step = self.get_net_with_mock(\n            classifier_data, classifier_module, monitor=monitor)\n        score = mock_step.call_args_list[0][0][0]\n        np.isclose(score, net.history[-2, monitor])\n\n    def test_reduce_lr_monitor_with_callable(\n            self, classifier_data, classifier_module):\n        # step should always be called with the return value from the\n        # callable, 55\n        _, mock_step = self.get_net_with_mock(\n            classifier_data, classifier_module, monitor=lambda x: 55)\n        score = mock_step.call_args_list[0][0][0]\n        assert score == 55\n\n    @pytest.mark.parametrize(\'mode,score\', [\n        (\'min\', np.inf),\n        (\'max\', -np.inf)\n    ])\n    def test_reduce_lr_monitor_max(\n            self, classifier_data, classifier_module, mode, score):\n        X, y = classifier_data\n        net = NeuralNetClassifier(\n            classifier_module,\n            callbacks=[\n                (\'scheduler\', LRScheduler(\n                    ReduceLROnPlateau, monitor=\'train_loss\', mode=mode)),\n            ],\n            max_epochs=1,\n        )\n        net.fit(X, y)\n\n        policy = dict(net.callbacks_)[\'scheduler\'].lr_scheduler_\n        assert policy.best == score\n\n\nclass TestWarmRestartLR():\n    def assert_lr_correct(\n            self, optimizer, targets, epochs, min_lr, max_lr, base_period,\n            period_mult):\n        """"""Test that learning rate was set correctly.""""""\n        targets = [targets] if len(optimizer.param_groups) == 1 else targets\n        scheduler = WarmRestartLR(\n            optimizer, min_lr, max_lr, base_period, period_mult\n        )\n        for epoch in range(epochs):\n            optimizer.step()  # suppress warning about .step call order\n            scheduler.step(epoch)\n            for param_group, target in zip(optimizer.param_groups, targets):\n                assert param_group[\'lr\'] == pytest.approx(target[epoch])\n\n    def _single_period_targets(self, epochs, min_lr, max_lr, period):\n        targets = 1 + np.cos(np.arange(epochs) * np.pi / period)\n        targets = min_lr + 0.5 * (max_lr - min_lr) * targets\n        return targets.tolist()\n\n    # pylint: disable=missing-docstring\n    def _multi_period_targets(\n            self, epochs, min_lr, max_lr, base_period, period_mult):\n        remaining_epochs = epochs\n        current_period = base_period\n        targets = list()\n        while remaining_epochs > 0:\n            period_epochs = min(remaining_epochs, current_period + 1)\n            remaining_epochs -= period_epochs\n            targets += self._single_period_targets(\n                period_epochs, min_lr, max_lr, current_period\n            )\n            current_period = current_period * period_mult\n        return targets\n\n    @pytest.fixture()\n    def init_optimizer(self, classifier_module):\n        return SGD(classifier_module().parameters(), lr=0.05)\n\n    def test_raise_incompatible_len_on_min_lr_err(self, init_optimizer):\n        with pytest.raises(ValueError) as excinfo:\n            WarmRestartLR(init_optimizer, min_lr=[1e-1, 1e-2])\n        assert \'min_lr\' in str(excinfo.value)\n\n    def test_raise_incompatible_len_on_max_lr_err(self, init_optimizer):\n        with pytest.raises(ValueError) as excinfo:\n            WarmRestartLR(init_optimizer, max_lr=[1e-1, 1e-2])\n        assert \'max_lr\' in str(excinfo.value)\n\n    def test_single_period(self, init_optimizer):\n        optimizer = init_optimizer\n        epochs = 3\n        min_lr = 5e-5\n        max_lr = 5e-2\n        base_period = 3\n        period_mult = 1\n        targets = self._single_period_targets(\n            epochs, min_lr, max_lr, base_period)\n        self.assert_lr_correct(\n            optimizer,\n            targets,\n            epochs,\n            min_lr,\n            max_lr,\n            base_period,\n            period_mult\n        )\n\n    def test_multi_period_with_restart(self, init_optimizer):\n        optimizer = init_optimizer\n        epochs = 9\n        min_lr = 5e-5\n        max_lr = 5e-2\n        base_period = 2\n        period_mult = 2\n        targets = self._multi_period_targets(\n            epochs, min_lr, max_lr, base_period, period_mult\n        )\n        self.assert_lr_correct(\n            optimizer,\n            targets,\n            epochs,\n            min_lr,\n            max_lr,\n            base_period,\n            period_mult\n        )\n\n    def test_restarts_with_multiple_groups(self, classifier_module):\n        classifier = classifier_module()\n        optimizer = SGD(\n            [\n                {\'params\': classifier.sequential[0].parameters(), \'lr\': 1e-3},\n                {\'params\': classifier.sequential[1].parameters(), \'lr\': 1e-2},\n                {\'params\': classifier.sequential[2].parameters(), \'lr\': 1e-1},\n            ]\n        )\n\n        epochs = 9\n        min_lr_group = [1e-5, 1e-4, 1e-3]\n        max_lr_group = [1e-3, 1e-2, 1e-1]\n        base_period = 2\n        period_mult = 2\n        targets = list()\n        for min_lr, max_lr in zip(min_lr_group, max_lr_group):\n            targets.append(\n                self._multi_period_targets(\n                    epochs, min_lr, max_lr, base_period, period_mult\n                )\n            )\n        self.assert_lr_correct(\n            optimizer,\n            targets,\n            epochs,\n            min_lr_group,\n            max_lr_group,\n            base_period,\n            period_mult\n        )\n'"
skorch/tests/callbacks/test_regularization.py,0,"b""from unittest.mock import patch\n\nimport numpy as np\nimport pytest\n\nfrom skorch.utils import to_numpy\n\n\nclass TestGradientNormClipping:\n    @pytest.fixture\n    def grad_clip_cls_and_mock(self):\n        with patch('skorch.callbacks.regularization.clip_grad_norm_') as cgn:\n            from skorch.callbacks import GradientNormClipping\n            yield GradientNormClipping, cgn\n\n    def test_parameters_passed_correctly_to_torch_cgn(\n            self, grad_clip_cls_and_mock):\n        grad_norm_clip_cls, cgn = grad_clip_cls_and_mock\n\n        clipping = grad_norm_clip_cls(\n            gradient_clip_value=55, gradient_clip_norm_type=99)\n        named_parameters = [('p1', 1), ('p2', 2), ('p3', 3)]\n        parameter_values = [p for _, p in named_parameters]\n        clipping.on_grad_computed(None, named_parameters=named_parameters)\n\n        # Clip norm must receive values, not (name, value) pairs.\n        assert list(cgn.call_args_list[0][0][0]) == parameter_values\n        assert cgn.call_args_list[0][1]['max_norm'] == 55\n        assert cgn.call_args_list[0][1]['norm_type'] == 99\n\n    def test_no_parameter_updates_when_norm_0(\n            self, classifier_module, classifier_data):\n        from copy import deepcopy\n        from skorch import NeuralNetClassifier\n        from skorch.callbacks import GradientNormClipping\n\n        net = NeuralNetClassifier(\n            classifier_module,\n            callbacks=[('grad_norm', GradientNormClipping(0))],\n            train_split=None,\n            warm_start=True,\n            max_epochs=1,\n        )\n        net.initialize()\n\n        params_before = deepcopy(list(net.module_.parameters()))\n        net.fit(*classifier_data)\n        params_after = net.module_.parameters()\n        for p0, p1 in zip(params_before, params_after):\n            p0, p1 = to_numpy(p0), to_numpy(p1)\n            assert np.allclose(p0, p1)\n"""
skorch/tests/callbacks/test_scoring.py,3,"b'""""""Tests for scoring""""""\n\nfrom functools import partial\nfrom unittest.mock import Mock\nfrom unittest.mock import patch\n\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, make_scorer\nimport pytest\n\nfrom skorch.utils import to_numpy\n\n\nclass TestEpochScoring:\n    @pytest.fixture(params=[{\'use_caching\': True}, {\'use_caching\': False}])\n    def scoring_cls(self, request):\n        from skorch.callbacks import EpochScoring\n        return partial(EpochScoring, **request.param)\n\n    @pytest.fixture\n    def caching_scoring_cls(self):\n        from skorch.callbacks import EpochScoring\n        return partial(EpochScoring, use_caching=True)\n\n    @pytest.fixture(params=[{\'use_caching\': True}, {\'use_caching\': False}])\n    def mse_scoring(self, request, scoring_cls):\n        return scoring_cls(\n            \'neg_mean_squared_error\',\n            name=\'nmse\',\n            **request.param,\n        ).initialize()\n\n    def test_correct_valid_score(\n            self, net_cls, module_cls, mse_scoring, train_split, data,\n    ):\n        net = net_cls(\n            module=module_cls,\n            callbacks=[mse_scoring],\n            train_split=train_split,\n            max_epochs=2,\n        )\n        net.fit(*data)\n\n        expected = -np.mean([(3 - 5) ** 2, (0 - 4) ** 2])\n        loss = net.history[:, \'nmse\']\n        assert np.allclose(loss, expected)\n\n    def test_correct_train_score(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module=module_cls,\n            callbacks=[scoring_cls(\n                \'neg_mean_squared_error\',\n                on_train=True,\n                name=\'nmse\',\n                lower_is_better=False,\n            )],\n            train_split=train_split,\n            max_epochs=2,\n        )\n        net.fit(*data)\n\n        expected = -np.mean([(0 - -1) ** 2, (2 - 0) ** 2])\n        loss = net.history[:, \'nmse\']\n        assert np.allclose(loss, expected)\n\n    def test_scoring_uses_score_when_none(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(scoring=None)],\n            max_epochs=5,\n            train_split=train_split,\n        )\n        with patch.object(net, \'score\', side_effect=[10, 8, 6, 11, 7]):\n            net.fit(*data)\n\n        result = net.history[:, \'score\']\n        expected = [10, 8, 6, 11, 7]\n        assert result == expected\n\n    @pytest.mark.parametrize(\'lower_is_better, expected\', [\n        (True, [True, True, True, False, False]),\n        (False, [True, False, False, True, False]),\n    ])\n    @pytest.mark.parametrize(\'initial_epochs\', [1, 2, 3, 4])\n    def test_scoring_uses_best_score_when_continuing_training(\n            self, net_cls, module_cls, scoring_cls, data,\n            lower_is_better, expected, tmpdir, initial_epochs,\n    ):\n        # set scoring to None so that mocked net.score is used\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(\n                scoring=None,\n                on_train=True,\n                lower_is_better=lower_is_better)],\n            max_epochs=initial_epochs,\n            # Set train_split to None so that the default \'valid_loss\'\n            # callback is effectively disabled and does not write to\n            # the history. This should not cause problems when trying\n            # to load best score for this scorer.\n            train_split=None,\n        )\n\n        with patch.object(net, \'score\', side_effect=[10, 8, 6, 11, 7]):\n            net.fit(*data)\n\n            history_fn = tmpdir.mkdir(\'skorch\').join(\'history.json\')\n            net.save_params(f_history=str(history_fn))\n\n            net.initialize()\n            net.load_params(f_history=str(history_fn))\n            net.max_epochs = 5 - initial_epochs\n            net.partial_fit(*data)\n\n        is_best = net.history[:, \'score_best\']\n        assert is_best == expected\n\n    @pytest.mark.parametrize(\'lower_is_better, expected\', [\n        (True, [True, True, True, False, False]),\n        (False, [True, False, False, True, False]),\n        (None, []),\n    ])\n    def test_best_score_when_lower_is_better(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n            lower_is_better, expected,\n    ):\n        # set scoring to None so that mocked net.score is used\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(\n                scoring=None,\n                lower_is_better=lower_is_better)],\n            train_split=train_split,\n            max_epochs=5,\n        )\n\n        with patch.object(net, \'score\', side_effect=[10, 8, 6, 11, 7]):\n            net.fit(*data)\n\n        if lower_is_better is not None:\n            is_best = net.history[:, \'score_best\']\n            assert is_best == expected\n        else:\n            # if lower_is_better==None, don\'t write score\n            with pytest.raises(KeyError):\n                # pylint: disable=pointless-statement\n                net.history[:, \'score_best\']\n\n    def test_no_error_when_no_valid_data(\n            self, net_cls, module_cls, mse_scoring, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[mse_scoring],\n            max_epochs=3,\n            train_split=train_split,\n        )\n        net.fit(*data)\n\n        net.train_split = None\n        # does not raise\n        net.partial_fit(*data)\n\n        # only the first 3 epochs wrote scores\n        assert len(net.history[:, \'nmse\']) == 3\n\n    def test_with_accuracy_score(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(\'accuracy\')],\n            max_epochs=2,\n            train_split=train_split,\n        )\n        net.fit(*data)\n\n        result = net.history[:, \'accuracy\']\n        assert result == [0, 0]\n\n    def test_with_make_scorer_accuracy_score(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(make_scorer(accuracy_score))],\n            max_epochs=2,\n            train_split=train_split,\n        )\n        net.fit(*data)\n\n        result = net.history[:, \'accuracy_score\']\n        assert result == [0, 0]\n\n    def test_with_callable_accuracy_score(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(accuracy_score)],\n            max_epochs=2,\n            train_split=train_split,\n        )\n        net.fit(*data)\n\n        result = net.history[:, \'accuracy_score\']\n        assert result == [0, 0]\n\n    def test_with_score_nonexisting_string(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(\'does-not-exist\')],\n            max_epochs=2,\n            train_split=train_split,\n        )\n        with pytest.raises(ValueError) as exc:\n            net.fit(*data)\n        msg = ""\'does-not-exist\' is not a valid scoring value.""\n        assert exc.value.args[0].startswith(msg)\n\n    def test_with_score_as_custom_func(\n            self, net_cls, module_cls, scoring_cls, train_split, data, score55,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(score55)],\n            max_epochs=2,\n            train_split=train_split,\n        )\n        net.fit(*data)\n\n        result = net.history[:, \'score55\']\n        assert result == [55, 55]\n\n    def test_with_name_none_returns_score_as_name(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(scoring=None, name=None)],\n            max_epochs=1,\n            train_split=train_split,\n        )\n        net.fit(*data)\n        assert net.history[:, \'score\']\n\n    def test_explicit_name_is_used_in_history(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(scoring=None, name=\'myname\')],\n            max_epochs=1,\n            train_split=train_split,\n        )\n        net.fit(*data)\n        assert net.history[:, \'myname\']\n\n    def test_with_scoring_str_and_name_none(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(\n                scoring=\'neg_mean_squared_error\', name=None)],\n            max_epochs=1,\n            train_split=train_split,\n        )\n        net.fit(*data)\n        assert net.history[:, \'neg_mean_squared_error\']\n\n    def test_with_with_custom_func_and_name_none(\n            self, net_cls, module_cls, scoring_cls, train_split, data, score55,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(score55, name=None)],\n            max_epochs=2,\n            train_split=train_split,\n        )\n        net.fit(*data)\n        assert net.history[:, \'score55\']\n\n    def test_with_with_partial_custom_func_and_name_none(\n            self, net_cls, module_cls, scoring_cls, train_split, data, score55,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(partial(score55, foo=0), name=None)],\n            max_epochs=2,\n            train_split=train_split,\n        )\n        net.fit(*data)\n        assert net.history[:, \'score55\']\n\n    def test_target_extractor_is_called(\n            self, net_cls, module_cls, train_split, scoring_cls, data):\n        X, y = data\n        extractor = Mock(side_effect=to_numpy)\n        scoring = scoring_cls(\n            name=\'nmse\',\n            scoring=\'neg_mean_squared_error\',\n            target_extractor=extractor,\n        )\n        net = net_cls(\n            module_cls, batch_size=1, train_split=train_split,\n            callbacks=[scoring], max_epochs=2)\n        net.fit(X, y)\n\n        # With caching in use the extractor should be called for\n        # each y of a batch. Without caching it should called\n        # once per epoch (since we get all data at once).\n        if scoring.use_caching:\n            assert len(y) // net.batch_size == 4\n            assert extractor.call_count == 4\n        else:\n            assert extractor.call_count == 2\n\n    def test_without_target_data_works(\n            self, net_cls, module_cls, scoring_cls, data,\n    ):\n        score_calls = 0\n\n        def myscore(net, X, y=None):\n            nonlocal score_calls\n            score_calls += 1\n            assert y is None\n\n            # In case we use caching X is a dataset. We need to\n            # extract X ourselves.\n            if dict(net.callbacks_)[\'EpochScoring\'].use_caching:\n                return np.mean(X.X)\n            return np.mean(X)\n\n        # pylint: disable=unused-argument\n        def mysplit(dataset, y):\n            # set y_valid to None\n            ds_train = dataset\n            ds_valid = type(dataset)(dataset.X, y=None)\n            return ds_train, ds_valid\n\n        X, y = data\n        net = net_cls(\n            module=module_cls,\n            callbacks=[scoring_cls(myscore)],\n            train_split=mysplit,\n            max_epochs=2,\n        )\n        net.fit(X, y)\n\n        assert score_calls == 2\n\n        expected = np.mean(X)\n        loss = net.history[:, \'myscore\']\n        assert np.allclose(loss, expected)\n\n    def net_input_is_scoring_input(\n            self, net_cls, module_cls, scoring_cls, input_data,\n            train_split, expected_type, caching,\n    ):\n        score_calls = 0\n        def myscore(net, X, y=None):  # pylint: disable=unused-argument\n            nonlocal score_calls\n            score_calls += 1\n            assert type(X) == expected_type\n            return 0\n\n        max_epochs = 2\n        net = net_cls(\n            module=module_cls,\n            callbacks=[scoring_cls(myscore, use_caching=caching)],\n            train_split=train_split,\n            max_epochs=max_epochs,\n        )\n        net.fit(*input_data)\n        assert score_calls == max_epochs\n\n    def test_net_input_is_scoring_input(\n            self, net_cls, module_cls, scoring_cls, data,\n    ):\n        # Make sure that whatever data type is put in the network is\n        # received at the scoring side as well. For the caching case\n        # we only receive datasets.\n        import skorch\n        from skorch.dataset import CVSplit\n        import torch.utils.data.dataset\n        from torch.utils.data.dataset import Subset\n\n        class MyTorchDataset(torch.utils.data.dataset.TensorDataset):\n            def __init__(self, X, y):\n                super().__init__(\n                    skorch.utils.to_tensor(X.reshape(-1, 1), device=\'cpu\'),\n                    skorch.utils.to_tensor(y, device=\'cpu\'))\n\n        class MySkorchDataset(skorch.dataset.Dataset):\n            pass\n\n        rawsplit = lambda ds, _: (ds, ds)\n        cvsplit = CVSplit(2, random_state=0)\n\n        table = [\n            # Test a split where type(input) == type(output) is guaranteed\n            (data, rawsplit, np.ndarray, False),\n            (data, rawsplit, skorch.dataset.Dataset, True),\n            ((MyTorchDataset(*data), None), rawsplit, MyTorchDataset, False),\n            ((MyTorchDataset(*data), None), rawsplit, MyTorchDataset, True),\n            ((MySkorchDataset(*data), None), rawsplit, np.ndarray, False),\n            ((MySkorchDataset(*data), None), rawsplit, MySkorchDataset, True),\n\n            # Test a split that splits datasets using torch Subset\n            (data, cvsplit, np.ndarray, False),\n            (data, cvsplit, Subset, True),\n            ((MyTorchDataset(*data), None), cvsplit, Subset, False),\n            ((MyTorchDataset(*data), None), cvsplit, Subset, True),\n            ((MySkorchDataset(*data), None), cvsplit, np.ndarray, False),\n            ((MySkorchDataset(*data), None), cvsplit, Subset, True),\n        ]\n\n        for input_data, train_split, expected_type, caching in table:\n            self.net_input_is_scoring_input(\n                net_cls,\n                module_cls,\n                scoring_cls,\n                input_data,\n                train_split,\n                expected_type,\n                caching)\n\n    def test_multiple_scorings_share_cache(\n            self, net_cls, module_cls, train_split, caching_scoring_cls, data,\n    ):\n        net = net_cls(\n            module=module_cls,\n            callbacks=[\n                (\'a1\', caching_scoring_cls(\'accuracy\')),\n                (\'a2\', caching_scoring_cls(\'accuracy\')),\n            ],\n            train_split=train_split,\n            max_epochs=2,\n        )\n\n        # on_train_end clears cache, overwrite so we can inspect the contents.\n        with patch(\'skorch.callbacks.scoring.EpochScoring.on_train_end\',\n                   lambda *x, **y: None):\n            net.fit(*data)\n\n        cbs = dict(net.callbacks_)\n        assert cbs[\'a1\'].use_caching\n        assert cbs[\'a2\'].use_caching\n        assert len(cbs[\'a1\'].y_preds_) > 0\n\n        for c1, c2 in zip(cbs[\'a1\'].y_preds_, cbs[\'a2\'].y_preds_):\n            assert id(c1) == id(c2)\n\n        for c1, c2 in zip(cbs[\'a1\'].y_trues_, cbs[\'a2\'].y_trues_):\n            assert id(c1) == id(c2)\n\n    def test_multiple_scorings_with_dict(\n            self, net_cls, module_cls, train_split, scoring_cls, data):\n        # This test checks if an exception is raised when a dictionary is passed as scorer.\n        net = net_cls(\n            module=module_cls,\n            callbacks=[\n                scoring_cls({\'a1\': \'accuracy\', \'a2\': \'accuracy\'}),\n            ],\n            train_split=train_split,\n            max_epochs=2,\n        )\n\n        msg = ""Dict not supported as scorer for multi-metric scoring""\n        with pytest.raises(ValueError, match=msg):\n                net.fit(*data)\n\n    @pytest.mark.parametrize(\'use_caching, count\', [(False, 1), (True, 0)])\n    def test_with_caching_get_iterator_not_called(\n            self, net_cls, module_cls, train_split, caching_scoring_cls, data,\n            use_caching, count,\n    ):\n        max_epochs = 3\n        net = net_cls(\n            module=module_cls,\n            callbacks=[\n                (\'acc\', caching_scoring_cls(\'accuracy\', use_caching=use_caching)),\n            ],\n            train_split=train_split,\n            max_epochs=max_epochs,\n        )\n\n        get_iterator = net.get_iterator\n        net.get_iterator = Mock(side_effect=get_iterator)\n        net.fit(*data)\n\n        # expected count should be:\n        # max_epochs * (1 (train) + 1 (valid) + 0 or 1 (from scoring,\n        # depending on caching))\n        count_expected = max_epochs * (1 + 1 + count)\n        assert net.get_iterator.call_count == count_expected\n\n    def test_subclassing_epoch_scoring(\n            self, classifier_module, classifier_data):\n        # This test\'s purpose is to check that it is possible to\n        # easily subclass EpochScoring by overriding on_epoch_end to\n        # record 2 scores.\n        from skorch import NeuralNetClassifier\n        from skorch.callbacks import EpochScoring\n\n        class MyScoring(EpochScoring):\n            def on_epoch_end(\n                    self,\n                    net,\n                    dataset_train,\n                    dataset_valid,\n                    **kwargs):\n                _, y_test, y_proba = self.get_test_data(\n                    dataset_train, dataset_valid)\n                y_pred = np.concatenate(y_proba).argmax(1)\n\n                # record 2 valid scores\n                score_0_valid = accuracy_score(y_test, y_pred)\n                net.history.record(\'score_0\', score_0_valid)\n                score_1_valid = accuracy_score(y_test, y_pred) + 1\n                net.history.record(\'score_1\', score_1_valid)\n\n        X, y = classifier_data\n        net = NeuralNetClassifier(\n            classifier_module,\n            callbacks=[MyScoring(scoring=None)],\n            max_epochs=1,\n        )\n        net.fit(X, y)\n\n        row = net.history[-1]\n        keys_history = set(row.keys())\n        keys_expected = {\'score_0\', \'score_1\'}\n\n        assert keys_expected.issubset(keys_history)\n        assert np.isclose(row[\'score_0\'], row[\'score_1\'] - 1)\n\n\nclass TestBatchScoring:\n    @pytest.fixture(params=[{\'use_caching\': True}, {\'use_caching\': False}])\n    def scoring_cls(self, request):\n        from skorch.callbacks import BatchScoring\n        return partial(BatchScoring, **request.param)\n\n    @pytest.fixture\n    def mse_scoring(self, scoring_cls):\n        return scoring_cls(\n            name=\'nmse\',\n            scoring=\'neg_mean_squared_error\',\n        ).initialize()\n\n    @pytest.fixture\n    def net(self, net_cls, module_cls, train_split, mse_scoring, data):\n        net = net_cls(\n            module_cls, batch_size=1, train_split=train_split,\n            callbacks=[mse_scoring], max_epochs=2)\n        return net.fit(*data)\n\n    @pytest.fixture\n    def train_loss(self, scoring_cls):\n        from skorch.utils import train_loss_score\n        return scoring_cls(\n            train_loss_score,\n            name=\'train_loss\',\n            on_train=True,\n        ).initialize()\n\n    @pytest.fixture\n    def valid_loss(self, scoring_cls):\n        from skorch.utils import valid_loss_score\n        return scoring_cls(\n            valid_loss_score,\n            name=\'valid_loss\',\n        ).initialize()\n\n    @pytest.fixture\n    def history(self, net):\n        return net.history\n\n    def test_correct_train_loss_values(self, history):\n        train_losses = history[:, \'train_loss\']\n        expected = np.mean([(0 - -1) ** 2, (2 - 0) ** 2])\n        assert np.allclose(train_losses, expected)\n\n    def test_correct_valid_loss_values(self, history):\n        valid_losses = history[:, \'valid_loss\']\n        expected = np.mean([(3 - 5) ** 2, (0 - 4) ** 2])\n        assert np.allclose(valid_losses, expected)\n\n    def test_correct_mse_values_for_batches(self, history):\n        nmse = history[:, \'batches\', :, \'nmse\']\n        expected_per_epoch = [-(3 - 5) ** 2, -(0 - 4) ** 2]\n        # for the 2 epochs, the loss is the same\n        expected = [expected_per_epoch, expected_per_epoch]\n        assert np.allclose(nmse, expected)\n\n    def test_missing_batch_size(self, train_loss, history):\n        """"""We skip one batch size entry in history. This batch should\n        simply be ignored.\n\n        """"""\n        history.new_epoch()\n        history.new_batch()\n        history.record_batch(\'train_loss\', 10)\n        history.record_batch(\'train_batch_size\', 1)\n        history.new_batch()\n        history.record_batch(\'train_loss\', 20)\n        # missing batch size, loss of 20 is ignored\n\n        net = Mock(history=history)\n        train_loss.on_epoch_end(net)\n\n        assert history[-1, \'train_loss\'] == 10\n\n    def test_average_honors_weights(self, train_loss, history):\n        """"""The batches may have different batch sizes, which is why it\n        necessary to honor the batch sizes. Here we use different\n        batch sizes to verify this.\n\n        """"""\n        from skorch.history import History\n\n        history = History()\n        history.new_epoch()\n        history.new_batch()\n        history.record_batch(\'train_loss\', 10)\n        history.record_batch(\'train_batch_size\', 1)\n        history.new_batch()\n        history.record_batch(\'train_loss\', 40)\n        history.record_batch(\'train_batch_size\', 2)\n\n        net = Mock(history=history)\n        train_loss.on_epoch_end(net)\n\n        assert history[0, \'train_loss\'] == 30\n\n    @pytest.mark.parametrize(\'lower_is_better, expected\', [\n        (True, [True, True, True, False, False]),\n        (False, [True, False, False, True, False]),\n    ])\n    @pytest.mark.parametrize(\'initial_epochs\', [1, 2, 3, 4])\n    def test_scoring_uses_best_score_when_continuing_training(\n            self, net_cls, module_cls, scoring_cls, data,\n            lower_is_better, expected, tmpdir, initial_epochs,\n    ):\n        # set scoring to None so that mocked net.score is used\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(\n                scoring=None,\n                on_train=True,\n                lower_is_better=lower_is_better)],\n            max_epochs=initial_epochs,\n            # Set train_split to None so that the default \'valid_loss\'\n            # callback is effectively disabled and does not write to\n            # the history. This should not cause problems when trying\n            # to load best score for this scorer.\n            train_split=None,\n        )\n\n        with patch.object(net, \'score\', side_effect=[10, 8, 6, 11, 7]):\n            net.fit(*data)\n\n            history_fn = tmpdir.mkdir(\'skorch\').join(\'history.json\')\n            net.save_params(f_history=str(history_fn))\n\n            net.max_epochs = 5 - initial_epochs\n            net.initialize()\n            net.load_params(f_history=str(history_fn))\n            net.partial_fit(*data)\n\n        is_best = net.history[:, \'score_best\']\n        assert is_best == expected\n\n    @pytest.mark.parametrize(\'lower_is_better, expected\', [\n        (True, [True, True, True, False, False]),\n        (False, [True, False, False, True, False]),\n        (None, []),\n    ])\n    def test_best_score_when_lower_is_better(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n            lower_is_better, expected,\n    ):\n        # set scoring to None so that mocked net.score is used\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(\n                scoring=None,\n                lower_is_better=lower_is_better)],\n            train_split=train_split,\n            max_epochs=5,\n        )\n        with patch.object(net, \'score\', side_effect=[10, 8, 6, 11, 7]):\n            net.fit(*data)\n\n        if lower_is_better is not None:\n            is_best = net.history[:, \'score_best\']\n            assert is_best == expected\n        else:\n            # if lower_is_better==None, don\'t write score\n            with pytest.raises(KeyError):\n                # pylint: disable=pointless-statement\n                net.history[:, \'score_best\']\n\n    def test_no_error_when_no_valid_data(\n            self, net_cls, module_cls, mse_scoring, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[mse_scoring],\n            max_epochs=1,\n            train_split=None,\n        )\n        # does not raise\n        net.fit(*data)\n\n    def test_with_accuracy_score(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(\'accuracy\')],\n            batch_size=1,\n            max_epochs=2,\n            train_split=train_split,\n        )\n        net.fit(*data)\n\n        score_epochs = net.history[:, \'accuracy\']\n        assert np.allclose(score_epochs, [0, 0])\n\n        score_batches = net.history[:, \'batches\', :, \'accuracy\']\n        assert np.allclose(score_batches, [[0, 0], [0, 0]])\n\n    def test_with_make_scorer_accuracy_score(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(make_scorer(accuracy_score))],\n            batch_size=1,\n            max_epochs=2,\n            train_split=train_split,\n        )\n        net.fit(*data)\n\n        score_epochs = net.history[:, \'accuracy_score\']\n        assert np.allclose(score_epochs, [0, 0])\n\n        score_batches = net.history[:, \'batches\', :, \'accuracy_score\']\n        assert np.allclose(score_batches, [[0, 0], [0, 0]])\n\n    def test_with_callable_accuracy_score(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(accuracy_score)],\n            batch_size=1,\n            max_epochs=2,\n            train_split=train_split,\n        )\n        net.fit(*data)\n\n        score_epochs = net.history[:, \'accuracy_score\']\n        assert np.allclose(score_epochs, [0, 0])\n\n        score_batches = net.history[:, \'batches\', :, \'accuracy_score\']\n        assert np.allclose(score_batches, [[0, 0], [0, 0]])\n\n    def test_with_score_nonexisting_string(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(\'does-not-exist\')],\n            max_epochs=2,\n            train_split=train_split,\n        )\n        with pytest.raises(ValueError) as exc:\n            net.fit(*data)\n        msg = ""\'does-not-exist\' is not a valid scoring value.""\n        assert exc.value.args[0].startswith(msg)\n\n    def test_with_score_as_custom_func(\n            self, net_cls, module_cls, scoring_cls, train_split, data, score55,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(score55)],\n            max_epochs=2,\n            train_split=train_split,\n        )\n        net.fit(*data)\n\n        score_epochs = net.history[:, \'score55\']\n        assert np.allclose(score_epochs, [55, 55])\n\n        score_batches = net.history[:, \'batches\', :, \'score55\']\n        assert np.allclose(score_batches, [[55, 55], [55, 55]])\n\n    def test_with_name_none_returns_score_as_name(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(scoring=None, name=None)],\n            max_epochs=1,\n            train_split=train_split,\n        )\n        net.fit(*data)\n        assert net.history[:, \'score\']\n\n    def test_explicit_name_is_used_in_history(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(scoring=None, name=\'myname\')],\n            max_epochs=1,\n            train_split=train_split,\n        )\n        net.fit(*data)\n        assert net.history[:, \'myname\']\n\n    def test_with_scoring_str_and_name_none(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(\n                scoring=\'neg_mean_squared_error\', name=None)],\n            max_epochs=1,\n            train_split=train_split,\n        )\n        net.fit(*data)\n        assert net.history[:, \'neg_mean_squared_error\']\n\n    def test_with_with_custom_func_and_name_none(\n            self, net_cls, module_cls, scoring_cls, train_split, data, score55,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(score55, name=None)],\n            max_epochs=2,\n            train_split=train_split,\n        )\n        net.fit(*data)\n        assert net.history[:, \'score55\']\n\n    def test_with_with_partial_custom_func_and_name_none(\n            self, net_cls, module_cls, scoring_cls, train_split, data, score55,\n    ):\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(partial(score55, foo=0), name=None)],\n            max_epochs=2,\n            train_split=train_split,\n        )\n        net.fit(*data)\n        assert net.history[:, \'score55\']\n\n    def test_target_extractor_is_called(\n            self, net_cls, module_cls, train_split, scoring_cls, data):\n        X, y = data\n        extractor = Mock(side_effect=to_numpy)\n        scoring = scoring_cls(\n            name=\'nmse\',\n            scoring=\'neg_mean_squared_error\',\n            target_extractor=extractor,\n        )\n        net = net_cls(\n            module_cls, batch_size=1, train_split=train_split,\n            callbacks=[scoring], max_epochs=2)\n        net.fit(X, y)\n\n        assert extractor.call_count == 2 * 2\n\n    def test_without_target_data_works(\n            self, net_cls, module_cls, scoring_cls, data,\n    ):\n        score_calls = 0\n\n        def myscore(net, X, y=None):  # pylint: disable=unused-argument\n            nonlocal score_calls\n            score_calls += 1\n            assert y is None\n            return X.mean().data.item()\n\n        # pylint: disable=unused-argument\n        def mysplit(dataset, y):\n            # set y_valid to None\n            ds_train = dataset\n            ds_valid = type(dataset)(dataset.X, y=None)\n            return ds_train, ds_valid\n\n        X, y = data\n        net = net_cls(\n            module=module_cls,\n            callbacks=[scoring_cls(myscore)],\n            train_split=mysplit,\n            max_epochs=2,\n        )\n        net.fit(X, y)\n\n        assert score_calls == 2\n\n        expected = np.mean(X)\n        loss = net.history[:, \'myscore\']\n        assert np.allclose(loss, expected)\n\n    def test_scoring_with_cache_and_fit_interrupt_resets_infer(\n            self, net_cls, module_cls, scoring_cls, data, train_split):\n        # This test addresses a bug that occurred with caching in\n        # scoring when training is interrupted irregularly\n        # (e.g. through KeyboardInterrupt). In that case, it is\n        # important that the net\'s infer method is still reset.\n\n        def interrupt_scoring(net, X, y):\n            raise KeyboardInterrupt\n\n        X, y = data\n        net = net_cls(\n            module_cls,\n            callbacks=[(\'interrupt\', scoring_cls(interrupt_scoring))],\n            train_split=train_split,\n        )\n        net.fit(X, y)\n\n        y_pred = net.predict(X)\n        # We test that we predict as many outputs as we put in. With\n        # the bug, the cache would be exhausted early because of the\n        # train split, and we would get back less.\n        assert len(y_pred) == len(X)\n\n\nclass TestPassthrougScoring:\n    @pytest.fixture\n    def scoring_cls(self, request):\n        from skorch.callbacks import PassthroughScoring\n        return PassthroughScoring\n\n    @pytest.fixture\n    def train_loss(self, scoring_cls):\n        # use train batch size to stand in for batch-level scores\n        return scoring_cls(name=\'train_batch_size\', on_train=True)\n\n    @pytest.fixture\n    def valid_loss(self, scoring_cls):\n        # use valid batch size to stand in for batch-level scores\n        return scoring_cls(name=\'valid_batch_size\')\n\n    @pytest.fixture\n    def net(self, classifier_module, train_loss, valid_loss, classifier_data):\n        from skorch import NeuralNetClassifier\n        net = NeuralNetClassifier(\n            classifier_module,\n            batch_size=10,\n            # use train and valid batch size to stand in for\n            # batch-level scores\n            callbacks=[train_loss, valid_loss],\n            max_epochs=2)\n\n        X, y = classifier_data\n        n = 75\n        # n=75 with a 4/5 train/valid split -> 60/15 samples; with a\n        # batch size of 10, that leads to train batch sizes of\n        # [10,10,10,10] and valid batch sizes of [10,5]; all labels\n        # are set to 0 to ensure that the stratified split is exactly\n        # equal to the desired split\n        y = np.zeros_like(y)\n        return net.fit(X[:n], y[:n])\n\n    @pytest.fixture\n    def history(self, net):\n        return net.history\n\n    @pytest.fixture\n    def history_empty(self):\n        from skorch.history import History\n        return History()\n\n    def test_correct_train_pass_through_scores(self, history):\n        # train: average of [10,10,10,10,10] is 10\n        train_scores = history[:, \'train_batch_size\']\n        assert np.allclose(train_scores, 10.0)\n\n    def test_correct_valid_pass_through_scores(self, history):\n        # valid: average of [10,5] with weights also being [10,5] =\n        # (10*10 + 5*5)/15\n        expected = (10 * 10 + 5 * 5) / 15  # 8.333..\n        valid_losses = history[:, \'valid_batch_size\']\n        assert np.allclose(valid_losses, [expected, expected])\n\n    def test_missing_entry_in_epoch(self, scoring_cls, history_empty):\n        """"""We skip one entry in history_empty. This batch should simply be\n        ignored.\n\n        """"""\n        history_empty.new_epoch()\n        history_empty.new_batch()\n        history_empty.record_batch(\'score\', 10)\n        history_empty.record_batch(\'train_batch_size\', 10)\n\n        history_empty.new_batch()\n        # this score is ignored since it has no associated batch size\n        history_empty.record_batch(\'score\', 20)\n\n        net = Mock(history=history_empty)\n        cb = scoring_cls(name=\'score\', on_train=True).initialize()\n        cb.on_epoch_end(net)\n\n        train_score = history_empty[-1, \'score\']\n        assert np.isclose(train_score, 10.0)\n\n    @pytest.mark.parametrize(\'lower_is_better, expected\', [\n        (True, [True, True, True, False, False]),\n        (False, [True, False, False, True, False]),\n        (None, []),\n    ])\n    def test_lower_is_better_is_honored(\n            self, net_cls, module_cls, scoring_cls, train_split, data,\n            history_empty, lower_is_better, expected,\n    ):\n        # results in expected patterns of True and False\n        scores = [10, 8, 6, 11, 7]\n\n        cb = scoring_cls(\n            name=\'score\',\n            lower_is_better=lower_is_better,\n        ).initialize()\n\n        net = Mock(history=history_empty)\n        for score in scores:\n            history_empty.new_epoch()\n            history_empty.new_batch()\n            history_empty.record_batch(\'score\', score)\n            history_empty.record_batch(\'valid_batch_size\', 55)  # doesn\'t matter\n            cb.on_epoch_end(net)\n\n        if lower_is_better is not None:\n            is_best = history_empty[:, \'score_best\']\n            assert is_best == expected\n        else:\n            # if lower_is_better==None, don\'t write score\n            with pytest.raises(KeyError):\n                # pylint: disable=pointless-statement\n                history_empty[:, \'score_best\']\n\n    def test_no_error_when_no_valid_data(\n            self, net_cls, module_cls, scoring_cls, data,\n    ):\n        # we set the name to \'valid_batch_size\' but disable\n        # train/valid split -- there should be no error\n        net = net_cls(\n            module_cls,\n            callbacks=[scoring_cls(name=\'valid_batch_size\')],\n            max_epochs=1,\n            train_split=None,\n        )\n        # does not raise\n        net.fit(*data)\n'"
skorch/tests/callbacks/test_training.py,1,"b'""""""Tests for callbacks in training.py""""""\n\nfrom functools import partial\nfrom unittest.mock import Mock\nfrom unittest.mock import patch\nfrom unittest.mock import call\n\nimport numpy as np\nimport pytest\nfrom sklearn.base import clone\n\n\nclass TestCheckpoint:\n    @pytest.fixture\n    def checkpoint_cls(self):\n        from skorch.callbacks import Checkpoint\n        return Checkpoint\n\n    @pytest.fixture\n    def save_params_mock(self):\n        with patch(\'skorch.NeuralNet.save_params\') as mock:\n            yield mock\n\n    @pytest.fixture\n    def pickle_dump_mock(self):\n        with patch(\'pickle.dump\') as mock:\n            yield mock\n\n    @pytest.fixture\n    def net_cls(self):\n        """"""very simple network that trains for 10 epochs""""""\n        from skorch import NeuralNetRegressor\n        from skorch.toy import make_regressor\n\n        module_cls = make_regressor(\n            input_units=1,\n            num_hidden=0,\n            output_units=1,\n        )\n\n        return partial(\n            NeuralNetRegressor,\n            module=module_cls,\n            max_epochs=10,\n            batch_size=10)\n\n    @pytest.fixture(scope=\'module\')\n    def data(self):\n        # have 10 examples so we can do a nice CV split\n        X = np.zeros((10, 1), dtype=\'float32\')\n        y = np.zeros((10, 1), dtype=\'float32\')\n        return X, y\n\n    def test_none_monitor_saves_always(\n            self, save_params_mock, net_cls, checkpoint_cls, data):\n        sink = Mock()\n        net = net_cls(callbacks=[\n            checkpoint_cls(monitor=None, sink=sink,\n                           event_name=\'event_another\'),\n        ])\n        net.fit(*data)\n\n        assert save_params_mock.call_count == 3*len(net.history)\n        assert sink.call_count == len(net.history)\n        assert all((x is True) for x in net.history[:, \'event_another\'])\n\n    @pytest.mark.parametrize(\'message,files\', [\n        (\'Unable to save model parameters to params.pt, \'\n         \'Exception: encoding error\',\n         {\'f_params\': \'params.pt\', \'f_optimizer\': None, \'f_history\': None}),\n        (\'Unable to save optimizer state to optimizer.pt, \'\n         \'Exception: encoding error\',\n         {\'f_params\': None, \'f_optimizer\': \'optimizer.pt\', \'f_history\': None}),\n        (\'Unable to save history to history.json, \'\n         \'Exception: encoding error\',\n         {\'f_params\': None, \'f_optimizer\': None, \'f_history\': \'history.json\'})\n    ])\n    def test_outputs_to_sink_when_save_params_errors(\n            self, save_params_mock, net_cls, checkpoint_cls, data,\n            message, files):\n        sink = Mock()\n        save_params_mock.side_effect = Exception(\'encoding error\')\n        net = net_cls(callbacks=[\n            checkpoint_cls(monitor=None, sink=sink, **files)\n        ])\n        net.fit(*data)\n\n        assert save_params_mock.call_count == len(net.history)\n        assert sink.call_count == 2*len(net.history)\n        save_error_messages = [call(message)] * len(net.history)\n        sink.assert_has_calls(save_error_messages, any_order=True)\n\n    @pytest.mark.parametrize(\'f_name, mode\', [\n        (\'f_params\', \'w\'),\n        (\'f_optimizer\', \'w\'),\n        (\'f_history\', \'w\'),\n        (\'f_pickle\', \'wb\')\n    ])\n    def test_init_with_dirname_and_file_like_object_error(\n            self, checkpoint_cls, tmpdir, f_name, mode):\n        from skorch.exceptions import SkorchException\n\n        skorch_dir = tmpdir.mkdir(""skorch"")\n        exp_dir = skorch_dir.join(""exp1"")\n        f = skorch_dir.join(f_name + "".pt"")\n\n        with f.open(mode) as fp:\n            with pytest.raises(SkorchException) as e:\n                checkpoint_cls(**{f_name: fp}, dirname=str(exp_dir))\n        expected = ""dirname can only be used when f_* are strings""\n        assert str(e.value) == expected\n\n    @pytest.mark.parametrize(\'f_name, mode\', [\n        (\'f_params\', \'w\'),\n        (\'f_optimizer\', \'w\'),\n        (\'f_history\', \'w\'),\n        (\'f_pickle\', \'wb\')\n    ])\n    def test_initialize_with_dirname_and_file_like_object_error(\n            self, checkpoint_cls, tmpdir, f_name, mode):\n        from skorch.exceptions import SkorchException\n\n        skorch_dir = tmpdir.mkdir(""skorch"")\n        exp_dir = skorch_dir.join(""exp1"")\n        f = skorch_dir.join(f_name + "".pt"")\n\n        with f.open(mode) as fp:\n            with pytest.raises(SkorchException) as e:\n                cp = checkpoint_cls(dirname=str(exp_dir))\n                setattr(cp, f_name, fp)\n                cp.initialize()\n        expected = ""dirname can only be used when f_* are strings""\n        assert str(e.value) == expected\n\n    def test_default_without_validation_raises_meaningful_error(\n            self, net_cls, checkpoint_cls, data):\n        net = net_cls(\n            callbacks=[\n                checkpoint_cls(),\n            ],\n            train_split=None\n        )\n        from skorch.exceptions import SkorchException\n        with pytest.raises(SkorchException) as e:\n            net.fit(*data)\n            expected = (\n                ""Monitor value \'{}\' cannot be found in history. ""\n                ""Make sure you have validation data if you use ""\n                ""validation scores for checkpointing."".format(\n                    \'valid_loss_best\')\n            )\n            assert str(e.value) == expected\n\n    def test_string_monitor_and_formatting(\n            self, save_params_mock, net_cls, checkpoint_cls, data):\n        def epoch_3_scorer(net, *_):\n            return 1 if net.history[-1, \'epoch\'] == 3 else 0\n\n        from skorch.callbacks import EpochScoring\n        scoring = EpochScoring(\n            scoring=epoch_3_scorer, on_train=True, lower_is_better=False)\n\n        sink = Mock()\n        cb = checkpoint_cls(\n            monitor=\'epoch_3_scorer_best\',\n            f_params=\'model_{last_epoch[epoch]}_{net.max_epochs}.pt\',\n            f_optimizer=\'optimizer_{last_epoch[epoch]}_{net.max_epochs}.pt\',\n            sink=sink)\n        net = net_cls(callbacks=[\n            (\'my_score\', scoring), cb\n        ])\n        net.fit(*data)\n\n        assert save_params_mock.call_count == 6\n        assert cb.get_formatted_files(net) == {\n            \'f_params\': \'model_3_10.pt\',\n            \'f_optimizer\': \'optimizer_3_10.pt\',\n            \'f_history\': \'history.json\',\n            \'f_pickle\': None\n        }\n        save_params_mock.assert_has_calls(\n            [call(f_params=\'model_1_10.pt\'),\n             call(f_optimizer=\'optimizer_1_10.pt\'),\n             call(f_history=\'history.json\'),\n             call(f_params=\'model_3_10.pt\'),\n             call(f_optimizer=\'optimizer_3_10.pt\'),\n             call(f_history=\'history.json\')]\n        )\n        assert sink.call_count == 2\n        # The first epoch will always be saved. `epoch_3_scorer` returns 1 at\n        # epoch 3, which will trigger another checkpoint. For all other epochs\n        # `epoch_3_scorer` returns 0, which does not trigger a checkpoint.\n        assert [True, False, True] + [False] * 7 == net.history[:, \'event_cp\']\n\n    def test_save_all_targets(\n            self, save_params_mock, pickle_dump_mock,\n            net_cls, checkpoint_cls, data):\n        net = net_cls(callbacks=[\n            checkpoint_cls(\n                monitor=None, f_params=\'params.pt\',\n                f_history=\'history.json\', f_pickle=\'model.pkl\',\n                f_optimizer=\'optimizer.pt\'),\n        ])\n        net.fit(*data)\n\n        assert save_params_mock.call_count == 3*len(net.history)\n        assert pickle_dump_mock.call_count == len(net.history)\n\n        print(save_params_mock.call_args_list)\n        save_params_mock.assert_has_calls(\n            [call(f_params=\'params.pt\'),\n             call(f_optimizer=\'optimizer.pt\'),\n             call(f_history=\'history.json\')] * len(net.history)\n        )\n\n    def test_save_all_targets_with_prefix(\n            self, save_params_mock, pickle_dump_mock,\n            net_cls, checkpoint_cls, data):\n\n        cp = checkpoint_cls(\n            monitor=None,\n            f_params=\'params.pt\',\n            f_history=\'history.json\',\n            f_pickle=\'model.pkl\',\n            f_optimizer=\'optimizer.pt\',\n            fn_prefix=""exp1_"")\n        net = net_cls(callbacks=[cp])\n        net.fit(*data)\n\n        assert cp.f_history_ == ""exp1_history.json""\n        assert save_params_mock.call_count == 3*len(net.history)\n        assert pickle_dump_mock.call_count == len(net.history)\n        save_params_mock.assert_has_calls(\n            [call(f_params=\'exp1_params.pt\'),\n             call(f_optimizer=\'exp1_optimizer.pt\'),\n             call(f_history=\'exp1_history.json\')] * len(net.history)\n        )\n\n    def test_save_all_targets_with_prefix_and_dirname(\n            self, save_params_mock, pickle_dump_mock,\n            net_cls, checkpoint_cls, data, tmpdir):\n\n        skorch_dir = tmpdir.mkdir(\'skorch\').join(\'exp1\')\n\n        cp = checkpoint_cls(\n            monitor=None,\n            f_params=\'params.pt\',\n            f_history=\'history.json\',\n            f_pickle=\'model.pkl\',\n            f_optimizer=\'optimizer.pt\',\n            fn_prefix=""unet_"",\n            dirname=str(skorch_dir))\n        net = net_cls(callbacks=[cp])\n        net.fit(*data)\n\n        f_params = skorch_dir.join(\'unet_params.pt\')\n        f_optimizer = skorch_dir.join(\'unet_optimizer.pt\')\n        f_history = skorch_dir.join(\'unet_history.json\')\n\n        assert cp.f_history_ == str(f_history)\n        assert save_params_mock.call_count == 3*len(net.history)\n        assert pickle_dump_mock.call_count == len(net.history)\n        save_params_mock.assert_has_calls(\n            [call(f_params=str(f_params)),\n             call(f_optimizer=str(f_optimizer)),\n             call(f_history=str(f_history))] * len(net.history)\n        )\n        assert skorch_dir.exists()\n\n    def test_save_no_targets(\n            self, save_params_mock, pickle_dump_mock,\n            net_cls, checkpoint_cls, data):\n        net = net_cls(callbacks=[\n            checkpoint_cls(\n                monitor=None, f_params=None, f_optimizer=None,\n                f_history=None, f_pickle=None),\n        ])\n        net.fit(*data)\n\n        assert save_params_mock.call_count == 0\n        assert pickle_dump_mock.call_count == 0\n\n    def test_warnings_when_monitor_appears_in_history(\n            self, net_cls, checkpoint_cls, save_params_mock, data):\n        net = net_cls(callbacks=[\n            checkpoint_cls(monitor=""valid_loss"")],\n            max_epochs=1)\n\n        exp_warn = (\n            ""Checkpoint monitor parameter is set to \'valid_loss\' and the ""\n            ""history contains \'valid_loss_best\'. Perhaps you meant to set the ""\n            ""parameter to \'valid_loss_best\'"")\n\n        with pytest.warns(UserWarning, match=exp_warn):\n            net.fit(*data)\n        assert save_params_mock.call_count == 3\n\n\nclass TestEarlyStopping:\n\n    @pytest.fixture\n    def early_stopping_cls(self):\n        from skorch.callbacks import EarlyStopping\n        return EarlyStopping\n\n    @pytest.fixture\n    def epoch_scoring_cls(self):\n        from skorch.callbacks import EpochScoring\n        return EpochScoring\n\n    @pytest.fixture\n    def net_clf_cls(self):\n        from skorch import NeuralNetClassifier\n        return NeuralNetClassifier\n\n    @pytest.fixture\n    def broken_classifier_module(self, classifier_module):\n        """"""Return a classifier that does not improve over time.""""""\n        class BrokenClassifier(classifier_module.func):\n            def forward(self, x):\n                return super().forward(x) * 0 + 0.5\n        return BrokenClassifier\n\n    def test_typical_use_case_nonstop(\n            self, net_clf_cls, classifier_module, classifier_data,\n            early_stopping_cls):\n        patience = 5\n        max_epochs = 8\n        early_stopping_cb = early_stopping_cls(patience=patience)\n\n        net = net_clf_cls(\n            classifier_module,\n            callbacks=[\n                early_stopping_cb,\n            ],\n            max_epochs=max_epochs,\n        )\n        net.fit(*classifier_data)\n\n        assert len(net.history) == max_epochs\n\n    def test_typical_use_case_stopping(\n            self, net_clf_cls, broken_classifier_module, classifier_data,\n            early_stopping_cls):\n        patience = 5\n        max_epochs = 8\n        side_effect = []\n\n        def sink(x):\n            side_effect.append(x)\n\n        early_stopping_cb = early_stopping_cls(patience=patience, sink=sink)\n\n        net = net_clf_cls(\n            broken_classifier_module,\n            callbacks=[\n                early_stopping_cb,\n            ],\n            max_epochs=max_epochs,\n        )\n        net.fit(*classifier_data)\n\n        assert len(net.history) == patience + 1 < max_epochs\n\n        # check correct output message\n        assert len(side_effect) == 1\n        msg = side_effect[0]\n        expected_msg = (""Stopping since valid_loss has not improved in ""\n                        ""the last 5 epochs."")\n        assert msg == expected_msg\n\n    def test_custom_scoring_nonstop(\n            self, net_clf_cls, classifier_module, classifier_data,\n            early_stopping_cls, epoch_scoring_cls,\n    ):\n        lower_is_better = False\n        scoring_name = \'valid_roc_auc\'\n        patience = 5\n        max_epochs = 8\n        scoring_mock = Mock(side_effect=list(range(2, 10)))\n        scoring_cb = epoch_scoring_cls(\n            scoring_mock, lower_is_better, name=scoring_name)\n        early_stopping_cb = early_stopping_cls(\n            patience=patience, lower_is_better=lower_is_better,\n            monitor=scoring_name)\n\n        net = net_clf_cls(\n            classifier_module,\n            callbacks=[\n                scoring_cb,\n                early_stopping_cb,\n            ],\n            max_epochs=max_epochs,\n        )\n        net.fit(*classifier_data)\n\n        assert len(net.history) == max_epochs\n\n    def test_custom_scoring_stop(\n            self, net_clf_cls, broken_classifier_module, classifier_data,\n            early_stopping_cls, epoch_scoring_cls,\n    ):\n        lower_is_better = False\n        scoring_name = \'valid_roc_auc\'\n        patience = 5\n        max_epochs = 8\n        scoring_cb = epoch_scoring_cls(\n            \'roc_auc\', lower_is_better, name=scoring_name)\n        early_stopping_cb = early_stopping_cls(\n            patience=patience, lower_is_better=lower_is_better,\n            monitor=scoring_name)\n\n        net = net_clf_cls(\n            broken_classifier_module,\n            callbacks=[\n                scoring_cb,\n                early_stopping_cb,\n            ],\n            max_epochs=max_epochs,\n        )\n        net.fit(*classifier_data)\n\n        assert len(net.history) < max_epochs\n\n    def test_stopping_big_absolute_threshold(\n            self, net_clf_cls, classifier_module, classifier_data,\n            early_stopping_cls):\n        patience = 5\n        max_epochs = 8\n        early_stopping_cb = early_stopping_cls(patience=patience,\n                                               threshold_mode=\'abs\',\n                                               threshold=0.1)\n\n        net = net_clf_cls(\n            classifier_module,\n            callbacks=[\n                early_stopping_cb,\n            ],\n            max_epochs=max_epochs,\n        )\n        net.fit(*classifier_data)\n\n        assert len(net.history) == patience + 1 < max_epochs\n\n    def test_wrong_threshold_mode(\n            self, net_clf_cls, classifier_module, classifier_data,\n            early_stopping_cls):\n        patience = 5\n        max_epochs = 8\n        early_stopping_cb = early_stopping_cls(\n            patience=patience, threshold_mode=\'incorrect\')\n        net = net_clf_cls(\n            classifier_module,\n            callbacks=[\n                early_stopping_cb,\n            ],\n            max_epochs=max_epochs,\n        )\n\n        with pytest.raises(ValueError) as exc:\n            net.fit(*classifier_data)\n\n        expected_msg = ""Invalid threshold mode: \'incorrect\'""\n        assert exc.value.args[0] == expected_msg\n\n\n\nclass TestParamMapper:\n\n    @pytest.fixture\n    def initializer(self):\n        from skorch.callbacks import Initializer\n        return Initializer\n\n    @pytest.fixture\n    def freezer(self):\n        from skorch.callbacks import Freezer\n        return Freezer\n\n    @pytest.fixture\n    def unfreezer(self):\n        from skorch.callbacks import Unfreezer\n        return Unfreezer\n\n    @pytest.fixture\n    def param_mapper(self):\n        from skorch.callbacks import ParamMapper\n        return ParamMapper\n\n    @pytest.fixture\n    def net_cls(self):\n        from skorch import NeuralNetClassifier\n        return NeuralNetClassifier\n\n    @pytest.mark.parametrize(\'at\', [0, -1])\n    def test_subzero_at_fails(self, net_cls, classifier_module,\n                              param_mapper, at):\n        cb = param_mapper(patterns=\'*\', at=at)\n        net = net_cls(classifier_module, callbacks=[cb])\n        with pytest.raises(ValueError):\n            net.initialize()\n\n    @pytest.mark.parametrize(\'mod_init\', [False, True])\n    @pytest.mark.parametrize(\'weight_pattern\', [\n        \'sequential.*.weight\',\n        lambda name: name.startswith(\'sequential\') and name.endswith(\'.weight\'),\n    ])\n    def test_initialization_is_effective(self, net_cls, classifier_module,\n                                         classifier_data, initializer,\n                                         mod_init, weight_pattern):\n        from torch.nn.init import constant_\n        from skorch.utils import to_numpy\n\n        module = classifier_module() if mod_init else classifier_module\n\n        net = net_cls(\n            module,\n            lr=0,\n            max_epochs=1,\n            callbacks=[\n                initializer(weight_pattern, partial(constant_, val=5)),\n                initializer(\'sequential.3.bias\', partial(constant_, val=10)),\n            ])\n\n        net.fit(*classifier_data)\n\n        assert np.allclose(to_numpy(net.module_.sequential[0].weight), 5)\n        assert np.allclose(to_numpy(net.module_.sequential[3].weight), 5)\n        assert np.allclose(to_numpy(net.module_.sequential[3].bias), 10)\n\n    @pytest.mark.parametrize(\'mod_init\', [False, True])\n    @pytest.mark.parametrize(\'mod_kwargs\', [\n        {},\n        # Supply a module__ parameter so the model is forced\n        # to re-initialize. Even then parameters should be\n        # frozen correctly.\n        {\'module__hidden_units\': 5},\n    ])\n    def test_freezing_is_effective(self, net_cls, classifier_module,\n                                   classifier_data, freezer, mod_init,\n                                   mod_kwargs):\n        from skorch.utils import to_numpy\n\n        module = classifier_module() if mod_init else classifier_module\n\n        net = net_cls(\n            module,\n            max_epochs=2,\n            callbacks=[\n                freezer(\'sequential.*.weight\'),\n                freezer(\'sequential.3.bias\'),\n            ],\n            **mod_kwargs)\n\n        net.initialize()\n\n        assert net.module_.sequential[0].weight.requires_grad\n        assert net.module_.sequential[3].weight.requires_grad\n        assert net.module_.sequential[0].bias.requires_grad\n        assert net.module_.sequential[3].bias.requires_grad\n\n        dense0_weight_pre = to_numpy(net.module_.sequential[0].weight).copy()\n        dense1_weight_pre = to_numpy(net.module_.sequential[3].weight).copy()\n        dense0_bias_pre = to_numpy(net.module_.sequential[0].bias).copy()\n        dense1_bias_pre = to_numpy(net.module_.sequential[3].bias).copy()\n\n        # use partial_fit to not re-initialize the module (weights)\n        net.partial_fit(*classifier_data)\n\n        dense0_weight_post = to_numpy(net.module_.sequential[0].weight).copy()\n        dense1_weight_post = to_numpy(net.module_.sequential[3].weight).copy()\n        dense0_bias_post = to_numpy(net.module_.sequential[0].bias).copy()\n        dense1_bias_post = to_numpy(net.module_.sequential[3].bias).copy()\n\n        assert not net.module_.sequential[0].weight.requires_grad\n        assert not net.module_.sequential[3].weight.requires_grad\n        assert net.module_.sequential[0].bias.requires_grad\n        assert not net.module_.sequential[3].bias.requires_grad\n\n        assert np.allclose(dense0_weight_pre, dense0_weight_post)\n        assert np.allclose(dense1_weight_pre, dense1_weight_post)\n        assert not np.allclose(dense0_bias_pre, dense0_bias_post)\n        assert np.allclose(dense1_bias_pre, dense1_bias_post)\n\n    def test_unfreezing_is_effective(self, net_cls, classifier_module,\n                                     classifier_data, freezer, unfreezer):\n        from skorch.utils import to_numpy\n\n        net = net_cls(\n            classifier_module,\n            max_epochs=1,\n            callbacks=[\n                freezer(\'sequential.*.weight\'),\n                freezer(\'sequential.3.bias\'),\n                unfreezer(\'sequential.*.weight\', at=2),\n                unfreezer(\'sequential.3.bias\', at=2),\n            ])\n\n        net.initialize()\n\n        # epoch 1, freezing parameters\n        net.partial_fit(*classifier_data)\n\n        assert not net.module_.sequential[0].weight.requires_grad\n        assert not net.module_.sequential[3].weight.requires_grad\n        assert net.module_.sequential[0].bias.requires_grad\n        assert not net.module_.sequential[3].bias.requires_grad\n\n        dense0_weight_pre = to_numpy(net.module_.sequential[0].weight).copy()\n        dense1_weight_pre = to_numpy(net.module_.sequential[3].weight).copy()\n        dense0_bias_pre = to_numpy(net.module_.sequential[0].bias).copy()\n        dense1_bias_pre = to_numpy(net.module_.sequential[3].bias).copy()\n\n        # epoch 2, unfreezing parameters\n        net.partial_fit(*classifier_data)\n\n        assert net.module_.sequential[0].weight.requires_grad\n        assert net.module_.sequential[3].weight.requires_grad\n        assert net.module_.sequential[0].bias.requires_grad\n        assert net.module_.sequential[3].bias.requires_grad\n\n        # epoch 3, modifications should have been made\n        net.partial_fit(*classifier_data)\n\n        dense0_weight_post = to_numpy(net.module_.sequential[0].weight).copy()\n        dense1_weight_post = to_numpy(net.module_.sequential[3].weight).copy()\n        dense0_bias_post = to_numpy(net.module_.sequential[0].bias).copy()\n        dense1_bias_post = to_numpy(net.module_.sequential[3].bias).copy()\n\n        assert not np.allclose(dense0_weight_pre, dense0_weight_post)\n        assert not np.allclose(dense1_weight_pre, dense1_weight_post)\n        assert not np.allclose(dense0_bias_pre, dense0_bias_post)\n        assert not np.allclose(dense1_bias_pre, dense1_bias_post)\n\n\n    def test_schedule_is_effective(self, net_cls, classifier_module,\n                                   classifier_data, param_mapper):\n        from skorch.utils import to_numpy, noop\n        from skorch.utils import freeze_parameter, unfreeze_parameter\n\n        def schedule(net):\n            if len(net.history) == 1:\n                return freeze_parameter\n            elif len(net.history) == 2:\n                return unfreeze_parameter\n            return noop\n\n        net = net_cls(\n            classifier_module,\n            max_epochs=1,\n            callbacks=[\n                param_mapper(\n                    [\'sequential.*.weight\', \'sequential.3.bias\'],\n                    schedule=schedule,\n                ),\n            ])\n\n        net.initialize()\n\n        # epoch 1, freezing parameters\n        net.partial_fit(*classifier_data)\n\n        assert not net.module_.sequential[0].weight.requires_grad\n        assert not net.module_.sequential[3].weight.requires_grad\n        assert net.module_.sequential[0].bias.requires_grad\n        assert not net.module_.sequential[3].bias.requires_grad\n\n        dense0_weight_pre = to_numpy(net.module_.sequential[0].weight).copy()\n        dense1_weight_pre = to_numpy(net.module_.sequential[3].weight).copy()\n        dense0_bias_pre = to_numpy(net.module_.sequential[0].bias).copy()\n        dense1_bias_pre = to_numpy(net.module_.sequential[3].bias).copy()\n\n        # epoch 2, unfreezing parameters\n        net.partial_fit(*classifier_data)\n\n        assert net.module_.sequential[0].weight.requires_grad\n        assert net.module_.sequential[3].weight.requires_grad\n        assert net.module_.sequential[0].bias.requires_grad\n        assert net.module_.sequential[3].bias.requires_grad\n\n        # epoch 3, modifications should have been made\n        net.partial_fit(*classifier_data)\n\n        dense0_weight_post = to_numpy(net.module_.sequential[0].weight).copy()\n        dense1_weight_post = to_numpy(net.module_.sequential[3].weight).copy()\n        dense0_bias_post = to_numpy(net.module_.sequential[0].bias).copy()\n        dense1_bias_post = to_numpy(net.module_.sequential[3].bias).copy()\n\n        assert not np.allclose(dense0_weight_pre, dense0_weight_post)\n        assert not np.allclose(dense1_weight_pre, dense1_weight_post)\n        assert not np.allclose(dense0_bias_pre, dense0_bias_post)\n        assert not np.allclose(dense1_bias_pre, dense1_bias_post)\n\n\nclass TestLoadInitState:\n\n    @pytest.fixture\n    def checkpoint_cls(self):\n        from skorch.callbacks import Checkpoint\n        return Checkpoint\n\n    @pytest.fixture\n    def loadinitstate_cls(self):\n        from skorch.callbacks import LoadInitState\n        return LoadInitState\n\n    @pytest.fixture\n    def net_cls(self):\n        """"""very simple network that trains for 10 epochs""""""\n        from skorch import NeuralNetRegressor\n        from skorch.toy import make_regressor\n\n        module_cls = make_regressor(\n            input_units=1,\n            num_hidden=0,\n            output_units=1,\n        )\n\n        return partial(\n            NeuralNetRegressor,\n            module=module_cls,\n            max_epochs=10,\n            batch_size=10)\n\n    @pytest.fixture(scope=\'module\')\n    def data(self):\n        # have 10 examples so we can do a nice CV split\n        X = np.zeros((10, 1), dtype=\'float32\')\n        y = np.zeros((10, 1), dtype=\'float32\')\n        return X, y\n\n    def test_load_initial_state(\n            self, checkpoint_cls, net_cls, loadinitstate_cls,\n            data, tmpdir):\n        skorch_dir = tmpdir.mkdir(\'skorch\')\n        f_params = skorch_dir.join(\'params.pt\')\n        f_optimizer = skorch_dir.join(\'optimizer.pt\')\n        f_history = skorch_dir.join(\'history.json\')\n\n        cp = checkpoint_cls(\n            monitor=None,\n            f_params=str(f_params),\n            f_optimizer=str(f_optimizer),\n            f_history=str(f_history)\n        )\n        load_init_state = loadinitstate_cls(cp)\n        net = net_cls(callbacks=[cp, load_init_state])\n        net.fit(*data)\n\n        assert f_params.exists()\n        assert f_optimizer.exists()\n        assert f_history.exists()\n\n        assert len(net.history) == 10\n        del net\n\n        new_net = net_cls(callbacks=[cp, load_init_state])\n        new_net.fit(*data)\n\n        assert len(new_net.history) == 20\n\n    def test_load_initial_state_custom_scoring(\n            self, checkpoint_cls, net_cls, loadinitstate_cls,\n            data, tmpdir):\n        def epoch_3_scorer(net, *_):\n            return 1 if net.history[-1, \'epoch\'] == 3 else 0\n\n        from skorch.callbacks import EpochScoring\n        scoring = EpochScoring(\n            scoring=epoch_3_scorer, on_train=True, lower_is_better=False)\n\n        skorch_dir = tmpdir.mkdir(\'skorch\')\n        f_params = skorch_dir.join(\n            \'model_epoch_{last_epoch[epoch]}.pt\')\n        f_optimizer = skorch_dir.join(\n            \'optimizer_epoch_{last_epoch[epoch]}.pt\')\n        f_history = skorch_dir.join(\n            \'history.json\')\n\n        cp = checkpoint_cls(\n            monitor=\'epoch_3_scorer_best\',\n            f_params=str(f_params),\n            f_optimizer=str(f_optimizer),\n            f_history=str(f_history)\n        )\n        load_init_state = loadinitstate_cls(cp)\n        net = net_cls(callbacks=[load_init_state, scoring, cp])\n\n        net.fit(*data)\n\n        assert skorch_dir.join(\'model_epoch_3.pt\').exists()\n        assert skorch_dir.join(\'optimizer_epoch_3.pt\').exists()\n        assert skorch_dir.join(\'history.json\').exists()\n\n        assert len(net.history) == 10\n        del net\n\n        new_net = net_cls(callbacks=[load_init_state, scoring, cp])\n        new_net.fit(*data)\n\n        # new_net starts from the best epoch of the first run\n        # the best epcoh of the previous run was at epoch 3\n        # the second run went through 10 epochs, thus\n        # 3 + 10 = 13\n        assert len(new_net.history) == 13\n        assert new_net.history[:, \'event_cp\'] == [\n            True, False, True] + [False] * 10\n\n\nclass TestTrainEndCheckpoint:\n    @pytest.fixture\n    def trainendcheckpoint_cls(self):\n        from skorch.callbacks import TrainEndCheckpoint\n        return TrainEndCheckpoint\n\n    @pytest.fixture\n    def save_params_mock(self):\n        with patch(\'skorch.NeuralNet.save_params\') as mock:\n            yield mock\n\n    @pytest.fixture\n    def pickle_dump_mock(self):\n        with patch(\'pickle.dump\') as mock:\n            yield mock\n\n    @pytest.fixture\n    def net_cls(self):\n        """"""very simple network that trains for 10 epochs""""""\n        from skorch import NeuralNetRegressor\n        from skorch.toy import make_regressor\n\n        module_cls = make_regressor(\n            input_units=1,\n            num_hidden=0,\n            output_units=1,\n        )\n\n        return partial(\n            NeuralNetRegressor,\n            module=module_cls,\n            max_epochs=10,\n            batch_size=10)\n\n    @pytest.fixture(scope=\'module\')\n    def data(self):\n        # have 10 examples so we can do a nice CV split\n        X = np.zeros((10, 1), dtype=\'float32\')\n        y = np.zeros((10, 1), dtype=\'float32\')\n        return X, y\n\n    def test_saves_at_end(\n            self, save_params_mock, net_cls, trainendcheckpoint_cls, data):\n        sink = Mock()\n        net = net_cls(callbacks=[\n            trainendcheckpoint_cls(\n                sink=sink, dirname=\'exp1\', fn_prefix=\'train_end_\')\n        ])\n        net.fit(*data)\n\n        assert save_params_mock.call_count == 3\n        assert sink.call_args == call(""Final checkpoint triggered"")\n        save_params_mock.assert_has_calls([\n            call(f_params=\'exp1/train_end_params.pt\'),\n            call(f_optimizer=\'exp1/train_end_optimizer.pt\'),\n            call(f_history=\'exp1/train_end_history.json\')\n        ])\n\n    def test_saves_at_end_with_custom_formatting(\n            self, save_params_mock, net_cls, trainendcheckpoint_cls, data):\n        sink = Mock()\n        net = net_cls(callbacks=[\n            trainendcheckpoint_cls(\n                sink=sink, dirname=\'exp1\',\n                f_params=\'model_{last_epoch[epoch]}.pt\',\n                f_optimizer=\'optimizer_{last_epoch[epoch]}.pt\',\n                fn_prefix=\'train_end_\'\n            )\n        ])\n        net.fit(*data)\n\n        assert save_params_mock.call_count == 3\n        assert sink.call_args == call(""Final checkpoint triggered"")\n        save_params_mock.assert_has_calls([\n            call(f_params=\'exp1/train_end_model_10.pt\'),\n            call(f_optimizer=\'exp1/train_end_optimizer_10.pt\'),\n            call(f_history=\'exp1/train_end_history.json\')\n        ])\n\n    def test_cloneable(self, trainendcheckpoint_cls):\n        # reproduces bug #459\n        cp = trainendcheckpoint_cls()\n        clone(cp)  # does not raise\n\n    def test_train_end_with_load_init(self, trainendcheckpoint_cls, net_cls, data):\n        # test for https://github.com/skorch-dev/skorch/issues/528\n        # Check that the initial state is indeed loaded from the checkpoint.\n        from skorch.callbacks import LoadInitState\n        from sklearn.metrics import mean_squared_error\n\n        X, y = data\n        cp = trainendcheckpoint_cls()\n        net = net_cls(callbacks=[cp], max_epochs=3, lr=0.1).initialize()\n        score_before = mean_squared_error(y, net.predict(X))\n        net.partial_fit(X, y)\n        score_after = mean_squared_error(y, net.predict(X))\n\n        # make sure the net learned at all\n        assert score_after < score_before\n\n        net_new = net_cls(callbacks=[LoadInitState(cp)], max_epochs=0)\n        net_new.fit(X, y)\n        score_loaded = mean_squared_error(y, net_new.predict(X))\n\n        # the same score as after the end of training of the initial\n        # net should be obtained\n        assert np.isclose(score_loaded, score_after)\n'"
