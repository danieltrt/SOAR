file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\nsetup(\n    name=""torchsummary"",\n    version=""1.5.1"",\n    description=""Model summary in PyTorch similar to `model.summary()` in Keras"",\n    url=""https://github.com/sksq96/pytorch-summary"",\n    author=""Shubham Chandel @sksq96"",\n    author_email=""shubham.zeez@gmail.com"",\n    packages=[""torchsummary""],\n)\n'"
torchsummary/__init__.py,0,"b'from .torchsummary import summary, summary_string\n'"
torchsummary/torchsummary.py,8,"b'import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom collections import OrderedDict\nimport numpy as np\n\n\ndef summary(model, input_size, batch_size=-1, device=torch.device(\'cuda:0\'), dtypes=None):\n    result, params_info = summary_string(\n        model, input_size, batch_size, device, dtypes)\n    print(result)\n\n    return params_info\n\n\ndef summary_string(model, input_size, batch_size=-1, device=torch.device(\'cuda:0\'), dtypes=None):\n    if dtypes == None:\n        dtypes = [torch.FloatTensor]*len(input_size)\n\n    summary_str = \'\'\n\n    def register_hook(module):\n        def hook(module, input, output):\n            class_name = str(module.__class__).split(""."")[-1].split(""\'"")[0]\n            module_idx = len(summary)\n\n            m_key = ""%s-%i"" % (class_name, module_idx + 1)\n            summary[m_key] = OrderedDict()\n            summary[m_key][""input_shape""] = list(input[0].size())\n            summary[m_key][""input_shape""][0] = batch_size\n            if isinstance(output, (list, tuple)):\n                summary[m_key][""output_shape""] = [\n                    [-1] + list(o.size())[1:] for o in output\n                ]\n            else:\n                summary[m_key][""output_shape""] = list(output.size())\n                summary[m_key][""output_shape""][0] = batch_size\n\n            params = 0\n            if hasattr(module, ""weight"") and hasattr(module.weight, ""size""):\n                params += torch.prod(torch.LongTensor(list(module.weight.size())))\n                summary[m_key][""trainable""] = module.weight.requires_grad\n            if hasattr(module, ""bias"") and hasattr(module.bias, ""size""):\n                params += torch.prod(torch.LongTensor(list(module.bias.size())))\n            summary[m_key][""nb_params""] = params\n\n        if (\n            not isinstance(module, nn.Sequential)\n            and not isinstance(module, nn.ModuleList)\n        ):\n            hooks.append(module.register_forward_hook(hook))\n\n    # multiple inputs to the network\n    if isinstance(input_size, tuple):\n        input_size = [input_size]\n\n    # batch_size of 2 for batchnorm\n    x = [torch.rand(2, *in_size).type(dtype).to(device=device)\n         for in_size, dtype in zip(input_size, dtypes)]\n\n    # create properties\n    summary = OrderedDict()\n    hooks = []\n\n    # register hook\n    model.apply(register_hook)\n\n    # make a forward pass\n    # print(x.shape)\n    model(*x)\n\n    # remove these hooks\n    for h in hooks:\n        h.remove()\n\n    summary_str += ""----------------------------------------------------------------"" + ""\\n""\n    line_new = ""{:>20}  {:>25} {:>15}"".format(\n        ""Layer (type)"", ""Output Shape"", ""Param #"")\n    summary_str += line_new + ""\\n""\n    summary_str += ""================================================================"" + ""\\n""\n    total_params = 0\n    total_output = 0\n    trainable_params = 0\n    for layer in summary:\n        # input_shape, output_shape, trainable, nb_params\n        line_new = ""{:>20}  {:>25} {:>15}"".format(\n            layer,\n            str(summary[layer][""output_shape""]),\n            ""{0:,}"".format(summary[layer][""nb_params""]),\n        )\n        total_params += summary[layer][""nb_params""]\n\n        total_output += np.prod(summary[layer][""output_shape""])\n        if ""trainable"" in summary[layer]:\n            if summary[layer][""trainable""] == True:\n                trainable_params += summary[layer][""nb_params""]\n        summary_str += line_new + ""\\n""\n\n    # assume 4 bytes/number (float on cuda).\n    total_input_size = abs(np.prod(sum(input_size, ()))\n                           * batch_size * 4. / (1024 ** 2.))\n    total_output_size = abs(2. * total_output * 4. /\n                            (1024 ** 2.))  # x2 for gradients\n    total_params_size = abs(total_params * 4. / (1024 ** 2.))\n    total_size = total_params_size + total_output_size + total_input_size\n\n    summary_str += ""================================================================"" + ""\\n""\n    summary_str += ""Total params: {0:,}"".format(total_params) + ""\\n""\n    summary_str += ""Trainable params: {0:,}"".format(trainable_params) + ""\\n""\n    summary_str += ""Non-trainable params: {0:,}"".format(total_params -\n                                                        trainable_params) + ""\\n""\n    summary_str += ""----------------------------------------------------------------"" + ""\\n""\n    summary_str += ""Input size (MB): %0.2f"" % total_input_size + ""\\n""\n    summary_str += ""Forward/backward pass size (MB): %0.2f"" % total_output_size + ""\\n""\n    summary_str += ""Params size (MB): %0.2f"" % total_params_size + ""\\n""\n    summary_str += ""Estimated Total Size (MB): %0.2f"" % total_size + ""\\n""\n    summary_str += ""----------------------------------------------------------------"" + ""\\n""\n    # return summary\n    return summary_str, (total_params, trainable_params)\n'"
torchsummary/tests/test_models/test_model.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass SingleInputNet(nn.Module):\n    def __init__(self):\n        super(SingleInputNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d(0.3)\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\nclass MultipleInputNet(nn.Module):\n    def __init__(self):\n        super(MultipleInputNet, self).__init__()\n        self.fc1a = nn.Linear(300, 50)\n        self.fc1b = nn.Linear(50, 10)\n\n        self.fc2a = nn.Linear(300, 50)\n        self.fc2b = nn.Linear(50, 10)\n\n    def forward(self, x1, x2):\n        x1 = F.relu(self.fc1a(x1))\n        x1 = self.fc1b(x1)\n        x2 = F.relu(self.fc2a(x2))\n        x2 = self.fc2b(x2)\n        x = torch.cat((x1, x2), 0)\n        return F.log_softmax(x, dim=1)\n\nclass MultipleInputNetDifferentDtypes(nn.Module):\n    def __init__(self):\n        super(MultipleInputNetDifferentDtypes, self).__init__()\n        self.fc1a = nn.Linear(300, 50)\n        self.fc1b = nn.Linear(50, 10)\n\n        self.fc2a = nn.Linear(300, 50)\n        self.fc2b = nn.Linear(50, 10)\n\n    def forward(self, x1, x2):\n        x1 = F.relu(self.fc1a(x1))\n        x1 = self.fc1b(x1)\n        x2 = x2.type(torch.FloatTensor)\n        x2 = F.relu(self.fc2a(x2))\n        x2 = self.fc2b(x2)\n        # set x2 to FloatTensor\n        x = torch.cat((x1, x2), 0)\n        return F.log_softmax(x, dim=1)\n'"
torchsummary/tests/unit_tests/torchsummary_test.py,5,"b'import unittest\nfrom torchsummary import summary, summary_string\nfrom torchsummary.tests.test_models.test_model import SingleInputNet, MultipleInputNet, MultipleInputNetDifferentDtypes\nimport torch\n\ngpu_if_available = ""cuda:0"" if torch.cuda.is_available() else ""cpu""\n\nclass torchsummaryTests(unittest.TestCase):\n    def test_single_input(self):\n        model = SingleInputNet()\n        input = (1, 28, 28)\n        total_params, trainable_params = summary(model, input, device=""cpu"")\n        self.assertEqual(total_params, 21840)\n        self.assertEqual(trainable_params, 21840)\n\n    def test_multiple_input(self):\n        model = MultipleInputNet()\n        input1 = (1, 300)\n        input2 = (1, 300)\n        total_params, trainable_params = summary(\n            model, [input1, input2], device=""cpu"")\n        self.assertEqual(total_params, 31120)\n        self.assertEqual(trainable_params, 31120)\n\n    def test_single_layer_network(self):\n        model = torch.nn.Linear(2, 5)\n        input = (1, 2)\n        total_params, trainable_params = summary(model, input, device=""cpu"")\n        self.assertEqual(total_params, 15)\n        self.assertEqual(trainable_params, 15)\n\n    def test_single_layer_network_on_gpu(self):\n        model = torch.nn.Linear(2, 5)\n        if torch.cuda.is_available():\n            model.cuda()\n        input = (1, 2)\n        total_params, trainable_params = summary(model, input, device=gpu_if_available)\n        self.assertEqual(total_params, 15)\n        self.assertEqual(trainable_params, 15)\n\n    def test_multiple_input_types(self):\n        model = MultipleInputNetDifferentDtypes()\n        input1 = (1, 300)\n        input2 = (1, 300)\n        dtypes = [torch.FloatTensor, torch.LongTensor]\n        total_params, trainable_params = summary(\n            model, [input1, input2], device=""cpu"", dtypes=dtypes)\n        self.assertEqual(total_params, 31120)\n        self.assertEqual(trainable_params, 31120)\n\n\nclass torchsummarystringTests(unittest.TestCase):\n    def test_single_input(self):\n        model = SingleInputNet()\n        input = (1, 28, 28)\n        result, (total_params, trainable_params) = summary_string(\n            model, input, device=""cpu"")\n        self.assertEqual(type(result), str)\n        self.assertEqual(total_params, 21840)\n        self.assertEqual(trainable_params, 21840)\n\n\nif __name__ == \'__main__\':\n    unittest.main(buffer=True)\n'"
