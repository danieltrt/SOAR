file_path,api_count,code
setup.py,0,"b'from distutils.core import setup\nimport setuptools\n\nwith open(\'./README.md\', \'r\', encoding=\'utf8\') as f:\n    long_description = f.read()\n\nwith open(\'./requirements.txt\', \'r\', encoding=\'utf8\') as f:\n    install_requires = list(map(lambda x: x.strip(), f.readlines()))\n\nsetup(\n    name=\'lightNLP\',\n    version=\'0.4.1\',\n    description=""lightsmile\'s nlp library"",\n    author=\'lightsmile\',\n    author_email=\'iamlightsmile@gmail.com\',\n    url=\'https://github.com/smilelight/lightNLP\',\n    packages=setuptools.find_packages(),\n    install_requires=install_requires,\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    license=\'Apache-2.0\',\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'Operating System :: OS Independent\',\n        \'Intended Audience :: Developers\',\n        \'License :: OSI Approved :: BSD License\',\n        \'Programming Language :: Python\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Topic :: Software Development :: Libraries\'\n    ],\n)'"
examples/test_cb.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.tg import CB\n\ncb_model = CB()\n\ntrain_path = '../data/cb/chat.train.sample.tsv'\ndev_path = '../data/cb/chat.test.sample.tsv'\nvec_path = 'D:/Data/NLP/embedding/word/sgns.zhihu.bigram-char'\n\n# cb_model.train(train_path, vectors_path=vec_path, dev_path=train_path, save_path='./cb_saves',\n#                log_dir='E:/Test/tensorboard/')\n\ncb_model.load('./cb_saves')\n\n# cb_model.test(train_path)\n\n# print(cb_model.predict('\xe6\x88\x91\xe8\xbf\x98\xe5\x96\x9c\xe6\xac\xa2\xe5\xa5\xb9,\xe6\x80\x8e\xe4\xb9\x88\xe5\x8a\x9e'))\n# print(cb_model.predict('\xe6\x80\x8e\xe4\xb9\x88\xe4\xba\x86'))\n# print(cb_model.predict('\xe5\xbc\x80\xe5\xbf\x83\xe4\xb8\x80\xe7\x82\xb9'))\n\ncb_model.deploy()\n"""
examples/test_cbow.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.we import CBOWHierarchicalSoftmaxModule, CBOWNegativeSamplingModule, CBOWBaseModule\n\n# cbow_model = CBOWHierarchicalSoftmaxModule()\ncbow_model = CBOWBaseModule()\n# cbow_model = CBOWNegativeSamplingModule()\n\ntrain_path = '../data/novel/test.txt'\ndev_path = '../data/novel/test.txt'\n\n# cbow_model.train(train_path, dev_path=dev_path, save_path='./cbow_saves', log_dir='E:/Test/tensorboard/')\n\ncbow_model.load('./cbow_saves')\ncbow_model.deploy()\n\n\n#\n# cbow_model.test(dev_path)\n\n# test_context = ['\xe6\x97\x8f\xe9\x95\xbf', '\xe6\x98\xaf', '\xe7\x9a\x84', '\xe7\x88\xb6\xe4\xba\xb2']\n# print(cbow_model.predict(test_context))\n# print(cbow_model.evaluate(test_context, '\xe4\xbb\x96'))\n# print(cbow_model.evaluate(test_context, '\xe6\x8f\x90\xe9\x98\xb2'))\n\n# cbow_model.save_embeddings('./cbow_saves/cbow_hs.bin')\n# cbow_model.save_embeddings('./cbow_saves/cbow_base.bin')\n# cbow_model.save_embeddings('./cbow_saves/cbow_ns.bin')\n"""
examples/test_cws.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.sl import CWS\n\ncws_model = CWS()\n\ntrain_path = '../data/cws/train.sample.txt'\ndev_path = '../data/corpus/cws/test.sample.txt'\nvec_path = 'D:/Data/NLP/embedding/char/token_vec_300.bin'\n\ncws_model.train(train_path, vectors_path=vec_path, dev_path=dev_path, save_path='./cws_saves',\n                log_dir='E:/Test/tensorboard/')\n\ncws_model.load('./cws_saves')\n\n\n# cws_model.test(dev_path)\n\nprint(cws_model.predict('\xe6\x8a\x97\xe6\x97\xa5\xe6\x88\x98\xe4\xba\x89\xe6\x97\xb6\xe6\x9c\x9f\xef\xbc\x8c\xe8\x83\xa1\xe8\x80\x81\xe5\x9c\xa8\xe4\xb8\x8e\xe4\xbe\xb5\xe5\x8d\x8e\xe6\x97\xa5\xe5\x86\x9b\xe4\xba\xa4\xe6\x88\x98\xe4\xb8\xad\xe5\x9b\x9b\xe6\xac\xa1\xe8\xb4\x9f\xe4\xbc\xa4\xef\xbc\x8c\xe6\x98\xaf\xe4\xb8\x80\xe4\xbd\x8d\xe4\xb8\x8d\xe6\x8a\x98\xe4\xb8\x8d\xe6\x89\xa3\xe7\x9a\x84\xe6\x8a\x97\xe6\x88\x98\xe8\x80\x81\xe8\x8b\xb1\xe9\x9b\x84'))"""
examples/test_gdp.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.sp import GDP\n\ngdp_model = GDP()\n\ntrain_path = '../data/gdp/train.sample.conll'\nvec_path = 'D:/Data/NLP/embedding/word/sgns.zhihu.bigram-char'\n\n\ngdp_model.train(train_path, dev_path=train_path, vectors_path=vec_path, save_path='./gdp_saves',\n                log_dir='E:/Test/tensorboard/')\n\ngdp_model.load('./gdp_saves')\n# gdp_model.test(train_path)\nword_list = ['\xe6\x9c\x80\xe9\xab\x98', '\xe4\xba\xba\xe6\xb0\x91', '\xe6\xa3\x80\xe5\xaf\x9f\xe9\x99\xa2', '\xe6\xa3\x80\xe5\xaf\x9f\xe9\x95\xbf', '\xe5\xbc\xa0\xe6\x80\x9d\xe5\x8d\xbf']\npos_list = ['nt', 'nt', 'nt', 'n', 'nr']\nheads, rels = gdp_model.predict(word_list, pos_list)\nprint(heads)\nprint(rels)"""
examples/test_lm.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.tg import LM\n\nlm_model = LM()\n\ntrain_path = '../data/lm/lm_test.txt'\ndev_path = '../data/lm/lm_test.txt'\nvec_path = 'D:/Data/NLP/embedding/char/token_vec_300.bin'\n\n# lm_model.train(train_path, vectors_path=vec_path, dev_path=train_path, save_path='./lm_saves',\n#                log_dir='E:/Test/tensorboard/')\n\nlm_model.load('./lm_saves')\nlm_model.deploy()\n# lm_model.test(train_path)\n\n# print(lm_model.next_word('\xe8\xa6\x81\xe4\xb8\x8d\xe6\x98\xaf', '\xe8\x90\xa7'))\n#\n# print(lm_model.generate_sentence('\xe5\xb0\x91\xe5\xb9\xb4\xe9\x9d\xa2\xe6\x97\xa0\xe8\xa1\xa8\xe6\x83\x85\xef\xbc\x8c\xe5\x94\x87\xe8\xa7\x92\xe6\x9c\x89\xe7\x9d\x80\xe4\xb8\x80\xe6\x8a\xb9\xe8\x87\xaa\xe5\x98\xb2'))\n#\n# print(lm_model.next_word_topk('\xe5\xb0\x91\xe5\xb9\xb4\xe9\x9d\xa2\xe6\x97\xa0\xe8\xa1\xa8\xe6\x83\x85\xef\xbc\x8c\xe5\x94\x87\xe8\xa7\x92'))\n#\n# print(lm_model.sentence_score('\xe5\xb0\x91\xe5\xb9\xb4\xe9\x9d\xa2\xe6\x97\xa0\xe8\xa1\xa8\xe6\x83\x85\xef\xbc\x8c\xe5\x94\x87\xe8\xa7\x92\xe6\x9c\x89\xe7\x9d\x80\xe4\xb8\x80\xe6\x8a\xb9\xe8\x87\xaa\xe5\x98\xb2'))\n"""
examples/test_mt.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.tg import MT\n\nmt_model = MT()\n\ntrain_path = '../data/mt/mt.train.sample.tsv'\ndev_path = '../data/mt/mt.test.sample.tsv'\nsource_vec_path = 'D:/Data/NLP/embedding/english/glove.6B.100d.txt'\ntarget_vec_path = 'D:/Data/NLP/embedding/word/sgns.zhihu.bigram-char'\n\nmt_model.train(train_path, source_vectors_path=source_vec_path, target_vectors_path=target_vec_path,\n               dev_path=train_path, save_path='./mt_saves', log_dir='E:/Test/tensorboard/')\n\nmt_model.load('./mt_saves')\n\nmt_model.test(train_path)\n\nprint(mt_model.predict('Hello!'))\nprint(mt_model.predict('Wait!'))\n"""
examples/test_ner.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.sl import NER\n\nner_model = NER()\n\ntrain_path = '../data/ner/train.sample.txt'\ndev_path = '../data/ner/test.sample.txt'\nvec_path = 'D:/Data/NLP/embedding/char/token_vec_300.bin'\n\n# ner_model.train(train_path, vectors_path=vec_path, dev_path=dev_path, save_path='./ner_saves',\n#                 log_dir='E:/Test/tensorboard/')\n\nner_model.load('./ner_saves')\nner_model.deploy()\n\n# from pprint import pprint\n#\n# ner_model.test(train_path)\n\n# pprint(ner_model.predict('\xe5\x8f\xa6\xe4\xb8\x80\xe4\xb8\xaa\xe5\xbe\x88\xe9\x85\xb7\xe7\x9a\x84\xe4\xba\x8b\xe6\x83\x85\xe6\x98\xaf\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87\xe6\xa1\x86\xe6\x9e\xb6\xe6\x88\x91\xe4\xbb\xac\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x81\x9c\xe6\xad\xa2\xe5\xb9\xb6\xe5\x9c\xa8\xe7\xa8\x8d\xe5\x90\x8e\xe6\x81\xa2\xe5\xa4\x8d\xe8\xae\xad\xe7\xbb\x83\xe3\x80\x82'))\n"""
examples/test_pos.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.sl import POS\n\npos_model = POS()\n\ntrain_path = '../data/pos/train.sample.txt'\ndev_path = '../data/pos/test.sample.txt'\nvec_path = 'D:/Data/NLP/embedding/char/token_vec_300.bin'\n\npos_model.train(train_path, vectors_path=vec_path, dev_path=dev_path, save_path='./pos_saves',\n                log_dir='E:/Test/tensorboard/')\n\npos_model.load('./pos_saves')\n\n# pos_model.test(dev_path)\n\nprint(pos_model.predict('\xe5\x90\x91\xe5\x85\xa8\xe5\x9b\xbd\xe5\x90\x84\xe6\x97\x8f\xe4\xba\xba\xe6\xb0\x91\xe8\x87\xb4\xe4\xbb\xa5\xe8\xaf\x9a\xe6\x8c\x9a\xe7\x9a\x84\xe9\x97\xae\xe5\x80\x99\xef\xbc\x81'))"""
examples/test_re.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.tc.re.tool import re_tool\n\ntrain_path = '../data/re/train.sample.txt'\ndev_path = '../data/re/test.sample.txt'\nvec_path = 'D:/Data/NLP/embedding/word/sgns.zhihu.bigram-char'\n\n# data_set = re_tool.get_dataset(train_path)\n#\n# print(len(data_set))\n#\n# item = data_set[0]\n# print(item.text)\n# print(item.label)\n\nfrom lightnlp.tc import RE\n\nre = RE()\n\nre.train(train_path, dev_path=train_path, vectors_path=vec_path, save_path='./re_saves',\n         log_dir='E:/Test/tensorboard/')\n\nre.load('./re_saves')\n# re.test(dev_path)\n# # # \xe9\x92\xb1\xe9\x92\x9f\xe4\xb9\xa6\t\xe8\xbe\x9b\xe7\xac\x9b\t\xe5\x90\x8c\xe9\x97\xa8\t\xe4\xb8\x8e\xe8\xbe\x9b\xe7\xac\x9b\xe4\xba\xac\xe6\xb2\xaa\xe5\x94\xb1\xe5\x92\x8c\xe8\x81\xbd\xe9\x92\xb1\xe9\x92\x9f\xe4\xb9\xa6\xe4\xb8\x8e\xe9\x92\xb1\xe9\x92\x9f\xe4\xb9\xa6\xe6\x98\xaf\xe6\xb8\x85\xe5\x8d\x8e\xe6\xa0\xa1\xe5\x8f\x8b\xef\xbc\x8c\xe9\x92\xb1\xe9\x92\x9f\xe4\xb9\xa6\xe9\xab\x98\xe8\xbe\x9b\xe7\xac\x9b\xe4\xb8\xa4\xe7\x8f\xad\xe3\x80\x82\nprint(re.predict('\xe9\x92\xb1\xe9\x92\x9f\xe4\xb9\xa6', '\xe8\xbe\x9b\xe7\xac\x9b', '\xe4\xb8\x8e\xe8\xbe\x9b\xe7\xac\x9b\xe4\xba\xac\xe6\xb2\xaa\xe5\x94\xb1\xe5\x92\x8c\xe8\x81\xbd\xe9\x92\xb1\xe9\x92\x9f\xe4\xb9\xa6\xe4\xb8\x8e\xe9\x92\xb1\xe9\x92\x9f\xe4\xb9\xa6\xe6\x98\xaf\xe6\xb8\x85\xe5\x8d\x8e\xe6\xa0\xa1\xe5\x8f\x8b\xef\xbc\x8c\xe9\x92\xb1\xe9\x92\x9f\xe4\xb9\xa6\xe9\xab\x98\xe8\xbe\x9b\xe7\xac\x9b\xe4\xb8\xa4\xe7\x8f\xad\xe3\x80\x82'))\n"""
examples/test_sa.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.tc import SA\n\nsa_model = SA()\n\ntrain_path = '../data/sa/train.sample.tsv'\ndev_path = '../data/sa/dev.sample.tsv'\nvec_path = 'D:/Data/NLP/embedding/word/sgns.zhihu.bigram-char'\n\n# sa_model.train(train_path, vectors_path=vec_path, dev_path=dev_path, save_path='./sa_saves',\n#                log_dir='E:/Test/tensorboard/')\n\nsa_model.load('./sa_saves')\n\nsa_model.deploy(debug=True)\n\n# from pprint import pprint\n#\n# sa_model.test(train_path)\n#\n# pprint(sa_model.predict('\xe5\xa4\x96\xe8\xa7\x82\xe6\xbc\x82\xe4\xba\xae\xef\xbc\x8c\xe5\xae\x89\xe5\x85\xa8\xe6\x80\xa7\xe4\xbd\xb3\xef\xbc\x8c\xe5\x8a\xa8\xe5\x8a\x9b\xe5\xa4\x9f\xe5\xbc\xba\xef\xbc\x8c\xe6\xb2\xb9\xe8\x80\x97\xe5\xa4\x9f\xe4\xbd\x8e\xe3\x80\x82'))"""
examples/test_skip_gram.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.we import SkipGramBaseModule, SkipGramNegativeSamplingModule, SkipGramHierarchicalSoftmaxModule\n\n# skip_gram_model = SkipGramHierarchicalSoftmaxModule()\n# skip_gram_model = SkipGramNegativeSamplingModule()\nskip_gram_model = SkipGramBaseModule()\n\ntrain_path = '../data/novel/test.txt'\ndev_path = '../data/novel/test.txt'\n\n# skip_gram_model.train(train_path, dev_path=dev_path, save_path='./skip_gram_saves', log_dir='E:/Test/tensorboard/')\n\nskip_gram_model.load('./skip_gram_saves')\nskip_gram_model.deploy()\n\n# skip_gram_model.test(dev_path)\n\ntest_target = '\xe6\x97\x8f\xe9\x95\xbf'\nprint(skip_gram_model.predict(test_target))\nprint(skip_gram_model.evaluate(test_target, '\xe4\xbb\x96'))\nprint(skip_gram_model.evaluate(test_target, '\xe6\x8f\x90\xe9\x98\xb2'))\n\n# skip_gram_model.save_embeddings('./skip_gram_saves/skip_gram_hs.bin')\n# skip_gram_model.save_embeddings('./skip_gram_saves/skip_gram_base.bin')\n# skip_gram_model.save_embeddings('./skip_gram_saves/skip_gram_ns.bin')\n"""
examples/test_srl.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.sl import SRL\n\nsrl_model = SRL()\n\ntrain_path = '../data/srl/train.sample.tsv'\ndev_path = '../data/srl/test.sample.tsv'\nvec_path = 'D:/Data/NLP/embedding/word/sgns.zhihu.bigram-char'\n\n\nsrl_model.train(train_path, vectors_path=vec_path, dev_path=dev_path, save_path='./srl_saves',\n                log_dir='E:/Test/tensorboard/')\n\n# srl_model.load('./srl_saves')\n\n# srl_model.test(dev_path)\n\nword_list = ['\xe4\xbb\xa3\xe8\xa1\xa8', '\xe6\x9c\x9d\xe6\x96\xb9', '\xe5\xaf\xb9', '\xe4\xb8\xad\xe5\x9b\xbd', '\xe5\x85\x9a\xe6\x94\xbf', '\xe9\xa2\x86\xe5\xaf\xbc\xe4\xba\xba', '\xe5\x92\x8c', '\xe4\xba\xba\xe6\xb0\x91', '\xe5\x93\x80\xe6\x82\xbc', '\xe9\x87\x91\xe6\x97\xa5\xe6\x88\x90', '\xe4\xb8\xbb\xe5\xb8\xad', '\xe9\x80\x9d\xe4\xb8\x96', '\xe8\xa1\xa8\xe7\xa4\xba', '\xe6\xb7\xb1\xe5\x88\x87', '\xe8\xb0\xa2\xe6\x84\x8f', '\xe3\x80\x82']\npos_list = ['VV', 'NN', 'P', 'NR', 'NN', 'NN', 'CC', 'NN', 'VV', 'NR', 'NN', 'VV', 'VV', 'JJ', 'NN', 'PU']\nrel_list = [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n\n# print(srl_model.predict(word_list, pos_list, rel_list))\n\n"""
examples/test_ss.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.sr import SS\n\nss_model = SS()\n\ntrain_path = '../data/ss/ss_train.tsv'\ndev_path = '../data/ss/ss_dev.tsv'\nvec_path = 'D:/Data/NLP/embedding/char/token_vec_300.bin'\n\nss_model.train(train_path, vectors_path=vec_path, dev_path=train_path, save_path='./ss_saves',\n               log_dir='E:/Test/tensorboard/')\n\n\n# ss_model.load('./ss_saves')\n# ss_model.test(dev_path)\n\nprint(float(ss_model.predict('\xe8\x8a\xb1\xe5\x91\x97\xe6\x94\xb6\xe6\xac\xbe\xe6\x94\xb6\xe4\xb8\x8d\xe4\xba\x86\xe6\x80\x8e\xe4\xb9\x88\xe5\x8a\x9e', '\xe5\xbc\x80\xe9\x80\x9a\xe8\x8a\xb1\xe5\x91\x97\xe6\x94\xb6\xe6\xac\xbe\xe4\xb8\xba\xe4\xbd\x95\xe4\xb8\x8d\xe8\x83\xbd\xe7\x94\xa8')))\nprint(float(ss_model.predict('\xe8\x8a\xb1\xe5\x91\x97\xe7\x9a\x84\xe5\xae\x89\xe5\x85\xa8\xe6\xb2\xa1\xe6\x9c\x89\xe9\xaa\x8c\xe8\xaf\x81\xe6\x88\x90\xe5\x8a\x9f', '\xe8\x8a\xb1\xe5\x91\x97\xe5\xae\x89\xe5\x85\xa8\xe9\xaa\x8c\xe8\xaf\x81\xe6\xb2\xa1\xe9\x80\x9a\xe8\xbf\x87\xe6\x80\x8e\xe4\xb9\x88\xe5\x9b\x9e\xe4\xba\x8b')))\nprint(float(ss_model.predict('\xe8\x8a\xb1\xe5\x91\x97\xe6\x94\xaf\xe4\xbb\x98\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xbd\xbf\xe7\x94\xa8\xe8\xb4\xad\xe7\x89\xa9\xe6\xb4\xa5\xe8\xb4\xb4\xe5\x90\x97', '\xe4\xbd\xbf\xe7\x94\xa8\xe8\xb4\xad\xe7\x89\xa9\xe6\xb4\xa5\xe8\xb4\xb4\xe7\x9a\x84\xe8\xb4\xb9\xe7\x94\xa8\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x94\xa8\xe8\x8a\xb1\xe5\x91\x97\xe5\x90\x97')))\nprint(float(ss_model.predict('\xe8\x8a\xb1\xe5\x91\x97\xe6\x9b\xb4\xe6\x94\xb9\xe7\xbb\x91\xe5\xae\x9a\xe9\x93\xb6\xe8\xa1\x8c\xe5\x8d\xa1', '\xe5\xa6\x82\xe4\xbd\x95\xe6\x9b\xb4\xe6\x8d\xa2\xe8\x8a\xb1\xe5\x91\x97\xe7\xbb\x91\xe5\xae\x9a\xe9\x93\xb6\xe8\xa1\x8c\xe5\x8d\xa1')))"""
examples/test_tdp.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.sp import TDP\n\ntdp_model = TDP()\n\ntrain_path = '../data/tdp/train.sample.txt'\ndev_path = '../data/tdp/dev.txt'\nvec_path = 'D:/Data/NLP/embedding/english/glove.6B.100d.txt'\n\ntdp_model.train(train_path, dev_path=dev_path, vectors_path=vec_path,save_path='./tdp_saves',\n                log_dir='E:/Test/tensorboard/')\n\ntdp_model.load('./tdp_saves')\ntdp_model.test(dev_path)\nfrom pprint import pprint\npprint(tdp_model.predict('Investors who want to change the required timing should write their representatives '\n                         'in Congress , he added . '))\n\n\n"""
examples/test_te.py,0,"b""import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.sr import TE\n\nte_model = TE()\n\ntrain_path = '../data/te/te_train.tsv'\ndev_path = '../data/te_dev.tsv'\nvec_path = 'D:/Data/NLP/embedding/char/token_vec_300.bin'\n\nte_model.train(train_path, vectors_path=vec_path, dev_path=train_path, save_path='./te_saves',\n               log_dir='E:/Test/tensorboard/')\n\nte_model.load('./te_saves')\n# te_model.test(dev_path)\n\nprint(te_model.predict('\xe4\xb8\x80\xe4\xb8\xaa\xe5\xb0\x8f\xe7\x94\xb7\xe5\xad\xa9\xe5\x9c\xa8\xe7\xa7\x8b\xe5\x8d\x83\xe4\xb8\x8a\xe7\x8e\xa9\xe3\x80\x82', '\xe5\xb0\x8f\xe7\x94\xb7\xe5\xad\xa9\xe7\x8e\xa9\xe7\xa7\x8b\xe5\x8d\x83'))\nprint(te_model.predict('\xe4\xb8\xa4\xe4\xb8\xaa\xe5\xb9\xb4\xe8\xbd\xbb\xe4\xba\xba\xe7\x94\xa8\xe6\xb3\xa1\xe6\xb2\xab\xe5\xa1\x91\xe6\x96\x99\xe6\x9d\xaf\xe5\xad\x90\xe5\x96\x9d\xe9\x85\x92\xe6\x97\xb6\xe5\x81\x9a\xe9\xac\xbc\xe8\x84\xb8\xe3\x80\x82', '\xe4\xb8\xa4\xe4\xb8\xaa\xe4\xba\xba\xe5\x9c\xa8\xe8\xb7\xb3\xe5\x8d\x83\xe6\x96\xa4\xe9\xa1\xb6\xe3\x80\x82'))"""
examples/test_ts.py,0,"b'import os\nimport sys\nsys.path.append(os.path.split(os.path.realpath(__file__))[0])\n\nfrom lightnlp.tg import TS\n\nts_model = TS()\n\ntrain_path = \'../data/ts/train.sample.tsv\'\ndev_path = \'../data/ts/test.sample.tsv\'\nvec_path = \'D:/Data/NLP/embedding/word/sgns.zhihu.bigram-char\'\n\nts_model.train(train_path, vectors_path=vec_path, dev_path=train_path, save_path=\'./ts_saves\',\n               log_dir=\'E:/Test/tensorboard/\')\n\n# ts_model.load(\'./ts_saves\')\n#\n# ts_model.test(train_path)\n\ntest_str = """"""\n            \xe8\xbf\x91\xe6\x97\xa5\xef\xbc\x8c\xe5\x9b\xa0\xe5\xa4\xa9\xe6\xb0\x94\xe5\xa4\xaa\xe7\x83\xad\xef\xbc\x8c\xe5\xae\x89\xe5\xbe\xbd\xe4\xb8\x80\xe8\x80\x81\xe5\xa4\xaa\xe5\x9c\xa8\xe4\xb9\xb0\xe8\x82\x89\xe8\xb7\xaf\xe4\xb8\x8a\xe7\xaa\x81\xe7\x84\xb6\xe7\x9c\xbc\xe5\x89\x8d\xe4\xb8\x80\xe9\xbb\x91\xef\xbc\x8c\xe6\x91\x94\xe5\x80\x92\xe5\x9c\xa8\xe5\x9c\xb0\xe3\x80\x82\xe5\xa5\xb9\xe6\x80\x95\xe5\x88\xab\xe4\xba\xba\xe4\xb8\x8d\xe6\x89\xb6\xe5\xa5\xb9\xef\xbc\x8c\xe8\xbf\x9e\xe5\xbf\x99\xe8\xaf\xb4""\xe5\xbf\xab\xe6\x89\xb6\xe6\x88\x91\xe8\xb5\xb7\xe6\x9d\xa5\xef\xbc\x8c\xe6\x88\x91\xe4\xb8\x8d\xe8\xae\xb9\xe4\xbd\xa0\xef\xbc\x8c\xe5\x9c\xb0\xe4\xb8\x8a\xe5\xa4\xaa\xe7\x83\xad\xe6\x88\x91\xe8\xa6\x81\xe7\x86\x9f\xe4\xba\x86\xef\xbc\x81""\xe8\xbf\x99\xe4\xb8\x80\xe5\x96\x8a\xe5\x91\xa8\xe5\x9b\xb4\xe4\xba\xba\xe9\x83\xbd\xe7\xac\x91\xe4\xba\x86\xef\xbc\x8c\xe8\x80\x81\xe4\xba\xba\xe9\x9a\x8f\xe5\x90\x8e\xe8\xa2\xab\xe6\x89\xb6\xe5\x88\xb0\xe8\xb7\xaf\xe8\xbe\xb9\xe4\xbc\x91\xe6\x81\xaf\xe3\x80\x82(\xe9\xa2\x8d\xe5\xb7\x9e\xe6\x99\x9a\xe6\x8a\xa5)[\xe8\xaf\x9d\xe7\xad\x92]\xe6\x9c\x80\xe8\xbf\x91\xe8\x80\x81\xe4\xba\xba\xe5\xb0\xbd\xe9\x87\x8f\xe9\x81\xbf\xe5\x85\x8d\xe5\x87\xba\xe9\x97\xa8!\n            """"""\n\nprint(ts_model.predict(test_str))\n'"
examples/test_word_vectors.py,0,"b""from lightnlp.utils.word_vector import WordVectors\n\nvector_path = 'E:/Projects/myProjects/lightNLP/examples/cbow_saves/cbow_base.bin'\nword_vectors = WordVectors(vector_path)\nprint(word_vectors.get_similar_words('\xe5\xb0\x91\xe5\xa5\xb3', dis_type='cos'))\n"""
lightnlp/__init__.py,0,b''
test/test_flask.py,0,b''
lightnlp/base/__init__.py,0,b''
lightnlp/base/config.py,1,"b""import torch\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nDEFAULT_CONFIG = {\n    'save_path': './saves'\n}"""
lightnlp/base/model.py,4,"b""import os\nimport pickle\n\nimport torch\nimport torch.nn as nn\n\nfrom .config import DEFAULT_CONFIG\nfrom ..utils.log import logger\n\n\nclass BaseConfig(object):\n    def __init__(self):\n        pass\n    \n    @staticmethod\n    def load(path=DEFAULT_CONFIG['save_path']):\n        config_path = os.path.join(path, 'config.pkl')\n        with open(config_path, 'rb') as f:\n            config = pickle.load(f)\n        logger.info('loadding config from {}'.format(config_path))\n        config.save_path = path\n        return config\n    \n    def save(self, path=None):\n        if not hasattr(self, 'save_path'):\n            raise AttributeError('config object must init save_path attr in init method!')\n        path = path if path else self.save_path\n        if not os.path.isdir(path):\n            os.mkdir(path)\n        config_path = os.path.join(path, 'config.pkl')\n        with open(os.path.join(path, 'config.pkl'), 'wb') as f:\n            pickle.dump(self, f)\n        logger.info('saved config to {}'.format(config_path))\n\n\nclass BaseModel(nn.Module):\n    def __init__(self, args):\n        super(BaseModel, self).__init__()\n        self.args = args\n        self.save_path = args.save_path\n    \n    def load(self, path=None):\n        path = path if path else self.save_path\n        map_location = None if torch.cuda.is_available() else 'cpu'\n        model_path = os.path.join(path, 'model.pkl')\n        self.load_state_dict(torch.load(model_path, map_location=map_location))\n        logger.info('loadding model from {}'.format(model_path))\n    \n    def save(self, path=None):\n        path = path if path else self.save_path\n        if not os.path.isdir(path):\n            os.mkdir(path)\n        model_path = os.path.join(path, 'model.pkl')\n        torch.save(self.state_dict(), model_path)\n        logger.info('saved model to {}'.format(model_path))\n"""
lightnlp/base/module.py,0,"b'from abc import ABCMeta, abstractclassmethod\n\n\nclass Module(metaclass=ABCMeta):\n    @abstractclassmethod\n    def train(cls, *args, **kwargs):\n        pass\n\n    @abstractclassmethod\n    def load(cls, *args, **kwargs):\n        pass\n\n    @abstractclassmethod\n    def _validate(cls, *args, **kwargs):\n        pass\n\n    @abstractclassmethod\n    def test(cls, *args, **kwargs):\n        pass\n\n    @abstractclassmethod\n    def deploy(cls, *args, **kwargs):\n        pass\n'"
lightnlp/base/tool.py,0,"b'from abc import ABCMeta, abstractclassmethod\n\n\nclass Tool(metaclass=ABCMeta):\n    @abstractclassmethod\n    def get_dataset(cls, *args, **kwargs):\n        pass\n\n    @abstractclassmethod\n    def get_vocab(cls, *args, **kwargs):\n        pass\n\n    @abstractclassmethod\n    def get_vectors(cls, *args, **kwargs):\n        pass\n\n    @abstractclassmethod\n    def get_iterator(cls, *args, **kwargs):\n        pass\n\n    @abstractclassmethod\n    def get_score(cls, *args, **kwargs):\n        pass\n'"
lightnlp/sl/__init__.py,0,"b""from .ner.module import NER\nfrom .pos.module import POS\nfrom .cws.module import CWS\nfrom .srl.module import SRL\n__all__ = ['NER', 'POS', 'CWS', 'SRL']\n"""
lightnlp/sp/__init__.py,0,"b""from .tdp.module import TDP\nfrom .gdp.module import GDP\n__all__ = ['TDP', 'GDP']"""
lightnlp/sr/__init__.py,0,"b""# sentence relation\nfrom .ss.module import SS\nfrom .te.module import TE\n__all__ = ['SS', 'TE']"""
lightnlp/tc/__init__.py,0,"b""from .sa.module import SA\nfrom .re.module import RE\n__all__ = ['SA', 'RE']\n"""
lightnlp/tg/__init__.py,0,"b""# text generation\nfrom .lm.module import LM\nfrom .cb.module import CB\nfrom .mt.module import MT\nfrom .ts.module import TS\n__all__ = ['LM', 'CB', 'MT', 'TS']\n"""
lightnlp/utils/__init__.py,0,b''
lightnlp/utils/deploy.py,0,"b'import socket\nfrom ..base.module import Module\n\n\ndef get_free_tcp_port():\n    tcp = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    tcp.bind((\'\', 0))\n    addr, port = tcp.getsockname()\n    tcp.close()\n    return port\n\n\nclass App:\n    def __init__(self):\n        self.services_list = []\n\n    def add_service(self, module: Module, route_path="""", host=""localhost"", port=None, debug=False):\n        pass\n'"
lightnlp/utils/learning.py,0,"b'def adjust_learning_rate(optimizer, new_lr):\n    """"""\n    Shrinks learning rate by a specified factor.\n\n    :param optimizer: optimizer whose learning rates must be decayed\n    :param new_lr: new learning rate\n    """"""\n\n    # print(""\\nDECAYING learning rate."")\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = new_lr\n    # print(""The new learning rate is %f\\n"" % (optimizer.param_groups[0][\'lr\'],))\n'"
lightnlp/utils/log.py,0,"b'#!/usr/bin/env python3\n# -*- coding:utf-8 -*-\n\n""""""\n    log.py\n    ~~~~~~\n\n    log module\n\n    :author:    lightless <root@lightless.me>\n    :homepage:  None\n    :license:   GPL-3.0, see LICENSE for more details.\n    :copyright: Copyright (c) 2017 lightless. All rights reserved\n""""""\n\nimport os\nimport logging\nimport logging.handlers\n\n__all__ = [""logger""]\n\n\n# \xe7\x94\xa8\xe6\x88\xb7\xe9\x85\x8d\xe7\xbd\xae\xe9\x83\xa8\xe5\x88\x86 \xe2\x86\x93\nLEVEL_COLOR = {\n    \'DEBUG\': \'cyan\',\n    \'INFO\': \'green\',\n    \'WARNING\': \'yellow\',\n    \'ERROR\': \'red\',\n    \'CRITICAL\': \'red,bg_white\',\n}\nSTDOUT_LOG_FMT = ""%(log_color)s[%(asctime)s] [%(levelname)s] [%(threadName)s] [%(filename)s:%(lineno)d] %(message)s""\nSTDOUT_DATE_FMT = ""%Y-%m-%d %H:%M:%S""\nFILE_LOG_FMT = ""[%(asctime)s] [%(levelname)s] [%(threadName)s] [%(filename)s:%(lineno)d] %(message)s""\nFILE_DATE_FMT = ""%Y-%m-%d %H:%M:%S""\n# \xe7\x94\xa8\xe6\x88\xb7\xe9\x85\x8d\xe7\xbd\xae\xe9\x83\xa8\xe5\x88\x86 \xe2\x86\x91\n\n\nclass ColoredFormatter(logging.Formatter):\n\n    COLOR_MAP = {\n        ""black"": ""30"",\n        ""red"": ""31"",\n        ""green"": ""32"",\n        ""yellow"": ""33"",\n        ""blue"": ""34"",\n        ""magenta"": ""35"",\n        ""cyan"": ""36"",\n        ""white"": ""37"",\n        ""bg_black"": ""40"",\n        ""bg_red"": ""41"",\n        ""bg_green"": ""42"",\n        ""bg_yellow"": ""43"",\n        ""bg_blue"": ""44"",\n        ""bg_magenta"": ""45"",\n        ""bg_cyan"": ""46"",\n        ""bg_white"": ""47"",\n        ""light_black"": ""1;30"",\n        ""light_red"": ""1;31"",\n        ""light_green"": ""1;32"",\n        ""light_yellow"": ""1;33"",\n        ""light_blue"": ""1;34"",\n        ""light_magenta"": ""1;35"",\n        ""light_cyan"": ""1;36"",\n        ""light_white"": ""1;37"",\n        ""light_bg_black"": ""100"",\n        ""light_bg_red"": ""101"",\n        ""light_bg_green"": ""102"",\n        ""light_bg_yellow"": ""103"",\n        ""light_bg_blue"": ""104"",\n        ""light_bg_magenta"": ""105"",\n        ""light_bg_cyan"": ""106"",\n        ""light_bg_white"": ""107"",\n    }\n\n    def __init__(self, fmt, datefmt):\n        super(ColoredFormatter, self).__init__(fmt, datefmt)\n\n    def parse_color(self, level_name):\n        color_name = LEVEL_COLOR.get(level_name, """")\n        if not color_name:\n            return """"\n\n        color_value = []\n        color_name = color_name.split("","")\n        for _cn in color_name:\n            color_code = self.COLOR_MAP.get(_cn, """")\n            if color_code:\n                color_value.append(color_code)\n\n        return ""\\033["" + "";"".join(color_value) + ""m""\n\n\n    def format(self, record):\n        record.log_color = self.parse_color(record.levelname)\n        message = super(ColoredFormatter, self).format(record) + ""\\033[0m""\n\n        return message\n\n\ndef _get_logger(log_to_file=True, log_filename=""default.log"", log_level=""DEBUG""):\n\n    _logger = logging.getLogger(__name__)\n\n    stdout_handler = logging.StreamHandler()\n    stdout_handler.setFormatter(\n        ColoredFormatter(\n            fmt=STDOUT_LOG_FMT,\n            datefmt=STDOUT_DATE_FMT,\n        )\n    )\n    _logger.addHandler(stdout_handler)\n\n    if log_to_file:\n        _tmp_path = os.path.dirname(os.path.abspath(__file__))\n        _tmp_path = os.path.join(_tmp_path, ""../logs/{}"".format(log_filename))\n        file_handler = logging.handlers.TimedRotatingFileHandler(_tmp_path, when=""midnight"", backupCount=30)\n        file_formatter = logging.Formatter(\n            fmt=FILE_LOG_FMT,\n            datefmt=FILE_DATE_FMT,\n        )\n        file_handler.setFormatter(file_formatter)\n        _logger.addHandler(file_handler)\n\n    _logger.setLevel(log_level)\n    return _logger\n\n\nlogger = _get_logger(log_to_file=False)\n'"
lightnlp/utils/score_func.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\np1 = torch.nn.PairwiseDistance(p=1)\np2 = torch.nn.PairwiseDistance(p=2)\n\n\ndef l1_score(vec1, vec2):\n    return p1(vec1, vec2)\n\n\ndef l2_score(vec1, vec2):\n    return p2(vec1, vec2)\n\n\ndef cos_score(vec1, vec2):\n    return F.cosine_similarity(vec1, vec2)\n'"
lightnlp/utils/word_vector.py,4,"b""import torch\nfrom torchtext.vocab import Vectors\n\nfrom .score_func import l1_score, l2_score, cos_score\n\n\nclass WordVectors(Vectors):\n\n    def __init__(self, *args, **kwargs):\n        super(WordVectors, self).__init__(*args, **kwargs)\n\n    def get_similar_words(self, word: str, topk=3, dis_type='cos'):\n        word_vector = self[word].view(1, -1)\n        if dis_type == 'l1':\n            dis_func = l1_score\n            similar_vectors = torch.tensor([torch.exp(-1 * dis_func(word_vector, self[x].view(1, -1))) for x in self.stoi])\n        elif dis_type == 'l2':\n            dis_func = l2_score\n            similar_vectors = torch.tensor([torch.exp(-1 * dis_func(word_vector, self[x].view(1, -1))) for x in self.stoi])\n        else:\n            dis_func = cos_score\n            similar_vectors = torch.tensor([dis_func(word_vector, self[x].view(1, -1)) for x in self.stoi])\n\n        topk_score, topk_index = torch.topk(similar_vectors, topk)\n        topk_score = topk_score.cpu().tolist()\n        topk_index = [self.itos[x] for x in topk_index]\n        return list(zip(topk_index, topk_score))\n\n\nif __name__ == '__main__':\n    vector_path = '/home/lightsmile/Projects/MyGithub/lightNLP/examples/cbow_saves/cbow_base.bin'\n    word_vectors = WordVectors(vector_path)\n    print(word_vectors.get_similar_words('\xe5\xb0\x91\xe5\xa5\xb3', dis_type='cos'))\n"""
lightnlp/we/__init__.py,0,"b""from .cbow.base.module import CBOWBaseModule\nfrom .cbow.hierarchical_softmax.module import CBOWHierarchicalSoftmaxModule\nfrom .cbow.negative_sampling.module import CBOWNegativeSamplingModule\nfrom .skip_gram.base.module import SkipGramBaseModule\nfrom .skip_gram.negative_sampling.module import SkipGramNegativeSamplingModule\nfrom .skip_gram.hierarchical_softmax.module import SkipGramHierarchicalSoftmaxModule\n__all__ = ['CBOWBaseModule', 'CBOWNegativeSamplingModule', 'CBOWHierarchicalSoftmaxModule', 'SkipGramBaseModule',\n           'SkipGramNegativeSamplingModule', 'SkipGramHierarchicalSoftmaxModule']\n"""
lightnlp/sl/cws/__init__.py,0,b''
lightnlp/sl/cws/config.py,0,"b""from ...base.config import DEVICE\nDEFAULT_CONFIG = {\n    'lr': 0.02,\n    'epoch': 30,\n    'lr_decay': 0.05,\n    'batch_size': 128,\n    'dropout': 0.5,\n    'static': False,\n    'non_static': False,\n    'embedding_dim': 300,\n    'num_layers': 2,\n    'pad_index': 1,\n    'vector_path': '',\n    'tag_num': 0,\n    'vocabulary_size': 0,\n    'word_vocab': None,\n    'tag_vocab': None,\n    'save_path': './cws_saves'\n}"""
lightnlp/sl/cws/model.py,7,"b""import torch\nimport torch.nn as nn\nfrom torchcrf import CRF\nfrom torchtext.vocab import Vectors\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom ...base.model import BaseConfig, BaseModel\n\n\nclass Config(BaseConfig):\n    def __init__(self, word_vocab, tag_vocab, vector_path, **kwargs):\n        super(Config, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.tag_vocab = tag_vocab\n        self.tag_num = len(self.tag_vocab)\n        self.vocabulary_size = len(self.word_vocab)\n        self.vector_path = vector_path\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass BiLstmCrf(BaseModel):\n    def __init__(self, args):\n        super(BiLstmCrf, self).__init__(args)\n        self.args = args\n        self.hidden_dim = 300\n        self.tag_num = args.tag_num\n        self.batch_size = args.batch_size\n        self.bidirectional = True\n        self.num_layers = args.num_layers\n        self.pad_index = args.pad_index\n        self.dropout = args.dropout\n        self.save_path = args.save_path\n\n        vocabulary_size = args.vocabulary_size\n        embedding_dimension = args.embedding_dim\n\n        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension).to(DEVICE)\n        if args.static:\n            logger.info('logging word vectors from {}'.format(args.vector_path))\n            vectors = Vectors(args.vector_path).vectors\n            self.embedding = self.embedding.from_pretrained(vectors, freeze=not args.non_static).to(DEVICE)\n\n        self.lstm = nn.LSTM(embedding_dimension, self.hidden_dim // 2, bidirectional=self.bidirectional,\n                            num_layers=self.num_layers, dropout=self.dropout).to(DEVICE)\n        self.hidden2label = nn.Linear(self.hidden_dim, self.tag_num).to(DEVICE)\n        self.crflayer = CRF(self.tag_num).to(DEVICE)\n\n        # self.init_weight()\n    \n    def init_weight(self):\n        nn.init.xavier_normal_(self.embedding.weight)\n        for name, param in self.lstm.named_parameters():\n            if 'weight' in name:\n                nn.init.xavier_normal_(param)\n        nn.init.xavier_normal_(self.hidden2label.weight)\n\n    def init_hidden(self, batch_size=None):\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n\n        return h0, c0\n\n    def loss(self, x, sent_lengths, y):\n        mask = torch.ne(x, self.pad_index)\n        emissions = self.lstm_forward(x, sent_lengths)\n        return self.crflayer(emissions, y, mask=mask)\n\n    def forward(self, x, sent_lengths):\n        mask = torch.ne(x, self.pad_index)\n        emissions = self.lstm_forward(x, sent_lengths)\n        return self.crflayer.decode(emissions, mask=mask)\n\n    def lstm_forward(self, sentence, sent_lengths):\n        x = self.embedding(sentence.to(DEVICE)).to(DEVICE)\n        x = pack_padded_sequence(x, sent_lengths)\n        self.hidden = self.init_hidden(batch_size=len(sent_lengths))\n        lstm_out, self.hidden = self.lstm(x, self.hidden)\n        lstm_out, new_batch_size = pad_packed_sequence(lstm_out)\n        assert torch.equal(sent_lengths, new_batch_size.to(DEVICE))\n        y = self.hidden2label(lstm_out.to(DEVICE))\n        return y.to(DEVICE)\n"""
lightnlp/sl/cws/module.py,6,"b'import torch\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ...utils.learning import adjust_learning_rate\nfrom ...utils.log import logger\nfrom ...utils.deploy import get_free_tcp_port\nfrom ...base.module import Module\n\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom .model import Config, BiLstmCrf\nfrom .tool import cws_tool\nfrom .utils.convert import bis_cws\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass CWS(Module):\n    """"""\n    """"""\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self._tag_vocab = None\n    \n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = cws_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = cws_tool.get_dataset(dev_path)\n            word_vocab, tag_vocab = cws_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab, tag_vocab = cws_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        self._tag_vocab = tag_vocab\n        train_iter = cws_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, tag_vocab, save_path=save_path, vector_path=vectors_path, **kwargs)\n        bilstmcrf = BiLstmCrf(config)\n        self._model = bilstmcrf\n        optim = torch.optim.Adam(bilstmcrf.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            bilstmcrf.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                bilstmcrf.zero_grad()\n                item_text_sentences = item.text[0]\n                item_text_lengths = item.text[1]\n                item_loss = (-bilstmcrf.loss(item_text_sentences, item_text_lengths, item.tag)) / item.tag.size(1)\n                acc_loss += item_loss.view(-1).cpu().data.tolist()[0]\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'cws_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'cws_train/dev_score\', dev_score, epoch)\n            writer.flush()\n\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        bilstmcrf.save()\n\n    def predict(self, text):\n        self._model.eval()\n        vec_text = torch.tensor([self._word_vocab.stoi[x] for x in text])\n        len_text = torch.tensor([len(vec_text)]).to(DEVICE)\n        vec_predict = self._model(vec_text.view(-1, 1).to(DEVICE), len_text)[0]\n        tag_predict = [self._tag_vocab.itos[i] for i in vec_predict]\n        return bis_cws([x for x in text], tag_predict)\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        bilstmcrf = BiLstmCrf(config)\n        bilstmcrf.load()\n        self._model = bilstmcrf\n        self._word_vocab = config.word_vocab\n        self._tag_vocab = config.tag_vocab\n    \n    def test(self, test_path):\n        test_dataset = cws_tool.get_dataset(test_path)\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n    \n    def _validate(self, dev_dataset):\n        self._model.eval()\n        dev_score_list = []\n        for dev_item in tqdm(dev_dataset):\n            item_score = cws_tool.get_score(self._model, dev_item.text, dev_item.tag, self._word_vocab, self._tag_vocab)\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def deploy(self, route_path=""/cws"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/predict"", methods=[\'POST\', \'GET\'])\n        def predict():\n            text = request.args.get(\'text\', \'\')\n            result = self.predict(text)\n            return flask.jsonify({\n                \'state\': \'OK\',\n                \'result\': {\n                    \'words\': result\n                }\n            })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/sl/cws/tool.py,4,"b""import torch\nfrom torchtext.data import Dataset, Field, BucketIterator, ReversibleField\nfrom torchtext.vocab import Vectors\nfrom torchtext.datasets import SequenceTaggingDataset\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\ndef light_tokenize(sequence: str):\n    return [sequence]\n\n\nTEXT = Field(sequential=True, tokenize=light_tokenize, include_lengths=True)\nTAG = ReversibleField(sequential=True, tokenize=light_tokenize, is_target=True, unk_token=None)\nFields = [('text', TEXT), ('tag', TAG)]\n\n\nclass CWSTool(Tool):\n    def get_dataset(self, path: str, fields=Fields, separator=' '):\n        logger.info('loading dataset from {}'.format(path))\n        st_dataset = SequenceTaggingDataset(path, fields=fields, separator=separator)\n        logger.info('successed loading dataset')\n        return st_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        TEXT.build_vocab(*dataset)\n        logger.info('successed building word vocab')\n        logger.info('building tag vocab...')\n        TAG.build_vocab(*dataset)\n        logger.info('successed building tag vocab')\n        return TEXT.vocab, TAG.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('successed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset: Dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE,\n                     sort_key=lambda x: len(x.text), sort_within_batch=True):\n        return BucketIterator(dataset, batch_size=batch_size, device=device, sort_key=sort_key,\n                              sort_within_batch=sort_within_batch)\n\n    def get_score(self, model, x, y, field_x, field_y, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        vec_x = torch.tensor([field_x.stoi[i] for i in x])\n        len_vec_x = torch.tensor([len(vec_x)]).to(DEVICE)\n        predict_y = model(vec_x.view(-1, 1).to(DEVICE), len_vec_x)[0]\n        true_y = [field_y.stoi[i] for i in y]\n        assert len(true_y) == len(predict_y)\n        return metric_func(predict_y, true_y, average='micro')\n\n\ncws_tool = CWSTool()\n"""
lightnlp/sl/ner/__init__.py,0,b''
lightnlp/sl/ner/config.py,0,"b""from ...base.config import DEVICE\nDEFAULT_CONFIG = {\n    'lr': 0.02,\n    'epoch': 30,\n    'lr_decay': 0.05,\n    'batch_size': 128,\n    'dropout': 0.5,\n    'static': False,\n    'non_static': False,\n    'embedding_dim': 300,\n    'num_layers': 2,\n    'pad_index': 1,\n    'vector_path': '',\n    'tag_num': 0,\n    'vocabulary_size': 0,\n    'word_vocab': None,\n    'tag_vocab': None,\n    'save_path': './ner_saves'\n}"""
lightnlp/sl/ner/model.py,7,"b""import torch\nimport torch.nn as nn\nfrom torchcrf import CRF\nfrom torchtext.vocab import Vectors\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom ...base.model import BaseConfig, BaseModel\n\n\nclass Config(BaseConfig):\n    def __init__(self, word_vocab, tag_vocab, vector_path, **kwargs):\n        super(Config, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.tag_vocab = tag_vocab\n        self.tag_num = len(self.tag_vocab)\n        self.vocabulary_size = len(self.word_vocab)\n        self.vector_path = vector_path\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass BiLstmCrf(BaseModel):\n    def __init__(self, args):\n        super(BiLstmCrf, self).__init__(args)\n        self.args = args\n        self.hidden_dim = 300\n        self.tag_num = args.tag_num\n        self.batch_size = args.batch_size\n        self.bidirectional = True\n        self.num_layers = args.num_layers\n        self.pad_index = args.pad_index\n        self.dropout = args.dropout\n        self.save_path = args.save_path\n\n        vocabulary_size = args.vocabulary_size\n        embedding_dimension = args.embedding_dim\n\n        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension).to(DEVICE)\n        if args.static:\n            logger.info('logging word vectors from {}'.format(args.vector_path))\n            vectors = Vectors(args.vector_path).vectors\n            self.embedding = self.embedding.from_pretrained(vectors, freeze=not args.non_static).to(DEVICE)\n\n        self.lstm = nn.LSTM(embedding_dimension, self.hidden_dim // 2, bidirectional=self.bidirectional,\n                            num_layers=self.num_layers, dropout=self.dropout).to(DEVICE)\n        self.hidden2label = nn.Linear(self.hidden_dim, self.tag_num).to(DEVICE)\n        self.crflayer = CRF(self.tag_num).to(DEVICE)\n\n        # self.init_weight()\n    \n    def init_weight(self):\n        nn.init.xavier_normal_(self.embedding.weight)\n        for name, param in self.lstm.named_parameters():\n            if 'weight' in name:\n                nn.init.xavier_normal_(param)\n        nn.init.xavier_normal_(self.hidden2label.weight)\n\n    def init_hidden(self, batch_size=None):\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n\n        return h0, c0\n\n    def loss(self, x, sent_lengths, y):\n        mask = torch.ne(x, self.pad_index)\n        emissions = self.lstm_forward(x, sent_lengths)\n        return self.crflayer(emissions, y, mask=mask)\n\n    def forward(self, x, sent_lengths):\n        mask = torch.ne(x, self.pad_index)\n        emissions = self.lstm_forward(x, sent_lengths)\n        return self.crflayer.decode(emissions, mask=mask)\n\n    def lstm_forward(self, sentence, sent_lengths):\n        x = self.embedding(sentence.to(DEVICE)).to(DEVICE)\n        x = pack_padded_sequence(x, sent_lengths)\n        self.hidden = self.init_hidden(batch_size=len(sent_lengths))\n        lstm_out, self.hidden = self.lstm(x, self.hidden)\n        lstm_out, new_batch_size = pad_packed_sequence(lstm_out)\n        assert torch.equal(sent_lengths, new_batch_size.to(DEVICE))\n        y = self.hidden2label(lstm_out.to(DEVICE))\n        return y.to(DEVICE)\n"""
lightnlp/sl/ner/module.py,6,"b'import torch\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ...utils.deploy import get_free_tcp_port\nfrom ...utils.learning import adjust_learning_rate\nfrom ...utils.log import logger\nfrom ...base.module import Module\n\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom .model import Config, BiLstmCrf\nfrom .tool import ner_tool\nfrom .utils.convert import iob_ranges\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass NER(Module):\n    """"""\n    """"""\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self._tag_vocab = None\n    \n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = ner_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = ner_tool.get_dataset(dev_path)\n            word_vocab, tag_vocab = ner_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab, tag_vocab = ner_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        self._tag_vocab = tag_vocab\n        train_iter = ner_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, tag_vocab, save_path=save_path, vector_path=vectors_path, **kwargs)\n        bilstmcrf = BiLstmCrf(config)\n        self._model = bilstmcrf\n        optim = torch.optim.Adam(bilstmcrf.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            bilstmcrf.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                bilstmcrf.zero_grad()\n                item_text_sentences = item.text[0]\n                item_text_lengths = item.text[1]\n                item_loss = (-bilstmcrf.loss(item_text_sentences, item_text_lengths, item.tag)) / item.tag.size(1)\n                acc_loss += item_loss.view(-1).cpu().data.tolist()[0]\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'ner_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'ner_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        bilstmcrf.save()\n\n    def predict(self, text):\n        self._model.eval()\n        vec_text = torch.tensor([self._word_vocab.stoi[x] for x in text])\n        len_text = torch.tensor([len(vec_text)]).to(DEVICE)\n        vec_predict = self._model(vec_text.view(-1, 1).to(DEVICE), len_text)[0]\n        tag_predict = [self._tag_vocab.itos[i] for i in vec_predict]\n        return iob_ranges([x for x in text], tag_predict)\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        bilstmcrf = BiLstmCrf(config)\n        bilstmcrf.load()\n        self._model = bilstmcrf\n        self._word_vocab = config.word_vocab\n        self._tag_vocab = config.tag_vocab\n    \n    def test(self, test_path):\n        test_dataset = ner_tool.get_dataset(test_path)\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n    \n    def _validate(self, dev_dataset):\n        self._model.eval()\n        dev_score_list = []\n        for dev_item in tqdm(dev_dataset):\n            item_score = ner_tool.get_score(self._model, dev_item.text, dev_item.tag, self._word_vocab, self._tag_vocab)\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def deploy(self, route_path=""/ner"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/predict"", methods=[\'POST\', \'GET\'])\n        def predict():\n            text = request.args.get(\'text\', \'\')\n            result = self.predict(text)\n            return flask.jsonify({\n                \'state\': \'OK\',\n                \'result\': {\n                    \'result\': result\n                }\n            })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/sl/ner/tool.py,4,"b""import torch\nfrom torchtext.data import Dataset, Field, BucketIterator, ReversibleField\nfrom torchtext.vocab import Vectors\nfrom torchtext.datasets import SequenceTaggingDataset\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\ndef light_tokenize(sequence: str):\n    return [sequence]\n\n\nTEXT = Field(sequential=True, tokenize=light_tokenize, include_lengths=True)\nTAG = ReversibleField(sequential=True, tokenize=light_tokenize, is_target=True, unk_token=None)\nFields = [('text', TEXT), ('tag', TAG)]\n\n\nclass NERTool(Tool):\n    def get_dataset(self, path: str, fields=Fields, separator=' '):\n        logger.info('loading dataset from {}'.format(path))\n        st_dataset = SequenceTaggingDataset(path, fields=fields, separator=separator)\n        logger.info('successed loading dataset')\n        return st_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        TEXT.build_vocab(*dataset)\n        logger.info('successed building word vocab')\n        logger.info('building tag vocab...')\n        TAG.build_vocab(*dataset)\n        logger.info('successed building tag vocab')\n        return TEXT.vocab, TAG.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('successed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset: Dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE,\n                     sort_key=lambda x: len(x.text), sort_within_batch=True):\n        return BucketIterator(dataset, batch_size=batch_size, device=device, sort_key=sort_key,\n                              sort_within_batch=sort_within_batch)\n\n    def get_score(self, model, x, y, field_x, field_y, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        vec_x = torch.tensor([field_x.stoi[i] for i in x])\n        len_vec_x = torch.tensor([len(vec_x)]).to(DEVICE)\n        predict_y = model(vec_x.view(-1, 1).to(DEVICE), len_vec_x)[0]\n        true_y = [field_y.stoi[i] for i in y]\n        assert len(true_y) == len(predict_y)\n        return metric_func(predict_y, true_y, average='micro')\n\n\nner_tool = NERTool()\n"""
lightnlp/sl/pos/__init__.py,0,b''
lightnlp/sl/pos/config.py,0,"b""from ...base.config import DEVICE\nDEFAULT_CONFIG = {\n    'lr': 0.02,\n    'epoch': 30,\n    'lr_decay': 0.05,\n    'batch_size': 128,\n    'dropout': 0.5,\n    'static': False,\n    'non_static': False,\n    'embedding_dim': 300,\n    'num_layers': 2,\n    'pad_index': 1,\n    'vector_path': '',\n    'tag_num': 0,\n    'vocabulary_size': 0,\n    'word_vocab': None,\n    'tag_vocab': None,\n    'save_path': './pos_saves'\n}"""
lightnlp/sl/pos/model.py,7,"b""import torch\nimport torch.nn as nn\nfrom torchcrf import CRF\nfrom torchtext.vocab import Vectors\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom ...base.model import BaseConfig, BaseModel\n\n\nclass Config(BaseConfig):\n    def __init__(self, word_vocab, tag_vocab, vector_path, **kwargs):\n        super(Config, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.tag_vocab = tag_vocab\n        self.tag_num = len(self.tag_vocab)\n        self.vocabulary_size = len(self.word_vocab)\n        self.vector_path = vector_path\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass BiLstmCrf(BaseModel):\n    def __init__(self, args):\n        super(BiLstmCrf, self).__init__(args)\n        self.args = args\n        self.hidden_dim = 300\n        self.tag_num = args.tag_num\n        self.batch_size = args.batch_size\n        self.bidirectional = True\n        self.num_layers = args.num_layers\n        self.pad_index = args.pad_index\n        self.dropout = args.dropout\n        self.save_path = args.save_path\n\n        vocabulary_size = args.vocabulary_size\n        embedding_dimension = args.embedding_dim\n\n        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension).to(DEVICE)\n        if args.static:\n            logger.info('logging word vectors from {}'.format(args.vector_path))\n            vectors = Vectors(args.vector_path).vectors\n            self.embedding = self.embedding.from_pretrained(vectors, freeze=not args.non_static).to(DEVICE)\n\n        self.lstm = nn.LSTM(embedding_dimension, self.hidden_dim // 2, bidirectional=self.bidirectional,\n                            num_layers=self.num_layers, dropout=self.dropout).to(DEVICE)\n        self.hidden2label = nn.Linear(self.hidden_dim, self.tag_num).to(DEVICE)\n        self.crflayer = CRF(self.tag_num).to(DEVICE)\n\n        # self.init_weight()\n    \n    def init_weight(self):\n        nn.init.xavier_normal_(self.embedding.weight)\n        for name, param in self.lstm.named_parameters():\n            if 'weight' in name:\n                nn.init.xavier_normal_(param)\n        nn.init.xavier_normal_(self.hidden2label.weight)\n\n    def init_hidden(self, batch_size=None):\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n\n        return h0, c0\n\n    def loss(self, x, sent_lengths, y):\n        mask = torch.ne(x, self.pad_index)\n        emissions = self.lstm_forward(x, sent_lengths)\n        return self.crflayer(emissions, y, mask=mask)\n\n    def forward(self, x, sent_lengths):\n        mask = torch.ne(x, self.pad_index)\n        emissions = self.lstm_forward(x, sent_lengths)\n        return self.crflayer.decode(emissions, mask=mask)\n\n    def lstm_forward(self, sentence, sent_lengths):\n        x = self.embedding(sentence.to(DEVICE)).to(DEVICE)\n        x = pack_padded_sequence(x, sent_lengths)\n        self.hidden = self.init_hidden(batch_size=len(sent_lengths))\n        lstm_out, self.hidden = self.lstm(x, self.hidden)\n        lstm_out, new_batch_size = pad_packed_sequence(lstm_out)\n        assert torch.equal(sent_lengths, new_batch_size.to(DEVICE))\n        y = self.hidden2label(lstm_out.to(DEVICE))\n        return y.to(DEVICE)\n"""
lightnlp/sl/pos/module.py,6,"b'import torch\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nimport flask\nfrom flask import Flask, request\n\nfrom ...utils.deploy import get_free_tcp_port\nfrom ...utils.learning import adjust_learning_rate\nfrom ...utils.log import logger\nfrom ...base.module import Module\n\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom .model import Config, BiLstmCrf\nfrom .tool import pos_tool\nfrom .utils.convert import bis_pos\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass POS(Module):\n    """"""\n    """"""\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self._tag_vocab = None\n    \n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = pos_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = pos_tool.get_dataset(dev_path)\n            word_vocab, tag_vocab = pos_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab, tag_vocab = pos_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        self._tag_vocab = tag_vocab\n        train_iter = pos_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, tag_vocab, save_path=save_path, vector_path=vectors_path, **kwargs)\n        bilstmcrf = BiLstmCrf(config)\n        self._model = bilstmcrf\n        optim = torch.optim.Adam(bilstmcrf.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            bilstmcrf.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                bilstmcrf.zero_grad()\n                item_text_sentences = item.text[0]\n                item_text_lengths = item.text[1]\n                item_loss = (-bilstmcrf.loss(item_text_sentences, item_text_lengths, item.tag)) / item.tag.size(1)\n                acc_loss += item_loss.view(-1).cpu().data.tolist()[0]\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'pos_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'pos_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        bilstmcrf.save()\n\n    def predict(self, text):\n        self._model.eval()\n        vec_text = torch.tensor([self._word_vocab.stoi[x] for x in text])\n        len_text = torch.tensor([len(vec_text)]).to(DEVICE)\n        vec_predict = self._model(vec_text.view(-1, 1).to(DEVICE), len_text)[0]\n        tag_predict = [self._tag_vocab.itos[i] for i in vec_predict]\n        return bis_pos([x for x in text], tag_predict)\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        bilstmcrf = BiLstmCrf(config)\n        bilstmcrf.load()\n        self._model = bilstmcrf\n        self._word_vocab = config.word_vocab\n        self._tag_vocab = config.tag_vocab\n    \n    def test(self, test_path):\n        test_dataset = pos_tool.get_dataset(test_path)\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n    \n    def _validate(self, dev_dataset):\n        self._model.eval()\n        dev_score_list = []\n        for dev_item in tqdm(dev_dataset):\n            item_score = pos_tool.get_score(self._model, dev_item.text, dev_item.tag, self._word_vocab, self._tag_vocab)\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def deploy(self, route_path=""/pos"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/predict"", methods=[\'POST\', \'GET\'])\n        def predict():\n            text = request.args.get(\'text\', \'\')\n            result = self.predict(text)\n            return flask.jsonify({\n                \'state\': \'OK\',\n                \'result\': {\n                    \'result\': result\n                }\n            })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/sl/pos/tool.py,4,"b""import torch\nfrom torchtext.data import Dataset, Field, BucketIterator, ReversibleField\nfrom torchtext.vocab import Vectors\nfrom torchtext.datasets import SequenceTaggingDataset\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\ndef light_tokenize(sequence: str):\n    return [sequence]\n\n\nTEXT = Field(sequential=True, tokenize=light_tokenize, include_lengths=True)\nTAG = ReversibleField(sequential=True, tokenize=light_tokenize, is_target=True, unk_token=None)\nFields = [('text', TEXT), ('tag', TAG)]\n\n\nclass POSTool(Tool):\n    def get_dataset(self, path: str, fields=Fields, separator=' '):\n        logger.info('loading dataset from {}'.format(path))\n        st_dataset = SequenceTaggingDataset(path, fields=fields, separator=separator)\n        logger.info('successed loading dataset')\n        return st_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        TEXT.build_vocab(*dataset)\n        logger.info('successed building word vocab')\n        logger.info('building tag vocab...')\n        TAG.build_vocab(*dataset)\n        logger.info('successed building tag vocab')\n        return TEXT.vocab, TAG.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('successed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset: Dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE,\n                     sort_key=lambda x: len(x.text), sort_within_batch=True):\n        return BucketIterator(dataset, batch_size=batch_size, device=device, sort_key=sort_key,\n                              sort_within_batch=sort_within_batch)\n\n    def get_score(self, model, x, y, field_x, field_y, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        vec_x = torch.tensor([field_x.stoi[i] for i in x])\n        len_vec_x = torch.tensor([len(vec_x)]).to(DEVICE)\n        predict_y = model(vec_x.view(-1, 1).to(DEVICE), len_vec_x)[0]\n        true_y = [field_y.stoi[i] for i in y]\n        assert len(true_y) == len(predict_y)\n        return metric_func(predict_y, true_y, average='micro')\n\n\npos_tool = POSTool()\n"""
lightnlp/sl/srl/__init__.py,0,b''
lightnlp/sl/srl/config.py,0,"b""from ...base.config import DEVICE\nDEFAULT_CONFIG = {\n    'lr': 0.02,\n    'epoch': 30,\n    'lr_decay': 0.05,\n    'batch_size': 128,\n    'dropout': 0.5,\n    'static': False,\n    'non_static': False,\n    'embedding_dim': 300,\n    'pos_dim': 50,\n    'num_layers': 2,\n    'pad_index': 1,\n    'vector_path': '',\n    'tag_num': 0,\n    'vocabulary_size': 0,\n    'pos_size': 0,\n    'word_vocab': None,\n    'pos_vocab': None,\n    'tag_vocab': None,\n    'save_path': './srl_saves'\n}"""
lightnlp/sl/srl/model.py,8,"b""import torch\nimport torch.nn as nn\nfrom torchcrf import CRF\nfrom torchtext.vocab import Vectors\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom ...base.model import BaseConfig, BaseModel\n\n\nclass Config(BaseConfig):\n    def __init__(self, word_vocab, pos_vocab, tag_vocab, vector_path, **kwargs):\n        super(Config, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.pos_vocab = pos_vocab\n        self.tag_vocab = tag_vocab\n        self.tag_num = len(self.tag_vocab)\n        self.vocabulary_size = len(self.word_vocab)\n        self.pos_size = len(self.pos_vocab)\n        self.vector_path = vector_path\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass BiLstmCrf(BaseModel):\n    def __init__(self, args):\n        super(BiLstmCrf, self).__init__(args)\n        self.args = args\n        self.hidden_dim = 300\n        self.tag_num = args.tag_num\n        self.batch_size = args.batch_size\n        self.bidirectional = True\n        self.num_layers = args.num_layers\n        self.pad_index = args.pad_index\n        self.dropout = args.dropout\n        self.save_path = args.save_path\n\n        vocabulary_size = args.vocabulary_size\n        embedding_dimension = args.embedding_dim\n        pos_size = args.pos_size\n        pos_dim = args.pos_dim\n\n        self.word_embedding = nn.Embedding(vocabulary_size, embedding_dimension).to(DEVICE)\n        if args.static:\n            logger.info('logging word vectors from {}'.format(args.vector_path))\n            vectors = Vectors(args.vector_path).vectors\n            self.word_embedding = nn.Embedding.from_pretrained(vectors, freeze=not args.non_static).to(DEVICE)\n        self.pos_embedding = nn.Embedding(pos_size, pos_dim).to(DEVICE)\n\n        self.lstm = nn.LSTM(embedding_dimension + pos_dim + 1, self.hidden_dim // 2, bidirectional=self.bidirectional,\n                            num_layers=self.num_layers, dropout=self.dropout).to(DEVICE)\n        self.hidden2label = nn.Linear(self.hidden_dim, self.tag_num).to(DEVICE)\n        self.crflayer = CRF(self.tag_num).to(DEVICE)\n\n        # self.init_weight()\n    \n    def init_weight(self):\n        nn.init.xavier_normal_(self.embedding.weight)\n        for name, param in self.lstm.named_parameters():\n            if 'weight' in name:\n                nn.init.xavier_normal_(param)\n        nn.init.xavier_normal_(self.hidden2label.weight)\n\n    def init_hidden(self, batch_size=None):\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n\n        return h0, c0\n\n    def loss(self, x, sent_lengths, pos, rel, y):\n        mask = torch.ne(x, self.pad_index)\n        emissions = self.lstm_forward(x, pos, rel, sent_lengths)\n        return self.crflayer(emissions, y, mask=mask)\n\n    def forward(self, x, poses, rels, sent_lengths):\n        mask = torch.ne(x, self.pad_index)\n        emissions = self.lstm_forward(x, poses, rels, sent_lengths)\n        return self.crflayer.decode(emissions, mask=mask)\n\n    def lstm_forward(self, sentence, poses, rels, sent_lengths):\n        word = self.word_embedding(sentence.to(DEVICE)).to(DEVICE)\n        pos = self.pos_embedding(poses.to(DEVICE)).to(DEVICE)\n        rels = rels.view(rels.size(0), rels.size(1), 1).float().to(DEVICE)\n        x = torch.cat((word, pos, rels), dim=2)\n        x = pack_padded_sequence(x, sent_lengths)\n        self.hidden = self.init_hidden(batch_size=len(sent_lengths))\n        lstm_out, self.hidden = self.lstm(x, self.hidden)\n        lstm_out, new_batch_size = pad_packed_sequence(lstm_out)\n        assert torch.equal(sent_lengths, new_batch_size.to(DEVICE))\n        y = self.hidden2label(lstm_out.to(DEVICE))\n        return y.to(DEVICE)\n"""
lightnlp/sl/srl/module.py,8,"b'import torch\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ...utils.deploy import get_free_tcp_port\nfrom ...utils.learning import adjust_learning_rate\nfrom ...utils.log import logger\nfrom ...base.module import Module\n\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom .model import Config, BiLstmCrf\nfrom .tool import srl_tool\nfrom .utils.convert import iobes_ranges\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass SRL(Module):\n    """"""\n    """"""\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self._tag_vocab = None\n        self._pos_vocab = None\n    \n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = srl_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = srl_tool.get_dataset(dev_path)\n            word_vocab, pos_vocab, tag_vocab = srl_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab, pos_vocab, tag_vocab = srl_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        self._pos_vocab = pos_vocab\n        self._tag_vocab = tag_vocab\n        train_iter = srl_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, pos_vocab, tag_vocab, save_path=save_path, vector_path=vectors_path, **kwargs)\n        bilstmcrf = BiLstmCrf(config)\n        self._model = bilstmcrf\n        optim = torch.optim.Adam(bilstmcrf.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            bilstmcrf.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                bilstmcrf.zero_grad()\n                item_text_sentences = item.text[0]\n                item_text_lengths = item.text[1]\n                item_loss = (-bilstmcrf.loss(item_text_sentences, item_text_lengths, item.pos, item.rel, item.tag)) / item.tag.size(1)\n                acc_loss += item_loss.view(-1).cpu().data.tolist()[0]\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'srl_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'srl_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        bilstmcrf.save()\n\n    def predict(self, word_list, pos_list, rel_list):\n        self._model.eval()\n        assert len(word_list) == len(pos_list) == len(rel_list)\n        vec_text = torch.tensor([self._word_vocab.stoi[x] for x in word_list]).view(-1, 1).to(DEVICE)\n        len_text = torch.tensor([len(vec_text)]).to(DEVICE)\n        vec_pos = torch.tensor([self._pos_vocab.stoi[x] for x in pos_list]).view(-1, 1).to(DEVICE)\n        vec_rel = torch.tensor([int(x) for x in rel_list]).view(-1, 1).to(DEVICE)\n        vec_predict = self._model(vec_text, vec_pos, vec_rel, len_text)[0]\n        tag_predict = [self._tag_vocab.itos[i] for i in vec_predict]\n        return iobes_ranges([x for x in word_list], tag_predict)\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        bilstmcrf = BiLstmCrf(config)\n        bilstmcrf.load()\n        self._model = bilstmcrf\n        self._word_vocab = config.word_vocab\n        self._tag_vocab = config.tag_vocab\n        self._pos_vocab = config.pos_vocab\n    \n    def test(self, test_path):\n        test_dataset = srl_tool.get_dataset(test_path)\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n    \n    def _validate(self, dev_dataset):\n        self._model.eval()\n        dev_score_list = []\n        for dev_item in tqdm(dev_dataset):\n            item_score = srl_tool.get_score(self._model, dev_item.text, dev_item.tag, dev_item.pos, dev_item.rel,\n                                            self._word_vocab, self._tag_vocab, self._pos_vocab)\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def deploy(self, route_path=""/srl"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + \'/predict\', methods=[\'POST\', \'GET\'])\n        def predict():\n            word_list = request.args.get(\'word_list\', [])\n            pos_list = request.args.get(\'pos_list\', [])\n            rel_list = request.args.get(\'rel_list\', [])\n            result = self.predict(word_list, pos_list, rel_list)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'result\': result\n                        }\n                })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/sl/srl/tool.py,6,"b""import torch\nfrom torchtext.data import Dataset, Field, BucketIterator\nfrom torchtext.vocab import Vectors\nfrom torchtext.datasets import SequenceTaggingDataset\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\ndef light_tokenize(sequence: str):\n    return [sequence]\n\n\ndef post_process(arr, _):\n    return [[int(item) for item in arr_item] for arr_item in arr]\n\n\nTEXT = Field(sequential=True, tokenize=light_tokenize, include_lengths=True)\nPOS = Field(sequential=True, tokenize=light_tokenize)\nREL = Field(sequential=True, use_vocab=False, unk_token=None, pad_token=0, postprocessing=post_process)\nTAG = Field(sequential=True, tokenize=light_tokenize, is_target=True, unk_token=None)\nFields = [('text', TEXT), ('pos', POS), ('rel', REL), ('tag', TAG)]\n\n\nclass SRLTool(Tool):\n    def get_dataset(self, path: str, fields=Fields, separator='\\t'):\n        logger.info('loading dataset from {}'.format(path))\n        st_dataset = SequenceTaggingDataset(path, fields=fields, separator=separator)\n        logger.info('successed loading dataset')\n        return st_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        TEXT.build_vocab(*dataset)\n        logger.info('successed building word vocab')\n        logger.info('building pos vocab...')\n        POS.build_vocab(*dataset)\n        logger.info('successed building pos vocab')\n        logger.info('building tag vocab...')\n        TAG.build_vocab(*dataset)\n        logger.info('successed building tag vocab')\n        return TEXT.vocab, POS.vocab, TAG.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('successed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset: Dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE,\n                     sort_key=lambda x: len(x.text), sort_within_batch=True):\n        return BucketIterator(dataset, batch_size=batch_size, device=device, sort_key=sort_key,\n                              sort_within_batch=sort_within_batch)\n\n    def get_score(self, model, x, y, pos, rel, field_x, field_y, field_pos, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        vec_x = torch.tensor([field_x.stoi[i] for i in x])\n        len_vec_x = torch.tensor([len(vec_x)]).to(DEVICE)\n        vec_pos = torch.tensor([field_pos.stoi[i] for i in pos])\n        vec_rel = torch.tensor([int(x) for x in rel])\n        predict_y = model(vec_x.view(-1, 1).to(DEVICE), vec_pos.view(-1, 1).to(DEVICE), vec_rel.view(-1, 1).to(DEVICE),\n                          len_vec_x)[0]\n        true_y = [field_y.stoi[i] for i in y]\n        assert len(true_y) == len(predict_y)\n        return metric_func(predict_y, true_y, average='micro')\n\n\nsrl_tool = SRLTool()\n"""
lightnlp/sp/gdp/__init__.py,0,b''
lightnlp/sp/gdp/config.py,0,"b'from ...base.config import DEVICE\n\nDEFAULT_CONFIG = {\n    \'lr\': 2e-3,\n    \'beta_1\': 0.9,\n    \'beta_2\': 0.9,\n    \'epsilon\': 1e-12,\n    \'decay\': .75,\n    \'decay_steps\': 5000,\n    \'epoch\': 50,\n    \'patience\': 100,\n    \'pad_index\': 1,\n    \'lr_decay\': 0.05,\n    \'batch_size\': 2,\n    \'dropout\': 0.5,\n    \'static\': True,\n    \'non_static\': False,\n    \'word_dim\': 300,\n    \'embed_dropout\': 0.33,\n    \'vector_path\': \'\',\n    \'class_num\': 0,\n    \'vocabulary_size\': 0,\n    \'word_vocab\': None,\n    \'pos_vocab\': None,\n    \'ref_vocab\': None,\n    \'save_path\': \'./gdp_saves\',\n    \'pos_dim\': 100,\n    \'lstm_hidden\': 400,\n    \'lstm_layers\': 3,\n    \'lstm_dropout\': 0.33,\n    \'mlp_arc\': 500,\n    \'mlp_rel\': 100,\n    \'mlp_dropout\': 0.33\n}\n\nROOT = \'<ROOT>\'\n\n\nclass Actions:\n    """"""Simple Enum for each possible parser action""""""\n    SHIFT = 0\n    REDUCE_L = 1\n    REDUCE_R = 2\n\n    NUM_ACTIONS = 3\n\n    action_to_ix = { ""SHIFT"": SHIFT,\n                     ""REDUCE_L"": REDUCE_L,\n                     ""REDUCE_R"": REDUCE_R }\n\n\n'"
lightnlp/sp/gdp/model.py,6,"b""import torch\nimport torch.nn as nn\nfrom torchtext.vocab import Vectors\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom ...base.model import BaseConfig, BaseModel\nfrom .components.dropout import IndependentDropout, SharedDropout\nfrom .components import LSTM, MLP, Biaffine\n\n\nclass Config(BaseConfig):\n    def __init__(self, word_vocab, pos_vocab, ref_vocab, vector_path, **kwargs):\n        super(Config, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.pos_vocab = pos_vocab\n        self.ref_vocab = ref_vocab\n        self.pos_num = len(self.pos_vocab)\n        self.ref_num = len(self.ref_vocab)\n        self.vocabulary_size = len(self.word_vocab)\n        self.vector_path = vector_path\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass BiaffineParser(BaseModel):\n    def __init__(self, args):\n        super(BiaffineParser, self).__init__(args)\n        self.args = args\n        self.hidden_dim = args.lstm_hidden\n        # self.tag_num = args.tag_num\n        self.batch_size = args.batch_size\n        self.bidirectional = True\n        # self.num_layers = args.num_layers\n        self.lstm_layters = args.lstm_layers\n        self.pad_index = args.pad_index\n        self.dropout = args.dropout\n        self.save_path = args.save_path\n\n        vocabulary_size = args.vocabulary_size\n        word_dim = args.word_dim\n        pos_num = args.pos_num\n        pos_dim = args.pos_dim\n\n        # the embedding layer\n        self.word_embedding = nn.Embedding(vocabulary_size, word_dim).to(DEVICE)\n        vectors = Vectors(args.vector_path).vectors\n        self.pretrained_embedding = nn.Embedding.from_pretrained(vectors).to(DEVICE)\n        self.pos_embedding = nn.Embedding(pos_num, pos_dim).to(DEVICE)\n        self.embed_dropout = IndependentDropout(p=args.embed_dropout).to(DEVICE)\n\n        # if args.static:\n        #     logger.info('logging word vectors from {}'.format(args.vector_path))\n        #     vectors = Vectors(args.vector_path).vectors\n        #     self.word_embedding = nn.Embedding.from_pretrained(vectors, freeze=not args.non_static).to(DEVICE)\n\n        # the word-lstm layer\n        self.lstm = LSTM(word_dim + pos_dim, self.hidden_dim, bidirectional=self.bidirectional,\n                            num_layers=self.lstm_layters, dropout=args.lstm_dropout).to(DEVICE)\n        self.lstm_dropout = SharedDropout(p=args.lstm_dropout).to(DEVICE)\n\n        # the MLP layers\n        self.mlp_arc_h = MLP(n_in=args.lstm_hidden*2, n_hidden=args.mlp_arc, dropout=args.mlp_dropout).to(DEVICE)\n        self.mlp_arc_d = MLP(n_in=args.lstm_hidden*2, n_hidden=args.mlp_arc, dropout=args.mlp_dropout).to(DEVICE)\n        self.mlp_rel_h = MLP(n_in=args.lstm_hidden*2, n_hidden=args.mlp_rel, dropout=args.mlp_dropout).to(DEVICE)\n        self.mlp_rel_d = MLP(n_in=args.lstm_hidden*2, n_hidden=args.mlp_rel, dropout=args.mlp_dropout).to(DEVICE)\n\n        # the Biaffine layers\n        self.arc_attn = Biaffine(n_in=args.mlp_arc, bias_x=True, bias_y=False).to(DEVICE)\n        self.rel_attn = Biaffine(n_in=args.mlp_rel, n_out=args.ref_num, bias_x=True, bias_y=True).to(DEVICE)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.zeros_(self.word_embedding.weight)\n\n    def init_hidden(self, batch_size=None):\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n\n        return h0, c0\n\n    def forward(self, words, tags):\n        # get the mask and lengths of given batch\n        mask = words.ne(self.pad_index)\n        lens = mask.sum(dim=1)\n        # get outputs from embedding layers\n        embed = self.pretrained_embedding(words)\n        embed += self.word_embedding(words.masked_fill_(words.ge(self.word_embedding.num_embeddings), 0))\n        tag_embed = self.pos_embedding(tags)\n        embed, tag_embed = self.embed_dropout(embed, tag_embed)\n        # concatenate the word and tag representations\n        x = torch.cat((embed, tag_embed), dim=-1)\n\n        sorted_lens, indices = torch.sort(lens, descending=True)\n        inverse_indices = indices.argsort()\n        x = pack_padded_sequence(x[indices], sorted_lens, True)\n        x = self.lstm(x)\n        x, _ = pad_packed_sequence(x, True)\n        x = self.lstm_dropout(x)[inverse_indices]\n\n        # apply MLPs to the LSTM output states\n        arc_h = self.mlp_arc_h(x)\n        arc_d = self.mlp_arc_d(x)\n        rel_h = self.mlp_rel_h(x)\n        rel_d = self.mlp_rel_d(x)\n\n        # get arc and rel scores from the bilinear attention\n        # [batch_size, seq_len, seq_len]\n        s_arc = self.arc_attn(arc_d, arc_h)\n        # [batch_size, seq_len, seq_len, n_rels]\n        s_rel = self.rel_attn(rel_d, rel_h).permute(0, 2, 3, 1)\n        # set the scores that exceed the length of each sentence to -inf\n        s_arc.masked_fill_((1 - mask).unsqueeze(1), float('-inf'))\n\n        return s_arc, s_rel\n"""
lightnlp/sp/gdp/module.py,9,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ...utils.deploy import get_free_tcp_port\nfrom ...utils.log import logger\nfrom ...base.module import Module\n\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom .model import Config, BiaffineParser\nfrom .tool import gdp_tool, WORD, POS, REF, ROOT\nfrom .utils.metric import AttachmentMethod\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass GDP(Module):\n    """"""\n    """"""\n\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self._pos_vocab = None\n        self._ref_vocab = None\n        self._pad_index = None\n\n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = gdp_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = gdp_tool.get_dataset(dev_path)\n            word_vocab, pos_vocab, head_vocab, ref_vocab = gdp_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab, pos_vocab, head_vocab, ref_vocab = gdp_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        self._pos_vocab = pos_vocab\n        self._ref_vocab = ref_vocab\n        train_iter = gdp_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, pos_vocab, ref_vocab, save_path=save_path, vector_path=vectors_path, **kwargs)\n        biaffine_parser = BiaffineParser(config)\n        self._model = biaffine_parser\n        self._pad_index = config.pad_index\n        optim = torch.optim.Adam(biaffine_parser.parameters(), lr=config.lr, betas=(config.beta_1, config.beta_2), eps=config.epsilon)\n        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optim,\n                                                      lr_lambda=lambda x: config.decay ** (x / config.decay_steps))\n        for epoch in range(config.epoch):\n            biaffine_parser.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                biaffine_parser.zero_grad()\n                words = item.word\n                tags = item.pos\n                refs = item.ref\n                arcs = item.head\n                # print(\'arcs\', arcs)\n                mask = words.ne(config.pad_index)\n                mask[:, 0] = 0\n                s_arc, s_rel = self._model(words, tags)\n                s_arc, s_rel = s_arc[mask], s_rel[mask]\n                # print(\'s_arc\', s_arc)\n                # print(s_arc.shape)\n                gold_arcs, gold_rels = arcs[mask], refs[mask]\n                # print(gold_arcs)\n                # print(gold_arcs.shape)\n                item_loss = self._get_loss(s_arc, s_rel, gold_arcs, gold_rels)\n                acc_loss += item_loss.cpu().item()\n                item_loss.backward()\n                nn.utils.clip_grad_norm_(self._model.parameters(), 5.0)\n                optim.step()\n                scheduler.step()\n            acc_loss /= len(train_iter)\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'gdp_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score, dev_metric = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                logger.info(\'metric:{}\'.format(dev_metric))\n                writer.add_scalar(\'gdp_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            # adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        biaffine_parser.save()\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        biaffine_parser = BiaffineParser(config)\n        biaffine_parser.load()\n        self._model = biaffine_parser\n        self._word_vocab = config.word_vocab\n        self._pos_vocab = config.pos_vocab\n        self._ref_vocab = config.ref_vocab\n        self._pad_index = config.pad_index\n        self._check_vocab()\n\n    def test(self, test_path):\n        test_dataset = gdp_tool.get_dataset(test_path)\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n\n    def _check_vocab(self):\n        if not hasattr(WORD, \'vocab\'):\n            WORD.vocab = self._word_vocab\n        if not hasattr(POS, \'vocab\'):\n            POS.vocab = self._pos_vocab\n        if not hasattr(REF, \'vocab\'):\n            REF.vocab = self._ref_vocab\n\n    def _validate(self, dev_dataset):\n        self._model.eval()\n        loss, metric = 0, AttachmentMethod()\n        dev_iter = gdp_tool.get_iterator(dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        for dev_item in tqdm(dev_iter):\n            mask = dev_item.word.ne(self._pad_index)\n            # ignore the first token of each sentence\n            mask[:, 0] = 0\n            s_arc, s_rel = self._model(dev_item.word, dev_item.pos)\n            s_arc, s_rel = s_arc[mask], s_rel[mask]\n            gold_arcs, gold_rels = dev_item.head[mask], dev_item.ref[mask]\n            pred_arcs, pred_rels = self._decode(s_arc, s_rel)\n            loss += self._get_loss(s_arc, s_rel, gold_arcs, gold_rels)\n            metric(pred_arcs, pred_rels, gold_arcs, gold_rels)\n        loss /= len(dev_iter)\n        return loss, metric\n\n    def predict(self, word_list: list, pos_list: list):\n        self._model.eval()\n        assert len(word_list) == len(pos_list)\n        word_list.insert(0, ROOT)\n        pos_list.insert(0, ROOT)\n        vec_word = WORD.numericalize([word_list]).to(DEVICE)\n        vec_pos = POS.numericalize([pos_list]).to(DEVICE)\n        mask = vec_word.ne(self._pad_index)\n        s_arc, s_rel = self._model(vec_word, vec_pos)\n        s_arc, s_rel = s_arc[mask], s_rel[mask]\n        pred_arcs, pred_rels = self._decode(s_arc, s_rel)\n        pred_arcs = pred_arcs.cpu().tolist()\n        pred_arcs[0] = 0\n        pred_rels = [self._ref_vocab.itos[rel] for rel in pred_rels]\n        pred_rels[0] = ROOT\n        return pred_arcs, pred_rels\n\n    def _get_loss(self, s_arc, s_rel, gold_arcs, gold_rels):\n        s_rel = s_rel[torch.arange(len(s_rel)), gold_arcs]\n        arc_loss = F.cross_entropy(s_arc, gold_arcs)\n        rel_loss = F.cross_entropy(s_rel, gold_rels)\n        loss = arc_loss + rel_loss\n        return loss\n\n    def _decode(self, s_arc, s_rel):\n        pred_arcs = s_arc.argmax(dim=-1)\n        pred_rels = s_rel[torch.arange(len(s_rel)), pred_arcs].argmax(dim=-1)\n\n        return pred_arcs, pred_rels\n\n    def deploy(self, route_path=""/cb"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/predict"", methods=[\'POST\', \'GET\'])\n        def predict():\n            words = request.args.get(\'words\', \'\')\n            pos = request.args.get(\'pos\', \'\')\n            result = self.predict(words, pos)\n            return flask.jsonify({\n                \'state\': \'OK\',\n                \'result\': {\n                    \'arcs\': result[0],\n                    \'rels\': result[1]\n                }\n            })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/sp/gdp/tool.py,2,"b""import torch\nfrom torchtext.data import Dataset, Field, BucketIterator\nfrom torchtext.vocab import Vectors\nfrom torchtext.datasets import SequenceTaggingDataset\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG, ROOT\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n# id, from, lemma, cpostag, postag, feats, head, depref\n\n\ndef light_tokenize(sequence: str):\n    return [sequence]\n\n\ndef head_tokenize(sequence: str):\n    return int(sequence)\n\n\ndef post_process(arr, _):\n    return [[int(item) for item in arr_item] for arr_item in arr]\n\n\n# WORD = Field(sequential=True, tokenize=light_tokenize, include_lengths=True)\nWORD = Field(sequential=True, tokenize=light_tokenize, batch_first=True, init_token=ROOT)\nPOS = Field(sequential=True, tokenize=light_tokenize, unk_token=None, batch_first=True, init_token=ROOT)\nHEAD = Field(sequential=True, use_vocab=False, unk_token=None, pad_token=0, postprocessing=post_process,\n             batch_first=True, init_token=0)\nREF = Field(sequential=True, tokenize=light_tokenize, unk_token=None, batch_first=True, init_token=ROOT)\nFields = [('id', None), ('word', WORD), ('lemma', None), ('cpostag', None), ('pos', POS),\n          ('feats', None), ('head', HEAD), ('ref', REF)]\n\n\nclass GDPTool(Tool):\n    def get_dataset(self, path: str, fields=Fields, separator='\\t'):\n        logger.info('loading dataset from {}'.format(path))\n        gdp_dataset = SequenceTaggingDataset(path, fields=fields, separator=separator)\n        logger.info('successed loading dataset')\n        return gdp_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        WORD.build_vocab(*dataset)\n        logger.info('successed building word vocab')\n        logger.info('building pos vocab...')\n        POS.build_vocab(*dataset)\n        logger.info('successed building pos vocab')\n        logger.info('building head vocab...')\n        HEAD.build_vocab(*dataset)\n        logger.info('successed building head vocab')\n        logger.info('building ref vocab...')\n        REF.build_vocab(*dataset)\n        logger.info('successed building ref vocab')\n        return WORD.vocab, POS.vocab, HEAD.vocab, REF.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('successed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset: Dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE,\n                     sort_key=lambda x: len(x.word), sort_within_batch=True):\n        return BucketIterator(dataset, batch_size=batch_size, device=device, sort_key=sort_key,\n                              sort_within_batch=sort_within_batch)\n\n    def get_score(self):\n        pass\n\n\ngdp_tool = GDPTool()\n\n"""
lightnlp/sp/tdp/__init__.py,0,b''
lightnlp/sp/tdp/config.py,0,"b'from ...base.config import DEVICE\n\nDEFAULT_CONFIG = {\n    \'lr\': 0.01,\n    \'epoch\': 5,\n    \'lr_decay\': 0.05,\n    \'batch_size\': 1,\n    \'dropout\': 0.5,\n    \'static\': True,\n    \'non_static\': False,\n    \'embedding_dim\': 300,\n    \'vector_path\': \'\',\n    \'class_num\': 0,\n    \'vocabulary_size\': 0,\n    \'word_vocab\': None,\n    \'action_vocab\': None,\n    \'save_path\': \'./tdp_saves\',\n    \'embedding_type\': \'lstm\',\n    \'embedding_lstm_layers\': 1,\n    \'embedding_lstm_dropout\': 0.0,\n    \'combiner\': \'lstm\',\n    \'combiner_lstm_layers\': 1,\n    \'combiner_lstm_dropout\': 0.0,\n    \'action_chooser\': \'default\',\n    \'feature_extractor\': \'default\',\n    \'num_features\': 3,\n    \'word_embedding_dim\': 100,\n    \'stack_embedding_dim\': 100\n}\n\n\nclass Actions:\n    """"""Simple Enum for each possible parser action""""""\n    SHIFT = 0\n    REDUCE_L = 1\n    REDUCE_R = 2\n\n    NUM_ACTIONS = 3\n\n    action_to_ix = { ""SHIFT"": SHIFT,\n                     ""REDUCE_L"": REDUCE_L,\n                     ""REDUCE_R"": REDUCE_R }\n\n\nEND_OF_INPUT_TOK = ""<END-OF-INPUT>""\nNULL_STACK_TOK = ""<NULL-STACK>""\nROOT_TOK = ""<ROOT>""\n'"
lightnlp/sp/tdp/model.py,3,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchtext.vocab import Vectors\n\nfrom ...utils.log import logger\nfrom ...base.model import BaseConfig, BaseModel\n\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nfrom collections import deque\n\nfrom .config import Actions, NULL_STACK_TOK, END_OF_INPUT_TOK, ROOT_TOK\nfrom .tool import TEXT\n\nfrom .utils.parser_state import ParserState, DepGraphEdge\nfrom .utils import vectors\nfrom .utils.feature_extractor import SimpleFeatureExtractor\nfrom .components.combiner import MLPCombinerNetwork, LSTMCombinerNetwork\nfrom .components.action_chooser import ActionChooserNetwork\nfrom .components.word_embedding import VanillaWordEmbeddingLookup, BiLSTMWordEmbeddingLookup\n\n\nclass Config(BaseConfig):\n    def __init__(self, word_vocab, action_vocab, vector_path, **kwargs):\n        super(Config, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.action_vocab = action_vocab\n        self.action_num = len(self.action_vocab)\n        self.vocabulary_size = len(self.word_vocab)\n        self.vector_path = vector_path\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass TransitionParser(BaseModel):\n\n    def __init__(self, args):\n        super(TransitionParser, self).__init__(args)\n\n        self.args = args\n        self.action_num = args.action_num\n        self.batch_size = args.batch_size\n        self.save_path = args.save_path\n\n        vocabulary_size = args.vocabulary_size\n        embedding_dimension = args.embedding_dim\n\n        # feature extractor\n        self.feature_extractor = SimpleFeatureExtractor() if args.feature_extractor == 'default' else None\n\n        if args.embedding_type == 'lstm':\n            self.word_embedding_component = BiLSTMWordEmbeddingLookup(vocabulary_size, args.word_embedding_dim,\n                                                                      args.stack_embedding_dim, args.embedding_lstm_layers,\n                                                                      args.embedding_lstm_dropout, args.vector_path,\n                                                                      args.non_static)\n        elif args.embedding_type == 'vanilla':\n            self.word_embedding_component = VanillaWordEmbeddingLookup(vocabulary_size, args.word_embedding_dim)\n        else:\n            self.word_embedding_component = None\n        \n        self.action_chooser = ActionChooserNetwork(args.stack_embedding_dim * args.num_features) if args.action_chooser == 'default' else None\n\n        if args.combiner == 'lstm':\n            self.combiner = LSTMCombinerNetwork(args.stack_embedding_dim, args.combiner_lstm_layers, args.combiner_lstm_dropout)\n        elif args.combiner == 'mlp':\n            self.combiner = MLPCombinerNetwork(args.stack_embedding_dim)\n        else:\n            self.combiner = None\n\n        self.null_stack_tok_embed = torch.randn(1, self.word_embedding_component.output_dim).to(DEVICE)\n\n    def forward(self, sentence, actions=None):\n        self.refresh()  # clear up hidden states from last run, if need be\n\n        # Initialize the parser state\n        sentence_embs = self.word_embedding_component(sentence)\n\n        parser_state = ParserState(sentence, sentence_embs, self.combiner,\n                                   null_stack_tok_embed=self.null_stack_tok_embed)\n        outputs = []  # Holds the output of each action decision\n        actions_done = []  # Holds all actions we have done\n\n        dep_graph = set()  # Build this up as you go\n\n        # Make the action queue if we have it\n        if actions is not None:\n            action_queue = deque()\n            action_queue.extend([Actions.action_to_ix[a] for a in actions])\n            have_gold_actions = True\n        else:\n            have_gold_actions = False\n\n        while True:\n            if parser_state.done_parsing():\n                break\n            # get features\n            features = self.feature_extractor.get_features(parser_state)\n\n            # get log probabilities over actions\n            log_probs = self.action_chooser(features)\n\n            # get next step action\n            if have_gold_actions:\n                temp_action = action_queue.popleft()\n            else:\n                temp_action = vectors.argmax(log_probs)\n            \n            # rectify action\n            if parser_state.input_buffer_len() == 1:\n                temp_action = Actions.REDUCE_R\n            elif parser_state.stack_len() < 2:\n                temp_action = Actions.SHIFT\n\n            # update parser_state\n            if temp_action == Actions.SHIFT:\n                parser_state.shift()\n                reduction = None\n            elif temp_action == Actions.REDUCE_L:\n                reduction = parser_state.reduce_left()\n            elif temp_action == Actions.REDUCE_R:\n                reduction = parser_state.reduce_right()\n            else:\n                raise Exception('unvalid action!: {}'.format(temp_action))\n            \n            # keep track of states\n            outputs.append(log_probs)\n            if reduction:\n                dep_graph.add(reduction)\n            actions_done.append(temp_action)\n\n        dep_graph.add(DepGraphEdge((TEXT.vocab.stoi[ROOT_TOK], -1), (parser_state.stack[-1].headword, parser_state.stack[-1].headword_pos)))\n        return outputs, dep_graph, actions_done\n\n    def refresh(self):\n        if isinstance(self.combiner, LSTMCombinerNetwork):\n            self.combiner.clear_hidden_state()\n        if isinstance(self.word_embedding_component, BiLSTMWordEmbeddingLookup):\n            self.word_embedding_component.clear_hidden_state()\n\n    def predict(self, sentence):\n        _, dep_graph, _ = self.forward(sentence)\n        return dep_graph\n\n    def predict_actions(self, sentence):\n        _, _, actions_done = self.forward(sentence)\n        return actions_done\n\n    def to_cuda(self):\n        self.word_embedding_component.use_cuda = True\n        self.combiner.use_cuda = True\n        self.cuda()\n\n    def to_cpu(self):\n        self.word_embedding_component.use_cuda = False\n        self.combiner.use_cuda = False\n        self.cpu()\n\n"""
lightnlp/sp/tdp/module.py,6,"b'import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ...utils.deploy import get_free_tcp_port\nfrom ...utils.learning import adjust_learning_rate\nfrom ...utils.log import logger\nfrom ...base.module import Module\n\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom .model import Config, TransitionParser\nfrom .tool import tdp_tool, TEXT, ACTION, light_tokenize\nfrom .utils.parser_state import DepGraphEdge\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass TDP(Module):\n    """"""\n    """"""\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self._action_vocab = None\n    \n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = tdp_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = tdp_tool.get_dataset(dev_path)\n            word_vocab, action_vocab = tdp_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab, action_vocab = tdp_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        self._action_vocab = action_vocab\n        train_iter = tdp_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, action_vocab, save_path=save_path, vector_path=vectors_path, **kwargs)\n        trainsition_parser = TransitionParser(config)\n        self._model = trainsition_parser\n        optim = torch.optim.Adam(trainsition_parser.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            trainsition_parser.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                self._model.refresh()\n                optim.zero_grad()\n                outputs, dep_graph, actions_done = self._model(item.text)\n                item_loss = 0\n                for step_output, step_action in zip(outputs, item.action):\n                    # print(step_output)\n                    # print(step_action)\n                    item_loss += F.cross_entropy(step_output, step_action)\n                acc_loss += item_loss.item()\n                item_loss.backward()\n                optim.step()\n            acc_loss /= len(train_iter)\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'tdp_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'tdp_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        trainsition_parser.save()\n\n    def predict(self, text):\n        self._model.eval()\n        sentences = light_tokenize(text)\n        vec_text = torch.tensor([self._word_vocab.stoi[x] for x in sentences])\n        outputs, dep_graph, actions_done = self._model(vec_text.view(-1, 1).to(DEVICE))\n        # tag_predict = [self._action_vocab.itos[i] for i in vec_predict]\n        # return iob_ranges([x for x in text], tag_predict)\n        results = set()\n        for edge in dep_graph:\n            results.add(DepGraphEdge((self._word_vocab.itos[edge.head[0]], edge.head[1]),\n                                     (self._word_vocab.itos[edge.modifier[0]], edge.modifier[1])))\n        return results\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        trainsition_parser = TransitionParser(config)\n        trainsition_parser.load()\n        self._model = trainsition_parser\n        self._word_vocab = config.word_vocab\n        self._action_vocab = config.action_vocab\n        self._check_vocab()\n    \n    def test(self, test_path):\n        test_dataset = tdp_tool.get_dataset(test_path)\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n    \n    def _validate(self, dev_dataset):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = tdp_tool.get_iterator(dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        for dev_item in tqdm(dev_iter):\n            self._model.refresh()\n            item_score = tdp_tool.get_score(self._model, dev_item.text, dev_item.action)\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def _check_vocab(self):\n        if not hasattr(TEXT, \'vocab\'):\n            TEXT.vocab = self._word_vocab\n        if not hasattr(ACTION, \'vocab\'):\n            ACTION.vocab = self._action_vocab\n\n    def deploy(self, route_path=""/tdp"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + \'/predict\', methods=[\'POST\', \'GET\'])\n        def predict():\n            text = request.args.get(\'text\', \'\')\n            result = self.predict(text)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'results\': result\n                        }\n                })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/sp/tdp/tool.py,2,"b""import torch\nfrom torchtext.data import Dataset, Field, BucketIterator, ReversibleField\nfrom torchtext.vocab import Vectors\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .utils.dataset import TransitionDataset\nfrom .config import DEVICE, DEFAULT_CONFIG, END_OF_INPUT_TOK, ROOT_TOK, NULL_STACK_TOK\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\ndef light_tokenize(sequence: str):\n    return sequence.split()\n\n\ndef action_tokenize(sequence: str):\n    return [sequence]\n\n\nTEXT = Field(sequential=True, tokenize=light_tokenize, eos_token=END_OF_INPUT_TOK, pad_token=None)\nACTION = ReversibleField(\n    sequential=True, tokenize=action_tokenize, is_target=True, unk_token=None, pad_token=None)\n\nFields = [('text', TEXT), ('action', ACTION)]\n\n\nclass TDPTool(Tool):\n    def get_dataset(self, path: str, fields=Fields, separator=' ||| '):\n        logger.info('loading dataset from {}'.format(path))\n        tdp_dataset = TransitionDataset(path, fields=fields, separator=separator)\n        logger.info('successed loading dataset')\n        return tdp_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        TEXT.build_vocab(*dataset, specials=[ROOT_TOK, NULL_STACK_TOK])\n        logger.info('successed building word vocab')\n        logger.info('building tag vocab...')\n        ACTION.build_vocab(*dataset)\n        logger.info('successed building tag vocab')\n        return TEXT.vocab, ACTION.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('successed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset: Dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE):\n        return BucketIterator(dataset, batch_size=batch_size, device=device)\n\n    def get_score(self, model, x, y, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        outputs, dep_graph, actions_done = model(x)\n        assert len(actions_done) == len(y)\n        predict_y = actions_done\n        true_y = y.cpu().view(-1).tolist()\n        # print(actions_done, y)\n        # print(actions_done)\n        # print(true_y)\n        return metric_func(predict_y, true_y, average='micro')\n\n\ntdp_tool = TDPTool()\n"""
lightnlp/sr/ss/__init__.py,0,b'# sentence similarity'
lightnlp/sr/ss/config.py,0,"b""from ...base.config import DEVICE\nDEFAULT_CONFIG = {\n    'lr': 0.02,\n    'epoch': 30,\n    'lr_decay': 0.05,\n    'batch_size': 16,\n    'dropout': 0.5,\n    'static': False,\n    'non_static': False,\n    'embedding_dim': 300,\n    'num_layers': 2,\n    'pad_index': 1,\n    'vector_path': '',\n    'tag_num': 0,\n    'fix_length': 32,\n    'vocabulary_size': 0,\n    'word_vocab': None,\n    'tag_vocab': None,\n    'save_path': './ss_saves'\n}"""
lightnlp/sr/ss/model.py,7,"b""import torch\nimport torch.nn as nn\nfrom torchtext.vocab import Vectors\n\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom ...base.model import BaseConfig, BaseModel\n\n\nclass Config(BaseConfig):\n    def __init__(self, word_vocab, label_vocab, vector_path, **kwargs):\n        super(Config, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.label_vocab = label_vocab\n        self.class_num = len(self.label_vocab)\n        self.vocabulary_size = len(self.word_vocab)\n        self.vector_path = vector_path\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass MaLSTM(BaseModel):\n    def __init__(self, args):\n        super(MaLSTM, self).__init__(args)\n\n        self.args = args\n        self.hidden_dim = 300\n        self.tag_num = args.tag_num\n        self.batch_size = args.batch_size\n        self.bidirectional = True\n        self.num_layers = args.num_layers\n        self.pad_index = args.pad_index\n        self.dropout = args.dropout\n        self.save_path = args.save_path\n\n        vocabulary_size = args.vocabulary_size\n        embedding_dimension = args.embedding_dim\n\n        self.pwd = torch.nn.PairwiseDistance(p=1)\n\n        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension).to(DEVICE)\n        if args.static:\n            logger.info('logging word vectors from {}'.format(args.vector_path))\n            vectors = Vectors(args.vector_path).vectors\n            self.embedding = self.embedding.from_pretrained(vectors, freeze=not args.non_static).to(DEVICE)\n\n        self.lstm = nn.LSTM(embedding_dimension, self.hidden_dim // 2, bidirectional=self.bidirectional,\n                            num_layers=self.num_layers, dropout=self.dropout).to(DEVICE)\n        self.hidden2label = nn.Linear(self.hidden_dim, self.tag_num).to(DEVICE)\n\n    def init_hidden(self, batch_size=None):\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n\n        return h0, c0\n\n    def forward(self, left, right):\n        left_vec = self.embedding(left.to(DEVICE)).to(DEVICE)\n        #         left_vec = pack_padded_sequence(left_vec, left_sent_lengths)\n        right_vec = self.embedding(right.to(DEVICE)).to(DEVICE)\n        #         right_vec = pack_padded_sequence(right_vec, right_sent_lengths)\n\n        self.hidden = self.init_hidden(batch_size=left.size(1))\n\n        left_lstm_out, (left_lstm_hidden, _) = self.lstm(left_vec, self.hidden)\n        #         left_lstm_out, left_batch_size = pad_packed_sequence(left_lstm_out)\n        #         assert torch.equal(left_sent_lengths, left_batch_size.to(DEVICE))\n\n        right_lstm_out, (right_lstm_hidden, _) = self.lstm(right_vec, self.hidden)\n        #         right_lstm_out, right_batch_size = pad_packed_sequence(right_lstm_out)\n        #         assert torch.equal(right_sent_lengths, right_batch_size.to(DEVICE))\n\n        return self.manhattan_distance(left_lstm_hidden[0], right_lstm_hidden[0])\n\n    def manhattan_distance(self, left, right):\n        return torch.exp(-self.pwd(left, right))\n"""
lightnlp/sr/ss/module.py,8,"b'import torch\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ...utils.deploy import get_free_tcp_port\nfrom ...utils.learning import adjust_learning_rate\nfrom ...utils.log import logger\nfrom ...base.module import Module\n\nfrom .tool import ss_tool, TEXT, LABEL\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom .model import Config, MaLSTM\nfrom .utils.pad import pad_sequnce\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass SS(Module):\n    """"""\n    """"""\n\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self._label_vocab = None\n\n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = ss_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = ss_tool.get_dataset(dev_path)\n            word_vocab, tag_vocab = ss_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab, tag_vocab = ss_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        self._label_vocab = tag_vocab\n        train_iter = ss_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, tag_vocab, save_path=save_path, vector_path=vectors_path, **kwargs)\n        malstm = MaLSTM(config)\n        self._model = malstm\n        optim = torch.optim.Adam(self._model.parameters(), lr=config.lr)\n        loss_func = torch.nn.MSELoss().to(DEVICE)\n        for epoch in range(config.epoch):\n            self._model.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                self._model.zero_grad()\n                left_text = item.texta\n                right_text = item.textb\n                predict_dis = self._model(left_text, right_text)\n                item_loss = loss_func(predict_dis, item.label.type(torch.float32))\n                acc_loss += item_loss.view(-1).cpu().item()\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'ss_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'ss_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        self._model.save()\n\n    def predict(self, texta: str, textb: str):\n        self._model.eval()\n        pad_texta = pad_sequnce([x for x in texta], DEFAULT_CONFIG[\'fix_length\'])\n        vec_texta = torch.tensor([self._word_vocab.stoi[x] for x in pad_texta])\n        pad_textb = pad_sequnce([x for x in textb], DEFAULT_CONFIG[\'fix_length\'])\n        vec_textb = torch.tensor([self._word_vocab.stoi[x] for x in pad_textb])\n        vec_predict = self._model(vec_texta.view(-1, 1).to(DEVICE),\n                                  vec_textb.view(-1, 1).to(DEVICE))[0]\n        return vec_predict.cpu().item()\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        malstm = MaLSTM(config)\n        malstm.load()\n        self._model = malstm\n        self._word_vocab = config.word_vocab\n        self._label_vocab = config.label_vocab\n\n    def test(self, test_path):\n        test_dataset = ss_tool.get_dataset(test_path)\n        if not hasattr(TEXT, \'vocab\'):\n            TEXT.vocab = self._word_vocab\n        if not hasattr(LABEL, \'vocab\'):\n            LABEL.vocab = self._label_vocab\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n\n    def _validate(self, dev_dataset):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = ss_tool.get_iterator(dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        for dev_item in tqdm(dev_iter):\n            item_score = ss_tool.get_score(self._model, dev_item.texta, dev_item.textb, dev_item.label)\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def deploy(self, route_path=""/ss"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + \'/predict\', methods=[\'POST\', \'GET\'])\n        def predict():\n            texta = request.args.get(\'texta\', \'\')\n            textb = request.args.get(\'textb\', \'\')\n            result = self.predict(texta, textb)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'prob\': result\n                        }\n                })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/sr/ss/tool.py,3,"b""import re\nimport torch\nfrom torchtext.data import TabularDataset, Field, Dataset, BucketIterator\nfrom torchtext.vocab import Vectors\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\nregex = re.compile(r'[^\\u4e00-\\u9fa5aA-Za-z0-9\xef\xbc\x8c\xe3\x80\x82\xef\xbc\x9f\xef\xbc\x9a\xef\xbc\x81\xef\xbc\x9b\xe2\x80\x9c\xe2\x80\x9d]')\n\n\ndef light_tokenize(text):\n    text = regex.sub(' ', text)\n    return [word for word in text if word.strip()]\n\n\nTEXT = Field(lower=True, tokenize=light_tokenize, fix_length=DEFAULT_CONFIG['fix_length'])\nLABEL = Field(sequential=False, unk_token=None)\nFields = [\n            ('index', None),\n            ('texta', TEXT),\n            ('textb', TEXT),\n            ('label', LABEL),\n        ]\n\n\nclass SSTool(Tool):\n    def get_dataset(self, path: str, fields=Fields, file_type='tsv', skip_header=True):\n        logger.info('loading dataset from {}'.format(path))\n        st_dataset = TabularDataset(path, format=file_type, fields=fields, skip_header=skip_header)\n        logger.info('successed loading dataset')\n        return st_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        TEXT.build_vocab(*dataset)\n        logger.info('successed building word vocab')\n        logger.info('building label vocab...')\n        LABEL.build_vocab(*dataset)\n        logger.info('successed building label vocab')\n        return TEXT.vocab, LABEL.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('successed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset: Dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE,\n                 sort_key=lambda x: len(x.texta)):\n        return BucketIterator(dataset, batch_size=batch_size, device=device, sort_key=sort_key)\n\n    def get_score(self, model, texta, textb, labels, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        assert texta.size(1) == textb.size(1) == len(labels)\n        predict_prob = model(texta, textb)\n        # print('predict', predict_prob)\n        # print('labels', labels)\n        predict_labels = torch.gt(predict_prob, 0.5)\n        predict_labels = predict_labels.view(-1).cpu().data.numpy()\n        labels = labels.view(-1).cpu().data.numpy()\n        return metric_func(predict_labels, labels, average='micro')\n\n\nss_tool = SSTool()"""
lightnlp/sr/te/__init__.py,0,b'# text entailment'
lightnlp/sr/te/config.py,0,"b""from ...base.config import DEVICE\nDEFAULT_CONFIG = {\n    'lr': 0.02,\n    'epoch': 10,\n    'lr_decay': 0.05,\n    'batch_size': 16,\n    'dropout': 0.5,\n    'static': False,\n    'non_static': False,\n    'embedding_dim': 300,\n    'num_layers': 2,\n    'pad_index': 1,\n    'vector_path': '',\n    'tag_num': 0,\n    'fix_length': 32,\n    'vocabulary_size': 0,\n    'word_vocab': None,\n    'tag_vocab': None,\n    'save_path': './te_saves'\n}"""
lightnlp/sr/te/model.py,4,"b""import torch\nimport torch.nn as nn\nfrom torchtext.vocab import Vectors\n\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom ...base.model import BaseConfig, BaseModel\n\n\nclass Config(BaseConfig):\n    def __init__(self, word_vocab, label_vocab, vector_path, **kwargs):\n        super(Config, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.label_vocab = label_vocab\n        self.class_num = len(self.label_vocab)\n        self.vocabulary_size = len(self.word_vocab)\n        self.vector_path = vector_path\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass SharedLSTM(BaseModel):\n    def __init__(self, args):\n        super(SharedLSTM, self).__init__(args)\n\n        self.args = args\n        self.hidden_dim = 300\n        self.class_num = args.class_num\n        self.batch_size = args.batch_size\n        self.bidirectional = True\n        self.num_layers = args.num_layers\n        self.pad_index = args.pad_index\n        self.dropout = args.dropout\n        self.save_path = args.save_path\n\n        vocabulary_size = args.vocabulary_size\n        embedding_dimension = args.embedding_dim\n\n        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension).to(DEVICE)\n        if args.static:\n            logger.info('logging word vectors from {}'.format(args.vector_path))\n            vectors = Vectors(args.vector_path).vectors\n            self.embedding = self.embedding.from_pretrained(vectors, freeze=not args.non_static).to(DEVICE)\n\n        self.lstm = nn.LSTM(embedding_dimension, self.hidden_dim // 2, bidirectional=self.bidirectional,\n                            num_layers=self.num_layers, dropout=self.dropout).to(DEVICE)\n        self.dropout_layer = nn.Dropout(self.dropout).to(DEVICE)\n        self.batch_norm = nn.BatchNorm1d(self.hidden_dim * 2).to(DEVICE)\n        self.hidden2label = nn.Linear(self.hidden_dim * 2, self.class_num).to(DEVICE)\n\n    def init_hidden(self, batch_size=None):\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n\n        return h0, c0\n\n    def forward(self, left, right):\n        left_vec = self.embedding(left.to(DEVICE)).to(DEVICE)\n        right_vec = self.embedding(right.to(DEVICE)).to(DEVICE)\n\n        self.hidden = self.init_hidden(batch_size=left.size(1))\n\n        left_lstm_out, (left_lstm_hidden, _) = self.lstm(left_vec, self.hidden)\n\n        right_lstm_out, (right_lstm_hidden, _) = self.lstm(right_vec, self.hidden)\n\n        merged = torch.cat((left_lstm_out[-1], right_lstm_out[-1]), dim=1)\n\n        merged = self.dropout_layer(merged)\n\n        merged = self.batch_norm(merged)\n\n        predict = self.hidden2label(merged)\n\n        return predict\n"""
lightnlp/sr/te/module.py,9,"b'import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ...utils.deploy import get_free_tcp_port\nfrom ...utils.learning import adjust_learning_rate\nfrom ...utils.log import logger\nfrom ...base.module import Module\n\nfrom .tool import te_tool, TEXT, LABEL\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom .model import Config, SharedLSTM\nfrom .utils.pad import pad_sequnce\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass TE(Module):\n    """"""\n    """"""\n\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self._label_vocab = None\n\n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = te_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = te_tool.get_dataset(dev_path)\n            word_vocab, label_vocab = te_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab, label_vocab = te_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        self._label_vocab = label_vocab\n        train_iter = te_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, label_vocab, save_path=save_path, vector_path=vectors_path, **kwargs)\n        shared_lstm = SharedLSTM(config)\n        self._model = shared_lstm\n        optim = torch.optim.Adam(self._model.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            self._model.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                self._model.zero_grad()\n                left_text = item.texta\n                right_text = item.textb\n                predict_dis = self._model(left_text, right_text)\n                item_loss = F.cross_entropy(predict_dis, item.label)\n                acc_loss += item_loss.item()\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'te_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'te_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        self._model.save()\n\n    def predict(self, texta: str, textb: str):\n        self._model.eval()\n        pad_texta = pad_sequnce([x for x in texta], DEFAULT_CONFIG[\'fix_length\'])\n        vec_texta = torch.tensor([self._word_vocab.stoi[x] for x in pad_texta])\n        pad_textb = pad_sequnce([x for x in textb], DEFAULT_CONFIG[\'fix_length\'])\n        vec_textb = torch.tensor([self._word_vocab.stoi[x] for x in pad_textb])\n        vec_predict = self._model(vec_texta.view(-1, 1).to(DEVICE),\n                                  vec_textb.view(-1, 1).to(DEVICE))[0]\n        soft_predict = torch.softmax(vec_predict, dim=0)\n        predict_prob, predict_index = torch.max(soft_predict.cpu().data, dim=0)\n        predict_class = self._label_vocab.itos[predict_index]\n        predict_prob = predict_prob.item()\n        return predict_prob, predict_class\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        shared_lstm = SharedLSTM(config)\n        shared_lstm.load()\n        self._model = shared_lstm\n        self._word_vocab = config.word_vocab\n        self._label_vocab = config.label_vocab\n\n    def test(self, test_path):\n        test_dataset = te_tool.get_dataset(test_path)\n        if not hasattr(TEXT, \'vocab\'):\n            TEXT.vocab = self._word_vocab\n        if not hasattr(LABEL, \'vocab\'):\n            LABEL.vocab = self._label_vocab\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n\n    def _validate(self, dev_dataset):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = te_tool.get_iterator(dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        for dev_item in tqdm(dev_iter):\n            item_score = te_tool.get_score(self._model, dev_item.texta, dev_item.textb, dev_item.label)\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def deploy(self, route_path=""/te"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/predict"", methods=[\'POST\', \'GET\'])\n        def predict():\n            texta = request.args.get(\'texta\', \'\')\n            textb = request.args.get(\'textb\', \'\')\n            result = self.predict(texta, textb)\n            return flask.jsonify({\n                \'state\': \'OK\',\n                \'result\': {\n                    \'prob\': result[0],\n                    \'class\': result[1]\n                }\n            })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/sr/te/tool.py,4,"b""import re\nimport torch\nfrom torchtext.data import TabularDataset, Field, Iterator, Dataset, BucketIterator\nfrom torchtext.vocab import Vectors\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\nregex = re.compile(r'[^\\u4e00-\\u9fa5aA-Za-z0-9\xef\xbc\x8c\xe3\x80\x82\xef\xbc\x9f\xef\xbc\x9a\xef\xbc\x81\xef\xbc\x9b\xe2\x80\x9c\xe2\x80\x9d]')\n\n\ndef light_tokenize(text):\n    text = regex.sub(' ', text)\n    return [word for word in text if word.strip()]\n\n\nTEXT = Field(lower=True, tokenize=light_tokenize, fix_length=DEFAULT_CONFIG['fix_length'])\nLABEL = Field(sequential=False, unk_token=None)\nFields = [\n            ('texta', TEXT),\n            ('textb', TEXT),\n            ('label', LABEL)\n        ]\n\n\nclass TETool(Tool):\n    def get_dataset(self, path: str, fields=Fields, file_type='tsv', skip_header=True):\n        logger.info('loading dataset from {}'.format(path))\n        te_dataset = TabularDataset(path, format=file_type, fields=fields, skip_header=skip_header)\n        logger.info('successed loading dataset')\n        return te_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        TEXT.build_vocab(*dataset)\n        logger.info('successed building word vocab')\n        logger.info('building label vocab...')\n        LABEL.build_vocab(*dataset)\n        logger.info('successed building label vocab')\n        return TEXT.vocab, LABEL.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('successed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset: Dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE,\n                 sort_key=lambda x: len(x.texta)):\n        return BucketIterator(dataset, batch_size=batch_size, device=device, sort_key=sort_key)\n\n    def get_score(self, model, texta, textb, labels, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        assert texta.size(1) == textb.size(1) == len(labels)\n        vec_predict = model(texta, textb)\n        soft_predict = torch.softmax(vec_predict, dim=1)\n        predict_prob, predict_index = torch.max(soft_predict.cpu().data, dim=1)\n        # print('prob', predict_prob)\n        # print('index', predict_index)\n        # print('labels', labels)\n        labels = labels.view(-1).cpu().data.numpy()\n        return metric_func(predict_index, labels, average='micro')\n\n\nte_tool = TETool()"""
lightnlp/tc/re/__init__.py,0,b''
lightnlp/tc/re/config.py,0,"b""from ...base.config import DEVICE\nDEFAULT_CONFIG = {\n    'lr': 0.002,\n    'epoch': 5,\n    'lr_decay': 0.05,\n    'batch_size': 128,\n    'dropout': 0.5,\n    'static': False,\n    'non_static': False,\n    'embedding_dim': 300,\n    'vector_path': '',\n    'class_num': 0,\n    'vocabulary_size': 0,\n    'word_vocab': None,\n    'tag_vocab': None,\n    'save_path': './re_saves',\n    'filter_num': 100,\n    'filter_sizes': (3, 4, 5),\n    'multichannel': False\n}"""
lightnlp/tc/re/model.py,6,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchtext.vocab import Vectors\n\nfrom ...utils.log import logger\nfrom ...base.model import BaseConfig, BaseModel\n\nfrom .config import DEVICE, DEFAULT_CONFIG\n\n\nclass Config(BaseConfig):\n    def __init__(self, word_vocab, label_vocab, vector_path, **kwargs):\n        super(Config, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.label_vocab = label_vocab\n        self.class_num = len(self.label_vocab)\n        self.vocabulary_size = len(self.word_vocab)\n        self.vector_path = vector_path\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass TextCNN(BaseModel):\n    def __init__(self, args):\n        super(TextCNN, self).__init__(args)\n\n        self.class_num = args.class_num\n        self.chanel_num = 1\n        self.filter_num = args.filter_num\n        self.filter_sizes = args.filter_sizes\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n        self.embedding = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n        if args.static:\n            logger.info('logging word vectors from {}'.format(args.vector_path))\n            vectors = Vectors(args.vector_path).vectors\n            self.embedding = self.embedding.from_pretrained(vectors, freeze=not args.non_static).to(DEVICE)\n        if args.multichannel:\n            self.embedding2 = nn.Embedding(self.vocabulary_size, self.embedding_dimension).from_pretrained(\n                args.vectors).to(DEVICE)\n            self.chanel_num += 1\n        else:\n            self.embedding2 = None\n        self.convs = nn.ModuleList(\n            [nn.Conv2d(self.chanel_num, self.filter_num, (size, self.embedding_dimension)) for size in\n             self.filter_sizes]).to(DEVICE)\n        self.dropout = nn.Dropout(args.dropout).to(DEVICE)\n        self.fc = nn.Linear(len(self.filter_sizes) * self.filter_num, self.class_num).to(DEVICE)\n\n    def forward(self, x):\n        if self.embedding2:\n            x = torch.stack((self.embedding(x), self.embedding2(x)), dim=1).to(DEVICE)\n        else:\n            x = self.embedding(x).to(DEVICE)\n            x = x.unsqueeze(1)\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n        x = [F.max_pool1d(item, int(item.size(2))).squeeze(2) for item in x]\n        x = torch.cat(x, 1)\n        x = self.dropout(x)\n        logits = self.fc(x)\n        return logits\n\n\nclass LSTMClassifier(BaseModel):\n    def __init__(self, args):\n        super(LSTMClassifier, self).__init__(args)\n\n        self.hidden_dim = 300\n        self.class_num = args.class_num\n        self.batch_size = args.batch_size\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n\n        self.embedding = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n        if args.static:\n            self.embedding = self.embedding.from_pretrained(args.vectors, freeze=not args.non_static).to(DEVICE)\n        if args.multichannel:\n            self.embedding2 = nn.Embedding(self.vocabulary_size, self.embedding_dimension).from_pretrained(\n                args.vectors).to(DEVICE)\n        else:\n            self.embedding2 = None\n\n        self.lstm = nn.LSTM(self.embedding_dimension, self.hidden_dim).to(DEVICE)\n        self.hidden2label = nn.Linear(self.hidden_dim, self.class_num).to(DEVICE)\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self, batch_size=None):\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        h0 = torch.zeros(1, batch_size, self.hidden_dim).to(DEVICE)\n        c0 = torch.zeros(1, batch_size, self.hidden_dim).to(DEVICE)\n\n        return h0, c0\n\n    def forward(self, sentence):\n        embeds = self.embedding(sentence).to(DEVICE)\n\n        x = embeds.permute(1, 0, 2).to(DEVICE)\n        self.hidden = self.init_hidden(sentence.size()[0])\n        lstm_out, self.hidden = self.lstm(x, self.hidden)\n        lstm_out = lstm_out.to(DEVICE)\n        final = lstm_out[-1].to(DEVICE)\n        y = self.hidden2label(final)\n        return y\n"""
lightnlp/tc/re/module.py,8,"b'import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ...utils.deploy import get_free_tcp_port\nfrom ...utils.learning import adjust_learning_rate\nfrom ...utils.log import logger\nfrom ...base.module import Module\n\nfrom .model import Config, TextCNN\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom .tool import re_tool, TEXT, LABEL\nfrom .utils.preprocess import handle_line\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass RE(Module):\n    """"""\n    """"""\n\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self._label_vocab = None\n\n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None, **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = re_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = re_tool.get_dataset(dev_path)\n            word_vocab, label_vocab = re_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab, label_vocab = re_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        self._label_vocab = label_vocab\n        train_iter = re_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, label_vocab, save_path=save_path, vector_path=vectors_path, **kwargs)\n        textcnn = TextCNN(config)\n        # print(textcnn)\n        writer.add_graph(textcnn, (next(iter(train_iter))).text)\n        writer.flush()\n        self._model = textcnn\n        optim = torch.optim.Adam(textcnn.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            textcnn.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                optim.zero_grad()\n                logits = self._model(item.text)\n                item_loss = F.cross_entropy(logits, item.label)\n                acc_loss += item_loss.item()\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'re_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'re_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.flush()\n        config.save()\n        textcnn.save()\n\n    def predict(self, entity1: str, entity2: str, sentence: str):\n        self._model.eval()\n        text = handle_line(entity1, entity2, sentence)\n        vec_text = torch.tensor([self._word_vocab.stoi[x] for x in text])\n        vec_text = vec_text.reshape(1, -1).to(DEVICE)\n        vec_predict = self._model(vec_text)[0]\n        soft_predict = torch.softmax(vec_predict, dim=0)\n        predict_prob, predict_index = torch.max(soft_predict.cpu().data, dim=0)\n        predict_class = self._label_vocab.itos[predict_index]\n        predict_prob = predict_prob.item()\n        return predict_prob, predict_class\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        textcnn = TextCNN(config)\n        textcnn.load()\n        self._model = textcnn\n        self._word_vocab = config.word_vocab\n        self._label_vocab = config.label_vocab\n        self._check_vocab()\n\n    def test(self, test_path):\n        self._model.eval()\n        test_dataset = re_tool.get_dataset(test_path)\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n\n    def _validate(self, dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\']):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = re_tool.get_iterator(dev_dataset, batch_size=batch_size)\n        for dev_item in tqdm(dev_iter):\n            item_score = re_tool.get_score(self._model, dev_item.text, dev_item.label)\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def _check_vocab(self):\n        if not hasattr(TEXT, \'vocab\'):\n            TEXT.vocab = self._word_vocab\n        if not hasattr(LABEL, \'vocab\'):\n            LABEL.vocab = self._label_vocab\n\n    def deploy(self, route_path=""/re"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + \'/predict\', methods=[\'POST\', \'GET\'])\n        def predict():\n            entity1 = request.args.get(\'entity1\', \'\')\n            entity2 = request.args.get(\'entity2\', \'\')\n            sentence = request.args.get(\'sentence\', \'\')\n            result = self.predict(entity1, entity2, sentence)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'prob\': result[0],\n                        \'class\': result[1]\n                        }\n                })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/tc/re/tool.py,4,"b""import re\nimport torch\nfrom torchtext.data import Field, Iterator\nfrom torchtext.vocab import Vectors\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nfrom .utils.dataset import REDataset\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\ndef light_tokenize(text):\n    return text\n\n\nTEXT = Field(lower=True, tokenize=light_tokenize, batch_first=True)\nLABEL = Field(sequential=False, unk_token=None)\nFields = [\n            ('text', TEXT),\n            ('label', LABEL)\n        ]\n\n\nclass RETool(Tool):\n    def get_dataset(self, path: str, fields=Fields):\n        logger.info('loading dataset from {}'.format(path))\n        re_dataset = REDataset(path, fields)\n        logger.info('successed loading dataset')\n        return re_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        TEXT.build_vocab(*dataset)\n        logger.info('successed building word vocab')\n        logger.info('building label vocab...')\n        LABEL.build_vocab(*dataset)\n        logger.info('successed building label vocab')\n        return TEXT.vocab, LABEL.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('successed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE,\n                     sort_key=lambda x: len(x.text)):\n        return Iterator(dataset, batch_size=batch_size, device=device, sort_key=sort_key)\n\n    def get_score(self, model, texts, labels, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        assert texts.size(0) == len(labels)\n        vec_predict = model(texts)\n        soft_predict = torch.softmax(vec_predict, dim=1)\n        predict_prob, predict_index = torch.max(soft_predict.cpu().data, dim=1)\n        # print('prob', predict_prob)\n        # print('index', predict_index)\n        # print('labels', labels)\n        labels = labels.view(-1).cpu().data.numpy()\n        return metric_func(predict_index, labels, average='micro')\n\n\nre_tool = RETool()\n"""
lightnlp/tc/sa/__init__.py,0,b''
lightnlp/tc/sa/config.py,0,"b""from ...base.config import DEVICE\nDEFAULT_CONFIG = {\n    'lr': 0.02,\n    'epoch': 10,\n    'lr_decay': 0.05,\n    'batch_size': 128,\n    'dropout': 0.5,\n    'static': False,\n    'non_static': False,\n    'embedding_dim': 300,\n    'vector_path': '',\n    'class_num': 0,\n    'vocabulary_size': 0,\n    'word_vocab': None,\n    'tag_vocab': None,\n    'save_path': './sa_saves',\n    'filter_num': 100,\n    'filter_sizes': (3, 4, 5),\n    'multichannel': False\n}"""
lightnlp/tc/sa/model.py,6,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchtext.vocab import Vectors\n\nfrom ...utils.log import logger\nfrom ...base.model import BaseConfig, BaseModel\n\nfrom .config import DEVICE, DEFAULT_CONFIG\n\n\nclass Config(BaseConfig):\n    def __init__(self, word_vocab, label_vocab, vector_path, **kwargs):\n        super(Config, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.label_vocab = label_vocab\n        self.class_num = len(self.label_vocab)\n        self.vocabulary_size = len(self.word_vocab)\n        self.vector_path = vector_path\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass TextCNN(BaseModel):\n    def __init__(self, args):\n        super(TextCNN, self).__init__(args)\n\n        self.class_num = args.class_num\n        self.chanel_num = 1\n        self.filter_num = args.filter_num\n        self.filter_sizes = args.filter_sizes\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n        self.embedding = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n        if args.static:\n            logger.info('logging word vectors from {}'.format(args.vector_path))\n            vectors = Vectors(args.vector_path).vectors\n            self.embedding = self.embedding.from_pretrained(vectors, freeze=not args.non_static).to(DEVICE)\n        if args.multichannel:\n            self.embedding2 = nn.Embedding(self.vocabulary_size, self.embedding_dimension).from_pretrained(args.vectors).to(DEVICE)\n            self.chanel_num += 1\n        else:\n            self.embedding2 = None\n        self.convs = nn.ModuleList(\n            [nn.Conv2d(self.chanel_num, self.filter_num, (size, self.embedding_dimension)) for size in self.filter_sizes]).to(DEVICE)\n        self.dropout = nn.Dropout(args.dropout).to(DEVICE)\n        self.fc = nn.Linear(len(self.filter_sizes) * self.filter_num, self.class_num).to(DEVICE)\n\n    def forward(self, x):\n        if self.embedding2:\n            x = torch.stack((self.embedding(x), self.embedding2(x)), dim=1).to(DEVICE)\n        else:\n            x = self.embedding(x).to(DEVICE)\n            x = x.unsqueeze(1)\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n        x = [F.max_pool1d(item, int(item.size(2))).squeeze(2) for item in x]\n        x = torch.cat(x, 1)\n        x = self.dropout(x)\n        logits = self.fc(x)\n        return logits\n\n\nclass LSTMClassifier(BaseModel):\n    def __init__(self, args):\n        super(LSTMClassifier, self).__init__(args)\n\n        self.hidden_dim = 300\n        self.class_num = args.class_num\n        self.batch_size = args.batch_size\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n        \n        self.embedding = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n        if args.static:\n            self.embedding = self.embedding.from_pretrained(args.vectors, freeze=not args.non_static).to(DEVICE)\n        if args.multichannel:\n            self.embedding2 = nn.Embedding(self.vocabulary_size, self.embedding_dimension).from_pretrained(args.vectors).to(DEVICE)\n        else:\n            self.embedding2 = None\n\n        self.lstm = nn.LSTM(self.embedding_dimension, self.hidden_dim).to(DEVICE)\n        self.hidden2label = nn.Linear(self.hidden_dim, self.class_num).to(DEVICE)\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self, batch_size=None):\n        if batch_size is None:\n            batch_size = self.batch_size\n        \n        h0 = torch.zeros(1, batch_size, self.hidden_dim).to(DEVICE)\n        c0 = torch.zeros(1, batch_size, self.hidden_dim).to(DEVICE)\n\n        return h0, c0\n    \n    def forward(self, sentence):\n        embeds = self.embedding(sentence).to(DEVICE)\n\n        x = embeds.permute(1, 0, 2).to(DEVICE)\n        self.hidden = self.init_hidden(sentence.size()[0])\n        lstm_out, self.hidden = self.lstm(x, self.hidden)\n        lstm_out = lstm_out.to(DEVICE)\n        final = lstm_out[-1].to(DEVICE)\n        y = self.hidden2label(final)\n        return y\n"""
lightnlp/tc/sa/module.py,10,"b'import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ...utils.deploy import get_free_tcp_port\nfrom ...utils.learning import adjust_learning_rate\nfrom ...utils.log import logger\nfrom ...base.module import Module\n\nfrom .model import Config, TextCNN\nfrom .utils.pad import pad_sequnce\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom .tool import sa_tool, light_tokenize, TEXT, LABEL\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass SA(Module):\n    """"""\n    """"""\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self._label_vocab = None\n    \n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = sa_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = sa_tool.get_dataset(dev_path)\n            word_vocab, label_vocab = sa_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab, label_vocab = sa_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        self._label_vocab = label_vocab\n        train_iter = sa_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, label_vocab, save_path=save_path, vector_path=vectors_path, **kwargs)\n        textcnn = TextCNN(config)\n        # temp = torch.randint(1, 100, (128, 32)).to(DEVICE)\n        # writer.add_graph(textcnn, (torch.randint(1, 100, (128, 32)).to(DEVICE)))\n        writer.add_graph(textcnn, (next(iter(train_iter))).text)\n        writer.flush()\n        # print(textcnn)\n        self._model = textcnn\n        optim = torch.optim.Adam(textcnn.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            textcnn.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                optim.zero_grad()\n                logits = self._model(item.text)\n                item_loss = F.cross_entropy(logits, item.label)\n                acc_loss += item_loss.item()\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'sa_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'sa_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        textcnn.save()\n\n    def predict(self, text):\n        self._model.eval()\n        text_list = light_tokenize(text)\n        text = pad_sequnce(text_list, 5)\n        vec_text = torch.tensor([self._word_vocab.stoi[x] for x in text])\n        vec_text = vec_text.reshape(1, -1).to(DEVICE)\n        vec_predict = self._model(vec_text)[0]\n        soft_predict = torch.softmax(vec_predict, dim=0)\n        predict_prob, predict_index = torch.max(soft_predict.cpu().data, dim=0)\n        predict_class = self._label_vocab.itos[predict_index]\n        predict_prob = predict_prob.item()\n        return predict_prob, predict_class\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        textcnn = TextCNN(config)\n        textcnn.load()\n        self._model = textcnn\n        self._word_vocab = config.word_vocab\n        self._label_vocab = config.label_vocab\n        self._check_vocab()\n    \n    def test(self, test_path):\n        test_dataset = sa_tool.get_dataset(test_path)\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n    \n    def _validate(self, dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\']):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = sa_tool.get_iterator(dev_dataset, batch_size=batch_size)\n        for dev_item in tqdm(dev_iter):\n            item_score = sa_tool.get_score(self._model, dev_item.text, dev_item.label)\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def _check_vocab(self):\n        if not hasattr(TEXT, \'vocab\'):\n            TEXT.vocab = self._word_vocab\n        if not hasattr(LABEL, \'vocab\'):\n            LABEL.vocab = self._label_vocab\n\n    def deploy(self, route_path=""/sa"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + \'/predict\', methods=[\'POST\', \'GET\'])\n        def predict():\n            text = request.args.get(\'text\', \'\')\n            result = self.predict(text)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'prob\': result[0],\n                        \'class\': result[1]\n                        }\n                })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/tc/sa/tool.py,4,"b""import re\nimport torch\nfrom torchtext.data import TabularDataset, Field, Iterator\nfrom torchtext.vocab import Vectors\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\nimport jieba\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\nregex = re.compile(r'[^\\u4e00-\\u9fa5aA-Za-z0-9\xef\xbc\x8c\xe3\x80\x82\xef\xbc\x9f\xef\xbc\x9a\xef\xbc\x81\xef\xbc\x9b\xe2\x80\x9c\xe2\x80\x9d]')\n\n\ndef light_tokenize(text):\n    text = regex.sub(' ', text)\n    return [word for word in jieba.cut(text) if word.strip()]\n\n\nTEXT = Field(lower=True, tokenize=light_tokenize, batch_first=True)\nLABEL = Field(sequential=False, unk_token=None)\nFields = [\n            ('index', None),\n            ('label', LABEL),\n            ('text', TEXT)\n        ]\n\n\nclass SATool(Tool):\n    def get_dataset(self, path: str, fields=Fields, file_type='tsv', skip_header=True):\n        logger.info('loading dataset from {}'.format(path))\n        st_dataset = TabularDataset(path, format=file_type, fields=fields, skip_header=skip_header)\n        logger.info('successed loading dataset')\n        return st_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        TEXT.build_vocab(*dataset)\n        logger.info('successed building word vocab')\n        logger.info('building label vocab...')\n        LABEL.build_vocab(*dataset)\n        logger.info('successed building label vocab')\n        return TEXT.vocab, LABEL.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('successed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE,\n                     sort_key=lambda x: len(x.text)):\n        return Iterator(dataset, batch_size=batch_size, device=device, sort_key=sort_key)\n\n    def get_score(self, model, texts, labels, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        assert len(texts) == len(labels)\n        vec_predict = model(texts)\n        soft_predict = torch.softmax(vec_predict, dim=1)\n        predict_prob, predict_index = torch.max(soft_predict.cpu().data, dim=1)\n        # print('prob', predict_prob)\n        # print('index', predict_index)\n        # print('labels', labels)\n        labels = labels.view(-1).cpu().data.numpy()\n        return metric_func(predict_index, labels, average='micro')\n\n\nsa_tool = SATool()\n"""
lightnlp/tg/cb/__init__.py,0,b'# chat bot\xef\xbc\x8c\xe8\x81\x8a\xe5\xa4\xa9\xe6\x9c\xba\xe5\x99\xa8\xe4\xba\xba\n'
lightnlp/tg/cb/config.py,0,"b""from ...base.config import DEVICE\nDEFAULT_CONFIG = {\n    'lr': 0.02,\n    'epoch': 500,\n    'lr_decay': 0.05,\n    'batch_size': 128,\n    'dropout': 0.5,\n    'static': False,\n    'non_static': False,\n    'embedding_dim': 100,\n    'clip': 10,\n    'num_layers': 2,\n    'vector_path': '',\n    'vocabulary_size': 0,\n    'word_vocab': None,\n    'save_path': './cb_saves',\n    'method': 'dot',\n    'teacher_forcing_ratio': 0.5\n}\n"""
lightnlp/tg/cb/model.py,0,"b'from .config import DEVICE, DEFAULT_CONFIG\nfrom .models.encoder import Encoder\nfrom .models.decoder import Decoder\nfrom .models.seq2seq import Seq2Seq\nfrom ...base.model import BaseConfig, BaseModel\n\n\nclass CBConfig(BaseConfig):\n    def __init__(self, word_vocab, vector_path, **kwargs):\n        super(CBConfig, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.vocabulary_size = len(self.word_vocab)\n        self.vector_path = vector_path\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass CBSeq2Seq(BaseModel):\n    def __init__(self, args):\n        super(CBSeq2Seq, self).__init__(args)\n        self.args = args\n        self.hidden_dim = args.embedding_dim\n        self.vocabulary_size = args.vocabulary_size\n        self.batch_size = args.batch_size\n        self.save_path = args.save_path\n        self.num_layers = args.num_layers\n        self.dropout = args.dropout\n        self.teacher_forcing_ratio = args.teacher_forcing_ratio\n\n        vocabulary_size = args.vocabulary_size\n        embedding_dimension = args.embedding_dim\n\n        encoder = Encoder(vocabulary_size, embedding_dimension, self.hidden_dim, self.num_layers,\n                          self.dropout).to(DEVICE)\n        decoder = Decoder(self.hidden_dim, embedding_dimension, vocabulary_size, self.num_layers, self.dropout,\n                          args.method).to(DEVICE)\n        self.seq2seq = Seq2Seq(encoder, decoder).to(DEVICE)\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        return self.seq2seq(src, trg, teacher_forcing_ratio)\n\n    def predict(self, src, src_lens, sos, max_len):\n        return self.seq2seq.predict(src, src_lens, sos, max_len)\n'"
lightnlp/tg/cb/module.py,9,"b'import torch\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ...utils.deploy import get_free_tcp_port\nfrom ...utils.learning import adjust_learning_rate\nfrom ...utils.log import logger\nfrom ...base.module import Module\n\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom .model import CBConfig, CBSeq2Seq\nfrom .tool import cb_tool, light_tokenize, TEXT\n\n\nclass CB(Module):\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n\n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = cb_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = cb_tool.get_dataset(dev_path)\n            word_vocab = cb_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab = cb_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        train_iter = cb_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = CBConfig(word_vocab, save_path=save_path, vector_path=vectors_path, **kwargs)\n        cbseq2seq = CBSeq2Seq(config)\n\n        self._model = cbseq2seq\n        optim = torch.optim.Adam(cbseq2seq.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            cbseq2seq.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                optim.zero_grad()\n                src, src_lens = item.query\n                trg, trg_lens = item.answer\n                output = cbseq2seq(src, src_lens, trg)\n                output = output[1:].contiguous()\n                output = output.view(-1, output.shape[-1])\n                trg = trg.transpose(1, 0)\n                trg = trg[1:].contiguous()\n                trg = trg.view(-1)\n                item_loss = F.cross_entropy(output, trg, ignore_index=TEXT.vocab.stoi[\'<pad>\'])\n                acc_loss += item_loss.item()\n                item_loss.backward()\n                clip_grad_norm_(cbseq2seq.parameters(), config.clip)\n                optim.step()\n            print(\'epoch:{}, acc_loss:{}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'cb_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'cb_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        cbseq2seq.save()\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = CBConfig.load(save_path)\n        cbseq2seq = CBSeq2Seq(config)\n        cbseq2seq .load()\n        self._model = cbseq2seq\n        self._word_vocab = config.word_vocab\n\n    def test(self, test_path):\n        test_dataset = cb_tool.get_dataset(test_path)\n        if not hasattr(TEXT, \'vocab\'):\n            TEXT.vocab = self._word_vocab\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n\n    def _validate(self, dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\']):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = cb_tool.get_iterator(dev_dataset, batch_size=batch_size)\n        for dev_item in tqdm(dev_iter):\n            src, src_lens = dev_item.query\n            trg, trg_lens = dev_item.answer\n            item_score = cb_tool.get_score(self._model, src, src_lens, trg)\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def predict(self, text: str, max_len=10):\n        self._model.eval()\n        text_list = light_tokenize(text)\n        vec_text = torch.tensor([self._word_vocab.stoi[x] for x in text_list])\n        vec_text = vec_text.reshape(1, -1).to(DEVICE)\n        len_text = torch.tensor([len(vec_text)]).to(DEVICE)\n        sos = torch.tensor([self._word_vocab.stoi[\'<sos>\']]).to(DEVICE)\n        output = self._model.predict(vec_text, len_text, sos, max_len).squeeze(1)\n        soft_predict = torch.softmax(output, dim=1)\n        predict_prob, predict_index = torch.max(soft_predict.cpu().data, dim=1)\n        predict_sentence = [self._word_vocab.itos[x] for x in predict_index]\n        predict_prob = predict_prob.cpu().data.tolist()\n        result_sentence = []\n        result_score = 1.0\n        for i in range(1, len(predict_sentence)):\n            if predict_sentence[i] != \'<eos>\':\n                result_sentence.append(predict_sentence[i])\n                result_score *= predict_prob[i]\n            else:\n                break\n        return \'\'.join(result_sentence), result_score\n\n    def deploy(self, route_path=""/cb"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/predict"", methods=[\'POST\', \'GET\'])\n        def predict():\n            text = request.args.get(\'text\', \'\')\n            max_len = int(request.args.get(\'max_len\', 10))\n            result = self.predict(text, max_len)\n            return flask.jsonify({\n                \'state\': \'OK\',\n                \'result\': {\n                    \'result\': result[0],\n                    \'score\': result[1]\n                }\n            })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/tg/cb/tool.py,4,"b""import re\nimport torch\nfrom torchtext.data import TabularDataset, Field, Dataset, BucketIterator\nfrom torchtext.vocab import Vectors\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\nimport jieba\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\nregex = re.compile(r'[^\\u4e00-\\u9fa5aA-Za-z0-9\xef\xbc\x8c\xe3\x80\x82\xef\xbc\x9f\xef\xbc\x9a\xef\xbc\x81\xef\xbc\x9b\xe2\x80\x9c\xe2\x80\x9d]')\n\n\ndef light_tokenize(text):\n    # text = regex.sub(' ', text)\n    return [word for word in jieba.cut(text) if word.strip()]\n\n\nTEXT = Field(lower=True, tokenize=light_tokenize, include_lengths=True, batch_first=True, init_token='<sos>',\n             eos_token='<eos>')\nFields = [\n            ('query', TEXT),\n            ('answer', TEXT)\n        ]\n\n\nclass CBTool(Tool):\n    def get_dataset(self, path: str, fields=Fields, file_type='tsv', skip_header=False):\n        logger.info('loading dataset from {}'.format(path))\n        cb_dataset = TabularDataset(path, format=file_type, fields=fields, skip_header=skip_header)\n        logger.info('successed loading dataset')\n        return cb_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        TEXT.build_vocab(*dataset)\n        logger.info('successed building word vocab')\n        return TEXT.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('successed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset: Dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE,\n                     sort_key=lambda x: len(x.query), sort_within_batch=True):\n        return BucketIterator(dataset, batch_size=batch_size, device=device, sort_key=sort_key,\n                              sort_within_batch=sort_within_batch)\n\n    def get_score(self, model, src, src_lens, trg, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        output = model(src, src_lens, trg)\n        output = output[1:].contiguous()\n        output = output.view(-1, output.shape[-1])\n        trg = trg.transpose(1, 0)\n        trg = trg[1:].contiguous()\n        trg = trg.view(-1)\n        soft_predict = torch.softmax(output, dim=1)\n        predict_prob, predict_index = torch.max(soft_predict.cpu().data, dim=1)\n        labels = trg.cpu().data.numpy()\n        return metric_func(predict_index, labels, average='micro')\n\n\ncb_tool = CBTool()\n"""
lightnlp/tg/lm/__init__.py,0,b'# language model\xef\xbc\x8c\xe8\xaf\xad\xe8\xa8\x80\xe6\xa8\xa1\xe5\x9e\x8b\n'
lightnlp/tg/lm/config.py,0,"b""from ...base.config import DEVICE\nDEFAULT_CONFIG = {\n    'lr': 0.01,\n    'epoch': 10,\n    'lr_decay': 0.05,\n    'batch_size': 128,\n    'bptt_len': 16,\n    'dropout': 0.0,\n    'static': False,\n    'non_static': False,\n    'embedding_dim': 300,\n    'num_layers': 1,\n    'pad_index': 1,\n    'vector_path': '',\n    'tag_num': 0,\n    'vocabulary_size': 0,\n    'word_vocab': None,\n    'tag_vocab': None,\n    'save_path': './lm_saves'\n}"""
lightnlp/tg/lm/model.py,3,"b""import torch\nimport torch.nn as nn\nfrom torchtext.vocab import Vectors\n\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom ...base.model import BaseConfig, BaseModel\n\n\nclass LMConfig(BaseConfig):\n    def __init__(self, word_vocab, vector_path, **kwargs):\n        super(LMConfig, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.vocabulary_size = len(self.word_vocab)\n        self.vector_path = vector_path\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass RNNLM(BaseModel):\n    def __init__(self, args):\n        super(RNNLM, self).__init__(args)\n        self.args = args\n        self.hidden_dim = args.embedding_dim\n        self.vocabulary_size = args.vocabulary_size\n        self.batch_size = args.batch_size\n        self.save_path = args.save_path\n        self.num_layers = args.num_layers\n        self.dropout = args.dropout\n\n        vocabulary_size = args.vocabulary_size\n        embedding_dimension = args.embedding_dim\n\n        self.embedding = nn.Embedding(vocabulary_size, embedding_dimension).to(DEVICE)\n        if args.static:\n            logger.info('logging word vectors from {}'.format(args.vector_path))\n            vectors = Vectors(args.vector_path).vectors\n            self.embedding = self.embedding.from_pretrained(vectors, freeze=not args.non_static).to(DEVICE)\n\n        self.lstm = nn.LSTM(embedding_dimension, self.hidden_dim,\n                            num_layers=self.num_layers, dropout=self.dropout).to(DEVICE)\n        # self.dropout = nn.Dropout(args.dropout)\n        self.bath_norm = nn.BatchNorm1d(embedding_dimension).to(DEVICE)\n        # self.bath_norm2 = nn.BatchNorm1d(vocabulary_size).to(DEVICE)\n        self.hidden2label = nn.Linear(self.hidden_dim, self.vocabulary_size).to(DEVICE)\n\n        # self.init_weight()\n\n    def init_weight(self):\n        nn.init.xavier_normal_(self.embedding.weight)\n        for name, param in self.lstm.named_parameters():\n            if 'weight' in name:\n                nn.init.xavier_normal_(param)\n                nn.init.xavier_normal_(self.hidden2label.weight)\n\n    def init_hidden(self, batch_size=None):\n        if batch_size is None:\n            batch_size = self.batch_size\n\n        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(DEVICE)\n        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(DEVICE)\n\n        return h0, c0\n    \n    def forward(self, sentence):\n        x = self.embedding(sentence.to(DEVICE)).to(DEVICE)\n        self.hidden = self.init_hidden(batch_size=sentence.size(1))\n        lstm_out, self.hidden = self.lstm(x, self.hidden)\n        lstm_out = lstm_out.view(-1, lstm_out.size(2))\n        lstm_out = self.bath_norm(lstm_out)\n        y = self.hidden2label(lstm_out.to(DEVICE))\n        return y.to(DEVICE)\n"""
lightnlp/tg/lm/module.py,13,"b'import torch\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ...utils.deploy import get_free_tcp_port\nfrom ...utils.learning import adjust_learning_rate\nfrom ...utils.log import logger\nfrom ...base.module import Module\n\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom .model import LMConfig, RNNLM\nfrom .tool import lm_tool, light_tokenize, TEXT\n\n\nclass LM(Module):\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n\n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = lm_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = lm_tool.get_dataset(dev_path)\n            word_vocab = lm_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab = lm_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        train_iter = lm_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'], bptt_len=DEFAULT_CONFIG[\'bptt_len\'])\n        config = LMConfig(word_vocab, save_path=save_path, vector_path=vectors_path, **kwargs)\n        rnnlm = RNNLM(config)\n        self._model = rnnlm\n        optim = torch.optim.Adam(rnnlm.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            rnnlm.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                optim.zero_grad()\n                logits = rnnlm(item.text)\n                item_loss = F.cross_entropy(logits, item.target.view(-1))\n                acc_loss += item_loss.item()\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'lm_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'lm_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        rnnlm.save()\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = LMConfig.load(save_path)\n        rnnlm = RNNLM(config)\n        rnnlm .load()\n        self._model = rnnlm\n        self._word_vocab = config.word_vocab\n\n    def test(self, test_path):\n        test_dataset = lm_tool.get_dataset(test_path)\n        if not hasattr(TEXT, \'vocab\'):\n            TEXT.vocab = self._word_vocab\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n\n    def _validate(self, dev_dataset):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = lm_tool.get_iterator(dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'],\n                                  bptt_len=DEFAULT_CONFIG[\'bptt_len\'])\n        for dev_item in tqdm(dev_iter):\n            item_score = lm_tool.get_score(self._model, dev_item.text, dev_item.target)\n            dev_score_list.append(item_score)\n        # print(dev_score_list)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def _predict_next_word_max(self, sentence_list: list):\n        test_item = torch.tensor([[self._word_vocab.stoi[x]] for x in sentence_list], device=DEVICE)\n        pred_prob, pred_index = torch.max(torch.softmax(self._model(test_item)[-1], dim=0).cpu().data, dim=0)\n        pred_word = TEXT.vocab.itos[pred_index]\n        pred_prob = pred_prob.item()\n        return pred_word, pred_prob\n\n    def _predict_next_word_sample(self, sentence_list: list):\n        # \xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe9\x87\x87\xe6\xa0\xb7\xef\xbc\x8c\xe4\xbb\xa5\xe8\x8e\xb7\xe5\xbe\x97\xe9\x9a\x8f\xe6\x9c\xba\xe7\xbb\x93\xe6\x9e\x9c\n        test_item = torch.tensor([[self._word_vocab.stoi[x]] for x in sentence_list], device=DEVICE)\n        pred_index = torch.multinomial(torch.softmax(self._model(test_item)[-1], dim=0).cpu().data, 1)\n        pred_word = self._word_vocab.itos[pred_index]\n        return pred_word\n\n    def _predict_next_word_topk(self, sentence_list: list, topK=5):\n        # \xe8\x8e\xb7\xe5\x8f\x96topK\xe4\xb8\xaanext\xe4\xb8\xaa\xe8\xaf\x8d\xe7\x9a\x84\xe5\x8f\xaf\xe8\x83\xbd\xe5\x8f\x96\xe5\x80\xbc\xe5\x92\x8c\xe5\xaf\xb9\xe5\xba\x94\xe6\xa6\x82\xe7\x8e\x87\n        test_item = torch.tensor([[self._word_vocab.stoi[x]] for x in sentence_list], device=DEVICE)\n        predict_softmax = torch.softmax(self._model(test_item)[-1], dim=0).cpu().data\n        topK_prob, topK_index = torch.topk(predict_softmax, topK)\n        topK_prob = topK_prob.tolist()\n        topK_vocab = [self._word_vocab.itos[x] for x in topK_index]\n        return list(zip(topK_vocab, topK_prob))\n\n    def _predict_next_word_prob(self, sentence_list: list, next_word: str):\n        test_item = torch.tensor([[self._word_vocab.stoi[x]] for x in sentence_list], device=DEVICE)\n        predict_prob = torch.softmax(self._model(test_item)[-1], dim=0).cpu().data\n        next_word_index = self._word_vocab.stoi[next_word]\n        return predict_prob[next_word_index]\n\n    def next_word(self, sentence: str, next_word: str):\n        self._model.eval()\n        temp_str = [x for x in light_tokenize(sentence)]\n        predict_prob = self._predict_next_word_prob(temp_str, next_word)\n        return predict_prob.item()\n\n    def _next_word_score(self, sentence: str, next_word: str):\n        self._model.eval()\n        temp_str = [x for x in light_tokenize(sentence)]\n        predict_prob = self._predict_next_word_prob(temp_str, next_word)\n        return torch.log10(predict_prob).item()\n\n    def next_word_topk(self, sentence: str, topK=5):\n        self._model.eval()\n        return self._predict_next_word_topk(sentence, topK)\n\n    def sentence_score(self, sentence: str):\n        self._model.eval()\n        total_score = 0\n        assert len(sentence) > 1\n        for i in range(1, len(sentence)):\n            temp_score = self._next_word_score(sentence[:i], sentence[i])\n            total_score += temp_score\n        return total_score\n\n    def _predict_sentence(self, sentence: str, gen_len=30):\n        results = []\n        temp_str = [x for x in light_tokenize(sentence)]\n        for i in range(gen_len):\n            temp_result = self._predict_next_word_sample(temp_str)\n            results.append(temp_result)\n            temp_str.append(temp_result)\n        return results\n\n    def generate_sentence(self, sentence: str, gen_len=30):\n        self._model.eval()\n        results = self._predict_sentence(sentence, gen_len)\n        predict_sen = \'\'.join([x for x in results])\n        return sentence + predict_sen\n\n    def deploy(self, route_path=""/lm"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/next_word"", methods=[\'POST\', \'GET\'])\n        def next_word():\n            sentence = request.args.get(\'sentence\', \'\')\n            word = request.args.get(\'word\', \'\')\n            result = self.next_word(sentence, word)\n            return flask.jsonify({\n                \'state\': \'OK\',\n                \'result\': {\n                    \'prob\': result\n                }\n            })\n\n        @app.route(route_path + ""/generate_sentence"", methods=[\'POST\', \'GET\'])\n        def generate_sentence():\n            sentence = request.args.get(\'sentence\', \'\')\n            gen_len = int(request.args.get(\'gen_len\', 30))\n            result = self.generate_sentence(sentence, gen_len)\n            return flask.jsonify({\n                \'state\': \'OK\',\n                \'result\': {\n                    \'sentence\': result\n                }\n            })\n\n        @app.route(route_path + ""/next_word_topk"", methods=[\'POST\', \'GET\'])\n        def next_word_topk():\n            sentence = request.args.get(\'sentence\', \'\')\n            topk = int(request.args.get(\'topk\', 5))\n            result = self.next_word_topk(sentence, topK=topk)\n            return flask.jsonify({\n                \'state\': \'OK\',\n                \'result\': {\n                    \'words\': result\n                }\n            })\n\n        @app.route(route_path + ""/sentence_score"", methods=[\'POST\', \'GET\'])\n        def sentence_score():\n            sentence = request.args.get(\'sentence\', \'\')\n            result = self.sentence_score(sentence)\n            return flask.jsonify({\n                \'state\': \'OK\',\n                \'result\': {\n                    \'score\': result\n                }\n            })\n\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/tg/lm/tool.py,3,"b""import torch\nfrom torchtext.data import ReversibleField, BPTTIterator, Dataset\nfrom torchtext.datasets import LanguageModelingDataset\nfrom torchtext.vocab import Vectors\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\ndef light_tokenize(sequence: str):\n    return [x for x in sequence.strip()]\n\n\nTEXT = ReversibleField(sequential=True, tokenize=light_tokenize)\n\n\nclass LMTool(Tool):\n    def tokenize(self, sequence: str):\n        return [x for x in sequence.strip()]\n\n    def get_dataset(self, path: str, field=TEXT, newline_eos=False):\n        logger.info('loading dataset from {}'.format(path))\n        lm_dataset = LanguageModelingDataset(path, text_field=field, newline_eos=newline_eos)\n        logger.info('successed loading dataset')\n        return lm_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        TEXT.build_vocab(*dataset)\n        logger.info('successed building word vocab')\n        return TEXT.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('successed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset: Dataset, batch_size=DEFAULT_CONFIG['batch_size'],\n                     bptt_len=DEFAULT_CONFIG['bptt_len'], device=DEVICE):\n        return BPTTIterator(dataset, batch_size=batch_size,\n                            bptt_len=bptt_len, device=device)\n\n    def get_score(self, model, x, y, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        vec_x = x\n        predict_y = model(vec_x.view(-1, 1).to(DEVICE))\n        predict_index = torch.max(torch.softmax(predict_y, dim=1).cpu().data, dim=1)[1]\n        predict_index = predict_index.data.numpy()\n        true_y = y.view(-1).cpu().data.numpy()\n        assert len(true_y) == len(predict_index)\n        return metric_func(predict_index, true_y, average='micro')\n\n\nlm_tool = LMTool()\n"""
lightnlp/tg/mt/__init__.py,0,b'# machine translation\xef\xbc\x8c\xe6\x9c\xba\xe5\x99\xa8\xe7\xbf\xbb\xe8\xaf\x91\n'
lightnlp/tg/mt/config.py,0,"b""from ...base.config import DEVICE\nDEFAULT_CONFIG = {\n    'lr': 0.02,\n    'epoch': 300,\n    'lr_decay': 0.05,\n    'batch_size': 4,\n    'dropout': 0.5,\n    'static': False,\n    'non_static': False,\n    'source_embedding_dim': 100,\n    'target_embedding_dim': 100,\n    'hidden_dim': 100,\n    'clip': 10,\n    'num_layers': 2,\n    'source_vector_path': '',\n    'target_vector_path': '',\n    'source_vocabulary_size': 0,\n    'target_vocabulary_size': 0,\n    'source_word_vocab': None,\n    'target_word_vocab': None,\n    'save_path': './mt_saves',\n    'method': 'dot',\n    'teacher_forcing_ratio': 0.5\n}\n"""
lightnlp/tg/mt/model.py,0,"b'from .config import DEVICE, DEFAULT_CONFIG\nfrom .models.encoder import Encoder\nfrom .models.decoder import Decoder\nfrom .models.seq2seq import Seq2Seq\nfrom ...base.model import BaseConfig, BaseModel\n\n\nclass MTConfig(BaseConfig):\n    def __init__(self, source_word_vocab, target_word_vocab, source_vector_path, target_vector_path, **kwargs):\n        super(MTConfig, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.source_word_vocab = source_word_vocab\n        self.source_vocabulary_size = len(self.source_word_vocab)\n        self.source_vector_path = source_vector_path\n        self.target_word_vocab = target_word_vocab\n        self.target_vocabulary_size = len(self.target_word_vocab)\n        self.target_vector_path = target_vector_path\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass MTSeq2Seq(BaseModel):\n    def __init__(self, args):\n        super(MTSeq2Seq, self).__init__(args)\n        self.args = args\n        self.hidden_dim = args.hidden_dim\n        self.source_embedding_dim = args.source_embedding_dim\n        self.target_embedding_dim = args.target_embedding_dim\n        self.source_vector_path = args.source_vector_path\n        self.target_vector_path = args.target_vector_path\n        self.source_vocabulary_size = args.source_vocabulary_size\n        self.target_vocabulary_size = args.target_vocabulary_size\n        self.batch_size = args.batch_size\n        self.save_path = args.save_path\n        self.num_layers = args.num_layers\n        self.dropout = args.dropout\n        self.teacher_forcing_ratio = args.teacher_forcing_ratio\n\n        encoder = Encoder(self.source_vocabulary_size, self.source_embedding_dim, self.hidden_dim,\n                          self.num_layers, self.dropout).to(DEVICE)\n        decoder = Decoder(self.hidden_dim, self.target_embedding_dim, self.target_vocabulary_size,\n                          self.num_layers, self.dropout,\n                          args.method).to(DEVICE)\n        self.seq2seq = Seq2Seq(encoder, decoder).to(DEVICE)\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        return self.seq2seq(src, trg, teacher_forcing_ratio)\n\n    def predict(self, src, src_lens, sos, max_len):\n        return self.seq2seq.predict(src, src_lens, sos, max_len)\n'"
lightnlp/tg/mt/module.py,9,"b'import torch\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ...utils.deploy import get_free_tcp_port\nfrom ...utils.learning import adjust_learning_rate\nfrom ...utils.log import logger\nfrom ...base.module import Module\n\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom .model import MTConfig, MTSeq2Seq\nfrom .tool import mt_tool, eng_tokenize, SOURCE, TARGET\n\n\nclass MT(Module):\n    def __init__(self):\n        self._model = None\n        self._source_vocab = None\n        self._target_vocab = None\n\n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, source_vectors_path=None,\n              target_vectors_path=None, log_dir=None, **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = mt_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = mt_tool.get_dataset(dev_path)\n            source_vocab, target_vocab = mt_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            source_vocab, target_vocab = mt_tool.get_vocab(train_dataset)\n        self._source_vocab = source_vocab\n        self._target_vocab = target_vocab\n        train_iter = mt_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = MTConfig(source_vocab, target_vocab, source_vectors_path, target_vectors_path, save_path=save_path,\n                          **kwargs)\n        mtseq2seq = MTSeq2Seq(config)\n        self._model = mtseq2seq\n        optim = torch.optim.Adam(mtseq2seq.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            mtseq2seq.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                optim.zero_grad()\n                src, src_lens = item.source\n                trg, trg_lens = item.target\n                output = mtseq2seq(src, src_lens, trg)\n                output = output[1:].contiguous()\n                output = output.view(-1, output.shape[-1])\n                trg = trg.transpose(1, 0)\n                trg = trg[1:].contiguous()\n                trg = trg.view(-1)\n                item_loss = F.cross_entropy(output, trg, ignore_index=TARGET.vocab.stoi[\'<pad>\'])\n                acc_loss += item_loss.item()\n                item_loss.backward()\n                clip_grad_norm_(mtseq2seq.parameters(), config.clip)\n                optim.step()\n            print(\'epoch:{}, acc_loss:{}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'mt_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'mt_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        mtseq2seq.save()\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = MTConfig.load(save_path)\n        mtseq2seq = MTSeq2Seq(config)\n        mtseq2seq .load()\n        self._model = mtseq2seq\n        self._source_vocab = config.source_word_vocab\n        self._target_vocab = config.target_word_vocab\n\n    def test(self, test_path):\n        test_dataset = mt_tool.get_dataset(test_path)\n        if not hasattr(SOURCE, \'vocab\'):\n            SOURCE.vocab = self._source_vocab\n        if not hasattr(TARGET, \'vocab\'):\n            TARGET.vocab = self._source_vocab\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n\n    def _validate(self, dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\']):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = mt_tool.get_iterator(dev_dataset, batch_size=batch_size)\n        for dev_item in tqdm(dev_iter):\n            src, src_lens = dev_item.source\n            trg, trg_lens = dev_item.target\n            item_score = mt_tool.get_score(self._model, src, src_lens, trg)\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def predict(self, text: str, max_len=10):\n        self._model.eval()\n        text_list = eng_tokenize(text)\n        text_list = [x.lower() for x in text_list]\n        vec_text = torch.tensor([self._source_vocab.stoi[x] for x in text_list])\n        vec_text = vec_text.reshape(1, -1).to(DEVICE)\n        len_text = torch.tensor([len(vec_text)]).to(DEVICE)\n        sos = torch.tensor([self._source_vocab.stoi[\'<sos>\']]).to(DEVICE)\n        output = self._model.predict(vec_text, len_text, sos, max_len).squeeze(1)\n        soft_predict = torch.softmax(output, dim=1)\n        predict_prob, predict_index = torch.max(soft_predict.cpu().data, dim=1)\n        predict_sentence = [self._target_vocab.itos[x] for x in predict_index]\n        predict_prob = predict_prob.cpu().data.tolist()\n        result_sentence = []\n        result_score = 1.0\n        for i in range(1, len(predict_sentence)):\n            if predict_sentence[i] != \'<eos>\':\n                result_sentence.append(predict_sentence[i])\n                result_score *= predict_prob[i]\n            else:\n                break\n        return \'\'.join(result_sentence), result_score\n\n    def deploy(self, route_path=""/mt"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/predict"", methods=[\'POST\', \'GET\'])\n        def predict():\n            text = request.args.get(\'text\', \'\')\n            result = self.predict(text)\n            return flask.jsonify({\n                \'state\': \'OK\',\n                \'result\': {\n                    \'result\': result[0],\n                    \'score\': result[1]\n                }\n            })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/tg/mt/tool.py,4,"b""import re\nimport torch\nfrom torchtext.data import TabularDataset, Field, Dataset, BucketIterator\nfrom torchtext.vocab import Vectors\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\nimport jieba\nimport nltk\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\nregex = re.compile(r'[^\\u4e00-\\u9fa5aA-Za-z0-9\xef\xbc\x8c\xe3\x80\x82\xef\xbc\x9f\xef\xbc\x9a\xef\xbc\x81\xef\xbc\x9b\xe2\x80\x9c\xe2\x80\x9d]')\n\n\ndef light_tokenize(text):\n    text = regex.sub(' ', text)\n    return [word for word in jieba.cut(text) if word.strip()]\n\n\ndef eng_tokenize(text):\n    return nltk.word_tokenize(text)\n\n\nSOURCE = Field(lower=True, tokenize=eng_tokenize, include_lengths=True, batch_first=True, init_token='<sos>',\n               eos_token='<eos>')\nTARGET = Field(lower=True, tokenize=light_tokenize, include_lengths=True, batch_first=True, init_token='<sos>',\n               eos_token='<eos>')\nFields = [\n            ('source', SOURCE),\n            ('target', TARGET)\n        ]\n\n\nclass MTTool(Tool):\n    def get_dataset(self, path: str, fields=Fields, file_type='tsv', skip_header=False):\n        logger.info('loading dataset from {}'.format(path))\n        mt_dataset = TabularDataset(path, format=file_type, fields=fields, skip_header=skip_header)\n        logger.info('successed loading dataset')\n        return mt_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building source word vocab...')\n        SOURCE.build_vocab(*dataset)\n        logger.info('successed building source word vocab')\n        logger.info('building target word vocab...')\n        TARGET.build_vocab(*dataset)\n        logger.info('successed building target word vocab')\n        return SOURCE.vocab, TARGET.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('successed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset: Dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE,\n                     sort_key=lambda x: len(x.source), sort_within_batch=True):\n        return BucketIterator(dataset, batch_size=batch_size, device=device, sort_key=sort_key,\n                              sort_within_batch=sort_within_batch)\n\n    def get_score(self, model, src, src_lens, trg, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        output = model(src, src_lens, trg)\n        output = output[1:].contiguous()\n        output = output.view(-1, output.shape[-1])\n        trg = trg.transpose(1, 0)\n        trg = trg[1:].contiguous()\n        trg = trg.view(-1)\n        soft_predict = torch.softmax(output, dim=1)\n        predict_prob, predict_index = torch.max(soft_predict.cpu().data, dim=1)\n        labels = trg.cpu().data.numpy()\n        return metric_func(predict_index, labels, average='micro')\n\n\nmt_tool = MTTool()\n"""
lightnlp/tg/ts/__init__.py,0,b'# text summarization\xef\xbc\x8c\xe6\x96\x87\xe6\x9c\xac\xe6\x91\x98\xe8\xa6\x81\n'
lightnlp/tg/ts/config.py,0,"b""from ...base.config import DEVICE\nDEFAULT_CONFIG = {\n    'lr': 0.02,\n    'epoch': 300,\n    'lr_decay': 0.05,\n    'batch_size': 4,\n    'dropout': 0.5,\n    'static': False,\n    'non_static': False,\n    'embedding_dim': 100,\n    'clip': 10,\n    'num_layers': 2,\n    'vector_path': '',\n    'vocabulary_size': 0,\n    'word_vocab': None,\n    'save_path': './ts_saves',\n    'method': 'dot',\n    'teacher_forcing_ratio': 0.5\n}\n"""
lightnlp/tg/ts/model.py,0,"b'from .config import DEVICE, DEFAULT_CONFIG\nfrom .models.encoder import Encoder\nfrom .models.decoder import Decoder\nfrom .models.seq2seq import Seq2Seq\nfrom ...base.model import BaseConfig, BaseModel\n\n\nclass TSConfig(BaseConfig):\n    def __init__(self, word_vocab, vector_path, **kwargs):\n        super(TSConfig, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.vocabulary_size = len(self.word_vocab)\n        self.vector_path = vector_path\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass TSSeq2Seq(BaseModel):\n    def __init__(self, args):\n        super(TSSeq2Seq, self).__init__(args)\n        self.args = args\n        self.hidden_dim = args.embedding_dim\n        self.vocabulary_size = args.vocabulary_size\n        self.batch_size = args.batch_size\n        self.save_path = args.save_path\n        self.num_layers = args.num_layers\n        self.dropout = args.dropout\n        self.teacher_forcing_ratio = args.teacher_forcing_ratio\n\n        vocabulary_size = args.vocabulary_size\n        embedding_dimension = args.embedding_dim\n\n        encoder = Encoder(vocabulary_size, embedding_dimension, self.hidden_dim, self.num_layers,\n                          self.dropout).to(DEVICE)\n        decoder = Decoder(self.hidden_dim, embedding_dimension, vocabulary_size, self.num_layers, self.dropout,\n                          args.method).to(DEVICE)\n        self.seq2seq = Seq2Seq(encoder, decoder).to(DEVICE)\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        return self.seq2seq(src, trg, teacher_forcing_ratio)\n\n    def predict(self, src, src_lens, sos, max_len):\n        return self.seq2seq.predict(src, src_lens, sos, max_len)\n'"
lightnlp/tg/ts/module.py,9,"b'import torch\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom torch.nn.utils import clip_grad_norm_\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ...utils.deploy import get_free_tcp_port\nfrom ...utils.learning import adjust_learning_rate\nfrom ...utils.log import logger\nfrom ...base.module import Module\n\nfrom .config import DEVICE, DEFAULT_CONFIG\nfrom .model import TSConfig, TSSeq2Seq\nfrom .tool import ts_tool, light_tokenize, TEXT\n\n\nclass TS(Module):\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n\n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = ts_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = ts_tool.get_dataset(dev_path)\n            word_vocab = ts_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab = ts_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        train_iter = ts_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = TSConfig(word_vocab, save_path=save_path, vector_path=vectors_path, **kwargs)\n        tsseq2seq = TSSeq2Seq(config)\n        self._model = tsseq2seq\n        optim = torch.optim.Adam(tsseq2seq.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            tsseq2seq.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                optim.zero_grad()\n                src, src_lens = item.text\n                trg, trg_lens = item.summarization\n                output = tsseq2seq(src, src_lens, trg)\n                output = output[1:].contiguous()\n                output = output.view(-1, output.shape[-1])\n                trg = trg.transpose(1, 0)\n                trg = trg[1:].contiguous()\n                trg = trg.view(-1)\n                item_loss = F.cross_entropy(output, trg, ignore_index=TEXT.vocab.stoi[\'<pad>\'])\n                acc_loss += item_loss.item()\n                item_loss.backward()\n                clip_grad_norm_(tsseq2seq.parameters(), config.clip)\n                optim.step()\n            print(\'epoch:{}, acc_loss:{}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'ts_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'ts_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        tsseq2seq.save()\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = TSConfig.load(save_path)\n        tsseq2seq = TSSeq2Seq(config)\n        tsseq2seq .load()\n        self._model = tsseq2seq\n        self._word_vocab = config.word_vocab\n\n    def test(self, test_path):\n        test_dataset = ts_tool.get_dataset(test_path)\n        if not hasattr(TEXT, \'vocab\'):\n            TEXT.vocab = self._word_vocab\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n\n    def _validate(self, dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\']):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = ts_tool.get_iterator(dev_dataset, batch_size=batch_size)\n        for dev_item in tqdm(dev_iter):\n            src, src_lens = dev_item.text\n            trg, trg_lens = dev_item.summarization\n            item_score = ts_tool.get_score(self._model, src, src_lens, trg)\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def predict(self, text: str, max_len=20):\n        self._model.eval()\n        text_list = light_tokenize(text)\n        vec_text = torch.tensor([self._word_vocab.stoi[x] for x in text_list])\n        vec_text = vec_text.reshape(1, -1).to(DEVICE)\n        len_text = torch.tensor([len(vec_text)]).to(DEVICE)\n        sos = torch.tensor([self._word_vocab.stoi[\'<sos>\']]).to(DEVICE)\n        output = self._model.predict(vec_text, len_text, sos, max_len).squeeze(1)\n        soft_predict = torch.softmax(output, dim=1)\n        predict_prob, predict_index = torch.max(soft_predict.cpu().data, dim=1)\n        predict_sentence = [self._word_vocab.itos[x] for x in predict_index]\n        predict_prob = predict_prob.cpu().data.tolist()\n        result_sentence = []\n        result_score = 1.0\n        for i in range(1, len(predict_sentence)):\n            if predict_sentence[i] != \'<eos>\':\n                result_sentence.append(predict_sentence[i])\n                result_score *= predict_prob[i]\n            else:\n                break\n        return \'\'.join(result_sentence), result_score\n\n    def deploy(self, route_path=""/ts"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/predict"", methods=[\'POST\', \'GET\'])\n        def predict():\n            text = request.args.get(\'text\', \'\')\n            max_len = int(request.args.get(\'max_len\', 20))\n            result = self.predict(text, max_len)\n            return flask.jsonify({\n                \'state\': \'OK\',\n                \'result\': {\n                    \'result\': result[0],\n                    \'score\': result[1]\n                }\n            })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/tg/ts/tool.py,4,"b""import re\nimport torch\nfrom torchtext.data import TabularDataset, Field, Dataset, BucketIterator\nfrom torchtext.vocab import Vectors\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\nimport jieba\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\nregex = re.compile(r'[^\\u4e00-\\u9fa5aA-Za-z0-9\xef\xbc\x8c\xe3\x80\x82\xef\xbc\x9f\xef\xbc\x9a\xef\xbc\x81\xef\xbc\x9b\xe2\x80\x9c\xe2\x80\x9d]')\n\n\ndef light_tokenize(text):\n    # text = regex.sub(' ', text)\n    return [word for word in jieba.cut(text) if word.strip()]\n\n\nTEXT = Field(lower=True, tokenize=light_tokenize, include_lengths=True, batch_first=True, init_token='<sos>',\n             eos_token='<eos>')\nFields = [\n            ('text', TEXT),\n            ('summarization', TEXT)\n        ]\n\n\nclass TSTool(Tool):\n    def get_dataset(self, path: str, fields=Fields, file_type='tsv', skip_header=False):\n        logger.info('loading dataset from {}'.format(path))\n        ts_dataset = TabularDataset(path, format=file_type, fields=fields, skip_header=skip_header)\n        logger.info('successed loading dataset')\n        return ts_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        TEXT.build_vocab(*dataset)\n        logger.info('successed building word vocab')\n        return TEXT.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('successed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset: Dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE,\n                     sort_key=lambda x: len(x.text), sort_within_batch=True):\n        return BucketIterator(dataset, batch_size=batch_size, device=device, sort_key=sort_key,\n                              sort_within_batch=sort_within_batch)\n\n    def get_score(self, model, src, src_lens, trg, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        output = model(src, src_lens, trg)\n        output = output[1:].contiguous()\n        output = output.view(-1, output.shape[-1])\n        trg = trg.transpose(1, 0)\n        trg = trg[1:].contiguous()\n        trg = trg.view(-1)\n        soft_predict = torch.softmax(output, dim=1)\n        predict_prob, predict_index = torch.max(soft_predict.cpu().data, dim=1)\n        labels = trg.cpu().data.numpy()\n        return metric_func(predict_index, labels, average='micro')\n\n\nts_tool = TSTool()\n"""
lightnlp/we/cbow/__init__.py,0,b'\n'
lightnlp/we/cbow/config.py,0,"b""from ...base.config import DEVICE\n\n\nclass Feature:\n    normal = 'Normal'\n    negative_sampling = 'Negative_sampling'\n    hierarchical_softmax = 'Hierarchical_Softmax'\n\n\nDEFAULT_CONFIG = {\n    'lr': 0.005,\n    'epoch': 30,\n    'lr_decay': 0.05,\n    'batch_size': 128,\n    'embedding_dim': 300,\n    'vocabulary_size': 0,\n    'word_vocab': None,\n    'save_path': './cbow_saves',\n    'window_size': 3,\n    'neg_num': 3,\n    'feature': Feature.hierarchical_softmax\n}\n"""
lightnlp/we/cbow/model.py,13,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ...base.model import BaseConfig, BaseModel\n\nfrom .config import DEVICE, DEFAULT_CONFIG, Feature\n\n\nclass Config(BaseConfig):\n    def __init__(self, word_vocab, **kwargs):\n        super(Config, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.vocabulary_size = len(self.word_vocab)\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass CBOWBase(BaseModel):\n    def __init__(self, args):\n        super(CBOWBase, self).__init__(args)\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n\n        self.word_embeddings = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n        self.linear = nn.Linear(self.embedding_dimension, self.vocabulary_size).to(DEVICE)\n\n    def forward(self, context):\n        context_embedding = torch.sum(self.word_embeddings(context), dim=1)\n        target_embedding = self.linear(context_embedding)\n        return target_embedding\n\n    def loss(self, context, target):\n        context_embedding = torch.sum(self.word_embeddings(context), dim=1)\n        target_embedding = self.linear(context_embedding)\n        return F.cross_entropy(target_embedding, target.view(-1))\n\n\nclass CBOWNegativeSampling(BaseModel):\n    def __init__(self, args):\n        super(CBOWNegativeSampling, self).__init__(args)\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n\n        self.word_embeddings = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n        self.context_embeddings = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n\n    def forward(self, context, target):\n        context_embedding = torch.sum(self.word_embeddings(context), dim=1)\n        target_embedding = self.context_embeddings(target)\n        target_score = torch.bmm(target_embedding, context_embedding.unsqueeze(2))\n        return torch.sigmoid(target_score)\n\n    def loss(self, context, pos, neg):\n        context_embedding = torch.sum(self.word_embeddings(context), dim=1)\n        pos_embedding = self.context_embeddings(pos)\n        neg_embedding = self.context_embeddings(neg).squeeze()\n        pos_score = torch.bmm(pos_embedding, context_embedding.unsqueeze(2)).squeeze()\n        neg_score = torch.bmm(neg_embedding, context_embedding.unsqueeze(2)).squeeze()\n        pos_score = torch.sum(F.logsigmoid(pos_score), dim=0)\n        neg_score = torch.sum(F.logsigmoid(-1 * neg_score), dim=0)\n        return -1*(torch.sum(pos_score) + torch.sum(neg_score))\n\n\nclass CBOWHierarchicalSoftmax(BaseModel):\n    def __init__(self, args):\n        super(CBOWHierarchicalSoftmax, self).__init__(args)\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n\n        self.word_embeddings = nn.Embedding(2*self.vocabulary_size-1, self.embedding_dimension,\n                                            sparse=True).to(DEVICE)\n        self.context_embeddings = nn.Embedding(2*self.vocabulary_size-1, self.embedding_dimension,\n                                               sparse=True).to(DEVICE)\n\n    def forward(self, x):\n        pass\n\n    def loss(self, pos_context, pos_path, neg_context, neg_path):\n        pass\n'"
lightnlp/we/cbow/module.py,0,b''
lightnlp/we/cbow/tool.py,4,"b""import torch\nfrom torchtext.data import Field, Iterator\nfrom torchtext.vocab import Vectors\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\nimport jieba\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nfrom .utils.dataset import CBOWDataset\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nWORD = Field(tokenize=lambda x: [x], batch_first=True)\nFields = [\n            ('context', WORD),\n            ('target', WORD)\n        ]\n\n\ndef default_tokenize(sentence):\n    return list(jieba.cut(sentence))\n\n\nclass CBOWTool(Tool):\n    def get_dataset(self, path: str, fields=Fields):\n        logger.info('loading dataset from {}'.format(path))\n        cbow_dataset = CBOWDataset(path, fields)\n        logger.info('succeed loading dataset')\n        return cbow_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        WORD.build_vocab(*dataset)\n        logger.info('succeed building word vocab')\n        return WORD.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('succeed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE,\n                     sort_key=lambda x: len(x.text)):\n        return Iterator(dataset, batch_size=batch_size, device=device, sort_key=sort_key)\n\n    def get_score(self, model, texts, labels, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        assert texts.size(0) == len(labels)\n        vec_predict = model(texts)\n        soft_predict = torch.softmax(vec_predict, dim=1)\n        predict_prob, predict_index = torch.max(soft_predict.cpu().data, dim=1)\n        labels = labels.view(-1).cpu().data.numpy()\n        return metric_func(predict_index, labels, average='micro')\n\n\ncbow_tool = CBOWTool()\n"""
lightnlp/we/skip_gram/__init__.py,0,b'\n'
lightnlp/we/skip_gram/config.py,0,"b""from ...base.config import DEVICE\n\n\nclass Feature:\n    normal = 'Normal'\n    negative_sampling = 'Negative_sampling'\n    hierarchical_softmax = 'Hierarchical_Softmax'\n\n\nDEFAULT_CONFIG = {\n    'lr': 0.005,\n    'epoch': 10,\n    'lr_decay': 0.05,\n    'batch_size': 128,\n    'embedding_dim': 300,\n    'vocabulary_size': 0,\n    'word_vocab': None,\n    'save_path': './skip_gram_saves',\n    'window_size': 3,\n    'neg_num': 3,\n    'feature': Feature.hierarchical_softmax\n}\n"""
lightnlp/we/skip_gram/model.py,13,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ...base.model import BaseConfig, BaseModel\n\nfrom .config import DEVICE, DEFAULT_CONFIG, Feature\n\n\nclass Config(BaseConfig):\n    def __init__(self, word_vocab, **kwargs):\n        super(Config, self).__init__()\n        for name, value in DEFAULT_CONFIG.items():\n            setattr(self, name, value)\n        self.word_vocab = word_vocab\n        self.vocabulary_size = len(self.word_vocab)\n        for name, value in kwargs.items():\n            setattr(self, name, value)\n\n\nclass CBOWBase(BaseModel):\n    def __init__(self, args):\n        super(CBOWBase, self).__init__(args)\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n\n        self.word_embeddings = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n        self.linear = nn.Linear(self.embedding_dimension, self.vocabulary_size).to(DEVICE)\n\n    def forward(self, context):\n        context_embedding = torch.sum(self.word_embeddings(context), dim=1)\n        target_embedding = self.linear(context_embedding)\n        return target_embedding\n\n    def loss(self, context, target):\n        context_embedding = torch.sum(self.word_embeddings(context), dim=1)\n        target_embedding = self.linear(context_embedding)\n        return F.cross_entropy(target_embedding, target.view(-1))\n\n\nclass CBOWNegativeSampling(BaseModel):\n    def __init__(self, args):\n        super(CBOWNegativeSampling, self).__init__(args)\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n\n        self.word_embeddings = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n        self.context_embeddings = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n\n    def forward(self, context, target):\n        context_embedding = torch.sum(self.word_embeddings(context), dim=1)\n        target_embedding = self.context_embeddings(target)\n        target_score = torch.bmm(target_embedding, context_embedding.unsqueeze(2))\n        return torch.sigmoid(target_score)\n\n    def loss(self, context, pos, neg):\n        context_embedding = torch.sum(self.word_embeddings(context), dim=1)\n        pos_embedding = self.context_embeddings(pos)\n        neg_embedding = self.context_embeddings(neg).squeeze()\n        pos_score = torch.bmm(pos_embedding, context_embedding.unsqueeze(2)).squeeze()\n        neg_score = torch.bmm(neg_embedding, context_embedding.unsqueeze(2)).squeeze()\n        pos_score = torch.sum(F.logsigmoid(pos_score), dim=0)\n        neg_score = torch.sum(F.logsigmoid(-1 * neg_score), dim=0)\n        return -1*(torch.sum(pos_score) + torch.sum(neg_score))\n\n\nclass CBOWHierarchicalSoftmax(BaseModel):\n    def __init__(self, args):\n        super(CBOWHierarchicalSoftmax, self).__init__(args)\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n\n        self.word_embeddings = nn.Embedding(2*self.vocabulary_size-1, self.embedding_dimension,\n                                            sparse=True).to(DEVICE)\n        self.context_embeddings = nn.Embedding(2*self.vocabulary_size-1, self.embedding_dimension,\n                                               sparse=True).to(DEVICE)\n\n    def forward(self, x):\n        pass\n\n    def loss(self, pos_context, pos_path, neg_context, neg_path):\n        pass\n'"
lightnlp/we/skip_gram/module.py,0,b''
lightnlp/we/skip_gram/tool.py,4,"b""import torch\nfrom torchtext.data import Field, Iterator\nfrom torchtext.vocab import Vectors\nfrom sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score\nimport jieba\n\nfrom ...base.tool import Tool\nfrom ...utils.log import logger\nfrom .config import DEVICE, DEFAULT_CONFIG\n\nfrom .utils.dataset import SkipGramDataset\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nWORD = Field(tokenize=lambda x: [x], batch_first=True)\nFields = [\n            ('context', WORD),\n            ('target', WORD)\n        ]\n\n\ndef default_tokenize(sentence):\n    return list(jieba.cut(sentence))\n\n\nclass SkipGramTool(Tool):\n    def get_dataset(self, path: str, fields=Fields):\n        logger.info('loading dataset from {}'.format(path))\n        cbow_dataset = SkipGramDataset(path, fields)\n        logger.info('succeed loading dataset')\n        return cbow_dataset\n\n    def get_vocab(self, *dataset):\n        logger.info('building word vocab...')\n        WORD.build_vocab(*dataset)\n        logger.info('succeed building word vocab')\n        return WORD.vocab\n\n    def get_vectors(self, path: str):\n        logger.info('loading vectors from {}'.format(path))\n        vectors = Vectors(path)\n        logger.info('succeed loading vectors')\n        return vectors\n\n    def get_iterator(self, dataset, batch_size=DEFAULT_CONFIG['batch_size'], device=DEVICE,\n                     sort_key=lambda x: len(x.text)):\n        return Iterator(dataset, batch_size=batch_size, device=device, sort_key=sort_key)\n\n    def get_score(self, model, texts, labels, score_type='f1'):\n        metrics_map = {\n            'f1': f1_score,\n            'p': precision_score,\n            'r': recall_score,\n            'acc': accuracy_score\n        }\n        metric_func = metrics_map[score_type] if score_type in metrics_map else metrics_map['f1']\n        assert texts.size(0) == len(labels)\n        vec_predict = model(texts)\n        soft_predict = torch.softmax(vec_predict, dim=1)\n        predict_prob, predict_index = torch.max(soft_predict.cpu().data, dim=1)\n        labels = labels.view(-1).cpu().data.numpy()\n        return metric_func(predict_index, labels, average='micro')\n\n\nskip_gram_tool = SkipGramTool()\n"""
lightnlp/sl/cws/utils/__init__.py,0,b''
lightnlp/sl/cws/utils/convert.py,0,"b""def bis_cws(words, tags):\n    assert len(words) == len(tags)\n    poses = []\n\n    for i, tag in enumerate(tags):\n        if tag in ['B', 'S']:\n            begin = i\n        if i == len(tags) - 1:\n            poses.append(''.join(words[begin: i + 1]))\n        elif tags[i + 1] != 'I':\n            poses.append(''.join(words[begin: i + 1]))\n            begin = i + 1\n    return poses\n"""
lightnlp/sl/ner/utils/__init__.py,0,b''
lightnlp/sl/ner/utils/convert.py,0,"b'def iob_ranges(words, tags):\n    """"""\n    IOB -> Ranges\n    """"""\n    assert len(words) == len(tags)\n    ranges = []\n\n    def check_if_closing_range():\n        if i == len(tags) - 1 or tags[i + 1].split(\'_\')[0] == \'O\':\n            ranges.append({\n                \'entity\': \'\'.join(words[begin: i + 1]),\n                \'type\': temp_type,\n                \'start\': begin,\n                \'end\': i\n            })\n\n    for i, tag in enumerate(tags):\n        if tag.split(\'_\')[0] == \'O\':\n            pass\n        elif tag.split(\'_\')[0] == \'B\':\n            begin = i\n            temp_type = tag.split(\'_\')[1]\n            check_if_closing_range()\n        elif tag.split(\'_\')[0] == \'I\':\n            check_if_closing_range()\n    return ranges\n'"
lightnlp/sl/pos/utils/__init__.py,0,b''
lightnlp/sl/pos/utils/convert.py,0,"b""def bis_pos(words, tags):\n    assert len(words) == len(tags)\n    poses = []\n\n    for i, tag in enumerate(tags):\n        if tag.split('-')[0] in ['B', 'S']:\n            begin = i\n            temp_type = tag.split('-')[1]\n        if i == len(tags) - 1:\n            poses.append((''.join(words[begin: i + 1]), temp_type))\n        elif tags[i + 1].split('-')[0] != 'I' or tags[i + 1].split('-')[1] != temp_type:\n            poses.append((''.join(words[begin: i + 1]), temp_type))\n            begin = i + 1\n            temp_type = tags[i + 1].split('-')[1]\n    return poses\n"""
lightnlp/sl/srl/utils/__init__.py,0,b''
lightnlp/sl/srl/utils/convert.py,0,"b'def iobes_iob(tags):\n    """"""\n    IOBES -> IOB\n    """"""\n    new_tags = []\n    for i, tag in enumerate(tags):\n        if tag == \'rel\':\n            new_tags.append(tag)\n        elif tag.split(\'-\')[0] == \'B\':\n            new_tags.append(tag)\n        elif tag.split(\'-\')[0] == \'I\':\n            new_tags.append(tag)\n        elif tag.split(\'-\')[0] == \'S\':\n            new_tags.append(tag.replace(\'S-\', \'B-\'))\n        elif tag.split(\'-\')[0] == \'E\':\n            new_tags.append(tag.replace(\'E-\', \'I-\'))\n        elif tag.split(\'-\')[0] == \'O\':\n            new_tags.append(tag)\n        else:\n            raise Exception(\'Invalid format!\')\n    return new_tags\n\n\ndef iob_ranges(words, tags):\n    """"""\n    IOB -> Ranges\n    """"""\n    assert len(words) == len(tags)\n    events = {}\n\n    def check_if_closing_range():\n        if i == len(tags) - 1 or tags[i + 1].split(\'-\')[0] == \'O\' or tags[i+1] == \'rel\':\n            events[temp_type] = \'\'.join(words[begin: i + 1])\n\n    for i, tag in enumerate(tags):\n        if tag == \'rel\':\n            events[\'rel\'] = words[i]\n        elif tag.split(\'-\')[0] == \'O\':\n            pass\n        elif tag.split(\'-\')[0] == \'B\':\n            begin = i\n            temp_type = tag.split(\'-\')[1]\n            check_if_closing_range()\n        elif tag.split(\'-\')[0] == \'I\':\n            check_if_closing_range()\n    return events\n\n\ndef iobes_ranges(words, tags):\n    new_tags = iobes_iob(tags)\n    return iob_ranges(words, new_tags)\n'"
lightnlp/sp/gdp/components/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom .biaffine import Biaffine\nfrom .lstm import LSTM\nfrom .mlp import MLP\n\n\n__all__ = ['LSTM', 'MLP', 'Biaffine']\n"""
lightnlp/sp/gdp/components/biaffine.py,5,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\n\n\nclass Biaffine(nn.Module):\n\n    def __init__(self, n_in, n_out=1, bias_x=True, bias_y=True):\n        super(Biaffine, self).__init__()\n\n        self.n_in = n_in\n        self.n_out = n_out\n        self.bias_x = bias_x\n        self.bias_y = bias_y\n        self.weight = nn.Parameter(torch.Tensor(n_out,\n                                                n_in + bias_x,\n                                                n_in + bias_y))\n        self.reset_parameters()\n\n    def extra_repr(self):\n        info = f""n_in={self.n_in}, n_out={self.n_out}""\n        if self.bias_x:\n            info += f"", bias_x={self.bias_x}""\n        if self.bias_y:\n            info += f"", bias_y={self.bias_y}""\n\n        return info\n\n    def reset_parameters(self):\n        nn.init.zeros_(self.weight)\n\n    def forward(self, x, y):\n        if self.bias_x:\n            x = torch.cat([x, x.new_ones(x.shape[:-1]).unsqueeze(-1)], -1)\n        if self.bias_y:\n            y = torch.cat([y, y.new_ones(y.shape[:-1]).unsqueeze(-1)], -1)\n        # [batch_size, 1, seq_len, d]\n        x = x.unsqueeze(1)\n        # [batch_size, 1, seq_len, d]\n        y = y.unsqueeze(1)\n        # [batch_size, n_out, seq_len, seq_len]\n        s = x @ self.weight @ torch.transpose(y, -1, -2)\n        # remove dim 1 if n_out == 1\n        s = s.squeeze(1)\n\n        return s\n'"
lightnlp/sp/gdp/components/dropout.py,4,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\n\n\nclass SharedDropout(nn.Module):\n\n    def __init__(self, p=0.5, batch_first=True):\n        super(SharedDropout, self).__init__()\n\n        self.p = p\n        self.batch_first = batch_first\n\n    def extra_repr(self):\n        info = f""p={self.p}""\n        if self.batch_first:\n            info += f"", batch_first={self.batch_first}""\n\n        return info\n\n    def forward(self, x):\n        if self.training:\n            if self.batch_first:\n                mask = self.get_mask(x[:, 0], self.p)\n            else:\n                mask = self.get_mask(x[0], self.p)\n            x *= mask.unsqueeze(1) if self.batch_first else mask\n\n        return x\n\n    @staticmethod\n    def get_mask(x, p):\n        mask = x.new_full(x.shape, 1 - p)\n        mask = torch.bernoulli(mask) / (1 - p)\n\n        return mask\n\n\nclass IndependentDropout(nn.Module):\n\n    def __init__(self, p=0.5):\n        super(IndependentDropout, self).__init__()\n\n        self.p = p\n\n    def extra_repr(self):\n        return f""p={self.p}""\n\n    def forward(self, x, y, eps=1e-12):\n        if self.training:\n            x_mask = torch.bernoulli(x.new_full(x.shape[:2], 1 - self.p))\n            y_mask = torch.bernoulli(y.new_full(y.shape[:2], 1 - self.p))\n            scale = 3.0 / (2.0 * x_mask + y_mask + eps)\n            x_mask *= scale\n            y_mask *= scale\n            x *= x_mask.unsqueeze(dim=-1)\n            y *= y_mask.unsqueeze(dim=-1)\n\n        return x, y\n'"
lightnlp/sp/gdp/components/lstm.py,8,"b'# -*- coding: utf-8 -*-\n\nfrom .dropout import SharedDropout\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import PackedSequence\n\n\nclass LSTM(nn.Module):\n\n    def __init__(self, input_size, hidden_size, num_layers=1,\n                 dropout=0, bidirectional=False):\n        super(LSTM, self).__init__()\n\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.dropout = dropout\n        self.bidirectional = bidirectional\n        self.num_directions = 2 if bidirectional else 1\n\n        self.f_cells = nn.ModuleList()\n        self.b_cells = nn.ModuleList()\n        for layer in range(self.num_layers):\n            self.f_cells.append(nn.LSTMCell(input_size=input_size,\n                                            hidden_size=hidden_size))\n            if bidirectional:\n                self.b_cells.append(nn.LSTMCell(input_size=input_size,\n                                                hidden_size=hidden_size))\n            input_size = hidden_size * self.num_directions\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for i in self.parameters():\n            # apply orthogonal_ to weight\n            if len(i.shape) > 1:\n                nn.init.orthogonal_(i)\n            # apply zeros_ to bias\n            else:\n                nn.init.zeros_(i)\n\n    def layer_forward(self, x, hx, cell, batch_sizes, reverse=False):\n        h, c = hx\n        init_h, init_c = h, c\n        output, seq_len = [], len(x)\n        steps = reversed(range(seq_len)) if reverse else range(seq_len)\n        if self.training:\n            hid_mask = SharedDropout.get_mask(h, self.dropout)\n\n        for t in steps:\n            batch_size = batch_sizes[t]\n            if len(h) < batch_size:\n                h = torch.cat((h, init_h[last_batch_size:batch_size]))\n                c = torch.cat((c, init_c[last_batch_size:batch_size]))\n            else:\n                h = h[:batch_size]\n                c = c[:batch_size]\n            h, c = cell(input=x[t], hx=(h, c))\n            output.append(h)\n            if self.training:\n                h = h * hid_mask[:batch_size]\n            last_batch_size = batch_size\n        if reverse:\n            output.reverse()\n        output = torch.cat(output)\n\n        return output\n\n    def forward(self, x, hx=None):\n        x, batch_sizes = x\n        batch_size = batch_sizes[0]\n\n        if hx is None:\n            init = x.new_zeros(batch_size, self.hidden_size)\n            hx = (init, init)\n\n        for layer in range(self.num_layers):\n            if self.training:\n                mask = SharedDropout.get_mask(x[:batch_size], self.dropout)\n                mask = torch.cat([mask[:batch_size]\n                                  for batch_size in batch_sizes])\n                x *= mask\n            x = torch.split(x, batch_sizes.tolist())\n            f_output = self.layer_forward(x=x,\n                                          hx=hx,\n                                          cell=self.f_cells[layer],\n                                          batch_sizes=batch_sizes,\n                                          reverse=False)\n\n            if self.bidirectional:\n                b_output = self.layer_forward(x=x,\n                                              hx=hx,\n                                              cell=self.b_cells[layer],\n                                              batch_sizes=batch_sizes,\n                                              reverse=True)\n            if self.bidirectional:\n                x = torch.cat([f_output, b_output], -1)\n            else:\n                x = f_output\n        x = PackedSequence(x, batch_sizes)\n\n        return x\n'"
lightnlp/sp/gdp/components/mlp.py,1,"b'# -*- coding: utf-8 -*-\n\nfrom .dropout import SharedDropout\n\nimport torch.nn as nn\n\n\nclass MLP(nn.Module):\n\n    def __init__(self, n_in, n_hidden, dropout):\n        super(MLP, self).__init__()\n\n        self.linear = nn.Linear(n_in, n_hidden)\n        self.activation = nn.LeakyReLU(negative_slope=0.1)\n        self.dropout = SharedDropout(dropout)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.orthogonal_(self.linear.weight)\n        nn.init.zeros_(self.linear.bias)\n\n    def forward(self, x):\n        x = self.linear(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n\n        return x\n'"
lightnlp/sp/gdp/utils/__init__.py,0,"b""# -*- coding: utf-8 -*-\n\nfrom .dataset import TextDataset, collate_fn\nfrom .reader import Corpus, Embedding\nfrom .vocab import Vocab\n\n\n__all__ = ['Corpus', 'Embedding', 'TextDataset', 'Vocab', 'collate_fn']\n"""
lightnlp/sp/gdp/utils/dataset.py,3,"b'# -*- coding: utf-8 -*-\n\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset\n\n\ndef collate_fn(data):\n    reprs = (pad_sequence(i, True) for i in zip(*data))\n    if torch.cuda.is_available():\n        reprs = (i.cuda() for i in reprs)\n\n    return reprs\n\n\nclass TextDataset(Dataset):\n\n    def __init__(self, items):\n        super(TextDataset, self).__init__()\n\n        self.items = items\n\n    def __getitem__(self, index):\n        return tuple(item[index] for item in self.items)\n\n    def __len__(self):\n        return len(self.items[0])\n'"
lightnlp/sp/gdp/utils/metric.py,0,"b'# -*- coding: utf-8 -*-\n\nimport torch\n\n\nclass Metric(object):\n\n    def __lt__(self, other):\n        return self.score < other\n\n    def __le__(self, other):\n        return self.score <= other\n\n    def __eq__(self, other):\n        return self.score == other\n\n    def __ge__(self, other):\n        return self.score >= other\n\n    def __gt__(self, other):\n        return self.score > other\n\n    def __ne__(self, other):\n        return self.score != other\n\n    @property\n    def score(self):\n        raise AttributeError\n\n\nclass AttachmentMethod(Metric):\n\n    def __init__(self, eps=1e-5):\n        super(AttachmentMethod, self).__init__()\n\n        self.eps = eps\n        self.total = 0.0\n        self.correct_arcs = 0.0\n        self.correct_rels = 0.0\n\n    def __call__(self, pred_arcs, pred_rels, gold_arcs, gold_rels):\n        arc_mask = pred_arcs.eq(gold_arcs)\n        rel_mask = pred_rels.eq(gold_rels) & arc_mask\n\n        self.total += len(arc_mask)\n        self.correct_arcs += arc_mask.sum().item()\n        self.correct_rels += rel_mask.sum().item()\n\n    def __repr__(self):\n        return f""UAS: {self.uas:.2%} LAS: {self.las:.2%}""\n\n    @property\n    def score(self):\n        return self.las\n\n    @property\n    def uas(self):\n        return self.correct_arcs / (self.total + self.eps)\n\n    @property\n    def las(self):\n        return self.correct_rels / (self.total + self.eps)\n'"
lightnlp/sp/gdp/utils/reader.py,1,"b'# -*- coding: utf-8 -*-\n\nfrom collections import namedtuple\n\nimport torch\n\n\nSentence = namedtuple(typename=\'Sentence\',\n                      field_names=[\'ID\', \'FORM\', \'LEMMA\', \'CPOS\',\n                                   \'POS\', \'FEATS\', \'HEAD\', \'DEPREL\',\n                                   \'PHEAD\', \'PDEPREL\'])\n\n\nclass Corpus(object):\n    ROOT = \'<ROOT>\'\n\n    def __init__(self, sentences):\n        super(Corpus, self).__init__()\n\n        self.sentences = sentences\n\n    def __len__(self):\n        return len(self.sentences)\n\n    def __repr__(self):\n        return \'\\n\'.join(\n            \'\\n\'.join(\'\\t\'.join(map(str, i)) for i in zip(*sentence)) + \'\\n\'\n            for sentence in self\n        )\n\n    def __getitem__(self, index):\n        return self.sentences[index]\n\n    @property\n    def words(self):\n        return [[self.ROOT] + [word for word in sentence.FORM]\n                for sentence in self.sentences]\n\n    @property\n    def tags(self):\n        return [[self.ROOT] + list(sentence.POS)\n                for sentence in self.sentences]\n\n    @property\n    def heads(self):\n        return [[0] + list(map(int, sentence.HEAD))\n                for sentence in self.sentences]\n\n    @property\n    def rels(self):\n        return [[self.ROOT] + list(sentence.DEPREL)\n                for sentence in self.sentences]\n\n    @heads.setter\n    def heads(self, sequences):\n        self.sentences = [sentence._replace(HEAD=sequence)\n                          for sentence, sequence in zip(self, sequences)]\n\n    @rels.setter\n    def rels(self, sequences):\n        self.sentences = [sentence._replace(DEPREL=sequence)\n                          for sentence, sequence in zip(self, sequences)]\n\n    @classmethod\n    def load(cls, fname):\n        start, sentences = 0, []\n        with open(fname, \'r\') as f:\n            lines = [line for line in f]\n        for i, line in enumerate(lines):\n            if len(line) <= 1:\n                sentence = Sentence(*zip(*[l.split() for l in lines[start:i]]))\n                sentences.append(sentence)\n                start = i + 1\n        corpus = cls(sentences)\n\n        return corpus\n\n    def save(self, fname):\n        with open(fname, \'w\') as f:\n            f.write(f""{self}\\n"")\n\n\nclass Embedding(object):\n\n    def __init__(self, words, vectors):\n        super(Embedding, self).__init__()\n\n        self.words = words\n        self.vectors = vectors\n        self.pretrained = {w: v for w, v in zip(words, vectors)}\n\n    def __len__(self):\n        return len(self.words)\n\n    def __contains__(self, word):\n        return word in self.pretrained\n\n    def __getitem__(self, word):\n        return torch.tensor(self.pretrained[word])\n\n    @property\n    def dim(self):\n        return len(self.vectors[0])\n\n    @classmethod\n    def load(cls, fname):\n        with open(fname, \'r\') as f:\n            lines = [line for line in f]\n        splits = [line.split() for line in lines]\n        reprs = [(s[0], list(map(float, s[1:]))) for s in splits]\n        words, vectors = map(list, zip(*reprs))\n        embedding = cls(words, vectors)\n\n        return embedding\n'"
lightnlp/sp/gdp/utils/vocab.py,7,"b'# -*- coding: utf-8 -*-\n\nfrom collections import Counter\n\nimport regex\nimport torch\nimport torch.nn as nn\n\n\nclass Vocab(object):\n    PAD = \'<PAD>\'\n    UNK = \'<UNK>\'\n\n    def __init__(self, words, tags, rels):\n        self.pad_index = 0\n        self.unk_index = 1\n\n        self.words = [self.PAD, self.UNK] + sorted(words)\n        self.tags = [self.PAD, self.UNK] + sorted(tags)\n        self.rels = sorted(rels)\n\n        self.word_dict = {word: i for i, word in enumerate(self.words)}\n        self.tag_dict = {tag: i for i, tag in enumerate(self.tags)}\n        self.rel_dict = {rel: i for i, rel in enumerate(self.rels)}\n\n        # ids of punctuation that appear in words\n        self.puncts = sorted(i for word, i in self.word_dict.items()\n                             if regex.match(r\'\\p{P}+$\', word))\n\n        self.n_words = len(self.words)\n        self.n_tags = len(self.tags)\n        self.n_rels = len(self.rels)\n        self.n_train_words = self.n_words\n\n    def __repr__(self):\n        info = f""{self.__class__.__name__}(\\n""\n        info += f""  num of words: {self.n_words}\\n""\n        info += f""  num of tags: {self.n_tags}\\n""\n        info += f""  num of rels: {self.n_rels}\\n""\n        info += f"")""\n\n        return info\n\n    def word2id(self, sequence):\n        return torch.tensor([self.word_dict.get(word.lower(), self.unk_index)\n                             for word in sequence])\n\n    def tag2id(self, sequence):\n        return torch.tensor([self.tag_dict.get(tag, self.unk_index)\n                             for tag in sequence])\n\n    def rel2id(self, sequence):\n        return torch.tensor([self.rel_dict.get(rel, 0)\n                             for rel in sequence])\n\n    def id2rel(self, ids):\n        return [self.rels[i] for i in ids]\n\n    def read_embeddings(self, embed, unk=None):\n        words = embed.words\n        # if the UNK token has existed in pretrained vocab,\n        # then replace it with a self-defined one\n        if unk in embed:\n            words[words.index(unk)] = self.UNK\n\n        self.extend(words)\n        self.embeddings = torch.zeros(self.n_words, embed.dim)\n\n        for i, word in enumerate(self.words):\n            if word in embed:\n                self.embeddings[i] = embed[word]\n        self.embeddings /= torch.std(self.embeddings)\n\n    def extend(self, words):\n        self.words.extend(sorted(set(words).difference(self.word_dict)))\n        self.word_dict = {word: i for i, word in enumerate(self.words)}\n        self.puncts = sorted(i for word, i in self.word_dict.items()\n                             if regex.match(r\'\\p{P}+$\', word))\n        self.n_words = len(self.words)\n\n    def numericalize(self, corpus):\n        words = [self.word2id(seq) for seq in corpus.words]\n        tags = [self.tag2id(seq) for seq in corpus.tags]\n        arcs = [torch.tensor(seq) for seq in corpus.heads]\n        rels = [self.rel2id(seq) for seq in corpus.rels]\n\n        return words, tags, arcs, rels\n\n    @classmethod\n    def from_corpus(cls, corpus, min_freq=1):\n        words = Counter(word.lower() for seq in corpus.words for word in seq)\n        words = list(word for word, freq in words.items() if freq >= min_freq)\n        tags = list({tag for seq in corpus.tags for tag in seq})\n        rels = list({rel for seq in corpus.rels for rel in seq})\n        vocab = cls(words, tags, rels)\n\n        return vocab\n'"
lightnlp/sp/tdp/components/__init__.py,0,b''
lightnlp/sp/tdp/components/action_chooser.py,2,"b'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..config import DEVICE, DEFAULT_CONFIG, Actions\nfrom ..utils import vectors\n\n\nclass ActionChooserNetwork(nn.Module):\n\n    def __init__(self, input_dim):\n        super(ActionChooserNetwork, self).__init__()\n\n        self.hidden_dim = input_dim\n        self.linear1 = nn.Linear(input_dim, self.hidden_dim).to(DEVICE)\n        self.linear2 = nn.Linear(self.hidden_dim, Actions.NUM_ACTIONS).to(DEVICE)\n\n    def forward(self, inputs):\n        input_vec = vectors.concat_and_flatten(inputs)\n        temp_vec = self.linear1(input_vec)\n        temp_vec = F.relu(temp_vec).to(DEVICE)\n        result = self.linear2(temp_vec)\n        return result\n'"
lightnlp/sp/tdp/components/combiner.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..config import DEVICE, DEFAULT_CONFIG, Actions\nfrom ..utils import vectors\n\n\nclass MLPCombinerNetwork(nn.Module):\n\n    def __init__(self, embedding_dim):\n        super(MLPCombinerNetwork, self).__init__()\n\n        self.linear1 = nn.Linear(embedding_dim*2, embedding_dim).to(DEVICE)\n        self.linear2 = nn.Linear(embedding_dim, embedding_dim).to(DEVICE)\n\n    def forward(self, head_embed, modifier_embed):\n        input_vec = vectors.concat_and_flatten((head_embed, modifier_embed))\n        temp_vec = self.linear1(input_vec)\n        temp_vec = torch.tanh(temp_vec)\n        result = self.linear2(temp_vec)\n        return result\n\n\nclass LSTMCombinerNetwork(nn.Module):\n\n    def __init__(self, embedding_dim, num_layers, dropout):\n        super(LSTMCombinerNetwork, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.num_layers = num_layers\n        self.use_cuda = False\n\n        self.linear = nn.Linear(self.embedding_dim*2, self.embedding_dim).to(DEVICE)\n        self.hidden_dim = self.embedding_dim\n        self.lstm = nn.LSTM(self.hidden_dim, self.hidden_dim, num_layers=self.num_layers, dropout=dropout).to(DEVICE)\n\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self, batch_size=1):\n        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(DEVICE)\n        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(DEVICE)\n\n        return h0, c0\n\n    def forward(self, head_embed, modifier_embed):\n        input_vec = vectors.concat_and_flatten((head_embed, modifier_embed))\n        temp_vec = self.linear(input_vec).view(1, 1, -1)\n        \n        lstm_hiddens, self.hidden = self.lstm(temp_vec, self.hidden)\n        return lstm_hiddens[-1]\n\n    def clear_hidden_state(self):\n        self.hidden = self.init_hidden()\n'"
lightnlp/sp/tdp/components/word_embedding.py,3,"b'import torch\nimport torch.nn as nn\nfrom torchtext.vocab import Vectors\n\nfrom ..config import DEVICE\nfrom ....utils.log import logger\n\n\nclass VanillaWordEmbeddingLookup(nn.Module):\n    """"""\n    A component that simply returns a list of the word embeddings as\n    autograd Variables.\n    """"""\n\n    def __init__(self, vocabulary_size, embedding_dim, vector_path=None, non_static=False):\n        super(VanillaWordEmbeddingLookup, self).__init__()\n\n        self.vocabulary_size = vocabulary_size\n        self.embedding_dim = embedding_dim\n\n        self.output_dim = embedding_dim\n\n        self.word_embeddings = nn.Embedding(self.vocabulary_size, self.embedding_dim).to(DEVICE)\n        if vector_path:\n            logger.info(\'logging word vectors from {}\'.format(vector_path))\n            word_vectors = Vectors(vector_path).vectors\n            self.word_embeddings = self.word_embeddings.from_pretrained(word_vectors, freeze=not non_static).to(DEVICE)\n\n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence.to(DEVICE)).to(DEVICE)\n\n        return embeds\n\n\nclass BiLSTMWordEmbeddingLookup(nn.Module):\n\n    def __init__(self, vocabulary_size, word_embedding_dim, hidden_dim, num_layers, dropout, vector_path=None, non_static=False):\n        super(BiLSTMWordEmbeddingLookup, self).__init__()\n\n        self.vocabulary_size = vocabulary_size\n        self.num_layers = num_layers\n        self.word_embedding_dim = word_embedding_dim\n        self.hidden_dim = hidden_dim\n\n        self.output_dim = hidden_dim\n\n        self.word_embeddings = nn.Embedding(self.vocabulary_size, self.word_embedding_dim).to(DEVICE)\n        if vector_path:\n            logger.info(\'logging word vectors from {}\'.format(vector_path))\n            word_vectors = Vectors(vector_path).vectors\n            self.word_embeddings = self.word_embeddings.from_pretrained(word_vectors, freeze=not non_static).to(DEVICE)\n\n        self.lstm = nn.LSTM(self.word_embedding_dim, self.hidden_dim // 2, bidirectional=True, num_layers=num_layers, dropout=dropout).to(DEVICE)\n\n        self.hidden = self.init_hidden()\n\n    def forward(self, sentence):\n\n        # word embeddings\n        embeddings = self.word_embeddings(sentence)\n\n        # lstm hidden\n        # self.hidden = self.init_hidden()\n        lstm_hiddens, self.hidden = self.lstm(embeddings, self.hidden)\n        # print(lstm_hiddens.shape)\n        return lstm_hiddens\n\n    def init_hidden(self, batch_size=1):\n        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim // 2).to(DEVICE)\n\n        return h0, c0\n\n    def clear_hidden_state(self):\n        self.hidden = self.init_hidden()\n'"
lightnlp/sp/tdp/utils/__init__.py,0,b''
lightnlp/sp/tdp/utils/dataset.py,0,"b'from torchtext.data import Dataset, Example\n\n\nclass TransitionDataset(Dataset):\n    """"""Defines a Dataset of transition-based denpendency parsing format.\n    eg:\n    The bill intends ||| SHIFT SHIFT REDUCE_L SHIFT REDUCE_L\n    The bill intends ||| SHIFT SHIFT REDUCE_L SHIFT REDUCE_L\n    """"""\n\n    def __init__(self, path, fields, encoding=""utf-8"", separator=\' ||| \', **kwargs):\n        examples = []\n        with open(path, ""r"", encoding=encoding) as f:\n            \n            for inst in f:\n                sentence, actions = inst.split(separator)\n\n                # Make sure there is no leading/trailing whitespace\n                sentence = sentence.strip().split()\n                actions = actions.strip().split()\n\n                examples.append(Example.fromlist((sentence, actions), fields))\n        super(TransitionDataset, self).__init__(examples, fields, **kwargs)\n\n'"
lightnlp/sp/tdp/utils/feature_extractor.py,0,"b'class SimpleFeatureExtractor:\n\n    def get_features(self, parser_state, **kwargs):\n       \n        stack_len = 2\n        input_buffer_len = 1\n        stack_items = parser_state.stack_peek_n(stack_len)\n        input_buffer_items = parser_state.input_buffer_peek_n(input_buffer_len)\n        features = []\n        assert len(stack_items) == stack_len\n        assert len(input_buffer_items) == input_buffer_len\n        features.extend([x.embedding for x in stack_items])\n        features.extend([x.embedding for x in input_buffer_items])\n        return features\n'"
lightnlp/sp/tdp/utils/parser_state.py,0,"b'from collections import namedtuple\n\nfrom ..config import Actions, NULL_STACK_TOK\n\nDepGraphEdge = namedtuple(""DepGraphEdge"", [""head"", ""modifier""])\n\nStackEntry = namedtuple(""StackEntry"", [""headword"", ""headword_pos"", ""embedding""])\n\n\nclass ParserState:\n\n    def __init__(self, sentence, sentence_embs, combiner, null_stack_tok_embed=None):\n        self.combiner = combiner\n        self.curr_input_buff_idx = 0\n        self.input_buffer = [StackEntry(we[0], pos, we[1]) for pos, we in enumerate(zip(sentence, sentence_embs))]\n\n        self.stack = []\n        self.null_stack_tok_embed = null_stack_tok_embed\n\n    def shift(self):\n        next_item = self.input_buffer[self.curr_input_buff_idx]\n        self.stack.append(next_item)\n        self.curr_input_buff_idx += 1\n\n    def reduce_left(self):\n        return self._reduce(Actions.REDUCE_L)\n\n    def reduce_right(self):\n        return self._reduce(Actions.REDUCE_R)\n\n    def done_parsing(self):\n        if len(self.stack) == 1 and self.curr_input_buff_idx == len(self.input_buffer) - 1:\n            return True\n        else:\n            return False\n\n    def stack_len(self):\n        return len(self.stack)\n\n    def input_buffer_len(self):\n        return len(self.input_buffer) - self.curr_input_buff_idx\n\n    def stack_peek_n(self, n):\n        if len(self.stack) - n < 0:\n            return [StackEntry(NULL_STACK_TOK, -1, self.null_stack_tok_embed)] * (n - len(self.stack)) \\\n                   + self.stack[:]\n        return self.stack[-n:]\n\n    def input_buffer_peek_n(self, n):\n        assert self.curr_input_buff_idx + n - 1 <= len(self.input_buffer)\n        return self.input_buffer[self.curr_input_buff_idx:self.curr_input_buff_idx+n]\n\n    def _reduce(self, action):\n        assert len(self.stack) >= 2, ""ERROR: Cannot reduce with stack length less than 2""\n        \n        if action == Actions.REDUCE_L:\n            head = self.stack.pop()\n            modifier = self.stack.pop()\n        elif action == Actions.REDUCE_R:\n            modifier = self.stack.pop()\n            head = self.stack.pop()\n        head_embedding = self.combiner(head.embedding, modifier.embedding)\n        self.stack.append(StackEntry(head.headword, head.headword_pos, head_embedding))\n        return DepGraphEdge((head.headword, head.headword_pos), (modifier.headword, modifier.headword_pos))\n\n    def __str__(self):\n        return ""Stack: {}\\nInput Buffer: {}\\n"".format([entry.headword for entry in self.stack],\n                                                      [entry.headword for entry\n                                                       in self.input_buffer[self.curr_input_buff_idx:]])\n'"
lightnlp/sp/tdp/utils/vectors.py,7,"b'import torch\nimport torch.autograd as ag\n\nfrom ..config import DEVICE\n\ndef word_to_variable_embed(word, word_embeddings, word_to_ix):\n    return word_embeddings(torch.tensor([word_to_ix[word]]).to(DEVICE))\n\n\ndef sequence_to_variable(sequence, to_ix):\n    return torch.tensor([to_ix[t] for t in sequence]).to(DEVICE)\n\n\ndef to_scalar(var):\n    """"""\n    Wrap up the terse, obnoxious code to go from torch.Tensor to\n    a python int / float (is there a better way?)\n    """"""\n    if isinstance(var, ag.Variable):\n        return var.data.view(-1).tolist()[0]\n    else:\n        return var.view(-1).tolist()[0]\n\n\ndef argmax(vector):\n    """"""\n    Takes in a row vector (1xn) and returns its argmax\n    """"""\n    _, idx = torch.max(vector, 1)\n    return to_scalar(idx)\n\n\ndef concat_and_flatten(items):\n    """"""\n    Concatenate feature vectors together in a way that they can be handed into\n    a linear layer\n    :param items A list of ag.Variables which are vectors\n    :return One long row vector of all of the items concatenated together\n    """"""\n    return torch.cat(items, 1).view(1, -1)\n\n\ndef initialize_with_pretrained(pretrained_embeds, word_embedding_component):\n    """"""\n    Initialize the embedding lookup table of word_embedding_component with the embeddings\n    from pretrained_embeds.\n    Remember that word_embedding_component has a word_to_ix member you will have to use.\n    For every word that we do not have a pretrained embedding for, keep the default initialization.\n    :param pretrained_embeds dict mapping word to python list of floats (the embedding\n        of that word)\n    :param word_embedding_component The network component to initialize (i.e, a VanillaWordEmbeddingLookup\n        or BiLSTMWordEmbeddingLookup)\n    """"""\n    # STUDENT\n    for word, index in word_embedding_component.word_to_ix.items():\n        if word in pretrained_embeds:\n            word_embedding_component.word_embeddings.weight.data[index] = torch.Tensor(pretrained_embeds[word])\n    # END STUDENT\n'"
lightnlp/sr/ss/utils/__init__.py,0,b''
lightnlp/sr/ss/utils/pad.py,0,"b""def pad_sequnce(sequence, seq_length, pad_token='<pad>'):\n    padded_seq = sequence[:]\n    if len(padded_seq) < seq_length:\n        padded_seq.extend([pad_token for _ in range(len(padded_seq), seq_length)])\n    return padded_seq[:seq_length]\n"""
lightnlp/sr/te/utils/__init__.py,0,b''
lightnlp/sr/te/utils/pad.py,0,"b""def pad_sequnce(sequence, seq_length, pad_token='<pad>'):\n    padded_seq = sequence[:]\n    if len(padded_seq) < seq_length:\n        padded_seq.extend([pad_token for _ in range(len(padded_seq), seq_length)])\n    return padded_seq[:seq_length]\n"""
lightnlp/tc/re/utils/__init__.py,0,b''
lightnlp/tc/re/utils/dataset.py,0,"b'from torchtext.data import Dataset, Example\nfrom .preprocess import handle_line\n\n\nclass REDataset(Dataset):\n    """"""Defines a Dataset of relation extraction format.\n    eg:\n    \xe9\x92\xb1\xe9\x92\x9f\xe4\xb9\xa6\t\xe8\xbe\x9b\xe7\xac\x9b\t\xe5\x90\x8c\xe9\x97\xa8\t\xe4\xb8\x8e\xe8\xbe\x9b\xe7\xac\x9b\xe4\xba\xac\xe6\xb2\xaa\xe5\x94\xb1\xe5\x92\x8c\xe8\x81\xbd\xe9\x92\xb1\xe9\x92\x9f\xe4\xb9\xa6\xe4\xb8\x8e\xe9\x92\xb1\xe9\x92\x9f\xe4\xb9\xa6\xe6\x98\xaf\xe6\xb8\x85\xe5\x8d\x8e\xe6\xa0\xa1\xe5\x8f\x8b\xef\xbc\x8c\xe9\x92\xb1\xe9\x92\x9f\xe4\xb9\xa6\xe9\xab\x98\xe8\xbe\x9b\xe7\xac\x9b\xe4\xb8\xa4\xe7\x8f\xad\xe3\x80\x82\n    \xe5\x85\x83\xe6\xad\xa6\t\xe5\x85\x83\xe5\x8d\x8e\tunknown\t\xe4\xba\x8e\xe5\xb8\x88\xe5\x82\x85\xe5\x9c\xa8\xe4\xb8\x80\xe6\xac\xa1\xe4\xba\xac\xe5\x89\xa7\xe8\xa1\xa8\xe6\xbc\x94\xe4\xb8\xad\xef\xbc\x8c\xe9\x80\x89\xe4\xba\x86\xe5\x85\x83\xe9\xbe\x99\xef\xbc\x88\xe6\xb4\xaa\xe9\x87\x91\xe5\xae\x9d\xef\xbc\x89\xe3\x80\x81\xe5\x85\x83\xe6\xa5\xbc\xef\xbc\x88\xe5\x85\x83\xe5\xa5\x8e\xef\xbc\x89\xe3\x80\x81\xe5\x85\x83\xe5\xbd\xaa\xe3\x80\x81\xe6\x88\x90\xe9\xbe\x99\xe3\x80\x81\xe5\x85\x83\xe5\x8d\x8e\xe3\x80\x81\xe5\x85\x83\xe6\xad\xa6\xe3\x80\x81\xe5\x85\x83\xe6\xb3\xb07\xe4\xba\xba\xe6\x8b\x85\xe4\xbb\xbb\xe4\xb8\x83\xe5\xb0\x8f\xe7\xa6\x8f\xe7\x9a\x84\xe4\xb8\xbb\xe8\xa7\x92\xe3\x80\x82\n    """"""\n\n    def __init__(self, path, fields, encoding=""utf-8"", **kwargs):\n        examples = []\n        with open(path, ""r"", encoding=encoding) as f:\n            for line in f:\n                chunks = line.split()\n                entity_1, entity_2, relation, sentence = tuple(chunks)\n                sentence_list = handle_line(entity_1, entity_2, sentence)\n\n                examples.append(Example.fromlist((sentence_list, relation), fields))\n        super(REDataset, self).__init__(examples, fields, **kwargs)\n\n'"
lightnlp/tc/re/utils/preprocess.py,0,"b""import jieba\n\n\ndef handle_line(entity1, entity2, sentence, begin_e1_token='<e1>', end_e1_token='</e1>', begin_e2_token='<e2>',\n                end_e2_token='</e2>'):\n    assert entity1 in sentence\n    assert entity2 in sentence\n    sentence = sentence.replace(entity1, begin_e1_token + entity1 + end_e1_token)\n    sentence = sentence.replace(entity2, begin_e2_token + entity2 + end_e2_token)\n    sentence = ' '.join(jieba.cut(sentence))\n    sentence = sentence.replace('< e1 >', begin_e1_token)\n    sentence = sentence.replace('< / e1 >', end_e1_token)\n    sentence = sentence.replace('< e2 >', begin_e2_token)\n    sentence = sentence.replace('< / e2 >', end_e2_token)\n    return sentence.split()\n\n\nif __name__ == '__main__':\n    test_str = '\xe6\x9b\xbe\xe7\xbb\x8f\xe6\xb2\xa7\xe6\xb5\xb7\xe9\x9a\xbe\xe4\xb8\xba\xe6\xb0\xb4\xe5\x93\x88\xe5\x93\x88\xe8\xb0\x81\xe8\xaf\xb4\xe4\xb8\x8d\xe6\x98\xaf\xe5\x91\xa2\xef\xbc\x9f\xef\xbc\x81\xe5\x91\xb5\xe5\x91\xb5 \xe4\xbd\x8e\xe5\xa4\xb4\xe4\xb8\x8d\xe8\xa7\x81\xe6\x8a\xac\xe5\xa4\xb4\xe8\xa7\x81'\n    e1 = '\xe6\xb2\xa7\xe6\xb5\xb7'\n    e2 = '\xe4\xb8\x8d\xe6\x98\xaf'\n    print(list(jieba.cut(test_str)))\n    print(handle_line(e1, e2, test_str))\n\n"""
lightnlp/tc/sa/utils/__init__.py,0,b''
lightnlp/tc/sa/utils/pad.py,0,"b""def pad_sequnce(sequence, seq_length, pad_token='<pad>'):\n    padded_seq = sequence[:]\n    if len(padded_seq) < seq_length:\n        padded_seq.extend([pad_token for i in range(len(padded_seq), seq_length)])\n    return padded_seq\n"""
lightnlp/tg/cb/models/__init__.py,0,b''
lightnlp/tg/cb/models/attention.py,10,"b'import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nfrom ..config import DEVICE, DEFAULT_CONFIG\n\n\nclass Attention(nn.Module):\n    """"""\n    several score types like dot,general and concat\n    """"""\n    def __init__(self, method=\'dot\', hidden_size=None):\n        super(Attention, self).__init__()\n        self.method = method\n        if self.method != \'dot\':\n            self.hidden_size = hidden_size\n            if self.method == \'general\':\n                self.W = nn.Linear(hidden_size, hidden_size)\n            elif self.method == \'concat\':\n                self.W = nn.Linear(self.hidden_size * 2, hidden_size)\n                self.v = nn.Parameter(torch.rand(1, hidden_size))  # \xe6\xad\xa4\xe5\xa4\x84\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xbaLinear\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\n                nn.init.xavier_normal_(self.v.data)\n\n    def forward(self, query, key, value, mask=None, dropout=0):\n        if self.method == \'general\':\n            scores = self.general(query, key)\n        elif self.method == \'concat\':\n            scores = self.concat(query, key)\n        else:\n            scores = self.dot(query, key)\n\n        # normalize\n        # scores = scores / math.sqrt(query.size(-1))\n\n        # mask\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        # softmax\n        p_attn = F.softmax(scores, dim=-1)\n\n        # dropout\n        if not dropout:\n            p_attn = F.dropout(p_attn, dropout)\n\n        return torch.matmul(p_attn, value), p_attn\n\n    def dot(self, query, key):\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        return scores\n\n    def general(self, query, key):\n        scores = torch.matmul(self.W(query), key.transpose(-2, -1))\n        return scores\n\n    def concat(self, query, key):\n        scores = torch.cat((query.expand(-1, key.size(1), -1), key), dim=2)\n        scores = self.W(scores)\n        scores = F.tanh(scores)\n        scores = torch.matmul(scores, self.v.t()).transpose(-2, -1)\n        return scores\n\n\nif __name__ == \'__main__\':\n    dim = 10\n    key_size = 5\n    batch_size = 4\n    q = torch.rand((batch_size, 1, dim))\n    k = torch.rand(batch_size, key_size, dim)\n    v = k\n    # dot\n    # attention = Attention()\n    # general\n    # attention = Attention(\'general\', dim)\n    # concat\n    attention = Attention(\'concat\', dim)\n    output, score = attention(q, k, v)\n    print(output)\n    print(output.shape)\n    print(score)\n    print(score.shape)\n'"
lightnlp/tg/cb/models/decoder.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .attention import Attention\nfrom ..config import DEVICE, DEFAULT_CONFIG\n\n\nclass Decoder(nn.Module):\n    def __init__(self, embed_size, hidden_size, output_size,\n                 n_layers=1, dropout=0.2, method='dot'):\n        super(Decoder, self).__init__()\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n\n        self.embed = nn.Embedding(output_size, embed_size).to(DEVICE)\n        self.dropout = nn.Dropout(dropout, inplace=True).to(DEVICE)\n        self.attention = Attention(method, hidden_size).to(DEVICE)\n        self.gru = nn.GRU(hidden_size + embed_size, hidden_size,\n                          n_layers, dropout=dropout, batch_first=True).to(DEVICE)\n        self.out = nn.Linear(hidden_size * 2, output_size).to(DEVICE)\n\n    def forward(self, word, last_hidden, encoder_outputs):\n        # Get the embedding of the current input word (last output word)\n        embedded = self.embed(word).unsqueeze(1)  # (B,1,N)\n        embedded = self.dropout(embedded)\n\n        # Calculate attention weights and apply to encoder outputs\n        context, attn_weights = self.attention(last_hidden[-1].unsqueeze(1), encoder_outputs, encoder_outputs)\n        # Combine embedded input word and attended context, run through RNN\n        context = F.relu(context)\n        rnn_input = torch.cat((embedded, context), 2)\n        output, hidden = self.gru(rnn_input, last_hidden)\n        output = output.squeeze(1)  # (B,1,N) -> (B,N)\n        context = context.squeeze(1)\n        output = torch.cat((output, context), 1)\n        output = self.out(output)\n        # output = F.softmax(output, dim=1)\n        return output, hidden, attn_weights\n"""
lightnlp/tg/cb/models/encoder.py,2,"b'import torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom ..config import DEVICE, DEFAULT_CONFIG\n\n\nclass Encoder(nn.Module):\n    """"""\n    basic GRU encoder\n    """"""\n    def __init__(self, input_size, embed_size, hidden_size,\n                 n_layers=1, dropout=0.5):\n        super(Encoder, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.embed_size = embed_size\n        self.embed = nn.Embedding(input_size, embed_size).to(DEVICE)\n        self.gru = nn.GRU(embed_size, hidden_size, n_layers, batch_first=True,\n                          dropout=dropout, bidirectional=True).to(DEVICE)\n\n    def forward(self, sentences, lengths, hidden=None):\n        embedded = self.embed(sentences)\n        packed = pack_padded_sequence(embedded, lengths, batch_first=True)\n        outputs, hidden = self.gru(packed, hidden)\n        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n        # outputs, hidden = self.gru(embedded, hidden)\n        # sum bidirectional outputs\n        outputs = (outputs[:, :, :self.hidden_size] +\n                   outputs[:, :, self.hidden_size:])\n        return outputs, hidden\n'"
lightnlp/tg/cb/models/seq2seq.py,4,"b'import random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..config import DEVICE, DEFAULT_CONFIG\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, src_lens, trg, teacher_forcing_ratio=0.5):\n        batch_size = trg.size(0)\n        max_len = trg.size(1)\n        trg_vocab_size = self.decoder.output_size\n\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(DEVICE)\n\n        encoder_output, hidden = self.encoder(src, src_lens)\n        hidden = hidden[:self.decoder.n_layers]\n        decoder_input = trg.data[:, 0]  # sos\n        for t in range(1, max_len):\n            output, hidden, attn_weights = self.decoder(\n                decoder_input, hidden, encoder_output)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.data.max(1)[1]\n            if teacher_force:\n                decoder_input = trg.data[:, t].clone().detach().to(DEVICE)\n            else:\n                decoder_input = top1.to(DEVICE)\n        return outputs\n\n    def predict(self, src, src_lens, sos, max_len):\n        batch_size = src.size(0)\n        trg_vocab_size = self.decoder.output_size\n\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(DEVICE)\n\n        encoder_output, hidden = self.encoder(src, src_lens)\n        hidden = hidden[:self.decoder.n_layers]\n        decoder_input = sos  # sos\n        for t in range(1, max_len):\n            output, hidden, attn_weights = self.decoder(\n                decoder_input, hidden, encoder_output)\n            outputs[t] = output\n            top1 = output.data.max(1)[1]\n            decoder_input = top1.to(DEVICE)\n        return outputs\n'"
lightnlp/tg/mt/models/__init__.py,0,b''
lightnlp/tg/mt/models/attention.py,10,"b'import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nfrom ..config import DEVICE, DEFAULT_CONFIG\n\n\nclass Attention(nn.Module):\n    """"""\n    several score types like dot,general and concat\n    """"""\n    def __init__(self, method=\'dot\', hidden_size=None):\n        super(Attention, self).__init__()\n        self.method = method\n        if self.method != \'dot\':\n            self.hidden_size = hidden_size\n            if self.method == \'general\':\n                self.W = nn.Linear(hidden_size, hidden_size)\n            elif self.method == \'concat\':\n                self.W = nn.Linear(self.hidden_size * 2, hidden_size)\n                self.v = nn.Parameter(torch.rand(1, hidden_size))  # \xe6\xad\xa4\xe5\xa4\x84\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xbaLinear\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\n                nn.init.xavier_normal_(self.v.data)\n\n    def forward(self, query, key, value, mask=None, dropout=0):\n        if self.method == \'general\':\n            scores = self.general(query, key)\n        elif self.method == \'concat\':\n            scores = self.concat(query, key)\n        else:\n            scores = self.dot(query, key)\n\n        # normalize\n        # scores = scores / math.sqrt(query.size(-1))\n\n        # mask\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        # softmax\n        p_attn = F.softmax(scores, dim=-1)\n\n        # dropout\n        if not dropout:\n            p_attn = F.dropout(p_attn, dropout)\n\n        return torch.matmul(p_attn, value), p_attn\n\n    def dot(self, query, key):\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        return scores\n\n    def general(self, query, key):\n        scores = torch.matmul(self.W(query), key.transpose(-2, -1))\n        return scores\n\n    def concat(self, query, key):\n        scores = torch.cat((query.expand(-1, key.size(1), -1), key), dim=2)\n        scores = self.W(scores)\n        scores = F.tanh(scores)\n        scores = torch.matmul(scores, self.v.t()).transpose(-2, -1)\n        return scores\n\n\nif __name__ == \'__main__\':\n    dim = 10\n    key_size = 5\n    batch_size = 4\n    q = torch.rand((batch_size, 1, dim))\n    k = torch.rand(batch_size, key_size, dim)\n    v = k\n    # dot\n    # attention = Attention()\n    # general\n    # attention = Attention(\'general\', dim)\n    # concat\n    attention = Attention(\'concat\', dim)\n    output, score = attention(q, k, v)\n    print(output)\n    print(output.shape)\n    print(score)\n    print(score.shape)\n'"
lightnlp/tg/mt/models/decoder.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .attention import Attention\nfrom ..config import DEVICE, DEFAULT_CONFIG\n\n\nclass Decoder(nn.Module):\n    def __init__(self, embed_size, hidden_size, output_size,\n                 n_layers=1, dropout=0.2, method='dot'):\n        super(Decoder, self).__init__()\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n\n        self.embed = nn.Embedding(output_size, embed_size).to(DEVICE)\n        self.dropout = nn.Dropout(dropout, inplace=True).to(DEVICE)\n        self.attention = Attention(method, hidden_size).to(DEVICE)\n        self.gru = nn.GRU(hidden_size + embed_size, hidden_size,\n                          n_layers, dropout=dropout, batch_first=True).to(DEVICE)\n        self.out = nn.Linear(hidden_size * 2, output_size).to(DEVICE)\n\n    def forward(self, word, last_hidden, encoder_outputs):\n        # Get the embedding of the current input word (last output word)\n        embedded = self.embed(word).unsqueeze(1)  # (B,1,N)\n        embedded = self.dropout(embedded)\n\n        # Calculate attention weights and apply to encoder outputs\n        context, attn_weights = self.attention(last_hidden[-1].unsqueeze(1), encoder_outputs, encoder_outputs)\n        # Combine embedded input word and attended context, run through RNN\n        context = F.relu(context)\n        rnn_input = torch.cat((embedded, context), 2)\n        output, hidden = self.gru(rnn_input, last_hidden)\n        output = output.squeeze(1)  # (B,1,N) -> (B,N)\n        context = context.squeeze(1)\n        output = torch.cat((output, context), 1)\n        output = self.out(output)\n        # output = F.softmax(output, dim=1)\n        return output, hidden, attn_weights\n"""
lightnlp/tg/mt/models/encoder.py,2,"b'import torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom ..config import DEVICE, DEFAULT_CONFIG\n\n\nclass Encoder(nn.Module):\n    """"""\n    basic GRU encoder\n    """"""\n    def __init__(self, input_size, embed_size, hidden_size,\n                 n_layers=1, dropout=0.5):\n        super(Encoder, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.embed_size = embed_size\n        self.embed = nn.Embedding(input_size, embed_size).to(DEVICE)\n        self.gru = nn.GRU(embed_size, hidden_size, n_layers, batch_first=True,\n                          dropout=dropout, bidirectional=True).to(DEVICE)\n\n    def forward(self, sentences, lengths, hidden=None):\n        embedded = self.embed(sentences)\n        packed = pack_padded_sequence(embedded, lengths, batch_first=True)\n        outputs, hidden = self.gru(packed, hidden)\n        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n        # outputs, hidden = self.gru(embedded, hidden)\n        # sum bidirectional outputs\n        outputs = (outputs[:, :, :self.hidden_size] +\n                   outputs[:, :, self.hidden_size:])\n        return outputs, hidden\n'"
lightnlp/tg/mt/models/seq2seq.py,4,"b'import random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..config import DEVICE, DEFAULT_CONFIG\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, src_lens, trg, teacher_forcing_ratio=0.5):\n        batch_size = trg.size(0)\n        max_len = trg.size(1)\n        trg_vocab_size = self.decoder.output_size\n\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(DEVICE)\n\n        encoder_output, hidden = self.encoder(src, src_lens)\n        hidden = hidden[:self.decoder.n_layers]\n        decoder_input = trg.data[:, 0]  # sos\n        for t in range(1, max_len):\n            output, hidden, attn_weights = self.decoder(\n                decoder_input, hidden, encoder_output)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.data.max(1)[1]\n            if teacher_force:\n                decoder_input = trg.data[:, t].clone().detach().to(DEVICE)\n            else:\n                decoder_input = top1.to(DEVICE)\n        return outputs\n\n    def predict(self, src, src_lens, sos, max_len):\n        batch_size = src.size(0)\n        trg_vocab_size = self.decoder.output_size\n\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(DEVICE)\n\n        encoder_output, hidden = self.encoder(src, src_lens)\n        hidden = hidden[:self.decoder.n_layers]\n        decoder_input = sos  # sos\n        for t in range(1, max_len):\n            output, hidden, attn_weights = self.decoder(\n                decoder_input, hidden, encoder_output)\n            outputs[t] = output\n            top1 = output.data.max(1)[1]\n            decoder_input = top1.to(DEVICE)\n        return outputs\n'"
lightnlp/tg/ts/models/__init__.py,0,b''
lightnlp/tg/ts/models/attention.py,10,"b'import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nfrom ..config import DEVICE, DEFAULT_CONFIG\n\n\nclass Attention(nn.Module):\n    """"""\n    several score types like dot,general and concat\n    """"""\n    def __init__(self, method=\'dot\', hidden_size=None):\n        super(Attention, self).__init__()\n        self.method = method\n        if self.method != \'dot\':\n            self.hidden_size = hidden_size\n            if self.method == \'general\':\n                self.W = nn.Linear(hidden_size, hidden_size)\n            elif self.method == \'concat\':\n                self.W = nn.Linear(self.hidden_size * 2, hidden_size)\n                self.v = nn.Parameter(torch.rand(1, hidden_size))  # \xe6\xad\xa4\xe5\xa4\x84\xe5\xae\x9a\xe4\xb9\x89\xe4\xb8\xbaLinear\xe4\xb9\x9f\xe5\x8f\xaf\xe4\xbb\xa5\n                nn.init.xavier_normal_(self.v.data)\n\n    def forward(self, query, key, value, mask=None, dropout=0):\n        if self.method == \'general\':\n            scores = self.general(query, key)\n        elif self.method == \'concat\':\n            scores = self.concat(query, key)\n        else:\n            scores = self.dot(query, key)\n\n        # normalize\n        # scores = scores / math.sqrt(query.size(-1))\n\n        # mask\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        # softmax\n        p_attn = F.softmax(scores, dim=-1)\n\n        # dropout\n        if not dropout:\n            p_attn = F.dropout(p_attn, dropout)\n\n        return torch.matmul(p_attn, value), p_attn\n\n    def dot(self, query, key):\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        return scores\n\n    def general(self, query, key):\n        scores = torch.matmul(self.W(query), key.transpose(-2, -1))\n        return scores\n\n    def concat(self, query, key):\n        scores = torch.cat((query.expand(-1, key.size(1), -1), key), dim=2)\n        scores = self.W(scores)\n        scores = F.tanh(scores)\n        scores = torch.matmul(scores, self.v.t()).transpose(-2, -1)\n        return scores\n\n\nif __name__ == \'__main__\':\n    dim = 10\n    key_size = 5\n    batch_size = 4\n    q = torch.rand((batch_size, 1, dim))\n    k = torch.rand(batch_size, key_size, dim)\n    v = k\n    # dot\n    # attention = Attention()\n    # general\n    # attention = Attention(\'general\', dim)\n    # concat\n    attention = Attention(\'concat\', dim)\n    output, score = attention(q, k, v)\n    print(output)\n    print(output.shape)\n    print(score)\n    print(score.shape)\n'"
lightnlp/tg/ts/models/decoder.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .attention import Attention\nfrom ..config import DEVICE, DEFAULT_CONFIG\n\n\nclass Decoder(nn.Module):\n    def __init__(self, embed_size, hidden_size, output_size,\n                 n_layers=1, dropout=0.2, method='dot'):\n        super(Decoder, self).__init__()\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n\n        self.embed = nn.Embedding(output_size, embed_size).to(DEVICE)\n        self.dropout = nn.Dropout(dropout, inplace=True).to(DEVICE)\n        self.attention = Attention(method, hidden_size).to(DEVICE)\n        self.gru = nn.GRU(hidden_size + embed_size, hidden_size,\n                          n_layers, dropout=dropout, batch_first=True).to(DEVICE)\n        self.out = nn.Linear(hidden_size * 2, output_size).to(DEVICE)\n\n    def forward(self, word, last_hidden, encoder_outputs):\n        # Get the embedding of the current input word (last output word)\n        embedded = self.embed(word).unsqueeze(1)  # (B,1,N)\n        embedded = self.dropout(embedded)\n\n        # Calculate attention weights and apply to encoder outputs\n        context, attn_weights = self.attention(last_hidden[-1].unsqueeze(1), encoder_outputs, encoder_outputs)\n        # Combine embedded input word and attended context, run through RNN\n        context = F.relu(context)\n        rnn_input = torch.cat((embedded, context), 2)\n        output, hidden = self.gru(rnn_input, last_hidden)\n        output = output.squeeze(1)  # (B,1,N) -> (B,N)\n        context = context.squeeze(1)\n        output = torch.cat((output, context), 1)\n        output = self.out(output)\n        # output = F.softmax(output, dim=1)\n        return output, hidden, attn_weights\n"""
lightnlp/tg/ts/models/encoder.py,2,"b'import torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nfrom ..config import DEVICE, DEFAULT_CONFIG\n\n\nclass Encoder(nn.Module):\n    """"""\n    basic GRU encoder\n    """"""\n    def __init__(self, input_size, embed_size, hidden_size,\n                 n_layers=1, dropout=0.5):\n        super(Encoder, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.embed_size = embed_size\n        self.embed = nn.Embedding(input_size, embed_size).to(DEVICE)\n        self.gru = nn.GRU(embed_size, hidden_size, n_layers, batch_first=True,\n                          dropout=dropout, bidirectional=True).to(DEVICE)\n\n    def forward(self, sentences, lengths, hidden=None):\n        embedded = self.embed(sentences)\n        packed = pack_padded_sequence(embedded, lengths, batch_first=True)\n        outputs, hidden = self.gru(packed, hidden)\n        outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n        # outputs, hidden = self.gru(embedded, hidden)\n        # sum bidirectional outputs\n        outputs = (outputs[:, :, :self.hidden_size] +\n                   outputs[:, :, self.hidden_size:])\n        return outputs, hidden\n'"
lightnlp/tg/ts/models/seq2seq.py,4,"b'import random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..config import DEVICE, DEFAULT_CONFIG\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, src, src_lens, trg, teacher_forcing_ratio=0.5):\n        batch_size = trg.size(0)\n        max_len = trg.size(1)\n        trg_vocab_size = self.decoder.output_size\n\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(DEVICE)\n\n        encoder_output, hidden = self.encoder(src, src_lens)\n        hidden = hidden[:self.decoder.n_layers]\n        decoder_input = trg.data[:, 0]  # sos\n        for t in range(1, max_len):\n            output, hidden, attn_weights = self.decoder(\n                decoder_input, hidden, encoder_output)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.data.max(1)[1]\n            if teacher_force:\n                decoder_input = trg.data[:, t].clone().detach().to(DEVICE)\n            else:\n                decoder_input = top1.to(DEVICE)\n        return outputs\n\n    def predict(self, src, src_lens, sos, max_len):\n        batch_size = src.size(0)\n        trg_vocab_size = self.decoder.output_size\n\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(DEVICE)\n\n        encoder_output, hidden = self.encoder(src, src_lens)\n        hidden = hidden[:self.decoder.n_layers]\n        decoder_input = sos  # sos\n        for t in range(1, max_len):\n            output, hidden, attn_weights = self.decoder(\n                decoder_input, hidden, encoder_output)\n            outputs[t] = output\n            top1 = output.data.max(1)[1]\n            decoder_input = top1.to(DEVICE)\n        return outputs\n'"
lightnlp/we/cbow/base/__init__.py,0,b''
lightnlp/we/cbow/base/model.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ....base.model import BaseModel\n\nfrom ..config import DEVICE\n\n\nclass CBOWBase(BaseModel):\n    def __init__(self, args):\n        super(CBOWBase, self).__init__(args)\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n\n        self.word_embeddings = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n        self.linear = nn.Linear(self.embedding_dimension, self.vocabulary_size).to(DEVICE)\n\n    def forward(self, context):\n        context_embedding = torch.sum(self.word_embeddings(context), dim=1)\n        target_embedding = self.linear(context_embedding)\n        return target_embedding\n\n    def loss(self, context, target):\n        context_embedding = torch.sum(self.word_embeddings(context), dim=1)\n        target_embedding = self.linear(context_embedding)\n        return F.cross_entropy(target_embedding, target.view(-1))\n'"
lightnlp/we/cbow/base/module.py,10,"b'import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom typing import List\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ....utils.deploy import get_free_tcp_port\nfrom ....utils.learning import adjust_learning_rate\nfrom ....utils.log import logger\nfrom ....base.module import Module\n\nfrom ..model import Config\nfrom .model import CBOWBase\nfrom ..config import DEVICE, DEFAULT_CONFIG\nfrom ..tool import cbow_tool, WORD\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass CBOWBaseModule(Module):\n    """"""\n    """"""\n\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self.model_type = None\n\n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = cbow_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = cbow_tool.get_dataset(dev_path)\n            word_vocab = cbow_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab = cbow_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        train_iter = cbow_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, save_path=save_path, **kwargs)\n        self.model_type = config.feature\n\n        cbow = CBOWBase(config)\n        self._model = cbow\n        optim = torch.optim.Adam(cbow.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            cbow.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                optim.zero_grad()\n                item_loss = self._model.loss(item.context, item.target)\n                acc_loss += item_loss.item()\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'cbow_base_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'cbow_base_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        cbow.save()\n\n    def predict(self, context: List[str], topk=3):\n        self._model.eval()\n\n        vec_context = torch.tensor([self._word_vocab.stoi[x] for x in context])\n        vec_context = vec_context.reshape(1, -1).to(DEVICE)\n        vec_predict = self._model(vec_context)[0]\n        vec_score = F.softmax(vec_predict, dim=0)\n        vec_prob, vec_index = torch.topk(vec_score, topk, dim=0)\n        vec_prob = vec_prob.cpu().tolist()\n        vec_index = [self._word_vocab.itos[i] for i in vec_index]\n\n        return list(zip(vec_index, vec_prob))\n\n    def evaluate(self, context, target):\n        self._model.eval()\n\n        vec_context = torch.tensor([self._word_vocab.stoi[x] for x in context])\n        vec_context = vec_context.reshape(1, -1).to(DEVICE)\n        vec_target = torch.tensor([self._word_vocab.stoi[target]])\n        vec_target = vec_target.reshape(1, -1).to(DEVICE)\n        score = torch.exp(- self._model.loss(vec_context, vec_target)).item()\n\n        return score\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        cbow = CBOWBase(config)\n        cbow.load()\n        self._model = cbow\n        self._word_vocab = config.word_vocab\n        self.model_type = config.feature\n        self._check_vocab()\n\n    def save_embeddings(self, save_path: str, save_mode=\'word2vec\'):\n        embeddings = self._model.word_embeddings.weight.data.cpu().numpy()\n        with open(save_path, \'w\', encoding=\'utf8\') as f:\n            if save_mode == \'word2vec\':\n                f.write(\'{} {}\\n\'.format(self._model.vocabulary_size, self._model.embedding_dimension))\n            elif save_mode == \'glove\':\n                pass\n            for word, word_id in self._word_vocab.stoi.items():\n                if word == \' \':\n                    continue\n                word_embedding = embeddings[word_id]\n                word_embedding = \' \'.join(map(lambda x: str(x), word_embedding))\n                f.write(\'{} {}\\n\'.format(word, word_embedding))\n            logger.info(\'succeed saving embeddings data to {}\'.format(save_path))\n\n    def test(self, test_path):\n        self._model.eval()\n        test_dataset = cbow_tool.get_dataset(test_path)\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n\n    def _validate(self, dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\']):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = cbow_tool.get_iterator(dev_dataset, batch_size=batch_size)\n        for dev_item in tqdm(dev_iter):\n            item_score = self._model.loss(dev_item.context, dev_item.target).item()\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def _check_vocab(self):\n        if not hasattr(WORD, \'vocab\'):\n            WORD.vocab = self._word_vocab\n\n    def deploy(self, route_path=""/cbow"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/predict"", methods=[\'POST\', \'GET\'])\n        def predict():\n            print(request.args)\n            words = request.args.get(\'words\', [])\n            result = self.predict(words)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'word\': result\n                        }\n                })\n\n        @app.route(route_path + ""/evaluate"", methods=[\'POST\', \'GET\'])\n        def evaluate():\n            print(request.args)\n            context = request.args.get(\'context\', \'\')\n            word = request.args.get(\'word\', \'\')\n            result = self.evaluate(context, word)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'score\': result\n                        }\n                })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/we/cbow/hierarchical_softmax/__init__.py,0,b''
lightnlp/we/cbow/hierarchical_softmax/model.py,16,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ....base.model import BaseModel\n\nfrom ..config import DEVICE\n\n\nclass CBOWHierarchicalSoftmax(BaseModel):\n    def __init__(self, args):\n        super(CBOWHierarchicalSoftmax, self).__init__(args)\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n\n        self.word_embeddings = nn.Embedding(2*self.vocabulary_size-1, self.embedding_dimension,\n                                            sparse=True).to(DEVICE)\n        self.context_embeddings = nn.Embedding(2*self.vocabulary_size-1, self.embedding_dimension,\n                                               sparse=True).to(DEVICE)\n\n    def forward(self, pos_context, pos_path, neg_context, neg_path):\n        pos_context_embedding = torch.sum(self.word_embeddings(pos_context), dim=1, keepdim=True)\n        pos_path_embedding = self.context_embeddings(pos_path)\n        pos_score = torch.bmm(pos_context_embedding, pos_path_embedding.transpose(2, 1)).squeeze()\n        neg_context_embedding = torch.sum(self.word_embeddings(neg_context), dim=1, keepdim=True)\n        neg_path_embedding = self.context_embeddings(neg_path)\n        neg_score = torch.bmm(neg_context_embedding, neg_path_embedding.transpose(2, 1)).squeeze()\n        pos_sigmoid_score = torch.lt(torch.sigmoid(pos_score), 0.5)\n        neg_sigmoid_score = torch.gt(torch.sigmoid(neg_score), 0.5)\n        sigmoid_score = torch.cat((pos_sigmoid_score, neg_sigmoid_score))\n        sigmoid_score = torch.sum(sigmoid_score, dim=0).item() / sigmoid_score.size(0)\n        return sigmoid_score\n\n    def loss(self, pos_context, pos_path, neg_context, neg_path):\n        pos_context_embedding = torch.sum(self.word_embeddings(pos_context), dim=1, keepdim=True)\n        pos_path_embedding = self.context_embeddings(pos_path)\n        pos_score = torch.bmm(pos_context_embedding, pos_path_embedding.transpose(2, 1)).squeeze()\n        neg_context_embedding = torch.sum(self.word_embeddings(neg_context), dim=1, keepdim=True)\n        neg_path_embedding = self.context_embeddings(neg_path)\n        neg_score = torch.bmm(neg_context_embedding, neg_path_embedding.transpose(2, 1)).squeeze()\n        pos_score = torch.sum(F.logsigmoid(-1 * pos_score))\n        neg_score = torch.sum(F.logsigmoid(neg_score))\n        loss = -1 * (pos_score + neg_score)\n        return loss\n\n'"
lightnlp/we/cbow/hierarchical_softmax/module.py,20,"b'import torch\nfrom tqdm import tqdm\nfrom typing import List\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ....utils.deploy import get_free_tcp_port\nfrom ....utils.learning import adjust_learning_rate\nfrom ....utils.log import logger\nfrom ....base.module import Module\n\nfrom ..model import Config\nfrom .model import CBOWHierarchicalSoftmax\nfrom ..config import DEVICE, DEFAULT_CONFIG\nfrom ..tool import cbow_tool, WORD\nfrom ..utils.huffman_tree import HuffmanTree\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass CBOWHierarchicalSoftmaxModule(Module):\n    """"""\n    """"""\n\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self.model_type = None\n        self.huffman_tree = None\n        self.huffman_pos_path = None\n        self.huffman_neg_path = None\n\n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = cbow_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = cbow_tool.get_dataset(dev_path)\n            word_vocab = cbow_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab = cbow_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        train_iter = cbow_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, save_path=save_path, **kwargs)\n        self.model_type = config.feature\n\n        cbow = CBOWHierarchicalSoftmax(config)\n\n        self._model = cbow\n        word_id_frequency_dict = {self._word_vocab.stoi[s]: self._word_vocab.freqs[s] for s in self._word_vocab.stoi}\n        self.huffman_tree = HuffmanTree(word_id_frequency_dict)\n        self.huffman_pos_path, self.huffman_neg_path = self.huffman_tree.get_all_pos_and_neg_path()\n        optim = torch.optim.SparseAdam(cbow.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            cbow.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                optim.zero_grad()\n                pos_pairs = []\n                neg_pairs = []\n                for i in range(item.batch_size):\n                    pos_path = self.huffman_pos_path[item.target[i]]\n                    neg_path = self.huffman_neg_path[item.target[i]]\n                    pos_pairs.extend(zip([item.context[i]] * len(pos_path), pos_path))\n                    neg_pairs.extend(zip([item.context[i]] * len(neg_path), neg_path))\n                pos_context_vec = torch.cat(tuple(pair[0].view(1, -1) for pair in pos_pairs), dim=0).to(DEVICE)\n                pos_path_vec = torch.tensor([[pair[1]] for pair in pos_pairs]).to(DEVICE)\n                neg_context_vec = torch.cat(tuple(pair[0].view(1, -1) for pair in neg_pairs), dim=0).to(DEVICE)\n                neg_path_vec = torch.tensor([[pair[1]] for pair in neg_pairs]).to(DEVICE)\n\n                item_loss = self._model.loss(pos_context_vec, pos_path_vec, neg_context_vec, neg_path_vec)\n                acc_loss += item_loss.item()\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'cbow_hs_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'cbow_hs_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        cbow.save()\n\n    def predict(self, context: List[str], topk=3):\n        self._model.eval()\n        vec_context = torch.tensor([self._word_vocab.stoi[x] for x in context])\n        vec_context = vec_context.reshape(1, -1).to(DEVICE)\n        vec_target = torch.tensor([i for i in range(len(self._word_vocab.itos))])\n        vec_target = vec_target.reshape(1, -1).to(DEVICE)\n        vec_predict = self._model(vec_context, vec_target).squeeze()\n        vec_prob, vec_index = torch.topk(vec_predict, topk, dim=0)\n        vec_prob = vec_prob.cpu().tolist()\n        vec_index = [self._word_vocab.itos[i] for i in vec_index]\n        return list(zip(vec_index, vec_prob))\n\n    def evaluate(self, context, target):\n        self._model.eval()\n        vec_context = torch.tensor([self._word_vocab.stoi[x] for x in context])\n        vec_context = vec_context.to(DEVICE)\n        pos_path = self.huffman_pos_path[self._word_vocab.stoi[target]]\n        neg_path = self.huffman_neg_path[self._word_vocab.stoi[target]]\n        pos_pairs = list(zip([vec_context] * len(pos_path), pos_path))\n        neg_pairs = list(zip([vec_context] * len(neg_path), neg_path))\n        pos_context_vec = torch.cat(tuple(pair[0].view(1, -1) for pair in pos_pairs), dim=0).to(DEVICE)\n        pos_path_vec = torch.tensor([[pair[1]] for pair in pos_pairs]).to(DEVICE)\n        neg_context_vec = torch.cat(tuple(pair[0].view(1, -1) for pair in neg_pairs), dim=0).to(DEVICE)\n        neg_path_vec = torch.tensor([[pair[1]] for pair in neg_pairs]).to(DEVICE)\n\n        item_score = self._model(pos_context_vec, pos_path_vec, neg_context_vec, neg_path_vec)\n        return item_score\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        cbow = CBOWHierarchicalSoftmax(config)\n        cbow.load()\n        self._model = cbow\n        self._word_vocab = config.word_vocab\n        self.model_type = config.feature\n        self._check_vocab()\n        word_id_frequency_dict = {self._word_vocab.stoi[s]: self._word_vocab.freqs[s] for s in self._word_vocab.stoi}\n        self.huffman_tree = HuffmanTree(word_id_frequency_dict)\n        self.huffman_pos_path, self.huffman_neg_path = self.huffman_tree.get_all_pos_and_neg_path()\n\n    def save_embeddings(self, save_path: str, save_mode=\'word2vec\'):\n        embeddings = self._model.word_embeddings.weight.data.cpu().numpy()\n        with open(save_path, \'w\', encoding=\'utf8\') as f:\n            if save_mode == \'word2vec\':\n                f.write(\'{} {}\\n\'.format(self._model.vocabulary_size, self._model.embedding_dimension))\n            elif save_mode == \'glove\':\n                pass\n            for word, word_id in self._word_vocab.stoi.items():\n                if word == \' \':\n                    continue\n                word_embedding = embeddings[word_id]\n                word_embedding = \' \'.join(map(lambda x: str(x), word_embedding))\n                f.write(\'{} {}\\n\'.format(word, word_embedding))\n            logger.info(\'succeed saving embeddings data to {}\'.format(save_path))\n\n    def test(self, test_path):\n        self._model.eval()\n        test_dataset = cbow_tool.get_dataset(test_path)\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n\n    def _validate(self, dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\']):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = cbow_tool.get_iterator(dev_dataset, batch_size=batch_size)\n        for dev_item in tqdm(dev_iter):\n            pos_pairs = []\n            neg_pairs = []\n            for i in range(dev_item.batch_size):\n                pos_path = self.huffman_pos_path[dev_item.target[i]]\n                neg_path = self.huffman_neg_path[dev_item.target[i]]\n                pos_pairs.extend(zip([dev_item.context[i]] * len(pos_path), pos_path))\n                neg_pairs.extend(zip([dev_item.context[i]] * len(neg_path), neg_path))\n            pos_context_vec = torch.cat(tuple(pair[0].view(1, -1) for pair in pos_pairs), dim=0).to(DEVICE)\n            pos_path_vec = torch.tensor([[pair[1]] for pair in pos_pairs]).to(DEVICE)\n            neg_context_vec = torch.cat(tuple(pair[0].view(1, -1) for pair in neg_pairs), dim=0).to(DEVICE)\n            neg_path_vec = torch.tensor([[pair[1]] for pair in neg_pairs]).to(DEVICE)\n\n            item_score = self._model(pos_context_vec, pos_path_vec, neg_context_vec, neg_path_vec)\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def _check_vocab(self):\n        if not hasattr(WORD, \'vocab\'):\n            WORD.vocab = self._word_vocab\n\n    def deploy(self, route_path=""/cbow"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/predict"", methods=[\'POST\', \'GET\'])\n        def predict():\n            print(request.args)\n            words = request.args.get(\'words\', [])\n            result = self.predict(words)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'word\': result\n                        }\n                })\n\n        @app.route(route_path + ""/evaluate"", methods=[\'POST\', \'GET\'])\n        def evaluate():\n            print(request.args)\n            context = request.args.get(\'context\', \'\')\n            word = request.args.get(\'word\', \'\')\n            result = self.evaluate(context, word)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'score\': result\n                        }\n                })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/we/cbow/negative_sampling/__init__.py,0,b''
lightnlp/we/cbow/negative_sampling/model.py,11,"b'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ....base.model import BaseModel\n\nfrom ..config import DEVICE\n\n\nclass CBOWNegativeSampling(BaseModel):\n    def __init__(self, args):\n        super(CBOWNegativeSampling, self).__init__(args)\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n\n        self.word_embeddings = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n        self.context_embeddings = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n\n    def forward(self, context, target):\n        context_embedding = torch.sum(self.word_embeddings(context), dim=1)\n        target_embedding = self.context_embeddings(target)\n        target_score = torch.bmm(target_embedding, context_embedding.unsqueeze(2))\n        return torch.sigmoid(target_score)\n\n    def loss(self, context, pos, neg):\n        context_embedding = torch.sum(self.word_embeddings(context), dim=1)\n        pos_embedding = self.context_embeddings(pos)\n        neg_embedding = self.context_embeddings(neg).squeeze()\n        pos_score = torch.bmm(pos_embedding, context_embedding.unsqueeze(2)).squeeze()\n        neg_score = torch.bmm(neg_embedding, context_embedding.unsqueeze(2)).squeeze()\n        pos_score = torch.sum(F.logsigmoid(pos_score), dim=0)\n        neg_score = torch.sum(F.logsigmoid(-1 * neg_score), dim=0)\n        return -1*(torch.sum(pos_score) + torch.sum(neg_score))\n\n\n'"
lightnlp/we/cbow/negative_sampling/module.py,10,"b'import torch\nfrom tqdm import tqdm\nfrom typing import List\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ....utils.deploy import get_free_tcp_port\nfrom ....utils.learning import adjust_learning_rate\nfrom ....utils.log import logger\nfrom ....base.module import Module\n\nfrom ..model import Config\nfrom .model import CBOWNegativeSampling\nfrom ..config import DEVICE, DEFAULT_CONFIG\nfrom ..tool import cbow_tool, WORD\nfrom ..utils.sampling import Sampling\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass CBOWNegativeSamplingModule(Module):\n    """"""\n    """"""\n\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self.model_type = None\n\n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = cbow_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = cbow_tool.get_dataset(dev_path)\n            word_vocab = cbow_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab = cbow_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        train_iter = cbow_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, save_path=save_path, **kwargs)\n        self.model_type = config.feature\n\n        cbow = CBOWNegativeSampling(config)\n        # print(textcnn)\n        self._model = cbow\n        sampling = Sampling(word_vocab)\n        optim = torch.optim.Adam(cbow.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            cbow.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                optim.zero_grad()\n                neg = torch.tensor(sampling.sampling(config.neg_num *\n                                                     item.context.size(0))).view(item.context.size(0), 1,\n                                                                                 config.neg_num).to(DEVICE)\n                item_loss = self._model.loss(item.context, item.target, neg)\n                acc_loss += item_loss.item()\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'cbow_ns_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'cbow_ns_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        cbow.save()\n\n    def predict(self, context: List[str], topk=3):\n        self._model.eval()\n        vec_context = torch.tensor([self._word_vocab.stoi[x] for x in context])\n        vec_context = vec_context.reshape(1, -1).to(DEVICE)\n        vec_target = torch.tensor([i for i in range(len(self._word_vocab.itos))])\n        vec_target = vec_target.reshape(1, -1).to(DEVICE)\n        vec_predict = self._model(vec_context, vec_target).squeeze()\n        vec_prob, vec_index = torch.topk(vec_predict, topk, dim=0)\n        vec_prob = vec_prob.cpu().tolist()\n        vec_index = [self._word_vocab.itos[i] for i in vec_index]\n        return list(zip(vec_index, vec_prob))\n\n    def evaluate(self, context, target):\n        self._model.eval()\n        vec_context = torch.tensor([self._word_vocab.stoi[x] for x in context])\n        vec_context = vec_context.reshape(1, -1).to(DEVICE)\n        vec_target = torch.tensor([self._word_vocab.stoi[target]])\n        vec_target = vec_target.reshape(1, -1).to(DEVICE)\n        score = self._model(vec_context, vec_target).item()\n        return score\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        cbow = CBOWNegativeSampling(config)\n        cbow.load()\n        self._model = cbow\n        self._word_vocab = config.word_vocab\n        self.model_type = config.feature\n        self._check_vocab()\n\n    def save_embeddings(self, save_path: str, save_mode=\'word2vec\'):\n        embeddings = self._model.word_embeddings.weight.data.cpu().numpy()\n        with open(save_path, \'w\', encoding=\'utf8\') as f:\n            if save_mode == \'word2vec\':\n                f.write(\'{} {}\\n\'.format(self._model.vocabulary_size, self._model.embedding_dimension))\n            elif save_mode == \'glove\':\n                pass\n            for word, word_id in self._word_vocab.stoi.items():\n                if word == \' \':\n                    continue\n                word_embedding = embeddings[word_id]\n                word_embedding = \' \'.join(map(lambda x: str(x), word_embedding))\n                f.write(\'{} {}\\n\'.format(word, word_embedding))\n            logger.info(\'succeed saving embeddings data to {}\'.format(save_path))\n\n    def test(self, test_path):\n        self._model.eval()\n        test_dataset = cbow_tool.get_dataset(test_path)\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n\n    def _validate(self, dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\']):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = cbow_tool.get_iterator(dev_dataset, batch_size=batch_size)\n        for dev_item in tqdm(dev_iter):\n            item_score = self._model(dev_item.context, dev_item.target).squeeze().cpu().tolist()\n            dev_score_list.extend(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def _check_vocab(self):\n        if not hasattr(WORD, \'vocab\'):\n            WORD.vocab = self._word_vocab\n\n    def deploy(self, route_path=""/cbow"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/predict"", methods=[\'POST\', \'GET\'])\n        def predict():\n            print(request.args)\n            words = request.args.get(\'words\', [])\n            result = self.predict(words)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'word\': result\n                        }\n                })\n\n        @app.route(route_path + ""/evaluate"", methods=[\'POST\', \'GET\'])\n        def evaluate():\n            print(request.args)\n            context = request.args.get(\'context\', \'\')\n            word = request.args.get(\'word\', \'\')\n            result = self.evaluate(context, word)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'score\': result\n                        }\n                })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/we/cbow/utils/__init__.py,0,b''
lightnlp/we/cbow/utils/dataset.py,0,"b'from torchtext.data import Dataset, Example\nfrom torchtext.data import Field\n\nimport jieba\n\nCONTEXT = Field(tokenize=lambda x: [x], batch_first=True)\nTARGET = Field(tokenize=lambda x: [x], batch_first=True)\nFields = [\n            (\'context\', CONTEXT),\n            (\'target\', TARGET)\n        ]\n\n\ndef default_tokenize(sentence):\n    return list(jieba.cut(sentence))\n\n\nclass CBOWDataset(Dataset):\n\n    def __init__(self, path, fields, window_size=3, tokenize=default_tokenize, encoding=""utf-8"", **kwargs):\n        examples = []\n        with open(path, ""r"", encoding=encoding) as f:\n            for line in f:\n                words = tokenize(line.strip())\n                if len(words) < window_size + 1:\n                    continue\n                for i in range(len(words)):\n                    example = (words[max(0, i - window_size):i] +\n                               words[min(i+1, len(words)):min(len(words), i + window_size) + 1], words[i])\n                    examples.append(Example.fromlist(example, fields))\n        super(CBOWDataset, self).__init__(examples, fields, **kwargs)\n\n\nif __name__ == \'__main__\':\n    test_path = \'/home/lightsmile/NLP/corpus/novel/test.txt\'\n    dataset = CBOWDataset(test_path, Fields)\n    print(len(dataset))\n    print(dataset[0])\n    print(dataset[0].context)\n    print(dataset[0].target)\n\n    TARGET.build_vocab(dataset)\n\n    from sampling import Sampling\n\n    samp = Sampling(TARGET.vocab)\n\n    print(samp.sampling(3))\n'"
lightnlp/we/cbow/utils/huffman_tree.py,0,"b'""""""\n    \xe6\x9e\x84\xe5\xbb\xba\xe9\x9c\x8d\xe5\xa4\xab\xe6\x9b\xbc\xe6\xa0\x91\n""""""\n\n\nclass HuffmanNode:\n    def __init__(self, word_id, frequency):\n        self.word_id = word_id  # \xe5\x8f\xb6\xe5\xad\x90\xe7\xbb\x93\xe7\x82\xb9\xe5\xad\x98\xe8\xaf\x8d\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84id, \xe4\xb8\xad\xe9\x97\xb4\xe8\x8a\x82\xe7\x82\xb9\xe5\xad\x98\xe4\xb8\xad\xe9\x97\xb4\xe8\x8a\x82\xe7\x82\xb9id\n        self.frequency = frequency  # \xe5\xad\x98\xe5\x8d\x95\xe8\xaf\x8d\xe9\xa2\x91\xe6\xac\xa1\n        self.left_child = None\n        self.right_child = None\n        self.father = None\n        self.Huffman_code = []  # \xe9\x9c\x8d\xe5\xa4\xab\xe6\x9b\xbc\xe7\xa0\x81\xef\xbc\x88\xe5\xb7\xa61\xe5\x8f\xb30\xef\xbc\x89\n        self.path = []  # \xe6\xa0\xb9\xe5\x88\xb0\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xe7\x9a\x84\xe4\xb8\xad\xe9\x97\xb4\xe8\x8a\x82\xe7\x82\xb9id\n\n\nclass HuffmanTree:\n    def __init__(self, wordid_frequency_dict):\n        self.word_count = len(wordid_frequency_dict)  # \xe5\x8d\x95\xe8\xaf\x8d\xe6\x95\xb0\xe9\x87\x8f\n        self.wordid_code = dict()\n        self.wordid_path = dict()\n        self.root = None\n        unmerge_node_list = [HuffmanNode(wordid, frequency) for wordid, frequency in\n                             wordid_frequency_dict.items()]  # \xe6\x9c\xaa\xe5\x90\x88\xe5\xb9\xb6\xe8\x8a\x82\xe7\x82\xb9list\n        self.huffman = [HuffmanNode(wordid, frequency) for wordid, frequency in\n                        wordid_frequency_dict.items()]  # \xe5\xad\x98\xe5\x82\xa8\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xe5\x92\x8c\xe4\xb8\xad\xe9\x97\xb4\xe8\x8a\x82\xe7\x82\xb9\n        # \xe6\x9e\x84\xe5\xbb\xbahuffman tree\n        self.build_tree(unmerge_node_list)\n        # \xe7\x94\x9f\xe6\x88\x90huffman code\n        self.generate_huffman_code_and_path()\n\n    def merge_node(self, node1, node2):\n        sum_frequency = node1.frequency + node2.frequency\n        mid_node_id = len(self.huffman)  # \xe4\xb8\xad\xe9\x97\xb4\xe8\x8a\x82\xe7\x82\xb9\xe7\x9a\x84value\xe5\xad\x98\xe4\xb8\xad\xe9\x97\xb4\xe8\x8a\x82\xe7\x82\xb9id\n        father_node = HuffmanNode(mid_node_id, sum_frequency)\n        if node1.frequency >= node2.frequency:\n            father_node.left_child = node1\n            father_node.right_child = node2\n        else:\n            father_node.left_child = node2\n            father_node.right_child = node1\n        self.huffman.append(father_node)\n        return father_node\n\n    def build_tree(self, node_list):\n        while len(node_list) > 1:\n            i1 = 0  # \xe6\xa6\x82\xe7\x8e\x87\xe6\x9c\x80\xe5\xb0\x8f\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\n            i2 = 1  # \xe6\xa6\x82\xe7\x8e\x87\xe7\xac\xac\xe4\xba\x8c\xe5\xb0\x8f\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\n            if node_list[i2].frequency < node_list[i1].frequency:\n                [i1, i2] = [i2, i1]\n            for i in range(2, len(node_list)):\n                if node_list[i].frequency < node_list[i2].frequency:\n                    i2 = i\n                    if node_list[i2].frequency < node_list[i1].frequency:\n                        [i1, i2] = [i2, i1]\n            father_node = self.merge_node(node_list[i1], node_list[i2])  # \xe5\x90\x88\xe5\xb9\xb6\xe6\x9c\x80\xe5\xb0\x8f\xe7\x9a\x84\xe4\xb8\xa4\xe4\xb8\xaa\xe8\x8a\x82\xe7\x82\xb9\n            if i1 < i2:\n                node_list.pop(i2)\n                node_list.pop(i1)\n            elif i1 > i2:\n                node_list.pop(i1)\n                node_list.pop(i2)\n            else:\n                raise RuntimeError(\'i1 should not be equal to i2\')\n            node_list.insert(0, father_node)  # \xe6\x8f\x92\xe5\x85\xa5\xe6\x96\xb0\xe8\x8a\x82\xe7\x82\xb9\n        self.root = node_list[0]\n\n    def generate_huffman_code_and_path(self):\n        stack = [self.root]\n        while len(stack) > 0:\n            node = stack.pop()\n            # \xe9\xa1\xba\xe7\x9d\x80\xe5\xb7\xa6\xe5\xad\x90\xe6\xa0\x91\xe8\xb5\xb0\n            while node.left_child or node.right_child:\n                code = node.Huffman_code\n                path = node.path\n                node.left_child.Huffman_code = code + [1]\n                node.right_child.Huffman_code = code + [0]\n                node.left_child.path = path + [node.word_id]\n                node.right_child.path = path + [node.word_id]\n                # \xe6\x8a\x8a\xe6\xb2\xa1\xe8\xb5\xb0\xe8\xbf\x87\xe7\x9a\x84\xe5\x8f\xb3\xe5\xad\x90\xe6\xa0\x91\xe5\x8a\xa0\xe5\x85\xa5\xe6\xa0\x88\n                stack.append(node.right_child)\n                node = node.left_child\n            word_id = node.word_id\n            word_code = node.Huffman_code\n            word_path = node.path\n            self.huffman[word_id].Huffman_code = word_code\n            self.huffman[word_id].path = word_path\n            # \xe6\x8a\x8a\xe8\x8a\x82\xe7\x82\xb9\xe8\xae\xa1\xe7\xae\x97\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe9\x9c\x8d\xe5\xa4\xab\xe6\x9b\xbc\xe7\xa0\x81\xe3\x80\x81\xe8\xb7\xaf\xe5\xbe\x84  \xe5\x86\x99\xe5\x85\xa5\xe8\xaf\x8d\xe5\x85\xb8\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\xe4\xb8\xad\n            self.wordid_code[word_id] = word_code\n            self.wordid_path[word_id] = word_path\n\n    # \xe8\x8e\xb7\xe5\x8f\x96\xe6\x89\x80\xe6\x9c\x89\xe8\xaf\x8d\xe7\x9a\x84\xe6\xad\xa3\xe5\x90\x91\xe8\x8a\x82\xe7\x82\xb9id\xe5\x92\x8c\xe8\xb4\x9f\xe5\x90\x91\xe8\x8a\x82\xe7\x82\xb9id\xe6\x95\xb0\xe7\xbb\x84\n    def get_all_pos_and_neg_path(self):\n        positive = []  # \xe6\x89\x80\xe6\x9c\x89\xe8\xaf\x8d\xe7\x9a\x84\xe6\xad\xa3\xe5\x90\x91\xe8\xb7\xaf\xe5\xbe\x84\xe6\x95\xb0\xe7\xbb\x84\n        negative = []  # \xe6\x89\x80\xe6\x9c\x89\xe8\xaf\x8d\xe7\x9a\x84\xe8\xb4\x9f\xe5\x90\x91\xe8\xb7\xaf\xe5\xbe\x84\xe6\x95\xb0\xe7\xbb\x84\n        for word_id in range(self.word_count):\n            pos_id = []  # \xe5\xad\x98\xe6\x94\xbe\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\x8d \xe8\xb7\xaf\xe5\xbe\x84\xe4\xb8\xad\xe7\x9a\x84\xe6\xad\xa3\xe5\x90\x91\xe8\x8a\x82\xe7\x82\xb9id\n            neg_id = []  # \xe5\xad\x98\xe6\x94\xbe\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\x8d \xe8\xb7\xaf\xe5\xbe\x84\xe4\xb8\xad\xe7\x9a\x84\xe8\xb4\x9f\xe5\x90\x91\xe8\x8a\x82\xe7\x82\xb9id\n            for i, code in enumerate(self.huffman[word_id].Huffman_code):\n                if code == 1:\n                    pos_id.append(self.huffman[word_id].path[i])\n                else:\n                    neg_id.append(self.huffman[word_id].path[i])\n            positive.append(pos_id)\n            negative.append(neg_id)\n        return positive, negative\n\n\ndef test():\n    word_frequency = {0: 4, 1: 6, 2: 3, 3: 2, 4: 2}\n    print(word_frequency)\n    tree = HuffmanTree(word_frequency)\n    print(tree.wordid_code)\n    print(tree.wordid_path)\n    for i in range(len(word_frequency)):\n        print(tree.huffman[i].path)\n    print(tree.get_all_pos_and_neg_path())\n\n\nif __name__ == \'__main__\':\n    test()\n'"
lightnlp/we/cbow/utils/sampling.py,1,"b'import torch\nfrom torchtext.vocab import Vocab\n\n\nclass Sampling(object):\n    def __init__(self, vocab: Vocab, weight=0.75):\n        self.vocab = vocab\n        self.weight = weight\n        self.weighted_list = [self.vocab.freqs[s]**self.weight for s in self.vocab.itos]\n\n    def sampling(self, num):\n        return torch.multinomial(torch.tensor(self.weighted_list), num).tolist()\n'"
lightnlp/we/skip_gram/base/__init__.py,0,b''
lightnlp/we/skip_gram/base/model.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ....base.model import BaseModel\n\nfrom ..config import DEVICE\n\n\nclass SkipGramBase(BaseModel):\n    def __init__(self, args):\n        super(SkipGramBase, self).__init__(args)\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n\n        self.word_embeddings = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n        self.linear = nn.Linear(self.embedding_dimension, self.vocabulary_size).to(DEVICE)\n\n    def forward(self, target):\n        target_embedding = self.word_embeddings(target)\n        context_embedding = self.linear(target_embedding).squeeze()\n        return context_embedding\n\n    def loss(self, target, context):\n        target_embedding = self.word_embeddings(target)\n        context_embedding = self.linear(target_embedding).reshape(target_embedding.size(0), -1)\n        return F.cross_entropy(context_embedding, context.view(-1))\n'"
lightnlp/we/skip_gram/base/module.py,13,"b'import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ....utils.deploy import get_free_tcp_port\nfrom ....utils.learning import adjust_learning_rate\nfrom ....utils.log import logger\nfrom ....base.module import Module\n\nfrom ..model import Config\nfrom .model import SkipGramBase\nfrom ..config import DEVICE, DEFAULT_CONFIG\nfrom ..tool import skip_gram_tool, WORD\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass SkipGramBaseModule(Module):\n    """"""\n    """"""\n\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self.model_type = None\n\n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = skip_gram_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = skip_gram_tool.get_dataset(dev_path)\n            word_vocab = skip_gram_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab = skip_gram_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        train_iter = skip_gram_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, save_path=save_path, **kwargs)\n        self.model_type = config.feature\n\n        skip_gram = SkipGramBase(config)\n        self._model = skip_gram\n        optim = torch.optim.Adam(skip_gram.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            skip_gram.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                optim.zero_grad()\n                item_loss = self._model.loss(item.target, item.context)\n                acc_loss += item_loss.item()\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'skip_gram_base_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'skip_gram_base_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        skip_gram.save()\n\n    def predict(self, target: str, topk=3):\n        self._model.eval()\n\n        # vec_target = torch.tensor([self._word_vocab.stoi[target]])\n        # vec_target = vec_target.reshape(1, -1).to(DEVICE)\n        # vec_context = torch.tensor([i for i in range(len(self._word_vocab.itos))])\n        # vec_context = vec_context.reshape(1, -1).to(DEVICE)\n        # temp = self._model.loss(vec_target, vec_context)\n        # vec_predict = self._model.loss(vec_target, vec_context).squeeze()\n        # vec_prob, vec_index = torch.topk(vec_predict, topk, dim=0)\n        # vec_prob = vec_prob.cpu().tolist()\n        # vec_index = [self._word_vocab.itos[i] for i in vec_index]\n        #\n        # return list(zip(vec_index, vec_prob))\n        random_predict = torch.rand(len(self._word_vocab.itos)).to(DEVICE)\n        vec_prob, vec_index = torch.topk(random_predict, topk, dim=0)\n        vec_prob = vec_prob.cpu().tolist()\n        vec_index = [self._word_vocab.itos[i] for i in vec_index]\n        return list(zip(vec_index, vec_prob))\n\n    def evaluate(self, target, context):\n        self._model.eval()\n\n        vec_target = torch.tensor([self._word_vocab.stoi[target]])\n        vec_target = vec_target.reshape(1, -1).to(DEVICE)\n        vec_context = torch.tensor([self._word_vocab.stoi[context]])\n        vec_context = vec_context.reshape(1, -1).to(DEVICE)\n        score = torch.exp(- self._model.loss(vec_target, vec_context)).item()\n\n        return score\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        skip_gram = SkipGramBase(config)\n        skip_gram.load()\n        self._model = skip_gram\n        self._word_vocab = config.word_vocab\n        self.model_type = config.feature\n        self._check_vocab()\n\n    def save_embeddings(self, save_path: str, save_mode=\'word2vec\'):\n        embeddings = self._model.word_embeddings.weight.data.cpu().numpy()\n        with open(save_path, \'w\', encoding=\'utf8\') as f:\n            if save_mode == \'word2vec\':\n                f.write(\'{} {}\\n\'.format(self._model.vocabulary_size, self._model.embedding_dimension))\n            elif save_mode == \'glove\':\n                pass\n            for word, word_id in self._word_vocab.stoi.items():\n                if word == \' \':\n                    continue\n                word_embedding = embeddings[word_id]\n                word_embedding = \' \'.join(map(lambda x: str(x), word_embedding))\n                f.write(\'{} {}\\n\'.format(word, word_embedding))\n            logger.info(\'succeed saving embeddings data to {}\'.format(save_path))\n\n    def test(self, test_path):\n        self._model.eval()\n        test_dataset = skip_gram_tool.get_dataset(test_path)\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n\n    def _validate(self, dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\']):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = skip_gram_tool.get_iterator(dev_dataset, batch_size=batch_size)\n        for dev_item in tqdm(dev_iter):\n            item_score = self._model.loss(dev_item.target, dev_item.context).item()\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def _check_vocab(self):\n        if not hasattr(WORD, \'vocab\'):\n            WORD.vocab = self._word_vocab\n\n    def deploy(self, route_path=""/skip_gram"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/predict"", methods=[\'POST\', \'GET\'])\n        def predict():\n            print(request.args)\n            word = request.args.get(\'word\', \'\')\n            result = self.predict(word)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'words\': result\n                        }\n                })\n\n        @app.route(route_path + ""/evaluate"", methods=[\'POST\', \'GET\'])\n        def evaluate():\n            print(request.args)\n            context = request.args.get(\'context\', \'\')\n            word = request.args.get(\'word\', \'\')\n            result = self.evaluate(context, word)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'score\': result\n                        }\n                })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/we/skip_gram/hierarchical_softmax/__init__.py,0,b''
lightnlp/we/skip_gram/hierarchical_softmax/model.py,16,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ....base.model import BaseModel\n\nfrom ..config import DEVICE\n\n\nclass SkipGramHierarchicalSoftmax(BaseModel):\n    def __init__(self, args):\n        super(SkipGramHierarchicalSoftmax, self).__init__(args)\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n\n        self.word_embeddings = nn.Embedding(2*self.vocabulary_size-1, self.embedding_dimension,\n                                            sparse=True).to(DEVICE)\n        self.context_embeddings = nn.Embedding(2*self.vocabulary_size-1, self.embedding_dimension,\n                                               sparse=True).to(DEVICE)\n\n    def forward(self, pos_target, pos_path, neg_target, neg_path):\n        pos_target_embedding = torch.sum(self.word_embeddings(pos_target), dim=1, keepdim=True)\n        pos_path_embedding = self.context_embeddings(pos_path)\n        pos_score = torch.bmm(pos_target_embedding, pos_path_embedding.transpose(2, 1)).squeeze()\n        neg_target_embedding = torch.sum(self.word_embeddings(neg_target), dim=1, keepdim=True)\n        neg_path_embedding = self.context_embeddings(neg_path)\n        neg_score = torch.bmm(neg_target_embedding, neg_path_embedding.transpose(2, 1)).squeeze()\n        pos_sigmoid_score = torch.lt(torch.sigmoid(pos_score), 0.5)\n        neg_sigmoid_score = torch.gt(torch.sigmoid(neg_score), 0.5)\n        sigmoid_score = torch.cat((pos_sigmoid_score, neg_sigmoid_score))\n        sigmoid_score = torch.sum(sigmoid_score, dim=0).item() / sigmoid_score.size(0)\n        return sigmoid_score\n\n    def loss(self, pos_target, pos_path, neg_target, neg_path):\n        pos_target_embedding = torch.sum(self.word_embeddings(pos_target), dim=1, keepdim=True)\n        pos_path_embedding = self.context_embeddings(pos_path)\n        pos_score = torch.bmm(pos_target_embedding, pos_path_embedding.transpose(2, 1)).squeeze()\n        neg_target_embedding = torch.sum(self.word_embeddings(neg_target), dim=1, keepdim=True)\n        neg_path_embedding = self.context_embeddings(neg_path)\n        neg_score = torch.bmm(neg_target_embedding, neg_path_embedding.transpose(2, 1)).squeeze()\n        pos_score = torch.sum(F.logsigmoid(-1 * pos_score))\n        neg_score = torch.sum(F.logsigmoid(neg_score))\n        loss = -1 * (pos_score + neg_score)\n        return loss\n\n'"
lightnlp/we/skip_gram/hierarchical_softmax/module.py,19,"b'import torch\nfrom tqdm import tqdm\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ....utils.deploy import get_free_tcp_port\nfrom ....utils.learning import adjust_learning_rate\nfrom ....utils.log import logger\nfrom ....base.module import Module\n\nfrom ..model import Config\nfrom .model import SkipGramHierarchicalSoftmax\nfrom ..config import DEVICE, DEFAULT_CONFIG\nfrom ..tool import skip_gram_tool, WORD\nfrom ..utils.huffman_tree import HuffmanTree\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass SkipGramHierarchicalSoftmaxModule(Module):\n    """"""\n    """"""\n\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self.model_type = None\n        self.huffman_tree = None\n        self.huffman_pos_path = None\n        self.huffman_neg_path = None\n\n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = skip_gram_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = skip_gram_tool.get_dataset(dev_path)\n            word_vocab = skip_gram_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab = skip_gram_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        train_iter = skip_gram_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, save_path=save_path, **kwargs)\n        self.model_type = config.feature\n\n        skip_gram = SkipGramHierarchicalSoftmax(config)\n\n        self._model = skip_gram\n        word_id_frequency_dict = {self._word_vocab.stoi[s]: self._word_vocab.freqs[s] for s in self._word_vocab.stoi}\n        self.huffman_tree = HuffmanTree(word_id_frequency_dict)\n        self.huffman_pos_path, self.huffman_neg_path = self.huffman_tree.get_all_pos_and_neg_path()\n        optim = torch.optim.SparseAdam(skip_gram.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            skip_gram.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                optim.zero_grad()\n                pos_pairs = []\n                neg_pairs = []\n                for i in range(item.batch_size):\n                    pos_path = self.huffman_pos_path[item.context[i]]\n                    neg_path = self.huffman_neg_path[item.context[i]]\n                    pos_pairs.extend(zip([item.target[i]] * len(pos_path), pos_path))\n                    neg_pairs.extend(zip([item.target[i]] * len(neg_path), neg_path))\n                pos_context_vec = torch.cat(tuple(pair[0].view(1, -1) for pair in pos_pairs), dim=0).to(DEVICE)\n                pos_path_vec = torch.tensor([[pair[1]] for pair in pos_pairs]).to(DEVICE)\n                neg_context_vec = torch.cat(tuple(pair[0].view(1, -1) for pair in neg_pairs), dim=0).to(DEVICE)\n                neg_path_vec = torch.tensor([[pair[1]] for pair in neg_pairs]).to(DEVICE)\n\n                item_loss = self._model.loss(pos_context_vec, pos_path_vec, neg_context_vec, neg_path_vec)\n                acc_loss += item_loss.item()\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'skip_gram_hs_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'skip_gram_hs_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        skip_gram.save()\n\n    def predict(self, target: str, topk=3):\n        self._model.eval()\n\n        random_predict = torch.rand(len(self._word_vocab.itos)).to(DEVICE)\n        vec_prob, vec_index = torch.topk(random_predict, topk, dim=0)\n        vec_prob = vec_prob.cpu().tolist()\n        vec_index = [self._word_vocab.itos[i] for i in vec_index]\n        return list(zip(vec_index, vec_prob))\n\n    def evaluate(self, context, target):\n        self._model.eval()\n        vec_context = torch.tensor([self._word_vocab.stoi[x] for x in context])\n        vec_context = vec_context.to(DEVICE)\n        pos_path = self.huffman_pos_path[self._word_vocab.stoi[target]]\n        neg_path = self.huffman_neg_path[self._word_vocab.stoi[target]]\n        pos_pairs = list(zip([vec_context] * len(pos_path), pos_path))\n        neg_pairs = list(zip([vec_context] * len(neg_path), neg_path))\n        pos_context_vec = torch.cat(tuple(pair[0].view(1, -1) for pair in pos_pairs), dim=0).to(DEVICE)\n        pos_path_vec = torch.tensor([[pair[1]] for pair in pos_pairs]).to(DEVICE)\n        neg_context_vec = torch.cat(tuple(pair[0].view(1, -1) for pair in neg_pairs), dim=0).to(DEVICE)\n        neg_path_vec = torch.tensor([[pair[1]] for pair in neg_pairs]).to(DEVICE)\n\n        item_score = self._model(pos_context_vec, pos_path_vec, neg_context_vec, neg_path_vec)\n        return item_score\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        skip_gram = SkipGramHierarchicalSoftmax(config)\n        skip_gram.load()\n        self._model = skip_gram\n        self._word_vocab = config.word_vocab\n        self.model_type = config.feature\n        self._check_vocab()\n        word_id_frequency_dict = {self._word_vocab.stoi[s]: self._word_vocab.freqs[s] for s in self._word_vocab.stoi}\n        self.huffman_tree = HuffmanTree(word_id_frequency_dict)\n        self.huffman_pos_path, self.huffman_neg_path = self.huffman_tree.get_all_pos_and_neg_path()\n\n    def save_embeddings(self, save_path: str, save_mode=\'word2vec\'):\n        embeddings = self._model.word_embeddings.weight.data.cpu().numpy()\n        with open(save_path, \'w\', encoding=\'utf8\') as f:\n            if save_mode == \'word2vec\':\n                f.write(\'{} {}\\n\'.format(self._model.vocabulary_size, self._model.embedding_dimension))\n            elif save_mode == \'glove\':\n                pass\n            for word, word_id in self._word_vocab.stoi.items():\n                if word == \' \':\n                    continue\n                word_embedding = embeddings[word_id]\n                word_embedding = \' \'.join(map(lambda x: str(x), word_embedding))\n                f.write(\'{} {}\\n\'.format(word, word_embedding))\n            logger.info(\'succeed saving embeddings data to {}\'.format(save_path))\n\n    def test(self, test_path):\n        self._model.eval()\n        test_dataset = skip_gram_tool.get_dataset(test_path)\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n\n    def _validate(self, dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\']):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = skip_gram_tool.get_iterator(dev_dataset, batch_size=batch_size)\n        for dev_item in tqdm(dev_iter):\n            pos_pairs = []\n            neg_pairs = []\n            for i in range(dev_item.batch_size):\n                pos_path = self.huffman_pos_path[dev_item.context[i]]\n                neg_path = self.huffman_neg_path[dev_item.context[i]]\n                pos_pairs.extend(zip([dev_item.target[i]] * len(pos_path), pos_path))\n                neg_pairs.extend(zip([dev_item.target[i]] * len(neg_path), neg_path))\n            pos_context_vec = torch.cat(tuple(pair[0].view(1, -1) for pair in pos_pairs), dim=0).to(DEVICE)\n            pos_path_vec = torch.tensor([[pair[1]] for pair in pos_pairs]).to(DEVICE)\n            neg_context_vec = torch.cat(tuple(pair[0].view(1, -1) for pair in neg_pairs), dim=0).to(DEVICE)\n            neg_path_vec = torch.tensor([[pair[1]] for pair in neg_pairs]).to(DEVICE)\n\n            item_score = self._model(pos_context_vec, pos_path_vec, neg_context_vec, neg_path_vec)\n            dev_score_list.append(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def _check_vocab(self):\n        if not hasattr(WORD, \'vocab\'):\n            WORD.vocab = self._word_vocab\n\n    def deploy(self, route_path=""/skip_gram"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/predict"", methods=[\'POST\', \'GET\'])\n        def predict():\n            print(request.args)\n            word = request.args.get(\'word\', \'\')\n            result = self.predict(word)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'words\': result\n                        }\n                })\n\n        @app.route(route_path + ""/evaluate"", methods=[\'POST\', \'GET\'])\n        def evaluate():\n            print(request.args)\n            context = request.args.get(\'context\', \'\')\n            word = request.args.get(\'word\', \'\')\n            result = self.evaluate(context, word)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'score\': result\n                        }\n                })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/we/skip_gram/negative_sampling/__init__.py,0,b''
lightnlp/we/skip_gram/negative_sampling/model.py,9,"b'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ....base.model import BaseModel\n\nfrom ..config import DEVICE\n\n\nclass SkipGramNegativeSampling(BaseModel):\n    def __init__(self, args):\n        super(SkipGramNegativeSampling, self).__init__(args)\n\n        self.vocabulary_size = args.vocabulary_size\n        self.embedding_dimension = args.embedding_dim\n\n        self.word_embeddings = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n        self.context_embeddings = nn.Embedding(self.vocabulary_size, self.embedding_dimension).to(DEVICE)\n\n    def forward(self, target, context):\n        target_embedding = self.word_embeddings(target)\n        context_embedding = self.context_embeddings(context)\n        target_score = torch.matmul(target_embedding, context_embedding.transpose(2, 1)).squeeze()\n        return torch.sigmoid(target_score)\n\n    def loss(self, target, pos, neg):\n        target_embedding = self.word_embeddings(target)\n        pos_embedding = self.context_embeddings(pos)\n        neg_embedding = self.context_embeddings(neg).squeeze()\n        pos_score = torch.matmul(target_embedding, pos_embedding.transpose(2, 1)).squeeze()\n        neg_score = torch.matmul(target_embedding, neg_embedding.transpose(1, 2)).squeeze()\n        pos_score = torch.sum(F.logsigmoid(pos_score), dim=0)\n        neg_score = torch.sum(F.logsigmoid(-1 * neg_score), dim=0)\n        return -1*(torch.sum(pos_score) + torch.sum(neg_score))\n\n\n'"
lightnlp/we/skip_gram/negative_sampling/module.py,10,"b'import torch\nfrom tqdm import tqdm\nimport torch.nn.functional as F\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport flask\nfrom flask import Flask, request\n\nfrom ....utils.deploy import get_free_tcp_port\nfrom ....utils.learning import adjust_learning_rate\nfrom ....utils.log import logger\nfrom ....base.module import Module\n\nfrom ..model import Config\nfrom .model import SkipGramNegativeSampling\nfrom ..config import DEVICE, DEFAULT_CONFIG\nfrom ..tool import skip_gram_tool, WORD\nfrom ..utils.sampling import Sampling\n\nseed = 2019\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\n\n\nclass SkipGramNegativeSamplingModule(Module):\n    """"""\n    """"""\n\n    def __init__(self):\n        self._model = None\n        self._word_vocab = None\n        self.model_type = None\n\n    def train(self, train_path, save_path=DEFAULT_CONFIG[\'save_path\'], dev_path=None, vectors_path=None, log_dir=None,\n              **kwargs):\n        writer = SummaryWriter(log_dir=log_dir)\n        train_dataset = skip_gram_tool.get_dataset(train_path)\n        if dev_path:\n            dev_dataset = skip_gram_tool.get_dataset(dev_path)\n            word_vocab = skip_gram_tool.get_vocab(train_dataset, dev_dataset)\n        else:\n            word_vocab = skip_gram_tool.get_vocab(train_dataset)\n        self._word_vocab = word_vocab\n        train_iter = skip_gram_tool.get_iterator(train_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\'])\n        config = Config(word_vocab, save_path=save_path, **kwargs)\n        self.model_type = config.feature\n\n        skip_gram = SkipGramNegativeSampling(config)\n        # print(textcnn)\n        self._model = skip_gram\n        sampling = Sampling(word_vocab)\n        optim = torch.optim.Adam(skip_gram.parameters(), lr=config.lr)\n        for epoch in range(config.epoch):\n            skip_gram.train()\n            acc_loss = 0\n            for item in tqdm(train_iter):\n                optim.zero_grad()\n                neg = torch.tensor(sampling.sampling(config.neg_num *\n                                                     item.context.size(0))).view(item.context.size(0), 1,\n                                                                                 config.neg_num).to(DEVICE)\n                item_loss = self._model.loss(item.target, item.context, neg)\n                acc_loss += item_loss.item()\n                item_loss.backward()\n                optim.step()\n            logger.info(\'epoch: {}, acc_loss: {}\'.format(epoch, acc_loss))\n            writer.add_scalar(\'skip_gram_ns_train/acc_loss\', acc_loss, epoch)\n            if dev_path:\n                dev_score = self._validate(dev_dataset)\n                logger.info(\'dev score:{}\'.format(dev_score))\n                writer.add_scalar(\'skip_gram_ns_train/dev_score\', dev_score, epoch)\n            writer.flush()\n            adjust_learning_rate(optim, config.lr / (1 + (epoch + 1) * config.lr_decay))\n        writer.close()\n        config.save()\n        skip_gram.save()\n\n    def predict(self, target: str, topk=3):\n        self._model.eval()\n\n        vec_target = torch.tensor([self._word_vocab.stoi[target]])\n        vec_target = vec_target.reshape(1, -1).to(DEVICE)\n        vec_predict = self._model(vec_target)\n        vec_score = F.softmax(vec_predict, dim=0)\n        vec_prob, vec_index = torch.topk(vec_score, topk, dim=0)\n        vec_prob = vec_prob.cpu().tolist()\n        vec_index = [self._word_vocab.itos[i] for i in vec_index]\n\n        return list(zip(vec_index, vec_prob))\n\n    def evaluate(self, target, context):\n        self._model.eval()\n\n        vec_target = torch.tensor([self._word_vocab.stoi[target]])\n        vec_target = vec_target.reshape(1, -1).to(DEVICE)\n        vec_context = torch.tensor([self._word_vocab.stoi[context]])\n        vec_context = vec_context.reshape(1, -1).to(DEVICE)\n        score = self._model(vec_target, vec_context).item()\n\n        return score\n\n    def load(self, save_path=DEFAULT_CONFIG[\'save_path\']):\n        config = Config.load(save_path)\n        skip_gram = SkipGramNegativeSampling(config)\n        skip_gram.load()\n        self._model = skip_gram\n        self._word_vocab = config.word_vocab\n        self.model_type = config.feature\n        self._check_vocab()\n\n    def save_embeddings(self, save_path: str, save_mode=\'word2vec\'):\n        embeddings = self._model.word_embeddings.weight.data.cpu().numpy()\n        with open(save_path, \'w\', encoding=\'utf8\') as f:\n            if save_mode == \'word2vec\':\n                f.write(\'{} {}\\n\'.format(self._model.vocabulary_size, self._model.embedding_dimension))\n            elif save_mode == \'glove\':\n                pass\n            for word, word_id in self._word_vocab.stoi.items():\n                if word == \' \':\n                    continue\n                word_embedding = embeddings[word_id]\n                word_embedding = \' \'.join(map(lambda x: str(x), word_embedding))\n                f.write(\'{} {}\\n\'.format(word, word_embedding))\n            logger.info(\'succeed saving embeddings data to {}\'.format(save_path))\n\n    def test(self, test_path):\n        self._model.eval()\n        test_dataset = skip_gram_tool.get_dataset(test_path)\n        test_score = self._validate(test_dataset)\n        logger.info(\'test score:{}\'.format(test_score))\n\n    def _validate(self, dev_dataset, batch_size=DEFAULT_CONFIG[\'batch_size\']):\n        self._model.eval()\n        dev_score_list = []\n        dev_iter = skip_gram_tool.get_iterator(dev_dataset, batch_size=batch_size)\n        for dev_item in tqdm(dev_iter):\n            item_score = self._model(dev_item.target, dev_item.context).squeeze().cpu().tolist()\n            dev_score_list.extend(item_score)\n        return sum(dev_score_list) / len(dev_score_list)\n\n    def _check_vocab(self):\n        if not hasattr(WORD, \'vocab\'):\n            WORD.vocab = self._word_vocab\n\n    def deploy(self, route_path=""/skip_gram"", host=""localhost"", port=None, debug=False):\n        app = Flask(__name__)\n\n        @app.route(route_path + ""/predict"", methods=[\'POST\', \'GET\'])\n        def predict():\n            print(request.args)\n            word = request.args.get(\'word\', \'\')\n            result = self.predict(word)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'words\': result\n                        }\n                })\n\n        @app.route(route_path + ""/evaluate"", methods=[\'POST\', \'GET\'])\n        def evaluate():\n            print(request.args)\n            context = request.args.get(\'context\', \'\')\n            word = request.args.get(\'word\', \'\')\n            result = self.evaluate(context, word)\n            return flask.jsonify({\n                    \'state\': \'OK\',\n                    \'result\': {\n                        \'score\': result\n                        }\n                })\n        if not port:\n            port = get_free_tcp_port()\n        app.run(host=host, port=port, debug=debug)\n'"
lightnlp/we/skip_gram/utils/__init__.py,0,b''
lightnlp/we/skip_gram/utils/dataset.py,0,"b'from torchtext.data import Dataset, Example\nfrom torchtext.data import Field\n\nimport jieba\n\nCONTEXT = Field(tokenize=lambda x: [x], batch_first=True)\nTARGET = Field(tokenize=lambda x: [x], batch_first=True)\nFields = [\n            (\'context\', CONTEXT),\n            (\'target\', TARGET)\n        ]\n\n\ndef default_tokenize(sentence):\n    return list(jieba.cut(sentence))\n\n\nclass SkipGramDataset(Dataset):\n\n    def __init__(self, path, fields, window_size=3, tokenize=default_tokenize, encoding=""utf-8"", **kwargs):\n        examples = []\n        with open(path, ""r"", encoding=encoding) as f:\n            for line in f:\n                words = tokenize(line.strip())\n                if len(words) < window_size + 1:\n                    continue\n                for i in range(len(words)):\n                    contexts = words[max(0, i - window_size):i] + \\\n                               words[min(i+1, len(words)):min(len(words), i + window_size) + 1]\n                    for context in contexts:\n                        examples.append(Example.fromlist((context, words[i]), fields))\n        super(SkipGramDataset, self).__init__(examples, fields, **kwargs)\n\n\nif __name__ == \'__main__\':\n    test_path = \'/home/lightsmile/NLP/corpus/novel/test.txt\'\n    dataset = SkipGramDataset(test_path, Fields)\n    print(len(dataset))\n    print(dataset[0])\n    print(dataset[0].context)\n    print(dataset[0].target)\n\n    TARGET.build_vocab(dataset)\n\n    from sampling import Sampling\n\n    samp = Sampling(TARGET.vocab)\n\n    print(samp.sampling(3))\n'"
lightnlp/we/skip_gram/utils/huffman_tree.py,0,"b'""""""\n    \xe6\x9e\x84\xe5\xbb\xba\xe9\x9c\x8d\xe5\xa4\xab\xe6\x9b\xbc\xe6\xa0\x91\n""""""\n\n\nclass HuffmanNode:\n    def __init__(self, word_id, frequency):\n        self.word_id = word_id  # \xe5\x8f\xb6\xe5\xad\x90\xe7\xbb\x93\xe7\x82\xb9\xe5\xad\x98\xe8\xaf\x8d\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84id, \xe4\xb8\xad\xe9\x97\xb4\xe8\x8a\x82\xe7\x82\xb9\xe5\xad\x98\xe4\xb8\xad\xe9\x97\xb4\xe8\x8a\x82\xe7\x82\xb9id\n        self.frequency = frequency  # \xe5\xad\x98\xe5\x8d\x95\xe8\xaf\x8d\xe9\xa2\x91\xe6\xac\xa1\n        self.left_child = None\n        self.right_child = None\n        self.father = None\n        self.Huffman_code = []  # \xe9\x9c\x8d\xe5\xa4\xab\xe6\x9b\xbc\xe7\xa0\x81\xef\xbc\x88\xe5\xb7\xa61\xe5\x8f\xb30\xef\xbc\x89\n        self.path = []  # \xe6\xa0\xb9\xe5\x88\xb0\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xe7\x9a\x84\xe4\xb8\xad\xe9\x97\xb4\xe8\x8a\x82\xe7\x82\xb9id\n\n\nclass HuffmanTree:\n    def __init__(self, wordid_frequency_dict):\n        self.word_count = len(wordid_frequency_dict)  # \xe5\x8d\x95\xe8\xaf\x8d\xe6\x95\xb0\xe9\x87\x8f\n        self.wordid_code = dict()\n        self.wordid_path = dict()\n        self.root = None\n        unmerge_node_list = [HuffmanNode(wordid, frequency) for wordid, frequency in\n                             wordid_frequency_dict.items()]  # \xe6\x9c\xaa\xe5\x90\x88\xe5\xb9\xb6\xe8\x8a\x82\xe7\x82\xb9list\n        self.huffman = [HuffmanNode(wordid, frequency) for wordid, frequency in\n                        wordid_frequency_dict.items()]  # \xe5\xad\x98\xe5\x82\xa8\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe5\x8f\xb6\xe5\xad\x90\xe8\x8a\x82\xe7\x82\xb9\xe5\x92\x8c\xe4\xb8\xad\xe9\x97\xb4\xe8\x8a\x82\xe7\x82\xb9\n        # \xe6\x9e\x84\xe5\xbb\xbahuffman tree\n        self.build_tree(unmerge_node_list)\n        # \xe7\x94\x9f\xe6\x88\x90huffman code\n        self.generate_huffman_code_and_path()\n\n    def merge_node(self, node1, node2):\n        sum_frequency = node1.frequency + node2.frequency\n        mid_node_id = len(self.huffman)  # \xe4\xb8\xad\xe9\x97\xb4\xe8\x8a\x82\xe7\x82\xb9\xe7\x9a\x84value\xe5\xad\x98\xe4\xb8\xad\xe9\x97\xb4\xe8\x8a\x82\xe7\x82\xb9id\n        father_node = HuffmanNode(mid_node_id, sum_frequency)\n        if node1.frequency >= node2.frequency:\n            father_node.left_child = node1\n            father_node.right_child = node2\n        else:\n            father_node.left_child = node2\n            father_node.right_child = node1\n        self.huffman.append(father_node)\n        return father_node\n\n    def build_tree(self, node_list):\n        while len(node_list) > 1:\n            i1 = 0  # \xe6\xa6\x82\xe7\x8e\x87\xe6\x9c\x80\xe5\xb0\x8f\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\n            i2 = 1  # \xe6\xa6\x82\xe7\x8e\x87\xe7\xac\xac\xe4\xba\x8c\xe5\xb0\x8f\xe7\x9a\x84\xe8\x8a\x82\xe7\x82\xb9\n            if node_list[i2].frequency < node_list[i1].frequency:\n                [i1, i2] = [i2, i1]\n            for i in range(2, len(node_list)):\n                if node_list[i].frequency < node_list[i2].frequency:\n                    i2 = i\n                    if node_list[i2].frequency < node_list[i1].frequency:\n                        [i1, i2] = [i2, i1]\n            father_node = self.merge_node(node_list[i1], node_list[i2])  # \xe5\x90\x88\xe5\xb9\xb6\xe6\x9c\x80\xe5\xb0\x8f\xe7\x9a\x84\xe4\xb8\xa4\xe4\xb8\xaa\xe8\x8a\x82\xe7\x82\xb9\n            if i1 < i2:\n                node_list.pop(i2)\n                node_list.pop(i1)\n            elif i1 > i2:\n                node_list.pop(i1)\n                node_list.pop(i2)\n            else:\n                raise RuntimeError(\'i1 should not be equal to i2\')\n            node_list.insert(0, father_node)  # \xe6\x8f\x92\xe5\x85\xa5\xe6\x96\xb0\xe8\x8a\x82\xe7\x82\xb9\n        self.root = node_list[0]\n\n    def generate_huffman_code_and_path(self):\n        stack = [self.root]\n        while len(stack) > 0:\n            node = stack.pop()\n            # \xe9\xa1\xba\xe7\x9d\x80\xe5\xb7\xa6\xe5\xad\x90\xe6\xa0\x91\xe8\xb5\xb0\n            while node.left_child or node.right_child:\n                code = node.Huffman_code\n                path = node.path\n                node.left_child.Huffman_code = code + [1]\n                node.right_child.Huffman_code = code + [0]\n                node.left_child.path = path + [node.word_id]\n                node.right_child.path = path + [node.word_id]\n                # \xe6\x8a\x8a\xe6\xb2\xa1\xe8\xb5\xb0\xe8\xbf\x87\xe7\x9a\x84\xe5\x8f\xb3\xe5\xad\x90\xe6\xa0\x91\xe5\x8a\xa0\xe5\x85\xa5\xe6\xa0\x88\n                stack.append(node.right_child)\n                node = node.left_child\n            word_id = node.word_id\n            word_code = node.Huffman_code\n            word_path = node.path\n            self.huffman[word_id].Huffman_code = word_code\n            self.huffman[word_id].path = word_path\n            # \xe6\x8a\x8a\xe8\x8a\x82\xe7\x82\xb9\xe8\xae\xa1\xe7\xae\x97\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84\xe9\x9c\x8d\xe5\xa4\xab\xe6\x9b\xbc\xe7\xa0\x81\xe3\x80\x81\xe8\xb7\xaf\xe5\xbe\x84  \xe5\x86\x99\xe5\x85\xa5\xe8\xaf\x8d\xe5\x85\xb8\xe7\x9a\x84\xe6\x95\xb0\xe5\x80\xbc\xe4\xb8\xad\n            self.wordid_code[word_id] = word_code\n            self.wordid_path[word_id] = word_path\n\n    # \xe8\x8e\xb7\xe5\x8f\x96\xe6\x89\x80\xe6\x9c\x89\xe8\xaf\x8d\xe7\x9a\x84\xe6\xad\xa3\xe5\x90\x91\xe8\x8a\x82\xe7\x82\xb9id\xe5\x92\x8c\xe8\xb4\x9f\xe5\x90\x91\xe8\x8a\x82\xe7\x82\xb9id\xe6\x95\xb0\xe7\xbb\x84\n    def get_all_pos_and_neg_path(self):\n        positive = []  # \xe6\x89\x80\xe6\x9c\x89\xe8\xaf\x8d\xe7\x9a\x84\xe6\xad\xa3\xe5\x90\x91\xe8\xb7\xaf\xe5\xbe\x84\xe6\x95\xb0\xe7\xbb\x84\n        negative = []  # \xe6\x89\x80\xe6\x9c\x89\xe8\xaf\x8d\xe7\x9a\x84\xe8\xb4\x9f\xe5\x90\x91\xe8\xb7\xaf\xe5\xbe\x84\xe6\x95\xb0\xe7\xbb\x84\n        for word_id in range(self.word_count):\n            pos_id = []  # \xe5\xad\x98\xe6\x94\xbe\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\x8d \xe8\xb7\xaf\xe5\xbe\x84\xe4\xb8\xad\xe7\x9a\x84\xe6\xad\xa3\xe5\x90\x91\xe8\x8a\x82\xe7\x82\xb9id\n            neg_id = []  # \xe5\xad\x98\xe6\x94\xbe\xe4\xb8\x80\xe4\xb8\xaa\xe8\xaf\x8d \xe8\xb7\xaf\xe5\xbe\x84\xe4\xb8\xad\xe7\x9a\x84\xe8\xb4\x9f\xe5\x90\x91\xe8\x8a\x82\xe7\x82\xb9id\n            for i, code in enumerate(self.huffman[word_id].Huffman_code):\n                if code == 1:\n                    pos_id.append(self.huffman[word_id].path[i])\n                else:\n                    neg_id.append(self.huffman[word_id].path[i])\n            positive.append(pos_id)\n            negative.append(neg_id)\n        return positive, negative\n\n\ndef test():\n    word_frequency = {0: 4, 1: 6, 2: 3, 3: 2, 4: 2}\n    print(word_frequency)\n    tree = HuffmanTree(word_frequency)\n    print(tree.wordid_code)\n    print(tree.wordid_path)\n    for i in range(len(word_frequency)):\n        print(tree.huffman[i].path)\n    print(tree.get_all_pos_and_neg_path())\n\n\nif __name__ == \'__main__\':\n    test()\n'"
lightnlp/we/skip_gram/utils/sampling.py,1,"b'import torch\nfrom torchtext.vocab import Vocab\n\n\nclass Sampling(object):\n    def __init__(self, vocab: Vocab, weight=0.75):\n        self.vocab = vocab\n        self.weight = weight\n        self.weighted_list = [self.vocab.freqs[s]**self.weight for s in self.vocab.itos]\n\n    def sampling(self, num):\n        return torch.multinomial(torch.tensor(self.weighted_list), num).tolist()\n'"
