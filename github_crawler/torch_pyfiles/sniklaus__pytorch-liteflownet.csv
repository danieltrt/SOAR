file_path,api_count,code
run.py,107,"b""#!/usr/bin/env python\n\nimport torch\n\nimport getopt\nimport math\nimport numpy\nimport os\nimport PIL\nimport PIL.Image\nimport sys\n\ntry:\n\tfrom .correlation import correlation # the custom cost volume layer\nexcept:\n\tsys.path.insert(0, './correlation'); import correlation # you should consider upgrading python\n# end\n\n##########################################################\n\nassert(int(str('').join(torch.__version__.split('.')[0:2])) >= 13) # requires at least pytorch version 1.3.0\n\ntorch.set_grad_enabled(False) # make sure to not compute gradients for computational performance\n\ntorch.backends.cudnn.enabled = True # make sure to use cudnn for computational performance\n\n##########################################################\n\narguments_strModel = 'default'\narguments_strFirst = './images/first.png'\narguments_strSecond = './images/second.png'\narguments_strOut = './out.flo'\n\nfor strOption, strArgument in getopt.getopt(sys.argv[1:], '', [ strParameter[2:] + '=' for strParameter in sys.argv[1::2] ])[0]:\n\tif strOption == '--model' and strArgument != '': arguments_strModel = strArgument # which model to use\n\tif strOption == '--first' and strArgument != '': arguments_strFirst = strArgument # path to the first frame\n\tif strOption == '--second' and strArgument != '': arguments_strSecond = strArgument # path to the second frame\n\tif strOption == '--out' and strArgument != '': arguments_strOut = strArgument # path to where the output should be stored\n# end\n\n##########################################################\n\nbackwarp_tenGrid = {}\n\ndef backwarp(tenInput, tenFlow):\n\tif str(tenFlow.size()) not in backwarp_tenGrid:\n\t\ttenHorizontal = torch.linspace(-1.0, 1.0, tenFlow.shape[3]).view(1, 1, 1, tenFlow.shape[3]).expand(tenFlow.shape[0], -1, tenFlow.shape[2], -1)\n\t\ttenVertical = torch.linspace(-1.0, 1.0, tenFlow.shape[2]).view(1, 1, tenFlow.shape[2], 1).expand(tenFlow.shape[0], -1, -1, tenFlow.shape[3])\n\n\t\tbackwarp_tenGrid[str(tenFlow.size())] = torch.cat([ tenHorizontal, tenVertical ], 1).cuda()\n\t# end\n\n\ttenFlow = torch.cat([ tenFlow[:, 0:1, :, :] / ((tenInput.shape[3] - 1.0) / 2.0), tenFlow[:, 1:2, :, :] / ((tenInput.shape[2] - 1.0) / 2.0) ], 1)\n\n\treturn torch.nn.functional.grid_sample(input=tenInput, grid=(backwarp_tenGrid[str(tenFlow.size())] + tenFlow).permute(0, 2, 3, 1), mode='bilinear', padding_mode='zeros', align_corners=True)\n# end\n\n##########################################################\n\nclass Network(torch.nn.Module):\n\tdef __init__(self):\n\t\tsuper(Network, self).__init__()\n\n\t\tclass Features(torch.nn.Module):\n\t\t\tdef __init__(self):\n\t\t\t\tsuper(Features, self).__init__()\n\n\t\t\t\tself.netOne = torch.nn.Sequential(\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=7, stride=1, padding=3),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1)\n\t\t\t\t)\n\n\t\t\t\tself.netTwo = torch.nn.Sequential(\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1)\n\t\t\t\t)\n\n\t\t\t\tself.netThr = torch.nn.Sequential(\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1)\n\t\t\t\t)\n\n\t\t\t\tself.netFou = torch.nn.Sequential(\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=64, out_channels=96, kernel_size=3, stride=2, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=96, out_channels=96, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1)\n\t\t\t\t)\n\n\t\t\t\tself.netFiv = torch.nn.Sequential(\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=96, out_channels=128, kernel_size=3, stride=2, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1)\n\t\t\t\t)\n\n\t\t\t\tself.netSix = torch.nn.Sequential(\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=128, out_channels=192, kernel_size=3, stride=2, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1)\n\t\t\t\t)\n\t\t\t# end\n\n\t\t\tdef forward(self, tenInput):\n\t\t\t\ttenOne = self.netOne(tenInput)\n\t\t\t\ttenTwo = self.netTwo(tenOne)\n\t\t\t\ttenThr = self.netThr(tenTwo)\n\t\t\t\ttenFou = self.netFou(tenThr)\n\t\t\t\ttenFiv = self.netFiv(tenFou)\n\t\t\t\ttenSix = self.netSix(tenFiv)\n\n\t\t\t\treturn [ tenOne, tenTwo, tenThr, tenFou, tenFiv, tenSix ]\n\t\t\t# end\n\t\t# end\n\n\t\tclass Matching(torch.nn.Module):\n\t\t\tdef __init__(self, intLevel):\n\t\t\t\tsuper(Matching, self).__init__()\n\n\t\t\t\tself.fltBackwarp = [ 0.0, 0.0, 10.0, 5.0, 2.5, 1.25, 0.625 ][intLevel]\n\n\t\t\t\tif intLevel != 2:\n\t\t\t\t\tself.netFeat = torch.nn.Sequential()\n\n\t\t\t\telif intLevel == 2:\n\t\t\t\t\tself.netFeat = torch.nn.Sequential(\n\t\t\t\t\t\ttorch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=1, stride=1, padding=0),\n\t\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1)\n\t\t\t\t\t)\n\n\t\t\t\t# end\n\n\t\t\t\tif intLevel == 6:\n\t\t\t\t\tself.netUpflow = None\n\n\t\t\t\telif intLevel != 6:\n\t\t\t\t\tself.netUpflow = torch.nn.ConvTranspose2d(in_channels=2, out_channels=2, kernel_size=4, stride=2, padding=1, bias=False, groups=2)\n\n\t\t\t\t# end\n\n\t\t\t\tif intLevel >= 4:\n\t\t\t\t\tself.netUpcorr = None\n\n\t\t\t\telif intLevel < 4:\n\t\t\t\t\tself.netUpcorr = torch.nn.ConvTranspose2d(in_channels=49, out_channels=49, kernel_size=4, stride=2, padding=1, bias=False, groups=49)\n\n\t\t\t\t# end\n\n\t\t\t\tself.netMain = torch.nn.Sequential(\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=49, out_channels=128, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=32, out_channels=2, kernel_size=[ 0, 0, 7, 5, 5, 3, 3 ][intLevel], stride=1, padding=[ 0, 0, 3, 2, 2, 1, 1 ][intLevel])\n\t\t\t\t)\n\t\t\t# end\n\n\t\t\tdef forward(self, tenFirst, tenSecond, tenFeaturesFirst, tenFeaturesSecond, tenFlow):\n\t\t\t\ttenFeaturesFirst = self.netFeat(tenFeaturesFirst)\n\t\t\t\ttenFeaturesSecond = self.netFeat(tenFeaturesSecond)\n\n\t\t\t\tif tenFlow is not None:\n\t\t\t\t\ttenFlow = self.netUpflow(tenFlow)\n\t\t\t\t# end\n\n\t\t\t\tif tenFlow is not None:\n\t\t\t\t\ttenFeaturesSecond = backwarp(tenInput=tenFeaturesSecond, tenFlow=tenFlow * self.fltBackwarp)\n\t\t\t\t# end\n\n\t\t\t\tif self.netUpcorr is None:\n\t\t\t\t\ttenCorrelation = torch.nn.functional.leaky_relu(input=correlation.FunctionCorrelation(tenFirst=tenFeaturesFirst, tenSecond=tenFeaturesSecond, intStride=1), negative_slope=0.1, inplace=False)\n\n\t\t\t\telif self.netUpcorr is not None:\n\t\t\t\t\ttenCorrelation = self.netUpcorr(torch.nn.functional.leaky_relu(input=correlation.FunctionCorrelation(tenFirst=tenFeaturesFirst, tenSecond=tenFeaturesSecond, intStride=2), negative_slope=0.1, inplace=False))\n\n\t\t\t\t# end\n\n\t\t\t\treturn (tenFlow if tenFlow is not None else 0.0) + self.netMain(tenCorrelation)\n\t\t\t# end\n\t\t# end\n\n\t\tclass Subpixel(torch.nn.Module):\n\t\t\tdef __init__(self, intLevel):\n\t\t\t\tsuper(Subpixel, self).__init__()\n\n\t\t\t\tself.fltBackward = [ 0.0, 0.0, 10.0, 5.0, 2.5, 1.25, 0.625 ][intLevel]\n\n\t\t\t\tif intLevel != 2:\n\t\t\t\t\tself.netFeat = torch.nn.Sequential()\n\n\t\t\t\telif intLevel == 2:\n\t\t\t\t\tself.netFeat = torch.nn.Sequential(\n\t\t\t\t\t\ttorch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=1, stride=1, padding=0),\n\t\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1)\n\t\t\t\t\t)\n\n\t\t\t\t# end\n\n\t\t\t\tself.netMain = torch.nn.Sequential(\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=[ 0, 0, 130, 130, 194, 258, 386 ][intLevel], out_channels=128, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=32, out_channels=2, kernel_size=[ 0, 0, 7, 5, 5, 3, 3 ][intLevel], stride=1, padding=[ 0, 0, 3, 2, 2, 1, 1 ][intLevel])\n\t\t\t\t)\n\t\t\t# end\n\n\t\t\tdef forward(self, tenFirst, tenSecond, tenFeaturesFirst, tenFeaturesSecond, tenFlow):\n\t\t\t\ttenFeaturesFirst = self.netFeat(tenFeaturesFirst)\n\t\t\t\ttenFeaturesSecond = self.netFeat(tenFeaturesSecond)\n\n\t\t\t\tif tenFlow is not None:\n\t\t\t\t\ttenFeaturesSecond = backwarp(tenInput=tenFeaturesSecond, tenFlow=tenFlow * self.fltBackward)\n\t\t\t\t# end\n\n\t\t\t\treturn (tenFlow if tenFlow is not None else 0.0) + self.netMain(torch.cat([ tenFeaturesFirst, tenFeaturesSecond, tenFlow ], 1))\n\t\t\t# end\n\t\t# end\n\n\t\tclass Regularization(torch.nn.Module):\n\t\t\tdef __init__(self, intLevel):\n\t\t\t\tsuper(Regularization, self).__init__()\n\n\t\t\t\tself.fltBackward = [ 0.0, 0.0, 10.0, 5.0, 2.5, 1.25, 0.625 ][intLevel]\n\n\t\t\t\tself.intUnfold = [ 0, 0, 7, 5, 5, 3, 3 ][intLevel]\n\n\t\t\t\tif intLevel >= 5:\n\t\t\t\t\tself.netFeat = torch.nn.Sequential()\n\n\t\t\t\telif intLevel < 5:\n\t\t\t\t\tself.netFeat = torch.nn.Sequential(\n\t\t\t\t\t\ttorch.nn.Conv2d(in_channels=[ 0, 0, 32, 64, 96, 128, 192 ][intLevel], out_channels=128, kernel_size=1, stride=1, padding=0),\n\t\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1)\n\t\t\t\t\t)\n\n\t\t\t\t# end\n\n\t\t\t\tself.netMain = torch.nn.Sequential(\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=[ 0, 0, 131, 131, 131, 131, 195 ][intLevel], out_channels=128, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1),\n\t\t\t\t\ttorch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1),\n\t\t\t\t\ttorch.nn.LeakyReLU(inplace=False, negative_slope=0.1)\n\t\t\t\t)\n\n\t\t\t\tif intLevel >= 5:\n\t\t\t\t\tself.netDist = torch.nn.Sequential(\n\t\t\t\t\t\ttorch.nn.Conv2d(in_channels=32, out_channels=[ 0, 0, 49, 25, 25, 9, 9 ][intLevel], kernel_size=[ 0, 0, 7, 5, 5, 3, 3 ][intLevel], stride=1, padding=[ 0, 0, 3, 2, 2, 1, 1 ][intLevel])\n\t\t\t\t\t)\n\n\t\t\t\telif intLevel < 5:\n\t\t\t\t\tself.netDist = torch.nn.Sequential(\n\t\t\t\t\t\ttorch.nn.Conv2d(in_channels=32, out_channels=[ 0, 0, 49, 25, 25, 9, 9 ][intLevel], kernel_size=([ 0, 0, 7, 5, 5, 3, 3 ][intLevel], 1), stride=1, padding=([ 0, 0, 3, 2, 2, 1, 1 ][intLevel], 0)),\n\t\t\t\t\t\ttorch.nn.Conv2d(in_channels=[ 0, 0, 49, 25, 25, 9, 9 ][intLevel], out_channels=[ 0, 0, 49, 25, 25, 9, 9 ][intLevel], kernel_size=(1, [ 0, 0, 7, 5, 5, 3, 3 ][intLevel]), stride=1, padding=(0, [ 0, 0, 3, 2, 2, 1, 1 ][intLevel]))\n\t\t\t\t\t)\n\n\t\t\t\t# end\n\n\t\t\t\tself.netScaleX = torch.nn.Conv2d(in_channels=[ 0, 0, 49, 25, 25, 9, 9 ][intLevel], out_channels=1, kernel_size=1, stride=1, padding=0)\n\t\t\t\tself.netScaleY = torch.nn.Conv2d(in_channels=[ 0, 0, 49, 25, 25, 9, 9 ][intLevel], out_channels=1, kernel_size=1, stride=1, padding=0)\n\t\t\t# eny\n\n\t\t\tdef forward(self, tenFirst, tenSecond, tenFeaturesFirst, tenFeaturesSecond, tenFlow):\n\t\t\t\ttenDifference = (tenFirst - backwarp(tenInput=tenSecond, tenFlow=tenFlow * self.fltBackward)).pow(2.0).sum(1, True).sqrt().detach()\n\n\t\t\t\ttenDist = self.netDist(self.netMain(torch.cat([ tenDifference, tenFlow - tenFlow.view(tenFlow.shape[0], 2, -1).mean(2, True).view(tenFlow.shape[0], 2, 1, 1), self.netFeat(tenFeaturesFirst) ], 1)))\n\t\t\t\ttenDist = tenDist.pow(2.0).neg()\n\t\t\t\ttenDist = (tenDist - tenDist.max(1, True)[0]).exp()\n\n\t\t\t\ttenDivisor = tenDist.sum(1, True).reciprocal()\n\n\t\t\t\ttenScaleX = self.netScaleX(tenDist * torch.nn.functional.unfold(input=tenFlow[:, 0:1, :, :], kernel_size=self.intUnfold, stride=1, padding=int((self.intUnfold - 1) / 2)).view_as(tenDist)) * tenDivisor\n\t\t\t\ttenScaleY = self.netScaleY(tenDist * torch.nn.functional.unfold(input=tenFlow[:, 1:2, :, :], kernel_size=self.intUnfold, stride=1, padding=int((self.intUnfold - 1) / 2)).view_as(tenDist)) * tenDivisor\n\n\t\t\t\treturn torch.cat([ tenScaleX, tenScaleY ], 1)\n\t\t\t# end\n\t\t# end\n\n\t\tself.netFeatures = Features()\n\t\tself.netMatching = torch.nn.ModuleList([ Matching(intLevel) for intLevel in [ 2, 3, 4, 5, 6 ] ])\n\t\tself.netSubpixel = torch.nn.ModuleList([ Subpixel(intLevel) for intLevel in [ 2, 3, 4, 5, 6 ] ])\n\t\tself.netRegularization = torch.nn.ModuleList([ Regularization(intLevel) for intLevel in [ 2, 3, 4, 5, 6 ] ])\n\n\t\tself.load_state_dict({ strKey.replace('module', 'net'): tenWeight for strKey, tenWeight in torch.load(__file__.replace('run.py', 'network-' + arguments_strModel + '.pytorch')).items() })\n\t# end\n\n\tdef forward(self, tenFirst, tenSecond):\n\t\ttenFirst[:, 0, :, :] = tenFirst[:, 0, :, :] - 0.411618\n\t\ttenFirst[:, 1, :, :] = tenFirst[:, 1, :, :] - 0.434631\n\t\ttenFirst[:, 2, :, :] = tenFirst[:, 2, :, :] - 0.454253\n\n\t\ttenSecond[:, 0, :, :] = tenSecond[:, 0, :, :] - 0.410782\n\t\ttenSecond[:, 1, :, :] = tenSecond[:, 1, :, :] - 0.433645\n\t\ttenSecond[:, 2, :, :] = tenSecond[:, 2, :, :] - 0.452793\n\n\t\ttenFeaturesFirst = self.netFeatures(tenFirst)\n\t\ttenFeaturesSecond = self.netFeatures(tenSecond)\n\n\t\ttenFirst = [ tenFirst ]\n\t\ttenSecond = [ tenSecond ]\n\n\t\tfor intLevel in [ 1, 2, 3, 4, 5 ]:\n\t\t\ttenFirst.append(torch.nn.functional.interpolate(input=tenFirst[-1], size=(tenFeaturesFirst[intLevel].shape[2], tenFeaturesFirst[intLevel].shape[3]), mode='bilinear', align_corners=False))\n\t\t\ttenSecond.append(torch.nn.functional.interpolate(input=tenSecond[-1], size=(tenFeaturesSecond[intLevel].shape[2], tenFeaturesSecond[intLevel].shape[3]), mode='bilinear', align_corners=False))\n\t\t# end\n\n\t\ttenFlow = None\n\n\t\tfor intLevel in [ -1, -2, -3, -4, -5 ]:\n\t\t\ttenFlow = self.netMatching[intLevel](tenFirst[intLevel], tenSecond[intLevel], tenFeaturesFirst[intLevel], tenFeaturesSecond[intLevel], tenFlow)\n\t\t\ttenFlow = self.netSubpixel[intLevel](tenFirst[intLevel], tenSecond[intLevel], tenFeaturesFirst[intLevel], tenFeaturesSecond[intLevel], tenFlow)\n\t\t\ttenFlow = self.netRegularization[intLevel](tenFirst[intLevel], tenSecond[intLevel], tenFeaturesFirst[intLevel], tenFeaturesSecond[intLevel], tenFlow)\n\t\t# end\n\n\t\treturn tenFlow * 20.0\n\t# end\n# end\n\nnetNetwork = None\n\n##########################################################\n\ndef estimate(tenFirst, tenSecond):\n\tglobal netNetwork\n\n\tif netNetwork is None:\n\t\tnetNetwork = Network().cuda().eval()\n\t# end\n\n\tassert(tenFirst.shape[1] == tenSecond.shape[1])\n\tassert(tenFirst.shape[2] == tenSecond.shape[2])\n\n\tintWidth = tenFirst.shape[2]\n\tintHeight = tenFirst.shape[1]\n\n\tassert(intWidth == 1024) # remember that there is no guarantee for correctness, comment this line out if you acknowledge this and want to continue\n\tassert(intHeight == 436) # remember that there is no guarantee for correctness, comment this line out if you acknowledge this and want to continue\n\n\ttenPreprocessedFirst = tenFirst.cuda().view(1, 3, intHeight, intWidth)\n\ttenPreprocessedSecond = tenSecond.cuda().view(1, 3, intHeight, intWidth)\n\n\tintPreprocessedWidth = int(math.floor(math.ceil(intWidth / 32.0) * 32.0))\n\tintPreprocessedHeight = int(math.floor(math.ceil(intHeight / 32.0) * 32.0))\n\n\ttenPreprocessedFirst = torch.nn.functional.interpolate(input=tenPreprocessedFirst, size=(intPreprocessedHeight, intPreprocessedWidth), mode='bilinear', align_corners=False)\n\ttenPreprocessedSecond = torch.nn.functional.interpolate(input=tenPreprocessedSecond, size=(intPreprocessedHeight, intPreprocessedWidth), mode='bilinear', align_corners=False)\n\n\ttenFlow = torch.nn.functional.interpolate(input=netNetwork(tenPreprocessedFirst, tenPreprocessedSecond), size=(intHeight, intWidth), mode='bilinear', align_corners=False)\n\n\ttenFlow[:, 0, :, :] *= float(intWidth) / float(intPreprocessedWidth)\n\ttenFlow[:, 1, :, :] *= float(intHeight) / float(intPreprocessedHeight)\n\n\treturn tenFlow[0, :, :, :].cpu()\n# end\n\n##########################################################\n\nif __name__ == '__main__':\n\ttenFirst = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(arguments_strFirst))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n\ttenSecond = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(arguments_strSecond))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n\n\ttenOutput = estimate(tenFirst, tenSecond)\n\n\tobjOutput = open(arguments_strOut, 'wb')\n\n\tnumpy.array([ 80, 73, 69, 72 ], numpy.uint8).tofile(objOutput)\n\tnumpy.array([ tenOutput.shape[2], tenOutput.shape[1] ], numpy.int32).tofile(objOutput)\n\tnumpy.array(tenOutput.numpy().transpose(1, 2, 0), numpy.float32).tofile(objOutput)\n\n\tobjOutput.close()\n# end"""
comparison/comparison.py,1,"b""#!/usr/bin/env python\n\nimport math\nimport moviepy\nimport moviepy.editor\nimport numpy\nimport PIL\nimport PIL.Image\nimport PIL.ImageFont\nimport PIL.ImageDraw\n\nintX = 32\nintY = 436 - 64\n\nobjImages = [ {\n\t'strFile': 'official - caffe.png',\n\t'strText': 'official - Caffe'\n}, {\n\t'strFile': 'this - pytorch.png',\n\t'strText': 'this - PyTorch'\n} ]\n\nnpyImages = []\n\nfor objImage in objImages:\n\tobjOutput = PIL.Image.open(objImage['strFile']).convert('RGB')\n\n\tfor intU in [ intShift - 10 for intShift in range(20) ]:\n\t\tfor intV in [ intShift - 10 for intShift in range(20) ]:\n\t\t\tif math.sqrt(math.pow(intU, 2.0) + math.pow(intV, 2.0)) <= 5.0:\n\t\t\t\tPIL.ImageDraw.Draw(objOutput).text((intX + intU, intY + intV), objImage['strText'], (255, 255, 255), PIL.ImageFont.truetype('freefont/FreeSerifBold.ttf', 32))\n\t\t\t# end\n\t\t# end\n\t# end\n\n\tPIL.ImageDraw.Draw(objOutput).text((intX, intY), objImage['strText'], (0, 0, 0), PIL.ImageFont.truetype('freefont/FreeSerifBold.ttf', 32))\n\n\tnpyImages.append(numpy.array(objOutput))\n# end\n\nmoviepy.editor.ImageSequenceClip(sequence=npyImages, fps=1).write_gif(filename='comparison.gif', program='ImageMagick', opt='optimizeplus')"""
correlation/correlation.py,2,"b'#!/usr/bin/env python\n\nimport torch\n\nimport cupy\nimport math\nimport re\n\nkernel_Correlation_rearrange = \'\'\'\n\textern ""C"" __global__ void kernel_Correlation_rearrange(\n\t\tconst int n,\n\t\tconst float* input,\n\t\tfloat* output\n\t) {\n\t  int intIndex = (blockIdx.x * blockDim.x) + threadIdx.x;\n\n\t  if (intIndex >= n) {\n\t    return;\n\t  }\n\n\t  int intSample = blockIdx.z;\n\t  int intChannel = blockIdx.y;\n\n\t  float fltValue = input[(((intSample * SIZE_1(input)) + intChannel) * SIZE_2(input) * SIZE_3(input)) + intIndex];\n\n\t  __syncthreads();\n\n\t  int intPaddedY = (intIndex / SIZE_3(input)) + 3*{{intStride}};\n\t  int intPaddedX = (intIndex % SIZE_3(input)) + 3*{{intStride}};\n\t  int intRearrange = ((SIZE_3(input) + 6*{{intStride}}) * intPaddedY) + intPaddedX;\n\n\t  output[(((intSample * SIZE_1(output) * SIZE_2(output)) + intRearrange) * SIZE_1(input)) + intChannel] = fltValue;\n\t}\n\'\'\'\n\nkernel_Correlation_updateOutput = \'\'\'\n\textern ""C"" __global__ void kernel_Correlation_updateOutput(\n\t  const int n,\n\t  const float* rbot0,\n\t  const float* rbot1,\n\t  float* top\n\t) {\n\t  extern __shared__ char patch_data_char[];\n\t  \n\t  float *patch_data = (float *)patch_data_char;\n\t  \n\t  // First (upper left) position of kernel upper-left corner in current center position of neighborhood in image 1\n\t  int x1 = (blockIdx.x + 3) * {{intStride}};\n\t  int y1 = (blockIdx.y + 3) * {{intStride}};\n\t  int item = blockIdx.z;\n\t  int ch_off = threadIdx.x;\n\t  \n\t  // Load 3D patch into shared shared memory\n\t  for (int j = 0; j < 1; j++) { // HEIGHT\n\t    for (int i = 0; i < 1; i++) { // WIDTH\n\t      int ji_off = (j + i) * SIZE_3(rbot0);\n\t      for (int ch = ch_off; ch < SIZE_3(rbot0); ch += 32) { // CHANNELS\n\t        int idx1 = ((item * SIZE_1(rbot0) + y1+j) * SIZE_2(rbot0) + x1+i) * SIZE_3(rbot0) + ch;\n\t        int idxPatchData = ji_off + ch;\n\t        patch_data[idxPatchData] = rbot0[idx1];\n\t      }\n\t    }\n\t  }\n\t  \n\t  __syncthreads();\n\t  \n\t  __shared__ float sum[32];\n\t  \n\t  // Compute correlation\n\t  for (int top_channel = 0; top_channel < SIZE_1(top); top_channel++) {\n\t    sum[ch_off] = 0;\n\t  \n\t    int s2o = (top_channel % 7 - 3) * {{intStride}};\n\t    int s2p = (top_channel / 7 - 3) * {{intStride}};\n\t    \n\t    for (int j = 0; j < 1; j++) { // HEIGHT\n\t      for (int i = 0; i < 1; i++) { // WIDTH\n\t        int ji_off = (j + i) * SIZE_3(rbot0);\n\t        for (int ch = ch_off; ch < SIZE_3(rbot0); ch += 32) { // CHANNELS\n\t          int x2 = x1 + s2o;\n\t          int y2 = y1 + s2p;\n\t          \n\t          int idxPatchData = ji_off + ch;\n\t          int idx2 = ((item * SIZE_1(rbot0) + y2+j) * SIZE_2(rbot0) + x2+i) * SIZE_3(rbot0) + ch;\n\t          \n\t          sum[ch_off] += patch_data[idxPatchData] * rbot1[idx2];\n\t        }\n\t      }\n\t    }\n\t    \n\t    __syncthreads();\n\t    \n\t    if (ch_off == 0) {\n\t      float total_sum = 0;\n\t      for (int idx = 0; idx < 32; idx++) {\n\t        total_sum += sum[idx];\n\t      }\n\t      const int sumelems = SIZE_3(rbot0);\n\t      const int index = ((top_channel*SIZE_2(top) + blockIdx.y)*SIZE_3(top))+blockIdx.x;\n\t      top[index + item*SIZE_1(top)*SIZE_2(top)*SIZE_3(top)] = total_sum / (float)sumelems;\n\t    }\n\t  }\n\t}\n\'\'\'\n\nkernel_Correlation_updateGradFirst = \'\'\'\n\t#define ROUND_OFF 50000\n\n\textern ""C"" __global__ void kernel_Correlation_updateGradFirst(\n\t  const int n,\n\t  const int intSample,\n\t  const float* rbot0,\n\t  const float* rbot1,\n\t  const float* gradOutput,\n\t  float* gradFirst,\n\t  float* gradSecond\n\t) { for (int intIndex = (blockIdx.x * blockDim.x) + threadIdx.x; intIndex < n; intIndex += blockDim.x * gridDim.x) {\n\t  int n = intIndex % SIZE_1(gradFirst); // channels\n\t  int l = (intIndex / SIZE_1(gradFirst)) % SIZE_3(gradFirst) + 3*{{intStride}}; // w-pos\n\t  int m = (intIndex / SIZE_1(gradFirst) / SIZE_3(gradFirst)) % SIZE_2(gradFirst) + 3*{{intStride}}; // h-pos\n\t  \n\t  // round_off is a trick to enable integer division with ceil, even for negative numbers\n\t  // We use a large offset, for the inner part not to become negative.\n\t  const int round_off = ROUND_OFF;\n\t  const int round_off_s1 = {{intStride}} * round_off;\n\t  \n\t  // We add round_off before_s1 the int division and subtract round_off after it, to ensure the formula matches ceil behavior:\n\t  int xmin = (l - 3*{{intStride}} + round_off_s1 - 1) / {{intStride}} + 1 - round_off; // ceil (l - 3*{{intStride}}) / {{intStride}}\n\t  int ymin = (m - 3*{{intStride}} + round_off_s1 - 1) / {{intStride}} + 1 - round_off; // ceil (l - 3*{{intStride}}) / {{intStride}}\n\t  \n\t  // Same here:\n\t  int xmax = (l - 3*{{intStride}} + round_off_s1) / {{intStride}} - round_off; // floor (l - 3*{{intStride}}) / {{intStride}}\n\t  int ymax = (m - 3*{{intStride}} + round_off_s1) / {{intStride}} - round_off; // floor (m - 3*{{intStride}}) / {{intStride}}\n\t  \n\t  float sum = 0;\n\t  if (xmax>=0 && ymax>=0 && (xmin<=SIZE_3(gradOutput)-1) && (ymin<=SIZE_2(gradOutput)-1)) {\n\t    xmin = max(0,xmin);\n\t    xmax = min(SIZE_3(gradOutput)-1,xmax);\n\t    \n\t    ymin = max(0,ymin);\n\t    ymax = min(SIZE_2(gradOutput)-1,ymax);\n\t    \n\t    for (int p = -3; p <= 3; p++) {\n\t      for (int o = -3; o <= 3; o++) {\n\t        // Get rbot1 data:\n\t        int s2o = {{intStride}} * o;\n\t        int s2p = {{intStride}} * p;\n\t        int idxbot1 = ((intSample * SIZE_1(rbot0) + (m+s2p)) * SIZE_2(rbot0) + (l+s2o)) * SIZE_3(rbot0) + n;\n\t        float bot1tmp = rbot1[idxbot1]; // rbot1[l+s2o,m+s2p,n]\n\t        \n\t        // Index offset for gradOutput in following loops:\n\t        int op = (p+3) * 7 + (o+3); // index[o,p]\n\t        int idxopoffset = (intSample * SIZE_1(gradOutput) + op);\n\t        \n\t        for (int y = ymin; y <= ymax; y++) {\n\t          for (int x = xmin; x <= xmax; x++) {\n\t            int idxgradOutput = (idxopoffset * SIZE_2(gradOutput) + y) * SIZE_3(gradOutput) + x; // gradOutput[x,y,o,p]\n\t            sum += gradOutput[idxgradOutput] * bot1tmp;\n\t          }\n\t        }\n\t      }\n\t    }\n\t  }\n\t  const int sumelems = SIZE_1(gradFirst);\n\t  const int bot0index = ((n * SIZE_2(gradFirst)) + (m-3*{{intStride}})) * SIZE_3(gradFirst) + (l-3*{{intStride}});\n\t  gradFirst[bot0index + intSample*SIZE_1(gradFirst)*SIZE_2(gradFirst)*SIZE_3(gradFirst)] = sum / (float)sumelems;\n\t} }\n\'\'\'\n\nkernel_Correlation_updateGradSecond = \'\'\'\n\t#define ROUND_OFF 50000\n\n\textern ""C"" __global__ void kernel_Correlation_updateGradSecond(\n\t  const int n,\n\t  const int intSample,\n\t  const float* rbot0,\n\t  const float* rbot1,\n\t  const float* gradOutput,\n\t  float* gradFirst,\n\t  float* gradSecond\n\t) { for (int intIndex = (blockIdx.x * blockDim.x) + threadIdx.x; intIndex < n; intIndex += blockDim.x * gridDim.x) {\n\t  int n = intIndex % SIZE_1(gradSecond); // channels\n\t  int l = (intIndex / SIZE_1(gradSecond)) % SIZE_3(gradSecond) + 3*{{intStride}}; // w-pos\n\t  int m = (intIndex / SIZE_1(gradSecond) / SIZE_3(gradSecond)) % SIZE_2(gradSecond) + 3*{{intStride}}; // h-pos\n\t  \n\t  // round_off is a trick to enable integer division with ceil, even for negative numbers\n\t  // We use a large offset, for the inner part not to become negative.\n\t  const int round_off = ROUND_OFF;\n\t  const int round_off_s1 = {{intStride}} * round_off;\n\t  \n\t  float sum = 0;\n\t  for (int p = -3; p <= 3; p++) {\n\t    for (int o = -3; o <= 3; o++) {\n\t      int s2o = {{intStride}} * o;\n\t      int s2p = {{intStride}} * p;\n\t      \n\t      //Get X,Y ranges and clamp\n\t      // We add round_off before_s1 the int division and subtract round_off after it, to ensure the formula matches ceil behavior:\n\t      int xmin = (l - 3*{{intStride}} - s2o + round_off_s1 - 1) / {{intStride}} + 1 - round_off; // ceil (l - 3*{{intStride}} - s2o) / {{intStride}}\n\t      int ymin = (m - 3*{{intStride}} - s2p + round_off_s1 - 1) / {{intStride}} + 1 - round_off; // ceil (l - 3*{{intStride}} - s2o) / {{intStride}}\n\t      \n\t      // Same here:\n\t      int xmax = (l - 3*{{intStride}} - s2o + round_off_s1) / {{intStride}} - round_off; // floor (l - 3*{{intStride}} - s2o) / {{intStride}}\n\t      int ymax = (m - 3*{{intStride}} - s2p + round_off_s1) / {{intStride}} - round_off; // floor (m - 3*{{intStride}} - s2p) / {{intStride}}\n          \n\t      if (xmax>=0 && ymax>=0 && (xmin<=SIZE_3(gradOutput)-1) && (ymin<=SIZE_2(gradOutput)-1)) {\n\t        xmin = max(0,xmin);\n\t        xmax = min(SIZE_3(gradOutput)-1,xmax);\n\t        \n\t        ymin = max(0,ymin);\n\t        ymax = min(SIZE_2(gradOutput)-1,ymax);\n\t        \n\t        // Get rbot0 data:\n\t        int idxbot0 = ((intSample * SIZE_1(rbot0) + (m-s2p)) * SIZE_2(rbot0) + (l-s2o)) * SIZE_3(rbot0) + n;\n\t        float bot0tmp = rbot0[idxbot0]; // rbot1[l+s2o,m+s2p,n]\n\t        \n\t        // Index offset for gradOutput in following loops:\n\t        int op = (p+3) * 7 + (o+3); // index[o,p]\n\t        int idxopoffset = (intSample * SIZE_1(gradOutput) + op);\n\t        \n\t        for (int y = ymin; y <= ymax; y++) {\n\t          for (int x = xmin; x <= xmax; x++) {\n\t            int idxgradOutput = (idxopoffset * SIZE_2(gradOutput) + y) * SIZE_3(gradOutput) + x; // gradOutput[x,y,o,p]\n\t            sum += gradOutput[idxgradOutput] * bot0tmp;\n\t          }\n\t        }\n\t      }\n\t    }\n\t  }\n\t  const int sumelems = SIZE_1(gradSecond);\n\t  const int bot1index = ((n * SIZE_2(gradSecond)) + (m-3*{{intStride}})) * SIZE_3(gradSecond) + (l-3*{{intStride}});\n\t  gradSecond[bot1index + intSample*SIZE_1(gradSecond)*SIZE_2(gradSecond)*SIZE_3(gradSecond)] = sum / (float)sumelems;\n\t} }\n\'\'\'\n\ndef cupy_kernel(strFunction, objVariables):\n\tstrKernel = globals()[strFunction].replace(\'{{intStride}}\', str(objVariables[\'intStride\']))\n\n\twhile True:\n\t\tobjMatch = re.search(\'(SIZE_)([0-4])(\\()([^\\)]*)(\\))\', strKernel)\n\n\t\tif objMatch is None:\n\t\t\tbreak\n\t\t# end\n\n\t\tintArg = int(objMatch.group(2))\n\n\t\tstrTensor = objMatch.group(4)\n\t\tintSizes = objVariables[strTensor].size()\n\n\t\tstrKernel = strKernel.replace(objMatch.group(), str(intSizes[intArg]))\n\t# end\n\n\twhile True:\n\t\tobjMatch = re.search(\'(VALUE_)([0-4])(\\()([^\\)]+)(\\))\', strKernel)\n\n\t\tif objMatch is None:\n\t\t\tbreak\n\t\t# end\n\n\t\tintArgs = int(objMatch.group(2))\n\t\tstrArgs = objMatch.group(4).split(\',\')\n\n\t\tstrTensor = strArgs[0]\n\t\tintStrides = objVariables[strTensor].stride()\n\t\tstrIndex = [ \'((\' + strArgs[intArg + 1].replace(\'{\', \'(\').replace(\'}\', \')\').strip() + \')*\' + str(intStrides[intArg]) + \')\' for intArg in range(intArgs) ]\n\n\t\tstrKernel = strKernel.replace(objMatch.group(0), strTensor + \'[\' + str.join(\'+\', strIndex) + \']\')\n\t# end\n\n\treturn strKernel\n# end\n\n@cupy.util.memoize(for_each_device=True)\ndef cupy_launch(strFunction, strKernel):\n\treturn cupy.cuda.compile_with_cache(strKernel).get_function(strFunction)\n# end\n\nclass _FunctionCorrelation(torch.autograd.Function):\n\t@staticmethod\n\tdef forward(self, first, second, intStride):\n\t\trbot0 = first.new_zeros([ first.shape[0], first.shape[2] + (6 * intStride), first.shape[3] + (6 * intStride), first.shape[1] ])\n\t\trbot1 = first.new_zeros([ first.shape[0], first.shape[2] + (6 * intStride), first.shape[3] + (6 * intStride), first.shape[1] ])\n\n\t\tself.save_for_backward(first, second, rbot0, rbot1)\n\n\t\tself.intStride = intStride\n\n\t\tassert(first.is_contiguous() == True)\n\t\tassert(second.is_contiguous() == True)\n\n\t\toutput = first.new_zeros([ first.shape[0], 49, int(math.ceil(first.shape[2] / intStride)), int(math.ceil(first.shape[3] / intStride)) ])\n\n\t\tif first.is_cuda == True:\n\t\t\tn = first.shape[2] * first.shape[3]\n\t\t\tcupy_launch(\'kernel_Correlation_rearrange\', cupy_kernel(\'kernel_Correlation_rearrange\', {\n\t\t\t\t\'intStride\': self.intStride,\n\t\t\t\t\'input\': first,\n\t\t\t\t\'output\': rbot0\n\t\t\t}))(\n\t\t\t\tgrid=tuple([ int((n + 16 - 1) / 16), first.shape[1], first.shape[0] ]),\n\t\t\t\tblock=tuple([ 16, 1, 1 ]),\n\t\t\t\targs=[ n, first.data_ptr(), rbot0.data_ptr() ]\n\t\t\t)\n\n\t\t\tn = second.shape[2] * second.shape[3]\n\t\t\tcupy_launch(\'kernel_Correlation_rearrange\', cupy_kernel(\'kernel_Correlation_rearrange\', {\n\t\t\t\t\'intStride\': self.intStride,\n\t\t\t\t\'input\': second,\n\t\t\t\t\'output\': rbot1\n\t\t\t}))(\n\t\t\t\tgrid=tuple([ int((n + 16 - 1) / 16), second.shape[1], second.shape[0] ]),\n\t\t\t\tblock=tuple([ 16, 1, 1 ]),\n\t\t\t\targs=[ n, second.data_ptr(), rbot1.data_ptr() ]\n\t\t\t)\n\n\t\t\tn = output.shape[1] * output.shape[2] * output.shape[3]\n\t\t\tcupy_launch(\'kernel_Correlation_updateOutput\', cupy_kernel(\'kernel_Correlation_updateOutput\', {\n\t\t\t\t\'intStride\': self.intStride,\n\t\t\t\t\'rbot0\': rbot0,\n\t\t\t\t\'rbot1\': rbot1,\n\t\t\t\t\'top\': output\n\t\t\t}))(\n\t\t\t\tgrid=tuple([ output.shape[3], output.shape[2], output.shape[0] ]),\n\t\t\t\tblock=tuple([ 32, 1, 1 ]),\n\t\t\t\tshared_mem=first.shape[1] * 4,\n\t\t\t\targs=[ n, rbot0.data_ptr(), rbot1.data_ptr(), output.data_ptr() ]\n\t\t\t)\n\n\t\telif first.is_cuda == False:\n\t\t\traise NotImplementedError()\n\n\t\t# end\n\n\t\treturn output\n\t# end\n\n\t@staticmethod\n\tdef backward(self, gradOutput):\n\t\tfirst, second, rbot0, rbot1 = self.saved_tensors\n\n\t\tassert(gradOutput.is_contiguous() == True)\n\n\t\tgradFirst = first.new_zeros([ first.shape[0], first.shape[1], first.shape[2], first.shape[3] ]) if self.needs_input_grad[0] == True else None\n\t\tgradSecond = first.new_zeros([ first.shape[0], first.shape[1], first.shape[2], first.shape[3] ]) if self.needs_input_grad[1] == True else None\n\n\t\tif first.is_cuda == True:\n\t\t\tif gradFirst is not None:\n\t\t\t\tfor intSample in range(first.shape[0]):\n\t\t\t\t\tn = first.shape[1] * first.shape[2] * first.shape[3]\n\t\t\t\t\tcupy_launch(\'kernel_Correlation_updateGradFirst\', cupy_kernel(\'kernel_Correlation_updateGradFirst\', {\n\t\t\t\t\t\t\'intStride\': self.intStride,\n\t\t\t\t\t\t\'rbot0\': rbot0,\n\t\t\t\t\t\t\'rbot1\': rbot1,\n\t\t\t\t\t\t\'gradOutput\': gradOutput,\n\t\t\t\t\t\t\'gradFirst\': gradFirst,\n\t\t\t\t\t\t\'gradSecond\': None\n\t\t\t\t\t}))(\n\t\t\t\t\t\tgrid=tuple([ int((n + 512 - 1) / 512), 1, 1 ]),\n\t\t\t\t\t\tblock=tuple([ 512, 1, 1 ]),\n\t\t\t\t\t\targs=[ n, intSample, rbot0.data_ptr(), rbot1.data_ptr(), gradOutput.data_ptr(), gradFirst.data_ptr(), None ]\n\t\t\t\t\t)\n\t\t\t\t# end\n\t\t\t# end\n\n\t\t\tif gradSecond is not None:\n\t\t\t\tfor intSample in range(first.shape[0]):\n\t\t\t\t\tn = first.shape[1] * first.shape[2] * first.shape[3]\n\t\t\t\t\tcupy_launch(\'kernel_Correlation_updateGradSecond\', cupy_kernel(\'kernel_Correlation_updateGradSecond\', {\n\t\t\t\t\t\t\'intStride\': self.intStride,\n\t\t\t\t\t\t\'rbot0\': rbot0,\n\t\t\t\t\t\t\'rbot1\': rbot1,\n\t\t\t\t\t\t\'gradOutput\': gradOutput,\n\t\t\t\t\t\t\'gradFirst\': None,\n\t\t\t\t\t\t\'gradSecond\': gradSecond\n\t\t\t\t\t}))(\n\t\t\t\t\t\tgrid=tuple([ int((n + 512 - 1) / 512), 1, 1 ]),\n\t\t\t\t\t\tblock=tuple([ 512, 1, 1 ]),\n\t\t\t\t\t\targs=[ n, intSample, rbot0.data_ptr(), rbot1.data_ptr(), gradOutput.data_ptr(), None, gradSecond.data_ptr() ]\n\t\t\t\t\t)\n\t\t\t\t# end\n\t\t\t# end\n\n\t\telif first.is_cuda == False:\n\t\t\traise NotImplementedError()\n\n\t\t# end\n\n\t\treturn gradFirst, gradSecond, None\n\t# end\n# end\n\ndef FunctionCorrelation(tenFirst, tenSecond, intStride):\n\treturn _FunctionCorrelation.apply(tenFirst, tenSecond, intStride)\n# end\n\nclass ModuleCorrelation(torch.nn.Module):\n\tdef __init__(self):\n\t\tsuper(ModuleCorrelation, self).__init__()\n\t# end\n\n\tdef forward(self, tenFirst, tenSecond, intStride):\n\t\treturn _FunctionCorrelation.apply(tenFirst, tenSecond, intStride)\n\t# end\n# end'"
