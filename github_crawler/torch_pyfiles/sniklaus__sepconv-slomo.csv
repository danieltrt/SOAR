file_path,api_count,code
benchmark.py,2,"b""#!/usr/bin/env python\n\nimport torch\n\nimport glob\nimport numpy\nimport PIL\nimport PIL.Image\nimport skimage\nimport skimage.measure\n\nimport run\n\n##########################################################\n\nrun.arguments_strModel = 'l1' # making sure to load the l1 model since it is the one that should be used for quantiative evaluations\n\n##########################################################\n\nif __name__ == '__main__':\n\tfltPsnr = []\n\tfltSsim = []\n\n\tfor strTruth in sorted(glob.glob('./middlebury/*/frame10i11.png')):\n\t\ttenFirst = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(strTruth.replace('frame10i11', 'frame10')))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n\t\ttenSecond = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(strTruth.replace('frame10i11', 'frame11')))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n\t\t\n\t\tnpyEstimate = (run.estimate(tenFirst, tenSecond).clamp(0.0, 1.0).numpy().transpose(1, 2, 0) * 255.0).astype(numpy.uint8)\n\n\t\tfltPsnr.append(skimage.measure.compare_psnr(im_true=numpy.array(PIL.Image.open(strTruth))[:, :, ::-1], im_test=npyEstimate, data_range=255))\n\t\tfltSsim.append(skimage.measure.compare_ssim(X=numpy.array(PIL.Image.open(strTruth))[:, :, ::-1], Y=npyEstimate, data_range=255, multichannel=True))\n\t# end\n\n\tprint('computed average psnr', numpy.mean(fltPsnr))\n\tprint('computed average ssim', numpy.mean(fltSsim))\n\tprint('')\n\tprint('see table below for reference results')\n\tprint('')\n\tprint('+---------+------------+---------+---------+')\n\tprint('| model   | padding    | psnr    | ssim    |')\n\tprint('+---------+------------+---------+---------+')\n\tprint('| l1      | paper      | 35.73   | 0.959   |')\n\tprint('| lf      | paper      | 35.03   | 0.954   |')\n\tprint('| l1      | improved   | 35.85   | 0.959   |')\n\tprint('| lf      | improved   | 35.16   | 0.954   |')\n\tprint('+---------+------------+---------+---------+')\n# end"""
run.py,40,"b""#!/usr/bin/env python\n\nimport torch\n\nimport getopt\nimport math\nimport numpy\nimport os\nimport PIL\nimport PIL.Image\nimport random\nimport shutil\nimport sys\nimport tempfile\n\ntry:\n\tfrom .sepconv import sepconv # the custom separable convolution layer\nexcept:\n\tsys.path.insert(0, './sepconv'); import sepconv # you should consider upgrading python\n# end\n\n##########################################################\n\nassert(int(str('').join(torch.__version__.split('.')[0:2])) >= 13) # requires at least pytorch version 1.3.0\n\ntorch.set_grad_enabled(False) # make sure to not compute gradients for computational performance\n\ntorch.backends.cudnn.enabled = True # make sure to use cudnn for computational performance\n\n##########################################################\n\narguments_strModel = 'lf'\narguments_strPadding = 'improved'\narguments_strFirst = './images/first.png'\narguments_strSecond = './images/second.png'\narguments_strVideo = './videos/car-turn.mp4'\narguments_strOut = './out.png'\n\nfor strOption, strArgument in getopt.getopt(sys.argv[1:], '', [ strParameter[2:] + '=' for strParameter in sys.argv[1::2] ])[0]:\n\tif strOption == '--model' and strArgument != '': arguments_strModel = strArgument # which model to use, l1 or lf, please see our paper for more details\n\tif strOption == '--padding' and strArgument != '': arguments_strPadding = strArgument # which padding to use, the one used in the paper or the improved one\n\tif strOption == '--first' and strArgument != '': arguments_strFirst = strArgument # path to the first frame\n\tif strOption == '--second' and strArgument != '': arguments_strSecond = strArgument # path to the second frame\n\tif strOption == '--video' and strArgument != '': arguments_strVideo = strArgument # path to a video\n\tif strOption == '--out' and strArgument != '': arguments_strOut = strArgument # path to where the output should be stored\n# end\n\n##########################################################\n\nclass Network(torch.nn.Module):\n\tdef __init__(self):\n\t\tsuper(Network, self).__init__()\n\n\t\tdef Basic(intInput, intOutput):\n\t\t\treturn torch.nn.Sequential(\n\t\t\t\ttorch.nn.Conv2d(in_channels=intInput, out_channels=intOutput, kernel_size=3, stride=1, padding=1),\n\t\t\t\ttorch.nn.ReLU(inplace=False),\n\t\t\t\ttorch.nn.Conv2d(in_channels=intOutput, out_channels=intOutput, kernel_size=3, stride=1, padding=1),\n\t\t\t\ttorch.nn.ReLU(inplace=False),\n\t\t\t\ttorch.nn.Conv2d(in_channels=intOutput, out_channels=intOutput, kernel_size=3, stride=1, padding=1),\n\t\t\t\ttorch.nn.ReLU(inplace=False)\n\t\t\t)\n\t\t# end\n\n\t\tdef Upsample(intInput, intOutput):\n\t\t\treturn torch.nn.Sequential(\n\t\t\t\ttorch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n\t\t\t\ttorch.nn.Conv2d(in_channels=intOutput, out_channels=intOutput, kernel_size=3, stride=1, padding=1),\n\t\t\t\ttorch.nn.ReLU(inplace=False)\n\t\t\t)\n\t\t# end\n\n\t\tdef Subnet():\n\t\t\treturn torch.nn.Sequential(\n\t\t\t\ttorch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n\t\t\t\ttorch.nn.ReLU(inplace=False),\n\t\t\t\ttorch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n\t\t\t\ttorch.nn.ReLU(inplace=False),\n\t\t\t\ttorch.nn.Conv2d(in_channels=64, out_channels=51, kernel_size=3, stride=1, padding=1),\n\t\t\t\ttorch.nn.ReLU(inplace=False),\n\t\t\t\ttorch.nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n\t\t\t\ttorch.nn.Conv2d(in_channels=51, out_channels=51, kernel_size=3, stride=1, padding=1)\n\t\t\t)\n\t\t# end\n\n\t\tself.netConv1 = Basic(6, 32)\n\t\tself.netConv2 = Basic(32, 64)\n\t\tself.netConv3 = Basic(64, 128)\n\t\tself.netConv4 = Basic(128, 256)\n\t\tself.netConv5 = Basic(256, 512)\n\n\t\tself.netDeconv5 = Basic(512, 512)\n\t\tself.netDeconv4 = Basic(512, 256)\n\t\tself.netDeconv3 = Basic(256, 128)\n\t\tself.netDeconv2 = Basic(128, 64)\n\n\t\tself.netUpsample5 = Upsample(512, 512)\n\t\tself.netUpsample4 = Upsample(256, 256)\n\t\tself.netUpsample3 = Upsample(128, 128)\n\t\tself.netUpsample2 = Upsample(64, 64)\n\n\t\tself.netVertical1 = Subnet()\n\t\tself.netVertical2 = Subnet()\n\t\tself.netHorizontal1 = Subnet()\n\t\tself.netHorizontal2 = Subnet()\n\n\t\tself.load_state_dict({ strKey.replace('module', 'net'): tenWeight for strKey, tenWeight in torch.load(__file__.replace('run.py', 'network-' + arguments_strModel + '.pytorch')).items() })\n\t# end\n\n\tdef forward(self, tenFirst, tenSecond):\n\t\ttenConv1 = self.netConv1(torch.cat([ tenFirst, tenSecond ], 1))\n\t\ttenConv2 = self.netConv2(torch.nn.functional.avg_pool2d(input=tenConv1, kernel_size=2, stride=2, count_include_pad=False))\n\t\ttenConv3 = self.netConv3(torch.nn.functional.avg_pool2d(input=tenConv2, kernel_size=2, stride=2, count_include_pad=False))\n\t\ttenConv4 = self.netConv4(torch.nn.functional.avg_pool2d(input=tenConv3, kernel_size=2, stride=2, count_include_pad=False))\n\t\ttenConv5 = self.netConv5(torch.nn.functional.avg_pool2d(input=tenConv4, kernel_size=2, stride=2, count_include_pad=False))\n\n\t\ttenDeconv5 = self.netUpsample5(self.netDeconv5(torch.nn.functional.avg_pool2d(input=tenConv5, kernel_size=2, stride=2, count_include_pad=False)))\n\t\ttenDeconv4 = self.netUpsample4(self.netDeconv4(tenDeconv5 + tenConv5))\n\t\ttenDeconv3 = self.netUpsample3(self.netDeconv3(tenDeconv4 + tenConv4))\n\t\ttenDeconv2 = self.netUpsample2(self.netDeconv2(tenDeconv3 + tenConv3))\n\n\t\ttenCombine = tenDeconv2 + tenConv2\n\n\t\ttenFirst = torch.nn.functional.pad(input=tenFirst, pad=[ int(math.floor(51 / 2.0)), int(math.floor(51 / 2.0)), int(math.floor(51 / 2.0)), int(math.floor(51 / 2.0)) ], mode='replicate')\n\t\ttenSecond = torch.nn.functional.pad(input=tenSecond, pad=[ int(math.floor(51 / 2.0)), int(math.floor(51 / 2.0)), int(math.floor(51 / 2.0)), int(math.floor(51 / 2.0)) ], mode='replicate')\n\n\t\ttenDot1 = sepconv.FunctionSepconv(tenInput=tenFirst, tenVertical=self.netVertical1(tenCombine), tenHorizontal=self.netHorizontal1(tenCombine))\n\t\ttenDot2 = sepconv.FunctionSepconv(tenInput=tenSecond, tenVertical=self.netVertical2(tenCombine), tenHorizontal=self.netHorizontal2(tenCombine))\n\n\t\treturn tenDot1 + tenDot2\n\t# end\n# end\n\nnetNetwork = None\n\n##########################################################\n\ndef estimate(tenFirst, tenSecond):\n\tglobal netNetwork\n\n\tif netNetwork is None:\n\t\tnetNetwork = Network().cuda().eval()\n\t# end\n\n\tassert(tenFirst.shape[1] == tenSecond.shape[1])\n\tassert(tenFirst.shape[2] == tenSecond.shape[2])\n\n\tintWidth = tenFirst.shape[2]\n\tintHeight = tenFirst.shape[1]\n\n\tassert(intWidth <= 1280) # while our approach works with larger images, we do not recommend it unless you are aware of the implications\n\tassert(intHeight <= 720) # while our approach works with larger images, we do not recommend it unless you are aware of the implications\n\n\ttenPreprocessedFirst = tenFirst.cuda().view(1, 3, intHeight, intWidth)\n\ttenPreprocessedSecond = tenSecond.cuda().view(1, 3, intHeight, intWidth)\n\n\tif arguments_strPadding == 'paper':\n\t\tintPaddingLeft, intPaddingTop, intPaddingBottom, intPaddingRight = int(math.floor(51 / 2.0)), int(math.floor(51 / 2.0)), int(math.floor(51 / 2.0)) ,int(math.floor(51 / 2.0))\n\n\telif arguments_strPadding == 'improved':\n\t\tintPaddingLeft, intPaddingTop, intPaddingBottom, intPaddingRight = 0, 0, 0, 0\n\n\t# end\n\n\tintPreprocessedWidth = intPaddingLeft + intWidth + intPaddingRight\n\tintPreprocessedHeight = intPaddingTop + intHeight + intPaddingBottom\n\n\tif intPreprocessedWidth != ((intPreprocessedWidth >> 7) << 7):\n\t\tintPreprocessedWidth = (((intPreprocessedWidth >> 7) + 1) << 7) # more than necessary\n\t# end\n\t\n\tif intPreprocessedHeight != ((intPreprocessedHeight >> 7) << 7):\n\t\tintPreprocessedHeight = (((intPreprocessedHeight >> 7) + 1) << 7) # more than necessary\n\t# end\n\n\tintPaddingRight = intPreprocessedWidth - intWidth - intPaddingLeft\n\tintPaddingBottom = intPreprocessedHeight - intHeight - intPaddingTop\n\n\ttenPreprocessedFirst = torch.nn.functional.pad(input=tenPreprocessedFirst, pad=[ intPaddingLeft, intPaddingRight, intPaddingTop, intPaddingBottom ], mode='replicate')\n\ttenPreprocessedSecond = torch.nn.functional.pad(input=tenPreprocessedSecond, pad=[ intPaddingLeft, intPaddingRight, intPaddingTop, intPaddingBottom ], mode='replicate')\n\n\treturn torch.nn.functional.pad(input=netNetwork(tenPreprocessedFirst, tenPreprocessedSecond), pad=[ 0 - intPaddingLeft, 0 - intPaddingRight, 0 - intPaddingTop, 0 - intPaddingBottom ], mode='replicate')[0, :, :, :].cpu()\n# end\n\n##########################################################\n\nif __name__ == '__main__':\n\tif arguments_strOut.split('.')[-1] in [ 'bmp', 'jpg', 'jpeg', 'png' ]:\n\t\ttenFirst = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(arguments_strFirst))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n\t\ttenSecond = torch.FloatTensor(numpy.ascontiguousarray(numpy.array(PIL.Image.open(arguments_strSecond))[:, :, ::-1].transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n\n\t\ttenOutput = estimate(tenFirst, tenSecond)\n\n\t\tPIL.Image.fromarray((tenOutput.clamp(0.0, 1.0).numpy().transpose(1, 2, 0)[:, :, ::-1] * 255.0).astype(numpy.uint8)).save(arguments_strOut)\n\n\telif arguments_strOut.split('.')[-1] in [ 'avi', 'mp4', 'webm', 'wmv' ]:\n\t\timport moviepy\n\t\timport moviepy.editor\n\n\t\tstrTempdir = tempfile.gettempdir() + '/' + str.join('', [ random.choice('abcdefghijklmnopqrstuvwxyz0123456789') for intCount in range(20) ]); os.makedirs(strTempdir + '/')\n\n\t\tintFrames = 0\n\t\ttenFrames = [ None, None, None, None, None ]\n\n\t\tfor intFrame, npyFrame in enumerate(npyFrame[:, :, ::-1] for npyFrame in moviepy.editor.VideoFileClip(filename=arguments_strVideo).iter_frames()):\n\t\t\ttenFrames[4] = torch.FloatTensor(numpy.ascontiguousarray(npyFrame.transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n\n\t\t\tif tenFrames[0] is not None:\n\t\t\t\ttenFrames[2] = estimate(tenFrames[0], tenFrames[4])\n\t\t\t\ttenFrames[1] = estimate(tenFrames[0], tenFrames[2])\n\t\t\t\ttenFrames[3] = estimate(tenFrames[2], tenFrames[4])\n\n\t\t\t\tPIL.Image.fromarray((tenFrames[0].clamp(0.0, 1.0).numpy().transpose(1, 2, 0)[:, :, ::-1] * 255.0).astype(numpy.uint8)).save(strTempdir + '/' + str(intFrames).zfill(5) + '.png'); intFrames += 1\n\t\t\t\tPIL.Image.fromarray((tenFrames[1].clamp(0.0, 1.0).numpy().transpose(1, 2, 0)[:, :, ::-1] * 255.0).astype(numpy.uint8)).save(strTempdir + '/' + str(intFrames).zfill(5) + '.png'); intFrames += 1\n\t\t\t\tPIL.Image.fromarray((tenFrames[2].clamp(0.0, 1.0).numpy().transpose(1, 2, 0)[:, :, ::-1] * 255.0).astype(numpy.uint8)).save(strTempdir + '/' + str(intFrames).zfill(5) + '.png'); intFrames += 1\n\t\t\t\tPIL.Image.fromarray((tenFrames[3].clamp(0.0, 1.0).numpy().transpose(1, 2, 0)[:, :, ::-1] * 255.0).astype(numpy.uint8)).save(strTempdir + '/' + str(intFrames).zfill(5) + '.png'); intFrames += 1\n\t\t\t# end\n\n\t\t\ttenFrames[0] = torch.FloatTensor(numpy.ascontiguousarray(npyFrame.transpose(2, 0, 1).astype(numpy.float32) * (1.0 / 255.0)))\n\t\t# end\n\n\t\tmoviepy.editor.ImageSequenceClip(sequence=strTempdir + '/', fps=25).write_videofile(arguments_strOut)\n\n\t\tshutil.rmtree(strTempdir + '/')\n\n\t# end\n# end"""
sepconv/sepconv.py,2,"b'#!/usr/bin/env python\n\nimport torch\n\nimport cupy\nimport re\n\nkernel_Sepconv_updateOutput = \'\'\'\n\textern ""C"" __global__ void kernel_Sepconv_updateOutput(\n\t\tconst int n,\n\t\tconst float* input,\n\t\tconst float* vertical,\n\t\tconst float* horizontal,\n\t\tfloat* output\n\t) { for (int intIndex = (blockIdx.x * blockDim.x) + threadIdx.x; intIndex < n; intIndex += blockDim.x * gridDim.x) {\n\t\tfloat fltOutput = 0.0;\n\n\t\tconst int intN = ( intIndex / SIZE_3(output) / SIZE_2(output) / SIZE_1(output) ) % SIZE_0(output);\n\t\tconst int intC = ( intIndex / SIZE_3(output) / SIZE_2(output)                  ) % SIZE_1(output);\n\t\tconst int intY = ( intIndex / SIZE_3(output)                                   ) % SIZE_2(output);\n\t\tconst int intX = ( intIndex                                                    ) % SIZE_3(output);\n\n\t\tfor (int intFilterY = 0; intFilterY < SIZE_1(vertical); intFilterY += 1) {\n\t\t\tfor (int intFilterX = 0; intFilterX < SIZE_1(horizontal); intFilterX += 1) {\n\t\t\t\tfltOutput += VALUE_4(input, intN, intC, intY + intFilterY, intX + intFilterX) * VALUE_4(vertical, intN, intFilterY, intY, intX) * VALUE_4(horizontal, intN, intFilterX, intY, intX);\n\t\t\t}\n\t\t}\n\n\t\toutput[intIndex] = fltOutput;\n\t} }\n\'\'\'\n\ndef cupy_kernel(strFunction, objVariables):\n\tstrKernel = globals()[strFunction]\n\n\twhile True:\n\t\tobjMatch = re.search(\'(SIZE_)([0-4])(\\()([^\\)]*)(\\))\', strKernel)\n\n\t\tif objMatch is None:\n\t\t\tbreak\n\t\t# end\n\n\t\tintArg = int(objMatch.group(2))\n\n\t\tstrTensor = objMatch.group(4)\n\t\tintSizes = objVariables[strTensor].size()\n\n\t\tstrKernel = strKernel.replace(objMatch.group(), str(intSizes[intArg]))\n\t# end\n\n\twhile True:\n\t\tobjMatch = re.search(\'(VALUE_)([0-4])(\\()([^\\)]+)(\\))\', strKernel)\n\n\t\tif objMatch is None:\n\t\t\tbreak\n\t\t# end\n\n\t\tintArgs = int(objMatch.group(2))\n\t\tstrArgs = objMatch.group(4).split(\',\')\n\n\t\tstrTensor = strArgs[0]\n\t\tintStrides = objVariables[strTensor].stride()\n\t\tstrIndex = [ \'((\' + strArgs[intArg + 1].replace(\'{\', \'(\').replace(\'}\', \')\').strip() + \')*\' + str(intStrides[intArg]) + \')\' for intArg in range(intArgs) ]\n\n\t\tstrKernel = strKernel.replace(objMatch.group(0), strTensor + \'[\' + str.join(\'+\', strIndex) + \']\')\n\t# end\n\n\treturn strKernel\n# end\n\n@cupy.util.memoize(for_each_device=True)\ndef cupy_launch(strFunction, strKernel):\n\treturn cupy.cuda.compile_with_cache(strKernel).get_function(strFunction)\n# end\n\nclass _FunctionSepconv(torch.autograd.Function):\n\t@staticmethod\n\tdef forward(self, input, vertical, horizontal):\n\t\tself.save_for_backward(input, vertical, horizontal)\n\n\t\tintSample = input.shape[0]\n\t\tintInputDepth = input.shape[1]\n\t\tintInputHeight = input.shape[2]\n\t\tintInputWidth = input.shape[3]\n\t\tintFilterSize = min(vertical.shape[1], horizontal.shape[1])\n\t\tintOutputHeight = min(vertical.shape[2], horizontal.shape[2])\n\t\tintOutputWidth = min(vertical.shape[3], horizontal.shape[3])\n\n\t\tassert(intInputHeight - intFilterSize == intOutputHeight - 1)\n\t\tassert(intInputWidth - intFilterSize == intOutputWidth - 1)\n\n\t\tassert(input.is_contiguous() == True)\n\t\tassert(vertical.is_contiguous() == True)\n\t\tassert(horizontal.is_contiguous() == True)\n\n\t\toutput = input.new_zeros([ intSample, intInputDepth, intOutputHeight, intOutputWidth ])\n\n\t\tif input.is_cuda == True:\n\t\t\tn = output.nelement()\n\t\t\tcupy_launch(\'kernel_Sepconv_updateOutput\', cupy_kernel(\'kernel_Sepconv_updateOutput\', {\n\t\t\t\t\'input\': input,\n\t\t\t\t\'vertical\': vertical,\n\t\t\t\t\'horizontal\': horizontal,\n\t\t\t\t\'output\': output\n\t\t\t}))(\n\t\t\t\tgrid=tuple([ int((n + 512 - 1) / 512), 1, 1 ]),\n\t\t\t\tblock=tuple([ 512, 1, 1 ]),\n\t\t\t\targs=[ n, input.data_ptr(), vertical.data_ptr(), horizontal.data_ptr(), output.data_ptr() ]\n\t\t\t)\n\n\t\telif first.is_cuda == False:\n\t\t\traise NotImplementedError()\n\n\t\t# end\n\n\t\treturn output\n\t# end\n\n\t@staticmethod\n\tdef backward(self, gradOutput):\n\t\tinput, vertical, horizontal = self.saved_tensors\n\n\t\tintSample = input.shape[0]\n\t\tintInputDepth = input.shape[1]\n\t\tintInputHeight = input.shape[2]\n\t\tintInputWidth = input.shape[3]\n\t\tintFilterSize = min(vertical.shape[1], horizontal.shape[1])\n\t\tintOutputHeight = min(vertical.shape[2], horizontal.shape[2])\n\t\tintOutputWidth = min(vertical.shape[3], horizontal.shape[3])\n\n\t\tassert(intInputHeight - intFilterSize == intOutputHeight - 1)\n\t\tassert(intInputWidth - intFilterSize == intOutputWidth - 1)\n\n\t\tassert(gradOutput.is_contiguous() == True)\n\n\t\tgradInput = input.new_zeros([ intSample, intInputDepth, intInputHeight, intInputWidth ]) if self.needs_input_grad[0] == True else None\n\t\tgradVertical = input.new_zeros([ intSample, intFilterSize, intOutputHeight, intOutputWidth ]) if self.needs_input_grad[1] == True else None\n\t\tgradHorizontal = input.new_zeros([ intSample, intFilterSize, intOutputHeight, intOutputWidth ]) if self.needs_input_grad[2] == True else None\n\n\t\tif input.is_cuda == True:\n\t\t\traise NotImplementedError()\n\n\t\telif input.is_cuda == False:\n\t\t\traise NotImplementedError()\n\n\t\t# end\n\n\t\treturn gradInput, gradVertical, gradHorizontal\n\t# end\n# end\n\ndef FunctionSepconv(tenInput, tenVertical, tenHorizontal):\n\treturn _FunctionSepconv.apply(tenInput, tenVertical, tenHorizontal)\n# end\n\nclass ModuleSepconv(torch.nn.Module):\n\tdef __init__(self):\n\t\tsuper(ModuleSepconv, self).__init__()\n\t# end\n\n\tdef forward(self, tenInput, tenVertical, tenHorizontal):\n\t\treturn _FunctionSepconv.apply(tenInput, tenVertical, tenHorizontal)\n\t# end\n# end'"
