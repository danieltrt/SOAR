file_path,api_count,code
drawrect.py,8,"b'from model.DFL import DFL_VGG16\r\nfrom utils.util import *\r\nfrom utils.transform import *\r\nfrom train import *\r\nfrom validate import *\r\nfrom utils.init import *\r\nimport sys\r\nimport argparse\r\nimport os\r\nimport random\r\nimport shutil\r\nimport time\r\nimport warnings\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.parallel\r\nimport torch.backends.cudnn as cudnn\r\nimport torch.distributed as dist\r\nimport torch.optim\r\nimport torch.utils.data\r\nimport torch.utils.data.distributed\r\nimport torchvision.transforms as transforms\r\nimport torchvision.datasets as datasets\r\nfrom utils.MyImageFolderWithPaths import ImageFolderWithPaths\r\nfrom PIL import Image, ImageFont, ImageDraw\r\nimport os\r\nimport re\r\nimport numpy as np\r\nimport cv2\r\ndef scale_width(img, target_width):\r\n    ow, oh = img.size\r\n    w = target_width\r\n    target_height = int(target_width * oh / ow)\r\n    h = target_height\r\n    return img.resize((w, h), Image.BICUBIC)\r\n    \r\n    \r\ndef transform_onlysize():\r\n    transform_list = []\r\n    transform_list.append(transforms.Resize(448))\r\n    transform_list.append(transforms.CenterCrop((448, 448)))\r\n    transform_list.append(transforms.Pad((42, 42)))\r\n    return transforms.Compose(transform_list)\r\n\r\ndef find_classes(dir):\r\n    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\r\n    classes.sort()\r\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\r\n    return classes, class_to_idx\r\n\r\ndef read_specific_line(line, path):\r\n    target = int(line)\r\n    with open(path, \'r\') as f:\r\n        line = f.readline()\r\n        c = []\r\n        while line:\r\n            currentline = line\r\n            c.append(currentline)\r\n            line = f.readline()\r\n        \r\n    reg =  c[target-1].split(\',\')[-1]     \r\n    return reg\r\n\r\ndef path_to_contents(path):\r\n    filename = path.split(\'/\')[-1]\r\n    index_gtline = re.split(\'_|.jpg\', filename)[-2]\r\n    index_image = filename.split(\'_\')[1]\r\n    gt_dir = \'/data1/data_sdj/ICDAR2015/end2end/train/gt\'\r\n    gt_file = os.path.join(gt_dir, \'gt_img_\'+str(index_image)+\'.txt\')\r\n    # I want to read gt_file of specific line index_gtline\r\n    contents = read_specific_line(int(index_gtline), gt_file)\r\n    #print(index_image, index_gtline, contents)\r\n    return contents\r\n\r\ndef create_font(fontfile, contents):\r\n    # text and font\r\n    unicode_text = contents\r\n    if isinstance(unicode_text,str) and unicode_text.find(\'###\') != -1 or unicode_text == \'\':\r\n        print(\'######################\')\r\n        return None\r\n    try:\r\n        font = ImageFont.truetype(fontfile, 36, encoding = \'unic\')\r\n    \r\n        # get line size\r\n        # text_width, text_font.getsize(unicode_text)\r\n    \r\n        canvas = Image.new(\'RGB\', (128, 48), ""white"")\r\n    \r\n        draw = ImageDraw.Draw(canvas)\r\n        draw.text((5,5), unicode_text, \'black\', font)\r\n\r\n    #canvas.save(\'unicode-text.png\',\'PNG\')\r\n    #canvas.show()\r\n        print(canvas.size)\r\n        return canvas\r\n    except:\r\n        return None\r\n\r\ndef concat_images(imga, imgb):\r\n    """"""\r\n    Combines two color image ndarrays side-by-side.\r\n    """"""\r\n    #imga = Image.fromarray(imga)\r\n    #imgb = Image.fromarray(imgb)\r\n    w1,h1 = imga.size\r\n    w2,h2 = imgb.size\r\n    img = Image.new(""RGB"",(256, 48))\r\n    img.paste(imga, (0,0))\r\n    img.paste(imgb, (128, 0))\r\n    return img\r\n\r\ndef get_transform():\r\n    transform_list = []\r\n    \r\n    transform_list.append(transforms.Lambda(lambda img:scale_keep_ar_min_fixed(img, 448)))\r\n    \r\n    #transform_list.append(transforms.RandomHorizontalFlip(p=0.3))\r\n    \r\n    transform_list.append(transforms.CenterCrop((448, 448)))\r\n    \r\n    transform_list.append(transforms.ToTensor())\r\n    \r\n    transform_list.append(transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5)))\r\n    \r\n    return transforms.Compose(transform_list)\r\n\r\n\r\ndef draw_patch(epoch, model, index2classlist, args):\r\n    """"""Implement: use model to predict images and draw ten boxes by POOL6\r\n    path to images need to predict is in \'./dataset/bird\'\r\n\r\n    result : directory to accept images with ten boxes\r\n    subdirectory is epoch, e,g.0,1,2...\r\n\r\n    index2classlist : transform predict label to specific classname\r\n    """"""\r\n    result = os.path.abspath(args.result)\r\n    if not os.path.isdir(result):\r\n        os.mkdir(result)\r\n\r\n    path_img = os.path.join(os.path.abspath(\'./\'), \'vis_img\')\r\n    num_imgs = len(os.listdir(path_img))\r\n\r\n    dirs = os.path.join(result, str(epoch))\r\n    if not os.path.exists(dirs):\r\n        os.mkdir(dirs)\r\n    \r\n    for original in range(num_imgs):\r\n        img_path = os.path.join(path_img, \'{}.jpg\'.format(original)) \r\n        \r\n        transform1 = get_transform()       # transform for predict \r\n        transform2 = transform_onlysize()  # transform for draw\r\n\r\n        img = Image.open(img_path)\r\n        img_pad = transform2(img)\r\n        img_tensor = transform1(img)\r\n        img_tensor = img_tensor.unsqueeze(0)\r\n        out1, out2, out3, indices = model(img_tensor)\r\n        out = out1 + out2 + 0.1 *out3\r\n    \r\n        value, index = torch.max(out.cpu(), 1)\r\n        vrange = np.arange(0, 10)  \r\n        # select from index - index+9 in 2000\r\n        # in test I use 1st class, so I choose indices[0, 9] \r\n        for i in vrange:\r\n            indice = indices[0, i]\r\n            row, col = indice/56, indice%56\r\n            p_tl = (8*col, 8*row)\r\n            p_br = (col*8+92, row*8+92)\r\n            draw = ImageDraw.Draw(img_pad)\r\n            draw.rectangle((p_tl, p_br), outline=\'red\')\r\n    \r\n        # search corresponding classname\r\n        idx = int(index[0])\r\n        dirname = index2classlist[idx]\r\n    \r\n        filename = \'epoch_\'+\'{:0>3}\'.format(epoch)+\'_[org]_\'+str(original)+\'_[predict]_\'+str(dirname)+\'.jpg\'\r\n        filepath = os.path.join(os.path.join(result,str(epoch)),filename)\r\n        img_pad.save(filepath, ""JPEG"") \r\n\r\n    \r\nif __name__ == \'__main__\':\r\n    draw_patch()\r\n'"
main.py,12,"b""from model.DFL import DFL_VGG16\r\nfrom utils.util import *\r\nfrom utils.transform import *\r\nfrom train import *\r\nfrom validate import *\r\nfrom utils.init import *\r\nimport sys\r\nimport argparse\r\nimport os\r\nimport random\r\nimport shutil\r\nimport time\r\nimport warnings\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.parallel\r\nimport torch.backends.cudnn as cudnn\r\nimport torch.distributed as dist\r\nimport torch.optim\r\nimport torch.utils.data\r\nimport torch.utils.data.distributed\r\nimport torchvision.transforms as transforms\r\nimport torchvision.datasets as datasets\r\nfrom utils.MyImageFolderWithPaths import *\r\nfrom drawrect import *\r\n\r\nparser = argparse.ArgumentParser(description='PyTorch ImageNet Training')\r\nparser.add_argument('--dataroot', metavar='DIR',\r\n                    help='path to dataset')\r\nparser.add_argument('--result', metavar='DIR',\r\n                    help='path to dataset')\r\nparser.add_argument('-j', '--workers', default=16, type=int, metavar='N',\r\n                    help='number of data loading workers (default: 4)')\r\nparser.add_argument('--epochs', default=100000, type=int, metavar='N',\r\n                    help='number of total epochs to run')\r\nparser.add_argument('--start-epoch', default=0, type=int, metavar='N',\r\n                    help='manual epoch number (useful on restarts)')\r\nparser.add_argument('--train_batchsize_per_gpu', default=14, type=int,\r\n                    metavar='N', help='mini-batch size (default: 64)')\r\nparser.add_argument('-testbatch', '--test_batch_size', default=64, type=int,\r\n                    metavar='N', help='mini-batch size (default: 32)')\r\nparser.add_argument('--init_type',  default='xavier', type=str,\r\n                    metavar='INIT',help='init net')\r\nparser.add_argument('--lr', '--learning-rate', default=0.1, type=float,\r\n                    metavar='LR', help='initial learning rate')\r\nparser.add_argument('--momentum', default=0.9, type=float,\r\n                    metavar='momentum', help='momentum')\r\nparser.add_argument('--weight_decay', '--wd', default=0.000005, type=float,\r\n                    metavar='W', help='weight decay (default: 1e-4)')\r\nparser.add_argument('--print-freq', '-p', default=1, type=int,\r\n                    metavar='N', help='print frequency (default: 10)')\r\nparser.add_argument('--resume', default='', type=str, metavar='PATH',\r\n                    help='path to latest checkpoint (default: none)')\r\nparser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true',\r\n                    help='evaluate model on validation set')\r\nparser.add_argument('--pretrained', dest='pretrained', action='store_true',\r\n                    help='use pre-trained model')\r\nparser.add_argument('--seed', default=None, type=int,\r\n                    help='seed for initializing training. ')\r\nparser.add_argument('--gpu', default=4, type=int,\r\n                    help='GPU nums to use.')\r\nparser.add_argument('--log_train_dir', default='log_train', type=str,\r\n                    help='log for train')\r\nparser.add_argument('--log_test_dir', default='log_test', type=str,\r\n                    help='log for test')\r\nparser.add_argument('--nclass', default=583, type=int,\r\n                    help='num of classes')\r\nparser.add_argument('--eval_epoch', default=2, type=int,\r\n                    help='every eval_epoch we will evaluate')\r\nparser.add_argument('--vis_epoch', default=2, type=int,\r\n                    help='every vis_epoch we will evaluate')\r\nparser.add_argument('--save_epoch', default=2, type=int,\r\n                    help='every save_epoch we will evaluate')\r\nparser.add_argument('--w', default=448, type=int,\r\n                    help='transform, seen as align')\r\nparser.add_argument('--h', default=448, type=int,\r\n                    help='transform, seen as align')\r\n\r\n\r\nbest_prec1 = 0\r\n\r\ndef main():\r\n    print('DFL-CNN <==> Part1 : prepare for parameters <==> Begin')\r\n    global args, best_prec1\r\n    args = parser.parse_args()\r\n    print('DFL-CNN <==> Part1 : prepare for parameters <==> Done')\r\n\r\n\r\n    \r\n    print('DFL-CNN <==> Part2 : Load Network  <==> Begin')\r\n    model = DFL_VGG16(k = 10, nclass = 200)     \r\n    if args.gpu is not None:\r\n        model = nn.DataParallel(model, device_ids=range(args.gpu))\r\n        model = model.cuda()\r\n        cudnn.benchmark = True\r\n    if args.init_type is not None:\r\n        try: \r\n            init_weights(model, init_type=args.init_type)\r\n        except:\r\n            sys.exit('DFL-CNN <==> Part2 : Load Network  <==> Init_weights error!')\r\n\r\n    criterion = nn.CrossEntropyLoss()\r\n    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay = args.weight_decay)\r\n\r\n    # Optionally resume from a checkpoint\r\n    if args.resume:\r\n        if os.path.isfile(args.resume):\r\n            checkpoint = torch.load(args.resume)\r\n            args.start_epoch = checkpoint['epoch']\r\n            best_prec1 = checkpoint['best_prec1']\r\n            model.load_state_dict(checkpoint['state_dict'])\r\n            optimizer.load_state_dict(checkpoint['optimizer'])\r\n            print('DFL-CNN <==> Part2 : Load Network  <==> Continue from {} epoch {}'.format(args.resume, checkpoint['epoch']))\r\n        else:\r\n            print('DFL-CNN <==> Part2 : Load Network  <==> Failed')\r\n    print('DFL-CNN <==> Part2 : Load Network  <==> Done')\r\n\r\n    \r\n\r\n    print('DFL-CNN <==> Part3 : Load Dataset  <==> Begin')\r\n    dataroot = os.path.abspath(args.dataroot)\r\n    traindir = os.path.join(dataroot, 'train')\r\n    testdir = os.path.join(dataroot, 'test')\r\n\r\n    # ImageFolder to process img\r\n    transform_train = get_transform_for_train()\r\n    transform_test  = get_transform_for_test()\r\n    transform_test_simple = get_transform_for_test_simple()\r\n\r\n    train_dataset = ImageFolderWithPaths(traindir, transform = transform_train)\r\n    test_dataset  = ImageFolderWithPaths(testdir,  transform = transform_test)\r\n    test_dataset_simple = ImageFolderWithPaths(testdir, transform = transform_test_simple)\r\n\r\n    # A list for target to classname\r\n    index2classlist = train_dataset.index2classlist()\r\n\r\n    # data loader   \r\n    train_loader = torch.utils.data.DataLoader(\r\n        train_dataset, batch_size=args.gpu * args.train_batchsize_per_gpu, shuffle=True,\r\n        num_workers=args.workers, pin_memory=True, drop_last = False)\r\n    test_loader = torch.utils.data.DataLoader(\r\n        test_dataset, batch_size=1, shuffle=True,\r\n        num_workers=args.workers, pin_memory=True, drop_last = False)\r\n    test_loader_simple = torch.utils.data.DataLoader(\r\n        test_dataset_simple, batch_size=1, shuffle=True,\r\n        num_workers=args.workers, pin_memory=True, drop_last = False)\r\n    print('DFL-CNN <==> Part3 : Load Dataset  <==> Done')\r\n   \r\n\r\n    print('DFL-CNN <==> Part4 : Train and Test  <==> Begin')\r\n    for epoch in range(args.start_epoch, args.epochs):\r\n        adjust_learning_rate(args, optimizer, epoch, gamma = 0.1)\r\n        \r\n        # train for one epoch\r\n        train(args, train_loader, model, criterion, optimizer, epoch)\r\n\r\n        # evaluate on validation set\r\n        if epoch % args.eval_epoch == 0:\r\n            prec1 = validate_simple(args, test_loader_simple, model, criterion, epoch)\r\n            \r\n            # remember best prec@1 and save checkpoint\r\n            is_best = prec1 > best_prec1\r\n            best_prec1 = max(prec1, best_prec1)\r\n            save_checkpoint({\r\n                'epoch': epoch + 1,\r\n                'state_dict': model.state_dict(),\r\n                'best_prec1': best_prec1,\r\n                'optimizer' : optimizer.state_dict(),\r\n                'prec1'     : prec1,\r\n            }, is_best) \r\n\r\n        # do a test for visualization    \r\n        if epoch % args.vis_epoch  == 0 and epoch != 0: \r\n            draw_patch(epoch, model, index2classlist, args)\r\n        \r\n\r\n\r\n\r\nif __name__ == '__main__':\r\n     main() \r\n"""
train.py,2,"b""import torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nfrom PIL import Image\r\nimport os\r\nimport sys\r\nimport time\r\nfrom utils.util import *\r\nfrom utils.save import *\r\nfrom torchvision import datasets, transforms, utils\r\nimport torchvision.models as models\r\nimport numpy as np\r\n\r\ndef train(args, train_loader, model, criterion, optimizer, epoch):\r\n    batch_time = AverageMeter()\r\n    data_time = AverageMeter()\r\n    losses = AverageMeter()\r\n    top1 = AverageMeter()\r\n    top5 = AverageMeter()\r\n    log = Log()\r\n    \r\n    losses1 = AverageMeter()\r\n    losses2 = AverageMeter()\r\n    losses3 = AverageMeter()\r\n    # switch to train mode\r\n    model.train()\r\n\r\n    for i, (data, target, paths) in enumerate(train_loader):\r\n        if args.gpu is not None:\r\n            data = data.cuda()\r\n            target = target.cuda()\r\n\r\n        out1, out2, out3, _ = model(data)\r\n        out = out1 + out2 + 0.1 * out3\r\n\r\n        loss1 = criterion(out1, target)\r\n        loss2 = criterion(out2, target)\r\n        loss3 = criterion(out3, target)\r\n        \r\n        loss = loss1 + loss2 + 0.1 * loss3\r\n        \r\n        # measure accuracy and record loss\r\n        prec1, prec5 = accuracy(out, target, topk=(1, 5))  # this is metric on trainset\r\n        batchsize = data.size(0)\r\n        losses.update(loss.item()  , batchsize)\r\n\r\n        if np.isnan(losses.val):\r\n            sys.exit('Loss diverged')\r\n\r\n        losses1.update(loss1.item(), batchsize)\r\n        losses2.update(loss2.item(), batchsize)\r\n        losses3.update(loss3.item(), batchsize)\r\n        top1.update(prec1[0], batchsize)\r\n        top5.update(prec5[0], batchsize)\r\n        \r\n        # compute gradient and do SGD step\r\n        optimizer.zero_grad()\r\n        loss.backward()\r\n        optimizer.step()\r\n        \r\n        if i % args.print_freq == 0:\r\n            print('DFL-CNN <==> Train Epoch: [{0}][{1}/{2}]\\n'\r\n                'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\r\n                'Loss1 {loss1.val:.4f} ({loss1.avg:.4f})\\t'\r\n                'Loss2 {loss2.val:.4f} ({loss2.avg:.4f})\\t'\r\n                'Loss3 {loss3.val:.4f} ({loss3.avg:.4f})\\n'\r\n                'Top1 {top1.val:.3f} ({top1.avg:.3f})\\t'\r\n                'Top5 {top5.val:.3f} ({top5.avg:.3f})'.format(\r\n                epoch, i, len(train_loader), loss=losses, loss1=losses1, loss2=losses2, loss3=losses3, top1=top1, top5=top5))\r\n            \r\n            totalloss = [losses, losses1, losses2, losses3]\r\n            log.save_train_info(epoch, i, len(train_loader), totalloss, top1, top5)\r\n\r\n\r\n\r\n"""
validate.py,0,"b""import torch\r\nimport time\r\nimport sys\r\nfrom utils.util import *\r\nfrom utils.save import *\r\n\r\ndef validate(args, val_loader, model, criterion, epoch):\r\n    batch_time = AverageMeter()\r\n    losses = AverageMeter()\r\n    top1 = AverageMeter()\r\n    top5 = AverageMeter()\r\n    log = Log()\r\n    model.eval()   \r\n    end = time.time()\r\n\r\n    # we may have ten d in data\r\n    for i, (data, target, paths) in enumerate(val_loader):\r\n        if args.gpu is not None:\r\n            data = data.cuda()\r\n            target = target.cuda()\r\n\r\n        # compute output\r\n        for idx, d in enumerate(data[0]):      # data [batchsize, 10_crop, 3, 448, 448]\r\n            d = d.unsqueeze(0) # d [1, 3, 448, 448]\r\n            output1, output2, output3, _ = model(d)\r\n            output = output1 + output2 + 0.1 * output3\r\n\r\n            # measure accuracy and record loss\r\n            prec1, prec5 = accuracy(output, target, topk=(1, 5))\r\n\r\n            top1.update(prec1[0], 1)\r\n            top5.update(prec5[0], 1)\r\n            print('DFL-CNN <==> Test <==> Img:{} No:{} Top1 {:.3f} Top5 {:.3f}'.format(i, idx, prec1.cpu().numpy()[0], prec5.cpu().numpy()[0]))\r\n\r\n    print('DFL-CNN <==> Test Total <==> Top1 {:.3f}% Top5 {:.3f}%'.format(top1.avg, top5.avg))\r\n    log.save_test_info(epoch, top1, top5)\r\n    return top1.avg\r\n\r\n\r\ndef validate_simple(args, val_loader, model, criterion, epoch):\r\n    batch_time = AverageMeter()\r\n    losses = AverageMeter()\r\n    top1 = AverageMeter()\r\n    top5 = AverageMeter()\r\n    log = Log()\r\n    model.eval()   \r\n    end = time.time()\r\n\r\n    # we may have ten d in data\r\n    for i, (data, target, paths) in enumerate(val_loader):\r\n        if args.gpu is not None:\r\n            data = data.cuda()\r\n            target = target.cuda()\r\n\r\n        # compute output\r\n        for idx, d in enumerate(data):      # data [batchsize, 10_crop, 3, 448, 448]\r\n            d = d.unsqueeze(0) # d [1, 3, 448, 448]\r\n            output1, output2, output3, _ = model(d)\r\n            output = output1 + output2 + 0.1 * output3\r\n\r\n            # measure accuracy and record loss\r\n            prec1, prec5 = accuracy(output, target, topk=(1, 5))\r\n\r\n            top1.update(prec1[0], 1)\r\n            top5.update(prec5[0], 1)\r\n            print('DFL-CNN <==> Test <==> Img:{} Top1 {:.3f} Top5 {:.3f}'.format(i, prec1.cpu().numpy()[0], prec5.cpu().numpy()[0]))\r\n\r\n    print('DFL-CNN <==> Test Total <==> Top1 {:.3f}% Top5 {:.3f}%'.format(top1.avg, top5.avg))\r\n    log.save_test_info(epoch, top1, top5)\r\n    return top1.avg\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"""
model/DFL.py,7,"b""import torch\r\nimport torch.nn as  nn\r\nimport torch.nn.functional as F\r\nimport torchvision\r\n\r\nclass DFL_VGG16(nn.Module):\r\n\tdef __init__(self, k = 10, nclass = 200):\r\n\t\tsuper(DFL_VGG16, self).__init__()\r\n\t\tself.k = k\r\n\t\tself.nclass = nclass\r\n\t\t\r\n\t\t# k channels for one class, nclass is total classes, therefore k * nclass for conv6\r\n\t\tvgg16featuremap = torchvision.models.vgg16_bn(pretrained=True).features\r\n\t\tconv1_conv4 = torch.nn.Sequential(*list(vgg16featuremap.children())[:-11])\r\n\t\tconv5 = torch.nn.Sequential(*list(vgg16featuremap.children())[-11:])\r\n\t\tconv6 = torch.nn.Conv2d(512, k * nclass, kernel_size = 1, stride = 1, padding = 0)\r\n\t\tpool6 = torch.nn.MaxPool2d((56, 56), stride = (56, 56), return_indices = True)\r\n\r\n\t\t# Feature extraction root\r\n\t\tself.conv1_conv4 = conv1_conv4\r\n\r\n\t\t# G-Stream\r\n\t\tself.conv5 = conv5\r\n\t\tself.cls5 = nn.Sequential(\r\n\t\t\tnn.Conv2d(512, 200, kernel_size=1, stride = 1, padding = 0),\r\n\t\t\tnn.BatchNorm2d(200),\r\n\t\t\tnn.ReLU(True),\r\n\t\t\tnn.AdaptiveAvgPool2d((1,1)),\r\n\t\t\t)\r\n\r\n\t\t# P-Stream\r\n\t\tself.conv6 = conv6\r\n\t\tself.pool6 = pool6\r\n\t\tself.cls6 = nn.Sequential(\r\n\t\t\tnn.Conv2d(k * nclass, nclass, kernel_size = 1, stride = 1, padding = 0),\r\n\t\t\tnn.AdaptiveAvgPool2d((1,1)),\r\n\t\t\t)\r\n\r\n\t\t# Side-branch\r\n\t\tself.cross_channel_pool = nn.AvgPool1d(kernel_size = k, stride = k, padding = 0)\r\n\r\n\tdef forward(self, x):\r\n\t\tbatchsize = x.size(0)\r\n\r\n\t\t# Stem: Feature extraction\r\n\t\tinter4 = self.conv1_conv4(x)\r\n\r\n        # G-stream\r\n\t\tx_g = self.conv5(inter4)\r\n\t\tout1 = self.cls5(x_g)\r\n\t\tout1 = out1.view(batchsize, -1)\r\n\r\n\t\t# P-stream ,indices is for visualization\r\n\t\tx_p = self.conv6(inter4)\r\n\t\tx_p, indices = self.pool6(x_p)\r\n\t\tinter6 = x_p\r\n\t\tout2 = self.cls6(x_p)\r\n\t\tout2 = out2.view(batchsize, -1)\r\n\t\t\r\n\t\t# Side-branch\r\n\t\tinter6 = inter6.view(batchsize, -1, self.k * self.nclass)\r\n\t\tout3 = self.cross_channel_pool(inter6)\r\n\t\tout3 = out3.view(batchsize, -1)\r\n\t\r\n\t\treturn out1, out2, out3, indices\r\n\r\nif __name__ == '__main__':\r\n\tinput_test = torch.ones(10,3,448,448)\r\n\tnet = DFL_VGG16()\r\n\toutput_test = net(input_test)\r\n\tprint(output_test[0].shape)\t\r\n\tprint(output_test[1].shape)\t\r\n\tprint(output_test[2].shape)\t\r\n\r\n\r\n"""
utils/MyImageFolderWithPaths.py,0,"b'import torch\r\nimport os\r\nfrom torchvision import datasets\r\n\r\nclass ImageFolderWithPaths(datasets.ImageFolder):\r\n\tdef __getitem__(self, index):\r\n\t\t\r\n\t\toriginal_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\r\n\t\t\r\n\t\tpath = self.imgs[index][0]\r\n\t\t\r\n\t\ttuple_with_path = (original_tuple + (path,)) \r\n\t\t\r\n\t\treturn tuple_with_path\r\n\t\r\n\tdef index2classlist(self):\r\n\t\t\r\n\t\treturn self._find_classes_(self.root)\r\n\r\n\tdef _find_classes_(self, dir):\r\n\t\t""""""\r\n\t\tlist : index of list coresponding to classname\r\n\t\t""""""\r\n\t\t\r\n\t\tclasses_list = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\r\n\t\t\r\n\t\tclasses_list.sort()\r\n\t\t\r\n\t\treturn classes_list'"
utils/init.py,1,"b""from torch.nn import init\r\ndef init_net(net, init_type='normal'):\r\n    init_weights(net, init_type)\r\n    return net\r\n\r\ndef init_weights(net, init_type='normal', gain=0.02):\r\n    def init_func(m):\r\n        # this will apply to each layer\r\n        classname = m.__class__.__name__\r\n        if hasattr(m, 'weight') and (classname.find('conv')!=-1 or classname.find('Linear')!=-1):\r\n            if init_type=='normal':\r\n                init.normal_(m.weight.data, 0.0, gain)\r\n            elif init_type == 'xavier':\r\n                init.xavier_normal_(m.weight.data, gain=gain)\r\n            elif init_type == 'kaiming':\r\n                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')#good for relu\r\n            elif init_type == 'orthogonal':\r\n                init.orthogonal_(m.weight.data, gain=gain)\r\n            else:\r\n                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\r\n            \r\n            if hasattr(m, 'bias') and m.bias is not None:\r\n                init.constant_(m.bias.data, 0.0)\r\n        elif classname.find('BatchNorm2d') != -1:\r\n            init.normal_(m.weight.data, 1.0, gain)\r\n            init.constant_(m.bias.data, 0.0)\r\n    #print('initialize network with %s' % init_type)\r\n    net.apply(init_func)"""
utils/save.py,1,"b'import torch\r\nimport os\r\nimport shutil\r\nimport datetime\r\nfrom utils.util import *\r\nclass Log(object):\r\n    def save_train_info(self, epoch, batch, maxbatch, losses, top1, top5):\r\n        """"""\r\n        loss may contain several parts\r\n        """"""\r\n        loss = losses[0]\r\n        loss1 = losses[1]\r\n        loss2 = losses[2]\r\n        loss3 = losses[3]\r\n        root_dir = os.path.abspath(\'./\')\r\n        log_dir = os.path.join(root_dir, \'log\') \r\n        if not os.path.exists(log_dir):\r\n            os.mkdir(log_dir)\r\n        \r\n        log_file = os.path.join(log_dir, \'log_train.txt\')\r\n        if not os.path.exists(log_file):\r\n            os.mknod(log_file)\r\n\r\n        with open(log_file, \'a\') as f:\r\n            f.write(\'DFL-CNN <==> Train <==> Epoch: [{0}][{1}/{2}]\\n\'\r\n                    \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\r\n                    \'Loss1 {loss1.val:.4f} ({loss1.avg:.4f})\\t\'\r\n                    \'Loss2 {loss2.val:.4f} ({loss2.avg:.4f})\\t\'\r\n                    \'Loss3 {loss3.val:.4f} ({loss3.avg:.4f})\\n\'\r\n                    \'Prec@1 ({top1.avg:.3f})\\t\'\r\n                    \'Prec@5 ({top5.avg:.3f})\\n\'.format(epoch, batch, maxbatch,loss = loss,loss1 = loss1,loss2 = loss2, loss3=loss3, top1=top1, top5=top5))\r\n\r\n    \r\n    def save_test_info(self, epoch, top1, top5):\r\n        root_dir = os.path.abspath(\'./\')\r\n        log_dir = os.path.join(root_dir, \'log\') \r\n        # check log_dir \r\n        if not os.path.exists(log_dir):\r\n            os.mkdir(log_dir)\r\n        log_file = os.path.join(log_dir, \'log_test.txt\')\r\n\r\n        if not os.path.exists(log_file):\r\n            os.mknod(log_file)\r\n\r\n        with open(log_file, \'a\') as f:\r\n            f.write(\'DFL-CNN <==> Test <==> Epoch: [{:4d}] Top1:{top1.avg:.3f}% Top5:{top5.avg:.3f}%\\n\'.format(epoch, top1=top1, top5=top5))\r\n\r\n# this is for weight\r\ndef save_checkpoint(state, is_best, filename=\'checkpoint.pth.tar\'):\r\n    """"""[summary]\r\n    \r\n    [description]\r\n    \r\n    Arguments:\r\n        state {[type]} -- [description] a dict describe some params\r\n        is_best {bool} -- [description] a bool value\r\n    \r\n    Keyword Arguments:\r\n        filename {str} -- [description] (default: {\'checkpoint.pth.tar\'})\r\n    """"""\r\n    root_dir = get_root_dir()\r\n    weight_dir = os.path.join(root_dir, \'weight\')\r\n    if not os.path.exists(weight_dir):\r\n        os.mkdir(weight_dir)\r\n    \r\n    epoch = state[\'epoch\']\r\n    prec1 = state[\'prec1\']\r\n\r\n    file_path = os.path.join(weight_dir, \'epoch_{:04d}_top1_{:02d}_{}\'.format(int(epoch), int(prec1), filename))  \r\n    torch.save(state, file_path)\r\n    \r\n    best_path = os.path.join(weight_dir, \'model_best.pth.tar\')\r\n    \r\n    if is_best:\r\n        shutil.copyfile(file_path, best_path)\r\n\r\n\r\n\r\n\r\n\r\n'"
utils/transform.py,1,"b'from PIL import Image, ImageOps\r\nfrom torchvision import datasets, transforms, utils\r\nimport torch\r\n\r\ndef scale_width_keep_ar(img, target_width):\r\n\t""""""\r\n\tresize image keeping aspect ratio\r\n\t""""""\r\n\tow, oh = img.size\r\n\t\r\n\ttarget_height = int(target_width * oh // ow)\r\n\t\r\n\treturn img.resize((target_width, target_height), Image.BICUBIC)\r\n\r\ndef scale_keep_ar_min_fixed(img, fixed_min):\r\n    ow, oh = img.size\r\n\r\n    if ow < oh:\r\n        \r\n        nw = fixed_min\r\n\r\n        nh = nw * oh // ow\r\n    \r\n    else:\r\n        \r\n        nh = fixed_min \r\n\r\n        nw = nh * ow // oh\r\n    return img.resize((nw, nh), Image.BICUBIC)\r\n\r\ndef get_transform_for_train():\r\n\r\n    transform_list = []\r\n    \r\n    transform_list.append(transforms.Lambda(lambda img:scale_keep_ar_min_fixed(img, 448)))\r\n    \r\n    transform_list.append(transforms.RandomHorizontalFlip(p=0.3))\r\n    \r\n    transform_list.append(transforms.RandomCrop((448, 448)))\r\n    \r\n    transform_list.append(transforms.ToTensor())\r\n    \r\n    transform_list.append(transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5)))\r\n    \r\n    return transforms.Compose(transform_list)\r\n\r\ndef get_transform_for_test():\r\n\r\n    transform_list = []\r\n    \r\n    transform_list.append(transforms.Lambda(lambda img:scale_keep_ar_min_fixed(img, 560)))\r\n   \r\n    transform_list.append(transforms.TenCrop(448)) \r\n    \r\n    transform_list.append(transforms.Lambda(lambda crops: torch.stack([transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5))((transforms.ToTensor())(crop)) for crop in crops])) )\r\n    \r\n    #transform_list.append(transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5)))\r\n    \r\n    return transforms.Compose(transform_list)\r\n\r\ndef get_transform_for_test_simple():\r\n\r\n    transform_list = []\r\n    \r\n    transform_list.append(transforms.Lambda(lambda img:scale_keep_ar_min_fixed(img, 448)))\r\n    \r\n    transform_list.append(transforms.CenterCrop((448, 448)))\r\n    \r\n    transform_list.append(transforms.ToTensor())\r\n    \r\n    transform_list.append(transforms.Normalize(mean=(0.5,0.5,0.5),std=(0.5,0.5,0.5)))\r\n    \r\n    return transforms.Compose(transform_list)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n'"
utils/util.py,2,"b'import torch\r\nimport datetime\r\nimport os\r\n    \r\ndef get_root_dir():\r\n    return os.path.abspath(os.path.join(os.path.dirname(__file__), \'..\')) \r\n   \r\ndef get_today():\r\n    return str(datetime.date.today())\r\n# gpu\r\ndef get_device_ids(num_gpu):\r\n    assert isinstance(num_gpu, int),\'num_gpu is not int\'\r\n    device_ids = []\r\n    for i in range(num_gpu):\r\n        device_ids.append(i)\r\n    return device_ids \r\n\r\n# compute accurate\r\ndef accuracy(output, target, topk=(1, 5)):\r\n    maxk = max(topk)\r\n    batch_size = target.size(0)\r\n    _, pred = output.topk(maxk, 1, True, True) \r\n    pred = pred.t()            \r\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\r\n    res = []\r\n    for k in topk:\r\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim = True)\r\n        res.append(correct_k.mul_(100.0 / batch_size))\r\n    return res\r\n\r\n\r\n\r\n#data load\r\nclass AverageMeter(object):\r\n    """"""Computes and stores the average and current value""""""\r\n    def __init__(self):\r\n        self.reset()\r\n\r\n    def reset(self):\r\n        self.val = 0\r\n        self.avg = 0\r\n        self.sum = 0\r\n        self.count = 0\r\n\r\n    def update(self, val, n=1):\r\n        self.val = val\r\n        self.sum += val * n\r\n        self.count += n\r\n        self.avg = self.sum / self.count\r\n        \r\ndef make_weights_for_balanced_classes(images, nclasses):\r\n    count = [0] * nclasses\r\n    # num of every class \r\n    for item in images:\r\n        count[item[1]] += 1\r\n\r\n    weight_per_class = [0.] * nclasses\r\n    N = float(sum(count))\r\n    for i in range(nclasses):\r\n        weight_per_class[i] = N/float(count[i])\r\n\r\n    # weight for each image\r\n    weight = [0]*len(images)\r\n    for idx, val in enumerate(images):\r\n        weight[idx] = weight_per_class[val[1]]\r\n\r\n    return weight\r\n\r\nclass option_for_dataset_transform(object):\r\n    def __init__(self, scale_width_keep_ar =None, random_crop =None, totensor=True, normalize=True):\r\n        self.scale_width_keep_ar = scale_width_keep_ar\r\n        self.random_crop = random_crop\r\n        self.totensor = totensor\r\n        self.normalize = normalize  \r\n\r\n# network\r\ndef diagnose_network(net, name=\'network\'):\r\n    mean, count = 0.0, 0\r\n    params = list(net.parameters())\r\n    for layer, param in enumerate(net.parameters()):\r\n        if param.grad is not None and layer < 10:\r\n            print(\'layer: {:} \'.format(layer))\r\n            print(\'mean\',torch.mean(torch.abs(param.data)))\r\n            print(\'grad\',torch.mean(torch.abs(param.grad.data)))\r\n\r\n# train\r\ndef adjust_learning_rate(args, optimizer, epoch, gamma=0.1):\r\n    """"""Sets the learning rate to the initial LR decayed 0.9 every 50 epochs""""""\r\n    lr = args.lr * (0.9 ** (epoch // 10))\r\n    for param_group in optimizer.param_groups:\r\n        param_group[\'lr\'] = lr'"
