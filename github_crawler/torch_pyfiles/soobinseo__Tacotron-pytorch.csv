file_path,api_count,code
data.py,1,"b'import hyperparams as hp\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport librosa\nimport numpy as np\nfrom Tacotron.text import text_to_sequence\nimport collections\nfrom scipy import signal\n\nclass LJDatasets(Dataset):\n    """"""LJSpeech dataset.""""""\n\n    def __init__(self, csv_file, root_dir):\n        """"""\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the wavs.\n\n        """"""\n        self.landmarks_frame = pd.read_csv(csv_file, sep=\'|\', header=None)\n        self.root_dir = root_dir\n\n    def load_wav(self, filename):\n        return librosa.load(filename, sr=hp.sample_rate)\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        wav_name = os.path.join(self.root_dir, self.landmarks_frame.ix[idx, 0]) + \'.wav\'\n        text = self.landmarks_frame.ix[idx, 1]\n        text = np.asarray(text_to_sequence(text, [hp.cleaners]), dtype=np.int32)\n        wav = np.asarray(self.load_wav(wav_name)[0], dtype=np.float32)\n        sample = {\'text\': text, \'wav\': wav}\n\n        return sample\n\ndef collate_fn(batch):\n\n    # Puts each data field into a tensor with outer dimension batch size\n    if isinstance(batch[0], collections.Mapping):\n        keys = list()\n\n        text = [d[\'text\'] for d in batch]\n        wav = [d[\'wav\'] for d in batch]\n\n        # PAD sequences with largest length of the batch\n        text = _prepare_data(text).astype(np.int32)\n        wav = _prepare_data(wav)\n\n        magnitude = np.array([spectrogram(w) for w in wav])\n        mel = np.array([melspectrogram(w) for w in wav])\n        timesteps = mel.shape[-1]\n\n        # PAD with zeros that can be divided by outputs per step\n        if timesteps % hp.outputs_per_step != 0:\n            magnitude = _pad_per_step(magnitude)\n            mel = _pad_per_step(mel)\n\n        return text, magnitude, mel\n\n    raise TypeError((""batch must contain tensors, numbers, dicts or lists; found {}""\n                     .format(type(batch[0]))))\n\n# These pre-processing functions are referred from https://github.com/keithito/tacotron\n\n_mel_basis = None\n\ndef save_wav(wav, path):\n  wav *= 32767 / max(0.01, np.max(np.abs(wav)))\n  librosa.output.write_wav(path, wav.astype(np.int16), hp.sample_rate)\n\n\ndef _linear_to_mel(spectrogram):\n    global _mel_basis\n    if _mel_basis is None:\n        _mel_basis = _build_mel_basis()\n    return np.dot(_mel_basis, spectrogram)\n\ndef _build_mel_basis():\n    n_fft = (hp.num_freq - 1) * 2\n    return librosa.filters.mel(hp.sample_rate, n_fft, n_mels=hp.num_mels)\n\ndef _normalize(S):\n    return np.clip((S - hp.min_level_db) / -hp.min_level_db, 0, 1)\n\ndef _denormalize(S):\n    return (np.clip(S, 0, 1) * -hp.min_level_db) + hp.min_level_db\n\ndef _stft_parameters():\n    n_fft = (hp.num_freq - 1) * 2\n    hop_length = int(hp.frame_shift_ms / 1000 * hp.sample_rate)\n    win_length = int(hp.frame_length_ms / 1000 * hp.sample_rate)\n    return n_fft, hop_length, win_length\n\ndef _amp_to_db(x):\n    return 20 * np.log10(np.maximum(1e-5, x))\n\ndef _db_to_amp(x):\n    return np.power(10.0, x * 0.05)\n\ndef preemphasis(x):\n    return signal.lfilter([1, -hp.preemphasis], [1], x)\n\n\ndef inv_preemphasis(x):\n    return signal.lfilter([1], [1, -hp.preemphasis], x)\n\n\ndef spectrogram(y):\n    D = _stft(preemphasis(y))\n    S = _amp_to_db(np.abs(D)) - hp.ref_level_db\n    return _normalize(S)\n\n\ndef inv_spectrogram(spectrogram):\n    \'\'\'Converts spectrogram to waveform using librosa\'\'\'\n\n    S = _denormalize(spectrogram)\n    S = _db_to_amp(S + hp.ref_level_db)  # Convert back to linear\n\n    return inv_preemphasis(_griffin_lim(S ** hp.power))          # Reconstruct phase\n\ndef _griffin_lim(S):\n    \'\'\'librosa implementation of Griffin-Lim\n    Based on https://github.com/librosa/librosa/issues/434\n    \'\'\'\n    angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n    S_complex = np.abs(S).astype(np.complex)\n    y = _istft(S_complex * angles)\n    for i in range(hp.griffin_lim_iters):\n        angles = np.exp(1j * np.angle(_stft(y)))\n        y = _istft(S_complex * angles)\n    return y\n\ndef _istft(y):\n    _, hop_length, win_length = _stft_parameters()\n    return librosa.istft(y, hop_length=hop_length, win_length=win_length)\n\n\ndef melspectrogram(y):\n    D = _stft(preemphasis(y))\n    S = _amp_to_db(_linear_to_mel(np.abs(D)))\n    return _normalize(S)\n\ndef _stft(y):\n    n_fft, hop_length, win_length = _stft_parameters()\n    return librosa.stft(y=y, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n\ndef find_endpoint(wav, threshold_db=-40, min_silence_sec=0.8):\n  window_length = int(hp.sample_rate * min_silence_sec)\n  hop_length = int(window_length / 4)\n  threshold = _db_to_amp(threshold_db)\n  for x in range(hop_length, len(wav) - window_length, hop_length):\n    if np.max(wav[x:x+window_length]) < threshold:\n      return x + hop_length\n  return len(wav)\n\ndef _pad_data(x, length):\n    _pad = 0\n    return np.pad(x, (0, length - x.shape[0]), mode=\'constant\', constant_values=_pad)\n\ndef _prepare_data(inputs):\n    max_len = max((len(x) for x in inputs))\n    return np.stack([_pad_data(x, max_len) for x in inputs])\n\ndef _pad_per_step(inputs):\n    timesteps = inputs.shape[-1]\n    return np.pad(inputs, [[0,0],[0,0],[0, hp.outputs_per_step - (timesteps % hp.outputs_per_step)]], mode=\'constant\', constant_values=0.0)\n\ndef get_param_size(model):\n    params = 0\n    for p in model.parameters():\n        tmp = 1\n        for x in p.size():\n            tmp *= x\n        params += tmp\n    return params\n\ndef get_dataset():\n    return LJDatasets(os.path.join(hp.data_path,\'metadata.csv\'), os.path.join(hp.data_path,\'wavs\'))\n'"
hyperparams.py,0,"b""# Audio\n\nnum_mels = 80\nnum_freq = 1024\nsample_rate = 20000\nframe_length_ms = 50.\nframe_shift_ms = 12.5\npreemphasis = 0.97\nmin_level_db = -100\nref_level_db = 20\nhidden_size = 128\nembedding_size = 256\n\nmax_iters = 200\ngriffin_lim_iters = 60\npower = 1.5\noutputs_per_step = 5\nteacher_forcing_ratio = 1.0\n\nepochs = 10000\nlr = 0.001\ndecay_step = [500000, 1000000, 2000000]\nlog_step = 100\nsave_step = 2000\n\ncleaners='english_cleaners'\n\ndata_path = '../data'\noutput_path = './result'\ncheckpoint_path = './model_new'"""
module.py,16,"b'import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import OrderedDict\nimport numpy as np\nimport hyperparams as hp\n\nuse_cuda = torch.cuda.is_available()\n\nclass SeqLinear(nn.Module):\n    """"""\n    Linear layer for sequences\n    """"""\n    def __init__(self, input_size, output_size, time_dim=2):\n        """"""\n        :param input_size: dimension of input\n        :param output_size: dimension of output\n        :param time_dim: index of time dimension\n        """"""\n        super(SeqLinear, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.time_dim = time_dim\n        self.linear = nn.Linear(input_size, output_size)\n\n\n    def forward(self, input_):\n        """"""\n\n        :param input_: sequences\n        :return: outputs\n        """"""\n        batch_size = input_.size()[0]\n        if self.time_dim == 2:\n            input_ = input_.transpose(1, 2).contiguous()\n        input_ = input_.view(-1, self.input_size)\n\n        out = self.linear(input_).view(batch_size, -1, self.output_size)\n\n        if self.time_dim == 2:\n            out = out.contiguous().transpose(1, 2)\n\n        return out\n\nclass Prenet(nn.Module):\n    """"""\n    Prenet before passing through the network\n    """"""\n    def __init__(self, input_size, hidden_size, output_size):\n        """"""\n\n        :param input_size: dimension of input\n        :param hidden_size: dimension of hidden unit\n        :param output_size: dimension of output\n        """"""\n        super(Prenet, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.layer = nn.Sequential(OrderedDict([\n             (\'fc1\', SeqLinear(self.input_size, self.hidden_size)),\n             (\'relu1\', nn.ReLU()),\n             (\'dropout1\', nn.Dropout(0.5)),\n             (\'fc2\', SeqLinear(self.hidden_size, self.output_size)),\n             (\'relu2\', nn.ReLU()),\n             (\'dropout2\', nn.Dropout(0.5)),\n        ]))\n\n    def forward(self, input_):\n\n        out = self.layer(input_)\n\n        return out\n\nclass CBHG(nn.Module):\n    """"""\n    CBHG Module\n    """"""\n    def __init__(self, hidden_size, K=16, projection_size = 128, num_gru_layers=2, max_pool_kernel_size=2, is_post=False):\n        """"""\n\n        :param hidden_size: dimension of hidden unit\n        :param K: # of convolution banks\n        :param projection_size: dimension of projection unit\n        :param num_gru_layers: # of layers of GRUcell\n        :param max_pool_kernel_size: max pooling kernel size\n        :param is_post: whether post processing or not\n        """"""\n        super(CBHG, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_gru_layers = num_gru_layers\n        self.projection_size = projection_size\n        self.convbank_list = nn.ModuleList()\n        self.convbank_list.append(nn.Conv1d(in_channels=projection_size,\n                                                out_channels=hidden_size,\n                                                kernel_size=1,\n                                                padding=int(np.floor(1/2))))\n\n        for i in range(2, K+1):\n            self.convbank_list.append(nn.Conv1d(in_channels=hidden_size,\n                                                out_channels=hidden_size,\n                                                kernel_size=i,\n                                                padding=int(np.floor(i/2))))\n\n        self.batchnorm_list = nn.ModuleList()\n        for i in range(1, K+1):\n            self.batchnorm_list.append(nn.BatchNorm1d(hidden_size))\n\n        convbank_outdim = hidden_size * K\n        if is_post:\n            self.conv_projection_1 = nn.Conv1d(in_channels=convbank_outdim,\n                                             out_channels=hidden_size * 2,\n                                             kernel_size=3,\n                                             padding=int(np.floor(3/2)))\n            self.conv_projection_2 = nn.Conv1d(in_channels=hidden_size * 2,\n                                               out_channels=projection_size,\n                                               kernel_size=3,\n                                               padding=int(np.floor(3/2)))\n            self.batchnorm_proj_1 = nn.BatchNorm1d(hidden_size * 2)\n\n        else:\n            self.conv_projection_1 = nn.Conv1d(in_channels=convbank_outdim,\n                                             out_channels=hidden_size,\n                                             kernel_size=3,\n                                             padding=int(np.floor(3 / 2)))\n            self.conv_projection_2 = nn.Conv1d(in_channels=hidden_size,\n                                               out_channels=projection_size,\n                                               kernel_size=3,\n                                               padding=int(np.floor(3 / 2)))\n            self.batchnorm_proj_1 = nn.BatchNorm1d(hidden_size)\n\n        self.batchnorm_proj_2 = nn.BatchNorm1d(projection_size)\n\n\n        self.max_pool = nn.MaxPool1d(max_pool_kernel_size, stride=1, padding=1)\n        self.highway = Highwaynet(self.projection_size)\n        self.gru = nn.GRU(self.projection_size, self.hidden_size, num_layers=2,\n                          batch_first=True,\n                          bidirectional=True)\n\n\n    def _conv_fit_dim(self, x, kernel_size=3):\n        if kernel_size % 2 == 0:\n            return x[:,:,:-1]\n        else:\n            return x\n\n    def forward(self, input_):\n\n        input_ = input_.contiguous()\n        batch_size = input_.size()[0]\n\n        convbank_list = list()\n        convbank_input = input_\n\n        # Convolution bank filters\n        for k, (conv, batchnorm) in enumerate(zip(self.convbank_list, self.batchnorm_list)):\n            convbank_input = F.relu(batchnorm(self._conv_fit_dim(conv(convbank_input), k+1).contiguous()))\n            convbank_list.append(convbank_input)\n\n        # Concatenate all features\n        conv_cat = torch.cat(convbank_list, dim=1)\n\n        # Max pooling\n        conv_cat = self.max_pool(conv_cat)[:,:,:-1]\n\n        # Projection\n        conv_projection = F.relu(self.batchnorm_proj_1(self._conv_fit_dim(self.conv_projection_1(conv_cat))))\n        conv_projection = self.batchnorm_proj_2(self._conv_fit_dim(self.conv_projection_2(conv_projection))) + input_\n\n        # Highway networks\n        highway = self.highway.forward(conv_projection)\n        highway = torch.transpose(highway, 1,2)\n\n        # Bidirectional GRU\n        if use_cuda:\n            init_gru = Variable(torch.zeros(2 * self.num_gru_layers, batch_size, self.hidden_size)).cuda()\n        else:\n            init_gru = Variable(torch.zeros(2 * self.num_gru_layers, batch_size, self.hidden_size))\n\n        self.gru.flatten_parameters()\n        out, _ = self.gru(highway, init_gru)\n\n        return out\n\n\nclass Highwaynet(nn.Module):\n    """"""\n    Highway network\n    """"""\n    def __init__(self, num_units, num_layers=4):\n        """"""\n\n        :param num_units: dimension of hidden unit\n        :param num_layers: # of highway layers\n        """"""\n        super(Highwaynet, self).__init__()\n        self.num_units = num_units\n        self.num_layers = num_layers\n        self.gates = nn.ModuleList()\n        self.linears = nn.ModuleList()\n        for _ in range(self.num_layers):\n            self.linears.append(SeqLinear(num_units, num_units))\n            self.gates.append(SeqLinear(num_units, num_units))\n\n    def forward(self, input_):\n\n        out = input_\n\n        # highway gated function\n        for fc1, fc2 in zip(self.linears, self.gates):\n\n            h = F.relu(fc1.forward(out))\n            t = F.sigmoid(fc2.forward(out))\n\n            c = 1. - t\n            out = h * t + out * c\n\n        return out\n\nclass AttentionDecoder(nn.Module):\n    """"""\n    Decoder with attention mechanism (Vinyals et al.)\n    """"""\n    def __init__(self, num_units):\n        """"""\n\n        :param num_units: dimension of hidden units\n        """"""\n        super(AttentionDecoder, self).__init__()\n        self.num_units = num_units\n\n        self.v = nn.Linear(num_units, 1, bias=False)\n        self.W1 = nn.Linear(num_units, num_units, bias=False)\n        self.W2 = nn.Linear(num_units, num_units, bias=False)\n\n        self.attn_grucell = nn.GRUCell(num_units // 2, num_units)\n        self.gru1 = nn.GRUCell(num_units, num_units)\n        self.gru2 = nn.GRUCell(num_units, num_units)\n\n        self.attn_projection = nn.Linear(num_units * 2, num_units)\n        self.out = nn.Linear(num_units, hp.num_mels * hp.outputs_per_step)\n\n    def forward(self, decoder_input, memory, attn_hidden, gru1_hidden, gru2_hidden):\n\n        memory_len = memory.size()[1]\n        batch_size = memory.size()[0]\n\n        # Get keys\n        keys = self.W1(memory.contiguous().view(-1, self.num_units))\n        keys = keys.view(-1, memory_len, self.num_units)\n\n        # Get hidden state (query) passed through GRUcell\n        d_t = self.attn_grucell(decoder_input, attn_hidden)\n\n        # Duplicate query with same dimension of keys for matrix operation (Speed up)\n        d_t_duplicate = self.W2(d_t).unsqueeze(1).expand_as(memory)\n\n        # Calculate attention score and get attention weights\n        attn_weights = self.v(F.tanh(keys + d_t_duplicate).view(-1, self.num_units)).view(-1, memory_len, 1)\n        attn_weights = attn_weights.squeeze(2)\n        attn_weights = F.softmax(attn_weights)\n\n        # Concatenate with original query\n        d_t_prime = torch.bmm(attn_weights.view([batch_size,1,-1]), memory).squeeze(1)\n\n        # Residual GRU\n        gru1_input = self.attn_projection(torch.cat([d_t, d_t_prime], 1))\n        gru1_hidden = self.gru1(gru1_input, gru1_hidden)\n        gru2_input = gru1_input + gru1_hidden\n\n        gru2_hidden = self.gru2(gru2_input, gru2_hidden)\n        bf_out = gru2_input + gru2_hidden\n\n        # Output\n        output = self.out(bf_out).view(-1, hp.num_mels, hp.outputs_per_step)\n\n        return output, d_t, gru1_hidden, gru2_hidden\n\n    def inithidden(self, batch_size):\n        if use_cuda:\n            attn_hidden = Variable(torch.zeros(batch_size, self.num_units), requires_grad=False).cuda()\n            gru1_hidden = Variable(torch.zeros(batch_size, self.num_units), requires_grad=False).cuda()\n            gru2_hidden = Variable(torch.zeros(batch_size, self.num_units), requires_grad=False).cuda()\n        else:\n            attn_hidden = Variable(torch.zeros(batch_size, self.num_units), requires_grad=False)\n            gru1_hidden = Variable(torch.zeros(batch_size, self.num_units), requires_grad=False)\n            gru2_hidden = Variable(torch.zeros(batch_size, self.num_units), requires_grad=False)\n\n        return attn_hidden, gru1_hidden, gru2_hidden'"
network.py,4,"b'#-*- coding: utf-8 -*-\n\nfrom module import *\nfrom text.symbols import symbols\nimport hyperparams as hp\nimport random\n\nclass Encoder(nn.Module):\n    """"""\n    Encoder\n    """"""\n    def __init__(self, embedding_size):\n        """"""\n\n        :param embedding_size: dimension of embedding\n        """"""\n        super(Encoder, self).__init__()\n        self.embedding_size = embedding_size\n        self.embed = nn.Embedding(len(symbols), embedding_size)\n        self.prenet = Prenet(embedding_size, hp.hidden_size * 2, hp.hidden_size)\n        self.cbhg = CBHG(hp.hidden_size)\n\n    def forward(self, input_):\n\n        input_ = torch.transpose(self.embed(input_),1,2)\n        prenet = self.prenet.forward(input_)\n        memory = self.cbhg.forward(prenet)\n\n        return memory\n\nclass MelDecoder(nn.Module):\n    """"""\n    Decoder\n    """"""\n    def __init__(self):\n        super(MelDecoder, self).__init__()\n        self.prenet = Prenet(hp.num_mels, hp.hidden_size * 2, hp.hidden_size)\n        self.attn_decoder = AttentionDecoder(hp.hidden_size * 2)\n\n    def forward(self, decoder_input, memory):\n\n        # Initialize hidden state of GRUcells\n        attn_hidden, gru1_hidden, gru2_hidden = self.attn_decoder.inithidden(decoder_input.size()[0])\n        outputs = list()\n\n        # Training phase\n        if self.training:\n            # Prenet\n            dec_input = self.prenet.forward(decoder_input)\n            timesteps = dec_input.size()[2] // hp.outputs_per_step\n\n            # [GO] Frame\n            prev_output = dec_input[:, :, 0]\n\n            for i in range(timesteps):\n                prev_output, attn_hidden, gru1_hidden, gru2_hidden = self.attn_decoder.forward(prev_output, memory,\n                                                                                             attn_hidden=attn_hidden,\n                                                                                             gru1_hidden=gru1_hidden,\n                                                                                             gru2_hidden=gru2_hidden)\n\n                outputs.append(prev_output)\n\n                if random.random() < hp.teacher_forcing_ratio:\n                    # Get spectrum at rth position\n                    prev_output = dec_input[:, :, i * hp.outputs_per_step]\n                else:\n                    # Get last output\n                    prev_output = prev_output[:, :, -1]\n\n            # Concatenate all mel spectrogram\n            outputs = torch.cat(outputs, 2)\n\n        else:\n            # [GO] Frame\n            prev_output = decoder_input\n\n            for i in range(hp.max_iters):\n                prev_output = self.prenet.forward(prev_output)\n                prev_output = prev_output[:,:,0]\n                prev_output, attn_hidden, gru1_hidden, gru2_hidden = self.attn_decoder.forward(prev_output, memory,\n                                                                                         attn_hidden=attn_hidden,\n                                                                                         gru1_hidden=gru1_hidden,\n                                                                                         gru2_hidden=gru2_hidden)\n                outputs.append(prev_output)\n                prev_output = prev_output[:, :, -1].unsqueeze(2)\n\n            outputs = torch.cat(outputs, 2)\n\n        return outputs\n\nclass PostProcessingNet(nn.Module):\n    """"""\n    Post-processing Network\n    """"""\n    def __init__(self):\n        super(PostProcessingNet, self).__init__()\n        self.postcbhg = CBHG(hp.hidden_size,\n                             K=8,\n                             projection_size=hp.num_mels,\n                             is_post=True)\n        self.linear = SeqLinear(hp.hidden_size * 2,\n                                hp.num_freq)\n\n    def forward(self, input_):\n        out = self.postcbhg.forward(input_)\n        out = self.linear.forward(torch.transpose(out,1,2))\n\n        return out\n\nclass Tacotron(nn.Module):\n    """"""\n    End-to-end Tacotron Network\n    """"""\n    def __init__(self):\n        super(Tacotron, self).__init__()\n        self.encoder = Encoder(hp.embedding_size)\n        self.decoder1 = MelDecoder()\n        self.decoder2 = PostProcessingNet()\n\n    def forward(self, characters, mel_input):\n        memory = self.encoder.forward(characters)\n        mel_output = self.decoder1.forward(mel_input, memory)\n        linear_output = self.decoder2.forward(mel_output)\n\n        return mel_output, linear_output'"
synthesis.py,4,"b'#-*- coding: utf-8 -*-\n\nfrom network import *\nfrom data import inv_spectrogram, find_endpoint, save_wav, spectrogram\nimport numpy as np\nimport argparse\nimport os, sys\nimport io\nfrom text import text_to_sequence\n\nuse_cuda = torch.cuda.is_available()\n\ndef main(args):\n\n    # Make model\n    if use_cuda:\n        model = nn.DataParallel(Tacotron().cuda())\n\n    # Load checkpoint\n    try:\n        checkpoint = torch.load(os.path.join(hp.checkpoint_path,\'checkpoint_%d.pth.tar\'% args.restore_step))\n        model.load_state_dict(checkpoint[\'model\'])\n        print(""\\n--------model restored at step %d--------\\n"" % args.restore_step)\n\n    except:\n        raise FileNotFoundError(""\\n------------Model not exists------------\\n"")\n\n    # Evaluation\n    model = model.eval()\n\n    # Make result folder if not exists\n    if not os.path.exists(hp.output_path):\n        os.mkdir(hp.output_path)\n\n    # Sentences for generation\n    sentences = [\n        ""And it is worth mention in passing that, as an example of fine typography,"",\n        # From July 8, 2017 New York Times:\n        \'Scientists at the CERN laboratory say they have discovered a new particle.\',\n        \'There\xe2\x80\x99s a way to measure the acute emotional intelligence that has never gone out of style.\',\n        \'President Trump met with other leaders at the Group of 20 conference.\',\n        \'The Senate\\\'s bill to repeal and replace the Affordable Care Act is now imperiled.\',\n        # From Google\'s Tacotron example page:\n        \'Generative adversarial network or variational auto-encoder.\',\n        \'The buses aren\\\'t the problem, they actually provide a solution.\',\n        \'Does the quick brown fox jump over the lazy dog?\',\n        \'Talib Kweli confirmed to AllHipHop that he will be releasing an album in the next year.\',\n    ]\n\n    # Synthesis and save to wav files\n    for i, text in enumerate(sentences):\n        wav = generate(model, text)\n        path = os.path.join(hp.output_path, \'result_%d_%d.wav\' % (args.restore_step, i+1))\n        with open(path, \'wb\') as f:\n            f.write(wav)\n\n        f.close()\n        print(""save wav file at step %d ..."" % (i+1))\n\ndef generate(model, text):\n\n    # Text to index sequence\n    cleaner_names = [x.strip() for x in hp.cleaners.split(\',\')]\n    seq = np.expand_dims(np.asarray(text_to_sequence(text, cleaner_names), dtype=np.int32), axis=0)\n\n    # Provide [GO] Frame\n    mel_input = np.zeros([seq.shape[0], hp.num_mels, 1], dtype=np.float32)\n\n    # Variables\n    characters = Variable(torch.from_numpy(seq).type(torch.cuda.LongTensor), volatile=True).cuda()\n    mel_input = Variable(torch.from_numpy(mel_input).type(torch.cuda.FloatTensor), volatile=True).cuda()\n\n    # Spectrogram to wav\n    _, linear_output = model.forward(characters, mel_input)\n    wav = inv_spectrogram(linear_output[0].data.cpu().numpy())\n    wav = wav[:find_endpoint(wav)]\n    out = io.BytesIO()\n    save_wav(wav, out)\n\n    return out.getvalue()\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--restore_step\', type=int, help=\'Global step to restore checkpoint\', default=0)\n    parser.add_argument(\'--batch_size\', type=int, help=\'Batch size\', default=1)\n    args = parser.parse_args()\n    main(args)\n'"
train.py,14,"b'from network import *\nfrom data import get_dataset, DataLoader, collate_fn, get_param_size\nfrom torch import optim\nimport numpy as np\nimport argparse\nimport os\nimport time\nimport torch\nimport torch.nn as nn\n\nuse_cuda = torch.cuda.is_available()\n\ndef main(args):\n\n    # Get dataset\n    dataset = get_dataset()\n\n    # Construct model\n    if use_cuda:\n        model = nn.DataParallel(Tacotron().cuda())\n    else:\n        model = Tacotron()\n\n    # Make optimizer\n    optimizer = optim.Adam(model.parameters(), lr=hp.lr)\n\n    # Load checkpoint if exists\n    try:\n        checkpoint = torch.load(os.path.join(hp.checkpoint_path,\'checkpoint_%d.pth.tar\'% args.restore_step))\n        model.load_state_dict(checkpoint[\'model\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        print(""\\n--------model restored at step %d--------\\n"" % args.restore_step)\n\n    except:\n        print(""\\n--------Start New Training--------\\n"")\n\n    # Training\n    model = model.train()\n\n    # Make checkpoint directory if not exists\n    if not os.path.exists(hp.checkpoint_path):\n        os.mkdir(hp.checkpoint_path)\n\n    # Decide loss function\n    if use_cuda:\n        criterion = nn.L1Loss().cuda()\n    else:\n        criterion = nn.L1Loss()\n\n    # Loss for frequency of human register\n    n_priority_freq = int(3000 / (hp.sample_rate * 0.5) * hp.num_freq)\n\n    for epoch in range(hp.epochs):\n\n        dataloader = DataLoader(dataset, batch_size=args.batch_size,\n                                shuffle=True, collate_fn=collate_fn, drop_last=True, num_workers=8)\n\n        for i, data in enumerate(dataloader):\n\n            current_step = i + args.restore_step + epoch * len(dataloader) + 1\n\n            optimizer.zero_grad()\n\n            # Make decoder input by concatenating [GO] Frame\n            try:\n                mel_input = np.concatenate((np.zeros([args.batch_size, hp.num_mels, 1], dtype=np.float32),data[2][:,:,1:]), axis=2)\n            except:\n                raise TypeError(""not same dimension"")\n\n            if use_cuda:\n                characters = Variable(torch.from_numpy(data[0]).type(torch.cuda.LongTensor), requires_grad=False).cuda()\n                mel_input = Variable(torch.from_numpy(mel_input).type(torch.cuda.FloatTensor), requires_grad=False).cuda()\n                mel_spectrogram = Variable(torch.from_numpy(data[2]).type(torch.cuda.FloatTensor), requires_grad=False).cuda()\n                linear_spectrogram = Variable(torch.from_numpy(data[1]).type(torch.cuda.FloatTensor), requires_grad=False).cuda()\n\n            else:\n                characters = Variable(torch.from_numpy(data[0]).type(torch.LongTensor), requires_grad=False)\n                mel_input = Variable(torch.from_numpy(mel_input).type(torch.FloatTensor), requires_grad=False)\n                mel_spectrogram = Variable(torch.from_numpy(data[2]).type(torch.FloatTensor), requires_grad=False)\n                linear_spectrogram = Variable(torch.from_numpy(data[1]).type(torch.FloatTensor), requires_grad=False)\n\n            # Forward\n            mel_output, linear_output = model.forward(characters, mel_input)\n\n            # Calculate loss\n            mel_loss = criterion(mel_output, mel_spectrogram)\n            linear_loss = torch.abs(linear_output-linear_spectrogram)\n            linear_loss = 0.5 * torch.mean(linear_loss) + 0.5 * torch.mean(linear_loss[:,:n_priority_freq,:])\n            loss = mel_loss + linear_loss\n            loss = loss.cuda()\n\n            start_time = time.time()\n\n            # Calculate gradients\n            loss.backward()\n\n            # clipping gradients\n            nn.utils.clip_grad_norm(model.parameters(), 1.)\n\n            # Update weights\n            optimizer.step()\n\n            time_per_step = time.time() - start_time\n\n            if current_step % hp.log_step == 0:\n                print(""time per step: %.2f sec"" % time_per_step)\n                print(""At timestep %d"" % current_step)\n                print(""linear loss: %.4f"" % linear_loss.data[0])\n                print(""mel loss: %.4f"" % mel_loss.data[0])\n                print(""total loss: %.4f"" % loss.data[0])\n\n            if current_step % hp.save_step == 0:\n                save_checkpoint({\'model\':model.state_dict(),\n                                 \'optimizer\':optimizer.state_dict()},\n                                os.path.join(hp.checkpoint_path,\'checkpoint_%d.pth.tar\' % current_step))\n                print(""save model at step %d ..."" % current_step)\n\n            if current_step in hp.decay_step:\n                optimizer = adjust_learning_rate(optimizer, current_step)\n\ndef save_checkpoint(state, filename=\'checkpoint.pth.tar\'):\n    torch.save(state, filename)\n\ndef adjust_learning_rate(optimizer, step):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    if step == 500000:\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = 0.0005\n\n    elif step == 1000000:\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = 0.0003\n\n    elif step == 2000000:\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = 0.0001\n\n    return optimizer\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--restore_step\', type=int, help=\'Global step to restore checkpoint\', default=0)\n    parser.add_argument(\'--batch_size\', type=int, help=\'Batch size\', default=32)\n    args = parser.parse_args()\n    main(args)\n\n'"
text/__init__.py,0,"b'#-*- coding: utf-8 -*-\n\nimport re\nfrom Tacotron.text import cleaners\nfrom Tacotron.text.symbols import symbols\n\n\n\n\n# Mappings from symbol to numeric ID and vice versa:\n_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n\n# Regular expression matching text enclosed in curly braces:\n_curly_re = re.compile(r\'(.*?)\\{(.+?)\\}(.*)\')\n\n\ndef text_to_sequence(text, cleaner_names):\n  \'\'\'Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n\n    The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n    in it. For example, ""Turn left on {HH AW1 S S T AH0 N} Street.""\n\n    Args:\n      text: string to convert to a sequence\n      cleaner_names: names of the cleaner functions to run the text through\n\n    Returns:\n      List of integers corresponding to the symbols in the text\n  \'\'\'\n  sequence = []\n\n  # Check for curly braces and treat their contents as ARPAbet:\n  while len(text):\n    m = _curly_re.match(text)\n    if not m:\n      sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n      break\n    sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n    sequence += _arpabet_to_sequence(m.group(2))\n    text = m.group(3)\n\n  # Append EOS token\n  sequence.append(_symbol_to_id[\'~\'])\n  return sequence\n\n\ndef sequence_to_text(sequence):\n  \'\'\'Converts a sequence of IDs back to a string\'\'\'\n  result = \'\'\n  for symbol_id in sequence:\n    if symbol_id in _id_to_symbol:\n      s = _id_to_symbol[symbol_id]\n      # Enclose ARPAbet back in curly braces:\n      if len(s) > 1 and s[0] == \'@\':\n        s = \'{%s}\' % s[1:]\n      result += s\n  return result.replace(\'}{\', \' \')\n\n\ndef _clean_text(text, cleaner_names):\n  for name in cleaner_names:\n    cleaner = getattr(cleaners, name)\n    if not cleaner:\n      raise Exception(\'Unknown cleaner: %s\' % name)\n    text = cleaner(text)\n  return text\n\n\ndef _symbols_to_sequence(symbols):\n  return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]\n\n\ndef _arpabet_to_sequence(text):\n  return _symbols_to_sequence([\'@\' + s for s in text.split()])\n\n\ndef _should_keep_symbol(s):\n  return s in _symbol_to_id and s is not \'_\' and s is not \'~\''"
text/cleaners.py,0,"b'#-*- coding: utf-8 -*-\n\n\n\'\'\'\nCleaners are transformations that run over the input text at both training and eval time.\n\nCleaners can be selected by passing a comma-delimited list of cleaner names as the ""cleaners""\nhyperparameter. Some cleaners are English-specific. You\'ll typically want to use:\n  1. ""english_cleaners"" for English text\n  2. ""transliteration_cleaners"" for non-English text that can be transliterated to ASCII using\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n  3. ""basic_cleaners"" if you do not want to transliterate (in this case, you should also update\n     the symbols in symbols.py to match your data).\n\'\'\'\n\nimport re\nfrom unidecode import unidecode\nfrom .numbers import normalize_numbers\n\n\n# Regular expression matching whitespace:\n_whitespace_re = re.compile(r\'\\s+\')\n\n# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = [(re.compile(\'\\\\b%s\\\\.\' % x[0], re.IGNORECASE), x[1]) for x in [\n  (\'mrs\', \'misess\'),\n  (\'mr\', \'mister\'),\n  (\'dr\', \'doctor\'),\n  (\'st\', \'saint\'),\n  (\'co\', \'company\'),\n  (\'jr\', \'junior\'),\n  (\'maj\', \'major\'),\n  (\'gen\', \'general\'),\n  (\'drs\', \'doctors\'),\n  (\'rev\', \'reverend\'),\n  (\'lt\', \'lieutenant\'),\n  (\'hon\', \'honorable\'),\n  (\'sgt\', \'sergeant\'),\n  (\'capt\', \'captain\'),\n  (\'esq\', \'esquire\'),\n  (\'ltd\', \'limited\'),\n  (\'col\', \'colonel\'),\n  (\'ft\', \'fort\'),\n]]\n\n\ndef expand_abbreviations(text):\n  for regex, replacement in _abbreviations:\n    text = re.sub(regex, replacement, text)\n  return text\n\n\ndef expand_numbers(text):\n  return normalize_numbers(text)\n\n\ndef lowercase(text):\n  return text.lower()\n\n\ndef collapse_whitespace(text):\n  return re.sub(_whitespace_re, \' \', text)\n\n\ndef convert_to_ascii(text):\n  return unidecode(text)\n\n\ndef basic_cleaners(text):\n  \'\'\'Basic pipeline that lowercases and collapses whitespace without transliteration.\'\'\'\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef transliteration_cleaners(text):\n  \'\'\'Pipeline for non-English text that transliterates to ASCII.\'\'\'\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef english_cleaners(text):\n  \'\'\'Pipeline for English text, including number and abbreviation expansion.\'\'\'\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = expand_numbers(text)\n  text = expand_abbreviations(text)\n  text = collapse_whitespace(text)\n  return text\n'"
text/cmudict.py,0,"b'#-*- coding: utf-8 -*-\n\n\nimport re\n\n\nvalid_symbols = [\n  \'AA\', \'AA0\', \'AA1\', \'AA2\', \'AE\', \'AE0\', \'AE1\', \'AE2\', \'AH\', \'AH0\', \'AH1\', \'AH2\',\n  \'AO\', \'AO0\', \'AO1\', \'AO2\', \'AW\', \'AW0\', \'AW1\', \'AW2\', \'AY\', \'AY0\', \'AY1\', \'AY2\',\n  \'B\', \'CH\', \'D\', \'DH\', \'EH\', \'EH0\', \'EH1\', \'EH2\', \'ER\', \'ER0\', \'ER1\', \'ER2\', \'EY\',\n  \'EY0\', \'EY1\', \'EY2\', \'F\', \'G\', \'HH\', \'IH\', \'IH0\', \'IH1\', \'IH2\', \'IY\', \'IY0\', \'IY1\',\n  \'IY2\', \'JH\', \'K\', \'L\', \'M\', \'N\', \'NG\', \'OW\', \'OW0\', \'OW1\', \'OW2\', \'OY\', \'OY0\',\n  \'OY1\', \'OY2\', \'P\', \'R\', \'S\', \'SH\', \'T\', \'TH\', \'UH\', \'UH0\', \'UH1\', \'UH2\', \'UW\',\n  \'UW0\', \'UW1\', \'UW2\', \'V\', \'W\', \'Y\', \'Z\', \'ZH\'\n]\n\n_valid_symbol_set = set(valid_symbols)\n\n\nclass CMUDict:\n  \'\'\'Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict\'\'\'\n  def __init__(self, file_or_path, keep_ambiguous=True):\n    if isinstance(file_or_path, str):\n      with open(file_or_path, encoding=\'latin-1\') as f:\n        entries = _parse_cmudict(f)\n    else:\n      entries = _parse_cmudict(file_or_path)\n    if not keep_ambiguous:\n      entries = {word: pron for word, pron in entries.items() if len(pron) == 1}\n    self._entries = entries\n\n\n  def __len__(self):\n    return len(self._entries)\n\n\n  def lookup(self, word):\n    \'\'\'Returns list of ARPAbet pronunciations of the given word.\'\'\'\n    return self._entries.get(word.upper())\n\n\n\n_alt_re = re.compile(r\'\\([0-9]+\\)\')\n\n\ndef _parse_cmudict(file):\n  cmudict = {}\n  for line in file:\n    if len(line) and (line[0] >= \'A\' and line[0] <= \'Z\' or line[0] == ""\'""):\n      parts = line.split(\'  \')\n      word = re.sub(_alt_re, \'\', parts[0])\n      pronunciation = _get_pronunciation(parts[1])\n      if pronunciation:\n        if word in cmudict:\n          cmudict[word].append(pronunciation)\n        else:\n          cmudict[word] = [pronunciation]\n  return cmudict\n\n\ndef _get_pronunciation(s):\n  parts = s.strip().split(\' \')\n  for part in parts:\n    if part not in _valid_symbol_set:\n      return None\n  return \' \'.join(parts)\n'"
text/numbers.py,0,"b""#-*- coding: utf-8 -*-\n\nimport inflect\nimport re\n\n\n_inflect = inflect.engine()\n_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')\n_decimal_number_re = re.compile(r'([0-9]+\\.[0-9]+)')\n_pounds_re = re.compile(r'\xc2\xa3([0-9\\,]*[0-9]+)')\n_dollars_re = re.compile(r'\\$([0-9\\.\\,]*[0-9]+)')\n_ordinal_re = re.compile(r'[0-9]+(st|nd|rd|th)')\n_number_re = re.compile(r'[0-9]+')\n\n\ndef _remove_commas(m):\n  return m.group(1).replace(',', '')\n\n\ndef _expand_decimal_point(m):\n  return m.group(1).replace('.', ' point ')\n\n\ndef _expand_dollars(m):\n  match = m.group(1)\n  parts = match.split('.')\n  if len(parts) > 2:\n    return match + ' dollars'  # Unexpected format\n  dollars = int(parts[0]) if parts[0] else 0\n  cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n  if dollars and cents:\n    dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n    cent_unit = 'cent' if cents == 1 else 'cents'\n    return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n  elif dollars:\n    dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n    return '%s %s' % (dollars, dollar_unit)\n  elif cents:\n    cent_unit = 'cent' if cents == 1 else 'cents'\n    return '%s %s' % (cents, cent_unit)\n  else:\n    return 'zero dollars'\n\n\ndef _expand_ordinal(m):\n  return _inflect.number_to_words(m.group(0))\n\n\ndef _expand_number(m):\n  num = int(m.group(0))\n  if num > 1000 and num < 3000:\n    if num == 2000:\n      return 'two thousand'\n    elif num > 2000 and num < 2010:\n      return 'two thousand ' + _inflect.number_to_words(num % 100)\n    elif num % 100 == 0:\n      return _inflect.number_to_words(num // 100) + ' hundred'\n    else:\n      return _inflect.number_to_words(num, andword='', zero='oh', group=2).replace(', ', ' ')\n  else:\n    return _inflect.number_to_words(num, andword='')\n\n\ndef normalize_numbers(text):\n  text = re.sub(_comma_number_re, _remove_commas, text)\n  text = re.sub(_pounds_re, r'\\1 pounds', text)\n  text = re.sub(_dollars_re, _expand_dollars, text)\n  text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n  text = re.sub(_ordinal_re, _expand_ordinal, text)\n  text = re.sub(_number_re, _expand_number, text)\n  return text\n"""
text/symbols.py,0,"b'#-*- coding: utf-8 -*-\n\n\n\'\'\'\nDefines the set of symbols used in text input to the model.\n\nThe default is a set of ASCII characters that works well for English or text that has been run\nthrough Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details.\n\'\'\'\nfrom Tacotron.text import cmudict\n\n_pad        = \'_\'\n_eos        = \'~\'\n_characters = \'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\\\'(),-.:;? \'\n\n# Prepend ""@"" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n_arpabet = [\'@\' + s for s in cmudict.valid_symbols]\n\n# Export all symbols:\nsymbols = [_pad, _eos] + list(_characters) + _arpabet\n\n\nif __name__ == \'__main__\':\n    print(symbols)'"
