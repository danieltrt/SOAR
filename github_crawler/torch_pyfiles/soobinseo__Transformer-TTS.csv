file_path,api_count,code
hyperparams.py,0,"b""# Audio\nnum_mels = 80\n# num_freq = 1024\nn_fft = 2048\nsr = 22050\n# frame_length_ms = 50.\n# frame_shift_ms = 12.5\npreemphasis = 0.97\nframe_shift = 0.0125 # seconds\nframe_length = 0.05 # seconds\nhop_length = int(sr*frame_shift) # samples.\nwin_length = int(sr*frame_length) # samples.\nn_mels = 80 # Number of Mel banks to generate\npower = 1.2 # Exponent for amplifying the predicted magnitude\nmin_level_db = -100\nref_level_db = 20\nhidden_size = 256\nembedding_size = 512\nmax_db = 100\nref_db = 20\n    \nn_iter = 60\n# power = 1.5\noutputs_per_step = 1\n\nepochs = 10000\nlr = 0.001\nsave_step = 2000\nimage_step = 500\nbatch_size = 32\n\ncleaners='english_cleaners'\n\ndata_path = './data/LJSpeech-1.1'\ncheckpoint_path = './checkpoint'\nsample_path = './samples'"""
module.py,2,"b'import torch.nn as nn\nimport torch as t\nimport torch.nn.functional as F\nimport math\nimport hyperparams as hp\nfrom text.symbols import symbols\nimport numpy as np\nimport copy\nfrom collections import OrderedDict\n\ndef clones(module, N):\n    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\nclass Linear(nn.Module):\n    """"""\n    Linear Module\n    """"""\n    def __init__(self, in_dim, out_dim, bias=True, w_init=\'linear\'):\n        """"""\n        :param in_dim: dimension of input\n        :param out_dim: dimension of output\n        :param bias: boolean. if True, bias is included.\n        :param w_init: str. weight inits with xavier initialization.\n        """"""\n        super(Linear, self).__init__()\n        self.linear_layer = nn.Linear(in_dim, out_dim, bias=bias)\n\n        nn.init.xavier_uniform_(\n            self.linear_layer.weight,\n            gain=nn.init.calculate_gain(w_init))\n\n    def forward(self, x):\n        return self.linear_layer(x)\n\n\nclass Conv(nn.Module):\n    """"""\n    Convolution Module\n    """"""\n    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n                 padding=0, dilation=1, bias=True, w_init=\'linear\'):\n        """"""\n        :param in_channels: dimension of input\n        :param out_channels: dimension of output\n        :param kernel_size: size of kernel\n        :param stride: size of stride\n        :param padding: size of padding\n        :param dilation: dilation rate\n        :param bias: boolean. if True, bias is included.\n        :param w_init: str. weight inits with xavier initialization.\n        """"""\n        super(Conv, self).__init__()\n\n        self.conv = nn.Conv1d(in_channels, out_channels,\n                              kernel_size=kernel_size, stride=stride,\n                              padding=padding, dilation=dilation,\n                              bias=bias)\n\n        nn.init.xavier_uniform_(\n            self.conv.weight, gain=nn.init.calculate_gain(w_init))\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\nclass EncoderPrenet(nn.Module):\n    """"""\n    Pre-network for Encoder consists of convolution networks.\n    """"""\n    def __init__(self, embedding_size, num_hidden):\n        super(EncoderPrenet, self).__init__()\n        self.embedding_size = embedding_size\n        self.embed = nn.Embedding(len(symbols), embedding_size, padding_idx=0)\n\n        self.conv1 = Conv(in_channels=embedding_size,\n                          out_channels=num_hidden,\n                          kernel_size=5,\n                          padding=int(np.floor(5 / 2)),\n                          w_init=\'relu\')\n        self.conv2 = Conv(in_channels=num_hidden,\n                          out_channels=num_hidden,\n                          kernel_size=5,\n                          padding=int(np.floor(5 / 2)),\n                          w_init=\'relu\')\n\n        self.conv3 = Conv(in_channels=num_hidden,\n                          out_channels=num_hidden,\n                          kernel_size=5,\n                          padding=int(np.floor(5 / 2)),\n                          w_init=\'relu\')\n\n        self.batch_norm1 = nn.BatchNorm1d(num_hidden)\n        self.batch_norm2 = nn.BatchNorm1d(num_hidden)\n        self.batch_norm3 = nn.BatchNorm1d(num_hidden)\n\n        self.dropout1 = nn.Dropout(p=0.2)\n        self.dropout2 = nn.Dropout(p=0.2)\n        self.dropout3 = nn.Dropout(p=0.2)\n        self.projection = Linear(num_hidden, num_hidden)\n\n    def forward(self, input_):\n        input_ = self.embed(input_) \n        input_ = input_.transpose(1, 2) \n        input_ = self.dropout1(t.relu(self.batch_norm1(self.conv1(input_)))) \n        input_ = self.dropout2(t.relu(self.batch_norm2(self.conv2(input_)))) \n        input_ = self.dropout3(t.relu(self.batch_norm3(self.conv3(input_)))) \n        input_ = input_.transpose(1, 2) \n        input_ = self.projection(input_) \n\n        return input_\n\n\nclass FFN(nn.Module):\n    """"""\n    Positionwise Feed-Forward Network\n    """"""\n    \n    def __init__(self, num_hidden):\n        """"""\n        :param num_hidden: dimension of hidden \n        """"""\n        super(FFN, self).__init__()\n        self.w_1 = Conv(num_hidden, num_hidden * 4, kernel_size=1, w_init=\'relu\')\n        self.w_2 = Conv(num_hidden * 4, num_hidden, kernel_size=1)\n        self.dropout = nn.Dropout(p=0.1)\n        self.layer_norm = nn.LayerNorm(num_hidden)\n\n    def forward(self, input_):\n        # FFN Network\n        x = input_.transpose(1, 2) \n        x = self.w_2(t.relu(self.w_1(x))) \n        x = x.transpose(1, 2) \n\n\n        # residual connection\n        x = x + input_ \n\n        # dropout\n        # x = self.dropout(x) \n\n        # layer normalization\n        x = self.layer_norm(x) \n\n        return x\n\n\nclass PostConvNet(nn.Module):\n    """"""\n    Post Convolutional Network (mel --> mel)\n    """"""\n    def __init__(self, num_hidden):\n        """"""\n        \n        :param num_hidden: dimension of hidden \n        """"""\n        super(PostConvNet, self).__init__()\n        self.conv1 = Conv(in_channels=hp.num_mels * hp.outputs_per_step,\n                          out_channels=num_hidden,\n                          kernel_size=5,\n                          padding=4,\n                          w_init=\'tanh\')\n        self.conv_list = clones(Conv(in_channels=num_hidden,\n                                     out_channels=num_hidden,\n                                     kernel_size=5,\n                                     padding=4,\n                                     w_init=\'tanh\'), 3)\n        self.conv2 = Conv(in_channels=num_hidden,\n                          out_channels=hp.num_mels * hp.outputs_per_step,\n                          kernel_size=5,\n                          padding=4)\n\n        self.batch_norm_list = clones(nn.BatchNorm1d(num_hidden), 3)\n        self.pre_batchnorm = nn.BatchNorm1d(num_hidden)\n\n        self.dropout1 = nn.Dropout(p=0.1)\n        self.dropout_list = nn.ModuleList([nn.Dropout(p=0.1) for _ in range(3)])\n\n    def forward(self, input_, mask=None):\n        # Causal Convolution (for auto-regressive)\n        input_ = self.dropout1(t.tanh(self.pre_batchnorm(self.conv1(input_)[:, :, :-4])))\n        for batch_norm, conv, dropout in zip(self.batch_norm_list, self.conv_list, self.dropout_list):\n            input_ = dropout(t.tanh(batch_norm(conv(input_)[:, :, :-4])))\n        input_ = self.conv2(input_)[:, :, :-4]\n        return input_\n\n\nclass MultiheadAttention(nn.Module):\n    """"""\n    Multihead attention mechanism (dot attention)\n    """"""\n    def __init__(self, num_hidden_k):\n        """"""\n        :param num_hidden_k: dimension of hidden \n        """"""\n        super(MultiheadAttention, self).__init__()\n\n        self.num_hidden_k = num_hidden_k\n        self.attn_dropout = nn.Dropout(p=0.1)\n\n    def forward(self, key, value, query, mask=None, query_mask=None):\n        # Get attention score\n        attn = t.bmm(query, key.transpose(1, 2))\n        attn = attn / math.sqrt(self.num_hidden_k)\n\n        # Masking to ignore padding (key side)\n        if mask is not None:\n            attn = attn.masked_fill(mask, -2 ** 32 + 1)\n            attn = t.softmax(attn, dim=-1)\n        else:\n            attn = t.softmax(attn, dim=-1)\n\n        # Masking to ignore padding (query side)\n        if query_mask is not None:\n            attn = attn * query_mask\n\n        # Dropout\n        # attn = self.attn_dropout(attn)\n        \n        # Get Context Vector\n        result = t.bmm(attn, value)\n\n        return result, attn\n\n\nclass Attention(nn.Module):\n    """"""\n    Attention Network\n    """"""\n    def __init__(self, num_hidden, h=4):\n        """"""\n        :param num_hidden: dimension of hidden\n        :param h: num of heads \n        """"""\n        super(Attention, self).__init__()\n\n        self.num_hidden = num_hidden\n        self.num_hidden_per_attn = num_hidden // h\n        self.h = h\n\n        self.key = Linear(num_hidden, num_hidden, bias=False)\n        self.value = Linear(num_hidden, num_hidden, bias=False)\n        self.query = Linear(num_hidden, num_hidden, bias=False)\n\n        self.multihead = MultiheadAttention(self.num_hidden_per_attn)\n\n        self.residual_dropout = nn.Dropout(p=0.1)\n\n        self.final_linear = Linear(num_hidden * 2, num_hidden)\n\n        self.layer_norm_1 = nn.LayerNorm(num_hidden)\n\n    def forward(self, memory, decoder_input, mask=None, query_mask=None):\n\n        batch_size = memory.size(0)\n        seq_k = memory.size(1)\n        seq_q = decoder_input.size(1)\n        \n        # Repeat masks h times\n        if query_mask is not None:\n            query_mask = query_mask.unsqueeze(-1).repeat(1, 1, seq_k)\n            query_mask = query_mask.repeat(self.h, 1, 1)\n        if mask is not None:\n            mask = mask.repeat(self.h, 1, 1)\n\n        # Make multihead\n        key = self.key(memory).view(batch_size, seq_k, self.h, self.num_hidden_per_attn)\n        value = self.value(memory).view(batch_size, seq_k, self.h, self.num_hidden_per_attn)\n        query = self.query(decoder_input).view(batch_size, seq_q, self.h, self.num_hidden_per_attn)\n\n        key = key.permute(2, 0, 1, 3).contiguous().view(-1, seq_k, self.num_hidden_per_attn)\n        value = value.permute(2, 0, 1, 3).contiguous().view(-1, seq_k, self.num_hidden_per_attn)\n        query = query.permute(2, 0, 1, 3).contiguous().view(-1, seq_q, self.num_hidden_per_attn)\n\n        # Get context vector\n        result, attns = self.multihead(key, value, query, mask=mask, query_mask=query_mask)\n\n        # Concatenate all multihead context vector\n        result = result.view(self.h, batch_size, seq_q, self.num_hidden_per_attn)\n        result = result.permute(1, 2, 0, 3).contiguous().view(batch_size, seq_q, -1)\n        \n        # Concatenate context vector with input (most important)\n        result = t.cat([decoder_input, result], dim=-1)\n        \n        # Final linear\n        result = self.final_linear(result)\n\n        # Residual dropout & connection\n        result = result + decoder_input\n\n        # result = self.residual_dropout(result)\n\n        # Layer normalization\n        result = self.layer_norm_1(result)\n\n        return result, attns\n    \n\nclass Prenet(nn.Module):\n    """"""\n    Prenet before passing through the network\n    """"""\n    def __init__(self, input_size, hidden_size, output_size, p=0.5):\n        """"""\n        :param input_size: dimension of input\n        :param hidden_size: dimension of hidden unit\n        :param output_size: dimension of output\n        """"""\n        super(Prenet, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.layer = nn.Sequential(OrderedDict([\n             (\'fc1\', Linear(self.input_size, self.hidden_size)),\n             (\'relu1\', nn.ReLU()),\n             (\'dropout1\', nn.Dropout(p)),\n             (\'fc2\', Linear(self.hidden_size, self.output_size)),\n             (\'relu2\', nn.ReLU()),\n             (\'dropout2\', nn.Dropout(p)),\n        ]))\n\n    def forward(self, input_):\n\n        out = self.layer(input_)\n\n        return out\n    \nclass CBHG(nn.Module):\n    """"""\n    CBHG Module\n    """"""\n    def __init__(self, hidden_size, K=16, projection_size = 256, num_gru_layers=2, max_pool_kernel_size=2, is_post=False):\n        """"""\n        :param hidden_size: dimension of hidden unit\n        :param K: # of convolution banks\n        :param projection_size: dimension of projection unit\n        :param num_gru_layers: # of layers of GRUcell\n        :param max_pool_kernel_size: max pooling kernel size\n        :param is_post: whether post processing or not\n        """"""\n        super(CBHG, self).__init__()\n        self.hidden_size = hidden_size\n        self.projection_size = projection_size\n        self.convbank_list = nn.ModuleList()\n        self.convbank_list.append(nn.Conv1d(in_channels=projection_size,\n                                                out_channels=hidden_size,\n                                                kernel_size=1,\n                                                padding=int(np.floor(1/2))))\n\n        for i in range(2, K+1):\n            self.convbank_list.append(nn.Conv1d(in_channels=hidden_size,\n                                                out_channels=hidden_size,\n                                                kernel_size=i,\n                                                padding=int(np.floor(i/2))))\n\n        self.batchnorm_list = nn.ModuleList()\n        for i in range(1, K+1):\n            self.batchnorm_list.append(nn.BatchNorm1d(hidden_size))\n\n        convbank_outdim = hidden_size * K\n        \n        self.conv_projection_1 = nn.Conv1d(in_channels=convbank_outdim,\n                                             out_channels=hidden_size,\n                                             kernel_size=3,\n                                             padding=int(np.floor(3 / 2)))\n        self.conv_projection_2 = nn.Conv1d(in_channels=hidden_size,\n                                               out_channels=projection_size,\n                                               kernel_size=3,\n                                               padding=int(np.floor(3 / 2)))\n        self.batchnorm_proj_1 = nn.BatchNorm1d(hidden_size)\n\n        self.batchnorm_proj_2 = nn.BatchNorm1d(projection_size)\n\n\n        self.max_pool = nn.MaxPool1d(max_pool_kernel_size, stride=1, padding=1)\n        self.highway = Highwaynet(self.projection_size)\n        self.gru = nn.GRU(self.projection_size, self.hidden_size // 2, num_layers=num_gru_layers,\n                          batch_first=True,\n                          bidirectional=True)\n\n\n    def _conv_fit_dim(self, x, kernel_size=3):\n        if kernel_size % 2 == 0:\n            return x[:,:,:-1]\n        else:\n            return x\n\n    def forward(self, input_):\n\n        input_ = input_.contiguous()\n        batch_size = input_.size(0)\n        total_length = input_.size(-1)\n\n        convbank_list = list()\n        convbank_input = input_\n\n        # Convolution bank filters\n        for k, (conv, batchnorm) in enumerate(zip(self.convbank_list, self.batchnorm_list)):\n            convbank_input = t.relu(batchnorm(self._conv_fit_dim(conv(convbank_input), k+1).contiguous()))\n            convbank_list.append(convbank_input)\n\n        # Concatenate all features\n        conv_cat = t.cat(convbank_list, dim=1)\n\n        # Max pooling\n        conv_cat = self.max_pool(conv_cat)[:,:,:-1]\n\n        # Projection\n        conv_projection = t.relu(self.batchnorm_proj_1(self._conv_fit_dim(self.conv_projection_1(conv_cat))))\n        conv_projection = self.batchnorm_proj_2(self._conv_fit_dim(self.conv_projection_2(conv_projection))) + input_\n\n        # Highway networks\n        highway = self.highway.forward(conv_projection.transpose(1,2))\n        \n\n        # Bidirectional GRU\n        \n        self.gru.flatten_parameters()\n        out, _ = self.gru(highway)\n\n        return out\n\n\nclass Highwaynet(nn.Module):\n    """"""\n    Highway network\n    """"""\n    def __init__(self, num_units, num_layers=4):\n        """"""\n        :param num_units: dimension of hidden unit\n        :param num_layers: # of highway layers\n        """"""\n        super(Highwaynet, self).__init__()\n        self.num_units = num_units\n        self.num_layers = num_layers\n        self.gates = nn.ModuleList()\n        self.linears = nn.ModuleList()\n        for _ in range(self.num_layers):\n            self.linears.append(Linear(num_units, num_units))\n            self.gates.append(Linear(num_units, num_units))\n\n    def forward(self, input_):\n\n        out = input_\n\n        # highway gated function\n        for fc1, fc2 in zip(self.linears, self.gates):\n\n            h = t.relu(fc1.forward(out))\n            t_ = t.sigmoid(fc2.forward(out))\n\n            c = 1. - t_\n            out = h * t_ + out * c\n\n        return out\n'"
network.py,0,"b'from module import *\nfrom utils import get_positional_table, get_sinusoid_encoding_table\nimport hyperparams as hp\nimport copy\n\nclass Encoder(nn.Module):\n    """"""\n    Encoder Network\n    """"""\n    def __init__(self, embedding_size, num_hidden):\n        """"""\n        :param embedding_size: dimension of embedding\n        :param num_hidden: dimension of hidden\n        """"""\n        super(Encoder, self).__init__()\n        self.alpha = nn.Parameter(t.ones(1))\n        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(1024, num_hidden, padding_idx=0),\n                                                    freeze=True)\n        self.pos_dropout = nn.Dropout(p=0.1)\n        self.encoder_prenet = EncoderPrenet(embedding_size, num_hidden)\n        self.layers = clones(Attention(num_hidden), 3)\n        self.ffns = clones(FFN(num_hidden), 3)\n\n    def forward(self, x, pos):\n\n        # Get character mask\n        if self.training:\n            c_mask = pos.ne(0).type(t.float)\n            mask = pos.eq(0).unsqueeze(1).repeat(1, x.size(1), 1)\n\n        else:\n            c_mask, mask = None, None\n\n        # Encoder pre-network\n        x = self.encoder_prenet(x)\n\n        # Get positional embedding, apply alpha and add\n        pos = self.pos_emb(pos)\n        x = pos * self.alpha + x\n\n        # Positional dropout\n        x = self.pos_dropout(x)\n\n        # Attention encoder-encoder\n        attns = list()\n        for layer, ffn in zip(self.layers, self.ffns):\n            x, attn = layer(x, x, mask=mask, query_mask=c_mask)\n            x = ffn(x)\n            attns.append(attn)\n\n        return x, c_mask, attns\n\n\nclass MelDecoder(nn.Module):\n    """"""\n    Decoder Network\n    """"""\n    def __init__(self, num_hidden):\n        """"""\n        :param num_hidden: dimension of hidden\n        """"""\n        super(MelDecoder, self).__init__()\n        self.pos_emb = nn.Embedding.from_pretrained(get_sinusoid_encoding_table(1024, num_hidden, padding_idx=0),\n                                                    freeze=True)\n        self.pos_dropout = nn.Dropout(p=0.1)\n        self.alpha = nn.Parameter(t.ones(1))\n        self.decoder_prenet = Prenet(hp.num_mels, num_hidden * 2, num_hidden, p=0.2)\n        self.norm = Linear(num_hidden, num_hidden)\n\n        self.selfattn_layers = clones(Attention(num_hidden), 3)\n        self.dotattn_layers = clones(Attention(num_hidden), 3)\n        self.ffns = clones(FFN(num_hidden), 3)\n        self.mel_linear = Linear(num_hidden, hp.num_mels * hp.outputs_per_step)\n        self.stop_linear = Linear(num_hidden, 1, w_init=\'sigmoid\')\n\n        self.postconvnet = PostConvNet(num_hidden)\n\n    def forward(self, memory, decoder_input, c_mask, pos):\n        batch_size = memory.size(0)\n        decoder_len = decoder_input.size(1)\n\n        # get decoder mask with triangular matrix\n        if self.training:\n            m_mask = pos.ne(0).type(t.float)\n            mask = m_mask.eq(0).unsqueeze(1).repeat(1, decoder_len, 1)\n            if next(self.parameters()).is_cuda:\n                mask = mask + t.triu(t.ones(decoder_len, decoder_len).cuda(), diagonal=1).repeat(batch_size, 1, 1).byte()\n            else:\n                mask = mask + t.triu(t.ones(decoder_len, decoder_len), diagonal=1).repeat(batch_size, 1, 1).byte()\n            mask = mask.gt(0)\n            zero_mask = c_mask.eq(0).unsqueeze(-1).repeat(1, 1, decoder_len)\n            zero_mask = zero_mask.transpose(1, 2)\n        else:\n            if next(self.parameters()).is_cuda:\n                mask = t.triu(t.ones(decoder_len, decoder_len).cuda(), diagonal=1).repeat(batch_size, 1, 1).byte()\n            else:\n                mask = t.triu(t.ones(decoder_len, decoder_len), diagonal=1).repeat(batch_size, 1, 1).byte()\n            mask = mask.gt(0)\n            m_mask, zero_mask = None, None\n\n        # Decoder pre-network\n        decoder_input = self.decoder_prenet(decoder_input)\n\n        # Centered position\n        decoder_input = self.norm(decoder_input)\n\n        # Get positional embedding, apply alpha and add\n        pos = self.pos_emb(pos)\n        decoder_input = pos * self.alpha + decoder_input\n\n        # Positional dropout\n        decoder_input = self.pos_dropout(decoder_input)\n\n        # Attention decoder-decoder, encoder-decoder\n        attn_dot_list = list()\n        attn_dec_list = list()\n\n        for selfattn, dotattn, ffn in zip(self.selfattn_layers, self.dotattn_layers, self.ffns):\n            decoder_input, attn_dec = selfattn(decoder_input, decoder_input, mask=mask, query_mask=m_mask)\n            decoder_input, attn_dot = dotattn(memory, decoder_input, mask=zero_mask, query_mask=m_mask)\n            decoder_input = ffn(decoder_input)\n            attn_dot_list.append(attn_dot)\n            attn_dec_list.append(attn_dec)\n\n        # Mel linear projection\n        mel_out = self.mel_linear(decoder_input)\n        \n        # Post Mel Network\n        postnet_input = mel_out.transpose(1, 2)\n        out = self.postconvnet(postnet_input)\n        out = postnet_input + out\n        out = out.transpose(1, 2)\n\n        # Stop tokens\n        stop_tokens = self.stop_linear(decoder_input)\n\n        return mel_out, out, attn_dot_list, stop_tokens, attn_dec_list\n\n\nclass Model(nn.Module):\n    """"""\n    Transformer Network\n    """"""\n    def __init__(self):\n        super(Model, self).__init__()\n        self.encoder = Encoder(hp.embedding_size, hp.hidden_size)\n        self.decoder = MelDecoder(hp.hidden_size)\n\n    def forward(self, characters, mel_input, pos_text, pos_mel):\n        memory, c_mask, attns_enc = self.encoder.forward(characters, pos=pos_text)\n        mel_output, postnet_output, attn_probs, stop_preds, attns_dec = self.decoder.forward(memory, mel_input, c_mask,\n                                                                                             pos=pos_mel)\n\n        return mel_output, postnet_output, attn_probs, stop_preds, attns_enc, attns_dec\n\n\nclass ModelPostNet(nn.Module):\n    """"""\n    CBHG Network (mel --> linear)\n    """"""\n    def __init__(self):\n        super(ModelPostNet, self).__init__()\n        self.pre_projection = Conv(hp.n_mels, hp.hidden_size)\n        self.cbhg = CBHG(hp.hidden_size)\n        self.post_projection = Conv(hp.hidden_size, (hp.n_fft // 2) + 1)\n\n    def forward(self, mel):\n        mel = mel.transpose(1, 2)\n        mel = self.pre_projection(mel)\n        mel = self.cbhg(mel).transpose(1, 2)\n        mag_pred = self.post_projection(mel).transpose(1, 2)\n\n        return mag_pred'"
prepare_data.py,1,"b'import numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nfrom utils import get_spectrograms\nimport hyperparams as hp\nimport librosa\n\nclass PrepareDataset(Dataset):\n    """"""LJSpeech dataset.""""""\n\n    def __init__(self, csv_file, root_dir):\n        """"""\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the wavs.\n\n        """"""\n        self.landmarks_frame = pd.read_csv(csv_file, sep=\'|\', header=None)\n        self.root_dir = root_dir\n\n    def load_wav(self, filename):\n        return librosa.load(filename, sr=hp.sample_rate)\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        wav_name = os.path.join(self.root_dir, self.landmarks_frame.ix[idx, 0]) + \'.wav\'\n        mel, mag = get_spectrograms(wav_name)\n        \n        np.save(wav_name[:-4] + \'.pt\', mel)\n        np.save(wav_name[:-4] + \'.mag\', mag)\n\n        sample = {\'mel\':mel, \'mag\': mag}\n\n        return sample\n    \nif __name__ == \'__main__\':\n    dataset = PrepareDataset(os.path.join(hp.data_path,\'metadata.csv\'), os.path.join(hp.data_path,\'wavs\'))\n    dataloader = DataLoader(dataset, batch_size=1, drop_last=False, num_workers=8)\n    from tqdm import tqdm\n    pbar = tqdm(dataloader)\n    for d in pbar:\n        pass\n'"
preprocess.py,1,"b'import hyperparams as hp\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport librosa\nimport numpy as np\nfrom text import text_to_sequence\nimport collections\nfrom scipy import signal\nimport torch as t\nimport math\n\n\nclass LJDatasets(Dataset):\n    """"""LJSpeech dataset.""""""\n\n    def __init__(self, csv_file, root_dir):\n        """"""\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the wavs.\n\n        """"""\n        self.landmarks_frame = pd.read_csv(csv_file, sep=\'|\', header=None)\n        self.root_dir = root_dir\n\n    def load_wav(self, filename):\n        return librosa.load(filename, sr=hp.sample_rate)\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        wav_name = os.path.join(self.root_dir, self.landmarks_frame.ix[idx, 0]) + \'.wav\'\n        text = self.landmarks_frame.ix[idx, 1]\n\n        text = np.asarray(text_to_sequence(text, [hp.cleaners]), dtype=np.int32)\n        mel = np.load(wav_name[:-4] + \'.pt.npy\')\n        mel_input = np.concatenate([np.zeros([1,hp.num_mels], np.float32), mel[:-1,:]], axis=0)\n        text_length = len(text)\n        pos_text = np.arange(1, text_length + 1)\n        pos_mel = np.arange(1, mel.shape[0] + 1)\n\n        sample = {\'text\': text, \'mel\': mel, \'text_length\':text_length, \'mel_input\':mel_input, \'pos_mel\':pos_mel, \'pos_text\':pos_text}\n\n        return sample\n    \nclass PostDatasets(Dataset):\n    """"""LJSpeech dataset.""""""\n\n    def __init__(self, csv_file, root_dir):\n        """"""\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the wavs.\n\n        """"""\n        self.landmarks_frame = pd.read_csv(csv_file, sep=\'|\', header=None)\n        self.root_dir = root_dir\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        wav_name = os.path.join(self.root_dir, self.landmarks_frame.ix[idx, 0]) + \'.wav\'\n        mel = np.load(wav_name[:-4] + \'.pt.npy\')\n        mag = np.load(wav_name[:-4] + \'.mag.npy\')\n        sample = {\'mel\':mel, \'mag\':mag}\n\n        return sample\n    \ndef collate_fn_transformer(batch):\n\n    # Puts each data field into a tensor with outer dimension batch size\n    if isinstance(batch[0], collections.Mapping):\n\n        text = [d[\'text\'] for d in batch]\n        mel = [d[\'mel\'] for d in batch]\n        mel_input = [d[\'mel_input\'] for d in batch]\n        text_length = [d[\'text_length\'] for d in batch]\n        pos_mel = [d[\'pos_mel\'] for d in batch]\n        pos_text= [d[\'pos_text\'] for d in batch]\n        \n        text = [i for i,_ in sorted(zip(text, text_length), key=lambda x: x[1], reverse=True)]\n        mel = [i for i, _ in sorted(zip(mel, text_length), key=lambda x: x[1], reverse=True)]\n        mel_input = [i for i, _ in sorted(zip(mel_input, text_length), key=lambda x: x[1], reverse=True)]\n        pos_text = [i for i, _ in sorted(zip(pos_text, text_length), key=lambda x: x[1], reverse=True)]\n        pos_mel = [i for i, _ in sorted(zip(pos_mel, text_length), key=lambda x: x[1], reverse=True)]\n        text_length = sorted(text_length, reverse=True)\n        # PAD sequences with largest length of the batch\n        text = _prepare_data(text).astype(np.int32)\n        mel = _pad_mel(mel)\n        mel_input = _pad_mel(mel_input)\n        pos_mel = _prepare_data(pos_mel).astype(np.int32)\n        pos_text = _prepare_data(pos_text).astype(np.int32)\n\n\n        return t.LongTensor(text), t.FloatTensor(mel), t.FloatTensor(mel_input), t.LongTensor(pos_text), t.LongTensor(pos_mel), t.LongTensor(text_length)\n\n    raise TypeError((""batch must contain tensors, numbers, dicts or lists; found {}""\n                     .format(type(batch[0]))))\n    \ndef collate_fn_postnet(batch):\n\n    # Puts each data field into a tensor with outer dimension batch size\n    if isinstance(batch[0], collections.Mapping):\n\n        mel = [d[\'mel\'] for d in batch]\n        mag = [d[\'mag\'] for d in batch]\n        \n        # PAD sequences with largest length of the batch\n        mel = _pad_mel(mel)\n        mag = _pad_mel(mag)\n\n        return t.FloatTensor(mel), t.FloatTensor(mag)\n\n    raise TypeError((""batch must contain tensors, numbers, dicts or lists; found {}""\n                     .format(type(batch[0]))))\n\ndef _pad_data(x, length):\n    _pad = 0\n    return np.pad(x, (0, length - x.shape[0]), mode=\'constant\', constant_values=_pad)\n\ndef _prepare_data(inputs):\n    max_len = max((len(x) for x in inputs))\n    return np.stack([_pad_data(x, max_len) for x in inputs])\n\ndef _pad_per_step(inputs):\n    timesteps = inputs.shape[-1]\n    return np.pad(inputs, [[0,0],[0,0],[0, hp.outputs_per_step - (timesteps % hp.outputs_per_step)]], mode=\'constant\', constant_values=0.0)\n\ndef get_param_size(model):\n    params = 0\n    for p in model.parameters():\n        tmp = 1\n        for x in p.size():\n            tmp *= x\n        params += tmp\n    return params\n\ndef get_dataset():\n    return LJDatasets(os.path.join(hp.data_path,\'metadata.csv\'), os.path.join(hp.data_path,\'wavs\'))\n\ndef get_post_dataset():\n    return PostDatasets(os.path.join(hp.data_path,\'metadata.csv\'), os.path.join(hp.data_path,\'wavs\'))\n\ndef _pad_mel(inputs):\n    _pad = 0\n    def _pad_one(x, max_len):\n        mel_len = x.shape[0]\n        return np.pad(x, [[0,max_len - mel_len],[0,0]], mode=\'constant\', constant_values=_pad)\n    max_len = max((x.shape[0] for x in inputs))\n    return np.stack([_pad_one(x, max_len) for x in inputs])\n\n'"
synthesis.py,0,"b'import torch as t\nfrom utils import spectrogram2wav\nfrom scipy.io.wavfile import write\nimport hyperparams as hp\nfrom text import text_to_sequence\nimport numpy as np\nfrom network import ModelPostNet, Model\nfrom collections import OrderedDict\nfrom tqdm import tqdm\nimport argparse\n\ndef load_checkpoint(step, model_name=""transformer""):\n    state_dict = t.load(\'./checkpoint/checkpoint_%s_%d.pth.tar\'% (model_name, step))   \n    new_state_dict = OrderedDict()\n    for k, value in state_dict[\'model\'].items():\n        key = k[7:]\n        new_state_dict[key] = value\n\n    return new_state_dict\n\ndef synthesis(text, args):\n    m = Model()\n    m_post = ModelPostNet()\n\n    m.load_state_dict(load_checkpoint(args.restore_step1, ""transformer""))\n    m_post.load_state_dict(load_checkpoint(args.restore_step2, ""postnet""))\n\n    text = np.asarray(text_to_sequence(text, [hp.cleaners]))\n    text = t.LongTensor(text).unsqueeze(0)\n    text = text.cuda()\n    mel_input = t.zeros([1,1, 80]).cuda()\n    pos_text = t.arange(1, text.size(1)+1).unsqueeze(0)\n    pos_text = pos_text.cuda()\n\n    m=m.cuda()\n    m_post = m_post.cuda()\n    m.train(False)\n    m_post.train(False)\n    \n    pbar = tqdm(range(args.max_len))\n    with t.no_grad():\n        for i in pbar:\n            pos_mel = t.arange(1,mel_input.size(1)+1).unsqueeze(0).cuda()\n            mel_pred, postnet_pred, attn, stop_token, _, attn_dec = m.forward(text, mel_input, pos_text, pos_mel)\n            mel_input = t.cat([mel_input, postnet_pred[:,-1:,:]], dim=1)\n\n        mag_pred = m_post.forward(postnet_pred)\n        \n    wav = spectrogram2wav(mag_pred.squeeze(0).cpu().numpy())\n    write(hp.sample_path + ""/test.wav"", hp.sr, wav)\n    \nif __name__ == \'__main__\':\n    \n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--restore_step1\', type=int, help=\'Global step to restore checkpoint\', default=172000)\n    parser.add_argument(\'--restore_step2\', type=int, help=\'Global step to restore checkpoint\', default=100000)\n    parser.add_argument(\'--max_len\', type=int, help=\'Global step to restore checkpoint\', default=400)\n\n    args = parser.parse_args()\n    synthesis(""Transformer model is so fast!"",args)\n'"
train_postnet.py,0,"b'from preprocess import get_post_dataset, DataLoader, collate_fn_postnet\nfrom network import *\nfrom tensorboardX import SummaryWriter\nimport torchvision.utils as vutils\nimport os\nfrom tqdm import tqdm\n\ndef adjust_learning_rate(optimizer, step_num, warmup_step=4000):\n    lr = hp.lr * warmup_step**0.5 * min(step_num * warmup_step**-1.5, step_num**-0.5)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n        \ndef main():\n\n    dataset = get_post_dataset()\n    global_step = 0\n    \n    m = nn.DataParallel(ModelPostNet().cuda())\n\n    m.train()\n    optimizer = t.optim.Adam(m.parameters(), lr=hp.lr)\n\n    writer = SummaryWriter()\n\n    for epoch in range(hp.epochs):\n\n        dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_postnet, drop_last=True, num_workers=8)\n        pbar = tqdm(dataloader)\n        for i, data in enumerate(pbar):\n            pbar.set_description(""Processing at epoch %d""%epoch)\n            global_step += 1\n            if global_step < 400000:\n                adjust_learning_rate(optimizer, global_step)\n                \n            mel, mag = data\n        \n            mel = mel.cuda()\n            mag = mag.cuda()\n            \n            mag_pred = m.forward(mel)\n\n            loss = nn.L1Loss()(mag_pred, mag)\n            \n            writer.add_scalars(\'training_loss\',{\n                    \'loss\':loss,\n\n                }, global_step)\n                    \n            optimizer.zero_grad()\n            # Calculate gradients\n            loss.backward()\n            \n            nn.utils.clip_grad_norm_(m.parameters(), 1.)\n            \n            # Update weights\n            optimizer.step()\n\n            if global_step % hp.save_step == 0:\n                t.save({\'model\':m.state_dict(),\n                                 \'optimizer\':optimizer.state_dict()},\n                                os.path.join(hp.checkpoint_path,\'checkpoint_postnet_%d.pth.tar\' % global_step))\n\n            \n            \n\n\nif __name__ == \'__main__\':\n    main()'"
train_transformer.py,0,"b'from preprocess import get_dataset, DataLoader, collate_fn_transformer\nfrom network import *\nfrom tensorboardX import SummaryWriter\nimport torchvision.utils as vutils\nimport os\nfrom tqdm import tqdm\n\ndef adjust_learning_rate(optimizer, step_num, warmup_step=4000):\n    lr = hp.lr * warmup_step**0.5 * min(step_num * warmup_step**-1.5, step_num**-0.5)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n        \ndef main():\n\n    dataset = get_dataset()\n    global_step = 0\n    \n    m = nn.DataParallel(Model().cuda())\n\n    m.train()\n    optimizer = t.optim.Adam(m.parameters(), lr=hp.lr)\n\n    pos_weight = t.FloatTensor([5.]).cuda()\n    writer = SummaryWriter()\n    \n    for epoch in range(hp.epochs):\n\n        dataloader = DataLoader(dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=collate_fn_transformer, drop_last=True, num_workers=16)\n        pbar = tqdm(dataloader)\n        for i, data in enumerate(pbar):\n            pbar.set_description(""Processing at epoch %d""%epoch)\n            global_step += 1\n            if global_step < 400000:\n                adjust_learning_rate(optimizer, global_step)\n                \n            character, mel, mel_input, pos_text, pos_mel, _ = data\n            \n            stop_tokens = t.abs(pos_mel.ne(0).type(t.float) - 1)\n            \n            character = character.cuda()\n            mel = mel.cuda()\n            mel_input = mel_input.cuda()\n            pos_text = pos_text.cuda()\n            pos_mel = pos_mel.cuda()\n            \n            mel_pred, postnet_pred, attn_probs, stop_preds, attns_enc, attns_dec = m.forward(character, mel_input, pos_text, pos_mel)\n\n            mel_loss = nn.L1Loss()(mel_pred, mel)\n            post_mel_loss = nn.L1Loss()(postnet_pred, mel)\n            \n            loss = mel_loss + post_mel_loss\n            \n            writer.add_scalars(\'training_loss\',{\n                    \'mel_loss\':mel_loss,\n                    \'post_mel_loss\':post_mel_loss,\n\n                }, global_step)\n                \n            writer.add_scalars(\'alphas\',{\n                    \'encoder_alpha\':m.module.encoder.alpha.data,\n                    \'decoder_alpha\':m.module.decoder.alpha.data,\n                }, global_step)\n            \n            \n            if global_step % hp.image_step == 1:\n                \n                for i, prob in enumerate(attn_probs):\n                    \n                    num_h = prob.size(0)\n                    for j in range(4):\n                \n                        x = vutils.make_grid(prob[j*16] * 255)\n                        writer.add_image(\'Attention_%d_0\'%global_step, x, i*4+j)\n                \n                for i, prob in enumerate(attns_enc):\n                    num_h = prob.size(0)\n                    \n                    for j in range(4):\n                \n                        x = vutils.make_grid(prob[j*16] * 255)\n                        writer.add_image(\'Attention_enc_%d_0\'%global_step, x, i*4+j)\n            \n                for i, prob in enumerate(attns_dec):\n\n                    num_h = prob.size(0)\n                    for j in range(4):\n                \n                        x = vutils.make_grid(prob[j*16] * 255)\n                        writer.add_image(\'Attention_dec_%d_0\'%global_step, x, i*4+j)\n                \n            optimizer.zero_grad()\n            # Calculate gradients\n            loss.backward()\n            \n            nn.utils.clip_grad_norm_(m.parameters(), 1.)\n            \n            # Update weights\n            optimizer.step()\n\n            if global_step % hp.save_step == 0:\n                t.save({\'model\':m.state_dict(),\n                                 \'optimizer\':optimizer.state_dict()},\n                                os.path.join(hp.checkpoint_path,\'checkpoint_transformer_%d.pth.tar\' % global_step))\n\n            \n            \n\n\nif __name__ == \'__main__\':\n    main()'"
utils.py,0,"b'import numpy as np\nimport librosa\nimport os, copy\nfrom scipy import signal\nimport hyperparams as hp\nimport torch as t\n\ndef get_spectrograms(fpath):\n    \'\'\'Parse the wave file in `fpath` and\n    Returns normalized melspectrogram and linear spectrogram.\n    Args:\n      fpath: A string. The full path of a sound file.\n    Returns:\n      mel: A 2d array of shape (T, n_mels) and dtype of float32.\n      mag: A 2d array of shape (T, 1+n_fft/2) and dtype of float32.\n    \'\'\'\n    # Loading sound file\n    y, sr = librosa.load(fpath, sr=hp.sr)\n\n    # Trimming\n    y, _ = librosa.effects.trim(y)\n\n    # Preemphasis\n    y = np.append(y[0], y[1:] - hp.preemphasis * y[:-1])\n\n    # stft\n    linear = librosa.stft(y=y,\n                          n_fft=hp.n_fft,\n                          hop_length=hp.hop_length,\n                          win_length=hp.win_length)\n\n    # magnitude spectrogram\n    mag = np.abs(linear)  # (1+n_fft//2, T)\n\n    # mel spectrogram\n    mel_basis = librosa.filters.mel(hp.sr, hp.n_fft, hp.n_mels)  # (n_mels, 1+n_fft//2)\n    mel = np.dot(mel_basis, mag)  # (n_mels, t)\n\n    # to decibel\n    mel = 20 * np.log10(np.maximum(1e-5, mel))\n    mag = 20 * np.log10(np.maximum(1e-5, mag))\n\n    # normalize\n    mel = np.clip((mel - hp.ref_db + hp.max_db) / hp.max_db, 1e-8, 1)\n    mag = np.clip((mag - hp.ref_db + hp.max_db) / hp.max_db, 1e-8, 1)\n\n    # Transpose\n    mel = mel.T.astype(np.float32)  # (T, n_mels)\n    mag = mag.T.astype(np.float32)  # (T, 1+n_fft//2)\n\n    return mel, mag\n\ndef spectrogram2wav(mag):\n    \'\'\'# Generate wave file from linear magnitude spectrogram\n    Args:\n      mag: A numpy array of (T, 1+n_fft//2)\n    Returns:\n      wav: A 1-D numpy array.\n    \'\'\'\n    # transpose\n    mag = mag.T\n\n    # de-noramlize\n    mag = (np.clip(mag, 0, 1) * hp.max_db) - hp.max_db + hp.ref_db\n\n    # to amplitude\n    mag = np.power(10.0, mag * 0.05)\n\n    # wav reconstruction\n    wav = griffin_lim(mag**hp.power)\n\n    # de-preemphasis\n    wav = signal.lfilter([1], [1, -hp.preemphasis], wav)\n\n    # trim\n    wav, _ = librosa.effects.trim(wav)\n\n    return wav.astype(np.float32)\n\ndef griffin_lim(spectrogram):\n    \'\'\'Applies Griffin-Lim\'s raw.\'\'\'\n    X_best = copy.deepcopy(spectrogram)\n    for i in range(hp.n_iter):\n        X_t = invert_spectrogram(X_best)\n        est = librosa.stft(X_t, hp.n_fft, hp.hop_length, win_length=hp.win_length)\n        phase = est / np.maximum(1e-8, np.abs(est))\n        X_best = spectrogram * phase\n    X_t = invert_spectrogram(X_best)\n    y = np.real(X_t)\n\n    return y\n\ndef invert_spectrogram(spectrogram):\n    \'\'\'Applies inverse fft.\n    Args:\n      spectrogram: [1+n_fft//2, t]\n    \'\'\'\n    return librosa.istft(spectrogram, hp.hop_length, win_length=hp.win_length, window=""hann"")\n\ndef get_positional_table(d_pos_vec, n_position=1024):\n    position_enc = np.array([\n        [pos / np.power(10000, 2*i/d_pos_vec) for i in range(d_pos_vec)]\n        if pos != 0 else np.zeros(d_pos_vec) for pos in range(n_position)])\n\n    position_enc[1:, 0::2] = np.sin(position_enc[1:, 0::2]) # dim 2i\n    position_enc[1:, 1::2] = np.cos(position_enc[1:, 1::2]) # dim 2i+1\n    return t.from_numpy(position_enc).type(t.FloatTensor)\n\ndef get_sinusoid_encoding_table(n_position, d_hid, padding_idx=None):\n    \'\'\' Sinusoid position encoding table \'\'\'\n\n    def cal_angle(position, hid_idx):\n        return position / np.power(10000, 2 * (hid_idx // 2) / d_hid)\n\n    def get_posi_angle_vec(position):\n        return [cal_angle(position, hid_j) for hid_j in range(d_hid)]\n\n    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n\n    if padding_idx is not None:\n        # zero vector for padding dimension\n        sinusoid_table[padding_idx] = 0.\n\n    return t.FloatTensor(sinusoid_table)\n\ndef guided_attention(N, T, g=0.2):\n    \'\'\'Guided attention. Refer to page 3 on the paper.\'\'\'\n    W = np.zeros((N, T), dtype=np.float32)\n    for n_pos in range(W.shape[0]):\n        for t_pos in range(W.shape[1]):\n            W[n_pos, t_pos] = 1 - np.exp(-(t_pos / float(T) - n_pos / float(N)) ** 2 / (2 * g * g))\n    return W\n'"
text/__init__.py,0,"b'#-*- coding: utf-8 -*-\n\nimport re\nfrom text import cleaners\nfrom text.symbols import symbols\n\n\n\n\n# Mappings from symbol to numeric ID and vice versa:\n_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n\n# Regular expression matching text enclosed in curly braces:\n_curly_re = re.compile(r\'(.*?)\\{(.+?)\\}(.*)\')\n\n\ndef text_to_sequence(text, cleaner_names):\n  \'\'\'Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n\n    The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n    in it. For example, ""Turn left on {HH AW1 S S T AH0 N} Street.""\n\n    Args:\n      text: string to convert to a sequence\n      cleaner_names: names of the cleaner functions to run the text through\n\n    Returns:\n      List of integers corresponding to the symbols in the text\n  \'\'\'\n  sequence = []\n\n  # Check for curly braces and treat their contents as ARPAbet:\n  while len(text):\n    m = _curly_re.match(text)\n    if not m:\n      sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n      break\n    sequence += _symbols_to_sequence(_clean_text(m.group(1), cleaner_names))\n    sequence += _arpabet_to_sequence(m.group(2))\n    text = m.group(3)\n\n  # Append EOS token\n  sequence.append(_symbol_to_id[\'~\'])\n  return sequence\n\n\ndef sequence_to_text(sequence):\n  \'\'\'Converts a sequence of IDs back to a string\'\'\'\n  result = \'\'\n  for symbol_id in sequence:\n    if symbol_id in _id_to_symbol:\n      s = _id_to_symbol[symbol_id]\n      # Enclose ARPAbet back in curly braces:\n      if len(s) > 1 and s[0] == \'@\':\n        s = \'{%s}\' % s[1:]\n      result += s\n  return result.replace(\'}{\', \' \')\n\n\ndef _clean_text(text, cleaner_names):\n  for name in cleaner_names:\n    cleaner = getattr(cleaners, name)\n    if not cleaner:\n      raise Exception(\'Unknown cleaner: %s\' % name)\n    text = cleaner(text)\n  return text\n\n\ndef _symbols_to_sequence(symbols):\n  return [_symbol_to_id[s] for s in symbols if _should_keep_symbol(s)]\n\n\ndef _arpabet_to_sequence(text):\n  return _symbols_to_sequence([\'@\' + s for s in text.split()])\n\n\ndef _should_keep_symbol(s):\n  return s in _symbol_to_id and s is not \'_\' and s is not \'~\''"
text/cleaners.py,0,"b'#-*- coding: utf-8 -*-\n\n\n\'\'\'\nCleaners are transformations that run over the input text at both training and eval time.\n\nCleaners can be selected by passing a comma-delimited list of cleaner names as the ""cleaners""\nhyperparameter. Some cleaners are English-specific. You\'ll typically want to use:\n  1. ""english_cleaners"" for English text\n  2. ""transliteration_cleaners"" for non-English text that can be transliterated to ASCII using\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n  3. ""basic_cleaners"" if you do not want to transliterate (in this case, you should also update\n     the symbols in symbols.py to match your data).\n\'\'\'\n\nimport re\nfrom unidecode import unidecode\nfrom .numbers import normalize_numbers\n\n\n# Regular expression matching whitespace:\n_whitespace_re = re.compile(r\'\\s+\')\n\n# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = [(re.compile(\'\\\\b%s\\\\.\' % x[0], re.IGNORECASE), x[1]) for x in [\n  (\'mrs\', \'misess\'),\n  (\'mr\', \'mister\'),\n  (\'dr\', \'doctor\'),\n  (\'st\', \'saint\'),\n  (\'co\', \'company\'),\n  (\'jr\', \'junior\'),\n  (\'maj\', \'major\'),\n  (\'gen\', \'general\'),\n  (\'drs\', \'doctors\'),\n  (\'rev\', \'reverend\'),\n  (\'lt\', \'lieutenant\'),\n  (\'hon\', \'honorable\'),\n  (\'sgt\', \'sergeant\'),\n  (\'capt\', \'captain\'),\n  (\'esq\', \'esquire\'),\n  (\'ltd\', \'limited\'),\n  (\'col\', \'colonel\'),\n  (\'ft\', \'fort\'),\n]]\n\n\ndef expand_abbreviations(text):\n  for regex, replacement in _abbreviations:\n    text = re.sub(regex, replacement, text)\n  return text\n\n\ndef expand_numbers(text):\n  return normalize_numbers(text)\n\n\ndef lowercase(text):\n  return text.lower()\n\n\ndef collapse_whitespace(text):\n  return re.sub(_whitespace_re, \' \', text)\n\n\ndef convert_to_ascii(text):\n  return unidecode(text)\n\n\ndef basic_cleaners(text):\n  \'\'\'Basic pipeline that lowercases and collapses whitespace without transliteration.\'\'\'\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef transliteration_cleaners(text):\n  \'\'\'Pipeline for non-English text that transliterates to ASCII.\'\'\'\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = collapse_whitespace(text)\n  return text\n\n\ndef english_cleaners(text):\n  \'\'\'Pipeline for English text, including number and abbreviation expansion.\'\'\'\n  text = convert_to_ascii(text)\n  text = lowercase(text)\n  text = expand_numbers(text)\n  text = expand_abbreviations(text)\n  text = collapse_whitespace(text)\n  return text\n'"
text/cmudict.py,0,"b'#-*- coding: utf-8 -*-\n\n\nimport re\n\n\nvalid_symbols = [\n  \'AA\', \'AA0\', \'AA1\', \'AA2\', \'AE\', \'AE0\', \'AE1\', \'AE2\', \'AH\', \'AH0\', \'AH1\', \'AH2\',\n  \'AO\', \'AO0\', \'AO1\', \'AO2\', \'AW\', \'AW0\', \'AW1\', \'AW2\', \'AY\', \'AY0\', \'AY1\', \'AY2\',\n  \'B\', \'CH\', \'D\', \'DH\', \'EH\', \'EH0\', \'EH1\', \'EH2\', \'ER\', \'ER0\', \'ER1\', \'ER2\', \'EY\',\n  \'EY0\', \'EY1\', \'EY2\', \'F\', \'G\', \'HH\', \'IH\', \'IH0\', \'IH1\', \'IH2\', \'IY\', \'IY0\', \'IY1\',\n  \'IY2\', \'JH\', \'K\', \'L\', \'M\', \'N\', \'NG\', \'OW\', \'OW0\', \'OW1\', \'OW2\', \'OY\', \'OY0\',\n  \'OY1\', \'OY2\', \'P\', \'R\', \'S\', \'SH\', \'T\', \'TH\', \'UH\', \'UH0\', \'UH1\', \'UH2\', \'UW\',\n  \'UW0\', \'UW1\', \'UW2\', \'V\', \'W\', \'Y\', \'Z\', \'ZH\'\n]\n\n_valid_symbol_set = set(valid_symbols)\n\n\nclass CMUDict:\n  \'\'\'Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict\'\'\'\n  def __init__(self, file_or_path, keep_ambiguous=True):\n    if isinstance(file_or_path, str):\n      with open(file_or_path, encoding=\'latin-1\') as f:\n        entries = _parse_cmudict(f)\n    else:\n      entries = _parse_cmudict(file_or_path)\n    if not keep_ambiguous:\n      entries = {word: pron for word, pron in entries.items() if len(pron) == 1}\n    self._entries = entries\n\n\n  def __len__(self):\n    return len(self._entries)\n\n\n  def lookup(self, word):\n    \'\'\'Returns list of ARPAbet pronunciations of the given word.\'\'\'\n    return self._entries.get(word.upper())\n\n\n\n_alt_re = re.compile(r\'\\([0-9]+\\)\')\n\n\ndef _parse_cmudict(file):\n  cmudict = {}\n  for line in file:\n    if len(line) and (line[0] >= \'A\' and line[0] <= \'Z\' or line[0] == ""\'""):\n      parts = line.split(\'  \')\n      word = re.sub(_alt_re, \'\', parts[0])\n      pronunciation = _get_pronunciation(parts[1])\n      if pronunciation:\n        if word in cmudict:\n          cmudict[word].append(pronunciation)\n        else:\n          cmudict[word] = [pronunciation]\n  return cmudict\n\n\ndef _get_pronunciation(s):\n  parts = s.strip().split(\' \')\n  for part in parts:\n    if part not in _valid_symbol_set:\n      return None\n  return \' \'.join(parts)\n'"
text/numbers.py,0,"b""#-*- coding: utf-8 -*-\n\nimport inflect\nimport re\n\n\n_inflect = inflect.engine()\n_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')\n_decimal_number_re = re.compile(r'([0-9]+\\.[0-9]+)')\n_pounds_re = re.compile(r'\xc2\xa3([0-9\\,]*[0-9]+)')\n_dollars_re = re.compile(r'\\$([0-9\\.\\,]*[0-9]+)')\n_ordinal_re = re.compile(r'[0-9]+(st|nd|rd|th)')\n_number_re = re.compile(r'[0-9]+')\n\n\ndef _remove_commas(m):\n  return m.group(1).replace(',', '')\n\n\ndef _expand_decimal_point(m):\n  return m.group(1).replace('.', ' point ')\n\n\ndef _expand_dollars(m):\n  match = m.group(1)\n  parts = match.split('.')\n  if len(parts) > 2:\n    return match + ' dollars'  # Unexpected format\n  dollars = int(parts[0]) if parts[0] else 0\n  cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n  if dollars and cents:\n    dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n    cent_unit = 'cent' if cents == 1 else 'cents'\n    return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n  elif dollars:\n    dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n    return '%s %s' % (dollars, dollar_unit)\n  elif cents:\n    cent_unit = 'cent' if cents == 1 else 'cents'\n    return '%s %s' % (cents, cent_unit)\n  else:\n    return 'zero dollars'\n\n\ndef _expand_ordinal(m):\n  return _inflect.number_to_words(m.group(0))\n\n\ndef _expand_number(m):\n  num = int(m.group(0))\n  if num > 1000 and num < 3000:\n    if num == 2000:\n      return 'two thousand'\n    elif num > 2000 and num < 2010:\n      return 'two thousand ' + _inflect.number_to_words(num % 100)\n    elif num % 100 == 0:\n      return _inflect.number_to_words(num // 100) + ' hundred'\n    else:\n      return _inflect.number_to_words(num, andword='', zero='oh', group=2).replace(', ', ' ')\n  else:\n    return _inflect.number_to_words(num, andword='')\n\n\ndef normalize_numbers(text):\n  text = re.sub(_comma_number_re, _remove_commas, text)\n  text = re.sub(_pounds_re, r'\\1 pounds', text)\n  text = re.sub(_dollars_re, _expand_dollars, text)\n  text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n  text = re.sub(_ordinal_re, _expand_ordinal, text)\n  text = re.sub(_number_re, _expand_number, text)\n  return text\n"""
text/symbols.py,0,"b'#-*- coding: utf-8 -*-\n\n\n\'\'\'\nDefines the set of symbols used in text input to the model.\n\nThe default is a set of ASCII characters that works well for English or text that has been run\nthrough Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details.\n\'\'\'\nfrom text import cmudict\n\n_pad        = \'_\'\n_eos        = \'~\'\n_characters = \'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\\\'(),-.:;? \'\n\n# Prepend ""@"" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n_arpabet = [\'@\' + s for s in cmudict.valid_symbols]\n\n# Export all symbols:\nsymbols = [_pad, _eos] + list(_characters) + _arpabet\n\n\nif __name__ == \'__main__\':\n    print(symbols)'"
