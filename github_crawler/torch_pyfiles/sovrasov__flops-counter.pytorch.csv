file_path,api_count,code
sample.py,1,"b""import sys\nimport argparse\n\nimport torchvision.models as models\nimport torch\n\nfrom ptflops import get_model_complexity_info\n\npt_models = {'resnet18': models.resnet18,\n             'resnet50': models.resnet50,\n             'alexnet': models.alexnet,\n             'vgg16': models.vgg16,\n             'squeezenet': models.squeezenet1_0,\n             'densenet': models.densenet161,\n             'inception': models.inception_v3}\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='ptflops sample script')\n    parser.add_argument('--device', type=int, default=0,\n                        help='Device to store the model.')\n    parser.add_argument('--model', choices=list(pt_models.keys()),\n                        type=str, default='resnet18')\n    parser.add_argument('--result', type=str, default=None)\n    args = parser.parse_args()\n\n    if args.result is None:\n        ost = sys.stdout\n    else:\n        ost = open(args.result, 'w')\n\n    net = pt_models[args.model]()\n\n    if torch.cuda.is_available():\n        net.cuda(device=args.device)\n\n    macs, params = get_model_complexity_info(net, (3, 224, 224),\n                                             as_strings=True,\n                                             print_per_layer_stat=True,\n                                             ost=ost)\n    print('{:<30}  {:<8}'.format('Computational complexity: ', macs))\n    print('{:<30}  {:<8}'.format('Number of parameters: ', params))\n"""
setup.py,0,"b""from setuptools import setup, find_packages\n\nreadme = open('README.md').read()\n\nVERSION = '0.6.2'\n\nrequirements = [\n    'torch',\n]\n\nsetup(\n    # Metadata\n    name='ptflops',\n    version=VERSION,\n    author='Vladislav Sovrasov',\n    author_email='sovrasov.vlad@gmail.com',\n    url='https://github.com/sovrasov/flops-counter.pytorch',\n    description='Flops counter for convolutional networks in pytorch framework',\n    long_description=readme,\n    long_description_content_type='text/markdown',\n    license='MIT',\n\n    # Package info\n    packages=find_packages(exclude=('*test*',)),\n\n    #\n    zip_safe=True,\n    install_requires=requirements,\n\n    # Classifiers\n    classifiers=[\n        'Programming Language :: Python :: 3',\n    ],\n)\n"""
ptflops/__init__.py,0,b'from .flops_counter import get_model_complexity_info\n'
ptflops/flops_counter.py,3,"b'\'\'\'\nCopyright (C) 2019 Sovrasov V. - All Rights Reserved\n * You may use, distribute and modify this code under the\n * terms of the MIT license.\n * You should have received a copy of the MIT license with\n * this file. If not visit https://opensource.org/licenses/MIT\n\'\'\'\n\nimport sys\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n\ndef get_model_complexity_info(model, input_res,\n                              print_per_layer_stat=True,\n                              as_strings=True,\n                              input_constructor=None, ost=sys.stdout,\n                              verbose=False, ignore_modules=[],\n                              custom_modules_hooks={}):\n    assert type(input_res) is tuple\n    assert len(input_res) >= 1\n    assert isinstance(model, nn.Module)\n    global CUSTOM_MODULES_MAPPING\n    CUSTOM_MODULES_MAPPING = custom_modules_hooks\n    flops_model = add_flops_counting_methods(model)\n    flops_model.eval()\n    flops_model.start_flops_count(ost=ost, verbose=verbose, ignore_list=ignore_modules)\n    if input_constructor:\n        input = input_constructor(input_res)\n        _ = flops_model(**input)\n    else:\n        try:\n            batch = torch.ones(()).new_empty((1, *input_res),\n                                             dtype=next(flops_model.parameters()).dtype,\n                                             device=next(flops_model.parameters()).device)\n        except StopIteration:\n            batch = torch.ones(()).new_empty((1, *input_res))\n\n        _ = flops_model(batch)\n\n    flops_count, params_count = flops_model.compute_average_flops_cost()\n    if print_per_layer_stat:\n        print_model_with_flops(flops_model, flops_count, params_count, ost=ost)\n    flops_model.stop_flops_count()\n    CUSTOM_MODULES_MAPPING = {}\n\n    if as_strings:\n        return flops_to_string(flops_count), params_to_string(params_count)\n\n    return flops_count, params_count\n\n\ndef flops_to_string(flops, units=\'GMac\', precision=2):\n    if units is None:\n        if flops // 10**9 > 0:\n            return str(round(flops / 10.**9, precision)) + \' GMac\'\n        elif flops // 10**6 > 0:\n            return str(round(flops / 10.**6, precision)) + \' MMac\'\n        elif flops // 10**3 > 0:\n            return str(round(flops / 10.**3, precision)) + \' KMac\'\n        else:\n            return str(flops) + \' Mac\'\n    else:\n        if units == \'GMac\':\n            return str(round(flops / 10.**9, precision)) + \' \' + units\n        elif units == \'MMac\':\n            return str(round(flops / 10.**6, precision)) + \' \' + units\n        elif units == \'KMac\':\n            return str(round(flops / 10.**3, precision)) + \' \' + units\n        else:\n            return str(flops) + \' Mac\'\n\n\ndef params_to_string(params_num, units=None, precision=2):\n    if units is None:\n        if params_num // 10 ** 6 > 0:\n            return str(round(params_num / 10 ** 6, 2)) + \' M\'\n        elif params_num // 10 ** 3:\n            return str(round(params_num / 10 ** 3, 2)) + \' k\'\n        else:\n            return str(params_num)\n    else:\n        if units == \'M\':\n            return str(round(params_num / 10.**6, precision)) + \' \' + units\n        elif units == \'K\':\n            return str(round(params_num / 10.**3, precision)) + \' \' + units\n        else:\n            return str(params_num)\n\n\ndef print_model_with_flops(model, total_flops, total_params, units=\'GMac\',\n                           precision=3, ost=sys.stdout):\n\n    def accumulate_params(self):\n        if is_supported_instance(self):\n            return self.__params__\n        else:\n            sum = 0\n            for m in self.children():\n                sum += m.accumulate_params()\n            return sum\n\n    def accumulate_flops(self):\n        if is_supported_instance(self):\n            return self.__flops__ / model.__batch_counter__\n        else:\n            sum = 0\n            for m in self.children():\n                sum += m.accumulate_flops()\n            return sum\n\n    def flops_repr(self):\n        accumulated_params_num = self.accumulate_params()\n        accumulated_flops_cost = self.accumulate_flops()\n        return \', \'.join([params_to_string(accumulated_params_num, units=\'M\', precision=precision),\n                          \'{:.3%} Params\'.format(accumulated_params_num / total_params),\n                          flops_to_string(accumulated_flops_cost, units=units, precision=precision),\n                          \'{:.3%} MACs\'.format(accumulated_flops_cost / total_flops),\n                          self.original_extra_repr()])\n\n    def add_extra_repr(m):\n        m.accumulate_flops = accumulate_flops.__get__(m)\n        m.accumulate_params = accumulate_params.__get__(m)\n        flops_extra_repr = flops_repr.__get__(m)\n        if m.extra_repr != flops_extra_repr:\n            m.original_extra_repr = m.extra_repr\n            m.extra_repr = flops_extra_repr\n            assert m.extra_repr != m.original_extra_repr\n\n    def del_extra_repr(m):\n        if hasattr(m, \'original_extra_repr\'):\n            m.extra_repr = m.original_extra_repr\n            del m.original_extra_repr\n        if hasattr(m, \'accumulate_flops\'):\n            del m.accumulate_flops\n\n    model.apply(add_extra_repr)\n    print(model, file=ost)\n    model.apply(del_extra_repr)\n\n\ndef get_model_parameters_number(model):\n    params_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    return params_num\n\n\ndef add_flops_counting_methods(net_main_module):\n    # adding additional methods to the existing module object,\n    # this is done this way so that each function has access to self object\n    net_main_module.start_flops_count = start_flops_count.__get__(net_main_module)\n    net_main_module.stop_flops_count = stop_flops_count.__get__(net_main_module)\n    net_main_module.reset_flops_count = reset_flops_count.__get__(net_main_module)\n    net_main_module.compute_average_flops_cost = compute_average_flops_cost.__get__(net_main_module)\n\n    net_main_module.reset_flops_count()\n\n    return net_main_module\n\n\ndef compute_average_flops_cost(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Returns current mean flops consumption per image.\n\n    """"""\n\n    batches_count = self.__batch_counter__\n    flops_sum = 0\n    params_sum = 0\n    for module in self.modules():\n        if is_supported_instance(module):\n            flops_sum += module.__flops__\n    params_sum = get_model_parameters_number(self)\n    return flops_sum / batches_count, params_sum\n\n\ndef start_flops_count(self, **kwargs):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Activates the computation of mean flops consumption per image.\n    Call it before you run the network.\n\n    """"""\n    add_batch_counter_hook_function(self)\n\n    seen_types = set()\n    def add_flops_counter_hook_function(module, ost, verbose, ignore_list):\n        if type(module) in ignore_list:\n            seen_types.add(type(module))\n            if is_supported_instance(module):\n                module.__params__ = 0\n        elif is_supported_instance(module):\n            if hasattr(module, \'__flops_handle__\'):\n                return\n            if type(module) in CUSTOM_MODULES_MAPPING:\n                handle = module.register_forward_hook(CUSTOM_MODULES_MAPPING[type(module)])\n            else:\n                handle = module.register_forward_hook(MODULES_MAPPING[type(module)])\n            module.__flops_handle__ = handle\n            seen_types.add(type(module))\n        else:\n            if verbose and not type(module) in (nn.Sequential, nn.ModuleList) and not type(module) in seen_types:\n                print(\'Warning: module \' + type(module).__name__ + \' is treated as a zero-op.\', file=ost)\n            seen_types.add(type(module))\n\n    self.apply(partial(add_flops_counter_hook_function, **kwargs))\n\n\ndef stop_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Stops computing the mean flops consumption per image.\n    Call whenever you want to pause the computation.\n\n    """"""\n    remove_batch_counter_hook_function(self)\n    self.apply(remove_flops_counter_hook_function)\n\n\ndef reset_flops_count(self):\n    """"""\n    A method that will be available after add_flops_counting_methods() is called\n    on a desired net object.\n\n    Resets statistics computed so far.\n\n    """"""\n    add_batch_counter_variables_or_reset(self)\n    self.apply(add_flops_counter_variable_or_reset)\n\n\n# ---- Internal functions\ndef empty_flops_counter_hook(module, input, output):\n    module.__flops__ += 0\n\n\ndef upsample_flops_counter_hook(module, input, output):\n    output_size = output[0]\n    batch_size = output_size.shape[0]\n    output_elements_count = batch_size\n    for val in output_size.shape[1:]:\n        output_elements_count *= val\n    module.__flops__ += int(output_elements_count)\n\n\ndef relu_flops_counter_hook(module, input, output):\n    active_elements_count = output.numel()\n    module.__flops__ += int(active_elements_count)\n\n\ndef linear_flops_counter_hook(module, input, output):\n    input = input[0]\n    output_last_dim = output.shape[-1]  # pytorch checks dimensions, so here we don\'t care much\n    module.__flops__ += int(np.prod(input.shape) * output_last_dim)\n\n\ndef pool_flops_counter_hook(module, input, output):\n    input = input[0]\n    module.__flops__ += int(np.prod(input.shape))\n\n\ndef bn_flops_counter_hook(module, input, output):\n    module.affine\n    input = input[0]\n\n    batch_flops = np.prod(input.shape)\n    if module.affine:\n        batch_flops *= 2\n    module.__flops__ += int(batch_flops)\n\n\ndef deconv_flops_counter_hook(conv_module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n\n    batch_size = input.shape[0]\n    input_height, input_width = input.shape[2:]\n\n    kernel_height, kernel_width = conv_module.kernel_size\n    in_channels = conv_module.in_channels\n    out_channels = conv_module.out_channels\n    groups = conv_module.groups\n\n    filters_per_channel = out_channels // groups\n    conv_per_position_flops = kernel_height * kernel_width * in_channels * filters_per_channel\n\n    active_elements_count = batch_size * input_height * input_width\n    overall_conv_flops = conv_per_position_flops * active_elements_count\n    bias_flops = 0\n    if conv_module.bias is not None:\n        output_height, output_width = output.shape[2:]\n        bias_flops = out_channels * batch_size * output_height * output_height\n    overall_flops = overall_conv_flops + bias_flops\n\n    conv_module.__flops__ += int(overall_flops)\n\n\ndef conv_flops_counter_hook(conv_module, input, output):\n    # Can have multiple inputs, getting the first one\n    input = input[0]\n\n    batch_size = input.shape[0]\n    output_dims = list(output.shape[2:])\n\n    kernel_dims = list(conv_module.kernel_size)\n    in_channels = conv_module.in_channels\n    out_channels = conv_module.out_channels\n    groups = conv_module.groups\n\n    filters_per_channel = out_channels // groups\n    conv_per_position_flops = int(np.prod(kernel_dims)) * in_channels * filters_per_channel\n\n    active_elements_count = batch_size * int(np.prod(output_dims))\n\n    overall_conv_flops = conv_per_position_flops * active_elements_count\n\n    bias_flops = 0\n\n    if conv_module.bias is not None:\n\n        bias_flops = out_channels * active_elements_count\n\n    overall_flops = overall_conv_flops + bias_flops\n\n    conv_module.__flops__ += int(overall_flops)\n\n\ndef batch_counter_hook(module, input, output):\n    batch_size = 1\n    if len(input) > 0:\n        # Can have multiple inputs, getting the first one\n        input = input[0]\n        batch_size = len(input)\n    else:\n        pass\n        print(\'Warning! No positional inputs found for a module, assuming batch size is 1.\')\n    module.__batch_counter__ += batch_size\n\n\ndef rnn_flops(flops, rnn_module, w_ih, w_hh, input_size):\n    # matrix matrix mult ih state and internal state\n    flops += w_ih.shape[0]*w_ih.shape[1]\n    # matrix matrix mult hh state and internal state\n    flops += w_hh.shape[0]*w_hh.shape[1]\n    if isinstance(rnn_module, (nn.RNN, nn.RNNCell)):\n        # add both operations\n        flops += rnn_module.hidden_size\n    elif isinstance(rnn_module, (nn.GRU, nn.GRUCell)):\n        # hadamard of r\n        flops += rnn_module.hidden_size\n        # adding operations from both states\n        flops += rnn_module.hidden_size*3\n        # last two hadamard product and add\n        flops += rnn_module.hidden_size*3\n    elif isinstance(rnn_module, (nn.LSTM, nn.LSTMCell)):\n        # adding operations from both states\n        flops += rnn_module.hidden_size*4\n        # two hadamard product and add for C state\n        flops += rnn_module.hidden_size + rnn_module.hidden_size + rnn_module.hidden_size\n        # final hadamard\n        flops += rnn_module.hidden_size + rnn_module.hidden_size + rnn_module.hidden_size\n    return flops\n\n\ndef rnn_flops_counter_hook(rnn_module, input, output):\n    """"""\n    Takes into account batch goes at first position, contrary\n    to pytorch common rule (but actually it doesn\'t matter).\n    IF sigmoid and tanh are made hard, only a comparison FLOPS should be accurate\n    """"""\n    flops = 0\n    inp = input[0] # input is a tuble containing a sequence to process and (optionally) hidden state\n    batch_size = inp.shape[0]\n    seq_length = inp.shape[1]\n    num_layers = rnn_module.num_layers\n\n    for i in range(num_layers):\n        w_ih = rnn_module.__getattr__(\'weight_ih_l\' + str(i))\n        w_hh = rnn_module.__getattr__(\'weight_hh_l\' + str(i))\n        if i == 0:\n            input_size = rnn_module.input_size\n        else:\n            input_size = rnn_module.hidden_size\n        flops = rnn_flops(flops, rnn_module, w_ih, w_hh, input_size)\n        if rnn_module.bias:\n            b_ih = rnn_module.__getattr__(\'bias_ih_l\' + str(i))\n            b_hh = rnn_module.__getattr__(\'bias_hh_l\' + str(i))\n            flops += b_ih.shape[0] + b_hh.shape[0]\n\n    flops *= batch_size\n    flops *= seq_length\n    if rnn_module.bidirectional:\n        flops *= 2\n    rnn_module.__flops__ += int(flops)\n\n\ndef rnn_cell_flops_counter_hook(rnn_cell_module, input, output):\n    flops = 0\n    inp = input[0]\n    batch_size = inp.shape[0]\n    w_ih = rnn_cell_module.__getattr__(\'weight_ih\')\n    w_hh = rnn_cell_module.__getattr__(\'weight_hh\')\n    input_size = inp.shape[1]\n    flops = rnn_flops(flops, rnn_cell_module, w_ih, w_hh, input_size)\n    if rnn_cell_module.bias:\n        b_ih = rnn_cell_module.__getattr__(\'bias_ih\')\n        b_hh = rnn_cell_module.__getattr__(\'bias_hh\')\n        flops += b_ih.shape[0] + b_hh.shape[0]\n\n    flops *= batch_size\n    rnn_cell_module.__flops__ += int(flops)\n\n\ndef add_batch_counter_variables_or_reset(module):\n\n    module.__batch_counter__ = 0\n\n\ndef add_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        return\n\n    handle = module.register_forward_hook(batch_counter_hook)\n    module.__batch_counter_handle__ = handle\n\n\ndef remove_batch_counter_hook_function(module):\n    if hasattr(module, \'__batch_counter_handle__\'):\n        module.__batch_counter_handle__.remove()\n        del module.__batch_counter_handle__\n\n\ndef add_flops_counter_variable_or_reset(module):\n    if is_supported_instance(module):\n        if hasattr(module, \'__flops__\') or hasattr(module, \'__params__\'):\n            print(\'Warning: variables __flops__ or __params__ are already \'\n                    \'defined for the module\' + type(module).__name__ +\n                    \' ptflops can affect your code!\')\n        module.__flops__ = 0\n        module.__params__ = get_model_parameters_number(module)\n\nCUSTOM_MODULES_MAPPING = {}\n\nMODULES_MAPPING = {\n    # convolutions\n    nn.Conv1d: conv_flops_counter_hook,\n    nn.Conv2d: conv_flops_counter_hook,\n    nn.Conv3d: conv_flops_counter_hook,\n    # activations\n    nn.ReLU: relu_flops_counter_hook,\n    nn.PReLU: relu_flops_counter_hook,\n    nn.ELU: relu_flops_counter_hook,\n    nn.LeakyReLU: relu_flops_counter_hook,\n    nn.ReLU6: relu_flops_counter_hook,\n    # poolings\n    nn.MaxPool1d: pool_flops_counter_hook,\n    nn.AvgPool1d: pool_flops_counter_hook,\n    nn.AvgPool2d: pool_flops_counter_hook,\n    nn.MaxPool2d: pool_flops_counter_hook,\n    nn.MaxPool3d: pool_flops_counter_hook,\n    nn.AvgPool3d: pool_flops_counter_hook,\n    nn.AdaptiveMaxPool1d: pool_flops_counter_hook,\n    nn.AdaptiveAvgPool1d: pool_flops_counter_hook,\n    nn.AdaptiveMaxPool2d: pool_flops_counter_hook,\n    nn.AdaptiveAvgPool2d: pool_flops_counter_hook,\n    nn.AdaptiveMaxPool3d: pool_flops_counter_hook,\n    nn.AdaptiveAvgPool3d: pool_flops_counter_hook,\n    # BNs\n    nn.BatchNorm1d: bn_flops_counter_hook,\n    nn.BatchNorm2d: bn_flops_counter_hook,\n    nn.BatchNorm3d: bn_flops_counter_hook,\n    # FC\n    nn.Linear: linear_flops_counter_hook,\n    # Upscale\n    nn.Upsample: upsample_flops_counter_hook,\n    # Deconvolution\n    nn.ConvTranspose2d: deconv_flops_counter_hook,\n    # RNN\n    nn.RNN: rnn_flops_counter_hook,\n    nn.GRU: rnn_flops_counter_hook,\n    nn.LSTM: rnn_flops_counter_hook,\n    nn.RNNCell: rnn_cell_flops_counter_hook,\n    nn.LSTMCell: rnn_cell_flops_counter_hook,\n    nn.GRUCell: rnn_cell_flops_counter_hook\n}\n\n\ndef is_supported_instance(module):\n    if type(module) in MODULES_MAPPING or type(module) in CUSTOM_MODULES_MAPPING:\n        return True\n    return False\n\n\ndef remove_flops_counter_hook_function(module):\n    if is_supported_instance(module):\n        if hasattr(module, \'__flops_handle__\'):\n            module.__flops_handle__.remove()\n            del module.__flops_handle__\n'"
