file_path,api_count,code
evaluate.py,8,"b'import argparse\r\nimport scipy\r\nfrom scipy import ndimage\r\nimport cv2\r\nimport numpy as np\r\nimport sys\r\nimport json\r\n\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torchvision.models as models\r\nimport torch.nn.functional as F\r\nfrom torch.utils import data\r\nfrom networks.ccnet import Res_Deeplab\r\nfrom dataset.datasets import CSDataSet\r\nfrom collections import OrderedDict\r\nimport os\r\nimport scipy.ndimage as nd\r\nfrom math import ceil\r\nfrom PIL import Image as PILImage\r\n\r\nimport torch.nn as nn\r\nIMG_MEAN = np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32)\r\n\r\nDATA_DIRECTORY = \'cityscapes\'\r\nDATA_LIST_PATH = \'./dataset/list/cityscapes/val.lst\'\r\nIGNORE_LABEL = 255\r\nNUM_CLASSES = 19\r\nNUM_STEPS = 500 # Number of images in the validation set.\r\nINPUT_SIZE = \'769,769\'\r\nRESTORE_FROM = \'./deeplab_resnet.ckpt\'\r\n\r\ndef get_arguments():\r\n    """"""Parse all the arguments provided from the CLI.\r\n    \r\n    Returns:\r\n      A list of parsed arguments.\r\n    """"""\r\n    parser = argparse.ArgumentParser(description=""DeepLabLFOV Network"")\r\n    parser.add_argument(""--data-dir"", type=str, default=DATA_DIRECTORY,\r\n                        help=""Path to the directory containing the PASCAL VOC dataset."")\r\n    parser.add_argument(""--data-list"", type=str, default=DATA_LIST_PATH,\r\n                        help=""Path to the file listing the images in the dataset."")\r\n    parser.add_argument(""--ignore-label"", type=int, default=IGNORE_LABEL,\r\n                        help=""The index of the label to ignore during the training."")\r\n    parser.add_argument(""--num-classes"", type=int, default=NUM_CLASSES,\r\n                        help=""Number of classes to predict (including background)."")\r\n    parser.add_argument(""--restore-from"", type=str, default=RESTORE_FROM,\r\n                        help=""Where restore model parameters from."")\r\n    parser.add_argument(""--gpu"", type=str, default=\'0\',\r\n                        help=""choose gpu device."")\r\n    parser.add_argument(""--recurrence"", type=int, default=1,\r\n                        help=""choose the number of recurrence."")\r\n    parser.add_argument(""--input-size"", type=str, default=INPUT_SIZE,\r\n                        help=""Comma-separated string with height and width of images."")\r\n    parser.add_argument(""--whole"", type=bool, default=False,\r\n                        help=""use whole input size."")\r\n    return parser.parse_args()\r\n\r\ndef get_palette(num_cls):\r\n    """""" Returns the color map for visualizing the segmentation mask.\r\n    Args:\r\n        num_cls: Number of classes\r\n    Returns:\r\n        The color map\r\n    """"""\r\n\r\n    n = num_cls\r\n    palette = [0] * (n * 3)\r\n    for j in range(0, n):\r\n        lab = j\r\n        palette[j * 3 + 0] = 0\r\n        palette[j * 3 + 1] = 0\r\n        palette[j * 3 + 2] = 0\r\n        i = 0\r\n        while lab:\r\n            palette[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\r\n            palette[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\r\n            palette[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\r\n            i += 1\r\n            lab >>= 3\r\n    return palette\r\n\r\ndef pad_image(img, target_size):\r\n    """"""Pad an image up to the target size.""""""\r\n    rows_missing = target_size[0] - img.shape[2]\r\n    cols_missing = target_size[1] - img.shape[3]\r\n    padded_img = np.pad(img, ((0, 0), (0, 0), (0, rows_missing), (0, cols_missing)), \'constant\')\r\n    return padded_img\r\n\r\ndef predict_sliding(net, image, tile_size, classes, flip_evaluation, recurrence):\r\n    interp = nn.Upsample(size=tile_size, mode=\'bilinear\', align_corners=True)\r\n    image_size = image.shape\r\n    overlap = 1.0/3.0\r\n\r\n    stride = ceil(tile_size[0] * (1 - overlap))\r\n    tile_rows = int(ceil((image_size[2] - tile_size[0]) / stride) + 1)  # strided convolution formula\r\n    tile_cols = int(ceil((image_size[3] - tile_size[1]) / stride) + 1)\r\n    print(""Need %i x %i prediction tiles @ stride %i px"" % (tile_cols, tile_rows, stride))\r\n    full_probs = np.zeros((image_size[2], image_size[3], classes))\r\n    count_predictions = np.zeros((image_size[2], image_size[3], classes))\r\n    tile_counter = 0\r\n\r\n    for row in range(tile_rows):\r\n        for col in range(tile_cols):\r\n            x1 = int(col * stride)\r\n            y1 = int(row * stride)\r\n            x2 = min(x1 + tile_size[1], image_size[3])\r\n            y2 = min(y1 + tile_size[0], image_size[2])\r\n            x1 = max(int(x2 - tile_size[1]), 0)  # for portrait images the x1 underflows sometimes\r\n            y1 = max(int(y2 - tile_size[0]), 0)  # for very few rows y1 underflows\r\n\r\n            img = image[:, :, y1:y2, x1:x2]\r\n            padded_img = pad_image(img, tile_size)\r\n            # plt.imshow(padded_img)\r\n            # plt.show()\r\n            tile_counter += 1\r\n            print(""Predicting tile %i"" % tile_counter)\r\n            padded_prediction = net(Variable(torch.from_numpy(padded_img), volatile=True).cuda(), recurrence)\r\n            if isinstance(padded_prediction, list):\r\n                padded_prediction = padded_prediction[0]\r\n            padded_prediction = interp(padded_prediction).cpu().data[0].numpy().transpose(1,2,0)\r\n            prediction = padded_prediction[0:img.shape[2], 0:img.shape[3], :]\r\n            count_predictions[y1:y2, x1:x2] += 1\r\n            full_probs[y1:y2, x1:x2] += prediction  # accumulate the predictions also in the overlapping regions\r\n\r\n    # average the predictions in the overlapping regions\r\n    full_probs /= count_predictions\r\n    # visualize normalization Weights\r\n    # plt.imshow(np.mean(count_predictions, axis=2))\r\n    # plt.show()\r\n    return full_probs\r\n\r\ndef predict_whole(net, image, tile_size, recurrence):\r\n    image = torch.from_numpy(image)\r\n    interp = nn.Upsample(size=tile_size, mode=\'bilinear\', align_corners=True)\r\n    prediction = net(image.cuda(), recurrence)\r\n    if isinstance(prediction, list):\r\n        prediction = prediction[0]\r\n    prediction = interp(prediction).cpu().data[0].numpy().transpose(1,2,0)\r\n    return prediction\r\n\r\ndef predict_multiscale(net, image, tile_size, scales, classes, flip_evaluation, recurrence):\r\n    """"""\r\n    Predict an image by looking at it with different scales.\r\n        We choose the ""predict_whole_img"" for the image with less than the original input size,\r\n        for the input of larger size, we would choose the cropping method to ensure that GPU memory is enough.\r\n    """"""\r\n    image = image.data\r\n    N_, C_, H_, W_ = image.shape\r\n    full_probs = np.zeros((H_, W_, classes))  \r\n    for scale in scales:\r\n        scale = float(scale)\r\n        print(""Predicting image scaled by %f"" % scale)\r\n        scale_image = ndimage.zoom(image, (1.0, 1.0, scale, scale), order=1, prefilter=False)\r\n        scaled_probs = predict_whole(net, scale_image, tile_size, recurrence)\r\n        if flip_evaluation == True:\r\n            flip_scaled_probs = predict_whole(net, scale_image[:,:,:,::-1].copy(), tile_size, recurrence)\r\n            scaled_probs = 0.5 * (scaled_probs + flip_scaled_probs[:,::-1,:])\r\n        full_probs += scaled_probs\r\n    full_probs /= len(scales)\r\n    return full_probs\r\n\r\ndef get_confusion_matrix(gt_label, pred_label, class_num):\r\n        """"""\r\n        Calcute the confusion matrix by given label and pred\r\n        :param gt_label: the ground truth label\r\n        :param pred_label: the pred label\r\n        :param class_num: the nunber of class\r\n        :return: the confusion matrix\r\n        """"""\r\n        index = (gt_label * class_num + pred_label).astype(\'int32\')\r\n        label_count = np.bincount(index)\r\n        confusion_matrix = np.zeros((class_num, class_num))\r\n\r\n        for i_label in range(class_num):\r\n            for i_pred_label in range(class_num):\r\n                cur_index = i_label * class_num + i_pred_label\r\n                if cur_index < len(label_count):\r\n                    confusion_matrix[i_label, i_pred_label] = label_count[cur_index]\r\n\r\n        return confusion_matrix\r\n\r\ndef main():\r\n    """"""Create the model and start the evaluation process.""""""\r\n    args = get_arguments()\r\n\r\n    # gpu0 = args.gpu\r\n    os.environ[""CUDA_VISIBLE_DEVICES""]=args.gpu\r\n    h, w = map(int, args.input_size.split(\',\'))\r\n    if args.whole:\r\n        input_size = (1024, 2048)\r\n    else:\r\n        input_size = (h, w)\r\n\r\n    model = Res_Deeplab(num_classes=args.num_classes)\r\n    \r\n    saved_state_dict = torch.load(args.restore_from)\r\n    model.load_state_dict(saved_state_dict)\r\n\r\n    model.eval()\r\n    model.cuda()\r\n\r\n    testloader = data.DataLoader(CSDataSet(args.data_dir, args.data_list, crop_size=(1024, 2048), mean=IMG_MEAN, scale=False, mirror=False), \r\n                                    batch_size=1, shuffle=False, pin_memory=True)\r\n\r\n    data_list = []\r\n    confusion_matrix = np.zeros((args.num_classes,args.num_classes))\r\n    palette = get_palette(256)\r\n    interp = nn.Upsample(size=(1024, 2048), mode=\'bilinear\', align_corners=True)\r\n\r\n    if not os.path.exists(\'outputs\'):\r\n        os.makedirs(\'outputs\')\r\n\r\n    for index, batch in enumerate(testloader):\r\n        if index % 100 == 0:\r\n            print(\'%d processd\'%(index))\r\n        image, label, size, name = batch\r\n        size = size[0].numpy()\r\n        with torch.no_grad():\r\n            if args.whole:\r\n                output = predict_multiscale(model, image, input_size, [0.75, 1.0, 1.25], args.num_classes, True, args.recurrence)\r\n            else:\r\n                output = predict_sliding(model, image.numpy(), input_size, args.num_classes, True, args.recurrence)\r\n        # padded_prediction = model(Variable(image, volatile=True).cuda())\r\n        # output = interp(padded_prediction).cpu().data[0].numpy().transpose(1,2,0)\r\n        seg_pred = np.asarray(np.argmax(output, axis=2), dtype=np.uint8)\r\n        output_im = PILImage.fromarray(seg_pred)\r\n        output_im.putpalette(palette)\r\n        output_im.save(\'outputs/\'+name[0]+\'.png\')\r\n\r\n        seg_gt = np.asarray(label[0].numpy()[:size[0],:size[1]], dtype=np.int)\r\n    \r\n        ignore_index = seg_gt != 255\r\n        seg_gt = seg_gt[ignore_index]\r\n        seg_pred = seg_pred[ignore_index]\r\n        # show_all(gt, output)\r\n        confusion_matrix += get_confusion_matrix(seg_gt, seg_pred, args.num_classes)\r\n\r\n    pos = confusion_matrix.sum(1)\r\n    res = confusion_matrix.sum(0)\r\n    tp = np.diag(confusion_matrix)\r\n\r\n    IU_array = (tp / np.maximum(1.0, pos + res - tp))\r\n    mean_IU = IU_array.mean()\r\n    \r\n    # getConfusionMatrixPlot(confusion_matrix)\r\n    print({\'meanIU\':mean_IU, \'IU_array\':IU_array})\r\n    with open(\'result.txt\', \'w\') as f:\r\n        f.write(json.dumps({\'meanIU\':mean_IU, \'IU_array\':IU_array.tolist()}))\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
test.py,7,"b'import argparse\r\nimport scipy\r\nfrom scipy import ndimage\r\nimport cv2\r\nimport numpy as np\r\nimport sys\r\nimport json\r\n\r\nimport torch\r\nfrom torch.autograd import Variable\r\nimport torchvision.models as models\r\nimport torch.nn.functional as F\r\nfrom torch.utils import data\r\nfrom networks.ccnet import Res_Deeplab\r\nfrom dataset.datasets import CSDataTestSet\r\nfrom collections import OrderedDict\r\nimport os\r\nimport scipy.ndimage as nd\r\nfrom math import ceil\r\nfrom PIL import Image as PILImage\r\n\r\nimport torch.nn as nn\r\nIMG_MEAN = np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32)\r\n\r\nDATA_DIRECTORY = \'cityscapes\'\r\nDATA_LIST_PATH = \'./dataset/list/cityscapes/test.lst\'\r\nIGNORE_LABEL = 255\r\nNUM_CLASSES = 19\r\nINPUT_SIZE = \'769,769\'\r\nRESTORE_FROM = \'./deeplab_resnet.ckpt\'\r\n\r\ndef get_arguments():\r\n    """"""Parse all the arguments provided from the CLI.\r\n    \r\n    Returns:\r\n      A list of parsed arguments.\r\n    """"""\r\n    parser = argparse.ArgumentParser(description=""DeepLabLFOV Network"")\r\n    parser.add_argument(""--data-dir"", type=str, default=DATA_DIRECTORY,\r\n                        help=""Path to the directory containing the PASCAL VOC dataset."")\r\n    parser.add_argument(""--data-list"", type=str, default=DATA_LIST_PATH,\r\n                        help=""Path to the file listing the images in the dataset."")\r\n    parser.add_argument(""--ignore-label"", type=int, default=IGNORE_LABEL,\r\n                        help=""The index of the label to ignore during the training."")\r\n    parser.add_argument(""--num-classes"", type=int, default=NUM_CLASSES,\r\n                        help=""Number of classes to predict (including background)."")\r\n    parser.add_argument(""--restore-from"", type=str, default=RESTORE_FROM,\r\n                        help=""Where restore model parameters from."")\r\n    parser.add_argument(""--gpu"", type=str, default=\'0\',\r\n                        help=""choose gpu device."")\r\n    parser.add_argument(""--recurrence"", type=int, default=1,\r\n                        help=""choose the number of recurrence."")\r\n    parser.add_argument(""--input-size"", type=str, default=INPUT_SIZE,\r\n                        help=""Comma-separated string with height and width of images."")\r\n    parser.add_argument(""--whole"", type=bool, default=False,\r\n                        help=""use whole input size."")\r\n    return parser.parse_args()\r\n\r\ndef get_palette(num_cls):\r\n    """""" Returns the color map for visualizing the segmentation mask.\r\n    Args:\r\n        num_cls: Number of classes\r\n    Returns:\r\n        The color map\r\n    """"""\r\n\r\n    n = num_cls\r\n    palette = [0] * (n * 3)\r\n    for j in range(0, n):\r\n        lab = j\r\n        palette[j * 3 + 0] = 0\r\n        palette[j * 3 + 1] = 0\r\n        palette[j * 3 + 2] = 0\r\n        i = 0\r\n        while lab:\r\n            palette[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\r\n            palette[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\r\n            palette[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\r\n            i += 1\r\n            lab >>= 3\r\n    return palette\r\n\r\ndef pad_image(img, target_size):\r\n    """"""Pad an image up to the target size.""""""\r\n    rows_missing = target_size[0] - img.shape[2]\r\n    cols_missing = target_size[1] - img.shape[3]\r\n    padded_img = np.pad(img, ((0, 0), (0, 0), (0, rows_missing), (0, cols_missing)), \'constant\')\r\n    return padded_img\r\n\r\ndef predict_sliding(net, image, tile_size, classes, flip_evaluation, recurrence):\r\n    interp = nn.Upsample(size=tile_size, mode=\'bilinear\', align_corners=True)\r\n    image_size = image.shape\r\n    overlap = 1/3\r\n\r\n    stride = ceil(tile_size[0] * (1 - overlap))\r\n    tile_rows = int(ceil((image_size[2] - tile_size[0]) / stride) + 1)  # strided convolution formula\r\n    tile_cols = int(ceil((image_size[3] - tile_size[1]) / stride) + 1)\r\n    print(""Need %i x %i prediction tiles @ stride %i px"" % (tile_cols, tile_rows, stride))\r\n    full_probs = np.zeros((image_size[2], image_size[3], classes))\r\n    count_predictions = np.zeros((image_size[2], image_size[3], classes))\r\n    tile_counter = 0\r\n\r\n    for row in range(tile_rows):\r\n        for col in range(tile_cols):\r\n            x1 = int(col * stride)\r\n            y1 = int(row * stride)\r\n            x2 = min(x1 + tile_size[1], image_size[3])\r\n            y2 = min(y1 + tile_size[0], image_size[2])\r\n            x1 = max(int(x2 - tile_size[1]), 0)  # for portrait images the x1 underflows sometimes\r\n            y1 = max(int(y2 - tile_size[0]), 0)  # for very few rows y1 underflows\r\n\r\n            img = image[:, :, y1:y2, x1:x2]\r\n            padded_img = pad_image(img, tile_size)\r\n            # plt.imshow(padded_img)\r\n            # plt.show()\r\n            tile_counter += 1\r\n            print(""Predicting tile %i"" % tile_counter)\r\n            padded_prediction = net(Variable(torch.from_numpy(padded_img), volatile=True).cuda(), recurrence)\r\n            if isinstance(padded_prediction, list):\r\n                padded_prediction = padded_prediction[0]\r\n            padded_prediction = interp(padded_prediction).cpu().data[0].numpy().transpose(1,2,0)\r\n            prediction = padded_prediction[0:img.shape[2], 0:img.shape[3], :]\r\n            count_predictions[y1:y2, x1:x2] += 1\r\n            full_probs[y1:y2, x1:x2] += prediction  # accumulate the predictions also in the overlapping regions\r\n\r\n    # average the predictions in the overlapping regions\r\n    full_probs /= count_predictions\r\n    # visualize normalization Weights\r\n    # plt.imshow(np.mean(count_predictions, axis=2))\r\n    # plt.show()\r\n    return full_probs\r\n\r\ndef predict_whole(net, image, tile_size, flip_evaluation, recurrence):\r\n    interp = nn.Upsample(size=tile_size, mode=\'bilinear\', align_corners=True)\r\n    prediction = net(image.cuda(), recurrence)\r\n    if isinstance(prediction, list):\r\n        prediction = prediction[0]\r\n    prediction = interp(prediction).cpu().data[0].numpy().transpose(1,2,0)\r\n    return prediction\r\n\r\ndef id2trainId(label, id_to_trainid, reverse=False):\r\n        label_copy = label.copy()\r\n        if reverse:\r\n            for v, k in id_to_trainid.items():\r\n                label_copy[label == k] = v\r\n        else:\r\n            for k, v in id_to_trainid.items():\r\n                label_copy[label == k] = v\r\n        return label_copy\r\n\r\ndef main():\r\n    """"""Create the model and start the evaluation process.""""""\r\n    args = get_arguments()\r\n\r\n    # gpu0 = args.gpu\r\n    os.environ[""CUDA_VISIBLE_DEVICES""]=args.gpu\r\n    h, w = map(int, args.input_size.split(\',\'))\r\n    if args.whole:\r\n        input_size = (1024, 2048)\r\n    else:\r\n        input_size = (h, w)\r\n    ignore_label= args.ignore_label\r\n\r\n    model = Res_Deeplab(num_classes=args.num_classes)\r\n    \r\n    saved_state_dict = torch.load(args.restore_from)\r\n    model.load_state_dict(saved_state_dict)\r\n\r\n    model.eval()\r\n    model.cuda()\r\n\r\n    testloader = data.DataLoader(CSDataTestSet(args.data_dir, args.data_list, crop_size=(1024, 2048), mean=IMG_MEAN), \r\n                                    batch_size=1, shuffle=False, pin_memory=True)\r\n\r\n    data_list = []\r\n    confusion_matrix = np.zeros((args.num_classes,args.num_classes))\r\n    palette = get_palette(256)\r\n    id_to_trainid = {-1: ignore_label, 0: ignore_label, 1: ignore_label, 2: ignore_label,\r\n                  3: ignore_label, 4: ignore_label, 5: ignore_label, 6: ignore_label,\r\n                  7: 0, 8: 1, 9: ignore_label, 10: ignore_label, 11: 2, 12: 3, 13: 4,\r\n                  14: ignore_label, 15: ignore_label, 16: ignore_label, 17: 5,\r\n                  18: ignore_label, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\r\n                  28: 15, 29: ignore_label, 30: ignore_label, 31: 16, 32: 17, 33: 18}\r\n    interp = nn.Upsample(size=(1024, 2048), mode=\'bilinear\', align_corners=True)\r\n\r\n    if not os.path.exists(\'outputs\'):\r\n        os.makedirs(\'outputs\')\r\n\r\n    for index, batch in enumerate(testloader):\r\n        if index % 100 == 0:\r\n            print(\'%d processd\'%(index))\r\n        image, size, name = batch\r\n        size = size[0].numpy()\r\n        with torch.no_grad():\r\n            if args.whole:\r\n                output = predict_whole(model, image, input_size, True, args.recurrence)\r\n            else:\r\n                output = predict_sliding(model, image.numpy(), input_size, args.num_classes, True, args.recurrence)\r\n\r\n        seg_pred = np.asarray(np.argmax(output, axis=2), dtype=np.uint8)\r\n        seg_pred = id2trainId(seg_pred, id_to_trainid, reverse=True)\r\n        output_im = PILImage.fromarray(seg_pred)\r\n        output_im.putpalette(palette)\r\n        output_im.save(\'outputs/\'+name[0]+\'.png\')\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
train.py,9,"b'import argparse\r\n\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.utils import data\r\nimport numpy as np\r\nimport pickle\r\nimport cv2\r\nimport torch.optim as optim\r\nimport scipy.misc\r\nimport torch.backends.cudnn as cudnn\r\nimport sys\r\nimport os\r\nfrom tqdm import tqdm\r\nimport os.path as osp\r\nfrom networks.ccnet import Res_Deeplab\r\nfrom dataset.datasets import CSDataSet\r\n#import matplotlib.pyplot as plt\r\nimport random\r\nimport timeit\r\nimport logging\r\nfrom tensorboardX import SummaryWriter\r\nfrom utils.utils import decode_labels, inv_preprocess, decode_predictions\r\nfrom utils.criterion import CriterionCrossEntropy, CriterionOhemCrossEntropy, CriterionDSN, CriterionOhemDSN\r\nfrom utils.encoding import DataParallelModel, DataParallelCriterion\r\n\r\ntorch_ver = torch.__version__[:3]\r\nif torch_ver == \'0.3\':\r\n    from torch.autograd import Variable\r\n\r\nstart = timeit.default_timer()\r\n\r\nIMG_MEAN = np.array((104.00698793,116.66876762,122.67891434), dtype=np.float32)\r\n\r\nBATCH_SIZE = 8\r\nDATA_DIRECTORY = \'cityscapes\'\r\nDATA_LIST_PATH = \'./dataset/list/cityscapes/train.lst\'\r\nIGNORE_LABEL = 255\r\nINPUT_SIZE = \'769,769\'\r\nLEARNING_RATE = 1e-2\r\nMOMENTUM = 0.9\r\nNUM_CLASSES = 19\r\nNUM_STEPS = 60000\r\nPOWER = 0.9\r\nRANDOM_SEED = 1234\r\nRESTORE_FROM = \'./dataset/resnet101-imagenet.pth\'\r\nSAVE_NUM_IMAGES = 2\r\nSAVE_PRED_EVERY = 10000\r\nSNAPSHOT_DIR = \'snapshots/\'\r\nWEIGHT_DECAY = 0.0005\r\n\r\ndef str2bool(v):\r\n    if v.lower() in (\'yes\', \'true\', \'t\', \'y\', \'1\'):\r\n        return True\r\n    elif v.lower() in (\'no\', \'false\', \'f\', \'n\', \'0\'):\r\n        return False\r\n    else:\r\n        raise argparse.ArgumentTypeError(\'Boolean value expected.\')\r\n\r\ndef get_arguments():\r\n    """"""Parse all the arguments provided from the CLI.\r\n    \r\n    Returns:\r\n      A list of parsed arguments.\r\n    """"""\r\n    parser = argparse.ArgumentParser(description=""DeepLab-ResNet Network"")\r\n    parser.add_argument(""--batch-size"", type=int, default=BATCH_SIZE,\r\n                        help=""Number of images sent to the network in one step."")\r\n    parser.add_argument(""--data-dir"", type=str, default=DATA_DIRECTORY,\r\n                        help=""Path to the directory containing the PASCAL VOC dataset."")\r\n    parser.add_argument(""--data-list"", type=str, default=DATA_LIST_PATH,\r\n                        help=""Path to the file listing the images in the dataset."")\r\n    parser.add_argument(""--ignore-label"", type=int, default=IGNORE_LABEL,\r\n                        help=""The index of the label to ignore during the training."")\r\n    parser.add_argument(""--input-size"", type=str, default=INPUT_SIZE,\r\n                        help=""Comma-separated string with height and width of images."")\r\n    parser.add_argument(""--is-training"", action=""store_true"",\r\n                        help=""Whether to updates the running means and variances during the training."")\r\n    parser.add_argument(""--learning-rate"", type=float, default=LEARNING_RATE,\r\n                        help=""Base learning rate for training with polynomial decay."")\r\n    parser.add_argument(""--momentum"", type=float, default=MOMENTUM,\r\n                        help=""Momentum component of the optimiser."")\r\n    parser.add_argument(""--not-restore-last"", action=""store_true"",\r\n                        help=""Whether to not restore last (FC) layers."")\r\n    parser.add_argument(""--num-classes"", type=int, default=NUM_CLASSES,\r\n                        help=""Number of classes to predict (including background)."")\r\n    parser.add_argument(""--start-iters"", type=int, default=0,\r\n                        help=""Number of classes to predict (including background)."")\r\n    parser.add_argument(""--num-steps"", type=int, default=NUM_STEPS,\r\n                        help=""Number of training steps."")\r\n    parser.add_argument(""--power"", type=float, default=POWER,\r\n                        help=""Decay parameter to compute the learning rate."")\r\n    parser.add_argument(""--random-mirror"", action=""store_true"",\r\n                        help=""Whether to randomly mirror the inputs during the training."")\r\n    parser.add_argument(""--random-scale"", action=""store_true"",\r\n                        help=""Whether to randomly scale the inputs during the training."")\r\n    parser.add_argument(""--random-seed"", type=int, default=RANDOM_SEED,\r\n                        help=""Random seed to have reproducible results."")\r\n    parser.add_argument(""--restore-from"", type=str, default=RESTORE_FROM,\r\n                        help=""Where restore model parameters from."")\r\n    parser.add_argument(""--save-num-images"", type=int, default=SAVE_NUM_IMAGES,\r\n                        help=""How many images to save."")\r\n    parser.add_argument(""--save-pred-every"", type=int, default=SAVE_PRED_EVERY,\r\n                        help=""Save summaries and checkpoint every often."")\r\n    parser.add_argument(""--snapshot-dir"", type=str, default=SNAPSHOT_DIR,\r\n                        help=""Where to save snapshots of the model."")\r\n    parser.add_argument(""--weight-decay"", type=float, default=WEIGHT_DECAY,\r\n                        help=""Regularisation parameter for L2-loss."")\r\n    parser.add_argument(""--gpu"", type=str, default=\'None\',\r\n                        help=""choose gpu device."")\r\n    parser.add_argument(""--recurrence"", type=int, default=1,\r\n                        help=""choose the number of recurrence."")\r\n    parser.add_argument(""--ft"", type=bool, default=False,\r\n                        help=""fine-tune the model with large input size."")\r\n\r\n    parser.add_argument(""--ohem"", type=str2bool, default=\'False\',\r\n                        help=""use hard negative mining"")\r\n    parser.add_argument(""--ohem-thres"", type=float, default=0.6,\r\n                        help=""choose the samples with correct probability underthe threshold."")\r\n    parser.add_argument(""--ohem-keep"", type=int, default=200000,\r\n                        help=""choose the samples with correct probability underthe threshold."")\r\n    return parser.parse_args()\r\n\r\nargs = get_arguments()\r\n\r\n\r\ndef lr_poly(base_lr, iter, max_iter, power):\r\n    return base_lr*((1-float(iter)/max_iter)**(power))\r\n            \r\ndef adjust_learning_rate(optimizer, i_iter):\r\n    """"""Sets the learning rate to the initial LR divided by 5 at 60th, 120th and 160th epochs""""""\r\n    lr = lr_poly(args.learning_rate, i_iter, args.num_steps, args.power)\r\n    optimizer.param_groups[0][\'lr\'] = lr\r\n    return lr\r\n\r\ndef set_bn_eval(m):\r\n    classname = m.__class__.__name__\r\n    if classname.find(\'BatchNorm\') != -1:\r\n        m.eval()\r\n\r\ndef set_bn_momentum(m):\r\n    classname = m.__class__.__name__\r\n    if classname.find(\'BatchNorm\') != -1 or classname.find(\'InPlaceABN\') != -1:\r\n        m.momentum = 0.0003\r\n\r\ndef main():\r\n    """"""Create the model and start the training.""""""\r\n    writer = SummaryWriter(args.snapshot_dir)\r\n    \r\n    if not args.gpu == \'None\':\r\n        os.environ[""CUDA_VISIBLE_DEVICES""]=args.gpu\r\n    h, w = map(int, args.input_size.split(\',\'))\r\n    input_size = (h, w)\r\n\r\n    cudnn.enabled = True\r\n\r\n    # Create network.\r\n    deeplab = Res_Deeplab(num_classes=args.num_classes)\r\n    print(deeplab)\r\n\r\n    saved_state_dict = torch.load(args.restore_from)\r\n    new_params = deeplab.state_dict().copy()\r\n    for i in saved_state_dict:\r\n        i_parts = i.split(\'.\')\r\n        if not i_parts[0]==\'fc\':\r\n            new_params[\'.\'.join(i_parts[0:])] = saved_state_dict[i] \r\n    \r\n    deeplab.load_state_dict(new_params)\r\n\r\n\r\n    model = DataParallelModel(deeplab)\r\n    model.train()\r\n    model.float()\r\n    # model.apply(set_bn_momentum)\r\n    model.cuda()    \r\n\r\n    if args.ohem:\r\n        criterion = CriterionOhemDSN(thresh=args.ohem_thres, min_kept=args.ohem_keep)\r\n    else:\r\n        criterion = CriterionDSN() #CriterionCrossEntropy()\r\n    criterion = DataParallelCriterion(criterion)\r\n    criterion.cuda()\r\n    \r\n    cudnn.benchmark = True\r\n\r\n    if not os.path.exists(args.snapshot_dir):\r\n        os.makedirs(args.snapshot_dir)\r\n\r\n\r\n    trainloader = data.DataLoader(CSDataSet(args.data_dir, args.data_list, max_iters=args.num_steps*args.batch_size, crop_size=input_size, \r\n                    scale=args.random_scale, mirror=args.random_mirror, mean=IMG_MEAN), \r\n                    batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\r\n\r\n    optimizer = optim.SGD([{\'params\': filter(lambda p: p.requires_grad, deeplab.parameters()), \'lr\': args.learning_rate }], \r\n                lr=args.learning_rate, momentum=args.momentum,weight_decay=args.weight_decay)\r\n    optimizer.zero_grad()\r\n\r\n    for i_iter, batch in enumerate(trainloader):\r\n        i_iter += args.start_iters\r\n        images, labels, _, _ = batch\r\n        images = images.cuda()\r\n        labels = labels.long().cuda()\r\n        if torch_ver == ""0.3"":\r\n            images = Variable(images)\r\n            labels = Variable(labels)\r\n\r\n        optimizer.zero_grad()\r\n        lr = adjust_learning_rate(optimizer, i_iter)\r\n        preds = model(images, args.recurrence)\r\n\r\n        loss = criterion(preds, labels)\r\n        loss.backward()\r\n        optimizer.step()\r\n\r\n        if i_iter % 100 == 0:\r\n            writer.add_scalar(\'learning_rate\', lr, i_iter)\r\n            writer.add_scalar(\'loss\', loss.data.cpu().numpy(), i_iter)\r\n\r\n        # if i_iter % 5000 == 0:\r\n        #     images_inv = inv_preprocess(images, args.save_num_images, IMG_MEAN)\r\n        #     labels_colors = decode_labels(labels, args.save_num_images, args.num_classes)\r\n        #     if isinstance(preds, list):\r\n        #         preds = preds[0]\r\n        #     preds_colors = decode_predictions(preds, args.save_num_images, args.num_classes)\r\n        #     for index, (img, lab) in enumerate(zip(images_inv, labels_colors)):\r\n        #         writer.add_image(\'Images/\'+str(index), img, i_iter)\r\n        #         writer.add_image(\'Labels/\'+str(index), lab, i_iter)\r\n        #         writer.add_image(\'preds/\'+str(index), preds_colors[index], i_iter)\r\n\r\n        print(\'iter = {} of {} completed, loss = {}\'.format(i_iter, args.num_steps, loss.data.cpu().numpy()))\r\n\r\n        if i_iter >= args.num_steps-1:\r\n            print(\'save model ...\')\r\n            torch.save(deeplab.state_dict(),osp.join(args.snapshot_dir, \'CS_scenes_\'+str(args.num_steps)+\'.pth\'))\r\n            break\r\n\r\n        if i_iter % args.save_pred_every == 0:\r\n            print(\'taking snapshot ...\')\r\n            torch.save(deeplab.state_dict(),osp.join(args.snapshot_dir, \'CS_scenes_\'+str(i_iter)+\'.pth\'))     \r\n\r\n    end = timeit.default_timer()\r\n    print(end-start,\'seconds\')\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
cc_attention/__init__.py,0,"b'from .functions import CrissCrossAttention, ca_weight, ca_map'"
cc_attention/build.py,1,"b'import os\n\nfrom torch.utils.ffi import create_extension\n\nsources = [\'src/lib_cffi.cpp\']\nheaders = [\'src/lib_cffi.h\']\nextra_objects = [\'src/ca.o\']\nwith_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    \'_ext\',\n    headers=headers,\n    sources=sources,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects,\n    extra_compile_args=[""-std=c++11""]\n)\n\nif __name__ == \'__main__\':\n    ffi.build()\n'"
cc_attention/functions.py,12,"b'import torch\nimport torch.nn as nn\n\nimport torch.autograd as autograd\nimport torch.cuda.comm as comm\nimport torch.nn.functional as F\nfrom torch.autograd.function import once_differentiable\nimport time\nimport functools\n\n# from libs import InPlaceABN, InPlaceABNSync\n# BatchNorm2d = functools.partial(InPlaceABNSync, activation=\'none\')\n\nfrom . import _ext\n\n\ndef _check_contiguous(*args):\n    if not all([mod is None or mod.is_contiguous() for mod in args]):\n        raise ValueError(""Non-contiguous input"")\n\n\nclass CA_Weight(autograd.Function):\n    @staticmethod\n    def forward(ctx, t, f):\n        # Save context\n        n, c, h, w = t.size()\n        size = (n, h+w-1, h, w)\n        weight = torch.zeros(size, dtype=t.dtype, layout=t.layout, device=t.device)\n\n        _ext.ca_forward_cuda(t, f, weight)\n        \n        # Output\n        ctx.save_for_backward(t, f)\n\n        return weight\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dw):\n        t, f = ctx.saved_tensors\n\n        dt = torch.zeros_like(t)\n        df = torch.zeros_like(f)\n\n        _ext.ca_backward_cuda(dw.contiguous(), t, f, dt, df)\n\n        _check_contiguous(dt, df)\n\n        return dt, df\n\nclass CA_Map(autograd.Function):\n    @staticmethod\n    def forward(ctx, weight, g):\n        # Save context\n        out = torch.zeros_like(g)\n        _ext.ca_map_forward_cuda(weight, g, out)\n        \n        # Output\n        ctx.save_for_backward(weight, g)\n\n        return out\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dout):\n        weight, g = ctx.saved_tensors\n\n        dw = torch.zeros_like(weight)\n        dg = torch.zeros_like(g)\n\n        _ext.ca_map_backward_cuda(dout.contiguous(), weight, g, dw, dg)\n\n        _check_contiguous(dw, dg)\n\n        return dw, dg\n\nca_weight = CA_Weight.apply\nca_map = CA_Map.apply\n\n\nclass CrissCrossAttention(nn.Module):\n    """""" Criss-Cross Attention Module""""""\n    def __init__(self,in_dim):\n        super(CrissCrossAttention,self).__init__()\n        self.chanel_in = in_dim\n\n        self.query_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n        self.key_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim//8 , kernel_size= 1)\n        self.value_conv = nn.Conv2d(in_channels = in_dim , out_channels = in_dim , kernel_size= 1)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n    def forward(self,x):\n        proj_query = self.query_conv(x)\n        proj_key = self.key_conv(x)\n        proj_value = self.value_conv(x)\n\n        energy = ca_weight(proj_query, proj_key)\n        attention = F.softmax(energy, 1)\n        out = ca_map(attention, proj_value)\n        out = self.gamma*out + x\n\n        return out\n\n\n\n__all__ = [""CrissCrossAttention"", ""ca_weight"", ""ca_map""]\n'"
dataset/__init__.py,0,b''
dataset/datasets.py,1,"b'import os\r\nimport os.path as osp\r\nimport numpy as np\r\nimport random\r\nimport collections\r\nimport torch\r\nimport torchvision\r\nimport cv2\r\nfrom torch.utils import data\r\n\r\n\r\nclass VOCDataSet(data.Dataset):\r\n    def __init__(self, root, list_path, max_iters=None, crop_size=(321, 321), mean=(128, 128, 128), scale=True, mirror=True, ignore_label=255):\r\n        self.root = root\r\n        self.list_path = list_path\r\n        self.crop_h, self.crop_w = crop_size\r\n        self.scale = scale\r\n        self.ignore_label = ignore_label\r\n        self.mean = mean\r\n        self.is_mirror = mirror\r\n        # self.mean_bgr = np.array([104.00698793, 116.66876762, 122.67891434])\r\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\r\n        if not max_iters==None:\r\n            self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\r\n        self.files = []\r\n        # for split in [""train"", ""trainval"", ""val""]:\r\n        for name in self.img_ids:\r\n            img_file = osp.join(self.root, ""JPEGImages/%s.jpg"" % name)\r\n            label_file = osp.join(self.root, ""SegmentationClassAug/%s.png"" % name)\r\n            self.files.append({\r\n                ""img"": img_file,\r\n                ""label"": label_file,\r\n                ""name"": name\r\n            })\r\n\r\n    def __len__(self):\r\n        return len(self.files)\r\n\r\n    def generate_scale_label(self, image, label):\r\n        f_scale = 0.5 + random.randint(0, 11) / 10.0\r\n        image = cv2.resize(image, None, fx=f_scale, fy=f_scale, interpolation = cv2.INTER_LINEAR)\r\n        label = cv2.resize(label, None, fx=f_scale, fy=f_scale, interpolation = cv2.INTER_NEAREST)\r\n        return image, label\r\n\r\n    def __getitem__(self, index):\r\n        datafiles = self.files[index]\r\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\r\n        label = cv2.imread(datafiles[""label""], cv2.IMREAD_GRAYSCALE)\r\n        size = image.shape\r\n        name = datafiles[""name""]\r\n        if self.scale:\r\n            image, label = self.generate_scale_label(image, label)\r\n        image = np.asarray(image, np.float32)\r\n        image -= self.mean\r\n        img_h, img_w = label.shape\r\n        pad_h = max(self.crop_h - img_h, 0)\r\n        pad_w = max(self.crop_w - img_w, 0)\r\n        if pad_h > 0 or pad_w > 0:\r\n            img_pad = cv2.copyMakeBorder(image, 0, pad_h, 0, \r\n                pad_w, cv2.BORDER_CONSTANT, \r\n                value=(0.0, 0.0, 0.0))\r\n            label_pad = cv2.copyMakeBorder(label, 0, pad_h, 0, \r\n                pad_w, cv2.BORDER_CONSTANT,\r\n                value=(self.ignore_label,))\r\n        else:\r\n            img_pad, label_pad = image, label\r\n\r\n        img_h, img_w = label_pad.shape\r\n        h_off = random.randint(0, img_h - self.crop_h)\r\n        w_off = random.randint(0, img_w - self.crop_w)\r\n        # roi = cv2.Rect(w_off, h_off, self.crop_w, self.crop_h);\r\n        image = np.asarray(img_pad[h_off : h_off+self.crop_h, w_off : w_off+self.crop_w], np.float32)\r\n        label = np.asarray(label_pad[h_off : h_off+self.crop_h, w_off : w_off+self.crop_w], np.float32)\r\n        #image = image[:, :, ::-1]  # change to BGR\r\n        image = image.transpose((2, 0, 1))\r\n        if self.is_mirror:\r\n            flip = np.random.choice(2) * 2 - 1\r\n            image = image[:, :, ::flip]\r\n            label = label[:, ::flip]\r\n\r\n        return image.copy(), label.copy(), np.array(size), name\r\n\r\n\r\nclass VOCDataTestSet(data.Dataset):\r\n    def __init__(self, root, list_path, crop_size=(505, 505), mean=(128, 128, 128)):\r\n        self.root = root\r\n        self.list_path = list_path\r\n        self.crop_h, self.crop_w = crop_size\r\n        self.mean = mean\r\n        # self.mean_bgr = np.array([104.00698793, 116.66876762, 122.67891434])\r\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\r\n        self.files = [] \r\n        # for split in [""train"", ""trainval"", ""val""]:\r\n        for name in self.img_ids:\r\n            img_file = osp.join(self.root, ""JPEGImages/%s.jpg"" % name)\r\n            self.files.append({\r\n                ""img"": img_file\r\n            })\r\n\r\n    def __len__(self):\r\n        return len(self.files)\r\n\r\n    def __getitem__(self, index):\r\n        datafiles = self.files[index]\r\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\r\n        size = image.shape\r\n        name = osp.splitext(osp.basename(datafiles[""img""]))[0]\r\n        image = np.asarray(image, np.float32)\r\n        image -= self.mean\r\n        \r\n        img_h, img_w, _ = image.shape\r\n        pad_h = max(self.crop_h - img_h, 0)\r\n        pad_w = max(self.crop_w - img_w, 0)\r\n        if pad_h > 0 or pad_w > 0:\r\n            image = cv2.copyMakeBorder(image, 0, pad_h, 0, \r\n                pad_w, cv2.BORDER_CONSTANT, \r\n                value=(0.0, 0.0, 0.0))\r\n        image = image.transpose((2, 0, 1))\r\n        return image, name, size\r\n\r\nclass CSDataSet(data.Dataset):\r\n    def __init__(self, root, list_path, max_iters=None, crop_size=(321, 321), mean=(128, 128, 128), scale=True, mirror=True, ignore_label=255):\r\n        self.root = root\r\n        self.list_path = list_path\r\n        self.crop_h, self.crop_w = crop_size\r\n        self.scale = scale\r\n        self.ignore_label = ignore_label\r\n        self.mean = mean\r\n        self.is_mirror = mirror\r\n        # self.mean_bgr = np.array([104.00698793, 116.66876762, 122.67891434])\r\n        self.img_ids = [i_id.strip().split() for i_id in open(list_path)]\r\n        if not max_iters==None:\r\n                self.img_ids = self.img_ids * int(np.ceil(float(max_iters) / len(self.img_ids)))\r\n        self.files = []\r\n        # for split in [""train"", ""trainval"", ""val""]:\r\n        for item in self.img_ids:\r\n            image_path, label_path = item\r\n            name = osp.splitext(osp.basename(label_path))[0]\r\n            img_file = osp.join(self.root, image_path)\r\n            label_file = osp.join(self.root, label_path)\r\n            self.files.append({\r\n                ""img"": img_file,\r\n                ""label"": label_file,\r\n                ""name"": name\r\n            })\r\n        self.id_to_trainid = {-1: ignore_label, 0: ignore_label, 1: ignore_label, 2: ignore_label,\r\n                              3: ignore_label, 4: ignore_label, 5: ignore_label, 6: ignore_label,\r\n                              7: 0, 8: 1, 9: ignore_label, 10: ignore_label, 11: 2, 12: 3, 13: 4,\r\n                              14: ignore_label, 15: ignore_label, 16: ignore_label, 17: 5,\r\n                              18: ignore_label, 19: 6, 20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13, 27: 14,\r\n                              28: 15, 29: ignore_label, 30: ignore_label, 31: 16, 32: 17, 33: 18}\r\n        print(\'{} images are loaded!\'.format(len(self.img_ids)))\r\n\r\n    def __len__(self):\r\n        return len(self.files)\r\n\r\n    def generate_scale_label(self, image, label):\r\n        f_scale = 0.7 + random.randint(0, 14) / 10.0\r\n        image = cv2.resize(image, None, fx=f_scale, fy=f_scale, interpolation = cv2.INTER_LINEAR)\r\n        label = cv2.resize(label, None, fx=f_scale, fy=f_scale, interpolation = cv2.INTER_NEAREST)\r\n        return image, label\r\n\r\n    def id2trainId(self, label, reverse=False):\r\n        label_copy = label.copy()\r\n        if reverse:\r\n            for v, k in self.id_to_trainid.items():\r\n                label_copy[label == k] = v\r\n        else:\r\n            for k, v in self.id_to_trainid.items():\r\n                label_copy[label == k] = v\r\n        return label_copy\r\n\r\n    def __getitem__(self, index):\r\n        datafiles = self.files[index]\r\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\r\n        label = cv2.imread(datafiles[""label""], cv2.IMREAD_GRAYSCALE)\r\n        label = self.id2trainId(label)\r\n        size = image.shape\r\n        name = datafiles[""name""]\r\n        if self.scale:\r\n            image, label = self.generate_scale_label(image, label)\r\n        image = np.asarray(image, np.float32)\r\n        image -= self.mean\r\n        img_h, img_w = label.shape\r\n        pad_h = max(self.crop_h - img_h, 0)\r\n        pad_w = max(self.crop_w - img_w, 0)\r\n        if pad_h > 0 or pad_w > 0:\r\n            img_pad = cv2.copyMakeBorder(image, 0, pad_h, 0, \r\n                pad_w, cv2.BORDER_CONSTANT, \r\n                value=(0.0, 0.0, 0.0))\r\n            label_pad = cv2.copyMakeBorder(label, 0, pad_h, 0, \r\n                pad_w, cv2.BORDER_CONSTANT,\r\n                value=(self.ignore_label,))\r\n        else:\r\n            img_pad, label_pad = image, label\r\n\r\n        img_h, img_w = label_pad.shape\r\n        h_off = random.randint(0, img_h - self.crop_h)\r\n        w_off = random.randint(0, img_w - self.crop_w)\r\n        # roi = cv2.Rect(w_off, h_off, self.crop_w, self.crop_h);\r\n        image = np.asarray(img_pad[h_off : h_off+self.crop_h, w_off : w_off+self.crop_w], np.float32)\r\n        label = np.asarray(label_pad[h_off : h_off+self.crop_h, w_off : w_off+self.crop_w], np.float32)\r\n        #image = image[:, :, ::-1]  # change to BGR\r\n        image = image.transpose((2, 0, 1))\r\n        if self.is_mirror:\r\n            flip = np.random.choice(2) * 2 - 1\r\n            image = image[:, :, ::flip]\r\n            label = label[:, ::flip]\r\n\r\n        return image.copy(), label.copy(), np.array(size), name\r\n\r\n\r\nclass CSDataTestSet(data.Dataset):\r\n    def __init__(self, root, list_path, crop_size=(505, 505), mean=(128, 128, 128)):\r\n        self.root = root\r\n        self.list_path = list_path\r\n        self.crop_h, self.crop_w = crop_size\r\n        self.mean = mean\r\n        # self.mean_bgr = np.array([104.00698793, 116.66876762, 122.67891434])\r\n        self.img_ids = [i_id.strip() for i_id in open(list_path)]\r\n        self.files = [] \r\n        # for split in [""train"", ""trainval"", ""val""]:\r\n        for item in self.img_ids:\r\n            image_path = item\r\n            name = osp.splitext(osp.basename(image_path))[0]\r\n            img_file = osp.join(self.root, image_path)\r\n            self.files.append({\r\n                ""img"": img_file\r\n            })\r\n\r\n    def __len__(self):\r\n        return len(self.files)\r\n\r\n    def __getitem__(self, index):\r\n        datafiles = self.files[index]\r\n        image = cv2.imread(datafiles[""img""], cv2.IMREAD_COLOR)\r\n        size = image.shape\r\n        name = osp.splitext(osp.basename(datafiles[""img""]))[0]\r\n        image = np.asarray(image, np.float32)\r\n        image -= self.mean\r\n        \r\n        img_h, img_w, _ = image.shape\r\n        pad_h = max(self.crop_h - img_h, 0)\r\n        pad_w = max(self.crop_w - img_w, 0)\r\n        if pad_h > 0 or pad_w > 0:\r\n            image = cv2.copyMakeBorder(image, 0, pad_h, 0, \r\n                pad_w, cv2.BORDER_CONSTANT, \r\n                value=(0.0, 0.0, 0.0))\r\n        image = image.transpose((2, 0, 1))\r\n        return image, np.array(size), name\r\n\r\nif __name__ == \'__main__\':\r\n    dst = VOCDataSet(""./data"", is_transform=True)\r\n    trainloader = data.DataLoader(dst, batch_size=4)\r\n    for i, data in enumerate(trainloader):\r\n        imgs, labels = data\r\n        if i == 0:\r\n            img = torchvision.utils.make_grid(imgs).numpy()\r\n            img = np.transpose(img, (1, 2, 0))\r\n            img = img[:, :, ::-1]\r\n            plt.imshow(img)\r\n            plt.show()\r\n'"
libs/__init__.py,0,"b'from .bn import ABN, InPlaceABN, InPlaceABNWrapper, InPlaceABNSync, InPlaceABNSyncWrapper\nfrom .misc import GlobalAvgPool2d\nfrom .residual import IdentityResidualBlock\nfrom .dense import DenseModule\n'"
libs/bn.py,11,"b'from collections import OrderedDict, Iterable\nfrom itertools import repeat\n\ntry:\n    # python 3\n    from queue import Queue\nexcept ImportError:\n    # python 2\n    from Queue import Queue\n\nimport torch\nimport torch.nn as nn\nimport torch.autograd as autograd\n\nfrom .functions import inplace_abn, inplace_abn_sync\n\n\ndef _pair(x):\n    if isinstance(x, Iterable):\n        return x\n    return tuple(repeat(x, 2))\n\n\nclass ABN(nn.Sequential):\n    """"""Activated Batch Normalization\n\n    This gathers a `BatchNorm2d` and an activation function in a single module\n    """"""\n\n    def __init__(self, num_features, activation=nn.ReLU(inplace=True), **kwargs):\n        """"""Creates an Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        activation : nn.Module\n            Module used as an activation function.\n        kwargs\n            All other arguments are forwarded to the `BatchNorm2d` constructor.\n        """"""\n        super(ABN, self).__init__(OrderedDict([\n            (""bn"", nn.BatchNorm2d(num_features, **kwargs)),\n            (""act"", activation)\n        ]))\n\n\nclass InPlaceABN(nn.Module):\n    """"""InPlace Activated Batch Normalization""""""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"", slope=0.01):\n        """"""Creates an InPlace Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(InPlaceABN, self).__init__()\n        self.num_features = num_features\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        self.activation = activation\n        self.slope = slope\n        if self.affine:\n            self.weight = nn.Parameter(torch.Tensor(num_features))\n            self.bias = nn.Parameter(torch.Tensor(num_features))\n        else:\n            self.register_parameter(\'weight\', None)\n            self.register_parameter(\'bias\', None)\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        self.running_mean.zero_()\n        self.running_var.fill_(1)\n        if self.affine:\n            self.weight.data.fill_(1)\n            self.bias.data.zero_()\n\n    def forward(self, x):\n        return inplace_abn(x, self.weight, self.bias, autograd.Variable(self.running_mean),\n                           autograd.Variable(self.running_var), self.training, self.momentum, self.eps,\n                           self.activation, self.slope)\n\n    def __repr__(self):\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n              \' affine={affine}, activation={activation}\'\n        if self.activation == ""leaky_relu"":\n            rep += \' slope={slope})\'\n        else:\n            rep += \')\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass InPlaceABNSync(nn.Module):\n    """"""InPlace Activated Batch Normalization with cross-GPU synchronization\n\n    This assumes that it will be replicated across GPUs using the same mechanism as in `nn.DataParallel`.\n    """"""\n\n    def __init__(self, num_features, devices=None, eps=1e-5, momentum=0.1, affine=True, activation=""leaky_relu"",\n                 slope=0.01):\n        """"""Creates a synchronized, InPlace Activated Batch Normalization module\n\n        Parameters\n        ----------\n        num_features : int\n            Number of feature channels in the input and output.\n        devices : list of int or None\n            IDs of the GPUs that will run the replicas of this module.\n        eps : float\n            Small constant to prevent numerical issues.\n        momentum : float\n            Momentum factor applied to compute running statistics as.\n        affine : bool\n            If `True` apply learned scale and shift transformation after normalization.\n        activation : str\n            Name of the activation functions, one of: `leaky_relu`, `elu` or `none`.\n        slope : float\n            Negative slope for the `leaky_relu` activation.\n        """"""\n        super(InPlaceABNSync, self).__init__()\n        self.num_features = num_features\n        self.devices = devices if devices else list(range(torch.cuda.device_count()))\n        self.affine = affine\n        self.eps = eps\n        self.momentum = momentum\n        self.activation = activation\n        self.slope = slope\n        if self.affine:\n            self.weight = nn.Parameter(torch.Tensor(num_features))\n            self.bias = nn.Parameter(torch.Tensor(num_features))\n        else:\n            self.register_parameter(\'weight\', None)\n            self.register_parameter(\'bias\', None)\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n        self.reset_parameters()\n\n        # Initialize queues\n        self.worker_ids = self.devices[1:]\n        self.master_queue = Queue(len(self.worker_ids))\n        self.worker_queues = [Queue(1) for _ in self.worker_ids]\n\n    def reset_parameters(self):\n        self.running_mean.zero_()\n        self.running_var.fill_(1)\n        if self.affine:\n            self.weight.data.fill_(1)\n            self.bias.data.zero_()\n\n    def forward(self, x):\n        if x.get_device() == self.devices[0]:\n            # Master mode\n            extra = {\n                ""is_master"": True,\n                ""master_queue"": self.master_queue,\n                ""worker_queues"": self.worker_queues,\n                ""worker_ids"": self.worker_ids\n            }\n        else:\n            # Worker mode\n            extra = {\n                ""is_master"": False,\n                ""master_queue"": self.master_queue,\n                ""worker_queue"": self.worker_queues[self.worker_ids.index(x.get_device())]\n            }\n\n        return inplace_abn_sync(x, self.weight, self.bias, autograd.Variable(self.running_mean),\n                                autograd.Variable(self.running_var), extra, self.training, self.momentum, self.eps,\n                                self.activation, self.slope)\n\n    def __repr__(self):\n        rep = \'{name}({num_features}, eps={eps}, momentum={momentum},\' \\\n              \' affine={affine}, devices={devices}, activation={activation}\'\n        if self.activation == ""leaky_relu"":\n            rep += \' slope={slope})\'\n        else:\n            rep += \')\'\n        return rep.format(name=self.__class__.__name__, **self.__dict__)\n\n\nclass InPlaceABNWrapper(nn.Module):\n    """"""Wrapper module to make `InPlaceABN` compatible with `ABN`""""""\n\n    def __init__(self, *args, **kwargs):\n        super(InPlaceABNWrapper, self).__init__()\n        self.bn = InPlaceABN(*args, **kwargs)\n\n    def forward(self, input):\n        return self.bn(input)\n\n\nclass InPlaceABNSyncWrapper(nn.Module):\n    """"""Wrapper module to make `InPlaceABNSync` compatible with `ABN`""""""\n\n    def __init__(self, *args, **kwargs):\n        super(InPlaceABNSyncWrapper, self).__init__()\n        self.bn = InPlaceABNSync(*args, **kwargs)\n\n    def forward(self, input):\n        return self.bn(input)\n'"
libs/build.py,1,"b'import os\n\nfrom torch.utils.ffi import create_extension\n\nsources = [\'src/lib_cffi.cpp\']\nheaders = [\'src/lib_cffi.h\']\nextra_objects = [\'src/bn.o\']\nwith_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    \'_ext\',\n    headers=headers,\n    sources=sources,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects,\n    extra_compile_args=[""-std=c++11""]\n)\n\nif __name__ == \'__main__\':\n    ffi.build()\n'"
libs/dense.py,3,"b'from collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\nfrom .bn import ABN\n\n\nclass DenseModule(nn.Module):\n    def __init__(self, in_channels, growth, layers, bottleneck_factor=4, norm_act=ABN, dilation=1):\n        super(DenseModule, self).__init__()\n        self.in_channels = in_channels\n        self.growth = growth\n        self.layers = layers\n\n        self.convs1 = nn.ModuleList()\n        self.convs3 = nn.ModuleList()\n        for i in range(self.layers):\n            self.convs1.append(nn.Sequential(OrderedDict([\n                (""bn"", norm_act(in_channels)),\n                (""conv"", nn.Conv2d(in_channels, self.growth * bottleneck_factor, 1, bias=False))\n            ])))\n            self.convs3.append(nn.Sequential(OrderedDict([\n                (""bn"", norm_act(self.growth * bottleneck_factor)),\n                (""conv"", nn.Conv2d(self.growth * bottleneck_factor, self.growth, 3, padding=dilation, bias=False,\n                                   dilation=dilation))\n            ])))\n            in_channels += self.growth\n\n    @property\n    def out_channels(self):\n        return self.in_channels + self.growth * self.layers\n\n    def forward(self, x):\n        inputs = [x]\n        for i in range(self.layers):\n            x = torch.cat(inputs, dim=1)\n            x = self.convs1[i](x)\n            x = self.convs3[i](x)\n            inputs += [x]\n\n        return torch.cat(inputs, dim=1)'"
libs/functions.py,3,"b'import torch.autograd as autograd\nimport torch.cuda.comm as comm\nfrom torch.autograd.function import once_differentiable\n\nfrom . import _ext\n\n# Activation names\nACT_LEAKY_RELU = ""leaky_relu""\nACT_ELU = ""elu""\nACT_NONE = ""none""\n\n\ndef _check(fn, *args, **kwargs):\n    success = fn(*args, **kwargs)\n    if not success:\n        raise RuntimeError(""CUDA Error encountered in {}"".format(fn))\n\n\ndef _broadcast_shape(x):\n    out_size = []\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            out_size.append(1)\n        else:\n            out_size.append(s)\n    return out_size\n\n\ndef _reduce(x):\n    if len(x.size()) == 2:\n        return x.sum(dim=0)\n    else:\n        n, c = x.size()[0:2]\n        return x.contiguous().view((n, c, -1)).sum(2).sum(0)\n\n\ndef _count_samples(x):\n    count = 1\n    for i, s in enumerate(x.size()):\n        if i != 1:\n            count *= s\n    return count\n\n\ndef _act_forward(ctx, x):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _check(_ext.leaky_relu_cuda, x, ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _check(_ext.elu_cuda, x)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\ndef _act_backward(ctx, x, dx):\n    if ctx.activation == ACT_LEAKY_RELU:\n        _check(_ext.leaky_relu_backward_cuda, x, dx, ctx.slope)\n        _check(_ext.leaky_relu_cuda, x, 1. / ctx.slope)\n    elif ctx.activation == ACT_ELU:\n        _check(_ext.elu_backward_cuda, x, dx)\n        _check(_ext.elu_inv_cuda, x)\n    elif ctx.activation == ACT_NONE:\n        pass\n\n\ndef _check_contiguous(*args):\n    if not all([mod is None or mod.is_contiguous() for mod in args]):\n        raise ValueError(""Non-contiguous input"")\n\n\nclass InPlaceABN(autograd.Function):\n    @staticmethod\n    def forward(ctx, x, weight, bias, running_mean, running_var,\n                training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01):\n        # Save context\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n\n        n = _count_samples(x)\n\n        if ctx.training:\n            mean = x.new().resize_as_(running_mean)\n            var = x.new().resize_as_(running_var)\n            _check_contiguous(x, mean, var)\n            _check(_ext.bn_mean_var_cuda, x, mean, var)\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * n / (n - 1))\n        else:\n            mean, var = running_mean, running_var\n\n        _check_contiguous(x, mean, var, weight, bias)\n        _check(_ext.bn_forward_cuda,\n               x, mean, var,\n               weight if weight is not None else x.new(),\n               bias if bias is not None else x.new(),\n               x, x, ctx.eps)\n\n        # Activation\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, weight, bias, running_mean, running_var)\n        ctx.mark_dirty(x)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, weight, bias, running_mean, running_var = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.needs_input_grad[0]:\n            dx = dz.new().resize_as_(dz)\n        else:\n            dx = None\n\n        if ctx.needs_input_grad[1]:\n            dweight = dz.new().resize_as_(running_mean).zero_()\n        else:\n            dweight = None\n\n        if ctx.needs_input_grad[2]:\n            dbias = dz.new().resize_as_(running_mean).zero_()\n        else:\n            dbias = None\n\n        if ctx.training:\n            edz = dz.new().resize_as_(running_mean)\n            eydz = dz.new().resize_as_(running_mean)\n            _check_contiguous(z, dz, weight, bias, edz, eydz)\n            _check(_ext.bn_edz_eydz_cuda,\n                   z, dz,\n                   weight if weight is not None else dz.new(),\n                   bias if bias is not None else dz.new(),\n                   edz, eydz, ctx.eps)\n        else:\n            # TODO: implement CUDA backward for inference mode\n            edz = dz.new().resize_as_(running_mean).zero_()\n            eydz = dz.new().resize_as_(running_mean).zero_()\n\n        _check_contiguous(dz, z, ctx.var, weight, bias, edz, eydz, dx, dweight, dbias)\n        _check(_ext.bn_backard_cuda,\n               dz, z, ctx.var,\n               weight if weight is not None else dz.new(),\n               bias if bias is not None else dz.new(),\n               edz, eydz,\n               dx if dx is not None else dz.new(),\n               dweight if dweight is not None else dz.new(),\n               dbias if dbias is not None else dz.new(),\n               ctx.eps)\n\n        del ctx.var\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None\n\n\nclass InPlaceABNSync(autograd.Function):\n    @classmethod\n    def forward(cls, ctx, x, weight, bias, running_mean, running_var,\n                extra, training=True, momentum=0.1, eps=1e-05, activation=ACT_LEAKY_RELU, slope=0.01):\n        # Save context\n        cls._parse_extra(ctx, extra)\n        ctx.training = training\n        ctx.momentum = momentum\n        ctx.eps = eps\n        ctx.activation = activation\n        ctx.slope = slope\n\n        n = _count_samples(x) * (ctx.master_queue.maxsize + 1)\n\n        if ctx.training:\n            mean = x.new().resize_(1, running_mean.size(0))\n            var = x.new().resize_(1, running_var.size(0))\n            _check_contiguous(x, mean, var)\n            _check(_ext.bn_mean_var_cuda, x, mean, var)\n\n            if ctx.is_master:\n                means, vars = [mean], [var]\n                for _ in range(ctx.master_queue.maxsize):\n                    mean_w, var_w = ctx.master_queue.get()\n                    ctx.master_queue.task_done()\n                    means.append(mean_w)\n                    vars.append(var_w)\n\n                means = comm.gather(means)\n                vars = comm.gather(vars)\n\n                mean = means.mean(0)\n                var = (vars + (mean - means) ** 2).mean(0)\n\n                tensors = comm.broadcast_coalesced((mean, var), [mean.get_device()] + ctx.worker_ids)\n                for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                    queue.put(ts)\n            else:\n                ctx.master_queue.put((mean, var))\n                mean, var = ctx.worker_queue.get()\n                ctx.worker_queue.task_done()\n\n            # Update running stats\n            running_mean.mul_((1 - ctx.momentum)).add_(ctx.momentum * mean)\n            running_var.mul_((1 - ctx.momentum)).add_(ctx.momentum * var * n / (n - 1))\n        else:\n            mean, var = running_mean, running_var\n\n        _check_contiguous(x, mean, var, weight, bias)\n        _check(_ext.bn_forward_cuda,\n               x, mean, var,\n               weight if weight is not None else x.new(),\n               bias if bias is not None else x.new(),\n               x, x, ctx.eps)\n\n        # Activation\n        _act_forward(ctx, x)\n\n        # Output\n        ctx.var = var\n        ctx.save_for_backward(x, weight, bias, running_mean, running_var)\n        ctx.mark_dirty(x)\n        return x\n\n    @staticmethod\n    @once_differentiable\n    def backward(ctx, dz):\n        z, weight, bias, running_mean, running_var = ctx.saved_tensors\n        dz = dz.contiguous()\n\n        # Undo activation\n        _act_backward(ctx, z, dz)\n\n        if ctx.needs_input_grad[0]:\n            dx = dz.new().resize_as_(dz)\n        else:\n            dx = None\n\n        if ctx.needs_input_grad[1]:\n            dweight = dz.new().resize_as_(running_mean).zero_()\n        else:\n            dweight = None\n\n        if ctx.needs_input_grad[2]:\n            dbias = dz.new().resize_as_(running_mean).zero_()\n        else:\n            dbias = None\n\n        if ctx.training:\n            edz = dz.new().resize_as_(running_mean)\n            eydz = dz.new().resize_as_(running_mean)\n            _check_contiguous(z, dz, weight, bias, edz, eydz)\n            _check(_ext.bn_edz_eydz_cuda,\n                   z, dz,\n                   weight if weight is not None else dz.new(),\n                   bias if bias is not None else dz.new(),\n                   edz, eydz, ctx.eps)\n\n            if ctx.is_master:\n                edzs, eydzs = [edz], [eydz]\n                for _ in range(len(ctx.worker_queues)):\n                    edz_w, eydz_w = ctx.master_queue.get()\n                    ctx.master_queue.task_done()\n                    edzs.append(edz_w)\n                    eydzs.append(eydz_w)\n\n                edz = comm.reduce_add(edzs) / (ctx.master_queue.maxsize + 1)\n                eydz = comm.reduce_add(eydzs) / (ctx.master_queue.maxsize + 1)\n\n                tensors = comm.broadcast_coalesced((edz, eydz), [edz.get_device()] + ctx.worker_ids)\n                for ts, queue in zip(tensors[1:], ctx.worker_queues):\n                    queue.put(ts)\n            else:\n                ctx.master_queue.put((edz, eydz))\n                edz, eydz = ctx.worker_queue.get()\n                ctx.worker_queue.task_done()\n        else:\n            edz = dz.new().resize_as_(running_mean).zero_()\n            eydz = dz.new().resize_as_(running_mean).zero_()\n\n        _check_contiguous(dz, z, ctx.var, weight, bias, edz, eydz, dx, dweight, dbias)\n        _check(_ext.bn_backard_cuda,\n               dz, z, ctx.var,\n               weight if weight is not None else dz.new(),\n               bias if bias is not None else dz.new(),\n               edz, eydz,\n               dx if dx is not None else dz.new(),\n               dweight if dweight is not None else dz.new(),\n               dbias if dbias is not None else dz.new(),\n               ctx.eps)\n\n        del ctx.var\n\n        return dx, dweight, dbias, None, None, None, None, None, None, None, None\n\n    @staticmethod\n    def _parse_extra(ctx, extra):\n        ctx.is_master = extra[""is_master""]\n        if ctx.is_master:\n            ctx.master_queue = extra[""master_queue""]\n            ctx.worker_queues = extra[""worker_queues""]\n            ctx.worker_ids = extra[""worker_ids""]\n        else:\n            ctx.master_queue = extra[""master_queue""]\n            ctx.worker_queue = extra[""worker_queue""]\n\n\ninplace_abn = InPlaceABN.apply\ninplace_abn_sync = InPlaceABNSync.apply\n\n__all__ = [""inplace_abn"", ""inplace_abn_sync""]\n'"
libs/misc.py,1,"b'import torch.nn as nn\n\n\nclass GlobalAvgPool2d(nn.Module):\n    def __init__(self):\n        """"""Global average pooling over the input\'s spatial dimensions""""""\n        super(GlobalAvgPool2d, self).__init__()\n\n    def forward(self, inputs):\n        in_size = inputs.size()\n        return inputs.view((in_size[0], in_size[1], -1)).mean(dim=2)\n'"
libs/residual.py,1,"b'from collections import OrderedDict\n\nimport torch.nn as nn\n\nfrom .bn import ABN\n\n\nclass IdentityResidualBlock(nn.Module):\n    def __init__(self,\n                 in_channels,\n                 channels,\n                 stride=1,\n                 dilation=1,\n                 groups=1,\n                 norm_act=ABN,\n                 dropout=None):\n        """"""Configurable identity-mapping residual block\n\n        Parameters\n        ----------\n        in_channels : int\n            Number of input channels.\n        channels : list of int\n            Number of channels in the internal feature maps. Can either have two or three elements: if three construct\n            a residual block with two `3 x 3` convolutions, otherwise construct a bottleneck block with `1 x 1`, then\n            `3 x 3` then `1 x 1` convolutions.\n        stride : int\n            Stride of the first `3 x 3` convolution\n        dilation : int\n            Dilation to apply to the `3 x 3` convolutions.\n        groups : int\n            Number of convolution groups. This is used to create ResNeXt-style blocks and is only compatible with\n            bottleneck blocks.\n        norm_act : callable\n            Function to create normalization / activation Module.\n        dropout: callable\n            Function to create Dropout Module.\n        """"""\n        super(IdentityResidualBlock, self).__init__()\n\n        # Check parameters for inconsistencies\n        if len(channels) != 2 and len(channels) != 3:\n            raise ValueError(""channels must contain either two or three values"")\n        if len(channels) == 2 and groups != 1:\n            raise ValueError(""groups > 1 are only valid if len(channels) == 3"")\n\n        is_bottleneck = len(channels) == 3\n        need_proj_conv = stride != 1 or in_channels != channels[-1]\n\n        self.bn1 = norm_act(in_channels)\n        if not is_bottleneck:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels, channels[0], 3, stride=stride, padding=dilation, bias=False,\n                                    dilation=dilation)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    dilation=dilation))\n            ]\n            if dropout is not None:\n                layers = layers[0:2] + [(""dropout"", dropout())] + layers[2:]\n        else:\n            layers = [\n                (""conv1"", nn.Conv2d(in_channels, channels[0], 1, stride=stride, padding=0, bias=False)),\n                (""bn2"", norm_act(channels[0])),\n                (""conv2"", nn.Conv2d(channels[0], channels[1], 3, stride=1, padding=dilation, bias=False,\n                                    groups=groups, dilation=dilation)),\n                (""bn3"", norm_act(channels[1])),\n                (""conv3"", nn.Conv2d(channels[1], channels[2], 1, stride=1, padding=0, bias=False))\n            ]\n            if dropout is not None:\n                layers = layers[0:4] + [(""dropout"", dropout())] + layers[4:]\n        self.convs = nn.Sequential(OrderedDict(layers))\n\n        if need_proj_conv:\n            self.proj_conv = nn.Conv2d(in_channels, channels[-1], 1, stride=stride, padding=0, bias=False)\n\n    def forward(self, x):\n        if hasattr(self, ""proj_conv""):\n            bn1 = self.bn1(x)\n            shortcut = self.proj_conv(bn1)\n        else:\n            shortcut = x.clone()\n            bn1 = self.bn1(x)\n\n        out = self.convs(bn1)\n        out.add_(shortcut)\n\n        return out\n'"
networks/__init__.py,0,b''
networks/ccnet.py,6,"b'#import encoding.nn as nn\r\n#import encoding.functions as F\r\nimport torch.nn as nn\r\nfrom torch.nn import functional as F\r\nimport math\r\nimport torch.utils.model_zoo as model_zoo\r\nimport torch\r\nimport numpy as np\r\nfrom torch.autograd import Variable\r\naffine_par = True\r\nimport functools\r\n\r\nimport sys, os\r\n\r\nfrom libs import InPlaceABN, InPlaceABNSync\r\nfrom cc_attention import CrissCrossAttention\r\n\r\n\r\nBatchNorm2d = functools.partial(InPlaceABNSync, activation=\'none\')\r\n\r\ndef outS(i):\r\n    i = int(i)\r\n    i = (i+1)/2\r\n    i = int(np.ceil((i+1)/2.0))\r\n    i = (i+1)/2\r\n    return i\r\n\r\ndef conv3x3(in_planes, out_planes, stride=1):\r\n    ""3x3 convolution with padding""\r\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n                     padding=1, bias=False)\r\n\r\n\r\nclass Bottleneck(nn.Module):\r\n    expansion = 4\r\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None, fist_dilation=1, multi_grid=1):\r\n        super(Bottleneck, self).__init__()\r\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\r\n        self.bn1 = BatchNorm2d(planes)\r\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\r\n                               padding=dilation*multi_grid, dilation=dilation*multi_grid, bias=False)\r\n        self.bn2 = BatchNorm2d(planes)\r\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\r\n        self.bn3 = BatchNorm2d(planes * 4)\r\n        self.relu = nn.ReLU(inplace=False)\r\n        self.relu_inplace = nn.ReLU(inplace=True)\r\n        self.downsample = downsample\r\n        self.dilation = dilation\r\n        self.stride = stride\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv2(out)\r\n        out = self.bn2(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv3(out)\r\n        out = self.bn3(out)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(x)\r\n\r\n        out = out + residual      \r\n        out = self.relu_inplace(out)\r\n\r\n        return out\r\n\r\nclass PSPModule(nn.Module):\r\n    """"""\r\n    Reference: \r\n        Zhao, Hengshuang, et al. *""Pyramid scene parsing network.""*\r\n    """"""\r\n    def __init__(self, features, out_features=512, sizes=(1, 2, 3, 6)):\r\n        super(PSPModule, self).__init__()\r\n\r\n        self.stages = []\r\n        self.stages = nn.ModuleList([self._make_stage(features, out_features, size) for size in sizes])\r\n        self.bottleneck = nn.Sequential(\r\n            nn.Conv2d(features+len(sizes)*out_features, out_features, kernel_size=3, padding=1, dilation=1, bias=False),\r\n            InPlaceABNSync(out_features),\r\n            nn.Dropout2d(0.1)\r\n            )\r\n\r\n    def _make_stage(self, features, out_features, size):\r\n        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\r\n        conv = nn.Conv2d(features, out_features, kernel_size=1, bias=False)\r\n        bn = InPlaceABNSync(out_features)\r\n        return nn.Sequential(prior, conv, bn)\r\n\r\n    def forward(self, feats):\r\n        h, w = feats.size(2), feats.size(3)\r\n        priors = [F.upsample(input=stage(feats), size=(h, w), mode=\'bilinear\', align_corners=True) for stage in self.stages] + [feats]\r\n        bottle = self.bottleneck(torch.cat(priors, 1))\r\n        return bottle\r\n\r\nclass RCCAModule(nn.Module):\r\n    def __init__(self, in_channels, out_channels, num_classes):\r\n        super(RCCAModule, self).__init__()\r\n        inter_channels = in_channels // 4\r\n        self.conva = nn.Sequential(nn.Conv2d(in_channels, inter_channels, 3, padding=1, bias=False),\r\n                                   InPlaceABNSync(inter_channels))\r\n        self.cca = CrissCrossAttention(inter_channels)\r\n        self.convb = nn.Sequential(nn.Conv2d(inter_channels, inter_channels, 3, padding=1, bias=False),\r\n                                   InPlaceABNSync(inter_channels))\r\n\r\n        self.bottleneck = nn.Sequential(\r\n            nn.Conv2d(in_channels+inter_channels, out_channels, kernel_size=3, padding=1, dilation=1, bias=False),\r\n            InPlaceABNSync(out_channels),\r\n            nn.Dropout2d(0.1),\r\n            nn.Conv2d(512, num_classes, kernel_size=1, stride=1, padding=0, bias=True)\r\n            )\r\n\r\n    def forward(self, x, recurrence=1):\r\n        output = self.conva(x)\r\n        for i in range(recurrence):\r\n            output = self.cca(output)\r\n        output = self.convb(output)\r\n\r\n        output = self.bottleneck(torch.cat([x, output], 1))\r\n        return output\r\n\r\nclass ResNet(nn.Module):\r\n    def __init__(self, block, layers, num_classes):\r\n        self.inplanes = 128\r\n        super(ResNet, self).__init__()\r\n        self.conv1 = conv3x3(3, 64, stride=2)\r\n        self.bn1 = BatchNorm2d(64)\r\n        self.relu1 = nn.ReLU(inplace=False)\r\n        self.conv2 = conv3x3(64, 64)\r\n        self.bn2 = BatchNorm2d(64)\r\n        self.relu2 = nn.ReLU(inplace=False)\r\n        self.conv3 = conv3x3(64, 128)\r\n        self.bn3 = BatchNorm2d(128)\r\n        self.relu3 = nn.ReLU(inplace=False)\r\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n\r\n        self.relu = nn.ReLU(inplace=False)\r\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True) # change\r\n        self.layer1 = self._make_layer(block, 64, layers[0])\r\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\r\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\r\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4, multi_grid=(1,1,1))\r\n        #self.layer5 = PSPModule(2048, 512)\r\n        self.head = RCCAModule(2048, 512, num_classes)\r\n\r\n        self.dsn = nn.Sequential(\r\n            nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1),\r\n            InPlaceABNSync(512),\r\n            nn.Dropout2d(0.1),\r\n            nn.Conv2d(512, num_classes, kernel_size=1, stride=1, padding=0, bias=True)\r\n            )\r\n\r\n    def get_learnable_parameters(self, freeze_layers=[True,True,True,True,False,False,False]):\r\n        lr_parameters = []\r\n\r\n        if not freeze_layers[0]:\r\n            for i in [self.conv1, self.bn1, self.conv2, self.bn2, self.conv3, self.bn3]:\r\n                params = i.named_parameters()\r\n                for name, p in params:\r\n                    print(name)\r\n                    lr_parameters.append(p)\r\n\r\n        layers = [self.layer1, self.layer2, self.layer3, self.layer4, self.layer5, self.layer6]\r\n        for freeze, layer in zip(freeze_layers[1:], layers):\r\n            if not freeze:\r\n                params = layer.named_parameters()\r\n                for name, p in params:\r\n                    print(name)\r\n                    lr_parameters.append(p)\r\n\r\n        return lr_parameters\r\n\r\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, multi_grid=1):\r\n        downsample = None\r\n        if stride != 1 or self.inplanes != planes * block.expansion:\r\n            downsample = nn.Sequential(\r\n                nn.Conv2d(self.inplanes, planes * block.expansion,\r\n                          kernel_size=1, stride=stride, bias=False),\r\n                BatchNorm2d(planes * block.expansion,affine = affine_par))\r\n\r\n        layers = []\r\n        generate_multi_grid = lambda index, grids: grids[index%len(grids)] if isinstance(grids, tuple) else 1\r\n        layers.append(block(self.inplanes, planes, stride,dilation=dilation, downsample=downsample, multi_grid=generate_multi_grid(0, multi_grid)))\r\n        self.inplanes = planes * block.expansion\r\n        for i in range(1, blocks):\r\n            layers.append(block(self.inplanes, planes, dilation=dilation, multi_grid=generate_multi_grid(i, multi_grid)))\r\n\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x, recurrence=1):\r\n        x = self.relu1(self.bn1(self.conv1(x)))\r\n        x = self.relu2(self.bn2(self.conv2(x)))\r\n        x = self.relu3(self.bn3(self.conv3(x)))\r\n        x = self.maxpool(x)\r\n        x = self.layer1(x)\r\n        x = self.layer2(x)\r\n        x = self.layer3(x)\r\n        x_dsn = self.dsn(x)\r\n        x = self.layer4(x)\r\n        x = self.head(x, recurrence)\r\n        return [x, x_dsn]\r\n\r\n\r\ndef Res_Deeplab(num_classes=21):\r\n    model = ResNet(Bottleneck,[3, 4, 23, 3], num_classes)\r\n    return model\r\n\r\n'"
networks/deeplabv3.py,5,"b'import torch.nn as nn\r\nfrom torch.nn import functional as F\r\nimport math\r\nimport torch.utils.model_zoo as model_zoo\r\nimport torch\r\nimport numpy as np\r\nfrom torch.autograd import Variable\r\naffine_par = True\r\nimport functools\r\n\r\nimport sys, os\r\n\r\nfrom libs import InPlaceABN, InPlaceABNSync\r\nBatchNorm2d = functools.partial(InPlaceABNSync, activation=\'none\')\r\n\r\ndef conv3x3(in_planes, out_planes, stride=1):\r\n    ""3x3 convolution with padding""\r\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n                     padding=1, bias=False)\r\n\r\n\r\nclass Bottleneck(nn.Module):\r\n    expansion = 4\r\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None, fist_dilation=1, multi_grid=1):\r\n        super(Bottleneck, self).__init__()\r\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\r\n        self.bn1 = BatchNorm2d(planes)\r\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\r\n                               padding=dilation*multi_grid, dilation=dilation*multi_grid, bias=False)\r\n        self.bn2 = BatchNorm2d(planes)\r\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\r\n        self.bn3 = BatchNorm2d(planes * 4)\r\n        self.relu = nn.ReLU(inplace=False)\r\n        self.relu_inplace = nn.ReLU(inplace=True)\r\n        self.downsample = downsample\r\n        self.dilation = dilation\r\n        self.stride = stride\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv2(out)\r\n        out = self.bn2(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv3(out)\r\n        out = self.bn3(out)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(x)\r\n\r\n        out = out + residual      \r\n        out = self.relu_inplace(out)\r\n\r\n        return out\r\n\r\nclass ASPPModule(nn.Module):\r\n    """"""\r\n    Reference: \r\n        Chen, Liang-Chieh, et al. *""Rethinking Atrous Convolution for Semantic Image Segmentation.""*\r\n    """"""\r\n    def __init__(self, features, inner_features=256, out_features=512, dilations=(12, 24, 36)):\r\n        super(ASPPModule, self).__init__()\r\n\r\n        self.conv1 = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)),\r\n                                   nn.Conv2d(features, inner_features, kernel_size=1, padding=0, dilation=1, bias=False),\r\n                                   InPlaceABNSync(inner_features))\r\n        self.conv2 = nn.Sequential(nn.Conv2d(features, inner_features, kernel_size=1, padding=0, dilation=1, bias=False),\r\n                                   InPlaceABNSync(inner_features))\r\n        self.conv3 = nn.Sequential(nn.Conv2d(features, inner_features, kernel_size=3, padding=dilations[0], dilation=dilations[0], bias=False),\r\n                                   InPlaceABNSync(inner_features))\r\n        self.conv4 = nn.Sequential(nn.Conv2d(features, inner_features, kernel_size=3, padding=dilations[1], dilation=dilations[1], bias=False),\r\n                                   InPlaceABNSync(inner_features))\r\n        self.conv5 = nn.Sequential(nn.Conv2d(features, inner_features, kernel_size=3, padding=dilations[2], dilation=dilations[2], bias=False),\r\n                                   InPlaceABNSync(inner_features))\r\n\r\n        self.bottleneck = nn.Sequential(\r\n            nn.Conv2d(inner_features * 5, out_features, kernel_size=1, padding=0, dilation=1, bias=False),\r\n            InPlaceABNSync(out_features),\r\n            nn.Dropout2d(0.1)\r\n            )\r\n        \r\n    def forward(self, x):\r\n\r\n        _, _, h, w = x.size()\r\n\r\n        feat1 = F.upsample(self.conv1(x), size=(h, w), mode=\'bilinear\', align_corners=True)\r\n\r\n        feat2 = self.conv2(x)\r\n        feat3 = self.conv3(x)\r\n        feat4 = self.conv4(x)\r\n        feat5 = self.conv5(x)\r\n        out = torch.cat((feat1, feat2, feat3, feat4, feat5), 1)\r\n\r\n        bottle = self.bottleneck(out)\r\n        return bottle\r\n\r\nclass ResNet(nn.Module):\r\n    def __init__(self, block, layers, num_classes):\r\n        self.inplanes = 128\r\n        super(ResNet, self).__init__()\r\n        self.conv1 = conv3x3(3, 64, stride=2)\r\n        self.bn1 = BatchNorm2d(64)\r\n        self.relu1 = nn.ReLU(inplace=False)\r\n        self.conv2 = conv3x3(64, 64)\r\n        self.bn2 = BatchNorm2d(64)\r\n        self.relu2 = nn.ReLU(inplace=False)\r\n        self.conv3 = conv3x3(64, 128)\r\n        self.bn3 = BatchNorm2d(128)\r\n        self.relu3 = nn.ReLU(inplace=False)\r\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n\r\n        self.relu = nn.ReLU(inplace=False)\r\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True) # change\r\n        self.layer1 = self._make_layer(block, 64, layers[0])\r\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\r\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\r\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4, multi_grid=(1,1,1))\r\n\r\n        self.head = nn.Sequential(ASPPModule(2048),\r\n            nn.Conv2d(512, num_classes, kernel_size=1, stride=1, padding=0, bias=True))\r\n\r\n        self.dsn = nn.Sequential(\r\n            nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1),\r\n            InPlaceABNSync(512),\r\n            nn.Dropout2d(0.1),\r\n            nn.Conv2d(512, num_classes, kernel_size=1, stride=1, padding=0, bias=True)\r\n            )\r\n\r\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, multi_grid=1):\r\n        downsample = None\r\n        if stride != 1 or self.inplanes != planes * block.expansion:\r\n            downsample = nn.Sequential(\r\n                nn.Conv2d(self.inplanes, planes * block.expansion,\r\n                          kernel_size=1, stride=stride, bias=False),\r\n                BatchNorm2d(planes * block.expansion,affine = affine_par))\r\n\r\n        layers = []\r\n        generate_multi_grid = lambda index, grids: grids[index%len(grids)] if isinstance(grids, tuple) else 1\r\n        layers.append(block(self.inplanes, planes, stride,dilation=dilation, downsample=downsample, multi_grid=generate_multi_grid(0, multi_grid)))\r\n        self.inplanes = planes * block.expansion\r\n        for i in range(1, blocks):\r\n            layers.append(block(self.inplanes, planes, dilation=dilation, multi_grid=generate_multi_grid(i, multi_grid)))\r\n\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        x = self.relu1(self.bn1(self.conv1(x)))\r\n        x = self.relu2(self.bn2(self.conv2(x)))\r\n        x = self.relu3(self.bn3(self.conv3(x)))\r\n        x = self.maxpool(x)\r\n        x = self.layer1(x)\r\n        x = self.layer2(x)\r\n        x = self.layer3(x)\r\n        x_dsn = self.dsn(x)\r\n        x = self.layer4(x)\r\n        x = self.head(x)\r\n        return [x, x_dsn]\r\n\r\n\r\ndef Res_Deeplab(num_classes=21):\r\n    model = ResNet(Bottleneck,[3, 4, 23, 3], num_classes)\r\n    return model\r\n\r\n'"
networks/pspnet.py,5,"b'import torch.nn as nn\r\nfrom torch.nn import functional as F\r\nimport math\r\nimport torch.utils.model_zoo as model_zoo\r\nimport torch\r\nimport numpy as np\r\nfrom torch.autograd import Variable\r\naffine_par = True\r\nimport functools\r\n\r\nimport sys, os\r\n\r\nfrom libs import InPlaceABN, InPlaceABNSync\r\nBatchNorm2d = functools.partial(InPlaceABNSync, activation=\'none\')\r\n\r\ndef conv3x3(in_planes, out_planes, stride=1):\r\n    ""3x3 convolution with padding""\r\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\r\n                     padding=1, bias=False)\r\n\r\n\r\nclass Bottleneck(nn.Module):\r\n    expansion = 4\r\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None, fist_dilation=1, multi_grid=1):\r\n        super(Bottleneck, self).__init__()\r\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\r\n        self.bn1 = BatchNorm2d(planes)\r\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\r\n                               padding=dilation*multi_grid, dilation=dilation*multi_grid, bias=False)\r\n        self.bn2 = BatchNorm2d(planes)\r\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\r\n        self.bn3 = BatchNorm2d(planes * 4)\r\n        self.relu = nn.ReLU(inplace=False)\r\n        self.relu_inplace = nn.ReLU(inplace=True)\r\n        self.downsample = downsample\r\n        self.dilation = dilation\r\n        self.stride = stride\r\n\r\n    def forward(self, x):\r\n        residual = x\r\n\r\n        out = self.conv1(x)\r\n        out = self.bn1(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv2(out)\r\n        out = self.bn2(out)\r\n        out = self.relu(out)\r\n\r\n        out = self.conv3(out)\r\n        out = self.bn3(out)\r\n\r\n        if self.downsample is not None:\r\n            residual = self.downsample(x)\r\n\r\n        out = out + residual      \r\n        out = self.relu_inplace(out)\r\n\r\n        return out\r\n\r\nclass PSPModule(nn.Module):\r\n    """"""\r\n    Reference: \r\n        Zhao, Hengshuang, et al. *""Pyramid scene parsing network.""*\r\n    """"""\r\n    def __init__(self, features, out_features=512, sizes=(1, 2, 3, 6)):\r\n        super(PSPModule, self).__init__()\r\n\r\n        self.stages = []\r\n        self.stages = nn.ModuleList([self._make_stage(features, out_features, size) for size in sizes])\r\n        self.bottleneck = nn.Sequential(\r\n            nn.Conv2d(features+len(sizes)*out_features, out_features, kernel_size=3, padding=1, dilation=1, bias=False),\r\n            InPlaceABNSync(out_features),\r\n            nn.Dropout2d(0.1)\r\n            )\r\n\r\n    def _make_stage(self, features, out_features, size):\r\n        prior = nn.AdaptiveAvgPool2d(output_size=(size, size))\r\n        conv = nn.Conv2d(features, out_features, kernel_size=1, bias=False)\r\n        bn = InPlaceABNSync(out_features)\r\n        return nn.Sequential(prior, conv, bn)\r\n\r\n    def forward(self, feats):\r\n        h, w = feats.size(2), feats.size(3)\r\n        priors = [F.upsample(input=stage(feats), size=(h, w), mode=\'bilinear\', align_corners=True) for stage in self.stages] + [feats]\r\n        bottle = self.bottleneck(torch.cat(priors, 1))\r\n        return bottle\r\n\r\nclass ResNet(nn.Module):\r\n    def __init__(self, block, layers, num_classes):\r\n        self.inplanes = 128\r\n        super(ResNet, self).__init__()\r\n        self.conv1 = conv3x3(3, 64, stride=2)\r\n        self.bn1 = BatchNorm2d(64)\r\n        self.relu1 = nn.ReLU(inplace=False)\r\n        self.conv2 = conv3x3(64, 64)\r\n        self.bn2 = BatchNorm2d(64)\r\n        self.relu2 = nn.ReLU(inplace=False)\r\n        self.conv3 = conv3x3(64, 128)\r\n        self.bn3 = BatchNorm2d(128)\r\n        self.relu3 = nn.ReLU(inplace=False)\r\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\r\n\r\n        self.relu = nn.ReLU(inplace=False)\r\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True) # change\r\n        self.layer1 = self._make_layer(block, 64, layers[0])\r\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\r\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\r\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4, multi_grid=(1,1,1))\r\n\r\n        self.head = nn.Sequential(PSPModule(2048, 512),\r\n            nn.Conv2d(512, num_classes, kernel_size=1, stride=1, padding=0, bias=True))\r\n\r\n        self.dsn = nn.Sequential(\r\n            nn.Conv2d(1024, 512, kernel_size=3, stride=1, padding=1),\r\n            InPlaceABNSync(512),\r\n            nn.Dropout2d(0.1),\r\n            nn.Conv2d(512, num_classes, kernel_size=1, stride=1, padding=0, bias=True)\r\n            )\r\n\r\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, multi_grid=1):\r\n        downsample = None\r\n        if stride != 1 or self.inplanes != planes * block.expansion:\r\n            downsample = nn.Sequential(\r\n                nn.Conv2d(self.inplanes, planes * block.expansion,\r\n                          kernel_size=1, stride=stride, bias=False),\r\n                BatchNorm2d(planes * block.expansion,affine = affine_par))\r\n\r\n        layers = []\r\n        generate_multi_grid = lambda index, grids: grids[index%len(grids)] if isinstance(grids, tuple) else 1\r\n        layers.append(block(self.inplanes, planes, stride,dilation=dilation, downsample=downsample, multi_grid=generate_multi_grid(0, multi_grid)))\r\n        self.inplanes = planes * block.expansion\r\n        for i in range(1, blocks):\r\n            layers.append(block(self.inplanes, planes, dilation=dilation, multi_grid=generate_multi_grid(i, multi_grid)))\r\n\r\n        return nn.Sequential(*layers)\r\n\r\n    def forward(self, x):\r\n        x = self.relu1(self.bn1(self.conv1(x)))\r\n        x = self.relu2(self.bn2(self.conv2(x)))\r\n        x = self.relu3(self.bn3(self.conv3(x)))\r\n        x = self.maxpool(x)\r\n        x = self.layer1(x)\r\n        x = self.layer2(x)\r\n        x = self.layer3(x)\r\n        x_dsn = self.dsn(x)\r\n        x = self.layer4(x)\r\n        x = self.head(x)\r\n        return [x, x_dsn]\r\n\r\n\r\ndef Res_Deeplab(num_classes=21):\r\n    model = ResNet(Bottleneck,[3, 4, 23, 3], num_classes)\r\n    return model\r\n\r\n'"
utils/__init__.py,0,b''
utils/criterion.py,7,"b'import torch.nn as nn\r\n# import encoding.nn as nn\r\nimport math\r\nimport torch.utils.model_zoo as model_zoo\r\nimport torch\r\nimport numpy as np\r\nfrom torch.nn import functional as F\r\nfrom torch.autograd import Variable\r\nfrom .loss import OhemCrossEntropy2d\r\nimport scipy.ndimage as nd\r\n\r\n\r\nclass CriterionCrossEntropy(nn.Module):\r\n    def __init__(self, ignore_index=255):\r\n        super(CriterionCrossEntropy, self).__init__()\r\n        self.ignore_index = ignore_index\r\n        self.criterion = torch.nn.CrossEntropyLoss(ignore_index=ignore_index) \r\n\r\n    def forward(self, preds, target):\r\n        h, w = target.size(1), target.size(2)\r\n\r\n        scale_pred = F.upsample(input=preds, size=(h, w), mode=\'bilinear\', align_corners=True)\r\n        loss = self.criterion(scale_pred, target)\r\n\r\n        return loss\r\n\r\nclass CriterionOhemCrossEntropy(nn.Module):\r\n    def __init__(self, ignore_index=255, thres=0.6, min_kept=200000):\r\n        super(CriterionOhemCrossEntropy, self).__init__()\r\n        self.ignore_index = ignore_index\r\n        # 1/10 of the pixels within a mini-batch, if we use 2x4 on two cards, it should be 200000\r\n        self.criterion = OhemCrossEntropy2d(ignore_index, thres, min_kept)\r\n\r\n    def forward(self, preds, target):\r\n        # assert len(preds) == 2\r\n        h, w = target.size(1), target.size(2)\r\n        scale_pred = F.upsample(input=preds, size=(h, w), mode=\'bilinear\', align_corners=True)\r\n        loss = self.criterion(scale_pred, target)\r\n        # print(\'OhemCrossEntropy2d Loss: {}\'.format(loss.data.cpu().numpy()[0]))\r\n        return loss\r\n\r\nclass CriterionDSN(nn.Module):\r\n    \'\'\'\r\n    DSN : We need to consider two supervision for the model.\r\n    \'\'\'\r\n    def __init__(self, ignore_index=255, use_weight=True, reduce=True):\r\n        super(CriterionDSN, self).__init__()\r\n        self.ignore_index = ignore_index\r\n        self.criterion = torch.nn.CrossEntropyLoss(ignore_index=ignore_index, reduce=reduce)\r\n        if not reduce:\r\n            print(""disabled the reduce."")\r\n\r\n    def forward(self, preds, target):\r\n        h, w = target.size(1), target.size(2)\r\n\r\n        scale_pred = F.upsample(input=preds[0], size=(h, w), mode=\'bilinear\', align_corners=True)\r\n        loss1 = self.criterion(scale_pred, target)\r\n\r\n        scale_pred = F.upsample(input=preds[1], size=(h, w), mode=\'bilinear\', align_corners=True)\r\n        loss2 = self.criterion(scale_pred, target)\r\n\r\n        # scale_pred = F.upsample(input=preds[2], size=(h, w), mode=\'bilinear\', align_corners=True)\r\n        # loss3 = self.criterion(scale_pred, target)\r\n        return loss1 + loss2*0.4\r\n\r\nclass CriterionOhemDSN(nn.Module):\r\n    \'\'\'\r\n    DSN : We need to consider two supervision for the model.\r\n    \'\'\'\r\n    def __init__(self, ignore_index=255, thresh=0.7, min_kept=100000, use_weight=True, reduce=True):\r\n        super(CriterionOhemDSN, self).__init__()\r\n        self.ignore_index = ignore_index\r\n        self.criterion1 = OhemCrossEntropy2d(ignore_index, thresh, min_kept)\r\n        self.criterion2 = torch.nn.CrossEntropyLoss(ignore_index=ignore_index, reduce=reduce)\r\n        if not reduce:\r\n            print(""disabled the reduce."")\r\n\r\n    def forward(self, preds, target):\r\n        h, w = target.size(1), target.size(2)\r\n\r\n        scale_pred = F.upsample(input=preds[0], size=(h, w), mode=\'bilinear\', align_corners=True)\r\n        loss1 = self.criterion1(scale_pred, target)\r\n\r\n        scale_pred = F.upsample(input=preds[1], size=(h, w), mode=\'bilinear\', align_corners=True)\r\n        loss2 = self.criterion2(scale_pred, target)\r\n\r\n        # scale_pred = F.upsample(input=preds[2], size=(h, w), mode=\'bilinear\', align_corners=True)\r\n        # loss3 = self.criterion2(scale_pred, target)\r\n        return loss1 + loss2*0.4'"
utils/encoding.py,9,"b'##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\r\n## Created by: Hang Zhang\r\n## ECE Department, Rutgers University\r\n## Email: zhang.hang@rutgers.edu\r\n## Copyright (c) 2017\r\n##\r\n## This source code is licensed under the MIT-style license found in the\r\n## LICENSE file in the root directory of this source tree\r\n##+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\r\n\r\n""""""Encoding Data Parallel""""""\r\nimport threading\r\nimport functools\r\nimport torch\r\nfrom torch.autograd import Variable, Function\r\nimport torch.cuda.comm as comm\r\nfrom torch.nn.parallel.data_parallel import DataParallel\r\nfrom torch.nn.parallel.parallel_apply import get_a_var\r\nfrom torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\r\n\r\ntorch_ver = torch.__version__[:3]\r\n\r\n__all__ = [\'allreduce\', \'DataParallelModel\', \'DataParallelCriterion\',\r\n           \'patch_replication_callback\']\r\n\r\ndef allreduce(*inputs):\r\n    """"""Cross GPU all reduce autograd operation for calculate mean and\r\n    variance in SyncBN.\r\n    """"""\r\n    return AllReduce.apply(*inputs)\r\n\r\nclass AllReduce(Function):\r\n    @staticmethod\r\n    def forward(ctx, num_inputs, *inputs):\r\n        ctx.num_inputs = num_inputs\r\n        ctx.target_gpus = [inputs[i].get_device() for i in range(0, len(inputs), num_inputs)]\r\n        inputs = [inputs[i:i + num_inputs]\r\n                 for i in range(0, len(inputs), num_inputs)]\r\n        # sort before reduce sum\r\n        inputs = sorted(inputs, key=lambda i: i[0].get_device())\r\n        results = comm.reduce_add_coalesced(inputs, ctx.target_gpus[0])\r\n        outputs = comm.broadcast_coalesced(results, ctx.target_gpus)\r\n        return tuple([t for tensors in outputs for t in tensors])\r\n\r\n    @staticmethod\r\n    def backward(ctx, *inputs):\r\n        inputs = [i.data for i in inputs]\r\n        inputs = [inputs[i:i + ctx.num_inputs]\r\n                 for i in range(0, len(inputs), ctx.num_inputs)]\r\n        results = comm.reduce_add_coalesced(inputs, ctx.target_gpus[0])\r\n        outputs = comm.broadcast_coalesced(results, ctx.target_gpus)\r\n        return (None,) + tuple([Variable(t) for tensors in outputs for t in tensors])\r\n\r\n\r\nclass Reduce(Function):\r\n    @staticmethod\r\n    def forward(ctx, *inputs):\r\n        ctx.target_gpus = [inputs[i].get_device() for i in range(len(inputs))]\r\n        inputs = sorted(inputs, key=lambda i: i.get_device())\r\n        return comm.reduce_add(inputs)\r\n\r\n    @staticmethod\r\n    def backward(ctx, gradOutput):\r\n        return Broadcast.apply(ctx.target_gpus, gradOutput)\r\n\r\n\r\nclass DataParallelModel(DataParallel):\r\n    """"""Implements data parallelism at the module level.\r\n\r\n    This container parallelizes the application of the given module by\r\n    splitting the input across the specified devices by chunking in the\r\n    batch dimension.\r\n    In the forward pass, the module is replicated on each device,\r\n    and each replica handles a portion of the input. During the backwards pass, gradients from each replica are summed into the original module.\r\n    Note that the outputs are not gathered, please use compatible\r\n    :class:`encoding.parallel.DataParallelCriterion`.\r\n\r\n    The batch size should be larger than the number of GPUs used. It should\r\n    also be an integer multiple of the number of GPUs so that each chunk is\r\n    the same size (so that each GPU processes the same number of samples).\r\n\r\n    Args:\r\n        module: module to be parallelized\r\n        device_ids: CUDA devices (default: all devices)\r\n\r\n    Reference:\r\n        Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi,\r\n        Amit Agrawal. \xe2\x80\x9cContext Encoding for Semantic Segmentation.\r\n        *The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2018*\r\n\r\n    Example::\r\n\r\n        >>> net = encoding.nn.DataParallelModel(model, device_ids=[0, 1, 2])\r\n        >>> y = net(x)\r\n    """"""\r\n    def gather(self, outputs, output_device):\r\n        return outputs\r\n\r\n    def replicate(self, module, device_ids):\r\n        modules = super(DataParallelModel, self).replicate(module, device_ids)\r\n        execute_replication_callbacks(modules)\r\n        return modules\r\n\r\n\r\nclass DataParallelCriterion(DataParallel):\r\n    """"""\r\n    Calculate loss in multiple-GPUs, which balance the memory usage for\r\n    Semantic Segmentation.\r\n\r\n    The targets are splitted across the specified devices by chunking in\r\n    the batch dimension. Please use together with :class:`encoding.parallel.DataParallelModel`.\r\n\r\n    Reference:\r\n        Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi,\r\n        Amit Agrawal. \xe2\x80\x9cContext Encoding for Semantic Segmentation.\r\n        *The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2018*\r\n\r\n    Example::\r\n\r\n        >>> net = encoding.nn.DataParallelModel(model, device_ids=[0, 1, 2])\r\n        >>> criterion = encoding.nn.DataParallelCriterion(criterion, device_ids=[0, 1, 2])\r\n        >>> y = net(x)\r\n        >>> loss = criterion(y, target)\r\n    """"""\r\n    def forward(self, inputs, *targets, **kwargs):\r\n        # input should be already scatterd\r\n        # scattering the targets instead\r\n        if not self.device_ids:\r\n            return self.module(inputs, *targets, **kwargs)\r\n        targets, kwargs = self.scatter(targets, kwargs, self.device_ids)\r\n        if len(self.device_ids) == 1:\r\n            return self.module(inputs, *targets[0], **kwargs[0])\r\n        replicas = self.replicate(self.module, self.device_ids[:len(inputs)])\r\n        outputs = _criterion_parallel_apply(replicas, inputs, targets, kwargs)\r\n        return Reduce.apply(*outputs) / len(outputs)\r\n        #return self.gather(outputs, self.output_device).mean()\r\n\r\n\r\ndef _criterion_parallel_apply(modules, inputs, targets, kwargs_tup=None, devices=None):\r\n    assert len(modules) == len(inputs)\r\n    assert len(targets) == len(inputs)\r\n    if kwargs_tup:\r\n        assert len(modules) == len(kwargs_tup)\r\n    else:\r\n        kwargs_tup = ({},) * len(modules)\r\n    if devices is not None:\r\n        assert len(modules) == len(devices)\r\n    else:\r\n        devices = [None] * len(modules)\r\n\r\n    lock = threading.Lock()\r\n    results = {}\r\n    if torch_ver != ""0.3"":\r\n        grad_enabled = torch.is_grad_enabled()\r\n\r\n    def _worker(i, module, input, target, kwargs, device=None):\r\n        if torch_ver != ""0.3"":\r\n            torch.set_grad_enabled(grad_enabled)\r\n        if device is None:\r\n            device = get_a_var(input).get_device()\r\n        try:\r\n            if not isinstance(input, tuple):\r\n                input = (input,)\r\n            with torch.cuda.device(device):\r\n                output = module(*(input + target), **kwargs)\r\n            with lock:\r\n                results[i] = output\r\n        except Exception as e:\r\n            with lock:\r\n                results[i] = e\r\n\r\n    if len(modules) > 1:\r\n        threads = [threading.Thread(target=_worker,\r\n                                    args=(i, module, input, target,\r\n                                          kwargs, device),)\r\n                   for i, (module, input, target, kwargs, device) in\r\n                   enumerate(zip(modules, inputs, targets, kwargs_tup, devices))]\r\n\r\n        for thread in threads:\r\n            thread.start()\r\n        for thread in threads:\r\n            thread.join()\r\n    else:\r\n        _worker(0, modules[0], inputs[0], kwargs_tup[0], devices[0])\r\n\r\n    outputs = []\r\n    for i in range(len(inputs)):\r\n        output = results[i]\r\n        if isinstance(output, Exception):\r\n            raise output\r\n        outputs.append(output)\r\n    return outputs\r\n\r\n\r\n###########################################################################\r\n# Adapted from Synchronized-BatchNorm-PyTorch.\r\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\r\n#\r\nclass CallbackContext(object):\r\n    pass\r\n\r\n\r\ndef execute_replication_callbacks(modules):\r\n    """"""\r\n    Execute an replication callback `__data_parallel_replicate__` on each module created\r\n    by original replication.\r\n\r\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\r\n\r\n    Note that, as all modules are isomorphism, we assign each sub-module with a context\r\n    (shared among multiple copies of this module on different devices).\r\n    Through this context, different copies can share some information.\r\n\r\n    We guarantee that the callback on the master copy (the first copy) will be called ahead\r\n    of calling the callback of any slave copies.\r\n    """"""\r\n    master_copy = modules[0]\r\n    nr_modules = len(list(master_copy.modules()))\r\n    ctxs = [CallbackContext() for _ in range(nr_modules)]\r\n\r\n    for i, module in enumerate(modules):\r\n        for j, m in enumerate(module.modules()):\r\n            if hasattr(m, \'__data_parallel_replicate__\'):\r\n                m.__data_parallel_replicate__(ctxs[j], i)\r\n\r\n\r\ndef patch_replication_callback(data_parallel):\r\n    """"""\r\n    Monkey-patch an existing `DataParallel` object. Add the replication callback.\r\n    Useful when you have customized `DataParallel` implementation.\r\n\r\n    Examples:\r\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\r\n        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\r\n        > patch_replication_callback(sync_bn)\r\n        # this is equivalent to\r\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\r\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\r\n    """"""\r\n\r\n    assert isinstance(data_parallel, DataParallel)\r\n\r\n    old_replicate = data_parallel.replicate\r\n\r\n    @functools.wraps(old_replicate)\r\n    def new_replicate(module, device_ids):\r\n        modules = old_replicate(module, device_ids)\r\n        execute_replication_callbacks(modules)\r\n        return modules\r\n\r\n    data_parallel.replicate = new_replicate'"
utils/loss.py,5,"b'import torch\r\nimport torch.nn.functional as F\r\nimport torch.nn as nn\r\nfrom torch.autograd import Variable\r\nimport numpy as np\r\nimport scipy.ndimage as nd\r\n\r\n\r\nclass OhemCrossEntropy2d(nn.Module):\r\n\r\n    def __init__(self, ignore_label=255, thresh=0.7, min_kept=100000, factor=8):\r\n        super(OhemCrossEntropy2d, self).__init__()\r\n        self.ignore_label = ignore_label\r\n        self.thresh = float(thresh)\r\n        # self.min_kept_ratio = float(min_kept_ratio)\r\n        self.min_kept = int(min_kept)\r\n        self.factor = factor\r\n        self.criterion = torch.nn.CrossEntropyLoss(ignore_index=ignore_label)\r\n\r\n    def find_threshold(self, np_predict, np_target):\r\n        # downsample 1/8\r\n        factor = self.factor\r\n        predict = nd.zoom(np_predict, (1.0, 1.0, 1.0/factor, 1.0/factor), order=1)\r\n        target = nd.zoom(np_target, (1.0, 1.0/factor, 1.0/factor), order=0)\r\n\r\n        n, c, h, w = predict.shape\r\n        min_kept = self.min_kept // (factor*factor) #int(self.min_kept_ratio * n * h * w)\r\n\r\n        input_label = target.ravel().astype(np.int32)\r\n        input_prob = np.rollaxis(predict, 1).reshape((c, -1))\r\n\r\n        valid_flag = input_label != self.ignore_label\r\n        valid_inds = np.where(valid_flag)[0]\r\n        label = input_label[valid_flag]\r\n        num_valid = valid_flag.sum()\r\n        if min_kept >= num_valid:\r\n            threshold = 1.0\r\n        elif num_valid > 0:\r\n            prob = input_prob[:,valid_flag]\r\n            pred = prob[label, np.arange(len(label), dtype=np.int32)]\r\n            threshold = self.thresh\r\n            if min_kept > 0:\r\n                k_th = min(len(pred), min_kept)-1\r\n                new_array = np.partition(pred, k_th)\r\n                new_threshold = new_array[k_th]\r\n                if new_threshold > self.thresh:\r\n                    threshold = new_threshold\r\n        return threshold\r\n\r\n\r\n    def generate_new_target(self, predict, target):\r\n        np_predict = predict.data.cpu().numpy()\r\n        np_target = target.data.cpu().numpy()\r\n        n, c, h, w = np_predict.shape\r\n\r\n        threshold = self.find_threshold(np_predict, np_target)\r\n\r\n        input_label = np_target.ravel().astype(np.int32)\r\n        input_prob = np.rollaxis(np_predict, 1).reshape((c, -1))\r\n\r\n        valid_flag = input_label != self.ignore_label\r\n        valid_inds = np.where(valid_flag)[0]\r\n        label = input_label[valid_flag]\r\n        num_valid = valid_flag.sum()\r\n\r\n        if num_valid > 0:\r\n            prob = input_prob[:,valid_flag]\r\n            pred = prob[label, np.arange(len(label), dtype=np.int32)]\r\n            kept_flag = pred <= threshold\r\n            valid_inds = valid_inds[kept_flag]\r\n            print(\'Labels: {} {}\'.format(len(valid_inds), threshold))\r\n\r\n        label = input_label[valid_inds].copy()\r\n        input_label.fill(self.ignore_label)\r\n        input_label[valid_inds] = label\r\n        new_target = torch.from_numpy(input_label.reshape(target.size())).long().cuda(target.get_device())\r\n\r\n        return new_target\r\n\r\n\r\n    def forward(self, predict, target, weight=None):\r\n        """"""\r\n            Args:\r\n                predict:(n, c, h, w)\r\n                target:(n, h, w)\r\n                weight (Tensor, optional): a manual rescaling weight given to each class.\r\n                                           If given, has to be a Tensor of size ""nclasses""\r\n        """"""\r\n        assert not target.requires_grad\r\n\r\n        input_prob = F.softmax(predict, 1)\r\n        target = self.generate_new_target(input_prob, target)\r\n        return self.criterion(predict, target)\r\n'"
utils/utils.py,0,"b'from PIL import Image\r\nimport numpy as np\r\nimport torch\r\n\r\n# colour map\r\nlabel_colours = [(0,0,0)\r\n                # 0=background\r\n                ,(128,0,0),(0,128,0),(128,128,0),(0,0,128),(128,0,128)\r\n                # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\r\n                ,(0,128,128),(128,128,128),(64,0,0),(192,0,0),(64,128,0)\r\n                # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\r\n                ,(192,128,0),(64,0,128),(192,0,128),(64,128,128),(192,128,128)\r\n                # 11=diningtable, 12=dog, 13=horse, 14=motorbike, 15=person\r\n                ,(0,64,0),(128,64,0),(0,192,0),(128,192,0),(0,64,128)]\r\n                # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\r\n\r\ndef decode_labels(mask, num_images=1, num_classes=21):\r\n    """"""Decode batch of segmentation masks.\r\n    \r\n    Args:\r\n      mask: result of inference after taking argmax.\r\n      num_images: number of images to decode from the batch.\r\n      num_classes: number of classes to predict (including background).\r\n    \r\n    Returns:\r\n      A batch with num_images RGB images of the same size as the input. \r\n    """"""\r\n    mask = mask.data.cpu().numpy()\r\n    n, h, w = mask.shape\r\n    assert(n >= num_images), \'Batch size %d should be greater or equal than number of images to save %d.\' % (n, num_images)\r\n    outputs = np.zeros((num_images, h, w, 3), dtype=np.uint8)\r\n    for i in range(num_images):\r\n      img = Image.new(\'RGB\', (len(mask[i, 0]), len(mask[i])))\r\n      pixels = img.load()\r\n      for j_, j in enumerate(mask[i, :, :]):\r\n          for k_, k in enumerate(j):\r\n              if k < num_classes:\r\n                  pixels[k_,j_] = label_colours[k]\r\n      outputs[i] = np.array(img)\r\n    return outputs\r\n\r\ndef decode_predictions(preds, num_images=1, num_classes=21):\r\n    """"""Decode batch of segmentation masks.\r\n    \r\n    Args:\r\n      mask: result of inference after taking argmax.\r\n      num_images: number of images to decode from the batch.\r\n      num_classes: number of classes to predict (including background).\r\n    \r\n    Returns:\r\n      A batch with num_images RGB images of the same size as the input. \r\n    """"""\r\n    if isinstance(preds, list):\r\n        preds_list = []\r\n        for pred in preds:\r\n            preds_list.append(pred[-1].data.cpu().numpy())\r\n        preds = np.concatenate(preds_list, axis=0)\r\n    else:\r\n        preds = preds.data.cpu().numpy()\r\n\r\n    preds = np.argmax(preds, axis=1)\r\n    n, h, w = preds.shape\r\n    assert(n >= num_images), \'Batch size %d should be greater or equal than number of images to save %d.\' % (n, num_images)\r\n    outputs = np.zeros((num_images, h, w, 3), dtype=np.uint8)\r\n    for i in range(num_images):\r\n      img = Image.new(\'RGB\', (len(preds[i, 0]), len(preds[i])))\r\n      pixels = img.load()\r\n      for j_, j in enumerate(preds[i, :, :]):\r\n          for k_, k in enumerate(j):\r\n              if k < num_classes:\r\n                  pixels[k_,j_] = label_colours[k]\r\n      outputs[i] = np.array(img)\r\n    return outputs\r\n\r\ndef inv_preprocess(imgs, num_images, img_mean):\r\n    """"""Inverse preprocessing of the batch of images.\r\n       Add the mean vector and convert from BGR to RGB.\r\n       \r\n    Args:\r\n      imgs: batch of input images.\r\n      num_images: number of images to apply the inverse transformations on.\r\n      img_mean: vector of mean colour values.\r\n  \r\n    Returns:\r\n      The batch of the size num_images with the same spatial dimensions as the input.\r\n    """"""\r\n    imgs = imgs.data.cpu().numpy()\r\n    n, c, h, w = imgs.shape\r\n    assert(n >= num_images), \'Batch size %d should be greater or equal than number of images to save %d.\' % (n, num_images)\r\n    outputs = np.zeros((num_images, h, w, c), dtype=np.uint8)\r\n    for i in range(num_images):\r\n        outputs[i] = (np.transpose(imgs[i], (1,2,0)) + img_mean).astype(np.uint8)\r\n    return outputs\r\n'"
cc_attention/_ext/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom .__ext import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
libs/_ext/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom .__ext import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
