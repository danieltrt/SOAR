file_path,api_count,code
data/convert.py,0,"b'import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n#raw labels\nraw_labels = pd.read_csv(""train_labels.csv"",header=None)[1:].values\n\nwith open(""train_only.csv"",""w"") as f:\n    for value in raw_labels:\n        filename = os.getcwd() + ""/images/train/""  +value[0]\n        if isinstance(value[1],float):\n            label = "",,,,""\n        else:\n            label = value[1].replace("" "","","") + "",gangjin""\n        f.write(filename+"",""+label + ""\\n"")\n'"
retinanet/__init__.py,0,b''
retinanet/anchors.py,2,"b'import numpy as np\nimport torch\nimport torch.nn as nn\n\n\nclass Anchors(nn.Module):\n    def __init__(self, pyramid_levels=None, strides=None, sizes=None, ratios=None, scales=None):\n        super(Anchors, self).__init__()\n\n        if pyramid_levels is None:\n            self.pyramid_levels = [3, 4, 5, 6, 7]\n        if strides is None:\n            self.strides = [2 ** x for x in self.pyramid_levels]\n        if sizes is None:\n            self.sizes = [2 ** (x + 2) for x in self.pyramid_levels]\n        if ratios is None:\n            self.ratios = np.array([0.5, 1, 2])\n        if scales is None:\n            self.scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n\n    def forward(self, image):\n        \n        image_shape = image.shape[2:]\n        image_shape = np.array(image_shape)\n        image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in self.pyramid_levels]\n\n        # compute anchors over all pyramid levels\n        all_anchors = np.zeros((0, 4)).astype(np.float32)\n\n        for idx, p in enumerate(self.pyramid_levels):\n            anchors         = generate_anchors(base_size=self.sizes[idx], ratios=self.ratios, scales=self.scales)\n            shifted_anchors = shift(image_shapes[idx], self.strides[idx], anchors)\n            all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n\n        all_anchors = np.expand_dims(all_anchors, axis=0)\n\n        return torch.from_numpy(all_anchors.astype(np.float32)).cuda()\n\ndef generate_anchors(base_size=16, ratios=None, scales=None):\n    """"""\n    Generate anchor (reference) windows by enumerating aspect ratios X\n    scales w.r.t. a reference window.\n    """"""\n\n    if ratios is None:\n        ratios = np.array([0.5, 1, 2])\n\n    if scales is None:\n        scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n\n    num_anchors = len(ratios) * len(scales)\n\n    # initialize output anchors\n    anchors = np.zeros((num_anchors, 4))\n\n    # scale base_size\n    anchors[:, 2:] = base_size * np.tile(scales, (2, len(ratios))).T\n\n    # compute areas of anchors\n    areas = anchors[:, 2] * anchors[:, 3]\n\n    # correct for ratios\n    anchors[:, 2] = np.sqrt(areas / np.repeat(ratios, len(scales)))\n    anchors[:, 3] = anchors[:, 2] * np.repeat(ratios, len(scales))\n\n    # transform from (x_ctr, y_ctr, w, h) -> (x1, y1, x2, y2)\n    anchors[:, 0::2] -= np.tile(anchors[:, 2] * 0.5, (2, 1)).T\n    anchors[:, 1::2] -= np.tile(anchors[:, 3] * 0.5, (2, 1)).T\n\n    return anchors\n\ndef compute_shape(image_shape, pyramid_levels):\n    """"""Compute shapes based on pyramid levels.\n\n    :param image_shape:\n    :param pyramid_levels:\n    :return:\n    """"""\n    image_shape = np.array(image_shape[:2])\n    image_shapes = [(image_shape + 2 ** x - 1) // (2 ** x) for x in pyramid_levels]\n    return image_shapes\n\n\ndef anchors_for_shape(\n    image_shape,\n    pyramid_levels=None,\n    ratios=None,\n    scales=None,\n    strides=None,\n    sizes=None,\n    shapes_callback=None,\n):\n\n    image_shapes = compute_shape(image_shape, pyramid_levels)\n\n    # compute anchors over all pyramid levels\n    all_anchors = np.zeros((0, 4))\n    for idx, p in enumerate(pyramid_levels):\n        anchors         = generate_anchors(base_size=sizes[idx], ratios=ratios, scales=scales)\n        shifted_anchors = shift(image_shapes[idx], strides[idx], anchors)\n        all_anchors     = np.append(all_anchors, shifted_anchors, axis=0)\n\n    return all_anchors\n\n\ndef shift(shape, stride, anchors):\n    shift_x = (np.arange(0, shape[1]) + 0.5) * stride\n    shift_y = (np.arange(0, shape[0]) + 0.5) * stride\n\n    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n\n    shifts = np.vstack((\n        shift_x.ravel(), shift_y.ravel(),\n        shift_x.ravel(), shift_y.ravel()\n    )).transpose()\n\n    # add A anchors (1, A, 4) to\n    # cell K shifts (K, 1, 4) to get\n    # shift anchors (K, A, 4)\n    # reshape to (K*A, 4) shifted anchors\n    A = anchors.shape[0]\n    K = shifts.shape[0]\n    all_anchors = (anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)))\n    all_anchors = all_anchors.reshape((K * A, 4))\n\n    return all_anchors\n\n'"
retinanet/coco_eval.py,1,"b""from __future__ import print_function\n\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nimport numpy as np\nimport json\nimport os\n\nimport torch\n\ndef evaluate_coco(dataset, model, threshold=0.05):\n    \n    model.eval()\n    \n    with torch.no_grad():\n\n        # start collecting results\n        results = []\n        image_ids = []\n\n        for index in range(len(dataset)):\n            data = dataset[index]\n            scale = data['scale']\n\n            # run network\n            scores, labels, boxes = model(data['img'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0))\n            scores = scores.cpu()\n            labels = labels.cpu()\n            boxes  = boxes.cpu()\n\n            # correct boxes for image scale\n            boxes /= scale\n\n            if boxes.shape[0] > 0:\n                # change to (x, y, w, h) (MS COCO standard)\n                boxes[:, 2] -= boxes[:, 0]\n                boxes[:, 3] -= boxes[:, 1]\n\n                # compute predicted labels and scores\n                #for box, score, label in zip(boxes[0], scores[0], labels[0]):\n                for box_id in range(boxes.shape[0]):\n                    score = float(scores[box_id])\n                    label = int(labels[box_id])\n                    box = boxes[box_id, :]\n\n                    # scores are sorted, so we can break\n                    if score < threshold:\n                        break\n\n                    # append detection for each positively labeled class\n                    image_result = {\n                        'image_id'    : dataset.image_ids[index],\n                        'category_id' : dataset.label_to_coco_label(label),\n                        'score'       : float(score),\n                        'bbox'        : box.tolist(),\n                    }\n\n                    # append detection to results\n                    results.append(image_result)\n\n            # append image to list of processed images\n            image_ids.append(dataset.image_ids[index])\n\n            # print progress\n            print('{}/{}'.format(index, len(dataset)), end='\\r')\n\n        if not len(results):\n            return\n\n        # write output\n        json.dump(results, open('{}_bbox_results.json'.format(dataset.set_name), 'w'), indent=4)\n\n        # load results in COCO evaluation tool\n        coco_true = dataset.coco\n        coco_pred = coco_true.loadRes('{}_bbox_results.json'.format(dataset.set_name))\n\n        # run COCO evaluation\n        coco_eval = COCOeval(coco_true, coco_pred, 'bbox')\n        coco_eval.params.imgIds = image_ids\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        coco_eval.summarize()\n\n        model.train()\n\n        return\n"""
retinanet/csv_eval.py,1,"b'from __future__ import print_function\n\nimport numpy as np\nimport json\nimport os\n\nimport torch\n\n\n\ndef compute_overlap(a, b):\n    """"""\n    Parameters\n    ----------\n    a: (N, 4) ndarray of float\n    b: (K, 4) ndarray of float\n    Returns\n    -------\n    overlaps: (N, K) ndarray of overlap between boxes and query_boxes\n    """"""\n    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n\n    iw = np.minimum(np.expand_dims(a[:, 2], axis=1), b[:, 2]) - np.maximum(np.expand_dims(a[:, 0], 1), b[:, 0])\n    ih = np.minimum(np.expand_dims(a[:, 3], axis=1), b[:, 3]) - np.maximum(np.expand_dims(a[:, 1], 1), b[:, 1])\n\n    iw = np.maximum(iw, 0)\n    ih = np.maximum(ih, 0)\n\n    ua = np.expand_dims((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), axis=1) + area - iw * ih\n\n    ua = np.maximum(ua, np.finfo(float).eps)\n\n    intersection = iw * ih\n\n    return intersection / ua\n\n\ndef _compute_ap(recall, precision):\n    """""" Compute the average precision, given the recall and precision curves.\n    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n    # Arguments\n        recall:    The recall curve (list).\n        precision: The precision curve (list).\n    # Returns\n        The average precision as computed in py-faster-rcnn.\n    """"""\n    # correct AP calculation\n    # first append sentinel values at the end\n    mrec = np.concatenate(([0.], recall, [1.]))\n    mpre = np.concatenate(([0.], precision, [0.]))\n\n    # compute the precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # to calculate area under PR curve, look for points\n    # where X axis (recall) changes value\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n\n    # and sum (\\Delta recall) * prec\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef _get_detections(dataset, retinanet, score_threshold=0.05, max_detections=100, save_path=None):\n    """""" Get the detections from the retinanet using the generator.\n    The result is a list of lists such that the size is:\n        all_detections[num_images][num_classes] = detections[num_detections, 4 + num_classes]\n    # Arguments\n        dataset         : The generator used to run images through the retinanet.\n        retinanet           : The retinanet to run on the images.\n        score_threshold : The score confidence threshold to use.\n        max_detections  : The maximum number of detections to use per image.\n        save_path       : The path to save the images with visualized detections to.\n    # Returns\n        A list of lists containing the detections for each image in the generator.\n    """"""\n    all_detections = [[None for i in range(dataset.num_classes())] for j in range(len(dataset))]\n\n    retinanet.eval()\n    \n    with torch.no_grad():\n\n        for index in range(len(dataset)):\n            data = dataset[index]\n            scale = data[\'scale\']\n\n            # run network\n            scores, labels, boxes = retinanet(data[\'img\'].permute(2, 0, 1).cuda().float().unsqueeze(dim=0))\n            scores = scores.cpu().numpy()\n            labels = labels.cpu().numpy()\n            boxes  = boxes.cpu().numpy()\n\n            # correct boxes for image scale\n            boxes /= scale\n\n            # select indices which have a score above the threshold\n            indices = np.where(scores > score_threshold)[0]\n            if indices.shape[0] > 0:\n                # select those scores\n                scores = scores[indices]\n\n                # find the order with which to sort the scores\n                scores_sort = np.argsort(-scores)[:max_detections]\n\n                # select detections\n                image_boxes      = boxes[indices[scores_sort], :]\n                image_scores     = scores[scores_sort]\n                image_labels     = labels[indices[scores_sort]]\n                image_detections = np.concatenate([image_boxes, np.expand_dims(image_scores, axis=1), np.expand_dims(image_labels, axis=1)], axis=1)\n\n                # copy detections to all_detections\n                for label in range(dataset.num_classes()):\n                    all_detections[index][label] = image_detections[image_detections[:, -1] == label, :-1]\n            else:\n                # copy detections to all_detections\n                for label in range(dataset.num_classes()):\n                    all_detections[index][label] = np.zeros((0, 5))\n\n            print(\'{}/{}\'.format(index + 1, len(dataset)), end=\'\\r\')\n\n    return all_detections\n\n\ndef _get_annotations(generator):\n    """""" Get the ground truth annotations from the generator.\n    The result is a list of lists such that the size is:\n        all_detections[num_images][num_classes] = annotations[num_detections, 5]\n    # Arguments\n        generator : The generator used to retrieve ground truth annotations.\n    # Returns\n        A list of lists containing the annotations for each image in the generator.\n    """"""\n    all_annotations = [[None for i in range(generator.num_classes())] for j in range(len(generator))]\n\n    for i in range(len(generator)):\n        # load the annotations\n        annotations = generator.load_annotations(i)\n\n        # copy detections to all_annotations\n        for label in range(generator.num_classes()):\n            all_annotations[i][label] = annotations[annotations[:, 4] == label, :4].copy()\n\n        print(\'{}/{}\'.format(i + 1, len(generator)), end=\'\\r\')\n\n    return all_annotations\n\n\ndef evaluate(\n    generator,\n    retinanet,\n    iou_threshold=0.5,\n    score_threshold=0.5,\n    max_detections=200,\n    save_path=None\n):\n    """""" Evaluate a given dataset using a given retinanet.\n    # Arguments\n        generator       : The generator that represents the dataset to evaluate.\n        retinanet           : The retinanet to evaluate.\n        iou_threshold   : The threshold used to consider when a detection is positive or negative.\n        score_threshold : The score confidence threshold to use for detections.\n        max_detections  : The maximum number of detections to use per image.\n        save_path       : The path to save images with visualized detections to.\n    # Returns\n        A dict mapping class names to mAP scores.\n    """"""\n\n\n\n    # gather all detections and annotations\n\n    all_detections     = _get_detections(generator, retinanet, score_threshold=score_threshold, max_detections=max_detections, save_path=save_path)\n    all_annotations    = _get_annotations(generator)\n\n    average_precisions = {}\n\n    for label in range(generator.num_classes()):\n        false_positives = np.zeros((0,))\n        true_positives  = np.zeros((0,))\n        scores          = np.zeros((0,))\n        num_annotations = 0.0\n\n        for i in range(len(generator)):\n            detections           = all_detections[i][label]\n            annotations          = all_annotations[i][label]\n            num_annotations     += annotations.shape[0]\n            detected_annotations = []\n\n            for d in detections:\n                scores = np.append(scores, d[4])\n\n                if annotations.shape[0] == 0:\n                    false_positives = np.append(false_positives, 1)\n                    true_positives  = np.append(true_positives, 0)\n                    continue\n\n                overlaps            = compute_overlap(np.expand_dims(d, axis=0), annotations)\n                assigned_annotation = np.argmax(overlaps, axis=1)\n                max_overlap         = overlaps[0, assigned_annotation]\n\n                if max_overlap >= iou_threshold and assigned_annotation not in detected_annotations:\n                    false_positives = np.append(false_positives, 0)\n                    true_positives  = np.append(true_positives, 1)\n                    detected_annotations.append(assigned_annotation)\n                else:\n                    false_positives = np.append(false_positives, 1)\n                    true_positives  = np.append(true_positives, 0)\n\n        # no annotations -> AP for this class is 0 (is this correct?)\n        if num_annotations == 0:\n            average_precisions[label] = 0, 0\n            continue\n\n        # sort by score\n        indices         = np.argsort(-scores)\n        false_positives = false_positives[indices]\n        true_positives  = true_positives[indices]\n\n        # compute false positives and true positives\n        false_positives = np.cumsum(false_positives)\n        true_positives  = np.cumsum(true_positives)\n\n        # compute recall and precision\n        recall    = true_positives / num_annotations\n        precision = true_positives / np.maximum(true_positives + false_positives, np.finfo(np.float64).eps)\n\n        # compute average precision\n        average_precision  = _compute_ap(recall, precision)\n        average_precisions[label] = average_precision, num_annotations\n    \n    print(\'\\nmAP:\')\n    for label in range(generator.num_classes()):\n        label_name = generator.label_to_name(label)\n        print(\'{}: {}\'.format(label_name, average_precisions[label][0]))\n    \n    return average_precisions\n\n'"
retinanet/dataloader.py,6,"b'from __future__ import print_function, division\nimport sys\nimport os\nimport torch\nimport numpy as np\nimport random\nimport csv\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nfrom torch.utils.data.sampler import Sampler\n\nfrom pycocotools.coco import COCO\n\nimport skimage.io\nimport skimage.transform\nimport skimage.color\nimport skimage\nfrom IPython import embed\nfrom PIL import Image\nfrom six import raise_from\n\nclass CocoDataset(Dataset):\n    """"""Coco dataset.""""""\n\n    def __init__(self, root_dir, set_name=\'train2017\', transform=None):\n        """"""\n        Args:\n            root_dir (string): COCO directory.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        """"""\n        self.root_dir = root_dir\n        self.set_name = set_name\n        self.transform = transform\n\n        self.coco      = COCO(os.path.join(self.root_dir, \'annotations\', \'instances_\' + self.set_name + \'.json\'))\n        self.image_ids = self.coco.getImgIds()\n\n        self.load_classes()\n\n    def load_classes(self):\n        # load class names (name -> label)\n        categories = self.coco.loadCats(self.coco.getCatIds())\n        categories.sort(key=lambda x: x[\'id\'])\n\n        self.classes             = {}\n        self.coco_labels         = {}\n        self.coco_labels_inverse = {}\n        for c in categories:\n            self.coco_labels[len(self.classes)] = c[\'id\']\n            self.coco_labels_inverse[c[\'id\']] = len(self.classes)\n            self.classes[c[\'name\']] = len(self.classes)\n\n        # also load the reverse (label -> name)\n        self.labels = {}\n        for key, value in self.classes.items():\n            self.labels[value] = key\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n\n        img = self.load_image(idx)\n        annot = self.load_annotations(idx)\n        sample = {\'img\': img, \'annot\': annot}\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n    def load_image(self, image_index):\n        image_info = self.coco.loadImgs(self.image_ids[image_index])[0]\n        path       = os.path.join(self.root_dir, \'images\', self.set_name, image_info[\'file_name\'])\n        img = skimage.io.imread(path)\n\n        if len(img.shape) == 2:\n            img = skimage.color.gray2rgb(img)\n\n        return img.astype(np.float32)/255.0\n\n    def load_annotations(self, image_index):\n        # get ground truth annotations\n        annotations_ids = self.coco.getAnnIds(imgIds=self.image_ids[image_index], iscrowd=False)\n        annotations     = np.zeros((0, 5))\n\n        # some images appear to miss annotations (like image with id 257034)\n        if len(annotations_ids) == 0:\n            return annotations\n\n        # parse annotations\n        coco_annotations = self.coco.loadAnns(annotations_ids)\n        for idx, a in enumerate(coco_annotations):\n\n            # some annotations have basically no width / height, skip them\n            if a[\'bbox\'][2] < 1 or a[\'bbox\'][3] < 1:\n                continue\n\n            annotation        = np.zeros((1, 5))\n            annotation[0, :4] = a[\'bbox\']\n            annotation[0, 4]  = self.coco_label_to_label(a[\'category_id\'])\n            annotations       = np.append(annotations, annotation, axis=0)\n\n        # transform from [x, y, w, h] to [x1, y1, x2, y2]\n        annotations[:, 2] = annotations[:, 0] + annotations[:, 2]\n        annotations[:, 3] = annotations[:, 1] + annotations[:, 3]\n\n        return annotations\n\n    def coco_label_to_label(self, coco_label):\n        return self.coco_labels_inverse[coco_label]\n\n\n    def label_to_coco_label(self, label):\n        return self.coco_labels[label]\n\n    def image_aspect_ratio(self, image_index):\n        image = self.coco.loadImgs(self.image_ids[image_index])[0]\n        return float(image[\'width\']) / float(image[\'height\'])\n\n    def num_classes(self):\n        return 80\n\n\nclass CSVDataset(Dataset):\n    """"""CSV dataset.""""""\n\n    def __init__(self, train_file, class_list, transform=None):\n        """"""\n        Args:\n            train_file (string): CSV file with training annotations\n            annotations (string): CSV file with class list\n            test_file (string, optional): CSV file with testing annotations\n        """"""\n        self.train_file = train_file\n        self.class_list = class_list\n        self.transform = transform\n\n        # parse the provided class file\n        try:\n            with self._open_for_csv(self.class_list) as file:\n                self.classes = self.load_classes(csv.reader(file, delimiter=\',\'))\n        except ValueError as e:\n            raise_from(ValueError(\'invalid CSV class file: {}: {}\'.format(self.class_list, e)), None)\n\n        self.labels = {}\n        for key, value in self.classes.items():\n            self.labels[value] = key\n\n        # csv with img_path, x1, y1, x2, y2, class_name\n        try:\n            with self._open_for_csv(self.train_file) as file:\n                self.image_data = self._read_annotations(csv.reader(file, delimiter=\',\'), self.classes)\n        except ValueError as e:\n            raise_from(ValueError(\'invalid CSV annotations file: {}: {}\'.format(self.train_file, e)), None)\n        self.image_names = list(self.image_data.keys())\n\n    def _parse(self, value, function, fmt):\n        """"""\n        Parse a string into a value, and format a nice ValueError if it fails.\n        Returns `function(value)`.\n        Any `ValueError` raised is catched and a new `ValueError` is raised\n        with message `fmt.format(e)`, where `e` is the caught `ValueError`.\n        """"""\n        try:\n            return function(value)\n        except ValueError as e:\n            raise_from(ValueError(fmt.format(e)), None)\n\n    def _open_for_csv(self, path):\n        """"""\n        Open a file with flags suitable for csv.reader.\n        This is different for python2 it means with mode \'rb\',\n        for python3 this means \'r\' with ""universal newlines"".\n        """"""\n        if sys.version_info[0] < 3:\n            return open(path, \'rb\')\n        else:\n            return open(path, \'r\', newline=\'\')\n\n\n    def load_classes(self, csv_reader):\n        result = {}\n\n        for line, row in enumerate(csv_reader):\n            line += 1\n\n            try:\n                class_name, class_id = row\n            except ValueError:\n                raise_from(ValueError(\'line {}: format should be \\\'class_name,class_id\\\'\'.format(line)), None)\n            class_id = self._parse(class_id, int, \'line {}: malformed class ID: {{}}\'.format(line))\n\n            if class_name in result:\n                raise ValueError(\'line {}: duplicate class name: \\\'{}\\\'\'.format(line, class_name))\n            result[class_name] = class_id\n        return result\n\n\n    def __len__(self):\n        return len(self.image_names)\n\n    def __getitem__(self, idx):\n\n        img = self.load_image(idx)\n        annot = self.load_annotations(idx)\n        sample = {\'img\': img, \'annot\': annot,\'filename\':self.image_names[idx]}\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n    def load_image(self, image_index):\n        img = skimage.io.imread(self.image_names[image_index])\n\n        if len(img.shape) == 2:\n            img = skimage.color.gray2rgb(img)\n\n        return img.astype(np.float32)/255.0\n\n    def load_annotations(self, image_index):\n        # get ground truth annotations\n        annotation_list = self.image_data[self.image_names[image_index]]\n        annotations     = np.zeros((0, 5))\n\n        # some images appear to miss annotations (like image with id 257034)\n        if len(annotation_list) == 0:\n            return annotations\n\n        # parse annotations\n        for idx, a in enumerate(annotation_list):\n            # some annotations have basically no width / height, skip them\n            x1 = a[\'x1\']\n            x2 = a[\'x2\']\n            y1 = a[\'y1\']\n            y2 = a[\'y2\']\n\n            if (x2-x1) < 1 or (y2-y1) < 1:\n                continue\n\n            annotation        = np.zeros((1, 5))\n            \n            annotation[0, 0] = x1\n            annotation[0, 1] = y1\n            annotation[0, 2] = x2\n            annotation[0, 3] = y2\n\n            annotation[0, 4]  = self.name_to_label(a[\'class\'])\n            annotations       = np.append(annotations, annotation, axis=0)\n\n        return annotations\n\n    def _read_annotations(self, csv_reader, classes):\n        result = {}\n        for line, row in enumerate(csv_reader):\n            line += 1\n\n            try:\n                img_file, x1, y1, x2, y2, class_name = row[:6]\n            except ValueError:\n                raise_from(ValueError(\'line {}: format should be \\\'img_file,x1,y1,x2,y2,class_name\\\' or \\\'img_file,,,,,\\\'\'.format(line)), None)\n\n            if img_file not in result:\n                result[img_file] = []\n\n            # If a row contains only an image path, it\'s an image without annotations.\n            if (x1, y1, x2, y2, class_name) == (\'\', \'\', \'\', \'\', \'\'):\n                continue\n\n            x1 = self._parse(x1, int, \'line {}: malformed x1: {{}}\'.format(line))\n            y1 = self._parse(y1, int, \'line {}: malformed y1: {{}}\'.format(line))\n            x2 = self._parse(x2, int, \'line {}: malformed x2: {{}}\'.format(line))\n            y2 = self._parse(y2, int, \'line {}: malformed y2: {{}}\'.format(line))\n\n            # Check that the bounding box is valid.\n            if x2 <= x1:\n                raise ValueError(\'line {}: x2 ({}) must be higher than x1 ({})\'.format(line, x2, x1))\n            if y2 <= y1:\n                raise ValueError(\'line {}: y2 ({}) must be higher than y1 ({})\'.format(line, y2, y1))\n\n            # check if the current class name is correctly present\n            if class_name not in classes:\n                raise ValueError(\'line {}: unknown class name: \\\'{}\\\' (classes: {})\'.format(line, class_name, classes))\n\n            result[img_file].append({\'x1\': x1, \'x2\': x2, \'y1\': y1, \'y2\': y2, \'class\': class_name})\n        return result\n\n    def name_to_label(self, name):\n        return self.classes[name]\n\n    def label_to_name(self, label):\n        return self.labels[label]\n\n    def num_classes(self):\n        return max(self.classes.values()) + 1\n\n    def image_aspect_ratio(self, image_index):\n        image = Image.open(self.image_names[image_index])\n        return float(image.width) / float(image.height)\n\n\ndef collater(data):\n\n    imgs = [s[\'img\'] for s in data]\n    annots = [s[\'annot\'] for s in data]\n    scales = [s[\'scale\'] for s in data]\n    widths = [int(s.shape[0]) for s in imgs]\n    heights = [int(s.shape[1]) for s in imgs]\n    batch_size = len(imgs)\n\n    max_width = np.array(widths).max()\n    max_height = np.array(heights).max()\n\n    padded_imgs = torch.zeros(batch_size, max_width, max_height, 3)\n\n    for i in range(batch_size):\n        img = imgs[i]\n        padded_imgs[i, :int(img.shape[0]), :int(img.shape[1]), :] = img\n\n    max_num_annots = max(annot.shape[0] for annot in annots)\n    \n    if max_num_annots > 0:\n\n        annot_padded = torch.ones((len(annots), max_num_annots, 5)) * -1\n\n        if max_num_annots > 0:\n            for idx, annot in enumerate(annots):\n                #print(annot.shape)\n                if annot.shape[0] > 0:\n                    annot_padded[idx, :annot.shape[0], :] = annot\n    else:\n        annot_padded = torch.ones((len(annots), 1, 5)) * -1\n\n\n    padded_imgs = padded_imgs.permute(0, 3, 1, 2)\n\n    return {\'img\': padded_imgs, \'annot\': annot_padded, \'scale\': scales}\n\nclass Resizer(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample, min_side=608, max_side=1024):\n        image, annots = sample[\'img\'], sample[\'annot\']\n\n        rows, cols, cns = image.shape\n\n        smallest_side = min(rows, cols)\n\n        # rescale the image so the smallest side is min_side\n        scale = min_side / smallest_side\n\n        # check if the largest side is now greater than max_side, which can happen\n        # when images have a large aspect ratio\n        largest_side = max(rows, cols)\n\n        if largest_side * scale > max_side:\n            scale = max_side / largest_side\n\n        # resize the image with the computed scale\n        image = skimage.transform.resize(image, (int(round(rows*scale)), int(round((cols*scale)))))\n        rows, cols, cns = image.shape\n\n        pad_w = 32 - rows%32\n        pad_h = 32 - cols%32\n\n        new_image = np.zeros((rows + pad_w, cols + pad_h, cns)).astype(np.float32)\n        new_image[:rows, :cols, :] = image.astype(np.float32)\n\n        annots[:, :4] *= scale\n\n        return {\'img\': torch.from_numpy(new_image), \'annot\': torch.from_numpy(annots), \'scale\': scale}\n\n\nclass Augmenter(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample, flip_x=0.5):\n\n        if np.random.rand() < flip_x:\n            image, annots = sample[\'img\'], sample[\'annot\']\n            image = image[:, ::-1, :]\n\n            rows, cols, channels = image.shape\n\n            x1 = annots[:, 0].copy()\n            x2 = annots[:, 2].copy()\n            \n            x_tmp = x1.copy()\n\n            annots[:, 0] = cols - x2\n            annots[:, 2] = cols - x_tmp\n\n            sample = {\'img\': image, \'annot\': annots}\n\n        return sample\n\n\nclass Normalizer(object):\n\n    def __init__(self):\n        self.mean = np.array([[[0.485, 0.456, 0.406]]])\n        self.std = np.array([[[0.229, 0.224, 0.225]]])\n\n    def __call__(self, sample):\n\n        image, annots = sample[\'img\'], sample[\'annot\']\n\n        return {\'img\':((image.astype(np.float32)-self.mean)/self.std), \'annot\': annots}\n\nclass UnNormalizer(object):\n    def __init__(self, mean=None, std=None):\n        if mean == None:\n            self.mean = [0.485, 0.456, 0.406]\n        else:\n            self.mean = mean\n        if std == None:\n            self.std = [0.229, 0.224, 0.225]\n        else:\n            self.std = std\n\n    def __call__(self, tensor):\n        """"""\n        Args:\n            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        Returns:\n            Tensor: Normalized image.\n        """"""\n        for t, m, s in zip(tensor, self.mean, self.std):\n            t.mul_(s).add_(m)\n        return tensor\n\n\nclass AspectRatioBasedSampler(Sampler):\n\n    def __init__(self, data_source, batch_size, drop_last):\n        self.data_source = data_source\n        self.batch_size = batch_size\n        self.drop_last = drop_last\n        self.groups = self.group_images()\n\n    def __iter__(self):\n        random.shuffle(self.groups)\n        for group in self.groups:\n            yield group\n\n    def __len__(self):\n        if self.drop_last:\n            return len(self.data_source) // self.batch_size\n        else:\n            return (len(self.data_source) + self.batch_size - 1) // self.batch_size\n\n    def group_images(self):\n        # determine the order of the images\n        order = list(range(len(self.data_source)))\n        order.sort(key=lambda x: self.data_source.image_aspect_ratio(x))\n\n        # divide into groups, one group = one batch\n        return [[order[x % len(order)] for x in range(i, i + self.batch_size)] for i in range(0, len(order), self.batch_size)]\n'"
retinanet/losses.py,34,"b'import numpy as np\nimport torch\nimport torch.nn as nn\n\ndef calc_iou(a, b):\n    area = (b[:, 2] - b[:, 0]) * (b[:, 3] - b[:, 1])\n\n    iw = torch.min(torch.unsqueeze(a[:, 2], dim=1), b[:, 2]) - torch.max(torch.unsqueeze(a[:, 0], 1), b[:, 0])\n    ih = torch.min(torch.unsqueeze(a[:, 3], dim=1), b[:, 3]) - torch.max(torch.unsqueeze(a[:, 1], 1), b[:, 1])\n\n    iw = torch.clamp(iw, min=0)\n    ih = torch.clamp(ih, min=0)\n\n    ua = torch.unsqueeze((a[:, 2] - a[:, 0]) * (a[:, 3] - a[:, 1]), dim=1) + area - iw * ih\n\n    ua = torch.clamp(ua, min=1e-8)\n\n    intersection = iw * ih\n\n    IoU = intersection / ua\n\n    return IoU\n\nclass FocalLoss(nn.Module):\n    #def __init__(self):\n\n    def forward(self, classifications, regressions, anchors, annotations):\n        alpha = 0.25\n        gamma = 2.0\n        batch_size = classifications.shape[0]\n        classification_losses = []\n        regression_losses = []\n\n        anchor = anchors[0, :, :]\n\n        anchor_widths  = anchor[:, 2] - anchor[:, 0]\n        anchor_heights = anchor[:, 3] - anchor[:, 1]\n        anchor_ctr_x   = anchor[:, 0] + 0.5 * anchor_widths\n        anchor_ctr_y   = anchor[:, 1] + 0.5 * anchor_heights\n\n        for j in range(batch_size):\n\n            classification = classifications[j, :, :]\n            regression = regressions[j, :, :]\n\n            bbox_annotation = annotations[j, :, :]\n            bbox_annotation = bbox_annotation[bbox_annotation[:, 4] != -1]\n\n            if bbox_annotation.shape[0] == 0:\n                regression_losses.append(torch.tensor(0).float().cuda())\n                classification_losses.append(torch.tensor(0).float().cuda())\n\n                continue\n\n            classification = torch.clamp(classification, 1e-4, 1.0 - 1e-4)\n\n            IoU = calc_iou(anchors[0, :, :], bbox_annotation[:, :4]) # num_anchors x num_annotations\n\n            IoU_max, IoU_argmax = torch.max(IoU, dim=1) # num_anchors x 1\n\n            #import pdb\n            #pdb.set_trace()\n\n            # compute the loss for classification\n            targets = torch.ones(classification.shape) * -1\n            targets = targets.cuda()\n\n            targets[torch.lt(IoU_max, 0.4), :] = 0\n\n            positive_indices = torch.ge(IoU_max, 0.5)\n\n            num_positive_anchors = positive_indices.sum()\n\n            assigned_annotations = bbox_annotation[IoU_argmax, :]\n\n            targets[positive_indices, :] = 0\n            targets[positive_indices, assigned_annotations[positive_indices, 4].long()] = 1\n\n            alpha_factor = torch.ones(targets.shape).cuda() * alpha\n\n            alpha_factor = torch.where(torch.eq(targets, 1.), alpha_factor, 1. - alpha_factor)\n            focal_weight = torch.where(torch.eq(targets, 1.), 1. - classification, classification)\n            focal_weight = alpha_factor * torch.pow(focal_weight, gamma)\n\n            bce = -(targets * torch.log(classification) + (1.0 - targets) * torch.log(1.0 - classification))\n\n            # cls_loss = focal_weight * torch.pow(bce, gamma)\n            cls_loss = focal_weight * bce\n\n            cls_loss = torch.where(torch.ne(targets, -1.0), cls_loss, torch.zeros(cls_loss.shape).cuda())\n\n            classification_losses.append(cls_loss.sum()/torch.clamp(num_positive_anchors.float(), min=1.0))\n\n            # compute the loss for regression\n\n            if positive_indices.sum() > 0:\n                assigned_annotations = assigned_annotations[positive_indices, :]\n\n                anchor_widths_pi = anchor_widths[positive_indices]\n                anchor_heights_pi = anchor_heights[positive_indices]\n                anchor_ctr_x_pi = anchor_ctr_x[positive_indices]\n                anchor_ctr_y_pi = anchor_ctr_y[positive_indices]\n\n                gt_widths  = assigned_annotations[:, 2] - assigned_annotations[:, 0]\n                gt_heights = assigned_annotations[:, 3] - assigned_annotations[:, 1]\n                gt_ctr_x   = assigned_annotations[:, 0] + 0.5 * gt_widths\n                gt_ctr_y   = assigned_annotations[:, 1] + 0.5 * gt_heights\n\n                # clip widths to 1\n                gt_widths  = torch.clamp(gt_widths, min=1)\n                gt_heights = torch.clamp(gt_heights, min=1)\n\n                targets_dx = (gt_ctr_x - anchor_ctr_x_pi) / anchor_widths_pi\n                targets_dy = (gt_ctr_y - anchor_ctr_y_pi) / anchor_heights_pi\n                targets_dw = torch.log(gt_widths / anchor_widths_pi)\n                targets_dh = torch.log(gt_heights / anchor_heights_pi)\n\n                targets = torch.stack((targets_dx, targets_dy, targets_dw, targets_dh))\n                targets = targets.t()\n\n                targets = targets/torch.Tensor([[0.1, 0.1, 0.2, 0.2]]).cuda()\n\n\n                negative_indices = 1 - positive_indices\n\n                regression_diff = torch.abs(targets - regression[positive_indices, :])\n\n                regression_loss = torch.where(\n                    torch.le(regression_diff, 1.0 / 9.0),\n                    0.5 * 9.0 * torch.pow(regression_diff, 2),\n                    regression_diff - 0.5 / 9.0\n                )\n                regression_losses.append(regression_loss.mean())\n            else:\n                regression_losses.append(torch.tensor(0).float().cuda())\n\n        return torch.stack(classification_losses).mean(dim=0, keepdim=True), torch.stack(regression_losses).mean(dim=0, keepdim=True)\n\n    \n'"
retinanet/main.py,11,"b'import time\nimport os\nimport copy\nimport argparse\nimport pdb\nimport collections\nimport sys\nimport shutil\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\nfrom torchvision import datasets, models, transforms\nimport torchvision\n\nimport model\nfrom anchors import Anchors\nimport losses\nfrom dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, UnNormalizer, Normalizer\nfrom torch.utils.data import Dataset, DataLoader\n\nimport coco_eval\nimport csv_eval\n\nassert torch.__version__.split(\'.\')[1] == \'4\'\n\nprint(\'CUDA available: {}\'.format(torch.cuda.is_available()))\n\nos.environ[""CUDA_VISIBLE_DEVICES""] = ""1""\ndef main(args=None):\n\n    parser     = argparse.ArgumentParser(description=\'Simple training script for training a RetinaNet network.\')\n\n    parser.add_argument(\'--dataset\', default=""csv"",help=\'Dataset type, must be one of csv or coco.\')\n    parser.add_argument(\'--coco_path\', help=\'Path to COCO directory\')\n    parser.add_argument(\'--csv_train\', default = ""./data/train_only.csv"",help=\'Path to file containing training annotations (see readme)\')\n    parser.add_argument(\'--csv_classes\', default = ""./data/classes.csv"",help=\'Path to file containing class list (see readme)\')\n    parser.add_argument(\'--csv_val\', default = ""./data/train_only.csv"",help=\'Path to file containing validation annotations (optional, see readme)\')\n\n    parser.add_argument(\'--depth\', help=\'Resnet depth, must be one of 18, 34, 50, 101, 152\', type=int, default=101)\n    parser.add_argument(\'--epochs\', help=\'Number of epochs\', type=int, default=40)\n\n    parser = parser.parse_args(args)\n\n    # Create the data loaders\n    if parser.dataset == \'coco\':\n\n        if parser.coco_path is None:\n            raise ValueError(\'Must provide --coco_path when training on COCO,\')\n\n        dataset_train = CocoDataset(parser.coco_path, set_name=\'train2017\', transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]))\n        dataset_val = CocoDataset(parser.coco_path, set_name=\'val2017\', transform=transforms.Compose([Normalizer(), Resizer()]))\n\n    elif parser.dataset == \'csv\':\n\n        if parser.csv_train is None:\n            raise ValueError(\'Must provide --csv_train when training on COCO,\')\n\n        if parser.csv_classes is None:\n            raise ValueError(\'Must provide --csv_classes when training on COCO,\')\n\n\n        dataset_train = CSVDataset(train_file=parser.csv_train, class_list=parser.csv_classes, transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]))\n\n        if parser.csv_val is None:\n            dataset_val = None\n            print(\'No validation annotations provided.\')\n        else:\n            dataset_val = CSVDataset(train_file=parser.csv_val, class_list=parser.csv_classes, transform=transforms.Compose([Normalizer(), Resizer()]))\n\n    else:\n        raise ValueError(\'Dataset type not understood (must be csv or coco), exiting.\')\n\n    sampler = AspectRatioBasedSampler(dataset_train, batch_size=1, drop_last=False)\n    dataloader_train = DataLoader(dataset_train, num_workers=3, collate_fn=collater, batch_sampler=sampler)\n\n    if dataset_val is not None:\n        sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n        dataloader_val = DataLoader(dataset_val, num_workers=3, collate_fn=collater, batch_sampler=sampler_val)\n\n    # Create the model\n    if parser.depth == 18:\n        retinanet = model.resnet18(num_classes=dataset_train.num_classes(), pretrained=True)\n    elif parser.depth == 34:\n        retinanet = model.resnet34(num_classes=dataset_train.num_classes(), pretrained=True)\n    elif parser.depth == 50:\n        retinanet = model.resnet50(num_classes=dataset_train.num_classes(), pretrained=True)\n    elif parser.depth == 101:\n        retinanet = model.resnet101(num_classes=dataset_train.num_classes(), pretrained=True)\n    elif parser.depth == 152:\n        retinanet = model.resnet152(num_classes=dataset_train.num_classes(), pretrained=True)\n    else:\n        raise ValueError(\'Unsupported model depth, must be one of 18, 34, 50, 101, 152\')\t\t\n\n    use_gpu = True\n\n    if use_gpu:\n        retinanet = retinanet.cuda()\n\n    retinanet = torch.nn.DataParallel(retinanet).cuda()\n\n    retinanet.training = True\n\n    optimizer = optim.Adam(retinanet.parameters(), lr=1e-5)\n\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True,mode=""max"")\n    #scheduler = optim.lr_scheduler.StepLR(optimizer,8)\n    loss_hist = collections.deque(maxlen=500)\n\n    retinanet.train()\n    retinanet.module.freeze_bn()\n    if not os.path.exists(""./logs""):\n        os.mkdir(""./logs"")\n    if not os.path.exists(\'best_models\'): \n        os.makedirs(\'best_models\')\n    \n    log_file = open(""./logs/log.txt"",""w"")\n    print(\'Num training images: {}\'.format(len(dataset_train)))\n    best_map = 0\n    print(""Training models..."")\n    for epoch_num in range(parser.epochs):\n\n        #scheduler.step(epoch_num)\t\n        retinanet.train()\n        retinanet.module.freeze_bn()\n        \n        epoch_loss = []\n        \n        for iter_num, data in enumerate(dataloader_train):\n            try:\n                #print(csv_eval.evaluate(dataset_val[:20], retinanet)[0])\n                #print(type(csv_eval.evaluate(dataset_val, retinanet)))\n                optimizer.zero_grad()\n\n                classification_loss, regression_loss = retinanet([data[\'img\'].cuda().float(), data[\'annot\']])\n\n                classification_loss = classification_loss.mean()\n                regression_loss = regression_loss.mean()\n\n                loss = classification_loss + regression_loss\n                \n                if bool(loss == 0):\n                    continue\n\n                loss.backward()\n\n                torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.1)\n\n                optimizer.step()\n\n                loss_hist.append(float(loss))\n\n                epoch_loss.append(float(loss))\n                if iter_num % 50 == 0:\n                    print(\'Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f}\'.format(epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist)))\n                    log_file.write(\'Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f} \\n\'.format(epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist)))\n                del classification_loss\n                del regression_loss\n            except Exception as e:\n                print(e)\n                continue\n\n        if parser.dataset == \'coco\':\n\n            print(\'Evaluating dataset\')\n\n            coco_eval.evaluate_coco(dataset_val, retinanet)\n\n        elif parser.dataset == \'csv\' and parser.csv_val is not None:\n\n            print(\'Evaluating dataset\')\n\n            mAP = csv_eval.evaluate(dataset_val, retinanet)\n        \n        try:\n            is_best_map = mAP[0][0] > best_map\n            best_map = max(mAP[0][0],best_map)\n        except:\n            pass\n        if is_best_map:\n            print(""Get better map: "",best_map)\n        \n            torch.save(retinanet.module, \'./logs/{}_scale15_{}.pt\'.format(epoch_num,best_map))\n            shutil.copyfile(\'./logs/{}_scale15_{}.pt\'.format(epoch_num,best_map),""./best_models/model.pt"")\n        else:\n            print(""Current map: "",best_map)\n        scheduler.step(best_map)\n    retinanet.eval()\n\n    torch.save(retinanet, \'./logs/model_final.pt\')\n\nif __name__ == \'__main__\':\n main()\n'"
retinanet/model.py,13,"b'import torch.nn as nn\nimport torch\nimport math\nimport time\nimport torch.utils.model_zoo as model_zoo\nfrom utils import BasicBlock, Bottleneck, BBoxTransform, ClipBoxes\nfrom anchors import Anchors\nimport losses\nfrom lib.nms.pth_nms import pth_nms\n\ndef nms(dets, thresh):\n    ""Dispatch to either CPU or GPU NMS implementations.\\\n    Accept dets as tensor""""""\n    return pth_nms(dets, thresh)\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n}\n\nclass PyramidFeatures(nn.Module):\n    def __init__(self, C3_size, C4_size, C5_size, feature_size=256):\n        super(PyramidFeatures, self).__init__()\n        \n        # upsample C5 to get P5 from the FPN paper\n        self.P5_1           = nn.Conv2d(C5_size, feature_size, kernel_size=1, stride=1, padding=0)\n        self.P5_upsampled   = nn.Upsample(scale_factor=2, mode=\'nearest\')\n        self.P5_2           = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n\n        # add P5 elementwise to C4\n        self.P4_1           = nn.Conv2d(C4_size, feature_size, kernel_size=1, stride=1, padding=0)\n        self.P4_upsampled   = nn.Upsample(scale_factor=2, mode=\'nearest\')\n        self.P4_2           = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n\n        # add P4 elementwise to C3\n        self.P3_1 = nn.Conv2d(C3_size, feature_size, kernel_size=1, stride=1, padding=0)\n        self.P3_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=1, padding=1)\n\n        # ""P6 is obtained via a 3x3 stride-2 conv on C5""\n        self.P6 = nn.Conv2d(C5_size, feature_size, kernel_size=3, stride=2, padding=1)\n\n        # ""P7 is computed by applying ReLU followed by a 3x3 stride-2 conv on P6""\n        self.P7_1 = nn.ReLU()\n        self.P7_2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, stride=2, padding=1)\n\n    def forward(self, inputs):\n\n        C3, C4, C5 = inputs\n\n        P5_x = self.P5_1(C5)\n        P5_upsampled_x = self.P5_upsampled(P5_x)\n        P5_x = self.P5_2(P5_x)\n        \n        P4_x = self.P4_1(C4)\n        P4_x = P5_upsampled_x + P4_x\n        P4_upsampled_x = self.P4_upsampled(P4_x)\n        P4_x = self.P4_2(P4_x)\n\n        P3_x = self.P3_1(C3)\n        P3_x = P3_x + P4_upsampled_x\n        P3_x = self.P3_2(P3_x)\n\n        P6_x = self.P6(C5)\n\n        P7_x = self.P7_1(P6_x)\n        P7_x = self.P7_2(P7_x)\n\n        return [P3_x, P4_x, P5_x, P6_x, P7_x]\n\n\nclass RegressionModel(nn.Module):\n    def __init__(self, num_features_in, num_anchors=9, feature_size=256):\n        super(RegressionModel, self).__init__()\n        \n        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n        self.act1 = nn.ReLU()\n\n        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act2 = nn.ReLU()\n\n        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act3 = nn.ReLU()\n\n        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act4 = nn.ReLU()\n\n        self.output = nn.Conv2d(feature_size, num_anchors*4, kernel_size=3, padding=1)\n\n    def forward(self, x):\n\n        out = self.conv1(x)\n        out = self.act1(out)\n\n        out = self.conv2(out)\n        out = self.act2(out)\n\n        out = self.conv3(out)\n        out = self.act3(out)\n\n        out = self.conv4(out)\n        out = self.act4(out)\n\n        out = self.output(out)\n\n        # out is B x C x W x H, with C = 4*num_anchors\n        out = out.permute(0, 2, 3, 1)\n\n        return out.contiguous().view(out.shape[0], -1, 4)\n\nclass ClassificationModel(nn.Module):\n    def __init__(self, num_features_in, num_anchors=9, num_classes=80, prior=0.01, feature_size=256):\n        super(ClassificationModel, self).__init__()\n\n        self.num_classes = num_classes\n        self.num_anchors = num_anchors\n        \n        self.conv1 = nn.Conv2d(num_features_in, feature_size, kernel_size=3, padding=1)\n        self.act1 = nn.ReLU()\n\n        self.conv2 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act2 = nn.ReLU()\n\n        self.conv3 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act3 = nn.ReLU()\n\n        self.conv4 = nn.Conv2d(feature_size, feature_size, kernel_size=3, padding=1)\n        self.act4 = nn.ReLU()\n\n        self.output = nn.Conv2d(feature_size, num_anchors*num_classes, kernel_size=3, padding=1)\n        self.output_act = nn.Sigmoid()\n\n    def forward(self, x):\n\n        out = self.conv1(x)\n        out = self.act1(out)\n\n        out = self.conv2(out)\n        out = self.act2(out)\n\n        out = self.conv3(out)\n        out = self.act3(out)\n\n        out = self.conv4(out)\n        out = self.act4(out)\n\n        out = self.output(out)\n        out = self.output_act(out)\n\n        # out is B x C x W x H, with C = n_classes + n_anchors\n        out1 = out.permute(0, 2, 3, 1)\n\n        batch_size, width, height, channels = out1.shape\n\n        out2 = out1.view(batch_size, width, height, self.num_anchors, self.num_classes)\n\n        return out2.contiguous().view(x.shape[0], -1, self.num_classes)\n\nclass ResNet(nn.Module):\n\n    def __init__(self, num_classes, block, layers):\n        self.inplanes = 64\n        super(ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        if block == BasicBlock:\n            fpn_sizes = [self.layer2[layers[1]-1].conv2.out_channels, self.layer3[layers[2]-1].conv2.out_channels, self.layer4[layers[3]-1].conv2.out_channels]\n        elif block == Bottleneck:\n            fpn_sizes = [self.layer2[layers[1]-1].conv3.out_channels, self.layer3[layers[2]-1].conv3.out_channels, self.layer4[layers[3]-1].conv3.out_channels]\n\n        self.fpn = PyramidFeatures(fpn_sizes[0], fpn_sizes[1], fpn_sizes[2])\n\n        self.regressionModel = RegressionModel(256)\n        self.classificationModel = ClassificationModel(256, num_classes=num_classes)\n\n        self.anchors = Anchors()\n\n        self.regressBoxes = BBoxTransform()\n\n        self.clipBoxes = ClipBoxes()\n        \n        self.focalLoss = losses.FocalLoss()\n                \n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n        prior = 0.01\n        \n        self.classificationModel.output.weight.data.fill_(0)\n        self.classificationModel.output.bias.data.fill_(-math.log((1.0-prior)/prior))\n\n        self.regressionModel.output.weight.data.fill_(0)\n        self.regressionModel.output.bias.data.fill_(0)\n\n        self.freeze_bn()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def freeze_bn(self):\n        \'\'\'Freeze BatchNorm layers.\'\'\'\n        for layer in self.modules():\n            if isinstance(layer, nn.BatchNorm2d):\n                layer.eval()\n\n    def forward(self, inputs):\n\n        if self.training:\n            img_batch, annotations = inputs\n        else:\n            img_batch = inputs\n            \n        x = self.conv1(img_batch)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x1 = self.layer1(x)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        features = self.fpn([x2, x3, x4])\n\n        regression = torch.cat([self.regressionModel(feature) for feature in features], dim=1)\n\n        classification = torch.cat([self.classificationModel(feature) for feature in features], dim=1)\n\n        anchors = self.anchors(img_batch)\n\n        if self.training:\n            return self.focalLoss(classification, regression, anchors, annotations)\n        else:\n            transformed_anchors = self.regressBoxes(anchors, regression)\n            transformed_anchors = self.clipBoxes(transformed_anchors, img_batch)\n\n            scores = torch.max(classification, dim=2, keepdim=True)[0]\n\n            scores_over_thresh = (scores>0.05)[0, :, 0]\n\n            if scores_over_thresh.sum() == 0:\n                # no boxes to NMS, just return\n                return [torch.zeros([1]).cuda(0), torch.zeros([1]).cuda(0), torch.zeros([1, 4]).cuda(0)]\n                #return [torch.zeros(0), torch.zeros(0), torch.zeros(0, 4)]\n\n            classification = classification[:, scores_over_thresh, :]\n            transformed_anchors = transformed_anchors[:, scores_over_thresh, :]\n            scores = scores[:, scores_over_thresh, :]\n\n            anchors_nms_idx = nms(torch.cat([transformed_anchors, scores], dim=2)[0, :, :], 0.25)\n\n            nms_scores, nms_class = classification[0, anchors_nms_idx, :].max(dim=1)\n\n            return [nms_scores, nms_class, transformed_anchors[0, anchors_nms_idx, :]]\n\n\n\ndef resnet18(num_classes, pretrained=False, **kwargs):\n    """"""Constructs a ResNet-18 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(num_classes, BasicBlock, [2, 2, 2, 2], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet18\'], model_dir=\'/home/user/.torch/models/\'), strict=False)\n    return model\n\n\ndef resnet34(num_classes, pretrained=False, **kwargs):\n    """"""Constructs a ResNet-34 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(num_classes, BasicBlock, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet34\'], model_dir=\'/home/user/.torch/models/\'), strict=False)\n    return model\n\n\ndef resnet50(num_classes, pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(num_classes, Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet50\'], model_dir=\'/home/user/.torch/models/\'), strict=False)\n    return model\n\ndef resnet101(num_classes, pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(num_classes, Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet101\'], model_dir=\'/home/user/.torch/models/\'), strict=False)\n    return model\n\n\ndef resnet152(num_classes, pretrained=False, **kwargs):\n    """"""Constructs a ResNet-152 model.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(num_classes, Bottleneck, [3, 8, 36, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls[\'resnet152\'], model_dir=\'/home/user/.torch/models/\'), strict=False)\n    return model\n'"
retinanet/oid_dataset.py,1,"b'from __future__ import print_function, division\n\nimport csv\nimport json\nimport os\nimport warnings\n\nimport numpy as np\nimport skimage\nimport skimage.color\nimport skimage.io\nimport skimage.transform\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n\n\ndef get_labels(metadata_dir, version=\'v4\'):\n    if version == \'v4\' or version == \'challenge2018\':\n        csv_file = \'class-descriptions-boxable.csv\' if version == \'v4\' else \'challenge-2018-class-descriptions-500.csv\'\n\n        boxable_classes_descriptions = os.path.join(metadata_dir, csv_file)\n        id_to_labels = {}\n        cls_index = {}\n\n        i = 0\n        with open(boxable_classes_descriptions) as f:\n            for row in csv.reader(f):\n                # make sure the csv row is not empty (usually the last one)\n                if len(row):\n                    label = row[0]\n                    description = row[1].replace(""\\"""", """").replace(""\'"", """").replace(\'`\', \'\')\n\n                    id_to_labels[i] = description\n                    cls_index[label] = i\n\n                    i += 1\n    else:\n        trainable_classes_path = os.path.join(metadata_dir, \'classes-bbox-trainable.txt\')\n        description_path = os.path.join(metadata_dir, \'class-descriptions.csv\')\n\n        description_table = {}\n        with open(description_path) as f:\n            for row in csv.reader(f):\n                # make sure the csv row is not empty (usually the last one)\n                if len(row):\n                    description_table[row[0]] = row[1].replace(""\\"""", """").replace(""\'"", """").replace(\'`\', \'\')\n\n        with open(trainable_classes_path, \'rb\') as f:\n            trainable_classes = f.read().split(\'\\n\')\n\n        id_to_labels = dict([(i, description_table[c]) for i, c in enumerate(trainable_classes)])\n        cls_index = dict([(c, i) for i, c in enumerate(trainable_classes)])\n\n    return id_to_labels, cls_index\n\n\ndef generate_images_annotations_json(main_dir, metadata_dir, subset, cls_index, version=\'v4\'):\n    validation_image_ids = {}\n\n    if version == \'v4\':\n        annotations_path = os.path.join(metadata_dir, subset, \'{}-annotations-bbox.csv\'.format(subset))\n    elif version == \'challenge2018\':\n        validation_image_ids_path = os.path.join(metadata_dir, \'challenge-2018-image-ids-valset-od.csv\')\n\n        with open(validation_image_ids_path, \'r\') as csv_file:\n            reader = csv.DictReader(csv_file, fieldnames=[\'ImageID\'])\n            reader.next()\n            for line, row in enumerate(reader):\n                image_id = row[\'ImageID\']\n                validation_image_ids[image_id] = True\n\n        annotations_path = os.path.join(metadata_dir, \'challenge-2018-train-annotations-bbox.csv\')\n    else:\n        annotations_path = os.path.join(metadata_dir, subset, \'annotations-human-bbox.csv\')\n\n    fieldnames = [\'ImageID\', \'Source\', \'LabelName\', \'Confidence\',\n                  \'XMin\', \'XMax\', \'YMin\', \'YMax\',\n                  \'IsOccluded\', \'IsTruncated\', \'IsGroupOf\', \'IsDepiction\', \'IsInside\']\n\n    id_annotations = dict()\n    with open(annotations_path, \'r\') as csv_file:\n        reader = csv.DictReader(csv_file, fieldnames=fieldnames)\n        next(reader)\n\n        images_sizes = {}\n        for line, row in enumerate(reader):\n            frame = row[\'ImageID\']\n\n            if version == \'challenge2018\':\n                if subset == \'train\':\n                    if frame in validation_image_ids:\n                        continue\n                elif subset == \'validation\':\n                    if frame not in validation_image_ids:\n                        continue\n                else:\n                    raise NotImplementedError(\'This generator handles only the train and validation subsets\')\n\n            class_name = row[\'LabelName\']\n\n            if class_name not in cls_index:\n                continue\n\n            cls_id = cls_index[class_name]\n\n            if version == \'challenge2018\':\n                # We recommend participants to use the provided subset of the training set as a validation set.\n                # This is preferable over using the V4 val/test sets, as the training set is more densely annotated.\n                img_path = os.path.join(main_dir, \'images\', \'train\', frame + \'.jpg\')\n            else:\n                img_path = os.path.join(main_dir, \'images\', subset, frame + \'.jpg\')\n\n            if frame in images_sizes:\n                width, height = images_sizes[frame]\n            else:\n                try:\n                    with Image.open(img_path) as img:\n                        width, height = img.width, img.height\n                        images_sizes[frame] = (width, height)\n                except Exception as ex:\n                    if version == \'challenge2018\':\n                        raise ex\n                    continue\n\n            x1 = float(row[\'XMin\'])\n            x2 = float(row[\'XMax\'])\n            y1 = float(row[\'YMin\'])\n            y2 = float(row[\'YMax\'])\n\n            x1_int = int(round(x1 * width))\n            x2_int = int(round(x2 * width))\n            y1_int = int(round(y1 * height))\n            y2_int = int(round(y2 * height))\n\n            # Check that the bounding box is valid.\n            if x2 <= x1:\n                raise ValueError(\'line {}: x2 ({}) must be higher than x1 ({})\'.format(line, x2, x1))\n            if y2 <= y1:\n                raise ValueError(\'line {}: y2 ({}) must be higher than y1 ({})\'.format(line, y2, y1))\n\n            if y2_int == y1_int:\n                warnings.warn(\'filtering line {}: rounding y2 ({}) and y1 ({}) makes them equal\'.format(line, y2, y1))\n                continue\n\n            if x2_int == x1_int:\n                warnings.warn(\'filtering line {}: rounding x2 ({}) and x1 ({}) makes them equal\'.format(line, x2, x1))\n                continue\n\n            img_id = row[\'ImageID\']\n            annotation = {\'cls_id\': cls_id, \'x1\': x1, \'x2\': x2, \'y1\': y1, \'y2\': y2}\n\n            if img_id in id_annotations:\n                annotations = id_annotations[img_id]\n                annotations[\'boxes\'].append(annotation)\n            else:\n                id_annotations[img_id] = {\'w\': width, \'h\': height, \'boxes\': [annotation]}\n    return id_annotations\n\n\nclass OidDataset(Dataset):\n    """"""Oid dataset.""""""\n\n    def __init__(self, main_dir, subset, version=\'v4\', annotation_cache_dir=\'.\', transform=None):\n        if version == \'v4\':\n            metadata = \'2018_04\'\n        elif version == \'challenge2018\':\n            metadata = \'challenge2018\'\n        elif version == \'v3\':\n            metadata = \'2017_11\'\n        else:\n            raise NotImplementedError(\'There is currently no implementation for versions older than v3\')\n\n        self.transform = transform\n\n        if version == \'challenge2018\':\n            self.base_dir = os.path.join(main_dir, \'images\', \'train\')\n        else:\n            self.base_dir = os.path.join(main_dir, \'images\', subset)\n\n        metadata_dir = os.path.join(main_dir, metadata)\n        annotation_cache_json = os.path.join(annotation_cache_dir, subset + \'.json\')\n\n        self.id_to_labels, cls_index = get_labels(metadata_dir, version=version)\n\n        if os.path.exists(annotation_cache_json):\n            with open(annotation_cache_json, \'r\') as f:\n                self.annotations = json.loads(f.read())\n        else:\n            self.annotations = generate_images_annotations_json(main_dir, metadata_dir, subset, cls_index,\n                                                                version=version)\n            json.dump(self.annotations, open(annotation_cache_json, ""w""))\n\n        self.id_to_image_id = dict([(i, k) for i, k in enumerate(self.annotations)])\n\n        # (label -> name)\n        self.labels = self.id_to_labels\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n\n        img = self.load_image(idx)\n        annot = self.load_annotations(idx)\n        sample = {\'img\': img, \'annot\': annot}\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n    def image_path(self, image_index):\n        path = os.path.join(self.base_dir, self.id_to_image_id[image_index] + \'.jpg\')\n        return path\n\n    def load_image(self, image_index):\n        path = self.image_path(image_index)\n        img = skimage.io.imread(path)\n\n        if len(img.shape) == 1:\n            img = img[0]\n\n        if len(img.shape) == 2:\n            img = skimage.color.gray2rgb(img)\n\n        try:\n            return img.astype(np.float32) / 255.0\n        except Exception:\n            print (path)\n            exit(0)\n\n    def load_annotations(self, image_index):\n        # get ground truth annotations\n        image_annotations = self.annotations[self.id_to_image_id[image_index]]\n\n        labels = image_annotations[\'boxes\']\n        height, width = image_annotations[\'h\'], image_annotations[\'w\']\n\n        boxes = np.zeros((len(labels), 5))\n        for idx, ann in enumerate(labels):\n            cls_id = ann[\'cls_id\']\n            x1 = ann[\'x1\'] * width\n            x2 = ann[\'x2\'] * width\n            y1 = ann[\'y1\'] * height\n            y2 = ann[\'y2\'] * height\n\n            boxes[idx, 0] = x1\n            boxes[idx, 1] = y1\n            boxes[idx, 2] = x2\n            boxes[idx, 3] = y2\n            boxes[idx, 4] = cls_id\n\n        return boxes\n\n    def image_aspect_ratio(self, image_index):\n        img_annotations = self.annotations[self.id_to_image_id[image_index]]\n        height, width = img_annotations[\'h\'], img_annotations[\'w\']\n        return float(width) / float(height)\n\n    def num_classes(self):\n        return len(self.id_to_labels)\n'"
retinanet/predict.py,6,"b'import numpy as np\nimport torchvision\nimport time\nimport os\nimport copy\nimport pdb\nimport time\nimport argparse\n\nimport sys\nimport cv2\nimport skimage\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, models, transforms\nfrom IPython import embed\nfrom dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, UnNormalizer, Normalizer\nfrom torchvision import transforms as T\nfrom glob import glob\nassert torch.__version__.split(\'.\')[1] == \'4\'\n\nprint(\'CUDA available: {}\'.format(torch.cuda.is_available()))\n\n# threshold for class score\nthreshold = 0.5\nresults_file = open(""./submit/larger%s.csv""%str(threshold),""w"")\n\nif not os.path.exists(""./submit/""):\n    os.mkdir(""./submit/"")\n\nif not os.path.exists(""./outputs/""):\n    os.mkdir(""./outputs/"")\nif not os.path.exists(""./best_models/""):\n    os.mkdir(""./best_models/"")\n\ndef demo(image_lists):\n    classes = [""gangjin""]\n    model = ""./best_models/model.pt""\n    retinanet = torch.load(model)\n    retinanet = retinanet.cuda()\n    retinanet.eval()\n    #detect\n    transforms = T.Compose([\n        Normalizer(),\n        Resizer()\n        ])\n    for filename in image_lists:\n        image = skimage.io.imread(filename)\n        sampler = {""img"":image.astype(np.float32)/255.0,""annot"":np.empty(shape=(5,5))}\n        image_tf = transforms(sampler)\n        scale = image_tf[""scale""]\n        new_shape = image_tf[\'img\'].shape\n        x = torch.autograd.Variable(image_tf[\'img\'].unsqueeze(0).transpose(1,3), volatile=True)\n        with torch.no_grad():\n            scores,_,bboxes = retinanet(x.cuda().float())\n            bboxes /= scale\n            scores = scores.cpu().data.numpy()\n            bboxes = bboxes.cpu().data.numpy()\n            # select threshold\n            idxs = np.where(scores > threshold)[0]\n            scores = scores[idxs]\n            bboxes = bboxes[idxs]\n            #embed()\n            for i,box in enumerate(bboxes):\n                 cv2.rectangle(image,(int(box[1]),int(box[0])),(int(box[3]),int(box[2])),color=(0,0,255),thickness=2 )\n                 results_file.write(filename.split(""/"")[-1] +"",""+ str(int(box[1])) + "" "" + str(int(box[0])) +  "" "" + str(int(box[3])) + "" "" +str(int(box[2])) + ""\\n"")\n            print(""Predicting image: %s ""%filename)\n            cv2.imwrite(""./outputs/%s""%filename.split(""/"")[-1],image)\nif __name__ == ""__main__"":\n    root = ""./data/images/test/""\n    image_lists = glob(root+""*.jpg"")\n    demo(image_lists)\n'"
retinanet/utils.py,10,"b'import torch\nimport torch.nn as nn\nimport numpy as np\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\nclass BBoxTransform(nn.Module):\n\n    def __init__(self, mean=None, std=None):\n        super(BBoxTransform, self).__init__()\n        if mean is None:\n            self.mean = torch.from_numpy(np.array([0, 0, 0, 0]).astype(np.float32)).cuda()\n        else:\n            self.mean = mean\n        if std is None:\n            self.std = torch.from_numpy(np.array([0.1, 0.1, 0.2, 0.2]).astype(np.float32)).cuda()\n        else:\n            self.std = std\n\n    def forward(self, boxes, deltas):\n\n        widths  = boxes[:, :, 2] - boxes[:, :, 0]\n        heights = boxes[:, :, 3] - boxes[:, :, 1]\n        ctr_x   = boxes[:, :, 0] + 0.5 * widths\n        ctr_y   = boxes[:, :, 1] + 0.5 * heights\n\n        dx = deltas[:, :, 0] * self.std[0] + self.mean[0]\n        dy = deltas[:, :, 1] * self.std[1] + self.mean[1]\n        dw = deltas[:, :, 2] * self.std[2] + self.mean[2]\n        dh = deltas[:, :, 3] * self.std[3] + self.mean[3]\n\n        pred_ctr_x = ctr_x + dx * widths\n        pred_ctr_y = ctr_y + dy * heights\n        pred_w     = torch.exp(dw) * widths\n        pred_h     = torch.exp(dh) * heights\n\n        pred_boxes_x1 = pred_ctr_x - 0.5 * pred_w\n        pred_boxes_y1 = pred_ctr_y - 0.5 * pred_h\n        pred_boxes_x2 = pred_ctr_x + 0.5 * pred_w\n        pred_boxes_y2 = pred_ctr_y + 0.5 * pred_h\n\n        pred_boxes = torch.stack([pred_boxes_x1, pred_boxes_y1, pred_boxes_x2, pred_boxes_y2], dim=2)\n\n        return pred_boxes\n\n\nclass ClipBoxes(nn.Module):\n\n    def __init__(self, width=None, height=None):\n        super(ClipBoxes, self).__init__()\n\n    def forward(self, boxes, img):\n\n        batch_size, num_channels, height, width = img.shape\n\n        boxes[:, :, 0] = torch.clamp(boxes[:, :, 0], min=0)\n        boxes[:, :, 1] = torch.clamp(boxes[:, :, 1], min=0)\n\n        boxes[:, :, 2] = torch.clamp(boxes[:, :, 2], max=width)\n        boxes[:, :, 3] = torch.clamp(boxes[:, :, 3], max=height)\n      \n        return boxes\n'"
retinanet/lib/__init__.py,0,b''
retinanet/lib/nms/__init__.py,0,b''
retinanet/lib/nms/build.py,2,"b""import os\nimport torch\nfrom torch.utils.ffi import create_extension\n\n\nsources = ['src/nms.c']\nheaders = ['src/nms.h']\ndefines = []\nwith_cuda = False\n\nif torch.cuda.is_available():\n    print('Including CUDA code.')\n    sources += ['src/nms_cuda.c']\n    headers += ['src/nms_cuda.h']\n    defines += [('WITH_CUDA', None)]\n    with_cuda = True\n\nthis_file = os.path.dirname(os.path.realpath(__file__))\nprint(this_file)\nextra_objects = ['src/cuda/nms_kernel.cu.o']\nextra_objects = [os.path.join(this_file, fname) for fname in extra_objects]\n\nffi = create_extension(\n    '_ext.nms',\n    headers=headers,\n    sources=sources,\n    define_macros=defines,\n    relative_to=__file__,\n    with_cuda=with_cuda,\n    extra_objects=extra_objects,\n    extra_compile_args=['-std=c99']\n)\n\nif __name__ == '__main__':\n    ffi.build()\n"""
retinanet/lib/nms/pth_nms.py,8,"b'import torch\nfrom ._ext import nms\nimport numpy as np\n\ndef pth_nms(dets, thresh):\n  """"""\n  dets has to be a tensor\n  """"""\n  if not dets.is_cuda:\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.sort(0, descending=True)[1]\n    # order = torch.from_numpy(np.ascontiguousarray(scores.numpy().argsort()[::-1])).long()\n\n    keep = torch.LongTensor(dets.size(0))\n    num_out = torch.LongTensor(1)\n    nms.cpu_nms(keep, num_out, dets, order, areas, thresh)\n\n    return keep[:num_out[0]]\n  else:\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.sort(0, descending=True)[1]\n    # order = torch.from_numpy(np.ascontiguousarray(scores.cpu().numpy().argsort()[::-1])).long().cuda()\n\n    dets = dets[order].contiguous()\n\n    keep = torch.LongTensor(dets.size(0))\n    num_out = torch.LongTensor(1)\n    # keep = torch.cuda.LongTensor(dets.size(0))\n    # num_out = torch.cuda.LongTensor(1)\n    nms.gpu_nms(keep, num_out, dets, thresh)\n\n    return order[keep[:num_out[0]].cuda()].contiguous()\n    # return order[keep[:num_out[0]]].contiguous()\n\n'"
retinanet/lib/nms/_ext/__init__.py,0,b''
retinanet/lib/nms/_ext/nms/__init__.py,1,"b'\nfrom torch.utils.ffi import _wrap_function\nfrom ._nms import lib as _lib, ffi as _ffi\n\n__all__ = []\ndef _import_symbols(locals):\n    for symbol in dir(_lib):\n        fn = getattr(_lib, symbol)\n        if callable(fn):\n            locals[symbol] = _wrap_function(fn, _ffi)\n        else:\n            locals[symbol] = fn\n        __all__.append(symbol)\n\n_import_symbols(locals())\n'"
