file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\nimport re\n\nwith open(\'README.md\', encoding=\'utf-8\') as f:\n    readme = f.read()\n\nwith open(""torchsat/__init__.py"", encoding=""utf-8"") as f:\n    version = re.search(r\'__version__ = ""(.*?)""\', f.read()).group(1)\n\nrequirements = [x.strip() for x in open(""requirements.txt"").readlines()]\n\nsetup(\n    name=""torchsat"",\n    version=version,\n    author=""sshuair"",\n    author_email=""sshuair@gmail.com"",\n    url=""https://github.com/sshuair/torchsat"",\n    description=""TorchSat is an open-source PyTorch framework for satellite imagery analysis."",\n    long_description=readme,\n    long_description_content_type=""text/markdown"",\n    packages=find_packages(),\n    license=""MIT"",\n    install_requires=requirements,\n    keywords=\'pytorch deep learning satellite remote sensing\',\n    entry_points=\'\'\'\n        [console_scripts]\n        ts=torchsat.cli.cli:entry_point\n    \'\'\'\n)\n'"
tests/test_models.py,2,"b""import pytest\nimport torch\n# from torchsat.models.classification import resnet, densenet, vgg, inception, mobilenet\nimport torchsat.models.classification as clss\nimport torchsat.models.segmentation as seg\n\nIN_CHANNELS = [1, 3, 8]\n\ncls_models = [k for k, v in clss.__dict__.items() if callable(v) and k.lower()==k and k[0]!='_']\n\n@pytest.mark.parametrize('model_name', cls_models)\ndef test_classification_models(model_name):\n    assert clss.__dict__[model_name](2, in_channels=1,  pretrained=False)\n    assert clss.__dict__[model_name](3, in_channels=2,  pretrained=False)\n    assert clss.__dict__[model_name](4, in_channels=3,  pretrained=False)\n    assert clss.__dict__[model_name](5, in_channels=7,  pretrained=False)\n    assert clss.__dict__[model_name](6, in_channels=3,  pretrained=False)\n    assert clss.__dict__[model_name](2, in_channels=5,  pretrained=True)\n    \n    num_classes=10\n    in_channels=5\n    batch_size=2\n    model = clss.__dict__[model_name](num_classes, in_channels=in_channels)\n    model.eval()\n    if model_name in ['inception_v3']:\n        input_shape = (batch_size, in_channels, 299, 299)\n    else:\n        input_shape = (batch_size, in_channels, 256, 256)\n    x = torch.rand(size=input_shape)\n    out = model(x)\n    assert out.shape[-1] == num_classes\n\n\nseg_models = [k for k, v in seg.__dict__.items() if callable(v) and k.lower()==k and k[0]!='_']\n@pytest.mark.parametrize('model_name', seg_models)\ndef test_segmentation_models(model_name):\n    assert seg.__dict__[model_name](2, in_channels=1,  pretrained=False)\n    assert seg.__dict__[model_name](3, in_channels=2,  pretrained=False)\n    assert seg.__dict__[model_name](4, in_channels=3,  pretrained=False)\n    assert seg.__dict__[model_name](5, in_channels=7,  pretrained=False)\n    assert seg.__dict__[model_name](6, in_channels=3,  pretrained=False)\n    assert seg.__dict__[model_name](2, in_channels=5,  pretrained=True)\n    \n    num_classes=10\n    in_channels=5\n    batch_size=2\n    model = seg.__dict__[model_name](num_classes, in_channels=in_channels)\n    model.eval()\n    input_shape = (batch_size, in_channels, 256, 256)\n    x = torch.rand(size=input_shape)\n    out = model(x)\n    assert out.shape[0] == batch_size\n    assert list(out.shape[2:]) == [input_shape[-2], input_shape[-1]]\n\n\ndef test_temp():\n    clss.resnet18(2, in_channels=3, pretrained=False)"""
tests/test_transform_cls.py,8,"b""from pathlib import Path\nimport math\n\nimport numpy as np\nimport pytest\nimport tifffile\nimport torch\nfrom PIL import Image\n\nfrom torchsat.transforms import transforms_cls\n\ntiff_files = [\n    './tests/fixtures/different-types/tiff_1channel_float.tif',\n    './tests/fixtures/different-types/tiff_1channel_uint16.tif',\n    './tests/fixtures/different-types/tiff_1channel_uint8.tif',\n    './tests/fixtures/different-types/tiff_3channel_float.tif',\n    './tests/fixtures/different-types/tiff_3channel_uint16.tif',\n    './tests/fixtures/different-types/tiff_3channel_uint8.tif',\n    './tests/fixtures/different-types/tiff_8channel_float.tif',\n    './tests/fixtures/different-types/tiff_8channel_uint16.tif',\n    './tests/fixtures/different-types/tiff_8channel_uint8.tif',\n]\n\njpeg_files = [\n    './tests/fixtures/different-types/jpeg_1channel_uint8.jpeg',\n    './tests/fixtures/different-types/jpeg_3channel_uint8.jpeg',\n    './tests/fixtures/different-types/jpeg_1channel_uint8.png',\n    './tests/fixtures/different-types/jpeg_3channel_uint8.png',\n]\n\n\ndef read_img(fp):\n    if Path(fp).suffix in ['.tif', '.tiff']:\n        img = tifffile.imread(fp)\n    else:\n        img = np.array(Image.open(fp))\n    return img\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_ToTensor(fp):\n    img = read_img(fp)\n    result = transforms_cls.Compose([\n        transforms_cls.ToTensor()\n    ])(img)\n    assert type(result) == torch.Tensor\n    assert len(result.shape) == 3\n    assert result.shape[1:3] == img.shape[0:2]\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_Normalize(fp):\n    img = read_img(fp)\n    channels = 1 if img.ndim==2 else img.shape[2]\n    mean = [img.mean()] if channels==1 else np.array(img.mean(axis=(0, 1))).tolist()\n    std = [img.std()] if channels==1 else np.array(img.std(axis=(0, 1))).tolist()\n\n    result = transforms_cls.Compose([\n        transforms_cls.ToTensor(),\n        transforms_cls.Normalize(mean, std)\n    ])(img)\n    assert type(result) == torch.Tensor\n    assert len(result.shape) == 3\n    assert result.shape[1:3] == img.shape[0:2]\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_ToGray(fp):\n    img = read_img(fp)\n    result = transforms_cls.Compose([\n        transforms_cls.ToGray()\n    ])(img)\n    assert result.dtype == img.dtype\n    assert result.ndim == 2\n\n    result = transforms_cls.Compose([\n        transforms_cls.ToGray(output_channels=5)\n    ])(img)\n    assert result.shape == (img.shape[0], img.shape[1], 5)\n    assert result.dtype == img.dtype\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_GaussianBlur(fp):\n    img = read_img(fp)\n    result = transforms_cls.Compose([\n        transforms_cls.GaussianBlur(kernel_size=5)\n    ])(img)\n    assert result.shape == img.shape\n    assert result.dtype == img.dtype\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomNoise(fp):\n    img = read_img(fp)\n    for item in ['gaussian', 'salt', 'pepper', 's&p']:\n        result = transforms_cls.Compose([\n            transforms_cls.RandomNoise(mode=item)\n        ])(img)\n        assert result.shape == img.shape\n        assert result.dtype == img.dtype\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomBrightness(fp):\n    img = read_img(fp)\n    result = transforms_cls.Compose([\n        transforms_cls.RandomBrightness()\n    ])(img)\n    assert result.shape == img.shape\n    assert result.dtype == img.dtype\n\n    result = transforms_cls.Compose([\n        transforms_cls.RandomBrightness(max_value=10)\n    ])(img)\n    assert result.shape == img.shape\n    assert result.dtype == img.dtype\n    if result.ndim == 2:\n        assert abs(float(result[0,0]) - float(img[0,0])) <=10\n    else:\n        assert abs(float(result[0,0,0]) - float(img[0,0,0])) <=10\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomContrast(fp):\n    img = read_img(fp)\n    result = transforms_cls.Compose([\n        transforms_cls.RandomContrast()\n    ])(img)\n    assert result.shape == img.shape\n    assert result.dtype == img.dtype\n\n    result = transforms_cls.Compose([\n        transforms_cls.RandomContrast(max_factor=1.2)\n    ])(img)\n    assert result.shape == img.shape\n    assert result.dtype == img.dtype\n    if result.ndim == 2:\n        assert abs(float(result[0,0]) / float(img[0,0])) <=1.2\n    else:\n        assert abs(float(result[0,0,0]) / float(img[0,0,0])) <=1.2\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_Resize(fp):\n    img = read_img(fp)\n    result = transforms_cls.Compose([\n        transforms_cls.Resize(300),\n        transforms_cls.ToTensor(),\n    ])(img)\n    assert result.shape[1:3] == torch.Size([300, 300])\n    assert type(result) == torch.Tensor\n\n    result = transforms_cls.Compose([\n        transforms_cls.Resize(833),\n    ])(img)\n    assert result.shape[0:2] == (833, 833)\n    assert result.dtype == img.dtype\n\n    result = transforms_cls.Compose([\n        transforms_cls.Resize((500,300)),\n    ])(img)\n    assert result.shape[0:2] == (500, 300)\n    assert result.dtype == img.dtype\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_CenterCrop(fp):\n    img = read_img(fp)\n    result = transforms_cls.Compose([\n        transforms_cls.CenterCrop(300),\n    ])(img)\n    assert result.shape[0:2] == (300,300)\n    assert result.dtype == img.dtype\n\n    result = transforms_cls.Compose([\n        transforms_cls.CenterCrop((500,300)),\n    ])(img)\n    assert result.shape[0:2] == (500,300)\n    assert result.dtype == img.dtype\n\n    with pytest.raises(ValueError) as excinfo:\n        transforms_cls.CenterCrop(1000)(img)\n        assert 'the output_size should' in str(excinfo.value)\n\n\n@pytest.mark.parametrize('fp',  tiff_files+jpeg_files)\ndef test_Pad(fp):\n    img = read_img(fp)\n    # constant value\n    result = transforms_cls.Pad(10, fill=1)(img)\n    if result.ndim == 2:\n        assert result[0,0] == 1\n    else:\n        assert result[0,0,0] == 1\n    \n    # reflect value\n    result = transforms_cls.Pad(20, padding_mode='reflect')(img)\n    assert result.shape[0:2] == (img.shape[0]+40, img.shape[1]+40)\n    if result.ndim == 2:\n        assert result[0,0] == img[20,20]\n    else:\n        assert result[0,0,0] == img[20,20,0]\n    assert result.dtype == img.dtype\n    \n    # all padding mode methods\n    for item in ['reflect','edge','linear_ramp','maximum','mean' , 'median', 'minimum','symmetric','wrap']:\n    # for item in ['edge']:\n        result = transforms_cls.Pad(10, padding_mode=item)(img)\n        assert result.dtype == img.dtype\n        assert result.shape[0:2] == (img.shape[0]+20, img.shape[1]+20)\n\n        result = transforms_cls.Pad((10,20), padding_mode=item)(img)\n        assert result.shape[0:2] == (img.shape[0]+40, img.shape[1]+20)\n        assert result.dtype == img.dtype\n\n        result = transforms_cls.Pad((10,20,30,40), padding_mode=item)(img)\n        assert result.shape[0:2] == (img.shape[0]+60, img.shape[1]+40)\n        assert result.dtype == img.dtype\n\n    result = transforms_cls.Compose([\n        transforms_cls.Pad(10, fill=1),\n        transforms_cls.ToTensor()\n    ])(img)\n    assert type(result) == torch.Tensor\n\n\n@pytest.mark.parametrize('fp',  tiff_files+jpeg_files)\ndef test_RandomCrop(fp):\n    img = read_img(fp)\n    result = transforms_cls.RandomCrop(111)(img)\n    assert result.dtype == img.dtype\n    assert result.shape[0:2] == (111,111)\n\n    result = transforms_cls.RandomCrop((100, 200))(img)\n    assert result.dtype == img.dtype\n    assert result.shape[0:2] == (100,200)\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomHorizontalFlip(fp):\n    img = read_img(fp)\n    result = transforms_cls.RandomHorizontalFlip(p=1)(img)\n    assert result.dtype == img.dtype\n    assert result.shape[0:2] == img.shape[0:2]\n    if result.ndim == 2:\n        height, width = img.shape\n        assert result[0,width-1] == img[0,0]\n    else:\n        height, width, depth = img.shape\n        assert (result[0,width-1,:] == img[0,0,:]).any() == True\n\n    # tensor\n    result = transforms_cls.Compose([\n        transforms_cls.RandomHorizontalFlip(p=1),\n        transforms_cls.ToTensor()\n    ])(img)\n    assert type(result) == torch.Tensor\n    assert result.shape[1:3] == img.shape[0:2]\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomVerticalFlip(fp):\n    img = read_img(fp)\n    result = transforms_cls.RandomVerticalFlip(p=1)(img)\n    assert result.dtype == img.dtype\n    assert result.shape[0:2] == img.shape[0:2]\n    if result.ndim == 2:\n        height, width = img.shape\n        assert result[height-1,0] == img[0,0]\n    else:\n        height, width, depth = img.shape\n        assert (result[height-1,0,:] == img[0,0,:]).any() == True\n\n    # tensor\n    result = transforms_cls.Compose([\n        transforms_cls.RandomVerticalFlip(p=1),\n        transforms_cls.ToTensor()\n    ])(img)\n    assert type(result) == torch.Tensor\n    assert result.shape[1:3] == img.shape[0:2]\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_Flip(fp):\n    img = read_img(fp)\n    result = transforms_cls.RandomFlip(p=0)(img)\n    assert result.dtype == img.dtype\n    assert result.shape[0:2] == img.shape[0:2]\n    if result.ndim == 2:\n        height, width = img.shape\n        assert result[0,0] == img[0,0]\n    else:\n        height, width, depth = img.shape\n        assert (result[0,0,:] == img[0,0,:]).any() == True\n\n    # tensor\n    result = transforms_cls.Compose([\n        transforms_cls.RandomFlip(p=0.1),\n        transforms_cls.ToTensor()\n    ])(img)\n    assert type(result) == torch.Tensor\n    assert result.shape[1:3] == img.shape[0:2]\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomResizedCrop(fp):\n    img = read_img(fp)\n    result = transforms_cls.RandomResizedCrop((500,300), 300)(img)\n    assert result.dtype == img.dtype\n    assert result.shape[0:2] == (300,300)\n\n    result = transforms_cls.RandomResizedCrop(500, (500,300))(img)\n    assert result.shape[0:2] == (500,300)\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_ElasticTransform(fp):\n    img = read_img(fp)\n    result = transforms_cls.ElasticTransform()(img)\n    assert result.dtype == img.dtype\n    assert result.shape[0:2] == img.shape[0:2]\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomRotation(fp):\n    img = read_img(fp)\n    result = transforms_cls.RandomRotation(45)(img)\n    assert result.dtype == img.dtype\n    assert result.shape[0:2] == img.shape[0:2]\n\n    result = transforms_cls.RandomRotation((-10, 30))(img)\n    assert result.dtype == img.dtype\n    assert result.shape[0:2] == img.shape[0:2]\n\n    result = transforms_cls.RandomRotation((-10, 30), center=(200,250))(img)\n    assert result.dtype == img.dtype\n    assert result.shape[0:2] == img.shape[0:2]\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomShift(fp):\n    img = read_img(fp)\n    result = transforms_cls.RandomShift(max_percent=0.1)(img)\n    assert result.dtype == img.dtype\n    assert result.shape[0:2] == img.shape[0:2]"""
tests/test_transform_det.py,4,"b""from pathlib import Path\nimport math\n\nimport numpy as np\nimport pytest\nimport tifffile\nimport torch\nfrom PIL import Image\n\nfrom torchsat.transforms import transforms_det\n\ntiff_files = [\n    './tests/fixtures/different-types/tiff_1channel_float.tif',\n    './tests/fixtures/different-types/tiff_1channel_uint16.tif',\n    './tests/fixtures/different-types/tiff_1channel_uint8.tif',\n    './tests/fixtures/different-types/tiff_3channel_float.tif',\n    './tests/fixtures/different-types/tiff_3channel_uint16.tif',\n    './tests/fixtures/different-types/tiff_3channel_uint8.tif',\n    './tests/fixtures/different-types/tiff_8channel_float.tif',\n    './tests/fixtures/different-types/tiff_8channel_uint16.tif',\n    './tests/fixtures/different-types/tiff_8channel_uint8.tif',\n]\n\njpeg_files = [\n    './tests/fixtures/different-types/jpeg_1channel_uint8.jpeg',\n    './tests/fixtures/different-types/jpeg_3channel_uint8.jpeg',\n    './tests/fixtures/different-types/jpeg_1channel_uint8.png',\n    './tests/fixtures/different-types/jpeg_3channel_uint8.png',\n]\n\nbboxes = np.array([\n    [0, 2, 417, 467],\n    [7, 39, 63, 94],\n    [362, 24, 422, 53],\n    [376, 36, 422, 81],\n    [373, 68, 422, 108],\n    [376, 98, 422, 210]\n], dtype=np.float)\n\nlabels = np.array([2,2,1,1,3,2], dtype=np.int64)\n\ndef read_img(fp):\n    if Path(fp).suffix in ['.tif', '.tiff']:\n        img = tifffile.imread(fp)\n    else:\n        img = np.array(Image.open(fp))\n    return img\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_ToTensor(fp):\n    img = read_img(fp)\n    out_img, out_bboxes, out_labels = transforms_det.Compose([\n        transforms_det.ToTensor()\n    ])(img, bboxes, labels)\n    assert type(out_bboxes) == torch.Tensor\n    assert out_bboxes.shape == bboxes.shape\n    assert type(out_labels) == torch.Tensor\n    assert out_labels.shape == labels.shape\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_Normalize(fp):\n    img = read_img(fp)\n    channels = 1 if img.ndim==2 else img.shape[2]\n    mean = [img.mean()] if channels==1 else np.array(img.mean(axis=(0, 1))).tolist()\n    std = [img.std()] if channels==1 else np.array(img.std(axis=(0, 1))).tolist()\n\n    out_img, out_bboxes, out_labels = transforms_det.Compose([\n        transforms_det.ToTensor(),\n        transforms_det.Normalize(mean, std)\n    ])(img, bboxes, labels)\n    assert type(out_bboxes) == torch.Tensor\n    assert out_bboxes.shape == bboxes.shape\n    assert type(out_labels) == torch.Tensor\n    assert out_labels.shape == labels.shape\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_ToGray(fp):\n    img = read_img(fp)\n    out_img, out_bboxes, out_labels = transforms_det.Compose([\n        transforms_det.ToGray()\n    ])(img, bboxes, labels)\n    assert out_bboxes.dtype == bboxes.dtype\n    assert out_bboxes.shape == bboxes.shape\n    assert out_labels.dtype == labels.dtype\n    assert out_labels.shape == labels.shape\n\n    out_img, out_bboxes, out_labels = transforms_det.Compose([\n        transforms_det.ToGray(output_channels=5)\n    ])(img, bboxes, labels)\n    assert out_bboxes.dtype == bboxes.dtype\n    assert out_bboxes.shape == bboxes.shape\n    assert out_labels.dtype == labels.dtype\n    assert out_labels.shape == labels.shape\n"""
tests/test_transform_seg.py,12,"b""from pathlib import Path\nimport math\n\nimport numpy as np\nimport pytest\nimport tifffile\nimport torch\nfrom PIL import Image\n\nfrom torchsat.transforms import transforms_seg\n\ntiff_files = [\n    './tests/fixtures/different-types/tiff_1channel_float.tif',\n    './tests/fixtures/different-types/tiff_1channel_uint16.tif',\n    './tests/fixtures/different-types/tiff_1channel_uint8.tif',\n    './tests/fixtures/different-types/tiff_3channel_float.tif',\n    './tests/fixtures/different-types/tiff_3channel_uint16.tif',\n    './tests/fixtures/different-types/tiff_3channel_uint8.tif',\n    './tests/fixtures/different-types/tiff_8channel_float.tif',\n    './tests/fixtures/different-types/tiff_8channel_uint16.tif',\n    './tests/fixtures/different-types/tiff_8channel_uint8.tif',\n]\n\njpeg_files = [\n    './tests/fixtures/different-types/jpeg_1channel_uint8.jpeg',\n    './tests/fixtures/different-types/jpeg_3channel_uint8.jpeg',\n    './tests/fixtures/different-types/jpeg_1channel_uint8.png',\n    './tests/fixtures/different-types/jpeg_3channel_uint8.png',\n]\n\nmask_file = './tests/fixtures/masks/mask_tiff_3channel_uint8.png'\n\n\ndef read_img(fp):\n    if Path(fp).suffix in ['.tif', '.tiff']:\n        img = tifffile.imread(fp)\n    else:\n        img = np.array(Image.open(fp))\n    return img\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_ToTensor(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    \n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.ToTensor()\n    ])(img, mask)\n    assert type(result_img) == torch.Tensor\n    assert len(result_img.shape) == 3\n    assert result_img.shape[1:3] == img.shape[0:2]\n\n    assert type(result_mask) == torch.Tensor\n    assert torch.all(torch.unique(result_mask) == torch.tensor([0,1,2,3])) == True\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_Normalize(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    channels = 1 if img.ndim==2 else img.shape[2]\n    mean = [img.mean()] if channels==1 else np.array(img.mean(axis=(0, 1))).tolist()\n    std = [img.std()] if channels==1 else np.array(img.std(axis=(0, 1))).tolist()\n\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.ToTensor(),\n        transforms_seg.Normalize(mean, std)\n    ])(img, mask)\n    assert type(result_img) == torch.Tensor\n    assert len(result_img.shape) == 3\n    assert result_img.shape[1:3] == img.shape[0:2]\n\n    assert type(result_mask) == torch.Tensor\n    assert torch.all(torch.unique(result_mask) == torch.tensor([0,1,2,3])) == True\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_ToGray(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.ToGray()\n    ])(img, mask)\n    assert result_img.dtype == img.dtype\n    assert result_img.ndim == 2\n\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.ToGray(output_channels=5)\n    ])(img, mask)\n    assert result_img.shape == (img.shape[0], img.shape[1], 5)\n    assert result_img.dtype == img.dtype\n\n    assert result_mask.dtype == mask.dtype\n    assert np.all(np.unique(result_mask) == np.array([0,1,2,3])) == True\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_GaussianBlur(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.GaussianBlur(kernel_size=5)\n    ])(img, mask)\n    assert result_img.shape == img.shape\n    assert result_img.dtype == img.dtype\n\n    assert result_mask.dtype == mask.dtype\n    assert np.all(np.unique(result_mask) == np.array([0,1,2,3])) == True\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomNoise(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    for item in ['gaussian', 'salt', 'pepper', 's&p']:\n        result_img, result_mask = transforms_seg.Compose([\n            transforms_seg.RandomNoise(mode=item)\n        ])(img, mask)\n        assert result_img.shape == img.shape\n        assert result_img.dtype == img.dtype\n\n        assert result_mask.dtype == mask.dtype\n        assert np.all(np.unique(result_mask) == np.array([0,1,2,3])) == True\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomBrightness(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.RandomBrightness()\n    ])(img, mask)\n    assert result_img.shape == img.shape\n    assert result_img.dtype == img.dtype\n    assert result_mask.dtype == mask.dtype\n    assert np.all(np.unique(result_mask) == np.array([0,1,2,3])) == True\n\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.RandomBrightness(max_value=10)\n    ])(img, mask)\n    assert result_img.shape == img.shape\n    assert result_img.dtype == img.dtype\n    if result_img.ndim == 2:\n        assert abs(float(result_img[0,0]) - float(img[0,0])) <=10\n    else:\n        assert abs(float(result_img[0,0,0]) - float(img[0,0,0])) <=10\n    assert result_mask.dtype == mask.dtype\n    assert np.all(np.unique(result_mask) == np.array([0,1,2,3])) == True\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomContrast(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.RandomContrast()\n    ])(img, mask)\n    assert result_img.shape == img.shape\n    assert result_img.dtype == img.dtype\n    assert result_mask.dtype == mask.dtype\n    assert np.all(np.unique(result_mask) == np.array([0,1,2,3])) == True\n\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.RandomContrast(max_factor=1.2)\n    ])(img, mask)\n    assert result_img.shape == img.shape\n    assert result_img.dtype == img.dtype\n    if result_img.ndim == 2:\n        assert abs(float(result_img[0,0]) / float(img[0,0])) <=1.2\n    else:\n        assert abs(float(result_img[0,0,0]) / float(img[0,0,0])) <=1.2\n    assert result_mask.dtype == mask.dtype\n    assert np.all(np.unique(result_mask) == np.array([0,1,2,3])) == True\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_Resize(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    assert mask.shape == (650,500)\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.Resize(300),\n        transforms_seg.ToTensor(),\n    ])(img, mask)\n    assert result_mask.shape == torch.Size([300, 300])\n    assert type(result_mask) == torch.Tensor\n    assert np.all(np.unique(result_mask) == np.array([0,1,2,3])) == True\n\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.Resize(833),\n    ])(img, mask)\n    assert result_mask.shape[0:2] == (833, 833)\n    assert result_mask.dtype == mask.dtype\n\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.Resize((500,300)),\n    ])(img, mask)\n    assert result_mask.shape[0:2] == (500, 300)\n    assert result_mask.dtype == mask.dtype\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_CenterCrop(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.CenterCrop(300),\n    ])(img, mask)\n    assert result_mask.shape[0:2] == (300,300)\n    assert result_mask.dtype == mask.dtype\n\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.CenterCrop((500,300)),\n    ])(img, mask)\n    assert result_mask.shape[0:2] == (500,300)\n    assert result_mask.dtype == mask.dtype\n\n    with pytest.raises(ValueError) as excinfo:\n        transforms_seg.CenterCrop(1000)(img, mask)\n        assert 'the output_size should' in str(excinfo.value)\n\n\n@pytest.mark.parametrize('fp',  tiff_files+jpeg_files)\ndef test_Pad(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    # constant value\n    result_img, result_mask = transforms_seg.Pad(10, fill=1)(img, mask)\n    if result_mask.ndim == 2:\n        assert result_mask[0,0] == 0\n    else:\n        assert result_mask[0,0,0] == 0\n    \n    # reflect value\n    result_img, result_mask = transforms_seg.Pad(20, padding_mode='reflect')(img, mask)\n    assert result_mask.shape[0:2] == (mask.shape[0]+40, mask.shape[1]+40)\n    assert result_mask[0,0] == mask[20,20]\n    assert result_mask.dtype == mask.dtype\n    \n    # all padding mode methods\n    for item in ['reflect','edge','linear_ramp','maximum', 'mean' , 'median', 'minimum', 'symmetric', 'wrap']:\n    # for item in ['edge']:\n        result_img, result_mask = transforms_seg.Pad(10, padding_mode=item)(img, mask)\n        assert result_mask.dtype == mask.dtype\n        assert result_mask.shape[0:2] == (mask.shape[0]+20, mask.shape[1]+20)\n\n        result_img, result_mask = transforms_seg.Pad((10,20), padding_mode=item)(img, mask)\n        assert result_mask.shape[0:2] == (mask.shape[0]+40, mask.shape[1]+20)\n        assert result_mask.dtype == mask.dtype\n\n        result_img, result_mask = transforms_seg.Pad((10,20,30,40), padding_mode=item)(img, mask)\n        assert result_mask.shape[0:2] == (mask.shape[0]+60, mask.shape[1]+40)\n        assert result_mask.dtype == mask.dtype\n\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.Pad(10, fill=1),\n        transforms_seg.ToTensor()\n    ])(img,mask)\n    assert type(result_mask) == torch.Tensor\n\n\n@pytest.mark.parametrize('fp',  tiff_files+jpeg_files)\ndef test_RandomCrop(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    result_img, result_mask = transforms_seg.RandomCrop(111)(img, mask)\n    assert result_mask.dtype == mask.dtype\n    assert result_mask.shape[0:2] == (111, 111)\n\n    result_img, result_mask = transforms_seg.RandomCrop((100, 200))(img, mask)\n    assert result_mask.dtype == mask.dtype\n    assert result_mask.shape[0:2] == (100,200)\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomHorizontalFlip(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    result_img, result_mask = transforms_seg.RandomHorizontalFlip(p=1)(img, mask)\n    assert result_mask.dtype == mask.dtype\n    assert result_mask.shape[0:2] == mask.shape[0:2]\n    if result_mask.ndim == 2:\n        height, width = mask.shape\n        assert result_mask[0,width-31] == mask[0,30]\n    else:\n        height, width, depth = mask.shape\n        assert (result_mask[0,width-31,:] == mask[0,30,:]).any() == True\n\n    # tensor\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.RandomHorizontalFlip(p=1),\n        transforms_seg.ToTensor()\n    ])(img, mask)\n    assert type(result_mask) == torch.Tensor\n    assert result_mask.shape[0:2] == mask.shape[0:2]\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomVerticalFlip(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    result_img, result_mask = transforms_seg.RandomVerticalFlip(p=1)(img, mask)\n    assert result_mask.dtype == mask.dtype\n    assert result_mask.shape[0:2] == mask.shape[0:2]\n    if result_mask.ndim == 2:\n        height, width = mask.shape\n        assert result_mask[height-1,30] == mask[0,30]\n    else:\n        height, width, depth = mask.shape\n        assert (result_mask[height-1,30,:] == mask[0,30,:]).any() == True\n\n    # tensor\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.RandomVerticalFlip(p=1),\n        transforms_seg.ToTensor()\n    ])(img, mask)\n    assert type(result_mask) == torch.Tensor\n    assert result_mask.shape[0:2] == mask.shape[0:2]\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomFlip(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    result_img, result_mask = transforms_seg.RandomFlip(p=0)(img, mask)\n    assert result_mask.dtype == mask.dtype\n    assert result_mask.shape[0:2] == mask.shape[0:2]\n    if result_mask.ndim == 2:\n        height, width = mask.shape\n        assert result_mask[0,0] == mask[0,0]\n    else:\n        height, width, depth = mask.shape\n        assert (result_mask[0,0,:] == mask[0,0,:]).any() == True\n\n    # tensor\n    result_img, result_mask = transforms_seg.Compose([\n        transforms_seg.RandomFlip(p=0.1),\n        transforms_seg.ToTensor()\n    ])(img, mask)\n    assert type(result_mask) == torch.Tensor\n    assert result_mask.shape[0:2] == mask.shape[0:2]\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomResizedCrop(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    result_img, result_mask = transforms_seg.RandomResizedCrop((500,300), 300)(img, mask)\n    assert result_mask.dtype == mask.dtype\n    assert result_mask.shape[0:2] == (300,300)\n\n    result_img, result_mask = transforms_seg.RandomResizedCrop(500, (500,300))(img, mask)\n    assert result_mask.shape[0:2] == (500,300)\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_ElasticTransform(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    result_img, result_mask = transforms_seg.ElasticTransform()(img, mask)\n    assert result_mask.dtype == mask.dtype\n    assert result_mask.shape[0:2] == mask.shape[0:2]\n    assert np.all(np.unique(result_mask) == np.array([0,1,2,3])) == True\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomRotation(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    result_img, result_mask = transforms_seg.RandomRotation(45)(img, mask)\n    assert result_mask.dtype == mask.dtype\n    assert result_mask.shape[0:2] == mask.shape[0:2]\n\n    result_img, result_mask = transforms_seg.RandomRotation((-10, 30))(img, mask)\n    assert result_mask.dtype == mask.dtype\n    assert result_mask.shape[0:2] == mask.shape[0:2]\n\n    result_img, result_mask = transforms_seg.RandomRotation((-10, 30), center=(200,250))(img, mask)\n    assert result_mask.dtype == mask.dtype\n    assert result_mask.shape[0:2] == mask.shape[0:2]\n    \n    assert np.all(np.unique(result_mask) == np.array([0,1,2,3])) == True\n\n\n\n@pytest.mark.parametrize('fp', tiff_files+jpeg_files)\ndef test_RandomShift(fp):\n    img = read_img(fp)\n    mask = read_img(mask_file)\n    result_img, result_mask = transforms_seg.RandomShift(max_percent=0.1)(img, mask)\n    assert result_mask.dtype == mask.dtype\n    assert result_mask.shape[0:2] == mask.shape[0:2]"""
torchsat/__init__.py,0,"b'__version__ = ""v0.1.0dev""\r\n\r\nfrom .models import *\r\n'"
docs/source/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../..\'))\n\n\n# -- Project information -----------------------------------------------------\n\nproject = \'TorchSat\'\ncopyright = \'2019, sshuair\'\nauthor = \'sshuair\'\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.mathjax\'\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\']\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\nmaster_doc = \'index\''"
tools/preprocessing/make_mask_cls.py,0,"b'""""""\n * @author sshuair\n * @email sshuair@gmail.com\n * @create date 2020-05-31 16:06:19\n * @modify date 2020-05-31 16:06:19\n * @desc this tool is to patch the large satellite image to small image\n""""""\n\nimport os\nfrom glob import glob\nfrom pathlib import Path\n\nimport click\nimport rasterio\nfrom rasterio.windows import Window\nfrom tqdm import tqdm\n\n\ndef patch_image(filepath, width, height, drop_last, outpath):\n    with rasterio.open(filepath, \'r\') as src:\n        rows = src.meta[\'height\'] // height if drop_last else src.meta[\'height\'] // height + 1\n        columns = src.meta[\'width\'] // width if drop_last else src.meta[\'width\'] // width + 1\n        for row in tqdm(range(rows)):\n            for col in range(columns):\n                outfile = os.path.join(outpath, Path(filepath).stem+\'_\'+str(row)+\'_\'+str(col)+Path(filepath).suffix)\n                window = Window(col * width, row * height, width, height)\n                patched_arr = src.read(window=window, boundless=True)\n                kwargs = src.meta.copy()\n                kwargs.update({\n                    \'height\': window.height,\n                    \'width\': window.width,\n                    \'transform\': rasterio.windows.transform(window, src.transform)})\n                with rasterio.open(outfile, \'w\', **kwargs) as dst:\n                    dst.write(patched_arr)\n\n\n@click.command(help=\'this tool is to patch the large satellite image to small image.\')\n@click.option(\'--filepath\', type=str, help=\'the target satellite image to split. Note: the file should have crs\')\n@click.option(\'--width\', default=256, type=int, help=\'the width of the patched image\')\n@click.option(\'--height\', default=256, type=int, help=\'the height of the patched image\')\n@click.option(\'--drop_last\', default=True, type=bool,\n              help=\'set to True to drop the last column and row, if the image size is not divisible by the height and width.\')\n@click.option(\'--outpath\', type=str, help=\'the output file path\')\ndef main(filepath: str, width: int, height: int, drop_last: bool, outpath: str):\n    if Path(filepath).is_file():\n        files = [filepath]\n    else:\n        files = glob(os.path.join(filepath, \'**/*.tif\'), recursive=True)\n    for idx, item in enumerate(files):\n        print(\'processing {}/{} file {} ...\'.format(idx + 1, len(files), item))\n        patch_image(item, width, height, drop_last, outpath)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
tools/preprocessing/make_mask_seg.py,0,"b'""""""\n * @author sshuair\n * @email sshuair@gmail.com\n * @create date 2020-05-31 16:06:19\n * @modify date 2020-05-31 21:15:30\n * @desc this tool is to patch the large satellite image to small image and label for segmentation.\n""""""\n\n\nimport os\nfrom glob import glob\nfrom pathlib import Path\n\nimport click\nimport rasterio\nfrom rasterio.windows import Window\nfrom rasterio.features import rasterize\nfrom tqdm import tqdm\nimport geopandas\nfrom shapely.geometry import Polygon\n\n\n@click.command(help=\'this tool is to patch the large satellite image to small image and label for segmentation.\')\n@click.option(\'--image_file\', type=str, help=\'the target satellite image to split. Note: the file should have crs\')\n@click.option(\'--label_file\', type=str, help=\'\'\'the corresponding label file of the satellite image. \n                vector or raster file. Note the crs should be same as satellite image.\'\'\')\n@click.option(\'--field\', type=str, help=\'field to burn\')\n@click.option(\'--width\', default=256, type=int, help=\'the width of the patched image\')\n@click.option(\'--height\', default=256, type=int, help=\'the height of the patched image\')\n@click.option(\'--drop_last\', default=True, type=bool,\n              help=\'set to True to drop the last column and row, if the image size is not divisible by the height and width.\')\n@click.option(\'--outpath\', type=str, help=\'the output file path\')\ndef main(image_file: str, label_file: str, field, width: int, height: int, drop_last: bool, outpath: str):\n    if not Path(image_file).is_file():\n        raise ValueError(\'file {} not exits.\'.format(image_file))\n    # TODO: Check the crs\n\n    # read the file and distinguish the label_file is raster or vector\n    try:\n        label_src = rasterio.open(label_file)\n        label_flag = \'raster\'\n    except rasterio.RasterioIOError:\n        label_df = geopandas.read_file(label_file)\n        # TODO: create spatial index to speed up the clip\n        label_flag = \'vector\'\n\n    img_src = rasterio.open(image_file)\n    rows = img_src.meta[\'height\'] // height if drop_last else img_src.meta[\'height\'] // height + 1\n    columns = img_src.meta[\'width\'] // width if drop_last else img_src.meta[\'width\'] // width + 1\n    for row in tqdm(range(rows)):\n        for col in range(columns):\n            # image\n            outfile_image = os.path.join(outpath, Path(image_file).stem+\'_\'+str(row)+\'_\'+str(col)+Path(image_file).suffix)\n            window = Window(col * width, row * height, width, height)\n            patched_arr = img_src.read(window=window, boundless=True)\n            kwargs = img_src.meta.copy()\n            patched_transform = rasterio.windows.transform(window, img_src.transform)\n            kwargs.update({\n                \'height\': window.height,\n                \'width\': window.width,\n                \'transform\': patched_transform})\n            with rasterio.open(outfile_image, \'w\', **kwargs) as dst:\n                dst.write(patched_arr)\n\n            # label\n            outfile_label = Path(outfile_image).with_suffix(\'.png\')\n            if label_flag == \'raster\':\n                label_arr = label_src.read(window=window, boundless=True)\n            else:\n                bounds = rasterio.windows.bounds(window, img_src.transform)\n                clipped_poly = geopandas.clip(label_df, Polygon.from_bounds(*bounds))\n                shapes = [(geom, value) for geom, value in zip(clipped_poly.geometry, clipped_poly[field])]\n                label_arr = rasterize(shapes, out_shape=(width, height), default_value=0, transform=patched_transform)   \n            \n            kwargs = img_src.meta.copy()\n            kwargs.update({\n                \'driver\': \'png\',\n                \'count\': 1,\n                \'height\': window.height,\n                \'width\': window.width,\n                \'transform\': patched_transform,\n                \'dtype\': \'uint8\'\n            })\n            with rasterio.open(outfile_label, \'w\', **kwargs) as dst:\n                dst.write(label_arr, 1)\n\n    img_src.close()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
torchsat/cli/__init__.py,0,b''
torchsat/cli/calcuate_mean_std.py,0,"b'import argparse\nimport os\nimport random\nfrom glob import glob\nfrom pathlib import Path\n\nimport click\nimport numpy as np\nimport tifffile\nfrom PIL import Image\nfrom tqdm import tqdm\n\n\ndef img_loader(fp):\n    if Path(fp).suffix.lower() in ["".jpg"", "".jpeg"", "".png""]:\n        arr = np.array(Image.open(fp))\n    else:\n        arr = tifffile.imread(fp)\n\n    return arr\n\n\n@click.command(help=\'calcuate the datasets mean and std value\')\n@click.option(\'--root\', type=click.Path(exists=True), required=True, help=\'root dir of image datasets\')\n@click.option(""--percent"", default=0.5, type=float, help=""percent of images to calcuate"")\n@click.option(""--channels"", default=3, type=int, help=""datasets image channels"")\n@click.option(""--max"", default=255, type=float, help=""max value of all images default: {255}"")\n@click.option(""--fmt"", default=[""jpg""], multiple=True, help=""file suffix to calcuate, default{jpg}, support suffix: jpg, jpeg, png, tif, tiff"")\ndef calcuate_mean_std(root, percent, channels, max, fmt):\n    print(root, percent, channels, max, fmt)\n    files = glob(os.path.join(root, ""**/*""), recursive=True)\n    files = [x for x in files if Path(x).suffix.replace(""."", """") in fmt]\n    random.shuffle(files)\n    files = files[0: int(len(files) * percent)]\n\n    if not files:\n        print(""INFO: No Image Found!"")\n        return\n\n    pixel_num = 0  # store all pixel number in the dataset\n    channel_sum = np.zeros(channels)\n    channel_sum_squared = np.zeros(channels)\n    for item in tqdm(files):\n        arr = img_loader(item)\n        arr = arr / max\n        pixel_num += arr.shape[0] * arr.shape[1]\n        channel_sum += np.sum(arr, axis=(0, 1))\n        channel_sum_squared += np.sum(np.square(arr), axis=(0, 1))\n\n    mean = channel_sum / pixel_num\n    std = np.sqrt(channel_sum_squared / pixel_num - np.square(mean))\n\n    print(""scaled  mean:{} \\nscaled  std: {} "".format(mean, std))\n    print(""orginal mean: {} \\norginal std: {}"".format(mean * max, std * max))\n'"
torchsat/cli/cli.py,0,b'import click\n\nfrom . import make_mask_cls\nfrom . import make_mask_seg\nfrom . import calcuate_mean_std\n\n\n@click.group()\ndef entry_point():\n    pass\n\n\nentry_point.add_command(make_mask_cls.make_mask_cls)\nentry_point.add_command(make_mask_seg.make_mask_seg)\nentry_point.add_command(calcuate_mean_std.calcuate_mean_std)\n'
torchsat/cli/make_mask_cls.py,0,"b'""""""\n * @author sshuair\n * @email sshuair@gmail.com\n * @create date 2020-05-31 16:06:19\n * @modify date 2020-05-31 16:06:19\n * @desc this tool is to patch the large satellite image to small image\n""""""\n\nimport os\nfrom glob import glob\nfrom pathlib import Path\n\nimport click\nimport rasterio\nfrom rasterio.windows import Window\nfrom tqdm import tqdm\n\n\ndef patch_image(filepath, width, height, drop_last, outpath):\n    with rasterio.open(filepath, \'r\') as src:\n        rows = src.meta[\'height\'] // height if drop_last else src.meta[\'height\'] // height + 1\n        columns = src.meta[\'width\'] // width if drop_last else src.meta[\'width\'] // width + 1\n        for row in tqdm(range(rows)):\n            for col in range(columns):\n                outfile = os.path.join(outpath, Path(filepath).stem+\'_\'+str(row)+\'_\'+str(col)+Path(filepath).suffix)\n                window = Window(col * width, row * height, width, height)\n                patched_arr = src.read(window=window, boundless=True)\n                kwargs = src.meta.copy()\n                kwargs.update({\n                    \'height\': window.height,\n                    \'width\': window.width,\n                    \'transform\': rasterio.windows.transform(window, src.transform)})\n                with rasterio.open(outfile, \'w\', **kwargs) as dst:\n                    dst.write(patched_arr)\n\n\n@click.command(help=\'this tool is to patch the large satellite image to small image.\')\n@click.option(\'--filepath\', type=str, help=\'the target satellite image to split. Note: the file should have crs\')\n@click.option(\'--width\', default=256, type=int, help=\'the width of the patched image\')\n@click.option(\'--height\', default=256, type=int, help=\'the height of the patched image\')\n@click.option(\'--drop_last\', default=True, type=bool,\n              help=\'set to True to drop the last column and row, if the image size is not divisible by the height and width.\')\n@click.option(\'--outpath\', type=str, help=\'the output file path\')\ndef make_mask_cls(filepath: str, width: int, height: int, drop_last: bool, outpath: str):\n    if Path(filepath).is_file():\n        files = [filepath]\n    else:\n        files = glob(os.path.join(filepath, \'**/*.tif\'), recursive=True)\n    for idx, item in enumerate(files):\n        print(\'processing {}/{} file {} ...\'.format(idx + 1, len(files), item))\n        patch_image(item, width, height, drop_last, outpath)\n'"
torchsat/cli/make_mask_seg.py,0,"b'""""""\n * @author sshuair\n * @email sshuair@gmail.com\n * @create date 2020-05-31 16:06:19\n * @modify date 2020-05-31 21:15:30\n * @desc this tool is to patch the large satellite image to small image and label for segmentation.\n""""""\n\n\nimport os\nfrom glob import glob\nfrom pathlib import Path\n\nimport click\nimport rasterio\nfrom rasterio.windows import Window\nfrom rasterio.features import rasterize\nfrom tqdm import tqdm\nimport geopandas\nfrom shapely.geometry import Polygon\n\n\n@click.command(help=\'this tool is to patch the large satellite image to small image and label for segmentation.\')\n@click.option(\'--image_file\', type=str, help=\'the target satellite image to split. Note: the file should have crs\')\n@click.option(\'--label_file\', type=str, help=\'\'\'the corresponding label file of the satellite image. \n                vector or raster file. Note the crs should be same as satellite image.\'\'\')\n@click.option(\'--field\', type=str, help=\'field to burn\')\n@click.option(\'--width\', default=256, type=int, help=\'the width of the patched image\')\n@click.option(\'--height\', default=256, type=int, help=\'the height of the patched image\')\n@click.option(\'--drop_last\', default=True, type=bool,\n              help=\'set to True to drop the last column and row, if the image size is not divisible by the height and width.\')\n@click.option(\'--outpath\', type=str, help=\'the output file path\')\ndef make_mask_seg(image_file: str, label_file: str, field, width: int, height: int, drop_last: bool, outpath: str):\n    if not Path(image_file).is_file():\n        raise ValueError(\'file {} not exits.\'.format(image_file))\n    # TODO: Check the crs\n\n    # read the file and distinguish the label_file is raster or vector\n    try:\n        label_src = rasterio.open(label_file)\n        label_flag = \'raster\'\n    except rasterio.RasterioIOError:\n        label_df = geopandas.read_file(label_file)\n        # TODO: create spatial index to speed up the clip\n        label_flag = \'vector\'\n\n    img_src = rasterio.open(image_file)\n    rows = img_src.meta[\'height\'] // height if drop_last else img_src.meta[\'height\'] // height + 1\n    columns = img_src.meta[\'width\'] // width if drop_last else img_src.meta[\'width\'] // width + 1\n    for row in tqdm(range(rows)):\n        for col in range(columns):\n            # image\n            outfile_image = os.path.join(outpath, Path(image_file).stem+\'_\'+str(row)+\'_\'+str(col)+Path(image_file).suffix)\n            window = Window(col * width, row * height, width, height)\n            patched_arr = img_src.read(window=window, boundless=True)\n            kwargs = img_src.meta.copy()\n            patched_transform = rasterio.windows.transform(window, img_src.transform)\n            kwargs.update({\n                \'height\': window.height,\n                \'width\': window.width,\n                \'transform\': patched_transform})\n            with rasterio.open(outfile_image, \'w\', **kwargs) as dst:\n                dst.write(patched_arr)\n\n            # label\n            outfile_label = Path(outfile_image).with_suffix(\'.png\')\n            if label_flag == \'raster\':\n                label_arr = label_src.read(window=window, boundless=True)\n            else:\n                bounds = rasterio.windows.bounds(window, img_src.transform)\n                clipped_poly = geopandas.clip(label_df, Polygon.from_bounds(*bounds))\n                shapes = [(geom, value) for geom, value in zip(clipped_poly.geometry, clipped_poly[field])]\n                label_arr = rasterize(shapes, out_shape=(width, height), default_value=0, transform=patched_transform)   \n            \n            kwargs = img_src.meta.copy()\n            kwargs.update({\n                \'driver\': \'png\',\n                \'count\': 1,\n                \'height\': window.height,\n                \'width\': window.width,\n                \'transform\': patched_transform,\n                \'dtype\': \'uint8\'\n            })\n            with rasterio.open(outfile_label, \'w\', **kwargs) as dst:\n                dst.write(label_arr, 1)\n\n    img_src.close()\n'"
torchsat/datasets/__init__.py,0,b'from .eurosat import EuroSAT\nfrom .sat import SAT\nfrom .nwpu_resisc45 import NWPU_RESISC45\nfrom .patternnet import PatternNet\n'
torchsat/datasets/eurosat.py,0,"b""import os\n\nfrom .folder import DatasetFolder\nfrom .utils import pil_loader, tifffile_loader\n\nCLASSES_TO_IDX = {\n        'AnnualCrop': 0,\n        'Forest': 1,\n        'HerbaceousVegetation': 2,\n        'Highway': 3,\n        'Industrial': 4,\n        'Pasture': 5,\n        'PermanentCrop': 6,\n        'Residential': 7,\n        'River': 8,\n        'SeaLake': 9,\n    }\n\n\nclass EuroSAT(DatasetFolder):\n    url_rgb = 'http://madm.dfki.de/files/sentinel/EuroSAT.zip'\n    url_allband = 'http://madm.dfki.de/files/sentinel/EuroSATallBands.zip'\n\n    def __init__(self, root, mode='RGB', download=False, **kwargs):\n        if mode not in ['RGB', 'AllBand']:\n            raise ValueError('{} is not suppport mode, replace with RGB or AllBand'.format(self.mode))\n\n        if mode == 'RGB':\n            self.loader = pil_loader\n            self.extensions = ['.jpg', '.jpeg']\n        else:\n            self.loader = tifffile_loader\n            self.extensions = ['.tif', '.tiff']\n            root = os.path.join(root, 'images/remote_sensing/otherDatasets/sentinel_2/tif')\n\n        classes = list(CLASSES_TO_IDX.keys())\n\n        super(EuroSAT, self).__init__(root, self.loader, self.extensions, \n        classes=classes, class_to_idx=CLASSES_TO_IDX, **kwargs\n        )\n\n    def download():\n        pass\n"""
torchsat/datasets/folder.py,1,"b'# original source code from https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\nimport os\nimport os.path\nimport sys\n\nimport numpy as np\nimport torch.utils.data as data\nfrom PIL import Image\n\nfrom .utils import default_loader\n\nIMG_EXTENSIONS = [\'.jpg\', \'.jpeg\', \'.png\', \'.ppm\', \'.bmp\', \'.pgm\', \'.tif\', \'.tiff\', \'webp\']\n\ndef has_file_allowed_extension(filename, extensions):\n    """"""Checks if a file is an allowed extension.\n\n    Args:\n        filename (string): path to a file\n        extensions (iterable of strings): extensions to consider (lowercase)\n\n    Returns:\n        bool: True if the filename ends with one of given extensions\n    """"""\n    filename_lower = filename.lower()\n    return any(filename_lower.endswith(ext) for ext in extensions)\n\n\ndef is_image_file(filename):\n    """"""Checks if a file is an allowed image extension.\n\n    Args:\n        filename (string): path to a file\n\n    Returns:\n        bool: True if the filename ends with a known image extension\n    """"""\n    return has_file_allowed_extension(filename, IMG_EXTENSIONS)\n\n\ndef make_dataset(dir, class_to_idx, extensions):\n    images = []\n    dir = os.path.expanduser(dir)\n    for target in sorted(class_to_idx.keys()):\n        d = os.path.join(dir, target)\n        if not os.path.isdir(d):\n            continue\n\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                if has_file_allowed_extension(fname, extensions):\n                    path = os.path.join(root, fname)\n                    item = (path, class_to_idx[target])\n                    images.append(item)\n\n    return images\n\n\nclass DatasetFolder(data.Dataset):\n    """"""A generic data loader where the samples are arranged in this way: ::\n\n        root/class_x/xxx.ext\n        root/class_x/xxy.ext\n        root/class_x/xxz.ext\n\n        root/class_y/123.ext\n        root/class_y/nsdf3.ext\n        root/class_y/asd932_.ext\n\n    Args:\n        root (string): Root directory path.\n        loader (callable): A function to load a sample given its path.\n        extensions (list[string]): A list of allowed extensions.\n        classes (callable, optional): List of the class names.\n        class_to_idx (callable, optional): Dict with items (class_name, class_index).\n        transform (callable, optional): A function/transform that takes in\n            a sample and returns a transformed version.\n            E.g, ``transforms.RandomCrop`` for images.\n        target_transform (callable, optional): A function/transform that takes\n            in the target and transforms it.\n\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        samples (list): List of (sample path, class_index) tuples\n        targets (list): The class_index value for each image in the dataset\n    """"""\n\n    def __init__(self, root, loader, extensions, classes=None, class_to_idx=None, transform=None, target_transform=None):\n        if not class_to_idx:\n            classes, class_to_idx = self._find_classes(root)\n        samples = make_dataset(root, class_to_idx, extensions)\n        if len(samples) == 0:\n            raise(RuntimeError(""Found 0 files in subfolders of: "" + root + ""\\n""\n                               ""Supported extensions are: "" + "","".join(extensions)))\n\n        self.root = root\n        self.loader = loader\n        self.extensions = extensions\n\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.samples = samples\n        self.targets = [s[1] for s in samples]\n\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def _find_classes(self, dir):\n        """"""\n        Finds the class folders in a dataset.\n\n        Args:\n            dir (string): Root directory path.\n\n        Returns:\n            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n\n        Ensures:\n            No class is a subdirectory of another.\n        """"""\n        if sys.version_info >= (3, 5):\n            # Faster and available in Python 3.5 and above\n            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n        else:\n            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n        classes.sort()\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n        return classes, class_to_idx\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target class.\n        """"""\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __repr__(self):\n        fmt_str = \'Dataset \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Number of datapoints: {}\\n\'.format(self.__len__())\n        fmt_str += \'    Root Location: {}\\n\'.format(self.root)\n        tmp = \'    Transforms (if any): \'\n        fmt_str += \'{0}{1}\\n\'.format(tmp, self.transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        tmp = \'    Target Transforms (if any): \'\n        fmt_str += \'{0}{1}\'.format(tmp, self.target_transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        return fmt_str\n\n\nclass ImageFolder(DatasetFolder):\n    """"""A generic data loader where the images are arranged in this way: ::\n\n        root/dog/xxx.png\n        root/dog/xxy.png\n        root/dog/xxz.png\n\n        root/cat/123.png\n        root/cat/nsdf3.png\n        root/cat/asd932_.png\n\n    Args:\n        root (string): Root directory path.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        loader (callable, optional): A function to load an image given its path.\n\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        imgs (list): List of (image path, class_index) tuples\n    """"""\n    def __init__(self, root, transform=None, target_transform=None,\n                 loader=default_loader, **kwargs):\n        super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n                                          transform=transform,\n                                          target_transform=target_transform, **kwargs)\n        self.imgs = self.samples\n'"
torchsat/datasets/nwpu_resisc45.py,0,"b""from .folder import DatasetFolder\nfrom .utils import default_loader\n\nCLASSES_TO_IDX = {\n    'airplane': 0,\n    'airport': 1,\n    'baseball_diamond': 2,\n    'basketball_court': 3,\n    'beach': 4,\n    'bridge': 5,\n    'chaparral': 6,\n    'church': 7,\n    'circular_farmland': 8,\n    'cloud': 9,\n    'commercial_area': 10,\n    'dense_residential': 11,\n    'desert': 12,\n    'forest': 13,\n    'freeway': 14,\n    'golf_course': 15,\n    'ground_track_field': 16,\n    'harbor': 17,\n    'industrial_area': 18,\n    'intersection': 19,\n    'island': 20,\n    'lake': 21,\n    'meadow': 22,\n    'medium_residential': 23,\n    'mobile_home_park': 24,\n    'mountain': 25,\n    'overpass': 26,\n    'palace': 27,\n    'parking_lot': 28,\n    'railway': 29,\n    'railway_station': 30,\n    'rectangular_farmland': 31,\n    'river': 32,\n    'roundabout': 33,\n    'runway': 34,\n    'sea_ice': 35,\n    'ship': 36,\n    'snowberg': 37,\n    'sparse_residential': 38,\n    'stadium': 39,\n    'storage_tank': 40,\n    'tennis_court': 41,\n    'terrace': 42,\n    'thermal_power_station': 43,\n    'wetland': 44,\n}\n\n\n\nclass NWPU_RESISC45(DatasetFolder):\n\n    url = 'https://sov8mq.dm.files.1drv.com/y4m_Fo6ujI52LiWHDzaRZVtkMIZxF7aqjX2q7KdVr329zVEurIO-wUjnqOAKHvHUAaoqCI0cjYlrlM7WCKVOLfjmUZz6KvN4FmV93qsaNIB9C8VN2AHp3JXOK-l1Dvqst8HzsSeOs-_5DOYMYspalpc1rt_TNAFtUQPsKylMWcdUMQ_n6SHRGRFPwJmSoJUOrOk2oXe9D7CPEq5cq9S9LI8hA/NWPU-RESISC45.rar?download&psid=1'\n\n    def __init__(self, root, download=False, **kwargs):\n        if download:\n            download()\n        \n        extensions = ['.jpg', '.jpeg']\n        classes = list(CLASSES_TO_IDX.keys())\n\n        super(NWPU_RESISC45, self).__init__(root, default_loader, extensions,\n        classes=classes, class_to_idx=CLASSES_TO_IDX , **kwargs)\n        \n\n    def download():\n        pass\n"""
torchsat/datasets/patternnet.py,0,"b""from .folder import DatasetFolder\nfrom .utils import default_loader\n\nCLASSES_TO_IDX = {\n    'airplane': 0,\n    'baseball_field': 1,\n    'basketball_court': 2,\n    'beach': 3,\n    'bridge': 4,\n    'cemetery': 5,\n    'chaparral': 6,\n    'christmas_tree_farm': 7,\n    'closed_road': 8,\n    'coastal_mansion': 9,\n    'crosswalk': 10,\n    'dense_residential': 11,\n    'ferry_terminal': 12,\n    'football_field': 13,\n    'forest': 14,\n    'freeway': 15,\n    'golf_course': 16,\n    'harbor': 17,\n    'intersection': 18,\n    'mobile_home_park': 19,\n    'nursing_home': 20,\n    'oil_gas_field': 21,\n    'oil_well': 22,\n    'overpass': 23,\n    'parking_lot': 24,\n    'parking_space': 25,\n    'railway': 26,\n    'river': 27,\n    'runway': 28,\n    'runway_marking': 29,\n    'shipping_yard': 30,\n    'solar_panel': 31,\n    'sparse_residential': 32,\n    'storage_tank': 33,\n    'swimming_pool': 34,\n    'tennis_court': 35,\n    'transformer_station': 36,\n    'wastewater_treatment_plant': 37,\n}\n\nclass PatternNet(DatasetFolder):\n    url = 'https://doc-0k-9c-docs.googleusercontent.com/docs/securesc/s4mst7k8sdlkn5gslv2v17dousor99pe/5kjb9nqbn6uv3dnpsqu7n7vbc2sjkm9n/1553925600000/13306064760021495251/10775530989497868365/127lxXYqzO6Bd0yZhvEbgIfz95HaEnr9K?e=download'\n    \n    def __init__(self, root, download=False, **kwargs):\n        if download:\n            download()\n        \n        extensions = ['.jpg']\n        classes = list(CLASSES_TO_IDX.keys())\n\n        super(PatternNet, self).__init__(root, default_loader, extensions,\n        classes=classes ,class_to_idx=CLASSES_TO_IDX, **kwargs)\n        \n\n    def download():\n        pass"""
torchsat/datasets/sat.py,1,"b'import os\n\nimport numpy as np\nimport torch.utils.data as data\n\nfrom scipy.io import loadmat\n\n\nclass SAT(data.Dataset):\n    """"""SAT-4 and SAT-6 datasets\n    \n    Arguments:\n        data {root} -- [description]\n    \n    Raises:\n        ValueError -- [description]\n        ValueError -- [description]\n    \n    Returns:\n        [type] -- [description]\n    """"""\n\n    classes_sat4 = {""barren land"": 0, ""trees"": 1, ""grassland"": 2, ""none"": 3}\n    classes_sat6 = {\n        ""building"": 0,\n        ""barren land"": 1,\n        ""trees"": 2,\n        ""grassland"": 3,\n        ""road"": 4,\n        ""water"": 5,\n    }\n\n    def __init__(\n        self,\n        root,\n        mode=""SAT-4"",\n        image_set=""train"",\n        download=False,\n        transform=False,\n        target_transform=False,\n    ):\n        if mode not in (""SAT-4"", ""SAT-6""):\n            raise ValueError(""Argument mode should be \'SAT-4\' or \'SAT-6\'"")\n\n        if image_set not in (""train"", ""test""):\n            raise ValueError(""Argument image_set should be \'train\' or \'test\'"")\n\n        if download:\n            download()\n\n        self.root = os.path.expanduser(root)\n        self.image_set = image_set\n        self.mode = mode\n        self.transform = transform\n        self.target_transform = target_transform\n\n        _mat_path = os.path.join(self.root, self.mode.lower() + ""-full.mat"")\n        self._mat = loadmat(_mat_path)\n\n        self.images, self.targets = self._load_data(self._mat, self.image_set)\n\n    def __getitem__(self, index):\n        image = self.images[:, :, :, index]\n        target = np.argmax(self.targets[:, index], axis=0)\n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            target = self.target_transform(target)\n\n        return image, target\n\n    def __len__(self):\n        return len(self.images)\n\n    def _load_data(self, mat, image_set):\n        if image_set == ""train"":\n            images = mat[""train_x""]\n            targets = mat[""train_y""]\n        else:\n            images = mat[""test_x""]\n            targets = mat[""test_y""]\n        return images, targets\n\n    def download(self):\n        pass\n'"
torchsat/datasets/seg.py,0,"b'import os\nfrom .utils import image_loader\n\nclass SegDataset(object):\n    """"""SegDataset  This class is for common semantic segmentation scenario. \n    Data organization adopted from PASCAL VOC datasets. http://host.robots.ox.ac.uk/pascal/VOC/index.html\n\n    The trainval.txt, train.txt or val.txt should organize by:\n    :: \n      \n        relative_path/train_image/a0001.jpg\\trelative_path/mask_image/a0001.png\n        relative_path/train_image/a0002.jpg\\trelative_path/mask_image/a0002.png\n        relative_path/train_image/a0003.jpg\\trelative_path/mask_image/a0003.png\n\n    the mask image should be gray sacle, value from 0~255, each value represent one class.\n\n    Args:\n        root (str): root dir of the datasets\n    \n    """"""\n    def __init__(self, root, image_set=\'train.txt\', transform=None):\n        self.root = root\n        self.image_set = image_set\n        self.transform = transform\n\n        with open(os.path.join(root, image_set)) as f:\n            image_files = [x for x in f.readlines()]\n\n        self.images = [os.path.join(root, x.split(\'\\t\')[0].strip()) for x in image_files]\n        self.masks = [os.path.join(root, x.split(\'\\t\')[1].strip()) for x in image_files]\n\n    def __getitem__(self, index):\n        img = image_loader(self.images[index])\n        target = image_loader(self.masks[index])\n        if self.transform is not None:\n            img, target = self.transform(img, target)\n        return img, target\n\n    def __len__(self):\n        return len(self.images)\n'"
torchsat/datasets/utils.py,1,"b'import os\nimport six\nfrom PIL import Image\nimport numpy as np\nfrom torch.utils.model_zoo import tqdm\n\n\ndef gen_bar_updater():\n    pbar = tqdm(total=None)\n\n    def bar_update(count, block_size, total_size):\n        if pbar.total is None and total_size:\n            pbar.total = total_size\n        progress_bytes = count * block_size\n        pbar.update(progress_bytes - pbar.n)\n\n    return bar_update\n\n\ndef download_url(url, root, filename=None, md5=None):\n    """"""Download a file from a url and place it in root.\n    Args:\n        url (str): URL to download file from\n        root (str): Directory to place downloaded file in\n        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n        md5 (str, optional): MD5 checksum of the download. If None, do not check\n    """"""\n    from six.moves import urllib\n\n    root = os.path.expanduser(root)\n    if not filename:\n        filename = os.path.basename(url)\n    fpath = os.path.join(root, filename)\n\n    os.makedirs(root, exist_ok=True)\n\n    # downloads file\n    if os.path.isfile(fpath):\n        print(\'Using downloaded and verified file: \' + fpath)\n    else:\n        try:\n            print(\'Downloading \' + url + \' to \' + fpath)\n            urllib.request.urlretrieve(\n                url, fpath,\n                reporthook=gen_bar_updater()\n            )\n        except OSError:\n            if url[:5] == \'https\':\n                url = url.replace(\'https:\', \'http:\')\n                print(\'Failed download. Trying https -> http instead.\'\n                      \' Downloading \' + url + \' to \' + fpath)\n                urllib.request.urlretrieve(\n                    url, fpath,\n                    reporthook=gen_bar_updater()\n                )\n\n\ndef tifffile_loader(path):\n    # all the loader should be numpy ndarray [height, width, channels]\n    # int16: (-32768 to 32767)\n    import tifffile\n    img = tifffile.imread(path)\n    if img.dtype in [np.uint8, np.uint16, np.float]:\n        return img\n    else:\n        raise TypeError(\'tiff file only support np.uint8, np.uint16, np.float, but got {}\'.format(img.dtype))\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    # all the loader should be numpy ndarray [height, width, channels]\n    with open(path, \'rb\') as f:\n        img = Image.open(f).convert(\'RGB\')\n        return np.array(img)\n\n\ndef image_loader(path):\n    if os.path.splitext(path)[1].lower() in [\'.tif\', \'.tiff\']:\n        return tifffile_loader(path)\n    else:\n        return pil_loader(path)\n\n\ndef accimage_loader(path):\n    import accimage\n    try:\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef default_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n'"
torchsat/models/__init__.py,0,b'from .classification import *\r\nfrom .segmentation import *\r\n# from .detection import *\r\n'
torchsat/models/utils.py,0,"b'\nfrom .classification.vgg import vgg11, vgg11_bn, vgg13, vgg13_bn, vgg16, vgg16_bn, vgg19_bn, vgg19\nfrom .classification.resnet import resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d, resnext101_32x8d, wide_resnet50_2,wide_resnet101_2 \nfrom .classification.densenet import densenet121, densenet169, densenet201\nfrom .classification.inception import inception_v3\nfrom .classification.mobilenet import mobilenet_v2\nfrom .segmentation.unet import unet34, unet101, unet152\n\n\n__all__ = [""get_model""]\n\nmodels = {\n    \'vgg11\': vgg11, \n    \'vgg11_bn\': vgg11_bn,\n    \'vgg13\': vgg13,\n    \'vgg13_bn\': vgg13_bn,\n    \'vgg16\': vgg16,\n    \'vgg16_bn\': vgg16_bn,\n    \'vgg19_bn\': vgg19_bn,\n    \'vgg19\': vgg19,\n    \'resnet18\': resnet18,\n    \'resnet34\': resnet34,\n    \'resnet50\': resnet50,\n    \'resnet101\': resnet101,\n    \'resnet152\': resnet152,\n    \'resnext50_32x4d\': resnext50_32x4d,\n    \'resnext101_32x8d\': resnext101_32x8d,\n    \'wide_resnet50_2\': wide_resnet50_2,\n    \'wide_resnet101_2\': wide_resnet101_2,\n    \'mobilenet_v2\': mobilenet_v2,\n    \'inception_v3\': inception_v3,\n    \'densenet121\': densenet121,\n    \'densenet169\': densenet169,\n    \'densenet201\': densenet201,\n    \'unet34\': unet34,\n    \'unet101\': unet101,\n    \'unet152\': unet152,\n}\n\n\ndef get_model(name: str, num_classes: int, **kwargs):\n    print(kwargs)\n    if name.lower() not in models:\n        raise ValueError(""no model named {}, should be one of {}"".format(name, \' \'.join(models)))\n\n    return models.get(name.lower())(num_classes, **kwargs)\n'"
torchsat/transforms/__init__.py,0,b'# TODO & NOTE: \xe6\xad\xa4\xe5\xa4\x84\xe8\x80\x83\xe8\x99\x91\xe5\x9b\xbe\xe5\x83\x8f\xe5\x88\x86\xe7\xb1\xbb\xe3\x80\x81\xe7\x9b\xae\xe6\xa0\x87\xe6\xa3\x80\xe6\xb5\x8b\xef\xbc\x88coco\xe3\x80\x81voc\xef\xbc\x89\xe3\x80\x81\xe5\xae\x9e\xe4\xbe\x8b\xe5\x88\x86\xe5\x89\xb2\xef\xbc\x88cooc\xef\xbc\x89\xe4\xb8\x89\xe7\xa7\x8d\xe6\x83\x85\xe5\x86\xb5\n# \xe6\xaf\x94\xe5\xa6\x82\n# ToTensor\xef\xbc\x9a\xe5\x9b\xbe\xe5\x83\x8f\xe5\x8f\x98\xef\xbc\x8c\xe7\x9b\xae\xe6\xa0\x87\xe4\xb8\x8d\xe5\x8f\x98\n# rotate\xef\xbc\x9a\xe5\x9b\xbe\xe5\x83\x8f\xe3\x80\x81\xe7\x9b\xae\xe6\xa0\x87\xe9\x83\xbd\xe8\xa6\x81\xe5\x8f\x98\n# brightness\xef\xbc\x9a\xe5\x9b\xbe\xe5\x83\x8f\xe5\x8f\x98\xef\xbc\x8c \xe7\x9b\xae\xe6\xa0\x87\xe4\xb8\x8d\xe5\x8f\x98'
torchsat/transforms/functional.py,14,"b'import collections\nimport numbers\nfrom functools import wraps\n\nimport cv2\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom scipy.ndimage.filters import gaussian_filter\n\n__numpy_type_map = {\n    ""float64"": torch.DoubleTensor,\n    ""float32"": torch.FloatTensor,\n    ""float16"": torch.HalfTensor,\n    ""int64"": torch.LongTensor,\n    ""int32"": torch.IntTensor,\n    ""int16"": torch.ShortTensor,\n    ""uint16"": torch.ShortTensor,\n    ""int8"": torch.CharTensor,\n    ""uint8"": torch.ByteTensor,\n}\n\n""""""image functional utils\n\n""""""\n\n# NOTE: all the function should recive the ndarray like image, should be W x H x C or W x H\n\n# \xe5\xa6\x82\xe6\x9e\x9c\xe5\xb0\x86\xe6\x89\x80\xe6\x9c\x89\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\xbb\xb4\xe5\xba\xa6\xe5\xa4\x9f\xe6\x90\x9e\xe6\x88\x90height\xef\xbc\x8cwidth\xef\xbc\x8cchannel \xe9\x82\xa3\xe4\xb9\x88\xe5\x8f\xaf\xe4\xbb\xa5\xe4\xb8\x8d\xe7\x94\xa8to_tensor??, \xe4\xb8\x8d\xe8\xa1\x8c\ndef preserve_channel_dim(func):\n    """"""Preserve dummy channel dim.""""""\n\n    @wraps(func)\n    def wrapped_function(img, *args, **kwargs):\n        shape = img.shape\n        result = func(img, *args, **kwargs)\n        if len(shape) == 3 and shape[-1] == 1 and len(result.shape) == 2:\n            result = np.expand_dims(result, axis=-1)\n        return result\n\n    return wrapped_function\n\n\ndef _is_tensor_image(img):\n    return torch.is_tensor(img) and img.ndimension() == 3\n\n\ndef _is_numpy_image(img):\n    return isinstance(img, np.ndarray) and (img.ndim in {2, 3})\n\n\ndef to_tensor(img):\n    """"""convert numpy.ndarray to torch tensor. \\n\n        if the image is uint8 , it will be divided by 255;\\n\n        if the image is uint16 , it will be divided by 65535;\\n\n        if the image is float , it will not be divided, we suppose your image range should between [0~1] ;\\n\n    \n    Arguments:\n        img {numpy.ndarray} -- image to be converted to tensor. torch.float32\n    """"""\n    if not _is_numpy_image(img):\n        raise TypeError(""data should be numpy ndarray. but got {}"".format(type(img)))\n\n    if img.ndim == 2:\n        img = img[:, :, None]\n\n    if img.dtype == np.uint8:\n        img = img.astype(np.float32) / 255\n    elif img.dtype == np.uint16:\n        img = img.astype(np.float32) / 65535\n    elif img.dtype in [np.float32, np.float64]:\n        img = img.astype(np.float32) / 1\n    else:\n        raise TypeError(""{} is not support"".format(img.dtype))\n\n    img = torch.from_numpy(img.transpose((2, 0, 1)))\n\n    return img\n\n\ndef to_pil_image(tensor):\n    # TODO\n    pass\n\n\ndef to_tiff_image(tensor):\n    # TODO\n    pass\n\n\ndef normalize(tensor, mean, std, inplace=False):\n    """"""Normalize a tensor image with mean and standard deviation.\n\n    .. note::\n        This transform acts out of place by default, i.e., it does not mutates the input tensor.\n\n    See :class:`~torchsat.transforms.Normalize` for more details.\n\n    Args:\n        tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n\n    Returns:\n        Tensor: Normalized Tensor image.\n    """"""\n    if not _is_tensor_image(tensor):\n        raise TypeError(""tensor is not a torch image."")\n\n    if not inplace:\n        tensor = tensor.clone()\n\n    mean = torch.as_tensor(mean, dtype=tensor.dtype, device=tensor.device)\n    std = torch.as_tensor(std, dtype=tensor.dtype, device=tensor.device)\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\n    return tensor\n\n\ndef noise(img, mode=""gaussain"", percent=0.02):\n    """"""\n    TODO: Not good for uint16 data\n    """"""\n    original_dtype = img.dtype\n    if mode == ""gaussian"":\n        mean = 0\n        var = 0.1\n        sigma = var * 0.5\n\n        if img.ndim == 2:\n            h, w = img.shape\n            gauss = np.random.normal(mean, sigma, (h, w))\n        else:\n            h, w, c = img.shape\n            gauss = np.random.normal(mean, sigma, (h, w, c))\n\n        if img.dtype not in [np.float32, np.float64]:\n            gauss = gauss * np.iinfo(img.dtype).max\n            img = np.clip(img.astype(np.float) + gauss, 0, np.iinfo(img.dtype).max)\n        else:\n            img = np.clip(img.astype(np.float) + gauss, 0, 1)\n\n    elif mode == ""salt"":\n        print(img.dtype)\n        s_vs_p = 1\n        num_salt = np.ceil(percent * img.size * s_vs_p)\n        coords = tuple([np.random.randint(0, i - 1, int(num_salt)) for i in img.shape])\n\n        if img.dtype in [np.float32, np.float64]:\n            img[coords] = 1\n        else:\n            img[coords] = np.iinfo(img.dtype).max\n            print(img.dtype)\n    elif mode == ""pepper"":\n        s_vs_p = 0\n        num_pepper = np.ceil(percent * img.size * (1.0 - s_vs_p))\n        coords = tuple(\n            [np.random.randint(0, i - 1, int(num_pepper)) for i in img.shape]\n        )\n        img[coords] = 0\n\n    elif mode == ""s&p"":\n        s_vs_p = 0.5\n\n        # Salt mode\n        num_salt = np.ceil(percent * img.size * s_vs_p)\n        coords = tuple([np.random.randint(0, i - 1, int(num_salt)) for i in img.shape])\n        if img.dtype in [np.float32, np.float64]:\n            img[coords] = 1\n        else:\n            img[coords] = np.iinfo(img.dtype).max\n\n        # Pepper mode\n        num_pepper = np.ceil(percent * img.size * (1.0 - s_vs_p))\n        coords = tuple(\n            [np.random.randint(0, i - 1, int(num_pepper)) for i in img.shape]\n        )\n        img[coords] = 0\n    else:\n        raise ValueError(""not support mode for {}"".format(mode))\n\n    noisy = img.astype(original_dtype)\n\n    return noisy\n\n\ndef gaussian_blur(img, kernel_size):\n    # When sigma=0, it is computed as `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`\n    return cv2.GaussianBlur(img, (kernel_size, kernel_size), sigmaX=0)\n\n\ndef adjust_brightness(img, value=0):\n    if img.dtype in [np.float, np.float32, np.float64, np.float128]:\n        dtype_min, dtype_max = 0, 1\n        dtype = np.float32\n    else:\n        dtype_min = np.iinfo(img.dtype).min\n        dtype_max = np.iinfo(img.dtype).max\n        dtype = np.iinfo(img.dtype)\n\n    result = np.clip(img.astype(np.float) + value, dtype_min, dtype_max).astype(dtype)\n\n    return result\n\n\ndef adjust_contrast(img, factor):\n    if img.dtype in [np.float, np.float32, np.float64, np.float128]:\n        dtype_min, dtype_max = 0, 1\n        dtype = np.float32\n    else:\n        dtype_min = np.iinfo(img.dtype).min\n        dtype_max = np.iinfo(img.dtype).max\n        dtype = np.iinfo(img.dtype)\n\n    result = np.clip(img.astype(np.float) * factor, dtype_min, dtype_max).astype(dtype)\n\n    return result\n\n\ndef adjust_saturation():\n    # TODO\n    pass\n\n\ndef adjust_hue():\n    # TODO\n    pass\n\n\ndef to_grayscale(img, output_channels=1):\n    """"""convert input ndarray image to gray sacle image.\n    \n    Arguments:\n        img {ndarray} -- the input ndarray image\n    \n    Keyword Arguments:\n        output_channels {int} -- output gray image channel (default: {1})\n    \n    Returns:\n        ndarray -- gray scale ndarray image\n    """"""\n    if img.ndim == 2:\n        gray_img = img\n    elif img.shape[2] == 3:\n        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    else:\n        gray_img = np.mean(img, axis=2)\n        gray_img = gray_img.astype(img.dtype)\n\n    if output_channels != 1:\n        gray_img = np.tile(gray_img, (output_channels, 1, 1))\n        gray_img = np.transpose(gray_img, [1, 2, 0])\n\n    return gray_img\n\n\ndef shift(img, top, left):\n    (h, w) = img.shape[0:2]\n    matrix = np.float32([[1, 0, left], [0, 1, top]])\n    dst = cv2.warpAffine(img, matrix, (w, h))\n\n    return dst\n\n\ndef rotate(img, angle, center=None, scale=1.0):\n    (h, w) = img.shape[:2]\n\n    if center is None:\n        center = (w / 2, h / 2)\n\n    M = cv2.getRotationMatrix2D(center, angle, scale)\n    rotated = cv2.warpAffine(img, M, (w, h))\n\n    return rotated\n\n\ndef resize(img, size, interpolation=Image.BILINEAR):\n    """"""resize the image\n    TODO: opencv resize \xe4\xb9\x8b\xe5\x90\x8e\xe5\x9b\xbe\xe5\x83\x8f\xe5\xb0\xb1\xe6\x88\x90\xe4\xba\x860~1\xe4\xba\x86\n    Arguments:\n        img {ndarray} -- the input ndarray image\n        size {int, iterable} -- the target size, if size is intger,  width and height will be resized to same \\\n                                otherwise, the size should be tuple (height, width) or list [height, width]\n                                \n    \n    Keyword Arguments:\n        interpolation {Image} -- the interpolation method (default: {Image.BILINEAR})\n    \n    Raises:\n        TypeError -- img should be ndarray\n        ValueError -- size should be intger or iterable vaiable and length should be 2.\n    \n    Returns:\n        img -- resize ndarray image\n    """"""\n\n    if not _is_numpy_image(img):\n        raise TypeError(\n            ""img shoud be ndarray image [w, h, c] or [w, h], but got {}"".format(\n                type(img)\n            )\n        )\n    if not (\n        isinstance(size, int)\n        or (isinstance(size, collections.Iterable) and len(size) == 2)\n    ):\n        raise ValueError(\n            ""size should be intger or iterable vaiable(length is 2), but got {}"".format(\n                type(size)\n            )\n        )\n\n    if isinstance(size, int):\n        height, width = (size, size)\n    else:\n        height, width = (size[0], size[1])\n\n    return cv2.resize(img, (width, height), interpolation=interpolation)\n\n\ndef pad(img, padding, fill=0, padding_mode=""constant""):\n    if isinstance(padding, int):\n        pad_left = pad_right = pad_top = pad_bottom = padding\n    if isinstance(padding, collections.Iterable) and len(padding) == 2:\n        pad_left = pad_right = padding[0]\n        pad_bottom = pad_top = padding[1]\n    if isinstance(padding, collections.Iterable) and len(padding) == 4:\n        pad_left = padding[0]\n        pad_top = padding[1]\n        pad_right = padding[2]\n        pad_bottom = padding[3]\n\n    if img.ndim == 2:\n        if padding_mode == ""constant"":\n            img = np.pad(\n                img,\n                ((pad_top, pad_bottom), (pad_left, pad_right)),\n                mode=padding_mode,\n                constant_values=fill,\n            )\n        else:\n            img = np.pad(\n                img, ((pad_top, pad_bottom), (pad_left, pad_right)), mode=padding_mode\n            )\n    if img.ndim == 3:\n        if padding_mode == ""constant"":\n            img = np.pad(\n                img,\n                ((pad_top, pad_bottom), (pad_left, pad_right), (0, 0)),\n                mode=padding_mode,\n                constant_values=fill,\n            )\n        else:\n            img = np.pad(\n                img,\n                ((pad_top, pad_bottom), (pad_left, pad_right), (0, 0)),\n                mode=padding_mode,\n            )\n    return img\n\n\ndef crop(img, top, left, height, width):\n    """"""crop image \n    \n    Arguments:\n        img {ndarray} -- image to be croped\n        top {int} -- top size\n        left {int} -- left size \n        height {int} -- croped height\n        width {int} -- croped width\n    """"""\n    if not _is_numpy_image(img):\n        raise TypeError(\n            ""the input image should be numpy ndarray with dimension 2 or 3.""\n            ""but got {}"".format(type(img))\n        )\n\n    if width < 0 or height < 0 or left < 0 or height < 0:\n        raise ValueError(\n            ""the input left, top, width, height should be greater than 0""\n            ""but got left={}, top={} width={} height={}"".format(\n                left, top, width, height\n            )\n        )\n    if img.ndim == 2:\n        img_height, img_width = img.shape\n    else:\n        img_height, img_width, _ = img.shape\n    if (left + width) > img_width or (top + height) > img_height:\n        raise ValueError(\n            ""the input crop width and height should be small or \\\n         equal to image width and height. ""\n        )\n\n    if img.ndim == 2:\n        return img[top : (top + height), left : (left + width)]\n    elif img.ndim == 3:\n        return img[top : (top + height), left : (left + width), :]\n\n\ndef center_crop(img, output_size):\n    """"""crop image\n    \n    Arguments:\n        img {ndarray} -- input image\n        output_size {number or sequence} -- the output image size. if sequence, should be [h, w]\n    \n    Raises:\n        ValueError -- the input image is large than original image.\n    \n    Returns:\n        ndarray image -- return croped ndarray image.\n    """"""\n    if img.ndim == 2:\n        img_height, img_width = img.shape\n    else:\n        img_height, img_width, _ = img.shape\n\n    if isinstance(output_size, numbers.Number):\n        output_size = (int(output_size), int(output_size))\n    if output_size[0] > img_height or output_size[1] > img_width:\n        raise ValueError(\n            ""the output_size should not greater than image size, but got {}"".format(\n                output_size\n            )\n        )\n\n    target_height, target_width = output_size\n\n    top = int(round((img_height - target_height) / 2))\n    left = int(round((img_width - target_width) / 2))\n\n    return crop(img, top, left, target_height, target_width)\n\n\ndef resized_crop(img, top, left, height, width, size, interpolation=Image.BILINEAR):\n\n    img = crop(img, top, left, height, width)\n    img = resize(img, size, interpolation)\n    return img\n\n\ndef vflip(img):\n    return cv2.flip(img, 0)\n\n\ndef hflip(img):\n    return cv2.flip(img, 1)\n\n\ndef flip(img, flip_code):\n    return cv2.flip(img, flip_code)\n\n\ndef elastic_transform(\n    image,\n    alpha,\n    sigma,\n    alpha_affine,\n    interpolation=cv2.INTER_LINEAR,\n    border_mode=cv2.BORDER_REFLECT_101,\n    random_state=None,\n    approximate=False,\n):\n    """"""Elastic deformation of images as described in [Simard2003]_ (with modifications).\n    Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5\n    .. [Simard2003] Simard, Steinkraus and Platt, ""Best Practices for\n         Convolutional Neural Networks applied to Visual Document Analysis"", in\n         Proc. of the International Conference on Document Analysis and\n         Recognition, 2003.\n    """"""\n    if random_state is None:\n        random_state = np.random.RandomState(1234)\n\n    height, width = image.shape[:2]\n\n    # Random affine\n    center_square = np.float32((height, width)) // 2\n    square_size = min((height, width)) // 3\n    alpha = float(alpha)\n    sigma = float(sigma)\n    alpha_affine = float(alpha_affine)\n\n    pts1 = np.float32(\n        [\n            center_square + square_size,\n            [center_square[0] + square_size, center_square[1] - square_size],\n            center_square - square_size,\n        ]\n    )\n    pts2 = pts1 + random_state.uniform(\n        -alpha_affine, alpha_affine, size=pts1.shape\n    ).astype(np.float32)\n    matrix = cv2.getAffineTransform(pts1, pts2)\n\n    image = cv2.warpAffine(\n        image, matrix, (width, height), flags=interpolation, borderMode=border_mode\n    )\n\n    if approximate:\n        # Approximate computation smooth displacement map with a large enough kernel.\n        # On large images (512+) this is approximately 2X times faster\n        dx = random_state.rand(height, width).astype(np.float32) * 2 - 1\n        cv2.GaussianBlur(dx, (17, 17), sigma, dst=dx)\n        dx *= alpha\n\n        dy = random_state.rand(height, width).astype(np.float32) * 2 - 1\n        cv2.GaussianBlur(dy, (17, 17), sigma, dst=dy)\n        dy *= alpha\n    else:\n        dx = np.float32(\n            gaussian_filter((random_state.rand(height, width) * 2 - 1), sigma) * alpha\n        )\n        dy = np.float32(\n            gaussian_filter((random_state.rand(height, width) * 2 - 1), sigma) * alpha\n        )\n\n    x, y = np.meshgrid(np.arange(width), np.arange(height))\n\n    mapx = np.float32(x + dx)\n    mapy = np.float32(y + dy)\n\n    return cv2.remap(image, mapx, mapy, interpolation, borderMode=border_mode)\n\n\ndef bbox_shift(bboxes, top, left):\n    """"""shift the bbox\n    \n    Arguments:\n        bboxes {ndarray} -- input bboxes, should be num x 4\n        top {int} -- shit to top value, positive mean move down, negative mean move up.\n        left {int} -- shit to left value, positive mean move right, negative mean move left.\n    \n    Returns:\n        [ndarray] -- shifted bboxes\n    """"""\n    shifted_bboxes = bboxes.copy()\n    shifted_bboxes[..., 0::2] = shifted_bboxes[..., 0::2] + left\n    shifted_bboxes[..., 1::2] = shifted_bboxes[..., 1::2] + top\n    \n    return shifted_bboxes\n\n\ndef bbox_vflip(bboxes, img_height):\n    """"""vertical flip the bboxes\n    ...........\n    .         .\n    .         .\n   >...........<\n    .         .\n    .         .\n    ...........\n    Args:\n        bbox (ndarray): bbox ndarray [box_nums, 4]\n        flip_code (int, optional): [description]. Defaults to 0.\n    """"""\n    flipped = bboxes.copy()\n    flipped[..., 1::2] = img_height - bboxes[..., 1::2]\n    flipped = flipped[..., [0, 3, 2, 1]]\n    return flipped\n\n\ndef bbox_hflip(bboxes, img_width):\n    """"""horizontal flip the bboxes\n          ^\n    .............\n    .     .     .\n    .     .     .\n    .     .     .\n    .     .     .\n    .............\n          ^\n    Args:\n        bbox (ndarray): bbox ndarray [box_nums, 4]\n        flip_code (int, optional): [description]. Defaults to 0.\n    """"""\n    flipped = bboxes.copy()\n    flipped[..., 0::2] = img_width - bboxes[..., 0::2]\n    flipped = flipped[..., [2, 1, 0, 3]]\n    return flipped\n\n\ndef bbox_resize(bboxes, img_size, target_size):\n    """"""resize the bbox\n    \n    Args:\n        bboxes (ndarray): bbox ndarray [box_nums, 4]\n        img_size (tuple): the image height and width\n        target_size (int, or tuple): the target bbox size. \n                Int or Tuple, if tuple the shape should be (height, width)\n    """"""\n    if isinstance(target_size, numbers.Number):\n        target_size = (target_size, target_size)\n\n    ratio_height = target_size[0] / img_size[0]\n    ratio_width = target_size[1] / img_size[1]\n\n    return bboxes[...,] * [ratio_width, ratio_height, ratio_width, ratio_height]\n\n\ndef bbox_crop(bboxes, top, left, height, width):\n    """"""crop bbox \n    \n    Arguments:\n        img {ndarray} -- image to be croped\n        top {int} -- top size\n        left {int} -- left size \n        height {int} -- croped height\n        width {int} -- croped width\n    """"""\n    croped_bboxes = bboxes.copy()\n\n    right = width + left\n    bottom = height + top\n\n    croped_bboxes[..., 0::2] = bboxes[..., 0::2].clip(left, right) - left\n    croped_bboxes[..., 1::2] = bboxes[..., 1::2].clip(top, bottom) - top\n\n    return croped_bboxes\n\n\ndef bbox_pad(bboxes, padding):\n    if isinstance(padding, int):\n        pad_left = pad_right = pad_top = pad_bottom = padding\n    if isinstance(padding, collections.Iterable) and len(padding) == 2:\n        pad_left = pad_right = padding[0]\n        pad_bottom = pad_top = padding[1]\n    if isinstance(padding, collections.Iterable) and len(padding) == 4:\n        pad_left = padding[0]\n        pad_top = padding[1]\n        pad_right = padding[2]\n        pad_bottom = padding[3]\n\n    pad_bboxes = bboxes.copy()\n    pad_bboxes[..., 0::2] = bboxes[..., 0::2] + pad_left\n    pad_bboxes[..., 1::2] = bboxes[..., 1::2] + pad_top\n\n    return pad_bboxes\n\n\ndef rotate_box(bboxes, angle,  center_x, center_y):\n    """"""rotate_box rotate the bboxes\n    \n    \n    Args:\n        bboxes (ndarray): the original bboxes, N(numbers) x 4\n        angle (float): rotate angle should be degree\n        center_x (int): rotate center x\n        center_y (int): rotate center y\n    \n    Returns:\n        ndarray: the rotated bboxes\n    """"""\n    corners = np.concatenate(\n                (bboxes[:,[0,1]], bboxes[:,[2,1]], bboxes[:,[2,3]], bboxes[:,[0,3]]),\n                axis=1\n              )\n    corners = corners.reshape(-1, 2)\n    corners = np.hstack((corners, np.ones((corners.shape[0],1), dtype = type(corners[0][0]))))\n    # 1. transform the coordinates of all four corners\n    M = cv2.getRotationMatrix2D((center_x, center_y), angle, 1.0)\n    corners_rotated = np.dot(M, corners.T).T\n    corners_rotated = corners_rotated.reshape(-1,8)\n    \n    # 2. Find the smallest of all four x\'s as min_x and the largest of all four x\'s and call it max_x\n    min_x = np.min(corners_rotated[:, 0::2], axis=1)\n    max_x = np.max(corners_rotated[:, 0::2], axis=1)\n    \n    # 3. Ditto with the y\'s\n    min_y = np.min(corners_rotated[:, 1::2], axis=1)\n    max_y = np.max(corners_rotated[:, 1::2], axis=1)\n    \n    # 4. Your bounding box is (min_x,min_y), (min_x,max_y), (max_x,max_y), (max_x,min_y)\n    bboxes_rotated = np.stack([min_x,min_y,max_x,max_y], axis=-1)\n    bboxes_rotated = np.round(bboxes_rotated)\n\n    return bboxes_rotated\n'"
torchsat/transforms/transforms_cls.py,1,"b'import collections\nimport numbers\nimport random\n\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom . import functional as F\n\n__all__ = [\n    ""Compose"",\n    ""Lambda"",\n    ""ToTensor"",\n    ""Normalize"",\n    ""ToGray"",\n    ""GaussianBlur"",\n    ""RandomNoise"",\n    ""RandomBrightness"",\n    ""RandomContrast"",\n    ""RandomShift"",\n    ""RandomRotation"",\n    ""Resize"",\n    ""Pad"",\n    ""CenterCrop"",\n    ""RandomCrop"",\n    ""RandomHorizontalFlip"",\n    ""RandomVerticalFlip"",\n    ""RandomFlip"",\n    ""RandomResizedCrop"",\n    ""ElasticTransform"",\n]\n\n\nclass Compose(object):\n    """"""Composes serveral classification transform together.\n    \n    Args:\n        transforms (list of ``transform`` objects): list of classification transforms to compose.\n    \n    Example:\n        >>> transforms_cls.Compose([\n        >>>     transforms_cls.Resize(300),\n        >>>     transforms_cls.ToTensor()\n        >>>     ])\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        for t in self.transforms:\n            img = t(img)\n        return img\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + ""(""\n        for t in self.transforms:\n            format_string += ""\\n""\n            format_string += ""    {0}"".format(t)\n        format_string += ""\\n)""\n        return format_string\n\n\nclass Lambda(object):\n    """"""Apply a user-defined lambda as function.\n    \n    Args:\n        lambd (function): Lambda/function to be used for transform.\n    \n    """"""\n\n    def __init__(self, lambd):\n        self.lambd = lambd\n\n    def __call__(self, img):\n        return self.lambd(img)\n\n    def __repr__(self):\n        return self.__class__.__namme + ""()""\n\n\nclass ToTensor(object):\n    """"""onvert numpy.ndarray to torch tensor.\n\n        if the image is uint8 , it will be divided by 255;\n        if the image is uint16 , it will be divided by 65535;\n        if the image is float , it will not be divided, we suppose your image range should between [0~1] ;\\n\n    \n    Args:\n        img {numpy.ndarray} -- image to be converted to tensor.\n    """"""\n\n    def __call__(self, img):\n\n        return F.to_tensor(img)\n\n\nclass Normalize(object):\n    """"""Normalize a tensor image with mean and standard deviation.\n\n    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n    will normalize each channel of the input ``torch.*Tensor`` i.e.\n    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n    .. note::\n        This transform acts out of place, i.e., it does not mutates the input tensor.\n    Args:\n        tensor (tensor): input torch tensor data.\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n        inplace (boolean): inplace apply the transform or not. (default: False)\n    """"""\n\n    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False):\n        self.mean = mean\n        self.std = std\n        self.inplace = inplace\n\n    def __call__(self, tensor):\n        return F.normalize(tensor, self.mean, self.std, self.inplace)\n\n\nclass ToPILImage(object):\n    # TODO\n    pass\n\n\nclass ToGray(object):\n    """"""Convert the image to grayscale\n    \n    Args:\n        output_channels (int): number of channels desired for output image. (default: 1)\n    \n    Returns:\n        [ndarray]: the graysacle version of input\n        - If output_channels=1 : returned single channel image (height, width)\n        - If output_channels>1 : returned multi-channels ndarray image (height, width, channels)\n    """"""\n\n    def __init__(self, output_channels=1):\n        self.output_channels = output_channels\n\n    def __call__(self, img):\n        return F.to_grayscale(img, self.output_channels)\n\n\nclass GaussianBlur(object):\n    """"""Convert the input ndarray image to blurred image by gaussian method.\n    \n    Args:\n        kernel_size (int): kernel size of gaussian blur method. (default: 3)\n    \n    Returns:\n        ndarray: the blurred image.\n    """"""\n\n    def __init__(self, kernel_size=3):\n        self.kernel_size = kernel_size\n\n    def __call__(self, img):\n        return F.gaussian_blur(img, self.kernel_size)\n\n\nclass RandomNoise(object):\n    """"""Add noise to the input ndarray image.\n    Args:\n        mode (str): the noise mode, should be one of ``gaussian``, ``salt``, ``pepper``, ``s&p``, (default: gaussian).\n        percent (float): noise percent, only work for ``salt``, ``pepper``, ``s&p`` mode. (default: 0.02)\n    \n    Returns:\n        ndarray: noised ndarray image.\n    """"""\n\n    def __init__(self, mode=""gaussian"", percent=0.02):\n        if mode not in [""gaussian"", ""salt"", ""pepper"", ""s&p""]:\n            raise ValueError(\n                ""mode should be gaussian, salt, pepper, but got {}"".format(mode)\n            )\n        self.mode = mode\n        self.percent = percent\n\n    def __call__(self, img):\n        return F.noise(img, self.mode, self.percent)\n\n\nclass RandomBrightness(object):\n    def __init__(self, max_value=0):\n        if isinstance(max_value, numbers.Number):\n            self.value = random.uniform(-max_value, max_value)\n        if isinstance(max_value, collections.Iterable) and len(max_value) == 2:\n            self.value = random.uniform(max_value[0], max_value[1])\n\n    def __call__(self, img):\n        return F.adjust_brightness(img, self.value)\n\n\nclass RandomContrast(object):\n    def __init__(self, max_factor=0):\n        if isinstance(max_factor, numbers.Number):\n            self.factor = random.uniform(-max_factor, max_factor)\n        if isinstance(max_factor, collections.Iterable) and len(max_factor) == 2:\n            self.factor = random.uniform(max_factor[0], max_factor[1])\n\n    def __call__(self, img):\n        return F.adjust_contrast(img, self.factor)\n\n\nclass RandomShift(object):\n    """"""random shift the ndarray with value or some percent.\n    \n    Args:\n        max_percent (float): shift percent of the image.\n    \n    Returns:\n        ndarray: return the shifted ndarray image.\n    """"""\n\n    def __init__(self, max_percent=0.4):\n        self.max_percent = max_percent\n\n    def __call__(self, img):\n        height, width = img.shape[0:2]\n        max_top = int(height * self.max_percent)\n        max_left = int(width * self.max_percent)\n        top = random.randint(-max_top, max_top)\n        left = random.randint(-max_left, max_left)\n\n        return F.shift(img, top, left)\n\n\nclass RandomRotation(object):\n    """"""random rotate the ndarray image with the degrees.\n    \n    Args:\n        degrees (number or sequence): the rotate degree.\n                                  If single number, it must be positive.\n                                  if squeence, it\'s length must 2 and first number should small than the second one.\n    \n    Raises:\n        ValueError: If degrees is a single number, it must be positive.\n        ValueError: If degrees is a sequence, it must be of len 2.\n    \n    Returns:\n        ndarray: return rotated ndarray image.\n    """"""\n\n    def __init__(self, degrees, center=None):\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError(""If degrees is a single number, it must be positive."")\n            self.degrees = (-degrees, degrees)\n        else:\n            if len(degrees) != 2:\n                raise ValueError(""If degrees is a sequence, it must be of len 2."")\n            self.degrees = degrees\n        self.center = center\n\n    def __call__(self, img):\n        angle = random.uniform(self.degrees[0], self.degrees[1])\n        return F.rotate(img, angle, self.center)\n\n\nclass Resize(object):\n    """"""resize the image\n    Args:\n        img {ndarray} : the input ndarray image\n        size {int, iterable} : the target size, if size is intger,  width and height will be resized to same \\\n                                otherwise, the size should be tuple (height, width) or list [height, width]\n                                \n    \n    Keyword Arguments:\n        interpolation {Image} : the interpolation method (default: {Image.BILINEAR})\n    \n    Raises:\n        TypeError : img should be ndarray\n        ValueError : size should be intger or iterable vaiable and length should be 2.\n    \n    Returns:\n        img (ndarray) : resize ndarray image\n    """"""\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        return F.resize(img, self.size, self.interpolation)\n\n\nclass Pad(object):\n    """"""Pad the given ndarray image with padding width.\n    Args:\n        padding : {int, sequence}, padding width \n                  If int, each border same.\n                  If sequence length is 2, this is the padding for left/right and top/bottom.\n                  If sequence length is 4, this is the padding for left, top, right, bottom.\n        fill: {int, sequence}: Pixel\n        padding_mode: str or function. contain{\xe2\x80\x98constant\xe2\x80\x99,\xe2\x80\x98edge\xe2\x80\x99,\xe2\x80\x98linear_ramp\xe2\x80\x99,\xe2\x80\x98maximum\xe2\x80\x99,\xe2\x80\x98mean\xe2\x80\x99\n            , \xe2\x80\x98median\xe2\x80\x99, \xe2\x80\x98minimum\xe2\x80\x99, \xe2\x80\x98reflect\xe2\x80\x99,\xe2\x80\x98symmetric\xe2\x80\x99,\xe2\x80\x98wrap\xe2\x80\x99} (default: constant)\n    Examples:\n        >>> transformed_img = Pad(img, 20, mode=\'reflect\')\n        >>> transformed_img = Pad(img, (10,20), mode=\'edge\')\n        >>> transformed_img = Pad(img, (10,20,30,40), mode=\'reflect\')\n    """"""\n\n    def __init__(self, padding, fill=0, padding_mode=""constant""):\n        self.padding = padding\n        self.fill = fill\n        self.padding_mode = padding_mode\n\n    def __call__(self, img):\n        return F.pad(img, self.padding, self.fill, self.padding_mode)\n\n\nclass CenterCrop(object):\n    """"""crop image\n    \n    Args:\n        img {ndarray}: input image\n        output_size {number or sequence}: the output image size. if sequence, should be [height, width]\n    \n    Raises:\n        ValueError: the input image is large than original image.\n    \n    Returns:\n        ndarray: return croped ndarray image.\n    """"""\n\n    def __init__(self, out_size):\n        self.out_size = out_size\n\n    def __call__(self, img):\n        return F.center_crop(img, self.out_size)\n\n\nclass RandomCrop(object):\n    """"""random crop the input ndarray image\n    \n    Args:\n        size (int, sequence): th output image size, if sequeue size should be [height, width]\n    \n    Returns:\n        ndarray:  return random croped ndarray image.\n    """"""\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (size, size)\n        else:\n            self.size = size\n\n    def __call__(self, img):\n        h, w = img.shape[0:2]\n        th, tw = self.size\n        if w == tw and h == tw:\n            return img\n\n        top = random.randint(0, h - th)\n        left = random.randint(0, w - tw)\n\n        return F.crop(img, top, left, th, tw)\n\n\nclass RandomHorizontalFlip(object):\n    """"""Flip the input image on central horizon line.\n    \n    Args:\n        p (float): probability apply the horizon flip.(default: 0.5)\n    \n    Returns:\n        ndarray: return the flipped image.\n    """"""\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            return F.hflip(img)\n        return img\n\n    def __repr__(self):\n        return self.__class__.__name__ + ""(p={})"".format(self.p)\n\n\nclass RandomVerticalFlip(object):\n    """"""Flip the input image on central vertical line.\n    \n    Args:\n        p (float): probability apply the vertical flip. (default: 0.5)\n    \n    Returns:\n        ndarray: return the flipped image.\n    """"""\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            return F.vflip(img)\n        return img\n\n    def __repr__(self):\n        return self.__class__.__name__ + ""(p={})"".format(self.p)\n\n\nclass RandomFlip(object):\n    """"""Flip the input image vertical or horizon.\n    \n    Args:\n        p (float): probability apply flip. (default: 0.5)\n    \n    Returns:\n        ndarray: return the flipped image.\n    """"""\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img):\n        if random.random() < self.p:\n            flip_code = random.randint(0, 1)\n            return F.flip(img, flip_code)\n        return img\n\n    def __repr__(self):\n        return self.__class__.__name__ + ""(p={})"".format(self.p)\n\n\nclass RandomResizedCrop(object):\n    """"""[summary]\n    \n    Args:\n        object ([type]): [description]\n    \n    Returns:\n        [type]: [description]\n    """"""\n\n    def __init__(self, crop_size, target_size, interpolation=Image.BILINEAR):\n        if isinstance(crop_size, numbers.Number):\n            self.crop_size = (crop_size, crop_size)\n        else:\n            self.crop_size = crop_size\n        self.target_size = target_size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        h, w = img.shape[0:2]\n        th, tw = self.crop_size\n        if w == tw and h == tw:\n            return img\n\n        top = random.randint(0, h - th)\n        left = random.randint(0, w - tw)\n\n        img = F.crop(img, top, left, th, tw)\n        img = F.resize(img, self.target_size, interpolation=self.interpolation)\n\n        return img\n\n\nclass ElasticTransform(object):\n    """"""\n    code modify from https://github.com/albu/albumentations.  \n    Elastic deformation of images as described in [Simard2003]_ (with modifications).\n    Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5\n    .. [Simard2003] Simard, Steinkraus and Platt, ""Best Practices for\n         Convolutional Neural Networks applied to Visual Document Analysis"", in\n         Proc. of the International Conference on Document Analysis and\n         Recognition, 2003.\n    Args:\n        approximate (boolean): Whether to smooth displacement map with fixed kernel size.\n                               Enabling this option gives ~2X speedup on large images.\n    Image types:\n        uint8, uint16 float32\n    """"""\n\n    def __init__(\n        self,\n        alpha=1,\n        sigma=50,\n        alpha_affine=50,\n        interpolation=cv2.INTER_LINEAR,\n        border_mode=cv2.BORDER_REFLECT_101,\n        random_state=None,\n        approximate=False,\n    ):\n        self.alpha = alpha\n        self.alpha_affine = alpha_affine\n        self.sigma = sigma\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.random_state = random_state\n        self.approximate = approximate\n\n    def __call__(self, img):\n        return F.elastic_transform(\n            img,\n            self.alpha,\n            self.sigma,\n            self.alpha_affine,\n            self.interpolation,\n            self.border_mode,\n            np.random.RandomState(self.random_state),\n            self.approximate,\n        )\n\n'"
torchsat/transforms/transforms_det.py,3,"b'import collections\nimport numbers\nimport random\n\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\n\nfrom . import functional as F\n\n__all__ = [\n    ""Compose"",\n    ""Lambda"",\n    ""ToTensor"",\n    ""Normalize"",\n    ""ToGray"",\n    ""GaussianBlur"",\n    ""RandomNoise"",\n    ""RandomBrightness"",\n    ""RandomContrast"",\n    ""Resize"",\n    ""Pad"",\n    ""CenterCrop"",\n    ""RandomCrop"",\n    ""RandomHorizontalFlip"",\n    ""RandomVerticalFlip"",\n    ""RandomFlip"",\n    ""RandomResizedCrop"",\n]\n\n# model input params\n# img: batch_size, channel, height, width\n# boxes(float32): batch_size, bbox_nums(4 numbers\xef\xbc\x8cleft(xmin), top(ymin), right(xmax), bottom(ymin)), 4\n# labels(int64): batch_size, bbox_nums(each bbox corresponding label)\n\n# \xe5\x90\x8c\xe5\xb8\xb8\xe6\x9c\x89\xe4\xb8\x89\xe7\xa7\x8d\xe6\x83\x85\xe5\x86\xb5\xef\xbc\x8c\n# \xe4\xb8\x80\xe7\xa7\x8d\xe6\x98\xaf\xe4\xb8\x89\xe4\xb8\xaa\xe5\x90\x88\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\n# \xe4\xba\x8c\xe7\xa7\x8d\xe6\x98\xafimg, (boxes, labels)\n\n\nclass Compose(object):\n    """"""Composes serveral classification transform together.\n    \n    Args:\n        transforms (list of ``transform`` objects): list of classification transforms to compose.\n    \n    Example:\n        >>> transforms_cls.Compose([\n        >>>     transforms_cls.Resize(300),\n        >>>     transforms_cls.ToTensor()\n        >>>     ])\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, bboxes, labels):\n        for t in self.transforms:\n            img, bboxes, labels = t(img, bboxes, labels)\n        return img, bboxes, labels\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + ""(""\n        for t in self.transforms:\n            format_string += ""\\n""\n            format_string += ""    {0}"".format(t)\n        format_string += ""\\n)""\n        return format_string\n\n\nclass Lambda(object):\n    """"""Apply a user-defined lambda as function.\n    \n    Args:\n        lambd (function): Lambda/function to be used for transform.\n    \n    """"""\n\n    def __init__(self, lambd):\n        self.lambd = lambd\n\n    def __call__(self, img, bboxes, labels):\n        return self.lambd(img, bboxes, labels)\n\n    def __repr__(self):\n        return self.__class__.__namme + ""()""\n\n\nclass ToTensor(object):\n    """"""onvert numpy.ndarray to torch tensor.\n\n        if the image is uint8 , it will be divided by 255;\n        if the image is uint16 , it will be divided by 65535;\n        if the image is float , it will not be divided, we suppose your image range should between [0~1] ;\\n\n    \n    Args:\n        img {numpy.ndarray} -- image to be converted to tensor.\n        bboxes {numpy.ndarray} -- target bbox to be converted to tensor. the input should be [box_nums, 4]\n        labels {numpy.ndarray} -- target labels to be converted to tensor. the input shape shold be [box_nums]\n    """"""\n\n    def __call__(self, img, bboxes, labels):\n\n        return (\n            F.to_tensor(img),\n            torch.tensor(bboxes, dtype=torch.float),\n            torch.tensor(labels, dtype=torch.int),\n        )\n\n\nclass Normalize(object):\n    """"""Normalize a tensor image with mean and standard deviation.\n\n    Given mean: ``(M1,...,Mn)`` and std: ``(S1,..,Sn)`` for ``n`` channels, this transform\n    will normalize each channel of the input ``torch.*Tensor`` i.e.\n    ``input[channel] = (input[channel] - mean[channel]) / std[channel]``\n    .. note::\n        This transform acts out of place, i.e., it does not mutates the input tensor.\n    Args:\n        tensor (tensor): input torch tensor data.\n        mean (sequence): Sequence of means for each channel.\n        std (sequence): Sequence of standard deviations for each channel.\n        inplace (boolean): inplace apply the transform or not. (default: False)\n    """"""\n\n    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False):\n        self.mean = mean\n        self.std = std\n        self.inplace = inplace\n\n    def __call__(self, tensor, bboxes, labels):\n        return F.normalize(tensor, self.mean, self.std, self.inplace), bboxes, labels\n\n\nclass ToPILImage(object):\n    # TODO\n    pass\n\n\nclass ToGray(object):\n    """"""Convert the image to grayscale\n    \n    Args:\n        output_channels (int): number of channels desired for output image. (default: 1)\n    \n    Returns:\n        [ndarray]: the graysacle version of input\n        - If output_channels=1 : returned single channel image (height, width)\n        - If output_channels>1 : returned multi-channels ndarray image (height, width, channels)\n    """"""\n\n    def __init__(self, output_channels=1):\n        self.output_channels = output_channels\n\n    def __call__(self, img, bboxes, labels):\n        return F.to_grayscale(img, self.output_channels), bboxes, labels\n\n\nclass GaussianBlur(object):\n    """"""Convert the input ndarray image to blurred image by gaussian method.\n    \n    Args:\n        kernel_size (int): kernel size of gaussian blur method. (default: 3)\n    \n    Returns:\n        ndarray: the blurred image.\n    """"""\n\n    def __init__(self, kernel_size=3):\n        self.kernel_size = kernel_size\n\n    def __call__(self, img, bboxes, labels):\n        return F.gaussian_blur(img, self.kernel_size), bboxes, labels\n\n\nclass RandomNoise(object):\n    """"""Add noise to the input ndarray image.\n    Args:\n        mode (str): the noise mode, should be one of ``gaussian``, ``salt``, ``pepper``, ``s&p``, (default: gaussian).\n        percent (float): noise percent, only work for ``salt``, ``pepper``, ``s&p`` mode. (default: 0.02)\n    \n    Returns:\n        ndarray: noised ndarray image.\n    """"""\n\n    def __init__(self, mode=""gaussian"", percent=0.02):\n        if mode not in [""gaussian"", ""salt"", ""pepper"", ""s&p""]:\n            raise ValueError(\n                ""mode should be gaussian, salt, pepper, but got {}"".format(mode)\n            )\n        self.mode = mode\n        self.percent = percent\n\n    def __call__(self, img, bboxes, labels):\n        return F.noise(img, self.mode, self.percent), bboxes, labels\n\n\nclass RandomBrightness(object):\n    def __init__(self, max_value=0):\n        if isinstance(max_value, numbers.Number):\n            self.value = random.uniform(-max_value, max_value)\n        if isinstance(max_value, collections.Iterable) and len(max_value) == 2:\n            self.value = random.uniform(max_value[0], max_value[1])\n\n    def __call__(self, img, bboxes, labels):\n        return F.adjust_brightness(img, self.value), bboxes, labels\n\n\nclass RandomContrast(object):\n    def __init__(self, max_factor=0):\n        if isinstance(max_factor, numbers.Number):\n            self.factor = random.uniform(-max_factor, max_factor)\n        if isinstance(max_factor, collections.Iterable) and len(max_factor) == 2:\n            self.factor = random.uniform(max_factor[0], max_factor[1])\n\n    def __call__(self, img, bboxes, labels):\n        return F.adjust_contrast(img, self.factor), bboxes, labels\n\n\nclass RandomShift(object):\n    """"""random shift the image and bbox with some percent.\n\n    Args:\n        max_percent (float): shift percent of the image and bbox.\n\n    Returns:\n        ndarray: return the shifted ndarray image.\n        bboxes: return the shifted bboxes\n        labels: return the shifted labels\n    """"""\n    def __init__(self, max_percent=0.4):\n        self.max_percent = max_percent\n\n    def __call__(self, img, bboxes, labels):\n        height, width = img.shape[0:2]\n        max_top = int(height * self.max_percent)\n        max_left = int(width * self.max_percent)\n        top = random.randint(-max_top, max_top)\n        left = random.randint(-max_left, max_left)\n\n        bboxes = F.bbox_shift(bboxes, top, left)\n\n        # clip bboxes\n        bboxes[bboxes<0] = 0\n        bboxes[bboxes>height] = height\n        bboxes[bboxes>width] = width\n\n        # find the outside boxes and remove them\n        x_check = bboxes[..., 0] == bboxes[..., 2]  # x direction(width)\n        y_check = bboxes[..., 1] == bboxes[..., 3]  # y direction(height)\n        bboxes = bboxes[~(x_check | y_check)]\n        labels = np.array(labels)[~(x_check | y_check)].tolist()\n\n        return F.shift(img, top, left), bboxes, labels\n\n\nclass RandomRotation(object):\n    """"""random rotate the ndarray image with the degrees.\n\n    Args:\n        degrees (number or sequence): the rotate degree.\n                                  If single number, it must be positive.\n                                  if squeence, it\'s length must 2 and first number should small than the second one.\n\n    Raises:\n        ValueError: If degrees is a single number, it must be positive.\n        ValueError: If degrees is a sequence, it must be of len 2.\n\n    Returns:\n        ndarray: return rotated ndarray image.\n    """"""\n    def __init__(self, degrees, center=None):\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError(""If degrees is a single number, it must be positive."")\n            self.degrees = (-degrees, degrees)\n        else:\n            if len(degrees) != 2:\n                raise ValueError(""If degrees is a sequence, it must be of len 2."")\n            self.degrees = degrees\n        self.center = center\n\n    def __call__(self, img, bboxes, labels):\n        angle = random.uniform(self.degrees[0], self.degrees[1])\n        height, width = img.shape[0:2]\n        bboxes = F.rotate_box(bboxes, angle,  width/2, height/2)\n        \n        # clip bboxes\n        bboxes[bboxes<0] = 0\n        bboxes[bboxes>height] = height\n        bboxes[bboxes>width] = width\n\n        # find the outside boxes and remove them\n        x_check = bboxes[..., 0] == bboxes[..., 2]  # x direction(width)\n        y_check = bboxes[..., 1] == bboxes[..., 3]  # y direction(height)\n        bboxes = bboxes[~(x_check | y_check)]\n        labels = np.array(labels)[~(x_check | y_check)].tolist()\n\n        return F.rotate(img, angle, self.center), bboxes, labels\n\n\nclass Resize(object):\n    """"""resize the image\n    Args:\n        img {ndarray} : the input ndarray image\n        size {int, iterable} : the target size, if size is intger,  width and height will be resized to same \\\n                                otherwise, the size should be tuple (height, width) or list [height, width]\n                                \n    \n    Keyword Arguments:\n        interpolation {Image} : the interpolation method (default: {Image.BILINEAR})\n    \n    Raises:\n        TypeError : img should be ndarray\n        ValueError : size should be intger or iterable vaiable and length should be 2.\n    \n    Returns:\n        img (ndarray) : resize ndarray image\n    """"""\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img, bboxes, labels):\n        return (\n            F.resize(img, self.size, self.interpolation),\n            F.bbox_resize(bboxes, img.shape[0:2], self.size),\n            labels,\n        )\n\n\nclass Pad(object):\n    """"""Pad the given ndarray image with padding width.\n    Args:\n        padding : {int, sequence}, padding width \n                  If int, each border same.\n                  If sequence length is 2, this is the padding for left/right and top/bottom.\n                  If sequence length is 4, this is the padding for left, top, right, bottom.\n        fill: {int, sequence}: Pixel\n        padding_mode: str or function. contain{\xe2\x80\x98constant\xe2\x80\x99,\xe2\x80\x98edge\xe2\x80\x99,\xe2\x80\x98linear_ramp\xe2\x80\x99,\xe2\x80\x98maximum\xe2\x80\x99,\xe2\x80\x98mean\xe2\x80\x99\n            , \xe2\x80\x98median\xe2\x80\x99, \xe2\x80\x98minimum\xe2\x80\x99, \xe2\x80\x98reflect\xe2\x80\x99,\xe2\x80\x98symmetric\xe2\x80\x99,\xe2\x80\x98wrap\xe2\x80\x99} (default: constant)\n    Examples:\n        >>> transformed_img = Pad(img, 20, mode=\'reflect\')\n        >>> transformed_img = Pad(img, (10,20), mode=\'edge\')\n        >>> transformed_img = Pad(img, (10,20,30,40), mode=\'reflect\')\n    """"""\n\n    def __init__(self, padding, fill=0, padding_mode=""constant""):\n        self.padding = padding\n        self.fill = fill\n        self.padding_mode = padding_mode\n\n    def __call__(self, img, bboxes, labels):\n        return (\n            F.pad(img, self.padding, self.fill, self.padding_mode),\n            F.bbox_pad(bboxes, self.padding),\n            labels,\n        )\n\n\nclass CenterCrop(object):\n    """"""crop image\n    \n    Args:\n        img {ndarray}: input image\n        output_size {number or sequence}: the output image size. if sequence, should be [height, width]\n    \n    Raises:\n        ValueError: the input image is large than original image.\n    \n    Returns:\n        ndarray: return croped ndarray image.\n    """"""\n\n    def __init__(self, out_size):\n        self.out_size = out_size\n        if isinstance(self.out_size, numbers.Number):\n            self.out_size = (int(self.out_size), int(self.out_size))\n\n    def __call__(self, img, bboxes, labels):\n        if img.ndim == 2:\n            img_height, img_width = img.shape\n        else:\n            img_height, img_width, _ = img.shape\n\n        if self.out_size[0] > img_height or self.out_size[1] > img_width:\n            raise ValueError(\n                ""the self.out_size should not greater than image size, but got {}"".format(\n                    self.out_size\n                )\n            )\n\n        target_height, target_width = self.out_size\n\n        top = int(round((img_height - target_height) / 2))\n        left = int(round((img_width - target_width) / 2))\n\n        bboxes = F.bbox_crop(bboxes, top, left, target_height, target_width)\n\n        # find the outside boxes and remove them\n        x_check = bboxes[..., 0] == bboxes[..., 2]  # x direction(width)\n        y_check = bboxes[..., 1] == bboxes[..., 3]  # y direction(height)\n        bboxes = bboxes[~(x_check | y_check)]\n        labels = np.array(labels)[~(x_check | y_check)].tolist()\n\n        return F.center_crop(img, self.out_size), bboxes, labels\n\n\nclass RandomCrop(object):\n    """"""random crop the input ndarray image\n    \n    Args:\n        size (int, sequence): th output image size, if sequeue size should be [height, width]\n    \n    Returns:\n        ndarray:  return random croped ndarray image.\n    """"""\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (size, size)\n        else:\n            self.size = size\n\n    def __call__(self, img, bboxes, labels):\n        h, w = img.shape[0:2]\n        th, tw = self.size\n        if w == tw and h == tw:\n            return img\n\n        top = random.randint(0, h - th)\n        left = random.randint(0, w - tw)\n\n        bboxes = F.bbox_crop(bboxes, top, left, th, tw)\n\n        # find the outside boxes and remove them\n        x_check = bboxes[..., 0] == bboxes[..., 2]  # x direction(width)\n        y_check = bboxes[..., 1] == bboxes[..., 3]  # y direction(height)\n        bboxes = bboxes[~(x_check | y_check)]\n        labels = np.array(labels)[~(x_check | y_check)].tolist()\n\n        return F.crop(img, top, left, th, tw), bboxes, labels\n\n\nclass RandomHorizontalFlip(object):\n    """"""Flip the input image on central horizon line.\n    \n    Args:\n        p (float): probability apply the horizon flip.(default: 0.5)\n    \n    Returns:\n        ndarray: return the flipped image.\n    """"""\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img, bboxes, labels):\n        if random.random() < self.p:\n            return F.hflip(img), F.bbox_hflip(bboxes, img.shape[1]), labels\n        return img, bboxes, labels\n\n    def __repr__(self):\n        return self.__class__.__name__ + ""(p={})"".format(self.p)\n\n\nclass RandomVerticalFlip(object):\n    """"""Flip the input image on central vertical line.\n    \n    Args:\n        p (float): probability apply the vertical flip. (default: 0.5)\n    \n    Returns:\n        ndarray: return the flipped image.\n    """"""\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img, bboxes, labels):\n        if random.random() < self.p:\n            return F.vflip(img), F.bbox_vflip(bboxes, img.shape[0]), labels\n        return img, bboxes, labels\n\n    def __repr__(self):\n        return self.__class__.__name__ + ""(p={})"".format(self.p)\n\n\nclass RandomFlip(object):\n    """"""Flip the input image vertical or horizon.\n    \n    Args:\n        p (float): probability apply flip. (default: 0.5)\n    \n    Returns:\n        ndarray: return the flipped image.\n    """"""\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img, bboxes, labels):\n        img_height, img_width = img.shape[0], img.shape[1]\n        if random.random() < self.p:\n            flip_code = random.randint(0, 1)\n            flipped_bboxes = (\n                F.bbox_vflip(bboxes, img_height)\n                if flip_code == 0\n                else F.bbox_hflip(bboxes, img_width)\n            )\n            return F.flip(img, flip_code), flipped_bboxes, labels\n        return img, bboxes, labels\n\n    def __repr__(self):\n        return self.__class__.__name__ + ""(p={})"".format(self.p)\n\n\nclass RandomResizedCrop(object):\n    """"""[summary]\n    \n    Args:\n        object ([type]): [description]\n    \n    Returns:\n        [type]: [description]\n    """"""\n\n    def __init__(self, crop_size, target_size, interpolation=Image.BILINEAR):\n        if isinstance(crop_size, numbers.Number):\n            self.crop_size = (crop_size, crop_size)\n        else:\n            self.crop_size = crop_size\n        self.target_size = target_size\n        self.interpolation = interpolation\n\n    def __call__(self, img, bboxes, labels):\n        h, w = img.shape[0:2]\n        th, tw = self.crop_size\n        if w == tw and h == tw:\n            return img, bboxes, labels\n\n        top = random.randint(0, h - th)\n        left = random.randint(0, w - tw)\n\n        img = F.crop(img, top, left, th, tw)\n        img = F.resize(img, self.target_size, interpolation=self.interpolation)\n\n        bboxes = F.bbox_crop(bboxes, top, left, th, tw)\n        # find the outside boxes and remove them\n        x_check = bboxes[..., 0] == bboxes[..., 2]  # x direction(width)\n        y_check = bboxes[..., 1] == bboxes[..., 3]  # y direction(height)\n        bboxes = bboxes[~(x_check | y_check)]\n        labels = np.array(labels)[~(x_check | y_check)].tolist()\n\n        bboxes = F.bbox_resize(bboxes, (th, tw), self.target_size)\n\n        return img, bboxes, labels\n\n\n# class ElasticTransform(object):\n#     """"""\n#     code modify from https://github.com/albu/albumentations.\n#     Elastic deformation of images as described in [Simard2003]_ (with modifications).\n#     Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5\n#     .. [Simard2003] Simard, Steinkraus and Platt, ""Best Practices for\n#          Convolutional Neural Networks applied to Visual Document Analysis"", in\n#          Proc. of the International Conference on Document Analysis and\n#          Recognition, 2003.\n#     Args:\n#         approximate (boolean): Whether to smooth displacement map with fixed kernel size.\n#                                Enabling this option gives ~2X speedup on large images.\n#     Image types:\n#         uint8, uint16 float32\n#     """"""\n\n#     def __init__(self, alpha=1, sigma=50, alpha_affine=50, interpolation=cv2.INTER_LINEAR,\n#                  border_mode=cv2.BORDER_REFLECT_101, random_state=None, approximate=False):\n#         self.alpha = alpha\n#         self.alpha_affine = alpha_affine\n#         self.sigma = sigma\n#         self.interpolation = interpolation\n#         self.border_mode = border_mode\n#         self.random_state = random_state\n#         self.approximate = approximate\n\n#     def __call__(self, img):\n#         return F.elastic_transform(img, self.alpha, self.sigma, self.alpha_affine, self.interpolation,\n#                                    self.border_mode, np.random.RandomState(self.random_state),\n#                                    self.approximate)\n'"
torchsat/transforms/transforms_seg.py,1,"b'import collections\nimport numbers\nimport random\n\nfrom PIL import Image\nimport numpy as np\nimport cv2\nimport torch\n\nfrom . import functional as F\n\n__all__ = [\n    ""Compose"",\n    ""Lambda"",\n    ""ToTensor"",\n    ""Normalize"",\n    ""ToGray"",\n    ""GaussianBlur"",\n    ""RandomNoise"",\n    ""RandomBrightness"",\n    ""RandomContrast"",\n    ""RandomShift"",\n    ""RandomRotation"",\n    ""Resize"",\n    ""Pad"",\n    ""CenterCrop"",\n    ""RandomCrop"",\n    ""RandomHorizontalFlip"",\n    ""RandomVerticalFlip"",\n    ""RandomFlip"",\n    ""RandomResizedCrop"",\n    ""ElasticTransform"",\n]\n\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img, mask):\n        for t in self.transforms:\n            img, mask = t(img, mask)\n        return img, mask\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + ""(""\n        for t in self.transforms:\n            format_string += ""\\n""\n            format_string += ""    {0}"".format(t)\n        format_string += ""\\n)""\n        return format_string\n\n\nclass Lambda(object):\n    def __init__(self, lambd):\n        self.lambd = lambd\n\n    def __call__(self, img, mask):\n        return self.lambd(img, mask)\n\n    def __repr__(self):\n        return self.__class__.__namme + ""()""\n\n\nclass ToTensor(object):\n    def __call__(self, img, mask):\n\n        return F.to_tensor(img), torch.tensor(mask, dtype=torch.long)\n\n\nclass Normalize(object):\n    def __init__(self, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], inplace=False):\n        self.mean = mean\n        self.std = std\n        self.inplace = inplace\n\n    def __call__(self, tensor, mask):\n        return F.normalize(tensor, self.mean, self.std, self.inplace), mask\n\n\nclass ToPILImage(object):\n    # TODO\n    pass\n\n\nclass ToGray(object):\n    """"""Convert the image to grayscale\n    \n    Args:\n        output_channels (int): number of channels desired for output image. (default: 1)\n    \n    Returns:\n        [ndarray]: the graysacle version of input\n        - If output_channels=1 : returned single channel image (height, width)\n        - If output_channels>1 : returned multi-channels ndarray image (height, width, channels)\n    """"""\n\n    def __init__(self, output_channels=1):\n        self.output_channels = output_channels\n\n    def __call__(self, img, mask):\n        return F.to_grayscale(img, self.output_channels), mask\n\n\nclass RandomNoise(object):\n    def __init__(self, mode=""gaussian""):\n        if mode not in [""gaussian"", ""salt"", ""pepper""]:\n            raise ValueError(\n                ""mode should be gaussian, salt, pepper, but got {}"".format(mode)\n            )\n\n    def __call__(self, img, mask):\n        return F.noise(img, self.mode), mask\n\n\nclass GaussianBlur(object):\n    def __init__(self, kernel_size=3):\n        self.kernel_size = kernel_size\n\n    def __call__(self, img, mask):\n        return F.gaussian_blur(img, self.kernel_size), mask\n\n\nclass RandomNoise(object):\n    """"""Add noise to the input ndarray image.\n    Args:\n        mode (str): the noise mode, should be one of ``gaussian``, ``salt``, ``pepper``, ``s&p``, (default: gaussian).\n        percent (float): noise percent, only work for ``salt``, ``pepper``, ``s&p`` mode. (default: 0.02)\n    \n    Returns:\n        ndarray: noised ndarray image.\n    """"""\n\n    def __init__(self, mode=""gaussian"", percent=0.02):\n        if mode not in [""gaussian"", ""salt"", ""pepper"", ""s&p""]:\n            raise ValueError(\n                ""mode should be gaussian, salt, pepper, but got {}"".format(mode)\n            )\n        self.mode = mode\n        self.percent = percent\n\n    def __call__(self, img, mask):\n        return F.noise(img, self.mode, self.percent), mask\n\n\nclass RandomBrightness(object):\n    def __init__(self, max_value=0):\n        if isinstance(max_value, numbers.Number):\n            self.value = random.uniform(-max_value, max_value)\n        if isinstance(max_value, collections.Iterable) and len(max_value) == 2:\n            self.value = random.uniform(max_value[0], max_value[1])\n\n    def __call__(self, img, mask):\n        return F.adjust_brightness(img, self.value), mask\n\n\nclass RandomContrast(object):\n    def __init__(self, max_factor=0):\n        if isinstance(max_factor, numbers.Number):\n            self.factor = random.uniform(-max_factor, max_factor)\n        if isinstance(max_factor, collections.Iterable) and len(max_factor) == 2:\n            self.factor = random.uniform(max_factor[0], max_factor[1])\n\n    def __call__(self, img, mask):\n        return F.adjust_contrast(img, self.factor), mask\n\n\nclass RandomShift(object):\n    """"""random shift the ndarray with value or some percent.\n    \n    Args:\n        max_percent (float): shift percent of the image.\n    \n    Returns:\n        ndarray: return the shifted ndarray image.\n    """"""\n\n    def __init__(self, max_percent=0.4):\n        self.max_percent = max_percent\n\n    def __call__(self, img, mask):\n        height, width = img.shape[0:2]\n        max_top = int(height * self.max_percent)\n        max_left = int(width * self.max_percent)\n        top = random.randint(-max_top, max_top)\n        left = random.randint(-max_left, max_left)\n\n        return F.shift(img, top, left), F.shift(mask, top, left)\n\n\nclass RandomRotation(object):\n    """"""random rotate the ndarray image with the degrees.\n    \n    Args:\n        degrees (number or sequence): the rotate degree.\n                                  If single number, it must be positive.\n                                  if squeence, it\'s length must 2 and first number should small than the second one.\n    \n    Raises:\n        ValueError: If degrees is a single number, it must be positive.\n        ValueError: If degrees is a sequence, it must be of len 2.\n    \n    Returns:\n        ndarray: return rotated ndarray image.\n    """"""\n\n    def __init__(self, degrees, center=None):\n        if isinstance(degrees, numbers.Number):\n            if degrees < 0:\n                raise ValueError(""If degrees is a single number, it must be positive."")\n            self.degrees = (-degrees, degrees)\n        else:\n            if len(degrees) != 2:\n                raise ValueError(""If degrees is a sequence, it must be of len 2."")\n            self.degrees = degrees\n        self.center = center\n\n    def __call__(self, img, mask):\n        angle = random.uniform(self.degrees[0], self.degrees[1])\n        return F.rotate(img, angle, self.center), F.rotate(mask, angle, self.center)\n\n\nclass Resize(object):\n    """"""resize the image\n    Args:\n        img {ndarray} : the input ndarray image\n        size {int, iterable} : the target size, if size is intger,  width and height will be resized to same \\\n                                otherwise, the size should be tuple (height, width) or list [height, width]\n                                \n    \n    Keyword Arguments:\n        interpolation {Image} : the interpolation method (default: {Image.BILINEAR})\n    \n    Raises:\n        TypeError : img should be ndarray\n        ValueError : size should be intger or iterable vaiable and length should be 2.\n    \n    Returns:\n        img (ndarray) : resize ndarray image\n    """"""\n\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img, mask):\n        return (\n            F.resize(img, self.size, self.interpolation),\n            F.resize(mask, self.size, Image.NEAREST),\n        )\n\n\nclass Pad(object):\n    """"""Pad the given ndarray image with padding width.\n    Args:\n        padding : {int, sequence}, padding width \n                  If int, each border same.\n                  If sequence length is 2, this is the padding for left/right and top/bottom.\n                  If sequence length is 4, this is the padding for left, top, right, bottom.\n        fill: {int, sequence}: Pixel\n        padding_mode: str or function. contain{\xe2\x80\x98constant\xe2\x80\x99,\xe2\x80\x98edge\xe2\x80\x99,\xe2\x80\x98linear_ramp\xe2\x80\x99,\xe2\x80\x98maximum\xe2\x80\x99,\xe2\x80\x98mean\xe2\x80\x99\n            , \xe2\x80\x98median\xe2\x80\x99, \xe2\x80\x98minimum\xe2\x80\x99, \xe2\x80\x98reflect\xe2\x80\x99,\xe2\x80\x98symmetric\xe2\x80\x99,\xe2\x80\x98wrap\xe2\x80\x99} (default: constant)\n    Examples:\n        >>> transformed_img = Pad(img, 20, mode=\'reflect\')\n        >>> transformed_img = Pad(img, (10,20), mode=\'edge\')\n        >>> transformed_img = Pad(img, (10,20,30,40), mode=\'reflect\')\n    """"""\n\n    def __init__(self, padding, fill=0, padding_mode=""constant""):\n        self.padding = padding\n        self.fill = fill\n        self.padding_mode = padding_mode\n\n    def __call__(self, img, mask):\n        img = F.pad(img, self.padding, self.fill, self.padding_mode)\n        if self.padding_mode == ""reflect"":\n            return img, F.pad(mask, self.padding, 0, self.padding_mode)\n        else:\n            return img, F.pad(mask, self.padding, 0, ""constant"")\n\n\nclass CenterCrop(object):\n    """"""crop image\n    \n    Args:\n        img {ndarray}: input image\n        output_size {number or sequence}: the output image size. if sequence, should be [height, width]\n    \n    Raises:\n        ValueError: the input image is large than original image.\n    \n    Returns:\n        ndarray: return croped ndarray image.\n    """"""\n\n    def __init__(self, out_size):\n        self.out_size = out_size\n\n    def __call__(self, img, mask):\n        return F.center_crop(img, self.out_size), F.center_crop(mask, self.out_size)\n\n\nclass RandomCrop(object):\n    """"""random crop the input ndarray image\n    \n    Args:\n        size (int, sequence): th output image size, if sequeue size should be [height, width]\n    \n    Returns:\n        ndarray:  return random croped ndarray image.\n    """"""\n\n    def __init__(self, size):\n        if isinstance(size, numbers.Number):\n            self.size = (size, size)\n        else:\n            self.size = size\n\n    def __call__(self, img, mask):\n        h, w = img.shape[0:2]\n        th, tw = self.size\n        if w == tw and h == tw:\n            return img\n\n        top = random.randint(0, h - th)\n        left = random.randint(0, w - tw)\n\n        return F.crop(img, top, left, th, tw), F.crop(mask, top, left, th, tw)\n\n\nclass RandomHorizontalFlip(object):\n    """"""Flip the input image on central horizon line.\n    \n    Args:\n        p (float): probability apply the horizon flip.(default: 0.5)\n    \n    Returns:\n        ndarray: return the flipped image.\n    """"""\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img, mask):\n        if random.random() < self.p:\n            return F.hflip(img), F.hflip(mask)\n        return img, mask\n\n    def __repr__(self):\n        return self.__class__.__name__ + ""(p={})"".format(self.p)\n\n\nclass RandomVerticalFlip(object):\n    """"""Flip the input image on central vertical line.\n    \n    Args:\n        p (float): probability apply the vertical flip. (default: 0.5)\n    \n    Returns:\n        ndarray: return the flipped image.\n    """"""\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img, mask):\n        if random.random() < self.p:\n            return F.vflip(img), F.vflip(mask)\n        return img, mask\n\n    def __repr__(self):\n        return self.__class__.__name__ + ""(p={})"".format(self.p)\n\n\nclass RandomFlip(object):\n    """"""Flip the input image vertical or horizon.\n    \n    Args:\n        p (float): probability apply flip. (default: 0.5)\n    \n    Returns:\n        ndarray: return the flipped image.\n    """"""\n\n    def __init__(self, p=0.5):\n        self.p = p\n\n    def __call__(self, img, mask):\n        if random.random() < self.p:\n            flip_code = random.randint(0, 1)\n            return F.flip(img, flip_code), F.flip(mask, flip_code)\n        return img, mask\n\n    def __repr__(self):\n        return self.__class__.__name__ + ""(p={})"".format(self.p)\n\n\nclass RandomResizedCrop(object):\n    """"""[summary]\n    \n    Args:\n        object ([type]): [description]\n    \n    Returns:\n        [type]: [description]\n    """"""\n\n    def __init__(self, crop_size, target_size, interpolation=Image.BILINEAR):\n        if isinstance(crop_size, numbers.Number):\n            self.crop_size = (crop_size, crop_size)\n        else:\n            self.crop_size = crop_size\n        self.target_size = target_size\n        self.interpolation = interpolation\n\n    def __call__(self, img, mask):\n        h, w = img.shape[0:2]\n        th, tw = self.crop_size\n        if w == tw and h == tw:\n            return img\n\n        top = random.randint(0, h - th)\n        left = random.randint(0, w - tw)\n\n        img = F.crop(img, top, left, th, tw)\n        img = F.resize(img, self.target_size, interpolation=self.interpolation)\n        mask = F.crop(mask, top, left, th, tw)\n        mask = F.resize(mask, self.target_size, interpolation=Image.NEAREST)\n\n        return img, mask\n\n\nclass ElasticTransform(object):\n    """"""\n    code modify from https://github.com/albu/albumentations.  \n    Elastic deformation of images as described in [Simard2003]_ (with modifications).\n    Based on https://gist.github.com/erniejunior/601cdf56d2b424757de5\n    .. [Simard2003] Simard, Steinkraus and Platt, ""Best Practices for\n         Convolutional Neural Networks applied to Visual Document Analysis"", in\n         Proc. of the International Conference on Document Analysis and\n         Recognition, 2003.\n    Args:\n        approximate (boolean): Whether to smooth displacement map with fixed kernel size.\n                               Enabling this option gives ~2X speedup on large images.\n    Image types:\n        uint8, uint16 float32\n    """"""\n\n    def __init__(\n        self,\n        alpha=1,\n        sigma=50,\n        alpha_affine=50,\n        interpolation=cv2.INTER_LINEAR,\n        border_mode=cv2.BORDER_REFLECT_101,\n        random_state=None,\n        approximate=False,\n    ):\n        self.alpha = alpha\n        self.alpha_affine = alpha_affine\n        self.sigma = sigma\n        self.interpolation = interpolation\n        self.border_mode = border_mode\n        self.random_state = random_state\n        self.approximate = approximate\n\n    def __call__(self, img, mask):\n        return (\n            F.elastic_transform(\n                img,\n                self.alpha,\n                self.sigma,\n                self.alpha_affine,\n                self.interpolation,\n                self.border_mode,\n                np.random.RandomState(self.random_state),\n                self.approximate,\n            ),\n            F.elastic_transform(\n                mask,\n                self.alpha,\n                self.sigma,\n                self.alpha_affine,\n                cv2.INTER_NEAREST,\n                self.border_mode,\n                np.random.RandomState(self.random_state),\n                self.approximate,\n            ),\n        )\n\n'"
torchsat/utils/__init__.py,0,b''
torchsat/utils/visualizer.py,0,"b'import matplotlib.pyplot as plt\nimport tifffile\nimport numpy as np\n\n__all__ = [""plot_img"", ""plot_bbox"", ""plot_mask""]\n\ncolor_map = {\n    1: ""tab:blue"",\n    2: ""tab:orange"",\n    3: ""tab:green"",\n    4: ""tab:red"",\n    5: ""tab:purple"",\n    6: ""tab:brown"",\n    7: ""tab:pink"",\n    8: ""tab:olive"",\n    9: ""tab:cyan"",\n}\n\n\ndef plot_img(img, channels=(1, 2, 3), **kwargs):\n    """"""plot the ndarray images\n    \n    Args:\n        img (ndarray): the ndarray image.\n        channels (tuple, optional): target channel to show. Defaults to (1,2,3).\n        kwargs (dict, optional): the plt.imshow kwargs.\n    """"""\n    plt.figure(figsize=(10, 8))\n    channels = channels if isinstance(channels, int) else [x - 1 for x in channels]\n    if img.ndim == 2:\n        tifffile.imshow(img, cmap=""gray"", **kwargs)\n    else:\n        tifffile.imshow(img[..., channels], **kwargs)\n\n\ndef plot_bbox(img, bboxes, channels=(1, 2, 3), labels=None, classes=None, **kwargs):\n    """"""plot the image with bboxes\n    \n    Args:\n        img (ndarray): the ndarray image.\n        bboxes (ndarray): the bboxes to show\n        channels (tuple, optional): target channel to show. Defaults to (1,2,3).\n        labels ([type], optional): the label id corresponding to the bbox . Defaults to None.\n        classes ([type], optional): the text name corresponding to the label id. Defaults to None.\n        kwargs (dict, optional): the plt.imshow kwargs.\n    Examples:\n        >>> import numpy as np\n        >>> from PIL import Image\n        >>> fp = \'./tests/fixtures/different-types/jpeg_3channel_uint8.jpeg\'\n        >>> img = np.array(Image.open(fp))\n        >>> bboxes = np.array([[  0.,   2., 100., 100.],\n                             [140., 300., 200., 350.]])\n        >>> plot_bbox(img, bboxes, labels=[1,4], classes=[\'car\', \'building\', \'tree\', \'road\'])\n    """"""\n    plt.figure(figsize=(10, 8))\n    channels = channels if isinstance(channels, int) else [x - 1 for x in channels]\n    if img.ndim == 2:\n        plt.imshow(img, cmap=""gray"", **kwargs)\n    else:\n        plt.imshow(img[..., channels], **kwargs)\n\n    if labels is None:\n        labels = [1] * len(bboxes)\n\n    for bbox, label in zip(bboxes.tolist(), labels):\n        rect = plt.Rectangle(\n            xy=(bbox[0], bbox[1]),\n            width=bbox[2] - bbox[0],\n            height=bbox[3] - bbox[1],\n            fill=False,\n            edgecolor=color_map[label],\n            linewidth=2,\n        )\n        plt.gca().add_patch(rect)\n        if classes is not None:\n            text_bbox = dict(\n                fc=color_map[label], alpha=0.5, lw=0, boxstyle=""Square, pad=0""\n            )\n            plt.gca().annotate(\n                classes[label - 1], xy=(bbox[0], bbox[1]), bbox=text_bbox, color=""w""\n            )\n\n    plt.show()\n\n\ndef plot_mask(img, mask, channels=(1, 2, 3), mask_alpha=0.6, **kwargs):\n    """"""plot image with mask\n    \n    Args:\n        img (ndarray): the input ndarray image.\n        mask (ndarray): the mask to show.\n        channels (tuple, optional): target channel to show.. Defaults to (1,2,3).\n        mask_alpha (float, optional): mask alpha. Defaults to 0.6.\n        kwargs (dict, optional): the plt.imshow kwargs.\n    """"""\n    plt.figure(figsize=(10, 8))\n    channels = channels if isinstance(channels, int) else [x - 1 for x in channels]\n    if img.ndim == 2:\n        plt.imshow(img, cmap=""gray"", **kwargs)\n    else:\n        plt.imshow(img[..., channels], **kwargs)\n    masked_data = np.ma.masked_where(mask == 0, mask)\n    plt.imshow(masked_data, alpha=mask_alpha, cmap=""viridis"")\n'"
torchsat/models/classification/__init__.py,0,b'from .densenet import *\nfrom .inception import *\nfrom .mobilenet import *\nfrom .resnet import *\nfrom .vgg import *\n'
torchsat/models/classification/densenet.py,12,"b'import re\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.checkpoint as cp\nfrom collections import OrderedDict\nfrom torch.hub import load_state_dict_from_url\n\n__all__ = [\'DenseNet\', \'densenet121\', \'densenet169\', \'densenet201\', \'densenet161\']\n\nmodel_urls = {\n    \'densenet121\': \'https://download.pytorch.org/models/densenet121-a639ec97.pth\',\n    \'densenet169\': \'https://download.pytorch.org/models/densenet169-b2777c0a.pth\',\n    \'densenet201\': \'https://download.pytorch.org/models/densenet201-c1103571.pth\',\n    \'densenet161\': \'https://download.pytorch.org/models/densenet161-8d451a50.pth\',\n}\n\n\ndef _bn_function_factory(norm, relu, conv):\n    def bn_function(*inputs):\n        concated_features = torch.cat(inputs, 1)\n        bottleneck_output = conv(relu(norm(concated_features)))\n        return bottleneck_output\n\n    return bn_function\n\n\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate, memory_efficient=False):\n        super(_DenseLayer, self).__init__()\n        self.add_module(\'norm1\', nn.BatchNorm2d(num_input_features)),\n        self.add_module(\'relu1\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv1\', nn.Conv2d(num_input_features, bn_size *\n                                           growth_rate, kernel_size=1, stride=1,\n                                           bias=False)),\n        self.add_module(\'norm2\', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module(\'relu2\', nn.ReLU(inplace=True)),\n        self.add_module(\'conv2\', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                                           kernel_size=3, stride=1, padding=1,\n                                           bias=False)),\n        self.drop_rate = drop_rate\n        self.memory_efficient = memory_efficient\n\n    def forward(self, *prev_features):\n        bn_function = _bn_function_factory(self.norm1, self.relu1, self.conv1)\n        if self.memory_efficient and any(prev_feature.requires_grad for prev_feature in prev_features):\n            bottleneck_output = cp.checkpoint(bn_function, *prev_features)\n        else:\n            bottleneck_output = bn_function(*prev_features)\n        new_features = self.conv2(self.relu2(self.norm2(bottleneck_output)))\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate,\n                                     training=self.training)\n        return new_features\n\n\nclass _DenseBlock(nn.Module):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate, memory_efficient=False):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(\n                num_input_features + i * growth_rate,\n                growth_rate=growth_rate,\n                bn_size=bn_size,\n                drop_rate=drop_rate,\n                memory_efficient=memory_efficient,\n            )\n            self.add_module(\'denselayer%d\' % (i + 1), layer)\n\n    def forward(self, init_features):\n        features = [init_features]\n        for name, layer in self.named_children():\n            new_features = layer(*features)\n            features.append(new_features)\n        return torch.cat(features, 1)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module(\'norm\', nn.BatchNorm2d(num_input_features))\n        self.add_module(\'relu\', nn.ReLU(inplace=True))\n        self.add_module(\'conv\', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module(\'pool\', nn.AvgPool2d(kernel_size=2, stride=2))\n\n\nclass DenseNet(nn.Module):\n    r""""""Densenet-BC model class, based on\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n          but slower. Default: *False*. See `""paper"" <https://arxiv.org/pdf/1707.06990.pdf>`_\n    """"""\n\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n                 num_init_features=64, bn_size=4, drop_rate=0,\n                 num_classes=1000, in_channels=3, memory_efficient=False):\n\n        super(DenseNet, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            (\'conv0\', nn.Conv2d(in_channels, num_init_features, kernel_size=7, stride=2,\n                                padding=3, bias=False)),\n            (\'norm0\', nn.BatchNorm2d(num_init_features)),\n            (\'relu0\', nn.ReLU(inplace=True)),\n            (\'pool0\', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(\n                num_layers=num_layers,\n                num_input_features=num_features,\n                bn_size=bn_size,\n                growth_rate=growth_rate,\n                drop_rate=drop_rate,\n                memory_efficient=memory_efficient\n            )\n            self.features.add_module(\'denseblock%d\' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features,\n                                    num_output_features=num_features // 2)\n                self.features.add_module(\'transition%d\' % (i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module(\'norm5\', nn.BatchNorm2d(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.adaptive_avg_pool2d(out, (1, 1))\n        out = torch.flatten(out, 1)\n        out = self.classifier(out)\n        return out\n\n\ndef _load_state_dict(model, model_url, progress):\n    # \'.\'s are no longer allowed in module names, but previous _DenseLayer\n    # has keys \'norm.1\', \'relu.1\', \'conv.1\', \'norm.2\', \'relu.2\', \'conv.2\'.\n    # They are also in the checkpoints in model_urls. This pattern is used\n    # to find such keys.\n    pattern = re.compile(\n        r\'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$\')\n\n    state_dict = load_state_dict_from_url(model_url, progress=progress)\n    for key in list(state_dict.keys()):\n        res = pattern.match(key)\n        if res:\n            new_key = res.group(1) + res.group(2)\n            state_dict[new_key] = state_dict[key]\n            del state_dict[key]\n    model.load_state_dict(state_dict)\n\n\ndef _densenet(arch, growth_rate, block_config, num_init_features, pretrained, progress,\n              num_classes, in_channels, **kwargs):\n    # if pretrained and in_channels != 3:\n    #     raise ValueError(\'ImageNet pretrained models only support 3 input channels, but got {}\'.format(in_channels))\n\n    if pretrained:\n        model = DenseNet(growth_rate, block_config, num_init_features, **kwargs)\n        _load_state_dict(model, model_urls[arch], progress)\n        conv0 = model.features.conv0\n        model.features.conv0 = nn.Conv2d(in_channels=in_channels,\n                                         out_channels=conv0.out_channels,\n                                         kernel_size=conv0.kernel_size,\n                                         stride=conv0.stride,\n                                         padding=conv0.padding,\n                                         bias=conv0.bias)\n\n        if in_channels <= 3:\n            model.features.conv0.weight.data = conv0.weight[:, 0:in_channels, :, :]\n        else:\n            multi = in_channels // 3\n            last = in_channels % 3\n            model.features.conv0.weight.data = torch.cat([conv0.weight for x in range(multi)], dim=1)\n            model.features.conv0.weight.data = conv0.weight[:, :last, :, :]\n        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n    else:\n        model = DenseNet(growth_rate, block_config, num_init_features,\n                         num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return model\n\n\ndef densenet121(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""Densenet-121 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n          but slower. Default: *False*. See `""paper"" <https://arxiv.org/pdf/1707.06990.pdf>`_\n    """"""\n    return _densenet(\'densenet121\', 32, (6, 12, 24, 16), 64, pretrained, progress,\n                     num_classes, in_channels, **kwargs)\n\n\ndef densenet161(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""Densenet-161 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n          but slower. Default: *False*. See `""paper"" <https://arxiv.org/pdf/1707.06990.pdf>`_\n    """"""\n    return _densenet(\'densenet161\', 48, (6, 12, 36, 24), 96, pretrained, progress,\n                     num_classes, in_channels, **kwargs)\n\n\ndef densenet169(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""Densenet-169 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n          but slower. Default: *False*. See `""paper"" <https://arxiv.org/pdf/1707.06990.pdf>`_\n    """"""\n    return _densenet(\'densenet169\', 32, (6, 12, 32, 32), 64, pretrained, progress,\n                     num_classes, in_channels, **kwargs)\n\n\ndef densenet201(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""Densenet-201 model from\n    `""Densely Connected Convolutional Networks"" <https://arxiv.org/pdf/1608.06993.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n        memory_efficient (bool) - If True, uses checkpointing. Much more memory efficient,\n          but slower. Default: *False*. See `""paper"" <https://arxiv.org/pdf/1707.06990.pdf>`_\n    """"""\n    return _densenet(\'densenet201\', 32, (6, 12, 48, 32), 64, pretrained, progress,\n                     num_classes, in_channels, **kwargs)'"
torchsat/models/classification/inception.py,20,"b'from collections import namedtuple\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.hub import load_state_dict_from_url\n\n\n__all__ = [\'Inception3\', \'inception_v3\']\n\n\nmodel_urls = {\n    # Inception v3 ported from TensorFlow\n    \'inception_v3_google\': \'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\',\n}\n\n_InceptionOutputs = namedtuple(\'InceptionOutputs\', [\'logits\', \'aux_logits\'])\n\n\ndef inception_v3(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""Inception v3 model architecture from\n    `""Rethinking the Inception Architecture for Computer Vision"" <http://arxiv.org/abs/1512.00567>`_.\n    .. note::\n        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n        aux_logits (bool): If True, add an auxiliary branch that can improve training.\n            Default: *True*\n        transform_input (bool): If True, preprocesses the input according to the method with which it\n            was trained on ImageNet. Default: *False*\n    """"""\n    # if pretrained and in_channels != 3:\n    #     raise ValueError(\'ImageNet pretrained models only support 3 input channels, but got {}\'.format(in_channels))\n    \n    if pretrained:\n        if \'transform_input\' not in kwargs:\n            kwargs[\'transform_input\'] = True\n        if \'aux_logits\' in kwargs:\n            original_aux_logits = kwargs[\'aux_logits\']\n            kwargs[\'aux_logits\'] = True\n        else:\n            original_aux_logits = True\n        model = Inception3(**kwargs)\n        state_dict = load_state_dict_from_url(model_urls[\'inception_v3_google\'],\n                                              progress=progress)\n        model.load_state_dict(state_dict)\n        conv0 = model.Conv2d_1a_3x3.conv\n        model.Conv2d_1a_3x3.conv = nn.Conv2d(in_channels=in_channels,\n                        out_channels=conv0.out_channels,\n                        kernel_size=conv0.kernel_size,\n                        stride=conv0.stride,\n                        padding=conv0.padding,\n                        bias=conv0.bias)\n        if in_channels <= 3:\n            model.Conv2d_1a_3x3.conv.weight.data = conv0.weight[:,0:in_channels,:,:]\n        else:\n            multi = in_channels//3\n            last = in_channels%3\n            model.Conv2d_1a_3x3.conv.weight.data = torch.cat([conv0.weight for x in range(multi)], dim=1)\n            model.Conv2d_1a_3x3.conv.weight.data = conv0.weight[:,:last,:,:]\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n        if not original_aux_logits:\n            model.aux_logits = False\n            del model.AuxLogits\n    else:\n        model = Inception3(num_classes, in_channels=in_channels, **kwargs)\n\n    return model\n\n\nclass Inception3(nn.Module):\n\n    def __init__(self, num_classes=1000, in_channels=3, aux_logits=True, transform_input=False):\n        super(Inception3, self).__init__()\n        self.aux_logits = aux_logits\n        self.transform_input = transform_input\n        self.Conv2d_1a_3x3 = BasicConv2d(in_channels, 32, kernel_size=3, stride=2)\n        self.Conv2d_2a_3x3 = BasicConv2d(32, 32, kernel_size=3)\n        self.Conv2d_2b_3x3 = BasicConv2d(32, 64, kernel_size=3, padding=1)\n        self.Conv2d_3b_1x1 = BasicConv2d(64, 80, kernel_size=1)\n        self.Conv2d_4a_3x3 = BasicConv2d(80, 192, kernel_size=3)\n        self.Mixed_5b = InceptionA(192, pool_features=32)\n        self.Mixed_5c = InceptionA(256, pool_features=64)\n        self.Mixed_5d = InceptionA(288, pool_features=64)\n        self.Mixed_6a = InceptionB(288)\n        self.Mixed_6b = InceptionC(768, channels_7x7=128)\n        self.Mixed_6c = InceptionC(768, channels_7x7=160)\n        self.Mixed_6d = InceptionC(768, channels_7x7=160)\n        self.Mixed_6e = InceptionC(768, channels_7x7=192)\n        if aux_logits:\n            self.AuxLogits = InceptionAux(768, num_classes)\n        self.Mixed_7a = InceptionD(768)\n        self.Mixed_7b = InceptionE(1280)\n        self.Mixed_7c = InceptionE(2048)\n        self.fc = nn.Linear(2048, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                import scipy.stats as stats\n                stddev = m.stddev if hasattr(m, \'stddev\') else 0.1\n                X = stats.truncnorm(-2, 2, scale=stddev)\n                values = torch.as_tensor(X.rvs(m.weight.numel()), dtype=m.weight.dtype)\n                values = values.view(m.weight.size())\n                with torch.no_grad():\n                    m.weight.copy_(values)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        if self.transform_input:\n            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n        # N x 3 x 299 x 299\n        x = self.Conv2d_1a_3x3(x)\n        # N x 32 x 149 x 149\n        x = self.Conv2d_2a_3x3(x)\n        # N x 32 x 147 x 147\n        x = self.Conv2d_2b_3x3(x)\n        # N x 64 x 147 x 147\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # N x 64 x 73 x 73\n        x = self.Conv2d_3b_1x1(x)\n        # N x 80 x 73 x 73\n        x = self.Conv2d_4a_3x3(x)\n        # N x 192 x 71 x 71\n        x = F.max_pool2d(x, kernel_size=3, stride=2)\n        # N x 192 x 35 x 35\n        x = self.Mixed_5b(x)\n        # N x 256 x 35 x 35\n        x = self.Mixed_5c(x)\n        # N x 288 x 35 x 35\n        x = self.Mixed_5d(x)\n        # N x 288 x 35 x 35\n        x = self.Mixed_6a(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6b(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6c(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6d(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_6e(x)\n        # N x 768 x 17 x 17\n        if self.training and self.aux_logits:\n            aux = self.AuxLogits(x)\n        # N x 768 x 17 x 17\n        x = self.Mixed_7a(x)\n        # N x 1280 x 8 x 8\n        x = self.Mixed_7b(x)\n        # N x 2048 x 8 x 8\n        x = self.Mixed_7c(x)\n        # N x 2048 x 8 x 8\n        # Adaptive average pooling\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        # N x 2048 x 1 x 1\n        x = F.dropout(x, training=self.training)\n        # N x 2048 x 1 x 1\n        x = torch.flatten(x, 1)\n        # N x 2048\n        x = self.fc(x)\n        # N x 1000 (num_classes)\n        if self.training and self.aux_logits:\n            return _InceptionOutputs(x, aux)\n        return x\n\n\nclass InceptionA(nn.Module):\n\n    def __init__(self, in_channels, pool_features):\n        super(InceptionA, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 64, kernel_size=1)\n\n        self.branch5x5_1 = BasicConv2d(in_channels, 48, kernel_size=1)\n        self.branch5x5_2 = BasicConv2d(48, 64, kernel_size=5, padding=2)\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, padding=1)\n\n        self.branch_pool = BasicConv2d(in_channels, pool_features, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionB(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionB, self).__init__()\n        self.branch3x3 = BasicConv2d(in_channels, 384, kernel_size=3, stride=2)\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 64, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(64, 96, kernel_size=3, padding=1)\n        self.branch3x3dbl_3 = BasicConv2d(96, 96, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        branch3x3 = self.branch3x3(x)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n\n        outputs = [branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionC(nn.Module):\n\n    def __init__(self, in_channels, channels_7x7):\n        super(InceptionC, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 192, kernel_size=1)\n\n        c7 = channels_7x7\n        self.branch7x7_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n        self.branch7x7_2 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7_3 = BasicConv2d(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n\n        self.branch7x7dbl_1 = BasicConv2d(in_channels, c7, kernel_size=1)\n        self.branch7x7dbl_2 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_3 = BasicConv2d(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7dbl_4 = BasicConv2d(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7dbl_5 = BasicConv2d(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n\n        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch7x7 = self.branch7x7_1(x)\n        branch7x7 = self.branch7x7_2(branch7x7)\n        branch7x7 = self.branch7x7_3(branch7x7)\n\n        branch7x7dbl = self.branch7x7dbl_1(x)\n        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionD(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionD, self).__init__()\n        self.branch3x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n        self.branch3x3_2 = BasicConv2d(192, 320, kernel_size=3, stride=2)\n\n        self.branch7x7x3_1 = BasicConv2d(in_channels, 192, kernel_size=1)\n        self.branch7x7x3_2 = BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3))\n        self.branch7x7x3_3 = BasicConv2d(192, 192, kernel_size=(7, 1), padding=(3, 0))\n        self.branch7x7x3_4 = BasicConv2d(192, 192, kernel_size=3, stride=2)\n\n    def forward(self, x):\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = self.branch3x3_2(branch3x3)\n\n        branch7x7x3 = self.branch7x7x3_1(x)\n        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n        outputs = [branch3x3, branch7x7x3, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionE(nn.Module):\n\n    def __init__(self, in_channels):\n        super(InceptionE, self).__init__()\n        self.branch1x1 = BasicConv2d(in_channels, 320, kernel_size=1)\n\n        self.branch3x3_1 = BasicConv2d(in_channels, 384, kernel_size=1)\n        self.branch3x3_2a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3_2b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch3x3dbl_1 = BasicConv2d(in_channels, 448, kernel_size=1)\n        self.branch3x3dbl_2 = BasicConv2d(448, 384, kernel_size=3, padding=1)\n        self.branch3x3dbl_3a = BasicConv2d(384, 384, kernel_size=(1, 3), padding=(0, 1))\n        self.branch3x3dbl_3b = BasicConv2d(384, 384, kernel_size=(3, 1), padding=(1, 0))\n\n        self.branch_pool = BasicConv2d(in_channels, 192, kernel_size=1)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass InceptionAux(nn.Module):\n\n    def __init__(self, in_channels, num_classes):\n        super(InceptionAux, self).__init__()\n        self.conv0 = BasicConv2d(in_channels, 128, kernel_size=1)\n        self.conv1 = BasicConv2d(128, 768, kernel_size=5)\n        self.conv1.stddev = 0.01\n        self.fc = nn.Linear(768, num_classes)\n        self.fc.stddev = 0.001\n\n    def forward(self, x):\n        # N x 768 x 17 x 17\n        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n        # N x 768 x 5 x 5\n        x = self.conv0(x)\n        # N x 128 x 5 x 5\n        x = self.conv1(x)\n        # N x 768 x 1 x 1\n        # Adaptive average pooling\n        x = F.adaptive_avg_pool2d(x, (1, 1))\n        # N x 768 x 1 x 1\n        x = torch.flatten(x, 1)\n        # N x 768\n        x = self.fc(x)\n        # N x 1000\n        return x\n\n\nclass BasicConv2d(nn.Module):\n\n    def __init__(self, in_channels, out_channels, **kwargs):\n        super(BasicConv2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return F.relu(x, inplace=True)'"
torchsat/models/classification/mobilenet.py,3,"b'import torch\nfrom torch import nn\nfrom torch.hub import load_state_dict_from_url\n\n\n__all__ = [\'MobileNetV2\', \'mobilenet_v2\']\n\n\nmodel_urls = {\n    \'mobilenet_v2\': \'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\',\n}\n\n\ndef _make_divisible(v, divisor, min_value=None):\n    """"""\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    :param v:\n    :param divisor:\n    :param min_value:\n    :return:\n    """"""\n    if min_value is None:\n        min_value = divisor\n    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n\n\nclass ConvBNReLU(nn.Sequential):\n    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n        padding = (kernel_size - 1) // 2\n        super(ConvBNReLU, self).__init__(\n            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n            nn.BatchNorm2d(out_planes),\n            nn.ReLU6(inplace=True)\n        )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = int(round(inp * expand_ratio))\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        layers = []\n        if expand_ratio != 1:\n            # pw\n            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n        layers.extend([\n            # dw\n            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n            # pw-linear\n            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n        ])\n        self.conv = nn.Sequential(*layers)\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, num_classes=1000, in_channels=3, width_mult=1.0, inverted_residual_setting=None, round_nearest=8):\n        """"""\n        MobileNet V2 main class\n        Args:\n            num_classes (int): Number of classes\n            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n            inverted_residual_setting: Network structure\n            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n            Set to 1 to turn off rounding\n        """"""\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        input_channel = 32\n        last_channel = 1280\n\n        if inverted_residual_setting is None:\n            inverted_residual_setting = [\n                # t, c, n, s\n                [1, 16, 1, 1],\n                [6, 24, 2, 2],\n                [6, 32, 3, 2],\n                [6, 64, 4, 2],\n                [6, 96, 3, 1],\n                [6, 160, 3, 2],\n                [6, 320, 1, 1],\n            ]\n\n        # only check the first element, assuming user knows t,c,n,s are required\n        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n            raise ValueError(""inverted_residual_setting should be non-empty ""\n                             ""or a 4-element list, got {}"".format(inverted_residual_setting))\n\n        # building first layer\n        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n        features = [ConvBNReLU(in_channels, input_channel, stride=2)]\n        # building inverted residual blocks\n        for t, c, n, s in inverted_residual_setting:\n            output_channel = _make_divisible(c * width_mult, round_nearest)\n            for i in range(n):\n                stride = s if i == 0 else 1\n                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, num_classes),\n        )\n\n        # weight initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.mean([2, 3])\n        x = self.classifier(x)\n        return x\n\n\ndef mobilenet_v2(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    """"""\n    Constructs a MobileNetV2 architecture from\n    `""MobileNetV2: Inverted Residuals and Linear Bottlenecks"" <https://arxiv.org/abs/1801.04381>`_.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    # if pretrained and in_channels != 3:\n    #     raise ValueError(\'ImageNet pretrained models only support 3 input channels, but got {}\'.format(in_channels))\n    \n    \n    if pretrained:\n        model = MobileNetV2(**kwargs)\n        state_dict = load_state_dict_from_url(model_urls[\'mobilenet_v2\'], progress=progress)\n        model.load_state_dict(state_dict)\n        conv0 = model.features[0][0]\n        model.features[0][0] = nn.Conv2d(in_channels=in_channels,\n                        out_channels=conv0.out_channels,\n                        kernel_size=conv0.kernel_size,\n                        stride=conv0.stride,\n                        padding=conv0.padding,\n                        bias=conv0.bias)\n        if in_channels <= 3:\n            model.features[0][0].weight.data = conv0.weight[:,0:in_channels,:,:]\n        else:\n            multi = in_channels//3\n            last = in_channels%3\n            model.features[0][0].weight.data = torch.cat([conv0.weight for x in range(multi)], dim=1)\n            model.features[0][0].weight.data = conv0.weight[:,:last,:,:]\n        model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)\n    else:\n        model = MobileNetV2(num_classes=num_classes,in_channels=in_channels, **kwargs)\n    return model'"
torchsat/models/classification/resnet.py,13,"b'import torch\nimport torch.nn as nn\nfrom torch.hub import load_state_dict_from_url\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\', \'resnext50_32x4d\', \'resnext101_32x8d\',\n           \'wide_resnet50_2\', \'wide_resnet101_2\']\n"""""" This script was taken from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py.\nAnd modified for muti-channel as inputs\n\n""""""\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n    \'resnext50_32x4d\': \'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\',\n    \'resnext101_32x8d\': \'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\',\n    \'wide_resnet50_2\': \'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\',\n    \'wide_resnet101_2\': \'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError(\'BasicBlock only supports groups=1 and base_width=64\')\n        if dilation > 1:\n            raise NotImplementedError(""Dilation > 1 not supported in BasicBlock"")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, in_channels=3, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(""replace_stride_with_dilation should be None ""\n                             ""or a 3-element tuple, got {}"".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(in_channels, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n\n        return x\n\n\ndef _resnet(arch, block, layers, pretrained, progress, num_classes, in_channels, **kwargs):\n    # if pretrained and in_channels != 3:\n        # raise ValueError(\'ImageNet pretrained models only support 3 input channels, but got {}\'.format(in_channels))\n    if pretrained:\n        model = ResNet(block, layers, **kwargs)\n        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n        model.load_state_dict(state_dict)\n        conv1 = model.conv1\n        model.conv1 = nn.Conv2d(in_channels=in_channels,\n                        out_channels=conv1.out_channels,\n                        kernel_size=conv1.kernel_size,\n                        stride=conv1.stride,\n                        padding=conv1.padding,\n                        bias=conv1.bias)\n\n        if in_channels <= 3:\n            model.conv1.weight.data = conv1.weight.data[:,0:in_channels,:,:]\n        else:\n            multi = in_channels//3\n            last = in_channels%3\n            model.conv1.weight.data = torch.cat([conv1.weight for x in range(multi)], dim=1)\n            model.conv1.weight.data = conv1.weight[:,:last,:,:]\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n    else:\n        model = ResNet(block, layers, num_classes=num_classes, in_channels=in_channels, **kwargs)\n    return model\n\n\ndef resnet18(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-18 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>\'_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet. \\\n                           If in_channels is greater than 3, it will copy the parameters of imagenet to fill in order.\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet18\', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n                   num_classes, in_channels, **kwargs)\n\n\ndef resnet34(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-34 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>\'_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet34\', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n                   num_classes, in_channels, **kwargs)\n\n\ndef resnet50(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-50 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>\'_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet50\', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n                   num_classes, in_channels, **kwargs)\n\n\ndef resnet101(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-101 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>\'_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet101\', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n                   num_classes, in_channels, **kwargs)\n\n\ndef resnet152(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-152 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>\'_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet152\', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n                   num_classes, in_channels, **kwargs)\n\n\ndef resnext50_32x4d(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""ResNeXt-50 32x4d model from\n    `""Aggregated Residual Transformation for Deep Neural Networks"" <https://arxiv.org/pdf/1611.05431.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'groups\'] = 32\n    kwargs[\'width_per_group\'] = 4\n    return _resnet(\'resnext50_32x4d\', Bottleneck, [3, 4, 6, 3],\n                   pretrained, progress,num_classes, in_channels,  **kwargs)\n\n\ndef resnext101_32x8d(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""ResNeXt-101 32x8d model from\n    `""Aggregated Residual Transformation for Deep Neural Networks"" <https://arxiv.org/pdf/1611.05431.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'groups\'] = 32\n    kwargs[\'width_per_group\'] = 8\n    return _resnet(\'resnext101_32x8d\', Bottleneck, [3, 4, 23, 3],\n                   pretrained, progress,num_classes, in_channels,  **kwargs)\n\n\ndef wide_resnet50_2(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""Wide ResNet-50-2 model from\n    `""Wide Residual Networks"" <https://arxiv.org/pdf/1605.07146.pdf>`_\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'width_per_group\'] = 64 * 2\n    return _resnet(\'wide_resnet50_2\', Bottleneck, [3, 4, 6, 3],\n                   pretrained, progress, num_classes, in_channels,  **kwargs)\n \n\ndef wide_resnet101_2(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""Wide ResNet-101-2 model from\n    `""Wide Residual Networks"" <https://arxiv.org/pdf/1605.07146.pdf>`_\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'width_per_group\'] = 64 * 2\n    return _resnet(\'wide_resnet101_2\', Bottleneck, [3, 4, 23, 3],\n                   pretrained, progress, num_classes, in_channels,  **kwargs)'"
torchsat/models/classification/vgg.py,12,"b'import torch\nimport torch.nn as nn\nfrom torch.hub import load_state_dict_from_url\n\n__all__ = [\n    \'VGG\', \'vgg11\', \'vgg11_bn\', \'vgg13\', \'vgg13_bn\', \'vgg16\', \'vgg16_bn\',\n    \'vgg19_bn\', \'vgg19\',\n]\n\n\nmodel_urls = {\n    \'vgg11\': \'https://download.pytorch.org/models/vgg11-bbd30ac9.pth\',\n    \'vgg13\': \'https://download.pytorch.org/models/vgg13-c768596a.pth\',\n    \'vgg16\': \'https://download.pytorch.org/models/vgg16-397923af.pth\',\n    \'vgg19\': \'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\',\n    \'vgg11_bn\': \'https://download.pytorch.org/models/vgg11_bn-6002323d.pth\',\n    \'vgg13_bn\': \'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth\',\n    \'vgg16_bn\': \'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\',\n    \'vgg19_bn\': \'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth\',\n}\n\n\nclass VGG(nn.Module):\n\n    def __init__(self, features, num_classes=1000, init_weights=True):\n        super(VGG, self).__init__()\n        self.features = features\n        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, num_classes),\n        )\n        if init_weights:\n            self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                nn.init.constant_(m.bias, 0)\n\n\ndef make_layers(cfg, in_channels=3, batch_norm=False):\n    layers = []\n    for v in cfg:\n        if v == \'M\':\n            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n        else:\n            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n            if batch_norm:\n                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n            else:\n                layers += [conv2d, nn.ReLU(inplace=True)]\n            in_channels = v\n    return nn.Sequential(*layers)\n\n\ncfgs = {\n    \'A\': [64, \'M\', 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'B\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, \'M\', 512, 512, \'M\', 512, 512, \'M\'],\n    \'D\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, \'M\', 512, 512, 512, \'M\', 512, 512, 512, \'M\'],\n    \'E\': [64, 64, \'M\', 128, 128, \'M\', 256, 256, 256, 256, \'M\', 512, 512, 512, 512, \'M\', 512, 512, 512, 512, \'M\'],\n}\n\n\ndef _vgg(arch, cfg, batch_norm, pretrained, progress, num_classes, in_channels, **kwargs):\n    # if pretrained and in_channels != 3:\n    #     raise ValueError(\'ImageNet pretrained models only support 3 input channels, but got {}\'.format(in_channels))\n        \n    if pretrained:\n        kwargs[\'init_weights\'] = False\n        model = VGG(make_layers(cfgs[cfg], in_channels=3, batch_norm=batch_norm), **kwargs)\n        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n        model.load_state_dict(state_dict)\n        conv0 = model.features[0]\n        model.features[0] = nn.Conv2d(in_channels=in_channels,\n                        out_channels=conv0.out_channels,\n                        kernel_size=conv0.kernel_size,\n                        stride=conv0.stride,\n                        padding=conv0.padding)\n        if in_channels <= 3:\n            model.features[0].weight.data = conv0.weight[:,0:in_channels,:,:]\n        else:\n            multi = in_channels//3\n            last = in_channels%3\n            model.features[0].weight.data = torch.cat([conv0.weight for x in range(multi)], dim=1)\n            model.features[0].weight.data = conv0.weight[:,:last,:,:]\n        model.classifier[6] = nn.Linear(model.classifier[6].in_features, 2)\n    else:\n        model = VGG(make_layers(cfgs[cfg], in_channels=in_channels, batch_norm=batch_norm), \n                    num_classes=num_classes, **kwargs)\n    return model\n\n\ndef vgg11(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""VGG 11-layer model (configuration ""A"") from\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>\'_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _vgg(\'vgg11\', \'A\', False, pretrained, progress, num_classes, in_channels, **kwargs)\n\n\ndef vgg11_bn(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""VGG 11-layer model (configuration ""A"") with batch normalization\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>\'_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _vgg(\'vgg11_bn\', \'A\', True, pretrained, progress, num_classes, in_channels, **kwargs)\n\n\ndef vgg13(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""VGG 13-layer model (configuration ""B"")\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>\'_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _vgg(\'vgg13\', \'B\', False, pretrained, progress, num_classes, in_channels, **kwargs)\n\n\ndef vgg13_bn(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""VGG 13-layer model (configuration ""B"") with batch normalization\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>\'_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _vgg(\'vgg13_bn\', \'B\', True, pretrained, progress, num_classes, in_channels, **kwargs)\n\n\ndef vgg16(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""VGG 16-layer model (configuration ""D"")\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>\'_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _vgg(\'vgg16\', \'D\', False, pretrained, progress, num_classes, in_channels, **kwargs)\n\n\ndef vgg16_bn(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""VGG 16-layer model (configuration ""D"") with batch normalization\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>\'_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _vgg(\'vgg16_bn\', \'D\', True, pretrained, progress, num_classes, in_channels, **kwargs)\n\n\ndef vgg19(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""VGG 19-layer model (configuration ""E"")\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>\'_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _vgg(\'vgg19\', \'E\', False, pretrained, progress, num_classes, in_channels, **kwargs)\n\n\ndef vgg19_bn(num_classes, in_channels=3, pretrained=False, progress=True, **kwargs):\n    r""""""VGG 19-layer model (configuration \'E\') with batch normalization\n    `""Very Deep Convolutional Networks For Large-Scale Image Recognition"" <https://arxiv.org/pdf/1409.1556.pdf>\'_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _vgg(\'vgg19_bn\', \'E\', True, pretrained, progress, num_classes, in_channels, **kwargs)\n\n# if __name__ == ""__main__"":\n    # vgg11(3, in_channels=3, pretrained=True)'"
torchsat/models/segmentation/__init__.py,0,b'from .unet import *'
torchsat/models/segmentation/unet.py,5,"b'from torch import nn\nfrom torch.nn import functional as F\nimport torch\nfrom torchvision import models\nimport torchvision\nfrom ..classification import resnet\n\n\n__all__ = [\'UNetResNet\',\'unet34\', \'unet101\', \'unet152\']\n\n""""""\nThis script has been taken (and modified) from :\nhttps://github.com/ternaus/TernausNet\n@ARTICLE{arXiv:1801.05746,\n         author = {V. Iglovikov and A. Shvets},\n          title = {TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation},\n        journal = {ArXiv e-prints},\n         eprint = {1801.05746}, \n           year = 2018\n        }\n""""""\n\n\ndef _conv3x3(in_, out):\n    return nn.Conv2d(in_, out, 3, padding=1)\n\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_, out):\n        super().__init__()\n        self.conv = _conv3x3(in_, out)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.activation(x)\n        return x\n\n\nclass NoOperation(nn.Module):\n    def forward(self, x):\n        return x\n\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super().__init__()\n\n        self.block = nn.Sequential(\n            ConvRelu(in_channels, middle_channels),\n            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass DecoderBlockV2(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True):\n        super(DecoderBlockV2, self).__init__()\n        self.in_channels = in_channels\n\n        if is_deconv:\n            """"""\n                Paramaters for Deconvolution were chosen to avoid artifacts, following\n                link https://distill.pub/2016/deconv-checkerboard/\n            """"""\n\n            self.block = nn.Sequential(\n                ConvRelu(in_channels, middle_channels),\n                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n                                   padding=1),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            self.block = nn.Sequential(\n                nn.Upsample(scale_factor=2, mode=\'bilinear\'),\n                ConvRelu(in_channels, middle_channels),\n                ConvRelu(middle_channels, out_channels),\n            )\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass UNetResNet(nn.Module):\n    """"""    PyTorch U-Net model using ResNet(34, 101 or 152) encoder.\n    UNet: https://arxiv.org/abs/1505.04597\n    ResNet: https://arxiv.org/abs/1512.03385\n    Proposed by Alexander Buslaev: https://www.linkedin.com/in/al-buslaev/\n    \n    Args:\n        encoder_depth (int): Depth of a ResNet encoder (34, 101 or 152).\n        num_classes (int): Number of output classes.\n        num_filters (int, optional): Number of filters in the last layer of decoder. Defaults to 32.\n        dropout_2d (float, optional): Probability factor of dropout layer before output layer. Defaults to 0.2.\n        pretrained (bool, optional):\n            False - no pre-trained weights are being used.\n            True  - ResNet encoder is pre-trained on ImageNet.\n            Defaults to False.\n        is_deconv (bool, optional):\n            False: bilinear interpolation is used in decoder.\n            True: deconvolution is used in decoder.\n            Defaults to False.\n    \n    Raises:\n        ValueError: [description]\n        NotImplementedError: [description]\n    \n    Returns:\n        [type]: [description]\n    """"""\n    def __init__(self, encoder_depth, num_classes, in_channels=3, num_filters=32, dropout_2d=0.0,\n                 pretrained=False, is_deconv=False):\n        super().__init__()\n        # if pretrained and in_channels != 3:\n        #     raise ValueError(\'ImageNet pretrained models only support 3 input channels, but got {}\'.format(in_channels))\n    \n        self.num_classes = num_classes\n        self.dropout_2d = dropout_2d\n\n        if encoder_depth == 34:\n            self.encoder = resnet.resnet34(num_classes, in_channels=in_channels, pretrained=pretrained)\n            bottom_channel_nr = 512\n        elif encoder_depth == 101:\n            self.encoder = resnet.resnet101(num_classes, in_channels=in_channels, pretrained=pretrained)\n            bottom_channel_nr = 2048\n        elif encoder_depth == 152:\n            self.encoder = resnet.resnet152(num_classes, in_channels=in_channels, pretrained=pretrained)\n            bottom_channel_nr = 2048\n        else:\n            raise NotImplementedError(\'only 34, 101, 152 version of Resnet are implemented\')\n\n        self.pool = nn.MaxPool2d(2, 2)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv1 = nn.Sequential(self.encoder.conv1,\n                                   self.encoder.bn1,\n                                   self.encoder.relu,\n                                   self.pool)\n\n        self.conv2 = self.encoder.layer1\n\n        self.conv3 = self.encoder.layer2\n\n        self.conv4 = self.encoder.layer3\n\n        self.conv5 = self.encoder.layer4\n\n        self.center = DecoderBlockV2(bottom_channel_nr, num_filters * 8 * 2, num_filters * 8, is_deconv)\n        self.dec5 = DecoderBlockV2(bottom_channel_nr + num_filters * 8, num_filters * 8 * 2, num_filters * 8, is_deconv)\n        self.dec4 = DecoderBlockV2(bottom_channel_nr // 2 + num_filters * 8, num_filters * 8 * 2, num_filters * 8,\n                                   is_deconv)\n        self.dec3 = DecoderBlockV2(bottom_channel_nr // 4 + num_filters * 8, num_filters * 4 * 2, num_filters * 2,\n                                   is_deconv)\n        self.dec2 = DecoderBlockV2(bottom_channel_nr // 8 + num_filters * 2, num_filters * 2 * 2, num_filters * 2 * 2,\n                                   is_deconv)\n        self.dec1 = DecoderBlockV2(num_filters * 2 * 2, num_filters * 2 * 2, num_filters, is_deconv)\n        self.dec0 = ConvRelu(num_filters, num_filters)\n        self.final = nn.Conv2d(num_filters, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(conv1)\n        conv3 = self.conv3(conv2)\n        conv4 = self.conv4(conv3)\n        conv5 = self.conv5(conv4)\n\n        pool = self.pool(conv5)\n        center = self.center(pool)\n\n        dec5 = self.dec5(torch.cat([center, conv5], 1))\n\n        dec4 = self.dec4(torch.cat([dec5, conv4], 1))\n        dec3 = self.dec3(torch.cat([dec4, conv3], 1))\n        dec2 = self.dec2(torch.cat([dec3, conv2], 1))\n        dec1 = self.dec1(dec2)\n        dec0 = self.dec0(dec1)\n\n        return self.final(F.dropout2d(dec0, p=self.dropout_2d))\n\n\ndef unet34(num_classes, in_channels=3, pretrained=False, **kwargs):\n    model = UNetResNet(34, num_classes, in_channels=in_channels, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef unet101(num_classes, in_channels=3, pretrained=False, **kwargs):\n    model = UNetResNet(101, num_classes, in_channels=in_channels, pretrained=pretrained, **kwargs)\n    return model\n\n\ndef unet152(num_classes, in_channels=3, pretrained=False, **kwargs):\n    model = UNetResNet(152, num_classes, in_channels=in_channels, pretrained=pretrained, **kwargs)\n    return model\n'"
