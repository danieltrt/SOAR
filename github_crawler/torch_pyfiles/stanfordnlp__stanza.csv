file_path,api_count,code
setup.py,0,"b'# Always prefer setuptools over distutils\nimport re\n\nfrom setuptools import setup, find_packages\n# To use a consistent encoding\nfrom codecs import open\nfrom os import path\n\nhere = path.abspath(path.dirname(__file__))\n\n# read the version from stanza/_version.py\nversion_file_contents = open(path.join(here, \'stanza/_version.py\'), encoding=\'utf-8\').read()\nVERSION = re.compile(\'__version__ = \\""(.*)\\""\').search(version_file_contents).group(1)\n\n# Get the long description from the README file\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\nsetup(\n    name=\'stanza\',\n\n    # Versions should comply with PEP440.  For a discussion on single-sourcing\n    # the version across setup.py and the project code, see\n    # https://packaging.python.org/en/latest/single_source_version.html\n    version=VERSION,\n\n    description=\'A Python NLP Library for Many Human Languages, by the Stanford NLP Group\',\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    # The project\'s main homepage.\n    url=\'https://github.com/stanfordnlp/stanza\',\n\n    # Author details\n    author=\'Stanford Natural Language Processing Group\',\n    author_email=\'jebolton@stanford.edu\',\n\n    # Choose your license\n    license=\'Apache License 2.0\',\n\n    # See https://pypi.python.org/pypi?%3Aaction=list_classifiers\n    classifiers=[\n        # How mature is this project? Common values are\n        #   3 - Alpha\n        #   4 - Beta\n        #   5 - Production/Stable\n        \'Development Status :: 4 - Beta\',\n\n        # Indicate who your project is intended for\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'Intended Audience :: Information Technology\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Scientific/Engineering :: Information Analysis\',\n        \'Topic :: Text Processing\',\n        \'Topic :: Text Processing :: Linguistic\',\n        \'Topic :: Software Development\',\n        \'Topic :: Software Development :: Libraries\',\n\n        # Specify the Python versions you support here. In particular, ensure\n        # that you indicate whether you support Python 2, Python 3 or both.\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Python :: 3.8\',\n    ],\n\n    # What does your project relate to?\n    keywords=\'natural-language-processing nlp natural-language-understanding stanford-nlp deep-learning\',\n\n    # You can just specify the packages manually here if your project is\n    # simple. Or you can use find_packages().\n    packages=find_packages(exclude=[\'data\', \'docs\', \'extern_data\', \'figures\', \'saved_models\']),\n\n    # List run-time dependencies here.  These will be installed by pip when\n    # your project is installed. For an analysis of ""install_requires"" vs pip\'s\n    # requirements files see:\n    # https://packaging.python.org/en/latest/requirements.html\n    install_requires=[\'numpy\', \'protobuf\', \'requests\', \'torch>=1.3.0\', \'tqdm\'],\n\n    # List required Python versions\n    python_requires=\'>=3.6\',\n\n    # List additional groups of dependencies here (e.g. development\n    # dependencies). You can install these using the following syntax,\n    # for example:\n    # $ pip install -e .[dev,test]\n    extras_require={\n        \'dev\': [\'check-manifest\'],\n        \'test\': [\'coverage\'],\n    },\n\n    # If there are data files included in your packages that need to be\n    # installed, specify them here.  If using Python 2.6 or less, then these\n    # have to be included in MANIFEST.in as well.\n    package_data={\n    },\n\n    # Although \'package_data\' is the preferred approach, in some case you may\n    # need to place data files outside of your packages. See:\n    # http://docs.python.org/3.4/distutils/setupscript.html#installing-additional-files # noqa\n    # In this case, \'data_file\' will be installed into \'<sys.prefix>/my_data\'\n    data_files=[],\n\n    # To provide executable scripts, use entry points in preference to the\n    # ""scripts"" keyword. Entry points provide cross-platform support and allow\n    # pip to create the appropriate form of executable for the target platform.\n    entry_points={\n    },\n)\n'"
demo/corenlp.py,0,"b'from stanza.server import CoreNLPClient\n\n# example text\nprint(\'---\')\nprint(\'input text\')\nprint(\'\')\n\ntext = ""Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.""\n\nprint(text)\n\n# set up the client\nprint(\'---\')\nprint(\'starting up Java Stanford CoreNLP Server...\')\n\n# set up the client\nwith CoreNLPClient(annotators=[\'tokenize\',\'ssplit\',\'pos\',\'lemma\',\'ner\',\'parse\',\'depparse\',\'coref\'], timeout=60000, memory=\'16G\') as client:\n    # submit the request to the server\n    ann = client.annotate(text)\n\n    # get the first sentence\n    sentence = ann.sentence[0]\n\n    # get the dependency parse of the first sentence\n    print(\'---\')\n    print(\'dependency parse of first sentence\')\n    dependency_parse = sentence.basicDependencies\n    print(dependency_parse)\n \n    # get the constituency parse of the first sentence\n    print(\'---\')\n    print(\'constituency parse of first sentence\')\n    constituency_parse = sentence.parseTree\n    print(constituency_parse)\n\n    # get the first subtree of the constituency parse\n    print(\'---\')\n    print(\'first subtree of constituency parse\')\n    print(constituency_parse.child[0])\n\n    # get the value of the first subtree\n    print(\'---\')\n    print(\'value of first subtree of constituency parse\')\n    print(constituency_parse.child[0].value)\n\n    # get the first token of the first sentence\n    print(\'---\')\n    print(\'first token of first sentence\')\n    token = sentence.token[0]\n    print(token)\n\n    # get the part-of-speech tag\n    print(\'---\')\n    print(\'part of speech tag of token\')\n    token.pos\n    print(token.pos)\n\n    # get the named entity tag\n    print(\'---\')\n    print(\'named entity tag of token\')\n    print(token.ner)\n\n    # get an entity mention from the first sentence\n    print(\'---\')\n    print(\'first entity mention in sentence\')\n    print(sentence.mentions[0])\n\n    # access the coref chain\n    print(\'---\')\n    print(\'coref chains for the example\')\n    print(ann.corefChain)\n\n    # Use tokensregex patterns to find who wrote a sentence.\n    pattern = \'([ner: PERSON]+) /wrote/ /an?/ []{0,3} /sentence|article/\'\n    matches = client.tokensregex(text, pattern)\n    # sentences contains a list with matches for each sentence.\n    assert len(matches[""sentences""]) == 3\n    # length tells you whether or not there are any matches in this\n    assert matches[""sentences""][1][""length""] == 1\n    # You can access matches like most regex groups.\n    matches[""sentences""][1][""0""][""text""] == ""Chris wrote a simple sentence""\n    matches[""sentences""][1][""0""][""1""][""text""] == ""Chris""\n\n    # Use semgrex patterns to directly find who wrote what.\n    pattern = \'{word:wrote} >nsubj {}=subject >dobj {}=object\'\n    matches = client.semgrex(text, pattern)\n    # sentences contains a list with matches for each sentence.\n    assert len(matches[""sentences""]) == 3\n    # length tells you whether or not there are any matches in this\n    assert matches[""sentences""][1][""length""] == 1\n    # You can access matches like most regex groups.\n    matches[""sentences""][1][""0""][""text""] == ""wrote""\n    matches[""sentences""][1][""0""][""$subject""][""text""] == ""Chris""\n    matches[""sentences""][1][""0""][""$object""][""text""] == ""sentence""\n\n'"
demo/pipeline_demo.py,0,"b'""""""\nbasic demo script\n""""""\n\nimport sys\nimport argparse\nimport os\n\nimport stanza\nfrom stanza.utils.resources import DEFAULT_MODEL_DIR\n\n\nif __name__ == \'__main__\':\n    # get arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-d\', \'--models_dir\', help=\'location of models files | default: ~/stanza_resources\',\n                        default=DEFAULT_MODEL_DIR)\n    parser.add_argument(\'-l\', \'--lang\', help=\'Demo language\',\n                        default=""en"")\n    parser.add_argument(\'-c\', \'--cpu\', action=\'store_true\', help=\'Use cpu as the device.\')\n    args = parser.parse_args()\n\n    example_sentences = {""en"": ""Barack Obama was born in Hawaii.  He was elected president in 2008."",\n            ""zh"": ""\xe9\x81\x94\xe6\xb2\x83\xe6\x96\xaf\xe4\xb8\x96\xe7\x95\x8c\xe7\xb6\x93\xe6\xbf\x9f\xe8\xab\x96\xe5\xa3\x87\xe6\x98\xaf\xe6\xaf\x8f\xe5\xb9\xb4\xe5\x85\xa8\xe7\x90\x83\xe6\x94\xbf\xe5\x95\x86\xe7\x95\x8c\xe9\xa0\x98\xe8\xa2\x96\xe8\x81\x9a\xe5\x9c\xa8\xe4\xb8\x80\xe8\xb5\xb7\xe7\x9a\x84\xe5\xb9\xb4\xe5\xba\xa6\xe7\x9b\x9b\xe4\xba\x8b\xe3\x80\x82"",\n            ""fr"": ""Van Gogh grandit au sein d\'une famille de l\'ancienne bourgeoisie. Il tente d\'abord de faire carri\xc3\xa8re comme marchand d\'art chez Goupil & C."",\n            ""vi"": ""Tr\xe1\xba\xadn Tr\xc3\xa2n Ch\xc3\xa2u C\xe1\xba\xa3ng (hay Chi\xe1\xba\xbfn d\xe1\xbb\x8bch Hawaii theo c\xc3\xa1ch g\xe1\xbb\x8di c\xe1\xbb\xa7a B\xe1\xbb\x99 T\xe1\xbb\x95ng t\xc6\xb0 l\xe1\xbb\x87nh \xc4\x90\xe1\xba\xbf qu\xe1\xbb\x91c Nh\xe1\xba\xadt B\xe1\xba\xa3n) l\xc3\xa0 m\xe1\xbb\x99t \xc4\x91\xc3\xb2n t\xe1\xba\xa5n c\xc3\xb4ng qu\xc3\xa2n s\xe1\xbb\xb1 b\xe1\xba\xa5t ng\xe1\xbb\x9d \xc4\x91\xc6\xb0\xe1\xbb\xa3c H\xe1\xba\xa3i qu\xc3\xa2n Nh\xe1\xba\xadt B\xe1\xba\xa3n th\xe1\xbb\xb1c hi\xe1\xbb\x87n nh\xe1\xba\xb1m v\xc3\xa0o c\xc4\x83n c\xe1\xbb\xa9 h\xe1\xba\xa3i qu\xc3\xa2n c\xe1\xbb\xa7a Hoa K\xe1\xbb\xb3 t\xe1\xba\xa1i Tr\xc3\xa2n Ch\xc3\xa2u C\xe1\xba\xa3ng thu\xe1\xbb\x99c ti\xe1\xbb\x83u bang Hawaii v\xc3\xa0o s\xc3\xa1ng Ch\xe1\xbb\xa7 Nh\xe1\xba\xadt, ng\xc3\xa0y 7 th\xc3\xa1ng 12 n\xc4\x83m 1941, d\xe1\xba\xabn \xc4\x91\xe1\xba\xbfn vi\xe1\xbb\x87c Hoa K\xe1\xbb\xb3 sau \xc4\x91\xc3\xb3 quy\xe1\xba\xbft \xc4\x91\xe1\xbb\x8bnh tham gia v\xc3\xa0o ho\xe1\xba\xa1t \xc4\x91\xe1\xbb\x99ng qu\xc3\xa2n s\xe1\xbb\xb1 trong Chi\xe1\xba\xbfn tranh th\xe1\xba\xbf gi\xe1\xbb\x9bi th\xe1\xbb\xa9 hai.""}\n\n    if args.lang not in example_sentences:\n        print(f\'Sorry, but we don\\\'t have a demo sentence for ""{args.lang}"" for the moment. Try one of these languages: {list(example_sentences.keys())}\')\n        sys.exit(1)\n\n    # download the models\n    stanza.download(args.lang, args.models_dir, confirm_if_exists=True)\n    # set up a pipeline\n    print(\'---\')\n    print(\'Building pipeline...\')\n    pipeline = stanza.Pipeline(models_dir=args.models_dir, lang=args.lang, use_gpu=(not args.cpu))\n    # process the document\n    doc = pipeline(example_sentences[args.lang])\n    # access nlp annotations\n    print(\'\')\n    print(\'Input: {}\'.format(example_sentences[args.lang]))\n    print(""The tokenizer split the input into {} sentences."".format(len(doc.sentences)))\n    print(\'---\')\n    print(\'tokens of first sentence: \')\n    doc.sentences[0].print_tokens()\n    print(\'\')\n    print(\'---\')\n    print(\'dependency parse of first sentence: \')\n    doc.sentences[0].print_dependencies()\n    print(\'\')\n\n'"
scripts/lang2code.py,0,"b'""""""\nConvert an input language name into its short code.\n""""""\nimport sys\n\nfrom stanza.models.common.constant import lang2lcode\n\nif len(sys.argv) <= 1:\n    raise Exception(""Language name not provided."")\n\nlang = sys.argv[1]\nif lang not in lang2lcode:\n    raise Exception(""Language name not found: {}"".format(lang))\ncode = lang2lcode[lang]\nsys.stdout.write(code)\n\n'"
stanza/__init__.py,0,"b'from stanza.pipeline.core import Pipeline\nfrom stanza.models.common.doc import Document\nfrom stanza.utils.resources import download\nfrom stanza._version import __version__, __resources_version__\n\nimport logging.config\nlogging.config.dictConfig(\n    {\n        ""version"": 1,\n        ""disable_existing_loggers"": False,\n        ""formatters"": {\n            ""standard"": {\n                ""format"": ""%(asctime)s %(levelname)s: %(message)s"",\n                \'datefmt\': \'%Y-%m-%d %H:%M:%S\'\n                }\n            },\n        ""handlers"": {\n            ""console"": {\n                ""class"": ""logging.StreamHandler"",\n                ""formatter"": ""standard"",\n            }\n        },\n        ""loggers"": {\n            """": {""handlers"": [""console""]}\n        },\n    }\n)\n'"
stanza/_version.py,0,"b'"""""" Single source of truth for version number """"""\n\n__version__ = ""1.0.1""\n__resources_version__ = \'1.0.0\'\n'"
tests/__init__.py,0,"b'""""""\nUtilities for testing\n""""""\n\nimport os\nimport re\n\n# Environment Variables\n# set this to specify working directory of tests\nTEST_HOME_VAR = \'STANZA_TEST_HOME\'\n\n# Global Variables\n# test working directory base name must be stanza_test\nTEST_DIR_BASE_NAME = \'stanza_test\'\n\n# check the working dir is set and compliant\nassert os.getenv(TEST_HOME_VAR) is not None, \\\n    f\'Please set {TEST_HOME_VAR} environment variable for test working dir, base name must be: {TEST_DIR_BASE_NAME}\'\nTEST_WORKING_DIR = os.getenv(TEST_HOME_VAR)\nassert os.path.basename(TEST_WORKING_DIR) == TEST_DIR_BASE_NAME, \\\n    f\'Base name of test home dir must be: {TEST_DIR_BASE_NAME}\'\n\nTEST_MODELS_DIR = f\'{TEST_WORKING_DIR}/models\'\n\n# server resources\nSERVER_TEST_PROPS = f\'{TEST_WORKING_DIR}/scripts/external_server.properties\'\n\n# langauge resources\nLANGUAGE_RESOURCES = {}\n\nTOKENIZE_MODEL = \'tokenizer.pt\'\nMWT_MODEL = \'mwt_expander.pt\'\nPOS_MODEL = \'tagger.pt\'\nPOS_PRETRAIN = \'pretrain.pt\'\nLEMMA_MODEL = \'lemmatizer.pt\'\nDEPPARSE_MODEL = \'parser.pt\'\nDEPPARSE_PRETRAIN = \'pretrain.pt\'\n\nMODEL_FILES = [TOKENIZE_MODEL, MWT_MODEL, POS_MODEL, POS_PRETRAIN, LEMMA_MODEL, DEPPARSE_MODEL, DEPPARSE_PRETRAIN]\n\n# English resources\nEN_KEY = \'en\'\nEN_SHORTHAND = \'en_ewt\'\n# models\nEN_MODELS_DIR = f\'{TEST_WORKING_DIR}/models/{EN_SHORTHAND}_models\'\nEN_MODEL_FILES = [f\'{EN_MODELS_DIR}/{EN_SHORTHAND}_{model_fname}\' for model_fname in MODEL_FILES]\n\n# French resources\nFR_KEY = \'fr\'\nFR_SHORTHAND = \'fr_gsd\'\n# regression file paths\nFR_TEST_IN = f\'{TEST_WORKING_DIR}/in/fr_gsd.test.txt\'\nFR_TEST_OUT = f\'{TEST_WORKING_DIR}/out/fr_gsd.test.txt.out\'\nFR_TEST_GOLD_OUT = f\'{TEST_WORKING_DIR}/out/fr_gsd.test.txt.out.gold\'\n# models\nFR_MODELS_DIR = f\'{TEST_WORKING_DIR}/models/{FR_SHORTHAND}_models\'\nFR_MODEL_FILES = [f\'{FR_MODELS_DIR}/{FR_SHORTHAND}_{model_fname}\' for model_fname in MODEL_FILES]\n\n# Other language resources\nAR_SHORTHAND = \'ar_padt\'\nDE_SHORTHAND = \'de_gsd\'\nKK_SHORTHAND = \'kk_ktb\'\nKO_SHORTHAND = \'ko_gsd\'\n\n\n# utils for clean up\n# only allow removal of dirs/files in this approved list\nREMOVABLE_PATHS = [\'en_ewt_models\', \'en_ewt_tokenizer.pt\', \'en_ewt_mwt_expander.pt\', \'en_ewt_tagger.pt\',\n                   \'en_ewt.pretrain.pt\', \'en_ewt_lemmatizer.pt\', \'en_ewt_parser.pt\', \'fr_gsd_models\',\n                   \'fr_gsd_tokenizer.pt\', \'fr_gsd_mwt_expander.pt\', \'fr_gsd_tagger.pt\', \'fr_gsd.pretrain.pt\',\n                   \'fr_gsd_lemmatizer.pt\', \'fr_gsd_parser.pt\', \'ar_padt_models\', \'ar_padt_tokenizer.pt\',\n                   \'ar_padt_mwt_expander.pt\', \'ar_padt_tagger.pt\', \'ar_padt.pretrain.pt\', \'ar_padt_lemmatizer.pt\',\n                   \'ar_padt_parser.pt\', \'de_gsd_models\', \'de_gsd_tokenizer.pt\', \'de_gsd_mwt_expander.pt\',\n                   \'de_gsd_tagger.pt\', \'de_gsd.pretrain.pt\', \'de_gsd_lemmatizer.pt\', \'de_gsd_parser.pt\',\n                   \'kk_ktb_models\', \'kk_ktb_tokenizer.pt\', \'kk_ktb_mwt_expander.pt\', \'kk_ktb_tagger.pt\',\n                   \'kk_ktb.pretrain.pt\', \'kk_ktb_lemmatizer.pt\', \'kk_ktb_parser.pt\', \'ko_gsd_models\',\n                   \'ko_gsd_tokenizer.pt\', \'ko_gsd_mwt_expander.pt\', \'ko_gsd_tagger.pt\', \'ko_gsd.pretrain.pt\',\n                   \'ko_gsd_lemmatizer.pt\', \'ko_gsd_parser.pt\']\n\n\ndef safe_rm(path_to_rm):\n    """"""\n    Safely remove a directory of files or a file\n    1.) check path exists, files are files, dirs are dirs\n    2.) only remove things on approved list REMOVABLE_PATHS\n    3.) assert no longer exists\n    """"""\n    # just return if path doesn\'t exist\n    if not os.path.exists(path_to_rm):\n        return\n    # handle directory\n    if os.path.isdir(path_to_rm):\n        files_to_rm = [f\'{path_to_rm}/{fname}\' for fname in os.listdir(path_to_rm)]\n        dir_to_rm = path_to_rm\n    else:\n        files_to_rm = [path_to_rm]\n        dir_to_rm = None\n    # clear out files\n    for file_to_rm in files_to_rm:\n        if os.path.isfile(file_to_rm) and os.path.basename(file_to_rm) in REMOVABLE_PATHS:\n            os.remove(file_to_rm)\n            assert not os.path.exists(file_to_rm), f\'Error removing: {file_to_rm}\'\n    # clear out directory\n    if dir_to_rm is not None and os.path.isdir(dir_to_rm):\n        os.rmdir(dir_to_rm)\n        assert not os.path.exists(dir_to_rm), f\'Error removing: {dir_to_rm}\'\n\ndef compare_ignoring_whitespace(predicted, expected):\n    predicted = re.sub(\'[ \\t]+\', \' \', predicted.strip())\n    expected = re.sub(\'[ \\t]+\', \' \', expected.strip())\n    assert predicted == expected\n\n'"
tests/test_client.py,0,"b'""""""\nTests that call a running CoreNLPClient.\n""""""\n\nimport pytest\nimport stanza.server as corenlp\nimport shlex\nimport subprocess\n\nfrom tests import *\n\n# set the marker for this module\npytestmark = [pytest.mark.travis, pytest.mark.client]\n\nTEXT = ""Chris wrote a simple sentence that he parsed with Stanford CoreNLP.\\n""\n\nMAX_REQUEST_ATTEMPTS = 5\n\nEN_GOLD = """"""\nSentence #1 (12 tokens):\nChris wrote a simple sentence that he parsed with Stanford CoreNLP.\n\nTokens:\n[Text=Chris CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=NNP]\n[Text=wrote CharacterOffsetBegin=6 CharacterOffsetEnd=11 PartOfSpeech=VBD]\n[Text=a CharacterOffsetBegin=12 CharacterOffsetEnd=13 PartOfSpeech=DT]\n[Text=simple CharacterOffsetBegin=14 CharacterOffsetEnd=20 PartOfSpeech=JJ]\n[Text=sentence CharacterOffsetBegin=21 CharacterOffsetEnd=29 PartOfSpeech=NN]\n[Text=that CharacterOffsetBegin=30 CharacterOffsetEnd=34 PartOfSpeech=WDT]\n[Text=he CharacterOffsetBegin=35 CharacterOffsetEnd=37 PartOfSpeech=PRP]\n[Text=parsed CharacterOffsetBegin=38 CharacterOffsetEnd=44 PartOfSpeech=VBD]\n[Text=with CharacterOffsetBegin=45 CharacterOffsetEnd=49 PartOfSpeech=IN]\n[Text=Stanford CharacterOffsetBegin=50 CharacterOffsetEnd=58 PartOfSpeech=NNP]\n[Text=CoreNLP CharacterOffsetBegin=59 CharacterOffsetEnd=66 PartOfSpeech=NNP]\n[Text=. CharacterOffsetBegin=66 CharacterOffsetEnd=67 PartOfSpeech=.]\n"""""".strip()\n\n\n@pytest.fixture(scope=""module"")\ndef corenlp_client():\n    """""" Client to run tests on """"""\n    client = corenlp.CoreNLPClient(annotators=\'tokenize,ssplit,pos,lemma,ner,depparse\',\n                                   server_id=\'stanza_main_test_server\')\n    yield client\n    client.stop()\n\n\ndef test_connect(corenlp_client):\n    corenlp_client.ensure_alive()\n    assert corenlp_client.is_active\n    assert corenlp_client.is_alive()\n\n\ndef test_context_manager():\n    with corenlp.CoreNLPClient(annotators=""tokenize,ssplit"",\n                               endpoint=""http://localhost:9001"") as context_client:\n        ann = context_client.annotate(TEXT)\n        assert corenlp.to_text(ann.sentence[0]) == TEXT[:-1]\n\ndef test_no_duplicate_servers():\n    """"""We expect a second server on the same port to fail""""""\n    with pytest.raises(corenlp.PermanentlyFailedException):\n        with corenlp.CoreNLPClient(annotators=""tokenize,ssplit"") as duplicate_server:\n            raise RuntimeError(""This should have failed"")\n\ndef test_annotate(corenlp_client):\n    ann = corenlp_client.annotate(TEXT)\n    assert corenlp.to_text(ann.sentence[0]) == TEXT[:-1]\n\n\ndef test_update(corenlp_client):\n    ann = corenlp_client.annotate(TEXT)\n    ann = corenlp_client.update(ann)\n    assert corenlp.to_text(ann.sentence[0]) == TEXT[:-1]\n\n\ndef test_tokensregex(corenlp_client):\n    pattern = \'([ner: PERSON]+) /wrote/ /an?/ []{0,3} /sentence|article/\'\n    matches = corenlp_client.tokensregex(TEXT, pattern)\n    assert len(matches[""sentences""]) == 1\n    assert matches[""sentences""][0][""length""] == 1\n    assert matches == {\n        ""sentences"": [{\n            ""0"": {\n                ""text"": ""Chris wrote a simple sentence"",\n                ""begin"": 0,\n                ""end"": 5,\n                ""1"": {\n                    ""text"": ""Chris"",\n                    ""begin"": 0,\n                    ""end"": 1\n                }},\n            ""length"": 1\n        },]}\n\n\ndef test_semgrex(corenlp_client):\n    pattern = \'{word:wrote} >nsubj {}=subject >obj {}=object\'\n    matches = corenlp_client.semgrex(TEXT, pattern, to_words=True)\n    assert matches == [\n        {\n            ""text"": ""wrote"",\n            ""begin"": 1,\n            ""end"": 2,\n            ""$subject"": {\n                ""text"": ""Chris"",\n                ""begin"": 0,\n                ""end"": 1\n            },\n            ""$object"": {\n                ""text"": ""sentence"",\n                ""begin"": 4,\n                ""end"": 5\n            },\n            ""sentence"": 0,}]\n\n\ndef test_external_server():\n    """""" Test starting up an external server and accessing with a client with start_server=False """"""\n    corenlp_home = os.getenv(\'CORENLP_HOME\')\n    start_cmd = f\'java -Xmx5g -cp ""{corenlp_home}/*"" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 \' \\\n                f\'-timeout 60000 -server_id stanza_external_server -serverProperties {SERVER_TEST_PROPS}\'\n    start_cmd = start_cmd and shlex.split(start_cmd)\n    external_server_process = subprocess.Popen(start_cmd)\n    with corenlp.CoreNLPClient(start_server=False, endpoint=""http://localhost:9001"") as external_server_client:\n        ann = external_server_client.annotate(TEXT, annotators=\'tokenize,ssplit,pos\', output_format=\'text\')\n    assert external_server_process\n    external_server_process.terminate()\n    external_server_process.wait(5)\n    assert ann.strip() == EN_GOLD\n'"
tests/test_depparse.py,0,"b'""""""\nBasic tests of the depparse processor boolean flags\n""""""\nimport pytest\n\nimport stanza\nfrom stanza.pipeline.core import PipelineRequirementsException\nfrom stanza.utils.conll import CoNLL\nfrom tests import *\n\npytestmark = pytest.mark.pipeline\n\n# data for testing\nEN_DOC = ""Barack Obama was born in Hawaii.  He was elected president in 2008.  Obama attended Harvard.""\n\nEN_DOC_CONLLU_PRETAGGED = """"""\n1\tBarack\t_\tPROPN\tNNP\tNumber=Sing\t0\t_\t_\t_\n2\tObama\t_\tPROPN\tNNP\tNumber=Sing\t1\t_\t_\t_\n3\twas\t_\tAUX\tVBD\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\t2\t_\t_\t_\n4\tborn\t_\tVERB\tVBN\tTense=Past|VerbForm=Part|Voice=Pass\t3\t_\t_\t_\n5\tin\t_\tADP\tIN\t_\t4\t_\t_\t_\n6\tHawaii\t_\tPROPN\tNNP\tNumber=Sing\t5\t_\t_\t_\n7\t.\t_\tPUNCT\t.\t_\t6\t_\t_\t_\n\n1\tHe\t_\tPRON\tPRP\tCase=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\t0\t_\t_\t_\n2\twas\t_\tAUX\tVBD\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\t1\t_\t_\t_\n3\telected\t_\tVERB\tVBN\tTense=Past|VerbForm=Part|Voice=Pass\t2\t_\t_\t_\n4\tpresident\t_\tPROPN\tNNP\tNumber=Sing\t3\t_\t_\t_\n5\tin\t_\tADP\tIN\t_\t4\t_\t_\t_\n6\t2008\t_\tNUM\tCD\tNumType=Card\t5\t_\t_\t_\n7\t.\t_\tPUNCT\t.\t_\t6\t_\t_\t_\n\n1\tObama\t_\tPROPN\tNNP\tNumber=Sing\t0\t_\t_\t_\n2\tattended\t_\tVERB\tVBD\tMood=Ind|Tense=Past|VerbForm=Fin\t1\t_\t_\t_\n3\tHarvard\t_\tPROPN\tNNP\tNumber=Sing\t2\t_\t_\t_\n4\t.\t_\tPUNCT\t.\t_\t3\t_\t_\t_\n\n\n"""""".lstrip()\n\nEN_DOC_DEPENDENCY_PARSES_GOLD = """"""\n(\'Barack\', \'4\', \'nsubj:pass\')\n(\'Obama\', \'1\', \'flat\')\n(\'was\', \'4\', \'aux:pass\')\n(\'born\', \'0\', \'root\')\n(\'in\', \'6\', \'case\')\n(\'Hawaii\', \'4\', \'obl\')\n(\'.\', \'4\', \'punct\')\n\n(\'He\', \'3\', \'nsubj:pass\')\n(\'was\', \'3\', \'aux:pass\')\n(\'elected\', \'0\', \'root\')\n(\'president\', \'3\', \'xcomp\')\n(\'in\', \'6\', \'case\')\n(\'2008\', \'3\', \'obl\')\n(\'.\', \'3\', \'punct\')\n\n(\'Obama\', \'2\', \'nsubj\')\n(\'attended\', \'0\', \'root\')\n(\'Harvard\', \'2\', \'obj\')\n(\'.\', \'2\', \'punct\')\n"""""".strip()\n\n\ndef test_depparse():\n    nlp = stanza.Pipeline(dir=TEST_MODELS_DIR, lang=\'en\')\n    doc = nlp(EN_DOC)\n    assert EN_DOC_DEPENDENCY_PARSES_GOLD == \'\\n\\n\'.join([sent.dependencies_string() for sent in doc.sentences])\n\n\ndef test_depparse_with_pretagged_doc():\n    nlp = stanza.Pipeline(**{\'processors\': \'depparse\', \'dir\': TEST_MODELS_DIR, \'lang\': \'en\',\n                                  \'depparse_pretagged\': True})\n\n    doc = stanza.Document(CoNLL.conll2dict(input_str=EN_DOC_CONLLU_PRETAGGED))\n    processed_doc = nlp(doc)\n\n    assert EN_DOC_DEPENDENCY_PARSES_GOLD == \'\\n\\n\'.join(\n        [sent.dependencies_string() for sent in processed_doc.sentences])\n\n\ndef test_raises_requirements_exception_if_pretagged_not_passed():\n    with pytest.raises(PipelineRequirementsException):\n        stanza.Pipeline(**{\'processors\': \'depparse\', \'dir\': TEST_MODELS_DIR, \'lang\': \'en\'})\n'"
tests/test_english_pipeline.py,0,"b'""""""\nBasic testing of the English pipeline\n""""""\n\nimport pytest\nimport stanza\nfrom stanza.utils.conll import CoNLL\n\nfrom tests import *\n\npytestmark = pytest.mark.pipeline\n\n# data for testing\nEN_DOC = ""Barack Obama was born in Hawaii.  He was elected president in 2008.  Obama attended Harvard.""\n\nEN_DOC_TOKENS_GOLD = """"""\n<Token id=1;words=[<Word id=1;text=Barack;lemma=Barack;upos=PROPN;xpos=NNP;feats=Number=Sing;head=4;deprel=nsubj:pass>]>\n<Token id=2;words=[<Word id=2;text=Obama;lemma=Obama;upos=PROPN;xpos=NNP;feats=Number=Sing;head=1;deprel=flat>]>\n<Token id=3;words=[<Word id=3;text=was;lemma=be;upos=AUX;xpos=VBD;feats=Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin;head=4;deprel=aux:pass>]>\n<Token id=4;words=[<Word id=4;text=born;lemma=bear;upos=VERB;xpos=VBN;feats=Tense=Past|VerbForm=Part|Voice=Pass;head=0;deprel=root>]>\n<Token id=5;words=[<Word id=5;text=in;lemma=in;upos=ADP;xpos=IN;head=6;deprel=case>]>\n<Token id=6;words=[<Word id=6;text=Hawaii;lemma=Hawaii;upos=PROPN;xpos=NNP;feats=Number=Sing;head=4;deprel=obl>]>\n<Token id=7;words=[<Word id=7;text=.;lemma=.;upos=PUNCT;xpos=.;head=4;deprel=punct>]>\n\n<Token id=1;words=[<Word id=1;text=He;lemma=he;upos=PRON;xpos=PRP;feats=Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs;head=3;deprel=nsubj:pass>]>\n<Token id=2;words=[<Word id=2;text=was;lemma=be;upos=AUX;xpos=VBD;feats=Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin;head=3;deprel=aux:pass>]>\n<Token id=3;words=[<Word id=3;text=elected;lemma=elect;upos=VERB;xpos=VBN;feats=Tense=Past|VerbForm=Part|Voice=Pass;head=0;deprel=root>]>\n<Token id=4;words=[<Word id=4;text=president;lemma=president;upos=PROPN;xpos=NNP;feats=Number=Sing;head=3;deprel=xcomp>]>\n<Token id=5;words=[<Word id=5;text=in;lemma=in;upos=ADP;xpos=IN;head=6;deprel=case>]>\n<Token id=6;words=[<Word id=6;text=2008;lemma=2008;upos=NUM;xpos=CD;feats=NumType=Card;head=3;deprel=obl>]>\n<Token id=7;words=[<Word id=7;text=.;lemma=.;upos=PUNCT;xpos=.;head=3;deprel=punct>]>\n\n<Token id=1;words=[<Word id=1;text=Obama;lemma=Obama;upos=PROPN;xpos=NNP;feats=Number=Sing;head=2;deprel=nsubj>]>\n<Token id=2;words=[<Word id=2;text=attended;lemma=attend;upos=VERB;xpos=VBD;feats=Mood=Ind|Tense=Past|VerbForm=Fin;head=0;deprel=root>]>\n<Token id=3;words=[<Word id=3;text=Harvard;lemma=Harvard;upos=PROPN;xpos=NNP;feats=Number=Sing;head=2;deprel=obj>]>\n<Token id=4;words=[<Word id=4;text=.;lemma=.;upos=PUNCT;xpos=.;head=2;deprel=punct>]>\n"""""".strip()\n\nEN_DOC_WORDS_GOLD = """"""\n<Word id=1;text=Barack;lemma=Barack;upos=PROPN;xpos=NNP;feats=Number=Sing;head=4;deprel=nsubj:pass>\n<Word id=2;text=Obama;lemma=Obama;upos=PROPN;xpos=NNP;feats=Number=Sing;head=1;deprel=flat>\n<Word id=3;text=was;lemma=be;upos=AUX;xpos=VBD;feats=Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin;head=4;deprel=aux:pass>\n<Word id=4;text=born;lemma=bear;upos=VERB;xpos=VBN;feats=Tense=Past|VerbForm=Part|Voice=Pass;head=0;deprel=root>\n<Word id=5;text=in;lemma=in;upos=ADP;xpos=IN;head=6;deprel=case>\n<Word id=6;text=Hawaii;lemma=Hawaii;upos=PROPN;xpos=NNP;feats=Number=Sing;head=4;deprel=obl>\n<Word id=7;text=.;lemma=.;upos=PUNCT;xpos=.;head=4;deprel=punct>\n\n<Word id=1;text=He;lemma=he;upos=PRON;xpos=PRP;feats=Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs;head=3;deprel=nsubj:pass>\n<Word id=2;text=was;lemma=be;upos=AUX;xpos=VBD;feats=Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin;head=3;deprel=aux:pass>\n<Word id=3;text=elected;lemma=elect;upos=VERB;xpos=VBN;feats=Tense=Past|VerbForm=Part|Voice=Pass;head=0;deprel=root>\n<Word id=4;text=president;lemma=president;upos=PROPN;xpos=NNP;feats=Number=Sing;head=3;deprel=xcomp>\n<Word id=5;text=in;lemma=in;upos=ADP;xpos=IN;head=6;deprel=case>\n<Word id=6;text=2008;lemma=2008;upos=NUM;xpos=CD;feats=NumType=Card;head=3;deprel=obl>\n<Word id=7;text=.;lemma=.;upos=PUNCT;xpos=.;head=3;deprel=punct>\n\n<Word id=1;text=Obama;lemma=Obama;upos=PROPN;xpos=NNP;feats=Number=Sing;head=2;deprel=nsubj>\n<Word id=2;text=attended;lemma=attend;upos=VERB;xpos=VBD;feats=Mood=Ind|Tense=Past|VerbForm=Fin;head=0;deprel=root>\n<Word id=3;text=Harvard;lemma=Harvard;upos=PROPN;xpos=NNP;feats=Number=Sing;head=2;deprel=obj>\n<Word id=4;text=.;lemma=.;upos=PUNCT;xpos=.;head=2;deprel=punct>\n"""""".strip()\n\nEN_DOC_DEPENDENCY_PARSES_GOLD = """"""\n(\'Barack\', \'4\', \'nsubj:pass\')\n(\'Obama\', \'1\', \'flat\')\n(\'was\', \'4\', \'aux:pass\')\n(\'born\', \'0\', \'root\')\n(\'in\', \'6\', \'case\')\n(\'Hawaii\', \'4\', \'obl\')\n(\'.\', \'4\', \'punct\')\n\n(\'He\', \'3\', \'nsubj:pass\')\n(\'was\', \'3\', \'aux:pass\')\n(\'elected\', \'0\', \'root\')\n(\'president\', \'3\', \'xcomp\')\n(\'in\', \'6\', \'case\')\n(\'2008\', \'3\', \'obl\')\n(\'.\', \'3\', \'punct\')\n\n(\'Obama\', \'2\', \'nsubj\')\n(\'attended\', \'0\', \'root\')\n(\'Harvard\', \'2\', \'obj\')\n(\'.\', \'2\', \'punct\')\n"""""".strip()\n\nEN_DOC_CONLLU_GOLD = """"""\n1\tBarack\tBarack\tPROPN\tNNP\tNumber=Sing\t4\tnsubj:pass\t_\tstart_char=0|end_char=6\n2\tObama\tObama\tPROPN\tNNP\tNumber=Sing\t1\tflat\t_\tstart_char=7|end_char=12\n3\twas\tbe\tAUX\tVBD\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\t4\taux:pass\t_\tstart_char=13|end_char=16\n4\tborn\tbear\tVERB\tVBN\tTense=Past|VerbForm=Part|Voice=Pass\t0\troot\t_\tstart_char=17|end_char=21\n5\tin\tin\tADP\tIN\t_\t6\tcase\t_\tstart_char=22|end_char=24\n6\tHawaii\tHawaii\tPROPN\tNNP\tNumber=Sing\t4\tobl\t_\tstart_char=25|end_char=31\n7\t.\t.\tPUNCT\t.\t_\t4\tpunct\t_\tstart_char=31|end_char=32\n\n1\tHe\the\tPRON\tPRP\tCase=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\t3\tnsubj:pass\t_\tstart_char=34|end_char=36\n2\twas\tbe\tAUX\tVBD\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\t3\taux:pass\t_\tstart_char=37|end_char=40\n3\telected\telect\tVERB\tVBN\tTense=Past|VerbForm=Part|Voice=Pass\t0\troot\t_\tstart_char=41|end_char=48\n4\tpresident\tpresident\tPROPN\tNNP\tNumber=Sing\t3\txcomp\t_\tstart_char=49|end_char=58\n5\tin\tin\tADP\tIN\t_\t6\tcase\t_\tstart_char=59|end_char=61\n6\t2008\t2008\tNUM\tCD\tNumType=Card\t3\tobl\t_\tstart_char=62|end_char=66\n7\t.\t.\tPUNCT\t.\t_\t3\tpunct\t_\tstart_char=66|end_char=67\n\n1\tObama\tObama\tPROPN\tNNP\tNumber=Sing\t2\tnsubj\t_\tstart_char=69|end_char=74\n2\tattended\tattend\tVERB\tVBD\tMood=Ind|Tense=Past|VerbForm=Fin\t0\troot\t_\tstart_char=75|end_char=83\n3\tHarvard\tHarvard\tPROPN\tNNP\tNumber=Sing\t2\tobj\t_\tstart_char=84|end_char=91\n4\t.\t.\tPUNCT\t.\t_\t2\tpunct\t_\tstart_char=91|end_char=92\n\n"""""".lstrip()\n\n\n@pytest.fixture(scope=""module"")\ndef processed_doc():\n    """""" Document created by running full English pipeline on a few sentences """"""\n    nlp = stanza.Pipeline(dir=TEST_MODELS_DIR)\n    return nlp(EN_DOC)\n\n\ndef test_text(processed_doc):\n    assert processed_doc.text == EN_DOC\n\n    \ndef test_conllu(processed_doc):\n    assert CoNLL.conll_as_string(CoNLL.convert_dict(processed_doc.to_dict())) == EN_DOC_CONLLU_GOLD\n\n\ndef test_tokens(processed_doc):\n    assert ""\\n\\n"".join([sent.tokens_string() for sent in processed_doc.sentences]) == EN_DOC_TOKENS_GOLD\n\n\ndef test_words(processed_doc):\n    assert ""\\n\\n"".join([sent.words_string() for sent in processed_doc.sentences]) == EN_DOC_WORDS_GOLD\n\n\ndef test_dependency_parse(processed_doc):\n    assert ""\\n\\n"".join([sent.dependencies_string() for sent in processed_doc.sentences]) == \\\n           EN_DOC_DEPENDENCY_PARSES_GOLD\n'"
tests/test_lemmatizer.py,0,"b'""""""\nBasic testing of lemmatization\n""""""\n\nimport pytest\nimport stanza\n\nfrom tests import *\n\npytestmark = pytest.mark.pipeline\n\nEN_DOC = ""Joe Smith was born in California.""\n\nEN_DOC_IDENTITY_GOLD = """"""\nJoe Joe\nSmith Smith\nwas was\nborn born\nin in\nCalifornia California\n. .\n"""""".strip()\n\nEN_DOC_LEMMATIZER_MODEL_GOLD = """"""\nJoe Joe\nSmith Smith\nwas be\nborn bear\nin in\nCalifornia California\n. .\n"""""".strip()\n\n\ndef test_identity_lemmatizer():\n    nlp = stanza.Pipeline(**{\'processors\': \'tokenize,lemma\', \'dir\': TEST_MODELS_DIR, \'lang\': \'en\',\n                                  \'lemma_use_identity\': True})\n    doc = nlp(EN_DOC)\n    word_lemma_pairs = []\n    for w in doc.iter_words():\n        word_lemma_pairs += [f""{w.text} {w.lemma}""]\n    assert EN_DOC_IDENTITY_GOLD == ""\\n"".join(word_lemma_pairs)\n\ndef test_full_lemmatizer():\n    nlp = stanza.Pipeline(**{\'processors\': \'tokenize,pos,lemma\', \'dir\': TEST_MODELS_DIR, \'lang\': \'en\'})\n    doc = nlp(EN_DOC)\n    word_lemma_pairs = []\n    for w in doc.iter_words():\n        word_lemma_pairs += [f""{w.text} {w.lemma}""]\n    assert EN_DOC_LEMMATIZER_MODEL_GOLD == ""\\n"".join(word_lemma_pairs)\n\n'"
tests/test_mwt_expander.py,0,"b'""""""\nBasic testing of multi-word-token expansion\n""""""\n\nimport pytest\nimport stanza\n\nfrom tests import *\n\npytestmark = pytest.mark.pipeline\n\n# mwt data for testing\nFR_MWT_SENTENCE = ""Alors encore inconnu du grand public, Emmanuel Macron devient en 2014 ministre de l\'\xc3\x89conomie, de "" \\\n                  ""l\'Industrie et du Num\xc3\xa9rique.""\n\n\nFR_MWT_TOKEN_TO_WORDS_GOLD = """"""\ntoken: Alors    \t\twords: [<Word id=1;text=Alors>]\ntoken: encore   \t\twords: [<Word id=2;text=encore>]\ntoken: inconnu  \t\twords: [<Word id=3;text=inconnu>]\ntoken: du       \t\twords: [<Word id=4;text=de>, <Word id=5;text=le>]\ntoken: grand    \t\twords: [<Word id=6;text=grand>]\ntoken: public   \t\twords: [<Word id=7;text=public>]\ntoken: ,        \t\twords: [<Word id=8;text=,>]\ntoken: Emmanuel \t\twords: [<Word id=9;text=Emmanuel>]\ntoken: Macron   \t\twords: [<Word id=10;text=Macron>]\ntoken: devient  \t\twords: [<Word id=11;text=devient>]\ntoken: en       \t\twords: [<Word id=12;text=en>]\ntoken: 2014     \t\twords: [<Word id=13;text=2014>]\ntoken: ministre \t\twords: [<Word id=14;text=ministre>]\ntoken: de       \t\twords: [<Word id=15;text=de>]\ntoken: l\'       \t\twords: [<Word id=16;text=l\'>]\ntoken: \xc3\x89conomie \t\twords: [<Word id=17;text=\xc3\x89conomie>]\ntoken: ,        \t\twords: [<Word id=18;text=,>]\ntoken: de       \t\twords: [<Word id=19;text=de>]\ntoken: l\'       \t\twords: [<Word id=20;text=l\'>]\ntoken: Industrie\t\twords: [<Word id=21;text=Industrie>]\ntoken: et       \t\twords: [<Word id=22;text=et>]\ntoken: du       \t\twords: [<Word id=23;text=de>, <Word id=24;text=le>]\ntoken: Num\xc3\xa9rique\t\twords: [<Word id=25;text=Num\xc3\xa9rique>]\ntoken: .        \t\twords: [<Word id=26;text=.>]\n"""""".strip()\n\nFR_MWT_WORD_TO_TOKEN_GOLD = """"""\nword: Alors    \t\ttoken parent:1-Alors\nword: encore   \t\ttoken parent:2-encore\nword: inconnu  \t\ttoken parent:3-inconnu\nword: de       \t\ttoken parent:4-5-du\nword: le       \t\ttoken parent:4-5-du\nword: grand    \t\ttoken parent:6-grand\nword: public   \t\ttoken parent:7-public\nword: ,        \t\ttoken parent:8-,\nword: Emmanuel \t\ttoken parent:9-Emmanuel\nword: Macron   \t\ttoken parent:10-Macron\nword: devient  \t\ttoken parent:11-devient\nword: en       \t\ttoken parent:12-en\nword: 2014     \t\ttoken parent:13-2014\nword: ministre \t\ttoken parent:14-ministre\nword: de       \t\ttoken parent:15-de\nword: l\'       \t\ttoken parent:16-l\'\nword: \xc3\x89conomie \t\ttoken parent:17-\xc3\x89conomie\nword: ,        \t\ttoken parent:18-,\nword: de       \t\ttoken parent:19-de\nword: l\'       \t\ttoken parent:20-l\'\nword: Industrie\t\ttoken parent:21-Industrie\nword: et       \t\ttoken parent:22-et\nword: de       \t\ttoken parent:23-24-du\nword: le       \t\ttoken parent:23-24-du\nword: Num\xc3\xa9rique\t\ttoken parent:25-Num\xc3\xa9rique\nword: .        \t\ttoken parent:26-.\n"""""".strip()\n\n\ndef test_mwt():\n    pipeline = stanza.Pipeline(processors=\'tokenize,mwt\', dir=TEST_MODELS_DIR, lang=\'fr\')\n    doc = pipeline(FR_MWT_SENTENCE)\n    token_to_words = ""\\n"".join(\n        [f\'token: {token.text.ljust(9)}\\t\\twords: [{"", "".join([word.pretty_print() for word in token.words])}]\' for sent in doc.sentences for token in sent.tokens]\n    ).strip()\n    word_to_token = ""\\n"".join(\n        [f\'word: {word.text.ljust(9)}\\t\\ttoken parent:{word.parent.id+""-""+word.parent.text}\'\n         for sent in doc.sentences for word in sent.words]).strip()\n    assert token_to_words == FR_MWT_TOKEN_TO_WORDS_GOLD\n    assert word_to_token == FR_MWT_WORD_TO_TOKEN_GOLD\n'"
tests/test_ner_tagger.py,0,"b'""""""\nBasic testing of the NER tagger.\n""""""\n\nimport pytest\nimport stanza\n\nfrom tests import *\nfrom stanza.models.ner.scorer import score_by_token, score_by_entity\n\npytestmark = pytest.mark.pipeline\n\nEN_DOC = ""Chris Manning is a good man. He works in Stanford University.""\n\nEN_DOC_GOLD = """"""\n<Span text=Chris Manning;type=PERSON;start_char=0;end_char=13>\n<Span text=Stanford University;type=ORG;start_char=41;end_char=60>\n"""""".strip()\n\n\ndef test_ner():\n    nlp = stanza.Pipeline(**{\'processors\': \'tokenize,ner\', \'dir\': TEST_MODELS_DIR, \'lang\': \'en\', \'logging_level\': \'error\'})\n    doc = nlp(EN_DOC)\n    assert EN_DOC_GOLD == \'\\n\'.join([ent.pretty_print() for ent in doc.ents])\n\n\ndef test_ner_scorer():\n    pred_sequences = [[\'O\', \'S-LOC\', \'O\', \'O\', \'B-PER\', \'E-PER\'],\n                    [\'O\', \'S-MISC\', \'O\', \'E-ORG\', \'O\', \'B-PER\', \'I-PER\', \'E-PER\']]\n    gold_sequences = [[\'O\', \'B-LOC\', \'E-LOC\', \'O\', \'B-PER\', \'E-PER\'],\n                    [\'O\', \'S-MISC\', \'B-ORG\', \'E-ORG\', \'O\', \'B-PER\', \'E-PER\', \'S-LOC\']]\n    \n    token_p, token_r, token_f = score_by_token(pred_sequences, gold_sequences)\n    assert pytest.approx(token_p, abs=0.00001) == 0.625\n    assert pytest.approx(token_r, abs=0.00001) == 0.5\n    assert pytest.approx(token_f, abs=0.00001) == 0.55555\n\n    entity_p, entity_r, entity_f = score_by_entity(pred_sequences, gold_sequences)\n    assert pytest.approx(entity_p, abs=0.00001) == 0.4\n    assert pytest.approx(entity_r, abs=0.00001) == 0.33333\n    assert pytest.approx(entity_f, abs=0.00001) == 0.36363'"
tests/test_protobuf.py,0,"b'""""""\nTests to read a stored protobuf.\nAlso serves as an example of how to parse sentences, tokens, pos, lemma,\nner, dependencies and mentions.\n\nThe test corresponds to annotations for the following sentence:\n    Chris wrote a simple sentence that he parsed with Stanford CoreNLP.\n""""""\nimport os\nimport pytest\n\nfrom pytest import fixture\nfrom stanza.protobuf import Document, Sentence, Token, DependencyGraph,\\\n                             CorefChain\nfrom stanza.protobuf import parseFromDelimitedString, writeToDelimitedString, to_text\n\n# set the marker for this module\npytestmark = [pytest.mark.travis, pytest.mark.client]\n\n# Text that was annotated\nTEXT = ""Chris wrote a simple sentence that he parsed with Stanford CoreNLP.\\n""\n\n\n@fixture\ndef doc_pb():\n    test_dir = os.path.dirname(os.path.abspath(__file__))\n    test_data = os.path.join(test_dir, \'data\', \'test.dat\')\n    with open(test_data, \'rb\') as f:\n        buf = f.read()\n    doc = Document()\n    parseFromDelimitedString(doc, buf)\n    return doc\n\n\ndef test_parse_protobuf(doc_pb):\n    assert doc_pb.ByteSize() == 4709\n\n\ndef test_write_protobuf(doc_pb):\n    stream = writeToDelimitedString(doc_pb)\n    buf = stream.getvalue()\n    stream.close()\n\n    doc_pb_ = Document()\n    parseFromDelimitedString(doc_pb_, buf)\n    assert doc_pb == doc_pb_\n\n\ndef test_document_text(doc_pb):\n    assert doc_pb.text == TEXT\n\n\ndef test_sentences(doc_pb):\n    assert len(doc_pb.sentence) == 1\n\n    sentence = doc_pb.sentence[0]\n    assert isinstance(sentence, Sentence)\n    # check sentence length\n    assert sentence.characterOffsetEnd - sentence.characterOffsetBegin == 67\n    # Note that the sentence text should actually be recovered from the tokens.\n    assert sentence.text == \'\'\n    assert to_text(sentence) == TEXT[:-1]\n\n\ndef test_tokens(doc_pb):\n    sentence = doc_pb.sentence[0]\n    tokens = sentence.token\n    assert len(tokens) == 12\n    assert isinstance(tokens[0], Token)\n\n    # Word\n    words = ""Chris wrote a simple sentence that he parsed with Stanford CoreNLP ."".split()\n    words_ = [t.word for t in tokens]\n    assert  words_ == words\n\n    # Lemma\n    lemmas = ""Chris write a simple sentence that he parse with Stanford CoreNLP ."".split()\n    lemmas_ = [t.lemma for t in tokens]\n    assert lemmas_ == lemmas\n\n    # POS\n    pos = ""NNP VBD DT JJ NN IN PRP VBD IN NNP NNP ."".split()\n    pos_ = [t.pos for t in tokens]\n    assert pos_ == pos\n\n    # NER\n    ner = ""PERSON O O O O O O O O ORGANIZATION O O"".split()\n    ner_ = [t.ner for t in tokens]\n    assert ner_ == ner\n\n    # character offsets\n    begin = [int(i) for i in ""0 6 12 14 21 30 35 38 45 50 59 66"".split()]\n    end =   [int(i) for i in ""5 11 13 20 29 34 37 44 49 58 66 67"".split()]\n    begin_ = [t.beginChar for t in tokens]\n    end_ = [t.endChar for t in tokens]\n    assert begin_ == begin\n    assert end_ == end\n\n\ndef test_dependency_parse(doc_pb):\n    """"""\n    Extract the dependency parse from the annotation.\n    """"""\n    sentence = doc_pb.sentence[0]\n\n    # You can choose from the following types of dependencies.\n    # In general, you\'ll want enhancedPlusPlus\n    assert sentence.basicDependencies.ByteSize() > 0\n    assert sentence.enhancedDependencies.ByteSize() > 0\n    assert sentence.enhancedPlusPlusDependencies.ByteSize() > 0\n\n    tree = sentence.enhancedPlusPlusDependencies\n    isinstance(tree, DependencyGraph)\n    # Indices are 1-indexd with 0 being the ""pseudo root""\n    assert tree.root  # \'wrote\' is the root. == [2]\n    # There are as many nodes as there are tokens.\n    assert len(tree.node) == len(sentence.token)\n\n    # Enhanced++ depdencies often contain additional edges and are\n    # not trees -- here, \'parsed\' would also have an edge to\n    # \'sentence\'\n    assert len(tree.edge) == 12\n\n    # This edge goes from ""wrote"" to ""Chirs""\n    edge = tree.edge[0]\n    assert edge.source == 2\n    assert edge.target == 1\n    assert edge.dep == ""nsubj""\n\n\ndef test_coref_chain(doc_pb):\n    """"""\n    Extract the corefence chains from the annotation.\n    """"""\n    # Coreference chains span sentences and are stored in the\n    # document.\n    chains = doc_pb.corefChain\n\n    # In this document there is 1 chain with Chris and he.\n    assert len(chains) == 1\n    chain = chains[0]\n    assert isinstance(chain, CorefChain)\n    assert chain.mention[0].beginIndex == 0  # \'Chris\'\n    assert chain.mention[0].endIndex == 1\n    assert chain.mention[0].gender == ""MALE""\n\n    assert chain.mention[1].beginIndex == 6  # \'he\'\n    assert chain.mention[1].endIndex == 7\n    assert chain.mention[1].gender == ""MALE""\n\n    assert chain.representative == 0  # Head of the chain is \'Chris\'\n'"
tests/test_requirements.py,0,"b'""""""\nTest the requirements functionality for processors\n""""""\n\nimport pytest\nimport stanza\n\nfrom stanza.pipeline.core import PipelineRequirementsException\nfrom stanza.pipeline.processor import ProcessorRequirementsException\nfrom tests import *\n\npytestmark = pytest.mark.pipeline\n\ndef check_exception_vals(req_exception, req_exception_vals):\n    """"""\n    Check the values of a ProcessorRequirementsException against a dict of expected values.\n    :param req_exception: the ProcessorRequirementsException to evaluate\n    :param req_exception_vals: expected values for the ProcessorRequirementsException\n    :return: None\n    """"""\n    assert isinstance(req_exception, ProcessorRequirementsException)\n    assert req_exception.processor_type == req_exception_vals[\'processor_type\']\n    assert req_exception.processors_list == req_exception_vals[\'processors_list\']\n    assert req_exception.err_processor.requires == req_exception_vals[\'requires\']\n\n\ndef test_missing_requirements():\n    """"""\n    Try to build several pipelines with bad configs and check thrown exceptions against gold exceptions.\n    :return: None\n    """"""\n    # list of (bad configs, list of gold ProcessorRequirementsExceptions that should be thrown) pairs\n    bad_config_lists = [\n        # missing tokenize\n        (\n            # input config\n            {\'processors\': \'pos,depparse\', \'dir\': TEST_MODELS_DIR, \'lang\': \'en\'},\n            # 2 expected exceptions\n            [\n                {\'processor_type\': \'POSProcessor\', \'processors_list\': [\'pos\', \'depparse\'], \'provided_reqs\': set([]),\n                 \'requires\': set([\'tokenize\'])},\n                {\'processor_type\': \'DepparseProcessor\', \'processors_list\': [\'pos\', \'depparse\'],\n                 \'provided_reqs\': set([]), \'requires\': set([\'tokenize\',\'pos\', \'lemma\'])}\n            ]\n        ),\n        # no pos when lemma_pos set to True; for english mwt should not be included in the loaded processor list\n        (\n            # input config\n            {\'processors\': \'tokenize,mwt,lemma\', \'dir\': TEST_MODELS_DIR, \'lang\': \'en\', \'lemma_pos\': True},\n            # 1 expected exception\n            [\n                {\'processor_type\': \'LemmaProcessor\', \'processors_list\': [\'tokenize\', \'lemma\'],\n                 \'provided_reqs\': set([\'tokenize\', \'mwt\']), \'requires\': set([\'tokenize\', \'pos\'])}\n            ]\n        )\n    ]\n    # try to build each bad config, catch exceptions, check against gold\n    pipeline_fails = 0\n    for bad_config, gold_exceptions in bad_config_lists:\n        try:\n            stanza.Pipeline(**bad_config)\n        except PipelineRequirementsException as e:\n            pipeline_fails += 1\n            assert isinstance(e, PipelineRequirementsException)\n            assert len(e.processor_req_fails) == len(gold_exceptions)\n            for processor_req_e, gold_exception in zip(e.processor_req_fails,gold_exceptions):\n                # compare the thrown ProcessorRequirementsExceptions against gold\n                check_exception_vals(processor_req_e, gold_exception)\n    # check pipeline building failed twice\n    assert pipeline_fails == 2\n\n\n'"
tests/test_run_pipeline.py,0,"b'""""""\nTests for the run_pipeline.py script, also serves as integration test\n""""""\n\nimport pytest\nimport re\nimport subprocess\n\nfrom datetime import datetime\nfrom tests import *\n\nDOWNLOAD_TEST_DIR = f\'{TEST_WORKING_DIR}/download\'\n\nRUN_PIPELINE_TEST_LANGUAGES = [AR_SHORTHAND, DE_SHORTHAND, FR_SHORTHAND, KK_SHORTHAND, KO_SHORTHAND]\n\n\ndef test_all_langs():\n    for lang_shorthand in RUN_PIPELINE_TEST_LANGUAGES:\n        run_pipeline_for_lang(lang_shorthand)\n\n\ndef run_pipeline_for_lang(lang_shorthand):\n    input_file = f\'{TEST_WORKING_DIR}/in/{lang_shorthand}.test.txt\'\n    output_file = f\'{TEST_WORKING_DIR}/out/{lang_shorthand}.test.txt.out\'\n    gold_output_file = f\'{TEST_WORKING_DIR}/out/{lang_shorthand}.test.txt.out.gold\'\n    models_download_dir = f\'{DOWNLOAD_TEST_DIR}/{lang_shorthand}_models\'\n    # check input files present\n    assert os.path.exists(input_file), f\'Missing test input file: {input_file}\'\n    assert os.path.exists(gold_output_file), f\'Missing test gold output file: {gold_output_file}\'\n    # verify models not downloaded and output file doesn\'t exist\n    safe_rm(output_file)\n    safe_rm(models_download_dir)\n    # run french pipeline command and check results\n    pipeline_cmd = \\\n        f""python -m stanza.run_pipeline -t {lang_shorthand} -d {DOWNLOAD_TEST_DIR} --force-download "" \\\n        f""-o {output_file} {input_file}""\n    subprocess.call(pipeline_cmd, shell=True)\n    # cleanup\n    # log this test run\'s final output\n    if os.path.exists(output_file):\n        curr_timestamp = re.sub(\' \', \'-\', str(datetime.now()))\n        os.rename(output_file, f\'{output_file}-{curr_timestamp}\')\n    safe_rm(models_download_dir)\n    assert open(gold_output_file).read() == open(f\'{output_file}-{curr_timestamp}\').read(), \\\n        f\'Test failure: output does not match gold\'\n'"
tests/test_server_misc.py,0,"b'""""""\nMisc tests for the server\n""""""\n\nimport pytest\nimport re\nimport stanza.server as corenlp\nfrom tests import compare_ignoring_whitespace\n\npytestmark = pytest.mark.client\n\nEN_DOC = ""Joe Smith lives in California.""\n\nEN_DOC_GOLD = """"""\nSentence #1 (6 tokens):\nJoe Smith lives in California.\n\nTokens:\n[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP Lemma=Joe NamedEntityTag=PERSON]\n[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP Lemma=Smith NamedEntityTag=PERSON]\n[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ Lemma=live NamedEntityTag=O]\n[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN Lemma=in NamedEntityTag=O]\n[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP Lemma=California NamedEntityTag=STATE_OR_PROVINCE]\n[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=. Lemma=. NamedEntityTag=O]\n\nDependency Parse (enhanced plus plus dependencies):\nroot(ROOT-0, lives-3)\ncompound(Smith-2, Joe-1)\nnsubj(lives-3, Smith-2)\ncase(California-5, in-4)\nobl(lives-3, California-5)\npunct(lives-3, .-6)\n\nExtracted the following NER entity mentions:\nJoe Smith       PERSON  PERSON:0.9972202689478088\nCalifornia      STATE_OR_PROVINCE       LOCATION:0.9990868267002156\n""""""\n\n\ndef test_english_request():\n    """""" Test case of starting server with Spanish defaults, and then requesting default English properties """"""\n    with corenlp.CoreNLPClient(properties=\'spanish\', server_id=\'test_english_request\') as client:\n        ann = client.annotate(EN_DOC, properties_key=\'english\', output_format=\'text\')\n        compare_ignoring_whitespace(ann, EN_DOC_GOLD)\n\n\n\ndef test_unknown_request():\n    """""" Test case of starting server with Spanish defaults, and then requesting UNBAN_MOX_OPAL properties """"""\n    with corenlp.CoreNLPClient(properties=\'spanish\', server_id=\'test_english_request\') as client:\n        with pytest.raises(ValueError):\n            ann = client.annotate(EN_DOC, properties_key=\'UNBAN_MOX_OPAL\', output_format=\'text\')\n\nexpected_codepoints = ((0, 1), (2, 4), (5, 8), (9, 15), (16, 20))\nexpected_characters = ((0, 1), (2, 4), (5, 10), (11, 17), (18, 22))\ncodepoint_doc = ""I am \xf0\x9d\x92\x9a\xcc\x82\xf0\x9d\x92\x8a random text""\n\ndef test_codepoints():\n    """""" Test case of asking for codepoints from the English tokenizer """"""\n    with corenlp.CoreNLPClient(annotators=[\'tokenize\',\'ssplit\'], # \'depparse\',\'coref\'],\n                               properties={\'tokenize.codepoint\': \'true\'}) as client:\n        ann = client.annotate(codepoint_doc)\n        for i, (codepoints, characters) in enumerate(zip(expected_codepoints, expected_characters)):\n            token = ann.sentence[0].token[i]\n            assert token.codepointOffsetBegin == codepoints[0]\n            assert token.codepointOffsetEnd == codepoints[1]\n            assert token.beginChar == characters[0]\n            assert token.endChar == characters[1]\n'"
tests/test_server_request.py,0,"b'""""""\nTests for setting request properties of servers\n""""""\n\nimport json\nimport pytest\nimport stanza.server as corenlp\n\nfrom stanza.protobuf import Document\nfrom tests import TEST_WORKING_DIR, compare_ignoring_whitespace\n\npytestmark = pytest.mark.client\n\nEN_DOC = ""Joe Smith lives in California.""\n\n# results with an example properties file\nEN_DOC_GOLD = """"""\nSentence #1 (6 tokens):\nJoe Smith lives in California.\n\nTokens:\n[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP]\n[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP]\n[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ]\n[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN]\n[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP]\n[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=.]\n""""""\n\nGERMAN_DOC = ""Angela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland.""\n\nGERMAN_DOC_GOLD = """"""\nSentence #1 (10 tokens):\nAngela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland.\n\nTokens:\n[Text=Angela CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=PROPN Lemma=angela NamedEntityTag=PERSON]\n[Text=Merkel CharacterOffsetBegin=7 CharacterOffsetEnd=13 PartOfSpeech=PROPN Lemma=merkel NamedEntityTag=PERSON]\n[Text=ist CharacterOffsetBegin=14 CharacterOffsetEnd=17 PartOfSpeech=AUX Lemma=ist NamedEntityTag=O]\n[Text=seit CharacterOffsetBegin=18 CharacterOffsetEnd=22 PartOfSpeech=ADP Lemma=seit NamedEntityTag=O]\n[Text=2005 CharacterOffsetBegin=23 CharacterOffsetEnd=27 PartOfSpeech=NUM Lemma=2005 NamedEntityTag=O]\n[Text=Bundeskanzlerin CharacterOffsetBegin=28 CharacterOffsetEnd=43 PartOfSpeech=NOUN Lemma=bundeskanzlerin NamedEntityTag=O]\n[Text=der CharacterOffsetBegin=44 CharacterOffsetEnd=47 PartOfSpeech=DET Lemma=der NamedEntityTag=O]\n[Text=Bundesrepublik CharacterOffsetBegin=48 CharacterOffsetEnd=62 PartOfSpeech=PROPN Lemma=bundesrepublik NamedEntityTag=LOCATION]\n[Text=Deutschland CharacterOffsetBegin=63 CharacterOffsetEnd=74 PartOfSpeech=PROPN Lemma=deutschland NamedEntityTag=LOCATION]\n[Text=. CharacterOffsetBegin=74 CharacterOffsetEnd=75 PartOfSpeech=PUNCT Lemma=. NamedEntityTag=O]\n\nDependency Parse (enhanced plus plus dependencies):\nroot(ROOT-0, Bundeskanzlerin-6)\nnsubj(Bundeskanzlerin-6, Angela-1)\nflat(Angela-1, Merkel-2)\ncop(Bundeskanzlerin-6, ist-3)\ncase(2005-5, seit-4)\nnmod:seit(Bundeskanzlerin-6, 2005-5)\ndet(Bundesrepublik-8, der-7)\nnmod(Bundeskanzlerin-6, Bundesrepublik-8)\nappos(Bundesrepublik-8, Deutschland-9)\npunct(Bundeskanzlerin-6, .-10)\n\nExtracted the following NER entity mentions:\nAngela Merkel   PERSON  PERSON:0.9999981583355767\nBundesrepublik Deutschland      LOCATION        LOCATION:0.968290232887181\n""""""\n\nFRENCH_CUSTOM_PROPS = {\'annotators\': \'tokenize,ssplit,mwt,pos,parse\',\n                       \'tokenize.language\': \'fr\',\n                       \'pos.model\': \'edu/stanford/nlp/models/pos-tagger/french-ud.tagger\',\n                       \'parse.model\': \'edu/stanford/nlp/models/srparser/frenchSR.ser.gz\',\n                       \'mwt.mappingFile\': \'edu/stanford/nlp/models/mwt/french/french-mwt.tsv\',\n                       \'mwt.pos.model\': \'edu/stanford/nlp/models/mwt/french/french-mwt.tagger\',\n                       \'mwt.statisticalMappingFile\': \'edu/stanford/nlp/models/mwt/french/french-mwt-statistical.tsv\',\n                       \'mwt.preserveCasing\': \'false\',\n                       \'outputFormat\': \'text\'}\n\nFRENCH_EXTRA_PROPS = {\'annotators\': \'tokenize,ssplit,mwt,pos,depparse\',\n                      \'tokenize.language\': \'fr\',\n                      \'pos.model\': \'edu/stanford/nlp/models/pos-tagger/french-ud.tagger\',\n                      \'mwt.mappingFile\': \'edu/stanford/nlp/models/mwt/french/french-mwt.tsv\',\n                      \'mwt.pos.model\': \'edu/stanford/nlp/models/mwt/french/french-mwt.tagger\',\n                      \'mwt.statisticalMappingFile\': \'edu/stanford/nlp/models/mwt/french/french-mwt-statistical.tsv\',\n                      \'mwt.preserveCasing\': \'false\',\n                      \'depparse.model\': \'edu/stanford/nlp/models/parser/nndep/UD_French.gz\'}\n\nFRENCH_DOC = ""Cette enqu\xc3\xaate pr\xc3\xa9liminaire fait suite aux r\xc3\xa9v\xc3\xa9lations de l\xe2\x80\x99hebdomadaire quelques jours plus t\xc3\xb4t.""\n\nFRENCH_CUSTOM_GOLD = """"""\nSentence #1 (16 tokens):\nCette enqu\xc3\xaate pr\xc3\xa9liminaire fait suite aux r\xc3\xa9v\xc3\xa9lations de l\xe2\x80\x99hebdomadaire quelques jours plus t\xc3\xb4t.\n\nTokens:\n[Text=Cette CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=DET]\n[Text=enqu\xc3\xaate CharacterOffsetBegin=6 CharacterOffsetEnd=13 PartOfSpeech=NOUN]\n[Text=pr\xc3\xa9liminaire CharacterOffsetBegin=14 CharacterOffsetEnd=26 PartOfSpeech=ADJ]\n[Text=fait CharacterOffsetBegin=27 CharacterOffsetEnd=31 PartOfSpeech=VERB]\n[Text=suite CharacterOffsetBegin=32 CharacterOffsetEnd=37 PartOfSpeech=NOUN]\n[Text=\xc3\xa0 CharacterOffsetBegin=38 CharacterOffsetEnd=41 PartOfSpeech=ADP]\n[Text=les CharacterOffsetBegin=38 CharacterOffsetEnd=41 PartOfSpeech=DET]\n[Text=r\xc3\xa9v\xc3\xa9lations CharacterOffsetBegin=42 CharacterOffsetEnd=53 PartOfSpeech=NOUN]\n[Text=de CharacterOffsetBegin=54 CharacterOffsetEnd=56 PartOfSpeech=ADP]\n[Text=l\xe2\x80\x99 CharacterOffsetBegin=57 CharacterOffsetEnd=59 PartOfSpeech=NOUN]\n[Text=hebdomadaire CharacterOffsetBegin=59 CharacterOffsetEnd=71 PartOfSpeech=ADJ]\n[Text=quelques CharacterOffsetBegin=72 CharacterOffsetEnd=80 PartOfSpeech=DET]\n[Text=jours CharacterOffsetBegin=81 CharacterOffsetEnd=86 PartOfSpeech=NOUN]\n[Text=plus CharacterOffsetBegin=87 CharacterOffsetEnd=91 PartOfSpeech=ADV]\n[Text=t\xc3\xb4t CharacterOffsetBegin=92 CharacterOffsetEnd=95 PartOfSpeech=ADV]\n[Text=. CharacterOffsetBegin=95 CharacterOffsetEnd=96 PartOfSpeech=PUNCT]\n\nConstituency parse: \n(ROOT\n  (SENT\n    (NP (DET Cette)\n      (MWN (NOUN enqu\xc3\xaate) (ADJ pr\xc3\xa9liminaire)))\n    (VN\n      (MWV (VERB fait) (NOUN suite)))\n    (PP (ADP \xc3\xa0)\n      (NP (DET les) (NOUN r\xc3\xa9v\xc3\xa9lations)\n        (PP (ADP de)\n          (NP (NOUN l\xe2\x80\x99)\n            (AP (ADJ hebdomadaire))))))\n    (NP (DET quelques) (NOUN jours))\n    (AdP (ADV plus) (ADV t\xc3\xb4t))\n    (PUNCT .)))\n\n\nBinary Constituency parse: \n(ROOT\n  (SENT\n    (NP (DET Cette)\n      (MWN (NOUN enqu\xc3\xaate) (ADJ pr\xc3\xa9liminaire)))\n    (@SENT\n      (@SENT\n        (@SENT\n          (@SENT\n            (VN\n              (MWV (VERB fait) (NOUN suite)))\n            (PP (ADP \xc3\xa0)\n              (NP\n                (@NP (DET les) (NOUN r\xc3\xa9v\xc3\xa9lations))\n                (PP (ADP de)\n                  (NP (NOUN l\xe2\x80\x99)\n                    (AP (ADJ hebdomadaire)))))))\n          (NP (DET quelques) (NOUN jours)))\n        (AdP (ADV plus) (ADV t\xc3\xb4t)))\n      (PUNCT .))))\n""""""\n\nFRENCH_EXTRA_GOLD = """"""\nSentence #1 (16 tokens):\nCette enqu\xc3\xaate pr\xc3\xa9liminaire fait suite aux r\xc3\xa9v\xc3\xa9lations de l\xe2\x80\x99hebdomadaire quelques jours plus t\xc3\xb4t.\n\nTokens:\n[Text=Cette CharacterOffsetBegin=0 CharacterOffsetEnd=5 PartOfSpeech=DET]\n[Text=enqu\xc3\xaate CharacterOffsetBegin=6 CharacterOffsetEnd=13 PartOfSpeech=NOUN]\n[Text=pr\xc3\xa9liminaire CharacterOffsetBegin=14 CharacterOffsetEnd=26 PartOfSpeech=ADJ]\n[Text=fait CharacterOffsetBegin=27 CharacterOffsetEnd=31 PartOfSpeech=VERB]\n[Text=suite CharacterOffsetBegin=32 CharacterOffsetEnd=37 PartOfSpeech=NOUN]\n[Text=\xc3\xa0 CharacterOffsetBegin=38 CharacterOffsetEnd=41 PartOfSpeech=ADP]\n[Text=les CharacterOffsetBegin=38 CharacterOffsetEnd=41 PartOfSpeech=DET]\n[Text=r\xc3\xa9v\xc3\xa9lations CharacterOffsetBegin=42 CharacterOffsetEnd=53 PartOfSpeech=NOUN]\n[Text=de CharacterOffsetBegin=54 CharacterOffsetEnd=56 PartOfSpeech=ADP]\n[Text=l\xe2\x80\x99 CharacterOffsetBegin=57 CharacterOffsetEnd=59 PartOfSpeech=NOUN]\n[Text=hebdomadaire CharacterOffsetBegin=59 CharacterOffsetEnd=71 PartOfSpeech=ADJ]\n[Text=quelques CharacterOffsetBegin=72 CharacterOffsetEnd=80 PartOfSpeech=DET]\n[Text=jours CharacterOffsetBegin=81 CharacterOffsetEnd=86 PartOfSpeech=NOUN]\n[Text=plus CharacterOffsetBegin=87 CharacterOffsetEnd=91 PartOfSpeech=ADV]\n[Text=t\xc3\xb4t CharacterOffsetBegin=92 CharacterOffsetEnd=95 PartOfSpeech=ADV]\n[Text=. CharacterOffsetBegin=95 CharacterOffsetEnd=96 PartOfSpeech=PUNCT]\n\nDependency Parse (enhanced plus plus dependencies):\nroot(ROOT-0, fait-4)\ndet(enqu\xc3\xaate-2, Cette-1)\nnsubj(fait-4, enqu\xc3\xaate-2)\namod(enqu\xc3\xaate-2, pr\xc3\xa9liminaire-3)\nobj(fait-4, suite-5)\ncase(r\xc3\xa9v\xc3\xa9lations-8, \xc3\xa0-6)\ndet(r\xc3\xa9v\xc3\xa9lations-8, les-7)\nobl:\xc3\xa0(fait-4, r\xc3\xa9v\xc3\xa9lations-8)\ncase(l\xe2\x80\x99-10, de-9)\nnmod:de(r\xc3\xa9v\xc3\xa9lations-8, l\xe2\x80\x99-10)\namod(r\xc3\xa9v\xc3\xa9lations-8, hebdomadaire-11)\ndet(jours-13, quelques-12)\nobl(fait-4, jours-13)\nadvmod(t\xc3\xb4t-15, plus-14)\nadvmod(jours-13, t\xc3\xb4t-15)\npunct(fait-4, .-16)\n""""""\n\nFRENCH_JSON_GOLD = json.loads(open(f\'{TEST_WORKING_DIR}/out/example_french.json\').read())\n\nES_DOC = \'Andr\xc3\xa9s Manuel L\xc3\xb3pez Obrador es el presidente de M\xc3\xa9xico.\'\n\nES_PROPS = {\'annotators\': \'tokenize,ssplit,mwt,pos,depparse\', \'tokenize.language\': \'es\',\n            \'pos.model\': \'edu/stanford/nlp/models/pos-tagger/spanish-ud.tagger\',\n            \'mwt.mappingFile\': \'edu/stanford/nlp/models/mwt/spanish/spanish-mwt.tsv\',\n            \'depparse.model\': \'edu/stanford/nlp/models/parser/nndep/UD_Spanish.gz\'}\n\nES_PROPS_GOLD = """"""\nSentence #1 (10 tokens):\nAndr\xc3\xa9s Manuel L\xc3\xb3pez Obrador es el presidente de M\xc3\xa9xico.\n\nTokens:\n[Text=Andr\xc3\xa9s CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=PROPN]\n[Text=Manuel CharacterOffsetBegin=7 CharacterOffsetEnd=13 PartOfSpeech=PROPN]\n[Text=L\xc3\xb3pez CharacterOffsetBegin=14 CharacterOffsetEnd=19 PartOfSpeech=PROPN]\n[Text=Obrador CharacterOffsetBegin=20 CharacterOffsetEnd=27 PartOfSpeech=PROPN]\n[Text=es CharacterOffsetBegin=28 CharacterOffsetEnd=30 PartOfSpeech=AUX]\n[Text=el CharacterOffsetBegin=31 CharacterOffsetEnd=33 PartOfSpeech=DET]\n[Text=presidente CharacterOffsetBegin=34 CharacterOffsetEnd=44 PartOfSpeech=NOUN]\n[Text=de CharacterOffsetBegin=45 CharacterOffsetEnd=47 PartOfSpeech=ADP]\n[Text=M\xc3\xa9xico CharacterOffsetBegin=48 CharacterOffsetEnd=54 PartOfSpeech=PROPN]\n[Text=. CharacterOffsetBegin=54 CharacterOffsetEnd=55 PartOfSpeech=PUNCT]\n\nDependency Parse (enhanced plus plus dependencies):\nroot(ROOT-0, presidente-7)\nnsubj(presidente-7, Andr\xc3\xa9s-1)\nflat(Andr\xc3\xa9s-1, Manuel-2)\nflat(Andr\xc3\xa9s-1, L\xc3\xb3pez-3)\nflat(Andr\xc3\xa9s-1, Obrador-4)\ncop(presidente-7, es-5)\ndet(presidente-7, el-6)\ncase(M\xc3\xa9xico-9, de-8)\nnmod:de(presidente-7, M\xc3\xa9xico-9)\npunct(presidente-7, .-10)\n""""""\n\n\n@pytest.fixture(scope=""module"")\ndef corenlp_client():\n    """""" Client to run tests on """"""\n    client = corenlp.CoreNLPClient(annotators=\'tokenize,ssplit,pos\', server_id=\'stanza_request_tests_server\')\n    client.register_properties_key(\'fr-custom\', FRENCH_CUSTOM_PROPS)\n    yield client\n    client.stop()\n\n\ndef test_basic(corenlp_client):\n    """""" Basic test of making a request, test default output format is a Document """"""\n    ann = corenlp_client.annotate(EN_DOC, output_format=""text"")\n    assert ann.strip() == EN_DOC_GOLD.strip()\n    ann = corenlp_client.annotate(EN_DOC)\n    assert isinstance(ann, Document)\n\n\ndef test_python_dict(corenlp_client):\n    """""" Test using a Python dictionary to specify all request properties """"""\n    ann = corenlp_client.annotate(ES_DOC, properties=ES_PROPS, output_format=""text"")\n    assert ann.strip() == ES_PROPS_GOLD.strip()\n\n\ndef test_properties_key_and_python_dict(corenlp_client):\n    """""" Test using a properties key and additional properties """"""\n    ann = corenlp_client.annotate(FRENCH_DOC, properties_key=\'fr-custom\', properties=FRENCH_EXTRA_PROPS)\n    assert ann.strip() == FRENCH_EXTRA_GOLD.strip()\n\n\ndef test_properties_key(corenlp_client):\n    """""" Test using the properties_key which was registered with the properties cache """"""\n    ann = corenlp_client.annotate(FRENCH_DOC, properties_key=\'fr-custom\')\n    assert ann.strip() == FRENCH_CUSTOM_GOLD.strip()\n\n\ndef test_switching_back_and_forth(corenlp_client):\n    """""" Test using a properties key, then properties key with python dict, then back to just properties key """"""\n    ann = corenlp_client.annotate(FRENCH_DOC, properties_key=\'fr-custom\')\n    assert ann.strip() == FRENCH_CUSTOM_GOLD.strip()\n    ann = corenlp_client.annotate(FRENCH_DOC, properties_key=\'fr-custom\', properties=FRENCH_EXTRA_PROPS)\n    assert ann.strip() == FRENCH_EXTRA_GOLD.strip()\n    ann = corenlp_client.annotate(FRENCH_DOC, properties_key=\'fr-custom\')\n    assert ann.strip() == FRENCH_CUSTOM_GOLD.strip()\n\n\ndef test_lang_setting(corenlp_client):\n    """""" Test using a Stanford CoreNLP supported languages as a properties key """"""\n    ann = corenlp_client.annotate(GERMAN_DOC, properties_key=""german"", output_format=""text"")\n    compare_ignoring_whitespace(ann, GERMAN_DOC_GOLD)\n\n\ndef test_annotators_and_output_format(corenlp_client):\n    """""" Test setting the annotators and output_format """"""\n    ann = corenlp_client.annotate(FRENCH_DOC, properties=FRENCH_EXTRA_PROPS,\n                                  annotators=""tokenize,ssplit,mwt,pos"", output_format=""json"")\n    assert FRENCH_JSON_GOLD == ann\n'"
tests/test_server_start.py,0,"b'""""""\nTests for starting a server in Python code\n""""""\n\nimport pytest\nimport stanza.server as corenlp\nfrom stanza.server.client import AnnotationException\nimport time\n\nfrom tests import *\n\npytestmark = pytest.mark.client\n\nEN_DOC = ""Joe Smith lives in California.""\n\n# results on EN_DOC with standard StanfordCoreNLP defaults\nEN_PRELOAD_GOLD = """"""\nSentence #1 (6 tokens):\nJoe Smith lives in California.\n\nTokens:\n[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP Lemma=Joe NamedEntityTag=PERSON]\n[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP Lemma=Smith NamedEntityTag=PERSON]\n[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ Lemma=live NamedEntityTag=O]\n[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN Lemma=in NamedEntityTag=O]\n[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP Lemma=California NamedEntityTag=STATE_OR_PROVINCE]\n[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=. Lemma=. NamedEntityTag=O]\n\nDependency Parse (enhanced plus plus dependencies):\nroot(ROOT-0, lives-3)\ncompound(Smith-2, Joe-1)\nnsubj(lives-3, Smith-2)\ncase(California-5, in-4)\nobl:in(lives-3, California-5)\npunct(lives-3, .-6)\n\nExtracted the following NER entity mentions:\nJoe Smith\tPERSON  PERSON:0.9972202689478088\nCalifornia\tSTATE_OR_PROVINCE       LOCATION:0.9990868267002156\n""""""\n\n# results with an example properties file\nEN_PROPS_FILE_GOLD = """"""\nSentence #1 (6 tokens):\nJoe Smith lives in California.\n\nTokens:\n[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP]\n[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP]\n[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ]\n[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN]\n[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP]\n[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=.]\n""""""\n\nGERMAN_DOC = ""Angela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland.""\n\n# results with standard German properties\nGERMAN_FULL_PROPS_GOLD = """"""\nSentence #1 (10 tokens):\nAngela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland.\n\nTokens:\n[Text=Angela CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=PROPN Lemma=angela NamedEntityTag=PERSON]\n[Text=Merkel CharacterOffsetBegin=7 CharacterOffsetEnd=13 PartOfSpeech=PROPN Lemma=merkel NamedEntityTag=PERSON]\n[Text=ist CharacterOffsetBegin=14 CharacterOffsetEnd=17 PartOfSpeech=AUX Lemma=ist NamedEntityTag=O]\n[Text=seit CharacterOffsetBegin=18 CharacterOffsetEnd=22 PartOfSpeech=ADP Lemma=seit NamedEntityTag=O]\n[Text=2005 CharacterOffsetBegin=23 CharacterOffsetEnd=27 PartOfSpeech=NUM Lemma=2005 NamedEntityTag=O]\n[Text=Bundeskanzlerin CharacterOffsetBegin=28 CharacterOffsetEnd=43 PartOfSpeech=NOUN Lemma=bundeskanzlerin NamedEntityTag=O]\n[Text=der CharacterOffsetBegin=44 CharacterOffsetEnd=47 PartOfSpeech=DET Lemma=der NamedEntityTag=O]\n[Text=Bundesrepublik CharacterOffsetBegin=48 CharacterOffsetEnd=62 PartOfSpeech=PROPN Lemma=bundesrepublik NamedEntityTag=LOCATION]\n[Text=Deutschland CharacterOffsetBegin=63 CharacterOffsetEnd=74 PartOfSpeech=PROPN Lemma=deutschland NamedEntityTag=LOCATION]\n[Text=. CharacterOffsetBegin=74 CharacterOffsetEnd=75 PartOfSpeech=PUNCT Lemma=. NamedEntityTag=O]\n\nDependency Parse (enhanced plus plus dependencies):\nroot(ROOT-0, Bundeskanzlerin-6)\nnsubj(Bundeskanzlerin-6, Angela-1)\nflat(Angela-1, Merkel-2)\ncop(Bundeskanzlerin-6, ist-3)\ncase(2005-5, seit-4)\nnmod:seit(Bundeskanzlerin-6, 2005-5)\ndet(Bundesrepublik-8, der-7)\nnmod(Bundeskanzlerin-6, Bundesrepublik-8)\nappos(Bundesrepublik-8, Deutschland-9)\npunct(Bundeskanzlerin-6, .-10)\n\nExtracted the following NER entity mentions:\nAngela Merkel   PERSON  PERSON:0.9999981583355767\nBundesrepublik Deutschland      LOCATION        LOCATION:0.968290232887181\n""""""\n\n\nGERMAN_SMALL_PROPS = {\'annotators\': \'tokenize,ssplit,pos\', \'tokenize.language\': \'de\',\n                      \'pos.model\': \'edu/stanford/nlp/models/pos-tagger/german-ud.tagger\'}\n\n# results with custom Python dictionary set properties\nGERMAN_SMALL_PROPS_GOLD = """"""\nSentence #1 (10 tokens):\nAngela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland.\n\nTokens:\n[Text=Angela CharacterOffsetBegin=0 CharacterOffsetEnd=6 PartOfSpeech=PROPN]\n[Text=Merkel CharacterOffsetBegin=7 CharacterOffsetEnd=13 PartOfSpeech=PROPN]\n[Text=ist CharacterOffsetBegin=14 CharacterOffsetEnd=17 PartOfSpeech=AUX]\n[Text=seit CharacterOffsetBegin=18 CharacterOffsetEnd=22 PartOfSpeech=ADP]\n[Text=2005 CharacterOffsetBegin=23 CharacterOffsetEnd=27 PartOfSpeech=NUM]\n[Text=Bundeskanzlerin CharacterOffsetBegin=28 CharacterOffsetEnd=43 PartOfSpeech=NOUN]\n[Text=der CharacterOffsetBegin=44 CharacterOffsetEnd=47 PartOfSpeech=DET]\n[Text=Bundesrepublik CharacterOffsetBegin=48 CharacterOffsetEnd=62 PartOfSpeech=PROPN]\n[Text=Deutschland CharacterOffsetBegin=63 CharacterOffsetEnd=74 PartOfSpeech=PROPN]\n[Text=. CharacterOffsetBegin=74 CharacterOffsetEnd=75 PartOfSpeech=PUNCT]\n""""""\n\n# results with custom Python dictionary set properties and annotators=tokenize,ssplit\nGERMAN_SMALL_PROPS_W_ANNOTATORS_GOLD = """"""\nSentence #1 (10 tokens):\nAngela Merkel ist seit 2005 Bundeskanzlerin der Bundesrepublik Deutschland.\n\nTokens:\n[Text=Angela CharacterOffsetBegin=0 CharacterOffsetEnd=6]\n[Text=Merkel CharacterOffsetBegin=7 CharacterOffsetEnd=13]\n[Text=ist CharacterOffsetBegin=14 CharacterOffsetEnd=17]\n[Text=seit CharacterOffsetBegin=18 CharacterOffsetEnd=22]\n[Text=2005 CharacterOffsetBegin=23 CharacterOffsetEnd=27]\n[Text=Bundeskanzlerin CharacterOffsetBegin=28 CharacterOffsetEnd=43]\n[Text=der CharacterOffsetBegin=44 CharacterOffsetEnd=47]\n[Text=Bundesrepublik CharacterOffsetBegin=48 CharacterOffsetEnd=62]\n[Text=Deutschland CharacterOffsetBegin=63 CharacterOffsetEnd=74]\n[Text=. CharacterOffsetBegin=74 CharacterOffsetEnd=75]\n""""""\n\n# properties for username/password example\nUSERNAME_PASS_PROPS = {\'annotators\': \'tokenize,ssplit,pos\'}\n\nUSERNAME_PASS_GOLD = """"""\nSentence #1 (6 tokens):\nJoe Smith lives in California.\n\nTokens:\n[Text=Joe CharacterOffsetBegin=0 CharacterOffsetEnd=3 PartOfSpeech=NNP]\n[Text=Smith CharacterOffsetBegin=4 CharacterOffsetEnd=9 PartOfSpeech=NNP]\n[Text=lives CharacterOffsetBegin=10 CharacterOffsetEnd=15 PartOfSpeech=VBZ]\n[Text=in CharacterOffsetBegin=16 CharacterOffsetEnd=18 PartOfSpeech=IN]\n[Text=California CharacterOffsetBegin=19 CharacterOffsetEnd=29 PartOfSpeech=NNP]\n[Text=. CharacterOffsetBegin=29 CharacterOffsetEnd=30 PartOfSpeech=.]\n""""""\n\n\ndef annotate_and_time(client, text, properties={}):\n    """""" Submit an annotation request and return how long it took """"""\n    start = time.time()\n    ann = client.annotate(text, properties=properties, output_format=""text"")\n    end = time.time()\n    return {\'annotation\': ann, \'start_time\': start, \'end_time\': end}\n\ndef test_preload():\n    """""" Test that the default annotators load fully immediately upon server start """"""\n    with corenlp.CoreNLPClient(server_id=\'test_server_start_preload\') as client:\n        # wait for annotators to load\n        time.sleep(140)\n        results = annotate_and_time(client, EN_DOC)\n        compare_ignoring_whitespace(results[\'annotation\'], EN_PRELOAD_GOLD)\n        assert results[\'end_time\'] - results[\'start_time\'] < 3\n\n\ndef test_props_file():\n    """""" Test starting the server with a props file """"""\n    with corenlp.CoreNLPClient(properties=SERVER_TEST_PROPS, server_id=\'test_server_start_props_file\') as client:\n        ann = client.annotate(EN_DOC, output_format=""text"")\n        assert ann.strip() == EN_PROPS_FILE_GOLD.strip()\n\n\ndef test_lang_start():\n    """""" Test starting the server with a Stanford CoreNLP language name """"""\n    with corenlp.CoreNLPClient(properties=\'german\', server_id=\'test_server_start_lang_name\') as client:\n        ann = client.annotate(GERMAN_DOC, output_format=\'text\')\n        compare_ignoring_whitespace(ann, GERMAN_FULL_PROPS_GOLD)\n\n\ndef test_python_dict():\n    """""" Test starting the server with a Python dictionary as default properties """"""\n    with corenlp.CoreNLPClient(properties=GERMAN_SMALL_PROPS, server_id=\'test_server_start_python_dict\') as client:\n        ann = client.annotate(GERMAN_DOC, output_format=\'text\')\n        assert ann.strip() == GERMAN_SMALL_PROPS_GOLD.strip()\n\n\ndef test_python_dict_w_annotators():\n    """""" Test starting the server with a Python dictionary as default properties, override annotators """"""\n    with corenlp.CoreNLPClient(properties=GERMAN_SMALL_PROPS, annotators=""tokenize,ssplit"",\n                               server_id=\'test_server_start_python_dict_w_annotators\') as client:\n        ann = client.annotate(GERMAN_DOC, output_format=\'text\')\n        assert ann.strip() == GERMAN_SMALL_PROPS_W_ANNOTATORS_GOLD.strip()\n\n\ndef test_username_password():\n    """""" Test starting a server with a username and password """"""\n    with corenlp.CoreNLPClient(properties=USERNAME_PASS_PROPS, username=\'user-1234\', password=\'1234\',\n                               server_id=""test_server_username_pass"") as client:\n        # check with correct password\n        ann = client.annotate(EN_DOC, output_format=\'text\', username=\'user-1234\', password=\'1234\')\n        assert ann.strip() == USERNAME_PASS_GOLD.strip()\n        # check with incorrect password, should throw AnnotationException\n        try:\n            ann = client.annotate(EN_DOC, output_format=\'text\', username=\'user-1234\', password=\'12345\')\n            assert False\n        except AnnotationException as ae:\n            pass\n        except Exception as e:\n            assert False\n\n\n'"
tests/test_tagger.py,0,"b'""""""\nBasic testing of part of speech tagging\n""""""\n\nimport pytest\nimport stanza\n\nfrom tests import *\n\npytestmark = pytest.mark.pipeline\n\nEN_DOC = ""Joe Smith was born in California.""\n\nEN_DOC_GOLD = """"""\n<Token id=1;words=[<Word id=1;text=Joe;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n<Token id=2;words=[<Word id=2;text=Smith;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n<Token id=3;words=[<Word id=3;text=was;upos=AUX;xpos=VBD;feats=Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin>]>\n<Token id=4;words=[<Word id=4;text=born;upos=VERB;xpos=VBN;feats=Tense=Past|VerbForm=Part|Voice=Pass>]>\n<Token id=5;words=[<Word id=5;text=in;upos=ADP;xpos=IN>]>\n<Token id=6;words=[<Word id=6;text=California;upos=PROPN;xpos=NNP;feats=Number=Sing>]>\n<Token id=7;words=[<Word id=7;text=.;upos=PUNCT;xpos=.>]>\n"""""".strip()\n\n\ndef test_part_of_speech():\n    nlp = stanza.Pipeline(**{\'processors\': \'tokenize,pos\', \'dir\': TEST_MODELS_DIR, \'lang\': \'en\'})\n    doc = nlp(EN_DOC)\n    assert EN_DOC_GOLD == \'\\n\\n\'.join([sent.tokens_string() for sent in doc.sentences])\n'"
tests/test_tokenizer.py,0,"b'""""""\nBasic testing of tokenization\n""""""\n\nimport pytest\nimport stanza\n\nfrom tests import *\n\npytestmark = pytest.mark.pipeline\n\nEN_DOC = ""Joe Smith lives in California. Joe\'s favorite food is pizza. He enjoys going to the beach.""\n\nEN_DOC_GOLD_TOKENS = """"""\n<Token id=1;words=[<Word id=1;text=Joe>]>\n<Token id=2;words=[<Word id=2;text=Smith>]>\n<Token id=3;words=[<Word id=3;text=lives>]>\n<Token id=4;words=[<Word id=4;text=in>]>\n<Token id=5;words=[<Word id=5;text=California>]>\n<Token id=6;words=[<Word id=6;text=.>]>\n\n<Token id=1;words=[<Word id=1;text=Joe>]>\n<Token id=2;words=[<Word id=2;text=\'s>]>\n<Token id=3;words=[<Word id=3;text=favorite>]>\n<Token id=4;words=[<Word id=4;text=food>]>\n<Token id=5;words=[<Word id=5;text=is>]>\n<Token id=6;words=[<Word id=6;text=pizza>]>\n<Token id=7;words=[<Word id=7;text=.>]>\n\n<Token id=1;words=[<Word id=1;text=He>]>\n<Token id=2;words=[<Word id=2;text=enjoys>]>\n<Token id=3;words=[<Word id=3;text=going>]>\n<Token id=4;words=[<Word id=4;text=to>]>\n<Token id=5;words=[<Word id=5;text=the>]>\n<Token id=6;words=[<Word id=6;text=beach>]>\n<Token id=7;words=[<Word id=7;text=.>]>\n"""""".strip()\n\n\nEN_DOC_PRETOKENIZED = \\\n    ""Joe Smith lives in California .\\nJoe\'s favorite  food is  pizza .\\n\\nHe enjoys going to the beach.\\n""\n\nEN_DOC_PRETOKENIZED_GOLD_TOKENS = """"""\n<Token id=1;words=[<Word id=1;text=Joe>]>\n<Token id=2;words=[<Word id=2;text=Smith>]>\n<Token id=3;words=[<Word id=3;text=lives>]>\n<Token id=4;words=[<Word id=4;text=in>]>\n<Token id=5;words=[<Word id=5;text=California>]>\n<Token id=6;words=[<Word id=6;text=.>]>\n\n<Token id=1;words=[<Word id=1;text=Joe\'s>]>\n<Token id=2;words=[<Word id=2;text=favorite>]>\n<Token id=3;words=[<Word id=3;text=food>]>\n<Token id=4;words=[<Word id=4;text=is>]>\n<Token id=5;words=[<Word id=5;text=pizza>]>\n<Token id=6;words=[<Word id=6;text=.>]>\n\n<Token id=1;words=[<Word id=1;text=He>]>\n<Token id=2;words=[<Word id=2;text=enjoys>]>\n<Token id=3;words=[<Word id=3;text=going>]>\n<Token id=4;words=[<Word id=4;text=to>]>\n<Token id=5;words=[<Word id=5;text=the>]>\n<Token id=6;words=[<Word id=6;text=beach.>]>\n"""""".strip()\n\n\nEN_DOC_PRETOKENIZED_LIST = [[\'Joe\', \'Smith\', \'lives\', \'in\', \'California\', \'.\'], [\'He\', \'loves\', \'pizza\', \'.\']]\n\nEN_DOC_PRETOKENIZED_LIST_GOLD_TOKENS = """"""\n<Token id=1;words=[<Word id=1;text=Joe>]>\n<Token id=2;words=[<Word id=2;text=Smith>]>\n<Token id=3;words=[<Word id=3;text=lives>]>\n<Token id=4;words=[<Word id=4;text=in>]>\n<Token id=5;words=[<Word id=5;text=California>]>\n<Token id=6;words=[<Word id=6;text=.>]>\n\n<Token id=1;words=[<Word id=1;text=He>]>\n<Token id=2;words=[<Word id=2;text=loves>]>\n<Token id=3;words=[<Word id=3;text=pizza>]>\n<Token id=4;words=[<Word id=4;text=.>]>\n"""""".strip()\n\nEN_DOC_NO_SSPLIT = [""This is a sentence. This is another."", ""This is a third.""]\n\nEN_DOC_NO_SSPLIT_SENTENCES = [[\'This\', \'is\', \'a\', \'sentence\', \'.\', \'This\', \'is\', \'another\', \'.\'], [\'This\', \'is\', \'a\', \'third\', \'.\']]\n\n\ndef test_tokenize():\n    nlp = stanza.Pipeline(processors=\'tokenize\', dir=TEST_MODELS_DIR, lang=\'en\')\n    doc = nlp(EN_DOC)\n    assert EN_DOC_GOLD_TOKENS == \'\\n\\n\'.join([sent.tokens_string() for sent in doc.sentences])\n    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])\n\n\ndef test_pretokenized():\n    nlp = stanza.Pipeline(**{\'processors\': \'tokenize\', \'dir\': TEST_MODELS_DIR, \'lang\': \'en\',\n                                  \'tokenize_pretokenized\': True})\n    doc = nlp(EN_DOC_PRETOKENIZED)\n    assert EN_DOC_PRETOKENIZED_GOLD_TOKENS == \'\\n\\n\'.join([sent.tokens_string() for sent in doc.sentences])\n    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])\n    doc = nlp(EN_DOC_PRETOKENIZED_LIST)\n    assert EN_DOC_PRETOKENIZED_LIST_GOLD_TOKENS == \'\\n\\n\'.join([sent.tokens_string() for sent in doc.sentences])\n    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])\n\ndef test_no_ssplit():\n    nlp = stanza.Pipeline(**{\'processors\': \'tokenize\', \'dir\': TEST_MODELS_DIR, \'lang\': \'en\',\n                                  \'tokenize_no_ssplit\': True})\n\n    doc = nlp(EN_DOC_NO_SSPLIT)\n    assert EN_DOC_NO_SSPLIT_SENTENCES == [[w.text for w in s.words] for s in doc.sentences]\n    assert all([doc.text[token._start_char: token._end_char] == token.text for sent in doc.sentences for token in sent.tokens])'"
stanza/models/__init__.py,0,b''
stanza/models/_training_logging.py,0,"b'import logging.config\nlogging.config.dictConfig(\n    {\n        ""version"": 1,\n        ""disable_existing_loggers"": False,\n        ""formatters"": {\n            ""minimal"": {\n                ""format"": ""%(message)s"",\n                }\n            },\n        ""handlers"": {\n            ""console"": {\n                ""class"": ""logging.StreamHandler"",\n                ""formatter"": ""minimal"",\n            }\n        },\n        ""loggers"": {\n            """": {""handlers"": [""console""], ""level"": ""DEBUG""}\n        },\n    }\n)\n'"
stanza/models/charlm.py,14,"b'""""""\nEntry point for training and evaluating a character-level neural language model.\n""""""\n\nimport random\nimport argparse\nfrom copy import copy\nfrom collections import Counter\nimport numpy as np\nimport torch\nimport math\nimport logging\nimport time\nimport os\n\nfrom stanza.models.common.char_model import CharacterLanguageModel\nfrom stanza.models.pos.vocab import CharVocab\nfrom stanza.models.common import utils\nfrom stanza.models import _training_logging\n\n# modify logging format\nformatter = logging.Formatter(fmt=\'%(asctime)s %(message)s\', datefmt=""%Y-%m-%d %H:%M:%S"")\nfor h in logging.getLogger().handlers:\n    h.setFormatter(formatter)\n\nlogger = logging.getLogger(\'stanza\')\n\ndef repackage_hidden(h):\n    """"""Wraps hidden states in new Tensors,\n    to detach them from their history.""""""\n    if isinstance(h, torch.Tensor):\n        return h.detach()\n    else:\n        return tuple(repackage_hidden(v) for v in h)\n\ndef batchify(data, bsz):\n    # Work out how cleanly we can divide the dataset into bsz parts.\n    nbatch = data.size(0) // bsz\n    # Trim off any extra elements that wouldn\'t cleanly fit (remainders).\n    data = data.narrow(0, 0, nbatch * bsz)\n    # Evenly divide the data across the bsz batches.\n    data = data.view(bsz, -1) # batch_first is True\n    return data\n\ndef get_batch(source, i, seq_len):\n    seq_len = min(seq_len, source.size(1) - 1 - i)\n    data = source[:, i:i+seq_len]\n    target = source[:, i+1:i+1+seq_len].reshape(-1)\n    return data, target\n\ndef build_vocab(path, cutoff=0):\n    # Requires a large amount of memeory, but only need to build once\n    if os.path.isdir(path):\n        # here we need some trick to deal with excessively large files\n        # for each file we accumulate the counter of characters, and \n        # at the end we simply pass a list of chars to the vocab builder\n        counter = Counter()\n        filenames = sorted(os.listdir(path))\n        for filename in filenames:\n            lines = open(path + \'/\' + filename).readlines()\n            for line in lines:\n                counter.update(list(line))\n        # remove infrequent characters from vocab\n        for k in list(counter.keys()):\n            if counter[k] < cutoff:\n                del counter[k]\n        # a singleton list of all characters\n        data = [sorted([x[0] for x in counter.most_common()])]\n        vocab = CharVocab(data) # skip cutoff argument because this has been dealt with\n    else:\n        lines = open(path).readlines() # reserve \'\\n\'\n        data = [list(line) for line in lines]\n        vocab = CharVocab(data, cutoff=cutoff)\n    return vocab\n\ndef load_file(path, vocab, direction):\n    lines = open(path).readlines() # reserve \'\\n\'\n    data = list(\'\'.join(lines))\n    idx = vocab[\'char\'].map(data)\n    if direction == \'backward\': idx = idx[::-1]\n    return torch.tensor(idx)\n\ndef load_data(path, vocab, direction):\n    if os.path.isdir(path):\n        filenames = sorted(os.listdir(path))\n        for filename in filenames:\n            logging.info(\'Loading data from {}\'.format(filename))\n            data = load_file(path + \'/\' + filename, vocab, direction)\n            yield data\n    else:\n        data = load_file(path, vocab, direction)\n        yield data\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--train_file\', type=str, help=""Input plaintext file"")\n    parser.add_argument(\'--train_dir\', type=str, help=""If non-emtpy, load from directory with multiple training files"")\n    parser.add_argument(\'--eval_file\', type=str, help=""Input plaintext file for the dev/test set"")\n    parser.add_argument(\'--lang\', type=str, help=""Language"")\n    parser.add_argument(\'--shorthand\', type=str, help=""UD treebank shorthand"")\n\n    parser.add_argument(\'--mode\', default=\'train\', choices=[\'train\', \'predict\'])\n    parser.add_argument(\'--direction\', default=\'forward\', choices=[\'forward\', \'backward\'], help=""Forward or backward language model"")\n\n    parser.add_argument(\'--char_emb_dim\', type=int, default=100, help=""Dimension of unit embeddings"")\n    parser.add_argument(\'--char_hidden_dim\', type=int, default=1024, help=""Dimension of hidden units"")\n    parser.add_argument(\'--char_num_layers\', type=int, default=1, help=""Layers of RNN in the language model"")\n    parser.add_argument(\'--char_dropout\', type=float, default=0.05, help=""Dropout probability"")\n    parser.add_argument(\'--char_unit_dropout\', type=float, default=1e-5, help=""Randomly set an input char to UNK during training"")\n    parser.add_argument(\'--char_rec_dropout\', type=float, default=0.0, help=""Recurrent dropout probability"")\n\n    parser.add_argument(\'--batch_size\', type=int, default=100, help=""Batch size to use"")\n    parser.add_argument(\'--bptt_size\', type=int, default=250, help=""Sequence length to consider at a time"")\n    parser.add_argument(\'--epochs\', type=int, default=50, help=""Total epochs to train the model for"")\n    parser.add_argument(\'--max_grad_norm\', type=float, default=0.25, help=""Maximum gradient norm to clip to"")\n    parser.add_argument(\'--lr0\', type=float, default=20, help=""Initial learning rate"")\n    parser.add_argument(\'--anneal\', type=float, default=0.25, help=""Anneal the learning rate by this amount when dev performance deteriorate"")\n    parser.add_argument(\'--patience\', type=int, default=1, help=""Patience for annealing the learning rate"")\n    parser.add_argument(\'--weight_decay\', type=float, default=0.0, help=""Weight decay"")\n    parser.add_argument(\'--momentum\', type=float, default=0.0, help=\'Momentum for SGD.\')\n    parser.add_argument(\'--cutoff\', type=int, default=1000, help=""Frequency cutoff for char vocab. By default we assume a very large corpus."")\n    \n    parser.add_argument(\'--report_steps\', type=int, default=50, help=""Update step interval to report loss"")\n    parser.add_argument(\'--save_name\', type=str, default=None, help=""File name to save the model"")\n    parser.add_argument(\'--vocab_save_name\', type=str, default=None, help=""File name to save the vocab"")\n    parser.add_argument(\'--save_dir\', type=str, default=\'saved_models/charlm\', help=""Directory to save models in"")\n    parser.add_argument(\'--summary\', action=\'store_true\', help=\'Use summary writer to record progress.\')\n    parser.add_argument(\'--cuda\', type=bool, default=torch.cuda.is_available())\n    parser.add_argument(\'--cpu\', action=\'store_true\', help=\'Ignore CUDA and run on CPU.\')\n    parser.add_argument(\'--seed\', type=int, default=1234)\n    args = parser.parse_args()\n    return args\n\ndef main():\n    args = parse_args()\n\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    if args.cpu:\n        args.cuda = False\n    elif args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    args = vars(args)\n    logger.info(""Running {} character-level language model in {} mode"".format(args[\'direction\'], args[\'mode\']))\n    \n    utils.ensure_dir(args[\'save_dir\'])\n\n    if args[\'mode\'] == \'train\':\n        train(args)\n    else:\n        evaluate(args)\n\ndef train_epoch(args, vocab, data, model, params, optimizer, criterion, epoch):\n    model.train()\n    for data_chunk in data:\n        batches = batchify(data_chunk, args[\'batch_size\'])\n        hidden = None\n        total_loss = 0.0\n        total_batches = math.ceil((batches.size(1) - 1) / args[\'bptt_size\'])\n        iteration, i = 0, 0\n        while i < batches.size(1) - 1 - 1:\n            start_time = time.time()\n            bptt = args[\'bptt_size\'] if np.random.random() < 0.95 else args[\'bptt_size\']/ 2.\n            # prevent excessively small or negative sequence lengths\n            seq_len = max(5, int(np.random.normal(bptt, 5)))\n            # prevent very large sequence length, must be <= 1.2 x bptt\n            seq_len = min(seq_len, int(args[\'bptt_size\'] * 1.2))\n            data, target = get_batch(batches, i, seq_len)\n            lens = [data.size(1) for i in range(data.size(0))]\n            if args[\'cuda\']: \n                data = data.cuda()\n                target = target.cuda()\n            \n            optimizer.zero_grad()\n\n            output, hidden, decoded = model.forward(data, lens, hidden)\n\n            loss = criterion(decoded.view(-1, len(vocab[\'char\'])), target)\n            total_loss += loss.data.item()\n            loss.backward()\n\n            torch.nn.utils.clip_grad_norm_(params, args[\'max_grad_norm\'])\n            optimizer.step()\n\n            hidden = repackage_hidden(hidden)\n\n            if (iteration + 1) % args[\'report_steps\'] == 0:\n                cur_loss = total_loss / args[\'report_steps\']\n                elapsed = time.time() - start_time\n                logger.info(\n                    ""| epoch {:5d} | {:5d}/{:5d} batches | sec/batch {:.6f} | loss {:5.2f} | ppl {:8.2f}"".format(\n                        epoch,\n                        iteration + 1,\n                        total_batches,\n                        elapsed / args[\'report_steps\'],\n                        cur_loss,\n                        math.exp(cur_loss),\n                    )\n                )\n                total_loss = 0.0\n\n            iteration += 1\n            i += seq_len\n    return\n\ndef evaluate_epoch(args, vocab, data, model, criterion):\n    model.eval()\n    hidden = None\n    total_loss = 0\n    data = list(data)\n    assert len(data) == 1, \'Only support single dev/test file\'\n    batches = batchify(data[0], args[\'batch_size\'])\n    with torch.no_grad():\n        for i in range(0, batches.size(1) - 1, args[\'bptt_size\']):\n            data, target = get_batch(batches, i, args[\'bptt_size\'])\n            lens = [data.size(1) for i in range(data.size(0))]\n            if args[\'cuda\']: \n                data = data.cuda()\n                target = target.cuda()\n\n            output, hidden, decoded = model.forward(data, lens, hidden)\n            loss = criterion(decoded.view(-1, len(vocab[\'char\'])), target)\n            \n            hidden = repackage_hidden(hidden)\n            total_loss += data.size(1) * loss.data.item()\n    return total_loss / batches.size(1)\n           \ndef train(args):\n    model_file = args[\'save_dir\'] + \'/\' + args[\'save_name\'] if args[\'save_name\'] is not None \\\n        else \'{}/{}_{}_charlm.pt\'.format(args[\'save_dir\'], args[\'shorthand\'], args[\'direction\'])\n    vocab_file = args[\'save_dir\'] + \'/\' + args[\'vocab_save_name\'] if args[\'vocab_save_name\'] is not None \\\n        else \'{}/{}_vocab.pt\'.format(args[\'save_dir\'], args[\'shorthand\'])\n\n    if os.path.exists(vocab_file):\n        logging.info(\'Loading existing vocab file\')\n        vocab = {\'char\': CharVocab.load_state_dict(torch.load(vocab_file, lambda storage, loc: storage))}\n    else:\n        logging.info(\'Building and saving vocab\')\n        vocab = {\'char\': build_vocab(args[\'train_file\'] if args[\'train_dir\'] is None else args[\'train_dir\'], cutoff=args[\'cutoff\'])}\n        torch.save(vocab[\'char\'].state_dict(), vocab_file)\n    logger.info(""Training model with vocab size: {}"".format(len(vocab[\'char\'])))\n\n    model = CharacterLanguageModel(args, vocab, is_forward_lm=True if args[\'direction\'] == \'forward\' else False)\n    if args[\'cuda\']: model = model.cuda()\n    params = [param for param in model.parameters() if param.requires_grad]\n    optimizer = torch.optim.SGD(params, lr=args[\'lr0\'], momentum=args[\'momentum\'], weight_decay=args[\'weight_decay\'])\n    criterion = torch.nn.CrossEntropyLoss()\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, verbose=True, factor=args[\'anneal\'], patience=args[\'patience\'])\n\n    writer = None\n    if args[\'summary\']:\n        from torch.utils.tensorboard import SummaryWriter\n        summary_dir = \'{}/{}_summary\'.format(args[\'save_dir\'], args[\'save_name\']) if args[\'save_name\'] is not None \\\n            else \'{}/{}_{}_charlm_summary\'.format(args[\'save_dir\'], args[\'shorthand\'], args[\'direction\'])\n        writer = SummaryWriter(log_dir=summary_dir)\n\n    best_loss = None\n    for epoch in range(args[\'epochs\']):\n        # load train data from train_dir if not empty, otherwise load from file\n        if args[\'train_dir\'] is not None:\n            train_path = args[\'train_dir\']\n        else:\n            train_path = args[\'train_file\']\n        train_data = load_data(train_path, vocab, args[\'direction\'])\n        dev_data = load_data(args[\'eval_file\'], vocab, args[\'direction\'])\n        train_epoch(args, vocab, train_data, model, params, optimizer, criterion, epoch+1)\n\n        start_time = time.time()\n        loss = evaluate_epoch(args, vocab, dev_data, model, criterion)\n        ppl = math.exp(loss)\n        elapsed = int(time.time() - start_time)\n        scheduler.step(loss)\n        logger.info(\n            ""| {:5d}/{:5d} epochs | time elapsed {:6d}s | loss {:5.2f} | ppl {:8.2f}"".format(\n                epoch + 1,\n                args[\'epochs\'],\n                elapsed,\n                loss,\n                ppl,\n            )\n        )\n        if best_loss is None or loss < best_loss:\n            best_loss = loss\n            model.save(model_file)\n            logger.info(\'new best model saved.\')\n        if writer:\n            writer.add_scalar(\'dev_loss\', loss, global_step=epoch+1)\n            writer.add_scalar(\'dev_ppl\', ppl, global_step=epoch+1)\n    if writer:\n        writer.close()\n    return\n\ndef evaluate(args):\n    model_file = args[\'save_dir\'] + \'/\' + args[\'save_name\'] if args[\'save_name\'] is not None \\\n        else \'{}/{}_{}_charlm.pt\'.format(args[\'save_dir\'], args[\'shorthand\'], args[\'direction\'])\n\n    model = CharacterLanguageModel.load(model_file)\n    if args[\'cuda\']: model = model.cuda()\n    vocab = model.vocab\n    data = load_data(args[\'eval_file\'], vocab, args[\'direction\'])\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    loss = evaluate_epoch(args, vocab, data, model, criterion)\n    logger.info(\n        ""| best model | loss {:5.2f} | ppl {:8.2f}"".format(\n            loss,\n            math.exp(loss),\n        )\n    )\n    return\n\nif __name__ == \'__main__\':\n    main()\n'"
stanza/models/identity_lemmatizer.py,0,"b'""""""\nAn indentity lemmatizer that mimics the behavior of a normal lemmatizer but directly uses word as lemma.\n""""""\n\nimport os\nimport argparse\nimport random\n\nfrom stanza.models.lemma.data import DataLoader\nfrom stanza.models.lemma import scorer\nfrom stanza.models.common import utils\nfrom stanza.models.common.doc import *\nfrom stanza.utils.conll import CoNLL\nfrom stanza.models import _training_logging\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_dir\', type=str, default=\'data/lemma\', help=\'Directory for all lemma data.\')\n    parser.add_argument(\'--train_file\', type=str, default=None, help=\'Input file for data loader.\')\n    parser.add_argument(\'--eval_file\', type=str, default=None, help=\'Input file for data loader.\')\n    parser.add_argument(\'--output_file\', type=str, default=None, help=\'Output CoNLL-U file.\')\n    parser.add_argument(\'--gold_file\', type=str, default=None, help=\'Output CoNLL-U file.\')\n\n    parser.add_argument(\'--mode\', default=\'train\', choices=[\'train\', \'predict\'])\n    parser.add_argument(\'--lang\', type=str, help=\'Language\')\n\n    parser.add_argument(\'--batch_size\', type=int, default=50)\n    parser.add_argument(\'--seed\', type=int, default=1234)\n    args = parser.parse_args()\n    return args\n\ndef main():\n    args = parse_args()\n    random.seed(args.seed)\n\n    args = vars(args)\n\n    print(""[Launching identity lemmatizer...]"")\n\n    if args[\'mode\'] == \'train\':\n        print(""[No training is required; will only generate evaluation output...]"")\n    \n    document = Document(CoNLL.conll2dict(input_file=args[\'eval_file\']))\n    batch = DataLoader(document, args[\'batch_size\'], args, evaluation=True, conll_only=True)\n    system_pred_file = args[\'output_file\']\n    gold_file = args[\'gold_file\']\n\n    # use identity mapping for prediction\n    preds = batch.doc.get([TEXT])\n\n    # write to file and score\n    batch.doc.set([LEMMA], preds)\n    CoNLL.dict2conll(batch.doc.to_dict(), system_pred_file)\n    if gold_file is not None:\n        _, _, score = scorer.score(system_pred_file, gold_file)\n\n        print(""Lemma score:"")\n        print(""{} {:.2f}"".format(args[\'lang\'], score*100))\n\nif __name__ == \'__main__\':\n    main()\n'"
stanza/models/lemmatizer.py,3,"b'""""""\nEntry point for training and evaluating a lemmatizer.\n\nThis lemmatizer combines a neural sequence-to-sequence architecture with an `edit` classifier \nand two dictionaries to produce robust lemmas from word forms.\nFor details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.\n""""""\n\nimport sys\nimport os\nimport shutil\nimport time\nfrom datetime import datetime\nimport argparse\nimport numpy as np\nimport random\nimport torch\nfrom torch import nn, optim\n\nfrom stanza.models.lemma.data import DataLoader\nfrom stanza.models.lemma.vocab import Vocab\nfrom stanza.models.lemma.trainer import Trainer\nfrom stanza.models.lemma import scorer, edit\nfrom stanza.models.common import utils\nimport stanza.models.common.seq2seq_constant as constant\nfrom stanza.models.common.doc import *\nfrom stanza.utils.conll import CoNLL\nfrom stanza.models import _training_logging\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_dir\', type=str, default=\'data/lemma\', help=\'Directory for all lemma data.\')\n    parser.add_argument(\'--train_file\', type=str, default=None, help=\'Input file for data loader.\')\n    parser.add_argument(\'--eval_file\', type=str, default=None, help=\'Input file for data loader.\')\n    parser.add_argument(\'--output_file\', type=str, default=None, help=\'Output CoNLL-U file.\')\n    parser.add_argument(\'--gold_file\', type=str, default=None, help=\'Output CoNLL-U file.\')\n\n    parser.add_argument(\'--mode\', default=\'train\', choices=[\'train\', \'predict\'])\n    parser.add_argument(\'--lang\', type=str, help=\'Language\')\n\n    parser.add_argument(\'--no_dict\', dest=\'ensemble_dict\', action=\'store_false\', help=\'Do not ensemble dictionary with seq2seq. By default use ensemble.\')\n    parser.add_argument(\'--dict_only\', action=\'store_true\', help=\'Only train a dictionary-based lemmatizer.\')\n\n    parser.add_argument(\'--hidden_dim\', type=int, default=200)\n    parser.add_argument(\'--emb_dim\', type=int, default=50)\n    parser.add_argument(\'--num_layers\', type=int, default=1)\n    parser.add_argument(\'--emb_dropout\', type=float, default=0.5)\n    parser.add_argument(\'--dropout\', type=float, default=0.5)\n    parser.add_argument(\'--max_dec_len\', type=int, default=50)\n    parser.add_argument(\'--beam_size\', type=int, default=1)\n\n    parser.add_argument(\'--attn_type\', default=\'soft\', choices=[\'soft\', \'mlp\', \'linear\', \'deep\'], help=\'Attention type\')\n    parser.add_argument(\'--pos_dim\', type=int, default=50)\n    parser.add_argument(\'--pos_dropout\', type=float, default=0.5)\n    parser.add_argument(\'--no_edit\', dest=\'edit\', action=\'store_false\', help=\'Do not use edit classifier in lemmatization. By default use an edit classifier.\')\n    parser.add_argument(\'--num_edit\', type=int, default=len(edit.EDIT_TO_ID))\n    parser.add_argument(\'--alpha\', type=float, default=1.0)\n    parser.add_argument(\'--no_pos\', dest=\'pos\', action=\'store_false\', help=\'Do not use UPOS in lemmatization. By default UPOS is used.\')\n\n    parser.add_argument(\'--sample_train\', type=float, default=1.0, help=\'Subsample training data.\')\n    parser.add_argument(\'--optim\', type=str, default=\'adam\', help=\'sgd, adagrad, adam or adamax.\')\n    parser.add_argument(\'--lr\', type=float, default=1e-3, help=\'Learning rate\')\n    parser.add_argument(\'--lr_decay\', type=float, default=0.9)\n    parser.add_argument(\'--decay_epoch\', type=int, default=30, help=""Decay the lr starting from this epoch."")\n    parser.add_argument(\'--num_epoch\', type=int, default=60)\n    parser.add_argument(\'--batch_size\', type=int, default=50)\n    parser.add_argument(\'--max_grad_norm\', type=float, default=5.0, help=\'Gradient clipping.\')\n    parser.add_argument(\'--log_step\', type=int, default=20, help=\'Print log every k steps.\')\n    parser.add_argument(\'--model_dir\', type=str, default=\'saved_models/lemma\', help=\'Root dir for saving models.\')\n\n    parser.add_argument(\'--seed\', type=int, default=1234)\n    parser.add_argument(\'--cuda\', type=bool, default=torch.cuda.is_available())\n    parser.add_argument(\'--cpu\', action=\'store_true\', help=\'Ignore CUDA.\')\n    args = parser.parse_args()\n    return args\n\ndef main():\n    args = parse_args()\n\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    if args.cpu:\n        args.cuda = False\n    elif args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    args = vars(args)\n    print(""Running lemmatizer in {} mode"".format(args[\'mode\']))\n\n    if args[\'mode\'] == \'train\':\n        train(args)\n    else:\n        evaluate(args)\n\ndef train(args):\n    # load data\n    print(""[Loading data with batch size {}...]"".format(args[\'batch_size\']))\n    train_doc = Document(CoNLL.conll2dict(input_file=args[\'train_file\']))\n    train_batch = DataLoader(train_doc, args[\'batch_size\'], args, evaluation=False)\n    vocab = train_batch.vocab\n    args[\'vocab_size\'] = vocab[\'char\'].size\n    args[\'pos_vocab_size\'] = vocab[\'pos\'].size\n    dev_doc = Document(CoNLL.conll2dict(input_file=args[\'eval_file\']))\n    dev_batch = DataLoader(dev_doc, args[\'batch_size\'], args, vocab=vocab, evaluation=True)\n\n    utils.ensure_dir(args[\'model_dir\'])\n    model_file = \'{}/{}_lemmatizer.pt\'.format(args[\'model_dir\'], args[\'lang\'])\n\n    # pred and gold path\n    system_pred_file = args[\'output_file\']\n    gold_file = args[\'gold_file\']\n\n    utils.print_config(args)\n\n    # skip training if the language does not have training or dev data\n    if len(train_batch) == 0 or len(dev_batch) == 0:\n        print(""[Skip training because no data available...]"")\n        sys.exit(0)\n\n    # start training\n    # train a dictionary-based lemmatizer\n    trainer = Trainer(args=args, vocab=vocab, use_cuda=args[\'cuda\'])\n    print(""[Training dictionary-based lemmatizer...]"")\n    trainer.train_dict(train_batch.doc.get([TEXT, UPOS, LEMMA]))\n    print(""Evaluating on dev set..."")\n    dev_preds = trainer.predict_dict(dev_batch.doc.get([TEXT, UPOS]))\n    dev_batch.doc.set([LEMMA], dev_preds)\n    CoNLL.dict2conll(dev_batch.doc.to_dict(), system_pred_file)\n    _, _, dev_f = scorer.score(system_pred_file, gold_file)\n    print(""Dev F1 = {:.2f}"".format(dev_f * 100))\n\n    if args.get(\'dict_only\', False):\n        # save dictionaries\n        trainer.save(model_file)\n    else:\n        # train a seq2seq model\n        print(""[Training seq2seq-based lemmatizer...]"")\n        global_step = 0\n        max_steps = len(train_batch) * args[\'num_epoch\']\n        dev_score_history = []\n        best_dev_preds = []\n        current_lr = args[\'lr\']\n        global_start_time = time.time()\n        format_str = \'{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}\'\n\n        # start training\n        for epoch in range(1, args[\'num_epoch\']+1):\n            train_loss = 0\n            for i, batch in enumerate(train_batch):\n                start_time = time.time()\n                global_step += 1\n                loss = trainer.update(batch, eval=False) # update step\n                train_loss += loss\n                if global_step % args[\'log_step\'] == 0:\n                    duration = time.time() - start_time\n                    print(format_str.format(datetime.now().strftime(""%Y-%m-%d %H:%M:%S""), global_step,\\\n                            max_steps, epoch, args[\'num_epoch\'], loss, duration, current_lr))\n\n            # eval on dev\n            print(""Evaluating on dev set..."")\n            dev_preds = []\n            dev_edits = []\n            for i, batch in enumerate(dev_batch):\n                preds, edits = trainer.predict(batch, args[\'beam_size\'])\n                dev_preds += preds\n                if edits is not None:\n                    dev_edits += edits\n            dev_preds = trainer.postprocess(dev_batch.doc.get([TEXT]), dev_preds, edits=dev_edits)\n\n            # try ensembling with dict if necessary\n            if args.get(\'ensemble_dict\', False):\n                print(""[Ensembling dict with seq2seq model...]"")\n                dev_preds = trainer.ensemble(dev_batch.doc.get([TEXT, UPOS]), dev_preds)\n            dev_batch.doc.set([LEMMA], dev_preds)\n            CoNLL.dict2conll(dev_batch.doc.to_dict(), system_pred_file)\n            _, _, dev_score = scorer.score(system_pred_file, gold_file)\n\n            train_loss = train_loss / train_batch.num_examples * args[\'batch_size\'] # avg loss per batch\n            print(""epoch {}: train_loss = {:.6f}, dev_score = {:.4f}"".format(epoch, train_loss, dev_score))\n\n            # save best model\n            if epoch == 1 or dev_score > max(dev_score_history):\n                trainer.save(model_file)\n                print(""new best model saved."")\n                best_dev_preds = dev_preds\n\n            # lr schedule\n            if epoch > args[\'decay_epoch\'] and dev_score <= dev_score_history[-1] and \\\n                    args[\'optim\'] in [\'sgd\', \'adagrad\']:\n                current_lr *= args[\'lr_decay\']\n                trainer.update_lr(current_lr)\n\n            dev_score_history += [dev_score]\n            print("""")\n\n        print(""Training ended with {} epochs."".format(epoch))\n\n        best_f, best_epoch = max(dev_score_history)*100, np.argmax(dev_score_history)+1\n        print(""Best dev F1 = {:.2f}, at epoch = {}"".format(best_f, best_epoch))\n\ndef evaluate(args):\n    # file paths\n    system_pred_file = args[\'output_file\']\n    gold_file = args[\'gold_file\']\n    model_file = \'{}/{}_lemmatizer.pt\'.format(args[\'model_dir\'], args[\'lang\'])\n\n    # load model\n    use_cuda = args[\'cuda\'] and not args[\'cpu\']\n    trainer = Trainer(model_file=model_file, use_cuda=use_cuda)\n    loaded_args, vocab = trainer.args, trainer.vocab\n\n    for k in args:\n        if k.endswith(\'_dir\') or k.endswith(\'_file\') or k in [\'shorthand\']:\n            loaded_args[k] = args[k]\n\n    # laod data\n    print(""Loading data with batch size {}..."".format(args[\'batch_size\']))\n    doc = Document(CoNLL.conll2dict(input_file=args[\'eval_file\']))\n    batch = DataLoader(doc, args[\'batch_size\'], loaded_args, vocab=vocab, evaluation=True)\n\n    # skip eval if dev data does not exist\n    if len(batch) == 0:\n        print(""Skip evaluation because no dev data is available..."")\n        print(""Lemma score:"")\n        print(""{} "".format(args[\'lang\']))\n        sys.exit(0)\n\n    dict_preds = trainer.predict_dict(batch.doc.get([TEXT, UPOS]))\n\n    if loaded_args.get(\'dict_only\', False):\n        preds = dict_preds\n    else:\n        print(""Running the seq2seq model..."")\n        preds = []\n        edits = []\n        for i, b in enumerate(batch):\n            ps, es = trainer.predict(b, args[\'beam_size\'])\n            preds += ps\n            if es is not None:\n                edits += es\n        preds = trainer.postprocess(batch.doc.get([TEXT]), preds, edits=edits)\n\n        if loaded_args.get(\'ensemble_dict\', False):\n            print(""[Ensembling dict with seq2seq lemmatizer...]"")\n            preds = trainer.ensemble(batch.doc.get([TEXT, UPOS]), preds)\n\n    # write to file and score\n    batch.doc.set([LEMMA], preds)\n    CoNLL.dict2conll(batch.doc.to_dict(), system_pred_file)\n    if gold_file is not None:\n        _, _, score = scorer.score(system_pred_file, gold_file)\n\n        print(""Lemma score:"")\n        print(""{} {:.2f}"".format(args[\'lang\'], score*100))\n\nif __name__ == \'__main__\':\n    main()\n'"
stanza/models/mwt_expander.py,3,"b'""""""\nEntry point for training and evaluating a multi-word token (MWT) expander.\n\nThis MWT expander combines a neural sequence-to-sequence architecture with a dictionary\nto decode the token into multiple words.\nFor details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.\n""""""\n\nimport sys\nimport os\nimport shutil\nimport time\nfrom datetime import datetime\nimport argparse\nimport numpy as np\nimport random\nimport torch\nfrom torch import nn, optim\nimport copy\n\nfrom stanza.models.mwt.data import DataLoader\nfrom stanza.models.mwt.vocab import Vocab\nfrom stanza.models.mwt.trainer import Trainer\nfrom stanza.models.mwt import scorer\nfrom stanza.models.common import utils\nimport stanza.models.common.seq2seq_constant as constant\nfrom stanza.models.common.doc import Document\nfrom stanza.utils.conll import CoNLL\nfrom stanza.models import _training_logging\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_dir\', type=str, default=\'data/mwt\', help=\'Root dir for saving models.\')\n    parser.add_argument(\'--train_file\', type=str, default=None, help=\'Input file for data loader.\')\n    parser.add_argument(\'--eval_file\', type=str, default=None, help=\'Input file for data loader.\')\n    parser.add_argument(\'--output_file\', type=str, default=None, help=\'Output CoNLL-U file.\')\n    parser.add_argument(\'--gold_file\', type=str, default=None, help=\'Output CoNLL-U file.\')\n\n    parser.add_argument(\'--mode\', default=\'train\', choices=[\'train\', \'predict\'])\n    parser.add_argument(\'--lang\', type=str, help=\'Language\')\n    parser.add_argument(\'--shorthand\', type=str, help=""Treebank shorthand"")\n\n    parser.add_argument(\'--no_dict\', dest=\'ensemble_dict\', action=\'store_false\', help=\'Do not ensemble dictionary with seq2seq. By default ensemble a dict.\')\n    parser.add_argument(\'--ensemble_early_stop\', action=\'store_true\', help=\'Early stopping based on ensemble performance.\')\n    parser.add_argument(\'--dict_only\', action=\'store_true\', help=\'Only train a dictionary-based MWT expander.\')\n\n    parser.add_argument(\'--hidden_dim\', type=int, default=100)\n    parser.add_argument(\'--emb_dim\', type=int, default=50)\n    parser.add_argument(\'--num_layers\', type=int, default=1)\n    parser.add_argument(\'--emb_dropout\', type=float, default=0.5)\n    parser.add_argument(\'--dropout\', type=float, default=0.5)\n    parser.add_argument(\'--max_dec_len\', type=int, default=50)\n    parser.add_argument(\'--beam_size\', type=int, default=1)\n    parser.add_argument(\'--attn_type\', default=\'soft\', choices=[\'soft\', \'mlp\', \'linear\', \'deep\'], help=\'Attention type\')\n\n    parser.add_argument(\'--sample_train\', type=float, default=1.0, help=\'Subsample training data.\')\n    parser.add_argument(\'--optim\', type=str, default=\'adam\', help=\'sgd, adagrad, adam or adamax.\')\n    parser.add_argument(\'--lr\', type=float, default=1e-3, help=\'Learning rate\')\n    parser.add_argument(\'--lr_decay\', type=float, default=0.9)\n    parser.add_argument(\'--decay_epoch\', type=int, default=30, help=""Decay the lr starting from this epoch."")\n    parser.add_argument(\'--num_epoch\', type=int, default=30)\n    parser.add_argument(\'--batch_size\', type=int, default=50)\n    parser.add_argument(\'--max_grad_norm\', type=float, default=5.0, help=\'Gradient clipping.\')\n    parser.add_argument(\'--log_step\', type=int, default=20, help=\'Print log every k steps.\')\n    parser.add_argument(\'--save_dir\', type=str, default=\'saved_models/mwt\', help=\'Root dir for saving models.\')\n    parser.add_argument(\'--save_name\', type=str, default=None, help=""File name to save the model"")\n\n    parser.add_argument(\'--seed\', type=int, default=1234)\n    parser.add_argument(\'--cuda\', type=bool, default=torch.cuda.is_available())\n    parser.add_argument(\'--cpu\', action=\'store_true\', help=\'Ignore CUDA.\')\n    args = parser.parse_args()\n    return args\n\ndef main():\n    args = parse_args()\n\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    if args.cpu:\n        args.cuda = False\n    elif args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    args = vars(args)\n    print(""Running MWT expander in {} mode"".format(args[\'mode\']))\n\n    if args[\'mode\'] == \'train\':\n        train(args)\n    else:\n        evaluate(args)\n\ndef train(args):\n    # load data\n    print(\'max_dec_len:\', args[\'max_dec_len\'])\n    print(""Loading data with batch size {}..."".format(args[\'batch_size\']))\n    train_doc = Document(CoNLL.conll2dict(input_file=args[\'train_file\']))\n    train_batch = DataLoader(train_doc, args[\'batch_size\'], args, evaluation=False)\n    vocab = train_batch.vocab\n    args[\'vocab_size\'] = vocab.size\n    dev_doc = Document(CoNLL.conll2dict(input_file=args[\'eval_file\']))\n    dev_batch = DataLoader(dev_doc, args[\'batch_size\'], args, vocab=vocab, evaluation=True)\n\n    utils.ensure_dir(args[\'save_dir\'])\n    model_file = args[\'save_dir\'] + \'/\' + args[\'save_name\'] if args[\'save_name\'] is not None \\\n            else \'{}/{}_mwt_expander.pt\'.format(args[\'save_dir\'], args[\'shorthand\'])\n\n    # pred and gold path\n    system_pred_file = args[\'output_file\']\n    gold_file = args[\'gold_file\']\n\n    # skip training if the language does not have training or dev data\n    if len(train_batch) == 0 or len(dev_batch) == 0:\n        print(""Skip training because no data available..."")\n        sys.exit(0)\n\n    # train a dictionary-based MWT expander\n    trainer = Trainer(args=args, vocab=vocab, use_cuda=args[\'cuda\'])\n    print(""Training dictionary-based MWT expander..."")\n    trainer.train_dict(train_batch.doc.get_mwt_expansions(evaluation=False))\n    print(""Evaluating on dev set..."")\n    dev_preds = trainer.predict_dict(dev_batch.doc.get_mwt_expansions(evaluation=True))\n    doc = copy.deepcopy(dev_batch.doc)\n    doc.set_mwt_expansions(dev_preds)\n    CoNLL.dict2conll(doc.to_dict(), system_pred_file)\n    _, _, dev_f = scorer.score(system_pred_file, gold_file)\n    print(""Dev F1 = {:.2f}"".format(dev_f * 100))\n\n    if args.get(\'dict_only\', False):\n        # save dictionaries\n        trainer.save(model_file)\n    else:\n        # train a seq2seq model\n        print(""Training seq2seq-based MWT expander..."")\n        global_step = 0\n        max_steps = len(train_batch) * args[\'num_epoch\']\n        dev_score_history = []\n        best_dev_preds = []\n        current_lr = args[\'lr\']\n        global_start_time = time.time()\n        format_str = \'{}: step {}/{} (epoch {}/{}), loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}\'\n\n        # start training\n        for epoch in range(1, args[\'num_epoch\']+1):\n            train_loss = 0\n            for i, batch in enumerate(train_batch):\n                start_time = time.time()\n                global_step += 1\n                loss = trainer.update(batch, eval=False) # update step\n                train_loss += loss\n                if global_step % args[\'log_step\'] == 0:\n                    duration = time.time() - start_time\n                    print(format_str.format(datetime.now().strftime(""%Y-%m-%d %H:%M:%S""), global_step,\\\n                            max_steps, epoch, args[\'num_epoch\'], loss, duration, current_lr))\n\n            # eval on dev\n            print(""Evaluating on dev set..."")\n            dev_preds = []\n            for i, batch in enumerate(dev_batch):\n                preds = trainer.predict(batch)\n                dev_preds += preds\n            if args.get(\'ensemble_dict\', False) and args.get(\'ensemble_early_stop\', False):\n                print(""[Ensembling dict with seq2seq model...]"")\n                dev_preds = trainer.ensemble(dev_batch.doc.get_mwt_expansions(evaluation=True), dev_preds)\n            doc = copy.deepcopy(dev_batch.doc)\n            doc.set_mwt_expansions(dev_preds)\n            CoNLL.dict2conll(doc.to_dict(), system_pred_file)\n            _, _, dev_score = scorer.score(system_pred_file, gold_file)\n\n            train_loss = train_loss / train_batch.num_examples * args[\'batch_size\'] # avg loss per batch\n            print(""epoch {}: train_loss = {:.6f}, dev_score = {:.4f}"".format(epoch, train_loss, dev_score))\n\n            # save best model\n            if epoch == 1 or dev_score > max(dev_score_history):\n                trainer.save(model_file)\n                print(""new best model saved."")\n                best_dev_preds = dev_preds\n\n            # lr schedule\n            if epoch > args[\'decay_epoch\'] and dev_score <= dev_score_history[-1]:\n                current_lr *= args[\'lr_decay\']\n                trainer.change_lr(current_lr)\n\n            dev_score_history += [dev_score]\n            print("""")\n\n        print(""Training ended with {} epochs."".format(epoch))\n\n        best_f, best_epoch = max(dev_score_history)*100, np.argmax(dev_score_history)+1\n        print(""Best dev F1 = {:.2f}, at epoch = {}"".format(best_f, best_epoch))\n\n        # try ensembling with dict if necessary\n        if args.get(\'ensemble_dict\', False):\n            print(""[Ensembling dict with seq2seq model...]"")\n            dev_preds = trainer.ensemble(dev_batch.doc.get_mwt_expansions(evaluation=True), best_dev_preds)\n            doc = copy.deepcopy(dev_batch.doc)\n            doc.set_mwt_expansions(dev_preds)\n            CoNLL.dict2conll(doc.to_dict(), system_pred_file)\n            _, _, dev_score = scorer.score(system_pred_file, gold_file)\n            print(""Ensemble dev F1 = {:.2f}"".format(dev_score*100))\n            best_f = max(best_f, dev_score)\n\ndef evaluate(args):\n    # file paths\n    system_pred_file = args[\'output_file\']\n    gold_file = args[\'gold_file\']\n    model_file = args[\'save_dir\'] + \'/\' + args[\'save_name\'] if args[\'save_name\'] is not None \\\n            else \'{}/{}_mwt_expander.pt\'.format(args[\'save_dir\'], args[\'shorthand\'])\n\n    # load model\n    use_cuda = args[\'cuda\'] and not args[\'cpu\']\n    trainer = Trainer(model_file=model_file, use_cuda=use_cuda)\n    loaded_args, vocab = trainer.args, trainer.vocab\n\n    for k in args:\n        if k.endswith(\'_dir\') or k.endswith(\'_file\') or k in [\'shorthand\']:\n            loaded_args[k] = args[k]\n    print(\'max_dec_len:\', loaded_args[\'max_dec_len\'])\n\n    # load data\n    print(""Loading data with batch size {}..."".format(args[\'batch_size\']))\n    doc = Document(CoNLL.conll2dict(input_file=args[\'eval_file\']))\n    batch = DataLoader(doc, args[\'batch_size\'], loaded_args, vocab=vocab, evaluation=True)\n\n    if len(batch) > 0:\n        dict_preds = trainer.predict_dict(batch.doc.get_mwt_expansions(evaluation=True))\n        # decide trainer type and run eval\n        if loaded_args[\'dict_only\']:\n            preds = dict_preds\n        else:\n            print(""Running the seq2seq model..."")\n            preds = []\n            for i, b in enumerate(batch):\n                preds += trainer.predict(b)\n\n            if loaded_args.get(\'ensemble_dict\', False):\n                preds = trainer.ensemble(batch.doc.get_mwt_expansions(evaluation=True), preds)\n    else:\n        # skip eval if dev data does not exist\n        preds = []\n\n    # write to file and score\n    doc = copy.deepcopy(batch.doc)\n    doc.set_mwt_expansions(preds)\n    CoNLL.dict2conll(doc.to_dict(), system_pred_file)\n\n    if gold_file is not None:\n        _, _, score = scorer.score(system_pred_file, gold_file)\n\n        print(""MWT expansion score:"")\n        print(""{} {:.2f}"".format(args[\'shorthand\'], score*100))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
stanza/models/ner_tagger.py,4,"b'""""""\nEntry point for training and evaluating an NER tagger.\n\nThis tagger uses BiLSTM layers with character and word-level representations, and a CRF decoding layer \nto produce NER predictions.\nFor details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.\n""""""\n\nimport sys\nimport os\nimport time\nfrom datetime import datetime\nimport argparse\nimport logging\nimport numpy as np\nimport random\nimport json\nimport torch\nfrom torch import nn, optim\n\nfrom stanza.models.ner.data import DataLoader\nfrom stanza.models.ner.trainer import Trainer\nfrom stanza.models.ner import scorer\nfrom stanza.models.common import utils\nfrom stanza.models.common.pretrain import Pretrain\nfrom stanza.utils.conll import CoNLL\nfrom stanza.models.common.doc import *\nfrom stanza.models import _training_logging\n\nlogger = logging.getLogger(\'stanza\')\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_dir\', type=str, default=\'data/ner\', help=\'Root dir for saving models.\')\n    parser.add_argument(\'--wordvec_dir\', type=str, default=\'extern_data/word2vec\', help=\'Directory of word vectors\')\n    parser.add_argument(\'--wordvec_file\', type=str, default=\'\', help=\'File that contains word vectors\')\n    parser.add_argument(\'--train_file\', type=str, default=None, help=\'Input file for data loader.\')\n    parser.add_argument(\'--eval_file\', type=str, default=None, help=\'Input file for data loader.\')\n\n    parser.add_argument(\'--mode\', default=\'train\', choices=[\'train\', \'predict\'])\n    parser.add_argument(\'--lang\', type=str, help=\'Language\')\n    parser.add_argument(\'--shorthand\', type=str, help=""Treebank shorthand"")\n\n    parser.add_argument(\'--hidden_dim\', type=int, default=256)\n    parser.add_argument(\'--char_hidden_dim\', type=int, default=100)\n    parser.add_argument(\'--word_emb_dim\', type=int, default=100)\n    parser.add_argument(\'--char_emb_dim\', type=int, default=100)\n    parser.add_argument(\'--num_layers\', type=int, default=1)\n    parser.add_argument(\'--char_num_layers\', type=int, default=1)\n    parser.add_argument(\'--pretrain_max_vocab\', type=int, default=100000)\n    parser.add_argument(\'--word_dropout\', type=float, default=0)\n    parser.add_argument(\'--locked_dropout\', type=float, default=0.0)\n    parser.add_argument(\'--dropout\', type=float, default=0.5)\n    parser.add_argument(\'--rec_dropout\', type=float, default=0, help=""Word recurrent dropout"")\n    parser.add_argument(\'--char_rec_dropout\', type=float, default=0, help=""Character recurrent dropout"")\n    parser.add_argument(\'--char_dropout\', type=float, default=0, help=""Character-level language model dropout"")\n    parser.add_argument(\'--no_char\', dest=\'char\', action=\'store_false\', help=""Turn off character model."")\n    parser.add_argument(\'--charlm\', action=\'store_true\', help=""Turn on contextualized char embedding using character-level language model."")\n    parser.add_argument(\'--charlm_save_dir\', type=str, default=\'saved_models/charlm\', help=""Root dir for pretrained character-level language model."")\n    parser.add_argument(\'--charlm_shorthand\', type=str, default=None, help=""Shorthand for character-level language model training corpus."")\n    parser.add_argument(\'--char_lowercase\', dest=\'char_lowercase\', action=\'store_true\', help=""Use lowercased characters in charater model."")\n    parser.add_argument(\'--no_lowercase\', dest=\'lowercase\', action=\'store_false\', help=""Use cased word vectors."")\n    parser.add_argument(\'--no_emb_finetune\', dest=\'emb_finetune\', action=\'store_false\', help=""Turn off finetuning of the embedding matrix."")\n    parser.add_argument(\'--no_input_transform\', dest=\'input_transform\', action=\'store_false\', help=""Do not use input transformation layer before tagger lstm."")\n    parser.add_argument(\'--scheme\', type=str, default=\'bioes\', help=""The tagging scheme to use: bio or bioes."")\n\n    parser.add_argument(\'--sample_train\', type=float, default=1.0, help=\'Subsample training data.\')\n    parser.add_argument(\'--optim\', type=str, default=\'sgd\', help=\'sgd, adagrad, adam or adamax.\')\n    parser.add_argument(\'--lr\', type=float, default=0.1, help=\'Learning rate.\')\n    parser.add_argument(\'--min_lr\', type=float, default=1e-4, help=\'Minimum learning rate to stop training.\')\n    parser.add_argument(\'--momentum\', type=float, default=0, help=\'Momentum for SGD.\')\n    parser.add_argument(\'--lr_decay\', type=float, default=0.5, help=""LR decay rate."")\n    parser.add_argument(\'--patience\', type=int, default=3, help=""Patience for LR decay."")\n\n    parser.add_argument(\'--max_steps\', type=int, default=200000)\n    parser.add_argument(\'--eval_interval\', type=int, default=500)\n    parser.add_argument(\'--batch_size\', type=int, default=32)\n    parser.add_argument(\'--max_grad_norm\', type=float, default=5.0, help=\'Gradient clipping.\')\n    parser.add_argument(\'--log_step\', type=int, default=20, help=\'Print log every k steps.\')\n    parser.add_argument(\'--save_dir\', type=str, default=\'saved_models/ner\', help=\'Root dir for saving models.\')\n    parser.add_argument(\'--save_name\', type=str, default=None, help=""File name to save the model"")\n\n    parser.add_argument(\'--seed\', type=int, default=1234)\n    parser.add_argument(\'--cuda\', type=bool, default=torch.cuda.is_available())\n    parser.add_argument(\'--cpu\', action=\'store_true\', help=\'Ignore CUDA.\')\n    args = parser.parse_args()\n    return args\n\ndef main():\n    args = parse_args()\n\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    if args.cpu:\n        args.cuda = False\n    elif args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    args = vars(args)\n    logger.info(""Running tagger in {} mode"".format(args[\'mode\']))\n\n    if args[\'mode\'] == \'train\':\n        train(args)\n    else:\n        evaluate(args)\n\ndef train(args):\n    utils.ensure_dir(args[\'save_dir\'])\n    model_file = args[\'save_dir\'] + \'/\' + args[\'save_name\'] if args[\'save_name\'] is not None \\\n            else \'{}/{}_nertagger.pt\'.format(args[\'save_dir\'], args[\'shorthand\'])\n\n    # load pretrained vectors\n    if len(args[\'wordvec_file\']) == 0:\n        vec_file = utils.get_wordvec_file(args[\'wordvec_dir\'], args[\'shorthand\'])\n    else:\n        vec_file = args[\'wordvec_file\']\n    # do not save pretrained embeddings individually\n    pretrain = Pretrain(None, vec_file, args[\'pretrain_max_vocab\'], save_to_file=False)\n\n    if args[\'charlm\']:\n        if args[\'charlm_shorthand\'] is None: \n            logger.info(""CharLM Shorthand is required for loading pretrained CharLM model..."")\n            sys.exit(0)\n        logger.info(\'Use pretrained contextualized char embedding\')\n        args[\'charlm_forward_file\'] = \'{}/{}_forward_charlm.pt\'.format(args[\'charlm_save_dir\'], args[\'charlm_shorthand\'])\n        args[\'charlm_backward_file\'] = \'{}/{}_backward_charlm.pt\'.format(args[\'charlm_save_dir\'], args[\'charlm_shorthand\'])\n\n    # load data\n    logger.info(""Loading data with batch size {}..."".format(args[\'batch_size\']))\n    train_doc = Document(json.load(open(args[\'train_file\'])))\n    train_batch = DataLoader(train_doc, args[\'batch_size\'], args, pretrain, evaluation=False)\n    vocab = train_batch.vocab\n    dev_doc = Document(json.load(open(args[\'eval_file\'])))\n    dev_batch = DataLoader(dev_doc, args[\'batch_size\'], args, pretrain, vocab=vocab, evaluation=True)\n    dev_gold_tags = dev_batch.tags\n\n    # skip training if the language does not have training or dev data\n    if len(train_batch) == 0 or len(dev_batch) == 0:\n        logger.info(""Skip training because no data available..."")\n        sys.exit(0)\n\n    logger.info(""Training tagger..."")\n    trainer = Trainer(args=args, vocab=vocab, pretrain=pretrain, use_cuda=args[\'cuda\'])\n    logger.info(trainer.model)\n\n    global_step = 0\n    max_steps = args[\'max_steps\']\n    dev_score_history = []\n    best_dev_preds = []\n    current_lr = trainer.optimizer.param_groups[0][\'lr\']\n    global_start_time = time.time()\n    format_str = \'{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}\'\n\n    # LR scheduling\n    if args[\'lr_decay\'] > 0:\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(trainer.optimizer, mode=\'max\', factor=args[\'lr_decay\'], \\\n            patience=args[\'patience\'], verbose=True, min_lr=args[\'min_lr\'])\n    else:\n        scheduler = None\n\n    # start training\n    train_loss = 0\n    while True:\n        should_stop = False\n        for i, batch in enumerate(train_batch):\n            start_time = time.time()\n            global_step += 1\n            loss = trainer.update(batch, eval=False) # update step\n            train_loss += loss\n            if global_step % args[\'log_step\'] == 0:\n                duration = time.time() - start_time\n                logger.info(format_str.format(datetime.now().strftime(""%Y-%m-%d %H:%M:%S""), global_step,\\\n                        max_steps, loss, duration, current_lr))\n\n            if global_step % args[\'eval_interval\'] == 0:\n                # eval on dev\n                logger.info(""Evaluating on dev set..."")\n                dev_preds = []\n                for batch in dev_batch:\n                    preds = trainer.predict(batch)\n                    dev_preds += preds\n                _, _, dev_score = scorer.score_by_entity(dev_preds, dev_gold_tags)\n\n                train_loss = train_loss / args[\'eval_interval\'] # avg loss per batch\n                logger.info(""step {}: train_loss = {:.6f}, dev_score = {:.4f}"".format(global_step, train_loss, dev_score))\n                train_loss = 0\n\n                # save best model\n                if len(dev_score_history) == 0 or dev_score > max(dev_score_history):\n                    trainer.save(model_file)\n                    logger.info(""New best model saved."")\n                    best_dev_preds = dev_preds\n\n                dev_score_history += [dev_score]\n                logger.info("""")\n\n                # lr schedule\n                if scheduler is not None:\n                    scheduler.step(dev_score)\n            \n            # check stopping\n            current_lr = trainer.optimizer.param_groups[0][\'lr\']\n            if global_step >= args[\'max_steps\'] or current_lr <= args[\'min_lr\']:\n                should_stop = True\n                break\n\n        if should_stop:\n            break\n\n        train_batch.reshuffle()\n\n    logger.info(""Training ended with {} steps."".format(global_step))\n\n    best_f, best_eval = max(dev_score_history)*100, np.argmax(dev_score_history)+1\n    logger.info(""Best dev F1 = {:.2f}, at iteration = {}"".format(best_f, best_eval * args[\'eval_interval\']))\n\ndef evaluate(args):\n    # file paths\n    model_file = args[\'save_dir\'] + \'/\' + args[\'save_name\'] if args[\'save_name\'] is not None \\\n            else \'{}/{}_nertagger.pt\'.format(args[\'save_dir\'], args[\'shorthand\'])\n\n    # load model\n    use_cuda = args[\'cuda\'] and not args[\'cpu\']\n    trainer = Trainer(model_file=model_file, use_cuda=use_cuda)\n    loaded_args, vocab = trainer.args, trainer.vocab\n\n    # load config\n    for k in args:\n        if k.endswith(\'_dir\') or k.endswith(\'_file\') or k in [\'shorthand\', \'mode\', \'scheme\']:\n            loaded_args[k] = args[k]\n\n    # load data\n    logger.info(""Loading data with batch size {}..."".format(args[\'batch_size\']))\n    doc = Document(json.load(open(args[\'eval_file\'])))\n    batch = DataLoader(doc, args[\'batch_size\'], loaded_args, vocab=vocab, evaluation=True)\n    \n    logger.info(""Start evaluation..."")\n    preds = []\n    for i, b in enumerate(batch):\n        preds += trainer.predict(b)\n\n    gold_tags = batch.tags\n    _, _, score = scorer.score_by_entity(preds, gold_tags)\n\n    logger.info(""NER tagger score:"")\n    logger.info(""{} {:.2f}"".format(args[\'shorthand\'], score*100))\n\nif __name__ == \'__main__\':\n    main()\n'"
stanza/models/parser.py,3,"b'""""""\nEntry point for training and evaluating a dependency parser.\n\nThis implementation combines a deep biaffine graph-based parser with linearization and distance features.\nFor details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.\n""""""\n\n""""""\nTraining and evaluation for the parser.\n""""""\n\nimport sys\nimport os\nimport shutil\nimport time\nfrom datetime import datetime\nimport argparse\nimport numpy as np\nimport random\nimport torch\nfrom torch import nn, optim\n\nfrom stanza.models.depparse.data import DataLoader\nfrom stanza.models.depparse.trainer import Trainer\nfrom stanza.models.depparse import scorer\nfrom stanza.models.common import utils\nfrom stanza.models.common.pretrain import Pretrain\nfrom stanza.models.common.doc import *\nfrom stanza.utils.conll import CoNLL\nfrom stanza.models import _training_logging\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_dir\', type=str, default=\'data/depparse\', help=\'Root dir for saving models.\')\n    parser.add_argument(\'--wordvec_dir\', type=str, default=\'extern_data/word2vec\', help=\'Directory of word vectors\')\n    parser.add_argument(\'--train_file\', type=str, default=None, help=\'Input file for data loader.\')\n    parser.add_argument(\'--eval_file\', type=str, default=None, help=\'Input file for data loader.\')\n    parser.add_argument(\'--output_file\', type=str, default=None, help=\'Output CoNLL-U file.\')\n    parser.add_argument(\'--gold_file\', type=str, default=None, help=\'Output CoNLL-U file.\')\n\n    parser.add_argument(\'--mode\', default=\'train\', choices=[\'train\', \'predict\'])\n    parser.add_argument(\'--lang\', type=str, help=\'Language\')\n    parser.add_argument(\'--shorthand\', type=str, help=""Treebank shorthand"")\n\n    parser.add_argument(\'--hidden_dim\', type=int, default=400)\n    parser.add_argument(\'--char_hidden_dim\', type=int, default=400)\n    parser.add_argument(\'--deep_biaff_hidden_dim\', type=int, default=400)\n    parser.add_argument(\'--composite_deep_biaff_hidden_dim\', type=int, default=100)\n    parser.add_argument(\'--word_emb_dim\', type=int, default=75)\n    parser.add_argument(\'--char_emb_dim\', type=int, default=100)\n    parser.add_argument(\'--tag_emb_dim\', type=int, default=50)\n    parser.add_argument(\'--transformed_dim\', type=int, default=125)\n    parser.add_argument(\'--num_layers\', type=int, default=3)\n    parser.add_argument(\'--char_num_layers\', type=int, default=1)\n    parser.add_argument(\'--pretrain_max_vocab\', type=int, default=250000)\n    parser.add_argument(\'--word_dropout\', type=float, default=0.33)\n    parser.add_argument(\'--dropout\', type=float, default=0.5)\n    parser.add_argument(\'--rec_dropout\', type=float, default=0, help=""Recurrent dropout"")\n    parser.add_argument(\'--char_rec_dropout\', type=float, default=0, help=""Recurrent dropout"")\n    parser.add_argument(\'--no_char\', dest=\'char\', action=\'store_false\', help=""Turn off character model."")\n    parser.add_argument(\'--no_pretrain\', dest=\'pretrain\', action=\'store_false\', help=""Turn off pretrained embeddings."")\n    parser.add_argument(\'--no_linearization\', dest=\'linearization\', action=\'store_false\', help=""Turn off linearization term."")\n    parser.add_argument(\'--no_distance\', dest=\'distance\', action=\'store_false\', help=""Turn off distance term."")\n\n    parser.add_argument(\'--sample_train\', type=float, default=1.0, help=\'Subsample training data.\')\n    parser.add_argument(\'--optim\', type=str, default=\'adam\', help=\'sgd, adagrad, adam or adamax.\')\n    parser.add_argument(\'--lr\', type=float, default=3e-3, help=\'Learning rate\')\n    parser.add_argument(\'--beta2\', type=float, default=0.95)\n\n    parser.add_argument(\'--max_steps\', type=int, default=50000)\n    parser.add_argument(\'--eval_interval\', type=int, default=100)\n    parser.add_argument(\'--max_steps_before_stop\', type=int, default=3000)\n    parser.add_argument(\'--batch_size\', type=int, default=5000)\n    parser.add_argument(\'--max_grad_norm\', type=float, default=1.0, help=\'Gradient clipping.\')\n    parser.add_argument(\'--log_step\', type=int, default=20, help=\'Print log every k steps.\')\n    parser.add_argument(\'--save_dir\', type=str, default=\'saved_models/depparse\', help=\'Root dir for saving models.\')\n    parser.add_argument(\'--save_name\', type=str, default=None, help=""File name to save the model"")\n\n    parser.add_argument(\'--seed\', type=int, default=1234)\n    parser.add_argument(\'--cuda\', type=bool, default=torch.cuda.is_available())\n    parser.add_argument(\'--cpu\', action=\'store_true\', help=\'Ignore CUDA.\')\n    args = parser.parse_args()\n    return args\n\ndef main():\n    args = parse_args()\n\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    if args.cpu:\n        args.cuda = False\n    elif args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    args = vars(args)\n    print(""Running parser in {} mode"".format(args[\'mode\']))\n\n    if args[\'mode\'] == \'train\':\n        train(args)\n    else:\n        evaluate(args)\n\ndef train(args):\n    utils.ensure_dir(args[\'save_dir\'])\n    model_file = args[\'save_dir\'] + \'/\' + args[\'save_name\'] if args[\'save_name\'] is not None \\\n            else \'{}/{}_parser.pt\'.format(args[\'save_dir\'], args[\'shorthand\'])\n\n    # load pretrained vectors if needed\n    pretrain = None\n    if args[\'pretrain\']:\n        vec_file = utils.get_wordvec_file(args[\'wordvec_dir\'], args[\'shorthand\'])\n        pretrain_file = \'{}/{}.pretrain.pt\'.format(args[\'save_dir\'], args[\'shorthand\'])\n        pretrain = Pretrain(pretrain_file, vec_file, args[\'pretrain_max_vocab\'])\n\n    # load data\n    print(""Loading data with batch size {}..."".format(args[\'batch_size\']))\n    train_doc = Document(CoNLL.conll2dict(input_file=args[\'train_file\']))\n    train_batch = DataLoader(train_doc, args[\'batch_size\'], args, pretrain, evaluation=False)\n    vocab = train_batch.vocab\n    dev_doc = Document(CoNLL.conll2dict(input_file=args[\'eval_file\']))\n    dev_batch = DataLoader(dev_doc, args[\'batch_size\'], args, pretrain, vocab=vocab, evaluation=True, sort_during_eval=True)\n\n    # pred and gold path\n    system_pred_file = args[\'output_file\']\n    gold_file = args[\'gold_file\']\n\n    # skip training if the language does not have training or dev data\n    if len(train_batch) == 0 or len(dev_batch) == 0:\n        print(""Skip training because no data available..."")\n        sys.exit(0)\n\n    print(""Training parser..."")\n    trainer = Trainer(args=args, vocab=vocab, pretrain=pretrain, use_cuda=args[\'cuda\'])\n\n    global_step = 0\n    max_steps = args[\'max_steps\']\n    dev_score_history = []\n    best_dev_preds = []\n    current_lr = args[\'lr\']\n    global_start_time = time.time()\n    format_str = \'{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}\'\n\n    using_amsgrad = False\n    last_best_step = 0\n    # start training\n    train_loss = 0\n    while True:\n        do_break = False\n        for i, batch in enumerate(train_batch):\n            start_time = time.time()\n            global_step += 1\n            loss = trainer.update(batch, eval=False) # update step\n            train_loss += loss\n            if global_step % args[\'log_step\'] == 0:\n                duration = time.time() - start_time\n                print(format_str.format(datetime.now().strftime(""%Y-%m-%d %H:%M:%S""), global_step,\\\n                        max_steps, loss, duration, current_lr))\n\n            if global_step % args[\'eval_interval\'] == 0:\n                # eval on dev\n                print(""Evaluating on dev set..."")\n                dev_preds = []\n                for batch in dev_batch:\n                    preds = trainer.predict(batch)\n                    dev_preds += preds\n                dev_preds = utils.unsort(dev_preds, dev_batch.data_orig_idx)\n\n                dev_batch.doc.set([HEAD, DEPREL], [y for x in dev_preds for y in x])\n                CoNLL.dict2conll(dev_batch.doc.to_dict(), system_pred_file)\n                _, _, dev_score = scorer.score(system_pred_file, gold_file)\n\n                train_loss = train_loss / args[\'eval_interval\'] # avg loss per batch\n                print(""step {}: train_loss = {:.6f}, dev_score = {:.4f}"".format(global_step, train_loss, dev_score))\n                train_loss = 0\n\n                # save best model\n                if len(dev_score_history) == 0 or dev_score > max(dev_score_history):\n                    last_best_step = global_step\n                    trainer.save(model_file)\n                    print(""new best model saved."")\n                    best_dev_preds = dev_preds\n\n                dev_score_history += [dev_score]\n                print("""")\n\n            if global_step - last_best_step >= args[\'max_steps_before_stop\']:\n                if not using_amsgrad:\n                    print(""Switching to AMSGrad"")\n                    last_best_step = global_step\n                    using_amsgrad = True\n                    trainer.optimizer = optim.Adam(trainer.model.parameters(), amsgrad=True, lr=args[\'lr\'], betas=(.9, args[\'beta2\']), eps=1e-6)\n                else:\n                    do_break = True\n                    break\n\n            if global_step >= args[\'max_steps\']:\n                do_break = True\n                break\n\n        if do_break: break\n\n        train_batch.reshuffle()\n\n    print(""Training ended with {} steps."".format(global_step))\n\n    best_f, best_eval = max(dev_score_history)*100, np.argmax(dev_score_history)+1\n    print(""Best dev F1 = {:.2f}, at iteration = {}"".format(best_f, best_eval * args[\'eval_interval\']))\n\ndef evaluate(args):\n    # file paths\n    system_pred_file = args[\'output_file\']\n    gold_file = args[\'gold_file\']\n    model_file = args[\'save_dir\'] + \'/\' + args[\'save_name\'] if args[\'save_name\'] is not None \\\n            else \'{}/{}_parser.pt\'.format(args[\'save_dir\'], args[\'shorthand\'])\n\n    # load pretrain; note that we allow the pretrain_file to be non-existent\n    pretrain_file = \'{}/{}.pretrain.pt\'.format(args[\'save_dir\'], args[\'shorthand\'])\n    pretrain = Pretrain(pretrain_file)\n\n    # load model\n    print(""Loading model from: {}"".format(model_file))\n    use_cuda = args[\'cuda\'] and not args[\'cpu\']\n    trainer = Trainer(pretrain=pretrain, model_file=model_file, use_cuda=use_cuda)\n    loaded_args, vocab = trainer.args, trainer.vocab\n\n    # load config\n    for k in args:\n        if k.endswith(\'_dir\') or k.endswith(\'_file\') or k in [\'shorthand\'] or k == \'mode\':\n            loaded_args[k] = args[k]\n\n    # load data\n    print(""Loading data with batch size {}..."".format(args[\'batch_size\']))\n    doc = Document(CoNLL.conll2dict(input_file=args[\'eval_file\']))\n    batch = DataLoader(doc, args[\'batch_size\'], loaded_args, pretrain, vocab=vocab, evaluation=True, sort_during_eval=True)\n\n    if len(batch) > 0:\n        print(""Start evaluation..."")\n        preds = []\n        for i, b in enumerate(batch):\n            preds += trainer.predict(b)\n    else:\n        # skip eval if dev data does not exist\n        preds = []\n    preds = utils.unsort(preds, batch.data_orig_idx)\n\n    # write to file and score\n    batch.doc.set([HEAD, DEPREL], [y for x in preds for y in x])\n    CoNLL.dict2conll(batch.doc.to_dict(), system_pred_file)\n\n    if gold_file is not None:\n        _, _, score = scorer.score(system_pred_file, gold_file)\n\n        print(""Parser score:"")\n        print(""{} {:.2f}"".format(args[\'shorthand\'], score*100))\n\nif __name__ == \'__main__\':\n    main()\n'"
stanza/models/tagger.py,3,"b'""""""\nEntry point for training and evaluating a POS/morphological features tagger.\n\nThis tagger uses highway BiLSTM layers with character and word-level representations, and biaffine classifiers\nto produce consistant POS and UFeats predictions.\nFor details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.\n""""""\n\nimport sys\nimport os\nimport shutil\nimport time\nfrom datetime import datetime\nimport argparse\nimport numpy as np\nimport random\nimport torch\nfrom torch import nn, optim\n\nfrom stanza.models.pos.data import DataLoader\nfrom stanza.models.pos.trainer import Trainer\nfrom stanza.models.pos import scorer\nfrom stanza.models.common import utils\nfrom stanza.models.common.pretrain import Pretrain\nfrom stanza.models.common.doc import *\nfrom stanza.utils.conll import CoNLL\nfrom stanza.models import _training_logging\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_dir\', type=str, default=\'data/pos\', help=\'Root dir for saving models.\')\n    parser.add_argument(\'--wordvec_dir\', type=str, default=\'extern_data/word2vec\', help=\'Directory of word vectors\')\n    parser.add_argument(\'--train_file\', type=str, default=None, help=\'Input file for data loader.\')\n    parser.add_argument(\'--eval_file\', type=str, default=None, help=\'Input file for data loader.\')\n    parser.add_argument(\'--output_file\', type=str, default=None, help=\'Output CoNLL-U file.\')\n    parser.add_argument(\'--gold_file\', type=str, default=None, help=\'Output CoNLL-U file.\')\n\n    parser.add_argument(\'--mode\', default=\'train\', choices=[\'train\', \'predict\'])\n    parser.add_argument(\'--lang\', type=str, help=\'Language\')\n    parser.add_argument(\'--shorthand\', type=str, help=""Treebank shorthand"")\n\n    parser.add_argument(\'--hidden_dim\', type=int, default=200)\n    parser.add_argument(\'--char_hidden_dim\', type=int, default=400)\n    parser.add_argument(\'--deep_biaff_hidden_dim\', type=int, default=400)\n    parser.add_argument(\'--composite_deep_biaff_hidden_dim\', type=int, default=100)\n    parser.add_argument(\'--word_emb_dim\', type=int, default=75)\n    parser.add_argument(\'--char_emb_dim\', type=int, default=100)\n    parser.add_argument(\'--tag_emb_dim\', type=int, default=50)\n    parser.add_argument(\'--transformed_dim\', type=int, default=125)\n    parser.add_argument(\'--num_layers\', type=int, default=2)\n    parser.add_argument(\'--char_num_layers\', type=int, default=1)\n    parser.add_argument(\'--pretrain_max_vocab\', type=int, default=250000)\n    parser.add_argument(\'--word_dropout\', type=float, default=0.33)\n    parser.add_argument(\'--dropout\', type=float, default=0.5)\n    parser.add_argument(\'--rec_dropout\', type=float, default=0, help=""Recurrent dropout"")\n    parser.add_argument(\'--char_rec_dropout\', type=float, default=0, help=""Recurrent dropout"")\n    parser.add_argument(\'--no_char\', dest=\'char\', action=\'store_false\', help=""Turn off character model."")\n    parser.add_argument(\'--no_pretrain\', dest=\'pretrain\', action=\'store_false\', help=""Turn off pretrained embeddings."")\n    parser.add_argument(\'--share_hid\', action=\'store_true\', help=""Share hidden representations for UPOS, XPOS and UFeats."")\n    parser.set_defaults(share_hid=False)\n\n    parser.add_argument(\'--sample_train\', type=float, default=1.0, help=\'Subsample training data.\')\n    parser.add_argument(\'--optim\', type=str, default=\'adam\', help=\'sgd, adagrad, adam or adamax.\')\n    parser.add_argument(\'--lr\', type=float, default=3e-3, help=\'Learning rate\')\n    parser.add_argument(\'--beta2\', type=float, default=0.95)\n\n    parser.add_argument(\'--max_steps\', type=int, default=50000)\n    parser.add_argument(\'--eval_interval\', type=int, default=100)\n    parser.add_argument(\'--fix_eval_interval\', dest=\'adapt_eval_interval\', action=\'store_false\', \\\n            help=""Use fixed evaluation interval for all treebanks, otherwise by default the interval will be increased for larger treebanks."")\n    parser.add_argument(\'--max_steps_before_stop\', type=int, default=3000)\n    parser.add_argument(\'--batch_size\', type=int, default=5000)\n    parser.add_argument(\'--max_grad_norm\', type=float, default=1.0, help=\'Gradient clipping.\')\n    parser.add_argument(\'--log_step\', type=int, default=20, help=\'Print log every k steps.\')\n    parser.add_argument(\'--save_dir\', type=str, default=\'saved_models/pos\', help=\'Root dir for saving models.\')\n    parser.add_argument(\'--save_name\', type=str, default=None, help=""File name to save the model"")\n\n    parser.add_argument(\'--seed\', type=int, default=1234)\n    parser.add_argument(\'--cuda\', type=bool, default=torch.cuda.is_available())\n    parser.add_argument(\'--cpu\', action=\'store_true\', help=\'Ignore CUDA.\')\n    args = parser.parse_args()\n    return args\n\ndef main():\n    args = parse_args()\n\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    if args.cpu:\n        args.cuda = False\n    elif args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    args = vars(args)\n    print(""Running tagger in {} mode"".format(args[\'mode\']))\n\n    if args[\'mode\'] == \'train\':\n        train(args)\n    else:\n        evaluate(args)\n\ndef train(args):\n    utils.ensure_dir(args[\'save_dir\'])\n    model_file = args[\'save_dir\'] + \'/\' + args[\'save_name\'] if args[\'save_name\'] is not None \\\n            else \'{}/{}_tagger.pt\'.format(args[\'save_dir\'], args[\'shorthand\'])\n\n    # load pretrained vectors if needed\n    pretrain = None\n    if args[\'pretrain\']:\n        vec_file = utils.get_wordvec_file(args[\'wordvec_dir\'], args[\'shorthand\'])\n        pretrain_file = \'{}/{}.pretrain.pt\'.format(args[\'save_dir\'], args[\'shorthand\'])\n        pretrain = Pretrain(pretrain_file, vec_file, args[\'pretrain_max_vocab\'])\n\n    # load data\n    print(""Loading data with batch size {}..."".format(args[\'batch_size\']))\n    train_doc = Document(CoNLL.conll2dict(input_file=args[\'train_file\']))\n    train_batch = DataLoader(train_doc, args[\'batch_size\'], args, pretrain, evaluation=False)\n    vocab = train_batch.vocab\n    dev_doc = Document(CoNLL.conll2dict(input_file=args[\'eval_file\']))\n    dev_batch = DataLoader(dev_doc, args[\'batch_size\'], args, pretrain, vocab=vocab, evaluation=True, sort_during_eval=True)\n\n    # pred and gold path\n    system_pred_file = args[\'output_file\']\n    gold_file = args[\'gold_file\']\n\n    # skip training if the language does not have training or dev data\n    if len(train_batch) == 0 or len(dev_batch) == 0:\n        print(""Skip training because no data available..."")\n        sys.exit(0)\n\n    print(""Training tagger..."")\n    trainer = Trainer(args=args, vocab=vocab, pretrain=pretrain, use_cuda=args[\'cuda\'])\n\n    global_step = 0\n    max_steps = args[\'max_steps\']\n    dev_score_history = []\n    best_dev_preds = []\n    current_lr = args[\'lr\']\n    global_start_time = time.time()\n    format_str = \'{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch), lr: {:.6f}\'\n\n    if args[\'adapt_eval_interval\']:\n        args[\'eval_interval\'] = utils.get_adaptive_eval_interval(dev_batch.num_examples, 2000, args[\'eval_interval\'])\n        print(""Evaluating the model every {} steps..."".format(args[\'eval_interval\']))\n\n    using_amsgrad = False\n    last_best_step = 0\n    # start training\n    train_loss = 0\n    while True:\n        do_break = False\n        for i, batch in enumerate(train_batch):\n            start_time = time.time()\n            global_step += 1\n            loss = trainer.update(batch, eval=False) # update step\n            train_loss += loss\n            if global_step % args[\'log_step\'] == 0:\n                duration = time.time() - start_time\n                print(format_str.format(datetime.now().strftime(""%Y-%m-%d %H:%M:%S""), global_step,\\\n                        max_steps, loss, duration, current_lr))\n\n            if global_step % args[\'eval_interval\'] == 0:\n                # eval on dev\n                print(""Evaluating on dev set..."")\n                dev_preds = []\n                for batch in dev_batch:\n                    preds = trainer.predict(batch)\n                    dev_preds += preds\n                dev_preds = utils.unsort(dev_preds, dev_batch.data_orig_idx)\n                dev_batch.doc.set([UPOS, XPOS, FEATS], [y for x in dev_preds for y in x])\n                CoNLL.dict2conll(dev_batch.doc.to_dict(), system_pred_file)\n                _, _, dev_score = scorer.score(system_pred_file, gold_file)\n\n                train_loss = train_loss / args[\'eval_interval\'] # avg loss per batch\n                print(""step {}: train_loss = {:.6f}, dev_score = {:.4f}"".format(global_step, train_loss, dev_score))\n                train_loss = 0\n\n                # save best model\n                if len(dev_score_history) == 0 or dev_score > max(dev_score_history):\n                    last_best_step = global_step\n                    trainer.save(model_file)\n                    print(""new best model saved."")\n                    best_dev_preds = dev_preds\n\n                dev_score_history += [dev_score]\n                print("""")\n\n            if global_step - last_best_step >= args[\'max_steps_before_stop\']:\n                if not using_amsgrad:\n                    print(""Switching to AMSGrad"")\n                    last_best_step = global_step\n                    using_amsgrad = True\n                    trainer.optimizer = optim.Adam(trainer.model.parameters(), amsgrad=True, lr=args[\'lr\'], betas=(.9, args[\'beta2\']), eps=1e-6)\n                else:\n                    do_break = True\n                    break\n\n            if global_step >= args[\'max_steps\']:\n                do_break = True\n                break\n\n        if do_break: break\n\n        train_batch.reshuffle()\n\n    print(""Training ended with {} steps."".format(global_step))\n\n    best_f, best_eval = max(dev_score_history)*100, np.argmax(dev_score_history)+1\n    print(""Best dev F1 = {:.2f}, at iteration = {}"".format(best_f, best_eval * args[\'eval_interval\']))\n\ndef evaluate(args):\n    # file paths\n    system_pred_file = args[\'output_file\']\n    gold_file = args[\'gold_file\']\n    model_file = args[\'save_dir\'] + \'/\' + args[\'save_name\'] if args[\'save_name\'] is not None \\\n            else \'{}/{}_tagger.pt\'.format(args[\'save_dir\'], args[\'shorthand\'])\n\n    # load pretrain; note that we allow the pretrain_file to be non-existent\n    pretrain_file = \'{}/{}.pretrain.pt\'.format(args[\'save_dir\'], args[\'shorthand\'])\n    pretrain = Pretrain(pretrain_file)\n\n    # load model\n    print(""Loading model from: {}"".format(model_file))\n    use_cuda = args[\'cuda\'] and not args[\'cpu\']\n    trainer = Trainer(pretrain=pretrain, model_file=model_file, use_cuda=use_cuda)\n    loaded_args, vocab = trainer.args, trainer.vocab\n\n    # load config\n    for k in args:\n        if k.endswith(\'_dir\') or k.endswith(\'_file\') or k in [\'shorthand\'] or k == \'mode\':\n            loaded_args[k] = args[k]\n\n    # load data\n    print(""Loading data with batch size {}..."".format(args[\'batch_size\']))\n    doc = Document(CoNLL.conll2dict(input_file=args[\'eval_file\']))\n    batch = DataLoader(doc, args[\'batch_size\'], loaded_args, pretrain, vocab=vocab, evaluation=True, sort_during_eval=True)\n    if len(batch) > 0:\n        print(""Start evaluation..."")\n        preds = []\n        for i, b in enumerate(batch):\n            preds += trainer.predict(b)\n    else:\n        # skip eval if dev data does not exist\n        preds = []\n    preds = utils.unsort(preds, batch.data_orig_idx)\n\n    # write to file and score\n    batch.doc.set([UPOS, XPOS, FEATS], [y for x in preds for y in x])\n    CoNLL.dict2conll(batch.doc.to_dict(), system_pred_file)\n\n    if gold_file is not None:\n        _, _, score = scorer.score(system_pred_file, gold_file)\n\n        print(""Tagger score:"")\n        print(""{} {:.2f}"".format(args[\'shorthand\'], score*100))\n\nif __name__ == \'__main__\':\n    main()\n'"
stanza/models/tokenizer.py,3,"b'""""""\nEntry point for training and evaluating a neural tokenizer.\n\nThis tokenizer treats tokenization and sentence segmentation as a tagging problem, and uses a combination of \nrecurrent and convolutional architectures.\nFor details please refer to paper: https://nlp.stanford.edu/pubs/qi2018universal.pdf.\n""""""\n\nimport random\nimport argparse\nfrom copy import copy\nimport numpy as np\nimport torch\n\nfrom stanza.models.common import utils\nfrom stanza.models.tokenize.trainer import Trainer\nfrom stanza.models.tokenize.data import DataLoader\nfrom stanza.models.tokenize.utils import load_mwt_dict, eval_model, output_predictions\nfrom stanza.models import _training_logging\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--txt_file\', type=str, help=""Input plaintext file"")\n    parser.add_argument(\'--label_file\', type=str, default=None, help=""Character-level label file"")\n    parser.add_argument(\'--json_file\', type=str, default=None, help=""JSON file with pre-chunked units"")\n    parser.add_argument(\'--mwt_json_file\', type=str, default=None, help=""JSON file for MWT expansions"")\n    parser.add_argument(\'--conll_file\', type=str, default=None, help=""CoNLL file for output"")\n    parser.add_argument(\'--dev_txt_file\', type=str, help=""(Train only) Input plaintext file for the dev set"")\n    parser.add_argument(\'--dev_label_file\', type=str, default=None, help=""(Train only) Character-level label file for the dev set"")\n    parser.add_argument(\'--dev_json_file\', type=str, default=None, help=""(Train only) JSON file with pre-chunked units for the dev set"")\n    parser.add_argument(\'--dev_conll_gold\', type=str, default=None, help=""(Train only) CoNLL-U file for the dev set for early stopping"")\n    parser.add_argument(\'--lang\', type=str, help=""Language"")\n    parser.add_argument(\'--shorthand\', type=str, help=""UD treebank shorthand"")\n\n    parser.add_argument(\'--mode\', default=\'train\', choices=[\'train\', \'predict\'])\n    parser.add_argument(\'--skip_newline\', action=\'store_true\', help=""Whether to skip newline characters in input. Particularly useful for languages like Chinese."")\n\n    parser.add_argument(\'--emb_dim\', type=int, default=32, help=""Dimension of unit embeddings"")\n    parser.add_argument(\'--hidden_dim\', type=int, default=64, help=""Dimension of hidden units"")\n    parser.add_argument(\'--conv_filters\', type=str, default=""1,9"", help=""Configuration of conv filters. ,, separates layers and , separates filter sizes in the same layer."")\n    parser.add_argument(\'--no-residual\', dest=\'residual\', action=\'store_false\', help=""Add linear residual connections"")\n    parser.add_argument(\'--no-hierarchical\', dest=\'hierarchical\', action=\'store_false\', help=""\\""Hierarchical\\"" RNN tokenizer"")\n    parser.add_argument(\'--hier_invtemp\', type=float, default=0.5, help=""Inverse temperature used in propagating tokenization predictions between RNN layers"")\n    parser.add_argument(\'--input_dropout\', action=\'store_true\', help=""Dropout input embeddings as well"")\n    parser.add_argument(\'--conv_res\', type=str, default=None, help=""Convolutional residual layers for the RNN"")\n    parser.add_argument(\'--rnn_layers\', type=int, default=1, help=""Layers of RNN in the tokenizer"")\n\n    parser.add_argument(\'--max_grad_norm\', type=float, default=1.0, help=""Maximum gradient norm to clip to"")\n    parser.add_argument(\'--anneal\', type=float, default=.999, help=""Anneal the learning rate by this amount when dev performance deteriorate"")\n    parser.add_argument(\'--anneal_after\', type=int, default=2000, help=""Anneal the learning rate no earlier than this step"")\n    parser.add_argument(\'--lr0\', type=float, default=2e-3, help=""Initial learning rate"")\n    parser.add_argument(\'--dropout\', type=float, default=0.33, help=""Dropout probability"")\n    parser.add_argument(\'--unit_dropout\', type=float, default=0.33, help=""Unit dropout probability"")\n    parser.add_argument(\'--tok_noise\', type=float, default=0.02, help=""Probability to induce noise to the input of the higher RNN"")\n    parser.add_argument(\'--weight_decay\', type=float, default=0.0, help=""Weight decay"")\n    parser.add_argument(\'--max_seqlen\', type=int, default=100, help=""Maximum sequence length to consider at a time"")\n    parser.add_argument(\'--batch_size\', type=int, default=32, help=""Batch size to use"")\n    parser.add_argument(\'--epochs\', type=int, default=10, help=""Total epochs to train the model for"")\n    parser.add_argument(\'--steps\', type=int, default=20000, help=""Steps to train the model for, if unspecified use epochs"")\n    parser.add_argument(\'--report_steps\', type=int, default=20, help=""Update step interval to report loss"")\n    parser.add_argument(\'--shuffle_steps\', type=int, default=100, help=""Step interval to shuffle each paragragraph in the generator"")\n    parser.add_argument(\'--eval_steps\', type=int, default=200, help=""Step interval to evaluate the model on the dev set for early stopping"")\n    parser.add_argument(\'--save_name\', type=str, default=None, help=""File name to save the model"")\n    parser.add_argument(\'--load_name\', type=str, default=None, help=""File name to load a saved model"")\n    parser.add_argument(\'--save_dir\', type=str, default=\'saved_models/tokenize\', help=""Directory to save models in"")\n    parser.add_argument(\'--cuda\', type=bool, default=torch.cuda.is_available())\n    parser.add_argument(\'--cpu\', action=\'store_true\', help=\'Ignore CUDA and run on CPU.\')\n    parser.add_argument(\'--seed\', type=int, default=1234)\n    args = parser.parse_args()\n    return args\n\ndef main():\n    args = parse_args()\n\n    torch.manual_seed(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    if args.cpu:\n        args.cuda = False\n    elif args.cuda:\n        torch.cuda.manual_seed(args.seed)\n\n    args = vars(args)\n    print(""Running tokenizer in {} mode"".format(args[\'mode\']))\n\n    args[\'feat_funcs\'] = [\'space_before\', \'capitalized\', \'all_caps\', \'numeric\']\n    args[\'feat_dim\'] = len(args[\'feat_funcs\'])\n    args[\'save_name\'] = ""{}/{}"".format(args[\'save_dir\'], args[\'save_name\']) if args[\'save_name\'] is not None \\\n            else \'{}/{}_tokenizer.pt\'.format(args[\'save_dir\'], args[\'shorthand\'])\n    utils.ensure_dir(args[\'save_dir\'])\n\n    if args[\'mode\'] == \'train\':\n        train(args)\n    else:\n        evaluate(args)\n\ndef train(args):\n    mwt_dict = load_mwt_dict(args[\'mwt_json_file\'])\n\n    train_input_files = {\n            \'json\': args[\'json_file\'],\n            \'txt\': args[\'txt_file\'],\n            \'label\': args[\'label_file\']\n            }\n    train_batches = DataLoader(args, input_files=train_input_files)\n    vocab = train_batches.vocab\n    args[\'vocab_size\'] = len(vocab)\n\n    dev_input_files = {\n            \'json\': args[\'dev_json_file\'],\n            \'txt\': args[\'dev_txt_file\'],\n            \'label\': args[\'dev_label_file\']\n            }\n    dev_batches = DataLoader(args, input_files=dev_input_files, vocab=vocab, evaluation=True)\n\n    trainer = Trainer(args=args, vocab=vocab, use_cuda=args[\'cuda\'])\n\n    if args[\'load_name\'] is not None:\n        load_name = ""{}/{}"".format(args[\'save_dir\'], args[\'load_name\'])\n        trainer.load(load_name)\n    trainer.change_lr(args[\'lr0\'])\n\n    N = len(train_batches)\n    steps = args[\'steps\'] if args[\'steps\'] is not None else int(N * args[\'epochs\'] / args[\'batch_size\'] + .5)\n    lr = args[\'lr0\']\n\n    prev_dev_score = -1\n    best_dev_score = -1\n    best_dev_step = -1\n\n    for step in range(1, steps+1):\n        batch = train_batches.next(unit_dropout=args[\'unit_dropout\'])\n\n        loss = trainer.update(batch)\n        if step % args[\'report_steps\'] == 0:\n            print(""Step {:6d}/{:6d} Loss: {:.3f}"".format(step, steps, loss))\n\n        if args[\'shuffle_steps\'] > 0 and step % args[\'shuffle_steps\'] == 0:\n            train_batches.shuffle()\n\n        if step % args[\'eval_steps\'] == 0:\n            dev_score = eval_model(args, trainer, dev_batches, vocab, mwt_dict)\n            reports = [\'Dev score: {:6.3f}\'.format(dev_score * 100)]\n            if step >= args[\'anneal_after\'] and dev_score < prev_dev_score:\n                reports += [\'lr: {:.6f} -> {:.6f}\'.format(lr, lr * args[\'anneal\'])]\n                lr *= args[\'anneal\']\n                trainer.change_lr(lr)\n\n            prev_dev_score = dev_score\n\n            if dev_score > best_dev_score:\n                reports += [\'New best dev score!\']\n                best_dev_score = dev_score\n                best_dev_step = step\n                trainer.save(args[\'save_name\'])\n            print(\'\\t\'.join(reports))\n\n    print(\'Best dev score={} at step {}\'.format(best_dev_score, best_dev_step))\n\ndef evaluate(args):\n    mwt_dict = load_mwt_dict(args[\'mwt_json_file\'])\n    use_cuda = args[\'cuda\'] and not args[\'cpu\']\n    trainer = Trainer(model_file=args[\'save_name\'], use_cuda=use_cuda)\n    loaded_args, vocab = trainer.args, trainer.vocab\n    for k in loaded_args:\n        if not k.endswith(\'_file\') and k not in [\'cuda\', \'mode\', \'save_dir\', \'save_name\']:\n            args[k] = loaded_args[k]\n\n    eval_input_files = {\n            \'json\': args[\'json_file\'],\n            \'txt\': args[\'txt_file\'],\n            \'label\': args[\'label_file\']\n            }\n\n    batches = DataLoader(args, input_files=eval_input_files, vocab=vocab, evaluation=True)\n\n    oov_count, N, _, _ = output_predictions(args[\'conll_file\'], trainer, batches, vocab, mwt_dict, args[\'max_seqlen\'])\n\n    print(""OOV rate: {:6.3f}% ({:6d}/{:6d})"".format(oov_count / N * 100, oov_count, N))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
stanza/pipeline/__init__.py,0,b''
stanza/pipeline/_constants.py,0,"b'"""""" Module defining constants """"""\n\n# string constants for processor names\nTOKENIZE = \'tokenize\'\nMWT = \'mwt\'\nPOS = \'pos\'\nLEMMA = \'lemma\'\nDEPPARSE = \'depparse\'\nNER = \'ner\'\n\n# supported external packages\nSUPPORTED_TOKENIZERS = [\'spacy\', \'jieba\']\n'"
stanza/pipeline/core.py,1,"b'""""""\nPipeline that runs tokenize,mwt,pos,lemma,depparse\n""""""\n\nimport io\nimport itertools\nimport sys\nimport torch\nimport logging\nimport json\nimport os\n\nfrom distutils.util import strtobool\nfrom stanza.pipeline._constants import *\nfrom stanza.models.common.doc import Document\nfrom stanza.pipeline.processor import Processor, ProcessorRequirementsException\nfrom stanza.pipeline.tokenize_processor import TokenizeProcessor\nfrom stanza.pipeline.mwt_processor import MWTProcessor\nfrom stanza.pipeline.pos_processor import POSProcessor\nfrom stanza.pipeline.lemma_processor import LemmaProcessor\nfrom stanza.pipeline.depparse_processor import DepparseProcessor\nfrom stanza.pipeline.ner_processor import NERProcessor\nfrom stanza.utils.resources import DEFAULT_MODEL_DIR, PIPELINE_NAMES, \\\n    maintain_processor_list, add_dependencies, build_default_config, set_logging_level, process_pipeline_parameters, sort_processors\nfrom stanza.utils.helper_func import make_table\n\nlogger = logging.getLogger(\'stanza\')\n\nNAME_TO_PROCESSOR_CLASS = {TOKENIZE: TokenizeProcessor, MWT: MWTProcessor, POS: POSProcessor,\n                           LEMMA: LemmaProcessor, DEPPARSE: DepparseProcessor, NER: NERProcessor}\n\n# list of settings for each processor\nPROCESSOR_SETTINGS = {\n    TOKENIZE: [\'batch_size\', \'pretokenized\', \'no_ssplit\'],\n    MWT: [\'batch_size\', \'dict_only\', \'ensemble_dict\'],\n    POS: [\'batch_size\'],\n    LEMMA: [\'batch_size\', \'beam_size\', \'dict_only\', \'ensemble_dict\', \'use_identity\'],\n    DEPPARSE: [\'batch_size\', \'pretagged\'],\n    NER: [\'batch_size\']\n} # TODO: ducumentation\n\nclass PipelineRequirementsException(Exception):\n    """"""\n    Exception indicating one or more requirements failures while attempting to build a pipeline.\n    Contains a ProcessorRequirementsException list.\n    """"""\n\n    def __init__(self, processor_req_fails):\n        self._processor_req_fails = processor_req_fails\n        self.build_message()\n\n    @property\n    def processor_req_fails(self):\n        return self._processor_req_fails\n\n    def build_message(self):\n        err_msg = io.StringIO()\n        print(*[req_fail.message for req_fail in self.processor_req_fails], sep=\'\\n\', file=err_msg)\n        self.message = \'\\n\\n\' + err_msg.getvalue()\n\n    def __str__(self):\n        return self.message\n\n\nclass Pipeline:\n    \n    def __init__(self, lang=\'en\', dir=DEFAULT_MODEL_DIR, package=\'default\', processors={}, logging_level=\'INFO\', verbose=None, use_gpu=True, **kwargs):\n        self.lang, self.dir, self.kwargs = lang, dir, kwargs\n        \n        # set global logging level\n        set_logging_level(logging_level, verbose)\n        self.logging_level = logging.getLevelName(logger.level)\n        # process different pipeline parameters\n        lang, dir, package, processors = process_pipeline_parameters(lang, dir, package, processors)\n\n        # Load resources.json to obtain latest packages.\n        logger.debug(\'Loading resource file...\')\n        resources_filepath = os.path.join(dir, \'resources.json\')\n        if not os.path.exists(resources_filepath):\n            raise Exception(f""Resources file not found at: {resources_filepath}. Try to download the model again."")\n        with open(resources_filepath) as infile:\n            resources = json.load(infile)\n        if lang in resources:\n            if \'alias\' in resources[lang]:\n                logger.info(f\'""{lang}"" is an alias for ""{resources[lang][""alias""]}""\')\n                lang = resources[lang][\'alias\']\n            lang_name = resources[lang][\'lang_name\'] if \'lang_name\' in resources[lang] else \'\'\n        else:\n            logger.warning(f\'Unsupported language: {lang}.\')\n\n        # Maintain load list\n        self.load_list = maintain_processor_list(resources, lang, package, processors) if lang in resources else []\n        self.load_list = add_dependencies(resources, lang, self.load_list) if lang in resources else []\n        self.load_list = self.update_kwargs(kwargs, self.load_list)\n        if len(self.load_list) == 0: raise Exception(\'No processor to load. Please check if your language or package is correctly set.\')\n        load_table = make_table([\'Processor\', \'Package\'], [row[:2] for row in self.load_list])\n        logger.info(f\'Loading these models for language: {lang} ({lang_name}):\\n{load_table}\')\n\n        self.config = build_default_config(resources, lang, dir, self.load_list)\n        self.config.update(kwargs)\n\n        # Load processors\n        self.processors = {}\n\n        # configs that are the same for all processors\n        pipeline_level_configs = {\'lang\': lang, \'mode\': \'predict\'}\n        self.use_gpu = torch.cuda.is_available() and use_gpu\n        logger.info(""Use device: {}"".format(""gpu"" if self.use_gpu else ""cpu""))\n        \n        # set up processors\n        pipeline_reqs_exceptions = []\n        for item in self.load_list:\n            processor_name, _, _ = item\n            logger.info(\'Loading: \' + processor_name)\n            curr_processor_config = self.filter_config(processor_name, self.config)\n            curr_processor_config.update(pipeline_level_configs)\n            logger.debug(\'With settings: \')\n            logger.debug(curr_processor_config)\n            try:\n                # try to build processor, throw an exception if there is a requirements issue\n                self.processors[processor_name] = NAME_TO_PROCESSOR_CLASS[processor_name](config=curr_processor_config,\n                                                                                          pipeline=self,\n                                                                                          use_gpu=self.use_gpu)\n            except ProcessorRequirementsException as e:\n                # if there was a requirements issue, add it to list which will be printed at end\n                pipeline_reqs_exceptions.append(e)\n                # add the broken processor to the loaded processors for the sake of analyzing the validity of the\n                # entire proposed pipeline, but at this point the pipeline will not be built successfully\n                self.processors[processor_name] = e.err_processor\n\n        # if there are any processor exceptions, throw an exception to indicate pipeline build failure\n        if pipeline_reqs_exceptions:\n            logger.info(\'\\n\')\n            raise PipelineRequirementsException(pipeline_reqs_exceptions)\n\n        logger.info(""Done loading processors!"")\n\n    def update_kwargs(self, kwargs, processor_list):\n        processor_dict = {processor: {\'package\': package, \'dependencies\': dependencies} for (processor, package, dependencies) in processor_list}\n        for key, value in kwargs.items():\n            k, v = key.split(\'_\', 1)\n            if v == \'model_path\':\n                package = value if len(value) < 25 else value[:10]+ \'...\' + value[-10:]\n                dependencies = processor_dict.get(k, {}).get(\'dependencies\')\n                processor_dict[k] = {\'package\': package, \'dependencies\': dependencies}\n        processor_list = [[processor, processor_dict[processor][\'package\'], processor_dict[processor][\'dependencies\']] for processor in processor_dict]\n        processor_list = sort_processors(processor_list)\n        return processor_list\n\n    def filter_config(self, prefix, config_dict):\n        filtered_dict = {}\n        for key in config_dict.keys():\n            k, v = key.split(\'_\', 1) # split tokenize_pretokenize to tokenize+pretokenize\n            if k == prefix:\n                filtered_dict[v] = config_dict[key]\n        return filtered_dict\n\n    @property\n    def loaded_processors(self):\n        """"""\n        Return all currently loaded processors in execution order.\n        :return: list of Processor instances\n        """"""\n        return [self.processors[processor_name] for processor_name in PIPELINE_NAMES if self.processors.get(processor_name)]\n\n    def process(self, doc):\n        # run the pipeline\n        for processor_name in PIPELINE_NAMES:\n            if self.processors.get(processor_name):\n                doc = self.processors[processor_name].process(doc)\n        return doc\n\n    def __call__(self, doc):\n        assert any([isinstance(doc, str), isinstance(doc, list),\n                    isinstance(doc, Document)]), \'input should be either str, list or Document\'\n        doc = self.process(doc)\n        return doc\n\n'"
stanza/pipeline/depparse_processor.py,0,"b'""""""\nProcessor for performing dependency parsing\n""""""\n\nfrom stanza.models.common import doc\nfrom stanza.models.common.pretrain import Pretrain\nfrom stanza.models.common.utils import unsort\nfrom stanza.models.depparse.data import DataLoader\nfrom stanza.models.depparse.trainer import Trainer\nfrom stanza.pipeline._constants import *\nfrom stanza.pipeline.processor import UDProcessor\n\n\nclass DepparseProcessor(UDProcessor):\n\n    # set of processor requirements this processor fulfills\n    PROVIDES_DEFAULT = set([DEPPARSE])\n    # set of processor requirements for this processor\n    REQUIRES_DEFAULT = set([TOKENIZE, POS, LEMMA])\n\n    def __init__(self, config, pipeline, use_gpu):\n        self._pretagged = None\n        super().__init__(config, pipeline, use_gpu)\n\n    def _set_up_requires(self):\n        if self._pretagged:\n            self._requires = set()\n        else:\n            self._requires = self.__class__.REQUIRES_DEFAULT\n\n    def _set_up_model(self, config, use_gpu):\n        self._pretagged = config.get(\'pretagged\')\n        self._pretrain = Pretrain(config[\'pretrain_path\'])\n        self._trainer = Trainer(pretrain=self.pretrain, model_file=config[\'model_path\'], use_cuda=use_gpu)\n\n    def process(self, document):\n        batch = DataLoader(\n            document, self.config[\'batch_size\'], self.config, self.pretrain, vocab=self.vocab, evaluation=True,\n            sort_during_eval=True)\n        preds = []\n        for i, b in enumerate(batch):\n            preds += self.trainer.predict(b)\n        preds = unsort(preds, batch.data_orig_idx)\n        batch.doc.set([doc.HEAD, doc.DEPREL], [y for x in preds for y in x])\n        # build dependencies based on predictions\n        for sentence in batch.doc.sentences:\n            sentence.build_dependencies()\n        return batch.doc\n'"
stanza/pipeline/lemma_processor.py,0,"b'""""""\nProcessor for performing lemmatization\n""""""\n\nfrom stanza.models.common import doc\nfrom stanza.models.lemma.data import DataLoader\nfrom stanza.models.lemma.trainer import Trainer\nfrom stanza.pipeline._constants import *\nfrom stanza.pipeline.processor import UDProcessor\n\nclass LemmaProcessor(UDProcessor):\n\n    # set of processor requirements this processor fulfills\n    PROVIDES_DEFAULT = set([LEMMA])\n    # set of processor requirements for this processor\n    REQUIRES_DEFAULT = set([TOKENIZE])\n    # default batch size\n    DEFAULT_BATCH_SIZE = 5000\n\n    def __init__(self, config, pipeline, use_gpu):\n        # run lemmatizer in identity mode\n        self._use_identity = None\n        super().__init__(config, pipeline, use_gpu)\n\n    @property\n    def use_identity(self):\n        return self._use_identity\n\n    def _set_up_model(self, config, use_gpu):\n        if config.get(\'use_identity\') in [\'True\', True]:\n            self._use_identity = True\n            self._config = config\n            self.config[\'batch_size\'] = LemmaProcessor.DEFAULT_BATCH_SIZE\n        else:\n            self._use_identity = False\n            self._trainer = Trainer(model_file=config[\'model_path\'], use_cuda=use_gpu)\n\n    def _set_up_requires(self):\n        if self.config.get(\'pos\') and not self.use_identity:\n            self._requires = LemmaProcessor.REQUIRES_DEFAULT.union(set([POS]))\n        else:\n            self._requires = LemmaProcessor.REQUIRES_DEFAULT\n\n    def process(self, document):\n        if not self.use_identity:\n            batch = DataLoader(document, self.config[\'batch_size\'], self.config, vocab=self.vocab, evaluation=True)\n        else:\n            batch = DataLoader(document, self.config[\'batch_size\'], self.config, evaluation=True, conll_only=True)\n        if self.use_identity:\n            preds = [word.text for sent in batch.doc.sentences for word in sent.words]\n        elif self.config.get(\'dict_only\', False):\n            preds = self.trainer.predict_dict(batch.doc.get([doc.TEXT, doc.UPOS]))\n        else:\n            if self.config.get(\'ensemble_dict\', False):\n                # skip the seq2seq model when we can\n                skip = self.trainer.skip_seq2seq(batch.doc.get([doc.TEXT, doc.UPOS]))\n                seq2seq_batch = DataLoader(document, self.config[\'batch_size\'], self.config, vocab=self.vocab,\n                                           evaluation=True, skip=skip)\n            else:\n                seq2seq_batch = batch\n\n            preds = []\n            edits = []\n            for i, b in enumerate(seq2seq_batch):\n                ps, es = self.trainer.predict(b, self.config[\'beam_size\'])\n                preds += ps\n                if es is not None:\n                    edits += es\n\n            if self.config.get(\'ensemble_dict\', False):\n                preds = self.trainer.postprocess([x for x, y in zip(batch.doc.get([doc.TEXT]), skip) if not y], preds, edits=edits)\n                # expand seq2seq predictions to the same size as all words\n                i = 0\n                preds1 = []\n                for s in skip:\n                    if s:\n                        preds1.append(\'\')\n                    else:\n                        preds1.append(preds[i])\n                        i += 1\n                preds = self.trainer.ensemble(batch.doc.get([doc.TEXT, doc.UPOS]), preds1)\n            else:\n                preds = self.trainer.postprocess(batch.doc.get([doc.TEXT]), preds, edits=edits)\n\n        # map empty string lemmas to \'_\'\n        preds = [max([(len(x), x), (0, \'_\')])[1] for x in preds]\n        batch.doc.set([doc.LEMMA], preds)\n        return batch.doc\n'"
stanza/pipeline/mwt_processor.py,0,"b'""""""\nProcessor for performing multi-word-token expansion\n""""""\n\nimport io\n\nfrom stanza.models.mwt.data import DataLoader\nfrom stanza.models.mwt.trainer import Trainer\nfrom stanza.pipeline._constants import *\nfrom stanza.pipeline.processor import UDProcessor\n\n\nclass MWTProcessor(UDProcessor):\n\n    # set of processor requirements this processor fulfills\n    PROVIDES_DEFAULT = set([MWT])\n    # set of processor requirements for this processor\n    REQUIRES_DEFAULT = set([TOKENIZE])\n\n    def _set_up_model(self, config, use_gpu):\n        self._trainer = Trainer(model_file=config[\'model_path\'], use_cuda=use_gpu)\n\n    def process(self, document):\n        batch = DataLoader(document, self.config[\'batch_size\'], self.config, vocab=self.vocab, evaluation=True)\n        if len(batch) > 0:\n            dict_preds = self.trainer.predict_dict(batch.doc.get_mwt_expansions(evaluation=True))\n            # decide trainer type and run eval\n            if self.config[\'dict_only\']:\n                preds = dict_preds\n            else:\n                preds = []\n                for i, b in enumerate(batch):\n                    preds += self.trainer.predict(b)\n\n                if self.config.get(\'ensemble_dict\', False):\n                    preds = self.trainer.ensemble(batch.doc.get_mwt_expansions(evaluation=True), preds)\n        else:\n            # skip eval if dev data does not exist\n            preds = []\n\n        batch.doc.set_mwt_expansions(preds)\n        return batch.doc\n'"
stanza/pipeline/ner_processor.py,0,"b'""""""\nProcessor for performing named entity tagging.\n""""""\nimport logging\n\nfrom stanza.models.common import doc\nfrom stanza.models.common.utils import unsort\nfrom stanza.models.ner.data import DataLoader\nfrom stanza.models.ner.trainer import Trainer\nfrom stanza.pipeline._constants import *\nfrom stanza.pipeline.processor import UDProcessor\n\nlogger = logging.getLogger(\'stanza\')\n\nclass NERProcessor(UDProcessor):\n\n    # set of processor requirements this processor fulfills\n    PROVIDES_DEFAULT = set([NER])\n    # set of processor requirements for this processor\n    REQUIRES_DEFAULT = set([TOKENIZE])\n\n    def _set_up_model(self, config, use_gpu):\n        # set up trainer\n        args = {\'charlm_forward_file\': config[\'forward_charlm_path\'], \'charlm_backward_file\': config[\'backward_charlm_path\']}\n        self._trainer = Trainer(args=args, model_file=config[\'model_path\'], use_cuda=use_gpu)\n\n    def process(self, document):\n        # set up a eval-only data loader and skip tag preprocessing\n        batch = DataLoader(\n            document, self.config[\'batch_size\'], self.config, vocab=self.vocab, evaluation=True, preprocess_tags=False)\n        preds = []\n        for i, b in enumerate(batch):\n            preds += self.trainer.predict(b)\n        batch.doc.set([doc.NER], [y for x in preds for y in x], to_token=True)\n        # collect entities into document attribute\n        total = len(batch.doc.build_ents())\n        logger.debug(f\'{total} entities found in document.\')\n        return batch.doc\n'"
stanza/pipeline/pos_processor.py,0,"b'""""""\nProcessor for performing part-of-speech tagging\n""""""\n\nfrom stanza.models.common import doc\nfrom stanza.models.common.pretrain import Pretrain\nfrom stanza.models.common.utils import unsort\nfrom stanza.models.pos.data import DataLoader\nfrom stanza.models.pos.trainer import Trainer\nfrom stanza.pipeline._constants import *\nfrom stanza.pipeline.processor import UDProcessor\n\n\nclass POSProcessor(UDProcessor):\n\n    # set of processor requirements this processor fulfills\n    PROVIDES_DEFAULT = set([POS])\n    # set of processor requirements for this processor\n    REQUIRES_DEFAULT = set([TOKENIZE])\n\n    def _set_up_model(self, config, use_gpu):\n        # get pretrained word vectors\n        self._pretrain = Pretrain(config[\'pretrain_path\'])\n        # set up trainer\n        self._trainer = Trainer(pretrain=self.pretrain, model_file=config[\'model_path\'], use_cuda=use_gpu)\n\n    def process(self, document):\n        batch = DataLoader(\n            document, self.config[\'batch_size\'], self.config, self.pretrain, vocab=self.vocab, evaluation=True,\n            sort_during_eval=True)\n        preds = []\n        for i, b in enumerate(batch):\n            preds += self.trainer.predict(b)\n        preds = unsort(preds, batch.data_orig_idx)\n        batch.doc.set([doc.UPOS, doc.XPOS, doc.FEATS], [y for x in preds for y in x])\n        return batch.doc\n'"
stanza/pipeline/processor.py,0,"b'""""""\nBase classes for processors\n""""""\n\nfrom abc import ABC, abstractmethod\n\n\nclass ProcessorRequirementsException(Exception):\n    """""" Exception indicating a processor\'s requirements will not be met """"""\n\n    def __init__(self, processors_list, err_processor, provided_reqs):\n        self._err_processor = err_processor\n        # mark the broken processor as inactive, drop resources\n        self.err_processor.mark_inactive()\n        self._processors_list = processors_list\n        self._provided_reqs = provided_reqs\n        self.build_message()\n\n    @property\n    def err_processor(self):\n        """""" The processor that raised the exception """"""\n        return self._err_processor\n\n    @property\n    def processor_type(self):\n        return type(self.err_processor).__name__\n\n    @property\n    def processors_list(self):\n        return self._processors_list\n\n    @property\n    def provided_reqs(self):\n        return self._provided_reqs\n\n    def build_message(self):\n        self.message = (f""---\\nPipeline Requirements Error!\\n""\n                        f""\\tProcessor: {self.processor_type}\\n""\n                        f""\\tPipeline processors list: {\',\'.join(self.processors_list)}\\n""\n                        f""\\tProcessor Requirements: {self.err_processor.requires}\\n""\n                        f""\\t\\t- fulfilled: {self.err_processor.requires.intersection(self.provided_reqs)}\\n""\n                        f""\\t\\t- missing: {self.err_processor.requires - self.provided_reqs}\\n""\n                        f""\\nThe processors list provided for this pipeline is invalid.  Please make sure all ""\n                        f""prerequisites are met for every processor.\\n\\n"")\n\n    def __str__(self):\n        return self.message\n\n\nclass Processor(ABC):\n    """""" Base class for all processors """"""\n\n    @abstractmethod\n    def process(self, doc):\n        """""" Process a Document.  This is the main method of a processor. """"""\n        pass\n\n    def _set_up_provides(self):\n        """""" Set up what processor requirements this processor fulfills.  Default is to use a class defined list. """"""\n        self._provides = self.__class__.PROVIDES_DEFAULT\n\n    def _set_up_requires(self):\n        """""" Set up requirements for this processor.  Default is to use a class defined list. """"""\n        self._requires = self.__class__.REQUIRES_DEFAULT\n\n    @property\n    def config(self):\n        """""" Configurations for the processor """"""\n        return self._config\n\n    @property\n    def pipeline(self):\n        """""" The pipeline that this processor belongs to """"""\n        return self._pipeline\n\n    @property\n    def provides(self):\n        return self._provides\n\n    @property\n    def requires(self):\n        return self._requires\n\n    def _check_requirements(self):\n        """""" Given a list of fulfilled requirements, check if all of this processor\'s requirements are met or not. """"""\n        provided_reqs = set.union(*[processor.provides for processor in self.pipeline.loaded_processors]+[set([])])\n        if self.requires - provided_reqs:\n            load_names = [item[0] for item in self.pipeline.load_list]\n            raise ProcessorRequirementsException(load_names, self, provided_reqs)\n\n\nclass UDProcessor(Processor):\n    """""" Base class for the neural UD Processors (tokenize,mwt,pos,lemma,depparse) """"""\n    def __init__(self, config, pipeline, use_gpu):\n        # overall config for the processor\n        self._config = None\n        # pipeline building this processor (presently processors are only meant to exist in one pipeline)\n        self._pipeline = pipeline\n        # UD model resources, set up is processor specific\n        self._pretrain = None\n        self._trainer = None\n        self._vocab = None\n        self._set_up_model(config, use_gpu)\n        # run set up process\n        # build the final config for the processor\n        self._set_up_final_config(config)\n        # set up what annotations are required based on config\n        self._set_up_requires()\n        # set up what annotations are provided based on config\n        self._set_up_provides()\n        # given pipeline constructing this processor, check if requirements are met, throw exception if not\n        self._check_requirements()\n\n    @abstractmethod\n    def _set_up_model(self, config, gpu):\n        pass\n\n    def _set_up_final_config(self, config):\n        """""" Finalize the configurations for this processor, based off of values from a UD model. """"""\n        # set configurations from loaded model\n        if self._trainer is not None:\n            loaded_args, self._vocab = self._trainer.args, self._trainer.vocab\n            # filter out unneeded args from model\n            loaded_args = {k: v for k, v in loaded_args.items() if not UDProcessor.filter_out_option(k)}\n        else:\n            loaded_args = {}\n        loaded_args.update(config)\n        self._config = loaded_args\n\n    def mark_inactive(self):\n        """""" Drop memory intensive resources if keeping this processor around for reasons other than running it. """"""\n        self._trainer = None\n        self._vocab = None\n\n    @property\n    def pretrain(self):\n        return self._pretrain\n\n    @property\n    def trainer(self):\n        return self._trainer\n\n    @property\n    def vocab(self):\n        return self._vocab\n\n    @staticmethod\n    def filter_out_option(option):\n        """""" Filter out non-processor configurations """"""\n        options_to_filter = [\'cpu\', \'cuda\', \'dev_conll_gold\', \'epochs\', \'lang\', \'mode\', \'save_name\', \'shorthand\']\n        if option.endswith(\'_file\') or option.endswith(\'_dir\'):\n            return True\n        elif option in options_to_filter:\n            return True\n        else:\n            return False\n'"
stanza/pipeline/tokenize_processor.py,0,"b'""""""\nProcessor for performing tokenization\n""""""\n\nimport io\nimport logging\n\nfrom stanza.models.tokenize.data import DataLoader\nfrom stanza.models.tokenize.trainer import Trainer\nfrom stanza.models.tokenize.utils import output_predictions\nfrom stanza.pipeline._constants import *\nfrom stanza.pipeline.processor import UDProcessor\nfrom stanza.utils.postprocess_vietnamese_tokenizer_data import paras_to_chunks\nfrom stanza.models.common import doc\nfrom stanza.utils.jieba import JiebaTokenizer\nfrom stanza.utils.spacy import SpacyTokenizer\n\nlogger = logging.getLogger(\'stanza\')\n\n# class for running the tokenizer\nclass TokenizeProcessor(UDProcessor):\n\n    # set of processor requirements this processor fulfills\n    PROVIDES_DEFAULT = set([TOKENIZE])\n    # set of processor requirements for this processor\n    REQUIRES_DEFAULT = set([])\n    # default max sequence length\n    MAX_SEQ_LENGTH_DEFAULT = 1000\n\n    def _set_up_model(self, config, use_gpu):\n        # set up trainer\n        if config.get(\'pretokenized\'):\n            self._trainer = None\n        elif config.get(\'with_jieba\', False):\n            self._trainer = None\n            self._jieba_tokenizer = JiebaTokenizer(config.get(\'lang\'))\n            logger.info(""Using jieba as tokenizer"")\n        elif config.get(\'with_spacy\', False):\n            self._trainer = None\n            self._spacy_tokenizer = SpacyTokenizer(config.get(\'lang\'))\n            logger.info(""Using spaCy as tokenizer"")\n        else:\n            self._trainer = Trainer(model_file=config[\'model_path\'], use_cuda=use_gpu)\n\n    def process_pre_tokenized_text(self, input_src):\n        """"""\n        Pretokenized text can be provided in 2 manners:\n\n        1.) str, tokenized by whitespace, sentence split by newline\n        2.) list of token lists, each token list represents a sentence\n\n        generate dictionary data structure\n        """"""\n\n        document = []\n        if isinstance(input_src, str):\n            sentences = [sent.strip().split() for sent in input_src.strip().split(\'\\n\') if len(sent.strip()) > 0]\n        elif isinstance(input_src, list):\n            sentences = input_src\n        idx = 0\n        for sentence in sentences:\n            sent = []\n            for token_id, token in enumerate(sentence):\n                sent.append({doc.ID: str(token_id + 1), doc.TEXT: token, doc.MISC: f\'start_char={idx}|end_char={idx + len(token)}\'})\n                idx += len(token) + 1\n            document.append(sent)\n        raw_text = \' \'.join([\' \'.join(sentence) for sentence in sentences])\n        return raw_text, document\n\n    def process(self, document):\n        assert isinstance(document, str) or (self.config.get(\'pretokenized\') or self.config.get(\'no_ssplit\', False)), \\\n            ""If neither \'pretokenized\' or \'no_ssplit\' option is enabled, the input to the TokenizerProcessor must be a string.""\n\n        if self.config.get(\'pretokenized\'):\n            raw_text, document = self.process_pre_tokenized_text(document)\n        elif self.config.get(\'with_jieba\', False):\n            return self._jieba_tokenizer.tokenize(document)\n        elif self.config.get(\'with_spacy\', False):\n            return self._spacy_tokenizer.tokenize(document)\n        else:\n            raw_text = \'\\n\\n\'.join(document) if isinstance(document, list) else document\n            # set up batches\n            if self.config.get(\'lang\') == \'vi\':\n                # special processing is due for Vietnamese\n                text = \'\\n\\n\'.join([x for x in raw_text.split(\'\\n\\n\')]).rstrip()\n                dummy_labels = \'\\n\\n\'.join([\'0\' * len(x) for x in text.split(\'\\n\\n\')])\n                data = paras_to_chunks(text, dummy_labels)\n                batches = DataLoader(self.config, input_data=data, vocab=self.vocab, evaluation=True)\n            else:\n                batches = DataLoader(self.config, input_text=raw_text, vocab=self.vocab, evaluation=True)\n            # get dict data\n            _, _, _, document = output_predictions(None, self.trainer, batches, self.vocab, None,\n                                   self.config.get(\'max_seqlen\', TokenizeProcessor.MAX_SEQ_LENGTH_DEFAULT),\n                                   orig_text=raw_text,\n                                   no_ssplit=self.config.get(\'no_ssplit\', False))\n        return doc.Document(document, raw_text)\n'"
stanza/protobuf/CoreNLP_pb2.py,0,"b'# -*- coding: utf-8 -*-\n# Generated by the protocol buffer compiler.  DO NOT EDIT!\n# source: CoreNLP.proto\n\nfrom google.protobuf.internal import enum_type_wrapper\nfrom google.protobuf import descriptor as _descriptor\nfrom google.protobuf import message as _message\nfrom google.protobuf import reflection as _reflection\nfrom google.protobuf import symbol_database as _symbol_database\n# @@protoc_insertion_point(imports)\n\n_sym_db = _symbol_database.Default()\n\n\n\n\nDESCRIPTOR = _descriptor.FileDescriptor(\n  name=\'CoreNLP.proto\',\n  package=\'edu.stanford.nlp.pipeline\',\n  syntax=\'proto2\',\n  serialized_options=b\'\\n\\031edu.stanford.nlp.pipelineB\\rCoreNLPProtos\',\n  serialized_pb=b\'\\n\\rCoreNLP.proto\\x12\\x19\\x65\\x64u.stanford.nlp.pipeline\\""\\xe1\\x05\\n\\x08\\x44ocument\\x12\\x0c\\n\\x04text\\x18\\x01 \\x02(\\t\\x12\\x35\\n\\x08sentence\\x18\\x02 \\x03(\\x0b\\x32#.edu.stanford.nlp.pipeline.Sentence\\x12\\x39\\n\\ncorefChain\\x18\\x03 \\x03(\\x0b\\x32%.edu.stanford.nlp.pipeline.CorefChain\\x12\\r\\n\\x05\\x64ocID\\x18\\x04 \\x01(\\t\\x12\\x0f\\n\\x07\\x64ocDate\\x18\\x07 \\x01(\\t\\x12\\x10\\n\\x08\\x63\\x61lendar\\x18\\x08 \\x01(\\x04\\x12;\\n\\x11sentencelessToken\\x18\\x05 \\x03(\\x0b\\x32 .edu.stanford.nlp.pipeline.Token\\x12\\x33\\n\\tcharacter\\x18\\n \\x03(\\x0b\\x32 .edu.stanford.nlp.pipeline.Token\\x12/\\n\\x05quote\\x18\\x06 \\x03(\\x0b\\x32 .edu.stanford.nlp.pipeline.Quote\\x12\\x37\\n\\x08mentions\\x18\\t \\x03(\\x0b\\x32%.edu.stanford.nlp.pipeline.NERMention\\x12#\\n\\x1bhasEntityMentionsAnnotation\\x18\\r \\x01(\\x08\\x12\\x0e\\n\\x06xmlDoc\\x18\\x0b \\x01(\\x08\\x12\\x34\\n\\x08sections\\x18\\x0c \\x03(\\x0b\\x32\\"".edu.stanford.nlp.pipeline.Section\\x12<\\n\\x10mentionsForCoref\\x18\\x0e \\x03(\\x0b\\x32\\"".edu.stanford.nlp.pipeline.Mention\\x12!\\n\\x19hasCorefMentionAnnotation\\x18\\x0f \\x01(\\x08\\x12\\x1a\\n\\x12hasCorefAnnotation\\x18\\x10 \\x01(\\x08\\x12+\\n#corefMentionToEntityMentionMappings\\x18\\x11 \\x03(\\x05\\x12+\\n#entityMentionToCorefMentionMappings\\x18\\x12 \\x03(\\x05*\\x05\\x08\\x64\\x10\\x80\\x02\\""\\x8e\\x0f\\n\\x08Sentence\\x12/\\n\\x05token\\x18\\x01 \\x03(\\x0b\\x32 .edu.stanford.nlp.pipeline.Token\\x12\\x18\\n\\x10tokenOffsetBegin\\x18\\x02 \\x02(\\r\\x12\\x16\\n\\x0etokenOffsetEnd\\x18\\x03 \\x02(\\r\\x12\\x15\\n\\rsentenceIndex\\x18\\x04 \\x01(\\r\\x12\\x1c\\n\\x14\\x63haracterOffsetBegin\\x18\\x05 \\x01(\\r\\x12\\x1a\\n\\x12\\x63haracterOffsetEnd\\x18\\x06 \\x01(\\r\\x12\\x37\\n\\tparseTree\\x18\\x07 \\x01(\\x0b\\x32$.edu.stanford.nlp.pipeline.ParseTree\\x12@\\n\\x12\\x62inarizedParseTree\\x18\\x1f \\x01(\\x0b\\x32$.edu.stanford.nlp.pipeline.ParseTree\\x12@\\n\\x12\\x61nnotatedParseTree\\x18  \\x01(\\x0b\\x32$.edu.stanford.nlp.pipeline.ParseTree\\x12\\x11\\n\\tsentiment\\x18! \\x01(\\t\\x12=\\n\\x0fkBestParseTrees\\x18\\"" \\x03(\\x0b\\x32$.edu.stanford.nlp.pipeline.ParseTree\\x12\\x45\\n\\x11\\x62\\x61sicDependencies\\x18\\x08 \\x01(\\x0b\\x32*.edu.stanford.nlp.pipeline.DependencyGraph\\x12I\\n\\x15\\x63ollapsedDependencies\\x18\\t \\x01(\\x0b\\x32*.edu.stanford.nlp.pipeline.DependencyGraph\\x12T\\n collapsedCCProcessedDependencies\\x18\\n \\x01(\\x0b\\x32*.edu.stanford.nlp.pipeline.DependencyGraph\\x12K\\n\\x17\\x61lternativeDependencies\\x18\\r \\x01(\\x0b\\x32*.edu.stanford.nlp.pipeline.DependencyGraph\\x12?\\n\\x0copenieTriple\\x18\\x0e \\x03(\\x0b\\x32).edu.stanford.nlp.pipeline.RelationTriple\\x12<\\n\\tkbpTriple\\x18\\x10 \\x03(\\x0b\\x32).edu.stanford.nlp.pipeline.RelationTriple\\x12\\x45\\n\\x10\\x65ntailedSentence\\x18\\x0f \\x03(\\x0b\\x32+.edu.stanford.nlp.pipeline.SentenceFragment\\x12\\x43\\n\\x0e\\x65ntailedClause\\x18# \\x03(\\x0b\\x32+.edu.stanford.nlp.pipeline.SentenceFragment\\x12H\\n\\x14\\x65nhancedDependencies\\x18\\x11 \\x01(\\x0b\\x32*.edu.stanford.nlp.pipeline.DependencyGraph\\x12P\\n\\x1c\\x65nhancedPlusPlusDependencies\\x18\\x12 \\x01(\\x0b\\x32*.edu.stanford.nlp.pipeline.DependencyGraph\\x12\\x33\\n\\tcharacter\\x18\\x13 \\x03(\\x0b\\x32 .edu.stanford.nlp.pipeline.Token\\x12\\x11\\n\\tparagraph\\x18\\x0b \\x01(\\r\\x12\\x0c\\n\\x04text\\x18\\x0c \\x01(\\t\\x12\\x12\\n\\nlineNumber\\x18\\x14 \\x01(\\r\\x12\\x1e\\n\\x16hasRelationAnnotations\\x18\\x33 \\x01(\\x08\\x12\\x31\\n\\x06\\x65ntity\\x18\\x34 \\x03(\\x0b\\x32!.edu.stanford.nlp.pipeline.Entity\\x12\\x35\\n\\x08relation\\x18\\x35 \\x03(\\x0b\\x32#.edu.stanford.nlp.pipeline.Relation\\x12$\\n\\x1chasNumerizedTokensAnnotation\\x18\\x36 \\x01(\\x08\\x12\\x37\\n\\x08mentions\\x18\\x37 \\x03(\\x0b\\x32%.edu.stanford.nlp.pipeline.NERMention\\x12<\\n\\x10mentionsForCoref\\x18\\x38 \\x03(\\x0b\\x32\\"".edu.stanford.nlp.pipeline.Mention\\x12\\""\\n\\x1ahasCorefMentionsAnnotation\\x18\\x39 \\x01(\\x08\\x12\\x12\\n\\nsentenceID\\x18: \\x01(\\t\\x12\\x13\\n\\x0bsectionDate\\x18; \\x01(\\t\\x12\\x14\\n\\x0csectionIndex\\x18< \\x01(\\r\\x12\\x13\\n\\x0bsectionName\\x18= \\x01(\\t\\x12\\x15\\n\\rsectionAuthor\\x18> \\x01(\\t\\x12\\r\\n\\x05\\x64ocID\\x18? \\x01(\\t\\x12\\x15\\n\\rsectionQuoted\\x18@ \\x01(\\x08\\x12#\\n\\x1bhasEntityMentionsAnnotation\\x18\\x41 \\x01(\\x08\\x12\\x1f\\n\\x17hasKBPTriplesAnnotation\\x18\\x44 \\x01(\\x08\\x12\\""\\n\\x1ahasOpenieTriplesAnnotation\\x18\\x45 \\x01(\\x08\\x12\\x14\\n\\x0c\\x63hapterIndex\\x18\\x42 \\x01(\\r\\x12\\x16\\n\\x0eparagraphIndex\\x18\\x43 \\x01(\\r*\\x05\\x08\\x64\\x10\\x80\\x02\\""\\x9a\\x0c\\n\\x05Token\\x12\\x0c\\n\\x04word\\x18\\x01 \\x01(\\t\\x12\\x0b\\n\\x03pos\\x18\\x02 \\x01(\\t\\x12\\r\\n\\x05value\\x18\\x03 \\x01(\\t\\x12\\x10\\n\\x08\\x63\\x61tegory\\x18\\x04 \\x01(\\t\\x12\\x0e\\n\\x06\\x62\\x65\\x66ore\\x18\\x05 \\x01(\\t\\x12\\r\\n\\x05\\x61\\x66ter\\x18\\x06 \\x01(\\t\\x12\\x14\\n\\x0coriginalText\\x18\\x07 \\x01(\\t\\x12\\x0b\\n\\x03ner\\x18\\x08 \\x01(\\t\\x12\\x11\\n\\tcoarseNER\\x18> \\x01(\\t\\x12\\x16\\n\\x0e\\x66ineGrainedNER\\x18? \\x01(\\t\\x12\\x15\\n\\rnerLabelProbs\\x18\\x42 \\x03(\\t\\x12\\x15\\n\\rnormalizedNER\\x18\\t \\x01(\\t\\x12\\r\\n\\x05lemma\\x18\\n \\x01(\\t\\x12\\x11\\n\\tbeginChar\\x18\\x0b \\x01(\\r\\x12\\x0f\\n\\x07\\x65ndChar\\x18\\x0c \\x01(\\r\\x12\\x11\\n\\tutterance\\x18\\r \\x01(\\r\\x12\\x0f\\n\\x07speaker\\x18\\x0e \\x01(\\t\\x12\\x12\\n\\nbeginIndex\\x18\\x0f \\x01(\\r\\x12\\x10\\n\\x08\\x65ndIndex\\x18\\x10 \\x01(\\r\\x12\\x17\\n\\x0ftokenBeginIndex\\x18\\x11 \\x01(\\r\\x12\\x15\\n\\rtokenEndIndex\\x18\\x12 \\x01(\\r\\x12\\x34\\n\\ntimexValue\\x18\\x13 \\x01(\\x0b\\x32 .edu.stanford.nlp.pipeline.Timex\\x12\\x15\\n\\rhasXmlContext\\x18\\x15 \\x01(\\x08\\x12\\x12\\n\\nxmlContext\\x18\\x16 \\x03(\\t\\x12\\x16\\n\\x0e\\x63orefClusterID\\x18\\x17 \\x01(\\r\\x12\\x0e\\n\\x06\\x61nswer\\x18\\x18 \\x01(\\t\\x12\\x15\\n\\rheadWordIndex\\x18\\x1a \\x01(\\r\\x12\\x35\\n\\x08operator\\x18\\x1b \\x01(\\x0b\\x32#.edu.stanford.nlp.pipeline.Operator\\x12\\x35\\n\\x08polarity\\x18\\x1c \\x01(\\x0b\\x32#.edu.stanford.nlp.pipeline.Polarity\\x12\\x14\\n\\x0cpolarity_dir\\x18\\\' \\x01(\\t\\x12-\\n\\x04span\\x18\\x1d \\x01(\\x0b\\x32\\x1f.edu.stanford.nlp.pipeline.Span\\x12\\x11\\n\\tsentiment\\x18\\x1e \\x01(\\t\\x12\\x16\\n\\x0equotationIndex\\x18\\x1f \\x01(\\x05\\x12\\x42\\n\\x0e\\x63onllUFeatures\\x18  \\x01(\\x0b\\x32*.edu.stanford.nlp.pipeline.MapStringString\\x12\\x11\\n\\tcoarseTag\\x18! \\x01(\\t\\x12\\x38\\n\\x0f\\x63onllUTokenSpan\\x18\\"" \\x01(\\x0b\\x32\\x1f.edu.stanford.nlp.pipeline.Span\\x12\\x12\\n\\nconllUMisc\\x18# \\x01(\\t\\x12G\\n\\x13\\x63onllUSecondaryDeps\\x18$ \\x01(\\x0b\\x32*.edu.stanford.nlp.pipeline.MapStringString\\x12\\x17\\n\\x0fwikipediaEntity\\x18% \\x01(\\t\\x12\\x11\\n\\tisNewline\\x18& \\x01(\\x08\\x12\\x0e\\n\\x06gender\\x18\\x33 \\x01(\\t\\x12\\x10\\n\\x08trueCase\\x18\\x34 \\x01(\\t\\x12\\x14\\n\\x0ctrueCaseText\\x18\\x35 \\x01(\\t\\x12\\x13\\n\\x0b\\x63hineseChar\\x18\\x36 \\x01(\\t\\x12\\x12\\n\\nchineseSeg\\x18\\x37 \\x01(\\t\\x12\\x16\\n\\x0e\\x63hineseXMLChar\\x18< \\x01(\\t\\x12\\x13\\n\\x0bsectionName\\x18\\x38 \\x01(\\t\\x12\\x15\\n\\rsectionAuthor\\x18\\x39 \\x01(\\t\\x12\\x13\\n\\x0bsectionDate\\x18: \\x01(\\t\\x12\\x17\\n\\x0fsectionEndLabel\\x18; \\x01(\\t\\x12\\x0e\\n\\x06parent\\x18= \\x01(\\t\\x12\\x19\\n\\x11\\x63orefMentionIndex\\x18@ \\x03(\\r\\x12\\x1a\\n\\x12\\x65ntityMentionIndex\\x18\\x41 \\x01(\\r\\x12\\r\\n\\x05isMWT\\x18\\x43 \\x01(\\x08\\x12\\x12\\n\\nisFirstMWT\\x18\\x44 \\x01(\\x08\\x12\\x0f\\n\\x07mwtText\\x18\\x45 \\x01(\\t\\x12\\x14\\n\\x0cnumericValue\\x18\\x46 \\x01(\\x04\\x12\\x13\\n\\x0bnumericType\\x18G \\x01(\\t\\x12\\x1d\\n\\x15numericCompositeValue\\x18H \\x01(\\x04\\x12\\x1c\\n\\x14numericCompositeType\\x18I \\x01(\\t\\x12\\x1c\\n\\x14\\x63odepointOffsetBegin\\x18J \\x01(\\r\\x12\\x1a\\n\\x12\\x63odepointOffsetEnd\\x18K \\x01(\\r*\\x05\\x08\\x64\\x10\\x80\\x02\\""\\xe4\\x03\\n\\x05Quote\\x12\\x0c\\n\\x04text\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05\\x62\\x65gin\\x18\\x02 \\x01(\\r\\x12\\x0b\\n\\x03\\x65nd\\x18\\x03 \\x01(\\r\\x12\\x15\\n\\rsentenceBegin\\x18\\x05 \\x01(\\r\\x12\\x13\\n\\x0bsentenceEnd\\x18\\x06 \\x01(\\r\\x12\\x12\\n\\ntokenBegin\\x18\\x07 \\x01(\\r\\x12\\x10\\n\\x08tokenEnd\\x18\\x08 \\x01(\\r\\x12\\r\\n\\x05\\x64ocid\\x18\\t \\x01(\\t\\x12\\r\\n\\x05index\\x18\\n \\x01(\\r\\x12\\x0e\\n\\x06\\x61uthor\\x18\\x0b \\x01(\\t\\x12\\x0f\\n\\x07mention\\x18\\x0c \\x01(\\t\\x12\\x14\\n\\x0cmentionBegin\\x18\\r \\x01(\\r\\x12\\x12\\n\\nmentionEnd\\x18\\x0e \\x01(\\r\\x12\\x13\\n\\x0bmentionType\\x18\\x0f \\x01(\\t\\x12\\x14\\n\\x0cmentionSieve\\x18\\x10 \\x01(\\t\\x12\\x0f\\n\\x07speaker\\x18\\x11 \\x01(\\t\\x12\\x14\\n\\x0cspeakerSieve\\x18\\x12 \\x01(\\t\\x12\\x18\\n\\x10\\x63\\x61nonicalMention\\x18\\x13 \\x01(\\t\\x12\\x1d\\n\\x15\\x63\\x61nonicalMentionBegin\\x18\\x14 \\x01(\\r\\x12\\x1b\\n\\x13\\x63\\x61nonicalMentionEnd\\x18\\x15 \\x01(\\r\\x12N\\n\\x1a\\x61ttributionDependencyGraph\\x18\\x16 \\x01(\\x0b\\x32*.edu.stanford.nlp.pipeline.DependencyGraph\\""\\xc7\\x01\\n\\tParseTree\\x12\\x33\\n\\x05\\x63hild\\x18\\x01 \\x03(\\x0b\\x32$.edu.stanford.nlp.pipeline.ParseTree\\x12\\r\\n\\x05value\\x18\\x02 \\x01(\\t\\x12\\x17\\n\\x0fyieldBeginIndex\\x18\\x03 \\x01(\\r\\x12\\x15\\n\\ryieldEndIndex\\x18\\x04 \\x01(\\r\\x12\\r\\n\\x05score\\x18\\x05 \\x01(\\x01\\x12\\x37\\n\\tsentiment\\x18\\x06 \\x01(\\x0e\\x32$.edu.stanford.nlp.pipeline.Sentiment\\""\\x96\\x03\\n\\x0f\\x44\\x65pendencyGraph\\x12=\\n\\x04node\\x18\\x01 \\x03(\\x0b\\x32/.edu.stanford.nlp.pipeline.DependencyGraph.Node\\x12=\\n\\x04\\x65\\x64ge\\x18\\x02 \\x03(\\x0b\\x32/.edu.stanford.nlp.pipeline.DependencyGraph.Edge\\x12\\x10\\n\\x04root\\x18\\x03 \\x03(\\rB\\x02\\x10\\x01\\x1a\\x44\\n\\x04Node\\x12\\x15\\n\\rsentenceIndex\\x18\\x01 \\x02(\\r\\x12\\r\\n\\x05index\\x18\\x02 \\x02(\\r\\x12\\x16\\n\\x0e\\x63opyAnnotation\\x18\\x03 \\x01(\\r\\x1a\\xac\\x01\\n\\x04\\x45\\x64ge\\x12\\x0e\\n\\x06source\\x18\\x01 \\x02(\\r\\x12\\x0e\\n\\x06target\\x18\\x02 \\x02(\\r\\x12\\x0b\\n\\x03\\x64\\x65p\\x18\\x03 \\x01(\\t\\x12\\x0f\\n\\x07isExtra\\x18\\x04 \\x01(\\x08\\x12\\x12\\n\\nsourceCopy\\x18\\x05 \\x01(\\r\\x12\\x12\\n\\ntargetCopy\\x18\\x06 \\x01(\\r\\x12>\\n\\x08language\\x18\\x07 \\x01(\\x0e\\x32#.edu.stanford.nlp.pipeline.Language:\\x07Unknown\\""\\xc6\\x02\\n\\nCorefChain\\x12\\x0f\\n\\x07\\x63hainID\\x18\\x01 \\x02(\\x05\\x12\\x43\\n\\x07mention\\x18\\x02 \\x03(\\x0b\\x32\\x32.edu.stanford.nlp.pipeline.CorefChain.CorefMention\\x12\\x16\\n\\x0erepresentative\\x18\\x03 \\x02(\\r\\x1a\\xc9\\x01\\n\\x0c\\x43orefMention\\x12\\x11\\n\\tmentionID\\x18\\x01 \\x01(\\x05\\x12\\x13\\n\\x0bmentionType\\x18\\x02 \\x01(\\t\\x12\\x0e\\n\\x06number\\x18\\x03 \\x01(\\t\\x12\\x0e\\n\\x06gender\\x18\\x04 \\x01(\\t\\x12\\x0f\\n\\x07\\x61nimacy\\x18\\x05 \\x01(\\t\\x12\\x12\\n\\nbeginIndex\\x18\\x06 \\x01(\\r\\x12\\x10\\n\\x08\\x65ndIndex\\x18\\x07 \\x01(\\r\\x12\\x11\\n\\theadIndex\\x18\\t \\x01(\\r\\x12\\x15\\n\\rsentenceIndex\\x18\\n \\x01(\\r\\x12\\x10\\n\\x08position\\x18\\x0b \\x01(\\r\\""\\xef\\x08\\n\\x07Mention\\x12\\x11\\n\\tmentionID\\x18\\x01 \\x01(\\x05\\x12\\x13\\n\\x0bmentionType\\x18\\x02 \\x01(\\t\\x12\\x0e\\n\\x06number\\x18\\x03 \\x01(\\t\\x12\\x0e\\n\\x06gender\\x18\\x04 \\x01(\\t\\x12\\x0f\\n\\x07\\x61nimacy\\x18\\x05 \\x01(\\t\\x12\\x0e\\n\\x06person\\x18\\x06 \\x01(\\t\\x12\\x12\\n\\nstartIndex\\x18\\x07 \\x01(\\r\\x12\\x10\\n\\x08\\x65ndIndex\\x18\\t \\x01(\\r\\x12\\x11\\n\\theadIndex\\x18\\n \\x01(\\x05\\x12\\x12\\n\\nheadString\\x18\\x0b \\x01(\\t\\x12\\x11\\n\\tnerString\\x18\\x0c \\x01(\\t\\x12\\x13\\n\\x0boriginalRef\\x18\\r \\x01(\\x05\\x12\\x1a\\n\\x12goldCorefClusterID\\x18\\x0e \\x01(\\x05\\x12\\x16\\n\\x0e\\x63orefClusterID\\x18\\x0f \\x01(\\x05\\x12\\x12\\n\\nmentionNum\\x18\\x10 \\x01(\\x05\\x12\\x0f\\n\\x07sentNum\\x18\\x11 \\x01(\\x05\\x12\\r\\n\\x05utter\\x18\\x12 \\x01(\\x05\\x12\\x11\\n\\tparagraph\\x18\\x13 \\x01(\\x05\\x12\\x11\\n\\tisSubject\\x18\\x14 \\x01(\\x08\\x12\\x16\\n\\x0eisDirectObject\\x18\\x15 \\x01(\\x08\\x12\\x18\\n\\x10isIndirectObject\\x18\\x16 \\x01(\\x08\\x12\\x1b\\n\\x13isPrepositionObject\\x18\\x17 \\x01(\\x08\\x12\\x0f\\n\\x07hasTwin\\x18\\x18 \\x01(\\x08\\x12\\x0f\\n\\x07generic\\x18\\x19 \\x01(\\x08\\x12\\x13\\n\\x0bisSingleton\\x18\\x1a \\x01(\\x08\\x12\\x1a\\n\\x12hasBasicDependency\\x18\\x1b \\x01(\\x08\\x12\\x1d\\n\\x15hasEnhancedDepenedncy\\x18\\x1c \\x01(\\x08\\x12\\x1b\\n\\x13hasContextParseTree\\x18\\x1d \\x01(\\x08\\x12?\\n\\x0fheadIndexedWord\\x18\\x1e \\x01(\\x0b\\x32&.edu.stanford.nlp.pipeline.IndexedWord\\x12=\\n\\rdependingVerb\\x18\\x1f \\x01(\\x0b\\x32&.edu.stanford.nlp.pipeline.IndexedWord\\x12\\x38\\n\\x08headWord\\x18  \\x01(\\x0b\\x32&.edu.stanford.nlp.pipeline.IndexedWord\\x12;\\n\\x0bspeakerInfo\\x18! \\x01(\\x0b\\x32&.edu.stanford.nlp.pipeline.SpeakerInfo\\x12=\\n\\rsentenceWords\\x18\\x32 \\x03(\\x0b\\x32&.edu.stanford.nlp.pipeline.IndexedWord\\x12<\\n\\x0coriginalSpan\\x18\\x33 \\x03(\\x0b\\x32&.edu.stanford.nlp.pipeline.IndexedWord\\x12\\x12\\n\\ndependents\\x18\\x34 \\x03(\\t\\x12\\x19\\n\\x11preprocessedTerms\\x18\\x35 \\x03(\\t\\x12\\x13\\n\\x0b\\x61ppositions\\x18\\x36 \\x03(\\x05\\x12\\x1c\\n\\x14predicateNominatives\\x18\\x37 \\x03(\\x05\\x12\\x18\\n\\x10relativePronouns\\x18\\x38 \\x03(\\x05\\x12\\x13\\n\\x0blistMembers\\x18\\x39 \\x03(\\x05\\x12\\x15\\n\\rbelongToLists\\x18: \\x03(\\x05\\""X\\n\\x0bIndexedWord\\x12\\x13\\n\\x0bsentenceNum\\x18\\x01 \\x01(\\x05\\x12\\x12\\n\\ntokenIndex\\x18\\x02 \\x01(\\x05\\x12\\r\\n\\x05\\x64ocID\\x18\\x03 \\x01(\\x05\\x12\\x11\\n\\tcopyCount\\x18\\x04 \\x01(\\r\\""4\\n\\x0bSpeakerInfo\\x12\\x13\\n\\x0bspeakerName\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x08mentions\\x18\\x02 \\x03(\\x05\\""\\""\\n\\x04Span\\x12\\r\\n\\x05\\x62\\x65gin\\x18\\x01 \\x02(\\r\\x12\\x0b\\n\\x03\\x65nd\\x18\\x02 \\x02(\\r\\""w\\n\\x05Timex\\x12\\r\\n\\x05value\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x08\\x61ltValue\\x18\\x02 \\x01(\\t\\x12\\x0c\\n\\x04text\\x18\\x03 \\x01(\\t\\x12\\x0c\\n\\x04type\\x18\\x04 \\x01(\\t\\x12\\x0b\\n\\x03tid\\x18\\x05 \\x01(\\t\\x12\\x12\\n\\nbeginPoint\\x18\\x06 \\x01(\\r\\x12\\x10\\n\\x08\\x65ndPoint\\x18\\x07 \\x01(\\r\\""\\xdb\\x01\\n\\x06\\x45ntity\\x12\\x11\\n\\theadStart\\x18\\x06 \\x01(\\r\\x12\\x0f\\n\\x07headEnd\\x18\\x07 \\x01(\\r\\x12\\x13\\n\\x0bmentionType\\x18\\x08 \\x01(\\t\\x12\\x16\\n\\x0enormalizedName\\x18\\t \\x01(\\t\\x12\\x16\\n\\x0eheadTokenIndex\\x18\\n \\x01(\\r\\x12\\x0f\\n\\x07\\x63orefID\\x18\\x0b \\x01(\\t\\x12\\x10\\n\\x08objectID\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0b\\x65xtentStart\\x18\\x02 \\x01(\\r\\x12\\x11\\n\\textentEnd\\x18\\x03 \\x01(\\r\\x12\\x0c\\n\\x04type\\x18\\x04 \\x01(\\t\\x12\\x0f\\n\\x07subtype\\x18\\x05 \\x01(\\t\\""\\xb7\\x01\\n\\x08Relation\\x12\\x0f\\n\\x07\\x61rgName\\x18\\x06 \\x03(\\t\\x12.\\n\\x03\\x61rg\\x18\\x07 \\x03(\\x0b\\x32!.edu.stanford.nlp.pipeline.Entity\\x12\\x11\\n\\tsignature\\x18\\x08 \\x01(\\t\\x12\\x10\\n\\x08objectID\\x18\\x01 \\x01(\\t\\x12\\x13\\n\\x0b\\x65xtentStart\\x18\\x02 \\x01(\\r\\x12\\x11\\n\\textentEnd\\x18\\x03 \\x01(\\r\\x12\\x0c\\n\\x04type\\x18\\x04 \\x01(\\t\\x12\\x0f\\n\\x07subtype\\x18\\x05 \\x01(\\t\\""\\xb2\\x01\\n\\x08Operator\\x12\\x0c\\n\\x04name\\x18\\x01 \\x02(\\t\\x12\\x1b\\n\\x13quantifierSpanBegin\\x18\\x02 \\x02(\\x05\\x12\\x19\\n\\x11quantifierSpanEnd\\x18\\x03 \\x02(\\x05\\x12\\x18\\n\\x10subjectSpanBegin\\x18\\x04 \\x02(\\x05\\x12\\x16\\n\\x0esubjectSpanEnd\\x18\\x05 \\x02(\\x05\\x12\\x17\\n\\x0fobjectSpanBegin\\x18\\x06 \\x02(\\x05\\x12\\x15\\n\\robjectSpanEnd\\x18\\x07 \\x02(\\x05\\""\\xa9\\x04\\n\\x08Polarity\\x12K\\n\\x12projectEquivalence\\x18\\x01 \\x02(\\x0e\\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\\x12Q\\n\\x18projectForwardEntailment\\x18\\x02 \\x02(\\x0e\\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\\x12Q\\n\\x18projectReverseEntailment\\x18\\x03 \\x02(\\x0e\\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\\x12H\\n\\x0fprojectNegation\\x18\\x04 \\x02(\\x0e\\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\\x12K\\n\\x12projectAlternation\\x18\\x05 \\x02(\\x0e\\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\\x12\\x45\\n\\x0cprojectCover\\x18\\x06 \\x02(\\x0e\\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\\x12L\\n\\x13projectIndependence\\x18\\x07 \\x02(\\x0e\\x32/.edu.stanford.nlp.pipeline.NaturalLogicRelation\\""\\xdd\\x02\\n\\nNERMention\\x12\\x15\\n\\rsentenceIndex\\x18\\x01 \\x01(\\r\\x12%\\n\\x1dtokenStartInSentenceInclusive\\x18\\x02 \\x02(\\r\\x12#\\n\\x1btokenEndInSentenceExclusive\\x18\\x03 \\x02(\\r\\x12\\x0b\\n\\x03ner\\x18\\x04 \\x02(\\t\\x12\\x15\\n\\rnormalizedNER\\x18\\x05 \\x01(\\t\\x12\\x12\\n\\nentityType\\x18\\x06 \\x01(\\t\\x12/\\n\\x05timex\\x18\\x07 \\x01(\\x0b\\x32 .edu.stanford.nlp.pipeline.Timex\\x12\\x17\\n\\x0fwikipediaEntity\\x18\\x08 \\x01(\\t\\x12\\x0e\\n\\x06gender\\x18\\t \\x01(\\t\\x12\\x1a\\n\\x12\\x65ntityMentionIndex\\x18\\n \\x01(\\r\\x12#\\n\\x1b\\x63\\x61nonicalEntityMentionIndex\\x18\\x0b \\x01(\\r\\x12\\x19\\n\\x11\\x65ntityMentionText\\x18\\x0c \\x01(\\t\\""Y\\n\\x10SentenceFragment\\x12\\x12\\n\\ntokenIndex\\x18\\x01 \\x03(\\r\\x12\\x0c\\n\\x04root\\x18\\x02 \\x01(\\r\\x12\\x14\\n\\x0c\\x61ssumedTruth\\x18\\x03 \\x01(\\x08\\x12\\r\\n\\x05score\\x18\\x04 \\x01(\\x01\\"":\\n\\rTokenLocation\\x12\\x15\\n\\rsentenceIndex\\x18\\x01 \\x01(\\r\\x12\\x12\\n\\ntokenIndex\\x18\\x02 \\x01(\\r\\""\\x9a\\x03\\n\\x0eRelationTriple\\x12\\x0f\\n\\x07subject\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x08relation\\x18\\x02 \\x01(\\t\\x12\\x0e\\n\\x06object\\x18\\x03 \\x01(\\t\\x12\\x12\\n\\nconfidence\\x18\\x04 \\x01(\\x01\\x12?\\n\\rsubjectTokens\\x18\\r \\x03(\\x0b\\x32(.edu.stanford.nlp.pipeline.TokenLocation\\x12@\\n\\x0erelationTokens\\x18\\x0e \\x03(\\x0b\\x32(.edu.stanford.nlp.pipeline.TokenLocation\\x12>\\n\\x0cobjectTokens\\x18\\x0f \\x03(\\x0b\\x32(.edu.stanford.nlp.pipeline.TokenLocation\\x12\\x38\\n\\x04tree\\x18\\x08 \\x01(\\x0b\\x32*.edu.stanford.nlp.pipeline.DependencyGraph\\x12\\x0e\\n\\x06istmod\\x18\\t \\x01(\\x08\\x12\\x10\\n\\x08prefixBe\\x18\\n \\x01(\\x08\\x12\\x10\\n\\x08suffixBe\\x18\\x0b \\x01(\\x08\\x12\\x10\\n\\x08suffixOf\\x18\\x0c \\x01(\\x08\\""-\\n\\x0fMapStringString\\x12\\x0b\\n\\x03key\\x18\\x01 \\x03(\\t\\x12\\r\\n\\x05value\\x18\\x02 \\x03(\\t\\""*\\n\\x0cMapIntString\\x12\\x0b\\n\\x03key\\x18\\x01 \\x03(\\r\\x12\\r\\n\\x05value\\x18\\x02 \\x03(\\t\\""\\xfc\\x01\\n\\x07Section\\x12\\x11\\n\\tcharBegin\\x18\\x01 \\x02(\\r\\x12\\x0f\\n\\x07\\x63harEnd\\x18\\x02 \\x02(\\r\\x12\\x0e\\n\\x06\\x61uthor\\x18\\x03 \\x01(\\t\\x12\\x17\\n\\x0fsentenceIndexes\\x18\\x04 \\x03(\\r\\x12\\x10\\n\\x08\\x64\\x61tetime\\x18\\x05 \\x01(\\t\\x12\\x30\\n\\x06quotes\\x18\\x06 \\x03(\\x0b\\x32 .edu.stanford.nlp.pipeline.Quote\\x12\\x17\\n\\x0f\\x61uthorCharBegin\\x18\\x07 \\x01(\\r\\x12\\x15\\n\\rauthorCharEnd\\x18\\x08 \\x01(\\r\\x12\\x30\\n\\x06xmlTag\\x18\\t \\x02(\\x0b\\x32 .edu.stanford.nlp.pipeline.Token*\\xa3\\x01\\n\\x08Language\\x12\\x0b\\n\\x07Unknown\\x10\\x00\\x12\\x07\\n\\x03\\x41ny\\x10\\x01\\x12\\n\\n\\x06\\x41rabic\\x10\\x02\\x12\\x0b\\n\\x07\\x43hinese\\x10\\x03\\x12\\x0b\\n\\x07\\x45nglish\\x10\\x04\\x12\\n\\n\\x06German\\x10\\x05\\x12\\n\\n\\x06\\x46rench\\x10\\x06\\x12\\n\\n\\x06Hebrew\\x10\\x07\\x12\\x0b\\n\\x07Spanish\\x10\\x08\\x12\\x14\\n\\x10UniversalEnglish\\x10\\t\\x12\\x14\\n\\x10UniversalChinese\\x10\\n*h\\n\\tSentiment\\x12\\x13\\n\\x0fSTRONG_NEGATIVE\\x10\\x00\\x12\\x11\\n\\rWEAK_NEGATIVE\\x10\\x01\\x12\\x0b\\n\\x07NEUTRAL\\x10\\x02\\x12\\x11\\n\\rWEAK_POSITIVE\\x10\\x03\\x12\\x13\\n\\x0fSTRONG_POSITIVE\\x10\\x04*\\x93\\x01\\n\\x14NaturalLogicRelation\\x12\\x0f\\n\\x0b\\x45QUIVALENCE\\x10\\x00\\x12\\x16\\n\\x12\\x46ORWARD_ENTAILMENT\\x10\\x01\\x12\\x16\\n\\x12REVERSE_ENTAILMENT\\x10\\x02\\x12\\x0c\\n\\x08NEGATION\\x10\\x03\\x12\\x0f\\n\\x0b\\x41LTERNATION\\x10\\x04\\x12\\t\\n\\x05\\x43OVER\\x10\\x05\\x12\\x10\\n\\x0cINDEPENDENCE\\x10\\x06\\x42*\\n\\x19\\x65\\x64u.stanford.nlp.pipelineB\\rCoreNLPProtos\'\n)\n\n_LANGUAGE = _descriptor.EnumDescriptor(\n  name=\'Language\',\n  full_name=\'edu.stanford.nlp.pipeline.Language\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'Unknown\', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'Any\', index=1, number=1,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'Arabic\', index=2, number=2,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'Chinese\', index=3, number=3,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'English\', index=4, number=4,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'German\', index=5, number=5,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'French\', index=6, number=6,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'Hebrew\', index=7, number=7,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'Spanish\', index=8, number=8,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'UniversalEnglish\', index=9, number=9,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'UniversalChinese\', index=10, number=10,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=9560,\n  serialized_end=9723,\n)\n_sym_db.RegisterEnumDescriptor(_LANGUAGE)\n\nLanguage = enum_type_wrapper.EnumTypeWrapper(_LANGUAGE)\n_SENTIMENT = _descriptor.EnumDescriptor(\n  name=\'Sentiment\',\n  full_name=\'edu.stanford.nlp.pipeline.Sentiment\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'STRONG_NEGATIVE\', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'WEAK_NEGATIVE\', index=1, number=1,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'NEUTRAL\', index=2, number=2,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'WEAK_POSITIVE\', index=3, number=3,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'STRONG_POSITIVE\', index=4, number=4,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=9725,\n  serialized_end=9829,\n)\n_sym_db.RegisterEnumDescriptor(_SENTIMENT)\n\nSentiment = enum_type_wrapper.EnumTypeWrapper(_SENTIMENT)\n_NATURALLOGICRELATION = _descriptor.EnumDescriptor(\n  name=\'NaturalLogicRelation\',\n  full_name=\'edu.stanford.nlp.pipeline.NaturalLogicRelation\',\n  filename=None,\n  file=DESCRIPTOR,\n  values=[\n    _descriptor.EnumValueDescriptor(\n      name=\'EQUIVALENCE\', index=0, number=0,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'FORWARD_ENTAILMENT\', index=1, number=1,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'REVERSE_ENTAILMENT\', index=2, number=2,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'NEGATION\', index=3, number=3,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'ALTERNATION\', index=4, number=4,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'COVER\', index=5, number=5,\n      serialized_options=None,\n      type=None),\n    _descriptor.EnumValueDescriptor(\n      name=\'INDEPENDENCE\', index=6, number=6,\n      serialized_options=None,\n      type=None),\n  ],\n  containing_type=None,\n  serialized_options=None,\n  serialized_start=9832,\n  serialized_end=9979,\n)\n_sym_db.RegisterEnumDescriptor(_NATURALLOGICRELATION)\n\nNaturalLogicRelation = enum_type_wrapper.EnumTypeWrapper(_NATURALLOGICRELATION)\nUnknown = 0\nAny = 1\nArabic = 2\nChinese = 3\nEnglish = 4\nGerman = 5\nFrench = 6\nHebrew = 7\nSpanish = 8\nUniversalEnglish = 9\nUniversalChinese = 10\nSTRONG_NEGATIVE = 0\nWEAK_NEGATIVE = 1\nNEUTRAL = 2\nWEAK_POSITIVE = 3\nSTRONG_POSITIVE = 4\nEQUIVALENCE = 0\nFORWARD_ENTAILMENT = 1\nREVERSE_ENTAILMENT = 2\nNEGATION = 3\nALTERNATION = 4\nCOVER = 5\nINDEPENDENCE = 6\n\n\n\n_DOCUMENT = _descriptor.Descriptor(\n  name=\'Document\',\n  full_name=\'edu.stanford.nlp.pipeline.Document\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'text\', full_name=\'edu.stanford.nlp.pipeline.Document.text\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sentence\', full_name=\'edu.stanford.nlp.pipeline.Document.sentence\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'corefChain\', full_name=\'edu.stanford.nlp.pipeline.Document.corefChain\', index=2,\n      number=3, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'docID\', full_name=\'edu.stanford.nlp.pipeline.Document.docID\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'docDate\', full_name=\'edu.stanford.nlp.pipeline.Document.docDate\', index=4,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'calendar\', full_name=\'edu.stanford.nlp.pipeline.Document.calendar\', index=5,\n      number=8, type=4, cpp_type=4, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sentencelessToken\', full_name=\'edu.stanford.nlp.pipeline.Document.sentencelessToken\', index=6,\n      number=5, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'character\', full_name=\'edu.stanford.nlp.pipeline.Document.character\', index=7,\n      number=10, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'quote\', full_name=\'edu.stanford.nlp.pipeline.Document.quote\', index=8,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mentions\', full_name=\'edu.stanford.nlp.pipeline.Document.mentions\', index=9,\n      number=9, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hasEntityMentionsAnnotation\', full_name=\'edu.stanford.nlp.pipeline.Document.hasEntityMentionsAnnotation\', index=10,\n      number=13, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'xmlDoc\', full_name=\'edu.stanford.nlp.pipeline.Document.xmlDoc\', index=11,\n      number=11, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sections\', full_name=\'edu.stanford.nlp.pipeline.Document.sections\', index=12,\n      number=12, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mentionsForCoref\', full_name=\'edu.stanford.nlp.pipeline.Document.mentionsForCoref\', index=13,\n      number=14, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hasCorefMentionAnnotation\', full_name=\'edu.stanford.nlp.pipeline.Document.hasCorefMentionAnnotation\', index=14,\n      number=15, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hasCorefAnnotation\', full_name=\'edu.stanford.nlp.pipeline.Document.hasCorefAnnotation\', index=15,\n      number=16, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'corefMentionToEntityMentionMappings\', full_name=\'edu.stanford.nlp.pipeline.Document.corefMentionToEntityMentionMappings\', index=16,\n      number=17, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'entityMentionToCorefMentionMappings\', full_name=\'edu.stanford.nlp.pipeline.Document.entityMentionToCorefMentionMappings\', index=17,\n      number=18, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=True,\n  syntax=\'proto2\',\n  extension_ranges=[(100, 256), ],\n  oneofs=[\n  ],\n  serialized_start=45,\n  serialized_end=782,\n)\n\n\n_SENTENCE = _descriptor.Descriptor(\n  name=\'Sentence\',\n  full_name=\'edu.stanford.nlp.pipeline.Sentence\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'token\', full_name=\'edu.stanford.nlp.pipeline.Sentence.token\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tokenOffsetBegin\', full_name=\'edu.stanford.nlp.pipeline.Sentence.tokenOffsetBegin\', index=1,\n      number=2, type=13, cpp_type=3, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tokenOffsetEnd\', full_name=\'edu.stanford.nlp.pipeline.Sentence.tokenOffsetEnd\', index=2,\n      number=3, type=13, cpp_type=3, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sentenceIndex\', full_name=\'edu.stanford.nlp.pipeline.Sentence.sentenceIndex\', index=3,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'characterOffsetBegin\', full_name=\'edu.stanford.nlp.pipeline.Sentence.characterOffsetBegin\', index=4,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'characterOffsetEnd\', full_name=\'edu.stanford.nlp.pipeline.Sentence.characterOffsetEnd\', index=5,\n      number=6, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'parseTree\', full_name=\'edu.stanford.nlp.pipeline.Sentence.parseTree\', index=6,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'binarizedParseTree\', full_name=\'edu.stanford.nlp.pipeline.Sentence.binarizedParseTree\', index=7,\n      number=31, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'annotatedParseTree\', full_name=\'edu.stanford.nlp.pipeline.Sentence.annotatedParseTree\', index=8,\n      number=32, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sentiment\', full_name=\'edu.stanford.nlp.pipeline.Sentence.sentiment\', index=9,\n      number=33, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'kBestParseTrees\', full_name=\'edu.stanford.nlp.pipeline.Sentence.kBestParseTrees\', index=10,\n      number=34, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'basicDependencies\', full_name=\'edu.stanford.nlp.pipeline.Sentence.basicDependencies\', index=11,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'collapsedDependencies\', full_name=\'edu.stanford.nlp.pipeline.Sentence.collapsedDependencies\', index=12,\n      number=9, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'collapsedCCProcessedDependencies\', full_name=\'edu.stanford.nlp.pipeline.Sentence.collapsedCCProcessedDependencies\', index=13,\n      number=10, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'alternativeDependencies\', full_name=\'edu.stanford.nlp.pipeline.Sentence.alternativeDependencies\', index=14,\n      number=13, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'openieTriple\', full_name=\'edu.stanford.nlp.pipeline.Sentence.openieTriple\', index=15,\n      number=14, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'kbpTriple\', full_name=\'edu.stanford.nlp.pipeline.Sentence.kbpTriple\', index=16,\n      number=16, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'entailedSentence\', full_name=\'edu.stanford.nlp.pipeline.Sentence.entailedSentence\', index=17,\n      number=15, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'entailedClause\', full_name=\'edu.stanford.nlp.pipeline.Sentence.entailedClause\', index=18,\n      number=35, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'enhancedDependencies\', full_name=\'edu.stanford.nlp.pipeline.Sentence.enhancedDependencies\', index=19,\n      number=17, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'enhancedPlusPlusDependencies\', full_name=\'edu.stanford.nlp.pipeline.Sentence.enhancedPlusPlusDependencies\', index=20,\n      number=18, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'character\', full_name=\'edu.stanford.nlp.pipeline.Sentence.character\', index=21,\n      number=19, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'paragraph\', full_name=\'edu.stanford.nlp.pipeline.Sentence.paragraph\', index=22,\n      number=11, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'text\', full_name=\'edu.stanford.nlp.pipeline.Sentence.text\', index=23,\n      number=12, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'lineNumber\', full_name=\'edu.stanford.nlp.pipeline.Sentence.lineNumber\', index=24,\n      number=20, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hasRelationAnnotations\', full_name=\'edu.stanford.nlp.pipeline.Sentence.hasRelationAnnotations\', index=25,\n      number=51, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'entity\', full_name=\'edu.stanford.nlp.pipeline.Sentence.entity\', index=26,\n      number=52, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'relation\', full_name=\'edu.stanford.nlp.pipeline.Sentence.relation\', index=27,\n      number=53, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hasNumerizedTokensAnnotation\', full_name=\'edu.stanford.nlp.pipeline.Sentence.hasNumerizedTokensAnnotation\', index=28,\n      number=54, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mentions\', full_name=\'edu.stanford.nlp.pipeline.Sentence.mentions\', index=29,\n      number=55, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mentionsForCoref\', full_name=\'edu.stanford.nlp.pipeline.Sentence.mentionsForCoref\', index=30,\n      number=56, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hasCorefMentionsAnnotation\', full_name=\'edu.stanford.nlp.pipeline.Sentence.hasCorefMentionsAnnotation\', index=31,\n      number=57, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sentenceID\', full_name=\'edu.stanford.nlp.pipeline.Sentence.sentenceID\', index=32,\n      number=58, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sectionDate\', full_name=\'edu.stanford.nlp.pipeline.Sentence.sectionDate\', index=33,\n      number=59, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sectionIndex\', full_name=\'edu.stanford.nlp.pipeline.Sentence.sectionIndex\', index=34,\n      number=60, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sectionName\', full_name=\'edu.stanford.nlp.pipeline.Sentence.sectionName\', index=35,\n      number=61, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sectionAuthor\', full_name=\'edu.stanford.nlp.pipeline.Sentence.sectionAuthor\', index=36,\n      number=62, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'docID\', full_name=\'edu.stanford.nlp.pipeline.Sentence.docID\', index=37,\n      number=63, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sectionQuoted\', full_name=\'edu.stanford.nlp.pipeline.Sentence.sectionQuoted\', index=38,\n      number=64, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hasEntityMentionsAnnotation\', full_name=\'edu.stanford.nlp.pipeline.Sentence.hasEntityMentionsAnnotation\', index=39,\n      number=65, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hasKBPTriplesAnnotation\', full_name=\'edu.stanford.nlp.pipeline.Sentence.hasKBPTriplesAnnotation\', index=40,\n      number=68, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hasOpenieTriplesAnnotation\', full_name=\'edu.stanford.nlp.pipeline.Sentence.hasOpenieTriplesAnnotation\', index=41,\n      number=69, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'chapterIndex\', full_name=\'edu.stanford.nlp.pipeline.Sentence.chapterIndex\', index=42,\n      number=66, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'paragraphIndex\', full_name=\'edu.stanford.nlp.pipeline.Sentence.paragraphIndex\', index=43,\n      number=67, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=True,\n  syntax=\'proto2\',\n  extension_ranges=[(100, 256), ],\n  oneofs=[\n  ],\n  serialized_start=785,\n  serialized_end=2719,\n)\n\n\n_TOKEN = _descriptor.Descriptor(\n  name=\'Token\',\n  full_name=\'edu.stanford.nlp.pipeline.Token\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'word\', full_name=\'edu.stanford.nlp.pipeline.Token.word\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'pos\', full_name=\'edu.stanford.nlp.pipeline.Token.pos\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'edu.stanford.nlp.pipeline.Token.value\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'category\', full_name=\'edu.stanford.nlp.pipeline.Token.category\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'before\', full_name=\'edu.stanford.nlp.pipeline.Token.before\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'after\', full_name=\'edu.stanford.nlp.pipeline.Token.after\', index=5,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'originalText\', full_name=\'edu.stanford.nlp.pipeline.Token.originalText\', index=6,\n      number=7, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'ner\', full_name=\'edu.stanford.nlp.pipeline.Token.ner\', index=7,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'coarseNER\', full_name=\'edu.stanford.nlp.pipeline.Token.coarseNER\', index=8,\n      number=62, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'fineGrainedNER\', full_name=\'edu.stanford.nlp.pipeline.Token.fineGrainedNER\', index=9,\n      number=63, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'nerLabelProbs\', full_name=\'edu.stanford.nlp.pipeline.Token.nerLabelProbs\', index=10,\n      number=66, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'normalizedNER\', full_name=\'edu.stanford.nlp.pipeline.Token.normalizedNER\', index=11,\n      number=9, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'lemma\', full_name=\'edu.stanford.nlp.pipeline.Token.lemma\', index=12,\n      number=10, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'beginChar\', full_name=\'edu.stanford.nlp.pipeline.Token.beginChar\', index=13,\n      number=11, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'endChar\', full_name=\'edu.stanford.nlp.pipeline.Token.endChar\', index=14,\n      number=12, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'utterance\', full_name=\'edu.stanford.nlp.pipeline.Token.utterance\', index=15,\n      number=13, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'speaker\', full_name=\'edu.stanford.nlp.pipeline.Token.speaker\', index=16,\n      number=14, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'beginIndex\', full_name=\'edu.stanford.nlp.pipeline.Token.beginIndex\', index=17,\n      number=15, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'endIndex\', full_name=\'edu.stanford.nlp.pipeline.Token.endIndex\', index=18,\n      number=16, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tokenBeginIndex\', full_name=\'edu.stanford.nlp.pipeline.Token.tokenBeginIndex\', index=19,\n      number=17, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tokenEndIndex\', full_name=\'edu.stanford.nlp.pipeline.Token.tokenEndIndex\', index=20,\n      number=18, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'timexValue\', full_name=\'edu.stanford.nlp.pipeline.Token.timexValue\', index=21,\n      number=19, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hasXmlContext\', full_name=\'edu.stanford.nlp.pipeline.Token.hasXmlContext\', index=22,\n      number=21, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'xmlContext\', full_name=\'edu.stanford.nlp.pipeline.Token.xmlContext\', index=23,\n      number=22, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'corefClusterID\', full_name=\'edu.stanford.nlp.pipeline.Token.corefClusterID\', index=24,\n      number=23, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'answer\', full_name=\'edu.stanford.nlp.pipeline.Token.answer\', index=25,\n      number=24, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'headWordIndex\', full_name=\'edu.stanford.nlp.pipeline.Token.headWordIndex\', index=26,\n      number=26, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'operator\', full_name=\'edu.stanford.nlp.pipeline.Token.operator\', index=27,\n      number=27, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'polarity\', full_name=\'edu.stanford.nlp.pipeline.Token.polarity\', index=28,\n      number=28, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'polarity_dir\', full_name=\'edu.stanford.nlp.pipeline.Token.polarity_dir\', index=29,\n      number=39, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'span\', full_name=\'edu.stanford.nlp.pipeline.Token.span\', index=30,\n      number=29, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sentiment\', full_name=\'edu.stanford.nlp.pipeline.Token.sentiment\', index=31,\n      number=30, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'quotationIndex\', full_name=\'edu.stanford.nlp.pipeline.Token.quotationIndex\', index=32,\n      number=31, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'conllUFeatures\', full_name=\'edu.stanford.nlp.pipeline.Token.conllUFeatures\', index=33,\n      number=32, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'coarseTag\', full_name=\'edu.stanford.nlp.pipeline.Token.coarseTag\', index=34,\n      number=33, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'conllUTokenSpan\', full_name=\'edu.stanford.nlp.pipeline.Token.conllUTokenSpan\', index=35,\n      number=34, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'conllUMisc\', full_name=\'edu.stanford.nlp.pipeline.Token.conllUMisc\', index=36,\n      number=35, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'conllUSecondaryDeps\', full_name=\'edu.stanford.nlp.pipeline.Token.conllUSecondaryDeps\', index=37,\n      number=36, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'wikipediaEntity\', full_name=\'edu.stanford.nlp.pipeline.Token.wikipediaEntity\', index=38,\n      number=37, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'isNewline\', full_name=\'edu.stanford.nlp.pipeline.Token.isNewline\', index=39,\n      number=38, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'gender\', full_name=\'edu.stanford.nlp.pipeline.Token.gender\', index=40,\n      number=51, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'trueCase\', full_name=\'edu.stanford.nlp.pipeline.Token.trueCase\', index=41,\n      number=52, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'trueCaseText\', full_name=\'edu.stanford.nlp.pipeline.Token.trueCaseText\', index=42,\n      number=53, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'chineseChar\', full_name=\'edu.stanford.nlp.pipeline.Token.chineseChar\', index=43,\n      number=54, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'chineseSeg\', full_name=\'edu.stanford.nlp.pipeline.Token.chineseSeg\', index=44,\n      number=55, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'chineseXMLChar\', full_name=\'edu.stanford.nlp.pipeline.Token.chineseXMLChar\', index=45,\n      number=60, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sectionName\', full_name=\'edu.stanford.nlp.pipeline.Token.sectionName\', index=46,\n      number=56, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sectionAuthor\', full_name=\'edu.stanford.nlp.pipeline.Token.sectionAuthor\', index=47,\n      number=57, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sectionDate\', full_name=\'edu.stanford.nlp.pipeline.Token.sectionDate\', index=48,\n      number=58, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sectionEndLabel\', full_name=\'edu.stanford.nlp.pipeline.Token.sectionEndLabel\', index=49,\n      number=59, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'parent\', full_name=\'edu.stanford.nlp.pipeline.Token.parent\', index=50,\n      number=61, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'corefMentionIndex\', full_name=\'edu.stanford.nlp.pipeline.Token.corefMentionIndex\', index=51,\n      number=64, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'entityMentionIndex\', full_name=\'edu.stanford.nlp.pipeline.Token.entityMentionIndex\', index=52,\n      number=65, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'isMWT\', full_name=\'edu.stanford.nlp.pipeline.Token.isMWT\', index=53,\n      number=67, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'isFirstMWT\', full_name=\'edu.stanford.nlp.pipeline.Token.isFirstMWT\', index=54,\n      number=68, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mwtText\', full_name=\'edu.stanford.nlp.pipeline.Token.mwtText\', index=55,\n      number=69, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'numericValue\', full_name=\'edu.stanford.nlp.pipeline.Token.numericValue\', index=56,\n      number=70, type=4, cpp_type=4, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'numericType\', full_name=\'edu.stanford.nlp.pipeline.Token.numericType\', index=57,\n      number=71, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'numericCompositeValue\', full_name=\'edu.stanford.nlp.pipeline.Token.numericCompositeValue\', index=58,\n      number=72, type=4, cpp_type=4, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'numericCompositeType\', full_name=\'edu.stanford.nlp.pipeline.Token.numericCompositeType\', index=59,\n      number=73, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'codepointOffsetBegin\', full_name=\'edu.stanford.nlp.pipeline.Token.codepointOffsetBegin\', index=60,\n      number=74, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'codepointOffsetEnd\', full_name=\'edu.stanford.nlp.pipeline.Token.codepointOffsetEnd\', index=61,\n      number=75, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=True,\n  syntax=\'proto2\',\n  extension_ranges=[(100, 256), ],\n  oneofs=[\n  ],\n  serialized_start=2722,\n  serialized_end=4284,\n)\n\n\n_QUOTE = _descriptor.Descriptor(\n  name=\'Quote\',\n  full_name=\'edu.stanford.nlp.pipeline.Quote\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'text\', full_name=\'edu.stanford.nlp.pipeline.Quote.text\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'begin\', full_name=\'edu.stanford.nlp.pipeline.Quote.begin\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'end\', full_name=\'edu.stanford.nlp.pipeline.Quote.end\', index=2,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sentenceBegin\', full_name=\'edu.stanford.nlp.pipeline.Quote.sentenceBegin\', index=3,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sentenceEnd\', full_name=\'edu.stanford.nlp.pipeline.Quote.sentenceEnd\', index=4,\n      number=6, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tokenBegin\', full_name=\'edu.stanford.nlp.pipeline.Quote.tokenBegin\', index=5,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tokenEnd\', full_name=\'edu.stanford.nlp.pipeline.Quote.tokenEnd\', index=6,\n      number=8, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'docid\', full_name=\'edu.stanford.nlp.pipeline.Quote.docid\', index=7,\n      number=9, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'index\', full_name=\'edu.stanford.nlp.pipeline.Quote.index\', index=8,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'author\', full_name=\'edu.stanford.nlp.pipeline.Quote.author\', index=9,\n      number=11, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mention\', full_name=\'edu.stanford.nlp.pipeline.Quote.mention\', index=10,\n      number=12, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mentionBegin\', full_name=\'edu.stanford.nlp.pipeline.Quote.mentionBegin\', index=11,\n      number=13, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mentionEnd\', full_name=\'edu.stanford.nlp.pipeline.Quote.mentionEnd\', index=12,\n      number=14, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mentionType\', full_name=\'edu.stanford.nlp.pipeline.Quote.mentionType\', index=13,\n      number=15, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mentionSieve\', full_name=\'edu.stanford.nlp.pipeline.Quote.mentionSieve\', index=14,\n      number=16, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'speaker\', full_name=\'edu.stanford.nlp.pipeline.Quote.speaker\', index=15,\n      number=17, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'speakerSieve\', full_name=\'edu.stanford.nlp.pipeline.Quote.speakerSieve\', index=16,\n      number=18, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'canonicalMention\', full_name=\'edu.stanford.nlp.pipeline.Quote.canonicalMention\', index=17,\n      number=19, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'canonicalMentionBegin\', full_name=\'edu.stanford.nlp.pipeline.Quote.canonicalMentionBegin\', index=18,\n      number=20, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'canonicalMentionEnd\', full_name=\'edu.stanford.nlp.pipeline.Quote.canonicalMentionEnd\', index=19,\n      number=21, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'attributionDependencyGraph\', full_name=\'edu.stanford.nlp.pipeline.Quote.attributionDependencyGraph\', index=20,\n      number=22, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4287,\n  serialized_end=4771,\n)\n\n\n_PARSETREE = _descriptor.Descriptor(\n  name=\'ParseTree\',\n  full_name=\'edu.stanford.nlp.pipeline.ParseTree\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'child\', full_name=\'edu.stanford.nlp.pipeline.ParseTree.child\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'edu.stanford.nlp.pipeline.ParseTree.value\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'yieldBeginIndex\', full_name=\'edu.stanford.nlp.pipeline.ParseTree.yieldBeginIndex\', index=2,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'yieldEndIndex\', full_name=\'edu.stanford.nlp.pipeline.ParseTree.yieldEndIndex\', index=3,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'score\', full_name=\'edu.stanford.nlp.pipeline.ParseTree.score\', index=4,\n      number=5, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sentiment\', full_name=\'edu.stanford.nlp.pipeline.ParseTree.sentiment\', index=5,\n      number=6, type=14, cpp_type=8, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4774,\n  serialized_end=4973,\n)\n\n\n_DEPENDENCYGRAPH_NODE = _descriptor.Descriptor(\n  name=\'Node\',\n  full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.Node\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'sentenceIndex\', full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.Node.sentenceIndex\', index=0,\n      number=1, type=13, cpp_type=3, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'index\', full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.Node.index\', index=1,\n      number=2, type=13, cpp_type=3, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'copyAnnotation\', full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.Node.copyAnnotation\', index=2,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5139,\n  serialized_end=5207,\n)\n\n_DEPENDENCYGRAPH_EDGE = _descriptor.Descriptor(\n  name=\'Edge\',\n  full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.Edge\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'source\', full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.Edge.source\', index=0,\n      number=1, type=13, cpp_type=3, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'target\', full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.Edge.target\', index=1,\n      number=2, type=13, cpp_type=3, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'dep\', full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.Edge.dep\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'isExtra\', full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.Edge.isExtra\', index=3,\n      number=4, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sourceCopy\', full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.Edge.sourceCopy\', index=4,\n      number=5, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'targetCopy\', full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.Edge.targetCopy\', index=5,\n      number=6, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'language\', full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.Edge.language\', index=6,\n      number=7, type=14, cpp_type=8, label=1,\n      has_default_value=True, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5210,\n  serialized_end=5382,\n)\n\n_DEPENDENCYGRAPH = _descriptor.Descriptor(\n  name=\'DependencyGraph\',\n  full_name=\'edu.stanford.nlp.pipeline.DependencyGraph\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'node\', full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.node\', index=0,\n      number=1, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'edge\', full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.edge\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'root\', full_name=\'edu.stanford.nlp.pipeline.DependencyGraph.root\', index=2,\n      number=3, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=b\'\\020\\001\', file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_DEPENDENCYGRAPH_NODE, _DEPENDENCYGRAPH_EDGE, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=4976,\n  serialized_end=5382,\n)\n\n\n_COREFCHAIN_COREFMENTION = _descriptor.Descriptor(\n  name=\'CorefMention\',\n  full_name=\'edu.stanford.nlp.pipeline.CorefChain.CorefMention\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'mentionID\', full_name=\'edu.stanford.nlp.pipeline.CorefChain.CorefMention.mentionID\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mentionType\', full_name=\'edu.stanford.nlp.pipeline.CorefChain.CorefMention.mentionType\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'number\', full_name=\'edu.stanford.nlp.pipeline.CorefChain.CorefMention.number\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'gender\', full_name=\'edu.stanford.nlp.pipeline.CorefChain.CorefMention.gender\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'animacy\', full_name=\'edu.stanford.nlp.pipeline.CorefChain.CorefMention.animacy\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'beginIndex\', full_name=\'edu.stanford.nlp.pipeline.CorefChain.CorefMention.beginIndex\', index=5,\n      number=6, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'endIndex\', full_name=\'edu.stanford.nlp.pipeline.CorefChain.CorefMention.endIndex\', index=6,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'headIndex\', full_name=\'edu.stanford.nlp.pipeline.CorefChain.CorefMention.headIndex\', index=7,\n      number=9, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sentenceIndex\', full_name=\'edu.stanford.nlp.pipeline.CorefChain.CorefMention.sentenceIndex\', index=8,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'position\', full_name=\'edu.stanford.nlp.pipeline.CorefChain.CorefMention.position\', index=9,\n      number=11, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5510,\n  serialized_end=5711,\n)\n\n_COREFCHAIN = _descriptor.Descriptor(\n  name=\'CorefChain\',\n  full_name=\'edu.stanford.nlp.pipeline.CorefChain\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'chainID\', full_name=\'edu.stanford.nlp.pipeline.CorefChain.chainID\', index=0,\n      number=1, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mention\', full_name=\'edu.stanford.nlp.pipeline.CorefChain.mention\', index=1,\n      number=2, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'representative\', full_name=\'edu.stanford.nlp.pipeline.CorefChain.representative\', index=2,\n      number=3, type=13, cpp_type=3, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[_COREFCHAIN_COREFMENTION, ],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5385,\n  serialized_end=5711,\n)\n\n\n_MENTION = _descriptor.Descriptor(\n  name=\'Mention\',\n  full_name=\'edu.stanford.nlp.pipeline.Mention\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'mentionID\', full_name=\'edu.stanford.nlp.pipeline.Mention.mentionID\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mentionType\', full_name=\'edu.stanford.nlp.pipeline.Mention.mentionType\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'number\', full_name=\'edu.stanford.nlp.pipeline.Mention.number\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'gender\', full_name=\'edu.stanford.nlp.pipeline.Mention.gender\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'animacy\', full_name=\'edu.stanford.nlp.pipeline.Mention.animacy\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'person\', full_name=\'edu.stanford.nlp.pipeline.Mention.person\', index=5,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'startIndex\', full_name=\'edu.stanford.nlp.pipeline.Mention.startIndex\', index=6,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'endIndex\', full_name=\'edu.stanford.nlp.pipeline.Mention.endIndex\', index=7,\n      number=9, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'headIndex\', full_name=\'edu.stanford.nlp.pipeline.Mention.headIndex\', index=8,\n      number=10, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'headString\', full_name=\'edu.stanford.nlp.pipeline.Mention.headString\', index=9,\n      number=11, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'nerString\', full_name=\'edu.stanford.nlp.pipeline.Mention.nerString\', index=10,\n      number=12, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'originalRef\', full_name=\'edu.stanford.nlp.pipeline.Mention.originalRef\', index=11,\n      number=13, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'goldCorefClusterID\', full_name=\'edu.stanford.nlp.pipeline.Mention.goldCorefClusterID\', index=12,\n      number=14, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'corefClusterID\', full_name=\'edu.stanford.nlp.pipeline.Mention.corefClusterID\', index=13,\n      number=15, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mentionNum\', full_name=\'edu.stanford.nlp.pipeline.Mention.mentionNum\', index=14,\n      number=16, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sentNum\', full_name=\'edu.stanford.nlp.pipeline.Mention.sentNum\', index=15,\n      number=17, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'utter\', full_name=\'edu.stanford.nlp.pipeline.Mention.utter\', index=16,\n      number=18, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'paragraph\', full_name=\'edu.stanford.nlp.pipeline.Mention.paragraph\', index=17,\n      number=19, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'isSubject\', full_name=\'edu.stanford.nlp.pipeline.Mention.isSubject\', index=18,\n      number=20, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'isDirectObject\', full_name=\'edu.stanford.nlp.pipeline.Mention.isDirectObject\', index=19,\n      number=21, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'isIndirectObject\', full_name=\'edu.stanford.nlp.pipeline.Mention.isIndirectObject\', index=20,\n      number=22, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'isPrepositionObject\', full_name=\'edu.stanford.nlp.pipeline.Mention.isPrepositionObject\', index=21,\n      number=23, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hasTwin\', full_name=\'edu.stanford.nlp.pipeline.Mention.hasTwin\', index=22,\n      number=24, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'generic\', full_name=\'edu.stanford.nlp.pipeline.Mention.generic\', index=23,\n      number=25, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'isSingleton\', full_name=\'edu.stanford.nlp.pipeline.Mention.isSingleton\', index=24,\n      number=26, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hasBasicDependency\', full_name=\'edu.stanford.nlp.pipeline.Mention.hasBasicDependency\', index=25,\n      number=27, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hasEnhancedDepenedncy\', full_name=\'edu.stanford.nlp.pipeline.Mention.hasEnhancedDepenedncy\', index=26,\n      number=28, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'hasContextParseTree\', full_name=\'edu.stanford.nlp.pipeline.Mention.hasContextParseTree\', index=27,\n      number=29, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'headIndexedWord\', full_name=\'edu.stanford.nlp.pipeline.Mention.headIndexedWord\', index=28,\n      number=30, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'dependingVerb\', full_name=\'edu.stanford.nlp.pipeline.Mention.dependingVerb\', index=29,\n      number=31, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'headWord\', full_name=\'edu.stanford.nlp.pipeline.Mention.headWord\', index=30,\n      number=32, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'speakerInfo\', full_name=\'edu.stanford.nlp.pipeline.Mention.speakerInfo\', index=31,\n      number=33, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sentenceWords\', full_name=\'edu.stanford.nlp.pipeline.Mention.sentenceWords\', index=32,\n      number=50, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'originalSpan\', full_name=\'edu.stanford.nlp.pipeline.Mention.originalSpan\', index=33,\n      number=51, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'dependents\', full_name=\'edu.stanford.nlp.pipeline.Mention.dependents\', index=34,\n      number=52, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'preprocessedTerms\', full_name=\'edu.stanford.nlp.pipeline.Mention.preprocessedTerms\', index=35,\n      number=53, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'appositions\', full_name=\'edu.stanford.nlp.pipeline.Mention.appositions\', index=36,\n      number=54, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'predicateNominatives\', full_name=\'edu.stanford.nlp.pipeline.Mention.predicateNominatives\', index=37,\n      number=55, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'relativePronouns\', full_name=\'edu.stanford.nlp.pipeline.Mention.relativePronouns\', index=38,\n      number=56, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'listMembers\', full_name=\'edu.stanford.nlp.pipeline.Mention.listMembers\', index=39,\n      number=57, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'belongToLists\', full_name=\'edu.stanford.nlp.pipeline.Mention.belongToLists\', index=40,\n      number=58, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=5714,\n  serialized_end=6849,\n)\n\n\n_INDEXEDWORD = _descriptor.Descriptor(\n  name=\'IndexedWord\',\n  full_name=\'edu.stanford.nlp.pipeline.IndexedWord\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'sentenceNum\', full_name=\'edu.stanford.nlp.pipeline.IndexedWord.sentenceNum\', index=0,\n      number=1, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tokenIndex\', full_name=\'edu.stanford.nlp.pipeline.IndexedWord.tokenIndex\', index=1,\n      number=2, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'docID\', full_name=\'edu.stanford.nlp.pipeline.IndexedWord.docID\', index=2,\n      number=3, type=5, cpp_type=1, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'copyCount\', full_name=\'edu.stanford.nlp.pipeline.IndexedWord.copyCount\', index=3,\n      number=4, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=6851,\n  serialized_end=6939,\n)\n\n\n_SPEAKERINFO = _descriptor.Descriptor(\n  name=\'SpeakerInfo\',\n  full_name=\'edu.stanford.nlp.pipeline.SpeakerInfo\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'speakerName\', full_name=\'edu.stanford.nlp.pipeline.SpeakerInfo.speakerName\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mentions\', full_name=\'edu.stanford.nlp.pipeline.SpeakerInfo.mentions\', index=1,\n      number=2, type=5, cpp_type=1, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=6941,\n  serialized_end=6993,\n)\n\n\n_SPAN = _descriptor.Descriptor(\n  name=\'Span\',\n  full_name=\'edu.stanford.nlp.pipeline.Span\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'begin\', full_name=\'edu.stanford.nlp.pipeline.Span.begin\', index=0,\n      number=1, type=13, cpp_type=3, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'end\', full_name=\'edu.stanford.nlp.pipeline.Span.end\', index=1,\n      number=2, type=13, cpp_type=3, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=6995,\n  serialized_end=7029,\n)\n\n\n_TIMEX = _descriptor.Descriptor(\n  name=\'Timex\',\n  full_name=\'edu.stanford.nlp.pipeline.Timex\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'edu.stanford.nlp.pipeline.Timex.value\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'altValue\', full_name=\'edu.stanford.nlp.pipeline.Timex.altValue\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'text\', full_name=\'edu.stanford.nlp.pipeline.Timex.text\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'edu.stanford.nlp.pipeline.Timex.type\', index=3,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tid\', full_name=\'edu.stanford.nlp.pipeline.Timex.tid\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'beginPoint\', full_name=\'edu.stanford.nlp.pipeline.Timex.beginPoint\', index=5,\n      number=6, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'endPoint\', full_name=\'edu.stanford.nlp.pipeline.Timex.endPoint\', index=6,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=7031,\n  serialized_end=7150,\n)\n\n\n_ENTITY = _descriptor.Descriptor(\n  name=\'Entity\',\n  full_name=\'edu.stanford.nlp.pipeline.Entity\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'headStart\', full_name=\'edu.stanford.nlp.pipeline.Entity.headStart\', index=0,\n      number=6, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'headEnd\', full_name=\'edu.stanford.nlp.pipeline.Entity.headEnd\', index=1,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'mentionType\', full_name=\'edu.stanford.nlp.pipeline.Entity.mentionType\', index=2,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'normalizedName\', full_name=\'edu.stanford.nlp.pipeline.Entity.normalizedName\', index=3,\n      number=9, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'headTokenIndex\', full_name=\'edu.stanford.nlp.pipeline.Entity.headTokenIndex\', index=4,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'corefID\', full_name=\'edu.stanford.nlp.pipeline.Entity.corefID\', index=5,\n      number=11, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'objectID\', full_name=\'edu.stanford.nlp.pipeline.Entity.objectID\', index=6,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'extentStart\', full_name=\'edu.stanford.nlp.pipeline.Entity.extentStart\', index=7,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'extentEnd\', full_name=\'edu.stanford.nlp.pipeline.Entity.extentEnd\', index=8,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'edu.stanford.nlp.pipeline.Entity.type\', index=9,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'subtype\', full_name=\'edu.stanford.nlp.pipeline.Entity.subtype\', index=10,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=7153,\n  serialized_end=7372,\n)\n\n\n_RELATION = _descriptor.Descriptor(\n  name=\'Relation\',\n  full_name=\'edu.stanford.nlp.pipeline.Relation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'argName\', full_name=\'edu.stanford.nlp.pipeline.Relation.argName\', index=0,\n      number=6, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'arg\', full_name=\'edu.stanford.nlp.pipeline.Relation.arg\', index=1,\n      number=7, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'signature\', full_name=\'edu.stanford.nlp.pipeline.Relation.signature\', index=2,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'objectID\', full_name=\'edu.stanford.nlp.pipeline.Relation.objectID\', index=3,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'extentStart\', full_name=\'edu.stanford.nlp.pipeline.Relation.extentStart\', index=4,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'extentEnd\', full_name=\'edu.stanford.nlp.pipeline.Relation.extentEnd\', index=5,\n      number=3, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'type\', full_name=\'edu.stanford.nlp.pipeline.Relation.type\', index=6,\n      number=4, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'subtype\', full_name=\'edu.stanford.nlp.pipeline.Relation.subtype\', index=7,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=7375,\n  serialized_end=7558,\n)\n\n\n_OPERATOR = _descriptor.Descriptor(\n  name=\'Operator\',\n  full_name=\'edu.stanford.nlp.pipeline.Operator\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'name\', full_name=\'edu.stanford.nlp.pipeline.Operator.name\', index=0,\n      number=1, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'quantifierSpanBegin\', full_name=\'edu.stanford.nlp.pipeline.Operator.quantifierSpanBegin\', index=1,\n      number=2, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'quantifierSpanEnd\', full_name=\'edu.stanford.nlp.pipeline.Operator.quantifierSpanEnd\', index=2,\n      number=3, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'subjectSpanBegin\', full_name=\'edu.stanford.nlp.pipeline.Operator.subjectSpanBegin\', index=3,\n      number=4, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'subjectSpanEnd\', full_name=\'edu.stanford.nlp.pipeline.Operator.subjectSpanEnd\', index=4,\n      number=5, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'objectSpanBegin\', full_name=\'edu.stanford.nlp.pipeline.Operator.objectSpanBegin\', index=5,\n      number=6, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'objectSpanEnd\', full_name=\'edu.stanford.nlp.pipeline.Operator.objectSpanEnd\', index=6,\n      number=7, type=5, cpp_type=1, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=7561,\n  serialized_end=7739,\n)\n\n\n_POLARITY = _descriptor.Descriptor(\n  name=\'Polarity\',\n  full_name=\'edu.stanford.nlp.pipeline.Polarity\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'projectEquivalence\', full_name=\'edu.stanford.nlp.pipeline.Polarity.projectEquivalence\', index=0,\n      number=1, type=14, cpp_type=8, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'projectForwardEntailment\', full_name=\'edu.stanford.nlp.pipeline.Polarity.projectForwardEntailment\', index=1,\n      number=2, type=14, cpp_type=8, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'projectReverseEntailment\', full_name=\'edu.stanford.nlp.pipeline.Polarity.projectReverseEntailment\', index=2,\n      number=3, type=14, cpp_type=8, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'projectNegation\', full_name=\'edu.stanford.nlp.pipeline.Polarity.projectNegation\', index=3,\n      number=4, type=14, cpp_type=8, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'projectAlternation\', full_name=\'edu.stanford.nlp.pipeline.Polarity.projectAlternation\', index=4,\n      number=5, type=14, cpp_type=8, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'projectCover\', full_name=\'edu.stanford.nlp.pipeline.Polarity.projectCover\', index=5,\n      number=6, type=14, cpp_type=8, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'projectIndependence\', full_name=\'edu.stanford.nlp.pipeline.Polarity.projectIndependence\', index=6,\n      number=7, type=14, cpp_type=8, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=7742,\n  serialized_end=8295,\n)\n\n\n_NERMENTION = _descriptor.Descriptor(\n  name=\'NERMention\',\n  full_name=\'edu.stanford.nlp.pipeline.NERMention\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'sentenceIndex\', full_name=\'edu.stanford.nlp.pipeline.NERMention.sentenceIndex\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tokenStartInSentenceInclusive\', full_name=\'edu.stanford.nlp.pipeline.NERMention.tokenStartInSentenceInclusive\', index=1,\n      number=2, type=13, cpp_type=3, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tokenEndInSentenceExclusive\', full_name=\'edu.stanford.nlp.pipeline.NERMention.tokenEndInSentenceExclusive\', index=2,\n      number=3, type=13, cpp_type=3, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'ner\', full_name=\'edu.stanford.nlp.pipeline.NERMention.ner\', index=3,\n      number=4, type=9, cpp_type=9, label=2,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'normalizedNER\', full_name=\'edu.stanford.nlp.pipeline.NERMention.normalizedNER\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'entityType\', full_name=\'edu.stanford.nlp.pipeline.NERMention.entityType\', index=5,\n      number=6, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'timex\', full_name=\'edu.stanford.nlp.pipeline.NERMention.timex\', index=6,\n      number=7, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'wikipediaEntity\', full_name=\'edu.stanford.nlp.pipeline.NERMention.wikipediaEntity\', index=7,\n      number=8, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'gender\', full_name=\'edu.stanford.nlp.pipeline.NERMention.gender\', index=8,\n      number=9, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'entityMentionIndex\', full_name=\'edu.stanford.nlp.pipeline.NERMention.entityMentionIndex\', index=9,\n      number=10, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'canonicalEntityMentionIndex\', full_name=\'edu.stanford.nlp.pipeline.NERMention.canonicalEntityMentionIndex\', index=10,\n      number=11, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'entityMentionText\', full_name=\'edu.stanford.nlp.pipeline.NERMention.entityMentionText\', index=11,\n      number=12, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=8298,\n  serialized_end=8647,\n)\n\n\n_SENTENCEFRAGMENT = _descriptor.Descriptor(\n  name=\'SentenceFragment\',\n  full_name=\'edu.stanford.nlp.pipeline.SentenceFragment\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'tokenIndex\', full_name=\'edu.stanford.nlp.pipeline.SentenceFragment.tokenIndex\', index=0,\n      number=1, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'root\', full_name=\'edu.stanford.nlp.pipeline.SentenceFragment.root\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'assumedTruth\', full_name=\'edu.stanford.nlp.pipeline.SentenceFragment.assumedTruth\', index=2,\n      number=3, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'score\', full_name=\'edu.stanford.nlp.pipeline.SentenceFragment.score\', index=3,\n      number=4, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=8649,\n  serialized_end=8738,\n)\n\n\n_TOKENLOCATION = _descriptor.Descriptor(\n  name=\'TokenLocation\',\n  full_name=\'edu.stanford.nlp.pipeline.TokenLocation\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'sentenceIndex\', full_name=\'edu.stanford.nlp.pipeline.TokenLocation.sentenceIndex\', index=0,\n      number=1, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tokenIndex\', full_name=\'edu.stanford.nlp.pipeline.TokenLocation.tokenIndex\', index=1,\n      number=2, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=8740,\n  serialized_end=8798,\n)\n\n\n_RELATIONTRIPLE = _descriptor.Descriptor(\n  name=\'RelationTriple\',\n  full_name=\'edu.stanford.nlp.pipeline.RelationTriple\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'subject\', full_name=\'edu.stanford.nlp.pipeline.RelationTriple.subject\', index=0,\n      number=1, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'relation\', full_name=\'edu.stanford.nlp.pipeline.RelationTriple.relation\', index=1,\n      number=2, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'object\', full_name=\'edu.stanford.nlp.pipeline.RelationTriple.object\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'confidence\', full_name=\'edu.stanford.nlp.pipeline.RelationTriple.confidence\', index=3,\n      number=4, type=1, cpp_type=5, label=1,\n      has_default_value=False, default_value=float(0),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'subjectTokens\', full_name=\'edu.stanford.nlp.pipeline.RelationTriple.subjectTokens\', index=4,\n      number=13, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'relationTokens\', full_name=\'edu.stanford.nlp.pipeline.RelationTriple.relationTokens\', index=5,\n      number=14, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'objectTokens\', full_name=\'edu.stanford.nlp.pipeline.RelationTriple.objectTokens\', index=6,\n      number=15, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'tree\', full_name=\'edu.stanford.nlp.pipeline.RelationTriple.tree\', index=7,\n      number=8, type=11, cpp_type=10, label=1,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'istmod\', full_name=\'edu.stanford.nlp.pipeline.RelationTriple.istmod\', index=8,\n      number=9, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'prefixBe\', full_name=\'edu.stanford.nlp.pipeline.RelationTriple.prefixBe\', index=9,\n      number=10, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'suffixBe\', full_name=\'edu.stanford.nlp.pipeline.RelationTriple.suffixBe\', index=10,\n      number=11, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'suffixOf\', full_name=\'edu.stanford.nlp.pipeline.RelationTriple.suffixOf\', index=11,\n      number=12, type=8, cpp_type=7, label=1,\n      has_default_value=False, default_value=False,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=8801,\n  serialized_end=9211,\n)\n\n\n_MAPSTRINGSTRING = _descriptor.Descriptor(\n  name=\'MapStringString\',\n  full_name=\'edu.stanford.nlp.pipeline.MapStringString\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'edu.stanford.nlp.pipeline.MapStringString.key\', index=0,\n      number=1, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'edu.stanford.nlp.pipeline.MapStringString.value\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=9213,\n  serialized_end=9258,\n)\n\n\n_MAPINTSTRING = _descriptor.Descriptor(\n  name=\'MapIntString\',\n  full_name=\'edu.stanford.nlp.pipeline.MapIntString\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'key\', full_name=\'edu.stanford.nlp.pipeline.MapIntString.key\', index=0,\n      number=1, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'value\', full_name=\'edu.stanford.nlp.pipeline.MapIntString.value\', index=1,\n      number=2, type=9, cpp_type=9, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=9260,\n  serialized_end=9302,\n)\n\n\n_SECTION = _descriptor.Descriptor(\n  name=\'Section\',\n  full_name=\'edu.stanford.nlp.pipeline.Section\',\n  filename=None,\n  file=DESCRIPTOR,\n  containing_type=None,\n  fields=[\n    _descriptor.FieldDescriptor(\n      name=\'charBegin\', full_name=\'edu.stanford.nlp.pipeline.Section.charBegin\', index=0,\n      number=1, type=13, cpp_type=3, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'charEnd\', full_name=\'edu.stanford.nlp.pipeline.Section.charEnd\', index=1,\n      number=2, type=13, cpp_type=3, label=2,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'author\', full_name=\'edu.stanford.nlp.pipeline.Section.author\', index=2,\n      number=3, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'sentenceIndexes\', full_name=\'edu.stanford.nlp.pipeline.Section.sentenceIndexes\', index=3,\n      number=4, type=13, cpp_type=3, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'datetime\', full_name=\'edu.stanford.nlp.pipeline.Section.datetime\', index=4,\n      number=5, type=9, cpp_type=9, label=1,\n      has_default_value=False, default_value=b"""".decode(\'utf-8\'),\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'quotes\', full_name=\'edu.stanford.nlp.pipeline.Section.quotes\', index=5,\n      number=6, type=11, cpp_type=10, label=3,\n      has_default_value=False, default_value=[],\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'authorCharBegin\', full_name=\'edu.stanford.nlp.pipeline.Section.authorCharBegin\', index=6,\n      number=7, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'authorCharEnd\', full_name=\'edu.stanford.nlp.pipeline.Section.authorCharEnd\', index=7,\n      number=8, type=13, cpp_type=3, label=1,\n      has_default_value=False, default_value=0,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n    _descriptor.FieldDescriptor(\n      name=\'xmlTag\', full_name=\'edu.stanford.nlp.pipeline.Section.xmlTag\', index=8,\n      number=9, type=11, cpp_type=10, label=2,\n      has_default_value=False, default_value=None,\n      message_type=None, enum_type=None, containing_type=None,\n      is_extension=False, extension_scope=None,\n      serialized_options=None, file=DESCRIPTOR),\n  ],\n  extensions=[\n  ],\n  nested_types=[],\n  enum_types=[\n  ],\n  serialized_options=None,\n  is_extendable=False,\n  syntax=\'proto2\',\n  extension_ranges=[],\n  oneofs=[\n  ],\n  serialized_start=9305,\n  serialized_end=9557,\n)\n\n_DOCUMENT.fields_by_name[\'sentence\'].message_type = _SENTENCE\n_DOCUMENT.fields_by_name[\'corefChain\'].message_type = _COREFCHAIN\n_DOCUMENT.fields_by_name[\'sentencelessToken\'].message_type = _TOKEN\n_DOCUMENT.fields_by_name[\'character\'].message_type = _TOKEN\n_DOCUMENT.fields_by_name[\'quote\'].message_type = _QUOTE\n_DOCUMENT.fields_by_name[\'mentions\'].message_type = _NERMENTION\n_DOCUMENT.fields_by_name[\'sections\'].message_type = _SECTION\n_DOCUMENT.fields_by_name[\'mentionsForCoref\'].message_type = _MENTION\n_SENTENCE.fields_by_name[\'token\'].message_type = _TOKEN\n_SENTENCE.fields_by_name[\'parseTree\'].message_type = _PARSETREE\n_SENTENCE.fields_by_name[\'binarizedParseTree\'].message_type = _PARSETREE\n_SENTENCE.fields_by_name[\'annotatedParseTree\'].message_type = _PARSETREE\n_SENTENCE.fields_by_name[\'kBestParseTrees\'].message_type = _PARSETREE\n_SENTENCE.fields_by_name[\'basicDependencies\'].message_type = _DEPENDENCYGRAPH\n_SENTENCE.fields_by_name[\'collapsedDependencies\'].message_type = _DEPENDENCYGRAPH\n_SENTENCE.fields_by_name[\'collapsedCCProcessedDependencies\'].message_type = _DEPENDENCYGRAPH\n_SENTENCE.fields_by_name[\'alternativeDependencies\'].message_type = _DEPENDENCYGRAPH\n_SENTENCE.fields_by_name[\'openieTriple\'].message_type = _RELATIONTRIPLE\n_SENTENCE.fields_by_name[\'kbpTriple\'].message_type = _RELATIONTRIPLE\n_SENTENCE.fields_by_name[\'entailedSentence\'].message_type = _SENTENCEFRAGMENT\n_SENTENCE.fields_by_name[\'entailedClause\'].message_type = _SENTENCEFRAGMENT\n_SENTENCE.fields_by_name[\'enhancedDependencies\'].message_type = _DEPENDENCYGRAPH\n_SENTENCE.fields_by_name[\'enhancedPlusPlusDependencies\'].message_type = _DEPENDENCYGRAPH\n_SENTENCE.fields_by_name[\'character\'].message_type = _TOKEN\n_SENTENCE.fields_by_name[\'entity\'].message_type = _ENTITY\n_SENTENCE.fields_by_name[\'relation\'].message_type = _RELATION\n_SENTENCE.fields_by_name[\'mentions\'].message_type = _NERMENTION\n_SENTENCE.fields_by_name[\'mentionsForCoref\'].message_type = _MENTION\n_TOKEN.fields_by_name[\'timexValue\'].message_type = _TIMEX\n_TOKEN.fields_by_name[\'operator\'].message_type = _OPERATOR\n_TOKEN.fields_by_name[\'polarity\'].message_type = _POLARITY\n_TOKEN.fields_by_name[\'span\'].message_type = _SPAN\n_TOKEN.fields_by_name[\'conllUFeatures\'].message_type = _MAPSTRINGSTRING\n_TOKEN.fields_by_name[\'conllUTokenSpan\'].message_type = _SPAN\n_TOKEN.fields_by_name[\'conllUSecondaryDeps\'].message_type = _MAPSTRINGSTRING\n_QUOTE.fields_by_name[\'attributionDependencyGraph\'].message_type = _DEPENDENCYGRAPH\n_PARSETREE.fields_by_name[\'child\'].message_type = _PARSETREE\n_PARSETREE.fields_by_name[\'sentiment\'].enum_type = _SENTIMENT\n_DEPENDENCYGRAPH_NODE.containing_type = _DEPENDENCYGRAPH\n_DEPENDENCYGRAPH_EDGE.fields_by_name[\'language\'].enum_type = _LANGUAGE\n_DEPENDENCYGRAPH_EDGE.containing_type = _DEPENDENCYGRAPH\n_DEPENDENCYGRAPH.fields_by_name[\'node\'].message_type = _DEPENDENCYGRAPH_NODE\n_DEPENDENCYGRAPH.fields_by_name[\'edge\'].message_type = _DEPENDENCYGRAPH_EDGE\n_COREFCHAIN_COREFMENTION.containing_type = _COREFCHAIN\n_COREFCHAIN.fields_by_name[\'mention\'].message_type = _COREFCHAIN_COREFMENTION\n_MENTION.fields_by_name[\'headIndexedWord\'].message_type = _INDEXEDWORD\n_MENTION.fields_by_name[\'dependingVerb\'].message_type = _INDEXEDWORD\n_MENTION.fields_by_name[\'headWord\'].message_type = _INDEXEDWORD\n_MENTION.fields_by_name[\'speakerInfo\'].message_type = _SPEAKERINFO\n_MENTION.fields_by_name[\'sentenceWords\'].message_type = _INDEXEDWORD\n_MENTION.fields_by_name[\'originalSpan\'].message_type = _INDEXEDWORD\n_RELATION.fields_by_name[\'arg\'].message_type = _ENTITY\n_POLARITY.fields_by_name[\'projectEquivalence\'].enum_type = _NATURALLOGICRELATION\n_POLARITY.fields_by_name[\'projectForwardEntailment\'].enum_type = _NATURALLOGICRELATION\n_POLARITY.fields_by_name[\'projectReverseEntailment\'].enum_type = _NATURALLOGICRELATION\n_POLARITY.fields_by_name[\'projectNegation\'].enum_type = _NATURALLOGICRELATION\n_POLARITY.fields_by_name[\'projectAlternation\'].enum_type = _NATURALLOGICRELATION\n_POLARITY.fields_by_name[\'projectCover\'].enum_type = _NATURALLOGICRELATION\n_POLARITY.fields_by_name[\'projectIndependence\'].enum_type = _NATURALLOGICRELATION\n_NERMENTION.fields_by_name[\'timex\'].message_type = _TIMEX\n_RELATIONTRIPLE.fields_by_name[\'subjectTokens\'].message_type = _TOKENLOCATION\n_RELATIONTRIPLE.fields_by_name[\'relationTokens\'].message_type = _TOKENLOCATION\n_RELATIONTRIPLE.fields_by_name[\'objectTokens\'].message_type = _TOKENLOCATION\n_RELATIONTRIPLE.fields_by_name[\'tree\'].message_type = _DEPENDENCYGRAPH\n_SECTION.fields_by_name[\'quotes\'].message_type = _QUOTE\n_SECTION.fields_by_name[\'xmlTag\'].message_type = _TOKEN\nDESCRIPTOR.message_types_by_name[\'Document\'] = _DOCUMENT\nDESCRIPTOR.message_types_by_name[\'Sentence\'] = _SENTENCE\nDESCRIPTOR.message_types_by_name[\'Token\'] = _TOKEN\nDESCRIPTOR.message_types_by_name[\'Quote\'] = _QUOTE\nDESCRIPTOR.message_types_by_name[\'ParseTree\'] = _PARSETREE\nDESCRIPTOR.message_types_by_name[\'DependencyGraph\'] = _DEPENDENCYGRAPH\nDESCRIPTOR.message_types_by_name[\'CorefChain\'] = _COREFCHAIN\nDESCRIPTOR.message_types_by_name[\'Mention\'] = _MENTION\nDESCRIPTOR.message_types_by_name[\'IndexedWord\'] = _INDEXEDWORD\nDESCRIPTOR.message_types_by_name[\'SpeakerInfo\'] = _SPEAKERINFO\nDESCRIPTOR.message_types_by_name[\'Span\'] = _SPAN\nDESCRIPTOR.message_types_by_name[\'Timex\'] = _TIMEX\nDESCRIPTOR.message_types_by_name[\'Entity\'] = _ENTITY\nDESCRIPTOR.message_types_by_name[\'Relation\'] = _RELATION\nDESCRIPTOR.message_types_by_name[\'Operator\'] = _OPERATOR\nDESCRIPTOR.message_types_by_name[\'Polarity\'] = _POLARITY\nDESCRIPTOR.message_types_by_name[\'NERMention\'] = _NERMENTION\nDESCRIPTOR.message_types_by_name[\'SentenceFragment\'] = _SENTENCEFRAGMENT\nDESCRIPTOR.message_types_by_name[\'TokenLocation\'] = _TOKENLOCATION\nDESCRIPTOR.message_types_by_name[\'RelationTriple\'] = _RELATIONTRIPLE\nDESCRIPTOR.message_types_by_name[\'MapStringString\'] = _MAPSTRINGSTRING\nDESCRIPTOR.message_types_by_name[\'MapIntString\'] = _MAPINTSTRING\nDESCRIPTOR.message_types_by_name[\'Section\'] = _SECTION\nDESCRIPTOR.enum_types_by_name[\'Language\'] = _LANGUAGE\nDESCRIPTOR.enum_types_by_name[\'Sentiment\'] = _SENTIMENT\nDESCRIPTOR.enum_types_by_name[\'NaturalLogicRelation\'] = _NATURALLOGICRELATION\n_sym_db.RegisterFileDescriptor(DESCRIPTOR)\n\nDocument = _reflection.GeneratedProtocolMessageType(\'Document\', (_message.Message,), {\n  \'DESCRIPTOR\' : _DOCUMENT,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Document)\n  })\n_sym_db.RegisterMessage(Document)\n\nSentence = _reflection.GeneratedProtocolMessageType(\'Sentence\', (_message.Message,), {\n  \'DESCRIPTOR\' : _SENTENCE,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Sentence)\n  })\n_sym_db.RegisterMessage(Sentence)\n\nToken = _reflection.GeneratedProtocolMessageType(\'Token\', (_message.Message,), {\n  \'DESCRIPTOR\' : _TOKEN,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Token)\n  })\n_sym_db.RegisterMessage(Token)\n\nQuote = _reflection.GeneratedProtocolMessageType(\'Quote\', (_message.Message,), {\n  \'DESCRIPTOR\' : _QUOTE,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Quote)\n  })\n_sym_db.RegisterMessage(Quote)\n\nParseTree = _reflection.GeneratedProtocolMessageType(\'ParseTree\', (_message.Message,), {\n  \'DESCRIPTOR\' : _PARSETREE,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.ParseTree)\n  })\n_sym_db.RegisterMessage(ParseTree)\n\nDependencyGraph = _reflection.GeneratedProtocolMessageType(\'DependencyGraph\', (_message.Message,), {\n\n  \'Node\' : _reflection.GeneratedProtocolMessageType(\'Node\', (_message.Message,), {\n    \'DESCRIPTOR\' : _DEPENDENCYGRAPH_NODE,\n    \'__module__\' : \'CoreNLP_pb2\'\n    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.DependencyGraph.Node)\n    })\n  ,\n\n  \'Edge\' : _reflection.GeneratedProtocolMessageType(\'Edge\', (_message.Message,), {\n    \'DESCRIPTOR\' : _DEPENDENCYGRAPH_EDGE,\n    \'__module__\' : \'CoreNLP_pb2\'\n    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.DependencyGraph.Edge)\n    })\n  ,\n  \'DESCRIPTOR\' : _DEPENDENCYGRAPH,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.DependencyGraph)\n  })\n_sym_db.RegisterMessage(DependencyGraph)\n_sym_db.RegisterMessage(DependencyGraph.Node)\n_sym_db.RegisterMessage(DependencyGraph.Edge)\n\nCorefChain = _reflection.GeneratedProtocolMessageType(\'CorefChain\', (_message.Message,), {\n\n  \'CorefMention\' : _reflection.GeneratedProtocolMessageType(\'CorefMention\', (_message.Message,), {\n    \'DESCRIPTOR\' : _COREFCHAIN_COREFMENTION,\n    \'__module__\' : \'CoreNLP_pb2\'\n    # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.CorefChain.CorefMention)\n    })\n  ,\n  \'DESCRIPTOR\' : _COREFCHAIN,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.CorefChain)\n  })\n_sym_db.RegisterMessage(CorefChain)\n_sym_db.RegisterMessage(CorefChain.CorefMention)\n\nMention = _reflection.GeneratedProtocolMessageType(\'Mention\', (_message.Message,), {\n  \'DESCRIPTOR\' : _MENTION,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Mention)\n  })\n_sym_db.RegisterMessage(Mention)\n\nIndexedWord = _reflection.GeneratedProtocolMessageType(\'IndexedWord\', (_message.Message,), {\n  \'DESCRIPTOR\' : _INDEXEDWORD,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.IndexedWord)\n  })\n_sym_db.RegisterMessage(IndexedWord)\n\nSpeakerInfo = _reflection.GeneratedProtocolMessageType(\'SpeakerInfo\', (_message.Message,), {\n  \'DESCRIPTOR\' : _SPEAKERINFO,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SpeakerInfo)\n  })\n_sym_db.RegisterMessage(SpeakerInfo)\n\nSpan = _reflection.GeneratedProtocolMessageType(\'Span\', (_message.Message,), {\n  \'DESCRIPTOR\' : _SPAN,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Span)\n  })\n_sym_db.RegisterMessage(Span)\n\nTimex = _reflection.GeneratedProtocolMessageType(\'Timex\', (_message.Message,), {\n  \'DESCRIPTOR\' : _TIMEX,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Timex)\n  })\n_sym_db.RegisterMessage(Timex)\n\nEntity = _reflection.GeneratedProtocolMessageType(\'Entity\', (_message.Message,), {\n  \'DESCRIPTOR\' : _ENTITY,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Entity)\n  })\n_sym_db.RegisterMessage(Entity)\n\nRelation = _reflection.GeneratedProtocolMessageType(\'Relation\', (_message.Message,), {\n  \'DESCRIPTOR\' : _RELATION,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Relation)\n  })\n_sym_db.RegisterMessage(Relation)\n\nOperator = _reflection.GeneratedProtocolMessageType(\'Operator\', (_message.Message,), {\n  \'DESCRIPTOR\' : _OPERATOR,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Operator)\n  })\n_sym_db.RegisterMessage(Operator)\n\nPolarity = _reflection.GeneratedProtocolMessageType(\'Polarity\', (_message.Message,), {\n  \'DESCRIPTOR\' : _POLARITY,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Polarity)\n  })\n_sym_db.RegisterMessage(Polarity)\n\nNERMention = _reflection.GeneratedProtocolMessageType(\'NERMention\', (_message.Message,), {\n  \'DESCRIPTOR\' : _NERMENTION,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.NERMention)\n  })\n_sym_db.RegisterMessage(NERMention)\n\nSentenceFragment = _reflection.GeneratedProtocolMessageType(\'SentenceFragment\', (_message.Message,), {\n  \'DESCRIPTOR\' : _SENTENCEFRAGMENT,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.SentenceFragment)\n  })\n_sym_db.RegisterMessage(SentenceFragment)\n\nTokenLocation = _reflection.GeneratedProtocolMessageType(\'TokenLocation\', (_message.Message,), {\n  \'DESCRIPTOR\' : _TOKENLOCATION,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.TokenLocation)\n  })\n_sym_db.RegisterMessage(TokenLocation)\n\nRelationTriple = _reflection.GeneratedProtocolMessageType(\'RelationTriple\', (_message.Message,), {\n  \'DESCRIPTOR\' : _RELATIONTRIPLE,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.RelationTriple)\n  })\n_sym_db.RegisterMessage(RelationTriple)\n\nMapStringString = _reflection.GeneratedProtocolMessageType(\'MapStringString\', (_message.Message,), {\n  \'DESCRIPTOR\' : _MAPSTRINGSTRING,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.MapStringString)\n  })\n_sym_db.RegisterMessage(MapStringString)\n\nMapIntString = _reflection.GeneratedProtocolMessageType(\'MapIntString\', (_message.Message,), {\n  \'DESCRIPTOR\' : _MAPINTSTRING,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.MapIntString)\n  })\n_sym_db.RegisterMessage(MapIntString)\n\nSection = _reflection.GeneratedProtocolMessageType(\'Section\', (_message.Message,), {\n  \'DESCRIPTOR\' : _SECTION,\n  \'__module__\' : \'CoreNLP_pb2\'\n  # @@protoc_insertion_point(class_scope:edu.stanford.nlp.pipeline.Section)\n  })\n_sym_db.RegisterMessage(Section)\n\n\nDESCRIPTOR._options = None\n_DEPENDENCYGRAPH.fields_by_name[\'root\']._options = None\n# @@protoc_insertion_point(module_scope)\n'"
stanza/protobuf/__init__.py,0,"b'from __future__ import absolute_import\n\nfrom io import BytesIO\nimport warnings\n\nfrom google.protobuf.internal.encoder import _EncodeVarint\nfrom google.protobuf.internal.decoder import _DecodeVarint\nfrom google.protobuf.message import DecodeError\nfrom .CoreNLP_pb2 import *\n\ndef parseFromDelimitedString(obj, buf, offset=0):\n    """"""\n    Stanford CoreNLP uses the Java ""writeDelimitedTo"" function, which\n    writes the size (and offset) of the buffer before writing the object.\n    This function handles parsing this message starting from offset 0.\n\n    @returns how many bytes of @buf were consumed.\n    """"""\n    size, pos = _DecodeVarint(buf, offset)\n    try:\n        obj.ParseFromString(buf[offset+pos:offset+pos+size])\n    except DecodeError as e:\n        warnings.warn(""Failed to decode a serialized output from CoreNLP server. An incomplete or empty object will be returned."", \\\n            RuntimeWarning)\n    return pos+size\n\ndef writeToDelimitedString(obj, stream=None):\n    """"""\n    Stanford CoreNLP uses the Java ""writeDelimitedTo"" function, which\n    writes the size (and offset) of the buffer before writing the object.\n    This function handles parsing this message starting from offset 0.\n\n    @returns how many bytes of @buf were consumed.\n    """"""\n    if stream is None:\n        stream = BytesIO()\n\n    _EncodeVarint(stream.write, obj.ByteSize(), True)\n    stream.write(obj.SerializeToString())\n    return stream\n\ndef to_text(sentence):\n    """"""\n    Helper routine that converts a Sentence protobuf to a string from\n    its tokens.\n    """"""\n    text = """"\n    for i, tok in enumerate(sentence.token):\n        if i != 0:\n            text += tok.before\n        text += tok.word\n    return text\n'"
stanza/server/__init__.py,0,"b'from stanza.protobuf import to_text\nfrom stanza.protobuf import Document, Sentence, Token, IndexedWord, Span\nfrom stanza.protobuf import ParseTree, DependencyGraph, CorefChain\nfrom stanza.protobuf import Mention, NERMention, Entity, Relation, RelationTriple, Timex\nfrom stanza.protobuf import Quote, SpeakerInfo\nfrom stanza.protobuf import Operator, Polarity\nfrom stanza.protobuf import SentenceFragment, TokenLocation\nfrom stanza.protobuf import MapStringString, MapIntString\nfrom .client import CoreNLPClient, AnnotationException, TimeoutException, PermanentlyFailedException\nfrom .annotator import Annotator\n'"
stanza/server/annotator.py,0,"b'""""""\nDefines a base class that can be used to annotate.\n""""""\nimport io\nfrom multiprocessing import Process\nfrom six.moves.BaseHTTPServer import BaseHTTPRequestHandler, HTTPServer\nfrom six.moves import http_client as HTTPStatus\n\nfrom stanza.protobuf import Document, parseFromDelimitedString, writeToDelimitedString\n\nclass Annotator(Process):\n    """"""\n    This annotator base class hosts a lightweight server that accepts\n    annotation requests from CoreNLP.\n    Each annotator simply defines 3 functions: requires, provides and annotate.\n\n    This class takes care of defining appropriate endpoints to interface\n    with CoreNLP.\n    """"""\n    @property\n    def name(self):\n        """"""\n        Name of the annotator (used by CoreNLP)\n        """"""\n        raise NotImplementedError()\n\n    @property\n    def requires(self):\n        """"""\n        Requires has to specify all the annotations required before we\n        are called.\n        """"""\n        raise NotImplementedError()\n\n    @property\n    def provides(self):\n        """"""\n        The set of annotations guaranteed to be provided when we are done.\n        NOTE: that these annotations are either fully qualified Java\n        class names or refer to nested classes of\n        edu.stanford.nlp.ling.CoreAnnotations (as is the case below).\n        """"""\n        raise NotImplementedError()\n\n    def annotate(self, ann):\n        """"""\n        @ann: is a protobuf annotation object.\n        Actually populate @ann with tokens.\n        """"""\n        raise NotImplementedError()\n\n    @property\n    def properties(self):\n        """"""\n        Defines a Java property to define this anntoator to CoreNLP.\n        """"""\n        return {\n            ""customAnnotatorClass.{}"".format(self.name): ""edu.stanford.nlp.pipeline.GenericWebServiceAnnotator"",\n            ""generic.endpoint"": ""http://{}:{}"".format(self.host, self.port),\n            ""generic.requires"": "","".join(self.requires),\n            ""generic.provides"": "","".join(self.provides),\n            }\n\n    class _Handler(BaseHTTPRequestHandler):\n        annotator = None\n\n        def __init__(self, request, client_address, server):\n            BaseHTTPRequestHandler.__init__(self, request, client_address, server)\n\n        def do_GET(self):\n            """"""\n            Handle a ping request\n            """"""\n            if not self.path.endswith(""/""): self.path += ""/""\n            if self.path == ""/ping/"":\n                msg = ""pong"".encode(""UTF-8"")\n\n                self.send_response(HTTPStatus.OK)\n                self.send_header(""Content-Type"", ""text/application"")\n                self.send_header(""Content-Length"", len(msg))\n                self.end_headers()\n                self.wfile.write(msg)\n            else:\n                self.send_response(HTTPStatus.BAD_REQUEST)\n                self.end_headers()\n\n        def do_POST(self):\n            """"""\n            Handle an annotate request\n            """"""\n            if not self.path.endswith(""/""): self.path += ""/""\n            if self.path == ""/annotate/"":\n                # Read message\n                length = int(self.headers.get(\'content-length\'))\n                msg = self.rfile.read(length)\n\n                # Do the annotation\n                doc = Document()\n                parseFromDelimitedString(doc, msg)\n                self.annotator.annotate(doc)\n\n                with io.BytesIO() as stream:\n                    writeToDelimitedString(doc, stream)\n                    msg = stream.getvalue()\n\n                # write message\n                self.send_response(HTTPStatus.OK)\n                self.send_header(""Content-Type"", ""application/x-protobuf"")\n                self.send_header(""Content-Length"", len(msg))\n                self.end_headers()\n                self.wfile.write(msg)\n\n            else:\n                self.send_response(HTTPStatus.BAD_REQUEST)\n                self.end_headers()\n\n    def __init__(self, host="""", port=8432):\n        """"""\n        Launches a server endpoint to communicate with CoreNLP\n        """"""\n        Process.__init__(self)\n        self.host, self.port = host, port\n        self._Handler.annotator = self\n\n    def run(self):\n        """"""\n        Runs the server using Python\'s simple HTTPServer.\n        TODO: make this multithreaded.\n        """"""\n        httpd = HTTPServer((self.host, self.port), self._Handler)\n        sa = httpd.socket.getsockname()\n        serve_message = ""Serving HTTP on {host} port {port} (http://{host}:{port}/) ...""\n        print(serve_message.format(host=sa[0], port=sa[1]))\n        try:\n            httpd.serve_forever()\n        except KeyboardInterrupt:\n            print(""\\nKeyboard interrupt received, exiting."")\n            httpd.shutdown()\n'"
stanza/server/client.py,0,"b'""""""\nClient for accessing Stanford CoreNLP in Python\n""""""\n\nimport atexit\nimport contextlib\nimport io\nimport os\nimport re\nimport requests\nimport logging\nimport json\nimport shlex\nimport socket\nimport subprocess\nimport time\nimport sys\nimport uuid\n\nfrom six.moves.urllib.parse import urlparse\n\nfrom stanza.protobuf import Document, parseFromDelimitedString, writeToDelimitedString, to_text\n__author__ = \'arunchaganty, kelvinguu, vzhong, wmonroe4\'\n\nlogger = logging.getLogger(\'stanza\')\n\n# pattern tmp props file should follow\nSERVER_PROPS_TMP_FILE_PATTERN = re.compile(\'corenlp_server-(.*).props\')\n\n# info for Stanford CoreNLP supported languages\nLANGUAGE_SHORTHANDS_TO_FULL = {\n    ""ar"": ""arabic"",\n    ""zh"": ""chinese"",\n    ""en"": ""english"",\n    ""fr"": ""french"",\n    ""de"": ""german"",\n    ""es"": ""spanish""\n}\n\nLANGUAGE_DEFAULT_ANNOTATORS = {\n    ""arabic"": ""tokenize,ssplit,pos,parse"",\n    ""chinese"": ""tokenize,ssplit,pos,lemma,ner,parse,coref"",\n    ""english"": ""tokenize,ssplit,pos,lemma,ner,depparse"",\n    ""french"": ""tokenize,ssplit,pos,depparse"",\n    ""german"": ""tokenize,ssplit,pos,ner,parse"",\n    ""spanish"": ""tokenize,ssplit,pos,lemma,ner,depparse,kbp""\n}\n\nENGLISH_DEFAULT_REQUEST_PROPERTIES = {\n    ""annotators"": ""tokenize,ssplit,pos,lemma,ner,depparse"",\n    ""tokenize.language"": ""en"",\n    ""pos.model"": ""edu/stanford/nlp/models/pos-tagger/english-left3words-distsim.tagger"",\n    ""ner.model"": ""edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz,""\n                 ""edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz,""\n                 ""edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz"",\n    ""sutime.language"": ""english"",\n    ""sutime.rules"": ""edu/stanford/nlp/models/sutime/defs.sutime.txt,""\n                    ""edu/stanford/nlp/models/sutime/english.sutime.txt,""\n                    ""edu/stanford/nlp/models/sutime/english.holidays.sutime.txt"",\n    ""ner.applyNumericClassifiers"": ""true"",\n    ""ner.useSUTime"": ""true"",\n\n    ""ner.fine.regexner.mapping"": ""ignorecase=true,validpospattern=^(NN|JJ).*,""\n                                 ""edu/stanford/nlp/models/kbp/english/gazetteers/regexner_caseless.tab;"",\n                                 ""edu/stanford/nlp/models/kbp/english/gazetteers/regexner_cased.tab""\n    ""ner.fine.regexner.noDefaultOverwriteLabels"": ""CITY"",\n    ""ner.language"": ""en"",\n    ""depparse.model"": ""edu/stanford/nlp/models/parser/nndep/english_UD.gz""\n}\n\n\nclass AnnotationException(Exception):\n    """""" Exception raised when there was an error communicating with the CoreNLP server. """"""\n    pass\n\n\nclass TimeoutException(AnnotationException):\n    """""" Exception raised when the CoreNLP server timed out. """"""\n    pass\n\n\nclass ShouldRetryException(Exception):\n    """""" Exception raised if the service should retry the request. """"""\n    pass\n\n\nclass PermanentlyFailedException(Exception):\n    """""" Exception raised if the service should NOT retry the request. """"""\n    pass\n\n\ndef clean_props_file(props_file):\n    # check if there is a temp server props file to remove and remove it\n    if props_file:\n        if (os.path.isfile(props_file) and\n            SERVER_PROPS_TMP_FILE_PATTERN.match(os.path.basename(props_file))):\n            os.remove(props_file)\n\nclass RobustService(object):\n    """""" Service that resuscitates itself if it is not available. """"""\n    CHECK_ALIVE_TIMEOUT = 120\n\n    def __init__(self, start_cmd, stop_cmd, endpoint, stdout=sys.stdout,\n                 stderr=sys.stderr, be_quiet=False, host=None, port=None):\n        self.start_cmd = start_cmd and shlex.split(start_cmd)\n        self.stop_cmd = stop_cmd and shlex.split(stop_cmd)\n        self.endpoint = endpoint\n        self.stdout = stdout\n        self.stderr = stderr\n\n        self.server = None\n        self.is_active = False\n        self.be_quiet = be_quiet\n        self.host = host\n        self.port = port\n        atexit.register(self.atexit_kill)\n\n    def is_alive(self):\n        try:\n            if self.server is not None and self.server.poll() is not None:\n                return False\n            return requests.get(self.endpoint + ""/ping"").ok\n        except requests.exceptions.ConnectionError as e:\n            raise ShouldRetryException(e)\n\n    def start(self):\n        if self.start_cmd:\n            if self.host and self.port:\n                with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as sock:\n                    try:\n                        sock.bind((self.host, self.port))\n                    except socket.error:\n                        raise PermanentlyFailedException(""Error: unable to start the CoreNLP server on port %d (possibly something is already running there)"" % self.port)\n            if self.be_quiet:\n                # Issue #26: subprocess.DEVNULL isn\'t supported in python 2.7.\n                stderr = open(os.devnull, \'w\')\n            else:\n                stderr = self.stderr\n            print(f""Starting server with command: {\' \'.join(self.start_cmd)}"")\n            self.server = subprocess.Popen(self.start_cmd,\n                                           stderr=stderr,\n                                           stdout=stderr)\n\n    def atexit_kill(self):\n        # make some kind of effort to stop the service (such as a\n        # CoreNLP server) at the end of the program.  not waiting so\n        # that the python script exiting isn\'t delayed\n        if self.server and self.server.poll() is None:\n            self.server.terminate()\n\n    def stop(self):\n        if self.server:\n            self.server.terminate()\n            try:\n                self.server.wait(5)\n            except subprocess.TimeoutExpired:\n                # Resorting to more aggressive measures...\n                self.server.kill()\n                try:\n                    self.server.wait(5)\n                except subprocess.TimeoutExpired:\n                    # oh well\n                    pass\n            self.server = None\n        if self.stop_cmd:\n            subprocess.run(self.stop_cmd, check=True)\n        self.is_active = False\n\n    def __enter__(self):\n        self.start()\n        return self\n\n    def __exit__(self, _, __, ___):\n        self.stop()\n\n    def ensure_alive(self):\n        # Check if the service is active and alive\n        if self.is_active:\n            try:\n                if self.is_alive():\n                    return\n                else:\n                    self.stop()\n            except ShouldRetryException:\n                pass\n\n        # If not, try to start up the service.\n        if self.server is None:\n            self.start()\n\n        # Wait for the service to start up.\n        start_time = time.time()\n        while True:\n            try:\n                if self.is_alive():\n                    break\n            except ShouldRetryException:\n                pass\n\n            if time.time() - start_time < self.CHECK_ALIVE_TIMEOUT:\n                time.sleep(1)\n            else:\n                raise PermanentlyFailedException(""Timed out waiting for service to come alive."")\n\n        # At this point we are guaranteed that the service is alive.\n        self.is_active = True\n\n\nclass CoreNLPClient(RobustService):\n    """""" A CoreNLP client to the Stanford CoreNLP server. """"""\n\n    DEFAULT_ENDPOINT = ""http://localhost:9000""\n    DEFAULT_TIMEOUT = 60000\n    DEFAULT_THREADS = 5\n    DEFAULT_ANNOTATORS = ""tokenize,ssplit,pos,lemma,ner,depparse""\n    DEFAULT_OUTPUT_FORMAT = ""serialized""\n    DEFAULT_MEMORY = ""5G""\n    DEFAULT_MAX_CHAR_LENGTH = 100000\n    DEFAULT_SERIALIZER = ""edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer""\n    DEFAULT_INPUT_FORMAT = ""text""\n    PIPELINE_LANGUAGES = \\\n        [\'ar\', \'arabic\', \'chinese\', \'zh\', \'english\', \'en\', \'french\', \'fr\', \'de\', \'german\', \'es\', \'spanish\']\n\n    def __init__(self, start_server=True,\n                 endpoint=DEFAULT_ENDPOINT,\n                 timeout=DEFAULT_TIMEOUT,\n                 threads=DEFAULT_THREADS,\n                 annotators=None,\n                 properties=None,\n                 output_format=None,\n                 stdout=sys.stdout,\n                 stderr=sys.stderr,\n                 memory=DEFAULT_MEMORY,\n                 be_quiet=True,\n                 max_char_length=DEFAULT_MAX_CHAR_LENGTH,\n                 preload=True,\n                 classpath=None,\n                 **kwargs):\n\n        # properties cache maps keys to properties dictionaries for convenience\n        self.properties_cache = {}\n        self.server_props_file = {\'is_temp\': False, \'path\': None}\n        # start the server\n        if start_server:\n            # set up default properties for server\n            self._setup_default_server_props(properties, annotators, output_format)\n            # at this point self.server_start_info and self.server_props_file should be set\n            host, port = urlparse(endpoint).netloc.split("":"")\n            port = int(port)\n            assert host == ""localhost"", ""If starting a server, endpoint must be localhost""\n            if classpath == \'$CLASSPATH\':\n                classpath = os.getenv(""CLASSPATH"")\n            elif classpath is None:\n                classpath = os.getenv(""CORENLP_HOME"")\n                assert classpath is not None, \\\n                    ""Please define $CORENLP_HOME to be location of your CoreNLP distribution or pass in a classpath parameter""\n                classpath = classpath + ""/*""\n            start_cmd = f""java -Xmx{memory} -cp \'{classpath}\'  edu.stanford.nlp.pipeline.StanfordCoreNLPServer "" \\\n                        f""-port {port} -timeout {timeout} -threads {threads} -maxCharLength {max_char_length} "" \\\n                        f""-quiet {be_quiet} -serverProperties {self.server_props_file[\'path\']}""\n            if preload and self.server_start_info.get(\'preload_annotators\'):\n                start_cmd += f"" -preload {self.server_start_info[\'preload_annotators\']}""\n            # additional options for server:\n            # - server_id\n            # - ssl\n            # - status_port\n            # - uriContext\n            # - strict\n            # - key\n            # - username\n            # - password\n            # - blacklist\n            for kw in [\'ssl\', \'strict\']:\n                if kwargs.get(kw) is not None:\n                    start_cmd += f"" -{kw}""\n            for kw in [\'status_port\', \'uriContext\', \'key\', \'username\', \'password\', \'blacklist\', \'server_id\']:\n                if kwargs.get(kw) is not None:\n                    start_cmd += f"" -{kw} {kwargs.get(kw)}""\n            stop_cmd = None\n        else:\n            start_cmd = stop_cmd = None\n            host = port = None\n            self.server_start_info = {}\n\n        super(CoreNLPClient, self).__init__(start_cmd, stop_cmd, endpoint,\n                                            stdout, stderr, be_quiet, host=host, port=port)\n\n        self.timeout = timeout\n\n    def _setup_default_server_props(self, properties, annotators, output_format):\n        """"""\n        Set up the default properties for the server from either:\n\n        1. File path on system or in CLASSPATH (e.g. /path/to/server.props or StanfordCoreNLP-french.properties\n        2. Stanford CoreNLP supported language (e.g. french)\n        3. Python dictionary (properties written to tmp file for Java server, erased at end)\n        4. Default (just use standard defaults set server side in Java code, with the exception that the default\n                    default outputFormat is changed to serialized)\n\n        If defaults are being set client side, values of annotators and output_format will overwrite the\n        client side properties.  If the defaults are being set server side, those parameters will be ignored.\n\n        Info about the properties used to start the server is stored in self.server_start_info\n        If a file is used, info about the file (path, whether tmp or not) is stored in self.server_props_file\n        """"""\n        # store information about server start up\n        self.server_start_info = {}\n        # ensure properties is str or dict\n        if properties is None or (not isinstance(properties, str) and not isinstance(properties, dict)):\n            if properties is not None:\n                print(\'Warning: properties passed invalid value (not a str or dict), setting properties = {}\')\n            properties = {}\n        # check if properties is a string\n        if isinstance(properties, str):\n            # translate Stanford CoreNLP language name to properties file if properties is a language name\n            if properties.lower() in CoreNLPClient.PIPELINE_LANGUAGES:\n                lang_name = properties.lower()\n                if lang_name in LANGUAGE_SHORTHANDS_TO_FULL:\n                    lang_name = LANGUAGE_SHORTHANDS_TO_FULL[lang_name]\n                if lang_name in [\'en\', \'english\']:\n                    self.server_props_file[\'path\'] = f\'StanfordCoreNLP.properties\'\n                else:\n                    self.server_props_file[\'path\'] = f\'StanfordCoreNLP-{lang_name}.properties\'\n                self.server_start_info[\'preload_annotators\'] = LANGUAGE_DEFAULT_ANNOTATORS[lang_name]\n                print(f""Using Stanford CoreNLP default properties for: {lang_name}.  Make sure to have {lang_name} ""\n                      f""models jar (available for download here: https://stanfordnlp.github.io/CoreNLP/) in CLASSPATH"")\n            # otherwise assume properties string is a path\n            else:\n                self.server_props_file[\'path\'] = properties\n                if os.path.isfile(properties):\n                    props_from_file = read_corenlp_props(properties)\n                    self.server_start_info[\'props\'] = props_from_file\n                    self.server_start_info[\'preload_annotators\'] = props_from_file.get(\'annotators\')\n                else:\n                    print(f""Warning: {properties} does not correspond to a file path."")\n            print(f""Setting server defaults from: {self.server_props_file[\'path\']}"")\n            self.server_start_info[\'props_file\'] = self.server_props_file[\'path\']\n            self.server_start_info[\'server_side\'] = True\n            if annotators is not None:\n                print(f""Warning: Server defaults being set server side, ignoring annotators={annotators}"")\n            if output_format is not None:\n                print(f""Warning: Server defaults being set server side, ignoring output_format={output_format}"")\n        # check if client side should set default properties\n        else:\n            # set up properties from client side\n            # the Java Stanford CoreNLP server defaults to ""json"" for outputFormat\n            # but by default servers started by Python interface will override this to return serialized object\n            client_side_properties = {\n                \'annotators\': CoreNLPClient.DEFAULT_ANNOTATORS,\n                \'outputFormat\': CoreNLPClient.DEFAULT_OUTPUT_FORMAT,\n                \'serializer\': CoreNLPClient.DEFAULT_SERIALIZER\n            }\n            client_side_properties.update(properties)\n            # override if a specific annotators list was specified\n            if annotators:\n                client_side_properties[\'annotators\'] = \\\n                    "","".join(annotators) if isinstance(annotators, list) else annotators\n            # override if a specific output format was specified\n            if output_format is not None and isinstance(output_format, str):\n                client_side_properties[\'outputFormat\'] = output_format\n            # write client side props to a tmp file which will be erased at end\n            self.server_props_file[\'path\'] = write_corenlp_props(client_side_properties)\n            atexit.register(clean_props_file, self.server_props_file[\'path\'])\n            self.server_props_file[\'is_temp\'] = True\n            # record server start up info\n            self.server_start_info[\'client_side\'] = True\n            self.server_start_info[\'props\'] = client_side_properties\n            self.server_start_info[\'props_file\'] = self.server_props_file[\'path\']\n            self.server_start_info[\'preload_annotators\'] = client_side_properties[\'annotators\']\n\n    def _request(self, buf, properties, **kwargs):\n        """"""\n        Send a request to the CoreNLP server.\n\n        :param (str | bytes) buf: data to be sent with the request\n        :param (dict) properties: properties that the server expects\n        :return: request result\n        """"""\n        self.ensure_alive()\n\n        try:\n            input_format = properties.get(""inputFormat"", ""text"")\n            if input_format == ""text"":\n                ctype = ""text/plain; charset=utf-8""\n            elif input_format == ""serialized"":\n                ctype = ""application/x-protobuf""\n            else:\n                raise ValueError(""Unrecognized inputFormat "" + input_format)\n            # handle auth\n            if \'username\' in kwargs and \'password\' in kwargs:\n                kwargs[\'auth\'] = requests.auth.HTTPBasicAuth(kwargs[\'username\'], kwargs[\'password\'])\n                kwargs.pop(\'username\')\n                kwargs.pop(\'password\')\n            r = requests.post(self.endpoint,\n                              params={\'properties\': str(properties)},\n                              data=buf, headers={\'content-type\': ctype},\n                              timeout=(self.timeout*2)/1000, **kwargs)\n            r.raise_for_status()\n            return r\n        except requests.HTTPError as e:\n            if r.text == ""CoreNLP request timed out. Your document may be too long."":\n                raise TimeoutException(r.text)\n            else:\n                raise AnnotationException(r.text)\n\n    def register_properties_key(self, key, props):\n        """""" Register a properties dictionary with a key in the client\'s properties_cache """"""\n        if key in CoreNLPClient.PIPELINE_LANGUAGES:\n            print(f\'Key {key} not registered in properties cache.  Names of Stanford CoreNLP supported languages are \'\n                  f\'reserved for Stanford CoreNLP defaults for that language.  For instance ""french"" or ""fr"" \'\n                  f\'corresponds to using the defaults in StanfordCoreNLP-french.properties which are stored with the \'\n                  f\'server.  If you want to store custom defaults for that language, it is suggested to use a key like \'\n                  f\' ""fr-custom"", etc...\'\n                  )\n        else:\n            self.properties_cache[key] = props\n\n    def annotate(self, text, annotators=None, output_format=None, properties_key=None, properties=None, **kwargs):\n        """"""\n        Send a request to the CoreNLP server.\n\n        :param (str | unicode) text: raw text for the CoreNLPServer to parse\n        :param (list | string) annotators: list of annotators to use\n        :param (str) output_format: output type from server: serialized, json, text, conll, conllu, or xml\n        :param (str) properties_key: key into properties cache for the client\n        :param (dict) properties: additional request properties (written on top of defaults)\n\n        The properties for a request are written in this order:\n\n        1. Server default properties (server side)\n        2. Properties from client\'s properties_cache corresponding to properties_key (client side)\n           If the properties_key is the name of a Stanford CoreNLP supported language:\n           [Arabic, Chinese, English, French, German, Spanish], the Stanford CoreNLP defaults will be used (server side)\n        3. Additional properties corresponding to properties (client side)\n        4. Special case specific properties: annotators, output_format (client side)\n\n        :return: request result\n        """"""\n        # set properties for server call\n        # first look for a cached default properties set\n        # if a Stanford CoreNLP supported language is specified, just pass {pipelineLanguage=""french""}\n        if properties_key is not None:\n            if properties_key.lower() in [\'en\', \'english\']:\n                request_properties = dict(ENGLISH_DEFAULT_REQUEST_PROPERTIES)\n            elif properties_key.lower() in CoreNLPClient.PIPELINE_LANGUAGES:\n                request_properties = {\'pipelineLanguage\': properties_key.lower()}\n            elif properties_key not in self.properties_cache:\n                raise ValueError(""Properties cache does not have \'%s\'"" % properties_key)\n            else:\n                request_properties = dict(self.properties_cache[properties_key])\n        else:\n            request_properties = {}\n        # add on custom properties for this request\n        if properties is None:\n            properties = {}\n        request_properties.update(properties)\n        # if annotators list is specified, override with that\n        if annotators is not None:\n            request_properties[\'annotators\'] = "","".join(annotators) if isinstance(annotators, list) else annotators\n        # always send an output format with request\n        # in some scenario\'s the server\'s default output format is unknown, so default to serialized\n        if output_format is not None:\n            request_properties[\'outputFormat\'] = output_format\n        if request_properties.get(\'outputFormat\') is None:\n            if self.server_start_info.get(\'props\', {}).get(\'outputFormat\'):\n                request_properties[\'outputFormat\'] = self.server_start_info[\'props\'][\'outputFormat\']\n            else:\n                request_properties[\'outputFormat\'] = CoreNLPClient.DEFAULT_OUTPUT_FORMAT\n        # make the request\n        r = self._request(text.encode(\'utf-8\'), request_properties, **kwargs)\n        if request_properties[""outputFormat""] == ""json"":\n            return r.json()\n        elif request_properties[""outputFormat""] == ""serialized"":\n            doc = Document()\n            parseFromDelimitedString(doc, r.content)\n            return doc\n        elif request_properties[""outputFormat""] in [""text"", ""conllu"", ""conll"", ""xml""]:\n            return r.text\n        else:\n            return r\n\n    def update(self, doc, annotators=None, properties=None):\n        if properties is None:\n            properties = {}\n            properties.update({\n                \'inputFormat\': \'serialized\',\n                \'outputFormat\': \'serialized\',\n                \'serializer\': \'edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer\'\n            })\n        if annotators:\n            properties[\'annotators\'] = "","".join(annotators) if isinstance(annotators, list) else annotators\n        with io.BytesIO() as stream:\n            writeToDelimitedString(doc, stream)\n            msg = stream.getvalue()\n\n        r = self._request(msg, properties)\n        doc = Document()\n        parseFromDelimitedString(doc, r.content)\n        return doc\n\n    def tokensregex(self, text, pattern, filter=False, to_words=False, annotators=None, properties=None):\n        # this is required for some reason\n        matches = self.__regex(\'/tokensregex\', text, pattern, filter, annotators, properties)\n        if to_words:\n            matches = regex_matches_to_indexed_words(matches)\n        return matches\n\n    def semgrex(self, text, pattern, filter=False, to_words=False, annotators=None, properties=None):\n        matches = self.__regex(\'/semgrex\', text, pattern, filter, annotators, properties)\n        if to_words:\n            matches = regex_matches_to_indexed_words(matches)\n        return matches\n\n    def tregex(self, text, pattern, filter=False, annotators=None, properties=None):\n        return self.__regex(\'/tregex\', text, pattern, filter, annotators, properties)\n\n    def __regex(self, path, text, pattern, filter, annotators=None, properties=None):\n        """"""\n        Send a regex-related request to the CoreNLP server.\n        :param (str | unicode) path: the path for the regex endpoint\n        :param text: raw text for the CoreNLPServer to apply the regex\n        :param (str | unicode) pattern: regex pattern\n        :param (bool) filter: option to filter sentences that contain matches, if false returns matches\n        :param properties: option to filter sentences that contain matches, if false returns matches\n        :return: request result\n        """"""\n        self.ensure_alive()\n        if properties is None:\n            properties = {}\n            properties.update({\n                \'inputFormat\': \'text\',\n                \'serializer\': \'edu.stanford.nlp.pipeline.ProtobufAnnotationSerializer\'\n            })\n        if annotators:\n            properties[\'annotators\'] = "","".join(annotators) if isinstance(annotators, list) else annotators\n\n        # force output for regex requests to be json\n        properties[\'outputFormat\'] = \'json\'\n\n        # TODO: get rid of this once corenlp 4.0.0 is released?\n        # the ""stupid reason"" has hopefully been fixed on the corenlp side\n        # but maybe people are married to corenlp 3.9.2 for some reason\n        # HACK: For some stupid reason, CoreNLPServer will timeout if we\n        # need to annotate something from scratch. So, we need to call\n        # this to ensure that the _regex call doesn\'t timeout.\n        self.annotate(text, properties=properties)\n\n        try:\n            # Error occurs unless put properties in params\n            input_format = properties.get(""inputFormat"", ""text"")\n            if input_format == ""text"":\n                ctype = ""text/plain; charset=utf-8""\n            elif input_format == ""serialized"":\n                ctype = ""application/x-protobuf""\n            else:\n                raise ValueError(""Unrecognized inputFormat "" + input_format)\n            # change request method from `get` to `post` as required by CoreNLP\n            r = requests.post(\n                self.endpoint + path, params={\n                    \'pattern\': pattern,\n                    \'filter\': filter,\n                    \'properties\': str(properties)\n                },\n                data=text.encode(\'utf-8\'),\n                headers={\'content-type\': ctype},\n                timeout=(self.timeout*2)/1000,\n            )\n            r.raise_for_status()\n            return json.loads(r.text)\n        except requests.HTTPError as e:\n            if r.text.startswith(""Timeout""):\n                raise TimeoutException(r.text)\n            else:\n                raise AnnotationException(r.text)\n        except json.JSONDecodeError:\n            raise AnnotationException(r.text)\n\n\ndef read_corenlp_props(props_path):\n    """""" Read a Stanford CoreNLP properties file into a dict """"""\n    props_dict = {}\n    if os.path.exists(props_path):\n        with open(props_path) as props_file:\n            entry_lines = \\\n                [entry_line for entry_line in props_file.read().split(\'\\n\')\n                 if entry_line.strip() and not entry_line.startswith(\'#\')]\n            for entry_line in entry_lines:\n                k = entry_line.split(\'=\')[0]\n                k_len = len(k+""="")\n                v = entry_line[k_len:]\n                props_dict[k.strip()] = v\n        return props_dict\n\n    else:\n        raise RuntimeError(f\'Error: Properties file at {props_path} does not exist!\')\n\n\ndef write_corenlp_props(props_dict, file_path=None):\n    """""" Write a Stanford CoreNLP properties dict to a file """"""\n    if file_path is None:\n        file_path = f""corenlp_server-{uuid.uuid4().hex[:16]}.props""\n        # confirm tmp file path matches pattern\n        assert SERVER_PROPS_TMP_FILE_PATTERN.match(file_path)\n    with open(file_path, \'w\') as props_file:\n        for k, v in props_dict.items():\n            if isinstance(v, list):\n                writeable_v = "","".join(v)\n            else:\n                writeable_v = v\n            props_file.write(f\'{k} = {writeable_v}\\n\\n\')\n    return file_path\n\n\ndef regex_matches_to_indexed_words(matches):\n    """"""\n    Transforms tokensregex and semgrex matches to indexed words.\n    :param matches: unprocessed regex matches\n    :return: flat array of indexed words\n    """"""\n    words = [dict(v, **dict([(\'sentence\', i)]))\n             for i, s in enumerate(matches[\'sentences\'])\n             for k, v in s.items() if k != \'length\']\n    return words\n\n\n__all__ = [""CoreNLPClient"", ""AnnotationException"", ""TimeoutException"", ""to_text""]\n'"
stanza/server/main.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n""""""\nSimple shell program to pipe in \n""""""\n\nimport corenlp\n\nimport json\nimport re\nimport csv\nimport sys\nfrom collections import namedtuple, OrderedDict\n\nFLOAT_RE = re.compile(r""\\d*\\.\\d+"")\nINT_RE = re.compile(r""\\d+"")\n\ndef dictstr(arg):\n    """"""\n    Parse a key=value string as a tuple (key, value) that can be provided as an argument to dict()\n    """"""\n    key, value = arg.split(""="")\n\n    if value.lower() == ""true"" or value.lower() == ""false"":\n        value = bool(value)\n    elif INT_RE.match(value):\n        value = int(value)\n    elif FLOAT_RE.match(value):\n        value = float(value)\n    return (key, value)\n\n\ndef do_annotate(args):\n    args.props = dict(args.props) if args.props else {}\n    if args.sentence_mode:\n        args.props[""ssplit.isOneSentence""] = True\n\n    with corenlp.CoreNLPClient(annotators=args.annotators, properties=args.props, be_quiet=not args.verbose_server) as client:\n        for line in args.input:\n            if line.startswith(""#""): continue\n\n            ann = client.annotate(line.strip(), output_format=args.format)\n\n            if args.format == ""json"":\n                if args.sentence_mode:\n                    ann = ann[""sentences""][0]\n\n                args.output.write(json.dumps(ann))\n                args.output.write(""\\n"")\n\ndef main():\n    import argparse\n    parser = argparse.ArgumentParser(description=\'Annotate data\')\n    parser.add_argument(\'-i\', \'--input\', type=argparse.FileType(\'r\'), default=sys.stdin, help=""Input file to process; each line contains one document (default: stdin)"")\n    parser.add_argument(\'-o\', \'--output\', type=argparse.FileType(\'w\'), default=sys.stdout, help=""File to write annotations to (default: stdout)"")\n    parser.add_argument(\'-f\', \'--format\', choices=[""json"",], default=""json"", help=""Output format"")\n    parser.add_argument(\'-a\', \'--annotators\', nargs=""+"", type=str, default=[""tokenize ssplit lemma pos""], help=""A list of annotators"")\n    parser.add_argument(\'-s\', \'--sentence-mode\', action=""store_true"",help=""Assume each line of input is a sentence."")\n    parser.add_argument(\'-v\', \'--verbose-server\', action=""store_true"",help=""Server is made verbose"")\n    parser.add_argument(\'-m\', \'--memory\', type=str, default=""4G"", help=""Memory to use for the server"")\n    parser.add_argument(\'-p\', \'--props\', nargs=""+"", type=dictstr, help=""Properties as a list of key=value pairs"")\n    parser.set_defaults(func=do_annotate)\n\n    ARGS = parser.parse_args()\n    if ARGS.func is None:\n        parser.print_help()\n        sys.exit(1)\n    else:\n        ARGS.func(ARGS)\n\nif __name__ == ""__main__"":\n    main()\n'"
stanza/utils/__init__.py,0,b''
stanza/utils/avg_sent_len.py,0,"b""import sys\nimport json\n\ntoklabels = sys.argv[1]\n\nif toklabels.endswith('.json'):\n    with open(toklabels, 'r') as f:\n        l = json.load(f)\n\n    l = [''.join([str(x[1]) for x in para]) for para in l]\nelse:\n    with open(toklabels, 'r') as f:\n        l = ''.join(f.readlines())\n\n    l = l.split('\\n\\n')\n\nsentlen = [len(x) + 1 for para in l for x in para.split('2')]\nprint(sum(sentlen) / len(sentlen))\n"""
stanza/utils/conll.py,0,"b'""""""\nUtility functions for the loading and conversion of CoNLL-format files.\n""""""\nimport os\nimport io\n\nFIELD_NUM = 10\n\nID = \'id\'\nTEXT = \'text\'\nLEMMA = \'lemma\'\nUPOS = \'upos\'\nXPOS = \'xpos\'\nFEATS = \'feats\'\nHEAD = \'head\'\nDEPREL = \'deprel\'\nDEPS = \'deps\'\nMISC = \'misc\'\nFIELD_TO_IDX = {ID: 0, TEXT: 1, LEMMA: 2, UPOS: 3, XPOS: 4, FEATS: 5, HEAD: 6, DEPREL: 7, DEPS: 8, MISC: 9}\n\nclass CoNLL:\n\n    @staticmethod\n    def load_conll(f, ignore_gapping=True):\n        """""" Load the file or string into the CoNLL-U format data.\n        Input: file or string reader, where the data is in CoNLL-U format.\n        Output: a list of list of list for each token in each sentence in the data, where the innermost list represents \n        all fields of a token.\n        """"""\n        # f is open() or io.StringIO()\n        doc, sent = [], []\n        for line in f:\n            line = line.strip()\n            if len(line) == 0:\n                if len(sent) > 0:\n                    doc.append(sent)\n                    sent = []\n            else:\n                if line.startswith(\'#\'): # skip comment line\n                    continue\n                array = line.split(\'\\t\')\n                if ignore_gapping and \'.\' in array[0]:\n                    continue\n                assert len(array) == FIELD_NUM, \\\n                        f""Cannot parse CoNLL line: expecting {FIELD_NUM} fields, {len(array)} found.""\n                sent += [array]\n        if len(sent) > 0:\n            doc.append(sent)\n        return doc\n\n    @staticmethod\n    def convert_conll(doc_conll):\n        """""" Convert the CoNLL-U format input data to a dictionary format output data.\n        Input: list of token fields loaded from the CoNLL-U format data, where the outmost list represents a list of sentences, and the inside list represents all fields of a token.\n        Output: a list of list of dictionaries for each token in each sentence in the document.\n        """"""\n        doc_dict = []\n        for sent_conll in doc_conll:\n            sent_dict = []\n            for token_conll in sent_conll:\n                token_dict = CoNLL.convert_conll_token(token_conll)\n                sent_dict.append(token_dict)\n            doc_dict.append(sent_dict)\n        return doc_dict\n    \n    @staticmethod\n    def convert_conll_token(token_conll):\n        """""" Convert the CoNLL-U format input token to the dictionary format output token.\n        Input: a list of all CoNLL-U fields for the token.\n        Output: a dictionary that maps from field name to value.\n        """"""\n        token_dict = {}\n        for field in FIELD_TO_IDX:\n            value = token_conll[FIELD_TO_IDX[field]]\n            if value != \'_\':\n                if field == HEAD:\n                    token_dict[field] = int(value)\n                else:\n                    token_dict[field] = value\n            # special case if text is \'_\'\n            if token_conll[FIELD_TO_IDX[TEXT]] == \'_\':\n                token_dict[TEXT] = token_conll[FIELD_TO_IDX[TEXT]]\n                token_dict[LEMMA] = token_conll[FIELD_TO_IDX[LEMMA]]\n        return token_dict\n        \n    @staticmethod\n    def conll2dict(input_file=None, input_str=None, ignore_gapping=True):\n        """""" Load the CoNLL-U format data from file or string into lists of dictionaries.\n        """"""\n        assert any([input_file, input_str]) and not all([input_file, input_str]), \'either input input file or input string\'\n        if input_str:\n            infile = io.StringIO(input_str)\n        else:\n            infile = open(input_file)\n        doc_conll = CoNLL.load_conll(infile, ignore_gapping)\n        doc_dict = CoNLL.convert_conll(doc_conll)\n        return doc_dict\n\n    @staticmethod\n    def convert_dict(doc_dict):\n        """""" Convert the dictionary format input data to the CoNLL-U format output data. This is the reverse function of \n        `convert_conll`.\n        Input: dictionary format data, which is a list of list of dictionaries for each token in each sentence in the data.\n        Output: CoNLL-U format data, which is a list of list of list for each token in each sentence in the data.\n        """"""\n        doc_conll = []\n        for sent_dict in doc_dict:\n            sent_conll = []\n            for token_dict in sent_dict:\n                token_conll = CoNLL.convert_token_dict(token_dict)\n                sent_conll.append(token_conll)\n            doc_conll.append(sent_conll)\n        return doc_conll\n            \n    @staticmethod\n    def convert_token_dict(token_dict):\n        """""" Convert the dictionary format input token to the CoNLL-U format output token. This is the reverse function of \n        `convert_conll_token`.\n        Input: dictionary format token, which is a dictionaries for the token.\n        Output: CoNLL-U format token, which is a list for the token.\n        """"""\n        token_conll = [\'_\' for i in range(FIELD_NUM)]\n        for key in token_dict:\n            if key in FIELD_TO_IDX:\n                token_conll[FIELD_TO_IDX[key]] = str(token_dict[key])\n        # when a word (not mwt token) without head is found, we insert dummy head as required by the UD eval script\n        if \'-\' not in token_dict[ID] and HEAD not in token_dict:\n            token_conll[FIELD_TO_IDX[HEAD]] = str(int(token_dict[ID]) - 1) # evaluation script requires head: int\n        return token_conll\n\n    @staticmethod\n    def conll_as_string(doc):\n        """""" Dump the loaded CoNLL-U format list data to string. """"""\n        return_string = """"\n        for sent in doc:\n            for ln in sent:\n                return_string += (""\\t"".join(ln)+""\\n"")\n            return_string += ""\\n""\n        return return_string\n    \n    @staticmethod\n    def dict2conll(doc_dict, filename):\n        """""" Convert the dictionary format input data to the CoNLL-U format output data and write to a file.\n        """"""\n        doc_conll = CoNLL.convert_dict(doc_dict)\n        conll_string = CoNLL.conll_as_string(doc_conll)\n        with open(filename, \'w\') as outfile:\n            outfile.write(conll_string)\n        return\n'"
stanza/utils/conll18_ud_eval.py,0,"b'#!/usr/bin/env python3\n\n# Compatible with Python 2.7 and 3.2+, can be used either as a module\n# or a standalone executable.\n#\n# Copyright 2017, 2018 Institute of Formal and Applied Linguistics (UFAL),\n# Faculty of Mathematics and Physics, Charles University, Czech Republic.\n#\n# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n#\n# Authors: Milan Straka, Martin Popel <surname@ufal.mff.cuni.cz>\n#\n# Changelog:\n# - [12 Apr 2018] Version 0.9: Initial release.\n# - [19 Apr 2018] Version 1.0: Fix bug in MLAS (duplicate entries in functional_children).\n#                              Add --counts option.\n# - [02 May 2018] Version 1.1: When removing spaces to match gold and system characters,\n#                              consider all Unicode characters of category Zs instead of\n#                              just ASCII space.\n# - [25 Jun 2018] Version 1.2: Use python3 in the she-bang (instead of python).\n#                              In Python2, make the whole computation use `unicode` strings.\n\n# Command line usage\n# ------------------\n# conll18_ud_eval.py [-v] gold_conllu_file system_conllu_file\n#\n# - if no -v is given, only the official CoNLL18 UD Shared Task evaluation metrics\n#   are printed\n# - if -v is given, more metrics are printed (as precision, recall, F1 score,\n#   and in case the metric is computed on aligned words also accuracy on these):\n#   - Tokens: how well do the gold tokens match system tokens\n#   - Sentences: how well do the gold sentences match system sentences\n#   - Words: how well can the gold words be aligned to system words\n#   - UPOS: using aligned words, how well does UPOS match\n#   - XPOS: using aligned words, how well does XPOS match\n#   - UFeats: using aligned words, how well does universal FEATS match\n#   - AllTags: using aligned words, how well does UPOS+XPOS+FEATS match\n#   - Lemmas: using aligned words, how well does LEMMA match\n#   - UAS: using aligned words, how well does HEAD match\n#   - LAS: using aligned words, how well does HEAD+DEPREL(ignoring subtypes) match\n#   - CLAS: using aligned words with content DEPREL, how well does\n#       HEAD+DEPREL(ignoring subtypes) match\n#   - MLAS: using aligned words with content DEPREL, how well does\n#       HEAD+DEPREL(ignoring subtypes)+UPOS+UFEATS+FunctionalChildren(DEPREL+UPOS+UFEATS) match\n#   - BLEX: using aligned words with content DEPREL, how well does\n#       HEAD+DEPREL(ignoring subtypes)+LEMMAS match\n# - if -c is given, raw counts of correct/gold_total/system_total/aligned words are printed\n#   instead of precision/recall/F1/AlignedAccuracy for all metrics.\n\n# API usage\n# ---------\n# - load_conllu(file)\n#   - loads CoNLL-U file from given file object to an internal representation\n#   - the file object should return str in both Python 2 and Python 3\n#   - raises UDError exception if the given file cannot be loaded\n# - evaluate(gold_ud, system_ud)\n#   - evaluate the given gold and system CoNLL-U files (loaded with load_conllu)\n#   - raises UDError if the concatenated tokens of gold and system file do not match\n#   - returns a dictionary with the metrics described above, each metric having\n#     three fields: precision, recall and f1\n\n# Description of token matching\n# -----------------------------\n# In order to match tokens of gold file and system file, we consider the text\n# resulting from concatenation of gold tokens and text resulting from\n# concatenation of system tokens. These texts should match -- if they do not,\n# the evaluation fails.\n#\n# If the texts do match, every token is represented as a range in this original\n# text, and tokens are equal only if their range is the same.\n\n# Description of word matching\n# ----------------------------\n# When matching words of gold file and system file, we first match the tokens.\n# The words which are also tokens are matched as tokens, but words in multi-word\n# tokens have to be handled differently.\n#\n# To handle multi-word tokens, we start by finding ""multi-word spans"".\n# Multi-word span is a span in the original text such that\n# - it contains at least one multi-word token\n# - all multi-word tokens in the span (considering both gold and system ones)\n#   are completely inside the span (i.e., they do not ""stick out"")\n# - the multi-word span is as small as possible\n#\n# For every multi-word span, we align the gold and system words completely\n# inside this span using LCS on their FORMs. The words not intersecting\n# (even partially) any multi-word span are then aligned as tokens.\n\n\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport argparse\nimport io\nimport sys\nimport unicodedata\nimport unittest\n\n# CoNLL-U column names\nID, FORM, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC = range(10)\n\n# Content and functional relations\nCONTENT_DEPRELS = {\n    ""nsubj"", ""obj"", ""iobj"", ""csubj"", ""ccomp"", ""xcomp"", ""obl"", ""vocative"",\n    ""expl"", ""dislocated"", ""advcl"", ""advmod"", ""discourse"", ""nmod"", ""appos"",\n    ""nummod"", ""acl"", ""amod"", ""conj"", ""fixed"", ""flat"", ""compound"", ""list"",\n    ""parataxis"", ""orphan"", ""goeswith"", ""reparandum"", ""root"", ""dep""\n}\n\nFUNCTIONAL_DEPRELS = {\n    ""aux"", ""cop"", ""mark"", ""det"", ""clf"", ""case"", ""cc""\n}\n\nUNIVERSAL_FEATURES = {\n    ""PronType"", ""NumType"", ""Poss"", ""Reflex"", ""Foreign"", ""Abbr"", ""Gender"",\n    ""Animacy"", ""Number"", ""Case"", ""Definite"", ""Degree"", ""VerbForm"", ""Mood"",\n    ""Tense"", ""Aspect"", ""Voice"", ""Evident"", ""Polarity"", ""Person"", ""Polite""\n}\n\n# UD Error is used when raising exceptions in this module\nclass UDError(Exception):\n    pass\n\n# Conversion methods handling `str` <-> `unicode` conversions in Python2\ndef _decode(text):\n    return text if sys.version_info[0] >= 3 or not isinstance(text, str) else text.decode(""utf-8"")\n\ndef _encode(text):\n    return text if sys.version_info[0] >= 3 or not isinstance(text, unicode) else text.encode(""utf-8"")\n\n# Load given CoNLL-U file into internal representation\ndef load_conllu(file):\n    # Internal representation classes\n    class UDRepresentation:\n        def __init__(self):\n            # Characters of all the tokens in the whole file.\n            # Whitespace between tokens is not included.\n            self.characters = []\n            # List of UDSpan instances with start&end indices into `characters`.\n            self.tokens = []\n            # List of UDWord instances.\n            self.words = []\n            # List of UDSpan instances with start&end indices into `characters`.\n            self.sentences = []\n    class UDSpan:\n        def __init__(self, start, end):\n            self.start = start\n            # Note that self.end marks the first position **after the end** of span,\n            # so we can use characters[start:end] or range(start, end).\n            self.end = end\n    class UDWord:\n        def __init__(self, span, columns, is_multiword):\n            # Span of this word (or MWT, see below) within ud_representation.characters.\n            self.span = span\n            # 10 columns of the CoNLL-U file: ID, FORM, LEMMA,...\n            self.columns = columns\n            # is_multiword==True means that this word is part of a multi-word token.\n            # In that case, self.span marks the span of the whole multi-word token.\n            self.is_multiword = is_multiword\n            # Reference to the UDWord instance representing the HEAD (or None if root).\n            self.parent = None\n            # List of references to UDWord instances representing functional-deprel children.\n            self.functional_children = []\n            # Only consider universal FEATS.\n            self.columns[FEATS] = ""|"".join(sorted(feat for feat in columns[FEATS].split(""|"")\n                                                  if feat.split(""="", 1)[0] in UNIVERSAL_FEATURES))\n            # Let\'s ignore language-specific deprel subtypes.\n            self.columns[DEPREL] = columns[DEPREL].split("":"")[0]\n            # Precompute which deprels are CONTENT_DEPRELS and which FUNCTIONAL_DEPRELS\n            self.is_content_deprel = self.columns[DEPREL] in CONTENT_DEPRELS\n            self.is_functional_deprel = self.columns[DEPREL] in FUNCTIONAL_DEPRELS\n\n    ud = UDRepresentation()\n\n    # Load the CoNLL-U file\n    index, sentence_start = 0, None\n    while True:\n        line = file.readline()\n        if not line:\n            break\n        line = _decode(line.rstrip(""\\r\\n""))\n\n        # Handle sentence start boundaries\n        if sentence_start is None:\n            # Skip comments\n            if line.startswith(""#""):\n                continue\n            # Start a new sentence\n            ud.sentences.append(UDSpan(index, 0))\n            sentence_start = len(ud.words)\n        if not line:\n            # Add parent and children UDWord links and check there are no cycles\n            def process_word(word):\n                if word.parent == ""remapping"":\n                    raise UDError(""There is a cycle in a sentence"")\n                if word.parent is None:\n                    head = int(word.columns[HEAD])\n                    if head < 0 or head > len(ud.words) - sentence_start:\n                        raise UDError(""HEAD \'{}\' points outside of the sentence"".format(_encode(word.columns[HEAD])))\n                    if head:\n                        parent = ud.words[sentence_start + head - 1]\n                        word.parent = ""remapping""\n                        process_word(parent)\n                        word.parent = parent\n\n            for word in ud.words[sentence_start:]:\n                process_word(word)\n            # func_children cannot be assigned within process_word\n            # because it is called recursively and may result in adding one child twice.\n            for word in ud.words[sentence_start:]:\n                if word.parent and word.is_functional_deprel:\n                    word.parent.functional_children.append(word)\n\n            # Check there is a single root node\n            if len([word for word in ud.words[sentence_start:] if word.parent is None]) != 1:\n                raise UDError(""There are multiple roots in a sentence"")\n\n            # End the sentence\n            ud.sentences[-1].end = index\n            sentence_start = None\n            continue\n\n        # Read next token/word\n        columns = line.split(""\\t"")\n        if len(columns) != 10:\n            raise UDError(""The CoNLL-U line does not contain 10 tab-separated columns: \'{}\'"".format(_encode(line)))\n\n        # Skip empty nodes\n        if ""."" in columns[ID]:\n            continue\n\n        # Delete spaces from FORM, so gold.characters == system.characters\n        # even if one of them tokenizes the space. Use any Unicode character\n        # with category Zs.\n        columns[FORM] = """".join(filter(lambda c: unicodedata.category(c) != ""Zs"", columns[FORM]))\n        if not columns[FORM]:\n            raise UDError(""There is an empty FORM in the CoNLL-U file"")\n\n        # Save token\n        ud.characters.extend(columns[FORM])\n        ud.tokens.append(UDSpan(index, index + len(columns[FORM])))\n        index += len(columns[FORM])\n\n        # Handle multi-word tokens to save word(s)\n        if ""-"" in columns[ID]:\n            try:\n                start, end = map(int, columns[ID].split(""-""))\n            except:\n                raise UDError(""Cannot parse multi-word token ID \'{}\'"".format(_encode(columns[ID])))\n\n            for _ in range(start, end + 1):\n                word_line = _decode(file.readline().rstrip(""\\r\\n""))\n                word_columns = word_line.split(""\\t"")\n                if len(word_columns) != 10:\n                    raise UDError(""The CoNLL-U line does not contain 10 tab-separated columns: \'{}\'"".format(_encode(word_line)))\n                ud.words.append(UDWord(ud.tokens[-1], word_columns, is_multiword=True))\n        # Basic tokens/words\n        else:\n            try:\n                word_id = int(columns[ID])\n            except:\n                raise UDError(""Cannot parse word ID \'{}\'"".format(_encode(columns[ID])))\n            if word_id != len(ud.words) - sentence_start + 1:\n                raise UDError(""Incorrect word ID \'{}\' for word \'{}\', expected \'{}\'"".format(\n                    _encode(columns[ID]), _encode(columns[FORM]), len(ud.words) - sentence_start + 1))\n\n            try:\n                head_id = int(columns[HEAD])\n            except:\n                raise UDError(""Cannot parse HEAD \'{}\'"".format(_encode(columns[HEAD])))\n            if head_id < 0:\n                raise UDError(""HEAD cannot be negative"")\n\n            ud.words.append(UDWord(ud.tokens[-1], columns, is_multiword=False))\n\n    if sentence_start is not None:\n        raise UDError(""The CoNLL-U file does not end with empty line"")\n\n    return ud\n\n# Evaluate the gold and system treebanks (loaded using load_conllu).\ndef evaluate(gold_ud, system_ud):\n    class Score:\n        def __init__(self, gold_total, system_total, correct, aligned_total=None):\n            self.correct = correct\n            self.gold_total = gold_total\n            self.system_total = system_total\n            self.aligned_total = aligned_total\n            self.precision = correct / system_total if system_total else 0.0\n            self.recall = correct / gold_total if gold_total else 0.0\n            self.f1 = 2 * correct / (system_total + gold_total) if system_total + gold_total else 0.0\n            self.aligned_accuracy = correct / aligned_total if aligned_total else aligned_total\n    class AlignmentWord:\n        def __init__(self, gold_word, system_word):\n            self.gold_word = gold_word\n            self.system_word = system_word\n    class Alignment:\n        def __init__(self, gold_words, system_words):\n            self.gold_words = gold_words\n            self.system_words = system_words\n            self.matched_words = []\n            self.matched_words_map = {}\n        def append_aligned_words(self, gold_word, system_word):\n            self.matched_words.append(AlignmentWord(gold_word, system_word))\n            self.matched_words_map[system_word] = gold_word\n\n    def spans_score(gold_spans, system_spans):\n        correct, gi, si = 0, 0, 0\n        while gi < len(gold_spans) and si < len(system_spans):\n            if system_spans[si].start < gold_spans[gi].start:\n                si += 1\n            elif gold_spans[gi].start < system_spans[si].start:\n                gi += 1\n            else:\n                correct += gold_spans[gi].end == system_spans[si].end\n                si += 1\n                gi += 1\n\n        return Score(len(gold_spans), len(system_spans), correct)\n\n    def alignment_score(alignment, key_fn=None, filter_fn=None):\n        if filter_fn is not None:\n            gold = sum(1 for gold in alignment.gold_words if filter_fn(gold))\n            system = sum(1 for system in alignment.system_words if filter_fn(system))\n            aligned = sum(1 for word in alignment.matched_words if filter_fn(word.gold_word))\n        else:\n            gold = len(alignment.gold_words)\n            system = len(alignment.system_words)\n            aligned = len(alignment.matched_words)\n\n        if key_fn is None:\n            # Return score for whole aligned words\n            return Score(gold, system, aligned)\n\n        def gold_aligned_gold(word):\n            return word\n        def gold_aligned_system(word):\n            return alignment.matched_words_map.get(word, ""NotAligned"") if word is not None else None\n        correct = 0\n        for words in alignment.matched_words:\n            if filter_fn is None or filter_fn(words.gold_word):\n                if key_fn(words.gold_word, gold_aligned_gold) == key_fn(words.system_word, gold_aligned_system):\n                    correct += 1\n\n        return Score(gold, system, correct, aligned)\n\n    def beyond_end(words, i, multiword_span_end):\n        if i >= len(words):\n            return True\n        if words[i].is_multiword:\n            return words[i].span.start >= multiword_span_end\n        return words[i].span.end > multiword_span_end\n\n    def extend_end(word, multiword_span_end):\n        if word.is_multiword and word.span.end > multiword_span_end:\n            return word.span.end\n        return multiword_span_end\n\n    def find_multiword_span(gold_words, system_words, gi, si):\n        # We know gold_words[gi].is_multiword or system_words[si].is_multiword.\n        # Find the start of the multiword span (gs, ss), so the multiword span is minimal.\n        # Initialize multiword_span_end characters index.\n        if gold_words[gi].is_multiword:\n            multiword_span_end = gold_words[gi].span.end\n            if not system_words[si].is_multiword and system_words[si].span.start < gold_words[gi].span.start:\n                si += 1\n        else: # if system_words[si].is_multiword\n            multiword_span_end = system_words[si].span.end\n            if not gold_words[gi].is_multiword and gold_words[gi].span.start < system_words[si].span.start:\n                gi += 1\n        gs, ss = gi, si\n\n        # Find the end of the multiword span\n        # (so both gi and si are pointing to the word following the multiword span end).\n        while not beyond_end(gold_words, gi, multiword_span_end) or \\\n              not beyond_end(system_words, si, multiword_span_end):\n            if gi < len(gold_words) and (si >= len(system_words) or\n                                         gold_words[gi].span.start <= system_words[si].span.start):\n                multiword_span_end = extend_end(gold_words[gi], multiword_span_end)\n                gi += 1\n            else:\n                multiword_span_end = extend_end(system_words[si], multiword_span_end)\n                si += 1\n        return gs, ss, gi, si\n\n    def compute_lcs(gold_words, system_words, gi, si, gs, ss):\n        lcs = [[0] * (si - ss) for i in range(gi - gs)]\n        for g in reversed(range(gi - gs)):\n            for s in reversed(range(si - ss)):\n                if gold_words[gs + g].columns[FORM].lower() == system_words[ss + s].columns[FORM].lower():\n                    lcs[g][s] = 1 + (lcs[g+1][s+1] if g+1 < gi-gs and s+1 < si-ss else 0)\n                lcs[g][s] = max(lcs[g][s], lcs[g+1][s] if g+1 < gi-gs else 0)\n                lcs[g][s] = max(lcs[g][s], lcs[g][s+1] if s+1 < si-ss else 0)\n        return lcs\n\n    def align_words(gold_words, system_words):\n        alignment = Alignment(gold_words, system_words)\n\n        gi, si = 0, 0\n        while gi < len(gold_words) and si < len(system_words):\n            if gold_words[gi].is_multiword or system_words[si].is_multiword:\n                # A: Multi-word tokens => align via LCS within the whole ""multiword span"".\n                gs, ss, gi, si = find_multiword_span(gold_words, system_words, gi, si)\n\n                if si > ss and gi > gs:\n                    lcs = compute_lcs(gold_words, system_words, gi, si, gs, ss)\n\n                    # Store aligned words\n                    s, g = 0, 0\n                    while g < gi - gs and s < si - ss:\n                        if gold_words[gs + g].columns[FORM].lower() == system_words[ss + s].columns[FORM].lower():\n                            alignment.append_aligned_words(gold_words[gs+g], system_words[ss+s])\n                            g += 1\n                            s += 1\n                        elif lcs[g][s] == (lcs[g+1][s] if g+1 < gi-gs else 0):\n                            g += 1\n                        else:\n                            s += 1\n            else:\n                # B: No multi-word token => align according to spans.\n                if (gold_words[gi].span.start, gold_words[gi].span.end) == (system_words[si].span.start, system_words[si].span.end):\n                    alignment.append_aligned_words(gold_words[gi], system_words[si])\n                    gi += 1\n                    si += 1\n                elif gold_words[gi].span.start <= system_words[si].span.start:\n                    gi += 1\n                else:\n                    si += 1\n\n        return alignment\n\n    # Check that the underlying character sequences do match.\n    if gold_ud.characters != system_ud.characters:\n        index = 0\n        while index < len(gold_ud.characters) and index < len(system_ud.characters) and \\\n                gold_ud.characters[index] == system_ud.characters[index]:\n            index += 1\n\n        raise UDError(\n            ""The concatenation of tokens in gold file and in system file differ!\\n"" +\n            ""First 20 differing characters in gold file: \'{}\' and system file: \'{}\'"".format(\n                """".join(map(_encode, gold_ud.characters[index:index + 20])),\n                """".join(map(_encode, system_ud.characters[index:index + 20]))\n            )\n        )\n\n    # Align words\n    alignment = align_words(gold_ud.words, system_ud.words)\n\n    # Compute the F1-scores\n    return {\n        ""Tokens"": spans_score(gold_ud.tokens, system_ud.tokens),\n        ""Sentences"": spans_score(gold_ud.sentences, system_ud.sentences),\n        ""Words"": alignment_score(alignment),\n        ""UPOS"": alignment_score(alignment, lambda w, _: w.columns[UPOS]),\n        ""XPOS"": alignment_score(alignment, lambda w, _: w.columns[XPOS]),\n        ""UFeats"": alignment_score(alignment, lambda w, _: w.columns[FEATS]),\n        ""AllTags"": alignment_score(alignment, lambda w, _: (w.columns[UPOS], w.columns[XPOS], w.columns[FEATS])),\n        ""Lemmas"": alignment_score(alignment, lambda w, ga: w.columns[LEMMA] if ga(w).columns[LEMMA] != ""_"" else ""_""),\n        ""UAS"": alignment_score(alignment, lambda w, ga: ga(w.parent)),\n        ""LAS"": alignment_score(alignment, lambda w, ga: (ga(w.parent), w.columns[DEPREL])),\n        ""CLAS"": alignment_score(alignment, lambda w, ga: (ga(w.parent), w.columns[DEPREL]),\n                                filter_fn=lambda w: w.is_content_deprel),\n        ""MLAS"": alignment_score(alignment, lambda w, ga: (ga(w.parent), w.columns[DEPREL], w.columns[UPOS], w.columns[FEATS],\n                                                         [(ga(c), c.columns[DEPREL], c.columns[UPOS], c.columns[FEATS])\n                                                          for c in w.functional_children]),\n                                filter_fn=lambda w: w.is_content_deprel),\n        ""BLEX"": alignment_score(alignment, lambda w, ga: (ga(w.parent), w.columns[DEPREL],\n                                                          w.columns[LEMMA] if ga(w).columns[LEMMA] != ""_"" else ""_""),\n                                filter_fn=lambda w: w.is_content_deprel),\n    }\n\n\ndef load_conllu_file(path):\n    _file = open(path, mode=""r"", **({""encoding"": ""utf-8""} if sys.version_info >= (3, 0) else {}))\n    return load_conllu(_file)\n\ndef evaluate_wrapper(args):\n    # Load CoNLL-U files\n    gold_ud = load_conllu_file(args.gold_file)\n    system_ud = load_conllu_file(args.system_file)\n    return evaluate(gold_ud, system_ud)\n\ndef main():\n    # Parse arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""gold_file"", type=str,\n                        help=""Name of the CoNLL-U file with the gold data."")\n    parser.add_argument(""system_file"", type=str,\n                        help=""Name of the CoNLL-U file with the predicted data."")\n    parser.add_argument(""--verbose"", ""-v"", default=False, action=""store_true"",\n                        help=""Print all metrics."")\n    parser.add_argument(""--counts"", ""-c"", default=False, action=""store_true"",\n                        help=""Print raw counts of correct/gold/system/aligned words instead of prec/rec/F1 for all metrics."")\n    args = parser.parse_args()\n\n    # Evaluate\n    evaluation = evaluate_wrapper(args)\n\n    # Print the evaluation\n    if not args.verbose and not args.counts:\n        print(""LAS F1 Score: {:.2f}"".format(100 * evaluation[""LAS""].f1))\n        print(""MLAS Score: {:.2f}"".format(100 * evaluation[""MLAS""].f1))\n        print(""BLEX Score: {:.2f}"".format(100 * evaluation[""BLEX""].f1))\n    else:\n        if args.counts:\n            print(""Metric     | Correct   |      Gold | Predicted | Aligned"")\n        else:\n            print(""Metric     | Precision |    Recall |  F1 Score | AligndAcc"")\n        print(""-----------+-----------+-----------+-----------+-----------"")\n        for metric in[""Tokens"", ""Sentences"", ""Words"", ""UPOS"", ""XPOS"", ""UFeats"", ""AllTags"", ""Lemmas"", ""UAS"", ""LAS"", ""CLAS"", ""MLAS"", ""BLEX""]:\n            if args.counts:\n                print(""{:11}|{:10} |{:10} |{:10} |{:10}"".format(\n                    metric,\n                    evaluation[metric].correct,\n                    evaluation[metric].gold_total,\n                    evaluation[metric].system_total,\n                    evaluation[metric].aligned_total or (evaluation[metric].correct if metric == ""Words"" else """")\n                ))\n            else:\n                print(""{:11}|{:10.2f} |{:10.2f} |{:10.2f} |{}"".format(\n                    metric,\n                    100 * evaluation[metric].precision,\n                    100 * evaluation[metric].recall,\n                    100 * evaluation[metric].f1,\n                    ""{:10.2f}"".format(100 * evaluation[metric].aligned_accuracy) if evaluation[metric].aligned_accuracy is not None else """"\n                ))\n\nif __name__ == ""__main__"":\n    main()\n\n# Tests, which can be executed with `python -m unittest conll18_ud_eval`.\nclass TestAlignment(unittest.TestCase):\n    @staticmethod\n    def _load_words(words):\n        """"""Prepare fake CoNLL-U files with fake HEAD to prevent multiple roots errors.""""""\n        lines, num_words = [], 0\n        for w in words:\n            parts = w.split("" "")\n            if len(parts) == 1:\n                num_words += 1\n                lines.append(""{}\\t{}\\t_\\t_\\t_\\t_\\t{}\\t_\\t_\\t_"".format(num_words, parts[0], int(num_words>1)))\n            else:\n                lines.append(""{}-{}\\t{}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t_"".format(num_words + 1, num_words + len(parts) - 1, parts[0]))\n                for part in parts[1:]:\n                    num_words += 1\n                    lines.append(""{}\\t{}\\t_\\t_\\t_\\t_\\t{}\\t_\\t_\\t_"".format(num_words, part, int(num_words>1)))\n        return load_conllu((io.StringIO if sys.version_info >= (3, 0) else io.BytesIO)(""\\n"".join(lines+[""\\n""])))\n\n    def _test_exception(self, gold, system):\n        self.assertRaises(UDError, evaluate, self._load_words(gold), self._load_words(system))\n\n    def _test_ok(self, gold, system, correct):\n        metrics = evaluate(self._load_words(gold), self._load_words(system))\n        gold_words = sum((max(1, len(word.split("" "")) - 1) for word in gold))\n        system_words = sum((max(1, len(word.split("" "")) - 1) for word in system))\n        self.assertEqual((metrics[""Words""].precision, metrics[""Words""].recall, metrics[""Words""].f1),\n                         (correct / system_words, correct / gold_words, 2 * correct / (gold_words + system_words)))\n\n    def test_exception(self):\n        self._test_exception([""a""], [""b""])\n\n    def test_equal(self):\n        self._test_ok([""a""], [""a""], 1)\n        self._test_ok([""a"", ""b"", ""c""], [""a"", ""b"", ""c""], 3)\n\n    def test_equal_with_multiword(self):\n        self._test_ok([""abc a b c""], [""a"", ""b"", ""c""], 3)\n        self._test_ok([""a"", ""bc b c"", ""d""], [""a"", ""b"", ""c"", ""d""], 4)\n        self._test_ok([""abcd a b c d""], [""ab a b"", ""cd c d""], 4)\n        self._test_ok([""abc a b c"", ""de d e""], [""a"", ""bcd b c d"", ""e""], 5)\n\n    def test_alignment(self):\n        self._test_ok([""abcd""], [""a"", ""b"", ""c"", ""d""], 0)\n        self._test_ok([""abc"", ""d""], [""a"", ""b"", ""c"", ""d""], 1)\n        self._test_ok([""a"", ""bc"", ""d""], [""a"", ""b"", ""c"", ""d""], 2)\n        self._test_ok([""a"", ""bc b c"", ""d""], [""a"", ""b"", ""cd""], 2)\n        self._test_ok([""abc a BX c"", ""def d EX f""], [""ab a b"", ""cd c d"", ""ef e f""], 4)\n        self._test_ok([""ab a b"", ""cd bc d""], [""a"", ""bc"", ""d""], 2)\n        self._test_ok([""a"", ""bc b c"", ""d""], [""ab AX BX"", ""cd CX a""], 1)\n'"
stanza/utils/contract_mwt.py,0,"b'import sys\n\ndef contract_mwt(ignore_gapping=True):\n    with open(sys.argv[2], \'w\') as fout:\n        with open(sys.argv[1], \'r\') as fin:\n            idx = 0\n            mwt_begin = 0\n            mwt_end = -1\n            for line in fin:\n                line = line.strip()\n    \n                if line.startswith(\'#\'):\n                    print(line, file=fout)\n                    continue\n                elif len(line) <= 0:\n                    print(line, file=fout)\n                    idx = 0\n                    mwt_begin = 0\n                    mwt_end = -1\n                    continue\n    \n                line = line.split(\'\\t\')\n\n                # ignore gapping word\n                if ignore_gapping and \'.\' in line[0]:\n                    continue\n\n                idx += 1\n                if \'-\' in line[0]:\n                    mwt_begin, mwt_end = [int(x) for x in line[0].split(\'-\')]\n                    print(""{}\\t{}\\t{}"".format(idx, ""\\t"".join(line[1:-1]), ""MWT=Yes"" if line[-1] == \'_\' else line[-1] + ""|MWT=Yes""), file=fout)\n                    idx -= 1\n                elif mwt_begin <= idx <= mwt_end:\n                    continue\n                else:\n                    print(""{}\\t{}"".format(idx, ""\\t"".join(line[1:])), file=fout)\n\nif __name__ == \'__main__\':\n    contract_mwt()\n\n'"
stanza/utils/helper_func.py,0,"b""def make_table(header, content, column_width=None):\n    '''\n    Input:\n    header -> List[str]: table header\n    content -> List[List[str]]: table content\n    column_width -> int: table column width; set to None for dynamically calculated widths\n    \n    Output:\n    table_str -> str: well-formatted string for the table\n    '''\n    table_str = ''\n    len_column, len_row = len(header), len(content) + 1\n    if column_width is None:\n        # dynamically decide column widths\n        lens = [[len(str(h)) for h in header]]\n        lens += [[len(str(x)) for x in row] for row in content]\n        column_widths = [max(c)+3 for c in zip(*lens)]\n    else:\n        column_widths = [column_width] * len_column\n    \n    table_str += '=' * (sum(column_widths) + 1) + '\\n'\n    \n    table_str += '|'\n    for i, item in enumerate(header):\n        table_str += ' ' + str(item).ljust(column_widths[i] - 2) + '|'\n    table_str += '\\n'\n    \n    table_str += '-' * (sum(column_widths) + 1) + '\\n'\n    \n    for line in content:\n        table_str += '|'\n        for i, item in enumerate(line):\n            table_str += ' ' + str(item).ljust(column_widths[i] - 2) + '|'\n        table_str += '\\n'\n    \n    table_str += '=' * (sum(column_widths) + 1) + '\\n'\n    \n    return table_str\n"""
stanza/utils/jieba.py,0,"b'""""""\nUtilities related to using Jieba in the pipeline.\n""""""\n\nimport re\n\nfrom stanza.models.common import doc\n\ndef check_jieba():\n    """"""\n    Import necessary components from Jieba to perform tokenization.\n    """"""\n    try:\n        import jieba\n    except ImportError:\n        raise ImportError(\n            ""Jieba is used but not installed on your machine. Go to https://pypi.org/project/jieba/ for installation instructions.""\n        )\n    return True\n\nclass JiebaTokenizer():\n    def __init__(self, lang=\'zh-hans\'):\n        """""" Construct a Jieba-based tokenizer by loading the Jieba pipeline.\n\n        Note that this tokenizer uses regex for sentence segmentation.\n        """"""\n        if lang not in [\'zh\', \'zh-hans\', \'zh-hant\']:\n            raise Exception(""Jieba tokenizer is currently only allowed in Chinese (simplified or traditional) pipelines."")\n\n        check_jieba()\n        import jieba\n        self.nlp = jieba\n\n    def tokenize(self, text):\n        """""" Tokenize a document with the Jieba tokenizer and wrap the results into a Doc object.\n        """"""\n        if not isinstance(text, str):\n            raise Exception(""Must supply a string to the Jieba tokenizer."")\n        tokens = self.nlp.cut(text, cut_all=False)\n\n        sentences = []\n        current_sentence = []\n        offset = 0\n        for token in tokens:\n            if re.match(\'\\s+\', token):\n                offset += len(token)\n                continue\n\n            token_entry = {\n                doc.TEXT: token,\n                doc.MISC: f""{doc.START_CHAR}={offset}|{doc.END_CHAR}={offset+len(token)}""\n            }\n            current_sentence.append(token_entry)\n            offset += len(token)\n\n            if token in [\'\xe3\x80\x82\', \'\xef\xbc\x81\', \'\xef\xbc\x9f\', \'!\', \'?\']:\n                sentences.append(current_sentence)\n                current_sentence = []\n\n        if len(current_sentence) > 0:\n            sentences.append(current_sentence)\n\n        return doc.Document(sentences, text)\n'"
stanza/utils/max_mwt_length.py,0,"b'import sys\n\nimport json\n\nwith open(sys.argv[1]) as f:\n    d = json.load(f)\n    l = max([0] + [len("" "".join(x[0][1])) for x in d])\n\nwith open(sys.argv[2]) as f:\n    d = json.load(f)\n    l = max([l] + [len("" "".join(x[0][1])) for x in d])\n\nprint(l)\n'"
stanza/utils/postprocess_vietnamese_tokenizer_data.py,0,"b'import argparse\nimport re\nimport sys\nfrom collections import Counter\nimport json\n\ndef para_to_chunks(text, char_level_pred):\n    chunks = []\n    preds = []\n    lastchunk = \'\'\n    lastpred = \'\'\n    for idx in range(len(text)):\n        if re.match(\'^\\w$\', text[idx], flags=re.UNICODE):\n            lastchunk += text[idx]\n        else:\n            if len(lastchunk) > 0 and not re.match(\'^\\W+$\', lastchunk, flags=re.UNICODE):\n                chunks += [lastchunk]\n                assert len(lastpred) > 0\n                preds += [int(lastpred)]\n                lastchunk = \'\'\n            if not re.match(\'^\\s$\', text[idx], flags=re.UNICODE):\n                # punctuation\n                chunks += [text[idx]]\n                preds += [int(char_level_pred[idx])]\n            else:\n                # prepend leading white spaces to chunks so we can tell the difference between ""2 , 2"" and ""2,2""\n                lastchunk += text[idx]\n        lastpred = char_level_pred[idx]\n\n    if len(lastchunk) > 0:\n        chunks += [lastchunk]\n        preds += [int(lastpred)]\n\n    return list(zip(chunks, preds))\n\ndef paras_to_chunks(text, char_level_pred):\n    return [para_to_chunks(re.sub(\'\\s\', \' \', pt.rstrip()), pc) for pt, pc in zip(text.split(\'\\n\\n\'), char_level_pred.split(\'\\n\\n\'))]\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'plaintext_file\', type=str, help=""Plaintext file containing the raw input"")\n    parser.add_argument(\'--char_level_pred\', type=str, default=None, help=""Plaintext file containing character-level predictions"")\n    parser.add_argument(\'-o\', \'--output\', default=None, type=str, help=""Output file name; output to the console if not specified (the default)"")\n\n    args = parser.parse_args()\n\n    with open(args.plaintext_file, \'r\') as f:\n        text = \'\'.join(f.readlines()).rstrip()\n        text = \'\\n\\n\'.join([x for x in text.split(\'\\n\\n\')])\n\n    if args.char_level_pred is not None:\n        with open(args.char_level_pred, \'r\') as f:\n            char_level_pred = \'\'.join(f.readlines())\n    else:\n        char_level_pred = \'\\n\\n\'.join([\'0\' * len(x) for x in text.split(\'\\n\\n\')])\n\n    assert len(text) == len(char_level_pred), \'Text has {} characters but there are {} char-level labels!\'.format(len(text), len(char_level_pred))\n\n    output = sys.stdout if args.output is None else open(args.output, \'w\')\n\n    json.dump(paras_to_chunks(text, char_level_pred), output)\n\n    output.close()\n'"
stanza/utils/prepare_ner_data.py,0,"b'""""""\nThis script converts NER data from the CoNLL03 format to the latest CoNLL-U format. The script assumes that in the \ninput column format data, the token is always in the first column, while the NER tag is always in the last column.\n""""""\n\nimport argparse\nimport json\n\nMIN_NUM_FIELD = 2\nMAX_NUM_FIELD = 5\n\nDOC_START_TOKEN = \'-DOCSTART-\'\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=""Convert the conll03 format data into conllu format."")\n    parser.add_argument(\'input\', help=\'Input conll03 format data filename.\')\n    parser.add_argument(\'output\', help=\'Output json filename.\')\n    args = parser.parse_args()\n    return args\n\ndef main():\n    args = parse_args()\n\n    sentences = load_conll03(args.input)\n    print(""{} examples loaded from {}"".format(len(sentences), args.input))\n    \n    document = []\n    for (words, tags) in sentences:\n        sent = []\n        for w, t in zip(words, tags):\n            sent += [{\'text\': w, \'ner\': t}]\n        document += [sent]\n\n    with open(args.output, \'w\') as outfile:\n        json.dump(document, outfile)\n    print(""Generated json file {}."".format(args.output))\n\ndef load_conll03(filename, skip_doc_start=True):\n    cached_lines = []\n    examples = []\n    with open(filename) as infile:\n        for line in infile:\n            line = line.strip()\n            if skip_doc_start and DOC_START_TOKEN in line:\n                continue\n            if len(line) > 0:\n                array = line.split()\n                if len(array) < MIN_NUM_FIELD:\n                    continue\n                else:\n                    cached_lines.append(line)\n            elif len(cached_lines) > 0:\n                example = process_cache(cached_lines)\n                examples.append(example)\n                cached_lines = []\n        if len(cached_lines) > 0:\n            examples.append(process_cache(cached_lines))\n    return examples\n\ndef process_cache(cached_lines):\n    tokens = []\n    ner_tags = []\n    for line in cached_lines:\n        array = line.split()\n        assert len(array) >= MIN_NUM_FIELD and len(array) <= MAX_NUM_FIELD\n        tokens.append(array[0])\n        ner_tags.append(array[-1])\n    return (tokens, ner_tags)\n\nif __name__ == \'__main__\':\n    main()\n'"
stanza/utils/prepare_resources.py,0,"b'import json\nimport argparse\nimport os\nfrom pathlib import Path\nimport hashlib\nimport shutil\nimport zipfile\n\n# default treebank for languages\ndefault_treebanks = {\n  ""af"": ""afribooms"",\n  ""grc"": ""proiel"",\n  ""ar"": ""padt"",\n  ""hy"": ""armtdp"",\n  ""eu"": ""bdt"",\n  ""bg"": ""btb"",\n  ""bxr"": ""bdt"",\n  ""ca"": ""ancora"",\n  ""zh-hant"": ""gsd"",\n  ""hr"": ""set"",\n  ""cs"": ""pdt"",\n  ""da"": ""ddt"",\n  ""nl"": ""alpino"",\n  ""en"": ""ewt"",\n  ""et"": ""edt"",\n  ""fi"": ""tdt"",\n  ""fr"": ""gsd"",\n  ""gl"": ""ctg"",\n  ""de"": ""gsd"",\n  ""got"": ""proiel"",\n  ""el"": ""gdt"",\n  ""he"": ""htb"",\n  ""hi"": ""hdtb"",\n  ""hu"": ""szeged"",\n  ""id"": ""gsd"",\n  ""ga"": ""idt"",\n  ""it"": ""isdt"",\n  ""ja"": ""gsd"",\n  ""kk"": ""ktb"",\n  ""ko"": ""kaist"",\n  ""kmr"": ""mg"",\n  ""la"": ""ittb"",\n  ""lv"": ""lvtb"",\n  ""sme"": ""giella"",\n  ""cu"": ""proiel"",\n  ""fro"": ""srcmf"",\n  ""fa"": ""seraji"",\n  ""pl"": ""lfg"",\n  ""pt"": ""bosque"",\n  ""ro"": ""rrt"",\n  ""ru"": ""syntagrus"",\n  ""sr"": ""set"",\n  ""sk"": ""snk"",\n  ""sl"": ""ssj"",\n  ""es"": ""ancora"",\n  ""sv"": ""talbanken"",\n  ""tr"": ""imst"",\n  ""uk"": ""iu"",\n  ""hsb"": ""ufal"",\n  ""ur"": ""udtb"",\n  ""ug"": ""udt"",\n  ""vi"": ""vtb"",\n  ""lt"": ""hse"",\n  ""wo"": ""wtb"",\n  ""nb"": ""bokmaal"",\n  ""mt"": ""mudt"",\n  ""swl"": ""sslc"",\n  ""cop"": ""scriptorium"",\n  ""be"": ""hse"",\n  ""zh-hans"": ""gsdsimp"",\n  ""lzh"": ""kyoto"",\n  ""gd"": ""arcosg"",\n  ""olo"": ""kkpp"",\n  ""ta"": ""ttb"",\n  ""te"": ""mtg"",\n  ""orv"": ""torot"",\n  ""nn"": ""nynorsk"",\n  ""mr"": ""ufal""\n}\n\n\n# default ner for languages\ndefault_ners = {\n  ""ar"": ""aqmar"",\n  ""de"": ""conll03"",\n  ""en"": ""ontonotes"",\n  ""es"": ""conll02"",\n  ""fr"": ""wikiner"",\n  ""nl"": ""conll02"",\n  ""ru"": ""wikiner"",\n  ""zh-hans"": ""ontonotes""\n}\n\n\n# default charlms for languages\ndefault_charlms = {\n  ""ar"": ""ccwiki"",\n  ""de"": ""newswiki"",\n  ""en"": ""1billion"",\n  ""es"": ""newswiki"",\n  ""fr"": ""newswiki"",\n  ""nl"": ""ccwiki"",\n  ""ru"": ""newswiki"",\n  ""zh-hans"": ""gigaword""\n}\n\n\n# map processor name to file ending\nprocessor_to_ending = {\n  ""tokenize"": ""tokenizer"",\n  ""mwt"": ""mwt_expander"",\n  ""pos"": ""tagger"",\n  ""lemma"": ""lemmatizer"",\n  ""depparse"": ""parser"",\n  ""ner"": ""nertagger"",\n  ""pretrain"": ""pretrain"",\n  ""forward_charlm"": ""forward_charlm"",\n  ""backward_charlm"": ""backward_charlm""\n}\nending_to_processor = {j: i for i, j in processor_to_ending.items()}\n\n# add full language name to language code and add alias in resources\nlcode2lang = {\n    ""af"": ""Afrikaans"",\n    ""grc"": ""Ancient_Greek"",\n    ""ar"": ""Arabic"",\n    ""hy"": ""Armenian"",\n    ""eu"": ""Basque"",\n    ""be"": ""Belarusian"",\n    ""br"": ""Breton"",\n    ""bg"": ""Bulgarian"",\n    ""bxr"": ""Buryat"",\n    ""ca"": ""Catalan"",\n    ""zh-hant"": ""Traditional_Chinese"",\n    ""lzh"": ""Classical_Chinese"",\n    ""cop"": ""Coptic"",\n    ""hr"": ""Croatian"",\n    ""cs"": ""Czech"",\n    ""da"": ""Danish"",\n    ""nl"": ""Dutch"",\n    ""en"": ""English"",\n    ""et"": ""Estonian"",\n    ""fo"": ""Faroese"",\n    ""fi"": ""Finnish"",\n    ""fr"": ""French"",\n    ""gl"": ""Galician"",\n    ""de"": ""German"",\n    ""got"": ""Gothic"",\n    ""el"": ""Greek"",\n    ""he"": ""Hebrew"",\n    ""hi"": ""Hindi"",\n    ""hu"": ""Hungarian"",\n    ""id"": ""Indonesian"",\n    ""ga"": ""Irish"",\n    ""it"": ""Italian"",\n    ""ja"": ""Japanese"",\n    ""kk"": ""Kazakh"",\n    ""ko"": ""Korean"",\n    ""kmr"": ""Kurmanji"",\n    ""lt"": ""Lithuanian"",\n    ""olo"": ""Livvi"",\n    ""la"": ""Latin"",\n    ""lv"": ""Latvian"",\n    ""mt"": ""Maltese"",\n    ""mr"": ""Marathi"",\n    ""pcm"": ""Naija"",\n    ""sme"": ""North_Sami"",\n    ""nb"": ""Norwegian_Bokmaal"",\n    ""nn"": ""Norwegian_Nynorsk"",\n    ""cu"": ""Old_Church_Slavonic"",\n    ""fro"": ""Old_French"",\n    ""orv"": ""Old_Russian"",\n    ""fa"": ""Persian"",\n    ""pl"": ""Polish"",\n    ""pt"": ""Portuguese"",\n    ""ro"": ""Romanian"",\n    ""ru"": ""Russian"",\n    ""gd"": ""Scottish_Gaelic"",\n    ""sr"": ""Serbian"",\n    ""zh-hans"": ""Simplified_Chinese"",\n    ""sk"": ""Slovak"",\n    ""sl"": ""Slovenian"",\n    ""es"": ""Spanish"",\n    ""sv"": ""Swedish"",\n    ""swl"": ""Swedish_Sign_Language"",\n    ""ta"": ""Tamil"",\n    ""te"": ""Telugu"",\n    ""th"": ""Thai"",\n    ""tr"": ""Turkish"",\n    ""uk"": ""Ukrainian"",\n    ""hsb"": ""Upper_Sorbian"",\n    ""ur"": ""Urdu"",\n    ""ug"": ""Uyghur"",\n    ""vi"": ""Vietnamese"",\n    ""wo"": ""Wolof""\n}\n\n\ndef ensure_dir(dir):\n    Path(dir).mkdir(parents=True, exist_ok=True)\n\n\ndef copy_file(src, dst):\n    ensure_dir(Path(dst).parent)\n    shutil.copy(src, dst)\n\n\ndef get_md5(path):\n    data = open(path, \'rb\').read()\n    return hashlib.md5(data).hexdigest()\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--input_dir\', type=str, help=\'Input dir for various models.\')\n    parser.add_argument(\'--output_dir\', type=str, help=\'Output dir for various models.\')\n    args = parser.parse_args()\n    return args\n\n\ndef process_dirs(args):\n    dirs = sorted(os.listdir(args.input_dir))\n    resources = {}\n\n    for dir in dirs:\n        print(dir)\n        models = sorted(os.listdir(os.path.join(args.input_dir, dir)))\n        for model in models:\n            if not model.endswith(\'.pt\'): continue\n            # get processor\n            lang, package, processor = model.replace(\'.pt\', \'\').replace(\'.\', \'_\').split(\'_\', 2)\n            processor = ending_to_processor[processor]\n            # copy file\n            input_path = os.path.join(args.input_dir, dir, model)\n            output_path = os.path.join(args.output_dir, lang, processor, package + \'.pt\')\n            ensure_dir(Path(output_path).parent)\n            shutil.copy(input_path, output_path)\n            # maintain md5\n            md5 = get_md5(output_path)\n            # maintain dependencies\n            if processor == \'pos\' or processor == \'depparse\':\n                dependencies = [{\'model\': \'pretrain\', \'package\': package}]\n            elif processor == \'ner\':\n                charlm_package = default_charlms[lang]\n                dependencies = [{\'model\': \'forward_charlm\', \'package\': charlm_package}, {\'model\': \'backward_charlm\', \'package\': charlm_package}]\n            else:\n                dependencies = None\n            # maintain resources\n            if lang not in resources: resources[lang] = {}\n            if processor not in resources[lang]: resources[lang][processor] = {}\n            if dependencies:\n                resources[lang][processor][package] = {\'md5\': md5, \'dependencies\': dependencies}\n            else:\n                resources[lang][processor][package] = {\'md5\': md5}\n    json.dump(resources, open(os.path.join(args.output_dir, \'resources.json\'), \'w\'), indent=2)\n\n\ndef process_defaults(args):\n    resources = json.load(open(os.path.join(args.output_dir, \'resources.json\')))\n    for lang in resources:\n        if lang not in default_treebanks: \n            print(lang + \' not in default treebanks!!!\')\n            continue\n        print(lang)\n\n        ud_package = default_treebanks[lang]\n        os.chdir(os.path.join(args.output_dir, lang))\n        zipf = zipfile.ZipFile(\'default.zip\', \'w\', zipfile.ZIP_DEFLATED)\n        default_processors = {}\n        default_dependencies = {\'pos\': [{\'model\': \'pretrain\', \'package\': ud_package}], \'depparse\': [{\'model\': \'pretrain\', \'package\': ud_package}]}\n\n        if lang in default_ners:\n            ner_package = default_ners[lang]\n            charlm_package = default_charlms[lang]\n            default_dependencies[\'ner\'] = [{\'model\': \'forward_charlm\', \'package\': charlm_package}, {\'model\': \'backward_charlm\', \'package\': charlm_package}]\n        \n        processors = [\'tokenize\', \'mwt\', \'lemma\', \'pos\', \'depparse\', \'ner\', \'pretrain\', \'forward_charlm\', \'backward_charlm\'] if lang in default_ners else [\'tokenize\', \'mwt\', \'lemma\', \'pos\', \'depparse\', \'pretrain\']\n        for processor in processors:\n            if processor == \'ner\': package = ner_package\n            elif processor in [\'forward_charlm\', \'backward_charlm\']: package = charlm_package\n            else: package = ud_package\n\n            if os.path.exists(os.path.join(args.output_dir, lang, processor, package + \'.pt\')):\n                if processor in [\'tokenize\', \'mwt\', \'lemma\', \'pos\', \'depparse\', \'ner\']:\n                     default_processors[processor] = package\n                zipf.write(processor)\n                zipf.write(os.path.join(processor, package + \'.pt\'))\n        zipf.close()\n        default_md5 = get_md5(os.path.join(args.output_dir, lang, \'default.zip\'))\n        resources[lang][\'default_processors\'] = default_processors\n        resources[lang][\'default_dependencies\'] = default_dependencies\n        resources[lang][\'default_md5\'] = default_md5\n\n    json.dump(resources, open(os.path.join(args.output_dir, \'resources.json\'), \'w\'), indent=2)\n\n\ndef process_lcode(args):\n    resources = json.load(open(os.path.join(args.output_dir, \'resources.json\')))\n    resources_new = {}\n    for lang in resources:\n        if lang not in lcode2lang:\n            print(lang + \' not found in lcode2lang!\')\n            continue\n        lang_name = lcode2lang[lang]\n        resources[lang][\'lang_name\'] = lang_name\n        resources_new[lang.lower()] = resources[lang.lower()]\n        resources_new[lang_name.lower()] = {\'alias\': lang.lower()}\n    json.dump(resources_new, open(os.path.join(args.output_dir, \'resources.json\'), \'w\'), indent=2)\n\n\ndef process_misc(args):\n    resources = json.load(open(os.path.join(args.output_dir, \'resources.json\')))\n    resources[\'no\'] = {\'alias\': \'nb\'}\n    resources[\'zh\'] = {\'alias\': \'zh-hans\'}\n    resources[\'url\'] = \'http://nlp.stanford.edu/software/stanza\'\n    json.dump(resources, open(os.path.join(args.output_dir, \'resources.json\'), \'w\'), indent=2)\n\n\ndef main():\n    args = parse_args()\n    process_dirs(args)\n    process_defaults(args)\n    process_lcode(args)\n    process_misc(args)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
stanza/utils/prepare_tokenizer_data.py,0,"b'import argparse\nimport re\nimport sys\n\nPARAGRAPH_BREAK = re.compile(\'\\n\\s*\\n\')\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\'plaintext_file\', type=str, help=""Plaintext file containing the raw input"")\nparser.add_argument(\'conllu_file\', type=str, help=""CoNLL-U file containing tokens and sentence breaks"")\nparser.add_argument(\'-o\', \'--output\', default=None, type=str, help=""Output file name; output to the console if not specified (the default)"")\nparser.add_argument(\'-m\', \'--mwt_output\', default=None, type=str, help=""Output file name for MWT expansions; output to the console if not specified (the default)"")\n\nargs = parser.parse_args()\n\nwith open(args.plaintext_file, \'r\') as f:\n    text = \'\'.join(f.readlines())\ntextlen = len(text)\n\noutput = sys.stdout if args.output is None else open(args.output, \'w\')\n\nindex = 0 # character offset in rawtext\n\ndef is_para_break(index, text):\n    """""" Detect if a paragraph break can be found, and return the length of the paragraph break sequence. """"""\n    if text[index] == \'\\n\':\n        para_break = PARAGRAPH_BREAK.match(text[index:])\n        if para_break:\n            break_len = len(para_break.group(0))\n            return True, break_len\n    return False, 0\n\ndef find_next_word(index, text, word, output):\n    """"""\n    Locate the next word in the text. In case a paragraph break is found, also write paragraph break to labels.\n    """"""\n    idx = 0\n    word_sofar = \'\'\n    yeah=False\n    while index < len(text) and idx < len(word):\n        para_break, break_len = is_para_break(index, text)\n        if para_break:\n            # multiple newlines found, paragraph break\n            if len(word_sofar) > 0:\n                assert re.match(r\'^\\s+$\', word_sofar), \'Found non-empty string at the end of a paragraph that doesn\\\'t match any token: |{}|\'.format(word_sofar)\n                word_sofar = \'\'\n\n            output.write(\'\\n\\n\')\n            index += break_len - 1\n        elif re.match(r\'^\\s$\', text[index]) and not re.match(r\'^\\s$\', word[idx]):\n            # whitespace found, and whitespace is not part of a word\n            word_sofar += text[index]\n        else:\n            # non-whitespace char, or a whitespace char that\'s part of a word\n            word_sofar += text[index]\n            assert text[index].replace(\'\\n\', \' \') == word[idx], ""Character mismatch: raw text contains |%s| but the next word is |%s|."" % (word_sofar, word)\n            idx += 1\n        index += 1\n    return index, word_sofar\n\nmwt_expansions = []\nwith open(args.conllu_file, \'r\') as f:\n    buf = \'\'\n    mwtbegin = 0\n    mwtend = -1\n    expanded = []\n    last_comments = """"\n    for line in f:\n        line = line.strip()\n        if len(line):\n            if line[0] == ""#"":\n                # comment, don\'t do anything\n                if len(last_comments) == 0:\n                    last_comments = line\n                continue\n\n            line = line.split(\'\\t\')\n            if \'.\' in line[0]:\n                # the tokenizer doesn\'t deal with ellipsis\n                continue\n\n            word = line[1]\n            if \'-\' in line[0]:\n                # multiword token\n                mwtbegin, mwtend = [int(x) for x in line[0].split(\'-\')]\n                lastmwt = word\n                expanded = []\n            elif mwtbegin <= int(line[0]) < mwtend:\n                expanded += [word]\n                continue\n            elif int(line[0]) == mwtend:\n                expanded += [word]\n                expanded = [x.lower() for x in expanded] # evaluation doesn\'t care about case\n                mwt_expansions += [(lastmwt, tuple(expanded))]\n                if lastmwt[0].islower() and not expanded[0][0].islower():\n                    print(\'Sentence ID with potential wrong MWT expansion: \', last_comments, file=sys.stderr)\n                mwtbegin = 0\n                mwtend = -1\n                lastmwt = None\n                continue\n\n            if len(buf):\n                output.write(buf)\n            index, word_found = find_next_word(index, text, word, output)\n            buf = \'0\' * (len(word_found)-1) + (\'1\' if \'-\' not in line[0] else \'3\')\n        else:\n            # sentence break found\n            if len(buf):\n                assert int(buf[-1]) >= 1\n                output.write(buf[:-1] + \'{}\'.format(int(buf[-1]) + 1))\n                buf = \'\'\n\n            last_comments = \'\'\n\noutput.close()\n\nfrom collections import Counter\nmwts = Counter(mwt_expansions)\nif args.mwt_output is None:\n    print(\'MWTs:\', mwts)\nelse:\n    import json\n    with open(args.mwt_output, \'w\') as f:\n        json.dump(list(mwts.items()), f)\n\n    print(\'{} unique MWTs found in data\'.format(len(mwts)))\n'"
stanza/utils/resources.py,0,"b'""""""\nutilities for getting resources\n""""""\n\nimport os\nimport requests\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport json\nimport hashlib\nimport zipfile\nimport shutil\nimport logging\n\nfrom stanza.utils.helper_func import make_table\nfrom stanza.pipeline._constants import TOKENIZE, MWT, POS, LEMMA, DEPPARSE, NER, SUPPORTED_TOKENIZERS\nfrom stanza._version import __resources_version__\n\nlogger = logging.getLogger(\'stanza\')\n\n# set home dir for default\nHOME_DIR = str(Path.home())\nDEFAULT_RESOURCES_URL = \'https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master\'\nDEFAULT_MODEL_DIR = os.getenv(\'STANZA_RESOURCES_DIR\', os.path.join(HOME_DIR, \'stanza_resources\'))\nPIPELINE_NAMES = [TOKENIZE, MWT, POS, LEMMA, DEPPARSE, NER]\n\n# given a language and models path, build a default configuration\ndef build_default_config(resources, lang, dir, load_list):\n    default_config = {}\n    for item in load_list:\n        processor, package, dependencies = item\n\n        # handle case when spacy or jieba is specified as tokenizer\n        if processor == TOKENIZE and package in SUPPORTED_TOKENIZERS:\n            default_config[f""{TOKENIZE}_with_{package}""] = True\n        # handle case when identity is specified as lemmatizer\n        elif processor == LEMMA and package == \'identity\':\n            default_config[f""{LEMMA}_use_identity""] = True\n        else:\n            default_config[f""{processor}_model_path""] = os.path.join(dir, lang, processor, package + \'.pt\')\n\n        if not dependencies: continue\n        for dependency in dependencies:\n            dep_processor, dep_model = dependency\n            default_config[f""{processor}_{dep_processor}_path""] = os.path.join(dir, lang, dep_processor, dep_model + \'.pt\')\n\n    return default_config\n\ndef ensure_dir(dir):\n    Path(dir).mkdir(parents=True, exist_ok=True)\n\ndef get_md5(path):\n    data = open(path, \'rb\').read()\n    return hashlib.md5(data).hexdigest()\n\ndef unzip(dir, filename):\n    logger.debug(f\'Unzip: {dir}/{filename}...\')\n    with zipfile.ZipFile(os.path.join(dir, filename)) as f:\n        f.extractall(dir)\n\ndef is_file_existed(path, md5):\n    return os.path.exists(path) and get_md5(path) == md5\n\ndef download_file(url, path):\n    verbose = logger.level in [0, 10, 20]\n    r = requests.get(url, stream=True)\n    with open(path, \'wb\') as f:\n        file_size = int(r.headers.get(\'content-length\'))\n        default_chunk_size = 131072\n        desc = \'Downloading \' + url\n        with tqdm(total=file_size, unit=\'B\', unit_scale=True, disable=not verbose, desc=desc) as pbar:\n            for chunk in r.iter_content(chunk_size=default_chunk_size):\n                if chunk:\n                    f.write(chunk)\n                    f.flush()\n                    pbar.update(len(chunk))\n\ndef request_file(url, path, md5=None):\n    ensure_dir(Path(path).parent)\n    if is_file_existed(path, md5):\n        logger.info(f\'File exists: {path}.\')\n        return\n    download_file(url, path)\n    assert(not md5 or is_file_existed(path, md5))\n\ndef sort_processors(processor_list):\n    sorted_list = []\n    for processor in PIPELINE_NAMES:\n        for item in processor_list:\n            if item[0] == processor:\n                sorted_list.append(item)\n    return sorted_list\n\ndef maintain_processor_list(resources, lang, package, processors):\n    processor_list = {}\n    # resolve processor models\n    if processors:\n        logger.debug(f\'Processing parameter ""processors""...\')\n        for key, value in processors.items():\n            assert(key in PIPELINE_NAMES)\n            assert(isinstance(key, str) and isinstance(value, str))\n            # check if keys and values can be found\n            if key in resources[lang] and value in resources[lang][key]:\n                logger.debug(f\'Find {key}: {value}.\')\n                processor_list[key] = value\n            # allow values to be default in some cases\n            elif key in resources[lang][\'default_processors\'] and value == \'default\':\n                logger.debug(f\'Find {key}: {resources[lang][""default_processors""][key]}.\')\n                processor_list[key] = resources[lang][\'default_processors\'][key]\n            # allow tokenize to be set to ""spacy"" or ""jieba""\n            elif key == TOKENIZE and value in SUPPORTED_TOKENIZERS:\n                logger.debug(f\'Find {key}: {value}. Using external {value} library as tokenizer.\')\n                processor_list[key] = value\n            # allow lemma to be set to ""identity""\n            elif key == LEMMA and value == \'identity\':\n                logger.debug(f\'Find {key}: {value}. Using identical lemmatizer.\')\n                processor_list[key] = value\n            # cannot find and warn user\n            else:\n                logger.warning(f\'Can not find {key}: {value} from official model list. Ignoring it.\')\n    # resolve package\n    if package:\n        logger.debug(f\'Processing parameter ""package""...\')\n        if package == \'default\':\n            for key, value in resources[lang][\'default_processors\'].items():\n                if key not in processor_list:\n                    logger.debug(f\'Find {key}: {value}.\')\n                    processor_list[key] = value\n        else:\n            flag = False\n            for key in PIPELINE_NAMES:\n                if key not in resources[lang]: continue\n                if package in resources[lang][key]:\n                    flag = True\n                    if key not in processor_list:\n                        logger.debug(f\'Find {key}: {package}.\')\n                        processor_list[key] = package\n                    else:\n                        logger.debug(f\'{key}: {package} is overwritten by {key}: {processors[key]}.\')\n            if not flag: logger.warning((f\'Can not find package: {package}.\'))\n    processor_list = [[key, value] for key, value in processor_list.items()]\n    processor_list = sort_processors(processor_list)\n    return processor_list\n\ndef add_dependencies(resources, lang, processor_list):\n    default_dependencies = resources[lang][\'default_dependencies\']\n    for item in processor_list:\n        processor, package = item\n        dependencies = default_dependencies.get(processor, None)\n        # skip dependency checking for special spacy/jieba tokenizer and identity lemmatizer\n        if not any([processor == TOKENIZE and package in SUPPORTED_TOKENIZERS, processor == LEMMA and package == \'identity\']):\n            dependencies = resources[lang][processor][package].get(\'dependencies\', dependencies)\n        if dependencies:\n            dependencies = [[dependency[\'model\'], dependency[\'package\']] for dependency in dependencies]\n        item.append(dependencies)\n    return processor_list\n\ndef flatten_processor_list(processor_list):\n    flattened_processor_list = []\n    dependencies_list = []\n    for item in processor_list:\n        processor, package, dependencies = item\n        flattened_processor_list.append([processor, package])\n        if dependencies: dependencies_list += [tuple(dependency) for dependency in dependencies]\n    dependencies_list = [list(item) for item in set(dependencies_list)]\n    for processor, package in dependencies_list:\n        logger.debug(f\'Find dependency {processor}: {package}.\')\n    flattened_processor_list += dependencies_list\n    return flattened_processor_list\n\ndef set_logging_level(logging_level, verbose):\n    # Check verbose for easy logging control\n    if verbose == False:\n        logging_level = \'ERROR\'\n    elif verbose == True:\n        logging_level = \'INFO\'\n\n    # Set logging level\n    logging_level = logging_level.upper()\n    all_levels = [\'DEBUG\', \'INFO\', \'WARNING\', \'WARN\', \'ERROR\', \'CRITICAL\', \'FATAL\']\n    if logging_level not in all_levels:\n        raise Exception(f""Unrecognized logging level for pipeline: {logging_level}. Must be one of {\', \'.join(all_levels)}."")\n    logger.setLevel(logging_level)\n    return logging_level\n\ndef process_pipeline_parameters(lang, dir, package, processors):\n    # Check parameter types and convert values to lower case\n    if isinstance(lang, str):\n        lang = lang.strip().lower()\n    elif lang is not None:\n        raise Exception(f""The parameter \'lang\' should be str, but got {type(lang).__name__} instead."")\n\n    if isinstance(dir, str):\n        dir = dir.strip()\n    elif dir is not None:\n        raise Exception(f""The parameter \'dir\' should be str, but got {type(dir).__name__} instead."")\n\n    if isinstance(package, str):\n        package = package.strip().lower()\n    elif package is not None:\n        raise Exception(f""The parameter \'package\' should be str, but got {type(package).__name__} instead."")\n\n    if isinstance(processors, str):\n        # Special case: processors is str, compatible with older verson\n        processors = {processor.strip().lower(): package for processor in processors.split(\',\')}\n        package = None\n    elif isinstance(processors, dict):\n        processors = {k.strip().lower(): v.strip().lower() for k, v in processors.items()}\n    elif processors is not None:\n        raise Exception(f""The parameter \'processors\' should be dict or str, but got {type(processors).__name__} instead."")\n\n    return lang, dir, package, processors\n\n# main download function\ndef download(lang=\'en\', dir=DEFAULT_MODEL_DIR, package=\'default\', processors={}, logging_level=\'INFO\', verbose=None):\n    # set global logging level\n    set_logging_level(logging_level, verbose)\n    # process different pipeline parameters\n    lang, dir, package, processors = process_pipeline_parameters(lang, dir, package, processors)\n\n    # Download resources.json to obtain latest packages.\n    logger.debug(\'Downloading resource file...\')\n    request_file(f\'{DEFAULT_RESOURCES_URL}/resources_{__resources_version__}.json\', os.path.join(dir, \'resources.json\'))\n    resources = json.load(open(os.path.join(dir, \'resources.json\')))\n    if lang not in resources:\n        raise Exception(f\'Unsupported language: {lang}.\')\n    if \'alias\' in resources[lang]:\n        logger.info(f\'""{lang}"" is an alias for ""{resources[lang][""alias""]}""\')\n        lang = resources[lang][\'alias\']\n    lang_name = resources[lang][\'lang_name\'] if \'lang_name\' in resources[lang] else \'\'\n    url = resources[\'url\']\n\n    # Default: download zipfile and unzip\n    if package == \'default\' and (processors is None or len(processors) == 0):\n        logger.info(f\'Downloading default packages for language: {lang} ({lang_name})...\')\n        request_file(f\'{url}/{__resources_version__}/{lang}/default.zip\', os.path.join(dir, lang, f\'default.zip\'), md5=resources[lang][\'default_md5\'])\n        unzip(os.path.join(dir, lang), \'default.zip\')\n    # Customize: maintain download list\n    else:\n        download_list = maintain_processor_list(resources, lang, package, processors)\n        download_list = add_dependencies(resources, lang, download_list)\n        download_list = flatten_processor_list(download_list)\n        download_table = make_table([\'Processor\', \'Package\'], download_list)\n        logger.info(f\'Downloading these customized packages for language: {lang} ({lang_name})...\\n{download_table}\')\n\n        # Download packages\n        for key, value in download_list:\n            try:\n                request_file(f\'{url}/{__resources_version__}/{lang}/{key}/{value}.pt\', os.path.join(dir, lang, key, f\'{value}.pt\'), md5=resources[lang][key][value][\'md5\'])\n            except KeyError as e:\n                raise Exception(f""Cannot find the following processor and model name combination: {key}, {value}. Please check if you have provided the correct model name."") from e\n    logger.info(f\'Finished downloading models and saved to {dir}.\')\n'"
stanza/utils/select_backoff.py,0,"b'import sys\n\nbackoff_models = { ""UD_Breton-KEB"": ""ga_idt"",\n                   ""UD_Czech-PUD"": ""cs_pdt"",\n                   ""UD_English-PUD"": ""en_ewt"",\n                   ""UD_Faroese-OFT"": ""nn_nynorsk"",\n                   ""UD_Finnish-PUD"": ""fi_tdt"",\n                   ""UD_Japanese-Modern"": ""ja_gsd"",\n                   ""UD_Naija-NSC"": ""en_ewt"",\n                   ""UD_Swedish-PUD"": ""sv_talbanken""\n                 }\n\nprint(backoff_models[sys.argv[1]])\n'"
stanza/utils/spacy.py,0,"b'""""""\nUtilities related to using spaCy in the pipeline.\n""""""\n\nfrom stanza.models.common import doc\n\ndef check_spacy():\n    """"""\n    Import necessary components from spaCy to perform tokenization.\n    """"""\n    try:\n        import spacy\n    except ImportError:\n        raise ImportError(\n            ""spaCy is used but not installed on your machine. Go to https://spacy.io/usage for installation instructions.""\n        )\n    return True\n\nclass SpacyTokenizer():\n    def __init__(self, lang=\'en\'):\n        """""" Construct a spaCy-based tokenizer by loading the spaCy pipeline.\n        """"""\n        if lang != \'en\':\n            raise Exception(""spaCy tokenizer is currently only allowed in English pipeline."")\n\n        try:\n            import spacy\n            from spacy.lang.en import English\n        except ImportError:\n            raise ImportError(\n                ""spaCy 2.0+ is used but not installed on your machine. Go to https://spacy.io/usage for installation instructions.""\n            )\n        \n        # Create a Tokenizer with the default settings for English\n        # including punctuation rules and exceptions\n        self.nlp = English()\n        # by default spacy uses dependency parser to do ssplit\n        # we need to add a sentencizer for fast rule-based ssplit\n        sentencizer = self.nlp.create_pipe(\'sentencizer\')\n        self.nlp.add_pipe(sentencizer)\n    \n    def tokenize(self, text):\n        """""" Tokenize a document with the spaCy tokenizer and wrap the results into a Doc object.\n        """"""\n        if not isinstance(text, str):\n            raise Exception(""Must supply a string to the spaCy tokenizer."")\n        spacy_doc = self.nlp(text)\n        \n        sentences = []\n        for sent in spacy_doc.sents:\n            tokens = []\n            for tok in sent:\n                token_entry = {\n                    doc.TEXT: tok.text,\n                    doc.MISC: f""{doc.START_CHAR}={tok.idx}|{doc.END_CHAR}={tok.idx+len(tok.text)}""\n                }\n                tokens.append(token_entry)\n            sentences.append(tokens)\n\n        return doc.Document(sentences, text)'"
stanza/models/common/__init__.py,0,b''
stanza/models/common/beam.py,2,"b'from __future__ import division\nimport torch\n\nimport stanza.models.common.seq2seq_constant as constant\n\n""""""\n Adapted and modified from the OpenNMT project.\n\n Class for managing the internals of the beam search process.\n\n\n         hyp1-hyp1---hyp1 -hyp1\n                 \\             /\n         hyp2 \\-hyp2 /-hyp2hyp2\n                               /      \\\n         hyp3-hyp3---hyp3 -hyp3\n         ========================\n\n Takes care of beams, back pointers, and scores.\n""""""\n\n\nclass Beam(object):\n    def __init__(self, size, cuda=False):\n\n        self.size = size\n        self.done = False\n\n        self.tt = torch.cuda if cuda else torch\n\n        # The score for each translation on the beam.\n        self.scores = self.tt.FloatTensor(size).zero_()\n        self.allScores = []\n\n        # The backpointers at each time-step.\n        self.prevKs = []\n\n        # The outputs at each time-step.\n        self.nextYs = [self.tt.LongTensor(size).fill_(constant.PAD_ID)]\n        self.nextYs[0][0] = constant.SOS_ID\n\n        # The copy indices for each time\n        self.copy = []\n\n    def get_current_state(self):\n        ""Get the outputs for the current timestep.""\n        return self.nextYs[-1]\n\n    def get_current_origin(self):\n        ""Get the backpointers for the current timestep.""\n        return self.prevKs[-1]\n\n    def advance(self, wordLk, copy_indices=None):\n        """"""\n        Given prob over words for every last beam `wordLk` and attention\n        `attnOut`: Compute and update the beam search.\n\n        Parameters:\n\n        * `wordLk`- probs of advancing from the last step (K x words)\n        * `copy_indices` - copy indices (K x ctx_len)\n\n        Returns: True if beam search is complete.\n        """"""\n        if self.done:\n            return True\n        numWords = wordLk.size(1)\n\n        # Sum the previous scores.\n        if len(self.prevKs) > 0:\n            beamLk = wordLk + self.scores.unsqueeze(1).expand_as(wordLk)\n        else:\n            # first step, expand from the first position\n            beamLk = wordLk[0]\n\n        flatBeamLk = beamLk.view(-1)\n\n        bestScores, bestScoresId = flatBeamLk.topk(self.size, 0, True, True)\n        self.allScores.append(self.scores)\n        self.scores = bestScores\n\n        # bestScoresId is flattened beam x word array, so calculate which\n        # word and beam each score came from\n        prevK = bestScoresId / numWords\n        self.prevKs.append(prevK)\n        self.nextYs.append(bestScoresId - prevK * numWords)\n        if copy_indices is not None:\n            self.copy.append(copy_indices.index_select(0, prevK))\n\n        # End condition is when top-of-beam is EOS.\n        if self.nextYs[-1][0] == constant.EOS_ID:\n            self.done = True\n            self.allScores.append(self.scores)\n\n        return self.done\n\n    def sort_best(self):\n        return torch.sort(self.scores, 0, True)\n\n    def get_best(self):\n        ""Get the score of the best in the beam.""\n        scores, ids = self.sortBest()\n        return scores[1], ids[1]\n\n    def get_hyp(self, k):\n        """"""\n        Walk back to construct the full hypothesis.\n\n        Parameters:\n\n             * `k` - the position in the beam to construct.\n\n         Returns: The hypothesis\n        """"""\n        hyp = []\n        cpy = []\n        for j in range(len(self.prevKs) - 1, -1, -1):\n            hyp.append(self.nextYs[j+1][k])\n            if len(self.copy) > 0:\n                cpy.append(self.copy[j][k])\n            k = self.prevKs[j][k]\n         \n        hyp = hyp[::-1]\n        cpy = cpy[::-1]\n        # postprocess: if cpy index is not -1, use cpy index instead of hyp word\n        for i,cidx in enumerate(cpy):\n            if cidx >= 0:\n                hyp[i] = -(cidx+1) # make index 1-based and flip it for token generation\n\n        return hyp\n'"
stanza/models/common/biaffine.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PairwiseBilinear(nn.Module):\n    \'\'\' A bilinear module that deals with broadcasting for efficient memory usage.\n    Input: tensors of sizes (N x L1 x D1) and (N x L2 x D2)\n    Output: tensor of size (N x L1 x L2 x O)\'\'\'\n    def __init__(self, input1_size, input2_size, output_size, bias=True):\n        super().__init__()\n\n        self.input1_size = input1_size\n        self.input2_size = input2_size\n        self.output_size = output_size\n\n        self.weight = nn.Parameter(torch.Tensor(input1_size, input2_size, output_size))\n        self.bias = nn.Parameter(torch.Tensor(output_size)) if bias else 0\n\n    def forward(self, input1, input2):\n        input1_size = list(input1.size())\n        input2_size = list(input2.size())\n        output_size = [input1_size[0], input1_size[1], input2_size[1], self.output_size]\n\n        # ((N x L1) x D1) * (D1 x (D2 x O)) -> (N x L1) x (D2 x O)\n        intermediate = torch.mm(input1.view(-1, input1_size[-1]), self.weight.view(-1, self.input2_size * self.output_size))\n        # (N x L2 x D2) -> (N x D2 x L2)\n        input2 = input2.transpose(1, 2)\n        # (N x (L1 x O) x D2) * (N x D2 x L2) -> (N x (L1 x O) x L2)\n        output = intermediate.view(input1_size[0], input1_size[1] * self.output_size, input2_size[2]).bmm(input2)\n        # (N x (L1 x O) x L2) -> (N x L1 x L2 x O)\n        output = output.view(input1_size[0], input1_size[1], self.output_size, input2_size[1]).transpose(2, 3)\n\n        return output\n\nclass BiaffineScorer(nn.Module):\n    def __init__(self, input1_size, input2_size, output_size):\n        super().__init__()\n        self.W_bilin = nn.Bilinear(input1_size + 1, input2_size + 1, output_size)\n\n        self.W_bilin.weight.data.zero_()\n        self.W_bilin.bias.data.zero_()\n\n    def forward(self, input1, input2):\n        input1 = torch.cat([input1, input1.new_ones(*input1.size()[:-1], 1)], len(input1.size())-1)\n        input2 = torch.cat([input2, input2.new_ones(*input2.size()[:-1], 1)], len(input2.size())-1)\n        return self.W_bilin(input1, input2)\n\nclass PairwiseBiaffineScorer(nn.Module):\n    def __init__(self, input1_size, input2_size, output_size):\n        super().__init__()\n        self.W_bilin = PairwiseBilinear(input1_size + 1, input2_size + 1, output_size)\n\n        self.W_bilin.weight.data.zero_()\n        self.W_bilin.bias.data.zero_()\n\n    def forward(self, input1, input2):\n        input1 = torch.cat([input1, input1.new_ones(*input1.size()[:-1], 1)], len(input1.size())-1)\n        input2 = torch.cat([input2, input2.new_ones(*input2.size()[:-1], 1)], len(input2.size())-1)\n        return self.W_bilin(input1, input2)\n\nclass DeepBiaffineScorer(nn.Module):\n    def __init__(self, input1_size, input2_size, hidden_size, output_size, hidden_func=F.relu, dropout=0, pairwise=True):\n        super().__init__()\n        self.W1 = nn.Linear(input1_size, hidden_size)\n        self.W2 = nn.Linear(input2_size, hidden_size)\n        self.hidden_func = hidden_func\n        if pairwise:\n            self.scorer = PairwiseBiaffineScorer(hidden_size, hidden_size, output_size)\n        else:\n            self.scorer = BiaffineScorer(hidden_size, hidden_size, output_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input1, input2):\n        return self.scorer(self.dropout(self.hidden_func(self.W1(input1))), self.dropout(self.hidden_func(self.W2(input2))))\n\nif __name__ == ""__main__"":\n    x1 = torch.randn(3,4)\n    x2 = torch.randn(3,5)\n    scorer = DeepBiaffineScorer(4, 5, 6, 7)\n    print(scorer(x1, x2))\n'"
stanza/models/common/char_model.py,10,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_sequence, pad_packed_sequence, pack_padded_sequence, PackedSequence\n\nfrom stanza.models.common.packed_lstm import PackedLSTM\nfrom stanza.models.common.utils import tensor_unsort, unsort\nfrom stanza.models.common.dropout import SequenceUnitDropout\nfrom stanza.models.common.vocab import UNK_ID\nfrom stanza.models.pos.vocab import CharVocab\n\nclass CharacterModel(nn.Module):\n    def __init__(self, args, vocab, pad=False, bidirectional=False, attention=True):\n        super().__init__()\n        self.args = args\n        self.pad = pad\n        self.num_dir = 2 if bidirectional else 1\n        self.attn = attention\n\n        # char embeddings\n        self.char_emb = nn.Embedding(len(vocab[\'char\']), self.args[\'char_emb_dim\'], padding_idx=0)\n        if self.attn: \n            self.char_attn = nn.Linear(self.num_dir * self.args[\'char_hidden_dim\'], 1, bias=False)\n            self.char_attn.weight.data.zero_()\n\n        # modules\n        self.charlstm = PackedLSTM(self.args[\'char_emb_dim\'], self.args[\'char_hidden_dim\'], self.args[\'char_num_layers\'], batch_first=True, \\\n                dropout=0 if self.args[\'char_num_layers\'] == 1 else args[\'dropout\'], rec_dropout = self.args[\'char_rec_dropout\'], bidirectional=bidirectional)\n        self.charlstm_h_init = nn.Parameter(torch.zeros(self.num_dir * self.args[\'char_num_layers\'], 1, self.args[\'char_hidden_dim\']))\n        self.charlstm_c_init = nn.Parameter(torch.zeros(self.num_dir * self.args[\'char_num_layers\'], 1, self.args[\'char_hidden_dim\']))\n\n        self.dropout = nn.Dropout(args[\'dropout\'])\n\n    def forward(self, chars, chars_mask, word_orig_idx, sentlens, wordlens):\n        embs = self.dropout(self.char_emb(chars))\n        batch_size = embs.size(0)\n        embs = pack_padded_sequence(embs, wordlens, batch_first=True)\n        output = self.charlstm(embs, wordlens, hx=(\\\n                self.charlstm_h_init.expand(self.num_dir * self.args[\'char_num_layers\'], batch_size, self.args[\'char_hidden_dim\']).contiguous(), \\\n                self.charlstm_c_init.expand(self.num_dir * self.args[\'char_num_layers\'], batch_size, self.args[\'char_hidden_dim\']).contiguous()))\n         \n        # apply attention, otherwise take final states\n        if self.attn:\n            char_reps = output[0]\n            weights = torch.sigmoid(self.char_attn(self.dropout(char_reps.data)))\n            char_reps = PackedSequence(char_reps.data * weights, char_reps.batch_sizes)\n            char_reps, _ = pad_packed_sequence(char_reps, batch_first=True)\n            res = char_reps.sum(1)\n        else:\n            h, c = output[1]\n            res = h[-2:].transpose(0,1).contiguous().view(batch_size, -1)\n\n        # recover character order and word separation\n        res = tensor_unsort(res, word_orig_idx)\n        res = pack_sequence(res.split(sentlens))\n        if self.pad:\n            res = pad_packed_sequence(res, batch_first=True)[0]\n\n        return res\n\nclass CharacterLanguageModel(nn.Module):\n\n    def __init__(self, args, vocab, pad=False, is_forward_lm=True):\n        super().__init__()\n        self.args = args\n        self.vocab = vocab\n        self.is_forward_lm = is_forward_lm\n        self.pad = pad\n        self.finetune = True # always finetune unless otherwise specified\n\n        # char embeddings\n        self.char_emb = nn.Embedding(len(self.vocab[\'char\']), self.args[\'char_emb_dim\'], padding_idx=None) # we use space as padding, so padding_idx is not necessary\n        \n        # modules\n        self.charlstm = PackedLSTM(self.args[\'char_emb_dim\'], self.args[\'char_hidden_dim\'], self.args[\'char_num_layers\'], batch_first=True, \\\n                dropout=0 if self.args[\'char_num_layers\'] == 1 else args[\'char_dropout\'], rec_dropout = self.args[\'char_rec_dropout\'], bidirectional=False)\n        self.charlstm_h_init = nn.Parameter(torch.zeros(self.args[\'char_num_layers\'], 1, self.args[\'char_hidden_dim\']))\n        self.charlstm_c_init = nn.Parameter(torch.zeros(self.args[\'char_num_layers\'], 1, self.args[\'char_hidden_dim\']))\n\n        # decoder\n        self.decoder = nn.Linear(self.args[\'char_hidden_dim\'], len(self.vocab[\'char\']))\n        self.dropout = nn.Dropout(args[\'char_dropout\'])\n        self.char_dropout = SequenceUnitDropout(args.get(\'char_unit_dropout\', 0), UNK_ID)\n\n    def forward(self, chars, charlens, hidden=None):\n        chars = self.char_dropout(chars)\n        embs = self.dropout(self.char_emb(chars))\n        batch_size = embs.size(0)\n        embs = pack_padded_sequence(embs, charlens, batch_first=True)\n        if hidden is None: \n            hidden = (self.charlstm_h_init.expand(self.args[\'char_num_layers\'], batch_size, self.args[\'char_hidden_dim\']).contiguous(),\n                      self.charlstm_c_init.expand(self.args[\'char_num_layers\'], batch_size, self.args[\'char_hidden_dim\']).contiguous())\n        output, hidden = self.charlstm(embs, charlens, hx=hidden)\n        output = self.dropout(pad_packed_sequence(output, batch_first=True)[0])\n        decoded = self.decoder(output)\n        return output, hidden, decoded\n\n    def get_representation(self, chars, charoffsets, charlens, char_orig_idx):\n        with torch.no_grad():\n            output, _, _ = self.forward(chars, charlens)\n            res = [output[i, offsets] for i, offsets in enumerate(charoffsets)]\n            res = unsort(res, char_orig_idx)\n            res = pack_sequence(res)\n            if self.pad:\n                res = pad_packed_sequence(res, batch_first=True)[0]\n        return res\n    \n    def train(self, mode=True):\n        """"""\n        Override the default train() function, so that when self.finetune == False, the training mode \n        won\'t be impacted by the parent models\' status change.\n        """"""\n        if not mode: # eval() is always allowed, regardless of finetune status\n            super().train(mode)\n        else:\n            if self.finetune: # only set to training mode in finetune status\n                super().train(mode)\n\n    def save(self, filename):\n        state = {\n            \'vocab\': self.vocab[\'char\'].state_dict(),\n            \'args\': self.args,\n            \'state_dict\': self.state_dict(),\n            \'pad\': self.pad,\n            \'is_forward_lm\': self.is_forward_lm\n        }\n        torch.save(state, filename)\n\n    @classmethod\n    def load(cls, filename, finetune=False):\n        state = torch.load(filename, lambda storage, loc: storage)\n        vocab = {\'char\': CharVocab.load_state_dict(state[\'vocab\'])}\n        model = cls(state[\'args\'], vocab, state[\'pad\'], state[\'is_forward_lm\'])\n        model.load_state_dict(state[\'state_dict\'])\n        model.eval()\n        model.finetune = finetune # set finetune status\n        return model\n'"
stanza/models/common/chuliu_edmonds.py,0,"b'# Adapted from Tim\'s code here: https://github.com/tdozat/Parser-v3/blob/master/scripts/chuliu_edmonds.py\n\nimport numpy as np\n\ndef tarjan(tree):\n    """"""""""""\n\n    indices = -np.ones_like(tree)\n    lowlinks = -np.ones_like(tree)\n    onstack = np.zeros_like(tree, dtype=bool)\n    stack = list()\n    _index = [0]\n    cycles = []\n    #-------------------------------------------------------------\n    def strong_connect(i):\n        _index[0] += 1\n        index = _index[-1]\n        indices[i] = lowlinks[i] = index - 1\n        stack.append(i)\n        onstack[i] = True\n        dependents = np.where(np.equal(tree, i))[0]\n        for j in dependents:\n            if indices[j] == -1:\n                strong_connect(j)\n                lowlinks[i] = min(lowlinks[i], lowlinks[j])\n            elif onstack[j]:\n                lowlinks[i] = min(lowlinks[i], indices[j])\n\n        # There\'s a cycle!\n        if lowlinks[i] == indices[i]:\n            cycle = np.zeros_like(indices, dtype=bool)\n            while stack[-1] != i:\n                j = stack.pop()\n                onstack[j] = False\n                cycle[j] = True\n            stack.pop()\n            onstack[i] = False\n            cycle[i] = True\n            if cycle.sum() > 1:\n                cycles.append(cycle)\n        return\n    #-------------------------------------------------------------\n    for i in range(len(tree)):\n        if indices[i] == -1:\n            strong_connect(i)\n    return cycles\n\ndef chuliu_edmonds(scores):\n    """"""""""""\n\n    np.fill_diagonal(scores, -float(\'inf\')) # prevent self-loops\n    scores[0] = -float(\'inf\')\n    scores[0,0] = 0\n    tree = np.argmax(scores, axis=1)\n    cycles = tarjan(tree)\n    #print(scores)\n    #print(cycles)\n    if not cycles:\n        return tree\n    else:\n        # t = len(tree); c = len(cycle); n = len(noncycle)\n        # locations of cycle; (t) in [0,1]\n        cycle = cycles.pop()\n        # indices of cycle in original tree; (c) in t\n        cycle_locs = np.where(cycle)[0]\n        # heads of cycle in original tree; (c) in t\n        cycle_subtree = tree[cycle]\n        # scores of cycle in original tree; (c) in R\n        cycle_scores = scores[cycle, cycle_subtree]\n        # total score of cycle; () in R\n        cycle_score = cycle_scores.sum()\n\n        # locations of noncycle; (t) in [0,1]\n        noncycle = np.logical_not(cycle)\n        # indices of noncycle in original tree; (n) in t\n        noncycle_locs = np.where(noncycle)[0]\n        #print(cycle_locs, noncycle_locs)\n\n        # scores of cycle\'s potential heads; (c x n) - (c) + () -> (n x c) in R\n        metanode_head_scores = scores[cycle][:,noncycle] - cycle_scores[:,None] + cycle_score\n        # scores of cycle\'s potential dependents; (n x c) in R\n        metanode_dep_scores = scores[noncycle][:,cycle]\n        # best noncycle head for each cycle dependent; (n) in c\n        metanode_heads = np.argmax(metanode_head_scores, axis=0)\n        # best cycle head for each noncycle dependent; (n) in c\n        metanode_deps = np.argmax(metanode_dep_scores, axis=1)\n\n        # scores of noncycle graph; (n x n) in R\n        subscores = scores[noncycle][:,noncycle]\n        # pad to contracted graph; (n+1 x n+1) in R\n        subscores = np.pad(subscores, ( (0,1) , (0,1) ), \'constant\')\n        # set the contracted graph scores of cycle\'s potential heads; (c x n)[:, (n) in n] in R -> (n) in R\n        subscores[-1, :-1] = metanode_head_scores[metanode_heads, np.arange(len(noncycle_locs))]\n        # set the contracted graph scores of cycle\'s potential dependents; (n x c)[(n) in n] in R-> (n) in R\n        subscores[:-1,-1] = metanode_dep_scores[np.arange(len(noncycle_locs)), metanode_deps]\n\n        # MST with contraction; (n+1) in n+1\n        contracted_tree = chuliu_edmonds(subscores)\n        # head of the cycle; () in n\n        #print(contracted_tree)\n        cycle_head = contracted_tree[-1]\n        # fixed tree: (n) in n+1\n        contracted_tree = contracted_tree[:-1]\n        # initialize new tree; (t) in 0\n        new_tree = -np.ones_like(tree)\n        #print(0, new_tree)\n        # fixed tree with no heads coming from the cycle: (n) in [0,1]\n        contracted_subtree = contracted_tree < len(contracted_tree)\n        # add the nodes to the new tree (t)[(n)[(n) in [0,1]] in t] in t = (n)[(n)[(n) in [0,1]] in n] in t\n        new_tree[noncycle_locs[contracted_subtree]] = noncycle_locs[contracted_tree[contracted_subtree]]\n        #print(1, new_tree)\n        # fixed tree with heads coming from the cycle: (n) in [0,1]\n        contracted_subtree = np.logical_not(contracted_subtree)\n        # add the nodes to the tree (t)[(n)[(n) in [0,1]] in t] in t = (c)[(n)[(n) in [0,1]] in c] in t\n        new_tree[noncycle_locs[contracted_subtree]] = cycle_locs[metanode_deps[contracted_subtree]]\n        #print(2, new_tree)\n        # add the old cycle to the tree; (t)[(c) in t] in t = (t)[(c) in t] in t\n        new_tree[cycle_locs] = tree[cycle_locs]\n        #print(3, new_tree)\n        # root of the cycle; (n)[() in n] in c = () in c\n        cycle_root = metanode_heads[cycle_head]\n        # add the root of the cycle to the new tree; (t)[(c)[() in c] in t] = (c)[() in c]\n        new_tree[cycle_locs[cycle_root]] = noncycle_locs[cycle_head]\n        #print(4, new_tree)\n        return new_tree\n\n#===============================================================\ndef chuliu_edmonds_one_root(scores):\n    """"""""""""\n\n    scores = scores.astype(np.float64)\n    tree = chuliu_edmonds(scores)\n    roots_to_try = np.where(np.equal(tree[1:], 0))[0]+1\n    if len(roots_to_try) == 1:\n        return tree\n\n    #-------------------------------------------------------------\n    def set_root(scores, root):\n        root_score = scores[root,0]\n        scores = np.array(scores)\n        scores[1:,0] = -float(\'inf\')\n        scores[root] = -float(\'inf\')\n        scores[root,0] = 0\n        return scores, root_score\n    #-------------------------------------------------------------\n\n    best_score, best_tree = -np.inf, None # This is what\'s causing it to crash\n    for root in roots_to_try:\n        _scores, root_score = set_root(scores, root)\n        _tree = chuliu_edmonds(_scores)\n        tree_probs = _scores[np.arange(len(_scores)), _tree]\n        tree_score = (tree_probs).sum()+(root_score) if (tree_probs > -np.inf).all() else -np.inf\n        if tree_score > best_score:\n            best_score = tree_score\n            best_tree = _tree\n    try:\n        assert best_tree is not None\n    except:\n        with open(\'debug.log\', \'w\') as f:\n            f.write(\'{}: {}, {}\\n\'.format(tree, scores, roots_to_try))\n            f.write(\'{}: {}, {}, {}\\n\'.format(_tree, _scores, tree_probs, tree_score))\n        raise\n    return best_tree\n'"
stanza/models/common/constant.py,0,"b'""""""\nGlobal constants.\n""""""\n\nlcode2lang = {\n    ""af"": ""Afrikaans"",\n    ""grc"": ""Ancient_Greek"",\n    ""ar"": ""Arabic"",\n    ""hy"": ""Armenian"",\n    ""eu"": ""Basque"",\n    ""be"": ""Belarusian"",\n    ""br"": ""Breton"",\n    ""bg"": ""Bulgarian"",\n    ""bxr"": ""Buryat"",\n    ""ca"": ""Catalan"",\n    ""zh-hant"": ""Traditional_Chinese"",\n    ""lzh"": ""Classical_Chinese"",\n    ""cop"": ""Coptic"",\n    ""hr"": ""Croatian"",\n    ""cs"": ""Czech"",\n    ""da"": ""Danish"",\n    ""nl"": ""Dutch"",\n    ""en"": ""English"",\n    ""et"": ""Estonian"",\n    ""fo"": ""Faroese"",\n    ""fi"": ""Finnish"",\n    ""fr"": ""French"",\n    ""gl"": ""Galician"",\n    ""de"": ""German"",\n    ""got"": ""Gothic"",\n    ""el"": ""Greek"",\n    ""he"": ""Hebrew"",\n    ""hi"": ""Hindi"",\n    ""hu"": ""Hungarian"",\n    ""id"": ""Indonesian"",\n    ""ga"": ""Irish"",\n    ""it"": ""Italian"",\n    ""ja"": ""Japanese"",\n    ""kk"": ""Kazakh"",\n    ""ko"": ""Korean"",\n    ""kmr"": ""Kurmanji"",\n    ""lt"": ""Lithuanian"",\n    ""olo"": ""Livvi"",\n    ""la"": ""Latin"",\n    ""lv"": ""Latvian"",\n    ""mt"": ""Maltese"",\n    ""mr"": ""Marathi"",\n    ""pcm"": ""Naija"",\n    ""sme"": ""North_Sami"",\n    ""nb"": ""Norwegian_Bokmaal"",\n    ""nn"": ""Norwegian_Nynorsk"",\n    ""cu"": ""Old_Church_Slavonic"",\n    ""fro"": ""Old_French"",\n    ""orv"": ""Old_Russian"",\n    ""fa"": ""Persian"",\n    ""pl"": ""Polish"",\n    ""pt"": ""Portuguese"",\n    ""ro"": ""Romanian"",\n    ""ru"": ""Russian"",\n    ""gd"": ""Scottish_Gaelic"",\n    ""sr"": ""Serbian"",\n    ""zh-hans"": ""Simplified_Chinese"",\n    ""sk"": ""Slovak"",\n    ""sl"": ""Slovenian"",\n    ""es"": ""Spanish"",\n    ""sv"": ""Swedish"",\n    ""swl"": ""Swedish_Sign_Language"",\n    ""ta"": ""Tamil"",\n    ""te"": ""Telugu"",\n    ""th"": ""Thai"",\n    ""tr"": ""Turkish"",\n    ""uk"": ""Ukrainian"",\n    ""hsb"": ""Upper_Sorbian"",\n    ""ur"": ""Urdu"",\n    ""ug"": ""Uyghur"",\n    ""vi"": ""Vietnamese"",\n    ""wo"": ""Wolof""\n}\n\nlang2lcode = {lcode2lang[k]: k for k in lcode2lang}\nlanglower2lcode = {lcode2lang[k].lower(): k.lower() for k in lcode2lang}\n\n# additional useful code to language mapping\n# added after dict invert to avoid conflict\nlcode2lang[\'nb\'] = \'Norwegian\' # Norwegian Bokmall mapped to default norwegian\n'"
stanza/models/common/crf.py,11,"b'""""""\nCRF loss and viterbi decoding.\n""""""\n\nimport math\nfrom numbers import Number\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.init as init\n\nclass CRFLoss(nn.Module):\n    """"""\n    Calculate log-space crf loss, given unary potentials, a transition matrix\n    and gold tag sequences.\n    """"""\n    def __init__(self, num_tag, batch_average=True):\n        super().__init__()\n        self._transitions = nn.Parameter(torch.zeros(num_tag, num_tag))\n        self._batch_average = batch_average # if not batch average, average on all tokens\n\n    def forward(self, inputs, masks, tag_indices):\n        """"""\n        inputs: batch_size x seq_len x num_tags\n        masks: batch_size x seq_len\n        tag_indices: batch_size x seq_len\n        \n        @return:\n            loss: CRF negative log likelihood on all instances.\n            transitions: the transition matrix\n        """"""\n        # TODO: handle <start> and <end> tags\n        self.bs, self.sl, self.nc = inputs.size()\n        unary_scores = self.crf_unary_score(inputs, masks, tag_indices)\n        binary_scores = self.crf_binary_score(inputs, masks, tag_indices)\n        log_norm = self.crf_log_norm(inputs, masks, tag_indices)\n        log_likelihood = unary_scores + binary_scores - log_norm # batch_size\n        loss = torch.sum(-log_likelihood)\n        if self._batch_average:\n            loss = loss / self.bs\n        else:\n            total = masks.eq(0).sum()\n            loss = loss / (total + 1e-8)\n        return loss, self._transitions\n\n    def crf_unary_score(self, inputs, masks, tag_indices):\n        """"""\n        @return:\n            unary_scores: batch_size\n        """"""\n        flat_inputs = inputs.view(self.bs, -1)\n        flat_tag_indices = tag_indices + \\\n                set_cuda(torch.arange(self.sl).long().unsqueeze(0) * self.nc, tag_indices.is_cuda)\n        unary_scores = torch.gather(flat_inputs, 1, flat_tag_indices).view(self.bs, -1)\n        unary_scores.masked_fill_(masks, 0)\n        return unary_scores.sum(dim=1)\n    \n    def crf_binary_score(self, inputs, masks, tag_indices):\n        """"""\n        @return:\n            binary_scores: batch_size\n        """"""\n        # get number of transitions\n        nt = tag_indices.size(-1) - 1\n        start_indices = tag_indices[:, :nt]\n        end_indices = tag_indices[:, 1:]\n        # flat matrices\n        flat_transition_indices = start_indices * self.nc + end_indices\n        flat_transition_indices = flat_transition_indices.view(-1)\n        flat_transition_matrix = self._transitions.view(-1)\n        binary_scores = torch.gather(flat_transition_matrix, 0, flat_transition_indices)\\\n                .view(self.bs, -1)\n        score_masks = masks[:, 1:]\n        binary_scores.masked_fill_(score_masks, 0)\n        return binary_scores.sum(dim=1)\n\n    def crf_log_norm(self, inputs, masks, tag_indices):\n        """"""\n        Calculate the CRF partition in log space for each instance, following:\n            http://www.cs.columbia.edu/~mcollins/fb.pdf\n        @return:\n            log_norm: batch_size\n        """"""\n        start_inputs = inputs[:,0,:] # bs x nc\n        rest_inputs = inputs[:,1:,:]\n        rest_masks = masks[:,1:]\n        alphas = start_inputs # bs x nc\n        trans = self._transitions.unsqueeze(0) # 1 x nc x nc\n        # accumulate alphas in log space\n        for i in range(rest_inputs.size(1)):\n            transition_scores = alphas.unsqueeze(2) + trans # bs x nc x nc\n            new_alphas = rest_inputs[:,i,:] + log_sum_exp(transition_scores, dim=1)\n            m = rest_masks[:,i].unsqueeze(1).expand_as(new_alphas) # bs x nc, 1 for padding idx\n            # apply masks\n            new_alphas.masked_scatter_(m, alphas.masked_select(m))\n            alphas = new_alphas\n        log_norm = log_sum_exp(alphas, dim=1)\n        return log_norm\n\ndef viterbi_decode(scores, transition_params):\n    """"""\n    Decode a tag sequence with viterbi algorithm.\n    scores: seq_len x num_tags (numpy array)\n    transition_params: num_tags x num_tags (numpy array)\n    @return:\n        viterbi: a list of tag ids with highest score\n        viterbi_score: the highest score\n    """"""\n    trellis = np.zeros_like(scores)\n    backpointers = np.zeros_like(scores, dtype=np.int32)\n    trellis[0] = scores[0]\n\n    for t in range(1, scores.shape[0]):\n        v = np.expand_dims(trellis[t-1], 1) + transition_params\n        trellis[t] = scores[t] + np.max(v, 0)\n        backpointers[t] = np.argmax(v, 0)\n\n    viterbi = [np.argmax(trellis[-1])]\n    for bp in reversed(backpointers[1:]):\n        viterbi.append(bp[viterbi[-1]])\n    viterbi.reverse()\n    viterbi_score = np.max(trellis[-1])\n    return viterbi, viterbi_score\n\ndef log_sum_exp(value, dim=None, keepdim=False):\n    """"""Numerically stable implementation of the operation\n    value.exp().sum(dim, keepdim).log()\n    """"""\n    if dim is not None:\n        m, _ = torch.max(value, dim=dim, keepdim=True)\n        value0 = value - m\n        if keepdim is False:\n            m = m.squeeze(dim)\n        return m + torch.log(torch.sum(torch.exp(value0),\n                                       dim=dim, keepdim=keepdim))\n    else:\n        m = torch.max(value)\n        sum_exp = torch.sum(torch.exp(value - m))\n        if isinstance(sum_exp, Number):\n            return m + math.log(sum_exp)\n        else:\n            return m + torch.log(sum_exp)\n\ndef set_cuda(var, cuda):\n    if cuda:\n        return var.cuda()\n    return var\n'"
stanza/models/common/data.py,4,"b'""""""\nUtility functions for data transformations.\n""""""\n\nimport torch\n\nimport stanza.models.common.seq2seq_constant as constant\n\ndef map_to_ids(tokens, vocab):\n    ids = [vocab[t] if t in vocab else constant.UNK_ID for t in tokens]\n    return ids\n\ndef get_long_tensor(tokens_list, batch_size, pad_id=constant.PAD_ID):\n    """""" Convert (list of )+ tokens to a padded LongTensor. """"""\n    sizes = []\n    x = tokens_list\n    while isinstance(x[0], list):\n        sizes.append(max(len(y) for y in x))\n        x = [z for y in x for z in y]\n    tokens = torch.LongTensor(batch_size, *sizes).fill_(pad_id)\n    for i, s in enumerate(tokens_list):\n        tokens[i, :len(s)] = torch.LongTensor(s)\n    return tokens\n\ndef get_float_tensor(features_list, batch_size):\n    if features_list is None or features_list[0] is None:\n        return None\n    seq_len = max(len(x) for x in features_list)\n    feature_len = len(features_list[0][0])\n    features = torch.FloatTensor(batch_size, seq_len, feature_len).zero_()\n    for i,f in enumerate(features_list):\n        features[i,:len(f),:] = torch.FloatTensor(f)\n    return features\n\ndef sort_all(batch, lens):\n    """""" Sort all fields by descending order of lens, and return the original indices. """"""\n    unsorted_all = [lens] + [range(len(lens))] + list(batch)\n    sorted_all = [list(t) for t in zip(*sorted(zip(*unsorted_all), reverse=True))]\n    return sorted_all[2:], sorted_all[1]\n'"
stanza/models/common/doc.py,0,"b'""""""\nBasic data structures\n""""""\n\nimport io\nimport re\nimport json\n\nfrom stanza.models.ner.utils import decode_from_bioes\n\nmulti_word_token_id = re.compile(r""([0-9]+)-([0-9]+)"")\nmulti_word_token_misc = re.compile(r"".*MWT=Yes.*"")\n\nID = \'id\'\nTEXT = \'text\'\nLEMMA = \'lemma\'\nUPOS = \'upos\'\nXPOS = \'xpos\'\nFEATS = \'feats\'\nHEAD = \'head\'\nDEPREL = \'deprel\'\nDEPS = \'deps\'\nMISC = \'misc\'\nNER = \'ner\'\nSTART_CHAR = \'start_char\'\nEND_CHAR = \'end_char\'\nTYPE = \'type\'\n\nclass Document:\n    """""" A document class that stores attributes of a document and carries a list of sentences.\n    """"""\n\n    def __init__(self, sentences, text=None):\n        """""" Construct a document given a list of sentences in the form of lists of CoNLL-U dicts.\n\n        Args:\n            sentences: a list of sentences, which being a list of token entry, in the form of a CoNLL-U dict.\n            text: the raw text of the document.\n        """"""\n        self._sentences = []\n        self._text = None\n        self._num_tokens = 0\n        self._num_words = 0\n\n        self.text = text\n        self._process_sentences(sentences)\n        self._ents = []\n\n    @property\n    def text(self):\n        """""" Access the raw text for this document. """"""\n        return self._text\n\n    @text.setter\n    def text(self, value):\n        """""" Set the raw text for this document. """"""\n        self._text = value\n\n    @property\n    def sentences(self):\n        """""" Access the list of sentences for this document. """"""\n        return self._sentences\n\n    @sentences.setter\n    def sentences(self, value):\n        """""" Set the list of tokens for this document. """"""\n        self._sentences = value\n    \n    @property\n    def num_tokens(self):\n        """""" Access the number of tokens for this document. """"""\n        return self._num_tokens\n\n    @num_tokens.setter\n    def num_tokens(self, value):\n        """""" Set the number of tokens for this document. """"""\n        self._num_tokens = value\n\n    @property\n    def num_words(self):\n        """""" Access the number of words for this document. """"""\n        return self._num_words\n\n    @num_words.setter\n    def num_words(self, value):\n        """""" Set the number of words for this document. """"""\n        self._num_words = value\n\n    @property\n    def ents(self):\n        """""" Access the list of entities in this document. """"""\n        return self._ents\n\n    @ents.setter\n    def ents(self, value):\n        """""" Set the list of entities in this document. """"""\n        self._ents = value\n\n    @property\n    def entities(self):\n        """""" Access the list of entities. This is just an alias of `ents`. """"""\n        return self._ents\n\n    @entities.setter\n    def entities(self, value):\n        """""" Set the list of entities in this document. """"""\n        self._ents = value\n\n    def _process_sentences(self, sentences):\n        self.sentences = []\n        for tokens in sentences:\n            self.sentences.append(Sentence(tokens, doc=self))\n            begin_idx, end_idx = self.sentences[-1].tokens[0].start_char, self.sentences[-1].tokens[-1].end_char\n            if all([self.text is not None, begin_idx is not None, end_idx is not None]): self.sentences[-1].text = self.text[begin_idx: end_idx]\n\n        self.num_tokens = sum([len(sentence.tokens) for sentence in self.sentences])\n        self.num_words = sum([len(sentence.words) for sentence in self.sentences])\n\n    def get(self, fields, as_sentences=False, from_token=False):\n        """""" Get fields from a list of field names. If only one field name is provided, return a list\n        of that field; if more than one, return a list of list. Note that all returned fields are after\n        multi-word expansion.\n\n        Args:\n            fields: name of the fields as a list\n            as_sentences: if True, return the fields as a list of sentences; otherwise as a whole list\n            from_token: if True, get the fields from Token; otherwise from Word\n        \n        Returns:\n            All requested fields.\n        """"""\n        assert isinstance(fields, list), ""Must provide field names as a list.""\n        assert len(fields) >= 1, ""Must have at least one field.""\n\n        results = []\n        for sentence in self.sentences:\n            cursent = []\n            # decide word or token\n            if from_token:\n                units = sentence.tokens\n            else:\n                units = sentence.words\n            for unit in units:\n                if len(fields) == 1:\n                    cursent += [getattr(unit, fields[0])]\n                else:\n                    cursent += [[getattr(unit, field) for field in fields]]\n\n            # decide whether append the results as a sentence or a whole list\n            if as_sentences:\n                results.append(cursent)\n            else:\n                results += cursent\n        return results\n\n    def set(self, fields, contents, to_token=False):\n        """""" Set fields based on contents. If only one field (singleton list) is provided, then a list\n        of content will be expected; otherwise a list of list of contents will be expected.\n\n        Args:\n            fields: name of the fields as a list\n            contents: field values to set; total length should be equal to number of words/tokens\n            to_token: if True, set field values to tokens; otherwise to words\n        """"""\n        assert isinstance(fields, list), ""Must provide field names as a list.""\n        assert isinstance(contents, list), ""Must provide contents as a list (one item per line).""\n        assert len(fields) >= 1, ""Must have at least one field.""\n\n        assert (to_token and self.num_tokens == len(contents)) or self.num_words == len(contents), \\\n            ""Contents must have the same number as the original file.""\n\n        cidx = 0\n        for sentence in self.sentences:\n            # decide word or token\n            if to_token:\n                units = sentence.tokens\n            else:\n                units = sentence.words\n            for unit in units:\n                if len(fields) == 1:\n                    setattr(unit, fields[0], contents[cidx])\n                else:\n                    for field, content in zip(fields, contents[cidx]):\n                        setattr(unit, field, content)\n                cidx += 1\n        return\n\n    def set_mwt_expansions(self, expansions):\n        """""" Extend the multi-word tokens annotated by tokenizer. A list of list of expansions\n        will be expected for each multi-word token.\n        """"""\n        idx_e = 0\n        for sentence in self.sentences:\n            idx_w = 0\n            for token in sentence.tokens:\n                idx_w += 1\n                m = multi_word_token_id.match(token.id)\n                n = multi_word_token_misc.match(token.misc) if token.misc is not None else None\n                if not m and not n:\n                    for word in token.words:\n                        word.id = str(idx_w)\n                        word.head, word.deprel = None, None # delete dependency information\n                else:\n                    expanded = [x for x in expansions[idx_e].split(\' \') if len(x) > 0]\n                    idx_e += 1\n                    idx_w_end = idx_w + len(expanded) - 1\n                    token.misc = None if token.misc == \'MWT=Yes\' else \'|\'.join([x for x in token.misc.split(\'|\') if x != \'MWT=Yes\'])\n                    token.id = f\'{idx_w}-{idx_w_end}\'\n                    token.words = []\n                    for i, e_word in enumerate(expanded):\n                        token.words.append(Word({ID: str(idx_w + i), TEXT: e_word}))\n                    idx_w = idx_w_end\n            sentence._process_tokens(sentence.to_dict()) # reprocess to update sentence.words and sentence.dependencies\n        self._process_sentences(self.to_dict()) # reprocess to update number of words\n        assert idx_e == len(expansions), ""{} {}"".format(idx_e, len(expansions))\n        return\n\n    def get_mwt_expansions(self, evaluation=False):\n        """""" Get the multi-word tokens. For training, return a list of\n        (multi-word token, extended multi-word token); otherwise, return a list of\n        multi-word token only.\n        """"""\n        expansions = []\n        for sentence in self.sentences:\n            for token in sentence.tokens:\n                m = multi_word_token_id.match(token.id)\n                n = multi_word_token_misc.match(token.misc) if token.misc is not None else None\n                if m or n:\n                    src = token.text\n                    dst = \' \'.join([word.text for word in token.words])\n                    expansions.append([src, dst])\n        if evaluation: expansions = [e[0] for e in expansions]\n        return expansions\n\n    def build_ents(self):\n        """""" Build the list of entities by iterating over all words. Return all entities as a list. """"""\n        self.ents = []\n        for s in self.sentences:\n            s_ents = s.build_ents()\n            self.ents += s_ents\n        return self.ents\n\n    def iter_words(self):\n        """""" An iterator that returns all of the words in this Document. """"""\n        for s in self.sentences:\n            yield from s.words\n\n    def iter_tokens(self):\n        """""" An iterator that returns all of the tokens in this Document. """"""\n        for s in self.sentences:\n            yield from s.tokens\n\n    def to_dict(self):\n        """""" Dumps the whole document into a list of list of dictionary for each token in each sentence in the doc.\n        """"""\n        return [sentence.to_dict() for sentence in self.sentences]\n\n    def __repr__(self):\n        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)\n\n\nclass Sentence:\n    """""" A sentence class that stores attributes of a sentence and carries a list of tokens.\n    """"""\n\n    def __init__(self, tokens, doc=None):\n        """""" Construct a setence given a list of tokens in the form of CoNLL-U dicts.\n        """"""\n        self._tokens = []\n        self._words = []\n        self._dependencies = []\n        self._text = None\n        self._ents = []\n        self._doc = doc\n\n        self._process_tokens(tokens)\n\n    def _process_tokens(self, tokens):\n        st, en = -1, -1\n        self.tokens, self.words = [], []\n        for i, entry in enumerate(tokens):\n            if ID not in entry: # manually set a 1-based id for word if not exist\n                entry[ID] = str(i+1)\n            m = multi_word_token_id.match(entry.get(ID))\n            n = multi_word_token_misc.match(entry.get(MISC)) if entry.get(MISC, None) is not None else None\n            if m or n: # if this token is a multi-word token\n                if m: st, en = int(m.group(1)), int(m.group(2))\n                self.tokens.append(Token(entry))\n            else: # else this token is a word\n                new_word = Word(entry)\n                self.words.append(new_word)\n                idx = int(entry.get(ID))\n                if idx <= en:\n                    self.tokens[-1].words.append(new_word)\n                else:\n                    self.tokens.append(Token(entry, words=[new_word]))\n                new_word.parent = self.tokens[-1]\n\n        # check if there is dependency info\n        is_complete_dependencies = all([word.head is not None and word.deprel is not None for word in self.words])\n        is_complete_words = (len(self.words) >= len(self.tokens)) and (len(self.words) == int(self.words[-1].id))\n        if is_complete_dependencies and is_complete_words: self.build_dependencies()\n    \n    @property\n    def doc(self):\n        """""" Access the parent doc of this span. """"""\n        return self._doc\n\n    @doc.setter\n    def doc(self, value):\n        """""" Set the parent doc of this span. """"""\n        self._doc = value\n\n    @property\n    def text(self):\n        """""" Access the raw text for this sentence. """"""\n        return self._text\n\n    @text.setter\n    def text(self, value):\n        """""" Set the raw text for this sentence. """"""\n        self._text = value\n\n    @property\n    def dependencies(self):\n        """""" Access list of dependencies for this sentence. """"""\n        return self._dependencies\n\n    @dependencies.setter\n    def dependencies(self, value):\n        """""" Set the list of dependencies for this sentence. """"""\n        self._dependencies = value\n\n    @property\n    def tokens(self):\n        """""" Access the list of tokens for this sentence. """"""\n        return self._tokens\n\n    @tokens.setter\n    def tokens(self, value):\n        """""" Set the list of tokens for this sentence. """"""\n        self._tokens = value\n\n    @property\n    def words(self):\n        """""" Access the list of words for this sentence. """"""\n        return self._words\n\n    @words.setter\n    def words(self, value):\n        """""" Set the list of words for this sentence. """"""\n        self._words = value\n    \n    @property\n    def ents(self):\n        """""" Access the list of entities in this sentence. """"""\n        return self._ents\n\n    @ents.setter\n    def ents(self, value):\n        """""" Set the list of entities in this sentence. """"""\n        self._ents = value\n\n    @property\n    def entities(self):\n        """""" Access the list of entities. This is just an alias of `ents`. """"""\n        return self._ents\n\n    @entities.setter\n    def entities(self, value):\n        """""" Set the list of entities in this sentence. """"""\n        self._ents = value\n    \n    def build_ents(self):\n        """""" Build the list of entities by iterating over all tokens. Return all entities as a list. \n        \n        Note that unlike other attributes, since NER requires raw text, the actual tagging are always\n        performed at and attached to the `Token`s, instead of `Word`s.\n        """"""\n        self.ents = []\n        tags = [w.ner for w in self.tokens]\n        decoded = decode_from_bioes(tags)\n        for e in decoded:\n            ent_tokens = self.tokens[e[\'start\']:e[\'end\']+1]\n            self.ents.append(Span(tokens=ent_tokens, type=e[\'type\'], doc=self.doc, sent=self))\n        return self.ents\n\n    def build_dependencies(self):\n        """""" Build the dependency graph for this sentence. Each dependency graph entry is\n        a list of (head, deprel, word).\n        """"""\n        self.dependencies = []\n        for word in self.words:\n            if int(word.head) == 0:\n                # make a word for the ROOT\n                word_entry = {ID: ""0"", TEXT: ""ROOT""}\n                head = Word(word_entry)\n            else:\n                # id is index in words list + 1\n                head = self.words[int(word.head) - 1]\n                assert(int(word.head) == int(head.id))\n            self.dependencies.append((head, word.deprel, word))\n\n    def print_dependencies(self, file=None):\n        """""" Print the dependencies for this sentence. """"""\n        for dep_edge in self.dependencies:\n            print((dep_edge[2].text, dep_edge[0].id, dep_edge[1]), file=file)\n\n    def dependencies_string(self):\n        """""" Dump the dependencies for this sentence into string. """"""\n        dep_string = io.StringIO()\n        self.print_dependencies(file=dep_string)\n        return dep_string.getvalue().strip()\n\n    def print_tokens(self, file=None):\n        """""" Print the tokens for this sentence. """"""\n        for tok in self.tokens:\n            print(tok.pretty_print(), file=file)\n\n    def tokens_string(self):\n        """""" Dump the tokens for this sentence into string. """"""\n        toks_string = io.StringIO()\n        self.print_tokens(file=toks_string)\n        return toks_string.getvalue().strip()\n\n    def print_words(self, file=None):\n        """""" Print the words for this sentence. """"""\n        for word in self.words:\n            print(word.pretty_print(), file=file)\n\n    def words_string(self):\n        """""" Dump the words for this sentence into string. """"""\n        wrds_string = io.StringIO()\n        self.print_words(file=wrds_string)\n        return wrds_string.getvalue().strip()\n\n    def to_dict(self):\n        """""" Dumps the sentence into a list of dictionary for each token in the sentence.\n        """"""\n        ret = []\n        for token in self.tokens:\n            ret += token.to_dict()\n        return ret\n\n    def __repr__(self):\n        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)\n\n\nclass Token:\n    """""" A token class that stores attributes of a token and carries a list of words. A token corresponds to a unit in the raw\n    text. In some languages such as English, a token has a one-to-one mapping to a word, while in other languages such as French,\n    a (multi-word) token might be expanded into multiple words that carry syntactic annotations.\n    """"""\n\n    def __init__(self, token_entry, words=None):\n        """""" Construct a token given a dictionary format token entry. Optionally link itself to the corresponding words.\n        """"""\n        assert token_entry.get(ID) and token_entry.get(TEXT), \'id and text should be included for the token\'\n        self._id, self._text, self._misc, self._words, self._start_char, self._end_char, self._ner = [None] * 7\n\n        self.id = token_entry.get(ID)\n        self.text = token_entry.get(TEXT)\n        self.misc = token_entry.get(MISC, None)\n        self.ner = token_entry.get(NER, None)\n        self.words = words if words is not None else []\n\n        if self.misc is not None:\n            self.init_from_misc()\n\n    def init_from_misc(self):\n        """""" Create attributes by parsing from the `misc` field.\n        """"""\n        for item in self._misc.split(\'|\'):\n            key_value = item.split(\'=\', 1)\n            if len(key_value) == 1: continue # some key_value can not be splited\n            key, value = key_value\n            if key in [START_CHAR, END_CHAR]:\n                value = int(value)\n            # set attribute\n            attr = f\'_{key}\'\n            if hasattr(self, attr):\n                setattr(self, attr, value)\n\n    @property\n    def id(self):\n        """""" Access the index of this token. """"""\n        return self._id\n\n    @id.setter\n    def id(self, value):\n        """""" Set the token\'s id value. """"""\n        self._id = value\n\n    @property\n    def text(self):\n        """""" Access the text of this token. Example: \'The\' """"""\n        return self._text\n\n    @text.setter\n    def text(self, value):\n        """""" Set the token\'s text value. Example: \'The\' """"""\n        self._text = value\n\n    @property\n    def misc(self):\n        """""" Access the miscellaneousness of this token. """"""\n        return self._misc\n\n    @misc.setter\n    def misc(self, value):\n        """""" Set the token\'s miscellaneousness value. """"""\n        self._misc = value if self._is_null(value) == False else None\n\n    @property\n    def words(self):\n        """""" Access the list of syntactic words underlying this token. """"""\n        return self._words\n\n    @words.setter\n    def words(self, value):\n        """""" Set this token\'s list of underlying syntactic words. """"""\n        self._words = value\n        for w in self._words:\n            w.parent = self\n\n    @property\n    def start_char(self):\n        """""" Access the start character index for this token in the raw text. """"""\n        return self._start_char\n\n    @property\n    def end_char(self):\n        """""" Access the end character index for this token in the raw text. """"""\n        return self._end_char\n    \n    @property\n    def ner(self):\n        """""" Access the NER tag of this token. Example: \'B-ORG\'""""""\n        return self._ner\n\n    @ner.setter\n    def ner(self, value):\n        """""" Set the token\'s NER tag. Example: \'B-ORG\'""""""\n        self._ner = value if self._is_null(value) == False else None\n\n    def __repr__(self):\n        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)\n\n    def to_dict(self, fields=[ID, TEXT, NER, MISC]):\n        """""" Dumps the token into a list of dictionary for this token with its extended words\n        if the token is a multi-word token.\n        """"""\n        ret = []\n        if multi_word_token_id.match(self.id):\n            token_dict = {}\n            for field in fields:\n                if getattr(self, field) is not None:\n                    token_dict[field] = getattr(self, field)\n            ret.append(token_dict)\n        for word in self.words:\n            ret.append(word.to_dict())\n        return ret\n\n    def pretty_print(self):\n        """""" Print this token with its extended words in one line. """"""\n        return f""<{self.__class__.__name__} id={self.id};words=[{\', \'.join([word.pretty_print() for word in self.words])}]>""\n\n    def _is_null(self, value):\n        return (value is None) or (value == \'_\')\n\nclass Word:\n    """""" A word class that stores attributes of a word.\n    """"""\n\n    def __init__(self, word_entry):\n        """""" Construct a word given a dictionary format word entry.\n        """"""\n        assert word_entry.get(ID) and word_entry.get(TEXT), \'id and text should be included for the word. {}\'.format(word_entry)\n        self._id, self._text, self._lemma, self._upos, self._xpos, self._feats, self._head, self._deprel, self._deps, \\\n            self._misc, self._parent = [None] * 11\n\n        self.id = word_entry.get(ID)\n        self.text = word_entry.get(TEXT)\n        self.lemma = word_entry.get(LEMMA, None)\n        self.upos = word_entry.get(UPOS, None)\n        self.xpos = word_entry.get(XPOS, None)\n        self.feats = word_entry.get(FEATS, None)\n        self.head = word_entry.get(HEAD, None)\n        self.deprel = word_entry.get(DEPREL, None)\n        self.deps = word_entry.get(DEPS, None)\n        self.misc = word_entry.get(MISC, None)\n\n        if self.misc is not None:\n            self.init_from_misc()\n\n    def init_from_misc(self):\n        """""" Create attributes by parsing from the `misc` field.\n        """"""\n        for item in self._misc.split(\'|\'):\n            key_value = item.split(\'=\', 1)\n            if len(key_value) == 1: continue # some key_value can not be splited\n            key, value = key_value\n            # set attribute\n            attr = f\'_{key}\'\n            if hasattr(self, attr):\n                setattr(self, attr, value)\n\n    @property\n    def id(self):\n        """""" Access the index of this word. """"""\n        return self._id\n\n    @id.setter\n    def id(self, value):\n        """""" Set the word\'s index value. """"""\n        self._id = value\n\n    @property\n    def text(self):\n        """""" Access the text of this word. Example: \'The\'""""""\n        return self._text\n\n    @text.setter\n    def text(self, value):\n        """""" Set the word\'s text value. Example: \'The\'""""""\n        self._text = value\n\n    @property\n    def lemma(self):\n        """""" Access the lemma of this word. """"""\n        return self._lemma\n\n    @lemma.setter\n    def lemma(self, value):\n        """""" Set the word\'s lemma value. """"""\n        self._lemma = value if self._is_null(value) == False or self._text == \'_\' else None\n\n    @property\n    def upos(self):\n        """""" Access the universal part-of-speech of this word. Example: \'NOUN\'""""""\n        return self._upos\n\n    @upos.setter\n    def upos(self, value):\n        """""" Set the word\'s universal part-of-speech value. Example: \'NOUN\'""""""\n        self._upos = value if self._is_null(value) == False else None\n\n    @property\n    def xpos(self):\n        """""" Access the treebank-specific part-of-speech of this word. Example: \'NNP\'""""""\n        return self._xpos\n\n    @xpos.setter\n    def xpos(self, value):\n        """""" Set the word\'s treebank-specific part-of-speech value. Example: \'NNP\'""""""\n        self._xpos = value if self._is_null(value) == False else None\n\n    @property\n    def feats(self):\n        """""" Access the morphological features of this word. Example: \'Gender=Fem\'""""""\n        return self._feats\n\n    @feats.setter\n    def feats(self, value):\n        """""" Set this word\'s morphological features. Example: \'Gender=Fem\'""""""\n        self._feats = value if self._is_null(value) == False else None\n\n    @property\n    def head(self):\n        """""" Access the id of the governer of this word. """"""\n        return self._head\n\n    @head.setter\n    def head(self, value):\n        """""" Set the word\'s governor id value. """"""\n        self._head = int(value) if self._is_null(value) == False else None\n\n    @property\n    def deprel(self):\n        """""" Access the dependency relation of this word. Example: \'nmod\'""""""\n        return self._deprel\n\n    @deprel.setter\n    def deprel(self, value):\n        """""" Set the word\'s dependency relation value. Example: \'nmod\'""""""\n        self._deprel = value if self._is_null(value) == False else None\n\n    @property\n    def deps(self):\n        """""" Access the dependencies of this word. """"""\n        return self._deps\n\n    @deps.setter\n    def deps(self, value):\n        """""" Set the word\'s dependencies value. """"""\n        self._deps = value if self._is_null(value) == False else None\n\n    @property\n    def misc(self):\n        """""" Access the miscellaneousness of this word. """"""\n        return self._misc\n\n    @misc.setter\n    def misc(self, value):\n        """""" Set the word\'s miscellaneousness value. """"""\n        self._misc = value if self._is_null(value) == False else None\n\n    @property\n    def parent(self):\n        """""" Access the parent token of this word. In the case of a multi-word token, a token can be the parent of\n        multiple words. Note that this should return a reference to the parent token object.\n        """"""\n        return self._parent\n\n    @parent.setter\n    def parent(self, value):\n        """""" Set this word\'s parent token. In the case of a multi-word token, a token can be the parent of\n        multiple words. Note that value here should be a reference to the parent token object.\n        """"""\n        self._parent = value\n\n    @property\n    def pos(self):\n        """""" Access the universal part-of-speech of this word. Example: \'NOUN\'""""""\n        return self._upos\n\n    @pos.setter\n    def pos(self, value):\n        """""" Set the word\'s universal part-of-speech value. Example: \'NOUN\'""""""\n        self._upos = value if self._is_null(value) == False else None\n\n    def __repr__(self):\n        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)\n\n    def to_dict(self, fields=[ID, TEXT, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL, DEPS, MISC]):\n        """""" Dumps the word into a dictionary.\n        """"""\n        word_dict = {}\n        for field in fields:\n            if getattr(self, field) is not None:\n                word_dict[field] = getattr(self, field)\n        return word_dict\n\n    def pretty_print(self):\n        """""" Print the word in one line. """"""\n        features = [ID, TEXT, LEMMA, UPOS, XPOS, FEATS, HEAD, DEPREL]\n        feature_str = "";"".join([""{}={}"".format(k, getattr(self, k)) for k in features if getattr(self, k) is not None])\n        return f""<{self.__class__.__name__} {feature_str}>""\n\n    def _is_null(self, value):\n        return (value is None) or (value == \'_\')\n\n\nclass Span:\n    """""" A span class that stores attributes of a textual span. A span can be typed.\n    A range of objects (e.g., entity mentions) can be represented as spans.\n    """"""\n\n    def __init__(self, span_entry=None, tokens=None, type=None, doc=None, sent=None):\n        """""" Construct a span given a span entry or a list of tokens. A valid reference to a doc\n        must be provided to construct a span (otherwise the text of the span cannot be initialized).\n        """"""\n        assert span_entry is not None or (tokens is not None and type is not None), \\\n                \'Either a span_entry or a token list needs to be provided to construct a span.\'\n        assert doc is not None, \'A parent doc must be provided to construct a span.\'\n        self._text, self._type, self._start_char, self._end_char = [None] * 4\n        self._tokens = []\n        self._words = []\n        self._doc = doc\n        self._sent = sent\n\n        if span_entry is not None:\n            self.init_from_entry(span_entry)\n\n        if tokens is not None:\n            self.init_from_tokens(tokens, type)\n\n    def init_from_entry(self, span_entry):\n        self.text = span_entry.get(TEXT, None)\n        self.type = span_entry.get(TYPE, None)\n        self.start_char = span_entry.get(START_CHAR, None)\n        self.end_char = span_entry.get(END_CHAR, None)\n\n    def init_from_tokens(self, tokens, type):\n        assert isinstance(tokens, list), \'Tokens must be provided as a list to construct a span.\'\n        assert len(tokens) > 0, ""Tokens of a span cannot be an empty list.""\n        self.tokens = tokens\n        self.type = type\n        # load start and end char offsets from tokens\n        self.start_char = self.tokens[0].start_char\n        self.end_char = self.tokens[-1].end_char\n        # assume doc is already provided and not None\n        self.text = self.doc.text[self.start_char:self.end_char]\n        # collect the words of the span following tokens\n        self.words = [w for t in tokens for w in t.words]\n\n    @property\n    def doc(self):\n        """""" Access the parent doc of this span. """"""\n        return self._doc\n\n    @doc.setter\n    def doc(self, value):\n        """""" Set the parent doc of this span. """"""\n        self._doc = value\n\n    @property\n    def text(self):\n        """""" Access the text of this span. Example: \'Stanford University\'""""""\n        return self._text\n\n    @text.setter\n    def text(self, value):\n        """""" Set the span\'s text value. Example: \'Stanford University\'""""""\n        self._text = value\n\n    @property\n    def tokens(self):\n        """""" Access reference to a list of tokens that correspond to this span. """"""\n        return self._tokens\n\n    @tokens.setter\n    def tokens(self, value):\n        """""" Set the span\'s list of tokens. """"""\n        self._tokens = value\n    \n    @property\n    def words(self):\n        """""" Access reference to a list of words that correspond to this span. """"""\n        return self._words\n\n    @words.setter\n    def words(self, value):\n        """""" Set the span\'s list of words. """"""\n        self._words = value\n\n    @property\n    def type(self):\n        """""" Access the type of this span. Example: \'PERSON\'""""""\n        return self._type\n\n    @type.setter\n    def type(self, value):\n        """""" Set the type of this span. """"""\n        self._type = value\n\n    @property\n    def start_char(self):\n        """""" Access the start character offset of this span. """"""\n        return self._start_char\n\n    @start_char.setter\n    def start_char(self, value):\n        """""" Set the start character offset of this span. """"""\n        self._start_char = value\n\n    @property\n    def end_char(self):\n        """""" Access the end character offset of this span. """"""\n        return self._end_char\n\n    @end_char.setter\n    def end_char(self, value):\n        """""" Set the end character offset of this span. """"""\n        self._end_char = value\n\n    def to_dict(self):\n        """""" Dumps the span into a dictionary. """"""\n        attrs = [\'text\', \'type\', \'start_char\', \'end_char\']\n        span_dict = dict([(attr_name, getattr(self, attr_name)) for attr_name in attrs])\n        return span_dict\n\n    def __repr__(self):\n        return json.dumps(self.to_dict(), indent=2, ensure_ascii=False)\n\n    def pretty_print(self):\n        """""" Print the span in one line. """"""\n        span_dict = self.to_dict()\n        feature_str = "";"".join([""{}={}"".format(k,v) for k,v in span_dict.items()])\n        return f""<{self.__class__.__name__} {feature_str}>""\n'"
stanza/models/common/dropout.py,3,"b'import torch\nimport torch.nn as nn\n\nclass WordDropout(nn.Module):\n    """""" A word dropout layer that\'s designed for embedded inputs (e.g., any inputs to an LSTM layer).\n    Given a batch of embedded inputs, this layer randomly set some of them to be a replacement state.\n    Note that this layer assumes the last dimension of the input to be the hidden dimension of a unit.\n    """"""\n    def __init__(self, dropprob):\n        super().__init__()\n        self.dropprob = dropprob\n\n    def forward(self, x, replacement=None):\n        if not self.training or self.dropprob == 0:\n            return x\n\n        masksize = [y for y in x.size()]\n        masksize[-1] = 1\n        dropmask = torch.rand(*masksize, device=x.device) < self.dropprob\n\n        res = x.masked_fill(dropmask, 0)\n        if replacement is not None:\n            res = res + dropmask.float() * replacement\n\n        return res\n    \n    def extra_repr(self):\n        return \'p={}\'.format(self.dropprob)\n\nclass LockedDropout(nn.Module):\n    """"""\n    A variant of dropout layer that consistently drops out the same parameters over time. Also known as the variational dropout. \n    This implentation was modified from the LockedDropout implementation in the flair library (https://github.com/zalandoresearch/flair).\n    """"""\n    def __init__(self, dropprob, batch_first=True):\n        super().__init__()\n        self.dropprob = dropprob\n        self.batch_first = batch_first\n\n    def forward(self, x):\n        if not self.training or self.dropprob == 0:\n            return x\n\n        if not self.batch_first:\n            m = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.dropprob)\n        else:\n            m = x.new_empty(x.size(0), 1, x.size(2), requires_grad=False).bernoulli_(1 - self.dropprob)\n\n        mask = m.div(1 - self.dropprob).expand_as(x)\n        return mask * x\n    \n    def extra_repr(self):\n        return \'p={}\'.format(self.dropprob)\n\nclass SequenceUnitDropout(nn.Module):\n    """""" A unit dropout layer that\'s designed for input of sequence units (e.g., word sequence, char sequence, etc.).\n    Given a sequence of unit indices, this layer randomly set some of them to be a replacement id (usually set to be <UNK>).\n    """"""\n    def __init__(self, dropprob, replacement_id):\n        super().__init__()\n        self.dropprob = dropprob\n        self.replacement_id = replacement_id\n\n    def forward(self, x):\n        """""" :param: x must be a LongTensor of unit indices. """"""\n        if not self.training or self.dropprob == 0:\n            return x\n        masksize = [y for y in x.size()]\n        dropmask = torch.rand(*masksize, device=x.device) < self.dropprob\n        res = x.masked_fill(dropmask, self.replacement_id)\n        return res\n    \n    def extra_repr(self):\n        return \'p={}, replacement_id={}\'.format(self.dropprob, self.replacement_id)\n\n'"
stanza/models/common/hlstm.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence\n\nfrom stanza.models.common.packed_lstm import PackedLSTM\n\nclass HLSTMCell(nn.modules.rnn.RNNCellBase):\n    """"""\n    A Highway LSTM Cell as proposed in Zhang et al. (2018) Highway Long Short-Term Memory RNNs for \n    Distant Speech Recognition.\n    """"""\n    def __init__(self, input_size, hidden_size, bias=True):\n        super(HLSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n\n        # LSTM parameters\n        self.Wi = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)\n        self.Wf = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)\n        self.Wo = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)\n        self.Wg = nn.Linear(input_size + hidden_size, hidden_size, bias=bias)\n\n        # highway gate parameters\n        self.gate = nn.Linear(input_size + 2 * hidden_size, hidden_size, bias=bias)\n\n    def forward(self, input, c_l_minus_one=None, hx=None):\n        self.check_forward_input(input)\n        if hx is None:\n            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n            hx = (hx, hx)\n        if c_l_minus_one is None:\n            c_l_minus_one = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n\n        self.check_forward_hidden(input, hx[0], \'[0]\')\n        self.check_forward_hidden(input, hx[1], \'[1]\')\n        self.check_forward_hidden(input, c_l_minus_one, \'c_l_minus_one\')\n\n        # vanilla LSTM computation\n        rec_input = torch.cat([input, hx[0]], 1)\n        i = F.sigmoid(self.Wi(rec_input))\n        f = F.sigmoid(self.Wf(rec_input))\n        o = F.sigmoid(self.Wo(rec_input))\n        g = F.tanh(self.Wg(rec_input))\n\n        # highway gates\n        gate = F.sigmoid(self.gate(torch.cat([c_l_minus_one, hx[1], input], 1)))\n\n        c = gate * c_l_minus_one + f * hx[1] + i * g\n        h = o * F.tanh(c)\n\n        return h, c\n\n# Highway LSTM network, does NOT use the HLSTMCell above\nclass HighwayLSTM(nn.Module):\n    """"""\n    A Highway LSTM network, as used in the original Tensorflow version of the Dozat parser. Note that this\n    is independent from the HLSTMCell above.\n    """"""\n    def __init__(self, input_size, hidden_size,\n                 num_layers=1, bias=True, batch_first=False,\n                 dropout=0, bidirectional=False, rec_dropout=0, highway_func=None, pad=False):\n        super(HighwayLSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.bias = bias\n        self.batch_first = batch_first\n        self.dropout = dropout\n        self.dropout_state = {}\n        self.bidirectional = bidirectional\n        self.num_directions = 2 if bidirectional else 1\n        self.highway_func = highway_func\n        self.pad = pad\n\n        self.lstm = nn.ModuleList()\n        self.highway = nn.ModuleList()\n        self.gate = nn.ModuleList()\n        self.drop = nn.Dropout(dropout, inplace=True)\n\n        in_size = input_size\n        for l in range(num_layers):\n            self.lstm.append(PackedLSTM(in_size, hidden_size, num_layers=1, bias=bias,\n                batch_first=batch_first, dropout=0, bidirectional=bidirectional, rec_dropout=rec_dropout))\n            self.highway.append(nn.Linear(in_size, hidden_size * self.num_directions))\n            self.gate.append(nn.Linear(in_size, hidden_size * self.num_directions))\n            self.highway[-1].bias.data.zero_()\n            self.gate[-1].bias.data.zero_()\n            in_size = hidden_size * self.num_directions\n\n    def forward(self, input, seqlens, hx=None):\n        highway_func = (lambda x: x) if self.highway_func is None else self.highway_func\n\n        hs = []\n        cs = []\n\n        if not isinstance(input, PackedSequence):\n            input = pack_padded_sequence(input, seqlens, batch_first=self.batch_first)\n\n        for l in range(self.num_layers):\n            if l > 0:\n                input = PackedSequence(self.drop(input.data), input.batch_sizes)\n            layer_hx = (hx[0][l * self.num_directions:(l+1)*self.num_directions], hx[1][l * self.num_directions:(l+1)*self.num_directions]) if hx is not None else None\n            h, (ht, ct) = self.lstm[l](input, seqlens, layer_hx)\n\n            hs.append(ht)\n            cs.append(ct)\n\n            input = PackedSequence(h.data + torch.sigmoid(self.gate[l](input.data)) * highway_func(self.highway[l](input.data)), input.batch_sizes)\n\n        if self.pad:\n            input = pad_packed_sequence(input, batch_first=self.batch_first)[0]\n        return input, (torch.cat(hs, 0), torch.cat(cs, 0))\n\nif __name__ == ""__main__"":\n    T = 10\n    bidir = True\n    num_dir = 2 if bidir else 1\n    rnn = HighwayLSTM(10, 20, num_layers=2, bidirectional=True)\n    input = torch.randn(T, 3, 10)\n    hx = torch.randn(2 * num_dir, 3, 20)\n    cx = torch.randn(2 * num_dir, 3, 20)\n    output = rnn(input, (hx, cx))\n    print(output)\n'"
stanza/models/common/loss.py,4,"b'""""""\nDifferent loss functions.\n""""""\n\nimport torch\nimport torch.nn as nn\n\nimport stanza.models.common.seq2seq_constant as constant\n\ndef SequenceLoss(vocab_size):\n    weight = torch.ones(vocab_size)\n    weight[constant.PAD_ID] = 0\n    crit = nn.NLLLoss(weight)\n    return crit\n\nclass MixLoss(nn.Module):\n    """"""\n    A mixture of SequenceLoss and CrossEntropyLoss.\n    Loss = SequenceLoss + alpha * CELoss\n    """"""\n    def __init__(self, vocab_size, alpha):\n        super().__init__()\n        self.seq_loss = SequenceLoss(vocab_size)\n        self.ce_loss = nn.CrossEntropyLoss()\n        assert alpha >= 0\n        self.alpha = alpha\n\n    def forward(self, seq_inputs, seq_targets, class_inputs, class_targets):\n        sl = self.seq_loss(seq_inputs, seq_targets)\n        cel = self.ce_loss(class_inputs, class_targets)\n        loss = sl + self.alpha * cel\n        return loss\n\nclass MaxEntropySequenceLoss(nn.Module):\n    """"""\n    A max entropy loss that encourage the model to have large entropy,\n    therefore giving more diverse outputs.\n\n    Loss = NLLLoss + alpha * EntropyLoss\n    """"""\n    def __init__(self, vocab_size, alpha):\n        super().__init__()\n        weight = torch.ones(vocab_size)\n        weight[constant.PAD_ID] = 0\n        self.nll = nn.NLLLoss(weight)\n        self.alpha = alpha\n\n    def forward(self, inputs, targets):\n        """"""\n        inputs: [N, C]\n        targets: [N]\n        """"""\n        assert inputs.size(0) == targets.size(0)\n        nll_loss = self.nll(inputs, targets)\n        # entropy loss\n        mask = targets.eq(constant.PAD_ID).unsqueeze(1).expand_as(inputs)\n        masked_inputs = inputs.clone().masked_fill_(mask, 0.0)\n        p = torch.exp(masked_inputs)\n        ent_loss = p.mul(masked_inputs).sum() / inputs.size(0) # average over minibatch\n        loss = nll_loss + self.alpha * ent_loss\n        return loss\n\n'"
stanza/models/common/packed_lstm.py,8,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence\n\nclass PackedLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, pad=False, rec_dropout=0):\n        super().__init__()\n\n        self.batch_first = batch_first\n        self.pad = pad\n        if rec_dropout == 0:\n            # use the fast, native LSTM implementation\n            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional)\n        else:\n            self.lstm = LSTMwRecDropout(input_size, hidden_size, num_layers, bias=bias, batch_first=batch_first, dropout=dropout, bidirectional=bidirectional, rec_dropout=rec_dropout)\n\n    def forward(self, input, lengths, hx=None):\n        if not isinstance(input, PackedSequence):\n            input = pack_padded_sequence(input, lengths, batch_first=self.batch_first)\n\n        res = self.lstm(input, hx)\n        if self.pad:\n            res = (pad_packed_sequence(res[0], batch_first=self.batch_first)[0], res[1])\n        return res\n\nclass LSTMwRecDropout(nn.Module):\n    """""" An LSTM implementation that supports recurrent dropout """"""\n    def __init__(self, input_size, hidden_size, num_layers, bias=True, batch_first=False, dropout=0, bidirectional=False, pad=False, rec_dropout=0):\n        super().__init__()\n        self.batch_first = batch_first\n        self.pad = pad\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n\n        self.dropout = dropout\n        self.drop = nn.Dropout(dropout, inplace=True)\n        self.rec_drop = nn.Dropout(rec_dropout, inplace=True)\n\n        self.num_directions = 2 if bidirectional else 1\n\n        self.cells = nn.ModuleList()\n        for l in range(num_layers):\n            in_size = input_size if l == 0 else self.num_directions * hidden_size\n            for d in range(self.num_directions):\n                self.cells.append(nn.LSTMCell(in_size, hidden_size, bias=bias))\n\n    def forward(self, input, hx=None):\n        def rnn_loop(x, batch_sizes, cell, inits, reverse=False):\n            # RNN loop for one layer in one direction with recurrent dropout\n            # Assumes input is PackedSequence, returns PackedSequence as well\n            batch_size = batch_sizes[0].item()\n            states = [list(init.split([1] * batch_size)) for init in inits]\n            h_drop_mask = x.new_ones(batch_size, self.hidden_size)\n            h_drop_mask = self.rec_drop(h_drop_mask)\n            resh = []\n\n            if not reverse:\n                st = 0\n                for bs in batch_sizes:\n                    s1 = cell(x[st:st+bs], (torch.cat(states[0][:bs], 0) * h_drop_mask[:bs], torch.cat(states[1][:bs], 0)))\n                    resh.append(s1[0])\n                    for j in range(bs):\n                        states[0][j] = s1[0][j].unsqueeze(0)\n                        states[1][j] = s1[1][j].unsqueeze(0)\n                    st += bs\n            else:\n                en = x.size(0)\n                for i in range(batch_sizes.size(0)-1, -1, -1):\n                    bs = batch_sizes[i]\n                    s1 = cell(x[en-bs:en], (torch.cat(states[0][:bs], 0) * h_drop_mask[:bs], torch.cat(states[1][:bs], 0)))\n                    resh.append(s1[0])\n                    for j in range(bs):\n                        states[0][j] = s1[0][j].unsqueeze(0)\n                        states[1][j] = s1[1][j].unsqueeze(0)\n                    en -= bs\n                resh = list(reversed(resh))\n\n            return torch.cat(resh, 0), tuple(torch.cat(s, 0) for s in states)\n\n        all_states = [[], []]\n        inputdata, batch_sizes = input.data, input.batch_sizes\n        for l in range(self.num_layers):\n            new_input = []\n\n            if self.dropout > 0 and l > 0:\n                inputdata = self.drop(inputdata)\n            for d in range(self.num_directions):\n                idx = l * self.num_directions + d\n                cell = self.cells[idx]\n                out, states = rnn_loop(inputdata, batch_sizes, cell, (hx[i][idx] for i in range(2)) if hx is not None else (input.data.new_zeros(input.batch_sizes[0].item(), self.hidden_size, requires_grad=False) for _ in range(2)), reverse=(d == 1))\n\n                new_input.append(out)\n                all_states[0].append(states[0].unsqueeze(0))\n                all_states[1].append(states[1].unsqueeze(0))\n\n            if self.num_directions > 1:\n                # concatenate both directions\n                inputdata = torch.cat(new_input, 1)\n            else:\n                inputdata = new_input[0]\n\n        input = PackedSequence(inputdata, batch_sizes)\n\n        return input, tuple(torch.cat(x, 0) for x in all_states)\n'"
stanza/models/common/pretrain.py,3,"b'""""""\nSupports for pretrained data.\n""""""\nimport os\nimport re\n\nimport lzma\nimport logging\nimport numpy as np\nimport torch\n\nfrom .vocab import BaseVocab, VOCAB_PREFIX\n\nlogger = logging.getLogger(\'stanza\')\n\nclass PretrainedWordVocab(BaseVocab):\n    def build_vocab(self):\n        self._id2unit = VOCAB_PREFIX + self.data\n        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}\n\nclass Pretrain:\n    """""" A loader and saver for pretrained embeddings. """"""\n\n    def __init__(self, filename=None, vec_filename=None, max_vocab=-1, save_to_file=True):\n        self.filename = filename\n        self._vec_filename = vec_filename\n        self._max_vocab = max_vocab\n        self._save_to_file = save_to_file\n\n    @property\n    def vocab(self):\n        if not hasattr(self, \'_vocab\'):\n            self._vocab, self._emb = self.load()\n        return self._vocab\n\n    @property\n    def emb(self):\n        if not hasattr(self, \'_emb\'):\n            self._vocab, self._emb = self.load()\n        return self._emb\n\n    def load(self):\n        if self.filename is not None and os.path.exists(self.filename):\n            try:\n                data = torch.load(self.filename, lambda storage, loc: storage)\n            except (KeyboardInterrupt, SystemExit):\n                raise\n            except BaseException as e:\n                logger.warning(""Pretrained file exists but cannot be loaded from {}, due to the following exception:\\n\\t{}"".format(self.filename, e))\n                return self.read_pretrain()\n            return PretrainedWordVocab.load_state_dict(data[\'vocab\']), data[\'emb\']\n        else:\n            return self.read_pretrain()\n\n    def read_pretrain(self):\n        # load from pretrained filename\n        if self._vec_filename is None:\n            raise Exception(""Vector file is not provided."")\n        logger.info(""Reading pretrained vectors from {}..."".format(self._vec_filename))\n\n        # first try reading as xz file, if failed retry as text file\n        try:\n            words, emb, failed = self.read_from_file(self._vec_filename, open_func=lzma.open)\n        except lzma.LZMAError as err:\n            logging.warning(""Cannot decode vector file as xz file. Retrying as text file..."")\n            words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)\n\n        if failed > 0: # recover failure\n            emb = emb[:-failed]\n        if len(emb) - len(VOCAB_PREFIX) != len(words):\n            raise Exception(""Loaded number of vectors does not match number of words."")\n        \n        # Use a fixed vocab size\n        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words):\n            words = words[:self._max_vocab - len(VOCAB_PREFIX)]\n            emb = emb[:self._max_vocab]\n\n        vocab = PretrainedWordVocab(words)\n        \n        if self._save_to_file:\n            assert self.filename is not None, ""Filename must be provided to save pretrained vector to file.""\n            # save to file\n            data = {\'vocab\': vocab.state_dict(), \'emb\': emb}\n            try:\n                torch.save(data, self.filename)\n                logger.info(""Saved pretrained vocab and vectors to {}"".format(self.filename))\n            except (KeyboardInterrupt, SystemExit):\n                raise\n            except BaseException as e:\n                logger.warning(""Saving pretrained data failed due to the following exception... continuing anyway.\\n\\t{}"".format(e))\n\n        return vocab, emb\n\n    def read_from_file(self, filename, open_func=open):\n        """"""\n        Open a vector file using the provided function and read from it.\n        """"""\n        # some vector files, such as Google News, use tabs\n        tab_space_pattern = re.compile(""[ \\t]+"")\n        first = True\n        words = []\n        failed = 0\n        with open_func(filename, \'rb\') as f:\n            for i, line in enumerate(f):\n                try:\n                    line = line.decode()\n                except UnicodeDecodeError:\n                    failed += 1\n                    continue\n                if first:\n                    # the first line contains the number of word vectors and the dimensionality\n                    first = False\n                    line = line.strip().split(\' \')\n                    rows, cols = [int(x) for x in line]\n                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)\n                    continue\n\n                line = tab_space_pattern.split((line.rstrip()))\n                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]\n                words.append(\' \'.join(line[:-cols]))\n        return words, emb, failed\n\n\nif __name__ == \'__main__\':\n    with open(\'test.txt\', \'w\') as fout:\n        fout.write(\'3 2\\na 1 1\\nb -1 -1\\nc 0 0\\n\')\n    # 1st load: save to pt file\n    pretrain = Pretrain(\'test.pt\', \'test.txt\')\n    print(pretrain.emb)\n    # verify pt file\n    x = torch.load(\'test.pt\')\n    print(x)\n    # 2nd load: load saved pt file\n    pretrain = Pretrain(\'test.pt\', \'test.txt\')\n    print(pretrain.emb)'"
stanza/models/common/seq2seq_constant.py,0,"b'""""""\nConstants for seq2seq models.\n""""""\n\nPAD = \'<PAD>\'\nPAD_ID = 0\nUNK = \'<UNK>\'\nUNK_ID = 1\nSOS = \'<SOS>\'\nSOS_ID = 2\nEOS = \'<EOS>\'\nEOS_ID = 3\n\nVOCAB_PREFIX = [PAD, UNK, SOS, EOS]\n\nEMB_INIT_RANGE = 1.0\nINFINITY_NUMBER = 1e12\n'"
stanza/models/common/seq2seq_model.py,15,"b'""""""\nThe full encoder-decoder model, built on top of the base seq2seq modules.\n""""""\n\nimport logging\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport numpy as np\n\nimport stanza.models.common.seq2seq_constant as constant\nfrom stanza.models.common import utils\nfrom stanza.models.common.seq2seq_modules import LSTMAttention\nfrom stanza.models.common.beam import Beam\n\nlogger = logging.getLogger(\'stanza\')\n\nclass Seq2SeqModel(nn.Module):\n    """"""\n    A complete encoder-decoder model, with optional attention.\n    """"""\n    def __init__(self, args, emb_matrix=None, use_cuda=False):\n        super().__init__()\n        self.vocab_size = args[\'vocab_size\']\n        self.emb_dim = args[\'emb_dim\']\n        self.hidden_dim = args[\'hidden_dim\']\n        self.nlayers = args[\'num_layers\'] # encoder layers, decoder layers = 1\n        self.emb_dropout = args.get(\'emb_dropout\', 0.0)\n        self.dropout = args[\'dropout\']\n        self.pad_token = constant.PAD_ID\n        self.max_dec_len = args[\'max_dec_len\']\n        self.use_cuda = use_cuda\n        self.top = args.get(\'top\', 1e10)\n        self.args = args\n        self.emb_matrix = emb_matrix\n\n        logger.debug(""Building an attentional Seq2Seq model..."")\n        logger.debug(""Using a Bi-LSTM encoder"")\n        self.num_directions = 2\n        self.enc_hidden_dim = self.hidden_dim // 2\n        self.dec_hidden_dim = self.hidden_dim\n\n        self.use_pos = args.get(\'pos\', False)\n        self.pos_dim = args.get(\'pos_dim\', 0)\n        self.pos_vocab_size = args.get(\'pos_vocab_size\', 0)\n        self.pos_dropout = args.get(\'pos_dropout\', 0)\n        self.edit = args.get(\'edit\', False)\n        self.num_edit = args.get(\'num_edit\', 0)\n\n        self.emb_drop = nn.Dropout(self.emb_dropout)\n        self.drop = nn.Dropout(self.dropout)\n        self.embedding = nn.Embedding(self.vocab_size, self.emb_dim, self.pad_token)\n        self.encoder = nn.LSTM(self.emb_dim, self.enc_hidden_dim, self.nlayers, \\\n                bidirectional=True, batch_first=True, dropout=self.dropout if self.nlayers > 1 else 0)\n        self.decoder = LSTMAttention(self.emb_dim, self.dec_hidden_dim, \\\n                batch_first=True, attn_type=self.args[\'attn_type\'])\n        self.dec2vocab = nn.Linear(self.dec_hidden_dim, self.vocab_size)\n        if self.use_pos and self.pos_dim > 0:\n            logger.debug(""Using POS in encoder"")\n            self.pos_embedding = nn.Embedding(self.pos_vocab_size, self.pos_dim, self.pad_token)\n            self.pos_drop = nn.Dropout(self.pos_dropout)\n        if self.edit:\n            edit_hidden = self.hidden_dim//2\n            self.edit_clf = nn.Sequential(\n                    nn.Linear(self.hidden_dim, edit_hidden),\n                    nn.ReLU(),\n                    nn.Linear(edit_hidden, self.num_edit))\n\n        self.SOS_tensor = torch.LongTensor([constant.SOS_ID])\n        self.SOS_tensor = self.SOS_tensor.cuda() if self.use_cuda else self.SOS_tensor\n\n        self.init_weights()\n\n    def init_weights(self):\n        # initialize embeddings\n        init_range = constant.EMB_INIT_RANGE\n        if self.emb_matrix is not None:\n            if isinstance(self.emb_matrix, np.ndarray):\n                self.emb_matrix = torch.from_numpy(self.emb_matrix)\n            assert self.emb_matrix.size() == (self.vocab_size, self.emb_dim), \\\n                    ""Input embedding matrix must match size: {} x {}"".format(self.vocab_size, self.emb_dim)\n            self.embedding.weight.data.copy_(self.emb_matrix)\n        else:\n            self.embedding.weight.data.uniform_(-init_range, init_range)\n        # decide finetuning\n        if self.top <= 0:\n            logger.debug(""Do not finetune embedding layer."")\n            self.embedding.weight.requires_grad = False\n        elif self.top < self.vocab_size:\n            logger.debug(""Finetune top {} embeddings."".format(self.top))\n            self.embedding.weight.register_hook(lambda x: utils.keep_partial_grad(x, self.top))\n        else:\n            logger.debug(""Finetune all embeddings."")\n        # initialize pos embeddings\n        if self.use_pos:\n            self.pos_embedding.weight.data.uniform_(-init_range, init_range)\n\n    def cuda(self):\n        super().cuda()\n        self.use_cuda = True\n\n    def cpu(self):\n        super().cpu()\n        self.use_cuda = False\n\n    def zero_state(self, inputs):\n        batch_size = inputs.size(0)\n        h0 = torch.zeros(self.encoder.num_layers*2, batch_size, self.enc_hidden_dim, requires_grad=False)\n        c0 = torch.zeros(self.encoder.num_layers*2, batch_size, self.enc_hidden_dim, requires_grad=False)\n        if self.use_cuda:\n            return h0.cuda(), c0.cuda()\n        return h0, c0\n\n    def encode(self, enc_inputs, lens):\n        """""" Encode source sequence. """"""\n        self.h0, self.c0 = self.zero_state(enc_inputs)\n\n        packed_inputs = nn.utils.rnn.pack_padded_sequence(enc_inputs, lens, batch_first=True)\n        packed_h_in, (hn, cn) = self.encoder(packed_inputs, (self.h0, self.c0))\n        h_in, _ = nn.utils.rnn.pad_packed_sequence(packed_h_in, batch_first=True)\n        hn = torch.cat((hn[-1], hn[-2]), 1)\n        cn = torch.cat((cn[-1], cn[-2]), 1)\n        return h_in, (hn, cn)\n\n    def decode(self, dec_inputs, hn, cn, ctx, ctx_mask=None):\n        """""" Decode a step, based on context encoding and source context states.""""""\n        dec_hidden = (hn, cn)\n        h_out, dec_hidden = self.decoder(dec_inputs, dec_hidden, ctx, ctx_mask)\n\n        h_out_reshape = h_out.contiguous().view(h_out.size(0) * h_out.size(1), -1)\n        decoder_logits = self.dec2vocab(h_out_reshape)\n        decoder_logits = decoder_logits.view(h_out.size(0), h_out.size(1), -1)\n        log_probs = self.get_log_prob(decoder_logits)\n        return log_probs, dec_hidden\n\n    def forward(self, src, src_mask, tgt_in, pos=None):\n        # prepare for encoder/decoder\n        batch_size = src.size(0)\n        enc_inputs = self.emb_drop(self.embedding(src))\n        dec_inputs = self.emb_drop(self.embedding(tgt_in))\n        if self.use_pos:\n            assert pos is not None, ""Missing POS input for seq2seq lemmatizer.""\n            pos_inputs = self.pos_drop(self.pos_embedding(pos))\n            enc_inputs = torch.cat([pos_inputs.unsqueeze(1), enc_inputs], dim=1)\n            pos_src_mask = src_mask.new_zeros([batch_size, 1])\n            src_mask = torch.cat([pos_src_mask, src_mask], dim=1)\n        src_lens = list(src_mask.data.eq(0).long().sum(1))\n\n        h_in, (hn, cn) = self.encode(enc_inputs, src_lens)\n\n        if self.edit:\n            edit_logits = self.edit_clf(hn)\n        else:\n            edit_logits = None\n\n        log_probs, _ = self.decode(dec_inputs, hn, cn, h_in, src_mask)\n        return log_probs, edit_logits\n\n    def get_log_prob(self, logits):\n        logits_reshape = logits.view(-1, self.vocab_size)\n        log_probs = F.log_softmax(logits_reshape, dim=1)\n        if logits.dim() == 2:\n            return log_probs\n        return log_probs.view(logits.size(0), logits.size(1), logits.size(2))\n\n    def predict_greedy(self, src, src_mask, pos=None):\n        """""" Predict with greedy decoding. """"""\n        enc_inputs = self.embedding(src)\n        batch_size = enc_inputs.size(0)\n        if self.use_pos:\n            assert pos is not None, ""Missing POS input for seq2seq lemmatizer.""\n            pos_inputs = self.pos_drop(self.pos_embedding(pos))\n            enc_inputs = torch.cat([pos_inputs.unsqueeze(1), enc_inputs], dim=1)\n            pos_src_mask = src_mask.new_zeros([batch_size, 1])\n            src_mask = torch.cat([pos_src_mask, src_mask], dim=1)\n        src_lens = list(src_mask.data.eq(constant.PAD_ID).long().sum(1))\n\n        # encode source\n        h_in, (hn, cn) = self.encode(enc_inputs, src_lens)\n\n        if self.edit:\n            edit_logits = self.edit_clf(hn)\n        else:\n            edit_logits = None\n\n        # greedy decode by step\n        dec_inputs = self.embedding(self.SOS_tensor)\n        dec_inputs = dec_inputs.expand(batch_size, dec_inputs.size(0), dec_inputs.size(1))\n\n        done = [False for _ in range(batch_size)]\n        total_done = 0\n        max_len = 0\n        output_seqs = [[] for _ in range(batch_size)]\n\n        while total_done < batch_size and max_len < self.max_dec_len:\n            log_probs, (hn, cn) = self.decode(dec_inputs, hn, cn, h_in, src_mask)\n            assert log_probs.size(1) == 1, ""Output must have 1-step of output.""\n            _, preds = log_probs.squeeze(1).max(1, keepdim=True)\n            dec_inputs = self.embedding(preds) # update decoder inputs\n            max_len += 1\n            for i in range(batch_size):\n                if not done[i]:\n                    token = preds.data[i][0].item()\n                    if token == constant.EOS_ID:\n                        done[i] = True\n                        total_done += 1\n                    else:\n                        output_seqs[i].append(token)\n        return output_seqs, edit_logits\n\n    def predict(self, src, src_mask, pos=None, beam_size=5):\n        """""" Predict with beam search. """"""\n        if beam_size == 1:\n            return self.predict_greedy(src, src_mask, pos=pos)\n\n        enc_inputs = self.embedding(src)\n        batch_size = enc_inputs.size(0)\n        if self.use_pos:\n            assert pos is not None, ""Missing POS input for seq2seq lemmatizer.""\n            pos_inputs = self.pos_drop(self.pos_embedding(pos))\n            enc_inputs = torch.cat([pos_inputs.unsqueeze(1), enc_inputs], dim=1)\n            pos_src_mask = src_mask.new_zeros([batch_size, 1])\n            src_mask = torch.cat([pos_src_mask, src_mask], dim=1)\n        src_lens = list(src_mask.data.eq(constant.PAD_ID).long().sum(1))\n\n        # (1) encode source\n        h_in, (hn, cn) = self.encode(enc_inputs, src_lens)\n\n        if self.edit:\n            edit_logits = self.edit_clf(hn)\n        else:\n            edit_logits = None\n\n        # (2) set up beam\n        with torch.no_grad():\n            h_in = h_in.data.repeat(beam_size, 1, 1) # repeat data for beam search\n            src_mask = src_mask.repeat(beam_size, 1)\n            # repeat decoder hidden states\n            hn = hn.data.repeat(beam_size, 1)\n            cn = cn.data.repeat(beam_size, 1)\n        beam = [Beam(beam_size, self.use_cuda) for _ in range(batch_size)]\n\n        def update_state(states, idx, positions, beam_size):\n            """""" Select the states according to back pointers. """"""\n            for e in states:\n                br, d = e.size()\n                s = e.contiguous().view(beam_size, br // beam_size, d)[:,idx]\n                s.data.copy_(s.data.index_select(0, positions))\n\n        # (3) main loop\n        for i in range(self.max_dec_len):\n            dec_inputs = torch.stack([b.get_current_state() for b in beam]).t().contiguous().view(-1, 1)\n            dec_inputs = self.embedding(dec_inputs)\n            log_probs, (hn, cn) = self.decode(dec_inputs, hn, cn, h_in, src_mask)\n            log_probs = log_probs.view(beam_size, batch_size, -1).transpose(0,1)\\\n                    .contiguous() # [batch, beam, V]\n\n            # advance each beam\n            done = []\n            for b in range(batch_size):\n                is_done = beam[b].advance(log_probs.data[b])\n                if is_done:\n                    done += [b]\n                # update beam state\n                update_state((hn, cn), b, beam[b].get_current_origin(), beam_size)\n\n            if len(done) == batch_size:\n                break\n\n        # back trace and find hypothesis\n        all_hyp, all_scores = [], []\n        for b in range(batch_size):\n            scores, ks = beam[b].sort_best()\n            all_scores += [scores[0]]\n            k = ks[0]\n            hyp = beam[b].get_hyp(k)\n            hyp = utils.prune_hyp(hyp)\n            hyp = [i.item() for i in hyp]\n            all_hyp += [hyp]\n\n        return all_hyp, edit_logits\n\n'"
stanza/models/common/seq2seq_modules.py,12,"b'""""""\nPytorch implementation of basic sequence to Sequence modules.\n""""""\n\nimport logging\nimport torch\nimport torch.nn as nn\nimport math\nimport numpy as np\n\nimport stanza.models.common.seq2seq_constant as constant\n\nlogger = logging.getLogger(\'stanza\')\n\nclass BasicAttention(nn.Module):\n    """"""\n    A basic MLP attention layer.\n    """"""\n    def __init__(self, dim):\n        super(BasicAttention, self).__init__()\n        self.linear_in = nn.Linear(dim, dim, bias=False)\n        self.linear_c = nn.Linear(dim, dim)\n        self.linear_v = nn.Linear(dim, 1, bias=False)\n        self.linear_out = nn.Linear(dim * 2, dim, bias=False)\n        self.tanh = nn.Tanh()\n        self.sm = nn.Softmax(dim=1)\n\n    def forward(self, input, context, mask=None, attn_only=False):\n        """"""\n        input: batch x dim\n        context: batch x sourceL x dim\n        """"""\n        batch_size = context.size(0)\n        source_len = context.size(1)\n        dim = context.size(2)\n        target = self.linear_in(input) # batch x dim\n        source = self.linear_c(context.contiguous().view(-1, dim)).view(batch_size, source_len, dim)\n        attn = target.unsqueeze(1).expand_as(context) + source\n        attn = self.tanh(attn) # batch x sourceL x dim\n        attn = self.linear_v(attn.view(-1, dim)).view(batch_size, source_len)\n\n        if mask is not None:\n            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)\n\n        attn = self.sm(attn)\n        if attn_only:\n            return attn\n\n        weighted_context = torch.bmm(attn.unsqueeze(1), context).squeeze(1)\n        h_tilde = torch.cat((weighted_context, input), 1)\n        h_tilde = self.tanh(self.linear_out(h_tilde))\n\n        return h_tilde, attn\n\nclass SoftDotAttention(nn.Module):\n    """"""Soft Dot Attention.\n\n    Ref: http://www.aclweb.org/anthology/D15-1166\n    Adapted from PyTorch OPEN NMT.\n    """"""\n\n    def __init__(self, dim):\n        """"""Initialize layer.""""""\n        super(SoftDotAttention, self).__init__()\n        self.linear_in = nn.Linear(dim, dim, bias=False)\n        self.sm = nn.Softmax(dim=1)\n        self.linear_out = nn.Linear(dim * 2, dim, bias=False)\n        self.tanh = nn.Tanh()\n        self.mask = None\n\n    def forward(self, input, context, mask=None, attn_only=False):\n        """"""Propogate input through the network.\n\n        input: batch x dim\n        context: batch x sourceL x dim\n        """"""\n        target = self.linear_in(input).unsqueeze(2)  # batch x dim x 1\n\n        # Get attention\n        attn = torch.bmm(context, target).squeeze(2)  # batch x sourceL\n\n        if mask is not None:\n            # sett the padding attention logits to -inf\n            assert mask.size() == attn.size(), ""Mask size must match the attention size!""\n            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)\n\n        attn = self.sm(attn)\n        if attn_only:\n            return attn\n\n        attn3 = attn.view(attn.size(0), 1, attn.size(1))  # batch x 1 x sourceL\n\n        weighted_context = torch.bmm(attn3, context).squeeze(1)  # batch x dim\n        h_tilde = torch.cat((weighted_context, input), 1)\n\n        h_tilde = self.tanh(self.linear_out(h_tilde))\n\n        return h_tilde, attn\n\n\nclass LinearAttention(nn.Module):\n    """""" A linear attention form, inspired by BiDAF:\n        a = W (u; v; u o v)\n    """"""\n\n    def __init__(self, dim):\n        super(LinearAttention, self).__init__()\n        self.linear = nn.Linear(dim*3, 1, bias=False)\n        self.linear_out = nn.Linear(dim * 2, dim, bias=False)\n        self.sm = nn.Softmax(dim=1)\n        self.tanh = nn.Tanh()\n        self.mask = None\n\n    def forward(self, input, context, mask=None, attn_only=False):\n        """"""\n        input: batch x dim\n        context: batch x sourceL x dim\n        """"""\n        batch_size = context.size(0)\n        source_len = context.size(1)\n        dim = context.size(2)\n        u = input.unsqueeze(1).expand_as(context).contiguous().view(-1, dim)  # batch*sourceL x dim\n        v = context.contiguous().view(-1, dim)\n        attn_in = torch.cat((u, v, u.mul(v)), 1)\n        attn = self.linear(attn_in).view(batch_size, source_len)\n\n        if mask is not None:\n            # sett the padding attention logits to -inf\n            assert mask.size() == attn.size(), ""Mask size must match the attention size!""\n            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)\n\n        attn = self.sm(attn)\n        if attn_only:\n            return attn\n\n        attn3 = attn.view(batch_size, 1, source_len)  # batch x 1 x sourceL\n\n        weighted_context = torch.bmm(attn3, context).squeeze(1)  # batch x dim\n        h_tilde = torch.cat((weighted_context, input), 1)\n        h_tilde = self.tanh(self.linear_out(h_tilde))\n        return h_tilde, attn\n\nclass DeepAttention(nn.Module):\n    """""" A deep attention form, invented by Robert:\n        u = ReLU(Wx)\n        v = ReLU(Wy)\n        a = V.(u o v)\n    """"""\n\n    def __init__(self, dim):\n        super(DeepAttention, self).__init__()\n        self.linear_in = nn.Linear(dim, dim, bias=False)\n        self.linear_v = nn.Linear(dim, 1, bias=False)\n        self.linear_out = nn.Linear(dim * 2, dim, bias=False)\n        self.relu = nn.ReLU()\n        self.sm = nn.Softmax(dim=1)\n        self.tanh = nn.Tanh()\n        self.mask = None\n\n    def forward(self, input, context, mask=None, attn_only=False):\n        """"""\n        input: batch x dim\n        context: batch x sourceL x dim\n        """"""\n        batch_size = context.size(0)\n        source_len = context.size(1)\n        dim = context.size(2)\n        u = input.unsqueeze(1).expand_as(context).contiguous().view(-1, dim)  # batch*sourceL x dim\n        u = self.relu(self.linear_in(u))\n        v = self.relu(self.linear_in(context.contiguous().view(-1, dim)))\n        attn = self.linear_v(u.mul(v)).view(batch_size, source_len)\n\n        if mask is not None:\n            # sett the padding attention logits to -inf\n            assert mask.size() == attn.size(), ""Mask size must match the attention size!""\n            attn.masked_fill_(mask, -constant.INFINITY_NUMBER)\n\n        attn = self.sm(attn)\n        if attn_only:\n            return attn\n\n        attn3 = attn.view(batch_size, 1, source_len)  # batch x 1 x sourceL\n\n        weighted_context = torch.bmm(attn3, context).squeeze(1)  # batch x dim\n        h_tilde = torch.cat((weighted_context, input), 1)\n        h_tilde = self.tanh(self.linear_out(h_tilde))\n        return h_tilde, attn\n\nclass LSTMAttention(nn.Module):\n    r""""""A long short-term memory (LSTM) cell with attention.""""""\n\n    def __init__(self, input_size, hidden_size, batch_first=True, attn_type=\'soft\'):\n        """"""Initialize params.""""""\n        super(LSTMAttention, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.batch_first = batch_first\n\n        self.lstm_cell = nn.LSTMCell(input_size, hidden_size)\n\n        if attn_type == \'soft\':\n            self.attention_layer = SoftDotAttention(hidden_size)\n        elif attn_type == \'mlp\':\n            self.attention_layer = BasicAttention(hidden_size)\n        elif attn_type == \'linear\':\n            self.attention_layer = LinearAttention(hidden_size)\n        elif attn_type == \'deep\':\n            self.attention_layer = DeepAttention(hidden_size)\n        else:\n            raise Exception(""Unsupported LSTM attention type: {}"".format(attn_type))\n        logger.debug(""Using {} attention for LSTM."".format(attn_type))\n\n    def forward(self, input, hidden, ctx, ctx_mask=None):\n        """"""Propogate input through the network.""""""\n        if self.batch_first:\n            input = input.transpose(0,1)\n\n        output = []\n        steps = range(input.size(0))\n        for i in steps:\n            hidden = self.lstm_cell(input[i], hidden)\n            hy, cy = hidden\n            h_tilde, alpha = self.attention_layer(hy, ctx, mask=ctx_mask)\n            output.append(h_tilde)\n        output = torch.cat(output, 0).view(input.size(0), *output[0].size())\n\n        if self.batch_first:\n            output = output.transpose(0,1)\n\n        return output, hidden\n\n'"
stanza/models/common/seq2seq_utils.py,4,"b'""""""\nUtils for seq2seq models.\n""""""\nfrom collections import Counter\nimport random\nimport json\nimport unicodedata\nimport torch\n\nimport stanza.models.common.seq2seq_constant as constant\n\n# torch utils\ndef get_optimizer(name, parameters, lr):\n    if name == \'sgd\':\n        return torch.optim.SGD(parameters, lr=lr)\n    elif name == \'adagrad\':\n        return torch.optim.Adagrad(parameters, lr=lr)\n    elif name == \'adam\':\n        return torch.optim.Adam(parameters) # use default lr\n    elif name == \'adamax\':\n        return torch.optim.Adamax(parameters) # use default lr\n    else:\n        raise Exception(""Unsupported optimizer: {}"".format(name))\n\ndef change_lr(optimizer, new_lr):\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = new_lr\n\ndef flatten_indices(seq_lens, width):\n    flat = []\n    for i, l in enumerate(seq_lens):\n        for j in range(l):\n            flat.append(i * width + j)\n    return flat\n\ndef set_cuda(var, cuda):\n    if cuda:\n        return var.cuda()\n    return var\n\ndef keep_partial_grad(grad, topk):\n    """"""\n    Keep only the topk rows of grads.\n    """"""\n    assert topk < grad.size(0)\n    grad.data[topk:].zero_()\n    return grad\n\n# other utils\ndef save_config(config, path, verbose=True):\n    with open(path, \'w\') as outfile:\n        json.dump(config, outfile, indent=2)\n    if verbose:\n        print(""Config saved to file {}"".format(path))\n    return config\n\ndef load_config(path, verbose=True):\n    with open(path) as f:\n        config = json.load(f)\n    if verbose:\n        print(""Config loaded from file {}"".format(path))\n    return config\n\ndef normalize_text(text):\n    return unicodedata.normalize(\'NFD\', text)\n\ndef unmap_with_copy(indices, src_tokens, vocab):\n    """"""\n    Unmap a list of list of indices, by optionally copying from src_tokens.\n    """"""\n    result = []\n    for ind, tokens in zip(indices, src_tokens):\n        words = []\n        for idx in ind:\n            if idx >= 0:\n                words.append(vocab.id2word[idx])\n            else:\n                idx = -idx - 1 # flip and minus 1\n                words.append(tokens[idx])\n        result += [words]\n    return result\n\ndef prune_decoded_seqs(seqs):\n    """"""\n    Prune decoded sequences after EOS token.\n    """"""\n    out = []\n    for s in seqs:\n        if constant.EOS in s:\n            idx = s.index(constant.EOS_TOKEN)\n            out += [s[:idx]]\n        else:\n            out += [s]\n    return out\n\ndef prune_hyp(hyp):\n    """"""\n    Prune a decoded hypothesis\n    """"""\n    if constant.EOS_ID in hyp:\n        idx = hyp.index(constant.EOS_ID)\n        return hyp[:idx]\n    else:\n        return hyp\n\ndef prune(data_list, lens):\n    assert len(data_list) == len(lens)\n    nl = []\n    for d, l in zip(data_list, lens):\n        nl.append(d[:l])\n    return nl\n\ndef sort(packed, ref, reverse=True):\n    """"""\n    Sort a series of packed list, according to a ref list.\n    Also return the original index before the sort.\n    """"""\n    assert (isinstance(packed, tuple) or isinstance(packed, list)) and isinstance(ref, list)\n    packed = [ref] + [range(len(ref))] + list(packed)\n    sorted_packed = [list(t) for t in zip(*sorted(zip(*packed), reverse=reverse))]\n    return tuple(sorted_packed[1:])\n\ndef unsort(sorted_list, oidx):\n    """"""\n    Unsort a sorted list, based on the original idx.\n    """"""\n    assert len(sorted_list) == len(oidx), ""Number of list elements must match with original indices.""\n    _, unsorted = [list(t) for t in zip(*sorted(zip(oidx, sorted_list)))]\n    return unsorted\n\n'"
stanza/models/common/trainer.py,2,"b""import torch\n\nclass Trainer:\n    def change_lr(self, new_lr):\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = new_lr\n\n    def save(self, filename):\n        savedict = {\n                   'model': self.model.state_dict(),\n                   'optimizer': self.optimizer.state_dict()\n                   }\n        torch.save(savedict, filename)\n\n    def load(self, filename):\n        savedict = torch.load(filename, lambda storage, loc: storage)\n\n        self.model.load_state_dict(savedict['model'])\n        if self.args['mode'] == 'train':\n            self.optimizer.load_state_dict(savedict['optimizer'])\n"""
stanza/models/common/utils.py,4,"b'""""""\nUtility functions.\n""""""\nimport os\nfrom collections import Counter\nimport random\nimport json\nimport unicodedata\nimport torch\n\nfrom stanza.models.common.constant import lcode2lang\nimport stanza.models.common.seq2seq_constant as constant\nimport stanza.utils.conll18_ud_eval as ud_eval\n\n# filenames\ndef get_wordvec_file(wordvec_dir, shorthand):\n    """""" Lookup the name of the word vectors file, given a directory and the language shorthand.\n    """"""\n    lcode, tcode = shorthand.split(\'_\', 1)\n    lang = lcode2lang[lcode]\n    # locate language folder\n    word2vec_dir = os.path.join(wordvec_dir, \'word2vec\', lang)\n    fasttext_dir = os.path.join(wordvec_dir, \'fasttext\', lang)\n    if os.path.exists(word2vec_dir): # first try word2vec\n        lang_dir = word2vec_dir\n    elif os.path.exists(fasttext_dir): # otherwise try fasttext\n        lang_dir = fasttext_dir\n    else:\n        raise Exception(""Cannot locate word vector directory for language: {}"".format(lang))\n    # look for wordvec filename in {lang_dir}\n    filename = os.path.join(lang_dir, \'{}.vectors\'.format(lcode))\n    if os.path.exists(filename + "".xz""):\n        filename = filename + "".xz""\n    elif os.path.exists(filename + "".txt""):\n        filename = filename + "".txt""\n    return filename\n\n# training schedule\ndef get_adaptive_eval_interval(cur_dev_size, thres_dev_size, base_interval):\n    """""" Adjust the evaluation interval adaptively.\n    If cur_dev_size <= thres_dev_size, return base_interval;\n    else, linearly increase the interval (round to integer times of base interval).\n    """"""\n    if cur_dev_size <= thres_dev_size:\n        return base_interval\n    else:\n        alpha = round(cur_dev_size / thres_dev_size)\n        return base_interval * alpha\n\n# ud utils\ndef ud_scores(gold_conllu_file, system_conllu_file):\n    gold_ud = ud_eval.load_conllu_file(gold_conllu_file)\n    system_ud = ud_eval.load_conllu_file(system_conllu_file)\n    evaluation = ud_eval.evaluate(gold_ud, system_ud)\n\n    return evaluation\n\ndef harmonic_mean(a, weights=None):\n    if any([x == 0 for x in a]):\n        return 0\n    else:\n        assert weights is None or len(weights) == len(a), \'Weights has length {} which is different from that of the array ({}).\'.format(len(weights), len(a))\n        if weights is None:\n            return len(a) / sum([1/x for x in a])\n        else:\n            return sum(weights) / sum(w/x for x, w in zip(a, weights))\n\n# torch utils\ndef get_optimizer(name, parameters, lr, betas=(0.9, 0.999), eps=1e-8, momentum=0):\n    if name == \'sgd\':\n        return torch.optim.SGD(parameters, lr=lr, momentum=momentum)\n    elif name == \'adagrad\':\n        return torch.optim.Adagrad(parameters, lr=lr)\n    elif name == \'adam\':\n        return torch.optim.Adam(parameters, lr=lr, betas=betas, eps=eps)\n    elif name == \'adamax\':\n        return torch.optim.Adamax(parameters) # use default lr\n    else:\n        raise Exception(""Unsupported optimizer: {}"".format(name))\n\ndef change_lr(optimizer, new_lr):\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = new_lr\n\ndef flatten_indices(seq_lens, width):\n    flat = []\n    for i, l in enumerate(seq_lens):\n        for j in range(l):\n            flat.append(i * width + j)\n    return flat\n\ndef set_cuda(var, cuda):\n    if cuda:\n        return var.cuda()\n    return var\n\ndef keep_partial_grad(grad, topk):\n    """"""\n    Keep only the topk rows of grads.\n    """"""\n    assert topk < grad.size(0)\n    grad.data[topk:].zero_()\n    return grad\n\n# other utils\ndef ensure_dir(d, verbose=True):\n    if not os.path.exists(d):\n        if verbose:\n            print(""Directory {} do not exist; creating..."".format(d))\n        os.makedirs(d)\n\ndef save_config(config, path, verbose=True):\n    with open(path, \'w\') as outfile:\n        json.dump(config, outfile, indent=2)\n    if verbose:\n        print(""Config saved to file {}"".format(path))\n    return config\n\ndef load_config(path, verbose=True):\n    with open(path) as f:\n        config = json.load(f)\n    if verbose:\n        print(""Config loaded from file {}"".format(path))\n    return config\n\ndef print_config(config):\n    info = ""Running with the following configs:\\n""\n    for k,v in config.items():\n        info += ""\\t{} : {}\\n"".format(k, str(v))\n    print(""\\n"" + info + ""\\n"")\n    return\n\ndef normalize_text(text):\n    return unicodedata.normalize(\'NFD\', text)\n\ndef unmap_with_copy(indices, src_tokens, vocab):\n    """"""\n    Unmap a list of list of indices, by optionally copying from src_tokens.\n    """"""\n    result = []\n    for ind, tokens in zip(indices, src_tokens):\n        words = []\n        for idx in ind:\n            if idx >= 0:\n                words.append(vocab.id2word[idx])\n            else:\n                idx = -idx - 1 # flip and minus 1\n                words.append(tokens[idx])\n        result += [words]\n    return result\n\ndef prune_decoded_seqs(seqs):\n    """"""\n    Prune decoded sequences after EOS token.\n    """"""\n    out = []\n    for s in seqs:\n        if constant.EOS in s:\n            idx = s.index(constant.EOS_TOKEN)\n            out += [s[:idx]]\n        else:\n            out += [s]\n    return out\n\ndef prune_hyp(hyp):\n    """"""\n    Prune a decoded hypothesis\n    """"""\n    if constant.EOS_ID in hyp:\n        idx = hyp.index(constant.EOS_ID)\n        return hyp[:idx]\n    else:\n        return hyp\n\ndef prune(data_list, lens):\n    assert len(data_list) == len(lens)\n    nl = []\n    for d, l in zip(data_list, lens):\n        nl.append(d[:l])\n    return nl\n\ndef sort(packed, ref, reverse=True):\n    """"""\n    Sort a series of packed list, according to a ref list.\n    Also return the original index before the sort.\n    """"""\n    assert (isinstance(packed, tuple) or isinstance(packed, list)) and isinstance(ref, list)\n    packed = [ref] + [range(len(ref))] + list(packed)\n    sorted_packed = [list(t) for t in zip(*sorted(zip(*packed), reverse=reverse))]\n    return tuple(sorted_packed[1:])\n\ndef unsort(sorted_list, oidx):\n    """"""\n    Unsort a sorted list, based on the original idx.\n    """"""\n    assert len(sorted_list) == len(oidx), ""Number of list elements must match with original indices.""\n    _, unsorted = [list(t) for t in zip(*sorted(zip(oidx, sorted_list)))]\n    return unsorted\n\ndef tensor_unsort(sorted_tensor, oidx):\n    """"""\n    Unsort a sorted tensor on its 0-th dimension, based on the original idx.\n    """"""\n    assert sorted_tensor.size(0) == len(oidx), ""Number of list elements must match with original indices.""\n    backidx = [x[0] for x in sorted(enumerate(oidx), key=lambda x: x[1])]\n    return sorted_tensor[backidx]\n'"
stanza/models/common/vocab.py,0,"b'from copy import copy\nfrom collections import Counter, OrderedDict\nimport os\nimport pickle\n\nPAD = \'<PAD>\'\nPAD_ID = 0\nUNK = \'<UNK>\'\nUNK_ID = 1\nEMPTY = \'<EMPTY>\'\nEMPTY_ID = 2\nROOT = \'<ROOT>\'\nROOT_ID = 3\nVOCAB_PREFIX = [PAD, UNK, EMPTY, ROOT]\n\nclass BaseVocab:\n    """""" A base class for common vocabulary operations. Each subclass should at least \n    implement its own build_vocab() function.""""""\n    def __init__(self, data=None, lang="""", idx=0, cutoff=0, lower=False):\n        self.data = data\n        self.lang = lang\n        self.idx = idx\n        self.cutoff = cutoff\n        self.lower = lower\n        if data is not None:\n            self.build_vocab()\n        self.state_attrs = [\'lang\', \'idx\', \'cutoff\', \'lower\', \'_unit2id\', \'_id2unit\']\n\n    def build_vocab(self):\n        raise NotImplementedError()\n\n    def state_dict(self):\n        """""" Returns a dictionary containing all states that are necessary to recover\n        this vocab. Useful for serialization.""""""\n        state = OrderedDict()\n        for attr in self.state_attrs:\n            if hasattr(self, attr):\n                state[attr] = getattr(self, attr)\n        return state\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        """""" Returns a new Vocab instance constructed from a state dict. """"""\n        new = cls()\n        for attr, value in state_dict.items():\n            setattr(new, attr, value)\n        return new\n\n    def normalize_unit(self, unit):\n        if self.lower:\n            return unit.lower()\n        return unit\n\n    def unit2id(self, unit):\n        unit = self.normalize_unit(unit)\n        if unit in self._unit2id:\n            return self._unit2id[unit]\n        else:\n            return self._unit2id[UNK]\n\n    def id2unit(self, id):\n        return self._id2unit[id]\n\n    def map(self, units):\n        return [self.unit2id(x) for x in units]\n\n    def unmap(self, ids):\n        return [self.id2unit(x) for x in ids]\n\n    def __len__(self):\n        return len(self._id2unit)\n\n    def __getitem__(self, key):\n        if isinstance(key, str):\n            return self.unit2id(key)\n        elif isinstance(key, int) or isinstance(key, list):\n            return self.id2unit(key)\n        else:\n            raise TypeError(""Vocab key must be one of str, list, or int"")\n\n    def __contains__(self, key):\n        return key in self._unit2id\n\n    @property\n    def size(self):\n        return len(self)\n\nclass CompositeVocab(BaseVocab):\n    \'\'\' Vocabulary class that handles parsing and printing composite values such as\n    compositional XPOS and universal morphological features (UFeats).\n\n    Two key options are `keyed` and `sep`. `sep` specifies the separator used between\n    different parts of the composite values, which is `|` for UFeats, for example.\n    If `keyed` is `True`, then the incoming value is treated similarly to UFeats, where\n    each part is a key/value pair separated by an equal sign (`=`). There are no inherit\n    order to the keys, and we sort them alphabetically for serialization and deserialization.\n    Whenever a part is absent, its internal value is a special `<EMPTY>` symbol that will\n    be treated accordingly when generating the output. If `keyed` is `False`, then the parts\n    are treated as positioned values, and `<EMPTY>` is used to pad parts at the end when the\n    incoming value is not long enough.\'\'\'\n\n    def __init__(self, data=None, lang="""", idx=0, sep="""", keyed=False):\n        self.sep = sep\n        self.keyed = keyed\n        super().__init__(data, lang, idx=idx)\n        self.state_attrs += [\'sep\', \'keyed\']\n\n    def unit2parts(self, unit):\n        # unpack parts of a unit\n        if self.sep == """":\n            parts = [x for x in unit]\n        else:\n            parts = unit.split(self.sep)\n        if self.keyed:\n            if len(parts) == 1 and parts[0] == \'_\':\n                return dict()\n            parts = [x.split(\'=\') for x in parts]\n\n            # Just treat multi-valued properties values as one possible value\n            parts = dict(parts)\n        elif unit == \'_\':\n            parts = []\n        return parts\n\n    def unit2id(self, unit):\n        parts = self.unit2parts(unit)\n        if self.keyed:\n            # treat multi-valued properties as singletons\n            return [self._unit2id[k].get(parts[k], UNK_ID) if k in parts else EMPTY_ID for k in self._unit2id]\n        else:\n            return [self._unit2id[i].get(parts[i], UNK_ID) if i < len(parts) else EMPTY_ID for i in range(len(self._unit2id))]\n\n    def id2unit(self, id):\n        items = []\n        for v, k in zip(id, self._id2unit.keys()):\n            if v == EMPTY_ID: continue\n            if self.keyed:\n                items.append(""{}={}"".format(k, self._id2unit[k][v]))\n            else:\n                items.append(self._id2unit[k][v])\n        res = self.sep.join(items)\n        if res == """":\n            res = ""_""\n        return res\n\n    def build_vocab(self):\n        allunits = [w[self.idx] for sent in self.data for w in sent]\n        if self.keyed:\n            self._id2unit = dict()\n\n            for u in allunits:\n                parts = self.unit2parts(u)\n                for key in parts:\n                    if key not in self._id2unit:\n                        self._id2unit[key] = copy(VOCAB_PREFIX)\n\n                    # treat multi-valued properties as singletons\n                    if parts[key] not in self._id2unit[key]:\n                        self._id2unit[key].append(parts[key])\n\n            # special handle for the case where upos/xpos/ufeats are always empty\n            if len(self._id2unit) == 0:\n                self._id2unit[\'_\'] = copy(VOCAB_PREFIX) # use an arbitrary key\n\n        else:\n            self._id2unit = dict()\n\n            allparts = [self.unit2parts(u) for u in allunits]\n            maxlen = max([len(p) for p in allparts])\n\n            for parts in allparts:\n                for i, p in enumerate(parts):\n                    if i not in self._id2unit:\n                        self._id2unit[i] = copy(VOCAB_PREFIX)\n                    if i < len(parts) and p not in self._id2unit[i]:\n                        self._id2unit[i].append(p)\n\n            # special handle for the case where upos/xpos/ufeats are always empty\n            if len(self._id2unit) == 0:\n                self._id2unit[0] = copy(VOCAB_PREFIX) # use an arbitrary key\n\n        self._id2unit = OrderedDict([(k, self._id2unit[k]) for k in sorted(self._id2unit.keys())])\n        self._unit2id = {k: {w:i for i, w in enumerate(self._id2unit[k])} for k in self._id2unit}\n\n    def lens(self):\n        return [len(self._unit2id[k]) for k in self._unit2id]\n\nclass BaseMultiVocab:\n    """""" A convenient vocab container that can store multiple BaseVocab instances, and support \n    safe serialization of all instances via state dicts. Each subclass of this base class \n    should implement the load_state_dict() function to specify how a saved state dict \n    should be loaded back.""""""\n    def __init__(self, vocab_dict=None):\n        self._vocabs = OrderedDict()\n        if vocab_dict is None:\n            return\n        # check all values provided must be a subclass of the Vocab base class\n        assert all([isinstance(v, BaseVocab) for v in vocab_dict.values()])\n        for k, v in vocab_dict.items():\n            self._vocabs[k] = v\n\n    def __setitem__(self, key, item):\n        self._vocabs[key] = item\n\n    def __getitem__(self, key):\n        return self._vocabs[key]\n\n    def state_dict(self):\n        """""" Build a state dict by iteratively calling state_dict() of all vocabs. """"""\n        state = OrderedDict()\n        for k, v in self._vocabs.items():\n            state[k] = v.state_dict()\n        return state\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        """""" Construct a MultiVocab by reading from a state dict.""""""\n        raise NotImplementedError\n\n\n\n'"
stanza/models/depparse/__init__.py,0,b''
stanza/models/depparse/data.py,2,"b'import random\nimport logging\nimport torch\n\nfrom stanza.models.common.data import map_to_ids, get_long_tensor, get_float_tensor, sort_all\nfrom stanza.models.common.vocab import PAD_ID, VOCAB_PREFIX, ROOT_ID, CompositeVocab\nfrom stanza.models.pos.vocab import CharVocab, WordVocab, XPOSVocab, FeatureVocab, MultiVocab\nfrom stanza.models.pos.xpos_vocab_factory import xpos_vocab_factory\nfrom stanza.models.common.doc import *\n\nlogger = logging.getLogger(\'stanza\')\n\nclass DataLoader:\n\n    def __init__(self, doc, batch_size, args, pretrain, vocab=None, evaluation=False, sort_during_eval=False):\n        self.batch_size = batch_size\n        self.args = args\n        self.eval = evaluation\n        self.shuffled = not self.eval\n        self.sort_during_eval = sort_during_eval\n        self.doc = doc\n        data = self.load_doc(doc)\n\n        # handle vocab\n        if vocab is None:\n            self.vocab = self.init_vocab(data)\n        else:\n            self.vocab = vocab\n        \n        # handle pretrain; pretrain vocab is used when args[\'pretrain\'] == True and pretrain is not None\n        self.pretrain_vocab = None\n        if pretrain is not None and args[\'pretrain\']:\n            self.pretrain_vocab = pretrain.vocab\n\n        # filter and sample data\n        if args.get(\'sample_train\', 1.0) < 1.0 and not self.eval:\n            keep = int(args[\'sample_train\'] * len(data))\n            data = random.sample(data, keep)\n            logger.debug(""Subsample training set with rate {:g}"".format(args[\'sample_train\']))\n\n        data = self.preprocess(data, self.vocab, self.pretrain_vocab, args)\n        # shuffle for training\n        if self.shuffled:\n            random.shuffle(data)\n        self.num_examples = len(data)\n\n        # chunk into batches\n        self.data = self.chunk_batches(data)\n        logger.debug(""{} batches created."".format(len(self.data)))\n\n    def init_vocab(self, data):\n        assert self.eval == False # for eval vocab must exist\n        charvocab = CharVocab(data, self.args[\'shorthand\'])\n        wordvocab = WordVocab(data, self.args[\'shorthand\'], cutoff=7, lower=True)\n        uposvocab = WordVocab(data, self.args[\'shorthand\'], idx=1)\n        xposvocab = xpos_vocab_factory(data, self.args[\'shorthand\'])\n        featsvocab = FeatureVocab(data, self.args[\'shorthand\'], idx=3)\n        lemmavocab = WordVocab(data, self.args[\'shorthand\'], cutoff=7, idx=4, lower=True)\n        deprelvocab = WordVocab(data, self.args[\'shorthand\'], idx=6)\n        vocab = MultiVocab({\'char\': charvocab,\n                            \'word\': wordvocab,\n                            \'upos\': uposvocab,\n                            \'xpos\': xposvocab,\n                            \'feats\': featsvocab,\n                            \'lemma\': lemmavocab,\n                            \'deprel\': deprelvocab})\n        return vocab\n\n    def preprocess(self, data, vocab, pretrain_vocab, args):\n        processed = []\n        xpos_replacement = [[ROOT_ID] * len(vocab[\'xpos\'])] if isinstance(vocab[\'xpos\'], CompositeVocab) else [ROOT_ID]\n        feats_replacement = [[ROOT_ID] * len(vocab[\'feats\'])]\n        for sent in data:\n            processed_sent = [[ROOT_ID] + vocab[\'word\'].map([w[0] for w in sent])]\n            processed_sent += [[[ROOT_ID]] + [vocab[\'char\'].map([x for x in w[0]]) for w in sent]]\n            processed_sent += [[ROOT_ID] + vocab[\'upos\'].map([w[1] for w in sent])]\n            processed_sent += [xpos_replacement + vocab[\'xpos\'].map([w[2] for w in sent])]\n            processed_sent += [feats_replacement + vocab[\'feats\'].map([w[3] for w in sent])]\n            if pretrain_vocab is not None:\n                # always use lowercase lookup in pretrained vocab\n                processed_sent += [[ROOT_ID] + pretrain_vocab.map([w[0].lower() for w in sent])]\n            else:\n                processed_sent += [[ROOT_ID] + [PAD_ID] * len(sent)]\n            processed_sent += [[ROOT_ID] + vocab[\'lemma\'].map([w[4] for w in sent])]\n            processed_sent += [[to_int(w[5], ignore_error=self.eval) for w in sent]]\n            processed_sent += [vocab[\'deprel\'].map([w[6] for w in sent])]\n            processed.append(processed_sent)\n        return processed\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, key):\n        """""" Get a batch with index. """"""\n        if not isinstance(key, int):\n            raise TypeError\n        if key < 0 or key >= len(self.data):\n            raise IndexError\n        batch = self.data[key]\n        batch_size = len(batch)\n        batch = list(zip(*batch))\n        assert len(batch) == 9\n\n        # sort sentences by lens for easy RNN operations\n        lens = [len(x) for x in batch[0]]\n        batch, orig_idx = sort_all(batch, lens)\n\n        # sort words by lens for easy char-RNN operations\n        batch_words = [w for sent in batch[1] for w in sent]\n        word_lens = [len(x) for x in batch_words]\n        batch_words, word_orig_idx = sort_all([batch_words], word_lens)\n        batch_words = batch_words[0]\n        word_lens = [len(x) for x in batch_words]\n\n        # convert to tensors\n        words = batch[0]\n        words = get_long_tensor(words, batch_size)\n        words_mask = torch.eq(words, PAD_ID)\n        wordchars = get_long_tensor(batch_words, len(word_lens))\n        wordchars_mask = torch.eq(wordchars, PAD_ID)\n\n        upos = get_long_tensor(batch[2], batch_size)\n        xpos = get_long_tensor(batch[3], batch_size)\n        ufeats = get_long_tensor(batch[4], batch_size)\n        pretrained = get_long_tensor(batch[5], batch_size)\n        sentlens = [len(x) for x in batch[0]]\n        lemma = get_long_tensor(batch[6], batch_size)\n        head = get_long_tensor(batch[7], batch_size)\n        deprel = get_long_tensor(batch[8], batch_size)\n        return words, words_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, orig_idx, word_orig_idx, sentlens, word_lens\n\n    def load_doc(self, doc):\n        data = doc.get([TEXT, UPOS, XPOS, FEATS, LEMMA, HEAD, DEPREL], as_sentences=True)\n        data = self.resolve_none(data)\n        return data\n\n    def resolve_none(self, data):\n        # replace None to \'_\'\n        for sent_idx in range(len(data)):\n            for tok_idx in range(len(data[sent_idx])):\n                for feat_idx in range(len(data[sent_idx][tok_idx])):\n                    if data[sent_idx][tok_idx][feat_idx] is None:\n                        data[sent_idx][tok_idx][feat_idx] = \'_\'\n        return data\n\n    def __iter__(self):\n        for i in range(self.__len__()):\n            yield self.__getitem__(i)\n\n    def reshuffle(self):\n        data = [y for x in self.data for y in x]\n        self.data = self.chunk_batches(data)\n        random.shuffle(self.data)\n\n    def chunk_batches(self, data):\n        res = []\n\n        if not self.eval:\n            # sort sentences (roughly) by length for better memory utilization\n            data = sorted(data, key = lambda x: len(x[0]), reverse=random.random() > .5)\n        elif self.sort_during_eval:\n            (data, ), self.data_orig_idx = sort_all([data], [len(x[0]) for x in data])\n\n        current = []\n        currentlen = 0\n        for x in data:\n            if len(x[0]) + currentlen > self.batch_size:\n                res.append(current)\n                current = []\n                currentlen = 0\n            current.append(x)\n            currentlen += len(x[0])\n\n        if currentlen > 0:\n            res.append(current)\n\n        return res\n\ndef to_int(string, ignore_error=False):\n    try:\n        res = int(string)\n    except ValueError as err:\n        if ignore_error:\n            return 0\n        else:\n            raise err\n    return res\n'"
stanza/models/depparse/model.py,20,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence\n\nfrom stanza.models.common.biaffine import DeepBiaffineScorer\nfrom stanza.models.common.hlstm import HighwayLSTM\nfrom stanza.models.common.dropout import WordDropout\nfrom stanza.models.common.vocab import CompositeVocab\nfrom stanza.models.common.char_model import CharacterModel\n\nclass Parser(nn.Module):\n    def __init__(self, args, vocab, emb_matrix=None, share_hid=False):\n        super().__init__()\n\n        self.vocab = vocab\n        self.args = args\n        self.share_hid = share_hid\n        self.unsaved_modules = []\n\n        def add_unsaved_module(name, module):\n            self.unsaved_modules += [name]\n            setattr(self, name, module)\n\n        # input layers\n        input_size = 0\n        if self.args['word_emb_dim'] > 0:\n            # frequent word embeddings\n            self.word_emb = nn.Embedding(len(vocab['word']), self.args['word_emb_dim'], padding_idx=0)\n            self.lemma_emb = nn.Embedding(len(vocab['lemma']), self.args['word_emb_dim'], padding_idx=0)\n            input_size += self.args['word_emb_dim'] * 2\n\n        if self.args['tag_emb_dim'] > 0:\n            self.upos_emb = nn.Embedding(len(vocab['upos']), self.args['tag_emb_dim'], padding_idx=0)\n\n            if not isinstance(vocab['xpos'], CompositeVocab):\n                self.xpos_emb = nn.Embedding(len(vocab['xpos']), self.args['tag_emb_dim'], padding_idx=0)\n            else:\n                self.xpos_emb = nn.ModuleList()\n\n                for l in vocab['xpos'].lens():\n                    self.xpos_emb.append(nn.Embedding(l, self.args['tag_emb_dim'], padding_idx=0))\n\n            self.ufeats_emb = nn.ModuleList()\n\n            for l in vocab['feats'].lens():\n                self.ufeats_emb.append(nn.Embedding(l, self.args['tag_emb_dim'], padding_idx=0))\n\n            input_size += self.args['tag_emb_dim'] * 2\n\n        if self.args['char'] and self.args['char_emb_dim'] > 0:\n            self.charmodel = CharacterModel(args, vocab)\n            self.trans_char = nn.Linear(self.args['char_hidden_dim'], self.args['transformed_dim'], bias=False)\n            input_size += self.args['transformed_dim']\n\n        if self.args['pretrain']:\n            # pretrained embeddings, by default this won't be saved into model file\n            add_unsaved_module('pretrained_emb', nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True))\n            self.trans_pretrained = nn.Linear(emb_matrix.shape[1], self.args['transformed_dim'], bias=False)\n            input_size += self.args['transformed_dim']\n\n        # recurrent layers\n        self.parserlstm = HighwayLSTM(input_size, self.args['hidden_dim'], self.args['num_layers'], batch_first=True, bidirectional=True, dropout=self.args['dropout'], rec_dropout=self.args['rec_dropout'], highway_func=torch.tanh)\n        self.drop_replacement = nn.Parameter(torch.randn(input_size) / np.sqrt(input_size))\n        self.parserlstm_h_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))\n        self.parserlstm_c_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))\n\n        # classifiers\n        self.unlabeled = DeepBiaffineScorer(2 * self.args['hidden_dim'], 2 * self.args['hidden_dim'], self.args['deep_biaff_hidden_dim'], 1, pairwise=True, dropout=args['dropout'])\n        self.deprel = DeepBiaffineScorer(2 * self.args['hidden_dim'], 2 * self.args['hidden_dim'], self.args['deep_biaff_hidden_dim'], len(vocab['deprel']), pairwise=True, dropout=args['dropout'])\n        if args['linearization']:\n            self.linearization = DeepBiaffineScorer(2 * self.args['hidden_dim'], 2 * self.args['hidden_dim'], self.args['deep_biaff_hidden_dim'], 1, pairwise=True, dropout=args['dropout'])\n        if args['distance']:\n            self.distance = DeepBiaffineScorer(2 * self.args['hidden_dim'], 2 * self.args['hidden_dim'], self.args['deep_biaff_hidden_dim'], 1, pairwise=True, dropout=args['dropout'])\n\n        # criterion\n        self.crit = nn.CrossEntropyLoss(ignore_index=-1, reduction='sum') # ignore padding\n\n        self.drop = nn.Dropout(args['dropout'])\n        self.worddrop = WordDropout(args['word_dropout'])\n\n    def forward(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, word_orig_idx, sentlens, wordlens):\n        def pack(x):\n            return pack_padded_sequence(x, sentlens, batch_first=True)\n\n        inputs = []\n        if self.args['pretrain']:\n            pretrained_emb = self.pretrained_emb(pretrained)\n            pretrained_emb = self.trans_pretrained(pretrained_emb)\n            pretrained_emb = pack(pretrained_emb)\n            inputs += [pretrained_emb]\n\n        #def pad(x):\n        #    return pad_packed_sequence(PackedSequence(x, pretrained_emb.batch_sizes), batch_first=True)[0]\n\n        if self.args['word_emb_dim'] > 0:\n            word_emb = self.word_emb(word)\n            word_emb = pack(word_emb)\n            lemma_emb = self.lemma_emb(lemma)\n            lemma_emb = pack(lemma_emb)\n            inputs += [word_emb, lemma_emb]\n\n        if self.args['tag_emb_dim'] > 0:\n            pos_emb = self.upos_emb(upos)\n\n            if isinstance(self.vocab['xpos'], CompositeVocab):\n                for i in range(len(self.vocab['xpos'])):\n                    pos_emb += self.xpos_emb[i](xpos[:, :, i])\n            else:\n                pos_emb += self.xpos_emb(xpos)\n            pos_emb = pack(pos_emb)\n\n            feats_emb = 0\n            for i in range(len(self.vocab['feats'])):\n                feats_emb += self.ufeats_emb[i](ufeats[:, :, i])\n            feats_emb = pack(feats_emb)\n\n            inputs += [pos_emb, feats_emb]\n\n        if self.args['char'] and self.args['char_emb_dim'] > 0:\n            char_reps = self.charmodel(wordchars, wordchars_mask, word_orig_idx, sentlens, wordlens)\n            char_reps = PackedSequence(self.trans_char(self.drop(char_reps.data)), char_reps.batch_sizes)\n            inputs += [char_reps]\n\n        lstm_inputs = torch.cat([x.data for x in inputs], 1)\n\n        lstm_inputs = self.worddrop(lstm_inputs, self.drop_replacement)\n        lstm_inputs = self.drop(lstm_inputs)\n\n        lstm_inputs = PackedSequence(lstm_inputs, inputs[0].batch_sizes)\n\n        lstm_outputs, _ = self.parserlstm(lstm_inputs, sentlens, hx=(self.parserlstm_h_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous(), self.parserlstm_c_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous()))\n        lstm_outputs, _ = pad_packed_sequence(lstm_outputs, batch_first=True)\n\n        unlabeled_scores = self.unlabeled(self.drop(lstm_outputs), self.drop(lstm_outputs)).squeeze(3)\n        deprel_scores = self.deprel(self.drop(lstm_outputs), self.drop(lstm_outputs))\n\n        #goldmask = head.new_zeros(*head.size(), head.size(-1)+1, dtype=torch.uint8)\n        #goldmask.scatter_(2, head.unsqueeze(2), 1)\n\n        if self.args['linearization'] or self.args['distance']:\n            head_offset = torch.arange(word.size(1), device=head.device).view(1, 1, -1).expand(word.size(0), -1, -1) - torch.arange(word.size(1), device=head.device).view(1, -1, 1).expand(word.size(0), -1, -1)\n\n        if self.args['linearization']:\n            lin_scores = self.linearization(self.drop(lstm_outputs), self.drop(lstm_outputs)).squeeze(3)\n            unlabeled_scores += F.logsigmoid(lin_scores * torch.sign(head_offset).float()).detach()\n\n        if self.args['distance']:\n            dist_scores = self.distance(self.drop(lstm_outputs), self.drop(lstm_outputs)).squeeze(3)\n            dist_pred = 1 + F.softplus(dist_scores)\n            dist_target = torch.abs(head_offset)\n            dist_kld = -torch.log((dist_target.float() - dist_pred)**2/2 + 1)\n            unlabeled_scores += dist_kld.detach()\n\n        diag = torch.eye(head.size(-1)+1, dtype=torch.bool, device=head.device).unsqueeze(0)\n        unlabeled_scores.masked_fill_(diag, -float('inf'))\n\n        preds = []\n\n        if self.training:\n            unlabeled_scores = unlabeled_scores[:, 1:, :] # exclude attachment for the root symbol\n            unlabeled_scores = unlabeled_scores.masked_fill(word_mask.unsqueeze(1), -float('inf'))\n            unlabeled_target = head.masked_fill(word_mask[:, 1:], -1)\n            loss = self.crit(unlabeled_scores.contiguous().view(-1, unlabeled_scores.size(2)), unlabeled_target.view(-1))\n\n            deprel_scores = deprel_scores[:, 1:] # exclude attachment for the root symbol\n            #deprel_scores = deprel_scores.masked_select(goldmask.unsqueeze(3)).view(-1, len(self.vocab['deprel']))\n            deprel_scores = torch.gather(deprel_scores, 2, head.unsqueeze(2).unsqueeze(3).expand(-1, -1, -1, len(self.vocab['deprel']))).view(-1, len(self.vocab['deprel']))\n            deprel_target = deprel.masked_fill(word_mask[:, 1:], -1)\n            loss += self.crit(deprel_scores.contiguous(), deprel_target.view(-1))\n\n            if self.args['linearization']:\n                #lin_scores = lin_scores[:, 1:].masked_select(goldmask)\n                lin_scores = torch.gather(lin_scores[:, 1:], 2, head.unsqueeze(2)).view(-1)\n                lin_scores = torch.cat([-lin_scores.unsqueeze(1)/2, lin_scores.unsqueeze(1)/2], 1)\n                #lin_target = (head_offset[:, 1:] > 0).long().masked_select(goldmask)\n                lin_target = torch.gather((head_offset[:, 1:] > 0).long(), 2, head.unsqueeze(2))\n                loss += self.crit(lin_scores.contiguous(), lin_target.view(-1))\n\n            if self.args['distance']:\n                #dist_kld = dist_kld[:, 1:].masked_select(goldmask)\n                dist_kld = torch.gather(dist_kld[:, 1:], 2, head.unsqueeze(2))\n                loss -= dist_kld.sum()\n\n            loss /= wordchars.size(0) # number of words\n        else:\n            loss = 0\n            preds.append(F.log_softmax(unlabeled_scores, 2).detach().cpu().numpy())\n            preds.append(deprel_scores.max(3)[1].detach().cpu().numpy())\n\n        return loss, preds\n"""
stanza/models/depparse/scorer.py,0,"b'""""""\nUtils and wrappers for scoring parsers.\n""""""\nimport logging\n\nfrom stanza.models.common.utils import ud_scores\n\nlogger = logging.getLogger(\'stanza\')\n\ndef score(system_conllu_file, gold_conllu_file, verbose=True):\n    """""" Wrapper for UD parser scorer. """"""\n    evaluation = ud_scores(gold_conllu_file, system_conllu_file)\n    el = evaluation[\'LAS\']\n    p = el.precision\n    r = el.recall\n    f = el.f1\n    if verbose:\n        scores = [evaluation[k].f1 * 100 for k in [\'LAS\', \'MLAS\', \'BLEX\']]\n        logger.info(""LAS\\tMLAS\\tBLEX"")\n        logger.info(""{:.2f}\\t{:.2f}\\t{:.2f}"".format(*scores))\n    return p, r, f\n\n'"
stanza/models/depparse/trainer.py,3,"b'""""""\nA trainer class to handle training and testing of models.\n""""""\n\nimport sys\nimport logging\nimport torch\nfrom torch import nn\n\nfrom stanza.models.common.trainer import Trainer as BaseTrainer\nfrom stanza.models.common import utils, loss\nfrom stanza.models.common.chuliu_edmonds import chuliu_edmonds_one_root\nfrom stanza.models.depparse.model import Parser\nfrom stanza.models.pos.vocab import MultiVocab\n\nlogger = logging.getLogger(\'stanza\')\n\ndef unpack_batch(batch, use_cuda):\n    """""" Unpack a batch from the data loader. """"""\n    if use_cuda:\n        inputs = [b.cuda() if b is not None else None for b in batch[:11]]\n    else:\n        inputs = batch[:11]\n    orig_idx = batch[11]\n    word_orig_idx = batch[12]\n    sentlens = batch[13]\n    wordlens = batch[14]\n    return inputs, orig_idx, word_orig_idx, sentlens, wordlens\n\nclass Trainer(BaseTrainer):\n    """""" A trainer for training models. """"""\n    def __init__(self, args=None, vocab=None, pretrain=None, model_file=None, use_cuda=False):\n        self.use_cuda = use_cuda\n        if model_file is not None:\n            # load everything from file\n            self.load(model_file, pretrain)\n        else:\n            # build model from scratch\n            self.args = args\n            self.vocab = vocab\n            self.model = Parser(args, vocab, emb_matrix=pretrain.emb if pretrain is not None else None)\n        self.parameters = [p for p in self.model.parameters() if p.requires_grad]\n        if self.use_cuda:\n            self.model.cuda()\n        else:\n            self.model.cpu()\n        self.optimizer = utils.get_optimizer(self.args[\'optim\'], self.parameters, self.args[\'lr\'], betas=(0.9, self.args[\'beta2\']), eps=1e-6)\n\n    def update(self, batch, eval=False):\n        inputs, orig_idx, word_orig_idx, sentlens, wordlens = unpack_batch(batch, self.use_cuda)\n        word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel = inputs\n\n        if eval:\n            self.model.eval()\n        else:\n            self.model.train()\n            self.optimizer.zero_grad()\n        loss, _ = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, word_orig_idx, sentlens, wordlens)\n        loss_val = loss.data.item()\n        if eval:\n            return loss_val\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args[\'max_grad_norm\'])\n        self.optimizer.step()\n        return loss_val\n\n    def predict(self, batch, unsort=True):\n        inputs, orig_idx, word_orig_idx, sentlens, wordlens = unpack_batch(batch, self.use_cuda)\n        word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel = inputs\n\n        self.model.eval()\n        batch_size = word.size(0)\n        _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, word_orig_idx, sentlens, wordlens)\n        head_seqs = [chuliu_edmonds_one_root(adj[:l, :l])[1:] for adj, l in zip(preds[0], sentlens)] # remove attachment for the root\n        deprel_seqs = [self.vocab[\'deprel\'].unmap([preds[1][i][j+1][h] for j, h in enumerate(hs)]) for i, hs in enumerate(head_seqs)]\n\n        pred_tokens = [[[str(head_seqs[i][j]), deprel_seqs[i][j]] for j in range(sentlens[i]-1)] for i in range(batch_size)]\n        if unsort:\n            pred_tokens = utils.unsort(pred_tokens, orig_idx)\n        return pred_tokens\n\n    def save(self, filename, skip_modules=True):\n        model_state = self.model.state_dict()\n        # skip saving modules like pretrained embeddings, because they are large and will be saved in a separate file\n        if skip_modules:\n            skipped = [k for k in model_state.keys() if k.split(\'.\')[0] in self.model.unsaved_modules]\n            for k in skipped:\n                del model_state[k]\n        params = {\n                \'model\': model_state,\n                \'vocab\': self.vocab.state_dict(),\n                \'config\': self.args\n                }\n        try:\n            torch.save(params, filename)\n            logger.info(""Model saved to {}"".format(filename))\n        except BaseException:\n            logger.warning(""Saving failed... continuing anyway."")\n\n    def load(self, filename, pretrain):\n        """"""\n        Load a model from file, with preloaded pretrain embeddings. Here we allow the pretrain to be None or a dummy input,\n        and the actual use of pretrain embeddings will depend on the boolean config ""pretrain"" in the loaded args.\n        """"""\n        try:\n            checkpoint = torch.load(filename, lambda storage, loc: storage)\n        except BaseException:\n            logger.exception(""Cannot load model from {}"".format(filename))\n            sys.exit(1)\n        self.args = checkpoint[\'config\']\n        self.vocab = MultiVocab.load_state_dict(checkpoint[\'vocab\'])\n        # load model\n        emb_matrix = None\n        if self.args[\'pretrain\'] and pretrain is not None: # we use pretrain only if args[\'pretrain\'] == True and pretrain is not None\n            emb_matrix = pretrain.emb\n        self.model = Parser(self.args, self.vocab, emb_matrix=emb_matrix)\n        self.model.load_state_dict(checkpoint[\'model\'], strict=False)\n\n'"
stanza/models/lemma/__init__.py,0,b''
stanza/models/lemma/data.py,3,"b'import random\nimport numpy as np\nimport os\nfrom collections import Counter\nimport logging\nimport torch\n\nimport stanza.models.common.seq2seq_constant as constant\nfrom stanza.models.common.data import map_to_ids, get_long_tensor, get_float_tensor, sort_all\nfrom stanza.models.lemma.vocab import Vocab, MultiVocab\nfrom stanza.models.lemma import edit\nfrom stanza.models.common.doc import *\n\nlogger = logging.getLogger(\'stanza\')\n\nclass DataLoader:\n    def __init__(self, doc, batch_size, args, vocab=None, evaluation=False, conll_only=False, skip=None):\n        self.batch_size = batch_size\n        self.args = args\n        self.eval = evaluation\n        self.shuffled = not self.eval\n        self.doc = doc\n\n        data = self.load_doc(self.doc)\n\n        if conll_only: # only load conll file\n            return\n\n        if skip is not None:\n            assert len(data) == len(skip)\n            data = [x for x, y in zip(data, skip) if not y]\n\n        # handle vocab\n        if vocab is not None:\n            self.vocab = vocab\n        else:\n            self.vocab = dict()\n            char_vocab, pos_vocab = self.init_vocab(data)\n            self.vocab = MultiVocab({\'char\': char_vocab, \'pos\': pos_vocab})\n\n        # filter and sample data\n        if args.get(\'sample_train\', 1.0) < 1.0 and not self.eval:\n            keep = int(args[\'sample_train\'] * len(data))\n            data = random.sample(data, keep)\n            logger.debug(""Subsample training set with rate {:g}"".format(args[\'sample_train\']))\n\n        data = self.preprocess(data, self.vocab[\'char\'], self.vocab[\'pos\'], args)\n        # shuffle for training\n        if self.shuffled:\n            indices = list(range(len(data)))\n            random.shuffle(indices)\n            data = [data[i] for i in indices]\n        self.num_examples = len(data)\n\n        # chunk into batches\n        data = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]\n        self.data = data\n        logger.debug(""{} batches created."".format(len(data)))\n\n    def init_vocab(self, data):\n        assert self.eval is False, ""Vocab file must exist for evaluation""\n        char_data = """".join(d[0] + d[2] for d in data)\n        char_vocab = Vocab(char_data, self.args[\'lang\'])\n        pos_data = [d[1] for d in data]\n        pos_vocab = Vocab(pos_data, self.args[\'lang\'])\n        return char_vocab, pos_vocab\n\n    def preprocess(self, data, char_vocab, pos_vocab, args):\n        processed = []\n        for d in data:\n            edit_type = edit.EDIT_TO_ID[edit.get_edit_type(d[0], d[2])]\n            src = list(d[0])\n            src = [constant.SOS] + src + [constant.EOS]\n            src = char_vocab.map(src)\n            pos = d[1]\n            pos = pos_vocab.unit2id(pos)\n            tgt = list(d[2])\n            tgt_in = char_vocab.map([constant.SOS] + tgt)\n            tgt_out = char_vocab.map(tgt + [constant.EOS])\n            processed += [[src, tgt_in, tgt_out, pos, edit_type]]\n        return processed\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, key):\n        """""" Get a batch with index. """"""\n        if not isinstance(key, int):\n            raise TypeError\n        if key < 0 or key >= len(self.data):\n            raise IndexError\n        batch = self.data[key]\n        batch_size = len(batch)\n        batch = list(zip(*batch))\n        assert len(batch) == 5\n\n        # sort all fields by lens for easy RNN operations\n        lens = [len(x) for x in batch[0]]\n        batch, orig_idx = sort_all(batch, lens)\n\n        # convert to tensors\n        src = batch[0]\n        src = get_long_tensor(src, batch_size)\n        src_mask = torch.eq(src, constant.PAD_ID)\n        tgt_in = get_long_tensor(batch[1], batch_size)\n        tgt_out = get_long_tensor(batch[2], batch_size)\n        pos = torch.LongTensor(batch[3])\n        edits = torch.LongTensor(batch[4])\n        assert tgt_in.size(1) == tgt_out.size(1), ""Target input and output sequence sizes do not match.""\n        return src, src_mask, tgt_in, tgt_out, pos, edits, orig_idx\n\n    def __iter__(self):\n        for i in range(self.__len__()):\n            yield self.__getitem__(i)\n\n    def load_doc(self, doc):\n        data = doc.get([TEXT, UPOS, LEMMA])\n        data = self.resolve_none(data)\n        return data\n\n    def resolve_none(self, data):\n        # replace None to \'_\'\n        for tok_idx in range(len(data)):\n            for feat_idx in range(len(data[tok_idx])):\n                if data[tok_idx][feat_idx] is None:\n                    data[tok_idx][feat_idx] = \'_\'\n        return data'"
stanza/models/lemma/edit.py,0,"b'""""""\nUtilities for calculating edits between word and lemma forms.\n""""""\n\nEDIT_TO_ID = {\'none\': 0, \'identity\': 1, \'lower\': 2}\n\ndef get_edit_type(word, lemma):\n    """""" Calculate edit types. """"""\n    if lemma == word:\n        return \'identity\'\n    elif lemma == word.lower():\n        return \'lower\'\n    return \'none\'\n\ndef edit_word(word, pred, edit_id):\n    """"""\n    Edit a word, given edit and seq2seq predictions.\n    """"""\n    if edit_id == 1:\n        return word\n    elif edit_id == 2:\n        return word.lower()\n    elif edit_id == 0:\n        return pred\n    else:\n        raise Exception(""Unrecognized edit ID: {}"".format(edit_id))\n\n'"
stanza/models/lemma/scorer.py,0,"b'""""""\nUtils and wrappers for scoring lemmatizers.\n""""""\nfrom stanza.utils import conll18_ud_eval as ud_eval\n\ndef score(system_conllu_file, gold_conllu_file):\n    """""" Wrapper for lemma scorer. """"""\n    gold_ud = ud_eval.load_conllu_file(gold_conllu_file)\n    system_ud = ud_eval.load_conllu_file(system_conllu_file)\n    evaluation = ud_eval.evaluate(gold_ud, system_ud)\n    el = evaluation[""Lemmas""]\n    p, r, f = el.precision, el.recall, el.f1\n    return p, r, f\n\n'"
stanza/models/lemma/trainer.py,4,"b'""""""\nA trainer class to handle training and testing of models.\n""""""\n\nimport sys\nimport numpy as np\nfrom collections import Counter\nimport logging\nimport torch\nfrom torch import nn\nimport torch.nn.init as init\n\nimport stanza.models.common.seq2seq_constant as constant\nfrom stanza.models.common.seq2seq_model import Seq2SeqModel\nfrom stanza.models.common import utils, loss\nfrom stanza.models.lemma import edit\nfrom stanza.models.lemma.vocab import MultiVocab\n\nlogger = logging.getLogger(\'stanza\')\n\ndef unpack_batch(batch, use_cuda):\n    """""" Unpack a batch from the data loader. """"""\n    if use_cuda:\n        inputs = [b.cuda() if b is not None else None for b in batch[:6]]\n    else:\n        inputs = [b if b is not None else None for b in batch[:6]]\n    orig_idx = batch[6]\n    return inputs, orig_idx\n\nclass Trainer(object):\n    """""" A trainer for training models. """"""\n    def __init__(self, args=None, vocab=None, emb_matrix=None, model_file=None, use_cuda=False):\n        self.use_cuda = use_cuda\n        if model_file is not None:\n            # load everything from file\n            self.load(model_file, use_cuda)\n        else:\n            # build model from scratch\n            self.args = args\n            self.model = None if args[\'dict_only\'] else Seq2SeqModel(args, emb_matrix=emb_matrix, use_cuda=use_cuda)\n            self.vocab = vocab\n            # dict-based components\n            self.word_dict = dict()\n            self.composite_dict = dict()\n        if not self.args[\'dict_only\']:\n            if self.args.get(\'edit\', False):\n                self.crit = loss.MixLoss(self.vocab[\'char\'].size, self.args[\'alpha\'])\n                logger.debug(""Running seq2seq lemmatizer with edit classifier..."")\n            else:\n                self.crit = loss.SequenceLoss(self.vocab[\'char\'].size)\n            self.parameters = [p for p in self.model.parameters() if p.requires_grad]\n            if use_cuda:\n                self.model.cuda()\n                self.crit.cuda()\n            else:\n                self.model.cpu()\n                self.crit.cpu()\n            self.optimizer = utils.get_optimizer(self.args[\'optim\'], self.parameters, self.args[\'lr\'])\n\n    def update(self, batch, eval=False):\n        inputs, orig_idx = unpack_batch(batch, self.use_cuda)\n        src, src_mask, tgt_in, tgt_out, pos, edits = inputs\n\n        if eval:\n            self.model.eval()\n        else:\n            self.model.train()\n            self.optimizer.zero_grad()\n        log_probs, edit_logits = self.model(src, src_mask, tgt_in, pos)\n        if self.args.get(\'edit\', False):\n            assert edit_logits is not None\n            loss = self.crit(log_probs.view(-1, self.vocab[\'char\'].size), tgt_out.view(-1), \\\n                    edit_logits, edits)\n        else:\n            loss = self.crit(log_probs.view(-1, self.vocab[\'char\'].size), tgt_out.view(-1))\n        loss_val = loss.data.item()\n        if eval:\n            return loss_val\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args[\'max_grad_norm\'])\n        self.optimizer.step()\n        return loss_val\n\n    def predict(self, batch, beam_size=1):\n        inputs, orig_idx = unpack_batch(batch, self.use_cuda)\n        src, src_mask, tgt, tgt_mask, pos, edits = inputs\n\n        self.model.eval()\n        batch_size = src.size(0)\n        preds, edit_logits = self.model.predict(src, src_mask, pos=pos, beam_size=beam_size)\n        pred_seqs = [self.vocab[\'char\'].unmap(ids) for ids in preds] # unmap to tokens\n        pred_seqs = utils.prune_decoded_seqs(pred_seqs)\n        pred_tokens = ["""".join(seq) for seq in pred_seqs] # join chars to be tokens\n        pred_tokens = utils.unsort(pred_tokens, orig_idx)\n        if self.args.get(\'edit\', False):\n            assert edit_logits is not None\n            edits = np.argmax(edit_logits.data.cpu().numpy(), axis=1).reshape([batch_size]).tolist()\n            edits = utils.unsort(edits, orig_idx)\n        else:\n            edits = None\n        return pred_tokens, edits\n\n    def postprocess(self, words, preds, edits=None):\n        """""" Postprocess, mainly for handing edits. """"""\n        assert len(words) == len(preds), ""Lemma predictions must have same length as words.""\n        edited = []\n        if self.args.get(\'edit\', False):\n            assert edits is not None and len(words) == len(edits)\n            for w, p, e in zip(words, preds, edits):\n                lem = edit.edit_word(w, p, e)\n                edited += [lem]\n        else:\n            edited = preds # do not edit\n        # final sanity check\n        assert len(edited) == len(words)\n        final = []\n        for lem, w in zip(edited, words):\n            if len(lem) == 0 or constant.UNK in lem:\n                final += [w] # invalid prediction, fall back to word\n            else:\n                final += [lem]\n        return final\n\n    def update_lr(self, new_lr):\n        utils.change_lr(self.optimizer, new_lr)\n\n    def train_dict(self, triples):\n        """""" Train a dict lemmatizer given training (word, pos, lemma) triples. """"""\n        # accumulate counter\n        ctr = Counter()\n        ctr.update([(p[0], p[1], p[2]) for p in triples])\n        # find the most frequent mappings\n        for p, _ in ctr.most_common():\n            w, pos, l = p\n            if (w,pos) not in self.composite_dict:\n                self.composite_dict[(w,pos)] = l\n            if w not in self.word_dict:\n                self.word_dict[w] = l\n        return\n\n    def predict_dict(self, pairs):\n        """""" Predict a list of lemmas using the dict model given (word, pos) pairs. """"""\n        lemmas = []\n        for p in pairs:\n            w, pos = p\n            if (w,pos) in self.composite_dict:\n                lemmas += [self.composite_dict[(w,pos)]]\n            elif w in self.word_dict:\n                lemmas += [self.word_dict[w]]\n            else:\n                lemmas += [w]\n        return lemmas\n\n    def skip_seq2seq(self, pairs):\n        """""" Determine if we can skip the seq2seq module when ensembling with the frequency lexicon. """"""\n\n        skip = []\n        for p in pairs:\n            w, pos = p\n            if (w,pos) in self.composite_dict:\n                skip.append(True)\n            elif w in self.word_dict:\n                skip.append(True)\n            else:\n                skip.append(False)\n        return skip\n\n    def ensemble(self, pairs, other_preds):\n        """""" Ensemble the dict with statistical model predictions. """"""\n        lemmas = []\n        assert len(pairs) == len(other_preds)\n        for p, pred in zip(pairs, other_preds):\n            w, pos = p\n            if (w,pos) in self.composite_dict:\n                lemma = self.composite_dict[(w,pos)]\n            elif w in self.word_dict:\n                lemma = self.word_dict[w]\n            else:\n                lemma = pred\n            if lemma is None:\n                lemma = w\n            lemmas.append(lemma)\n        return lemmas\n\n    def save(self, filename):\n        params = {\n                \'model\': self.model.state_dict() if self.model is not None else None,\n                \'dicts\': (self.word_dict, self.composite_dict),\n                \'vocab\': self.vocab.state_dict(),\n                \'config\': self.args\n                }\n        try:\n            torch.save(params, filename)\n            logger.info(""Model saved to {}"".format(filename))\n        except BaseException:\n            logger.warning(""Saving failed... continuing anyway."")\n\n    def load(self, filename, use_cuda=False):\n        try:\n            checkpoint = torch.load(filename, lambda storage, loc: storage)\n        except BaseException:\n            logger.exception(""Cannot load model from {}"".format(filename))\n            sys.exit(1)\n        self.args = checkpoint[\'config\']\n        self.word_dict, self.composite_dict = checkpoint[\'dicts\']\n        if not self.args[\'dict_only\']:\n            self.model = Seq2SeqModel(self.args, use_cuda=use_cuda)\n            self.model.load_state_dict(checkpoint[\'model\'])\n        else:\n            self.model = None\n        self.vocab = MultiVocab.load_state_dict(checkpoint[\'vocab\'])\n'"
stanza/models/lemma/vocab.py,0,"b'from collections import Counter\n\nfrom stanza.models.common.vocab import BaseVocab, BaseMultiVocab\nfrom stanza.models.common.seq2seq_constant import VOCAB_PREFIX\n\nclass Vocab(BaseVocab):\n    def build_vocab(self):\n        counter = Counter(self.data)\n        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))\n        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}\n\nclass MultiVocab(BaseMultiVocab):\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        new = cls()\n        for k,v in state_dict.items():\n            new[k] = Vocab.load_state_dict(v)\n        return new\n'"
stanza/models/mwt/__init__.py,0,b''
stanza/models/mwt/data.py,1,"b'import random\nimport numpy as np\nimport os\nfrom collections import Counter\nimport logging\nimport torch\n\nimport stanza.models.common.seq2seq_constant as constant\nfrom stanza.models.common.data import map_to_ids, get_long_tensor, get_float_tensor, sort_all\nfrom stanza.models.mwt.vocab import Vocab\nfrom stanza.models.common.doc import Document\n\nlogger = logging.getLogger(\'stanza\')\n\nclass DataLoader:\n    def __init__(self, doc, batch_size, args, vocab=None, evaluation=False):\n        self.batch_size = batch_size\n        self.args = args\n        self.eval = evaluation\n        self.shuffled = not self.eval\n        self.doc = doc\n\n        data = self.load_doc(self.doc, evaluation=self.eval)\n\n        # handle vocab\n        if vocab is None:\n            self.vocab = self.init_vocab(data)\n        else:\n            self.vocab = vocab\n\n        # filter and sample data\n        if args.get(\'sample_train\', 1.0) < 1.0 and not self.eval:\n            keep = int(args[\'sample_train\'] * len(data))\n            data = random.sample(data, keep)\n            logger.debug(""Subsample training set with rate {:g}"".format(args[\'sample_train\']))\n\n        data = self.preprocess(data, self.vocab, args)\n        # shuffle for training\n        if self.shuffled:\n            indices = list(range(len(data)))\n            random.shuffle(indices)\n            data = [data[i] for i in indices]\n        self.num_examples = len(data)\n\n        # chunk into batches\n        data = [data[i:i+batch_size] for i in range(0, len(data), batch_size)]\n        self.data = data\n        logger.debug(""{} batches created."".format(len(data)))\n\n    def init_vocab(self, data):\n        assert self.eval == False # for eval vocab must exist\n        vocab = Vocab(data, self.args[\'shorthand\'])\n        return vocab\n\n    def preprocess(self, data, vocab, args):\n        processed = []\n        for d in data:\n            src = list(d[0])\n            src = [constant.SOS] + src + [constant.EOS]\n            src = vocab.map(src)\n            if self.eval:\n                tgt = src # as a placeholder\n            else:\n                tgt = list(d[1])\n            tgt_in = vocab.map([constant.SOS] + tgt)\n            tgt_out = vocab.map(tgt + [constant.EOS])\n            processed += [[src, tgt_in, tgt_out]]\n        return processed\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, key):\n        """""" Get a batch with index. """"""\n        if not isinstance(key, int):\n            raise TypeError\n        if key < 0 or key >= len(self.data):\n            raise IndexError\n        batch = self.data[key]\n        batch_size = len(batch)\n        batch = list(zip(*batch))\n        assert len(batch) == 3\n\n        # sort all fields by lens for easy RNN operations\n        lens = [len(x) for x in batch[0]]\n        batch, orig_idx = sort_all(batch, lens)\n\n        # convert to tensors\n        src = batch[0]\n        src = get_long_tensor(src, batch_size)\n        src_mask = torch.eq(src, constant.PAD_ID)\n        tgt_in = get_long_tensor(batch[1], batch_size)\n        tgt_out = get_long_tensor(batch[2], batch_size)\n        assert tgt_in.size(1) == tgt_out.size(1), \\\n                ""Target input and output sequence sizes do not match.""\n        return (src, src_mask, tgt_in, tgt_out, orig_idx)\n\n    def __iter__(self):\n        for i in range(self.__len__()):\n            yield self.__getitem__(i)\n\n    def load_doc(self, doc, evaluation=False):\n        data = doc.get_mwt_expansions(evaluation)\n        if evaluation: data = [[e] for e in data]\n        return data\n\n'"
stanza/models/mwt/scorer.py,0,"b'""""""\nUtils and wrappers for scoring lemmatizers.\n""""""\nfrom stanza.models.common.utils import ud_scores\n\ndef score(system_conllu_file, gold_conllu_file):\n    """""" Wrapper for word segmenter scorer. """"""\n    evaluation = ud_scores(gold_conllu_file, system_conllu_file)\n    el = evaluation[""Words""]\n    p, r, f = el.precision, el.recall, el.f1\n    return p, r, f\n\n'"
stanza/models/mwt/trainer.py,4,"b'""""""\nA trainer class to handle training and testing of models.\n""""""\n\nimport sys\nimport numpy as np\nfrom collections import Counter\nimport logging\nimport torch\nfrom torch import nn\nimport torch.nn.init as init\n\nimport stanza.models.common.seq2seq_constant as constant\nfrom stanza.models.common.trainer import Trainer as BaseTrainer\nfrom stanza.models.common.seq2seq_model import Seq2SeqModel\nfrom stanza.models.common import utils, loss\nfrom stanza.models.mwt.vocab import Vocab\n\nlogger = logging.getLogger(\'stanza\')\n\ndef unpack_batch(batch, use_cuda):\n    """""" Unpack a batch from the data loader. """"""\n    if use_cuda:\n        inputs = [b.cuda() if b is not None else None for b in batch[:4]]\n    else:\n        inputs = [b if b is not None else None for b in batch[:4]]\n    orig_idx = batch[4]\n    return inputs, orig_idx\n\nclass Trainer(object):\n    """""" A trainer for training models. """"""\n    def __init__(self, args=None, vocab=None, emb_matrix=None, model_file=None, use_cuda=False):\n        self.use_cuda = use_cuda\n        if model_file is not None:\n            # load from file\n            self.load(model_file, use_cuda)\n        else:\n            self.args = args\n            self.model = None if args[\'dict_only\'] else Seq2SeqModel(args, emb_matrix=emb_matrix)\n            self.vocab = vocab\n            self.expansion_dict = dict()\n        if not self.args[\'dict_only\']:\n            self.crit = loss.SequenceLoss(self.vocab.size)\n            self.parameters = [p for p in self.model.parameters() if p.requires_grad]\n            if use_cuda:\n                self.model.cuda()\n                self.crit.cuda()\n            else:\n                self.model.cpu()\n                self.crit.cpu()\n            self.optimizer = utils.get_optimizer(self.args[\'optim\'], self.parameters, self.args[\'lr\'])\n\n    def update(self, batch, eval=False):\n        inputs, orig_idx = unpack_batch(batch, self.use_cuda)\n        src, src_mask, tgt_in, tgt_out = inputs\n\n        if eval:\n            self.model.eval()\n        else:\n            self.model.train()\n            self.optimizer.zero_grad()\n        log_probs, _ = self.model(src, src_mask, tgt_in)\n        loss = self.crit(log_probs.view(-1, self.vocab.size), tgt_out.view(-1))\n        loss_val = loss.data.item()\n        if eval:\n            return loss_val\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args[\'max_grad_norm\'])\n        self.optimizer.step()\n        return loss_val\n\n    def predict(self, batch, unsort=True):\n        inputs, orig_idx = unpack_batch(batch, self.use_cuda)\n        src, src_mask, tgt, tgt_mask = inputs\n\n        self.model.eval()\n        batch_size = src.size(0)\n        preds, _ = self.model.predict(src, src_mask, self.args[\'beam_size\'])\n        pred_seqs = [self.vocab.unmap(ids) for ids in preds] # unmap to tokens\n        pred_seqs = utils.prune_decoded_seqs(pred_seqs)\n        pred_tokens = ["""".join(seq) for seq in pred_seqs] # join chars to be tokens\n        if unsort:\n            pred_tokens = utils.unsort(pred_tokens, orig_idx)\n        return pred_tokens\n\n    def train_dict(self, pairs):\n        """""" Train a MWT expander given training word-expansion pairs. """"""\n        # accumulate counter\n        ctr = Counter()\n        ctr.update([(p[0], p[1]) for p in pairs])\n        seen = set()\n        # find the most frequent mappings\n        for p, _ in ctr.most_common():\n            w, l = p\n            if w not in seen and w != l:\n                self.expansion_dict[w] = l\n            seen.add(w)\n        return\n\n    def predict_dict(self, words):\n        """""" Predict a list of expansions given words. """"""\n        expansions = []\n        for w in words:\n            if w in self.expansion_dict:\n                expansions += [self.expansion_dict[w]]\n            elif w.lower() in self.expansion_dict:\n                expansions += [self.expansion_dict[w.lower()]]\n            else:\n                expansions += [w]\n        return expansions\n\n    def ensemble(self, cands, other_preds):\n        """""" Ensemble the dict with statistical model predictions. """"""\n        expansions = []\n        assert len(cands) == len(other_preds)\n        for c, pred in zip(cands, other_preds):\n            if c in self.expansion_dict:\n                expansions += [self.expansion_dict[c]]\n            elif c.lower() in self.expansion_dict:\n                expansions += [self.expansion_dict[c.lower()]]\n            else:\n                expansions += [pred]\n        return expansions\n\n    def save(self, filename):\n        params = {\n                \'model\': self.model.state_dict() if self.model is not None else None,\n                \'dict\': self.expansion_dict,\n                \'vocab\': self.vocab.state_dict(),\n                \'config\': self.args\n                }\n        try:\n            torch.save(params, filename)\n            logger.info(""Model saved to {}"".format(filename))\n        except BaseException:\n            logger.warning(""Saving failed... continuing anyway."")\n\n    def load(self, filename, use_cuda=False):\n        try:\n            checkpoint = torch.load(filename, lambda storage, loc: storage)\n        except BaseException:\n            logger.exception(""Cannot load model from {}"".format(filename))\n            sys.exit(1)\n        self.args = checkpoint[\'config\']\n        self.expansion_dict = checkpoint[\'dict\']\n        if not self.args[\'dict_only\']:\n            self.model = Seq2SeqModel(self.args, use_cuda=use_cuda)\n            self.model.load_state_dict(checkpoint[\'model\'])\n        else:\n            self.model = None\n        self.vocab = Vocab.load_state_dict(checkpoint[\'vocab\'])\n\n'"
stanza/models/mwt/vocab.py,0,"b'from collections import Counter\n\nfrom stanza.models.common.vocab import BaseVocab\nimport stanza.models.common.seq2seq_constant as constant\n\nclass Vocab(BaseVocab):\n    def build_vocab(self):\n        pairs = self.data\n        allchars = """".join([src + tgt for src, tgt in pairs])\n        counter = Counter(allchars)\n\n        self._id2unit = constant.VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))\n        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}\n'"
stanza/models/ner/__init__.py,0,b''
stanza/models/ner/data.py,4,"b'import random\nimport logging\nimport torch\n\nfrom stanza.models.common.data import map_to_ids, get_long_tensor, get_float_tensor, sort_all\nfrom stanza.models.common.vocab import PAD_ID, VOCAB_PREFIX\nfrom stanza.models.pos.vocab import CharVocab, WordVocab\nfrom stanza.models.ner.vocab import TagVocab, MultiVocab\nfrom stanza.models.common.doc import *\nfrom stanza.models.ner.utils import is_bio_scheme, to_bio2, bio2_to_bioes\n\nlogger = logging.getLogger(\'stanza\')\n\nclass DataLoader:\n    def __init__(self, doc, batch_size, args, pretrain=None, vocab=None, evaluation=False, preprocess_tags=True):\n        self.batch_size = batch_size\n        self.args = args\n        self.eval = evaluation\n        self.shuffled = not self.eval\n        self.doc = doc\n        self.preprocess_tags = preprocess_tags\n\n        data = self.load_doc(self.doc)\n        self.tags = [[w[1] for w in sent] for sent in data]\n\n        # handle vocab\n        self.pretrain = pretrain\n        if vocab is None:\n            self.vocab = self.init_vocab(data)\n        else:\n            self.vocab = vocab\n\n        # filter and sample data\n        if args.get(\'sample_train\', 1.0) < 1.0 and not self.eval:\n            keep = int(args[\'sample_train\'] * len(data))\n            data = random.sample(data, keep)\n            logger.debug(""Subsample training set with rate {:g}"".format(args[\'sample_train\']))\n\n        data = self.preprocess(data, self.vocab, args)\n        # shuffle for training\n        if self.shuffled:\n            random.shuffle(data)\n        self.num_examples = len(data)\n\n        # chunk into batches\n        self.data = self.chunk_batches(data)\n        logger.debug(""{} batches created."".format(len(self.data)))\n\n    def init_vocab(self, data):\n        def from_model(model_filename):\n            """""" Try loading vocab from charLM model file. """"""\n            state_dict = torch.load(model_filename, lambda storage, loc: storage)\n            assert \'vocab\' in state_dict, ""Cannot find vocab in charLM model file.""\n            return state_dict[\'vocab\']\n\n        if self.eval:\n            raise Exception(""Vocab must exist for evaluation."")\n        if self.args[\'charlm\']:\n            charvocab = CharVocab.load_state_dict(from_model(self.args[\'charlm_forward_file\']))\n        else: \n            charvocab = CharVocab(data, self.args[\'shorthand\'])\n        wordvocab = self.pretrain.vocab\n        tagvocab = TagVocab(data, self.args[\'shorthand\'], idx=1)\n        vocab = MultiVocab({\'char\': charvocab,\n                            \'word\': wordvocab,\n                            \'tag\': tagvocab})\n        return vocab\n\n    def preprocess(self, data, vocab, args):\n        processed = []\n        if args.get(\'lowercase\', True): # handle word case\n            case = lambda x: x.lower()\n        else:\n            case = lambda x: x\n        if args.get(\'char_lowercase\', False): # handle character case\n            char_case = lambda x: x.lower()\n        else:\n            char_case = lambda x: x\n        for sent in data:\n            processed_sent = [vocab[\'word\'].map([case(w[0]) for w in sent])]\n            processed_sent += [[vocab[\'char\'].map([char_case(x) for x in w[0]]) for w in sent]]\n            processed_sent += [vocab[\'tag\'].map([w[1] for w in sent])]\n            processed.append(processed_sent)\n        return processed\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, key):\n        """""" Get a batch with index. """"""\n        if not isinstance(key, int):\n            raise TypeError\n        if key < 0 or key >= len(self.data):\n            raise IndexError\n        batch = self.data[key]\n        batch_size = len(batch)\n        batch = list(zip(*batch))\n        assert len(batch) == 3 # words: List[List[int]], chars: List[List[List[int]]], tags: List[List[int]]\n\n        # sort sentences by lens for easy RNN operations\n        sentlens = [len(x) for x in batch[0]]\n        batch, orig_idx = sort_all(batch, sentlens)\n        sentlens = [len(x) for x in batch[0]]\n\n        # sort chars by lens for easy char-LM operations\n        chars_forward, chars_backward, charoffsets_forward, charoffsets_backward, charlens = self.process_chars(batch[1])\n        chars_sorted, char_orig_idx = sort_all([chars_forward, chars_backward, charoffsets_forward, charoffsets_backward], charlens)\n        chars_forward, chars_backward, charoffsets_forward, charoffsets_backward = chars_sorted\n        charlens = [len(sent) for sent in chars_forward]\n\n        # sort words by lens for easy char-RNN operations\n        batch_words = [w for sent in batch[1] for w in sent]\n        wordlens = [len(x) for x in batch_words]\n        batch_words, word_orig_idx = sort_all([batch_words], wordlens)\n        batch_words = batch_words[0]\n        wordlens = [len(x) for x in batch_words]\n\n        # convert to tensors\n        words = get_long_tensor(batch[0], batch_size)\n        words_mask = torch.eq(words, PAD_ID)\n        wordchars = get_long_tensor(batch_words, len(wordlens))\n        wordchars_mask = torch.eq(wordchars, PAD_ID)\n        chars_forward = get_long_tensor(chars_forward, batch_size, pad_id=self.vocab[\'char\'].unit2id(\' \'))\n        chars_backward = get_long_tensor(chars_backward, batch_size, pad_id=self.vocab[\'char\'].unit2id(\' \'))\n        chars = torch.cat([chars_forward.unsqueeze(0), chars_backward.unsqueeze(0)]) # padded forward and backward char idx\n        charoffsets = [charoffsets_forward, charoffsets_backward] # idx for forward and backward lm to get word representation\n        tags = get_long_tensor(batch[2], batch_size)\n\n        return words, words_mask, wordchars, wordchars_mask, chars, tags, orig_idx, word_orig_idx, char_orig_idx, sentlens, wordlens, charlens, charoffsets\n\n    def __iter__(self):\n        for i in range(self.__len__()):\n            yield self.__getitem__(i)\n\n    def load_doc(self, doc):\n        data = doc.get([TEXT, NER], as_sentences=True, from_token=True)\n        if self.preprocess_tags: # preprocess tags\n            data = self.process_tags(data)\n        return data\n\n    def process_tags(self, sentences):\n        res = []\n        # check if tag conversion is needed\n        convert_to_bioes = False\n        is_bio = is_bio_scheme([x[1] for sent in sentences for x in sent])\n        if is_bio and self.args.get(\'scheme\', \'bio\').lower() == \'bioes\':\n            convert_to_bioes = True\n            logger.debug(""BIO tagging scheme found in input; converting into BIOES scheme..."")\n        # process tags\n        for sent in sentences:\n            words, tags = zip(*sent)\n            # NER field sanity checking\n            if any([x is None or x == \'_\' for x in tags]):\n                raise Exception(""NER tag not found for some input data."")\n            # first ensure BIO2 scheme\n            tags = to_bio2(tags)\n            # then convert to BIOES\n            if convert_to_bioes:\n                tags = bio2_to_bioes(tags)\n            res.append([[w,t] for w,t in zip(words, tags)])\n        return res\n\n    def process_chars(self, sents):\n        start_id, end_id = self.vocab[\'char\'].unit2id(\'\\n\'), self.vocab[\'char\'].unit2id(\' \') # special token\n        start_offset, end_offset = 1, 1\n        chars_forward, chars_backward, charoffsets_forward, charoffsets_backward = [], [], [], []\n        # get char representation for each sentence\n        for sent in sents:\n            chars_forward_sent, chars_backward_sent, charoffsets_forward_sent, charoffsets_backward_sent = [start_id], [start_id], [], []\n            # forward lm\n            for word in sent:\n                chars_forward_sent += word\n                charoffsets_forward_sent = charoffsets_forward_sent + [len(chars_forward_sent)] # add each token offset in the last for forward lm\n                chars_forward_sent += [end_id]\n            # backward lm\n            for word in sent[::-1]:\n                chars_backward_sent += word[::-1]\n                charoffsets_backward_sent = [len(chars_backward_sent)] + charoffsets_backward_sent # add each offset in the first for backward lm\n                chars_backward_sent += [end_id]\n            # store each sentence\n            chars_forward.append(chars_forward_sent)\n            chars_backward.append(chars_backward_sent)\n            charoffsets_forward.append(charoffsets_forward_sent)\n            charoffsets_backward.append(charoffsets_backward_sent)\n        charlens = [len(sent) for sent in chars_forward] # forward lm and backward lm should have the same lengths\n        return chars_forward, chars_backward, charoffsets_forward, charoffsets_backward, charlens\n\n    def reshuffle(self):\n        data = [y for x in self.data for y in x]\n        random.shuffle(data)\n        self.data = self.chunk_batches(data)\n\n    def chunk_batches(self, data):\n        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n        return data\n\n'"
stanza/models/ner/model.py,8,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence\n\nfrom stanza.models.common.packed_lstm import PackedLSTM\nfrom stanza.models.common.dropout import WordDropout, LockedDropout\nfrom stanza.models.common.char_model import CharacterModel, CharacterLanguageModel\nfrom stanza.models.common.crf import CRFLoss\nfrom stanza.models.common.vocab import PAD_ID\n\nclass NERTagger(nn.Module):\n    def __init__(self, args, vocab, emb_matrix=None):\n        super().__init__()\n\n        self.vocab = vocab\n        self.args = args\n        self.unsaved_modules = []\n\n        def add_unsaved_module(name, module):\n            self.unsaved_modules += [name]\n            setattr(self, name, module)\n\n        # input layers\n        input_size = 0\n        if self.args[\'word_emb_dim\'] > 0:\n            self.word_emb = nn.Embedding(len(self.vocab[\'word\']), self.args[\'word_emb_dim\'], PAD_ID)\n            # load pretrained embeddings if specified\n            if emb_matrix is not None:\n                self.init_emb(emb_matrix)\n            if not self.args.get(\'emb_finetune\', True):\n                self.word_emb.weight.detach_()\n            input_size += self.args[\'word_emb_dim\']\n\n        if self.args[\'char\'] and self.args[\'char_emb_dim\'] > 0:\n            if self.args[\'charlm\']:\n                add_unsaved_module(\'charmodel_forward\', CharacterLanguageModel.load(args[\'charlm_forward_file\'], finetune=False))\n                add_unsaved_module(\'charmodel_backward\', CharacterLanguageModel.load(args[\'charlm_backward_file\'], finetune=False))\n            else:\n                self.charmodel = CharacterModel(args, vocab, bidirectional=True, attention=False)\n            input_size += self.args[\'char_hidden_dim\'] * 2\n        \n        # optionally add a input transformation layer\n        if self.args.get(\'input_transform\', False):\n            self.input_transform = nn.Linear(input_size, input_size)\n        else:\n            self.input_transform = None\n       \n        # recurrent layers\n        self.taggerlstm = PackedLSTM(input_size, self.args[\'hidden_dim\'], self.args[\'num_layers\'], batch_first=True, \\\n                bidirectional=True, dropout=0 if self.args[\'num_layers\'] == 1 else self.args[\'dropout\'])\n        # self.drop_replacement = nn.Parameter(torch.randn(input_size) / np.sqrt(input_size))\n        self.drop_replacement = None\n        self.taggerlstm_h_init = nn.Parameter(torch.zeros(2 * self.args[\'num_layers\'], 1, self.args[\'hidden_dim\']), requires_grad=False)\n        self.taggerlstm_c_init = nn.Parameter(torch.zeros(2 * self.args[\'num_layers\'], 1, self.args[\'hidden_dim\']), requires_grad=False)\n\n        # tag classifier\n        num_tag = len(self.vocab[\'tag\'])\n        self.tag_clf = nn.Linear(self.args[\'hidden_dim\']*2, num_tag)\n        self.tag_clf.bias.data.zero_()\n\n        # criterion\n        self.crit = CRFLoss(num_tag)\n\n        self.drop = nn.Dropout(args[\'dropout\'])\n        self.worddrop = WordDropout(args[\'word_dropout\'])\n        self.lockeddrop = LockedDropout(args[\'locked_dropout\'])\n\n    def init_emb(self, emb_matrix):\n        if isinstance(emb_matrix, np.ndarray):\n            emb_matrix = torch.from_numpy(emb_matrix)\n        vocab_size = len(self.vocab[\'word\'])\n        dim = self.args[\'word_emb_dim\']\n        assert emb_matrix.size() == (vocab_size, dim), \\\n            ""Input embedding matrix must match size: {} x {}"".format(vocab_size, dim)\n        self.word_emb.weight.data.copy_(emb_matrix)\n\n    def forward(self, word, word_mask, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx):\n        \n        def pack(x):\n            return pack_padded_sequence(x, sentlens, batch_first=True)\n        \n        inputs = []\n        if self.args[\'word_emb_dim\'] > 0:\n            word_emb = self.word_emb(word)\n            word_emb = pack(word_emb)\n            inputs += [word_emb]\n\n        def pad(x):\n            return pad_packed_sequence(PackedSequence(x, word_emb.batch_sizes), batch_first=True)[0]\n\n        if self.args[\'char\'] and self.args[\'char_emb_dim\'] > 0:\n            if self.args.get(\'charlm\', None):\n                char_reps_forward = self.charmodel_forward.get_representation(chars[0], charoffsets[0], charlens, char_orig_idx)\n                char_reps_forward = PackedSequence(char_reps_forward.data, char_reps_forward.batch_sizes)\n                char_reps_backward = self.charmodel_backward.get_representation(chars[1], charoffsets[1], charlens, char_orig_idx)\n                char_reps_backward = PackedSequence(char_reps_backward.data, char_reps_backward.batch_sizes)\n                inputs += [char_reps_forward, char_reps_backward]\n            else:\n                char_reps = self.charmodel(wordchars, wordchars_mask, word_orig_idx, sentlens, wordlens)\n                char_reps = PackedSequence(char_reps.data, char_reps.batch_sizes)\n                inputs += [char_reps]\n\n        lstm_inputs = torch.cat([x.data for x in inputs], 1)\n        if self.args[\'word_dropout\'] > 0:\n            lstm_inputs = self.worddrop(lstm_inputs, self.drop_replacement)\n        lstm_inputs = self.drop(lstm_inputs)\n        lstm_inputs = pad(lstm_inputs)\n        lstm_inputs = self.lockeddrop(lstm_inputs)\n        lstm_inputs = pack(lstm_inputs).data\n\n        if self.input_transform:\n            lstm_inputs = self.input_transform(lstm_inputs)\n\n        lstm_inputs = PackedSequence(lstm_inputs, inputs[0].batch_sizes)\n        lstm_outputs, _ = self.taggerlstm(lstm_inputs, sentlens, hx=(\\\n                self.taggerlstm_h_init.expand(2 * self.args[\'num_layers\'], word.size(0), self.args[\'hidden_dim\']).contiguous(), \\\n                self.taggerlstm_c_init.expand(2 * self.args[\'num_layers\'], word.size(0), self.args[\'hidden_dim\']).contiguous()))\n        lstm_outputs = lstm_outputs.data\n\n\n        # prediction layer\n        lstm_outputs = self.drop(lstm_outputs)\n        lstm_outputs = pad(lstm_outputs)\n        lstm_outputs = self.lockeddrop(lstm_outputs)\n        lstm_outputs = pack(lstm_outputs).data\n        logits = pad(self.tag_clf(lstm_outputs)).contiguous()\n        loss, trans = self.crit(logits, word_mask, tags)\n        \n        return loss, logits, trans\n'"
stanza/models/ner/scorer.py,0,"b'""""""\nAn NER scorer that calculates F1 score given gold and predicted tags.\n""""""\nimport sys\nimport os\nimport logging\nfrom collections import Counter\n\nfrom stanza.models.ner.utils import decode_from_bioes\n\nlogger = logging.getLogger(\'stanza\')\n\ndef score_by_entity(pred_tag_sequences, gold_tag_sequences, verbose=True):\n    """""" Score predicted tags at the entity level.\n\n    Args:\n        pred_tags_sequences: a list of list of predicted tags for each word\n        gold_tags_sequences: a list of list of gold tags for each word\n        verbose: print log with results\n    \n    Returns:\n        Precision, recall and F1 scores.\n    """"""\n    assert(len(gold_tag_sequences) == len(pred_tag_sequences)), \\\n        ""Number of predicted tag sequences does not match gold sequences.""\n    \n    def decode_all(tag_sequences):\n        # decode from all sequences, each sequence with a unique id\n        ents = []\n        for sent_id, tags in enumerate(tag_sequences):\n            for ent in decode_from_bioes(tags):\n                ent[\'sent_id\'] = sent_id\n                ents += [ent]\n        return ents\n\n    gold_ents = decode_all(gold_tag_sequences)\n    pred_ents = decode_all(pred_tag_sequences)\n\n    # scoring\n    correct_by_type = Counter()\n    guessed_by_type = Counter()\n    gold_by_type = Counter()\n\n    for p in pred_ents:\n        guessed_by_type[p[\'type\']] += 1\n        if p in gold_ents:\n            correct_by_type[p[\'type\']] += 1\n    for g in gold_ents:\n        gold_by_type[g[\'type\']] += 1\n    \n    prec_micro = 0.0\n    if sum(guessed_by_type.values()) > 0:\n        prec_micro = sum(correct_by_type.values()) * 1.0 / sum(guessed_by_type.values())\n    rec_micro = 0.0\n    if sum(gold_by_type.values()) > 0:\n        rec_micro = sum(correct_by_type.values()) * 1.0 / sum(gold_by_type.values())\n    f_micro = 0.0\n    if prec_micro + rec_micro > 0:\n        f_micro = 2.0 * prec_micro * rec_micro / (prec_micro + rec_micro)\n    \n    if verbose:\n        logger.info(""Prec.\\tRec.\\tF1"")\n        logger.info(""{:.2f}\\t{:.2f}\\t{:.2f}"".format( \\\n            prec_micro*100, rec_micro*100, f_micro*100))\n    return prec_micro, rec_micro, f_micro\n\n\ndef score_by_token(pred_tag_sequences, gold_tag_sequences, verbose=True):\n    """""" Score predicted tags at the token level.\n\n    Args:\n        pred_tags_sequences: a list of list of predicted tags for each word\n        gold_tags_sequences: a list of list of gold tags for each word\n        verbose: print log with results\n    \n    Returns:\n        Precision, recall and F1 scores.\n    """"""\n    assert(len(gold_tag_sequences) == len(pred_tag_sequences)), \\\n        ""Number of predicted tag sequences does not match gold sequences.""\n    \n    correct_by_tag = Counter()\n    guessed_by_tag = Counter()\n    gold_by_tag = Counter()\n\n    for gold_tags, pred_tags in zip(gold_tag_sequences, pred_tag_sequences):\n        assert(len(gold_tags) == len(pred_tags)), \\\n            ""Number of predicted tags does not match gold.""\n        for g, p in zip(gold_tags, pred_tags):\n            if g == \'O\' and p == \'O\':\n                continue\n            elif g == \'O\' and p != \'O\':\n                guessed_by_tag[p] += 1\n            elif g != \'O\' and p == \'O\':\n                gold_by_tag[g] += 1\n            else:\n                guessed_by_tag[p] += 1\n                gold_by_tag[p] += 1\n                if g == p:\n                    correct_by_tag[p] += 1\n    \n    prec_micro = 0.0\n    if sum(guessed_by_tag.values()) > 0:\n        prec_micro = sum(correct_by_tag.values()) * 1.0 / sum(guessed_by_tag.values())\n    rec_micro = 0.0\n    if sum(gold_by_tag.values()) > 0:\n        rec_micro = sum(correct_by_tag.values()) * 1.0 / sum(gold_by_tag.values())\n    f_micro = 0.0\n    if prec_micro + rec_micro > 0:\n        f_micro = 2.0 * prec_micro * rec_micro / (prec_micro + rec_micro)\n    \n    if verbose:\n        logger.info(""Prec.\\tRec.\\tF1"")\n        logger.info(""{:.2f}\\t{:.2f}\\t{:.2f}"".format( \\\n            prec_micro*100, rec_micro*100, f_micro*100))\n    return prec_micro, rec_micro, f_micro\n\ndef test():\n    pred_sequences = [[\'O\', \'S-LOC\', \'O\', \'O\', \'B-PER\', \'E-PER\'],\n                    [\'O\', \'S-MISC\', \'O\', \'E-ORG\', \'O\', \'B-PER\', \'I-PER\', \'E-PER\']]\n    gold_sequences = [[\'O\', \'B-LOC\', \'E-LOC\', \'O\', \'B-PER\', \'E-PER\'],\n                    [\'O\', \'S-MISC\', \'B-ORG\', \'E-ORG\', \'O\', \'B-PER\', \'E-PER\', \'S-LOC\']]\n    print(score_by_token(pred_sequences, gold_sequences))\n    print(score_by_entity(pred_sequences, gold_sequences))\n\nif __name__ == \'__main__\':\n    test()\n\n'"
stanza/models/ner/trainer.py,3,"b'""""""\nA trainer class to handle training and testing of models.\n""""""\n\nimport sys\nimport logging\nimport torch\nfrom torch import nn\n\nfrom stanza.models.common.trainer import Trainer as BaseTrainer\nfrom stanza.models.common import utils, loss\nfrom stanza.models.ner.model import NERTagger\nfrom stanza.models.ner.vocab import MultiVocab\nfrom stanza.models.common.crf import viterbi_decode\n\nlogger = logging.getLogger(\'stanza\')\n\ndef unpack_batch(batch, use_cuda):\n    """""" Unpack a batch from the data loader. """"""\n    if use_cuda:\n        inputs = [b.cuda() if b is not None else None for b in batch[:6]]\n    else:\n        inputs = batch[:6]\n    orig_idx = batch[6]\n    word_orig_idx = batch[7]\n    char_orig_idx = batch[8]\n    sentlens = batch[9]\n    wordlens = batch[10]\n    charlens = batch[11]\n    charoffsets = batch[12]\n    return inputs, orig_idx, word_orig_idx, char_orig_idx, sentlens, wordlens, charlens, charoffsets\n\nclass Trainer(BaseTrainer):\n    """""" A trainer for training models. """"""\n    def __init__(self, args=None, vocab=None, pretrain=None, model_file=None, use_cuda=False):\n        self.use_cuda = use_cuda\n        if model_file is not None:\n            # load everything from file\n            self.load(model_file, args)\n        else:\n            assert all(var is not None for var in [args, vocab, pretrain])\n            # build model from scratch\n            self.args = args\n            self.vocab = vocab\n            self.model = NERTagger(args, vocab, emb_matrix=pretrain.emb)\n        self.parameters = [p for p in self.model.parameters() if p.requires_grad]\n        if self.use_cuda:\n            self.model.cuda()\n        else:\n            self.model.cpu()\n        self.optimizer = utils.get_optimizer(self.args[\'optim\'], self.parameters, self.args[\'lr\'], momentum=self.args[\'momentum\'])\n\n    def update(self, batch, eval=False):\n        inputs, orig_idx, word_orig_idx, char_orig_idx, sentlens, wordlens, charlens, charoffsets = unpack_batch(batch, self.use_cuda)\n        word, word_mask, wordchars, wordchars_mask, chars, tags = inputs\n\n        if eval:\n            self.model.eval()\n        else:\n            self.model.train()\n            self.optimizer.zero_grad()\n        loss, _, _ = self.model(word, word_mask, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx)\n        loss_val = loss.data.item()\n        if eval:\n            return loss_val\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args[\'max_grad_norm\'])\n        self.optimizer.step()\n        return loss_val\n\n    def predict(self, batch, unsort=True):\n        inputs, orig_idx, word_orig_idx, char_orig_idx, sentlens, wordlens, charlens, charoffsets = unpack_batch(batch, self.use_cuda)\n        word, word_mask, wordchars, wordchars_mask, chars, tags = inputs\n\n        self.model.eval()\n        batch_size = word.size(0)\n        _, logits, trans = self.model(word, word_mask, wordchars, wordchars_mask, tags, word_orig_idx, sentlens, wordlens, chars, charoffsets, charlens, char_orig_idx)\n\n        # decode\n        trans = trans.data.cpu().numpy()\n        scores = logits.data.cpu().numpy()\n        bs = logits.size(0)\n        tag_seqs = []\n        for i in range(bs):\n            tags, _ = viterbi_decode(scores[i, :sentlens[i]], trans)\n            tags = self.vocab[\'tag\'].unmap(tags)\n            tag_seqs += [tags]\n\n        if unsort:\n            tag_seqs = utils.unsort(tag_seqs, orig_idx)\n        return tag_seqs\n\n    def save(self, filename, skip_modules=True):\n        model_state = self.model.state_dict()\n        # skip saving modules like pretrained embeddings, because they are large and will be saved in a separate file\n        if skip_modules:\n            skipped = [k for k in model_state.keys() if k.split(\'.\')[0] in self.model.unsaved_modules]\n            for k in skipped:\n                del model_state[k]\n        params = {\n                \'model\': model_state,\n                \'vocab\': self.vocab.state_dict(),\n                \'config\': self.args\n                }\n        try:\n            torch.save(params, filename)\n            logger.info(""Model saved to {}"".format(filename))\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            logger.warning(""Saving failed... continuing anyway."")\n\n    def load(self, filename, args=None):\n        try:\n            checkpoint = torch.load(filename, lambda storage, loc: storage)\n        except BaseException:\n            logger.exception(""Cannot load model from {}"".format(filename))\n            sys.exit(1)\n        self.args = checkpoint[\'config\']\n        if args: self.args.update(args)\n        self.vocab = MultiVocab.load_state_dict(checkpoint[\'vocab\'])\n        self.model = NERTagger(self.args, self.vocab)\n        self.model.load_state_dict(checkpoint[\'model\'], strict=False)\n\n'"
stanza/models/ner/utils.py,0,"b'""""""\nUtility functions for dealing with NER tagging.\n""""""\n\ndef is_bio_scheme(all_tags):\n    """"""\n    Check if BIO tagging scheme is used. Return True if so.\n\n    Args:\n        all_tags: a list of NER tags\n    \n    Returns:\n        True if the tagging scheme is BIO, otherwise False\n    """"""\n    for tag in all_tags:\n        if tag == \'O\':\n            continue\n        elif len(tag) > 2 and tag[:2] in (\'B-\', \'I-\'):\n            continue\n        else:\n            return False\n    return True\n\ndef to_bio2(tags):\n    """"""\n    Convert the original tag sequence to BIO2 format. If the input is already in BIO2 format,\n    the original input is returned.\n\n    Args:\n        tags: a list of tags in either BIO or BIO2 format\n    \n    Returns:\n        new_tags: a list of tags in BIO2 format\n    """"""\n    new_tags = []\n    for i, tag in enumerate(tags):\n        if tag == \'O\':\n            new_tags.append(tag)\n        elif tag[0] == \'I\':\n            if i == 0 or tags[i-1] == \'O\' or tags[i-1][1:] != tag[1:]:\n                new_tags.append(\'B\' + tag[1:])\n            else:\n                new_tags.append(tag)\n        else:\n            new_tags.append(tag)\n    return new_tags\n\ndef bio2_to_bioes(tags):\n    """"""\n    Convert the BIO2 tag sequence into a BIOES sequence.\n\n    Args:\n        tags: a list of tags in BIO2 format\n\n    Returns:\n        new_tags: a list of tags in BIOES format\n    """"""\n    new_tags = []\n    for i, tag in enumerate(tags):\n        if tag == \'O\':\n            new_tags.append(tag)\n        else:\n            if len(tag) < 2:\n                raise Exception(f""Invalid BIO2 tag found: {tag}"")\n            else:\n                if tag[:2] == \'I-\': # convert to E- if next tag is not I-\n                    if i+1 < len(tags) and tags[i+1][:2] == \'I-\':\n                        new_tags.append(tag)\n                    else:\n                        new_tags.append(\'E-\' + tag[2:])\n                elif tag[:2] == \'B-\': # convert to S- if next tag is not I-\n                    if i+1 < len(tags) and tags[i+1][:2] == \'I-\':\n                        new_tags.append(tag)\n                    else:\n                        new_tags.append(\'S-\' + tag[2:])\n                else:\n                    raise Exception(f""Invalid IOB tag found: {tag}"")\n    return new_tags\n\ndef decode_from_bioes(tags):\n    """"""\n    Decode from a sequence of BIOES tags, assuming default tag is \'O\'.\n    Args:\n        tags: a list of BIOES tags\n    \n    Returns:\n        A list of dict with start_idx, end_idx, and type values.\n    """"""\n    res = []\n    ent_idxs = []\n    cur_type = None\n\n    def flush():\n        if len(ent_idxs) > 0:\n            res.append({\n                \'start\': ent_idxs[0], \n                \'end\': ent_idxs[-1], \n                \'type\': cur_type})\n\n    for idx, tag in enumerate(tags):\n        if tag is None:\n            tag = \'O\'\n        if tag == \'O\':\n            flush()\n            ent_idxs = []\n        elif tag.startswith(\'B-\'): # start of new ent\n            flush()\n            ent_idxs = [idx]\n            cur_type = tag[2:]\n        elif tag.startswith(\'I-\'): # continue last ent\n            ent_idxs.append(idx)\n            cur_type = tag[2:]\n        elif tag.startswith(\'E-\'): # end last ent\n            ent_idxs.append(idx)\n            cur_type = tag[2:]\n            flush()\n            ent_idxs = []\n        elif tag.startswith(\'S-\'): # start single word ent\n            flush()\n            ent_idxs = [idx]\n            cur_type = tag[2:]\n            flush()\n            ent_idxs = []\n    # flush after whole sentence\n    flush()\n    return res\n'"
stanza/models/ner/vocab.py,0,"b'from collections import Counter, OrderedDict\n\nfrom stanza.models.common.vocab import BaseVocab, BaseMultiVocab\nfrom stanza.models.common.vocab import VOCAB_PREFIX\nfrom stanza.models.common.pretrain import PretrainedWordVocab\nfrom stanza.models.pos.vocab import CharVocab, WordVocab\n\nclass TagVocab(BaseVocab):\n    """""" A vocab for the output tag sequence. """"""\n    def build_vocab(self):\n        counter = Counter([w[self.idx] for sent in self.data for w in sent])\n\n        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))\n        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}\n\nclass MultiVocab(BaseMultiVocab):\n    def state_dict(self):\n        """""" Also save a vocab name to class name mapping in state dict. """"""\n        state = OrderedDict()\n        key2class = OrderedDict()\n        for k, v in self._vocabs.items():\n            state[k] = v.state_dict()\n            key2class[k] = type(v).__name__\n        state[\'_key2class\'] = key2class\n        return state\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        class_dict = {\'CharVocab\': CharVocab,\n                \'PretrainedWordVocab\': PretrainedWordVocab,\n                \'TagVocab\': TagVocab}\n        new = cls()\n        assert \'_key2class\' in state_dict, ""Cannot find class name mapping in state dict!""\n        key2class = state_dict.pop(\'_key2class\')\n        for k,v in state_dict.items():\n            classname = key2class[k]\n            new[k] = class_dict[classname].load_state_dict(v)\n        return new\n\n'"
stanza/models/pos/__init__.py,0,b''
stanza/models/pos/build_xpos_vocab_factory.py,0,"b'from collections import defaultdict\nimport os\nimport sys\nfrom stanza.models.common.vocab import VOCAB_PREFIX\nfrom stanza.models.common.constant import lang2lcode\nfrom stanza.models.pos.vocab import XPOSVocab, WordVocab\nfrom stanza.models.common.doc import *\nfrom stanza.utils.conll import CoNLL\n\nif len(sys.argv) != 3:\n    print(\'Usage: {} list_of_tb_file output_factory_file\'.format(sys.argv[0]))\n    sys.exit(0)\n\n# Read list of all treebanks of concern\nlist_of_tb_file, output_file = sys.argv[1:]\n\ndef treebank_to_short_name(treebank):\n    """""" Convert treebank name to short code. """"""\n    if treebank.startswith(\'UD_\'):\n        treebank = treebank[3:]\n    splits = treebank.split(\'-\')\n    assert len(splits) == 2\n    lang, corpus = splits\n    lcode = lang2lcode[lang]\n    short = ""{}_{}"".format(lcode, corpus.lower())\n    return short\n\nshorthands = []\nfullnames = []\nwith open(list_of_tb_file) as f:\n    for line in f:\n        treebank = line.strip()\n        fullnames.append(treebank)\n        shorthands.append(treebank_to_short_name(treebank))\n\ndef filter_data(data, idx):\n    data_filtered = []\n    for sentence in data:\n        flag = True\n        for token in sentence:\n            if token[idx] is None:\n                flag = False\n        if flag: data_filtered.append(sentence)\n    return data_filtered\n\n\n# For each treebank, we would like to find the XPOS Vocab configuration that minimizes\n# the number of total classes needed to predict by all tagger classifiers. This is\n# achieved by enumerating different options of separators that different treebanks might\n# use, and comparing that to treating the XPOS tags as separate categories (using a\n# WordVocab).\nmapping = defaultdict(list)\nfor sh, fn in zip(shorthands, fullnames):\n    print(\'Resolving vocab option for {}...\'.format(sh))\n    if not os.path.exists(\'data/pos/{}.train.in.conllu\'.format(sh)):\n        raise UserWarning(\'Training data for {} not found in the data directory, falling back to using WordVocab. To generate the \'\n            \'XPOS vocabulary for this treebank properly, please run the following command first:\\n\'\n            \'\\tbash scripts/prep_pos_data.sh {}\'.format(fn, fn))\n        # without the training file, there\'s not much we can do\n        key = \'WordVocab(data, shorthand, idx=2)\'\n        mapping[key].append(sh)\n        continue\n\n    doc = Document(CoNLL.conll2dict(input_file=\'data/pos/{}.train.in.conllu\'.format(sh)))\n    data = doc.get([TEXT, UPOS, XPOS, FEATS], as_sentences=True)\n    print(f\'Original length = {len(data)}\')\n    data = filter_data(data, idx=2)\n    print(f\'Filtered length = {len(data)}\')\n    vocab = WordVocab(data, sh, idx=2, ignore=[""_""])\n    key = \'WordVocab(data, shorthand, idx=2, ignore=[""_""])\'\n    best_size = len(vocab) - len(VOCAB_PREFIX)\n    if best_size > 20:\n        for sep in [\'\', \'-\', \'+\', \'|\', \',\', \':\']: # separators\n            vocab = XPOSVocab(data, sh, idx=2, sep=sep)\n            length = sum(len(x) - len(VOCAB_PREFIX) for x in vocab._id2unit.values())\n            if length < best_size:\n                key = \'XPOSVocab(data, shorthand, idx=2, sep=""{}"")\'.format(sep)\n                best_size = length\n    mapping[key].append(sh)\n\n# Generate code. This takes the XPOS vocabulary classes selected above, and generates the\n# actual factory class as seen in models.pos.xpos_vocab_factory.\nfirst = True\nwith open(output_file, \'w\') as f:\n    print(\'\'\'# This is the XPOS factory method generated automatically from models.pos.build_xpos_factory.\n# Please don\'t edit it!\n\nfrom models.pos.vocab import WordVocab, XPOSVocab\n\ndef xpos_vocab_factory(data, shorthand):\'\'\', file=f)\n\n    for key in mapping:\n        print(""    {} shorthand in [{}]:"".format(\'if\' if first else \'elif\', \', \'.join([\'""{}""\'.format(x) for x in mapping[key]])), file=f)\n        print(""        return {}"".format(key), file=f)\n\n        first = False\n    print(\'\'\'    else:\n        raise NotImplementedError(\'Language shorthand ""{}"" not found!\'.format(shorthand))\'\'\', file=f)\n\nprint(\'Done!\')\n'"
stanza/models/pos/data.py,2,"b'import random\nimport logging\nimport torch\n\nfrom stanza.models.common.data import map_to_ids, get_long_tensor, get_float_tensor, sort_all\nfrom stanza.models.common.vocab import PAD_ID, VOCAB_PREFIX\nfrom stanza.models.pos.vocab import CharVocab, WordVocab, XPOSVocab, FeatureVocab, MultiVocab\nfrom stanza.models.pos.xpos_vocab_factory import xpos_vocab_factory\nfrom stanza.models.common.doc import *\n\nlogger = logging.getLogger(\'stanza\')\n\nclass DataLoader:\n    def __init__(self, doc, batch_size, args, pretrain, vocab=None, evaluation=False, sort_during_eval=False):\n        self.batch_size = batch_size\n        self.args = args\n        self.eval = evaluation\n        self.shuffled = not self.eval\n        self.sort_during_eval = sort_during_eval\n        self.doc = doc\n\n        data = self.load_doc(self.doc)\n\n        # handle vocab\n        if vocab is None:\n            self.vocab = self.init_vocab(data)\n        else:\n            self.vocab = vocab\n        \n        # handle pretrain; pretrain vocab is used when args[\'pretrain\'] == True and pretrain is not None\n        self.pretrain_vocab = None\n        if pretrain is not None and args[\'pretrain\']:\n            self.pretrain_vocab = pretrain.vocab\n\n        # filter and sample data\n        if args.get(\'sample_train\', 1.0) < 1.0 and not self.eval:\n            keep = int(args[\'sample_train\'] * len(data))\n            data = random.sample(data, keep)\n            logger.debug(""Subsample training set with rate {:g}"".format(args[\'sample_train\']))\n\n        data = self.preprocess(data, self.vocab, self.pretrain_vocab, args)\n        # shuffle for training\n        if self.shuffled:\n            random.shuffle(data)\n        self.num_examples = len(data)\n\n        # chunk into batches\n        self.data = self.chunk_batches(data)\n        logger.debug(""{} batches created."".format(len(self.data)))\n\n    def init_vocab(self, data):\n        assert self.eval == False # for eval vocab must exist\n        charvocab = CharVocab(data, self.args[\'shorthand\'])\n        wordvocab = WordVocab(data, self.args[\'shorthand\'], cutoff=7, lower=True)\n        uposvocab = WordVocab(data, self.args[\'shorthand\'], idx=1)\n        xposvocab = xpos_vocab_factory(data, self.args[\'shorthand\'])\n        featsvocab = FeatureVocab(data, self.args[\'shorthand\'], idx=3)\n        vocab = MultiVocab({\'char\': charvocab,\n                            \'word\': wordvocab,\n                            \'upos\': uposvocab,\n                            \'xpos\': xposvocab,\n                            \'feats\': featsvocab})\n        return vocab\n\n    def preprocess(self, data, vocab, pretrain_vocab, args):\n        processed = []\n        for sent in data:\n            processed_sent = [vocab[\'word\'].map([w[0] for w in sent])]\n            processed_sent += [[vocab[\'char\'].map([x for x in w[0]]) for w in sent]]\n            processed_sent += [vocab[\'upos\'].map([w[1] for w in sent])]\n            processed_sent += [vocab[\'xpos\'].map([w[2] for w in sent])]\n            processed_sent += [vocab[\'feats\'].map([w[3] for w in sent])]\n            if pretrain_vocab is not None:\n                # always use lowercase lookup in pretrained vocab\n                processed_sent += [pretrain_vocab.map([w[0].lower() for w in sent])]\n            else:\n                processed_sent += [[PAD_ID] * len(sent)]\n            processed.append(processed_sent)\n        return processed\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, key):\n        """""" Get a batch with index. """"""\n        if not isinstance(key, int):\n            raise TypeError\n        if key < 0 or key >= len(self.data):\n            raise IndexError\n        batch = self.data[key]\n        batch_size = len(batch)\n        batch = list(zip(*batch))\n        assert len(batch) == 6\n\n        # sort sentences by lens for easy RNN operations\n        lens = [len(x) for x in batch[0]]\n        batch, orig_idx = sort_all(batch, lens)\n\n        # sort words by lens for easy char-RNN operations\n        batch_words = [w for sent in batch[1] for w in sent]\n        word_lens = [len(x) for x in batch_words]\n        batch_words, word_orig_idx = sort_all([batch_words], word_lens)\n        batch_words = batch_words[0]\n        word_lens = [len(x) for x in batch_words]\n\n        # convert to tensors\n        words = batch[0]\n        words = get_long_tensor(words, batch_size)\n        words_mask = torch.eq(words, PAD_ID)\n        wordchars = get_long_tensor(batch_words, len(word_lens))\n        wordchars_mask = torch.eq(wordchars, PAD_ID)\n\n        upos = get_long_tensor(batch[2], batch_size)\n        xpos = get_long_tensor(batch[3], batch_size)\n        ufeats = get_long_tensor(batch[4], batch_size)\n        pretrained = get_long_tensor(batch[5], batch_size)\n        sentlens = [len(x) for x in batch[0]]\n        return words, words_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, orig_idx, word_orig_idx, sentlens, word_lens\n\n    def __iter__(self):\n        for i in range(self.__len__()):\n            yield self.__getitem__(i)\n\n    def load_doc(self, doc):\n        data = doc.get([TEXT, UPOS, XPOS, FEATS], as_sentences=True)\n        data = self.resolve_none(data)\n        return data\n\n    def resolve_none(self, data):\n        # replace None to \'_\'\n        for sent_idx in range(len(data)):\n            for tok_idx in range(len(data[sent_idx])):\n                for feat_idx in range(len(data[sent_idx][tok_idx])):\n                    if data[sent_idx][tok_idx][feat_idx] is None:\n                        data[sent_idx][tok_idx][feat_idx] = \'_\'\n        return data\n\n    def reshuffle(self):\n        data = [y for x in self.data for y in x]\n        self.data = self.chunk_batches(data)\n        random.shuffle(self.data)\n\n    def chunk_batches(self, data):\n        res = []\n\n        if not self.eval:\n            # sort sentences (roughly) by length for better memory utilization\n            data = sorted(data, key = lambda x: len(x[0]), reverse=random.random() > .5)\n        elif self.sort_during_eval:\n            (data, ), self.data_orig_idx = sort_all([data], [len(x[0]) for x in data])\n\n        current = []\n        currentlen = 0\n        for x in data:\n            if len(x[0]) + currentlen > self.batch_size and currentlen > 0:\n                res.append(current)\n                current = []\n                currentlen = 0\n            current.append(x)\n            currentlen += len(x[0])\n\n        if currentlen > 0:\n            res.append(current)\n\n        return res\n'"
stanza/models/pos/model.py,11,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence, PackedSequence\n\nfrom stanza.models.common.biaffine import BiaffineScorer\nfrom stanza.models.common.hlstm import HighwayLSTM\nfrom stanza.models.common.dropout import WordDropout\nfrom stanza.models.common.vocab import CompositeVocab\nfrom stanza.models.common.char_model import CharacterModel\n\nclass Tagger(nn.Module):\n    def __init__(self, args, vocab, emb_matrix=None, share_hid=False):\n        super().__init__()\n\n        self.vocab = vocab\n        self.args = args\n        self.share_hid = share_hid\n        self.unsaved_modules = []\n\n        def add_unsaved_module(name, module):\n            self.unsaved_modules += [name]\n            setattr(self, name, module)\n\n        # input layers\n        input_size = 0\n        if self.args['word_emb_dim'] > 0:\n            # frequent word embeddings\n            self.word_emb = nn.Embedding(len(vocab['word']), self.args['word_emb_dim'], padding_idx=0)\n            input_size += self.args['word_emb_dim']\n\n        if not share_hid:\n            # upos embeddings\n            self.upos_emb = nn.Embedding(len(vocab['upos']), self.args['tag_emb_dim'], padding_idx=0)\n\n        if self.args['char'] and self.args['char_emb_dim'] > 0:\n            self.charmodel = CharacterModel(args, vocab)\n            self.trans_char = nn.Linear(self.args['char_hidden_dim'], self.args['transformed_dim'], bias=False)\n            input_size += self.args['transformed_dim']\n\n        if self.args['pretrain']:    \n            # pretrained embeddings, by default this won't be saved into model file\n            add_unsaved_module('pretrained_emb', nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True))\n            self.trans_pretrained = nn.Linear(emb_matrix.shape[1], self.args['transformed_dim'], bias=False)\n            input_size += self.args['transformed_dim']\n        \n        # recurrent layers\n        self.taggerlstm = HighwayLSTM(input_size, self.args['hidden_dim'], self.args['num_layers'], batch_first=True, bidirectional=True, dropout=self.args['dropout'], rec_dropout=self.args['rec_dropout'], highway_func=torch.tanh)\n        self.drop_replacement = nn.Parameter(torch.randn(input_size) / np.sqrt(input_size))\n        self.taggerlstm_h_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))\n        self.taggerlstm_c_init = nn.Parameter(torch.zeros(2 * self.args['num_layers'], 1, self.args['hidden_dim']))\n\n        # classifiers\n        self.upos_hid = nn.Linear(self.args['hidden_dim'] * 2, self.args['deep_biaff_hidden_dim'])\n        self.upos_clf = nn.Linear(self.args['deep_biaff_hidden_dim'], len(vocab['upos']))\n        self.upos_clf.weight.data.zero_()\n        self.upos_clf.bias.data.zero_()\n\n        if share_hid:\n            clf_constructor = lambda insize, outsize: nn.Linear(insize, outsize)\n        else:\n            self.xpos_hid = nn.Linear(self.args['hidden_dim'] * 2, self.args['deep_biaff_hidden_dim'] if not isinstance(vocab['xpos'], CompositeVocab) else self.args['composite_deep_biaff_hidden_dim'])\n            self.ufeats_hid = nn.Linear(self.args['hidden_dim'] * 2, self.args['composite_deep_biaff_hidden_dim'])\n            clf_constructor = lambda insize, outsize: BiaffineScorer(insize, self.args['tag_emb_dim'], outsize)\n\n        if isinstance(vocab['xpos'], CompositeVocab):\n            self.xpos_clf = nn.ModuleList()\n            for l in vocab['xpos'].lens():\n                self.xpos_clf.append(clf_constructor(self.args['composite_deep_biaff_hidden_dim'], l))\n        else:\n            self.xpos_clf = clf_constructor(self.args['deep_biaff_hidden_dim'], len(vocab['xpos']))\n            if share_hid:\n                self.xpos_clf.weight.data.zero_()\n                self.xpos_clf.bias.data.zero_()\n\n        self.ufeats_clf = nn.ModuleList()\n        for l in vocab['feats'].lens():\n            if share_hid:\n                self.ufeats_clf.append(clf_constructor(self.args['deep_biaff_hidden_dim'], l))\n                self.ufeats_clf[-1].weight.data.zero_()\n                self.ufeats_clf[-1].bias.data.zero_()\n            else:\n                self.ufeats_clf.append(clf_constructor(self.args['composite_deep_biaff_hidden_dim'], l))\n\n        # criterion\n        self.crit = nn.CrossEntropyLoss(ignore_index=0) # ignore padding\n\n        self.drop = nn.Dropout(args['dropout'])\n        self.worddrop = WordDropout(args['word_dropout'])\n\n    def forward(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens):\n        \n        def pack(x):\n            return pack_padded_sequence(x, sentlens, batch_first=True)\n        \n        inputs = []\n        if self.args['word_emb_dim'] > 0:\n            word_emb = self.word_emb(word)\n            word_emb = pack(word_emb)\n            inputs += [word_emb]\n\n        if self.args['pretrain']:\n            pretrained_emb = self.pretrained_emb(pretrained)\n            pretrained_emb = self.trans_pretrained(pretrained_emb)\n            pretrained_emb = pack(pretrained_emb)\n            inputs += [pretrained_emb]\n\n        def pad(x):\n            return pad_packed_sequence(PackedSequence(x, word_emb.batch_sizes), batch_first=True)[0]\n\n        if self.args['char'] and self.args['char_emb_dim'] > 0:\n            char_reps = self.charmodel(wordchars, wordchars_mask, word_orig_idx, sentlens, wordlens)\n            char_reps = PackedSequence(self.trans_char(self.drop(char_reps.data)), char_reps.batch_sizes)\n            inputs += [char_reps]\n\n        lstm_inputs = torch.cat([x.data for x in inputs], 1)\n        lstm_inputs = self.worddrop(lstm_inputs, self.drop_replacement)\n        lstm_inputs = self.drop(lstm_inputs)\n        lstm_inputs = PackedSequence(lstm_inputs, inputs[0].batch_sizes)\n\n        lstm_outputs, _ = self.taggerlstm(lstm_inputs, sentlens, hx=(self.taggerlstm_h_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous(), self.taggerlstm_c_init.expand(2 * self.args['num_layers'], word.size(0), self.args['hidden_dim']).contiguous()))\n        lstm_outputs = lstm_outputs.data\n\n        upos_hid = F.relu(self.upos_hid(self.drop(lstm_outputs)))\n        upos_pred = self.upos_clf(self.drop(upos_hid))\n\n        preds = [pad(upos_pred).max(2)[1]]\n\n        upos = pack(upos).data\n        loss = self.crit(upos_pred.view(-1, upos_pred.size(-1)), upos.view(-1))\n\n        if self.share_hid:\n            xpos_hid = upos_hid\n            ufeats_hid = upos_hid\n\n            clffunc = lambda clf, hid: clf(self.drop(hid))\n        else:\n            xpos_hid = F.relu(self.xpos_hid(self.drop(lstm_outputs)))\n            ufeats_hid = F.relu(self.ufeats_hid(self.drop(lstm_outputs)))\n\n            if self.training:\n                upos_emb = self.upos_emb(upos)\n            else:\n                upos_emb = self.upos_emb(upos_pred.max(1)[1])\n\n            clffunc = lambda clf, hid: clf(self.drop(hid), self.drop(upos_emb))\n\n        xpos = pack(xpos).data\n        if isinstance(self.vocab['xpos'], CompositeVocab):\n            xpos_preds = []\n            for i in range(len(self.vocab['xpos'])):\n                xpos_pred = clffunc(self.xpos_clf[i], xpos_hid)\n                loss += self.crit(xpos_pred.view(-1, xpos_pred.size(-1)), xpos[:, i].view(-1))\n                xpos_preds.append(pad(xpos_pred).max(2, keepdim=True)[1])\n            preds.append(torch.cat(xpos_preds, 2))\n        else:\n            xpos_pred = clffunc(self.xpos_clf, xpos_hid)\n            loss += self.crit(xpos_pred.view(-1, xpos_pred.size(-1)), xpos.view(-1))\n            preds.append(pad(xpos_pred).max(2)[1])\n\n        ufeats_preds = []\n        ufeats = pack(ufeats).data\n        for i in range(len(self.vocab['feats'])):\n            ufeats_pred = clffunc(self.ufeats_clf[i], ufeats_hid)\n            loss += self.crit(ufeats_pred.view(-1, ufeats_pred.size(-1)), ufeats[:, i].view(-1))\n            ufeats_preds.append(pad(ufeats_pred).max(2, keepdim=True)[1])\n        preds.append(torch.cat(ufeats_preds, 2))\n\n        return loss, preds\n"""
stanza/models/pos/scorer.py,0,"b'""""""\nUtils and wrappers for scoring taggers.\n""""""\nimport logging\n\nfrom stanza.models.common.utils import ud_scores\n\nlogger = logging.getLogger(\'stanza\')\n\ndef score(system_conllu_file, gold_conllu_file, verbose=True):\n    """""" Wrapper for tagger scorer. """"""\n    evaluation = ud_scores(gold_conllu_file, system_conllu_file)\n    el = evaluation[\'AllTags\']\n    p = el.precision\n    r = el.recall\n    f = el.f1\n    if verbose:\n        scores = [evaluation[k].f1 * 100 for k in [\'UPOS\', \'XPOS\', \'UFeats\', \'AllTags\']]\n        logger.info(""UPOS\\tXPOS\\tUFeats\\tAllTags"")\n        logger.info(""{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}"".format(*scores))\n    return p, r, f\n\n'"
stanza/models/pos/trainer.py,3,"b'""""""\nA trainer class to handle training and testing of models.\n""""""\n\nimport sys\nimport logging\nimport torch\nfrom torch import nn\n\nfrom stanza.models.common.trainer import Trainer as BaseTrainer\nfrom stanza.models.common import utils, loss\nfrom stanza.models.pos.model import Tagger\nfrom stanza.models.pos.vocab import MultiVocab\n\nlogger = logging.getLogger(\'stanza\')\n\ndef unpack_batch(batch, use_cuda):\n    """""" Unpack a batch from the data loader. """"""\n    if use_cuda:\n        inputs = [b.cuda() if b is not None else None for b in batch[:8]]\n    else:\n        inputs = batch[:8]\n    orig_idx = batch[8]\n    word_orig_idx = batch[9]\n    sentlens = batch[10]\n    wordlens = batch[11]\n    return inputs, orig_idx, word_orig_idx, sentlens, wordlens\n\nclass Trainer(BaseTrainer):\n    """""" A trainer for training models. """"""\n    def __init__(self, args=None, vocab=None, pretrain=None, model_file=None, use_cuda=False):\n        self.use_cuda = use_cuda\n        if model_file is not None:\n            # load everything from file\n            self.load(model_file, pretrain)\n        else:\n            # build model from scratch\n            self.args = args\n            self.vocab = vocab\n            self.model = Tagger(args, vocab, emb_matrix=pretrain.emb if pretrain is not None else None, share_hid=args[\'share_hid\'])\n        self.parameters = [p for p in self.model.parameters() if p.requires_grad]\n        if self.use_cuda:\n            self.model.cuda()\n        else:\n            self.model.cpu()\n        self.optimizer = utils.get_optimizer(self.args[\'optim\'], self.parameters, self.args[\'lr\'], betas=(0.9, self.args[\'beta2\']), eps=1e-6)\n\n    def update(self, batch, eval=False):\n        inputs, orig_idx, word_orig_idx, sentlens, wordlens = unpack_batch(batch, self.use_cuda)\n        word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained = inputs\n\n        if eval:\n            self.model.eval()\n        else:\n            self.model.train()\n            self.optimizer.zero_grad()\n        loss, _ = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens)\n        loss_val = loss.data.item()\n        if eval:\n            return loss_val\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args[\'max_grad_norm\'])\n        self.optimizer.step()\n        return loss_val\n\n    def predict(self, batch, unsort=True):\n        inputs, orig_idx, word_orig_idx, sentlens, wordlens = unpack_batch(batch, self.use_cuda)\n        word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained = inputs\n\n        self.model.eval()\n        batch_size = word.size(0)\n        _, preds = self.model(word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens)\n        upos_seqs = [self.vocab[\'upos\'].unmap(sent) for sent in preds[0].tolist()]\n        xpos_seqs = [self.vocab[\'xpos\'].unmap(sent) for sent in preds[1].tolist()]\n        feats_seqs = [self.vocab[\'feats\'].unmap(sent) for sent in preds[2].tolist()]\n\n        pred_tokens = [[[upos_seqs[i][j], xpos_seqs[i][j], feats_seqs[i][j]] for j in range(sentlens[i])] for i in range(batch_size)]\n        if unsort:\n            pred_tokens = utils.unsort(pred_tokens, orig_idx)\n        return pred_tokens\n\n    def save(self, filename, skip_modules=True):\n        model_state = self.model.state_dict()\n        # skip saving modules like pretrained embeddings, because they are large and will be saved in a separate file\n        if skip_modules:\n            skipped = [k for k in model_state.keys() if k.split(\'.\')[0] in self.model.unsaved_modules]\n            for k in skipped:\n                del model_state[k]\n        params = {\n                \'model\': model_state,\n                \'vocab\': self.vocab.state_dict(),\n                \'config\': self.args\n                }\n        try:\n            torch.save(params, filename)\n            logger.info(""Model saved to {}"".format(filename))\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            logger.warning(""Saving failed... continuing anyway."")\n\n    def load(self, filename, pretrain):\n        """"""\n        Load a model from file, with preloaded pretrain embeddings. Here we allow the pretrain to be None or a dummy input,\n        and the actual use of pretrain embeddings will depend on the boolean config ""pretrain"" in the loaded args.\n        """"""\n        try:\n            checkpoint = torch.load(filename, lambda storage, loc: storage)\n        except BaseException:\n            logger.exception(""Cannot load model from {}"".format(filename))\n            sys.exit(1)\n        self.args = checkpoint[\'config\']\n        self.vocab = MultiVocab.load_state_dict(checkpoint[\'vocab\'])\n        # load model\n        emb_matrix = None\n        if self.args[\'pretrain\'] and pretrain is not None: # we use pretrain only if args[\'pretrain\'] == True and pretrain is not None\n            emb_matrix = pretrain.emb\n        self.model = Tagger(self.args, self.vocab, emb_matrix=emb_matrix, share_hid=self.args[\'share_hid\'])\n        self.model.load_state_dict(checkpoint[\'model\'], strict=False)\n'"
stanza/models/pos/vocab.py,0,"b'from collections import Counter, OrderedDict\n\nfrom stanza.models.common.vocab import BaseVocab, BaseMultiVocab\nfrom stanza.models.common.vocab import CompositeVocab, VOCAB_PREFIX, EMPTY, EMPTY_ID\n\nclass CharVocab(BaseVocab):\n    def build_vocab(self):\n        if type(self.data[0][0]) is list: # general data from DataLoader\n            counter = Counter([c for sent in self.data for w in sent for c in w[self.idx]])\n            for k in list(counter.keys()):\n                if counter[k] < self.cutoff:\n                    del counter[k]\n        else: # special data from Char LM\n            counter = Counter([c for sent in self.data for c in sent])\n        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: (counter[k], k), reverse=True))\n        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}\n\nclass WordVocab(BaseVocab):\n    def __init__(self, data=None, lang="""", idx=0, cutoff=0, lower=False, ignore=[]):\n        self.ignore = ignore\n        super().__init__(data, lang=lang, idx=idx, cutoff=cutoff, lower=lower)\n        self.state_attrs += [\'ignore\']\n\n    def id2unit(self, id):\n        if len(self.ignore) > 0 and id == EMPTY_ID:\n            return \'_\'\n        else:\n            return super().id2unit(id)\n\n    def unit2id(self, unit):\n        if len(self.ignore) > 0 and unit in self.ignore:\n            return self._unit2id[EMPTY]\n        else:\n            return super().unit2id(unit)\n\n    def build_vocab(self):\n        if self.lower:\n            counter = Counter([w[self.idx].lower() for sent in self.data for w in sent])\n        else:\n            counter = Counter([w[self.idx] for sent in self.data for w in sent])\n        for k in list(counter.keys()):\n            if counter[k] < self.cutoff or k in self.ignore:\n                del counter[k]\n\n        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))\n        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}\n\nclass XPOSVocab(CompositeVocab):\n    def __init__(self, data=None, lang="""", idx=0, sep="""", keyed=False):\n        super().__init__(data, lang, idx=idx, sep=sep, keyed=keyed)\n\nclass FeatureVocab(CompositeVocab):\n    def __init__(self, data=None, lang="""", idx=0, sep=""|"", keyed=True):\n        super().__init__(data, lang, idx=idx, sep=sep, keyed=keyed)\n\nclass MultiVocab(BaseMultiVocab):\n    def state_dict(self):\n        """""" Also save a vocab name to class name mapping in state dict. """"""\n        state = OrderedDict()\n        key2class = OrderedDict()\n        for k, v in self._vocabs.items():\n            state[k] = v.state_dict()\n            key2class[k] = type(v).__name__\n        state[\'_key2class\'] = key2class\n        return state\n\n    @classmethod\n    def load_state_dict(cls, state_dict):\n        class_dict = {\'CharVocab\': CharVocab,\n                \'WordVocab\': WordVocab,\n                \'XPOSVocab\': XPOSVocab,\n                \'FeatureVocab\': FeatureVocab}\n        new = cls()\n        assert \'_key2class\' in state_dict, ""Cannot find class name mapping in state dict!""\n        key2class = state_dict.pop(\'_key2class\')\n        for k,v in state_dict.items():\n            classname = key2class[k]\n            new[k] = class_dict[classname].load_state_dict(v)\n        return new\n\n'"
stanza/models/pos/xpos_vocab_factory.py,0,"b'# This is the XPOS factory method generated automatically from models.pos.build_xpos_factory.\n# Please don\'t edit it!\n\nfrom stanza.models.pos.vocab import WordVocab, XPOSVocab\n\ndef xpos_vocab_factory(data, shorthand):\n    if shorthand in [""af_afribooms"", ""grc_perseus"", ""ar_padt"", ""bg_btb"", ""hr_set"", ""cs_cac"", ""cs_cltt"", ""cs_fictree"", ""cs_pdt"", ""en_partut"", ""fr_partut"", ""gl_ctg"", ""it_isdt"", ""it_partut"", ""it_postwita"", ""it_twittiro"", ""it_vit"", ""ja_gsd"", ""lv_lvtb"", ""lt_alksnis"", ""ro_nonstandard"", ""ro_rrt"", ""gd_arcosg"", ""sr_set"", ""sk_snk"", ""sl_ssj"", ""ta_ttb"", ""uk_iu"", ""gl_treegal"", ""la_perseus"", ""sl_sst""]:\n        return XPOSVocab(data, shorthand, idx=2, sep="""")\n    elif shorthand in [""grc_proiel"", ""hy_armtdp"", ""eu_bdt"", ""be_hse"", ""ca_ancora"", ""zh-hant_gsd"", ""zh-hans_gsdsimp"", ""lzh_kyoto"", ""cop_scriptorium"", ""da_ddt"", ""en_ewt"", ""en_gum"", ""et_edt"", ""fi_tdt"", ""fr_ftb"", ""fr_gsd"", ""fr_sequoia"", ""fr_spoken"", ""de_gsd"", ""de_hdt"", ""got_proiel"", ""el_gdt"", ""he_htb"", ""hi_hdtb"", ""hu_szeged"", ""ga_idt"", ""ja_bccwj"", ""la_proiel"", ""lt_hse"", ""mt_mudt"", ""mr_ufal"", ""nb_bokmaal"", ""nn_nynorsk"", ""nn_nynorsklia"", ""cu_proiel"", ""fro_srcmf"", ""orv_torot"", ""fa_seraji"", ""pt_bosque"", ""pt_gsd"", ""ru_gsd"", ""ru_syntagrus"", ""ru_taiga"", ""es_ancora"", ""es_gsd"", ""swl_sslc"", ""te_mtg"", ""tr_imst"", ""ug_udt"", ""vi_vtb"", ""wo_wtb"", ""bxr_bdt"", ""et_ewt"", ""kk_ktb"", ""kmr_mg"", ""olo_kkpp"", ""sme_giella"", ""hsb_ufal""]:\n        return WordVocab(data, shorthand, idx=2, ignore=[""_""])\n    elif shorthand in [""nl_alpino"", ""nl_lassysmall"", ""la_ittb"", ""sv_talbanken""]:\n        return XPOSVocab(data, shorthand, idx=2, sep=""|"")\n    elif shorthand in [""en_lines"", ""sv_lines"", ""ur_udtb""]:\n        return XPOSVocab(data, shorthand, idx=2, sep=""-"")\n    elif shorthand in [""fi_ftb""]:\n        return XPOSVocab(data, shorthand, idx=2, sep="","")\n    elif shorthand in [""id_gsd"", ""ko_gsd"", ""ko_kaist""]:\n        return XPOSVocab(data, shorthand, idx=2, sep=""+"")\n    elif shorthand in [""pl_lfg"", ""pl_pdb""]:\n        return XPOSVocab(data, shorthand, idx=2, sep="":"")\n    else:\n        raise NotImplementedError(\'Language shorthand ""{}"" not found!\'.format(shorthand))\n'"
stanza/models/tokenize/__init__.py,0,b''
stanza/models/tokenize/data.py,1,"b'from bisect import bisect_right\nfrom copy import copy\nimport json\nimport numpy as np\nimport random\nimport logging\nimport re\nimport torch\n\nfrom .vocab import Vocab\n\nlogger = logging.getLogger(\'stanza\')\n\nclass DataLoader:\n    def __init__(self, args, input_files={\'json\': None, \'txt\': None, \'label\': None}, input_text=None, input_data=None, vocab=None, evaluation=False):\n        self.args = args\n        self.eval = evaluation\n\n        # get input files\n        json_file = input_files[\'json\']\n        txt_file = input_files[\'txt\']\n        label_file = input_files[\'label\']\n\n        # Load data and process it\n        if input_data is not None:\n            self.data = input_data\n        elif json_file is not None:\n            with open(json_file) as f:\n                self.data = json.load(f)\n        else:\n            # set up text from file or input string\n            assert txt_file is not None or input_text is not None\n            if input_text is None:\n                with open(txt_file) as f:\n                    text = \'\'.join(f.readlines()).rstrip()\n            else:\n                text = input_text\n\n            if label_file is not None:\n                with open(label_file) as f:\n                    labels = \'\'.join(f.readlines()).rstrip()\n            else:\n                labels = \'\\n\\n\'.join([\'0\' * len(pt.rstrip()) for pt in re.split(\'\\n\\s*\\n\', text)])\n\n            self.data = [[(re.sub(\'\\s\', \' \', char), int(label)) # substitute special whitespaces\n                    for char, label in zip(pt.rstrip(), pc) if not (args.get(\'skip_newline\', False) and char == \'\\n\')] # check if newline needs to be eaten\n                    for pt, pc in zip(re.split(\'\\n\\s*\\n\', text), labels.split(\'\\n\\n\')) if len(pt.rstrip()) > 0]\n\n        self.vocab = vocab if vocab is not None else self.init_vocab()\n\n        # data comes in a list of paragraphs, where each paragraph is a list of units with unit-level labels\n        self.sentences = [self.para_to_sentences(para) for para in self.data]\n\n        self.init_sent_ids()\n        logger.debug(f""{len(self.sentence_ids)} sentences loaded."")\n\n    def init_vocab(self):\n        vocab = Vocab(self.data, self.args[\'lang\'])\n        return vocab\n\n    def init_sent_ids(self):\n        self.sentence_ids = []\n        self.cumlen = [0]\n        for i, para in enumerate(self.sentences):\n            for j in range(len(para)):\n                self.sentence_ids += [(i, j)]\n                self.cumlen += [self.cumlen[-1] + len(self.sentences[i][j])]\n\n    def para_to_sentences(self, para):\n        res = []\n        funcs = []\n        for feat_func in self.args[\'feat_funcs\']:\n            if feat_func == \'end_of_para\' or feat_func == \'start_of_para\':\n                # skip for position-dependent features\n                continue\n            if feat_func == \'space_before\':\n                func = lambda x: 1 if x.startswith(\' \') else 0\n            elif feat_func == \'capitalized\':\n                func = lambda x: 1 if x[0].isupper() else 0\n            elif feat_func == \'all_caps\':\n                func = lambda x: 1 if x.isupper() else 0\n            elif feat_func == \'numeric\':\n                func = lambda x: 1 if (re.match(\'^([\\d]+[,\\.]*)+$\', x) is not None) else 0\n            else:\n                raise Exception(\'Feature function ""{}"" is undefined.\'.format(feat_func))\n\n            funcs.append(func)\n        \n        # stacking all featurize functions\n        composite_func = lambda x: list(map(lambda f: f(x), funcs))\n\n        def process_sentence(sent):\n            return [(self.vocab.unit2id(y[0]), y[1], y[2], y[0]) for y in sent]\n\n        current = []\n        for i, (unit, label) in enumerate(para):\n            label1 = label if not self.eval else 0\n            feats = composite_func(unit)\n            # position-dependent features\n            if \'end_of_para\' in self.args[\'feat_funcs\']:\n                f = 1 if i == len(para)-1 else 0\n                feats.append(f)\n            if \'start_of_para\' in self.args[\'feat_funcs\']:\n                f = 1 if i == 0 else 0\n                feats.append(f)\n            current += [[unit, label, feats]]\n            if label1 == 2 or label1 == 4: # end of sentence\n                if len(current) <= self.args[\'max_seqlen\']:\n                    # get rid of sentences that are too long during training of the tokenizer\n                    res += [process_sentence(current)]\n                current = []\n\n        if len(current) > 0:\n            if self.eval or len(current) <= self.args[\'max_seqlen\']:\n                res += [process_sentence(current)]\n\n        return res\n\n    def __len__(self):\n        return len(self.sentence_ids)\n\n    def shuffle(self):\n        for para in self.sentences:\n            random.shuffle(para)\n        self.init_sent_ids()\n\n    def next(self, eval_offsets=None, unit_dropout=0.0):\n        null_feats = [0] * len(self.sentences[0][0][0][2])\n\n        def strings_starting(id_pair, offset=0, pad_len=self.args[\'max_seqlen\']):\n            pid, sid = id_pair\n            res = copy(self.sentences[pid][sid][offset:])\n\n            assert self.eval or len(res) <= self.args[\'max_seqlen\'], \'The maximum sequence length {} is less than that of the longest sentence length ({}) in the data, consider increasing it! {}\'.format(self.args[\'max_seqlen\'], len(res), \' \'.join([""{}/{}"".format(*x) for x in self.sentences[pid][sid]]))\n            for sid1 in range(sid+1, len(self.sentences[pid])):\n                res += self.sentences[pid][sid1]\n\n                if not self.eval and len(res) >= self.args[\'max_seqlen\']:\n                    res = res[:self.args[\'max_seqlen\']]\n                    break\n\n            if unit_dropout > 0 and not self.eval:\n                unkid = self.vocab.unit2id(\'<UNK>\')\n                res = [(unkid, x[1], x[2], \'<UNK>\') if random.random() < unit_dropout else x for x in res]\n\n            # pad with padding units and labels if necessary\n            if pad_len > 0 and len(res) < pad_len:\n                padid = self.vocab.unit2id(\'<PAD>\')\n                res += [(padid, -1, null_feats, \'<PAD>\')] * (pad_len - len(res))\n\n            return res\n\n        if eval_offsets is not None:\n            # find max padding length\n            pad_len = 0\n            for eval_offset in eval_offsets:\n                if eval_offset < self.cumlen[-1]:\n                    pair_id = bisect_right(self.cumlen, eval_offset) - 1\n                    pair = self.sentence_ids[pair_id]\n                    pad_len = max(pad_len, len(strings_starting(pair, offset=eval_offset-self.cumlen[pair_id], pad_len=0)))\n\n            res = []\n            pad_len += 1\n            for eval_offset in eval_offsets:\n                # find unit\n                if eval_offset >= self.cumlen[-1]:\n                    padid = self.vocab.unit2id(\'<PAD>\')\n                    res += [[(padid, -1, null_feats, \'<PAD>\')] * pad_len]\n                    continue\n\n                pair_id = bisect_right(self.cumlen, eval_offset) - 1\n                pair = self.sentence_ids[pair_id]\n                res += [strings_starting(pair, offset=eval_offset-self.cumlen[pair_id], pad_len=pad_len)]\n        else:\n            id_pairs = random.sample(self.sentence_ids, min(len(self.sentence_ids), self.args[\'batch_size\']))\n            res = [strings_starting(pair) for pair in id_pairs]\n\n        units = [[y[0] for y in x] for x in res]\n        labels = [[y[1] for y in x] for x in res]\n        features = [[y[2] for y in x] for x in res]\n        raw_units = [[y[3] for y in x] for x in res]\n\n        convert = lambda t: (torch.from_numpy(np.array(t[0], dtype=t[1])))\n\n        units, labels, features = list(map(convert, [(units, np.int64), (labels, np.int64), (features, np.float32)]))\n\n        return units, labels, features, raw_units\n\n'"
stanza/models/tokenize/model.py,6,"b""import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nclass Tokenizer(nn.Module):\n    def __init__(self, args, nchars, emb_dim, hidden_dim, N_CLASSES=5, dropout=0):\n        super().__init__()\n\n        self.args = args\n        feat_dim = args['feat_dim']\n\n        self.embeddings = nn.Embedding(nchars, emb_dim, padding_idx=0)\n\n        self.rnn = nn.LSTM(emb_dim + feat_dim, hidden_dim, num_layers=self.args['rnn_layers'], bidirectional=True, batch_first=True, dropout=dropout if self.args['rnn_layers'] > 1 else 0)\n\n        if self.args['conv_res'] is not None:\n            self.conv_res = nn.ModuleList()\n            self.conv_sizes = [int(x) for x in self.args['conv_res'].split(',')]\n\n            for si, size in enumerate(self.conv_sizes):\n                l = nn.Conv1d(emb_dim + feat_dim, hidden_dim * 2, size, padding=size//2, bias=self.args.get('hier_conv_res', False) or (si == 0))\n                self.conv_res.append(l)\n\n            if self.args.get('hier_conv_res', False):\n                self.conv_res2 = nn.Conv1d(hidden_dim * 2 * len(self.conv_sizes), hidden_dim * 2, 1)\n        self.tok_clf = nn.Linear(hidden_dim * 2, 1)\n        self.sent_clf = nn.Linear(hidden_dim * 2, 1)\n        self.mwt_clf = nn.Linear(hidden_dim * 2, 1)\n\n        if args['hierarchical']:\n            in_dim = hidden_dim * 2\n            self.rnn2 = nn.LSTM(in_dim, hidden_dim, num_layers=1, bidirectional=True, batch_first=True)\n            self.tok_clf2 = nn.Linear(hidden_dim * 2, 1, bias=False)\n            self.sent_clf2 = nn.Linear(hidden_dim * 2, 1, bias=False)\n            self.mwt_clf2 = nn.Linear(hidden_dim * 2, 1, bias=False)\n\n        self.dropout = nn.Dropout(dropout)\n        self.toknoise = nn.Dropout(self.args['tok_noise'])\n\n    def forward(self, x, feats):\n        emb = self.embeddings(x)\n\n        emb = self.dropout(emb)\n\n        emb = torch.cat([emb, feats], 2)\n\n        inp, _ = self.rnn(emb)\n\n        if self.args['conv_res'] is not None:\n            conv_input = emb.transpose(1, 2).contiguous()\n            if not self.args.get('hier_conv_res', False):\n                for l in self.conv_res:\n                    inp = inp + l(conv_input).transpose(1, 2).contiguous()\n            else:\n                hid = []\n                for l in self.conv_res:\n                    hid += [l(conv_input)]\n                hid = torch.cat(hid, 1)\n                hid = F.relu(hid)\n                hid = self.dropout(hid)\n                inp = inp + self.conv_res2(hid).transpose(1, 2).contiguous()\n\n        inp = self.dropout(inp)\n\n        tok0 = self.tok_clf(inp)\n        sent0 = self.sent_clf(inp)\n        mwt0 = self.mwt_clf(inp)\n\n        if self.args['hierarchical']:\n            if self.args['hier_invtemp'] > 0:\n                inp2, _ = self.rnn2(inp * (1 - self.toknoise(torch.sigmoid(-tok0 * self.args['hier_invtemp']))))\n            else:\n                inp2, _ = self.rnn2(inp)\n\n            inp2 = self.dropout(inp2)\n\n            tok0 = tok0 + self.tok_clf2(inp2)\n            sent0 = sent0 + self.sent_clf2(inp2)\n            mwt0 = mwt0 + self.mwt_clf2(inp2)\n\n        nontok = F.logsigmoid(-tok0)\n        tok = F.logsigmoid(tok0)\n        nonsent = F.logsigmoid(-sent0)\n        sent = F.logsigmoid(sent0)\n        nonmwt = F.logsigmoid(-mwt0)\n        mwt = F.logsigmoid(mwt0)\n\n        pred = torch.cat([nontok, tok+nonsent+nonmwt, tok+sent+nonmwt, tok+nonsent+mwt, tok+sent+mwt], 2)\n\n        return pred\n"""
stanza/models/tokenize/trainer.py,4,"b'import sys\nimport logging\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom stanza.models.common.trainer import Trainer as BaseTrainer\n\nfrom .model import Tokenizer\nfrom .vocab import Vocab\n\nlogger = logging.getLogger(\'stanza\')\n\nclass Trainer(BaseTrainer):\n    def __init__(self, args=None, vocab=None, model_file=None, use_cuda=False):\n        self.use_cuda = use_cuda\n        if model_file is not None:\n            # load everything from file\n            self.load(model_file)\n        else:\n            # build model from scratch\n            self.args = args\n            self.vocab = vocab\n            self.model = Tokenizer(self.args, self.args[\'vocab_size\'], self.args[\'emb_dim\'], self.args[\'hidden_dim\'], dropout=self.args[\'dropout\'])\n        self.criterion = nn.CrossEntropyLoss(ignore_index=-1)\n        if use_cuda:\n            self.model.cuda()\n            self.criterion.cuda()\n        else:\n            self.model.cpu()\n            self.criterion.cpu()\n        self.parameters = [p for p in self.model.parameters() if p.requires_grad]\n        self.optimizer = optim.Adam(self.parameters, lr=self.args[\'lr0\'], betas=(.9, .9), weight_decay=self.args[\'weight_decay\'])\n        self.feat_funcs = self.args.get(\'feat_funcs\', None)\n        self.lang = self.args[\'lang\'] # language determines how token normalization is done\n\n    def update(self, inputs):\n        self.model.train()\n        units, labels, features, _ = inputs\n\n        if self.use_cuda:\n            units = units.cuda()\n            labels = labels.cuda()\n            features = features.cuda()\n\n        pred = self.model(units, features)\n\n        self.optimizer.zero_grad()\n        classes = pred.size(2)\n        loss = self.criterion(pred.view(-1, classes), labels.view(-1))\n\n        loss.backward()\n        nn.utils.clip_grad_norm_(self.model.parameters(), self.args[\'max_grad_norm\'])\n        self.optimizer.step()\n\n        return loss.item()\n\n    def predict(self, inputs):\n        self.model.eval()\n        units, labels, features, _ = inputs\n\n        if self.use_cuda:\n            units = units.cuda()\n            labels = labels.cuda()\n            features = features.cuda()\n\n        pred = self.model(units, features)\n\n        return pred.data.cpu().numpy()\n\n    def save(self, filename):\n        params = {\n                \'model\': self.model.state_dict() if self.model is not None else None,\n                \'vocab\': self.vocab.state_dict(),\n                \'config\': self.args\n                }\n        try:\n            torch.save(params, filename)\n            logger.info(""Model saved to {}"".format(filename))\n        except BaseException:\n            logger.warning(""Saving failed... continuing anyway."")\n\n    def load(self, filename):\n        try:\n            checkpoint = torch.load(filename, lambda storage, loc: storage)\n        except BaseException:\n            logger.exception(""Cannot load model from {}"".format(filename))\n            sys.exit(1)\n        self.args = checkpoint[\'config\']\n        self.model = Tokenizer(self.args, self.args[\'vocab_size\'], self.args[\'emb_dim\'], self.args[\'hidden_dim\'], dropout=self.args[\'dropout\'])\n        self.model.load_state_dict(checkpoint[\'model\'])\n        self.vocab = Vocab.load_state_dict(checkpoint[\'vocab\'])\n'"
stanza/models/tokenize/utils.py,0,"b'from collections import Counter\nfrom copy import copy\nimport json\nimport numpy as np\nimport re\nimport logging\n\nfrom stanza.models.common.utils import ud_scores, harmonic_mean\nfrom stanza.utils.conll import CoNLL\nfrom stanza.models.common.doc import *\n\nlogger = logging.getLogger(\'stanza\')\n\ndef load_mwt_dict(filename):\n    if filename is not None:\n        with open(filename, \'r\') as f:\n            mwt_dict0 = json.load(f)\n\n        mwt_dict = dict()\n        for item in mwt_dict0:\n            (key, expansion), count = item\n\n            if key not in mwt_dict or mwt_dict[key][1] < count:\n                mwt_dict[key] = (expansion, count)\n\n        return mwt_dict\n    else:\n        return\n\ndef process_sentence(sentence, mwt_dict=None):\n    sent = []\n    i = 0\n    for tok, p, additional_info in sentence:\n        expansion = None\n        if (p == 3 or p == 4) and mwt_dict is not None:\n            # MWT found, (attempt to) expand it!\n            if tok in mwt_dict:\n                expansion = mwt_dict[tok][0]\n            elif tok.lower() in mwt_dict:\n                expansion = mwt_dict[tok.lower()][0]\n        if expansion is not None:\n            infostr = None if len(additional_info) == 0 else \'|\'.join([f""{k}={additional_info[k]}"" for k in additional_info])\n            sent.append({ID: f\'{i+1}-{i+len(expansion)}\', TEXT: tok})\n            if infostr is not None: sent[-1][MISC] = infostr\n            for etok in expansion:\n                sent.append({ID: f\'{i+1}\', TEXT: etok})\n                i += 1\n        else:\n            if len(tok) <= 0:\n                continue\n            if p == 3 or p == 4:\n                additional_info[\'MWT\'] = \'Yes\'\n            infostr = None if len(additional_info) == 0 else \'|\'.join([f""{k}={additional_info[k]}"" for k in additional_info])\n            sent.append({ID: f\'{i+1}\', TEXT: tok})\n            if infostr is not None: sent[-1][MISC] = infostr\n            i += 1\n    return sent\n\ndef find_token(token, text):\n    """"""\n    Robustly finds the first occurrence of token in the text, and return its offset and it\'s underlying original string.\n    Ignores whitespace mismatches between the text and the token.\n    """"""\n    m = re.search(r\'\\s*\'.join([r\'\\s\' if re.match(r\'\\s\', x) else re.escape(x) for x in token]), text)\n    return m.start(), m.group()\n\ndef output_predictions(output_file, trainer, data_generator, vocab, mwt_dict, max_seqlen=1000, orig_text=None, no_ssplit=False):\n    paragraphs = []\n    for i, p in enumerate(data_generator.sentences):\n        start = 0 if i == 0 else paragraphs[-1][2]\n        length = sum([len(x) for x in p])\n        paragraphs += [(i, start, start+length, length+1)] # para idx, start idx, end idx, length\n\n    paragraphs = list(sorted(paragraphs, key=lambda x: x[3], reverse=True))\n\n    all_preds = [None] * len(paragraphs)\n    all_raw = [None] * len(paragraphs)\n\n    eval_limit = max(3000, max_seqlen)\n\n    batch_size = trainer.args[\'batch_size\']\n    batches = int((len(paragraphs) + batch_size - 1) / batch_size)\n\n    t = 0\n    for i in range(batches):\n        batchparas = paragraphs[i * batch_size : (i + 1) * batch_size]\n        offsets = [x[1] for x in batchparas]\n        t += sum([x[3] for x in batchparas])\n\n        batch = data_generator.next(eval_offsets=offsets)\n        raw = batch[3]\n\n        N = len(batch[3][0])\n        if N <= eval_limit:\n            pred = np.argmax(trainer.predict(batch), axis=2)\n        else:\n            idx = [0] * len(batchparas)\n            Ns = [p[3] for p in batchparas]\n            pred = [[] for _ in batchparas]\n            while True:\n                ens = [min(N - idx1, eval_limit) for idx1, N in zip(idx, Ns)]\n                en = max(ens)\n                batch1 = batch[0][:, :en], batch[1][:, :en], batch[2][:, :en], [x[:en] for x in batch[3]]\n                pred1 = np.argmax(trainer.predict(batch1), axis=2)\n\n                for j in range(len(batchparas)):\n                    sentbreaks = np.where((pred1[j] == 2) + (pred1[j] == 4))[0]\n                    if len(sentbreaks) <= 0 or idx[j] >= Ns[j] - eval_limit:\n                        advance = ens[j]\n                    else:\n                        advance = np.max(sentbreaks) + 1\n\n                    pred[j] += [pred1[j, :advance]]\n                    idx[j] += advance\n\n                if all([idx1 >= N for idx1, N in zip(idx, Ns)]):\n                    break\n                batch = data_generator.next(eval_offsets=[x+y for x, y in zip(idx, offsets)])\n\n            pred = [np.concatenate(p, 0) for p in pred]\n\n        for j, p in enumerate(batchparas):\n            len1 = len([1 for x in raw[j] if x != \'<PAD>\'])\n            if pred[j][len1-1] < 2:\n                pred[j][len1-1] = 2\n            elif pred[j][len1-1] > 2:\n                pred[j][len1-1] = 4\n            all_preds[p[0]] = pred[j][:len1]\n            all_raw[p[0]] = raw[j]\n\n    offset = 0\n    oov_count = 0\n    doc = []\n\n    text = orig_text\n    char_offset = 0\n\n    for j in range(len(paragraphs)):\n        raw = all_raw[j]\n        pred = all_preds[j]\n\n        current_tok = \'\'\n        current_sent = []\n\n        for t, p in zip(raw, pred):\n            if t == \'<PAD>\':\n                break\n            # hack la_ittb\n            if trainer.args[\'shorthand\'] == \'la_ittb\' and t in ["":"", "";""]:\n                p = 2\n            offset += 1\n            if vocab.unit2id(t) == vocab.unit2id(\'<UNK>\'):\n                oov_count += 1\n\n            current_tok += t\n            if p >= 1:\n                tok = vocab.normalize_token(current_tok)\n                assert \'\\t\' not in tok, tok\n                if len(tok) <= 0:\n                    current_tok = \'\'\n                    continue\n                if orig_text is not None:\n                    st0, tok0 = find_token(tok, text)\n                    st = char_offset + st0\n                    text = text[st0 + len(tok0):]\n                    char_offset += st0 + len(tok0)\n                    additional_info = {START_CHAR: st, END_CHAR: st + len(tok0)}\n                else:\n                    additional_info = dict()\n                current_sent += [(tok, p, additional_info)]\n                current_tok = \'\'\n                if (p == 2 or p == 4) and not no_ssplit:\n                    doc.append(process_sentence(current_sent, mwt_dict))\n                    current_sent = []\n\n        assert(len(current_tok) == 0)\n        if len(current_sent):\n            doc.append(process_sentence(current_sent, mwt_dict))\n\n    if output_file: CoNLL.dict2conll(doc, output_file)\n    return oov_count, offset, all_preds, doc\n\ndef eval_model(args, trainer, batches, vocab, mwt_dict):\n    oov_count, N, all_preds, doc = output_predictions(args[\'conll_file\'], trainer, batches, vocab, mwt_dict, args[\'max_seqlen\'])\n\n    all_preds = np.concatenate(all_preds, 0)\n    labels = [y[1] for x in batches.data for y in x]\n    counter = Counter(zip(all_preds, labels))\n\n    def f1(pred, gold, mapping):\n        pred = [mapping[p] for p in pred]\n        gold = [mapping[g] for g in gold]\n\n        lastp = -1; lastg = -1\n        tp = 0; fp = 0; fn = 0\n        for i, (p, g) in enumerate(zip(pred, gold)):\n            if p == g > 0 and lastp == lastg:\n                lastp = i\n                lastg = i\n                tp += 1\n            elif p > 0 and g > 0:\n                lastp = i\n                lastg = i\n                fp += 1\n                fn += 1\n            elif p > 0:\n                # and g == 0\n                lastp = i\n                fp += 1\n            elif g > 0:\n                lastg = i\n                fn += 1\n\n        if tp == 0:\n            return 0\n        else:\n            return 2 * tp / (2 * tp + fp + fn)\n\n    f1tok = f1(all_preds, labels, {0:0, 1:1, 2:1, 3:1, 4:1})\n    f1sent = f1(all_preds, labels, {0:0, 1:0, 2:1, 3:0, 4:1})\n    f1mwt = f1(all_preds, labels, {0:0, 1:1, 2:1, 3:2, 4:2})\n    logger.info(f""{args[\'shorthand\']}: token F1 = {f1tok*100:.2f}, sentence F1 = {f1sent*100:.2f}, mwt F1 = {f1mwt*100:.2f}"")\n    return harmonic_mean([f1tok, f1sent, f1mwt], [1, 1, .01])\n\n'"
stanza/models/tokenize/vocab.py,0,"b""from collections import Counter\nimport re\n\nfrom stanza.models.common.vocab import BaseVocab\nfrom stanza.models.common.vocab import UNK, PAD\n\nclass Vocab(BaseVocab):\n    def build_vocab(self):\n        paras = self.data\n        counter = Counter()\n        for para in paras:\n            for unit in para:\n                normalized = self.normalize_unit(unit[0])\n                counter[normalized] += 1\n\n        self._id2unit = [PAD, UNK] + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))\n        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}\n\n    def normalize_unit(self, unit):\n        # Normalize minimal units used by the tokenizer\n        # For Vietnamese this means a syllable, for other languages this means a character\n        normalized = unit\n        if self.lang.startswith('vi'):\n            normalized = normalized.lstrip()\n\n        return normalized\n\n    def normalize_token(self, token):\n        token = re.sub('\\s', ' ', token.lstrip())\n\n        if any([self.lang.startswith(x) for x in ['zh', 'ja', 'ko']]):\n            token = token.replace(' ', '')\n\n        return token\n"""
stanza/pipeline/demo/demo_server.py,0,"b""from flask import Flask, request, abort\nimport json\nimport stanza\nimport os\napp = Flask(__name__, static_url_path='', static_folder=os.path.abspath(os.path.dirname(__file__)))\n\npipelineCache = dict()\n\ndef get_file(path):\n    res = os.path.join(os.path.dirname(os.path.abspath(__file__)), path)\n    print(res)\n    return res\n\n@app.route('/<path:path>')\ndef static_file(path):\n    if path in ['stanza-brat.css', 'stanza-brat.js', 'stanza-parseviewer.js', 'loading.gif',\n            'favicon.png', 'stanza-logo.png']:\n        return app.send_static_file(path)\n    elif path in 'index.html':\n        return app.send_static_file('stanza-brat.html')\n    else:\n        abort(403)\n\n@app.route('/', methods=['GET'])\ndef index():\n    return static_file('index.html')\n\n@app.route('/', methods=['POST'])\ndef annotate():\n    global pipelineCache\n\n    properties = request.args.get('properties', '')\n    lang = request.args.get('pipelineLanguage', '')\n    text = list(request.form.keys())[0]\n\n    if lang not in pipelineCache:\n        pipelineCache[lang] = stanza.Pipeline(lang=lang, use_gpu=False)\n\n    res = pipelineCache[lang](text)\n\n    annotated_sentences = []\n    for sentence in res.sentences:\n        tokens = []\n        deps = []\n        for word in sentence.words:\n            tokens.append({'index': word.id, 'word': word.text, 'lemma': word.lemma, 'pos': word.xpos, 'upos': word.upos, 'feats': word.feats, 'ner': word.parent.ner if word.parent.ner is None or word.parent.ner == 'O' else word.parent.ner[2:]})\n            deps.append({'dep': word.deprel, 'governor': word.head, 'governorGloss': sentence.words[word.head-1].text,\n                'dependent': word.id, 'dependentGloss': word.text})\n        annotated_sentences.append({'basicDependencies': deps, 'tokens': tokens})\n\n    return json.dumps({'sentences': annotated_sentences})\n\ndef create_app():\n    return app\n"""
