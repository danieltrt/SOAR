file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\nfrom os import path\nimport re\n\n\ndef readme():\n    with open(\'README.md\', encoding=\'utf-8\') as f:\n        return f.read()\n\n\ndef version():\n    this_directory = path.abspath(path.dirname(__file__))\n    with open(path.join(this_directory, \'livelossplot/version.py\')) as f:\n        version_file = f.read()\n        version_match = re.search(r""^__version__ = [\'\\""]([^\'\\""]*)[\'\\""]"", version_file, re.M)\n        version = version_match.group(1)\n\n    return version\n\n\nsetup(\n    name=\'livelossplot\',\n    version=version(),\n    python_requires="">=3.5"",\n    install_requires=[\n        \'ipython\', \'matplotlib;python_version>=""3.6""\', \'matplotlib<3.1;python_version<""3.6""\',\n        \'numpy<1.18;python_version<""3.6""\',\n        \'bokeh;python_version>=""3.6""\', \'bokeh<=1.4.0;python_version<""3.6""\'\n    ],\n    description=\'Live training loss plot in Jupyter Notebook for Keras, PyTorch and others.\',\n    long_description=readme(),\n    long_description_content_type=\'text/markdown\',\n    url=\'https://github.com/stared/livelossplot\',\n    author=\'Piotr Migda\xc5\x82\',\n    author_email=\'pmigdal@gmail.com\',\n    keywords=[\'keras\', \'pytorch\', \'plot\', \'chart\', \'deep-learning\'],\n    license=\'MIT\',\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        \'Framework :: Jupyter\',\n        \'Intended Audience :: Developers\',\n        \'Intended Audience :: Education\',\n        \'Intended Audience :: Science/Research\',\n        \'Topic :: Scientific/Engineering\',\n        \'Topic :: Scientific/Engineering :: Artificial Intelligence\',\n        \'Topic :: Scientific/Engineering :: Visualization\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Programming Language :: Python :: 3\',\n        \'Programming Language :: Python :: 3.5\',\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Python :: 3.8\',\n    ],\n    packages=find_packages(),\n    zip_safe=False\n)\n'"
examples/neptune.py,0,"b""# TO START:\n# pip install neptune-client, livelossplot\n# export environment variables\n# enjoy results\n\nimport os\nfrom time import sleep\n\nimport numpy as np\n\nfrom livelossplot.outputs import NeptuneLogger\nfrom livelossplot import PlotLosses\n\n\ndef main():\n    api_token = os.environ.get('NEPTUNE_API_TOKEN')\n    project_qualified_name = os.environ.get('NEPTUNE_PROJECT_NAME')\n    logger = NeptuneLogger(api_token=api_token, project_qualified_name=project_qualified_name)\n    liveplot = PlotLosses(outputs=[logger])\n    for i in range(20):\n        liveplot.update(\n            {\n                'accuracy': 1 - np.random.rand() / (i + 2.),\n                'val_accuracy': 1 - np.random.rand() / (i + 0.5),\n                'mse': 1. / (i + 2.),\n                'val_mse': 1. / (i + 0.5)\n            }\n        )\n        liveplot.send()\n        sleep(.5)\n\n\nif __name__ == '__main__':\n    main()\n"""
livelossplot/__init__.py,0,"b'import sys\nimport warnings\nfrom importlib.util import find_spec\n\nfrom .main_logger import MainLogger\nfrom .plot_losses import PlotLosses\nfrom . import inputs\nfrom .inputs import *\nfrom . import outputs\nfrom .version import __version__\n\n_input_plugin_dict = {\n    \'keras\': \'Keras\',\n    \'tf_keras\': \'KerasTF\',\n    \'pytorch_ignite\': \'Ignite\',\n    \'poutyne\': \'Poutyne\',\n}\n\n\nclass OldDependenciesFinder:\n    """"""\n    Data package module loader finder. This class sits on `sys.meta_path` and returns the\n    loader it knows for a given path, if it knows a compatible loader.\n    """"""\n    @classmethod\n    def find_spec(self, fullname, *_, **__):\n        """"""\n        This functions is what gets executed by the loader.\n        """"""\n        parts = fullname.split(\'.\')\n        if len(parts) == 2 and parts[0] == \'livelossplot\' and parts[1] in _input_plugin_dict:\n            name = parts[1]\n            msg = \'livelossplot.{name} will be deprecated, please use livelossplot.inputs.{name}\\n\'\n            msg += \'or use callback directly: from livelossplot import PlotLosses{new_name}\'\n            warnings.warn(msg.format(name=name, new_name=_input_plugin_dict[name]), DeprecationWarning)\n            fullname = \'livelossplot.inputs.{name}\'.format(name=name)\n            return find_spec(fullname)\n        return None\n\n\nsys.meta_path.append(OldDependenciesFinder())\n\n__all__ = [\n    \'MainLogger\', \'inputs\', \'outputs\', \'PlotLosses\', \'PlotLossesKeras\', \'PlotLossesKerasTF\', \'PlotLossesIgnite\',\n    \'PlotLossesPoutyne\'\n]\n'"
livelossplot/main_logger.py,0,"b'import re\nfrom collections import OrderedDict\nfrom typing import NamedTuple, Dict, List, Pattern, Tuple, Optional\n\n# Value of metrics - for value later, we want to support numpy arrays etc\nLogItem = NamedTuple(\'LogItem\', [(\'step\', int), (\'value\', float)])\nCOMMON_NAME_SHORTCUTS = {\n    \'acc\': \'Accuracy\',\n    \'nll\': \'Log Loss (cost function)\',\n    \'mse\': \'Mean Squared Error\',\n    \'loss\': \'Loss\'\n}\n\n\nclass MainLogger:\n    """"""\n    Main logger - the aim of this class is to store every log from training\n    Log is a float value with corresponding training engine step\n    """"""\n    def __init__(\n        self,\n        groups: Optional[Dict[str, List[str]]] = None,\n        metric_to_name: Optional[Dict[str, str]] = None,\n        current_step: int = -1,\n        auto_generate_groups_if_not_available: bool = True,\n        auto_generate_metric_to_name: bool = True,\n        group_patterns: List[Tuple[str, str]] = [\n            (r\'^(?!val(_|-))(.*)\', \'training \'),\n            (r\'^(val(_|-))(.*)\', \'validation \'),\n        ]\n    ):\n        """"""\n        :param groups - dictionary with grouped metrics for example one group can contains\n         one metric in different stages for example validation, training etc.:\n        :param metric_to_name - transformation of metric name which can be used to display name:\n        :param current_step - current step of the train engine:\n        :param auto_generate_groups_if_not_available - flag, that enable auto-creation of metric groups:\n        :param auto_generate_metric_to_name - flag, that enable auto-creation of metric long names,\n         based on common shortcuts:\n        :param group_patterns - you can put there regular expressions to match a few metric names with group\n         and replace its name using second value:\n        """"""\n        self.log_history = {}\n        self.groups = groups if groups is not None else {}\n        self.metric_to_name = metric_to_name if metric_to_name else {}\n        self.current_step = current_step\n        self.auto_generate_groups = all((not groups, auto_generate_groups_if_not_available))\n        self.auto_generate_metric_to_name = auto_generate_metric_to_name\n        self.group_patterns = tuple((re.compile(pattern), replace_with) for pattern, replace_with in group_patterns)\n\n    def update(self, logs: dict, current_step: Optional[int] = None) -> None:\n        """"""Update logs - loop step can be controlled outside or inside main logger""""""\n        if current_step is None:\n            self.current_step += 1\n            current_step = self.current_step\n        else:\n            self.current_step = current_step\n        for k, v in logs.items():\n            if k not in self.log_history:\n                self._add_new_metric(k)\n            self.log_history[k].append(LogItem(step=current_step, value=v))\n\n    def _add_new_metric(self, metric_name: str):\n        """"""Add empty list for a new metric and extend metric name transformations""""""\n        self.log_history[metric_name] = []\n        if not self.metric_to_name.get(metric_name):\n            self._auto_generate_metrics_to_name(metric_name)\n\n    def _auto_generate_metrics_to_name(self, metric_name: str):\n        """"""\n        The function generate transforms for metric names base on patterns.\n        For example it can create transformation from val_acc to Validation Accuracy\n        :param metric_name - name of new appended metric\n        :param patterns - a tuple with pairs - pattern and value to replace it with\n        """"""\n        suffix = self._find_suffix_with_group_patterns(metric_name)\n        if suffix is None and suffix != metric_name:\n            return\n        similar_metric_names = [m for m in self.log_history.keys() if m.endswith(suffix)]\n        if len(similar_metric_names) == 1:\n            return\n        for name in similar_metric_names:\n            new_name = name\n            for pattern_to_replace, replace_with in self.group_patterns:\n                new_name = re.sub(pattern_to_replace, replace_with, new_name)\n            if suffix in COMMON_NAME_SHORTCUTS.keys():\n                new_name = new_name.replace(suffix, COMMON_NAME_SHORTCUTS[suffix])\n            self.metric_to_name[name] = new_name\n\n    def grouped_log_history(self,\n                            raw_names: bool = False,\n                            raw_group_names: bool = False) -> Dict[str, Dict[str, List[LogItem]]]:\n        """"""\n        :param raw_names - return raw names instead of transformed by metric to name (as in update() input dictionary):\n        :param raw_group_names - return group names without transforming them with COMMON_NAME_SHORTCUTS:\n        :return: logs grouped by metric groups - groups are passed in the class constructor\n        method use group patterns instead of groups if they are available\n        """"""\n        if self.auto_generate_groups:\n            self.groups = self._auto_generate_groups()\n        ret = {}\n        sorted_groups = OrderedDict(sorted(self.groups.items(), key=lambda t: t[0]))\n        for group_name, names in sorted_groups.items():\n            group_name = group_name if raw_group_names else COMMON_NAME_SHORTCUTS.get(group_name, group_name)\n            ret[group_name] = {\n                name if raw_names else self.metric_to_name.get(name, name): self.log_history[name]\n                for name in names\n            }\n        return ret\n\n    def _auto_generate_groups(self) -> Dict[str, List[str]]:\n        """"""\n        Auto create groups base on val_ prefix - this step is skipped if groups are set\n        or if group patterns are available\n        """"""\n        groups = {}\n        for key in self.log_history.keys():\n            abs_key = self._find_suffix_with_group_patterns(key)\n            if not groups.get(abs_key):\n                groups[abs_key] = []\n            groups[abs_key].append(key)\n        return groups\n\n    def _find_suffix_with_group_patterns(self, metric_name: str) -> str:\n        suffix = metric_name\n        for pattern, _ in self.group_patterns:\n            match = re.match(pattern, metric_name)\n            if match:\n                suffix = match.groups()[-1]\n        return suffix\n\n    def reset(self) -> None:\n        """"""Method clears logs, groups and reset step counter""""""\n        self.log_history = {}\n        self.groups = {}\n        self.current_step = -1\n\n    @property\n    def groups(self) -> Dict[str, List[str]]:\n        """"""groups getter""""""\n        return self._groups\n\n    @groups.setter\n    def groups(self, value: Dict[str, List[str]]) -> None:\n        """"""groups setter - groups should be dictionary""""""\n        if value is None:\n            self._groups = {}\n        self._groups = value\n\n    @property\n    def log_history(self) -> Dict[str, List[LogItem]]:\n        """"""logs getter""""""\n        return self._log_history\n\n    @log_history.setter\n    def log_history(self, value: Dict[str, List[LogItem]]) -> None:\n        """"""logs setter - logs can not be overwritten - you can only reset it to empty state""""""\n        if len(value) > 0:\n            raise RuntimeError(\'Cannot overwrite log history with non empty dictionary\')\n        self._log_history = value\n'"
livelossplot/plot_losses.py,0,"b'import warnings\nfrom typing import Type, Optional, TypeVar, List\n\nfrom livelossplot.main_logger import MainLogger\nfrom livelossplot.outputs import BaseOutput, MatplotlibPlot, ExtremaPrinter\n\nBO = TypeVar(\'BO\', bound=BaseOutput)\n\n\nclass PlotLosses:\n    """"""\n    Class collect metrics from the training engine and send it to plugins, when send is called\n    """"""\n    def __init__(self, outputs: Optional[List[Type[BO]]] = None, mode: str = \'notebook\', **kwargs):\n        """"""\n        :param outputs: list of callbacks (outputs) which are called with send method\n        :param mode: Options: \'notebook\' or \'script\' - some of outputs need to change some behaviors,\n         depending on the working environment\n        :param kwargs: key-arguments which are passed to MainLogger\n        """"""\n        self.logger = MainLogger(**kwargs)\n        self.outputs = outputs if outputs is not None else [MatplotlibPlot(), ExtremaPrinter()]\n        for out in self.outputs:\n            out.set_output_mode(mode)\n\n    def update(self, *args, **kwargs):\n        """"""update logs with arguments that will be passed to main logger""""""\n        self.logger.update(*args, **kwargs)\n\n    def send(self):\n        """"""Method will send logs to every output class""""""\n        for output in self.outputs:\n            output.send(self.logger)\n\n    def draw(self):\n        """"""Send method substitute from old livelossplot api""""""\n        warnings.warn(\'draw will be deprecated, please use send method\', PendingDeprecationWarning)\n        self.send()\n'"
livelossplot/version.py,0,"b'""""""Livelossplot version number""""""\n\n__version__ = \'0.5.1\'\n'"
tests/external_api_test_neptune.py,0,"b'import numpy as np\n\nfrom livelossplot import PlotLosses\nfrom livelossplot.outputs import NeptuneLogger\n\nimport neptune\n\n\ndef test_neptune():\n    neptune_logger = NeptuneLogger(\n        api_token=""ANONYMOUS"", project_qualified_name=""shared/colab-test-run"", tags=[\'livelossplot\', \'github-actions\']\n    )\n\n    plotlosses = PlotLosses(outputs=[neptune_logger])\n\n    assert neptune_logger.experiment.state == \'running\'\n\n    for i in range(3):\n        plotlosses.update(\n            {\n                \'acc\': 1 - np.random.rand() / (i + 2.),\n                \'val_acc\': 1 - np.random.rand() / (i + 0.5),\n                \'loss\': 1. / (i + 2.),\n                \'val_loss\': 1. / (i + 0.5)\n            }\n        )\n        plotlosses.send()\n\n    assert neptune_logger.experiment.state == \'running\'\n\n    neptune_logger.close()\n\n    assert neptune_logger.experiment.state == \'succeeded\'\n\n    url = neptune.project._get_experiment_link(neptune_logger.experiment)\n\n    assert len(url) > 0\n'"
tests/external_test_examples.py,0,"b""import os\n\nimport nbformat\nfrom nbconvert.preprocessors import ExecutePreprocessor\n\nNOTEBOOKS_TO_TEST = [\n    'minimal.ipynb',\n]\n\n\ndef run_notebook(notebook_path):\n    nb_name, _ = os.path.splitext(os.path.basename(notebook_path))\n    dirname = os.path.dirname(notebook_path)\n\n    with open(notebook_path) as f:\n        nb = nbformat.read(f, as_version=4)\n\n    nb['cells'] = [cell for cell in nb['cells'] if not cell['source'].startswith('!')]\n\n    proc = ExecutePreprocessor(timeout=600, kernel_name='python3')\n    proc.allow_errors = True\n\n    proc.preprocess(nb, {'metadata': {'path': 'examples/'}})\n    output_path = os.path.join(dirname, '_test_{}.ipynb'.format(nb_name))\n\n    with open(output_path, mode='wt') as f:\n        nbformat.write(nb, f)\n    errors = []\n    for cell in nb.cells:\n        if 'outputs' in cell:\n            for output in cell['outputs']:\n                if output.output_type == 'error':\n                    errors.append(output)\n    return nb, errors\n\n\ndef test_examples():\n    localdir = './examples'\n    for file in NOTEBOOKS_TO_TEST:\n        nb, errors = run_notebook(os.path.join(localdir, file))\n        assert len(errors) == 0\n"""
tests/external_test_keras.py,0,"b""from random import randint\n\nfrom keras import Sequential\nfrom keras.layers import LSTM, Dense\nfrom numpy import argmax\nfrom numpy import array\n\nfrom livelossplot import MainLogger, PlotLossesKeras\nfrom livelossplot.outputs import BaseOutput\n\nNUM_OF_GENERATED = 5\n\n\nclass CheckOutput(BaseOutput):\n    def send(self, logger: MainLogger):\n        assert isinstance(logger, MainLogger)\n        grouped_log_history = logger.grouped_log_history(raw_names=True, raw_group_names=True)\n        print(grouped_log_history['accuracy'].keys())\n        assert len(grouped_log_history['accuracy']) == 2\n        assert grouped_log_history['accuracy'].get('val_accuracy') is not None\n\n\ndef generate_sequence(length=5):\n    return [randint(0, NUM_OF_GENERATED - 1) for _ in range(length)]\n\n\ndef one_hot_encode(sequence, n_unique=NUM_OF_GENERATED):\n    encoding = list()\n    for value in sequence:\n        vector = [0 for _ in range(n_unique)]\n        vector[value] = 1\n        encoding.append(vector)\n    return array(encoding)\n\n\ndef one_hot_decode(encoded_seq):\n    return [argmax(vector) for vector in encoded_seq]\n\n\ndef generate_data():\n    sequence = generate_sequence()\n    encoded = one_hot_encode(sequence)\n    X = encoded.reshape(encoded.shape[0], 1, encoded.shape[1])\n    return X, encoded\n\n\ndef test_keras():\n    callback = PlotLossesKeras(outputs=(CheckOutput(), ))\n    model = Sequential()\n    model.add(LSTM(5, input_shape=(1, NUM_OF_GENERATED)))\n    model.add(Dense(NUM_OF_GENERATED, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    X_train, Y_train = generate_data()\n    X_test, Y_test = generate_data()\n    model.fit(X_train, Y_train, epochs=2, validation_data=(X_test, Y_test), callbacks=[callback], verbose=False)\n"""
tests/external_test_pytorch_ignite.py,3,"b""import torch\nfrom ignite import engine\nfrom torch import nn, optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom livelossplot import MainLogger, PlotLossesIgnite\nfrom livelossplot.outputs import BaseOutput\n\n\nclass CheckOutput(BaseOutput):\n    def send(self, logger: MainLogger):\n        assert isinstance(logger, MainLogger)\n        log_history = logger.log_history\n        assert log_history.get('loss') is not None\n\n\nclass Model(nn.Sequential):\n    def __init__(self):\n        super().__init__(nn.Linear(12, 5))\n\n\ndef get_random_data():\n    dataset_size, num_inputs, num_outputs = 100, 12, 5\n    inputs = torch.rand(dataset_size, num_inputs)\n    labels = torch.randint(num_outputs, (dataset_size, ))\n    dataset = TensorDataset(inputs, labels)\n    dataloader = DataLoader(dataset, batch_size=10)\n    return dataloader\n\n\ndef test_ignite():\n    callback = PlotLossesIgnite(outputs=(CheckOutput(), ))\n    model = Model()\n    optimizer = optim.Adam(params=model.parameters(), lr=0.001)\n    loss_fn = nn.CrossEntropyLoss()\n    trainer = engine.create_supervised_trainer(\n        model, optimizer, loss_fn, output_transform=lambda x, y, y_pred, loss: {'loss': loss.item()}\n    )\n\n    losses = []\n\n    @trainer.on(engine.Events.ITERATION_COMPLETED)\n    def _save_losses(engine):\n        losses.append(engine.state.output['loss'])\n\n    @trainer.on(engine.Events.EPOCH_COMPLETED)\n    def _compute_epoch_loss(engine):\n        engine.state.metrics = {'loss': sum(losses) / len(losses)}\n\n    callback.attach(trainer)\n\n    train_dataloader = get_random_data()\n    trainer.run(train_dataloader, max_epochs=2)\n"""
tests/external_test_tensorboard.py,0,"b""import os\n\nimport numpy as np\n\nfrom livelossplot import PlotLosses\nfrom livelossplot.outputs import TensorboardTFLogger\n\n\ndef test_tensorboard():\n    groups = {'acccuracy': ['acc', 'val_acc'], 'log-loss': ['loss', 'val_loss']}\n    logger = TensorboardTFLogger()\n\n    liveplot = PlotLosses(groups=groups, outputs=(logger, ))\n\n    for i in range(3):\n        liveplot.update(\n            {\n                'acc': 1 - np.random.rand() / (i + 2.),\n                'val_acc': 1 - np.random.rand() / (i + 0.5),\n                'loss': 1. / (i + 2.),\n                'val_loss': 1. / (i + 0.5)\n            }\n        )\n        liveplot.send()\n\n    assert all([f.startswith('events.out.tfevents.') for f in os.listdir(logger._path)])\n"""
tests/test_bokeh_output.py,0,"b""import os\n\nimport numpy as np\n\nfrom livelossplot import PlotLosses\nfrom livelossplot.outputs import BokehPlot\n\n\ndef test_bokeh_plot():\n    logger = BokehPlot()\n\n    liveplot = PlotLosses(outputs=[logger], mode='script')\n\n    for i in range(3):\n        liveplot.update(\n            {\n                'acc': 1 - np.random.rand() / (i + 2.),\n                'val_acc': 1 - np.random.rand() / (i + 0.5),\n                'loss': 1. / (i + 2.),\n                'val_loss': 1. / (i + 0.5)\n            }\n        )\n        liveplot.send()\n\n    assert os.path.isfile(logger.output_file)\n"""
tests/test_extrema_printer.py,0,"b'from livelossplot import PlotLosses\nfrom livelossplot.outputs import ExtremaPrinter\n\n\ndef test_extrema_print():\n    """"""Test if plugin object cache contains valid values""""""\n    groups = {\'accuracy\': [\'acc\', \'val_acc\'], \'log-loss\': [\'loss\', \'val_loss\']}\n    plugin = ExtremaPrinter()\n    outputs = (plugin, )\n    liveplot = PlotLosses(outputs=outputs, groups=groups)\n    liveplot.update({\'acc\': 0.5, \'val_acc\': 0.4, \'loss\': 1.2, \'val_loss\': 1.1})\n    liveplot.update({\'acc\': 0.55, \'val_acc\': 0.45, \'loss\': 1.1, \'val_loss\': 1.0})\n    liveplot.update({\'acc\': 0.65, \'val_acc\': 0.35, \'loss\': 0.5, \'val_loss\': 0.9})\n    liveplot.update({\'acc\': 0.65, \'val_acc\': 0.55, \'loss\': 1.0, \'val_loss\': 0.9})\n    liveplot.send()\n    assert len(plugin.extrema_cache[\'log-loss\']) == 2\n    assert len(plugin.extrema_cache[\'log-loss\'][\'training \']) == 3\n    assert plugin.extrema_cache[\'accuracy\'][\'validation \'][\'min\'] == 0.35\n    assert plugin.extrema_cache[\'accuracy\'][\'validation \'][\'max\'] == 0.55\n    assert plugin.extrema_cache[\'accuracy\'][\'validation \'][\'current\'] == 0.55\n'"
tests/test_main_logger.py,0,"b'import re\n\nfrom livelossplot import MainLogger\n\n\ndef test_main_logger():\n    """"""Test basic usage""""""\n    logger = MainLogger()\n    logs = {\'loss\': 0.6}\n    logger.update(logs)\n    assert \'loss\' in logger.log_history.keys()\n    assert len(logger.log_history[\'loss\']) == 1\n\n\ndef test_main_logger_with_groups():\n    """"""Test groups""""""\n    groups = {\'acccuracy\': [\'acc\', \'val_acc\'], \'log-loss\': [\'loss\', \'val_loss\']}\n    logger = MainLogger(groups=groups)\n    logger.update({\'acc\': 0.5, \'val_acc\': 0.4, \'loss\': 1.2, \'val_loss\': 1.1})\n    logger.update({\'acc\': 0.55, \'val_acc\': 0.45, \'loss\': 1.1, \'val_loss\': 1.0})\n    logger.update({\'acc\': 0.65, \'val_acc\': 0.55, \'loss\': 1.0, \'val_loss\': 0.9})\n    grouped_log_history = logger.grouped_log_history(raw_names=True, raw_group_names=True)\n    assert len(grouped_log_history) == 2\n    assert len(grouped_log_history[\'acccuracy\']) == 2\n    assert len(grouped_log_history[\'acccuracy\'][\'val_acc\']) == 3\n    assert len(grouped_log_history[\'log-loss\'][\'loss\']) == 3\n    grouped_log_history = logger.grouped_log_history()\n    assert len(grouped_log_history) == 2\n    assert len(grouped_log_history[\'acccuracy\']) == 2\n    assert len(grouped_log_history[\'acccuracy\'][\'validation \']) == 3\n    assert len(grouped_log_history[\'log-loss\'][\'training \']) == 3\n\n\ndef test_main_logger_with_default_groups():\n    """"""Test group patterns""""""\n    logger = MainLogger()\n    logger.update({\'acc\': 0.5, \'val_acc\': 0.4, \'loss\': 1.2, \'val_loss\': 1.1})\n    logger.update({\'acc\': 0.55, \'val_acc\': 0.45, \'loss\': 1.1, \'val_loss\': 1.0})\n    logger.update({\'acc\': 0.65, \'val_acc\': 0.55, \'loss\': 1.0, \'val_loss\': 0.9})\n    grouped_log_history = logger.grouped_log_history(raw_names=True, raw_group_names=True)\n    assert len(grouped_log_history) == 2\n    assert len(grouped_log_history[\'acc\']) == 2\n    assert len(grouped_log_history[\'acc\'][\'val_acc\']) == 3\n    grouped_log_history = logger.grouped_log_history()\n    assert len(grouped_log_history) == 2\n    assert len(grouped_log_history[\'Accuracy\']) == 2\n    assert len(grouped_log_history[\'Accuracy\'][\'validation \']) == 3\n\n\ndef test_main_logger_metrc_to_name():\n    """"""Test group patterns""""""\n    logger = MainLogger()\n    logger.update({\'acc\': 0.5, \'val_acc\': 0.4, \'loss\': 1.2, \'val_loss\': 1.1, \'lr\': 0.01})\n    logger.update({\'acc\': 0.55, \'val_acc\': 0.45, \'loss\': 1.1, \'val_loss\': 1.0, \'lr\': 0.001})\n    logger.update({\'acc\': 0.65, \'val_acc\': 0.55, \'loss\': 1.0, \'val_loss\': 0.9, \'lr\': 0.0001})\n    metric_to_name = logger.metric_to_name\n    assert \'lr\' not in metric_to_name\n    target_metric_to_name = {\n        \'acc\': \'training \',\n        \'val_acc\': \'validation \',\n        \'loss\': \'training \',\n        \'val_loss\': \'validation \',\n    }\n    for metric, metric_name in metric_to_name.items():\n        assert metric_name == target_metric_to_name.get(metric)\n\n\ndef test_main_logger_autogroups():\n    """"""Test group patterns""""""\n    logger = MainLogger()\n    logger.update({\'acc\': 0.5, \'val_acc\': 0.4, \'loss\': 1.2, \'val_loss\': 1.1, \'lr\': 0.01})\n    logger.update({\'acc\': 0.55, \'val_acc\': 0.45, \'loss\': 1.1, \'val_loss\': 1.0, \'lr\': 0.001})\n    logger.update({\'acc\': 0.65, \'val_acc\': 0.55, \'loss\': 1.0, \'val_loss\': 0.9, \'lr\': 0.0001})\n    grouped_log_history = logger.grouped_log_history()\n    target_groups = {\'Accuracy\': (\'validation \', \'training \'), \'Loss\': (\'validation \', \'training \'), \'lr\': (\'lr\', )}\n    for target_group, target_metrics in target_groups.items():\n        for m1, m2 in zip(sorted(grouped_log_history[target_group].keys()), sorted(target_metrics)):\n            assert m1 == m2\n'"
tests/test_plot_losses.py,0,"b'from livelossplot import MainLogger, PlotLosses\nfrom livelossplot.outputs import BaseOutput\n\n\nclass CheckOutput(BaseOutput):\n    def send(self, logger: MainLogger):\n        assert isinstance(logger, MainLogger)\n        grouped_log_history = logger.grouped_log_history(raw_names=True, raw_group_names=True)\n        assert len(grouped_log_history) == 2\n        assert len(grouped_log_history[\'acc\']) == 2\n        assert len(grouped_log_history[\'acc\'][\'val_acc\']) == 2\n        grouped_log_history = logger.grouped_log_history()\n        assert len(grouped_log_history) == 2\n        assert len(grouped_log_history[\'Accuracy\']) == 2\n        print(grouped_log_history)\n        assert len(grouped_log_history[\'Accuracy\'][\'validation \']) == 2\n\n\ndef test_plot_losses():\n    """"""Test basic usage""""""\n    loss_plotter = PlotLosses(outputs=(CheckOutput(), ))\n    loss_plotter.update({\'acc\': 0.5, \'val_acc\': 0.4, \'loss\': 1.2, \'val_loss\': 1.1})\n    loss_plotter.update({\n        \'acc\': 0.55,\n        \'loss\': 1.1,\n    })\n    loss_plotter.update({\'acc\': 0.65, \'val_acc\': 0.55, \'loss\': 1.0, \'val_loss\': 0.9})\n    loss_plotter.update({\n        \'acc\': 0.55,\n        \'loss\': 1.1,\n    })\n    loss_plotter.send()\n'"
livelossplot/inputs/__init__.py,0,"b'def PlotLossesKeras(**kwargs):\r\n    """"""PlotLosses callback for Keras (as a standalone library).\r\n    Requires keras to be installed.\r\n    :param kwargs: key-arguments which are passed to PlotLosses\r\n    """"""\r\n    from .keras import PlotLossesCallback\r\n    return PlotLossesCallback(**kwargs)\r\n\r\n\r\ndef PlotLossesKerasTF(**kwargs):\r\n    """"""PlotLosses callback for Keras (as a module of TensorFlow).\r\n    Requires tensorflow to be installed.\r\n    :param kwargs: key-arguments which are passed to PlotLosses\r\n    """"""\r\n    from .tf_keras import PlotLossesCallback\r\n    return PlotLossesCallback(**kwargs)\r\n\r\n\r\ndef PlotLossesPoutyne(**kwargs):\r\n    """"""PlotLosses callback for Poutyne, a library for PyTorch.\r\n    Requires poutyne to be installed, https://poutyne.org/.\r\n    :param kwargs: key-arguments which are passed to PlotLosses\r\n    """"""\r\n    from .poutyne import PlotLossesCallback\r\n    return PlotLossesCallback(**kwargs)\r\n\r\n\r\ndef PlotLossesIgnite(**kwargs):\r\n    """"""PlotLosses callback for Poutyne, a library for PyTorch.\r\n    Requires pytorch-ignite to be installed, https://github.com/pytorch/ignite.\r\n    :param kwargs: key-arguments which are passed to PlotLosses\r\n    """"""\r\n    from .pytorch_ignite import PlotLossesCallback\r\n    return PlotLossesCallback(**kwargs)\r\n'"
livelossplot/inputs/generic_keras.py,0,"b'from livelossplot.plot_losses import PlotLosses\n\n\nclass _PlotLossesCallback:\n    """"""Base keras callback class for keras and tensorflow.keras""""""\n    def __init__(self, **kwargs):\n        self.liveplot = PlotLosses(**kwargs)\n\n    def on_epoch_end(self, epoch, logs):\n        """"""Send metrics to livelossplot""""""\n        self.liveplot.update(logs.copy(), epoch)\n        self.liveplot.send()\n'"
livelossplot/inputs/keras.py,0,"b'import keras\n\nfrom .generic_keras import _PlotLossesCallback\n\n\nclass PlotLossesCallback(_PlotLossesCallback, keras.callbacks.Callback):\n    """"""Pure keras callback""""""\n    def __init__(self, **kwargs):\n        keras.callbacks.Callback.__init__(self)\n        _PlotLossesCallback.__init__(self, **kwargs)\n'"
livelossplot/inputs/poutyne.py,0,"b'from poutyne.framework import Callback\nfrom ..plot_losses import PlotLosses\n\n\nclass PlotLossesCallback(Callback):\n    """"""Poutyne is a keras-like api framework for pytorch""""""\n    def __init__(self, **kwargs):\n        """"""\n        :param kwargs: key-word arguments of PlotLosses\n        """"""\n        super(PlotLossesCallback, self).__init__()\n        self.liveplot = PlotLosses(**kwargs)\n        self.metrics = None\n\n    def on_train_begin(self, logs):\n        """"""Create metric names""""""\n        metrics = [\'loss\'] + self.model.metrics_names\n        self.metrics = list(metrics)\n        self.metrics += [\'val_\' + metric for metric in metrics]\n\n    def on_epoch_end(self, epoch, logs):\n        """"""Send metrics to livelossplot""""""\n        metric_logs = {metric: logs[metric] for metric in self.metrics if metric in logs}\n        self.liveplot.update(metric_logs, epoch)\n        self.liveplot.send()\n'"
livelossplot/inputs/pytorch_ignite.py,0,"b'from typing import Optional\n\nimport ignite.engine\nfrom ignite.handlers import global_step_from_engine\nfrom livelossplot.plot_losses import PlotLosses\n\n\nclass PlotLossesCallback:\n    def __init__(self, train_engine: Optional[ignite.engine.Engine] = None, **kwargs):\n        """"""\n        Args:\n            train_engine: engine with global step information, send metohod callback will be attached to it\n                if None send method will be called on the end of each store call it may cause warnings and errors in\n                the case of multiple engines attached\n\n        Keyword Args:\n            **kwargs: keyword args that will be passed to livelossplot PlotLosses class\n        """"""\n        self.liveplot = PlotLosses(**kwargs)\n        self.train_engine = train_engine\n        if self.train_engine:\n            self.train_engine.add_event_handler(ignite.engine.Events.EPOCH_STARTED, self.send)\n            self.train_engine.add_event_handler(ignite.engine.Events.COMPLETED, self.send)\n\n    def attach(self, engine: ignite.engine.Engine):\n        """"""Attach callback to ignite engine, attached method will be called on the end of each epoch\n        Args:\n            engine: engine that computes metrics on the end of each epoch and / or on the end of each iteration\n\n        Notes:\n            metrics computation plugins have to be attached before this one\n        """"""\n        engine.add_event_handler(ignite.engine.Events.EPOCH_COMPLETED, self.store)\n        engine.add_event_handler(ignite.engine.Events.ITERATION_COMPLETED, self.store)\n\n    def store(self, engine: ignite.engine.Engine):\n        """"""Store computed metrics, that will be send to main logger\n        Args:\n            engine: engine with state.metrics\n        """"""\n        metrics = {}\n        if not hasattr(engine.state, \'metrics\') or len(engine.state.metrics) == 0:\n            return\n        kwargs = dict(\n            current_step=global_step_from_engine(self.train_engine)\n            (self.train_engine, self.train_engine.last_event_name)\n        ) if self.train_engine else {}\n        for key, val in engine.state.metrics.items():\n            metrics[key] = val\n        self.liveplot.update(metrics, **kwargs)\n        if not self.train_engine:\n            self.send()\n\n    def send(self, _: Optional[ignite.engine.Engine] = None):\n        self.liveplot.send()\n'"
livelossplot/inputs/tf_keras.py,0,"b'from tensorflow import keras\nfrom .generic_keras import _PlotLossesCallback\n\n\nclass PlotLossesCallback(_PlotLossesCallback, keras.callbacks.Callback):\n    """"""Callback for tensorflow keras""""""\n    def __init__(self, **kwargs):\n        keras.callbacks.Callback.__init__(self)\n        _PlotLossesCallback.__init__(self, **kwargs)\n'"
livelossplot/outputs/__init__.py,0,"b'# technical\r\n\r\nfrom .base_output import BaseOutput\r\n\r\n# default\r\n\r\nfrom .matplotlib_plot import MatplotlibPlot\r\nfrom .extrema_printer import ExtremaPrinter\r\n\r\n# with external dependencies\r\n# import are respective __init__ methods\r\n# hack-ish, but works (and I am not aware of a more proper way to do so)\r\n\r\nfrom .bokeh_plot import BokehPlot\r\nfrom .neptune_logger import NeptuneLogger\r\nfrom .tensorboard_logger import TensorboardLogger\r\nfrom .tensorboard_tf_logger import TensorboardTFLogger\r\n\r\n# with external dependencies\r\n\r\nfrom . import matplotlib_subplots\r\n'"
livelossplot/outputs/base_output.py,0,"b'from abc import ABC, abstractmethod\n\nfrom livelossplot.main_logger import MainLogger\n\n\nclass BaseOutput(ABC):\n    @abstractmethod\n    def send(self, logger: MainLogger):\n        """"""Abstract method - handle logs for a plugin""""""\n        ...\n\n    def close(self):\n        """"""Overwrite it with last steps""""""\n        ...\n\n    def set_output_mode(self, mode: str):\n        """"""Some of output plugins needs to know target format""""""\n        assert mode in (\'notebook\', \'script\')\n        self._set_output_mode(mode)\n\n    def _set_output_mode(self, mode: str):\n        """"""\n        :param mode: mode for callbacks - some of outputs need to change some behaviors,\n         depending on the working environment (scripts and jupyter notebooks)\n        """"""\n        ...\n'"
livelossplot/outputs/bokeh_plot.py,0,"b'from typing import List, Dict, Tuple\n\nfrom livelossplot.main_logger import MainLogger, LogItem\nfrom livelossplot.outputs.base_output import BaseOutput\n\n\nclass BokehPlot(BaseOutput):\n    """"""Simple plugin for a bokeh framework""""""\n    def __init__(\n        self,\n        max_cols: int = 2,\n        skip_first: int = 2,\n        cell_size: Tuple[int, int] = (400, 300),\n        output_file: str = \'./bokeh_output.html\'\n    ):\n        from bokeh import plotting, io, palettes\n        self.plotting = plotting\n        self.io = io\n        self.plot_width, self.plot_height = cell_size\n        self.max_cols = max_cols\n        self.skip_first = skip_first  # think about it\n        self.figures = {}\n        self.is_notebook = False\n        self.output_file = output_file\n        self.colors = palettes.Category10[10]\n\n    def send(self, logger: MainLogger) -> None:\n        """"""Draw figures with metrics and show""""""\n        log_groups = logger.grouped_log_history()\n        new_grid_plot = False\n        for idx, (group_name, group_logs) in enumerate(log_groups.items(), start=1):\n            fig = self.figures.get(group_name)\n            if not fig:\n                fig = self.plotting.figure(title=group_name)\n                new_grid_plot = True\n            self.figures[group_name] = self._draw_metric_subplot(fig, group_logs)\n        if new_grid_plot:\n            self._create_grid_plot()\n        if self.is_notebook:\n            self.io.push_notebook(handle=self.target)\n        else:\n            self.plotting.save(self.grid)\n\n    def _draw_metric_subplot(self, fig, group_logs: Dict[str, List[LogItem]]):\n        # for now, with local imports, no output annotation  -> self.plotting.Figure\n        # there used to be skip first part, but I skip it first\n        from bokeh.models import ColumnDataSource, HoverTool\n        for i, (name, logs) in enumerate(group_logs.items()):\n            if len(logs) > 0:\n                source = ColumnDataSource(\n                    data={\n                        \'step\': [log.step for log in logs],\n                        \'value\': [log.value for log in logs],\n                    }\n                )\n                fig.line(x=\'step\', y=\'value\', color=self.colors[i], legend_label=name, source=source)\n\n        fig.add_tools(\n            HoverTool(\n                tooltips=[\n                    (\'step\', \'@step\'),\n                    (\'value\', \'@value{0.3f}\'),\n                ],\n                formatters={\n                    \'step\': \'printf\',\n                    \'value\': \'printf\',\n                },\n                mode=\'vline\'\n            )\n        )\n        return fig\n\n    def _create_grid_plot(self):\n        rows = []\n        row = []\n        for idx, fig in enumerate(self.figures.values(), start=1):\n            row.append(fig)\n            if idx % self.max_cols == 0:\n                rows.append(row)\n                row = []\n        self.grid = self.plotting.gridplot(\n            rows, sizing_mode=\'scale_width\', plot_width=self.plot_width, plot_height=self.plot_height\n        )\n        self.target = self.plotting.show(self.grid, notebook_handle=self.is_notebook)\n\n    def _set_output_mode(self, mode: str):\n        """"""Set notebook or script mode""""""\n        self.is_notebook = mode == \'notebook\'\n        if self.is_notebook:\n            self.io.output_notebook()\n        else:\n            self.io.output_file(self.output_file)\n'"
livelossplot/outputs/extrema_printer.py,0,"b'from typing import Dict, List\n\nfrom livelossplot.main_logger import LogItem\nfrom livelossplot.main_logger import MainLogger\nfrom .base_output import BaseOutput\n\n\nclass ExtremaPrinter(BaseOutput):\n    def __init__(\n        self,\n        massage_template: str = \'\\t{metric_name:16} \\t (min: {min:8.3f},\'\n        \' max: {max:8.3f}, cur: {current:8.3f})\'\n    ):\n        """"""\n        :param massage_template: you can specify massage which use all or a few values (min, max, current)\n        """"""\n        self.massage_template = massage_template\n        self.extrema_cache = {}\n\n    def send(self, logger: MainLogger):\n        """"""Create massages with log_history and massage template""""""\n        log_groups = logger.grouped_log_history()\n        massages = self._create_massages(log_groups)\n        print(\'\\n\'.join(massages))\n\n    def _create_massages(self, log_groups: Dict[str, Dict[str, List[LogItem]]]) -> List[str]:\n        """"""Update cache and create massages""""""\n        massages = []\n        for group_name, group_logs in log_groups.items():\n            massages.append(group_name)\n            for metric_name, metric_values in group_logs.items():\n                self.update_extrema(metric_name, group_name, metric_values)\n                msg = self.massage_template.format(\n                    metric_name=metric_name, **self.extrema_cache[group_name][metric_name]\n                )\n                massages.append(msg)\n        return massages\n\n    def update_extrema(self, metric_name: str, group_name: str, metric_values: List[LogItem]) -> None:\n        """"""Write highest, lower and current value to cache (or initialize if no exist)""""""\n        current_val = metric_values[-1].value\n        if not self.extrema_cache.get(group_name):\n            self.extrema_cache[group_name] = {}\n        cache = self.extrema_cache[group_name].get(metric_name)\n        if not cache:\n            min_val = min(metric_values, key=lambda i: i.value).value\n            max_val = max(metric_values, key=lambda i: i.value).value\n            current_val = metric_values[-1].value\n            self.extrema_cache[group_name][metric_name] = {\n                \'min\': min_val,\n                \'max\': max_val,\n                \'current\': current_val,\n            }\n        else:\n            min_val = min(cache[\'min\'], current_val)\n            max_val = max(cache[\'max\'], current_val)\n            cache[\'min\'] = min_val\n            cache[\'max\'] = max_val\n            cache[\'current\'] = current_val\n\n    @property\n    def extrema_cache(self) -> Dict[str, Dict[str, Dict[str, float]]]:\n        """"""Cache getter""""""\n        return self._extrema_cache\n\n    @extrema_cache.setter\n    def extrema_cache(self, value: Dict[str, Dict[str, Dict[str, float]]]) -> None:\n        """"""Cache setter - can initialize cache only with empty dictionary""""""\n        if len(value) > 0:\n            raise RuntimeError(\'Cannot overwrite cache with non empty dictionary\')\n        self._extrema_cache = value\n\n    def close(self):\n        """"""Clear cache""""""\n        self.extrema_cache = {}\n'"
livelossplot/outputs/matplotlib_plot.py,0,"b'from typing import Tuple, List, Dict, Optional\n\nimport warnings\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom livelossplot.main_logger import MainLogger, LogItem\nfrom livelossplot.outputs.base_output import BaseOutput\n\n\nclass MatplotlibPlot(BaseOutput):\n    """"""NOTE: Removed figsize and dynamix_x_axis.""""""\n    def __init__(\n        self,\n        cell_size: Tuple[int, int] = (6, 4),\n        max_cols: int = 2,\n        max_epoch: int = None,\n        skip_first: int = 2,\n        extra_plots=[],\n        figpath: Optional[str] = None\n    ):\n        self.cell_size = cell_size\n        self.max_cols = max_cols\n        self.max_epoch = max_epoch\n        self.skip_first = skip_first  # think about it\n        self.extra_plots = extra_plots\n        self.max_epoch = max_epoch\n        self.figpath = figpath\n        self.file_idx = 0  # now only for saving files\n\n    def send(self, logger: MainLogger):\n        """"""Draw figures with metrics and show""""""\n        log_groups = logger.grouped_log_history()\n        figsize_x = self.max_cols * self.cell_size[0]\n        figsize_y = ((len(log_groups) + 1) // self.max_cols + 1) * self.cell_size[1]\n\n        max_rows = (len(log_groups) + len(self.extra_plots) + 1) // self.max_cols + 1\n\n        clear_output(wait=True)\n        plt.figure(figsize=(figsize_x, figsize_y))\n\n        for group_id, (group_name, group_logs) in enumerate(log_groups.items()):\n            plt.subplot(max_rows, self.max_cols, group_id + 1)\n            self._draw_metric_subplot(group_logs, group_name=group_name)\n\n        for i, extra_plot in enumerate(self.extra_plots):\n            plt.subplot(max_rows, self.max_cols, i + len(log_groups) + 1)\n            extra_plot(logger)\n\n        plt.tight_layout()\n        if self.figpath is not None:\n            plt.savefig(self.figpath.format(i=self.file_idx))\n            self.file_idx += 1\n        plt.show()\n\n    def _draw_metric_subplot(self, group_logs: Dict[str, List[LogItem]], group_name: str = \'\'):\n        # there used to be skip first part, but I skip it first\n        if self.max_epoch is not None:\n            plt.xlim(0, self.max_epoch)\n\n        for name, logs in group_logs.items():\n            if len(logs) > 0:\n                xs = [log.step for log in logs]\n                ys = [log.value for log in logs]\n                plt.plot(xs, ys, label=name)\n\n        plt.title(group_name)\n        plt.xlabel(\'epoch\')\n        plt.legend(loc=\'center right\')\n\n    def _not_inline_warning(self):\n        backend = matplotlib.get_backend()\n        if ""backend_inline"" not in backend:\n            warnings.warn(\n                ""livelossplot requires inline plots.\\nYour current backend is: {}""\n                ""\\nRun in a Jupyter environment and execute \'%matplotlib inline\'."".format(backend)\n            )\n'"
livelossplot/outputs/matplotlib_subplots.py,3,"b'import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\n\n\nclass BaseSubplot:\n    def __init__(self):\n        pass\n\n    def draw(self, *args, **kwargs):\n        raise Exception(""Not implemented"")\n\n    def __call__(self, *args, **kwargs):\n        self.draw(*args, **kwargs)\n\n\nclass LossSubplot(BaseSubplot):\n    """"""To rewrire, this one now won\'t work""""""\n    def __init__(\n        self, metric, title="""", series_fmt={\n            \'training\': \'{}\',\n            \'validation\': \'val_{}\'\n        }, skip_first=2, max_epoch=None\n    ):\n        super().__init__(self)\n        self.metric = metric\n        self.title = title\n        self.series_fmt = series_fmt\n        self.skip_first = skip_first\n        self.max_epoch = max_epoch\n        raise NotImplementedError()\n\n    def _how_many_to_skip(self, log_length, skip_first):\n        if log_length < skip_first:\n            return 0\n        elif log_length < 2 * skip_first:\n            return log_length - skip_first\n        else:\n            return skip_first\n\n    def draw(self, logs):\n        skip = self._how_many_to_skip(len(logs), self.skip_first)\n\n        if self.max_epoch is not None:\n            plt.xlim(1 + skip, self.max_epoch)\n\n        for serie_label, serie_fmt in self.series_fmt.items():\n\n            serie_metric_name = serie_fmt.format(self.metric)\n            serie_metric_logs = [\n                (log.get(\'_i\', i + 1), log[serie_metric_name])\n                for i, log in enumerate(logs[skip:]) if serie_metric_name in log\n            ]\n\n            if len(serie_metric_logs) > 0:\n                xs, ys = zip(*serie_metric_logs)\n                plt.plot(xs, ys, label=serie_label)\n\n        plt.title(self.title)\n        plt.xlabel(\'epoch\')\n        plt.legend(loc=\'center right\')\n\n\nclass Plot1D(BaseSubplot):\n    def __init__(self, model, X, Y):\n        super().__init__(self)\n        self.model = model\n        self.X = X\n        self.Y = Y\n\n    def predict(self, model, X):\n        # e.g. model(torch.fromnumpy(X)).detach().numpy()\n        return model.predict(X)\n\n    def draw(self, *args, **kwargs):\n        plt.plot(self.X, self.Y, \'r.\', label=""Ground truth"")\n        plt.plot(self.X, self.predict(self.model, self.X), \'-\', label=""Model"")\n        plt.title(""Prediction"")\n        plt.legend(loc=\'lower right\')\n\n\nclass Plot2d(BaseSubplot):\n    def __init__(self, model, X, Y, valiation_data=(None, None), h=0.02, margin=0.25, device=\'cpu\'):\n        super().__init__()\n\n        self.model = model\n        self.X = X\n        self.Y = Y\n        self.X_test, self.Y_test = valiation_data\n\n        # add size assertions\n\n        self.cm_bg = plt.cm.RdBu\n        self.cm_points = ListedColormap([\'#FF0000\', \'#0000FF\'])\n\n        x_min = X[:, 0].min() - margin\n        x_max = X[:, 0].max() + margin\n\n        y_min = X[:, 1].min() - margin\n        y_max = X[:, 1].max() + margin\n\n        self.xx, self.yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n        self.torch_device = device\n\n    def _predict_pytorch(self, model, x_numpy):\n        import torch\n        x = torch.from_numpy(x_numpy).to(self.torch_device).float()\n        return model(x).softmax(dim=1).detach().cpu().numpy()\n\n    def predict(self, model, X):\n        # e.g. model(torch.fromnumpy(X)).detach().numpy()\n        return model.predict(X)\n\n    def send(self, logger):\n        Z = self._predict_pytorch(self.model, np.c_[self.xx.ravel(), self.yy.ravel()])[:, 1]\n        Z = Z.reshape(self.xx.shape)\n        plt.contourf(self.xx, self.yy, Z, cmap=self.cm_bg, alpha=.8)\n        plt.scatter(self.X[:, 0], self.X[:, 1], c=self.Y, cmap=self.cm_points)\n        if self.X_test is not None:\n            plt.scatter(self.X_test[:, 0], self.X_test[:, 1], c=self.Y_test, cmap=self.cm_points, alpha=0.3)\n'"
livelossplot/outputs/neptune_logger.py,0,"b'from typing import Optional\nfrom livelossplot.main_logger import MainLogger\nfrom livelossplot.outputs.base_output import BaseOutput\n\n\nclass NeptuneLogger(BaseOutput):\n    """"""See: https://github.com/neptune-ai/neptune-client\n    YOUR_API_TOKEN and USERNAME/PROJECT_NAME\n    """"""\n    def __init__(self, api_token: Optional[str] = None, project_qualified_name: Optional[str] = None, **kwargs):\n        """"""\n        Set secrets and create experiment\n        :param api_token - your api token, you can create NEPTUNE_API_TOKEN environment variable instead:\n        :param project_qualified_name - <user>/<project>, you can create NEPTUNE_PROJECT environment variable instead:\n        :param kwargs - keyword args, that will be passed to create_experiment function:\n        """"""\n        import neptune\n        self.neptune = neptune\n        self.neptune.init(api_token=api_token, project_qualified_name=project_qualified_name)\n        self.experiment = self.neptune.create_experiment(**kwargs)\n\n    def close(self):\n        """"""Close connection""""""\n        self.neptune.stop()\n\n    def send(self, logger: MainLogger):\n        """"""Send metrics collected in last step to neptune server""""""\n        for name, log_items in logger.log_history.items():\n            last_log_item = log_items[-1]\n            self.neptune.send_metric(name, x=last_log_item.step, y=last_log_item.value)\n'"
livelossplot/outputs/tensorboard_logger.py,0,"b'from datetime import datetime\nfrom os import path\n\nfrom livelossplot.main_logger import MainLogger\nfrom livelossplot.outputs.base_output import BaseOutput\n\n\nclass TensorboardLogger(BaseOutput):\n    """"""\n    Class write logs to TensorBoard (using pure TensorBoard, not one from TensorFlow).\n    """"""\n    def __init__(self, logdir=""./tensorboard_logs/"", run_id=None):\n        """"""\n        :param logdir: dir where TensorBoard events will be written\n        :param run_id: name for log id, otherwise it usses datetime\n        """"""\n        from tensorboard import summary\n        self.summary = summary\n        time_str = datetime.now().isoformat()[:-7].replace(""T"", "" "").replace("":"", ""_"")\n        self._path = path.join(logdir, time_str)\n        self.writer = summary.create_file_writer(self._path)\n\n    def close(self):\n        """"""Close tensorboard writer""""""\n        self.writer.close()\n\n    def log_scalar(self, name: str, value: float, global_step: float):\n        """"""\n        :param name: name of metric\n        :param value: float value of metric\n        :param global_step: current step of the training loop\n        :return:\n        """"""\n        with self.writer.as_default():\n            self.summary.scalar(name, value, step=global_step)\n        self.writer.flush()\n\n    def send(self, logger: MainLogger):\n        """"""Take log history from logger and store it in tensorboard event""""""\n        for name, log_items in logger.log_history.items():\n            last_log_item = log_items[-1]\n            self.log_scalar(name, last_log_item.value, last_log_item.step)\n'"
livelossplot/outputs/tensorboard_tf_logger.py,0,"b'from datetime import datetime\nfrom os import path\n\nfrom livelossplot.main_logger import MainLogger\nfrom livelossplot.outputs.base_output import BaseOutput\n\n\nclass TensorboardTFLogger(BaseOutput):\n    """"""\n    Class write logs to TensorBoard (from TensorFlow).\n    """"""\n    def __init__(self, logdir=""./tensorboard_logs/"", run_id=None):\n        """"""\n        :param logdir: dir where TensorBoard events will be written\n        :param run_id: name for log id, otherwise it usses datetime\n        """"""\n        from tensorflow import summary\n        self.summary = summary\n        run_id = datetime.now().isoformat()[:-7].replace(""T"", "" "").replace("":"", ""_"") if run_id is None else run_id\n        self._path = path.join(logdir, run_id)\n        self.writer = summary.create_file_writer(self._path)\n\n    def close(self):\n        """"""Close tensorboard writer""""""\n        self.writer.close()\n\n    def log_scalar(self, name: str, value: float, global_step: int):\n        """"""\n        :param name: name of metric\n        :param value: float value of metric\n        :param global_step: current step of the training loop\n        :return:\n        """"""\n        with self.writer.as_default():\n            self.summary.scalar(name, value, step=global_step)\n        self.writer.flush()\n\n    def send(self, logger: MainLogger):\n        """"""Take log history from logger and store it in tensorboard event""""""\n        for name, log_items in logger.log_history.items():\n            last_log_item = log_items[-1]\n            self.log_scalar(name, last_log_item.value, last_log_item.step)\n'"
