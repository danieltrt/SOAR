file_path,api_count,code
SimpleHRNet.py,15,"b'import cv2\nimport numpy as np\nimport torch\nfrom torchvision.transforms import transforms\n\nfrom models.hrnet import HRNet\nfrom models.poseresnet import PoseResNet\nfrom models.detectors.YOLOv3 import YOLOv3\n\n\nclass SimpleHRNet:\n    """"""\n    SimpleHRNet class.\n\n    The class provides a simple and customizable method to load the HRNet network, load the official pre-trained\n    weights, and predict the human pose on single images.\n    Multi-person support with the YOLOv3 detector is also included (and enabled by default).\n    """"""\n\n    def __init__(self,\n                 c,\n                 nof_joints,\n                 checkpoint_path,\n                 model_name=\'HRNet\',\n                 resolution=(384, 288),\n                 interpolation=cv2.INTER_CUBIC,\n                 multiperson=True,\n                 return_bounding_boxes=False,\n                 max_batch_size=32,\n                 yolo_model_def=""./models/detectors/yolo/config/yolov3.cfg"",\n                 yolo_class_path=""./models/detectors/yolo/data/coco.names"",\n                 yolo_weights_path=""./models/detectors/yolo/weights/yolov3.weights"",\n                 device=torch.device(""cpu"")):\n        """"""\n        Initializes a new SimpleHRNet object.\n        HRNet (and YOLOv3) are initialized on the torch.device(""device"") and\n        its (their) pre-trained weights will be loaded from disk.\n\n        Args:\n            c (int): number of channels (when using HRNet model) or resnet size (when using PoseResNet model).\n            nof_joints (int): number of joints.\n            checkpoint_path (str): path to an official hrnet checkpoint or a checkpoint obtained with `train_coco.py`.\n            model_name (str): model name (HRNet or PoseResNet).\n                Valid names for HRNet are: `HRNet`, `hrnet`\n                Valid names for PoseResNet are: `PoseResNet`, `poseresnet`, `ResNet`, `resnet`\n                Default: ""HRNet""\n            resolution (tuple): hrnet input resolution - format: (height, width).\n                Default: (384, 288)\n            interpolation (int): opencv interpolation algorithm.\n                Default: cv2.INTER_CUBIC\n            multiperson (bool): if True, multiperson detection will be enabled.\n                This requires the use of a people detector (like YOLOv3).\n                Default: True\n            return_bounding_boxes (bool): if True, bounding boxes will be returned along with poses by self.predict.\n                Default: False\n            max_batch_size (int): maximum batch size used in hrnet inference.\n                Useless without multiperson=True.\n                Default: 16\n            yolo_model_def (str): path to yolo model definition file.\n                Default: ""./models/detectors/yolo/config/yolov3.cfg""\n            yolo_class_path (str): path to yolo class definition file.\n                Default: ""./models/detectors/yolo/data/coco.names""\n            yolo_weights_path (str): path to yolo pretrained weights file.\n                Default: ""./models/detectors/yolo/weights/yolov3.weights.cfg""\n            device (:class:`torch.device`): the hrnet (and yolo) inference will be run on this device.\n                Default: torch.device(""cpu"")\n        """"""\n\n        self.c = c\n        self.nof_joints = nof_joints\n        self.checkpoint_path = checkpoint_path\n        self.model_name = model_name\n        self.resolution = resolution  # in the form (height, width) as in the original implementation\n        self.interpolation = interpolation\n        self.multiperson = multiperson\n        self.return_bounding_boxes = return_bounding_boxes\n        self.max_batch_size = max_batch_size\n        self.yolo_model_def = yolo_model_def\n        self.yolo_class_path = yolo_class_path\n        self.yolo_weights_path = yolo_weights_path\n        self.device = device\n\n        if model_name in (\'HRNet\', \'hrnet\'):\n            self.model = HRNet(c=c, nof_joints=nof_joints)\n        elif model_name in (\'PoseResNet\', \'poseresnet\', \'ResNet\', \'resnet\'):\n            self.model = PoseResNet(resnet_size=c, nof_joints=nof_joints)\n        else:\n            raise ValueError(\'Wrong model name.\')\n\n        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n        if \'model\' in checkpoint:\n            self.model.load_state_dict(checkpoint[\'model\'])\n        else:\n            self.model.load_state_dict(checkpoint)\n\n        if \'cuda\' in str(self.device):\n            print(""device: \'cuda\' - "", end="""")\n\n            if \'cuda\' == str(self.device):\n                # if device is set to \'cuda\', all available GPUs will be used\n                print(""%d GPU(s) will be used"" % torch.cuda.device_count())\n                device_ids = None\n            else:\n                # if device is set to \'cuda:IDS\', only that/those device(s) will be used\n                print(""GPU(s) \'%s\' will be used"" % str(self.device))\n                device_ids = [int(x) for x in str(self.device)[5:].split(\',\')]\n\n            self.model = torch.nn.DataParallel(self.model, device_ids=device_ids)\n        elif \'cpu\' == str(self.device):\n            print(""device: \'cpu\'"")\n        else:\n            raise ValueError(\'Wrong device name.\')\n\n        self.model = self.model.to(device)\n        self.model.eval()\n\n        if not self.multiperson:\n            self.transform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ])\n\n        else:\n            self.detector = YOLOv3(model_def=yolo_model_def,\n                                   class_path=yolo_class_path,\n                                   weights_path=yolo_weights_path,\n                                   classes=(\'person\',),\n                                   max_batch_size=self.max_batch_size,\n                                   device=device)\n            self.transform = transforms.Compose([\n                transforms.ToPILImage(),\n                transforms.Resize((self.resolution[0], self.resolution[1])),  # (height, width)\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ])\n\n    def predict(self, image):\n        """"""\n        Predicts the human pose on a single image or a stack of n images.\n\n        Args:\n            image (:class:`np.ndarray`):\n                the image(s) on which the human pose will be estimated.\n\n                image is expected to be in the opencv format.\n                image can be:\n                    - a single image with shape=(height, width, BGR color channel)\n                    - a stack of n images with shape=(n, height, width, BGR color channel)\n\n        Returns:\n            :class:`np.ndarray`:\n                a numpy array containing human joints for each (detected) person.\n\n                Format:\n                    if image is a single image:\n                        shape=(# of people, # of joints (nof_joints), 3);  dtype=(np.float32).\n                    if image is a stack of n images:\n                        list of n np.ndarrays with\n                        shape=(# of people, # of joints (nof_joints), 3);  dtype=(np.float32).\n\n                Each joint has 3 values: (y position, x position, joint confidence).\n\n                If self.return_bounding_boxes, the class returns a list with (bounding boxes, human joints)\n        """"""\n        if len(image.shape) == 3:\n            return self._predict_single(image)\n        elif len(image.shape) == 4:\n            return self._predict_batch(image)\n        else:\n            raise ValueError(\'Wrong image format.\')\n\n    def _predict_single(self, image):\n        if not self.multiperson:\n            old_res = image.shape\n            if self.resolution is not None:\n                image = cv2.resize(\n                    image,\n                    (self.resolution[1], self.resolution[0]),  # (width, height)\n                    interpolation=self.interpolation\n                )\n\n            images = self.transform(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)).unsqueeze(dim=0)\n            boxes = np.asarray([[0, 0, old_res[1], old_res[0]]], dtype=np.float32)  # [x1, y1, x2, y2]\n\n        else:\n            detections = self.detector.predict_single(image)\n\n            nof_people = len(detections) if detections is not None else 0\n            boxes = np.empty((nof_people, 4), dtype=np.int32)\n            images = torch.empty((nof_people, 3, self.resolution[0], self.resolution[1]))  # (height, width)\n\n            if detections is not None:\n                for i, (x1, y1, x2, y2, conf, cls_conf, cls_pred) in enumerate(detections):\n                    x1 = int(round(x1.item()))\n                    x2 = int(round(x2.item()))\n                    y1 = int(round(y1.item()))\n                    y2 = int(round(y2.item()))\n\n                    # Adapt detections to match HRNet input aspect ratio (as suggested by xtyDoge in issue #14)\n                    correction_factor = self.resolution[0] / self.resolution[1] * (x2 - x1) / (y2 - y1)\n                    if correction_factor > 1:\n                        # increase y side\n                        center = y1 + (y2 - y1) // 2\n                        length = int(round((y2 - y1) * correction_factor))\n                        y1 = max(0, center - length // 2)\n                        y2 = min(image.shape[0], center + length // 2)\n                    elif correction_factor < 1:\n                        # increase x side\n                        center = x1 + (x2 - x1) // 2\n                        length = int(round((x2 - x1) * 1 / correction_factor))\n                        x1 = max(0, center - length // 2)\n                        x2 = min(image.shape[1], center + length // 2)\n\n                    boxes[i] = [x1, y1, x2, y2]\n                    images[i] = self.transform(image[y1:y2, x1:x2, ::-1])\n\n        if images.shape[0] > 0:\n            images = images.to(self.device)\n\n            with torch.no_grad():\n                if len(images) <= self.max_batch_size:\n                    out = self.model(images)\n\n                else:\n                    out = torch.empty(\n                        (images.shape[0], self.nof_joints, self.resolution[0] // 4, self.resolution[1] // 4),\n                        device=self.device\n                    )\n                    for i in range(0, len(images), self.max_batch_size):\n                        out[i:i + self.max_batch_size] = self.model(images[i:i + self.max_batch_size])\n\n            out = out.detach().cpu().numpy()\n            pts = np.empty((out.shape[0], out.shape[1], 3), dtype=np.float32)\n            # For each human, for each joint: y, x, confidence\n            for i, human in enumerate(out):\n                for j, joint in enumerate(human):\n                    pt = np.unravel_index(np.argmax(joint), (self.resolution[0] // 4, self.resolution[1] // 4))\n                    # 0: pt_y / (height // 4) * (bb_y2 - bb_y1) + bb_y1\n                    # 1: pt_x / (width // 4) * (bb_x2 - bb_x1) + bb_x1\n                    # 2: confidences\n                    pts[i, j, 0] = pt[0] * 1. / (self.resolution[0] // 4) * (boxes[i][3] - boxes[i][1]) + boxes[i][1]\n                    pts[i, j, 1] = pt[1] * 1. / (self.resolution[1] // 4) * (boxes[i][2] - boxes[i][0]) + boxes[i][0]\n                    pts[i, j, 2] = joint[pt]\n\n        else:\n            pts = np.empty((0, 0, 3), dtype=np.float32)\n\n        if self.return_bounding_boxes:\n            return boxes, pts\n        else:\n            return pts\n\n    def _predict_batch(self, images):\n        if not self.multiperson:\n            old_res = images[0].shape\n\n            if self.resolution is not None:\n                images_tensor = torch.empty(images.shape[0], 3, self.resolution[0], self.resolution[1])\n            else:\n                images_tensor = torch.empty(images.shape[0], 3, images.shape[1], images.shape[2])\n\n            for i, image in enumerate(images):\n                if self.resolution is not None:\n                    image = cv2.resize(\n                        image,\n                        (self.resolution[1], self.resolution[0]),  # (width, height)\n                        interpolation=self.interpolation\n                    )\n\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n                images_tensor[i] = self.transform(image)\n\n            images = images_tensor\n            boxes = np.repeat(\n                np.asarray([[0, 0, old_res[1], old_res[0]]], dtype=np.float32), len(images), axis=0\n            )  # [x1, y1, x2, y2]\n\n        else:\n            image_detections = self.detector.predict(images)\n\n            base_index = 0\n            nof_people = int(np.sum([len(d) for d in image_detections if d is not None]))\n            boxes = np.empty((nof_people, 4), dtype=np.int32)\n            images_tensor = torch.empty((nof_people, 3, self.resolution[0], self.resolution[1]))  # (height, width)\n\n            for d, detections in enumerate(image_detections):\n                image = images[d]\n                if detections is not None and len(detections) > 0:\n                    for i, (x1, y1, x2, y2, conf, cls_conf, cls_pred) in enumerate(detections):\n                        x1 = int(round(x1.item()))\n                        x2 = int(round(x2.item()))\n                        y1 = int(round(y1.item()))\n                        y2 = int(round(y2.item()))\n\n                        # Adapt detections to match HRNet input aspect ratio (as suggested by xtyDoge in issue #14)\n                        correction_factor = self.resolution[0] / self.resolution[1] * (x2 - x1) / (y2 - y1)\n                        if correction_factor > 1:\n                            # increase y side\n                            center = y1 + (y2 - y1) // 2\n                            length = int(round((y2 - y1) * correction_factor))\n                            y1 = max(0, center - length // 2)\n                            y2 = min(image.shape[0], center + length // 2)\n                        elif correction_factor < 1:\n                            # increase x side\n                            center = x1 + (x2 - x1) // 2\n                            length = int(round((x2 - x1) * 1 / correction_factor))\n                            x1 = max(0, center - length // 2)\n                            x2 = min(image.shape[1], center + length // 2)\n\n                        boxes[base_index + i] = [x1, y1, x2, y2]\n                        images_tensor[base_index + i] = self.transform(image[y1:y2, x1:x2, ::-1])\n\n                    base_index += len(detections)\n\n            images = images_tensor\n\n        images = images.to(self.device)\n\n        if images.shape[0] > 0:\n            with torch.no_grad():\n                if len(images) <= self.max_batch_size:\n                    out = self.model(images)\n\n                else:\n                    out = torch.empty(\n                        (images.shape[0], self.nof_joints, self.resolution[0] // 4, self.resolution[1] // 4),\n                        device=self.device\n                    )\n                    for i in range(0, len(images), self.max_batch_size):\n                        out[i:i + self.max_batch_size] = self.model(images[i:i + self.max_batch_size])\n\n            out = out.detach().cpu().numpy()\n            pts = np.empty((out.shape[0], out.shape[1], 3), dtype=np.float32)\n            # For each human, for each joint: y, x, confidence\n            for i, human in enumerate(out):\n                for j, joint in enumerate(human):\n                    pt = np.unravel_index(np.argmax(joint), (self.resolution[0] // 4, self.resolution[1] // 4))\n                    # 0: pt_y / (height // 4) * (bb_y2 - bb_y1) + bb_y1\n                    # 1: pt_x / (width // 4) * (bb_x2 - bb_x1) + bb_x1\n                    # 2: confidences\n                    pts[i, j, 0] = pt[0] * 1. / (self.resolution[0] // 4) * (boxes[i][3] - boxes[i][1]) + boxes[i][1]\n                    pts[i, j, 1] = pt[1] * 1. / (self.resolution[1] // 4) * (boxes[i][2] - boxes[i][0]) + boxes[i][0]\n                    pts[i, j, 2] = joint[pt]\n\n            if self.multiperson:\n                # re-add the removed batch axis (n)\n                pts_batch = []\n                index = 0\n                for detections in image_detections:\n                    if detections is not None:\n                        pts_batch.append(pts[index:index + len(detections)])\n                        index += len(detections)\n                    else:\n                        pts_batch.append(np.zeros((0, self.nof_joints, 3), dtype=np.float32))\n                pts = pts_batch\n\n            else:\n                pts = np.expand_dims(pts, axis=1)\n\n        else:\n            boxes = np.asarray([], dtype=np.int32)\n            if self.multiperson:\n                pts = []\n                for _ in range(len(image_detections)):\n                    pts.append(np.zeros((0, self.nof_joints, 3), dtype=np.float32))\n            else:\n                raise ValueError  # should never happen\n\n        if self.return_bounding_boxes:\n            return boxes, pts\n        else:\n            return pts\n'"
datasets/COCO.py,0,"b'# Part of this code is derived/taken from https://github.com/leoxiaobin/deep-high-resolution-net.pytorch\nimport os\nimport pickle\nimport random\nfrom collections import OrderedDict\nfrom collections import defaultdict\n\nimport cv2\nimport json_tricks as json\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\nfrom misc.nms.nms import oks_nms\nfrom misc.nms.nms import soft_oks_nms\nfrom misc.utils import fliplr_joints, affine_transform, get_affine_transform, evaluate_pck_accuracy\nfrom .HumanPoseEstimation import HumanPoseEstimationDataset as Dataset\n\n\nclass COCODataset(Dataset):\n    """"""\n    COCODataset class.\n    """"""\n\n    def __init__(self,\n                 root_path=""./datasets/COCO"", data_version=""train2017"", is_train=True, use_gt_bboxes=True, bbox_path="""",\n                 image_width=288, image_height=384, color_rgb=True,\n                 scale=True, scale_factor=0.35, flip_prob=0.5, rotate_prob=0.5, rotation_factor=45., half_body_prob=0.3,\n                 use_different_joints_weight=False, heatmap_sigma=3, soft_nms=False,\n                 ):\n        """"""\n        Initializes a new COCODataset object.\n\n        Image and annotation indexes are loaded and stored in memory.\n        Annotations are preprocessed to have a simple list of annotations to iterate over.\n\n        Bounding boxes can be loaded from the ground truth or from a pickle file (in this case, no annotations are\n        provided).\n\n        Args:\n            root_path (str): dataset root path.\n                Default: ""./datasets/COCO""\n            data_version (str): desired version/folder of COCO. Possible options are ""train2017"", ""val2017"".\n                Default: ""train2017""\n            is_train (bool): train or eval mode. If true, train mode is used.\n                Default: True\n            use_gt_bboxes (bool): use ground truth bounding boxes. If False, bbox_path is required.\n                Default: True\n            bbox_path (str): bounding boxes pickle file path.\n                Default: """"\n            image_width (int): image width.\n                Default: 288\n            image_height (int): image height.\n                Default: ``384``\n            color_rgb (bool): rgb or bgr color mode. If True, rgb color mode is used.\n                Default: True\n            scale (bool): scale mode.\n                Default: True\n            scale_factor (float): scale factor.\n                Default: 0.35\n            flip_prob (float): flip probability.\n                Default: 0.5\n            rotate_prob (float): rotate probability.\n                Default: 0.5\n            rotation_factor (float): rotation factor.\n                Default: 45.\n            half_body_prob (float): half body probability.\n                Default: 0.3\n            use_different_joints_weight (bool): use different joints weights.\n                If true, the following joints weights will be used:\n                [1., 1., 1., 1., 1., 1., 1., 1.2, 1.2, 1.5, 1.5, 1., 1., 1.2, 1.2, 1.5, 1.5]\n                Default: False\n            heatmap_sigma (float): sigma of the gaussian used to create the heatmap.\n                Default: 3\n            soft_nms (bool): enable soft non-maximum suppression.\n                Default: False\n        """"""\n        super(COCODataset, self).__init__()\n\n        self.root_path = root_path\n        self.data_version = data_version\n        self.is_train = is_train\n        self.use_gt_bboxes = use_gt_bboxes\n        self.bbox_path = bbox_path\n        self.image_width = image_width\n        self.image_height = image_height\n        self.color_rgb = color_rgb\n        self.scale = scale  # ToDo Check\n        self.scale_factor = scale_factor\n        self.flip_prob = flip_prob\n        self.rotate_prob = rotate_prob\n        self.rotation_factor = rotation_factor\n        self.half_body_prob = half_body_prob\n        self.use_different_joints_weight = use_different_joints_weight  # ToDo Check\n        self.heatmap_sigma = heatmap_sigma\n        self.soft_nms = soft_nms\n\n        self.data_path = os.path.join(self.root_path, self.data_version)\n        self.annotation_path = os.path.join(\n            self.root_path, \'annotations\', \'person_keypoints_%s.json\' % self.data_version\n        )\n\n        self.image_size = (self.image_width, self.image_height)\n        self.aspect_ratio = self.image_width * 1.0 / self.image_height\n        self.heatmap_size = (int(self.image_width / 4), int(self.image_height / 4))\n        self.heatmap_type = \'gaussian\'\n        self.pixel_std = 200  # I don\'t understand the meaning of pixel_std (=200) in the original implementation\n\n        self.nof_joints = 17\n        self.nof_joints_half_body = 8\n        self.flip_pairs = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16]]\n        self.upper_body_ids = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n        self.lower_body_ids = [11, 12, 13, 14, 15, 16]\n        self.joints_weight = np.asarray(\n            [1., 1., 1., 1., 1., 1., 1., 1.2, 1.2, 1.5, 1.5, 1., 1., 1.2, 1.2, 1.5, 1.5],\n            dtype=np.float32\n        ).reshape((self.nof_joints, 1))\n\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ])\n\n        # Load COCO dataset - Create COCO object then load images and annotations\n        self.coco = COCO(self.annotation_path)\n        self.imgIds = self.coco.getImgIds()\n\n        # Create a list of annotations and the corresponding image (each image can contain more than one detection)\n\n        # Load bboxes and joints\n        #  if self.use_gt_bboxes -> Load GT bboxes and joints\n        #  else -> Load pre-predicted bboxes by a detector (as YOLOv3) and null joints\n\n        if not self.use_gt_bboxes:\n            # bboxes must be saved as the original COCO annotations\n            # i.e. the format must be:\n            #  bboxes = {\n            #    \'<imgId>\': [\n            #      {\n            #        \'id\': <annId>,  # progressive id for debugging\n            #        \'clean_bbox\': np.array([<x>, <y>, <w>, <h>])}\n            #      },\n            #      ...\n            #    ],\n            #    ...\n            #  }\n            with open(self.bbox_path, \'rb\') as fd:\n                bboxes = pickle.load(fd)\n\n        self.data = []\n        # load annotations for each image of COCO\n        for imgId in tqdm(self.imgIds):\n\n            ann_ids = self.coco.getAnnIds(imgIds=imgId, iscrowd=False)\n\n            img = self.coco.loadImgs(imgId)[0]\n\n            if self.use_gt_bboxes:\n                objs = self.coco.loadAnns(ann_ids)\n\n                # sanitize bboxes\n                valid_objs = []\n                for obj in objs:\n                    # Skip non-person objects (it should never happen)\n                    if obj[\'category_id\'] != 1:\n                        continue\n\n                    # ignore objs without keypoints annotation\n                    if max(obj[\'keypoints\']) == 0:\n                        continue\n\n                    x, y, w, h = obj[\'bbox\']\n                    x1 = np.max((0, x))\n                    y1 = np.max((0, y))\n                    x2 = np.min((img[\'width\'] - 1, x1 + np.max((0, w - 1))))\n                    y2 = np.min((img[\'height\'] - 1, y1 + np.max((0, h - 1))))\n\n                    # Use only valid bounding boxes\n                    if obj[\'area\'] > 0 and x2 >= x1 and y2 >= y1:\n                        obj[\'clean_bbox\'] = [x1, y1, x2 - x1, y2 - y1]\n                        valid_objs.append(obj)\n\n                objs = valid_objs\n\n            else:\n                objs = bboxes[imgId]\n\n            # for each annotation of this image, add the formatted annotation to self.data\n            for obj in objs:\n                joints = np.zeros((self.nof_joints, 2), dtype=np.float)\n                joints_visibility = np.ones((self.nof_joints, 2), dtype=np.float)\n\n                if self.use_gt_bboxes:\n                    # COCO pre-processing\n\n                    # # Moved above\n                    # # Skip non-person objects (it should never happen)\n                    # if obj[\'category_id\'] != 1:\n                    #     continue\n                    #\n                    # # ignore objs without keypoints annotation\n                    # if max(obj[\'keypoints\']) == 0:\n                    #     continue\n\n                    for pt in range(self.nof_joints):\n                        joints[pt, 0] = obj[\'keypoints\'][pt * 3 + 0]\n                        joints[pt, 1] = obj[\'keypoints\'][pt * 3 + 1]\n                        t_vis = int(np.clip(obj[\'keypoints\'][pt * 3 + 2], 0, 1))  # ToDo check correctness\n                        # COCO:\n                        #   if visibility == 0 -> keypoint is not in the image.\n                        #   if visibility == 1 -> keypoint is in the image BUT not visible (e.g. behind an object).\n                        #   if visibility == 2 -> keypoint looks clearly (i.e. it is not hidden).\n                        joints_visibility[pt, 0] = t_vis\n                        joints_visibility[pt, 1] = t_vis\n\n                center, scale = self._box2cs(obj[\'clean_bbox\'][:4])\n\n                self.data.append({\n                    \'imgId\': imgId,\n                    \'annId\': obj[\'id\'],\n                    \'imgPath\': os.path.join(self.root_path, self.data_version, \'%012d.jpg\' % imgId),\n                    \'center\': center,\n                    \'scale\': scale,\n                    \'joints\': joints,\n                    \'joints_visibility\': joints_visibility,\n                })\n\n        # Done check if we need prepare_data -> We should not\n        print(\'\\nCOCO dataset loaded!\')\n\n        # Default values\n        self.bbox_thre = 1.0\n        self.image_thre = 0.0\n        self.in_vis_thre = 0.2\n        self.nms_thre = 1.0\n        self.oks_thre = 0.9\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        joints_data = self.data[index].copy()\n\n        # Read the image from disk\n        image = cv2.imread(joints_data[\'imgPath\'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n\n        if self.color_rgb:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if image is None:\n            raise ValueError(\'Fail to read %s\' % image)\n\n        joints = joints_data[\'joints\']\n        joints_vis = joints_data[\'joints_visibility\']\n\n        c = joints_data[\'center\']\n        s = joints_data[\'scale\']\n        score = joints_data[\'score\'] if \'score\' in joints_data else 1\n        r = 0\n\n        # Apply data augmentation\n        if self.is_train:\n            if self.half_body_prob and \\\n                    random.random() < self.half_body_prob and \\\n                    np.sum(joints_vis[:, 0]) > self.nof_joints_half_body:\n                c_half_body, s_half_body = self._half_body_transform(joints, joints_vis)\n\n                if c_half_body is not None and s_half_body is not None:\n                    c, s = c_half_body, s_half_body\n\n            sf = self.scale_factor\n            rf = self.rotation_factor\n\n            if self.scale:\n                s = s * np.clip(random.random() * sf + 1, 1 - sf, 1 + sf)  # A random scale factor in [1 - sf, 1 + sf]\n\n            if self.rotate_prob and random.random() < self.rotate_prob:\n                r = np.clip(random.random() * rf, -rf * 2, rf * 2)  # A random rotation factor in [-2 * rf, 2 * rf]\n            else:\n                r = 0\n\n            if self.flip_prob and random.random() < self.flip_prob:\n                image = image[:, ::-1, :]\n                joints, joints_vis = fliplr_joints(joints, joints_vis, image.shape[1], self.flip_pairs)\n                c[0] = image.shape[1] - c[0] - 1\n\n        # Apply affine transform on joints and image\n        trans = get_affine_transform(c, s, self.pixel_std, r, self.image_size)\n        image = cv2.warpAffine(\n            image,\n            trans,\n            (int(self.image_size[0]), int(self.image_size[1])),\n            flags=cv2.INTER_LINEAR\n        )\n\n        for i in range(self.nof_joints):\n            if joints_vis[i, 0] > 0.:\n                joints[i, 0:2] = affine_transform(joints[i, 0:2], trans)\n\n        # Convert image to tensor and normalize\n        if self.transform is not None:  # I could remove this check\n            image = self.transform(image)\n\n        target, target_weight = self._generate_target(joints, joints_vis)\n\n        # Update metadata\n        joints_data[\'joints\'] = joints\n        joints_data[\'joints_visibility\'] = joints_vis\n        joints_data[\'center\'] = c\n        joints_data[\'scale\'] = s\n        joints_data[\'rotation\'] = r\n        joints_data[\'score\'] = score\n\n        return image, target.astype(np.float32), target_weight.astype(np.float32), joints_data\n\n    def evaluate_accuracy(self, output, target, params=None):\n        if params is not None:\n            hm_type = params[\'hm_type\']\n            thr = params[\'thr\']\n            accs, avg_acc, cnt, joints_preds, joints_target = evaluate_pck_accuracy(output, target, hm_type, thr)\n        else:\n            accs, avg_acc, cnt, joints_preds, joints_target = evaluate_pck_accuracy(output, target)\n\n        return accs, avg_acc, cnt, joints_preds, joints_target\n\n    def evaluate_overall_accuracy(self, predictions, bounding_boxes, image_paths, output_dir, rank=0.):\n\n        res_folder = os.path.join(output_dir, \'results\')\n        if not os.path.exists(res_folder):\n            os.makedirs(res_folder, 0o755, exist_ok=True)\n\n        res_file = os.path.join(res_folder, \'keypoints_{}_results_{}.json\'.format(self.data_version, rank))\n\n        # person x (keypoints)\n        _kpts = []\n        for idx, kpt in enumerate(predictions):\n            _kpts.append({\n                \'keypoints\': kpt,\n                \'center\': bounding_boxes[idx][0:2],\n                \'scale\': bounding_boxes[idx][2:4],\n                \'area\': bounding_boxes[idx][4],\n                \'score\': bounding_boxes[idx][5],\n                \'image\': int(image_paths[idx][-16:-4])\n            })\n\n        # image x person x (keypoints)\n        kpts = defaultdict(list)\n        for kpt in _kpts:\n            kpts[kpt[\'image\']].append(kpt)\n\n        # rescoring and oks nms\n        num_joints = self.nof_joints\n        in_vis_thre = self.in_vis_thre\n        oks_thre = self.oks_thre\n        oks_nmsed_kpts = []\n        for img in kpts.keys():\n            img_kpts = kpts[img]\n            for n_p in img_kpts:\n                box_score = n_p[\'score\']\n                kpt_score = 0\n                valid_num = 0\n                for n_jt in range(0, num_joints):\n                    t_s = n_p[\'keypoints\'][n_jt][2]\n                    if t_s > in_vis_thre:\n                        kpt_score = kpt_score + t_s\n                        valid_num = valid_num + 1\n                if valid_num != 0:\n                    kpt_score = kpt_score / valid_num\n                # rescoring\n                n_p[\'score\'] = kpt_score * box_score\n\n            if self.soft_nms:\n                keep = soft_oks_nms([img_kpts[i] for i in range(len(img_kpts))], oks_thre)\n            else:\n                keep = oks_nms([img_kpts[i] for i in range(len(img_kpts))], oks_thre)\n\n            if len(keep) == 0:\n                oks_nmsed_kpts.append(img_kpts)\n            else:\n                oks_nmsed_kpts.append([img_kpts[_keep] for _keep in keep])\n\n        self._write_coco_keypoint_results(oks_nmsed_kpts, res_file)\n        if \'test\' not in self.data_version:\n            info_str = self._do_python_keypoint_eval(res_file)\n            name_value = OrderedDict(info_str)\n            return name_value, name_value[\'AP\']\n        else:\n            return {\'Null\': 0}, 0\n\n    # Private methods\n\n    def _box2cs(self, box):\n        x, y, w, h = box[:4]\n        return self._xywh2cs(x, y, w, h)\n\n    def _xywh2cs(self, x, y, w, h):\n        center = np.zeros((2,), dtype=np.float32)\n        center[0] = x + w * 0.5\n        center[1] = y + h * 0.5\n\n        if w > self.aspect_ratio * h:\n            h = w * 1.0 / self.aspect_ratio\n        elif w < self.aspect_ratio * h:\n            w = h * self.aspect_ratio\n        scale = np.array(\n            [w * 1.0 / self.pixel_std, h * 1.0 / self.pixel_std],\n            dtype=np.float32)\n        if center[0] != -1:\n            scale = scale * 1.25\n\n        return center, scale\n\n    def _half_body_transform(self, joints, joints_vis):\n        upper_joints = []\n        lower_joints = []\n        for joint_id in range(self.nof_joints):\n            if joints_vis[joint_id][0] > 0:\n                if joint_id in self.upper_body_ids:\n                    upper_joints.append(joints[joint_id])\n                else:\n                    lower_joints.append(joints[joint_id])\n\n        if random.random() < 0.5 and len(upper_joints) > 2:\n            selected_joints = upper_joints\n        else:\n            selected_joints = lower_joints \\\n                if len(lower_joints) > 2 else upper_joints\n\n        if len(selected_joints) < 2:\n            return None, None\n\n        selected_joints = np.array(selected_joints, dtype=np.float32)\n        center = selected_joints.mean(axis=0)[:2]\n\n        left_top = np.amin(selected_joints, axis=0)\n        right_bottom = np.amax(selected_joints, axis=0)\n\n        w = right_bottom[0] - left_top[0]\n        h = right_bottom[1] - left_top[1]\n\n        if w > self.aspect_ratio * h:\n            h = w * 1.0 / self.aspect_ratio\n        elif w < self.aspect_ratio * h:\n            w = h * self.aspect_ratio\n\n        scale = np.array(\n            [\n                w * 1.0 / self.pixel_std,\n                h * 1.0 / self.pixel_std\n            ],\n            dtype=np.float32\n        )\n\n        scale = scale * 1.5\n\n        return center, scale\n\n    def _generate_target(self, joints, joints_vis):\n        """"""\n        :param joints:  [nof_joints, 3]\n        :param joints_vis: [nof_joints, 3]\n        :return: target, target_weight(1: visible, 0: invisible)\n        """"""\n        target_weight = np.ones((self.nof_joints, 1), dtype=np.float32)\n        target_weight[:, 0] = joints_vis[:, 0]\n\n        if self.heatmap_type == \'gaussian\':\n            target = np.zeros((self.nof_joints,\n                               self.heatmap_size[1],\n                               self.heatmap_size[0]),\n                              dtype=np.float32)\n\n            tmp_size = self.heatmap_sigma * 3\n\n            for joint_id in range(self.nof_joints):\n                feat_stride = np.asarray(self.image_size) / np.asarray(self.heatmap_size)\n                mu_x = int(joints[joint_id][0] / feat_stride[0] + 0.5)\n                mu_y = int(joints[joint_id][1] / feat_stride[1] + 0.5)\n                # Check that any part of the gaussian is in-bounds\n                ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n                br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n                if ul[0] >= self.heatmap_size[0] or ul[1] >= self.heatmap_size[1] \\\n                        or br[0] < 0 or br[1] < 0:\n                    # If not, just return the image as is\n                    target_weight[joint_id] = 0\n                    continue\n\n                # # Generate gaussian\n                size = 2 * tmp_size + 1\n                x = np.arange(0, size, 1, np.float32)\n                y = x[:, np.newaxis]\n                x0 = y0 = size // 2\n                # The gaussian is not normalized, we want the center value to equal 1\n                g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * self.heatmap_sigma ** 2))\n\n                # Usable gaussian range\n                g_x = max(0, -ul[0]), min(br[0], self.heatmap_size[0]) - ul[0]\n                g_y = max(0, -ul[1]), min(br[1], self.heatmap_size[1]) - ul[1]\n                # Image range\n                img_x = max(0, ul[0]), min(br[0], self.heatmap_size[0])\n                img_y = max(0, ul[1]), min(br[1], self.heatmap_size[1])\n\n                v = target_weight[joint_id]\n                if v > 0.5:\n                    target[joint_id][img_y[0]:img_y[1], img_x[0]:img_x[1]] = \\\n                        g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n        else:\n            raise NotImplementedError\n\n        if self.use_different_joints_weight:\n            target_weight = np.multiply(target_weight, self.joints_weight)\n\n        return target, target_weight\n\n    def _write_coco_keypoint_results(self, keypoints, res_file):\n        data_pack = [\n            {\n                \'cat_id\': 1,  # 1 == \'person\'\n                \'cls\': \'person\',\n                \'ann_type\': \'keypoints\',\n                \'keypoints\': keypoints\n            }\n        ]\n\n        results = self._coco_keypoint_results_one_category_kernel(data_pack[0])\n        with open(res_file, \'w\') as f:\n            json.dump(results, f, sort_keys=True, indent=4)\n        try:\n            json.load(open(res_file))\n        except Exception:\n            content = []\n            with open(res_file, \'r\') as f:\n                for line in f:\n                    content.append(line)\n            content[-1] = \']\'\n            with open(res_file, \'w\') as f:\n                for c in content:\n                    f.write(c)\n\n    def _coco_keypoint_results_one_category_kernel(self, data_pack):\n        cat_id = data_pack[\'cat_id\']\n        keypoints = data_pack[\'keypoints\']\n        cat_results = []\n\n        for img_kpts in keypoints:\n            if len(img_kpts) == 0:\n                continue\n\n            _key_points = np.array([img_kpts[k][\'keypoints\'] for k in range(len(img_kpts))], dtype=np.float32)\n            key_points = np.zeros((_key_points.shape[0], self.nof_joints * 3), dtype=np.float32)\n\n            for ipt in range(self.nof_joints):\n                key_points[:, ipt * 3 + 0] = _key_points[:, ipt, 0]\n                key_points[:, ipt * 3 + 1] = _key_points[:, ipt, 1]\n                key_points[:, ipt * 3 + 2] = _key_points[:, ipt, 2]  # keypoints score.\n\n            result = [\n                {\n                    \'image_id\': img_kpts[k][\'image\'],\n                    \'category_id\': cat_id,\n                    \'keypoints\': list(key_points[k]),\n                    \'score\': img_kpts[k][\'score\'].astype(np.float32),\n                    \'center\': list(img_kpts[k][\'center\']),\n                    \'scale\': list(img_kpts[k][\'scale\'])\n                }\n                for k in range(len(img_kpts))\n            ]\n            cat_results.extend(result)\n\n        return cat_results\n\n    def _do_python_keypoint_eval(self, res_file):\n        coco_dt = self.coco.loadRes(res_file)\n        coco_eval = COCOeval(self.coco, coco_dt, \'keypoints\')\n        coco_eval.params.useSegm = None\n        coco_eval.evaluate()\n        coco_eval.accumulate()\n        coco_eval.summarize()\n\n        stats_names = [\'AP\', \'Ap .5\', \'AP .75\', \'AP (M)\', \'AP (L)\', \'AR\', \'AR .5\', \'AR .75\', \'AR (M)\', \'AR (L)\']\n\n        info_str = []\n        for ind, name in enumerate(stats_names):\n            info_str.append((name, coco_eval.stats[ind]))\n\n        return info_str\n\n\nif __name__ == \'__main__\':\n    coco = COCODataset(rotate_prob=0., half_body_prob=0.)\n    item = coco.__getitem__(0)\n    print(\'ok!!\')\n    # img = np.clip(np.transpose(item[0].numpy(), (1, 2, 0))[:, :, ::-1] * np.asarray([0.229, 0.224, 0.225]) +\n    #     np.asarray([0.485, 0.456, 0.406]), 0, 1) * 255\n    # cv2.imwrite(\'./tmp.png\', img.astype(np.uint8))\n    # print(item[-1])\n    pass\n'"
datasets/HumanPoseEstimation.py,1,"b'from torch.utils.data import Dataset\n\n\nclass HumanPoseEstimationDataset(Dataset):\n    """"""\n    HumanPoseEstimationDataset class.\n\n    Generic class for HPE datasets.\n    """"""\n    def __init__(self):\n        pass\n\n    def __len__(self):\n        pass\n\n    def __getitem__(self, item):\n        pass\n\n    def evaluate_accuracy(self, output, target, params=None):\n        pass\n'"
datasets/LiveCamera.py,4,"b'import cv2\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom models.detectors.YOLOv3 import YOLOv3\n\n\nclass LiveCameraDataset(Dataset):\n    def __init__(self, camera_id=0, epoch_length=1, resolution=(384, 288), interpolation=cv2.INTER_CUBIC,\n                 multiperson=False, device=torch.device(\'cpu\')):\n        super(LiveCameraDataset, self).__init__()\n        self.camera_id = camera_id\n        self.epoch_length = epoch_length\n        self.resolution = resolution\n        self.interpolation = interpolation\n        self.multiperson = multiperson\n        self.device = device\n\n        self.camera = cv2.VideoCapture(self.camera_id)\n        assert self.camera.isOpened()\n\n        if not self.multiperson:\n            self.transform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ])\n\n        else:\n            self.detector = YOLOv3(model_def=""./models/detectors/yolo/config/yolov3.cfg"",\n                                   class_path=""./models/detectors/yolo/data/coco.names"",\n                                   weights_path=""./models/detectors/yolo/weights/yolov3.weights"",\n                                   classes=(\'person\',), device=device)\n\n            self.transform = transforms.Compose([\n                transforms.ToPILImage(),\n                transforms.Resize((self.resolution[1], self.resolution[0])),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n            ])\n\n    def __len__(self):\n        return self.epoch_length\n\n    def __getitem__(self, item):\n        ret, frame = self.camera.read()\n\n        if ret:\n            if not self.multiperson:\n                if self.resolution is not None:\n                    frame = cv2.resize(frame, tuple(self.resolution), interpolation=self.interpolation)\n\n                frame_torch = self.transform(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).unsqueeze(0)\n\n                return ret, frame, frame_torch, []\n\n            else:\n                detections = self.detector.predict_single(frame)\n\n                if detections is not None:\n                    boxes = []\n                    frames = torch.zeros((len(detections), 3, self.resolution[1], self.resolution[0]))\n                    for i, (x1, y1, x2, y2, conf, cls_conf, cls_pred) in enumerate(detections):\n                        x1 = int(round(x1.item()))\n                        x2 = int(round(x2.item()))\n                        y1 = int(round(y1.item()))\n                        y2 = int(round(y2.item()))\n\n                        boxes.append([x1, y1, x2, y2])\n                        frames[i] = self.transform(frame[y1:y2, x1:x2, ::-1])\n\n                    return ret, frame, boxes, frames\n                else:\n                    boxes = []\n                    frames = torch.zeros((0, 3, self.resolution[1], self.resolution[0]))\n                    return ret, frame, boxes, frames\n\n        else:\n            return ret, [], [], []\n\n    def __del__(self):\n        if self.camera.isOpened():\n            self.camera.release()\n'"
datasets/__init__.py,0,b''
losses/__init__.py,0,b''
losses/loss.py,5,"b'import torch\nimport torch.nn as nn\n\n\n# derived from https://github.com/leoxiaobin/deep-high-resolution-net.pytorch\ndef ohkm(loss, topk):\n    ohkm_loss = 0.\n    for i in range(loss.shape[0]):\n        sub_loss = loss[i]\n        topk_val, topk_idx = torch.topk(\n            sub_loss, k=topk, dim=0, sorted=False\n        )\n        tmp_loss = torch.gather(sub_loss, 0, topk_idx)\n        ohkm_loss += torch.sum(tmp_loss) / topk\n    ohkm_loss /= loss.shape[0]\n    return ohkm_loss\n\n\n# derived from https://github.com/leoxiaobin/deep-high-resolution-net.pytorch\nclass JointsMSELoss(nn.Module):\n    def __init__(self, use_target_weight=True):\n        """"""\n        MSE loss between output and GT body joints\n\n        Args:\n            use_target_weight (bool): use target weight.\n                WARNING! This should be always true, otherwise the loss will sum the error for non-visible joints too.\n                This has not the same meaning of joint_weights in the COCO dataset.\n        """"""\n        super(JointsMSELoss, self).__init__()\n        self.criterion = nn.MSELoss(reduction=\'mean\')\n        self.use_target_weight = use_target_weight\n\n    def forward(self, output, target, target_weight=None):\n        batch_size = output.shape[0]\n        num_joints = output.shape[1]\n        heatmaps_pred = output.reshape((batch_size, num_joints, -1)).split(1, 1)\n        heatmaps_gt = target.reshape((batch_size, num_joints, -1)).split(1, 1)\n        loss = 0\n\n        for idx in range(num_joints):\n            heatmap_pred = heatmaps_pred[idx].squeeze()\n            heatmap_gt = heatmaps_gt[idx].squeeze()\n            if self.use_target_weight:\n                if target_weight is None:\n                    raise NameError\n                loss += 0.5 * self.criterion(\n                    heatmap_pred.mul(target_weight[:, idx]),\n                    heatmap_gt.mul(target_weight[:, idx])\n                )\n            else:\n                loss += 0.5 * self.criterion(heatmap_pred, heatmap_gt)\n\n        return loss / num_joints\n\n\n# derived from https://github.com/leoxiaobin/deep-high-resolution-net.pytorch\nclass JointsOHKMMSELoss(nn.Module):\n    def __init__(self, use_target_weight=True, topk=8):\n        """"""\n        MSE loss between output and GT body joints\n\n        Args:\n            use_target_weight (bool): use target weight.\n                WARNING! This should be always true, otherwise the loss will sum the error for non-visible joints too.\n                This has not the same meaning of joint_weights in the COCO dataset.\n        """"""\n        super(JointsOHKMMSELoss, self).__init__()\n        self.criterion = nn.MSELoss(reduction=\'none\')\n        self.use_target_weight = use_target_weight\n        self.topk = topk\n\n    def forward(self, output, target, target_weight):\n        batch_size = output.shape[0]\n        num_joints = output.shape[1]\n        heatmaps_pred = output.reshape((batch_size, num_joints, -1)).split(1, 1)\n        heatmaps_gt = target.reshape((batch_size, num_joints, -1)).split(1, 1)\n\n        loss = []\n        for idx in range(num_joints):\n            heatmap_pred = heatmaps_pred[idx].squeeze()\n            heatmap_gt = heatmaps_gt[idx].squeeze()\n            if self.use_target_weight:\n                loss.append(0.5 * self.criterion(\n                    heatmap_pred.mul(target_weight[:, idx]),\n                    heatmap_gt.mul(target_weight[:, idx])\n                ))\n            else:\n                loss.append(0.5 * self.criterion(heatmap_pred, heatmap_gt))\n\n        loss = [l.mean(dim=1).unsqueeze(dim=1) for l in loss]\n        loss = torch.cat(loss, dim=1)\n\n        return self.ohkm(loss, self.topk)\n'"
misc/__init__.py,0,b''
misc/checkpoint.py,9,"b'import os\nimport torch\n\n\ndef save_checkpoint(path, epoch, model, optimizer, params=None):\n    """"""\n    Save a PyTorch checkpoint.\n\n    Args:\n        path (str): path where the checkpoint will be saved.\n        epoch (int): current epoch.\n        model (torch.nn.Module): model whose parameters will be saved.\n        optimizer (torch.optim.Optimizer): optimizer whose parameters will be saved.\n        params (dict): other parameters. Optional.\n            Default: None\n\n    """"""\n    if os.path.isdir(path):\n        path = os.path.join(path, \'checkpoint.pth\')\n    torch.save(\n        {\n            \'epoch\': epoch,\n            \'model\': model.state_dict(),\n            \'optimizer\': optimizer.state_dict(),\n            \'params\': params,\n        },\n        path\n    )\n\n\ndef load_checkpoint(path, model, optimizer=None, device=None):\n    """"""\n    Load a PyTorch checkpoint.\n\n    Args:\n        path (str): checkpoint path.\n        model (torch.nn.Module): model whose parameters will be loaded.\n        optimizer (torch.optim.Optimizer): optimizer whose parameters will be loaded. Optional.\n            Default: None\n        device (torch.device): device to be used.\n            Default: None\n\n    Returns:\n        epoch (int): saved epoch\n        model (torch.nn.Module): reference to `model`\n        optimizer (torch.nn.Optimizer): reference to `optimizer`\n        params (dict): other saved params\n\n    """"""\n    if os.path.isdir(path):\n        path = os.path.join(path, \'checkpoint.pth\')\n    checkpoint = torch.load(path, map_location=device)\n    epoch = checkpoint[\'epoch\']\n    model.load_state_dict(checkpoint[\'model\'])\n    if optimizer is not None:\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n    params = checkpoint[\'params\']\n\n    return epoch, model, optimizer, params\n'"
misc/utils.py,18,"b'import math\nimport cv2\nimport munkres\nimport numpy as np\nimport torch\n\n\n# solution proposed in https://github.com/pytorch/pytorch/issues/229#issuecomment-299424875 \ndef flip_tensor(tensor, dim=0):\n    """"""\n    flip the tensor on the dimension dim\n    """"""\n    inv_idx = torch.arange(tensor.shape[dim] - 1, -1, -1).to(tensor.device)\n    return tensor.index_select(dim, inv_idx)\n\n\n#\n# derived from https://github.com/leoxiaobin/deep-high-resolution-net.pytorch\ndef flip_back(output_flipped, matched_parts):\n    assert len(output_flipped.shape) == 4, \'output_flipped has to be [batch_size, num_joints, height, width]\'\n\n    output_flipped = flip_tensor(output_flipped, dim=-1)\n\n    for pair in matched_parts:\n        tmp = output_flipped[:, pair[0]].clone()\n        output_flipped[:, pair[0]] = output_flipped[:, pair[1]]\n        output_flipped[:, pair[1]] = tmp\n\n    return output_flipped\n\n\ndef fliplr_joints(joints, joints_vis, width, matched_parts):\n    # Flip horizontal\n    joints[:, 0] = width - joints[:, 0] - 1\n\n    # Change left-right parts\n    for pair in matched_parts:\n        joints[pair[0], :], joints[pair[1], :] = \\\n            joints[pair[1], :], joints[pair[0], :].copy()\n        joints_vis[pair[0], :], joints_vis[pair[1], :] = \\\n            joints_vis[pair[1], :], joints_vis[pair[0], :].copy()\n\n    return joints * joints_vis, joints_vis\n\n\ndef get_affine_transform(center, scale, pixel_std, rot, output_size, shift=np.array([0, 0], dtype=np.float32), inv=0):\n    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n        print(scale)\n        scale = np.array([scale, scale])\n\n    scale_tmp = scale * 1.0 * pixel_std  # It was scale_tmp = scale * 200.0\n    src_w = scale_tmp[0]\n    dst_w = output_size[0]\n    dst_h = output_size[1]\n\n    rot_rad = np.pi * rot / 180\n    src_dir = get_dir([0, src_w * -0.5], rot_rad)\n    dst_dir = np.array([0, dst_w * -0.5], np.float32)\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = center + scale_tmp * shift\n    src[1, :] = center + src_dir + scale_tmp * shift\n    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\n    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir\n\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    return trans\n\n\ndef affine_transform(pt, t):\n    new_pt = np.array([pt[0], pt[1], 1.]).T\n    new_pt = np.dot(t, new_pt)\n    return new_pt[:2]\n\n\ndef get_3rd_point(a, b):\n    direct = a - b\n    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\n\n\ndef get_dir(src_point, rot_rad):\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n\n    src_result = [0, 0]\n    src_result[0] = src_point[0] * cs - src_point[1] * sn\n    src_result[1] = src_point[0] * sn + src_point[1] * cs\n\n    return src_result\n\n\ndef crop(img, center, scale, pixel_std, output_size, rot=0):\n    trans = get_affine_transform(center, scale, pixel_std, rot, output_size)\n\n    dst_img = cv2.warpAffine(\n        img, trans, (int(output_size[0]), int(output_size[1])),\n        flags=cv2.INTER_LINEAR\n    )\n\n    return dst_img\n\n\n#\n#\n\n\n#\n# derived from https://github.com/leoxiaobin/deep-high-resolution-net.pytorch\ndef transform_preds(coords, center, scale, pixel_std, output_size):\n    coords = coords.detach().cpu().numpy()\n    target_coords = np.zeros(coords.shape, dtype=np.float32)\n    trans = get_affine_transform(center, scale, pixel_std, 0, output_size, inv=1)\n    for p in range(coords.shape[0]):\n        target_coords[p, 0:2] = affine_transform(coords[p, 0:2], trans)\n    return torch.from_numpy(target_coords)\n\n\ndef get_max_preds(batch_heatmaps):\n    """"""\n    get predictions from score maps\n    heatmaps: numpy.ndarray([batch_size, num_joints, height, width])\n    """"""\n    # assert isinstance(batch_heatmaps, np.ndarray), \'batch_heatmaps should be numpy.ndarray\'\n    assert isinstance(batch_heatmaps, torch.Tensor), \'batch_heatmaps should be torch.Tensor\'\n    assert len(batch_heatmaps.shape) == 4, \'batch_images should be 4-ndim\'\n\n    batch_size = batch_heatmaps.shape[0]\n    num_joints = batch_heatmaps.shape[1]\n    width = batch_heatmaps.shape[3]\n    heatmaps_reshaped = batch_heatmaps.reshape(batch_size, num_joints, -1)\n    maxvals, idx = torch.max(heatmaps_reshaped, dim=2)\n\n    maxvals = maxvals.unsqueeze(dim=-1)\n    idx = idx.float()\n\n    preds = torch.zeros((batch_size, num_joints, 2)).to(batch_heatmaps.device)\n\n    preds[:, :, 0] = idx % width  # column\n    preds[:, :, 1] = torch.floor(idx / width)  # row\n\n    pred_mask = torch.gt(maxvals, 0.0).repeat(1, 1, 2).float().to(batch_heatmaps.device)\n\n    preds *= pred_mask\n    return preds, maxvals\n\n\ndef get_final_preds(post_processing, batch_heatmaps, center, scale, pixel_std):\n    coords, maxvals = get_max_preds(batch_heatmaps)\n\n    heatmap_height = batch_heatmaps.shape[2]\n    heatmap_width = batch_heatmaps.shape[3]\n\n    # post-processing\n    if post_processing:\n        for n in range(coords.shape[0]):\n            for p in range(coords.shape[1]):\n                hm = batch_heatmaps[n][p]\n                px = int(math.floor(coords[n][p][0] + 0.5))\n                py = int(math.floor(coords[n][p][1] + 0.5))\n                if 1 < px < heatmap_width - 1 and 1 < py < heatmap_height - 1:\n                    diff = torch.tensor(\n                        [\n                            hm[py][px + 1] - hm[py][px - 1],\n                            hm[py + 1][px] - hm[py - 1][px]\n                        ]\n                    ).to(batch_heatmaps.device)\n                    coords[n][p] += torch.sign(diff) * .25\n\n    preds = coords.clone()\n\n    # Transform back\n    for i in range(coords.shape[0]):\n        preds[i] = transform_preds(coords[i], center[i], scale[i], pixel_std, [heatmap_width, heatmap_height])\n\n    return preds, maxvals\n\n\ndef calc_dists(preds, target, normalize):\n    preds = preds.type(torch.float32)\n    target = target.type(torch.float32)\n    dists = torch.zeros((preds.shape[1], preds.shape[0])).to(preds.device)\n    for n in range(preds.shape[0]):\n        for c in range(preds.shape[1]):\n            if target[n, c, 0] > 1 and target[n, c, 1] > 1:\n                normed_preds = preds[n, c, :] / normalize[n]\n                normed_targets = target[n, c, :] / normalize[n]\n                # # dists[c, n] = np.linalg.norm(normed_preds - normed_targets)\n                dists[c, n] = torch.norm(normed_preds - normed_targets)\n            else:\n                dists[c, n] = -1\n    return dists\n\n\ndef dist_acc(dists, thr=0.5):\n    """"""\n    Return percentage below threshold while ignoring values with a -1\n    """"""\n    dist_cal = torch.ne(dists, -1)\n    num_dist_cal = dist_cal.sum()\n    if num_dist_cal > 0:\n        return torch.lt(dists[dist_cal], thr).float().sum() / num_dist_cal\n    else:\n        return -1\n\n\ndef evaluate_pck_accuracy(output, target, hm_type=\'gaussian\', thr=0.5):\n    """"""\n    Calculate accuracy according to PCK,\n    but uses ground truth heatmap rather than y,x locations\n    First value to be returned is average accuracy across \'idxs\',\n    followed by individual accuracies\n    """"""\n    idx = list(range(output.shape[1]))\n    if hm_type == \'gaussian\':\n        pred, _ = get_max_preds(output)\n        target, _ = get_max_preds(target)\n        h = output.shape[2]\n        w = output.shape[3]\n        norm = torch.ones((pred.shape[0], 2)) * torch.tensor([h, w],\n                                                             dtype=torch.float32) / 10  # Why they divide this by 10?\n        norm = norm.to(output.device)\n    else:\n        raise NotImplementedError\n    dists = calc_dists(pred, target, norm)\n\n    acc = torch.zeros(len(idx)).to(dists.device)\n    avg_acc = 0\n    cnt = 0\n\n    for i in range(len(idx)):\n        acc[i] = dist_acc(dists[idx[i]], thr=thr)\n        if acc[i] >= 0:\n            avg_acc = avg_acc + acc[i]\n            cnt += 1\n\n    avg_acc = avg_acc / cnt if cnt != 0 else 0\n    return acc, avg_acc, cnt, pred, target\n#\n#\n\n\n#\n# Operations on bounding boxes (rectangles)\ndef bbox_area(bbox):\n    """"""\n    Area of a bounding box (a rectangles).\n\n    Args:\n        bbox (:class:`np.ndarray`): rectangle in the form (x_min, y_min, x_max, y_max)\n\n    Returns:\n        float: Bounding box area.\n    """"""\n    x1, y1, x2, y2 = bbox\n\n    dx = x2 - x1\n    dy = y2 - y1\n\n    return dx * dy\n\n\ndef bbox_intersection(bbox_a, bbox_b):\n    """"""\n    Intersection between two buonding boxes (two rectangles).\n\n    Args:\n        bbox_a (:class:`np.ndarray`): rectangle in the form (x_min, y_min, x_max, y_max)\n        bbox_b (:class:`np.ndarray`): rectangle in the form (x_min, y_min, x_max, y_max)\n\n    Returns:\n        (:class:`np.ndarray`, float):\n            Intersection limits and area.\n\n            Format: (x_min, y_min, x_max, y_max), area\n    """"""\n    x1 = np.max((bbox_a[0], bbox_b[0]))  # Left\n    x2 = np.min((bbox_a[2], bbox_b[2]))  # Right\n    y1 = np.max((bbox_a[1], bbox_b[1]))  # Top\n    y2 = np.min((bbox_a[3], bbox_b[3]))  # Bottom\n\n    if x2 < x1 or y2 < y1:\n        bbox_i = np.asarray([0, 0, 0, 0])\n        area_i = 0\n    else:\n        bbox_i = np.asarray([x1, y1, x2, y2], dtype=bbox_a.dtype)\n        area_i = bbox_area(bbox_i)\n\n    return bbox_i, area_i\n\n\ndef bbox_union(bbox_a, bbox_b):\n    """"""\n    Union between two buonding boxes (two rectangles).\n\n    Args:\n        bbox_a (:class:`np.ndarray`): rectangle in the form (x_min, y_min, x_max, y_max)\n        bbox_b (:class:`np.ndarray`): rectangle in the form (x_min, y_min, x_max, y_max)\n\n    Returns:\n        float: Union.\n    """"""\n    area_a = bbox_area(bbox_a)\n    area_b = bbox_area(bbox_b)\n\n    bbox_i, area_i = bbox_intersection(bbox_a, bbox_b)\n    area_u = area_a + area_b - area_i\n\n    return area_u\n\n\ndef bbox_iou(bbox_a, bbox_b):\n    """"""\n    Intersection over Union (IoU) between two buonding boxes (two rectangles).\n\n    Args:\n        bbox_a (:class:`np.ndarray`): rectangle in the form (x_min, y_min, x_max, y_max)\n        bbox_b (:class:`np.ndarray`): rectangle in the form (x_min, y_min, x_max, y_max)\n\n    Returns:\n        float: Intersection over Union (IoU).\n    """"""\n    area_u = bbox_union(bbox_a, bbox_b)\n    bbox_i, area_i = bbox_intersection(bbox_a, bbox_b)\n\n    iou = area_i / area_u\n\n    return iou\n#\n#\n\n\n#\n# Bounding box/pose similarity and association\ndef oks_iou(g, d, a_g, a_d, sigmas=None, in_vis_thre=None):\n    if not isinstance(sigmas, np.ndarray):\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62, .62, 1.07, 1.07, .87, .87, .89, .89])/10.0\n    vars = (sigmas * 2) ** 2\n    yg = g[:, 0]\n    xg = g[:, 1]\n    vg = g[:, 2]\n    ious = np.zeros((d.shape[0]))\n    for n_d in range(0, d.shape[0]):\n        yd = d[n_d, :, 0]\n        xd = d[n_d, :, 1]\n        vd = d[n_d, :, 2]\n        dx = xd - xg\n        dy = yd - yg\n        e = (dx ** 2 + dy ** 2) / vars / ((a_g + a_d[n_d]) / 2 + np.spacing(1)) / 2\n        if in_vis_thre is not None:\n            ind = list(vg > in_vis_thre) and list(vd > in_vis_thre)\n            e = e[ind]\n        ious[n_d] = np.sum(np.exp(-e)) / e.shape[0] if e.shape[0] != 0 else 0.0\n    return ious\n\n\ndef compute_similarity_matrices(bboxes_a, bboxes_b, poses_a, poses_b):\n    assert len(bboxes_a) == len(poses_a) and len(bboxes_b) == len(poses_b)\n\n    result_bbox = np.zeros((len(bboxes_a), len(bboxes_b)), dtype=np.float32)\n    result_pose = np.zeros((len(poses_a), len(poses_b)), dtype=np.float32)\n\n    for i, (bbox_a, pose_a) in enumerate(zip(bboxes_a, poses_a)):\n        area_bboxes_b = np.asarray([bbox_area(bbox_b) for bbox_b in bboxes_b])\n        result_pose[i, :] = oks_iou(pose_a, poses_b, bbox_area(bbox_a), area_bboxes_b)\n        for j, (bbox_b, pose_b) in enumerate(zip(bboxes_b, poses_b)):\n            result_bbox[i, j] = bbox_iou(bbox_a, bbox_b)\n\n    return result_bbox, result_pose\n\n\ndef find_person_id_associations(boxes, pts, prev_boxes, prev_pts, prev_person_ids, next_person_id=0,\n                                pose_alpha=0.5, similarity_threshold=0.5, smoothing_alpha=0.):\n    """"""\n    Find associations between previous and current skeletons and apply temporal smoothing.\n    It requires previous and current bounding boxes, skeletons, and previous person_ids.\n\n    Args:\n        boxes (:class:`np.ndarray`): current person bounding boxes\n        pts (:class:`np.ndarray`): current human joints\n        prev_boxes (:class:`np.ndarray`): previous person bounding boxes\n        prev_pts (:class:`np.ndarray`): previous human joints\n        prev_person_ids (:class:`np.ndarray`): previous person ids\n        next_person_id (int): the id that will be assigned to the next novel detected person\n            Default: 0\n        pose_alpha (float): parameter to weight between bounding box similarity and pose (oks) similarity.\n            pose_alpha * pose_similarity + (1 - pose_alpha) * bbox_similarity\n            Default: 0.5\n        similarity_threshold (float): lower similarity threshold to have a correct match between previous and\n            current detections.\n            Default: 0.5\n        smoothing_alpha (float): linear temporal smoothing filter. Set 0 to disable, 1 to keep the previous detection.\n            Default: 0.1\n\n    Returns:\n            (:class:`np.ndarray`, :class:`np.ndarray`, :class:`np.ndarray`):\n                A list with (boxes, pts, person_ids) where boxes and pts are temporally smoothed.\n    """"""\n    bbox_similarity_matrix, pose_similarity_matrix = compute_similarity_matrices(boxes, prev_boxes, pts, prev_pts)\n    similarity_matrix = pose_similarity_matrix * pose_alpha + bbox_similarity_matrix * (1 - pose_alpha)\n\n    m = munkres.Munkres()\n    assignments = np.asarray(m.compute((1 - similarity_matrix).tolist()))  # Munkres require a cost => 1 - similarity\n\n    person_ids = np.ones(len(pts), dtype=np.int32) * -1\n    for assignment in assignments:\n        if similarity_matrix[assignment[0], assignment[1]] > similarity_threshold:\n            person_ids[assignment[0]] = prev_person_ids[assignment[1]]\n            if smoothing_alpha:\n                boxes[assignment[0]] = (1 - smoothing_alpha) * boxes[assignment[0]] + smoothing_alpha * prev_boxes[assignment[1]]\n                pts[assignment[0]] = (1 - smoothing_alpha) * pts[assignment[0]] + smoothing_alpha * prev_pts[assignment[1]]\n\n    person_ids[person_ids == -1] = np.arange(next_person_id, next_person_id + np.sum(person_ids == -1))\n\n    return boxes, pts, person_ids\n#\n#\n'"
misc/visualization.py,8,"b'import cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision\nimport ffmpeg\n\n\ndef joints_dict():\n    joints = {\n        ""coco"": {\n            ""keypoints"": {\n                0: ""nose"",\n                1: ""left_eye"",\n                2: ""right_eye"",\n                3: ""left_ear"",\n                4: ""right_ear"",\n                5: ""left_shoulder"",\n                6: ""right_shoulder"",\n                7: ""left_elbow"",\n                8: ""right_elbow"",\n                9: ""left_wrist"",\n                10: ""right_wrist"",\n                11: ""left_hip"",\n                12: ""right_hip"",\n                13: ""left_knee"",\n                14: ""right_knee"",\n                15: ""left_ankle"",\n                16: ""right_ankle""\n            },\n            ""skeleton"": [\n                # # [16, 14], [14, 12], [17, 15], [15, 13], [12, 13], [6, 12], [7, 13], [6, 7], [6, 8],\n                # # [7, 9], [8, 10], [9, 11], [2, 3], [1, 2], [1, 3], [2, 4], [3, 5], [4, 6], [5, 7]\n                # [15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12], [5, 6], [5, 7],\n                # [6, 8], [7, 9], [8, 10], [1, 2], [0, 1], [0, 2], [1, 3], [2, 4], [3, 5], [4, 6]\n                [15, 13], [13, 11], [16, 14], [14, 12], [11, 12], [5, 11], [6, 12], [5, 6], [5, 7],\n                [6, 8], [7, 9], [8, 10], [1, 2], [0, 1], [0, 2], [1, 3], [2, 4],  # [3, 5], [4, 6]\n                [0, 5], [0, 6]\n            ]\n        },\n        ""mpii"": {\n            ""keypoints"": {\n                0: ""right_ankle"",\n                1: ""right_knee"",\n                2: ""right_hip"",\n                3: ""left_hip"",\n                4: ""left_knee"",\n                5: ""left_ankle"",\n                6: ""pelvis"",\n                7: ""thorax"",\n                8: ""upper_neck"",\n                9: ""head top"",\n                10: ""right_wrist"",\n                11: ""right_elbow"",\n                12: ""right_shoulder"",\n                13: ""left_shoulder"",\n                14: ""left_elbow"",\n                15: ""left_wrist""\n            },\n            ""skeleton"": [\n                # [5, 4], [4, 3], [0, 1], [1, 2], [3, 2], [13, 3], [12, 2], [13, 12], [13, 14],\n                # [12, 11], [14, 15], [11, 10], # [2, 3], [1, 2], [1, 3], [2, 4], [3, 5], [4, 6], [5, 7]\n                [5, 4], [4, 3], [0, 1], [1, 2], [3, 2], [3, 6], [2, 6], [6, 7], [7, 8], [8, 9],\n                [13, 7], [12, 7], [13, 14], [12, 11], [14, 15], [11, 10],\n            ]\n        },\n    }\n    return joints\n\n\ndef draw_points(image, points, color_palette=\'tab20\', palette_samples=16):\n    """"""\n    Draws `points` on `image`.\n\n    Args:\n        image: image in opencv format\n        points: list of points to be drawn.\n            Shape: (nof_points, 3)\n            Format: each point should contain (y, x, confidence)\n        color_palette: name of a matplotlib color palette\n            Default: \'tab20\'\n        palette_samples: number of different colors sampled from the `color_palette`\n            Default: 16\n\n    Returns:\n        A new image with overlaid points\n\n    """"""\n    try:\n        colors = np.round(\n            np.array(plt.get_cmap(color_palette).colors) * 255\n        ).astype(np.uint8)[:, ::-1].tolist()\n    except AttributeError:  # if palette has not pre-defined colors\n        colors = np.round(\n            np.array(plt.get_cmap(color_palette)(np.linspace(0, 1, palette_samples))) * 255\n        ).astype(np.uint8)[:, -2::-1].tolist()\n\n    circle_size = max(1, min(image.shape[:2]) // 160)  # ToDo Shape it taking into account the size of the detection\n    # circle_size = max(2, int(np.sqrt(np.max(np.max(points, axis=0) - np.min(points, axis=0)) // 16)))\n\n    for i, pt in enumerate(points):\n        if pt[2] > 0.5:\n            image = cv2.circle(image, (int(pt[1]), int(pt[0])), circle_size, tuple(colors[i % len(colors)]), -1)\n\n    return image\n\n\ndef draw_skeleton(image, points, skeleton, color_palette=\'Set2\', palette_samples=8, person_index=0):\n    """"""\n    Draws a `skeleton` on `image`.\n\n    Args:\n        image: image in opencv format\n        points: list of points to be drawn.\n            Shape: (nof_points, 3)\n            Format: each point should contain (y, x, confidence)\n        skeleton: list of joints to be drawn\n            Shape: (nof_joints, 2)\n            Format: each joint should contain (point_a, point_b) where `point_a` and `point_b` are an index in `points`\n        color_palette: name of a matplotlib color palette\n            Default: \'Set2\'\n        palette_samples: number of different colors sampled from the `color_palette`\n            Default: 8\n        person_index: index of the person in `image`\n            Default: 0\n\n    Returns:\n        A new image with overlaid joints\n\n    """"""\n    try:\n        colors = np.round(\n            np.array(plt.get_cmap(color_palette).colors) * 255\n        ).astype(np.uint8)[:, ::-1].tolist()\n    except AttributeError:  # if palette has not pre-defined colors\n        colors = np.round(\n            np.array(plt.get_cmap(color_palette)(np.linspace(0, 1, palette_samples))) * 255\n        ).astype(np.uint8)[:, -2::-1].tolist()\n\n    for i, joint in enumerate(skeleton):\n        pt1, pt2 = points[joint]\n        if pt1[2] > 0.5 and pt2[2] > 0.5:\n            image = cv2.line(\n                image, (int(pt1[1]), int(pt1[0])), (int(pt2[1]), int(pt2[0])),\n                tuple(colors[person_index % len(colors)]), 2\n            )\n\n    return image\n\n\ndef draw_points_and_skeleton(image, points, skeleton, points_color_palette=\'tab20\', points_palette_samples=16,\n                             skeleton_color_palette=\'Set2\', skeleton_palette_samples=8, person_index=0):\n    """"""\n    Draws `points` and `skeleton` on `image`.\n\n    Args:\n        image: image in opencv format\n        points: list of points to be drawn.\n            Shape: (nof_points, 3)\n            Format: each point should contain (y, x, confidence)\n        skeleton: list of joints to be drawn\n            Shape: (nof_joints, 2)\n            Format: each joint should contain (point_a, point_b) where `point_a` and `point_b` are an index in `points`\n        points_color_palette: name of a matplotlib color palette\n            Default: \'tab20\'\n        points_palette_samples: number of different colors sampled from the `color_palette`\n            Default: 16\n        skeleton_color_palette: name of a matplotlib color palette\n            Default: \'Set2\'\n        skeleton_palette_samples: number of different colors sampled from the `color_palette`\n            Default: 8\n        person_index: index of the person in `image`\n            Default: 0\n\n    Returns:\n        A new image with overlaid joints\n\n    """"""\n    image = draw_skeleton(image, points, skeleton, color_palette=skeleton_color_palette,\n                          palette_samples=skeleton_palette_samples, person_index=person_index)\n    image = draw_points(image, points, color_palette=points_color_palette, palette_samples=points_palette_samples)\n    return image\n\n\ndef save_images(images, target, joint_target, output, joint_output, joint_visibility, summary_writer=None, step=0,\n                prefix=\'\'):\n    """"""\n    Creates a grid of images with gt joints and a grid with predicted joints.\n    This is a basic function for debugging purposes only.\n\n    If summary_writer is not None, the grid will be written in that SummaryWriter with name ""{prefix}_images"" and\n    ""{prefix}_predictions"".\n\n    Args:\n        images (torch.Tensor): a tensor of images with shape (batch x channels x height x width).\n        target (torch.Tensor): a tensor of gt heatmaps with shape (batch x channels x height x width).\n        joint_target (torch.Tensor): a tensor of gt joints with shape (batch x joints x 2).\n        output (torch.Tensor): a tensor of predicted heatmaps with shape (batch x channels x height x width).\n        joint_output (torch.Tensor): a tensor of predicted joints with shape (batch x joints x 2).\n        joint_visibility (torch.Tensor): a tensor of joint visibility with shape (batch x joints).\n        summary_writer (tb.SummaryWriter): a SummaryWriter where write the grids.\n            Default: None\n        step (int): summary_writer step.\n            Default: 0\n        prefix (str): summary_writer name prefix.\n            Default: """"\n\n    Returns:\n        A pair of images which are built from torchvision.utils.make_grid\n    """"""\n    # Input images with gt\n    images_ok = images.detach().clone()\n    images_ok[:, 0].mul_(0.229).add_(0.485)\n    images_ok[:, 1].mul_(0.224).add_(0.456)\n    images_ok[:, 2].mul_(0.225).add_(0.406)\n    for i in range(images.shape[0]):\n        joints = joint_target[i] * 4.\n        joints_vis = joint_visibility[i]\n\n        for joint, joint_vis in zip(joints, joints_vis):\n            if joint_vis[0]:\n                a = int(joint[1].item())\n                b = int(joint[0].item())\n                # images_ok[i][:, a-1:a+1, b-1:b+1] = torch.tensor([1, 0, 0])\n                images_ok[i][0, a - 1:a + 1, b - 1:b + 1] = 1\n                images_ok[i][1:, a - 1:a + 1, b - 1:b + 1] = 0\n    grid_gt = torchvision.utils.make_grid(images_ok, nrow=int(images_ok.shape[0] ** 0.5), padding=2, normalize=False)\n    if summary_writer is not None:\n        summary_writer.add_image(prefix + \'images\', grid_gt, global_step=step)\n\n    # Input images with prediction\n    images_ok = images.detach().clone()\n    images_ok[:, 0].mul_(0.229).add_(0.485)\n    images_ok[:, 1].mul_(0.224).add_(0.456)\n    images_ok[:, 2].mul_(0.225).add_(0.406)\n    for i in range(images.shape[0]):\n        joints = joint_output[i] * 4.\n        joints_vis = joint_visibility[i]\n\n        for joint, joint_vis in zip(joints, joints_vis):\n            if joint_vis[0]:\n                a = int(joint[1].item())\n                b = int(joint[0].item())\n                # images_ok[i][:, a-1:a+1, b-1:b+1] = torch.tensor([1, 0, 0])\n                images_ok[i][0, a - 1:a + 1, b - 1:b + 1] = 1\n                images_ok[i][1:, a - 1:a + 1, b - 1:b + 1] = 0\n    grid_pred = torchvision.utils.make_grid(images_ok, nrow=int(images_ok.shape[0] ** 0.5), padding=2, normalize=False)\n    if summary_writer is not None:\n        summary_writer.add_image(prefix + \'predictions\', grid_pred, global_step=step)\n\n    # Heatmaps\n    # ToDo\n    # for h in range(0,17):\n    #     heatmap = torchvision.utils.make_grid(output[h].detach(), nrow=int(np.sqrt(output.shape[0])),\n    #                                            padding=2, normalize=True, range=(0, 1))\n    #     summary_writer.add_image(\'train_heatmap_%d\' % h, heatmap, global_step=step + epoch*len_dl_train)\n\n    return grid_gt, grid_pred\n\n\ndef check_video_rotation(filename):\n    # thanks to\n    # https://stackoverflow.com/questions/53097092/frame-from-video-is-upside-down-after-extracting/55747773#55747773\n\n    # this returns meta-data of the video file in form of a dictionary\n    meta_dict = ffmpeg.probe(filename)\n\n    # from the dictionary, meta_dict[\'streams\'][0][\'tags\'][\'rotate\'] is the key\n    # we are looking for\n    rotation_code = None\n    try:\n        if int(meta_dict[\'streams\'][0][\'tags\'][\'rotate\']) == 90:\n            rotation_code = cv2.ROTATE_90_CLOCKWISE\n        elif int(meta_dict[\'streams\'][0][\'tags\'][\'rotate\']) == 180:\n            rotation_code = cv2.ROTATE_180\n        elif int(meta_dict[\'streams\'][0][\'tags\'][\'rotate\']) == 270:\n            rotation_code = cv2.ROTATE_90_COUNTERCLOCKWISE\n        else:\n            raise ValueError\n    except KeyError:\n        pass\n\n    return rotation_code\n'"
models/__init__.py,0,b''
models/hrnet.py,8,"b'import torch\nfrom torch import nn\nfrom models.modules import BasicBlock, Bottleneck\n\n\nclass StageModule(nn.Module):\n    def __init__(self, stage, output_branches, c, bn_momentum):\n        super(StageModule, self).__init__()\n        self.stage = stage\n        self.output_branches = output_branches\n\n        self.branches = nn.ModuleList()\n        for i in range(self.stage):\n            w = c * (2 ** i)\n            branch = nn.Sequential(\n                BasicBlock(w, w, bn_momentum=bn_momentum),\n                BasicBlock(w, w, bn_momentum=bn_momentum),\n                BasicBlock(w, w, bn_momentum=bn_momentum),\n                BasicBlock(w, w, bn_momentum=bn_momentum),\n            )\n            self.branches.append(branch)\n\n        self.fuse_layers = nn.ModuleList()\n        # for each output_branches (i.e. each branch in all cases but the very last one)\n        for i in range(self.output_branches):\n            self.fuse_layers.append(nn.ModuleList())\n            for j in range(self.stage):  # for each branch\n                if i == j:\n                    self.fuse_layers[-1].append(nn.Sequential())  # Used in place of ""None"" because it is callable\n                elif i < j:\n                    self.fuse_layers[-1].append(nn.Sequential(\n                        nn.Conv2d(c * (2 ** j), c * (2 ** i), kernel_size=(1, 1), stride=(1, 1), bias=False),\n                        nn.BatchNorm2d(c * (2 ** i), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n                        nn.Upsample(scale_factor=(2.0 ** (j - i)), mode=\'nearest\'),\n                    ))\n                elif i > j:\n                    ops = []\n                    for k in range(i - j - 1):\n                        ops.append(nn.Sequential(\n                            nn.Conv2d(c * (2 ** j), c * (2 ** j), kernel_size=(3, 3), stride=(2, 2), padding=(1, 1),\n                                      bias=False),\n                            nn.BatchNorm2d(c * (2 ** j), eps=1e-05, momentum=0.1, affine=True,\n                                           track_running_stats=True),\n                            nn.ReLU(inplace=True),\n                        ))\n                    ops.append(nn.Sequential(\n                        nn.Conv2d(c * (2 ** j), c * (2 ** i), kernel_size=(3, 3), stride=(2, 2), padding=(1, 1),\n                                  bias=False),\n                        nn.BatchNorm2d(c * (2 ** i), eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n                    ))\n                    self.fuse_layers[-1].append(nn.Sequential(*ops))\n\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        assert len(self.branches) == len(x)\n\n        x = [branch(b) for branch, b in zip(self.branches, x)]\n\n        x_fused = []\n        for i in range(len(self.fuse_layers)):\n            for j in range(0, len(self.branches)):\n                if j == 0:\n                    x_fused.append(self.fuse_layers[i][0](x[0]))\n                else:\n                    x_fused[i] = x_fused[i] + self.fuse_layers[i][j](x[j])\n\n        for i in range(len(x_fused)):\n            x_fused[i] = self.relu(x_fused[i])\n\n        return x_fused\n\n\nclass HRNet(nn.Module):\n    def __init__(self, c=48, nof_joints=17, bn_momentum=0.1):\n        super(HRNet, self).__init__()\n\n        # Input (stem net)\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        self.bn1 = nn.BatchNorm2d(64, eps=1e-05, momentum=bn_momentum, affine=True, track_running_stats=True)\n        self.conv2 = nn.Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        self.bn2 = nn.BatchNorm2d(64, eps=1e-05, momentum=bn_momentum, affine=True, track_running_stats=True)\n        self.relu = nn.ReLU(inplace=True)\n\n        # Stage 1 (layer1)      - First group of bottleneck (resnet) modules\n        downsample = nn.Sequential(\n            nn.Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n            nn.BatchNorm2d(256, eps=1e-05, momentum=bn_momentum, affine=True, track_running_stats=True),\n        )\n        self.layer1 = nn.Sequential(\n            Bottleneck(64, 64, downsample=downsample),\n            Bottleneck(256, 64),\n            Bottleneck(256, 64),\n            Bottleneck(256, 64),\n        )\n\n        # Fusion layer 1 (transition1)      - Creation of the first two branches (one full and one half resolution)\n        self.transition1 = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(256, c, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n                nn.BatchNorm2d(c, eps=1e-05, momentum=bn_momentum, affine=True, track_running_stats=True),\n                nn.ReLU(inplace=True),\n            ),\n            nn.Sequential(nn.Sequential(  # Double Sequential to fit with official pretrained weights\n                nn.Conv2d(256, c * (2 ** 1), kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n                nn.BatchNorm2d(c * (2 ** 1), eps=1e-05, momentum=bn_momentum, affine=True, track_running_stats=True),\n                nn.ReLU(inplace=True),\n            )),\n        ])\n\n        # Stage 2 (stage2)      - Second module with 1 group of bottleneck (resnet) modules. This has 2 branches\n        self.stage2 = nn.Sequential(\n            StageModule(stage=2, output_branches=2, c=c, bn_momentum=bn_momentum),\n        )\n\n        # Fusion layer 2 (transition2)      - Creation of the third branch (1/4 resolution)\n        self.transition2 = nn.ModuleList([\n            nn.Sequential(),  # None,   - Used in place of ""None"" because it is callable\n            nn.Sequential(),  # None,   - Used in place of ""None"" because it is callable\n            nn.Sequential(nn.Sequential(  # Double Sequential to fit with official pretrained weights\n                nn.Conv2d(c * (2 ** 1), c * (2 ** 2), kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n                nn.BatchNorm2d(c * (2 ** 2), eps=1e-05, momentum=bn_momentum, affine=True, track_running_stats=True),\n                nn.ReLU(inplace=True),\n            )),  # ToDo Why the new branch derives from the ""upper"" branch only?\n        ])\n\n        # Stage 3 (stage3)      - Third module with 4 groups of bottleneck (resnet) modules. This has 3 branches\n        self.stage3 = nn.Sequential(\n            StageModule(stage=3, output_branches=3, c=c, bn_momentum=bn_momentum),\n            StageModule(stage=3, output_branches=3, c=c, bn_momentum=bn_momentum),\n            StageModule(stage=3, output_branches=3, c=c, bn_momentum=bn_momentum),\n            StageModule(stage=3, output_branches=3, c=c, bn_momentum=bn_momentum),\n        )\n\n        # Fusion layer 3 (transition3)      - Creation of the fourth branch (1/8 resolution)\n        self.transition3 = nn.ModuleList([\n            nn.Sequential(),  # None,   - Used in place of ""None"" because it is callable\n            nn.Sequential(),  # None,   - Used in place of ""None"" because it is callable\n            nn.Sequential(),  # None,   - Used in place of ""None"" because it is callable\n            nn.Sequential(nn.Sequential(  # Double Sequential to fit with official pretrained weights\n                nn.Conv2d(c * (2 ** 2), c * (2 ** 3), kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False),\n                nn.BatchNorm2d(c * (2 ** 3), eps=1e-05, momentum=bn_momentum, affine=True, track_running_stats=True),\n                nn.ReLU(inplace=True),\n            )),  # ToDo Why the new branch derives from the ""upper"" branch only?\n        ])\n\n        # Stage 4 (stage4)      - Fourth module with 3 groups of bottleneck (resnet) modules. This has 4 branches\n        self.stage4 = nn.Sequential(\n            StageModule(stage=4, output_branches=4, c=c, bn_momentum=bn_momentum),\n            StageModule(stage=4, output_branches=4, c=c, bn_momentum=bn_momentum),\n            StageModule(stage=4, output_branches=1, c=c, bn_momentum=bn_momentum),\n        )\n\n        # Final layer (final_layer)\n        self.final_layer = nn.Conv2d(c, nof_joints, kernel_size=(1, 1), stride=(1, 1))\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = [trans(x) for trans in self.transition1]  # Since now, x is a list (# == nof branches)\n\n        x = self.stage2(x)\n        # x = [trans(x[-1]) for trans in self.transition2]    # New branch derives from the ""upper"" branch only\n        x = [\n            self.transition2[0](x[0]),\n            self.transition2[1](x[1]),\n            self.transition2[2](x[-1])\n        ]  # New branch derives from the ""upper"" branch only\n\n        x = self.stage3(x)\n        # x = [trans(x) for trans in self.transition3]    # New branch derives from the ""upper"" branch only\n        x = [\n            self.transition3[0](x[0]),\n            self.transition3[1](x[1]),\n            self.transition3[2](x[2]),\n            self.transition3[3](x[-1])\n        ]  # New branch derives from the ""upper"" branch only\n\n        x = self.stage4(x)\n\n        x = self.final_layer(x[0])\n\n        return x\n\n\nif __name__ == \'__main__\':\n    # model = HRNet(48, 17, 0.1)\n    model = HRNet(32, 17, 0.1)\n\n    # print(model)\n\n    model.load_state_dict(\n        # torch.load(\'./weights/pose_hrnet_w48_384x288.pth\')\n        torch.load(\'./weights/pose_hrnet_w32_256x192.pth\')\n    )\n    print(\'ok!!\')\n\n    if torch.cuda.is_available() and False:\n        torch.backends.cudnn.deterministic = True\n        device = torch.device(\'cuda:0\')\n    else:\n        device = torch.device(\'cpu\')\n\n    print(device)\n\n    model = model.to(device)\n\n    y = model(torch.ones(1, 3, 384, 288).to(device))\n    print(y.shape)\n    print(torch.min(y).item(), torch.mean(y).item(), torch.max(y).item())\n'"
models/modules.py,0,"b'import torch\nfrom torch import nn\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, bn_momentum=0.1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion, momentum=bn_momentum)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, bn_momentum=0.1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=bn_momentum)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n'"
models/poseresnet.py,7,"b""import torch\nfrom torch import nn\nfrom models.modules import BasicBlock, Bottleneck\n\n\nresnet_spec = {\n    18: (BasicBlock, [2, 2, 2, 2]),\n    34: (BasicBlock, [3, 4, 6, 3]),\n    50: (Bottleneck, [3, 4, 6, 3]),\n    101: (Bottleneck, [3, 4, 23, 3]),\n    152: (Bottleneck, [3, 8, 36, 3])\n}\n\n\n# derived from https://github.com/leoxiaobin/deep-high-resolution-net.pytorch\nclass PoseResNet(nn.Module):\n    def __init__(self, resnet_size=50, nof_joints=17, bn_momentum=0.1):\n        super(PoseResNet, self).__init__()\n\n        assert resnet_size in resnet_spec.keys()\n        block, layers = resnet_spec[resnet_size]\n\n        self.inplanes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64, momentum=bn_momentum)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0], bn_momentum=bn_momentum)\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, bn_momentum=bn_momentum)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, bn_momentum=bn_momentum)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, bn_momentum=bn_momentum)\n\n        # used for deconv layers\n        self.deconv_with_bias = False\n        self.deconv_layers = self._make_deconv_layer(\n            3,\n            [256, 256, 256],\n            [4, 4, 4],\n            bn_momentum=bn_momentum\n        )\n\n        self.final_layer = nn.Conv2d(\n            in_channels=256,\n            out_channels=nof_joints,\n            kernel_size=1,\n            stride=1,\n            padding=0\n        )\n\n    def _make_layer(self, block, planes, blocks, stride=1, bn_momentum=0.1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion, momentum=bn_momentum),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _get_deconv_cfg(self, deconv_kernel, index):\n        if deconv_kernel == 4:\n            padding = 1\n            output_padding = 0\n        elif deconv_kernel == 3:\n            padding = 1\n            output_padding = 1\n        elif deconv_kernel == 2:\n            padding = 0\n            output_padding = 0\n\n        return deconv_kernel, padding, output_padding\n\n    def _make_deconv_layer(self, num_layers, num_filters, num_kernels, bn_momentum=0.1):\n        assert num_layers == len(num_filters), \\\n            'ERROR: num_deconv_layers is different len(num_deconv_filters)'\n        assert num_layers == len(num_kernels), \\\n            'ERROR: num_deconv_layers is different len(num_deconv_filters)'\n\n        layers = []\n        for i in range(num_layers):\n            kernel, padding, output_padding = \\\n                self._get_deconv_cfg(num_kernels[i], i)\n\n            planes = num_filters[i]\n            layers.append(\n                nn.ConvTranspose2d(\n                    in_channels=self.inplanes,\n                    out_channels=planes,\n                    kernel_size=kernel,\n                    stride=2,\n                    padding=padding,\n                    output_padding=output_padding,\n                    bias=self.deconv_with_bias))\n            layers.append(nn.BatchNorm2d(planes, momentum=bn_momentum))\n            layers.append(nn.ReLU(inplace=True))\n            self.inplanes = planes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.deconv_layers(x)\n        x = self.final_layer(x)\n\n        return x\n\n\nif __name__ == '__main__':\n    model = PoseResNet(50, 17, 0.1)\n\n    # print(model)\n\n    model.load_state_dict(\n        torch.load('./weights/pose_resnet_50_256x192.pth')\n    )\n    print('ok!!')\n\n    if torch.cuda.is_available():\n        torch.backends.cudnn.deterministic = True\n        device = torch.device('cuda:0')\n    else:\n        device = torch.device('cpu')\n\n    print(device)\n\n    model = model.to(device)\n\n    y = model(torch.ones(1, 3, 256, 192).to(device))\n    print(y.shape)\n    print(torch.min(y).item(), torch.mean(y).item(), torch.max(y).item())\n"""
scripts/extract-keypoints.py,5,"b'import os\nimport sys\nimport argparse\nimport ast\nimport csv\nimport cv2\nimport time\nimport torch\n\nsys.path.insert(1, os.getcwd())\nfrom SimpleHRNet import SimpleHRNet\nfrom misc.visualization import check_video_rotation\n\n\ndef main(filename, hrnet_m, hrnet_c, hrnet_j, hrnet_weights, image_resolution, single_person, use_tiny_yolo,\n         max_batch_size, csv_output_filename, csv_delimiter, device):\n    if device is not None:\n        device = torch.device(device)\n    else:\n        if torch.cuda.is_available():\n            torch.backends.cudnn.deterministic = True\n            device = torch.device(\'cuda\')\n        else:\n            device = torch.device(\'cpu\')\n\n    # print(device)\n\n    image_resolution = ast.literal_eval(image_resolution)\n\n    rotation_code = check_video_rotation(filename)\n    video = cv2.VideoCapture(filename)\n    assert video.isOpened()\n    nof_frames = video.get(cv2.CAP_PROP_FRAME_COUNT)\n\n    assert csv_output_filename.endswith(\'.csv\')\n    with open(csv_output_filename, \'wt\', newline=\'\') as fd:\n        csv_output = csv.writer(fd, delimiter=csv_delimiter)\n\n        if use_tiny_yolo:\n             yolo_model_def=""./models/detectors/yolo/config/yolov3-tiny.cfg""\n             yolo_class_path=""./models/detectors/yolo/data/coco.names""\n             yolo_weights_path=""./models/detectors/yolo/weights/yolov3-tiny.weights""\n        else:\n             yolo_model_def=""./models/detectors/yolo/config/yolov3.cfg""\n             yolo_class_path=""./models/detectors/yolo/data/coco.names""\n             yolo_weights_path=""./models/detectors/yolo/weights/yolov3.weights""\n\n        model = SimpleHRNet(\n            hrnet_c,\n            hrnet_j,\n            hrnet_weights,\n            model_name=hrnet_m,\n            resolution=image_resolution,\n            multiperson=not single_person,\n            max_batch_size=max_batch_size,\n            yolo_model_def=yolo_model_def,\n            yolo_class_path=yolo_class_path,\n            yolo_weights_path=yolo_weights_path,\n            device=device\n        )\n\n        index = 0\n        while True:\n            t = time.time()\n\n            ret, frame = video.read()\n            if not ret:\n                break\n            if rotation_code is not None:\n                frame = cv2.rotate(frame, rotation_code)\n\n            pts = model.predict(frame)\n\n            # csv format is:\n            #   frame_index,detection_index,<point 0>,<point 1>,...,<point hrnet_j>\n            # where each <point N> corresponds to three elements:\n            #   y_coordinate,x_coordinate,confidence\n            for j, pt in enumerate(pts):\n                row = [index, j] + pt.flatten().tolist()\n                csv_output.writerow(row)\n\n            fps = 1. / (time.time() - t)\n            print(\'\\rframe: % 4d / %d - framerate: %f fps \' % (index, nof_frames - 1, fps), end=\'\')\n\n            index += 1\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(\n        description=\'csv format is:\\n\'\n                    \'  frame_index,detection_index,<point 0>,<point 1>,...,<point hrnet_j>\\n\'\n                    \'where each <point N> corresponds to three elements:\\n\'\n                    \'  y_coordinate,x_coordinate,confidence\')\n    parser.add_argument(""--filename"", ""-f"", help=""open the specified video"",\n                        type=str, default=None)\n    parser.add_argument(""--hrnet_m"", ""-m"", help=""network model - HRNet or PoseResNet"", type=str, default=\'HRNet\')\n    parser.add_argument(""--hrnet_c"", ""-c"", help=""hrnet parameters - number of channels (if model is HRNet), ""\n                                                ""resnet size (if model is PoseResNet)"", type=int, default=48)\n    parser.add_argument(""--hrnet_j"", ""-j"", help=""hrnet parameters - number of joints"", type=int, default=17)\n    parser.add_argument(""--hrnet_weights"", ""-w"", help=""hrnet parameters - path to the pretrained weights"",\n                        type=str, default=""./weights/pose_hrnet_w48_384x288.pth"")\n    parser.add_argument(""--image_resolution"", ""-r"", help=""image resolution"", type=str, default=\'(384, 288)\')\n    parser.add_argument(""--single_person"",\n                        help=""disable the multiperson detection (YOLOv3 or an equivalen detector is required for""\n                             ""multiperson detection)"",\n                        action=""store_true"")\n    parser.add_argument(""--use_tiny_yolo"",\n                        help=""Use YOLOv3-tiny in place of YOLOv3 (faster person detection). Ignored if --single_person"",\n                        action=""store_true"")\n    parser.add_argument(""--max_batch_size"", help=""maximum batch size used for inference"", type=int, default=16)\n    parser.add_argument(""--csv_output_filename"", help=""filename of the csv that will be written."", type=str,\n                        default=\'output.csv\')\n    parser.add_argument(""--csv_delimiter"", help=""csv delimiter"", type=str, default=\',\')\n    parser.add_argument(""--device"", help=""device to be used (default: cuda, if available).""\n                                         ""Set to `cuda` to use all available GPUs (default); ""\n                                         ""set to `cuda:IDS` to use one or more specific GPUs ""\n                                         ""(e.g. `cuda:0` `cuda:1,2`); ""\n                                         ""set to `cpu` to run on cpu."", type=str, default=None)\n    args = parser.parse_args()\n    main(**args.__dict__)\n'"
scripts/live-demo.py,5,"b'import os\nimport sys\nimport argparse\nimport ast\nimport cv2\nimport time\nimport torch\nfrom vidgear.gears import CamGear\nimport numpy as np\n\nsys.path.insert(1, os.getcwd())\nfrom SimpleHRNet import SimpleHRNet\nfrom misc.visualization import draw_points, draw_skeleton, draw_points_and_skeleton, joints_dict, check_video_rotation\nfrom misc.utils import find_person_id_associations\n\ndef main(camera_id, filename, hrnet_m, hrnet_c, hrnet_j, hrnet_weights, hrnet_joints_set, image_resolution,\n         single_person, use_tiny_yolo, disable_tracking, max_batch_size, disable_vidgear, save_video, video_format,\n         video_framerate, device):\n    if device is not None:\n        device = torch.device(device)\n    else:\n        if torch.cuda.is_available():\n            torch.backends.cudnn.deterministic = True\n            device = torch.device(\'cuda\')\n        else:\n            device = torch.device(\'cpu\')\n\n    # print(device)\n\n    image_resolution = ast.literal_eval(image_resolution)\n    has_display = \'DISPLAY\' in os.environ.keys() or sys.platform == \'win32\'\n    video_writer = None\n\n    if filename is not None:\n        rotation_code = check_video_rotation(filename)\n        video = cv2.VideoCapture(filename)\n        assert video.isOpened()\n    else:\n        rotation_code = None\n        if disable_vidgear:\n            video = cv2.VideoCapture(camera_id)\n            assert video.isOpened()\n        else:\n            video = CamGear(camera_id).start()\n\n    if use_tiny_yolo:\n         yolo_model_def=""./models/detectors/yolo/config/yolov3-tiny.cfg""\n         yolo_class_path=""./models/detectors/yolo/data/coco.names""\n         yolo_weights_path=""./models/detectors/yolo/weights/yolov3-tiny.weights""\n    else:\n         yolo_model_def=""./models/detectors/yolo/config/yolov3.cfg""\n         yolo_class_path=""./models/detectors/yolo/data/coco.names""\n         yolo_weights_path=""./models/detectors/yolo/weights/yolov3.weights""\n\n    model = SimpleHRNet(\n        hrnet_c,\n        hrnet_j,\n        hrnet_weights,\n        model_name=hrnet_m,\n        resolution=image_resolution,\n        multiperson=not single_person,\n        return_bounding_boxes=not disable_tracking,\n        max_batch_size=max_batch_size,\n        yolo_model_def=yolo_model_def,\n        yolo_class_path=yolo_class_path,\n        yolo_weights_path=yolo_weights_path,\n        device=device\n    )\n\n    if not disable_tracking:\n        prev_boxes = None\n        prev_pts = None\n        prev_person_ids = None\n        next_person_id = 0\n\n    while True:\n        t = time.time()\n\n        if filename is not None or disable_vidgear:\n            ret, frame = video.read()\n            if not ret:\n                break\n            if rotation_code is not None:\n                frame = cv2.rotate(frame, rotation_code)\n        else:\n            frame = video.read()\n            if frame is None:\n                break\n\n        pts = model.predict(frame)\n\n        if not disable_tracking:\n            boxes, pts = pts\n\n        if not disable_tracking:\n            if len(pts) > 0:\n                if prev_pts is None and prev_person_ids is None:\n                    person_ids = np.arange(next_person_id, len(pts) + next_person_id, dtype=np.int32)\n                    next_person_id = len(pts) + 1\n                else:\n                    boxes, pts, person_ids = find_person_id_associations(\n                        boxes=boxes, pts=pts, prev_boxes=prev_boxes, prev_pts=prev_pts, prev_person_ids=prev_person_ids,\n                        next_person_id=next_person_id, pose_alpha=0.2, similarity_threshold=0.4, smoothing_alpha=0.1,\n                    )\n                    next_person_id = max(next_person_id, np.max(person_ids) + 1)\n            else:\n                person_ids = np.array((), dtype=np.int32)\n\n            prev_boxes = boxes.copy()\n            prev_pts = pts.copy()\n            prev_person_ids = person_ids\n\n        else:\n            person_ids = np.arange(len(pts), dtype=np.int32)\n\n        for i, (pt, pid) in enumerate(zip(pts, person_ids)):\n            frame = draw_points_and_skeleton(frame, pt, joints_dict()[hrnet_joints_set][\'skeleton\'], person_index=pid,\n                                             points_color_palette=\'gist_rainbow\', skeleton_color_palette=\'jet\',\n                                             points_palette_samples=10)\n\n        fps = 1. / (time.time() - t)\n        print(\'\\rframerate: %f fps\' % fps, end=\'\')\n\n        if has_display:\n            cv2.imshow(\'frame.png\', frame)\n            k = cv2.waitKey(1)\n            if k == 27:  # Esc button\n                if disable_vidgear:\n                    video.release()\n                else:\n                    video.stop()\n                break\n        else:\n            cv2.imwrite(\'frame.png\', frame)\n\n        if save_video:\n            if video_writer is None:\n                fourcc = cv2.VideoWriter_fourcc(*video_format)  # video format\n                video_writer = cv2.VideoWriter(\'output.avi\', fourcc, video_framerate, (frame.shape[1], frame.shape[0]))\n            video_writer.write(frame)\n\n    if save_video:\n        video_writer.release()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--camera_id"", ""-d"", help=""open the camera with the specified id"", type=int, default=0)\n    parser.add_argument(""--filename"", ""-f"", help=""open the specified video (overrides the --camera_id option)"",\n                        type=str, default=None)\n    parser.add_argument(""--hrnet_m"", ""-m"", help=""network model - \'HRNet\' or \'PoseResNet\'"", type=str, default=\'HRNet\')\n    parser.add_argument(""--hrnet_c"", ""-c"", help=""hrnet parameters - number of channels (if model is HRNet), ""\n                                                ""resnet size (if model is PoseResNet)"", type=int, default=48)\n    parser.add_argument(""--hrnet_j"", ""-j"", help=""hrnet parameters - number of joints"", type=int, default=17)\n    parser.add_argument(""--hrnet_weights"", ""-w"", help=""hrnet parameters - path to the pretrained weights"",\n                        type=str, default=""./weights/pose_hrnet_w48_384x288.pth"")\n    parser.add_argument(""--hrnet_joints_set"",\n                        help=""use the specified set of joints (\'coco\' and \'mpii\' are currently supported)"",\n                        type=str, default=""coco"")\n    parser.add_argument(""--image_resolution"", ""-r"", help=""image resolution"", type=str, default=\'(384, 288)\')\n    parser.add_argument(""--single_person"",\n                        help=""disable the multiperson detection (YOLOv3 or an equivalen detector is required for""\n                             ""multiperson detection)"",\n                        action=""store_true"")\n    parser.add_argument(""--use_tiny_yolo"",\n                        help=""Use YOLOv3-tiny in place of YOLOv3 (faster person detection). Ignored if --single_person"",\n                        action=""store_true"")\n    parser.add_argument(""--disable_tracking"",\n                        help=""disable the skeleton tracking and temporal smoothing functionality"",\n                        action=""store_true"")\n    parser.add_argument(""--max_batch_size"", help=""maximum batch size used for inference"", type=int, default=16)\n    parser.add_argument(""--disable_vidgear"",\n                        help=""disable vidgear (which is used for slightly better realtime performance)"",\n                        action=""store_true"")  # see https://pypi.org/project/vidgear/\n    parser.add_argument(""--save_video"", help=""save output frames into a video."", action=""store_true"")\n    parser.add_argument(""--video_format"", help=""fourcc video format. Common formats: `MJPG`, `XVID`, `X264`.""\n                                                     ""See http://www.fourcc.org/codecs.php"", type=str, default=\'MJPG\')\n    parser.add_argument(""--video_framerate"", help=""video framerate"", type=float, default=30)\n    parser.add_argument(""--device"", help=""device to be used (default: cuda, if available).""\n                                         ""Set to `cuda` to use all available GPUs (default); ""\n                                         ""set to `cuda:IDS` to use one or more specific GPUs ""\n                                         ""(e.g. `cuda:0` `cuda:1,2`); ""\n                                         ""set to `cpu` to run on cpu."", type=str, default=None)\n    args = parser.parse_args()\n    main(**args.__dict__)\n'"
scripts/train_coco.py,10,"b'import argparse\nimport ast\nimport os\nimport random\nimport sys\nfrom datetime import datetime\n\nimport numpy as np\nimport torch\n\nsys.path.insert(1, os.getcwd())\nfrom datasets.COCO import COCODataset\nfrom training.COCO import COCOTrain\n\n\ndef main(exp_name,\n         epochs=210,\n         batch_size=2,\n         num_workers=4,\n         lr=0.001,\n         disable_lr_decay=False,\n         lr_decay_steps=\'(170, 200)\',\n         lr_decay_gamma=0.1,\n         optimizer=\'Adam\',\n         weight_decay=0.,\n         momentum=0.9,\n         nesterov=False,\n         pretrained_weight_path=None,\n         checkpoint_path=None,\n         log_path=\'./logs\',\n         disable_tensorboard_log=False,\n         model_c=48,\n         model_nof_joints=17,\n         model_bn_momentum=0.1,\n         disable_flip_test_images=False,\n         image_resolution=\'(384, 288)\',\n         coco_root_path=""./datasets/COCO"",\n         coco_bbox_path=None,\n         seed=1,\n         device=None):\n\n    # Seeds\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.enabled = True  # Enables cudnn\n        torch.backends.cudnn.benchmark = True  # It should improve runtime performances when batch shape is fixed. See https://discuss.pytorch.org/t/what-does-torch-backends-cudnn-benchmark-do/5936\n        torch.backends.cudnn.deterministic = True  # To have ~deterministic results\n\n    # torch device\n    if device is not None:\n        device = torch.device(device)\n    else:\n        if torch.cuda.is_available():\n            device = torch.device(\'cuda:0\')\n        else:\n            device = torch.device(\'cpu\')\n\n    print(device)\n\n    print(""\\nStarting experiment `%s` @ %s\\n"" % (exp_name, datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")))\n\n    lr_decay = not disable_lr_decay\n    use_tensorboard = not disable_tensorboard_log\n    flip_test_images = not disable_flip_test_images\n    image_resolution = ast.literal_eval(image_resolution)\n    lr_decay_steps = ast.literal_eval(lr_decay_steps)\n\n    print(""\\nLoading train and validation datasets..."")\n\n    # load train and val datasets\n    ds_train = COCODataset(\n        root_path=coco_root_path, data_version=""train2017"", is_train=True, use_gt_bboxes=True, bbox_path="""",\n        image_width=image_resolution[1], image_height=image_resolution[0], color_rgb=True,\n    )\n\n    ds_val = COCODataset(\n        root_path=coco_root_path, data_version=""val2017"", is_train=False, use_gt_bboxes=(coco_bbox_path is None),\n        bbox_path=coco_bbox_path, image_width=image_resolution[1], image_height=image_resolution[0], color_rgb=True,\n    )\n\n    train = COCOTrain(\n        exp_name=exp_name,\n        ds_train=ds_train,\n        ds_val=ds_val,\n        epochs=epochs,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        loss=\'JointsMSELoss\',\n        lr=lr,\n        lr_decay=lr_decay,\n        lr_decay_steps=lr_decay_steps,\n        lr_decay_gamma=lr_decay_gamma,\n        optimizer=optimizer,\n        weight_decay=weight_decay,\n        momentum=momentum,\n        nesterov=nesterov,\n        pretrained_weight_path=pretrained_weight_path,\n        checkpoint_path=checkpoint_path,\n        log_path=log_path,\n        use_tensorboard=use_tensorboard,\n        model_c=model_c,\n        model_nof_joints=model_nof_joints,\n        model_bn_momentum=model_bn_momentum,\n        flip_test_images=flip_test_images,\n        device=device\n    )\n\n    train.run()\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--exp_name"", ""-n"",\n                        help=""experiment name. A folder with this name will be created in the log_path."",\n                        type=str, default=str(datetime.now().strftime(""%Y%m%d_%H%M"")))\n    parser.add_argument(""--epochs"", ""-e"", help=""number of epochs"", type=int, default=200)\n    parser.add_argument(""--batch_size"", ""-b"", help=""batch size"", type=int, default=16)\n    parser.add_argument(""--num_workers"", ""-w"", help=""number of DataLoader workers"", type=int, default=4)\n    parser.add_argument(""--lr"", ""-l"", help=""initial learning rate"", type=float, default=0.001)\n    parser.add_argument(""--disable_lr_decay"", help=""disable learning rate decay"", action=""store_true"")\n    parser.add_argument(""--lr_decay_steps"", help=""learning rate decay steps"", type=str, default=""(170, 200)"")\n    parser.add_argument(""--lr_decay_gamma"", help=""learning rate decay gamma"", type=float, default=0.1)\n    parser.add_argument(""--optimizer"", ""-o"", help=""optimizer name. Currently, only `SGD` and `Adam` are supported."",\n                        type=str, default=\'Adam\')\n    parser.add_argument(""--weight_decay"", help=""weight decay"", type=float, default=0.)\n    parser.add_argument(""--momentum"", ""-m"", help=""momentum"", type=float, default=0.9)\n    parser.add_argument(""--nesterov"", help=""enable nesterov"", action=""store_true"")\n    parser.add_argument(""--pretrained_weight_path"", ""-p"",\n                        help=""pre-trained weight path. Weights will be loaded before training starts."",\n                        type=str, default=None)\n    parser.add_argument(""--checkpoint_path"", ""-c"",\n                        help=""previous checkpoint path. Checkpoint will be loaded before training starts. It includes ""\n                             ""the model, the optimizer, the epoch, and other parameters."",\n                        type=str, default=None)\n    parser.add_argument(""--log_path"", help=""log path. tensorboard logs and checkpoints will be saved here."",\n                        type=str, default=\'./logs\')\n    parser.add_argument(""--disable_tensorboard_log"", ""-u"", help=""disable tensorboard logging"", action=""store_true"")\n    parser.add_argument(""--model_c"", help=""HRNet c parameter"", type=int, default=48)\n    parser.add_argument(""--model_nof_joints"", help=""HRNet nof_joints parameter"", type=int, default=17)\n    parser.add_argument(""--model_bn_momentum"", help=""HRNet bn_momentum parameter"", type=float, default=0.1)\n    parser.add_argument(""--disable_flip_test_images"", help=""disable image flip during evaluation"", action=""store_true"")\n    parser.add_argument(""--image_resolution"", ""-r"", help=""image resolution"", type=str, default=\'(384, 288)\')\n    parser.add_argument(""--coco_root_path"", help=""COCO dataset root path"", type=str, default=""./datasets/COCO"")\n    parser.add_argument(""--coco_bbox_path"", help=""path of detected bboxes to use during evaluation"",\n                        type=str, default=None)\n    parser.add_argument(""--seed"", ""-s"", help=""seed"", type=int, default=1)\n    parser.add_argument(""--device"", ""-d"", help=""device"", type=str, default=None)\n    args = parser.parse_args()\n\n    main(**args.__dict__)\n'"
testing/Test.py,6,"b'import os\nfrom datetime import datetime\n\nimport torch\nfrom torch.utils.data.dataloader import DataLoader\nfrom tqdm import tqdm\n\nfrom datasets.HumanPoseEstimation import HumanPoseEstimationDataset\nfrom losses.loss import JointsMSELoss, JointsOHKMMSELoss\nfrom misc.checkpoint import load_checkpoint\nfrom misc.utils import flip_tensor, flip_back\nfrom misc.visualization import save_images\nfrom models.hrnet import HRNet\n\n\nclass Test(object):\n    """"""\n    Test class.\n\n    The class provides a basic tool for testing HRNet checkpoints.\n\n    The only method supposed to be directly called is `run()`.\n    """"""\n\n    def __init__(self,\n                 ds_test,\n                 batch_size=1,\n                 num_workers=4,\n                 loss=\'JointsMSELoss\',\n                 checkpoint_path=None,\n                 model_c=48,\n                 model_nof_joints=17,\n                 model_bn_momentum=0.1,\n                 flip_test_images=True,\n                 device=None\n                 ):\n        """"""\n        Initializes a new Test object.\n\n        The HRNet model is initialized and the saved checkpoint is loaded.\n        The DataLoader and the loss function are defined.\n\n        Args:\n            ds_test (HumanPoseEstimationDataset): test dataset.\n            batch_size (int): batch size.\n                Default: 1\n            num_workers (int): number of workers for each DataLoader\n                Default: 4\n            loss (str): loss function. Valid values are \'JointsMSELoss\' and \'JointsOHKMMSELoss\'.\n                Default: ""JointsMSELoss""\n            checkpoint_path (str): path to a previous checkpoint.\n                Default: None\n            model_c (int): hrnet parameters - number of channels.\n                Default: 48\n            model_nof_joints (int): hrnet parameters - number of joints.\n                Default: 17\n            model_bn_momentum (float): hrnet parameters - path to the pretrained weights.\n                Default: 0.1\n            flip_test_images (bool): flip images during validating.\n                Default: True\n            device (torch.device): device to be used (default: cuda, if available).\n                Default: None\n        """"""\n        super(Test, self).__init__()\n\n        self.ds_test = ds_test\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.loss = loss\n        self.checkpoint_path = checkpoint_path\n        self.model_c = model_c\n        self.model_nof_joints = model_nof_joints\n        self.model_bn_momentum = model_bn_momentum\n        self.flip_test_images = flip_test_images\n        self.epoch = 0\n\n        # torch device\n        if device is not None:\n            self.device = device\n        else:\n            if torch.cuda.is_available():\n                self.device = torch.device(\'cuda:0\')\n            else:\n                self.device = torch.device(\'cpu\')\n\n        print(self.device)\n\n        #\n        # load model\n        self.model = HRNet(c=self.model_c, nof_joints=self.model_nof_joints,\n                           bn_momentum=self.model_bn_momentum).to(self.device)\n\n        #\n        # define loss\n        if self.loss == \'JointsMSELoss\':\n            self.loss_fn = JointsMSELoss().to(self.device)\n        elif self.loss == \'JointsOHKMMSELoss\':\n            self.loss_fn = JointsOHKMMSELoss().to(self.device)\n        else:\n            raise NotImplementedError\n\n        #\n        # load previous checkpoint\n        if self.checkpoint_path is not None:\n            print(\'Loading checkpoint %s...\' % self.checkpoint_path)\n            if os.path.isdir(self.checkpoint_path):\n                path = os.path.join(self.checkpoint_path, \'checkpoint_last.pth\')\n            else:\n                path = self.checkpoint_path\n            self.starting_epoch, self.model, _, self.params = load_checkpoint(path, self.model, device=self.device)\n        else:\n            raise ValueError(\'checkpoint_path is not defined\')\n\n        #\n        # load test dataset\n        self.dl_test = DataLoader(self.ds_test, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n        self.len_dl_test = len(self.dl_test)\n\n        #\n        # initialize variables\n        self.mean_loss_test = 0.\n        self.mean_acc_test = 0.\n\n    def _test(self):\n        self.model.eval()\n        with torch.no_grad():\n            for step, (image, target, target_weight, joints_data) in enumerate(tqdm(self.dl_test, desc=\'Test\')):\n                image = image.to(self.device)\n                target = target.to(self.device)\n                target_weight = target_weight.to(self.device)\n\n                output = self.model(image)\n\n                if self.flip_test_images:\n                    image_flipped = flip_tensor(image, dim=-1)\n                    output_flipped = self.model(image_flipped)\n\n                    output_flipped = flip_back(output_flipped, self.ds_test.flip_pairs)\n\n                    output = (output + output_flipped) * 0.5\n\n                loss = self.loss_fn(output, target, target_weight)\n\n                # Evaluate accuracy\n                # Get predictions on the input\n                accs, avg_acc, cnt, joints_preds, joints_target = \\\n                    self.ds_test.evaluate_accuracy(output, target)\n\n                self.mean_loss_test += loss.item()\n                self.mean_acc_test += avg_acc.item()\n                if step == 0:\n                    save_images(image, target, joints_target, output, joints_preds, joints_data[\'joints_visibility\'])\n\n        self.mean_loss_test /= self.len_dl_test\n        self.mean_acc_test /= self.len_dl_test\n\n        print(\'\\nTest: Loss %f - Accuracy %f\' % (self.mean_loss_test, self.mean_acc_test))\n\n    def run(self):\n        """"""\n        Runs the test.\n        """"""\n\n        print(\'\\nTest started @ %s\' % datetime.now().strftime(""%Y-%m-%d %H:%M:%S""))\n\n        # start testing\n        print(\'\\nLoaded checkpoint %s @ %s\\nSaved epoch %d\' %\n              (self.checkpoint_path, datetime.now().strftime(""%Y-%m-%d %H:%M:%S""), self.starting_epoch))\n\n        self.mean_loss_test = 0.\n        self.mean_acc_test = 0.\n\n        #\n        # Test\n\n        self._test()\n\n        print(\'\\nTest ended @ %s\' % datetime.now().strftime(""%Y-%m-%d %H:%M:%S""))\n'"
testing/__init__.py,0,b''
training/COCO.py,2,"b'import numpy as np\nimport torch\nfrom tqdm import tqdm\n\nfrom datasets.HumanPoseEstimation import HumanPoseEstimationDataset\nfrom misc.utils import flip_tensor, flip_back, get_final_preds\nfrom misc.visualization import save_images\nfrom training.Train import Train\n\n\nclass COCOTrain(Train):\n    """"""\n    COCOTrain class.\n\n    Extension of the Train class for the COCO dataset.\n    """"""\n\n    def __init__(self,\n                 exp_name,\n                 ds_train,\n                 ds_val,\n                 epochs=210,\n                 batch_size=16,\n                 num_workers=4,\n                 loss=\'JointsMSELoss\',\n                 lr=0.001,\n                 lr_decay=True,\n                 lr_decay_steps=(170, 200),\n                 lr_decay_gamma=0.1,\n                 optimizer=\'Adam\',\n                 weight_decay=0.,\n                 momentum=0.9,\n                 nesterov=False,\n                 pretrained_weight_path=None,\n                 checkpoint_path=None,\n                 log_path=\'./logs\',\n                 use_tensorboard=True,\n                 model_c=48,\n                 model_nof_joints=17,\n                 model_bn_momentum=0.1,\n                 flip_test_images=True,\n                 device=None\n                 ):\n        """"""\n        Initializes a new COCOTrain object which extends the parent Train class.\n        The initialization function calls the init function of the Train class.\n\n        Args:\n            exp_name (str):  experiment name.\n            ds_train (HumanPoseEstimationDataset): train dataset.\n            ds_val (HumanPoseEstimationDataset): validation dataset.\n            epochs (int): number of epochs.\n                Default: 210\n            batch_size (int): batch size.\n                Default: 16\n            num_workers (int): number of workers for each DataLoader\n                Default: 4\n            loss (str): loss function. Valid values are \'JointsMSELoss\' and \'JointsOHKMMSELoss\'.\n                Default: ""JointsMSELoss""\n            lr (float): learning rate.\n                Default: 0.001\n            lr_decay (bool): learning rate decay.\n                Default: True\n            lr_decay_steps (tuple): steps for the learning rate decay scheduler.\n                Default: (170, 200)\n            lr_decay_gamma (float): scale factor for each learning rate decay step.\n                Default: 0.1\n            optimizer (str): network optimizer. Valid values are \'Adam\' and \'SGD\'.\n                Default: ""Adam""\n            weight_decay (float): weight decay.\n                Default: 0.\n            momentum (float): momentum factor.\n                Default: 0.9\n            nesterov (bool): Nesterov momentum.\n                Default: False\n            pretrained_weight_path (str): path to pre-trained weights (such as weights from pre-train on imagenet).\n                Default: None\n            checkpoint_path (str): path to a previous checkpoint.\n                Default: None\n            log_path (str): path where tensorboard data and checkpoints will be saved.\n                Default: ""./logs""\n            use_tensorboard (bool): enables tensorboard use.\n                Default: True\n            model_c (int): hrnet parameters - number of channels.\n                Default: 48\n            model_nof_joints (int): hrnet parameters - number of joints.\n                Default: 17\n            model_bn_momentum (float): hrnet parameters - path to the pretrained weights.\n                Default: 0.1\n            flip_test_images (bool): flip images during validating.\n                Default: True\n            device (torch.device): device to be used (default: cuda, if available).\n                Default: None\n        """"""\n        super(COCOTrain, self).__init__(\n            exp_name=exp_name,\n            ds_train=ds_train,\n            ds_val=ds_val,\n            epochs=epochs,\n            batch_size=batch_size,\n            num_workers=num_workers,\n            loss=loss,\n            lr=lr,\n            lr_decay=lr_decay,\n            lr_decay_steps=lr_decay_steps,\n            lr_decay_gamma=lr_decay_gamma,\n            optimizer=optimizer,\n            weight_decay=weight_decay,\n            momentum=momentum,\n            nesterov=nesterov,\n            pretrained_weight_path=pretrained_weight_path,\n            checkpoint_path=checkpoint_path,\n            log_path=log_path,\n            use_tensorboard=use_tensorboard,\n            model_c=model_c,\n            model_nof_joints=model_nof_joints,\n            model_bn_momentum=model_bn_momentum,\n            flip_test_images=flip_test_images,\n            device=device\n        )\n\n    def _train(self):\n\n        num_samples = self.len_dl_train * self.batch_size\n        all_preds = np.zeros((num_samples, self.model_nof_joints, 3), dtype=np.float32)\n        all_boxes = np.zeros((num_samples, 6), dtype=np.float32)\n        image_paths = []\n        idx = 0\n\n        self.model.train()\n        for step, (image, target, target_weight, joints_data) in enumerate(tqdm(self.dl_train, desc=\'Training\')):\n            image = image.to(self.device)\n            target = target.to(self.device)\n            target_weight = target_weight.to(self.device)\n\n            self.optim.zero_grad()\n\n            output = self.model(image)\n\n            loss = self.loss_fn(output, target, target_weight)\n\n            loss.backward()\n\n            self.optim.step()\n\n            # Evaluate accuracy\n            # Get predictions on the resized images (given as input)\n            accs, avg_acc, cnt, joints_preds, joints_target = \\\n                self.ds_train.evaluate_accuracy(output, target)\n\n            # Original\n            num_images = image.shape[0]\n\n            # measure elapsed time\n            c = joints_data[\'center\'].numpy()\n            s = joints_data[\'scale\'].numpy()\n            score = joints_data[\'score\'].numpy()\n            pixel_std = 200  # ToDo Parametrize this\n\n            # Get predictions on the original imagee\n            preds, maxvals = get_final_preds(True, output.detach(), c, s,\n                                             pixel_std)  # ToDo check what post_processing exactly does\n\n            all_preds[idx:idx + num_images, :, 0:2] = preds[:, :, 0:2].detach().cpu().numpy()\n            all_preds[idx:idx + num_images, :, 2:3] = maxvals.detach().cpu().numpy()\n            all_boxes[idx:idx + num_images, 0:2] = c[:, 0:2]\n            all_boxes[idx:idx + num_images, 2:4] = s[:, 0:2]\n            all_boxes[idx:idx + num_images, 4] = np.prod(s * pixel_std, 1)\n            all_boxes[idx:idx + num_images, 5] = score\n            image_paths.extend(joints_data[\'imgPath\'])\n\n            idx += num_images\n\n            self.mean_loss_train += loss.item()\n            if self.use_tensorboard:\n                self.summary_writer.add_scalar(\'train_loss\', loss.item(),\n                                               global_step=step + self.epoch * self.len_dl_train)\n                self.summary_writer.add_scalar(\'train_acc\', avg_acc.item(),\n                                               global_step=step + self.epoch * self.len_dl_train)\n                if step == 0:\n                    save_images(image, target, joints_target, output, joints_preds, joints_data[\'joints_visibility\'],\n                                self.summary_writer, step=step + self.epoch * self.len_dl_train, prefix=\'train_\')\n\n        self.mean_loss_train /= len(self.dl_train)\n\n        # COCO evaluation\n        print(\'\\nTrain AP/AR\')\n        self.train_accs, self.mean_mAP_train = self.ds_train.evaluate_overall_accuracy(\n            all_preds, all_boxes, image_paths, output_dir=self.log_path)\n\n    def _val(self):\n        num_samples = len(self.ds_val)\n        all_preds = np.zeros((num_samples, self.model_nof_joints, 3), dtype=np.float32)\n        all_boxes = np.zeros((num_samples, 6), dtype=np.float32)\n        image_paths = []\n        idx = 0\n        self.model.eval()\n        with torch.no_grad():\n            for step, (image, target, target_weight, joints_data) in enumerate(tqdm(self.dl_val, desc=\'Validating\')):\n                image = image.to(self.device)\n                target = target.to(self.device)\n                target_weight = target_weight.to(self.device)\n\n                output = self.model(image)\n\n                if self.flip_test_images:\n                    image_flipped = flip_tensor(image, dim=-1)\n                    output_flipped = self.model(image_flipped)\n\n                    output_flipped = flip_back(output_flipped, self.ds_val.flip_pairs)\n\n                    output = (output + output_flipped) * 0.5\n\n                loss = self.loss_fn(output, target, target_weight)\n\n                # Evaluate accuracy\n                # Get predictions on the resized images (given as input)\n                accs, avg_acc, cnt, joints_preds, joints_target = \\\n                    self.ds_train.evaluate_accuracy(output, target)\n\n                # Original\n                num_images = image.shape[0]\n\n                # measure elapsed time\n                c = joints_data[\'center\'].numpy()\n                s = joints_data[\'scale\'].numpy()\n                score = joints_data[\'score\'].numpy()\n                pixel_std = 200  # ToDo Parametrize this\n\n                preds, maxvals = get_final_preds(True, output, c, s,\n                                                 pixel_std)  # ToDo check what post_processing exactly does\n\n                all_preds[idx:idx + num_images, :, 0:2] = preds[:, :, 0:2].detach().cpu().numpy()\n                all_preds[idx:idx + num_images, :, 2:3] = maxvals.detach().cpu().numpy()\n                # double check this all_boxes parts\n                all_boxes[idx:idx + num_images, 0:2] = c[:, 0:2]\n                all_boxes[idx:idx + num_images, 2:4] = s[:, 0:2]\n                all_boxes[idx:idx + num_images, 4] = np.prod(s * pixel_std, 1)\n                all_boxes[idx:idx + num_images, 5] = score\n                image_paths.extend(joints_data[\'imgPath\'])\n\n                idx += num_images\n\n                self.mean_loss_val += loss.item()\n                self.mean_acc_val += avg_acc.item()\n                if self.use_tensorboard:\n                    self.summary_writer.add_scalar(\'val_loss\', loss.item(),\n                                                   global_step=step + self.epoch * self.len_dl_val)\n                    self.summary_writer.add_scalar(\'val_acc\', avg_acc.item(),\n                                                   global_step=step + self.epoch * self.len_dl_val)\n                    if step == 0:\n                        save_images(image, target, joints_target, output, joints_preds,\n                                    joints_data[\'joints_visibility\'], self.summary_writer,\n                                    step=step + self.epoch * self.len_dl_train, prefix=\'test_\')\n\n        self.mean_loss_val /= len(self.dl_val)\n        self.mean_acc_val /= len(self.dl_val)\n\n        # COCO evaluation\n        print(\'\\nVal AP/AR\')\n        self.val_accs, self.mean_mAP_val = self.ds_val.evaluate_overall_accuracy(\n            all_preds, all_boxes, image_paths, output_dir=self.log_path)\n'"
training/Train.py,9,"b'import os\nfrom datetime import datetime\n\nimport tensorboardX as tb\nimport torch\nfrom torch.optim import SGD, Adam\nfrom torch.optim.lr_scheduler import MultiStepLR\nfrom torch.utils.data.dataloader import DataLoader\nfrom tqdm import tqdm\n\nfrom datasets.HumanPoseEstimation import HumanPoseEstimationDataset\nfrom losses.loss import JointsMSELoss, JointsOHKMMSELoss\nfrom misc.checkpoint import save_checkpoint, load_checkpoint\nfrom misc.utils import flip_tensor, flip_back\nfrom misc.visualization import save_images\nfrom models.hrnet import HRNet\n\n\nclass Train(object):\n    """"""\n    Train  class.\n\n    The class provides a basic tool for training HRNet.\n    Most of the training options are customizable.\n\n    The only method supposed to be directly called is `run()`.\n    """"""\n\n    def __init__(self,\n                 exp_name,\n                 ds_train,\n                 ds_val,\n                 epochs=210,\n                 batch_size=16,\n                 num_workers=4,\n                 loss=\'JointsMSELoss\',\n                 lr=0.001,\n                 lr_decay=True,\n                 lr_decay_steps=(170, 200),\n                 lr_decay_gamma=0.1,\n                 optimizer=\'Adam\',\n                 weight_decay=0.,\n                 momentum=0.9,\n                 nesterov=False,\n                 pretrained_weight_path=None,\n                 checkpoint_path=None,\n                 log_path=\'./logs\',\n                 use_tensorboard=True,\n                 model_c=48,\n                 model_nof_joints=17,\n                 model_bn_momentum=0.1,\n                 flip_test_images=True,\n                 device=None\n                 ):\n        """"""\n        Initializes a new Train object.\n\n        The log folder is created, the HRNet model is initialized and optional pre-trained weights or saved checkpoints\n        are loaded.\n        The DataLoaders, the loss function, and the optimizer are defined.\n\n        Args:\n            exp_name (str):  experiment name.\n            ds_train (HumanPoseEstimationDataset): train dataset.\n            ds_val (HumanPoseEstimationDataset): validation dataset.\n            epochs (int): number of epochs.\n                Default: 210\n            batch_size (int): batch size.\n                Default: 16\n            num_workers (int): number of workers for each DataLoader\n                Default: 4\n            loss (str): loss function. Valid values are \'JointsMSELoss\' and \'JointsOHKMMSELoss\'.\n                Default: ""JointsMSELoss""\n            lr (float): learning rate.\n                Default: 0.001\n            lr_decay (bool): learning rate decay.\n                Default: True\n            lr_decay_steps (tuple): steps for the learning rate decay scheduler.\n                Default: (170, 200)\n            lr_decay_gamma (float): scale factor for each learning rate decay step.\n                Default: 0.1\n            optimizer (str): network optimizer. Valid values are \'Adam\' and \'SGD\'.\n                Default: ""Adam""\n            weight_decay (float): weight decay.\n                Default: 0.\n            momentum (float): momentum factor.\n                Default: 0.9\n            nesterov (bool): Nesterov momentum.\n                Default: False\n            pretrained_weight_path (str): path to pre-trained weights (such as weights from pre-train on imagenet).\n                Default: None\n            checkpoint_path (str): path to a previous checkpoint.\n                Default: None\n            log_path (str): path where tensorboard data and checkpoints will be saved.\n                Default: ""./logs""\n            use_tensorboard (bool): enables tensorboard use.\n                Default: True\n            model_c (int): hrnet parameters - number of channels.\n                Default: 48\n            model_nof_joints (int): hrnet parameters - number of joints.\n                Default: 17\n            model_bn_momentum (float): hrnet parameters - path to the pretrained weights.\n                Default: 0.1\n            flip_test_images (bool): flip images during validating.\n                Default: True\n            device (torch.device): device to be used (default: cuda, if available).\n                Default: None\n        """"""\n        super(Train, self).__init__()\n\n        self.exp_name = exp_name\n        self.ds_train = ds_train\n        self.ds_val = ds_val\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n        self.loss = loss\n        self.lr = lr\n        self.lr_decay = lr_decay\n        self.lr_decay_steps = lr_decay_steps\n        self.lr_decay_gamma = lr_decay_gamma\n        self.optimizer = optimizer\n        self.weight_decay = weight_decay\n        self.momentum = momentum\n        self.nesterov = nesterov\n        self.pretrained_weight_path = pretrained_weight_path\n        self.checkpoint_path = checkpoint_path\n        self.log_path = os.path.join(log_path, self.exp_name)\n        self.use_tensorboard = use_tensorboard\n        self.model_c = model_c\n        self.model_nof_joints = model_nof_joints\n        self.model_bn_momentum = model_bn_momentum\n        self.flip_test_images = flip_test_images\n        self.epoch = 0\n\n        # torch device\n        if device is not None:\n            self.device = device\n        else:\n            if torch.cuda.is_available():\n                self.device = torch.device(\'cuda:0\')\n            else:\n                self.device = torch.device(\'cpu\')\n\n        print(self.device)\n\n        os.makedirs(self.log_path, 0o755, exist_ok=False)  # exist_ok=False to avoid overwriting\n        if self.use_tensorboard:\n            self.summary_writer = tb.SummaryWriter(self.log_path)\n\n        #\n        # write all experiment parameters in parameters.txt and in tensorboard text field\n        self.parameters = [x + \': \' + str(y) + \'\\n\' for x, y in locals().items()]\n        with open(os.path.join(self.log_path, \'parameters.txt\'), \'w\') as fd:\n            fd.writelines(self.parameters)\n        if self.use_tensorboard:\n            self.summary_writer.add_text(\'parameters\', \'\\n\'.join(self.parameters))\n\n        #\n        # load model\n        self.model = HRNet(c=self.model_c, nof_joints=self.model_nof_joints,\n                           bn_momentum=self.model_bn_momentum).to(self.device)\n\n        #\n        # define loss and optimizers\n        if self.loss == \'JointsMSELoss\':\n            self.loss_fn = JointsMSELoss().to(self.device)\n        elif self.loss == \'JointsOHKMMSELoss\':\n            self.loss_fn = JointsOHKMMSELoss().to(self.device)\n        else:\n            raise NotImplementedError\n\n        if optimizer == \'SGD\':\n            self.optim = SGD(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay,\n                             momentum=self.momentum, nesterov=self.nesterov)\n        elif optimizer == \'Adam\':\n            self.optim = Adam(self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n        else:\n            raise NotImplementedError\n\n        #\n        # load pre-trained weights (such as those pre-trained on imagenet)\n        if self.pretrained_weight_path is not None:\n            self.model.load_state_dict(torch.load(self.pretrained_weight_path, map_location=self.device), strict=True)\n            print(\'Pre-trained weights loaded.\')\n\n        #\n        # load previous checkpoint\n        if self.checkpoint_path is not None:\n            print(\'Loading checkpoint %s...\' % self.checkpoint_path)\n            if os.path.isdir(self.checkpoint_path):\n                path = os.path.join(self.checkpoint_path, \'checkpoint_last.pth\')\n            else:\n                path = self.checkpoint_path\n            self.starting_epoch, self.model, self.optim, self.params = load_checkpoint(path, self.model, self.optim,\n                                                                                       self.device)\n        else:\n            self.starting_epoch = 0\n\n        if lr_decay:\n            self.lr_scheduler = MultiStepLR(self.optim, list(self.lr_decay_steps), gamma=self.lr_decay_gamma,\n                                            last_epoch=self.starting_epoch if self.starting_epoch else -1)\n\n        #\n        # load train and val datasets\n        self.dl_train = DataLoader(self.ds_train, batch_size=self.batch_size, shuffle=True,\n                                   num_workers=self.num_workers, drop_last=True)\n        self.len_dl_train = len(self.dl_train)\n\n        # dl_val = DataLoader(self.ds_val, batch_size=1, shuffle=False, num_workers=num_workers)\n        self.dl_val = DataLoader(self.ds_val, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n        self.len_dl_val = len(self.dl_val)\n\n        #\n        # initialize variables\n        self.mean_loss_train = 0.\n        self.mean_acc_train = 0.\n        self.mean_loss_val = 0.\n        self.mean_acc_val = 0.\n        self.mean_mAP_val = 0.\n\n        self.best_loss = None\n        self.best_acc = None\n        self.best_mAP = None\n\n    def _train(self):\n        self.model.train()\n\n        for step, (image, target, target_weight, joints_data) in enumerate(tqdm(self.dl_train, desc=\'Training\')):\n            image = image.to(self.device)\n            target = target.to(self.device)\n            target_weight = target_weight.to(self.device)\n\n            self.optim.zero_grad()\n\n            output = self.model(image)\n\n            loss = self.loss_fn(output, target, target_weight)\n\n            loss.backward()\n\n            self.optim.step()\n\n            # Evaluate accuracy\n            # Get predictions on the input\n            accs, avg_acc, cnt, joints_preds, joints_target = self.ds_train.evaluate_accuracy(output, target)\n\n            self.mean_loss_train += loss.item()\n            self.mean_acc_train += avg_acc.item()\n            if self.use_tensorboard:\n                self.summary_writer.add_scalar(\'train_loss\', loss.item(),\n                                               global_step=step + self.epoch * self.len_dl_train)\n                self.summary_writer.add_scalar(\'train_acc\', avg_acc.item(),\n                                               global_step=step + self.epoch * self.len_dl_train)\n                if step == 0:\n                    save_images(image, target, joints_target, output, joints_preds, joints_data[\'joints_visibility\'],\n                                self.summary_writer, step=step + self.epoch * self.len_dl_train, prefix=\'train_\')\n\n        self.mean_loss_train /= len(self.dl_train)\n        self.mean_acc_train /= len(self.dl_train)\n\n        print(\'\\nTrain: Loss %f - Accuracy %f\' % (self.mean_loss_train, self.mean_acc_train))\n\n    def _val(self):\n        self.model.eval()\n\n        with torch.no_grad():\n            for step, (image, target, target_weight, joints_data) in enumerate(tqdm(self.dl_val, desc=\'Validating\')):\n                image = image.to(self.device)\n                target = target.to(self.device)\n                target_weight = target_weight.to(self.device)\n\n                output = self.model(image)\n\n                if self.flip_test_images:\n                    image_flipped = flip_tensor(image, dim=-1)\n                    output_flipped = self.model(image_flipped)\n\n                    output_flipped = flip_back(output_flipped, self.ds_val.flip_pairs)\n\n                    output = (output + output_flipped) * 0.5\n\n                loss = self.loss_fn(output, target, target_weight)\n\n                # Evaluate accuracy\n                # Get predictions on the input\n                accs, avg_acc, cnt, joints_preds, joints_target = \\\n                    self.ds_train.evaluate_accuracy(output, target)\n\n                self.mean_loss_train += loss.item()\n                self.mean_acc_train += avg_acc.item()\n                if self.use_tensorboard:\n                    self.summary_writer.add_scalar(\'val_loss\', loss.item(),\n                                                   global_step=step + self.epoch * self.len_dl_train)\n                    self.summary_writer.add_scalar(\'val_acc\', avg_acc.item(),\n                                                   global_step=step + self.epoch * self.len_dl_train)\n                    if step == 0:\n                        save_images(image, target, joints_target, output, joints_preds,\n                                    joints_data[\'joints_visibility\'], self.summary_writer,\n                                    step=step + self.epoch * self.len_dl_train, prefix=\'val_\')\n\n        self.mean_loss_val /= len(self.dl_val)\n        self.mean_acc_val /= len(self.dl_val)\n\n        print(\'\\nValidation: Loss %f - Accuracy %f\' % (self.mean_loss_val, self.mean_acc_val))\n\n    def _checkpoint(self):\n\n        save_checkpoint(path=os.path.join(self.log_path, \'checkpoint_last.pth\'), epoch=self.epoch + 1, model=self.model,\n                        optimizer=self.optim, params=self.parameters)\n\n        if self.best_loss is None or self.best_loss > self.mean_loss_val:\n            self.best_loss = self.mean_loss_val\n            print(\'best_loss %f at epoch %d\' % (self.best_loss, self.epoch + 1))\n            save_checkpoint(path=os.path.join(self.log_path, \'checkpoint_best_loss.pth\'), epoch=self.epoch + 1,\n                            model=self.model, optimizer=self.optim, params=self.parameters)\n        if self.best_acc is None or self.best_acc < self.mean_acc_val:\n            self.best_acc = self.mean_acc_val\n            print(\'best_acc %f at epoch %d\' % (self.best_acc, self.epoch + 1))\n            save_checkpoint(path=os.path.join(self.log_path, \'checkpoint_best_acc.pth\'), epoch=self.epoch + 1,\n                            model=self.model, optimizer=self.optim, params=self.parameters)\n        if self.best_mAP is None or self.best_mAP < self.mean_mAP_val:\n            self.best_mAP = self.mean_mAP_val\n            print(\'best_mAP %f at epoch %d\' % (self.best_mAP, self.epoch + 1))\n            save_checkpoint(path=os.path.join(self.log_path, \'checkpoint_best_mAP.pth\'), epoch=self.epoch + 1,\n                            model=self.model, optimizer=self.optim, params=self.parameters)\n\n    def run(self):\n        """"""\n        Runs the training.\n        """"""\n\n        print(\'\\nTraining started @ %s\' % datetime.now().strftime(""%Y-%m-%d %H:%M:%S""))\n\n        # start training\n        for self.epoch in range(self.starting_epoch, self.epochs):\n            print(\'\\nEpoch %d of %d @ %s\' % (self.epoch + 1, self.epochs, datetime.now().strftime(""%Y-%m-%d %H:%M:%S"")))\n\n            self.mean_loss_train = 0.\n            self.mean_loss_val = 0.\n            self.mean_acc_train = 0.\n            self.mean_acc_val = 0.\n            self.mean_mAP_val = 0.\n\n            #\n            # Train\n\n            self._train()\n\n            #\n            # Val\n\n            self._val()\n\n            #\n            # LR Update\n\n            if self.lr_decay:\n                self.lr_scheduler.step()\n\n            #\n            # Checkpoint\n\n            self._checkpoint()\n\n        print(\'\\nTraining ended @ %s\' % datetime.now().strftime(""%Y-%m-%d %H:%M:%S""))\n'"
training/__init__.py,0,b''
misc/nms/__init__.py,0,b''
misc/nms/nms.py,0,"b'# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Modified from py-faster-rcnn (https://github.com/rbgirshick/py-faster-rcnn)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\n\nfrom cpu_nms import cpu_nms\nfrom gpu_nms import gpu_nms\n\n\ndef py_nms_wrapper(thresh):\n    def _nms(dets):\n        return nms(dets, thresh)\n    return _nms\n\n\ndef cpu_nms_wrapper(thresh):\n    def _nms(dets):\n        return cpu_nms(dets, thresh)\n    return _nms\n\n\ndef gpu_nms_wrapper(thresh, device_id):\n    def _nms(dets):\n        return gpu_nms(dets, thresh, device_id)\n    return _nms\n\n\ndef nms(dets, thresh):\n    """"""\n    greedily select boxes with high confidence and overlap with current maximum <= thresh\n    rule out overlap >= thresh\n    :param dets: [[x1, y1, x2, y2 score]]\n    :param thresh: retain overlap < thresh\n    :return: indexes to keep\n    """"""\n    if dets.shape[0] == 0:\n        return []\n\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores = dets[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        ovr = inter / (areas[i] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n\n\ndef oks_iou(g, d, a_g, a_d, sigmas=None, in_vis_thre=None):\n    if not isinstance(sigmas, np.ndarray):\n        sigmas = np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62, .62, 1.07, 1.07, .87, .87, .89, .89]) / 10.0\n    vars = (sigmas * 2) ** 2\n    xg = g[0::3]\n    yg = g[1::3]\n    vg = g[2::3]\n    ious = np.zeros((d.shape[0]))\n    for n_d in range(0, d.shape[0]):\n        xd = d[n_d, 0::3]\n        yd = d[n_d, 1::3]\n        vd = d[n_d, 2::3]\n        dx = xd - xg\n        dy = yd - yg\n        e = (dx ** 2 + dy ** 2) / vars / ((a_g + a_d[n_d]) / 2 + np.spacing(1)) / 2\n        if in_vis_thre is not None:\n            ind = list(vg > in_vis_thre) and list(vd > in_vis_thre)\n            e = e[ind]\n        ious[n_d] = np.sum(np.exp(-e)) / e.shape[0] if e.shape[0] != 0 else 0.0\n    return ious\n\n\ndef oks_nms(kpts_db, thresh, sigmas=None, in_vis_thre=None):\n    """"""\n    greedily select boxes with high confidence and overlap with current maximum <= thresh\n    rule out overlap >= thresh, overlap = oks\n    :param kpts_db\n    :param thresh: retain overlap < thresh\n    :return: indexes to keep\n    """"""\n    if len(kpts_db) == 0:\n        return []\n\n    scores = np.array([kpts_db[i][\'score\'] for i in range(len(kpts_db))])\n    kpts = np.array([kpts_db[i][\'keypoints\'].flatten() for i in range(len(kpts_db))])\n    areas = np.array([kpts_db[i][\'area\'] for i in range(len(kpts_db))])\n\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n\n        oks_ovr = oks_iou(kpts[i], kpts[order[1:]], areas[i], areas[order[1:]], sigmas, in_vis_thre)\n\n        inds = np.where(oks_ovr <= thresh)[0]\n        order = order[inds + 1]\n\n    return keep\n\n\ndef rescore(overlap, scores, thresh, type=\'gaussian\'):\n    assert overlap.shape[0] == scores.shape[0]\n    if type == \'linear\':\n        inds = np.where(overlap >= thresh)[0]\n        scores[inds] = scores[inds] * (1 - overlap[inds])\n    else:\n        scores = scores * np.exp(- overlap**2 / thresh)\n\n    return scores\n\n\ndef soft_oks_nms(kpts_db, thresh, sigmas=None, in_vis_thre=None):\n    """"""\n    greedily select boxes with high confidence and overlap with current maximum <= thresh\n    rule out overlap >= thresh, overlap = oks\n    :param kpts_db\n    :param thresh: retain overlap < thresh\n    :return: indexes to keep\n    """"""\n    if len(kpts_db) == 0:\n        return []\n\n    scores = np.array([kpts_db[i][\'score\'] for i in range(len(kpts_db))])\n    kpts = np.array([kpts_db[i][\'keypoints\'].flatten() for i in range(len(kpts_db))])\n    areas = np.array([kpts_db[i][\'area\'] for i in range(len(kpts_db))])\n\n    order = scores.argsort()[::-1]\n    scores = scores[order]\n\n    # max_dets = order.size\n    max_dets = 20\n    keep = np.zeros(max_dets, dtype=np.intp)\n    keep_cnt = 0\n    while order.size > 0 and keep_cnt < max_dets:\n        i = order[0]\n\n        oks_ovr = oks_iou(kpts[i], kpts[order[1:]], areas[i], areas[order[1:]], sigmas, in_vis_thre)\n\n        order = order[1:]\n        scores = rescore(oks_ovr, scores[1:], thresh)\n\n        tmp = scores.argsort()[::-1]\n        order = order[tmp]\n        scores = scores[tmp]\n\n        keep[keep_cnt] = i\n        keep_cnt += 1\n\n    keep = keep[:keep_cnt]\n\n    return keep\n    # kpts_db = kpts_db[:keep_cnt]\n\n    # return kpts_db\n'"
misc/nms/setup_linux.py,0,"b'# --------------------------------------------------------\n# Pose.gluon\n# Copyright (c) 2018-present Microsoft\n# Licensed under The MIT License [see LICENSE for details]\n# Modified from py-faster-rcnn (https://github.com/rbgirshick/py-faster-rcnn)\n# --------------------------------------------------------\n\nimport os\nfrom os.path import join as pjoin\nfrom setuptools import setup\nfrom distutils.extension import Extension\nfrom Cython.Distutils import build_ext\nimport numpy as np\n\n\ndef find_in_path(name, path):\n    ""Find a file in a search path""\n    # Adapted fom\n    # http://code.activestate.com/recipes/52224-find-a-file-given-a-search-path/\n    for dir in path.split(os.pathsep):\n        binpath = pjoin(dir, name)\n        if os.path.exists(binpath):\n            return os.path.abspath(binpath)\n    return None\n\n\ndef locate_cuda():\n    """"""Locate the CUDA environment on the system\n    Returns a dict with keys \'home\', \'nvcc\', \'include\', and \'lib64\'\n    and values giving the absolute path to each directory.\n    Starts by looking for the CUDAHOME env variable. If not found, everything\n    is based on finding \'nvcc\' in the PATH.\n    """"""\n\n    # first check if the CUDAHOME env variable is in use\n    if \'CUDAHOME\' in os.environ:\n        home = os.environ[\'CUDAHOME\']\n        nvcc = pjoin(home, \'bin\', \'nvcc\')\n    else:\n        # otherwise, search the PATH for NVCC\n        default_path = pjoin(os.sep, \'usr\', \'local\', \'cuda\', \'bin\')\n        nvcc = find_in_path(\'nvcc\', os.environ[\'PATH\'] + os.pathsep + default_path)\n        if nvcc is None:\n            raise EnvironmentError(\'The nvcc binary could not be \'\n                \'located in your $PATH. Either add it to your path, or set $CUDAHOME\')\n        home = os.path.dirname(os.path.dirname(nvcc))\n\n    cudaconfig = {\'home\':home, \'nvcc\':nvcc,\n                  \'include\': pjoin(home, \'include\'),\n                  \'lib64\': pjoin(home, \'lib64\')}\n    for k, v in cudaconfig.items():\n        if not os.path.exists(v):\n            raise EnvironmentError(\'The CUDA %s path could not be located in %s\' % (k, v))\n\n    return cudaconfig\nCUDA = locate_cuda()\n\n\n# Obtain the numpy include directory.  This logic works across numpy versions.\ntry:\n    numpy_include = np.get_include()\nexcept AttributeError:\n    numpy_include = np.get_numpy_include()\n\n\ndef customize_compiler_for_nvcc(self):\n    """"""inject deep into distutils to customize how the dispatch\n    to gcc/nvcc works.\n    If you subclass UnixCCompiler, it\'s not trivial to get your subclass\n    injected in, and still have the right customizations (i.e.\n    distutils.sysconfig.customize_compiler) run on it. So instead of going\n    the OO route, I have this. Note, it\'s kindof like a wierd functional\n    subclassing going on.""""""\n\n    # tell the compiler it can processes .cu\n    self.src_extensions.append(\'.cu\')\n\n    # save references to the default compiler_so and _comple methods\n    default_compiler_so = self.compiler_so\n    super = self._compile\n\n    # now redefine the _compile method. This gets executed for each\n    # object but distutils doesn\'t have the ability to change compilers\n    # based on source extension: we add it.\n    def _compile(obj, src, ext, cc_args, extra_postargs, pp_opts):\n        if os.path.splitext(src)[1] == \'.cu\':\n            # use the cuda for .cu files\n            self.set_executable(\'compiler_so\', CUDA[\'nvcc\'])\n            # use only a subset of the extra_postargs, which are 1-1 translated\n            # from the extra_compile_args in the Extension class\n            postargs = extra_postargs[\'nvcc\']\n        else:\n            postargs = extra_postargs[\'gcc\']\n\n        super(obj, src, ext, cc_args, postargs, pp_opts)\n        # reset the default compiler_so, which we might have changed for cuda\n        self.compiler_so = default_compiler_so\n\n    # inject our redefined _compile method into the class\n    self._compile = _compile\n\n\n# run the customize_compiler\nclass custom_build_ext(build_ext):\n    def build_extensions(self):\n        customize_compiler_for_nvcc(self.compiler)\n        build_ext.build_extensions(self)\n\n\next_modules = [\n    Extension(\n        ""cpu_nms"",\n        [""cpu_nms.pyx""],\n        extra_compile_args={\'gcc\': [""-Wno-cpp"", ""-Wno-unused-function""]},\n        include_dirs = [numpy_include]\n    ),\n    Extension(\'gpu_nms\',\n        [\'nms_kernel.cu\', \'gpu_nms.pyx\'],\n        library_dirs=[CUDA[\'lib64\']],\n        libraries=[\'cudart\'],\n        language=\'c++\',\n        runtime_library_dirs=[CUDA[\'lib64\']],\n        # this syntax is specific to this build system\n        # we\'re only going to use certain compiler args with nvcc and not with\n        # gcc the implementation of this trick is in customize_compiler() below\n        extra_compile_args={\'gcc\': [""-Wno-unused-function""],\n                            \'nvcc\': [\'-arch=sm_35\',\n                                     \'--ptxas-options=-v\',\n                                     \'-c\',\n                                     \'--compiler-options\',\n                                     ""\'-fPIC\'""]},\n        include_dirs = [numpy_include, CUDA[\'include\']]\n    ),\n]\n\nsetup(\n    name=\'nms\',\n    ext_modules=ext_modules,\n    # inject our custom trigger\n    cmdclass={\'build_ext\': custom_build_ext},\n)\n'"
models/detectors/YOLOv3.py,7,"b'from __future__ import division\n\nimport os\nimport sys\nimport cv2\nimport numpy as np\nimport torch\nfrom torchvision.transforms import transforms\n\nsys.path.append(os.path.join(os.getcwd(), \'models\', \'detectors\', \'yolo\'))\n\nfrom .yolo.models import Darknet\nfrom .yolo.utils.utils import load_classes, non_max_suppression\n\n\ndef filter_classes(detections, classes):\n    mask = torch.stack([torch.stack([detections[:, -1] == cls]) for cls in classes])\n    mask = torch.sum(torch.squeeze(mask, dim=1), dim=0)\n    return detections[mask > 0]\n\n\n# derived from https://github.com/ultralytics/yolov3/\ndef letterbox(img, new_shape=416, color=(127.5, 127.5, 127.5), mode=\'auto\'):\n    # Resize a rectangular image to a 32 pixel multiple rectangle\n    shape = img.shape[:2]  # current shape [height, width]\n    if isinstance(new_shape, int):\n        ratio = float(new_shape) / max(shape)\n    else:\n        ratio = max(new_shape) / max(shape)  # ratio  = new / old\n    new_unpad = (int(round(shape[1] * ratio)), int(round(shape[0] * ratio)))\n\n    if mode is \'auto\':  # minimum rectangle\n        dw = np.mod(new_shape - new_unpad[0], 32) / 2  # width padding\n        dh = np.mod(new_shape - new_unpad[1], 32) / 2  # height padding\n    elif mode is \'square\':  # square\n        dw = (new_shape - new_unpad[0]) / 2  # width padding\n        dh = (new_shape - new_unpad[1]) / 2  # height padding\n    else:\n        raise NotImplementedError\n\n    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n    img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)  # resized, no border\n    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # padded square\n    return img, ratio, dw, dh\n\n\n# derived from https://github.com/ultralytics/yolov3/\ndef scale_coords(coords, from_image_shape, to_image_shape):\n    # Rescale coords (xyxy) from from_image_shape to to_image_shape\n    gain = max(from_image_shape) / max(to_image_shape)  # gain  = old / new\n    coords[:, [0, 2]] -= (from_image_shape[1] - to_image_shape[1] * gain) / 2  # x padding\n    coords[:, [1, 3]] -= (from_image_shape[0] - to_image_shape[0] * gain) / 2  # y padding\n    coords[:, :4] /= gain\n    coords[:, :4] = coords[:, :4].clamp(min=0)\n    return coords\n\n\ndef prepare_data(images, color_mode=\'BGR\', new_shape=416, color=(127.5, 127.5, 127.5), mode=\'square\'):\n    images_ok = np.zeros((images.shape[0], new_shape, new_shape, 3), dtype=images[0].dtype)\n    images_tensor = torch.zeros((images.shape[0], 3, new_shape, new_shape), dtype=torch.float32)\n    for i in range(len(images)):\n        if color_mode == \'BGR\':\n            images[i] = cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB)\n        elif color_mode == \'RGB\':\n            pass\n        else:\n            raise NotImplementedError\n        images_ok[i], _, _, _ = letterbox(images[i], new_shape, color, mode)\n\n        images_tensor[i] = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.ToTensor(),\n        ])(images_ok[i])\n\n    return images_tensor\n\n\nclass YOLOv3:\n    def __init__(self,\n                 model_def=""config/yolov3.cfg"",\n                 class_path=""data/coco.names"",\n                 weights_path=""weights/yolov3.weights"",\n                 conf_thres=0.2,\n                 nms_thres=0.4,\n                 img_size=416,\n                 classes=None,\n                 max_batch_size=16,\n                 device=torch.device(\'cpu\')):\n\n        self.model_def = model_def\n        self.weights_path = weights_path\n        self.class_path = class_path\n        self.conf_thres = conf_thres\n        self.nms_thres = nms_thres\n        self.img_size = img_size\n        self.max_batch_size = max_batch_size\n        self.device = device\n\n        # Set up model\n        self.model = Darknet(model_def, img_size=img_size).to(self.device)\n\n        if weights_path.endswith("".weights""):\n            # Load darknet weights\n            self.model.load_darknet_weights(weights_path)\n        else:\n            # Load checkpoint weights\n            self.model.load_state_dict(torch.load(weights_path))\n\n        self.model.eval()  # Set in evaluation mode\n\n        self.classes_file = load_classes(class_path)  # Extracts class labels from file\n        self.classes = classes\n\n        self.classes_id = []\n        for i, c in enumerate(self.classes_file):\n            if c in self.classes:\n                self.classes_id.append(i)\n\n    def predict_single(self, image, color_mode=\'BGR\'):\n        return self.predict(np.expand_dims(image.copy(), axis=0), color_mode=color_mode)[0]\n\n    def predict(self, images, color_mode=\'BGR\'):\n        images_rescaled = prepare_data(images.copy(), color_mode=color_mode)\n        with torch.no_grad():\n            images_rescaled = images_rescaled.to(self.device)\n\n            if len(images_rescaled) <= self.max_batch_size:\n                detections = self.model(images_rescaled)\n            else:\n                detections = torch.empty((images_rescaled.shape[0], 10647, 85)).to(self.device)\n                for i in range(0, len(images_rescaled), self.max_batch_size):\n                    detections[i:i + self.max_batch_size] = self.model(images_rescaled[i:i + self.max_batch_size]).detach()\n\n            detections = non_max_suppression(detections, self.conf_thres, self.nms_thres)\n            for i in range(len(images)):\n                if detections[i] is not None:\n                    detections[i] = filter_classes(detections[i], self.classes_id)\n                    detections[i] = scale_coords(detections[i], images_rescaled[i].shape[1:], images[i].shape[:2])\n\n            return detections\n'"
