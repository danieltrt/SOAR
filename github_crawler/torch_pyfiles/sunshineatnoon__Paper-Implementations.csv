file_path,api_count,code
BEGAN/began.py,17,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom models import Discriminator\nfrom models import Generator\nfrom data.dataset import CelebA\nimport numpy as np\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--batchSize\', type=int, default=16, help=\'input batch size\')\nparser.add_argument(\'--loadSize\', type=int, default=64, help=\'the height / width of the input image to network\')\nparser.add_argument(\'--fineSize\', type=int, default=64, help=\'the height / width of the input image to network\')\nparser.add_argument(\'--flip\', type=int, default=1, help=\'1 for flipping image randomly, 0 for not\')\nparser.add_argument(\'--nz\', type=int, default=64, help=\'size of the latent z vector\')\nparser.add_argument(\'--ngf\', type=int, default=64)\nparser.add_argument(\'--ndf\', type=int, default=64)\nparser.add_argument(\'--niter\', type=int, default=300000, help=\'number of iterations to train for\')\nparser.add_argument(\'--lr\', type=float, default=0.0001, help=\'learning rate, default=0.0002\')\nparser.add_argument(\'--beta1\', type=float, default=0.5, help=\'beta1 for adam. default=0.5\')\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\')\nparser.add_argument(\'--outf\', default=\'dcgan/\', help=\'folder to output images and model checkpoints\')\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'--dataPath\', default=\'data/CelebA/images/\', help=\'which dataset to train on\')\nparser.add_argument(\'--lambda_k\', type=float, default=0.001, help=\'learning rate of k\')\nparser.add_argument(\'--gamma\', type=float, default=0.5, help=\'balance bewteen D and G\')\nparser.add_argument(\'--save_step\', type=int, default=10000, help=\'save weights every 50000 iterations \')\nparser.add_argument(\'--hidden_size\', type=int, default=64, help=\'bottleneck dimension of Discriminator\')\nparser.add_argument(\'--lr_decay_every\', type=int, default=3000, help=\'decay lr this many iterations\')\n\nopt = parser.parse_args()\nprint(opt)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ncudnn.benchmark = True\n\n###############   DATASET   ##################\ndataset = CelebA(opt.dataPath,opt.loadSize,opt.fineSize,opt.flip)\nloader_ = torch.utils.data.DataLoader(dataset=dataset,\n                                           batch_size=opt.batchSize,\n                                           shuffle=True,\n                                           num_workers=2)\nloader = iter(loader_)\n\n###############   MODEL   ####################\ncriterion = nn.L1Loss()\nndf = opt.ndf\nngf = opt.ngf\nnc = 3\n\nnetD = Discriminator(nc, ndf, opt.hidden_size,opt.fineSize)\nnetG = Generator(nc, ngf, opt.nz,opt.fineSize)\nif(opt.cuda):\n    netD.cuda()\n    netG.cuda()\n\n###########   LOSS & OPTIMIZER   ##########\noptimizerD = torch.optim.Adam(netD.parameters(),lr=opt.lr, betas=(opt.beta1, 0.999))\noptimizerG = torch.optim.Adam(netG.parameters(),lr=opt.lr, betas=(opt.beta1, 0.999))\n\n##########   GLOBAL VARIABLES   ###########\nnoise = torch.FloatTensor(opt.batchSize, opt.nz, 1, 1)\nreal = torch.FloatTensor(opt.batchSize, nc, opt.fineSize, opt.fineSize)\nlabel = torch.FloatTensor(1)\n\nnoise = Variable(noise)\nreal = Variable(real)\nlabel = Variable(label)\nif(opt.cuda):\n    noise = noise.cuda()\n    real = real.cuda()\n    label = label.cuda()\n    criterion.cuda()\n\n########### Training   ###########\ndef adjust_learning_rate(optimizer, niter):\n    """"""Sets the learning rate to the initial LR decayed by 10 every 30 epochs""""""\n    lr = opt.lr * (0.95 ** (niter // opt.lr_decay_every))\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return optimizer\n\nk = 0 \nfor iteration in range(1,opt.niter+1):\n    try:\n        images = loader.next()\n    except StopIteration:\n        loader = iter(loader_)\n        images = loader.next()\n    netD.zero_grad()\n\n    real.data.resize_(images.size()).copy_(images)\n\n    # generate fake data\n    noise.data.resize_(images.size(0), opt.nz)\n    noise.data.uniform_(-1,1)\n    fake = netG(noise)\n\n    fake_recons = netD(fake.detach())\n    real_recons = netD(real)\n\n    err_real = torch.mean(torch.abs(real_recons-real))\n    err_fake = torch.mean(torch.abs(fake_recons-fake))\n\n    errD = err_real - k*err_fake\n    errD.backward()\n    optimizerD.step()\n\n    netG.zero_grad()\n    fake = netG(noise)\n    fake_recons = netD(fake)\n    errG = torch.mean(torch.abs(fake_recons-fake))\n    errG.backward()\n    optimizerG.step()\n\n    balance = (opt.gamma * err_real - err_fake).data[0]\n    k = min(max(k + opt.lambda_k * balance,0),1)\n    measure = err_real.data[0] + np.abs(balance)\n    ########### Logging #########\n    print(\'[%d/%d] Loss_D: %.4f Loss_G: %.4f Measure: %.4f K: %.4f LR: %.8f\'\n              % (iteration, opt.niter, \n                 errD.data[0], errG.data[0], measure, k, optimizerD.param_groups[0][\'lr\']))\n\n    ########### Learning Rate Decay #########\n    optimizerD = adjust_learning_rate(optimizerD,iteration)\n    optimizerG = adjust_learning_rate(optimizerG,iteration)\n    ########## Visualize #########\n    if(iteration % 1000 == 0):\n        vutils.save_image(fake.data,\n                    \'%s/fake_samples_iteration_%03d.png\' % (opt.outf, iteration),\n                    normalize=True)\n        vutils.save_image(real.data,\n                    \'%s/real_samples_iteration_%03d.png\' % (opt.outf, iteration),\n                    normalize=True)\n        vutils.save_image(real_recons.data,\n                    \'%s/real_recons_samples_iteration_%03d.png\' % (opt.outf, iteration),\n                    normalize=True)\n\n    if(iteration % opt.save_step == 0):\n        torch.save(netG.state_dict(), \'%s/netG_%d.pth\' % (opt.outf,iteration))\n        torch.save(netD.state_dict(), \'%s/netD_%d.pth\' % (opt.outf,iteration))\n'"
BEGAN/generate.py,8,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom models import Generator\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--batchSize\', type=int, default=64, help=\'input batch size\')\nparser.add_argument(\'--imageSize\', type=int, default=128, help=\'the height / width of the input image to network\')\nparser.add_argument(\'--nz\', type=int, default=128, help=\'size of the latent z vector\')\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\')\nparser.add_argument(\'--netG\', default=\'\', help=""path to netG (to continue training)"")\nparser.add_argument(\'--outf\', default=\'dcgan/\', help=\'folder to output images and model checkpoints\')\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'--ngf\', type=int, default=128)\n\nopt = parser.parse_args()\nprint(opt)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ncudnn.benchmark = True\n\n###########   Load netG   ###########\nassert opt.netG != \'\', ""netG must be provided!""\nnc = 3\nnetG = Generator(nc, opt.ngf, opt.nz, opt.imageSize)\nnetG.load_state_dict(torch.load(opt.netG))\n\n###########   Generate   ###########\nnoise = torch.FloatTensor(opt.batchSize, opt.nz)\nnoise = Variable(noise)\n\nif(opt.cuda):\n    netG.cuda()\n    noise = noise.cuda()\n\nnoise.data.uniform_(-1,1)\nfake = netG(noise)\nvutils.save_image(fake.data,\n            \'%s/samples.png\' % (opt.outf),\n            normalize=True)\n'"
BEGAN/models.py,1,"b'import torch.nn as nn\n\ndef conv_block(in_dim,out_dim):\n    return nn.Sequential(nn.Conv2d(in_dim,in_dim,kernel_size=3,stride=1,padding=1),\n                         nn.ELU(True),\n                         nn.Conv2d(in_dim,in_dim,kernel_size=3,stride=1,padding=1),\n                         nn.ELU(True),\n                         nn.Conv2d(in_dim,out_dim,kernel_size=1,stride=1,padding=0),\n                         nn.AvgPool2d(kernel_size=2,stride=2))\ndef deconv_block(in_dim,out_dim):\n    return nn.Sequential(nn.Conv2d(in_dim,out_dim,kernel_size=3,stride=1,padding=1),\n                         nn.ELU(True),\n                         nn.Conv2d(out_dim,out_dim,kernel_size=3,stride=1,padding=1),\n                         nn.ELU(True),\n                         nn.UpsamplingNearest2d(scale_factor=2))\n\nclass Discriminator(nn.Module):\n    def __init__(self,nc,ndf,hidden_size,imageSize):\n        super(Discriminator,self).__init__()\n        # 64 x 64 \n        self.conv1 = nn.Sequential(nn.Conv2d(nc,ndf,kernel_size=3,stride=1,padding=1),\n                                    nn.ELU(True),\n                                    conv_block(ndf,ndf))\n        # 32 x 32 \n        self.conv2 = conv_block(ndf, ndf*2)\n        # 16 x 16 \n        self.conv3 = conv_block(ndf*2, ndf*3)\n        if(imageSize == 64):\n            # 8 x 8\n            self.conv4 = nn.Sequential(nn.Conv2d(ndf*3,ndf*3,kernel_size=3,stride=1,padding=1),\n                             nn.ELU(True),\n                             nn.Conv2d(ndf*3,ndf*3,kernel_size=3,stride=1,padding=1),\n                             nn.ELU(True)) \n            self.embed1 = nn.Linear(ndf*3*8*8, hidden_size)\n        else:\n            self.conv4 = conv_block(ndf*3, ndf*4)\n            self.conv5 = nn.Sequential(nn.Conv2d(ndf*4,ndf*4,kernel_size=3,stride=1,padding=1),\n                             nn.ELU(True),\n                             nn.Conv2d(ndf*4,ndf*4,kernel_size=3,stride=1,padding=1),\n                             nn.ELU(True)) \n            self.embed1 = nn.Linear(ndf*4*8*8, hidden_size)\n        self.embed2 = nn.Linear(hidden_size, ndf*8*8)\n\n        # 8 x 8\n        self.deconv1 = deconv_block(ndf, ndf)\n        # 16 x 16\n        self.deconv2 = deconv_block(ndf, ndf)\n        # 32 x 32\n        self.deconv3 = deconv_block(ndf, ndf)\n        if(imageSize == 64):\n        # 64 x 64\n            self.deconv4 = nn.Sequential(nn.Conv2d(ndf,ndf,kernel_size=3,stride=1,padding=1),\n                             nn.ELU(True),\n                             nn.Conv2d(ndf,ndf,kernel_size=3,stride=1,padding=1),\n                             nn.ELU(True),\n                             nn.Conv2d(ndf, nc, kernel_size=3, stride=1, padding=1))\n        else:\n            self.deconv4 = deconv_block(ndf, ndf)\n            self.deconv5 = nn.Sequential(nn.Conv2d(ndf,ndf,kernel_size=3,stride=1,padding=1),\n                             nn.ELU(True),\n                             nn.Conv2d(ndf,ndf,kernel_size=3,stride=1,padding=1),\n                             nn.ELU(True),\n                             nn.Conv2d(ndf, nc, kernel_size=3, stride=1, padding=1),\n                             nn.Tanh())\n\n\tself.ndf = ndf\n        self.imageSize = imageSize\n\n    def forward(self,x):\n        out = self.conv1(x)\n        out = self.conv2(out)\n        out = self.conv3(out)\n        out = self.conv4(out)\n        if(self.imageSize == 128):\n            out = self.conv5(out)\n            out = out.view(out.size(0), self.ndf*4 * 8 * 8)\n        else:\n            out = out.view(out.size(0), self.ndf*3 * 8 * 8)\n        out = self.embed1(out)\n        out = self.embed2(out)\n        out = out.view(out.size(0), self.ndf, 8, 8)\n        out = self.deconv1(out)\n        out = self.deconv2(out)\n        out = self.deconv3(out)\n        out = self.deconv4(out)\n        if(self.imageSize == 128):\n            out = self.deconv5(out)\n        return out\n\nclass Generator(nn.Module):\n    def __init__(self,nc,ngf,nz,imageSize):\n        super(Generator,self).__init__()\n        self.embed1 = nn.Linear(nz, ngf*8*8)\n\n        # 8 x 8\n        self.deconv1 = deconv_block(ngf, ngf)\n        # 16 x 16\n        self.deconv2 = deconv_block(ngf, ngf)\n        # 32 x 32\n        self.deconv3 = deconv_block(ngf, ngf)\n        if(imageSize == 128):\n            self.deconv4 = deconv_block(ngf, ngf)\n            # 128 x 128 \n            self.deconv5 = nn.Sequential(nn.Conv2d(ngf,ngf,kernel_size=3,stride=1,padding=1),\n                             nn.ELU(True),\n                             nn.Conv2d(ngf,ngf,kernel_size=3,stride=1,padding=1),\n                             nn.ELU(True),\n                             nn.Conv2d(ngf, nc, kernel_size=3, stride=1, padding=1))\n        else:\n            self.deconv4 = nn.Sequential(nn.Conv2d(ngf,ngf,kernel_size=3,stride=1,padding=1),\n                             nn.ELU(True),\n                             nn.Conv2d(ngf,ngf,kernel_size=3,stride=1,padding=1),\n                             nn.ELU(True),\n                             nn.Conv2d(ngf, nc, kernel_size=3, stride=1, padding=1),\n                             nn.Tanh())\n        self.ngf = ngf\n        self.imageSize = imageSize\n\n    def forward(self,x):\n        out = self.embed1(x)\n        out = out.view(out.size(0), self.ngf, 8, 8)\n        out = self.deconv1(out)\n        out = self.deconv2(out)\n        out = self.deconv3(out)\n        out = self.deconv4(out)\n        if(self.imageSize == 128):\n            out = self.deconv5(out)\n        return out\n'"
DiscoGAN/DiscoGAN.py,19,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom itertools import chain\n\nfrom utils.dataset import DATASET\nfrom model.Discriminator import Discriminator\nfrom model.Generator import Generator\n\nparser = argparse.ArgumentParser(description=\'train pix2pix model\')\nparser.add_argument(\'--batchSize\', type=int, default=200, help=\'with batchSize=1 equivalent to instance normalization.\')\nparser.add_argument(\'--ngf\', type=int, default=64)\nparser.add_argument(\'--ndf\', type=int, default=64)\nparser.add_argument(\'--niter\', type=int, default=10000, help=\'number of iterations to train for\')\nparser.add_argument(\'--lr\', type=float, default=0.0002, help=\'learning rate, default=0.0002\')\nparser.add_argument(\'--beta1\', type=float, default=0.5, help=\'beta1 for adam. default=0.5\')\nparser.add_argument(\'--weight_decay\', type=float, default=1e-4, help=\'weight decay in network D, default=1e-4\')\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\')\nparser.add_argument(\'--outf\', default=\'checkpoints/\', help=\'folder to output images and model checkpoints\')\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'--dataPath\', default=\'facades/train/\', help=\'path to training images\')\nparser.add_argument(\'--loadSize\', type=int, default=64, help=\'scale image to this size\')\nparser.add_argument(\'--fineSize\', type=int, default=64, help=\'random crop image to this size\')\nparser.add_argument(\'--flip\', type=int, default=1, help=\'1 for flipping image randomly, 0 for not\')\nparser.add_argument(\'--input_nc\', type=int, default=3, help=\'channel number of input image\')\nparser.add_argument(\'--output_nc\', type=int, default=3, help=\'channel number of output image\')\nparser.add_argument(\'--G_AB\', default=\'\', help=\'path to pre-trained G_AB\')\nparser.add_argument(\'--G_BA\', default=\'\', help=\'path to pre-trained G_BA\')\nparser.add_argument(\'--save_step\', type=int, default=5000, help=\'save interval\')\nparser.add_argument(\'--log_step\', type=int, default=100, help=\'log interval\')\n\nopt = parser.parse_args()\nprint(opt)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ncudnn.benchmark = True\n##########   DATASET   ###########\ndatasetA = DATASET(os.path.join(opt.dataPath,\'A\'),opt.loadSize,opt.fineSize,opt.flip)\ndatasetB = DATASET(os.path.join(opt.dataPath,\'B\'),opt.loadSize,opt.fineSize,opt.flip)\nloader_A = torch.utils.data.DataLoader(dataset=datasetA,\n                                       batch_size=opt.batchSize,\n                                       shuffle=True,\n                                       num_workers=2)\nloaderA = iter(loader_A)\nloader_B = torch.utils.data.DataLoader(dataset=datasetB,\n                                       batch_size=opt.batchSize,\n                                       shuffle=True,\n                                       num_workers=2)\nloaderB = iter(loader_B)\n\n###########   MODEL   ###########\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find(\'BatchNorm\') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\nndf = opt.ndf\nngf = opt.ngf\nnc = 3\n\nD_A = Discriminator(opt.input_nc,ndf)\nD_B = Discriminator(opt.output_nc,ndf)\nG_AB = Generator(opt.input_nc, opt.output_nc, opt.ngf)\nG_BA = Generator(opt.output_nc, opt.input_nc, opt.ngf)\n\nif(opt.G_AB != \'\'):\n    print(\'Warning! Loading pre-trained weights.\')\n    G_AB.load_state_dict(torch.load(opt.G_AB))\n    G_BA.load_state_dict(torch.load(opt.G_BA))\nelse:\n    G_AB.apply(weights_init)\n    G_BA.apply(weights_init)\n\nif(opt.cuda):\n    D_A.cuda()\n    D_B.cuda()\n    G_AB.cuda()\n    G_BA.cuda()\n\n\nD_A.apply(weights_init)\nD_B.apply(weights_init)\n\n###########   LOSS & OPTIMIZER   ##########\ncriterionMSE = nn.MSELoss()\ncriterion = nn.BCELoss()\n# chain is used to update two generators simultaneously\noptimizerD = torch.optim.Adam(chain(D_A.parameters(),D_B.parameters()),lr=opt.lr, betas=(opt.beta1, 0.999), weight_decay=opt.weight_decay)\noptimizerG = torch.optim.Adam(chain(G_AB.parameters(),G_BA.parameters()),lr=opt.lr, betas=(opt.beta1, 0.999))\n\n###########   GLOBAL VARIABLES   ###########\ninput_nc = opt.input_nc\noutput_nc = opt.output_nc\nfineSize = opt.fineSize\n\nreal_A = torch.FloatTensor(opt.batchSize, input_nc, fineSize, fineSize)\nreal_B = torch.FloatTensor(opt.batchSize, output_nc, fineSize, fineSize)\nlabel = torch.FloatTensor(opt.batchSize)\n\nreal_A = Variable(real_A)\nreal_B = Variable(real_B)\nlabel = Variable(label)\n\nif(opt.cuda):\n    real_A = real_A.cuda()\n    real_B = real_B.cuda()\n    label = label.cuda()\n    criterion.cuda()\n    criterionMSE.cuda()\n\nreal_label = 1\nfake_label = 0\n\n###########   Testing    ###########\ndef test(niter):\n    loaderA, loaderB = iter(loader_A), iter(loader_B)\n    imgA = loaderA.next()\n    imgB = loaderB.next()\n    real_A.data.resize_(imgA.size()).copy_(imgA)\n    real_B.data.resize_(imgB.size()).copy_(imgB)\n    AB = G_AB(real_A)\n    BA = G_BA(real_B)\n\n    vutils.save_image(AB.data,\n            \'AB_niter_%03d.png\' % (niter),\n            normalize=True)\n    vutils.save_image(BA.data,\n            \'BA_niter_%03d.png\' % (niter),\n            normalize=True)\n\n\n###########   Training   ###########\nD_A.train()\nD_B.train()\nG_AB.train()\nG_BA.train()\nfor iteration in range(1,opt.niter+1):\n    ###########   data  ###########\n    try:\n        imgA = loaderA.next()\n        imgB = loaderB.next()\n    except StopIteration:\n        loaderA, loaderB = iter(loader_A), iter(loader_B)\n        imgA = loaderA.next()\n        imgB = loaderB.next()\n\n    real_A.data.resize_(imgA.size()).copy_(imgA)\n    real_B.data.resize_(imgB.size()).copy_(imgB)\n    label.data.resize_(imgA.size(0))\n\n    ###########   fDx   ###########\n    D_A.zero_grad()\n    D_B.zero_grad()\n\n    # train with real\n    label.data.fill_(real_label)\n    outA = D_A(real_A)\n    outB = D_B(real_B)\n    l_A = criterion(outA, label)\n    l_B = criterion(outB, label)\n    errD_real = l_A + l_B\n    errD_real.backward()\n\n    # train with fake\n    label.data.fill_(fake_label)\n\n    AB = G_AB(real_A)\n    BA = G_BA(real_B)\n    out_BA = D_A(BA.detach())\n    out_AB = D_B(AB.detach())\n\n    l_BA = criterion(out_BA,label)\n    l_AB = criterion(out_AB,label)\n\n    errD_fake = l_BA + l_AB\n    errD_fake.backward()\n\n    errD = errD_real + errD_fake\n    optimizerD.step()\n\n    ########### fGx ###########\n    G_AB.zero_grad()\n    G_BA.zero_grad()\n    label.data.fill_(real_label)\n\n    AB = G_AB(real_A)\n    ABA = G_BA(AB)\n\n    BA = G_BA(real_B)\n    BAB = G_AB(BA)\n\n    out_BA = D_A(BA)\n    out_AB = D_B(AB)\n\n    l_BA = criterion(out_BA,label)\n    l_AB = criterion(out_AB,label)\n\n    # reconstruction loss\n    l_rec_ABA = criterionMSE(ABA, real_A)\n    l_rec_BAB = criterionMSE(BAB, real_B)\n\n    errGAN = l_BA + l_AB\n    errMSE =  l_rec_ABA + l_rec_BAB\n    errG = errGAN + errMSE\n    errG.backward()\n\n    optimizerG.step()\n\n    ###########   Logging   ############\n    if(iteration % opt.log_step):\n        print(\'[%d/%d] Loss_D: %.4f Loss_G: %.4f Loss_MSE: %.4f\'\n                  % (iteration, opt.niter,\n                     errD.data[0], errGAN.data[0], errMSE.data[0]))\n    ########## Visualize #########\n    if(iteration % 1000 == 0):\n        test(iteration)\n\n    if iteration % opt.save_step == 0:\n        torch.save(G_AB.state_dict(), \'{}/G_AB_{}.pth\'.format(opt.outf, iteration))\n        torch.save(G_BA.state_dict(), \'{}/G_BA_{}.pth\'.format(opt.outf, iteration))\n        torch.save(D_A.state_dict(), \'{}/D_A_{}.pth\'.format(opt.outf, iteration))\n        torch.save(D_B.state_dict(), \'{}/D_B_{}.pth\'.format(opt.outf, iteration))\n'"
DiscoGAN/generate.py,11,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\n\nfrom utils.dataset import DATASET \nfrom model.Discriminator import Discriminator\nfrom model.Generator import Generator\n\nparser = argparse.ArgumentParser(description=\'train pix2pix model\')\nparser.add_argument(\'--batchSize\', type=int, default=40, help=\'with batchSize=1 equivalent to instance normalization.\')\nparser.add_argument(\'--ngf\', type=int, default=64)\nparser.add_argument(\'--ndf\', type=int, default=64)\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\')\nparser.add_argument(\'--outf\', default=\'samples/\', help=\'folder to output images and model checkpoints\')\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'--dataPath\', default=\'facades/train/\', help=\'path to training images\')\nparser.add_argument(\'--loadSize\', type=int, default=64, help=\'scale image to this size\')\nparser.add_argument(\'--fineSize\', type=int, default=64, help=\'random crop image to this size\')\nparser.add_argument(\'--flip\', type=int, default=0, help=\'1 for flipping image randomly, 0 for not\')\nparser.add_argument(\'--input_nc\', type=int, default=3, help=\'channel number of input image\')\nparser.add_argument(\'--output_nc\', type=int, default=3, help=\'channel number of output image\')\nparser.add_argument(\'--G_AB\', default=\'\', help=\'path to pre-trained G_AB\')\nparser.add_argument(\'--G_BA\', default=\'\', help=\'path to pre-trained G_BA\')\n\nopt = parser.parse_args()\nprint(opt)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ncudnn.benchmark = True\n##########   DATASET   ###########\ndatasetA = DATASET(os.path.join(opt.dataPath,\'A\'),opt.loadSize,opt.fineSize,opt.flip)\ndatasetB = DATASET(os.path.join(opt.dataPath,\'B\'),opt.loadSize,opt.fineSize,opt.flip)\nloader_A = torch.utils.data.DataLoader(dataset=datasetA,\n                                       batch_size=opt.batchSize,\n                                       shuffle=True,\n                                       num_workers=2)\nloaderA = iter(loader_A)\nloader_B = torch.utils.data.DataLoader(dataset=datasetB,\n                                       batch_size=opt.batchSize,\n                                       shuffle=True,\n                                       num_workers=2)\nloaderB = iter(loader_B)\n###########   MODEL   ###########\nndf = opt.ndf\nngf = opt.ngf\nnc = 3\n\nG_AB = Generator(opt.input_nc, opt.output_nc, opt.ngf)\nG_BA = Generator(opt.output_nc, opt.input_nc, opt.ngf)\n\nif(opt.G_AB != \'\'):\n    print(\'Warning! Loading pre-trained weights.\')\n    G_AB.load_state_dict(torch.load(opt.G_AB))\n    G_BA.load_state_dict(torch.load(opt.G_BA))\nelse:\n    print(\'ERROR! G_AB and G_BA must be provided!\')\n\nif(opt.cuda):\n    G_AB.cuda()\n    G_BA.cuda()\n\n\n###########   GLOBAL VARIABLES   ###########\ninput_nc = opt.input_nc\noutput_nc = opt.output_nc\nfineSize = opt.fineSize\n\nreal_A = torch.FloatTensor(opt.batchSize, input_nc, fineSize, fineSize)\nreal_B = torch.FloatTensor(opt.batchSize, output_nc, fineSize, fineSize)\n\nreal_A = Variable(real_A)\nreal_B = Variable(real_B)\n\nif(opt.cuda):\n    real_A = real_A.cuda()\n    real_B = real_B.cuda()\n\n###########   Testing    ###########\ndef test():\n    imgA = loaderA.next() \n    imgB = loaderB.next()\n    real_A.data.copy_(imgA)\n    real_B.data.copy_(imgB)\n\n    AB = G_AB(real_A)\n    ABA = G_BA(AB)\n\n    vutils.save_image(AB.data,\n            \'%s/AB.png\' % (opt.outf),\n            normalize=True,\n\t    nrow=4)\n    vutils.save_image(ABA.data,\n            \'%s/ABA.png\' % (opt.outf),\n            normalize=True,\n\t    nrow=4)\n    vutils.save_image(imgA,\n            \'%s/A.png\' % (opt.outf),\n            normalize=True,\n\t    nrow=4)\n\n    BA = G_BA(real_B)\n    BAB = G_AB(BA)\n\n    vutils.save_image(BA.data,\n            \'%s/BA.png\' % (opt.outf),\n            normalize=True,\n\t    nrow=4)\n    vutils.save_image(BAB.data,\n            \'%s/BAB.png\' % (opt.outf),\n            normalize=True,\n\t    nrow=4)\n    vutils.save_image(imgB,\n            \'%s/B.png\' % (opt.outf),\n            normalize=True,\n\t    nrow=4)\n\ntest()\n\n'"
NeuralSytleTransfer/train.py,17,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport numpy as np\nimport torchvision.models as models\nfrom vgg import VGG\nimport util\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--cuda"", action=""store_true"", help=\'enables cuda\')\nparser.add_argument(\'--imageSize\', type=int, default=512, help=\'the height / width of the input image to network\')\nparser.add_argument(""--style_image"", default=""images/picasso.jpg"", help=\'path to style image\')\nparser.add_argument(""--content_image"", default=""images/dancing.jpg"", help=\'path to style image\')\nparser.add_argument(""--niter"", type=int, default=100, help=\'number of epochs to train for\')\nparser.add_argument(""--lr"", type=float, default=1e1, help=\'learning rate, default=0.0002\')\nparser.add_argument(""--outf"", default=""images/"", help=\'folder to output images and model checkpoints\')\nparser.add_argument(""--manualSeed"", type=int, help=\'manual seed\')\nparser.add_argument(""--content_weight"", type=int, default=5e0, help=\'content loss weight\')\nparser.add_argument(""--style_weight"", type=int, default=1e2, help=\'style loss weight\')\nparser.add_argument(""--content_layers"", default=""r42"", help=\'layers for content\')\nparser.add_argument(""--style_layers"", default=""r11,r21,r31,r41,r51"", help=\'layers for style\')\nparser.add_argument(""--vgg_dir"", default=""models/vgg_conv.pth"", help=\'path to pretrained VGG19 net\')\nparser.add_argument(""--color_histogram_matching"", action=""store_true"", help=\'using histogram matching to preserve color in content image\')\nparser.add_argument(""--luminance_only"", action=""store_true"", help=\'perform neural style transfer only on luminance to preserve color in content image\')\nparser.add_argument(""--BNMatching"", action=""store_true"", help=\'use BN matching instead of Gram Matrix as style loss\')\n\nopt = parser.parse_args()\n# turn content layers and style layers to a list\nopt.content_layers = opt.content_layers.split(\',\')\nopt.style_layers = opt.style_layers.split(\',\')\nprint(opt)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ncudnn.benchmark = True\n\n###############   DATASET   ##################\ntransform = transforms.Compose([\n    transforms.Scale(opt.imageSize),\n    transforms.ToTensor(),\n    transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n    transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961],std=[1,1,1]),\n    transforms.Lambda(lambda x: x.mul_(255)),\n    ])\ndef load_image(path,style=False):\n    img = Image.open(path)\n    img = Variable(transform(img))\n    img = img.unsqueeze(0)\n    return img\n\ndef save_image(img):\n    post = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n         transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], std=[1,1,1]),\n         transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n         ])\n    img = post(img)\n    img = img.clamp_(0,1)\n    vutils.save_image(img,\n                \'%s/transfer.png\' % (opt.outf),\n                normalize=True)\n    return\n\nif(opt.color_histogram_matching):\n    styleImg = transform(util.open_and_resize_image(opt.style_image,256)) # 1x3x512x512\n    contentImg = transform(util.open_and_resize_image(opt.content_image,256)) # 1x3x512x512\n    styleImg = styleImg.unsqueeze(0)\n    contentImg = contentImg.unsqueeze(0)\n\n    styleImg = util.match_color_histogram(styleImg.numpy(),contentImg.numpy())\n    styleImg = Variable(torch.from_numpy(styleImg))\n    contentImg = Variable(contentImg)\nelif(opt.luminance_only):\n    styleImg = transform(util.open_and_resize_image(opt.style_image,256)) # 1x3x512x512\n    contentImg = transform(util.open_and_resize_image(opt.content_image,256)) # 1x3x512x512\n    styleImg = styleImg.unsqueeze(0)\n    contentImg = contentImg.unsqueeze(0)\n    styleImg,contentImg,content_iq = util.luminance_transfer(styleImg.numpy(),contentImg.numpy())\n    styleImg = Variable(torch.from_numpy(styleImg))\n    contentImg = Variable(torch.from_numpy(contentImg))\nelse:\n    styleImg = load_image(opt.style_image) # 1x3x512x512\n    contentImg = load_image(opt.content_image) # 1x3x512x512\n\nif(opt.cuda):\n    styleImg = styleImg.cuda()\n    contentImg = contentImg.cuda()\n\n###############   MODEL   ####################\nvgg = VGG()\nvgg.load_state_dict(torch.load(opt.vgg_dir))\nfor param in vgg.parameters():\n    param.requires_grad = False\nif(opt.cuda):\n    vgg.cuda()\n###########   LOSS & OPTIMIZER   ##########\nclass GramMatrix(nn.Module):\n    def forward(self,input):\n        b, c, h, w = input.size()\n        f = input.view(b,c,h*w) # bxcx(hxw)\n        # torch.bmm(batch1, batch2, out=None)   #\n        # batch1: bxmxp, batch2: bxpxn -> bxmxn #\n        G = torch.bmm(f,f.transpose(1,2)) # f: bxcx(hxw), f.transpose: bx(hxw)xc -> bxcxc\n        return G.div_(h*w)\n\nclass styleLoss(nn.Module):\n    def forward(self,input,target):\n        GramInput = GramMatrix()(input)\n        return nn.MSELoss()(GramInput,target)\n\nclass BNMatching(nn.Module):\n    # A style loss by aligning the BN statistics (mean and standard deviation)\n    # of two feature maps between two images. Details can be found in\n    # https://arxiv.org/abs/1701.01036\n    def FeatureMean(self,input):\n        b,c,h,w = input.size()\n        f = input.view(b,c,h*w) # bxcx(hxw)\n        return torch.mean(f,dim=2)\n    def FeatureStd(self,input):\n        b,c,h,w = input.size()\n        f = input.view(b,c,h*w) # bxcx(hxw)\n        return torch.std(f, dim=2)\n    def forward(self,input,target):\n        # input: 1 x c x H x W\n        mu_input = self.FeatureMean(input)\n        mu_target = self.FeatureMean(target)\n        std_input = self.FeatureStd(input)\n        std_target = self.FeatureStd(target)\n        return nn.MSELoss()(mu_input,mu_target) + nn.MSELoss()(std_input,std_target)\n\nstyleTargets = []\nfor t in vgg(styleImg,opt.style_layers):\n    t = t.detach()\n    if(opt.BNMatching):\n        styleTargets.append(t)\n    else:\n        styleTargets.append(GramMatrix()(t))\ncontentTargets = []\nfor t in vgg(contentImg,opt.content_layers):\n    t = t.detach()\n    contentTargets.append(t)\n\nif(opt.BNMatching):\n    styleLosses = [BNMatching()] * len(opt.style_layers)\nelse:\n    styleLosses = [styleLoss()] * len(opt.style_layers)\ncontentLosses = [nn.MSELoss()] * len(opt.content_layers)\n\n# summary style and content loss so that we only need to go through the vgg once to get\n# all style losses and content losses\nlosses = styleLosses + contentLosses\ntargets = styleTargets + contentTargets\nloss_layers = opt.style_layers + opt.content_layers\nweights = [opt.style_weight]*len(opt.style_layers) + [opt.content_weight]*len(opt.content_layers)\n\noptImg = Variable(contentImg.data.clone(), requires_grad=True)\noptimizer = optim.LBFGS([optImg]);\n\n# shift everything to cuda if possible\nif(opt.cuda):\n    for loss in losses:\n        loss = loss.cuda()\n    optImg.cuda()\n###########   TRAINING   ##########\n\nfor iteration in range(1,opt.niter+1):\n    print(\'Iteration [%d]/[%d]\'%(iteration,opt.niter))\n    def closure():\n        optimizer.zero_grad()\n        out = vgg(optImg,loss_layers)\n        totalLossList = []\n        for i in range(len(out)):\n            layer_output = out[i]\n            loss_i = losses[i]\n            target_i = targets[i]\n            totalLossList.append(loss_i(layer_output,target_i) * weights[i])\n        totalLoss = sum(totalLossList)\n        totalLoss.backward()\n        print(\'loss: %f\'%(totalLoss.data[0]))\n        return totalLoss\n    optimizer.step(closure)\noutImg = optImg.data[0].cpu()\nif(opt.luminance_only):\n    outImg = np.expand_dims(outImg.numpy(),0)\n    outImg = util.join_yiq_to_bgr(outImg,content_iq)\n    save_image(torch.from_numpy(outImg).squeeze())\nelse:\n    save_image(outImg.squeeze())\n'"
NeuralSytleTransfer/util.py,0,"b""from __future__ import print_function\nfrom PIL import Image\nimport numpy as np\nimport scipy.misc\nfrom matplotlib import pyplot as plt\nimport six\nimport colorsys\n\ndef open_and_resize_image(path, target_width):\n    image = Image.open(path).convert('RGB')\n    width, height = image.size\n    target_height = int(round(float(height * target_width) / width))\n    image = image.resize((target_width, target_height), Image.BILINEAR)\n    return image\n\ndef match_color_histogram(x, y):\n    z = np.zeros_like(x)\n    shape = x[0].shape\n    for i in six.moves.range(len(x)):\n        a = x[i].reshape((3, -1))\n        a_mean = np.mean(a, axis=1, keepdims=True)\n        a_var = np.cov(a)\n        d, v = np.linalg.eig(a_var)\n        d += 1e-6\n        a_sigma_inv = v.dot(np.diag(d ** (-0.5))).dot(v.T)\n\n        b = y[i].reshape((3, -1))\n        b_mean = np.mean(b, axis=1, keepdims=True)\n        b_var = np.cov(b)\n        d, v = np.linalg.eig(b_var)\n        b_sigma = v.dot(np.diag(d ** 0.5)).dot(v.T)\n\n        transform = b_sigma.dot(a_sigma_inv)\n        z[i,:] = (transform.dot(a - a_mean) + b_mean).reshape(shape)\n    return z\n\ndef bgr_to_yiq(x):\n    transform = np.asarray([[0.114, 0.587, 0.299], [-0.322, -0.274, 0.596], [0.312, -0.523, 0.211]], dtype=np.float32)\n    n, c, h, w = x.shape\n    x = x.transpose((1, 0, 2, 3)).reshape((c, -1))\n    x = transform.dot(x)\n    return x.reshape((c, n, h, w)).transpose((1, 0, 2, 3))\n\ndef yiq_to_bgr(x):\n    transform = np.asarray([[1, -1.106, 1.703], [1, -0.272, -0.647], [1, 0.956, 0.621]], dtype=np.float32)\n    n, c, h, w = x.shape\n    x = x.transpose((1, 0, 2, 3)).reshape((c, -1))\n    x = transform.dot(x)\n    return x.reshape((c, n, h, w)).transpose((1, 0, 2, 3))\n\ndef split_bgr_to_yiq(x):\n    x = bgr_to_yiq(x)\n    y = x[:,0:1,:,:]\n    iq = x[:,1:,:,:]\n    return np.repeat(y, 3, axis=1), iq\n\ndef join_yiq_to_bgr(y, iq):\n    y = bgr_to_yiq(y)[:,0:1,:,:]\n    return yiq_to_bgr(np.concatenate((y, iq), axis=1))\n\ndef luminance_transfer(x,y):\n    # x: style, y:content\n    x_l, x_iq = split_bgr_to_yiq(x) # 1x3x512x512\n    y_l, y_iq = split_bgr_to_yiq(y)\n\n    x_l_mean = np.mean(x_l)\n    y_l_mean = np.mean(y_l)\n    x_l_std = np.std(x_l)\n    y_l_std = np.std(y_l)\n\n    x_l = (y_l_std/x_l_std)*(x_l - x_l_mean) + y_l_mean\n    return x_l, y_l, y_iq\n"""
NeuralSytleTransfer/vgg.py,2,"b""#vgg definition that conveniently let's you grab the outputs from any layer\n#credits: https://github.com/leongatys/PytorchNeuralStyleTransfer\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass VGG(nn.Module):\n    def __init__(self, pool='max'):\n        super(VGG, self).__init__()\n        #vgg modules\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        if pool == 'max':\n            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n        elif pool == 'avg':\n            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x, out_keys):\n        out = {}\n        out['r11'] = F.relu(self.conv1_1(x))\n        out['r12'] = F.relu(self.conv1_2(out['r11']))\n        out['p1'] = self.pool1(out['r12'])\n        out['r21'] = F.relu(self.conv2_1(out['p1']))\n        out['r22'] = F.relu(self.conv2_2(out['r21']))\n        out['p2'] = self.pool2(out['r22'])\n        out['r31'] = F.relu(self.conv3_1(out['p2']))\n        out['r32'] = F.relu(self.conv3_2(out['r31']))\n        out['r33'] = F.relu(self.conv3_3(out['r32']))\n        out['r34'] = F.relu(self.conv3_4(out['r33']))\n        out['p3'] = self.pool3(out['r34'])\n        out['r41'] = F.relu(self.conv4_1(out['p3']))\n        out['r42'] = F.relu(self.conv4_2(out['r41']))\n        out['r43'] = F.relu(self.conv4_3(out['r42']))\n        out['r44'] = F.relu(self.conv4_4(out['r43']))\n        out['p4'] = self.pool4(out['r44'])\n        out['r51'] = F.relu(self.conv5_1(out['p4']))\n        out['r52'] = F.relu(self.conv5_2(out['r51']))\n        out['r53'] = F.relu(self.conv5_3(out['r52']))\n        out['r54'] = F.relu(self.conv5_4(out['r53']))\n        out['p5'] = self.pool5(out['r54'])\n        return [out[key] for key in out_keys]\n"""
VAE/vae.py,12,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nimport numpy as np\nfrom torch.autograd import Variable\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--batchSize\', type=int, default=64, help=\'input batch size\')\nparser.add_argument(\'--imageSize\', type=int, default=28, help=\'the height / width of the input image to network\')\nparser.add_argument(\'--niter\', type=int, default=10, help=\'number of epochs to train for\')\nparser.add_argument(\'--lr\', type=float, default=1e-3, help=\'learning rate, default=0.0002\')\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\')\nparser.add_argument(\'--outf\', default=\'samples/\', help=\'folder to output images and model checkpoints\')\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'--log_interval\', type=int, default=50, help=\'manual seed\')\nparser.add_argument(\'--hidden_size\', type=int, default=20, help=\'size of z\')\n\nopt = parser.parse_args()\nprint(opt)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ncudnn.benchmark = True\n\n###############   DATASET   ##################\ndataset = dset.MNIST(root = \'../data/\',\n                     transform=transforms.Compose([\n                           transforms.ToTensor(),\n                       ]),\n                      download = True)\n\n\nloader = torch.utils.data.DataLoader(dataset = dataset,\n                                     batch_size = opt.batchSize,\n                                     shuffle = True)\n\n###############   MODEL   ##################\nclass VAE(nn.Module):\n    def __init__(self):\n        super(VAE,self).__init__()\n        # 28 x 28\n        n = 64\n        self.conv1 = nn.Sequential(nn.Conv2d(1,n,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(n),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 14 x 14\n        self.conv2 = nn.Sequential(nn.Conv2d(n,n*2,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(n*2),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 7 x 7\n        self.conv3 = nn.Sequential(nn.Conv2d(n*2,n,kernel_size=3,stride=1,padding=1),\n                                 nn.BatchNorm2d(n),\n                                 nn.LeakyReLU(0.2,inplace=True))\n\n        self.fc11 = nn.Linear(n * 7 * 7, opt.hidden_size)\n        self.fc12 = nn.Linear(n * 7 * 7, opt.hidden_size)\n        self.fc2 = nn.Linear(opt.hidden_size, n * 7 * 7)\n\n        self.deconv1 = nn.Sequential(nn.ConvTranspose2d(n,n,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(n),\n                                 nn.ReLU())\n        # 14 x 14\n        self.deconv2 = nn.Sequential(nn.ConvTranspose2d(n,1,kernel_size=4,stride=2,padding=1),\n                                 nn.Sigmoid())\n        # 28 x 28\n\n    def encoder(self, x):\n        # input: noise output: mu and sigma\n        # opt.batchSize x 1 x 28 x 28\n        out = self.conv1(x)\n        # opt.batchSize x n x 14 x 14\n        out = self.conv2(out)\n        # opt.batchSize x n x 7 x 7\n        out = self.conv3(out)\n        return self.fc11(out.view(out.size(0),-1)),self.fc12(out.view(out.size(0),-1))\n\n    def sampler(self, mu, logvar):\n        var = logvar.mul(0.5).exp_()\n        eps = torch.FloatTensor(var.size()).normal_()\n        eps = Variable(eps)\n        if(opt.cuda):\n            eps = eps.cuda()\n        return eps.mul(var).add_(mu)\n\n    def decoder(self, x):\n        out = self.fc2(x)\n        out = self.deconv1(out.view(x.size(0), 64, 7, 7))\n        # opt.batchSize x n x 7 x 7\n        out = self.deconv2(out)\n        # opt.batchSize x n x 14 x 14\n        return out\n        # opt.batchSize x 1 x 28 x 28\n\n    def forward(self, x):\n        mu, logvar = self.encoder(x)\n        out = self.sampler(mu, logvar)\n        out = self.decoder(out)\n        return out, mu, logvar\n\nmodel = VAE()\nif(opt.cuda):\n    model.cuda()\n###########   LOSS & OPTIMIZER   ##########\nbce = nn.BCELoss()\nbce.size_average = False\nif(opt.cuda):\n    bce.cuda()\ndef LossFunction(out, target, mu, logvar):\n    bceloss = bce(out, target)\n    kld = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n    kldloss = torch.sum(kld).mul_(-0.5)\n    return bceloss + kldloss\noptimizer = optim.Adam(model.parameters(),lr=opt.lr)\n\n##########   GLOBAL VARIABLES   ###########\ndata = torch.Tensor(opt.batchSize, opt.imageSize * opt.imageSize)\ndata = Variable(data)\nif(opt.cuda):\n    data = data.cuda()\n###############   TRAINING   ##################\ndef sample(epoch):\n    model.eval()\n    eps = torch.FloatTensor(opt.batchSize, opt.hidden_size).normal_()\n    eps = Variable(eps)\n    if(opt.cuda):\n        eps = eps.cuda()\n    fake = model.decoder(eps)\n    vutils.save_image(fake.data.resize_(opt.batchSize,1,opt.imageSize,opt.imageSize),\n                \'%s/fake_samples_epoch_%03d.png\' % (opt.outf, epoch),\n                normalize=True)\n\ndef sample2d(epoch):\n    model.eval()\n    eps = torch.FloatTensor(400, opt.hidden_size)\n    nx = ny = 20\n    x_values = np.linspace(-3, 3, nx)\n    y_values = np.linspace(-3, 3, ny)\n    for i in range(nx):\n        for j in range(ny):\n            eps[i*20+j][0] = x_values[i]\n            eps[i*20+j][1] = y_values[j]\n\n    eps = Variable(eps)\n    if(opt.cuda):\n        eps = eps.cuda()\n    fake = model.decoder(eps)\n    vutils.save_image(fake.data.resize_(400,1,opt.imageSize,opt.imageSize),\n                \'%s/fake_samples_epoch_%03d.png\' % (opt.outf, epoch),\n                normalize=True,\n                nrow=20)\ndef train(epoch):\n    model.train()\n    for i, (images,_) in enumerate(loader):\n        model.zero_grad()\n        data.data.resize_(images.size()).copy_(images)\n        recon, mu, logvar = model(data)\n        loss = LossFunction(recon, data, mu, logvar)\n        loss.backward()\n        optimizer.step()\n        if i % opt.log_interval == 0:\n            sample(epoch)\n            print(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, i * len(data), len(loader.dataset),\n                100. * i / len(loader),\n                loss.data[0] / len(data)))\n\nfor epoch in range(1, opt.niter + 1):\n    train(epoch)\nif(opt.hidden_size == 2):\n    sample2d(epoch)\n'"
char-rnn/train.py,9,"b'import matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport time, math\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nimport string\nimport random\nimport codecs\nimport re\nimport argparse\n\n# Hyperparameters\nparser = argparse.ArgumentParser(description=\'PyTorch Char RNN\')\nparser.add_argument(\'--chunk_len\', type=int, default=100, metavar=\'N\',\n                    help=\'split characters into chunks for training (default: 100)\')\nparser.add_argument(\'--epochs\', type=int, default=4000, metavar=\'N\',\n                    help=\'number of epochs to train (default: 2000)\')\nparser.add_argument(\'--lr\', type=float, default=0.005, metavar=\'LR\',\n                    help=\'learning rate (default: 0.005)\')\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                    help=\'enables CUDA training\')\nparser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                    help=\'random seed (default: 1)\')\nparser.add_argument(\'--log-interval\', type=int, default=50, metavar=\'N\',\n                    help=\'how many batches to wait before logging training status\')\nparser.add_argument(\'--hidden_size\', type=int, default=100, metavar=\'N\',\n                    help=\'hidden size of RNN (default:100)\')\nparser.add_argument(\'--n_layers\', type=int, default=1, metavar=\'N\',\n                    help=\'layer number of RNN (default: 1)\')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\n#############  DATASET   ################\n# Dictionary\nall_characters = string.printable\nn_characters = len(all_characters)\n\n# load training data\nfile = codecs.open(""input.txt"", ""r"", ""utf-8"").read()\nprint(\'Found %d chracters.\'%(len(file)))\n\n# turn a string into a tensor\ndef char_tensor(s):\n    l = len(s)\n    t = torch.LongTensor(l)\n    for i in range(l):\n        t[i] = all_characters.index(s[i])\n\n    return Variable(t)\n\n# generate a random chunk\ndef random_chunk(file,chunk_len):\n    start_idx = random.randint(0,len(file) - chunk_len)\n    end_idx = start_idx + chunk_len + 1\n    return file[start_idx:end_idx]\n\ndef random_training_set(file, chunk_len):\n    chunk = random_chunk(file, chunk_len)\n    inp = char_tensor(chunk[:-1])\n    target = char_tensor(chunk[1:])\n    return inp, target\n\n##############   Helper Functions   ##############\ndef time_since(since):\n    s = time.time() - since\n    m = math.floor(s / 60)\n    s -= m * 60\n    return \'%dm %ds\' % (m, s)\n\n##############   RNN   ###################\nclass RNN(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size, n_layers):\n        super(RNN,self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.n_layers = n_layers\n\n        self.encoder = nn.Embedding(input_size, hidden_size)\n        self.rnn = nn.GRU(hidden_size, hidden_size, n_layers)\n        self.decoder = nn.Linear(hidden_size, output_size)\n\n    def forward(self,x,hidden):\n        embedding = self.encoder(x.view(1,-1))\n        output, hidden = self.rnn(embedding.view(1,1,-1),hidden)\n        output = self.decoder(output.view(1,-1))\n        return output, hidden\n\n    def init_hidden(self):\n        return Variable(torch.Tensor(n_layers, 1, hidden_size).fill_(0))\n\n###############   training/evaluating   ###############\ndef evaluate(prime_str=\'A\', predict_len=100, temperature=0.5):\n    hidden = decoder.init_hidden()\n    predicted = prime_str\n    prime_tensor = char_tensor(prime_str)\n\n    # feed prime_str to RNN\n    for i in range(len(prime_str)-1):\n        _,hidden = decoder(prime_tensor[i], hidden)\n\n    # generate a string of length predict_len\n    inp = prime_tensor[-1]\n    for i in range(predict_len):\n        output, hidden = decoder(inp, hidden)\n\n        # sample a multi_nominal Gaussian to get different character each time\n        output_dist = output.data.view(-1).div(temperature).exp()\n        idx = torch.multinomial(output_dist, 1)[0]\n        character = all_characters[idx]\n        predicted += character\n\n        inp = char_tensor(character)\n\n    return predicted\n\ndef train(inp, target):\n    hidden = decoder.init_hidden()\n    decoder.zero_grad()\n    loss = 0\n\n    # forward\n    for i in range(len(inp)):\n        output, hidden = decoder(inp[i], hidden)\n        loss += criterion(output, target[i])\n\n    # backward\n    loss.backward()\n    decoder_optimizer.step()\n\n    return loss.data[0]/len(inp)\n\n###############   Main Script   #################\nn_epochs = args.epochs\nprint_every = args.log_interval\nplot_every = 10\nhidden_size = args.hidden_size\nn_layers = args.n_layers\nlr = args.lr\nchunk_len = args.chunk_len\n\ndecoder = RNN(n_characters, hidden_size, n_characters, n_layers)\ndecoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\n\nstart = time.time()\nall_losses = []\nloss_avg = 0\n\nfor epoch in range(1, n_epochs + 1):\n    loss = train(*random_training_set(file, chunk_len))\n    loss_avg += loss\n\n    if epoch % print_every == 0:\n        print(\'[%s (%d %d%%) %.4f]\' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n        print(evaluate(\'Wh\', 100), \'\\n\')\n\n    if epoch % plot_every == 0:\n        all_losses.append(loss_avg / plot_every)\n        loss_avg = 0\n\n# plot losses\nplt.figure()\nplt.plot(all_losses)\nplt.show()\n\n# evaluate\nprint(evaluate(\'Th\', 200, temperature=0.5))\n'"
classification/mnist.py,9,"b""import torch\nimport torch.nn as nn\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nimport torchnet.meter as meter\nimport time\nimport argparse\nfrom model.CNN import CNN\nfrom model.NIN import NIN\nfrom model.ResNet import ResNet, ResidualBlock\n\n############  Hyper Parameters   ############\n# Training settings\nparser = argparse.ArgumentParser(description='PyTorch MNIST Example')\nparser.add_argument('--batch-size', type=int, default=100, metavar='N',\n                    help='input batch size for training (default: 64)')\nparser.add_argument('--epochs', type=int, default=5, metavar='N',\n                    help='number of epochs to train (default: 5)')\nparser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n                    help='learning rate (default: 0.01)')\nparser.add_argument('--no-cuda', action='store_true', default=False,\n                    help='enables CUDA training')\nparser.add_argument('--seed', type=int, default=1, metavar='S',\n                    help='random seed (default: 1)')\nparser.add_argument('--log-interval', type=int, default=50, metavar='N',\n                    help='how many batches to wait before logging training status')\nparser.add_argument('--network', type=str, default='CNN', metavar='N',\n                    help='which model to use, CNN|NIN|ResNet')\nargs = parser.parse_args()\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\n#############  DATASET   ################\ntrain_dataset = datasets.MNIST(root = '../data/',\n                               train = True,\n                               transform = transforms.ToTensor(),\n                               download = True)\n\ntest_dataset = datasets.MNIST(root = '../data/',\n                              train = False,\n                              transform = transforms.ToTensor(),\n                              download = True)\n\ntrain_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                           batch_size = args.batch_size,\n                                           shuffle = True)\n\ntest_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n                                           batch_size = args.batch_size,\n                                           shuffle = False)\n\n###############   Model   ##################\nif(args.network == 'CNN'):\n    cnn = CNN()\nelif(args.network == 'NIN'):\n    cnn = NIN()\nelif(args.network == 'ResNet'):\n    cnn = ResNet(ResidualBlock, [2, 2, 2, 2])\nif not args.no_cuda:\n    cnn.cuda()\nprint(cnn)\n\n################   Loss   #################\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(cnn.parameters(),lr=args.lr)\nmtr = meter.ConfusionMeter(k=10)\n\n################   Training   #############\ndef train(epoch):\n    cnn.train()\n\n    for i , (images,labels) in enumerate(train_loader):\n        if not args.no_cuda:\n            images = images.cuda()\n            labels = labels.cuda()\n\n        images = Variable(images)\n        labels = Variable(labels)\n\n        # forward\n        optimizer.zero_grad()\n        outputs = cnn(images)\n\n        loss = criterion(outputs,labels)\n\n        # backward\n        loss.backward()\n        optimizer.step()\n        if (i+1) % args.log_interval == 0:\n            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f'\n                   %(epoch+1, 5, i+1, len(train_dataset)//args.batch_size, loss.data[0]))\n\ndef test():\n    cnn.eval()\n\n    # training data test\n    for images,labels in train_loader:\n        if not args.no_cuda:\n            images = images.cuda()\n        images = Variable(images)\n\n        # forward\n        outputs = cnn(images)\n        mtr.add(outputs.data, labels)\n\n    trainacc = mtr.value().diagonal().sum()*1.0/len(train_dataset)\n    mtr.reset()\n\n    # testing data test\n    for images,labels in test_loader:\n        if not args.no_cuda:\n            images = images.cuda()\n        images = Variable(images)\n\n        # forward\n        outputs = cnn(images)\n        mtr.add(outputs.data, labels)\n\n    testacc = mtr.value().diagonal().sum()*1.0/len(test_dataset)\n    mtr.reset()\n\n    # logging\n    print('Accuracy on training data is: %f . Accuracy on testing data is: %f. '%(trainacc, testacc) )\n\n##################   Main   ##################\nfor epoch in range(args.epochs):\n    train(epoch)\n    test()\ntorch.save(cnn.state_dict(), 'cnn.pkl')\n"""
cycleGAN/CycleGAN.py,22,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom itertools import chain\n\nfrom utils.dataset import DATASET\nfrom utils.ImagePool import ImagePool\nfrom model.Discriminator import Discriminator\nfrom model.Generator import Generator\n\nparser = argparse.ArgumentParser(description=\'train pix2pix model\')\nparser.add_argument(\'--batchSize\', type=int, default=1, help=\'with batchSize=1 equivalent to instance normalization.\')\nparser.add_argument(\'--ngf\', type=int, default=64)\nparser.add_argument(\'--ndf\', type=int, default=64)\nparser.add_argument(\'--niter\', type=int, default=40000, help=\'number of iterations to train for\')\nparser.add_argument(\'--lr\', type=float, default=0.0002, help=\'learning rate, default=0.0002\')\nparser.add_argument(\'--beta1\', type=float, default=0.5, help=\'beta1 for adam. default=0.5\')\nparser.add_argument(\'--weight_decay\', type=float, default=1e-4, help=\'weight decay in network D, default=1e-4\')\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\')\nparser.add_argument(\'--outf\', default=\'checkpoints/\', help=\'folder to output images and model checkpoints\')\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'--dataPath\', default=\'facades/train/\', help=\'path to training images\')\nparser.add_argument(\'--loadSize\', type=int, default=143, help=\'scale image to this size\')\nparser.add_argument(\'--fineSize\', type=int, default=128, help=\'random crop image to this size\')\nparser.add_argument(\'--flip\', type=int, default=1, help=\'1 for flipping image randomly, 0 for not\')\nparser.add_argument(\'--input_nc\', type=int, default=3, help=\'channel number of input image\')\nparser.add_argument(\'--output_nc\', type=int, default=3, help=\'channel number of output image\')\nparser.add_argument(\'--G_AB\', default=\'\', help=\'path to pre-trained G_AB\')\nparser.add_argument(\'--G_BA\', default=\'\', help=\'path to pre-trained G_BA\')\nparser.add_argument(\'--save_step\', type=int, default=20000, help=\'save interval\')\nparser.add_argument(\'--log_step\', type=int, default=100, help=\'log interval\')\nparser.add_argument(\'--loss_type\', default=\'mse\', help=\'GAN loss type, bce|mse default is negative likelihood loss\')\nparser.add_argument(\'--poolSize\', type=int, default=50, help=\'size of buffer in lsGAN, poolSize=0 indicates not using history\')\nparser.add_argument(\'--lambda_ABA\', type=float, default=10.0, help=\'weight of cycle loss ABA\')\nparser.add_argument(\'--lambda_BAB\', type=float, default=10.0, help=\'weight of cycle loss BAB\')\n\nopt = parser.parse_args()\nprint(opt)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ncudnn.benchmark = True\n##########   DATASET   ###########\ndatasetA = DATASET(os.path.join(opt.dataPath,\'A\'),opt.loadSize,opt.fineSize,opt.flip)\ndatasetB = DATASET(os.path.join(opt.dataPath,\'B\'),opt.loadSize,opt.fineSize,opt.flip)\nloader_A = torch.utils.data.DataLoader(dataset=datasetA,\n                                       batch_size=opt.batchSize,\n                                       shuffle=True,\n                                       num_workers=2)\nloaderA = iter(loader_A)\nloader_B = torch.utils.data.DataLoader(dataset=datasetB,\n                                       batch_size=opt.batchSize,\n                                       shuffle=True,\n                                       num_workers=2)\nloaderB = iter(loader_B)\nABPool = ImagePool(opt.poolSize)\nBAPool = ImagePool(opt.poolSize)\n###########   MODEL   ###########\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find(\'BatchNorm\') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\nndf = opt.ndf\nngf = opt.ngf\nnc = 3\n\nD_A = Discriminator(opt.input_nc,ndf)\nD_B = Discriminator(opt.output_nc,ndf)\nG_AB = Generator(opt.input_nc, opt.output_nc, opt.ngf)\nG_BA = Generator(opt.output_nc, opt.input_nc, opt.ngf)\n\nif(opt.G_AB != \'\'):\n    print(\'Warning! Loading pre-trained weights.\')\n    G_AB.load_state_dict(torch.load(opt.G_AB))\n    G_BA.load_state_dict(torch.load(opt.G_BA))\nelse:\n    G_AB.apply(weights_init)\n    G_BA.apply(weights_init)\n\nif(opt.cuda):\n    D_A.cuda()\n    D_B.cuda()\n    G_AB.cuda()\n    G_BA.cuda()\n\n\nD_A.apply(weights_init)\nD_B.apply(weights_init)\n\n###########   LOSS & OPTIMIZER   ##########\ncriterionMSE = nn.L1Loss()\nif(opt.loss_type == \'bce\'):\n    criterion = nn.BCELoss()\nelse:\n    criterion = nn.MSELoss()\n# chain is used to update two generators simultaneously\noptimizerD_A = torch.optim.Adam(D_A.parameters(),lr=opt.lr, betas=(opt.beta1, 0.999), weight_decay=opt.weight_decay)\noptimizerD_B = torch.optim.Adam(D_B.parameters(),lr=opt.lr, betas=(opt.beta1, 0.999), weight_decay=opt.weight_decay)\noptimizerG = torch.optim.Adam(chain(G_AB.parameters(),G_BA.parameters()),lr=opt.lr, betas=(opt.beta1, 0.999))\n\n###########   GLOBAL VARIABLES   ###########\ninput_nc = opt.input_nc\noutput_nc = opt.output_nc\nfineSize = opt.fineSize\n\nreal_A = torch.FloatTensor(opt.batchSize, input_nc, fineSize, fineSize)\nAB = torch.FloatTensor(opt.batchSize, input_nc, fineSize, fineSize)\nreal_B = torch.FloatTensor(opt.batchSize, output_nc, fineSize, fineSize)\nBA = torch.FloatTensor(opt.batchSize, output_nc, fineSize, fineSize)\nlabel = torch.FloatTensor(opt.batchSize)\n\nreal_A = Variable(real_A)\nreal_B = Variable(real_B)\nlabel = Variable(label)\nAB = Variable(AB)\nBA = Variable(BA)\n\nif(opt.cuda):\n    real_A = real_A.cuda()\n    real_B = real_B.cuda()\n    label = label.cuda()\n    AB = AB.cuda()\n    BA = BA.cuda()\n    criterion.cuda()\n    criterionMSE.cuda()\n\nreal_label = 1\nfake_label = 0\n\n###########   Testing    ###########\ndef test(niter):\n    loaderA, loaderB = iter(loader_A), iter(loader_B)\n    imgA = loaderA.next()\n    imgB = loaderB.next()\n    real_A.data.resize_(imgA.size()).copy_(imgA)\n    real_B.data.resize_(imgB.size()).copy_(imgB)\n    AB = G_AB(real_A)\n    BA = G_BA(real_B)\n\n    vutils.save_image(AB.data,\n            \'AB_niter_%03d_1.png\' % (niter),\n            normalize=True)\n    vutils.save_image(BA.data,\n            \'BA_niter_%03d_1.png\' % (niter),\n            normalize=True)\n\n    imgA = loaderA.next()\n    imgB = loaderB.next()\n    real_A.data.resize_(imgA.size()).copy_(imgA)\n    real_B.data.resize_(imgB.size()).copy_(imgB)\n    AB = G_AB(real_A)\n    BA = G_BA(real_B)\n\n    vutils.save_image(AB.data,\n            \'AB_niter_%03d_2.png\' % (niter),\n            normalize=True)\n    vutils.save_image(BA.data,\n            \'BA_niter_%03d_2.png\' % (niter),\n            normalize=True)\n\n\n###########   Training   ###########\nD_A.train()\nD_B.train()\nG_AB.train()\nG_BA.train()\nfor iteration in range(1,opt.niter+1):\n    ###########   data  ###########\n    try:\n        imgA = loaderA.next()\n        imgB = loaderB.next()\n    except StopIteration:\n        loaderA, loaderB = iter(loader_A), iter(loader_B)\n        imgA = loaderA.next()\n        imgB = loaderB.next()\n\n    real_A.data.resize_(imgA.size()).copy_(imgA)\n    real_B.data.resize_(imgB.size()).copy_(imgB)\n\n    ###########   fDx   ###########\n    D_A.zero_grad()\n    D_B.zero_grad()\n\n    # train with real\n    outA = D_A(real_A)\n    outB = D_B(real_B)\n    label.data.resize_(outA.size())\n    label.data.fill_(real_label)\n    l_A = criterion(outA, label)\n    l_B = criterion(outB, label)\n    errD_real = l_A + l_B\n    errD_real.backward()\n\n    # train with fake\n    label.data.fill_(fake_label)\n\n    AB_tmp = G_AB(real_A)\n    AB.data.resize_(AB_tmp.data.size()).copy_(ABPool.Query(AB_tmp.cpu().data))\n    BA_tmp = G_BA(real_B)\n    BA.data.resize_(BA_tmp.data.size()).copy_(BAPool.Query(BA_tmp.cpu().data))\n    \n    out_BA = D_A(BA.detach())\n    out_AB = D_B(AB.detach())\n\n    l_BA = criterion(out_BA,label)\n    l_AB = criterion(out_AB,label)\n\n    errD_fake = l_BA + l_AB\n    errD_fake.backward()\n\n    errD = (errD_real + errD_fake)*0.5\n    optimizerD_A.step()\n    optimizerD_B.step()\n\n    ########### fGx ###########\n    G_AB.zero_grad()\n    G_BA.zero_grad()\n    label.data.fill_(real_label)\n\n    AB = G_AB(real_A)\n    ABA = G_BA(AB)\n\n    BA = G_BA(real_B)\n    BAB = G_AB(BA)\n\n    out_BA = D_A(BA)\n    out_AB = D_B(AB)\n\n    l_BA = criterion(out_BA,label)\n    l_AB = criterion(out_AB,label)\n\n    # reconstruction loss\n    l_rec_ABA = criterionMSE(ABA, real_A) * opt.lambda_ABA\n    l_rec_BAB = criterionMSE(BAB, real_B) * opt.lambda_BAB\n\n    errGAN = l_BA + l_AB\n    errMSE =  l_rec_ABA + l_rec_BAB\n    errG = errGAN + errMSE\n    errG.backward()\n\n    optimizerG.step()\n\n    ###########   Logging   ############\n    if(iteration % opt.log_step):\n        print(\'[%d/%d] Loss_D: %.4f Loss_G: %.4f Loss_MSE: %.4f\'\n                  % (iteration, opt.niter,\n                     errD.data[0], errGAN.data[0], errMSE.data[0]))\n    ########## Visualize #########\n    if(iteration % 1000 == 0):\n        test(iteration)\n\n    if iteration % opt.save_step == 0:\n        torch.save(G_AB.state_dict(), \'{}/G_AB_{}.pth\'.format(opt.outf, iteration))\n        torch.save(G_BA.state_dict(), \'{}/G_BA_{}.pth\'.format(opt.outf, iteration))\n        torch.save(D_A.state_dict(), \'{}/D_A_{}.pth\'.format(opt.outf, iteration))\n        torch.save(D_B.state_dict(), \'{}/D_B_{}.pth\'.format(opt.outf, iteration))\n'"
cycleGAN/generate.py,17,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\n\nfrom utils.dataset import DATASET \nfrom model.Discriminator import Discriminator\nfrom model.Generator import Generator\n\nparser = argparse.ArgumentParser(description=\'train pix2pix model\')\nparser.add_argument(\'--batchSize\', type=int, default=1, help=\'with batchSize=1 equivalent to instance normalization.\')\nparser.add_argument(\'--ngf\', type=int, default=64)\nparser.add_argument(\'--ndf\', type=int, default=64)\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\')\nparser.add_argument(\'--outf\', default=\'samples/\', help=\'folder to output images and model checkpoints\')\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'--dataPath\', default=\'facades/test/\', help=\'path to training images\')\nparser.add_argument(\'--loadSize\', type=int, default=128, help=\'scale image to this size\')\nparser.add_argument(\'--fineSize\', type=int, default=128, help=\'random crop image to this size\')\nparser.add_argument(\'--flip\', type=int, default=0, help=\'1 for flipping image randomly, 0 for not\')\nparser.add_argument(\'--input_nc\', type=int, default=3, help=\'channel number of input image\')\nparser.add_argument(\'--output_nc\', type=int, default=3, help=\'channel number of output image\')\nparser.add_argument(\'--G_AB\', default=\'\', help=\'path to pre-trained G_AB\')\nparser.add_argument(\'--G_BA\', default=\'\', help=\'path to pre-trained G_BA\')\nparser.add_argument(\'--imgNum\', type=int, default=32, help=\'image number\')\n\nopt = parser.parse_args()\nprint(opt)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ncudnn.benchmark = True\n##########   DATASET   ###########\ndatasetA = DATASET(os.path.join(opt.dataPath,\'A\'),opt.loadSize,opt.fineSize,opt.flip)\ndatasetB = DATASET(os.path.join(opt.dataPath,\'B\'),opt.loadSize,opt.fineSize,opt.flip)\nloader_A = torch.utils.data.DataLoader(dataset=datasetA,\n                                       batch_size=opt.batchSize,\n                                       shuffle=True,\n                                       num_workers=2)\nloaderA = iter(loader_A)\nloader_B = torch.utils.data.DataLoader(dataset=datasetB,\n                                       batch_size=opt.batchSize,\n                                       shuffle=True,\n                                       num_workers=2)\nloaderB = iter(loader_B)\n###########   MODEL   ###########\nndf = opt.ndf\nngf = opt.ngf\nnc = 3\n\nG_AB = Generator(opt.input_nc, opt.output_nc, opt.ngf)\nG_BA = Generator(opt.output_nc, opt.input_nc, opt.ngf)\n\nif(opt.G_AB != \'\'):\n    print(\'Warning! Loading pre-trained weights.\')\n    G_AB.load_state_dict(torch.load(opt.G_AB))\n    G_BA.load_state_dict(torch.load(opt.G_BA))\nelse:\n    print(\'ERROR! G_AB and G_BA must be provided!\')\n\nif(opt.cuda):\n    G_AB.cuda()\n    G_BA.cuda()\n\n\n###########   GLOBAL VARIABLES   ###########\ninput_nc = opt.input_nc\noutput_nc = opt.output_nc\nfineSize = opt.fineSize\n\nreal_A = torch.FloatTensor(opt.batchSize, input_nc, fineSize, fineSize)\nreal_B = torch.FloatTensor(opt.batchSize, output_nc, fineSize, fineSize)\n\nreal_A = Variable(real_A)\nreal_B = Variable(real_B)\n\nif(opt.cuda):\n    real_A = real_A.cuda()\n    real_B = real_B.cuda()\n\n###########   Testing    ###########\ndef test():\n    AB_all = torch.Tensor(opt.imgNum, 3, opt.fineSize, opt.fineSize)\n    ABA_all = torch.Tensor(opt.imgNum, 3, opt.fineSize, opt.fineSize)\n    A_all = torch.Tensor(opt.imgNum, 3, opt.fineSize, opt.fineSize)\n    if(opt.cuda):\n\tAB_all = AB_all.cuda()\n\tABA_all = ABA_all.cuda()\n\tA_all = A_all.cuda()\n\n    for i in range(0,opt.imgNum,opt.batchSize):\n        imgA = loaderA.next() \n        imgB = loaderB.next()\n        real_A.data.copy_(imgA)\n        real_B.data.copy_(imgB)\n\n        AB = G_AB(real_A)\n        ABA = G_BA(AB)\n        AB_all[i,:,:,:] = AB.data\n        ABA_all[i,:,:,:] = ABA.data\n        A_all[i,:,:,:] = imgA\n\n\n    vutils.save_image(AB_all,\n            \'%s/AB.png\' % (opt.outf),\n            normalize=True,\n\t    nrow=4)\n    vutils.save_image(ABA_all,\n            \'%s/ABA.png\' % (opt.outf),\n            normalize=True,\n\t    nrow=4)\n    vutils.save_image(A_all,\n            \'%s/A.png\' % (opt.outf),\n            normalize=True,\n\t    nrow=4)\n\n    BA_all = torch.Tensor(opt.imgNum, 3, opt.fineSize, opt.fineSize)\n    BAB_all = torch.Tensor(opt.imgNum, 3, opt.fineSize, opt.fineSize)\n    B_all = torch.Tensor(opt.imgNum, 3, opt.fineSize, opt.fineSize)\n    if(opt.cuda):\n\tBA_all = AB_all.cuda()\n\tBAB_all = ABA_all.cuda()\n\tB_all = A_all.cuda()\n\n    for i in range(0,opt.imgNum,opt.batchSize):\n        imgA = loaderA.next() \n        imgB = loaderB.next()\n        real_A.data.copy_(imgA)\n        real_B.data.copy_(imgB)\n\n        BA = G_BA(real_B)\n        BAB = G_AB(BA)\n        BA_all[i,:,:,:] = BA.data\n        BAB_all[i,:,:,:] = BAB.data\n        B_all[i,:,:,:] = imgB\n\n\n    vutils.save_image(BA_all,\n            \'%s/BA.png\' % (opt.outf),\n            normalize=True,\n\t    nrow=4)\n    vutils.save_image(BAB_all,\n            \'%s/BAB.png\' % (opt.outf),\n            normalize=True,\n\t    nrow=4)\n    vutils.save_image(B_all,\n            \'%s/B.png\' % (opt.outf),\n            normalize=True,\n\t    nrow=4)\n\ntest()\n\n'"
dcgan/dcgan.py,14,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom model.Discriminator import Discriminator\nfrom model.Generator import Generator\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--batchSize\', type=int, default=64, help=\'input batch size\')\nparser.add_argument(\'--imageSize\', type=int, default=32, help=\'the height / width of the input image to network\')\nparser.add_argument(\'--nz\', type=int, default=100, help=\'size of the latent z vector\')\nparser.add_argument(\'--ngf\', type=int, default=64)\nparser.add_argument(\'--ndf\', type=int, default=64)\nparser.add_argument(\'--niter\', type=int, default=25, help=\'number of epochs to train for\')\nparser.add_argument(\'--lr\', type=float, default=0.0002, help=\'learning rate, default=0.0002\')\nparser.add_argument(\'--beta1\', type=float, default=0.5, help=\'beta1 for adam. default=0.5\')\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\')\nparser.add_argument(\'--outf\', default=\'dcgan/\', help=\'folder to output images and model checkpoints\')\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'--dataset\', default=\'CIFAR\', help=\'which dataset to train on, CIFAR|MNIST\')\n\nopt = parser.parse_args()\nprint(opt)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ncudnn.benchmark = True\n\n###############   DATASET   ##################\nif(opt.dataset == \'CIFAR\'):\n    dataset = dset.CIFAR10(root=\'../data/\', download=True,\n                           transform=transforms.Compose([\n                                   transforms.Scale(opt.imageSize),\n                                   transforms.ToTensor(),\n                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n                               ]))\nelse:\n    dataset = dset.MNIST(root = \'../data/\',\n                         transform=transforms.Compose([\n                               transforms.Scale(opt.imageSize),\n                               transforms.ToTensor(),\n                           ]),\n                          download = True)\n\n\nloader = torch.utils.data.DataLoader(dataset = dataset,\n                                     batch_size = opt.batchSize,\n                                     shuffle = True)\n\n###############   MODEL   ####################\nndf = opt.ndf\nngf = opt.ngf\nnc = 1\nif(opt.dataset == \'CIFAR\'):\n    nc = 3\n\nnetD = Discriminator(nc, ndf)\nnetG = Generator(nc, ngf, opt.nz)\nif(opt.cuda):\n    netD.cuda()\n    netG.cuda()\n\n###########   LOSS & OPTIMIZER   ##########\ncriterion = nn.BCELoss()\noptimizerD = torch.optim.Adam(netD.parameters(),lr=opt.lr, betas=(opt.beta1, 0.999))\noptimizerG = torch.optim.Adam(netG.parameters(),lr=opt.lr, betas=(opt.beta1, 0.999))\n\n##########   GLOBAL VARIABLES   ###########\nnoise = torch.FloatTensor(opt.batchSize, opt.nz, 1, 1)\nreal = torch.FloatTensor(opt.batchSize, nc, opt.imageSize, opt.imageSize)\nlabel = torch.FloatTensor(opt.batchSize)\nreal_label = 1\nfake_label = 0\n\nnoise = Variable(noise)\nreal = Variable(real)\nlabel = Variable(label)\nif(opt.cuda):\n    noise = noise.cuda()\n    real = real.cuda()\n    label = label.cuda()\n\n########### Training   ###########\nfor epoch in range(1,opt.niter+1):\n    for i, (images,_) in enumerate(loader):\n        ########### fDx ###########\n        netD.zero_grad()\n        # train with real data, resize real because last batch may has less than\n        # opt.batchSize images\n        real.data.resize_(images.size()).copy_(images)\n        label.data.resize_(images.size(0)).fill_(real_label)\n\n        output = netD(real)\n        errD_real = criterion(output, label)\n        errD_real.backward()\n\n        # train with fake data\n        label.data.fill_(fake_label)\n        noise.data.resize_(images.size(0), opt.nz, 1, 1)\n        noise.data.normal_(0,1)\n\n        fake = netG(noise)\n        # detach gradients here so that gradients of G won\'t be updated\n        output = netD(fake.detach())\n        errD_fake = criterion(output,label)\n        errD_fake.backward()\n\n        errD = errD_fake + errD_real\n        optimizerD.step()\n\n        ########### fGx ###########\n        netG.zero_grad()\n        label.data.fill_(real_label)\n        output = netD(fake)\n        errG = criterion(output, label)\n        errG.backward()\n        optimizerG.step()\n\n        ########### Logging #########\n        print(\'[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f \'\n                  % (epoch, opt.niter, i, len(loader),\n                     errD.data[0], errG.data[0]))\n\n        ########## Visualize #########\n        if(i % 50 == 0):\n            vutils.save_image(fake.data,\n                        \'%s/fake_samples_epoch_%03d.png\' % (opt.outf, epoch),\n                        normalize=True)\n\ntorch.save(netG.state_dict(), \'%s/netG.pth\' % (opt.outf))\ntorch.save(netD.state_dict(), \'%s/netD.pth\' % (opt.outf))\n'"
dcgan/generate.py,8,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom model.Generator import Generator\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--batchSize\', type=int, default=64, help=\'input batch size\')\nparser.add_argument(\'--imageSize\', type=int, default=32, help=\'the height / width of the input image to network\')\nparser.add_argument(\'--nz\', type=int, default=100, help=\'size of the latent z vector\')\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\')\nparser.add_argument(\'--netG\', default=\'\', help=""path to netG (to continue training)"")\nparser.add_argument(\'--outf\', default=\'dcgan/\', help=\'folder to output images and model checkpoints\')\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'--ngf\', type=int, default=64)\nparser.add_argument(\'--dataset\', default=\'CIFAR\', help=\'which dataset to train on, CIFAR|MNIST\')\n\nopt = parser.parse_args()\nprint(opt)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ncudnn.benchmark = True\n\n###########   Load netG   ###########\nassert opt.netG != \'\', ""netG must be provided!""\nnc = 1\nif(opt.dataset == \'CIFAR\'):\n    nc = 3\nnetG = Generator(nc, opt.ngf, opt.nz)\nnetG.load_state_dict(torch.load(opt.netG))\n\n###########   Generate   ###########\nnoise = torch.FloatTensor(opt.batchSize, opt.nz, 1, 1)\nnoise = Variable(noise)\n\nif(opt.cuda):\n    netG.cuda()\n    noise = noise.cuda()\n\nnoise.data.normal_(0,1)\nfake = netG(noise)\nvutils.save_image(fake.data,\n            \'%s/samples.png\' % (opt.outf),\n            normalize=True)\n'"
fast-neural-style/test.py,8,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom utils.transformer import TransformerNet \nfrom PIL import Image\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--imageSize\', type=int, default=256, help=\'the height / width of the input image to network\')\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\')\nparser.add_argument(\'--transform_net\', default=\'\', help=""path to the transformer net"")\nparser.add_argument(\'--outf\', default=\'images/\', help=\'folder to output images and model checkpoints\')\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(""--content_image"", default=""images/dancing.jpg"", help=\'path to style image\')\n\nopt = parser.parse_args()\nprint(opt)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ncudnn.benchmark = True\n\n###########   Load content image   ###########\ntransform = transforms.Compose([\n    transforms.Scale(opt.imageSize),\n    transforms.CenterCrop(opt.imageSize),\n    transforms.ToTensor(),\n    transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n    transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961],std=[1,1,1]),\n    transforms.Lambda(lambda x: x.mul_(255)),\n    ])\n\ndef load_image(path,style=False):\n    img = Image.open(path)\n    img = Variable(transform(img))\n    img = img.unsqueeze(0)\n    return img\n\ncontentImg = load_image(opt.content_image) # 1x3x512x512\n###########   Load netG   ###########\nassert opt.transform_net != \'\', ""transformer net must be provided!""\ncnn = TransformerNet()\ncnn.load_state_dict(torch.load(opt.transform_net))\n\nif(opt.cuda):\n    cnn.cuda()\n    contentImg = contentImg.cuda()\n\ntransffered = cnn(contentImg)\ntransffered = transffered.clamp(0,255)\nvutils.save_image(transffered.data,\n            \'%s/fast_neural_transfer.png\' % (opt.outf),\n            normalize=True)\n'"
fast-neural-style/train.py,13,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom PIL import Image\nimport numpy as np\nimport torchvision.models as models\n\nfrom utils.vgg import VGG\nfrom utils.transformer import TransformerNet\n\nparser = argparse.ArgumentParser()\nparser.add_argument(""--style_image"", default=""images/picasso.jpg"", help=\'path to style image\')\nparser.add_argument(""--outf"", default=""images/"", help=\'folder to output images and model checkpoints\')\nparser.add_argument(""--dataPath"", default=""data/"", help=\'folder to training image\')\nparser.add_argument(""--content_layers"", default=""r33"", help=\'layers for content\')\nparser.add_argument(""--style_layers"", default=""r12,r22,r33,r43"", help=\'layers for style\')\nparser.add_argument(""--batchSize"", type=int,default=4, help=\'batch size\')\nparser.add_argument(""--niter"", type=int,default=40000, help=\'iterations to train the model\')\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\')\nparser.add_argument(\'--imageSize\', type=int, default=256, help=\'image size\')\nparser.add_argument(""--vgg_dir"", default=""models/vgg_conv.pth"", help=\'path to pretrained VGG19 net\')\nparser.add_argument(""--lr"", type=float, default=1e-3, help=\'learning rate, default=0.0002\')\nparser.add_argument(""--content_weight"", type=float, default=1.0, help=\'content loss weight\')\nparser.add_argument(""--style_weight"", type=float, default=5.0, help=\'style loss weight\')\nparser.add_argument(""--save_image_every"", type=int, default=5, help=\'save transferred image every this much times\')\nopt = parser.parse_args()\n\n# turn content layers and style layers to a list\nopt.content_layers = opt.content_layers.split(\',\')\nopt.style_layers = opt.style_layers.split(\',\')\nprint(opt)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ncudnn.benchmark = True\n\n\n###############   DATASET   ##################\n\ntransform = transforms.Compose([\n    transforms.Scale(opt.imageSize),\n    transforms.CenterCrop(opt.imageSize),\n    transforms.ToTensor(),\n    transforms.Lambda(lambda x: x.mul_(255)),\n    ])\n\ntrain_dataset = datasets.ImageFolder(opt.dataPath, transform)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                            batch_size=opt.batchSize,\n                            shuffle=True,\n                            num_workers=2)\nloader = iter(train_loader)\n\n# style image\ndef load_image(path,style=False):\n    img = Image.open(path)\n    img = Variable(transform(img))\n    img = img.unsqueeze(0)\n    return img\n\nstyleImg = load_image(opt.style_image) # 1x3x512x512\nif(opt.cuda):\n    styleImg = styleImg.cuda()\n\n###########   MODEL   ###########\n## pre-trained VGG net\nvgg = VGG()\nvgg.load_state_dict(torch.load(opt.vgg_dir))\nfor param in vgg.parameters():\n    param.requires_grad = False\n\n## transformer net\ncnn = TransformerNet()\n\nif(opt.cuda):\n    vgg.cuda()\n    cnn.cuda()\n\n###########   LOSS & OPTIMIZER   ##########\nclass GramMatrix(nn.Module):\n    def forward(self,input):\n        b, c, h, w = input.size()\n        f = input.view(b,c,h*w) # bxcx(hxw)\n        # torch.bmm(batch1, batch2, out=None)   #\n        # batch1: bxmxp, batch2: bxpxn -> bxmxn #\n        G = torch.bmm(f,f.transpose(1,2)) # f: bxcx(hxw), f.transpose: bx(hxw)xc -> bxcxc\n        return G.div_(c*h*w)\n\nclass styleLoss(nn.Module):\n    def forward(self,input,target):\n        GramInput = GramMatrix()(input)\n        return nn.MSELoss()(GramInput,target)\n\nstyleTargets = []\nfor t in vgg(styleImg,opt.style_layers):\n    t = t.detach()\n    temp = GramMatrix()(t)\n    temp = temp.repeat(opt.batchSize,1,1,1)\n    styleTargets.append(temp)\nstyleLosses = [styleLoss()] * len(opt.style_layers)\ncontentLosses = [nn.MSELoss()] * len(opt.content_layers)\nlosses = styleLosses + contentLosses\n\nloss_layers = opt.style_layers + opt.content_layers\nweights = [opt.style_weight]*len(opt.style_layers) + [opt.content_weight]*len(opt.content_layers)\n\noptimizer = optim.Adam(cnn.parameters(), opt.lr)\n\n# shift everything to cuda if possible\nif(opt.cuda):\n    for loss in losses:\n        loss = loss.cuda()\n\n###########   GLOBAL VARIABLES   ###########\nimages = torch.FloatTensor(opt.batchSize, 3, opt.imageSize, opt.imageSize)\nimages = Variable(images)\nif(opt.cuda):\n    images = images.cuda()\n\n###########   Training   ###########\nfor iteration in range(1,opt.niter+1):\n    optimizer.zero_grad()\n    try:\n        img,_ =  loader.next()\n    except StopIteration:\n        loader = iter(train_loader)\n        img,_ = loader.next()\n    if(img.size(0) < opt.batchSize):\n        continue\n\n    images.data.resize_(img.size()).copy_(img)\n\n    optImg = cnn(images)\n\n    contentTargets = []\n    for t in vgg(images,opt.content_layers):\n        t = t.detach()\n        contentTargets.append(t)\n    targets = styleTargets + contentTargets\n\n    out = vgg(optImg, loss_layers)\n    totalLoss = 0\n    for i in range(len(out)):\n        layer_output = out[i]\n        loss_i = losses[i]\n        target_i = targets[i]\n        totalLoss += loss_i(layer_output,target_i) * weights[i]\n    totalLoss.backward()\n    print(\'loss: %f\'%(totalLoss.data[0]))\n    optimizer.step()\n\n    # save transffered image\n    if(iteration % opt.save_image_every == 0):\n        optImg = optImg.clamp(0,255)\n        vutils.save_image(optImg.data,\n            \'%s/transffered.png\' % (opt.outf),\n            normalize=True)\ntorch.save(cnn.state_dict(), \'%s/transform_net.pth\' % (opt.outf))\n'"
pix2pix/generate.py,11,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom model.Generator import Generator\nfrom utils.dataset import Facades\n\nparser = argparse.ArgumentParser(description=\'test pix2pix model\')\nparser.add_argument(\'--batchSize\', type=int, default=1, help=\'input batch size\')\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\')\nparser.add_argument(\'--netG\', default=\'\', help=""path to netG (to continue training)"")\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'--loadSize\', type=int, default=256, help=\'scale image to this size\')\nparser.add_argument(\'--fineSize\', type=int, default=256, help=\'random crop image to this size\')\nparser.add_argument(\'--input_nc\', type=int, default=3, help=\'channel number of input image\')\nparser.add_argument(\'--output_nc\', type=int, default=3, help=\'channel number of output image\')\nparser.add_argument(\'--flip\', type=int, default=0, help=\'1 for flipping image randomly, 0 for not\')\nparser.add_argument(\'--dataPath\', default=\'facades/test/\', help=\'path to training images\')\nparser.add_argument(\'--which_direction\', default=\'AtoB\', help=\'AtoB or BtoA\')\nparser.add_argument(\'--outf\', default=\'samples/\', help=\'folder to output images and model checkpoints\')\nparser.add_argument(\'--ngf\', type=int, default=64)\nparser.add_argument(\'--imgNum\', type=int, default=32, help=\'How many images to generate?\')\n\nopt = parser.parse_args()\nprint(opt)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ncudnn.benchmark = True\n\n###########   Load netG   ###########\nassert opt.netG != \'\', ""netG must be provided!""\nnetG = Generator(opt.input_nc, opt.output_nc, opt.ngf)\nnetG.load_state_dict(torch.load(opt.netG))\n###########   Generate   ###########\nfacades = Facades(opt.dataPath,opt.loadSize,opt.fineSize,opt.flip)\ntrain_loader = torch.utils.data.DataLoader(dataset=facades,\n                                           batch_size=opt.batchSize,\n                                           shuffle=True,\n                                           num_workers=2)\nfakeB = torch.FloatTensor(opt.imgNum, opt.output_nc, opt.fineSize, opt.fineSize)\nA = torch.FloatTensor(opt.imgNum, opt.output_nc, opt.fineSize, opt.fineSize)\nrealB = torch.FloatTensor(opt.imgNum, opt.output_nc, opt.fineSize, opt.fineSize)\n\nif(opt.cuda):\n    netG.cuda()\n    fakeB = fakeB.cuda()\n    A = A.cuda()\n\nfor i, image in enumerate(train_loader):\n    if(opt.which_direction == \'AtoB\'):\n        imgA = image[1]\n        imgB = image[0]\n    else:\n        imgA = image[0]\n        imgB = image[1]\n    imgA = Variable(imgA)\n    if(opt.cuda):\n        imgA = imgA.cuda()\n    fake = netG(imgA)\n\n    fakeB[i,:,:,:] = fake.data\n    A[i,:,:,:] = imgA.data\n    realB[i,:,:,:] = imgB\n\n    if(i+1 >= opt.imgNum):\n        break\n\nvutils.save_image(fakeB,\n            \'%s/fakeB.png\' % (opt.outf),\n            normalize=True,\n            scale_each=True)\nvutils.save_image(A,\n            \'%s/A.png\' % (opt.outf),\n            normalize=True,\n            scale_each=True)\nvutils.save_image(realB,\n            \'%s/realB.png\' % (opt.outf),\n            normalize=True,\n            scale_each=True)\n'"
pix2pix/train.py,16,"b'from __future__ import print_function\nimport argparse\nimport os\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\n\nfrom utils.dataset import Facades\nfrom model.Discriminator import Discriminator\nfrom model.Generator import Generator\n\n\nparser = argparse.ArgumentParser(description=\'train pix2pix model\')\nparser.add_argument(\'--batchSize\', type=int, default=1, help=\'with batchSize=1 equivalent to instance normalization.\')\nparser.add_argument(\'--ngf\', type=int, default=64)\nparser.add_argument(\'--ndf\', type=int, default=64)\nparser.add_argument(\'--niter\', type=int, default=200, help=\'number of epochs to train for\')\nparser.add_argument(\'--lr\', type=float, default=0.0002, help=\'learning rate, default=0.0002\')\nparser.add_argument(\'--beta1\', type=float, default=0.5, help=\'beta1 for adam. default=0.5\')\nparser.add_argument(\'--cuda\', action=\'store_true\', help=\'enables cuda\')\nparser.add_argument(\'--outf\', default=\'checkpoints/\', help=\'folder to output images and model checkpoints\')\nparser.add_argument(\'--manualSeed\', type=int, help=\'manual seed\')\nparser.add_argument(\'--dataPath\', default=\'facades/train/\', help=\'path to training images\')\nparser.add_argument(\'--loadSize\', type=int, default=286, help=\'scale image to this size\')\nparser.add_argument(\'--fineSize\', type=int, default=256, help=\'random crop image to this size\')\nparser.add_argument(\'--flip\', type=int, default=1, help=\'1 for flipping image randomly, 0 for not\')\nparser.add_argument(\'--input_nc\', type=int, default=3, help=\'channel number of input image\')\nparser.add_argument(\'--output_nc\', type=int, default=3, help=\'channel number of output image\')\nparser.add_argument(\'--which_direction\', default=\'AtoB\', help=\'AtoB or BtoA\')\nparser.add_argument(\'--lamb\', type=int, default=100, help=\'weight on L1 term in objective\')\n\nopt = parser.parse_args()\nprint(opt)\n\ntry:\n    os.makedirs(opt.outf)\nexcept OSError:\n    pass\n\nif opt.manualSeed is None:\n    opt.manualSeed = random.randint(1, 10000)\nprint(""Random Seed: "", opt.manualSeed)\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nif opt.cuda:\n    torch.cuda.manual_seed_all(opt.manualSeed)\n\ncudnn.benchmark = True\n\n###########   DATASET   ###########\nfacades = Facades(opt.dataPath,opt.loadSize,opt.fineSize,opt.flip)\ntrain_loader = torch.utils.data.DataLoader(dataset=facades,\n                                           batch_size=opt.batchSize,\n                                           shuffle=True,\n                                           num_workers=2)\n\n###########   MODEL   ###########\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find(\'Conv\') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n        m.bias.data.fill_(0)\n    elif classname.find(\'BatchNorm\') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)\n\nndf = opt.ndf\nngf = opt.ngf\nnc = 3\n\nnetD = Discriminator(opt.input_nc,opt.output_nc,ndf)\nnetG = Generator(opt.input_nc, opt.output_nc, opt.ngf)\nif(opt.cuda):\n    netD.cuda()\n    netG.cuda()\n\nnetG.apply(weights_init)\nnetD.apply(weights_init)\nprint(netD)\nprint(netG)\n\n###########   LOSS & OPTIMIZER   ##########\ncriterion = nn.BCELoss()\ncriterionL1 = nn.L1Loss()\noptimizerD = torch.optim.Adam(netD.parameters(),lr=opt.lr, betas=(opt.beta1, 0.999))\noptimizerG = torch.optim.Adam(netG.parameters(),lr=opt.lr, betas=(opt.beta1, 0.999))\n\n###########   GLOBAL VARIABLES   ###########\ninput_nc = opt.input_nc\noutput_nc = opt.output_nc\nfineSize = opt.fineSize\n\nreal_A = torch.FloatTensor(opt.batchSize, input_nc, fineSize, fineSize)\nreal_B = torch.FloatTensor(opt.batchSize, input_nc, fineSize, fineSize)\nlabel = torch.FloatTensor(opt.batchSize)\n\nreal_A = Variable(real_A)\nreal_B = Variable(real_B)\nlabel = Variable(label)\n\nif(opt.cuda):\n    real_A = real_A.cuda()\n    real_B = real_B.cuda()\n    label = label.cuda()\n\nreal_label = 1\nfake_label = 0\n\n########### Training   ###########\nnetD.train()\nnetG.train()\nfor epoch in range(1,opt.niter+1):\n    for i, image in enumerate(train_loader):\n        ########### fDx ###########\n        netD.zero_grad()\n        if(opt.which_direction == \'AtoB\'):\n            imgA = image[1]\n            imgB = image[0]\n        else:\n            imgA = image[0]\n            imgB = image[1]\n\n        # train with real data\n        real_A.data.resize_(imgA.size()).copy_(imgA)\n        real_B.data.resize_(imgB.size()).copy_(imgB)\n        real_AB = torch.cat((real_A, real_B), 1)\n\n\n        output = netD(real_AB)\n        label.data.resize_(output.size())\n        label.data.fill_(real_label)\n        errD_real = criterion(output, label)\n        errD_real.backward()\n\n        # train with fake\n        fake_B = netG(real_A)\n        label.data.fill_(fake_label)\n\n        fake_AB = torch.cat((real_A, fake_B), 1)\n        output = netD(fake_AB.detach())\n        errD_fake = criterion(output,label)\n        errD_fake.backward()\n\n        errD = (errD_fake + errD_real)/2\n        optimizerD.step()\n\n        ########### fGx ###########\n        netG.zero_grad()\n        label.data.fill_(real_label)\n        output = netD(fake_AB)\n        errGAN = criterion(output, label)\n        errL1 = criterionL1(fake_B,real_B)\n        errG = errGAN + opt.lamb*errL1\n\n        errG.backward()\n\n        optimizerG.step()\n\n        ########### Logging ##########\n        if(i % 50 == 0):\n            print(\'[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f Loss_L1: %.4f\'\n                      % (epoch, opt.niter, i, len(train_loader),\n                         errD.data[0], errGAN.data[0], errL1.data[0]))\n\n    ########## Visualize #########\n    if(epoch % 5 == 0):\n        vutils.save_image(fake_B.data,\n                    \'fake_samples_epoch_%03d.png\' % (epoch),\n                    normalize=True)\n\ntorch.save(netG.state_dict(), \'%s/netG.pth\' % (opt.outf))\ntorch.save(netD.state_dict(), \'%s/netD.pth\' % (opt.outf))\n'"
BEGAN/Data/face_detect.py,0,"b""import cv2\nimport os\nimport numpy as np\n\n# Get user supplied values\n# Create the haar cascade\ncascPath = 'haarcascade_frontalface_default.xml'\nfaceCascade = cv2.CascadeClassifier(cascPath)\n\n# Read the image\nfor fn in sorted(os.listdir('../data/CelebA/images')):\n    print fn\n    image = cv2.imread(os.path.abspath('../data/CelebA/images/' + fn))\n    print(os.path.abspath('../data/CelebA/images/' + fn))\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n    faces = faceCascade.detectMultiScale(gray, 5, 5)\n\n    if len(faces) == 0:\n        pass\n    else:\n        x,y,w,h = faces[0]\n        image_crop = image[y:y+w, x:x+w, :]\n        image_resize = cv2.resize(image_crop, (64, 64))\n\n        if not os.path.exists('64_crop/'):\n            os.makedirs('64_crop/')\n        cv2.imwrite('64_crop/' + fn[:-4] + '_crop' + fn[-4:], image_resize)\n\n        if not os.path.exists('128_crop/'):\n            os.makedirs('128_crop/')\n        image_resize = cv2.resize(image_crop, (128, 128))\n        cv2.imwrite('128_crop/' + fn[:-4] + '_crop' + fn[-4:], image_resize)\n\n\n\n\n\n"""
BEGAN/data/__init__.py,0,b''
BEGAN/data/dataset.py,7,"b'import torch\nimport numpy as np\nimport torch.utils.data as data\nfrom os import listdir\nfrom os.path import join\nimport os\nfrom PIL import Image\nimport random\nimport math\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in ["".png"", "".jpg"", "".jpeg""])\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\ndef ToTensor(pic):\n    """"""Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    """"""\n    if isinstance(pic, np.ndarray):\n        # handle numpy array\n        img = torch.from_numpy(pic.transpose((2, 0, 1)))\n        # backard compability\n        return img.float().div(255)\n    # handle PIL Image\n    if pic.mode == \'I\':\n        img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n    elif pic.mode == \'I;16\':\n        img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n    else:\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n    # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n    if pic.mode == \'YCbCr\':\n        nchannel = 3\n    elif pic.mode == \'I;16\':\n        nchannel = 1\n    else:\n        nchannel = len(pic.mode)\n    img = img.view(pic.size[1], pic.size[0], nchannel)\n    # put it from HWC to CHW format\n    # yikes, this transpose takes 80% of the loading time/CPU\n    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n    if isinstance(img, torch.ByteTensor):\n        return img.float().div(255)\n    else:\n        return img\n\n\n# You should build custom dataset as below.\nclass CelebA(data.Dataset):\n    def __init__(self,dataPath=\'data/CelebA/images/\',loadSize=64,fineSize=64,flip=1):\n        super(CelebA, self).__init__()\n        # list all images into a list\n        self.image_list = [x for x in listdir(dataPath) if is_image_file(x)]\n        self.dataPath = dataPath\n        self.loadSize = loadSize\n        self.fineSize = fineSize\n        self.flip = flip\n\n    def __getitem__(self, index):\n        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n        path = os.path.join(self.dataPath,self.image_list[index])\n        img = default_loader(path) \n        w,h = img.size\n\n        if(h != self.loadSize):\n            img = img.resize((self.loadSize, self.loadSize), Image.BILINEAR)\n\n        if(self.loadSize != self.fineSize):\n            #x1 = random.randint(0, self.loadSize - self.fineSize)\n            #y1 = random.randint(0, self.loadSize - self.fineSize)\n             \n            x1 = math.floor((self.loadSize - self.fineSize)/2)\n            y1 = math.floor((self.loadSize - self.fineSize)/2)\n            img = img.crop((x1, y1, x1 + self.fineSize, y1 + self.fineSize))\n\n        if(self.flip == 1):\n            if random.random() < 0.5:\n                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n\n        img = ToTensor(img) # 3 x 256 x 256\n\n        img = img.mul_(2).add_(-1)\n        # 3. Return a data pair (e.g. image and label).\n        return img\n\n    def __len__(self):\n        # You should change 0 to the total size of your dataset.\n        return len(self.image_list)\n\n'"
DiscoGAN/model/Discriminator.py,1,"b'import torch.nn as nn\n\nclass Discriminator(nn.Module):\n    def __init__(self,input_nc,ndf):\n        super(Discriminator,self).__init__()\n        # 64 x 64\n        self.layer1 = nn.Sequential(nn.Conv2d(input_nc,ndf,kernel_size=4,stride=2,padding=1,bias=False),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 32 x 32\n        self.layer2 = nn.Sequential(nn.Conv2d(ndf,ndf*2,kernel_size=4,stride=2,padding=1,bias=False),\n                                 nn.BatchNorm2d(ndf*2),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 16 x 16\n        self.layer3 = nn.Sequential(nn.Conv2d(ndf*2,ndf*4,kernel_size=4,stride=2,padding=1,bias=False),\n                                 nn.BatchNorm2d(ndf*4),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 8 x 8\n        self.layer4 = nn.Sequential(nn.Conv2d(ndf*4,ndf*8,kernel_size=4,stride=2,padding=1,bias=False),\n                                 nn.BatchNorm2d(ndf*8),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 4 x 4\n        self.layer5 = nn.Sequential(nn.Conv2d(ndf*8,1,kernel_size=4,stride=1,padding=0,bias=False),\n                                 nn.Sigmoid())\n        # 1 x 1\n\n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        return out\n\n'"
DiscoGAN/model/Generator.py,1,"b'import torch.nn as nn\n\nclass Generator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf):\n        super(Generator,self).__init__()\n        # 64 x 64\n        self.conv1 = nn.Sequential(nn.Conv2d(input_nc,ngf,kernel_size=4,stride=2,padding=1,bias=False),\n                                 nn.LeakyReLU(0.2, inplace=True))\n        # 32 x 32\n        self.conv2 = nn.Sequential(nn.Conv2d(ngf,ngf*2,kernel_size=4,stride=2,padding=1,bias=False),\n                                 nn.BatchNorm2d(ngf*2),\n                                 nn.LeakyReLU(0.2, inplace=True))\n        # 16 x 16\n        self.conv3 = nn.Sequential(nn.Conv2d(ngf*2,ngf*4,kernel_size=4,stride=2,padding=1,bias=False),\n                                 nn.BatchNorm2d(ngf*4),\n                                 nn.LeakyReLU(0.2, inplace=True))\n        # 8 x 8\n        self.conv4 = nn.Sequential(nn.Conv2d(ngf*4,ngf*8,kernel_size=4,stride=2,padding=1,bias=False),\n                                 nn.BatchNorm2d(ngf*8),\n                                 nn.LeakyReLU(0.2, inplace=True))\n        # 4 x 4\n        self.deconv1 = nn.Sequential(nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n                                     nn.BatchNorm2d(ngf*4),\n                                     nn.ReLU(True))\n        self.deconv2 = nn.Sequential(nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n                                     nn.BatchNorm2d(ngf*2),\n                                     nn.ReLU(True))\n        self.deconv3 = nn.Sequential(nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n                                     nn.BatchNorm2d(ngf),\n                                     nn.ReLU(True))\n        self.deconv4 = nn.Sequential(nn.ConvTranspose2d(ngf, output_nc, 4, 2, 1, bias=False),\n                                     nn.Tanh())\n\n    def forward(self,x):\n        out = self.conv1(x)\n        out = self.conv2(out)\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.deconv1(out)\n        out = self.deconv2(out)\n        out = self.deconv3(out)\n        out = self.deconv4(out)\n        return out\n\n'"
DiscoGAN/model/__init__.py,0,b''
DiscoGAN/scripts/PrepareDataset.py,0,"b""# We use dataset from pix2pix. This is a paired dataset, so we need to split\n# each pair and generate unpaired dataset\n\nimport argparse\nimport os\nfrom PIL import Image\nfrom glob import glob\n\nparser = argparse.ArgumentParser(description='split paired dataset to unpaired dataset')\nparser.add_argument('--dataPath', default='../facades', help='path to dataset folder')\n\n########   make A and B folder   ########\nopt = parser.parse_args()\npaths = glob(opt.dataPath+'/*')\nfor path in paths:\n    if not os.path.exists(os.path.join(path,'A')):\n        os.makedirs(os.path.join(path,'A'))\n    if not os.path.exists(os.path.join(path,'B')):\n        os.makedirs(os.path.join(path,'B'))\n\n########   split images and put them into corresponding folders   ########\ndef isImg(filename):\n    ext = filename.split('.')[-1]\n    if(ext in ['png','jpg','jpeg','ppm']):\n        return True\n\ndef split(imgPath):\n    img = Image.open(imgPath)\n    w,h = img.size\n    imgA = img.crop((0, 0, w/2, h))\n    imgB = img.crop((w/2, 0, w, h))\n    return imgA, imgB\n\nfor path in paths:\n    print('Processing folder: '+path)\n    for fn in os.listdir(path):\n        if(isImg(fn)):\n            imgA,imgB = split(os.path.join(path,fn))\n            imgA.save(os.path.join(path,'A',fn))\n            imgB.save(os.path.join(path,'B',fn))\n\nchoice = raw_input('Delete original paired data? (y/n) WARNING: DELETED DATASET IS UNRECOVERABLE')\nif(choice == 'y'):\n    for path in paths:\n        for fn in os.listdir(path):\n            if(isImg(fn)):\n                os.system('rm '+os.path.join(path,fn))\n"""
DiscoGAN/utils/__init__.py,0,b''
DiscoGAN/utils/dataset.py,7,"b'import torch\nimport numpy as np\nimport torch.utils.data as data\nfrom os import listdir\nfrom os.path import join\nfrom utils import is_image_file\nimport os\nfrom PIL import Image\nimport random\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\ndef ToTensor(pic):\n    """"""Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    """"""\n    if isinstance(pic, np.ndarray):\n        # handle numpy array\n        img = torch.from_numpy(pic.transpose((2, 0, 1)))\n        # backard compability\n        return img.float().div(255)\n    # handle PIL Image\n    if pic.mode == \'I\':\n        img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n    elif pic.mode == \'I;16\':\n        img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n    else:\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n    # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n    if pic.mode == \'YCbCr\':\n        nchannel = 3\n    elif pic.mode == \'I;16\':\n        nchannel = 1\n    else:\n        nchannel = len(pic.mode)\n    img = img.view(pic.size[1], pic.size[0], nchannel)\n    # put it from HWC to CHW format\n    # yikes, this transpose takes 80% of the loading time/CPU\n    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n    if isinstance(img, torch.ByteTensor):\n        return img.float().div(255)\n    else:\n        return img\n\n\n# You should build custom dataset as below.\nclass DATASET(data.Dataset):\n    def __init__(self,dataPath=\'\',loadSize=72,fineSize=64,flip=1):\n        super(DATASET, self).__init__()\n        # list all images into a list\n        self.list = [x for x in listdir(dataPath) if is_image_file(x)]\n        self.dataPath = dataPath\n        self.loadSize = loadSize\n        self.fineSize = fineSize\n        self.flip = flip\n\n    def __getitem__(self, index):\n        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n        path = os.path.join(self.dataPath,self.list[index])\n        img = default_loader(path) # 256x256\n\n        # 2. seperate image A and B; Scale; Random Crop; to Tensor\n        w,h = img.size\n\n        if(h != self.loadSize):\n            img = img.resize((self.loadSize, self.loadSize), Image.BILINEAR)\n\n        if(self.loadSize != self.fineSize):\n            x1 = random.randint(0, self.loadSize - self.fineSize)\n            y1 = random.randint(0, self.loadSize - self.fineSize)\n            img = img.crop((x1, y1, x1 + self.fineSize, y1 + self.fineSize))\n\n        if(self.flip == 1):\n            if random.random() < 0.5:\n                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n\n        img = ToTensor(img) # 3 x 256 x 256\n\n        img = img.mul_(2).add_(-1)\n        # 3. Return a data pair (e.g. image and label).\n        return img\n\n    def __len__(self):\n        # You should change 0 to the total size of your dataset.\n        return len(self.list)\n'"
DiscoGAN/utils/utils.py,0,"b'import numpy as np\nfrom scipy.misc import imread, imresize, imsave\nimport torch\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in ["".png"", "".jpg"", "".jpeg""])\n\ndef deprocess(img):\n    img = img.add_(1).div_(2)\n\n    return img\n'"
classification/model/CNN.py,1,"b'import torch.nn as nn\n\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN,self).__init__()\n        self.layer1 = nn.Sequential(nn.Conv2d(1,16,kernel_size=5,padding=2),\n                                 nn.BatchNorm2d(16),\n                                 nn.ReLU(),\n                                 nn.MaxPool2d(2))\n        self.layer2 = nn.Sequential(nn.Conv2d(16,32,kernel_size=5,padding=2),\n                                 nn.BatchNorm2d(32),\n                                 nn.ReLU(),\n                                 nn.MaxPool2d(2))\n        self.fc = nn.Linear(7*7*32,10)\n\n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(out.size(0),-1)\n        out = self.fc(out)\n        return out\n'"
classification/model/NIN.py,2,"b'import torch.nn as nn\nimport torch\n\nclass NINBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n        super(NINBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels,out_channels,kernel_size=kernel_size,stride=stride, padding=padding)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        residual = x\n        out = self.conv(x)\n        out = self.bn(out)\n        out = self.relu(out)\n        return out\n\nclass NIN(nn.Module):\n    def __init__(self):\n        super(NIN,self).__init__()\n        self.block1 = nn.Sequential(NINBlock(1, 192, 5, 1, 2),\n                                    NINBlock(192, 160, 1, 1, 0),\n                                    NINBlock(160, 96, 1, 1, 0),\n                                    nn.MaxPool2d(3,2,ceil_mode=True),\n                                    nn.Dropout(inplace = True))\n\n        self.block2 = nn.Sequential(NINBlock(96, 192, 5, 1, 2),\n                                    NINBlock(192, 192, 1, 1, 0),\n                                    NINBlock(192, 192, 1, 1, 0),\n                                    nn.AvgPool2d(3,2,ceil_mode=True),\n                                    nn.Dropout(inplace = True))\n\n        self.block3 = nn.Sequential(NINBlock(192, 192, 3, 1, 1),\n                                    NINBlock(192, 192, 1, 1, 0),\n                                    NINBlock(192, 10, 1, 1, 0),\n                                    nn.AvgPool2d(7,1,ceil_mode=True))\n\n    def forward(self, x):\n        out = self.block1(x)\n        out = self.block2(out)\n        out = self.block3(out)\n        out.view(out.size(0),10)\n        out = torch.squeeze(out)\n        return out\n'"
classification/model/ResNet.py,1,"b""import torch\nimport torch.nn as nn\n\ndef conv3x3(in_channels,out_channels,stride=1):\n    return nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=stride,padding=1)\n\nclass ResidualBlock(nn.Module):\n    '''\n    A single Residual Block:\n    in ->\n        conv(in_channels,out_channels,stride) -> BN -> ReLU\n        conv(out_channels,out_channels,1) -> BN\n    -> out\n    (downsample)in + out\n    '''\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock,self).__init__()\n        self.conv1 = conv3x3(in_channels,out_channels,stride)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.conv2 = conv3x3(out_channels,out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.downsample = downsample\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if(self.downsample):\n            out = self.downsample(x) + out\n        else:\n            out = out + x\n\n        out = self.relu(out)\n\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes = 10):\n        super(ResNet, self).__init__()\n        self.in_channels = 16\n        self.conv = conv3x3(1, 16)\n        self.bn = nn.BatchNorm2d(16)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.layer1 = self._make_layer(ResidualBlock, 16, 1, layers[0])\n        self.layer2 = self._make_layer(ResidualBlock, 32, 2, layers[1])\n        self.layer3 = self._make_layer(ResidualBlock, 64, 2, layers[2])\n\n        self.avgPool = nn.AvgPool2d(7)\n        self.fc = nn.Linear(64, num_classes)\n\n\n    def _make_layer(self, block, out_channels, stride, Nblock):\n        '''\n        Given a block, stack the block Nblock times to form a layer:\n        1. Block(in_channels, out_channels, stride)\n        2. Block(out_channels, out_channels, 1)\n        ...\n        Nblock. Block(out_channels, out_channels, 1)\n        '''\n        layers = []\n        downsample = None\n        if(self.in_channels != out_channels) or (stride != 1):\n            downsample = nn.Sequential(\n                        conv3x3(self.in_channels, out_channels, stride),\n                        nn.BatchNorm2d(out_channels)\n            )\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for i in range(1,Nblock):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n\n    def forward(self,x):\n        out = self.conv(x)\n        out = self.bn(out)\n        out = self.relu(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avgPool(out)\n        out = out.view(out.size(0),-1)\n        out = self.fc(out)\n        return out\n"""
classification/model/__init__.py,0,b''
cycleGAN/model/Discriminator.py,8,"b'import torch.nn as nn\nimport torch\n\n# courtesy: https://github.com/darkstar112358/fast-neural-style/blob/master/neural_style/transformer_net.py\nclass InstanceNormalization(torch.nn.Module):\n    """"""InstanceNormalization\n    Improves convergence of neural-style.\n    ref: https://arxiv.org/pdf/1607.08022.pdf\n    """"""\n\n    def __init__(self, dim, eps=1e-9):\n        super(InstanceNormalization, self).__init__()\n        self.scale = nn.Parameter(torch.FloatTensor(dim))\n        self.shift = nn.Parameter(torch.FloatTensor(dim))\n        self.eps = eps\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        self.scale.data.uniform_()\n        self.shift.data.zero_()\n\n    def forward(self, x):\n        n = x.size(2) * x.size(3)\n        t = x.view(x.size(0), x.size(1), n)\n        mean = torch.mean(t, 2).unsqueeze(2).expand_as(x)\n        # Calculate the biased var. torch.var returns unbiased var\n        var = torch.var(t, 2).unsqueeze(2).expand_as(x) * ((n - 1) / float(n))\n        scale_broadcast = self.scale.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n        scale_broadcast = scale_broadcast.expand_as(x)\n        shift_broadcast = self.shift.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n        shift_broadcast = shift_broadcast.expand_as(x)\n        out = (x - mean) / torch.sqrt(var + self.eps)\n        out = out * scale_broadcast + shift_broadcast\n        return out\n\nclass Discriminator(nn.Module):\n    def __init__(self,input_nc,ndf):\n        super(Discriminator,self).__init__()\n        # 256 x 256\n        self.layer1 = nn.Sequential(nn.Conv2d(input_nc,ndf,kernel_size=4,stride=2,padding=1),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 64 x 64\n        self.layer2 = nn.Sequential(nn.Conv2d(ndf,ndf*2,kernel_size=4,stride=2,padding=1),\n                                 InstanceNormalization(ndf*2),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 32 x 32\n        self.layer3 = nn.Sequential(nn.Conv2d(ndf*2,ndf*4,kernel_size=4,stride=1,padding=1),\n                                 InstanceNormalization(ndf*4),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 31 x 31\n        self.layer4 = nn.Sequential(nn.Conv2d(ndf*4,1,kernel_size=4,stride=1,padding=1),\n                                 nn.Sigmoid())\n        # 30 x 30\n\n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        return out\n'"
cycleGAN/model/Generator.py,8,"b'import torch.nn as nn\nimport torch\n\n# courtesy: https://github.com/darkstar112358/fast-neural-style/blob/master/neural_style/transformer_net.py\nclass InstanceNormalization(torch.nn.Module):\n    """"""InstanceNormalization\n    Improves convergence of neural-style.\n    ref: https://arxiv.org/pdf/1607.08022.pdf\n    """"""\n\n    def __init__(self, dim, eps=1e-9):\n        super(InstanceNormalization, self).__init__()\n        self.scale = nn.Parameter(torch.FloatTensor(dim))\n        self.shift = nn.Parameter(torch.FloatTensor(dim))\n        self.eps = eps\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        self.scale.data.uniform_()\n        self.shift.data.zero_()\n\n    def forward(self, x):\n        n = x.size(2) * x.size(3)\n        t = x.view(x.size(0), x.size(1), n)\n        mean = torch.mean(t, 2).unsqueeze(2).expand_as(x)\n        # Calculate the biased var. torch.var returns unbiased var\n        var = torch.var(t, 2).unsqueeze(2).expand_as(x) * ((n - 1) / float(n))\n        scale_broadcast = self.scale.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n        scale_broadcast = scale_broadcast.expand_as(x)\n        shift_broadcast = self.shift.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n        shift_broadcast = shift_broadcast.expand_as(x)\n        out = (x - mean) / torch.sqrt(var + self.eps)\n        out = out * scale_broadcast + shift_broadcast\n        return out\n\ndef build_conv_block(dim):\n    return nn.Sequential(nn.ReflectionPad2d((1,1,1,1)),\n                         nn.Conv2d(dim,dim,3,1,0),\n                         InstanceNormalization(dim),\n                         nn.ReLU(True),\n                         nn.ReflectionPad2d((1,1,1,1)),\n                         nn.Conv2d(dim,dim,3,1,0),\n                         InstanceNormalization(dim))\n\nclass ResidualBlock(nn.Module):\n    \'\'\'\n    A single Residual Block:\n    in ->\n        conv(in_channels,out_channels,stride) -> BN -> ReLU\n        conv(out_channels,out_channels,1) -> BN\n    -> out\n    (downsample)in + out\n    \'\'\'\n    def __init__(self, dim):\n        super(ResidualBlock,self).__init__()\n        self.conv = build_conv_block(dim)\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = out + x\n\n        return out\n\nclass Generator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf):\n        super(Generator,self).__init__()\n        # 128 x 128\n        self.layer1 = nn.Sequential(nn.ReflectionPad2d((3,3,3,3)),\n                                    nn.Conv2d(input_nc,ngf,kernel_size=7,stride=1),\n                                    InstanceNormalization(ngf),\n                                    nn.ReLU(True))\n        # 128 x 128\n        self.layer2 = nn.Sequential(nn.Conv2d(ngf,ngf*2,kernel_size=3,stride=2,padding=1),\n                                   InstanceNormalization(ngf*2),\n                                   nn.ReLU(True))\n        # 64 x 64\n        self.layer3 = nn.Sequential(nn.Conv2d(ngf*2,ngf*4,kernel_size=3,stride=2,padding=1),\n                                   InstanceNormalization(ngf*4),\n                                   nn.ReLU(True))\n        # 32 x 32\n        self.layer4 = nn.Sequential(ResidualBlock(ngf*4),\n                                    ResidualBlock(ngf*4),\n                                    ResidualBlock(ngf*4),\n                                    ResidualBlock(ngf*4),\n                                    ResidualBlock(ngf*4),\n                                    ResidualBlock(ngf*4))\n        # 32 x 32\n        self.layer5 = nn.Sequential(nn.ConvTranspose2d(ngf*4, ngf*2, 3, 2, 1, 1),\n                                     InstanceNormalization(ngf*2),\n                                     nn.ReLU(True))\n        # 64 x 64\n        self.layer6 = nn.Sequential(nn.ConvTranspose2d(ngf*2, ngf, 3, 2, 1, 1),\n                                    InstanceNormalization(ngf),\n                                    nn.ReLU(True))\n        # 128 x 128\n        self.layer7 = nn.Sequential(nn.ReflectionPad2d((3,3,3,3)),\n                                     nn.Conv2d(ngf,output_nc,kernel_size=7,stride=1),\n                                     nn.Tanh())\n        # 128 x 128\n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        out = self.layer6(out)\n        out = self.layer7(out)\n        return out\n'"
cycleGAN/model/__init__.py,0,b''
cycleGAN/scripts/PrepareDataset.py,0,"b""# We use dataset from pix2pix. This is a paired dataset, so we need to split\n# each pair and generate unpaired dataset\n\nimport argparse\nimport os\nfrom PIL import Image\nfrom glob import glob\n\nparser = argparse.ArgumentParser(description='split paired dataset to unpaired dataset')\nparser.add_argument('--dataPath', default='../facades', help='path to dataset folder')\n\n########   make A and B folder   ########\nopt = parser.parse_args()\npaths = glob(opt.dataPath+'/*')\nfor path in paths:\n    if not os.path.exists(os.path.join(path,'A')):\n        os.makedirs(os.path.join(path,'A'))\n    if not os.path.exists(os.path.join(path,'B')):\n        os.makedirs(os.path.join(path,'B'))\n\n########   split images and put them into corresponding folders   ########\ndef isImg(filename):\n    ext = filename.split('.')[-1]\n    if(ext in ['png','jpg','jpeg','ppm']):\n        return True\n\ndef split(imgPath):\n    img = Image.open(imgPath)\n    w,h = img.size\n    imgA = img.crop((0, 0, w/2, h))\n    imgB = img.crop((w/2, 0, w, h))\n    return imgA, imgB\n\nfor path in paths:\n    print('Processing folder: '+path)\n    for fn in os.listdir(path):\n        if(isImg(fn)):\n            imgA,imgB = split(os.path.join(path,fn))\n            imgA.save(os.path.join(path,'A',fn))\n            imgB.save(os.path.join(path,'B',fn))\n\nchoice = raw_input('Delete original paired data? (y/n) WARNING: DELETED DATASET IS UNRECOVERABLE')\nif(choice == 'y'):\n    for path in paths:\n        for fn in os.listdir(path):\n            if(isImg(fn)):\n                os.system('rm '+os.path.join(path,fn))\n"""
cycleGAN/utils/ImagePool.py,0,"b'import torch\nimport random\n\nclass ImagePool(object):\n    def __init__(self,poolSize):\n        super(ImagePool,self).__init__()\n        self.poolSize = poolSize\n        if(poolSize > 0):\n            self.num_imgs = 0\n            self.images = []\n\n    def Query(self,img):\n        # not using lsGAN\n        if(self.poolSize == 0):\n            return img\n        if(self.num_imgs < self.poolSize):\n            # pool is not full\n            self.images.append(img)\n            self.num_imgs = self.num_imgs + 1\n            return img\n        else:\n            # pool is full, by 50% chance randomly select an image tensor,\n            # return it and replace it with the new tensor, by 50% return\n            # the newly generated image\n            p = random.random()\n            if(p > 0.5):\n                idx = random.randint(0,self.poolSize-1)\n                tmp = self.images[idx]\n                self.images[idx] = img\n                return tmp\n            else:\n                return img\n'"
cycleGAN/utils/__init__.py,0,b''
cycleGAN/utils/dataset.py,7,"b'import torch\nimport numpy as np\nimport torch.utils.data as data\nfrom os import listdir\nfrom os.path import join\nfrom utils import is_image_file\nimport os\nfrom PIL import Image\nimport random\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\ndef ToTensor(pic):\n    """"""Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    """"""\n    if isinstance(pic, np.ndarray):\n        # handle numpy array\n        img = torch.from_numpy(pic.transpose((2, 0, 1)))\n        # backard compability\n        return img.float().div(255)\n    # handle PIL Image\n    if pic.mode == \'I\':\n        img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n    elif pic.mode == \'I;16\':\n        img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n    else:\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n    # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n    if pic.mode == \'YCbCr\':\n        nchannel = 3\n    elif pic.mode == \'I;16\':\n        nchannel = 1\n    else:\n        nchannel = len(pic.mode)\n    img = img.view(pic.size[1], pic.size[0], nchannel)\n    # put it from HWC to CHW format\n    # yikes, this transpose takes 80% of the loading time/CPU\n    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n    if isinstance(img, torch.ByteTensor):\n        return img.float().div(255)\n    else:\n        return img\n\n\n# You should build custom dataset as below.\nclass DATASET(data.Dataset):\n    def __init__(self,dataPath=\'\',loadSize=72,fineSize=64,flip=1):\n        super(DATASET, self).__init__()\n        # list all images into a list\n        self.list = [x for x in listdir(dataPath) if is_image_file(x)]\n        self.dataPath = dataPath\n        self.loadSize = loadSize\n        self.fineSize = fineSize\n        self.flip = flip\n\n    def __getitem__(self, index):\n        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n        path = os.path.join(self.dataPath,self.list[index])\n        img = default_loader(path) # 256x256\n\n        # 2. seperate image A and B; Scale; Random Crop; to Tensor\n        w,h = img.size\n\n        if(h != self.loadSize):\n            img = img.resize((self.loadSize, self.loadSize), Image.BILINEAR)\n\n        if(self.loadSize != self.fineSize):\n            x1 = random.randint(0, self.loadSize - self.fineSize)\n            y1 = random.randint(0, self.loadSize - self.fineSize)\n            img = img.crop((x1, y1, x1 + self.fineSize, y1 + self.fineSize))\n\n        if(self.flip == 1):\n            if random.random() < 0.5:\n                img = img.transpose(Image.FLIP_LEFT_RIGHT)\n\n        img = ToTensor(img) # 3 x 256 x 256\n\n        img = img.mul_(2).add_(-1)\n        # 3. Return a data pair (e.g. image and label).\n        return img\n\n    def __len__(self):\n        # You should change 0 to the total size of your dataset.\n        return len(self.list)\n'"
cycleGAN/utils/utils.py,0,"b'import numpy as np\nfrom scipy.misc import imread, imresize, imsave\nimport torch\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in ["".png"", "".jpg"", "".jpeg""])\n\ndef deprocess(img):\n    img = img.add_(1).div_(2)\n\n    return img\n'"
dcgan/model/Discriminator.py,1,"b'import torch.nn as nn\n\nclass Discriminator(nn.Module):\n    def __init__(self,nc,ndf):\n        super(Discriminator,self).__init__()\n        # 32 x 32\n        self.layer1 = nn.Sequential(nn.Conv2d(nc,ndf,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(ndf),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 16 x 16\n        self.layer2 = nn.Sequential(nn.Conv2d(ndf,ndf*2,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(ndf*2),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 8 x 8\n        self.layer3 = nn.Sequential(nn.Conv2d(ndf*2,ndf*4,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(ndf*4),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 4 x 4\n        self.layer4 = nn.Sequential(nn.Conv2d(ndf*4,1,kernel_size=4,stride=1,padding=0),\n                                 nn.Sigmoid())\n\n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        return out\n'"
dcgan/model/Generator.py,1,"b'import torch.nn as nn\n\nclass Generator(nn.Module):\n    def __init__(self, nc, ngf, nz):\n        super(Generator,self).__init__()\n        self.layer1 = nn.Sequential(nn.ConvTranspose2d(nz,ngf*4,kernel_size=4),\n                                 nn.BatchNorm2d(ngf*4),\n                                 nn.ReLU())\n        # 4 x 4\n        self.layer2 = nn.Sequential(nn.ConvTranspose2d(ngf*4,ngf*2,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(ngf*2),\n                                 nn.ReLU())\n        # 8 x 8\n        self.layer3 = nn.Sequential(nn.ConvTranspose2d(ngf*2,ngf,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(ngf),\n                                 nn.ReLU())\n        # 16 x 16\n        self.layer4 = nn.Sequential(nn.ConvTranspose2d(ngf,nc,kernel_size=4,stride=2,padding=1),\n                                 nn.Tanh())\n\n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        return out\n'"
dcgan/model/__init__.py,0,b''
fast-neural-style/utils/__init__.py,0,b''
fast-neural-style/utils/transformer.py,14,"b'# credit: https://github.com/darkstar112358/fast-neural-style/blob/master/neural_style/transformer_net.py\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\n\nclass TransformerNet(torch.nn.Module):\n    def __init__(self):\n        super(TransformerNet, self).__init__()\n\n        # Initial convolution layers\n        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n        self.in1 = InstanceNormalization(32)\n        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n        self.in2 = InstanceNormalization(64)\n        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n        self.in3 = InstanceNormalization(128)\n\n        # Residual layers\n        self.res1 = ResidualBlock(128)\n        self.res2 = ResidualBlock(128)\n        self.res3 = ResidualBlock(128)\n        self.res4 = ResidualBlock(128)\n        self.res5 = ResidualBlock(128)\n\n        # Upsampling Layers\n        self.deconv1 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2)\n        self.in4 = InstanceNormalization(64)\n        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2)\n        self.in5 = InstanceNormalization(32)\n        self.deconv3 = ConvLayer(32, 3, kernel_size=9, stride=1)\n\n        # Non-linearities\n        self.relu = nn.ReLU()\n\n    def forward(self, X):\n        in_X = X\n        y = self.relu(self.in1(self.conv1(in_X)))\n        y = self.relu(self.in2(self.conv2(y)))\n        y = self.relu(self.in3(self.conv3(y)))\n        y = self.res1(y)\n        y = self.res2(y)\n        y = self.res3(y)\n        y = self.res4(y)\n        y = self.res5(y)\n        y = self.relu(self.in4(self.deconv1(y)))\n        y = self.relu(self.in5(self.deconv2(y)))\n        y = self.deconv3(y)\n        return y \n\n\nclass ConvLayer(torch.nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride):\n        super(ConvLayer, self).__init__()\n        reflection_padding = int(np.floor(kernel_size / 2))\n        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n\n    def forward(self, x):\n        out = self.reflection_pad(x)\n        out = self.conv2d(out)\n        return out\n\n\nclass ResidualBlock(torch.nn.Module):\n    """"""ResidualBlock\n    introduced in: https://arxiv.org/abs/1512.03385\n    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n    """"""\n\n    def __init__(self, channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in1 = InstanceNormalization(channels)\n        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n        self.in2 = InstanceNormalization(channels)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        residual = x\n        out = self.relu(self.in1(self.conv1(x)))\n        out = self.in2(self.conv2(out))\n        out = out + residual\n        return out\n\n\nclass UpsampleConvLayer(torch.nn.Module):\n    """"""UpsampleConvLayer\n    Upsamples the input and then does a convolution. This method gives better results\n    compared to ConvTranspose2d.\n    ref: http://distill.pub/2016/deconv-checkerboard/\n    """"""\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n        super(UpsampleConvLayer, self).__init__()\n        self.upsample = upsample\n        if upsample:\n            self.upsample_layer = torch.nn.UpsamplingNearest2d(scale_factor=upsample)\n        reflection_padding = int(np.floor(kernel_size / 2))\n        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n\n    def forward(self, x):\n        x_in = x\n        if self.upsample:\n            x_in = self.upsample_layer(x_in)\n        out = self.reflection_pad(x_in)\n        out = self.conv2d(out)\n        return out\n\n\nclass InstanceNormalization(torch.nn.Module):\n    """"""InstanceNormalization\n    Improves convergence of neural-style.\n    ref: https://arxiv.org/pdf/1607.08022.pdf\n    """"""\n\n    def __init__(self, dim, eps=1e-9):\n        super(InstanceNormalization, self).__init__()\n        self.scale = nn.Parameter(torch.FloatTensor(dim))\n        self.shift = nn.Parameter(torch.FloatTensor(dim))\n        self.eps = eps\n        self._reset_parameters()\n\n    def _reset_parameters(self):\n        self.scale.data.uniform_()\n        self.shift.data.zero_()\n\n    def forward(self, x):\n        n = x.size(2) * x.size(3)\n        t = x.view(x.size(0), x.size(1), n)\n        mean = torch.mean(t, 2).unsqueeze(2).expand_as(x)\n        # Calculate the biased var. torch.var returns unbiased var\n        var = torch.var(t, 2).unsqueeze(2).expand_as(x) * ((n - 1) / float(n))\n        scale_broadcast = self.scale.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n        scale_broadcast = scale_broadcast.expand_as(x)\n        shift_broadcast = self.shift.unsqueeze(1).unsqueeze(1).unsqueeze(0)\n        shift_broadcast = shift_broadcast.expand_as(x)\n        out = (x - mean) / torch.sqrt(var + self.eps)\n        out = out * scale_broadcast + shift_broadcast\n        return out\n'"
fast-neural-style/utils/vgg.py,2,"b""#vgg definition that conveniently let's you grab the outputs from any layer\n#credits: https://github.com/leongatys/PytorchNeuralStyleTransfer\nimport torch.nn as nn\nimport torch.nn.functional as F\nclass VGG(nn.Module):\n    def __init__(self, pool='max'):\n        super(VGG, self).__init__()\n        #vgg modules\n        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n        if pool == 'max':\n            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n        elif pool == 'avg':\n            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n\n    def forward(self, x, out_keys):\n        out = {}\n        out['r11'] = F.relu(self.conv1_1(x))\n        out['r12'] = F.relu(self.conv1_2(out['r11']))\n        out['p1'] = self.pool1(out['r12'])\n        out['r21'] = F.relu(self.conv2_1(out['p1']))\n        out['r22'] = F.relu(self.conv2_2(out['r21']))\n        out['p2'] = self.pool2(out['r22'])\n        out['r31'] = F.relu(self.conv3_1(out['p2']))\n        out['r32'] = F.relu(self.conv3_2(out['r31']))\n        out['r33'] = F.relu(self.conv3_3(out['r32']))\n        out['r34'] = F.relu(self.conv3_4(out['r33']))\n        out['p3'] = self.pool3(out['r34'])\n        out['r41'] = F.relu(self.conv4_1(out['p3']))\n        out['r42'] = F.relu(self.conv4_2(out['r41']))\n        out['r43'] = F.relu(self.conv4_3(out['r42']))\n        out['r44'] = F.relu(self.conv4_4(out['r43']))\n        out['p4'] = self.pool4(out['r44'])\n        out['r51'] = F.relu(self.conv5_1(out['p4']))\n        out['r52'] = F.relu(self.conv5_2(out['r51']))\n        out['r53'] = F.relu(self.conv5_3(out['r52']))\n        out['r54'] = F.relu(self.conv5_4(out['r53']))\n        out['p5'] = self.pool5(out['r54'])\n        return [out[key] for key in out_keys]\n"""
pix2pix/model/Discriminator.py,1,"b'import torch.nn as nn\n\nclass Discriminator(nn.Module):\n    def __init__(self,input_nc,output_nc,ndf):\n        super(Discriminator,self).__init__()\n        # 256 x 256\n        self.layer1 = nn.Sequential(nn.Conv2d(input_nc+output_nc,ndf,kernel_size=4,stride=2,padding=1),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 128 x 128\n        self.layer2 = nn.Sequential(nn.Conv2d(ndf,ndf*2,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(ndf*2),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 64 x 64\n        self.layer3 = nn.Sequential(nn.Conv2d(ndf*2,ndf*4,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(ndf*4),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 32 x 32\n        self.layer4 = nn.Sequential(nn.Conv2d(ndf*4,ndf*8,kernel_size=4,stride=1,padding=1),\n                                 nn.BatchNorm2d(ndf*8),\n                                 nn.LeakyReLU(0.2,inplace=True))\n        # 31 x 31\n        self.layer5 = nn.Sequential(nn.Conv2d(ndf*8,1,kernel_size=4,stride=1,padding=1),\n                                 nn.Sigmoid())\n        # 30 x 30\n\n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        return out\n'"
pix2pix/model/Generator.py,15,"b""#Credit: code copied from https://github.com/mrzhu-cool/pix2pix-pytorch/blob/master/models.py\nimport torch.nn as nn\nimport torch\n\nclass Generator(nn.Module):\n    def __init__(self, input_nc, output_nc, ngf):\n        super(Generator,self).__init__()\n        '''\n        # 256 x 256\n        self.e1 = nn.Sequential(nn.Conv2d(input_nc,ngf,kernel_size=4,stride=2,padding=1),\n                                 nn.LeakyReLU(0.2, inplace=True))\n        # 128 x 128\n        self.e2 = nn.Sequential(nn.Conv2d(ngf,ngf*2,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(ngf*2),\n                                 nn.LeakyReLU(0.2, inplace=True))\n        # 64 x 64\n        self.e3 = nn.Sequential(nn.Conv2d(ngf*2,ngf*4,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(ngf*4),\n                                 nn.LeakyReLU(0.2, inplace=True))\n        # 32 x 32\n        self.e4 = nn.Sequential(nn.Conv2d(ngf*4,ngf*8,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(ngf*8),\n                                 nn.LeakyReLU(0.2, inplace=True))\n        # 16 x 16\n        self.e5 = nn.Sequential(nn.Conv2d(ngf*8,ngf*8,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(ngf*8),\n                                 nn.LeakyReLU(0.2, inplace=True))\n        # 8 x 8\n        self.e6 = nn.Sequential(nn.Conv2d(ngf*8,ngf*8,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(ngf*8),\n                                 nn.LeakyReLU(0.2, inplace=True))\n        # 4 x 4\n        self.e7 = nn.Sequential(nn.Conv2d(ngf*8,ngf*8,kernel_size=4,stride=2,padding=1),\n                                 nn.BatchNorm2d(ngf*8),\n                                 nn.LeakyReLU(0.2, inplace=True))\n        # 2 x 2\n        self.e8 = nn.Sequential(nn.Conv2d(ngf*8,ngf*8,kernel_size=4,stride=2,padding=1),\n                                 nn.LeakyReLU(0.2, inplace=True))\n        # 1 x 1\n        self.d1 = nn.Sequential(nn.ConvTranspose2d(ngf*8,ngf*8,kernel_size=4,stride=2,padding=1),\n                                nn.BatchNorm2d(ngf*8),\n                                nn.Dropout())\n        # 2 x 2\n        self.d2 = nn.Sequential(nn.ConvTranspose2d(ngf*8*2,ngf*8,kernel_size=4,stride=2,padding=1),\n                                nn.BatchNorm2d(ngf*8),\n                                nn.Dropout())\n        # 4 x 4\n        self.d3 = nn.Sequential(nn.ConvTranspose2d(ngf*8*2,ngf*8,kernel_size=4,stride=2,padding=1),\n                                nn.BatchNorm2d(ngf*8),\n                                nn.Dropout())\n        # 8 x 8\n        self.d4 = nn.Sequential(nn.ConvTranspose2d(ngf*8*2,ngf*8,kernel_size=4,stride=2,padding=1),\n                                nn.BatchNorm2d(ngf*8))\n        # 16 x 16\n        self.d5 = nn.Sequential(nn.ConvTranspose2d(ngf*8*2,ngf*4,kernel_size=4,stride=2,padding=1),\n                                nn.BatchNorm2d(ngf*4))\n        # 32 x 32\n        self.d6 = nn.Sequential(nn.ConvTranspose2d(ngf*4*2,ngf*2,kernel_size=4,stride=2,padding=1),\n                                nn.BatchNorm2d(ngf*2))\n        # 64 x 64\n        self.d7 = nn.Sequential(nn.ConvTranspose2d(ngf*2*2,ngf,kernel_size=4,stride=2,padding=1),\n                                nn.BatchNorm2d(ngf))\n        # 128 x 128\n        self.d8 = nn.ConvTranspose2d(ngf*2,output_nc,kernel_size=4,stride=2,padding=1)\n        # 256 x 256\n\n        self.relu = nn.ReLU(inplace=True)\n        self.tanh = nn.Tanh()\n\n    def forward(self,x):\n        # encoder\n        out_e1 = self.e1(x)              # 128 x 128\n        out_e2 = self.e2(out_e1)             # 64 x 64\n        out_e3 = self.e3(out_e2)             # 32 x 32\n        out_e4 = self.e4(out_e3)             # 16 x 16\n        out_e5 = self.e5(out_e4)             # 8 x 8\n        out_e6 = self.e6(out_e5)             # 4 x 4\n        out_e7 = self.e7(out_e6)             # 2 x 2\n        out_e8 = self.e8(out_e7)             # 1 x 1\n\n        # decoder\n        out_d1 = self.d1(self.relu(out_e8))  # 2 x 2\n        out_d1_ = torch.cat((out_d1, out_e7),1)\n\n        out_d2 = self.d2(self.relu(out_d1_)) # 4 x 4\n        out_d2_ = torch.cat((out_d2, out_e6),1)\n\n        out_d3 = self.d3(self.relu(out_d2_)) # 8 x 8\n        out_d3_ = torch.cat((out_d3, out_e5),1)\n\n        out_d4 = self.d4(self.relu(out_d3_)) # 16 x 16\n        out_d4_ = torch.cat((out_d4, out_e4),1)\n\n        out_d5 = self.d5(self.relu(out_d4_)) # 32 x 32\n        out_d5_ = torch.cat((out_d5, out_e3),1)\n\n        out_d6 = self.d6(self.relu(out_d5_)) # 64 x 64\n        out_d6_ = torch.cat((out_d6, out_e2),1)\n\n        out_d7 = self.d7(self.relu(out_d6_)) # 128 x 128\n        out_d7_ = torch.cat((out_d7, out_e1),1)\n\n        out_d8 = self.d8(self.relu(out_d7_)) # 256 x 256\n        out = self.tanh(out_d8)\n\n        return out\n        '''\n        self.conv1 = nn.Conv2d(input_nc, ngf, 4, 2, 1)\n        self.conv2 = nn.Conv2d(ngf, ngf * 2, 4, 2, 1)\n        self.conv3 = nn.Conv2d(ngf * 2, ngf * 4, 4, 2, 1)\n        self.conv4 = nn.Conv2d(ngf * 4, ngf * 8, 4, 2, 1)\n        self.conv5 = nn.Conv2d(ngf * 8, ngf * 8, 4, 2, 1)\n        self.conv6 = nn.Conv2d(ngf * 8, ngf * 8, 4, 2, 1)\n        self.conv7 = nn.Conv2d(ngf * 8, ngf * 8, 4, 2, 1)\n        self.conv8 = nn.Conv2d(ngf * 8, ngf * 8, 4, 2, 1)\n        self.dconv1 = nn.ConvTranspose2d(ngf * 8, ngf * 8, 4, 2, 1)\n        self.dconv2 = nn.ConvTranspose2d(ngf * 8 * 2, ngf * 8, 4, 2, 1)\n        self.dconv3 = nn.ConvTranspose2d(ngf * 8 * 2, ngf * 8, 4, 2, 1)\n        self.dconv4 = nn.ConvTranspose2d(ngf * 8 * 2, ngf * 8, 4, 2, 1)\n        self.dconv5 = nn.ConvTranspose2d(ngf * 8 * 2, ngf * 4, 4, 2, 1)\n        self.dconv6 = nn.ConvTranspose2d(ngf * 4 * 2, ngf * 2, 4, 2, 1)\n        self.dconv7 = nn.ConvTranspose2d(ngf * 2 * 2, ngf, 4, 2, 1)\n        self.dconv8 = nn.ConvTranspose2d(ngf * 2, output_nc, 4, 2, 1)\n\n        self.batch_norm = nn.BatchNorm2d(ngf)\n        self.batch_norm2 = nn.BatchNorm2d(ngf * 2)\n        self.batch_norm4 = nn.BatchNorm2d(ngf * 4)\n        self.batch_norm8 = nn.BatchNorm2d(ngf * 8)\n\n        self.leaky_relu = nn.LeakyReLU(0.2, True)\n        self.relu = nn.ReLU(True)\n\n        self.dropout = nn.Dropout(0.5)\n\n        self.tanh = nn.Tanh()\n\n    def forward(self, input):\n        # Encoder\n        # Convolution layers:\n        # input is (nc) x 256 x 256\n        e1 = self.conv1(input)\n        # state size is (ngf) x 128 x 128\n        e2 = self.batch_norm2(self.conv2(self.leaky_relu(e1)))\n        # state size is (ngf x 2) x 64 x 64\n        e3 = self.batch_norm4(self.conv3(self.leaky_relu(e2)))\n        # state size is (ngf x 4) x 32 x 32\n        e4 = self.batch_norm8(self.conv4(self.leaky_relu(e3)))\n        # state size is (ngf x 8) x 16 x 16\n        e5 = self.batch_norm8(self.conv5(self.leaky_relu(e4)))\n        # state size is (ngf x 8) x 8 x 8\n        e6 = self.batch_norm8(self.conv6(self.leaky_relu(e5)))\n        # state size is (ngf x 8) x 4 x 4\n        e7 = self.batch_norm8(self.conv7(self.leaky_relu(e6)))\n        # state size is (ngf x 8) x 2 x 2\n        # No batch norm on output of Encoder\n        e8 = self.conv8(self.leaky_relu(e7))\n\n        # Decoder\n        # Deconvolution layers:\n        # state size is (ngf x 8) x 1 x 1\n        d1_ = self.dropout(self.batch_norm8(self.dconv1(self.relu(e8))))\n        # state size is (ngf x 8) x 2 x 2\n        d1 = torch.cat((d1_, e7), 1)\n        d2_ = self.dropout(self.batch_norm8(self.dconv2(self.relu(d1))))\n        # state size is (ngf x 8) x 4 x 4\n        d2 = torch.cat((d2_, e6), 1)\n        d3_ = self.dropout(self.batch_norm8(self.dconv3(self.relu(d2))))\n        # state size is (ngf x 8) x 8 x 8\n        d3 = torch.cat((d3_, e5), 1)\n        d4_ = self.batch_norm8(self.dconv4(self.relu(d3)))\n        # state size is (ngf x 8) x 16 x 16\n        d4 = torch.cat((d4_, e4), 1)\n        d5_ = self.batch_norm4(self.dconv5(self.relu(d4)))\n        # state size is (ngf x 4) x 32 x 32\n        d5 = torch.cat((d5_, e3), 1)\n        d6_ = self.batch_norm2(self.dconv6(self.relu(d5)))\n        # state size is (ngf x 2) x 64 x 64\n        d6 = torch.cat((d6_, e2), 1)\n        d7_ = self.batch_norm(self.dconv7(self.relu(d6)))\n        # state size is (ngf) x 128 x 128\n        d7 = torch.cat((d7_, e1), 1)\n        d8 = self.dconv8(self.relu(d7))\n        # state size is (nc) x 256 x 256\n        output = self.tanh(d8)\n        return output\n"""
pix2pix/model/__init__.py,0,b''
pix2pix/utils/__init__.py,0,b''
pix2pix/utils/dataset.py,7,"b'import torch\nimport numpy as np\nimport torch.utils.data as data\nfrom os import listdir\nfrom os.path import join\nfrom utils import is_image_file\nimport os\nfrom PIL import Image\nimport random\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n\ndef ToTensor(pic):\n    """"""Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    """"""\n    if isinstance(pic, np.ndarray):\n        # handle numpy array\n        img = torch.from_numpy(pic.transpose((2, 0, 1)))\n        # backard compability\n        return img.float().div(255)\n    # handle PIL Image\n    if pic.mode == \'I\':\n        img = torch.from_numpy(np.array(pic, np.int32, copy=False))\n    elif pic.mode == \'I;16\':\n        img = torch.from_numpy(np.array(pic, np.int16, copy=False))\n    else:\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n    # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n    if pic.mode == \'YCbCr\':\n        nchannel = 3\n    elif pic.mode == \'I;16\':\n        nchannel = 1\n    else:\n        nchannel = len(pic.mode)\n    img = img.view(pic.size[1], pic.size[0], nchannel)\n    # put it from HWC to CHW format\n    # yikes, this transpose takes 80% of the loading time/CPU\n    img = img.transpose(0, 1).transpose(0, 2).contiguous()\n    if isinstance(img, torch.ByteTensor):\n        return img.float().div(255)\n    else:\n        return img\n\n\n# You should build custom dataset as below.\nclass Facades(data.Dataset):\n    def __init__(self,dataPath=\'facades/train\',loadSize=286,fineSize=256,flip=1):\n        super(Facades, self).__init__()\n        # list all images into a list\n        self.image_list = [x for x in listdir(dataPath) if is_image_file(x)]\n        self.dataPath = dataPath\n        self.loadSize = loadSize\n        self.fineSize = fineSize\n        self.flip = flip\n\n    def __getitem__(self, index):\n        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open).\n        path = os.path.join(self.dataPath,self.image_list[index])\n        img = default_loader(path) # 512x256\n        #img = ToTensor(img) # 3 x 256 x 512\n\n        # 2. seperate image A and B; Scale; Random Crop; to Tensor\n        w,h = img.size\n        imgA = img.crop((0, 0, w/2, h))\n        imgB = img.crop((w/2, 0, w, h))\n\n        if(h != self.loadSize):\n            imgA = imgA.resize((self.loadSize, self.loadSize), Image.BILINEAR)\n            imgB = imgB.resize((self.loadSize, self.loadSize), Image.BILINEAR)\n\n        if(self.loadSize != self.fineSize):\n            x1 = random.randint(0, self.loadSize - self.fineSize)\n            y1 = random.randint(0, self.loadSize - self.fineSize)\n            imgA = imgA.crop((x1, y1, x1 + self.fineSize, y1 + self.fineSize))\n            imgB = imgB.crop((x1, y1, x1 + self.fineSize, y1 + self.fineSize))\n\n        if(self.flip == 1):\n            if random.random() < 0.5:\n                imgA = imgA.transpose(Image.FLIP_LEFT_RIGHT)\n                imgB = imgB.transpose(Image.FLIP_LEFT_RIGHT)\n\n        imgA = ToTensor(imgA) # 3 x 256 x 256\n        imgB = ToTensor(imgB) # 3 x 256 x 256\n\n        imgA = imgA.mul_(2).add_(-1)\n        imgB = imgB.mul_(2).add_(-1)\n        # 3. Return a data pair (e.g. image and label).\n        return imgA, imgB\n\n    def __len__(self):\n        # You should change 0 to the total size of your dataset.\n        return len(self.image_list)\n'"
pix2pix/utils/utils.py,0,"b'import numpy as np\nfrom scipy.misc import imread, imresize, imsave\nimport torch\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in ["".png"", "".jpg"", "".jpeg""])\n\ndef deprocess(img):\n    img = img.add_(1).div_(2)\n\n    return img\n'"
