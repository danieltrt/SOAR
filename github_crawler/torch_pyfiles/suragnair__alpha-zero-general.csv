file_path,api_count,code
Arena.py,0,"b'import logging\n\nfrom tqdm import tqdm\n\nlog = logging.getLogger(__name__)\n\n\nclass Arena():\n    """"""\n    An Arena class where any 2 agents can be pit against each other.\n    """"""\n\n    def __init__(self, player1, player2, game, display=None):\n        """"""\n        Input:\n            player 1,2: two functions that takes board as input, return action\n            game: Game object\n            display: a function that takes board as input and prints it (e.g.\n                     display in othello/OthelloGame). Is necessary for verbose\n                     mode.\n\n        see othello/OthelloPlayers.py for an example. See pit.py for pitting\n        human players/other baselines with each other.\n        """"""\n        self.player1 = player1\n        self.player2 = player2\n        self.game = game\n        self.display = display\n\n    def playGame(self, verbose=False):\n        """"""\n        Executes one episode of a game.\n\n        Returns:\n            either\n                winner: player who won the game (1 if player1, -1 if player2)\n            or\n                draw result returned from the game that is neither 1, -1, nor 0.\n        """"""\n        players = [self.player2, None, self.player1]\n        curPlayer = 1\n        board = self.game.getInitBoard()\n        it = 0\n        while self.game.getGameEnded(board, curPlayer) == 0:\n            it += 1\n            if verbose:\n                assert self.display\n                print(""Turn "", str(it), ""Player "", str(curPlayer))\n                self.display(board)\n            action = players[curPlayer + 1](self.game.getCanonicalForm(board, curPlayer))\n\n            valids = self.game.getValidMoves(self.game.getCanonicalForm(board, curPlayer), 1)\n\n            if valids[action] == 0:\n                log.error(f\'Action {action} is not valid!\')\n                log.debug(f\'valids = {valids}\')\n                assert valids[action] > 0\n            board, curPlayer = self.game.getNextState(board, curPlayer, action)\n        if verbose:\n            assert self.display\n            print(""Game over: Turn "", str(it), ""Result "", str(self.game.getGameEnded(board, 1)))\n            self.display(board)\n        return curPlayer * self.game.getGameEnded(board, curPlayer)\n\n    def playGames(self, num, verbose=False):\n        """"""\n        Plays num games in which player1 starts num/2 games and player2 starts\n        num/2 games.\n\n        Returns:\n            oneWon: games won by player1\n            twoWon: games won by player2\n            draws:  games won by nobody\n        """"""\n\n        num = int(num / 2)\n        oneWon = 0\n        twoWon = 0\n        draws = 0\n        for _ in tqdm(range(num), desc=""Arena.playGames (1)""):\n            gameResult = self.playGame(verbose=verbose)\n            if gameResult == 1:\n                oneWon += 1\n            elif gameResult == -1:\n                twoWon += 1\n            else:\n                draws += 1\n\n        self.player1, self.player2 = self.player2, self.player1\n\n        for _ in tqdm(range(num), desc=""Arena.playGames (2)""):\n            gameResult = self.playGame(verbose=verbose)\n            if gameResult == -1:\n                oneWon += 1\n            elif gameResult == 1:\n                twoWon += 1\n            else:\n                draws += 1\n\n        return oneWon, twoWon, draws\n'"
Coach.py,0,"b'import logging\nimport os\nimport sys\nfrom collections import deque\nfrom pickle import Pickler, Unpickler\nfrom random import shuffle\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom Arena import Arena\nfrom MCTS import MCTS\n\nlog = logging.getLogger(__name__)\n\n\nclass Coach():\n    """"""\n    This class executes the self-play + learning. It uses the functions defined\n    in Game and NeuralNet. args are specified in main.py.\n    """"""\n\n    def __init__(self, game, nnet, args):\n        self.game = game\n        self.nnet = nnet\n        self.pnet = self.nnet.__class__(self.game)  # the competitor network\n        self.args = args\n        self.mcts = MCTS(self.game, self.nnet, self.args)\n        self.trainExamplesHistory = []  # history of examples from args.numItersForTrainExamplesHistory latest iterations\n        self.skipFirstSelfPlay = False  # can be overriden in loadTrainExamples()\n\n    def executeEpisode(self):\n        """"""\n        This function executes one episode of self-play, starting with player 1.\n        As the game is played, each turn is added as a training example to\n        trainExamples. The game is played till the game ends. After the game\n        ends, the outcome of the game is used to assign values to each example\n        in trainExamples.\n\n        It uses a temp=1 if episodeStep < tempThreshold, and thereafter\n        uses temp=0.\n\n        Returns:\n            trainExamples: a list of examples of the form (canonicalBoard, currPlayer, pi,v)\n                           pi is the MCTS informed policy vector, v is +1 if\n                           the player eventually won the game, else -1.\n        """"""\n        trainExamples = []\n        board = self.game.getInitBoard()\n        self.curPlayer = 1\n        episodeStep = 0\n\n        while True:\n            episodeStep += 1\n            canonicalBoard = self.game.getCanonicalForm(board, self.curPlayer)\n            temp = int(episodeStep < self.args.tempThreshold)\n\n            pi = self.mcts.getActionProb(canonicalBoard, temp=temp)\n            sym = self.game.getSymmetries(canonicalBoard, pi)\n            for b, p in sym:\n                trainExamples.append([b, self.curPlayer, p, None])\n\n            action = np.random.choice(len(pi), p=pi)\n            board, self.curPlayer = self.game.getNextState(board, self.curPlayer, action)\n\n            r = self.game.getGameEnded(board, self.curPlayer)\n\n            if r != 0:\n                return [(x[0], x[2], r * ((-1) ** (x[1] != self.curPlayer))) for x in trainExamples]\n\n    def learn(self):\n        """"""\n        Performs numIters iterations with numEps episodes of self-play in each\n        iteration. After every iteration, it retrains neural network with\n        examples in trainExamples (which has a maximum length of maxlenofQueue).\n        It then pits the new neural network against the old one and accepts it\n        only if it wins >= updateThreshold fraction of games.\n        """"""\n\n        for i in range(1, self.args.numIters + 1):\n            # bookkeeping\n            log.info(f\'Starting Iter #{i} ...\')\n            # examples of the iteration\n            if not self.skipFirstSelfPlay or i > 1:\n                iterationTrainExamples = deque([], maxlen=self.args.maxlenOfQueue)\n\n                for _ in tqdm(range(self.args.numEps), desc=""Self Play""):\n                    self.mcts = MCTS(self.game, self.nnet, self.args)  # reset search tree\n                    iterationTrainExamples += self.executeEpisode()\n\n                # save the iteration examples to the history \n                self.trainExamplesHistory.append(iterationTrainExamples)\n\n            if len(self.trainExamplesHistory) > self.args.numItersForTrainExamplesHistory:\n                log.warning(\n                    f""Removing the oldest entry in trainExamples. len(trainExamplesHistory) = {len(self.trainExamplesHistory)}"")\n                self.trainExamplesHistory.pop(0)\n            # backup history to a file\n            # NB! the examples were collected using the model from the previous iteration, so (i-1)  \n            self.saveTrainExamples(i - 1)\n\n            # shuffle examples before training\n            trainExamples = []\n            for e in self.trainExamplesHistory:\n                trainExamples.extend(e)\n            shuffle(trainExamples)\n\n            # training new network, keeping a copy of the old one\n            self.nnet.save_checkpoint(folder=self.args.checkpoint, filename=\'temp.pth.tar\')\n            self.pnet.load_checkpoint(folder=self.args.checkpoint, filename=\'temp.pth.tar\')\n            pmcts = MCTS(self.game, self.pnet, self.args)\n\n            self.nnet.train(trainExamples)\n            nmcts = MCTS(self.game, self.nnet, self.args)\n\n            log.info(\'PITTING AGAINST PREVIOUS VERSION\')\n            arena = Arena(lambda x: np.argmax(pmcts.getActionProb(x, temp=0)),\n                          lambda x: np.argmax(nmcts.getActionProb(x, temp=0)), self.game)\n            pwins, nwins, draws = arena.playGames(self.args.arenaCompare)\n\n            log.info(\'NEW/PREV WINS : %d / %d ; DRAWS : %d\' % (nwins, pwins, draws))\n            if pwins + nwins == 0 or float(nwins) / (pwins + nwins) < self.args.updateThreshold:\n                log.info(\'REJECTING NEW MODEL\')\n                self.nnet.load_checkpoint(folder=self.args.checkpoint, filename=\'temp.pth.tar\')\n            else:\n                log.info(\'ACCEPTING NEW MODEL\')\n                self.nnet.save_checkpoint(folder=self.args.checkpoint, filename=self.getCheckpointFile(i))\n                self.nnet.save_checkpoint(folder=self.args.checkpoint, filename=\'best.pth.tar\')\n\n    def getCheckpointFile(self, iteration):\n        return \'checkpoint_\' + str(iteration) + \'.pth.tar\'\n\n    def saveTrainExamples(self, iteration):\n        folder = self.args.checkpoint\n        if not os.path.exists(folder):\n            os.makedirs(folder)\n        filename = os.path.join(folder, self.getCheckpointFile(iteration) + "".examples"")\n        with open(filename, ""wb+"") as f:\n            Pickler(f).dump(self.trainExamplesHistory)\n        f.closed\n\n    def loadTrainExamples(self):\n        modelFile = os.path.join(self.args.load_folder_file[0], self.args.load_folder_file[1])\n        examplesFile = modelFile + "".examples""\n        if not os.path.isfile(examplesFile):\n            log.warning(f\'File ""{examplesFile}"" with trainExamples not found!\')\n            r = input(""Continue? [y|n]"")\n            if r != ""y"":\n                sys.exit()\n        else:\n            log.info(""File with trainExamples found. Loading it..."")\n            with open(examplesFile, ""rb"") as f:\n                self.trainExamplesHistory = Unpickler(f).load()\n            log.info(\'Loading done!\')\n\n            # examples based on the model were already collected (loaded)\n            self.skipFirstSelfPlay = True\n'"
Game.py,0,"b'class Game():\n    """"""\n    This class specifies the base Game class. To define your own game, subclass\n    this class and implement the functions below. This works when the game is\n    two-player, adversarial and turn-based.\n\n    Use 1 for player1 and -1 for player2.\n\n    See othello/OthelloGame.py for an example implementation.\n    """"""\n    def __init__(self):\n        pass\n\n    def getInitBoard(self):\n        """"""\n        Returns:\n            startBoard: a representation of the board (ideally this is the form\n                        that will be the input to your neural network)\n        """"""\n        pass\n\n    def getBoardSize(self):\n        """"""\n        Returns:\n            (x,y): a tuple of board dimensions\n        """"""\n        pass\n\n    def getActionSize(self):\n        """"""\n        Returns:\n            actionSize: number of all possible actions\n        """"""\n        pass\n\n    def getNextState(self, board, player, action):\n        """"""\n        Input:\n            board: current board\n            player: current player (1 or -1)\n            action: action taken by current player\n\n        Returns:\n            nextBoard: board after applying action\n            nextPlayer: player who plays in the next turn (should be -player)\n        """"""\n        pass\n\n    def getValidMoves(self, board, player):\n        """"""\n        Input:\n            board: current board\n            player: current player\n\n        Returns:\n            validMoves: a binary vector of length self.getActionSize(), 1 for\n                        moves that are valid from the current board and player,\n                        0 for invalid moves\n        """"""\n        pass\n\n    def getGameEnded(self, board, player):\n        """"""\n        Input:\n            board: current board\n            player: current player (1 or -1)\n\n        Returns:\n            r: 0 if game has not ended. 1 if player won, -1 if player lost,\n               small non-zero value for draw.\n               \n        """"""\n        pass\n\n    def getCanonicalForm(self, board, player):\n        """"""\n        Input:\n            board: current board\n            player: current player (1 or -1)\n\n        Returns:\n            canonicalBoard: returns canonical form of board. The canonical form\n                            should be independent of player. For e.g. in chess,\n                            the canonical form can be chosen to be from the pov\n                            of white. When the player is white, we can return\n                            board as is. When the player is black, we can invert\n                            the colors and return the board.\n        """"""\n        pass\n\n    def getSymmetries(self, board, pi):\n        """"""\n        Input:\n            board: current board\n            pi: policy vector of size self.getActionSize()\n\n        Returns:\n            symmForms: a list of [(board,pi)] where each tuple is a symmetrical\n                       form of the board and the corresponding pi vector. This\n                       is used when training the neural network from examples.\n        """"""\n        pass\n\n    def stringRepresentation(self, board):\n        """"""\n        Input:\n            board: current board\n\n        Returns:\n            boardString: a quick conversion of board to a string format.\n                         Required by MCTS for hashing.\n        """"""\n        pass\n'"
MCTS.py,0,"b'import logging\nimport math\n\nimport numpy as np\n\nEPS = 1e-8\n\nlog = logging.getLogger(__name__)\n\n\nclass MCTS():\n    """"""\n    This class handles the MCTS tree.\n    """"""\n\n    def __init__(self, game, nnet, args):\n        self.game = game\n        self.nnet = nnet\n        self.args = args\n        self.Qsa = {}  # stores Q values for s,a (as defined in the paper)\n        self.Nsa = {}  # stores #times edge s,a was visited\n        self.Ns = {}  # stores #times board s was visited\n        self.Ps = {}  # stores initial policy (returned by neural net)\n\n        self.Es = {}  # stores game.getGameEnded ended for board s\n        self.Vs = {}  # stores game.getValidMoves for board s\n\n    def getActionProb(self, canonicalBoard, temp=1):\n        """"""\n        This function performs numMCTSSims simulations of MCTS starting from\n        canonicalBoard.\n\n        Returns:\n            probs: a policy vector where the probability of the ith action is\n                   proportional to Nsa[(s,a)]**(1./temp)\n        """"""\n        for i in range(self.args.numMCTSSims):\n            self.search(canonicalBoard)\n\n        s = self.game.stringRepresentation(canonicalBoard)\n        counts = [self.Nsa[(s, a)] if (s, a) in self.Nsa else 0 for a in range(self.game.getActionSize())]\n\n        if temp == 0:\n            bestAs = np.array(np.argwhere(counts == np.max(counts))).flatten()\n            bestA = np.random.choice(bestAs)\n            probs = [0] * len(counts)\n            probs[bestA] = 1\n            return probs\n\n        counts = [x ** (1. / temp) for x in counts]\n        counts_sum = float(sum(counts))\n        probs = [x / counts_sum for x in counts]\n        return probs\n\n    def search(self, canonicalBoard):\n        """"""\n        This function performs one iteration of MCTS. It is recursively called\n        till a leaf node is found. The action chosen at each node is one that\n        has the maximum upper confidence bound as in the paper.\n\n        Once a leaf node is found, the neural network is called to return an\n        initial policy P and a value v for the state. This value is propagated\n        up the search path. In case the leaf node is a terminal state, the\n        outcome is propagated up the search path. The values of Ns, Nsa, Qsa are\n        updated.\n\n        NOTE: the return values are the negative of the value of the current\n        state. This is done since v is in [-1,1] and if v is the value of a\n        state for the current player, then its value is -v for the other player.\n\n        Returns:\n            v: the negative of the value of the current canonicalBoard\n        """"""\n\n        s = self.game.stringRepresentation(canonicalBoard)\n\n        if s not in self.Es:\n            self.Es[s] = self.game.getGameEnded(canonicalBoard, 1)\n        if self.Es[s] != 0:\n            # terminal node\n            return -self.Es[s]\n\n        if s not in self.Ps:\n            # leaf node\n            self.Ps[s], v = self.nnet.predict(canonicalBoard)\n            valids = self.game.getValidMoves(canonicalBoard, 1)\n            self.Ps[s] = self.Ps[s] * valids  # masking invalid moves\n            sum_Ps_s = np.sum(self.Ps[s])\n            if sum_Ps_s > 0:\n                self.Ps[s] /= sum_Ps_s  # renormalize\n            else:\n                # if all valid moves were masked make all valid moves equally probable\n\n                # NB! All valid moves may be masked if either your NNet architecture is insufficient or you\'ve get overfitting or something else.\n                # If you have got dozens or hundreds of these messages you should pay attention to your NNet and/or training process.   \n                log.error(""All valid moves were masked, doing a workaround."")\n                self.Ps[s] = self.Ps[s] + valids\n                self.Ps[s] /= np.sum(self.Ps[s])\n\n            self.Vs[s] = valids\n            self.Ns[s] = 0\n            return -v\n\n        valids = self.Vs[s]\n        cur_best = -float(\'inf\')\n        best_act = -1\n\n        # pick the action with the highest upper confidence bound\n        for a in range(self.game.getActionSize()):\n            if valids[a]:\n                if (s, a) in self.Qsa:\n                    u = self.Qsa[(s, a)] + self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s]) / (\n                            1 + self.Nsa[(s, a)])\n                else:\n                    u = self.args.cpuct * self.Ps[s][a] * math.sqrt(self.Ns[s] + EPS)  # Q = 0 ?\n\n                if u > cur_best:\n                    cur_best = u\n                    best_act = a\n\n        a = best_act\n        next_s, next_player = self.game.getNextState(canonicalBoard, 1, a)\n        next_s = self.game.getCanonicalForm(next_s, next_player)\n\n        v = self.search(next_s)\n\n        if (s, a) in self.Qsa:\n            self.Qsa[(s, a)] = (self.Nsa[(s, a)] * self.Qsa[(s, a)] + v) / (self.Nsa[(s, a)] + 1)\n            self.Nsa[(s, a)] += 1\n\n        else:\n            self.Qsa[(s, a)] = v\n            self.Nsa[(s, a)] = 1\n\n        self.Ns[s] += 1\n        return -v\n'"
NeuralNet.py,0,"b'class NeuralNet():\n    """"""\n    This class specifies the base NeuralNet class. To define your own neural\n    network, subclass this class and implement the functions below. The neural\n    network does not consider the current player, and instead only deals with\n    the canonical form of the board.\n\n    See othello/NNet.py for an example implementation.\n    """"""\n\n    def __init__(self, game):\n        pass\n\n    def train(self, examples):\n        """"""\n        This function trains the neural network with examples obtained from\n        self-play.\n\n        Input:\n            examples: a list of training examples, where each example is of form\n                      (board, pi, v). pi is the MCTS informed policy vector for\n                      the given board, and v is its value. The examples has\n                      board in its canonical form.\n        """"""\n        pass\n\n    def predict(self, board):\n        """"""\n        Input:\n            board: current board in its canonical form.\n\n        Returns:\n            pi: a policy vector for the current board- a numpy array of length\n                game.getActionSize\n            v: a float in [-1,1] that gives the value of the current board\n        """"""\n        pass\n\n    def save_checkpoint(self, folder, filename):\n        """"""\n        Saves the current neural network (with its parameters) in\n        folder/filename\n        """"""\n        pass\n\n    def load_checkpoint(self, folder, filename):\n        """"""\n        Loads parameters of the neural network from folder/filename\n        """"""\n        pass\n'"
main.py,1,"b'import logging\n\nimport coloredlogs\n\nfrom Coach import Coach\nfrom othello.OthelloGame import OthelloGame as Game\nfrom othello.pytorch.NNet import NNetWrapper as nn\nfrom utils import *\n\nlog = logging.getLogger(__name__)\n\ncoloredlogs.install(level=\'INFO\')  # Change this to DEBUG to see more info.\n\nargs = dotdict({\n    \'numIters\': 1000,\n    \'numEps\': 100,              # Number of complete self-play games to simulate during a new iteration.\n    \'tempThreshold\': 15,        #\n    \'updateThreshold\': 0.6,     # During arena playoff, new neural net will be accepted if threshold or more of games are won.\n    \'maxlenOfQueue\': 200000,    # Number of game examples to train the neural networks.\n    \'numMCTSSims\': 25,          # Number of games moves for MCTS to simulate.\n    \'arenaCompare\': 40,         # Number of games to play during arena play to determine if new net will be accepted.\n    \'cpuct\': 1,\n\n    \'checkpoint\': \'./temp/\',\n    \'load_model\': False,\n    \'load_folder_file\': (\'/dev/models/8x100x50\',\'best.pth.tar\'),\n    \'numItersForTrainExamplesHistory\': 20,\n\n})\n\n\ndef main():\n    log.info(\'Loading %s...\', Game.__name__)\n    g = Game(6)\n\n    log.info(\'Loading %s...\', nn.__name__)\n    nnet = nn(g)\n\n    if args.load_model:\n        log.info(\'Loading checkpoint ""%s/%s""...\', args.load_folder_file)\n        nnet.load_checkpoint(args.load_folder_file[0], args.load_folder_file[1])\n    else:\n        log.warning(\'Not loading a checkpoint!\')\n\n    log.info(\'Loading the Coach...\')\n    c = Coach(g, nnet, args)\n\n    if args.load_model:\n        log.info(""Loading \'trainExamples\' from file..."")\n        c.loadTrainExamples()\n\n    log.info(\'Starting the learning process \xf0\x9f\x8e\x89\')\n    c.learn()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
pit.py,1,"b'import Arena\nfrom MCTS import MCTS\nfrom othello.OthelloGame import OthelloGame\nfrom othello.OthelloPlayers import *\nfrom othello.pytorch.NNet import NNetWrapper as NNet\n\n\nimport numpy as np\nfrom utils import *\n\n""""""\nuse this script to play any two agents against each other, or play manually with\nany agent.\n""""""\n\nmini_othello = False  # Play in 6x6 instead of the normal 8x8.\nhuman_vs_cpu = True\n\nif mini_othello:\n    g = OthelloGame(6)\nelse:\n    g = OthelloGame(8)\n\n# all players\nrp = RandomPlayer(g).play\ngp = GreedyOthelloPlayer(g).play\nhp = HumanOthelloPlayer(g).play\n\n\n\n# nnet players\nn1 = NNet(g)\nif mini_othello:\n    n1.load_checkpoint(\'./pretrained_models/othello/pytorch/\',\'6x100x25_best.pth.tar\')\nelse:\n    n1.load_checkpoint(\'./pretrained_models/othello/pytorch/\',\'8x8_100checkpoints_best.pth.tar\')\nargs1 = dotdict({\'numMCTSSims\': 50, \'cpuct\':1.0})\nmcts1 = MCTS(g, n1, args1)\nn1p = lambda x: np.argmax(mcts1.getActionProb(x, temp=0))\n\nif human_vs_cpu:\n    player2 = hp\nelse:\n    n2 = NNet(g)\n    n2.load_checkpoint(\'./pretrained_models/othello/pytorch/\', \'8x8_100checkpoints_best.pth.tar\')\n    args2 = dotdict({\'numMCTSSims\': 50, \'cpuct\': 1.0})\n    mcts2 = MCTS(g, n2, args2)\n    n2p = lambda x: np.argmax(mcts2.getActionProb(x, temp=0))\n\n    player2 = n2p  # Player 2 is neural network if it\'s cpu vs cpu.\n\narena = Arena.Arena(n1p, player2, g, display=OthelloGame.display)\n\nprint(arena.playGames(2, verbose=True))\n'"
test_all_games.py,1,"b'""""""""\n\n    This is a Regression Test Suite to automatically test all combinations of games and ML frameworks. Each test\n    plays two quick games using an untrained neural network (randomly initialized) against a random player.\n\n    In order for the entire test suite to run successfully, all the required libraries must be installed.  They are:\n    Pytorch, Keras, Tensorflow.\n\n     [ Games ]      Pytorch     Tensorflow  Keras\n      -----------   -------     ----------  -----\n    - Othello       [Yes]       [Yes]       [Yes]\n    - TicTacToe                             [Yes]\n    - Connect4                  [Yes]\n    - Gobang                    [Yes]       [Yes]\n\n""""""\n\nimport unittest\n\nimport Arena\nfrom MCTS import MCTS\n\nfrom tictactoe.TicTacToeGame import TicTacToeGame\nfrom tictactoe.TicTacToePlayers import *\nfrom tictactoe.keras.NNet import NNetWrapper as TicTacToeKerasNNet\n\nfrom tictactoe_3d.TicTacToeGame import TicTacToeGame as TicTacToe3DGame\nfrom tictactoe_3d.TicTacToePlayers import *\nfrom tictactoe_3d.keras.NNet import NNetWrapper as TicTacToe3DKerasNNet\n\nfrom othello.OthelloGame import OthelloGame\nfrom othello.OthelloPlayers import *\nfrom othello.pytorch.NNet import NNetWrapper as OthelloPytorchNNet\nfrom othello.tensorflow.NNet import NNetWrapper as OthelloTensorflowNNet\nfrom othello.keras.NNet import NNetWrapper as OthelloKerasNNet\n\nfrom connect4.Connect4Game import Connect4Game\nfrom connect4.Connect4Players import *\nfrom connect4.tensorflow.NNet import NNetWrapper as Connect4TensorflowNNet\n\nfrom gobang.GobangGame import GobangGame\nfrom gobang.GobangPlayers import *\nfrom gobang.keras.NNet import NNetWrapper as GobangKerasNNet\nfrom gobang.tensorflow.NNet import NNetWrapper as GobangTensorflowNNet\n\nimport numpy as np\nfrom utils import *\n\nclass TestAllGames(unittest.TestCase):\n\n    @staticmethod\n    def execute_game_test(game, neural_net):\n        rp = RandomPlayer(game).play\n\n        args = dotdict({\'numMCTSSims\': 25, \'cpuct\': 1.0})\n        mcts = MCTS(game, neural_net(game), args)\n        n1p = lambda x: np.argmax(mcts.getActionProb(x, temp=0))\n\n        arena = Arena.Arena(n1p, rp, game)\n        print(arena.playGames(2, verbose=False))\n\n    def test_othello_pytorch(self):\n        self.execute_game_test(OthelloGame(6), OthelloPytorchNNet)\n\n    def test_othello_tensorflow(self):\n        self.execute_game_test(OthelloGame(6), OthelloTensorflowNNet)\n\n    def test_othello_keras(self):\n        self.execute_game_test(OthelloGame(6), OthelloKerasNNet)\n\n    def test_tictactoe_keras(self):\n        self.execute_game_test(TicTacToeGame(), TicTacToeKerasNNet)\n\n    def test_connect4_tensorflow(self):\n        self.execute_game_test(Connect4Game(), Connect4TensorflowNNet)\n\n    def test_gobang_keras(self):\n        self.execute_game_test(GobangGame(), GobangKerasNNet)\n\n    def test_gobang_tensorflow(self):\n        self.execute_game_test(GobangGame(), GobangTensorflowNNet)\n\n\nif __name__ == \'__main__\':\n    unittest.main()'"
utils.py,0,"b'class AverageMeter(object):\n    """"""From https://github.com/pytorch/examples/blob/master/imagenet/main.py""""""\n\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def __repr__(self):\n        return f\'{self.avg:.2e}\'\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\nclass dotdict(dict):\n    def __getattr__(self, name):\n        return self[name]\n'"
connect4/Connect4Game.py,0,"b'import sys\nimport numpy as np\n\nsys.path.append(\'..\')\nfrom Game import Game\nfrom .Connect4Logic import Board\n\n\nclass Connect4Game(Game):\n    """"""\n    Connect4 Game class implementing the alpha-zero-general Game interface.\n    """"""\n\n    def __init__(self, height=None, width=None, win_length=None, np_pieces=None):\n        Game.__init__(self)\n        self._base_board = Board(height, width, win_length, np_pieces)\n\n    def getInitBoard(self):\n        return self._base_board.np_pieces\n\n    def getBoardSize(self):\n        return (self._base_board.height, self._base_board.width)\n\n    def getActionSize(self):\n        return self._base_board.width\n\n    def getNextState(self, board, player, action):\n        """"""Returns a copy of the board with updated move, original board is unmodified.""""""\n        b = self._base_board.with_np_pieces(np_pieces=np.copy(board))\n        b.add_stone(action, player)\n        return b.np_pieces, -player\n\n    def getValidMoves(self, board, player):\n        ""Any zero value in top row in a valid move""\n        return self._base_board.with_np_pieces(np_pieces=board).get_valid_moves()\n\n    def getGameEnded(self, board, player):\n        b = self._base_board.with_np_pieces(np_pieces=board)\n        winstate = b.get_win_state()\n        if winstate.is_ended:\n            if winstate.winner is None:\n                # draw has very little value.\n                return 1e-4\n            elif winstate.winner == player:\n                return +1\n            elif winstate.winner == -player:\n                return -1\n            else:\n                raise ValueError(\'Unexpected winstate found: \', winstate)\n        else:\n            # 0 used to represent unfinished game.\n            return 0\n\n    def getCanonicalForm(self, board, player):\n        # Flip player from 1 to -1\n        return board * player\n\n    def getSymmetries(self, board, pi):\n        """"""Board is left/right board symmetric""""""\n        return [(board, pi), (board[:, ::-1], pi[::-1])]\n\n    def stringRepresentation(self, board):\n        return board.tostring()\n\n    @staticmethod\n    def display(board):\n        print("" -----------------------"")\n        print(\' \'.join(map(str, range(len(board[0])))))\n        print(board)\n        print("" -----------------------"")\n'"
connect4/Connect4Logic.py,0,"b'from collections import namedtuple\nimport numpy as np\n\nDEFAULT_HEIGHT = 6\nDEFAULT_WIDTH = 7\nDEFAULT_WIN_LENGTH = 4\n\nWinState = namedtuple(\'WinState\', \'is_ended winner\')\n\n\nclass Board():\n    """"""\n    Connect4 Board.\n    """"""\n\n    def __init__(self, height=None, width=None, win_length=None, np_pieces=None):\n        ""Set up initial board configuration.""\n        self.height = height or DEFAULT_HEIGHT\n        self.width = width or DEFAULT_WIDTH\n        self.win_length = win_length or DEFAULT_WIN_LENGTH\n\n        if np_pieces is None:\n            self.np_pieces = np.zeros([self.height, self.width], dtype=np.int)\n        else:\n            self.np_pieces = np_pieces\n            assert self.np_pieces.shape == (self.height, self.width)\n\n    def add_stone(self, column, player):\n        ""Create copy of board containing new stone.""\n        available_idx, = np.where(self.np_pieces[:, column] == 0)\n        if len(available_idx) == 0:\n            raise ValueError(""Can\'t play column %s on board %s"" % (column, self))\n\n        self.np_pieces[available_idx[-1]][column] = player\n\n    def get_valid_moves(self):\n        ""Any zero value in top row in a valid move""\n        return self.np_pieces[0] == 0\n\n    def get_win_state(self):\n        for player in [-1, 1]:\n            player_pieces = self.np_pieces == -player\n            # Check rows & columns for win\n            if (self._is_straight_winner(player_pieces) or\n                self._is_straight_winner(player_pieces.transpose()) or\n                self._is_diagonal_winner(player_pieces)):\n                return WinState(True, -player)\n\n        # draw has very little value.\n        if not self.get_valid_moves().any():\n            return WinState(True, None)\n\n        # Game is not ended yet.\n        return WinState(False, None)\n\n    def with_np_pieces(self, np_pieces):\n        """"""Create copy of board with specified pieces.""""""\n        if np_pieces is None:\n            np_pieces = self.np_pieces\n        return Board(self.height, self.width, self.win_length, np_pieces)\n\n    def _is_diagonal_winner(self, player_pieces):\n        """"""Checks if player_pieces contains a diagonal win.""""""\n        win_length = self.win_length\n        for i in range(len(player_pieces) - win_length + 1):\n            for j in range(len(player_pieces[0]) - win_length + 1):\n                if all(player_pieces[i + x][j + x] for x in range(win_length)):\n                    return True\n            for j in range(win_length - 1, len(player_pieces[0])):\n                if all(player_pieces[i + x][j - x] for x in range(win_length)):\n                    return True\n        return False\n\n    def _is_straight_winner(self, player_pieces):\n        """"""Checks if player_pieces contains a vertical or horizontal win.""""""\n        run_lengths = [player_pieces[:, i:i + self.win_length].sum(axis=1)\n                       for i in range(len(player_pieces) - self.win_length + 2)]\n        return max([x.max() for x in run_lengths]) >= self.win_length\n\n    def __str__(self):\n        return str(self.np_pieces)\n'"
connect4/Connect4Players.py,0,"b'import numpy as np\n\n\nclass RandomPlayer():\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        a = np.random.randint(self.game.getActionSize())\n        valids = self.game.getValidMoves(board, 1)\n        while valids[a] != 1:\n            a = np.random.randint(self.game.getActionSize())\n        return a\n\n\nclass HumanConnect4Player():\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        valid_moves = self.game.getValidMoves(board, 1)\n        print(\'\\nMoves:\', [i for (i, valid) in enumerate(valid_moves) if valid])\n\n        while True:\n            move = int(input())\n            if valid_moves[move]: break\n            else: print(\'Invalid move\')\n        return move\n\n\nclass OneStepLookaheadConnect4Player():\n    """"""Simple player who always takes a win if presented, or blocks a loss if obvious, otherwise is random.""""""\n    def __init__(self, game, verbose=True):\n        self.game = game\n        self.player_num = 1\n        self.verbose = verbose\n\n    def play(self, board):\n        valid_moves = self.game.getValidMoves(board, self.player_num)\n        win_move_set = set()\n        fallback_move_set = set()\n        stop_loss_move_set = set()\n        for move, valid in enumerate(valid_moves):\n            if not valid: continue\n            if self.player_num == self.game.getGameEnded(*self.game.getNextState(board, self.player_num, move)):\n                win_move_set.add(move)\n            if -self.player_num == self.game.getGameEnded(*self.game.getNextState(board, -self.player_num, move)):\n                stop_loss_move_set.add(move)\n            else:\n                fallback_move_set.add(move)\n\n        if len(win_move_set) > 0:\n            ret_move = np.random.choice(list(win_move_set))\n            if self.verbose: print(\'Playing winning action %s from %s\' % (ret_move, win_move_set))\n        elif len(stop_loss_move_set) > 0:\n            ret_move = np.random.choice(list(stop_loss_move_set))\n            if self.verbose: print(\'Playing loss stopping action %s from %s\' % (ret_move, stop_loss_move_set))\n        elif len(fallback_move_set) > 0:\n            ret_move = np.random.choice(list(fallback_move_set))\n            if self.verbose: print(\'Playing random action %s from %s\' % (ret_move, fallback_move_set))\n        else:\n            raise Exception(\'No valid moves remaining: %s\' % game.stringRepresentation(board))\n\n        return ret_move\n'"
connect4/__init__.py,0,b''
connect4/test_connect4.py,0,"b'""""""\nTo run tests:\npytest-3 connect4\n""""""\n\nfrom collections import namedtuple\nimport textwrap\nimport numpy as np\n\nfrom .Connect4Game import Connect4Game\n\n# Tuple of (Board, Player, Game) to simplify testing.\nBPGTuple = namedtuple(\'BPGTuple\', \'board player game\')\n\n\ndef init_board_from_moves(moves, height=None, width=None):\n    """"""Returns a BPGTuple based on series of specified moved.""""""\n    game = Connect4Game(height=height, width=width)\n    board, player = game.getInitBoard(), 1\n    for move in moves:\n        board, player = game.getNextState(board, player, move)\n    return BPGTuple(board, player, game)\n\n\ndef init_board_from_array(board, player):\n    """"""Returns a BPGTuple based on series of specified moved.""""""\n    game = Connect4Game(height=len(board), width=len(board[0]))\n    return BPGTuple(board, player, game)\n\n\ndef test_simple_moves():\n    board, player, game = init_board_from_moves([4, 5, 4, 3, 0, 6])\n    expected = textwrap.dedent(""""""\\\n        [[ 0.  0.  0.  0.  0.  0.  0.]\n         [ 0.  0.  0.  0.  0.  0.  0.]\n         [ 0.  0.  0.  0.  0.  0.  0.]\n         [ 0.  0.  0.  0.  0.  0.  0.]\n         [ 0.  0.  0.  0.  1.  0.  0.]\n         [ 1.  0.  0. -1.  1. -1. -1.]]"""""")\n    assert expected == game.stringRepresentation(board)\n\n\ndef test_overfull_column():\n    for height in range(1, 10):\n        # Fill to max height is ok\n        init_board_from_moves([4] * height, height=height)\n\n        # Check overfilling causes an error.\n        try:\n            init_board_from_moves([4] * (height + 1), height=height)\n            assert False, ""Expected error when overfilling column""\n        except ValueError:\n            pass  # Expected.\n\n\ndef test_get_valid_moves():\n    """"""Tests vector of valid moved is correct.""""""\n    move_valid_pairs = [\n        ([], [True] * 7),\n        ([0, 1, 2, 3, 4, 5, 6], [True] * 7),\n        ([0, 1, 2, 3, 4, 5, 6] * 5, [True] * 7),\n        ([0, 1, 2, 3, 4, 5, 6] * 6, [False] * 7),\n        ([0, 1, 2] * 3 + [3, 4, 5, 6] * 6, [True] * 3 + [False] * 4),\n    ]\n\n    for moves, expected_valid in move_valid_pairs:\n        board, player, game = init_board_from_moves(moves)\n        assert (np.array(expected_valid) == game.getValidMoves(board, player)).all()\n\n\ndef test_symmetries():\n    """"""Tests symetric board are produced.""""""\n    board, player, game = init_board_from_moves([0, 0, 1, 0, 6])\n    pi = [0.1, 0.2, 0.3]\n    (board1, pi1), (board2, pi2) = game.getSymmetries(board, pi)\n    assert [0.1, 0.2, 0.3] == pi1 and [0.3, 0.2, 0.1] == pi2\n\n    expected_board1 = textwrap.dedent(""""""\\\n        [[ 0.  0.  0.  0.  0.  0.  0.]\n         [ 0.  0.  0.  0.  0.  0.  0.]\n         [ 0.  0.  0.  0.  0.  0.  0.]\n         [-1.  0.  0.  0.  0.  0.  0.]\n         [-1.  0.  0.  0.  0.  0.  0.]\n         [ 1.  1.  0.  0.  0.  0.  1.]]"""""")\n    assert expected_board1 == game.stringRepresentation(board1)\n\n    expected_board2 = textwrap.dedent(""""""\\\n        [[ 0.  0.  0.  0.  0.  0.  0.]\n         [ 0.  0.  0.  0.  0.  0.  0.]\n         [ 0.  0.  0.  0.  0.  0.  0.]\n         [ 0.  0.  0.  0.  0.  0. -1.]\n         [ 0.  0.  0.  0.  0.  0. -1.]\n         [ 1.  0.  0.  0.  0.  1.  1.]]"""""")\n    assert expected_board2 == game.stringRepresentation(board2)\n\n\ndef test_game_ended():\n    """"""Tests game end detection logic based on fixed boards.""""""\n    array_end_state_pairs = [\n        (np.array([[0, 0, 0, 0, 0, 0, 0],\n                   [0, 0, 0, 0, 0, 0, 0],\n                   [0, 0, 0, 0, 0, 0, 0],\n                   [0, 0, 0, 0, 0, 0, 0],\n                   [0, 0, 0, 0, 0, 0, 0]]), 1, 0),\n        (np.array([[0, 0, 0, 0, 0, 0, 0],\n                   [0, 0, 0, 0, 0, 1, 0],\n                   [0, 0, 0, 0, 1, 0, 0],\n                   [0, 0, 0, 1, 0, 0, 0],\n                   [0, 0, 1, 0, 0, 0, 0],\n                   [0, 0, 0, 0, 0, 0, 0]]), 1, 1),\n        (np.array([[0, 0, 0, 0, 1, 0, 0],\n                   [0, 0, 0, 1, 0, 0, 0],\n                   [0, 0, 1, 0, 0, 0, 0],\n                   [0, 1, 0, 0, 0, 0, 0],\n                   [0, 0, 0, 0, 0, 0, 0]]), -1, -1),\n        (np.array([[0, 0, 0, 0, 0, 0, 0],\n                   [0, 0, 1, 0, 0, 0, 0],\n                   [0, 0, 0, 1, 0, 0, 0],\n                   [0, 0, 0, 0, 1, 0, 0],\n                   [0, 0, 0, 0, 0, 1, 0]]), -1, -1),\n        (np.array([[0, 0, 0, -1],\n                   [0, 0, -1, 0],\n                   [0, -1, 0, 0],\n                   [-1, 0, 0, 0]]), 1, -1),\n        (np.array([[0, 0, 0, 0, 1],\n                   [0, 0, 0, 1, 0],\n                   [0, 0, 1, 0, 0],\n                   [0, 1, 0, 0, 0]]), -1, -1),\n        (np.array([[1, 0, 0, 0, 0],\n                   [0, 1, 0, 0, 0],\n                   [0, 0, 1, 0, 0],\n                   [0, 0, 0, 1, 0]]), -1, -1),\n        (np.array([[ 0,  0,  0,  0,  0,  0,  0],\n                   [ 0,  0,  0, -1,  0,  0,  0],\n                   [ 0,  0,  0, -1,  0,  0,  1],\n                   [ 0,  0,  0,  1,  1, -1, -1],\n                   [ 0,  0,  0, -1,  1,  1,  1],\n                   [ 0, -1,  0, -1,  1, -1,  1]]), -1, 0),\n        (np.array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.],\n                   [ 0.,  0.,  0., -1.,  0.,  0.,  0.],\n                   [ 1.,  0.,  1., -1.,  0.,  0.,  0.],\n                   [-1., -1.,  1.,  1.,  0.,  0.,  0.],\n                   [ 1.,  1.,  1., -1.,  0.,  0.,  0.],\n                   [ 1., -1.,  1., -1.,  0., -1.,  0.]]), -1, -1),\n        (np.array([[ 0.,  0.,  0.,  1.,  0.,  0.,  0.,],\n                   [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,],\n                   [ 0.,  0.,  0., -1.,  0.,  0.,  0.,],\n                   [ 0.,  0.,  1.,  1., -1.,  0., -1.,],\n                   [ 0.,  0., -1.,  1.,  1.,  1.,  1.,],\n                   [-1.,  0., -1.,  1., -1., -1., -1.,],]), 1, 1),\n        ]\n\n    for np_pieces, player, expected_end_state in array_end_state_pairs:\n        board, player, game = init_board_from_array(np_pieces, player)\n        end_state = game.getGameEnded(board, player)\n        assert expected_end_state == end_state, (""expected=%s, actual=%s, board=\\n%s"" % (expected_end_state, end_state, board))\n\n\ndef test_immutable_move():\n    """"""Test original board is not mutated whtn getNextState() called.""""""\n    board, player, game = init_board_from_moves([1, 2, 3, 3, 4])\n    original_board_string = game.stringRepresentation(board)\n\n    new_np_pieces, new_player = game.getNextState(board, 3, -1)\n\n    assert original_board_string == game.stringRepresentation(board)\n    assert original_board_string != game.stringRepresentation(new_np_pieces)\n'"
docker/jupyter_notebook_config.py,0,"b'# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\nimport os\nfrom IPython.lib import passwd\n\nc = c  # pylint:disable=undefined-variable\nc.NotebookApp.ip = \'0.0.0.0\'\nc.NotebookApp.port = int(os.getenv(\'PORT\', 8888))\nc.NotebookApp.open_browser = False\n\n# sets a password if PASSWORD is set in the environment\nif \'PASSWORD\' in os.environ:\n  password = os.environ[\'PASSWORD\']\n  if password:\n    c.NotebookApp.password = passwd(password)\n  else:\n    c.NotebookApp.password = \'\'\n    c.NotebookApp.token = \'\'\n  del os.environ[\'PASSWORD\']\n'"
gobang/GobangGame.py,0,"b'from __future__ import print_function\nimport sys\nsys.path.append(\'..\')\nfrom Game import Game\nfrom .GobangLogic import Board\nimport numpy as np\n\n\nclass GobangGame(Game):\n    def __init__(self, n=15, nir=5):\n        self.n = n\n        self.n_in_row = nir\n\n    def getInitBoard(self):\n        # return initial board (numpy board)\n        b = Board(self.n)\n        return np.array(b.pieces)\n\n    def getBoardSize(self):\n        # (a,b) tuple\n        return (self.n, self.n)\n\n    def getActionSize(self):\n        # return number of actions\n        return self.n * self.n + 1\n\n    def getNextState(self, board, player, action):\n        # if player takes action on board, return next (board,player)\n        # action must be a valid move\n        if action == self.n * self.n:\n            return (board, -player)\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n        move = (int(action / self.n), action % self.n)\n        b.execute_move(move, player)\n        return (b.pieces, -player)\n\n    # modified\n    def getValidMoves(self, board, player):\n        # return a fixed size binary vector\n        valids = [0] * self.getActionSize()\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n        legalMoves = b.get_legal_moves(player)\n        if len(legalMoves) == 0:\n            valids[-1] = 1\n            return np.array(valids)\n        for x, y in legalMoves:\n            valids[self.n * x + y] = 1\n        return np.array(valids)\n\n    # modified\n    def getGameEnded(self, board, player):\n        # return 0 if not ended, 1 if player 1 won, -1 if player 1 lost\n        # player = 1\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n        n = self.n_in_row\n\n        for w in range(self.n):\n            for h in range(self.n):\n                if (w in range(self.n - n + 1) and board[w][h] != 0 and\n                        len(set(board[i][h] for i in range(w, w + n))) == 1):\n                    return board[w][h]\n                if (h in range(self.n - n + 1) and board[w][h] != 0 and\n                        len(set(board[w][j] for j in range(h, h + n))) == 1):\n                    return board[w][h]\n                if (w in range(self.n - n + 1) and h in range(self.n - n + 1) and board[w][h] != 0 and\n                        len(set(board[w + k][h + k] for k in range(n))) == 1):\n                    return board[w][h]\n                if (w in range(self.n - n + 1) and h in range(n - 1, self.n) and board[w][h] != 0 and\n                        len(set(board[w + l][h - l] for l in range(n))) == 1):\n                    return board[w][h]\n        if b.has_legal_moves():\n            return 0\n        return 1e-4\n\n    def getCanonicalForm(self, board, player):\n        # return state if player==1, else return -state if player==-1\n        return player * board\n\n    # modified\n    def getSymmetries(self, board, pi):\n        # mirror, rotational\n        assert(len(pi) == self.n**2 + 1)  # 1 for pass\n        pi_board = np.reshape(pi[:-1], (self.n, self.n))\n        l = []\n\n        for i in range(1, 5):\n            for j in [True, False]:\n                newB = np.rot90(board, i)\n                newPi = np.rot90(pi_board, i)\n                if j:\n                    newB = np.fliplr(newB)\n                    newPi = np.fliplr(newPi)\n                l += [(newB, list(newPi.ravel()) + [pi[-1]])]\n        return l\n\n    def stringRepresentation(self, board):\n        # 8x8 numpy array (canonical board)\n        return board.tostring()\n\n    @staticmethod\n    def display(board):\n        n = board.shape[0]\n\n        for y in range(n):\n            print(y, ""|"", end="""")\n        print("""")\n        print("" -----------------------"")\n        for y in range(n):\n            print(y, ""|"", end="""")    # print the row #\n            for x in range(n):\n                piece = board[y][x]    # get the piece to print\n                if piece == -1:\n                    print(""b "", end="""")\n                elif piece == 1:\n                    print(""W "", end="""")\n                else:\n                    if x == n:\n                        print(""-"", end="""")\n                    else:\n                        print(""- "", end="""")\n            print(""|"")\n        print(""   -----------------------"")\n'"
gobang/GobangLogic.py,0,"b'\'\'\'\nAuthor: MBoss\nDate: Jan 17, 2018.\nBoard class.\nBoard data:\n  1=white, -1=black, 0=empty\n  first dim is column , 2nd is row:\n     pieces[1][7] is the square in column 2,\n     at the opposite end of the board in row 8.\nSquares are stored and manipulated as (x,y) tuples.\nx is the column, y is the row.\n\'\'\'\nclass Board():\n    def __init__(self, n):\n        ""Set up initial board configuration.""\n        self.n = n\n        # Create the empty board array.\n        self.pieces = [None]*self.n\n        for i in range(self.n):\n            self.pieces[i] = [0]*self.n\n\n    # add [][] indexer syntax to the Board\n    def __getitem__(self, index): \n        return self.pieces[index]\n\n    def get_legal_moves(self, color):\n        """"""Returns all the legal moves for the given color.\n        (1 for white, -1 for black\n        """"""\n        moves = set()  # stores the legal moves.\n\n        # Get all empty locations.\n        for y in range(self.n):\n            for x in range(self.n):\n                if self[x][y] == 0:\n                    moves.add((x, y))\n        return list(moves)\n\n    def has_legal_moves(self):\n        """"""Returns True if has legal move else False\n        """"""\n        # Get all empty locations.\n        for y in range(self.n):\n            for x in range(self.n):\n                if self[x][y] == 0:\n                    return True\n        return False\n\n    def execute_move(self, move, color):\n        """"""Perform the given move on the board; flips pieces as necessary.\n        color gives the color pf the piece to play (1=white,-1=black)\n        """"""\n        (x,y) = move\n        assert self[x][y] == 0\n        self[x][y] = color\n\n'"
gobang/GobangPlayers.py,0,"b""import numpy as np\n\n\nclass RandomPlayer():\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        a = np.random.randint(self.game.getActionSize())\n        valids = self.game.getValidMoves(board, 1)\n        while valids[a]!=1:\n            a = np.random.randint(self.game.getActionSize())\n        return a\n\n\nclass HumanGobangPlayer():\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        # display(board)\n        valid = self.game.getValidMoves(board, 1)\n        for i in range(len(valid)):\n            if valid[i]:\n                print(int(i/self.game.n), int(i%self.game.n))\n        while True:\n            a = input()\n\n            x,y = [int(x) for x in a.split(' ')]\n            a = self.game.n * x + y if x!= -1 else self.game.n ** 2\n            if valid[a]:\n                break\n            else:\n                print('Invalid')\n\n        return a\n\n\nclass GreedyGobangPlayer():\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        valids = self.game.getValidMoves(board, 1)\n        candidates = []\n        for a in range(self.game.getActionSize()):\n            if valids[a]==0:\n                continue\n            nextBoard, _ = self.game.getNextState(board, 1, a)\n            score = self.game.getScore(nextBoard, 1)\n            candidates += [(-score, a)]\n        candidates.sort()\n        return candidates[0][1]\n"""
gobang/__init__.py,0,b''
othello/OthelloGame.py,0,"b'from __future__ import print_function\nimport sys\nsys.path.append(\'..\')\nfrom Game import Game\nfrom .OthelloLogic import Board\nimport numpy as np\n\nclass OthelloGame(Game):\n    square_content = {\n        -1: ""X"",\n        +0: ""-"",\n        +1: ""O""\n    }\n\n    @staticmethod\n    def getSquarePiece(piece):\n        return OthelloGame.square_content[piece]\n\n    def __init__(self, n):\n        self.n = n\n\n    def getInitBoard(self):\n        # return initial board (numpy board)\n        b = Board(self.n)\n        return np.array(b.pieces)\n\n    def getBoardSize(self):\n        # (a,b) tuple\n        return (self.n, self.n)\n\n    def getActionSize(self):\n        # return number of actions\n        return self.n*self.n + 1\n\n    def getNextState(self, board, player, action):\n        # if player takes action on board, return next (board,player)\n        # action must be a valid move\n        if action == self.n*self.n:\n            return (board, -player)\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n        move = (int(action/self.n), action%self.n)\n        b.execute_move(move, player)\n        return (b.pieces, -player)\n\n    def getValidMoves(self, board, player):\n        # return a fixed size binary vector\n        valids = [0]*self.getActionSize()\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n        legalMoves =  b.get_legal_moves(player)\n        if len(legalMoves)==0:\n            valids[-1]=1\n            return np.array(valids)\n        for x, y in legalMoves:\n            valids[self.n*x+y]=1\n        return np.array(valids)\n\n    def getGameEnded(self, board, player):\n        # return 0 if not ended, 1 if player 1 won, -1 if player 1 lost\n        # player = 1\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n        if b.has_legal_moves(player):\n            return 0\n        if b.has_legal_moves(-player):\n            return 0\n        if b.countDiff(player) > 0:\n            return 1\n        return -1\n\n    def getCanonicalForm(self, board, player):\n        # return state if player==1, else return -state if player==-1\n        return player*board\n\n    def getSymmetries(self, board, pi):\n        # mirror, rotational\n        assert(len(pi) == self.n**2+1)  # 1 for pass\n        pi_board = np.reshape(pi[:-1], (self.n, self.n))\n        l = []\n\n        for i in range(1, 5):\n            for j in [True, False]:\n                newB = np.rot90(board, i)\n                newPi = np.rot90(pi_board, i)\n                if j:\n                    newB = np.fliplr(newB)\n                    newPi = np.fliplr(newPi)\n                l += [(newB, list(newPi.ravel()) + [pi[-1]])]\n        return l\n\n    def stringRepresentation(self, board):\n        return board.tostring()\n\n    def stringRepresentationReadable(self, board):\n        board_s = """".join(self.square_content[square] for row in board for square in row)\n        return board_s\n\n    def getScore(self, board, player):\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n        return b.countDiff(player)\n\n    @staticmethod\n    def display(board):\n        n = board.shape[0]\n        print(""   "", end="""")\n        for y in range(n):\n            print(y, end="" "")\n        print("""")\n        print(""-----------------------"")\n        for y in range(n):\n            print(y, ""|"", end="""")    # print the row #\n            for x in range(n):\n                piece = board[y][x]    # get the piece to print\n                print(OthelloGame.square_content[piece], end="" "")\n            print(""|"")\n\n        print(""-----------------------"")\n'"
othello/OthelloLogic.py,0,"b'\'\'\'\nAuthor: Eric P. Nichols\nDate: Feb 8, 2008.\nBoard class.\nBoard data:\n  1=white, -1=black, 0=empty\n  first dim is column , 2nd is row:\n     pieces[1][7] is the square in column 2,\n     at the opposite end of the board in row 8.\nSquares are stored and manipulated as (x,y) tuples.\nx is the column, y is the row.\n\'\'\'\nclass Board():\n\n    # list of all 8 directions on the board, as (x,y) offsets\n    __directions = [(1,1),(1,0),(1,-1),(0,-1),(-1,-1),(-1,0),(-1,1),(0,1)]\n\n    def __init__(self, n):\n        ""Set up initial board configuration.""\n\n        self.n = n\n        # Create the empty board array.\n        self.pieces = [None]*self.n\n        for i in range(self.n):\n            self.pieces[i] = [0]*self.n\n\n        # Set up the initial 4 pieces.\n        self.pieces[int(self.n/2)-1][int(self.n/2)] = 1\n        self.pieces[int(self.n/2)][int(self.n/2)-1] = 1\n        self.pieces[int(self.n/2)-1][int(self.n/2)-1] = -1;\n        self.pieces[int(self.n/2)][int(self.n/2)] = -1;\n\n    # add [][] indexer syntax to the Board\n    def __getitem__(self, index): \n        return self.pieces[index]\n\n    def countDiff(self, color):\n        """"""Counts the # pieces of the given color\n        (1 for white, -1 for black, 0 for empty spaces)""""""\n        count = 0\n        for y in range(self.n):\n            for x in range(self.n):\n                if self[x][y]==color:\n                    count += 1\n                if self[x][y]==-color:\n                    count -= 1\n        return count\n\n    def get_legal_moves(self, color):\n        """"""Returns all the legal moves for the given color.\n        (1 for white, -1 for black\n        """"""\n        moves = set()  # stores the legal moves.\n\n        # Get all the squares with pieces of the given color.\n        for y in range(self.n):\n            for x in range(self.n):\n                if self[x][y]==color:\n                    newmoves = self.get_moves_for_square((x,y))\n                    moves.update(newmoves)\n        return list(moves)\n\n    def has_legal_moves(self, color):\n        for y in range(self.n):\n            for x in range(self.n):\n                if self[x][y]==color:\n                    newmoves = self.get_moves_for_square((x,y))\n                    if len(newmoves)>0:\n                        return True\n        return False\n\n    def get_moves_for_square(self, square):\n        """"""Returns all the legal moves that use the given square as a base.\n        That is, if the given square is (3,4) and it contains a black piece,\n        and (3,5) and (3,6) contain white pieces, and (3,7) is empty, one\n        of the returned moves is (3,7) because everything from there to (3,4)\n        is flipped.\n        """"""\n        (x,y) = square\n\n        # determine the color of the piece.\n        color = self[x][y]\n\n        # skip empty source squares.\n        if color==0:\n            return None\n\n        # search all possible directions.\n        moves = []\n        for direction in self.__directions:\n            move = self._discover_move(square, direction)\n            if move:\n                # print(square,move,direction)\n                moves.append(move)\n\n        # return the generated move list\n        return moves\n\n    def execute_move(self, move, color):\n        """"""Perform the given move on the board; flips pieces as necessary.\n        color gives the color pf the piece to play (1=white,-1=black)\n        """"""\n\n        #Much like move generation, start at the new piece\'s square and\n        #follow it on all 8 directions to look for a piece allowing flipping.\n\n        # Add the piece to the empty square.\n        # print(move)\n        flips = [flip for direction in self.__directions\n                      for flip in self._get_flips(move, direction, color)]\n        assert len(list(flips))>0\n        for x, y in flips:\n            #print(self[x][y],color)\n            self[x][y] = color\n\n    def _discover_move(self, origin, direction):\n        """""" Returns the endpoint for a legal move, starting at the given origin,\n        moving by the given increment.""""""\n        x, y = origin\n        color = self[x][y]\n        flips = []\n\n        for x, y in Board._increment_move(origin, direction, self.n):\n            if self[x][y] == 0:\n                if flips:\n                    # print(""Found"", x,y)\n                    return (x, y)\n                else:\n                    return None\n            elif self[x][y] == color:\n                return None\n            elif self[x][y] == -color:\n                # print(""Flip"",x,y)\n                flips.append((x, y))\n\n    def _get_flips(self, origin, direction, color):\n        """""" Gets the list of flips for a vertex and direction to use with the\n        execute_move function """"""\n        #initialize variables\n        flips = [origin]\n\n        for x, y in Board._increment_move(origin, direction, self.n):\n            #print(x,y)\n            if self[x][y] == 0:\n                return []\n            if self[x][y] == -color:\n                flips.append((x, y))\n            elif self[x][y] == color and len(flips) > 0:\n                #print(flips)\n                return flips\n\n        return []\n\n    @staticmethod\n    def _increment_move(move, direction, n):\n        # print(move)\n        """""" Generator expression for incrementing moves """"""\n        move = list(map(sum, zip(move, direction)))\n        #move = (move[0]+direction[0], move[1]+direction[1])\n        while all(map(lambda x: 0 <= x < n, move)): \n        #while 0<=move[0] and move[0]<n and 0<=move[1] and move[1]<n:\n            yield move\n            move=list(map(sum,zip(move,direction)))\n            #move = (move[0]+direction[0],move[1]+direction[1])\n\n'"
othello/OthelloPlayers.py,0,"b'import numpy as np\n\n\nclass RandomPlayer():\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        a = np.random.randint(self.game.getActionSize())\n        valids = self.game.getValidMoves(board, 1)\n        while valids[a]!=1:\n            a = np.random.randint(self.game.getActionSize())\n        return a\n\n\nclass HumanOthelloPlayer():\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        # display(board)\n        valid = self.game.getValidMoves(board, 1)\n        for i in range(len(valid)):\n            if valid[i]:\n                print(""["", int(i/self.game.n), int(i%self.game.n), end=""] "")\n        while True:\n            input_move = input()\n            input_a = input_move.split("" "")\n            if len(input_a) == 2:\n                try:\n                    x,y = [int(i) for i in input_a]\n                    if ((0 <= x) and (x < self.game.n) and (0 <= y) and (y < self.game.n)) or \\\n                            ((x == self.game.n) and (y == 0)):\n                        a = self.game.n * x + y if x != -1 else self.game.n ** 2\n                        if valid[a]:\n                            break\n                except ValueError:\n                    # Input needs to be an integer\n                    \'Invalid integer\'\n            print(\'Invalid move\')\n        return a\n\n\nclass GreedyOthelloPlayer():\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        valids = self.game.getValidMoves(board, 1)\n        candidates = []\n        for a in range(self.game.getActionSize()):\n            if valids[a]==0:\n                continue\n            nextBoard, _ = self.game.getNextState(board, 1, a)\n            score = self.game.getScore(nextBoard, 1)\n            candidates += [(-score, a)]\n        candidates.sort()\n        return candidates[0][1]\n'"
othello/__init__.py,0,b''
rts/RTSGame.py,0,"b'import sys\nfrom typing import Tuple\n\nimport numpy as np\n\nfrom rts.src.config_class import CONFIG\n\nsys.path.append(\'..\')\nfrom rts.src.Board import Board\nfrom rts.src.config import NUM_ENCODERS, NUM_ACTS, P_NAME_IDX, A_TYPE_IDX, TIME_IDX, FPS\n\n"""""" USE_TIMEOUT, MAX_TIME, d_a_type, a_max_health, INITIAL_GOLD, TIMEOUT, visibility""""""\n\n""""""\nRTSGame.pyefined rules for RTS game TD2020\nIncludes: \n- init - contains board configuration\n- getGameEnded - contains end game checking\n""""""\n\n\n# noinspection PyPep8Naming,PyMethodMayBeStatic\nclass RTSGame:\n\n    def __init__(self) -> None:\n        self.n = CONFIG.grid_size\n\n        self.initial_board_config = CONFIG.initial_board_config\n\n    def setInitBoard(self, board_config) -> None:\n        """"""\n        Sets initial_board_config. This function can be used dynamically to change board configuration. It is currently being used by rts_ue4.py, to set board configuration from ue4 game state\n        :param board_config: new initial board configuration\n        """"""\n        self.initial_board_config = board_config\n\n    def getInitBoard(self) -> np.ndarray:\n        """"""\n        :return: Returns new board from initial_board_config. That config can be dynamically changed as game progresses.\n        """"""\n        b = Board(self.n)\n        remaining_time = None  # when setting initial board, remaining time might be different\n        for e in self.initial_board_config:\n            b.pieces[e.x, e.y] = [e.player, e.a_type, e.health, e.carry, e.gold, e.timeout]\n            remaining_time = e.timeout\n        # remaining time is stored in all squares\n        b.pieces[:, :, TIME_IDX] = remaining_time\n        return np.array(b.pieces)\n\n    def getBoardSize(self) -> Tuple[int, int, int]:\n        # (a,b) tuple\n        return self.n, self.n, NUM_ENCODERS\n\n    def getActionSize(self) -> int:\n        return self.n * self.n * NUM_ACTS + 1\n\n    def getNextState(self, board: np.ndarray, player: int, action: int) -> Tuple[np.ndarray, int]:\n        """"""\n        Gets next state for board. It also updates tick for board as game tick iterations are transfered within board as 6. parameter\n        :param board: current board\n        :param player: player executing action\n        :param action: action to apply to new board\n        :return: new board with applied action\n        """"""\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n\n        y, x, action_index = np.unravel_index(action, [self.n, self.n, NUM_ACTS])\n        move = (x, y, action_index)\n\n        # first execute move, then run time function to destroy any actors if needed\n        b.execute_move(move, player)\n\n        # get config for timeout\n        if player == 1:\n            USE_TIMEOUT = CONFIG.player1_config.USE_TIMEOUT\n        else:\n            USE_TIMEOUT = CONFIG.player2_config.USE_TIMEOUT\n\n        # update timer on every tile:\n        if USE_TIMEOUT:\n            b.pieces[:, :, TIME_IDX] -= 1\n        else:\n            b.pieces[:, :, TIME_IDX] += 1\n            b.time_killer(player)\n\n        return b.pieces, -player\n\n    def getValidMoves(self, board: np.ndarray, player: int):\n\n        valids = []\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n\n        if player == 1:\n            config = CONFIG.player1_config\n        else:\n            config = CONFIG.player2_config\n\n        for y in range(self.n):\n            for x in range(self.n):\n                if b[x][y][P_NAME_IDX] == player and b[x][y][A_TYPE_IDX] != 1:  # for this player and not Gold\n                    valids.extend(b.get_moves_for_square(x, y, config=config))\n                else:\n                    valids.extend([0] * NUM_ACTS)\n        valids.append(0)  # because of that +1 in action Size\n\n        return np.array(valids)\n\n    # noinspection PyUnusedLocal\n    def getGameEnded(self, board: np.ndarray, player) -> float:\n        """"""\n        Ok, this function is where it gets complicated...\n        See, its  hard to decide when to finish rts game, as players might not have enough time to execute wanted actions, but in the other hand, if players are left to play for too long, games become very long, or even \'infinitely\' long\n        Few different approaches have been used - one is with killer_function that is starting to gradually reduce health of players as the game progresses, so players that produce more units could live longer or players that attack enemy actors, could pull themselves in winning position, as enemy now has less health\n        And the other is using timeout. Timeout just cuts game and evaluates winner using one of 3 elo functions. We\'ve found this one to be more useful, as it can be applied in 3d rts games easier and more sensibly.\n        :param board: current game state\n        :param player: current player\n        :return: real number on interval [-1,1] - return 0 if not ended, 1 if player 1 won, -1 if player 1 lost, 0.001 if tie\n        """"""\n\n        n = board.shape[0]\n\n        # detect timeout\n        if player == 1:\n            USE_TIMEOUT = CONFIG.player1_config.USE_TIMEOUT\n        else:\n            USE_TIMEOUT = CONFIG.player2_config.USE_TIMEOUT\n\n        if USE_TIMEOUT:\n            if board[0, 0, TIME_IDX] < 1:\n\n                score_player1 = self.getScore(board, player)\n                score_player2 = self.getScore(board, -player)\n\n                if score_player1 == score_player2:\n                    return 0.001\n                better_player = 1 if score_player1 > score_player2 else -1\n                return better_player\n        else:\n            if player == 1:\n                MAX_TIME = CONFIG.player1_config.MAX_TIME\n            else:\n                MAX_TIME = CONFIG.player2_config.MAX_TIME\n\n            if board[0, 0, TIME_IDX] >= MAX_TIME:\n                return 0.001\n\n        # detect win condition\n        sum_p1 = 0\n        sum_p2 = 0\n        for y in range(n):\n            for x in range(n):\n                if board[x][y][P_NAME_IDX] == 1:\n                    sum_p1 += 1\n                if board[x][y][P_NAME_IDX] == -1:\n                    sum_p2 += 1\n\n        if sum_p1 < 2:  # SUM IS 1 WHEN PLAYER ONLY HAS MINERALS LEFT\n            return -1\n        if sum_p2 < 2:  # SUM IS 1 WHEN PLAYER ONLY HAS MINERALS LEFT\n            return +1\n\n        # detect no valid actions - possible tie by overpopulating on non-attacking units and buildings - all fields are full or one player is surrounded:\n        if sum(self.getValidMoves(board, 1)) == 0:\n            return -1\n\n        if sum(self.getValidMoves(board, -1)) == 0:\n            return 1\n        # continue game\n        return 0\n\n    def getCanonicalForm(self, board: np.ndarray, player: int):\n        b = np.copy(board)\n        b[:, :, P_NAME_IDX] = b[:, :, P_NAME_IDX] * player\n        return b\n\n    def getSymmetries(self, board: np.ndarray, pi):\n        # mirror, rotational\n        assert (len(pi) == self.n * self.n * NUM_ACTS + 1)  # 1 for pass\n        pi_board = np.reshape(pi[:-1], (self.n, self.n, NUM_ACTS))\n        return_list = []\n        for i in range(1, 5):\n            for j in [True, False]:\n                newB = np.rot90(board, i)\n                newPi = np.rot90(pi_board, i)\n                if j:\n                    newB = np.fliplr(newB)\n                    newPi = np.fliplr(newPi)\n                return_list += [(newB, list(newPi.ravel()) + [pi[-1]])]\n        return return_list\n\n    def stringRepresentation(self, board: np.ndarray):\n        return board.tostring()\n\n    def getScore(self, board: np.array, player: int):\n        """"""\n        Uses one of 3 elo functions that determine better player\n        :param board: game state\n        :param player: current player\n        :return: elo for current player on this board\n        """"""\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n\n        # can use different score functions for each player\n        if player == 1:\n            score_function = CONFIG.player1_config.score_function\n        else:\n            score_function = CONFIG.player2_config.score_function\n\n        if score_function == 1:\n            return b.get_health_score(player)\n        elif score_function == 2:\n            return b.get_money_score(player)\n        else:\n            return b.get_combined_score(player)\n\n\ndef display(board):\n    """"""\n    Console presentation of board\n    :param board: game state\n    :return: /\n    """"""\n    from rts.visualization.rts_pygame import init_visuals, update_graphics\n\n    if not CONFIG.visibility:\n        return\n\n    n = board.shape[0]\n    if CONFIG.visibility > 3:\n        game_display, clock = init_visuals(n, n, CONFIG.visibility)\n        update_graphics(board, game_display, clock, FPS)\n    else:\n        for y in range(n):\n            print(\'-\' * (n * 8 + 1))\n            for x in range(n):\n                a_player = board[x][y][P_NAME_IDX]\n                if a_player == 1:\n                    a_player = \'+1\'\n                if a_player == -1:\n                    a_player = \'-1\'\n                if a_player == 0:\n                    a_player = \' 0\'\n                print(""|"" + a_player + "" "" + str(board[x][y][A_TYPE_IDX]) + "" "", end="""")\n            print(""|"")\n        print(\'-\' * (n * 8 + 1))\n'"
rts/RTSPlayers.py,0,"b'import ctypes\nimport os\nimport sys\nfrom math import sqrt\nfrom typing import List\n\nimport numpy as np\nimport pygame\nfrom pygame.rect import Rect\n\nsys.path.append(\'..\')\nfrom rts.src.config import NUM_ACTS, P_NAME_IDX, A_TYPE_IDX, d_user_shortcuts, FPS, ACTS, d_a_type, ACTS_REV, d_user_shortcuts_rev\nfrom rts.visualization.rts_pygame import init_visuals, update_graphics, message_display\nfrom utils import dotdict\n\n""""""\nRTSPlayers.py\n\nContains 3 players (human player, random player, greedy player (if searching for nnet player, it is defined by pre-learnt model)\nHuman player has defined input controls for Pygame and console\n""""""\n\n\nclass RandomPlayer:\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        a = np.random.randint(self.game.getActionSize())\n        valids = self.game.getValidMoves(board, 1)\n        while valids[a] != 1:\n            a = np.random.randint(self.game.getActionSize())\n        return a\n\n\nclass HumanRTSPlayer:\n    def __init__(self, game) -> None:\n        self.game = game\n        self.USER_PLAYER = 1  # used by Human Player - this does not change if human pit player is 1 or -1\n\n    def play(self, board: np.ndarray) -> List:\n        """"""\n        Manages input using PyGame canvas/ console input\n        :param board: current board\n        :return: action to execute on current board\n        """"""\n        from rts.src.config_class import CONFIG\n\n        n = board.shape[0]\n        valid = self.game.getValidMoves(board, 1)\n        self.display_valid_moves(board, valid)\n        while True:\n\n            if CONFIG.visibility > 3:\n                a = self._manage_input(board)\n                x, y, action_index = a\n\n            else:\n                a = (input(\'type one of above actions in ""x y action_index"" format\\n\')).split("" "")\n                x, y, action = a\n                action_index = ACTS[action]\n            # convert to action index in valids array\n\n            try:\n\n                tup = (int(y), int(x), int(action_index))\n                a = np.ravel_multi_index(tup, (n, n, NUM_ACTS))\n            except Exception as e:\n                print(""Could not parse action"")\n            if valid[a]:\n                break\n            else:\n                print(\'This action is invalid!\')\n                self.display_valid_moves(board, valid)\n\n        return a\n\n    def display_valid_moves(self, board, valid) -> None:\n        """"""\n        Displays all valid moves in console for specific board\n        :param board: board to display moves upon\n        :param valid: vector of valid moves\n        """"""\n        if valid is None:\n            valid = self.game.getValidMoves(board, 1)\n        n = board.shape[0]\n        print(""----------"")\n        for i in range(len(valid)):\n            if valid[i]:\n                y, x, action_index = np.unravel_index(i, [n, n, NUM_ACTS])\n                print(x, y, ACTS_REV[action_index])\n                print(""----------"")\n\n    @staticmethod\n    def select_object(board: np.ndarray, click_location: tuple) -> dotdict:\n        """"""\n        Selects object on PyGame canvas using mouse click\n        :param board: game state board\n        :param click_location: tuple (x,y) that represents canvas click location\n        :return: game tile coordinate (x,y)\n        """"""\n        n = board.shape[0]\n        canvas_scale = int(ctypes.windll.user32.GetSystemMetrics(1) * (16 / 30) / n)  # for drawing - it takes 2 thirds of screen height\n\n        # select object by clicking on it - you can select only your objects\n\n        for y in range(n):\n            for x in range(n):\n                actor_location = (int(x * canvas_scale + canvas_scale / 2 + canvas_scale), int(y * canvas_scale + canvas_scale / 2) + canvas_scale)\n                actor_x, actor_y = actor_location\n                actor_size = int(canvas_scale / 3)\n\n                click_x, click_y = click_location\n\n                dist = sqrt((actor_x - click_x) ** 2 + (actor_y - click_y) ** 2)\n                if dist <= actor_size:\n                    return dotdict({""x"": x, ""y"": y})\n        return dotdict({""x"": -1, ""y"": -1})\n\n    def _manage_input(self, board: np.ndarray) -> list:\n        """"""\n        Manages click and keyboard selections on PyGame canvas\n        :param board: game state\n        :return: /\n        """"""\n        from rts.src.Board import Board\n        from rts.src.config_class import CONFIG\n\n        n = board.shape[0]\n\n        game_display, clock = init_visuals(n, n, CONFIG.visibility)\n        update_graphics(board, game_display, clock, FPS)\n\n        canvas_scale: int = int(ctypes.windll.user32.GetSystemMetrics(1) * (16 / 30) / n)\n        clicked_actor = None\n        clicked_actor_index_arr = []\n        while True:\n            for event in pygame.event.get():\n                # print(event)\n                if event.type == pygame.QUIT:\n                    pygame.quit()\n                    raise SystemExit(0)\n                if event.type == pygame.KEYDOWN:\n\n                    if clicked_actor and (board[clicked_actor.x][clicked_actor.y][P_NAME_IDX] == self.USER_PLAYER):\n                        try:\n\n                            shortcut_pressed = d_user_shortcuts[event.unicode]\n                            action_to_execute = shortcut_pressed\n                            clicked_actor_index_arr.append(action_to_execute)\n                            return clicked_actor_index_arr\n                        except Exception as e:\n                            print(""shortcut \'"" + event.unicode + ""\' not supported."")\n\n                    if event.key == pygame.K_ESCAPE:\n                        pygame.quit()\n                        os._exit(0)\n\n                # handle mouse\n                if event.type == pygame.MOUSEBUTTONUP:\n                    lmb, rmb = 1, 3\n                    pos = pygame.mouse.get_pos()\n\n                    if event.button == lmb:\n                        clicked_actor = self.select_object(board, pos)\n                        if clicked_actor and board[clicked_actor.x][clicked_actor.y][P_NAME_IDX] == self.USER_PLAYER and board[clicked_actor.x][clicked_actor.y][A_TYPE_IDX] != d_a_type[\'Gold\']:\n                            clicked_actor_index_arr = [clicked_actor.x, clicked_actor.y]\n\n                            # draw selected bounding box\n                            game_display, clock = init_visuals(n, n, CONFIG.visibility)\n                            update_graphics(board, game_display, clock, FPS)\n\n                            actor_size = int(canvas_scale / 3)\n                            actor_location = (int(clicked_actor.x * canvas_scale + canvas_scale / 2 + canvas_scale - actor_size), int(clicked_actor.y * canvas_scale + canvas_scale / 2 + canvas_scale - actor_size))\n                            rect = Rect(actor_location, (2 * actor_size, 2 * actor_size))\n\n                            blue = (0, 0, 255)\n                            pygame.draw.rect(game_display, blue, rect, int(canvas_scale / 20))\n\n                            # display valid actions on canvas\n                            b = Board(n)\n                            b.pieces = np.copy(board)\n\n                            if self.USER_PLAYER == 1:\n                                config = CONFIG.player1_config\n                            else:\n                                config = CONFIG.player2_config\n                            valids_square = b.get_moves_for_square(clicked_actor.x, clicked_actor.y, config=config)\n\n                            printed_actions = 0\n                            for i in range(len(valids_square)):\n                                if valids_square[i]:\n                                    text_scale = int(actor_size * 0.5)\n                                    message_display(game_display, u"""" + ACTS_REV[i] + "" s: \'"" + d_user_shortcuts_rev[i] + ""\'"", (3 * canvas_scale + int(printed_actions % 3) * canvas_scale * 2, (n + 1) * canvas_scale + text_scale / 2 + int(printed_actions / 3) * text_scale + int(text_scale / 4)),\n                                                    text_scale)\n                                    printed_actions += 1\n                            # update display\n                            pygame.display.update()\n\n                        else:\n                            print(""You can select only your actors!"")\n                    if event.button == rmb:\n                        if clicked_actor:\n\n                            l_x = clicked_actor.x\n                            l_y = clicked_actor.y\n                            l_type = board[l_x][l_y][A_TYPE_IDX]\n\n                            right_clicked_actor = self.select_object(board, pos)\n\n                            # right clicked actor exists and (if player 1 or player -1) and not clicked self\n                            if right_clicked_actor and board[right_clicked_actor.x][right_clicked_actor.y][P_NAME_IDX] != 0 and right_clicked_actor != clicked_actor:\n                                r_x = right_clicked_actor.x\n                                r_y = right_clicked_actor.y\n                                r_type = board[r_x][r_y][A_TYPE_IDX]\n                                r_player = board[r_x][r_y][P_NAME_IDX]\n\n                                # this is actor of type MyActor\n                                if l_type == d_a_type[\'Work\']:\n                                    if r_player == self.USER_PLAYER:\n                                        if r_type == d_a_type[\'Gold\']:\n                                            clicked_actor_index_arr.append(ACTS[""mine_resources""])\n                                        if r_type == d_a_type[\'Hall\']:\n                                            clicked_actor_index_arr.append(ACTS[""return_resources""])\n\n                                if l_type == d_a_type[\'Rifl\']:\n                                    if r_player != self.USER_PLAYER and r_type != d_a_type[\'Gold\']:\n                                        clicked_actor_index_arr.append(ACTS[""attack""])\n                            else:\n\n                                actor_size = int(canvas_scale / 3)\n\n                                clicked_x, clicked_y = pos\n\n                                clicked_actors_world_x = int(l_x * canvas_scale + canvas_scale / 2 + canvas_scale - actor_size)\n                                clicked_actors_world_y = int(l_y * canvas_scale + canvas_scale / 2 + canvas_scale - actor_size)\n                                if abs(clicked_y - clicked_actors_world_y) > abs(clicked_x - clicked_actors_world_x):\n                                    # we moved mouse more in y direction than x, so its vertical movement\n                                    if clicked_y < clicked_actors_world_y:\n                                        print(""clicked up..."")\n                                        clicked_actor_index_arr.append(ACTS[""up""])\n                                    if clicked_y > clicked_actors_world_y:\n                                        print(""clicked down..."")\n                                        clicked_actor_index_arr.append(ACTS[""down""])\n                                else:\n                                    # we moved mouse more in x direction than y, so its horizontal movement\n                                    if clicked_x < clicked_actors_world_x:\n                                        print(""clicked left..."")\n                                        clicked_actor_index_arr.append(ACTS[""left""])\n\n                                    if clicked_x > clicked_actors_world_x:\n                                        print(""clicked right..."")\n                                        clicked_actor_index_arr.append(ACTS[""right""])\n                            if len(clicked_actor_index_arr) == 3:\n                                return clicked_actor_index_arr\n                            else:\n                                print(""invalid"")\n                                self.display_valid_moves(board, None)\n                        else:\n                            print(""First left click on actor to select it"")\n\n\nclass GreedyRTSPlayer:\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        valids = self.game.getValidMoves(board, 1)\n\n        print(""sum valids"", sum(valids))\n        candidates = []\n        for a in range(self.game.getActionSize()):\n            if valids[a] == 0:\n                continue\n            next_board, _ = self.game.getNextState(board, 1, a)\n            score = self.game.getScore(next_board, 1)\n            candidates += [(-score, a)]\n        candidates.sort()\n\n        n = board.shape[0]\n        y, x, action_index = np.unravel_index(candidates[0][1], [n, n, NUM_ACTS])\n\n        print(""returned act"", x, y, ACTS_REV[action_index])\n\n        return candidates[0][1]\n'"
rts/learn.py,0,"b'import sys\n\nfrom rts.src.config_class import CONFIG\n\nsys.path.append(\'..\')\nfrom Coach import Coach\n# from rts.configurations.ConfigWrapper import LearnArgs\nfrom rts.RTSGame import RTSGame as Game\nfrom rts.keras.NNet import NNetWrapper as nn\n\n# from rts.src.config import grid_size\n\n""""""\nrts/learn.py\n\nTeaches neural network playing of specified game configuration using self play\nThis configuration needs to be kept seperate, as different nnet and game configs are set\n""""""\n\nif __name__ == ""__main__"":\n\n    CONFIG.set_runner(\'learn\')  # set visibility as learn\n\n    # create nnet for this game\n    g = Game()\n    nnet = nn(g, CONFIG.nnet_args.encoder)\n\n    # If training examples should be loaded from file\n    if CONFIG.learn_args.load_model:\n        nnet.load_checkpoint(CONFIG.learn_args.load_folder_file[0], CONFIG.learn_args.load_folder_file[1])\n\n    # Create coach instance that starts teaching nnet on newly created game using self-play\n    c = Coach(g, nnet, CONFIG.learn_args)\n    if CONFIG.learn_args.load_model:\n        print(""Load trainExamples from file"")\n        c.loadTrainExamples()\n    c.learn()\n'"
rts/pit.py,0,"b'import sys\n\nfrom rts.src.config_class import CONFIG\n\nsys.path.append(\'..\')\nimport Arena\nfrom rts.RTSGame import display, RTSGame\n\n""""""\nrts/pit.py\n\nCompares 2 players against each other and outputs num wins p1/ num wins p2/ draws\n""""""\nCONFIG.set_runner(\'pit\')  # set visibility as pit\ng = RTSGame()\nplayer1, player2 = CONFIG.pit_args.create_players(g)\narena = Arena.Arena(player1, player2, g, display=display)\nprint(arena.playGames(CONFIG.pit_args.num_games, verbose=CONFIG.visibility))\n'"
tafl/Digits.py,0,"b'#https://stackoverflow.com/questions/2267362/how-to-convert-an-integer-in-any-base-to-a-string\n\nimport string\ndigs = string.digits + string.ascii_letters\n\n\ndef int2base(x, base, length):\n    if x < 0:\n        sign = -1\n    elif x == 0:\n        return digs[0]\n    else:\n        sign = 1\n\n    x *= sign\n    digits = []\n\n    while x:\n        digits.append(digs[int(x % base)])\n        x = int(x / base)\n\n    if sign < 0:\n        digits.append(\'-\')\n\n    while len(digits)<length: digits.extend([""0""])\n    \n    return list(map(lambda x: int(x),digits))\n    \n\ndef test():\n    size=7\n    validmoves = [[3, 0, 1, 0], [3, 0, 2, 0], [3, 0, 4, 0], [3, 0, 5, 0], [3, 1, 0, 1], [3, 1, 1, 1], [3, 1, 2, 1], [3, 1, 4, 1], [3, 1, 5, 1], [3, 1, 6, 1], [0, 3, 0, 1], [0, 3, 0, 2], [0, 3, 0, 4], [0, 3, 0, 5], [1, 3, 1, 0], [1, 3, 1, 1], [1, 3, 1, 2], [1, 3, 1, 4], [1, 3, 1, 5], [1, 3, 1, 6], [3, 6, 1, 6], [3, 6, 2, 6], [3, 6, 4, 6], [3, 6, 5, 6], [3, 5, 0, 5], [3, 5, 1, 5], [3, 5, 2, 5], [3, 5, 4, 5], [3, 5, 5, 5], [3, 5, 6, 5], [6, 3, 6, 1], [6, 3, 6, 2], [6, 3, 6, 4], [6, 3, 6, 5], [5, 3, 5, 0], [5, 3, 5, 1], [5, 3, 5, 2], [5, 3, 5, 4], [5, 3, 5, 5], [5, 3, 5, 6]]\n    print(validmoves)\n    for m in validmoves:\n        i = m[0]+m[1]*size+m[2]*size**2+m[3]*size**3\n        print(i,"":"",int2base(i,size,4))\n\n#test()\n\n'"
tafl/GameVariants.py,0,"b'#https://en.wikipedia.org/wiki/Tafl_games\n\nclass Tafl:\n    size=0\n    board=[]\n    pieces=[]\n    def expandeighth(self,size,eighth):\n        hs=size//2\n        aquarter=eighth.copy()\n        for b in eighth:\n            if b[0]!=b[1]: aquarter.extend([[b[1],b[0],b[2]]])\n        whole=aquarter.copy()\n        for b in aquarter:\n            if (b[0]!=hs): whole.extend([[size-b[0]-1,b[1],b[2]]])\n            if (b[1]!=hs): whole.extend([[b[0],size-b[1]-1,b[2]]])\n            if (b[0]!=hs and b[1]!=hs): whole.extend([[size-b[0]-1,size-b[1]-1,b[2]]])\n        return whole    \n\n\nclass Brandubh(Tafl):\n   def __init__(self): \n     self.size=7   \n     self.board=self.expandeighth(self.size,[[0,0,1],[3,3,2]])\n     self.pieces=self.expandeighth(self.size,[[3,0,-1],[3,1,-1],[3,2,1],[3,3,2]])\n\nclass ArdRi(Tafl):\n   def __init__(self): \n     self.size=7   \n     self.board=self.expandeighth(self.size,[[0,0,1],[3,3,2]])\n     self.pieces=self.expandeighth(self.size,[[2,0,-1],[3,0,-1],[3,1,-1],[3,2,1],[2,2,1],[3,3,2]])\n\nclass Tablut(Tafl):\n   def __init__(self): \n     self.size=9\n     self.board=self.expandeighth(self.size,[[0,0,1],[4,4,2]])\n     self.pieces=self.expandeighth(self.size,[[3,0,-1],[4,0,-1],[4,1,-1],[4,2,1],[4,3,1],[4,4,2]])\n\nclass Tawlbwrdd(Tafl):\n   def __init__(self): \n     self.size=11   \n     self.board=self.expandeighth(self.size,[[0,0,1],[5,5,2]])\n     self.pieces=self.expandeighth(self.size,[[4,0,-1],[5,0,-1],[4,1,-1],[5,2,-1],[5,3,1],[5,4,1],[4,4,1],[5,5,2]])\n\nclass Hnefatafl(Tafl):\n   def __init__(self): \n     self.size=11  \n     self.board=self.expandeighth(self.size,[[0,0,1],[5,5,2]])\n     self.pieces=self.expandeighth(self.size,[[3,0,-1],[4,0,-1],[5,0,-1],[5,1,-1],[5,3,1],[5,4,1],[4,4,1],[5,5,2]])\n\nclass AleaEvangelii(Tafl):\n   def __init__(self): \n     self.size=19  \n     self.board=self.expandeighth(self.size,[[0,0,1],[9,9,2]])\n     self.pieces=self.expandeighth(self.size,[[2,0,-1],[5,0,-1],[5,2,-1],[7,3,-1],[9,3,-1],[6,4,-1],[5,5,-1],[8,4,1],[9,6,1],[8,7,1],[9,8,1],[9,9,2]])\n\n\n\n'"
tafl/TaflGame.py,0,"b'from __future__ import print_function\nimport sys\nsys.path.append(\'..\')\nfrom Game import Game\nfrom .TaflLogic import Board\nimport numpy as np\nfrom .GameVariants import *\nfrom .Digits import int2base\n\nclass TaflGame(Game):\n\n    def __init__(self, name):\n        self.name = name\n        self.getInitBoard()\n\n    def getInitBoard(self):    \n        board=Board(Brandubh())\n        if self.name==""Brandubh"": board=Board(Brandubh())\n        if self.name==""ArdRi"": board=Board(ArdRi())\n        if self.name==""Tablut"": board=Board(Tablut())\n        if self.name==""Tawlbwrdd"": board=Board(Tawlbwrdd())\n        if self.name==""Hnefatafl"": board=Board(Hnefatafl())\n        if self.name==""AleaEvangelii"": board=Board(AleaEvangelii())\n        self.n=board.size         \n        return board\n        \n\n    def getBoardSize(self):\n        # (a,b) tuple\n        return (self.n, self.n)\n\n    def getActionSize(self):\n        # return number of actions\n        return self.n**4 \n\n    def getNextState(self, board, player, action):\n        # if player takes action on board, return next (board,player)\n        # action must be a valid move\n        b = board.getCopy()\n        move = int2base(action,self.n,4)\n        b.execute_move(move, player)\n        return (b, -player)\n\n    def getValidMoves(self, board, player):\n        # return a fixed size binary vector\n        #Note: Ignoreing the passed in player variable since we are not inverting colors for getCanonicalForm and Arena calls with constant 1.\n        valids = [0]*self.getActionSize()\n        b = board.getCopy()\n        legalMoves =  b.get_legal_moves(board.getPlayerToMove())\n        if len(legalMoves)==0:\n            valids[-1]=1\n            return np.array(valids)\n        for x1, y1, x2, y2 in legalMoves:\n            valids[x1+y1*self.n+x2*self.n**2+y2*self.n**3]=1\n        return np.array(valids)\n\n    def getGameEnded(self, board, player):\n        # return 0 if not ended, if player 1 won, -1 if player 1 lost\n        return board.done*player\n\n    def getCanonicalForm(self, board, player):\n        b = board.getCopy()\n        # rules and objectives are different for the different players, so inverting board results in an invalid state.\n        return b\n\n    def getSymmetries(self, board, pi):\n        return [(board,pi)]\n        # mirror, rotational\n        #assert(len(pi) == self.n**4)  \n        #pi_board = np.reshape(pi[:-1], (self.n, self.n))\n        #l = []\n\n        #for i in range(1, 5):\n        #    for j in [True, False]:\n        #        newB = np.rot90(board, i)\n        #        newPi = np.rot90(pi_board, i)\n        #        if j:\n        #            newB = np.fliplr(newB)\n        #            newPi = np.fliplr(newPi)\n        #        l += [(newB, list(newPi.ravel()) + [pi[-1]])]\n        #return l\n\n    def stringRepresentation(self, board):\n        #print(""->"",str(board))\n        return str(board)\n\n    def getScore(self, board, player):\n        if board.done: return 1000*board.done*player\n        return board.countDiff(player)\n\n\n\ndef display(board):\n       render_chars = {\n             ""-1"": ""b"",\n              ""0"": "" "",\n              ""1"": ""W"",\n              ""2"": ""K"",\n             ""10"": ""#"",\n             ""12"": ""E"",\n             ""20"": ""_"",\n             ""22"": ""x"",\n       }\n       print(""---------------------"")\n       image=board.getImage()\n\n       print(""  "", "" "".join(str(i) for i in range(len(image))))\n       for i in range(len(image)-1,-1,-1):\n           print(""{:2}"".format(i), end="" "")\n\n           row=image[i]\n           for col in row:\n               c = render_chars[str(col)]\n               sys.stdout.write(c)\n           print("" "") \n       #if (board.done!=0): print(""***** Done: "",board.done)  \n       print(""---------------------"")\n\n\n'"
tafl/TaflLogic.py,0,"b'import numpy as np\nfrom .GameVariants import Tafl\n\nclass Board():\n\n\n    def __init__(self, gv):\n      self.size=gv.size  \n      self.width=gv.size\n      self.height=gv.size\n      self.board=gv.board #[x,y,type]\n      self.pieces=gv.pieces #[x,y,type]\n      self.time=0\n      self.done=0\n\n    def __str__(self):\n        return str(self.getPlayerToMove()) + \'\'.join(str(r) for v in self.getImage() for r in v) \n\n    # add [][] indexer syntax to the Board\n    def __getitem__(self, index): \n        return np.array(self.getImage())[index]\n\n    def astype(self,t):\n        return np.array(self.getImage()).astype(t)\n\n    def getCopy(self):\n      gv=Tafl()\n      gv.size=self.size\n      gv.board=np.copy(np.array(self.board)).tolist()\n      gv.pieces=np.copy(np.array(self.pieces)).tolist()\n      b = Board(gv)\n      b.time=self.time\n      b.done=self.done\n      return b\n\n\n    def countDiff(self, color):\n        """"""Counts the # pieces of the given color\n        (1 for white, -1 for black, 0 for empty spaces)""""""\n        count = 0\n        for p in self.pieces:\n            if p[0] >= 0:\n               if p[2]*color > 0:\n                   count += 1\n               else:\n                   count -= 1\n        return count\n\n    def get_legal_moves(self, color):\n        """"""Returns all the legal moves for the given color.\n        (1 for white, -1 for black\n        """"""\n        return self._getValidMoves(color)\n     \n    def has_legal_moves(self, color):\n        vm = self._getValidMoves(color)\n        if len(vm)>0: return True\n        return False\n\n\n    def execute_move(self, move, color):\n        """"""Perform the given move on the board.\n        color gives the color pf the piece to play (1=white,-1=black)\n        """"""\n        x1,y1,x2,y2 = move\n        pieceno = self._getPieceNo(x1,y1)\n        legal = self._isLegalMove(pieceno,x2,y2)\n        if legal>=0:\n           #print(""Accepted move: "",move) \n           self._moveByPieceNo(pieceno,x2,y2)\n        #else:\n           #print(""Illegal move:"",move,legal)\n   \n    def getImage(self):\n        image = [[0 for col in range(self.width)] for row in range(self.height)]\n        for item in self.board:\n            image[item[1]][item[0]] = item[2]*10\n        for piece in self.pieces:\n            if piece[0] >= 0: image[piece[1]][piece[0]] = piece[2] + image[piece[1]][piece[0]]\n        return image\n\n    def getPlayerToMove(self):\n        return -(self.time%2*2-1)\n\n\n################## Internal methods ##################\n\n    def _isLegalMove(self,pieceno,x2,y2):\n      try:\n\n         if x2 < 0 or y2 < 0 or x2 >= self.width or y2 > self.height: return -1\n         \n         piece = self.pieces[pieceno]\n         x1=piece[0]\n         y1=piece[1]\n         if x1<0: return -2 #piece was captured\n         if x1 != x2 and y1 != y2: return -3 #must move in straight line\n         if x1 == x2 and y1 == y2: return -4 #no move\n\n         piecetype = piece[2]\n         if (piecetype == -1 and self.time%2 == 0) or (piecetype != -1 and self.time%2 == 1): return -5 #wrong player\n\n         for item in self.board:\n            if item[0] == x2 and item[1] == y2 and item[2] > 0:\n                if piecetype != 2: return -10 #forbidden space\n         for apiece in self.pieces:\n            if y1==y2 and y1 == apiece[1] and ((x1 < apiece[0] and x2 >= apiece[0]) or (x1 > apiece[0] and x2 <= apiece[0])): return -20 #interposing piece\n            if x1==x2 and x1 == apiece[0] and ((y1 < apiece[1] and y2 >= apiece[1]) or (y1 > apiece[1] and y2 <= apiece[1])): return -20 #interposing piece\n\n         return 0 # legal move\n      except Exception as ex:\n         print(""error in islegalmove "",ex,pieceno,x2,y2)\n         raise\n\n   \n    def _getCaptures(self,pieceno,x2,y2):\n       #Assumes was already checked for legal move\n       captures=[]\n       piece=self.pieces[pieceno]\n       piecetype = piece[2]\n       for apiece in self.pieces:\n          if piecetype*apiece[2] < 0:\n             d1 = apiece[0]-x2 \n             d2 = apiece[1]-y2\n             if (abs(d1)==1 and d2==0) or (abs(d2)==1 and d1==0): \n                 for bpiece in self.pieces:\n                    if piecetype*bpiece[2] > 0 and not(piece[0]==bpiece[0] and piece[1]==bpiece[1]):\n                       e1 = bpiece[0]-apiece[0]\n                       e2 = bpiece[1]-apiece[1]\n                       if d1==e1 and d2==e2:\n                          captures.extend([apiece])\n       return captures\n\n    # returns code for invalid mode (<0) or number of pieces captured\n    def _moveByPieceNo(self,pieceno,x2,y2):\n      \n      legal = self._isLegalMove(pieceno,x2,y2)\n      if legal != 0: return legal\n\n      self.time = self.time + 1\n\n      piece=self.pieces[pieceno]\n      piece[0]=x2\n      piece[1]=y2\n      caps = self._getCaptures(pieceno,x2,y2)\n      #print(""Captures = "",caps)\n      for c in caps:\n          c[0]=-99\n\n      self.done = self._getWinLose()\n      \n      return len(caps)\n        \n\n\n    def _getWinLose(self):\n       if self.time > 50: return -1\n       for apiece in self.pieces:\n           if apiece[2]==2 and apiece[0] > -1:\n               for item in self.board:\n                 if item[0]==apiece[0] and item[1]==apiece[1] and item[2]==1:\n                     return 1 #white won\n               return 0 # no winner\n       return -1  #white lost\n   \n    def _getPieceNo(self,x,y):\n       for pieceno in range(len(self.pieces)):\n           piece=self.pieces[pieceno]\n           if piece[0]==x and piece[1]==y: return pieceno\n       return -1    \n   \n    def _getValidMoves(self,player):\n       moves=[]\n       for pieceno in range(len(self.pieces)):\n           piece=self.pieces[pieceno]\n           if piece[2]*player > 0:\n              #print(""checking pieceno "",pieceno,piece)\n              for x in range(0,self.width):\n                  if self._isLegalMove(pieceno,x,piece[1])>=0:moves.extend([[piece[0],piece[1],x,piece[1]]])\n              for y in range(0,self.height):\n                  if self._isLegalMove(pieceno,piece[0],y)>=0:moves.extend([[piece[0],piece[1],piece[0],y]])\n       #print(""moves "",moves)\n       return moves\n\n\n'"
tafl/TaflPlayers.py,0,"b""import numpy as np\nfrom .Digits import int2base\n\nclass RandomTaflPlayer():\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        a = np.random.randint(self.game.getActionSize())\n        valids = self.game.getValidMoves(board, board.getPlayerToMove())\n        while valids[a]!=1:\n            a = np.random.randint(self.game.getActionSize())\n        return a\n\n\nclass HumanTaflPlayer():\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        # display(board)\n        valid = self.game.getValidMoves(board, board.getPlayerToMove())\n        m=[]\n        for i in range(len(valid)):\n            if valid[i]:\n                m.extend([int2base(i,self.game.n,4)])\n        print(m)    \n        while True:\n            a = input()\n\n            x1,y1,x2,y2 = [int(x) for x in a.strip().split(' ')]\n            a = x1 + y1 * self.game.n + x2 * self.game.n**2 + y2 * self.game.n**3 \n            if valid[a]:\n                break\n            else:\n                print('Invalid')\n\n        return a\n\n\nclass GreedyTaflPlayer():\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        valids = self.game.getValidMoves(board, board.getPlayerToMove())\n        candidates = []\n        for a in range(self.game.getActionSize()):\n            if valids[a]==0:\n                continue\n            nextBoard, _ = self.game.getNextState(board, board.getPlayerToMove(), a)\n            score = self.game.getScore(nextBoard, board.getPlayerToMove())\n            candidates += [(-score, a)]\n        candidates.sort()\n        return candidates[0][1]\n\n\n    \n"""
tafl/__init__.py,0,b''
tafl/mainTafl.py,1,"b'from Coach import Coach\nfrom tafl.TaflGame import TaflGame as Game\nfrom tafl.pytorch.NNet import NNetWrapper as nn\nfrom utils import *\n\nargs = dotdict({\n    \'numIters\': 1000,\n    \'numEps\': 100,\n    \'tempThreshold\': 15,\n    \'updateThreshold\': 0.6,\n    \'maxlenOfQueue\': 200000,\n    \'numMCTSSims\': 25,\n    \'arenaCompare\': 40,\n    \'cpuct\': 1,\n\n    \'checkpoint\': \'./temp/\',\n    \'load_model\': False,\n    \'load_folder_file\': (\'~/dev/models/8x100x50\',\'best.pth.tar\'),\n    \'numItersForTrainExamplesHistory\': 20,\n\n})\n\nif __name__==""__main__"":\n    g = Game(6)\n    nnet = nn(g)\n\n    if args.load_model:\n        nnet.load_checkpoint(args.load_folder_file[0], args.load_folder_file[1])\n\n    c = Coach(g, nnet, args)\n    if args.load_model:\n        print(""Load trainExamples from file"")\n        c.loadTrainExamples()\n    c.learn()\n'"
tafl/pitTafl.py,0,"b'\n# Note: Run this file from Arena directory (the one above /tafl)\n\nimport Arena\nfrom MCTS import MCTS\nfrom tafl.TaflGame import TaflGame, display\nfrom tafl.TaflPlayers import *\n#from tafl.keras.NNet import NNetWrapper as NNet\n\nimport numpy as np\nfrom utils import *\n\n""""""\nuse this script to play any two agents against each other, or play manually with\nany agent.\n""""""\n\ng = TaflGame(""Brandubh"")\n\n# all players\nrp = RandomTaflPlayer(g).play\ngp = GreedyTaflPlayer(g).play\nhp = HumanTaflPlayer(g).play\n\n# nnet players\n#n1 = NNet(g)\n#n1.load_checkpoint(\'./pretrained_models/tafl/keras/\',\'6x100x25_best.pth.tar\')\n#args1 = dotdict({\'numMCTSSims\': 50, \'cpuct\':1.0})\n#mcts1 = MCTS(g, n1, args1)\n#n1p = lambda x: np.argmax(mcts1.getActionProb(x, temp=0))\n\n\narena = Arena.Arena(hp, gp, g, display=display)\n#arena = Arena.Arena(gp, rp, g, display=display)\nprint(arena.playGames(2, verbose=True))\n'"
tictactoe/TicTacToeGame.py,0,"b'from __future__ import print_function\nimport sys\nsys.path.append(\'..\')\nfrom Game import Game\nfrom .TicTacToeLogic import Board\nimport numpy as np\n\n""""""\nGame class implementation for the game of TicTacToe.\nBased on the OthelloGame then getGameEnded() was adapted to new rules.\n\nAuthor: Evgeny Tyurin, github.com/evg-tyurin\nDate: Jan 5, 2018.\n\nBased on the OthelloGame by Surag Nair.\n""""""\nclass TicTacToeGame(Game):\n    def __init__(self, n=3):\n        self.n = n\n\n    def getInitBoard(self):\n        # return initial board (numpy board)\n        b = Board(self.n)\n        return np.array(b.pieces)\n\n    def getBoardSize(self):\n        # (a,b) tuple\n        return (self.n, self.n)\n\n    def getActionSize(self):\n        # return number of actions\n        return self.n*self.n + 1\n\n    def getNextState(self, board, player, action):\n        # if player takes action on board, return next (board,player)\n        # action must be a valid move\n        if action == self.n*self.n:\n            return (board, -player)\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n        move = (int(action/self.n), action%self.n)\n        b.execute_move(move, player)\n        return (b.pieces, -player)\n\n    def getValidMoves(self, board, player):\n        # return a fixed size binary vector\n        valids = [0]*self.getActionSize()\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n        legalMoves =  b.get_legal_moves(player)\n        if len(legalMoves)==0:\n            valids[-1]=1\n            return np.array(valids)\n        for x, y in legalMoves:\n            valids[self.n*x+y]=1\n        return np.array(valids)\n\n    def getGameEnded(self, board, player):\n        # return 0 if not ended, 1 if player 1 won, -1 if player 1 lost\n        # player = 1\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n\n        if b.is_win(player):\n            return 1\n        if b.is_win(-player):\n            return -1\n        if b.has_legal_moves():\n            return 0\n        # draw has a very little value \n        return 1e-4\n\n    def getCanonicalForm(self, board, player):\n        # return state if player==1, else return -state if player==-1\n        return player*board\n\n    def getSymmetries(self, board, pi):\n        # mirror, rotational\n        assert(len(pi) == self.n**2+1)  # 1 for pass\n        pi_board = np.reshape(pi[:-1], (self.n, self.n))\n        l = []\n\n        for i in range(1, 5):\n            for j in [True, False]:\n                newB = np.rot90(board, i)\n                newPi = np.rot90(pi_board, i)\n                if j:\n                    newB = np.fliplr(newB)\n                    newPi = np.fliplr(newPi)\n                l += [(newB, list(newPi.ravel()) + [pi[-1]])]\n        return l\n\n    def stringRepresentation(self, board):\n        # 8x8 numpy array (canonical board)\n        return board.tostring()\n\n    @staticmethod\n    def display(board):\n        n = board.shape[0]\n\n        print(""   "", end="""")\n        for y in range(n):\n            print (y,"""", end="""")\n        print("""")\n        print(""  "", end="""")\n        for _ in range(n):\n            print (""-"", end=""-"")\n        print(""--"")\n        for y in range(n):\n            print(y, ""|"",end="""")    # print the row #\n            for x in range(n):\n                piece = board[y][x]    # get the piece to print\n                if piece == -1: print(""X "",end="""")\n                elif piece == 1: print(""O "",end="""")\n                else:\n                    if x==n:\n                        print(""-"",end="""")\n                    else:\n                        print(""- "",end="""")\n            print(""|"")\n\n        print(""  "", end="""")\n        for _ in range(n):\n            print (""-"", end=""-"")\n        print(""--"")\n'"
tictactoe/TicTacToeLogic.py,0,"b'\'\'\'\nBoard class for the game of TicTacToe.\nDefault board size is 3x3.\nBoard data:\n  1=white(O), -1=black(X), 0=empty\n  first dim is column , 2nd is row:\n     pieces[0][0] is the top left square,\n     pieces[2][0] is the bottom left square,\nSquares are stored and manipulated as (x,y) tuples.\n\nAuthor: Evgeny Tyurin, github.com/evg-tyurin\nDate: Jan 5, 2018.\n\nBased on the board for the game of Othello by Eric P. Nichols.\n\n\'\'\'\n# from bkcharts.attributes import color\nclass Board():\n\n    # list of all 8 directions on the board, as (x,y) offsets\n    __directions = [(1,1),(1,0),(1,-1),(0,-1),(-1,-1),(-1,0),(-1,1),(0,1)]\n\n    def __init__(self, n=3):\n        ""Set up initial board configuration.""\n\n        self.n = n\n        # Create the empty board array.\n        self.pieces = [None]*self.n\n        for i in range(self.n):\n            self.pieces[i] = [0]*self.n\n\n    # add [][] indexer syntax to the Board\n    def __getitem__(self, index): \n        return self.pieces[index]\n\n    def get_legal_moves(self, color):\n        """"""Returns all the legal moves for the given color.\n        (1 for white, -1 for black)\n        @param color not used and came from previous version.        \n        """"""\n        moves = set()  # stores the legal moves.\n\n        # Get all the empty squares (color==0)\n        for y in range(self.n):\n            for x in range(self.n):\n                if self[x][y]==0:\n                    newmove = (x,y)\n                    moves.add(newmove)\n        return list(moves)\n\n    def has_legal_moves(self):\n        for y in range(self.n):\n            for x in range(self.n):\n                if self[x][y]==0:\n                    return True\n        return False\n    \n    def is_win(self, color):\n        """"""Check whether the given player has collected a triplet in any direction; \n        @param color (1=white,-1=black)\n        """"""\n        win = self.n\n        # check y-strips\n        for y in range(self.n):\n            count = 0\n            for x in range(self.n):\n                if self[x][y]==color:\n                    count += 1\n            if count==win:\n                return True\n        # check x-strips\n        for x in range(self.n):\n            count = 0\n            for y in range(self.n):\n                if self[x][y]==color:\n                    count += 1\n            if count==win:\n                return True\n        # check two diagonal strips\n        count = 0\n        for d in range(self.n):\n            if self[d][d]==color:\n                count += 1\n        if count==win:\n            return True\n        count = 0\n        for d in range(self.n):\n            if self[d][self.n-d-1]==color:\n                count += 1\n        if count==win:\n            return True\n        \n        return False\n\n    def execute_move(self, move, color):\n        """"""Perform the given move on the board; \n        color gives the color pf the piece to play (1=white,-1=black)\n        """"""\n\n        (x,y) = move\n\n        # Add the piece to the empty square.\n        assert self[x][y] == 0\n        self[x][y] = color\n\n'"
tictactoe/TicTacToePlayers.py,0,"b'import numpy as np\n\n""""""\nRandom and Human-ineracting players for the game of TicTacToe.\n\nAuthor: Evgeny Tyurin, github.com/evg-tyurin\nDate: Jan 5, 2018.\n\nBased on the OthelloPlayers by Surag Nair.\n\n""""""\nclass RandomPlayer():\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        a = np.random.randint(self.game.getActionSize())\n        valids = self.game.getValidMoves(board, 1)\n        while valids[a]!=1:\n            a = np.random.randint(self.game.getActionSize())\n        return a\n\n\nclass HumanTicTacToePlayer():\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        # display(board)\n        valid = self.game.getValidMoves(board, 1)\n        for i in range(len(valid)):\n            if valid[i]:\n                print(int(i/self.game.n), int(i%self.game.n))\n        while True: \n            # Python 3.x\n            a = input()\n            # Python 2.x \n            # a = raw_input()\n\n            x,y = [int(x) for x in a.split(\' \')]\n            a = self.game.n * x + y if x!= -1 else self.game.n ** 2\n            if valid[a]:\n                break\n            else:\n                print(\'Invalid\')\n\n        return a\n'"
tictactoe/__init__.py,0,b''
tictactoe_3d/TicTacToeGame.py,0,"b'from __future__ import print_function\nimport sys\nsys.path.append(\'..\')\nfrom Game import Game\nfrom .TicTacToeLogic import Board\nimport numpy as np\n\n""""""\nGame class implementation for the game of 3D TicTacToe or Qubic.\n\nAuthor: Adam Lawson, github.com/goshawk22\nDate: Feb 05, 2020\n\nBased on the TicTacToeGame by Evgeny Tyurin.\n""""""\nclass TicTacToeGame(Game):\n    def __init__(self, n):\n        self.n = n\n\n    def getInitBoard(self):\n        # return initial board (numpy board)\n        b = Board(self.n)\n        return np.array(b.pieces)\n\n    def getBoardSize(self):\n        # (a,b) tuple\n        return (self.n, self.n, self.n)\n\n    def getActionSize(self):\n        # return number of actions\n        return self.n*self.n*self.n + 1\n\n    def getNextState(self, board, player, action):\n        # if player takes action on board, return next (board,player)\n        # action must be a valid move\n        if action == self.n*self.n*self.n:\n            return (board, -player)\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n        boardvalues = np.arange(0,(self.n*self.n*self.n)).reshape(self.n,self.n,self.n)\n        \n        move = np.argwhere(boardvalues==action)[0]\n        b.execute_move(move, player)\n        return (b.pieces, -player)\n\n    def getValidMoves(self, board, player):\n        # return a fixed size binary vector\n        valids = [0]*self.getActionSize()\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n        legalMoves =  b.get_legal_moves(player)\n        if len(legalMoves)==0:\n            valids[-1]=1\n            return np.array(valids)\n        for z, x, y in legalMoves:\n            boardvalues = np.arange(0,(self.n*self.n*self.n)).reshape(self.n,self.n,self.n)\n            valids[boardvalues[z][x][y]] = 1\n        return np.array(valids)\n\n    def getGameEnded(self, board, player):\n        # return 0 if not ended, 1 if player 1 won, -1 if player 1 lost\n        # player = 1\n        b = Board(self.n)\n        b.pieces = np.copy(board)\n\n        if b.is_win(player):\n            return 1\n        if b.is_win(-player):\n            return -1\n        if b.has_legal_moves():\n            return 0\n        # draw has a very little value \n        return 1e-4\n\n    def getCanonicalForm(self, board, player):\n        # return state if player==1, else return -state if player==-1\n        return player*board\n\n    def getSymmetries(self, board, pi):\n        # mirror, rotational\n        pi_board = np.reshape(pi[:-1], (self.n, self.n, self.n))\n        l = []\n        newB = np.reshape(board, (self.n*self.n, self.n))\n        newPi = pi_board\n        for i in range(1,5):\n\n            for z in [True, False]:\n                for j in [True, False]:\n                    if j:\n                        newB = np.fliplr(newB)\n                        newPi = np.fliplr(newPi)\n                    if z:\n                        newB = np.flipud(newB)\n                        newPi = np.flipud(newPi)\n                    \n                    newB = np.reshape(newB, (self.n,self.n,self.n))\n                    newPi = np.reshape(newPi, (self.n,self.n,self.n))\n                    l += [(newB, list(newPi.ravel()) + [pi[-1]])]\n        return l\n\n    def stringRepresentation(self, board):\n        # 8x8 numpy array (canonical board)\n        return board.tostring()\n\n    @staticmethod\n    def display(board):\n        n = board.shape[0]\n        for z in range(n):\n            print(""   "", end="""")\n            for y in range(n):\n                print (y,"""", end="""")\n            print("""")\n            print(""  "", end="""")\n            for _ in range(n):\n                print (""-"", end=""-"")\n            print(""--"")\n            for y in range(n):\n                print(y, ""|"",end="""")    # print the row #\n                for x in range(n):\n                    piece = board[z][y][x]    # get the piece to print\n                    if piece == -1: print(""X "",end="""")\n                    elif piece == 1: print(""O "",end="""")\n                    else:\n                        if x==n:\n                            print(""-"",end="""")\n                        else:\n                            print(""- "",end="""")\n                print(""|"")\n\n            print(""  "", end="""")\n            for _ in range(n):\n                print (""-"", end=""-"")\n            print(""--"")\n'"
tictactoe_3d/TicTacToeLogic.py,0,"b'import numpy as np\n\'\'\'\nBoard class for the game of TicTacToe.\nDefault board size is 3x3.\nBoard data:\n  1=white(O), -1=black(X), 0=empty\n  first dim is column , 2nd is row:\n     pieces[0][0] is the top left square,\n     pieces[2][0] is the bottom left square,\nSquares are stored and manipulated as (x,y) tuples.\n\nAuthor: Evgeny Tyurin, github.com/evg-tyurin\nDate: Jan 5, 2018.\n\nBased on the board for the game of Othello by Eric P. Nichols.\n\n\'\'\'\n# from bkcharts.attributes import color\nclass Board():\n\n    # list of all 8 directions on the board, as (x,y) offsets\n    __directions = [(1,1),(1,0),(1,-1),(0,-1),(-1,-1),(-1,0),(-1,1),(0,1)]\n\n    def __init__(self, n=3):\n        ""Set up initial board configuration.""\n\n        self.n = n\n        # Create the empty board array.\n        self.pieces = np.zeros((n,n,n))\n\n    # add [][] indexer syntax to the Board\n    def __getitem__(self, index): \n        index1 = [None,None,None]\n        for i in range(3):\n            index1[i] = str(index[i])\n        for i in range(len(index1)):\n            x = index1[i]\n            index1[i] = str(int(x) - 1)\n        return self.pieces[list(map(int, index1))]\n\n    def get_legal_moves(self, color):\n        """"""Returns all the legal moves for the given color.\n        (1 for white, -1 for black)\n        @param color not used and came from previous version.        \n        """"""\n        moves = set()  # stores the legal moves.\n\n        # Get all the empty squares (color==0)\n        for z in range(self.n): \n            for y in range(self.n):\n                for x in range(self.n):\n                    if self.pieces[z][x][y]==0:\n                        newmove = (z,x,y)\n                        moves.add(newmove)\n        return list(moves)\n\n    def has_legal_moves(self):\n        for z in range(self.n):\n            for y in range(self.n):\n                for x in range(self.n):\n                    if self.pieces[z][x][y]==0:\n                        return True\n        return False\n    \n    def is_win(self, color):\n        """"""Check whether the given player has collected a triplet in any direction; \n        @param color (1=white,-1=black)\n        """"""\n        win = self.n\n        # check z-dimension\n        count = 0\n        for z in range(self.n):\n            count = 0\n            for y in range(self.n):\n                count = 0\n                for x in range(self.n):\n                    if self.pieces[z,x,y]==color:\n                        count += 1\n                if count==win:\n                    return True\n\n        count = 0\n        for z in range(self.n):\n            count = 0\n            for x in range(self.n):\n                count = 0\n                for y in range(self.n):\n                    if self.pieces[z,x,y]==color:\n                        count += 1\n                if count==win:\n                    return True\n        \n        # check x dimension\n        count = 0\n        for x in range(self.n):\n            count = 0\n            for z in range(self.n):\n                count = 0\n                for y in range(self.n):\n                    if self.pieces[z,x,y]==color:\n                        count += 1\n                if count==win:\n                    return True\n\n        count = 0\n        for x in range(self.n):\n            count = 0\n            for y in range(self.n):\n                count = 0\n                for z in range(self.n):\n                    if self.pieces[z,x,y]==color:\n                        count += 1\n                if count==win:\n                    return True\n\n        # check y dimension\n        count = 0\n        for y in range(self.n):\n            count = 0\n            for x in range(self.n):\n                count = 0\n                for z in range(self.n):\n                    if self.pieces[z,x,y]==color:\n                        count += 1\n                if count==win:\n                    return True\n        \n        count = 0\n        for y in range(self.n):\n            count = 0\n            for z in range(self.n):\n                count = 0\n                for x in range(self.n):\n                    if self.pieces[z,x,y]==color:\n                        count += 1\n                if count==win:\n                    return True\n        \n        # check flat diagonals\n        # check z dimension\n        count = 0\n        for z in range(self.n):\n            count = 0\n            for d in range(self.n):\n                if self.pieces[z,d,d]==color:\n                    count += 1\n            if count==win:\n                return True\n        \n        count = 0\n        for z in range(self.n):\n            count = 0\n            for d in range(self.n):\n                if self.pieces[z,d,self.n-d-1]==color:\n                    count += 1\n            if count==win:\n                return True\n\n        # check x dimension\n        count = 0\n        for x in range(self.n):\n            count = 0\n            for d in range(self.n):\n                if self.pieces[d,x,d]==color:\n                    count += 1\n            if count==win:\n                return True\n\n        count = 0\n        for x in range(self.n):\n            count = 0\n            for d in range(self.n):\n                if self.pieces[d,x,self.n-d-1]==color:\n                    count += 1\n            if count==win:\n                return True\n\n        # check y dimension\n        count = 0\n        for y in range(self.n):\n            count = 0\n            for d in range(self.n):\n                if self.pieces[d,d,y]==color:\n                    count += 1\n            if count==win:\n                return True\n\n        count = 0\n        for y in range(self.n):\n            count = 0\n            for d in range(self.n):\n                if self.pieces[self.n-d-1,d,y]==color:\n                    count += 1\n            if count==win:\n                return True\n        \n        # check 4 true diagonals\n        count = 0\n        if self.pieces[0,0,0] == color:\n            count += 1\n            if self.pieces[1,1,1] == color:\n                count += 1\n                if self.pieces[2,2,2] == color:\n                    count += 1\n                    if count == win:\n                        return True\n            \n        count = 0\n        if self.pieces[2,0,0] == color:\n            count += 1\n            if self.pieces[1,1,1] == color:\n                count += 1\n                if self.pieces[0,2,2] == color:\n                    count += 1\n                    if count == win:\n                        return True\n        \n        count = 0\n        if self.pieces[2,2,0] == color:\n            count += 1\n            if self.pieces[1,1,1] == color:\n                count += 1\n                if self.pieces[0,0,2] == color:\n                    count += 1\n                    if count == win:\n                        return True\n        \n        count = 0\n        if self.pieces[0,2,0] == color:\n            count += 1\n            if self.pieces[1,1,1] == color:\n                count += 1\n                if self.pieces[2,0,2] == color:\n                    count += 1\n                    if count == win:\n                        return True\n\n        # return false if no 3 is reached\n        return False\n\n    def execute_move(self, move, color):\n        """"""Perform the given move on the board; \n        color gives the color pf the piece to play (1=white,-1=black)\n        """"""\n\n        (z,x,y) = move\n\n        # Add the piece to the empty square.\n        assert self.pieces[z][x][y] == 0\n        self.pieces[z][x][y] = color\n\n'"
tictactoe_3d/TicTacToePlayers.py,0,"b'import numpy as np\n\n""""""\nRandom and Human-ineracting players for the game of TicTacToe.\n\nAuthor: Evgeny Tyurin, github.com/evg-tyurin\nDate: Jan 5, 2018.\n\nBased on the OthelloPlayers by Surag Nair.\n\n""""""\nclass RandomPlayer():\n    def __init__(self, game):\n        self.game = game\n\n    def play(self, board):\n        a = np.random.randint(self.game.getActionSize())\n        valids = self.game.getValidMoves(board, 1)\n        while valids[a]!=1:\n            a = np.random.randint(self.game.getActionSize())\n        return a\n\n\nclass HumanTicTacToePlayer():\n    def __init__(self, game, n):\n        self.game = game\n        self.n = n\n\n    def play(self, board):\n        boardvalues = np.arange(self.n*self.n*self.n).reshape(self.n,self.n,self.n)\n        validvalue = np.arange(self.n*self.n*self.n)\n        # display(board)\n        valid = self.game.getValidMoves(board, 1)\n        for i in range(len(valid)):\n            if valid[i] == 1:\n                action = validvalue[i]\n                print(np.argwhere(boardvalues == action))\n\n        while True: \n            # Python 3.x\n            a = input()\n            # Python 2.x \n            # a = raw_input()\n\n            z,x,y = [int(x) for x in a.split(\' \')]\n            boardvalues = np.arange(self.n*self.n*self.n).reshape(self.n,self.n,self.n)\n            a = boardvalues[z][x][y]\n            if valid[a]:\n                break\n            else:\n                print(\'Invalid\')\n\n        return a\n'"
connect4/tensorflow/Connect4NNet.py,0,"b'import sys\nsys.path.append(\'..\')\nfrom utils import *\n\nimport tensorflow as tf\n\n## Code based on OthelloNNet with minimal changes.\n\n\nclass Connect4NNet():\n    def __init__(self, game, args):\n        # game params\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n        self.args = args\n\n        # Renaming functions\n        Relu = tf.nn.relu\n        Tanh = tf.nn.tanh\n        BatchNormalization = tf.layers.batch_normalization\n        Dropout = tf.layers.dropout\n        Dense = tf.layers.dense\n\n        # Neural Net\n        self.graph = tf.Graph()\n        with self.graph.as_default():\n            self.input_boards = tf.placeholder(tf.float32, shape=[None, self.board_x, self.board_y])    # s: batch_size x board_x x board_y\n            self.dropout = tf.placeholder(tf.float32)\n            self.isTraining = tf.placeholder(tf.bool, name=""is_training"")\n\n            x_image = tf.reshape(self.input_boards, [-1, self.board_x, self.board_y, 1])                    # batch_size  x board_x x board_y x 1\n            h_conv1 = Relu(BatchNormalization(self.conv2d(x_image, args.num_channels, \'same\'), axis=3, training=self.isTraining))      # batch_size  x board_x x board_y x num_channels\n            h_conv2 = Relu(BatchNormalization(self.conv2d(h_conv1, args.num_channels, \'same\'), axis=3, training=self.isTraining))      # batch_size  x board_x x board_y x num_channels\n            h_conv3 = Relu(BatchNormalization(self.conv2d(h_conv2, args.num_channels, \'valid\'), axis=3, training=self.isTraining))     # batch_size  x (board_x-2) x (board_y-2) x num_channels\n            h_conv4 = Relu(BatchNormalization(self.conv2d(h_conv3, args.num_channels, \'valid\'), axis=3, training=self.isTraining))     # batch_size  x (board_x-4) x (board_y-4) x num_channels\n            h_conv4_flat = tf.reshape(h_conv4, [-1, args.num_channels * (self.board_x - 4) * (self.board_y - 4)])\n            s_fc1 = Dropout(Relu(BatchNormalization(Dense(h_conv4_flat, 1024), axis=1, training=self.isTraining)), rate=self.dropout)  # batch_size x 1024\n            s_fc2 = Dropout(Relu(BatchNormalization(Dense(s_fc1, 512), axis=1, training=self.isTraining)), rate=self.dropout)          # batch_size x 512\n            self.pi = Dense(s_fc2, self.action_size)                                                        # batch_size x self.action_size\n            self.prob = tf.nn.softmax(self.pi)\n            self.v = Tanh(Dense(s_fc2, 1))                                                               # batch_size x 1\n\n            self.calculate_loss()\n\n    def conv2d(self, x, out_channels, padding):\n        return tf.layers.conv2d(x, out_channels, kernel_size=[3, 3], padding=padding)\n\n    def calculate_loss(self):\n        self.target_pis = tf.placeholder(tf.float32, shape=[None, self.action_size])\n        self.target_vs = tf.placeholder(tf.float32, shape=[None])\n        self.loss_pi = tf.losses.softmax_cross_entropy(self.target_pis, self.pi)\n        self.loss_v = tf.losses.mean_squared_error(self.target_vs, tf.reshape(self.v, shape=[-1, ]))\n        self.total_loss = self.loss_pi + self.loss_v\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            self.train_step = tf.train.AdamOptimizer(self.args.lr).minimize(self.total_loss)\n'"
connect4/tensorflow/NNet.py,0,"b'import os\nimport sys\nimport time\n\nimport numpy as np\nfrom tqdm import tqdm\n\nsys.path.append(\'../../\')\nfrom utils import *\nfrom NeuralNet import NeuralNet\n\nimport tensorflow as tf\nfrom .Connect4NNet import Connect4NNet as onnet\n\nargs = dotdict({\n    \'lr\': 0.001,\n    \'dropout\': 0.3,\n    \'epochs\': 10,\n    \'batch_size\': 64,\n    \'num_channels\': 512,\n})\n\n\n## Code based on othello.NNetWrapper with minimal changes.\n\nclass NNetWrapper(NeuralNet):\n    def __init__(self, game):\n        self.nnet = onnet(game, args)\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n\n        self.sess = tf.Session(graph=self.nnet.graph)\n        self.saver = None\n        with tf.Session() as temp_sess:\n            temp_sess.run(tf.global_variables_initializer())\n        self.sess.run(tf.variables_initializer(self.nnet.graph.get_collection(\'variables\')))\n\n    def train(self, examples):\n        """"""\n        examples: list of examples, each example is of form (board, pi, v)\n        """"""\n\n        for epoch in range(args.epochs):\n            print(\'EPOCH ::: \' + str(epoch + 1))\n            pi_losses = AverageMeter()\n            v_losses = AverageMeter()\n            batch_count = int(len(examples) / args.batch_size)\n\n            # self.sess.run(tf.local_variables_initializer())\n            t = tqdm(range(batch_count), desc=\'Training Net\')\n            for _ in t:\n                sample_ids = np.random.randint(len(examples), size=args.batch_size)\n                boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n\n                # predict and compute gradient and do SGD step\n                input_dict = {self.nnet.input_boards: boards, self.nnet.target_pis: pis, self.nnet.target_vs: vs,\n                              self.nnet.dropout: args.dropout, self.nnet.isTraining: True}\n\n                # record loss\n                self.sess.run(self.nnet.train_step, feed_dict=input_dict)\n                pi_loss, v_loss = self.sess.run([self.nnet.loss_pi, self.nnet.loss_v], feed_dict=input_dict)\n                pi_losses.update(pi_loss, len(boards))\n                v_losses.update(v_loss, len(boards))\n                t.set_postfix(Loss_pi=pi_losses, Loss_v=v_losses)\n\n    def predict(self, board):\n        """"""\n        board: np array with board\n        """"""\n        # timing\n        start = time.time()\n\n        # preparing input\n        board = board[np.newaxis, :, :]\n\n        # run\n        prob, v = self.sess.run([self.nnet.prob, self.nnet.v],\n                                feed_dict={self.nnet.input_boards: board, self.nnet.dropout: 0,\n                                           self.nnet.isTraining: False})\n\n        # print(\'PREDICTION TIME TAKEN : {0:03f}\'.format(time.time()-start))\n        return prob[0], v[0]\n\n    def save_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(folder):\n            print(""Checkpoint Directory does not exist! Making directory {}"".format(folder))\n            os.mkdir(folder)\n        else:\n            print(""Checkpoint Directory exists! "")\n        if self.saver == None:\n            self.saver = tf.train.Saver(self.nnet.graph.get_collection(\'variables\'))\n        with self.nnet.graph.as_default():\n            self.saver.save(self.sess, filepath)\n\n    def load_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(filepath + \'.meta\'):\n            raise (""No model in path {}"".format(filepath))\n        with self.nnet.graph.as_default():\n            self.saver = tf.train.Saver()\n            self.saver.restore(self.sess, filepath)\n'"
connect4/tensorflow/__init__.py,0,b''
gobang/keras/GobangNNet.py,0,"b""import sys\nsys.path.append('..')\nfrom utils import *\n\nimport argparse\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\n\nclass GobangNNet():\n    def __init__(self, game, args):\n        # game params\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n        self.args = args\n\n        # Neural Net\n        self.input_boards = Input(shape=(self.board_x, self.board_y))    # s: batch_size x board_x x board_y\n\n        x_image = Reshape((self.board_x, self.board_y, 1))(self.input_boards)                # batch_size  x board_x x board_y x 1\n        h_conv1 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='same')(x_image)))         # batch_size  x board_x x board_y x num_channels\n        h_conv2 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='same')(h_conv1)))         # batch_size  x board_x x board_y x num_channels\n        h_conv3 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='valid')(h_conv2)))        # batch_size  x (board_x-2) x (board_y-2) x num_channels\n        h_conv4 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='valid')(h_conv3)))        # batch_size  x (board_x-4) x (board_y-4) x num_channels\n        h_conv4_flat = Flatten()(h_conv4)       \n        s_fc1 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(1024)(h_conv4_flat))))  # batch_size x 1024\n        s_fc2 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(512)(s_fc1))))          # batch_size x 1024\n        self.pi = Dense(self.action_size, activation='softmax', name='pi')(s_fc2)   # batch_size x self.action_size\n        self.v = Dense(1, activation='tanh', name='v')(s_fc2)                    # batch_size x 1\n\n        self.model = Model(inputs=self.input_boards, outputs=[self.pi, self.v])\n        self.model.compile(loss=['categorical_crossentropy','mean_squared_error'], optimizer=Adam(args.lr))\n"""
gobang/keras/NNet.py,0,"b'import argparse\nimport os\nimport shutil\nimport time\nimport random\nimport numpy as np\nimport math\nimport sys\nimport tensorflow as tf\nsys.path.append(\'..\')\nfrom utils import *\nfrom NeuralNet import NeuralNet\n\nimport argparse\nfrom .GobangNNet import GobangNNet as onnet\n\nargs = dotdict({\n    \'lr\': 0.001,\n    \'dropout\': 0.3,\n    \'epochs\': 10,\n    \'batch_size\': 64,\n    \'cuda\': True,\n    \'num_channels\': 512,\n})\n\n\n\nclass NNetWrapper(NeuralNet):\n    def __init__(self, game):\n        self.graph = tf.get_default_graph()\n        self.nnet = onnet(game, args)\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n\n    def train(self, examples):\n        """"""\n        examples: list of examples, each example is of form (board, pi, v)\n        """"""\n        input_boards, target_pis, target_vs = list(zip(*examples))\n        input_boards = np.asarray(input_boards)\n        target_pis = np.asarray(target_pis)\n        target_vs = np.asarray(target_vs)\n        self.nnet.model.fit(x = input_boards, y = [target_pis, target_vs], batch_size = args.batch_size, epochs = args.epochs)\n\n    def predict(self, board):\n        """"""\n        board: np array with board\n        """"""\n        # timing\n        # start = time.time()\n\n        # preparing input\n        board = board[np.newaxis, :, :]\n        with self.graph.as_default():\n            # run\n            self.nnet.model._make_predict_function()\n            pi, v = self.nnet.model.predict(board)\n\n        #print(\'PREDICTION TIME TAKEN : {0:03f}\'.format(time.time()-start))\n        return pi[0], v[0]\n\n    def save_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(folder):\n            print(""Checkpoint Directory does not exist! Making directory {}"".format(folder))\n            os.mkdir(folder)\n        else:\n            print(""Checkpoint Directory exists! "")\n        self.nnet.model.save_weights(filepath)\n\n    def load_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(filepath):\n            raise(""No model in path {}"".format(filepath))\n        self.nnet.model.load_weights(filepath)\n'"
gobang/tensorflow/GobangNNet.py,0,"b'import sys\nsys.path.append(\'..\')\nfrom utils import *\n\nimport tensorflow as tf\n\nclass GobangNNet():\n    def __init__(self, game, args):\n        # game params\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n        self.args = args\n\n        # Renaming functions \n        Relu = tf.nn.relu\n        Tanh = tf.nn.tanh\n        BatchNormalization = tf.layers.batch_normalization\n        Dropout = tf.layers.dropout\n        Dense = tf.layers.dense\n\n        # Neural Net\n        self.graph = tf.Graph()\n        with self.graph.as_default():\n            self.input_boards = tf.placeholder(tf.float32, shape=[None, self.board_x, self.board_y])    # s: batch_size x board_x x board_y\n            self.dropout = tf.placeholder(tf.float32)\n            self.isTraining = tf.placeholder(tf.bool, name=""is_training"")\n\n            x_image = tf.reshape(self.input_boards, [-1, self.board_x, self.board_y, 1])                    # batch_size  x board_x x board_y x 1\n            h_conv1 = Relu(BatchNormalization(self.conv2d(x_image, args.num_channels, \'same\'), axis=3, training=self.isTraining))     # batch_size  x board_x x board_y x num_channels\n            h_conv2 = Relu(BatchNormalization(self.conv2d(h_conv1, args.num_channels, \'same\'), axis=3, training=self.isTraining))     # batch_size  x board_x x board_y x num_channels\n            h_conv3 = Relu(BatchNormalization(self.conv2d(h_conv2, args.num_channels, \'valid\'), axis=3, training=self.isTraining))    # batch_size  x (board_x-2) x (board_y-2) x num_channels\n            h_conv4 = Relu(BatchNormalization(self.conv2d(h_conv3, args.num_channels, \'valid\'), axis=3, training=self.isTraining))    # batch_size  x (board_x-4) x (board_y-4) x num_channels\n            h_conv4_flat = tf.reshape(h_conv4, [-1, args.num_channels*(self.board_x-4)*(self.board_y-4)])\n            s_fc1 = Dropout(Relu(BatchNormalization(Dense(h_conv4_flat, 1024), axis=1, training=self.isTraining)), rate=self.dropout) # batch_size x 1024\n            s_fc2 = Dropout(Relu(BatchNormalization(Dense(s_fc1, 512), axis=1, training=self.isTraining)), rate=self.dropout)         # batch_size x 512\n            self.pi = Dense(s_fc2, self.action_size)                                                        # batch_size x self.action_size\n            self.prob = tf.nn.softmax(self.pi)\n            self.v = Tanh(Dense(s_fc2, 1))                                                               # batch_size x 1\n\n            self.calculate_loss()\n\n    def conv2d(self, x, out_channels, padding):\n      return tf.layers.conv2d(x, out_channels, kernel_size=[3,3], padding=padding)\n\n    def calculate_loss(self):\n        self.target_pis = tf.placeholder(tf.float32, shape=[None, self.action_size])\n        self.target_vs = tf.placeholder(tf.float32, shape=[None])\n        self.loss_pi =  tf.losses.softmax_cross_entropy(self.target_pis, self.pi)\n        self.loss_v = tf.losses.mean_squared_error(self.target_vs, tf.reshape(self.v, shape=[-1,]))\n        self.total_loss = self.loss_pi + self.loss_v\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            self.train_step = tf.train.AdamOptimizer(self.args.lr).minimize(self.total_loss)\n\n\n\n\n'"
gobang/tensorflow/NNet.py,0,"b'import os\nimport sys\n\nimport numpy as np\nfrom tqdm import tqdm\n\nsys.path.append(\'../../\')\nfrom utils import *\nfrom NeuralNet import NeuralNet\n\nimport tensorflow as tf\nfrom .GobangNNet import GobangNNet as onnet\n\nargs = dotdict({\n    \'lr\': 0.001,\n    \'dropout\': 0.3,\n    \'epochs\': 10,\n    \'batch_size\': 64,\n    \'num_channels\': 512,\n})\n\n\nclass NNetWrapper(NeuralNet):\n    def __init__(self, game):\n        self.nnet = onnet(game, args)\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n\n        self.sess = tf.Session(graph=self.nnet.graph)\n        self.saver = None\n        with tf.Session() as temp_sess:\n            temp_sess.run(tf.global_variables_initializer())\n        self.sess.run(tf.variables_initializer(self.nnet.graph.get_collection(\'variables\')))\n\n    def train(self, examples):\n        """"""\n        examples: list of examples, each example is of form (board, pi, v)\n        """"""\n\n        for epoch in range(args.epochs):\n            print(\'EPOCH ::: \' + str(epoch + 1))\n            pi_losses = AverageMeter()\n            v_losses = AverageMeter()\n            batch_count = int(len(examples) / args.batch_size)\n\n            # self.sess.run(tf.local_variables_initializer())\n            t = tqdm(range(batch_count), desc=\'Training Net\')\n            for _ in t:\n                sample_ids = np.random.randint(len(examples), size=args.batch_size)\n                boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n\n                # predict and compute gradient and do SGD step\n                input_dict = {self.nnet.input_boards: boards, self.nnet.target_pis: pis, self.nnet.target_vs: vs,\n                              self.nnet.dropout: args.dropout, self.nnet.isTraining: True}\n\n                # record loss\n                self.sess.run(self.nnet.train_step, feed_dict=input_dict)\n                pi_loss, v_loss = self.sess.run([self.nnet.loss_pi, self.nnet.loss_v], feed_dict=input_dict)\n                pi_losses.update(pi_loss, len(boards))\n                v_losses.update(v_loss, len(boards))\n                t.set_postfix(Loss_pi=pi_losses, Loss_v=v_losses)\n\n    def predict(self, board):\n        """"""\n        board: np array with board\n        """"""\n        # timing\n        # start = time.time()\n\n        # preparing input\n        board = board[np.newaxis, :, :]\n\n        # run\n        prob, v = self.sess.run([self.nnet.prob, self.nnet.v],\n                                feed_dict={self.nnet.input_boards: board, self.nnet.dropout: 0,\n                                           self.nnet.isTraining: False})\n\n        # print(\'PREDICTION TIME TAKEN : {0:03f}\'.format(time.time()-start))\n        return prob[0], v[0]\n\n    def save_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(folder):\n            print(""Checkpoint Directory does not exist! Making directory {}"".format(folder))\n            os.mkdir(folder)\n        else:\n            print(""Checkpoint Directory exists! "")\n        if self.saver == None:\n            self.saver = tf.train.Saver(self.nnet.graph.get_collection(\'variables\'))\n        with self.nnet.graph.as_default():\n            self.saver.save(self.sess, filepath)\n\n    def load_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(filepath + \'.meta\'):\n            raise (""No model in path {}"".format(filepath))\n        with self.nnet.graph.as_default():\n            self.saver = tf.train.Saver()\n            self.saver.restore(self.sess, filepath)\n'"
gobang/tensorflow/__init__.py,0,b''
othello/chainer/NNet.py,0,"b'import os\nimport sys\nimport time\n\nimport chainer\nimport chainer.functions as F\nimport numpy as np\nfrom chainer import optimizers, cuda, serializers, training\nfrom chainer.dataset import concat_examples\nfrom chainer.iterators import SerialIterator\nfrom chainer.training import extensions\nfrom tqdm import tqdm\n\nsys.path.append(\'../../\')\nfrom utils import *\nfrom NeuralNet import NeuralNet\nfrom .OthelloNNet import OthelloNNet as onnet\n\nargs = dotdict({\n    \'lr\': 0.001,\n    \'dropout\': 0.3,\n    \'epochs\': 10,\n    \'batch_size\': 64,\n    \'device\': 0 if chainer.cuda.available else -1,  # GPU device id for training model, -1 indicates to use CPU.\n    \'num_channels\': 512,\n    \'out\': \'result_chainer\',  # Output directory for chainer\n    \'train_mode\': \'trainer\'  # \'trainer\' or \'custom_loop\' supported.\n})\n\n\ndef converter(batch, device=None):\n    """"""Convert arrays to float32""""""\n    batch_list = concat_examples(batch, device=device)\n    xp = cuda.get_array_module(batch_list[0])\n    batch = tuple([xp.asarray(elem, dtype=xp.float32) for elem in batch_list])\n    return batch\n\n\nclass NNetWrapper(NeuralNet):\n\n    def __init__(self, game):\n        super(NNetWrapper, self).__init__(game)\n        self.nnet = onnet(game, args)\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n\n        device = args.device\n        if device >= 0:\n            chainer.cuda.get_device_from_id(device).use()  # Make a specified GPU current\n            self.nnet.to_gpu()\n\n    def train(self, examples):\n        if args.train_mode == \'trainer\':\n            self._train_trainer(examples)\n        elif args.train_mode == \'custom_loop\':\n            self._train_custom_loop(examples)\n        else:\n            raise ValueError(""[ERROR] Unexpected value args.train_mode={}""\n                             .format(args.train_mode))\n\n    def _train_trainer(self, examples):\n        """"""Training with chainer trainer module""""""\n        train_iter = SerialIterator(examples, args.batch_size)\n        optimizer = optimizers.Adam(alpha=args.lr)\n        optimizer.setup(self.nnet)\n\n        def loss_func(boards, target_pis, target_vs):\n            out_pi, out_v = self.nnet(boards)\n            l_pi = self.loss_pi(target_pis, out_pi)\n            l_v = self.loss_v(target_vs, out_v)\n            total_loss = l_pi + l_v\n            chainer.reporter.report({\n                \'loss\': total_loss,\n                \'loss_pi\': l_pi,\n                \'loss_v\': l_v,\n            }, observer=self.nnet)\n            return total_loss\n\n        updater = training.StandardUpdater(\n            train_iter, optimizer, device=args.device, loss_func=loss_func, converter=converter)\n        # Set up the trainer.\n        trainer = training.Trainer(updater, (args.epochs, \'epoch\'), out=args.out)\n        # trainer.extend(extensions.snapshot(), trigger=(args.epochs, \'epoch\'))\n        trainer.extend(extensions.LogReport())\n        trainer.extend(extensions.PrintReport([\n            \'epoch\', \'main/loss\', \'main/loss_pi\', \'main/loss_v\', \'elapsed_time\']))\n        trainer.extend(extensions.ProgressBar(update_interval=10))\n        trainer.run()\n\n    def _train_custom_loop(self, examples):\n        """"""\n        examples: list of examples, each example is of form (board, pi, v)\n        """"""\n        optimizer = optimizers.Adam(alpha=args.lr)\n        optimizer.setup(self.nnet)\n\n        for epoch in range(args.epochs):\n            print(\'EPOCH ::: \' + str(epoch + 1))\n            # self.nnet.train()\n            pi_losses = AverageMeter()\n            v_losses = AverageMeter()\n            batch_count = int(len(examples) / args.batch_size)\n\n            t = tqdm(range(batch_count), desc=\'Training Net\')\n            for _ in t:\n                sample_ids = np.random.randint(len(examples), size=args.batch_size)\n                boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n                xp = self.nnet.xp\n                boards = xp.array(boards, dtype=xp.float32)\n                target_pis = xp.array(pis, dtype=xp.float32)\n                target_vs = xp.array(vs, dtype=xp.float32)\n\n                # compute output\n                out_pi, out_v = self.nnet(boards)\n                l_pi = self.loss_pi(target_pis, out_pi)\n                l_v = self.loss_v(target_vs, out_v)\n                total_loss = l_pi + l_v\n\n                # record loss\n                pi_loss = l_pi.data\n                v_loss = l_v.data\n                pi_losses.update(cuda.to_cpu(pi_loss), boards.shape[0])\n                v_losses.update(cuda.to_cpu(v_loss), boards.shape[0])\n                t.set_postfix(Loss_pi=pi_losses, Loss_v=v_losses)\n\n                # compute gradient and do SGD step\n                self.nnet.cleargrads()\n                total_loss.backward()\n                optimizer.update()\n\n    def predict(self, board):\n        """"""\n        board: np array with board\n        """"""\n        # timing\n        start = time.time()\n\n        # preparing input\n        xp = self.nnet.xp\n        board = xp.array(board, dtype=xp.float32)\n        with chainer.using_config(\'train\', False), chainer.no_backprop_mode():\n            board = xp.reshape(board, (1, self.board_x, self.board_y))\n            pi, v = self.nnet(board)\n        return np.exp(cuda.to_cpu(pi.array)[0]), cuda.to_cpu(v.array)[0]\n\n    def loss_pi(self, targets, outputs):\n        return -F.sum(targets * outputs) / targets.shape[0]\n\n    def loss_v(self, targets, outputs):\n        return F.mean_squared_error(targets[:, None], outputs)\n\n    def save_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(folder):\n            print(""Checkpoint Directory does not exist! Making directory {}"".format(folder))\n            os.mkdir(folder)\n        else:\n            # print(""Checkpoint Directory exists! "")\n            pass\n        print(\'Saving model at {}\'.format(filepath))\n        serializers.save_npz(filepath, self.nnet)\n\n    def load_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(filepath):\n            raise (""No model in path {}"".format(filepath))\n        serializers.load_npz(filepath, self.nnet)\n'"
othello/chainer/OthelloNNet.py,0,"b'import chainer\nimport chainer.functions as F  # NOQA\nimport chainer.links as L  # NOQA\n\n\nclass OthelloNNet(chainer.Chain):\n\n    def __init__(self, game, args):\n        # game params\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n        self.args = args\n\n        super(OthelloNNet, self).__init__()\n        with self.init_scope():\n            self.conv1 = L.Convolution2D(1, args.num_channels, 3, stride=1, pad=1)\n            self.conv2 = L.Convolution2D(args.num_channels, args.num_channels, 3, stride=1, pad=1)\n            self.conv3 = L.Convolution2D(args.num_channels, args.num_channels, 3, stride=1)\n            self.conv4 = L.Convolution2D(args.num_channels, args.num_channels, 3, stride=1)\n\n            self.bn1 = L.BatchNormalization(args.num_channels)\n            self.bn2 = L.BatchNormalization(args.num_channels)\n            self.bn3 = L.BatchNormalization(args.num_channels)\n            self.bn4 = L.BatchNormalization(args.num_channels)\n\n            self.fc1 = L.Linear(args.num_channels*(self.board_x-4)*(self.board_y-4), 1024)\n            self.fc_bn1 = L.BatchNormalization(1024)\n\n            self.fc2 = L.Linear(1024, 512)\n            self.fc_bn2 = L.BatchNormalization(512)\n\n            self.fc3 = L.Linear(512, self.action_size)\n\n            self.fc4 = L.Linear(512, 1)\n\n    def forward(self, s):\n        #                                                      s: batch_size x board_x x board_y\n        s = F.reshape(s, (-1, 1, self.board_x, self.board_y))  # batch_size x 1 x board_x x board_y\n        s = F.relu(self.bn1(self.conv1(s)))                    # batch_size x num_channels x board_x x board_y\n        s = F.relu(self.bn2(self.conv2(s)))                    # batch_size x num_channels x board_x x board_y\n        s = F.relu(self.bn3(self.conv3(s)))                    # batch_size x num_channels x (board_x-2) x (board_y-2)\n        s = F.relu(self.bn4(self.conv4(s)))                    # batch_size x num_channels x (board_x-4) x (board_y-4)\n        s = F.reshape(s, (-1, self.args.num_channels*(self.board_x-4)*(self.board_y-4)))\n\n        s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), ratio=self.args.dropout)  # batch_size x 1024\n        s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), ratio=self.args.dropout)  # batch_size x 512\n\n        pi = self.fc3(s)                                             # batch_size x action_size\n        v = self.fc4(s)                                              # batch_size x 1\n\n        return F.log_softmax(pi, axis=1), F.tanh(v)\n'"
othello/chainer/__init__.py,0,b''
othello/keras/NNet.py,0,"b'import argparse\nimport os\nimport shutil\nimport time\nimport random\nimport numpy as np\nimport math\nimport sys\nsys.path.append(\'../..\')\nfrom utils import *\nfrom NeuralNet import NeuralNet\n\nimport argparse\n\nfrom .OthelloNNet import OthelloNNet as onnet\n\nargs = dotdict({\n    \'lr\': 0.001,\n    \'dropout\': 0.3,\n    \'epochs\': 10,\n    \'batch_size\': 64,\n    \'cuda\': False,\n    \'num_channels\': 512,\n})\n\nclass NNetWrapper(NeuralNet):\n    def __init__(self, game):\n        self.nnet = onnet(game, args)\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n\n    def train(self, examples):\n        """"""\n        examples: list of examples, each example is of form (board, pi, v)\n        """"""\n        input_boards, target_pis, target_vs = list(zip(*examples))\n        input_boards = np.asarray(input_boards)\n        target_pis = np.asarray(target_pis)\n        target_vs = np.asarray(target_vs)\n        self.nnet.model.fit(x = input_boards, y = [target_pis, target_vs], batch_size = args.batch_size, epochs = args.epochs)\n\n    def predict(self, board):\n        """"""\n        board: np array with board\n        """"""\n        # timing\n        start = time.time()\n\n        # preparing input\n        board = board[np.newaxis, :, :]\n\n        # run\n        pi, v = self.nnet.model.predict(board)\n\n        #print(\'PREDICTION TIME TAKEN : {0:03f}\'.format(time.time()-start))\n        return pi[0], v[0]\n\n    def save_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(folder):\n            print(""Checkpoint Directory does not exist! Making directory {}"".format(folder))\n            os.mkdir(folder)\n        else:\n            print(""Checkpoint Directory exists! "")\n        self.nnet.model.save_weights(filepath)\n\n    def load_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(filepath):\n            raise(""No model in path {}"".format(filepath))\n        self.nnet.model.load_weights(filepath)\n'"
othello/keras/OthelloNNet.py,0,"b""import sys\nsys.path.append('..')\nfrom utils import *\n\nimport argparse\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\n\nclass OthelloNNet():\n    def __init__(self, game, args):\n        # game params\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n        self.args = args\n\n        # Neural Net\n        self.input_boards = Input(shape=(self.board_x, self.board_y))    # s: batch_size x board_x x board_y\n\n        x_image = Reshape((self.board_x, self.board_y, 1))(self.input_boards)                # batch_size  x board_x x board_y x 1\n        h_conv1 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='same', use_bias=False)(x_image)))         # batch_size  x board_x x board_y x num_channels\n        h_conv2 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='same', use_bias=False)(h_conv1)))         # batch_size  x board_x x board_y x num_channels\n        h_conv3 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='valid', use_bias=False)(h_conv2)))        # batch_size  x (board_x-2) x (board_y-2) x num_channels\n        h_conv4 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='valid', use_bias=False)(h_conv3)))        # batch_size  x (board_x-4) x (board_y-4) x num_channels\n        h_conv4_flat = Flatten()(h_conv4)       \n        s_fc1 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(1024, use_bias=False)(h_conv4_flat))))  # batch_size x 1024\n        s_fc2 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(512, use_bias=False)(s_fc1))))          # batch_size x 1024\n        self.pi = Dense(self.action_size, activation='softmax', name='pi')(s_fc2)   # batch_size x self.action_size\n        self.v = Dense(1, activation='tanh', name='v')(s_fc2)                    # batch_size x 1\n\n        self.model = Model(inputs=self.input_boards, outputs=[self.pi, self.v])\n        self.model.compile(loss=['categorical_crossentropy','mean_squared_error'], optimizer=Adam(args.lr))\n"""
othello/keras/__init__.py,0,b''
othello/pytorch/NNet.py,12,"b'import os\nimport sys\nimport time\n\nimport numpy as np\nfrom tqdm import tqdm\n\nsys.path.append(\'../../\')\nfrom utils import *\nfrom NeuralNet import NeuralNet\n\nimport torch\nimport torch.optim as optim\n\nfrom .OthelloNNet import OthelloNNet as onnet\n\nargs = dotdict({\n    \'lr\': 0.001,\n    \'dropout\': 0.3,\n    \'epochs\': 10,\n    \'batch_size\': 64,\n    \'cuda\': torch.cuda.is_available(),\n    \'num_channels\': 512,\n})\n\n\nclass NNetWrapper(NeuralNet):\n    def __init__(self, game):\n        self.nnet = onnet(game, args)\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n\n        if args.cuda:\n            self.nnet.cuda()\n\n    def train(self, examples):\n        """"""\n        examples: list of examples, each example is of form (board, pi, v)\n        """"""\n        optimizer = optim.Adam(self.nnet.parameters())\n\n        for epoch in range(args.epochs):\n            print(\'EPOCH ::: \' + str(epoch + 1))\n            self.nnet.train()\n            pi_losses = AverageMeter()\n            v_losses = AverageMeter()\n\n            batch_count = int(len(examples) / args.batch_size)\n\n            t = tqdm(range(batch_count), desc=\'Training Net\')\n            for _ in t:\n                sample_ids = np.random.randint(len(examples), size=args.batch_size)\n                boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n                boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n                target_pis = torch.FloatTensor(np.array(pis))\n                target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n\n                # predict\n                if args.cuda:\n                    boards, target_pis, target_vs = boards.contiguous().cuda(), target_pis.contiguous().cuda(), target_vs.contiguous().cuda()\n\n                # compute output\n                out_pi, out_v = self.nnet(boards)\n                l_pi = self.loss_pi(target_pis, out_pi)\n                l_v = self.loss_v(target_vs, out_v)\n                total_loss = l_pi + l_v\n\n                # record loss\n                pi_losses.update(l_pi.item(), boards.size(0))\n                v_losses.update(l_v.item(), boards.size(0))\n                t.set_postfix(Loss_pi=pi_losses, Loss_v=v_losses)\n\n                # compute gradient and do SGD step\n                optimizer.zero_grad()\n                total_loss.backward()\n                optimizer.step()\n\n    def predict(self, board):\n        """"""\n        board: np array with board\n        """"""\n        # timing\n        start = time.time()\n\n        # preparing input\n        board = torch.FloatTensor(board.astype(np.float64))\n        if args.cuda: board = board.contiguous().cuda()\n        board = board.view(1, self.board_x, self.board_y)\n        self.nnet.eval()\n        with torch.no_grad():\n            pi, v = self.nnet(board)\n\n        # print(\'PREDICTION TIME TAKEN : {0:03f}\'.format(time.time()-start))\n        return torch.exp(pi).data.cpu().numpy()[0], v.data.cpu().numpy()[0]\n\n    def loss_pi(self, targets, outputs):\n        return -torch.sum(targets * outputs) / targets.size()[0]\n\n    def loss_v(self, targets, outputs):\n        return torch.sum((targets - outputs.view(-1)) ** 2) / targets.size()[0]\n\n    def save_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(folder):\n            print(""Checkpoint Directory does not exist! Making directory {}"".format(folder))\n            os.mkdir(folder)\n        else:\n            print(""Checkpoint Directory exists! "")\n        torch.save({\n            \'state_dict\': self.nnet.state_dict(),\n        }, filepath)\n\n    def load_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(filepath):\n            raise (""No model in path {}"".format(filepath))\n        map_location = None if args.cuda else \'cpu\'\n        checkpoint = torch.load(filepath, map_location=map_location)\n        self.nnet.load_state_dict(checkpoint[\'state_dict\'])\n'"
othello/pytorch/OthelloNNet.py,5,"b""import sys\nsys.path.append('..')\nfrom utils import *\n\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\nclass OthelloNNet(nn.Module):\n    def __init__(self, game, args):\n        # game params\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n        self.args = args\n\n        super(OthelloNNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, args.num_channels, 3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n        self.conv4 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n\n        self.bn1 = nn.BatchNorm2d(args.num_channels)\n        self.bn2 = nn.BatchNorm2d(args.num_channels)\n        self.bn3 = nn.BatchNorm2d(args.num_channels)\n        self.bn4 = nn.BatchNorm2d(args.num_channels)\n\n        self.fc1 = nn.Linear(args.num_channels*(self.board_x-4)*(self.board_y-4), 1024)\n        self.fc_bn1 = nn.BatchNorm1d(1024)\n\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc_bn2 = nn.BatchNorm1d(512)\n\n        self.fc3 = nn.Linear(512, self.action_size)\n\n        self.fc4 = nn.Linear(512, 1)\n\n    def forward(self, s):\n        #                                                           s: batch_size x board_x x board_y\n        s = s.view(-1, 1, self.board_x, self.board_y)                # batch_size x 1 x board_x x board_y\n        s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n        s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n        s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n        s = F.relu(self.bn4(self.conv4(s)))                          # batch_size x num_channels x (board_x-4) x (board_y-4)\n        s = s.view(-1, self.args.num_channels*(self.board_x-4)*(self.board_y-4))\n\n        s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.args.dropout, training=self.training)  # batch_size x 1024\n        s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), p=self.args.dropout, training=self.training)  # batch_size x 512\n\n        pi = self.fc3(s)                                                                         # batch_size x action_size\n        v = self.fc4(s)                                                                          # batch_size x 1\n\n        return F.log_softmax(pi, dim=1), torch.tanh(v)\n"""
othello/pytorch/__init__.py,0,b''
othello/tensorflow/NNet.py,0,"b'import os\nimport sys\nimport time\n\nimport numpy as np\nfrom tqdm import tqdm\n\nsys.path.append(\'../../\')\nfrom utils import *\nfrom NeuralNet import NeuralNet\n\nimport tensorflow as tf\nfrom .OthelloNNet import OthelloNNet as onnet\n\nargs = dotdict({\n    \'lr\': 0.001,\n    \'dropout\': 0.3,\n    \'epochs\': 10,\n    \'batch_size\': 64,\n    \'num_channels\': 512,\n})\n\n\nclass NNetWrapper(NeuralNet):\n    def __init__(self, game):\n        self.nnet = onnet(game, args)\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n\n        self.sess = tf.Session(graph=self.nnet.graph)\n        self.saver = None\n        with tf.Session() as temp_sess:\n            temp_sess.run(tf.global_variables_initializer())\n        self.sess.run(tf.variables_initializer(self.nnet.graph.get_collection(\'variables\')))\n\n    def train(self, examples):\n        """"""\n        examples: list of examples, each example is of form (board, pi, v)\n        """"""\n\n        for epoch in range(args.epochs):\n            print(\'EPOCH ::: \' + str(epoch + 1))\n            pi_losses = AverageMeter()\n            v_losses = AverageMeter()\n            batch_count = int(len(examples) / args.batch_size)\n\n            # self.sess.run(tf.local_variables_initializer())\n            t = tqdm(range(batch_count), desc=\'Training Net\')\n            for _ in t:\n                sample_ids = np.random.randint(len(examples), size=args.batch_size)\n                boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n\n                # predict and compute gradient and do SGD step\n                input_dict = {self.nnet.input_boards: boards, self.nnet.target_pis: pis, self.nnet.target_vs: vs,\n                              self.nnet.dropout: args.dropout, self.nnet.isTraining: True}\n\n                # record loss\n                self.sess.run(self.nnet.train_step, feed_dict=input_dict)\n                pi_loss, v_loss = self.sess.run([self.nnet.loss_pi, self.nnet.loss_v], feed_dict=input_dict)\n                pi_losses.update(pi_loss, len(boards))\n                v_losses.update(v_loss, len(boards))\n                t.set_postfix(Loss_pi=pi_losses, Loss_v=v_losses)\n\n    def predict(self, board):\n        """"""\n        board: np array with board\n        """"""\n        # timing\n        start = time.time()\n\n        # preparing input\n        board = board[np.newaxis, :, :]\n\n        # run\n        prob, v = self.sess.run([self.nnet.prob, self.nnet.v],\n                                feed_dict={self.nnet.input_boards: board, self.nnet.dropout: 0,\n                                           self.nnet.isTraining: False})\n\n        # print(\'PREDICTION TIME TAKEN : {0:03f}\'.format(time.time()-start))\n        return prob[0], v[0]\n\n    def save_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(folder):\n            print(""Checkpoint Directory does not exist! Making directory {}"".format(folder))\n            os.mkdir(folder)\n        else:\n            print(""Checkpoint Directory exists! "")\n        if self.saver == None:\n            self.saver = tf.train.Saver(self.nnet.graph.get_collection(\'variables\'))\n        with self.nnet.graph.as_default():\n            self.saver.save(self.sess, filepath)\n\n    def load_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(filepath + \'.meta\'):\n            raise (""No model in path {}"".format(filepath))\n        with self.nnet.graph.as_default():\n            self.saver = tf.train.Saver()\n            self.saver.restore(self.sess, filepath)\n'"
othello/tensorflow/OthelloNNet.py,0,"b'import sys\nsys.path.append(\'..\')\nfrom utils import *\n\nimport tensorflow as tf\n\nclass OthelloNNet():\n    def __init__(self, game, args):\n        # game params\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n        self.args = args\n\n        # Renaming functions \n        Relu = tf.nn.relu\n        Tanh = tf.nn.tanh\n        BatchNormalization = tf.layers.batch_normalization\n        Dropout = tf.layers.dropout\n        Dense = tf.layers.dense\n\n        # Neural Net\n        self.graph = tf.Graph()\n        with self.graph.as_default(): \n            self.input_boards = tf.placeholder(tf.float32, shape=[None, self.board_x, self.board_y])    # s: batch_size x board_x x board_y\n            self.dropout = tf.placeholder(tf.float32)\n            self.isTraining = tf.placeholder(tf.bool, name=""is_training"")\n\n            x_image = tf.reshape(self.input_boards, [-1, self.board_x, self.board_y, 1])                    # batch_size  x board_x x board_y x 1\n            h_conv1 = Relu(BatchNormalization(self.conv2d(x_image, args.num_channels, \'same\'), axis=3, training=self.isTraining))     # batch_size  x board_x x board_y x num_channels\n            h_conv2 = Relu(BatchNormalization(self.conv2d(h_conv1, args.num_channels, \'same\'), axis=3, training=self.isTraining))     # batch_size  x board_x x board_y x num_channels\n            h_conv3 = Relu(BatchNormalization(self.conv2d(h_conv2, args.num_channels, \'valid\'), axis=3, training=self.isTraining))    # batch_size  x (board_x-2) x (board_y-2) x num_channels\n            h_conv4 = Relu(BatchNormalization(self.conv2d(h_conv3, args.num_channels, \'valid\'), axis=3, training=self.isTraining))    # batch_size  x (board_x-4) x (board_y-4) x num_channels\n            h_conv4_flat = tf.reshape(h_conv4, [-1, args.num_channels*(self.board_x-4)*(self.board_y-4)])\n            s_fc1 = Dropout(Relu(BatchNormalization(Dense(h_conv4_flat, 1024, use_bias=False), axis=1, training=self.isTraining)), rate=self.dropout) # batch_size x 1024\n            s_fc2 = Dropout(Relu(BatchNormalization(Dense(s_fc1, 512, use_bias=False), axis=1, training=self.isTraining)), rate=self.dropout)         # batch_size x 512\n            self.pi = Dense(s_fc2, self.action_size)                                                        # batch_size x self.action_size\n            self.prob = tf.nn.softmax(self.pi)\n            self.v = Tanh(Dense(s_fc2, 1))                                                               # batch_size x 1\n\n            self.calculate_loss()\n\n    def conv2d(self, x, out_channels, padding):\n      return tf.layers.conv2d(x, out_channels, kernel_size=[3,3], padding=padding, use_bias=False)\n\n    def calculate_loss(self):\n        self.target_pis = tf.placeholder(tf.float32, shape=[None, self.action_size])\n        self.target_vs = tf.placeholder(tf.float32, shape=[None])\n        self.loss_pi =  tf.losses.softmax_cross_entropy(self.target_pis, self.pi)\n        self.loss_v = tf.losses.mean_squared_error(self.target_vs, tf.reshape(self.v, shape=[-1,]))\n        self.total_loss = self.loss_pi + self.loss_v\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            self.train_step = tf.train.AdamOptimizer(self.args.lr).minimize(self.total_loss)\n\nclass ResNet():\n    def __init__(self, game, args):\n        # game params\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n        self.args = args\n\n        # Neural Net\n        self.graph = tf.Graph()\n        with self.graph.as_default(): \n            self.input_boards = tf.placeholder(tf.float32, shape=[None, self.board_x, self.board_y])    # s: batch_size x board_x x board_y\n            self.dropout = tf.placeholder(tf.float32)\n            self.isTraining = tf.placeholder(tf.bool, name=""is_training"")\n\n            x_image = tf.reshape(self.input_boards, [-1, self.board_x, self.board_y, 1])                    # batch_size  x board_x x board_y x 1\n            x_image = tf.layers.conv2d(x_image, args.num_channels, kernel_size=(3, 3), strides=(1, 1),name=\'conv\',padding=\'same\',use_bias=False)\n            x_image = tf.layers.batch_normalization(x_image, axis=1, name=\'conv_bn\', training=self.isTraining)\n            x_image = tf.nn.relu(x_image)\n\n            residual_tower = self.residual_block(inputLayer=x_image, kernel_size=3, filters=args.num_channels, stage=1, block=\'a\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=2, block=\'b\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=3, block=\'c\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=4, block=\'d\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=5, block=\'e\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=6, block=\'g\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=7, block=\'h\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=8, block=\'i\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=9, block=\'j\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=10, block=\'k\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=11, block=\'m\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=12, block=\'n\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=13, block=\'o\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=14, block=\'p\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=15, block=\'q\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=16, block=\'r\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=17, block=\'s\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=18, block=\'t\')\n            residual_tower = self.residual_block(inputLayer=residual_tower, kernel_size=3, filters=args.num_channels, stage=19, block=\'u\')\n\n            policy = tf.layers.conv2d(residual_tower, 2,kernel_size=(1, 1), strides=(1, 1),name=\'pi\',padding=\'same\',use_bias=False)\n            policy = tf.layers.batch_normalization(policy, axis=3, name=\'bn_pi\', training=self.isTraining)\n            policy = tf.nn.relu(policy)\n            policy = tf.layers.flatten(policy, name=\'p_flatten\')\n            self.pi = tf.layers.dense(policy, self.action_size)\n            self.prob = tf.nn.softmax(self.pi)\n\n            value = tf.layers.conv2d(residual_tower, 1,kernel_size=(1, 1), strides=(1, 1),name=\'v\',padding=\'same\',use_bias=False)\n            value = tf.layers.batch_normalization(value, axis=3, name=\'bn_v\', training=self.isTraining)\n            value = tf.nn.relu(value)\n            value = tf.layers.flatten(value, name=\'v_flatten\')\n            value = tf.layers.dense(value, units=256)\n            value = tf.nn.relu(value)\n            value = tf.layers.dense(value, 1)\n            self.v = tf.nn.tanh(value) \n                                                              \n            self.calculate_loss()\n\n    def residual_block(self,inputLayer, filters,kernel_size,stage,block):\n        conv_name = \'res\' + str(stage) + block + \'_branch\'\n        bn_name = \'bn\' + str(stage) + block + \'_branch\'\n\n        shortcut = inputLayer\n\n        residual_layer = tf.layers.conv2d(inputLayer, filters,kernel_size=(kernel_size, kernel_size), strides=(1, 1),name=conv_name+\'2a\',padding=\'same\',use_bias=False)\n        residual_layer = tf.layers.batch_normalization(residual_layer, axis=3, name=bn_name+\'2a\', training=self.isTraining)\n        residual_layer = tf.nn.relu(residual_layer)\n        residual_layer = tf.layers.conv2d(residual_layer, filters,kernel_size=(kernel_size, kernel_size), strides=(1, 1),name=conv_name+\'2b\',padding=\'same\',use_bias=False)\n        residual_layer = tf.layers.batch_normalization(residual_layer, axis=3, name=bn_name+\'2b\', training=self.isTraining)\n        add_shortcut = tf.add(residual_layer, shortcut)\n        residual_result = tf.nn.relu(add_shortcut)\n        \n        return residual_result\n\n    def calculate_loss(self):\n        self.target_pis = tf.placeholder(tf.float32, shape=[None, self.action_size])\n        self.target_vs = tf.placeholder(tf.float32, shape=[None])\n        self.loss_pi =  tf.losses.softmax_cross_entropy(self.target_pis, self.pi)\n        self.loss_v = tf.losses.mean_squared_error(self.target_vs, tf.reshape(self.v, shape=[-1,]))\n        self.total_loss = self.loss_pi + self.loss_v\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n        with tf.control_dependencies(update_ops):\n            self.train_step = tf.train.AdamOptimizer(self.args.lr).minimize(self.total_loss)\n\n\n'"
othello/tensorflow/__init__.py,0,b''
rts/keras/NNet.py,0,"b'import os\nimport sys\n\nimport numpy as np\n\nsys.path.append(\'../..\')\nfrom NeuralNet import NeuralNet\nfrom rts.keras.RTSNNet import RTSNNet\nfrom rts.src.config import VERBOSE_MODEL_FIT\n\n""""""\nNNet.py\n\nNNet wrapper uses defined nnet model to train and predict\n\nfunny error message from tf :) - tensorflow.python.framework.errors_impl.NotFoundError: No algorithm worked!\n""""""\n\n\n# noinspection PyMissingConstructor\nclass NNetWrapper(NeuralNet):\n    def __init__(self, game, encoder=None):\n        """"""\n        Creates nnet wrapper with game configuration and encoder\n        :param game: game configuration\n        :param encoder: encoded that will be used for training and later predictions\n        """"""\n        from rts.src.config_class import CONFIG\n\n        # default\n        encoder = encoder or CONFIG.nnet_args.encoder\n\n        self.nnet = RTSNNet(game, encoder)\n        self.board_x, self.board_y, num_encoders = game.getBoardSize()\n        self.action_size = game.getActionSize()\n\n        self.encoder = encoder\n\n    def train(self, examples):\n        """"""\n        Encodes examples using one of 2 encoders and starts fitting.\n        :param examples: list of examples, each example is of form (board, pi, v)\n        """"""\n        from rts.src.config_class import CONFIG\n\n        input_boards, target_pis, target_vs = list(zip(*examples))\n        input_boards = np.asarray(input_boards)\n        target_pis = np.asarray(target_pis)\n        target_vs = np.asarray(target_vs)\n\n        """"""\n        input_boards = CONFIG.nnet_args.encoder.encode_multiple(input_boards)\n        """"""\n        input_boards = self.encoder.encode_multiple(input_boards)\n\n        self.nnet.model.fit(x=input_boards, y=[target_pis, target_vs], batch_size=CONFIG.nnet_args.batch_size, epochs=CONFIG.nnet_args.epochs, verbose=VERBOSE_MODEL_FIT)\n\n    def predict(self, board, player=None):\n        """"""\n        Predicts action.\n        It encodes board with encoder, that has been used for learning.\n        :param board: specific board\n        :param player: specific player\n        :return: vector of predicted actions and win prediction (Pi, V)\n        """"""\n        board = self.encoder.encode(board)\n\n        # preparing input\n        board = board[np.newaxis, :, :]\n\n        # run\n        pi, v = self.nnet.model.predict(board)\n        return pi[0], v[0]\n\n    def save_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(folder):\n            print(""Checkpoint Directory does not exist! Making directory {}"".format(folder))\n            os.mkdir(folder)\n        else:\n            print(""Checkpoint Directory exists! "")\n        self.nnet.model.save_weights(filepath)\n\n    def load_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        self.nnet.model.load_weights(filepath)\n'"
rts/keras/RTSNNet.py,0,"b'import os\nimport sys\n\nfrom tensorflow.python.keras import Input, Model\nfrom tensorflow.python.keras.layers import Conv2D, BatchNormalization, Activation, Dense, Dropout, Flatten, Reshape\nfrom tensorflow.python.keras.optimizers import Adam\n\nsys.path.append(\'../..\')\nfrom rts.src.config import USE_TF_CPU, SHOW_TENSORFLOW_GPU\n\nif USE_TF_CPU:\n    print(""Using TensorFlow CPU"")\n    os.environ[\'CUDA_VISIBLE_DEVICES\'] = \'-1\'\nif not SHOW_TENSORFLOW_GPU:\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'2\'\n\n""""""\nRTSNNet\n\nDefined NNet model used for game TD2020\n""""""\n\n\nclass RTSNNet:\n    def __init__(self, game, encoder):\n        """"""\n        NNet model, copied from Othello NNet, with reduced fully connected layers fc1 and fc2 and reduced nnet_args.num_channels\n        :param game: game configuration\n        :param encoder: Encoder, used to encode game boards\n        """"""\n        from rts.src.config_class import CONFIG\n\n        # game params\n        self.board_x, self.board_y, num_encoders = game.getBoardSize()\n        self.action_size = game.getActionSize()\n\n        """"""\n        num_encoders = CONFIG.nnet_args.encoder.num_encoders\n        """"""\n        num_encoders = encoder.num_encoders\n\n        # Neural Net\n        self.input_boards = Input(shape=(self.board_x, self.board_y, num_encoders))  # s: batch_size x board_x x board_y x num_encoders\n\n        x_image = Reshape((self.board_x, self.board_y, num_encoders))(self.input_boards)  # batch_size  x board_x x board_y x num_encoders\n        h_conv1 = Activation(\'relu\')(BatchNormalization(axis=3)(Conv2D(CONFIG.nnet_args.num_channels, 3, padding=\'same\', use_bias=False)(x_image)))  # batch_size  x board_x x board_y x num_channels\n        h_conv2 = Activation(\'relu\')(BatchNormalization(axis=3)(Conv2D(CONFIG.nnet_args.num_channels, 3, padding=\'same\', use_bias=False)(h_conv1)))  # batch_size  x board_x x board_y x num_channels\n        h_conv3 = Activation(\'relu\')(BatchNormalization(axis=3)(Conv2D(CONFIG.nnet_args.num_channels, 3, padding=\'valid\', use_bias=False)(h_conv2)))  # batch_size  x (board_x-2) x (board_y-2) x num_channels\n        h_conv4 = Activation(\'relu\')(BatchNormalization(axis=3)(Conv2D(CONFIG.nnet_args.num_channels, 3, padding=\'valid\', use_bias=False)(h_conv3)))  # batch_size  x (board_x-4) x (board_y-4) x num_channels\n        h_conv4_flat = Flatten()(h_conv4)\n        s_fc1 = Dropout(CONFIG.nnet_args.dropout)(Activation(\'relu\')(BatchNormalization(axis=1)(Dense(256, use_bias=False)(h_conv4_flat))))  # batch_size x 1024\n        s_fc2 = Dropout(CONFIG.nnet_args.dropout)(Activation(\'relu\')(BatchNormalization(axis=1)(Dense(128, use_bias=False)(s_fc1))))  # batch_size x 1024\n        self.pi = Dense(self.action_size, activation=\'softmax\', name=\'pi\')(s_fc2)  # batch_size x self.action_size\n        self.v = Dense(1, activation=\'tanh\', name=\'v\')(s_fc2)  # batch_size x 1\n\n        self.model = Model(inputs=self.input_boards, outputs=[self.pi, self.v])\n        self.model.compile(loss=[\'categorical_crossentropy\', \'mean_squared_error\'], optimizer=Adam(CONFIG.nnet_args.lr))\n'"
rts/src/Board.py,0,"b'import sys\nfrom typing import Any\n\nimport numpy as np\n\nsys.path.append(\'../..\')\nfrom rts.src.config import d_a_type, d_acts, A_TYPE_IDX, P_NAME_IDX, CARRY_IDX, MONEY_IDX, NUM_ACTS, ACTS_REV, NUM_ENCODERS, HEALTH_IDX, TIME_IDX\n\n""""""\nBoard.py\n\nDefines game rules (action checking, end-game conditions)\ncan_execute_move is checking if move can be executed and execute_move is applying this move to new board\n""""""\n\n\nclass Board:\n\n    def __init__(self, n) -> None:\n        self.n = n\n        self.pieces = np.zeros((self.n, self.n, NUM_ENCODERS))\n\n    def __getitem__(self, index: int) -> np.array:\n        return self.pieces[index]\n\n    def execute_move(self, move, player) -> None:\n        """"""\n        Executes move on this board for specified player\n        :param move: (x, y, action_index), that define which action should be executed on which tile\n        :param player: int - player that is executing action\n        :return: /\n        """"""\n        from rts.src.config_class import CONFIG\n\n        if player == 1:\n            config = CONFIG.player1_config\n        else:\n            config = CONFIG.player2_config\n\n        x, y, action_index = move\n        act = ACTS_REV[action_index]\n        if act == ""idle"":\n            return\n        if act == ""up"":\n            new_x, new_y = x, y - 1\n            self._move(x, y, new_x, new_y)\n            return\n        if act == ""down"":\n            new_x, new_y = x, y + 1\n            self._move(x, y, new_x, new_y)\n            return\n        if act == ""right"":\n            new_x, new_y = x + 1, y\n            self._move(x, y, new_x, new_y)\n            return\n        if act == ""left"":\n            new_x, new_y = x - 1, y\n            self._move(x, y, new_x, new_y)\n            return\n        if act == ""mine_resources"":\n            self[x][y][CARRY_IDX] = 1\n            return\n        if act == ""return_resources"":\n            self[x][y][CARRY_IDX] = 0\n            self._update_money(player, config.MONEY_INC)\n            return\n        if act == ""attack_up"":\n            self._attack(x, y, x, y - 1, config=config)\n            return\n        if act == ""attack_down"":\n            self._attack(x, y, x, y + 1, config=config)\n            return\n        if act == ""attack_left"":\n            self._attack(x, y, x - 1, y, config=config)\n            return\n        if act == ""attack_right"":\n            self._attack(x, y, x + 1, y, config=config)\n            return\n\n        if act == ""heal_up"":\n            self._heal(x, y, x, y - 1, config=config)\n            return\n        if act == ""heal_down"":\n            self._heal(x, y, x, y + 1, config=config)\n            return\n        if act == ""heal_left"":\n            self._heal(x, y, x - 1, y, config=config)\n            return\n        if act == ""heal_right"":\n            self._heal(x, y, x + 1, y, config=config)\n            return\n\n        if act == ""npc_up"":\n            self._update_money(player, -config.a_cost[2])\n            self._spawn(x, y, x, y - 1, 2, config=config)\n            return\n        if act == ""npc_down"":\n            self._update_money(player, -config.a_cost[2])\n            self._spawn(x, y, x, y + 1, 2, config=config)\n            return\n        if act == ""npc_left"":\n            self._update_money(player, -config.a_cost[2])\n            self._spawn(x, y, x - 1, y, 2, config=config)\n            return\n        if act == ""npc_right"":\n            self._update_money(player, -config.a_cost[2])\n            self._spawn(x, y, x + 1, y, 2, config=config)\n            return\n\n        if act == ""barracks_up"":\n            self._update_money(player, -config.a_cost[3])\n            self._spawn(x, y, x, y - 1, 3, config=config)\n            return\n        if act == ""barracks_down"":\n            self._update_money(player, -config.a_cost[3])\n            self._spawn(x, y, x, y + 1, 3, config=config)\n            return\n        if act == ""barracks_left"":\n            self._update_money(player, -config.a_cost[3])\n            self._spawn(x, y, x - 1, y, 3, config=config)\n            return\n        if act == ""barracks_right"":\n            self._update_money(player, -config.a_cost[3])\n            self._spawn(x, y, x + 1, y, 3, config=config)\n            return\n\n        if act == ""rifle_infantry_up"":\n            self._update_money(player, -config.a_cost[4])\n            self._spawn(x, y, x, y - 1, 4, config=config)\n            return\n        if act == ""rifle_infantry_down"":\n            self._update_money(player, -config.a_cost[4])\n            self._spawn(x, y, x, y + 1, 4, config=config)\n            return\n        if act == ""rifle_infantry_left"":\n            self._update_money(player, -config.a_cost[4])\n            self._spawn(x, y, x - 1, y, 4, config=config)\n            return\n        if act == ""rifle_infantry_right"":\n            self._update_money(player, -config.a_cost[4])\n            self._spawn(x, y, x + 1, y, 4, config=config)\n            return\n\n        if act == ""town_hall_up"":\n            self._update_money(player, -config.a_cost[5])\n            self._spawn(x, y, x, y - 1, 5, config=config)\n            return\n        if act == ""town_hall_down"":\n            self._update_money(player, -config.a_cost[5])\n            self._spawn(x, y, x, y + 1, 5, config=config)\n            return\n        if act == ""town_hall_left"":\n            self._update_money(player, -config.a_cost[5])\n            self._spawn(x, y, x - 1, y, 5, config=config)\n            return\n        if act == ""town_hall_right"":\n            self._update_money(player, -config.a_cost[5])\n            self._spawn(x, y, x + 1, y, 5, config=config)\n            return\n\n    def _move(self, x, y, new_x, new_y):\n        """"""\n        Move actor to new location\n        :param x: int - coordinate x where actor is located\n        :param y: int - coordinate y where actor is located\n        :param new_x: int - coordinate x where actor needs to be moved to\n        :param new_y: int - coordinate y where actor needs to be moved to\n        """"""\n        self[new_x][new_y] = self[x][y]\n        self[x][y] = [0] * NUM_ENCODERS\n        self[x][y][TIME_IDX] = self[new_x][new_y][TIME_IDX]  # set time back to empty tile\n\n    def _update_money(self, player, money_update):\n        """"""\n        :param player: int - player to which money gets appended/ decreased\n        :param money_update: int - amount of money\n        """"""\n        for y in range(self.n):\n            for x in range(self.n):\n                if self[x][y][P_NAME_IDX] == player:\n                    assert self[x][y][MONEY_IDX] + money_update >= 0\n                    self[x][y][MONEY_IDX] = self[x][y][MONEY_IDX] + money_update\n\n    def _attack(self, x, y, n_x, n_y, config):\n        """"""\n        Actor attacks new actor on other coordinate\n        :param x: actor on coordinate x\n        :param y: actor on coordinate y\n        :param n_x: attack new actor on coordinate n_x\n        :param n_y: attack new actor on coordinate n_x\n        :param config: config that specifies damage - different config can be used for each player\n        """"""\n        self[n_x][n_y][HEALTH_IDX] -= config.DAMAGE\n        if self[n_x][n_y][HEALTH_IDX] <= 0:\n            self[n_x][n_y] = [0] * NUM_ENCODERS\n            self[n_x][n_y][TIME_IDX] = self[x][y][TIME_IDX]  # set time back to empty tile just in case\n\n    def _spawn(self, x, y, n_x, n_y, a_type, config):\n        """"""\n        Actor spawns actor on other coordinate\n        :param x: coordinate of building that is spawning new actor\n        :param y: coordinate of building that is spawning new actor\n        :param n_x: coordinate where new actor will spawn to\n        :param n_y: coordinate where new actor will spawn to\n        :param a_type: type of unit to spawn on new coordinate\n        :param config: additional config that is separate for each player (maximum actor health for this type)\n        """"""\n        self[n_x][n_y] = [self[x][y][P_NAME_IDX], a_type, config.a_max_health[a_type], 0, self[x][y][MONEY_IDX], self[x][y][TIME_IDX]]\n\n    def _heal(self, x, y, n_x, n_y, config):\n        """"""\n        Actor heals actor on other coordinate\n        :param x: coordinate of actor executing heal action\n        :param y: oordinate of actor executing heal action\n        :param n_x: coordinate of actor that will receive heal\n        :param n_y: coordinate of actor that will receive heal\n        :param config: additional config that is separate for each player (heal_cost, heal_amount, max_actor_health)\n        """"""\n        if config.SACRIFICIAL_HEAL:\n            self[x][x][HEALTH_IDX] -= config.HEAL_COST\n            if self[x][y][HEALTH_IDX] <= 0:\n                self[x][y] = [0] * NUM_ENCODERS\n                self[x][y][TIME_IDX] = self[x][y][TIME_IDX]\n        elif self[n_x][n_y][MONEY_IDX] - config.HEAL_AMOUNT >= 0:\n            self[n_x][n_y][HEALTH_IDX] += config.HEAL_AMOUNT\n            self._update_money(self[n_x][n_y][P_NAME_IDX], -config.HEAL_COST)\n\n        # clamp value to max\n        self[n_x][n_y][HEALTH_IDX] = self.clamp(self[n_x][n_y][HEALTH_IDX] + config.HEAL_AMOUNT, 0, config.a_max_health[self[n_x][n_y][A_TYPE_IDX]])\n\n    def get_moves_for_square(self, x, y, config) -> Any:\n        """"""\n        Returns all valid actions for specific tile\n        :param x: x coordinate of tile\n        :param y: y coordinate of tile\n        :param config: additional config that is separate for each player\n        :return: array of valid actions\n        """"""\n        # determine the color of the piece.\n        player = self[x][y][P_NAME_IDX]\n\n        if player == 0:\n            return None\n        a_type = self[x][y][A_TYPE_IDX]\n        acts = d_acts[a_type]\n        moves = [0] * NUM_ACTS\n        for i in range(NUM_ACTS):\n            act = ACTS_REV[i]\n\n            if act in acts:\n                # a is now string action\n                move = self._valid_act(x, y, act, config=config) * 1\n                if move:\n                    moves[i] = move\n        # return the generated move list\n        return moves\n\n    def _valid_act(self, x, y, act, config):\n        """"""\n        Returns true if action on specific tile is valid, false otherwise\n        :param x: tile x that action will be executing upon\n        :param y: tile y that action will be executing upon\n        :param act: str: action that will be executing on this tile\n        :param config: additional config that gets passed to functions\n        :return: true/false\n        """"""\n        money = self[x][y][MONEY_IDX]\n        if act == ""idle"":\n            return config.acts_enabled.idle\n        if act == ""up"":\n            return config.acts_enabled.up and self._check_if_empty(x, y - 1)\n        if act == ""down"":\n            return config.acts_enabled.down and self._check_if_empty(x, y + 1)\n        if act == ""right"":\n            return config.acts_enabled.right and self._check_if_empty(x + 1, y)\n        if act == ""left"":\n            return config.acts_enabled.left and self._check_if_empty(x - 1, y)\n\n        if act == ""mine_resources"":\n            return config.acts_enabled.mine_resources and self[x][y][CARRY_IDX] == 0 and self._check_if_nearby(x, y, d_a_type[\'Gold\'])\n        if act == ""return_resources"":\n            return config.acts_enabled.return_resources and self[x][y][CARRY_IDX] == 1 and self._check_if_nearby(x, y, d_a_type[\'Hall\'], check_friendly=True) and (config.MAX_GOLD >= self[x][y][MONEY_IDX] + config.MONEY_INC)\n\n        if act == ""attack_up"":\n            return config.acts_enabled.attack and self._check_if_attack(x, y, x, y - 1)\n        if act == ""attack_down"":\n            return config.acts_enabled.attack and self._check_if_attack(x, y, x, y + 1)\n        if act == ""attack_right"":\n            return config.acts_enabled.attack and self._check_if_attack(x, y, x + 1, y)\n        if act == ""attack_left"":\n            return config.acts_enabled.attack and self._check_if_attack(x, y, x - 1, y)\n\n        if act == ""heal_up"":\n            return config.acts_enabled.heal and self._check_if_heal(x, y - 1, config=config)\n        if act == ""heal_down"":\n            return config.acts_enabled.heal and self._check_if_heal(x, y + 1, config=config)\n        if act == ""heal_right"":\n            return config.acts_enabled.heal and self._check_if_heal(x + 1, y, config=config)\n        if act == ""heal_left"":\n            return config.acts_enabled.heal and self._check_if_heal(x - 1, y, config=config)\n\n        if act == ""npc_up"":\n            return config.acts_enabled.npc and config.a_cost[2] <= money and self._check_if_empty(x, y - 1)\n        if act == ""npc_down"":\n            return config.acts_enabled.npc and config.a_cost[2] <= money and self._check_if_empty(x, y + 1)\n        if act == ""npc_right"":\n            return config.acts_enabled.npc and config.a_cost[2] <= money and self._check_if_empty(x + 1, y)\n        if act == ""npc_left"":\n            return config.acts_enabled.npc and config.a_cost[2] <= money and self._check_if_empty(x - 1, y)\n\n        if act == ""barracks_up"":\n            return config.acts_enabled.barracks and config.a_cost[3] <= money and self._check_if_empty(x, y - 1)\n        if act == ""barracks_down"":\n            return config.acts_enabled.barracks and config.a_cost[3] <= money and self._check_if_empty(x, y + 1)\n        if act == ""barracks_right"":\n            return config.acts_enabled.barracks and config.a_cost[3] <= money and self._check_if_empty(x + 1, y)\n        if act == ""barracks_left"":\n            return config.acts_enabled.barracks and config.a_cost[3] <= money and self._check_if_empty(x - 1, y)\n\n        if act == ""rifle_infantry_up"":\n            return config.acts_enabled.rifle_infantry and config.a_cost[4] <= money and self._check_if_empty(x, y - 1)\n        if act == ""rifle_infantry_down"":\n            return config.acts_enabled.rifle_infantry and config.a_cost[4] <= money and self._check_if_empty(x, y + 1)\n        if act == ""rifle_infantry_right"":\n            return config.acts_enabled.rifle_infantry and config.a_cost[4] <= money and self._check_if_empty(x + 1, y)\n        if act == ""rifle_infantry_left"":\n            return config.acts_enabled.rifle_infantry and config.a_cost[4] <= money and self._check_if_empty(x - 1, y)\n\n        if act == ""town_hall_up"":\n            return config.acts_enabled.town_hall and config.a_cost[5] <= money and self._check_if_empty(x, y - 1)\n        if act == ""town_hall_down"":\n            return config.acts_enabled.town_hall and config.a_cost[5] <= money and self._check_if_empty(x, y + 1)\n        if act == ""town_hall_right"":\n            return config.acts_enabled.town_hall and config.a_cost[5] <= money and self._check_if_empty(x + 1, y)\n        if act == ""town_hall_left"":\n            return config.acts_enabled.town_hall and config.a_cost[5] <= money and self._check_if_empty(x - 1, y)\n        print(""Unrecognised action"", act)\n        sys.exit(0)\n\n    def _check_if_empty(self, x, y):\n        """"""\n        Checks if tile is empty\n        :param x: if x coordinate is empty\n        :param y: if y coordinate is empty\n        :return: true/false\n        """"""\n        # noinspection PyChainedComparisons\n        return self.n > x >= 0 and 0 <= y < self.n and self[x][y][P_NAME_IDX] == 0\n\n    def _check_if_attack(self, x, y, n_x, n_y):\n        """"""\n        Check if actor on x,y can attack actor on n_x,n_y\n        :param x: actor on coordinate x\n        :param y: actor on coordinate y\n        :param n_x: can attack actor on coordinate n_x\n        :param n_y: can attack actor on coordinate n_y\n        :return: true/false\n        """"""\n        return 0 <= n_x < self.n and 0 <= n_y < self.n and self[x][y][P_NAME_IDX] == -self[n_x][n_y][P_NAME_IDX] and self[n_x][n_y][A_TYPE_IDX] != d_a_type[\'Gold\']\n\n    def _check_if_heal(self, x, y, config):\n        """"""\n        Check if actor on x,y can be healed\n        :param x: coordinate of actor that is getting to be healed\n        :param y: coordinate of actor that is getting to be healed\n        :param config: special config specific for each player (max_health, heal_cost)\n        :return: true/false\n        """"""\n        return 0 <= x < self.n and 0 <= y < self.n and self[x][y][P_NAME_IDX] == self[x][y][P_NAME_IDX] and self[x][y][A_TYPE_IDX] != d_a_type[\'Gold\'] and self[x][y][A_TYPE_IDX] > 0 and self[x][y][HEALTH_IDX] < config.a_max_health[self[x][y][A_TYPE_IDX]] and (\n                config.SACRIFICIAL_HEAL or self[x][y][MONEY_IDX] - config.HEAL_COST >= 0)\n\n    def _check_if_nearby(self, x, y, a_type, check_friendly=False):\n        """"""\n        Checks if actor is nearby - friendly or foe\n        :param x: coordinate of current actor\n        :param y: coordinate of current actor\n        :param a_type: type of nearby actor\n        :param check_friendly: check if nearby actor should be friendly\n        :return: true/false\n        """"""\n        coordinates = [(x - 1, y + 1),\n                       (x, y + 1),\n                       (x + 1, y + 1),\n                       (x - 1, y),\n                       (x + 1, y),\n                       (x - 1, y - 1),\n                       (x, y - 1),\n                       (x + 1, y - 1)]\n        for n_x, n_y in coordinates:\n            if 0 <= n_x < self.n and 0 <= n_y < self.n:\n                if self[n_x][n_y][A_TYPE_IDX] == a_type:\n                    if not check_friendly:\n                        return True\n                    if self[n_x][n_y][P_NAME_IDX] == self[x][y][P_NAME_IDX]:\n                        return True\n        return False\n\n    @staticmethod\n    def _num_destroys(time):\n        """"""\n        Defines parameter in time_killer function, how many actors should be damaged each round\n        :param time: current time frame\n        :return: amount of damaged actors\n        """"""\n        return int((time / 256) ** 2 + 1)\n\n    @staticmethod\n    def _damage(time):\n        """"""\n        Defines parameter in time_killer function, how much each damaged actor should receive damage\n        :param time: current time frame\n        :return: damage to each actor\n        """"""\n        return int((time / 8) ** 2.718 / (time * 8))\n\n    def time_killer(self, player):\n        """"""\n        Additional function that can be used to stop players from looping in game. It is defined using _damage and _num_destroys functions, which define how much and how many actors should be damaged each round.\n        This prevents players playing games too long, as its starting to kill them. Better players should gather more gold and therefore create more advanced actors prolonging their lives.\n        :param player: which player is currently executing action\n        :return: /\n        """"""\n        # I can pass player through, because this board is canonical board that this action gets executed upon\n\n        current_time = self[0][0][TIME_IDX]\n\n        destroys_per_round = self._num_destroys(current_time)\n        damage_amount = self._damage(current_time)\n\n        # Damage as many actors as ""damage_amount"" parameter provides\n        currently_damaged_actors = 0\n        for y in range(self.n):\n            for x in range(self.n):\n                if self[x][y][P_NAME_IDX] == player and self[x][y][A_TYPE_IDX] != 1:  # for current player and not gold\n                    if currently_damaged_actors >= destroys_per_round:\n                        return\n                    self[x][y][HEALTH_IDX] -= damage_amount\n\n                    if self[x][y][HEALTH_IDX] <= 0:\n                        time = self[x][y][TIME_IDX]\n                        self[x][y] = [0] * NUM_ENCODERS\n                        self[x][y][TIME_IDX] = time\n                    currently_damaged_actors += 1\n\n    @staticmethod\n    def clamp(num, min_value, max_value):\n        return max(min(num, max_value), min_value)\n\n    def get_money_score(self, player) -> int:\n        """"""\n        1. of 3 functions that define elo rating of specified player. This one takes into account only players money count\n        :param player: player that requires to know his money count\n        :return: money count for specified player\n        """"""\n        return sum([self[x][y][MONEY_IDX] for x in range(self.n) for y in range(self.n) if self[x][y][P_NAME_IDX] == player])\n\n    def get_health_score(self, player) -> int:\n        """"""\n        2. of 3 functions that define elo rating for specified player. This one takes into account only total current health of units. Players with more units, which have more health should win.\n        :param player: player that requires to know sum of health for his units\n        :return: sum of health for specified player\n        """"""\n        return sum([self[x][y][HEALTH_IDX] for x in range(self.n) for y in range(self.n) if self[x][y][P_NAME_IDX] == player])\n\n    def get_combined_score(self, player) -> int:\n        """"""\n        3. of 3 functions that define elo rating for specified player. This takes into account both 1. and 2. functions and joins them together.\n        :param player: player that requires to know his money count + sum of health of his units\n        :return: count of money + sum of health of specified players\' units\n        """"""\n        # money is not worth more than 1hp because this forces players to spend money in order to create new units\n        return sum([self[x][y][HEALTH_IDX] + self[x][y][MONEY_IDX] for x in range(self.n) for y in range(self.n) if self[x][y][P_NAME_IDX] == player])\n'"
rts/src/config.py,0,"b'import os\nimport sys\nfrom typing import List, Tuple\n\nimport numpy as np\n\nsys.path.append(\'../..\')\nfrom rts.src.encoders import OneHotEncoder, NumericEncoder\nfrom utils import dotdict\n\n# ####################################################################################\n# ###################### INITIAL CONFIGS AND OUTPUTS ##################################\n# ####################################################################################\n\n# specifically choose TF cpu if needed. This will have no effect if GPU is not present\nUSE_TF_CPU = False\n\n# helper path so model weights are imported and exported correctly when transferring project\nPATH = os.path.dirname(os.path.realpath(__file__))\n\n# Show initial TF configuration when TF is getting initialized\nSHOW_TENSORFLOW_GPU = True\n\n# Show initial Pygame welcome message when Pygame is getting initialized\nSHOW_PYGAME_WELCOME = False\n\n# If keras should output while fitting data\nVERBOSE_MODEL_FIT = 0\n\n# Maximum number of fps Pygame will render game at. Only relevant when running with verbose > 3\nFPS = 1000\n\n# ##################################\n# ########### ENCODERS #############\n# ##################################\n\n# Defining number of encoders\nNUM_ENCODERS = 6  # player_name, act_type, health, carrying, money, remaining_time\n\n# Setting indexes to each encoder\nP_NAME_IDX = 0\nA_TYPE_IDX = 1\nHEALTH_IDX = 2\nCARRY_IDX = 3\nMONEY_IDX = 4\nTIME_IDX = 5\n\n# ##################################\n# ########### ACTORS ###############\n# ##################################\n\n\n# Dictionary for actors\nd_a_type = dotdict({\n    \'Gold\': 1,\n    \'Work\': 2,\n    \'Barr\': 3,\n    \'Rifl\': 4,\n    \'Hall\': 5,\n})\n\n# Reverse dictionary for actors\nd_type_rev = dotdict({\n    1: \'Gold\',\n    2: \'Work\',\n    3: \'Barr\',\n    4: \'Rifl\',\n    5: \'Hall\',\n})\n\n# ##################################\n# ########## ACTIONS ###############\n# ##################################\n\n# Dictionary for actions and which actor can execute them\nd_acts = dotdict({\n    1: [],  # Gold\n    2: [\'up\', \'down\', \'left\', \'right\', \'mine_resources\', \'return_resources\', \'barracks_up\', \'barracks_down\', \'barracks_right\', \'barracks_left\', \'town_hall_up\', \'town_hall_down\', \'town_hall_right\', \'town_hall_left\', \'idle\', \'heal_up\', \'heal_down\', \'heal_right\', \'heal_left\'],  # Work\n    3: [\'rifle_infantry_up\', \'rifle_infantry_down\', \'rifle_infantry_right\', \'rifle_infantry_left\', \'idle\', \'heal_up\', \'heal_down\', \'heal_right\', \'heal_left\'],  # Barr\n    4: [\'up\', \'down\', \'left\', \'right\', \'attack_up\', \'attack_down\', \'attack_right\', \'attack_left\', \'idle\', \'heal_up\', \'heal_down\', \'heal_right\', \'heal_left\'],  # Rifl\n    5: [\'npc_up\', \'npc_down\', \'npc_right\', \'npc_left\', \'idle\', \'heal_up\', \'heal_down\', \'heal_right\', \'heal_left\'],  # Hall\n})\n\n# Reverse dictionary for actions\nd_acts_int = dotdict({\n    1: [],  # Gold\n    2: [1, 2, 3, 4, 5, 6, 19, 20, 21, 22, 23, 24, 25, 26, 0, 27, 28, 29, 30],  # Work\n    3: [15, 16, 17, 18, 0, 27, 28, 29, 30],  # Barr\n    4: [1, 2, 3, 4, 7, 8, 9, 10, 0, 27, 28, 29, 30],  # Rifl\n    5: [11, 12, 13, 14, 0, 27, 28, 29, 30],  # Hall\n})\n\n# Defining all actions\nACTS = {\n    ""idle"": 0,\n\n    ""up"": 1,\n    ""down"": 2,\n    ""right"": 3,\n    ""left"": 4,\n\n    ""mine_resources"": 5,\n    ""return_resources"": 6,\n\n    ""attack_up"": 7,\n    ""attack_down"": 8,\n    ""attack_right"": 9,\n    ""attack_left"": 10,\n\n    ""npc_up"": 11,\n    ""npc_down"": 12,\n    ""npc_right"": 13,\n    ""npc_left"": 14,\n\n    ""rifle_infantry_up"": 15,\n    ""rifle_infantry_down"": 16,\n    ""rifle_infantry_right"": 17,\n    ""rifle_infantry_left"": 18,\n\n    ""barracks_up"": 19,\n    ""barracks_down"": 20,\n    ""barracks_right"": 21,\n    ""barracks_left"": 22,\n\n    ""town_hall_up"": 23,\n    ""town_hall_down"": 24,\n    ""town_hall_right"": 25,\n    ""town_hall_left"": 26,\n\n    ""heal_up"": 27,\n    ""heal_down"": 28,\n    ""heal_right"": 29,\n    ""heal_left"": 30\n\n}\n\n# Reverse dictionary for all actions\nACTS_REV = {\n    0: ""idle"",\n\n    1: ""up"",\n    2: ""down"",\n    3: ""right"",\n    4: ""left"",\n\n    5: ""mine_resources"",\n    6: ""return_resources"",\n\n    7: ""attack_up"",\n    8: ""attack_down"",\n    9: ""attack_right"",\n    10: ""attack_left"",\n\n    11: ""npc_up"",\n    12: ""npc_down"",\n    13: ""npc_right"",\n    14: ""npc_left"",\n\n    15: ""rifle_infantry_up"",\n    16: ""rifle_infantry_down"",\n    17: ""rifle_infantry_right"",\n    18: ""rifle_infantry_left"",\n\n    19: ""barracks_up"",\n    20: ""barracks_down"",\n    21: ""barracks_right"",\n    22: ""barracks_left"",\n\n    23: ""town_hall_up"",\n    24: ""town_hall_down"",\n    25: ""town_hall_right"",\n    26: ""town_hall_left"",\n\n    27: ""heal_up"",\n    28: ""heal_down"",\n    29: ""heal_right"",\n    30: ""heal_left""\n}\n\n# Count of all actions\nNUM_ACTS = len(ACTS)\n\n# ####################################################################################\n# ################################## PLAYING #########################################\n# ####################################################################################\n\n# User shortcuts that player can use using Pygame\nd_user_shortcuts = dotdict({\n    \' \': 0,  # idle\n    \'w\': 1,  # up\n    \'s\': 2,  # down\n    \'d\': 3,  # right\n    \'a\': 4,  # left\n    \'q\': 5,  # mine_resources\n    \'e\': 6,  # return_resources\n    \'1\': 7,  # attack_up\n    \'2\': 8,  # attack_down\n    \'3\': 9,  # attack_right\n    \'4\': 10,  # attack_left\n    \'6\': 11,  # npc_up\n    \'7\': 12,  # npc_down\n    \'8\': 13,  # npc_right\n    \'9\': 14,  # npc_left\n    \'t\': 15,  # rifle_infantry_up\n    \'z\': 16,  # rifle_infantry_down\n    \'u\': 17,  # rifle_infantry_right\n    \'i\': 18,  # rifle_infantry_left\n    \'f\': 19,  # barracks_up\n    \'g\': 20,  # barracks_down\n    \'h\': 21,  # barracks_right\n    \'j\': 22,  # barracks_left\n    \'y\': 23,  # town_hall_up\n    \'x\': 24,  # town_hall_down\n    \'c\': 25,  # town_hall_right\n    \'v\': 26,  # town_hall_left\n    \'b\': 27,  # heal_up\n    \'n\': 28,  # heal_down\n    \'m\': 29,  # heal_right\n    \',\': 30,  # heal_left\n})\n\n# Reverse dictionary for user shortcuts\nd_user_shortcuts_rev = dotdict({\n    0: \' \',  # idle\n\n    1: \'w\',  # up\n    2: \'s\',  # down\n    3: \'d\',  # right\n    4: \'a\',  # left\n\n    5: \'q\',  # mine_resources\n    6: \'e\',  # return_resources\n\n    7: \'1\',  # attack_up\n    8: \'2\',  # attack_down\n    9: \'3\',  # attack_right\n    10: \'4\',  # attack_left\n\n    11: \'6\',  # npc_up\n    12: \'7\',  # npc_down\n    13: \'8\',  # npc_right\n    14: \'9\',  # npc_left\n\n    15: \'t\',  # rifle_infantry_up\n    16: \'z\',  # rifle_infantry_down\n    17: \'u\',  # rifle_infantry_right\n    18: \'i\',  # rifle_infantry_left\n\n    19: \'f\',  # barracks_up\n    20: \'g\',  # barracks_down\n    21: \'h\',  # barracks_right\n    22: \'j\',  # barracks_left\n\n    23: \'y\',  # town_hall_up\n    24: \'x\',  # town_hall_down\n    25: \'c\',  # town_hall_right\n    26: \'v\',  # town_hall_left\n\n    27: \'b\',  # heal_up\n    28: \'n\',  # heal_down\n    29: \'m\',  # heal_right\n    30: \',\',  # heal_left\n})\n\n# Colors of actors displayed in Pygame\nd_a_color = dotdict({\n    1: (230, 0, 50),  # Gold\n    2: (0, 165, 208),  # Work\n    3: (255, 156, 255),  # Barr\n    4: (152, 0, 136),  # Rifl\n    5: (235, 255, 0),  # Hall\n})\n\n\nclass Configuration:\n    class _NNetArgs:\n        def __init__(self,\n                     use_one_hot_encoder,\n                     lr,\n                     dropout,\n                     epochs,\n                     batch_size,\n                     cuda,\n                     num_channels):\n\n            self.lr = lr  # learning rate\n            self.dropout = dropout\n            self.epochs = epochs  # times training examples are iterated through learning process\n            self.batch_size = batch_size  # how many train examples are taken together for learning\n            self.cuda = cuda  # this is only relevant when using TF GPU\n            self.num_channels = num_channels  # used by nnet conv layers\n\n            # Should one-hot encoder be used (recommended)\n            if use_one_hot_encoder:\n                self.encoder = OneHotEncoder()\n            else:\n                self.encoder = NumericEncoder()\n\n    class _GameConfig:\n        def __init__(self,\n                     onehot_encoder,\n                     money_increment,\n                     initial_gold,\n                     maximum_gold,\n                     sacrificial_heal,\n                     heal_amount,\n                     heal_cost,\n                     use_timeout,\n                     max_time,\n                     damage,\n                     destroy_all,\n                     a_max_health,\n                     a_cost,\n                     acts_enabled,\n                     score_function,\n                     timeout):\n\n            if onehot_encoder:\n                self.encoder = OneHotEncoder()\n            else:\n                self.encoder = NumericEncoder()\n\n            # ##################################\n            # ############# GOLD ###############\n            # ##################################\n\n            # how much money is returned when returned resources\n            self.MONEY_INC = money_increment\n\n            # how much initial gold do players get at game begining\n            self.INITIAL_GOLD = initial_gold\n\n            # Maximum gold that players can have - It is limited to 8 bits for one-hot onehot_encoder\n            self.MAX_GOLD = maximum_gold\n\n            # ##################################\n            # ############# HEAL ###############\n            # ##################################\n\n            # Game mechanic where actors can damage themselves to heal friendly unit. This is only used when player doesn\'t have any money to pay for heal action\n            self.SACRIFICIAL_HEAL = sacrificial_heal\n\n            # How much friendly unit is healed when executing heal action\n            self.HEAL_AMOUNT = heal_amount\n            # how much money should player pay when heal action is getting executed.\n\n            self.HEAL_COST = heal_cost\n\n            # ##################################\n            # ########### TIMEOUT ##############\n            # ##################################\n\n            # this gets used by kill function that determines the end point\n            self.MAX_TIME = max_time\n\n            # If timeout should be used. This causes game to finish after TIMEOUT number of actions. If timeout isnt used, Kill function is used, which is reducing number of hitpoints of units\n            self.USE_TIMEOUT = use_timeout\n\n            # Check if timeout is being used. Alternatively Kill function is used\n            if self.USE_TIMEOUT:\n                # how many turns until game end - this gets reduced when each turn is executed\n                self.TIMEOUT = timeout\n            else:\n                # sets initial tick to 0 and then in getGameEnded it gets incremented unitl number 8191\n                self.TIMEOUT = 0\n            # ##################################\n            # ########## ATTACKING #############\n            # ##################################\n\n            # how much damage is dealt to attacked actor\n            self.DAMAGE = damage\n            # when attacking, all enemy units are destroyed, resulting in victory for the attacking player\n            if destroy_all:\n                self.DAMAGE = 10000\n\n            # Maximum health that actor can have - this is also initial health that actor has.\n            self.a_max_health = dotdict(a_max_health or {\n                1: 10,  # Gold\n                2: 10,  # Work\n                3: 20,  # Barr\n                4: 20,  # Rifl\n                5: 30,  # Hall\n            })\n\n            # Cost of actor to produce (key - actor type, value - number of gold coins to pay)\n            self.a_cost = dotdict(a_cost or {\n                1: 0,  # Gold\n                2: 1,  # Work\n                3: 4,  # Barr\n                4: 2,  # Rifl\n                5: 7,  # Hall\n            })\n            self.acts_enabled = dotdict(acts_enabled or {\n                ""idle"": False,\n                ""up"": True,\n                ""down"": True,\n                ""right"": True,\n                ""left"": True,\n                ""mine_resources"": True,\n                ""return_resources"": True,\n                ""attack"": True,\n                ""npc"": True,\n                ""rifle_infantry"": True,\n                ""barracks"": True,\n                ""town_hall"": True,\n                ""heal"": True\n            })\n            self.score_function = score_function\n\n    class _PitArgs:\n\n        def __init__(self,\n                     player1_type,\n                     player2_type,\n                     player1_config,\n                     player2_config,\n                     player1_onehot_encoder,\n                     player2_onehot_encoder,\n                     player1_model_file,\n                     player2_model_file,\n                     num_games):\n\n            self.player1_type = player1_type\n            self.player2_type = player2_type\n            self.player1_model_file = player1_model_file\n            self.player2_model_file = player2_model_file\n            self.player1_onehot_encoder = player1_onehot_encoder\n            self.player2_onehot_encoder = player2_onehot_encoder\n            self.player1_config = player1_config or {\'numMCTSSims\': 2, \'cpuct\': 1.0}\n            self.player2_config = player2_config or {\'numMCTSSims\': 2, \'cpuct\': 1.0}\n            self.num_games = num_games\n\n        def create_players(self,\n                           game):\n\n            return self._create_player(game, self.player1_type, self.player1_config, self.player1_onehot_encoder, self.player1_model_file), self._create_player(game, self.player1_type, self.player1_config, self.player2_onehot_encoder, self.player2_model_file)\n\n        def _create_player(self,\n                           game,\n                           player_type: str,\n                           player_config: dict,\n                           onehot_encoder: bool,\n                           player_model_file: str):\n            from rts.RTSPlayers import RandomPlayer, GreedyRTSPlayer, HumanRTSPlayer\n\n            if player_type == \'nnet\':\n                if player_config is None:\n                    print(""Invalid pit configuration. Returning"")\n                    exit(1)\n                return self._PitNNetPlayer(game, player_config, onehot_encoder, player_model_file).play\n            if player_type == \'random\':\n                return RandomPlayer(game).play\n            if player_type == \'greedy\':\n                return GreedyRTSPlayer(game).play\n            if player_type == \'human\':\n                return HumanRTSPlayer(game).play\n            print(""Invalid player type. Returning"")\n            exit(1)\n\n        class _PitNNetPlayer:\n            def __init__(self,\n                         g,\n                         player_config,\n                         onehot_encoder,\n                         player_model_file):\n                from rts.keras.NNet import NNetWrapper as NNet\n                from MCTS import MCTS\n\n                if onehot_encoder:\n                    encoder = OneHotEncoder()\n                else:\n                    encoder = NumericEncoder()\n                n1 = NNet(g, encoder)\n                n1.load_checkpoint(\'.\\\\..\\\\temp\\\\\', player_model_file)\n                args1 = dotdict(player_config or {\'numMCTSSims\': 2, \'cpuct\': 1.0})\n                mcts1 = MCTS(g, n1, args1)\n                self.play = lambda x: np.argmax(mcts1.getActionProb(x, temp=0))\n\n    class _LearnArgs:\n        def __init__(self,\n                     num_iters,\n                     num_eps,\n                     temp_threshold,\n                     update_threshold,\n                     maxlen_of_queue,\n                     num_mcts_sims,\n                     arena_compare,\n                     cpuct,\n                     checkpoint,\n                     load_model,\n                     load_folder_file,\n                     num_iters_for_train_examples_history,\n                     save_train_examples,\n                     load_train_examples):\n            self.numIters = num_iters  # total number of games played from start to finish is numIters * numEps\n            self.numEps = num_eps  # How may game is played in this episode\n            self.tempThreshold = temp_threshold\n            self.updateThreshold = update_threshold  # Percentage that new model has to surpass by win rate to replace old model\n            self.maxlenOfQueue = maxlen_of_queue\n            self.numMCTSSims = num_mcts_sims  # How many MCTS tree searches are performing (mind that this MCTS doesnt use simulations)\n            self.arenaCompare = arena_compare  # How many comparisons are made between old and new model\n            self.cpuct = cpuct  # search parameter for MCTS\n\n            self.checkpoint = checkpoint\n            self.load_model = load_model  # Load training examples from file - WARNING - this is disabled in RTSPlayers.py because of memory errors received when loading data from file\n            self.load_folder_file = load_folder_file\n            self.numItersForTrainExamplesHistory = num_iters_for_train_examples_history  # maximum number of \'iterations\' that game episodes are kept in queue. After that last is popped and new one is added.\n\n            self.save_train_examples = save_train_examples\n            self.load_train_examples = load_train_examples\n\n    class BoardTile:\n        def __init__(self,\n                     player: int,\n                     x: int,\n                     y: int,\n                     a_type: str):\n            self.player = player\n            self.x = x\n            self.y = y\n            self.a_type = a_type  # \'Gold\'...\n\n    def __init__(self,\n                 grid_size=8,\n                 learn_visibility=0,\n                 pit_visibility=4,\n\n                 onehot_encoder_player1: bool = True,\n                 money_increment_player1: int = 3,\n                 initial_gold_player1: int = 1,\n                 maximum_gold_player1: int = 255,\n                 sacrificial_heal_player1: bool = False,\n                 heal_amount_player1: int = 5,\n                 heal_cost_player1: int = 1,\n                 use_timeout_player1: bool = True,\n                 max_time_player1: int = 2048,\n                 damage_player1: int = 20,\n                 destroy_all_player1: bool = False,\n                 a_max_health_player1: dict = None,\n                 a_cost_player1: dict = None,\n                 acts_enabled_player1: dict = None,\n                 score_function_player1: int = 3,\n                 timeout_player1: int = 200,\n                 player1_model_file: str = ""best_player1.pth.tar"",\n\n                 onehot_encoder_player2: bool = True,\n                 money_increment_player2: int = 3,\n                 initial_gold_player2: int = 1,\n                 maximum_gold_player2: int = 255,\n                 sacrificial_heal_player2: bool = False,\n                 heal_amount_player2: int = 5,\n                 heal_cost_player2: int = 1,\n                 use_timeout_player2: bool = True,\n                 max_time_player2: int = 2048,\n                 damage_player2: int = 20,\n                 destroy_all_player2: bool = False,\n                 a_max_health_player2: dict = None,\n                 a_cost_player2: dict = None,\n                 acts_enabled_player2: dict = None,\n                 score_function_player2: int = 3,\n                 timeout_player2: int = 200,\n                 player2_model_file: str = ""best_player2.pth.tar"",\n\n                 num_iters: int = 4,\n                 num_eps: int = 4,\n                 temp_threshold: int = 15,\n                 update_threshold: float = 0.6,\n                 maxlen_of_queue: int = 6400,\n                 num_mcts_sims: int = 10,\n                 arena_compare: int = 10,\n                 cpuct: float = 1,\n                 checkpoint: str = \'.\\\\..\\\\temp\\\\\',\n                 load_model: bool = False,\n                 load_folder_file: Tuple[str, str] = (\'.\\\\..\\\\temp\\\\\', \'checkpoint_13.pth.tar\'),\n                 num_iters_for_train_examples_history: int = 8,\n                 save_train_examples: bool = False,\n                 load_train_examples: bool = False,\n\n                 player1_type: str = \'nnet\',\n                 player2_type: str = \'nnet\',\n                 player1_config: dict = None,\n                 player2_config: dict = None,\n                 num_games: int = 4,\n\n                 use_one_hot_encoder: bool = True,\n                 lr: float = 0.01,\n                 dropout: float = 0.3,\n                 epochs: int = 30,\n                 batch_size: int = 256,\n                 cuda: bool = True,\n                 num_channels: int = 128,\n\n                 initial_board_config: List[BoardTile] = None):\n        """"""\n        :param grid_size: Grid size of game for example 8,6...\n        :param learn_visibility: How much console should output while running learn. If visibility.verbose > 3, Pygame is shown\n        :param pit_visibility: How much console should output while running pit. If visibility.verbose > 3, Pygame is shown\n\n        :param onehot_encoder_player1: Which encoder should this player use while pitting\n        :param money_increment_player1: How much money player should gain when worker returns gold coins\n        :param initial_gold_player1: How much initial gold should player have\n        :param maximum_gold_player1: Maximum gold for player (max allowed value is 255)\n        :param sacrificial_heal_player1: If actors can sacrifice their health to heal other actors if player doesn\'t have enough gold\n        :param heal_amount_player1: how much should action \'heal\' heal other actor\n        :param heal_cost_player1: how much should action \'heal\' cost gold coins. If sacrificial_heal is enabled, this is the amount that actors health will be reduced if player doesn\'t have enough gold\n        :param use_timeout_player1: If timeout function should be used. If false, \'kill function\' will be used\n        :param max_time_player1: Maximum amount of time after which game ends and score is evaluated\n        :param damage_player1: How much damage is inflicted upon action \'attack\' on other actor\n        :param destroy_all_player1: If by executing action \'attack\', all opponents actors are destroyed\n        :param a_max_health_player1: dictionary of maximum amount of healths for each actor. See its default values to override\n            ``\n            Example: {\n                1: 10,  # Gold\n                2: 10,  # Work\n                3: 20,  # Barr\n                4: 20,  # Rifl\n                5: 30,  # Hall\n            }\n            ``\n        :param a_cost_player1: dictionary of costs for each actor. See its default values to override\n            ``\n            Example: {\n                1: 0,  # Gold\n                2: 1,  # Work\n                3: 4,  # Barr\n                4: 2,  # Rifl\n                5: 7,  # Hall\n            }\n            ``\n        :param acts_enabled_player1: dictionary of which actions are enabled for player. See its default values to override.\n            ``\n            Example: {\n                ""idle"": False,\n                ""up"": True,\n                ""down"": True,\n                ""right"": True,\n                ""left"": True,\n                ""mine_resources"": True,\n                ""return_resources"": True,\n                ""attack"": True,\n                ""npc"": True,\n                ""rifle_infantry"": True,\n                ""barracks"": True,\n                ""town_hall"": True,\n                ""heal"": True\n            }\n            ``\n        :param score_function_player1: which function to use (1, 2 or 3)\n        :param timeout_player1: After what time game will timeout if \'useTimeout\' is set to true\n        :param player1_model_file: Filename in temp folder that player 1 nnet player uses\n\n        :param onehot_encoder_player2: Which encoder should this player use while pitting\n        :param money_increment_player2: How much money player should gain when worker returns gold coins\n        :param initial_gold_player2: How much initial gold should player have\n        :param maximum_gold_player2: Maximum gold for player (max allowed value is 255)\n        :param sacrificial_heal_player2: If actors can sacrifice their health to heal other actors if player doesn\'t have enough gold\n        :param heal_amount_player2: how much should action \'heal\' heal other actor\n        :param heal_cost_player2: how much should action \'heal\' cost gold coins. If sacrificial_heal is enabled, this is the amount that actors health will be reduced if player doesn\'t have enough gold\n        :param use_timeout_player2: If timeout function should be used. If false, \'kill function\' will be used\n        :param max_time_player2: Maximum amount of time after which game ends and score is evaluated\n        :param damage_player2: How much damage is inflicted upon action \'attack\' on other actor\n        :param destroy_all_player2: If by executing action \'attack\', all opponents actors are destroyed\n        :param a_max_health_player2: dictionary of maximum amout of healths for each actor. See its default values to override\n            ``\n            Example: {\n                1: 10,  # Gold\n                2: 10,  # Work\n                3: 20,  # Barr\n                4: 20,  # Rifl\n                5: 30,  # Hall\n            }\n            ``\n        :param a_cost_player2: dictionary of costs for each actor. See its default values to override\n            ``\n            Example: {\n                1: 0,  # Gold\n                2: 1,  # Work\n                3: 4,  # Barr\n                4: 2,  # Rifl\n                5: 7,  # Hall\n            }\n            ``\n        :param acts_enabled_player2: dictionary of which actions are enabled for player. See its default values to override\n            ``\n            Example: {\n                ""idle"": False,\n                ""up"": True,\n                ""down"": True,\n                ""right"": True,\n                ""left"": True,\n                ""mine_resources"": True,\n                ""return_resources"": True,\n                ""attack"": True,\n                ""npc"": True,\n                ""rifle_infantry"": True,\n                ""barracks"": True,\n                ""town_hall"": True,\n                ""heal"": True\n            }\n            ``\n        :param score_function_player2: which function to use (1, 2 or 3)\n        :param timeout_player2: After what time game will timeout if \'useTimeout\' is set to true\n        :param player2_model_file: Filename in temp folder that player 2 nnet player uses\n\n\n        :param num_iters: How many iterations of games it should be played\n        :param num_eps: How many episodes in each game iteration it should be played\n        :param temp_threshold: Used by coach. ""It uses a temp=1 if episodeStep < tempThreshold, and thereafter uses temp=0.""\n        :param update_threshold: Percentage of how much wins should newer model have to be accepted\n        :param maxlen_of_queue: How many train examples can be stored in each iteration\n        :param num_mcts_sims: How many MCTS sims are executed in each game episode while learning\n        :param arena_compare: How many comparations of newer and older model should be made before evaluating which is better\n        :param cpuct: Exploration parameter for MCTS\n        :param checkpoint: folder where checkpoints should be saved while learning\n        :param load_model: If model is loaded from checkpoint on learning start\n        :param load_folder_file: tuple(folder, file) where model is loaded from\n        :param num_iters_for_train_examples_history: How many iterations of train examples should be kept for learning. If this number is exceeded, oldest iteration of train exaples is removed from queue\n        :param save_train_examples: If train examples should be saved to file (Caution if choosing this, because of memory error)\n        :param load_train_examples: If train examples should be loaded from file (Caution if choosing this, because of memory error)\n\n        :param player1_type: What type should player 1 be (""nnet"", ""random"", ""greedy"", ""human"")\n        :param player2_type: What type should player 2 be (""nnet"", ""random"", ""greedy"", ""human"")\n        :param player1_config: If ""nnet"" player is chosen, config can be provided {\'numMCTSSims\': 2, \'cpuct\': 1.0}\n        :param player2_config: If ""nnet"" player is chosen, config can be provided {\'numMCTSSims\': 2, \'cpuct\': 1.0}\n        :param num_games: How many games should be played for pit config\n\n        :param use_one_hot_encoder: If oneHot encoder should be used for both players while learning. (While pitting see configs encoder_player1, onehot_encoder_player2)\n        :param lr: Learning rate of model\n        :param dropout: Dropout in NNet Model config\n        :param epochs: How many epochs should learning take\n        :param batch_size: How big batches of learning examples there should be while learning\n        :param cuda: Whether to use cuda if tensorflow gpu is installed and GPU supports cuda operations\n        :param num_channels: Number of channels in NNet Model config\n\n        :param initial_board_config: Configuration of initial non-empty tiles for actors. See its default values to override.\n            ``Example: initial_board_config=[\n                Configuration.BoardTile(1,4,4,\'Gold\'),\n                Configuration.BoardTile(-1,4,5,\'Gold\'),\n                Configuration.BoardTile(1,5,4,\'Hall\'),\n                Configuration.BoardTile(-1,5,5,\'Hall\')]\n            ``\n        """"""\n\n        # output for game stats during playing games (game_episode, game iteration, player name, action executed, action_name, action_direction, player_score...\n        self.config_file_pit = "".\\\\..\\\\temp\\\\config_pit.csv""\n        self.config_file_learn = "".\\\\..\\\\temp\\\\config_learn.csv""\n\n        self.grid_size = grid_size\n\n        self.visibility = 4\n        self._pit_visibility = pit_visibility\n        self._learn_visibility = learn_visibility\n\n        self.player1_config = self._GameConfig(\n            onehot_encoder=onehot_encoder_player1,\n            money_increment=money_increment_player1,\n            initial_gold=initial_gold_player1,\n            maximum_gold=maximum_gold_player1,\n            sacrificial_heal=sacrificial_heal_player1,\n            heal_amount=heal_amount_player1,\n            heal_cost=heal_cost_player1,\n            use_timeout=use_timeout_player1,\n            max_time=max_time_player1,\n            damage=damage_player1,\n            destroy_all=destroy_all_player1,\n            a_max_health=a_max_health_player1,\n            a_cost=a_cost_player1,\n            acts_enabled=acts_enabled_player1,\n            score_function=score_function_player1,\n            timeout=timeout_player1)\n\n        self.player2_config = self._GameConfig(\n            onehot_encoder=onehot_encoder_player2,\n            money_increment=money_increment_player2,\n            initial_gold=initial_gold_player2,\n            maximum_gold=maximum_gold_player2,\n            sacrificial_heal=sacrificial_heal_player2,\n            heal_amount=heal_amount_player2,\n            heal_cost=heal_cost_player2,\n            use_timeout=use_timeout_player2,\n            max_time=max_time_player2,\n            damage=damage_player2,\n            destroy_all=destroy_all_player2,\n            a_max_health=a_max_health_player2,\n            a_cost=a_cost_player2,\n            acts_enabled=acts_enabled_player2,\n            score_function=score_function_player2,\n            timeout=timeout_player2)\n\n        self.learn_args = self._LearnArgs(\n            num_iters=num_iters,\n            num_eps=num_eps,\n            temp_threshold=temp_threshold,\n            update_threshold=update_threshold,\n            maxlen_of_queue=maxlen_of_queue,\n            num_mcts_sims=num_mcts_sims,\n            arena_compare=arena_compare,\n            cpuct=cpuct,\n            checkpoint=checkpoint,\n            load_model=load_model,\n            load_folder_file=load_folder_file,\n            num_iters_for_train_examples_history=num_iters_for_train_examples_history,\n            save_train_examples=save_train_examples,\n            load_train_examples=load_train_examples)\n\n        self.pit_args = self._PitArgs(\n            player1_type=player1_type,\n            player2_type=player2_type,\n            player1_config=player1_config,\n            player2_config=player2_config,\n            player1_onehot_encoder=onehot_encoder_player1,\n            player2_onehot_encoder=onehot_encoder_player2,\n            player1_model_file=player1_model_file,\n            player2_model_file=player2_model_file,\n            num_games=num_games\n        )\n        self.nnet_args = self._NNetArgs(\n            use_one_hot_encoder=use_one_hot_encoder,\n            lr=lr,\n            dropout=dropout,\n            epochs=epochs,\n            batch_size=batch_size,\n            cuda=cuda,\n            num_channels=num_channels\n        )\n\n        if initial_board_config:\n            self.initial_board_config = []\n            for board_tile in initial_board_config:\n                self.initial_board_config.append(dotdict({\n                    \'x\': board_tile.x,\n                    \'y\': board_tile.y,\n                    \'player\': board_tile.player,\n                    \'a_type\': d_a_type[board_tile.a_type],\n                    \'health\': self.player1_config.a_max_health[d_a_type[board_tile.a_type]] if board_tile.player == 1 else self.player2_config.a_max_health[d_a_type[board_tile.a_type]],\n                    \'carry\': 0,\n                    \'gold\': self.player1_config.INITIAL_GOLD if board_tile.player == 1 else self.player2_config.INITIAL_GOLD,\n                    \'timeout\': self.player1_config.TIMEOUT if board_tile.player == 1 else self.player2_config.TIMEOUT\n                }))\n        else:\n            self.initial_board_config = initial_board_config or [\n\n                dotdict({\n                    \'x\': int(self.grid_size / 2) - 1,\n                    \'y\': int(self.grid_size / 2),\n                    \'player\': 1,\n                    \'a_type\': d_a_type[\'Gold\'],\n                    \'health\': self.player1_config.a_max_health[d_a_type[\'Gold\']],\n                    \'carry\': 0,\n                    \'gold\': self.player1_config.INITIAL_GOLD,\n                    \'timeout\': self.player1_config.TIMEOUT\n                }),\n                dotdict({\n                    \'x\': int(self.grid_size / 2),\n                    \'y\': int(self.grid_size / 2),\n                    \'player\': -1,\n                    \'a_type\': d_a_type[\'Gold\'],\n                    \'health\': self.player2_config.a_max_health[d_a_type[\'Gold\']],\n                    \'carry\': 0,\n                    \'gold\': self.player2_config.INITIAL_GOLD,\n                    \'timeout\': self.player2_config.TIMEOUT\n                }),\n                dotdict({\n                    \'x\': int(self.grid_size / 2) - 1,\n                    \'y\': int(self.grid_size / 2) - 1,\n                    \'player\': 1,\n                    \'a_type\': d_a_type[\'Hall\'],\n                    \'health\': self.player1_config.a_max_health[d_a_type[\'Hall\']],\n                    \'carry\': 0,\n                    \'gold\': self.player1_config.INITIAL_GOLD,\n                    \'timeout\': self.player1_config.TIMEOUT\n                }),\n                dotdict({\n                    \'x\': int(self.grid_size / 2),\n                    \'y\': int(self.grid_size / 2) - 1,\n                    \'player\': -1,\n                    \'a_type\': d_a_type[\'Hall\'],\n                    \'health\': self.player2_config.a_max_health[d_a_type[\'Hall\']],\n                    \'carry\': 0,\n                    \'gold\': self.player2_config.INITIAL_GOLD,\n                    \'timeout\': self.player2_config.TIMEOUT\n                }),\n            ]\n\n    def set_runner(self, runner: str):\n        self.runner = runner\n        if runner == \'pit\':\n            self.visibility = self._pit_visibility\n        elif runner == \'learn\':\n            self.visibility = self._learn_visibility\n        else:\n            print(""Unrecognised runner. Returning"")\n            exit(1)\n'"
rts/src/config_class.py,0,"b'# noinspection PyUnresolvedReferences\nfrom rts.src.config import Configuration\n\n# Basic config:\n# CONFIG = Configuration()\n\n# ################################# Examples #######################################\n# Two different encoders\n# Release:\n""""""\nhttps://github.com/JernejHabjan/alpha-zero-general/releases/tag/0.3.0\n""""""\n# Example\n""""""\nFirst learning model:\nCONFIG = Configuration(use_one_hot_encoder=True, onehot_encoder_player1=True, onehot_encoder_player2=False)\nRun learn\nDont forget to rename it to ""best_player1.pth.tar"" after learning \nSecond learning model:\nCONFIG = Configuration(use_one_hot_encoder=False, onehot_encoder_player1=True, onehot_encoder_player2=False) \nRun learn\nDont forget to rename it to ""best_player2.pth.tar"" after learning\nRun pit\n""""""\n\n# ######## Example learning ##################\n# Release:\n""""""\nhttps://github.com/JernejHabjan/alpha-zero-general/releases/tag/0.4.0\n""""""\n""""""\nCONFIG = Configuration(num_iters=10,\n                       num_eps=10,\n                       num_mcts_sims=30,\n                       epochs=100)\n""""""\n# Description\n""""""\nExample of longer learning with high number of eps and mcts sims.\n""""""\n\n# ################################# RUN 1 ##############################################\n\nCONFIG = Configuration(num_iters=100,\n                       num_iters_for_train_examples_history=30,\n                       num_eps=4,\n                       num_mcts_sims=5,\n                       arena_compare=7,\n                       epochs=100,\n                       initial_gold_player1=10,\n                       initial_gold_player2=10)\n# Release:\n""""""\nhttps://github.com/JernejHabjan/alpha-zero-general/releases/tag/1.0.0\n""""""\n\n# Description\n""""""\nNum iterations: Increased to 100, so graphing can be done correctly and multiple comparisons between models are done.\nTrain examples history: Increased to 30, because of high number of iterations. After 30 iterations, learning process becomes quite slow but efficient\nNum eps: Decreased to 4, so multiple iterations can be triggered faster\nNum mcts sims: Decreased to 5, because game is not played to end, it doesnt really contribute that much\nArena compare: 7 so comparisons between old and new model are quick but not resulting in overwriting better model\nEpochs: Increased to 100, because of GPU, where learning is done relatively fast even with such a number\nInitial gold: Increased for both players to 10. This is most important parameter change here, because it gives players enough money to start constructing different actors, which results in non-tie games and forces players to keep creating new actors and attacking with rifle units.\n""""""\n# Results\n""""""\nWorkers are very frequently gathering gold when near that actor. \nRandom movement has been greatly decreased over learning period, resulting in less time wasted.\nRifle units are also produced later in the game, which successfully attack enemy units when they are placed near them. Hunting for enemy actors doesn\'t occur, where player would try to annihilate enemy player.\nPlayers mostly gather gold and construct new actors with occasional attacks.\n""""""\n\n# ######## Pit with different board setup ###########\n""""""\nCONFIG = Configuration(num_iters=100,\n                       num_iters_for_train_examples_history=30,\n                       num_eps=4,\n                       num_mcts_sims=5,\n                       arena_compare=7,\n                       epochs=100,\n                       initial_gold_player1=10,\n                       initial_gold_player2=10,\n                       initial_board_config=[\n                           Configuration.BoardTile(1, 0, 4, \'Gold\'),\n                           Configuration.BoardTile(-1, 7, 4, \'Gold\'),\n                           Configuration.BoardTile(1, 3, 5, \'Hall\'),\n                           Configuration.BoardTile(-1, 4, 5, \'Hall\')]\n                       )\n\n""""""\n# Release:\n""""""\nhttps://github.com/JernejHabjan/alpha-zero-general/releases/tag/1.0.0\n""""""\n# Description\n""""""\nInitial board config: players have gold actors on edges of map\n""""""\n\n# Results\n""""""\nPlayers start game by constructing as much actors as they can with provided gold.\nPlayers continue to successfully gather gold when they get near gold minerals, but randomly walk around when they are not.\nAttacking units continue to damage and destroy enemy units when nearby, but attacks on enemy base are not initiated, resulting in annihilation\n""""""\n\n# ################################# RUN 2 ##############################################\n\n# First learning model (best_player1.pth.tar):\n""""""\nCONFIG = Configuration(use_one_hot_encoder=True,\n                       onehot_encoder_player1=True,\n                       onehot_encoder_player2=False,\n\n                       num_iters=20,\n                       num_iters_for_train_examples_history=5,\n                       num_eps=4,\n                       num_mcts_sims=5,\n                       arena_compare=7,\n                       epochs=100,\n                       initial_gold_player1=10,\n                       initial_gold_player2=10,\n                       \n                       num_games=100,\n                       pit_visibility=0)\n""""""\n# Second learning model (best_player2.pth.tar):\n""""""\nCONFIG = Configuration(use_one_hot_encoder=False,\n                       onehot_encoder_player1=True,\n                       onehot_encoder_player2=False,\n\n                       num_iters=20,\n                       num_iters_for_train_examples_history=5,\n                       num_eps=4,\n                       num_mcts_sims=5,\n                       arena_compare=7,\n                       epochs=100,\n                       initial_gold_player1=10,\n                       initial_gold_player2=10,\n\n                       num_games=100,\n                       pit_visibility=0)\n""""""\n# Release\n""""""\nhttps://github.com/JernejHabjan/alpha-zero-general/releases/tag/1.0.1\n""""""\n\n# Description\n""""""\nComparing model encoded using one-hot encoder against numeric encoder\n""""""\n\n# Results:\n""""""\n(62, 32, 6) (onehot, numeric, ties)\n""""""\n\n# ################################# OLD RUNS (Deprecated) ###########################################\n\n# Sample Health Task\n""""""\nCONFIG = Configuration(num_iters=20, \n                       num_eps=10,\n                       num_mcts_sims=30,\n                       epochs=100)\n""""""\n\n# Model Gathering Task\n""""""\nCONFIG = Configuration(num_iters=10,\n                       num_eps=10,\n                       num_mcts_sims=30,\n                       epochs=100,\n                       timeout_player1=100,\n                       timeout_player2=100,\n                       acts_enabled_player1={\n                           ""idle"": False,\n                           ""up"": True,\n                           ""down"": True,\n                           ""right"": True,\n                           ""left"": True,\n                           ""mine_resources"": True,\n                           ""return_resources"": True,\n                           ""attack"": False,\n                           ""npc"": False,\n                           ""rifle_infantry"": False,\n                           ""barracks"": False,\n                           ""town_hall"": False,\n                           ""heal"": False\n                       },\n                       acts_enabled_player2={\n                           ""idle"": False,\n                           ""up"": True,\n                           ""down"": True,\n                           ""right"": True,\n                           ""left"": True,\n                           ""mine_resources"": True,\n                           ""return_resources"": True,\n                           ""attack"": False,\n                           ""npc"": False,\n                           ""rifle_infantry"": False,\n                           ""barracks"": False,\n                           ""town_hall"": False,\n                           ""heal"": False\n                       },\n                       score_function_player1=1,\n                       score_function_player2=1,\n                       initial_board_config=[\n                           Configuration.BoardTile(1, 6, 4, \'Work\'),\n                           Configuration.BoardTile(-1, 6, 5, \'Work\'),\n                           Configuration.BoardTile(1, 4, 4, \'Gold\'),\n                           Configuration.BoardTile(-1, 4, 5, \'Gold\'),\n                           Configuration.BoardTile(1, 5, 4, \'Hall\'),\n                           Configuration.BoardTile(-1, 5, 5, \'Hall\')])\n""""""\n'"
rts/src/encoders.py,0,"b'from typing import List\n\nimport numpy as np\n\n""""""\nencoders.py\n\nDefines \'numeric\' and one-hot encoder\n\nNumeric encoder uses integers to encode game state (also negative numbers for player names)\nOne-hot encoder uses binary representation of integer numbers, with exception of player name, which is processed separately\n""""""\n\n\nclass Encoder:\n    def __init__(self):\n        self.NUM_ENCODERS = None\n\n    def encode(self, board) -> np.ndarray:\n        pass\n\n    def encode_multiple(self, boards: np.ndarray) -> np.ndarray:\n        pass\n\n    @property\n    def num_encoders(self):\n        return self.NUM_ENCODERS\n\n\nclass NumericEncoder(Encoder):\n\n    def __init__(self) -> None:\n        super().__init__()\n        self.NUM_ENCODERS = 6  # player_name, act_type, health, carrying, money, remaining_time\n\n    def encode_multiple(self, boards: np.ndarray) -> np.ndarray:\n        """"""\n        Do nothing - already encoded numerically\n        :param boards: just boards\n        :return: same boards\n        """"""\n        return boards\n\n    def encode(self, board) -> np.ndarray:\n        """"""\n        Do nothing - already encoded numerically\n        :param board: just board\n        :return: same board\n        """"""\n        return board\n\n\nclass OneHotEncoder(Encoder):\n    def __init__(self) -> None:\n        super().__init__()\n        self._build_indexes()\n\n    def _build_indexes(self):\n        """"""\n        Defines encoding indexes - you may change them as you would like, but do not reduce them below their actual encoders.\n        For example - if health is represented using 5 bits, don\'t set max_health_amount of actor to >2^5 - 1\n        """"""\n        self.P_NAME_IDX_INC_OH = 2  # playerName 2 bit - 00(neutral), 01(1) or 10(-1),\n        self.A_TYPE_IDX_INC_OH = 3  # actor type -> 3 bit,\n        self.HEALTH_IDX_INC_OH = 5  # health-> 5 bit,\n        self.CARRY_IDX_INC_OH = 1  # carrying-> 1 bit,\n        self.MONEY_IDX_INC_OH = 8  # money-> 8 bits (255) [every unit has the same for player]\n        self.REMAIN_IDX_INC_OH = 11  # 2^11 2048(za total annihilation)\n\n        # builds indexes for character encoding - if not using one hot encoding, max indexes are incremented by 1 from previous index, but for one hot encoding, its incremented by num bits\n        self.P_NAME_IDX_OH = 0\n        self.P_NAME_IDX_MAX_OH = self.P_NAME_IDX_INC_OH\n\n        self.A_TYPE_IDX_OH = self.P_NAME_IDX_MAX_OH\n        self.A_TYPE_IDX_MAX_OH = self.A_TYPE_IDX_OH + self.A_TYPE_IDX_INC_OH\n\n        self.HEALTH_IDX_OH = self.A_TYPE_IDX_MAX_OH\n        self.HEALTH_IDX_MAX_OH = self.HEALTH_IDX_OH + self.HEALTH_IDX_INC_OH\n\n        self.CARRY_IDX_OH = self.HEALTH_IDX_MAX_OH\n        self.CARRY_IDX_MAX_OH = self.CARRY_IDX_OH + self.CARRY_IDX_INC_OH\n\n        self.MONEY_IDX_OH = self.CARRY_IDX_MAX_OH\n        self.MONEY_IDX_MAX_OH = self.MONEY_IDX_OH + self.MONEY_IDX_INC_OH\n\n        self.REMAIN_IDX_OH = self.MONEY_IDX_MAX_OH\n        self.REMAIN_IDX_MAX_OH = self.REMAIN_IDX_OH + self.REMAIN_IDX_INC_OH\n\n        self.NUM_ENCODERS = self.REMAIN_IDX_MAX_OH\n\n    @staticmethod\n    def itb(num: int, length: int) -> List[int]:\n        """"""\n        Converts integer to bit array\n        Someone fix this please :D - it\'s horrible\n        :param num: number to convert to bits\n        :param length: length of bits to convert to\n        :return: bit array\n        """"""\n        num = int(num)\n        if length == 1:\n            return [int(i) for i in \'{0:01b}\'.format(num)]\n        if length == 2:\n            return [int(i) for i in \'{0:02b}\'.format(num)]\n        if length == 3:\n            return [int(i) for i in \'{0:03b}\'.format(num)]\n        if length == 4:\n            return [int(i) for i in \'{0:04b}\'.format(num)]\n        if length == 5:\n            return [int(i) for i in \'{0:05b}\'.format(num)]\n        if length == 8:\n            return [int(i) for i in \'{0:08b}\'.format(num)]\n        if length == 11:\n            return [int(i) for i in \'{0:011b}\'.format(num)]\n        raise TypeError(""Length not supported:"", length)\n\n    def encode_multiple(self, boards: np.ndarray) -> np.ndarray:\n        """"""\n        Encodes and returns multiple boards using onehot encoder\n        :param boards: array of boards to encode\n        :return: new boards, encoded using onehot encoder\n        """"""\n        new_boards = []\n        for board in boards:\n            new_boards.append(self.encode(board))\n        return np.asarray(new_boards)\n\n    def encode(self, board) -> np.ndarray:\n        """"""\n        Encode single board using onehot encoder\n        :param board: normal board\n        :return: new encoded board\n        """"""\n        from rts.src.config import P_NAME_IDX, A_TYPE_IDX, HEALTH_IDX, CARRY_IDX, MONEY_IDX, TIME_IDX\n\n        n = board.shape[0]\n\n        b = np.zeros((n, n, self.NUM_ENCODERS))\n        for y in range(n):\n            for x in range(n):\n                # switch player from -1 to 2\n                player = 0\n                if board[x, y, P_NAME_IDX] == 1:\n                    player = 1\n                elif board[x, y, P_NAME_IDX] == -1:\n                    player = 2\n\n                b[x, y][self.P_NAME_IDX_OH:self.P_NAME_IDX_MAX_OH] = self.itb(player, self.P_NAME_IDX_INC_OH)\n                b[x, y][self.A_TYPE_IDX_OH:self.A_TYPE_IDX_MAX_OH] = self.itb(board[x, y, A_TYPE_IDX], self.A_TYPE_IDX_INC_OH)\n                b[x, y][self.HEALTH_IDX_OH:self.HEALTH_IDX_MAX_OH] = self.itb(board[x, y, HEALTH_IDX], self.HEALTH_IDX_INC_OH)\n                b[x, y][self.CARRY_IDX_OH:self.CARRY_IDX_MAX_OH] = self.itb(board[x, y, CARRY_IDX], self.CARRY_IDX_INC_OH)\n                b[x, y][self.MONEY_IDX_OH:self.MONEY_IDX_MAX_OH] = self.itb(board[x, y, MONEY_IDX], self.MONEY_IDX_INC_OH)\n                b[x, y][self.REMAIN_IDX_OH:self.REMAIN_IDX_MAX_OH] = self.itb(board[x, y, TIME_IDX], self.REMAIN_IDX_INC_OH)\n        return b\n'"
rts/visualization/rts_pygame.py,0,"b'import ctypes\nimport sys\nfrom typing import Any, Tuple, Optional\n\nimport numpy as np\n\nsys.path.append(\'../..\')\nfrom rts.src.config import P_NAME_IDX, A_TYPE_IDX, d_a_color, d_type_rev, MONEY_IDX, TIME_IDX, CARRY_IDX, HEALTH_IDX\n\n""""""\nrts_pygame.py\n\nUsed for displaying Gama visualization using Pygame\n\n""""""\n\n\ndef message_display(game_display, text, position, text_size, color=(0, 0, 0)) -> None:\n    """"""\n    Display text on pygame window.\n    :param game_display: Which canvas text will be rendered upon\n    :param text: string text\n    :param position: coordinates on canvas where text will be displayed\n    :param text_size: ...\n    :param color: (r,g,b) color\n    """"""\n    import pygame\n\n    large_text = pygame.font.SysFont(\'arial\', text_size)\n    text_surf = large_text.render(text, True, color)\n    text_rect = text_surf.get_rect()\n    text_rect.center = position\n    game_display.blit(text_surf, text_rect)\n\n\ndef init_visuals(world_width: int, world_height: int, verbose=True) -> Optional[Tuple[Any, Any]]:\n    """"""\n    Creates canvas to draw upon and creates tick\n    :param world_width: ...\n    :param world_height: ...\n    :param verbose: if verbose is set to false, game will not be initialized\n    :return: game_display, clock\n    """"""\n    if verbose:\n        import pygame\n\n        pygame.init()\n        canvas_scale = int(ctypes.windll.user32.GetSystemMetrics(1) * (2 / 3) / world_height)  # for drawing - it takes 2 thirds of screen height\n\n        # square\n\n        display_width, display_height = world_width * canvas_scale, world_height * canvas_scale  # for example 800\n\n        game_display = pygame.display.set_mode((display_width, display_height))\n        pygame.display.set_caption(\'RTS visualization Python game\')\n\n        clock = pygame.time.Clock()\n\n        return game_display, clock\n\n\ndef update_graphics(board: np.ndarray, game_display, clock, fps: int = 1) -> None:\n    """"""\n    Executes game tick on canvas, redrawing whole game state. Values here are somewhat hardcoded, which can be changed to display game in some nicer config.\n    Board size 8x8 is working best with this config, 6x6 might work as well, but other might not.\n    :param board: game state that will be drawn\n    :param game_display: canvas to draw game state upon\n    :param clock: game tick\n    :param fps: how many fps should pygame draw. if value is set to higher number than your pc can handle, it will draw at max possible.\n    """"""\n    import pygame\n\n    n = board.shape[0]\n\n    canvas_scale = int(ctypes.windll.user32.GetSystemMetrics(1) * (16 / 30) / n)  # for drawing - it takes 2 thirds of screen height\n\n    # clear display\n    game_display.fill((255, 255, 255))\n    # self.display_img(game_display, x,y)\n\n    # title\n    # message_display(game_display, u"""" + \' \' + str(gold_p1), (int((n / 8) * canvas_scale), (n+1) * canvas_scale + int(int(canvas_scale / 12) + canvas_scale * (0 / 4) + int(canvas_scale * (1 / 8)))), int(canvas_scale / 6))\n\n    # draw grid:\n    for y in range(canvas_scale, (n + 2) * canvas_scale, canvas_scale):\n        pygame.draw.line(game_display, (0, 0, 0), [y, canvas_scale], [y, (n + 1) * canvas_scale])\n        for x in range(canvas_scale, (n + 2) * canvas_scale, canvas_scale):\n            pygame.draw.line(game_display, (0, 0, 0), [canvas_scale, x], [(n + 1) * canvas_scale, x])\n            if x < (n + 1) * canvas_scale and y < (n + 1) * canvas_scale:\n                message_display(game_display, u"""" + str(x / canvas_scale - 1) + "", "" + str(y / canvas_scale - 1), ((x + canvas_scale / 4), (y + canvas_scale / 10)), int(canvas_scale / 8))\n\n    # gold for each player:\n    gold_p1 = board[int(n / 2) - 1][int(n / 2)][MONEY_IDX]\n    gold_p2 = board[int(n / 2)][int(n / 2) - 1][MONEY_IDX]\n\n    message_display(game_display, u"""" + \'Gold Player +1: \' + str(gold_p1), (int((n / 8) * canvas_scale), (n + 1) * canvas_scale + int(int(canvas_scale / 12) + canvas_scale * (0 / 4) + int(canvas_scale * (1 / 8)))), int(canvas_scale / 6))\n    message_display(game_display, u"""" + \'Gold Player -1: \' + str(gold_p2), (int((n / 8) * canvas_scale), (n + 1) * canvas_scale + int(int(canvas_scale / 12) + canvas_scale * (1 / 4) + int(canvas_scale * (1 / 8)))), int(canvas_scale / 6))\n\n    time_remaining = board[0][0][TIME_IDX]\n    message_display(game_display, u"""" + \'Remaining \' + str(time_remaining), (int((n / 8) * canvas_scale), (n + 1) * canvas_scale + int(int(canvas_scale / 12) + canvas_scale * (2 / 4) + int(canvas_scale * (1 / 8)))), int(canvas_scale / 6))\n\n    for y in range(n):\n        for x in range(n):\n            a_player = board[x][y][P_NAME_IDX]\n\n            if a_player == 1 or a_player == -1:\n\n                a_type = board[x][y][A_TYPE_IDX]\n                actor_color = d_a_color[a_type]\n\n                actor_location = (int(x * canvas_scale + canvas_scale / 2 + canvas_scale), int(y * canvas_scale + canvas_scale / 2) + canvas_scale)\n                actor_x, actor_y = actor_location\n\n                actor_size = int(canvas_scale / 3)\n                actor_short_name = d_type_rev[a_type]\n\n                actor_carry = board[x][y][CARRY_IDX]\n                actor_health = board[x][y][HEALTH_IDX]\n\n                pygame.draw.circle(game_display, actor_color, actor_location, actor_size)\n\n                player_color = (0, 0, 0)\n                if a_player == 1:\n                    player_color = (0, 255, 0)\n                if a_player == -1:\n                    player_color = (255, 0, 0)\n\n                pygame.draw.circle(game_display, player_color, actor_location, actor_size, int(actor_size / 10))\n                message_display(game_display, u"""" + actor_short_name, actor_location, int(actor_size * 0.7))\n\n                if a_type != 1:  # if not gold\n                    message_display(game_display, u""hp: "" + str(actor_health), (actor_x, actor_y + canvas_scale * (2 / 10)), int(actor_size * 0.5))\n\n                if a_type == 2:  # if npc\n                    message_display(game_display, u""carry: "" + str(actor_carry), (actor_x, actor_y + canvas_scale * (4 / 10)), int(actor_size * 0.5))\n\n    pygame.display.update()\n\n    clock.tick(fps)\n'"
rts/visualization/rts_ue4.py,0,"b'# noinspection PyUnresolvedReferences\nimport gc\nimport os\n\nimport numpy as np\nimport tensorflow as tf\n# noinspection PyUnresolvedReferences\nimport unreal_engine as ue\n# noinspection PyUnresolvedReferences\nfrom TFPluginAPI import TFPluginAPI\n\nfrom MCTS import MCTS\nfrom rts.RTSGame import RTSGame\nfrom rts.keras.NNet import NNetWrapper as NNet\nfrom rts.src.config import ACTS_REV, NUM_ACTS\nfrom rts.src.encoders import OneHotEncoder\nfrom utils import dotdict\n\n""""""\nrts_ue4.py\n\nThis classes intended use is connecting to ue4 TensorFlow plugin as client and execute predict on given board.\nConnect to UE4 using https://github.com/getnamo/tensorflow-ue4\nSee this release https://github.com/getnamo/tensorflow-ue4/releases/tag/0.8.0\nMore info in readme.md\n""""""\n\n\n# noinspection PyPep8Naming\nclass TD2020LearnAPI(TFPluginAPI):\n    def __init__(self):\n        self.owning_player = None\n        self.initial_board_config = None\n        self.setup = False\n        self.g = None\n        self.graph_var = None\n        self.session_var = None\n        self.mcts = None\n\n    def onSetup(self):\n        """"""\n        Sets up nnet configs and mcts. It loads model in ram. Session variable is saved, so it can be then used async in \'onJsonInput\'\n        """"""\n        graph = tf.Graph()\n        with graph.as_default():\n            session = tf.Session()\n            with session.as_default():\n                current_directory = os.path.join(os.path.dirname(__file__), \'temp/\')\n                self.g = RTSGame()\n                n1 = NNet(self.g, OneHotEncoder())\n                n1.load_checkpoint(current_directory, \'best.pth.tar\')\n                args = dotdict({\'numMCTSSims\': 2, \'cpuct\': 1.0})\n                self.mcts = MCTS(self.g, n1, args)\n\n                self.graph_var = graph\n                self.session_var = session\n\n                self.setup = True\n\n    def onJsonInput(self, jsonInput):\n        """"""\n        Request for action for specific game state of specific player.\n        Json input is recieved from UE4, providing game state in ue4. This game state must reflect same configuration as Python one.\n        Keep in mind coordinate system orientation\n        :param jsonInput: initial board config and player, requesting action\n        :return: recommended action using our nnet\n        """"""\n        if not self.setup:\n            return\n        encoded_actors = jsonInput[\'data\']\n        initial_board_config = []\n        for encoded_actor in encoded_actors:\n            initial_board_config.append(\n                dotdict({\n                    \'x\': encoded_actor[\'x\'],\n                    \'y\': encoded_actor[\'y\'],\n                    \'player\': encoded_actor[\'player\'],\n                    \'a_type\': encoded_actor[\'actorType\'],\n                    \'health\': encoded_actor[\'health\'],\n                    \'carry\': encoded_actor[\'carry\'],\n                    \'gold\': encoded_actor[\'money\'],\n                    \'timeout\': encoded_actor[\'remaining\']\n                })\n            )\n\n        self.initial_board_config = initial_board_config\n        self.owning_player = jsonInput[\'player\']\n        ######\n        with self.graph_var.as_default():\n            with self.session_var.as_default():\n                self.g.setInitBoard(self.initial_board_config)\n                b = self.g.getInitBoard()\n\n                def n1p(board): return np.argmax(self.mcts.getActionProb(board, temp=0))\n\n                canonical_board = self.g.getCanonicalForm(b, self.owning_player)\n\n                recommended_act = n1p(canonical_board)\n                y, x, action_index = np.unravel_index(recommended_act, [b.shape[0], b.shape[0], NUM_ACTS])\n\n                # gc.collect()\n                act = {""x"": str(x), ""y"": str(y), ""action"": ACTS_REV[action_index]}\n                print(""Printing recommended action >>>>>>>>>>>>>>>>>>>>>>>>"" + str(act))\n        return act\n\n    def onBeginTraining(self):\n        pass\n\n    def run(self, args):\n        pass\n\n    # noinspection PyUnusedLocal\n    def close(self, args):\n        """"""\n        Just clear everything, so it\'s not memory leaking\n        :param args: /\n        """"""\n        print(""Closing Get Action"")\n        if self.session_var:\n            self.session_var.close()\n        self.owning_player = None\n        self.initial_board_config = None\n        self.setup = False\n        self.g = None\n        self.graph_var = None\n        self.session_var = None\n        self.mcts = None\n\n\n# required function to get our api\n# noinspection PyPep8Naming\ndef getApi():\n    return TD2020LearnAPI.getInstance()\n'"
tafl/keras/NNet.py,0,"b'import argparse\nimport os\nimport shutil\nimport time\nimport random\nimport numpy as np\nimport math\nimport sys\nsys.path.append(\'../..\')\nfrom utils import *\nfrom NeuralNet import NeuralNet\n\nimport argparse\nfrom .TaflNNet import TaflNNet as onnet\n\nargs = dotdict({\n    \'lr\': 0.001,\n    \'dropout\': 0.3,\n    \'epochs\': 10,\n    \'batch_size\': 64,\n    \'cuda\': False,\n    \'num_channels\': 512,\n})\n\nclass NNetWrapper(NeuralNet):\n    def __init__(self, game):\n        self.nnet = onnet(game, args)\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n\n    def train(self, examples):\n        """"""\n        examples: list of examples, each example is of form (board, pi, v)\n        """"""\n        input_boards, target_pis, target_vs = list(zip(*examples))\n        input_boards = np.asarray(input_boards)\n        target_pis = np.asarray(target_pis)\n        target_vs = np.asarray(target_vs)\n        self.nnet.model.fit(x = input_boards, y = [target_pis, target_vs], batch_size = args.batch_size, epochs = args.epochs)\n\n    def predict(self, board):\n        """"""\n        board: np array with board\n        """"""\n        # timing\n        start = time.time()\n\n        # preparing input\n        board = board[np.newaxis, :, :]\n\n        # run\n        pi, v = self.nnet.model.predict(board)\n\n        #print(\'PREDICTION TIME TAKEN : {0:03f}\'.format(time.time()-start))\n        return pi[0], v[0]\n\n    def save_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(folder):\n            print(""Checkpoint Directory does not exist! Making directory {}"".format(folder))\n            os.mkdir(folder)\n        else:\n            print(""Checkpoint Directory exists! "")\n        self.nnet.model.save_weights(filepath)\n\n    def load_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(filepath):\n            raise(""No model in path {}"".format(filepath))\n        self.nnet.model.load_weights(filepath)\n'"
tafl/keras/TaflNNet.py,0,"b""import sys\nsys.path.append('..')\nfrom utils import *\n\nimport argparse\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\n\nclass TaflNNet():\n    def __init__(self, game, args):\n        # game params\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n        self.args = args\n\n        # Neural Net\n        self.input_boards = Input(shape=(self.board_x, self.board_y))    # s: batch_size x board_x x board_y\n\n        x_image = Reshape((self.board_x, self.board_y, 1))(self.input_boards)                # batch_size  x board_x x board_y x 1\n        h_conv1 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='same', use_bias=False)(x_image)))         # batch_size  x board_x x board_y x num_channels\n        h_conv2 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='same', use_bias=False)(h_conv1)))         # batch_size  x board_x x board_y x num_channels\n        h_conv3 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='valid', use_bias=False)(h_conv2)))        # batch_size  x (board_x-2) x (board_y-2) x num_channels\n        h_conv4 = Activation('relu')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding='valid', use_bias=False)(h_conv3)))        # batch_size  x (board_x-4) x (board_y-4) x num_channels\n        h_conv4_flat = Flatten()(h_conv4)       \n        s_fc1 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(1024, use_bias=False)(h_conv4_flat))))  # batch_size x 1024\n        s_fc2 = Dropout(args.dropout)(Activation('relu')(BatchNormalization(axis=1)(Dense(512, use_bias=False)(s_fc1))))          # batch_size x 1024\n        self.pi = Dense(self.action_size, activation='softmax', name='pi')(s_fc2)   # batch_size x self.action_size\n        self.v = Dense(1, activation='tanh', name='v')(s_fc2)                    # batch_size x 1\n\n        self.model = Model(inputs=self.input_boards, outputs=[self.pi, self.v])\n        self.model.compile(loss=['categorical_crossentropy','mean_squared_error'], optimizer=Adam(args.lr))\n"""
tafl/keras/__init__.py,0,b''
tafl/pytorch/NNet.py,12,"b'import os\nimport sys\nimport time\n\nimport numpy as np\nfrom tqdm import tqdm\n\nsys.path.append(\'../../\')\nfrom utils import *\n\nfrom NeuralNet import NeuralNet\n\nimport torch\nimport torch.optim as optim\n\nfrom .TaflNNet import TaflNNet as onnet\n\nargs = dotdict({\n    \'lr\': 0.001,\n    \'dropout\': 0.3,\n    \'epochs\': 10,\n    \'batch_size\': 64,\n    \'cuda\': torch.cuda.is_available(),\n    \'num_channels\': 512,\n})\n\n\nclass NNetWrapper(NeuralNet):\n    def __init__(self, game):\n        self.nnet = onnet(game, args)\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n\n        if args.cuda:\n            self.nnet.cuda()\n\n    def train(self, examples):\n        """"""\n        examples: list of examples, each example is of form (board, pi, v)\n        """"""\n        optimizer = optim.Adam(self.nnet.parameters())\n\n        for epoch in range(args.epochs):\n            print(\'EPOCH ::: \' + str(epoch + 1))\n            self.nnet.train()\n            pi_losses = AverageMeter()\n            v_losses = AverageMeter()\n\n            batch_count = int(len(examples) / args.batch_size)\n\n            t = tqdm(range(batch_count), desc=\'Training Net\')\n            for _ in t:\n                sample_ids = np.random.randint(len(examples), size=args.batch_size)\n                boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n                boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n                target_pis = torch.FloatTensor(np.array(pis))\n                target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n\n                # predict\n                if args.cuda:\n                    boards, target_pis, target_vs = boards.contiguous().cuda(), target_pis.contiguous().cuda(), target_vs.contiguous().cuda()\n\n                # compute output\n                out_pi, out_v = self.nnet(boards)\n                l_pi = self.loss_pi(target_pis, out_pi)\n                l_v = self.loss_v(target_vs, out_v)\n                total_loss = l_pi + l_v\n\n                # record loss\n                pi_losses.update(l_pi.item(), boards.size(0))\n                v_losses.update(l_v.item(), boards.size(0))\n                t.set_postfix(Loss_pi=pi_losses, Loss_v=v_losses)\n\n                # compute gradient and do SGD step\n                optimizer.zero_grad()\n                total_loss.backward()\n                optimizer.step()\n\n    def predict(self, board):\n        """"""\n        board: np array with board\n        """"""\n        # timing\n        start = time.time()\n\n        # preparing input\n        board = torch.FloatTensor(board.astype(np.float64))\n        if args.cuda: board = board.contiguous().cuda()\n        board = board.view(1, self.board_x, self.board_y)\n        self.nnet.eval()\n        with torch.no_grad():\n            pi, v = self.nnet(board)\n\n        # print(\'PREDICTION TIME TAKEN : {0:03f}\'.format(time.time()-start))\n        return torch.exp(pi).data.cpu().numpy()[0], v.data.cpu().numpy()[0]\n\n    def loss_pi(self, targets, outputs):\n        return -torch.sum(targets * outputs) / targets.size()[0]\n\n    def loss_v(self, targets, outputs):\n        return torch.sum((targets - outputs.view(-1)) ** 2) / targets.size()[0]\n\n    def save_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(folder):\n            print(""Checkpoint Directory does not exist! Making directory {}"".format(folder))\n            os.mkdir(folder)\n        else:\n            print(""Checkpoint Directory exists! "")\n        torch.save({\n            \'state_dict\': self.nnet.state_dict(),\n        }, filepath)\n\n    def load_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(filepath):\n            raise (""No model in path {}"".format(filepath))\n        map_location = None if args.cuda else \'cpu\'\n        checkpoint = torch.load(filepath, map_location=map_location)\n        self.nnet.load_state_dict(checkpoint[\'state_dict\'])\n'"
tafl/pytorch/TaflNNet.py,5,"b""import sys\nsys.path.append('..')\nfrom utils import *\n\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\nclass TaflNNet(nn.Module):\n    def __init__(self, game, args):\n        # game params\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n        self.args = args\n\n        super(TaflNNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, args.num_channels, 3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n        self.conv4 = nn.Conv2d(args.num_channels, args.num_channels, 3, stride=1)\n\n        self.bn1 = nn.BatchNorm2d(args.num_channels)\n        self.bn2 = nn.BatchNorm2d(args.num_channels)\n        self.bn3 = nn.BatchNorm2d(args.num_channels)\n        self.bn4 = nn.BatchNorm2d(args.num_channels)\n\n        self.fc1 = nn.Linear(args.num_channels*(self.board_x-4)*(self.board_y-4), 1024)\n        self.fc_bn1 = nn.BatchNorm1d(1024)\n\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc_bn2 = nn.BatchNorm1d(512)\n\n        self.fc3 = nn.Linear(512, self.action_size)\n\n        self.fc4 = nn.Linear(512, 1)\n\n    def forward(self, s):\n        #                                                           s: batch_size x board_x x board_y\n        s = s.view(-1, 1, self.board_x, self.board_y)                # batch_size x 1 x board_x x board_y\n        s = F.relu(self.bn1(self.conv1(s)))                          # batch_size x num_channels x board_x x board_y\n        s = F.relu(self.bn2(self.conv2(s)))                          # batch_size x num_channels x board_x x board_y\n        s = F.relu(self.bn3(self.conv3(s)))                          # batch_size x num_channels x (board_x-2) x (board_y-2)\n        s = F.relu(self.bn4(self.conv4(s)))                          # batch_size x num_channels x (board_x-4) x (board_y-4)\n        s = s.view(-1, self.args.num_channels*(self.board_x-4)*(self.board_y-4))\n\n        s = F.dropout(F.relu(self.fc_bn1(self.fc1(s))), p=self.args.dropout, training=self.training)  # batch_size x 1024\n        s = F.dropout(F.relu(self.fc_bn2(self.fc2(s))), p=self.args.dropout, training=self.training)  # batch_size x 512\n\n        pi = self.fc3(s)                                                                         # batch_size x action_size\n        v = self.fc4(s)                                                                          # batch_size x 1\n\n        return F.log_softmax(pi, dim=1), torch.tanh(v)\n"""
tafl/pytorch/__init__.py,0,b''
tictactoe/keras/NNet.py,0,"b'import argparse\nimport os\nimport shutil\nimport time\nimport random\nimport numpy as np\nimport math\nimport sys\nsys.path.append(\'..\')\nfrom utils import *\nfrom NeuralNet import NeuralNet\n\nimport argparse\nfrom .TicTacToeNNet import TicTacToeNNet as onnet\n\n""""""\nNeuralNet wrapper class for the TicTacToeNNet.\n\nAuthor: Evgeny Tyurin, github.com/evg-tyurin\nDate: Jan 5, 2018.\n\nBased on (copy-pasted from) the NNet by SourKream and Surag Nair.\n""""""\n\nargs = dotdict({\n    \'lr\': 0.001,\n    \'dropout\': 0.3,\n    \'epochs\': 10,\n    \'batch_size\': 64,\n    \'cuda\': False,\n    \'num_channels\': 512,\n})\n\nclass NNetWrapper(NeuralNet):\n    def __init__(self, game):\n        self.nnet = onnet(game, args)\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n\n    def train(self, examples):\n        """"""\n        examples: list of examples, each example is of form (board, pi, v)\n        """"""\n        input_boards, target_pis, target_vs = list(zip(*examples))\n        input_boards = np.asarray(input_boards)\n        target_pis = np.asarray(target_pis)\n        target_vs = np.asarray(target_vs)\n        self.nnet.model.fit(x = input_boards, y = [target_pis, target_vs], batch_size = args.batch_size, epochs = args.epochs)\n\n    def predict(self, board):\n        """"""\n        board: np array with board\n        """"""\n        # timing\n        start = time.time()\n\n        # preparing input\n        board = board[np.newaxis, :, :]\n\n        # run\n        pi, v = self.nnet.model.predict(board)\n\n        #print(\'PREDICTION TIME TAKEN : {0:03f}\'.format(time.time()-start))\n        return pi[0], v[0]\n\n    def save_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(folder):\n            print(""Checkpoint Directory does not exist! Making directory {}"".format(folder))\n            os.mkdir(folder)\n        else:\n            print(""Checkpoint Directory exists! "")\n        self.nnet.model.save_weights(filepath)\n\n    def load_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(filepath):\n            raise(""No model in path \'{}\'"".format(filepath))\n        self.nnet.model.load_weights(filepath)\n'"
tictactoe/keras/TicTacToeNNet.py,0,"b'import sys\nsys.path.append(\'..\')\nfrom utils import *\n\nimport argparse\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\n\n""""""\nNeuralNet for the game of TicTacToe.\n\nAuthor: Evgeny Tyurin, github.com/evg-tyurin\nDate: Jan 5, 2018.\n\nBased on the OthelloNNet by SourKream and Surag Nair.\n""""""\nclass TicTacToeNNet():\n    def __init__(self, game, args):\n        # game params\n        self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n        self.args = args\n\n        # Neural Net\n        self.input_boards = Input(shape=(self.board_x, self.board_y))    # s: batch_size x board_x x board_y\n\n        x_image = Reshape((self.board_x, self.board_y, 1))(self.input_boards)                # batch_size  x board_x x board_y x 1\n        h_conv1 = Activation(\'relu\')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding=\'same\')(x_image)))         # batch_size  x board_x x board_y x num_channels\n        h_conv2 = Activation(\'relu\')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding=\'same\')(h_conv1)))         # batch_size  x board_x x board_y x num_channels\n        h_conv3 = Activation(\'relu\')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding=\'same\')(h_conv2)))        # batch_size  x (board_x) x (board_y) x num_channels\n        h_conv4 = Activation(\'relu\')(BatchNormalization(axis=3)(Conv2D(args.num_channels, 3, padding=\'valid\')(h_conv3)))        # batch_size  x (board_x-2) x (board_y-2) x num_channels\n        h_conv4_flat = Flatten()(h_conv4)       \n        s_fc1 = Dropout(args.dropout)(Activation(\'relu\')(BatchNormalization(axis=1)(Dense(1024)(h_conv4_flat))))  # batch_size x 1024\n        s_fc2 = Dropout(args.dropout)(Activation(\'relu\')(BatchNormalization(axis=1)(Dense(512)(s_fc1))))          # batch_size x 1024\n        self.pi = Dense(self.action_size, activation=\'softmax\', name=\'pi\')(s_fc2)   # batch_size x self.action_size\n        self.v = Dense(1, activation=\'tanh\', name=\'v\')(s_fc2)                    # batch_size x 1\n\n        self.model = Model(inputs=self.input_boards, outputs=[self.pi, self.v])\n        self.model.compile(loss=[\'categorical_crossentropy\',\'mean_squared_error\'], optimizer=Adam(args.lr))\n'"
tictactoe/keras/__init__.py,0,b''
tictactoe_3d/keras/NNet.py,0,"b'import argparse\nimport os\nimport shutil\nimport time\nimport random\nimport numpy as np\nimport math\nimport sys\nsys.path.append(\'..\')\nfrom utils import *\nfrom NeuralNet import NeuralNet\n\nimport argparse\nfrom .TicTacToeNNet import TicTacToeNNet as onnet\n\n""""""\nNeuralNet wrapper class for the TicTacToeNNet.\n\nAuthor: Evgeny Tyurin, github.com/evg-tyurin\nDate: Jan 5, 2018.\n\nBased on (copy-pasted from) the NNet by SourKream and Surag Nair.\n""""""\n\nargs = dotdict({\n    \'lr\': 0.001,\n    \'dropout\': 0.3,\n    \'epochs\': 10,\n    \'batch_size\': 64,\n    \'cuda\': False,\n    \'num_channels\': 512,\n})\n\nclass NNetWrapper(NeuralNet):\n    def __init__(self, game):\n        self.nnet = onnet(game, args)\n        self.board_z, self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n\n    def train(self, examples):\n        """"""\n        examples: list of examples, each example is of form (board, pi, v)\n        """"""\n        input_boards, target_pis, target_vs = list(zip(*examples))\n        input_boards = np.asarray(input_boards)\n        target_pis = np.asarray(target_pis)\n        target_vs = np.asarray(target_vs)\n        self.nnet.model.fit(x = input_boards, y = [target_pis, target_vs], batch_size = args.batch_size, epochs = args.epochs)\n\n    def predict(self, board):\n        """"""\n        board: np array with board\n        """"""\n        # timing\n        start = time.time()\n\n        # preparing input\n        board = board[np.newaxis, :, :]\n\n        # run\n        pi, v = self.nnet.model.predict(board)\n\n        #print(\'PREDICTION TIME TAKEN : {0:03f}\'.format(time.time()-start))\n        return pi[0], v[0]\n\n    def save_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(folder):\n            print(""Checkpoint Directory does not exist! Making directory {}"".format(folder))\n            os.mkdir(folder)\n        else:\n            print(""Checkpoint Directory exists! "")\n        self.nnet.model.save_weights(filepath)\n\n    def load_checkpoint(self, folder=\'checkpoint\', filename=\'checkpoint.pth.tar\'):\n        # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n        filepath = os.path.join(folder, filename)\n        if not os.path.exists(filepath):\n            raise(""No model in path \'{}\'"".format(filepath))\n        self.nnet.model.load_weights(filepath)\n'"
tictactoe_3d/keras/TicTacToeNNet.py,0,"b'import sys\nsys.path.append(\'..\')\nfrom utils import *\n\nimport argparse\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\n\n""""""\nNeuralNet for the game of TicTacToe.\n\nAuthor: Evgeny Tyurin, github.com/evg-tyurin\nDate: Jan 5, 2018.\n\nBased on the OthelloNNet by SourKream and Surag Nair.\n""""""\nclass TicTacToeNNet():\n    def __init__(self, game, args):\n        # game params\n        self.board_z, self.board_x, self.board_y = game.getBoardSize()\n        self.action_size = game.getActionSize()\n        self.args = args\n\n        # Neural Net\n        self.input_boards = Input(shape=(self.board_z, self.board_x, self.board_y))    # s: batch_size x board_x x board_y\n\n        x_image = Reshape((self.board_z, self.board_x, self.board_y, 1))(self.input_boards)                # batch_size  x board_x x board_y x 1\n        h_conv1 = Activation(\'relu\')(BatchNormalization(axis=3)(Conv3D(args.num_channels, 3, padding=\'same\')(x_image)))         # batch_size  x board_x x board_y x num_channels\n        h_conv2 = Activation(\'relu\')(BatchNormalization(axis=3)(Conv3D(args.num_channels, 3, padding=\'same\')(h_conv1)))         # batch_size  x board_x x board_y x num_channels\n        h_conv3 = Activation(\'relu\')(BatchNormalization(axis=3)(Conv3D(args.num_channels, 3, padding=\'same\')(h_conv2)))        # batch_size  x (board_x) x (board_y) x num_channels\n        h_conv4 = Activation(\'relu\')(BatchNormalization(axis=3)(Conv3D(args.num_channels, 3, padding=\'valid\')(h_conv3)))        # batch_size  x (board_x-2) x (board_y-2) x num_channels\n        h_conv4_flat = Flatten()(h_conv4)       \n        s_fc1 = Dropout(args.dropout)(Activation(\'relu\')(BatchNormalization(axis=1)(Dense(1024)(h_conv4_flat))))  # batch_size x 1024\n        s_fc2 = Dropout(args.dropout)(Activation(\'relu\')(BatchNormalization(axis=1)(Dense(512)(s_fc1))))          # batch_size x 1024\n        self.pi = Dense(self.action_size, activation=\'softmax\', name=\'pi\')(s_fc2)   # batch_size x self.action_size\n        self.v = Dense(1, activation=\'tanh\', name=\'v\')(s_fc2)                    # batch_size x 1\n\n        self.model = Model(inputs=self.input_boards, outputs=[self.pi, self.v])\n        self.model.compile(loss=[\'categorical_crossentropy\',\'mean_squared_error\'], optimizer=Adam(args.lr))\n'"
tictactoe_3d/keras/__init__.py,0,b''
