file_path,api_count,code
bin_mean_shift.py,21,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\nclass Bin_Mean_Shift(nn.Module):\n    def __init__(self, train_iter=5, test_iter=10, bandwidth=0.5, device=\'cpu\'):\n        super(Bin_Mean_Shift, self).__init__()\n        self.train_iter = train_iter\n        self.test_iter = test_iter\n        self.bandwidth = bandwidth / 2.\n        self.anchor_num = 10\n        self.sample_num = 3000\n        self.device = device\n\n    def generate_seed(self, point, bin_num):\n        """"""\n        :param point: tensor of size (K, 2)\n        :param bin_num: int\n        :return: seed_point\n        """"""\n        def get_start_end(a, b, k):\n            start = a + (b - a) / ((k + 1) * 2)\n            end = b - (b - a) / ((k + 1) * 2)\n            return start, end\n\n        min_x, min_y = point.min(dim=0)[0]\n        max_x, max_y = point.max(dim=0)[0]\n\n        start_x, end_x = get_start_end(min_x.item(), max_x.item(), bin_num)\n        start_y, end_y = get_start_end(min_y.item(), max_y.item(), bin_num)\n\n        x = torch.linspace(start_x, end_x, bin_num).view(bin_num, 1)\n        y = torch.linspace(start_y, end_y, bin_num).view(1, bin_num)\n\n        x_repeat = x.repeat(1, bin_num).view(-1, 1)\n        y_repeat = y.repeat(bin_num, 1).view(-1, 1)\n\n        return torch.cat((x_repeat, y_repeat), dim=1).to(self.device)\n\n    def filter_seed(self, point, prob, seed_point, bandwidth, min_count=3):\n        """"""\n        :param point: tensor of size (K, 2)\n        :param seed_point: tensor of size (n, 2)\n        :param prob: tensor of size (K, 1) indicate probability of being plane\n        :param min_count:  mini_count within a bandwith of seed point\n        :param bandwidth: float\n        :return: filtered_seed_points\n        """"""\n        distance_matrix = self.cal_distance_matrix(seed_point, point)  # (n, K)\n        thres_matrix = (distance_matrix < bandwidth).type(torch.float32) * prob.t()\n        count = thres_matrix.sum(dim=1)                  # (n, 1)\n        valid = count > min_count\n        return seed_point[valid]\n\n    def cal_distance_matrix(self, point_a, point_b):\n        """"""\n        :param point_a: tensor of size (m, 2)\n        :param point_b: tensor of size (n, 2)\n        :return: distance matrix of size (m, n)\n        """"""\n        m, n = point_a.size(0), point_b.size(0)\n\n        a_repeat = point_a.repeat(1, n).view(n * m, 2)                  # (n*m, 2)\n        b_repeat = point_b.repeat(m, 1)                                 # (n*m, 2)\n\n        distance = torch.nn.PairwiseDistance(keepdim=True)(a_repeat, b_repeat)  # (n*m, 1)\n\n        return distance.view(m, n)\n\n    def shift(self, point, prob, seed_point, bandwidth):\n        """"""\n        shift seed points\n        :param point: tensor of size (K, 2)\n        :param seed_point: tensor of size (n, 2)\n        :param prob: tensor of size (K, 1) indicate probability of being plane\n        :param bandwidth: float\n        :return:  shifted points with size (n, 2)\n        """"""\n        distance_matrix = self.cal_distance_matrix(seed_point, point)  # (n, K)\n        kernel_matrix = torch.exp((-0.5 / bandwidth**2) * (distance_matrix ** 2)) * (1. / (bandwidth * np.sqrt(2 * np.pi)))\n        weighted_matrix = kernel_matrix * prob.t()\n\n        # normalize matrix\n        normalized_matrix = weighted_matrix / weighted_matrix.sum(dim=1, keepdim=True)\n        shifted_point = torch.matmul(normalized_matrix, point)  # (n, K) * (K, 2) -> (n, 2)\n\n        return shifted_point\n\n    def label2onehot(self, labels):\n        """"""\n        convert a label to one hot vector\n        :param labels: tensor with size (n, 1)\n        :return: one hot vector tensor with size (n, max_lales+1)\n        """"""\n        n = labels.size(0)\n        label_num = torch.max(labels).int() + 1\n\n        onehot = torch.zeros((n, label_num))\n        onehot.scatter_(1, labels.long(), 1.)\n\n        return onehot.to(self.device)\n\n    def merge_center(self, seed_point, bandwidth=0.25):\n        """"""\n        merge close seed points\n        :param seed_point: tensor of size (n, 2)\n        :param bandwidth: float\n        :return: merged center\n        """"""\n        n = seed_point.size(0)\n\n        # 1. calculate intensity\n        distance_matrix = self.cal_distance_matrix(seed_point, seed_point)  # (n, n)\n        intensity = (distance_matrix < bandwidth).type(torch.float32).sum(dim=1)\n\n        # merge center if distance between two points less than bandwidth\n        sorted_intensity, indices = torch.sort(intensity, descending=True)\n        is_center = np.ones(n, dtype=np.bool)\n        indices = indices.cpu().numpy()\n        center = np.zeros(n, dtype=np.uint8)\n\n        labels = np.zeros(n, dtype=np.int32)\n        cur_label = 0\n        for i in range(n):\n            if is_center[i]:\n                labels[indices[i]] = cur_label\n                center[indices[i]] = 1\n                for j in range(i + 1, n):\n                    if is_center[j]:\n                        if distance_matrix[indices[i], indices[j]] < bandwidth:\n                            is_center[j] = 0\n                            labels[indices[j]] = cur_label\n                cur_label += 1\n        # print(labels)\n        # print(center)\n        # return seed_point[torch.ByteTensor(center)]\n\n        # change mask select to matrix multiply to select points\n        one_hot = self.label2onehot(torch.Tensor(labels).view(-1, 1))  # (n, label_num)\n        weight = one_hot / one_hot.sum(dim=0, keepdim=True)   # (n, label_num)\n\n        return torch.matmul(weight.t(), seed_point)\n\n    def cluster(self, point, center):\n        """"""\n        cluter each point to nearset center\n        :param point: tensor with size (K, 2)\n        :param center: tensor with size (n, 2)\n        :return: clustering results, tensor with size (K, n) and sum to one for each row\n        """"""\n        # plus 0.01 to avoid divide by zero\n        distance_matrix = 1. / (self.cal_distance_matrix(point, center)+0.01)  # (K, n)\n        segmentation = F.softmax(distance_matrix, dim=1)\n        return segmentation\n\n    def bin_shift(self, prob, embedding, param, gt_seg, bandwidth):\n        """"""\n        discrete seeding mean shift in training stage\n        :param prob: tensor with size (1, h, w) indicate probability of being plane\n        :param embedding: tensor with size (2, h, w)\n        :param param: tensor with size (3, h, w)\n        :param gt_seg: ground truth instance segmentation, used for sampling planar embeddings\n        :param bandwidth: float\n        :return: segmentation results, tensor with size (h*w, K), K is cluster number, row sum to 1\n                 sampled segmentation results, tensor with size (N, K) where N is sample size, K is cluster number, row sum to 1\n                center, tensor with size (K, 2) cluster center in embedding space\n                sample_prob, tensor with size (N, 1) sampled probability\n                sample_seg, tensor with size (N, 1) sampled ground truth instance segmentation\n                sample_params, tensor with size (3, N), sampled params\n        """"""\n\n        c, h, w = embedding.size()\n\n        embedding = embedding.view(c, h*w).t()\n        param = param.view(3, h*w)\n        prob = prob.view(h*w, 1)\n        seg = gt_seg.view(-1)\n\n        # random sample planar region data points using ground truth label to speed up training\n        rand_index = np.random.choice(np.arange(0, h * w)[seg.cpu().numpy() != 20], self.sample_num)\n\n        sample_embedding = embedding[rand_index]\n        sample_prob = prob[rand_index]\n        sample_param = param[:, rand_index]\n\n        # generate seed points and filter out those with low density to speed up training\n        seed_point = self.generate_seed(sample_embedding, self.anchor_num)\n        seed_point = self.filter_seed(sample_embedding, sample_prob, seed_point, bandwidth=self.bandwidth, min_count=3)\n        if torch.numel(seed_point) <= 0:\n            return None, None, None, None, None, None\n\n        with torch.no_grad():\n            for iter in range(self.train_iter):\n                seed_point = self.shift(sample_embedding, sample_prob, seed_point, self.bandwidth)\n\n        # filter again and merge seed points\n        seed_point = self.filter_seed(sample_embedding, sample_prob, seed_point, bandwidth=self.bandwidth, min_count=10)\n        if torch.numel(seed_point) <= 0:\n            return None, None, None, None, None, None\n\n        center = self.merge_center(seed_point, bandwidth=self.bandwidth)\n\n        # cluster points\n        segmentation = self.cluster(embedding, center)\n        sampled_segmentation = segmentation[rand_index]\n\n        return segmentation, sampled_segmentation, center, sample_prob, seg[rand_index].view(-1, 1), sample_param\n\n    def forward(self, logit, embedding, param, gt_seg):\n        batch_size, c, h, w = embedding.size()\n        assert(c == 2)\n\n        # apply mean shift to every item\n        segmentations, sample_segmentations, centers, sample_probs, sample_gt_segs, sample_params = [], [], [], [], [], []\n        for b in range(batch_size):\n            segmentation, sample_segmentation, center, prob, sample_seg, sample_param = \\\n                self.bin_shift(torch.sigmoid(logit[b]), embedding[b], param[b], gt_seg[b], self.bandwidth)\n\n            segmentations.append(segmentation)\n            sample_segmentations.append(sample_segmentation)\n            centers.append(center)\n            sample_probs.append(prob)\n            sample_gt_segs.append(sample_seg)\n            sample_params.append(sample_param)\n\n        return segmentations, sample_segmentations, sample_params, centers, sample_probs, sample_gt_segs\n\n    def test_forward(self, prob, embedding, param, mask_threshold):\n        """"""\n        :param prob: probability of planar, tensor with size (1, h, w)\n        :param embedding: tensor with size (2, h, w)\n        :param mask_threshold: threshold of planar region\n        :return: clustering results: numpy array with shape (h, w),\n                 sampled segmentation results, tensor with size (N, K) where N is sample size, K is cluster number, row sum to 1\n                 sample_params, tensor with size (3, N), sampled params\n        """"""\n\n        c, h, w = embedding.size()\n\n        embedding = embedding.view(c, h*w).t()\n        prob = prob.view(h*w, 1)\n        param = param.view(3, h * w)\n\n        # random sample planar region data points\n        rand_index = np.random.choice(np.arange(0, h * w)[prob.cpu().numpy().reshape(-1) > mask_threshold], self.sample_num)\n\n        sample_embedding = embedding[rand_index]\n        sample_prob = prob[rand_index]\n        sample_param = param[:, rand_index]\n\n        # generate seed points and filter out those with low density\n        seed_point = self.generate_seed(sample_embedding, self.anchor_num)\n        seed_point = self.filter_seed(sample_embedding, sample_prob, seed_point, bandwidth=self.bandwidth, min_count=3)\n\n        with torch.no_grad():\n            # start shift points\n            for iter in range(self.test_iter):\n                seed_point = self.shift(sample_embedding, sample_prob, seed_point, self.bandwidth)\n\n        # filter again and merge seed points\n        seed_point = self.filter_seed(sample_embedding, sample_prob, seed_point, bandwidth=self.bandwidth, min_count=10)\n\n        center = self.merge_center(seed_point, bandwidth=self.bandwidth)\n\n        # cluster points using sample_embedding\n        segmentation = self.cluster(embedding, center)\n\n        sampled_segmentation = segmentation[rand_index]\n\n        return segmentation, sampled_segmentation, sample_param\n\n'"
instance_parameter_loss.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass InstanceParameterLoss(nn.Module):\n    def __init__(self, k_inv_dot_xy1):\n        super(InstanceParameterLoss, self).__init__()\n        self.k_inv_dot_xy1 = k_inv_dot_xy1\n\n    def forward(self, segmentation, sample_segmentation, sample_params, valid_region, gt_depth, return_loss=True):\n        """"""\n        calculate loss of parameters\n        first we combine sample segmentation with sample params to get K plane parameters\n        then we used this parameter to infer plane based Q loss as done in PlaneRecover\n        the loss enforce parameter is consistent with ground truth depth\n\n        :param segmentation: tensor with size (h*w, K)\n        :param sample_segmentation: tensor with size (N, K)\n        :param sample_params: tensor with size (3, N), defined as n / d\n        :param valid_region: tensor with size (1, 1, h, w), indicate planar region\n        :param gt_depth: tensor with size (1, 1, h, w)\n        :param return_loss: bool\n        :return: loss\n                 inferred depth with size (1, 1, h, w) corresponded to instance parameters\n        """"""\n\n        n = sample_segmentation.size(0)\n        _, _, h, w = gt_depth.size()\n        assert (segmentation.size(1) == sample_segmentation.size(1) and segmentation.size(0) == h*w\n                and sample_params.size(1) == sample_segmentation.size(0))\n\n        # combine sample segmentation and sample params to get instance parameters\n        if not return_loss:\n            sample_segmentation[sample_segmentation < 0.5] = 0.\n        weight_matrix = F.normalize(sample_segmentation, p=1, dim=0)\n        instance_param = torch.matmul(sample_params, weight_matrix)      # (3, K)\n\n        # infer depth for every pixels and select the one with highest probability\n        depth_maps = 1. / torch.matmul(instance_param.t(), self.k_inv_dot_xy1)     # (K, h*w)\n        _, index = segmentation.max(dim=1)\n        inferred_depth = depth_maps.t()[range(h*w), index].view(1, 1, h, w)\n\n        if not return_loss:\n            return _, inferred_depth, _, instance_param\n\n        # select valid region\n        valid_region = ((valid_region + (gt_depth != 0.0) ) == 2).view(-1)\n        ray = self.k_inv_dot_xy1[:,  valid_region]                       # (3, N)\n        segmentation = segmentation[valid_region]                        # (N, K)\n        valid_depth = gt_depth.view(1, -1)[:, valid_region]              # (1, N)\n        valid_inferred_depth = inferred_depth.view(1, -1)[:, valid_region]\n\n        # Q_loss for every instance\n        Q = valid_depth * ray                                          # (3, N)\n        Q_loss = torch.abs(torch.matmul(instance_param.t(), Q) - 1.)   # (K, N)\n\n        # weight Q_loss with probability\n        weighted_Q_loss = Q_loss * segmentation.t()                    # (K, N)\n\n        loss = torch.sum(torch.mean(weighted_Q_loss, dim=1))\n\n        # abs distance for valid infered depth\n        abs_distance = torch.mean(torch.abs(valid_inferred_depth - valid_depth))\n\n        return loss, inferred_depth, abs_distance, instance_param\n'"
main.py,25,"b'import os\nimport cv2\nimport time\nimport random\nimport pickle\nimport numpy as np\nfrom PIL import Image\nfrom distutils.version import LooseVersion\n\nfrom sacred import Experiment\nfrom easydict import EasyDict as edict\n\nimport torch\nfrom torch.utils import data\nimport torch.nn.functional as F\nimport torchvision.transforms as tf\n\nfrom models.baseline_same import Baseline as UNet\nfrom utils.loss import hinge_embedding_loss, surface_normal_loss, parameter_loss, \\\n    class_balanced_cross_entropy_loss\nfrom utils.misc import AverageMeter, get_optimizer\nfrom utils.metric import eval_iou, eval_plane_prediction\nfrom utils.disp import tensor_to_image\nfrom utils.disp import colors_256 as colors\nfrom bin_mean_shift import Bin_Mean_Shift\nfrom modules import get_coordinate_map\nfrom utils.loss import Q_loss\nfrom instance_parameter_loss import InstanceParameterLoss\nfrom match_segmentation import MatchSegmentation\n\nex = Experiment()\n\n\nclass PlaneDataset(data.Dataset):\n    def __init__(self, subset=\'train\', transform=None, root_dir=None):\n        assert subset in [\'train\', \'val\']\n        self.subset = subset\n        self.transform = transform\n        self.root_dir = os.path.join(root_dir, subset)\n        self.txt_file = os.path.join(root_dir, subset + \'.txt\')\n\n        self.data_list = [line.strip() for line in open(self.txt_file, \'r\').readlines()]\n        self.precompute_K_inv_dot_xy_1()\n\n    def get_plane_parameters(self, plane, plane_nums, segmentation):\n        valid_region = segmentation != 20\n\n        plane = plane[:plane_nums]\n\n        tmp = plane[:, 1].copy()\n        plane[:, 1] = -plane[:, 2]\n        plane[:, 2] = tmp\n\n        # convert plane from n * d to n / d\n        plane_d = np.linalg.norm(plane, axis=1)\n        # normalize\n        plane /= plane_d.reshape(-1, 1)\n        # n / d\n        plane /= plane_d.reshape(-1, 1)\n\n        h, w = segmentation.shape\n        plane_parameters = np.ones((3, h, w))\n        for i in range(h):\n            for j in range(w):\n                d = segmentation[i, j]\n                if d >= 20: continue\n                plane_parameters[:, i, j] = plane[d, :]\n\n        # plane_instance parameter, padding zero to fix size\n        plane_instance_parameter = np.concatenate((plane, np.zeros((20-plane.shape[0], 3))), axis=0)\n        return plane_parameters, valid_region, plane_instance_parameter\n\n    def precompute_K_inv_dot_xy_1(self, h=192, w=256):\n        focal_length = 517.97\n        offset_x = 320\n        offset_y = 240\n\n        K = [[focal_length, 0, offset_x],\n             [0, focal_length, offset_y],\n             [0, 0, 1]]\n\n        K_inv = np.linalg.inv(np.array(K))\n        self.K_inv = K_inv\n\n        K_inv_dot_xy_1 = np.zeros((3, h, w))\n        for y in range(h):\n            for x in range(w):\n                yy = float(y) / h * 480\n                xx = float(x) / w * 640\n                \n                ray = np.dot(self.K_inv,\n                             np.array([xx, yy, 1]).reshape(3, 1))\n                K_inv_dot_xy_1[:, y, x] = ray[:, 0]\n\n        # precompute to speed up processing\n        self.K_inv_dot_xy_1 = K_inv_dot_xy_1\n\n    def plane2depth(self, plane_parameters, num_planes, segmentation, gt_depth, h=192, w=256):\n            \n        depth_map = 1. / np.sum(self.K_inv_dot_xy_1.reshape(3, -1) * plane_parameters.reshape(3, -1), axis=0)\n        depth_map = depth_map.reshape(h, w)\n\n        # replace non planer region depth using sensor depth map\n        depth_map[segmentation == 20] = gt_depth[segmentation == 20]\n        return depth_map\n\n    def __getitem__(self, index):\n        if self.subset == \'train\':\n            data_path = self.data_list[index]\n        else:\n            data_path = str(index) + \'.npz\'\n        data_path = os.path.join(self.root_dir, data_path)\n        data = np.load(data_path)\n\n        image = data[\'image\']\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = Image.fromarray(image)\n\n        if self.transform is not None:\n            image = self.transform(image)\n\n        plane = data[\'plane\']\n        num_planes = data[\'num_planes\'][0]\n\n        gt_segmentation = data[\'segmentation\']\n        gt_segmentation = gt_segmentation.reshape((192, 256))\n        segmentation = np.zeros([21, 192, 256], dtype=np.uint8)\n\n        _, h, w = segmentation.shape\n        for i in range(num_planes+1):\n            # deal with backgroud\n            if i == num_planes:\n                seg = gt_segmentation == 20\n            else:\n                seg = gt_segmentation == i\n\n            segmentation[i, :, :] = seg.reshape(h, w)\n\n        # surface plane parameters\n        plane_parameters, valid_region, plane_instance_parameter = \\\n            self.get_plane_parameters(plane, num_planes, gt_segmentation)\n\n        # since some depth is missing, we use plane to recover those depth following PlaneNet\n        gt_depth = data[\'depth\'].reshape(192, 256)\n        depth = self.plane2depth(plane_parameters, num_planes, gt_segmentation, gt_depth).reshape(1, 192, 256)\n\n        sample = {\n            \'image\': image,\n            \'num_planes\': num_planes,\n            \'instance\': torch.ByteTensor(segmentation),\n            # one for planar and zero for non-planar\n            \'semantic\': 1 - torch.FloatTensor(segmentation[num_planes, :, :]).unsqueeze(0),\n            \'gt_seg\': torch.LongTensor(gt_segmentation),\n            \'depth\': torch.FloatTensor(depth),\n            \'plane_parameters\': torch.FloatTensor(plane_parameters),\n            \'valid_region\': torch.ByteTensor(valid_region.astype(np.uint8)).unsqueeze(0),\n            \'plane_instance_parameter\': torch.FloatTensor(plane_instance_parameter)\n        }\n\n        return sample\n\n    def __len__(self):\n        return len(self.data_list)\n\n\ndef load_dataset(subset, cfg):\n    transforms = tf.Compose([\n        tf.ToTensor(),\n        tf.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    is_shuffle = subset == \'train\'\n    loaders = data.DataLoader(\n        PlaneDataset(subset=subset, transform=transforms, root_dir=cfg.root_dir),\n        batch_size=cfg.batch_size, shuffle=is_shuffle, num_workers=cfg.num_workers\n    )\n\n    return loaders\n\n\n@ex.command\ndef train(_run, _log):\n    cfg = edict(_run.config)\n\n    torch.manual_seed(cfg.seed)\n    np.random.seed(cfg.seed)\n    random.seed(cfg.seed)\n\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    if not (_run._id is None):\n        checkpoint_dir = os.path.join(_run.observers[0].basedir, str(_run._id), \'checkpoints\')\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n\n    # build network\n    network = UNet(cfg.model)\n\n    if not (cfg.resume_dir == \'None\'):\n        model_dict = torch.load(cfg.resume_dir, map_location=lambda storage, loc: storage)\n        network.load_state_dict(model_dict)\n\n    # load nets into gpu\n    if cfg.num_gpus > 1 and torch.cuda.is_available():\n        network = torch.nn.DataParallel(network)\n    network.to(device)\n\n    # set up optimizers\n    optimizer = get_optimizer(network.parameters(), cfg.solver)\n\n    # data loader\n    data_loader = load_dataset(\'train\', cfg.dataset)\n\n    # save losses per epoch\n    history = {\'losses\': [], \'losses_pull\': [], \'losses_push\': [],\n               \'losses_binary\': [], \'losses_depth\': [], \'ioues\': [], \'rmses\': []}\n\n    network.train(not cfg.model.fix_bn)\n\n    bin_mean_shift = Bin_Mean_Shift(device=device)\n    k_inv_dot_xy1 = get_coordinate_map(device)\n    instance_parameter_loss = InstanceParameterLoss(k_inv_dot_xy1)\n\n    # main loop\n    for epoch in range(cfg.num_epochs):\n        batch_time = AverageMeter()\n        losses = AverageMeter()\n        losses_pull = AverageMeter()\n        losses_push = AverageMeter()\n        losses_binary = AverageMeter()\n        losses_depth = AverageMeter()\n        losses_normal = AverageMeter()\n        losses_instance = AverageMeter()\n        ioues = AverageMeter()\n        rmses = AverageMeter()\n        instance_rmses = AverageMeter()\n        mean_angles = AverageMeter()\n\n        tic = time.time()\n        for iter, sample in enumerate(data_loader):\n            image = sample[\'image\'].to(device)\n            instance = sample[\'instance\'].to(device)\n            semantic = sample[\'semantic\'].to(device)\n            gt_depth = sample[\'depth\'].to(device)\n            gt_seg = sample[\'gt_seg\'].to(device)\n            gt_plane_parameters = sample[\'plane_parameters\'].to(device)\n            valid_region = sample[\'valid_region\'].to(device)\n            gt_plane_instance_parameter = sample[\'plane_instance_parameter\'].to(device)\n\n            # forward pass\n            logit, embedding, _, _, param = network(image)\n\n            segmentations, sample_segmentations, sample_params, centers, sample_probs, sample_gt_segs = \\\n                bin_mean_shift(logit, embedding, param, gt_seg)\n\n            # calculate loss\n            loss, loss_pull, loss_push, loss_binary, loss_depth, loss_normal, loss_parameters, loss_pw, loss_instance \\\n                = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n            batch_size = image.size(0)\n            for i in range(batch_size):\n                _loss, _loss_pull, _loss_push = hinge_embedding_loss(embedding[i:i+1], sample[\'num_planes\'][i:i+1],\n                                                                     instance[i:i+1], device)\n\n                _loss_binary = class_balanced_cross_entropy_loss(logit[i], semantic[i])\n\n                _loss_normal, mean_angle = surface_normal_loss(param[i:i+1], gt_plane_parameters[i:i+1],\n                                                               valid_region[i:i+1])\n\n                _loss_L1 = parameter_loss(param[i:i + 1], gt_plane_parameters[i:i + 1], valid_region[i:i + 1])\n                _loss_depth, rmse, infered_depth = Q_loss(param[i:i+1], k_inv_dot_xy1, gt_depth[i:i+1])\n\n                if segmentations[i] is None:\n                    continue\n\n                _instance_loss, instance_depth, instance_abs_disntace, _ = \\\n                    instance_parameter_loss(segmentations[i], sample_segmentations[i], sample_params[i],\n                                            valid_region[i:i+1], gt_depth[i:i+1])\n\n                _loss += _loss_binary + _loss_depth + _loss_normal + _instance_loss + _loss_L1\n\n                # planar segmentation iou\n                prob = torch.sigmoid(logit[i])\n                mask = (prob > 0.5).float().cpu().numpy()\n                iou = eval_iou(mask, semantic[i].cpu().numpy())\n                ioues.update(iou * 100)\n                instance_rmses.update(instance_abs_disntace.item())\n                rmses.update(rmse.item())\n                mean_angles.update(mean_angle.item())\n\n                loss += _loss\n                loss_pull += _loss_pull\n                loss_push += _loss_push\n                loss_binary += _loss_binary\n                loss_depth += _loss_depth\n                loss_normal += _loss_normal\n                loss_instance += _instance_loss\n\n            loss /= batch_size\n            loss_pull /= batch_size\n            loss_push /= batch_size\n            loss_binary /= batch_size\n            loss_depth /= batch_size\n            loss_normal /= batch_size\n            loss_instance /= batch_size\n\n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # update loss\n            losses.update(loss.item())\n            losses_pull.update(loss_pull.item())\n            losses_push.update(loss_push.item())\n            losses_binary.update(loss_binary.item())\n            losses_depth.update(loss_depth.item())\n            losses_normal.update(loss_normal.item())\n            losses_instance.update(loss_instance.item())\n\n            # update time\n            batch_time.update(time.time() - tic)\n            tic = time.time()\n\n            if iter % cfg.print_interval == 0:\n                _log.info(f""[{epoch:2d}][{iter:5d}/{len(data_loader):5d}] ""\n                          f""Time: {batch_time.val:.2f} ({batch_time.avg:.2f}) ""\n                          f""Loss: {losses.val:.4f} ({losses.avg:.4f}) ""\n                          f""Pull: {losses_pull.val:.4f} ({losses_pull.avg:.4f}) ""\n                          f""Push: {losses_push.val:.4f} ({losses_push.avg:.4f}) ""\n                          f""INS: {losses_instance.val:.4f} ({losses_instance.avg:.4f}) ""\n                          f""Binary: {losses_binary.val:.4f} ({losses_binary.avg:.4f}) ""\n                          f""IoU: {ioues.val:.2f} ({ioues.avg:.2f}) ""\n                          f""LN: {losses_normal.val:.4f} ({losses_normal.avg:.4f}) ""\n                          f""AN: {mean_angles.val:.4f} ({mean_angles.avg:.4f}) ""\n                          f""Depth: {losses_depth.val:.4f} ({losses_depth.avg:.4f}) ""\n                          f""INSDEPTH: {instance_rmses.val:.4f} ({instance_rmses.avg:.4f}) ""\n                          f""RMSE: {rmses.val:.4f} ({rmses.avg:.4f}) "")\n\n        _log.info(f""* epoch: {epoch:2d}\\t""\n                  f""Loss: {losses.avg:.6f}\\t""\n                  f""Pull: {losses_pull.avg:.6f}\\t""\n                  f""Push: {losses_push.avg:.6f}\\t""\n                  f""Binary: {losses_binary.avg:.6f}\\t""\n                  f""Depth: {losses_depth.avg:.6f}\\t""\n                  f""IoU: {ioues.avg:.2f}\\t""\n                  f""RMSE: {rmses.avg:.4f}\\t"")\n\n        # save history\n        history[\'losses\'].append(losses.avg)\n        history[\'losses_pull\'].append(losses_pull.avg)\n        history[\'losses_push\'].append(losses_push.avg)\n        history[\'losses_binary\'].append(losses_binary.avg)\n        history[\'losses_depth\'].append(losses_depth.avg)\n        history[\'ioues\'].append(ioues.avg)\n        history[\'rmses\'].append(rmses.avg)\n\n        # save checkpoint\n        if not (_run._id is None):\n            torch.save(network.state_dict(), os.path.join(checkpoint_dir, f""network_epoch_{epoch}.pt""))\n            pickle.dump(history, open(os.path.join(checkpoint_dir, \'history.pkl\'), \'wb\'))\n\n\n@ex.command\ndef eval(_run, _log):\n    cfg = edict(_run.config)\n\n    torch.manual_seed(cfg.seed)\n    np.random.seed(cfg.seed)\n    random.seed(cfg.seed)\n\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    if not (_run._id is None):\n        checkpoint_dir = os.path.join(\'experiments\', str(_run._id), \'checkpoints\')\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n\n    # build network\n    network = UNet(cfg.model)\n\n    if not (cfg.resume_dir == \'None\'):\n        model_dict = torch.load(cfg.resume_dir, map_location=lambda storage, loc: storage)\n        network.load_state_dict(model_dict)\n\n    # load nets into gpu\n    if cfg.num_gpus > 1 and torch.cuda.is_available():\n        network = torch.nn.DataParallel(network)\n    network.to(device)\n    network.eval()\n\n    # data loader\n    data_loader = load_dataset(\'val\', cfg.dataset)\n\n    pixel_recall_curve = np.zeros((13))\n    plane_recall_curve = np.zeros((13, 3))\n\n    bin_mean_shift = Bin_Mean_Shift(device=device)\n    k_inv_dot_xy1 = get_coordinate_map(device)\n    instance_parameter_loss = InstanceParameterLoss(k_inv_dot_xy1)\n    match_segmentatin = MatchSegmentation()\n\n    with torch.no_grad():\n        for iter, sample in enumerate(data_loader):\n            image = sample[\'image\'].to(device)\n            instance = sample[\'instance\'].to(device)\n            gt_seg = sample[\'gt_seg\'].numpy()\n            semantic = sample[\'semantic\'].to(device)\n            gt_depth = sample[\'depth\'].to(device)\n            # gt_plane_parameters = sample[\'plane_parameters\'].to(device)\n            valid_region = sample[\'valid_region\'].to(device)\n            gt_plane_num = sample[\'num_planes\'].int()\n            # gt_plane_instance_parameter = sample[\'plane_instance_parameter\'].numpy()\n            \n            # forward pass\n            logit, embedding, _, _, param = network(image)\n\n            prob = torch.sigmoid(logit[0])\n            \n            # infer per pixel depth using per pixel plane parameter\n            _, _, per_pixel_depth = Q_loss(param, k_inv_dot_xy1, gt_depth)\n\n            # fast mean shift\n            segmentation, sampled_segmentation, sample_param = bin_mean_shift.test_forward(\n                prob, embedding[0], param, mask_threshold=0.1)\n\n            # since GT plane segmentation is somewhat noise, the boundary of plane in GT is not well aligned, \n            # we thus use avg_pool_2d to smooth the segmentation results\n            b = segmentation.t().view(1, -1, 192, 256)\n            pooling_b = torch.nn.functional.avg_pool2d(b, (7, 7), stride=1, padding=(3, 3))\n            b = pooling_b.view(-1, 192*256).t()\n            segmentation = b\n\n            # infer instance depth\n            instance_loss, instance_depth, instance_abs_disntace, instance_parameter = \\\n                instance_parameter_loss(segmentation, sampled_segmentation, sample_param,\n                                        valid_region, gt_depth, False)\n\n            # greedy match of predict segmentation and ground truth segmentation using cross entropy\n            # to better visualization\n            matching = match_segmentatin(segmentation, prob.view(-1, 1), instance[0], gt_plane_num)\n\n            # return cluster results\n            predict_segmentation = segmentation.cpu().numpy().argmax(axis=1)\n\n            # reindexing to matching gt segmentation for better visualization\n            matching = matching.cpu().numpy().reshape(-1)\n            used = set([])\n            max_index = max(matching) + 1\n            for i, a in zip(range(len(matching)), matching):\n                if a in used:\n                    matching[i] = max_index\n                    max_index += 1\n                else:\n                    used.add(a)\n            predict_segmentation = matching[predict_segmentation]\n\n            # mask out non planar region\n            predict_segmentation[prob.cpu().numpy().reshape(-1) <= 0.1] = 20\n            predict_segmentation = predict_segmentation.reshape(192, 256)\n\n            # visualization and evaluation\n            h, w = 192, 256\n            image = tensor_to_image(image.cpu()[0])\n            semantic = semantic.cpu().numpy().reshape(h, w)\n            mask = (prob > 0.1).float().cpu().numpy().reshape(h, w)\n            gt_seg = gt_seg.reshape(h, w)\n            depth = instance_depth.cpu().numpy()[0, 0].reshape(h, w)\n            per_pixel_depth = per_pixel_depth.cpu().numpy()[0, 0].reshape(h, w)\n\n            # use per pixel depth for non planar region\n            depth = depth * (predict_segmentation != 20) + per_pixel_depth * (predict_segmentation == 20)\n            gt_depth = gt_depth.cpu().numpy()[0, 0].reshape(h, w)\n\n            # evaluation plane segmentation\n            pixelStatistics, planeStatistics = eval_plane_prediction(\n                predict_segmentation, gt_seg, depth, gt_depth)\n\n            pixel_recall_curve += np.array(pixelStatistics)\n            plane_recall_curve += np.array(planeStatistics)\n\n            print(""pixel and plane recall of test image "", iter)\n            print(pixel_recall_curve / float(iter+1))\n            print(plane_recall_curve[:, 0] / plane_recall_curve[:, 1])\n            print(""********"")\n\n            # visualization convert labels to color image\n            # change non-planar regions to zero, so non-planar regions use the black color\n            gt_seg += 1\n            gt_seg[gt_seg == 21] = 0\n            predict_segmentation += 1\n            predict_segmentation[predict_segmentation == 21] = 0\n\n            gt_seg_image = cv2.resize(np.stack([colors[gt_seg, 0],\n                                                colors[gt_seg, 1],\n                                                colors[gt_seg, 2]], axis=2), (w, h))\n            pred_seg = cv2.resize(np.stack([colors[predict_segmentation, 0],\n                                            colors[predict_segmentation, 1],\n                                            colors[predict_segmentation, 2]], axis=2), (w, h))\n\n            # blend image\n            blend_pred = (pred_seg * 0.7 + image * 0.3).astype(np.uint8)\n            blend_gt = (gt_seg_image * 0.7 + image * 0.3).astype(np.uint8)\n\n            semantic = cv2.resize((semantic * 255).astype(np.uint8), (w, h))\n            semantic = cv2.cvtColor(semantic, cv2.COLOR_GRAY2BGR)\n\n            mask = cv2.resize((mask * 255).astype(np.uint8), (w, h))\n            mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n\n            depth_diff = np.abs(gt_depth - depth)\n            depth_diff[gt_depth == 0.] = 0\n\n            # visualize depth map as PlaneNet\n            depth_diff = np.clip(depth_diff / 5 * 255, 0, 255).astype(np.uint8)\n            depth_diff = cv2.cvtColor(cv2.resize(depth_diff, (w, h)), cv2.COLOR_GRAY2BGR)\n\n            depth = 255 - np.clip(depth / 5 * 255, 0, 255).astype(np.uint8)\n            depth = cv2.cvtColor(cv2.resize(depth, (w, h)), cv2.COLOR_GRAY2BGR)\n\n            gt_depth = 255 - np.clip(gt_depth / 5 * 255, 0, 255).astype(np.uint8)\n            gt_depth = cv2.cvtColor(cv2.resize(gt_depth, (w, h)), cv2.COLOR_GRAY2BGR)\n\n            image_1 = np.concatenate((image, pred_seg, gt_seg_image), axis=1)\n            image_2 = np.concatenate((image, blend_pred, blend_gt), axis=1)\n            image_3 = np.concatenate((image, mask, semantic), axis=1)\n            image_4 = np.concatenate((depth_diff, depth, gt_depth), axis=1)\n            image = np.concatenate((image_1, image_2, image_3, image_4), axis=0)\n\n            # cv2.imshow(\'image\', image)\n            # cv2.waitKey(0)\n            # cv2.imwrite(""%d_segmentation.png""%iter, image)\n\n        print(""========================================"")\n        print(""pixel and plane recall of all test image"")\n        print(pixel_recall_curve / len(data_loader))\n        print(plane_recall_curve[:, 0] / plane_recall_curve[:, 1])\n        print(""****************************************"")\n\n\nif __name__ == \'__main__\':\n    assert LooseVersion(torch.__version__) >= LooseVersion(\'0.4.0\'), \\\n        \'PyTorch>=0.4.0 is required\'\n\n    ex.add_config(\'./configs/config.yaml\')\n    ex.run_commandline()\n'"
match_segmentation.py,6,"b'import torch\nimport torch.nn as nn\n\n\nclass MatchSegmentation(nn.Module):\n    def __init__(self):\n        super(MatchSegmentation, self).__init__()\n\n    def forward(self, segmentation, prob, gt_instance, gt_plane_num):\n        """"""\n        greedy matching\n        match segmentation with ground truth instance \n        :param segmentation: tensor with size (N, K)\n        :param prob: tensor with size (N, 1)\n        :param gt_instance: tensor with size (21, h, w)\n        :param gt_plane_num: int\n        :return: a (K, 1) long tensor indicate closest ground truth instance id, start from 0\n        """"""\n\n        n, k = segmentation.size()\n        _, h, w = gt_instance.size()\n        assert (prob.size(0) == n and h*w  == n)\n        \n        # ingnore non planar region\n        gt_instance = gt_instance[:gt_plane_num, :, :].view(1, -1, h*w)     # (1, gt_plane_num, h*w)\n\n        segmentation = segmentation.t().view(k, 1, h*w)                     # (k, 1, h*w)\n\n        # calculate instance wise cross entropy matrix (K, gt_plane_num)\n        gt_instance = gt_instance.type(torch.float32)\n\n        ce_loss = - (gt_instance * torch.log(segmentation + 1e-6) +\n            (1-gt_instance) * torch.log(1-segmentation + 1e-6))             # (k, gt_plane_num, k*w)\n\n        ce_loss = torch.mean(ce_loss, dim=2)                                # (k, gt_plane_num)\n        \n        matching = torch.argmin(ce_loss, dim=1, keepdim=True)\n\n        return matching\n\n'"
modules.py,6,"b'import torch\nimport numpy as np\n\n\ndef get_coordinate_map(device):\n    # define K for PlaneNet dataset\n    focal_length = 517.97\n    offset_x = 320\n    offset_y = 240\n\n    K = [[focal_length, 0, offset_x],\n         [0, focal_length, offset_y],\n         [0, 0, 1]]\n    K_inv = np.linalg.inv(np.array(K))\n\n    K = torch.FloatTensor(K).to(device)\n    K_inv = torch.FloatTensor(K_inv).to(device)\n\n    h, w = 192, 256\n\n    x = torch.arange(w, dtype=torch.float32).view(1, w) / w * 640\n    y = torch.arange(h, dtype=torch.float32).view(h, 1) / h * 480\n\n    x = x.to(device)\n    y = y.to(device)\n    xx = x.repeat(h, 1)\n    yy = y.repeat(1, w)\n    xy1 = torch.stack((xx, yy, torch.ones((h, w), dtype=torch.float32).to(device)))  # (3, h, w)\n    xy1 = xy1.view(3, -1)  # (3, h*w)\n\n    k_inv_dot_xy1 = torch.matmul(K_inv, xy1)  # (3, h*w)\n    return k_inv_dot_xy1\n'"
predict.py,12,"b'import os\nimport cv2\nimport random\nimport numpy as np\nfrom PIL import Image\nfrom distutils.version import LooseVersion\n\nfrom sacred import Experiment\nfrom easydict import EasyDict as edict\n\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as tf\n\nfrom models.baseline_same import Baseline as UNet\nfrom utils.disp import tensor_to_image\nfrom utils.disp import colors_256 as colors\nfrom bin_mean_shift import Bin_Mean_Shift\nfrom modules import get_coordinate_map\nfrom utils.loss import Q_loss\nfrom instance_parameter_loss import InstanceParameterLoss\n\nex = Experiment()\n\n\n@ex.main\ndef predict(_run, _log):\n    cfg = edict(_run.config)\n\n    torch.manual_seed(cfg.seed)\n    np.random.seed(cfg.seed)\n    random.seed(cfg.seed)\n\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    # build network\n    network = UNet(cfg.model)\n\n    if not (cfg.resume_dir == \'None\'):\n        model_dict = torch.load(cfg.resume_dir, map_location=lambda storage, loc: storage)\n        network.load_state_dict(model_dict)\n\n    # load nets into gpu\n    if cfg.num_gpus > 1 and torch.cuda.is_available():\n        network = torch.nn.DataParallel(network)\n    network.to(device)\n    network.eval()\n\n    transforms = tf.Compose([\n        tf.ToTensor(),\n        tf.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\n    bin_mean_shift = Bin_Mean_Shift(device=device)\n    k_inv_dot_xy1 = get_coordinate_map(device)\n    instance_parameter_loss = InstanceParameterLoss(k_inv_dot_xy1)\n\n    h, w = 192, 256\n\n    with torch.no_grad():\n        image = cv2.imread(cfg.image_path)\n        # the network is trained with 192*256 and the intrinsic parameter is set as ScanNet\n        image = cv2.resize(image, (w, h))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = Image.fromarray(image)\n        image = transforms(image)\n        image = image.to(device).unsqueeze(0)\n        # forward pass\n        logit, embedding, _, _, param = network(image)\n\n        prob = torch.sigmoid(logit[0])\n        \n        # infer per pixel depth using per pixel plane parameter, currently Q_loss need a dummy gt_depth as input\n        _, _, per_pixel_depth = Q_loss(param, k_inv_dot_xy1, torch.ones_like(logit))\n\n        # fast mean shift\n        segmentation, sampled_segmentation, sample_param = bin_mean_shift.test_forward(\n            prob, embedding[0], param, mask_threshold=0.1)\n\n        # since GT plane segmentation is somewhat noise, the boundary of plane in GT is not well aligned, \n        # we thus use avg_pool_2d to smooth the segmentation results\n        b = segmentation.t().view(1, -1, h, w)\n        pooling_b = torch.nn.functional.avg_pool2d(b, (7, 7), stride=1, padding=(3, 3))\n        b = pooling_b.view(-1, h*w).t()\n        segmentation = b\n\n        # infer instance depth\n        instance_loss, instance_depth, instance_abs_disntace, instance_parameter = instance_parameter_loss(\n            segmentation, sampled_segmentation, sample_param, torch.ones_like(logit), torch.ones_like(logit), False)\n\n        # return cluster results\n        predict_segmentation = segmentation.cpu().numpy().argmax(axis=1)\n\n        # mask out non planar region\n        predict_segmentation[prob.cpu().numpy().reshape(-1) <= 0.1] = 20\n        predict_segmentation = predict_segmentation.reshape(h, w)\n\n        # visualization and evaluation\n        image = tensor_to_image(image.cpu()[0])\n        mask = (prob > 0.1).float().cpu().numpy().reshape(h, w)\n        depth = instance_depth.cpu().numpy()[0, 0].reshape(h, w)\n        per_pixel_depth = per_pixel_depth.cpu().numpy()[0, 0].reshape(h, w)\n\n        # use per pixel depth for non planar region\n        depth = depth * (predict_segmentation != 20) + per_pixel_depth * (predict_segmentation == 20)\n\n        # change non planar to zero, so non planar region use the black color\n        predict_segmentation += 1\n        predict_segmentation[predict_segmentation == 21] = 0\n\n        pred_seg = cv2.resize(np.stack([colors[predict_segmentation, 0],\n                                        colors[predict_segmentation, 1],\n                                        colors[predict_segmentation, 2]], axis=2), (w, h))\n\n        # blend image\n        blend_pred = (pred_seg * 0.7 + image * 0.3).astype(np.uint8)\n\n        mask = cv2.resize((mask * 255).astype(np.uint8), (w, h))\n        mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n\n        # visualize depth map as PlaneNet\n        depth = 255 - np.clip(depth / 5 * 255, 0, 255).astype(np.uint8)\n        depth = cv2.cvtColor(cv2.resize(depth, (w, h)), cv2.COLOR_GRAY2BGR)\n\n        image = np.concatenate((image, pred_seg, blend_pred, mask, depth), axis=1)\n\n        cv2.imshow(\'image\', image)\n        cv2.waitKey(0)\n\n\nif __name__ == \'__main__\':\n    assert LooseVersion(torch.__version__) >= LooseVersion(\'0.4.0\'), \\\n        \'PyTorch>=0.4.0 is required\'\n\n    ex.add_config(\'./configs/predict.yaml\')\n    ex.run_commandline()\n'"
data_tools/RecordReaderAll.py,0,"b""# modified from https://github.com/art-programmer/PlaneNet\nimport tensorflow as tf\n\nHEIGHT=192\nWIDTH=256\nNUM_PLANES = 20\nNUM_THREADS = 4\n\n\nclass RecordReaderAll:\n    def __init__(self):\n        return\n\n    def getBatch(self, filename_queue, batchSize=1, min_after_dequeue=1000,\n                 random=False, getLocal=False, getSegmentation=False, test=True):\n        reader = tf.TFRecordReader()\n        _, serialized_example = reader.read(filename_queue)\n\n        features = tf.parse_single_example(\n            serialized_example,\n            # Defaults are not specified since both keys are required.\n            features={\n                'image_raw': tf.FixedLenFeature([], tf.string),\n                'image_path': tf.FixedLenFeature([], tf.string),\n                'num_planes': tf.FixedLenFeature([], tf.int64),\n                'plane': tf.FixedLenFeature([NUM_PLANES * 3], tf.float32),\n                'segmentation_raw': tf.FixedLenFeature([], tf.string),\n                'depth': tf.FixedLenFeature([HEIGHT * WIDTH], tf.float32),\n                'normal': tf.FixedLenFeature([HEIGHT * WIDTH * 3], tf.float32),\n                'semantics_raw': tf.FixedLenFeature([], tf.string),                \n                'boundary_raw': tf.FixedLenFeature([], tf.string),\n                'info': tf.FixedLenFeature([4 * 4 + 4], tf.float32),                \n            })\n\n        # Convert from a scalar string tensor (whose single string has\n        # length mnist.IMAGE_PIXELS) to a uint8 tensor with shape\n        # [mnist.IMAGE_PIXELS].\n        image = tf.decode_raw(features['image_raw'], tf.uint8)\n        image = tf.reshape(image, [HEIGHT, WIDTH, 3])\n\n        depth = features['depth']\n        depth = tf.reshape(depth, [HEIGHT, WIDTH, 1])\n\n        normal = features['normal']\n        normal = tf.reshape(normal, [HEIGHT, WIDTH, 3])\n\n        semantics = tf.decode_raw(features['semantics_raw'], tf.uint8)\n        semantics = tf.cast(tf.reshape(semantics, [HEIGHT, WIDTH]), tf.int32)\n\n        numPlanes = tf.cast(features['num_planes'], tf.int32)\n\n        planes = features['plane']\n        planes = tf.reshape(planes, [NUM_PLANES, 3])\n        \n        boundary = tf.decode_raw(features['boundary_raw'], tf.uint8)\n        boundary = tf.cast(tf.reshape(boundary, (HEIGHT, WIDTH, 2)), tf.float32)\n\n        segmentation = tf.decode_raw(features['segmentation_raw'], tf.uint8)\n        segmentation = tf.reshape(segmentation, [HEIGHT, WIDTH, 1])\n\n        image_inp, plane_inp, depth_gt, normal_gt, semantics_gt, segmentation_gt, boundary_gt, num_planes_gt, image_path, info = \\\n            tf.train.batch([image, planes, depth, normal, semantics, segmentation, boundary, numPlanes, features['image_path'], features['info']], batch_size=batchSize, capacity=(NUM_THREADS + 2) * batchSize, num_threads=1)\n        global_gt_dict = {'plane': plane_inp, 'depth': depth_gt, 'normal': normal_gt, 'semantics': semantics_gt,\n                          'segmentation': segmentation_gt, 'boundary': boundary_gt, 'num_planes': num_planes_gt,\n                          'image_path': image_path, 'info': info}\n        return image_inp, global_gt_dict, {}\n"""
data_tools/__init__.py,0,b''
data_tools/convert_tfrecords.py,0,"b'import tensorflow as tf\nimport numpy as np\nimport os\nimport argparse\n\nfrom RecordReaderAll import *\n\nos.environ[\'CUDA_VISIBLE_DEVICES\']=\'\'\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--input_tfrecords_file\', type=str,\n                    help=\'path .tfrecords file\',\n                    required=True)\nparser.add_argument(\'--output_dir\', type=str,\n                    help=\'where to store extracted frames\',\n                    required=True)\nparser.add_argument(\'--data_type\', type=str,\n                    help=\'where to store extracted frames\',\n                    required=True)\nargs = parser.parse_args()\n\ninput_tfrecords_file = args.input_tfrecords_file\noutput_dir = args.output_dir\ndata_type = args.data_type\n\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\n\nif data_type == \'train\':\n    file_list = open(output_dir + \'/train.txt\', \'w\')\n    output_dir = os.path.join(output_dir, \'train\')\n    os.makedirs(output_dir)\n    max_num = 50000\nelif data_type == \'val\':\n    file_list = open(output_dir + \'/val.txt\', \'w\')\n    output_dir = os.path.join(output_dir, \'val\')\n    os.makedirs(output_dir)\n    max_num = 760\nelse:\n    print(""unsupported data type"")\n    exit(-1)\n\n\nreader_train = RecordReaderAll()\nfilename_queue_train = tf.train.string_input_producer([input_tfrecords_file], num_epochs=1)\nimg_inp_train, global_gt_dict_train, local_gt_dict_train = reader_train.getBatch(filename_queue_train, batchSize=1, getLocal=True)\n\n# The op for initializing the variables.\ninit_op = tf.group(tf.global_variables_initializer(),\n                   tf.local_variables_initializer())\n\nwith tf.Session() as sess:\n    sess.run(init_op)\n\n    coord = tf.train.Coordinator()\n    threads = tf.train.start_queue_runners(coord=coord)\n\n    for i in range(max_num):\n        img, gt_dict = sess.run([img_inp_train, global_gt_dict_train])\n        plane = gt_dict[\'plane\'][0]\n        depth = gt_dict[\'depth\'][0]\n        normal = gt_dict[\'normal\'][0]\n        semantics = gt_dict[\'semantics\'][0]\n        segmentation = gt_dict[\'segmentation\'][0]\n        boundary = gt_dict[\'boundary\'][0]\n        num_planes = gt_dict[\'num_planes\'][0].reshape([-1])\n        image_path = gt_dict[\'image_path\'][0]\n        info = gt_dict[\'info\'][0]\n\n        np.savez(os.path.join(output_dir, \'%d.npz\' % (i, )),\n                 image=img[0], plane=plane, depth=depth, normal=normal, semantics=semantics,\n                 segmentation=segmentation, boundary=boundary, num_planes=num_planes,\n                 image_path=image_path, info=info)\n\n        file_list.write(\'%d.npz\\n\' % (i, ))\n\n        if i % 100 == 99: \n            print(i)\n\nfile_list.close()\n'"
models/__init__.py,0,b''
models/baseline_same.py,1,"b""import torch\nimport torch.nn as nn\n\nfrom models import resnet_scene as resnet\n\n\nclass ResNet(nn.Module):\n    def __init__(self, orig_resnet):\n        super(ResNet, self).__init__()\n\n        # take pretrained resnet, except AvgPool and FC\n        self.conv1 = orig_resnet.conv1\n        self.bn1 = orig_resnet.bn1\n        self.relu1 = orig_resnet.relu1\n\n        self.conv2 = orig_resnet.conv2\n        self.bn2 = orig_resnet.bn2\n        self.relu2 = orig_resnet.relu2\n\n        self.conv3 = orig_resnet.conv3\n        self.bn3 = orig_resnet.bn3\n        self.relu3 = orig_resnet.relu3\n\n        self.maxpool = orig_resnet.maxpool\n        self.layer1 = orig_resnet.layer1\n        self.layer2 = orig_resnet.layer2\n        self.layer3 = orig_resnet.layer3\n        self.layer4 = orig_resnet.layer4\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x1 = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x1)\n\n        x2 = self.layer1(x)\n        x3 = self.layer2(x2)\n        x4 = self.layer3(x3)\n        x5 = self.layer4(x4)\n\n        return x1, x2, x3, x4, x5\n\n\nclass Baseline(nn.Module):\n    def __init__(self, cfg):\n        super(Baseline, self).__init__()\n\n        orig_resnet = resnet.__dict__[cfg.arch](pretrained=cfg.pretrained)\n        self.backbone = ResNet(orig_resnet)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        channel = 64\n        # top down\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n        self.up_conv5 = nn.Conv2d(channel, channel, (1, 1))\n        self.up_conv4 = nn.Conv2d(channel, channel, (1, 1))\n        self.up_conv3 = nn.Conv2d(channel, channel, (1, 1))\n        self.up_conv2 = nn.Conv2d(channel, channel, (1, 1))\n        self.up_conv1 = nn.Conv2d(channel, channel, (1, 1))\n        self.up_conv0 = nn.Conv2d(channel, channel, (1, 1))\n\n        # lateral\n        self.c5_conv = nn.Conv2d(2048, channel, (1, 1))\n        self.c4_conv = nn.Conv2d(1024, channel, (1, 1))\n        self.c3_conv = nn.Conv2d(512, channel, (1, 1))\n        self.c2_conv = nn.Conv2d(256, channel, (1, 1))\n        self.c1_conv = nn.Conv2d(128, channel, (1, 1))\n\n        self.p0_conv = nn.Conv2d(channel, channel, (3, 3), padding=1)\n\n        # plane or non-plane classifier\n        self.pred_prob = nn.Conv2d(channel, 1, (1, 1), padding=0)\n        # embedding\n        self.embedding_conv = nn.Conv2d(channel, 2, (1, 1), padding=0)\n        # depth prediction\n        self.pred_depth = nn.Conv2d(channel, 1, (1, 1), padding=0)\n        # surface normal prediction\n        self.pred_surface_normal = nn.Conv2d(channel, 3, (1, 1), padding=0)\n        # surface plane parameters\n        self.pred_param = nn.Conv2d(channel, 3, (1, 1), padding=0)\n\n    def top_down(self, x):\n        c1, c2, c3, c4, c5 = x\n\n        p5 = self.relu(self.c5_conv(c5))\n        p4 = self.up_conv5(self.upsample(p5)) + self.relu(self.c4_conv(c4))\n        p3 = self.up_conv4(self.upsample(p4)) + self.relu(self.c3_conv(c3))\n        p2 = self.up_conv3(self.upsample(p3)) + self.relu(self.c2_conv(c2))\n        p1 = self.up_conv2(self.upsample(p2)) + self.relu(self.c1_conv(c1))\n\n        p0 = self.upsample(p1)\n\n        p0 = self.relu(self.p0_conv(p0))\n\n        return p0, p1, p2, p3, p4, p5\n\n    def forward(self, x):\n        # bottom up\n        c1, c2, c3, c4, c5 = self.backbone(x)\n\n        # top down\n        p0, p1, p2, p3, p4, p5 = self.top_down((c1, c2, c3, c4, c5))\n\n        # output\n        prob = self.pred_prob(p0)\n        embedding = self.embedding_conv(p0)\n        depth = self.pred_depth(p0)\n        surface_normal = self.pred_surface_normal(p0)\n        param = self.pred_param(p0)\n\n        return prob, embedding, depth, surface_normal, param\n"""
models/resnet_scene.py,2,"b'""""""\nCopy from https://github.com/CSAILVision/semantic-segmentation-pytorch\n""""""\n\nimport os\nimport torch\nimport torch.nn as nn\nimport math\n\ntry:\n    from urllib import urlretrieve\nexcept ImportError:\n    from urllib.request import urlretrieve\n\n\n__all__ = [\'ResNet\', \'resnet50\', \'resnet101\']\n\n\nmodel_urls = {\n    \'resnet50\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet50-imagenet.pth\',\n    \'resnet101\': \'http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet101-imagenet.pth\'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 128\n        super(ResNet, self).__init__()\n        self.conv1 = conv3x3(3, 64, stride=2)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(64, 64)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = conv3x3(64, 128)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.relu1(self.bn1(self.conv1(x)))\n        x = self.relu2(self.bn2(self.conv2(x)))\n        x = self.relu3(self.bn3(self.conv3(x)))\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet50(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet50\']), strict=True)\n    return model\n\n\ndef resnet101(pretrained=False, **kwargs):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on Places\n    """"""\n    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(load_url(model_urls[\'resnet101\']), strict=True)\n    return model\n\n\ndef load_url(url):\n    torch_home = os.path.expanduser(os.getenv(\'TORCH_HOME\', \'~/.torch\'))\n    model_dir = os.getenv(\'TORCH_MODEL_ZOO\', os.path.join(torch_home, \'models\'))\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n    filename = url.split(\'/\')[-1]\n    cached_file = os.path.join(model_dir, filename)\n    return torch.load(cached_file, map_location=lambda storage, loc: storage)\n\n\nif __name__ == \'__main__\':\n    model = resnet101(pretrained=True)\n'"
utils/__init__.py,0,b''
utils/disp.py,0,"b'""""""\nCopy from https://github.com/lxx1991/VS-ReID\n""""""\n\nimport os\nimport cv2\nimport numpy as np\nimport torchvision.transforms as transforms\n\n\ndef uint82bin(n, count=8):\n    """"""returns the binary of integer n, count refers to amount of bits""""""\n    return \'\'.join([str((n >> y) & 1) for y in range(count - 1, -1, -1)])\n\n\ndef labelcolormap(N):\n    cmap = np.zeros((N, 3), dtype=np.uint8)\n    for i in range(N):\n        r = 0\n        g = 0\n        b = 0\n        id = i\n        for j in range(7):\n            str_id = uint82bin(id)\n            r = r ^ (np.uint8(str_id[-1]) << (7 - j))\n            g = g ^ (np.uint8(str_id[-2]) << (7 - j))\n            b = b ^ (np.uint8(str_id[-3]) << (7 - j))\n            id = id >> 3\n        cmap[i, 0] = b\n        cmap[i, 1] = g\n        cmap[i, 2] = r\n    return cmap\n\n\ncolors_256 = labelcolormap(256)\n\ncolors = np.array([[255, 0, 0],\n                   [0, 255, 0],\n                   [0, 0, 255],\n                   [80, 128, 255],\n                   [255, 230, 180],\n                   [255, 0, 255],\n                   [0, 255, 255],\n                   [100, 0, 0],\n                   [0, 100, 0],\n                   [255, 255, 0],\n                   [50, 150, 0],\n                   [200, 255, 255],\n                   [255, 200, 255],\n                   [128, 128, 80],\n                   # [0, 50, 128],\n                   # [0, 100, 100],\n                   [0, 255, 128],\n                   [0, 128, 255],\n                   [255, 0, 128],\n                   [128, 0, 255],\n                   [255, 128, 0],\n                   [128, 255, 0],\n                   [0, 0, 0]\n                   ])\n\n\ndef show_frame(pred, image=None, out_file=\'\', vis=False):\n    if vis:\n        result = np.dstack((colors[pred, 0], colors[pred, 1],\n                            colors[pred, 2])).astype(np.uint8)\n\n    if out_file != \'\':\n        if not os.path.exists(os.path.split(out_file)[0]):\n            os.makedirs(os.path.split(out_file)[0])\n        if vis:\n            cv2.imwrite(out_file, result)\n        else:\n            cv2.imwrite(out_file, pred)\n\n    if vis and image is not None:\n        temp = image.astype(float) * 0.4 + result.astype(float) * 0.6\n        cv2.imshow(\'Result\', temp.astype(np.uint8))\n        cv2.waitKey()\n\n\nTensor_to_Image = transforms.Compose([\n    transforms.Normalize([0.0, 0.0, 0.0], [1.0/0.229, 1.0/0.224, 1.0/0.225]),\n    transforms.Normalize([-0.485, -0.456, -0.406], [1.0, 1.0, 1.0]),\n    transforms.ToPILImage()\n])\n\n\ndef tensor_to_image(image):\n    image = Tensor_to_Image(image)\n    image = np.asarray(image)\n    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n    return image\n'"
utils/loss.py,41,"b'import numpy as np\nimport torch\nimport torch.nn.functional as F\n\n\n# https://github.com/kmaninis/OSVOS-PyTorch\ndef class_balanced_cross_entropy_loss(output, label, size_average=True, batch_average=True):\n    """"""Define the class balanced cross entropy loss to train the network\n    Args:\n    output: Output of the network\n    label: Ground truth label\n    Returns:\n    Tensor that evaluates the loss\n    """"""\n\n    labels = label.float()\n\n    num_labels_pos = torch.sum(labels)\n    num_labels_neg = torch.sum(1.0 - labels)\n    num_total = num_labels_pos + num_labels_neg\n\n    output_gt_zero = torch.ge(output, 0).float()\n\n    loss_val = torch.mul(output, (labels - output_gt_zero)) - torch.log(\n        1 + torch.exp(output - 2 * torch.mul(output, output_gt_zero)))\n\n    loss_pos = torch.sum(-torch.mul(labels, loss_val))\n    loss_neg = torch.sum(-torch.mul(1.0 - labels, loss_val))\n\n    final_loss = num_labels_neg / num_total * loss_pos + num_labels_pos / num_total * loss_neg\n\n    if size_average:\n        final_loss /= int(np.prod(label.size()))\n    elif batch_average:\n        final_loss /= int(label.size(0))\n\n    return final_loss\n\n\ndef hinge_embedding_loss(embedding, num_planes, segmentation, device, t_pull=0.5, t_push=1.5):\n    b, c, h, w = embedding.size()\n    assert(b == 1)\n\n    num_planes = num_planes.numpy()[0]\n    embedding = embedding[0]\n    segmentation = segmentation[0]\n    embeddings = []\n    # select embedding with segmentation\n    for i in range(num_planes):\n        feature = torch.transpose(torch.masked_select(embedding, segmentation[i, :, :].view(1, h, w)).view(c, -1), 0, 1)\n        embeddings.append(feature)\n\n    centers = []\n    for feature in embeddings:\n        center = torch.mean(feature, dim=0).view(1, c)\n        centers.append(center)\n\n    # intra-embedding loss within a plane\n    pull_loss = torch.Tensor([0.0]).to(device)\n    for feature, center in zip(embeddings, centers):\n        dis = torch.norm(feature - center, 2, dim=1) - t_pull\n        dis = F.relu(dis)\n        pull_loss += torch.mean(dis)\n    pull_loss /= int(num_planes)\n\n    if num_planes == 1:\n        return pull_loss, pull_loss, torch.zeros(1).to(device)\n\n    # inter-plane loss\n    centers = torch.cat(centers, dim=0)\n    A = centers.repeat(1, int(num_planes)).view(-1, c)\n    B = centers.repeat(int(num_planes), 1)\n    distance = torch.norm(A - B, 2, dim=1).view(int(num_planes), int(num_planes))\n\n    # select pair wise distance from distance matrix\n    eye = torch.eye(int(num_planes)).to(device)\n    pair_distance = torch.masked_select(distance, eye == 0)\n\n    pair_distance = t_push - pair_distance\n    pair_distance = F.relu(pair_distance)\n    push_loss = torch.mean(pair_distance).view(-1)\n\n    loss = pull_loss + push_loss\n    return loss, pull_loss, push_loss\n\n\ndef surface_normal_loss(prediction, surface_normal, valid_region):\n    b, c, h, w = prediction.size()\n    if valid_region is None:\n        valid_predition = torch.transpose(prediction.view(c, -1), 0, 1)\n        valid_surface_normal = torch.transpose(surface_normal.view(c, -1), 0, 1)\n    else:\n        valid_predition = torch.transpose(torch.masked_select(prediction, valid_region).view(c, -1), 0, 1)\n        valid_surface_normal = torch.transpose(torch.masked_select(surface_normal, valid_region).view(c, -1), 0, 1)\n\n    similarity = torch.nn.functional.cosine_similarity(valid_predition, valid_surface_normal, dim=1)\n\n    loss = torch.mean(1-similarity)\n    mean_angle = torch.mean(torch.acos(torch.clamp(similarity, -1, 1)))\n    return loss, mean_angle / np.pi * 180\n\n\n# L1 parameter loss\ndef parameter_loss(prediction, param, valid_region):\n    b, c, h, w = prediction.size()\n    if valid_region is None:\n        valid_predition = torch.transpose(prediction.view(c, -1), 0, 1)\n        valid_param = torch.transpose(param.view(c, -1), 0, 1)\n    else:\n        valid_predition = torch.transpose(torch.masked_select(prediction, valid_region).view(c, -1), 0, 1)\n        valid_param = torch.transpose(torch.masked_select(param, valid_region).view(c, -1), 0, 1)\n\n    return torch.mean(torch.sum(torch.abs(valid_predition - valid_param), dim=1))\n\n\ndef Q_loss(param, k_inv_dot_xy1, gt_depth):\n    \'\'\'\n    infer per pixel depth using perpixel plane parameter and\n    return depth loss, mean abs distance to gt depth, perpixel depth map\n    :param param: plane parameters defined as n/d , tensor with size (1, 3, h, w)\n    :param k_inv_dot_xy1: tensor with size (3, h*w)\n    :param depth: tensor with size(1, 1, h, w)\n    :return: error and abs distance\n    \'\'\'\n\n    b, c, h, w = param.size()\n    assert (b == 1 and c == 3)\n\n    gt_depth = gt_depth.view(1, h*w)\n    param = param.view(c, h*w)\n\n    # infer depth for every pixel\n    infered_depth = 1. / torch.sum(param * k_inv_dot_xy1, dim=0, keepdim=True)  # (1, h*w)\n    infered_depth = infered_depth.view(1, h * w)\n\n    # ignore insufficient depth\n    infered_depth = torch.clamp(infered_depth, 1e-4, 10.0)\n\n    # select valid depth\n    mask = gt_depth != 0.0\n    valid_gt_depth = torch.masked_select(gt_depth, mask)\n    valid_depth = torch.masked_select(infered_depth, mask)\n    valid_param = torch.masked_select(param, mask).view(3, -1)\n    valid_ray = torch.masked_select(k_inv_dot_xy1, mask).view(3, -1)\n\n    diff = torch.abs(valid_depth - valid_gt_depth)\n    abs_distance = torch.mean(diff)\n\n    Q = valid_ray * valid_gt_depth   # (3, n)\n    q_diff = torch.abs(torch.sum(valid_param * Q, dim=0, keepdim=True) - 1.)\n    loss = torch.mean(q_diff)\n    return loss, abs_distance, infered_depth.view(1, 1, h, w)\n'"
utils/metric.py,0,"b'import numpy as np\n\n\n# https://github.com/davisvideochallenge/davis/blob/master/python/lib/davis/measures/jaccard.py\ndef eval_iou(annotation,segmentation):\n    """""" Compute region similarity as the Jaccard Index.\n\n    Arguments:\n        annotation   (ndarray): binary annotation   map.\n        segmentation (ndarray): binary segmentation map.\n\n    Return:\n        jaccard (float): region similarity\n\n    """"""\n\n    annotation   = annotation.astype(np.bool)\n    segmentation = segmentation.astype(np.bool)\n\n    if np.isclose(np.sum(annotation),0) and np.isclose(np.sum(segmentation),0):\n        return 1\n    else:\n        return np.sum((annotation & segmentation)) / \\\n                np.sum((annotation | segmentation),dtype=np.float32)\n\n\n# https://github.com/art-programmer/PlaneNet/blob/master/utils.py#L2115\ndef eval_plane_prediction(predSegmentations, gtSegmentations, predDepths, gtDepths, threshold=0.5):\n    predNumPlanes = len(np.unique(predSegmentations)) - 1\n    gtNumPlanes = len(np.unique(gtSegmentations)) - 1\n\n    if len(gtSegmentations.shape) == 2:\n        gtSegmentations = (np.expand_dims(gtSegmentations, -1) == np.arange(gtNumPlanes)).astype(np.float32)\n    if len(predSegmentations.shape) == 2:\n        predSegmentations = (np.expand_dims(predSegmentations, -1) == np.arange(predNumPlanes)).astype(np.float32)\n\n    planeAreas = gtSegmentations.sum(axis=(0, 1))\n    intersectionMask = np.expand_dims(gtSegmentations, -1) * np.expand_dims(predSegmentations, 2) > 0.5\n\n    # depthDiffs = np.expand_dims(gtDepths, -1) - np.expand_dims(predDepths, 2)\n    depthDiffs = gtDepths - predDepths\n    depthDiffs = depthDiffs[:, :, np.newaxis, np.newaxis]\n\n    intersection = np.sum((intersectionMask).astype(np.float32), axis=(0, 1))\n\n    planeDiffs = np.abs(depthDiffs * intersectionMask).sum(axis=(0, 1)) / np.maximum(intersection, 1e-4)\n\n    planeDiffs[intersection < 1e-4] = 1\n\n    union = np.sum(((np.expand_dims(gtSegmentations, -1) + np.expand_dims(predSegmentations, 2)) > 0.5).astype(np.float32), axis=(0, 1))\n    planeIOUs = intersection / np.maximum(union, 1e-4)\n\n    numPredictions = int(predSegmentations.max(axis=(0, 1)).sum())\n\n    numPixels = planeAreas.sum()\n\n    IOUMask = (planeIOUs > threshold).astype(np.float32)\n    minDiff = np.min(planeDiffs * IOUMask + 1000000 * (1 - IOUMask), axis=1)\n    stride = 0.05\n    pixelRecalls = []\n    planeStatistics = []\n    for step in range(int(0.61 / stride + 1)):\n        diff = step * stride\n        pixelRecalls.append(np.minimum((intersection * (planeDiffs <= diff).astype(np.float32) * IOUMask).sum(1),\n                                       planeAreas).sum() / numPixels)\n        planeStatistics.append(((minDiff <= diff).sum(), gtNumPlanes, numPredictions))\n\n    return pixelRecalls, planeStatistics\n\n\n#https://github.com/art-programmer/PlaneNet\ndef evaluateDepths(predDepths, gtDepths, validMasks, planeMasks=True, printInfo=True):\n    masks = np.logical_and(np.logical_and(validMasks, planeMasks), gtDepths > 1e-4)\n\n    numPixels = float(masks.sum())\n\n    rmse = np.sqrt((pow(predDepths - gtDepths, 2) * masks).sum() / numPixels)\n    rmse_log = np.sqrt((pow(np.log(predDepths) - np.log(gtDepths), 2) * masks).sum() / numPixels)\n    log10 = (np.abs(\n        np.log10(np.maximum(predDepths, 1e-4)) - np.log10(np.maximum(gtDepths, 1e-4))) * masks).sum() / numPixels\n    rel = (np.abs(predDepths - gtDepths) / np.maximum(gtDepths, 1e-4) * masks).sum() / numPixels\n    rel_sqr = (pow(predDepths - gtDepths, 2) / np.maximum(gtDepths, 1e-4) * masks).sum() / numPixels\n    deltas = np.maximum(predDepths / np.maximum(gtDepths, 1e-4), gtDepths / np.maximum(predDepths, 1e-4)) + (\n            1 - masks.astype(np.float32)) * 10000\n    accuracy_1 = (deltas < 1.25).sum() / numPixels\n    accuracy_2 = (deltas < pow(1.25, 2)).sum() / numPixels\n    accuracy_3 = (deltas < pow(1.25, 3)).sum() / numPixels\n    recall = float(masks.sum()) / validMasks.sum()\n    if printInfo:\n        print((\'evaluate\', rel, rel_sqr, log10, rmse, rmse_log, accuracy_1, accuracy_2, accuracy_3, recall))\n        pass\n    return rel, rel_sqr, log10, rmse, rmse_log, accuracy_1, accuracy_2, accuracy_3, recall\n\n\ndef eval_plane_and_pixel_recall_normal(segmentation, gt_segmentation, param, gt_param, threshold=0.5):\n    """"""\n    :param segmentation: label map for plane segmentation [h, w] where 20 indicate non-planar\n    :param gt_segmentation: ground truth label for plane segmentation where 20 indicate non-planar\n    :param threshold: value for iou\n    :return: percentage of correctly predicted ground truth planes correct plane\n    """"""\n    depth_threshold_list = np.linspace(0.0, 30, 13)\n\n    # both prediction and ground truth segmentation contains non-planar region which indicated by label 20\n    # so we minus one\n    plane_num = len(np.unique(segmentation)) - 1\n    gt_plane_num = len(np.unique(gt_segmentation)) - 1\n\n    # 13: 0:0.05:0.6\n    plane_recall = np.zeros((gt_plane_num, len(depth_threshold_list)))\n    pixel_recall = np.zeros((gt_plane_num, len(depth_threshold_list)))\n\n    plane_area = 0.0\n\n    gt_param = gt_param.reshape(20, 3)\n\n    # check if plane is correctly predict\n    for i in range(gt_plane_num):\n        gt_plane = gt_segmentation == i\n        plane_area += np.sum(gt_plane)\n\n        for j in range(plane_num):\n            pred_plane = segmentation == j\n            iou = eval_iou(gt_plane, pred_plane)\n\n            if iou > threshold:\n                # mean degree difference over overlap region:\n                gt_p = gt_param[i]\n                pred_p = param[j]\n\n                n_gt_p = gt_p / np.linalg.norm(gt_p)\n                n_pred_p = pred_p / np.linalg.norm(pred_p)\n\n                angle = np.arccos(np.clip(np.dot(n_gt_p, n_pred_p), -1.0, 1.0))\n                degree = np.degrees(angle)\n                depth_diff = degree\n\n                # compare with threshold difference\n                plane_recall[i] = (depth_diff < depth_threshold_list).astype(np.float32)\n                pixel_recall[i] = (depth_diff < depth_threshold_list).astype(np.float32) * \\\n                      (np.sum(gt_plane * pred_plane))\n                break\n\n    pixel_recall = np.sum(pixel_recall, axis=0).reshape(1, -1) / plane_area\n\n    return plane_recall, pixel_recall\n\n'"
utils/misc.py,4,"b'import torch\n\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n\n    def __init__(self):\n        self.val = None\n        self.avg = None\n        self.sum = None\n        self.count = None\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef get_optimizer(parameters, cfg):\n    if cfg.method == \'sgd\':\n        optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, parameters),\n                                    lr=cfg.lr, momentum=0.9, weight_decay=cfg.weight_decay)\n    elif cfg.method == \'adam\':\n        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, parameters),\n                                     lr=cfg.lr, weight_decay=cfg.weight_decay)\n    elif cfg.method == \'rmsprop\':\n        optimizer = torch.optim.RMSprop(filter(lambda p: p.requires_grad, parameters),\n                                        lr=cfg.lr, weight_decay=cfg.weight_decay)\n    elif cfg.method == \'adadelta\':\n        optimizer = torch.optim.Adadelta(filter(lambda p: p.requires_grad, parameters),\n                                         lr=cfg.lr, weight_decay=cfg.weight_decay)\n    else:\n        raise NotImplementedError\n    return optimizer\n'"
utils/write_ply.py,0,"b'import numpy as np\n\n\ndef get_K_inv_dot_xy_1(h=192, w=256):\n    focal_length = 517.97\n    offset_x = 320\n    offset_y = 240\n\n    K = [[focal_length, 0, offset_x],\n         [0, focal_length, offset_y],\n         [0, 0, 1]]\n\n    K_inv = np.linalg.inv(np.array(K))\n\n    K_inv_dot_xy_1 = np.zeros((3, h, w))\n\n    for y in range(h):\n        for x in range(w):\n            yy = float(y) / h * 480\n            xx = float(x) / w * 640\n                \n            ray = np.dot(K_inv,\n                         np.array([xx, yy, 1]).reshape(3, 1))\n            K_inv_dot_xy_1[:, y, x] = ray[:, 0]\n\n    return K_inv_dot_xy_1\n\n\nK_inv_dot_xy_1 = get_K_inv_dot_xy_1()\n\n\n# https://github.com/art-programmer/PlaneNet/blob/88e8c8d7e527ce61620b700babc8232de8804f55/code/utils.py#L860\ndef writePLYFileDepth(folder, index, depth, segmentation):\n    h, w = 192, 256\n    imageFilename = str(index) + \'_segmentation_pred_blended_0.png\'\n\n    # create face from segmentation\n    faces = []\n    for y in range(h-1):\n        for x in range(w-1):\n            segmentIndex = segmentation[y, x]\n            # ignore non planar region\n            if segmentIndex == 0:\n                continue\n\n            # add face if three pixel has same segmentatioin\n            depths = [depth[y][x], depth[y + 1][x], depth[y + 1][x + 1]]\n            if segmentation[y + 1, x] == segmentIndex and segmentation[y + 1, x + 1] == segmentIndex and min(depths) > 0 and max(depths) < 10:\n                faces.append((x, y, x, y + 1, x + 1, y + 1))\n\n            depths = [depth[y][x], depth[y][x + 1], depth[y + 1][x + 1]]\n            if segmentation[y][x + 1] == segmentIndex and segmentation[y + 1][x + 1] == segmentIndex and min(depths) > 0 and max(depths) < 10:\n                faces.append((x, y, x + 1, y + 1, x + 1, y))\n\n    with open(folder + \'/\' + str(index) + \'_model.ply\', \'w\') as f:\n        header = """"""ply\nformat ascii 1.0\ncomment VCGLIB generated\ncomment TextureFile """"""\n        header += imageFilename\n        header += """"""\nelement vertex """"""\n        header += str(h * w)\n        header += """"""\nproperty float x\nproperty float y\nproperty float z\nelement face """"""\n        header += str(len(faces))\n        header += """"""\nproperty list uchar int vertex_indices\nproperty list uchar float texcoord\nend_header\n""""""\n        f.write(header)\n        for y in range(h):\n            for x in range(w):\n                segmentIndex = segmentation[y][x]\n                if segmentIndex == 20:\n                    f.write(""0.0 0.0 0.0\\n"")\n                    continue\n                ray = K_inv_dot_xy_1[:, y, x]\n                X, Y, Z = ray * depth[y, x]\n                f.write(str(X) + \' \' + str(Y) + \' \' + str(Z) + \'\\n\')\n\n        for face in faces:\n            f.write(\'3 \')\n            for c in range(3):\n                f.write(str(face[c * 2 + 1] * w + face[c * 2]) + \' \')\n            f.write(\'6 \')\n            for c in range(3):\n                f.write(str(float(face[c * 2]) / w) + \' \' + str(1 - float(face[c * 2 + 1]) / h) + \' \')\n            f.write(\'\\n\')\n        f.close()\n    return\n'"
